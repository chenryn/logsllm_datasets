### A3: Flexible and Extensible Anonymity System

#### 3.2 Execution Model
A3LOG rules can be executed either in a centralized manner, such as a standard Datalog rule (e.g., `r1`), or in a distributed fashion, as in `r2`. In A3, initiators define relay selection policies using local rules (Section 5) and path instantiation using distributed rules, which are essentially a series of distributed recursive queries (as detailed in Section 6).

A3LOG shares a similar execution model with the Click modular router [18], which consists of interconnected elements to implement various network and flow control components. In the context of A3LOG, these elements include database operators (such as joins, aggregations, selections, and projections) that are directly generated from queries. For more details on the compilation process and execution model used in declarative networking, refer to [19].

#### 3.3 Materialized Soft-state Tables and Events
Declarative networking incorporates a soft-state storage model, where each relation has an explicit "time to live" (TTL) or lifetime. All facts in the relation must be periodically updated before their TTL expires, or they will be deleted.

A3LOG supports materialized soft-state tables through the `materialize` directive, which specifies the TTL of each relation. The `materialize` directive has the following form:
```sql
materialize(Relation, Timeout, MaxEntries, Keys)
```
- **Relation**: The name of the relation.
- **Timeout**: The maximum time in seconds that any fact in the relation may persist.
- **MaxEntries**: The maximum number of facts allowed in the relation before older facts are ejected according to a FIFO policy.
- **Keys**: Specifies the primary keys of the relation.

If a new fact is derived with the same primary keys as an existing fact in the same relation, the new fact replaces the old one, and the TTL is reset.

If a relation has no corresponding `materialize` directive, it is treated as an event predicate with zero lifetime. Event predicates, whose names are prefixed with an "e", denote transient tables used as input to rules.

#### 4. A3 Design Goals and Architecture
A3 is designed as a flexible and extensible anonymity system where protocol designers can publish their specific relay selection and path instantiation algorithms, along with descriptions of their performance and anonymity trade-offs. Unlike existing systems where relay selection algorithms are hardcoded, A3 allows senders to provide custom relay selection policies that specify how relays are chosen for anonymous paths.

The A3LOG policy language (Section 5) enables applications to intelligently tune relay selection for performance or anonymity [40, 36]. It also allows applications to define their own performance metrics, such as bandwidth, latency, loss, jitter, or a combination thereof. Additionally, A3 supports customization of path instantiation policies (Section 6).

A3's use of declarative networking provides the capability for applications to rapidly customize and refine policies to meet their constraints. However, our system does not preclude similar tuning outside the use of declarative languages. For example, a user unfamiliar with declarative policy languages can download and install an A3LOG policy that produces low-latency and low-jitter paths for VoIP applications, while using a different policy for high-throughput anonymous web browsing.

Since the anonymity of a path depends significantly on relay selection and path instantiation, policies should be used cautiously until their security properties are fully understood. A thorough review of the performance and anonymity properties of various relay selection and path instantiation algorithms is beyond the scope of this paper. Our goal is to provide a flexible architecture for developing, testing, and studying path strategies and implementations, making A3 a useful tool for conducting security evaluations.

#### 4.1 System Overview
An application, or a proxy acting on its behalf, provides relay and path instantiation policies reflecting the application's communication requirements. Figure 1 illustrates the architecture of the A3 client running on the initiator's host.

- **Relay Selection Engine**: Interprets the initiator's relay selection policy and applies it to produce (but not instantiate) an anonymous path consisting of relays from the Local Directory Cache. The cache is populated by periodically contacting a Directory Server to obtain membership information (a list of available relays) and, optionally, data from Information Providers.
- **Forwarding Engine**: Instantiates the path according to the provided path instantiation policy. After path establishment, a Proxy Service on the local machine intercepts the application's traffic and routes it through the anonymous path. Similarly, incoming data from the anonymous channel is transparently forwarded through the Proxy Service to the application.

#### 4.2 Components of A3

##### 4.2.1 Information Providers
To support non-trivial relay selection policies, A3 uses Information Providers that aggregate node and link performance data. Policies can utilize this information to more precisely define their requirements (e.g., "include only relays that have been online for at least an hour").

A3 imposes few restrictions on the types of Information Providers. Each Provider is interfaced through an adapter that resides on the A3 relay. Adapters are small programs or scripts that periodically query a Provider for new information, storing the results in the Local Directory Cache. Our current implementation includes adapters for Vivaldi [6] and CoMon [27], but others can be easily constructed.

**Network Coordinate Information Providers**: Traditional anonymous relay selection algorithms (e.g., Tor [9] and Snader and Borisov [40]) favor relays with high bandwidths. However, applications may also prefer paths with low latency, which is a link characteristic. Network coordinate systems, such as Vivaldi [6], PIC [5], NPS [25], and Big Bang Simulation [34], map each relay to multidimensional coordinates, allowing pairwise latencies to be estimated with high accuracy and low overhead. These systems linearize the information that must be stored and maintained by the Information Provider.

**Relay-Assisted Information Providers**: Relays can measure local performance indicators like upstream and downstream throughput, processor usage, and available memory. This information can be collected and stored in a Relay-Assisted Information Provider, such as the CoMon Monitoring Infrastructure [27] on PlanetLab [28].

**Other Potential Information Providers**: Systems like iPlane [21], IDMaps [11], OASIS [12], and Meridian [42] map the structure of the Internet and provide estimates of latency and bandwidth between arbitrary hosts. These systems can be adapted to provide information for A3's relay selection policies.

##### 4.2.2 Local Directory Cache
The Local Directory Cache periodically queries and stores performance data from Information Providers. The rate at which the cache polls Providers affects both the freshness of the data and the relay's communication overhead. The trade-off between update intervals and bandwidth costs depends on the rate at which performance characteristics change in the network, as explored in Section 7.

The Local Directory Cache uses adapters to query Information Providers and store the results in tables accessible by the Relay Selection Engine. Adapters define tables using the `materialize` keyword, as described in Section 3. For example, the following A3LOG statements:

```sql
materialize(tBandwidth, Infinity, Infinity, keys(1)).
materialize(tVivaldiCoordinates, Infinity, Infinity, keys(1)).
tBandwidth("10.0.0.1", 1000, 500, 3000).
tVivaldiCoordinates("10.0.0.1", [10,-6]).
```

create two tables: `tBandwidth` and `tVivaldiCoordinates`. The former holds the address of a remote node, its upstream and downstream bandwidth, and bandwidth capacity. The `keys(1)` argument specifies that an existing tuple should be replaced if a new tuple arrives with the same first field (the network address). The latter table stores the coordinates of a remote node. These statements are executed by the adapter as new data is polled from Information Providers.

##### 4.2.3 Relay Selection Engine
The Relay Selection Engine provides the flexibility to...