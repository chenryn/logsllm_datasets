When using the openblas based pypi package for numpy on windows, a simply
import allocates a lot (25-30MB/core) of committed memory per CPU core by
default. This leads to MemoryErrors due to memory exhaustion, when all memory
of a machine is committed used up. This can easily happen when using
multiprocessing or other multi process architectures on a server with many cpu
cores. Basically kills the server, unless you have at least (core*core)*30MB
of RAM free.
This is made worse by packages that simply import numpy for functionality
unrelated to openblas, like `openpyxl` that only import some type definitions
from numpy, so the memory usage comes as a big surprise.
The problem why this happens is OpenBLAS usage of VirtualAlloc to allocate
buffers on init. Using the MEMORY_COMMIT flag right away to emulate Linux mmap
behaviour, instead of using only MEMORY_RESERVE and a later MEMORY_COMMIT on
first actual use of the buffers, so forcing the reservation of physical
(commited) memory instead of just virtual memory.
Setting the OPENBLAS_NUM_THREADS=1 envvar fixes the memory allocation, but i
did not see any direct mention of it related to memory usage, only to
threading behaviour.
It would be nice if just importing the module did not allocate and commit all
that memory.
### Reproducing code example:
    import numpy
Check the memory usage with sysinternal vmmap.  
https://docs.microsoft.com/en-us/sysinternals/downloads/vmmap
The usage in the 'private data' section (private bytes in process explorer)
grows by megabytes, proportional to the number of CPU cores.
This can be reduced to zero growth when `OPENBLAS_NUM_THREADS` is set to 1
before the import.
### Numpy/Python version information:
> > > print(numpy. **version** , sys.version)  
>  ('1.16.2', '2.7.15 (v2.7.15:ca079a3ea3, Apr 30 2018, 16:30:26) [MSC v.1500
> 64 bit (AMD64)]')