10      mode="system"
11    }
12  )
```
我们总结了使用 CPU 的比率，并将其除以 CPU 总数。在我的例子中(截图如下)，目前只有 3%到 4%的 CPU 被使用。
这并不奇怪，因为大部分系统都处于静止状态。我们的集群目前没有发生太多事情。
![](img/2d2bf1f8-7ac6-4d21-a067-c6c523f0e45e.png)
Figure 3-24: Prometheus' graph screen with the percentage of available CPU
现在我们知道了如何获取整个集群中已用 CPU 的百分比，我们将把注意力转移到应用上。
我们将尝试发现我们有多少可分配的内核。从应用的角度来看，至少当它们在 Kubernetes 中运行时，可分配的 CPU 会显示 Pods 可以请求多少。可分配的中央处理器总是低于总中央处理器。
请键入下面的表达式，然后按“执行”按钮。
```
 1  kube_node_status_allocatable_cpu_cores
```
输出应该低于虚拟机使用的内核数量。可分配的内核显示了可分配给容器的 CPU 数量。更准确地说，可分配内核是指分配给节点的 CPU 数量减去系统级进程保留的 CPU 数量。在我的例子中(截图如下)，几乎有两个完全可分配的 CPU。
![](img/7283ecb4-bc9a-4432-ad59-3d1469a62c5f.png)
Figure 3-25: Prometheus' graph screen with the allocatable CPU for each of the nodes in the cluster
然而，在这种情况下，我们对可分配的 CPU 总量感兴趣，因为我们试图发现我们的 Pods 在整个集群中使用了多少。因此，我们将对可分配的内核进行求和。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(
 2    kube_node_status_allocatable_cpu_cores
 3  )
```
在我的例子中，总的可分配 CPU 大约是 5.8 个核心。具体数字，请悬停在图表线上。
现在我们知道了我们有多少可分配的 CPU，我们应该尝试发现 Pods 请求了多少。
请注意，请求的资源与使用的资源不同。我们稍后将讨论这个用例。目前，我们想知道我们向系统请求了多少。
请键入下面的表达式，然后按“执行”按钮。
```
 1  kube_pod_container_resource_requests_cpu_cores
```
我们可以看到请求的 CPU 相对较低。在我的例子中，所有请求了 CPU 的容器的值都低于 0.15(一百五十毫秒)。你的结果可能会不同。
就像可分配的中央处理器一样，我们对请求的中央处理器的总和感兴趣。稍后，我们将能够组合这两个结果，并推断集群中还有多少未保留。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(
 2    kube_pod_container_resource_requests_cpu_cores
 3  )
```
我们汇总了所有的 CPU 资源请求。因此，在我的情况下(截图如下)，所有请求的 CPU 都略低于 1.5。
![](img/0a3f5d81-4d68-4f87-8a3a-6c8c8bcfc4dd.png)
Figure 3-26: Prometheus' graph screen with the sum of the requested CPU
现在，让我们将这两个表达式结合起来，看看请求的 CPU 百分比。
请键入下面的表达式，然后按“执行”按钮。
```
 1  sum(
 2    kube_pod_container_resource_requests_cpu_cores
 3  ) /
 4  sum(
 5    kube_node_status_allocatable_cpu_cores
 6  )
```
在我的例子中，输出显示大约四分之一(0.25)的所有可分配 CPU 被保留。这意味着，在我们达到扩展集群的需求之前，我们可能会有四倍的 CPU 请求。当然，您已经知道，如果存在，集群自动缩放器将在此之前添加节点。尽管如此，知道我们接近达到 CPU 极限还是很重要的。群集自动缩放器可能工作不正常，或者甚至可能不活动。后一种情况适用于大多数(如果不是所有)内部集群。
让我们看看是否可以将我们探索的表达式转换为警报。
我们将探索一组新的图表值和我们以前使用的图表值之间的另一个区别。
```
 1  diff mon/prom-values-latency2.yml \
 2      mon/prom-values-cpu.yml
```
输出如下。
```
64c64
 0.1
---
>   expr: sum(rate(nginx_ingress_controller_requests[5m])) by (ingress) / sum(label_join(kube_deployment_status_replicas, "ingress", ",", "deployment")) by (ingress) > 1
87a88,103
> - alert: NotEnoughCPU
>   expr: sum(rate(node_cpu_seconds_total{mode!="idle", mode!="iowait", mode!~"^(?:guest.*)$"}[5m])) / count(node_cpu_seconds_total{mode="system"}) > 0.9
```
```
>   for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: There's not enough CPU
>     description: CPU usage of the cluster is above 90%
> - alert: TooMuchCPURequested
>   expr: sum(kube_pod_container_resource_requests_cpu_cores) / sum(kube_node_status_allocatable_cpu_cores) > 0.9
>   for: 30m
>   labels:
>     severity: notify
>   annotations:
>     summary: There's not enough allocatable CPU
>     description: More than 90% of allocatable CPU is requested
```
从差异中我们可以看出，我们将`TooManyRequests`的原始阈值恢复到了`1`，并且增加了两个新的警报，分别叫做`NotEnoughCPU`和`TooMuchCPURequested`。
如果整个集群中超过 90%的 CPU 使用时间超过 30 分钟，将会触发`NotEnoughCPU`警报。这样，如果 CPU 使用率出现暂时峰值，我们将避免设置警报。
`TooMuchCPURequested`也有百分之九十的阈值，持续超过三十分钟就会触发。该表达式计算请求的总量除以可分配的 CPU 总量。
这两个警告都是我们不久前执行的普罗米修斯表达式的反映，所以您应该已经熟悉了它们的目的。
让我们用新的值升级普罗米修斯的图表，并打开警报屏幕。
```
 1  helm upgrade -i prometheus \
 2    stable/prometheus \
 3    --namespace metrics \
 4    --version 7.1.3 \
 5    --set server.ingress.hosts={$PROM_ADDR} \
 6    --set alertmanager.ingress.hosts={$AM_ADDR} \
 7    -f mon/prom-values-cpu.yml
 8
 9  open "http://$PROM_ADDR/alerts"
```
剩下的就是等待两个新的警报出现。如果它们还没有出现，请刷新屏幕。
可能没有必要看到新的警报在起作用。到目前为止，你应该相信流量，没有理由相信它们不会触发。
![](img/73c6acaf-55dd-42ec-a35c-ccfadef135cd.png)
Figure 3-27: Prometheus' alerts screen
在“真实世界”场景中，根据我们使用的 Kubernetes 风格，接收两个警报之一可能会引发不同的反应。
如果我们有集群自动缩放器，我们可能不需要`NotEnoughCPU`和`TooMuchCPURequested`警报。只要我们的中央处理器请求设置正确，90%的节点中央处理器正在使用的事实并不妨碍集群正常运行。同样，保留 90%的可分配 CPU 也不是问题。如果 Kubernetes 由于所有 CPU 都被保留而无法安排一个新的 Pod，它将扩展集群。事实上，达到几乎满的 CPU 使用率或保留几乎所有可分配的 CPU 是一件好事。这意味着我们拥有所需数量的 CPU，并且我们不会为未使用的资源付费。不过，这种逻辑主要适用于云提供商，甚至不是所有提供商。今天(2018 年 10 月)，Cluster Autoscaler 仅在 AWS、GCE 和 Azure 中工作。
所有这些并不意味着我们应该只依赖集群自动缩放器。它会像其他东西一样失灵。然而，由于 CA 是基于观察不可聚合的 Pod，如果它确实无法工作，我们应该通过观察 Pod 的状态来检测，而不是 CPU 的使用。尽管如此，当 CPU 使用率过高时收到警报可能不是一个坏主意，但在这种情况下，我们可能希望将阈值提高到接近百分之百的值。
如果我们的群集是本地的，或者更准确地说，如果它没有群集自动缩放器，那么如果我们的群集扩展过程不是自动的或者速度很慢，那么我们探索的警报是必不可少的。逻辑很简单。如果我们需要几分钟以上的时间向集群添加新节点，我们不能等到 Pods 不可聚合。那就太晚了。相反，我们需要知道在群集变满(饱和)之前，我们已经没有可用容量了，这样我们就有足够的时间通过向群集添加新节点来做出反应。
尽管如此，拥有一个因为集群自动缩放器不起作用而不能自动缩放的集群并不是一个足够好的借口。我们可以使用许多其他工具来自动化我们的基础架构。当我们成功到达可以自动向集群添加新节点的位置时，警报的目的地应该会改变。我们可能不想接收对 Slack 的通知，而是希望向服务发送一个请求，该服务将执行脚本，从而将新节点添加到集群中。如果我们的集群运行在虚拟机上，我们总是可以通过脚本(或一些工具)添加更多。
接收这些通知给 Slack 的唯一真正借口是我们的集群是否在裸机上运行。在这种情况下，我们不能指望脚本神奇地创建新的服务器。对于其他所有人来说，在适当的自动化到位之前，当使用了太多的 CPU 或所有分配的 CPU 都被保留时，Slack 通知应该只是一个临时的解决方案。
现在，让我们尝试完成类似的目标，但是，这一次，通过测量内存使用和保留。
测量内存消耗类似于 CPU，但是我们应该考虑一些差异。但是，在我们到达那里之前，让我们回到普罗米修斯的图形屏幕，探索我们的第一个与内存相关的指标。
```
 1  open "http://$PROM_ADDR/graph"
```
就像 CPU 一样，首先我们需要找出每个节点有多少内存。
请输入下面的表达式，按下执行按钮，切换到*图形*选项卡。
```
 1  node_memory_MemTotal_bytes
```
你的结果可能和我的不同。在我的例子中，每个节点都有大约 4 GB 的内存。
如果不知道当前有多少可用内存，知道每个节点有多少内存是没有用的。我们可以通过`node_memory_MemAvailable_bytes`度量来获得这个信息。
请键入下面的表达式，然后按“执行”按钮。
```
 1  node_memory_MemAvailable_bytes
```
我们可以看到集群中每个节点上的可用内存。在我的例子中(截图如下)，每个都有大约 3 GB 的可用内存。
![](img/67a644d2-43b6-4eb3-902f-56c07fffd073.png)
Figure 3-28: Prometheus' graph screen with available memory in each of the nodes of the cluster
现在我们知道了如何从每个节点获取总的可用内存，我们应该组合查询来获取整个集群的已用内存百分比。
请键入下面的表达式，然后按“执行”按钮。
```
 1  1 -
 2  sum(
 3    node_memory_MemAvailable_bytes
 4  ) /
 5  sum(
 6    node_memory_MemTotal_bytes
 7  )
```
因为我们正在搜索已用内存的百分比，并且我们有可用内存的度量，所以我们以`1 -`开始表达式，它将反转结果。表达式的其余部分是可用内存和总内存的简单划分。在我的例子中(下面的截图)，每个节点上使用的内存不到 30%。
![](img/9144a5c6-047c-4b6d-b6cf-d3e1e63235a4.png)
Figure 3-29: Prometheus' graph screen with the percentage of available memory
就像 CPU 一样，可用内存和总内存并不能描绘出全貌。虽然这是有用的信息，也是潜在警报的基础，但我们还需要知道有多少内存是可分配的，以及 Pods 使用了多少内存。我们可以通过`kube_node_status_allocatable_memory_bytes`度量得到第一个数字。
请键入下面的表达式，然后按“执行”按钮。
```
 1  kube_node_status_allocatable_memory_bytes
```
根据 Kubernetes 风格和您使用的主机提供商，总内存和可分配内存之间可能会有很小或很大的差异。我正在 AKS 中运行集群，可分配内存比总内存少一整 GB。前者约为 3 GB 内存，后者约为 4 GB 内存。差别很大。我的 Pods 没有完整的 4 GB，但比这少了大约四分之一。剩下的大约 1 GB 内存用于系统级服务。更糟糕的是，每个节点上花费了 1 GB 的内存，在我的例子中，由于我的集群有三个节点，所以总共减少了 3 GB。考虑到内存总量和可分配内存量之间的巨大差异，拥有更少数量的更大节点有明显的好处。尽管如此，并不是每个人都需要大节点，如果我们希望将节点分布在所有区域，将节点数量减少到 3 个以下可能不是一个好主意。
现在我们知道了如何检索可分配的内存量，让我们看看如何为每个应用获取请求的内存量。