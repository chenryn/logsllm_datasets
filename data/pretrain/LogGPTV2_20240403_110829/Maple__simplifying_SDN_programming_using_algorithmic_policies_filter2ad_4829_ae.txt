paths from two sources to the same destination merge, the remain-
ing steps are the same. Maple identiﬁes safe cases where multiple
rules to the same destination can be replaced by a single, broader
rule that matches on destination only. Algorithm 6 shows the de-
tails of the algorithm. The elimination of rules with ToController
as action improves the effectiveness of the optimization, because
otherwise, the optimization will often fail to ﬁnd agreement among
overlapping rules.
THEOREM 3. ROUTEAGGREGATION does not alter the forward-
ing behavior of the network, provided rules at ingress ports include
all generated rules with action ToController.
5. MAPLE MULTICORE SCHEDULER
Even with the preceding optimizations, the Maple controller must
be powerful and scalable enough to service the “cache misses” from
a potentially large number of switches. Scalability and parallelism
within the controller are thus critical to achieving scalability of the
SDN as a whole. This section outlines key details of our imple-
mentation, in particular, on the scheduler, to achieve graceful scal-
ability. A technical report offers more details [22].
Programming language and runtime: While Maple’s architec-
ture is independent of any particular language or runtime infras-
tructure, its exact implementation will be language-dependent. Our
current system allows a programmer to deﬁne an algorithmic policy
f in either Haskell or Java. In this paper, we focus on implement-
ing f in Haskell, using the Glasgow Haskell Compiler (GHC). We
explore the scheduling of f on processor cores, efﬁciently manag-
ing message buffers, parsing and serializing control messages to
reduce memory trafﬁc, synchronization between cores, and syn-
chronization in the language runtime system.
Afﬁnity-based, switch-level scheduling of f on cores: Consider
application of f at the controller on a sequence of packets. One can
classify these packets according to the switches that originated the
requests. Maple’s scheduler uses afﬁnity-based, switch-level paral-
lel scheduling. Processing by f of packets from the same switch is
handled by a single, lightweight user-level thread from a set of user-
level threads atop of a smaller set of CPU cores [11, 12]. The sched-
uler maintains afﬁnity between lightweight threads and hardware
cores, shifting threads between cores only in response to persistent
load imbalances. This afﬁnity reduces the latency introduced by
transferring messages between cores, and enables threads serving
“busy” switches to retain cached state relevant to that switch.
This design provides abundant ﬁne-grained parallelism, based
on two assumptions. First, a typical network includes many more
switches than the controller has cores. Second, each individual
switch generates request trafﬁc at a rate that can easily be processed
by a single processor core. We expect these assumptions to hold in
realistic SDN networks [14].
Achieving I/O scalability: Bottlenecks can arise due to synchro-
nization in the handling of I/O operations. To achieve I/O scala-
bility, Maple further leverages its thread-per-switch design by en-
suring that each OS-level thread invokes I/O system calls (read,
write, and epoll in this case) on sockets for switches currently
assigned to that particular core. Since the scheduler assigns each
switch to one core at a time, this afﬁnity avoids I/O-related con-
tention on those sockets both at application level and within the OS
kernel code servicing system calls on those sockets.
Further, Maple internally processes messages in batches to han-
dle the issue that an OpenFlow switch typically sends on a TCP
stream many short, variable-length messages preceded by a length
ﬁeld, potentially requiring two expensive read system calls per
message. By reading batches of messages at once, and sending
batches of corresponding replies, Maple reduces system call costs.
Large batched read operations often leave the head of a switch’s
message stream in between message boundaries. This implies that
using multiple user-level threads to process a switch’s message
stream would cause frequent thread stalls while one thread waits
for the parsing state of a previous parsing thread to become avail-
able. Large non-atomic write operations cause similar problems
on the output path. However, since Maple dedicates a thread to
each switch, its switch-level parallelism avoids unnecessary syn-
chronization and facilitates efﬁcient request batching.
6. EVALUATIONS
In this section, we demonstrate that (1) Maple generates high
quality rules, (2) Maple can achieve high throughputs on augmenta-
tion and invalidation, and (3) Maple can effectively scale controller
computation over large multicore processors.
6.1 Quality of Maple Generated Flow Rules
We ﬁrst evaluate if Maple generates compact switch ﬂow rules.
Algorithmic policies: We use two types of policies. First, we use a
simple data center routing policy named mt-route. Speciﬁcally,
the network is divided into subnets, with each subnet assigned a
95/24 IPv4 preﬁx. The subnets are partitioned among multiple ten-
ants, and each tenant is assigned its own weights to network links
to build a virtual topology when computing shortest paths. Upon
receiving a packet, the mt-route policy reads the /24 preﬁxes of
both the source and the destination IPv4 addresses of the packet,
looks up the tenants of the source and the destination using the IP
preﬁxes, and then computes intra-tenant routing (same tenant) or
inter-tenant routing (e.g., deny or through middleboxes).
Second, we derive policies from ﬁlter sets generated by Class-
bench [20]. Speciﬁcally, we use parameter ﬁles provided with
Classbench to generate ﬁlter sets implementing Access Control Lists
(ACL), Firewalls (FW), and IP Chains (IPC). For each parame-
ter ﬁle, we generate two ﬁlter sets with roughly 1000 and 2000
rules, respectively. The ﬁrst column of Table 1 names the gener-
ated ﬁlter sets, and the second indicates the number of ﬁlters in each
Classbench-generated ﬁlter set (except for the mt-route policy,
which does not use a ﬁlter set). For example, acl1a and acl1b
are two ﬁlter sets generated from a parameter ﬁle implementing
ACL, with 973 and 1883 ﬁlters respectively. We program an f that
acts as a ﬁlter set interpreter, which does the following for a given
input ﬁlter set: upon receiving a packet, the policy tests the packet
against each ﬁlter, in sequence, until it ﬁnds the ﬁrst matching ﬁlter,
and then returns an action based on the matched rule. Since TCP
port ranges are not directly supported by Openﬂow, our interpreter
checks most TCP port ranges by reading the port value and then
performing the test using program logic. However, if the range con-
sists of all ports the interpreter omits the check, and if it consists of
a single port the interpreter performs an equality assertion. Further-
more, the interpreter takes advantage of a Maple extension which
allows a user-deﬁned f to perform a single assertion on multiple
conditions. The interpreter makes one or more assertions per ﬁlter,
and therefore makes heavy use of T nodes, unlike mt-route.
Packet-in: For each Classbench ﬁlter set, we use the trace ﬁle
(i.e., a sequence of packets) generated by Classbench to exercise
it. Since not all ﬁlters of a ﬁlter set are triggered by its given Class-
bench trace, we use the third column of Table 1 to show the number
of distinct ﬁlters triggered for each ﬁlter set. For the mt-route
policy, we generate trafﬁc according to [2], which provides a char-
acterization of network trafﬁc in data centers.
In our evaluations, each packet generates a packet-in message
at a variant of Cbench [5] — an Openﬂow switch emulator used to
benchmark Openﬂow controllers — which we modiﬁed to generate
packets from trace ﬁles. For experiments requiring accurate mea-
surements of switch behaviors such as ﬂow table misses, we further
modiﬁed Cbench to maintain a ﬂow table and process packets ac-
cording to the ﬂow table. This additional code was taken from the
Openﬂow reference switch implementation.
Results: Table 1 shows the results. We make the following obser-
vations. First, Maple generates compact switch ﬂow tables. For
example, the policy acl1a has 973 ﬁlters, and Maple generates a
total of only 1006 Openﬂow rules (see column 4) to handle pack-
ets generated by Classbench to test acl1a. The number of ﬂow
rules generated by Maple is typically higher than the number of
ﬁlters in a ﬁlter set, due to the need to turn port ranges into exact
matches and to add barriers to handle packets from ports that have
not been exactly matched yet. Deﬁne Maple compactness for each
ﬁlter set as the ratio of the number of rules generated by Maple
over the number of rules in the ﬁlter set. One can see (column 5)
that the largest compactness ratio is for acl3b, which is still only
1.31. One can also evaluate the compactness by using the triggered
ﬁlters in a ﬁlter set. The largest is still for acl3b, but at 2.09.
Second, we observe that Maple is effective at implementing com-
plex ﬁlter sets with a small number of ﬂow table priorities (column
Alg. policy
mt-route
acl1a
acl2a
acl3a
fw1a
fw2a
ipc1a
ipc2a
acl1b
acl2b
acl3b
fw1b
fw2b
ipc1b
ipc2b
#Filts
#Trg
973
949
989
856
812
977
689
1883
1834
1966
1700
1747
1935
1663
604
595
622
539
516
597
442
1187
1154
1234
1099
1126
1227
1044
#Rules
73563
1006
926
1119
821
731
1052
466
1874
1816
2575
1775
1762
2097
1169
Cmpkt
1.03
0.98
1.13
0.96
0.90
1.08
0.68
1.00
0.99
1.31
1.04
1.01
1.08
0.70
#Pr Mods/Rule
1.00
2.25
10.47
2.87
17.65
10.66
4.20
6.73
5.35
5.02
6.13
18.32
7.69
9.49
10.02
1
9
85
33
79
56
81
26
18
119
119
113
60
112
31
Table 1: Numbers of ﬂow rules, priorities and modiﬁcations
generated by Maple for evaluated policies.
6). For example, it uses only 9 priorities for acl1a, which has 973
ﬁlters. The mt-route policy uses only one priority level.
Third, operating in an online mode, Maple does need to issue
more ﬂow table modiﬁcation commands (column 7) than the ﬁnal
number of rules. For example, for acl1a, on average, 2.25 switch
modiﬁcation commands are issued for each ﬁnal ﬂow rule.
6.2 Effects of Optimizing Flow Rules
Maple generates wildcard ﬂow rules when possible to reduce
ﬂow table “cache” misses. To demonstrate the beneﬁts, we use the
mt-route policy and compare the performance of Maple with
that of a simple controller that uses only exact matches.
Flow table miss rates: We measure the switch ﬂow table miss rate,
deﬁned as the fraction of packets that are diverted from a switch
to the controller, at a single switch. We generate network trafﬁc
for a number of sessions between 10 hosts and 20 servers, each in
distinct tenants, with an average of 4 TCP sessions per pair of hosts.
Figure 6(a) shows the ﬂow table miss rates of Maple compared with
those of the exact-match controller, as a function of the number of
TCP packets per ﬂow, denoted F . We vary F from 4 to 80 packets
per ﬂow, as the size of most data center ﬂows fall in this range [2].
As expected, the exact match controller incurs a miss rate of ap-
proximately 1/F , for example incurring 25.5% and 12.8% miss
rates for F = 4 and F = 8, respectively. In contrast, Maple incurs
a miss rate 3 to 4 times lower, for example 6.7% and 4.1% at F = 4
and F = 8. Maple achieves this improvement by generating rules
for this policy that match only on source and destination addresses,
which therefore decreases the expected miss rate by a factor of 4,
the average number of ﬂows per host pair.
Real HP switch load: We further measure the effects using 3 real
HP 5406 Openﬂow switches (s1, s2, and s3). We build a simple
topology in which s1 connects to s2 and s2 connects to s3. A client
running httperf [17] at subnet 1 connected at s1 makes HTTP re-
quests to an HTTP server at subnet 3 connected at s3.
We use three controllers. The ﬁrst two are the same as above:
exact match and mt-route, which modiﬁed to match only on IP
and transport ﬁelds to accommodate the switches’ restrictions on
which ﬂows can be placed in hardware tables. We interpret an exact