### Generalized Mutual Agreement Analysis

To extend the application of mutual agreement analysis, we propose the introduction of a new outcome category, "uncertain," in addition to the traditional predictions of "benign" and "malicious." For instance, instead of dividing the vote region into two halves, we divide it into four quadrants. In the 0% to 25% range, the majority of votes agree that the result is negative (benign). Similarly, in the 75% to 100% range, the majority of votes agree that the result is positive (malicious). However, if the score falls between 25% and 75%, the individual classifiers disagree, and the outcome is classified as uncertain.

To facilitate comparison with simple ensemble voting predictions, this uncertain region can be further divided into two sub-regions:
- **Uncertain (Benign):** 25% to 50%
- **Uncertain (Malicious):** 50% to 75%

These classification outcomes are illustrated in Table II. The uncertain rate (UR) is defined as the proportion of observations that fall within the uncertain range.

### Quantifying Ensemble Classifier Agreement

To provide a more precise measure of agreement among individual votes in an ensemble classifier, we introduce the following metric:

\[ A = |v - 0.5| \times 2 \]

Where \( A \) is the ensemble classifier mutual agreement rate, and \( v \) is the proportion of votes for either class. This function, demonstrated in Figure 1, also shows the classifier outcomes resulting from a 50% mutual agreement threshold. The function's shape is driven by the end and middle points:
- If the classifier vote ratio is 0 or 1, the classifier has full agreement, and the mutual agreement rate is 1 (or 100%).
- If the classifier is split with 0.5 votes for each class, the mutual agreement rate is at its minimum, 0 (or 0%).

The choice of a linear function for this relationship is straightforward and allows for the selection of a given threshold on the classifier vote scores. The function does not need to follow the distribution of scores; any continuous curve would suffice.

### Threshold for Mutual Agreement

The threshold for mutual agreement is the boundary above which the classifier is considered to be in a state of ensemble agreement, and the resulting classification is deemed valid. Below this threshold, the classification is considered specious. We use a 50% boundary throughout most of this paper, but this value should be adjusted by the operator. Decreasing this threshold reduces the number of observations in the disagreement or uncertain classification zone. The tuning of this threshold is discussed in detail in Section VI.

### Efficacy in Identifying Poor Performances

Mutual agreement analysis is effective in identifying specific samples where the classifier performs poorly. In the context of evasion attacks, ensemble mutual agreement serves as a criterion for separating novel attacks and weak mimicry attacks from effective mimicry attacks. For novel attacks, the voting result is often distributed around 50%, indicating that the observations under consideration do not map consistently close to either benign or malicious samples in the training set. Since these attacks fall into the relatively rare uncertain range, they are easily discerned and are considered weak evasions. Strong mimicry attacks, on the other hand, have a vote distribution close to that of benign observations. Thus, typical novel attacks are identified by mutual agreement analysis, but strong mimicry attacks cannot be. Since uncertain observations are poorly supported by the training set, adding them to the training set can improve classifier accuracy.

### Operational Application

In practice, mutual agreement analysis is used to prevent evasion of an intrusion detection system. The mutual agreement rate is derived from the result provided by an ensemble classifier at the time of detection. The operator can use ensemble classifier agreement in various ways, including adjusting the vote threshold to prevent false positives or false negatives, filtering observations for quarantine or more detailed analysis, and prioritizing alerts. The strength of mutual agreement analysis lies in its ability to identify probable intrusion detection evasion at the time of evasion attempts.

### Evaluation

#### PDFrate Operational Data Set

We applied mutual agreement analysis to PDFrate scores for documents from a network monitor processing files transferred through web and email. The data set includes 110,000 PDF documents, which we randomly partitioned into two sets: an operational evaluation set with 100,000 documents and an operational training set with 10,000 documents. Ground truth was determined by scanning with multiple antivirus engines months after collection. These data sets included 273 and 24 malicious documents, respectively. Tables III and IV show the scores for the operational evaluation data set using both the Contagio and University classifiers of PDFrate. Figures 2 and 3 illustrate the distribution of PDFrate Contagio classifier scores for benign and malicious samples in the operational evaluation data set.

It is important to note that the scores for benign and malicious examples are heavily weighted to the far ends of their respective score ranges, with the distribution falling off quickly. In a typical system deployment, the number of observations in the uncertain range is very small, and the majority of misclassifications fall within this region. Therefore, mutual agreement analysis can be used to estimate the upper bound on the number of misclassifications, at least in the absence of strong evasion attacks.

#### Improving Classifier Performance

Ensemble classifier mutual agreement analysis is not only useful for identifying when the classifier is performing poorly but also for identifying specific examples that will provide the most needed support to improve the classifier. To demonstrate this, we replicated improvements to the classification scores that would occur in the operational evaluation data set as additional samples are added to the classifier training set. We started with the Contagio classifier and added samples from the operational training set.

Using the original Contagio training data set, we determined the rating of all the observations in the operational training set. In an operational setting, all observations above the uncertain threshold (scores greater than 25) would typically require additional investigation, whether the outcome is uncertain or malicious. There were 200 documents in the operational training set matching this criteria. Of these 200 samples, 43 would be false positives and 14 would be false negatives using a traditional threshold. We added these 200 observations to the Contagio training set with the correct ground truth and created another classifier.

For comparison, we also created additional classifiers with varying-sized randomly selected subsets of the operational training set to simulate randomly selected additions to the Contagio classifier. The performance of these classifiers applied to the operational evaluation set is shown in Tables V and VI.

These results indicate that local tuning of the classifier has a significant effect on improving its accuracy. Note that shifting a few samples across the score midpoint in the wrong direction, as occurs with the malicious observations, is not considered harmful since these samples are already deep in the uncertain range (very close to the 50% vote mark) as shown in Figure 3. The ratio of observations in the benign region (certain true negatives) rises from 98.3% to 99.8% for either of the top two re-training strategies, even surpassing the accuracy of the generally superior University classifier (99.5%). The corresponding drop in false positives is important because it coincides with a drop in uncertain observations. In this case, if an operator responds to all uncertain or malicious observations, the majority of alerts will be true positives.

The random subset training additions have the expected outcome. As the number of random samples added from the training set increases, the classification results on the partitioned evaluation data improve. Adding the samples above the uncertain threshold from the training partition results in a classifier that is very close in accuracy to that constructed with the complete training partition. This suggests that mutual agreement analysis is effective at identifying the observations on which the classifier performs poorly. It also indicates that adding these samples to the training set improves the classifier by providing support in the region near these samples. On the other hand, adding observations with high mutual agreement improves the classifier very little. The effort required to obtain ground truth and add 10,000 versus 200 observations to the training set is significantly different.

#### Mimicus Framework

To demonstrate the utility of mutual agreement analysis in identifying observations that evade detection, we reproduced the work of Šrndić and Laskov [43] and applied mutual agreement analysis to these evasion attempts. We used the Mimicus framework to generate PDF documents that implement various evasion attack scenarios. We used the same data sets as the Šrndić and Laskov publication and submitted the resulting documents to pdfrate.com to obtain scores. Because we used the same attack data, our results are limited to 100 samples per attack type. Our results closely mirrored those documented in the Mimicus study.

Table VII presents the results of classification using mutual agreement from the various attack scenarios. Note that since all of these documents are malicious, the correct classification is malicious. A rating of benign indicates successful evasion.

Figures 4 through 7 demonstrate the distribution of PDFrate voting scores for the documents in each non-GD-KDE scenario. The GD-KDE attacks will be addressed separately.