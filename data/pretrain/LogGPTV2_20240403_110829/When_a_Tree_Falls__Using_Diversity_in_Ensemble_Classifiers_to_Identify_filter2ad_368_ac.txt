To apply mutual agreement analysis generally, we propose
a new outcome, uncertain, in addition to the predictions of
benign and malicious. For example, instead of splitting the vote
region in half, we split it into 4 quadrants. In the 0% to 25%
region, the majority of the votes agree that the result is negative
(benign). Similarly, in the 75% to 100% region, the majority
of the votes agree that
is positive (malicious).
However, if the score is between 25% and 75%, the individual
classiﬁers disagree and the outcome is uncertain. To support
comparison with simple ensemble voting predictions, this area
can be split into the other two quadrants: uncertain (benign)
from 25% - 50% and uncertain (malicious) from 50% - 75%.
These classiﬁcation outcomes are demonstrated in Table II.
The uncertain rate (UR) is the portion of observations that fall
within the uncertain range.
the result
To be more precise about this concept, we introduce a
metric to quantify the agreement between individual votes in
an ensemble classiﬁer:
A = |v − 0.5| ∗ 2
Where A is the ensemble classiﬁer mutual agreement rate
and v is the portion of votes for either of the classes. This
function is demonstrated in Figure 1, which also shows the
classiﬁer outcomes resulting from a 50% mutual agreement
threshold. The end and middle points drive the general shape
of this function. If the classiﬁer vote ratio is either 0 or 1,
then the classiﬁer has full agreement on the result and the
mutual agreement should be 1 (or 100%). If the classiﬁer is
split with 0.5 of the votes for each class, then the mutual
agreement should be at the minimum of 0 (or 0%). As long
as a single threshold is used, it is not important what shape
is used for the lines between these end and middle points–
any continuous curve would allow the selection of a given
Fig. 1. Mutual Agreement Based on Ensemble Vote Result
threshold on the classiﬁer vote scores. The function need not
follow the distribution of scores, for example. We choose a
linear function because it is straightforward.
The threshold for mutual agreement is the boundary above
which the classiﬁer is said to be in a state of ensemble agree-
ment, and the resulting classiﬁcation should be considered
valid. Below this mutual agreement rating, the classiﬁcation is
specious. We use the boundary of 50% throughout most of this
paper. However, this value should be adjusted by the operator.
Decreasing this threshold decreases the number of observations
in the disagreement or uncertain classiﬁcation zone. Tuning of
this threshold is discussed in detail in Section VI.
Mutual agreement analysis is effective at identifying the
speciﬁc samples on which the classiﬁer performs poorly. In the
context of evasion attacks, ensemble mutual agreement serves
as criteria for separating novel attacks and weak mimicry
attacks from effective mimicry attacks. For novel attacks, it
is common for the voting result
to be distributed around
50%, indicating that the observations under consideration map
consistently close to neither the benign or malicious samples
in the training set. Since these attacks fall in the relatively rare
uncertain range, they are easily discerned and are considered
weak evasions. Strong mimicry attacks are those where the
distribution of the attack votes is close to that of the benign
observations. Hence, typical novel attacks are identiﬁed by
mutual agreement analysis, but strong mimicry attacks cannot
be. Since uncertain observations are supported poorly by the
training set, these observations are the most effective to add
to the training set in order to improve classiﬁer accuracy.
5
25 50 75 100 Ensemble Classifier Votes (%) Ensemble Mutual Agreement (%) 25 50 75 100 Agree Disagree Malicious  (Malicious)  (Benign) Benign Uncertain TABLE III.
PDFRATE OUTCOMES FOR BENIGN DOCUMENTS FROM
OPERATIONAL EVALUATION SET
Classiﬁer
Contagio
University
Benign
Malicious
Uncertain
98076
99217
1408
360
203
95
40
55
TABLE IV.
PDFRATE OUTCOMES FOR MALICIOUS DOCUMENTS
FROM OPERATIONAL EVALUATION SET
Classiﬁer
Contagio
University
Uncertain
19
0
0
0
0
0
Benign
Malicious
254
273
In operation, mutual agreement analysis is employed to
prevent evasion of an intrusion detection system. The mutual
agreement rate is trivially derived from the result provided
by an ensemble classiﬁer at the time that detection occurs.
Ensemble classiﬁer agreement can be used in many ways by
the operator, including adjusting the vote threshold to prevent
false positives or false negatives, ﬁltering observations for
quarantine or more expensive analysis, and prioritizing alerts.
The strength of mutual agreement analysis is that it can be used
to identify probable intrusion detection evasion at the time of
evasion attempts.
V. EVALUATION
To evaluate our approach, we apply mutual agreement
analysis to PDFrate using an operational data set taken from
a real world sensor and mimicry attack data taken from the
Mimicus and Reverse Mimicry attacks. We study the degree
to which mutual agreement analysis separates observations on
which the classiﬁer is reliable from classiﬁer evasions. We also
evaluate the utility of mutual agreement analysis in detecting
novel malware families using the Drebin Android malware
detector.
A. PDFrate Operational Data Set
We applied mutual agreement analysis to PDFrate scores
for documents taken from a network monitor processing ﬁles
transfered through web and email. This data set
includes
110,000 PDF documents, which we randomly partitioned into
two data sets. The operational evaluation set contains 100,000
documents and operational training set contains 10,000 doc-
uments. Ground truth for the documents was determined by
scanning with many antivirus engines months after collection.
These data sets included 273 and 24 malicious documents
respectively. Table III and Table IV show the scores for
the operational evaluation data set using both the Contagio
and the University classiﬁers of PDFrate. The distribution of
the PDFrate Contagio classiﬁer scores for the benign and
malicious samples of this operational evaluation data set are
shown in Figure 2 and Figure 3.
It is important to note that the scores for the benign and
malicious examples are weighted heavily to the far end of
their respective score range, with the distribution falling off
quickly. In a typical system deployment, the number of obser-
vations in the uncertain range is very small and the majority
of misclassiﬁcations fall within the uncertain region. Hence,
mutual agreement analysis can be used to make an estimate of
Fig. 2. Scores for Benign Documents From Operational Evaluation Set
Fig. 3. Scores for Malicious Documents From Operational Evaluation Set
the upper bound on the number of misclassiﬁcations, at least
in the absence of strong evasion attacks.
Not only is ensemble classiﬁer mutual agreement analysis
useful for identifying when the classiﬁer is performing poorly,
it is also effective for identifying speciﬁc examples which will
provide the most needed support to improve the classiﬁer.
To demonstrate this, we sought to replicate improvements to
the classiﬁcation scores that would occur in the operational
evaluation data set as additional samples are added to the
classiﬁer training set. We started with the Contagio classiﬁer
and added samples from the operational training set.
Using the original Contagio training data set, we deter-
mined the rating of all the observations in the operational
training set. In an operational setting, all observations above
the uncertain threshold (scores greater than 25) would typically
require additional investigation, whether the outcome is uncer-
tain or malicious. There were 200 documents in the operational
training set matching this criteria. Of these 200 samples, 43
would be false positives and 14 would be false negatives using
a traditional threshold. We added these 200 observations to the
Contagio training set with the correct ground truth and created
another classiﬁer.
For comparison, we also created additional classiﬁers with
varying sized randomly selected subsets of the operational
training set to simulate randomly selected additions to the Con-
6
02000040000600000255075100PDFrate Contagio Classifier ScoreDocument Count0501001502000255075100PDFrate Contagio Classifier ScoreDocument CountTABLE V.
SCORES OF BENIGN DOCUMENTS FROM OPERATIONAL
EVALUATION SET USING CONTAGIO CLASSIFIER SUPPLEMENTED WITH
OPERATIONAL TRAINING DATA
TABLE VII.
PDFRATE CONTAGIO CLASSIFIER OUTCOMES FOR
MIMICUS EVASION ATTACKS
Additional Training Data
None (original Contagio)
Random subset 2500
Random subset 5000
Random subset 7500
Uncertain and Malicious
Full training partition
Training Set Size
10000
12500
15000
17500
10200
20000
Benign
Malicious
Uncertain
98076
99332
99444
99502
99506
99540
1408
265
200
169
183
134
203
98
71
49
26
48
40
32
12
7
12
5
TABLE VI.
SCORES OF MALICIOUS DOCUMENTS FROM
OPERATIONAL EVALUATION SET USING CONTAGIO CLASSIFIER
SUPPLEMENTED WITH OPERATIONAL TRAINING DATA
Scenario
Baseline Attack
F mimicry
FC mimicry
FT mimicry
FTC mimicry
F gdkde
FT gdkde
Benign
Malicious
Additional Training Data
None (original Contagio)
Random subset 2500
Random subset 5000
Random subset 7500
Uncertain and Malicious
Full training partition
Training Set Size
10000
12500
15000
17500
10200
20000
0
0
0
0
0
0
Uncertain
19
0
4
14
14
4
4
14
7
14
14
4
254
255
255
255
252
255
Benign
Malicious
Uncertain
0
0
26
70
15
78
26
64
62
5
1
92
95
0
0
2
7
10
33
7
4
100
2
0
0
0
0
1
tagio classiﬁer. The performance of these classiﬁers applied to
the operational evaluation set is demonstrated in Table V and
Table VI.
These results indicate that local tuning of the classiﬁer has
a great effect on improving the accuracy of the classiﬁer. Note
that shifting a few samples across the score midpoint in the
wrong direction, as occurs with the malicious observations, is
not considered harmful as these samples are already deep in the
uncertain range (very close to the 50% vote mark) as shown
in Figure 3. The ratio of observations in the benign region
(certain true negatives) rises from 98.3% to 99.8% for either of
the top two re-training strategies, even surpassing the accuracy
of the generally superior University classiﬁer (99.5%). The
corresponding drop in false positives is important because it
coincides with a drop in uncertain observations. In this case, if
an operator responds to all uncertain or malicious observations,
the majority of alerts will be true positives.
The random subset training additions have the outcome
anticipated by intuition. As the number of random samples
added from the training set increases, the classiﬁcation results
on the partitioned evaluation data improve. Adding the samples
above the uncertain threshold from the training partition results
in a classiﬁer that is very close in accuracy to that constructed
with the complete training partition. It follows that mutual
agreement analysis is effective at identifying the observations
on which the classiﬁer performs poorly. It also follows that
adding these samples to the training set does indeed improve
the classiﬁer by providing support in the region near these
samples. On the other hand, adding the observations for which
there is high mutual agreement improves the classiﬁer very
little. The result of adding the whole training set and adding
the uncertain samples is similar, but the effort invested is
drastically different. The difference in obtaining ground truth
and adding 10,000 vs. 200 observations to the training set is
monumental.
B. Mimicus
To demonstrate the utility of mutual agreement analysis in
identifying observations that evade detection, we reproduced
Fig. 4. Score Distribution for F Mimicry Attack
the work of ˇSrndi´c and Laskov [43] and applied mutual agree-
ment analysis to these evasion attempts. We used the Mimicus
framework to generate PDF documents that implement various
evasion attack scenarios. We used the same data sets as the
ˇSrndi´c and Laskov publication and submitted the resulting
documents to pdfrate.com to obtain scores. Because we used
the same attack data, our results are limited to 100 samples
per attack type. We were able to achieve results that closely
mirrored those documented in the Mimicus study.
We present the results of classiﬁcation using mutual agree-
ment from the various attack scenarios in Table VII. Note that
since all of these documents are malicious, the correct classi-
ﬁcation is malicious. A rating of benign indicates successful
evasion.
The distribution of PDFrate voting scores for the doc-
uments in each non-GD-KDE scenario is demonstrated in
Figures 4 through 7. The GD-KDE attacks will be addressed