be unique, allowing a strong correlation of different devices
using the same hardware. For example, we found that two
Wi-Fi modules with a known security weakness of credential
leakage are potentially being used by 166 devices from 35
different device vendors. The total downloads of these apps
together are over 278,000 times.
Similar Protocol and Backend Service. A speciﬁc protocol
often has its own request and response format. Similarly, a spe-
ciﬁc backend service often exposes standard APIs. Cross-App
Analysis Engine can detect similarities in network interfaces
and thus correlate devices that speak the same protocol or
speak with the same backend service, even if such protocols
or backend services are not documented. For example, we
found that 39 different devices from 11 vendors are very likely
to speak the SSDP protocol, which was known to be vulnera-
ble as a reﬂector for DDoS attacks. As another example, we
found that 32 devices from 10 vendors relied on the same
cloud service to manage their devices, and the cloud service
has a reported security weakness that allows attackers to take
full control of the IoT devices by device ID and password
enumeration.
Future Work. There are additional dimensions that secu-
rity evaluation of an IoT device can beneﬁt from similarity
analysis. For example, previous works [16, 28] have shown
that same developers or sub-contractor may follow a similar
way of coding thus having the same set of bad security prac-
tices or vulnerabilities built into their devices. Similarly, the
same development toolchain (e.g., compiler) may transform
code in a similar way that leads to the same set of security
issues [7, 52, 54]. As a future work, we plan to extend our
analysis to cover more dimensions of similarities in order
to obtain a more accurate and complete evaluation of smart
home IoT devices.
2.5 Device Firmware Collector
Our platform features an additional component called De-
vice Firmware Collector which enriches the Device Firmware
Database through downloading ﬁrmware images of devices
corresponding to the apps being analyzed. The purpose of the
ﬁrmware images is to help us conﬁrm the ﬁndings from the
cross-app analysis phase. In our current platform, we collect
device ﬁrmware in two ways. First, we utilize the ﬁrmware
downloading links that are embedded in the mobile com-
panion apps. As IoT devices are usually headless (i.e., no
keyboard or screen for user interaction), they often deploy
ﬁrmware updates via the companion apps. As a result, links
are sometimes built into the app by the vendor. Such links are
often special URLs that can be extracted through imprint anal-
ysis. Second, we follow the app pages on Google Play, which
often direct to device vendors, to crawl potential ﬁrmware
ﬁles. Speciﬁcally, we used Google Custom Search API to pro-
grammatically search through vendor websites for ﬁrmware
image ﬁles.
For the ﬁles collected, we ﬁlter out non-ﬁrmware ﬁles by
checking their format using Binwalk [2]. Binwalk is a well-
known ﬁrmware unpacking tool which extracts various data
from a binary blob through pattern matching. Once a ﬁle is
decided to be a ﬁrmware, a special effort is made to correlate
ﬁrmware version with app version. As we will discuss in Sec-
tion 3.3, this helps us to decide at which version a particular
vulnerability is ﬁxed and whether or not that ﬁx has an impact
on the app.
Note that not all device ﬁrmware could be downloaded.
Even for the ones that we collected, there is still a considerable
amount of ﬁrmware encrypted or obfuscated that renders the
analysis difﬁcult. This is a limitation yet to overcome in
vulnerability conﬁrmation, as discussed in Section 4.2.
3 Dataset and Results
3.1 Dataset and Platform Statistics
Dataset. In total, our dataset comprises of 2,081 apps col-
lected through the method described in Section 2.2. The aver-
age size of the apps is 13MB (Min. 23KB and Max. 142MB).
These apps spread globally (271 languages) and have a total
download exceeding 1.2 billion. The apps cover 1,345 differ-
ent device vendors and, by our estimate, about 4,720 different
device models. We note that this dataset is still incomplete:
by comparing certain types of devices in our dataset (e.g., IP
camera) against online lists of devices [13,21–23] of the same
type, we estimate the dataset to cover ∼5-20% of the total
IoT companion apps.
Testbed and App Processing. Our app analysis platform ran
on a 4 Core, 3.33GHz Ubuntu 16.04 server with 16 GB RAM
and 1TB hard drive. The Android emulator is compiled from
Android Open Source Project, AOSP 4.4.4.
In total, our platform needed ∼68.3 hours to process the
2,081 apps, with an average processing time of 118.2 seconds
per app. In our experiment, we set the maximum processing
time to 10 minutes and the majority of the apps are processed
successfully within this time frame. The platform was not
able to fully analyze 73 (3.5%) apps within the timeout win-
USENIX Association
28th USENIX Security Symposium    1157
dow and therefore only partial analysis results are available
for them. In addition, 43 (2.1%) apps were not analyzed by
the platform because the tool we used (i.e., Soot) to build
CDG and DDG failed to handle the app bytecode during in-
terpretation. Overall, about 98% of the apps were either fully
or partially analyzed.
One practical concern is the obfuscation of the app and
its impact on the analysis. As reported in previous study, the
majority (85.8%) of the device companion apps are produced
by the standard tool in Android SDK (i.e., Proguard) [53],
which mangles the apps by renaming classes, methods and
ﬁelds. While Proguard introduces skews to the fuzzy hash
analysis, it does not affect our main analysis method (i.e., net-
work interface analysis) since it does not obfuscate network
APIs, data-ﬂow and control-ﬂow. Another concern is the pack-
ing of the app—some developers use packers to encrypt their
code, which would also have an impact on the network inter-
face analysis. However, consistent with observations made by
prior research [53], packers are often seen in malware, and
less adopted by benign apps. In our dataset, only a handful of
apps used commercial packers. Currently, we did not apply
any special processing to these apps. There is an orthogonal
line of research on developing better unpacking tools(e.g.,
DexHunter [59] and PackerGrind [57]) and our platform can
be supplemented by these tools.
Table 2: IoT device families
Type
Software
Rebranding
Hardware
Protocol
Backend
Number
of Families
19
28
14
40
48
Covered
Apps
139
156
61
271
460
Covered
Vendors
122
104
51
210
422
Device Family. Table 2 shows all the device families de-
tected via our cross-app analysis. For example, we were able
to identify 19 distinct device families covering 122 different
vendors and 139 apps that were using similar software within
the family. As another example, we were able to detect 14
distinct device families covering 51 different vendors that
were using similar hardware components within the family.
Note that these families are not mutually exclusive; a device
might share software components with one device and hard-
ware components with another. The largest device family we
identiﬁed includes 31 device vendors and the smallest device
family includes only 2 device vendors. Figure 5 shows a more
intuitive illustration of the device family map.
3.2 Results Validation
Our platform is solely based on code analysis of mobile com-
panion apps, without requiring the physical devices or their
ﬁrmware images. This is the key to a large-scale security
analysis of smart home IoT devices. However, the drawback
Figure 5: Device family map. Circle size indicates the number
of device vendors in the family (the largest circle covers 31
vendors, while the smallest covers two).
of such approach is the accuracy of the result: the output from
this analysis (e.g., a family of devices impacted by a particular
vulnerability) is a conjecture that points to potential security
issues that need to be validated with real devices.
In this paper, we validate and report some of the results we
obtained from our analysis to demonstrate the value of the
approach. We took a hybrid validation route, taking into con-
sideration practical limitations such as the budget. First, we
try to acquire the real device and test it in a local environment
(Figure 6 shows the devices we purchased for validation).
Second, if we do not have the device, we try to simulate, or in
some cases statically analyze, device ﬁrmware stored in the
Device Firmware Database (built from the method discussed
in Section 2.5). Third, if neither of the ﬁrst two methods
applies, we search through online reports including vendor
manuals and websites, bug reporting forums, IoT hacking
communities and so on. Fourth, we work with the vendor
and request their help in validating the results. We primarily
used the second and third method, as the ﬁrst method is very
expensive and the fourth method is often a black hole (i.e., no
responses from vendor). Upon conﬁrming our ﬁndings, we
also try to estimate the impact of the ﬁnding by searching the
online presence of the device on Shodan [4].
Ethics. Testing vulnerabilities and scanning real-world de-
vices often bring up serious ethical concerns. In our study, we
pay special attention to not cross legal and ethical boundaries.
For both real and simulated devices, we evaluate the device in
a local network that only allows outbound connections. The
device is brought ofﬂine immediately after the experiment
to avoid being exploited and used as a bot. To evaluate the
impact of a particular security issue, we collect data from
existing results of Shodan, instead of scanning vulnerable
1158    28th USENIX Security Symposium
USENIX Association
indeed vulnerable. Since these results are already publicly
disclosed, we included them in Table 3. Additionally, we
conﬁrmed through manual ﬁrmware analysis that six device
models from three IP camera vendors, Vendor A, Vendor B
and Vendor C, are also impacted by these vulnerabilities. We
have informed those vendors about the vulnerabilities but no
patch is released yet. Furthermore, we conﬁrmed through real
device that one baby monitor device from Vendor D is also
impacted by the vulnerabilities. Vendor D has asked us to
refrain from including their names until further investigation
is done on their side. In total, we conﬁrmed the existence of
the vulnerabilities on 52 device models from eight different
device vendors, with seven device models from four vendors
newly discovered.
While validating the results, we also encountered one case
where our platform mistakenly ﬂagged a device as vulner-
able. The analysis results output by the platform show that
three device models produced by KGUARD have very sim-
ilar interfaces with other vulnerable devices. However, our
manual validation on the real device as well as emulated
ﬁrmware shows that the KGUARD devices are not vulnera-
ble. We inspected the ﬁrmware, and found that the software
conﬁguration of KGUARD is indeed very similar to the vul-
nerable devices. Speciﬁcally, we found that 26 out of 31 CGI
programs and web pages are in common. But the vulnerable
code was removed. Our hypothesis is that since these devices
are relatively new on market (i.e., after the vulnerability was
reported), KGUARD may have customized the software con-
ﬁguration and patched the vulnerability before releasing the
product to the market.
We also want to highlight two observations we made during
the process of results validation. First, although the vulner-
abilities are old, we are still seeing a large set of devices to
be potentially vulnerable. A Shodan search shows that there
are potentially 58,456 devices running in the wild still being
vulnerable and the total number of app downloads is more
than 282,000 times. This is the result of a dataset with merely
∼2K apps. With a large-scale analysis covering more device
models and vendors, the problem can be even worse. This
demonstrates a common issue in the smart home IoT market:
the market is highly fragmented and many smaller vendors
never bother taking care of the device after selling the device.
Another observation to highlight is the difﬁculty to validate
the results, or rather the general challenge of evaluating the
security of an IoT device. We were not able to validate ﬁnd-
ings on 17 device models from seven out of the 16 vendors,
despite that the cross-app analysis tells us they might also be
vulnerable. The validation was not successful for a number
of reasons: some devices are not targeting U.S. market there-
fore we could not easily acquire, some devices do not provide
ﬁrmware download therefore we could not evaluate, for de-
vices that we could download ﬁrmware the encryption and
packing render analysis difﬁcult. In addition, the collabora-
tion with device vendors have been very difﬁcult. Many times,
Figure 6: Smart home IoT devices for vulnerability validation
devices directly. In this way, we don’t introduce extra net-
work scans. Most importantly, we release our ﬁndings to all
affected vendors, and refrain from including the real name of
any device that is still un-patched or under investigation.
3.3 Results Overview
We present our ﬁndings from the perspective of threats, by
showing how many smart home IoT devices are potentially
impacted by a given vulnerability or security weakness. How-
ever, it is also possible to look at the ﬁndings from a device’s
perspective, i.e., for a speciﬁc device, what kind of vulnera-
bility or security weakness it may suffer from. The results
are encouraging: we identiﬁed 324 device models from 73
vendors that are potentially vulnerable to a number of security
issues. For the devices that we can conﬁrm or disapprove,
about 91% are conﬁrmed to be vulnerable. The total number
of users of these devices is estimated to be over 11.1 million.
3.3.1 Vulnerable Software
To demonstrate how software vulnerabilities propagate across
devices, we applied our analysis to ﬁve high proﬁle vulner-
abilities (shown in Table 3) that were reported in GoAhead
web server which many smart home IoT devices utilize to
provide a web-based interface. These vulnerabilities range
from authentication bypass to backdoor account to remote
code execution. We started from mobile companion app ob-
ject.liouzx.client of NEO Coolcam IP Camera, which was
known to be vulnerable to these vulnerabilities, and utilized
the cross-app analysis to identify devices that might be similar
in their software. Since these are relatively old vulnerabili-
ties (reported in 2017), we expected fewer results. However,
in total we still identiﬁed 72 device models belonging to 16
distinct vendors that share similar software as the vulnerable
device. To validate the results, we utilized the methodology
discussed in Section 3.2. We were able to conﬁrm through
online reports that 45 device models from four vendors are
USENIX Association
28th USENIX Security Symposium    1159
Table 3: IoT devices impacted by vulnerable software and device rebanding
CVE
Impacted Vendor
Device
Models