monitors using streams; the specifications are used to delineate the dependen-
ciesbetweenstreamsofobservationsofthetargetsystemsandtheoutputofthe
monitoring process.
In [36], Zhou et al. propose a runtime verification based trace-oriented mon-
itoring framework for cloud computing systems. The requirements of the mon-
itoring can be specified by formal specification language, i.e. LTL, Finite State
Machine (FSM). The tracing adopted in this approach is fine-grained, in which
tracesareacollectionofeventsandrelationships:everyeventrecordsthedetails
of one execution step in handling the user request (function name, duration),
everyrelationshiprecordsthecausalrelationbetweentwoevents.Usingboththe
Towards Runtime Verification via Event Stream Processing 165
events and the relationships, it is possible to represent a trace into a so-called
trace tree. In a trace tree, a node represents an event and an edge represents a
relationshipbetweenevents.Thisapproachisgeneralizableatthecostofaccess-
ing the target source code to get the knowledge needed for instrumenting the
code and gaining information about events relationships. However, this is not
always the case, leading this approach difficult to exploit in practice. In [29],
Power and Kotonya propose Complex Patterns of Failure (CPoF), an approach
that provides reactive and proactive Fault-Tolerance (FT) via Complex Event
ProcessingandMachineLearningforIoT(InternetofThings).Reactive-FTsup-
portisusedtotrainMachineLearningmodelsthatproactivelyhandleimminent
future occurrences of known errors. Even if CPoF is intended for IoT systems,
it inspired us in the use of Complex Event Processing to build the monitor.
Theproposedapproachpresentsseveralpointsofnoveltycomparedtostate-
of-the-art studies and tools in runtime verification literature. In particular, the
proposed methodology relies on black-box tracing, instead of regular tracing,
avoidingknowingaboutsysteminternalsandthecollectionofinformationabout
therelationshipsbetweenevents(i.e.,uncorrelatedevents).Further,weprovidea
newsetofmonitoringrulesthatwellfitdistributedsystemsandcloudcomputing
infrastructure requirements, in which we need to face peculiar challenges like
multi-tenancy, complex communication between subsystems, lack of knowledge
of system internals. Based on the analysis of the events collected during system
operation, we can specify the normal behavior of the target system and perform
online anomaly detection.
Fig.1. Overview of the proposed approach.
3 Proposed Approach
Figure1 shows an overview of the proposed approach. Firstly, we instrument
the system under test to collect the events exchanged in the system during the
experiments (step 1 ). Our instrumentation is a form of black-box tracing since
we consider the distributed system as a set of black-box components interacting
via public service interfaces. To instrument the system, we do not require any
166 D. Cotroneo et al.
knowledge about the internals of the system under test, but only basic infor-
mation about the communication APIs being used. This approach is especially
suitablewhentestersmaynothaveafullanddetailedunderstandingoftheentire
cloud platform. Differently from traditional distributed system tracing [25], this
lightweight form of tracing does not leverage any propagation of the event IDs
to discriminate the events generated by different users or sessions.
In the step 2 , we collect the correct executions of the system. To define its
normal(i.e.,correct)behavior,weexercisethesystemin“fault-free”conditions,
i.e., without injecting any faults. Moreover, to take into account the variability
of the system, we repeat several times the execution of the system, collecting
different“fault-free traces”,onepereachexecution.Weconsidereveryfault-free
trace a past correct execution of the system.
Step 3 analyzesthecollectedfault-freetracestodefineasetoffailure mon-
itoring rules. These rules encode the expected, correct behavior of the system,
and detect a failure if a violation occurs. This step consists of two main oper-
ations. Firstly, the approach extracts only the attributes useful for expressing
the monitoring rules (e.g., the name of the method, the name of the target sys-
tem, the timestamp of the event, etc.). Then, we define the failure monitoring
rules by extracting “patterns” in the event traces. We define a “pattern” as
a recurring sequence of (not necessarily consecutive) events, repeated in every
fault-free trace, and associated with an operation triggered by a workload. In
this work, we identify patterns by manually inspecting the collected traces. In
future work, we aim to develop algorithms to identify patterns using statistical
analysis techniques, such as invariant analysis [17,20,34].
In general, we can express a monitoring rule by observing the events in the
traces. For example, suppose there is an event of a specific type, say A, that is
eventuallyfollowedbyaneventofadifferenttype,sayB,inthesameusersession
(i.e., same ID). The term event type refers to all the events related to a specific
API call. This rule can be translated into the following pseudo-formalism.
a→b and id(a)=id(b), with a∈A, b∈B (1)
The rules can be applied in the multi-user scenario and concurrent systems
as long as the information on the IDs is available. However, introducing an ID
in distributed tracing requires both in-depth knowledge about the internals and
intrusive instrumentation of the system. Therefore, to make our runtime verifi-
cation approach easier to apply, we propose a set of coarse-grained monitoring
rules(also known aslightweight monitoring rules)that donotrequiretheuseof
any propagation ID. To apply the rules in a multi-user scenario, we define two
different sets of events, A and B, as in the following.
A={all distinct events of type “A” happened in [t, t+Δ]}
(2)
B ={all distinct events of type “B” happened in [t, t+Δ]}
with |A| = |B| = n. Our monitoring rule for the multi-user case then asserts
that there should exist a binary relation R over A and B such that:
Towards Runtime Verification via Event Stream Processing 167
R={(a,b)∈A×B | a→b,
(cid:4)∃ a i,a j ∈A, b k ∈B | (a i,b k),(a j,b k), (3)
(cid:4)∃ b i,b j ∈B, a k ∈A | (a k,b i),(a k,b j) }
with i,j,k ∈ [1,n]. That is, every event in A has an event in B that follows
it, and every event a is paired with exactly one event b, and viceversa. These
rules are based on the observation that, if a group of users performs concurrent
operations on shared cloud infrastructure, then a specific number of events of
typeAiseventuallyfollowedbythesamenumberofeventsoftypeB.Theideais
inspired by the concept of flow conservation in network flow problems. Without
usingapropagationID,itisnotpossibletodefinethecoupleofeventsa andb
i i
referred to the same session or the same user i, but it is possible to verify that
the total number of events of type A is equal to the total number of events of
type B in a pre-defined time window. We assume that the format of these rules
can detect many of the failures that appear in cloud computing systems: if at
least one of the rules is violated, then a failure occurred.
Finally, we synthesize a monitor from failure monitoring rules, expressed
accordingtoaspecificationlanguage(step 4 ).Themonitor takes asinputsthe
eventsrelatedtothesystemunderexecution,anditchecks,atruntime,whether
the system’s behavior follows the desired behavior specified in the monitoring
rules (step 5 ). Any (runtime) violation of the defined rules alerts the system
operator of the detection of a failure.
4 Case Study
In this paper, we investigated the feasibility of the proposed approach in the
context of a large-scale, industry-applied case study. In particular, we applied
the approach in the OpenStack project, which is the basis for many commer-
cial cloud management products [26] and is widespread among public cloud
providers and private users [27]. Moreover, OpenStack is a representative real-
worldlargesoftwaresystem,whichincludesseveralsub-systemsandorchestrates
them to deliver rich cloud computing services. The most fundamental services
of OpenStack [15,32] are (i) the Nova subsystem, which provides services for
provisioning instances (VMs) and handling their life cycle; (ii) the Cinder sub-
system, which provides services for managing block storage for instances; and
(iii) the Neutron subsystem, which provides services for provisioning virtual
networks for instances, including resources such as floating IPs, ports and sub-
nets. Each subsystem includes several components (e.g., the Nova sub-system
includes nova-api, nova-compute, etc.), which interact through message queues
internally to OpenStack. The Nova, Cinder, and Neutron sub-systems provide
external REST API interfaces to cloud users. To collect the messages (i.e., the
events) exchanged in the system, we instrumented the OSLO Messaging library,
which uses a message queue library and it is used for communication among
OpenStack subsystems, and the RESTful API libraries of each OpenStack sub-
system, which are used are used for communication between OpenStack and its
168 D. Cotroneo et al.
clients. In total, we instrumented only 5 selected functions of these components
(e.g., the cast method of OSLO to broadcast messages), by adding very simple
annotations only at the beginning of these methods, for a total of 20 lines of
code. We neither added any further instrumentation to the subsystems under
test nor used any knowledge about OpenStack internals.
Wecollectedonehundredcorrectexecutionsbyrunningthesameworkloadin
fault-freeconditions.Thisworkloadconfiguresanewvirtualinfrastructurefrom
scratch, by stimulating all of the target subsystems (i.e., Nova, Neutron, and
Cinder) in a balanced way. The workload creates VM instances, along with key
pairs and a security group; attaches the instances to an existing volume; creates
avirtualnetworkconsistinginasubnetandavirtualrouter;assignsafloatingIP
to connect the instances to the virtual network; reboots the instances, and then
deletes them. We implemented this workload by reusing integration test cases
from the OpenStack Tempest project [23], since these tests are already designed
to trigger several subsystems and components of OpenStack and their virtual
resources.
After the fault-free traces collection, we extract the information associ-
ated with every event within the trace. In particular, we record the time at
which the communication API has been called and its duration, the component
that invoked the API (message sender), and the remote service that has been
requested through the API call (called service). Internally, the approach asso-
ciatesaneventname toeverycollectedeventwithinatrace,sothattwoeventsof
thesametypeareidentifiedbythesamename.Inparticular,weassignaunique
name to every distinct pair  (e.g., ).
4.1 Monitoring Rules
To determine the monitoring rules, we manually identified common patterns,
in terms of events, in the fault-free traces. In particular, we determined a set of
patternsforalltheoperationsrelatedtotheworkloadexecution(e.g.,operations
related to the instances, volumes, and networks). For example, the analysis of
the events related to the attach of a volume to an instance pointed out com-
mon three different patterns in the fault-free traces. We derived failure moni-
toring rules for each pattern. Listing 1.1 shows three different monitoring rules,
expressed in a pseudo-formalism, related to the Volume Attachment operation.
Listing 1.1. Volume Attachment monitoring rules
Rule#1: event(name = "compute_reserve_block_device_name") is eventually
followed by event( name = "compute_attach_volume")
Rule#2: event( name = "compute_attach_volume") is eventually followed by
event( name = "cinder-volume.localhost.(cid:2)
localdomain@lvm_initialize_connection")
Rule#3: Pattern of Rule#2 is eventually followed by event(name="cinder-volume
.localhost.localdomain@lvm_attach_volume")
Towards Runtime Verification via Event Stream Processing 169
We derived the first rule by observing that, during the attachment of a vol-
ume, the  event is always followed by
the  event in every fault-free trace. Indeed, to per-
formsuchanoperation,thereserve block device namemethod,asynchronous
RPC call, is called before the attach volume nova-compute API to get and
reserve the next available device name. Rule#2 follows the same structure of
theRule#1.Rule#3showsthepossibilitytowritemorecomplexrules,involving
more than just two events.
Inthesameway,wederivedrulesforallfurtheroperationsrelatedtothevol-
umesandtheinstances.Instead,theidentificationoftherulesfornetworkoper-
ations is different and more complex. Indeed, network operations are performed
bytheNeutronsub-systeminanasynchronousway,suchasbyexchangingperi-
odic and concurrent status polls among agents deployed in the datacenter and
the Neutron server. This behavior leads to more non-deterministic variations of
the events in the traces. Given the high source of non-determinism affecting the
network operations, it is not possible to create rules based on event ordering.
Therefore, to find these patterns, we observed the repetitions of the Neutron-
associated events both in the fault-free and the faulty traces (i.e., with a fault
injected in the Neutron subsystem). We found that, when the injected fault
experiences a failure in the Neutron component, some network-related events
occurred a much higher number than their occurrences in the fault-free traces.
For example, in several experiments targeting the Neutron component and
thatexperiencedafailureduringthenetworkoperations,wefoundcasesinwhich
the event  occurred more than 500 times.
However, analyzing all the fault-free executions, this specific event occurred at
most 3 times. Indeed, in such faulty experiments, the system repeatedly per-
formed the same operation since it was unable to complete the request. Based
on these observations, we defined the monitoring rules related to the network
operationsbychecking,atruntime,ifaspecificeventtypeoccurredinanumber
higher than a threshold. Thus, for each event type, we defined a threshold as
the higher number of times that the event type occurred during the fault-free
executions.Figure2showsthelogicadoptedfortherulesrelatedtothenetwork
operations.
After identifying the rules, it is necessary to translate the rules in a par-
ticular specification language. We select EPL (Event Processing Language)
as specification language. EPL is a formal language provided by the Esper
software [18], that allows expressing different types of rules, such as tempo-
ral or statistical rules. It is a SQL-standard language with extensions, offer-
ing both typical SQL clauses (e.g., select, from, where) and additional
clauses for event processing (e.g., pattern, output). In EPL, streams are the
source of data and events are the basic unit of data. The typical SQL clause
insert into is used to forward events to other streams for further down-
stream processing. We use the insert into clause for translating network
operation rules using three interconnected statements, as shown in Listing1.2.
170 D. Cotroneo et al.
Fig.2. Example of Neutron SSH failure monitoring rules.
Listing 1.2. EPL rules for the network operations
@name(’S1’) insert into EventNetworkStream select *
from Event where name=’q-plugin_release_dhcp_port’;
@name(’S2’) insert into countInfoStream select count(*) as count1 from
EventNetworkStream;
@name(’NetworkRule#1’) select * from countInfoStream where count1 > maxEvent1
output when OutputTriggerVar1 = true then set OutputTriggerVar1 = false;
The first statement (S1) extracts  events
and forwards them to the stream NetworkEventStream. The second state-
ment (S2) counts the number of events in NetworkEventStream and passes
this information to the stream CountInfoStream. Finally, the third statement
(NetworkRule#1)producesanoutputifthevalueinCountInfoStreamisbigger
than the maximum value. To avoid that the third statement outputs anytime
it receives a new  event after the first output,
we use the OutputTriggerVar boolean variable, initialized to true and set to
false after the first time the rule is verified.
The monitor synthesis is automatically performed once EPL rules are com-
piled. The Esper Runtime acts like a container for EPL statements which con-
tinuously executes the queries (expressed by the statements) against the data
arriving as inputs. For more detailed information on Esper, we refer the reader
to the official documentation [19].
4.2 Multi User Case
We applied the EPL statements, derived from the monitoring rules, also in the
multi-userscenario.SincewedonotcollectaneventID,weuseacounter totake
intoaccountmulti-useroperations.Indeed,weusethecounterasaneventIDto
relatecouplesofevents.Weassociateadifferentcountertoeacheventtype:when
an event of a specific type occurs, we increment its counter. In particular, the
translationofrulesdescribedinListing1.1usestheclauseof pattern,usefulfor
findingtimerelationships betweenevents.Patternexpressionsusuallyconsistof
filterexpressionscombinedwithpatternoperators.Weusethepatternoperators
every, followed-by (→), and timer:interval. The operator every defines
that every time a pattern subexpression connected to this operator turns true,
theEsperRuntimestartsanewactivesubexpression.Withoutthisoperator,the
Towards Runtime Verification via Event Stream Processing 171
subexpressionstopsafterthefirsttimeitbecomestrue.Theoperator→operates