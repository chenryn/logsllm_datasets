tions periodically by sampling from a buer of experience:
{st ,at ,rt ,st +1} . The training process is described in Algorithm
(1).
5.2.2
lRLA. For lRLA, we also use Keras to implement
the PG algorithm with a fully connected NN with 10 hidden
layer of 300 neurons. The RL agent takes a state (136 features
per-server (nl =11, ml =10)) and outputs probabilities for the
actions for all the active ows.
Summary The hyper-parameters (structure, number of
layer, height, and width of DNN) are chosen based on a few
empirical training sessions. Our observation is that more
complicated DNNs with more hidden layers and more pa-
rameters took longer to train and did not perform much
better than the chosen topologies. Overall, we nd that such
RLA congurations leads to good system performance and
is rather reasonable considering the importance of computa-
tion delay, as we reveal next in the evaluations.
6 EVALUATION
In this section, we evaluate the performance of AuTO using
real testbed experiments. We seek to understand: 1) With
stable trac (ow size distribution and trac load are xed),
how does AuTO compare to standard heuristics? 2) For vary-
ing trac characteristics, can AuTO adapt? 3) how fast can
AuTO respond to trac dynamics? 4) what are the perfor-
mance overheads and overall scalability?
Summary of results (grouped by scenarios):
• Homogeneous: For trac with xed ow size distribu-
tion and load, AuTO-generated thresholds converge, and
demonstrate similar or better performance compared to
standard heuristics, with up to 48.14% average FCT reduc-
tion.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 7: Testbed topology.
Figure 8: Trac distributions in evaluation.
• Spatially Heterogeneous: We divide the servers into 4
clusters; each is congured to generate trac with dif-
ferent ow size distribution and load. In these experi-
ments, AuTO-generated thresholds also converge, with
up to 37.20% average FCT reduction.
• Spatially & Temporally Heterogeneous: Building upon
the above scenario, we then change the ow size distri-
bution and load periodically for each cluster. For time-
varying ow size distributions and trac load, AuTO
exhibits learning and adaptation behavior. Compared to
xed heuristics that excel only for certain combinations of
trac settings, AuTO demonstrates steady performance
improvement across all combinations.
• System Overhead: The current AuTO implementation
can respond to state updates within 10ms on average.
AuTO also exhibits minimal end-host overhead in terms
of CPU utilization and throughput degradation.
Setting We deploy AuTO on a small-scale testbed (Figure 7)
that consists of 32 servers. Our switch supports ECN and
strict priority queuing with at most 8 class of service queues7
Each server is a Dell PowerEdge R320 with a 4-core Intel E5-
1410 2.8GHz CPU, 8G memory, and a Broadcom BCM5719
NetXtreme Gigabit Ethernet NIC with 4x1Gbps ports. Each
server runs 64-bit Debian 8.7 (3.16.39-1 Kernel). By default,
advanced NIC ooad mechanisms are enabled to reduce
the CPU overhead. The base round-trip time (RTT) of our
testbed is 100us.
We adopt the trac generator [20] used in prior works [2,
7, 9, 15] that produces network trac ows based on given
ow size distribution and trac load. We use two realis-
tic workloads (Figure 8): web search workload [8] and data
mining workload [10]. 15 servers hosting ow generators
are called application servers, and the remaining one hosts
7As in most production datacenters [8, 10, 14], some queues are reserved for
other services, such as latency-sensitive trac and management trac[15].
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Chen et al.
the CS. Each application server is connected to a data plane
switch using 3 of its ports, as well as to a control plane switch
to communicate with the CS server using the remaining port.
The 3 ports are congured to dierent subnets, forming 3
paths between any pair of application servers. Both switches
are Pronto-3297 48-port Gigabit Ethernet switch. States and
actions are sent on the control plane switch (Figure 7).
Comparison Targets We compare with two popular heuris-
tics in ow scheduling: Shortest-Job-First (SJF), and Least-
Attained-Service-First (LAS). The main dierence between
the two is that SJF schemes [1, 4, 29] require ow size at
the start of a ow, while LAS schemes [8, 14, 43] do not. For
these algorithms to work, suciently enough data should
be collected before calculating their parameters (thresholds).
The shortest period to collect enough ow information to
form an accurate and reliable ow size distribution is an
open research problem [9, 14, 21, 34], and we note that pre-
viously reported distributions are all collected over periods
of at least weeks (Figure 8), which indicates the turn-around
time are also at least weeks for these algorithms.
In the experiments, we mainly compare with quantized
version of SJF and LAS with 4 priority levels. The priority
levels are enforced both in the server using Linux qdisc [23]
and in the data plane switch using strict priority queue-
ing [8]:
• Quantized SJF (QSJF): QSJF has three thresholds: α0,α1,α2.
We can obtain ow size from the ow generator at its
start. For ow size s, if x≤α0, it is given highest priority;
if x∈(α0,α1], it is given the second priority; and so on. In
this way, shorter ows are given higher priority, similar
to SJF.
• Quantized LAS (QLAS): QLAS also has thresholds: β0,β1,β2.
All the ows are given high priority at the start. If a ow
sends more than βi bytes, it is then demoted to the (i+1)-
th priority. In this way, longer ows gradually drop to
lower priorities.
The thresholds for both schemes can be calculated using
methods described in [14] for "type-2/3 ows", and they are
dependent on the ow size distribution and trac load. In
each experiment, unless specied, we use the thresholds
calculated for DCTCP distribution at 80% load (i.e. the total
sending rate is at 80% of the network capacity).
6.1 Experiments
6.1.1 Homogeneous traic. In these scenarios, the ow
size distribution and the load generated from all 32 servers
are xed. We choose Web Search (WS) and Data Mining (DM)
distributions at 80% load. These two distributions represent
dierent group of ows: a mixture of short and long ows
(WS) and a set of short ows (DM). The average and 99th
percentile (p99) FCT are shown in Figure 9. We train AuTO
Figure 9: Homogeneous trac: Average and p99 FCT.
Figure 10: Spatially heterogeneous trac: Average and
p99 FCT.
for 8 hours and use the trained DNNs to schedule ows for
another hour (shown in Figure 9 as AuTO).
We make the following observations:
• For a mixture of short and long ows (WS), AuTO out-
performs the standard heuristics, achieving up to 48.14%
average FCT reduction. This is because it can dynamically
change priority of long ows, avoiding the starvation
problem in the heuristics.
• For distribution with mostly short ows (DM), AuTO per-
forms similar to the heuristics. Since AuTO also gives
any ow highest priority when it starts, AuTO performs
almost the same as QLAS.
• Training the RL network results in average FCT reduction
of 18.31% and 4.12% for WS&DM distribution respectively,
which demonstrates AuTO is capable to learn and adapt
to trac characteristics overtime.
• We further isolate the incast trac [16] from the collected
traces, and we nd that they are almost the same with
both QLAS and QSJF. This is because incast behavior is
best handled by the congestion control and parameter
setting. DCTCP [3], which is the transport we used in
the experiments, already handles incast very well with
appropriate parameter settings [3, 9].
6.1.2
Spatially heterogeneous traic. We proceed to di-
vide the servers into 4 clusters to create spatially hetero-
geneous trac. We congure the ow generators in each
cluster with dierent distribution and load pairs: ,
, , . We use AuTO to con-
trol all 4 clusters, and plot the average and p99 FCTs in
Figure 10. For the heuristics, we compute the thresholds for
AuTO: Scaling Deep Reinforcement Learning
for Datacenter-Scale Automatic Traic Optimization
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
Figure 11: Dynamic scenarios: average FCT.
Figure 12: Dynamic scenarios: p99 FCT.
each cluster individually according to its distribution and
load. We observe similar results compared to the homoge-
neous scenarios. Compared to QLAS(QSJF), AuTO is shown
to reduce the average FCT by 37.20%(27.95%) and p99 FCT
by 19.78%(11.98%). This demonstrates that AuTO can adapt
to spatial trac variations.
6.1.3 Temporally & spatially heterogeneous traic. In these
scenarios, we change the ow size distribution and network
load every hour: The load value is chosen from {60%,70%,80%},
and the distribution is randomly chosen from the ones in
Figure 8. We ensure that the same distribution/load does
not appear in consecutive hours. The experiment runs for 8
hours.
The average and p99 FCTs are plotted against time in
Figure 11&12. We can see:
• For heuristics with xed parameters, when the trac char-
acteristics match the parameter setting, both average and
p99 FCTs outperform the other schemes. But when mis-
match occurs, the FCTs sharply drop. This shows that
heuristics with xed parameter setting cannot adapt to
dynamic trac well. Their parameters are usually chosen
to perform well in the average case, but in practice, the
trac characteristics always change [8].
• AuTO is shown to steadily learn and adapt across time-
varying trac characteristics, in the last hour, AuTO achieves
8.71% (9.18%) reduction in average (p99) FCT compared
to QSJF. This is because that AuTO, using 2 DRL agents,
can dynamically change the priorities of ows in dierent
environments to achieve better performance. Without any
human involvement, this process can be done quickly and
scalably.
Figure 13: MLFQ thresholds from sRLA vs. optimal
thresholds.
Considering AuTO, a constant decline in FCTs indicates
learning behavior, which eventually, as we have discussed
in §4, lead to convergence to a local optimum for the dy-
namic trac generation process. Figure 11&12 conrms our
assumption that datacenter trac scheduling can be con-
verted into a RL problem and DRL techniques (§4) can be
applied to solve it.
6.2 Deep Dive
In the following, we inspect the design components of AuTO.
6.2.1 Optimizing MLFQ thresholds using DRL. We rst ex-
amine the MLFQ thresholds generated by sRLA. In Figure 13,
we compare the MLFQ thresholds generated by sRLA and
those by an optimizer [9, 14]. We obtain a set of 3 thresholds
(for 4 queues) from sRLA in CS after 8 hours of training for
each ow size distribution at 60% load. We observe that both
sets of thresholds are similar in the thresholds of the rst
3 queues, and the main dierence is in the last queue. For
example, the last sRLA threshold (α3) for Web Search distri-
bution is 64 packets, while α3 from optimizer is 87 packets.
The same is true for Data Mining distribution. However, the
discrepancy does not reect in signicant dierence in terms
of performance. We plot the average and p99 FCT results
for both sets of thresholds in Figure 14&15. The results are
grouped by ow size. For sRLA generated thresholds and
optimizer-generated thresholds, we observe that the dier-
ence in FCT is small in all groups of ow sizes. We conclude
that, after 8 hours of training, sRLA generated thresholds are
similar to optimizer-generated ones in terms of performance.
6.2.2 Optimizing Long Flows using DRL. Next we look at
how lRLA optimizes long ows. During the experiments in
§6.1.3, we log the number of long ows on each link for 5 min-
utes in lRLA. Denote L as the set of all links, Nl (t ) as the num-
ber of long ows on link l∈L at time t, and N (t )={Nl (t ),∀l}.
We plot max (N (t ))−min(N (t )),∀t in Figure16, which is the
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
L. Chen et al.
Figure 14: Average FCT using MLFQ thresholds from
sRLA vs. optimal thresholds.
Figure 17: CS response latency: Traces from 4 runs.
Figure 15: p99 FCT using MLFQ Thresholds from sRLA
vs. optimal thresholds.
Figure 16: Load balancing using lRLA (PG algorithm):
dierence in number of long ows on links.
dierence in number of long ows on the link that have the
most long ows and the link that have the least. This metric
is an indicator of load imbalance. We observe that this metric
is less than 10 most of the time. When temporary imbal-
ance occurs, as shown in the magnied portion of Figure 16
(from 24s to 28s), lRLA reacts to the imbalance by routing the
excess ows onto the less congested links. This is because,
as we discussed in §2.2, the reward of the PG algorithm is
directly linked to the throughput: when long ows share a
link, the total throughput is less than when they are using
dierent links. lRLA is rewarded when it places long ows
on dierent links, thus it learns to load balance long ows.
System Overhead. We proceed to investigate the
performance and overheads of AuTO modules. First, we look
6.2.3
Figure 18: CS response latency: Scaling short ows
(ms)
at the response latency of CS, as well as its scalability. Then
we examine the overheads of the end-host modules in PS.
CS Response Latency During experiments, response delay
of the CS server (Figure 17) is measured as follows: tu is the
time instant of CS receiving an update from one server, and
ts is the time instant of CS sending the action to that server,
so the response time is ts−tu. This metric directly shows how
fast can the scheduler adapt to trac dynamics reported by
PS. We observe that CS can respond to an update within
10ms on average for our 32-server testbed. This latency is
mainly due to the computation overhead of DNN, as well as
the queueing delay of servers’ updates at CS. AuTO currently
only uses CPU. To reduce this latency, one promising direc-
tion is CPU-GPU hybrid training and serving [46], where
CPUs handle the interaction with the environment, while
GPUs train the models in the background.
Response latency also increases with computation com-
plexity of DNN. In AuTO, the network size is dened by
{nl ,ml ,ms}. Since long ows are few, the increment of nl ,ml
are expected to be moderate even for datacenters with high
load. We increase {nl ,ml} from {11,10} to {1000,1000}, and
nd the average response time for lRLA becomes 81.82ms