### 5.2.2 Reinforcement Learning Agent (RLA) for Flow Scheduling

For the reinforcement learning agent (RLA), we use Keras to implement the policy gradient (PG) algorithm with a fully connected neural network (NN) consisting of 10 hidden layers, each with 300 neurons. The RL agent takes as input a state vector representing 136 features per server (with \( n_l = 11 \) and \( m_l = 10 \)) and outputs action probabilities for all active flows.

### Summary of Hyper-parameters

The hyper-parameters, including the structure, number of layers, and dimensions of the deep neural network (DNN), are chosen based on empirical training sessions. Our observations indicate that more complex DNNs with additional hidden layers and parameters require longer training times and do not significantly outperform the selected topologies. Overall, the chosen RLA configurations lead to good system performance and are reasonable given the importance of computational delay, as demonstrated in the following evaluations.

### 6. Evaluation

In this section, we evaluate the performance of AuTO using real testbed experiments. We aim to understand:
1. How does AuTO compare to standard heuristics under stable traffic conditions (fixed flow size distribution and traffic load)?
2. Can AuTO adapt to varying traffic characteristics?
3. How quickly can AuTO respond to traffic dynamics?
4. What are the performance overheads and overall scalability?

#### Summary of Results (Grouped by Scenarios)

- **Homogeneous Traffic:**
  - For traffic with fixed flow size distribution and load, AuTO-generated thresholds converge, demonstrating similar or better performance compared to standard heuristics, with up to a 48.14% average FCT reduction.
  
- **Spatially Heterogeneous Traffic:**
  - We divide the servers into 4 clusters, each generating traffic with different flow size distributions and loads. In these experiments, AuTO-generated thresholds also converge, achieving up to a 37.20% average FCT reduction.
  
- **Spatially & Temporally Heterogeneous Traffic:**
  - Building upon the above scenario, we periodically change the flow size distribution and load for each cluster. For time-varying flow size distributions and traffic load, AuTO exhibits learning and adaptation behavior. Compared to fixed heuristics, which excel only for certain combinations of traffic settings, AuTO demonstrates steady performance improvement across all combinations.
  
- **System Overhead:**
  - The current AuTO implementation can respond to state updates within 10ms on average. AuTO also exhibits minimal end-host overhead in terms of CPU utilization and throughput degradation.

### Testbed Setup

We deploy AuTO on a small-scale testbed (Figure 7) consisting of 32 servers. Each server is a Dell PowerEdge R320 with a 4-core Intel E5-1410 2.8GHz CPU, 8GB memory, and a Broadcom BCM5719 NetXtreme Gigabit Ethernet NIC with 4x1Gbps ports. Each server runs 64-bit Debian 8.7 (3.16.39-1 Kernel). Advanced NIC offload mechanisms are enabled by default to reduce CPU overhead. The base round-trip time (RTT) of our testbed is 100us.

We use a traffic generator [20] from prior works [2, 7, 9, 15] to produce network traffic flows based on given flow size distributions and traffic loads. Two realistic workloads (Figure 8) are used: web search workload [8] and data mining workload [10]. 15 servers host the flow generators (application servers), and the remaining one hosts the control server (CS). Each application server is connected to a data plane switch using 3 of its ports and to a control plane switch using the remaining port. The 3 ports are configured to different subnets, forming 3 paths between any pair of application servers. Both switches are Pronto-3297 48-port Gigabit Ethernet switches. States and actions are sent over the control plane switch (Figure 7).

### Comparison Targets

We compare AuTO with two popular heuristics in flow scheduling: Shortest-Job-First (SJF) and Least-Attained-Service-First (LAS). The main difference is that SJF schemes [1, 4, 29] require flow size at the start of a flow, while LAS schemes [8, 14, 43] do not. For these algorithms to work, sufficient data must be collected before calculating their parameters (thresholds). The shortest period to collect enough flow information to form an accurate and reliable flow size distribution is an open research problem [9, 14, 21, 34], and previously reported distributions are collected over periods of at least weeks (Figure 8), indicating a turnaround time of at least weeks for these algorithms.

In the experiments, we mainly compare with quantized versions of SJF and LAS with 4 priority levels. The priority levels are enforced both in the server using Linux qdisc [23] and in the data plane switch using strict priority queuing [8]:

- **Quantized SJF (QSJF):** QSJF has three thresholds: \( \alpha_0, \alpha_1, \alpha_2 \). For flow size \( s \), if \( s \leq \alpha_0 \), it is given the highest priority; if \( s \in (\alpha_0, \alpha_1] \), it is given the second priority; and so on. This prioritizes shorter flows, similar to SJF.
  
- **Quantized LAS (QLAS):** QLAS also has thresholds: \( \beta_0, \beta_1, \beta_2 \). All flows are given high priority at the start. If a flow sends more than \( \beta_i \) bytes, it is demoted to the (i+1)-th priority. This gradually demotes longer flows to lower priorities.

The thresholds for both schemes are calculated using methods described in [14] for "type-2/3 flows" and are dependent on the flow size distribution and traffic load. In each experiment, unless specified, we use the thresholds calculated for DCTCP distribution at 80% load (i.e., the total sending rate is at 80% of the network capacity).

### 6.1 Experiments

#### 6.1.1 Homogeneous Traffic

In these scenarios, the flow size distribution and the load generated from all 32 servers are fixed. We choose Web Search (WS) and Data Mining (DM) distributions at 80% load. These two distributions represent different groups of flows: a mixture of short and long flows (WS) and a set of short flows (DM). The average and 99th percentile (p99) FCT are shown in Figure 9. We train AuTO for 8 hours and use the trained DNNs to schedule flows for another hour (shown in Figure 9 as AuTO).

Key Observations:
- For a mixture of short and long flows (WS), AuTO outperforms the standard heuristics, achieving up to a 48.14% average FCT reduction. This is because it can dynamically change the priority of long flows, avoiding the starvation problem in the heuristics.
- For distributions with mostly short flows (DM), AuTO performs similarly to the heuristics. Since AuTO also gives any flow the highest priority when it starts, AuTO performs almost the same as QLAS.
- Training the RL network results in an average FCT reduction of 18.31% and 4.12% for WS and DM distributions, respectively, demonstrating AuTO's ability to learn and adapt to traffic characteristics over time.
- We further isolate the incast traffic [16] from the collected traces and find that they are almost the same with both QLAS and QSJF. This is because incast behavior is best handled by congestion control and parameter setting. DCTCP [3], which is the transport protocol used in the experiments, already handles incast very well with appropriate parameter settings [3, 9].

#### 6.1.2 Spatially Heterogeneous Traffic

We divide the servers into 4 clusters to create spatially heterogeneous traffic. We configure the flow generators in each cluster with different distribution and load pairs. We use AuTO to control all 4 clusters and plot the average and p99 FCTs in Figure 10. For the heuristics, we compute the thresholds for each cluster individually according to its distribution and load. We observe similar results compared to the homogeneous scenarios. Compared to QLAS and QSJF, AuTO reduces the average FCT by 37.20% and 27.95%, and the p99 FCT by 19.78% and 11.98%, respectively. This demonstrates that AuTO can adapt to spatial traffic variations.

#### 6.1.3 Temporally & Spatially Heterogeneous Traffic

In these scenarios, we change the flow size distribution and network load every hour. The load value is chosen from {60%, 70%, 80%}, and the distribution is randomly chosen from the ones in Figure 8. We ensure that the same distribution/load does not appear in consecutive hours. The experiment runs for 8 hours.

The average and p99 FCTs are plotted against time in Figures 11 and 12. Key Observations:
- For heuristics with fixed parameters, when the traffic characteristics match the parameter setting, both average and p99 FCTs outperform the other schemes. However, when a mismatch occurs, the FCTs sharply drop. This shows that heuristics with fixed parameter settings cannot adapt to dynamic traffic well. Their parameters are usually chosen to perform well in the average case, but in practice, traffic characteristics always change [8].
- AuTO steadily learns and adapts to time-varying traffic characteristics. In the last hour, AuTO achieves an 8.71% (9.18%) reduction in average (p99) FCT compared to QSJF. This is because AuTO, using 2 DRL agents, can dynamically change the priorities of flows in different environments to achieve better performance. Without any human involvement, this process can be done quickly and scalably.

Figures 11 and 12 confirm our assumption that datacenter traffic scheduling can be converted into a reinforcement learning (RL) problem, and deep reinforcement learning (DRL) techniques (ยง4) can be applied to solve it.

### 6.2 Deep Dive

In the following, we inspect the design components of AuTO.

#### 6.2.1 Optimizing MLFQ Thresholds Using DRL

We first examine the multi-level feedback queue (MLFQ) thresholds generated by the short-flow reinforcement learning agent (sRLA). In Figure 13, we compare the MLFQ thresholds generated by sRLA and those by an optimizer [9, 14]. We obtain a set of 3 thresholds (for 4 queues) from sRLA in the CS after 8 hours of training for each flow size distribution at 60% load. We observe that both sets of thresholds are similar in the first three queues, with the main difference being in the last queue. For example, the last sRLA threshold (\( \alpha_3 \)) for the Web Search distribution is 64 packets, while \( \alpha_3 \) from the optimizer is 87 packets. The same is true for the Data Mining distribution. However, the discrepancy does not result in significant differences in performance. We plot the average and p99 FCT results for both sets of thresholds in Figures 14 and 15. The results are grouped by flow size. For sRLA-generated thresholds and optimizer-generated thresholds, we observe that the difference in FCT is small across all groups of flow sizes. We conclude that, after 8 hours of training, sRLA-generated thresholds are similar to optimizer-generated ones in terms of performance.

#### 6.2.2 Optimizing Long Flows Using DRL

Next, we look at how the long-flow reinforcement learning agent (lRLA) optimizes long flows. During the experiments in ยง6.1.3, we log the number of long flows on each link for 5 minutes in lRLA. Denote \( L \) as the set of all links, \( N_l(t) \) as the number of long flows on link \( l \in L \) at time \( t \), and \( N(t) = \{N_l(t), \forall l\} \). We plot \( \max(N(t)) - \min(N(t)), \forall t \) in Figure 16, which is the difference in the number of long flows on the link with the most long flows and the link with the least. This metric is an indicator of load imbalance. We observe that this metric is less than 10 most of the time. When temporary imbalance occurs, as shown in the magnified portion of Figure 16 (from 24s to 28s), lRLA reacts to the imbalance by routing the excess flows onto the less congested links. This is because, as discussed in ยง2.2, the reward of the PG algorithm is directly linked to throughput: when long flows share a link, the total throughput is less than when they use different links. lRLA is rewarded when it places long flows on different links, thus it learns to load balance long flows.

#### 6.2.3 System Overhead

We investigate the performance and overheads of AuTO modules. First, we look at the response latency of the control server (CS) and its scalability. Then, we examine the overheads of the end-host modules in the packet scheduler (PS).

**CS Response Latency:**
During experiments, the response delay of the CS server (Figure 17) is measured as follows: \( t_u \) is the time instant of CS receiving an update from one server, and \( t_s \) is the time instant of CS sending the action to that server, so the response time is \( t_s - t_u \). This metric directly shows how fast the scheduler can adapt to traffic dynamics reported by PS. We observe that CS can respond to an update within 10ms on average for our 32-server testbed. This latency is mainly due to the computation overhead of the DNN and the queuing delay of servers' updates at CS. AuTO currently only uses CPU. To reduce this latency, one promising direction is CPU-GPU hybrid training and serving [46], where CPUs handle the interaction with the environment, while GPUs train the models in the background.

Response latency also increases with the computational complexity of the DNN. In AuTO, the network size is defined by \( \{n_l, m_l, m_s\} \). Since long flows are few, the increment of \( n_l \) and \( m_l \) is expected to be moderate even for datacenters with high load. We increase \( \{n_l, m_l\} \) from \( \{11, 10\} \) to \( \{1000, 1000\} \) and find that the average response time for lRLA becomes 81.82ms.