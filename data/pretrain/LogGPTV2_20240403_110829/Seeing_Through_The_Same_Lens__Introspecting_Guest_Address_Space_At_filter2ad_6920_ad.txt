tem performance, Bonnie++ [1] for disk performance
and SPECint 2006 [7] for CPU performance while con-
text switches during their executions are intercepted by
the hypervisor. Figure 7 reports the LMbench score for
context switch time where the performance drops about
50%.
Figure 7: LMBench: normalized result on context switch
time. The higher score means better performance.
Nonetheless, the interception does not seem to incur
noticeable impact to other benchmark results such as disk
I/O and network I/O, as shown in Figure 8, 9 and 10.
We attribute this effect to the relatively fewer number of
context switches involved during the macro-benchmark
runs, because the benchmark processes fully occupy the
CPU time slot. It is typical for a Linux process to have
between 1ms to 10 ms time-slot before being scheduled
off from the CPU.
Figure 8: LMBench: normalized result on others system
aspects. The higher score means better performance..
Figure 9: Bonnie++: normalized results on disk perfor-
mance. The higher score means better performance.
Figure 10: SPEC INT: normalized results on CPU per-
formance. The higher score means better performance.
To understand the impact of CR3 interception in real-
life scenarios, we test it with three different workloads
on the target VM: idle, online video streaming and ﬁle
downloading. Neither test shows noticeable performance
drop. When the target is under interception, the video is
rendered smoothly without noticeable jitters and the ﬁle
downloading still saturates the network bandwidth.
In our experiments, we ﬁnd that the introspection en-
counters few context switches in the target VM. To un-
derstand this phenomena, we run experiments to mea-
sure the intervals between context switches. Figure 11
shows the distribution of their lengths under different
workloads. The analysis shows that the context switch
is expected to occur after around 40 µs, which could be
used as a guideline for the VMI application to determine
the duration of a session. Note that an encounter with the
context switch costs about 6.5 µs for the introspection
and 0.58 µs for the target VM.
Figure 11: The frequency distribution of interval lengths
between context switches in three workloads: idle, video
streaming and ﬁle downloading. The x-axis is not dis-
played to the scale.
Lastly, the ImEE has a small memory footprint of a
few hundred KB on the host OS. LibVMI has a large
memory footprint as it uses up to 14MB to perform a
system call table dump.
6.2 Guest Access Speed
The turnaround time for accessing the VM refers to the
interval between sending a request and the arrival of the
reply. It consists of the time spent for checking the shared
buffers and the agent’s execution time. To assess the efﬁ-
ciency of the ImEE’s interface with the VMI application,
808    26th USENIX Security Symposium
USENIX Association
0.5$1$2p/0K$2p/16K$2p/64K$8p/16K$8p/64K$16p/16K$16p/64K$Score&W/O$Intercep5on$W/$Intercep5on$0"0.2"0.4"0.6"0.8"1"1.2"ﬁle"latency"local"comm"latency"local"comm"bandwidth"proc"latency"Score&W/O"Intercep>on"W/"Intercep>on"0"0.5"1"char*write"blk*write"rewrite"char*read"blk*read"Score&W/O"Intercep9on"W/"Intercep9on"0"0.5"1"perlbench"bzip2"gcc"mcf"gobmk"hmmer"sjeng"libquantum"h264ref"omnetpp"astar"xalancbmk"Score&W/O"IntercepCon"W/"IntercepCon"01020304050600.031.060.091.0121.0152.0182.0213.0244.0274.0Feq(%)Intervals	(in	microseconds)downloadingVideo-streamingIdle40uswe measure the turnaround time with the ImEE agent
performing no task but returning immediately. The re-
sult is approximately 265 CPU cycles (or 77 ns) in our
setting.
To evaluate the memory-reading performance of the
ImEE, we run experiments to evaluate the turnaround
time with normal read requests. Table 4 below reports
the turnaround time in comparison with LibVMI for the
same workload. To make a fair comparison, LibVMI’s
translation cache is turned on whereas the page-level data
cache is turned off.
# of Bytes
4
64
128
512
1024
ImEE (µs) LibVMI (µs)
0.353
0.358
0.389
1.643
1.715
18.4
18.5
18.4
18.9
38.1
Table 4: Memory read performance comparison.
We have also tested ImEE with the experiment de-
scribed in Section 2. The experiment shows that the
modiﬁcation on the cred address is caught immediately
when the malware makes the ﬁrst attack. Note that with
the ImEE support, it takes less than 1200 CPU cycles for
the VMI application to get a DWORD from the guest,
in contrast to more than 60,000 cycles using LibVMI.
The maximum introspection frequency of ImEE based
introspection is 2.83 MHz while an introspection using
LibVMI in our setting can only achieve 54 KHz in max-
imum.
6.3
Introspection Performance Compari-
son
We run introspection tools (syscalldump, pidlist, pslit
and credlist) in three settings: within the kernel, with
ImEE, and with LibVMI. Since this set of tests concerns
with real-life scenarios, we tested LibVMI on both KVM
and Xen for completeness. For each of the scenario, we
measure the turnaround time of introspection. The time
for the processing the semantics and the time for setting
up the ImEE/LibVMI are not included in the measure-
ment. Table 5 summarizes the results.
The experiments show that the ImEE-based introspec-
tion has a comparable performance to running inside the
kernel. It has a superior performance advantage over Lib-
VMI for traversing the kernel object lists. On KVM, The
LibVMI based introspection is around 50 times slower
than the ImEE with all caches and 300 times slower with-
out cache. On Xen, LibVMI is around 15 times and 70
times slower, respectively. Since the traversal only re-
turns a few bytes from different pages, LibVMI’s opti-
mization in bulk data transferring does not result in per-
formance gain.
6.4 Handling Multiple VMs
In a data center setting, a large number of VMs are hosted
on the same physical server. Therefore, for a VMI solu-
tion to be effective in such a setting, the capability to
handle multiple VM is important. Besides raw intro-
spection speed, two additional capabilities are important
for a VMI solution. Firstly, the VMI solution should
respond quickly to requests to introspect VMs encoun-
tered for the ﬁrst time. Secondly, it should also maintain
swift response for introspection requests on VMs already
launched.
We compared the time taken for LibVMI and ImEE to
perform a syscall table dump by our tool in two scenar-
ios. We launch four VMs on our experiment platform.
Firstly we measure the time for each solution to intro-
spect four VMs once for each in a sequence.
It takes
561 ms for LibVMI and 377 µs for ImEE, respectively.
In this case, LibVMI is about 1,400 times slower than
ImEE. The performance of LibVMI mainly due to the
initialization needed for each newly encountered VM.
Secondly, we measure the time taken for each solu-
tion for switching the introspection target among the four
VMs that are already scanned. The switching requires to
reset certain data between consecutive scans. For this
purpose, we slightly modiﬁed LibVMI to allow us to up-
date the CR3 value in the introspection context of a VM
with a new one. The experiment shows that it takes 19
ms for LibVMI to perform such work while 4.4 µs for
ImEE. ImEE shows around 4,300 times speed up. The
reason is that LibVMI’s software-based approach needs
to reset a number of memory states. In contrast, ImEE
only needs to fetch the current CR3 on the target VM’s
vCPU and replace the ImEE CR3, IP and the EPT root
pointer of the ImEE vCPU.
7 Discussions
7.1 CPU State
In-memory paging structure is only one of the factors
that determines the ﬁnal outcome of the translation of
a virtual address. In fact, the ﬁnal outcome is determined
by both in-memory state and in-CPU states. The affect-
ing in-CPU states include control registers and buffers
such as the TLB. For example, the TLB can be intention-
ally made out-of-sync with paging structures in memory,
therefore causes the introspection code to use a different
mapping from the one currently used by the target. An
ideal introspection solution should take into considera-
USENIX Association
26th USENIX Security Symposium    809
Tools
Kernel module
syscalldmp
pidlist
pslist
credlist
0.2
10
10.4
25.3
ImEE
mode
block
traversal
traversal
hybrid
time
2.9
31.6
38.6
25.6
without any cache without page cache with all cache
LibVMI(KVM / Xen)
28.2 / 43
5,887 / 2,180
8,319 / 1477
8,234 / 2,274
18.7 / 47
2,864 / 2,041
2,695 / 1,442
7,150 / 2,153
2 / 54
1,568 / 490
1,672 / 542
2,215 / 757
Table 5: Kernel object introspection performance (time in µs).
tion both sets of states because they collectively repre-
sent the current address translation.
However, for out-of-VM live introspection, it is re-
quired that it runs on a core that is independent of the
target VM. This limits the introspection’s capability to
utilize such in-CPU states because there is no mecha-
nism to fetch in-CPU states from another CPU. One pos-
sible solution is to preempt the vCPU of the target on
a physical core by a more privileged entity such as the
hypervisor, trying to preserve as many in-CPU states as
possible, including buffers and caches. However, the be-
havior of the buffers an caches when across VM transi-
tion is not ﬁxed. Therefore, without hardware assistance,
attempts to implement an ideal solution is likely met with
hardware-speciﬁc tweaks and hacks, making it very dif-
ﬁcult. We leave this issue as future work and present a
primitive solution in the Appendix.
Integration with Existing VMI Tools
7.2
The ImEE serves as the guest access engine for the VMI
applications without involving kernel semantics. It is not
challenging to retroﬁt exiting VMI tools that focus on
high-level semantics to beneﬁt from the ImEE’s perfor-
mance and security. We use VMST [19] as an exam-
ples to brieﬂy discuss how to combine a VMI application
with the ImEE. When an introspection instruction is ex-
ecuted in VMST, the XED library [10] decides whether
a data access should be redirected to the guest VM or
not. If so, the code fetches the data from the guest mem-
ory by traversing the guest VM’s page table in the same
way as LibVMI. It is easy to integrate VMST with the
ImEE. When a read redirection is generated by the XED
library, the code simply issues a memory read request
to the ImEE and waits for the reply. With the support
from the ImEE, shadow TLB and shadow CR3 proposed
in VMST are no longer needed.
ImEE vs. In-VM Introspection
7.3
Strictly speaking, the ImEE and in-VM introspection
systems are not comparable, as they are geared for dif-
ferent purposes. The ImEE is for effective target VM
access while in-VM systems are designed for reusing the
OS’s capability [23, 14] or for monitoring events in the
guest [34]. Since Process-Implant [23] and SYRINGE
[14] rely on a trusted guest kernel, we compare the ImEE
with SIM [34] from the perspective of accessing the tar-
get VM memory.
Security.
Address space isolation in SIM prevents
the target VM kernel from tampering with SIM data and
code. In a multicore VM, it does not prevent the target
VM kernel from interrupting SIM code execution by us-
ing non-maskable interrupts. By knocking down the SIM
thread from its CPU core, the rootkit can safely erase
the attack traces without being caught. In comparison,
the entire ImEE environment is separated from the target
VM. It is much more challenging (if not feasible) for the
target VM kernel to disrupt the ImEE agent’s execution.
Note that the manipulation on the page tables backﬁres
on the adversary since they are shared between the ad-
versary and the target.
Effectiveness. SIM does not enforce consistent address
mappings. The SIM code and the target VM threads
are in separated address spaces, namely using separated
page tables. The SIM hypervisor does not update the
SIM page tables according to the updates in the kernel.
In comparison, any update on the target VM page table
takes immediate effect on the ImEE and CR3 consistency
is ensured by the hypervisor.
Performance and Usability. Both SIM and ImEE make
native speed accesses to the memory without emulating
the MMU. ImEE uses EPT and does not require any
modiﬁcation on the target VM, while SIM relies on the
shadow page tables and makes non-negligible changes
on the target VM.
7.4 Paging Modes Compatibility
The design of ImEE is by nature compatible with various
paging modes such as Physical Address Extension mode
(PAE mode) and 64-bit paging. It only requires setting of
two additional bits in the control registers, namely PAE
bit in CR4 register and LME bit in EFER register so that
the ImEE core runs in the needed paging mode. To pre-
vent the adversary from changing the paging mode, the
hypervisor trap access to the above registers. To intro-
810    26th USENIX Security Symposium
USENIX Association
spect a 64-bit VM, the agent needs to be compiled into
64-bit code as well. In fact, the ImEE performs better
on a 64-bit platform, because there are more general pur-
pose registers available, reducing the number of address
space switches, and the PCID can be used to prevent the
needed TLB entries from being ﬂushed.
7.5 Architecture Compatibility
The ImEE’s design is also compatible to other multi-
core architectures such as ARM, on the condition that
the hardware supports MMU virtualization. Like the
x86 platform, ARM multicore processors also feature a
per-core MMU, thus each core’s translation can be per-
formed independently. As a result, a core can be set up to
use the translation used by the other, by setting it to use
the same root of paging structures. Moreover, by using
T T BR0 and T T BR1, the hypervisor can easily separate
the virtual address ranges used for the target accessing
and for the local usage. It simpliﬁes the design as both
can use separated page tables. The ARM processor also
grants the software more control over the TLB entries.
Thus, the needed TLB entries can be locked by the agent.
Therefore, we expect better performance than the current
design.