For different defense mechanisms, we find that Olympus reduces
attribute inference attacks the most (see Figure 14). However, it
jeopardizes the membership privacy to a large extent (see Figure 13).
For instance, the membership inference accuracy of the Talos model
(ResNet-50) on Place100 is 0.513 while the corresponding Olympus
model‚Äôs value is 0.631. The reason is that Olympus‚Äôs training pro-
cess utilizes the original training samples to fine-tune the whole
model, which leads to the model memorizing these samples with
the model‚Äôs full capacity. On the other hand, as mentioned in Sec-
tion 5.1, Talos is only performed on the training process of the base
encoder ùëì which considers each sample‚Äôs augmented views. The
original samples are only used to fine-tune the final classification
layer, the same as training a normal contrastive model. In other
words, the Talos model memorizes its training samples with only
its one-layer capacity. Therefore, Talos models are less prone to
membership inference. In addition, Olympus jeopardizes the target
model‚Äôs utility in multiple cases (see Figure 12b, Figure 12c, and
Figure 12d), the reason again lies in the training process of Olympus.
More specifically, Olympus needs to fine-tune the whole model in a
supervised way, this reduces the effect of contrastive learning in the
final model. Meanwhile, Talos preserves the contrastive learning
process to a large extent as its adversarial loss is applied together
with the contrastive loss during the training of the base encoder.
Since membership privacy, attribute privacy, and model utility are
equally important, we believe Talos is a better choice than Olympus.
Session 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea855(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 12: The performance of original classification tasks against original contrastive models, Talos, MemGuard, Olympus,
and AttriGuard with MobileNetV2, ResNet-18, and ResNet-50 on 4 different datasets. The x-axis represents different models.
The y-axis represents the accuracy of original classification tasks.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 13: The performance of NN-based membership inference attacks against original contrastive models, Talos, MemGuard,
Olympus, and AttriGuard with MobileNetV2, ResNet-18, and ResNet-50 on 4 different datasets. The x-axis represents different
models. The y-axis represents the accuracy of NN-based membership inference attacks.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 14: The performance of attribute inference attacks against original contrastive models, Talos, MemGuard, Olympus,
and AttriGuard with MobileNetV2, ResNet-18, and ResNet-50 on 4 different datasets. The x-axis represents different models.
The y-axis represents the accuracy of attribute inference attacks.
We also find that Talos, MemGuard, and AttriGuard models can
achieve similar utility as the original contrastive models (see Fig-
ure 12). However, Talos can mitigate attribute inference attacks to
a larger extent than AttriGuard and MemGuard (see Figure 14). For
instance, the attribute inference accuracy is only 0.132 on the Talos
model with ResNet-18 on the Places100 dataset, while 0.176 and
0.178 on the AttriGuard and MemGuard models. Also, as the con-
trastive learning procedure is preserved for Talos, AttriGuard, and
MemGuard, we observe that all these defenses are robust against
membership inference attacks (see Figure 13).
We also investigate the effect of the adversarial factor ùúÜ on the
performance of original classification tasks, membership inference
attacks, and attribute inference attacks. The results are summarized
in Figure 25, Figure 26, and Figure 27. First of all, we observe that
the performance of original classification tasks (Figure 25) and
membership inference attacks ( Figure 26) are relatively stable
with respect to different adversarial factors. However, for different
datasets or different model architectures, the best ùúÜ to defense
attributes inference attack may vary (Figure 27). In general, we
notice that setting ùúÜ to 2 or 3 can achieve nearly the best defense
performance on most datasets and model architectures. To perform
Talos in practice, we believe the model owner needs to tune the ùúÜ
on their validation dataset. During the process, concentrating more
on model utility or defense effectiveness depends on the ML model
owner‚Äôs purpose.
In conclusion, Talos can successfully defend attribute inference
attacks for contrastive models without jeopardizing their member-
ship privacy and model utility.
MobileNetV2ResNet-18ResNet-500.81.0AccuracyOriginalMemGuardTalosAttriGuardOlympusMobileNetV2ResNet-18ResNet-500.81.0AccuracyMobileNetV2ResNet-18ResNet-500.81.0AccuracyMobileNetV2ResNet-18ResNet-500.81.0AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyOriginalMemGuardTalosAttriGuardOlympusMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.40.50.60.7AccuracyMobileNetV2ResNet-18ResNet-500.000.250.50AccuracyOriginalMemGuardTalosAttriGuardOlympusMobileNetV2ResNet-18ResNet-500.00.1AccuracyMobileNetV2ResNet-18ResNet-500.00.2AccuracyMobileNetV2ResNet-18ResNet-500.00.20.4AccuracySession 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea8566 RELATED WORK
Contrastive Learning. Contrastive learning is one of the most
popular self-supervised learning paradigms [9, 18, 20, 29, 61, 67].
Oord et al. [61] propose contrastive predictive coding, which lever-
ages autoregressive models to predict future observations for data
samples. Wu et al. [64] utilize a memory bank to save instance rep-
resentation and k-nearest neighbors to conduct prediction. He et
al. [20] introduce MoCo, which relies on momentum to update the
key encoder with the query encoder to maintain consistency. Chen
et al. [9] propose SimCLR, which leverages data augmentation and
the projection head to enhance the performance of contrastive mod-
els. SimCLR is the most prominent contrastive learning paradigm
at the moment [34], thus we concentrate on it in this paper.
Membership Inference Attack. In membership inference, the
adversary‚Äôs goal is to infer whether a given data sample is used
to train a target model. Right now, membership inference is one
of the major means to measure privacy risks of machine learning
models [19, 23, 31, 38, 49, 52, 58, 66]. Shokri et al. [52] propose the
first membership inference attack in the black-box setting. Specifi-
cally, they rely on training multiple shadow models to mimic the
behavior of a target model to derive the data for training their at-
tack models. Salem et al. [49] further relax the assumptions made
by Shokri et al. [52] and propose three novel attacks. Later, Nasr et
al. [38] conduct a comprehensive analysis of membership privacy
under both black-box and white-box settings for centralized as well
as federated learning scenarios. Song et al. [58] study the synergy
between adversarial example and membership inference and show
that membership privacy risks increase when a model owner ap-
plies measures to defend against adversarial example attacks. To
mitigate membership inference, many defense mechanisms have
been proposed [28, 37, 49]. Nasr et al. [37] introduce an adversarial
regularization term into a target model‚Äôs loss function. Salem et
al. [49] propose to use dropout and model stacking to reduce model
overfitting, the main reason behind the success of membership
inference. Jia et al. [28] rely on adversarial examples to craft noise
to add to a target sample‚Äôs posteriors. Also, deferentially private
methods [39, 45] are introduced to mitigate membership inference.
Attribute Inference Attack. Another major type of privacy at-
tack against ML models is attribute inference. Here, an adversary
aims to infer a specific sensitive attribute of a data sample from
its representation generated by a target model [36, 56]. Melis et
al. [36] propose the first attribute inference attack against machine
learning, in particular federated learning. Song and Shmatikov [56]
later show that attribute inference attacks are also effective against
another training paradigm, namely model partitioning. They fur-
ther demonstrate that the success of attribute inference is due to
the overlearning behavior of ML models. More recently, Song and
Raghunathan [53] demonstrate that language models are also vul-
nerable to attribute inference.
Other Attacks Against Machine Learning Models. Besides
membership inference and attribute inference, there exist a plethora
of other attacks against ML models [3, 5, 22, 26, 32, 42, 43, 48, 51, 54].
One major attack is adversarial example [4, 6, 44, 59], where an
adversary aims to add imperceptible noises to data samples to evade
a target ML model. Another representative attack in this domain
is model extraction, the goal of which is to learn a target model‚Äôs
parameters [25, 30, 41, 60] or hyperparameters [40, 63].
7 DISCUSSION
Other Types of Datasets. In this paper, we only focus on image
datasets, as most of the current efforts on contrastive learning con-
centrate on the image domain. For other types of datasets like texts
or graphs, the main challenge is to define a suitable augmentation
method for the input sample. There indeed exist some preliminary
works of contrastive learning over texts or graphs [16, 67]. How-
ever, the effectiveness of these methods still needs to be further
evaluated. We believe it is straightforward to extend our analysis
to contrastive models trained on other types of data.
Novel Membership Inference Attacks Against Contrastive
Models. Traditional membership inference attacks use the original
data samples to query the model and get the corresponding poste-
riors to launch the attacks. However, such attacks is less effective
on contrastive models as shown in our paper. Since the contrastive
model is trained with some augmented views of each data sample,
the model itself may remember these augmented views as well.
This inspires us to use the augmented views of the original training
sample to query the contrastive model to obtain multiple posteriors
(one for each augmented version), and aggregate these posteriors as
the input to the membership inference attack model. However, our
initial attempt in this direction does not achieve a stronger attack.
One reason might be our aggregation method is not optimal (we
have tried averaging and concatenation). In the future, we plan to
investigate more advanced aggregation operations to establish a
membership inference attack tailored to contrastive models.
8 CONCLUSION
In this paper, we perform the first privacy quantification of the most
representative self-supervised learning paradigm, i.e., contrastive
learning. Concretely, we investigate the privacy risks of contrastive
models trained on image datasets through the lens of membership
inference and attribute inference. Empirical evaluation shows that
contrastive models are less vulnerable to membership inference
attacks compared to supervised models. This is due to the fact that
contrastive models are normally less overfitted. Meanwhile, con-
trastive models are more prone to attribute inference attacks. We
posit this is because contrastive models can generate more infor-
mative representations for data samples, which can be exploited by
an adversary to achieve effective attribute inference.
To reduce the risks of attribute inference stemming from con-
trastive models, we propose the first privacy-preserving contrastive
learning mechanism, namely Talos. Specifically, Talos introduces
an adversarial classifier to censor the sensitive attributes learned
by the contrastive models under the adversarial training frame-
work. Our evaluation shows that Talos can effectively mitigate the
attribute inference risks for contrastive models while maintaining
their membership privacy and model utility.
ACKNOWLEDGMENTS
This work is partially funded by the Helmholtz Association within
the project ‚ÄúTrustworthy Federated Data Analytics‚Äù (TFDA) (fund-
ing number ZT-I-OO1 4).
Session 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea857REFERENCES
[1] https://www.cs.toronto.edu/~kriz/cifar.html.
[2] https://github.com/Trusted-AI/adversarial-robustness-toolbox.
[3] Santiago Zanella B√©guelin, Lukas Wutschitz, Shruti Tople, Victor R√ºhle, Andrew
Paverd, Olga Ohrimenko, Boris K√∂pf, and Marc Brockschmidt. Analyzing In-
formation Leakage of Updates to Natural Language Models. In ACM SIGSAC
Conference on Computer and Communications Security (CCS), pages 363‚Äì375.
ACM, 2020.
[4] Battista Biggio, Igino Corona, Davide Maiorca, Blaine Nelson, Nedim Srndic,
Pavel Laskov, Giorgio Giacinto, and Fabio Roli. Evasion Attacks against Machine
Learning at Test Time. In European Conference on Machine Learning and Principles
and Practice of Knowledge Discovery in Databases (ECML/PKDD), pages 387‚Äì402.
Springer, 2013.
[5] Nicholas Carlini, Florian Tram√®r, Eric Wallace, Matthew Jagielski, Ariel Herbert-
Voss, Katherine Lee, Adam Roberts, Tom B. Brown, Dawn Song, √ölfar Erlingsson,
Alina Oprea, and Colin Raffel. Extracting Training Data from Large Language
Models. CoRR abs/2012.07805, 2020.
[6] Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of
Neural Networks. In IEEE Symposium on Security and Privacy (S&P), pages 39‚Äì57.
IEEE, 2017.
[7] Dingfan Chen, Ning Yu, Yang Zhang, and Mario Fritz. GAN-Leaks: A Taxonomy
of Membership Inference Attacks against Generative Models. In ACM SIGSAC
Conference on Computer and Communications Security (CCS), pages 343‚Äì362.
ACM, 2020.
[8] Min Chen, Zhikun Zhang, Tianhao Wang, Michael Backes, Mathias Humbert, and
Yang Zhang. When Machine Unlearning Jeopardizes Privacy. CoRR abs/2005.02205,
2020.
[9] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A
Simple Framework for Contrastive Learning of Visual Representations. In In-
ternational Conference on Machine Learning (ICML), pages 1597‚Äì1607. PMLR,
2020.
[10] Christopher A. Choquette Choo, Florian Tram√®r, Nicholas Carlini, and Nicolas
Papernot. Label-Only Membership Inference Attacks. CoRR abs/2007.14321, 2020.
[11] Adam Coates, Andrew Y. Ng, and Honglak Lee. An Analysis of Single-Layer
In International Conference on
Networks in Unsupervised Feature Learning.
Artificial Intelligence and Statistics (AISTATS), pages 215‚Äì223. JMLR, 2011.
[12] Maximin Coavoux, Shashi Narayan, and Shay B. Cohen. Privacy-preserving
Neural Representations of Text. In Conference on Empirical Methods in Natural
Language Processing (EMNLP), pages 1‚Äì10. ACL, 2018.
[13] Harrison Edwards and Amos J. Storkey. Censoring Representations with an
Adversary. In International Conference on Learning Representations (ICLR), 2016.
[14] Yanai Elazar and Yoav Goldberg. Adversarial Removal of Demographic Attributes
from Text Data. In Conference on Empirical Methods in Natural Language Process-
ing (EMNLP), pages 11‚Äì21. ACL, 2018.
[15] Yaroslav Ganin and Victor S. Lempitsky. Unsupervised Domain Adaptation by
Backpropagation. In International Conference on Machine Learning (ICML), pages
1180‚Äì1189. JMLR, 2015.
[16] John M. Giorgi, Osvald Nitski, Bo Wang, and Gary D. Bader. DeCLUTR: Deep
Contrastive Learning for Unsupervised Textual Representations.
In Annual
Meeting of the Association for Computational Linguistics (ACL), pages 879‚Äì895.
ACL, 2021.
[17] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative Adversarial Nets.
In Annual Conference on Neural Information Processing Systems (NIPS), pages
2672‚Äì2680. NIPS, 2014.
[18] Michael Gutmann and Aapo Hyv√§rinen. Noise-Contrastive Estimation: A New
Estimation Principle for Unnormalized Statistical Models. In International Con-
ference on Artificial Intelligence and Statistics (AISTATS), pages 297‚Äì304. JMLR,
2010.
[19] Jamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. LOGAN:
Evaluating Privacy Leakage of Generative Models Using Generative Adversarial
Networks. Symposium on Privacy Enhancing Technologies Symposium, 2019.
[20] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum
Contrast for Unsupervised Visual Representation Learning. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR), pages 9726‚Äì9735. IEEE, 2020.
[21] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning