### Defense Mechanisms and Their Impacts

For different defense mechanisms, we find that Olympus significantly reduces attribute inference attacks (see Figure 14). However, it substantially compromises membership privacy (see Figure 13). For example, the membership inference accuracy for the Talos model (ResNet-50) on the Place100 dataset is 0.513, while the corresponding value for the Olympus model is 0.631. This discrepancy can be attributed to Olympus's training process, which fine-tunes the entire model using the original training samples, leading to the model memorizing these samples with its full capacity. In contrast, as mentioned in Section 5.1, Talos only fine-tunes the base encoder \( f \) using augmented views of the samples. The original samples are used only to fine-tune the final classification layer, similar to training a standard contrastive model. Consequently, the Talos model memorizes its training samples with only one-layer capacity, making it less susceptible to membership inference.

Additionally, Olympus often degrades the target model's utility in multiple cases (see Figures 12b, 12c, and 12d). This degradation is again due to the supervised fine-tuning of the entire model, which diminishes the effect of contrastive learning. On the other hand, Talos largely preserves the contrastive learning process by applying adversarial loss alongside the contrastive loss during the training of the base encoder.

Given the importance of membership privacy, attribute privacy, and model utility, we believe Talos is a better choice than Olympus.

### Performance Comparison

Figures 12, 13, and 14 illustrate the performance of various models across different datasets. 

- **Figure 12** shows the accuracy of original classification tasks for MobileNetV2, ResNet-18, and ResNet-50 on four different datasets. The x-axis represents different models, and the y-axis represents the accuracy of the original classification tasks.
- **Figure 13** displays the performance of NN-based membership inference attacks against the same models and datasets. The x-axis represents different models, and the y-axis represents the accuracy of NN-based membership inference attacks.
- **Figure 14** depicts the performance of attribute inference attacks against the same models and datasets. The x-axis represents different models, and the y-axis represents the accuracy of attribute inference attacks.

We also observe that Talos, MemGuard, and AttriGuard models achieve similar utility to the original contrastive models (see Figure 12). However, Talos mitigates attribute inference attacks more effectively than AttriGuard and MemGuard (see Figure 14). For instance, the attribute inference accuracy for the Talos model with ResNet-18 on the Places100 dataset is 0.132, compared to 0.176 and 0.178 for AttriGuard and MemGuard, respectively. Since the contrastive learning procedure is preserved for Talos, AttriGuard, and MemGuard, these defenses are robust against membership inference attacks (see Figure 13).

### Effect of Adversarial Factor \(\lambda\)

We investigate the impact of the adversarial factor \(\lambda\) on the performance of original classification tasks, membership inference attacks, and attribute inference attacks. The results are summarized in Figures 25, 26, and 27.

- **Figure 25** shows the performance of original classification tasks, which remains relatively stable across different values of \(\lambda\).
- **Figure 26** illustrates the stability of membership inference attacks with respect to \(\lambda\).
- **Figure 27** highlights that the optimal \(\lambda\) for defending against attribute inference attacks may vary depending on the dataset and model architecture. Generally, setting \(\lambda\) to 2 or 3 achieves nearly the best defense performance on most datasets and model architectures. In practice, model owners should tune \(\lambda\) on their validation dataset, balancing model utility and defense effectiveness based on their specific needs.

### Conclusion

In summary, Talos effectively defends against attribute inference attacks for contrastive models without compromising membership privacy and model utility. 

### Related Work

#### Contrastive Learning
Contrastive learning is a popular self-supervised learning paradigm [9, 18, 20, 29, 61, 67]. Oord et al. [61] introduced contrastive predictive coding, which uses autoregressive models to predict future observations. Wu et al. [64] utilized a memory bank to store instance representations and k-nearest neighbors for prediction. He et al. [20] proposed MoCo, which maintains consistency by updating the key encoder with the query encoder using momentum. Chen et al. [9] introduced SimCLR, which enhances contrastive model performance through data augmentation and a projection head. SimCLR is currently the most prominent contrastive learning method [34], and we focus on it in this paper.

#### Membership Inference Attacks
Membership inference aims to determine whether a given data sample was used to train a target model. It is a major method for measuring the privacy risks of machine learning models [19, 23, 31, 38, 49, 52, 58, 66]. Shokri et al. [52] proposed the first membership inference attack in a black-box setting, using shadow models to mimic the target model's behavior. Salem et al. [49] relaxed the assumptions made by Shokri et al. [52] and introduced three new attacks. Nasr et al. [38] provided a comprehensive analysis of membership privacy under both black-box and white-box settings for centralized and federated learning. Song et al. [58] studied the synergy between adversarial examples and membership inference, showing that membership privacy risks increase when defending against adversarial examples. Various defense mechanisms have been proposed, including adversarial regularization [37], dropout and model stacking [49], and adding noise to target samples' posteriors [28].

#### Attribute Inference Attacks
Attribute inference attacks aim to infer specific sensitive attributes from the representations generated by a target model [36, 56]. Melis et al. [36] introduced the first attribute inference attack against federated learning. Song and Shmatikov [56] showed that attribute inference attacks are effective against model partitioning, attributing the success to overlearning by ML models. Song and Raghunathan [53] demonstrated that language models are also vulnerable to attribute inference.

#### Other Attacks Against Machine Learning Models
Other types of attacks include adversarial examples [4, 6, 44, 59], where imperceptible noise is added to data samples to evade the model, and model extraction, which aims to learn a target model's parameters [25, 30, 41, 60] or hyperparameters [40, 63].

### Discussion

#### Other Types of Datasets
This paper focuses on image datasets, as most current work on contrastive learning is in the image domain. For other types of datasets like text or graphs, the main challenge is defining suitable augmentation methods. Preliminary work on contrastive learning for texts and graphs exists [16, 67], but further evaluation is needed. Extending our analysis to other types of data is straightforward.

#### Novel Membership Inference Attacks Against Contrastive Models
Traditional membership inference attacks use original data samples to query the model and obtain posteriors. These attacks are less effective on contrastive models, as they are trained with augmented views. Using augmented views to query the model and aggregating the posteriors might improve the attack. Our initial attempts did not yield stronger attacks, possibly due to suboptimal aggregation methods (averaging and concatenation). Future work will explore more advanced aggregation techniques.

### Conclusion
This paper quantifies the privacy risks of contrastive models through membership and attribute inference. Empirical evaluation shows that contrastive models are less vulnerable to membership inference but more prone to attribute inference. To mitigate attribute inference, we propose Talos, a privacy-preserving contrastive learning mechanism. Evaluation shows that Talos effectively reduces attribute inference risks while maintaining membership privacy and model utility.

### Acknowledgments
This work is partially funded by the Helmholtz Association within the project "Trustworthy Federated Data Analytics" (TFDA) (funding number ZT-I-OO1 4).

### References
[References are listed as provided, with no changes.]

---

This revised version improves the clarity, coherence, and professionalism of the text, ensuring that the information is presented in a structured and easily understandable manner.