to data caps.
• HTTPS does not preclude classiﬁcation. We ex-
pected that the shaper would not identify applications
using HTTPS as their transport. However, we found that
in practice the devices do detect applications, speciﬁcally
via the SNI (Server Name Indication) ﬁeld in the TLS
handshake. Modifying this ﬁeld results in detection as
HTTPS (instead of the speciﬁc application).
A key question is whether our replay approach is eﬀective
for all shapers, not just those in our lab. While we can eval-
uate only the shapers we possessed, our replay approach suc-
cessfully detects diﬀerentiation in operational networks (Sec-
tion 6), indicating that our eﬀectiveness is not limited only
to the lab environment. It remains an open question whether
there are other shaper models that are not accounted for in
our design.
4.3 VPN overhead
Our control trials use a VPN tunnel, due to potential is-
sues that arise when randomized ports and payloads are clas-
siﬁed as P2P, or otherwise shaped. We now investigate the
overhead of this approach.
VPN overheads can stem from (1) IPSec encapsulation
and (2) latency added by going through the VPN (e.g., if
the VPN induces a circuitous route to the replay server).
The overhead for IPsec encapsulation varies depending on
the size of the payload, with smaller packets incurring higher
overheads. The impact on throughput is relatively small, as
shown in Table 2. For most applications, the diﬀerence in
Figure 4: The shaper has rules to rate limit YouTube and P2P
at 1Mbps and 512Kbps, respectively. Shaped at 1Mbps line is the
YouTube replay in plaintext where shaper correctly detects as
YouTube and shapes at 1Mbps. Shaped at 512Kbps is YouTube
replay with randomized payload and ports, where shaper detects
as P2P and limits at 512Kbps. The Not shaped line is YouTube
replay with string “youtube” being replaced by a random string,
which the shaper detects as “HTTP” and does not limit (since
there are no rules for generic HTTP traﬃc).
Reverse-engineering classiﬁcation. We use our testbed
to understand which features of the traﬃc were used to trig-
ger shaping, and should be preserved in replays. We replay a
recorded trace in our testbed multiple times, each time mod-
ifying a diﬀerent feature of the traﬃc (destination IP, ports,
and packet payloads) and observe its eﬀect on classiﬁcation.
Table 1 summarizes the results for running tests using
YouTube. We ﬁnd that regular expressions on packet pay-
load are the primary signature, with YouTube being cor-
rectly identiﬁed when payloads were unmodiﬁed, despite
changes to the server IP and ports (row 1). We modify three
diﬀerent aspects of the packet payload: the ﬁrst payload
byte, application-layer protocol details in the ﬁrst packet,
and arbitrary bytes after the ﬁrst packet. The second row
of the table shows that adding a one-byte packet caused the
shaper to classify the replay as HTTP (when using port 80)
or P2P (when using high, random ports). This behavior is
identical if replacing the ﬁrst byte, the “GET” command, or
removing the string “youtube” from the payload of the ﬁrst
packet (rows 3-8). However, if the GET command was un-
modiﬁed and at least one “youtube” string appeared in the
ﬁrst packet, the ﬂow was classiﬁed as YouTube. This indi-
cates that this shaping device is using regular expressions
and falling back to port-based classiﬁcation when regular
expressions do not match. Interestingly, modifying any sub-
sequent packets or payload after the GET command has no
impact on classiﬁcation.
We now summarize other key ﬁndings:
 0 1 2 3 4 5 6 7 8 9 0 2 4 6 8 10 12 14 16 18Cumulative transfer(Mbits)Time (s)Not shapedShaped at 1MbpsShaped at 512Kbps244App
Avg packet size (bytes) Avg throughput diﬀ (%)
Youtube
Netﬂix
Hangout
Skype
705
679
435
234
1.13
0.42
2.06
15.64
Table 2: Minimal eﬀect of VPN on our measurements. Skype
packets are small, leading to larger throughput overhead com-
pared to other apps.
throughput with the VPN is 2% or less. However, for Skype,
which has an average packet size of less than 300 bytes the
throughput overhead is higher (15%). Note that we both
record and replay traﬃc using an MTU that is small enough
to prevent fragmentation in the VPN tunnel.
To minimize the impact of latency, we run the replay
and VPN server on the same machine (currently running
on Amazon EC2), which adds less than 2ms of latency com-
pared to contacting the replay server directly (without a
VPN tunnel). We argue these overheads are acceptably low,
and represent a reasonable lower bound on the amount of
diﬀerentiation we can detect.
5. DETECTING DIFFERENTIATION
In this section, we present the ﬁrst study that uses ground-
truth information to compare the eﬀectiveness of various
techniques for detecting diﬀerentiation, and propose a new
test to address limitations of prior work. We ﬁnd that pre-
vious approaches for detecting diﬀerentiation are inaccurate
when tested against ground-truth data, and characterize un-
der what circumstances these approaches yield correct infor-
mation about traﬃc shaping as perceived by applications.
When determining the accuracy of detection, we sepa-
rately consider three scenarios regarding the shaping rate
and a given trace (shown in Figure 5):
• Region 1. If the shaping rate is less than the average
throughput of the recorded traﬃc, a traﬃc diﬀerentiation
detector should always identify diﬀerentiation because
the shaper is guaranteed to aﬀect the time it takes to
transfer data in the trace.
• Region 3. If the shaping rate is greater than the peak
throughput of the recorded trace, a diﬀerentiation de-
tector should never identify diﬀerentiation because the
shaper will not impact the application’s throughput (as
discussed in Section 2.2 we focus on shaping that will
actually impact the application).
• Region 2. Finally, if the shaping rate is between the
average and peak throughput of the recorded trace, the
application may achieve the same average throughput
but a lower peak throughput. In this case, it is possi-
ble to detect diﬀerentiation, but the impact of this dif-
ferentiation will depend on the application. For exam-
ple, a non-interactive download (e.g., ﬁle transfers for
app updates) may be sensitive only to changes in av-
erage throughput (but not peak transfer rates), while
a real-time app such as video-conferencing may experi-
ence lower QoE for lower peak rates. Given these cases,
there is no single deﬁnition of accuracy that covers all
applications in region 2. Instead, we show that we can
conﬁgure detection techniques to consistently detect dif-
ferentiation or consistently not detect diﬀerentiation in
this region, depending on the application.
Figure 5: Regions considered for determining detection accu-
racy.
If the shaping rate is less than the application’s average
throughput (left), we should always detect diﬀerentiation. Like-
wise, if the shaping rate is greater than peak throughput (right),
we should never detect diﬀerentiation. In the middle region, it
is possible to detect diﬀerentiation, but the performance impact
depends on the application.
Figure 6: KS Test statistic (left) and Area Test statistic (right).
In the former case, the diﬀerence between the distributions is
small in terms of throughput, but the KS Test statistic is large. In
the Area Test statistic, we ﬁnd the area between the distributions
and normalize it by the smaller of the peak throughputs for each
distribution.
In the remainder of this section, we use this region-based
classiﬁcation to evaluate three techniques for detecting dif-
ferentiation. We describe them in the next section, explain
how we calibrate their respective thresholds and parameters,
then discuss how resilient these approaches are to noise (e.g.,
packet loss), and how eﬃcient they are in terms of data con-
sumption.
5.1 Statistical tests
We explore approaches used in Glasnost [6] and NetPo-
lice [36], and propose a new technique that has high accuracy
in regions 1 and 3, and reliably does not detect diﬀerentia-
tion in region 2.
Glasnost: Maximum throughput test. Glasnost [6]
does not preserve the inter-packet timing when replay-
ing traﬃc, and identiﬁes diﬀerentiation if the maximum
throughput for control and exposed ﬂows diﬀer by more than
a threshold. We expect this will always detect diﬀerentia-
tion in region 1, but might generate false positives in regions
2 and 3 (because Glasnost may send traﬃc at a higher rate
than the recorded trace).
NetPolice: Two-sample KS Test.
As discussed in
NetPolice [36], looking at a single summary statistic (e.g.,
maximum) to detect diﬀerentiation can be misleading. The
issue is that two distributions of a metric (e.g., throughput)
may have the same mean, median, or maximum, but vastly
diﬀerent distributions. A diﬀerentiation detection approach
should be robust to this.
To
address
this, NetPolice uses
two-sample
Kolmogorov–Smirnov (KS) test, which compares two empir-
ical distribution functions (i.e., empirical CDFs) using the
maximum distance between two empirical CDF sample sets
the
Region 1(below avg)Region 2(between avg & max)Region 3(above max)Shaping rateAvgMaxReplay transfer rateKS TeststatisticThroughput (KB/s)CDFArea Test = a/wThroughput (KB/s)CDFwa2455.3 Evaluation criteria
We now describe the criteria under which we evaluate the
three diﬀerentiation tests.
Overall accuracy: Using the taxonomy in Figure 5, a
statistical test should always detect diﬀerentiation in region
1, and never detect diﬀerentiation in region 3. We deﬁne ac-
curacy as the fraction of samples for which the test correctly
identiﬁes the presence or absence of diﬀerentiation in these
regions.
Resilience to noise: Diﬀerences between two empirical
CDFs could be explained by a variety of reasons, including
random noise [30]. We need a test that retains high accuracy
in the face of large latencies and packet loss that occur in
the mobile environment. When evaluating and calibrating
our statistical tests, we take this into account by simulating
packet loss.
Data consumption: To account for network variations
over time, we run multiple iterations of control and exposed
trials back to back. We merge all control trials into a single
distribution, and similarly merge all exposed trials. As we
show below, this can improve accuracy compared to running
a single trial, but at the cost of longer/more expensive tests.
When picking a statistical test, we want the one that yields
the highest accuracy with the smallest data consumption.
5.4 Calibration
We identify the most accurate settings of threshold values.
Glasnost: We used the threshold suggested by Glasnost,
δ = 0.2. This yields perfect accuracy in region 1, but gener-
ates 100% false positives in region 3 (as expected). Glasnost
always detects diﬀerentiation in region 2. For applications
that are sensitive to changes in peak throughput, this test
will yield the correct result.
KS Test: We use thresholds suggested by NetPolice, i.e.,
α = 0.95 and β = 0.95. This yields good accuracy in re-
gions 1 and 3, but inconsistent behavior in region 2.
In
other words, this test will sometimes detect diﬀerentiation
in region 2, and sometimes not — making it diﬃcult to use
for detection in this region. We also observed that even for
tests with no added loss in our testbed over well-provisioned
wired network, up to 8% of tests were considered invalid by
KS Test due to the issue shown in Fig. 6, while the Area Test
can correctly detect for diﬀerentiation (or no diﬀerentiation)
in those cases.
Area Test: We ﬁnd that t = 0.1 or t = 0.2 yield the best
accuracy, depending on the application (Fig. 7). This yields
good accuracy in regions 1 and 3, and consistent decisions of
no diﬀerentiation for region 2. For apps that are insensitive
to changes only to peak throughput (average throughput
stays the same), this will yield the correct result.
5.5 Evaluation results
Summary results for low noise. We ﬁrst consider the
accuracy of detecting diﬀerentiation under low loss scenar-
ios. Table 3 presents the accuracy results for four popular
apps in regions 1 and 3. We ﬁnd that the KS Test and
Area Test have similarly high accuracy in both regions 1
and 3, but Glasnost performs poorly in region 3 because it
will detect diﬀerentiation, even if the shaping rate is above
the maximum. We explore region 2 behavior later in this
section; until then we focus on accuracy results for regions
Figure 7: Calibrating Area Test for Youtube. We pick 0.2 as
the threshold.
for a conﬁdence interval α. To validate the KS Test result,
NetPolice uses a resampling method as follows: randomly
select half of the samples from each of the two original in-
put distributions and apply the KS Test on the two sample
subsets, and repeat this r times. If the results of more than
β% of the r tests agree with the original test, they conclude
that the original KS Test statistic is valid.
Our approach: Area Test. The KS Test only consid-
ers the diﬀerence between distributions along the y-axis (as
shown in Figure 6, left) even if the diﬀerence in the x-axis
(in this case, throughput) is small. In our experiments, we
found this makes the test very sensitive to small changes in
performance not due to diﬀerentiation, which can lead to
inaccurate results or labeling valid tests as invalid.
To address this, we propose an Area Test that accounts for
the degree of diﬀerentiation detected: we ﬁnd the area, a,
between the two CDF curves (as shown in Fig. 6, right) and
normalize it by the minimum of peak throughputs for each
distribution, w. This test concludes there is diﬀerentiation if
the KS Test detects diﬀerentiation and the normalized area
between the curves is greater than a threshold t. We discuss
how we select thresholds in Sec. 5.4.
5.2 Testbed environment
Our testbed consists of a commercial traﬃc shaper, a re-
play client, and a replay server (Figure 3). We vary the
shaping rate using the commercial device to shape each ap-
plication to throughput values in regions 1-3. Depending
on the average and maximum throughputs for each trace,
we vary the shaping rate from 0.1 Mbps to 30 Mbps (9% to
300% of peak throughput).
We emulate noisy packet loss in our tests using the Linux
Traﬃc Control (tc) and Network Emulation (netem) tools
to add bursty packet loss according to Gilbert-Elliott (GE)
model [10]. We perform all our evaluations and calibrations
under three conditions: 1) low loss (no added loss), 2) mod-
erate loss (1.08%), and 3) high loss (1.45%). We found that
correlated losses higher than 1.5% had disastrous eﬀects on
TCP performance and all detection techniques were inaccu-
rate.
Evaluation method. We explore the parameter space by
varying the application, loss rate, shaping rate, and number
of replay repetitions for each statistical test. We present re-
sults for a few popular TCP-based applications (YouTube
and Netﬂix) and UDP-based applications (Skype and Hang-
out); we also performed tests on a number of other appli-
cations (omitted for space). We repeat each test 10 times,
where a test is a replay performed with a given set of pa-
rameters, with and without shaping. We present results that
aggregate these tests to produce statistically signiﬁcant re-
sults.
 0.4 0.5 0.6 0.7 0.8 0.9 1 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1Accuracyregions 1 & 3Area Test threshold246App
Netﬂix
YouTube
Hangout
Skype
100
100
100
100
KS Test Area Test Glasnost
R1 R3
R1 R3 R1
100
100
100
100