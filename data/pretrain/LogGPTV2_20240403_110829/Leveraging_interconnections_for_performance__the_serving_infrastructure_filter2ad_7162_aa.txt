title:Leveraging interconnections for performance: the serving infrastructure
of a large CDN
author:Florian Wohlfart and
Nikolaos Chatzis and
Caglar Dabanoglu and
Georg Carle and
Walter Willinger
Leveraging Interconnections for Performance
The Serving Infrastructure of a Large CDN
Florian Wohlfart∗§ Nikolaos Chatzis§ Caglar Dabanoglu§ Georg Carle∗ Walter Willinger‡
§Akamai Technologies
∗Technical University of Munich ‡NIKSUN, Inc.
ABSTRACT
Today’s large content providers (CP) are busy building out
their service infrastructures or łpeering edgesž to satisfy the
insatiable demand for content created by an ever-expanding
Internet edge. One component of these serving infrastruc-
tures that features prominently in this build-out is their con-
nectivity fabric; i.e., the set of all Internet interconnections
that content has to traverse en route from the CP’s various
łdeploymentsž or łserving sitesž to end users. However, these
connectivity fabrics have received little attention in the past
and remain largely ill-understood.
In this paper, we describe the results of an in-depth study
of the connectivity fabric of Akamai. Our study reveals that
Akamai’s connectivity fabric consists of some 6,100 different
łexplicitž peerings (i.e., Akamai is one of the two involved
peers) and about 28,500 different łimplicitž peerings (i.e.,
Akamai is neither of the two peers). Our work contributes to
a better understanding of real-world serving infrastructures
by providing an original account of implicit peerings and
demonstrating the performance benefits that Akamai can
reap from leveraging its rich connectivity fabric for serving
its customers’ content to end users.
CCS CONCEPTS
· Networks → Network architectures;
KEYWORDS
Content Providers, Content Delivery Networks, Peering
ACM Reference Format:
Florian Wohlfart, Nikolaos Chatzis, Caglar Dabanoglu, Georg Carle,
and Walter Willinger. 2018. Leveraging Interconnections for Perfor-
mance: The Serving Infrastructure of a Large CDN. In SIGCOMM ’18:
SIGCOMM 2018, August 20ś25, 2018, Budapest, Hungary. ACM, New
York, NY, USA, 15 pages. https://doi.org/10.1145/3230543.3230576
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’18, August 20ś25, 2018, Budapest, Hungary
© 2018 Copyright held by the owner/author(s). Publication rights licensed
to ACM.
ACM ISBN 978-1-4503-5567-4/18/08. . . $15.00
https://doi.org/10.1145/3230543.3230576
1 INTRODUCTION
Today’s large Internet content providers (CP) that include
the large content delivery networks (CDN) are faced with
the problem of having to serve ever-increasing traffic vol-
umes to a growing number of increasingly heterogeneous
end points (e.g., end users, IoT devices) that reside in differ-
ent types of networks, consume diverse types of content and
require ever more stringent performance guarantees. When
trying to solve this challenging problem, the serving infras-
tructures that these large CPs maintain take center stage.
Here, a CP’s serving infrastructure (also referred to as its
ł(Internet) peering edgež or łpeering surfacež) consists of
two main components. The first is its footprint; that is, a
set of łdeploymentsž, where a deployment comprises of one
or more clusters of servers. By this definition, deployments
may or may not contain servers that are directly involved in
serving content to end users. While our focus will be mainly
on deployments with such end user-facing server clusters
(known as łedge nodesž or łserving sitesž), the footprint of
a large CP’s serving infrastructure typically also contains
other types of deployments, and in this paper, we will specify
the deployment type unless it is obvious from the context.
The second component is its connectivity fabric; that is, the
set of Internet interconnections or peerings that the content
served by this CP has to traverse as it travels from the CP’s
deployments where it is ingested or resides to the end users
where it was requested. Aspects of these large CPs’ serving
infrastructures that are of particular interest are the extent
and structure of their footprints, the details of their connec-
tivity fabrics, and their ability to scale in a cost-effective
manner as traffic volumes keep increasing, performance con-
siderations gain in importance, and the required capabilities
(e.g., measurements of the network’s and infrastructure com-
ponents’ state and performance, directing end user clients
to servers) become more demanding.
The serving infrastructures that some of today’s large CPs
have built and rely on differ in size, design, and operations.
For example, in a recent paper [47], Google states that it
has łone of the largest peering surfaces in the worldž and
describes the structure of this łsurfacež as consisting of a
set of edge nodes (i.e., Google-supplied servers known as
Google Global Cache (GGC) that are deployed in third-party
networks) and a set of interconnected edge PoPs in some
206
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
F. Wohlfart, N. Chatzis, C. Dabanoglu, G. Carle, W. Willinger
70 metro areas where Google connects to the rest of the
world via peering [23]. Moreover, in support of this struc-
ture, Google operates a private inter-data center backbone
and a separate WAN that connects to external peers and
back to its large data centers [47]. Facebook describes its
serving infrastructure as consisting of łdozens of PoPs in six
continentsž where it has łthousands of peers and serves over
two billion usersž [42].1 While there have been a number of
recent papers that describe various aspects of these and other
CPs’ serving infrastructures (e.g., see [7, 9, 42, 47]), the focus
has been almost exclusively on their footprints and on traf-
fic engineering-related challenges posed by the sheer scale
of these infrastructures. At the same time, the connectivity
fabrics that these CPs utilize to get content from the various
deployments all the way to the end users have received little
or no attention and remain largely ill-understood.
In this paper, we provide a detailed account of the serving
infrastructure of Akamai, especially its connectivity fabric.
Akamai is a large global CDN whose serving infrastructure’s
footprint consists of a large number of deployments of differ-
ent types (including deployments in third-party networks).
Akamai also operates its own multi-service backbone to sup-
port the delivery of its customers’ content to end users. We
note, however, that despite detailing the serving infrastruc-
ture of Akamai, our study is not about which type of serving
infrastructure (e.g., one with or without deployments in third-
party networks, one with or without a backbone) is better
or worse. Instead, our work uses Akamai’s existing serving
infrastructure as an example to highlight the important but
subtle aspects that need to be considered when examining
this increasingly important part of a large CP’s infrastruc-
ture, especially with respect to determining and establishing
the exact extent and structure of its connectivity fabric.
To this end, we first report in Section 3 on an in-depth
study of the connectivity fabric component of Akamai’s serv-
ing infrastructure. Our study reveals a bifurcation of all of
the interconnections utilized by Akamai into (i) a set of 6.1k
łexplicitž peerings (e.g., traditional peering options where
Akamai is one of the two involved peers) and (ii) a set of 28.5k
łimplicitž peerings (i.e., traditional peering choices where nei-
ther of the involved peers is Akamai). We elaborate on what
information sources are required to fully and conclusively
account for such a rich connectivity fabric and discuss why
relying on publicly available BGP information (and possibly
other information obtained from additional active measure-
ment campaigns) provides only an inadequate picture of this
densely connected fabric. In the process, we explain why
1Searching various online sources produces other relevant details about
Facebook’s serving infrastructure, including (i) the deployment of Facebook-
owned and supplied servers known as Facebook Network Appliances (FNA)
in third-party networks [6] and (ii) the operation of a newly-deployed
private inter-data center backbone network called Express Backbone [20].
these implicit peerings have gone largely unnoticed in the
past (see, however, [13] for an earlier account of implicit
peerings at a large IXP) and have prevented researchers from
appreciating the full extent of the serving infrastructures
of large CPs such as Akamai. In addition, we show that the
contributions of Akamai’s different deployment types to its
sizable connectivity fabric are uneven, with some deploy-
ments contributing only explicit peerings and others only
implicit peerings, and we provide illustrative examples to ex-
plain this observation. Next, in view of Akamai’s objective to
optimize the performance of content delivery as experienced
by the end users, we quantify in Section 4 some performance
benefits that Akamai reaps from leveraging its rich connec-
tivity fabric when choosing from among the different options
it has for serving its content end users.
At first glance, determining the connectivity fabric of a
network A boils down to identifying the number and type of
direct peerings that A utilizes to connect to other networks
within the larger Internet, together with the equipment that
is located in the different deployments and is required to
establish and operate those peerings. These peerings can be
of the following well-known types and are łexplicitž in the
sense that network A is always one of the two parties to such
a peering: transit (dedicated PNI), private peering (via dedi-
cated PNI), public peering in the form of bilateral peering via
an IXP, and public peering in the form of multilateral peering
via an IXP’s route server. However, this simple picture of net-
work A’s connectivity fabric gets significantly more complex
when A is a large CP that operates, for example, deployments
in a third-party network B that happens to have a number
of eyeball networks as downstream customers. Note that
content destined from any of A’s deployments in network B
to any of the end users in any of B’s downstream customers
necessarily contributes to interdomain traffic; that is, such
content has to traverse existing explicit peerings between
network B that houses A’s deployments and B’s downstream
customers where the end users reside. In effect, as a result of
operating deployments in B’s network, the large CP A łinher-
itsž B’s explicit peerings with its downstream customers and
can leverage them to serve its customers’ content to those
networks’ end users. Since none of the two peers involved
in such an inherited peering is the large CP A, we refer to
them in this paper, as łimplicitž peerings to distinguish them
from the above-defined explicit peerings.
Note that in the above example, the large CP A also in-
herits from B any of B’s explicit peerings with its upstream
providers but the łterms-of-usež for these types of inherited
or implicit peerings are typically more restrictive (e.g., use for
cache fill is allowed; use of transit to serve other networks’
end users is restricted or not allowed) than those associ-
ated with B’s downstream-related implicit peerings. In this
paper, we are mainly concerned with downstream-related
207
Leveraging Interconnections for Performance
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
implicit peerings. However, to provide a complete picture of
the full connectivity fabrics of large CPs such as Akamai, we
will include the upstream-related implicit peerings and state
their number separately. Irrespective of the type of implicit
peering, from a practical perspective, the main difference
between explicit and implicit peerings is that the latter give
the large CP A no say in either establishing or operating
them. In fact, while A is by and large in charge of the traffic
that traverses any of its explicit peerings, A is just one of the
many contributors to the traffic carried by implicit peerings.
Nevertheless, since implicit peerings are used by A to carry
its customers’ content, we view them as being as critical a
part of A’s connectivity fabric as A’s explicit peerings, es-
pecially because of the mutual benefits that the large CP A
and network B derive from them. For example, in the case
of B’s downstream-related implicit peerings, their use by A
reduces transit costs for B and the cost for serving content
for A. At the same time, by getting content closest to eyeballs,
the use of these implicit peerings improves the performance
of content delivery as experienced by the end users (i.e., a
metric by which B and its downstreams get evaluated by end
users and A gets evaluated by its customers).
2 OVERVIEW OF AKAMAI’S SERVING
INFRASTRUCTURE
In this section, we describe Akamai’s serving infrastructure
in terms of its footprint, focusing in particular on the diverse
nature of the deployed servers and how Akamai’s servers
are organized into clusters and ultimately into deployments,
and present some typical types of deployments.
2.1 Basic Components: Footprint
As a general rule, Akamai’s servers can be grouped into
end-user facing (EUF) delivery servers, non-end-user facing
(non-EUF) delivery servers, and non-delivery servers. The
first group consists of HTTP and/or HTTPS (referred to as
HTTP/S throughout this paper) servers that are directly in-
volved in serving content to end users and other delivery
servers. In contrast, their non-EUF counterparts are HTTP/S
servers that participate only indirectly in the delivery of con-
tent to end users and other servers (e.g., serving content to
other servers such as storage servers or performing functions
such as transcoding media content). While these non-EUF
delivery servers may not be required for some scenarios
they are essential for the delivery of the type of content that
relies on them. Finally, Akamai also operates non-delivery
servers, and as the name indicates, the servers belonging to
this group do not play any role in the delivery of content
but are used for other purposes. An important example of
such servers are Akamai’s BGP collectors (see below). Our
interest in this paper is in the combination of Akamai’s EUF
delivery servers and Akamai’s BGP collectors.
Akamai’s EUF delivery servers run Akamai’s own soft-
ware stack on custom-built hardware. These servers can be
flexibly configured to serve many different purposes and
have various capabilities. Noteworthy and distinctive server
capabilities include aspects related to delivery, performance,
and caching. The same software is used for servers that serve
different workloads (e.g., cacheable, non-cacheable content),
different customer needs (e.g., origin offload), or different
traffic types (e.g., latency-sensitive traffic). What differs is
which servers and what capabilities of those servers are
used with respect to delivery, performance, or caching and
how they are organized (e.g., flat, hierarchically) in each
use case. To communicate, delivery servers talk HTTP/S to
other Akamai servers and customers’ origin servers as well
as to clients running on end user devices (e.g., browsers,
customers’ download clients or players, set-top boxes). Aka-
mai’s EUF delivery servers are organized into clusters that
are located in a total of about 3.3k deployments within more
than 1,600 networks around the world.
2.2 Typical Deployment Types
Although the 3.3k deployments that contain EUF delivery
servers vary in size, design, role, and capabilities, Table 1
summarizes the characteristics of four generic deployment
types and provides the main differences and similarities in
how they are architected and used. The listed Type 1, Type
2, Type 3, and Type 4 deployments are representative of
Akamai’s present-day serving infrastructure and account for
more than 85% of all deployments. We will describe aspects
of their external connectivity in Section 2.3.1 and mention
further relevant details about these four deployment types
when describing and explaining some of the main results of
our empirical analysis in Section 3 and Section 4.
For the purpose of this paper, it is important to note that
by virtue of operating an Akamai-owned border router (on
Akamai’s peering AS) that participates in BGP, Type 3 and
Type 4 deployments contribute only explicit but no implicit
peerings to Akamai’s connectivity fabric. In contrast, the
absence of an Akamai-owned router and reliance on a host-
ing network-owned border router for Type 1 and Type 2
deployments implies that they contribute only implicit but
no explicit peerings to Akamai’s connectivity fabric.
2.3 An Infrastructure for Measurement
Irrespective of whether or not they contain EUF delivery
servers, Akamai’s deployments also play an important role
as a set of vantage points of a global-scale measurement
platform that this CDN has developed over time and keeps
changing and improving in the face of ever-changing needs
208
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
F. Wohlfart, N. Chatzis, C. Dabanoglu, G. Carle, W. Willinger
Table 1: Characteristics of Akamai’s typical deployments with least one EUF delivery server group.
Deployments without an Akamai router
Deployments with an Akamai-owned router
Type 1
Type 2
Type 3
Type 4
Small/Medium
Typical size
Host network
IP address space used
Host network
AS used
No
Akamai transit link
Cache fill
Use of transit link
Can serve all end users
No
Target end users/networks Host/Downstreams
High/Very High
Proximity to end users
Example of typical setting
Eyeball network
Example of atypical setting Wholesale network
Medium/Large
Host network