exclusive, that is, crossover is followed by mutation with a
ﬁxed probability.
Crossover: Crossover is a simple operation wherein
two parent inputs are selected from the previous generation
and two new child inputs are generated. Figure 3 illustrates
the process of generating two child inputs from two parent
inputs.
Fig. 3. Crossover operation in VUzzer. In this single-point crossover, we
select the 5th offset as cut-point. For the ﬁrst parent (this is input1),
1=this and i2
1= is input1. For
this strategy breaks it into two parts: i1
2= the other
the second parent, we again get two parts: i1
1 | i2
input2. Now, we form two children by using i1
2, that is,
this the other input2 and I am is input1.
2=I am and i2
2 and i2
1 | i1
Mutation: Mutation is a more complex operation,
which involves several suboperations to change a given par-
ent input into the corresponding child input. The process is
detailed in the following steps:
Step 1: Randomly select tainted offsets from the set Oother
and insert strings at those offsets. The strings are formed
by bytes obtained from the set Limm.
Step 2: Randomly select offsets from the set Llea and mutate
such offsets in the string from Step 1 by replacing them
with interesting integer values, such as, 0, M AX U IN T ,
negative numbers.
Step 3: For all the tainted cmp instructions for the parent
input, if the values of op1 (cid:54)= op2, replace the value at
the tainted offset in the string from Step 2 with the value
of op2 or else with a ﬁxed probability replace the tainted
byte by a random sequence of bytes.
Step 4: Place the magic bytes at the corresponding offsets as
determined by our magic-byte detector.
B. Implementation Details
The core functionality of VUzzer is implemented in Python
2.7. Some of the implemented analyses, for example incre-
mental analysis for error-handling basic block detection, are
memory intensive and therefore we also make use of efﬁcient
data structures provided by more recent versions, such as
BitVector3. VUzzer internally consists of two main compo-
nents, comprising static and dynamic analyses, as further
detailed below.
Static analysis: VUzzer implements both of the static
analyses (constant string extraction and basic block weight
calculation) within IDA [27]. The analysis is written in Python
using IDAPython [18].
Dynamic analysis: VUzzer implements both dynamic
analyses (basic-block tracing and DTA) on the top of the Pin
dynamic analysis framework [31]. For basic block tracing,
we implemented a pintool to record each basic block, along
with its frequency, encountered during the execution. Our
pintool can selectively trace basic blocks executed by certain
libraries on-demand. Selective library monitoring allows us to
reduce the execution trace overhead and focus on the intended
application code.
Our DTA implementation is based on DataTracker, pro-
posed by Stamatogiannakis et.al [46], which in turn is based on
LibDFT [29]. As LibDFT can only handle 32-bit applications,
the current VUzzer prototype can only be used to fuzz 32-bit
applications (also used in our evaluation). Note that this is not a
fundamental limitation and we are, in fact, in the process of im-
plementing 64-bit support in VUzzer. Any updated version will
be made available at https://www.vusec.net/projects/fuzzing.
To make it suitable for our purposes, we also made several
changes to DataTracker:
• In DataTracker,
tags
taint
each
tuples: , i.e., unique ﬁle descriptor and the
offset of the ﬁle associated with that descriptor. Each
of these tuples is 64 bit long (32 bits for ufd and 32
associated with
3https://pypi.python.org/pypi/BitVector/3.4.4
8
I am is input1I am the other input2this is input1this the other input2bits for file_offset). Each memory location has a
set of these tuples associated with it to determine the
offsets and the ﬁles by which the memory location is
tainted. We changed this to a EWAHBoolArray type4
which is a compressed bitset data type. Since we only
need data-ﬂow information from one (input) ﬁle, we
modiﬁed DataTracker to propagate taint only through
that ﬁle. Thus, in our modiﬁed version, the taint tags
associated with each memory location are modeled as
a EWAHBoolArray which only contains offsets. As a
result, our implementation is at least 2x faster and uses
several times less memory than DataTracker.
• We added instrumentation callbacks for the cmp family of
instructions like CMP, CMPSW, CMPSB, CMPSL and
the lea instruction to catch byte-level taint information
for the operands involved in the computations.
• We rewrote hooks for each implemented system call
and also added hooks for some extra system calls such
as pread64, dup2, dup3, mmap2, etc. To evaluate our
performance on the DARPA dataset [15], we also im-
plemented hooks for DECREE-based system calls, which
are different from normal Linux system calls.
Crash triage: Once fuzzing starts producing crashes,
it may continue to produce more crashes and there should be
some mechanism to differentiate crashes due to different bugs
(or the same bug but different instances). In order to determine
the uniqueness of a crash, VUzzer uses a variant of stack hash,
proposed by Molnar et.al. [37]. In our pintool, we implemented
a ring buffer that keeps track of the last 5 function calls and
the last 10 basic blocks executed before we get a crash. We
calculate the hash of this buffer and each time a new crash is
encountered, we compare the newly generated hash with the
older ones to determine if the reported crash is a new unique
one.
V. EVALUATION
In order to measure the effectiveness of our proposed
fuzzing technique,
this section presents an evaluation of
VUzzer. To expose VUzzer to a variety of applications, we
chose to test VUzzer on three different datasets A. DARPA
CGC binaries [15], B. miscellaneous applications with binary
format as used in [43], and C. a set of buggy binaries recently
generated by LAVA [17].
We ran our experiments on an Ubuntu 14.04 LTS system
equipped with a 32-bit 2-core Intel CPU and 4 GB RAM.
For the DARPA CGC dataset,
the (provided) environment
is a VM with a customized OS called DECREE. We want
to emphasize that our main evaluation goal is to show how
effective VUzzer is in identifying bugs (that may be buried
deep in the execution) with much fewer inputs than state-of-
the-art fuzzers such as AFL. Our current VUzzer prototype
is not as optimized for fast input execution as AFL and we
therefore seek no comparison in this direction.
A. DARPA CGC Dataset
As part of Cyber Grand Challenge, DARPA released a set
of binaries that run in a customized OS called DECREE. There
4https://github.com/lemire/EWAHBoolArray
9
are 131 binaries in total, with various types of bugs injected
in them. However, we could not run VUzzer on all of them
for the following reasons:
• All of the binaries are interactive in nature by accepting
inputs from STDIN. Once started, many of them present
a menu to choose an action, including the option to quit.
Furthermore, in many cases, there are multiple menus (in
a different state of the program) with different options to
quit. As VUzzer requires a step to generate totally random
inputs (error-handling code detection, Section IV-A4),
executing such inputs puts the application in a loop,
looking for valid options, including the option to quit.
This causes the application to run forever. This is an
interfacing problem and not a fundamental limitation of
our fuzzing method.
• Some of these binaries are compiled with ﬂoating-point
instructions, which are not handled by LibDFT and thus
VUzzer cannot get correct data-ﬂow information.
• As VUzzer is based on Pin [32], we followed the given
procedure to run pintools in DECREE5. However, we
could not run some of the binaries with Pin.
• Some of the binaries involve interaction with other bina-
ries, which is not handled by VUzzer.
After considering the obstacles mentioned above, we are left
with a total of 63 binaries. In order to make a comparison
with AFL, we also ran AFLPIN, a pintool-based AFL imple-
mentation6. AFLPIN has the same fuzzing engine as AFL, but
a different mechanism to get execution traces. Our choice to
use AFLPIN instead of AFL is to have an identical interfacing
mechanism with the SUT, that is, passing input to the pintool
via ﬁle descriptor 0 (STDIN).
VUzzer found crashes in 29 of the CGC binaries, whereas
AFLPIN found only 23 crashes. As each CGC is also accom-
panied with a patched version, we veriﬁed each bug found
by VUzzer by running the patched version of the binary to
observe no further crashes. The most important result was the
number of executed inputs per crash in both of these fuzzers.
We ran both fuzzers for a maximum of 6 hours. Figure 4
depicts the number of executions for the cases where both of
the fuzzers found crashes (13 in total), evidencing that VUzzer
can signiﬁcantly prune the search space compared to AFL.
While fuzzing a speciﬁc binary NRFIN_00015, we ob-
served the importance of computing the ﬁtness score fi in a
discrete manner. The vulnerability in this binary is a typical
case of buffer overﬂow in a loop. We observed that after
generation 18,
there was no new BB discovered, but fi
kept increasing, indicating typical loop execution behavior. At
generation 63 (total executions 13K), we reach the boundary
of the buffer. AFLPIN could not detect this crash.
We note that our current results on this dataset are modest,
especially in the view of the results reported in Driller [47].
We further investigated the results and found some peculiarities
that may interfere with the performance of our current VUzzer
prototype on CGC.
5https://github.com/CyberGrandChallenge/cgc-release-
documentation/blob/master/walk-throughs/pin-for-decree.md
6https://github.com/mothran/aﬂpin
To improve readability, we restate the results from the
original LAVA paper in Table II. The last column in Table II
shows the results produced by VUzzer. The numbers shown
are the total unique bugs identiﬁed by VUzzer. In the case
of md5sum, we could not run VUzzer as it crashed on the
ﬁrst round of input generation without allowing the program
to parse more of any input. Each injected fault in the LAVA
binaries has an ID and the ID is printed on standard output
before each binary crashes due to that fault. This allowed us
to precisely identify the faults triggered by VUzzer. Table III
reports the IDs of the faults triggered by VUzzer for each
LAVA binary.
TABLE II.
LAVA-M DATASET: PERFORMANCE OF VUZZER
COMPARED TO PRIOR APPROACHES.
Program
Total bugs
FUZZER
SES
uniq
base64
md5sum
who
28
44
57
2136
7
7
2
0
0
9
0
18
VUzzer (unique bugs, to-
tal inputs)
27 (27K)
17 (14k)
1*
50 (5.8K)
A few interesting points emerge from our LAVA dataset
results. Most of the LAVA injected faults are based on artiﬁ-
cially injected path conditions, like lava to reach a particular
path and trigger the bug. This is very well captured by VUzzer,
thanks to its data-ﬂow features. For example, during base64
fuzzing, we learned that the ﬁrst four bytes should be either
‘val or lav‘ to follow a particular path. Similarly, we
discovered that the last few bytes should contain any of the fol-
lowing values to take different paths: las[,lat\x1b,Wsal,
etc. It should be noted that most of the path constraints
injected by LAVA are multibyte constraints. Such constraints
pose a serious problem for AFL to traverse deeper in the
execution (as also noted in [16]). Another interesting point
is the performance of VUzzer on who. The fuzzer used in the
LAVA paper failed to ﬁnd even a single bug, whereas VUzzer
found several unique crashes.
TABLE III.
FAULT IDS OF BUGS DETECTED BY VUZZER ON THE
LAVA-M DATASET.
Program
uniq
base64
md5sum
who
Fault IDs
468, 318, 293, 170, 130, 443, 171, 393, 169, 368, 112, 322,
166, 227, 371, 472, 321, 215, 222, 297, 372, 396, 446, 397,
471, 296, 447
1, 843, 817, 386, 786, 805, 576, 276, 222, 806, 284, 841, 584,
235, 278, 583, 788
-
4159, 4343, 3800, 83, 1188, 60, 137, 138, 1960, 59, 1458, 1,
159, 5, 1803, 1314, 79, 475, 18, 4, 9, 1804, 1816, 10, 7, 3, 58,
985, 179, 14, 319, 2617, 81, 22, 2, 63, 4364, 8, 672, 341, 26,
255, 20, 75, 474, 6, 4358, 4362, 587, 89
Overall, on both artiﬁcial datasets, VUzzer reported en-
couraging results, although, as expected, it did struggle with
interactive programs in the DARPA CGC dataset. We now
move on to evaluate VUzzer on real-world programs that have
also been considered by other fuzzers.
C. Various Applications (VA) Dataset
We use a dataset of real-world programs (djpeg/eog,
tcpdump, tcptrace, pdf2svg, mpg321, gif2png) to
evaluate the performance of VUzzer. Rebert et al. also eval-
uated these programs to report on several bugs [43] and
we therefore included these programs in our evaluation for
Fig. 4. Relative number of inputs executed for each of the CGC Binaries,
wherein both VUzzer and AFLPIN ﬁnd crashes. The numbers above the bars
are the total number of inputs (in thousands) executed.
• In several binaries, a buggy state is reached only by per-
forming a very speciﬁc set of actions from a given menu.
For exmaple,
in the CROMU_00001 application, one
has to perform: login A -> send many msg to
user B -> login B -> check msg . Currently,
VUzzer does not have capabilities to repeat a sequence.
• The notion of valid input is blurry. Recall that we use a
whole session, provided in the form of XML ﬁles by every
CGC binary, as one input. Therefore, there is essentially
no notion of invalid inputs. Because of this, we cannot
exploit the full power of VUzzer.
• Related to the above point is the issue of interesting
offsets. As the CGC binaries are interactive, the input is
essentially a sequence to explore application state, which
may vary from one input to another. For example, one
of the binaries allows a user to load a ﬁle. The bug is