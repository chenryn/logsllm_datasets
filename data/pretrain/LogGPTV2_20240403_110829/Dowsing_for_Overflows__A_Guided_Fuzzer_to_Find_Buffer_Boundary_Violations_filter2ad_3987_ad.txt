order does not matter. On the other hand, it will not help
if the order is ﬁxed.
6.1.3 Importance of guiding symbolic execution
We now use the nginx example to assess the importance
of guiding symbolic execution to a vulnerability condi-
tion. For nginx, the input message is a generic HTTP re-
quest. Since it exercises the vulnerable loop for this anal-
ysis group, its uri starts with ”//”. Taint analysis allows
us to detect that only the uri ﬁeld is important, so we
mark only this ﬁeld as symbolic. As we shall see, with-
out guidance, symbolic execution does not scale beyond
very short uri ﬁelds (5-6 byte long). In contrast, Dowser
successfully executes 50-byte-long symbolic uris.
When S2E [10] executes a loop, it can follow one of
the two search strategies: depth-ﬁrst search, or maximiz-
ing code coverage (as proposed in SAGE [18]). The ﬁrst
one aims at complete path coverage, and the second at
executing basic blocks that were not seen before. How-
ever, none can be applied in practice to examine the com-
plex loop in nginx. The search is so costly that we mea-
sured the runtime for only 5-6 byte long symbolic uri
ﬁelds. The DFS strategy handled the 5-byte-long input
in 139 seconds, the 6-byte-long in 824 seconds. A 7-byte
input requires more than 1 hour to ﬁnish. Likewise, the
code coverage strategy required 159, and 882 seconds,
respectively. The code coverage heuristic does not speed
up the search for buffer overﬂows either, since besides
executing speciﬁc instructions from the loop, memory
corruptions require a very particular execution context.
Even if 100% code coverage is reached, they may stay
undetected.
As we explained in Section 5, the strategy employed
by Dowser does not aim at full coverage.
Instead, it
actively searches for paths which involve new pointer
dereferences. The learning phase uses a 4-byte-long
symbolic input to observe access patterns in the loop.
It follows a simple depth ﬁrst search strategy. As the
bug clearly cannot be triggered with this input size, the
search continues in the second, hunting bugs, phase. The
result of the learning phase disables 66% of the condi-
tional branches signiﬁcantly reducing the exponentially
of the subsequent symbolic execution. Because of this
heuristic, Dowser easily scales up to 50 symbolic bytes
and ﬁnds the bug after just a few minutes. A 5-byte-long
symbolic input is handled in 20 seconds, 10 bytes in 42
seconds, 20 bytes in 63 seconds, 30 in 146 seconds, 40
in 174 seconds and 50 in 253 seconds. These numbers
maintain an exponential growth of 1.1 for each added
character. Even though Dowser still exhibits the expo-
nential behavior, the growth rate is fairly low. Even in
the presence of 50 symbolic bytes, Dowser quickly ﬁnds
the complex bug.
In practice, symbolic execution has problems dealing
with real world applications and input sizes. The number
of execution paths quickly overwhelms these systems.
Since triggering buffer overﬂows not only requires a vul-
nerable basic block, but also a special context, traditional
symbolic execution tools are ill suited. Dowser, instead,
requires the application to be executed symbolically for
only a very short input, and then it deals with real-world
input sizes instead of being limited to a few input bytes.
Combined with the ability to extract the relevant parts of
the original input, this enables searching for bugs in ap-
plications like web servers where input sizes were con-
sidered until now to be well beyond the scalability of
symbolic execution tools.
6.2 Overview
In this section, we consider several applications. First,
we evaluate the dowsing mechanism, and we show that
it successfully highlights vulnerable code fragments.
Then, we summarize the memory corruptions detected
by Dowser. They come from six real world applications
of several tens of thousands LoC, including the ffmpeg
videoplayer of 300K LoC. The bug in ffmpeg, and one
of the bugs in poppler were not documented before.
6.2.1 Dowsing for candidate instructions
We now examine several aspects of the dowsing mecha-
nism. First, we show that there is a correlation between
Dowser’s scoring function and the existence of memory
corruption vulnerabilities. Then, we discuss how our fo-
cus on complex loops limits the search space, i.e., the
amount of analysis groups to be tested. We start with a
description of our data set.
Data set To evaluate the effectiveness of Dowser,
we chose six real world programs: nginx, ffmpeg,
58  22nd USENIX Security Symposium 
USENIX Association
10
Program
Vulnerability
Dowsing
Symbolic input
Symbolic execution
nginx 0.6.32
ffmpeg 0.5
CVE-2009-2629
heap underﬂow
UNKNOWN
heap overread
AG score
Loops LoC
4th out of 62/140
517 66k
630 points
URI ﬁeld
50 bytes
V-S2E M-S2E Dowser
> 8 h
253 sec
> 8 h
3rd out of 727/1419
1286 300k
Huffman table
> 8 h
> 8 h
48 sec
2186 points
224 bytes
inspircd 1.1.22
CVE-2012-1836
1st out of 66/176
1750 45k
DNS response
200 sec
200 sec
32 sec
poppler 0.15.0
heap overﬂow
UNKNOWN
heap overread
625 points
39th out of 388/904
1737 120k
1075 points
301 bytes
JPEG image
1024 bytes
> 8 h
> 8 h
14 sec
poppler 0.15.0
CVE-2010-3704
59th out of 388/904
1737 120k
Embedded font
> 8 h
> 8 h
762 sec
libexif 0.6.20
CVE-2012-2841
8th out of 15/31
121 10k
heap overﬂow
910 points
libexif 0.6.20
libexif 0.6.20
snort 2.4.0
heap overﬂow
CVE-2012-2840
off-by-one error
CVE-2012-2813
heap overﬂow
CVE-2005-3252
stack overﬂow
501 points
15th out of 15/31
121 10k
40 points
15th out of 15/31
121 10k
40 points
24th out of 60/174
616 75k
246 points
1024 bytes
EXIF tag/length
1024 + 4 bytes
EXIF tag/length
1024 + 4 bytes
EXIF tag/length
1024 + 4 bytes
UDP packet
1100 bytes
> 8 h
652 sec
652 sec
> 8 h
347 sec
347 sec
> 8 h
277 sec
277 sec
> 8 h
> 8 h
617 sec
Table 2: Applications tested with Dowser. The Dowsing section presents the results of Dowser’s ranking scheme. AG score is
the complexity of the vulnerable analysis group - its position among other analysis groups; X/Y denotes all analysis groups that are
”complex enough” to be potentially analyzed/all analysis groups which access arrays; and the number of points it scores. Loops
counts outermost loops in the whole program, and LoC - the lines of code according to sloccount. Symbolic input speciﬁes how
many and which parts of the input were determined to be marked as symbolic by the ﬁrst two components of Dowser. The last
section shows symbolic execution times until revealing the bug. Almost all applications proved to be too complex for the vanilla
version of S2E (V-S2E). Magic S2E (M-S2E) is the time S2E takes to ﬁnd the bug when we feed it with an input with only a minimal
symbolic part (as identiﬁed in Symbolic input). Finally, the last column is the execution time of fully-ﬂedged Dowser.
inspircd, libexif, poppler, and snort. Addition-
ally, we consider the vulnerabilities in sendmail tested
by Zitser et al. [45]. For these applications, we analyzed
all buffer overﬂows reported in CVE [26] since 2009. For
ffmpeg, rather than include all possible codecs, we just
picked the ones for which we had test cases. Out of 27
CVE reports, we took 17 for the evaluation. The remain-
ing ten vulnerabilities are out of the scope of this paper –
nine of them are related to an erroneous usage of a cor-
rect function, e.g., strcpy, and one was not in a loop. In
this section, we consider the analysis groups from all the
applications together, giving us over 3000 samples, 17 of
which are known to be vulnerable4.
When evaluating Dowser’s scoring mechanism, we
also compare it to a straightforward scoring function that
treats all instructions uniformly. For each array access, it
considers exactly the same AGs as Dowser. However, in-
stead of the scoring algorithm (Table 1), each instruction
gets 10 points. We will refer to this metric as count.
Correlation For both Dowser’s and the count scor-
ing functions, we computed the correlation between the
number of points assigned to an analysis group and the
existence of a memory corruption vulnerability. We used
4Since the scoring functions are application agnostic, it is sound to
compare their results across applications.
the Spearman rank correlation [2], since it is a reliable
measure that is appropriate even when we do not know
the probability distribution of the variables, or when the
association between the variables is non-linear.
The positive correlation for Dowser is statistically sig-
niﬁcant at p < 0.0001, for count — at p < 0.005. The
correlation for Dowser is stronger.
Dowsing The Dowsing columns of Table 2 shows that
our focus on complex loops limits the search space from
thousands of LoC to hundreds of loops, and ﬁnally to a
small number of “interesting” analysis groups. Observe
that ffmpeg has more analysis groups than loops. That
is correct. If a loop accesses multiple arrays, it contains
multiple analysis groups.
By limiting the analysis to complex cases, we focus
on a smaller fraction of all AGs in the program, e.g., we
consider 36.9% of all the analysis groups in inspircd,
and 34.5% in snort. ffmpeg, on the other hand, con-
tains lots of complex loops that decode videos, so we also
observe many “complex” analysis groups.
In practice, symbolic execution, guided or not is ex-
pensive, and we can hardly afford a thorough analysis of
more than just a small fraction of the target AGs of an ap-
plication, say 20%-30%. For this reason, Dowser uses a
scoring function, and tests the analysis groups in order of
USENIX Association  
22nd USENIX Security Symposium  59
11
t
d
e
t
c
e
e
d
s
g
u
b
100
80
60
40
20
0
f
o
%
Dowser
Count
Random
0
20
40
60
% of analysis groups analyzed
80
100
Fig. 6: A comparison of random testing and two scoring func-
tions: Dowser’s and count. It illustrates how many bugs we
detect if we test a particular fraction of the analysis groups.
decreasing score. Speciﬁcally, Dowser looks at complex-
ity. However, alternative heuristics are also possible. For
instance, one may count the instructions that inﬂuence
array accesses in an AG. To evaluate whether Dowser’s
heuristics are useful, we compare how many bugs we dis-
cover if we examine increasing fractions of all AGs, in
descending order of the score. So, we determine how
many of the bugs we ﬁnd if we explore the top 10% of
all AGs, how many bugs we ﬁnd when we explore the
top 20%, and so on. In our evaluation, we are comparing
the following ranking functions: (1) Dowser’s complex-
ity metric, (2) counting instructions as described above,
and (3) random.
Figure 6 illustrates the results. The random ranking
serves as a baseline—clearly both count and Dowser
perform better. In order to detect all 17 bugs, Dowser
has to analyze 92.2% of all the analysis groups. How-
ever, even with just 15% of the targets, we ﬁnd almost
80% (13/17) of all the bugs. At that same fraction of
targets, count ﬁnds a little over 40% of the bugs (7/17).
Overall, Dowser outperforms count beyond the 10% in
the ranking. It also reaches the 100% bug score earlier
than the alternatives, although the difference is minimal.
The reason why Dowser still requires 92% of the AGs
to ﬁnd all bugs, is that some of the bugs were very sim-
ple. The “simplest” cases include a trivial buffer over-
ﬂow in poppler (worth 16 points), and two vulnera-
bilities in sendmail from 1999 (worth 20 points each).
Since Dowser is designed to prioritize complex array ac-
cesses, these buffer overﬂows end up in the low scoring
group. (The “simple” analysis groups – with less than 26
points – start at 47.9%). Clearly, both heuristics provide
much better results than random sampling. Except for
the tail, they ﬁnd the bugs signiﬁcantly quicker, which
proves their usefulness.
To summarize, we have shown that a testing strategy
based on Dowser’s scoring function is effective. It lets
us ﬁnd vulnerabilities quicker than random testing or a
scoring function based on the length of an analysis group.
6.2.2 Symbolic execution
Table 2 presents attacks detected by Dowser. The last
section shows how long it takes before symbolic execu-
tion detects the bug. Since the vanilla version of S2E
cannot handle these applications with the whole input
marked as symbolic, we also run the experiments with
minimal symbolic inputs (“Magic S2E”). It represents
the best-case scenario when an all-knowing oracle tells
the execution engine exactly which bytes it should make
symbolic. Finally, we present Dowser’s execution times.
We run S2E for as short a time as possible, e.g., a
single request/response in nginx and transcoding a sin-
gle frame in ffmpeg. Still, in most applications, vanilla
S2E fails to ﬁnd bugs in a reasonable amount of time.
inspircd is an exception, but in this case we explic-
itly tested the vulnerable DNS resolver only. In the case
of libexif, we can see no difference between “Magic
S2E” and Dowser, so Dowser’s guidance did not inﬂu-