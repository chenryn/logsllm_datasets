34.91
1 . 66
E58256°6t
cksun
20405
sdb
R. 250175
08827T
15.53
65* 8
[. ..]
---
## Page 397
360
Chapter 9 Disk I/0
The queued time is shown in the QUE(ms) column. This example of high queue times for reads
was from a USB flash drive using the CFQ I/O scheduler, Write I/O queues even more:
+ biosnoop -Q
TIHE (s)
COMH
PID
DISK
T SECTOR
BYTES
QUE (ns) LAT (ns)
[..]
2.338149
0
  0
8192
0. 00
2.72
2,354710
？
 0
122880
0. 00
16 .17
2, 371236
kxorker/u16:1
18754
sdb
W486703
122880 2070.06
16.51
2,387687
cp
20631
nvme0n1 R 73365192
2 62144
0.01
3.23
2.389213
kxorker/u16:1
18754
sdb
V486943
122880 2086.60
17.92
2, 404042
kxorker/u16:1
18754
sdb
V 487183
122880 2119,40
122880 2104.53
14.81
2,421539
kxorker/u16:1
18754
sdb
V 487423
17.43
[...]
The queue time for writes exceeds two seconds. Note that earlier I/O lacked most of the column
details: they were enqueued before tracing began, and so biosnoop(8) missed caching those
details and only shows the device latency.
BCC
Command line usage:
biosnoop [options]
Options include 0 for OS queued time.
bpftrace
on, which tra
ces the full duration of the I/O,
including queued time:
#!/usx /1ocal/b1n/bp trace
BEGIX
printf (*s12s 416s =6s 7s\n", *TINE (ns]*, *CONx*, "PID*, *LAT (ms)") ≠
Bstart[arg0] = nsecs7
Biopld[arg0] = pld;
Biocomn[arg0] = comng
---
## Page 398
9.3  BPF Tools
361
kprobe:blk_account_io_done
/Bstaxt [arg0] != 0 ss@iopid[axg0] != 0 &s iocomn [arg0] !="*/
Snow = nsecs;
printf (*=12u 16s 3=6d 7d*,
elapsed / 10ooooo, @ioconn[arg0], fiopid[azg0],
($nov - Bstart[arg0]) / 1000000] =
delete (estart[arg0]1
delete (eiopid[azg0]1
delete (6iocorn [arg0|1
EXD
1
clear (8start) 
clear (8iopid) 
clear (8loconm) ≠
The blk_account_io_start() function often fires in process context and occurs when the I/O is
queued. Later events, such as issuing the I/O to the device and I/O completion, may or may not
happen in process context, so you cannot rely on the value of the pid and comm builtins at those
later times. The solution is to store them in BPF maps during blk_account_io_start), keyed by the
request ID, so that they can be retrieved later.
As with biolatency(8), this tool can be rewritten to use the block tracepoints (see Section 9.5).
9.3.3 biotop
biotop(8)* is a BCC tool that is top(1) for disks. The following shows it running on a production
Hadoop instance, with -C to not clear the screen between updates:
 biotop -C
Tracing... Output every 1 secs. Hit Ctrl-C to end
06:09:47 1oadavg: 28.40 29.00 28.96 44/3812 124008
PID
COMX
D KAJ HIN DISK
I/0  Kbytes  AVGns
123693 kxorker/u258:0
BpAx 960+ Z02 
1979
86148
0.93
55024kwozkez/u25T:8
spnx 809b Z0Z N
1480
64068
Apsx 9LCS 707 8
0 ,T3
123693 kvorker/u258:0
143
5700
0,52
5381
Java
R 202 176
xvd1
B1
345 6
3.01
43297kxorker/u257:0
M 202 80
xvdf
4 8
1996
0,56
4 Origin: I created the first iotop using DTrace on 15-Jul-2005, and wrote this BCC version 6-Feb-2016. These were
inspired by top(1)by Wiliam Lefetvre.
---
## Page 399
362
Chapter 9 Disk I/0
5383
Javs
qpAxZI 202 8
27
115216.05
5383
Java
xpAX 2995 202 8
27
1152
3.45
5383
Java
opax2 22 8
27
1152
6, 79
5383
ese[
R 202 96
xvdg
24
1024
0.52
5383
Javs
wp.x261 207 8
R 202 5888 xvdx
24
1024
39.45
5383
ese(
24
1024
0.64
5383
Javs
ApAx 9405 207 8
R 202 4096 xvdq
24
1024
4, 74
5383
ese[
24
1024
3.07
5383
Javs
R 202 48
FpAx
24
1024
0, 62
5383
Java
R 202 5120 1
 xvdu
24
1024
4.20
5383
Javs
R 202 208
upAx
24
1024
2,54
5383
Java
R 202 80
xvdf
24
1024
0.66
5383
Javs
R 202 64
xvde
24
1024
8.08
5383
Java
R 202 32
xvdc
24
1024
0 . 63
5383
Javs
R 202 160
xpAx
24
1024
1. 42
[..-]
This shows that a Java process is reading from many different disks. Top of the list are kworker
threads initiating writes: this is background write flushing, and the real proces that dirtied the
pages is not known at this point (it can be identified using the file system tools from Chapter 8).
This works using the same events as biolatency(8), with similar overhead expectations.
Command line usage:
[ [unoo]  tenzsut  [euodo]  doota
Options include:
 -C: Don’t clear the screen
· -r Rons: Number of rows to print
The output is truncated to 20 rows by default, which can be tuned with r.
9.3.4 bitesize
bitesize(8)? is a BCC and bpftrace tool to show the size of disk I/O. The following shows the BCC
:aouesui doopeg uoonpond e uo Suquunu uorsian
+ bitesize
Tracing...
H1t Ctx1-C to end.
^C
[..-]
5 Origin: 1 frst created this as bite
McAleavy created the BCC version on 5-feb-2016, and I crested the bpftrsce one on 7-Sep-2018.
---
## Page 400
9.3 BPF Tools
363
Process Kame = kvorker/u257:10
Kbytes
: count
distributlon
{ -> 1
: 0
E  7
: 17
B => 15
: 12
16 > 31
: 79
∈9  1
: 0
2 -> 3
: 3
 -> 7
: 60
8 -> 15
: 68
16 -> 31
: 220
| * +
32 -> 63
: 3996
This output shows that both the kworker thread and java are calling I/O mostly in the 32- to
63-Kbyte range. Checking the I/O size can lead to optimizations:
■ Sequential workloads should try the largest possible I/O size for peak performance.
Larger sizes sometimes encounter slightly worse performance; there may be a sweet spot
(e.g., 128 Kbytes) based on memory allocators and device logic.
● Random workloads should try to match the I/O size with the application record size. Larger
I/O sizes pollute the page cache with data that isn’t needed; smaller I/O sizes result in more
I/O overhead than needed.
This works by instrumenting the block:block_rq_issue tracepoint.
BCC
bitesize(8) currently does not support options.
bpftrace
The following is the code for the bpftrace version:
+1/usx/local/bin/bpftrace
BEGIN
printf(*Tracing block device I/0.., Hit CtrlC to end.n*) 
---
## Page 401
364
Chapter 9 Disk I/0
tracepoint:block:block_rq_issue
B [args->conn]
END
1
f(teueu sseooad Aq sueabosTu (se/g) ezTs 0/u\)autad
The tracepoint provides the process name as args->comm, and the size as args->bytes. This insert
tracepoint fires when the request is inserted on the OS queue. Later tracepoints such as comple
tion do not provide args->comm, nor can the comm builtin be used, as they fire asynchronously
to the process (e.g., on device completion interrupt).
9.3.5seeksize
seeksize(8)° is a bpftrace tool to show how many sectors that processes are requesting the disks
to seek. This is only a problem for rotational magnetic media,? where the drive heads must
physically move from one sector offset to another, causing latency. Example output:
+ seeksize.bt
Attach.ing 3 pzobes.
Tracing block I/0
C
[. .-]
: [xe]s30q,09s@
[0]
82:20
1eee88e8eee8e 88eeeee8ee8eeeee88e8eee8e 88eeeee8ee8ee8e1
[1]
01
[2, 4]
[4, 8]
0 1
[8, 16}
882 188988
[16,32)
1897 1889889889888
[32, 64)
1588 18898898
[64, 128]
15021889889889
[128, 256)
1105 188e889
[256, 512}
73418898
6 Origin: 1 first created it as seeksize.d using DTrace on 11-Sep2004, ss seek issues on rotational disks were
common at the time. I crested the bpftrace version it for a blog post on 18-0ct-2018 and revised it for this book on
20-Mar-2019.
seeking across large ranges vs small: perhaps it's lbusting the flash equivalent of a TLB.
---
## Page 402
9.3 BPF Tools
365
[512, 1K)
5011869
[1K, 2K]
30218
[2K, 4K)
19418
[4K, 8K)
82 1
[8K, 16K)
0 1
[16x, 32K]
D 1
[32K, 64K]
61
[643K, 128K)