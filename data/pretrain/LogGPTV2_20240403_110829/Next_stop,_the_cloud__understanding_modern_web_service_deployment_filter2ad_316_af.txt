CDNs) has been previously studied in other settings [27], we are
unaware of any studies that assess its impact for the available di-
versity of modern public IaaS clouds. We therefore perform mea-
surements to help us answer the following two questions: (i) To
what extent does the choice of region impact performance experi-
enced by clients of a web service? (ii) To what extent does the use
Figure 9: Average throughput between representative clients and
EC2 regions in the US.
Figure 10: Average latency between representative clients and EC2
regions in the US.
of multiple regions (or zones) improve the client-perceived perfor-
mance?
Latency measurements. To study per-region latency performance,
we set up 40 m1.medium instances, 2 in each of the 20 availability
zones available to us on EC2. We selected 80 geographically dis-
tributed PlanetLab [17] nodes as stand-ins for real clients and we
used the hping3 utility to conduct 5 TCP pings to each of the 40 in-
stances, from which we derive the average RTT. Pings that timed
out were excluded from the calculations of the average. Probing
was performed once every 15 minutes for three consecutive days.
Throughput measurements. We used the same set of 40 m1.med-
ium EC2 instances and 80 PlanetLab nodes to measure throughput.
We divided the PlanetLab nodes into two groups of 40. Each node
in each group performed an HTTP get of a 2 MB ﬁle to one of the
40 EC2 instances (which were running Apache web server). At any
given time, only one HTTP connection was established with each
EC2 instance to avoid contention across throughput measurements.
In particular, the clients in each group performed an HTTP get op-
eration every 11.25 seconds; the download was canceled if it took
more than 10 seconds. Each client accessed all 40 servers in each
round, which means it took 450 seconds for one group to ﬁnish a
round of downloading the ﬁle from each of the servers. So, in total,
it took 15 minutes for 80 clients to perform one round of through-
put measurements. The ﬁnal throughput is measured as ﬁle_size
/ download_time. We ran the measurements for three consecutive
days, for a total of 288 data points per client. The throughput mea-
surements were intermingled with the latency measurements.
Performance across different regions. Figures 9 and 10 show the
latency and throughput measurements for 15 representative Plan-
etLab locations and for the three US EC2 regions. The PlanetLab
nodes are spread across the US and other parts of the world. We
make a few key observations: (i) Single-region deployments must
carefully choose a region. For example, the two US west regions do
not offer equivalent “across-the-board” performance, with ec2.us-
west-1 offering better average latency and throughput (130 ms and
1143 KB/s) than ec2.us-west-2 (145 ms and 895 KB/s) (averaged
across all client locations).
(ii) The charts show that the region
s
m
s
m
s
m
 600
 400
 200
 0
 600
 400
 200
 0
 600
 400
 200
 0
east-1
west-1
west-2
10
20
30
40
50
60
70 (hrs)
Figure 11: Latencies between Boulder site and three EC2 US re-
gions. The best performing region changes over time.
chosen to serve content to a given client can have a signiﬁcant per-
formance impact. For example, for the client in Seattle, using the
ec2.us-west-2 region can reduce latency by close to a factor of 6
and improve throughput by close to a factor of 5 compared to us-
ing ec2.us-east-1. (iii) We also note that the way a region is chosen
may depend on the client’s location: always choosing ec2.us-west-
1 for the Seattle client is a good idea, but for the client in Boulder,
the best choice of region may change dynamically (see Figure 11).
We now examine the relative beneﬁts of, and choices underlying,
multi-region deployments in more detail. We start by deriving an
upper bound on the performance from a k-region deployment of a
service. To do this, we determine the best k regions out of the 8, for
1 ≤ k ≤ 8. Using our measurement data we determine the overall
performance that the clients would have achieved given a routing
algorithm that picked the optimal region from the k for each client
and at each point in time. More speciﬁcally, for each value of k we:
(i) enumerate all size-k subsets of regions; (ii) for each size-k sub-
set compute the average performance across all clients assuming
each client uses the lowest latency or highest throughput region of
the k at each time round (15 min); and (iii) choose from these the
size-k subset with lowest latency or highest throughput. Figure 12
shows the results. We ﬁnd that while average performance can be
increased a signiﬁcant amount by adding more regions to one’s de-
ployment, there is evidence of diminishing returns after k = 3.
In particular, latency decreases by 33% when k = 3 compared to
k = 1, but only decreases by 39% when k = 4.
We now examine what constitutes the best k regions to use. The
choice of best regions, by throughput, is as follows: ec2.us-east-
1 (k = 1); ec2.us-east-1,ec2.eu-west-1 (k = 2); ec2.us-east-1,
ec2.eu-west-1, ec2.us-west-1 (k = 3); and ec2.us-east-1, ec2.eu-
west-1, ec2.us-west-1, ec2.ap-southeast-1 (k = 4). The choice
of best regions, by latency, is: ec2.us-east-1 (k = 1); ec2.us-
east-1,ec2.ap-northeast-1 (k = 2); ec2.us-east-1, ec2.ap-northeast-
1, ec2.us-west-1 (k = 3); and ec2.us-east-1, ec2.ap-northeast-1,
ec2.us-west-1, ec2.ap-southeast-1 (k = 4).
Performance across different zones. We also investigated the dif-
ference in performance should one use different zones in the same
region. We found that the zone has little impact on latency, with
almost equivalent average RTTs for all clients across the two days
(results omitted for brevity). For throughput, the variation appears
to be somewhat higher, but not as signiﬁcant as that seen across
regions. We believe such variation is due to local effects, such as
contention on shared instances or network switches. This is sug-
gested as well by other recent measurements of EC2 performance
variability [25]. Moreover, in the next section we show that the
Internet path variability between zones is low as well.
Summary and Implications. We ﬁnd that using multiple regions
can improve latency and throughput signiﬁcantly. However, lever-
aging multiple regions may not be easy: while a given region al-
ways offers best performance for some clients, the choice of region
for other clients will have to adapt in an online dynamic fashion.
(a) Latency
(b) Throughput
Figure 12: Average latency/throughput across all clients using an
optimal k-region deployment.
Region
ec2.us-east-1
ec2.us-west-1
ec2.us-west-2
ec2.eu-west-1
ec2.ap-northeast-1
ec2.ap-southeast-1
ec2.ap-southeast-2
ec2.sa-east-1
AZ1 AZ2 AZ3
36
34
n/a
18
19
19
13
10
9
9
11
n/a
n/a
4
4
n/a
36
19
19
11
n/a
12
4
4
Table 16: Number of downstream ISPs for each EC2 region and
zone.
This could be achieved via global request scheduling (effective, but
complex) or requesting from multiple regions in parallel (simple,
but increases server load).
While a multiple region deployment helps improve web service
performance and protects against major cloud failures, cloud ten-
ants must also consider other factors in making their decision of
how many and which regions to use. First, cloud providers charge
for inter-region network trafﬁc, potentially causing tenants to incur
additional charges when switching to a multi-region deployment.
Second, the design of particular cloud features may restrict how a
tenant’s data can be shared across regions: e.g., objects stored in
Amazon’s Simple Storage Service (S3) can only be stored in one
region at a time and Amazon Machine Images (AMIs) cannot be
shared between regions. Lastly, deployments that rely on fewer fea-
tures may be less susceptible to failures—e.g., deployments which
only use VMs, and not other services like Amazon Elastic Block
Storage or Amazon ELB, have not been affected by some major
outages [4, 6]—reducing the need for resiliency through the use of
multiple regions.
5.2 ISP Diversity
We now investigate tolerance to wide-area faults. Having in pre-
vious sections already established the reliance of many cloud-using
services on one zone or region, we now focus on diversity in the im-
mediate downstream ISPs at each EC2 zone. Greater diversity, and
an even spread of routes across downstream ISPs, generally indi-
cates greater tolerance to failures in Internet routing.
To do this study, we set up three m1.medium instances in each
of the available EC2 availability zones. Then we ran traceroute 50
times from each of these instances to each of 200 geographically
diverse PlanetLab nodes (Figure 2). Finally, we used the UNIX
‘whois’ utility to determine the autonomous system (AS) number
associated with the ﬁrst non-EC2 hop, and we count that AS as
an immediate downstream ISP for the zone hosting the instance.
The discovered ASs constitute a lower bound for the true number
of ASs. Table 16 gives the number of distinct ASs seen for each
zone and region. We note that: (i) different zones in a region have
(almost) the same number of downstream ISPs; and (ii) the extent
of diversity varies across regions, with some connected to more
than 30 downstream ISPs and others connected to just 4. Except
for South America and Asia Paciﬁc Sydney, other regions are well-
multihomed.
We also studied the spread of routes across downstream ISPs
(not shown). We found it to be rather uneven: even when using
well-connected regions – e.g., ec2.us-west-1 and ec2.eu-west-1–
we found that up to 31% (ec2.us-west-1) and 33% (ec2.eu-west-1)
of routes use the same downstream ISP.
Summary and Implications. Although individual regions are mul-
tihomed, the uneven spread of routes implies that local failures in
downstream ISPs can cause availability problems for large fractions
of clients of cloud-using web services. This could be overcome by
using multiple regions at once, or by leveraging dynamic route con-
trol solutions [20].
6. RELATED WORK
There have been several studies of the network characteristics
of data centers (DCs). For example, Kandula et al. [26] focus on
measuring the ﬂow level characteristics. Benson et. al [21] stud-
ied three types of DCs: universities, private enterprise and pub-
lic cloud. They characterize the internal network trafﬁc, and infer
the applications deployed in the DCs as well. Similarly, Mishra
et al. [32] characterize cloud backend workloads inside Google
compute clusters. Li et al. [29] benchmark and compare the com-
puting and storage service’s performance across different public
cloud providers. They also measure the network bandwidth be-
tween DCs. In roughly the same vein, Chen et al. [22] study the
wide-area trafﬁc patterns between Yahoo! DCs. Our work, in con-
trast, is more “user-facing” than the above, in that we focus on char-
acterizing deployment patterns of the front ends of cloud-using web
services, the wide-area trafﬁc they impose, and the implications of
these properties on client perceived performance and availability.
Our work therefore extends prior works on DC and cloud workload
characterization along important new axes.
7. CONCLUSION
In this work we performed the ﬁrst extensive measurement study
of usage of modern infrastructure-as-a-service (IaaS) clouds, in
particular Amazon EC2 and Windows Azure. This measurement
study, which combined data from a university packet capture, in-
terrogation of DNS records for websites listed on the Alexa top
1 million list, and lightweight probing, conﬁrms the oft-reported
anecdotes about the extent to which modern web services use these
cloud systems. We proﬁle the deployment patterns observed for
popular web services. We uncover that, in many ways, these de-
ployments are somewhat precarious, as the vast majority of (even
very popular) websites use only one cloud region. In addition to
resiliency beneﬁts, we show that multi-region deployments could
signiﬁcantly increase latency and throughput performance in EC2.
Beyond providing a snapshot on the state of cloud usage, we be-
lieve our work will spark further research on tracking cloud usage,
as well as developing cloud-region-aware routing and deployment
mechanisms. Finally, we make all data sets used in this paper pub-
licly available [10], with the exception of the packet capture.
8. ACKNOWLEDGMENTS
We would like to thank Dale Carder, Michael Hare and Mark
Tinberg from the Department of Information Technology at the of
University of Wisconsin-Madison for helping capture packet traces.
We would also like to thank our shepherd and the anonymous re-
viewers for their insightful feedback. This work is supported by
NSF awards 1040757 and 1065134.
9. REFERENCES
[1] Alexa. http://alexa.com/topsites.
[2] Alexa Web Information Service.
http://aws.amazon.com/awis/.
[3] Amazon CloudFront.
http://aws.amazon.com/cloudfront.
[4] Amazon ELB Service Event in the US-East Region.
http://aws.amazon.com/message/680587/.
[5] AWS Elastic Beanstalk.
http://aws.amazon.com/elasticbeanstalk.
[6] AWS Service Event in the US-East Region.
https://aws.amazon.com/message/680342/.
[7] Azure CDN. http://msdn.microsoft.com/en-
us/library/windowsazure/ee795176.aspx.
[8] Azure Datacenter IP Ranges.
http://microsoft.com/en-
us/download/details.aspx?id=29840.
[9] Azure TM. http://msdn.microsoft.com/en-
us/library/windowsazure/hh744833.aspx.
[10] Cloud Measurement Data. http://pages.cs.wisc.edu/
~keqhe/cloudmeasure_datasets.html.
[11] Cloudﬂare CDN. http://cloudflare.com.
[12] EC2 public IP ranges. https:
//forums.aws.amazon.com/ann.jspa?annID=1528.
[13] ELB.
http://aws.amazon.com/elasticloadbalancing.
[14] Heroku. http://heroku.com.
[15] Knock. http://code.google.com/p/knock.
[16] Passive DNS Network Mapper.
http://code.google.com/p/dnsmap/.
[17] PlanetLab. http://planet-lab.org/.
[18] B. Agarwal, A. Akella, A. Anand, A. Balachandran,
P. Chitnis, C. Muthukrishnan, R. Ramjee, and G. Varghese.
EndRE: An End-System Redundancy Elimination Service
for Enterprises. In NSDI, pages 419–432, 2010.
[19] A. Akella, S. Seshan, and A. Shaikh. An Empirical
Evaluation of Wide-area Internet Bottlenecks. In
Proceedings of the 3rd ACM SIGCOMM conference on
Internet measurement, pages 101–114. ACM, 2003.
[20] A. Akella, S. Seshan, and A. Shaikh. Multihoming
Performance Beneﬁts: An Experimental Evaluation of
Practical Enterprise Strategies. In USENIX Annual Technical
Conference, General Track, pages 113–126, 2004.
[21] T. Benson, A. Akella, and D. A. Maltz. Network Trafﬁc
Characteristics of Data Centers in the Wild. In Proceedings
of the 10th ACM SIGCOMM conference on Internet
measurement, pages 267–280. ACM, 2010.
[22] Y. Chen, S. Jain, V. K. Adhikari, Z.-L. Zhang, and K. Xu. A
First Look at Inter-Data Center Trafﬁc Characteristics via
Yahoo! Datasets. In INFOCOM, 2011 Proceedings IEEE,
pages 1620–1628. IEEE, 2011.
[23] I. Drago, M. Mellia, M. M Munafo, A. Sperotto, R. Sadre,
and A. Pras. Inside Dropbox: Understanding Personal Cloud
Storage Services. In Proceedings of the 2012 ACM
conference on Internet measurement conference, pages
481–494. ACM, 2012.
[24] J. Edberg. Post-Mortem of October 22, 2012 AWS
Degradation.
http://techblog.netflix.com/2012/10/post-
mortem-of-october-222012-aws.html.
[25] B. Farley, A. Juels, V. Varadarajan, T. Ristenpart, K. D.
Bowers, and M. M. Swift. More for Your Money: Exploiting
Performance Heterogeneity in Public Clouds. In Proceedings
of the Third ACM Symposium on Cloud Computing, page 20.
ACM, 2012.
[26] S. Kandula, S. Sengupta, A. Greenberg, P. Patel, and
R. Chaiken. The Nature of Data Center Trafﬁc:
Measurements & Analysis. In Proceedings of the 9th ACM
SIGCOMM conference on Internet measurement conference,
pages 202–208. ACM, 2009.
[27] R. Krishnan, H. V. Madhyastha, S. Srinivasan, S. Jain,
A. Krishnamurthy, T. Anderson, and J. Gao. Moving beyond
End-to-End Path Information to Optimize CDN
Performance. In Proceedings of the 9th ACM SIGCOMM
conference on Internet measurement conference, pages
190–201. ACM, 2009.
[28] C. Labovitz. How Big is Amazon’s Cloud?
http://deepfield.net/blog, April 2012.
[29] A. Li, X. Yang, S. Kandula, and M. Zhang. CloudCmp:
Comparing Public Cloud Providers. In Proceedings of the
10th ACM SIGCOMM conference on Internet measurement,
pages 1–14. ACM, 2010.
[30] A. Li, X. Zong, S. Kandula, X. Yang, and M. Zhang.
CloudProphet: Towards Application Performance Prediction
in Cloud. In ACM SIGCOMM Computer Communication
Review, volume 41, pages 426–427. ACM, 2011.
[31] R. McMillan. Amazon’s Secretive Cloud Carries 1 Percent of
the Internet. http://www.wired.com/
wiredenterprise/2012/04/amazon-cloud/, 2012.
[32] A. K. Mishra, J. L. Hellerstein, W. Cirne, and C. R. Das.
Towards Characterizing Cloud Backend Workloads: Insights
from Google Compute Clusters. ACM SIGMETRICS
Performance Evaluation Review, 37(4):34–41, 2010.
[33] V. Paxson. Bro: A System for Detecting Network Intruders in
Real-Time. In USENIX Security Symposium (SSYM), 1998.
[34] T. Ristenpart, E. Tromer, H. Shacham, and S. Savage. Hey,
You, Get Off of My Cloud: Exploring Information Leakage
in Third-party Compute Clouds. In Proceedings of the 16th
ACM conference on Computer and communications security,
pages 199–212. ACM, 2009.
[35] J. Schectman. Netﬂix Amazon Outage Shows Any Company
Can Fail.
http://blogs.wsj.com/cio/2012/12/27/netflix-
amazon-outage-shows-any-company-can-fail/,
2012.
[36] R. Teixeira, A. Shaikh, T. Grifﬁn, and J. Rexford. Dynamics
of Hot-potato Routing in IP Networks. In ACM
SIGMETRICS Performance Evaluation Review, volume 32,
pages 307–319. ACM, 2004.