with Dr eal . Finally, both the generative model and the synthetic
traces Dsyn should be robust against inference attacks, in order to
strengthen privacy by maximizing attackers’ probability of error in
estimating the true behavior of users over time.
We design and develop AdaTrace, a scalable and utility-aware
trajectory synthesizer with differential privacy and attack resilience.
To demonstrate its feasibility and effectiveness, we compare Ada-
Trace with existing state of the art mechanisms on both real and
simulated location trace datasets [9, 38, 56]. AdaTrace methodically
unites statistical privacy (e.g., differential privacy) and syntactic
privacy (e.g., attack resilience). It maximizes privacy by mitigating
location inference attacks, and minimizes utility loss by extracting
and upholding many useful types of statistical and spatial features
of real location traces throughout each phase of synthesis.
Figure 1 illustrates the system architecture of AdaTrace. First,
AdaTrace analyzes Dr eal , the database of real location traces, to
extract four spatial and statistical features and establish them in
its private synopsis. The features include a density-aware grid to
capture the complex spatial density and locality distributions; a
Markov mobility model to capture intra-trace mobility patterns; a
trip distribution to learn the correlations between trips’ pickup and
destination locations; and a length distribution to capture travel
distances and durations. Each of the four features are perturbed
by controlled noise under differential privacy and attack resilience
constraints. Although several mechanisms can be used to add noise
during each feature extraction and perturbation phase, in AdaTrace
we carefully choose the mechanisms such that: (i) we find a balance
in the trade-off between privacy, attack resilience and utility, and (ii)
we maximize utility under the same level of privacy and robustness.
3.2 Differential Privacy
We adapt the well-known notion of differential privacy to achieve
statistical privacy in AdaTrace. We enforce differential privacy
with respect to the complete location trace of an individual. It was
recently argued that such use of the differential privacy notion can
combat membership inference attacks relevant in machine learning
as well as aggregate location statistics [42, 45].
For an actual database Dr eal , let nbrs(Dr eal) denote the set of
databases neighboring Dr eal such that for all D′
r eal ∈ nbrs(Dr eal),
it holds that (Dr eal − D′
r eal − Dr eal) = {T}, where
T denotes one user’s trajectory. Further, let A be a probabilistic
algorithm, ε be the privacy parameter, and Range(A) denote the set
of A’s possible outcomes. A is said to be ε-differentially private
[18], if for all Dr eal and D′
r eal ∈ nbrs(Dr eal), and for all possible
r eal) ∪ (D′
budget or deteriorate privacy. For example, modifying the output
of A1 without accessing Dr eal , and releasing the modified version
still consumes ε1.
3.3 Privacy Threat Model and Attacks
Differential privacy is regarded as the de facto standard for privacy-
preserving data sharing due to its provable privacy guarantees.
However, like any other security mechanism, the resilience of
generic differential privacy against targeted, syntactic attacks has
recently been criticized [13, 14, 21, 28]. We categorize these attacks
broadly into three types of threats which are highly relevant in shar-
ing sanitized location traces, but are beyond the scope of protection
offered by differential privacy. We describe AdaTrace’s threat model
with respect to each attack and discuss why differential privacy is
not sufficient to solve the attacks.
Bayesian Inference Threat. Certain geographic zones (regions)
are sensitive by nature, e.g., hospitals, schools, health clinics. Let
Z denote a privacy sensitive zone. An adversary may have a prior
belief B(Z) regarding the users who visit these zones, e.g., where
they live, work, or which other locations they frequently visit.
We allow informed and uninformed priors, but by default address
stronger adversaries with informed priors, e.g., knowledge obtained
from publicly available information such as census records and
population averages. For example, if 10% of the general population
lives in a certain neighborhood, then B(Z) may assume that 10%
of the users who visit the hospital also live in that neighborhood.
The adversary formulates a posterior belief after observing the
synthesized trajectories, denoted by B(Z|Dsyn). If the difference
between B(Z) and B(Z|Dsyn) is significant, we assert that there is
a privacy leakage. For example, the adversary observes from Dsyn
that 50% of the hospital’s visitors live in a certain neighborhood
rather than the assumed prior of 10%, yielding the inference that
there is a flu outbreak in that neighborhood. Differential privacy is
not sufficient to prevent this threat, because despite its noise injec-
tion, priors and posteriors will be related. For example, if indeed
50% of the hospital’s visitors come from a certain neighborhood in
Dr eal , after Laplace noise we may have a 48% visiting rate from
the same neighborhood in Dsyn. This is still significantly higher
than the 10% prior. A proper defense should ensure that the differ-
ence between B(Z) and B(Z|Dsyn) is sufficiently small, so that an
adversary’s inference power is crippled.
Partial (Subtrajectory) Sniffing Threat. Assume that an adver-
sary can track users when they are in certain regions, such as gro-
cery stores or airports. We call these regions sniff regions. Tracking
in sniff regions can be enabled via surveillance cameras, biomet-
ric scanners, or simply by following users’ location check-ins on
a social network. The sniffed locations constitute a subtrajectory
(i.e., portion) of the user’s full trajectory. Armed with subtrajectory
knowledge, if the adversary can infer which trajectory belongs
to the user with high probability, the adversary can learn the re-
maining locations visited by the user outside the sniff region, such
as home and work locations. Let u denote the sniffed user and
Tu ∈ Dr eal denote her trajectory. Even when Dsyn is shared in
place of Dr eal , an adversary can launch this attack by linking the
subtrajectory Ts ∈ Dsyn which is geographically most similar to
Tu in the sniff region. This inference process is illustrated in Figure
Figure 2: User’s actual trace Tu drawn in black, synthetic
traces passing through the sniff region drawn in red. Syn-
thetic #1 is closest to Tu within the sniff region, hence it is
labeled Ts.
2. The threat is especially acute if Ts and Tu share points in one or
more sensitive zone.
Outlier Leakage Threat. A trajectory database may contain out-
liers that are unique in one or more ways such as the locations they
visit, their start/destination points, and their time of travel. It is
these outlier trajectories that are often hunted by adversaries. For
example, while a $15 taxi ride in downtown Manhattan may not
reveal much, a $125 trip from an isolated location to the airport
leaks the identity and travel plans of a particular individual – note
that this attack has actually been successfully exercised using NYC
Taxi data [2].
We argue that relying solely on differential privacy cannot com-
bat outliers for two reasons. First, since trajectories are often high-
dimensional and unique [17, 48], there may exist skewed instances
that are easily singled out and mapped to particular users with
high confidence. Differential privacy needs to inject large noise
amounts to hide such skewed instances. Second, even when large
noise amounts are added, an outlier may still result from the proba-
bilistic nature and algorithmic randomness of differential privacy.
We should therefore place deterministic constraints to combat out-
liers – we combat an outlier trajectory Tout ’s leakage threat by
ensuring that it plausibly blends into a crowd of κ users’ trajec-
tories. This guarantees that Tout can at best be associated with
a group of users (of size κ), and does not disclose the identity or
secrets of any one user.
3.4 Utility Reference Model
The utility reference model used in AdaTrace is primarily designed
based on the common utilities of location traces used in many geo-
spatial data analysis and mining tasks, such as: users’ frequency
of visiting popular semantic locations, location entropy calcula-
tion, spatial decomposition, aggregate human mobility modeling
(e.g., spatial, temporal and semantic mobility mining), and training
predictive models for next location and destination inference. We
identify the following three core utility notions as the prominent
categories of trajectory utility in our reference model.
(1) Spatial density and locality distribution, e.g., for analyzing
where users visit and their popular points of interest. This can have
various commercial benefits, including choosing ideal geographic
placement for advertisements or retail stores, spatial hotspot dis-
covery, and recommendation in location-based social networks.
(2) Spatio-temporal travel metrics, e.g., correlations between users’
trip start and destination locations, how long their trips take, etc.
Since real datasets often consist of trips such as taxi and Uber rides,
trajectories have well-defined pickup and destinations. The com-
mercial benefits of preserving the correlations and related statistics
include taxi service prediction and passenger demand analysis, as
well as governance benefits such as emergency response planning.
(3) Frequent and representative travel patterns, e.g., commonly pre-
ferred routes of travel. Preserving this information helps discover
associations or sequentiality between road segments, ultimately
benefiting road network performance analysis, traffic flow control,
and path recommendation in navigation services.
Although these utility categories are listed separately, they can
be used in conjunction to enable more sophisticated geo-spatial data
analysis and machine learning tasks, some of which are mentioned
above. A natural question here is what if instead of synthesizing
privacy-preserving traces, we kept the original traces without modifi-
cation but governed data analysts’ access with a differentially private
querying interface? This interactive approach suffers from several
drawbacks. First, it places an additional burden on the data owner
to create, maintain, and enforce a privacy-preserving querying
interface. Second, it jeopardizes the analysts’ ability to apply off-
the-shelf ML tools or libraries, since these tools assume availability
of raw records and are not compatible with noisy query interfaces.
Third, our generated traces can be used in arbitrary tasks, including
those that were unforeseen at time of generation. In contrast, the
querying interface is an ad hoc solution whose functionality must
be enhanced each time a new type of query or analysis is desired.
Due to these factors, our non-interactive trace generation strategy
has much higher benefit and convenience for all parties.
4 SYNTHETIC TRAJECTORY GENERATION
In this section, we will describe the features in AdaTrace’s private
synopsis, as well as its trajectory synthesis algorithm. For each of
the features, we will discuss how noise and perturbation is injected
so that privacy is preserved. A central privacy parameter here is ε,
the differential privacy budget. Since AdaTrace’s synopsis consists
of four features as shown in Figure 1, ε is divided into four sub-
i =1 εi = ε. Detailed
discussion on the privacy and utility implications of the budget
division is presented in Section 5.1.
budgets ε1 to ε4, one for each feature, such that4
4.1 Density-Aware Grid
Effective discretization of Ω(Dr eal), the geographic location space
of Dr eal , constitutes the first step towards preserving spatial densi-
ties. Without discretization, Ω(Dr eal) is continuous and we have
an unbounded number of possibilities to simulate a move from
one location to another when generating a trajectory. To develop
an efficient and accurate synthesis algorithm, we need to bound
such transition possibilities by high utility choices. This motivates
our design of a grid structure for space discretization. Although
grids have been previously used in the location privacy literature
[4, 6, 43], choosing an appropriate grid size and structure is not
trivial under our privacy and utility constraints. If the grid is too
coarse (e.g., 2x2), then each grid cell covers a large spatial area, and
knowing that T visited a certain cell is uninformative. If the grid is
Figure 3: Two-step grid construction, top-level with N = 2
on the left and density-aware bottom-level on the right.
too fine-grained (e.g., 50x50), we risk having many empty cells in
which there are very few or no visits, and more additions of Laplace
noise to each cell leads to higher inaccuracy as well as inefficiency.
We therefore implement a grid with density-adaptive cell granu-
larity [43]. That is, for low density regions in Ω(Dr eal) we place
large, coarse cells; whereas for high density regions we divide the
region into smaller cells with finer granularity. The grid is built
in two levels. In the top-level, we lay an N × N grid with uniform
cells. Depending on the density of these cells, in the bottom-level,
we subdivide each cell into varying sizes to reflect the density dis-
tribution of Dr eal . This allows us to selectively zoom into dense
regions of Ω(Dr eal) only when the need arises, which saves us
from redundant computation and noise addition to sparse regions.
First, the grid is initialized with N ×N identical cells, according to
2 cells in the top-level, which
an input parameter N . This produces N
we denote by C1, C2, ..., CN 2. Then, we encode each trajectory in
Dr eal as a list of cells, e.g., observe the trajectory T : C1 → C2 →
C4 in Figure 3. We denote by |T | the number of cells T contains.
Next, for each cell Ci, where 1 ≤ i ≤ N
2, we count the normalized
number of visits in that cell as follows:
д(Dr eal , Ci) = 
T ∈Dr eal
# of occurrences of Ci in T
|T |
We say that д is normalized since occurrences are divided by tra-
jectory lengths. We need to find how much noise should be added
to the query answers to satisfy differential privacy. In Appendix A
we show that the sensitivity of the set of queries:
W = {д(Dr eal , C1), д(Dr eal , C2), ..., д(Dr eal , CN 2)}
is ∆W = 1, therefore it suffices to add Lap(1/ε1) to each query an-
swer to obtain the noisy answers denoted ˆд(Dr eal , Ci). The reason
why we perform normalization becomes clear from Appendix A
– without division by |T |, we either have unbounded sensitivity
∆W (implying unbounded noise) or we have to resort to alternative
strategies to bound sensitivity (such as abruptly cutting trajectories
or removing large portions therein), which are ill-suited for density
measurement.
The final step in grid construction is to subdivide Ci using the
noisy visit counts ˆд(Dr eal , Ci). Each Ci is independently divided
further into Mi × Mi cells, where Mi is proportional to ˆд(Dr eal , Ci)
and a grid constant. This achieves our goal of zooming into dense
regions with large visit counts. For example, in Figure 3, we have
densities C2 > C1 ≈ C4 > C3, hence M2 = 3, M1 = M4 = 2 and
M3 = 1. The grid constant acts as a balancing factor to determine
the highest and lowest possible value of M, so that both M remains
sensitive to changes in spatial density and the total number of cells
resulting from this procedure is not excessively large.
4.2 Markov Chain Mobility Model
To generate high utility and realistic synthetic trajectories, we
need to mimic actual intra-trajectory mobility. AdaTrace employs
Markov chains for mobility modeling. A Markov chain of order r
asserts that the next location in a trajectory depends on the pre-
vious r locations instead of all previous locations. Our grid-based
discretization allows us to build a discrete state space Markov chain
as follows. We map each cell in the adaptive grid to a state in our
Markov chain. We assume each trajectory is represented as a time-
ordered sequence of cells, and denote by T [j] the jth entry in T .
We find the transition probability of T to a next cell Cnext having
observed its previous n locations T [1]T [2] . . . T [n]:
(cid:16)
T [n+1] = Cnext | T [1] . . . T [n](cid:17)
(cid:16)
T [n+1] = Cnext | T [n-r+1]T [n-r+2] . . . T [n](cid:17)
= Pr
Pr
probabilities Pr(cid:0)T [n+1] | T [1] . . . T [n](cid:1). The trajectory-specific model
which boils down to the ratio of the number of sequences in T con-
taining T [n-r+1]T [n-r+2] . . . T [n]Cnext divided by the sequences
containing the prefix T [n-r+1]T [n-r+2] . . . T [n]. We say that the
trajectory-specific mobility model, Π(T), is the collection of such
captures the mobility of a single user in Dr eal . An aggregate mo-
bility model for the whole Dr eal can be found by averaging the
individual mobility models of each user. We slightly abuse notation
and write Π(Dr eal) to denote the aggregate mobility model.
Similar to noise addition during grid construction, the aggregate
model Π(Dr eal) is also perturbed with Laplace noise to satisfy
differential privacy. We add noise to the Markov probabilities. Since
Markov probabilities are computed using a ratio of sequence counts,
and since these counts have sensitivity equal to 1, the required noise
amount is limited. Hence, Π(Dr eal) can remain robust to noise.
4.3 Trip Distribution
Real life trace databases often consist of trips, such as taxi trips,
Uber trips, home-work commutes, etc. The trip distribution aims to
preserve the association between the start-end points of these trips
when generating Dsyn. This trip distribution is also useful from
a technical sense to guide a random walk on the Markov model,