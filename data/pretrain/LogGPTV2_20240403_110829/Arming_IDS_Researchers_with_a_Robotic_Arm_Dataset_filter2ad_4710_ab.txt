Recall from §I the middlebox-based setup that we deploy
in the Hein Lab. We focus on chemical synthesis experiments
spanning six different CPS devices: (i) four-axis N9 robot arm
from North Robotics; (ii) six-axis UR3e robot arm from Univer-
sal Robots; (iii) C-Mag HS 7 magnetic stirrer and heater from
IKA; (iv) 100-240V, 50/60Hz Fisherbrand Mini-Centrifuge
from Fisher Scientiﬁc; (v) Cavro XLP 6000 syringe pump from
Tecan; and (vi) Quantos balance for solid dosing from Mettler
Toledo [5, 16–18, 24, 25]. Henceforth, we refer to these as
N9, UR3e, IKA, Centrifuge, Tecan, and Quantos, respectively.
Since both N9 and Centrifuge are controlled via N9’s controller
box, we treat them as a single device called the C9. Similarly,
we consider the Arduino-controlled stepper motor used for
z-axis control in Quantos to be a part of Quantos.
Prior to our deploying the middlebox, all the devices in the
Hein Lab were connected directly to the lab computer over
Ethernet or USB (see Fig. 2). The lab computer hosts the
Hein Lab’s Python packages, typically one for each device,
which expose an intuitive and uniform high-level programming
API [12]. This enables lab members to easily program large and
complex automation processes spanning heterogeneous devices.
Our RATracer framework incorporates three main design
features: interception points (i.e., where the lab computer and
device communication is intercepted and traced), programmer-
friendly tracing, and distributed implementation.
RATracer intercepts all communication at the boundary
between low-level third-party software packages (e.g., Python’s
networking interfaces for TCP [21] and serial communica-
tion [20], UR3e’s urx package [8]) and high-level Python
libraries from the Hein Lab [12], which obviates the need to rely
on third-party software. Intercepting at boundary points such
as class FtdiDevice [11], which are used by many devices to
interface with the proprietary Windows FTDI driver, also allows
for seamless extension to accommodate other similar devices.
For tracing, our key priority is programmer friendliness: We
want to make it possible to enable RATracer with minimal
modiﬁcation. Ideally, the main experiment script requires only
a single import statement, e.g., import ratracer.backends, and
allows for optional customization with a few lines of tracing
conﬁguration (see below). We achieve this ideal by using
Python’s support for dynamic class modiﬁcation (also known
as monkey patching). Speciﬁcally, we virtualize each class
on the data collection boundary. Higher-layer classes interact
with the virtualized classes. Each virtualized class executes
logic from the original class implementation and additionally
logs all class-level (static) and object-level accesses (including
responses from modules) to a MongoDB instance or a .csv ﬁle.
Fig. 3 (top) gives an overview of this approach.
We map the tracing process described above to a distributed
system by introducing a middlebox. We implement two differ-
ent modes using the gRPC remote procedure call framework.
In DIRECT mode, the middlebox simply collects trace data; in
REMOTE mode, the middlebox both captures trace data and
sends commands to the robots. DIRECT mode is useful for
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:16 UTC from IEEE Xplore.  Restrictions apply. 
102
Fig. 2. An overview of the Hein Lab’s software architecture [12]. Each box illustrates a Python class, its parent classes (if any), a connection variable, and its
main APIs, e.g., class ArduinoAugmentedQuantos inherits from class Quantos(Balance) and class ArduinoAugment, conn tcp denotes a socket connection used
to communicate with Quantos, conn stepper denotes an instance of a lower-level class ArduinoStepper, and api start dosing(...) and api move z axis(...)
denote respective APIs for dosing and motor control [13]. Boxes shaded in green denote classes whose API accesses we trace, e.g., class FtdiDevice [11].
Ovals shaded in pink denote third-party software, e.g., Python’s Socket [21] and Serial packages [20].
or TCP connection problems) are sorted out by IT.
Finally, we add a simple Python module to RATracer, which
collects power monitoring data from the UR3e robot arm at
25 Hz (Fig. 3, bottom). We currently do not collect power data
from other devices, since they do not provide similar APIs.
Overall, RATracer is a simple and extensible tracing frame-
work. To trace a new device that relies on a different networking
stack, we need to identify a Python class in the stack to
virtualize, add a virtual class mimicking the class’s method
signatures to backends.py (this step can be automated), and
then add the necessary import statements to the main script.
Other languages could be supported using appropriate language
bindings. RATracer is versatile, as demonstrated by integrating
it with different device-speciﬁc libraries (Fig. 2).
RATracer is also non-intrusive; Hein Lab researchers have
been using our distributed setup seamlessly for weeks while
prototyping new experiments and running fully automated ex-
periments. Nonetheless, we evaluated RATracer’s performance
by comparing the response times of N9 commands in DIRECT
and REMOTE modes, when the commands were invoked
via continuous button presses on a joystick.1 We summarize
the results for six different button press sequences in Fig. 4.
1RAD also enables end-to-end performance evaluation of network stacks
that connect the robot arms with the lab computer. For example, we rented a
high-end F16s v2 Windows Server instance on Azure [4] that is identical to
our middlebox (16 vCPUs and a 32GB RAM) and replayed the DIRECT mode
joystick traces by emulating N9 commands in the cloud server. The average
response times (∼ 60 ms), as shown in Fig. 4, are an order of magnitude
higher than DIRECT and REMOTE modes (< 10 ms), but are still an order
of magnitude lower than common robot arm movements (seconds), indicating
that cloud deployment is within the realms of feasibility.
Fig. 3.
(Bottom) Periodic monitoring of power data from the UR3e robot arm.
(Top) Tracing in RATracer using class FtdiDevice as an example.
verifying that RATracer is operating correctly, before allowing
it to interpose between the lab computer and the robotic arm.
REMOTE mode is useful for actually deploying the IDS
in a secure environment. RATracer also allows conﬁguring
some devices for DIRECT tracing and some for REMOTE
tracing. Such hybrid conﬁgurations are convenient in practice,
because we can immediately incorporate new devices while
their communication issues with the middlebox (e.g., cabling
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:16 UTC from IEEE Xplore.  Restrictions apply. 
103
commands from supervised experiments are labeled accordingly
whereas all other commands are labeled “unknown procedure”.
Procedures P1-P3 are a novel set of modular, closed-loop
solubility screening techniques proposed by the Hein Lab [9,
10, 40]. In a typical run, the robot arm iteratively increases the
amount of solvent in the solid until image analysis determines
that the solid has dissolved. Each run varies based on the
solids and solvents used, resulting in variations in the observed
sequences of commands. P4 corresponds to a user operating
the joystick to control the N9 to perform tasks such as lifting a
vial, uncapping a vial, and placing the vial in the Quantos (we
used these for evaluating RATracer’s performance in Fig. 4).
The power dataset contains 122 physical properties that are
collected every 40 ms, using the UR3e’s real-time monitoring
API. As a result, even though our power dataset contains more
than 40 million entries, the majority of these correspond to
quiescent periods. We store only a small fraction of the entries
that belong to quiescent periods (i.e., we store quiescent period
entries only on days with some activity).
Each entry in the power dataset measures a set of physical
properties, e.g., velocity, acceleration, current, moment, and
speed, for each of the UR3e robot’s speciﬁc joints. The analyses
in §VI focus on joint-speciﬁc current values that were collected
across multiple procedure runs of type P2, and two other simple
procedures: P5, UR3e movements with different velocities, and
P6, UR3e movements with different payload weights [14].
V. COMMAND DATASET ANALYSIS
We collected the command dataset for the purpose of building
an intrusion detection system, however, other use cases are
also possible, e.g., program synthesis, generating a sequence
of low-level commands from a high-level speciﬁcation, and
speciﬁcation mining, deriving a high-level program speciﬁcation
from low-level commands. All of these use cases are premised
on the assumption that the dataset contains identiﬁable under-
lying patterns. To evaluate this assumption, and to gather other
useful insights from the dataset, we use data mining and NLP
techniques that are easily interpretable, speciﬁcally n-gram,
TF-IDF, and perplexity analyses. We use these techniques to
answer the following questions – RQ1: Can we identify Hein
Lab’s different scientiﬁc procedures in the RAD? RQ2: Can
we identify unexpected variations in procedures in the RAD?
– which can guide future development of more sophisticated
(e.g., ML-based) IDS. All our code is open source, implemented
in Python using pandas [35] and sklearn [37].
A. RQ1: Identifying Procedures
We begin by considering a simple n-gram model2 to ask
if certain sequences of commands repeat more regularly
than others. Fig. 5(b) shows the distribution of n-grams
for n ∈ {2, 3, 4, 5}. The results indicate that, as in natural
language, some sequences occur more frequently than others.
Additionally, we ﬁnd that all the runs of a speciﬁc procedure
have similar n-gram frequencies.
2An n-gram is a contiguous sequence of n items from a given sample of text
or speech [26]. In our case, an n-gram refers to a sequence of n commands.
Fig. 4. Response time box plots for N9’s ARM command. The y-axis ranges
from 3 to 100 ms. Boxes denote the interquartile-range (IQR) between quartiles
Q1 and Q3; lower and upper whiskers denote points Q1 - 1.5 IQR and Q3
+ 1.5 IQR; and outliers denote response times smaller and greater than the
whiskers. Outliers beyond 100 ms are not shown.
The results indicate that REMOTE mode increases average
response time by around 2 ms, and occasionally, the latency
exceeds 30 ms. As robot arm movements are on the order
of seconds, this overhead is negligible, in general. Even for
joystick procedures, which are ﬁner grain, the quality-of-service
provided by RATracer remained intact.
IV. ROBOTIC ARM DATASET
We present a brief summary of our Robotic Arm Dataset
(RAD). Our online documentation contains an exhaustive list
of features and their explanations [23].
The dataset is divided into two parts: (i) command dataset,
which contains information curated from the traces collected
by RATracer when it intercepted Hein Lab’s software stack;
and (ii) power dataset, which refers to the power monitoring
data collected by RATracer directly from UR3e.
The command dataset traces the communication between
Hein Lab scripts and ﬁve automation devices: C9 (N9 and
Centrifuge), UR3e, IKA, Tecan, and Quantos. As of this writing,
the command dataset includes 128,785 trace objects, where
each trace object corresponds to a single command instance,
and each command instance corresponds to one of the 52
different command types. Fig. 5(a) illustrates the command-
wise distribution of all trace objects.
The dataset was collected over a three-month period during
which Hein Lab researchers executed several procedures,
including many short scripts for prototyping or for trying out
new libraries. We did not supervise all procedures that were
run, except for a few that we analyse in §V. Speciﬁcally,
we supervised a total of 25 procedure runs across four
types of procedures. P1: Automated Solubility with N9 (5
runs); P2: Automated Solubility with N9 and UR3e (4 runs);
P3: Crystal Solubility (4 runs); and P4: Joystick Movements
(12 runs) Among these, we mark three as anomalous, since they
resulted in crashes between a robot arm and another device.
The remaining 22 are marked benign; these executed either
successfully or were stopped by the lab operator (e.g., if the
operator put the wrong set of vials next to the robot arm,
they would terminate the process on the lab computer). All
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:16 UTC from IEEE Xplore.  Restrictions apply. 
104
(a)
(b)
Fig. 5. (a) Command-wise distribution of trace objects in the command dataset.
For non-intuitive command names, we provide their human-readable versions
in parentheses. The total number of command instances observed for each
device appears in the legend along with the device name. (b) Top ten bigrams,
trigram, four-grams, and ﬁve-grams in RAD, including those with repeated
commands. The Q command is the Tecan command for get_status (a).
Next, we use term frequency–inverse document frequency
(TF-IDF), to identify unique ﬁngerprints for each procedure. TF-
IDF quantiﬁes the importance of a word to a document relative
to the word’s importance in the entire corpus of documents [39].
TF-IDF assigns weights to each command to give more weight
to commands that appear more frequently in a procedure than
is expected, given their frequency in the entire dataset.
We use the following procedure to compute pairwise
similarities between each procedure in our dataset: (i) count
the number of times each command appears in a procedure run;
(ii) divide each count by the total number of commands in the
procedure run, so that the normalized counts in each procedure
run sum to one; (iii) use TF-IDF to scale each normalized
count to the signiﬁcance of the command in the procedure; and
(iv) compute pairwise similarity scores using cosine similarity
between the TF-IDF vectors generated in (iii) for all procedure
runs (the higher the score, the greater the similarity).
Fig. 6 shows the 625 pairwise similarity scores for the 25
procedures. The dark blue region in the upper left quadrant
indicates that the Joystick Movement procedures (P4) are all
quite similar. This is unsurprising, because the joystick API
that is part of Hein Lab’s software distribution translates each
button press into a continuous stream of speciﬁc commands
that are repeated until the button is released, thereby giving all
traces of P4 a distinct ﬂavor. Interestingly, although procedure
12 is an Automated Solubility with N9 procedure (P1), it is
more similar to the joystick procedure runs than to other P1
Fig. 6. Pair-wise similarity scores based on TF-IDF for 25 supervised runs
of procedures P1-P4. IDs 0-11 correspond to Joystick Movements (P4), 12-
16 correspond to Automated Solubility with N9 (P1), 17-20 correspond to
Automated Solubility with N9 and UR3e (P2), and 21-24 correspond to Crystal
Solubility (P3). A similarity score of 1.0 (dark blue) indicates highest similarity.
procedures. The metadata in our dataset shows that procedure
12 used the joystick for a substantial time to move N9 to its
starting position, unlike other P1 procedures. Additionally,
procedure 12 stopped midway due to a shortage of solid
and executed none of the Quantos and Tecan commands
that are part of the automated solubility procedures but
not part of the joystick procedures (e.g., start_dosing,
target_mass, Q, V). The remaining P1 procedures (i.e., 13-
16) exhibit moderately high similarity among themselves
(mostly above 0.8). Procedure 16 is an anomaly, because the
Quantos front door crashed with the robot. However, before
crashing the procedure initiated Quantos commands such as
start_dosing and target_mass, so it is still similar to
the other Automated Solubility procedure runs.
Procedures 17-20 correspond to the Automated Solubility
with N9 and UR3e experiment (P2). Among these, procedures
17 and 18 have low similarity (around 0.58) with procedures
19 and 20 but a high similarity (more than 0.9) with each other.
This is particularly interesting, because the metadata shows
that run 17 is an anomaly but run 18 is not. We determined
that both stopped executing about one-tenth of the way into
the experiment, which accounts for their similarity. However,
in 17, because Quantos’ front door crashed into UR3e, we
marked it as an anomaly, whereas in 18, since a lab researcher
had mistakenly used the wrong gripper conﬁguration, we did
not mark it as an anomaly. Procedure runs 19 and 20 were
complete, normal executions.
Procedures 21-24 are Crystal Solubility experiments (P3). All
runs exhibit high pairwise similarity scores (ranging between
0.9 and 0.99), even though Procedure run 22 is labeled an
anomaly. The robot arm crashed with the Tecan at the end of
the experiment. As the anomaly occurred at the end of the
procedure, run 22 is more similar to the other procedures of
the same type than the anomalous run for procedure P2.
In the preceding analysis, we considered only commands
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:16:16 UTC from IEEE Xplore.  Restrictions apply. 
105
and not their parameters. Despite this limitation, these TF-
IDF-based similarity scores do distinguish one procedure from
another (RQ1). More importantly, the analysis suggests that
the dataset captures sufﬁcient information to sometimes detect
anomalies after the fact, e.g., the anomalous P2 experiments;
the next section investigates a potentially better technique that
could be adapted to real time detection.
B. RQ2: Identifying Unexpected Variations
While the analysis in §V-A suggests that anomaly detection
is possible, it does not provide a real-time solution. We next
demonstrate how we can capture both command frequencies
and command orderings in the training dataset to identify
unexpected procedures.
Given a training dataset consisting of Nv valid command
sequences S1, S2, . . ., SNv, and a new command sequence
Snew, we want to compute the probability with which Snew is
likely to occur. This probability is a likelihood function for
Snew, which we can use to classify if Snew is an anomaly.
Suppose that Snew consists of commands c1, c2, . . . , c|Snew|.
Using the training dataset, we ﬁrst compute the n-gram
probability P (ci | ci−n+1, . . . , ci−1)) for each i ∈ {n,|Snew|},
i.e., the probability with which ci follows ci−n+1, . . . , ci−1 in
the training dataset. For example, the bigram probability of
command sequence Snew is deﬁned as P (c2|c1) × P (c3|c2) ×
. . . × P (c|Snew||c|Snew|−1). To account for varying procedure
lengths, we need to normalize the likelihood function. We
compute the perplexity score, which is the normalized inverse
probability of Snew; as this is an inverse of the probability, a
lower perplexity score suggests a normal or benign trace and
a higher perplexity score suggests an anomaly. The perplexity
(cid:2)|Snew|
i=1 1/P (ci|ci−n+1, . . . , ci−1))1/|Snew|.
score is deﬁned as (
We use the 25 supervised procedure runs in RAD to generate
both training and test data using 5-fold cross validation. That
is, we (i) shufﬂe all 25 procedure runs and divide them into
ﬁve groups of ﬁve; (ii) hold one group as a test set and use