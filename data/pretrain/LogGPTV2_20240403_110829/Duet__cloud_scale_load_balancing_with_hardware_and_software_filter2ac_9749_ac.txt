The VIP assignment problem is a variant of multi-dimensional
bin-packing problem [10], where the resources are the bins, and
the VIPs are the objects. Multi-dimensional bin-packing problems
are NP-hard [10]. DUET approximates it with a greedy algorithm,
which works quite well in our simulations based on real topology
and trafﬁc load of a large production network.
4.1 VIP Assignment
We deﬁne the notion of maximum resource utilization (MRU).
We have two types of resource – switches and links. MRU repre-
sents the maximum utilization across all switches and links.
Algorithm sketch: We sort a given set of VIPs in decreasing
trafﬁc volume, and attempt to assign them one by one (i.e., VIPs
with most trafﬁc are assigned ﬁrst). To assign a given VIP, we con-
sider all switches as possible candidates to host the VIP. Typically,
(a) Initial
(b) Final
(c) Through SMux
Figure 4: Memory deadlock problem during VIP migration.
VIPs V1 and V2 both occupy 60% of switch memory each. The
goal of migration is to migrate the VIPs from assignment in
(a) to (b); DUET eliminates this problem by migrating VIPs
through SMuxes, as shown in (c).
assigning a VIP to different switches will result in different MRU.
We pick the assignment that results in the smallest MRU, break-
ing ties at random. If the smallest MRU exceeds 100%, i.e., no
assignment can accommodate the load of the VIP, the algorithm
terminates. The remaining VIPs are not assigned to any switch –
their trafﬁc will be handled by the SMuxes. We now describe the
process of calculating MRU.
Calculating MRU: We calculate the additional utilization (load)
on every resource for each potential assignment. If the v-th VIP
is assigned to the s-th switch, the extra utilization on the i-th link
is Li,s,v = ti,s,v
where trafﬁc ti,s,v is calculated based on the
Ci
topology and routing information as the source/DIP locations and
trafﬁc load are known for every VIP. Similarly, the extra switch
, i.e., the number
memory utilization is calculated as Ls,s,v =
of DIPs for that VIP over the switch memory capacity.
|dv|
Cs
The cumulative resource utilization when the v-th VIP is as-
signed to the s-th switch is simply the sum of the resource utiliza-
tion from previously assigned (v-1) VIPs and the additional utiliza-
tion due to the v-th VIP:
Ui,s,v = Ui,v−1 + Li,s,v
The MRU is calculated as:
M RUs,v = max(Ui,s,v),∀i ∈ R
(1)
(2)
4.2 VIP Migration
Due to trafﬁc dynamics, network failures, as well as VIP addition
and removal, a VIP assignment calculated before may become out-
of-date. From time to time, DUET needs to re-calculate the VIP
assignment to see if it can handle more VIP trafﬁc through HMux
and/or reduce the MRU. If so, it will migrate VIPs from the old
assignment to the new one.
There are two challenges here: (1) how to calculate the new as-
signment that can quickly adapt to network and trafﬁc dynamics
without causing too much VIP reshufﬂing, which may lead to tran-
sient congestion and latency inﬂation. (2) how to migrate from the
current assignment to new one.
A simple approach would be to calculate the new assignment
from scratch using new inputs (i.e., new trafﬁc, new VIPs etc.),
and then migrate the VIPs whose assignment has changed between
the current assignment and the new one. To prevent routing black
holes during VIP migration, we would use make-before-break —
i.e., a VIP would be announced from the new switch before it is
withdrawn from the old switch. This simple approach is called
Non-sticky.
S2 (60%) SMux SMux VIP Traffic V1 V2  S3 (60%) S1 (60%) S2 (60%) SMux SMux VIP Traffic V1 V2  S3 (60%) S1 (60%) S2 SMux SMux VIP Traffic V1,V2 S3 V1,V2 S1 (60%) 31Figure 5: When the VIP assignment changes from ToR T2 to
T3, only the links inside container-2 are affected. As a result,
we can ﬁrst select best ToR in a container based on the links
within container, and then scan over all containers and remain-
ing Core and Agg switches.
The Non-sticky approach suffers from two problems. First, it
may lead to transitional memory deadlock. Figure 4 shows a sim-
ple example where initially VIP V1 and VIP V2 are assigned to
switches S2 and S3, respectively, but swap positions in the new
assignment. Further, either VIP takes 60% of the switch memory.
Because of limited free memory, there is no way to swap the VIPs
under the make-before-break approach. When there are a large
number of VIPs to migrate, ﬁnding a feasible migration plan be-
comes very challenging. Second, even if there was no such dead-
lock, calculating a new assignment from scratch may result in a lot
of VIP reshufﬂing, for potentially small gains.
DUET circumvents transitional memory deadlocks by using
SMux as a stepping stone. We ﬁrst withdraw the VIPs that need
to be moved from their currently assigned switches and let their
trafﬁc hit the SMux3. We then announce the VIPs from their newly
assigned switches, and let the trafﬁc move to the new switches.
This is illustrated in Figure 4(c) where both VIP’s (V1 and V2)
trafﬁc is handled by SMux during migration.
Because SMux is used as a stepping stone, we want to avoid un-
necessary VIP reshufﬂing to limit the amount of VIP trafﬁc that is
handled by SMux during migration. Hence, we devise a Sticky ver-
sion of the greedy VIP assignment algorithm that takes the current
assignment into account. A VIP is moved only if doing so results in
signiﬁcant reduction in MRU. Let us say that VIP v was assigned
to switch sc in the current assignment, and the MRU would be the
lowest if it is assigned to switch sn in the new assignment. We
assign v to sn only if (M RUsc,v − M RUsn,v) is greater than a
threshold. Else we leave v at sc.
Complexity: It is important for DUET to calculate the new as-
signment quickly in order to promptly adapt to network dynam-
ics. Since all Li,s,v can be pre-computed, the complexity to ﬁnd
the minimum MRU (Equation 2) for VIP-switch assignment is
O(|V | · |S| · |E|).
This complexity can be further reduced by leveraging the hier-
archy and symmetry in the data center network topology. The key
observation is that assigning a VIP to different ToR switches inside
a container will only affect the resource utilization inside the same
container (shown in Figure 5). Therefore, when assigning a VIP,
we only need to consider one ToR switch with the lowest MRU
inside each container. Because ToR switches constitute a majority
of the switches in the data center, this will signiﬁcantly reduce the
computation complexity to O(|V |·((|Score|+|Sagg|+|C|)·|E|+
|Stor| · |Ec|)). Here C and Ec denote the containers and links in-
side a container. Score, Sagg and Stor are the Core, Aggregation
and ToR switches respectively.
3Recall that SMux announces all VIPs to serve as a backstop
(§3.3.1)
Figure 6: Load balancing in virtualized clusters.
5. PRACTICAL ISSUES
We now describe how DUET handles important practical issues
such as failures and conﬁguration changes.
5.1 Failure Recovery
A critical requirement for load balancer is to maintain high avail-
ability even during failures. DUET achieves this primarily by using
SMuxes as a backstop.
HMux (switch) failure: The failure of an HMux is detected by
neighboring switches. The routing entries for the VIPs assigned
to the failed HMux are removed from all other switches via BGP
withdraw messages. After routing convergence, packets for these
VIPs are forwarded to SMuxes, since SMuxes announce all VIPs.
All HMux and SMux use the same hash function to select DIPs for
a given VIP, so existing connections are not broken, although they
may suffer some packet drops and/or reorderings during conver-
gence time (<40ms, see §7.2). Because in our production DCs we
rarely encounter failures that are more severe than three switch fail-
ures or single container failures at a time, we provision sufﬁcient
number of SMuxes to handle the failover VIP trafﬁc from HMuxes
due to those failures.
SMux failure: SMux failure has no impact on VIPs assigned to
HMux, and has only a small impact on VIPs that are assigned only
to SMuxes. Switches detect SMux failure through BGP, and use
ECMP to direct trafﬁc to other SMuxes. Existing connections are
not broken, although they may suffer packet drops and/or reorder-
ings during convergence.
Link failure: If a link failure isolates a switch, it is handled as a
switch failure. Otherwise, it has no impact on availability, although
it may cause VIP trafﬁc to re-route.
DIP failure: The DUET controller monitors DIP health and re-
moves failed DIP from the set of DIPs for the corresponding VIP.
Existing connections to the failed DIP are necessarily terminated.
Existing connections to other DIPs for the corresponding VIP are
still maintained using resilient hashing [2].
5.2 Other Functionalities
VIP addition: A new VIP is ﬁrst added to SMuxes, and then the
migration algorithm decides the right destination.
VIP removal: When a VIP assigned to an HMux is to be with-
drawn, the controller removes it both from that HMux and from all
SMuxes. VIPs assigned to only SMuxes need to be removed only
from SMuxes. BGP withdraw messages remove the corresponding
routing entries from all switches.
DIP addition: The key issue is to ensure that existing connec-
tions are not remapped if DIPs are added to a VIP. For VIPs as-
signed to SMuxes, this is easily achieved, since SMuxes maintain
detailed connection state to ensure that existing connections con-
tinue to go to the right DIPs. However, HMuxes can only use a
C1 C2 ToR Agg Core ToR Agg T1 A1 A2 Container-1 T3 T2 A3 A4 Container-2 VM-1 100.0.0.1 VM-2 100.0.0.2 HA 20.0.0.1 Tunneling Table Index Encap IP 0 20.0.0.1 1 20.0.0.1 2 20.0.0.2 VM-3 100.0.0.3 HA 20.0.0.2 Host-2 Host-1 VIP DIP HMUX 32Figure 7: Large fanout support.
Figure 8: Port-based load balancing.
hash function to map VIPs to DIPs (Figure 2). Resilient hashing
only ensures correct mapping in case of DIP removal – not DIP ad-
dition. Thus, to add a DIP to a VIP that is assigned to an HMux,
we ﬁrst remove the VIP from the HMux, causing SMuxes to take it
over, as described earlier. We then add the new DIP, and eventually
move the VIP back to an appropriate HMux.
DIP removal: DIP removal is handled in a manner similar to
DIP failure.
Virtualized clusters: In virtualized clusters, the HMux would
have to encapsulate the packet twice – outer header carries the IP
of the host (native) machine, while inner header carries IP of the
VM hosting the DIP. However, today’s switches cannot encapsulate
a single packet twice. So, we use HA in tandem with HMux, as
shown in Figure 6. The HMux encapsulates the packet with the IP
of the host machine (HIP) that is hosting the DIP. The HA on the
DIP decapsulates the packet and forwards it to the right DIP based
on the VIP. If a host has multiple DIPs, the ECMP and tunneling
table on the HMux holds multiple entries for that HIP (HIP 20.0.0.1
in Figure 6) to ensure equal splitting. At the host, the HA selects
the DIP by hashing the 5-tuple.
Heterogeneity among servers: When the DIPs for a given VIP
have different processing power, we can proportionally split the
trafﬁc using WCMP (Weighted Cost Multi-Path) where faster DIPs
are assigned larger weights. WCMP can be easily implemented on
commodity switches.
VIPs with large fanout: Typically the capacity of the tunneling
table on a single-chip switch is 512. To support a VIP that has more
than 512 DIPs, we use indirection, as shown in Figure 7. We divide
the DIPs into multiple partitions, each with at most 512 entries. We
assign a single transient IP (TIP) for each partition. As a VIP, a
TIP is a routable IP, and is assigned to a switch. When assigning
a VIP to an HMux, we store the TIPs (as opposed to DIPs) in the
tunneling table (Figure 7). When a packet for such a VIP is received
at the HMux, the HMux encapsulates the packet with one of the
TIPs and forwards it to the switch to which the TIP is assigned.
That switch decapsulates the TIP header and re-encapsulates the
packet with one of the DIPs, and forwards it. The latency inﬂation
is negligible, as commodity switches are capable of decapsulating
and re-encapsulating a packet at line rate. This allows us to support
up to 512∗ 512 = 262, 144 DIPs for a single VIP, albeit with small
extra propagation delay4.
Port-based load balancing: A VIP can have one set of DIPs for
the HTTP port and another for the FTP port. DUET supports this
using the tunneling table and ACL rules. ACL (Access Control)
Rules are similar to OpenFlow rules, but currently support a wider
range of ﬁelds. We store the DIPs for different destination ports at
different indices in the tunneling table (Figure 8). The ACL rules,
Figure 9: Components in DUET implementation.
match on the IP destination and destination port ﬁelds, and the ac-
tion is forwarding the packet to the corresponding tunneling table
entry. Typically the number of ACL rules supported is larger than
the tunneling table size, so it is not a bottleneck.
SNAT: Source NAT (SNAT) support is needed for DIPs to estab-
lish outgoing connections5. Ananta supports SNAT by maintaining
state on SMuxes [17]. However, as discussed earlier, switches can-
not maintain such connection state. Instead, DUET supports SNAT
by sharing the hash function used by HMux with the host agent
(HA). Like Ananta, DUET assigns disjoint port ranges to the DIPs,
but unlike Ananta, the HA on the DIP does not randomly choose an
unused port number. Instead, it selects a port such that the hash of
the 5-tuple would correctly match the ECMP table entry on HMux.
The HA can do this easily since it knows the hash function used
by HMux. Note that the HA needs to do this only during establish-
ment (i.e., ﬁrst packet) of outgoing connections. If an HA runs out
of available ports, it receives another set from the DUET controller.
6.
IMPLEMENTATION
In this section, we brieﬂy discuss the implementation of the key
components in DUET: (1) DUET Controller, (2) Host Agent, and
(3) Switch Agent, and (4) SMux, as shown in Figure 9.