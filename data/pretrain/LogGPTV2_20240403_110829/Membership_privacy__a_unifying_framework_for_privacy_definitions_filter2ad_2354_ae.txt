t ∈ T , let T ′ = T \ {t}, for any output event S
Pr[t ∈ T|A(T) ∈ S, T ′] ≤ ρ,
where T is a random variable drawn from the following distribu-
tion: each dataset T ′ ∪ {t′}, where t′ ∈ U \ T ′ is equally likely.
The notion of ρ-DI aims at limiting the posterior probability that
the tuple t is in the input dataset after observing the output even-
t S. To be able to compute the posterior probability, one has to
assume some prior beliefs the adversary may have. The notion of
ρ-DI speciﬁes the allowed distributions to be from the following
families.
C :
DISTRIBUTION FAMILY 5.5. Dm
probability choice BMI Distributions. Dm
1-out-of-m equal-
C , where m ≥ 2 is
a positive integer, includes every distributions that is fully speciﬁed
by ﬁxing a set T ⊂ U such that |U \ T | = m. To sample a dataset
from the distribution, one includes T plus one additional entity
sampled uniformly at random from U \ T . In other words, in each
distribution only one entity is uncertain, and this entity is assumed
to be drawn uniform from m possible entities.
Dm
C is a sub-family of DB, the family of BMI distributions, as it
allows only BMI distributions that satisfy three further conditions.
First, the sampled dataset must have size 1 + |{t | pt = 1}|. Sec-
ond, there must exist m entities with probability between 0 and 1.
Third, all these m entities must have the same probability.
To determine whether a non-trivial mechanism A satisﬁes ρ-DI
or not, one has to also specify the value m; thus it is more appropri-
ate to say (ρ, m)-DI, which requires that for any distribution in Dm
C
and any output event S, we have Pr[t|S] ≤ ρ. Note that (ρ, m)-DI
makes sense only when ρ > 1
m .
THEOREM 5.6. (ρ, m)-DI is equivalent to (Dm
C , γ)-PMP for
γ = ρm for γ = max(cid:16)ρm, m−1
m(1−ρ)(cid:17).
PROOF. (ρ, m)-DI means that for any distribution in Dm
C and
m , the posterior is at most ρ, i.e.
any entity with prior probability 1
Pr[t|S] ≤ ρ.
(Dm
C , γ)-PMP means that for any distribution in Dm
C , Pr[t|S] ≤
γ
m , 1 − m−1
mγ (cid:17).
two cases.
(cid:17) = min(cid:16) γ
min(cid:16)γ Pr[t], γ−1+Pr[t]
We only need to prove that ρ = min(cid:16) γ
and min(cid:16) γ
mγ (cid:17) = min(cid:16) m−1
Case one: we have ρm ≤ m−1
m2(1−ρ) from the case condition.
Case two: we have ρm > m−1
m , 1 − m−1
ρ ≤ m−1
m(1−ρ) , and thus γ = m−1
m , 1 − m−1
mγ (cid:17). There are
m2(1−ρ) , ρ(cid:17) = ρ because
m(1−ρ) ,
m(1−ρ) , and thus γ = ρm, and
min(cid:16) γ
m , 1 − m−1
mγ (cid:17) = min(cid:16)ρ, 1 − (m−1)/m
ρm (cid:17) = ρ because
> 1 − (m−1)/m
=
m−1
from the case condition, we have 1 − (m−1)/m
ρ.
ρm
m(1−ρ)
The following theorem show an interesting relationship between
DI and BDP.
THEOREM 5.7. When ρ > 1
possible to satisfy (ρ, m)-DI while violating ǫ-BDP for any ǫ.
1−ρ(cid:17). However, when m > 2 and ρ > 1
for ǫ = ln(cid:16) ρ
2 , (ρ, 2)-DI is equivalent to ǫ-BDP
m−1 , it is
PROOF. When m = 2, (ρ, 2)-DI requires that on all dataset
pairs T1 and T2 that have the same number of entities and differ
in only one entity, when T1 and T2 are equally likely, we have
Pr[T1|S] ≤ ρ, i.e., Pr[T1|S] =
Pr[S|T1]+Pr[S|T2] ≤ ρ; this holds
if and only if Pr[S|T1] ≤ ρ
1−ρ Pr[S|T2]; hence the equivalence
Pr[S|T1]
with ln(cid:16) ρ
1−ρ(cid:17)-BDP.
When m > 2 and ρ > 1
m−1 , consider the following case, the
universe consists of m entities t1 · · · tm, the mechanism A, on
input dataset T outputs a value chosen at uniform random from
{1, · · · , m}, except for the case T = {t1}, when it outputs a value
chosen at uniform random from {2, · · · , m}.
This trivially satisﬁes the DI condition for any input dataset con-
taining more than one entities, since the output is always uniform.
When one consider datasets containing exactly one entity, it sat-
isﬁes the DI condition too. First consider the case 1 is the out-
put. We have Pr[t1|1] = 0  eǫ Pr[1|{t1}] = 0 for any ǫ.
5.3 Differential Privacy Under Sampling
In [22], Li et al. proposed a relaxation to differential privacy that
exploits the adversary’s uncertainty about the dataset. While the o-
riginal deﬁnition of differential privacy assumes that the adversary
has precise knowledge of all the tuples in the dataset, Li et al. argue
that this, in fact, might be too strong for some data publishing sce-
narios. Instead, it is reasonable to relax the assumption to that the
adversary knows all attributes of a tuple t (but not whether t is in
the dataset), and in addition statistical information about the rest of
the dataset D. The privacy notion should prevent such an adversary
from substantially distinguishing between D and D ∪ {t} based on
the output. This intuition is formalized using the following deﬁni-
tion.
DEFINITION 5.8
((β, ǫ)-DPS [22]). An algorithm A gives
(β, ǫ)-DPS if and only if the algorithm Aβ gives ǫ-UDP, where Aβ
denotes the algorithm to ﬁrst sample each tuple with probability β,
and then apply A to the sampled dataset.
Similar to our analysis of differential privacy, we focus on posi-
tive DPS. It turns out that (β, ǫ)-DPS is equivalent to PMP for the
following family.
DISTRIBUTION FAMILY 5.9. Dβ
F : Fixed-probability Mutual-
ly Independent Distributions. This is a sub-family of DI. It in-
cludes all mutually independent distributions such that ∀t Pr[t] ∈
{0, β}. In other words, all entities that may appear have the same
probability.
THEOREM 5.10. A randomized mechanism satisﬁes (β, ǫ)-
F , γ)-PMP for γ =
Positive DPS if and only if it satisﬁes (Dβ
Proof is given in Appendix A.4.
max(cid:16)eǫ, eǫ−1+β
βeǫ (cid:17).
6. RELATED WORK
A number of syntactic privacy deﬁnitions have proposed over
the years; the most prominent ones include k-anonymity [30, 29],
l-diversity [24] and t-closeness [21]. Their weaknesses have been
often identiﬁed. See, e.g., Dwork [9] for a survey.
Instead, D-
work argues that we should consider privacy protection problems
in a more rigorous and formal way. This is the motivation of the
research of differential privacy. The notion of differential privacy
was developed in a series of works [6, 13, 4, 11, 8], and several
methods of satisfying have been developed [8, 11, 26, 28].
In [5], Cormode argued that differential privacy does not prevent
inferential disclosure. It is shown that, from differentially private
output, it is possible to infer potentially sensitive information about
an individual with non-trivial accuracy. In Section 2.3, we argued
that it may be inappropriate to use prevention of attribute disclosure
as the privacy objective. In this paper, we formalize membership
disclosure, and show that it closely matches the social and legal
deﬁnitions of privacy. Lee and Clifton [20] proposed the notion
of ρ-differential identiﬁability, which captures membership disclo-
sure under speciﬁc adversarial background knowledge. This notion
is a special case of membership privacy, and we analyze it in Sec-
tion 5.2. Li et al. [22] relax an adversary’s background knowledge
by sampling the dataset prior to applying the privacy mechanism.
We analyze differential privacy under sampling and its relation to
our privacy notion in Section 5.3.
In [19], Kifer and Machanavajjhala argued that it is not possible
to provide privacy and utility without making assumptions about
how the data are generated. Our modeling of the attacker’s back-
ground knowledge in terms of a family of distributions complies
with their ﬁnding. Kifer et al. also questioned whether the dif-
ferential privacy guarantees when data points are correlated. Our
analysis suggests that differential privacy only guarantees member-
ship protection under distributions where data points are mutually
independent.
Kifer and Lin [18] proposed the privacy axioms of choice and
the axiom of transformation, which we show that membership pri-
vacy satisfy. They also introduce a generalization of differential
privacy, called generic differential privacy, which follows the syn-
tactic structure of differential privacy, but allows more ﬂexible
deﬁnition of neighboring datasets and the condition between that
Pr[S|D] and Pr[S|D′] should satisfy. A similar approach is tak-
en by Machanavajjhala et al. [23], which introduced a framework
called ǫ-privacy, which limits the impact the inclusion of one entity
can have on the adversary’s belief about the individual’s attribute
value. Gehrke et al. [16] introduced zero-knowledge based deﬁni-
tion of privacy, which deﬁnes a mechanism to be private if its out-
put can be simulated by a simulator with access to some aggregate
function of the data, but without direct access to the data. One need-
s to choose appropriate aggregate function to instantiate the privacy
notion. Gehrke et al. [15] then introduced crowd-blending privacy,
which combines safe k-anonymization and differential privacy. We
also generalize differential privacy; however, our approach differs
in that we formalize membership privacy, which is justiﬁed from
analysis of privacy incidents, and in that our notion is parameter-
ized by families of dataset distributions.
In an attempt to make differential privacy more amenable to
more sensitive queries, several relaxations have been developed, in-
cluding (ǫ, δ)-differential privacy [6, 13, 4, 11]. Machanavajjhala
et al. [25] introduced a variant of (ǫ, δ)-differential privacy called
(ǫ, δ)-probabilistic differential privacy. Roughly, all these relax-
ations use δ to bound the probability that ǫ-DP is violated. Our pri-
vacy framework does not yet deal with this relaxation of allowing
a δ of error probability. It is interesting future work to investigate
how they can be accommodated by extending membership privacy
to allow such an error probability.
7. CONCLUSIONS
Through analysis of the recent privacy incidents, we have con-
cluded that what society often views as a privacy breach is the abil-
ity of an adversary to either re-identify or assert the membership of
any individual in a supposedly “anonymized” dataset. Thus we in-
troduce the membership privacy framework. We have demonstrat-
ed that differential privacy and several other related privacy notions
are instantiations of the framework.
Identifying the family under which a privacy notion guarantees
membership privacy provides deeper understanding of the power
and limitation of the privacy notion. In particular, they identify the
assumptions that are made by the privacy notion. As all practical
privacy notion requires some assumptions on the allowed distribu-
tions, it makes sense to analyze whether the assumption made in a
notion is appropriate for a given setting, and choose a privacy that
is neither too strong nor too weak, in order to maximize utility. Our
framework enables the development of new privacy notions. We
believe that the membership privacy framework opens doors for fu-
ture research at developing new privacy notions, understanding and
comparing privacy notions, and designing mechanisms for satisfy-
ing different privacy notions.
8. REFERENCES
[1] Directive 95/46/ec of the European Parliament and of the
council of 24 october 1995 on the protection of individuals
with regard to the processing of personal data and on the free
movement of such data. Ofﬁcial Journal L,
281(23/11):0031–0050, 1995.
[2] Standard for privacy of individually identiﬁable health
information. Federal Register, 67(157):53 181–53 273, Aug
2002.
http://www.hhs.gov/ocr/privacy/hipaa/
administrative/privacyrule/index.html.
[3] M. Barbaro and J. Tom Zeller. A face is exposed for aol
searcher no. 4417749. New York Times, Aug 2006.
[4] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical
privacy: the SuLQ framework. In PODS, pages 128–138,
2005.
[5] G. Cormode. Personal privacy vs population privacy:
learning to attack anonymization. In KDD, pages
1253–1261, 2011.
[6] I. Dinur and K. Nissim. Revealing information while
preserving privacy. In PODS.
[7] C. Dwork. Differential privacy. In in ICALP, pages 1–12.
Springer, 2006.
[8] C. Dwork. Differential privacy. In ICALP, pages 1–12, 2006.
[9] C. Dwork. An ad omnia approach to deﬁning and achieving
private data analysis. In PinKDD ’07, pages 1–13, Berlin,
Heidelberg, 2008. Springer-Verlag.
[10] C. Dwork, F. Mcsherry, K. Nissim, and A. Smith. Calibrating
noise to sensitivity in private data analysis. In In Proceedings
of the 3rd Theory of Cryptography Conference, pages
265–284. Springer, 2006.
[11] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data analysis. In
TCC, pages 265–284, 2006.
[12] C. Dwork and M. Naor. On the difﬁculties of disclosure
prevention in statistical databases or the case for differential
privacy. Journal of Privacy and Conﬁdentiality, 2(1):8, 2010.
[13] C. Dwork and K. Nissim. Privacy-preserving datamining on
vertically partitioned databases. In CRYPTO, pages 528–544.
Springer, 2004.
[14] D. Gale. A theorem on ﬂows in networks. Paciﬁc Journal of
Mathematics, 7(2):1073–1082, 1957.