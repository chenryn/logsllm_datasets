3.691
0.248
0.231
0.206
0.184
0.223
1.777
n
1
n
(cid:2)
it can only distort
Privacy Preserving Mapping. We borrow the notion of a
Privacy Preserving Mapping (PPM) from [57] to denote a
: Rd → Rd parameterized by θ, which is
mapping Dθ
trained to minimize the effectiveness of an imagined ad-
versary Aψ. Meanwhile, the PPM is required to follow the
utility constraint:
the embeddings in a
limited radius around the original embedding in order to
maintain the utility, or otherwise a trivial yet perfect de-
fense only needs to map all the embeddings to a constant
vector. Formally, we propose to learn the privacy preserv-
ing mapping Dθ by solving the following minimax game
i=1 Aψ(Dθ(zi), si), s.t. (cid:10)Dθ(z)−z(cid:10)2 ≤ . In
minθ maxψ
other words, the active defense accesses the plaintexts {xi}n
i=1,
derives the training set {(zi, si)}n
i=1 for an imagined white-
box adversary, and simulates the minimax game as we describe
above. As the defense at the user side usually has no access
to the intelligent service at the cloud, the utility constraint is
formulated as the upper bound on the 2-norm distance between
the protected and original embeddings, which is similar to that
in [57]. In practice, Dθ is implemented as an encoder-decoder
neural network and Aψ is implemented as an MLP.
However, we notice an additional challenge brought by
the language model setting. Although previous PPM-based
defenses have studied several efﬁcient approaches to solve
the minimax game when z takes discrete values and Dθ is
a combinatorial function [35], [57], our PPM is required to
work on the real-valued embeddings with Dθ implemented
as neural networks. By the best of our knowledge,
there
is no effective algorithm to exactly solve the constrained
optimization problem above. As an alternative, we propose
to reformulate the L2 constraint as a regularization term.
i=1 Aψ(Dθ(zi), si) +
In detail, it writes minθ maxψ 1/n
λ(cid:10)Dθ(zi) − zi(cid:10)2, where the positive coefﬁcient λ is expected
to control the privacy level of this active defense. Intuitively, a
larger λ corresponds to a stricter utility constraint and thus, a
(cid:2)
n
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1329
lower privacy level. To solve the unconstrained minimax game,
we use the simultaneous Gradient Descent algorithm, which
takes alternative gradient descent (ascent) steps on θ and ψ
[26].
Subspace Projection. Similar to the methodology in [14],
our defense ﬁrst calculates the privacy subspace w.r.t. the
semantic meaning we do not want the adversary to distinguish.
Speciﬁcally in the setting of keyword inference attack, we ex-
pect the adversary is unable to distinguish whether a sentence
contains certain keyword or not. Therefore, we ﬁrst collect two
sets of embeddings D1 and D0 that respectively correspond to
sentences with and without the target keyword. Then we com-
pute the privacy subspace as the linear space spanned by the
ﬁrst k orthonormal eigenvectors u1, . . . , uk of the following
(z − μi)(z − μi)T /|Di| where
matrix C =
z∈Di z/|Di|, i.e. the average sentence embedding in
μi =
Di. In our evaluations, we consider the ratio β between the
dimension of the privacy subspace k and the full dimension d
of the sentence embedding as the parameter of the subspace
defense, i.e., β = k/d.
i∈{0,1}
z∈Di
(cid:2)
(cid:2)
(cid:2)
k
i=1(I − vivT
Next, to remove the unwanted semantics from the embed-
ding (denoted as z), we simply project the embedding to
the subspace orthogonal to the privacy subspace, with the
following formulas: ˆz ← (cid:2)
)z, ˆz ← ˆz/(cid:10)ˆz(cid:10).
Evaluation Details. For PPM defense, we implement
the
virtual adversary Aψ as a 3-layer MLP with 200 sigmoid
hidden units and the PPM Dθ as an MLP of the architecture
(d − 200 − d) with ReLU activation. We train the virtual
adversary and the PPM alternatively for 1 and 5 iterations,
where the batch size is set as 64.
i
D. Fingerprinting Algorithm
type. First,
We propose the following ﬁngerprinting algorithm to relax
Assumption 1, with 100% accuracy on determining the speciﬁc
model
the adversary determines the candidate
model types according to the embedding dimension. For ex-
ample, if d = 768, the candidate set includes GPT, GPT-2 and
other three models. Next, the advervsary prepares an arbitrary
corpus and queries each language model for the embedddings.
Then, the adversary trains an off-the-shelf classiﬁer with the
embedding as input and the model type as label. Finally, when
the adversary gets a set of embeddings as victims, he/she ﬁrst
uses the language model classiﬁer at hand to determine the
model type and conducts the downstream attacks as introduced
in previous sections.
Fig. 9. Clustering phenomenon observed on (a) embeddings from 5 784-dim.
language models and (b) their MLP hidden representations of 1000 randomly-
sampled sentences on Medical.
INFERRED TOP-10 POSSIBLE KEYWORDS IN EACH SAMPLE FOR
REASSEMBLING THE SEMANTICS.
TABLE V
Sample #1. Destruction of malignant growth (1.1 to 2.0 centimeters) of trunk, arms, or legs
arm, trunk, malignant, repair, venous, skin, veins, lower, removal, artery
Sample #2. Application of ultraviolet light to skin
venous, veins, centimeters, radiation, older, guidance, arm, removal, arterial, injection
Sample #3. Removal of malignant growth (1.1 to 2.0 centimeters) of the trunk, arms, or legs
arm, trunk, malignant, repair, venous, veins, artery, removal, tissue, insertion
Sample #4. Removal of up to and including 15 skin tags
veins, centimeters, tissue, removal, insertion, arms, spinal, arterial, skin, legs
Sample #5. Biopsy of each additional growth of skin and/or tissue
lower, venous, veins, biopsy, tissue, insertion, centimeters, artery, endoscope, ultrasound
To evaluate our ﬁngerprinting algorithm, we implement the
language model classiﬁer as a (784-200-5) MLP, use the Yelp-
Food corpus of 2000 sentences for training and a subset of the
Medical corpus that consists of 1000 sentences for testing.
Strikingly, we ﬁnd the classiﬁer achieve 100% accuracy.
To better understand the phenomenon, we plot the original
embeddings in the test set and their hidden representations
at
layer of the MLP in Fig. 9, where different
colors implies different language models. As we can see, the
embeddings from different language models distribute in rather
divergent ways. After MLP’s nonlinear mapping, the embed-
dings directly collapse to separated clusters according to their
corresponding model type. To the best of our knowledge, we
are the ﬁrst to discover and report this interesting phenomenon.
the last
E. Semantic Reassembling with Keyword Inference Attack
Experimental Settings. Following the description in Sec-
tion VIII, we ﬁrst select 50 medical-related words to form
the candidate keyword set and train an DANN attack model
for each keyword with the same conﬁgurations in our original
work. The DANN attack accuracy is about 77% after being
averaged on the 50 words. Then, we randomly select 5 samples
from the test set of the medical case and use each DANN
model
the probability of the occurrence of the
corresponding keyword. We list the inferred Top-10 keywords
of each sentence in Table V.
to output
Results & Analysis. As we can see from Table V, the
attacker can actually reassemble the basic semantic meaning
of the original text with such a procedure. For example, in
Sample #3, when the adversary knows arm, trunk, malignant
as the Top-3 most possible keywords, then he/she can probably
infer the semantic meaning of the original description is related
with malignant growth at the arms or trunk. Compared with
the plain text of Sample #3, the inferred meaning is quite
consistent with the original meaning, despite some minor
details left out. Interestingly, we also ﬁnd, although DANN
may predict the occurrence of certain keywords with error,
the erroneous prediction may also contribute to the semantic
reconstruction. For example, in Sample #2, the adversary fails
to predict the occurrence of ultraviolet since this word is
not in the adversary’s candidate set. However, due to the
semantic similarity between ultraviolet and radiation 4, the
DANN attack model for radiation predicts the high probability
40.942 in cosine similarity of Bert word embeddings.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1330
Fig. 10. The training loss of LSTM decoder and attack accuracy of decoder-
based attack in the ﬁrst 5 epochs.
of the occurrence of the word radiation, which, despite the
inexactness, helps the adversary successfully guess that the
hidden semantic is about the radiation-related procedure, i.e.,
the application of ultraviolet light.
F. Keyword Inference Attack with Standard Decoder Section
Experimental Settings. We use the standard decoder mod-
ule [68], a one-layer bidirectional LSTM to implement the
decoder. The vocabulary of the LSTM is initialized the same
as the target language model. For most of them (excluding
XLNet and ERNIE), the vocabulary is publicly accessible. We
therefore implement the decoder-based attack on the rest 6
targets.
For training, we input the decoder both the embedding
and the corresponding sentence: the embedding is input as
the initial hidden state of the LSTM forwarding phase, while
the sentence supervises the generated tokens in a teacher-
forcing way [25]. During the evaluation phase, we input the
victim embedding as the initial hidden state to the LSTM
decoder and decode the tokens greedily. For background on
training and evaluating such an LSTM module for generating
sentence conditionally, please refer to e.g., [68]. To conduct the
keyword inference attack, we suppose the adversary directly
tests whether the keyword is contained in the generated
sentence. We conduct the decoder-based keyword inference
attack on Medical in the white-box setting, with exactly the
same conﬁgurations of the dataset. Fig. 10 reports the training
loss and the keyword inference attack accuracy in the ﬁrst 5
epochs. We omit the results for Transformer-XL because the
decoder cannot be trained on a 11G GPU due to the large
vocabulary containing over 220, 000 tokens.
Results & Analysis. As we can see from Fig. 10, for the
LSTM decoder on each language model we experiment with,
the training loss decreases to almost 0 in the ﬁrst several
epochs. In the console logs, we correspondingly observe
that, when the loss is close to 0,
the decoded sentences
in the training set is almost identical to the original ones.
However, when applied to decode from the embedding without
teacher-forcing, the decoder is observed to fail to decode any
meaningful sentences, always giving a sequence of repetition
of certain random word. As a result, none of the decoder-based
attacks work out better than a random guesser.
G. Experimental Environments
All the experiments were implemented with PyTorch [51],
which is an open-source software framework for numeric com-
putation and deep learning. We used the pretrained language
THROUGHPUT FOR TRAINING ATTACKS IN EACH CASE (ITER/SEC.)
TABLE VI
Citizen
Month
228.2
Date
213.7
Genome
311.2
Year
187.9
Airline/Medical
SVM
7209.6
MLP
352.3
DANN
316.8
models implemented by PaddlePaddle5 (for Ernie 2.0) and by
HuggingFace [74] (for other seven models). We deployed the
language models on a Linux server running Ubuntu 16.04, one
AMD Ryzen Threadripper 2990WX 32-core processor and 2
NVIDIA GTX RTX2080 GPUs. We conducted our attacks
and defenses on a Linux PC running Ubuntu 16.04, one Intel
Core i7 processor and 1 NVIDIA GTX 1070 GPU, querying
the server via local network. We report the time for quering
one mini-batch of training data in Table IV.
H. Other Omitted Statistics
BASIC INFORMATION OF THE THREE VARIANTS OF GPT-2 ARCHITECTURE
WE HAVE USED FOR ABLATION STUDIES.
TABLE VII
Name
Dimension
GPT-2
GPT-2-Medium
GPT-2-Large
768
1024
1280
# of Parameters
1.2 × 108
3.5 × 108
7.7 × 108
STATISTICS OF TEST SAMPLES FOR EACH KEYWORD ON AIRLINE &
TABLE VIII
MEDICAL
Airline
Medical
Hong Kong
808
Sydney
1434
leg
19804
head
4988
London
2656
Toronto
1320
Paris
948
Rome
736
Dubai
802
hand
3700
hip
2612
Bangkok
Singapore
Frankfurt
1260
spine
6222
arm
18600
1264
chest
3172
face
3938
586
ankle
1252
shoulder
2592
I. Learning Algorithm for DANN Attack
5https://github.com/PaddlePaddle/ERNIE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1331