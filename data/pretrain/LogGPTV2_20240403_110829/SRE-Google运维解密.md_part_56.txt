复合Paxos的信息流的过程在本章前面“复合式Paxos：消息流过程详解”
的，但是总体来说，大致在1毫秒到数毫秒之间。
将某条记录写入磁盘上的记录的时间根据所用硬件或者所用的虚拟化环境差距是很大
到持久化存储中，那么该接收者就可能会违反协议要求，从而造成数据不一致的情况。
的提议，就不能再接收低序列号的提议了。如果之前接收的和承诺过的记录没有被记录
到持久化存储中。例如，在Paxos协议中，某个接收者如果已经接收过一个更高序列号
为了使某个崩溃的节点返回集群后仍能够记住崩溃之前的状态，该节点需要将请求记录
磁盘访问
[Lam98])。
和[Sanll]。
会违反复制状态机所需要的全局排序机制。针对这种优化的详细讨论请参见文献[Bol11]
流水线中的一批请求仍然是用一个视图序号和交易序号全局排序的，所以这个方法并不
种优化与TCP/IP的滑动窗口机制很像。流水线机制通常会和批处理机制结合使用。
态的。针对这种情况，我们可以采用流水线机制，从而使得多个提议可以同时进行。这
统吞吐量。但是即使这样，某个共识组副本在等待其他人回复的时候仍然是处于闲置状
批处理机制，如上一页“快速Paxos协议：性能优化”
批处理
客户端到各成员的RTT要低。
领头人选举机制可以使客户端所见的延迟降低，因为领头人与共识组成员的RTT将会比
在广域网络中，在客户端分布在不同的地理区域而共识组分布相对集中的情况下，这种
·回复消息的传递。
提议者的一次硬盘写入操作。
每个接收者的磁盘写操作（并行)。
并行消息发送给接收者。
第23章
管理关键状态：利用分布式共识来提高可靠性
—因为这条Accept 消息其实是一个隐含的Accepted消息（参见文献
一节所描述的那样，可以增加系
一节中描述
---
## Page 305
维护时继续工作（这里假设其余两个副本可以承受系统负载）。
能够承受副本返回错误结果的情况，则需要3f+1个副本来承受f个副本失败（参看文献
共识组可以同时承受f个副本失败而继续运行（如果需要容忍拜占庭式失败，也就是要
副本的数量
系统设计者部署共识系统时，最重要的决策在于选择副本的数量和对应的部署位置。
分布式共识系统的部署
写入磁盘，可以进行重新排序以便更有效地写入（参见文献[Boll1]）。
日志的写操作必须直接刷写到硬盘，但是数据的改变通常可以写入内存缓存中，稍后再
在数据存储系统中，磁盘在维护日志之外还有其他用处：数据通常也是存放于磁盘中的。
更多操作，该系统也可以每秒处理更多操作。
（参见文献[Bol11])，也就降低了磁盘寻址操作消耗的时间。这些磁盘于是可以每秒处理
易日志合为一个。将这两个日志合并可以避免不停地向磁盘上的两个不同位置交替写入
日志，以便用于灾难恢复（正如其他数据存储那样）。共识算法的日志可以和RSM的交
正如上文所述，分布式共识算法经常用来实现一个复制状态机。RSM需要保留一份交易
并行进行写操作的。
每分钟100次。这些数字假设网络往返时间是可以忽略的，并且提议者是与其他接收者
如果对磁盘的一次小型随机写操作在10ms数量级上，那么共识操作将会被限制为大概
延迟就和发送两条消息，以及法定人数进程并行进行磁盘写操作成比例了。
进程同时并行写入磁盘，并且单独再发送一次Accept消息。于是，在这种情况下，系统
认为提议者的Accept 消息是一个隐含的Accepted 消息。在这个协议下，提议者和其他
系统的不可用通常是不可接受的，于是在这种情况下，我们需要5个副本同时运行，于
如果某个非计划内的失败情况在常规维护时发生了，那么共识系统就会不可用。而共识
时间都是由计划内维护造成的（参见文献[Ken12]）：3个副本使该系统可以在1个副本
则不能承受任何一个副本失败）。3个副本可以承受1个副本失败。大部分系统的不可用
[Cas99]））。针对非拜占庭式失败的情况，最小的副本数量是3一如果仅仅部署两个副本
一般来说，共识系统都要采用“大多数”的法定过程。也就是说，一组2f+1副本组成的
与网络延迟分布到更多操作上，从而提高系统吞吐量。
分布式共识系统的部署
263
<304
---
## Page 306
305
264
于是，针对任何系统的副本数量的考虑都是基于以下几个因素中的一个进行妥协：
每个分片都使用共识算法组成一组。随着分片数量的增多，每个额外副本的数量也会增多。
果副本的运行成本很高，例如Photon（参见文献[Anal3]），该系统使用分片配置，其中
的系统是一个简单集群，那么运行副本的资源可能就不是一个很大的考虑点。然而，如
成本也是一个管理副本时需考虑的因素：每个副本都需要很多的计算资源。如果所讨论
性能就可能越好。
也必须要参与仲裁过程。系统能够承受的失败和落后副本的数量越多，那么整体的系统
系统性能和仲裁过程中不需要的副本数量有直接关系：系统中的一小部分副本可以落后，
多数实例中包含领头人实例，那么就无法保障其他副本的信息及时性。
他进程，领头人进程仍然能保证拥有全部已经提交的提议的信息。反之，如果丢失的大
志中的各种空洞应该如何填补。如果5个实例的Raft系统丢失了除领头人进程之外的其
Raft描述了一个管理分布式日志的方法（参见文献[Ong14]），准确地定义了在分布式日
分布式日志并不总是分布式共识系统理论的一部分，但却是生产系统中很重要的一部分
得最新信息。
是某个速度慢的副本。为了保证系统的鲁棒性，副本必须要进行某种消息同步，以便获
不会讨论如何应对某个崩溃后又恢复的副本（这些副本可能缺少某一段共识信息）或者
变得非常重要。理论性的论文常常指出共识系统可以用来构建一个分布式日志，但是却
仅是等待那些机器恢复可用。当决策做出后，对系统日志的处理（以及对应的监控）就
在灾难处理过程中，系统管理员需要决定是否需要进行这种强制性的重新配置，或者仅
息来填充。但是数据丢失的可能性永远存在一
管理员可以强行改变小组成员列表，将新的副本添加到小组中，使用其他可用成员的信
访问。在这个状态下，有可能某个操作仅仅只在无法访问的副本上执行了。这时，系统
统理论上来说已经进入了一个无法恢复的状态。因为至少有一个副本的持久化日志无法
如果共识系统中大部分的副本已经无法访问，以至于无法完成一次法定进程，那么该系
要立刻增加一个或者两个额外的副本。
行，则不需要进行任何人工干预操作，但是如果仅仅只有3个能正常运行，那么我们需
是可以允许系统在两个副本不可用的情况下继续运行。如果5个副本中有4个能正常运
之间的性能差距很大，那么系统副本的任何一个失败都会影响性能，因为这时慢的副本
·计划内维护操作的频率
·对可靠性的要求
危险性
第23章
管理关键状态：利用分布式共识来提高可靠性
一这种情况应该全力避免。
---
## Page 307
靠的共识系统部署在不同的故障域范围内，我们也必须经常将数据备份在其他地方。
系统的副本也就是该系统数据的在线拷贝。然而，当处理关键数据时，即使已经有了可
需要特别高的吞吐量，或者特别低的延迟：例如，某个共识系统如果只是为了提供成员
常见的故障域包括：
一个故障域（failuredomain）是指系统中可能由于一个故障而同时不可用的一组组件。
的故障域数量。2.系统的延迟性要求。在选择副本位置时，会产生很多复杂问题。
关于共识集群进程的部署位置的决策主要来源于以下两个因素的取舍：1.系统应该承受
副本的位置
更频繁地进行维护性操作；而某些组织使用的硬件成本、质量和可靠性都有所不同。
最后的决策每个系统都会不同：每个系统都有不同的可用性服务水平目标；某些组织会
我们考虑副本位置时，应该将灾难恢复考虑在内：在某个存储关键数据的系统中，共识
出都没有任何回报。
没有实际价值。这种额外的成本，不管是延迟上的、吞吐量上的，还是计算资源上的付
是这是不是值得呢？很有可能不值得，因为该系统的所有客户端都无法工作了，该系统
式系统可以使得在这个故障域出现故障时继续工作（例如，龙卷风Sandy发生时），但
有客户端都在某个故障域内（例如，纽约州范围内），虽然在广域范围内部署一个分布
有的时候，不断增加系统可承受的故障域大小并不一定合理。例如，如果共识系统的所
处理数量的增加来提高吞吐量。
小部分时，性能并不是关键点。批处理居多的系统也很少会被延迟所影响：可以通过批
信息和领头人选举，服务一般不会负载很高，如果共识操作时间仅仅是领头人租约的一
对延迟的敏感性和对灾难的承受程度，每个系统很不一样。在某些共识系统架构下并不
迟的增加。
够承受的失败程度也会增加。对多数共识系统来说，网络往返时间的增加会导致操作延
一般来说，随着副本之间的距离增加，副本之间的网络往返时间也会增加，但是系统能
·处于同一个地理区域的一组数据中心，可能被同一个自然灾害所影响。
●成本
·性能
物理机器。
受单个光纤影响的一个数据中心。
数据中心中的数个机柜，使用同一个网络设备连接。
数据中心中用同一个供电设施的一个机柜。
分布式共识系统的部署
265
团
<307
---
## Page 308
0
266
图23-10：向系统中一个区域增加一个额外的副本可能会影响系统可用性。
是超过33%的副本不可用就会导致系统无法工作。
40%不可用的情况下仍然正常工作。当使用6个副本时，仲裁过程需要4个副本：也就
用5个副本，一个法定仲裁过程需要3个副本参与。整个系统可以在两个副本，也就是
低系统的可用性，如图23-10所示。对ZooKeeper和Chubby来说，一个常见的部署使
这里需要说明的是，向一个采取“大多数”法定仲裁过程系统中增加新的副本可能会降
增加副本可能是最好的选择。
所有进程的负载。然而，如果写操作有足够容量，而大量的读操作正在给系统造成压力
算法中，增加副本会增加领头人进程的负载，在P2P协议中增加一个副本则会增加其他
可以通过添加副本来提高读性能。增加更多的副本也有成本：在使用强势领头人过程的
以通过调整分片的数量来调整容量。然而，对那些可以从副本直接读取数据的系统来说
当设计某个部署场景时，我们必须保证有足够容量应对系统负载。在分片式部署时，可
容量规划和负载均衡
进行读操作而不需要进行共识操作时。
和EgalitarianPaxos可能有一定的性能优势，尤其是当应用程序设计为可以对任意副本
共识系统之间的RTT应该是最小化的。在广域网络中，无领头人的协议，例如Mencius
当决定副本位置的时候，记得最关键的因素是客户端可见性能：理想情况下，客户端和
情况。而人工操作可能会执行错误的命令，从而造成数据损坏。
中的Bug可能会在罕见情况下浮现造成数据丢失，而错误的系统配置也可能会造成类似
为有两个故障域是我们永远无法逃避的：软件本身和系统管理员的人为错误。软件系统
第23章管理关键状态：利用分布式共识来提高可靠性
现故障时整个共识组没有任何备用副本可用。
将多个副本放在同一个数据中心中可能会减少系统可用性：这里展示了当该数据中心出
---
## Page 309
中心，要么是平均分配，要么会涌入同一个数据中心。在这两种情况中，其他两个数据
样做可以保证系统不会由于一个数据中心的出口造成瓶颈，于是可以提升系统的整体
时启动时。
最终造成连锁故障。这种过载通常是由批处理任务触发的，尤其是当多个批处理任务同
域的一次负载升高就会导致最近的副本过载，接着将第二近的副本吞没，以此类推，
情况。如图23-11所示，如果一个系统简单地将读请求发往最近的副本，那么在某个区
当考虑究竟将副本放在哪里时，我们还需要考虑负载均衡机制，以及系统如何应对过载
如果客户端集中于某个物理区域，那么最好将系统副本放置在离客户端近的地方。然而，
么该数据中心的故障会同时造成该共识组的两个备用副本同时不可用，也就是将容量减
其他组中留下某个备用副本。如果第6个副本也被部署到这5个数据中心中的一个，那
并且通常在5个数据中心中各运行1个共识组副本，那么某个数据中心的故障仍可以在
在添加了第6个副本时，针对故障域的考虑就更重要了：如果某个组织有5个数据中心，
切等）。如图23-12所示，在这个灾难场景中，所有的领头人都应该切换到另外一个数据
据中心出现大规模故障时，整个系统会剧烈变动（如电源故障、网络设备故障、光纤被
将共识组置于不同的数据中心（参见图23-11）中的另外一个劣势是当领头人所处的数
性能。
统的组织可能会发现，保证领头人进程在数据中心之间分布相对均衡是很有必要的。这
数据的提议，但是其他副本发送的都只是回应数据，相对较小。运行高度分片的共识系