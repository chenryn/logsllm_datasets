or simply incorrect) values of r’s attributes, modeled as
an arbitrary probabilistic function Aux: X M → X M.
The attributes given to the adversary may be chosen
uniformly from the support of r, or according to some
other rule.2 Given this auxiliary information and an
anonymized sample ˆD of D, the adversary’s goal is
to reconstruct attribute values of the entire record r.
Note that there is no artiﬁcial distinction between quasi-
identiﬁers and sensitive attributes.
If the published records are sanitized by adding ran-
dom noise ZS, and the noise used in generating Aux is
ZA, then the adversary’s task is equivalent to the sce-
nario where the data are not perturbed but noise ZS +ZA
is used in generating Aux. This makes perturbation
equivalent to imprecision of Aux.
Privacy breach: formal deﬁnitions. What does it mean
to de-anonymize a record r? The naive answer is to
ﬁnd the “right” anonymized record in the public sample
ˆD. This is hard to capture formally, however, because it
requires assumptions about the data publishing process
(e.g., what if ˆD contains two copies of every original
record?). Fundamentally, the adversary’s objective is is
to learn as much as he can about r’s attributes that he
doesn’t already know. We give two different (but re-
lated) formal deﬁnitions, because there are two distinct
scenarios for privacy breaches in large databases.
The ﬁrst scenario is automated large-scale de-
anonymization. For every record r about which he has
some information, the adversary must produce a single
2For example, in the Netﬂix Prize case study we also pick uni-
formly from among the attributes whose supports are below a certain
threshold, e.g., movies that are outside the most popular 100 or 500
movies.
“prediction” for all attributes of r. An example is the
attack that inspired k-anonymity [25]: taking the demo-
graphic data from a voter database as auxiliary informa-
tion, the adversary joins it with the anonymized hospital
discharge database and uses the resulting combination to
determine the values of medical attributes for each per-
son who appears in both databases.
Deﬁnition 2 A database D can be (θ, ω)-deanonymized
w.r.t. auxiliary information Aux if there exists an algo-
rithm A which, on inputs D and Aux(r) where r ← D
outputs r(cid:1)
such that
Pr[Sim(r, r(cid:1)
) ≥ θ] ≥ ω
Deﬁnition 2 can be interpreted as an ampliﬁcation of
background knowledge: the adversary starts with aux =
Aux(r) which is close to r on a small subset of attributes,
and uses this to compute r(cid:1)
which is close to r on the
entire set of attributes. This captures the adversary’s
ability to gain information about his target record.
As long he ﬁnds some record which is guaranteed to be
very similar to the target record, i.e., contains the same
or similar attribute values, privacy breach has occurred.
If operating on a sample ˆD, the de-anonymization al-
gorithm must also detect whether the target record is part
of the sample, or has not been released at all. In the fol-
lowing, the probability is taken over the randomness of
the sampling of r from ˆD, Aux and A itself.
Deﬁnition 3 (De-anonymization) An arbitrary subset
ˆD of a database D can be (θ, ω)-deanonymized w.r.t.
auxiliary information Auxif there exists an algorithm A
which, on inputs ˆD and Aux(r) where r ← D
• If r ∈ ˆD, outputs r(cid:1)
s.t. Pr[Sim(r, r(cid:1)) ≥ θ] ≥ ω
• if r /∈ ˆD, outputs ⊥ with probability at least ω
The same error threshold (1 − ω) is used for both
false positives and false negatives because the parame-
ters of the algorithm can be adjusted so that both rates
are equal; this is the “equal error rate.”
In the second privacy breach scenario, the adversary
produces a set or “lineup” of candidate records that in-
clude his target record r, either because there is not
enough auxiliary information to identify r in the lineup
or because he expects to perform additional analysis to
complete de-anonymization. This is similar to commu-
nication anonymity in mix networks [24].
The number of candidate records is not a good met-
ric, because some of the records may be much likelier
candidates than others. Instead, we consider the prob-
ability distribution over the candidate records, and use
114
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
as the metric the conditional entropy of r given aux. In
the absence of an “oracle” to identify the target record
r in the lineup, the entropy of the distribution itself can
be used as a metric [24, 10]. If the adversary has such
an “oracle” (this is a technical device used to measure
the adversary’s success; in the real world, the adver-
sary may not have an oracle telling him whether de-
anonymization succeeded), then privacy breach can be
quantiﬁed as follows: how many bits of additional in-
formation does the adversary need in order to output a
record which is similar to his target record?
1
that
after
the
suppose
, . . . r(cid:1)
Thus,
executing the de-
anonymization algorithm,
adversary outputs
records r(cid:1)
k and the corresponding probabilities
p1, . . . pk. The latter can be viewed as an entropy
encoding of the candidate records. According to Shan-
non’s source coding theorem, the optimal code length
i is (− log pi). We denote by HS(Π, x)
for record r(cid:1)
this Shannon entropy of a record x w.r.t. a probability
distribution Π. In the following, the expectation is taken
over the coin tosses of A, the sampling of r and Aux.
Deﬁnition 4 (Entropic de-anonymization) A
(θ, H)-deanonymized w.r.t.
database D can be
auxiliary information Auxif there exists an algorithm A
which, on inputs D and Aux(r) where r ← D outputs a
set of candidate records D(cid:1)
and probability distribution
Π such that
E[minr(cid:1)∈D(cid:1),Sim(r,r(cid:1))≥θHS(Π, r(cid:1)
)] ≤ H
This deﬁnition measures the minimum Shannon en-
tropy of the candidate set of records which are similar to
the target record. As we will show, in sparse databases
this set is likely to contain a single record, thus taking
the minimum is but a syntactic requirement.
When the minimum is taken over an empty set, we
deﬁne it to be H0 = log2
N , the a priori entropy of
the target record. This models outputting a random
record from the entire database when the adversary can-
not compute a lineup of plausible candidates. Formally,
the adversary’s algorithm A can be converted into an al-
gorithm A(cid:1)
, which outputs the mean of two distributions:
one is the output of A, the other is the uniform distribu-
tion over D. Observe that for A(cid:1)
, the minimum is always
taken over a non-empty set, and the expectation for A(cid:1)
differs from that for A by at most 1 bit.
Chawla et al. [8] give a deﬁnition of privacy breach
via isolation which is similar to ours, but requires a met-
ric on attributes, whereas our general similarity measure
does not naturally lead to a metric (there is no feasible
way to derive a distance function from it that satisﬁes
the triangle inequality). This appears to be essential for
achieving robustness to completely erroneous attributes
in the adversary’s auxiliary information.
4 De-anonymization algorithm
We start by describing an algorithm template or meta-
algorithm. The inputs are a sample ˆD of database D
and auxiliary information aux = Aux(r), r ← D. The
output is either a record r(cid:1) ∈ ˆD, or a set of candidate
records and a probability distribution over those records
(following Deﬁnitions 3 and 4, respectively).
The three main components of the algorithm are the
scoring function, matching criterion, and record selec-
tion. The scoring function Score assigns a numerical
score to each record in ˆD based on how well it matches
the adversary’s auxiliary information Aux. The match-
ing criterion is the algorithm applied by the adversary
to the set of scores to determine if there is a match. Fi-
nally, record selection selects one “best-guess” record
or a probability distribution, if needed.
1. Compute Score(aux, r(cid:1)) for each r(cid:1) ∈ ˆD.
2. Apply the matching criterion to the resulting set of
scores and compute the matching set; if the match-
ing set is empty, output ⊥ and exit.
3. If a “best guess” is required (de-anonymization ac-
cording to Defs. 2 and 3), output r(cid:1) ∈ ˆD with the
highest score. If a probability distribution over can-
didate records is required (de-anonymization ac-
cording to Def. 4), compute and output some non-
decreasing distribution based on the scores.
Algorithm Scoreboard. The following simple instan-
tiation of the above template is sufﬁciently tractable to
be formally analyzed in the rest of this section.
• Score(aux, r(cid:1)) = mini∈supp(aux)Sim(auxi, r(cid:1)
i),
i.e., the score of a candidate record is determined
by the least similar attribute between it and the ad-
versary’s auxiliary information.
• The matching set D(cid:1) = {r(cid:1)
ˆD :
Score(aux, r(cid:1)) > α} for some ﬁxed constant α.
The matching criterion is that D(cid:1)
be nonempty.
∈
• Probability distribution is uniform on D(cid:1)
.
Algorithm Scoreboard-RH. Algorithm Scoreboard
is not sufﬁciently robust for some applications; in par-
ticular, it fails if any of the attributes in the adversary’s
auxiliary information are completely incorrect.
115
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 03:12:32 UTC from IEEE Xplore.  Restrictions apply. 
The following algorithm incorporates several heuris-
tics which have proved useful in practical analysis (see
section 5). First,
the scoring function gives higher
weight to statistically rare attributes. Intuitively, if the
auxiliary information tells the adversary that his target
has a certain rare attribute, this helps de-anonymization
much more than the knowledge of a common attribute
(e.g., it is more useful to know that the target has pur-
chased “The Dedalus Book of French Horror” than the
fact that she purchased a Harry Potter book).
Second, to improve robustness, the matching crite-
rion requires that the top score be signiﬁcantly above the
second-best score. This measures how much the identi-
ﬁed record “stands out” from other candidate records.
i∈supp(aux) wt(i)Sim(auxi, r(cid:1)
i)
• Score(aux, r(cid:1))=
(cid:1)
where wt(i) =
1
log |supp(i)| . 3
σ
• If a “best guess” is required, compute max =
max(S), max2 = max2(S) and σ = σ(S) where
S = {Score(aux, r(cid:1)) : r(cid:1) ∈ ˆD}, i.e., the highest
and second-highest scores and the standard devia-
tion of the scores. If max−max2
< φ, where φ is a
ﬁxed parameter called the eccentricity, then there
is no match; otherwise, the matching set consists of
the record with the highest score.4
• If entropic de-anonymization is required, output
distribution Π(r(cid:1)) = c · e Score(aux,r(cid:1))
for each r(cid:1)
,
where c is a constant that makes the distribution
sum up to 1. This weighs each matching record in
inverse proportion to the likelihood that the match
in question is a statistical ﬂuke.
σ
assumptions about the distribution from which the data
are drawn. In section 4.2, we will show that much less
auxiliary information is needed to de-anonymize records
drawn from sparse distributions (real-world transaction
and recommendation datasets are all sparse).
Let aux be the auxiliary information about some
record r; aux consists of m (non-null) attribute values,
which are close to the corresponding values of attributes
in r, that is, |aux| = m and Sim(auxi, ri) ≥ 1 −  ∀i ∈
supp(aux), where auxi (respectively, ri) is the ith at-
tribute of aux (respectively, r).
Theorem 1 Let 0 < , δ < 1 and let D be the
database. Let Aux be such that aux = Aux(r) con-
sists of at least m ≥ log N−log 
− log(1−δ) randomly selected
attribute values of the target record r, where ∀i ∈
supp(aux), Sim(auxi, ri) ≥ 1− . Then D can be
(1 −  − δ, 1 − )-deanonymized w.r.t. Aux.
Proof. Use Algorithm Scoreboard with α = 1 − 
to compute the set of all records in ˆD that match aux,
then output a record r(cid:1)
at random from the matching set.
It is sufﬁcient to prove that this randomly chosen r(cid:1)
must
be very similar to the target record r. (This satisﬁes our
deﬁnition of a privacy breach because it gives the adver-
sary almost everything he may want to learn about r.)
is a false match if Sim(r, r(cid:1)) ≤ 1−−δ (i.e.,
the likelihood that r(cid:1)
is similar to the target r is below
the threshold). We ﬁrst show that, with high probability,
there are no false matches in the matching set.
Record r(cid:1)
Lemma 1 If
is
i) ≥ 1 − ] < 1 − δ
Pri∈supp(r)[Sim(ri, r(cid:1)
false
a
match,
then
r(cid:1)
Note that there are two ways in which this algorithm
can fail to ﬁnd the correct record. First, an incorrect
record may be assigned the highest score. Second, the
correct record may not have a score which is signiﬁ-
cantly higher than the second-highest score.
4.1 Analysis: general case
We now quantify the amount of auxiliary informa-
tion needed to de-anonymize an arbitrary dataset using
Algorithm Scoreboard. The smaller the required in-
formation (i.e., the fewer attribute values the adversary