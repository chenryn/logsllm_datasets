Consequently, it is not necessary to measure the conversion rate
to understand proﬁtability. Instead, proﬁtability can be inferred by
correlating stock spam message volume with changes in the trading
volume and price for the associated stocks.
The work of Ma and Chen is similar to ours in that it analyzes in
detail the structure of a spamming operation. However, their focus
is on redirection chains employed by spammers as a search engine
optimization strategy [20].
3. THE STORM BOTNET
The measurements in this paper are carried out using the Storm
botnet and its spamming agents. While a complete technical de-
scription of Storm is outside the scope of this paper, we review
key mechanisms in Storm’s communication protocols and organi-
zational hierarchy.
Storm is a peer-to-peer botnet that propagates via spam (usu-
ally by directing recipients to download an executable from a Web
site). Storm communicates using two separate protocols: the ﬁrst
is an encrypted version of the UDP-based Overnet protocol (in turn
based on the Kademlia DHT [16]) and is used primarily as a di-
rectory service to ﬁnd other nodes. As well, Storm uses a custom
TCP-based protocol for managing command and control — the di-
rections informing each bot what actions it should take. We de-
scribe each of these below.
3.1 Overnet protocol
There are four basic messages to facilitate the basic functioning
of Overnet: Connect, Search, Publicize, and Publish. During the
bootstrap phase, a Storm node only has the initial list of peers that
it was shipped with. To gather more peers Storm chooses a OID
pseudo-randomly from the 128-bit Overnet address space and pro-
ceeds to Connect to all the peers in its bootstrap list. Each available
peer contacted returns a list of up to 20 peers. Storm does this for
a few rounds until it has gathered enough peers to be adequately
connected in Overnet. Once a new node has learned about enough
peers it switches to Publicizing its presence to nearby peers and
provider on TCP port 25. If this check fails the worker will remain
active but not participate in spamming campaigns.4
Figure 2 outlines the broad steps for launching spam campaigns
when the port check is successful. The worker ﬁnds a proxy (using
the time-varying protocol described earlier) and then sends an up-
date request (via the proxy) to an associated master server (Step 1),
which will respond with a spam workload task (Step 2). A spam
workload consists of three components: one or more spam tem-
plates, a delivery list of e-mail addresses, and a set of named “dic-
tionaries”. Spam templates are written in a custom macro language
for generating polymorphic messages [15]. The macros insert ele-
ments from the dictionaries (e.g., target e-mail addresses, message
subject lines), random identiﬁers (e.g., SMTP message identiﬁers,
IP addresses), the date and time, etc., into message ﬁelds and text.
Generated messages appear as if they originate from a valid MTA,
and use polymorphic content for evading spam ﬁlters.
Upon receiving a spam workload, a worker bot generates a
unique message for each of the addresses on the delivery list and
attempts to send the message to the MX of the recipient via SMTP
(Step 3). When the worker bot has exhausted its delivery list, it
requests two additional spam workloads and executes them. It then
sends a delivery report back to its proxy (Step 4). The report in-
cludes a result code for each attempted delivery. If an attempt was
successful, it includes the full e-mail address of the recipient; oth-
erwise, it reports an error code corresponding to the failure. The
proxy, in turn, relays these status reports back to the associated
master server.
To summarize, Storm uses a three-level self-organizing hierarchy
comprised of worker bots, proxy bots and master servers. Com-
mand and control is “pull-based”, driven by requests from individ-
ual worker bots. These requests are sent to proxies who, in turn,
automatically relay these requests to master servers and similarly
forward any attendant responses back to to the workers.
4. METHODOLOGY
Our measurement approach is based on botnet inﬁltration — that
is, insinuating ourselves into a botnet’s “command and control”
(C&C) network, passively observing the spam-related commands
and data it distributes and, where appropriate, actively changing
individual elements of these messages in transit. Storm’s archi-
tecture lends itself particularly well to inﬁltration since the proxy
bots, by design, interpose on the communications between individ-
ual worker bots and the master servers who direct them. Moreover,
since Storm compromises hosts indiscriminately (normally using
malware distributed via social engineering Web sites) it is straight-
forward to create a proxy bot on demand by infecting a globally
reachable host under our control with the Storm malware.
Figure 2 also illustrates our basic measurement infrastructure. At
the core, we instantiate eight unmodiﬁed Storm proxy bots within a
controlled virtual machine environment hosted on VMWare ESX 3
servers. The network trafﬁc for these bots is then routed through a
centralized gateway, providing a means for blocking unanticipated
behaviors (e.g., participation in DDoS attacks) and an interposition
point for parsing C&C messages and “rewriting” them as they pass
from proxies to workers. Most critically, by carefully rewriting the
spam template and dictionary entries sent by master servers, we ar-
range for worker bots to replace the intended site links in their spam
with URLs of our choosing. From this basic capability we synthe-
size experiments to measure the click-through and conversion rates
for several large spam campaigns.
4Such bots are still “useful” for other tasks such as mounting coor-
dinated DDoS attacks that Storm perpetrates from time to time.
Figure 1: The Storm botnet hierarchy.
periodically searching for its own OID to stay connected and learn
about new close-by peers to keep up with churn.
Overnet also provides two messages for storing and ﬁnding con-
tent in the network: Publish and Search which export a standard
DHT (key,value) pair interface. However, Storm uses this inter-
face in an unusual way. In particular, the keys encode a dynam-
ically changing rendezvous code that allow Storm nodes to ﬁnd
each other on demand.
A Storm node generates and uses three rendezvous keys simulta-
neously: one based on the current date, one based on the previous
date, and one based on the next date. To determine the correct date,
Storm ﬁrst sets the system clock using NTP.
In particular, each key is based on a combination of the time
(with 24-hour resolution) mixed with a random integer between 0
and 31. Thus there are 32 unique Storm keys in use per day but
a single Storm bot will only use 1 of the 32. Because keys are
based on time, Storm uses NTP to sync a bot’s clock and attempts
to normalize the time zone. Even so, to make sure bots around
the world can stay in sync, Storm uses 3 days of keys at once, the
previous, current, and next day.
In turn, these keys are used to rendezvous with Storm nodes that
implement the command and control (C&C) channel. A Storm
node that wishes to offer the C&C service will use the time-based
hashing algorithm to generate a key and encode its own IP address
and TCP port into the value. It will then search for the appropriate
peers close to the key and publish its (key, value) pair to them. A
peer wishing to locate a C&C channel can generate a time-based
key and search for previously published values to decode and con-
nect to the TCP network.
3.2 Storm hierarchy
There are three primary classes of Storm nodes involved in send-
ing spam (shown in Figure 1). Worker bots make requests for work
and, upon receiving orders, send spam as requested. Proxy bots
act as conduits between workers and master servers. Finally, the
master servers provide commands to the workers and receive their
status reports. In our experience there are a very small number of
master servers (typically hosted at so-called “bullet-proof” hosting
centers) and these are likely managed by the botmaster directly.
However, the distinction between worker and proxy is one that
is determined automatically. When Storm ﬁrst infects a host it tests
if it can be reached externally. If so, then it is eligible to become a
proxy. If not, then it becomes a worker.
3.3 Spam engine
Having decided to become a worker, a new bot ﬁrst checks
whether it can reach the SMTP server of a popular Web-based mail
the proxy server needs to maintain a connection for each of the
(many) workers, we use a preforked, multithreaded design. A pool
of 30 processes allowed us to handle the full worker load for the
eight Storm proxy bots at all times.
4.2 Measuring spam delivery
To evaluate the effect of spam ﬁltering along the e-mail delivery
path to user inboxes, we established a collection of test e-mail ac-
counts and arranged to have Storm worker bots send spam to those
accounts. We created multiple accounts at three popular free e-mail
providers (Gmail, Yahoo!, and Hotmail), accounts ﬁltered through
our department commercial spam ﬁltering appliance (a Barracuda
Spam Firewall Model 300 with slightly more permissive spam tag-
ging than the default setting), and multiple SMTP “sinks” at dis-
tinct institutions that accept any message sent to them (these served
as “controls” to ensure that spam e-mails were being successfully
delivered, absent any receiver-side spam ﬁltering). When worker
bots request spam workloads, our rewriter appends these e-mail
addresses to the end of each delivery list. When a worker bot re-
ports success or failure back to the master servers, we remove any
success reports for our e-mail addresses to hide our modiﬁcations
from the botmaster.
We periodically poll each e-mail account (both inbox and
“junk/spam” folders) for the messages that it received, and we log
them with their timestamps. However, some of the messages we
receive have nothing to do with our study and must be ﬁltered
out. These messages occur for a range of reasons, including spam
generated by “dictionary bots” that exhaustively target potential e-
mail addresses, or because the addresses we use are unintentionally
“leaked” (this can happen when a Storm worker bot connects to
our proxy and then leaves before it has ﬁnished sending its spam;
when it reconnects via a new proxy the delivery report to the mas-
ter servers will include our addresses). To ﬁlter such e-mail, we
validate that each message includes both a subject line used by our
selected campaigns and contains a link to one of the Web sites un-
der our control.
4.3 Measuring click-through and conversion
To evaluate how often users who receive spam actually visit the
sites advertised requires monitoring the advertised sites themselves.
Since it is generally impractical to monitor sites not under our con-
trol, we have arranged to have a fraction of Storm’s spam advertise
sites of our creation instead.
In particular, we have focused on two types of Storm spam cam-
paigns, a self-propagation campaign designed to spread the Storm
malware (typically under the guise of advertising an electronic
postcard site) and the other advertising a pharmacy site. These are
the two most popular Storm spam campaigns and represent over
40% of recent Storm activity [15].
For each of these campaigns, the Storm master servers distribute
a speciﬁc “dictionary” that contains the set of target URLs to be in-
serted into spam e-mails as they are generated by worker bots. To
divert user visits to our sites instead, the rewriter replaces any dic-
tionaries that pass through our proxies with entries only containing
URLs to our Web servers.
In general, we strive for verisimilitude with the actual Storm op-