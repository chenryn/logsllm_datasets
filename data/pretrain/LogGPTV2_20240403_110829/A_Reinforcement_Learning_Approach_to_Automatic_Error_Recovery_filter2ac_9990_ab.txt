Figure 2: Q-learning algorithm for optimal pol-
icy generation 
As stated in the previous section, we use Q-learning 
algorithm to obtain repair policy. The training process 
is  implemented  by  applying  Q-learning  algorithm  to 
each  error  type,  which  can  be  inferred  from  error 
symptoms  in  the  recovery  log.  The  procedure  de-
scribed  in  Figure  2  is  iteratively  used  on  the  recovery 
log to get an optimal Q-function. In the following sec-
tions, we provide a closer look at each key step. 
3.1. Error type inference and noise filtering 
In  this  paper,  we  attempt  to  extract  potential  faults 
based on the error symptoms in the recovery log.  
To  get  a  rough  idea  of  how  symptoms  are  distri-
buted,  we  generate  a  number  of  symptom  sets  from  a 
real-world  recovery  log  (to  be  introduced  in  section 
4.1). In each set, the symptoms are highly related based 
on  the  ratio  of  the  number  of  recovery  processes  in 
which  they  appear  together  out  of  all  the  recovery 
processes  in  which  one  symptom  appears.  Due  to  the 
fact that some symptoms may occur quite infrequently, 
we  use  m-pattern  algorithm  [19],  which  is  capable  of 
finding infrequent but highly correlated items, to mine 
mutually dependent symptoms in the log. The strength 
of mutual dependence is measured by parameter minp.  
We  summarize  the  percentage  of  the  recovery 
processes  with  only  highly  dependent  symptoms  for 
various  dependence  strength  in  Figure  3.  We  can  ob-
serve that the whole log is mainly made up of a number 
of highly cohesive symptom sets. Additionally, we find 
different sets share few intersections. This motivates us 
to  generate  policy  at  symptom  level  since  we  do  not 
have  any  knowledge  about  real  faults.  Actually,  we 
think  the  symptom  sets  may  have  strong  correlation 
with  the faults in the  system. Based on these observa-
tions, we define error type as the initial symptom of a 
recovery  process  to  approximate  the  real  fault.  For 
example,  if  the  sequence  of  symptoms  occurring  dur-
ing a recovery process is “A; B; C”, then we use symp-
tom “A” to represent its error type. We choose the ini-
tial  symptom  since  it  is  usually  representative  enough 
of  the  symptom  set  to  which  it  belongs  and  the  other 
symptoms in the recovery process often co-occur  with 
it.  Based on this definition, we employ the error type 
as the unit in building recovery policies. 
1
0.95
0.9
0.85
0.8
0.75
e
g
a
t
n
e
c
r
e
P
Symptom Sets
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
1
minp
Figure 3: Symptom sets extracted from recov-
ery log 
Moreover,  we  still  need  do  some  noise  filtering 
based  on  the  above  results  because  the  evaluation  is 
based on a simulation platform in our experiments and 
those noise data are often too difficult to simulate and 
may  impact  the  precision  of  the  evaluation.  Actually 
we  choose  minp  =  0.1  in  m-pattern  algorithm,  and  ul-
timately get 119 symptom clusters covering 96.67 % of 
the  total  logs.  The  left  3.33%  are  regarded  as  noisy 
cases that may contain more than one error. The noise 
data  only  take  up  a  trivial  part  within  the  logs,  so  the 
filtering  process  does  not  influence  the  conclusions 
much.  Although  our  RL  approach  can  also  be  applied 
to these noisy cases,  we still ignore them to get a pre-
cise evaluation. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20073.2. State transition 
We  use  error  types  (beginning  symptoms)  and  pre-
viously tried repair actions to define states. A state ݏ௧ is 
represented  by  a  tuple ሺ݁,ݎ,ሺܽ௢, ܽଵ,…,ܽ௧ିଵ ሻሻ,  where 
݁  is  error  type, ݎ  is  the  recovery  result  (failure  or 
health) before time ݐ, and ܽ௜, ݅ ൌ 0,1,… ,ݐ െ 1, are all 
repair  actions  executed  before.  From  this  definition,  it 
is obvious that before the last repair action the recovery 
tomatic error recovery a Markov decision process.  
result ݎ of  any  state  will  be ݂ (failure)  and  after  that  it 
will  become ݄ (health).  This  definition  also  makes  au-
Transition function, ߜ, here is partially known, since 
the  state ݏ௧ାଵ produced  by  the  acts  on ݏ௧ ൌ ሺ݁,݂,ሺܽ଴,
௙ ൌ
ܽଵ,…,ܽ௧ିଵሻሻ and ܽ௧  could  only  be  two  types, ݏ௧ାଵ
ሺ݁,݂,ሺܽ଴, ܽଵ,…,ܽ௧ିଵ, ܽ௧ሻሻ  or  ݏ௧ାଵ௛ ൌ ሺ݁,݄,ሺܽ଴, ܽଵ,… ,
ܽ௧ିଵ, ܽ௧ሻሻ,  the  probabilities  of  which  depend  on  the 
environment and properties of the errors. So the equa-
tion (3) could be rewritten as 
௙
ܳሺݏ௧,ܽ௧ሻ ൌ ܧሾܿሺݏ௧,ܽ௧ሻሿ ൅ ݍሺݏ௧,ܽ௧ሻmin௔೟శభ ܳሺݏ௧ାଵ
,ܽ௧ାଵሻ 
௙ ൌ  ߜሺݏ௧,ܽ௧ሻ                   ሺ4ሻ 
,where ݏ௧ାଵ
Figure  4  illustrates  the  decisions  and  possible  se-
quences in a recovery process after an error is detected. 
(
q s a
0
,
0
)
fs
1
0s
fq s a
(
,
1
1
q s a
(
,
f
2
)
fs
2
1
−
( ,
q s a
0
0
)
1
−
fq s a
(
,
1
1
)
1
−
fq s a
(
2
,
2
)
hs
1
hs
2
)
Q s a
0
(
,
0
)
2
fs
3
hs
3
Figure  4:  Error  recovery  process.  The  Q-
function ۿሺܛ,܉ሻ is  the  expected  time  cost  for 
both two directions (failure or health). 
We restrict the count of the repair actions to a finite 
number N for each recovery process (in the experiment 
we set N = 20). It means that if the preceding N-1  re-
pair  actions  fail  to  cure  the  problem,  we  will  end  the 
process  by  requesting  a  manual  repair  (the  recovery 
action  which  is  to  be  conducted  by  human).  Since  all 
policies  produced  with  this  limitation  are  proper,  ac-
cording to the theorem of value contraction in [14], our 
RL method will converge with probability 1. 
3.3. Explore strategy and table update 
To  explore  different  repair  actions,  we  first  need  to 
infer  the  state  transitions  based  on  the  existing  recov-
ery  processes.  This  amounts  to  finding  the  correct  re-
pair actions for each recovery process. The easiest me-
thod  is  to  choose  the  last  repair  action  as  the  correct 
one. However, it may not always be safe to make such 
assumptions  and  sometimes  some  stronger  repair  ac-
tions  also  play  an  important  role  in  the  recovery 
process.  A  more  realistic  assumption  is  to  regard both 
the last action and other stronger actions as the correct 
repair actions. Besides, since a stronger action includes 
the  processes  of  the  weaker  ones,  it  can  at  least  cause 
the same effect as the weaker ones. Based on this anal-
ysis,  our  hypotheses  about  the  recovery  process  is  as 
follows:  
1.  For  any  successful  recovery  process,  we  need  at 
least the same correct repair actions (including the 
last action and the stronger ones in the process) to 
achieve the same recovery result. 
2.  Stronger  actions  can  replace  weaker  ones  in  a 
successful recovery process. 
3.  Recovery  processes  for  different  errors  are  inde-
pendent of each other.  
With  these  hypotheses,  we  can  carry  out  the  explore 
strategy  and  estimate  the  time  cost  for  each  possible 
policy. 
Starting  from  some 
initial  states  of  recovery 
processes,  we  have  to  explore  a  large  enough  state 
space  first  and  then  find  the  optimal  policy.  We  can 
roughly divide the learning course into two phases, one 
for exploration and the other for search. Like the simu-
lated  annealing  algorithm,  we  use  a  temperature ܶ to 
Actually, at time ݐ for certain error ݁, we will utilize 
control the learning course from exploration to search.  
the  following  probability  distribution  (Boltzmann  dis-
tribution) to select a repair action stochastically,  
ܲሺܽ௜|ݏ௧ሻ ൌ ݁ିொሺ௦೟,௔೔ሻ
∑ ݁ିொሺ௦೟,௔೔ሻ
்
௝
,   ܽ௜ א ܣ             ሺ5ሻ 
Here,  the  temperature ܶ will  decrease  with  more  and 
more recovery processes analyzed, so the repair action 
will eventually be selected completely based on Q val-
ues, thus generating the policy. 
    When a repair action is selected, its time cost will be 
estimated  based  on  the  recovery  log.  Specifically,  one 
of the following values will be chosen: actual time cost 
in  the  recovery  process  average  success  time  cost,  or 
average  failing  time  cost.  Based  on  these  values,  we 
can  further update the Q-function and reasonably eva-
luate  the  policy.  As  we  show  in  Section  4.2,  this  ap-
proach works well in our experiments.  
்
Another important step in the  whole training course 
is how to update the Q values. In our method, we chose 
to use a table look-up representation of the Q-function 
and update the  Q values based on the  following equa-
tion,  
          ܳ௡ሺݏ,ܽሻ ՚ ሺ1 െ ߲௡ሻܳ௡ିଵሺݏ,ܽሻ
൅ ߲௡ ቂܿሺݏ,ܽሻ ൅ min௔ᇲ ܳ௡ିଵሺݏᇱ,ܽᇱሻቃ 
௏௜௦௜௧௦೙ሺ௦,௔ሻ                          ሺ6ሻ 
and, ߲௡ ൌ 
ଵ
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007where ܳሺݏ,ܽሻ  records  the  expected  value  of  the  Q-
function,  and ܸ݅ݏ݅ݐݏሺݏ,ܽሻ represents  how  many  times 
ሺݏ,ܽሻ pair is explored. It is easy to prove that this up-
dating  method  is  contracted  and  Q  values  will  even-
tually converge to the optimal ones [20]. 
3.4. Hybrid approach 
Occasionally, the RL-trained policy might fail to re-
pair  some  exceptional  error  cases.  To  get  beyond  this 
issue, we provide a hybrid approach that combines the 
trained policy  with the  user-defined one. In particular, 
if  an  error  still  exists  after  the  last  action  selected  ac-
cording  to  the  trained  policy,  we  will  automatically 
revert  to  the  user-defined  policy.  Since  these  noisy 
cases do not happen frequently, the hybrid policy  can-
not  only  guarantee  to  repair  all  errors  as  well  as  the 
user-defined  policy  does,  but  also  can  maintain  the 
advantage  of  automatic  policy  generation  with  RL,  as 
we show in Section 5.2.  
4. Experimental setup 
This  section  introduces  the  data  used  in  our  experi-
ments  and  the  simulation  platform  that  outputs  feed-
back  of  a  repair  action  on  a  state  -based  on  the  hypo-
theses.  
4.1. Experimental data 
Our experimental data are based on the recovery log 
collected  from  a  large-scale  cluster  system  with  thou-
sands  of  servers  that  contained  more  than  2  million 
entries of error symptoms and repair actions over near-
ly  half  a  year  of  operations.  The  recovery  policy  used 
in  the  real  system  is  user-defined,  which  mainly  tries 
the cheapest action enabled by the state. There are four 
actions  for  repairing  a  machine:  TRYNOP  (simply 
watch  and  do  not  try  any  operation),  REBOOT,  REI-
MAGE  (rebuild  the  operating  system),  and  RMA  (let 
human repair).  
Table  1:  Example  of  recovery  process  (ma-
chine name is omitted). 
Time 
3:07:12 am 
3:10:58 am 
3:23:26 am 
3:25:37 am 
3:27:34 am 
3:42:10 am 
4:13:07 am 
Description (details omitted) 
error:IFM-ISNWatchdog: … 
errorHardware:EventLog: ... 
TRYNOP 
errorHardware:EventLog: ... 
errorHardware:EventLog: ... 
REBOOT 
Success 
The  log  entries  can  be  represented  in  the  format  of 
,  in  which  the  de-
scription can be the repair action, symptom of an error, 
or  report  of  a  successful  recovery  that  occurs  at  the 
recorded  time  on  the  monitored  machine.  Therefore, 
the  logs  can  be  divided  into  an  ensemble  of  recovery 
processes. The processes start with the advent of a new 
error,  experience  a  series  of  repair  actions,  and  end 
with successful recovery. Table 1 gives an example of 
recovery process.  
After noise filtering, we get 97 error types from the 
recovery log with the error type inference method men-
tioned  in  Section  3.1.  To  guarantee  enough  training 
data,  we  choose  the  40  most  frequent  error  types, 
which  constitute  98.68%  of 
recovery 
processes.  
total 
the 
t
n
u
o
C
3500
3000