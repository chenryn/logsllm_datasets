### Figure 2: Q-learning Algorithm for Optimal Policy Generation

As discussed in the previous section, we employ the Q-learning algorithm to generate an optimal repair policy. The training process involves applying the Q-learning algorithm to each error type, which can be inferred from the error symptoms in the recovery log. The procedure illustrated in Figure 2 is iteratively applied to the recovery log to derive an optimal Q-function. In the following sections, we will delve into each key step of this process.

### 3.1. Error Type Inference and Noise Filtering

In this paper, we aim to extract potential faults based on the error symptoms recorded in the recovery log. To understand the distribution of these symptoms, we generate multiple symptom sets from a real-world recovery log (which will be introduced in Section 4.1). Each set contains symptoms that are highly related, as determined by the ratio of the number of recovery processes in which they co-occur to the total number of processes in which one of the symptoms appears. 

Given that some symptoms may occur infrequently, we use the m-pattern algorithm [19] to identify infrequent but highly correlated symptoms in the log. The strength of mutual dependence is measured by the parameter `minp`. Figure 3 summarizes the percentage of recovery processes with only highly dependent symptoms for various values of `minp`. We observe that the log is primarily composed of several highly cohesive symptom sets, with minimal overlap between different sets. This observation motivates us to generate policies at the symptom level, as we lack knowledge about the actual faults. We hypothesize that these symptom sets are strongly correlated with the underlying faults in the system.

Based on these observations, we define the error type as the initial symptom in a recovery process to approximate the real fault. For example, if the sequence of symptoms during a recovery process is "A; B; C," we use symptom "A" to represent the error type. The initial symptom is typically representative of the symptom set and often co-occurs with other symptoms in the recovery process. Using this definition, we employ the error type as the unit for building recovery policies.

**Figure 3: Symptom Sets Extracted from Recovery Log**

| minp | 0.1 | 0.2 | 0.3 | 0.4 | 0.5 | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |
|------|-----|-----|-----|-----|-----|-----|-----|-----|-----|-----|
| Percentage of Recovery Processes | 1.0 | 0.95 | 0.9 | 0.85 | 0.8 | 0.75 | - | - | - | - |

Additionally, we perform noise filtering based on the above results. Since our evaluation is based on a simulation platform, noisy data that are difficult to simulate can impact the precision of the evaluation. We set `minp = 0.1` in the m-pattern algorithm, resulting in 119 symptom clusters covering 96.67% of the total logs. The remaining 3.33% are considered noisy cases, potentially containing more than one error. These noisy data constitute a small fraction of the logs, so the filtering process does not significantly affect the conclusions. Although our RL approach can handle these noisy cases, we exclude them to ensure a precise evaluation.

### 3.2. State Transition

We define states using error types (initial symptoms) and previously tried repair actions. A state \( s_t \) is represented by the tuple \((e, r, (a_0, a_1, \ldots, a_{t-1}))\), where:
- \( e \) is the error type,
- \( r \) is the recovery result (failure or health) before time \( t \),
- \( a_i \) (for \( i = 0, 1, \ldots, t-1 \)) are the repair actions executed up to time \( t \).

From this definition, it is clear that before the last repair action, the recovery result \( r \) of any state will be failure (\( f \)), and after that, it will become healthy (\( h \)). This definition also transforms automatic error recovery into a Markov decision process.

The transition function \(\delta\) is partially known. The state \( s_{t+1} \) produced by the action \( a_t \) on \( s_t = (e, f, (a_0, a_1, \ldots, a_{t-1})) \) can only be two types: \( s_{t+1}^f = (e, f, (a_0, a_1, \ldots, a_{t-1}, a_t)) \) or \( s_{t+1}^h = (e, h, (a_0, a_1, \ldots, a_{t-1}, a_t)) \). The probabilities of these transitions depend on the environment and the properties of the errors. Thus, Equation (3) can be rewritten as:

\[ Q(s_t, a_t) = E[c(s_t, a_t)] + q(s_t, a_t) \min_{a_{t+1}} Q(s_{t+1}^f, a_{t+1}) \]

where \( s_{t+1}^f = \delta(s_t, a_t) \) (Equation 4).

**Figure 4: Error Recovery Process**

The Q-function \( Q(s, a) \) represents the expected time cost for both failure and health outcomes. We limit the number of repair actions to a finite number \( N \) (set to 20 in our experiments). If the first \( N-1 \) repair actions fail to resolve the problem, we end the process by requesting a manual repair. According to the value contraction theorem [14], our RL method will converge with probability 1.

### 3.3. Exploration Strategy and Table Update

To explore different repair actions, we first infer state transitions based on existing recovery processes. The simplest method is to assume the last repair action is the correct one. However, this assumption may not always be safe, and sometimes stronger repair actions play a crucial role in the recovery process. A more realistic assumption is to consider both the last action and other stronger actions as correct repair actions. Based on this analysis, our hypotheses about the recovery process are as follows:

1. For any successful recovery process, at least the same correct repair actions (including the last action and the stronger ones) are needed to achieve the same recovery result.
2. Stronger actions can replace weaker ones in a successful recovery process.
3. Recovery processes for different errors are independent of each other.

Using these hypotheses, we can implement the exploration strategy and estimate the time cost for each possible policy. Starting from some initial states, we explore a large enough state space and then find the optimal policy. We divide the learning process into two phases: exploration and search. Similar to the simulated annealing algorithm, we use a temperature \( T \) to control the transition from exploration to search.

At time \( t \) for a specific error \( e \), we use the following probability distribution (Boltzmann distribution) to stochastically select a repair action:

\[ P(a_i | s_t) = \frac{e^{-Q(s_t, a_i)/T}}{\sum_j e^{-Q(s_t, a_j)/T}}, \quad a_i \in A \]

The temperature \( T \) decreases as more recovery processes are analyzed, leading to the selection of repair actions based entirely on Q-values, thus generating the policy.

When a repair action is selected, its time cost is estimated based on the recovery log. Specifically, one of the following values is chosen: the actual time cost in the recovery process, the average success time cost, or the average failing time cost. Based on these values, we update the Q-function and evaluate the policy. As shown in Section 4.2, this approach performs well in our experiments.

Another critical step in the training process is updating the Q-values. We use a table look-up representation of the Q-function and update the Q-values using the following equation:

\[ Q_n(s, a) \leftarrow (1 - \alpha_n) Q_{n-1}(s, a) + \alpha_n \left[ c(s, a) + \min_{a'} Q_{n-1}(s', a') \right] \]

where \( \alpha_n = \frac{1}{N(s, a)} \), \( Q(s, a) \) records the expected value of the Q-function, and \( N(s, a) \) represents the number of times the \((s, a)\) pair has been explored. It can be proven that this updating method is contracted, and the Q-values will eventually converge to the optimal values [20].

### 3.4. Hybrid Approach

Occasionally, the RL-trained policy may fail to repair exceptional error cases. To address this, we propose a hybrid approach that combines the trained policy with a user-defined policy. Specifically, if an error persists after the last action selected by the trained policy, we automatically revert to the user-defined policy. Since these noisy cases are infrequent, the hybrid policy ensures that all errors are repaired, while maintaining the advantages of automatic policy generation with RL, as demonstrated in Section 5.2.

### 4. Experimental Setup

This section introduces the data used in our experiments and the simulation platform that provides feedback on the effect of a repair action on a given state, based on the hypotheses.

#### 4.1. Experimental Data

Our experimental data are derived from a recovery log collected from a large-scale cluster system with thousands of servers, containing over 2 million entries of error symptoms and repair actions over nearly half a year of operations. The recovery policy used in the real system is user-defined, primarily selecting the cheapest action enabled by the state. There are four repair actions: TRYNOP (simply watch and do not perform any operation), REBOOT, REIMAGE (rebuild the operating system), and RMA (manual repair).

**Table 1: Example of a Recovery Process (Machine Name Omitted)**

| Time          | Description (Details Omitted)       |
|---------------|-------------------------------------|
| 3:07:12 am    | error:IFM-ISNWatchdog: â€¦            |
| 3:10:58 am    | errorHardware:EventLog: ...         |
| 3:23:26 am    | TRYNOP                              |
| 3:25:37 am    | errorHardware:EventLog: ...         |
| 3:27:34 am    | errorHardware:EventLog: ...         |
| 3:42:10 am    | REBOOT                              |
| 4:13:07 am    | Success                             |

The log entries are formatted as \((\text{Time}, \text{Description})\), where the description can be a repair action, an error symptom, or a report of a successful recovery. The logs can be divided into a series of recovery processes, starting with a new error, undergoing a sequence of repair actions, and ending with a successful recovery. Table 1 provides an example of a recovery process.

After noise filtering, we identified 97 error types from the recovery log using the error type inference method described in Section 3.1. To ensure sufficient training data, we selected the 40 most frequent error types, which account for 98.68% of the total recovery processes.