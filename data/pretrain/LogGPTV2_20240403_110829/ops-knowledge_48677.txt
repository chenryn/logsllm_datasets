User: 请用Java语言编写一个简单的Web爬虫，爬取某个网站的标题和链接。
Assistant: 由于爬取网站的方式可能与网站结构和内容有关，因此下面提供一个简单的Web爬虫框架，供参考或修改。

```java
import java.io.IOException;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

public class WebCrawler {
    private final String baseUrl;  // 爬取的网站的基础URL
    private final int depth;  // 爬取的深度，即最多跟踪的链接层数
    private final String keyword;  // 关键词，用于过滤不相关的链接
    private final int maxRecords;  // 最多爬取的记录数
    private int counter;  // 已经爬取的记录数

    public WebCrawler(String baseUrl, int depth, String keyword, int maxRecords) {
        this.baseUrl = baseUrl;
        this.depth = depth;
        this.keyword = keyword;
        this.maxRecords = maxRecords;
        this.counter = 0;
    }

    public void crawl() {
        crawlUrl(baseUrl, 0);
    }

    private void crawlUrl(String url, int currentDepth) {
        if (counter >= maxRecords) {
            return;
        }

        try {
            System.out.println("Crawling " + url);

            Document doc = Jsoup.connect(url).get();  // 获取当前网页的HTML文档

            // 提取当前网页的标题和链接，并打印到控制台
            String title = doc.title();
            System.out.println("Title: " + title);
            Elements links = doc.select("a[href]");
            for (Element link : links) {
                String href = link.attr("href");
                if (href.startsWith(baseUrl) && href.contains(keyword)) {
                    String text = link.text();
                    System.out.println("  " + text + ": " + href);
                    counter++;
                }
            }

            // 如果爬取的深度还未达到最大值，则继续跟踪链接
            if (currentDepth < depth) {
                Elements moreLinks = doc.select("a[href^=" + baseUrl + "]");
                for (Element link : moreLinks) {
                    String href = link.attr("href");
                    crawlUrl(href, currentDepth + 1);
                }
            }
        } catch (IOException e) {
            System.err.println("Failed to crawl " + url);
        }
    }
}
```

使用方式：

```java
WebCrawler crawler = new WebCrawler("https://www.example.com/", 2, "example", 10);
crawler.crawl();
```

以上代码将会爬取以 `https://www.example.com/` 开始的链接，最多爬取两层链接，只保留包含关键词 `example` 的链接，并最多爬取10个记录，打印每个记录的标题和链接到控制台。您可以根据需要修改这些参数。有关Jsoup库的更多信息，请参阅官方网站（https://jsoup.org/）。