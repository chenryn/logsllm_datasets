which doubles silicon area.
C. Power Overhead
Power information is more challenging to estimate. Models
such as McPAT [48] are unable to account for the low
power consumption of small cores of approximately 34μW
per MHz [45] at 40nm, compared with 800μW per MHz for
a Cortex A57 [46] at 20nm. Using twelve small cores and
without scaling for feature size, we obtain a power overhead
of approximately 16% for our system, showing that the impact
is minimal. Since at 20nm the power consumption for the
Rocket core will be less, this represents an upper bound, and
we should expect the true value to be signiﬁcantly lower still.
D. Bigger Cores
The out-of-order cores we simulate are relatively small, de-
signed to mimic the behavior of conventional Arm systems.
However, our technique extends favorably to larger main cores
that are more aggressive, since these realise only a sublinear
increase in single-threaded performance. Although we would
need more checker cores, performance scales linearly with the
power and area budget devoted to them due to the exploitation
of thread-level parallelism. This means that relative overheads
diminish signiﬁcantly. Such cores may also feature simultane-
ous multithreading and for these each concurrent thread would
be separated to a different checker core, but otherwise the
scheme works similarly.
E. Summary
We have shown that twelve checker cores running at 1GHz
are enough to have a performance impact of 3.4% maximum
across a wide set of benchmarks, with mean error-detection
times of 770ns. We have estimated the area and power
overheads of the technique compared to an unchecked core at
around 24% and 16% respectively, which are both signiﬁcantly
lower than existing techniques [3], [10], [12], [13], [49].
VII. RELATED WORK
A. Lock-Stepping
There are many examples of hardware duplication for error
detection, where copies of a program are run through identical
logic and the results compared. This is currently used in
the ARM Cortex R series of processors [9], as they are
intended for high error and high reliability environments,
such as cars and space. More recently, triple-lockstep designs,
which perform majority voting to correct errors, have been
developed [3]. Historically, similar techniques were used by
the IBM G5 [10] and the Compaq Himalaya [49].
Mukherjee et al. [12] present a chip-level redundantly multi-
threaded scheme, where the second core trails the ﬁrst in order
to reduce cache misses. Gupta et al. [50] instead argue for
duplication at a ﬁner granularity, through multiple copies of
individual pipeline stages in a fabric, rather than the more
coarse-grained core duplication of industry schemes. This
allows better tolerance of hard faults when errors are common.
Hernandez and Abella [2] give a scheme to improve the
detection delay for light-lockstep systems, where only some
applications need error detection, and thus hardware can be
repurposed if the second core is needed for detection.
B. Redundant Multi-Threading Hardware
Rather than a static duplication of hardware, many schemes
have suggested using dynamic scheduling on processors fea-
turing simultaneous multi-threading. AR-SMT [11] presents a
redundant multi-threading scheme for fault detection, where
two threads are run on the same processor. However, this
does not cover hard errors, because the same hardware is
used for both computation. In addition, it uses up a processor
context that could be used for more computation, and comes
at a signiﬁcant performance overhead. Indeed, Mukherjee
et al. [12] suggest that redundant multi-threading techniques
come at a performance overhead of 32%. Schuchman and Vi-
jaykumar [13] improve the ability of redundant multi-threading
to detect hard faults by rearranging instructions within the
trailing thread, altering the hardware resources used, at the
expense of a further 15% performance degradation.
Reinhardt and Mukherjee [1] present
the concept of a
sphere of replication as it applies to redundant multi-threading:
the parts of the system which are replicated. They further
present the use of a load-value queue to forward results from
the computation thread to the replication thread, instead of
duplicating the page ﬁle as in AR-SMT [11]. This is similar
to that used in our scheme. Smolens et al. [51] suggest the
removal of this queue by noting that, in the common case, two
threads will observe the same values from cache loads without
explicit duplication, and instead use detection and recovery
to correct any differences by treating them as errors, at the
expense of performance.
Rashid et al. [14] utilise a similar form of parallelism to that
which we exploit, to run error detection on a homogeneous
multicore. The scheme pays a large area cost, but reduces
energy usage by dynamic frequency-voltage scaling. We build
347
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:31 UTC from IEEE Xplore.  Restrictions apply. 
on their insights by using a heterogeneous system to reduce
area and energy further, and design an alternative forwarding
system to increase parallelism and negate the need for a large
L1 cache per core.
C. Software Schemes
It is also possible to provide error detection entirely in soft-
ware, without hardware additions. Khudia and Mahlke [52]
detect errors in software for soft applications, where only parts
of the application are error-intolerant, such as video decod-
ing. The signiﬁcant overheads involved are reduced by only
repeating computation for error-intolerant portions. Thomas
and Pattabiraman [53] identify heuristics to select which parts
of applications to check for high error coverage. Wang and
Patel [54] provide a scheme for partial fault detection, by
only responding to errors which trigger exceptions when they
are not caught. Reis et al. [25] present SWIFT, a solution
which duplicates instructions in the same thread to provide
limited coverage of soft faults. Jeffery and Figueiredo [55]
give a virtual lockstepping scheme, where a hypervisor is
used to duplicate inputs and perform comparisons of multiple
virtualised copies of an operating system. Veeraraghavan et
al. [56] utilise a form of program slicing, as we use in our
system, to solve a different but related problem: deterministic
recording of execution for multicore workloads in software.
D. Hybrid Schemes
Hardware schemes suffer from a large cost in terms of silicon
area, and software schemes suffer from a lack of coverage
for hard errors, and high performance costs. To mitigate
these, hybrid schemes have been proposed. For example, Reis
et al. [24] present CRAFT, a combination of SWIFT [25], a
software only scheme, and redundant multi-threading [1], [11],
[12]. This uses compiler assistance to duplicate instructions,
changing redundant stores to perform checks using a special
hardware detection structure.
E. Heterogeneity
The use of heterogeneous cores for error resilience already has
precedence. Ansari et al. [57] couple a lightweight core with
a newer fast core. When the fast core begins to fail, it is used
to provide hints, such as branches and loads, to the slower,
functionally correct core,
to reduce the performance gap.
LaFrieda et al. [58] dynamically couple cores that can differ
due to manufacturing defects, so that those which are faster are
matched together to provide error detection, as are those which
are slower or broken. DIVA [21], [22] adds in-order execution
units towards the end of an unveriﬁed out-of-order pipeline to
repeat computation and check data forwarding. These units
run at the same clock speed as the rest of the core, and
achieve parallelism by checking each instruction individually.
This means ECC is required on all architectural state within
the original processor to avoid communication errors, which
is impractical in a high performance design.
F. Other Hardware Schemes
Other hardware fault tolerance schemes have been proposed,
for example Clover [59], which uses hardware wave detection
to detect cosmic rays hitting a system. Many schemes have
been proposed to deal with retiring components efﬁciently
once hard errors have been detected. Aggarwal et al. [60]
partition multicore hardware into fault zones once errors have
been detected, redistributing power dynamically based on how
much of the core is still alive. Romanescu and Sorin [61] allow
a fraction of the cores in a system to be used for spare parts at
the pipeline granularity, to ﬁx hard faults in a system. Gupta
et al. [62] use a tiled web architecture which allows slow or
broken pipeline stages to be weaved out. Powell et al. [63]
allow the use of partially broken hardware by detection and
migration of just the operations known to be faulty.
VIII. CONCLUSION
Current fault detection techniques are limited by high over-
heads,
in terms of energy, silicon area, and performance.
We have developed a technique to perform error detection
for high-performance, out-of-order processors at
low area,
performance and energy cost by exploiting new parallelism in
the redundant repetition of the program. Our scheme checks
multiple parts of the execution simultaneously on a set of small
cores embedded beside the main out-of-order CPU.
Evaluating over a wide variety of benchmarks, twelve small
checker cores running at 1GHz give enough performance to
limit average slowdown to 1.75% (maximum 3.4%). The mean
error detection delay for each evaluated benchmark averages
at 770ns, with 99.9% of all loads and stores checked within
5000ns, and all checked within 45μs: this is larger than with
a lock-step system, but is more than offset by the reduction in
chip area and power usage attainable, and is justiﬁable in the
relevant domain spaces.
Future work will look at extending the scheme to perform
correction of errors within a microprocessor, rather than just
detection, to enable low-overhead complete fault tolerance.
ACKNOWLEDGEMENTS
This work was supported by the Engineering and Physical
Sciences Research Council (EPSRC), through grant references
EP/K026399/1 and EP/M506485/1, and Arm Ltd. Additional
data related to this publication is available in the data reposi-
tory at https://doi.org/10.17863/CAM.21857.
REFERENCES
[1] S. K. Reinhardt and S. S. Mukherjee, “Transient fault detection via
simultaneous multithreading,” in ISCA, 2000.
[2] C. Hernandez and J. Abella, “Timely error detection for effective
recovery in light-lockstep automotive systems,” IEEE Transactions on
Computer-Aided Design of Integrated Circuits and Systems, vol. 34,
no. 11, 2015.
[3] X. Iturbe, B. Venu, E. Ozer, and S. Das, “A triple core lock-step
(TCLS) ARM R(cid:2)Cortex R(cid:2)-R5 processor for safety-critical and ultra-
reliable applications,” in DSN-W, 2016.
[4] M. Rausand, Reliability of Safety-Critical Systems: Theory and Appli-
cations. Wiley, 2014.
348
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:31 UTC from IEEE Xplore.  Restrictions apply. 
[5] M. Snir, R. W. Wisniewski, J. A. Abraham et al., “Addressing failures
in exascale computing,” Int. J. High Perform. Comput. Appl., vol. 28,
no. 2, May 2014.
[6] A. Geist and S. Dosanjh, “IESP exascale challenge: Co-design of
architectures and algorithms,” Int. J. High Perform. Comput. Appl.,
vol. 23, no. 4, Nov. 2009.
[7] D. Zhao, D. Zhang, K. Wang, and I. Raicu, “Exploring reliability of
exascale systems through simulations,” in HPC, 2013.
[8] C. Turner, “Safety and security for automotive SoC design -
arm,” http://www.arm.com/ﬁles/pdf/20160628 B02 ATF Korea Chris
Turner.pdf, 2016.
[9] N. Werdmuller, “Addressing functional safety applications with Arm
Cortex-R5,” http://community.arm.com/groups/embedded/blog/2015/01/
22/addressing-functional-safety-applications-with-arm-cortex-r5, 2015.
[10] T. J. Slegel, R. M. Averill III, M. A. Check et al., “IBM’s S/390 G5
microprocessor design,” IEEE Micro, vol. 19, no. 2, 1999.
[11] E. Rotenberg, “AR-SMT: A microarchitectural approach to fault toler-
ance in microprocessors,” in FTCS, 1999.
[12] S. S. Mukherjee, M. Kontz, and S. K. Reinhardt, “Detailed design and
evaluation of redundant multithreading alternatives,” in ISCA, 2002.
[13] E. Schuchman and T. N. Vijaykumar, “Blackjack: Hard error detection
with redundant threads on smt,” in DSN, 2007.
[14] M. W. Rashid, E. J. Tan, M. C. Huang, and D. H. Albonesi, “Exploiting
coarse-grain veriﬁcation parallelism for power-efﬁcient fault tolerance,”
in PACT, 2005.
[15] A. R. Pargeter, “An example of strong induction,” The Mathematical
Gazette, vol. 80, no. 488, 1996.
[16] https://www.siﬁve.com/products/coreplex-risc-v-ip/e51/.
[17] J. Srinivasan, S. V. Adve, P. Bose, and J. A. Rivers, “The impact of
technology scaling on lifetime reliability,” in DSN, 2004.
[18] S. E. Michalak, K. W. Harris, N. W. Hengartner, B. E. Takala, and
S. A. Wender, “Predicting the number of fatal soft errors in Los Alamos
national laboratory’s ASC Q supercomputer,” IEEE Transactions on
Device and Materials Reliability, 2005.
[19] S. Borkar and A. A. Chien, “The future of microprocessors,” Commu-
nications of the ACM, vol. 54, no. 5, 2011.
[20] B. Schroeder, E. Pinheiro, and W.-D. Weber, “DRAM errors in the wild:
A large-scale ﬁeld study,” in SIGMETRICS, 2009.
[21] T. M. Austin, “Diva: A reliable substrate for deep submicron microar-
chitecture design,” in MICRO, 1999.
[22] C. Weaver and T. M. Austin, “A fault tolerant approach to microproces-
sor design,” in DSN, 2001.
[23] B. Stolt, Y. Mittlefehldt, S. Dubey, G. Mittal, M. Lee, J. Friedrich, and
E. Fluhr, “Design and implementation of the POWER6 microprocessor,”
IEEE Journal of Solid-State Circuits, vol. 43, no. 1, 2008.
[24] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, D. I. August, and S. S.
Mukherjee, “Design and evaluation of hybrid fault-detection systems,”
in ISCA, 2005.
[25] G. A. Reis, J. Chang, N. Vachharajani, R. Rangan, and D. I. August,
“Swift: Software implemented fault tolerance,” in CGO, 2005.
[26] R. Hameed, W. Qadeer, M. Wachs, O. Azizi, A. Solomatnikov, B. C.
Lee, S. Richardson, C. Kozyrakis, and M. Horowitz, “Understanding
sources of inefﬁciency in general-purpose chips,” in ISCA, 2010.
[27] M. Shaﬁque and J. Henkel, “Agent-based distributed power management
for kilo-core processors,” in ICCAD, 2013.
[28] “Green 500,” http://www.top500.org/green500/lists/2017/11/, Nov. 2017.
[29] J. Fang, H. Sips, L. Zhang, C. Xu, Y. Che, and A. L. Varbanescu, “Test-
driving intel xeon phi,” in ICPE, 2014.
[30] G. Blake, R. G. Dreslinski, T. Mudge, and K. Flautner, “Evolution of
thread-level parallelism in desktop applications,” in ISCA, 2010.
[31] K. Mitropoulou, V. Porpodas,
“COMET:
Communication-optimised multi-threaded error-detection technique,” in
CASES, 2016.
and T. M.
Jones,
[32] C. Wang, H. s. Kim, Y. Wu, and V. Ying, “Compiler-managed software-
based redundant multi-threading for transient fault detection,” in CGO,
2007.
[33] International Organization for Standardization, “ISO 26262: Road vehi-
cles – functional safety,” 2011.
[34] N. Werdmuller, “Addressing functional safety applications with ARM
https://community.arm.com/iot/embedded/b/embedded-
Cortex-R5,”
blog/posts/addressing-functional-safety-applications-with-arm-cortex-
r5, Jan. 2015.
349
[35] D. J. Sorin, M. M. K. Martin, M. D. Hill, and D. A. Wood, “Safetynet:
Improving the availability of shared memory multiprocessors with global
checkpoint/recovery,” in ISCA, 2002.
[36] A. Jhingran and P. Khedkar, “Analysis of recovery in a database system
using a write-ahead log protocol,” in SIGMOD, 1992.
[37] M. Herlihy and J. E. B. Moss, “Transactional memory: Architectural
support for lock-free data structures,” in ISCA, 1993.
[38] https://community.arm.com/processors/f/discussions/4503/lock-step-
mode-execution-on-cortex-r5/11365#11365.
[39] P. R. Luszczek, D. H. Bailey, J. J. Dongarra, J. Kepner, R. F. Lucas,
R. Rabenseifner, and D. Takahashi, “The hpc challenge (hpcc) bench-
mark suite,” in SC, 2006.
[40] M. R. Guthaus, J. S. Ringenberg, D. Ernst, T. M. Austin, T. Mudge, and
R. B. Brown, “MiBench: A free, commercially representative embedded
benchmark suite,” in WWC, 2001.
[41] C. Bienia, “Benchmarking modern multiprocessors,” Ph.D. dissertation,
Princeton University, January 2011.
[42] N. Binkert, B. Beckmann, G. Black et al., “The gem5 simulator,”
SIGARCH Comput. Archit. News, vol. 39, no. 2, 2011.
[43] A. Gutierrez, J. Pusdesris, R. G. Dreslinski et al., “Sources of error in
full-system simulation,” in ISPASS, 2014.
[44] L. Bautista-Gomez, S. Tsuboi, D. Komatitsch, F. Cappello,
N. Maruyama, and S. Matsuoka, “FTI: High performance fault
tolerance interface for hybrid systems,” in SC, 2011.
[45] https://riscv.org/wp-content/uploads/2015/02/riscv-rocket-chip-
generator-tutorial-hpca2015.pdf.
[46] http://www.anandtech.com/show/8718/the-samsung-galaxy-note-4-
exynos-review/6.
[47] M. Yabuuchi, Y. Tsukamoto, M. Morimoto, M. Tanaka, and K. Nii,
“20nm high-density single-port and dual-port srams with wordline-
voltage-adjustment system for read/write assists,” in ISSCC, 2014.
[48] S. Li, J. H. Ahn, R. D. Strong, J. B. Brockman, D. M. Tullsen, and
N. P. Jouppi, “McPAT: An integrated power, area, and timing modeling
framework for multicore and manycore architectures,” in MICRO, 2009.
[49] A. Wood, “Data integrity concepts, features, and technology,” Tandem
Division, Compaq Computer Corporation, White Paper, 1999.
[50] S. Gupta, S. Feng, A. Ansari, J. Blome, and S. Mahlke, “The stagenet
fabric for constructing resilient multicore systems,” in MICRO, 2008.
[51] J. C. Smolens, B. T. Gold, B. Falsaﬁ, and J. C. Hoe, “Reunion:
Complexity-effective multicore redundancy,” in MICRO, 2006.
[52] D. S. Khudia and S. Mahlke, “Harnessing soft computations for low-
budget fault tolerance,” in MICRO, 2014.
[53] A. Thomas and K. Pattabiraman, “Error detector placement for soft
[54] N. J. Wang and S. J. Patel, “Restore: Symptom based soft error detection
computation,” in DSN, June 2013.
in microprocessors,” in DSN, 2005.
[55] C. M. Jeffery and R. J. O. Figueiredo, “A ﬂexible approach to improving
system reliability with virtual lockstep,” IEEE Transactions on Depend-
able and Secure Computing, vol. 9, no. 1, 2012.
[56] K. Veeraraghavan, D. Lee, B. Wester et al., “Doubleplay: Parallelizing
sequential logging and replay,” in ASPLOS, 2011.
[57] A. Ansari, S. Feng, S. Gupta, and S. Mahlke, “Necromancer: Enhancing
system throughput by animating dead cores,” in ISCA, 2010.
[58] C. LaFrieda, E. Ipek, J. F. Martinez, and R. Manohar, “Utilizing
dynamically coupled cores to form a resilient chip multiprocessor,” in
DSN, 2007.
[59] Q. Liu, C. Jung, D. Lee, and D. Tiwari, “Clover: Compiler directed
lightweight soft error resilience,” in LCTES, 2015.
[60] N. Aggarwal, P. Ranganathan, N. P. Jouppi, and J. E. Smith, “Con-
ﬁgurable isolation: Building high availability systems with commodity
multi-core processors,” in ISCA, 2007.
[61] B. F. Romanescu and D. J. Sorin, “Core cannibalization architecture:
Improving lifetime chip performance for multicore processors in the
presence of hard faults,” in PACT, 2008.
[62] S. Gupta, A. Ansari, S. Feng, and S. Mahlke, “Stageweb: Interweaving
pipeline stages into a wearout and variation tolerant cmp fabric,” in DSN,
2010.
[63] M. D. Powell, A. Biswas, S. Gupta, and S. S. Mukherjee, “Architectural
core salvaging in a multi-core processor for hard-error tolerance,” in
ISCA, 2009.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:31 UTC from IEEE Xplore.  Restrictions apply.