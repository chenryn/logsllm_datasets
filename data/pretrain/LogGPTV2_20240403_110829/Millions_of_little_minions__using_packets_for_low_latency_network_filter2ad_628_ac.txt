in software.
2.5 Other possibilities
The above examples illustrate how a single TPP interface enables
end-hosts to achieve many tasks. There are more tasks that we
couldn’t cover in detail. In the interest of space, we refer the reader
to the extended version of this paper for more details on some of
the tasks below [28].
Measurement: Since TPPs can read network state, they can be
used in a straightforward fashion for measuring any network statis-
tic at rapid timescales. As TPPs operate in the dataplane, they are
in a unique position to expose path characteristics experienced by
a speciﬁc packet that an end-host cares about.
Network veriﬁcation: TPPs also help in verifying whether net-
work devices meet certain requirements. For example, the path
visibility offered by TPPs help accurately verify that route con-
vergence times are within an acceptable value. This task can be
challenging today, if we rely on end-to-end reachability as a way
to measure convergence, because backup paths can still maintain
end-to-end connectivity when routes change. Also, the explicit vis-
ibility eases fault localization.
Fast network updates: By allowing secure applications to write
to a switch’s forwarding tables, network updates can be made very
fast. This can reduce the time window of a transient state when net-
work forwarding state hasn’t converged. For example, it is possible
to add a new route to all switches along a path in half a round-trip
time, as updating an IP forwarding table requires only 64 bits of
information per-hop: 32 bit address and a 32 bit netmask per hop,
tiny enough to ﬁt inside a packet.
Wireless Networks: TPPs can also be used in wireless networks
where access points can annotate end-host packets with rapidly
changing state such as channel SNR. Low-latency access to such
rapidly changing state is useful for network diagnosis, allows end-
hosts to distinguish between congestive losses and losses due to
poor channel quality, and query the bitrate that an AP selected for
a particular packet.
3 Design of TPP-Capable Switches
In this section, we discuss the TPP instructions, addressing
schemes, and the semantics of the TPP interface to a switch and
what it means for a switch to be TPP-capable. Network switches
have a variety of form factors and implementations; they could be
implemented in software (e.g., Click, Open vSwitch), or in network
processors (e.g., NetFPGA), or as hardware ASICs. A switch might
also be built hierarchically from multiple ASICs, as in ‘chassis’
based switches [18, Figure 3]. A TPP can be executed on each of
these platforms. Thus, it is useful for a TPP-capable switch and the
end-host to have a contract that preserves useful properties without
imposing a performance penalty. We achieve this by constraining
the instruction execution order and atomicity.
3.1 Background on a Switch Pipeline
We begin with an abstract model of a switch execution environ-
ment shown in Figure 5. The packet ﬂows from input to output(s)
through many pipelined modules. Once a packet arrives at an in-
put port, the dataplane tags the packet with metadata (such as its
ingress port number). Then, the packet passes through a parser
that extracts ﬁelds from the packet and passes it further down the
pipeline which consists of several match-action stages. This is also
known as multiple match table model [7]. For example, one stage
might use the parsed ﬁelds to route the packet (using a combina-
tion of layer 2 MAC table, layer 3 longest-preﬁx match table, and
a ﬂexible TCAM table). Finally, any modiﬁcations to the packet
are committed and the packet is queued in switch memory. Us-
ing metadata (such as the packet’s priority), the scheduler decides
when it is time for the packet to be transmitted out of the egress port
determined earlier in the pipeline. The egress stage also consists of
a number of match-action stages.
3.2 TPP Semantics
The read/write instructions within a TPP access two distinct mem-
ory spaces: memory within the switch (switch memory), and a per-
hop scratch space within the packet (packet memory). By all switch
memory, we only mean memory at the stages traversed by a TPP,
except the memory that stores packet contents. By all packet mem-
ory, we mean the TPP related ﬁelds in the packet. Now, we state our
requirements for read/write instructions accessing the two memory
spaces.
Switch memory: To expose statistics pertaining to a speciﬁc
packet as it traverses the network, it is important for the instruc-
tions in the TPP to have access to the same values that are used to
forward the packet. For read-only values, this requirement means
that reads by a TPP to a single memory location must necessarily
be atomic and after all writes by the forwarding logic to the same
memory location. For example, if a TPP accesses the memory that
holds the output port of a packet, it must return the same port that
the forwarding logic determines, and no other value. This is what
we mean by a “packet-consistent” view of network state.
For read-write memory addresses, it is useful if instructions
within the TPP were executed in the order speciﬁed by the TPP
to a given location after any modiﬁcations by the switch forward-
ing logic. Thus, writes by a TPP supersede those performed by
forwarding logic.
Packet memory: Since instructions can read from and write to
packet memory using PUSH and POP, writes to packet memory must
take effect sequentially in the order speciﬁed by the TPP. This guar-
antees that if a TPP pushes values at memory locations X, Y, and Z
onto packet memory, the end-host sees the values in the packet in
the same order. This does not require that reads to X, Y, and Z be
issued in the same order.
3.3 TPP Execution Model
TPPs are executed in the dataplane pipeline. TPPs are required to
ﬁt exactly within an MTU to avoid having the ASIC deal with frag-
mentation issues. This is not a big limitation, as end-hosts can split
a complex task into multiple smaller TPPs if a single packet has
insufﬁcient memory to query all the required statistics. By default,
a TPP executes at every hop, and instructions are not executed if
8Figure 5: A simpliﬁed block diagram of the dataplane pipeline in a switch ASIC. Packets arrive at the ingress, and pass through multiple
modules. The scheduler then forwards packets (that are not dropped) to the output ports computed at earlier stages in the pipeline.
(a) Parse graph for the two ways to parse TPPs: trans-
parent mode, or standalone mode.
(b) TPP’s packet structure.
Figure 6: The parse graph and structure of a TPP. We chose 0x6666 as the ethertype and source UDP port that uniquely identiﬁes a TPP.
With a programmable switch parser, this choice can be reprogrammed at any time.
they access memory that doesn’t exist. This ensures the TPP fails
gracefully.
Furthermore, the platform is free to reorder reads and writes
so they execute in any order. However, if the programmer needs
guarantees on ordering instructions due to data hazards (e.g., for
CEXEC, CSTORE), they must ensure the TPP accesses memory in
the pipeline order. For a vast majority of use cases, we argue this
restriction is not severe. From a practical standpoint, this require-
ment ensures that the switch pipeline remains feed-forward as it is
today in a majority of switches.
3.3.1 Uniﬁed Memory-Mapped IO
A TPP has access to any statistic computed by the switch that is ad-
dressable. The statistics can be broadly namespaced into per-switch
(i.e., global), per-port, per-queue and per-packet. Table 2 shows
example statistics in each of these namespaces. These statistics
may be scattered across different stages in the pipeline, but TPPs
access them via a uniﬁed address space. For instance, a switch
keeps metadata such as input port, the selected route, etc. for every
packet that can be made addressable. These address mappings are
known upfront to the TPP compiler that converts mnemonics such
as [PacketMetadata:InputPort] into virtual addresses.
TPPs also support a hop addressing scheme,
to the the base:offset x86-addressing mode.
3.3.2 Addressing Packet Memory
Memory is managed using a stack pointer and a PUSH in-
struction that appends values to preallocated packet mem-
simi-
ory.
lar
Here,
base:offset refers to the word at location base * hop_size
+ offset. Thus, if hop-size is 16 bytes, the instruction “LOAD
[Switch:SwitchID], [Packet:hop[1]]” will copy the switch
ID into PacketMemory[1] on the ﬁrst hop, PacketMemory[17]
on the second hop, etc. The offset is part of the instruction; the
base value (hop number) and per-hop memory size values are in
the TPP header. To simplify memory management in the dataplane,
the end-host must preallocate enough space in the TPP to hold per-
hop data structures.
Synchronization Instructions
3.3.3
Besides read and write, a useful
instruction in a concur-
rent programming environment is an atomic update instruction,
such as a conditional store CSTORE, conditioned on a mem-
ory location matching a speciﬁed value, halting subsequent in-
structions in the TPP if the update fails.
is, CSTORE
[X],[Packet:hop[Pre]],[Packet:hop[Post]] works as fol-
lows:
That
succeeded = False
if (value at X == value at Packet:hop[Pre]) {
value at X = value at Packet:hop[Post]
succeeded = True
}
value at Packet:hop[Pre] = value at X;
if (succeeded) {
allow subsequent instructions to execute
}
By having CSTORE return the value of X, an end-host can in-
fer if the instruction succeeded. Notice that the second and third
operands are read from a unique location at every hop. This is
needed to ensure correct semantics when the switch overwrites the
value at the second operand.
In a similar vein, we found a conditional execute (CEXEC) in-
struction useful; for example, it may be desirable to execute a net-
work task only on one switch, or on a subset of switches (say all
the top of rack switches in a datacenter). The conditional execute
instruction speciﬁes a memory address, a 32-bit mask, and a 32-
bit value (speciﬁed in the packet hop), which instructs the switch
to execute all subsequent instructions only when (switch_value
& mask) == value. All instructions that follow a failed CEXEC
check will not be executed.
3.4 Parsing: TPP Packet Format
As noted in §2, a TPP is any Ethernet frame from which we can
uniquely identify a TPP header, the instructions, packet memory,
and an optional payload. This allows end-hosts to use TPPs in two
ways: (i) piggy-back TPPs on any existing packet by encapsulating
Ingress ParsersMatch Action Stage 1Match Action Stage 2Match Action Stage nPacketsArriveEgress ParsersMatch Action Stage 1Match Action Stage 2Match Action Stage nIngress PipelineEgress PipelineSwitch Memory (Queues)PacketsDepartTPPARPEthernetIPv4UDPTCPTPPether.type=0x6666ether.type=0x0800tpp.proto=0x0800udp.dstport=0x6666non-TPPudp.dstport!=0x6666ether.type=0x0806ip.p=6ip.p=1712345InstructionsPacket memory(Initialized by end-hosts)Up to20 bytes40–200bytes1: Length of TPP2: Length of Packet memory3: Packet mem. addressing    mode (stack, hop, etc.)4: Hop number / stack pointer5: Per hop memory length    (used only when memory is     hop-addressed)6: TPP checksum7: Encapsulated TPP proto    (default 0, i.e., none)68 bytes72 bytesTPP Application ID4 bytes9and once all memory accesses have completed, the packet leaves
the stage.
Replicating execution units might seem expensive, but the ma-
jority of logic area in an ASIC is due to the large memories (for
packet buffers, counters, etc.), so the cost of execution units is not
prohibitive [7]. Figure 7 shows the TCPU if we zoom into one of
the match-action stages.
Serializing PUSH/POP instructions: Finally,
there are many
techniques to ensure the effect of PUSH and POP instructions ap-
pear if they executed inorder. Since the packet memory addresses
accessed by PUSH/POP instructions are known immediately when
they are parsed, they can be converted to equivalent LOAD/STOREs
that can then be executed out of order. For example, consider the
following TPP:
PUSH [PacketMetadata:OutputPort]
PUSH [PacketMetadata:InputPort]
PUSH [Stage1:Reg1]
POP [Stage3:Reg3]
After parsing the instructions, they can be converted to the fol-
lowing TPP which is equivalent to the above TPP:
LOAD [PacketMetadata:OutputPort], [Packet:Hop[0]]
LOAD [PacketMetadata:InputPort], [Packet:Hop[1]]
LOAD [Stage1:Reg1], [Packet:Hop[2]]
STORE [Stage3:Reg3], [Packet:Hop[2]]
Now, the TPP loads the values stored in two registers to the
packet memory addressed in the hop addressing format. Note that
the packet’s output port is not known until the packet is routed, i.e.,
at the end of the ingress stage. The execution proceeds as follows:
• By ingress stage 1, the metadata consists of four instructions, the
memory addresses they access (the four registers and the three
packet memory offsets), the packet’s hop number, the packet’s
headers, its input port, its CRC, etc.
• At stage 1, the packet’s input port is known. Stage 1 executes
the second instruction, and stores the input port value at the 2nd
word of the packet memory. Stage 1 also executes the third in-
struction, copying Reg1 to the 3rd word of packet memory.
from packet memory into Reg3.
• At stage 3, the fourth instruction executes, copying the 3rd word
• At the end of the ingress stage, the packet’s output port is already
computed, and the last stage copies the output port number to the
1st word of the packet memory before the packet is stored in the
ASIC packet buffers.
4 End-host Stack
Now that we have seen how to design a TPP-enabled ASIC, we
look at the support needed from end-hosts that use TPPs to achieve
a complex network functionality. Since TPP enables a wide range
of applications that can be deployed in the network stack (e.g., RCP
congestion control), or individual servers (e.g., network monitor-
ing), or a combination of both, we focus our efforts on the common
usage patterns.
End-host architecture: The purpose of the end-host stack (Fig-
ure 8) is to abstract out the common usage patterns of TPPs and
implement TPP access control policies. At every end-host, we have
a TPP control- and dataplane agent. The control plane is a software
agent that does not sit in the critical forwarding path, and interacts
with the network control plane if needed. The dataplane shim sits
on the critical path between the OS network stack and the network
interface and has access to every packet transmitted and received by
the end-host. This shim is responsible for transparently adding and
removing TPPs from application-generated packets, and enforcing
access control.
Figure 7: At every stage, the TCPU has execution units that can
access only local memory and registers, as well as packet metadata.
the packet within a TPP of ethertype 0x6666, or (ii) embed a TPP
into an otherwise normal UDP packet destined for port 0x6666,
which is a special port number usurped by TPP-enabled routers.
Figure 6a shows the two parse graphs depicting the two ways
in which our prototype uses TPPs. A parse graph depicts a state
machine for a packet parser, in which the nodes denote protocols
and edges denote state transitions when ﬁeld values match. We use
the same convention as in [7] to show the two ways in which we
can parse TPPs.
3.5 Putting it together: the TCPU
TPPs execute on a tiny processor, which we call the TCPU. A sim-
ple way to implement the TCPU is by having a RISC-like processor
at the end of the ingress match-action stages as we described in our
earlier position paper [19, Figure 5]. This simple approach could be
practical for software, or low-speed hardware switches, but might
be impractical in high-speed hardware switches as memory in an
ASIC is often distributed across modules. The wiring complexity
to provide read and write paths from each module to the TCPU
becomes prohibitively expensive within an ASIC, and is simply in-
feasible across line-cards in a chassis switch.
We overcome this limitation in two ways. First, our execution
model permits reordering reads and writes across different ASIC
memory locations. Second, end-hosts can statically analyze a de-
sired TPP and split it into smaller TPPs if one TPP is insufﬁcient.
For instance, if an end-host requires link utilization on all links at
all switches a packet traverses, it can stage the following sequence
of TPPs: (i) send one TPP to collect switch ID and link utiliza-
tions on links traversed by the packet, and (ii) send a new TPP to
each switch link on the switches traversed by TPP 1 to collect the
remaining statistics. To summarize:
• Loads and stores in a single packet can be executed in any or-
der, by having end-hosts ensure there are no write-after-write, or
read-after-write conﬂicts.