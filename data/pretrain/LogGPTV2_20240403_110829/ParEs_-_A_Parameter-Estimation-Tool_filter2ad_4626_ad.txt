size such that a worst-case version of conﬁdence intervals is
computed (actually, the conﬁdence interval for one parameter
would have to be examined depending on the curent values
of the other parameters; in the example above, the conﬁdence
interval for β is only between 0.43 and 0.64 for α = 4000).
Instead of beginning with an rectangle that will surely contain
the zero level and which is reduced step by step, for example
by running in nested loops through the emerging space and
searching for points for which the conﬁdence funtion yields a
positive value (in the rest of the paper referred to as positive
point), our algorithm goes just the other way around. We
start with a rectangle of point-size that is surely within the
conﬁdence interval, namely the maximum likelihood point,
and enlarge this rectangle step by step until it encloses the
whole zero level.
The algorithm therefore consists of one main loop that
continuously runs through all parameters. For each of these
parameters, ﬁrst the current lower conﬁdence bound and then
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:53 UTC from IEEE Xplore.  Restrictions apply. 
!
"
#


!

"

#

$

%

&
=

F
D
=
>
A
J
=
the current upper conﬁdence bound is checked for positive
points. If such a point is found, one can run from this point
with a ﬁnite stepsize into the corresponding direction until the
conﬁdence function takes on values smaller than or equal to
zero and therefore the conﬁdence bound for the parameter is
shiftet to this new value. The algorithm terminates if in one
loop on none of the borders a positive point can be found. The
open issue of this algorithm is whether there are still positive
points on one boundary of the rectangle and in the case there
are positive points, ﬁnding such a positive point.
The question, if there are positive points on one boundary,
can be reduced to the question, if the maximum value of
the conﬁdence function on one boundary is still bigger than
zero. Therefore the point that yields the maximum conﬁdence
function value on one boundary is searched. This could be
done using one of the local optimization algorithms described
in Section IV-A.1, but as the number of local optima usually
increases if one parameter is ﬁxed, a global optimization is
needed in practice.
Therefore, one can use an evolution strategy on each of
the boundaries. Second, as we have relatively small intervals
to investigate, also interval arithmetic can be employed. For
distributions with up to three parameters, this can be done
by a full recursive splitting as described in Section IV-A.2,
where the midpoint of each boundary is taken as the splitting
point. If the conﬁdence function yields a positive value on that
midpoint, the splitting can be stopped as one has a positive
point from which the conﬁdence bound can be improved. If
the midpoint is negative, one has to repeat splitting until all
intervals yield a maximum interval value smaller than zero (no
change of this conﬁdence bound is necessary) or a positive
midpoint is found.
The problem with this algorithm is that with real data one
needs about seven to eight splitting-recursions until the interval
size becomes small enough to exclude a large number of
boxes by interval evaluations. In the three-dimensional case,
each boundary is a two-dimensional rectangle that is split by
the midpoint into four boxes, such that after seven recursions
without exclusions one has to investigate “only” 47 = 16384
boxes on the lowest level. But in the six-dimensional case
as with the bathtub distribution, each boundary is split into
32 boxes in every step such that after seven recursions one
would have 327 = 34, 359, 738, 368 boxes on the lowest level
to investigate. Even when ignoring the memory problems of
storing the deﬁnitions of all these boxes, the investigation of
all these boxes is impossible because of runtime.
Therefore, a heuristic algorithm is used for distributions
with more than three parameters. The used heuristics is that
only the box that yields the maximum interval value is further
investigated as normally this box is most likely to contain
the maximum function value. Therefore, even in the six
dimensional case, at most k · 32 boxes in total have to be
i=1 32i boxes
investigated if there are k splits instead of the
in the full recursive version. However, the question is, how
much inaccuracy is introduced by this heuristics. Experiments
that compared the results of the full-recursive version with the
(cid:16)
k
heuristic version showed, that the inaccuracy of the heuristic
version is usually in the order of magnitude of the stepsize
that is used for running into each direction. Therefore, the
inaccuracy of the algorithm (slightly too small conﬁdence
intervals) can be compensated by choosing a slightly larger
stepsize.
V. THE TOOL AND EXAMPLES
This section introduces ParEs, a tool for parameter estima-
tion and shows the results of the application of the algorithms
described in the previous sections to two examples.
A. The ParEs-Tool
ParEs (Parameter Estimation) is a GUI-based pure Java-
tool that incorporates all of the algorithms given above to give
also the statistically non-experienced user the possibility to
estimate parameters from ﬁeld data doing input modeling.
As data sources the user has the choice between an ODBC-
datasource, which is usually a database in the background,
the manual input of failures and suspensions in a spreadsheet-
like form or the selection of single car components from a
tree-structure that contains all parts of a selected model type.
After the selection of the datasource, a manual selection of
the assumed distribution is currently necessary (an automatic
distribution identiﬁcation is currently under development).
After the selection of the estimation algorithm, the user may
insert start values for the estimation. By default, these values
are computed automatically, but if someone wishes to set
some parameters to a speciﬁc value he may use the manual
option. Finally, one can start either the computation of the
point estimation and conﬁdence-interval-calulation altogether
or just the point estimation and then the conﬁdence-interval-
calculation manually afterwards. The reason for this possibility
is, that the calculation of the point-estimation is usually very
fast while the computation of the conﬁdence-intervals takes
more time, such that one should ﬁrst be able to decide
whether the assumed distribution is “correct” before the longer
computation is started. To guide the user in the decision
process if the supposed distribution is “right”, a χ2-goodness-
of-ﬁt test is performed, too. The ﬁnal estimation-results are
presented to the user as the pure parameter values with the
conﬁdence bounds but also the typical plots of CDF, PDF,
hazard-rate or the regression-straight line (if regression has
been used) are provided. Finally,
the resulting parameters
are also provided as tagged data to facilitate the use of the
component by other tools.
The idea behind this data-export facility is, to make the
whole parameter estimation process completely invisible for
the user in the case of input modeling. Because in this case,
users are usually not
interested in and familiar with the
parameter estimation process and its special details but just
need appropriate parameters for their models. For example,
if one wishes to use a transition in a Petri net to model the
failure behavior of a special part, one just wants to select
this part and need not know anything about parameterization.
For this reason, a whole platform has been developed that
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:53 UTC from IEEE Xplore.  Restrictions apply. 
collects failure and production data for various model ranges
that allows easy access to the failure data (for example for a
special country, particular production months . . . ). This way,
the user just choses the apropriate part from the platform
when building up his model. When the analysis (discrete
event simulation or numerical analysis) is started, the data
for the selected part is collected and the parameter estimation
process is automatically performed in the background which
ﬁnally provides tha analysis process with the data it needs, the
distribution with its parameters.
However, the experienced and interested user still has the
possibility to have full control over the parameter estimation
process using the ParEs tool.
B. Examples
In this section,
the accuracy of the methods explained
before will be demonstrated on some examples. For a better
judgement of the results, no real data were used but data that
were generated by a random number generator for the given
distribution. For all examples, the conﬁdence level is set to 95
%. In the Weibull example, 50 failures have been generated
while in the bathtub case 500 failures were used. At the ﬁrst
glance, this seems to be only a few input-data if one considers
the fact, that in the case of ﬁeld data thousands of vehicles
are observed. But one has to consider the following fact: The
data used in this examples are single data while on the other
hand clusters will be used if we have thousands of failures.
Thus, even with a small cluster size of only 1000 miles, 500
clusters would deﬁne a range from zero to 500.000 miles
which will surely cover the whole lifetime of a car such that
the computation-times for the case with 500 failures are the
times that can be expected with real data.
The time for the evaluation of the likelihood function
depends linear on the number of failures and suspension in the
case of single data and on the number of clusters otherwise.
Thus, one would expect that enlarging the size of the sample
will always result in an enlarged runtime in the same order
of magnitude. Practically, this can be taken as a worst case
estimation because a larger sample usually contains more
information. Thus, fewer iteration steps will be needed if MLE
is used such that the increase in runtime will be sublinear in the
average case. As in the regression case there is a closed form
solution, runtime is within a few seconds even with thousands
of failures.
One question that is often asked is, how many failures are
needed to perform a reasonable estimation. If using MLE, the-
oretically one typical failure is enough if there are suspensions
[3]. But if x0 of the three-parameter Weibull distribution has
to be estimated, too, at least 15 failures are needed. Practical
applications show that one can expect reasonable estimates if
there are at least about 20 failures. But using these values
one has to keep in mind that this always means representative
failures for the failure mode under investigation and not only
random failures.
The ﬁrst example is a three-parameter Weibull-distribution.
Table I shows the result of the estimation with the conﬁdence
RESULTS FOR THE THREE-PARAMETER WEIBULL DISTRIBUTION
TABLE I
αl
α
αu
βl
β
βu
x0l
x0
x0u
true value
—
150000
—
—
3.2
—
—
40000
—
MLE
137407
151029
167381
1.572
3.092
4.542
0.736
40494
69144
regression
95435
150969
279169
2.620
2.710
2.801
44859
44859
44860
Fig. 8. The hazard rate for the three-parameter Weibull case
intervals for local optimization with heuristic conﬁdence in-
terval calculation and regression.
Here, al denotes the lower conﬁdence bound for the param-
eter a while au denotes the corresponding upper conﬁdence
bound.
The computation with the local optimization took 18 sec-
onds (point-estimation three seconds, conﬁdence-interval cal-
culation 15 seconds), while the regression needed only four
seconds. The results of the point estimation when using an
evolution strategy are the same as for the use of a local
optimization. There is only a slight difference in the upper
conﬁdence bound of β if an evolutionary strategy or the
deterministic version is used, but then the calculation takes
about ﬁve minutes. One can also easily see that the conﬁdence
intervals for β and x0 in the regression case are too optimistic,
compared to the computed likelihood-ratio bounds. In this
special case,
too optimistic means too small such that a
computation of a worst case failure rate would yield too low
values. Fig. 8 shows the hazard rate for the original and
the estimated parameters. The results for an example of the
bathtub distribution are shown in Table II.
TABLE II
RESULTS FOR THE BATHTUB DISTRIBUTION
parameter
α1
β1
α3
β3
x0
p
true
value
3000
0.5
80000
4.2
65000
0.15
lower
bound
981
0.38
79378
3.64
62562
0.11
point
estimation
2026
0.52
79944
4.16
64645
0.16
upper
bound
4707
0.70
80548
4.789
66032
0.22
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:53 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] http://www.averill law.com/, referenced at 10/07/02.
[2] A. Birolini, Qualitaet und Zuverlaessigkeit technischer Systeme, Theo-
rie, Praxis, Management, Dritte, voellig neubearbeitete und erweiterte
Auﬂage. Springer-Verlag, 1991.
[3] R. B. Abernethy, “The New Weibull Handbook,” 536 Oyster Road, North
Palm Beach, Florida, 1994.
[4] I. Bronstein, K. Semendjajew, G. Musiol, and H. Muehlig, Handbook of
Mathematics, 5th ed. Frankfurt am Main, Thun: Verlag Harri Deutsch,
2000.
[5] W. J. Thompson, Atlas for Computing Mathematical Functions. New
York, Chichester, Brisbane, Singapore, Toronto: John Wiley & Sons Inc.,
1997.
[6] J. Lawless, Statistical Models and Methods for Lifetime Data.
John
Wiley, 1982.
[7] http://functions.wolfram.com/
GammaBetaErf/InverseErf, referenced at 10/07/02.
[8] R. L. Burden and J. D. Faires, Numerical Analysis, 7th ed.
Paciﬁc
Grove: Brooks/Cole, 2001.
[9] S. S. Rao, Optimization,
theory and applications.
Bangalore – Bombay: Wiley Eastern Limited, 1978.
New Delhi –
[10] J. Hartung, B. Elpelt, and K.-H. Kloesener, Statistik, Lehr- und Hand-
buch der angewandten Statistik, zehnte ed. Muenchen, Wien: R.
Oldenbourg Verlag, 1995.
[11] K. S. Trivedi, Probability & Statistics With Reliability, Queuing, and
Computer Science Applications. Prentice Hall, 1982, ch. 10.
[12] H. Shekarforoush, M. Berthod, and J. Zerubia, “Direct Search General-
ized Simplex Algorithm for Optimizing Non-linear Functions,” research
report No 2535, INRIA - France, 1995.
[13] J. Heistermann, Genetische Algorithmen.
Stuttgart, Leipzig: B. G.
Teubner Verlagsgesellschaft, 1994.
[14] R. E. Moore, “Interval analysis,” prentice-Hall, 1966.
[15] R. Hammer, M. Hocks, U. Kulisch, and D. Ratz, Numerical Toolbox for
Veriﬁed Computing I, Basic Numerical Problems, ser. Springer Series
in Computational Mathematics 21. Springer-Verlag, 1991.
[16] J. Luethi and C. M. Llad´o, “Splitting Techniques for Interval Param-
eters in Performance Models,” Dezember 2000, bericht Nr. 2000-07,
Universitaet der Bundeswehr, Muenchen.
[17] W. Q. Meeker and L. A. Escobar, “Maximum Likelihood for Location-
Scale Based Distributions,” extracted from: Statistical Methods for
Reliability Data, John Wiley & Sons Inc., 1998.
Fig. 9. The hazard rate for the bathtub case
On the ﬁrst glance there seems to be a signiﬁcant difference
for the parameter α1. But the plot of the hazard rate in Fig. 9
shows that the inﬂuence of this inaccuracy on the bathtub is
very small. The time for point-estimation was 29 seconds with
the penalty-method and four seconds with the Simplex-method
while the conﬁdence-interval-calculation took 241 seconds on
a Pentium III with 1 GHz.
VI. CONCLUSION AND FUTURE WORK
In this paper we showed how parameter estimation proce-
dures can be used to extract distribution parameters from ﬁeld
data, that can afterwards be used for the input-modeling of
other stochastic models. Besides the application to “classical”
lifetime-distributions, these methods have also been adapted
to a new distribution type and to different data modes, while
the main interest is the usability of the developed algorithms
by standard-users. While runtime can always be optimized, we
think the only real weakness of the tool at the moment is the
necessity of a manual selection of an appropriate distribution.
We hope to overcome this weakness in the near future by an
automatic selection based on parametric and non-parametric
statistical tests, similar to the tests provided by the ExpertFit
tool [1].
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:08:53 UTC from IEEE Xplore.  Restrictions apply.