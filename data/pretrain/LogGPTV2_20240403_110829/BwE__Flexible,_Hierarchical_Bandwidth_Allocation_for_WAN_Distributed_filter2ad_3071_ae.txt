Our current approach is to assign satisßed FlowGroups a
stable allocation that re(cid:6)ects the fair share at inßnite demand.
his allocation is a FlowGroup’s allocation if its demand grew
to inßnity while demand for other FlowGroups remained
the same. When a high priority satisßed FlowGroup’s usage
increases, it will ramp almost immediately to its fair share.
Other low-priority FlowGroups will be throttled at the next
iteration of the BwE control loop. his implies that the ca-
pacity of a constrained link is oversubscribed and can result
in transient loss if a FlowGroup’s usage suddenly increases.
he naive approach for implementing user-fg allocation
involves running our global allocation algorithm multiple
times for each FlowGroup, assigning inßnite demand to the
target user-fg without modifying the demand of other user-
Figure (cid:149): Improving Network Utilization
fgs. Because multiple such runs across all FlowGroups does
not scale, we run one instance of the global algorithm and
pass to Cluster Enforcers the bandwidth function for the most
constrained link for each FlowGroup. Assuming the most
constrained link does not change, the Cluster Enforcer can
eıciently calculate allocation for a FlowGroup with∞ de-
mand in the constrained link, assuming it becomes the bot-
tleneck.
6.4 Improving Network Utilization
BwE allows network administrators to increase link uti-
lization by deploying high throughput NETBLT [ˇ«]-like
protocols for copy traıc. BwE is responsible for determin-
ing the (cid:6)ow transmission rate for these protocols. We mark
packets for such copy (cid:6)ows with low priority DSCP values so
that they absorb most of the transient network packet loss.
To ensure that the system achieves high utilization (>(cid:149)«(cid:236))
without a(cid:242)ecting latency/loss sensitive (cid:6)ows such as web and
video traıc, the BwE Global Enforcer supports two rounds
of allocation.
(for example at (cid:149)«(cid:236) of actual capacity). All traıc types
are allowed to participate in this round of allocation.
● In the ßrst round, link capacities are set conservatively
● In the second round, the Global Enforcer allocates only
● We also adjust link scaling factors depending on loss
copy traıc, but it scales up the links aggressively, e.g.,
to ˇ«(cid:1)(cid:236) of link capacity.
on the link. If a link shows loss for higher QoS classes,
we reduce the scaling factor. his allows us to better
achieve a balance between loss and utilization on a link.
Figure (cid:149) shows link utilization increasing from ˚«(cid:236) to (cid:149)˚(cid:236)
as we adjust the link capacity. he corresponding loss for
copy traıc also increases to an average (cid:134)(cid:236) loss with no in-
creases in loss for loss-sensitive traıc.
6.5 Redundancy and Failure Handling
For scale and fault tolerance, we run multiple replicas at
each level of the BwE hierarchy. here are N live and M cold
standby Job Enforcers in each cluster. Hosts report all task-fgs
belonging to the same job-fg to the same Job Enforcer, shard-
ing di(cid:242)erent job-fgs across Job Enforcers by hashing.
10Cluster Enforcers run as master/hot standby pairs. Job En-
forcers report all information to both. Both instances in-
dependently run the allocation algorithm and return band-
width allocations to Job Enforcers. he Job Enforcers enforce
the bandwidth allocations received from the master. If the
master is unreachable Job Enforcers switch to the allocations
received from the standby. We employ a similar redundancy
approach between Global Enforcers and Cluster Enforcers.
Communication between BwE components is high prior-
ity and is not enforced. However, there can be edge scenar-
ios where BwE components are unable to communicate with
each other. Some examples are: BwE job failures (e.g. binaries
go into a crash loop) causing hosts to stop receiving updated
bandwidth allocations, network routing failures preventing
Cluster Enforcers from receiving allocation from the Global
Enforcers, or the network model becoming stale.
he general strategy for handling these failures is that we
continue to use last known state (bandwidth allocations or
capacity) for several minutes. For longer/sustained failures,
in most cases we eliminate allocations and rely on QoS and
TCP congestion management. For some traıc patterns such
as copy-traıc we set a low static allocation. We have found
this design pattern of defense by falling back to sub-optimal
but still operable baseline systems invaluable to building ro-
bust network infrastructure.
7. EVALUATION
7.1 Micro-benchmarks on Test Jobs
We begin with some micro-benchmarks of the live BwE
system to establish its baseline behavior. Figure ˇ(cid:134)(a) demon-
strates BwE fairness across users running di(cid:242)erent number
of TCP connections. Two users send traıc across a network
conßgured to have ˇ««Mbps of available capacity between the
source/destination clusters. Userˇ has two connections and a
weight of one. We vary the number of connections for User(cid:134)
(shown on the x-axis) and its BwE assigned weight. he graph
shows the throughput ratio is equivalent to the users weight
ratio independent of the number of competing TCP (cid:6)ows.
Next we show how quickly BwE can enforce bandwidth al-
locations with and without the inßnite demand feature (Sec-
tion E.t). In this scenario there are (cid:134) users on a simulated
ˇ«« Mbps link. Initially, Userˇ has weight of t and User(cid:134) has
weight of ˇ. At ˇ(cid:134)«s, we change the weight of User(cid:134) to ˇ(cid:134). In
Figure ˇ(cid:134)(b), where the inßnite demand feature is disabled,
we observe that BwE converges at (cid:1)˚«s. In Figure ˇ(cid:134)(c), where
inßnite demand feature is enabled, we observe it converges at
ˇE«s. his demonstrates BwE can enforce bandwidth alloca-
tions and converge in intervals of tens of seconds. his delay
is reasonable for our production WAN network since large
bandwidth consumers are primarily copy traıc.
7.2 System Scale
A signißcant challenge for BwE deployment is the system’s
sheer scale. Apart from organic growth to (cid:6)ows and network
scale other reasons that a(cid:242)ect system scale were supporting
Figure ˇ«: FlowGroup Counts
Figure ˇˇ: Resource overhead (Control System)
ßne-grained bandwidth allocation and decreasing reporting
interval for enforcement frequency.
Figure ˇ« shows growth in FlowGroups over time. As ex-
pected, as we go up in the hierarchy the number of Flow-
Groups drops signißcantly, allowing us to scale global com-
ponents. Figure ˇˇ shows the amount of resources used by
our distributed deployment (excepting per-host overhead). It
also shows the communication overhead of the control plane.
We can conclude that the overall cost is very small relative to
enforcing traıc generated by hundreds of thousands of cores
using terabits/sec of bandwidth capacity.
Table (cid:134) shows the number of FlowGroups on a congested
link at one point in time relative to all outgoing (cid:6)ow groups
from a major cluster enforcer. It gives an idea of overall scale
in terms of the number of competing entities. here are por-
tions of the network with millions of competing FlowGroups.
Table t shows our algorithm run time at various levels in the
hierarchy. We show max and average (across multiple in-
stances) for each level except global. Overall, our goal is to
enforce large (cid:6)ows in a few minutes, which we are able to
achieve. he table also shows that the frequency of collecting
and distributing data is a major contributing factor to reac-
tion time.
One Congested Link
Largest Cluster Enforcer
Avg across cluster enforcers
Global
site
ttE
(cid:1)(cid:1)
(cid:1)t
ˇ(cid:1)(cid:149)(cid:16)
cluster
t.«k
t.(cid:1)k
ˇ.(cid:1)k
(cid:16);.(cid:16)k
user
(cid:16)«k
Etk
(cid:134)(cid:134)k
E˚(cid:134)k
job
(cid:16)««k
ˇE(cid:1)k
E«k
ˇ˚(cid:134)(cid:1)k
task
ˇ(cid:134);ˇ(cid:16)k
ˇ(cid:1)EE«k
E(cid:16)(cid:149)Ek
ˇ(cid:149)(cid:16)«˚˚k
Table (cid:134): Number of *-fgs (at various levels in BwE) for a con-
gested link and for a large cluster enforcer.
Global Enforcer
Cluster Enforcer
Job Enforcer
Algo Run-time
Max(s)
t
.ˇE
<«.«ˇ
Mean(s)
-
.ˇ(cid:1)
<«.«ˇ
Algo
Interval(s)
ˇ«
(cid:16)
(cid:16)
Reporting
Interval(s)
ˇ«
ˇ«
(cid:1)
Table t: Algorithm run time and feedback cycle in seconds.
Algorithm interval is how frequently algorithm is invoked
and Reporting interval is the duration between two reports
from the children in BwE hierarchy.
50.0k400.0k800.0k1.2M1.6MFeb 2012Jul 2012Jan 2013Jul 2013Dec 20130.030.0M60.0M90.0M120.0M150.0M180.0Muser/job/cluster flow groupstask flow groupsuser flow groups (left)job flow groups (left)cluster flow groups (left)task flow groups (right)0.0100.0M200.0M300.0M400.0M500.0M600.0M700.0MFeb 2012Jul 2012Jan 2013Jul 2013Dec 2013150200250300350400450500550Bits/sCoresCores (right)Control Traffic - WANControl Traffic - Total11(a) BwE Fairness
(b) Allocation Capped at Demand
(c) Allocation not Capped at Demand
Figure ˇ(cid:134): BwE compliance
Number of user-fg
Number of job-fg
Usage
Mean
;(cid:236)
E(cid:236)
(cid:149);(cid:236)
Min
t(cid:236)
t(cid:236)
(cid:149)(cid:16)(cid:236)
Max
ˇˇ(cid:236)
ˇ«(cid:236)
(cid:149)(cid:149)(cid:236)
Table (cid:16): Percentage of FlowGroups enforced.
BwE tracks about (cid:1)««k user-fgs and millions of job-fgs, but
only enforces a small fraction of these. Processing for unen-
forced FlowGroups is lightweight at higher levels of the BwE
hierarchy allowing the system to scale. Table (cid:16) shows the frac-
tion of enforced (cid:6)ows and the fraction of total usage they rep-
resent. BwE only enforces ˇ«(cid:236) of the (cid:6)ows but these (cid:6)ows ac-
count for (cid:149)(cid:16)(cid:236) of the traıc. We also found that for congested
links, those with more than ˚«(cid:236) utilization, less than ˇ(cid:236) of
the utilization belonged to unenforced (cid:6)ows. his indicates
BwE is able to focus its work on the subset of (cid:6)ows that most
contribute to utilization and any congestion.
We introduced a number of system optimizations to
address growth along the following dimensions:
ˇ) Flow
Groups: Organic growth and increase in specißcity (for e.g.,
delegation). For example, the overall design has been serv-
ing us through a growth of ˇ«x from BwE’s inception ((cid:134)«M to
(cid:134)««M). (cid:134)) Paths: Traıc Engineering introduced new paths in
the system. t) Links: Organic network Growth (cid:16)) Reporting
frequency: there is a balance between enforcement accuracy
and resource overhead. (cid:1)) Bottleneck links: he number of
bottleneck links a(cid:242)ects the overall run time of the algorithm
on the Global Enforcer.
8. DISCUSSION AND FUTURE WORK
BwE requires accurate network modeling since it is a key
input to the allocation problem. his is challenging in an en-
vironment where devices fail o(cid:22)en and new technologies are
being introduced rapidly. In many cases, we lack standard
APIs to expose network topology, routing and pathing infor-
mation. With So(cid:22)ware Deßned Networking, we hope to see
improvements in this area. Another challenge is that for scal-
ability, we o(cid:22)en combine a symmetric full mesh topology into
a single abstract link. his assumption however breaks dur-
ing network failures and handling these edge cases continues
to be a challenge.
Our FlowGroup abstraction is limited in that it allows
users sending from multiple source/destination cluster pairs
over the same bottleneck link to have an advantage. We are
exploring abstractions where we can provide fair share to all
users across all bottleneck links irrespective of the number
and placement of communicating cluster pairs. Other areas
of research include improving the reaction time of the control
system while scaling to a large number of FlowGroups, pro-
viding fairness at longer timescales (hours or days) and in-
cluding (cid:6)ow deadlines in the objective function for fairness.
8.1 Broader Applicability
Our work is motivated by the observation that per-(cid:6)ow
bandwidth allocation is no longer the ideal abstraction for
emerging WAN use cases. We have seen substantial bene-