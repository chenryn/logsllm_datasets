They also consider 40,000 seconds unavailable time per day for an equipment being a failure. This paper boasts 93% prediction accuracy (though class balance data is not shared), however the results shown are incomplete.These results would tend to show that there is potential for using more complex, RNN-type methods in NFP as propagating the time sequence context could provide better results than traditional single-vector inputs.S. Zhang et al. [46] work on a set of about 10,000 switches of 3 different models in a Data Center (DC). They point out that in the case of DCs, where there can be several thousand switches online at the same time, the ability to predict switch hardware failure is paramount to ensure desirable QoS levels. Indeed, switch failures are responsible for 74% of downtime in modern DCs. Therefore ensuring an uneventful and pre-ventive replacement of failing switches may prove to be a very effective way to ensure DC QoS.In a given DC, there is usually only one or two different models of switches installed, and the network is very homo-geneous. Thus they propose a model, PreFix, aimed to be trained specifically for each model used in DCs. PreFix uses batches of syslog messages in a switch, mapped onto generic templates. In this case, the duration covered by the batch is 30 minutes. They then extract four features from the batch : frequency, surge, seasonality and the sequence. The model then predicts whether or not the batch of syslog messages represents what the authors call a switch failure omen. A switch failure omen being a sequence of messages that appear less than a determined amount of time (in their case 24 hours) before a switch failure occurs in the equipment.This model was trained on three switch models, on a database composed of data from around 10,000 switches over a period of 3 years, where switch failures were manuallyclassified by network operators. The results obtained are an average of 60% recall on the evaluation, with a False Positive Rate (FPR) of around 1.2 per 10,000 switches, per day. This FPR is deemed acceptable by the audited DC. In practice, this would mean that, for the DC, the cost of replacing misdiagnosed switches added to the cost of replacing the failing switches that were accurately identified, is inferior to the total cost of downtime that would be caused by the failing switches that were accurately identified by the model. An idea could be to run additional diagnostics on the predicted positive class equipment to see if this cost to reward ratio could be improved a posteriori of the first prediction.In any case, this method seems to be effective in the case of very homogeneous networks, and with the wide time interval predicted, so it should provide enough time for logistics, a clean transition and replacement of the failing switches. The low FPR also contributes to the value of this model as it would help incorporate the model into the decision-making with minimal error cost. However, the dataset is heavily imbalanced and there seems to be around one failure per day in the dataset, which would mean there are more false positives than true positives predicted. As stated above, further verification could improve the cost to reward ratio of this model. This model also provides specific fault type information as the sequences of logs may be used to give a prognosis on the type or reason of the incoming failure. This model therefore seems adequate to base decisions upon. In the case of Heterogeneous Networks however, the method needs some adaptation to generalize it, as it is very specific to the equipment’s design.The authors plan to tune PreFix to more switch models, and try to tackle other types of switch failure. The trained models are available .
Boldt et al. [39] work on a set of 4G cellular base stations and the minimisation of the incident response time to restore the service provided by the base stations. They determine that given the increasing importance of networks, the capacity for cellular network service providers to anticipate service interruptions is paramount to ensuring maximal QoS for their customers.A point to be noted is that although the authors describe their work as alarm prediction, they describe what they predict as severe alarms which they define as radio commu-nications in one of the sectors of the cell is interrupted. This is for all intents and purposes a complete network failure for
10 	VOLUME 4, 2022
Murphy et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALSthat antenna in our case.
The authors seek to determine which ML models have the best performance with minimal optimization, and what is the best time ahead to use considering their data and chosen model. To this end, they setup two experiments : one to select a best-performing model based on the 1 hour ahead data based on prediction performance; and the second to evaluate the evolution of performance depending on time ahead selected (from 10min to 48h).The data they use consists of classic monitoring data for 4G cells, with 231 features. They balance the no-failure/failure classes by a method of sampling, and normal-ize the features.In the first experiment, DT, SVM, MLP, RF, and XGBoost models are evaluated using 10-fold cross-validation (a method where the model is evaluated 10 times along different splits of the training data in different training and validation sets) and performance is compared. They conclude that RF has performed best along the Area Under Curve (AUC) metric with ∼ 81% Precision and ∼ 62% Recall. MLP and XGBoost also perform similarly. RF is therefore chosen for the second experiment. The authors also evaluate the statistical relevance of these tests in comparing the different models.In the second experiment, the RF model is evaluated with 10min, 20min, 30min, 1h, 2h, 3h, 6h, 12h, 24h, 48h ahead time. This experiment shows high performance in F1-score, between 10min and 3h ahead time, with a sharp drop between 3h and 6h. Perhaps this gap could be investigated in more detail in the future to determine an optimal time ahead to prediction performance ratio. The ELI5 module, a python module dedicated to explaining the predictions of classifiers, is also used to investigate the feature importance in decisions and identify several features heavily linked to the prediction. They conclude that the necessary time ahead and precision metrics are linked to the decision making process that takes place after the prediction for maintenance of the model, so there is a need to investigate the decision boundary and the different performance behaviours of the models along these dimensions. Although they have one model that works well for one type of fault, they will experiment on more types, and train their models with more hyper-parameter tuning.With regards to prediction performance discussed in the study, in the case of the selected ML model, that is to say RF, the levels shown for 1h and 3h ahead time for example seem satisfactory while still being actionable. Respectively, the precision and recall results given are ∼ 81% and ∼ 62% be determined, however, in order to determine the best time for 1h, and ∼ 77% and ∼ 56% for 3h. What remains to ahead to prediction performance for the model, is the cost of false positives in the case of a cellular network and what corrective actions (with respect to their cost) can be taken to reduce the impact of failures before they occur.This study shows that there is a relevant need in investi-gating the different prediction performance levels at different time ahead values enabled by the data and the chosen mod-
els. Future works would be more complete if they were to integrate such a study.Rafique et al. [69] publish a tutorial to apply ML-based techniques for networks in general. In this work, they present the idea of a two-step fault detection method. The first phase is a low computation cost model using the minimal number of necessary features to have an overview of the health status of the equipment. This model determines whether there seems to be anomalous behavior within the network or the equip-ment. If such behavior is detected, the first model triggers a second, more in-depth, investigation. The second phase being a much more precise diagnosis, which uses more network and computation resources. Such an idea seems reasonable to apply to fault prediction, as this could reduce the network and computation costs of distributed fault prediction models, while aiding to reduce error in predictions and decision making.Velasco et al. indicate the previously cited method was tested for predictive maintenance in [54]. They indicate that ML techniques seem adequate to introduce dynamic adapta-tion of the network to conditions, however they do not share any results.
B. RTTF 
Predicting the RTTF is a regression problem. As was de-scribed in section II-B1, uncertainty should reduce as the incoming failure approaches.Pellegrini et al. [31] work on the effect of server appli-cations failures on the network equipment (the server). They point out that the slow build-up of random errors in applica-tions may account for a large part of anomalies on servers and addressing this in advance may be an opportunity to reduce equipment failure occurrence (and therefore network failure). Additionally, an effective technique for dealing with these errors is to force application or system rejuvenation, which consists in returning the application or system to a clean state. This is often achieved by restarting the system.They therefore propose Framework for building Failure Prediction Models (F²PM) [31], that is a framework to gener-ate application-caused failures on the network and use them to train a model to predict the exact Remaining Time To Failure (RTTF) for applications, due to memory leakage and non terminated thread build-ups. The paper uses data point aggregation and local derivation as their feature selection and pre-treatment. This is in order to allow a history of relevant information to be used without overloading the model with unnecessarily high resolution data.The paper presents experiments using the data the frame-work generates on six types of models. They obtain very precise predictions of failure in the 0 to 10 minutes RTTF interval, using M5P or REP-Tree models. However, in the models, the predicted RTTF seems to plateau and fall back down to around 10 minutes when the real RTTF starts to exceed 10 minutes. This is an issue as it could lead to a high false positive rate (defined in section II-B2) when selecting a predicted RTTF threshold to raise an alert. Maybe thisVOLUME 4, 2022 	11
Murphy et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS’plateau and fall-back’ issue can be explained by the design of the database where the failure is designed to always occur relatively quickly, and these issues could maybe be solved by widening the range of training examples and introducing seg-ments of data without a failure occurring at the end. However, the results presented seem to show no such problem in the 0 to 8 minute range. Therefore, maybe an acceptable threshold could be found. And, in practice, a 8 minute reprieve or less (when the model seems to still be accurate) is largely sufficient to save the application’s ongoing work and restart it from a clean state.Perhaps this method can be replicated to more causes of network faults, but it is important to note that these application-related errors stem from ever-growing stacks of small errors that do not release the computation and memory resources. Whereas non application-related faults may also depend on the user load and network congestion, which vary greatly with time, and the state of other equipment. This might have a significant impact on RTTF prediction accuracy if this method is used for other types of networks. To conclude, this paper proposes a good method to predict RTTF with good quality of prediction for RTTFs below 10 minutes for application related faults.C. NETWORK HEALTH 
Some studies have focused on network health as a whole, instead of individual equipment. Depending on the size of the network, this approach can help reduce the class imbalance issue as there will be more frequent failures when considering more equipment. Modeling network health as a whole can also help plan maintenance teams’ work.Snow et al. [29] tackle the dependability of 2G cellular networks infrastructure. The Federal Communications Com-mission (FCC) changed its rules on the declaration of out-ages. Since then, and at least during the time that study was conducted, mobile operators must declare network failures that exceed 30 minutes of time, and result in at least 15,000 lost subscriber hours of service. This makes it of capital importance for wireless carriers to improve their network dependability, in order to defend economic priorities.The authors assess that the optimal strategy with regards to improving network dependability has not been determined. They therefore study the evolution of network dependability under the new FCC rule, according to two dimensions of network dependability : reliability and maintainability. They also establish a model to attempt to predict the number of occurrences of network failures across time. They determine that a good strategy is to maintain reliability of the network (constant MTBF), while increasing maintainability (decrease Mean Time to Restore - MTR).This seems logical, as the MTR network operations influ-ences on the two points of the declarable outages rule of the FCC. Indeed, when the MTR is lower, there is both a higher chance that the failure will last less than 30 minutes, and that there will be less than 15,000 lost subscriber hours. This point is interesting, since, as seen in section III, other industryactors are affected by similar criteria. NSIs and ISPs have to restore network operation within differing timeframes, and pay differing penalties when they cannot, depending on the criticality of the failures (which usually depends on the num-ber of users dependant on the operation of the equipment).Kumar et al. [32] work on cellular network data. They recognize that with the growing complexity and importance of cellular networks, mobile network faults become more common as their causes multiply, ranging from power out-ages, software or hardware malfunction, and constructor incompatibility, to misconfiguration of networks and cell density. The authors contend that the reactive maintenance of such networks is no longer appropriate given the possibility for most mobile network providers to introduce predictive maintenance (large capacity for data collection and emer-gence of predictive techniques).Kumar et al. realize a study to test different models for predicting network faults. The experiments are conducted on a month’s worth of mobile operator data. They aim to predict when the network will require maintenance to ensure the best QoS for their clients, using several different models to determine the best of them. The methods tested were : linear regression [70]; exponential regression [71]; linear SVM regression [72]; Gaussian (RBF) SVM regression [72]; different simple Neural Network [67] models (with around 20 neurons); and an auto-encoder [68]. A Continuous Time Markov Chain Analysis [73] was also used to analyze the data. The prediction made is whether a new fault will occur in the next 4 hours in the whole cellular network. However, the formulation of the problem makes it so that the network is considered unhealthy 95% of the time, while only one or a few of its nodes are failing. So there may be problems with always considering the network as a whole (when it reaches a certain size), as there would be difficulty in finding the faulty node using such a model. Perhaps a distributed view, while more costly in calculation resources, might be more capable of adaptation to the different conditions present in the network. Redefining the bounds of the network considered could also allow for better class balance in the data. Further analysis is necessary before it is possible to use the prediction for solving incidents.We may note that after testing the prediction with several models, the auto-encoder model seems to perform better than the other models that were tested.It seems that large cellular networks are failing most of the time, but the failure is localized to few or a singular equipment. Therefore there is an additional need in this case to be able to locate where the failure will occur in the future, in order to launch maintenance operations in advance. For reference, in the case of a large scale cellular infrastructure, in 30% of cases, the healthy network will experience the next failure in less than an hour, and in 87% of cases, it will experience failure in less than six hours, according to the data of the study.12 	VOLUME 4, 2022
Murphy et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
D. LINK FAILURE 
Most studies focus on equipment failures since when the equipment has a problem and is repaired or replaced, the network problem is also solved. However, there is also the possibility to reconfigure routes and avoid failing links be-tween two network nodes, as some routing protocols rely on link states. Therefore there is a possibility of mitigating the consequences of link failures by rerouting in the case of predicted link failures.Ibrar et al. [34] work on a Hybrid SDN (HSDN) archi-tecture composed of legacy (non SDN) and SDN activated switches. They identify the need in HSDN networks for legacy switches link failure information to be communicated quicker to the Controller. Indeed when the legacy switches broadcast their link states, through the Open Shortest Path First (OSPF) routing protocol for example, this information must first reach SDN switches before the SDN switches themselves relay the information to the controller. This two-step process introduces delays and issues in the reconfigu-ration of the network to adapt to failures and causes perfor-mance issues.Therefore the authors propose to predict link failures in the HSDN so that information reaches the controller quicker and the configurations can be changed in time for minimal interruption (while maintaining network coherence), as it is one of the functions of the SDN controller.A simulation is built to accumulate data with 38 SDN compatible switches linked by 519 links into a single net-work. Varying percentages of switches are chosen randomly to function in legacy mode (not part of the SDN). Two models are then trained on the accrued data, a Logistic Regression and a SVM. In this study, the failure is considered predicted successfully if it is mitigated before it happens (there is no time in advance of the failure considered). In this case, miti-gation means that the routes were recalculated incorporating the failure prediction information and the new routes were propagated before the occurrence of total failure.The Logistic Regression model achieves the higher perfor-mances with 74% precision and 81% recall, and the SVM with 69% precision and 71% recall. These results seem to show that the models could very well be integrated into SDN controllers to increase prediction performance. However it is important to note that these results seem to vary depending on the proportion of SDN switches in the network and the rate of failure of the links. Further experimentation would be needed to determine the performance to be expected according to the number and proportion of equipment and rates of failure in order to determine the effectiveness of the method. Also the additional CPU and memory cost to the SDN controller should be taken into account as it might become an issue in larger networks.This study shows an alternative to equipment failure pre-diction as redirecting network flow in prevision for failure could allow for the network to always be running, and the replacement of equipment could still happen after the fact without QoS and QoE impacts. However this method relies
on the implementation of SDN in the network and SDN is still far from widespread today. Perhaps in the future this could be a direction to solve the NFP problem.E. ALARM PREDICTION 
While similar to equipment health prediction, alarm predic-tion consists of predicting whether an alert will be raised rather than complete failure will be identified. An alert can be raised if certain monitoring values rise above a certain threshold, or if a customer complaint is registered for the equipment associated with the service.Qian et al. [49] work on predicting incoming alarms in an Internet Protocol Television (IP TV) network. They spot that for IP TV networks, faults are primarily detected and reported by the end user, which introduces a delay in the troubleshooting and repair, and has a negative impact on customer satisfaction.They therefore try to automate fault detection in IP TV, as that would reduce delays in detection and enable the service provider to restore quality of service before the client experiences significant trouble.In order for the detection to be based on accurate data, they collect Quality of Service (QoS) data from the next-hop routers from the end users. They then select features among those KPIs by deleting those with low variance, those with low (linear or non-linear) correlation to the Quality of Ex-perience (QoE) of end users. The Random Forest algorithm was used to estimate non-linear correlation of the features with the label.The data is then fed into an AdaBoost model using its original error function and another AdaBoost model using F1 score as its error function in parallel [50].
They achieve 100% recall, with a 10% false positive rate on their test data with their improved AdaBoost model. How-ever, the prediction that is made is a prediction of whether a customer is willing or not to make a complaint. This definition of failure is subject to variance depending on the customer, and whether a certain QoS level will lead him to take action.The results show that it is possible to evaluate customer and network users QoE in the demanding setting of IP TV (high jitter and thoughput constraints). Such a system may be adapted to predict network faults and their criticality for the client or the end user.Zhuang et al. [59] work on a large scale network (pre-sumed to be IP). They recognize that network monitoring data is quite dirty, as there is often empty data, due to congestion or equipment failures, and data is not normalized, as different manufacturers give different performance mea-sures, etc. There is therefore a necessary and hefty process of cleaning the data or else NFP performance will not be satisfactory, and even cleaning the data does not yield better results as there is often not enough data left to adequately train models. As such, the authors try to build upon other works on generation of coherent data based on pre-processed real data.VOLUME 4, 2022 	13
Murphy et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
After cleaning and pre-processing their data based on 50,000 alarms and 10,000,000 performance samples, they are left with 58 alarm sequences of data. They propose to generate more data with a GAN model, and train the model accordingly until it reaches Nash equilibrium. They then use the model to generate 6000 samples, 4,500 for training, and 1,500 for testing. Performance of an MLP trained using this data is then compared to performance of a model trained on just the clean data, another trained on augmented data using another method.They boast a 99.9% accuracy and a 2.2% increase in performance compared to the other augmentation method. However, a common problem with data augmentation is the difficulty in determining the overlap between training and testing data, and it is therefore difficult to determine the part of increased performance that is due to an eventual overlap. A test of the augmented-data trained model using non-overlapping original data as a test set could be used to evaluate this effect.Perhaps this method could be more widely tested in a benchmark on several ML methods of the previously men-tioned studies in order to verify that these results generalize, but this method seems promising to ensure quality of data.
VI. PERSPECTIVESFault prediction has historically been more about predicting whether or not a network or a particular equipment would still be in a healthy state (functioning within performance expectations) in a future period of time. In some cases, it has also been about trying to determine the remaining time before the next failure on the equipment or the network, or link state failure, or predicting alarms raised. These predictions would then allow the actor in charge of maintenance to engage in proactive action and so reduce the MTTR.However, in order to fully neutralize the impact of a component failure, a system that would, at a minimum, allow zero-loss and zero-delay handover is needed. This system could additionally trigger the intervention (if needed) enough time before the failure occurs so that the operations happen before users experience faulty service. In order to be applied in real networks, such a model would need to give an accurate failure prediction, locate the faulty equipment, and be able to either implement a response automatically, or advise the network administrators on policies to mitigate the predicted failure. In addition, an estimate of the criticality of the failure would also be desirable, in order to allow network adminis-trators to budget and make decisions accordingly. Therefore, in order to enable zero-loss and zero-delay failure handover networks to emerge, NFP would need to become an extension of fault detection, root cause localization and fault mitigation in the future.We have already seen a system that goes in that direction in Ibrar et al. [34], where the authors use an SDN controller to reroute traffic to avoid failing links. However if the objective is to completely neutralize the impact of failures, the system could be generalized to other types of networks (not only
HSDN and not only switches), and alarms could be raised to ensure maintenance for faulty nodes.Several promising areas for future study have been identi-fied in the following paragraphs.
Network supervision data is usually noisy, presents miss-ing data points and is not normalized. Several studies have tried to establish a method for treating this issue [43], [49], [59]. However their methods have not been compared yet on a single dataset and perhaps better performing methods can be found.In the case where we have several concurrent elements to form one prediction - for example failure prediction occur-rence, localization of the failure, proactive mitigation and criticality - each predicted element needs to pertain to the specific fault instance. In the case of a large network for example, there could be several failures occurring in different places in the network in a short time frame. It would be highly undesirable for the different elements to concern unrelated failures. In order to ensure coherence of the global prediction, perhaps they could be realized in a sequential manner, using the output of all the previous predictions as input for the next one (for example using the predicted information of failure occurrence and fault localization as part of the input for predicting criticality of the failure). Another option could be to design the ML architecture using an intermediary com-mon feature extraction system as input for all the different models. A third option could be to remove the need for certain predictions through other means. The experiment in Hood et al. [61] may indicate the possibility that we could use a limited number of prediction nodes, scattered in the network, in order to locate incoming failures by some sort of triangulation. This would allow us to solve the equipment localization problem without designing a separate model to predict the future failing equipment.Another issue would be to determine the calculation costs for each method, that could be used for comparison and optimization purposes for choosing equipment and deter-mining running costs. In the case where the NFP model is implemented in network equipment such as is the case in SDN, it would be useful to ensure that the additional processing overhead necessary to run the predictions does not cause further network failures.One additional problem identified and partially treated in Boldt et al. [39] is that we have no benchmark to compare the different methods yet. This is mostly due to a lack of a com-monly shared database for failure prediction and the wide disparity of types of networks that were studied (cellular, IP, DC, IPTV, optical, etc). There is therefore a need to introduce one or several public datasets and establish a performance benchmark according to several common prediction metrics. Lastly, for a heterogeneous network NFP model to be implemented, a database large enough to cover most cases of faults and failures would be needed, and as such, should span multiple different networks. However, although there are databases for Fault Detection [74]–[76], and has been proposed for the sharing of databases for cognitive14 	VOLUME 4, 2022
Murphy et al.: Preparation of Papers for IEEE TRANSACTIONS and JOURNALS
network management in [77], to the best of our knowledge there are no databases available freely to researchers for Fault Prediction [23]. A freely available benchmark dataset of a heterogeneous network where all these methods could be used would be a first step towards bringing all these methods together.VII. CONCLUSIONS
An overview of NFP, and a survey of the different studies of NFP using ML, along with a presentation of the ML methods used, were outlined in this work. The expectations of an actor of the field and future perspectives for research were also laid out.It has been shown that in NFP, ML is a promising technol-ogy that remains in the early stages of development. Certain ML methods, such as transformers (attention-based models) and RNN, are receiving a lot of attention in other fields recently for their potential to deal with sequential input, and are therefore very promising for future NFP research.Efforts have often been centered on homogeneous net-works, dedicated to a single technology and manufacturer. However, companies now move towards a single network to transit all their multi-technology data, therefore making their networks ever more heterogeneous, and it is crucial to improve network dependability in order to reduce losses, and optimize performance. As a consequence, as networks are becoming ever more heterogeneous and critical, systems that are able to maintain efficiency while in such an environment are becoming increasingly desirable, and further work is required in this area.Training effective models for heterogeneous networks would also need a set of common features, shared across the different types of equipment involved. To this end, testing the variation of prediction performance between using a narrower set of features, common to all types of equipment, and using a wider set of specialized features that are not common to all types of equipment seems worthy of further investigation.Overall the different studies surveyed in the field of NFP treat a very wide selection of mostly-homogeneous network types, which makes the results difficult to compare to each other. Each of the applications considered tends to have a different set of characteristics and so the authors set up each problem differently.As a consequence of specific networks having different crucial constraints, the prediction performance metrics over-all are not uniform, which contributes to the difficulty in comparing studies.
The field of NFP lacks open data and a way to measure performance comparatively. NFP would benefit significantly from the establishment of one or several benchmark datasets, where the different methods studied could be tested, in order to set the stage for more consolidated research. Research studies remain to be explored to deal with the key issues of NFP for heterogeneous networks.REFERENCES
[1] “Calculating 	the 	cost 	of 	downtime 	in 	your 	business,”	Axcient, 	White 	Paper, 	2018. 	[Online]. 	Available: 		
[2] ying the results of 	deployed organizations,” IDC, White Paper #US44952119, April 2019.
	[Online]. Available: 	
[3] A. Larmo, P. Von Butovitsch, P. Campos Millos, and P. Berg, “Critical 	capabilities for private 5g networks,” Ericsson, White Paper.[4] B. Wang, Z. Qi, R. Ma, H. Guan, and A. V. Vasilakos, “A survey on data center networking for cloud computing,” Computer Networks, vol. 91, pp. 528–547, 2015.
[5] Y. Liang, Y. Zhang, A. Sivasubramaniam, M. Jette, and R. Sahoo, “Blue-	gene failure analysis and prediction models,” in International Conference 	on Dependable Systems and Networks (DSN’06). 	IEEE, 2006, pp. 425–	434.[6] F. C. Commission, “Docket no.04-35, fcc 04-188,” August 2004.
[7] A. Zolfaghari and F. J. Kaudel, “Framework for network survivability performance,” IEEE journal on selected areas in communications, vol. 12, no. 1, pp. 46–51, 1994.
[8] A. Snow, “A survivability metric for telecommunications: insights and 	shortcomings,” in Proc. Information Survivability Workshop, 1998.[9] ——, “The failure of a regulatory threshold and a carrier standard in 	recognizing significant communication loss.”	TPRC, 2003.
[10] D. Chen, S. Garg, and K. S. Trivedi, “Network survivability performance evaluation: A quantitative approach with applications in wireless ad-hoc networks,” in Proceedings of the 5th ACM international workshop on Modeling analysis and simulation of wireless and mobile systems, 2002, pp. 61–68.[11] A. P. Snow, U. Varshney, and A. D. Malloy, “Reliability and survivability of wireless and mobile networks,” Computer, vol. 33, no. 7, pp. 49–55, 2000.
[12] U. Varshney, A. P. Snow, and A. D. Malloy, “Measuring the reliability and survivability of infrastructure-oriented wireless networks,” in Proceedings LCN 2001. 26th Annual IEEE Conference on Local Computer Networks. IEEE, 2001, pp. 611–618.[13] A. Snow, U. Varshney, and A. Malloy, “A framework for simulation model-ing of reliable available and survivable wireless networks,” in Proceedings of Workshop on Wireless Local Networks, 2001.