title:Logical Partitions on Many-Core Platforms
author:Ramya Jayaram Masti and
Claudio Marforio and
Kari Kostiainen and
Claudio Soriente and
Srdjan Capkun
Logical Partitions on Many-Core Platforms
Ramya Jayaram Masti, Claudio Marforio, Kari Kostiainen,
Claudio Soriente, Srdjan Capkun
Institute of Information Security, ETH Zurich
ﬁPI:EMAIL
ABSTRACT
Cloud platforms that use logical partitions to allocate dedi-
cated resources to VMs can beneﬁt from small and therefore
secure hypervisors. Many-core platforms, with their abun-
dant resources, are an attractive basis to create and deploy
logical partitions on a large scale. However, many-core plat-
forms are designed for eﬃcient cross-core data sharing rather
than isolation, which is a key requirement for logical parti-
tions. Typically, logical partitions leverage hardware vir-
tualization extensions that require complex CPU core en-
hancements. These extensions are not optimal for many-
core platforms, where it is preferable to keep the cores as
simple as possible.
In this paper, we show that a simple address-space isola-
tion mechanism, that can be implemented in the Network-
on-Chip of the many-core processor, is suﬃcient to enable
logical partitions. We implement the proposed change for
the Intel Single-Chip Cloud Computer (SCC). We also de-
sign a cloud architecture that relies on a small and disen-
gaged hypervisor for the security-enhanced Intel SCC. Our
prototype hypervisor is 3.4K LOC which is comparable to
the smallest hypervisors available today. Furthermore, vir-
tual machines execute bare-metal avoiding runtime interac-
tion with the hypervisor and virtualization overhead.
1.
INTRODUCTION
A logical partition is a subset of the physical system re-
sources that can run an operating system independently of
the rest of the system [19]. Logical partitions are widely used
in high-assurance virtualized environments such as formally-
veriﬁed separation kernels [13, 26] and commercial hypervi-
sor deployments [1, 2, 19]. For example, IBM utilizes logical
partitions in its Infrastructure as a Service (IaaS) cloud [4].
Hypervisors that use logical partitions, provide dedicated
resources to the hosted operating systems and, therefore,
beneﬁt from lightweight resource management and a small
Trusted Computing Base (TCB). Logical partitions also al-
low to reduce the runtime attack surface by minimizing the
interaction between the hypervisor and the hosted operating
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ACSAC ’15, December 07-11, 2015, Los Angeles, CA, USA
c(cid:13) 2015 ACM. ISBN 978-1-4503-3682-6/15/12. . . $15.00
DOI: http://dx.doi.org/10.1145/2818000.2818026
systems — a technique called hypervisor disengagement [39].
Other beneﬁts of logical partitions include improved perfor-
mance [15] and eﬃcient nested virtualization [36].
A many-core processor consists of a large number of sim-
ple, energy-eﬃcient cores integrated on a single processor
die. Due to their abundant resources, many-core platforms
may be used, in principle, for deploying systems that scale
up to hundreds, or even thousands [7], of logical partitions.
However, the available many-core systems [7, 21, 22, 41] are
designed for high-performance computing applications and
allow data sharing across cores. Consequently, such many-
core architectures are not tailored to support eﬃcient isola-
tion which is a mandatory requirement for logical partitions.
In this paper, we investigate the feasibility of enabling log-
ical partitions on many-core processors, in order to integrate
the processors into IaaS clouds. Motivated by the scalability
beneﬁts of simple cores, we discard the option of relying on
hardware virtualization extensions. We show that, without
virtualization extensions, logical partitions can be facilitated
by a simple address-space isolation mechanism, implemented
in the Network-on-Chip of the many-core processor. This
simple hardware enhancement does not aﬀect the processor
core’s complexity and therefore supports many-core proces-
sor scalability.
We use the Intel SCC many-core architecture [22] as a
case study and demonstrate the feasibility of our solution
with an emulated software prototype. With the proposed
hardware changes in place, we design a simple, disengaged
hypervisor for many-core processors and integrate it into an
IaaS cloud. Our prototype hypervisor has an implementa-
tion size of only 3.4K LOC that is comparable to the smallest
hypervisors available today [46]. In contrast to solutions like
NoHype [39], our hypervisor does not rely on hardware vir-
tualization extensions. Furthermore, similar to [15, 36], our
solution allows bare-metal execution of VMs, hence elimi-
nating virtualization overhead.
This work demonstrates that many-core processors are an
attractive basis for implementing secure cloud computing
services. To summarize, we make the following contribu-
tions:
1. We show that on many-core platforms logical parti-
tions can be implemented with a simple hardware en-
hancement, avoiding the need for hardware virtualiza-
tion extensions. We use the Intel SCC architecture as a
case study and propose minor hardware enhancements
that enable secure partitions.
2. We design and implement a complete IaaS architec-
ture that leverages the security-enhanced Intel SCC
Figure 1: Many-core architecture overview. Most
many-core processors are organized in tiles that are con-
nected to memory and peripherals over a Network-on-Chip.
Each tile has one or more cores, network interface and op-
tionally some local memory and a DMA engine. Each pro-
cessor could include a host interface if it is a co-processor.
platform. The architecture includes a disengaged hy-
pervisor with a TCB size comparable to the smallest
known hypervisors.
3. We evaluate the performance of our architecture and
demonstrate its suitability for practical cloud deploy-
ments.
The rest of this paper is organized as follows. In Section 2
we provide background information on many-core architec-
tures. Section 3 describes hypervisor design alternatives and
explains the goals of this work. Section 4 presents our se-
curity enhancements to the Intel SCC platform and an em-
ulated implementation.
In Section 5 we describe an IaaS
cloud architecture, its implementation and evaluation.
In
Section 6 we discuss deployment properties. Section 7 re-
views related work and we conclude in Section 8.
2. BACKGROUND
2.1 Many-Core Architectures
A many-core platform consists of a large number of cores
(hundreds or even thousands [7]) integrated into a single
processor chip. Figure 1 shows the general architecture of
a many-core platform. Many-core processors are typically
organized as tiles that can access memory and peripherals
over an interconnect. A network interface connects the com-
ponents on each tile to the Network-on-Chip (NoC). Each
tile contains a set of cores and, optionally, some local mem-
ory and a DMA engine. All system resources, including
peripherals, are accessible via memory-mapped I/O. If the
platform is designed to be a co-processor, it may optionally
incorporate a dedicated memory module (DRAM) and an
Ethernet card that is separate from the host platform. The
number of cores, their underlying instruction set (e.g., x86,
RISC), and the type of interconnect (e.g., ring, mesh) vary
across platforms.
Many-core platforms can be broadly classiﬁed by their
adherence to the symmetric or asymmetric multi-processor
speciﬁcations (SMP or AMP). SMP systems, such as In-
tel Xeon Phi [21], include hardware support that enables
all cores in the system to be controlled by a single operating
system. SMP systems achieve this by implementing features
like a centralized reset for cores, the ability to access the
entire system memory from any core, and cache-coherence.
Figure 2: Intel SCC architecture overview. The pro-
cessor has 24 tiles and each tile hosts two cores. Each tile
is connected to a mesh Network-on-Chip (NoC) via a net-
work interfaces and a router. Cores access shared memory
(DRAM) on the processor and peripherals, via the NoC.
Asymmetric multi-core processors, such as Intel SCC [22],
are in contrast designed to execute multiple operating sys-
tems on the same platform.
While most current many-core platforms do not support
virtualization extensions, there are also examples of SMP ar-
chitectures that provide such functionality partially. For ex-
ample, the Tilera TILE-GX [41] processors include memory
virtualization support but lack CPU virtualization. Since
future many-core platforms are likely to scale to hundreds
or even thousands of cores, it is desirable to keep the cores
as simple as possible. Solutions where operating systems
run directly on the hardware, without explicit virtualization
mechanisms also have performance beneﬁts [15, 36].
We, therefore, focus on the Intel SCC platform which is
an AMP architecture capable of running multiple operating
systems directly on its cores.
2.2
Intel SCC
Figure 2 shows an overview of the Intel SCC architecture.
Intel SCC is a co-processor that is connected to a host plat-
form. The processor has 48 cores arranged in 24 tiles. Each
tile contains two Pentium P54C cores enhanced with an L2
cache and special instructions to use the local on-tile mem-
ory. Cores can be dynamically turned on and oﬀ depending
on current computing load. Each tile also has some local
memory that is shared between the cores called the Message
Passing Buﬀer (MPB) and an on-tile network interface that
connects the tile to a router. Each core is capable of run-
ning a separate operating system independent of the other
cores. No two cores on the system share any cache and the
platform does not implement cache-coherence.
The routers at each tile are connected to a mesh network
which allows cores to access oﬀ-tile memory (DRAM) and
I/O peripherals. Routers at the corners of the mesh network
are connected to memory controllers and each memory con-
troller is connected to a DRAM memory element. Each core
uses the DRAM module attached to the memory controller
at the closest NoC corner.
All resources in the Intel SCC are memory-mapped and
can be conﬁgured through a set of Look-Up Tables (LUTs) in
the network interface of each tile. Each LUT entry translates
a range of physical addresses to system-wide addresses, as
shown in Figure 3. A system-wide address can point either
Core(s)Caches MemoryDMA Network interfaceTILE DRAMNetwork-on-Chip (NoC)PeripheralsHostTile1Tile2TilenNoCMANY-CORE PROCESSOR...INTEL SINGLE-CHIP CLOUD COMPUTER (SCC)TILE STRUCTURERRRRRRRRRRRRRRRRTo DRAMTo DRAMTo DRAMTo DRAMTo System Interface (I/O)PentiumcoreL2 cachePentiumcoreTile memory Network interfaceL2 cacheRouterMESH-BASED INTERCONNECTThe footprint of the hypervisor depends on its virtual-
ization technique and available hardware virtualization ex-
tensions. If virtualization and OS management are imple-
mented in software (e.g., para-virtualization in Xen or bi-
nary translation in VMware), the hypervisor footprint is
typically above 100K LOC [40]. Hardware virtualization ex-
tensions enable hypervisors with smaller footprints [38, 46].
Typical virtualization extensions include cores with an addi-
tional de-privileged execution mode and hardware-assisted
second-level address translation. For example, Intel x86 pro-
cessors provide a guest mode for execution of the OS and
Extended Page Tables (EPT) to translate second-level ad-
dresses in the memory management unit (MMU). On plat-
forms with hardware virtualization extensions, the OS runs
in the de-privileged mode and manages its memory without
hypervisor involvement. These mechanisms simplify hyper-
visor functionality and hence reduce its size.
Despite the small footprint (P1), traditional hypervisors
require frequent interaction with the hosted operating sys-
tems and do not provide reduced interaction (P2). For ex-
ample, since each core is shared between the OS and the hy-
pervisor, each external I/O interrupt causes a context switch
from the OS to the hypervisor.
Distributed hypervisor.
In a distributed design [10,
32], a separate hypervisor instance manages each core or a
subset of the available cores (Figure 4.b). Similar to tradi-
tional hypervisors, the footprint of a distributed hypervisor
depends on the available virtualization support in the pro-
cessor. Since cores are shared between the hypervisor and
the hosted operating systems, reduced hypervisor-OS inter-
action is not possible. Therefore, distributed hypervisors
can provide property P1 but not property P2.
Centralized hypervisor. A centralized hypervisor [15,
36, 39] runs on a dedicated core and manages operating sys-
tems that are executed on the other cores of the platform
(Figure 4.c). Current centralized hypervisors leverage vir-
tualization extensions. A centralized design can limit the
runtime interaction with the hosted operating systems for
improved security [39] and performance [15, 36]. For ex-
ample, to launch an OS, NoHype [39] starts a small setup
utility in the privileged execution mode on the target core.
The setup utility conﬁgures the memory mappings for the
hardware-assisted second-level address translation. Then,
the hypervisor starts the OS execution in the de-privileged
guest mode. A centralized hypervisor can provide both
properties P1 and P2. This approach can also improve per-
formance, as the hosted operating systems run directly on
the hardware without runtime virtualization overhead.
3.2 Goal: Logical Partitions with Simple Cores
Our analysis shows that only a centralized hypervisor de-
sign can provide small footprint (P1) and reduced interac-
tion (P2). Our goal in this paper is to realize logical parti-
tions with both security properties on many-core platforms.
The known techniques to implement small and disengaged