and the difference among time costs of our 3 steps is subtle.
Here, we amplify this difference by running our method on a
smartphone (Nexus 6) with a single core, and the resolution
of all the frames are kept on 1920*1080. The time costs are
shown in Table V.
TABLE V: Time cost of implementation on smartphone.
Number of frames
Face extraction
Timing veriﬁcation
Face veriﬁcation
Total
50
6.11
0.03
0.08
6.22 (s)
100
11.70
0.05
0.12
11.87 (s)
200
20.12
0.11
0.20
20.43 (s)
300
28.63
0.22
0.27
29.12 (s)
We discover that the most time-consuming step is the face
extraction, which depends on algorithms we choose and the
precision of face detection we want to achieve. The lower
precision, the lower resolution of the input frame is needed
and thus less time is needed. Particularly, if we shrink the
input frame to half its size, the time will be reduced to about
1 second to extract the faces on 50 frames. The other way
to reduce the time cost
is leveraging the back-end server
(the Cloud) in parallel, as we mentioned above. In practice,
we keep the camera continuously recording the user’s video
and, parallely, sent “.mp4” ﬁles with each containing 30
frames recorded in one second, to our server through a 4G
network (with about 12 Mbps of bandwidth in our experiment)
for every second. Transferring one that ﬁle will consume
1.1 MB bandwidth. Once receiving a video, the server will
perform our veriﬁcations on it and judge whether the user is
benign. If the result of any second is negative, we regard this
whole authentication session as a malicious attempt. Table VI
demonstrates the time cost of this process. Compared with
the implementation only on the smartphone, using cloud can
signiﬁcantly reduces the waiting and thus greatly improved
user experiences.
TABLE VI: Time cost of implementation using cloud.
Number of frames
Recording in Front
Verifying in Cloud
Time to Wait
50
1.67
2.22
0.55 (s)
100
3.34
3.62
0.28 (s)
200
6.67
7.21
0.54 (s)
300
10
10.82
0.82 (s)
The storage space we need is the same as the size of
captured videos, and the storage complexity is O(N M ). In
real tests, 8.3Mb memory space is enough to store a video
consisting 100 frames in JPG format.
VII. RELATED WORKS
Various liveness detection techniques have been proposed
in the past decades. In this section, we discuss differences
between our method and those most relevant previous studies.
Our method could be categorized as a texture extrac-
tion method, according to the classiﬁcation in Chakraborty’s
survey [5]. The traditional methods in this category mainly
123456Scenario0.960.970.980.991Accuracy00.51vibration051015percentage(%)00.51vibration0.50.60.70.80.9accuracyuse various descriptors to extract features of images and
pass features through a classiﬁer to obtain the ﬁnal result.
For instance, Arashloo et al. [2] used multi-scale dynamic
binarized statical features; Benlamoudi et al. [2] used active
shape models with steam and LBP; Wen et al. [25] analyzed
distortion using 4 different features, etc. These methods work
well under experimental conditions. But
in our adversary
model, the attacker can forge a perfect face that would defeat
their approaches. In contrast, our method checks the geometric
shape of the subject under authentication, and detect whether
there are abnormal delays between responses and challenges.
Even the adversary is technically capable of creating a perfect
forged response, the time required in doing so will fail them.
Besides, previous works may fail due to the sub-optimal
environmental conditions. However, our method is robust to
that, as demonstrated in the evaluation part.
Our method is also a challenge-response protocol. The
traditional protocols are based on human reactions. Comparing
to them, our responses can be generated at the speed of light.
Li et al. [17] proposed a new protocol that records the inertial
sensors’ data while the user is moving around the mobile
phone. If the data is consistent with the video captured by
the mobile phone, the user is judged as a legitimate one. This
method’s challenge is the movement of mobile phone which
is controlled by the user and measured by sensors. And the
response is user’s facial video which is also produced by the
user. This method’s security guarantee is based on the precise
estimation of head poses. But we argued that the accuracy
cannot be high enough in wild environment for two reasons:
ﬁrst, as mentioned by the authors, the estimation algorithm has
about 7 degrees deviation; second, hand trembling produces
side effect to the precision of the mobile sensors. In contrast,
our approach is more robust, because, ﬁrstly, the challenges
are fully out of attackers’ control, and, secondly, our security
guarantees are based on detecting the indelible delay, rather
than the accurate estimation of the unstable head position.
Besides above methods, there is a close work published
by Kim et al. [12] who found that the diffusion speed is
the distinguishing characteristic between real faces and fake
faces. The reason is that the real face has more stereo shape
which makes the reﬂection random and irregular. But this
passive method will not work, when the environmental light
is inefﬁcient. From the ﬁgures shown in their paper, we can
hardly distinguish the so-called binarized reﬂectance maps
of malicious responses from legitimate responses, and these
”vague” maps are fed to SVM for the ﬁnal decision. So we
argue that this approach cannot defeat such attackers who have
the ability to forge a perfect fake face. In contrast, our security
guarantee is not only based on the stereo shape, but also the
delay between responses and challenges. It’s a very high bar
for adversaries to forge a perfect response in such critical time.
Another method leveraging reﬂection is proposed by Rudd et
al. [20]. The authors added two different polarization devices
on the camera. And these devices impede the most of incoming
light except the light in the particular direction. Comparing to
this approach, our method does not require special devices and
is more practical to use.
Specially, our work has overlaps with Andrew Bud’s
patent [4] using also the light reﬂection to do the authentica-
tion. However, we focus on the security instead of functionality
as they have done. Without our timing veriﬁcation, as we
demonstrated, there is no security guarantee and it is weak
to defend against MFF attacks. And our work is independent
from theirs.
In general, compared with above relative works, Face
Flashing is an active and effective approach with strong
security guarantee on time.
VIII. DISCUSSION
Resilience to novel attacks. An attack proposed by Mahmood
et al. [21] demonstrated that an attacker could impersonate
the victim by placing a customized mask around his eyes.
Although such an attack can deceive the state-of-the-art face
recognition system, however, we believe it will be defeated
by our method, as paper masks around the eyes can be easily
detected by our neural network model in the veriﬁcation of
face (see Fig 8a and 8c).
Challenge colors. We used 8 different colors in our exper-
iments. Considering the length of our challenge sequence,
we believe these 8 colors are enough to provide a strong
security guarantee. Because our security guarantee is achieved
by detecting the delays. If the adversary falsely infers one
challenge, the delay will be detected and her attempt will
fail. Of course, we can easily increase the space of the
challenge sequences by using the striped pictures with a more
sophisticated algorithm.
Authentication time. Our method needs a few seconds to
gather enough responses for authentication. As we mentioned
in data collection’s part, 3 seconds is a reasonable default
setting. In this period, we can choose sufﬁcient responses
with high quality, and the user can complete the instructed
expression. Essentially, 1 second is enough for our method to
ﬁnish the work, but the user will be in a hurry.
Other applications of our techniques. One interesting appli-
cation of our method is to improve the accuracy of state-of-
the-art face recognition algorithms by distilling the personal
information contained in the geometric shape. We believe the
shape is unique. The combined method will have stronger
ability to prevent advanced future attacks.
IX. LIMITATIONS
The silicone mask may pass our system. But, this mask is
hard to be fabricated (3D printed) due to the reasons mentioned
in Section II-B. And our system has the potential to defeat it
completely, owing to our unique challenges: lights of different
wavelength (colors). According to previous studies [26], light
reﬂected from human skin has an “albedo curve”, the curve
depicting reﬂectance of different wavelengths. Therefore, the
reﬂections from different surfaces can be distinguished by
discernible albedo curves, which enables Face Flashing to
recognize attackers wearing such “soft” masks. However, this
technique is sophisticated and deserves another paper.
Even though we raise the bar of the attacks, we cannot
totally neutralize adversaries’ advantages coming from super
devices. They still have a chance to pass our system, if they
somehow use an ultrahigh-speed camera (FASTCAM SA1.1
14
with 675000fps), an ultrahigh-speed screen in the similar
level (says with 100000Hz), and the solution to reduce the
transmission and buffering delays. In this situation, adversaries
can instantly forge the response to every challenge with small
delays and subtle variance, so our protocol will fail. However,
this attack is expensive and sophisticated. On the other side,
we can mitigate this threat, to some extent, by ﬂashing more
ﬁnely striped challenges (or chessboard-like patterns), but, with
better screen and camera.
X. CONCLUSION
In this paper, we proposed a novel challenge-response
protocol, Face Flashing, to defeat the main threats against
face authentication system—the 2D dynamic attacks. We have
systematically analyzed our method and illustrated that our
method has strong security guarantees. We implemented a
prototype that does veriﬁcations both on time and the face.
We have demonstrated that our method has high accuracy in
various environments and is robust to vibration and illumina-
tion. Experimental results prove that our protocol is effective
and efﬁcient.
ACKNOWLEDGMENT
We thank our shepherd Muhammad Naveed for his patient
guidance on improving this paper, and anonymous reviewers
for their insightful comments. We also want to thank Tao Mo
and Shizhan Zhu for their supports on the face alignment
and tracking algorithms. This work was partially supported by
National Natural Science Foundation of China (NSFC) under
Grant No. 61572415, Hong Kong S.A.R. Research Grants
Council (RGC) Early Career Scheme/General Research Fund
No. 24207815 and 14217816.
REFERENCES
[1] W. Bao, H. Li, N. Li, and W. Jiang, “A liveness detection method
for face recognition based on optical ﬂow ﬁeld,” in Image Analysis
and Signal Processing, 2009. IASP 2009. International Conference on.
IEEE, 2009, pp. 233–236.
[2] A. Benlamoudi, D. Samai, A. Ouaﬁ, A. Taleb-Ahmed, S. E. Bekhouche,
and A. Hadid, “Face spooﬁng detection from single images using active
shape models with stasm and lbp,” in Proceeding of
the Troisime
CONFERENCE INTERNATIONALE SUR LA VISION ARTIFICIELLE
CVA 2015, 2015.
[3] D. H. Brainard and B. A. Wandell, “Analysis of the retinex theory of
color vision,” JOSA A, vol. 3, no. 10, pp. 1651–1661, 1986.
[4] A. Bud, “Online pseudonym veriﬁcation and identity validation,” U.S.
Patent 9 479 500B2, Oct 25, 2016.
[5] S. Chakraborty and D. Das, “An overview of face liveness detection,”
International Journal on Information Theory, vol. 3, no. 2, 2014.
[6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, and C.-J. Lin,
“LIBLINEAR: A library for large linear classiﬁcation,” Journal of
Machine Learning Research, vol. 9, pp. 1871–1874, 2008.
[7] W. Fernando, L. Udawatta, and P. Pathirana, “Identiﬁcation of moving
obstacles with pyramidal
lucas kanade optical ﬂow and k means
clustering,” in 2007 Third International Conference on Information and
Automation for Sustainability.
IEEE, 2007, pp. 111–117.
[8] R. D. Findling and R. Mayrhofer, “Towards face unlock: on the
difﬁculty of reliably detecting faces on mobile phones,” in Proceedings
of the 10th International Conference on Advances in Mobile Computing
& Multimedia. ACM, 2012, pp. 275–280.
[9] H.-K. Jee, S.-U. Jung, and J.-H. Yoo, “Liveness detection for embed-
ded face recognition system,” International Journal of Biological and
Medical Sciences, vol. 1, no. 4, pp. 235–238, 2006.
[10] Y. Jia, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
S. Guadarrama, and T. Darrell, “Caffe: Convolutional architecture for
fast feature embedding,” in Proceedings of the 22nd ACM international
conference on Multimedia. ACM, 2014, pp. 675–678.
[11] G. Kim, S. Eum, J. K. Suhr, D. I. Kim, K. R. Park, and J. Kim, “Face
liveness detection based on texture and frequency analyses,” in 2012
5th IAPR International Conference on Biometrics (ICB).
IEEE, 2012,
pp. 67–72.
[12] W. Kim, S. Suh, and J.-J. Han, “Face liveness detection from a
single image via diffusion speed model,” IEEE Transactions on Image
Processing, vol. 24, no. 8, pp. 2456–2465, 2015.
[13] K. Kollreider, H. Fronthaler, and J. Bigun, “Non-intrusive liveness
detection by face images,” Image and Vision Computing, vol. 27, no. 3,
pp. 233–244, 2009.
[14] K. Kollreider, H. Fronthaler, M. I. Faraj, and J. Bigun, “Real-time face
detection and motion analysis with application in liveness assessment,”
IEEE Transactions on Information Forensics and Security, vol. 2, no. 3,
pp. 548–558, 2007.
[15] A. Lagorio, M. Tistarelli, M. Cadoni, C. Fookes, and S. Sridharan,
“Liveness detection based on 3d face shape analysis,” in Biometrics
and Forensics (IWBF), 2013 International Workshop on.
IEEE, 2013,
pp. 1–4.
J. Li, Y. Wang, T. Tan, and A. K. Jain, “Live face detection based on
the analysis of fourier spectra,” in Defense and Security.
International
Society for Optics and Photonics, 2004, pp. 296–303.
[16]
[17] Y. Li, Y. Li, Q. Yan, H. Kong, and R. H. Deng, “Seeing your face
is not enough: An inertial sensor-based liveness detection for face
authentication,” in Proceedings of the 22nd ACM SIGSAC Conference
on Computer and Communications Security. ACM, 2015, pp. 1558–
1569.
J. M¨a¨att¨a, A. Hadid, and M. Pietikainen, “Face spooﬁng detection from
single images using micro-texture analysis,” in Biometrics (IJCB), 2011
international joint conference on.
IEEE, 2011, pp. 1–7.
[18]
[19] R. Raghavendra and C. Busch, “Robust 2d/3d face mask presentation
attack detection scheme by exploring multiple features and compari-
son score level fusion,” in Information Fusion (FUSION), 2014 17th
International Conference on.
IEEE, 2014, pp. 1–7.
[20] E. M. Rudd, M. Gunther, and T. E. Boult, “Paraph: presentation
attack rejection by analyzing polarization hypotheses,” in Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition
Workshops, 2016, pp. 103–110.
[21] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to
a crime: Real and stealthy attacks on state-of-the-art face recognition,”
in Proceedings of the 23rd ACM SIGSAC Conference on Computer and
Communications Security, 2016.
[22] R. S. Smith and T. Windeatt, “Facial expression detection using ﬁltered
local binary pattern features with ecoc classiﬁers and platt scaling.” in
WAPA, 2010, pp. 111–118.
[23] L. Sun, G. Pan, Z. Wu, and S. Lao, “Blinking-based live face detec-
tion using conditional random ﬁelds,” in International Conference on
Biometrics. Springer, 2007, pp. 252–260.
[24] T. Wang, J. Yang, Z. Lei, S. Liao, and S. Z. Li, “Face liveness detection
using 3d structure recovered from a single camera,” in Biometrics (ICB),
2013 International Conference on.
IEEE, 2013, pp. 1–6.
[25] D. Wen, H. Han, and A. K. Jain, “Face spoof detection with image
distortion analysis,” IEEE Transactions on Information Forensics and
Security, vol. 10, no. 4, pp. 746–761, 2015.
[26] T. Weyrich, W. Matusik, H. Pﬁster, J. Lee, A. Ngan, H. Wann, and
M. G. Jensen, “A measurement-based skin reﬂectance model for face,”
2005.
[27] Y. Xu, T. Price, J.-M. Frahm, and F. Monrose, “Virtual u: Defeating face
liveness detection by building virtual models from your public photos,”
in 25th USENIX Security Symposium (USENIX Security 16). Austin,
TX: USENIX Association, Aug. 2016, pp. 497–512.
[28] B. Yang, J. Yan, Z. Lei, and S. Z. Li, “Convolutional channel features,”
in Proceedings of the IEEE International Conference on Computer
Vision, 2015, pp. 82–90.
[29] S. Zhu, C. Li, C. C. Loy, and X. Tang, “Unconstrained face alignment
via cascaded compositional learning,” 2016.
15