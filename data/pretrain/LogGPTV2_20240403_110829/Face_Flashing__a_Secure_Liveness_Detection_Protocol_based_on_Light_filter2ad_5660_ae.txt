### Time Cost Analysis of the Three Steps

The time costs for our three steps are relatively similar. To highlight these differences, we ran our method on a smartphone (Nexus 6) with a single core, maintaining a resolution of 1920*1080 for all frames. The results are presented in Table V.

**Table V: Time Cost of Implementation on Smartphone**

| Number of Frames | Face Extraction (s) | Timing Verification (s) | Face Verification (s) | Total (s) |
|------------------|---------------------|-------------------------|-----------------------|-----------|
| 50               | 6.11                | 0.03                    | 0.08                  | 6.22      |
| 100              | 11.70               | 0.05                    | 0.12                  | 11.87     |
| 200              | 20.12               | 0.11                    | 0.20                  | 20.43     |
| 300              | 28.63               | 0.22                    | 0.27                  | 29.12     |

We observed that face extraction is the most time-consuming step, which is influenced by the chosen algorithms and the desired precision of face detection. Lower precision requires lower input frame resolution, thus reducing the time needed. For example, halving the input frame size reduces the face extraction time to approximately 1 second for 50 frames. Another method to reduce time cost is to leverage a back-end server (the Cloud) in parallel, as previously mentioned.

In practice, we keep the camera continuously recording the user's video and send ".mp4" files, each containing 30 frames recorded in one second, to our server via a 4G network (with about 12 Mbps bandwidth in our experiment). Each file transfer consumes approximately 1.1 MB of bandwidth. Upon receiving a video, the server performs the verifications and determines if the user is benign. If any second's result is negative, the entire authentication session is considered a malicious attempt. Table VI shows the time cost of this process. Using the cloud significantly reduces waiting time and improves user experience compared to implementation only on the smartphone.

**Table VI: Time Cost of Implementation Using Cloud**

| Number of Frames | Recording in Front (s) | Verifying in Cloud (s) | Time to Wait (s) |
|------------------|------------------------|------------------------|------------------|
| 50               | 1.67                   | 2.22                   | 0.55             |
| 100              | 3.34                   | 3.62                   | 0.28             |
| 200              | 6.67                   | 7.21                   | 0.54             |
| 300              | 10                     | 10.82                  | 0.82             |

The storage space required is equivalent to the size of the captured videos, with a storage complexity of O(N M). In real tests, 8.3 MB of memory space is sufficient to store a video consisting of 100 frames in JPG format.

### Related Works

Various liveness detection techniques have been proposed over the past decades. This section discusses the differences between our method and the most relevant previous studies.

Our method can be categorized as a texture extraction method, according to Chakraborty's survey [5]. Traditional methods in this category use various descriptors to extract image features and pass them through a classifier to obtain the final result. For example, Arashloo et al. [2] used multi-scale dynamic binarized statistical features, Benlamoudi et al. [2] used active shape models with steam and LBP, and Wen et al. [25] analyzed distortion using four different features. These methods work well under experimental conditions but may fail under our adversary model, where an attacker can forge a perfect face. In contrast, our method checks the geometric shape of the subject and detects abnormal delays between responses and challenges. Even if an adversary creates a perfect forged response, the time required will fail them.

Previous works may also fail due to sub-optimal environmental conditions. Our method, however, is robust to such conditions, as demonstrated in the evaluation part.

Our method is also a challenge-response protocol. Traditional protocols are based on human reactions, whereas our responses can be generated at the speed of light. Li et al. [17] proposed a new protocol that records inertial sensor data while the user moves around the mobile phone. If the data is consistent with the video, the user is judged as legitimate. However, this method's security guarantee is based on precise head pose estimation, which can be inaccurate in real-world conditions. In contrast, our approach is more robust because the challenges are fully out of the attacker's control, and our security guarantees are based on detecting indelible delays rather than unstable head positions.

Another close work by Kim et al. [12] found that diffusion speed distinguishes real faces from fake ones. However, this passive method fails when the environmental light is insufficient. Our method, on the other hand, uses both stereo shape and delay to ensure strong security.

Rudd et al. [20] added two polarization devices to the camera, impeding most incoming light except for light in a particular direction. Our method does not require special devices and is more practical.

Our work overlaps with Andrew Bud's patent [4], which also uses light reflection for authentication. However, we focus on security rather than functionality. Without our timing verification, there is no security guarantee and it is weak against MFF attacks.

In summary, compared to related works, Face Flashing is an active and effective approach with strong security guarantees.

### Discussion

#### Resilience to Novel Attacks

An attack by Mahmood et al. [21] demonstrated that an attacker could impersonate a victim by placing a customized mask around their eyes. Although such an attack can deceive state-of-the-art face recognition systems, our method can detect paper masks around the eyes using our neural network model (see Fig 8a and 8c).

#### Challenge Colors

We used eight different colors in our experiments. Given the length of our challenge sequence, these colors provide a strong security guarantee. Our security is achieved by detecting delays. If an adversary falsely infers one challenge, the delay will be detected, and their attempt will fail. We can increase the challenge sequence space by using striped pictures with a more sophisticated algorithm.

#### Authentication Time

Our method needs a few seconds to gather enough responses for authentication. A reasonable default setting is 3 seconds, during which we can choose high-quality responses, and the user can complete the instructed expression. Essentially, 1 second is enough, but the user may feel rushed.

#### Other Applications

One interesting application of our method is to improve the accuracy of state-of-the-art face recognition algorithms by distilling personal information from the geometric shape. We believe the shape is unique, and the combined method will have a stronger ability to prevent advanced future attacks.

### Limitations

A silicone mask may pass our system, but it is difficult to fabricate due to reasons mentioned in Section II-B. Our system has the potential to defeat such masks using unique challenges: lights of different wavelengths (colors). According to previous studies [26], light reflected from human skin has an "albedo curve," which allows us to distinguish reflections from different surfaces. However, this technique is sophisticated and deserves further research.

Even though we raise the bar for attacks, adversaries with super devices (e.g., ultrahigh-speed cameras and screens) and solutions to reduce transmission and buffering delays can still pass our system. This attack is expensive and sophisticated, but we can mitigate it by flashing more finely striped challenges or chessboard-like patterns.

### Conclusion

In this paper, we proposed a novel challenge-response protocol, Face Flashing, to defeat the main threats against face authentication systemsâ€”2D dynamic attacks. We systematically analyzed our method and demonstrated its strong security guarantees. We implemented a prototype that verifies both time and face. Experimental results show that our protocol is effective and efficient in various environments and is robust to vibration and illumination.

### Acknowledgment

We thank our shepherd Muhammad Naveed for his patient guidance on improving this paper and anonymous reviewers for their insightful comments. We also want to thank Tao Mo and Shizhan Zhu for their support on the face alignment and tracking algorithms. This work was partially supported by the National Natural Science Foundation of China (NSFC) under Grant No. 61572415, Hong Kong S.A.R. Research Grants Council (RGC) Early Career Scheme/General Research Fund No. 24207815 and 14217816.

### References

[References remain unchanged]