in an input image that have high adversarial saliency scores.
The adversarial saliency map is calculated from the Jacobian
(gradient) matrix ∇xg(x) of the DNN model g(x) at the current
input x. The (c, p)th component in Jacobian matrix ∇xg(x)
describes the derivative of output class c with respect
to
feature pixel p. The adversarial saliency score of each pixel
is calculated to reﬂect how this pixel will increase the output
score of the target class t versus changing the score of all
other possible output classes. The process is repeated until
classiﬁcation into the target class is achieved, or it reaches
the maximum number of perturbed pixels. Essentially, JSMA
optimizes Equation (2) by measuring perturbation ∆(x, x(cid:48))
through the L0-norm.
5) Carlini/Wagner Attacks (L2, L∞ and L0, Targeted)
Carlini and Wagner recently introduced three new gradient-
based attack algorithms that are more eﬀective than all
previously-known methods in terms of the adversarial success
rates achieved with minimal perturbation amounts [6]. There
are versions of their attacks for L2, L∞, and L0 norms.
The CW2 attack formalizes the task of generating adver-
sarial examples as an optimization problem with two terms as
usual: the prediction term and the distance term. However, it
makes the optimization problem easier to solve with several
techniques. The ﬁrst is using the logits-based objective func-
tion instead of the softmax-cross-entropy loss that is commonly
used in other optimization-based attacks. This makes it robust
against the defensive distillation method [34]. The second is
converting the target variable to the argtanh space to bypass
the box-constraint on the input, making it more ﬂexible in
taking advantage of modern optimization solvers, such as
Adam. It also uses a binary search algorithm to select a
suitable coeﬃcient that performs a good trade-oﬀ between the
prediction and the distance terms. These improvements enable
the CW2 attack to ﬁnd adversarial examples with smaller
perturbations than previous attacks.
Their CW∞ attack recognizes the fact that L∞ norm is hard
to optimize and only the maximum term is penalized. Thus,
it revises the objective into limiting perturbations to be less
than a threshold τ (initially 1, decreasing in each iteration).
The optimization reduces τ iteratively until no solution can
be found. Consequently,
the
perturbations smaller than the speciﬁed τ.
the resulting solution has all
The basic idea of the CW0 attack is to iteratively use CW2
to ﬁnd the least important features and freeze them (so value
will never be changed) until the L2 attack fails with too many
features being frozen. As a result, only those features with
signiﬁcant impact on the prediction are changed. This is the
opposite of JSMA, which iteratively selects the most important
features and performs large perturbations until it successfully
fools the target classiﬁer.
C. Defensive Techniques
Papernot et al. [33] provide a comprehensive summary of
work on defending against adversarial samples, grouping work
into two broad categories: adversarial training and gradient
masking, which we discuss further below. A third approach is
to modify feature sets, but it has not previously been applied to
DNN models. Wang et al. proposed a theory that unnecessary
features are the primary cause of a classiﬁer’s vulnerability to
adversarial examples [41]. Zhang et al. proposed an adversary-
aware feature selection model
that can improve classiﬁer
robustness against evasion attacks [43]. Our proposed feature
squeezing method is broadly part of this theme.
Adversarial Training. Adversarial training introduces dis-
covered adversarial examples and the corresponding ground
truth labels to the training dataset [10], [39]. Ideally,
the
model will learn how to restore the ground truth from the
adversarial perturbations and perform robustly on the future
adversarial examples. This technique, however, suﬀers from
the high cost to generate adversarial examples and (at least)
doubles the training cost of DNN models due to its iterative
re-training procedure. Its eﬀectiveness also depends on having
a technique for eﬃciently generating adversarial examples
similar to the one used by the adversary, which may not be
the case in practice. As pointed out by Papernot et al. [33],
it is essential to include adversarial examples produced by
all known attacks in adversarial training, since this defensive
training is non-adaptive. But, it is computationally expensive
to ﬁnd adversarial inputs by most known techniques, and there
is no way to be conﬁdent the adversary is limited to techniques
that are known to the trainer.
Gradient Masking. These defenses seek to reduce the sensi-
tivity of DNN models to small changes made to their sample
inputs, by forcing the model to produce near-zero gradients. Gu
et al. proposed adding a gradient penalty term in the objective
function, which is deﬁned as the summation of the layer-by-
layer Frobenius norm of the Jacobian matrix [12]. Although
the trained model behaves more robustly against adversaries,
the penalty signiﬁcantly reduces the capacity of the model
and sacriﬁces accuracy on many tasks [33]. Papernot et al.
introduced defensive distillation to harden DNN models [34].
A defensively distilled model is trained with the smoothed
labels generated by a normally-trained DNN model. Then,
to hide model’s gradient information from an adversary, the
distilled model replaces its last layer with a “harder” softmax
function after training. Experimental results found that larger
perturbations are required when using JSMA to evade dis-
tilled models. However, two subsequent studies showed that
defensive distillation failed to mitigate a variant of JSMA
with a division trick [5] and a black-box attack [31]. Papernot
et al. concluded that methods designed to conceal gradient
information are bound to have limited success because of the
transferability of adversarial examples [33].
D. Detecting Adversarial Examples
A few recent studies [25], [11], [9] have focused on
detecting adversarial examples. The strategies they explored
can be considered into three groups: sample statistics, training
a detector and prediction inconsistency.
Sample Statistics. Grosse et al. [11] propose a statistical test
method for detecting adversarial examples using maximum
mean discrepancy and energy distance as the statistical distance
measures. Their method requires a large set of adversarial ex-
amples and legitimate samples and is not capable of detecting
individual adversarial examples, making it less useful in prac-
tice. Feinman et al. propose detecting adversarial examples us-
4
ing kernel density estimation [9], which measures the distance
between an unknown input example and a group of legitimate
examples in a manifold space (represented as features in some
middle layers of a DNN). It is computationally expensive
and can only detect adversarial examples lying far from the
manifolds of the legitimate population. Using sample statistics
to diﬀerentiate between adversarial examples and legitimate
inputs seems unlikely to be eﬀective against broad classes
of attacks due to the intrinsically deceptive nature of such
examples. Experimental results from both Grosse et al. [11]
and Feinman et al. [9] have found that strategies relying on
sample statistics gave inferior detection performance compared
to other strategies.
Training a Detector. Similar to adversarial training, adversar-
ial examples can also be used to train a detector. Because of the
large number of adversarial examples needed, this method is
expensive and prone to overﬁtting employed adversarial tech-
niques. Metzen et al. proposed attaching a CNN-based detector
as a branch oﬀ a middle layer of the original DNN [25]. The
detector outputs two classes and uses adversarial examples (as
one class) plus legitimate examples (as the other class) for
training. The detector is trained while freezing the weights of
the original DNN, so does not sacriﬁce classiﬁcation accuracy
on the legitimate inputs. Grosse et al. demonstrate a similar
detection method (previously proposed by Nguyen et al. [29])
that adds a new “adversarial” class in the last layer of the DNN
model [11]. The revised model is trained with both legitimate
and adversarial inputs, reducing the accuracy on legitimate
inputs due to the change to the model architecture.
Prediction Inconsistency. The basic idea of prediction incon-
sistency is to measure the disagreement among several models
in predicting an unknown input example, since one adversarial
example may not fool every DNN model. Feinman et al.
borrowed an idea from dropout [15] and designed a detection
technique they called Bayesian neural network uncertainty [9].
In its original form, a dropout layer randomly drops some
weights (by temporarily setting to zero) in each training
iteration and uses all weights at
the testing phase, which
can be interpreted as training many diﬀerent sub-models and
averaging their predictions in testing. For detecting adversarial
examples, Feinman et al. propose using the “training” mode
of dropout layers to generate many predictions of each input.
They reported that the disagreement among the predictions
of sub-models is rare on legitimate inputs but common on
adversarial examples, thus can be employed for detection.
III. Feature Squeezing Methods
Although the notion of feature squeezing is quite general,
we focus on two simple types of squeezing: reducing the
color depth of images (Section III-A), and using smoothing
(both local and non-local) to reduce the variation among
pixels (Section III-B). Section IV looks at the impact of each
squeezing method on classiﬁer accuracy and robustness against
adversarial inputs. These results enable feature squeezing to be
used for detecting adversarial examples in Section V.
A. Color Depth
A neural network, as a diﬀerentiable model, assumes that
the input space is continuous. However, digital computers only
support discrete representations as approximations of contin-
uous natural data. A standard digital image is represented by
an array of pixels, each of which is usually represented as a
number that represents a speciﬁc color.
Common image representations use color bit depths that
lead to irrelevant features, so we hypothesize that reducing
bit depth can reduce adversarial opportunity without harming
classiﬁer accuracy. Two common representations, which we
focus on here because of their use in our test datasets, are 8-
bit grayscale and 24-bit color. A grayscale image provides 28 =
256 possible values for each pixel. An 8-bit value represents
the intensity of a pixel where 0 is black, 255 is white, and
intermediate numbers represent diﬀerent shades of gray. The 8-
bit scale can be extended to display color images with separate
red, green and blue color channels. This provides 24 bits for
each pixel, representing 224 ≈ 16 million diﬀerent colors.
1) Squeezing Color Bits
While people usually prefer larger bit depth as it makes the
displayed image closer to the natural image, large color depths
are often not necessary for interpreting images (for example,
people have no problem recognizing most black-and-white
images). We investigate the bit depth squeezing with three
popular datasets for image classiﬁcation: MNIST, CIFAR-10
and ImageNet.
Greyscale Images (MNIST). The MNIST dataset contains
70,000 images of hand-written digits (0 to 9). Of these, 60,000
images are used as training data and the remaining 10,000
images are used for testing. Each image is 28× 28 pixels, and
each pixel is encoded as 8-bit grayscale.
Figure 2 shows one example of class 0 in the MNIST
dataset
in the ﬁrst row, with the original 8-bit grayscale
images in the leftmost and the 1-bit monochrome images
rightmost. The rightmost
images, generated by applying a
binary ﬁlter with 0.5 as the cutoﬀ, appear nearly identical to
the original images on the far left. The processed images are
still recognizable to humans, even though the feature space is
only 1/128th the size of the original 8-bit grayscale space.
Figure 3 hints at why reducing color depth can mitigate
adversarial examples generated by multiple attack techniques.
The top row shows one original example of class 1 from the
MNIST test set and six diﬀerent adversarial examples. The
middle row shows those examples after reducing the bit depth
of each pixel into binary. To a human eye, the binary-ﬁltered
images look more like the correct class; in our experiments,
we ﬁnd this is true for DNN classiﬁers also (Table III in
Section IV).
Color Images (CIFAR-10 and ImageNet). We use two
datasets of color images in this paper: the CIFAR-10 dataset
with tiny images and the ImageNet dataset with high-resolution
photographs. The CIFAR-10 dataset contains 60,000 images,
each with 32 × 32 pixels encoded with 24-bit color and
belonging to 10 diﬀerent classes. The ImageNet dataset is pro-
vided by ImageNet Large Scale Visual Recognition Challenge
2012 for the classiﬁcation task, which contains 1.2 million
training images and the other 50,000 images for validation.
The photographs in the ImageNet dataset are in diﬀerent sizes
5
(a) CIFAR-10.
(b) ImageNet.
Fig. 4: Examples of adversarial attacks and feature squeezing methods extracted from the CIFAR-10 and ImageNet datasets. The
ﬁrst row presents the original image and its squeezed versions, while the other rows presents the adversarial variants.
and hand-labeled with 1,000 classes. However, they are pre-
processed to 224×224 pixels encoded with 24-bit True Color
for the target model MobileNet [17], [24] we use in this paper.
The middle row and the bottom row of Figure 2 show that
we can reduce the original 8-bit (per RGB channel) images to
fewer bits without signiﬁcantly decreasing the image recogniz-
ability to humans. It is diﬃcult to tell the diﬀerence between
the original images with 8-bit per channel color and images
using as few as 4 bits of color depth. Unlike what we observed
in the MNIST datase, however, bit depths lower than 4 do
introduce some human-observable loss. This is because we
lose much more information in the color image even though we
reduce to the same number of bits per channel. For example, if
we reduce the bits-per-channel from 8 bits to 1 bit, the resulting
grayscale space is 1/128 large as the original; the resulting
RGB space is only 2−(24−3) = 1/2, 097, 152 of the original
size. Nevertheless, in Section IV-B we ﬁnd that squeezing to
4 bits is strong enough to mitigate a lot of adversarial examples
while preserving the accuracy on legitimate examples.
2) Implementation
We implement the bit depth reduction operation in Python
with the NumPy library. The input and output are in the same
numerical scale [0, 1] so that we don’t need to change anything
of the target models. For reducing to i-bit depth (1 ≤ i ≤ 7),
we ﬁrst multiply the input value with 2i−1 (minus 1 due to the
zero value) then round to integers. Next we scale the integers
back to [0, 1], divided by 2i − 1. The information capacity
of the representation is reduced from 8-bit to i-bit with the
integer-rounding operation.
B. Spatial Smoothing
Spatial smoothing (also known as blur) is a group of
techniques widely used in image processing for reducing image
noise. Next, we describe the two types of spatial smoothing
methods we used: local smoothing and non-local smoothing.
1) Local Smoothing
Local smoothing methods make use of the nearby pixels
to smooth each pixel. By selecting diﬀerent mechanisms in
weighting the neighbouring pixels, a local smoothing method
can be designed as Gaussian smoothing, mean smoothing or
the median smoothing method [42] we use. As we report
in Section IV-C, median smoothing (also known as median
blur or median ﬁlter) is particularly eﬀective in mitigating
adversarial examples generated by L0 attacks.
The median ﬁlter runs a sliding window over each pixel of
the image, where the center pixel is replaced by the median
value of the neighboring pixels within the window. It does
not actually reduce the number of pixels in the image, but
spreads pixel values across nearby pixels. The median ﬁlter
is essentially squeezing features out of the sample by making
adjacent pixels more similar.
The size of the window is a conﬁgurable parameter, ranging
from 1 up to the image size. If it were set to the image
size, it would (modulo edge eﬀects) ﬂatten the entire image
to one color. A square shape window is often used in me-
dian ﬁltering, though there are other design choices. Several
padding methods can be employed for the pixels on the edge,
since there are no real pixels to ﬁll the window. We choose
6
reﬂect padding [36], in which we mirror the image along with
the edge for calculating the median value of a window when
necessary.
method implemented in OpenCV. It ﬁrst converts a color image
to the CIELAB colorspace, then separately denoises its L and
AB components, then converts back to the RGB space.
Median smoothing is particularly eﬀective at removing
sparsely-occurring black and white pixels in an image (de-
scriptively known as salt-and-pepper noise), whilst preserving
edges of objects well.
Figure 4a presents some examples from CIFAR-10 with
median smoothing of a 2 × 2 window in the third column.
It suggests why local smoothing can eﬀectively mitigate ad-
versarial examples generated by the Jacobian-based saliency
map approach (JSMA) [32] (Section II-B4). JSMA identiﬁes
the most
inﬂuential pixels and modiﬁes their values to a
maximum or minimum. The top left is a seed image of the
class airplane from the CIFAR-10 dataset. The third image in
the ﬁrst row displays the result of applying a 2×2 median ﬁlter
to that image. The last row shows the generated adversarial
example using the targeted JSMA attack in the leftmost, and