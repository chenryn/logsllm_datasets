e
f
n
i
f
o
n
o
i
t
i
a
v
e
d
n
a
l
l
A
Server infections
Client infections
Independent infections
0
10
1
10
2
10
3
10
Time Interval (seconds)
4
10
5
10
Figure 9:
servers
Infections burstiness for clients and
Another interesting aspect of the extracted infection time
series that we study is burstiness across diﬀerent time scales.
To quantify burstiness, we compute the Allan deviation [2]
of the infection time series at diﬀerent scales. The Allan
deviation is given by the following equation:
σ2
x(τ ) =
h(∆x)2i
1
2
The time series is discretized into time intervals of length τ
and each interval yields a sample xi of the number of infec-
tions that occurred within it. The equation measures the
diﬀerence between successive samples xi for diﬀerent inter-
val lengths τ .
In Figure 9, the bold line in the bottom shows the min-
imum possible deviation which occurs when all infections
have independent time arrivals. Intuitively, the Allan devi-
ation should diverge from this reference signiﬁcantly in time
scales where the signal exhibits high burstiness. Figure 9
shows that server infections in low time-scales are almost
independent, however, this changes if we look at time scales
above one hour. This non-burstiness of server infections in
low time scales suggests that measuring the infections over
38hourly intervals can provide a useful long-term average of the
expected infections. This observation can be used to build
a predictor of near-future infection incidents using simple
linear time series models that capture short-range depen-
dences, like ARIMA. On the other hand, client infections
are consistently more bursty and this is more evident for
time-scales above two minutes.
6. RELATED WORK
IDS Evaluation: The DARPA dataset [35] remains to-
day one of the best options, although it dates back to 1999
and has several well-known shortcomings [1, 3]. Another
well-known dataset, the DARPA Cyber Panel Correlation
Technology Validation [31] was created in 2002 but unfortu-
nately is not anymore available. These datasets were created
in testbeds under controlled experiments. A lot of research
has focused on generating synthetic traﬃc. More related to
our work, MACE [48] is an environment for generating ma-
licious packets for evaluating IDSs in testbeds. In addition
to testbed experiments, in this work we stress the need to
use and label and use traces collected in the wild.
Intrusion Measurements: Large traces of intrusion
data, like IDS alerts and ﬁrewall logs collected by DShield [17],
have been analyzed in previous studies. In particular, Yeg-
neswaran et al. [52] made a number of observations regarding
the distribution, types, and prevalence of intrusions. In ad-
dition, they projected the global volume of intrusions and
estimated the potential of collaboration in intrusion detec-
tion. Kati et al. [34] analyzed a large trace of IDS alerts,
reported characteristics of correlated attacks, and investi-
gated how to eﬀectively collaborate in intrusion detection.
In this work we provide a number of further insights about
intrusions focusing speciﬁcally on infections, which have not
been studied as a separate class in the past.
Besides, a number of previous studies have focused on
IDS alert correlation and aggregation. These studies evalu-
ate proposed solutions on a small number of testbed-based
benchmarks, like the DARPA dataset, and are tailored for
general-purpose alert analysis rather than for extrusion de-
tection. In our work, we highlight the need for using and
analyzing measurements from real networks in addition to
testbed-based evaluation methods.
In particular, related
work on alert correlation and aggregation can be classiﬁed
in three categories.
Statistical/temporal Alert Correlation: A group of
studies explores statistical [41, 36] or temporal [42] alert cor-
relations to identify causality relationships between alerts.
Statistical correlation methods estimate the association be-
tween diﬀerent alerts by measuring the co-occurrence and
frequency of alert pairs within a speciﬁc time frame. Qin [41]
introduced a Bayesian network to model the causality rela-
tionship between alert pairs, while Ren et al. [43] proposed
an online system to construct attack scenarios based on
historic alert information. Temporal-based correlation ap-
proaches perform time series analysis on the alert stream
to compute the dependence between diﬀerent alerts. Qun
and Lee [42] generate time series variables on the number of
recorded alerts per time unit and use the Granger causality
test to identify causal relationships. We also use a statisti-
cal alert correlation test in our heuristic and show how alert
correlation can be useful speciﬁcally for extrusion detection.
Scenario- and Rule-based Alert Correlation: On
the other hand, a number of studies hardcode details about
attack steps into full scenarios [38, 12] or into rules [39,
6, 5], which are used to identify, summarize, and annotate
alert groups. Scenario-based correlation approaches try to
identify causal relationships between alerts in order to re-
construct high-level multi-stage events. Most approaches
rely on attack scenarios speciﬁed by human analysts using
an attack language [7, 13]. Deriving attack scenarios for
all observed attacks requires signiﬁcant technical expertise,
prior knowledge, and time. In order to automate the sce-
nario derivation process, machine learning techniques have
been used [8]. Rule-based approaches are based on the ob-
servation that attacks can be usually subdivided into stages.
These methods attempt to match speciﬁc alerts to the pre-
requisites and consequences of an active attack stage. The
idea is to correlate alerts if the precondition of the current
alert stream is satisﬁed by the postcondition of alerts that
were analyzed earlier in time. A main limitation of scenario-
and rule-based approaches is that the detection eﬀectiveness
is limited to attacks known a priori to the analyst or learned
during the training phase. Therefore, they are not useful
against novel attacks that have not been encountered in the
past.
Alert Aggregation: The goal of alert aggregation is to
group alerts into meaningful groups based on similarity cri-
teria, making them manageable for a human analyst and
amenable for subsequent statistical analysis. Each new alert
is compared against all active alert sequences and is asso-
ciated with the most relevant one. The set of attributes
used to compute the corresponding similarity measure are
diverse spanning from inherent alert features such as IP ad-
dresses, port numbers, and alert signatures, to topological
and contextual information about the monitored network,
such as the role and operation of nodes that triggered the
alert or the deployed operating system and services. The
work by Valdes et al. [51] represents each alert as a vector
of attributes and groups alerts based on the weighted sum
of the similarity of diﬀerent attributes. The weight of an
attribute is heuristically computed. Dain et al. [10, 9] pro-
pose a system that associates incoming alerts to groups in an
online fashion. Julisch [32] proposes a clustering technique
that aims at grouping alerts sharing the same root-causes,
based on attribute-oriented induction. To address the prob-
lem that prior knowledge regarding how the alerts should be
aggregated might not be available, machine learning tech-
niques have been used. Zhu et al. [53] propose a supervised
learning approach based on neural networks. Compared to
these studies, we also use a simple form of alert aggregation
in our heuristic, which we call alert bundling, that groups
spurts of almost identical alerts for further statistical anal-
ysis rather than potentially diverse alerts based on complex
similarity functions.
7. DISCUSSION
False Negatives: We opt to design our heuristic to pro-
duce a small number of false positives. This is one of our
main goals as the excessive amount of false positives is an
important limiting factor for IDSs. This means that in the
trade-oﬀ between false positives and negatives we prefer to
incur more false negatives in order to reduce the amount
of false positives. Quantifying the false negative rate in a
production environment is not possible. However, to assess
false-negative rates one can use synthetic or testbed-based
evaluation traces, as discussed in Section 6, where security
39incidents are known and controlled. Our work is comple-
mentary to such approaches and establishes techniques to
ﬁnd and validate security incidents in traces from produc-
tion environments.
Academic Infrastructure: Our characterization results
in Section 5 are based on data from an academic infrastruc-
ture and should only be carefully generalized, when possible,
to other types of networks. For example, we expect that sim-
ilar qualitative ﬁndings about the impact of infections and
the presence of heavy hitters hold in networks of diﬀerent
type. In contrast, we expect that the volume of infections
will be lower in more tightly managed environments.
8. CONCLUSIONS
In this paper, we present a novel approach to identify
active infections in a large population of hosts, using IDS
logs. We tailor our heuristic based on the observation that
alerts with high mutual information are very likely to be
correlated. Correlated alerts of speciﬁc types reveal that
an actual infection has occurred. By applying this heuristic
to a large dataset of collected alerts, we ﬁnd infections for
a population of more than 91 thousand unique hosts. We
perform an extensive validation study in order to assess the
eﬀectiveness of our method, and show that it manages to
reduce the false-positive rate of the raw IDS alerts to only
15%. Our characterization suggests that infections exhibit
high spatial correlations, and that the existing infections
open a wide attack vector for inbound attacks. Moreover,
we investigate attack heavy hitters and show that client in-
fections are signiﬁcantly more bursty compared to server
infections. We believe that our results are useful in several
diverse ﬁelds, such as evaluating network defenses, extrusion
detection, IDS false positive reduction, and network foren-
sics.
9. ACKNOWLEDGEMENTS
The authors wish to thank Prof. Bernhard Plattner and
Dr. Vincent Lenders for their invaluable help and fruitful
discussions. Furthermore, we would like to thank Matthias
Eggli for helping with the validation of the infection inci-
dents. We would also like to thank Stephan Sheridan and
Christian Hallqvist at ETH for their help in the collection
and archiving of the data used in this paper. This work was
supported by the Armasuisse Secmetrics [2-7841-09] project.
10. REFERENCES
[1] Testing intrusion detection systems: a critique of the
1998 and 1999 darpa intrusion detection system
evaluations as performed by lincoln laboratory. ACM
Trans. Inf. Syst. Secur., 3:262–294, November 2000.
[2] D. W. Allan. Time and frequency (time domain)
characterization, estimation and prediction of
precision clocks and oscillators. IEEE Trans. UFFC,
34, November 1987.
[3] Carson Brown, Alex Cowperthwaite, Abdulrahman
[4] David Brumley, Cody Hartwig, Zhenkai Liang, James
Newsome, Dawn Song, and Heng Yin. Automatically
identifying trigger-based behavior in malware, 2008.
[5] Steven Cheung, Ulf Lindqvist, and Martin W. Fong.
Modeling multistep cyber attacks for scenario
recognition, 2003.
[6] Fr´ed´eric Cuppens and Alexandre Mi`ege. Alert
correlation in a cooperative intrusion detection
framework. In Proceedings of the 2002 IEEE
Symposium on Security and Privacy, pages 202–,
Washington, DC, USA, 2002. IEEE Computer Society.
[7] Fr´ed´eric Cuppens and Rodolphe Ortalo. Lambda: A
language to model a database for detection of attacks.
In Proceedings of the Third International Workshop on
Recent Advances in Intrusion Detection, RAID ’00,
pages 197–216, London, UK, 2000. Springer-Verlag.
[8] D. Curry and H. Debar. Intrusion detection message
exchange format: Extensible markup language
document type deﬁnition, 2003.
[9] Oliver Dain and Robert K. Cunningham. Fusing a
heterogeneous alert stream into scenarios. In In
Proceedings of the 2001 ACM workshop on Data
Mining for Security Applications, pages 1–13, 2001.
[10] Oliver M. Dain and Robert K. Cunningham. Building
scenarios from a heterogeneous alert stream, 2002.
[11] Neil Daswani, The Google Click Quality, Security
Teams, and Google Inc. The anatomy of clickbot.a. In
In USENIX Hotbots’07, 2007.
[12] Herv´e Debar and Andreas Wespi. Aggregation and
correlation of intrusion-detection alerts. In Proceedings
of the 4th International Symposium on Recent
Advances in Intrusion Detection, RAID ’00, pages
85–103, London, UK, 2001. Springer-Verlag.
[13] Steven Eckmann, Giovanni Vigna, and Richard A.
Kemmerer. Statl: An attack language for state-based
intrusion detection, 2002.
[14] Advanced automated threat analysis system.
www.threatexpert.com.
[15] Anonymous postmasters early warning system.
www.apews.org.
[16] Common Vulnerabilities and Exposures dictionary of
known information security vulnerabilities.
cve.mitre.org.
[17] Cooperative Network Security Community - Internet
Security. www.dshield.org.
[18] Damballa - Botnet and Advanced Malware Detection
and Protection. www.damballa.com.
[19] Emerging Threats web page.
http://www.emergingthreats.net.
[20] Network Security Archive.
http://www.networksecurityarchive.org.
[21] Packet Storm Full Disclosure Information Security.
packetstormsecurity.org.
[22] Projecthoneypot web page.
www.projecthoneypot.org.
Hijazi, and Anil Somayaji. Analysis of the 1999
darpa/lincoln laboratory ids evaluation data with
netadhict. In Proceedings of the Second IEEE
international conference on Computational intelligence
for security and defense applications, CISDA’09, pages
67–73, Piscataway, NJ, USA, 2009. IEEE Press.
[23] Shadowserver Foundation web page.
www.shadowserver.org.
[24] Symantec SecurityFocus technical community.
www.securityfocus.com.
[25] The Nessus vulnerability scanner.
www.tenable.com/products/nessus.
40[26] The Open Vulnerability Assessment System.
www.openvas.org.
[27] The Spamhaus Project. www.spamhaus.org.
[28] The Urlblacklist web page. www.urlblacklist.org.
[29] TrustedSource Internet Reputation System.
www.trustedsource.org.
[30] Loic Etienne and Jean-Yves Le Boudec. Malicious
traﬃc detection in local networks with snort.
Technical report, EPFL, 2009.
[31] Joshua Haines, Dorene Kewley Ryder, Laura Tinnel,
and Stephen Taylor. Validation of sensor alert
correlators. IEEE Security and Privacy, 1:46–56,
January 2003.
[32] Klaus Julisch. Clustering intrusion detection alarms to
support root cause analysis. ACM Transactions on
Information and System Security, 6:443–471, 2003.
[33] Klaus Julisch and Marc Dacier. Mining intrusion
detection alarms for actionable knowledge. In KDD
’02: Proceedings of the eighth ACM SIGKDD
international conference on Knowledge discovery and
data mining, pages 366–375, New York, NY, USA,
2002. ACM.
[34] Sachin Katti, Balachander Krishnamurthy, and Dina
Katabi. Collaborating against common enemies. In
Proceedings of the 5th ACM SIGCOMM conference on
Internet Measurement, IMC ’05, pages 34–34,
Berkeley, CA, USA, 2005. USENIX Association.
[35] Richard Lippmann, Joshua W. Haines, David J. Fried,
Jonathan Korba, and Kumar Das. The 1999 darpa
oﬀ-line intrusion detection evaluation. Computer
Networks, 34:579–595, October 2000.
[36] Federico Maggi and Stefano Zanero. On the use of
diﬀerent statistical tests for alert correlation: short
paper. In Proceedings of the 10th international
conference on Recent advances in intrusion detection,