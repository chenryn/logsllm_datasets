levels. Similar to previous work [49, 52], we measure the overfitting
level of a target model by calculating the difference between its
training accuracy and testing accuracy. In Figure 5, we see that the
overfitting level is highly correlated with the attack performance:
if a model is more overfitted, it is more vulnerable to membership
inference attacks. For instance, in Figure 5a, on CIFAR100, the
contrastive model (upper right orange cross) has an overfitting
level of 0.249, and the NN-based attack accuracy is 0.625, while the
supervised model (upper right blue dot) has a larger overfitting level
(0.678) and higher attack accuracy (0.931). Another observation is
that compared to the supervised models, the overfitting levels of
the contrastive models reside in a smaller range.
NN-based method as well as some of the metric-based ones
(metric-ent and metric-ment) require the target model to provide
posteriors to launch the attacks. We further investigate whether
the number of posteriors provided by the target model can influ-
ence the attack performance. Concretely, we vary the number of
posteriors from 2 to 100 on CIFAR100 for both supervised and con-
trastive models. Figure 6 shows that the number of posteriors does
not have a strong influence on the attack performance. We further
measure the influence of the number of epochs used for training
each contrastive model‚Äôs classification layer on the attack perfor-
mance. Figure 7 shows that the attack accuracy is rather stable (the
performance of metric-based attacks are summarized in Figure 17 in
Appendix). These results show that contrastive models consistently
reduce the membership threat.
In conclusion, contrastive models are less vulnerable to member-
ship inference attacks compared to supervised models. The reason
is that contrastive models are less overfitted to their training sam-
ples than supervised models due to the design of the contrastive
learning paradigm.
4 ATTRIBUTE INFERENCE ATTACK
In this section, we take a different angle to measure the privacy
risks of contrastive learning using attribute inference attack [36, 56].
Similar to membership inference attacks, we use existing attribute
inference attacks [36, 56] to measure the contrastive model‚Äôs privacy
risks instead of inventing new methods.
4.1 Attack Definition and Threat Model
In attribute inference, the adversary‚Äôs goal is to infer a specific
sensitive attribute of a data sample from its representation gen-
erated by a target model. This sensitive attribute is not related to
the target ML model‚Äôs original classification task. For instance, a
target model is designed to classify an individual‚Äôs age from their
social network posts, while attribute inference aims to infer their
educational background.
Figure 4: Randomly selected images from STL10 and their
augmented views used during the process of contrastive
learning. The first and fourth columns show the original im-
ages (bounded by orange boxes), and the rest columns show
their augmented views.
ResNet-50 (Figure 16) in Appendix. In Figure 2, we see that all the
supervised models have higher attack accuracy than the contrastive
models. E.g., when the supervised model is MobileNetV2 trained
on CIFAR100, the accuracy of NN-based attack is 0.931, while the
accuracy for the corresponding contrastive model is only 0.625.
We observe that NN-based, metric-conf, and metric-ment attacks
achieve the best performance in all cases. The reason metric-conf
and metric-ment achieving better performance than metric-corr and
metric-ent is that metric-conf and metric-ment consider both pre-
diction correctness and confidence while metric-corr (metric-ent)
only considers prediction correctness (confidence). Interestingly, for
supervised models, metric-corr and metric-ent perform similarly,
while for contrastive models, metric-ent is worse than metric-corr.
This reason is that the posteriors generated by contrastive models
are more smooth compared to supervised model, which makes it
harder to distinguish members and non-members through the pos-
terior entropy. Label-only attacks perform worse than NN-based
attacks. This is expected since the adversary has less information
in these cases. Note that label-only attacks do not perform well on
binary classifiers, we will investigate the reason in the future.2
To further investigate why contrastive models are less vulnerable
to membership inference, we analyze the loss distribution between
members and non-members in both supervised models and con-
trastive models. Due to space limitations, we only show the results
of ResNet-18 trained on the CIFAR10 dataset in Figure 3. A clear
trend is that compared to the contrastive model, the supervised
model has a larger divergence between the classification loss (cross-
entropy) for members and non-members. Recall that contrastive
learning uses two augmented views of each sample in each epoch
to train its base encoder and the original sample to train its clas-
sification layer. This indicates that each sample is generalized to
multiple views during the contrastive model training process. In
this way, the contrastive model reduces its memorization of the
original sample itself.
Interestingly, Song et al. [58] observe that defense mechanisms
for mitigating adversarial examples [4, 6, 44, 59] increase the mem-
bership inference performance. This means such defense and con-
trastive learning have different effects on membership privacy. On
2Choquette-Choo et al. [10] also only perform label-only membership inference attacks
against datasets with more than two classes.
Session 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea850(a) MobileNetV2
(b) ResNet-18
(c) ResNet-50
Figure 5: The performance of membership inference attacks against both supervised models and contrastive models with Mo-
bileNetV2, ResNet-18, and ResNet-50 on 5 different datasets under different overfitting levels. The x-axis represents different
overfitting levels. The y-axis represents membership inference attacks‚Äô accuracy.
(a) NN-based
(b) Metric-ent
(c) Metric-ment
Figure 6: The performance of NN-based, metric-ent, and metric-ment attacks against both supervised models and contrastive
models with MobileNetV2, ResNet-18, and ResNet-50 on CIFAR100 under different numbers of posteriors given by the target
models. (S) and (C) denotes the supervised and contrastive models, respectively. The x-axis represents different numbers of
posteriors. The y-axis represents membership inference attacks‚Äô accuracy. Note that we do not report the performance of
metric-corr, metric-conf, and label-only attacks since the number of posteriors does not affect their performance.
characteristics of data samples. Such representation capability, in
some cases, can be exploited by an adversary to infer data samples‚Äô
sensitive attributes.
Once a contrastive model is trained, it can generate a represen-
tation for each sample with its base encoder ùëì . For a supervised
model, we consider the whole model without the classification layer
as its base encoder to generate a representation for each sample.
Note that the base encoder of contrastive model and supervised
model has the same architecture.
For attribute inference, given a data sample‚Äôs representation from
a target model, denoted by ‚Ñé, to conduct the attribute inference
attack, the adversary trains an attack model AAttInf : ‚Ñé ‚Ü¶‚Üí ùë†, where
ùë† represents the sensitive attribute.
We follow the same threat model as previous work [36, 56]: the
adversary only has access to the target sample‚Äôs embedding (repre-
sentation), but not the target sample itself. The adversary is also
assumed to have a set of samples‚Äô embeddings and their sensitive
attributes; this dataset is termed as an auxiliary dataset Daux. As
shown by previous work, attribute inference can be applied in both
federated learning [36] and model partitioning [56] settings.
(a) NN-based
(b) Label-only
Figure 7: The performance of NN-based and label-only mem-
bership inference attacks against contrastive models with
ResNet-50 on 8 different datasets under different numbers
of epochs for classification layer training. The x-axis rep-
resents different numbers of epochs. The y-axis represents
membership inference attacks‚Äô accuracy. Each line corre-
sponds to a specific dataset.
Attribute inference attacks have been successfully performed on
supervised models [36, 56]. The reason behind this is the intrinsic
overlearning property of ML models. Overlearning means that an
ML model trained for a certain task may also learn to represent other
0.00.20.40.6OverÔ¨ÅttingLevel0.50.60.70.80.9AccuracySupervisedModelContrastiveModel0.00.20.40.6OverÔ¨ÅttingLevel0.50.60.70.80.9Accuracy0.00.20.40.6OverÔ¨ÅttingLevel0.50.60.70.80.9Accuracy25102050100#.Posteriors0.40.60.81.0AccuracyMobileNetV2(S)ResNet-18(S)ResNet-50(S)MobileNetV2(C)ResNet-18(C)ResNet-50(C)25102050100#.Posteriors0.40.60.81.0Accuracy25102050100#.Posteriors0.40.60.81.0Accuracy255075TrainingEpochs0.60.81.0AccuracyCIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places20255075TrainingEpochs0.60.81.0AccuracySession 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea851(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 8: The performance of attribute inference attacks against both supervised models and contrastive models with Mo-
bileNetV2, ResNet-18, and ResNet-50 on 4 different datasets. The x-axis represents different models. The y-axis represents
attribute inference attacks‚Äô accuracy.
4.2 Methodology
We generalize the methodology of attribute inference attacks against
supervised models [36, 56] to contrastive models. The attack pro-
cess can be partitioned into two stages, i.e., attack model training
and attribute inference.
Attack Model Training. For each (‚Ñé, ùë†) ‚àà Daux, the adversary
takes the representation ‚Ñé as the input and the corresponding sen-
sitive attribute ùë† as the label to train the attack model.
Attribute Inference. To determine the sensitive attribute of a
data sample‚Äôs representation ‚Ñé, the adversary queries the attack
model AAttInf with ‚Ñé and obtains the result.
4.3 Experimental Setting
Datasets. We utilize UTKFace, Places100, Places50, and Places20
to evaluate attribute inference attacks as they contain extra at-
tributes that can be considered as sensitive attributes for our exper-
iments (see Section 3.3). In UTKFace, the target model‚Äôs classifica-
tion task is gender classification, and the sensitive attribute is race
(Black, White, Asian, Indian, and Other). In Places100, Places50, and
Places20, the target classification task is whether the scene is in-
door or outdoor, and the sensitive attribmaxcute is scene categories.
Similar to Song and Shmatikov [56], we take Dtrain
target to generate
the auxiliary dataset and train the attack model, and take Dtest
target
to test the attack performance.
Metric. We adopt accuracy as the metric to evaluate attribute
inference attacks following previous work [36, 56].
Models. All the target models‚Äô architectures are the same as those
for membership inference attacks. For the attack model, we leverage
a 3-layer MLP with the number of neurons in the hidden layer set
to 128. We use cross-entropy as the loss function and SGD as the
optimizer with a learning rate of 0.01. The attack model is trained
for 100 epochs. The dimension of each sample‚Äôs representation
from the base encoder, i.e., the attack model‚Äôs input, is 1,280 for
MobileNetV2, 512 for ResNet-18, and 2,048 for ResNet-50.
4.4 Results
The performance of attribute inference attacks is depicted in Fig-
ure 8. First, we observe that, in general, attribute inference achieves
effective performance except for the supervised model trained on
UTKFace dataset (close to the prior sensitive attribute distribu-
tion in the attack training dataset as shown in Table 1). Second,
Table 1: The baseline accuracy (random guessing based on
majority class labels) of attribute inference attack on differ-
ent datasets.
Dataset
UTKFace
Places100
Places50
Places20
#. Class Baseline Accuracy
5
100
50
20
0.421
0.012
0.023
0.053
compared to the supervised models, the contrastive models are
more vulnerable to attribute inference attacks. For instance, on
the UTKFace dataset with ResNet-18, we can achieve an attack
accuracy of 0.701 on the contrastive model while only 0.422 on the
supervised model. To better understand this, we extract samples‚Äô
representations (512-dimension) from ResNet-18 on UTKFace for
both the supervised model and the contrastive model and project
them into a 2-dimension space using t-Distributed Neighbor Em-
bedding (t-SNE) [62]: Figure 9a shows the results for the supervised
model on the original classification task, i.e., gender classification;
Figure 9b shows the results for the supervised model on attribute
inference, i.e., race. We see that in Figure 9a, male samples (blue)
and female samples (orange) reside in completely different regions,
which can be separated perfectly (the gender classification accuracy
is 0.875 in Figure 1). However, for the sensitive attribute (Figure 9b),
samples of different classes are clustered tightly, which increases
the difficulty for attribute inference. Figure 9c and Figure 9d show
the corresponding results for the contrastive model. We observe
that different samples‚Äô representations on the contrastive model
are less separable with respect to the original classification task
compared to the supervised model (see Figure 9c and Figure 9a),
but we can still successfully separate most of them correctly (the
gender classification accuracy is 0.858 in Figure 1) since most of the
male samples (blue) lie in the upper area while the female samples
(orange) are in the lower area. On the other hand, for the sensitive
attribute, compared to the supervised model (Figure 9b), represen-
tations generated by the contrastive model (Figure 9d) are more
distinguishable. Our finding reveals that the representations gener-
ated by the contrastive model are more informative, which can be
exploited not only for the original classification tasks but also for
attribute inference.
SupervisedContrastive0.00.20.40.6AccuracyMobileNetV2ResNet-18ResNet-50SupervisedContrastive0.000.050.100.15AccuracySupervisedContrastive0.00.10.20.3AccuracySupervisedContrastive0.00.20.4AccuracySession 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea852(a) Original Task (S)
(b) Sensitive Attribute (S)
(c) Original Task (C)
(d) Sensitive (C)
Figure 9: The representations for 200 randomly selected samples generated by both the supervised model and the contrastive
model with ResNet-18 on UTKFace projected into a 2-dimension space using t-SNE. (S) and (C) denotes the supervised and
contrastive models, respectively. Each point represents a sample.
(a) UTKFace
(b) Places100
(c) Places50
(d) Places20
Figure 10: The performance of attribute inference attacks against contrastive models on 4 different datasets under different
percentages of the attack training dataset. The x-axis represents different percentages of the attack training dataset. The y-axis
represents attribute inference attacks‚Äô accuracy.
(a) UTKFace
(b) Places100
(c) Places50