just wanted to work in this environment I’d already set up.”
Unfortunately, in most cases, switching contexts can be
diﬃcult because REs have to manually transfer information
back and forth between static and dynamic tools (e.g., instruc-
tions or memory states) (N=14). To overcome this challenge,
some REs opened both tools side-by-side to make compar-
isons easier (N=4). For example, P08V opened a debugger
in a window next to a disassembler and proceeded to step
through the main function in the debugger while following
along in the assembly code. As he walked through the pro-
gram, he regularly switched between the two. For example,
he would scan the possible control-ﬂow paths in the disas-
sembler to decide which branch to force execution down and
the necessary conditions would be set through the debugger.
Whenever he came across a speciﬁc question that could not
be answered just by scanning, he would switch to the debug-
ger. Because he stepped through the program as he scanned,
he could quickly list register values and relevant memory
addresses to get concrete variable values.
Experience and strategy guide where to look in the ﬁrst
two phases (RQ1). Initially, REs have to make decisions
about which metadata to look at, e.g., all strings and APIs or
speciﬁc subsets, (N=4) and what inputs to provide to exercise
basic behaviors (N=2). Once they run their overview analy-
ses, they must determine which outputs (strings, APIs, or UI
elements) are relevant to their investigation (N=16) and in
what order to process them (N=11). Reverse engineers ﬁrst
rely on prior experience to guide their actions (N=14). P04V
explained that when he looks for iPhone app vulnerabilities,
he has “a prioritized list of areas [APIs] I look at...it’s not a
huge list of things that can go horribly wrong from a secu-
rity standpoint when you make an iPhone app...So, I just go
through my list of APIs and make sure they’re using them
properly.” If REs are unable to relate their current context
to prior experience, then they fall back on basic strategies
(N=16) such as looking at the largest functions ﬁrst. P03V
said, “If I have no clue what to start looking at...I literally
USENIX Association
29th USENIX Security Symposium    1885
had many situations where I think this looks like an inﬁnite
loop, but it can’t be. It’s because Hex-Rays is buggy. Basically,
in programming, no one does anything all that odd.”
Preferred tools presented output in relation to the code
(RQ2). In almost all cases, the tools REs choose to use pro-
vide a simple method to connect results back to speciﬁc lines
of code (N=16). They choose to list strings and API calls
in a disassembler (N=15), such as IDA, which shows refer-
ences in the code with a few clicks, as opposed to using the
command-line strings command (N=0). Similarly, those par-
ticipants who discussed using advanced automated analyses,
i.e., fuzzing (N=1) and symbolic execution (N=1), reported
using them through disassembler plugins which overlaid anal-
ysis results on the code (e.g., code coverage highlighting for
fuzzing). P03V used Z3 for symbolic execution independently
of the code, supplying it with a list of possible states and
manually interpreting its output with respect to the program.
However, she explained this decision was made because she
did not know a tool that presented results in the context of the
code that could be used with the binary she was reversing. She
said, “The best tool for this is PAGAI. . . If you have source it
can give you ranges of variables at certain parts in a program,
like on function loops and stuﬀ.” Speciﬁcally, PAGAI lets
REs annotate source code to deﬁne variables of interest and
then presents results in context of these annotations [77].
Focused on improving readability (RQ2). Throughout,
REs pay special attention to improving code readability by
modifying it to include semantic information discovered dur-
ing their investigation. In most cases, the main purpose of
tools REs used was to improve code readability (N=9). Many
REs used decompilers to convert the assembly code to a more
readable high-level language (N=9), or tools like IDA’s lu-
mina server [78] to label well-known functions (N=2). Addi-
tionally, most REs performed several manual steps speciﬁcally
to improve readability, such as renaming variables (N=14),
taking notes (N=14), and reconstructing data structures (N=8).
P01M explained the beneﬁt of this approach when looking at
a ﬁle reading function by saying, “It just says call DWORD
40F880, and I have no idea what that means. . . so, I’ll just
rename this to read ﬁle. . . [now I know] it’s calling read ﬁle
and not some random function that I have no idea what it
is.” Taking notes was also useful when several manipulations
were performed on a variable. For example, to understand a
series of complex variable manipulations, P05V said “I would
type this out. A lot of times I could just imagine this in my
head. I think usually I can hold in my head two operations...If
it’s anything greater than that I’ll probably write it down.”
Online resources queried to understand complex under-
lying systems (RQ2). Regarding external resources, REs
most often reference system and API documentation (N=10).
They reference this documentation to determine speciﬁc de-
tails about assembly opcodes or API arguments and function-
ality. They also reference online articles (N=4) that provide in-
depth breakdowns of complicated, but poorly documented sys-
tem functions (e.g., memory management, networking, etc.).
When those options fail, some REs also reference question-
answering sites like StackOverﬂow (N=4) because “some-
times with esoteric opcodes or functions, you have to hope
that someone’s asked the question on StackOverﬂow because
there’s not really any good documentation” (P3). Many par-
ticipants also google speciﬁc constants or strings they assume
are unique to an algorithm (N=7). P10 explained, “For ex-
ample, MD5 contains an initialization vector with a constant.
You just google the constant and that tells you the algorithm.”
7 Discussion
Our key ﬁnding is the identiﬁcation and description of a three-
phase RE process model, along with cross-phase trends in
REs’ behaviors. This both conﬁrms and expands on prior
work, which described an RE model of increasingly reﬁned
hypotheses [46]. We demonstrate a process of hypothesis
generation and reﬁnement through each phase, but also show
the types of questions asked, hypotheses generated, actions
taken, and decisions made at each step as the RE expands
their program knowledge.
Our model highlights components of RE for tool design-
ers to focus on and provides a language for description and
comparison of RE tools. Building on this analysis model, we
propose ﬁve guidelines for RE tool design. For each guide-
line, we discuss the tools closest to meeting the guideline
(if any), how well it meets the guideline, and challenges in
adopting the guideline in future tool development. Table 2
provides a summary, example application, and challenges for
each guideline. While these guidelines are drawn directly
from our ﬁndings, further work is needed to validate their
eﬀectiveness.
G1. Match interaction with analysis phases. The most ob-
vious conclusion is that RE tools should be designed to mesh
with the three analysis phases identiﬁed in Section 5. This
means REs should ﬁrst be provided with a program overview
for familiarization and to provide feedback on where to focus
eﬀort (overview). As they explore sub-components, speciﬁc
slices of the program (beacons and data/control-ﬂow paths)
should be highlighted (sub-component scanning). Finally, con-
crete, detailed analysis information should be produced on
demand, allowing REs to reﬁne their program understanding
(focused experimentation).
While this guideline is straightforward, it is also signiﬁcant,
as it establishes an overarching view of the RE process for
tool developers. Because current RE tool development is ad-
hoc, tools generally perform a single part of the process and
leave the RE to stitch together the results of several tools. G1
provides valuable insights to single-purpose tool developers
by identifying how they should expect their tools to be used
1886    29th USENIX Security Symposium
USENIX Association
Reverse Engineering Tool Design Guidelines
Example Application
G1 Match interaction with analysis phases
G2
G3
G4
G5
Reverse engineering tools should be designed to facilitate each anal-
ysis phase: overview, sub-component scanning, and focused experi-
mentation.
Present input and output in the context of code
Integrate analysis interaction into the disassembler or decompiled
code view to support tool adoption
Allow data transfer between static and dynamic contexts
Static and dynamic analyses should be tightly coupled so that users
can switch between them during exploration.
Allow selection of analysis methods
When multiple options for analysis methods or levels of approximation
are available, ask the user to decide which to use.
Support readability improvements
Infer semantic information from the code where possible and allow
users to change variable names, add notes, and correct decompilation
to improve readability.
IDAPro [19], BinaryNinja [20], Radare2 [79]
Provide platforms for REs to combine analyses, but previously lacked
thorough RE process model to guide analysis development and inte-
gration.
Lighthouse [80]
Highlights output in the context of code, but does not support input in
code context.
None we are aware of
We do not know of any complex analysis examples. This is possibly
due to challenges with visualization and incremental analysis.
Hex-rays decompiler [81]
Minimally applies G4 by giving users a binary option of a potentially
imprecise decompiled view or a raw disassembly view.
DREAM++ decompiler [5]
Provides signiﬁcantly improved decompiled code readability through
several heuristics, but is limited to a preconﬁgured set of readability
transformations.
Table 2: Summary of guidelines for RE tool interaction design.
and the input and output formats they should support. Addi-
tionally, with the growing eﬀort to produce human-assisted
vulnerability discovery systems [4], G1 shows when and how
human experts should be queried to support automation.
The closest current tools to fulﬁlling G1 are popular re-
verse engineering platforms such as IDAPro [19], BinaryN-
inja [20], and Radare [79], which provide disassembly and
debugger functionality and support user-developed analysis
scripts. These tools allow REs to combine diﬀerent analy-
ses (N=16). However, due to these tools’ open-ended nature
and the lack of a prior RE process model, there are no clear
guidelines for script developers, and users often have to per-
form signiﬁcant work to ﬁnd the right tool for their needs and
incorporate it into their process.
G2. Present input and output in the context of code. We
found that most REs only used tools whose interactions were
tightly coupled with the code. This suggests that tool de-
velopers should place a high priority on allowing users to
interact directly with (disassembled or decompiled) code. The
best example of this we observed was given by P05V in the
code-coverage visualization plugin Lighthouse, which takes
execution traces and highlights covered basic blocks in a dis-
assembler view [80]. It also provides a “Boolean query where
you can say only show me covered blocks that were covered
by this trace and not that trace, or only show blocks covered in
a function whose name matches a regular expression.” How-
ever, Lighthouse does not fully follow our recommendation,
as there is no way to provide input in the context of the code.
For example, the user might want to determine all the inputs
reaching an instruction to compare their contents. However,
this is not currently possible in the tool.
G3. Allow data transfer between static and dynamic con-
texts. We found that almost all participants switched between
static and dynamic program representations at least once
(N=14). This demonstrates tools’ need to consider both static
and dynamic information, associate relevant components be-
tween static and dynamic contexts, and allow REs to seam-
lessly switch between contexts. For example, P04V suggested
a dynamic taint analysis tool that allows the user to select
sinks in the disassembler view, run the program and track
tainted instructions, then highlight tainted instructions again
in the disassembler view. This tool follows our suggested
guideline, as it provides results from a speciﬁc execution
trace, but also allows the user to contextualize the results in a
static setting.
We did observe one participant using a tool which dis-
played the current instruction in the disassembly view when
stepping through the code in a debugger, and there have been
several analyses developed which incorporate static and dy-
namic data [18,82–86]. However, we are unaware of any more
complex analyses that support user interaction with both static
and dynamic states. Following G3 requires overcoming two
diﬃcult challenges. First, the analysis author must determine
how to best represent dynamic information in a static setting
and vice versa. This requires careful design of the visualiza-
tion to ensure the user is provided relevant information in an
interpretable manner. Second, we speculate that incremental
program analyses (such as those of Szabo et al. [87]) may be
necessary in this setting to achieve acceptable performance
compared to current batch-oriented tools.
G4. Allow selection of analysis methods. Throughout the
RE process, REs choose which methods to use based on prior
experiences and speciﬁc needs, weighing the method’s beneﬁt
against any accuracy loss (N=5). These tradeoﬀ decisions are
inherent in most analyses. Therefore, we recommend tool
designers leverage REs’ ability to consider costs and also
recognize instances where the analysis fails. This can be done
by allowing REs to select the speciﬁc methods used and tune
analyses to ﬁt their needs. One example we observed was the
HexRays decompiler [81], which allows users to toggle be-
tween a potentially imprecise, but easier to read, decompiled
program view and the more complex disassembled view. This
USENIX Association
29th USENIX Security Symposium    1887
binary choice, though, is the minimum implementation of G4,
especially when considering more complex analyses where
the analysis developer must make several nuanced choices
involving analyses such as context, heap, and ﬁeld sensitiv-
ity [88]. This challenge becomes even more diﬃcult if the user
is allowed to mix analysis precision throughout the program,
as static analysis tools generally use uniform analysis sen-
sitivity. However, recent progress indicates that such hybrid
analyses are beginning to receive attention [89, 90].
G5. Support readability improvements. We found most
REs valued program readability improvements. Therefore,
RE tool designers should allow the user to add notes or change
naming to encode semantic information into any outputs. Fur-
ther, because annotation is such a common behavior (N=14),
tools should learn from these annotations and propagate them
to other similar outputs. The best example of a tool seeking to
follow this recommendation is the DREAM++ compiler by