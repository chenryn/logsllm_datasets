title:Adversarial Preprocessing: Understanding and Preventing Image-Scaling
Attacks in Machine Learning
author:Erwin Quiring and
David Klein and
Daniel Arp and
Martin Johns and
Konrad Rieck
Adversarial Preprocessing: Understanding and 
Preventing Image-Scaling Attacks in Machine Learning
Erwin Quiring, David Klein, Daniel Arp, Martin Johns, and Konrad Rieck, 
TU Braunschweig
https://www.usenix.org/conference/usenixsecurity20/presentation/quiring
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Adversarial Preprocessing: Understanding and Preventing
Image-Scaling Attacks in Machine Learning
Erwin Quiring, David Klein, Daniel Arp, Martin Johns and Konrad Rieck
Technische Universität Braunschweig, Germany
Abstract
Machine learning has made remarkable progress in the last
years, yet its success has been overshadowed by different at-
tacks that can thwart its correct operation. While a large body
of research has studied attacks against learning algorithms,
vulnerabilities in the preprocessing for machine learning have
received little attention so far. An exception is the recent work
of Xiao et al. that proposes attacks against image scaling. In
contrast to prior work, these attacks are agnostic to the learn-
ing algorithm and thus impact the majority of learning-based
approaches in computer vision. The mechanisms underlying
the attacks, however, are not understood yet, and hence their
root cause remains unknown.
In this paper, we provide the ﬁrst in-depth analysis of
image-scaling attacks. We theoretically analyze the attacks
from the perspective of signal processing and identify their
root cause as the interplay of downsampling and convolution.
Based on this ﬁnding, we investigate three popular imaging
libraries for machine learning (OpenCV, TensorFlow, and
Pillow) and conﬁrm the presence of this interplay in different
scaling algorithms. As a remedy, we develop a novel defense
against image-scaling attacks that prevents all possible at-
tack variants. We empirically demonstrate the efﬁcacy of this
defense against non-adaptive and adaptive adversaries.
1 Introduction
Machine learning techniques have enabled impressive
progress in several areas of computer science, such as in
computer vision [e.g., 11, 12, 13] and natural language pro-
cessing [e.g., 7, 18, 31]. This success, however, is increas-
ingly foiled by attacks from adversarial machine learning that
exploit weaknesses in learning algorithms and thwart their
correct operation. Prominent examples of these attacks are
methods for crafting adversarial examples [6, 32], backdoor-
ing neural networks [10, 15], and inferring properties from
learning models [9, 27]. While these attacks have gained
signiﬁcant attention in research, they are unfortunately not
the only weak spot in machine learning systems.
Recently, Xiao et al. [35] have demonstrated that data
preprocessing used in machine learning can also suffer from
vulnerabilities. In particular, they present a novel type of
attack that targets image scaling. The attack enables an ad-
versary to manipulate images, such that they change their
appearance when scaled to a speciﬁc dimension. As a result,
any learning-based system scaling images can be tricked into
working on attacker-controlled data. As an example, Figure 1
shows an attack against the scaling operation of the popular
TensorFlow library. The manipulated image (left) changes to
the output (right) when scaled to a speciﬁc dimension.
Attacks on image scaling pose a threat to the security of
machine learning: First, scaling is omnipresent in computer
vision, as learning algorithms typically require ﬁxed input
dimensions. Second, these attacks are agnostic to the learning
model, features, and training data. Third, the attacks can be
used for poisoning data during training as well as misleading
classiﬁers during prediction. In contrast to adversarial ex-
amples, image-scaling attacks do not depend on a particular
model or feature set, as the downscaling can create a perfect
image of the target class. As a consequence, there is a need
for effective defenses against image-scaling attacks. The un-
derlying mechanisms, however, are not understood so far and
the root cause for adversarial scaling is still unknown.
In this paper, we provide the ﬁrst comprehensive analysis
of image-scaling attacks. To this end, we theoretically ana-
lyze the attacks from the perspective of signal processing and
Figure 1: Example of an image-scaling attack. Left: a manipulated image
showing a cat. The scaling operation produces the right image with a dog.
USENIX Association
29th USENIX Security Symposium    1363
DownscalinginTensorFlowManipulatedimageOutputimageidentify the root cause of the attacks as the interplay of down-
sampling and convolution during scaling. That is, depending
on the downsampling frequency and the convolution kernel
used for smoothing, only very speciﬁc pixels are considered
for generating the scaled image. This limited processing of
the source image allows the adversary to take over control
of the scaling process by manipulating only a few pixels. To
validate this ﬁnding, we investigate three popular imaging
libraries for machine learning (OpenCV, TensorFlow, and
Pillow) and conﬁrm the presence of this insecure interplay in
different scaling algorithms.
Based on our theoretical analysis, we develop defenses for
fending off image-scaling attacks in practice. As a ﬁrst step,
we analyze the robustness of scaling algorithms in the three
imaging libraries and identify those algorithms that already
provide moderate protection from attacks. In the second step,
we devise a new defense that is capable of protecting from all
possible attack variants. The defense sanitizes explicitly those
pixels of an image that are processed by a scaling algorithm.
As a result, the adversary loses control of the scaled content,
while the quality of the source image is largely preserved.
We demonstrate the efﬁcacy of this strategy in an empirical
evaluation, where we prevent attacks from non-adaptive as
well as adaptive adversaries.
Finally, our work provides an interesting insight into re-
search on secure machine learning: While attacks against
learning algorithms are still hard to analyze due to the com-
plexity of learning models, the well-deﬁned structure of scal-
ing algorithms enables us to fully analyze scaling attacks and
develop effective defenses. As a consequence, we are opti-
mistic that attacks against other forms of data preprocessing
can also be prevented, given a thorough root-cause analysis.
Contributions. In summary, we make the following contri-
butions in this paper:
• Analysis of image-scaling attacks. We conduct the ﬁrst
in-depth analysis of image-scaling attacks and identify
the vulnerability underlying the attacks in theory as well
as in practical implementations.
• Effective Defenses. We develop a theoretical basis for
assessing the robustness of scaling algorithms and de-
signing effective defenses. We propose a novel defense
that protects from all possible attack variants.
• Comprehensive Evaluation. We empirically analyze scal-
ing algorithms of popular imaging libraries under attack
and demonstrate the effectivity of our defense against
adversaries of different strengths.
The rest of this paper is organized as follows: We review
the background of image scaling and attacks in Section 2. Our
theoretical analysis is presented in Section 3, and we develop
defenses in Section 4. An empirical evaluation of attacks and
defenses is given in Section 5. We discuss related work in
Section 6, and Section 7 concludes the paper.
Table 1: Scaling algorithms in deep learning frameworks.
Framework
Library
Library Version
Nearest
Bilinear
Bicubic
Lanczos
Area
Caffe
OpenCV
PyTorch
Pillow
TensorFlow
tf.image
4.1
•
•(*)
•
•
•
6.0
•(‡)
•(*)
•
•
•
1.14
•
•(*)
•
•
(*) Default algorithm. (‡) Default algorithm if Pillow is used directly without PyTorch.
2 Background
Before starting our theoretical analysis, we brieﬂy review the
background of image scaling in machine learning and then
present image-scaling attacks.
Image Scaling in Machine Learning
2.1
Image scaling is a standard procedure in computer vision and
a common preprocessing step in machine learning [21]. A
scaling algorithm takes a source image S and resizes it to
a scaled version D. As many learning algorithms require a
ﬁxed-size input, scaling is a mandatory step in most learning-
based systems operating on images. For instance, deep neural
networks for object recognition, such as VGG19 and Incep-
tion V3/V4 expect inputs of 224× 224 and 299× 299 pixels,
respectively, and can only be applied in practice if images are
scaled to these dimensions.
Generally, we can differentiate upscaling and downscaling,
where the ﬁrst operation enlarges an image by extrapolation,
while the latter reduces it through interpolation. In practice,
images are typically larger than the input dimension of learn-
ing models and thus image-scaling attacks focus on down-
scaling. Table 1 lists the most common scaling algorithms.
Although these algorithms address the same task, they differ
in how the content of the source S is weighted and smoothed
to form the scaled version D. For example, nearest-neighbor
scaling simply copies pixels from a grid of the source to the
destination, while bicubic scaling interpolates pixels using a
cubic function. We examine these algorithms in more detail
in Section 3 when analyzing the root cause of scaling attacks.
Due to the central role in computer vision, scaling algo-
rithms are an inherent part of several deep learning frame-
works. For example, Caffe, PyTorch, and TensorFlow imple-
ment all common algorithms, as shown in Table 1. Techni-
cally, TensorFlow uses its own implementation called tf.image,
whereas Caffe and PyTorch use the imaging libraries OpenCV
and Pillow, respectively. Other libraries for deep learning
either build on these frameworks or use the imaging libraries
directly. For instance, Keras uses Pillow and DeepLearning4j
builds on OpenCV. As a consequence, we focus our analysis
on these major imaging libraries.
1364    29th USENIX Security Symposium
USENIX Association
Image-Scaling Attacks
2.2
Recently, Xiao et al. [35] have shown that scaling algorithms
are vulnerable to attacks and can be misused to fool machine
learning systems. The proposed attack carefully manipulates
an image, such that it changes its appearance when scaled
to a speciﬁc dimension. In particular, the attack generates
an image A by slightly perturbing the source image S, such
that its scaled version matches a target image T . This process
is illustrated in Figure 2, which also serves as a running
example throughout this paper. In addition, Table 2 provides
an overview of our notation.
Table 2: Table of symbols for scaling attacks.
Symbol Size
S
m× n
T
A
D
m(cid:48) × n(cid:48)
m× n
m(cid:48) × n(cid:48)
Description
The source image that is used to create
the attack image.
The target image that the adversary wants
to obtain after scaling.
The attack image, a slightly perturbed
version of S
The output image of the scaling function
scale.
model: The attacks are model-independent and do not depend
on knowledge of the learning model, features, or training
data. Furthermore, image-scaling attacks are effective even if
neural networks were robust against adversarial examples, as
the downscaling can create a perfect image of the target class.
Finally, we note that these attacks are of particular concern in
all security-related applications where images are processed.
Figure 2: Principle of image-scaling attacks: An adversary computes A such
that it looks like S but downscales to T .
2.2.3 Attack Strategy
2.2.1 Capabilities and Knowledge
The attack is agnostic to the employed learning model and
does not require knowledge of the training data or extracted
features. Yet, the adversary needs to know two parameters:
(a) the used scaling algorithm and (b) the target size m(cid:48)×n(cid:48) of
the scaling operation. Xiao et al. describe how an adversary
can easily deduce both parameters with black-box access to
the machine learning system by sending speciﬁcally crafted
images [see 35]. Moreover, Table 1 shows that common open-
source libraries have a limited number of scaling options,
and thus only a few attempts are necessary to discover the
correct setup. In some settings, a ﬁxed algorithm can be even
enforced by speciﬁc image sizes, as we show in Appendix A.
2.2.2 Attack Scope
As the image is manipulated before any feature extraction,
image-scaling attacks can effectively mislead all subsequent
steps in a machine-learning pipeline, allowing different at-
tacks during train and test time. That is, an attacker can
conceal data poisoning attacks [see 24]. For instance, she can
modify the training data such that a backdoor pattern becomes
present in the downscaled image which was not visible in the
unscaled training image before.
Furthermore, she can trigger false predictions during the
application of a learning model by creating a downscaled
image of another, targeted class. Compared to adversarial
examples [32], both attacks accomplish the same goal. How-
ever, image-scaling attacks considerably differ in the threat
(O2) The attack image A needs to be indistinguishable from
target image: scale(A) ∼ T .
the source image: A ∼ S.
There exist a strong and a weak strategy for implementing
image-scaling attacks. In the strong strategy, the adversary
can choose the source and target image. In the weak version,
the adversary can only choose the target, and the calculated
attack image is meaningless and easily detectable. We thus
focus on the stronger attack strategy in our paper, which is of
particular concern in real-world applications.
Objectives. Formally, image-scaling attacks need to pursue
the following two objectives:
(O1) The downscaling operation on A needs to produce the
The ﬁrst objective ensures that the target image T is ob-
tained during scaling, while the second objective aims at
making the attack hard to detect. We verify objective O1 by
checking if the prediction of a neural network corresponds to
the target image’s class. Note that without the second objec-
tive, the attack would be trivial, as the adversary could simply
overwrite S with T . In this case, however, the attack would
be easily detectable and thus not effective in practice.
Strong Attack Strategy. The adversary seeks a minimal
perturbation ∆ of S, such that the downscaling of ∆ + S = A
produces an output similar to T . Both goals can be summa-
rized as the following optimization problem:
min((cid:107)∆(cid:107)2
2)
s.t. (cid:107)scale(S + ∆)− T(cid:107)∞ (cid:54) ε .
(1)
USENIX Association
29th USENIX Security Symposium    1365
SolveSourceImageSTargetImageTAttackImageAOutputImageDscaleA∼Sscale(A)∼TAdditionally, each pixel value of A needs to remain within the
ﬁxed range (e.g., [0,255] for 8-bit images). This problem can
be solved with Quadratic Programming [5]. When successful,
the adversary obtains an image A that looks like the source