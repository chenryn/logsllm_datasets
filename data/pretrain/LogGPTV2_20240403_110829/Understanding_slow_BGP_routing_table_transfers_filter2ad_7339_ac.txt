353 350
 300
 250
 200
 150
 100
 50
BGP router CPU
# installed routes
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
 0
00:00 02:00 04:00 06:00 08:00 10:00 12:00 14:00 16:00
Time (min)
(a) Sending the full table
 350
 300
 250
 200
 150
 100
 50
BGP router CPU
# installed routes
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
)
s
d
n
a
s
u
o
h
t
n
i
(
s
e
t
u
o
r
f
o
r
e
b
m
u
N
)
s
d
n
a
s
u
o
h
t
n
i
(
s
e
t
u
o
r
f
o
r
e
b
m
u
N
U
P
C
f
o
e
g
a
t
n
e
c
r
e
P
U
P
C
f
o
e
g
a
t
n
e
c
r
e
P
 0
 0
00:00 02:00 04:00 06:00 08:00 10:00 12:00 14:00 16:00
(b) Reducing the table size: sending a ﬁltered table
Time (min)
Figure 5: Reducing the table size
Limit 3: The receiver’s capacities
We now study possible limits at the receiver when we reduce the
table size. Sending a ﬁltered table increases the rate at which PE
installs routes, which might overload it. We study PE’s behavior in
both the ‘full table’ and the ‘ﬁltered table’ experiments.
Fig. 5(a) and Fig. 5(b) show PE’s behavior when it installs around
136,000 routes in the full and ﬁltered table experiments, respec-
tively. The x-axis presents the experiment time, right y-axis the
BGP CPU utilization, and left y-axis the number of routes. In the
‘full table’ experiments, we reset the session between PE and RR
at time 04:54. The number of installed routes goes down to zero
and PE starts a period of high CPU activity in which it mostly
removes previously installed routes and prepares BGP withdrawal
messages to customer routers. At time 07:05, the session with RR
is re-established and PE starts to receive the routes. The transfer
time is the time between the re-establishment of the session and
when PE installs all routes. During this time, PE selects the routes
needed for its customers. Then, at regular time intervals, PE in-
stalls selected routes in the corresponding VPN forwarding tables.
The execution of the install process generates small spikes of BGP
CPU (20%) at regular intervals.
In the ‘ﬁltered table’ experiment presented in Fig. 5(b), the total
table transfer time is around one minute and 30 seconds instead of
four minutes and 40 seconds in the ‘full table’ experiment. How-
ever, the BGP CPU activity is much higher (up to 90%) because
each time the router has to install routes, more new routes are avail-
able. The CPU activity remains high after PE installs all the routes.
These CPU spikes correspond to PE sending updates to the cus-
tomer routers.
The comparison of the two experiments shows that even though
PE installs the same number of routes, it experiences a much higher
load when it receives the ﬁltered table. Therefore, reducing the
table size increases the route installing rate, which can overload the
receiver.
By analogy, increasing the route sending rate can have the same
effect on the receiver. There is clearly a trade-off between fast table
transfers and controlled router load. Since routers have different
capacities, we argue that table sending rates should not be hard
coded. Operators should be able to tune the speed of table transfers
according to their needs. Table transfers between powerful routers
may not need rate limiting for instance.
6. RELATED WORK
Studies of events that trigger BGP table transfers. The events
responsible for the largest disruptions in the Internet are of two
kinds: BGP session resets or failures and routing changes caused
by internal events [4]. Such events are often followed by a full
or a partial BGP table transfer [2, 3]. Wang et al. [12] studied
the causes and impact of BGP session failures using routing mes-
sages and router logs from an Internet backbone; whereas Teixeira
et al. [1] studied BGP changes triggered by internal routing changes
and reported that just the transfer of 80,000 preﬁxes took 80 sec-
onds. These studies recognized that BGP table transfers are slow
with no explanation.
Techniques to detect BGP table transfers. Zhang et al. [3]
proposed an algorithm to detect BGP table transfers triggered by
session resets between BGP monitors and their direct peers. They
also observed that BGP can take several minutes to transfer a full
routing table, but do not explain the reasons behind this delay. Their
goal was to identify BGP table transfers on data collected at BGP
monitors to remove them from later analysis.
Techniques to improve BGP table transfers. Xiao et al. [13]
studied the impact of BGP timers and TCP retransmissions on BGP
session failures with the goal of improving the robustness of iBGP
sessions. Their analytical study cannot observe the implementation
issues that we found here. Wang et al. [14] proposed a bloom-ﬁlter
based mechanism that accelerates recovery after a BGP session re-
set. Their solution avoids full BGP table transfers by exchanging
only small digests of BGP tables. Despite its promises, this solu-
tion is not yet deployed, because it requires fundamental changes
to BGP.
7. CONCLUSION
This paper is the ﬁrst to study the causes of slow BGP table trans-
fers using testbed experiments and data collected from a large VPN
provider. We found that BGP table transfers are slow because of
the timer-driven implementation of the sending of BGP messages.
This implementation decision is prevalent on all routers we tested,
but it is not documented. Unfortunately, this decision considerably
slows down BGP table transfers. We studied different solutions for
reducing table transfer times, but these solutions come at the price
of higher router load.
Our modiﬁed version of SBGP and Vendor 1’s updated software
show that event-driven implementations signiﬁcantly speed up ta-
ble transfers. However, there is a trade-off between fast BGP table
transfers and controlled router load. The need for faster table trans-
fers or for more controlled router load depends on factors like the
nature of the network, the number of BGP neighbors, the number
of routes, or the routers capacities. For instance, routers at VPN
354providers are often in a private address space and are consequently
less vulnerable to attacks; at the same time VPN customers have
stricter availability requirements. Hence, operators of VPN back-
bones should tune their routers to speed up BGP table transfers. On
the other hand, large Internet providers may want to rate-limit BGP
table transfers to ensure controlled load on their routers. When a
router has many BGP sessions, the introduction of gaps at each
session is an easy way to multiplex the router among different ses-
sions. These two practical scenarios illustrate that picking one point
in the solution space between fast table transfer and router control
and hard-coding it in BGP’s implementation is too limiting. Router
vendors should expose this design decision to network operators
and let them customize their networks.
8. ACKNOWLEDGMENTS
We would like to thank Stephane Litkowski for his valuable com-
ments on this work. We are grateful to Sarah Nataf for her help
in setting up and debugging our testbed. We are also grateful to
Guillaume Gaulon, Bruno Decraene, Ítalo Cunha and the anony-
mous reviewers for their comments and helpful feedback on earlier
versions of this work. We also thank the router vendors for the
discussions and feedback.
9. REFERENCES
[1] R. Teixeira, A. Shaikh, T. Grifﬁn, and J. Rexford, “Dynamics
of Hot-Potato Routing in IP Networks,” in Proc. ACM
SIGMETRICS, 2004.
[2] L. Wang, X. Zhao, D. Pei, R. Bush, D. Massey, A. Mankin,
S. F. Wu, and L. Zhang, “Observation and analysis of BGP
behavior under stress,” in Proc. Internet Measurement
Workshop, 2002.
[3] B. Zhang, V. Kambhampati, M. Lad, D. Massey, and
L. Zhang, “Identifying BGP routing table transfers,” in Proc.
ACM SIGCOMM Workshop on mining network data
(MineNet), 2005.
[4] J. Wu, Z. M. Mao, J. Rexford, and J. Wang, “Finding a
needle in a haystack: Pinpointing signiﬁcant BGP routing
changes in an IP network,” in Proc. USENIX Symposium on
Networked Systems Design and Implementation, May 2005.
[5] Z. B. Houidi, R. Teixeira, and M. Capelle, “Origin of route
explosion in Virtual Private Networks,” in Proc. ACM
CoNEXT student workshop, 2007.
[6] T. Bates, R. Chandra, and E. Chen, “BGP Route
Reﬂection-An Alternative to Full Mesh IBGP.” RFC 2796,
Apr 2000.
[7] E. Chen, “Route Refresh Capability for BGP-4.” RFC 2918,
Sep 2000.
[8] J. Padhye, V.Firoiu, D.Towsley, and J. Kurose, “Modeling
TCP throughput: A simple model and its empirical
validation.,” in IEEE/ACM Trans. Networking, 2005.
[9] R. Zhang and M. Bartell, BGP design and implementation,
pp. 62–80. Cisco Press, 2004.
[10] C. Labovitz and M. Hirabaru, “MRT: Merit’s Multi-Threaded
Routing Toolkit .” http://mrt.sourceforge.net/.
[11] P. Marques et al., “Constrained Route Distribution for
BGP/MPLS IP VPNs.” RFC 4684, Nov 2006.
[12] L. Wang, M. Saranu, J. M. Gottlieb, and D. Pei,
“Understanding BGP Session Failures in a Large ISP,” in
Proc. IEEE INFOCOM, 2007.
[13] L. Xiao and K. Nahrstedt, “Reliability models and evaluation
of internal BGP networks,” in Proc. IEEE INFOCOM, 2004.
[14] L. Wang, D. Massey, K. Patel, and L. Zhang, “FRTR: A
scalable mechanism for global routing table consistency,” in
Proc. International Conference on Dependable Systems and
Networks, 2004.
355