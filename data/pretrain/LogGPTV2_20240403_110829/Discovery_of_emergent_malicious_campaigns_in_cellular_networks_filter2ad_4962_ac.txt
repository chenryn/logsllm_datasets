### Cluster Size Distribution

**Figure 6: Cluster Size Distribution**
Figure 6 illustrates a histogram of the average cluster sizes over multiple test windows. The data shows that approximately 10 clusters contain more than 100 high-degree nodes, while around 30 clusters have between 10 and 100 nodes. Additionally, about 50 clusters consist of between 5 and 10 nodes. Most clusters (the majority) have fewer than 5 nodes.

### Cluster Dynamics

A **new cluster** is defined as one that has no overlap with any clusters from the previous sliding window. Conversely, an **obsolete cluster** is one that does not overlap with any clusters in the next test window. Clusters with non-zero overlap with any cluster in the previous window are considered **active clusters**, indicating that the nodes within these clusters remain high-degree nodes with users frequently communicating with them.

For active clusters, we measure the similarity between two versions of the same cluster in different test windows using the Jaccard similarity coefficient, which is calculated as:

\[ J = \frac{|A \cap B|}{|A \cup B|} \]

**Figure 7: Average Jaccard Similarity of Clusters Between Two Successive Test Windows**
Figure 7 presents the Jaccard similarity coefficients for clusters in two adjacent sliding windows. On an average day, 85% of the clusters remain unchanged from one test window to the next. Of the 15% of clusters that do change, almost all are larger than 5 nodes. Given that there are approximately 100 clusters larger than 5 nodes, roughly 15 clusters change each day.

### New and Obsolete Clusters

**Figure 8: Number of New Clusters and New Clusters of Size > 3 in 15 Successive Sliding Test Windows**
Figure 8 displays the number of new clusters generated across several successive sliding test windows, along with the number of new clusters of size greater than 3. Approximately 96% to 97% of both obsolete and new clusters are of size 3 or less. Only about 3% of new clusters are larger than 3 nodes. On average, around 100 new clusters are generated daily, with only about 3 being large clusters per day.

Therefore, with approximately 3 new and 15 changed clusters per day, the system generates a manageable amount of work for human analysts to investigate.

### The Role of the Analyst

Up to this point, all steps have involved automated processing to filter out as much data as possible while retaining suspicious activity. The role of the human analyst is to determine whether a presented cluster is malicious or benign. Figure 10 provides an example of a cluster visually presented to an analyst. Our post-processing steps add significant value to the decision-making process.

Sometimes, it is relatively easy to determine that a cluster is not malicious by recognizing patterns in the domain names involved. For instance, some benign clusters include "current event clusters," "political clusters," and "holiday clusters." After removing apparently benign clusters, a finer level of manual analysis is required for suspicious clusters. The analyst can scrutinize the domains within the cluster to identify malicious or fraudulent intent. As with most investigations into malicious activity, a certain amount of detailed research into the event or activity is necessary.

### Evaluation and Case Studies

#### Traditional Evaluation

Traditional evaluation of a detection algorithm typically involves calculating true positive and false positive rates, often visualized in a Receiver Operating Characteristic (ROC) curve. However, establishing ground truth with real data and the system's role as a tool for analysts rather than a standalone blocking/detection mechanism necessitates alternative evaluation methods. At the network provider level, the process for blocking abusive users, such as those involved in spam and malware campaigns, centers around analysts gathering sufficient information before taking action.

#### Key Questions

We aim to answer two key questions:
1. Does our system detect widespread attacks in the clusters it produces?
2. Do we miss any widespread attacks?

To address the first question, we have confirmation that two premium numbers in our large example cluster were blocked due to fraud (details in section 4.1). We also successfully detected the resurgence of the Android "NotCompatible" malware on the first day of its outbreak (details in section 4.2). By comparing our clusters to user reports like SMS spam and domain reputation, we observe that the number of entities in clusters being blacklisted increases over time, demonstrating that our system detects malicious entities ahead of existing blacklisting systems.

For the second question, we note that a few malware campaigns were reported in the media. Two campaigns were successfully detected, but the system did not detect a small campaign where malware infected devices and controlled them via commands to send SMS spam [17]. This was because the malware download sites were high-degree nodes without enough overlap to be clustered, and the Command and Control (C&C) domains did not appear due to a low volume of infected users. If the campaign had grown, our algorithm would have detected the correlation in the initial infection vectors. No other widespread mobile malware campaigns were reported during our experiment period.

#### Growth in Blacklisted Members

One way to verify the system's effectiveness is to check the members of each cluster against public and private blacklists and see if the fraction of blacklisted entities in a cluster increases over time. We use three sources of blacklists:
- **SMS Numbers:** We check against 7726 data, a feed of user-reported SMS spam, and an internal list of numbers sending spam URLs. We only match against numbers highly likely to be spammers.
- **Domain Names:** We check our internal URL spam list and Web of Trustâ€™s score [2]. We only consider high-confidence listings, such as a score of 10 or less on Web of Trust, indicating the domain is blacklisted by Spamhaus [1] or has many user reports.

**Figure 9: Ratio of Anomalous High-Degree Nodes That Are Blacklisted Over Time**
Figure 9 shows the ratio of cluster members labeled as malicious by the blacklists. It is evident that the number of blacklisted members increases over time. Some of our clusters are identified as 100% malicious by the blacklists, providing further confidence in our detection of malicious campaigns.

We also checked against known botnet C&C servers and found only a dozen users sparsely communicating with 3 blacklisted domains. This suggests that we did not miss any significant known botnet activity in the cellular network. From public media reports and internal investigation, we found evidence of only one small botnet and no widespread mobile botnet incidents, indicating that our system did not miss any well-discovered widespread attacks during the reported period.

### Case Studies

In the remainder of this paper, we describe a few case studies centered around the most interesting malicious clusters encountered in the cellular network. The purpose is to provide insight into the types of malicious campaigns we uncovered.

#### 4.1 SMS Giftcard Scam

**Suspicious Cluster**
During July to September 2012, we detected a cluster consisting of 10-digit phone numbers, SMS shortcodes, and domain names. This cluster evolved constantly throughout this period. Over time, several entities in this cluster appeared in domain name blacklists and the 7726 blacklist. Figure 10 shows the cluster's evolution from the day it was first detected.

The presence of phone numbers, short codes, and domain names in the cluster at various times is intriguing, suggesting activity involving all three types of entities. Several changes occurred in the cluster from August to September. The cluster initially grew by adding several 10-digit phone numbers, which were soon blacklisted as SMS spammers. New domain names continually appeared, while old ones disappeared. One specific domain name seemed central, suggesting a series of redirects leading to it. A shortcode then appeared and remained strongly correlated with the cluster center. Successive incarnations of this cluster are visualized in Figure 10. Figure 11 shows the Jaccard similarity between successive versions of the cluster over a one-month period.

**Manual Analysis**
To confirm the malicious nature of this cluster, our analyst performed a detailed analysis starting with one of the URLs in the SMS messages. The following malicious campaign was discovered:
- A carefully crafted URL with a well-known brand name in the 3rd or 4th level domain position was placed in an SMS spam message, promising a free gift card from that brand.
- This initial URL redirected the user to another URL, where they were asked to enter a "winning code" provided in the SMS, along with their phone number and address, to "claim their prize."
- Next, the user was redirected to yet another URL, where they were told to complete a "survey" to receive the gift card. This "survey" actually elicited the user to participate in receiving one of the products advertised on the site.