59GB
65,942
16,239
341,517
35,990
N/A
N/A
Table 1: Statistics of PDF documents collected from VirusTotal
we found 52 PDF documents that we believe to have been falsely
classiﬁed as benign by all antivirus engines at VirusTotal. No
cases were found where PDF ﬁles belonging to the same group
of distinct ﬁles at the token level were assigned diﬀerent labels.
5. EXPERIMENTAL EVALUATION
The real-world nature and the sheer size of the VirusTotal data
make our evaluation especially challenging. First, the distinction
between “detected” and “undetected” corpora is somewhat vague,
as classiﬁcations by antivirus engines cannot be fully trusted. Sec-
ond, the huge size of the “detected” corpus makes its manual anal-
ysis infeasible. On the other hand, the small size of the labeled
JavaScript-bearing part of the “undetected” corpus is too small to
be used for training purposes.
As the baseline for comparison we consider Wepawet, a web-
based service based on JSand [7]. Wepawet performs both static
and dynamic analysis of PDF ﬁles based on their JavaScript con-
tent and can detect malware that it has a signature for (labeled as
malicious), as well as unknown malware (labeled as suspicious) us-
ing statistical features. In the evaluation, we treat both categories as
detections. Similar to our system, Wepawet generally does not rec-
ognize PDF malware that does not use JavaScript. Table 2 shows
Wepawet’s classiﬁcation on the “detected” and “undetected” parts
of all three corpora at our disposal. In some cases ﬁle uploads were
rejected by Wepawet, referred to as fail, or resulted in internal er-
rors despite multiple submissions, referred to as error. We treat
such cases (about 1.7% of the total data) as benign.
03. Nov. 2010
undet.
38
1
212
11
10
det.
12
15
3,860
1,474
1,265
19. Jan. 2011
undet.
det.
25
9
5
0
167
502
0
149
462
4
17. Feb. 2011
undet.
73
0
397
0
22
det.
19
83
1,050
257
6,117
Fail
Error
Benign
Suspicious
Malicious
Table 2: Wepawet classiﬁcation results
5.1 Objectives and Evaluation Criteria
Our experiments address the following questions:
1. How well do PJScan and Wepawet detect known malicious
documents? This question may appear meaningless: why
bother detecting something that is already detected? In prac-
tice, however, it is impossible to deploy all 42 antivirus en-
gines from VirusTotal. For a single method, attaining the
detection accuracy close to that of 42 established antivirus
products is still a very challenging goal16. The correspond-
ing quality measure is the true positive rate on known attacks
T PN deﬁned as the ratio of the number of ﬁles in the “de-
tected” corpus classiﬁed as malicious to the total number of
JavaScript-bearing ﬁles in that corpus.
2. How well do both methods detect attacks that were missed
by all 42 VirusTotal engines? We consider documents in
the “undetected” corpus as novel attacks if they are classiﬁed
as malicious during manual analysis. The true positive rate
on unknown attacks T PU is deﬁned as the ratio of the number
of ﬁles in the “undetected” corpus classiﬁed as malicious to
the total number of malicious JavaScript-bearing ﬁles in that
corpus.
3. How many normal documents are classiﬁed as malicious by
the methods in questions? The laboratory false positive rate
FPL is deﬁned as the ratio of the number of ﬁles in the “unde-
tected” corpus classiﬁed as malicious to the total number of
benign JavaScript-bearing ﬁles in the “undetected” corpus.
The operational false positive rate FPOP is the ratio of the
number of ﬁles in the “undetected” corpus classiﬁed as mali-
cious to the total number of benign ﬁles in the “undetected”
corpus.
The distinction between the laboratory and the operational false
positive rates is essential for estimation of the expected impact of
false positives in practical deployment.
5.2 Experimental Protocol
Our experiments were carried out using the following procedure.
We merged all 3 corpora from diﬀerent dates keeping only the dis-
tinction between “detected” and “undetected” parts. We then ran-
domly split the full “detected” corpus in two non-overlapping parts
such that the corresponding sets of token sequences are of the same
size. Due to a signiﬁcant redundancy of token sequences this re-
sults in two sets of ﬁles that are diﬀerent in size. One of these
half-corpora is used to train PJScan, the other half is used to eval-
uate T PK. To decrease the impact of non-determinism via random
splitting, we repeat the experiment the second time by swapping
the training and the evaluation datasets and averaging the detection
accuracy. This process is known as 2-fold cross-validation.
To determine the detection accuracy on unknown data, we ap-
ply the trained model on the full “undetected” corpus. We use the
ground truth information to compute T PU, FPL and FPOP. The re-
ported results are also averaged over the two partitions of the train-
ing data.
16Unfortunately we cannot compare any method against the best
detector at VirusTotal’s. The labels in batch data from VirusTotal
reﬂect only the number of detections but not the speciﬁc engines
that classiﬁed a document as malicious.
(cid:1)(cid:2)(cid:3)
Since the models used in Wepawet do not depend on our training
data (but rather on the data its statistical part was trained on), the
results presented for this method reﬂect the accuracy of scanning a
complete respective dataset (“detected” or “undetected”).
Some preliminary experimentation was needed to choose the pa-
rameters of OCSVM used in our method. We chose the training
rejection rate ν = 0.15 and the n-gram length of 4, which seem to
provide the best trade-oﬀ between the true positive and false posi-
tive rates. The full results of our preliminary screening for optimal
parameters cannot be presented due to space constraints.
5.3 Experimental Results
The results of a comparative evaluation of PJScan and Wepawet
according to the criteria speciﬁed in Section 5.1 are presented in
Table 3. Two conﬁgurations of PJScan were considered: using
only native JavaScript tokens and using a set of additional heuristic
tokens introduced in Section 3.2. It can be seen that PJScan sig-
niﬁcantly outperforms Wepawet on the known malicious data but
performs less accurately on previously unknown attacks. Most of
failed detections were caused by 11 ﬁles which are redundant at the
token level and contain the following code17:
app.setTimeOut(this.info.dgu,1)
In this example, the attack code resides not in a JavaScript en-
tity but in the Info dictionary18. It can be still accessed by a very
short entry-point JavaScript code above as text and gets interpreted
as JavaScript by calling the function setTimeOut() which is equiv-
alent to eval(). With an exception of this kind of attack, the detec-
tion rate of PJScan would have also reached the 90% mark.
It is not clear to us why Wepawet has performed relatively poorly
on known malicious data. In a related comparative evaluation against
Cujo [19] in the context of web-based JavaScript attacks (drive-by-
downloads), Wepawet was a clear winner with a detection rate of
99.8% compared to 94.4%. Most likely, the reason for worse per-
formance of Wepawet in our experiments lies in technical problems
with the extraction of JavaScript code from PDF documents.
Detection method
PJScan (native tokens only)
PJScan (with extra tokens)
Wepawet
T PK
84.80
85.17
63.60
T PU
71.15
71.15
90.38
FPL
16.35
17.35
0.0
FPOP
0.3694
0.3918
0.0
Table 3: Detection performance overview
A relative disadvantage of PJScan is the high false-positive rate.
Measured against only the JavaScript-bearing benign documents it
reaches the painful 16-17%; however, due to the rare presence of
JavaScript code in benign documents, its operational false-positive
rate remains acceptable and corresponds, for our data, to 1.7 false
alarms per day.
One can also see that heuristic tokens do not improve the per-
formance of PJScan and even lead to a slight degradation of the
false-positive rate. The causes for this eﬀect as well as for the false
positives are elucidated in the following section.
5.4 Signiﬁcant Features
As noted by Sommer and Paxson [22], a security practitioner
would always be interested to know what a learning method has
17All examples diﬀer in the name for the member of the this.info
dictionary (in this case, dgu).
18An Info dictionary is used to store meta-data about the PDF ﬁle,
such as author name, the software used to create it, etc.
actually learned. The model created by the OCSVM (the center
c of the sphere) produces a numeric ranking of essential features
encountered in malicious JavaScript code. Since no benign data
is used for training, this ranking does not reﬂect the diﬀerences
between two classes but rather describes only one class known to
it. Examples of the 5 most important and the 5 least important
features in one of the models learned by PJScan (created for one
half of the data) are shown in Table 4.
Although these features do not look particularly malicious, the
top 5 features clearly correspond to typical lexical patterns of pro-
gramming languages: member function dereferencing (Feature 1),
string variable assignment (Feature 2), function calls (Features 3
and 4) and variable declarations (Feature 5). On the other end of
the spectrum are the features that are obviously very atypical for
programming languages.
The scoring of a new data point in the detection phase involves
the identiﬁcation of an overlapping subset of features between this
data point and the learned model. The smaller the “weighted over-
lap” between the new point and the center (i.e.
the sum of the
weights in the model corresponding to the common features), the
larger the distance from the center. This property is conﬁrmed by
the examples of accepted and rejected points presented below.
For the accepted points (Table 5, one true positive and one false
positive), the main contributions are made by the top features of
the trained model. Such points are virtually indistinguishable in
our model, and this explains a high “laboratory” false positive rate
observed in our experiments. It turns out, however, that very few be-
nign examples share the “normal” programming language features
captured by the learned model. For the two examples of rejected
points (Table 6, one true negative and one false negative) the top
features have much lower ranks in the learned models. The major-
ity of benign examples have a small “weighted overlap” with the
model and hence are rejected.
The investigation of signiﬁcant features in our models suggests
that the key property that enables eﬀective discrimination between
malicious and benign code in PDF documents is the fact that be-
nign usage of JavaScript is very rudimentary from the programming
point of view. Anecdotally, the benign example with the highest re-
jection score corresponds to the code print(true).
5.5 Throughput
The throughput of PJScan was tested on a commodity PC with
a quad-core Intel Core i7 860 CPU, 8 GB of RAM and a 7,200rpm
SATA hard disk drive. Eight processes were run concurrently for
performance measurement.
Each phase of PJScan was run on a respective data partition
(training on one half of “detected” corpus, evaluation on the other
half and on the full “undetected” corpus). Unlike the accuracy mea-
surement, we learned and classiﬁed using all ﬁles ignoring their
redundancy. Learning with thousands of ﬁles instead of a few hun-
dred distinct token sequences reduces performance, but due to the
fast learning and classiﬁcation algorithms the diﬀerence is negli-
gible. Processing times for all stages of our method are shown
in Table 7.
In total, parsing of 65,942 PDF ﬁles, tokenization
of 341,517 JavaScript entities, learning on 15,279 “detected” ﬁles
with JavaScript and classiﬁcation of 960 “undetected” ﬁles with
JavaScript took 1,547 seconds (about 25 minutes). All measure-
ments are expressed in wall clock time19.
19Wall clock time measures real time that elapses between the be-
ginning and the end of a task. It includes CPU time, I/O time and
any overhead such as the time process spends waiting for execu-
tion. It is a good indicator of real performance but is aﬀected by
system load.
(cid:1)(cid:2)(cid:3)
Rank Weight
0.05285
0.05106
0.05092
0.04574
0.04314
1
2
3
4
5
Top 5
Feature
NAME . NAME (
NAME ASSIGN STR ;
NAME ( NAME )
( NAME ) ;
; VAR NAME ASSIGN
Rank
4051
4052
4053
4054
4055
Bottom 5
Weight
2.285e-05
2.285e-05
2.285e-05
2.285e-05
1.865e-05
Feature
) NAME ( THIS
+- NAME !== NAME
] ) - NAME
NAME ] ) -
TRUE } ; IF
Table 4: Features of the center point
True positive top 5
Rank Weight
0.00456
0.00441
0.00439
0.00395
0.00372
1
2
3
4
5
Feature
NAME . NAME (
NAME ASSIGN STR ;
NAME ( NAME )
( NAME ) ;
; VAR NAME ASSIGN
False positive top 5
Rank Weight
0.00554
0.00535
0.00452
0.00413
0.00386
1
2
5
6
7
Feature
NAME . NAME (
NAME ASSIGN STR ;
; VAR NAME ASSIGN
; NAME ( NAME
NAME ( STR )
Table 5: Features of TPs and FPs
True negative top 5
Rank Weight
0.00390
0.00390
0.00359
0.00338
0.00338
7
8
10
14
15