### Description
When using multiple feeds with a blocking feed storage, I encounter duplicated feed logs.

### Steps to Reproduce
1. Create a `test.py` file with the following spider and settings.
2. Run the spider using the command: `scrapy runspider test.py -L INFO`.

**Expected Behavior:**
```
2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: gs://bucket/output.json
2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: gs://bucket/output.csv
```

**Actual Behavior:**
```
2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: gs://bucket/output.csv
2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: gs://bucket/output.csv
```

### Versions
- Scrapy: 2.1.0
- lxml: 4.5.1.0
- libxml2: 2.9.10
- cssselect: 1.1.0
- parsel: 1.6.0
- w3lib: 1.22.0
- Twisted: 20.3.0
- Python: 3.7.4 (default, Sep  4 2019, 15:20:53) - [Clang 10.0.0 (clang-1000.10.44.4)]
- pyOpenSSL: 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)
- cryptography: 2.9.2
- Platform: Darwin-19.4.0-x86_64-i386-64bit

### Additional Context
We are using a custom `GoogleCloudFeedStorage` class, which takes some time to store the data. During the upload process, the next iteration of the for loop inside `FeedExporter.close_spider` is triggered, creating a new `log_args` object. This results in the use of the last `log_args` object in the closure, leading to duplicate log entries.