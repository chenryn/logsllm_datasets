### Description
When I have multiple feeds and a blocking feed storage, I get duplicated feed
logs.
### Steps to Reproduce
  1. Create a `test.py` file with this spider and settings
  2. `scrapy runspider test.py -L INFO`
**Expected behavior:**
    2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored json feed (10 items) in: gs://bucket/output.json
    2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: gs://bucket/output.csv
**Actual behavior:**
    2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: gs://bucket/output.csv
    2020-06-09 19:23:03 [scrapy.extensions.feedexport] INFO: Stored csv feed (10 items) in: gs://bucket/output.csv
### Versions
    Scrapy       : 2.1.0
    lxml         : 4.5.1.0
    libxml2      : 2.9.10
    cssselect    : 1.1.0
    parsel       : 1.6.0
    w3lib        : 1.22.0
    Twisted      : 20.3.0
    Python       : 3.7.4 (default, Sep  4 2019, 15:20:53) - [Clang 10.0.0 (clang-1000.10.44.4)]
    pyOpenSSL    : 19.1.0 (OpenSSL 1.1.1g  21 Apr 2020)
    cryptography : 2.9.2
    Platform     : Darwin-19.4.0-x86_64-i386-64bit
### Additional context
We use custom GoogleCloudFeedStorage and it takes some time to store the data.
While it's doing uploading, next iteration of a for loop inside
`FeedExporter.close_spider` comes and creates new `log_args` object, although
the last one is used in closure.