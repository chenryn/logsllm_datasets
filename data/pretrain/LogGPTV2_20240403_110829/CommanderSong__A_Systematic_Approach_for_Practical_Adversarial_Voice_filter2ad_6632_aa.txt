title:CommanderSong: A Systematic Approach for Practical Adversarial Voice
Recognition
author:Xuejing Yuan and
Yuxuan Chen and
Yue Zhao and
Yunhui Long and
Xiaokang Liu and
Kai Chen and
Shengzhi Zhang and
Heqing Huang and
Xiaofeng Wang and
Carl A. Gunter
CommanderSong: A Systematic Approach  
for Practical Adversarial Voice Recognition
Xuejing Yuan, SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, School 
of Cyber Security, University of Chinese Academy of Sciences; Yuxuan Chen, Florida Institute of 
Technology; Yue Zhao, SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, 
School of Cyber Security, University of Chinese Academy of Sciences; Yunhui Long, University 
of Illinois at Urbana-Champaign; Xiaokang Liu and Kai Chen, SKLOIS, Institute of Information 
Engineering, Chinese Academy of Sciences, School of Cyber Security, University of Chinese Academy 
of Sciences; Shengzhi Zhang, Florida Institute of Technology, Department of Computer Science, 
Metropolitan College, Boston University, USA; Heqing Huang, unaffiliated; Xiaofeng Wang, Indiana 
University Bloomington; Carl A. Gunter, University of Illinois at Urbana-Champaign
https://www.usenix.org/conference/usenixsecurity18/presentation/yuan-xuejing
This paper is included in the Proceedings of the 
27th USENIX Security Symposium.
August 15–17, 2018 • Baltimore, MD, USA
ISBN 978-1-939133-04-5
Open access to the Proceedings of the 
27th USENIX Security Symposium 
is sponsored by USENIX.
CommanderSong: A Systematic Approach for Practical Adversarial Voice
Recognition
Xuejing Yuan1,2, Yuxuan Chen3, Yue Zhao1,2, Yunhui Long4, Xiaokang Liu1,2, Kai Chen∗1,2, Shengzhi Zhang3,5,
Heqing Huang , XiaoFeng Wang6, and Carl A. Gunter4
1SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China
2School of Cyber Security, University of Chinese Academy of Sciences, China
3Department of Computer Science, Florida Institute of Technology, USA
4Department of Computer Science, University of Illinois at Urbana-Champaign, USA
5Department of Computer Science, Metropolitan College, Boston University, USA
6School of Informatics and Computing, Indiana University Bloomington, USA
Abstract
The popularity of automatic speech recognition (ASR)
systems, like Google Assistant, Cortana, brings in secu-
rity concerns, as demonstrated by recent attacks. The
impacts of such threats, however, are less clear, since they
are either less stealthy (producing noise-like voice com-
mands) or requiring the physical presence of an attack
device (using ultrasound speakers or transducers). In this
paper, we demonstrate that not only are more practical and
surreptitious attacks feasible but they can even be auto-
matically constructed. Speciﬁcally, we ﬁnd that the voice
commands can be stealthily embedded into songs, which,
when played, can effectively control the target system
through ASR without being noticed. For this purpose, we
developed novel techniques that address a key technical
challenge: integrating the commands into a song in a way
that can be effectively recognized by ASR through the
air, in the presence of background noise, while not being
detected by a human listener. Our research shows that this
can be done automatically against real world ASR applica-
tions1. We also demonstrate that such CommanderSongs
can be spread through Internet (e.g., YouTube) and radio,
potentially affecting millions of ASR users. Finally we
present mitigation techniques that defend existing ASR
systems against such threat.
1
Introduction
Intelligent voice control (IVC) has been widely used in
human-computer interaction, such as Amazon Alexa [1],
Google Assistant [6], Apple Siri [3], Microsoft Cor-
tana [14] and iFLYTEK [11]. Running the state-of-
the-art ASR techniques, these systems can effectively
interpret natural voice commands and execute the cor-
responding operations such as unlocking the doors of
∗Corresponding author: PI:EMAIL
1Demos
uploaded
attacks
are
of
(https://sites.google.com/view/commandersong/)
on
the website
home or cars, making online purchase, sending mes-
sages, and etc. This has been made possible by recent
progress in machine learning, deep learning [31] in par-
ticular, which vastly improves the accuracy of speech
recognition. In the meantime, these deep learning tech-
niques are known to be vulnerable to adversarial perturba-
tions [37, 21, 27, 25, 20, 49, 28, 44]. Hence, it becomes
imperative to understand the security implications of the
ASR systems in the presence of such attacks.
Threats to ASR Prior research shows that carefully-
crafted perturbations, even a small amount, could cause a
machine learning classiﬁer to misbehave in an unexpected
way. Although such adversarial learning has been exten-
sively studied in image recognition, little has been done in
speech recognition, potentially due to the new challenge
in this domain: unlike adversarial images, which include
the perturbations of less noticeable background pixels,
changes to voice commands often introduce noise that a
modern ASR system is designed to ﬁlter out and therefore
cannot be easily misled.
Indeed, a recent attack on ASR utilizes noise-like hid-
den voice command [22], but the white box attack is
based on a traditional speech recognition system that uses
a Gaussian Mixture Model (GMM), not the DNN behind
today’s ASR systems. Another attack transmits inaudible
commands through ultrasonic sound [53], but it exploits
microphone hardware vulnerabilities instead of the weak-
nesses of the DNN. Moreover, an attack device, e.g., an
ultrasonic transducer or speaker, needs to be placed close
to the target ASR system. So far little success has been
reported in generating “adversarial sound” that practically
fools deep learning technique but remains inconspicu-
ous to human ears, and meanwhile allows it to be played
from the remote (e.g., through YouTube) to attack a large
number of ASR systems.
To ﬁnd practical adversarial sound, a few technical
challenges need to be addressed: (C1) the adversarial au-
dio sample is expected to be effective in a complicated,
real-world audible environment, in the presence of elec-
USENIX Association
27th USENIX Security Symposium    49
tronic noise from speaker and other noises; (C2) it should
be stealthy, unnoticeable to ordinary users; (C3) impactful
adversarial sound should be remotely deliverable and can
be played by popular devices from online sources, which
can affect a large number of IVC devices. All these chal-
lenges have been found in our research to be completely
addressable, indicating that the threat of audio adversarial
learning is indeed realistic.
CommanderSong. More speciﬁcally, in this paper, we
report a practical and systematic adversarial attack on
real world speech recognition systems. Our attack can
automatically embed a set of commands into a (randomly
selected) song, to spread to a large amount of audience
(addressing C3). This revised song, which we call Com-
manderSong, can sound completely normal to ordinary
users, but will be interpreted as commands by ASR, lead-
ing to the attacks on real-world IVC devices. To build
such an attack, we leverage an open source ASR sys-
tem Kaldi [13], which includes acoustic model and lan-
guage model. By carefully synthesizing the outputs of the
acoustic model from both the song and the given voice
command, we are able to generate the adversarial audio
with minimum perturbations through gradient descent, so
that the CommanderSong can be less noticeable to hu-
man users (addressing C2, named WTA attack). To make
such adversarial samples practical, our approach has been
designed to capture the electronic noise produced by dif-
ferent speakers, and integrate a generic noise model into
the algorithm for seeking adversarial samples (addressing
C1, called WAA attack).
In our experiment, we generated over 200 Comman-
derSongs that contain different commands, and attacked
Kaldi with an 100% success rate in a WTA attack and a
96% success rate in a WAA attack. Our evaluation further
demonstrates that such a CommanderSong can be used to
perform a black box attack on a mainstream ASR system
iFLYTEK2 [11] (neither source code nor model is avail-
able). iFLYTEK has been used as the voice input method
by many popular commercial apps, including WeChat (a
social app with 963 million users), Sina Weibo (another
social app with 530 million users), JD (an online shop-
ping app with 270 million users), etc. To demonstrate the
impact of our attack, we show that CommanderSong can
be spread through YouTube, which might impact millions
of users. To understand the human perception of the at-
tack, we conducted a user study3 on Amazon Mechanical
Turk [2]. Among over 200 participants, none of them
identiﬁed the commands inside our CommanderSongs.
We further developed the defense solutions against this
attack and demonstrated their effectiveness.
2We have reported this to iFLYTEK, and are waiting for their re-
sponses.
3The study is approved by the IRB.
Contributions. The contributions of this paper are sum-
marized as follows:
• Practical adversarial attack against ASR systems. We
designed and implemented the ﬁrst practical adversarial
attacks against ASR systems. Our attack is demonstrated
to be robust, working across air in the presence of en-
vironmental interferences, transferable, effective on a
black box commercial ASR system (i.e., iFLYTEK) and
remotely deliverable, potentially impacting millions of
users.
• Defense against CommanderSong. We design two ap-
proaches (audio turbulence and audio squeezing) to de-
fend against the attack, which proves to be effective by
our preliminary experiments.
Roadmap. The rest of the paper is organized as fol-
lows: Section 2 gives the background information of our
study. Section 3 provides motivation and overviews our
approach. In Section 4, we elaborate the design and imple-
mentation of CommanderSong. In Section 5, we present
the experimental results, with emphasis on the difference
between machine and human comprehension. Section 6
investigates deeper understanding on CommanderSongs.
Section 7 shows the defense of the CommanderSong at-
tack. Section 8 compares our work with prior studies and
Section 9 concludes the paper.
2 Background
In this section, we overview existing speech recognition
system, and discuss the recent advance on the attacks
against both image and speech recognition systems.
2.1 Speech Recognition
Automatic speech recognition is a technique that allows
machines to recognize/understand the semantics of hu-
man voice. Besides the commercial products like Amazon
Alexa, Google Assistant, Apple Siri, iFLYTEK, etc., there
are also open-source platforms such as Kaldi toolkit [13],
Carnegie Mellon University’s Sphinx toolkit [5], HTK
toolkit [9], etc. Figure 1 presents an overview of a typical
speech recognition system, with two major components:
feature extraction and decoding based on pre-trained mod-
els (e.g., acoustic models and language models).
After the raw audio is ampliﬁed and ﬁltered, acoustic
features need to be extracted from the preprocessed au-
dio signal. The features contained in the signal change
signiﬁcantly over time, so short-time analysis is used to
evaluate them periodically. Common acoustic feature
extraction algorithms include Mel-Frequency Cepstral
Coefﬁcients (MFCC) [40], Linear Predictive Coefﬁcient
(LPC) [34], Perceptual Linear Predictive (PLP) [30], etc.
Among them, MFCC is the most frequently used one in
50    27th USENIX Security Symposium
USENIX Association
voice
Text
Figure 1: Architecture of Automatic Speech Recognition System.
both open source toolkit and commercial products [42].
GMM can be used to analyze the property of the acous-
tic features. The extracted acoustic features are matched
against pre-trained acoustic models to obtain the likeli-
hood probability of phonemes. Hidden Markov Models
(HMM) are commonly used for statistical speech recogni-
tion. As GMM is limited to describe a non-linear mani-
fold of the data, Deep Neural Network-Hidden Markov
Model (DNN-HMM) has been widely used for speech
recognition in academic and industry community since
2012 [32].
Recently, end-to-end deep learning becomes used in
speech recognition systems.
It applies a large scale
dataset and uses CTC (Connectionist Temporal Classi-
ﬁcation) loss function to directly obtain the characters
rather than phoneme sequence. CTC locates the align-
ment of text transcripts with input speech using an all-
neural, sequence-to-sequence neural network. Traditional
speech recognition systems involve many engineered pro-
cessing stages, while CTC can supersede these processing
stages via deep learning [17]. The architecture of end-to-
end ASR systems always includes an encoder network
corresponding to the acoustic model and a decoder net-
work corresponding to the language model [47]. Deep-
Speech [17] and Wav2Letter [24] are popular open source
end-to-end speech recognition systems.
2.2 Existing Attacks against Image and
Speech Recognition Systems
Nowadays people are enjoying the convenience of in-
tegrating image and speech as new input methods into
mobile devices. Hence, the accuracy and dependability of
image and speech recognition pose critical impact on the
security of such devices. Intuitively, the adversaries can
compromise the integrity of the training data if they have
either physical or remote access to it. By either revising
existing data or inserting extra data in the training dataset,
the adversaries can certainly tamper the dependability of
the trained models [38].
When adversaries do not have access to the training
data, attacks are still possible. Recent research has been
done to deceive image recognition systems into making
wrong decision by slightly revising the input data. The
fundamental idea is to revise an image slightly to make
it “look” different from the views of human being and
machines. Depending on whether the adversary knows
the algorithms and parameters used in the recognition sys-
tems, there exist white box and black box attacks. Note
that the adversary always needs to be able to interact
with the target system to observe corresponding output
for any input, in both white and black box attacks. Early
researches [50, 48, 19] focus on the revision and gener-
ation of the digital image ﬁle, which is directly fed into
the image recognition systems. The state-of-the-art re-
searches [37, 21, 27] advance in terms of practicality by
printing the adversarial image and presenting it to a device
with image recognition functionality.
However, the success of the attack against image recog-
nition systems has not been ported to the speech recogni-
tion systems until very recently, due to the complexity of
the latter. The speech, a time-domain continuous signal,
contains much more features compared to the static im-
ages. Hidden voice command [22] launched both black
box (i.e., inverse MFCC) and white box (i.e., gradient de-
cent) attacks against speech recognition systems, and gen-
erated obfuscated commands to ASR systems. Though
seminal in attacking speech recognition systems, it is
also limited to make practical attacks. For instance, a
large amount of human effort is involved as feedback for
the black box approach, and the white box approach is
based on GMM-based acoustic models, which have been
replaced by DNN-based ones in most modern speech
recognition systems. The recent work DolphinAttack [53]
proposed a completely inaudible voice attack by modu-
lating commands on ultrasound carriers and leveraging
microphone vulnerabilities (i.e., the nonlinearity of the
microphones). As noted by the authors, such attack can be
eliminated by an enhanced microphone that can suppress
acoustic signals on ultrasound carrier, like iPhone 6 Plus.
3 Overview
In this section, we present the motivation of our work, and
overview the proposed approach to generate the practical
adversarial attack.
3.1 Motivation
Recently, adversarial attacks on image classiﬁcation have
been extensively studied [21, 27]. Results show that even
the state-of-the-art DNN-based classiﬁer can be fooled
by small perturbations added to the original image [37],
producing erroneous classiﬁcation results. However, the
USENIX Association
27th USENIX Security Symposium    51
impact of adversarial attacks on the most advanced speech
recognition systems, such as those integrating DNN mod-
els, has never been systematically studied. Hence, in this
paper, we investigated DNN-based speech recognition
systems, and explored adversarial attacks against them.
Researches show that commands can be transmitted to
IVC devices through inaudible ultrasonic sound [53] and
noises [22]. Even though the existing works against ASR
systems are seminal, they are limited in some aspects.
Speciﬁcally, ultrasonic sound can be defeated by using
a low-pass ﬁlter (LPF) or analyzing the signal frequency
range, and noises are easy to be noticed by users.
Therefore, the research in this paper is motivated by
the following questions: (Q1) Is it possible to build the
practical adversarial attack against ASR systems, given
the facts that the most ASR systems are becoming more
intelligent (e.g., by integrating DNN models) and that the
generated adversarial samples should work in the very
complicated physical environment, e.g., electronic noise
from speaker, background noise, etc.? (Q2) Is it feasible
to generate the adversarial samples (including the target
commands) that are difﬁcult, or even impossible, to be
noticed by ordinary users, so the control over the ASR
systems can happen in a “hidden” fashion? (Q3) If such
adversarial audio samples can be produced, is it possible
to impact a large amount of victims in an automated way,
rather than solely relying on attackers to play the adver-
sarial audio and affecting victims nearby? Below, we will
detail how our attack is designed to address the above
questions.
3.2 The Philosophy of Designing Our At-