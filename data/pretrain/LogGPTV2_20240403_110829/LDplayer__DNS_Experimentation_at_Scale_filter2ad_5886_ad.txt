141 M
725 k
91
3 k
9.7 k
10 k
10 k
10 k
53 M
20 k
3.6 k
36 k
360 k
3.6 M
36 M
Table 1: DNS traces used in experiments and evaluation.
Mean and standard deviation of inter-arrival time for B-
Root and Rec traces.
Figure 7: Cumulative distribution of the inter-arrival time
of original and replayed traces.
Figure 6: Query timing difference between replayed and
original traces. Figure shows quartiles, minimum and maxi-
mum. The empty circles on x-axis exceed ± 20 ms (outliers).
Figure 8: Query rate differences between replayed and origi-
nal B-Root trace (5 trials). Black solid circles on the edge are
a few cases out of ±2%.
the queries within that domain. We repeat each type of trace replay
for 5 times to avoid outliers.
Query time: We use unique query names to identify the same
queries in original and replayed traces, and study the timing of each
query: the absolute time difference compared to the first query. We
ignore the first 20-seconds of the replay to avoid startup transients.
Figure 6 shows that timing differences in replay are tiny, usu-
ally quartiles are within ± 2.5 ms. We observe small, but noticeably
larger differences when the query interarrival is fixed at 0.1 s: ± 8 ms
quartiles. We are examining this case, but suggest it is an interac-
tion between application and kernel-level timers at this specific
timescale. Even when we look at minimum and maximum errors,
timing differences are small, within ± 17 ms.
ative timing with inter-arrival times.
Query Inter-arrival Time: We next shift from absolute to rel-
Figure 7 shows the CDF of experimental interarrival times for real
(B-Root-16) and synthetic traces of different interarrival rates. (Note
that timescale is shown on a logarithmic scale.) Interarrival is quite
close for traces with input inter-arrivals of 10 ms or more, and for
real-world traffic with varying interarrivals. We see larger variation
for very small, fixed interarrivals (less than 1 ms), although the
median is on target, there is some variation. This variation occurs
because it is hard to synchronize precisely at these fine timescales,
since the overhead from system calls to coordinate take nearly
as much time as the desired delay, adding a lot of jitter. We see
divergence for the smallest interarrivals for the real-world B-Root
trace, but little divergence for the 50% longest B-Root interarrivals.
Uneven spacing in real traces gives us fee time to synchronize. We
repeat this experiment for 5 times; all show similar results to the
one shown here.
Query Rate: We finally evaluate query rates. To do so, we replay
the B-Root-16 trace and compute the query rate in each second of
trace replay against the corresponding rate of that second in the
original trace. We repeat this test five times.
Figure 8 shows the CDF of the difference in these per-second
rates for all 3,600 seconds of each of the five replays. We observe
that almost all (4 trials with 98%-99% and 1 trial with 95%) of 3.6 k
data points (1-hour period) have tiny (± 0.1%) difference in average
query rate per second. This experiment uses the B-Root because it
has large query rate (median 38 k queries/s) and the rate varies over
time. We use a 1-second window to study overall replay rate; finer
(smaller) windows may show greater variation as OS-scheduling
variation becomes significant.
-20-15-10-505101520.0001.001.01.11B RoottraceSynthetic trace: query inter-arrival time (seconds)query time error (ms) in replay  0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.000001.00001.0001.001.01.11CDFquery inter-arrival time (seconds)dash lines: originaldots: replayedB Rootsynthetic0.1 mssynthetic1 mssynthetic10 mssynthetic100 mssynthetic1 s 0 0.2 0.4 0.6 0.8 1-2%-1%01%2%CDFquery rate (per second) diﬀerenceoriginal median query rate:38K q/sB root trace replayfor 5 times+0.1%-0.1%DNS Experimentation at Scale
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
We replay the B-Root-16 trace (Table 1) with a mix of different
key sizes and different portions of queries requiring DNSSEC, under
the previous experiment setup (§4.1).
Our new experiments of all queries with DNSSEC show that
going from 72% DO (as of mid-2016) to 100%, root response traffic
becomes 296 Mb/s (median) with 2048-bit ZSK in steady state (right
group in Figure 10). Compared to 225 Mb/s with current 72% DO
and 2048-bit ZSK, root response traffic could increase by 31% in
the future when all queries require DNSSEC. Our experiments also
demonstrate 32% traffic increase when DNS root ZSK was upgraded
to 2048-bit from 1024-bit keys, replicating experiments previously
done in a custom testbed [30]. As a future work, we could use
LDplayer to study the traffic under 4096-bit ZSK.
As future work, one could use this experiment to test LDplayer’s
predictive ability. For example, one could take 2016 data, adjust it
to 2018 DNSSEC levels, and see how well they match.
Figure 10: Bandwidth of responses under different DNSSEC
ZSK sizes. Trace: B-Root-16. Figures show medians, quar-
tiles, 5th and 95th percentiles.
Figure 11: CPU usage with different TCP timeouts under
minimal RTT (<1 ms). Trace: B-Root-17a. Figures show me-
dians, quartiles, 5th and 95th percentiles.
5.2 Performance of DNS over TCP and TLS at a
Root Server
We next use experiments to study DNS over TCP and TLS. Our goal
is to understand real-world resource usage at servers (memory and
Figure 9: The throughput of fast replay a continuous input
query stream over UDP directly: queries are sent immedi-
ately without timer events. Data point is sampled every two
seconds over total 5 minutes.
4.3 Single-Host Throughput
Having shown our query system is accurate to replay different
traces, we next evaluate the maximum throughput: how fast can
our system replay using a single host?
Methodology: We use an artificial query generator for con-
trolled, high-speed replay. We send a continuous stream of identical
queries (www.example.com) to the target, sending them with UDP,
without timeouts, to an authoritative server hosting example.com
zone with wildcards. We run our query replay system with one
distributor and six querier processes, along with the query genera-
tor (total 8 processes), on a single 4-core (8-hyperthread) host. We
monitor the packet rate and bandwidth after the query system is in
steady state.
Results: With this setup we replay 87 k queries/s (60 Mb/s), as
shown in Figure 9). This rate is more than twice of normal DNS
B-Root traffic rate (as of mid-2017). In this experiment the query
generator is the bottleneck (it completely saturates one CPU core),
while other processes (distributor and queriers) each consumes
about 50% of single CPU core. Higher rates would be possible with
faster query generation.
5 APPLICATIONS
With controlled, configurable and accurate trace replay, our system
provides a basis for large-scale DNS experimentation which can
produce new results and answer open research questions. We next
show such applications of LDplayer, including studying the impact
of increased DNSSEC queries and exploring the performance of
DNS over TCP and TLS at a Root DNS server.
5.1 Impact of Increased DNSSEC Queries
How does the root DNS traffic change when more and more applica-
tions start to use DNSSEC? We use LDplayer to enable DNSSEC for
all queries when we replay traces, allowing us to predict potential
future behavior. We start to answer this question and predict future
DNS root traffic. Prior studies used trace replay with current traffic
mixes [30].
020k40k60k80k100k050100150200250300 0 20 40 60 80 100query rate (q/s)bandwidth (Mbit/s)time (seconds) 0 50 100 150 200 250 300 350102420482048102420482048ZSK (bits):...normal...rollover72.3% queries with DO bit (current)...normal...rolloverAll queries with DO bitBandwidth of all responses (Mbit/s)  0 2 4 6 8 10 12 14 16 0 5 10 15 20 25 30 35 40overall percent of cpu usageTCP time-out window (seconds) at serveroriginal trace(3% queriesover TCP)all queriesover TCPall queriesover TLSIMC ’18, October 31-November 2, 2018, Boston, MA, USA
L. Zhu et al.
Figure 12: Network topology for experiments of replaying
Root DNS traces over TCP and TLS: controller (T), server (S),
and client instances (C)
CPU) and client latency. Our experiments here are the first to study
these topics at scale, with a full server implementation; prior work
used micro-benchmarks and modeling [33]. We convert all queries
to use TCP and TLS, demonstrating actual resource usage and also
revealing performance discontinuities in latency as a function of
RTT, which modeling cannot capture.
5.2.1 Experiment Setup and Methodology. To evaluate server re-
source requirements and query latency, we deploy a network topol-
ogy (Figure 12), separating control and experimental traffic. We
vary the client-to-server RTT for different experiments. All client
hosts use 16 GB RAM and 4-core (8-thread) 2.4 GHz Intel Xeon. To
support the all-TCP/TLS workload, we configure the authoritative
server with 64 GB RAM on a 24-core (48-thread) 2.2 GHz Intel Xeon,
and controller with 24 GB RAM on a 12-core (24-thread) 2.2 GHz
Intel Xeon. We run nsd-4.1.0 with 16 processes for all the exper-
iments, and a TLS-patched version [27] for TLS experiments. All
hosts run Ubuntu 16.04.2 LTS (64-bit) with 4.4.0-83-generic ker-
nel. Different server implementations may have different memory
requirements.
We conduct three types of query replay. First, we replay the
queries using the protocols in the original trace (3% TCP queries)
to establish a baseline for comparison. We then mutate the queries
so all employ TCP and TLS respectively for two different sets of
experiments. We vary either TCP timeouts (5 ms to 40 ms) at the
server, or the client-server RTTs (0 ms to 140 ms or based on a
distribution). In TCP and TLS experiments, we optimize TCP at
the client and server by enabling net.ipv4.tcp_low_latency in
Linux, and disable the Nagle algorithm [15] at the client.
We use two B-Root traces (Table 1) in the experiments in this
section. We first use 1-hour B-Root-17a trace to study server state
with controlled minimal RTT (<1 ms), verifying the experiment
reaches steady state in about 5 minutes. For later experiments we
use B-Root-17b, a 20-minute subset of the B-Root-17a trace.
We log server memory with top and ps, CPU with dstat, and
active TCP connections with netstat.
5.2.2 Memory and Connection Footprint. For DNS over TCP and
TLS, a server should keep idle connections open for some amount of
time, to amortize TCP connection and TLS session setup costs [18,
33]. However, a server cannot keep the connection open forever,
since maintaining concurrent connections costs server memory.
LDplayer provides the unique ability emulating and maintaining a
large number of concurrent connections for DNS that enables re-
playing queries over TCP and TLS at a root server, while most of the
previous DNS studies focus on UDP-dominated DNS. Server mem-
ory is an important metric to study to understand the constraints
of connection-oriented DNS.
Our experimental results confirm prior models [33] in a real-
world implementation, showing that even if all DNS were shifted
to connection oriented protocols, memory requirements are man-
ageable. Figures 13 and 14 show the memory and connections for
our experiments. We demonstrate that both the number of active
TCP connections and server memory consumption rise as the TCP
timeout increases. We show that with 20 s TCP timeout suggested
in prior work [33], our experimental server requires about 15 GB
RAM for TCP (Figure 13a) and 18 GB RAM for TLS (Figure 14a).
The server requires 180 k connections for TCP, one-third are ac-
tive (Figure 13c) and the rest in TIME_WAIT state (Figure 13b),
while TLS has similar connection requirement (Figure 14c and Fig-
ure 14b). These values are well within current commodity server
hardware, although much larger than today’s UDP-dominated DNS
(2 GB RAM, blue bottom line in Figure 13a). DNS operators with old
hardware will need to upgrade server when preparing for DNS over
TCP and TLS. Resource usage reaches steady state in about 5 min-
utes and is thereafter stable (approximately flat lines in Figure 13
and Figure 14).
Most of the extra memory cost is used for TCP connections. We
observe that server memory requirement increases significantly
(6× more) when shifting from UDP to TCP, while only increases
moderately (30% more) from TCP to TLS. Prior work [33] models
server memory without testing in a real-world implementation. One
possible way to reduce the memory requirement is using smaller
TCP read and write buffer in kernel, although future experiments
are needed.
We expected memory to vary depending on querier RTT, but the
memory does not change regardless of the distance from client to
server (figure is omitted due to length limit). This resource stability
is because memory is dominated by connection timeout duration,
which at 20 s is 200× longer than RTTs.
5.2.3 CPU Usage. LDplayer enables the first experimental evalua-
tion of CPU consumption of TCP and TLS; prior work was unable
to model CPU costs.
Figure 11 shows our evaluation of server CPU usage for DNS
over TCP and TLS. We observe that overall the CPU usage is about
5% (median) over 48 cores for all queries over TCP and 9% to 10%
(median) for all queries over TLS, again manageable on current
commodity server hardware. Results are stable regardless of the
connection timeout window (the flat lines). We observe a slightly
higher (2% more at median) CPU usage for TLS at 5 ms timeout,
likely due to more frequent connection timeout and setup.
In contrast, replaying original trace (3% TCP queries) requires
median 10% CPU, surprisingly higher (5% more at median) than
CPU usage of all queries over TCP. We are investigating the reason
for this surprisingly lower CPU usage in TCP. One possible is that
operating system and network stack might be highly optimized for
TCP. Another possible is the TCP optimizations built in network
interface card (Intel X710 40G in our experimental server), such
as TCP offload engine and TCP segmentation offload. These TCP
optimizations may help to reduce the server CPU usage, although
further investigation is needed.
STC1…IXPCn1Gb/s<1msIXP1Gb/s<1msDNS Experimentation at Scale
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
(a) Memory consumption.
(a) Memory consumption.
(b) Established TCP connections.
(b) Established TCP connections.
(c) TCP connections in TIME_WAIT state.
(c) TCP connections in TIME_WAIT state.
Figure 13: Evaluation of server memory and connections re-
quirement with different TCP timeouts and minimal RTT
(<1 ms). Trace: B-Root-17a. Protocol: TCP
Figure 14: Evaluation of server memory and connections re-
quirement with different TCP timeouts and minimal RTT
(<1 ms). Trace: B-Root-17a. Protocol: TLS
Our experiments confirm that connection tracking and cryptog-