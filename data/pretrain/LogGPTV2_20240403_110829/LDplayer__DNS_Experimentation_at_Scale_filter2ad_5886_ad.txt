### DNS Traces Used in Experiments and Evaluation

| Trace | Size (MB) |
|-------|-----------|
| B-Root-16 | 141 M |
| Rec-16 | 725 k |
| B-Root-17a | 91 |
| B-Root-17b | 3 k |
| B-Root-18 | 9.7 k |
| B-Root-19 | 10 k |
| B-Root-20 | 10 k |
| B-Root-21 | 10 k |
| B-Root-22 | 53 M |
| B-Root-23 | 20 k |
| B-Root-24 | 3.6 k |
| B-Root-25 | 36 k |
| B-Root-26 | 360 k |
| B-Root-27 | 3.6 M |
| B-Root-28 | 36 M |

**Table 1: DNS traces used in experiments and evaluation.**

### Inter-Arrival Time Analysis

**Figure 7: Cumulative distribution of the inter-arrival time of original and replayed traces.**

**Figure 6: Query timing difference between replayed and original traces.**
- The figure shows quartiles, minimum, and maximum.
- Empty circles on the x-axis represent outliers exceeding ± 20 ms.

### Query Timing and Repeatability

**Query Time:**
- We use unique query names to identify the same queries in both the original and replayed traces.
- We study the timing of each query by comparing the absolute time difference to the first query.
- The first 20 seconds of the replay are ignored to avoid startup transients.

**Results:**
- **Figure 6** shows that timing differences in the replay are minimal, with quartiles typically within ± 2.5 ms.
- For a fixed inter-arrival time of 0.1 s, the quartiles show slightly larger differences (± 8 ms). This is likely due to interactions between application and kernel-level timers at this specific timescale.
- Even the minimum and maximum errors remain small, within ± 17 ms.

### Inter-Arrival Time Distribution

**Figure 7:**
- Shows the cumulative distribution function (CDF) of experimental inter-arrival times for real (B-Root-16) and synthetic traces with different inter-arrival rates.
- The timescale is logarithmic.
- Inter-arrival times are close for traces with input inter-arrivals of 10 ms or more and for real-world traffic with varying inter-arrivals.
- Larger variations are observed for very small, fixed inter-arrivals (less than 1 ms), although the median remains on target.
- These variations occur because it is challenging to synchronize precisely at such fine timescales, as the overhead from system calls nearly equals the desired delay, introducing jitter.
- The experiment was repeated five times, all showing similar results.

### Query Rate Analysis

**Query Rate:**
- We evaluate the query rate by replaying the B-Root-16 trace and computing the query rate per second during the replay compared to the original trace.
- The test is repeated five times.

**Results:**
- **Figure 8** shows the CDF of the difference in per-second rates for all 3,600 seconds of each of the five replays.
- Almost all data points (4 trials with 98%-99% and 1 trial with 95%) have a tiny (± 0.1%) difference in the average query rate per second.
- The B-Root trace is used because it has a high query rate (median 38 k queries/s) and the rate varies over time.
- A 1-second window is used to study the overall replay rate; finer windows may show greater variation due to OS-scheduling effects.

### DNSSEC Impact on Traffic

**Experiment Setup:**
- We replay the B-Root-16 trace with a mix of different key sizes and portions of queries requiring DNSSEC, under the previous experiment setup (§4.1).

**Results:**
- When all queries require DNSSEC (100% DO), the root response traffic increases to 296 Mb/s (median) with 2048-bit ZSK in steady state.
- This is a 31% increase compared to the current 225 Mb/s with 72% DO and 2048-bit ZSK.
- Upgrading DNS root ZSK from 1024-bit to 2048-bit keys results in a 32% traffic increase, consistent with previous studies [30].
- Future work could include using LDplayer to study the traffic under 4096-bit ZSK.

**Figure 10: Bandwidth of responses under different DNSSEC ZSK sizes.**
- Trace: B-Root-16.
- Figures show medians, quartiles, 5th, and 95th percentiles.

### Performance of DNS over TCP and TLS at a Root Server

**Experiment Setup and Methodology:**
- We deploy a network topology (Figure 12) to separate control and experimental traffic.
- Client-to-server RTT is varied for different experiments.
- All client hosts use 16 GB RAM and 4-core (8-thread) 2.4 GHz Intel Xeon.
- The authoritative server is configured with 64 GB RAM on a 24-core (48-thread) 2.2 GHz Intel Xeon.
- The controller uses 24 GB RAM on a 12-core (24-thread) 2.2 GHz Intel Xeon.
- nsd-4.1.0 with 16 processes is used for all experiments, and a TLS-patched version [27] for TLS experiments.
- All hosts run Ubuntu 16.04.2 LTS (64-bit) with 4.4.0-83-generic kernel.

**Query Replay:**
- We replay the queries using the protocols in the original trace (3% TCP queries) to establish a baseline.
- We then mutate the queries so all employ TCP and TLS respectively for two different sets of experiments.
- TCP timeouts (5 ms to 40 ms) at the server and client-server RTTs (0 ms to 140 ms) are varied.
- TCP optimizations are enabled, including net.ipv4.tcp_low_latency and disabling the Nagle algorithm [15].

**Memory and Connection Footprint:**
- For DNS over TCP and TLS, the server must keep idle connections open to amortize TCP connection and TLS session setup costs.
- However, maintaining concurrent connections consumes server memory.
- Our experiments confirm that even if all DNS were shifted to connection-oriented protocols, memory requirements are manageable.
- With a 20 s TCP timeout, our experimental server requires about 15 GB RAM for TCP and 18 GB RAM for TLS.
- The server needs 180 k connections for TCP, with one-third active and the rest in TIME_WAIT state.
- These values are well within current commodity server hardware, though much larger than today's UDP-dominated DNS (2 GB RAM).

**CPU Usage:**
- **Figure 11** shows the evaluation of server CPU usage for DNS over TCP and TLS.
- Overall, the CPU usage is about 5% (median) over 48 cores for all queries over TCP and 9% to 10% (median) for all queries over TLS.
- Results are stable regardless of the connection timeout window.
- Surprisingly, replaying the original trace (3% TCP queries) requires a median 10% CPU, higher than the CPU usage for all queries over TCP.
- Further investigation is needed to understand this lower CPU usage in TCP, possibly due to OS and network stack optimizations or TCP offload engine and segmentation offload features in the network interface card.

**Figures 13 and 14:**
- Show the memory and connections requirement with different TCP timeouts and minimal RTT (<1 ms).
- Trace: B-Root-17a.
- Protocols: TCP and TLS.

### Conclusion

Our experiments confirm that connection tracking and cryptography can be managed within the constraints of current commodity server hardware, although some upgrades may be necessary for older systems. Further research is needed to optimize resource usage and understand the underlying mechanisms.