2, respectively, then V(σ2, p)=V(σ(cid:48)
σ(cid:48)
In other words, we prove in the first three lemmas that state
indistinguishability is preserved in each possible step of
execution due to CPU-local moves, then based on rely-guarantee
reasoning [33], show in the last lemma that state indistinguishability
must also be preserved due to query moves.
1, i.e., V(σ1, p)=V(σ(cid:48)
2, p).
2, p).
2, p).
The problem is that this information release does not preserve state
indistinguishability, even though it does not break confidentiality
since KServ cannot decrypt the private data. In general, what is
declassified is not known in advance; any page of a VM’s memory
may be declassified at any time, and the data may be declassified
for an arbitrary amount of time.
To address this problem, MicroV introduces data oracles
to model intentionally released information, such as encrypted
data. Instead of using the actual value of such information, the
value is specified by querying the data oracle. A data oracle is a
deterministic function that generates a sequence of integers based on
some machine state. We use a function based on a principal’s PDL,
guaranteeing that the value returned depends only on the state of
the principal reading the value. Each principal logically has its own
data oracle, ensuring that the returned value does not depend on the
behavior of other principals. For example, if the encrypted data is
masked using a data oracle and the next integer returned by KServ’s
data oracle is Ω, the value of the next shared, encrypted data read
by KServ will always be specified as Ω, whether the encrypted data
is B or B(cid:48). This way, intentional information release can be masked
by data oracles and will not break state indistinguishability:
2) Intentional information release: Because commodity systems
such as KVM may allow information release when explicitly
requested, to support KVM’s various virtualization features, we
must model this intentional information release and distinguish it
from unintentional data leakage. We call data that is intentionally
released non-private data.
Example 3 (Supporting declassification). To illustrate the chal-
lenge of supporting intentional information release, suppose VM m
grants KServ access to gfn 3 for performing network I/O. Since
KServ may copy gfn 3’s data to its private memory, gfn 3’s data
should be included in KServ’s private data lens. Private pages of
VM m (e.g., gfn 2) are handled the same as the previous example—
the content of gfn 2, whether b or b(cid:48), are not included in KServ’s
PDL, and do not affect state indistinguishability:
VM m may encrypt the data in gfn 2 and share it with KServ through
gfn 3, so that KServ can send the encrypted data via a backend par-
avirtual network I/O driver. Yet starting from two indistinguishable
states to KServ with different gfn 2 contents b and b(cid:48), writing en-
Data oracles are only used to mask reading non-private data,
decoupling the data read from the writer of the data. Integer
sequences returned by data oracles are purely logical, and can
yield any value; thus, our noninterference proofs account for all
possible values read from a data oracle. A data oracle is applied
dynamically, for example, masking a page of memory when it is
declassified, then no longer masking the page when it is used for
private data. Note that unintentional information leakage, such as
sharing unencrypted private data, is not modeled by data oracles
and will be detected since it will break state indistinguishability.
IV. RETROFITTING KVM INTO SEKVM
To use microverification, we observe that many parts of a hyper-
visor have little to do with security. If we can extract the core com-
ponents that enforce security and isolate them from the rest of the
system, we may focus our proof effort on the core while obtaining se-
curity guarantees for the entire system. Based on this observation, we
retrofit KVM/ARM into a small core, KCore, and a set of untrusted
services, KServ, according to the following desired security policies:
1787
Policy vm-memiso. KCore ensures that a VM’s private memory
cannot be accessed by KServ or other VMs. In its own private
memory, KCore tracks the ownership and sharing status of each
page. KCore ensures that any allocated page is owned by either itself,
KServ, or a VM. KCore manages stage 2 page tables, Arm’s NPTs,
for KServ and VMs to virtualize and restrict their memory access,
which in turn manage their own stage 1 page tables. By default,
KServ owns unused physical pages. KCore delegates complex
VM memory allocation to KServ, but validates its page allocation
proposals before performing the mapping itself, to ensure that the
page is not already owned by another VM or itself. KCore unmaps
the newly allocated page from KServ’s stage 2 page tables before
mapping the page to the VM. When a page is freed, KCore zeroes
out that page before allowing KServ to reallocate it, ensuring that
reclaimed memory does not leak VM data.
Policy vm-memdeviso. KCore leverages the System Memory
Management Unit (SMMU) [37], Arm’s IOMMU, to ensure that
a VM’s private memory cannot be accessed by devices assigned
to KServ or other VMs, including protecting against DMA attacks.
By taking full control of the SMMU, KCore ensures devices can
only access memory through the SMMU page tables it manages. It
uses the SMMU page tables to enforce memory isolation. KCore
validates all SMMU operations by only allowing the driver in KServ
to program the SMMU through Memory Mapped IO (MMIO)
accesses, which trap to KCore, and SMMU hypercalls. MMIO
accesses are trapped by unmapping the SMMU from KServ’s stage
2 page tables. SMMU hypercalls (1) allocate/deallocate an SMMU
translation unit, and its associated page tables, for a device, and (2)
map/unmap/walk the SMMU page tables for a given device. As part
of validating a KServ page allocation proposal for a VM, KCore
also ensures that the page being allocated is not mapped by any
SMMU page table for any device assigned to KServ or other VMs.
Policy vm-iodataiso. To avoid additional complexity in KCore,
we rely on KServ to support I/O virtualization. Similar to previous
work [7], [38], [39], we assume VMs employ end-to-end encryption
to protect I/O data against KServ.
Policy data-declassification. By default, KServ has no access to
VM data. KCore provides grant and revoke hypercalls that a
VM may use to voluntarily grant and revoke KServ access to the
VM’s pages. KCore also allows intentional information flow via a
VM’s general purpose registers (GPRs) to KServ to support MMIO.
V. VERIFYING SEKVM
We verify SeKVM by decomposing the KCore codebase into 34
security-preserving layers. From KCore’s modular implementation,
we craft its layer specification based on four layer design principles.
First, we introduce layers to simplify abstractions, when functional-
ity needed by lower layers is not needed by higher layers. Second, we
introduce layers to hide complexity, when low-level details are not
needed by higher layers. Third, we introduce layers to consolidate
functionality, so that such functionality only needs to be verified
once against its specification. For instance, by treating a module used
by other modules as its own separate layer, we do not have to redo
the proof of that module for all of the other modules, simplifying
verification. Fourth, we introduce layers to enforce invariants,
which are used to prove high-level properties. Introducing layers
modularizes verification, reducing proof effort and maintenance.
Fig. 5: Architecture of SeKVM.
Policy 1. vmdataiso:
• vm-cpuiso: a given VM’s data in CPU registers is isolated from
KServ and all other VMs.
• vm-memiso: a given VM’s data in private memory is isolated
from KServ and all other VMs.
• vm-memdeviso: a given VM’s data in private memory is isolated
from all devices assigned to KServ and all other VMs.
Policy 2. vm-iodataiso: the confidentiality and integrity of a given
VM’s I/O data is protected from KServ and all other VMs.
Policy 3. data-declassification: a given VM’s non-private data
may be intentionally released to support virtualization features.
Figure 5 shows the retrofitted KVM, SeKVM. KCore runs in Arm
VE’s higher-privilege hypervisor level, EL2. It implements the func-
tions that enforce access control or need direct access to VM data, as
well as the VM interface, which can logically be thought of as a set
of trap handlers. KServ runs at a lower-privilege kernel level, EL1. It
includes the Linux host OS and most of the KVM codebase. Just like
vanilla KVM, VMs run user-level code in EL0, the lowest privilege
level, and kernel code in EL1. Further details are described in [7].
When a VM raises an exception, it traps to KCore, which can
handle the exception directly or invoke KServ for more complex
functionality. Under the data-declassification policy, KServ
provides functionality that does not require access to private VM
data, such as resource allocation, bootstrapping and scheduling
VMs, and management features like snapshots and migration.
KCore isolates KServ’s access to VM resources to protect data
confidentiality and integrity. KServ does not have the privilege
to update VM data directly. Instead, KCore provides a restricted
hypercall interface to KServ for operations which require VM data
or EL2 privileges. The interface allows KServ to write updated
data to a memory page shared with KCore, which validates the
update before applying it to the real VM data. KCore also validates
cryptographic signatures of VM images and encrypts VM data
before it is exported for snapshots or migration.
Policy vm-cpuiso. When a VM is running on a CPU, its VCPU
registers are stored in CPU registers only accessible to the VM.
When a VM exits, and before running KServ, KCore saves the
VM’s VCPU registers in KCore’s private memory, inaccessible to
KServ or other VMs, and context switches the CPU so the VM’s
VCPU state is no longer available in the CPU registers. KCore may
share non-private data from VM general purpose registers to KServ
in accordance with the data-declassification policy.
1788
Hypercall Handlers:
VM management: register_vm, register_vcpu,
set_boot_info, remap_boot_image_page,
verify_vm_image, clear_vm, encrypt_vcpu,
decrypt_vcpu, encrypt_vm_mem, decrypt_vm_mem
Timer: set_timer
SMMU: smmu_alloc_unit, smmu_free_unit, smmu_map,
smmu_unmap, smmu_iova_to_phys
VM run: run_vcpu
VM-only: grant, revoke, psci_power
Exception Handlers:
Page Fault: host_page_fault, vm_page_fault
Interrupts: handle_irq
WFI/WFEs: handle_wfx
SysReg Access: handle_sysreg
Memory Operations:
mem_load, mem_store, dev_load, dev_store
TABLE I: TrapHandler interface. KServ calls VM management
hypercalls to boot and terminate VMs and obtain encrypted VM data
for migration and snapshots, the timer hypercall to set timers, SMMU
hypercalls to configure the SMMU, and the run_vcpu hypercall to run
a VCPU. VMs call VM-only hypercalls, not available to KServ, for power
management and to grant and revoke KServ access to VM (encrypted)
data for full KVM I/O support. Exception handlers handle stage 2 page
faults and other VM exceptions. Memory operations are not part of the
interface for KCore’s implementation, but are included in its specification
to logically model page-translated memory accesses issued by KServ and
VMs over our abstract hardware model.
We incrementally prove that each module of KCore’s
implementation transparently refines its layer specification, starting
from the bottom machine model until reaching the top layer,
then prove noninterference using the top layer specification. The
bottom machine model, AbsMachine, defines the machine state, an
instruction set, and a set of trusted primitives, such as cryptographic
library routines. The top layer specification, TrapHandler, is
the interface KCore exposes to its VMs and KServ. We prove
noninterference for any behavior of KServ and VMs interacting with
the TrapHandler interface, so that proven security properties hold
for the whole SeKVM system with any implementation of KServ.
A. Proving KCore Refines TrapHandler
Table I presents hypercalls and exception handlers provided by the
top-level interface TrapHandler; Appendix A has more details. All
proofs are developed and checked in Coq. We briefly describe some
of the refinement proofs, but omit details due to space constraints.
1) Synchronization: KCore’s spinlocks use an Arm assembly
lock implementation from Linux. This implementation uses acquire
and release barriers to prevent instruction reordering within the lock
routines, such that we can soundly verify that spinlocks correctly
enforce mutual exclusion in a similar manner to CertiKOS [5]. We
prove that all shared memory accesses in the code are correctly
protected by spinlocks. Because the barriers also prevent memory
accesses from being reordered beyond their critical sections, we
can easily show that KCore only exhibits sequentially consistent
behavior, such that our guarantees over KCore verified using a
sequentially consistent model still hold on Arm’s relaxed memory
model. The lock proofs required 4 layers.
2) VM management: KCore tracks the lifecycle of each VM
from boot until termination, maintaining per-VM metadata in a
VMInfo data structure. For example, KCore loads VM boot images
into protected memory and validates signed boot images using an
implementation of Ed25519. The implementation is verified by
porting the HACL* [14] verified crypto library to EL2. Because
no C standard library exists for EL2, this involved replacing the C
library function for memcpy used by HACL* with a standalone
memcpy implementation for EL2, which we verified. Similarly,
KCore provides crypto hypercalls using an implementation of AES,
also verified by using HACL*. Beyond HACL*, verifying VM
management required 4 layers.
3) VM CPU data protection: KCore saves VM CPU registers
to KCore memory in a VCPUContext data structure when a VM
exits, and restores them to run the VM. KCore does not make VM
CPU data directly available to KServ, and validates any updates
from KServ before writing them to the VM CPU data it maintains
in KCore memory. Many of the functions for saving, restoring, and
updating VM CPU data involving looping over registers to initialize,
copy, and process data. We verify these functions by first showing
that the loop body refines an atomic step using transparent trace
refinement, then use induction to prove that arbitrary many iterations
of the loop refine an atomic step. Finally, we use induction to prove
the loop condition is monotonic so that it will eventually terminate.
Verifying protection of the VM CPU data required 3 layers.
4) Multi-level paging with huge page support: KCore manages
its own stage 1 page table and a stage 2 page table per principal.
KCore assigns a dedicated page pool for allocating stage-2 page
tables to each principal, each capped with a page count quota,
ensuring each principal’s allocation cannot interfere with others’.
Like KVM, KCore’s page tables support multiprocessor VMs, 3-
and 4-level multi-level paging with dynamically allocated levels,
and huge pages to optimize paging performance; normal pages are
4KB and huge pages are 2MB.
Because page tables can be shared across multiple CPUs, a com-
mon feature of multiprocessor hypervisors, we use transparent trace
refinement to prove that their implementation refines their specifica-
tion (a logical map). Transparent trace refinement was essential for
verifying other layers that use stage 2 page tables, simplifying their
refinement by allowing the proofs to treat the primitives of the stage
2 page table map specification as atomic computation steps. Ap-
pendix B has more details. Using layers allowed most of the refine-
ment proofs to be written with the page table implementation details
abstracted away, making it possible to prove, for the first time, the
functional correctness of a system that supports shared page tables.
To prove that the multi-level page table implementation refines
the abstract map, we first show that the page table is a tree structure
with page table entries stored in the leaves, thereby guaranteeing
that multiple gfns cannot share the same page table entries. We then