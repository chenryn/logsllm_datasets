1100
1000
900
800
700
600
500
400
sliding window size: 10 
sliding window size: 20
2
3
4
5
6
7
8
9
10
Number of Available Replicas
l
d
e
t
c
e
e
S
s
a
c
i
l
p
e
R
f
o
r
e
b
m
u
N
e
g
a
r
e
v
A
10
8
6
4
2
0
80
100
(prob: 0.9, LUI: 4 secs)
(prob: 0.5, LUI: 4 secs)
(prob: 0.9, LUI: 2 secs)
(prob: 0.5, LUI: 2 secs)
e
r
u
l
i
a
F
g
n
m
T
i
i
f
o
y
t
i
l
i
b
a
b
o
r
P
d
e
v
r
e
s
b
O
200
220
140
180
120
Deadline (milliseconds)
160
0.3
0.25
0.2
0.15
0.1
0.05
0
80
100
(prob: 0.9, LUI: 4 secs)
(prob: 0.5, LUI: 4 secs)
(prob: 0.9, LUI: 2 secs)
(prob: 0.5, LUI: 2 secs)
140
180
120
Deadline (milliseconds)
160
200
220
Figure 3. Overhead of selection algorithm
6. Experimental Results
We now discuss the experiments we conducted to analyze
the performance of the probabilistic replica selection algo-
rithm, as implemented in AQuA. Our experimental setup
is composed of a set of uniprocessor Linux machines dis-
tributed over a 100 Mbps LAN. The processor speeds range
from 300 MHz to 1 GHz. All conﬁdence intervals for the re-
sults presented are at a 95% level, and have been computed
under the assumption that the number of timing failures fol-
lows a binomial distribution [4].
Figure 3 shows how the overhead of the probabilistic se-
lection algorithm varies with the number of available repli-
cas for sliding windows of sizes 10 and 20. We account
for these overheads when selecting the replicas. Computa-
tion of the response time distribution function contributes
to 90% of these overheads, while selection of the replica
subset using Algorithm 1 contributes to the remaining 10%.
The larger the sliding window, the greater the number of
data points used to compute the response time distribution,
and therefore the higher the selection overhead. We used a
sliding window of size 20 for the experiments we describe
below.
6.1. Validation of Probabilistic Model
We conducted experiments to validate the probabilistic
model by evaluating how effectively the subset of replicas
chosen by the probabilistic selection algorithm was able to
meet the QoS requested by the client. To do this, we used an
experimental setup composed of 10 server replicas, in addi-
tion to the sequencer. 4 of the server replicas were in the pri-
mary group, and the remaining ones were in the secondary
group. We simulated the background load on the servers by
having each replica respond to a request after a delay that
was normally distributed with a mean of 100 milliseconds
and a variance of 50 milliseconds. In our experiments, we
used two clients that ran on two different machines and in-
dependently issued requests to the replicated service with a
1000 millisecond request delay, which we deﬁne as the du-
ration that elapses before a client issues its next request after
completion of its previous request. In every run, each of the
two clients issued 1000 alternating write and read requests
to the service. One of the clients requested the same QoS
(a) Number of replicas se-
lected
(b) Timing failure probability
Figure 4. Adaptivity of probabilistic model
for all of the runs; the QoS included a staleness threshold
of value 4, a deadline of 200 milliseconds, and a minimum
probability of timely response of 0.1. The second client
speciﬁed a staleness threshold of value 2 in all of the runs,
but requested a different deadline in each run. To study the
behavior of the selection algorithm for different values of
the probability of timely response speciﬁed by a client, we
repeated the experiments for two different probability val-
ues speciﬁed by the second client in its QoS speciﬁcation:
0.9 and 0.5.
For each of the deadline values of the second client, we
experimentally computed the probability of timing failures
in a run by measuring the number of requests in the run for
which the second client failed to receive a response within
the requested deadline. Further, in order to study the ef-
fect of the staleness of the replicas on the timeliness of their
response, we repeated the experiments using different lazy
update intervals (LUI, also denoted by TL in Section 5.4.1).
Here we present the results for LUI of 2 seconds and 4 sec-
onds.
Figure 4a shows the average number of replicas selected
by the selection algorithm to service the second client for
each of its QoS speciﬁcations. From this ﬁgure we observe
that the number of replicas chosen by the algorithm to ser-
vice a request reduces as the client’s QoS speciﬁcation be-
comes less stringent. The reason for this is that our algo-
rithm, as outlined in Section 5.3, never selects more replicas
than are required in order to meet a client’s QoS require-
ment. The less stringent a client’s QoS speciﬁcation, the
higher the probability that a chosen replica will meet the
client’s speciﬁcation. Hence, as the QoS requirement be-
comes less stringent, the algorithm can satisfy the request
with fewer replicas.
Figure 4b shows how successful the replicas selected in
Figure 4a were in meeting the QoS speciﬁcations of the sec-
ond client. The ﬁrst observation from Figure 4 is that in
each case, the set of replicas selected by Algorithm 1 was
able to meet the client’s QoS requirements successfully by
maintaining the timing failure probability within the failure
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:23 UTC from IEEE Xplore.  Restrictions apply. 
probability acceptable to the client. For example, consider
the case in which the LUI is 4 seconds and the client has
speciﬁed that the probability of timely response must be at
least 0.9. The observed probability of timing failures in this
case varies from 0.1 to 0.02 as the deadline varies from 100
milliseconds to 200 milliseconds. We observe similar be-
havior for the other cases. Thus, for the experimental runs
we conducted, the model we used was able to predict the
set of replicas that would be able to return the appropriate
response by the client’s deadline, with at least the proba-
bility requested by the client. A second observation from
Figure 4b is that as the interval between lazy updates in-
creases, the observed probability of timely failures also in-
creases. The reason is that as the interval between lazy
updates increases, the replica’s state becomes increasingly
stale. That, in turn, increases the probability that a chosen
replica may have to defer its response until it has received
the next lazy update, in order to meet the client’s staleness
threshold. Thus, when the client speciﬁes a staleness thresh-
old that is much smaller than the lazy update interval, fewer
replicas are available to respond immediately to the client’s
request. This delayed response results in a higher probabil-
ity of timing failures.
7. Conclusions
We have presented a framework for providing tunable
consistency and timeliness at the middleware layer using
replication.
In addition to the experiments we have pre-
sented, we have also conducted other extensive experiments
by varying the different parameters, such as the lazy update
interval and request delay. All of the experimental results
we obtained show that our probabilistic approach can adapt
the selection of replicas to meet a client’s timeliness and
consistency constraints in the presence of delays and replica
failures, if enough replicas are available. Since we provide
probabilistic temporal guarantees, we currently admit all the
clients and inform a client if the observed failure probability
exceeds the client’s expectations after the failures have been
detected. However, with some modiﬁcations, we can also
use our framework to perform admission control, in order to
determine the clients that can be admitted based on the cur-
rent availability of the replicas. Finally, it is easy to extend
our framework so that the clients can replace the probability
of timely response with a higher-level speciﬁcation, such as
priority or the cost the client is willing to pay for timely de-
livery. The middleware can then internally map these higher
level inputs to an appropriate probability value and perform
adaptive replica selection, as described.
Acknowledgments: We thank the reviewers for their feed-
back. We thank Kaustubh Joshi for his feedback on the
probabilistic models. We are grateful to Jenny Applequist
for her comments.
References
[1] K. Birman. Building Secure and Reliable Network Applica-
tions. Manning, 1996.
[2] A. Demers, D. Greene, C. Hauser, W. Irish, and J. Larson.
Epidemic Algorithms for Replicated Database Maintenance.
In ACM Symp. on Principles of Distributed Computing, pages
1–12, 1987.
[3] M. Hayden. The Ensemble System. PhD thesis, Cornell Uni-
versity, January 1998.
[4] N. Johnson, S. Kotz, and A. Kemp. Univariate Discrete Dis-
tributions, chapter 3, pages 129–130. Addison-Wesley, sec-
ond edition, 1992.
[5] S. Krishnamurthy, W. H. Sanders, and M. Cukier. A Dynamic
Replica Selection Algorithm for Tolerating Timing Faults. In
Proc. of the International Conference on Dependable Sys-
tems and Networks, pages 107–116, July 2001.
[6] V. Krishnaswamy, M. Raynal, D. Bakken, and M. Ahamad.
Shared State Consistency for Time-Sensitive Distributed Ap-
plications.
In Proc. of the Intl. Conference on Distributed
Computing Systems, pages 606–614, April 2001.
[7] L. Lamport. Time, Clocks, and the Ordering of Events in Dis-
tributed Systems. Communications of the ACM, 21(7):558–
565, July 1978.
[8] L. Moser, P. Melliar-Smith, and P. Narasimhan. A Fault Tol-
erance Framework for CORBA.
In Proc. of the IEEE Intl.
Symp. on Fault-Tolerant Computing, pages 150–157, June
1999.
[9] K. Petersen, M. Spreitzer, D. Terry, M. Theimer, and A. De-
mers. Flexible Update Propagation for Weakly Consistent
Replication. In Proc. of the 16th ACM Symp. on Operating
Systems Principles, pages 288–301, October 1997.
[10] C. Pu and A. Leff. Replica Control in Distributed Systems:
An Asynchronous Approach. In Proc. of the ACM SIGMOD
Intl. Conference on Management of Data, pages 377–386,
May 1991.
[11] Y. (J.) Ren, T. Courtney, M. Cukier, C. Sabnis, W. H. Sanders,
M. Seri, D. A. Karr, P. Rubel, R. E. Schantz, and D. E.
Bakken. AQuA: An Adaptive Architecture that Provides De-
pendable Distributed Objects. IEEE Transactions on Com-
puters. To appear.
[12] P. Rubel. Passive Replication in the AQuA System. Master’s
thesis, University of Illinois at Urbana-Champaign, 2000.
[13] F. Torres-Rojas, M. Ahamad, and M. Raynal. Timed Consis-
tency for Shared Distributed Objects. In Proc. of the ACM
Symp. on Principles of Distributed Computing, pages 163–
172, May 1999.
[14] A. Vaysburd. Building Reliable Interoperable Distributed
Applications with Maestro Tools. PhD thesis, Cornell Uni-
versity, May 1998.
[15] H. Yu and A. Vahdat. Design and Evaluation of a Continuous
Consistency Model for Replicated Services. In Proc. of the
4th Symp. on Operating Systems Design and Implementation
(OSDI), October 2000.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:10:23 UTC from IEEE Xplore.  Restrictions apply.