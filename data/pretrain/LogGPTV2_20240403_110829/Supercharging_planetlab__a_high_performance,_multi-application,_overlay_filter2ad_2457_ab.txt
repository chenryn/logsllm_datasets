e
e
c
c
x
x
e
e
&
&
t
t
u
u
p
p
n
n
i
i
.
.
.
.
.
.
s
s
e
e
c
c
a
a
f
f
r
r
e
e
t
t
n
n
i
i
Figure 1. Generic application structure showing fast path 
(implemented on a network processor) and slow 
path on a general purpose server. 
blades. Section 6 describes the overall control architecture of the 
system and explains how it fits within the PlanetLab framework. 
The results of our evaluation are presented in Section 7, where we 
report on experiments with both an IPv4 router application and an 
implementation of a router for the Internet Indirection Infrastruc-
ture [ST02]. We finish with a short discussion of related work in 
Section  8,  some  alternative  approaches  in  Section  9  and  closing 
remarks in Section 10. 
2. SYSTEM OVERVIEW 
2.1. Objectives 
Our principal objective for the SPP is to enable PlanetLab applica-
tions  to  achieve  substantially  higher  levels  of  both  IO  perform-
ance  and  processing  performance,  while  making  it  reasonably 
straightforward for PlanetLab users to take advantage of the capa-
bilities offered by high performance components, such as network 
processor  subsystems.  We  also  require  that  legacy  PlanetLab 
applications run on the system without change. While unmodified 
applications will experience limited performance gains, the ability 
to support existing implementations can make it easier to migrate 
to higher performance implementations that take advantage of the 
network processor resources. 
To enable multiple PlanetLab applications to use the network 
processor resources concurrently, the system supports both multi-
ple  NP  subsystems  and  sharing  of  individual  subsystems.  Since 
modern  NPs  are  not  designed  to  be  shared,  this  creates  some 
challenges.  To  accommodate  the  limitations  of  the  NP  environ-
ment  and  to  simplify  the  porting  of  applications  to  the  NP,  we 
have chosen to provide support for the generic application struc-
ture shown in Figure 1. In this structure, applications are divided 
into  a  Fast  Path  (FP)  that  runs  on  an  NP  and  a  slow  path  for 
control  and  exception  processing  that  is  handled  by  a  separate 
Slice Manager (SM), running within a vServer [VS06] on a gen-
eral  purpose  compute  server.  The  SM  can  control  the  fast  path 
CP
CP
CP
e
e
e
n
n
n
L
L
L
i
i
i
d
d
d
r
r
r
a
a
a
C
C
C
h
h
h
c
c
c
t
t
t
i
i
i
w
w
w
S
S
S
GPE
GPE
GPE
...
...
...
NPE
NPE
NPE
...
...
...
Figure  2.  System  organization  showing  Control  Processor 
(CP), General Purpose Processing Engines (GPE) 
and Network Processing Engines (NPE); photo of 
current development platform 
through a generic control interface. A remote user can control the 
application by logging into the vServer hosting the SM. Slices can 
forward control messages “in-band” by sending them to the fast 
path,  which  inserts  them  in  the  appropriate  outgoing  queue,  or 
they can send them “out-of-band”. 
2.2. Node Abstraction 
The abstraction provided by the node seeks to mimic the Planet-
Lab node abstraction as closely as possible, while providing some 
additional features. The PlanetLab node abstraction seeks to give 
each vServer the illusion that it is running on a dedicated machine. 
The  illusion  is  imperfect,  because  practical  limits  on  IP  address 
availability force the different vServers to share the same IP ad-
dress, which in turn means that they must share common sets of 
TCP and UDP port numbers. PlanetLab also does not support any 
explicit “virtual link” concept. While slices may obtain a reserved 
allocation of network bandwidth, this allocation simply applies to 
the network interface bandwidth and does not imply any reserved 
capacity  from  node  to  node.  Typically,  PlanetLab  nodes  have  a 
single  network  interface,  so  the  bandwidth  reservation  simply 
gives the node the illusion of a dedicated network interface with a 
specified capacity. 
SPP nodes must operate under the same constraints and pro-
vide  the  same  abstraction  as  a  conventional  PlanetLab  node. 
However, since an SPP node can support multiple physical inter-
faces, we allow slices to reserve a share of each of the available 
interfaces. We also allow slices to associate multiple queues with 
each interface, and to divide their share of the interface bandwidth 
among their different queues. Through the fast path control inter-
face, an application may install filters that map packets to specific 
queues and output interfaces. The control interface also allows the 
application to specify each queue’s share of the outgoing interface 
and its capacity. 
As  mentioned  above,  PlanetLab  slices  running  in  the  same 
node share the same set of TCP and UDP port numbers. To enable 
remote users to send packets to a PlanetLab slice, users need to 
know  which  port  number  to  use  to  get the  packet  to the correct 
slice.  Since  nodes  are  shared,  slices  cannot  count  on  a  specific 
port  number  being  available  on  a  particular  node  and  Planetlab 
applications  must  be  prepared  to  cope  with  this.  Of  course,  this 
issue also arises in the SPP, but it is further complicated by the 
fact that an SPP node includes within it, multiple general purpose 
and NP subsystems. While the SPP node will often have multiple 
IP addresses (one for each of its physical interfaces), the number 
of IP addresses need not match the number of internal subsystems, 
so there can be no direct mapping. This means that not only must 
externally visible port numbers be shared among vServers within 
a  given  physical  server,  but  they  must  also  be  shared  among 
vServers in different physical servers and NP subsystems. We will 
elaborate on the implications of this in Section 6. 
2.3. System Components 
Figure 2 shows the main components of an SPP node. All input 
and output occurs through the Line Card (LC), which is an NP-
based subsystem with one or more physical interfaces (our current 
development  platform  has  10  gigabit  Ethernet  interfaces,  as 
shown in the diagram). The LC forwards each arriving packet to 
the system component configured to process it, and queues outgo-
ing  packets  for  transmission,  ensuring  that  each  slice  gets  the 
appropriate share of the network interface bandwidth. The archi-
tecture can support multiple LCs, but since the deployment con-
texts  for  PlanetLab  nodes  generally  constrains  the  available 
bandwidth, PlanetLab provides little motivation for systems with 
multiple  LCs.  The  General  Purpose  Processing  Engines  (GPE) 
are conventional dual processor server blades running the Planet-
Lab OS (currently Linux 2.6, with PlanetLab-specific extensions) 
and hosting vServers that serve application slices.  The Network 
Processing Engines (NPE) are NP subsystems comprising an Intel 
IXP  2850  NP,  with  17  internal  processor  cores,  3  banks  of 
SDRAM,  3  banks  of  QDR  SRAM  and  a  Ternary  Content  Ad-
dressable Memory (TCAM). The NPEs support fast path process-
ing for slices that elect to use this capability and each provides up 
to  5  Gb/s  of  IO  bandwidth.  There  are  two  NPE  subsystems  on 
each  physical  NP  blade.  The  Control  Processor  (CP)  is  another 
conventional server blade that hosts the software that coordinates 
the operation of the system as a whole. It can also host vServers 
serving application slices. The switch block is actually two sepa-
rate switches, a 10 Gigabit Ethernet switch for data, and a separate 
1 GE control switch. 
Figure  2  also  includes  a  photograph  of  the  current  develop-
ment platform for the system. This configuration includes a switch 
plus two NP blades; one implements the Line Card functions (the 
IO interfaces are at the rear of the chassis) and the other imple-
ments  two  NPEs.  The  configuration  also  includes  two  server 
blades,  (one  CP  and  one  GPE).  The  system  architecture  is  de-
signed to support larger configurations. In particular, these same 
components can be used in a 14 slot chassis, allowing for up to 12 
Loc. Mem.
Loc. Mem.
Loc. Mem.
C . . .T
C . . .T
C . . .T
C
C
C
T
T
T
d
d
d
a
a
a
e
e
e
r
r
r
h
h
h
t
t
t
s
s
s
t
t
t
x
x
x
e
e
e
t
t
t
n
n
n
o
o
o
c
c
c
ALU
ALU
ALU
E
E
E
M
M
M
E
E
E
M
M
M
. . .
. . .
E
E
E
M
M
M
input 
input 
MP
MP
MI 
MI 
output 
output 
PCI,GigE
PCI,GigE
SRAM DRAM
SRAM DRAM
Figure 3. IXP 2850 block diagram showing the 16 Micro-
Engines (ME) and the Management Processor (MP) 
conventional server blades or NP subsystems in a single Planet-
Lab node. Multi-chassis configurations are also possible. 
3. NETWORK PROCESSOR ISSUES 
To appreciate some of the NPE design issues it’s helpful to under-
stand a little bit about Network Processors and the Intel IXP 2850, 
in  particular  [IXP].  First,  NP  products  have  been  developed  for 
use  in  conventional  routers,  as  replacements  for  the  Application 
Specific Integrated Circuits (ASIC) that have typically been used 
to  provide  high  throughput  packet  processing.  Products,  like 
Cisco’s  CRS-1  use  proprietary  NPs  to  perform  all  the  line  card 
packet  processing  functions  [CI06],  and  the  IXP  family  of  net-
work  processors  is  used  in  a  wide  variety  of  products  made  by 
multiple  system  vendors.  Because  NPs  are  programmable,  they 
enable  more  rapid  development  and  more  rapid  correction  of 
design errors.   
To  enable  consistently  high  performance,  the  IXP  2850  is 
equipped with 16 multi-threaded Micro-Engines (ME) that do the 
bulk of the packet processing, plus several high bandwidth mem-
ory  interfaces  (see  Figure  3).  In  typical  applications  DRAM  is 
used  primarily  for  packet  buffers,  while  SRAM  is  used  for  im-
plementing  lookup  tables  and  linked  list  queues.  There  are  also 
special-purpose on-chip memory resources, both within the MEs 
and  shared  across  the  MEs.  An  xScale  Management  Processor 
(MP)  is  provided  for  overall  system  control.  The  MP  typically 
runs a general-purpose OS like Linux, and has direct access to all 
of system memory and direct control over the MEs. 
As  with  any  modern  processor,  the  primary  challenge  to 
achieving  high  performance  is  coping  with  the  large  proces-
sor/memory  latency  gap.  Retrieving  data  from  off-chip  memory 
can take 50-100 ns (or more), meaning that in the time it takes to 
retrieve a piece of data from memory, a processor can potentially 
execute over 100 instructions. The challenge for system designers 
is to try to ensure that the processor stays busy, in spite of this. 
Conventional  processors  cope  with  the  memory latency  gap  pri-
marily using caches. However for caches to be effective, applica-
tions  must  exhibit  locality  of  reference,  and  unfortunately, 
networking applications typically exhibit very limited locality of 
reference, with respect to their data. 
Since  caches  are  relatively  ineffective  for  networking  work-
loads, the IXP provides a different mechanism for coping with the 
memory latency gap, hardware multithreading. Each of the MEs 
has eight separate sets of processor registers (including Program 
Counter), which form the MEs hardware thread contexts. An ME 
can switch from one context to another in 2 clock cycles, allowing 
it to stay busy doing useful work, even when several of its hard-