### 1. 引言

图1展示了通用应用程序结构，其中快速路径（在网路处理器上实现）和慢速路径（在通用服务器上实现）。

### 2. 系统概述

#### 2.1 目标
SPP的主要目标是使PlanetLab应用程序能够显著提高I/O性能和处理性能，同时确保PlanetLab用户能够轻松利用高性能组件（如网络处理器子系统）提供的功能。我们还要求未经修改的PlanetLab应用程序可以在该系统上运行。尽管未修改的应用程序性能提升有限，但支持现有实现可以简化迁移到利用网络处理器资源的更高性能实现的过程。

为了使多个PlanetLab应用程序能够同时使用网络处理器资源，系统支持多个NP子系统以及单个子系统的共享。由于现代NPs不是为共享设计的，这带来了一些挑战。为了适应NP环境的限制并简化应用程序向NP的移植，我们选择支持图1所示的通用应用程序结构。在此结构中，应用程序被划分为在NP上运行的快速路径（FP）和由单独的Slice Manager（SM）处理的控制和异常处理慢速路径。SM在一个通用计算服务器上的vServer [VS06] 中运行，并通过一个通用控制接口控制快速路径。远程用户可以通过登录托管SM的vServer来控制应用程序。Slices可以通过将控制消息发送到快速路径（将其插入适当的出队列）或“带外”发送来进行控制。

#### 2.2 节点抽象
节点提供的抽象旨在尽可能接近PlanetLab节点抽象，同时提供一些额外的功能。PlanetLab节点抽象试图让每个vServer感觉自己在专用机器上运行。这种幻觉并不完美，因为IP地址可用性的实际限制迫使不同的vServer共享相同的IP地址，进而必须共享一组TCP和UDP端口号。PlanetLab也不支持任何显式的“虚拟链路”概念。虽然Slices可以获得保留的网络带宽分配，但这仅适用于网络接口带宽，并不意味着从节点到节点的保留容量。通常，PlanetLab节点只有一个网络接口，因此带宽预留只是给节点一种具有指定容量的专用网络接口的错觉。

SPP节点必须在相同的约束下操作，并提供与传统PlanetLab节点相同的抽象。然而，由于SPP节点可以支持多个物理接口，我们允许Slices预留每个可用接口的一部分。我们还允许Slices为每个接口关联多个队列，并将它们的接口带宽份额分配给不同的队列。通过快速路径控制接口，应用程序可以安装过滤器，将数据包映射到特定的队列和输出接口。控制接口还允许应用程序指定每个队列的出接口份额及其容量。

如上所述，在同一节点上运行的PlanetLab Slices共享相同的TCP和UDP端口号集。为了让远程用户能够向PlanetLab Slice发送数据包，用户需要知道要使用的端口号以将数据包发送到正确的Slice。由于节点是共享的，Slices不能依赖于特定节点上的特定端口号可用，因此PlanetLab应用程序必须准备好应对这种情况。当然，这个问题在SPP中也存在，但由于SPP节点包含多个通用目的和NP子系统，问题变得更加复杂。虽然SPP节点通常有多个IP地址（每个物理接口一个），但IP地址的数量不必与内部子系统的数量匹配，因此没有直接映射。这意味着不仅在给定物理服务器内的vServer之间必须共享外部可见的端口号，而且在不同物理服务器和NP子系统中的vServer之间也必须共享。我们将在第6节详细讨论这一问题的影响。

#### 2.3 系统组件
图2显示了SPP节点的主要组件。所有输入和输出都通过线路卡（LC）进行，这是一个基于NP的子系统，具有一个或多个物理接口（我们当前的开发平台有10千兆以太网接口，如图所示）。LC将每个到达的数据包转发到配置为处理它的系统组件，并排队待传输的出站数据包，确保每个Slice获得适当的网络接口带宽份额。架构可以支持多个LC，但由于PlanetLab节点部署环境通常限制了可用带宽，PlanetLab很少有多LC系统的需求。通用处理引擎（GPE）是运行PlanetLab OS（当前为Linux 2.6，带有PlanetLab特定扩展）并托管服务应用Slice的vServer的传统双处理器服务器刀片。网络处理引擎（NPE）是NP子系统，包括Intel IXP 2850 NP，具有17个内部处理器核心、3组SDRAM、3组QDR SRAM和三元内容可寻址存储器（TCAM）。NPE支持选择使用此功能的Slice的快速路径处理，每个NPE提供高达5 Gb/s的I/O带宽。每个物理NP刀片上有两个NPE子系统。控制处理器（CP）是另一个传统的服务器刀片，托管协调整个系统操作的软件。它也可以托管服务应用Slice的vServer。交换块实际上是两个独立的交换机：一个用于数据的10千兆以太网交换机和一个用于控制的1 GE交换机。

图2还包括当前开发平台的照片。该配置包括一个交换机和两个NP刀片；一个实现线路卡功能（IO接口位于机箱后部），另一个实现两个NPE。配置还包括两个服务器刀片（一个CP和一个GPE）。系统架构设计支持更大的配置。特别是，这些相同组件可以用于14插槽机箱，允许在一个PlanetLab节点中最多容纳12个常规服务器刀片或NP子系统。多机箱配置也是可能的。

### 3. 网络处理器问题
要理解NPE设计的一些问题，了解网络处理器（尤其是Intel IXP 2850 [IXP]）是有帮助的。首先，NP产品是为在传统路由器中使用而开发的，作为提供高吞吐量数据包处理的应用特定集成电路（ASIC）的替代品。例如，Cisco的CRS-1使用专有的NP来执行所有线路卡数据包处理功能[CI06]，IXP系列网络处理器被多家系统供应商广泛用于各种产品中。由于NP是可编程的，它们使得开发和纠正设计错误更加迅速。

为了实现一致的高性能，IXP 2850配备了16个多线程微引擎（ME），负责大部分数据包处理，还有几个高带宽内存接口（见图3）。在典型应用中，DRAM主要用于数据包缓冲区，而SRAM用于实现查找表和链表队列。还有一些特殊的片上内存资源，既在ME内也在ME之间共享。xScale管理处理器（MP）用于整体系统控制。MP通常运行像Linux这样的通用操作系统，并且可以直接访问所有系统内存并直接控制ME。

与任何现代处理器一样，实现高性能的主要挑战是应对巨大的处理器/内存延迟差距。从片外内存检索数据可能需要50-100纳秒（甚至更多），这意味着在从内存检索一块数据所需的时间内，处理器可以潜在地执行超过100条指令。系统设计者的挑战是尽量确保处理器保持忙碌状态。传统处理器主要通过缓存来应对内存延迟差距。然而，对于缓存有效，应用程序必须表现出局部性引用，不幸的是，网络应用程序通常在其数据方面表现出非常有限的局部性引用。

由于缓存对网络工作负载相对无效，IXP提供了另一种机制来应对内存延迟差距——硬件多线程。每个ME都有八组独立的处理器寄存器（包括程序计数器），形成ME的硬件线程上下文。ME可以在2个时钟周期内从一个上下文切换到另一个上下文，使其即使在多个硬件线程等待内存访问时也能保持忙碌并执行有用的工作。

希望这些改进能使您的文档更清晰、连贯和专业。如果有其他部分需要优化，请告诉我！