â—
â—
â—â—
â—â—
â—
â—â—
â—â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—â—
â—â—â—â—â—
â—â—â—â—â—
â—â—
â—
â—
â—
â—
â—
â—â—â—â—
â—â—â—â—â—â—â—
â—
â—â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—â—
â—
â—
â—â—â—â—â—
â—â—â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—â—
â—â—â—
â—
â—
â—
â—
â—
â—â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—â—
â—
â—
â—â—â—â—
â—
â—
â—
â—
â—â—â—
â—â—â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—â—â—â—â—
â—
â—â—
â—â—â—â—â—â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—â—â—â—â—â—â—â—â—â—â—â—â—â—
â—â—
â—
â—
â—
â—â—
â—â—
â—
â—
â—â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—â—
â—â—
â—
â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
â—
â—â—â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
#bugs
â—
â—
1
2
â—
â—
5
â—
20
10
â—
50
â—
â—
100
200
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—
â—â—
â—
1
10
100
1000
The xâˆ’th reported bug (logâˆ’scale)
(a) Probability that the ğ‘¥-th reported bug is a regression.
100%
75%
50%
25%
0%
1500
1000
i
n
o
s
s
e
r
g
e
r
a
s
i
g
u
b
h
t
âˆ’
x
e
h
t
t
a
h
t
y
t
i
l
i
b
a
b
o
r
P
d
e
t
r
o
p
e
r
s
a
w
g
u
b
t
s
r
i
f
e
h
t
i
e
c
n
s
s
y
a
d
.
o
N
500
0
%regression
0.7
0.8
0.9
1.0
#bugs
1
2
5
10
20
50
100
200
0
500
1000
1500
2000
The xâˆ’th reported bug
(b) Number of days between the first and ğ‘¥-th bug report.
Figure 1: Empirical investigation of bug reports in OSSFuzz.
Korea. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3460120.
3484596
1 INTRODUCTION
Greybox fuzzing has become one of the most successful methods
to discover security flaws in programs [3]. Well-known software
companies, such as Google [7] and Microsoft [2], are utilizing the
abundance of computational resources to amplify, by orders of mag-
nitude, the human resources that are available for vulnerability
discovery. For instance, within the OSSFuzz project, a small team
of Google employees is utilizing 100k machines and three grey-
box fuzzers (incl. AFL) to find bugs in more than 300 open-source
projects.
We analyzed the 23k fuzzer-generated bug reports from OSSFuzz
to understand how we can automatic vulnerability discovery even
more efficient. We found that most of the reported bugs are actually
introduced by changes to code that was working perfectly fine.
For the average project, almost four in five bug reports (77%) were
marked as regressions, i.e., a bug-introducing commit could be
identified. The majority of regressions were introduced five days or
more prior to being reported (2 months on average). This motivates
the need for faster regression fuzzing approaches.
Session 7B: Fuzzing CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2169bug related commit 1
bug related commit 2
p
r
o
g
r
a
m
s
t
a
t
e
m
e
n
t
s
DTD parsing changes
namespace related changes
allocating insufï¬cient memory
reading out of bounds
c
o
m
m
it 
h
i
s
t
o
r
y
all changes
ï¬les of an XML parser
Figure 2: Development of LibXML2. It is insufficient to focus
on each commit individually.
In OSSFuzz, the probability for a new bug report to be a regres-
sion increases from 20% for the first bug to 92% after a few hundred
bug reports. Why? Once onboarded, a project in OSSFuzz is con-
tinuously fuzzed. A few weeks after the onboarding, most of the
source code of the project remains unchanged. After many fuzzing
campaigns, most (non-regression) bugs in this code have now been
discovered. Only changes to the projectâ€™s source code can introduce
new bugs. In comparison, the code that has been changed since the
onboarding has also been fuzzed much less often.
So then, why should we continue to fuzz every piece of code
with equal priority? Can we somehow prioritize the fuzzing of
code that has changed more recently or more frequently to counter-
balance how often each piece of code has been fuzzed throughout
the projectâ€™s lifetime within OSSFuzz?
We could focus the fuzzer on a specific commit [5, 6, 17, 19, 33].
Figure 2 illustrates the challenges of this approach. Suppose, we
deploy the AFLGo directed fuzzer [6] to fuzz every commit during
the development of an XML parser library. In the past week, the
developer has been working on the proper handling of name spaces
and submitted several commits. In the week before, the developer
was working on the parser of Document Type Definitions (DTD).
With a strict focus on the given commit, bugs that are introduced
in untested commits or across several commits cannot be found.
Suppose, in an earlier commit, a new array was added but, due to
an integer overflow, insufficient memory was allocated. Because
the array is never accessed in this particular commit, AFLGo could
never find this bug. In one of the later commits, a memory access
is added. A fuzzer that also triggers the integer overflow and the
insufficient memory allocation in the earlier commit would also
discover a read-out-of-bounds. However, AFLGoÃbeing directed
only to the changes in this commitÃwould exercise the memory-
access but may not discover the out-of-bounds read. In fact, it would
be impractical if the developer ran AFLGo on every commit, thus
missing the bug entirely. It is also common to terminate a CI action
that takes more than one hour,1 while the current recommendation
for a reasonable fuzzing campaign length is 24 hours [16].
In this paper, we develop regression greybox fuzzing (RGF) and
propose to fuzz all commits simultaneously, but code present in more
(recent) commits with higher priority. An RGF focuses on code that
1https://docs.gitlab.com/ee/ci/pipelines/settings.html#timeout
is under development (i.e., the recent name space changes get more
priority than on the older DTD parser changes). Specifically, a RGFâ€™s
power schedule assigns more energy to seeds that execute code that
has changed more recently or more frequently. In vanilla greybox
fuzzing, the search process can be controlled using a power schedule,
which distributes energy across the seeds in the corpus. A seeds
with higher energy is also fuzzed more often.
Efficient fitness function. Like in directed greybox fuzzing,
we propose to conduct the heavy program analysis at compile
time to enable an efficient search at runtime. Unlike in directed
greybox fuzzing, every basic block (BB) becomes a target; only
the weight varies. Every BB is assigned a numerical weight that
measures how recently or how often it has been changed. This
information can be derived from a projectâ€™s versioning system. The
instrumentation also adds code to amplify and aggregate these
weights during execution. In our experiments, the instrumentation
overhead is negligible. At run time, the RGF collects the aggregated
fitness value for each executed input and normalizes it between the
minimum and maximum values. Using a simulated annealing-based
power schedule, the RGF maximizes the probability to generate
inputs with a higher normalized fitness value.
Amplifying weak signals. During our investigation, we no-
ticed that most BBs have never been changed (foundational code
[26]), and only a tiny proportion of BBs have been changed recently
(in the last commit) or frequently (one hundred times or more). If
we computed the average across BBs in an inputâ€™s execution trace,
the signal from the interesting (low-age, high-churn) BBs would be
very weak. Hence, we develop a methodology to amplify the signal
which involves the logarithm and the inverse.
Byte-level power schedule. During our experiments, we also
noticed that most bytes in any given seed have no or negligible
impact on the BBs of interest. We develop a lightweight technique
that learns a distribution over the bytes of a seed that describes
the degree of impact. We extend the concept of power schedules to
the assign energy to bytes of a seed. We develop a byte-level power
schedule based on Ant Colony Optimization [12] that assigns more
energy to bytes that generate more "interesting" inputs, and that
uses the alias method [37] for efficient weighted sampling.
Experiments. To investigate our hypothesis, we implemented
our technique into AFL [1] and conducted large-scale experiments
on the Fuzzbench fuzzer evaluation platform [20]. We call our tool
AFLChurn. For our experiments, we identified 20 regression bugs
in 15 open-source C programs using the OSSFuzz bug tracker. With
the kind and generous assistance of the Fuzzbench team, we con-
ducted 3+ CPU-years worth of fuzzing campaigns in an entirely
reproducible fashion.
Results. Our experiments demonstrate the validity of our main
hypothesis and the efficiency of RGF. AFLChurn discovers a re-
gression about 1.5x faster and in more campaigns than AFL. In one
case, AFLChurn reduces the time to produce the first crash from
17 to 9 hours. Investigating each heuristic individually, we found
that neither heuristic has a general edge over the other heuristic.
However, in particular cases one heuristic clearly outperforms the
other which motivates the combination of both heuristics. We also
found that the code that is part of a crashâ€™s stack trace often lives
in code that has been change more recently or more often.
Contributions. The main contributions of this work are:
Session 7B: Fuzzing CCS â€™21, November 15â€“19, 2021, Virtual Event, Republic of Korea2170â€¢ Empirical Motivation. We analyse 23k fuzzer-generated bug
reports in OSSFuzz [7] and identify regressions as a major
class of bugs. Once a project is well-fuzzed, most bugs are
found in the code that is currently under development. We
find no evidence that OSS Security improves over time.
â€¢ Technique. We propose regression greybox fuzzing which
fuzzes with higher priority code that has changed more re-
cently/often. We extend the concept of power schedule to
individual bytes in a seed and propose ACO [12] as suitable
search heuristic and the alias method for weighted sampling.
â€¢ Implementation and experiments. We conduct an evaluation
involving 20 bugs in 15 open-source C programs that were
available at OSSFuzz. We make our experiment infrastruc-
ture, implementation, data, and scripts publicly available.
2 EMPIRICAL STUDY: OSSFUZZ BUG REPORTS
We are interested in the prevalence and reporting rate of regres-
sion bugs among the fuzzer-generated bug reports in the OSSFuzz
continuous fuzzing platform [7]. In the past five years, OSSFuzz
has automatically discovered and reported 22,582 bugs in 376 open
source software (OSS) projects. OSS maintainers are welcome to
onboard their project at any time. Once onboarded, fully auto-
matically, bugs are reported, deduplicated, and the corresponding
bug-introducing commit (BIC) identified. To identify BIC, OSSFuzz
employs an efficient delta debugging approach [41]. We note that
we can only analyze bugs that are automatically discovered by the
available greybox fuzzers in the master branch of a OSS project.
We do not analyze project-specific bug reports for bugs that are
discovered by other means (e.g., by a manual security audit).
Methodology. We used the bug trackerâ€™s publicly accessible in-
terface to collect the data from all bug reports available on 30.12.2020.2
Each bug report also lists the OSS project, the report date, and the
date of the bug-introducing commit (if any). If a regression date
was available, we marked the corresponding bug as regression.
Format of Figure 1 (two scatter plots). We grouped the bug
reports by project and ordered them by report date. The x-axis
shows the rank of the bug report across all projects. The size of
each point illustrates the number of projects that have bug reports
with this rank. For instance, only 50 of the 376 projects have 100 bug