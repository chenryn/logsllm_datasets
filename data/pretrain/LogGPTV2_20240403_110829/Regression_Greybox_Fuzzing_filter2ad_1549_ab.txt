# Introduction

Greybox fuzzing has emerged as one of the most effective methods for discovering security vulnerabilities in software [3]. Leading technology companies, such as Google [7] and Microsoft [2], leverage extensive computational resources to significantly enhance the efficiency of vulnerability detection. For example, the OSSFuzz project, managed by a small team at Google, utilizes 100,000 machines and three greybox fuzzers (including AFL) to identify bugs in over 300 open-source projects.

To further optimize automated vulnerability discovery, we analyzed 23,000 fuzzer-generated bug reports from OSSFuzz. Our findings indicate that the majority of reported bugs are introduced by changes to previously stable code. On average, 77% of bug reports in a typical project were marked as regressions, with a bug-introducing commit identified. Most of these regressions were introduced at least five days before being reported, with an average delay of two months. This highlights the need for more efficient regression fuzzing techniques.

## Empirical Study: OSSFuzz Bug Reports

### Overview

OSSFuzz is a continuous fuzzing platform that has automatically discovered and reported 22,582 bugs in 376 open-source software (OSS) projects over the past five years. Project maintainers can onboard their projects at any time, and once onboarded, bugs are automatically reported, deduplicated, and the corresponding bug-introducing commit (BIC) is identified using an efficient delta debugging approach [41].

### Methodology

We collected data from all bug reports available on the OSSFuzz bug tracker as of December 30, 2020. Each report includes the OSS project, the report date, and the date of the BIC (if available). If a regression date was provided, the bug was marked as a regression.

### Analysis

#### Figure 1: Regression Probability and Reporting Time

- **(a) Probability that the x-th reported bug is a regression:**
  - The x-axis represents the rank of the bug report across all projects.
  - The y-axis shows the probability that the x-th reported bug is a regression.
  - The size of each point indicates the number of projects with bug reports of that rank.
  - Example: Only 50 out of 376 projects have 100 bug reports.

- **(b) Number of days between the first and x-th bug report:**
  - The x-axis represents the rank of the bug report.
  - The y-axis shows the number of days between the first and x-th bug report.

### Key Findings

- **Regression Probability:**
  - The probability of a new bug report being a regression increases from 20% for the first bug to 92% after a few hundred bug reports.
  - This trend is due to the continuous fuzzing of unchanged code, which eventually leads to the discovery of most non-regression bugs. New bugs are thus more likely to be introduced by recent or frequent changes.

- **Reporting Time:**
  - The average time between the first and x-th bug report varies, but most regressions are introduced at least five days before being reported, with an average delay of two months.

## Regression Greybox Fuzzing (RGF)

### Motivation

Given the high prevalence of regressions, it is inefficient to fuzz every piece of code with equal priority. We propose RGF, which prioritizes fuzzing code that has changed more recently or more frequently.

### Approach

- **Efficient Fitness Function:**
  - Conduct heavy program analysis at compile time to enable efficient runtime search.
  - Assign numerical weights to basic blocks (BBs) based on how recently or frequently they have been changed.
  - Use a simulated annealing-based power schedule to maximize the probability of generating inputs with higher normalized fitness values.

- **Amplifying Weak Signals:**
  - Most BBs have never been changed, while a small proportion have been changed recently or frequently.
  - Develop a methodology to amplify the signal from interesting BBs using logarithmic and inverse functions.

- **Byte-level Power Schedule:**
  - Learn a distribution over the bytes of a seed that describes the degree of impact.
  - Extend the concept of power schedules to assign energy to individual bytes.
  - Use Ant Colony Optimization (ACO) [12] and the alias method [37] for efficient weighted sampling.

### Experiments

- **Implementation:**
  - Integrated our technique into AFL [1] and conducted large-scale experiments on the Fuzzbench fuzzer evaluation platform [20].
  - Identified 20 regression bugs in 15 open-source C programs using the OSSFuzz bug tracker.
  - Conducted 3+ CPU-years worth of fuzzing campaigns in a fully reproducible manner.

- **Results:**
  - AFLChurn discovers regressions approximately 1.5 times faster than AFL.
  - In one case, AFLChurn reduced the time to produce the first crash from 17 to 9 hours.
  - Neither heuristic has a general edge over the other, but in specific cases, one clearly outperforms the other, motivating the combination of both heuristics.
  - Code involved in crashes often resides in recently or frequently changed code.

### Contributions

- **Empirical Motivation:**
  - Analyzed 23,000 fuzzer-generated bug reports in OSSFuzz and identified regressions as a major class of bugs.
  - Found no evidence that OSS security improves over time.

- **Technique:**
  - Proposed RGF, which prioritizes fuzzing code that has changed more recently or frequently.
  - Extended the concept of power schedules to individual bytes in a seed and proposed ACO and the alias method for weighted sampling.

- **Implementation and Experiments:**
  - Conducted an evaluation involving 20 bugs in 15 open-source C programs.
  - Made the experiment infrastructure, implementation, data, and scripts publicly available.

## Conclusion

Our study of OSSFuzz bug reports reveals that most reported bugs are regressions introduced by recent or frequent changes to code. To address this, we developed RGF, which prioritizes fuzzing of recently or frequently changed code. Our experiments demonstrate that RGF, implemented in AFLChurn, significantly improves the efficiency of regression bug detection.