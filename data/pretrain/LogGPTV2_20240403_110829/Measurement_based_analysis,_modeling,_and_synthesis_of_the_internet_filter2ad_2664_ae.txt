3
2
1
0
0
Measured
2X
4X
50X
t
e
g
a
n
e
c
r
e
P
r
e
t
s
u
C
l
t
s
e
g
r
a
L
400
600
800 1000 1200
Delay (ms)
(a)
Measured
2X
4X
50X
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
100
80
60
40
20
0
0
1
0.8
0.6
0.4
0.2
200
300
0
0
0.2
100
r (ms)
(d)
Measured
2X
4X
50X
100
200
Cutoff (ms)
(b)
300
y
t
i
l
i
b
a
b
o
r
P
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
Measured
2X
4X
50X
0.6
0.8
1
0.4
k/N
(e)
100
10−1
10−2
10−3
10−4
10−5
0
1
0.8
0.6
0.4
0.2
0
0
Measured
2X
4X
50X
10
20
30
In−degree
40
50
(c)
Measured
2X
4X
50X
20
40
60
80
Type 1 Violation Ratio (%)
(f)
100
Figure 13: DS2 vs measured data. (a) Delay distribution. (b) Clustering cutoff. (c) In-degree distribution. (d) Median B(2r)/B(r). (e)
D(k)/D(N). (f) Triangle inequality violation ratio distribution.
i
n
o
i
t
u
b
i
r
t
s
D
e
v
i
t
a
u
m
u
C
l
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10−1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
10−1
Measured
100
101
Penalty (ms)
102
103
Inet
100
101
Penalty (ms)
102
103
DS2 
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
i
n
o
i
t
u
b
i
r
t
s
D
e
v
i
t
a
u
m
u
C
l
0
Meridian−2k
10−1
Meridian−4k
Meridian−16k
Vivaldi−2k
Vivaldi−4k
Vivaldi−16k
1
Random−2k
0.9
Random−4k
0.8
Random−16k
0.7
n
o
i
t
i
u
b
i
r
t
s
D
e
v
i
t
l
a
u
m
u
C
100
101
Penalty (ms)
102
103
GT−ITM
0.6
0.5
0.4
0.3
0.2
0.1
0
10−1
100
101
Penalty (ms)
102
103
Figure 14: Performance comparison of three server selection
algorithms.
sample of the measured data; for both Inet and GT-ITM, we gener-
ate 2k, 4k and 16k data sets, respectively, using the same methodol-
ogy described in Section 2. In all server selection experiments, we
assume that there is only one service available in the network, and
all the nodes act as clients and servers simultaneously. Clients are
not allowed to select themselves as their servers. For each data set,
we run ﬁve experiments and the cumulative distributions of server
selection penalties are presented in Figure 14.
First of all, the synthesized 2k and 4k DS2 data sets yield virtu-
ally identical results as the 2k and 3997-node measured data, even
though they are synthesized from only 1k and 2k measured data
samples, respectively. Second, using the Inet delay model signif-
icantly underestimates the performance of Vivaldi. In fact, the re-
sults suggest that Vivaldi performs no better than random server se-
lection, while Vivaldi performs much better than random selection
if it is evaluated using the measured data or DS2 data. Thus, using
Inet as a delay model could lead to false conclusions about the per-
formance of Vivaldi. Third, although the relative performance rank
of the three algorithms is the same across all four delay models,
the absolute performance estimated with Inet and GT-ITM differs
dramatically from results achieved with the measured data or DS2
data. For example, on the 3997-node measured data, 40.4% of the
servers chosen by Meridian are within 1ms of the closest servers,
while this number is 17.2% and 50.4% on 4k Inet and 4k GT-ITM
data, respectively. Finally, the experiment based on the 16k DS2
synthetic data indicates that the performance of Vivaldi should al-
most remain constant under scaling, but this is not the case with
Inet and GT-ITM delay models. Similarly, Meridian’s performance
degrades more rapidly on Inet and GT-ITM than on DS2 data with
increasing network size. This illustrates that it is important to have
good delay space models that are beyond our ability to measure
since important performance trends sometimes only show at scale.
6.2 Structured Overlay Networks
Structured overlay networks like Chord, Kademlia and Pastry
use proximity neighbor selection (PNS) to choose overlay neigh-
bors [41, 23, 5]. PNS has been shown to effectively reduce routing
stretch, query delay and network load, and to increase overlay ro-
bustness to failures and even to certain security attacks [37].
Here, we show the importance of using a good delay space to
evaluate the effectiveness of PNS. To eliminate the inﬂuence of a
particular PNS implementation, we assume in our simulations that
the overlay chooses neighbors using perfect PNS, i.e., the closest
node is chosen among all overlay nodes that satisfy the structural
constraints imposed by a routing table entry. Unless otherwise
stated, the results in this section have been evaluated on a 4000
node overlay network using FreePastry [12], where the delay space
used was either based on measured data, DS2, Inet, or GT-ITM.
We ﬁrstly evaluate the following metrics: Overlay Indegree of
a node, which is the number of overlay nodes that have the node
in their routing tables. Hop Length Distribution of overlay route,
which determines the latency and network load of overlay lookups.
Route Convergence of overlay routes, which, given two nodes lo-
cated at distance d from each other, measures what fraction of their
overlay paths to a given destination is shared. This metric is impor-
tant for dynamic caching and for the efﬁciency of multicast distri-
bution trees.
Figure 15 shows that the results agree very well for the measured
delay data and DS2 data on all three metrics, while the results with
the Inet and GT-ITM models differ signiﬁcantly. The Inet model
yields different results on indegree, because the power-law connec-
tivity makes the closest leaf node of the high degree hubs in the
topology the nearest neighbor for a large number of nodes. For
the hop length distributions, we observe that the ﬁrst hop of over-
lay routes with the Inet model is signiﬁcantly larger than the ﬁrst
hop obtained with measured delay data. Finally, the route conver-
gence with Inet/GT-ITM is higher than with the measured data. The
deviations of these properties are rooted in the differences of the
D(k)/D(N) growth metric and the local clustering in-degree metric
among the delay models.
Next, we show how mis-predictions of the somewhat abstract
overlay metrics shown above can have an impact on application
performance metrics whose relevance is more immediately appar-
ent.
Effectiveness of PNS on Eclipse Attacks - In recent work on de-
fenses against Eclipse attacks [3] on structured overlay networks,
Singh et al. [37] argue that PNS alone is a weak defense. While
earlier work has shown that PNS is effective against Eclipse attacks
based on simulations with a GT-ITM delay model [15], Singh et al.
demonstrate that the defense breaks down when using measured
delay data as a basis for simulations. Moreover, they show that the
effectiveness of the defense diminishes with increasing size of the
overlay network. We have repeated their simulations using DS2
data and conﬁrmed that the results match the simulations with mea-
sured data. Moreover, we are able to show that the effectiveness of
PNS against Eclipse attacks continues to diminish as we increase
the network size beyond the scale of measured delay data. This is
another example of how the usage of inadequate delay models can
lead to wrong conclusions. It also shows that DS2 yields correct
results, and that it is important to be able to synthesize delay models
larger than measured data sets, in order to expose important trends.
Performance of Proactive Replication - The beneﬁts of proactive
replication in structured overlays to reduce overlay lookup hops
and latency has been explored by Beehive [28]. We experimented
with a simple prototype that does proactive replication based on the
number of desired replicas of an object. The placement of replicas
is biased towards nodes that share a longer preﬁx with the object
identiﬁer, in order to be able to intercept a large fraction of the
overlay routes towards the home node of the object. We ﬁrst eval-
uate the query lookup latency as a function of the total number of