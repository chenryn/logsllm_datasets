# Title: Intrusion Detection via Static Analysis
# Authors: David A. Wagner and Drew Dean

## Abstract
One of the primary challenges in intrusion detection is modeling typical application behavior to recognize attacks by their atypical effects without generating too many false alarms. We demonstrate how static analysis can be used to automatically derive a model of application behavior, resulting in a host-based intrusion detection system with three key advantages: a high degree of automation, protection against a broad class of attacks based on corrupted code, and the elimination of false alarms. We report on our experience with a prototype implementation of this technique.

## 1. Introduction
In the past five years, computer security has experienced a significant renaissance. Starting with Sun’s introduction of the Java language and its support for mobile code in 1995, programming languages have become a major focus of security research. Many papers have applied programming language theory to protection problems [25, 24], particularly in the area of information flow [17]. However, security is a multifaceted topic, and protection and information flow address only a subset of the challenges in building and deploying secure systems. As attackers and defenders engage in an arms race, relying solely on static but incomplete security measures is doomed to failure, as it gives the attacker the last move and, consequently, victory.

Formal methods alone are insufficient for building and deploying secure systems. Intrusion detection systems (IDS) have been developed to provide online auditing capabilities to alert the defender when something appears to be wrong. Unfortunately, most IDSs suffer from major problems, as described in Section 2. We take a new approach to the problem that eliminates many of these drawbacks.

Our approach constrains the system call trace of a program's execution to be consistent with the program's source code. We assume that the program was written with benign intent. This approach deals with attacks, such as buffer overflows, that cause a program to behave in a manner inconsistent with its author's intent. These are the most prevalent security problems. Some security issues, however, are directly attributable to faulty application logic, such as programs that fail to check authentication information before proceeding. One limitation of our intrusion detection system is that it does not detect attacks that exploit logic errors. In practice, application logic bugs are overshadowed by buffer overflow problems and other vulnerabilities that allow the execution of arbitrary machine code chosen by the attacker [8, 35]. Our focus is on the latter type of vulnerability.

The rest of this paper is organized as follows: Section 2 discusses related work, Section 3 presents our framework, Section 4 describes the models we use, Section 5 details our implementation, Section 6 evaluates our results, Section 7 discusses future work, and Section 8 concludes.

## 2. Related Work
Early work on intrusion detection was conducted by Anderson [1] and Denning [9]. Since then, it has become a very active field. Most intrusion detection systems (IDS) are based on one of two methodologies: they either generate a model of a program’s or system’s behavior from observing its behavior on known inputs (e.g., [14]), or they require the generation of a rule base (e.g., [3]). In both cases, these systems monitor the execution of the deployed program or system and raise an alarm if the execution diverges from the model. The current model-based approaches all share a common problem: a truly robust intrusion detection system must solve a special case of the machine learning problem, a classic AI problem. That is, to prevent false alarms, the IDS must be able to infer, from statistical data, whether the current execution of the system is valid or not. The false alarm rate of present systems is a major practical issue [2].

KO et al. and others have proposed a natural solution to this problem: every program should come with a specification of its intended behavior [21, 19, 22, 29]. This has been the dream of the formal methods community for 25 years, but it remains unrealized. Although KO et al.’s specification language is simple and admits relatively compact specifications, we believe that the need for manually written specifications will dramatically limit the impact of this work. We philosophically agree with the direction of KO et al.’s work, but we propose to side-step its main drawback by automatically deriving the specification from the program.

## 3. The Framework
We aim to detect cases where an application is penetrated and then exploited to harm other parts of the system. To achieve this, we define a specification of expected application behavior and monitor the actual behavior to see if it deviates from the specification. We first describe how we monitor application behavior and then propose techniques for automated specification construction.

To reduce the potentially huge volume of trace data, we consider only the security-relevant behavior of the application of interest. The monitoring strategy should ensure that a compromised application cannot compromise system integrity while evading detection. In general, it will always be possible for attackers to evade detection in our system if they do not cause any harm. However, if they want to cause harm, they will need to interact with the operating system in a way that risks detection.

In many practical cases, we may safely make the following assumption [15]:
- **Assumption**: A compromised application cannot cause much harm unless it interacts with the underlying operating system, and those interactions may be readily monitored.

If, as is typically the case, the only way to interact with the operating system is via system calls, it suffices to monitor just the application's system call trace. Since monitoring system call traces is usually straightforward, the main challenge will be to derive a specification of the application's expected interaction with the operating system.

We derive our specification of expected application behavior from the application source code, along with a fixed model of the operating system. We model the application as a transition system with some (possibly very large) set of states and admissible transitions. If we ever detect a system call trace that is incompatible with this transition system, we may conclude that the most likely explanation is that we are under attack. For instance, the adversary may have introduced malicious code of her own choosing and caused it to be executed, e.g., via a buffer overrun or format string attack. Therefore, to detect intrusions, our basic approach is to look for system call traces that could not have been generated by the underlying transition system.

One subtlety is that the adversary may adapt to our methods. Indeed, we later introduce a new type of attack, the mimicry attack, which applies to all intrusion detection systems and, in some cases, may allow the adversary to fool the intrusion detection system by camouflaging the malicious code so that it behaves much like the application would. We do not have a complete defense against mimicry attacks, but we make some progress towards quantifying resistance against this type of attacker tactic. See Section 6 for details.

Our intrusion detection system does not detect all attacks, but it does allow us to detect one of the most common effects of a penetration: the execution of corrupted code. We observe that, in practice, once an attacker has compromised the target application, she will often download some 'exploit code' of her choosing into the application and use it to execute various operations with the application's privileges. Since this exploit code is not originally present in the application source code, if it is ever executed, we expect to see behavior that is incompatible with the source code and thus to detect the attack.

One problem is that transition systems derived directly from the source are usually too complex to be useful. We could naively start a second 'slave' copy of the application running on the same inputs in an interpreter that simulates all interactions with the outside world, checking at every step whether we obtain the same system call trace from both the master and the slave. This naive replication strategy could probably be made to work, but it has two important disadvantages. First, replication may be hard to implement because it is likely to be very difficult in practice to remove every last shred of non-determinism from the application (e.g., random number generators, process scheduling, timing channels, interaction with the outside environment, etc.) [23]. Second, and more importantly, the slave is exposed to the same risks as the master: any set of inputs that triggers a security flaw in the master is likely to trigger the same flaw in the slave as well and thereby escape detection.

We tackle these problems by greatly simplifying the transition system, abstracting away unnecessary complexity. Since we care only about the sequence of system calls issued, we prune away all other aspects of the model, even to the point of disregarding the contents of local variables, data structures, and all other data flow. We then simulate the simplified transition system in an interpreter with correspondingly minimal operational semantics. This abstraction process has the potential to fix the problems of naive replication: it can be very fast because most of the code has been pruned away; we can afford to deal with non-determinism since the transition system has been drastically simplified (for instance, non-deterministic finite automata are not much more expensive to simulate than deterministic finite automata); and the minimal operational semantics may remove many of the pitfalls of C (e.g., buffer overrun attacks will not affect a model that ignores the contents of all buffers).

To summarize our approach: We first pre-compute a model of expected application behavior, built statically from program source code; then, we monitor the program and check its system call trace for compliance to the model at runtime. The primary challenge is in automating model generation, which we discuss next.

## 4. Models
In this section, we propose a sequence of models that we use to specify expected application behavior: first, a trivial model to illustrate the main idea; then, the callgraph model; third, a refinement, the abstract stack model; and finally, the low-overhead digraph model.

Each model is intended to satisfy a common soundness property: false alarms should never occur. To achieve this goal, we must make a number of mild assumptions about our operating environment. We consider only portable C code that has no implementation-defined behavior: for example, we assume there are no intentional array bounds violations, NULL-pointer dereferences, or other memory errors; we assume there is no function pointer arithmetic or type-casting between function pointers and other pointers; and we assume there is no application-defined runtime code generation. These assumptions are not critical: violations may introduce false alarms but will never cause us to miss attacks we otherwise would have detected. Nonetheless, in our experience, the security-critical applications in widespread use do conform to these assumptions.

From a formal language viewpoint, all of our models involve recognizing a sentence in a regular or context-free language. However, this viewpoint is less intuitive than dealing directly with automata and will not be discussed further. For ease of discussion, we will refer to terminating programs and finite or pushdown automata, as appropriate. All of our results directly extend to non-terminating programs.