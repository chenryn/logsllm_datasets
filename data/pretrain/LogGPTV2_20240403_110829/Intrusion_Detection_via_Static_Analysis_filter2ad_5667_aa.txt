title:Intrusion Detection via Static Analysis
author:David A. Wagner and
Drew Dean
Intrusion Detection via Static Analysis 
David Wagner 
U. C. Berkeley 
PI:EMAIL 
Drew Dean 
Xerox PARC 
PI:EMAIL 
Abstract 
One of  the primary  challenges in intrusion  detection  is 
modelling  typical application behavior, so that we can rec- 
ognize  attacks  by their atypical  effects without  raising  too 
many false  alarnis.  We show  how static  analysis may  be 
used to automatically  derive a model of application behav- 
ior:  The  result  is a  host-based intrusion detection  system 
with  three  advantages:  a  high degree of  automation, pro- 
tection against a broad class of attacks based on corrupted 
code,  and  the  elimination of false  alarnis.  We  report  on 
our experience with a prototype implementation of this tech- 
nique. 
1. Introduction 
Computer security has undergone a major renaissance in 
the last five years. Beginning with Sun’s introduction of the 
Java language and its support of mobile code in  1995, pro- 
gramming languages have  been  a major focus of  security 
research.  Many papers have been published  applying pro- 
gramming language theory to protection problems [25,24], 
especially  information  flow  [ 171.  Security,  however,  is a 
many-faceted topic. and protection and information flow ad- 
dress only a subset of the problems faced in building and de- 
ploying  secure systems.  As attackers  and defenders are in 
an arms race, deploying a system with strictly static but in- 
complete security measures is doomed to failure: this gives 
the attacker the last move, and therefore victory. 
Formal methods, alone, are insufficient for building and 
deploying secure systems. Intrusion detection systems have 
been  developed to provide an  online auditing  capability  to 
alert the defender that something appears to be wrong.  Un- 
fortunately,  most  intrusion  detection  systems suffer  from 
major  problems  as  described  in  Section  2.  We  take  a 
new approach to the problem that eliminates many of these 
drawbacks. 
Our approach  constrains the  system call  trace of  a pro- 
gram’s execution to be consistent with the program’s source 
code.  We  assume that  the  program was  written  with  be- 
nign intent. This approach deals with attacks (such as buffer 
overflows) that cause a program to behave in a manner in- 
consistent with its author’s intent. These are the most preva- 
lent security problems. Of course, some security problems 
are directly  attributable to faulty application  logic,  such as 
programs that  fail to check authentication information  be- 
fore proceeding, and one limitation  of  our intrusion detec- 
tion  system  is  that  it  does  not  detect  attacks that  exploit 
logic errors.  Application  logic bugs, however, are dwarfed 
in practice by buffer overflow problems and other vulnera- 
bilities  that  allow for execution  of  arbitrary  machine code 
of  the  attacker’s choice [8, 351, and  it is the  latter  type of 
vulnerability on which we focus. 
The rest of  this paper is organized as follows:  Section 2 
discusses related work, Section 3 discusses our framework, 
Section 4 discusses the models we use, Section 5 discusses 
our implementation, Section  6 evaluates  our results,  Sec- 
tion 7 discusses future work. and Section 8 concludes. 
2  Related Work 
Early  work  on  intrusion  detection  was  due  to  Ander- 
son  [I] and Denning [9].  Since then, it has become a very 
active  field.  Most  intrusion  detection  systems (IDS) are 
based  on  one of  two methodologies:  either they  generate 
a model of a program’s or system’s behavior from observ- 
ing its behavior on known inputs (e.g., [14]), or they require 
the generation of a rule base (e.g., [ 3 ] ) .  In both cases, these 
systems then monitor execution of the deployed program or 
system and raise an alarm if the execution diverges from the 
model.  The current model-based approaches all share one 
common problem: a truly robust intrusion detection system 
must solve a special case of the machine learning problem, a 
classic AI problem. That is, to prevent false alarms, the IDS 
must be able to infer, from statistical data, whether the cur- 
rent execution of the system is valid or not. The false alarm 
rate of present systems is a major problem in practice  [ 2 ] .  
1081-6011/01 $10.0002001 IEEE 
156 
KO et al., and  others have proposed a  very  natural  so- 
lution to this problem:  every program should come with a 
specification of its intended behavior [21, 19, 22, 291. This, 
of  course, has been the dream of the formal methods com- 
munity for 25 years, and is as yet unrealized. We believe it 
is likely to remain  unrealized  for some time  to come.  AI- 
though KO et al.’s specification language is simple and ad- 
mits  relatively compact specifications,  we  believe that the 
need  for manually  written  specifications  will  dramatically 
limit  the  impact of  this  work’.  We  philosophically  agree 
with  the  direction  of  KO et al.’s work,  but  we  propose  to 
side-step its main drawback by  automatically deriving the 
specification from the program. 
3. The framework 
We would like to detect the case where an application is 
penetrated and then exploited to harm other parts of the sys- 
tem.  To this end, we define a specification of expected ap- 
plication behavior, and then we monitor the actual behavior 
to see if  it ever deviates from the specification. We describe 
first how we monitor application behavior, and next we pro- 
pose techniques for automated specification construction. 
To reduce the potentially huge volume of trace data, we 
consider only the security-relevant behavior of  the applica- 
tion of interest. The monitoring strategy should then ensure 
that a compromised application cannot compromise system 
integrity’  while  still evading detection.  In  general,  it  will 
always be possible  for attackers  to evade detection  in  our 
system  if  they do not cause any  harm,  but  if  they  want  to 
cause  harm,  they  will  need  to  interact  with  the  operating 
system in a way which risks detection. 
In many cases of practical interest, we may safely make 
the following convenient assumption  [ 151: 
Assumption.  A  coniproniised  application  cannot  cause 
niiich harm unless it interacts with the underlying operating 
system, and those interactions may be readily monitored. 
If-as 
is  typically  the  case3-the  only  way  to  interact 
with  the  operating  system  is  via  system  calls,  it  suffices 
to monitor just  the  application’s  system  call  trace.  Since 
monitoring system call traces is usually  straightforward in 
’ However, one promising direction  to remedy these  limitations  can be 
found in  KO’s recent  work on blending manual  rule bases with automated 
specification  generation  [20].  Note  that  others have  used  runtime  tech- 
niques to identify program invariants [ 121; however, because the identified 
invariants  concern  dataflow. rather  than sequencing of  system calls,  they 
do not seem to be well-suited to intrusion detection. 
‘We  do not consider denial of service attacks in  this work. 
‘We  do not claim  that  the assumption  is always  true.  Some operating 
systems are starting  to  include  partial  exceptions to  this  rule  (e.g.,  user- 
level  networking).  However, few  security-critical  applications  use  these 
exceptional features, so we can simply forbid their use: the rare application 
which uses these features may introduce false alarms, but at least malicious 
code will not be able to exploit the special  features in an attack. 
practice,  the bulk of  the challenge will be to derive a spec- 
ification  of  the  application’s  expected  interaction  with  the 
operating system. 
We  derive our specification  of  expected application  be- 
havior from the application  source code, along with a fixed 
model  of  the operating  system.  We  model the  application 
as a transition system with some (possibly very large) set of 
states along with some admissible transitions. If we ever de- 
tect a system call trace that is incompatible with this transi- 
tion system, we may conclude that the most likely explana- 
tion is that we are under attack:  for instance, the adversary 
may  have introduced malicious code of  her  own choosing 
and caused  it  to be executed, e.g., via a buffer overrun  or 
format string attack. Therefore, to detect intrusions, our ba- 
sic approach is to look for system call traces that could not 
have been generated by the underlying transition system. 
One subtlety is that the adversary may adapt to our meth- 
ods.  Indeed, we  later  introduce a new  type  of  attack,  the 
mimicry attack, which applies to all intrusion detection sys- 
tems and in some cases may allow the adversary to fool the 
intrusion  detection system by  camouflaging  the  malicious 
code  so that  it  behaves  much  like  the  application  would. 
We do not have a complete defense against mimicry attacks, 
but we make some progress towards quantifying resistance 
against this type of attacker tactic. See Section 6 for details. 
Our  intrusion  detection  system  does  not  detect  all  at- 
tacks, but  it does allow  us to detect one of  the  most com- 
mon effects of a penetration:  execution of corrupted code. 
We  observe that, in practice, once an attacker has compro- 
mised the target application, she will often download some 
‘exploit code’ of her choosing into the application  and use 
it to execute various operations with the application’s privi- 
leges. Since this exploit code is not originally present in the 
application  source code, if it is ever executed we expect to 
see behavior that is incompatible with the source code and 
thus to detect the attack. 
One problem  is that transition  systems derived directly 
from the source are usually too complex to be  useful.  We 
could naively start a second  ‘slave’ copy of the application 
running on the same inputs in  an  interpreter that simulates 
all  interactions  with  the  outside world,  checking  at every 
step whether we obtain the same system call trace from both 
the  master  and  the  slave.  This  naive  replication  strategy 
could  probably be made to work, but it  has two  important 
disadvantages. First, replication may be hard to implement, 
because it is likely to be very difficult in practice to remove 
every  last  shred  of  non-determinism  from the  application 
(e.g., random number generators, process scheduling, tim- 
ing channels, interaction with the outside environment, etc.) 
[23]. Second, and more importantly, the slave is exposed to 
the same risks as the master:  any set of  inputs that tickles a 
security flaw in the master is likely to trigger the same flaw 
in the slave as well and thereby escape detection. 
157 
We  tackle  these problems by  simplifying the  transition 
system  greatly,  abstracting away  unnecessary  complexity. 
Since we care only  about the  sequence of system calls  is- 
sued, we  prune  away  all other aspects of the  model,  even 
to the point of disregarding the contents of local  variables, 
data structures, and all other data flow.  We  then  simulate 
the simplified transition  system  in  an  interpreter with  cor- 
respondingly minimal operational semantics.  This abstrac- 
tion process  has the  potential  to fix the  problems of naive 
replication:  it  can  be  very  fast,  because most of  the  code 
has  been  pruned  away;  we  can  afford  to  deal  with  non- 
determinism,  since  the  transition  system  has  been  drasti- 
cally  simplified  (for  instance,  non-deterministic  finite  au- 
tomata are not much more expensive to simulate than deter- 
ministic  finite  automata);  and the minimal operational  se- 
mantics may remove many of the pitfalls of C (e.g., buffer 
overrun attacks will not affect a model that ignores the con- 
tents of all buffers). 
To  summarize  our  approach:  We  first  pre-compute  a 
model  of  expected  application  behavior,  built  statically 
from program source code; then,  we monitor the program 
and check its system call trace for compliance to the model 
at runtime.  The primary  challenge is in  automating model 
generation, which we discuss next. 
4.  Models 
In this section, we propose a sequence of models that we 
use to specify expected application  behavior:  first, a trivial 
model to illustrate the main idea; then, the callgraph model; 
third, a refinement, the abstract stack model; and finally, the 
low-overhead digraph model. 
Each model is intended to satisfy a common soundness 
property:  false alarms should never occur.  To achieve this 
goal, we must make a number of mild  assumptions  about 
our  operating environment.  We  consider  only  portable  C 
code that has no implementation-defined behavior:  for ex- 
ample, we assume that there are no intentional array bounds 
violations,  NULL-pointer  dereferences,  or  other  memory 
errors; we  assume there  is  no  function  pointer arithmetic 
or type-casting between  function  pointers and other point- 
ers; and we assume there is no application-defined runtime 
code generation.  These assumptions are not critical:  viola- 
tions may introduce false alarms but will  never cause us to 
miss attacks we otherwise would  have detected.  Nonethe- 
less,  in  our experience  the  security-critical applications in 
widespread use do conform to these assumptions. 
From a formal language viewpoint, all of our models in- 
volve  recognizing  a  sentence  in  a  regular  or  context-free 
language.  However,  this  viewpoint is much  less  intuitive 
than  dealing directly  with  automata  and  will  not  be  dis- 
cussed  further.  For  ease  of  discussion,  we  will  refer  to 
terminating  programs  and  finite  or  pushdown  automata, 
as appropriate.  All  of  our  results directly  extend  to  non- 
terminating programs. 