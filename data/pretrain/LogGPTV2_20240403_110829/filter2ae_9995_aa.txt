近期我们对SSL协议、X.509证书进行研究，并使用python语言与Bro工具编写了辨别SSL证书真伪性的工具，并在其基础上进行效率优化。内容并不复杂，所以我也就简单的将我们的思路写到此处，方便做类似项目的同学进行参考。
## 前言介绍
本课题系统地研究SSL协议、X.509证书体系、中间人攻击原理和网络协议分析工具Bro，深入地调研中间人攻击案例和伪造SSL证书案例，旨在设计一种通过日志分析来检测伪造SSL证书的方法，能够通过分析Bro产生的SSL日志与X.509日志，发现证书伪造行为。
我们的设计思路大致如下：①获取访问流量包。②使用Bro工具将流量包解析出X.509日志。③通过数据库、pycurl包进行检测。④分析检测结果。
此项目重点在于将日志中的不同访问网页的证书信息进行分析，抓取到该网页的Issuer （颁发者）、Subject（主体）、Validity
（有效期）等。而网页中用来验证证书是否合法的方法如下：
  * 1、证书是否是信任的有效证书。所谓信任：浏览器内置了信任的根证书，就是看看web服务器的证书是不是这些信任根发的或者信任根的二级证书机构颁发的。所谓有效，就是看看web服务器证书是否在有效期，是否被吊销了。
  * 2、对方是不是上述证书的合法持有者。简单来说证明对方是否持有证书的对应私钥。验证方法两种，一种是对方签个名，我用证书验证签名；另外一种是用证书做个信封，看对方是否能解开。以上的所有验证，除了验证证书是否吊销需要和CA关联，其他都可以自己完成。验证正式是否吊销可以采用黑名单方式或者OCSP方式。黑名单就是定期从CA下载一个名单列表，里面有吊销的证书序列号，自己在本地比对一下就行。优点是效率高。缺点是不实时。OCSP是实时连接CA去验证，优点是实时，缺点是效率不高。
具体来说，现在存在的网站是如何验证证书是否是合法的呢？下面我引用下网友写的比较好的验证流程。
  1. 证书都是由CA组织下认可的根证书Root签发的（其中有两种形式，第一种是该组织有一个Root，每一家的Root Ca都需要其签名，该方案基于利益考量几乎没人采用，而是第二种方案，每家都有自己的Root CA 可以自签或者互相签名)。这个组织很难进，目前几乎完全由欧美控制,每年都有轮值主席负责该年CA组织工作，主要涉及到新的RFC审核和修改，新的CA申请和已有CA日志审核以及提出新的CA方案等。其他不通过该组织认证的证书签发者都是不安全的，此外该组织会对每年每个CA签发的证书进行审核。因此可以保证正常途径签发的证书根是绝对可信的。所有改组织通过的CA会强迫浏览器和系统安装(常见的厂商有VeriSign, Microsoft, Oracle和Molliza 这也是强制力的来源)
  2. 证书分为DV(Digital Verification)，OV(Organization Verification)和EV(Extended Verification)，其中EV证书最贵，可以在浏览器中看到绿色的就是EV证书。证书从申请到批准要走很久的流程，需要提供很多的公司认证信息和个人信息，否则是不会通过的。因此可以保证签发的证书内容是可信的。
  3. 证书是需要预装的，特别是根证书。IE和Chrome是通过内置在Windows系统中的TrustStore来管理根证书（当然自己也可以手动导入自签证书，浏览不会认可的因为有OCSP和CRL--之后细讲）；而Firefox则是内置在自己的浏览中。
  4. 综上，通俗的来说一个CA如果要商业化，要做以下几步：申请加入CA组织，然后向Microsoft提申请加入TrustStore(通过Windows自我更新或者通过其他证书导入时加入）和Mozilla组织申请加入Firefox TrustStore。
而证书的工作原理是如何的呢？
以访问
  1. 浏览器发现此为HTTPS请求，握手拿到google的证书，先从系统(Windows)或者浏览器内置（Firefox）检查证书链是否正确。
【补充】简略步骤如下
a. 客户端发送信息，带上支持的SSL或者TLS版本(注意不同浏览器版本支持不同)
b. 服务器返回确认使用的加密通信协议版本以及加密随机数和证书
c. 浏览器验证证书 -> OCSP或者CRL
结合自带truststore注意此处验证分为双向验证和单向验证，单向验证客户浏览器即可完成，即客户端truststore存放服务器public证书；双向验证客户浏览器需要带客户端证书到服务器端由服务器端验证，客户端truststore存放服务器端public证书，keystore存放自身private证书，服务器端truststore存放客户端public证书，keystore存放自身private证书。
  2. 如果验证失败则会拦截
  3. 之后浏览器会尝试查CRL(证书吊销列表)和OCSP(在线证书检查)，其中OCSP是前者的替代和新技术，这是由于CRL发布时间一般是7天(不过接到新通知要改为1天了）并且很大不方便。但是考虑到老浏览器只能用CRL，并且CRL可以缓存本地对于网速差情况还是有用的，此外Firefox虽然支持OCSP但是可以手动关闭也是CRL存在的原因。注意：CA不会直接暴露到外网的，如果需要访问CA服务器需要使用硬件Token并且多人在场录像，且只能远程访问。OCSP相当于证书数据库的备份而已是直接暴露在外网的可以通过HTTP或者HTTPS访问。
  4. 如果发现证书并没有被吊销或者过期则浏览器对EV证书会显示为绿色，对OV证书则是通过放行。否则弹出通知---该网站不可信(不同浏览器不同--Edge浏览器）
        而开始时我们也准备使用上述方法进行对工具的编写，可是我们发现我们并不能直接得到证书的公钥。而我们也得知非伪造的证书的subject是正常的，也就是说当我们拿到一个伪造证书时，可以curl一下此网页进行再访问，并获得subject。进行比对后我们就很清晰的得到其真伪性了。
下面我具体的交代一下此检验工具的编写思路。
## 系统编写思路
如上图所示，我们检测工具可由上述部分组成：
  * ①使用wireShark或者Bro工具获取流量包。
  * ②使用python脚本对所获取流量包进行日志解析以便我们能提取出所需要的ssl证书信息。
  * ③对证书进行详细分析（辨别真伪）并记录。
  * ④分析最终情况并得出所需结论。
下图展示ssl分析器的具体架构
对于日志解析器来说，它可以提取可用数据并进行解析。我们知道一个流量包中记录了访问记录的各种数据，而并不是所有的数据都可以为我所用。所以我只需提取出与项目相关的数据即可。  
由于日志解析器的内容相对较难编写，我将python代码公布于此。
    ''' This module handles the mechanics around easily pulling in Bro Log data
        The read_log method is a generator (in the python sense) for rows in a Bro log, 
        because of this, it's memory efficient and does not read the entire file into memory.
    '''
    import csv
    import datetime
    import optparse
    import itertools
    import os
    import sys
    import test_mysql
    class BroLogReader():
        ''' This class implements a python based Bro Log Reader. '''
        def __init__(self):
            ''' Init for BroLogReader. '''
            self._delimiter = '\t'
        def read_log(self, logfile, max_rows=None):
            ''' The read_log method is a generator for rows in a Bro log. 
                Usage: rows = my_bro_reader.read_log(logfile) 
                       for row in rows:
                           do something with row
                Because this method returns a generator, it's memory
                efficient and does not read the entire file in at once.
            '''
            # First parse the header of the bro log
            bro_fptr, field_names, field_types = self._parse_bro_header(logfile)
            # Note: The parse_bro_header method has advanced us to the first
            #       real data row, so we can use the normal csv reader.
            reader = csv.DictReader(bro_fptr, fieldnames=field_names,
                                    delimiter=self._delimiter, restval='BRO_STOP')
            for _row in itertools.islice(reader, 0, max_rows):
                values = self._cast_dict(_row)
                if (values):
                    yield values
        def _parse_bro_header(self, logfile):
            ''' This method tries to parse the Bro log header section.
                Note: My googling is failing me on the documentation on the format,
                      so just making a lot of assumptions and skipping some shit.
                Assumption 1: The delimeter is a tab.
                Assumption 2: Types are either time, string, int or float
                Assumption 3: The header is always ends with #fields and #types as
                              the last two lines.
                Format example:
                    #separator \x09
                    #set_separator  ,
                    #empty_field    (empty)
                    #unset_field    -                    #path   httpheader_recon
                    #fields ts  origin  useragent   header_events_json
                    #types  time    string  string  string
            '''
            # Open the logfile
            _file = open(logfile, 'rb')
            # Skip until you find the #fields line
            _line = next(_file)
            while (not _line.startswith('#fields')):
                _line = next(_file)
            # Read in the field names
            _field_names = _line.strip().split(self._delimiter)[
                           1:]  # [1:] represents record except #fields,self._delimiter is defined in __init__(self)
            # Read in the types
            _line = next(_file)
            _field_types = _line.strip().split(self._delimiter)[1:]
            # Return the header info
            return _file, _field_names, _field_types
        def _cast_dict(self, data_dict):
            ''' Internal method that makes sure any dictionary elements
                are properly cast into the correct types, instead of
                just treating everything like a string from the csv file
            '''