title:Trace Driven Analysis of the Long Term Evolution of Gnutella Peer-to-Peer
Traffic
author:William Acosta and
Surendar Chandra
Trace Driven Analysis of the Long Term
Evolution of Gnutella Peer-to-Peer Traﬃc(cid:2)
William Acosta and Surendar Chandra
University of Notre Dame, Notre Dame IN, 46556, USA
{wacosta,surendar}@cse.nd.edu
Abstract. Peer-to-Peer (P2P) applications, such as Gnutella, are evolv-
ing to address some of the observed performance issues. In this paper, we
analyze Gnutella behavior in 2003, 2005, and 2006. During this time, the
protocol evolved from v0.4 to v0.6 to address problems with overhead of
overlay maintenance and query traﬃc bandwidth. The goal of this paper
is to understand whether the newer protocols address the prior concerns.
We observe that the new architecture alleviated the bandwidth consump-
tion for low capacity peers while increasing the bandwidth consumption
at high capacity peers. We measured a decrease in incoming query rate.
However, highly connected ultra-peers must maintain many connections
to which they forward all queries thereby increasing the outgoing query
traﬃc. We also show that these changes have not signiﬁcantly improved
search performance. The eﬀective success rate experienced at a forward-
ing peer has only increased from 3.5% to 6.9%. Over 90% of queries
forwarded by a peer do not result in any query hits. With an average
query size of over 100 bytes and 30 neighbors for an ultra-peer, this
results in almost 1 GB of wasted bandwidth in a 24 hour session. We
outline solution approaches to solve this problem and make P2P systems
viable for a diverse range of applications.
1 Introduction
In recent years, Peer-to-Peer (P2P) systems have become popular platforms for
distributed and decentralized applications. P2P applications such as Gnutella[1],
Kazaa [5] and Overnet/eDonkey [6] are widely used and as such, their behavior
and performance have been studied widely [11,3,8]. Gnutella, although popular,
has been shown to suﬀer from problems such as free-riding users, high band-
width utilization and poor search performance. With the number of users in the
network growing and the ultra-peer architecture becoming more dominant [9],
solving the problems of bandwidth utilization and search performance becomes
increasingly important to the long-term viability of Gnutella as well as other
ﬁlesharing P2P applications.
In this paper we present an analysis of the trends in Gnutella traﬃc 2003,
2005, and 2006. First, we examined large-scale macro behaviors to determine how
(cid:2) This work was supported in part by the U.S. National Science Foundation (IIS-
0515674 and CNS-0447671).
S. Uhlig, K. Papagiannaki, and O. Bonaventure (Eds.): PAM 2007, LNCS 4427, pp. 42–51, 2007.
c(cid:2) Springer-Verlag Berlin Heidelberg 2007
Trace Driven Analysis of the Long Term Evolution
43
Gnutella traﬃc evolves over the long term. We captured traﬃc in 2003, 2005, and
2006 and compare the message rate, bandwidth utilization, and queueing prop-
erties of each Gnutella message type to determine the changes in characteristic
behavior of Gnutella traﬃc over the long term. We found that the percentage of
the bandwidth consumed by each message type changed considerably. We also
studied more localized behavior such as the query success rate experienced by
peers forwarding queries into the network. We found that the overwhelming ma-
jority (> 90%) of the queries forwarded by a peer do not yield any query hits
back to the forwarding peer. We show that earlier changes to the Gnutella proto-
col had not achieved the intended beneﬁts of alleviating poor search performance
and high bandwidth utilization.
2 Gnutella Overview
First we describe the Gnutella system. Gnutella is a popular P2P ﬁlesharing
application. Gnutella users connect to each other to form an overlay network.
This overlay is used to search for content shared by other peers. The Gnutella
protocol is a distributed and decentralized protocol. Peers issue request mes-
sages that are ﬂooded to all of their neighbors. Each neighboring peer, in turn,
forwards the request to all of its neighbors until a speciﬁed time-to-live (TTL)
is reached. This ﬂooding mechanism allows for reaching many peers, but can
quickly overwhelm the available network bandwidth. Next we describe the two
major versions of the Gnutella protocol.
2.1 Protocol v0.4
The original Gnutella protocol, v0.4, assumed that each Gnutella peer was equal
in terms of capacity and participation. The v0.4 protocol speciﬁed four major mes-
sages: Ping, Pong, Query, and QueryHit. The protocol also speciﬁed other mes-
sages such as Push requests, but these messages were rare and did not represent
a signiﬁcant percentage of the messages sent in the Gnutella network. The Ping
and Pong control messages are used by Gnutella clients to discover other peers in
the network. Ping and Query messages are ﬂooded to each neighbor. The ﬂooding
continues until the TTL for the request has been reached. Responses such as Pong
and QueryHit messages are routed back to the originator along the path of the re-
quest. When a peer receives a Query message it evaluates the query string and
determines if any of its shared content can satisfy the query. If so, the peer sends
back a QueryHit message along the path the it received the Query. Each QueryHit
message contains the following information: the number of objects that match the
query, the IP address and port of the peer where the objects are located, and a set
of results including the ﬁle name size of each matching object.
2.2 Protocol v0.6
As Gnutella became popular, both the size of the network and the amount of
traﬃc on the network increased. The increase in network traﬃc coupled with
44
W. Acosta and S. Chandra
the poor Internet connections of many users [11] overwhelmed many peers in
terms of routing and processing messages. In order to overcome the performance
limitations of the original protocol, the v0.6 [2] was introduced. The new v0.6
ultrapeer architecture reduced the incoming query message rate thus lowering
the bandwidth requirements to process queries. However, as we show in this
paper, the architecture change did not eliminate the problem of high bandwidth
consumption; it shifted it to a diﬀerent place in the network. Speciﬁcally, the
v0.6 protocol requires the ultrapeers to maintain many connections.
3 Related Work
Saroiu et. al. [11] and Ripeanu et. al. [10] studied several aspects of the Gnutella
ﬁle-sharing network. The authors discovered that new clients tended to connect
to Gnutella peers with many connections. This led to an overlay network whose
graph representation had a node degree distribution of a power law graph. The
authors also identiﬁed that most users do not share content (free-ride) and that
a large percentage of nodes had poor network connectivity. More recently, the
problem of free-riding on Gnutella was revisited in [4]. The authors found that
a greater number of users, as a percentage of the total users, are free-riding on
Gnutella than in 2000. Queries forwarded to free-riding users will not yield any
responses. Thus free-riding users degrade the high-level utility of the system and
the low-level performance
A more closely related work to this paper is by Rasti et al. [9]. Their aim was to
analyze the evolution of the v0.6 two-tier architecture with respect to the topolog-
ical properties of the network. The authors measured an increase in the number
of nodes in the network and showed that as nodes began to transition to the v0.6
protocol, the network became unbalanced with respect to the number of leaf and
ultra-peers. They showed that modiﬁcations to major Gnutella client software and
rapid adoption rate by users helped restore the overlay’s desired properties. Our
work is similar in that we analyze Gnutella’s evolution from the v0.4 to the v0.6
architecture. However, our work focuses on the evolution of traﬃc characteristics
and query performance. This allows for better understanding of the eﬀects of ar-
chitectural changes to the system. Further, understanding the evolution of the
system’s workload enables better decisions about future changes to the system
and facilitates better modeling and simulation of large-scale P2P networks.
4 Results
4.1 Methodology
We captured traces for two weeks in May and June of 2003 as well as October of
2005, and June through September of 2006. We modiﬁed the source code of Phex
[7], an open source Gnutella client written in Java to log traﬃc on the network.
The client would connect to the network and log every incoming and outgoing
Trace Driven Analysis of the Long Term Evolution
45
message that passed through it. It is important to note that our traﬃc capturing
client does not actively probe the system; it only logs messages that it receives
and forwards. Each log entry contained the following ﬁelds: timestamp, Gnutella
message ID, message direction (incoming/outgoing), message type, TTL, hops
taken, and the size of the message. In addition to logging incoming and outgoing
messages, the client also logged queries and query responses in separate log
ﬁles. For queries, we logged the query string. In the case of query responses,
the log contained the Gnutella message ID of the original query, the peer ID
where the matching objects are located and a list of the matching objects which
includes the ﬁlename and the size of the ﬁle. The original protocol did not
have separate classes of peers. As such, the traces from 2003 were collected
with our client running as a regular peer. With the introduction of the v0.6
protocol, peers were classiﬁed as either leaf or ultra-peers. Our traces from 2005
and 2006 were captured with our client acting as an ultra-peer. The client was
allowed to run 24 hours a day and logs were partitioned into two hour interval for
ease of processing. These traces give insight as to the evolution of the Gnutella
traﬃc over the last several years. We use the data from these traces to evaluate
our design choices. In the interest of space, we present only results from one
24 hour trace from a typical weekday. The same day of the week was used
(Thursday) for the traces of each year. Analysis of other days show similar
results as those presented in this paper with weekends showing only 3% more
traﬃc than weekdays.
4.2 Summary of Data
The average number of messages (incoming and outgoing) handled by the client
was 2.5M in 2003 and 2.67M in 2006 per 2 hour interval. The average ﬁle size
for a 2 hour trace was 292MB in 2003 and 253MB in 2006. This represents over
3GB of data per 24 hour period. In 2003, a 24 hour trace saw 2,784 diﬀerent
peers directly connected to it. In 2006, our client only saw 1,155 diﬀerent peers
directly connected to it in a 24 period. Although our 2006 traces saw fewer peers,
other studies [8,9] show that the network has grown over time. The reduction in
number of peers seen by our client can be attributed to a trend toward longer
session times. In 2003, more than 95% of the sessions lasted less than 30 minutes.
In 2006, more than 50% of the sessions lasted longer than 60 minutes. Longer
session times imply slower churn rate for the connections at each node and thus
our client would not see as many connections when the session times for peers
is longer.
It is important to note that our data from 2005 shows characteristics that are
consistent with the evolution of the protocol with respect to TTL and hops taken
for messages. However, the bandwidth data is somewhat skewed. Investigation of
this data revealed that although the client attempted to act as an ultra-peer, it
was not able to maintain many connections (< 10). In contrast, the same client
in 2006 was able to maintain over 30 connections consistently. Our traces from a
client running as a leaf node show diﬀerent behavior with a signiﬁcant reduction
in bandwidth for leaf nodes in 2006.
46
W. Acosta and S. Chandra
Table 1. Mean TTL left and Hops Taken for diﬀerent message types in a 24 hour
period from 2003, 2005, and 2006 data
All Messages
Control Messages
Query Messages
TTL Left Hops Taken TTL Left Hops Taken TTL Left Hops Taken
2003
2005
2006
2.39
2.95
0.52
4.46
3.41
3.34
2.84
3.17
0.37
3.11
3.76
3.42
2.36
2.52
2.27
4.57
2.71
2.47
4.3 Message TTL Analysis
The TTL and the number of hops taken for each give an indication of how far
messages travel in the network. Table 1 shows the mean TTL and number of
hops taken for diﬀerent classes of messages in a 24 hour trace for 2003, 2005
and 2006 traﬃc. In 2003, messages travel 6.85 hops into the network. Messages
required 4.46 hops to reach our logging client and arrived with a mean TTL left
of 2.39. Similarly, in 2005, messages travel 6.31 hops into the network requiring
3.41 hops to reach our client and arriving with a TTL of 2.95. In contrast, traﬃc
from 2006 travels 3.86 hops into the network requiring 3.34 hops to reach our
client and arriving with a mean TTL left of 0.52. This change can be attributed
to the v0.6 protocol. The new protocol typically restricts the initial TTL of the
message to between 4 and 5 hops to reduce the burden on the network due
ﬂooding from ultra-peers with many neighbors. A closer inspection of the TTL
left and hops taken for the diﬀerent types of messages reveals a change in the
traﬃc characteristics of the diﬀerent message types. In 2003, control messages
(Ping and Pong) traveled further into the network and had to be propagated to
many peers. On average, 2003 control traﬃc had a mean of 3.1 hops taken and
a mean TTL left of 2.8. In contrast, 2006 control traﬃc had a mean of 3.4 hops
taken and a mean TTL left of 0.37. Although messages arrive at a node from
similar distances in 2003 and 2006, they are not expected to be propagated as
far in 2006 as they were in 2003. Unlike control traﬃc, query traﬃc in 2006 is
expected to be propagated further at each node. In addition, the TTL left for
query traﬃc has remained fairly stable from 2003 to 2006: 2.36 in 2003 and 2.26
in 2006.
We should note the trend between 2003, 2005 and 2006. In 2005, control
messages traveled deep into the network (6.93 hops) compared to 2006 (3.79
hops). In contrast, query messages traveled 6.93 hops in 2003, 5.23 hops in 2005
and 4.68 in 2006. This shows that query messages did not travel as deep into the
network in 2005 as they did in 2003. This can be attributed to the transitioning
to the current state of the v0.6 protocol. The protocol speciﬁcation only sets
guidelines as to what values to set for the initial TTL of a message. In 2005,
vendors of Gnutella clients had already begun to set lower TTL values for query
messages, but had continued to use a TTL of 7 for control messages. In 2006,
these vendors transitioned to lower TTL values for all messages. In Figures 1
and 2 we show the hourly mean TTL and hops taken for control and query
Trace Driven Analysis of the Long Term Evolution
47
traﬃc respectively. The traﬃc represents data from 24 hour traces from 2003
and 2006. We note that the TTL left and hops taken for each message remain
relatively constant throughout the day even though bandwidth consumption
and the number of nodes in the network ﬂuctuate throughout the day. This
observation can be used to develop traﬃc models when simulating a P2P ﬁle-
sharing workload.
L
T
T
 7
 6
 5
 4
 3
 2
 1
 0
2003 Traffic
2006 Traffic
 0
 5
 10
Hour of Day
 15
(a) TTL left
 20
n
e
k
a
T
s
p
o
H
 7
 6
 5
 4
 3
 2
 1
 0
2003 Traffic
2006 Traffic
 0
 5
 10
Hour of Day
 15
 20
(b) Hops Taken
Fig. 1. Mean hourly TTL and hops taken for incoming control messages in a 24 hour
period for 2003 and 2006 traﬃc
4.4 Bandwidth Analysis
In this section we analyze the traces to identify changes in bandwidth consump-