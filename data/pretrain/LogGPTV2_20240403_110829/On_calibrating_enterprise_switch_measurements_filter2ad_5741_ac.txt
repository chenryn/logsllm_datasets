Given two traces comprising a trace pair from the same switch,
we expect each broadcast event to manifest in both traces. Our ﬁrst,
generally quite conservative method for identifying measurement
loss is therefore to verify this assumption and assess the loss rate
when it does not hold. We used the merged traces described in § 4
to look for orphans. We expect that each broadcast packet transmit-
ted by an end-host should appear in the merged trace exactly twice
(i.e., one copy from each of the component traces).
We ﬁrst identify all packets in the merged trace with the same
MD5 hash value. We then ﬂag instances where the number of
such packets captured on the ﬁrst interface differs from the num-
ber captured on the second interface. We record the imbalance as
the number of orphans observed in the trace with fewer instances
of the packet. This scheme is simplistic, and can fail in some cases.
For example, when a roughly equal number of losses for the same
MD5 hash value occurs in each of the component traces, the detec-
tion algorithm will underestimate the degree of measurement loss.
However, it is very likely that the orphans that the process identiﬁes
do reﬂect measurement issues, and should provide a lower bound
on the trace’s measurement loss rate.7
6Clearly, it could in principle occur anywhere in a trace, but we
would not expect that, given how the operators told us they had
acted. Furthermore, our “layout” analysis in § 6.2 provides strong
evidence that this phenomenon did not occur.
7Note that when only considering broadcast trafﬁc, ﬁnding an or-
We note that, along with the head/tail effect and the need to align
trace pairs as discussed above, one other effect apart from measure-
ment loss can also lead to orphans. Above we discussed how our
employment of a 5 msec threshold to eliminate phantoms covers
99.998% of the intervals between known phantoms in the broad-
cast trafﬁc (i.e., sole-sourced packets). Therefore, in about 0.002%
of the cases we miss removing some phantoms. This leaves more
trafﬁc in the “phantom-less” traces than we actually should have,
and we can then in turn misconstrue these extras as orphans because
of the lack of a corresponding packet in the other trace.
Analyzing our dataset with the above described orphan detec-
tion method yields 797 orphans across all traces. In the context of
the entire dataset this reﬂects a 0.007% measurement loss rate. We
ﬁnd that nearly 50% of the traces have no orphans. The maximum
measurement loss rate estimated using this (conservative) method
is 0.2% for one particular trace, roughly four times larger than the
next largest. (As discussed above, we defer our look at the individ-
ual per-trace estimates to Figure 4 below.)
5.2 Leveraging Phantoms
Next we examine a second strategy to assess measurement loss
that leverages the fact that we expect multiple copies of each broad-
cast packet—phantoms—to appear in our traces. For instance,
when the full complement of switch ports is active we would ex-
pect to ﬁnd 5 exact replicas of each broadcast packet in the trace
ﬁle. This expected redundancy allows us to consider decreases in
the replication level as possibly reﬂecting measurement loss.
An immediate issue for this approach is determining the ex-
pected replication level. We know this should be no more than
5 due to the number of ports we tap at once. However, the num-
ber of active switch ports varies across time in our traces (e.g., as
end hosts are powered on and off). We therefore analyze the traces
to delimit intervals across which the replication level remains con-
stant, and then use the size and duration of variations in adjacent
intervals to estimate loss rates.
We might reasonably expect to ﬁnd in each trace a relatively
steady baseline replication level that holds for a long time (min-
utes), pockmarked with brief depressions in the level caused by
measurement loss. However, across all of the traces we ﬁnd that
the median interval length was just 3.4 seconds, the 75th percentile
32 seconds, and the 95th percentile just over 10 minutes. This in-
dicates that there is not necessarily a solid baseline on which to
base our analysis and measurement loss detection. Further, when
the replication level changes it does not then necessarily simply
change back to the previous value when some anomalous interval
was over. We are largely still attempting to ﬁnd a way to make
sense of the progression of replication levels.
Given these puzzling dynamics, we instead employed a fairly
simple approach to establish a plausible lower bound on the mea-
surement loss rate. We observe that for 20% of the replication in-
tervals the following properties hold: (i) the interval includes only
a single packet (and its phantoms), (ii) the number of replicas in
the interval is less than in the adjacent intervals, and (iii) the repli-
cation levels in the adjacent intervals are equal.
In other words,
these intervals represent a slight depression in an otherwise steady
replication level, which likely reﬂects measurement loss. We use
the magnitude of the depression as the number of drops.
We ﬁnd that one-third of the traces exhibit no evidence of mea-
surement loss using this technique; in the remainder, we ﬁnd mod-
est levels of measurement loss, topping out at 0.08%. After taking
into account the discrepancy in the number of traces with no mea-
phan in the manner we sketch means that we missed all instances
of a particular packet.
148surement loss, the distribution of loss rates determined using the
orphan analysis and the replication analysis match fairly closely.
The discrepancy in the number of traces showing no measurement
loss likely arises due to the redundancy provided by broadcast traf-
ﬁc. Orphans indicate the absence of all copies of a packet from a
given trace. Since we know that in general the monitor will capture
for each trace multiple copies of each broadcast packet, orphans
thus indicate a fairly signiﬁcant loss event. In contrast, losses de-
tected via a slight depression in the replication level do not require
the same high bar for detection, and therefore we observe such loss
in a larger number of traces.
5.3 TCP Sequence Gap Analysis
We can assess measurement loss in subnet traces in a quite differ-
ent fashion by leveraging the structure of TCP transfers. Since TCP
provides a high degree of reliability, in the absence of measurement
loss we should only observe a receiver acknowledging data that a
trace shows was previously transmitted by the sender. Thus, if we
observe an acknowledgment (ACK) for a range of sequence num-
bers for which the trace lacks a copy of the corresponding data
transmission, we can infer with high probability that a measure-
ment loss occurred.8
We note that from a trace we can directly compute the volume
of missing data in terms of TCP payload bytes, but not the num-
ber of lost packets, other than by making assumptions about the
size of the missing packets. We also note that making similar infer-
ences regarding missing TCP ACKs is signiﬁcantly more difﬁcult,
as these measurement losses manifest by the TCP sender seem-
ingly transmitting too aggressively for correct congestion control.
Due to variations in how senders implement congestion control, ac-
curately identifying these situations requires developing a model of
the sender’s particular algorithms, a problematic undertaking [6].
For our purposes, the simpler estimate based on ACKs that cover
data sequence gaps sufﬁces, since our aim is to develop a cross-
check on the measurement loss estimates formulated earlier in this
section.
We proceed as follows. For each trace, we processed it using the
Bro network intrusion detection system [8], which performs TCP
stream reassembly in its analysis. Bro’s reassembly process already
includes instrumentation for detecting ACKs that span sequence
gaps. We extended this bookkeeping to count not only how often
such ACKs occur, but also the volume of missing data in the gap(s),
as well as how often the system processed a “candidate” ACK that
could have exhibited a sequence gap (i.e., an ACK that includes a
new sequence range not previously processed) and the total volume
of new data covered by such ACKs. We only apply this analysis to
fully established TCP connections, to eliminate ambiguities that
arise for traces that miss the beginning of connections (and thus
it’s not clear just which data sequence numbers might have already
been transmitted prior to the beginning of the trace), and also for
connections for which one side has already closed the connection
(these can lead to sequence number inconsistencies in the presence
of RST packets).
We then compute two estimates of measurement loss: LP , the
number of ACK packets with sequence gaps divided by the total
number of candidate ACK packets; and LB, the volume of data (in
bytes) missing in the sequence-gap ACKs over the total volume of
new data acknowledged by candidate ACKs.
Neither of these values directly measures the loss rate in terms
of fraction of missing packets. LP can be an underestimate if gaps
found for single ACKs cover multiple absent packets. Likewise,
8Malfunctioning TCP receivers can in fact send acknowledgments
for data never sent, but this situation occurs only quite rarely [8].
TCP missing bytes
Orphans
Replicates
%
,
)
d
n
e
g
e
l
e
e
s
(
d
n
u
o
b
r
e
p
p
u
e
t
a
r
s
s
o
L
1
1
.
0
1
0
.
0
1
0
0
.
0
1
0
0
0
.
0
0.002
0.005
0.020
0.050
0.200
0.500
2.000
TCP gap events loss rate upper bound, %
Figure 4: Estimated measurement loss rates computed using
different approaches. The X-axis gives rates based on the fre-
quency of observing gaps in the data acknowledged by TCP
receivers. The Y-axis shows rates estimated from the number
of missing bytes in such acknowledgments, as well as for the
analyses developed in § 5.1 and § 5.2.
LP can yield an overestimate if ACKs cover multiple packets of
which only one was missing (for example, ack-every-other could
yield for LP a value of 1.0 if every other packet is missing, rather
than 0.5). LB will emphasize the presence or absence of larger
packets over shorter ones, which again could constitute either an
underestimate or an overestimate.
That said, we would expect in aggregate that both measures
should often get us within say a factor of 2 of the true measurement
loss rate, if we assume that the measurement loss process is inde-
pendent from the particulars of the TCP stream’s packet structure.
If on the other hand we do not have such independence, then most
likely the dependence tends towards correlation of high-rate TCP
streams with measurement loss events. In that case, if the sender
employs full-sized packets then LB should come close to estimat-
ing the true measurement loss rate, while LP will overestimate due
to the common use of ack-every-other for TCP connections with
large receiver windows (and hence few delayed ACKs that cover
only a single packet).
Figure 4 compares LP (X-axis) with LB (Y-axis, circles) for
each trace, with both rates in terms of percentages. The diago-
nal line marks equality between the two, so we see that while LB
estimates tend to run a bit higher than LP , overall the two track
one another fairly closely and do not exhibit a clear-cut skew. We
also plot the estimates derived from mismatches in the number of
broadcast “orphans” (per § 5.1, plotted with squares) and reduc-
tions in the replication level (per § 5.2, plotted with triangles). We
see that the orphan- and replication-level-based approaches consis-
tently give considerably lower measurement loss estimates, which
ﬁts with their more conservative constructions.
Note, for all four estimates, we plot not the direct estimate but
upper bounds computed in terms of observing one more measure-
ment loss than what actually appeared in the trace. For example,
for LP we show the rate we would have observed if simply one
more candidate ACK had spanned a content gap, and for LB if we
had seen 1460 more missing data bytes. We aim with using these
bounds to control for the granularity of each loss-rate estimate; for
traces with few candidate ACKs / orphans / depressions in replica-
149tion level, we would like to err on the side of overestimating mea-
surement loss rather than underestimating it.
The fundamental conclusion we take from these measurement
loss-rate estimates is that (i) often measurement loss is quite low
(with the upper bound below 0.1% for the majority of traces), and
(ii) it very rarely exceeds 1%.
6. MAPPING THE MONITORED SUBNET
The ﬁnal class of calibration issues we address concern those
we framed previously as relating to layout: identifying key topo-
logical aspects of each trace. These include determining which
trafﬁc reﬂects intra-subnet vs. inter-subnet communication, iden-
tifying which end systems the tapping directly monitored, and de-
tecting instances of hidden switches/hubs, i.e., instances where in
fact a tapped link does not lead directly to a single end system,
but rather to another network element that provides connectivity to
multiple additional systems. All of these characteristics have po-
tentially signiﬁcant import for measurement analysis, particularly
for studies that emphasize trafﬁc locality or that require compre-
hensive end-system tracing (i.e., capturing all of an end system’s
network activity).
We note that for traditional measurement vantage points, layout
issues are often very simple to resolve. For example, when mon-
itoring a site’s access link one immediately can distinguish inter-
nal from external end systems, and when recording trafﬁc directly
on an endpoint there is no question of confusion regarding hidden
network elements. However, enterprise switch measurements in-
troduce signiﬁcant complications for understanding layout due to
the nature of the vantage point they reﬂect.
One might think that the way to deal with questions of layout is
proactively: ensure that operators accurately record such informa-
tion as they capture the traces. However, this solution is deﬁcient in
two regards. First, due to human error such records may in fact not
match the reality of what the traces captured. Second, the opera-
tors might simply not know all of the layout particulars—especially
a possibility with regard to hidden switches, which users can poten-
tially deploy without ever informing the operator of their presence
(unless the operators enforce that switch ports only accept trafﬁc
from registered MAC addresses).
Thus, while meta-information from operators is a highly use-
ful resource, as is generally the case when pursuing sound
measurement-based analysis, it behooves us to consider alternative
or additional ways of calibrating traces with respect to layout is-
sues. It turns out that elements of such calibration are quite sub-
tle and take considerable care to develop and apply. Furthermore,
some elements of determining layout characteristics rely upon oth-
ers, so we need to proceed in a deliberative fashion. We do so
by ﬁrst considering the problem of distinguishing between intra-
subnet and inter-subnet trafﬁc § 6.1. We then employ information
gained from that analysis to determine with high conﬁdence which
MAC addresses correspond to systems on the other end of moni-
tored switch links § 6.2. Finally, we combine both forms of infor-
mation to assess whether the links monitored in the traces include
multiple hosts behind hidden switches § 6.3.
6.1 Intra- vs. Inter-Subnet Trafﬁc
Our ﬁrst task is to reliably determine which trafﬁc ﬂows reﬂect
trafﬁc that stays inside the Ethernet subnet (broadcast domain) or
involves a remote endpoint outside the subnet. We will primarily
analyze IP trafﬁc (which dominates our traces), with brief com-
ments about non-IP trafﬁc.
For clarity of discussion, we adopt the following notation. Let A
and B reﬂect two hosts involved in communication. Unless other-
wise stated, we will assume that we are analyzing a unicast, unidi-
rectional ﬂow of packets sent from A to B. For these packets, let
MA and MB stand for the corresponding MAC addresses as seen
in the traces, and IA and IB the corresponding IP addresses.
We bootstrap our understanding of whether A or B lies exter-
nal to the subnet as follows. We assume we can readily identify
IP addresses corresponding to hosts external to the entire enter-
prise. If IA is such an address,9 then MA must correspond to a
router’s MAC address. For each trace, we gather up all such MA’s.
Inspecting these, we observe across our entire dataset just three dis-
tinct MAC addresses. Thus, with high conﬁdence we conclude that
those three addresses correspond to IP routers, and trafﬁc involving
them either is communication directly with the router (presumed
rare) or leaves the subnet. In the latter case, it is either inter-subnet
trafﬁc if sent to an IP address internal to the enterprise, or WAN
trafﬁc if not.
We are not yet done, however, because the enterprise’s topol-
ogy might include both WAN routers and internal routers used
only for inter-subnet communication. The above approach will not
have found those, since we seed it with MA’s that reﬂect WAN
addresses. So we next remove any trafﬁc involving these identi-
ﬁed routers. The remaining trafﬁc is either entirely intra-subnet (if
there are no other routers) or potentially involves other subnets at
the site. For this trafﬁc subset we compute the range of IP addresses
seen in the ﬂows (again taking care to remove nonsensical values
that arise from conﬁguration failures). We then widen this range
to the nearest accommodating CIDR preﬁx. In our case, we know
that the enterprise employs subnet blocks in the range of /24 to /22.
If the addresses in the possibly-entirely-intra-subnet trafﬁc fall into
such a preﬁx, then we have good conﬁdence that it indeed reﬂects
only intra-subnet trafﬁc.
For our traces, that is in fact what we found—for each trace, the
possibly-internal trafﬁc always fell within a CIDR preﬁx at most
/22 in width. We then asked the operators whether the preﬁxes
correctly described the enterprise’s subnets. In all cases they did,
except sometimes the inferred preﬁx was narrower than the actual
preﬁx, due to our traces not happening to include the full range of
intra-subnet IP addresses.
We could in principle apply a similar process for non-IP trafﬁc,
too. However, doing so is complicated by the need to understand
the speciﬁc inter-subnet forwarding/routing employed by what are
sometimes fairly obscure link-layer protocols. Instead, we simply
conﬁrmed with the operators that indeed the enterprise does not
route any non-IP trafﬁc between Ethernet subnets.
6.2 Determining Monitored Hosts