above). It can be shown that H∞(X) ≤ H(X) with equality
when X is uniformly distributed. In general, H(X) can be
arbitrary higher than H∞(X), since it can be arbitrary high
even if X assumes a given value with probability close to 1.
B. Framework
In this paper we consider a framework similar to the
probabilistic approaches to anonymity and information ﬂow
used e.g. in [6], [19], [25], and [34]. We restrict ourselves to
total protocols and programs with one high level input A, a
random variable over a ﬁnite set A, and one low level output
(observable) O, a random variable over a ﬁnite set O. We
represent a protocol/program by the matrix of the conditional
O := ⌊ log(A + 2) ⌋
END
In the case of wrong belief, i.e., when the attacker believes
that the value of A is even (resp. odd) when it actually is
odd (resp. even), her low observation of PROG C1 does not
allow her to correct her belief. Indeed, both observations can
be induced by any number.
Now suppose that A is the high input of the probabilistic
program C2 below, with low output O ∈ {−1, 0, 2} and
conditional probabilistic matrix as in Table I.
PROG C2:
BEGIN
R ‘sampled from {0, 2} with p(0) = λ and p(2) = 1 − λ’;
If A = R
Then O := A
p(o | a)
a0
a1
a2
a3
o0
1 − λ
1
λ
1
o1
λ
0
0
0
TABLE I
o2
0
0
1 − λ
0
Conditional probabilistic matrix of PROG C2
Else O := −1
END
Contrary to the PROG C1, the low output of PROG C2 may
allow the adversary to correct her wrong belief. In particular if
B = 1 and O is either 0 or 2 then the adversary knows that her
belief is wrong. But the observation O = −1 cannot help her
correct her wrong belief, as it is compatible with both beliefs.
III. Uncertainty vs accuracy
This section reviews the existing deﬁnitions for quantifying
information leakage. We begin by quantitative approaches
to information ﬂow based on Shannon entropy and mutual
information, and recall why they fail to give good security
guarantees. We then present an alternative approach based
on the adversary’s beliefs proposed by Clarkson, Myers and
Schneider [14]. We conclude the section by presenting a
more recent alternative approach based on the concept of
vulnerability introduced by Smith [34].
A. Shannon entropy approach
There seems to be a general consensus in the literature for
using Shannon entropy to measure uncertainty and mutual
information to quantify information leakage [6], [9], [11],
[12], [22]. We remind the reader that these approaches aim at
quantifying information ﬂow as a reduction of the adversary
uncertainty about the high input and take no account of the
adversary’s initial knowledge. Shannon entropy H(A) as a
measure of the uncertainty of A seems adequate to express the
adversary’s initial uncertainty about A. Similarly, as the con-
ditional entropy H(A | O) measures the amount of uncertainty
of A when O is known, it seems appropriate to express the
adversary’s remaining uncertainty. We thus have the following
deﬁnitions.
• initial uncertainty (IU): H(A)
• remaining uncertainty (RU): H(A | O)
• information leakage (IL):
IU − RU = H(A) − H(A | O) =
I(A; O)
PROG C3:
BEGIN
If A mod 8 = 0
Then O := A
Else O := 1
END
PROG C4:
BEGIN
O := A & 07k−11k+1
END
PROG C3 reveals completely the high input when A is a
multiple of 8 while it reveals nothing about A otherwise
(except of course the very fact that it is not a multiple of
8). On the contrary, PROG C4 reveals always and only the last
k + 1 bits of A.
According to the consensus deﬁnitions, we have IU = 8k,
RU = 7k−0.169 and IL = k+0.169 for PROG C3, and IU = 8k,
RU = 7k − 1 and IL = k + 1 for PROG C4 (the reader is
referred to [34] for the detailed calculations). So, under such
deﬁnitions, PROG C4 appears actually worse than PROG C3, as
7k − 1 < 7k − 0.169, even though intuitively C3 leaves A highly
vulnerable to being guessed (e.g., when it is a multiple of 8)
while C4 does not, at least for large k.
B. Belief approach
Recently Clarkson et al. [14] showed that the Shannon
entropy approach is inadequate for measuring information ﬂow
when the adversary makes assumptions about the high-level
secret and such assumptions might be incorrect. Based on
the conviction that it is unavoidable that the attacker makes
such (potentially inaccurate) assumptions, they proposed a
new metric. They formalised the idea of an adversary’s belief
simply as a distribution of A assumed by the adversary:
information ﬂow is then expressed as an increase of the
accuracy of such belief. The initial accuracy is the Kullback-
Leibler distance between the adversary’s initial belief and the
actual distribution of A; similarly the remaining accuracy is
the Kullback-Leibler distance between the Bayesian-updated
belief of the adversary after her low observation, and the actual
distribution of A.
However, as noticed by Smith [34], when the adversary’s
belief coincides with the a priori distribution of A,
then
the belief approach reduces again to the inadequate standard
approach illustrated above.
C. Vulnerability approach
Nevertheless, recent work by Smith [34] suggests that these
notions do not support security guarantees satisfactorily. In
particular the remaining uncertainty is generally of little value
in characterising the real threat that the adversary could guess
the value of A given her low observations. Smith uses the
following example to prove that.
Example 2: Consider the following programs C3 and C4,
where A is a uniformly distributed 8k-bit integer, k ≥ 2, &
denotes bitwise ‘AND’, and 07k−11k+1 a binary constant.
Having observed that both the consensus and the belief
approaches fail in general to give good security guarantees,
Smith [34] proposes a new metric for quantitative information
ﬂow based on the notions of vulnerability and min-entropy.
We brieﬂy revise these concepts here.
The vulnerability of a random variable A is the worse-
case probability that an adversary could guess the value of
A correctly in one try. The vulnerability of A, denoted V(A),
is thus formally deﬁned as follows.
Deﬁnition 1: V(A) = maxa∈A p(a).
The conditional vulnerability of a A given O measures the
expected probability of guessing A in one try given O. It is
denoted V(A | O) and deﬁned as follows.
Deﬁnition 2: V(A | O) = Po∈O p(o)V(A | o), where V(A | o)
is maxa∈A p(a | o).
The initial uncertainty about A is then deﬁned as the
negative logarithm of V(A), which turnouts to be the min-
entropy of the random variable A – cf. (6) above. And the
remaining uncertainty about A after observing O is deﬁned as
the min-entropy of A given O. Thus we have the following
vulnerability-based deﬁnitions:
• IU : H∞(A) = − log V(A)
• RU : H∞(A | O) = − log V(A | O)
• IL :
IU − RU = H∞(A) − H∞(A | O)
Now on the security guarantees of the vulnerability-based
approach. By applying these deﬁnitions to the programs of
Example 2, we have IU = 8k, RU = 8k − 3 and IL = 3
for PROG C3, and IU = 8k, RU = 7k − 1 and IL = k + 1
for PROG C4. While these quantities remain the same as in
the consensus approach for PROG C4, the new metric hugely
increases the leakage ascribed to PROG C3 reﬂecting the fact
that the low observations of PROG C3 leave the high input very
vulnerable to being guessed.
A related line of research has explored methods of statistical
inference,
in particular those from the hypothesis testing
framework. The idea is that the adversary’s best guess is that
the true input is the one which has the maximum a posteriori
probability (MAP rule) and that, therefore, the a posteriori
vulnerability of the system is the complement of the Bayes
Risk, which is the average probability of making the wrong
guess when using the MAP rule [7]. This is always at least
as high as the a priori vulnerability, which is the probability
of making the right guess just based on the knowledge of the
input distribution. It turns out that Smith’s notion of leakage
actually corresponds to the ratio between the a posteriori and
the a priori vulnerabilities [3], [34].
Concerning the eﬃcient computation of the channel matrix
and the leakage, the only work we are aware of is [1], in
which the authors propose various automatic techniques. One
of these is able to generate counterexamples, namely points on
the execution where the channel exhibits an excessive amount
of leakage. This method is therefore also useful to ﬁx unsound
protocols.
IV. Unifying Belief and Vulnerability
We now propose an alternative approach based on the
vulnerability concept that takes into account the adversary’s
belief.
A. Belief-vulnerability
Let B be the adversary’s additional information about a
random high level variable A. Then the belief-vulnerability of
A is the expected probability of guessing A in one try given the
adversary’s belief. Given an additional information B = b, the
adversary will choose a value having the maximal a posteriori
probability according to her belief, that is a value a′ ∈ Γb,
where Γb = argmaxa∈A pβ(a | b). The vulnerability of A given
b is then the real probability that the adversary’s choice is
correct given b, that is the a posteriori probability pρ(a′ | b).
As there might be many values of A with the maximal belief
a posteriori probability, the attacker will pick uniformly at
random one element in Γb. Hence we have the following
deﬁnition.
Deﬁnition 3: Let A be a random variable and B the adver-
sary’s extra knowledge about A. Then the belief-vulnerability
of A, denoted V(A : B), is deﬁned as
V(A : B) = Xb∈B
pρ(b)V(A : b)
(7)
where V(A : b) = 1
|Γb| Pa′∈Γb pρ(a′ | b).
Next, we show how to compute V(A : B) from the given
probabilities.
V(A : B) = Xb∈B
= Xb∈B
= Xb∈B
pρ(b)V(A : b)
pρ(b)(cid:16) 1
|Γb| Xa∈Γb
1
|Γb| Xa∈Γb
pρ(a | b)(cid:17)
pρ(a | b)pρ(b)
(by Bayes theorem)
= Xb∈B
1
|Γb| Xa∈Γb
pρ(b | a)pρ(a).
Thus the belief-vulnerability can be easily computed as fol-
lows.
Proposition 1: Let A be a random variable and B the
adversary’s extra knowledge about A. Then
V(A : B) = Xb∈B
1
|Γb| Xa∈Γb
pρ(b | a)pρ(a).
(8)
We then deﬁne the initial uncertainty as the min-entropy of
A : B. Thus we have the following deﬁnition.
Deﬁnition 4: Let A be a random variable and B the adver-
sary’s additional information about A. Then the initial threat
to A given B, denoted H∞(A : B), is deﬁned as
H∞(A : B) = log(cid:16)
1
V(A : B)(cid:17).
(9)
Example 3: Suppose that A is uniformly distributed over
{0, 1, 2, 3} and the adversary’s extra information is about the
parity of A. Assume that the a priori distribution of A is
publicly-known, i.e. ∀a ∈ A, pβ(a) = pρ(a). Assume also
that the adversary believes that her extra info is accurate, that
is she assumes the following correlation:
pβ(b | a)
a0
a1
a2
a3
b0
1
0
1
0
b1
0
1
0
1
pρ(b | a)
V(A : B)
H∞(A : B)
pρ1
0.49
1.03
pρ2
0.02
5.56
pρ3
0.50
1
pρ4
0
+∞
pρ1(b | a)
a0
a1
a2
a3
pρ3(b | a)
a0
a1
a2
a3
b0
0.98
0.02
0.98
0.02
b1
0.02
0.98
0.02
0.98
b0
1
0
1
0
b1
0
1
0
1
pρ2(b | a)
a0
a1
a2
a3
pρ4(b | a)