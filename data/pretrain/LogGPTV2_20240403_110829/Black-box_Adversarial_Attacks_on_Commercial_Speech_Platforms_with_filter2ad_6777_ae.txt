Cortana, Google Assistant, and Amazon Echo, respectively11.
et al. [31] achieved a 90% success rate of over-the-air attacks over
the attack distance of up to 6m by further removing device- and
environment-specific features. Besides, imperceptible audio AEs
were produced in [74] via the psychoacoustic model. Both imper-
ceptible and robust audio AEs were constructed in [69] against the
Lingvo ASR system, with a success rate of 50%.
Table 7 presents the results of the experiments on human percep-
tion. Overall, NI-Occam performs much better than the superposi-
tion attack. More than 67% volunteers think the audio generated
from NI-Occam is normal or just noisy, while 100% volunteers can
recognize the AEs from the superposition attack. This is because
the audios crafted in the superimposing manner can be more easily
noticed due to the human ear‚Äôs excellent ability of speech separa-
tion. We also assume that a proper noise level is acceptable because
the background environment may be noisy, and the equipment may
emit some noise due to a temporary fault. The results show that
NI-Occam is stealthy enough and cannot be easily perceived.
6 RELATED WORK
6.1 Audio Adversarial Examples
Adversarial Examples Against ASR Systems. Despite the great
success of adversarial examples in the image domain, the transcrip-
tion of spontaneous speech poses a more significant challenge for
crafting audio AEs. The early results have clearly indicated that
ASR systems are inherently vulnerable to AEs in white-box set-
tings. Among others, Carlini et al. [25] was the first to implement
an iterative optimization-based attack with a success rate of 100%
on the end-to-end Mozilla DeepSpeech model. However, it takes
approximately one hour for their attack to produce one adversarial
example on a single NVIDIA 1080Ti GPU, and the crafted adver-
sarial sample fails when being played over the air. Concurrently,
CommanderSong [90], which embedded malicious commands into
popular songs, was reported to have successfully attacked against
Kaldi [2]. It further launched a very limited over-the-air attack,
which is heavily dependent on the recording devices, speakers
and room settings. Efforts were also made by [86] to build robust
over-the-air AEs by utilizing impulse responses to simulate the
reverberation with a success rate of around 60%. Moreover, Chen
11The AEs are the ones that can fool the devices, as shown in Table 6.
Compared to the above attacks, our attacks have the following
superiorities: 1) Our attacks are more practical since these attacks
have access to the internal information of target model; 2) Instead
of only targeting one open-source system in the white-box setting,
our attacks evaluate the robustness of many representative audio
processing systems in real-world scenarios.
While adversarial example based white-box attacks have ob-
tained excellent results against open-source ASR systems, its black-
box counterpart hasn‚Äôt made big progress until recently. By lever-
aging the last layer (i.e., logits) of DNNs inside the DeepSpeech,
Taori et al. [80] obtained the fitness score of adversarial inputs, and
combined genetic algorithms with the gradient estimation to attack
DeepSpeech. In addition to the rather low success rates even after
300,000 queries, this type of black-box attack is not applicable to
commercial systems. Following this work, selective gradient esti-
mation attack [82] was proposed to achieve a success rate of 98%
in this setting. Moreover, multi-objective genetic algorithms were
also introduced to start a black-box adversarial attack against ASR
systems [53]. However, due to the relatively large word error rate
(WER) after many evolutions in the attack, it becomes ineffective
for generating audio AEs. A very recent work, Devil‚Äôs Whisper [32],
utilized confidence scores exposed by commercial ASR systems to
launch the black-box attack. However, it is worth noting that there
are many commercial ASR systems that do not return any score
information, and service providers are also apt to hide these scores
to reduce the risk of adversarial attacks, considering that these
information almost has no effect on the user experience and may
be mainly exploited by malicious attackers.
There are two key differences: 1) Our Occam, as a generic attack,
requires only the final decisions to generate audio AEs, effective to
both commercial ASR and SR services; 2) Our NI-Occam requires
no access to the targeted devices, but still can generate effective
audio AEs. Our attacks are more practical in real world scenarios.
Adversarial Examples Against SR Systems. When it comes to
adversarial example generation against SR systems, relatively little
work has been done in both white-box and black-box cases. Kreuk
et al. [55] presented a white-box adversarial attack against the DNN-
based speaker verification system. Gong et al. [39] demonstrated
the vulnerability of speaker identification system to audio AEs in
the white-box scenario. Obviously, these attacks require access to
internal structures and parameters of the target systems, and they
are impractical when facing commercial SR systems.
In a recent work, SirenAttack [35] was presented to launch a
black-box attack against a number of classification-oriented acous-
tic systems, including the SR system via the predicted probabili-
ties/scores. More recently, FakeBob [26] also utilized the predicted
probabilities/scores to conduct a black-box adversarial attacks against
Talentedsoft API [7] with a success rate of 100%. The main limita-
tion of SirenAttack and FakeBob is that they lose the effectiveness
when applied to commercial SR systems, which usually hide the
predicted scores. For example, the Microsoft Azure SR API service
only provides the decision (i.e., the predicted speaker) along with
three confidence levels (i.e., low, normal, or high) to users. Our
attack shows its superiority to prior attacks by achieving an attack
success rate of 100% against commercial SR systems even if the
service provider conceals the prediction score information.
6.2 Other Types of Attacks
In addition to audio AEs, researchers have also discovered that
intelligent voice systems are vulnerable to other types of attacks,
including misinterpretation attack and hidden voice attack.
Misinterpretation Attacks. Kumar et al. [57] conducted an em-
pirical analysis of misinterpretations and investigated security im-
plications on Amazon Alexa, based on which they introduced a
new attack called skill squatting to surreptitiously route users to
malicious third-party public services. Along this direction, Zhang
et al. [93] reported similar attacks against ASR systems, where a
malicious skill with the similarly pronounced name or paraphrased
name was exploited to impersonate a benign skill. Also targeting at
ASR systems, Zhang et al. [95] designed a linguistic-model guided
fuzzing tool called LipFuzzer to systematically discover misinter-
pretations leading to malicious attacks.
Hidden Voice Attacks. Besides, by either exploiting knowledge
of the feature extraction algorithm or hardware vulnerabilities in
microphone circuits, the adversary can embed hidden commands
into an audio carrier, in the form of noises, thereby compromising
the intelligent voice systems. To achieve this goal, hidden voice
command [23] utilized inverse MFCC to craft obfuscated commands
against ASR systems in a ‚Äúblack-box‚Äù manner with considerable
human effort for obtaining feedbacks. Four different perturbations
were introduced to generate noise-like adversarial audios against
ASR and SR systems [12]. Moreover, DophinAttack [92] was devised
to modulate voice commands on inaudible ultrasounds, which can
be interpreted by the ASR system, by exploiting the non-linearity of
the microphone circuits. However, compared to adversarial example
based attacks, these attacks could be easily defended or perceived.
Our initial idea stems from the image-based adversarial attacks.
We have overcome particular challenges for generating audio AEs.
Compared to the state-of-the-art black-box attacks on acoustic
systems [26, 32, 35, 80], ours is more generic and practical.
7 DISCUSSIONS
We discuss four possible countermeasures to defend against our
Occam and NI-Occam below, with detailed performance results of
countermeasures in Appendix H (Tables 14 and 15).
Local Smoothing. Because audio AEs are carefully constructed by
adding small perturbations, they can be mitigated by local smooth-
ing. We can apply a sliding window with the median filter to the
adversarial audio signals. Given a data point ùë•ùëñ, we replace it with
the average of ùëò samples before and after it, i.e., [ùë•ùëñ‚àí‚Ñé, ..., ùë•ùëñ, ..., ùë•ùëñ+‚Ñé].
Since the adversarial perturbation is carefully constructed in our
attacks, the audio AEs may become less effective after local smooth-
ing transformation. For example, when ‚Ñé = 1, the SRoA of Occam
drops from 100% to 20%, while the SRoA of NI-Occam drops from
60% to 40%. When ‚Ñé = 3, all AEs of Occam fail, and NI-Occam re-
mains an SRoA of 40%. We can find that NI-Occam is more robust to
local smoothing. The reason is that the recognized key parts of nat-
ural command audios are recovered from audio AEs in NI-Occam,
thus making them more robust.
Downsampling. Based on the sampling theory, the high-frequency
information in the audios will be lost after the downsampling pro-
cess, which may disrupt perturbations in the audio AEs. Thus, audio
AEs crafted in our Occam will fail to work after the downsampling
and upsampling process, e.g., audio AEs are first downsampled to
12kHz and then upsampled to 16kHz as indicated in our exper-
iments. Nonetheless, NI-Occam remains a better SRoA, i.e., 30%
when the downsampling rate (DSR) is 12kHz, indicating that the
AEs of NI-Occam have a greater chance of resisting downsam-
pling. While downsampling can help to mitigate our attacks, if the
dawnsampling/upsampling rates are known to the attacker, this
countermeasure will be invalid. This is because the adversary can
directly compensate the generated example for the information lost
in the downsampling and upsampling process.
Temporal Dependency Based Approach. The inherent tempo-
ral dependency in audio data was recently utilized in [88] to de-
tect audio AEs due to the disruption of the temporal information.
Namely, the first ùëò (0 ‚àº 1) portions of the whole audio were selected
and recognized as ùëÜùëò, and the ùëò portions of the transcription of
the whole audio ùëÜ{ùë§‚Ñéùëúùëôùëí,ùëò} is obtained and compared with ùëÜùëò. By
checking the consistency between ùëÜ{ùë§‚Ñéùëúùëôùëí,ùëò} and ùëÜùëò, audio AEs
crafted by Occam can be easily detected. This approach has a strong
discriminative ability in identifying AEs targeting at ASR systems,
and theoretically it can identify almost all audio AEs against ASR
systems. Even so, Occam is still effective in attacking SR API ser-
vices since AEs constructed for SR systems preserve the temporal
information like natural audios. For instance, Occam also achieves
a success rate of 80% against Microsoft Azure speaker identification
API when our audio AEs were randomly split into two parts, i.e., ùëò
and 1 ‚àí ùëò portions of the whole audio.
The defense [88] is better suited for ASR tasks since the temporal
dependency is stronger. To evaluate the effectiveness of the tem-
poral dependency based approach against (NI-)Occam for ASR, we
build a dataset of 40 audio AEs generated using (NI-)Occam with 40
natural command audios, and calculate the detection rates. For each
audio sample, we randomly select ùëò in the range of [0.2, 0.8] and
split the audio into two pieces. We then calculate the consistency of
the split audios and the whole audio by the word error rate (WER).
With a varying WER threshold, we can obtain the ROC curve and
finally calculate the area under curve (AUC), where a higher AUC
indicates better detection performance. The AUC is 100% in Occam,
showing that the defense can successfully detect all AEs generated
by Occam. However, the AUC drops to 68% when classifying AEs
from NI-Occam, which means that NI-Occam is robust to the tem-
poral dependency based approach. This is because the adversarial
perturbation generated by NI-Occam usually only occurs in a small
piece of audio, and splitting the audio does not disrupt the temporal
dependency. Therefore, it is hard for the classifier to detect the AEs
generated using NI-Occam by analyzing the temporal information.
Adversarial Training. Adversarial training [40, 61] is one of the
most effective defenses against adversarial attacks in the image
domain [17]. The basic idea of adversarial training is to train models
on AEs to make the models robust to AEs. It can be formulated as
a mini-max optimization problem:
(cid:104)
J(ùúÉ, ùë• + ùõø, ùë¶)(cid:105),
min
ùúÉ
E(ùë•,ùë¶)‚àºùê∑
max
‚à•ùõø ‚à•ùëù <ùúñ
(12)
where ùúÉ denotes the deep learning model, (ùë•, ùë¶) denotes the original
data point, ùõø is the adversarial perturbation, and ‚à•¬∑‚à•ùëù is the p-norm.
Here, the worst-case samples for the given model are found in the
inner maximization problem to train a more robust model via the
outer minimization operation.
So far, adversarial training has been extensively studied on image
classification tasks [18, 22, 76, 81, 85]. However, there is relatively
little research on this defense for speech recognition tasks, which
are more complex and challenging. Besides, generating strong au-
dio AEs incurs high computation costs. For example, [25] reports
that it takes approximately one hour to construct an audio AE in a
singe NVIDIA 1080Ti GPU, far exceeding the time of generating an
adversarial image. In order to study the effect of adversarial training
on ASR systems, we evaluate the performance of adversarial train-
ing on the open-source Kaldi. For solving the inner maximization
problem of Eq. (12) that , our targeted NI-Occam is not suitable
as it performs the minimization operation, and thus we adopt the
untargeted projected gradient descent (PGD) attack [61], which is
usually used in adversarial training, as follows:
ùë•ùëñ+1 = ùë• + Pùúñ,ùëù(ùë•ùëñ ‚àí ùë• + ùõº ¬∑ ùë†ùëñùëîùëõ(‚àáùë•ùëñ J(ùúÉ, ùë•ùëñ, ùë¶))),
(13)
where Pùúñ,ùëù is a projection operator on Lùëù ball, and ùõº the step size.
According to our experimental results on Kaldi (Mini LibriSpeech
model12) (see Table 15 in Appendix H), adversarial training is indeed
an effective defense against our targeted NI-Occam attack, e.g., the
SRoA of the AEs drops to 30% when ùúñ = 0.002. But, meanwhile,
the WER of the model is increasing from 10.69 to 19.82, which
means that the accuracy of the speech recognition drops about 10%.
Moreover, with the increase of ùúñ, both the SRoA and model accuracy
will be significantly reduced. Considering the extreme case when
almost all audio AEs fail, the model accuracy drops about 20%. And
if ùúñ was further increased to eliminate our attack, e.g., approaching
ùúñ = 0.006, adversarial training is not even able to converge. The
reason may be that the audio vector contains many values very
close to zero, and they are changed a lot with a high ùúñ.
12It is a TDNN based chain Kaldi model trained on Mini-Librispeech dataset. The URL
is https://github.com/kaldi-asr/kaldi/tree/master/egs/mini_librispeech/s5.
Overall, adversarial training on Kaldi is effective against our
attacks. However, while the SRoA can be reduced by 70%‚àº90%, it
also inevitably brings a significant performance degradation, i.e.,
a 10%‚àº20% accuracy loss, and such loss is unacceptable for com-
mercial ASR systems. Moreover, adversarial training will signifi-
cantly increase the training time and costs [51, 76], particularly
for ASR systems. For example, for the Mini LibriSpeech dataset
containing only 5 hours of audio data, it took about 10 days for
us to adversarially train Kaldi model on six NVIDIA 2080Ti GPUs,
while commonly-used voice datasets, like LibriSpeech and Com-
mon Voice, have around 1000 hours of voice data), making it almost
impractical on large-scale models and datasets. Thus, even with ad-
versarial training our attack cannot be prevented. Therefore, service
providers may not be willing to adopt the defense for ‚Äúblack-box‚Äù
commercial models due to the issues above.
Remarks. We point out several potential research directions to ob-
tain more practical attacks. Since the human perception experiment
has shown that some audio AEs can be recognized and regarded as
abnormal audios, it is necessary to further improve the stealthiness
of constructed audio AEs. Moreover, the physical attack against
voice control devices in this work will fail when the distance is
long or in a very noisy environment. For example, when we play
the AEs at a distance of 50cm from the devices in a noisy cafe, the
SRoAs of NI-Occam against Amazon Echo and Apple Siri decrease
to 20%. Thus, it is still challenging to launch physical and black-box
attacks on ASR systems at long distances or in noisy environments,
which requires more efforts in the future. Besides, robust physical
adversarial attacks against SR systems in the decision-only and
non-interactive settings should also be put on the agenda. Finally,
it is also an interesting and meaningful job to achieve black-box
adversarial attacks against both ASR and SR systems on one audio
AE, because some smart speakers, such as Apple HomePod, will
first perform speaker identification on the input audio.
8 CONCLUSION
In this paper, we proposed two novel black-box adversarial attacks
against commercial speech platforms, Occam and NI-Occam. Occam
constructs audio AEs against cloud speech APIs in the decision-
based black-box setting. It is effective under the strictly black-box
scenario where the attackers can rely solely on the final decisions.
Extensive experiments on targeted and untargeted attacks against
a wide range of popular open-source and commercial ASR and SR
systems demonstrated the effectiveness of Occam. Occam achieves
an average SNR of 14.37dB and 100% SRoA on commercial ASR
systems, outperforming the state-of-the-art black-box audio ad-
versarial attacks. NI-Occam is the first non-interactive physical
attack, simple but effective, which can successfully fool commercial
devices without needing any feedback information from the target
devices. Extensive experiments showed the effectiveness of the AEs
from NI-Occam in attacking Apple Siri, Microsoft Cortana, Google
Assistant, iFlytek, and Amazon Echo with an average SRoA of 52%
and SNR of 9.65dB.
ACKNOWLEDGMENTS
This work was partially supported by the National Key R&D Pro-
gram of China (2020AAA0107701), NSFC under Grants U20B2049,
61822207, 61822309, 61773310, 62132011, and U1736205, BNRist
under Grant BNR2020RC01013, RGC of Hong Kong under Grants
CityU 11217819, CityU 11217620, and R6021-20F, and Laboratory
for AI-Powered Financial Technologies.
REFERENCES
[1] 2014. Alexa. https://developer.amazon.com/en-US/alexa.
[2] 2014. Kaldi. http://kaldi-asr.org.
[3] 2015. iFlytek Speech-to-Text. https://www.xfyun.cn/services/voicedictation.
[4] 2018. Alexa Auto SDK. https://developer.amazon.com/en-US/alexa/alexa-auto/
[5] 2018. Google Cloud Speech-to-Text. https://cloud.google.com/speech-to-text.
[6] 2018. Microsoft Azure Speech Service. https://azure.microsoft.com/en-us/
services/cognitive-services/speech-services/.
[7] 2018. Talentedsoft. http://www.talentedsoft.com/.
[8] 2020. Common Voice Dataset. https://voice.mozilla.org/en/datasets.
[9] 2020. Jingdong Speaker Recognition. https://www.jdcloud.com/en/products/
sdk.
voiceprint-recognition.
product/1093.
[10] 2020. Tencent Short Speech Recognition. https://cloud.tencent.com/document/
[11] 2021. Alibaba Short Speech Recognition. https://www.alibabacloud.com/zh/
product/intelligent-speech-interaction.
[12] Hadi Abdullah, Washington Garcia, Christian Peeters, Patrick Traynor, Kevin
R. B. Butler, and Joseph Wilson. 2019. Practical Hidden Voice Attacks Against
Speech and Speaker Recognition Systems. In Proc. of NDSS.
[13] Moustafa Alzantot, Yash Sharma, Supriyo Chakraborty, Huan Zhang, Cho-Jui
Hsieh, and Mani B. Srivastava. 2019. Genattack: Practical Black-box Attacks
with Gradient-free Optimization. In Proc. of GECCO.
[14] Tawfiq Ammari, Jofish Kaye, Janice Y. Tsai, and Frank Bentley. 2019. Music,
Search, and IoT: How People (Really) Use Voice Assistants. ACM Transactions
on Computer-Human Interaction 26, 3 (2019), 17:1‚Äì17:28.
[15] Anne Auger. 2009. Benchmarking the (1+1) Evolution Strategy with One-fifth
Success Rule on the BBOB-2009 Function Testbed. In Proc. of GECCO.
[16] Steve Austin, Chris Barry, Yen-Lu Chow, Man Derr, Owen Kimball, Francis
Kubala, John Makhoul, Paul Placeway, William Russell, Richard M. Schwartz,
and George Yu. 1989. Improved HMM Models for High Performance Speech
Recognition. In Proc. of Human Language Technology (HLT).
[17] Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. 2021. Recent Advances
in Adversarial Training for Adversarial Robustness. In Proc. of IJCAI.
[18] Yogesh Balaji, Tom Goldstein, and Judy Hoffman. 2019.
Instance Adaptive
Adversarial Training: Improved Accuracy Tradeoffs in Neural Nets. CoRR
abs/1910.08051 (2019).
[19] Arjun Nitin Bhagoji, Warren He, Bo Li, and Dawn Song. 2018. Practical Black-box
Attacks on Deep Neural Networks Using Efficient Query Mechanisms. In Proc.
of ECCV.
[20] Debnath Bhattacharyya, Rahul Ranjan, Farkhod Alisherov, Minkyu Choi, et al.
International Journal of u-and e-
2009. Biometric Authentication: A Review.
Service, Science and Technology 2, 3 (2009), 13‚Äì28.
[21] Wieland Brendel, Jonas Rauber, and Matthias Bethge. 2018. Decision-based Ad-
versarial Attacks: Reliable Attacks Against Black-box Machine Learning Models.
In Proc. of ICLR.
[22] Qi-Zhi Cai, Chang Liu, and Dawn Song. 2018. Curriculum Adversarial Training.
In Proc. of IJCAI.
[23] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David A. Wagner, and Wenchao Zhou. 2016. Hidden Voice Com-
mands. In Proc. of USENIX Security.
[24] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness
of Neural Networks. In Proc. of IEEE S&P.
[25] Nicholas Carlini and David Wagner. 2018. Audio Adversarial Examples: Targeted
Attacks on Speech-to-text. In Proc. of IEEE SPW.
[27]
[26] Guangke Chen, Sen Chen, Lingling Fan, Xiaoning Du, Zhe Zhao, Fu Song, and
Yang Liu. 2021. Who is Real Bob? Adversarial Attacks on Speaker Recognition
Systems. In Proc. of IEEE S&P.
Jianbo Chen, Michael I. Jordan, and Martin J. Wainwright. 2020. HopSkipJumpAt-
tack: A Query-efficient Decision-based Attack. In Proc. of IEEE S&P.
[28] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh. 2017. Zoo: