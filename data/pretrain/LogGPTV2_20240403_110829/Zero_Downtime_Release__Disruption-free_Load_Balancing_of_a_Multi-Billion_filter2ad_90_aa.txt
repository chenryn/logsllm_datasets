title:Zero Downtime Release: Disruption-free Load Balancing of a Multi-Billion
User Website
author:Usama Naseer and
Luca Niccolini and
Udip Pant and
Alan Frindell and
Ranjeeth Dasineni and
Theophilus A. Benson
Disruption-free Load Balancing of a Multi-Billion User Website
Zero Downtime Release:
Usama Naseer∗
Brown University
Alan Frindell
Facebook, Inc.
Luca Niccolini
Facebook, Inc.
Ranjeeth Dasineni
Facebook, Inc.
Udip Pant
Facebook, Inc.
Theophilus A. Benson
Brown University
ABSTRACT
Modern network infrastructure has evolved into a complex organism
to satisfy the performance and availability requirements for the bil-
lions of users. Frequent releases such as code upgrades, bug fixes and
securityupdateshavebecomeanorm.Millionsofgloballydistributed
infrastructure components including servers and load-balancers
are restarted frequently from multiple times per-day to per-week.
However, every release brings possibilities of disruptions as it can
result in reduced cluster capacity, disturb intricate interaction of the
components operating at large scales and disrupt the end-users by
terminating their connections. The challenge is further complicated
by the scale and heterogeneity of supported services and protocols.
In this paper, we leverage different components of the end-to-
end networking infrastructure to prevent or mask any disruptions
in face of releases. Zero Downtime Release is a collection of mecha-
nisms used at Facebook to shield the end-users from any disruptions,
preserve the cluster capacity and robustness of the infrastructure
when updates are released globally. Our evaluation shows that these
mechanisms prevent any significant cluster capacity degradation
when a considerable number of productions servers and proxies are
restarted and minimizes the disruption for different services (notably
TCP, HTTP and publish/subscribe).
CCS CONCEPTS
• Networks → Network management; Network protocol design.
KEYWORDS
Update releases, Load-balancing, Reliable networks.
ACM Reference Format:
Usama Naseer, Luca Niccolini, Udip Pant, Alan Frindell, Ranjeeth Dasineni,
and Theophilus A. Benson. 2020. Zero Downtime Release: Disruption-free
Load Balancing of a Multi-Billion User Website. In Annual conference of
the ACM Special Interest Group on Data Communication on the applications,
technologies, architectures, and protocols for computer communication (SIG-
COMM ’20), August 10–14, 2020, Virtual Event, USA. ACM, New York, NY,
USA, 13 pages. https://doi.org/10.1145/3387514.3405885
∗Work done while at Facebook, Inc.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
© 2020 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-7955-7/20/08.
https://doi.org/10.1145/3387514.3405885
1 INTRODUCTION
Online service providers (OSP), e.g., Facebook, Google, Amazon,
deploy massively complex code-bases on large sprawling infrastruc-
tures to deliver rich web services to billions of users at a high quality
of experience. These code-bases are constantly being modified to
introduce bug fixes, performance optimizations, security patches,
new functionality, amongst a host of other reasons. Recent studies
from Facebook [50, 51] show that each day tens of thousands of
commits are pushed to tens of thousands of machines across the
globe.
In fact, the number of web-tier releases increased from once
per week in 2007 to tens of times a day in 2016, each comprising of
1000s of code commits [20, 21].
At the scale of multi billion users and millions of machines, code-
update and release techniques must be swift while simultaneously
incurring zero downtime. Today, the state of the art approach for
deploying these code changes requires draining connections from
servers with the old code and incrementally restarting the servers to
introduce the new code [17, 28, 52]. This technique can have a host of
undesirable consequences from lowering aggregate server capacity
and incurring CPU overheads to disrupting and degrading end user
experience. At the scale of billions of connections, restarting con-
nections is disastrous for the ISP, end-user, and the OSP [11, 18]. The
process of connection-restart incurs a number of handshakes (e.g.,
TLS and TCP) which we show (in Section 2.5) consumes as much
as 20% of the OSP’s CPU. Additionally, the flood of new connec-
tions triggers wireless protocol signaling at the cellular base-station
which simultaneously drains a mobile phone’s batteries and can over-
whelm the cellular provider’s infrastructure. Finally, we observed
that during the restarts, users can experience QoE degradation and
disruptions in the form of errors (e.g., HTTP 500 error) and slower
page loads times (i.e., due to retries over high-RTT WAN).
Motivated by the high code volatility and the potential disruption
arising from code deployment, many online service providers have
turned their attention to designing practical and light-weight ap-
proaches for transparently deploying code in a disruption free man-
ner, i.e., deploying code while ensuring zero downtime. The design of
such a disruption free update mechanism for large providers is chal-
lenge by the following characteristics which are unique to the scale
at which we operate: first, large providers employ a large range of
protocols and services, thus, the update mechanisms must be general
and robust to different services. For example, we run services over
both HTTP and MQTT (a publish-subscribe protocol [45]) which
have distinctly different tolerance and state requirements. Second,
whilemanyapplicationsarestateless,anon-trivialsetofapplications
are stateful, thus the update mechanisms must be able to seamlessly
migrate or transparently recreate this state at the new server. For
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Naseer et al.
Figure (1) End-to-end Infrastructure
example, a non-trivial number of connection are long-lived and often
transferlargeobjects,failingtomigratestatecansignificantlyimpact
end-user performance. Third, due to energy reasons and application-
specific requirements, a subset of servers are resource-constrained
(e.g., cache priming [12, 29] for HHVM servers [5] consumes most
available memory), which prevents us from running both the new
code and old code on the same server, thus preventing us from lever-
aging kernel mechanisms to migrate connections.
Our framework builds on several unique characteristics shared
by the infrastructure of online service providers such as Facebook
and Google. First, the end-to-end infrastructure is owned and ad-
ministered by the provider which implies that updates in a specific
tier, can leverage a downstream tier to shield the end-user from
disruptions, e.g., the application tier can leverage the reverse proxy
tier. Second, while application state is hard to extract and isolate, for
long lived connections with large objects, we can recreate the state
by identifying the partial requests at the old server and replaying
them to the new server. Together these insights allow us to transpar-
ently migrate from old to new code while restarting servers without
exposing the loss of state or server termination to the end-user.
Inourframework,anupdateisachievedbysignalingtheupstream
tier to handle connection migration and by indirectly externalizing
user-specific state and redirecting the state to an upstream tier. The
upstream tier redirects existing connections and the externalizes the
state to the new servers (or servers with the new code). Additionally,
zero downtime for a restarting L7LB is achieved by handing-over
traffic to side-car (instance with the new code) on the same machine.
To this end our framework consists of two mechanisms: a technique
for signaling and orchestrating connection hand-off via an upstream
tier, a method for detecting and externalizing state, and enhance-
ments to pre-existing hand-off kernel-based mechanisms to enable
them to scale to billions of users.
This framework has been deployed at Facebook for several years
and has helped to sustain an aggressive release schedule on a daily
basis. While comparing our framework to previously used release
methodologies,weobservedthatourframeworkprovidedthefollow-
ing benefits: (i) we reduced the release times to 25 and 90 minutes, for
the App. Server tier and the L7LB tiers respectively, (ii) we were able
to increase the effective L7LB CPU capacity by 15-20% , and (iii) pre-
vent millions of error codes from being propagated to the end-user.
2 BACKGROUND AND MOTIVATION
In this section, we introduce Facebook’s end-to-end web serving
infrastructure and present motivational measurements from the
production clusters.
2.1 Traffic Infrastructure
Figure 1 provides an overview of Facebook’s geographically dis-
tributed multi-tiered traffic infrastructure, comprising of DataCenter
(order of tens) and Edge PoPs (Point-of-Presence, order of hundreds).
At each infrastructure tier, Facebook uses software load-balancers
(LB) to efficiently handle diverse user workload requirements and
QoE. Additionally, at the origin data centers, there are also applica-
tion servers in addition to the LBs.
• L4LB (Layer4LB): Facebook uses Katran, a transparent, XDP-
based [7], L4LB layer that serves as a bridge in between the network
routers and L7LB (proxies). Routers use ECMP [30] to evenly dis-
tribute packets across the L4LB layer, which in turn uses consistent
hashing [7, 26] to load-balance across the fleet of L7LBs.
• L7LB (Layer7LB): For L7 load-balancing, Facebook uses Prox-
ygen, an in-house proxy with responsibilities encompassing beyond
those a typical traditional L7LB shoulders. Operating in different
modes, it serves as the reverse proxy for load-balancing, forward
proxy for outbound requests and HTTP server. Proxygen is the heart
of traffic management at Facebook, supporting multiple transport
protocols (TCP, UDP), application protocols (HTTP/1.1, HTTP/2,
QUIC, publish/subscribe [45] etc.), serving cached content for CDN,
maintaining security (TLS etc.), health-checking and monitoring
upstream app. servers etc.
• App. Server tier: Resides in the DataCenter and ranges from
web (HHVM, django, custom apps. built leveraging the Proxygen
HTTP server library [1, 5, 53]) to special-purpose servers (e.g., Pub-
lish/Subscribe brokers [45]). HHVM servers (our focus in application
tier) are a general purpose application server for HACK [54], with
workloads dominated by short-lived API requests. However, they
also service long-lived workloads (e.g., HTTP POST uploads).
2.2 Life of a Request
In this section we present a detailed view of how user requests are
processedbythevarioustiersofourinfrastructureandindoingsowe
highlight different application workflows and how they are treated.
(1) Edge PoP serves as the gateway into our infrastructure for
a user’s request and connections (TCP and TLS). These user re-
quests/connections are terminated by the Edge Proxygen.
(2) Edge Proxygen processes each request and, if the request can-
not be serviced at the Edge, it forwards the request to the upstream
Origin DataCenter. Otherwise, for cache-able content (e.g., web,
videos etc.) it responds to the user using Direct Server Return [7].
(3) Edge and Origin maintains long-lived HTTP/2 connections
over which user requests and MQTT connections are forwarded.
(4) Origin Proxygen forwards the request to the corresponding
App. Server based on the request’s context (e.g., web requests to
HHVM, django servers while persistent pub/sub connections to their
respective MQTT broker back-ends).
In this paper, we focus on restarts of Proxygen (at Edge and Origin)
and HHVM App. Server (at DataCenter), and focus on the traffic for
cache-able, uncache-able, and MQTT-backed content. Our goal is
to design a framework that shields transport (TCP and UDP) and
application protocols (HTTP and MQTT) from disruption, while still
maintaining reasonable update-speeds and zero downtime. This
work does not raise any ethical issues.
2.3 Release Updates
Traditionally, operators rely on over-provisioning the deployments
and incrementally release updates to subset of machines in batches.
Each restarting instance enters a draining mode during which it
receives no new connections (by failing health-checks from Katran
InternetL4LBEdge L7LBEdge L7LBEdge L7LBL4LBL4LBL4LBEdge L7LBEdge L7LBOrigin L7LBL4LBL4LBBackboneNetworkAppServersEdge PoP(order of hundred)Data Center(order of tens)Zero Downtime Release
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
(a) # of releases
(b) Root of L7LB restarts
(c) Root of app. tier restarts
(d) UDP mis-routing
Figure (2)
to remove the instance from the routing ring). This phase stays ac-
tive for the draining period [14, 15, 28], the time duration deemed
enough for existing connections to organically terminate. Once
draining period concludes, the existing connections are terminated
and the instance is restarted and the new code kicks-in.
2.4 Motivating Frequent Restarts
Toswiftlyaddresssecurityconcernsandadapttoevolvinguserexpec-
tations, frequent code releases have become the norm [21, 31, 50, 51],
not the exception. In Figure 2a, we present the number of global
roll-outs per week, over a period of 3 months for 10 Facebook’s Edge
and DataCenter clusters.
L7LB: Globally, at the L7LB tier, we observe on average three or
more releases per week. In Figure 2b, we analyze the root-cause of
these releases and observe that the dominants factors are binary (i.e.,
code) and configuration updates. We note that unlike other organiza-
tions, where configuration changes might not necessitate a release,
Facebook requires restarting the instances for configuration update.
This is an artifact of system design and, since Zero Downtime Release-
powered restarts do not results in disruptions, it removes the com-
plexityofmaintainingdifferentcodepaths,i.e.,oneforrobustrestarts
for binary updates and another for configuration-related changes.
Binary updates (due to code changes) always necessitatea restart and
account for∼47% of the releases, translating to multiple times a week.
App. Server: At the App. Server tier (Figure 2a), we observe that, at
themedian,updatesarereleasedasfrequentlyas100timesaweek[50,
51]. We also observed that each update contains any where from 10
to 1000 distinct code commits (Figure 2c) and such high degree of
code evolution necessitates frequent restarts. Conversations with
Facebook developers identified that the constant code changes to the
app. tier is a function a cultural adoption of the “Continuous Release”
philosophy. Although the App. Server tier evolves at a much higher
frequency, the impact of their restarts can be mitigated as L7LBs
terminate user connections and can shield the users from the App.
Server restarts. However, due to the stateful nature of the App. Server
tier and the high frequency of code updates (and thus restarts), some
users are bound to receive errors codes (e.g., HTTP 500) and timeouts.
2.5 Implications of Restarts
At Facebook scale, implications and consequences of frequent re-
leases can be subtle and far-reaching. The “disruption” induced by