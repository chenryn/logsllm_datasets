4.4 Handling Outliers
The main performance criterion used for evaluating
SocialCloud is the time required to finish computing tasks
for all nodes with tasks in the system. Accordingly, an
outlier (also called a computing straggler) is a node with
computational tasks that take a long time to finish, thus
increasing the overall time to finish and decreasing the
performance of the overall system. Detecting outliers in our
system is simple: since the total time is given in advance,
outliers are nodes with computing tasks that have longer
time to finish when other nodes participating in the same
outsourced computation are idle. Our method for handling
outliers is simple too: when an outlier is detected, we
outsource the remaining part of computations on all idle
nodes neighboring the original outsourcer. For that, we use
the same scheduling policy used by the outsourcer when
she first outsourced this task. In the simulation part, we
consider both scenarios of handled and unhandled outliers,
and observe how they affect the performance of the system.
Notice that our outlier handling requires an estimation
of tasks timing to be able to determine the time-to-finish,
and decided whether to reschedule a task or not. Alterna-
tively, we can use similar techniques like those used in other
distributed computing systems, like Mapreduce: once an
outlier is detected as the task that is taking longer than the
rest of other tasks to perform, it is rescheduled to other
available nodes without modifying it on the original node.
Depending on which node finishes the computation first,
the outsourcer then sends a signal to kill the computation
on the node that did not finish yet.
4.5 Deciding Workers Based on Resources
In real-world deployment of a system like SocialCloud, we
expect heterogeneity of resources, such as bandwidth,
storage, and computing power, in workers. This hetero-
geneity would result in different results and utilization
statistics of a system like SocialCloud, depending on which
nodes are used for what tasks. While our work does not
address this issue, and leaves it as a future work (c.f. Sections 6.6
and 8). We further believe that simple decisions can be
made in this regard so as to meet the design goals and
achieve the good performance. For example, we expect that
nodes would select workers among their social neighbors
that have resources and link capacities exceeding a thresh-
old, thus meeting an expected performance outcome.
5 SIMULATOR OF SOCIALCLOUD
To demonstrate the potential of SocialCloud as a computing
paradigm, we implement a batch-based simulator [21] that
considers a variety of scheduling algorithms, an outlier
handling mechanism, job generation handling, and failure
simulation.
The flow of the simulator is in Fig. 3. First, a node factory
uses the bootstrapping social graph to create nodes and
workers. Each node then decides on whether she has a task
Fig. 3. Flow diagram of SocialCloud: social graph is used for
bootstrapping the computing service and recruiting workers, nodes are
responsible for scheduling their tasks, and each worker uses its local
scheduler to divide compute time on neighbors’ compute tasks.
.
the interaction-based metric for
Interaction-based:
estimating the strength of ties between two nodes
captures the volume of interactions between those
nodes over time. Let the neighbors of a node vi be
NðviÞ ¼ fvi1; . . . ; vidg (where the degree of vi is d).
Let the volumes of interactions between the node vi
and its neighbors listed above be Ivi ¼ fIvi1 ; . . . ; Ividg
as observed over a period of time. The node vi, when
having a task to outsource, uses the interaction
volumes observed with other nodes as an indicator
for trust and weights the amount of computations
outsourced to them accordingly. That is, for a node
vix that is a neighbor of vi, the portion of computa-
tions outsourced is Iix=
P
8j Iij.
4.3.2 Evaluation of Trust-Based Scheduling
While evaluating the performance of SocialCloud under
those trust-based scheduling policies using the same
metric defined in Section 6.1 and used with other policies
is straightforward, it is worth noting that both models have
a slightly different implication on the underlying graphs,
which is worth considering in the experiments. It has been
observed in the literature that an adversary that introduces
malicious nodes in a social graph (e.g., similar to those
created in Sybil attacks, attacks in which a single node
creates multiple identities and pretends as if it is multiple
nodes; further details on those attacks and defenses are
in [38]) still cannot afford to create many meaningful inter-
actions with real social network nodes. Similarly, while it is
potentially possible to create a single association with a node
in a social network, it is quite harder to do that at scale by
creating associations with that node and a large portion of its
friends. Accordingly, if that adversary is to be considered in
the scheduling system, it will always end up with a small
portion of the computations outsourced to it. Even in case
where computations are not performed in a timely manner,
re-outsourcing them to a more trusted node would not
substantially delay the overall computation time (with respect
to the evaluation metric explained below for evaluating the
performance of SocialCloud).
To capture this difference from scheduling policies, we
assume that there is a small portion of nodes ðpdÞ that is
controlled by the adversary. Also, given that we lack any
information on a ground truth of the volume of interactions
of this adversary and other honest nodes, we assume a budget
of interactions distributed among all nodes controlled by this
adversary and study how that impacts the evaluation metric.
MOHAISEN ET AL.: TRUSTWORTHY DISTRIBUTED COMPUTING ON SOCIAL NETWORKS
339
or not, and if she has a task she schedules the task according
to her scheduling algorithm. If needed, each node then
transfers code on which computations are to be performed
to the worker along with the chunks of the data for these codes
to run on. Each worker then performs the computation
according to her scheduling algorithm and returns results to
the outsourcer. The implemented components of SocialCloud
are described in the previous section.
Timing
5.1
In SocialCloud, we use virtual time to simulate computations
and resources sharing. We scale down the simulated time
by 3 orders of magnitude of that in reality. This is, for every
second worth of computations in real-world, we use one
millisecond in the simulation environment. Thus, units of
times in the rest of this paper are in virtual seconds.
6 RESULTS AND ANALYSIS
In this section, and to derive insight on the potential of
SocialCloud, we experiment with the simulator described
above. Before getting into the details of the experiments,
we describe the data and evaluation metric used in this
section.
6.1 Evaluation Metric
To demonstrate the potential of operating SocialCloud, we
use the ‘‘normalized finishing time’’ of a task outsourced
by a user to other nodes in the SocialCloud as the performance
metric. We consider the same metric over the different
graphs used in the simulation. To demonstrate the perfor-
mance of all nodes that have tasks to be computed in the
system, we use the empirical CDF (commutative distribution
function) as an aggregate measure.
the CDF is defined as
FXðxÞ ¼ PrðX  xÞ. In our experiments, the CDF measures
the fraction (or percent) of nodes that finish their tasks
before a point in time x, as part of the overall number of
tasks. We define x as the factors of time of normal operation
per dedicated machines, if they were to be used instead of
outsourcing computations. This is, suppose that the overall
time of a task is Ttot and the time it takes to compute the
subtask by the slowest worker is Tlast, then x for that node is
defined as Tlast=Ttot.
For a random variable X,
Tasks Generation
6.2
To demonestrate the operation of our simulator and the
trade-off our system provides, we consider two different
approaches for the tasks generated by each user. The size of
each generated task is measured by virtual units of time,
and for our demonstration we use two different scenarios.
1) Constant task weight each outsourcer generates tasks
with an equal size. These tasks are divided into equal
shares and distributed among different workers in the
computing system. The size of each task is T . 2) Variable
task weight each outsourcer has a different task size. We
model the size of tasks as a uniformly distributed random
variable in the range of ½ T   ‘; T þ ‘ for some T
‘. Each
worker receives an equal share of the task from the
outsourcer. The generation of a variable task weight would
result in non-uniform load among neighbors for tasks to
9
compute, and would be an enabler for policies like shortest
(or longest) first and their relative performance.
G
p
G
6.3 Deciding Tasks Outsourcers
Not all nodes in the system are likely to have tasks to
outsource for computation at the same time. Accordingly,
we denote the fraction of nodes that have tasks to compute
by p, where 0
1. In our experiments we use p from 0.1
to 0.5 with increments of 0.1. We further consider that each
node in the network has a task to compute with probability
p, and has no task with probability 1   pVthus, whether a
node has a task to distribute among its neighbors and
compute or not follows a binomial distribution with a
parameter p. Once a node is determined to be among nodes
with tasks at the current round of run of the simulator, we
fix the task length. For tasks length, we use both scenarios
mentioned in Section 6.2; with fixed or constant and
variable tasks weights.
6.4 Social Graphs
To derive insight on the potential of SocialCloud, we run our
simulator on several social graphs with different size and
density, as shown in Table 2. The graphs used in these
experiments represent three co-authorship social struc-
tures (DBLP, Physics 1, and Physics 2), one voting network
(of Wiki-vote for wikipedia administrators election), and
one friendship network (of the consumer review website,
Epinion). Notice the varying density of these graphs, which
also reflects on varying topological characteristics. Also,
notice the nature of these social graphs, where they are built
in different social contexts and possess varying qualities of
trust that fits to the application scenario mentioned earlier.
The proposed architectural design of SocialClould, however,
minimally depends on these graphs, and other networks
can brought instead of them. As these graphs are widely
used for verifying other applications on social networks, we
believe they enjoy a set of representative characteristics to
other networks as well.
6.5 Main Results
In this section we demonstrate the performance of
SocialCloud and discuss the main results of this work under
various circumstances. Due to the lack of space, we delegate
additional results to the technical report in [39]. For all
measurements, our metric of performance and comparison is
the normalized time to finish metric, as explained in Section 6.1.
We note that all of the experiments, unless otherwise is
mentioned, do not consider an adversary in place. We
consider the adversarial model described in Section 3.4 we
describing the results of trust-based scheduling.
6.5.1 Number of Outsourcers
In the first experiment, we run our SocialCloud simulator on
the different social graphs discussed earlier to measure the
evaluation metric when the number of the outsourcers of
tasks increases. We consider p ¼ 0:1 to 0.5 with increments
of 0.1 at each time. The results of this experiment are in
Fig. 4. On the results of this experiment we make several
observations.
First, we observe the potential of SocialCloud, even when
the number of outsourcers of computations in the social
340
IEEE TRANSACTIONS ON SERVICES COMPUTING, VOL. 7, NO. 3,
JULY-SEPTEMBER 2014
Fig. 4. Normalized time it takes to perform outsourced computations in SocialCloud. Different graphs with different social characteristics have
different performance results, where those with well-defined social structures have self-load-balancing features, in general. These measurements are
taken with round-robin scheduling algorithm that uses the outlier handling policy in Section 4.4 for a fixed task size (of 1000 simulation time units). (a)
Physics 1. (b) DBLP. (c) Epinion. (d) Wiki-vote.
network is as high as 50 percent of the total number of nodes,
which translates into a small normalized time to finish even
in the worst performing social graphs (about 60 percent of all
nodes with tasks would finish in 2 normalized time units).
However, this advantage varies for different graphs: we
observe that sparse graphs,
like co-authorship graphs,
generally outperform other graphs used in the experiments
(by observing the tendency in the performance in Figs. 4a
and 4b versus Figs. 4c and 4d). In the aforementioned graphs,
for example, we see that when 10 percent of nodes in each
case is used, and by fixing x, the normalized time, to 1, the
difference of performance is about 30 percent. This difference
of performance can be observed by comparing the Physics
co-authorship graphsVwhere 95 percent of nodes finish their
computationsVand the Epinion graphVwhere only about
65 percent of nodes finish their computations at the same time.
Second, we observe that the impact of p, the fraction of
nodes with tasks in the system, would depend greatly on
the underlying graph rather than p alone. For example, in
Fig. 4a, we observe that moving from p ¼ 0:1 to p ¼ 0:5 (when
x ¼ 1) leads to a decrease in the fraction of nodes that finish
their computations from 95 percent to about 75 percent. On
the other hand, for the same settings, this would lead to a
decrease from about 80 percent to 40 percent, a decrease from
about 65 percent to 30 percent, and a decrease from 70 percent
to 30 percent in DBLP, Epinion, and Wiki-vote, respectively.
This suggests that the decreases in the performance are due to
an inherit property of each graph. The inherit property of
each graph and how it affects the performance of SocialCloud
is further illustrated in Fig. 5. We find that even when DBLP’s
size is two orders of magnitude the size of Wiki-vote, it
outperforms Wiki-vote when not using outlier handling,
and gives almost the same performance when using it.
6.5.2 Scheduling Policy
Now, we turn our attention to understanding the impact
of the different scheduling policies discussed in Section 4.2
on the performance of SocialCloud. We consider the different
datasets, and use p ¼ 0:1 to 0.5 with 0.2 increments (the
results are shown in Fig. 6). The observed consistent pattern
in almost all figures in this experiment tells that shortest
first policy always outperforms the round robin scheduling
policy, whereas the round robin scheduling policy outperforms
the longest first. This pattern is consistent regardless of