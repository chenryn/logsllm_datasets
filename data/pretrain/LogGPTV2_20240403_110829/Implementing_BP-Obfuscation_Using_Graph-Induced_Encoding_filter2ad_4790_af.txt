old version of NTL. The old version of NTL uses only scalar integer
operations, and is very naive and very cache-unfriendly; also, its
performance is not sensitive to the size of t, so we only collected
data for 60-bit t. For 20-bit t, the current version of NTL is about
35× faster than the old version for matrix multiplication, and about
70× faster for inversion. Looking at NTL vs FLINT, we see that for
60-bit t, the times are pretty close; as we already saw, for 20-bit t,
NTL’s floating point strategy gives it a huge advantage over FLINT.
7 IMPLEMENTATION DETAILS
7.1 Initialization, Obfuscation, and Evaluation
Our program includes three main stages: initialization, which gener-
ates the public and secret keys for the encoding scheme, obfuscation,
which uses the secret key to obfuscate a given branching program,
and evaluation, which computes the obfuscated program value on
a given input (using the public key). After each stage we write the
results to disk, then read them back as needed in the next stages.
12
q
I
(cid:16) R
(cid:17) (R|I) as per Section 4.1 (with
Initialization. This is where we choose the public and secret keys.
As described in Section 3, this includes choosing random node ma-
trices Ai with their trapdoors Ri, and also the “inner transformation
matrix” Pi ∈ Zn×n and “outer transformation matrix” Ti ∈ Zm×m
.
Once Ri and Ai are chosen, we compute the perturbation co-
variance matrix Σp = σx I − σz
σx and σz as derived in Section 5), then compute the conditional
covariance matrices as per Eqn. (3) in Section 4.4 (and we optimize
it as in Section 6.4). We also compute the modular inverses of the
transformation matrices, namely P−1 and T−1. (See Section 6.2.)
Since keeping all these matrices in memory consumes a lot of
RAM (especially T and T−1), our initialization phase processes one
node at a time, writing all the matrices to disk as soon as it computes
them and before moving to the next node.
Obfuscation. Given a branching program to obfuscate, we first
randomize it as described in Section 2, where for each matrix in
the program we generate a pair of higher-dimension “real” and
“dummy” matrices. We then use the trapdoors and transformation
matrices that we computed to encode the resulting pairs of matrices.
The most expensive parts of this stage are the trapdoor sampling
and the multiplication by the transformation matrices T and T−1,
all of which are part of the GGH15 encoding procedure.
Here too, we need to conserve memory, and so we only keep in
RAM one or two GGH15 nodes at a time. As the real and dummy
matrices in each pair are encoded relative to different edges, we
cannot encode them together. Hence, we first generate all the |Σ|
real/dummy pairs for the current input symbol and keep them all
in memory. (These matrices only take little memory.) We then read
from disk the edge on the “real” path and encode the “real” matrices
from all the pairs. Finally, we read the edge on the “dummy” path
and encode the “dummy”” matrices from all the pairs.
Evaluation. Once we have all the encoded matrices written on disk
and we are given the input string, we simply read the corresponding
matrices from disk, multiply them, subtract the “dummy” from the
“real” product and check for smallness. One important consideration
here is the order in which to multiply the matrices. Recall from
Section 3.2 that encodings relative to the sink-bound edges consist
of a single vector. So, it is much better to begin with these matrices
and then multiply backwards. In this way, we only need to do
matrix-vector products as we accumulate the multipliers, rather
than full matrix-matrix products. This optimization is one reason
why evaluation is many orders of magnitude faster than obfuscation.
(Another reason is that we only need to process two matrices per
step when evaluating, while during obfuscation we had to generate
2|Σ| matrices per step.)
7.2 Parallelization Strategies
We implemented and tested various multi-threading strategies, try-
ing to parallelize the computation at different levels. Below we
describe these different strategies, focusing mostly on the initializa-
tion and obfuscation stages (which are much more expensive). We
briefly touch on parallelism during the evaluation stage at the end.
7.2.1 Parallelism Across Different Nodes. The easiest strategy to
implement is a high-level strategy in which all of the nodes of the
graph are processed in parallel. This is trivial to implement, as the
Session D1:  Functional Encryption and ObfuscationCCS’17, October 30-November 3, 2017, Dallas, TX, USA794n = 4096
4
1.28
0.89
16
0.52
0.52
threads
NTL
FFLAS
FLINT
1
3.7
3.1
15.6
n = 8192
4
1
9.49
26.4
21.9
6.62
111.8
16
3.12
3.65
(a) Multiplication
n = 4096
4
1.78
16
1.27
threads
NTL
FFLAS
FLINT
1
4.8
6.7
32.6
n = 8192
4
1
37.9
11.8
43.2
219.9
16
7.6
(b) Inversion
Table 2: Time (in seconds) for multiplication and inversion of n × n matrices modulo a 20-bit prime
# bits
NTL
FLINT
NTL (old)
n = 4096
60
20
27.7
3.7
15.6
30.6
125.2
n = 8192
20
26.4
111.8
60
194.5
214.8
1000.9
(a) Multiplication
# bits
NTL
FLINT
NTL (old)
n = 4096
60
20
45.0
4.8
32.6
57.7
333.2
(b) Inversion
n = 8192
20
37.9
219.9
60
354.5
402.5
2776.3
Table 3: Time (in seconds) for multiplication and inversion of n × n matrices modulo 20- and 60-bit primes
computation at each node is independent of all other nodes. For
small parameters, this strategy works quite well. Unfortunately, it
does not scale very well, as it requires the data for many nodes to
be in memory at the same time. We found that this strategy quickly
consumed all available RAM, and ultimately had to be abandoned.
Instead, we opted to process the nodes in the graph sequentially,
and parallelized computations inside each node, as described below.
7.2.2 Trapdoor Sampling. As discussed in Section 3, when en-
coding a matrix M w.r.t. edge i → j, we choose a low-norm E and
compute B = [MAj +E]q, then use trapdoor sampling to find a small
norm matrix C such that AiC = B (mod q). This trapdoor sampling
routine samples each column of C separately, by invoking the trap-
door sampling procedure from Section 4.1 to solve AiCk = Bk
(mod q) (with Bk the k’th column of B and Ck the corresponding
column of C). In our implementation we therefore parallelize across
the different columns, sampling together as many of these columns
as we have threads. As discussed in Section 4.5, we used a stash
to speed up the computation, and we implemented the stash in a
thread-safe manner so that it could be shared between the threads.
We note that it is also possible to parallelize trapdoor sampling at
a lower level: specifically the procedure for solving A(cid:174)c = (cid:174)b involves
solving G(cid:174)z = (cid:174)u with G the “gadget matrix”. Due to the structure
of G, we can sample the entries of (cid:174)z in batches of size ek, where all
the batches can be processed independently. Although we did not
test it, we expect this strategy to perform worse than parallelism
across the different columns.
7.2.3 Gaussian Sampling. As discussed in Section 4.4, during
initialization we compute the conditional mean and covariance
matrices as in Eqn. (3). This computation essentially has the same
structure as standard Gaussian elimination, and we implemented a
parallel version of it as described in Section 6.4.
7.2.4 CRT-level Parallelism. A significant amount of time is
spent performing matrix multiplication and inversion operations
over Zq. Since q is chosen to be the product of small co-prime
factors and the matrices represented using Chinese remaindering,
13
these matrix operations can be performed independently modulo
each small factor qi.
7.2.5
Lower-level Parallelism. We also implemented multi-threaded
versions of matrix multiplication and inversion modulo each of the
small factors in our CRT base. However, we found empirically that
it was more effective not to parallelize at this lowest level, but rather
at the higher CRT level.
7.2.6 Disk I/O Pipelining. Each of the three stages (initialize,
obfuscate, evaluate) reads its inputs from disk and writes its output
to disk. The amount of data transferred between main memory and
disk is huge, and we found that a significant amount of time was
just spent waiting for disk I/O operations to complete. The problem
was only made worse as the multi-threading strategy reduced the
computation time relative to the I/O time. To mitigate this problem,
we used a multi-threaded “pipelining” strategy. One thread was
dedicated to reading from disk, one thread was dedicated to writing
to disk, and the remaining threads are used for the actual computa-
tions. In this way, while the next block of data to be processed is
being read in, and the previous block of data is being written out,
the current block of data is being processed.
7.2.7 Parallelizing the Evaluation Stage. Recall that in the evalu-
ation stage, we have to multiply encodings along a “main path” and
along a “dummy path”. In our implementation, each path is pro-
cessed on a different thread. Specifically, the system sets one thread
for processing the dummy branch and one for the main branch (re-
gardless of the total number of threads set during run-time). Then,
when processing each branch, the programs sets the number of
threads to half of the overall number of threads set during run-time.
However, since only each node multiplication is parallelized, and
the run-time is relatively negligible for this function, we do not see
a difference in the run-time of the evaluation for different number
of threads (see Figure 8).
Session D1:  Functional Encryption and ObfuscationCCS’17, October 30-November 3, 2017, Dallas, TX, USA795L
5
6
8
10
12
14
16
17
18
17
19
20
3352
3932
5621
6730
8339
9923
10925
11928
12403
11928
13564
14145
Intel Xeon CPU,E5-2698 v3:
249.80
503.01
1865.67
4084.14
8947.79
18469.30
38926.50
44027.80
out-of-RAM
66.61
135.33
603.06
1382.59
3207.72
7748.91
11475.60
16953.30
20700.00
4 x 16-core Xeon CPUs:
16523.7
36272.9
46996.8
84542.3
182001.4
243525.6
m Initialization Obfuscation Evaluation
5.81
13.03
56.61
125.39
300.32
621.48
949.41
1352.48
646.46
1139.36
1514.26
Table 4: Running time (seconds) as a function of the
branching-program length, with security λ = 80, 100 states,
and a binary alphabet (L=BPlength, m=large dimension)
7.3 Results
Most of our testing was done on a machine with Intel Xeon CPU,
E5-2698 v3 @2.30GHz (which is a Haswell processor), featuring 32
cores and 250GB of main memory. The compiler was GCC version
4.8.5, and we used NTL version 10.3.0 and GMP version 6.0 (see
http://gmplib.org).
Because of memory limitations, the largest value of L we could
test on that machine was L = 17 (though initialization was also
possible for L = 18). These tests are described in the top part of
Table 4. For even larger parameters, we run a few tests on a machine
with 4×16-core Xeon CPUs and 2TB of DRAM. All these tests were