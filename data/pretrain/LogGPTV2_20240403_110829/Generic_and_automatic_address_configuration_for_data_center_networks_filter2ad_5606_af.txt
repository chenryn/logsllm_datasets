### Table 6: Estimated Time (ms) of Autoconfiguration

| Topology       | Timeout (ms) | TC (ms) | Mapping (ms) | LD (ms) | Total (ms) |
|----------------|--------------|---------|--------------|---------|------------|
| BCube(8,4)     | 150          | 10      | 20           | 10      | 310        |
| FatTree(20)    | 150          | 10      | 1710         | 10      | 2000       |
| FatTree(100)   | 150          | 6       | 20           | 6       | 262        |
| VL2(20,100)    | 150          | 17.6    | 5950         | 26      | 6223.6     |
| VL2(100,100)   | 150          | 7.5     | 360          | 9.2     | 606.7      |
| DCell(2,3)     | 150          | 17.1    | 1760         | 25.2    | 2032.3     |
| DCell(6,3)     | 150          | 15      | 3            | 15      | 353        |
| CCB            | 150          | 82.8    | 44970        | 125.3   | 45578.1    |

### 6.4 Results for Malfunction Detection

Malfunctions that involve a change in degree can be easily detected. In this section, we focus on simulations of miswirings where there is no change in degree. We evaluate the accuracy of our algorithm proposed in Figure 8 in detecting such malfunctions. Our simulations are performed on all four structures: BCube(6,4), FatTree(40), VL2(20,100), and DCell(3,3). These structures have tens of thousands of devices.

Miswirings without degree change are exceedingly rare and require at least four miswired devices. In our simulations, we randomly create five groups of such miswirings with a total of 20 miswired nodes. We check how many miswired nodes are detected based on the number (or percentage) of anchor points selected. A miswired node is considered detected only if there is no normal node above it in the counter list. This is because administrators will rectify the miswirings according to our list sequentially and stop once they encounter a node that is not actually miswired.

Figure 11 demonstrates the results. It clearly shows that the number of detected malfunctions increases with the number of selected anchor points. In our experiments on all structures, we can detect all malfunctions with at most 1.5% of nodes selected as anchor points. Interestingly, we find that the counter values of good nodes and bad nodes are well-separated, indicating a clear drop in the sorted counter value list. The number of required anchor points varies across different structures. For example, in DCell, we need up to 500 pairs of nodes as anchor points to detect all 20 miswired devices, while in VL2, we need 350 pairs. In BCube and FatTree, we only need 150 and 100 anchor points, respectively. The differences are due to the size of the selected networks and the potential for false positives in the `Anchor_Pair_Selection()` function.

It is worth noting that the malfunction detection has been done efficiently. In the worst case, it took 809.36 seconds to detect all 20 malfunctioning devices in DCell from 500 anchor points. Furthermore, calculations starting from different anchor points are independent and can be performed in parallel for further acceleration.

### 7. Related Work

In this section, we review work related to DAC. The differences between DAC and other schemes in areas such as Ethernet and IP networks are due to different design goals for different scenarios.

#### Data Center Networking
Portland [8] is perhaps the most related work to DAC. It uses a distributed location discovery protocol (LDP) for PMAC (physical MAC) address assignment. LDP leverages the multi-rooted tree topology property for switches to decide their levels, as only edge switches directly connect to servers. DAC differs from Portland in several aspects:
1. DAC can be applied to arbitrary network topologies, whereas LDP only works for multi-rooted trees.
2. DAC follows a centralized design, which significantly simplifies the protocol design in distributed systems, and data centers are typically operated by a single entity.

#### Plug-and-Play in Ethernet
Ethernet, one of the most widely used networking technologies, has the "plug-and-play" property. Each host in an Ethernet possesses a persistent MAC address, and Ethernet bridges automatically learn host addresses during communication. Flat addressing simplifies handling topology dynamics and host mobility without human input to reassign addresses. However, it suffers from scalability issues. Efforts like [25â€“27] aim to create a scalable bridge architecture. SEATTLE [28] proposes distributing ARP state among switches using a one-hop DHT, making significant advances toward a plug-and-play Ethernet. However, it still cannot support large data centers due to:
1. Switch state growing with end-hosts.
2. Routing requiring all-to-all broadcast.
3. Forwarding loops still existing [8].

#### Autoconfiguration in IP Networks
Autoconfiguration protocols for traditional IP networks can be divided into stateless and stateful approaches. In stateful protocols, a central server records state information about assigned IP addresses. When a new host joins, the server allocates a new, unused IP to avoid conflicts. DHCP [3] is a representative protocol in this category. Stateful approaches do not rely on a central server. A new node proposes an IP address and verifies its uniqueness using a duplicate address detection procedure. Examples include IPv6 stateless address autoconfiguration [29] and IETF Zeroconf [30]. However, neither can solve the autoconfiguration problem in new data centers where addresses contain locality and topology information.

### 8. Conclusion

In this paper, we designed, evaluated, and implemented DAC, a generic and automatic Data Center Address Configuration system. To our knowledge, this is the first work in address autoconfiguration for generic data center networks. At the core of DAC is its device-to-logical ID mapping and malfunction detection. DAC innovates by abstracting the device-to-logical ID mapping to the graph isomorphism problem and solving it with low time-complexity by leveraging the sparsity and symmetry (or asymmetry) of data center structures. The DAC malfunction detection scheme can detect various errors, including the most difficult cases where miswirings do not cause any node degree change.

Our simulation results show that DAC can accurately find all the hardest-to-detect malfunctions and can autoconfigure a large data center with 3.8 million devices in 46 seconds. In our implementation on a 64-server BCube testbed, DAC successfully autoconfigured all the servers in less than 300 milliseconds. Our implementation experience and experiments show that DAC is a viable solution for data center network autoconfiguration.

### Acknowledgments

We thank Geoffry Nordlund, Wei Shen, and Yaohui Xu for sharing insights on IP address assignment within MSRA research data center; Guohan Lu for testbed setup; Junfeng Zhou for educating us on operation and miswiring issues in large commercial data centers; and Xi Chen for discussions on the complexity of the general graph isomorphism problem. We also thank the anonymous reviewers for their constructive comments and our shepherd Michael Mitzenmacher for his insightful and detailed feedback, which improved the content and presentation of this paper.

### 9. References

[References remain unchanged]

This revised version aims to improve clarity, coherence, and professionalism in the text.