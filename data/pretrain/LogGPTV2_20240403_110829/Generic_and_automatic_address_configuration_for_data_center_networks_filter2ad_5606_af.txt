BCube(8,4)
FatTree(20)
FatTree(100)
VL2(20,100)
VL2(100,100)
DCell(2,3)
DCell(6,3)
CCB
120
120
80
80
80
80
170
250
timeout
150
150
150
150
150
150
150
150
TC
10
10
6
17.6
7.5
17.1
15
82.8
mapping
20
1710
20
5950
360
1760
3
LD
10
10
6
26
9.2
25.2
15
44970
125.3
Total
310
2000
262
6223.6
606.7
2032.3
353
45578.1
Table 6: Estimated time (ms) of autoconﬁguration.
address. We assume all the links are 1G=s and all communications
use the full link speed. For each structure, we choose the smallest
and largest graphs in Table 3 for evaluation. The results are shown
in Table 6. From the table, we ﬁnd that, except DCell(6,3), the auto-
conﬁguration can be ﬁnished in less than 10 seconds. We also ﬁnd
that, for big topologies like BCube(8,4), DCell(6,3), FatTree(100)
and VL2(100,100), the mapping time dominates the entire autocon-
ﬁguration time. DCell(6,3) takes the longest time, nearly 45 seconds,
to do the mapping. While the CPU time for the mapping is only 8.88
seconds, the memory I/O time is 36.09 seconds. Here we use more
powerful Linux servers than what we used in the implementation, so
the mapping here is relatively faster than that in Section 5.
6.4 Results for Malfunction Detection
Since malfunctions with degree change can be detected readily, in
this section we focus on simulations on the miswirings where there
is no degree change. We evaluate the accuracy of our algorithm pro-
posed in Figure 8 in detecting such malfunction. Our simulations
are performed on all 4 structures. For each one, we select a moder-
ate size with tens of thousands of devices for evaluation, speciﬁcally,
they are BCube(6,4), FatTree(40), VL2(20,100) and DCell(3,3). As
we know, miswirings without degree change are exceedingly rare and
every such case requires at least 4 miswired devices. So in our simu-
lations, we randomly create 5 groups of such miswirings with a total
of 20 miswired nodes. In the output of our algorithm, we check how
many miswired nodes we have detected versus the number (or per-
cent) of anchor points we have selected. We say a miswired node is
detected only if there is no normal node above it in the counter list.
This is because the administrators will rectify the miswirings accord-
ing to our list sequentially and stop once they come to a node that is
not really miswired.
Figure 11 demonstrates the results. It clearly shows that the number
of detected malfunctions is increased with the number of selected an-
chor points. In our experiments on all structure, we can detect all the
malfunctions with at most 1:5% of nodes selected as anchor points.
Interestingly, we ﬁnd the counter values of good nodes and those of
bad nodes are well separated, i.e., there is a clear drop in the sorted
counter value list. We also ﬁnd that for different structures, we need
different numbers of anchor points in order to detect all 20 miswired
devices. For example, in DCell we require as many as 500 pairs of
nodes as anchor points to detect all the malfuctions; and in VL2, we
need 350 pairs of nodes to detect them all. However, in BCube and
FatTree, we only need 150 and 100 anchor points, respectively, to
detect all malfunctions. One reason for the difference is that our se-
lected DCell and VL2 networks are larger than BCube and FatTree.
Another reason is that different structures can result in different false
positives in Anchor_Pair_Selection().
At last, it is worth mentioning that the above malfunction detection
has been done efﬁciently. In the worst case, we used 809.36 seconds
to detect all the 20 malfunctioning devices in DCell from 500 anchor
points. Furthermore, as mentioned before, the calculations starting
from different anchor points are independent of each other, and can
be performed in parallel for further acceleration.
7. RELATED WORK
In this section, we review the work related to DAC. The differences
between DAC and other schemes in related areas such as Ethernet
and IP networks are caused by different design goals for different
scenarios.
Data Center Networking. Portland [8] is perhaps the most related
work to DAC. It uses a distributed location discovery protocol (LDP)
for PMAC (physical MAC) address assignment. LDP leverages the
multi-rooted tree topology property for switches to decide their lev-
els, since only edge switches directly connect to servers. DAC differs
from Portland in several aspects: 1) DAC can be applied to arbitrary
network topologies whereas LDP only works for multi-rooted tree.
2) DAC follows a centralized design because centralized design sig-
niﬁcantly simpliﬁes the protocol design in distributed systems, and
further, data centers are operated by a single entity.
Plug-and-play in Ethernet. Standing as one of the most widely
used networking technologies, Ethernet has the beautiful property
of “plug-and-play”. It is essentially another notion of autoconﬁgu-
ration in that each host in an Ethernet possesses a persistent MAC
address and Ethernet bridges automatically learn host addresses dur-
ing communication. Flat addressing simpliﬁes the handling of topol-
ogy dynamics and host mobility with no human input to reassign ad-
dresses. However, it suffers from scalability problems. Many efforts,
such as [25–27], have been made towards a scalable bridge architec-
ture. More recently, SEATTLE [28] proposes to distribute ARP state
among switches using a one-hop DHT and makes dramatic advances
toward a plug-and-play Ethernet. However, it still cannot well sup-
30(0.2%) 60(0.4%)90(0.6%)120(0.8%)150(1.0%)180(1.2%)05101520Number of anchor pointsMalfunction detected  BCube150(0.3%)200(0.4%)250(0.5%)300(0.6%)350(0.7%)400(0.8%)05101520Number of anchor pointsMalfunction detected  VL260(0.33%)70(0.39%)80(0.44%)90(0.5%)100(0.56%)110(0.61%)05101520Number of anchor pointsMalfunction detected  FatTree100(0.3%)200(0.6%)300(0.9%)400(1.2%)500(1.5%)600(1.8%)05101520Number of anchor pointsMalfunction detected  DCell49port large data centers since: 1)switch state grows with end-hosts; 2)
routing needs all-to-all broadcast; 3) forwarding loop still exists [8].
Autoconﬁguration in IP networks. Autoconﬁguration protocols
for traditional IP networks can be divided into stateless and state-
ful approaches. In stateful protocols, a central server is employed to
record state information about IP addresses that have already been as-
signed. When a new host joins, the severs allocate a new, unused IP
to the host to avoid conﬂict. DHCP [3] is a representative protocol for
this category. Autoconﬁguration in stateless approaches does not rely
on a central sever. A new node proposes an IP address for itself and
veriﬁes its uniqueness using a duplicate address detection procedure.
For example, a node broadcasts its proposed address to the network,
if it does not receive any message showing the address has been oc-
cupied, it successfully obtains that address. Examples include IPv6
stateless address autoconﬁguration protocol [29] and IETF Zeroconf
protocol [30]. However, neither of them can solve the autoconﬁgu-
ration problem in new data centers where addresses contain locality
and topology information.
8. CONCLUSION
In this paper, we have designed, evaluated and implemented DAC,
a generic and automatic Data center Address Conﬁguration system.
To the best of our knowledge, this is the ﬁrst work in address auto-
conﬁguration for generic data center networks. At the core of DAC
is its device-to-logical ID mapping and malfunction detection. DAC
has made an innovation in abstracting the device-to-logical ID map-
ping to the graph isomorphism problem, and solved it in low time-
complexity by leveraging the sparsity and symmetry (or asymmetry)
of data center structures. The DAC malfunction detection scheme is
able to detect various errors, including the most difﬁcult case where
miswirings do not cause any node degree change.
Our simulation results show that DAC can accurately ﬁnd all the
hardest-to-detect malfunctions and can autoconﬁgure a large data cen-
ter with 3.8 million devices in 46 seconds. In our implementation on
a 64-server BCube testbed, DAC has used less than 300 milliseconds
to successfully autoconﬁgure all the servers. Our implementation ex-
perience and experiments show that DAC is a viable solution for data
center network autoconﬁguration.
Acknowledgments
We thank Geoffry Nordlund, Wei Shen, and Yaohui Xu for telling
us how IP addresses are assigned within MSRA research data center;
Guohan Lu for testbed setup; Junfeng Zhou for his help on educating
us the operation and miswiring issues in large commercial data cen-
ters; Xi Chen for helpful discussion on the complexity of the general
graph isomorphism problem. We thank the anonymous reviewers for
their constructive comments and our shepherd Michael Mitzenmacher
for his insightful and detailed feedback and suggestions, which im-
prove the content and presentation of this paper.
9. REFERENCES
[1] R. H. Katz, “Tech Titans Building Boom,” IEEE SPECTRUM,
Feb 2009.
[2] L. Barroso, J. Dean, and U. H(cid:127)olzle, “Web Search for a Planet:
The Google Cluster Architecture,” IEEE Micro, March 2003.
[3] R. Droms, “ Dynamic Host Conﬁguration Protocol,” RFC
2131, March 1997.
[4] S. Ghemawat, H. Gobioff, and S.-T. Leung, “The Google File
System,” in SOSP, 2003.
[5] J. Dean and S. Ghemawat, “MapReduce: Simpliﬁed Data
Processing on Large Clusters,” in OSDI, 2004.
[6] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu, “DCell: A
Scalable and Fault Tolerant Network Structure for Data
Centers,” in SIGCOMM, 2008.
[7] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian,
Y. Zhang, and S. Lu, “BCube: A High Performance,
Server-centric Network Architecture for Modular Data
Centers,” in SIGCOMM, 2009.
[8] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri,
S. Radhakrishnan, V. Subramanya, and A. Vahdat, “PortLand:
A Scalable Fault-Tolerant Layer 2 Data Center Network
Fabric,” in SIGCOMM, 2009.
[9] A. Greenberg, N. Jain, S. Kandula, C. Kim, P. Lahiri, D. Maltz,
P. Patel, and S. Sengupta, “VL2: A Scalable and Flexible Data
Center Network,” in SIGCOMM, 2009.
[10] [Online]. Available: http://royal.pingdom.com/2007/10/30/
human-errors-most-common-reason-for-data-center-outages/
[11] Z. Kerravala, “As the value of enterprise networks escalates, so
does the need for conﬁguration management,” The Yankee
Group, Jan 2004.
[12] Juniper, “What is behind network downtime?” 2008.
[13] [Online]. Available: http://searchdatacenter.techtarget.com/
news/column/0,294698,sid80_gci1148903,00.html
[14] “Graph isomorphism problem,”
http://en.wikipedia.org/wiki/Graph_isomorphism_problem.
[15] B. D. McKay, “Practical graph isomorphism,” in Congressus
Numerantium, 1981.
[16] P. T. Darga, K. A. Sakallah, and I. L. Markov, “Faster
Symmetry Discovery using Sparsity of Symmetries,” in 45st
Design Automation Conference, 2008.
[17] D. Li, C. Guo, H. Wu, K. Tan, Y. Zhang, and S. Lu, “FiConn:
Using Backup Port for Server Interconnection in Data
Centers,” in Infocom, 2009.
[18] E. M. Luks, “Isomorphism of graphs of bounded valence can
be tested in polynomial time,” in Journal of Computer and
System Sciences, 1982.
[19] “Graph automorphism,”
http://en.wikipedia.org/wiki/Graph_automorphism.
[20] P. T. Darga, M. H. Lifﬁton, K. A. Sakallah, and I. L. Markov,
“Exploiting Structure in Symmetry Generation for CNF,” in
41st Design Automation Conference, 2004.
[21] “Data Center Network Overview,” Extreme Networks, 2009.
[22] “Maximum common subgraph problem,”
http://en.wikipedia.org/wiki/Maximum_common_subgraph
_isomorphism_problem.
[23] V. Kann, “On the approximability of the maximum common
subgraph problem,” Annual Symposium on Theoretical Aspects
of Computer Science, 1992.
[24] “Personal communications with opeartor of a large enterprise
data center,” 2009.
[25] T. Rodeheffer, C. Thekkath, and D. Anderson, “SmartBridge:
A scalable bridge architecture,” in SIGCOMM, 2000.
[26] A. Myers, E. Ng, and H. Zhang, “Rethinking the service
model: scaling Ethernet to a million nodes,” in HotNets, 2004.
[27] R. Perlman, “Rbridges: Transparent routing,” in Infocom, 2004.
[28] C. Kim, M. Caesar, and J. Rexford, “Floodless in SEATTLE: a
scalable ethernet architecture for large enterprises,” in
SIGCOMM, 2008.
[29] S. Thomson and T. Narten, “IPv6 Stateless Address
Autoconﬁguration,” Expired Internet Draft, December 1998.
[30] S. Cheshire, B. Aboba, and E. Guttman, “Dynamic
conﬁguration of IPv4 link-local addresses,” IETF Draft, 2003.
50