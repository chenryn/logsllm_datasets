the other hand, for online detection, Decamouﬂage is to tell
whether input images are attack images or benign images
during running time.
B. Metrics for Decamouﬂage
Decamouﬂage is basically built on the three image-scaling at-
tack detection approaches presented in Section III. Therefore,
it is essential to quantify the differences between attack images
and benign images for each approach.
Here, we recommend using MSE and SSIM [22] for scaling
detection III-A and ﬁltering detection III-B methods. We
considered several metrics, such as peak signal-to-noise ratio
(PSNR). However, we observed that MSE and SSIM are
most suitable for Decamouﬂage. Unlike MSE and SSIM,
we observed that PSNR could be ineffective in showing a
threshold to distinguish benign images from attack images
even though PSNR is also popularly used to calculate the
physical difference between the two images. We surmise that
this is due to peak errors that can signiﬁcantly affect PSNR
values. On the other hand, MSE relies on the cumulative
squared errors that soften the difference between the benign
and its rescaled or ﬁltered counterpart into lower level, which
can reduce the effects of peak errors.
As for the steganalysis detection method III-C, we recom-
mend using the number of centered spectrum points (CSP).
The deﬁnition of each metric is as follows:
• MSE computes the average of the squares of the dif-
ferences between two images A and B as given in
Equation 5, where y(i,j) is the pixel in the image A;
(cid:101)y(i,j) is the pixel in the image B; and m, n are the size
of both images. In Decamouﬂage, we use the same size
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
67
of input images A and B.
m(cid:88)
n(cid:88)
(cid:0)y(i,j) −(cid:101)y(i,j)
(cid:1)2
j=1
i=1
M SE =
1
mn
or steganalysis), calculates metrics (MSE, SSIM, or CSP),
and compares the calculated metric values with a predeﬁned
threshold to determine whether I is an attack image crafted
by the image-scaling attack or not.
(5)
• SSIM index is another popularly used metric to compute
the similarities of local luminance, contrast, and structure
between two images due to its excellent performance and
simple calculation. The SSIM index can be calculated in
windows with different sizes (block unit or image unit)
for two images. The SSIM index between two images A
and B can be calculated as follows:
SSIM (A, B) =
(2µAµB + c1) (2σAB + c2)
A + µ2
B + c1) (σ2
(µ2
A + σ2
where µAµB are the average of A and B; σ2
B and
σAB are their variance and covariance, respectively. Here,
c1 and c2 are variables to stabilize the division with weak
denominator.
B + c2)
A + σ2
(6)
• CSP is the number of centered spectrum points on an
image in the frequency domain space. To count
this
number from a given image, we ﬁrst apply the DFT
operation and then apply a low pass ﬁlter to allow only
low frequencies to be retained. Given a radius value DT
as a threshold, our low pass ﬁlter can be modeled as
follows:
(cid:26)1 if D(u, v) ≤ DT
0 if D(u, v) > DT
H(u, v) =
(7)
Finally, after applying the low pass ﬁlter on the image,
we obtain a binary spectrum image containing low fre-
quencies only. The number of bright low-frequency points
is then automatically counted by using a contour detection
function. This process is visualized in Figure 7.
Fig. 7: Process of computing the centered spectrum points on an original
image and an attack image. Given an image, we ﬁrst apply the FT operation
and then apply a low pass ﬁlter to extract the low frequencies of the image only
(see ‘Binary spectrum’). Finally, we count the number of centered spectrum
points using a contour detection algorithm. In this example, we can see three
centered spectrum points in the attack image while there is only one centered
spectrum point in the original image.
C. Overview of Decamouﬂage
The overview of Decamouﬂage is illustrated in Figure 8. We
present Algorithm 1, 2, and 3 to detail three different detection
methods (scaling, ﬁltering, and steganalysis). In each detection
method, given an input image I (which can potentially be
an attack image) for image-scaling operations, Decamouﬂage
ﬁrst runs the image processing operations (scaling, ﬁltering,
Fig. 8: Overview of Decamouﬂage.
Algorithm 1 describes the computational procedure of the
scaling detection method. In this algorithm, we initially set
Attackf lag to F alse (line 3). We convert the input image I
into D using a downscaling operation and then convert D into
S using an upscaling operation (lines 4–5). Next, we calculate
either M SE(I,S) or SSIM(I,S) between I and S depending
on M etricf lag indicating which metric is used (line 6–12).
If the calculated metric value Score is greater than or equal
to the predeﬁned threshold ScoreT , we set Attackf lag to
F alse (lines 13–15). Similarly, we design Algorithm 2 and 3
for the second and third methods, which have similar steps to
Algorithm 1, but we skip the details of those algorithms due
to the paper page limit.
To use each method effectively, we strategically set the
threshold value for the method. Our recommended threshold
values are presented in Section V-A.
(cid:46) D: downscaled image
(cid:46) S: upscaled image
(cid:46) I: input image, M etricf lag: input metric ﬂag
Attack f lag ← F alse
D ← scale down(I)
S ← scale up(D)
if M etricf lag == T rue then
Algorithm 1 Scaling detection
1: procedure SCALING DETECTION(I, M etricf lag)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: end procedure
Score ← M SE(I,S)
ScoreT ← M SET
Score ← SSIM (I,S)
ScoreT ← SSIM T (cid:46) SSIM T : SSIM Threshold
end if
if Score ≥ ScoreT then
Attack f lag ← T rue
end if
return Attack f lag
(cid:46) M SET : MSE Threshold
else
Summary: As an answer to RQ. 2, we present Decamouﬂage
to detect image-scaling attacks in an automated manner. To
achieve this goal, we identify three metrics (MSE, SSIM,
and CSP) that can be effectively integrated with the three
techniques in Section III.
68
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
(cid:46) F : ﬁltered image
(cid:46) I: input image, M etricf lag: input metric ﬂag
Attack f lag ← F alse
F ← minimum f ilter(I)
if M etricf lag == T rue then
Score ← M SE(I,F )
ScoreT ← M SET
Score ← SSIM (I,F )
ScoreT ← SSIM T (cid:46) SSIM T : SSIM Threshold
Algorithm 2 Filtering detection
1: procedure FILTERING DETECTION(I, M etricf lag)
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16: end procedure
end if
if Score ≥ ScoreT then
Attack f lag ← T rue
end if
return Attack f lag
(cid:46) M SET : MSE Threshold
else
Algorithm 3 Steganalysis detection
1: procedure STEGANALYSIS DETECTION(I)
(cid:46) I: input
image
Attack f lag ← F alse
2:
C ← centered spectrum image(I)
3:
(cid:46) C: centered spectrum image
4:
B ← convert binary(C)
(cid:46) B: binary image
5:
CSP B ← Count the centered spectrum points in B
6:
7: (cid:46) CSP B: the number of centered spectrum points in B
if CSP B ≥ CSP T then
(cid:46) CSP T : CSP threshold
8:
Attack f lag ← T rue
9:
10:
11:
12: end procedure
end if
return Attack f lag
V. EVALUATION
This section describes the experiment setup and performance
evaluation for Decamouﬂage.
A. Experiment Setup
For a more practical testing environment, we consider evalu-
ating the performance of Decamouﬂage for an unseen dataset.
We used “NeurIPS 2017 Adversarial Attacks and Defences
Competition Track” [14] to select the optimal threshold values
and “Caltech 256 image dataset” [15] to evaluate the perfor-
mance of Decamouﬂage with the selected threshold values in
detecting image-scaling attacks.
We ﬁrst evaluate the Decamouﬂage detection performance
under the white-box setting to validate the feasibility and then
under the black-box setting to demonstrate its practicality.
The main challenging question we explore in evaluation is
as follows:
RQ. 3: How can we determine an appropriate threshold in
white-box or black-box settings?
White-box setting (Feasibility study): Following the identi-
ﬁed threat model, as presented in Section IV-A, we assume
in the white-box setting that we have full access to the
attacker’s mechanism to mainly demonstrate the feasibility of
a detection method. In this setting, we follow the steps shown
in Figure 9. In the ﬁrst stage, we randomly selected 1000
original images and 1000 target images, respectively, from the
“NeurIPS 2017 Adversarial Attacks and Defences Competition
Track” image dataset [14] and generate 1000 attack images by
combining original images and target images sequentially; and
we select the optimal thresholds with those images (we call
them training dataset). Next, in the second stage, we randomly
select 1000 original images and 1000 target images from the
“Caltech 256 image dataset” [15] and evaluate the detection
performance of each detection method with those images (we
call them evaluation dataset).
Fig. 9: White-box setting to validate the feasibility of Decamouﬂage. (a)
Threshold selection, and (b) evaluation.
To select
the optimal
threshold value for
the scaling
detection method presented in Section III-A, we calculate
M SE(o,S), M SE(a,S), SSIM (o,S), and SSIM (a,S) for all
o ∈ O and for all a ∈ A. Here, our goal is to show that
we can select threshold values to distinguish M SE(o,S) and
SSIM (o,S) from M SE(a,S) and SSIM (a,S), respectively.
Similarly, to select the optimal threshold value for the ﬁlter-
ing detection method presented in Section III-B, we calculate
M SE(o,F ), M SE(a,F ), SSIM (o,F ), and SSIM (a,F ) for all
o ∈ O and for all a ∈ A.
Again, to select the optimal threshold value for the ﬁltering
detection method presented in Section III-C, we calculate
CSP o and CSP a for all o ∈ O and for all a ∈ A. In the
following sections, we show that there exists a clear recom-
mended threshold value for each method, and the threshold
value can be determined in an automated manner with a
training dataset only.
Selecting the optimal threshold for a detection method
in the white-box setting: To determine the threshold of a
metric M for a detection method in the white-box setting,
we developed a gradient descent method that searches for
the optimal threshold. The proposed gradient descent method
computes the metric values for original images (Moriginal)
and attack images (Mattack), respectively,
in the training
dataset. Next, the gradient descent method picks a metric value
from Moriginal and Mattack, respectively, after ascendingly
grading them and determines the threshold as the middle point
69
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 08:54:52 UTC from IEEE Xplore.  Restrictions apply. 
between them to assess the detection accuracy. This process
is repeated until the highest detection accuracy is achieved.
As an example, Figure 10 shows the selected threshold result
for the scaling detection method. For all detection methods
presented in Section III, we selected the best thresholds using
this gradient descent method.
• FAR is the percentage of attack images that are classiﬁed
as benign images by a detection method.
• FRR is the percentage of benign images that are classiﬁed
as attack images by a detection method.
• Accuracy (Acc.) is the percentage of correctly classiﬁed
images by a detection method.
• Precision (Pre.) is the percentage of images classiﬁed as
attack images by a detection method, which are actual
attack images.
• Recall (Rec.) is the percentage of attack images that were
accurately classiﬁed by a detection method.
Fig. 10: Threshold selection results for the scaling detection method in the
white-box setting. The best threshold values are represented by the red dash
lines.
Black-box setting (Practicality study): The black-box setting
evaluates the practicality of a detection method with no as-
sumed knowledge of the attacking mechanism. In this scenario,
we need to determine the threshold with benign images alone
because there is no access to attack images. The black-box
setting also follows two stages shown in Figure 11. In the
ﬁrst stage, we compute the metric values (i.e., MSE, SSIM,
and CSP) with benign images in the training dataset and
analyze their statistical distributions to determine the metrics’
thresholds. In the second stage, we use the detection methods
with the selected thresholds to evaluate the performance of the
detection method with the evaluation dataset.
In general, while FRR is an indication of detection systems’
reliability, FAR shows the security performance. Ideally, both
FRR and FAR should be 0%. Often, a detection system tries
to minimize its FAR while maintaining an acceptable FRR as
a trade-off, especially under security-critical applications.
B. Results of the Scaling Detection Method