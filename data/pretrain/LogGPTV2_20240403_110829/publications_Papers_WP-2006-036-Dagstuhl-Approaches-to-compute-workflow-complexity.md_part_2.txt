complexity metric can be derived. Therefore, four main complexity perspectives can be
identified: activity complexity, control-flow complexity, data-flow complexity, and
resource complexity. The reader should realize that these perspectives are not complete.
Activity Complexity
- Number of activities
• •
Control-flow Complexity
Approve
Home Loan - XOR, AND, and OR splits/joins
• ⊕⊕⊕⊕ • • ⊕⊕⊕⊕ • - Loops
HoC mh ee Lck oan HoR me eje Lc ot an HomNo et i Lfy oan - Starting/Ending points
• • Client - …
Approve Data-flow Complexity
• • • ⊕⊕⊕⊕ • • Home Loan Conditionally• ⊕⊕⊕⊕ A• rchive - Data structures
ReLF qo uaill n est C L Th o ye pac enk EducC ah tioe nck Loan • • EducN Cato li io eti n nfy tLoan Application - - I In nt te er rf fa ac ce e d ine tf ein gi rt aio tin o n
• ⊕⊕⊕⊕ CA ap rp Lro ov ae n ⊕⊕⊕⊕ - …
Check • • Notify Resource Complexity
Car Loan Car Loan
Reject Client - Human resources
Car Loan - IS resources
- IT resources
- …
Figure 4. Perspective top workflow complexity
Activity complexity. This view on complexity simply calculates the number of
activities a process has. While this complexity metric is very simple, it is very important to
complement other forms of complexity. The control-flow complexity of a process can be
very low while its activity complexity can be very high. For example, a sequential process
that has a thousand activities has a control-flow complexity of 0, whereas its activity
complexity is 100. This metric was inspired by lines-of-code (LOC) metric used with a
significant success rate in software engineering (Jones 1986).
Control-flow complexity. The control-flow perspective describes activities and their
execution ordering through different constructors, which permit flow of execution control.
Constructors include sequence, choice, parallelism, splits, joins, loops, and ending and
starting points (Cardoso 2005). Splits allow defining the possible control paths that exist in
a process. Joins have a different role; they express the type of synchronization that should
be made at a specific point in the process. A control-flow complexity model needs to take
into account the existence of XOR-split/join, OR-split/join, AND-split/join, loops, etc.
Data-flow complexity. This perspective is layered on top of the control perspective.
Documents and other data objects flow between activities. Local and global variables of the
workflow are used in effect pre- and post-conditions of activity execution (Aalst and
Hofstede 2005). The data-flow complexity of a process increases with the complexity of its
data structures, the number of formal parameters of activities, and the mappings between
activities’ data (Reijers and Vanderfeesten 2004). A data-flow complexity metric can be
composed of several sub-metrics which include: data complexity, interface complexity, and
interface integration complexity (Cardoso 2005). While the first two sub-metrics are related
6
Dagstuhl Seminar, “The Role of Business Processes in Service Oriented Architectures”, 16-21 July 2006, Dagstuhl, Germany.
to static data aspects (data declaration), the third metric is more dynamic in nature and
focuses on data dependencies between the different activities of a process.
Resource complexity. The resource perspective provides an organizational structure
anchor to the workflow in the form of human and device roles responsible for executing
activities (Aalst and Hofstede 2005). Activities in a process need to access resources
during their executions. A resource is defined to be any entity (e.g. human resources, IS
resources, and IT resources) required by an activity for its execution, such as an external
document, a database, a printer, an external application, or role (Du, Davis et al. 1999; zur
Mühlen 1999). Resources, such as actors and roles, can be structured in the context of an
organization. The structure that is used to shape the different types of resources can be
analyzed to determine its complexity. This analysis can help managers to lower
administrative costs and better optimize resource utilization.
6 Complexity metrics for workflows
The overall goal of workflow complexity analysis is to improve the comprehensibility of
workflows. The graphical representation of most workflow specification languages
provides the user with the capability to recognize complex areas of workflows. Thus, it is
important to develop methods and measurements to automatically identify complex
workflows and complex areas of workflows. Afterwards, these workflows can be
reengineered to reduce their complexity. Because of the difficulty involved in determining
a single universal rating number to describe the complexity of workflows, it is necessary to
developed multi-dimensional metrics of complexity and then validated the metrics through
objective and subjective measurements.
In this section we provide a theoretical survey of complexity considerations and metrics
from several fields and we relate them to workflow modeling. We view the complexity of a
workflow in different dimensions, namely information complexity, cyclomatic complexity,
Kolmogorov complexity, cognitive complexity, and computational complexity. A further
empirical investigation might ultimately lead to establishing a complexity theory for
workflow models.
6.1 Information theory
Information theory is a statistical theory concerning the storage and transmission of data
by dealing with the limits and efficiency of information processing. Claude Shannon was
the first in 1948 to propose the use of binary digits for coding information. Information
theory has lead to the introduction of the concept of entropy (also called Shannon’s
entropy). The concept of entropy describes how much randomness, disorder or uncertainty
there is in a closed system. An alternative way to look at this is to talk about how much
information is present in a system. Entropy is defined in terms of a discrete random event x,
with possible states 1..n as:
n
H(x)=−∑
p(i)log p(i)
2
i=1
7
Dagstuhl Seminar, “The Role of Business Processes in Service Oriented Architectures”, 16-21 July 2006, Dagstuhl, Germany.
The concept of entropy can be applied to workflows since we consider them to be
complex systems.
Figure 5. Entropy and workflow complexity
Let us consider the workflow in Figure 5. A token present in an activity indicates that
the activity has been invoked during the enactment of the workflow. Tokens with the same
pattern belong to the same instance. Obviously some activities have more tokens than
others. We can compute the runtime complexity using the entropy function by considering
p(i) to be the probability of activity i to have a token. The frequency of tokens in some
activities is not very high (e.g. activities with only one token), while other have a high
frequency (e.g. the initial and final activity). Calculating the entropy can obtain a measure
of the disorder or randomness of workflow instances. It should be noticed that is a runtime
complexity metric.
6.2 Kolmogorov complexity
Kolmogorov complexity can also be used to compute the complexity of a workflow. By
definition, the Kolmogorov complexity of an object is the length of the shortest binary
program run on a universal Turing Machine (TM) that can reproduce the object. Formally,
assuming that we have a string description of a Turing machine M, denoted , an input i
and a string s, then the concatenated string  i is a description of s. Several descriptions
for s exist. Among all the descriptions there is one with shortest length denoted d(s). The
minimal length of the minimal description of is expressed with function K(s), i.e.,
K(s) = |d(s)|
As a result, the amount of information in a string is the size of the shortest program that
outputs the string. Kolmogorov complexity has been connected with Information Theory
and proved to be closely related to Claude Shannon’s entropy rate of an information source.
8
Dagstuhl Seminar, “The Role of Business Processes in Service Oriented Architectures”, 16-21 July 2006, Dagstuhl, Germany.
Figure 6. Kolmogorov complexity of workflows
Let us assume that we have a workflow repository with n activities (a). Each activity is
i
represented with function y f (x) as shown in Figure 6. The challenge that we propose is
i= i i
to compose a workflow – with the help of the activities available in the repository – which
outputs the string abcdabcd when the string aabbccdd is given as an input. Additionally,
we are looking for the workflow that has a minimal length description according to
Kolmogorov complexity. In Figure 6 two such workflows are illustrated. As it can be seen
the workflow from the left has a lower length description. One of the important
characteristics of the theory of Kolmogorov complexity stems from this independence of
the description method. For workflows this means that the complexity is independent of the
workflow used to model a process.
6.3 Cyclomatic complexity
In our work previous work (Cardoso 2005; Cardoso 2005) we have designed a processes
complexity metric that borrowed some ideas from McCabe’s cyclomatic complexity. Our
objective was to develop a metric that could be used in the same way as the MCC metric
but to evaluate processes' complexity.
One of the first important observations that can be made from the MCC control flow
graph, shown in Figure 7, is that this graph is extremely similar to a process. One major
difference is that the nodes of a MCC control flow graph have identical semantics, while
process nodes (i.e., activities) can have different semantics (e.g., AND-splits, XOR-splits,
OR-joins, etc). Our approach has tackled this major difference.
9
Dagstuhl Seminar, “The Role of Business Processes in Service Oriented Architectures”, 16-21 July 2006, Dagstuhl, Germany.
Figure 7. Cyclomatic complexity
The metric that we have previously developed and tested, called Control-flow
Complexity (CFC) metric, was based on the analysis of XOR-splits, OR-splits, and AND-
splits control-flow elements. The main idea behind the metric was to evaluate the number
of mental states that have to be considered when a designer is developing a process. Splits
introduce the notion of mental states in processes. When a split (XOR, OR, or AND) is
introduced in a process, the business process designer has to mentally create a map or
structure that accounts for the number of states that can be reached from the split. The
notion of mental state is important since there are theories (Miller 1956) suggesting that
complexity beyond a certain point defeats the human mind's ability to perform accurate
symbolic manipulations, and hence results in error.
Mathematically, the control-flow complexity metric is additive, thus it is very easy to
calculate the complexity of a process, by simply adding the CFC of all split constructs. The
control-flow complexity was calculated as follows, where P is a process and a an activity.
The higher the value of CFC (a), CFC (a), and CFC (a), the more complex is a
XOR OR AND
process design, since developer has to handle all the states between control-flow constructs
(splits) and their associated outgoing transitions and activities. Each formula to calculate
the complexity of a split construct is based on the number of states that follow the
construct. CFC analysis seeks to evaluate complexity without direct execution of processes.
The advantages of the CFC metric is that it can be used as a maintenance and quality
metric, it gives the relative complexity of process designs, and it is easy to apply.
Disadvantages of the CFC metric include the inability to measure data complexity, only
control-flow complexity is measured. Additionally, the same weight is placed on nested and
non-nested loops. However, deeply nested conditional structures are harder to understand
than non-nested structures.
10
Dagstuhl Seminar, “The Role of Business Processes in Service Oriented Architectures”, 16-21 July 2006, Dagstuhl, Germany.
6.4 Cognitive complexity
Cognitive complexity is related to cognitive psychology that aims at studying, among
other things, thinking, reasoning, and decision making. Cognitive complexity focuses on
the analysis of how complicated a problem is from the perspective of the person trying to
solve it. For workflow designers, the ability of coping with complexity is a fundamental
issue and influences the quality of the final product.
Cognition involves both short-term and long-term memories. Most definitions of short-