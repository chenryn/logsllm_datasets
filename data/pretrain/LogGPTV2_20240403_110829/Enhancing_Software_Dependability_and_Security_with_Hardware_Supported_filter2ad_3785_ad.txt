injection attacks that evade ASLR. In addition to that, it was
demonstrated that ILR can prevent ROP attacks that target
a vulnerable Linux PDF viewer, xpdf [9]. The vulnerability
allows attackers to create a shell by using crafted ROP attack.
Another vulnerability in Adobe PDF viewer (9.3.0) allows
arc-injection and ROP attacks [24]. It was demonstrated that
randomizing control ﬂow at instruction level can thwart attacks
from crafted malicious PDF ﬁles that exploit the gadget based
programming [9], [24]. In short, ILR is effective to mitigate
actual ROP attacks that exploit vulnerabilities in real world
applications using return based programming and gadgets.
B. Gadget Analysis
It has been shown that ILR can effectively reduce the
attacking surfaces. Consequently,
there is less chance for
attackers to ﬁnd enough number of gadgets to mount a ROP
attack. To evaluate how effective our approach is to reduce
attacking surfaces, we use an open source gadget tool called
ROPgadget [25]. ROPgadget can scan a binary program to ﬁnd
speciﬁc gadgets within the executable. It has an auto-roper
for build attack payload automatically with the gadgets found
and facilitate the process of ROP exploitation. ROPgadget
contains a database of gadget patterns. We use version 4.0.1 of
ROPgadget. ROPgadget has a gadget compiler that can create
attack payload using matching gadgets found in an executable
binary. The assembled payloads can facilitate development of
actual attacks depending on the vulnerabilities of the binary.
Note that the payloads themselves are not sufﬁcient to mount
a successful attack but they form an important part of ROP
exploits. The payloads can be converted into attacks when
combined with vulnerabilities. If ROPgadget fails to create
attack payloads using gadgets found in a binary executable,
it means that even there is exploitable vulnerability, ROP
attack cannot be mounted using the existing attack payload
template. Typically, ROPgadget requires detection of multiple
gadgets in an executable to assemble a payload. If control ﬂow
randomization signiﬁcantly reduces the number of gadgets that
can be found in a binary executable, the likelihood an attack
payload can be assembled will become smaller because of the
reduced gadget pool.
To model
the environment of virtual control ﬂow ran-
domization faced by attackers, we modiﬁed ROPgadget to
Fig. 10. Support for control transfer without using the return instruction.
Callee in x86 may access return address store in the stack explicitly without
using the call return instruction. The stack stores randomized return
addresses.
outside the context of function call. A bitmap is used to
track which stack location stores a randomized return address.
For higher performance, the bitmap can be stored in paged
memory. Similar to the randomization/de-randomization tables,
pages containing the bitmap are set to be invisible to the user
space instructions. A small cache can be used to store parts of
the bitmap that are frequently accessed.
D. Remarks
Our approach mainly affects the interface between instruc-
tion fetch and the memory hierarchy for storing instructions.
We carefully design our system so impacts to other micro-
architecture components are minimized. For example, the in-
troduction of two program counters (RPC and UPC) facilitates
normal operations of predictors for both branch directions
and branch targets. Both predictions can be based on the de-
randomized program counter as illustrated in Figure 7. During
instruction execution and fetch, when UPC is absent,
the
instruction fetch unit will ﬁrst de-randomize the randomized
PC and then use the de-randomized PC for branch prediction.
In such a way, branch prediction rates will not be affected by
how instruction layout and control ﬂow are randomized. Since
our approach only randomizes instruction address space, which
contains read-only data, it can be applied to multi-core or
multi-processor based systems with easy. In addition, control
ﬂow randomization can be conﬁned within the same page,
which will further reduce its impact to iTLB. At system level,
the main impact is to extend application context to include the
de-randomization/randomization tables.
258258
UPC is auto-managed by the execution pipeline. It cannot be
directly accessed by the instructions in the user space.
VI. PERFORMANCE EVALUATION
In order to demonstrate the performance of our system
design, we have conducted several experiments and simu-
lations using detailed architectural models. We studied the
overall performance impacts of our design using 11 SPEC
CPU2006 benchmarks [31]. Along with that, we examined
the overheads to support native execution of ILR transformed
programs by adopting our design such as efﬁciency of run-
time de-randomization, execution speed of each benchmark
application, and etc. In particular, we extended XIOSim [16]
simulators to meet our need.
A. Implementations
We implemented a static binary rewriter which can random-
ize the instruction space given a third-party binary program.
The output of the static binary rewriter is a binary ﬁle with
randomized instruction segments and lookup tables that can
be used to de-randomize the instruction space. Currently, the
rewriter only works for statically linked binary with all the
libraries embedded.
For performance modeling, we use XIOSim [16]. XIOSim
is a highly detailed cycle based micro-architectural simulator
targeted at x86 micro-processors. XIOSim is based on Zesto
simulator [17]. It models Intel x86 pipeline according to
best available public knowledge. Performance reported from
XIOSim’s models stays well within 10% of real hardware
for the entire SPEC CPU2006 suite [16]. XIOSim provides
detailed x86 architecture models for simulating in-order and
out-of-order pipelines. The models include detailed branch
predictors, branch target buffers, return address stack (RAS)
predictors, cache prefetchers, memory controllers, and main
memory/DRAM models. For power modeling, XIOSim inte-
grates a modiﬁed version of McPAT [32] to create a power
consumption trace. In terms of average power consumption,
XIOSim’s model has less than 5% deviation when compared
against real measurement. We modiﬁed and extended XIOSim
with the proposed architecture. The fetch stage of XIOSim
is modiﬁed to use randomized instruction space, and support
two program counters (RPC and UPC). The execution models
of call and return instructions are modiﬁed according
to our design. Instruction fetch is extended to support a de-
randomization cache (DRC). The DRC cache connects to a
uniﬁed second level cache shared by IL1 and DL1. A power
model for DRC is also integrated with XIOSim.
B. Benchmarks
For performance evaluation, we used the single thread
SPEC CPU2006 benchmark suite [31] that is a set of bench-
mark applications designed to test the CPU performance. We
tested eleven memory intensive benchmarks of the SPEC
the benchmarks used are bzip2,
CPU2006. In particularly,
gcc, mcf, hmmer, sjeng,
libquantum, h264ref,
lbm, xalan,
nsmd, and soplex. The detailed descriptions of the benchmarks
can be found in the webpage [31]. The simulation started
when the application passed the initialization stage. The cycle
based simulation executed each benchmark application for 500
million instructions or until it ﬁnished depending on which one
was longer.
Fig. 11.
Percentage of gadgets removed from SPEC CPU2006 benchmark
applications after randomization. The gadets are detected using an open source
gadget tool, ROPGadget
[25].
take into account control ﬂow randomization in such a way
that it searches for gadgets using un-randomized instruction
locations. We tested a set of SPEC CPU2006 benchmark ap-
plications using the modiﬁed ROPgadget. Without control ﬂow
randomization, for every tested SPEC benchmark application,
ROPgadget is able to assemble attack payloads. After virtual
control ﬂow randomization, for all the benchmark applications,
no attack payloads can be generated. This suggests that ran-
domizing control ﬂow can reduce the likelihood of successfully
mounting a ROP attack. Virtual control ﬂow randomization
can reduce the number of gadgets. As indicated by Figure 11,
on average 98% of gadgets are removed after applying our
control ﬂow randomization. In addition, many published papers
in the literature already show that address space randomization
is effective to resist ROP attacks.
C. Remarks
a) Entropy: As studied previously, ILR can have high
entropy, which defends against attacks that
try to evade
the protection by reducing the entropy of a system. Since
randomization is done at instruction granularity, there is a
large randomization space. Although we have used ROP as
a representative attack method of code reuse, return-to-libc
is also powerful attack for systems. However, Shacham et al.
already demonstrated that return-to-libc attack can be protected
using ILR with 64 bit address space [14]. For this reason, we
have used ROP as a threaten of code reuse attack in this paper.
b) Code Injection: Our primary focus of this study
is to mitigate code reuse attacks with hardware support of
ILR. Our solution by itself doesn’t address code injection
attacks [26], [8], [27]. However, our approach can be combined
with solutions that are speciﬁcally designed to mitigate code
injection attacks and used in conjunction with these solutions
to thwart both ROP and code injection based exploits.
c) Protection of Address Translations: Similar to all
randomization based approaches, a common practice to prevent
leaking randomization/de-randomization tables to the attackers
is to apply regular re-randomization of the binary images
that will create a new sets of address translation tables and
new randomized images. Even an attacker managed to obtain
the old randomization/de-randomization tables, the information
would be outdated for mounting new attacks. Furthermore,
for online attacks, since the randomization/de-randomization
tables associated with an application instance are invisible to
the instructions executed in the user space, most randomized
return addresses cannot be leaked to the remote attackers. Side
channel exploits such as those described in earlier projects are
not effective to cause randomized control ﬂow addresses to be
leaked to remote attackers [28], [29], [30]. Program counter
259259
0255075100Removal (%)ApplicationC. Machine Parameters
We modiﬁed the XIOSim simulator to simulate support
for runtime instruction space de-randomization. The operation
of our proposed scheme is veriﬁed with single issue, in-order
processor. For this reason, the simulation was performed with
a x86 single issue, in-order CPU model running at 1.6GHz.
The overall pipeline is divided into ﬁve major components or
blocks, fetch, decode, allocation (alloc), execution (exec) and
commit blocks. Each component may further comprise pipeline
stages, queues, and other structures. The detailed processor
model includes, branch predictor (2-level gshare), BTB (branch
target buffer), RAS, instruction queue, load-store queue, pre-
fetcher, and functional units. The fetch stage includes the
PC generation (i.e., branch prediction). The fetch stage of
the simulator operates on entire lines from the instruction
cache which are placed in the byte queue. A pre-decode
pipeline performs the initial decoding of the variable-length
x86 instructions to individual macro-ops, which are placed into
the instruction queue (IQ) with one macro-op per entry. From
here, the instructions proceed to the decoder pipelines. The
instruction queue size is 18. The I-TLB and D-TLB have 64
fully associative entries. The CPU has a 32-entry load/store
queue. The L1 instruction cache size is two-way 32KB, 64-
byte block size, and has an access latency of 2 cycles. The L1
data cache is a 32KB, 2-way associative, write back cache with
64-byte block size, and also has an access latency of 2 cycles.
The L2 cache is uniﬁed, 512KB size, 8-way associativity, 64-
byte block size, and has an 12-cycle access latency.
The simulator integrates DRAMSim2 [33] as the memory
model. DRAMSim is a cycle accurate open source JEDEC
DDRx memory system simulator. It provides a DDR2/3 mem-
ory system model. It uses open page policy, and therefore
attempts to schedule accesses to the same pages together to
maximize row buffer hits. The DRAM model tracks individual
ranks and banks, and accounts for pre-charge latencies, CAS
and RAS latencies, and refresh effects. We experimented with
different de-randomization cache sizes, from 64 translation
entries to 512 translation entries. Each entry supports 32-bit
instruction address translation.
VII. RESULTS ANALYSIS
For the benchmarks, the performance improvements of our
approach with 128-entry DRC lookup buffer over a straight-
forward implementation of ILR are shown in Figure 12, the
average speedup is 1.63 for all the benchmarks. For application
namd, h264ref, mcf, xalancbmk, our approach achieves more
than double speedup. We study the performance overhead
incurred by virtual control ﬂow randomization over the default
baseline of no randomization. We further experiment with
different sizes of DRC lookup buffer, 512 entries, 128 entries,
and 64 entries. Figure 13 shows the results. The results indicate
that increasing the size of DRC lookup buffer can improve the
overall IPC. When the DRC lookup buffer has 512 entries, the
average IPC for all the benchmark applications with virtual
control ﬂow randomization is almost 98.9% of the baseline
condition of no randomization. With a lookup buffer size of
64 entries, on average, the applications still maintain 97.9%
performance in terms of IPC, meaning 2.1% overhead.
Figure 14 shows miss rates of the DRC lookup buffer.
There are two settings, 512 entries and 64 entries. The average
DRC miss rate under 512-entry lookup buffer is 4.5%. When
the DRC entry size is 64, the average miss rate increases to
Fig. 12. Performance speedup using DRC over straightforward implementa-
tion of ILR. Y-axis shows IPC ratio. DRC setting: 128 entries. The average
speedup is 1.63 for all the tested benchmarks.
Fig. 13. Normalized IPC performance under different DRC sizes. Y-axis
shows normalized IPC over the baseline IPC of no randomization. The average
IPC slowdown is less than 2.1%.
20.6%. The results indicate that lbm and xalancbmk have the
worst DRC miss rates. Note that sometimes, DRC cache miss
rate is not the only factor that affects performance. When there
is a DRC cache miss, the system will look up the L2 cache,
which is large enough for storing de-randomization table. In
short, our approach of hardware assisted control ﬂow ran-
domization incurs very small overhead over no randomization
for the studied SPEC CPU2006 benchmark applications. The
average overhead is 2.1% IPC decrease under a small 64-entry
DRC lookup buffer.
In terms of power consumption, our approach incurs very
small overhead. Thank to the power modeling framework
already integrated with XIOSim,
is easy to modify the
simulators to take into account the extra power consumed by
the mediation layer that does de-randomization/randomization.
Figure 15 shows the dynamic power overhead under 128-entry
DRC. The average dynamic power overhead for the studied
SPEC benchmarks is 0.18% of the total CPU dynamic power.
it
Fig. 14. DRC miss rates under two different settings, DRC with 512 entries
and DRC with 64 entries. Y-axis shows DRC lookup miss rates. The average