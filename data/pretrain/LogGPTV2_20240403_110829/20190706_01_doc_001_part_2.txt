term3: (AND 1)
●
Inverse FTS (FQS)
RUM index supported – store branches of query tree in addinfo
●
Find queries for the first message in postgres mailing lists
\d pg_query
Table "public.pg_query"
Column | Type | Modifiers
--------+---------+-----------
q | tsquery |
count | integer |
Indexes:
"pg_query_rum_idx" rum (q) 33818 queries
select q from pg_query pgq, pglist where q @@ pglist.fts and pglist.id=1;
q
--------------------------
'one' & 'one'
'postgresql' & 'freebsd'
(2 rows)
Inverse FTS (FQS)
RUM index supported – store branches of query tree in addinfo
●
Find queries for the first message in postgres mailing lists
create index pg_query_rum_idx on pg_query using rum(q);
select q from pg_query pgq, pglist where q @@ pglist.fts and pglist.id=1;
QUERY PLAN
--------------------------------------------------------------------------
Nested Loop (actual time=0.719..0.721 rows=2 loops=1)
-> Index Scan using pglist_id_idx on pglist
(actual time=0.013..0.013 rows=1 loops=1)
Index Cond: (id = 1)
-> Bitmap Heap Scan on pg_query pgq
(actual time=0.702..0.704 rows=2 loops=1)
Recheck Cond: (q @@ pglist.fts)
Heap Blocks: exact=2
-> Bitmap Index Scan on pg_query_rum_idx
(actual time=0.699..0.699 rows=2 loops=1)
Index Cond: (q @@ pglist.fts)
Planning time: 0.212 ms
Execution time: 0.759 ms
(10 rows)
Inverse FTS (FQS)
RUM index supported – store branches of query tree in addinfo
●
Monstrous postings
select id, t.subject, count(*) as cnt into pglist_q from pg_query,
(select id, fts, subject from pglist) t where t.fts @@ q
group by id, subject order by cnt desc limit 1000;
select * from pglist_q order by cnt desc limit 5;
id | subject | cnt
--------+-----------------------------------------------+------
248443 | Packages patch | 4472
282668 | Re: release.sgml, minor pg_autovacuum changes | 4184
282512 | Re: release.sgml, minor pg_autovacuum changes | 4151
282481 | release.sgml, minor pg_autovacuum changes | 4104
243465 | Re: [HACKERS] Re: Release notes | 3989
(5 rows))
RUM vs GIN
6 mln classifies, real fts quieries, concurrency 24,
●
duration 1 hour
GIN — 258087 qph
•
7x speedup
• RUM — 1885698 qph ( )
RUM has no pending list (not implemented) and
●
stores more data.
Insert 1 mln messages shows no significant
overhead:
Time(min):
GiST(10), GIN(10), GIN_no_fast(21), RUM(34)
WAL(GB):
GiST(3.5), GIN(7.5), GIN_no_fast(24), RUM(29)
RUM vs GIN
CREATE INDEX
●
GENERIC WAL (9.6) generates too big WAL traffic
•
Page
Used space
To insert Free space
Page
To generic WAL
New data
RUM vs GIN
CREATE INDEX
●
GENERIC WAL(9.6) generates too big WAL traffic.
•
It currently doesn't supports shift.
rum(fts, ts+order) generates 186 Gb of WAL !
RUM writes WAL AFTER creating index
•
+-----------------------------------------------------------+
|table | gin | rum (fts |rum(fts,ts)|rum(fts,ts+order|
+-----------------------------------------------------------+
Create time| | 147 s | 201 | 209 | 215 |
+-----------------------------------------------------------+
Size( mb) |2167/1302| 534 | 980 | 1531 | 1921 |
+-----------------------------------------------------------+
WAL (Gb) | | 0.9 | 0.68 | 1.1 | 1.5 |
+-----------------------------------------------------------+
RUM for jsonb
ctid | value
-------+------------------------
(0,1) | '{"array": [1, 2, 3]}'
(0,2) | '{"array": [2, 3]}'
GIN — no information about array elements positions!
array.#.1: (0,1)
array.#.2: (0,1); (0,2)
array.#.3: (0,1); (0,2)
RUM — with information about array elements positions!
array.#.1: (0,1) | 0
array.#.2: (0,1) | 1; (0,2) | 0
array.#.3: (0,1) | 2; (0,2) | 1
RUM vs GIN for jsonb
# EXPLAIN (ANALYZE, BUFFERS) SELECT count(*) FROM js -- GIN
WHERE js @@ 'tags.#16.term = "design"'::jsquery;
Aggregate (cost=4732.10..4732.11 rows=1 width=8) (actual time=101.047..101.047 rows=1 loops=1)
Buffers: shared hit=55546
-> Bitmap Heap Scan on js (cost=33.71..4728.97 rows=1253 width=0) (actual time=35.495..101.025 rows=10 loops=1)
Recheck Cond: (js @@ '"tags".#16."term" = "design"'::jsquery)
Rows Removed by Index Recheck: 64490
Heap Blocks: exact=55525
Buffers: shared hit=55546
-> Bitmap Index Scan on js_gin_idx (cost=0.00..33.40 rows=1253 width=0) (actual time=12.498..12.498 rows=64500 loops=1)
Index Cond: (js @@ '"tags".#16."term" = "design"'::jsquery)
Buffers: shared hit=21
Planning time: 0.104 ms
101.447 ms
Execution time:
# EXPLAIN (ANALYZE, BUFFERS) SELECT count(*) FROM js -- RUM
WHERE js @@ 'tags.#16.term = "design"'::jsquery;
Aggregate (cost=4732.10..4732.11 rows=1 width=8) (actual time=5.818..5.818 rows=1 loops=1)
Buffers: shared hit=71
-> Bitmap Heap Scan on js (cost=33.71..4728.97 rows=1253 width=0) (actual time=5.804..5.813 rows=10 loops=1)
Recheck Cond: (js @@ '"tags".#16."term" = "design"'::jsquery)
Heap Blocks: exact=10
Buffers: shared hit=71
-> Bitmap Index Scan on js_rum_idx (cost=0.00..33.40 rows=1253 width=0) (actual time=5.799..5.799 rows=10 loops=1)
Index Cond: (js @@ '"tags".#16."term" = "design"'::jsquery)
Buffers: shared hit=61
Planning time: 0.057 ms 17 times faster!!!
5.860 ms
Execution time:
RUM vs GIN for jsonb
Object | Size | Build time
-----------+---------+------------
Table | 1369 MB |
GIN index | 411 MB | 80,2 sec
RUM index | 516 MB | 86,6 sec
RUM for jsonb appears to be not much bigger (25%) than
GIN for jsonb.
rum_jsquery_ops: extension dependencies
RUM for jsonb is coming soon!
RUM Todo
Allow multiple additional info
●
(lexemes positions + timestamp)
Add support for arrays
●
improve ranking function to support TF/IDF
●
Improve insert time (pending list ?)
●
Improve GENERIC WAL to support shift
●
Availability:
9.6+ only: https://github.com/postgrespro/rum
●
Thanks !
Better FTS configurability
The problem
●
Search multilingual collection requires processing by several
•
language-specific dictionaries. Currently, logic of processing is
.
hidden from user and example would“nt works
ALTER TEXT SEARCH CONFIGURATION multi_conf
ALTER MAPPING FOR asciiword, asciihword, hword_asciipart,
word, hword, hword_part
WITH unaccent, german_ispell, english_ispell, simple;
Logic of tokens processing in FTS configuration
●
Example: German-English collection
•
ALTER TEXT SEARCH CONFIGURATION multi_conf
ALTER MAPPING FOR asciiword, asciihword, hword_asciipart,
word, hword, hword_part
WITH unaccent THEN (german_ispell AND english_ispell) OR simple;
Some FTS problems #4
Working with dictionaries can be difficult and slow
●
Installing dictionaries can be complicated
●
Dictionaries are loaded into memory for every session
●
(slow first query symptom) and eat memory.
time for i in {1..10}; do echo $i; psql postgres -c "select
ts_lexize('english_hunspell', 'evening')" > /dev/null; done
1
2
3
4
5
6
7
8
For russian hunspell dictionary:
9
10
real 0m3.809s
user0m0.015s
real 0m0.656s
sys 0m0.029s
user 0m0.015s
sys0m0.031s
Each session «eats» 20MB of RAM !
Dictionaries in shared memory
Now it“s easy (Artur Zakirov, Postgres Professional + Thomas
●
Vondra)
https://github.com/postgrespro/shared_ispell
CREATE EXTENSION shared_ispell;
CREATE TEXT SEARCH DICTIONARY english_shared (
TEMPLATE = shared_ispell,
DictFile = en_us,
AffFile = en_us,
StopWords = english
);
CREATE TEXT SEARCH DICTIONARY russian_shared (
TEMPLATE = shared_ispell,
DictFile = ru_ru,
AffFile = ru_ru,
StopWords = russian
);
time for i in {1..10}; do echo $i; psql postgres -c "select ts_lexize('russian_shared', 'туши')" > /dev/null; done
1
2
…..
10
real 0m3.809s
real 0m0.170s
user 0m0.015s VS user0m0.015s
sys 0m0.027s
sys 0m0.029s
Dictionaries as extensions
Now it's easy (Artur Zakirov, Postgres Professional)
●
https://github.com/postgrespro/hunspell_dicts
CREATE EXTENSION hunspell_ru_ru; -- creates russian_hunspell dictionary
CREATE EXTENSION hunspell_en_us; -- creates english_hunspell dictionary
CREATE EXTENSION hunspell_nn_no; -- creates norwegian_hunspell dictionary
SELECT ts_lexize('english_hunspell', 'evening');
ts_lexize
----------------
{evening,even}
(1 row)
Time: 57.612 ms Slow first query syndrom
SELECT ts_lexize('russian_hunspell', 'туши');
ts_lexize
------------------------
{туша,тушь,тушить,туш}
(1 row)
Time: 382.221 ms
SELECT ts_lexize('norwegian_hunspell','fotballklubber');
ts_lexize
--------------------------------
{fotball,klubb,fot,ball,klubb}
(1 row)
Time: 323.046 ms
Tsvector editing functions
Stas Kelvich (Postgres Professional)
●
setweight(tsvector, 'char', text[] - add label to lexemes from
●
text[] array
select setweight( to_tsvector('english', '20-th anniversary of PostgreSQL'),
'A', '{postgresql,20}');
setweight
------------------------------------------------
'20':1A 'anniversari':3 'postgresql':5A 'th':2
(1 row)
ts_delete(tsvector, text[]) - delete lexemes from tsvector
●
select ts_delete( to_tsvector('english', '20-th anniversary of PostgreSQL'),
'{20,postgresql}'::text[]);
ts_delete
------------------------
'anniversari':3 'th':2
(1 row)
Tsvector editing functions
unnest(tsvector)
●
select * from unnest( setweight( to_tsvector('english',
'20-th anniversary of PostgreSQL'),'A', '{postgresql,20}'));
lexeme | positions | weights
-------------+-----------+---------
●
20 | {1} | {A}
anniversari | {3} | {D}
postgresql | {5} | {A}
th | {2} | {D}
(4 rows)
tsvector_to_array(tsvector) — tsvector to text[] array
●
array_to_tsvector(text[])
select tsvector_to_array( to_tsvector('english',
'20-th anniversary of PostgreSQL'));
tsvector_to_array
--------------------------------
{20,anniversari,postgresql,th}
(1 row)
Tsvector editing functions
ts_filter(tsvector,text[]) - fetch lexemes with specific label{s}
●
select ts_filter($$'20':2A 'anniversari':4C 'postgresql':1A,6A 'th':3$$::tsvector,
'{C}');
ts_filter
------------------
'anniversari':4C
(1 row)
select ts_filter($$'20':2A 'anniversari':4C 'postgresql':1A,6A 'th':3$$::tsvector,
'{C,A}');
ts_filter
---------------------------------------------
'20':2A 'anniversari':4C 'postgresql':1A,6A
(1 row)