### 3.74
**A.J. Oliner, A.V. Kulkarni, and A. Aiken**

#### Table 2: Training Data
The "Unique" column indicates the number of unique length-six sequences. \( T_i \) is the maximum time from the beginning of one system call to the start of the next.

| Application | Version       | Calls | Time (sec) | Rate (calls/sec) | Unique | \( T_i \) (sec) |
|-------------|---------------|-------|------------|------------------|--------|-----------------|
| Adium       | 1.2.7         | 50,514 | 198.204    | 54.451           | 33,278 | 5.54098         |
| Camino      | 1.6.1Int-v2   | 103,634 | 1975.11    | 7.2605           | 57,385 | 4469            |
| Mail        | 3.3           | 126,467 | 2195.65    | 896.85           | 48,630 | -               |
| TextEdit    | 1.5 (244)     | 176,170 | -          | -                | 31,794 | -               |

The system classifies these sequences based on a threshold \( V \), reporting an epidemic when it detects the first score from the infected community.

We repeat this randomized process 1,000 times per example to obtain statistically meaningful metrics. Syzygy is always presented with an equal number of healthy and infected examples, although it does not use this information in its classification. This approach is not intended to reflect the base rate of intrusions in a real system but rather to increase the precision of the metrics. As the size of the training set approaches infinity, the influence of removing the current trace file from the training set diminishes to zero. It is sufficient to select random windows from the traces because Syzygy is memoryless outside of each sample. Unless otherwise noted, we set \( W_i = 1000 \) sequences and \( V = \mu_H + 2\sigma_H \), where \( H \) is the distribution of community scores for a community of size \( n \), as described in Section 3.3. The results of our controlled experiments are presented in Sections 6.2–6.5.

### 6.1 Data Collection
We collect system call and timing traces from commercial, off-the-shelf software under normal usage by the authors, using the utility `dtrace`. We use several desktop applications: a chat program (Adium), a web browser (Camino), a mail client (Mail), and a simple text editor (TextEdit). A summary of these data is provided in Table 2. When compared to the real deployments in Sections 4 and 5, we find that our simulations are a reasonable approximation. Note that, although Syzygy must build a dynamic model of application behavior, it does not need to learn exploit signatures.

Many exploits currently found in the wild are brazen about their misbehavior (large \( \delta \)) and are therefore easy for Syzygy to detect (see Section 3.3). In this section, we focus on Syzygy’s ability to detect next-generation exploits under adverse conditions. These exploits can infect the application at any execution point (i.e., multiple infection vectors), have access to all of Syzygy’s data and parameters, and can perform skillful mimicry. The adverse conditions include client heterogeneity and tainted training data.

To simulate such behavior, we use four next-generation exploits:
- **mailspam**: Infects Mail, then composes and sends a large number of emails (based on the open mail relay in the Sobig worm’s trojan payload).
- **prompttext**: Infects TextEdit, then asks the user for input that it writes to a file (based on file creation and deletion seen in SirCam, Chernobyl, or Klez [40]).
- **screenshot**: Infects Adium and takes screenshots.
- **showpages**: Infects Camino and displays specific pages.

### Figures
- **Figure 11**: The applications generate new sequences throughout training, with occasional bursts (e.g., program launches).
- **Figure 12**: F1 measure with \( n = 100 \) and varying infection size \( d \) using each of the four pairs of programs and exploits.

These figures illustrate the performance of Syzygy in detecting the specified exploits under various conditions.