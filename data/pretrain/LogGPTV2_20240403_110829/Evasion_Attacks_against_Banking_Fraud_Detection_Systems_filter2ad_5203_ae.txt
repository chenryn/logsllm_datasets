[4] Battista Biggio, Giorgio Fumera, and Fabio Roli. Se-
curity evaluation of pattern classiﬁers under attack.
IEEE transactions on knowledge and data engineering,
26(4):984–996, 2013.
[5] Battista Biggio and Fabio Roli. Wild patterns: Ten years
after the rise of adversarial machine learning. Pattern
Recognition, 84:317–331, 2018.
[6] R Brause, T Langsdorf, and Michael Hepp. Neural data
mining for credit card fraud detection. In Proceedings
11th International Conference on Tools with Artiﬁcial
Intelligence, pages 103–106. IEEE, 1999.
[7] Leo Breiman. Random forests. Machine learning,
45(1):5–32, 2001.
[8] Nicholas Carlini and David Wagner. Towards evaluat-
ing the robustness of neural networks. In 2017 IEEE
Symposium on Security and Privacy (SP), pages 39–57.
IEEE, 2017.
[9] Michele Carminati, Alessandro Baggio, Federico Maggi,
Umberto Spagnolini, and Stefano Zanero. Fraudbuster:
temporal analysis and detection of advanced ﬁnancial
In International Conference on Detection of
frauds.
Intrusions and Malware, and Vulnerability Assessment,
pages 211–233. Springer, 2018.
[10] Michele Carminati, Roberto Caron, Federico Maggi, Ile-
nia Epifani, and Stefano Zanero. Banksealer: An on-
line banking fraud analysis and decision support system.
In IFIP International Information Security Conference,
pages 380–394. Springer, Berlin, Heidelberg, 2014.
[11] Michele Carminati, Roberto Caron, Federico Maggi, Ile-
nia Epifani, and Stefano Zanero. Banksealer: A decision
support system for online banking fraud analysis and
investigation. computers & security, 53:175–186, 2015.
[12] Michele Carminati, Mario Polino, Andrea Continella,
Andrea Lanzi, Federico Maggi, and Stefano Zanero. Se-
curity evaluation of a banking fraud analysis system.
ACM Transactions on Privacy and Security (TOPS),
21(3):11, 2018.
[13] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable
tree boosting system. In Proceedings of the 22nd acm
sigkdd international conference on knowledge discovery
and data mining, pages 785–794. ACM, 2016.
[14] Andrea Continella, Michele Carminati, Mario Polino,
Andrea Lanzi, Stefano Zanero, and Federico Maggi.
Prometheus: Analyzing webinject-based information
stealers. Journal of Computer Security, 25(2):117–137,
2017.
[15] Andrea Dal Pozzolo, Olivier Caelen, Yann-Ael
Le Borgne, Serge Waterschoot, and Gianluca Bontempi.
Learned lessons in credit card fraud detection from
Expert systems with
a practitioner perspective.
applications, 41(10):4915–4928, 2014.
[16] Hung Dang, Yue Huang, and Ee-Chien Chang. Evading
classiﬁers by morphing in the dark. In Proceedings of
the 2017 ACM SIGSAC Conference on Computer and
Communications Security, pages 119–133. ACM, 2017.
296    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
[17] Ambra Demontis, Marco Melis, Maura Pintor, Matthew
Jagielski, Battista Biggio, Alina Oprea, Cristina Nita-
Rotaru, and Fabio Roli. Why do adversarial attacks
transfer? explaining transferability of evasion and poi-
soning attacks. In 28th {USENIX} Security Symposium
({USENIX} Security 19), pages 321–338, 2019.
[18] Jose R Dorronsoro, Francisco Ginel, C Sgnchez, and
Carlos S Cruz. Neural fraud detection in credit card oper-
ations. IEEE transactions on neural networks, 8(4):827–
834, 1997.
[19] Ian J Goodfellow, Jonathon Shlens, and Christian
Szegedy. Explaining and harnessing adversarial exam-
ples. arXiv preprint arXiv:1412.6572, 2014.
[20] Kathrin Grosse, Nicolas Papernot, Praveen Manoharan,
Michael Backes, and Patrick McDaniel. Adversarial
examples for malware detection. In European Sympo-
sium on Research in Computer Security, pages 62–79.
Springer, 2017.
[21] Ron Kohavi and George H John. The wrapper approach.
In Feature extraction, construction and selection, pages
33–50. Springer, 1998.
[22] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural
networks. In Advances in neural information processing
systems, pages 1097–1105, 2012.
[23] Nicolas Papernot, Patrick McDaniel, and Ian Goodfel-
low. Transferability in machine learning: from phe-
nomena to black-box attacks using adversarial samples.
arXiv preprint arXiv:1605.07277, 2016.
[24] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow,
Somesh Jha, Z Berkay Celik, and Ananthram Swami.
Practical black-box attacks against machine learning. In
Proceedings of the 2017 ACM on Asia conference on
computer and communications security, pages 506–519.
ACM, 2017.
[25] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt
Fredrikson, Z Berkay Celik, and Ananthram Swami. The
limitations of deep learning in adversarial settings. In
2016 IEEE European Symposium on Security and Pri-
vacy (EuroS&P), pages 372–387. IEEE, 2016.
[26] Kuldeep Randhawa, Chu Kiong Loo, Manjeevan Seera,
Chee Peng Lim, and Asoke K Nandi. Credit card fraud
IEEE
detection using adaboost and majority voting.
access, 6:14277–14284, 2018.
[27] Konrad Rieck, Philipp Trinius, Carsten Willems, and
Thorsten Holz. Automatic analysis of malware behavior
using machine learning. Journal of Computer Security,
19(4):639–668, 2011.
[28] Robin Sommer and Vern Paxson. Outside the closed
world: On using machine learning for network intrusion
detection. In 2010 IEEE symposium on security and
privacy, pages 305–316. IEEE, 2010.
[29] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing properties of neural networks. arXiv
preprint arXiv:1312.6199, 2013.
[30] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and
Lior Wolf. Deepface: Closing the gap to human-level
In Proceedings of
performance in face veriﬁcation.
the IEEE conference on computer vision and pattern
recognition, pages 1701–1708, 2014.
[31] Véronique Van Vlasselaer, Cristián Bravo, Olivier Cae-
len, Tina Eliassi-Rad, Leman Akoglu, Monique Snoeck,
and Bart Baesens. Apate: A novel approach for au-
tomated credit card transaction fraud detection using
network-based extensions. Decision Support Systems,
75:38–48, 2015.
[32] Kalyan Veeramachaneni, Ignacio Arnaldo, Vamsi Kor-
rapati, Constantinos Bassias, and Ke Li. Aiˆ 2: training
a big data machine to defend. In 2016 IEEE 2nd In-
ternational Conference on Big Data Security on Cloud
(BigDataSecurity), IEEE International Conference on
High Performance and Smart Computing (HPSC), and
IEEE International Conference on Intelligent Data and
Security (IDS), pages 49–54. IEEE, 2016.
[33] Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,
Ilya Sutskever, and Geoffrey Hinton. Grammar as a
foreign language. In Advances in neural information
processing systems, pages 2773–2781, 2015.
[34] David J Weston, David J Hand, Niall M Adams, Christo-
pher Whitrow, and Piotr Juszczak. Plastic card fraud
detection using peer group analysis. Advances in Data
Analysis and Classiﬁcation, 2(1):45–62, 2008.
[35] Christopher Whitrow, David J Hand, Piotr Juszczak,
D Weston, and Niall M Adams. Transaction aggregation
as a strategy for credit card fraud detection. Data mining
and knowledge discovery, 18(1):30–55, 2009.
[36] Vladimir Zaslavsky and Anna Strizhak. Credit card
fraud detection using self-organizing maps. Information
and Security, 18:48, 2006.
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    297
A Features extracted
The directly derivable features we use are:
• amount. Amount of the transaction in euro (e ), without
any type of transformation.
• time_x, time_y. These two features are computed from
the Timestamp. The time is cyclical and we must be
careful when deciding how to encode it as input to the
machine learning model. If we encode hours as integers
we have a problem: the distance computed, for example
between 23 and 22 is 23− 22 = 1, while the distance
that it calculates between midnight and 23 is 0− 23 =
−23.We need to change the encoding of the features
such that midnight and 23 are the same distance apart as
any other two hours. A common method for encoding
cyclical data is to transform the data into two dimensions
using a sine and cosine transformation. So time_x and
time_y are:t ← tsh · 3600 + tsmin · 60 + tssec,time_x =
cos t·2π
• is_national_iban. A Boolean value indicating if the ben-
86400 ,time_y = sin t·2π
86400
eﬁciary’s IBAN is national.
• is_national_asn. A Boolean value indicating if the con-
nection of the coming transaction has a national IP Ad-
dress.
• is_international. A Boolean value indicating if the con-
nection comes from a country different from the one of
the beneﬁciary IBAN.
• operation_type. It is one-hot-encoded, so the three op-
erations become respectively op_type_1, op_type_2,
op_type_3
• conﬁrm_sms. A Boolean value indicating if the transac-
tion requires a conﬁrmation sms.
The features obtained by aggregating transactions are:
• group_function_time. Consider function, group, time as
variable. A function is computed on the user transactions
grouped by group for a rolling window of size time.
• group can be: IBAN, IBAN_CC, IP, CC_ASN, Ses-
sionID an none, if we want to group only by user and
take all the transactions of that user.
• function can be:
– count. It counts the number of transactions in the
considered window
– sum. It computes the sum of the transactions
amounts in the considered window
– mean. It computes the mean of the transactions
amounts in the considered window
– std. It computes the standard deviation of the trans-
actions amounts in the considered window
• time can be: 1 hour, 1 day, 7 days, 30 days and none, if
we want to aggregate all the transaction regardless of the
time
• time_since_last_group. Indicates the time in hours
elapsed from the last transaction among those obtained
by grouping the transactions by user and by group.
• group_distance_from_mean. Indicates the L1 distance
of the transaction amount with respect to the mean
amount of the transactions obtained by grouping the
transactions ﬁrst by user then by group.
• is_new_iban. A Boolean value indicating if it is the ﬁrst
time the user made a transaction to that IBAN.
• is_new_iban_cc. A Boolean value indicating if it is the
ﬁrst time the user made a transaction to an IBAN from
that country.
• is_new_ip. A Boolean value indicating if it is the ﬁrst
time the user made a transaction coming from that IP
Address.
• is_new_cc_asn. A Boolean value indicating if it is the
ﬁrst time the user made a transaction coming from that
country.
B Adversarial Machine Learning Approaches
There are many studies on how to craft adversarial samples.
Szegedy et al. [29] generate adversarial examples using
box-constrained optimization method Limited Memory Broy-
den Fletcher Goldfarb Shanno (LBFGS). Given an image x
and a function f (x), that maps image pixels vector to a dis-
crete label, and assuming that f has an associated loss func-
tion denoted by loss f , their method ﬁnds a different image x(cid:48)
similar to x under L2 distance, yet is labeled differently by the
classiﬁer. They model the problem as a constrained minimiza-
2, subject to f (x(cid:48)) = l
tion problem: minimize
and x(cid:48) ∈ [0,1]n. In particular, they found an approximate so-
(cid:107)x−
lution by solving the analogous problem: minimize
2 + lossF,l(x(cid:48)), subject tox(cid:48) ∈ [0,1]n. The ﬁnal result is the
x(cid:48)(cid:107)2
the minimum c > 0 for which x− x(cid:48) satisﬁes f (x(cid:48)) = l, where
l is the target label.
(cid:107)x − x(cid:48)(cid:107)2
Goodfellow et al., proposed the Fast Gradient Sign Method
(FGSM) [19] that is designed primarily to be as fast as pos-
sible instead of accurately producing an adversarial sample
(i.e., it is not meant to produce minimal perturbed adversar-
ial samples). Moreover, it is optimized for the L∞ distance
metric. Given an image x, it computes the adversarial sam-
ple x(cid:48) = x−ε· sign(∇lossF,t (x)), where t represents the target
298    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association
Table 8: Success Rate and Transferability of the three AML
methods
SUCCESS RATE
TRANSFERABILITY
GRADIENT METHOD
JSMA
CARLINI & WAGNER L2
99.3%
98.2%
12%
16.1%
16.1%
9.3%
label and ε is chosen to ensure misclassiﬁcation without ex-
cessively altering the image.
The attack proposed by Papernot et al. [25] is known un-
der the name of Jacobian Saliency Map Approach (JSMA)
and it is based on a L0 distance. This attack is based on a
greedy algorithm in which, at each iteration, a saliency map
is built based on the gradient of the output neural network
with respect to the input image. This saliency map indicates
the impact of each pixel (i.e., how much each pixel inﬂuences
the outcome of the classiﬁcation) and, at each iteration, the
most signiﬁcant pixel is perturbed. The method iterates until
either the sample is misclassiﬁed by the classiﬁer or pixels
are modiﬁed more than a threshold.
2 + c· f ( 1
Carlini&Wagner [8] designed three attacks tailored to three
distance metrics: L2,L0,L∞ distances. In L2 Attack, given an
image sample x, a target class t s.t. t (cid:54)= C∗(x), they ﬁnd w
by solving this optimization problem: minimize(cid:107) 1
2 (tanhw +
1)− x(cid:107)2
2 (tanhw + 1)), where f , which is deﬁned as
f (x(cid:48)) = max(max{Z(x(cid:48))i : i (cid:54)= t}− Z(x(cid:48))t ,−κ), is the objec-
tive function and κ is a parameter that allows to tune the
conﬁdence of the adversarial sample. In L0 Attack, since the
L0 distance metric is non-differentiable it is not possible to
use the standard gradient descent, so they decided to use an
iterative algorithm. At each iteration they identify, through the
L2 attack, which pixel inﬂuence less the output of the classiﬁer
and ﬁx its value so that it will never be changed. By doing this,
the set of ﬁxed pixels grows at each iteration, until a minimal
subset of modiﬁable pixels is identiﬁed. In L∞ Attack, since
the L∞ distance metric is not fully differentiable and standard
gradient descent does not perform, a different iterative attack
is put in place. At each iteration the following optimization
problem is solved: minimizec · f (x + δ) + ∑i[(δi − τ)+], in
which if δi < τ for all i, they reduce τ by a factor of 0.9 and
repeat; otherwise they terminate the search.
B.1 Evaluation of standard AML approaches
for Fraud Detection
As highlighted in Section 2, the issues in performing eva-
sion attacks based on standard AML algorithms in the fraud
detection domain are mainly practical. It would be challeng-
ing (if not infeasible) for an attacker to perform a real bank
transfer (i.e., a real transaction) representing the adversar-
ial sample returned as output by the AML algorithm. The
X∗ ← X
while F(X∗) (cid:54)= Y∗ +C and (cid:107)δX(cid:107) < ϒ do
Algorithm 2 Craft Adversarial Samples. X is the benign
sample,sub Y∗ is the NN output, F is the function learned by
the NN during training, ϒ is the maximum distortion, θ is the
change made to features, C is a conﬁdence parameter
1: procedure GRADIENTMETHOD(X,Y∗,F,ϒ,θ,C)
2:
3:
4:
5:
6:
7:
8:
end while
9:
return X∗
10:
11: end procedure
G = ∇F(X∗)
S = sign(G)
A = abs(S)
X∗i ← X ∗i +θis.t.imax = argmaxI(A)· S
δX ← X∗ − X
reason resides in which features are modiﬁed and how. As
shown in Figure 3, the perturbed features are the aggregated
ones; hence, an attacker, to perform an evasion attack, should
then extract the direct features that correspond to valid, real
transactions under the constraints that, once aggregated, they
produce coherent aggregated features. If we add the practi-
cal domain constraints presented in Section 2, this problem
may be not feasible in terms of resources and return for an
attacker. However, for the sake of completeness, we evaluate
AML techniques to measure the theoretical performance of
such attacks in our domain. In particular, we make use of
the following algorithms: Carlini & Wagner l2 [8], Jacobian
Saliency Map Approach [25], and a simple approach based
on the computation of the gradient whose algorithm is shown
by Pseudocode 2.
For this evaluation, we use two models: a Neural Network,
necessary for applying the AML algorithms, and a Random
Forest classiﬁer that acts as the bank fraud detector. We used
the dataset 2012-13 as the training set and the same features
used for the Oracle in the previous experiments, shown in Ta-
ble 2, to train the Artiﬁcial Neural Network with two hidden
layers: the ﬁrst with 32 units and the second with 16 units. We
used ReLU as the activation function for the hidden layers,
while for the output layer, we used the Softmax function and
two neurons. We extracted frauds from dataset 2012-13, then
we perturbed these frauds using the three AML approaches
that are based on the neural network to compute the gradi-
ents. In order to test transferability, we used, as a bank fraud
detector, the model O1 from Table 3, which is trained on
the same dataset as the Neural Network and also uses the
same aggregation method. In order to evaluate our results,
we used the metrics deﬁned by Papernot et al. [24]: success
rate and transferability. The success rate is the proportion
of adversarial samples misclassiﬁed by the Neural Network.
The transferability of adversarial samples refers to the ran-
dom forest (O1) misclassiﬁcation rate of adversarial samples
crafted using the Neural Network. The results in Table 8 show
USENIX Association
23rd International Symposium on Research in Attacks, Intrusions and Defenses    299
Figure 3: The heatmap shows how much each feature has been perturbed as a percentage of its initial value. For example,
1.0 means that the value of that feature has been doubled, it is increased by 100% compared with the initial value
poor performance, and also, to achieve those results, we need
to apply wide perturbations to the initial samples, as we can
see from the heatmap in Figure 3.
C Machine Learning Metrics
We present the most common metrics used to evaluate the
quality of a machine learning model. The terms True Positive
(TP), False Positive (FP), True Negative (TN), False Negative
(FN), are used to compare the results of the classiﬁer under
test with trusted ground truth. The terms positive and negative
refer to the classiﬁer prediction while the terms true and false
refer to whether that prediction corresponds to the external
ground truth. In our work, the positive class represents the
frauds, and the negative represents legitimate transactions.
The most used metrics derived with those terms are:
- Accuracy. It represents the percentage of correctly
classiﬁed instances and it is deﬁned as Accuracy =
t p+tn+ f p+ f n.
t p+tn
- Precision. It is also called positive predictive value and
is deﬁned as the fraction of relevant instances among the
retrieved instances and it can be deﬁned as: Precision =
t p
t p+ f p.
- Recall. It is also known as sensitivity or true positive
rate. It is deﬁned as the fraction of the total amount of
relevant instances that were actually retrieved and can
be deﬁned as : Recall = t p
t p+ f n.
- F1-Score. It is the harmonic mean of the precision and
recall. F1 score reaches its best value at 1 (perfect pre-
cision and recall) and worst at 0 and can be deﬁned as:
F1− Score = 2· precision·recall
precision+recall
300    23rd International Symposium on Research in Attacks, Intrusions and Defenses
USENIX Association