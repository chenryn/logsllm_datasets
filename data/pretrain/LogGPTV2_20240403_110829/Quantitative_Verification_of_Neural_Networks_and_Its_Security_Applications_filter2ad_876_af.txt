al. [68] (the details of the procedure are outlined later). Our BNNs
models may obtain better attack effectivenessas the trojaning pro-
cedure progresses over time. Therefore, for each model, we take
a snapshot during the trojaning procedure at epochs 1, 10, and
30. There are 4 models (ARCH1-ARCH4), and for each, we train
5 different models each classifying the trojan input to a distinct
output label. Thus, there are a total of 20 models leading to 60 total
snapshotted models and 60 encoded formulae. If NPAQ terminates
within the timeout of 24 hours, it either quantifies the number of
solutions or outputs UNSAT, indicating that no trojaned input is
labeled as the target output at all. The effectiveness of the trojan
attack is measured by two metrics:
• PS(tr): The percentage of trojaned inputs labeled as the
target output to the size of input space, generated by NPAQ.
• ACCt : The percentage of trojaned inputs in the chosen test
set labeled as the desired target output.
Table 7. NPAQ estimates of bias in BNNs ARCH1..4 trained
on the UCI Adult dataset. For changes in values of the sensi-
tive features (marital status, gender and race), we compute,
PS(bias), the percentage of individuals classified as having
the same annual income (=), greater than (>) and less than
(
0.00
4.09
4.37
3.81
9.13
18.69
9.34
5.84
89.17
74.94
80.04
83.86
White → Black
5.57
14.34
6.24
5.84
84.87
79.82
78.23
82.21
Table 6 reports the PS(tr) and ACCt . Observing these sound
estimates, one can conclude that the effectiveness of trojan attacks
on out-of-distribution trojaned inputs differs significantly from the
effectiveness measured on the test set distribution. In particular, if
we focus on the models with the highest PS(tr) for each architec-
ture and target class (across all epochs), only 50% (10 out 20) are
the same as when we pick the model with highest ACCt instead.
Attack Procedure. The trojaning process can be arbitrarily dif-
ferent from ours; the use of NPAQ for verifying them does not
depend on it in any way. Our procedure is adapted from that of Liu
et al. which is specific to models with real-valued weights. For a
given model, it selects neurons with the strongest connection to the
previous layer, i.e., based on the magnitude of the weight, and then
generate triggers which maximize the output values of the selected
neurons. This heuristic does not apply to BNNs as they have {−1, 1}
weights. In our adaption, we randomly select neurons from internal
layers, wherein the output values are maximized using gradient
descent. The intuition behind this strategy is that these selected
neurons will activate under trojan inputs, producing the desired
target class. For this procedure, we need a set of trojan and benign
samples. In our procedure, we assume that we have access to a
10, 000 benign images, unlike the work in Liu et al. which generates
this from the model itself. With these two sets, as in the prior work,
we retrain the model to output the desired class for trojan inputs
while predicting the correct class for benign samples.
6.4 Case Study 3: Quantifying Model Fairness
We use NPAQ to estimate how often a given neural net treats
similar inputs, i.e., inputs differing in the value of a single feature,
differently. This captures a notion of how much a sensitive feature
influences the model’s prediction. We quantify fairness for 4 BNNs,
one for each architecture ARCH1-ARCH4, trained on the UCI Adult
(Income Census) dataset [2]. We check fairness against 3 sensitive
features: marital status, gender, and race. We encode 3 queries for
each model using Property P3— P5 (Section 3). Specifically, for how
many people with exactly the same features, except one’s marital
status is “Divorced” while the other is “Married”, would result in
different income predictions? We form similar queries for gender
(“Female” vs. “Male”) and race (“White” vs. “Black”) 3.
Effect of Sensitive Features. 4 models, 3 queries, and 3 different
sensitive features give 36 formulae. Table 7 reports the percentage
3We use the category and feature names verbatim as in the dataset. They do not reflect
the authors’ views.
of counts generated by NPAQ. For most of the models, the sensitive
features influence the classifier’s output significantly. Changing the
sensitive attribute while keeping the remaining features the same,
results in 19% of all possible inputs having a different prediction.
Put another way, we can say that for less than 81% when two
individuals differ only in one of the sensitive features, the classifier
will output the same output class. This means most of our models
have a “fairness score” of less than 81%.
Quantifying Direction of Bias. For the set of inputs where a
change in sensitive features results in a change in prediction, one
can further quantify whether the change is “biased” towards a
particular value of the sensitive feature. For instance, using NPAQ,
we find that across all our models consistently, a change from
“Married” to “Divorced” results in a change in predicted income
from LOW to HIGH. 4 For ARCH1, an individual with gender “Male”
would more likely (9.13%) to be predicted to have a higher income
than “Female” (2.07%) when all the other features are the same.
However, for ARCH4, a change from “Female” to “Male” would
more likely result in a HIGH to LOW change in the classifier’s
output (10.19%). Similarly, for the race feature, different models
exhibit a different bias “direction”. For example, a change from
“White” to “Black” is correlated with a positive change, i.e., from
LOW income to HIGH income, for ARCH2. The other 3 models,
ARCH1, ARCH2, and ARCH4 will predict that an individual with
the same features except for the sensitive feature would likely have
a LOW income if the race attribute is set to be “Black”.
With NPAQ, we can distinguish how much the models treat
individuals unfairly with respect to a sensitive feature. One can
encode other fairness properties, such as defining a metric of simi-
larity between individuals where non-sensitive features are within
a distance, similar to individual fairness [30]. NPAQ can be helpful
for such types of fairness formulations.
7 RELATED WORK
We summarize the closely related work to NPAQ.
Non-quantitative Neural Network Verification. Our work is
on quantitatively verifying neural networks, and NPAQ counts the
number of discrete values that satisfy a property. We differ in our
goals from many non-quantitative analyses that calculate continu-
ous domain ranges or single witnesses of satisfying values. Pulina
and Tacchella [82], who first studied the problem of verifying neu-
ral network safety, implement an abstraction-refinement algorithm
that allows generating spurious examples and adding them back to
the training set. Reluplex [60], an SMT solver with a theory of real
arithmetic, verifies properties of feed-forward networks with ReLU
activation functions. Huang et al. [56] leverage SMT by discretizing
an infinite region around an input to a set of points and then prove
that there is no inconsistency in the neural net outputs. Ehlers [32]
scope the work to verifying the correctness and robustness proper-
ties on piece-wise activation functions, i.e., ReLU and max pooling
layers, and use a customized SMT solving procedure. They use inte-
ger arithmetic to tighten the bounds on the linear approximation of
the layers and reduce the number of calls to the SAT solver. Wang
et al. [104] extend the use of integer arithmetic to reason about
neural networks with piece-wise linear activations. Narodytska et
4An income prediction of below $50, 000 is classified as LOW.
al. [73] propose an encoding of binarized neural networks as CNF
formulas and verifies robustness properties and equivalence using
SAT solving techniques. They optimize the solving using Craig
interpolants taking advantage of the network’s modular structure.
AI2 [44], DeepZ [89], DeepPoly [90] use abstract interpretation to
verify the robustness of neural networks with piece-wise linear
activations. They over-approximate each layer using an abstract
domain, i.e., a set of logical constraints capturing certain shapes
(e.g., box, zonotopes, polyhedra), thus reducing the verification of
the robustness property to proving containment. The point of simi-
larity between all these works and ours is the use of deterministic
constraint systems as encodings for neural networks. However, our
notion of equi-witnessability encodings applies to only specific
constructions and is the key to preserving model counts.
Non-quantitative verification as Optimization. Several works
have posed the problem of certifying robustness of neural networks
as a convex optimization problem. Ruan, Huang, & Kwiatkowska [85]
reduce the robustness verification of a neural network to the generic
reachability problem and then solve it as a convex optimization
problem. Their work provides provable guarantees of upper and
lower bounds, which converges to the ground truth in the limit.
Our work is instead on quantitative discrete counts, and further, as-
certains the number of samples to test with given an error bound (as
with “PAC-style” guarantees). Raghunathan, Steinhardt, & Percy [83]
verify the robustness of one-hidden layer networks by incorporat-
ing the robustness property in the optimization function. They
compute an upper bound which is the certificate of robustness
against all attacks and inputs, including adversarial inputs, within
linf ball of radius ϵ. Similarly, Wong and Kolter [106] train networks
with linear piecewise activation functions that are certifiably ro-
bust. Dvijotham et al. [29] address the problem of formally verifying
neural networks as an optimization problem and obtain provable
bounds on the tightness guarantees using a dual approach.
Quantitative Verification of Programs. Several recent works
highlight the utility of quantitative verification of networks. They
target the general paradigm of probabilistic programming and
decision-making programs [7, 55]. FairSquare [7] proposes a prob-
abilistic analysis for fairness properties based on weighted vol-
ume computation over formulas defining real closed fields. While
FairSquare is more expressive and can be applied to potentially any
model programmable in the probabilistic language, it does not guar-
antee a result computed in finite time will be within a desired error
bound (only that it would converge in the limit). Webb et al. [105]
using a statistical approach for quantitative verification but without
provable error bounds for computed results as in NPAQ.
CNF Model Counting. In his seminal paper, Valiant showed that
#CNF is #P-complete, where #P is the set of counting problems asso-
ciated with NP decision problems [101]. Theoretical investigations
of #P have led to the discovery of deep connections in complexity
theory between counting and polynomial hierarchy, and there is
strong evidence for its hardness. In particular, Toda showed that
every problem in the polynomial hierarchy could be solved by just
one invocation of #P oracle; more formally, PH ⊆ P#P [97].
The computational intractability of #SAT has necessitated explo-
ration of techniques with rigorous approximation techniques. A
significant breakthrough was achieved by Stockmeyer who showed
that one couls compute approximation with (ε, δ) guarantees given
access to an NP oracle. The key algorithmic idea relied on the usage
of hash functions but the algorithmic approach was computation-
ally prohibitive at the time and as such did not lead to development
of practical tools until early 2000s [71]. Motivated by the success
of SAT solvers, in particular development of solvers capable of han-
dling CNF and XOR constraints, there has been a surge of interest
in the design of hashing-based techniques for approximate model
counting for the past decade [6, 21, 22, 34, 49, 71, 72, 92].
8 CONCLUSION
We present a new algorithmic framework for approximate quanti-
tative verification of neural networks with formal PAC-style sound-
ness. The framework defines a notion of equi-witnessability en-
codings of neural networks into CNF formulae. Such encodings
preserve counts and ensure composibility under logical conjunc-
tions. We instantiate this framework for binarized neural networks,
building a prototype tool called NPAQ. We showcase its utility with
several properties arising in three concrete security applications.
9 ACKNOWLEDGMENTS
This research is supported by research grant DSOCL17019 from
DSO, Singapore. This research was partially supported by a grant
from the National Research Foundation, Prime Minister’s Office, Sin-
gapore under its National Cybersecurity R&D Program (TSUNAMi
project, No. NRF2014NCR-NCR001-21) and administered by the Na-
tional Cybersecurity R&D Directorate. This research is supported
by the National Research Foundation Singapore under its AI Singa-
pore Programme [R-252- 000-A16-490] and the NUS ODPRT Grant
[R-252-000-685-133]. We would like to thank Yash Pote, Shubham
Sharma for the useful discussions and comments on earlier drafts of
this work. We also thank Zheng Leong Chua for his help in setting
up experiments. Part of the computational work for this article was
performed on resources of the National Supercomputing Centre,
Singapore 5.
REFERENCES
[1] 2012. Correctional Offender Management Profiling for Alternative Sanctions.
http://www.northpointeinc.com/files/downloads/FAQ_Document.pdf. (2012).
[2] 2017. UCI Machine Learning Repository. http://archive.ics.uci.edu/ml. (2017).
[3]
Ignasi Abío, Robert Nieuwenhuis, Albert Oliveras, and Enric Rodríguez-
Carbonell. BDDs for pseudo-Boolean constraints–revisited. In SAT’11.
Ignasi Abío, Robert Nieuwenhuis, Albert Oliveras, and Enric Rodríguez-
Carbonell. A parametric approach for smaller and better encodings of cardinality
constraints. In CP’13.
[5] Dimitris Achlioptas and Federico Ricci-Tersenghi. On the solution-space geome-
[4]
try of random constraint satisfaction problems. In STOC’06.
[6] Dimitris Achlioptas and P. Theodoropoulos. Probabilistic Model Counting with
Short XORs. In SAT’17.
[7] Aws Albarghouthi, Loris D’Antoni, Samuel Drews, and Aditya V Nori. FairSquare:
probabilistic verification of program fairness. In OOPSLA’17.
[8] Roberto Asín, Robert Nieuwenhuis, Albert Oliveras, and Enric Rodríguez-
Carbonell. 2011. Cardinality networks: a theoretical and empirical study. Con-
straints (2011).
[9] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give
a false sense of security: Circumventing defenses to adversarial examples. In
ICML’18.
[10] Rehan Abdul Aziz, Geoffrey Chu, Christian Muise, and Peter Stuckey. #∃ SAT:
[11] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine
Projected Model Counting. In SAT’15.
translation by jointly learning to align and translate. In ICLR’15.
5https://www.nscc.sg/
[12] Kenneth E Batcher. Sorting networks and their applications. In AFIPS’1968.
[13] Siddhartha Bhattacharyya, Darren Cofer, D Musliner, Joseph Mueller, and Eric
Engstrom. Certification considerations for adaptive systems. In ICUAS’15.
[14] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against
support vector machines. In ICML’12.
[15] Bernard Boigelot, Sébastien Jodogne, and Pierre Wolper. An effective decision
procedure for linear arithmetic over the integers and reals. In TOCL’05.
[16] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat
Flepp, Prasoon Goyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai
Zhang, and Others. 2016. End to end learning for self-driving cars. arXiv (2016).
[17] Nicholas Carlini, Chang Liu, Jernej Kos, Úlfar Erlingsson, and Dawn Song. 2018.
The Secret Sharer: Measuring Unintended Neural Network Memorization &
Extracting Secrets. arXiv (2018).
[18] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David Wagner, and Wenchao Zhou. Hidden Voice Commands.. In
USENIX’16.
[19] Nicholas Carlini and David Wagner. Towards Evaluating the Robustness of
Neural Networks. In SP’17.
[20] Supratik Chakraborty, Dror Fried, Kuldeep S Meel, and Moshe Y Vardi. From
Weighted to Unweighted Model Counting.. In IJCAI’15.
[21] Supratik Chakraborty, Kuldeep S Meel, and Moshe Y Vardi. A scalable approxi-
mate model counter. In CP’13.
[22] Supratik Chakraborty, Kuldeep S Meel, and Moshe Y Vardi. Algorithmic im-
provements in approximate counting for probabilistic inference: From linear to
logarithmic SAT calls. In IJCAI’16.
[23] Supratik Chakraborty, Kuldeep S Meel, and Moshe Y Vardi. Balancing scalability
and uniformity in SAT witness generator. In DAC’14.
[24] Anupam Datta, Matthew Fredrikson, Gihyuk Ko, Piotr Mardziel, and Shayak
Sen. Use privacy in data-driven systems: Theory and experiments with machine
learnt programs. In CCS’17.
[25] Anupam Datta, Shayak Sen, and Yair Zick. Algorithmic transparency via quanti-
tative input influence: Theory and experiments with learning systems. In SP’16.
[26] Elvis Dohmatob. 2018. Limitations of adversarial robustness: strong No Free
Lunch Theorem. arXiv (2018).
Jeffrey M. Dudek, Kuldeep S. Meel, and Moshe Y. Vardi. Combining the k-CNF
and XOR phase-transitions. In IJCAI’16.
Jeffrey M. Dudek, Kuldeep S. Meel, and Moshe Y. Vardi. The Hard Problems Are
Almost Everywhere For Random CNF-XOR Formulas. In IJCAI’17.
[29] Krishnamurthy Dvijotham, Robert Stanforth, Sven Gowal, Timothy Mann, and
Pushmeet Kohli. 2018. A Dual Approach to Scalable Verification of Deep Net-
works. arXiv (2018).
[30] Cynthia Dwork, Moritz Hardt, Toniann Pitassi, Omer Reingold, and Richard
[27]
[28]
Zemel. Fairness through awareness. In ITCS’12.
[31] Niklas Eén and Niklas Sörensson. 2006. Translating Pseudo-Boolean Constraints
[32] Ruediger Ehlers. Formal verification of piece-wise linear feed-forward neural
into SAT. JSAT 2, 1-4 (2006), 1–26.
networks. In ATVA’17.
[43] Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and
Surya Nepal. 2019. STRIP: A Defence Against Trojan Attacks on Deep Neural
Networks. arXiv (2019).
[44] Timon Gehr, Matthew Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat
Chaudhuri, and Martin Vechev. AI2: Safety and Robustness Certification of
Neural Networks with Abstract Interpretation. In SP’18.
[33] Stefano Ermon, Carla P Gomes, Ashish Sabharwal, and Bart Selman. Embed and
project: Discrete sampling with universal hashing. In NIPS’13.
[34] Stefano Ermon, Carla P Gomes, Ashish Sabharwal, and Bart Selman. Taming the
Curse of Dimensionality: Discrete Integration by Hashing and Optimization. In