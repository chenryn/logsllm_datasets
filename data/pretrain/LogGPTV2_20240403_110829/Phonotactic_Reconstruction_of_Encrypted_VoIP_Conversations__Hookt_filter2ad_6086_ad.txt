consonants, we use three different articulatory features as
axes and calculate the Euclidean distance (see Figure 7)
between the points corresponding to the two phonemes
(scaled to a maximum of one unit). For consonants these
features are voice, manner, and place of articulation. For
vowels they are rounding, backness and height. Thus we
differentiate between substitution of a phonetically similar
segment, such as replacement of [s] (as in ‘see’) by [S] (as
in ‘she’), or of a completely different segment, such as of
[s] (as in ‘seen’) with [k] (as in ‘keen’).
Word 1
bæt ‘bat’
bit ‘beat’
did ‘deed’
b2t ‘but’
b2t ‘but’
bid ‘bead’
f6D3~ ‘father’
hUkt ‘hooked’
hEloU ‘hello’
Word 2
mæt ‘mat’
bæt ‘bat’
bid ‘bead’
bOt ‘bought’
bæt ‘bat’
bit ‘beat’
m2D3~ ‘mother’
f6nIks ‘phonics’
w3~ld‘world’
Phonetic
Distance
0.0722
0.1042
0.1050
0.1250
0.1267
0.5774
0.7292
2.9573
3.1811
Primary
Difference
manner
height
place
rounding
backness
voicing
n/a
n/a
n/a
Table V
EXAMPLES OF OUR PHONETIC EDIT DISTANCE BETWEEN PAIRS OF
EXAMPLE WORDS. THE LAST COLUMN LISTS THE PRIMARY
DIFFERENCE (IN TERMS OF ARTICULATORY PROCESSES).
Figure 8. Parameter space exploration, in terms of average word accuracy,
for our phonetic edit distance.
To match a sequence of phonemes to an English word,
we compute the phonetic distance between the sequence and
each pronunciation in our dictionary in order to obtain a list
of the closest pronunciations to the sequence. However, the
existence of homophones means that, even if the pronunci-
ation is correct, we may have many choices for the word
spoken. For example, ‘ate’ and ‘eight’ are indistinguishable
phonetically: both are pronounced [eIt].
In order to disambiguate between homophones, we in-
corporate a word and part-of-speech based language model
to choose between the candidate words using contextual
information from the sentence as a whole. Thus we can
disambiguate between ‘ate’ and ‘eight’ by ﬁnding the most
likely part of speech (e.g., noun, verb, pronoun, or adverb)
for that position in the sentence. Using the SRILM language
12
StopFricativeApproximantPlaceMannerLabiodentalVelarGlottalInterdentalfθw0.320.270.05SibilantBase1.01.21.41.61.82.0Offset0.00.20.40.60.81.0Word Accuracy16.816.917.017.117.2modeling toolkit, we train a trigram language model over
both words and parts-of-speech on the well-known Brown
corpus [20]. The part of speech tags used are those cur-
rently implemented in NLTK [40]. To improve the ability
of the language model to disambiguate between candidate
words, we assign each word a weight which estimates the
conditional probability of the observed pronunciation given
the candidate word.
To ﬁnd these weights, we need a measure of how likely
an observed pronunciation is given the phonetic distance
to the actual pronunciation of the given word; therefore,
we estimate the cumulative distribution function (CDF)
over phonetic distances by deriving an empirical CDF (see
Figure 9) from the distances of a large number of pronun-
ciation pairs. We then transform the given distance between
pronunciations into a probability estimate by evaluating the
empirical CDF at that distance. For each pronunciation in the
candidate list for an observed word, we weight the associated
words with the probability estimate for that pronunciation.7
Thus we have, for each word in an utterance, a list of
candidate words with associated conditional probability es-
timates. Disambiguation is performed by ﬁnding the max-
imum likelihood sequence of words given the candidates,
their probability estimates, and the language model.
Figure 9. Empirical CDF of phonetic edit distance.
At this point, the observant reader will have surely noted
that the overall process is fundamentally inexact because,
in the end, some sort of human judgement is required to
evaluate the hypothesized output. That is, we need some way
to measure the quality of our guesses, say, as assessed by
a human judge who compares them to the actual transcript.
Thankfully, the closely related problem of scoring machine
translations has been extensively studied. In what follows,
we discuss how we measure the accuracy of our guesses.
7A word which is associated with multiple pronunciations is weighted
according to the closest pronunciation, i.e., we take the maximum weight
of all associated weights for the given word.
E. Measuring the Quality of Our Output
Since the early 1990s, much work has gone into ﬁnding
appropriate metrics for scoring machine transcriptions from
automatic speech recognition and transcription systems. In
that context, the main task is to generate a literal transcrip-
tion of every word that is spoken. The closer the machine
transcription is to a human translation, the better it is. Early
approaches for automatically measuring such performance
simply relied on examining the proportion of word errors
between the actual and transcribed conversations (i.e., the
Word Error Rate (WER)), but WER has been shown to be
a poor indicator of the quality of a transcript since good
performance in this context depends not only on the amount
of errors, but also on the types of errors being made. For
example, from the perspective of human interpretation, it
often does not matter if the transcribed word is ‘governed’
instead of ‘governing’.
Hence, modern automatic scoring systems reward candi-
date text based on the transcription’s adequacy (i.e., how
well the meaning conveyed by the reference transcription
is also conveyed by the evaluated text) and ﬂuency (i.e.,
the lengths of contiguous subsequences of matching words).
To date, many such scoring systems have been designed,
with entire conferences and programs dedicated solely to this
topic. For instance, NIST has coordinated evaluations under
the Global Autonomous Language Exploitation (GALE)
program since the mid-nineties. While the search for better
metrics for translation evaluation remains an ongoing chal-
lenge, one widely accepted scoring system is the METEOR
Automatic Metric for Machine Translation by Lavie and
Denkowski [35]. METEOR was designed to produce quality
scores at the sentence level which correlate well with those
assigned by human judges. We evaluate the quality of our
guesses using METEOR; for concreteness, we now review
pertinent details of that scoring system.
Lavie and Denkowski’s method evaluates a hypothesized
transcription by comparison with a reference transcription.
The two transcripts are compared by aligning ﬁrst exact
word matches, followed by stemmed word matches, and
ﬁnally synonymous word matches. The alignment is per-
formed by matching each unigram string in the reference
transcription to at most one word in the hypothesis tran-
scription. To compute the score from such an alignment, let
m be the number of matched unigrams, h the number of
unigrams in the hypothesis, and r the number of unigrams
in the reference. The standard metrics of unigram precision
(P = m/h) and recall (R = m/r) are then computed.
Next, the parameterized f-score, i.e., the harmonic mean
of P and R given a relative weight (α) on precision, is
computed:
P ∗ R
Fmean =
α ∗ P + (1 − α) ∗ R
.
To penalize hypotheses which have relatively long sequences
13
0123456PhoneticEditDistance0.00.20.40.60.81.0Percentileof incorrect words, Lavie and Denkowski count the number
c of ‘chunk’ sequences of matched unigrams which are
adjacent, and in the correct order in the hypothesis. A frag-
mentation penalty is then computed as Pf rag = γ ∗ (c/m)β,
where γ and β are parameters determining the maximum
penalty and relative impact of fragmentation, respectively.
The ﬁnal METEOR score is then calculated as Sm =
(1 − Pf rag) ∗ Fmean for each hypothesis.
Figure 10. Example scoring of three hypothesized guesses. For each, the
hypothesized guess is on the left, with the reference on the right. Filled
circles represent exact matches. Hollow circles show matches based on
stemming.
Denkowski and Lavie [13] performed extensive analysis
to determine appropriate values for the parameters α, β,
and γ which optimize the correlation between METEOR
score and human judgments. In our experiments, we use
the parameter set that is optimized to correlate with the
Human Targeted translation Edit Rate (HTER) metric for
human judgement on the GALE-P2 dataset [14]. We disable
synonym matching as our system does no semantic analysis,
and thus any such matches would be entirely coincidental.
Some examples are shown in Figure 10. Notice that even a
single error can result in scores below 0.8 (e.g., in part (a)).
Moreover, in some cases, a low score does not necessarily
imply that the translation would be judged as poor by a
human (e.g., one can argue that the translation in part (c)
is in fact quite decent). Finally, Lavie indicates that scores
over 0.5 “generally reﬂect understandable translations” and
that scores over 0.7 “generally reﬂect good and ﬂuent
translations” in the context of machine translation [34].
VI. EMPIRICAL EVALUATION
In the analysis that follows, we explore both content-
dependent and content-independent evaluations. In both
cases, we assume a speaker-independent model wherein we
have no access to recordings of speech by the individual(s)
involved in the conversation. In the content-dependent case,
we perform two experiments, each incorporating multiple
different utterances of a particular sentence. We use TIMIT’s
SA1 and SA2 sentences for these experiments because each
is spoken exactly once by each of the 630 speakers, provid-
ing a rare instance of sufﬁcient examples for evaluation. In
the content-independent case, we incorporate all TIMIT ut-
14
terances.8 Except where explicitly speciﬁed, all experiments
are 10-fold cross-validation experiments and are performed
independently on each dialect. As discussed in Section V,
for these experiments we assume that the segmentation of
phonemes is correct to within human transcriber tolerances.
However,
the effects of this assumption are speciﬁcally
examined in a small experiment described separately below.
SA1:“She had your dark suit in greasy wash water all year.”
She had year dark suit a greasy wash water all year.
She had a dark suit a greasy wash water all year.
She had a dark suit and greasy wash water all year.
SA2:“Don’t ask me to carry an oily rag like that.”
Don’t asked me to carry an oily rag like that.
Don’t ask me to carry an oily rag like dark.
Don’t asked me to carry an oily rag like dark.
Score
0.67
0.67
0.67
Score
0.98
0.82
0.80
TOP SCORING HYPOTHESES FROM THE NEW ENGLAND DIALECT.
Table VI
Figure 11 shows the distributions of METEOR scores
under each of the dialects for the two content-dependent
experiments. For SA1, the results are fairly tightly grouped
around a score of 0.6. The SA2 scores show signiﬁcantly
more variance; while some hypotheses in this case were
relatively poor, others attained perfect scores. To ease inter-
pretation of the scores, we provide the three highest-scoring
hypotheses for each sentence, along with their scores, in
Table VI. In addition, recall that sentences with scores over
0.5 are generally considered understandable in the machine
translation context; 91% of our SA1 reconstructions and
98% of our SA2 reconstructions exceed this mark.
The independent case, on the other hand, proves to be a
more challenging test for our methodology. However, we are
still able to reconstruct a number of sentences that are easily
interpretable by humans. For instance, Table VII shows the
ﬁve highest-scoring hypotheses for this test on the New
England dialect. In addition, a number of phrases within
the sentences are exactly correct (e.g., ‘the two artists’). For
completeness, we note that only 2.3% of our reconstructions
score above 0.5. However, the average score for the top
10% (see Figure 12) is above 0.45. That said, we remind
the reader that no reconstruction, even a partial one, should
be possible; indeed, any cryptographic system that leaked
as much information as shown here would immediately be
deemed insecure.
To mitigate any concern regarding our two previous
simplifying assumptions, namely, the accurate segmentation
of frame size sequences on phoneme boundaries and of
(noisy) phoneme sequences on word boundaries, we perform
one ﬁnal experiment. We believe sufﬁcient evidence has
been given to show that we can accomplish these tasks
in isolation; however, one possible critique stems from the
8We follow the standard practice in the speech recognition community
and use the SA1 and SA2 sentences for training only.
it'snoteasy•tocreateilluminatingexamplesisnotexcepttocreateilluminatedexamples••◦•cliffwassoothed•••bytheluxuriousmassagecliffwassoothedbyaluxuriousmassage•••that'syourheadache◦•thatyouheadacheMETEOR Score: 0.53METEOR Score: 0.18METEOR Score: 0.78ABC(a) SA1: “She had your dark suit in greasy wash water all year.”
(b) SA2: “Don’t ask me to carry an oily rag like that.”
Figure 11. METEOR scores for all hypothesized transcripts of sentences SA1 and SA2 for each dialect in the TIMIT dataset.
Hypothesis
 Codes involves the displacement of aim.
 The two artists instance attendants.
 Artiﬁcial intelligence is carry all.
 Bitter unreasoning dignity.
 Jar, he whispered.
Reference Sentence
Change involves the displacement of form.
The two artists exchanged autographs.
Artiﬁcial intelligence is for real.
Bitter unreasoning jealousy.
Honey, he whispered.
METEOR Score
0.57
0.49
0.49
0.47
0.47
THE FIVE HIGHEST SCORING HYPOTHESES FROM THE NEW ENGLAND DIALECT UNDER THE CONTENT-INDEPENDENT MODEL.
Table VII
potential effects, when these assumptions are lifted, on the
efﬁcacy of the methodology as a whole. Thus we remove
these assumptions in a small, content-independent exper-
iment comprised of the audio samples spoken by female
speakers in the “Army Brat” dialect of the TIMIT corpus.
The average score for the top 10%, in this case, is 0.19, with
a high score of 0.27. We remind the reader that even such
low scoring hypotheses can be interpretable (see Figure 10),
and we stress that these results are preliminary and that
there is much room for improvement—in particular, recently
proposed techniques can be directly applied in our setting
(see Section V-C). Moreover, there are opportunities for
extensions and optimizations at every stage of our approach,
including, but not limited to, weighting the inﬂuence of the
different classiﬁcation and language models. In addition,
other scoring systems for machine translation exist (e.g.,
NIST and BLEU), which may be appropriate in our context.
We plan to explore these new techniques, optimizations and
metrics in the future.
A. An Adversarial Point of View (Measuring Conﬁdence)
Due to the difﬁcult nature of our task (i.e., numerous
factors inﬂuencing phonetic variation and the fact that we
operate on encrypted data), an adversary is unlikely to be
able to construct an accurate transcript of every sentence
uttered during a conversation. Therefore, she must have
some way to measure her conﬁdence in the output generated,
and only examine output with conﬁdence greater than some
threshold. To show this can be done, we deﬁne one such
conﬁdence measure, based on our phonetic edit distance,
which indicates the likelihood that a given transcript
is
approximately correct.
Our conﬁdence measure is based on the notion that
close pronunciation matches are more likely to be correct
than distant matches. We use the mean of the probability