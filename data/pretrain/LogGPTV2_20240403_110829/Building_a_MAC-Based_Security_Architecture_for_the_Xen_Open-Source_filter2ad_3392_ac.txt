main on the managed system (the current Xen approach),
or it can run on a separate special-purpose system, such as
the Hardware Management Console (HMC) used by PHYP
and other commercial virtualization solutions. The policy
management is needed to change or validate a policy; it is
not necessary to run the system and enforce the instantiated
policies.
4.3 Policy Enforcement
Mandatory access control is implemented as a reference
monitor. The mediation of references of VMs to shared
virtual resources is implemented by inserting security en-
forcement hooks into the code path inside the hypervisor
where VMs share virtual resources. Hooks call into the ac-
cess control module (ACM) for decisions and enforce them
locally at the hook. Isolation of individual virtual resources
is inherited from Xen since it is a general design issue for
hypervisors rather than a security-speci(cid:2)c requirement.
4.3.1 Reference Monitor
sHype strictly separates access control enforcement from
the access control policy, as in the Flask [33] architecture.
5
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:13:59 UTC from IEEE Xplore.  Restrictions apply. 
VM(cid:13)
(Subject)(cid:13)
XML(cid:13)
Security(cid:13)
Policy(cid:13)
Security(cid:13)
Policy(cid:13)
Manager(cid:13)
VM(cid:13)
1. H_Call(cid:13)
Hypervisor(cid:13)
Hook(cid:13)
Object(cid:13)
2. Authorization Query(cid:13)
3. Authorization Decision(cid:13)
Core Hypervisor(cid:13)
Binary(cid:13)
Security(cid:13)
Policy(cid:13)
A(cid:13)ccess(cid:13)
C(cid:13)ontrol(cid:13)
M(cid:13)odule(cid:13)
Figure 4. sHype security reference monitor
We describe the control architecture in the context of the
hypervisor, but it will also be used in the MAC domains.
Figure 4 shows the sHype access control architecture as part
of the core hypervisor and depicts the relationships between
its three major design components. Security enforcement
hooks are carefully inserted into the core hypervisor and
cover references of VMs to virtual resources. Enforcement
hooks retrieve access control decisions from the access con-
trol module (ACM).
The ACM authorizes access of VMs to resources based
on the policy rules and the security labels attached to VMs
(CW-types, TE-types) and resources (TE-types). The for-
mal security policy de(cid:2)nes these access rules as well as the
structure and interpretation of security labels for VMs and
resources. Finally, a hypervisor interface enables trusted
policy-management VMs to manage the ACM security pol-
icy.
4.3.2 Access Control Hooks
A security enforcement hook is a specialized access en-
forcement function that guards access to a virtual resource
by VMs. It enforces information (cid:3)ow constraints between
VMs according to the security policy. Each security hook
adheres to the following general pattern: (1) gather access
control information (determine VM labels, virtual resource
labels, and access operation type); (2) determine access de-
cision by calling the ACM; and (3) enforce access control
decision. Hooks are functionally transparent if the access is
allowed, and they return an error code otherwise.
Using security hooks, sHype minimizes the interference
with the core hypervisor while enforcing the security pol-
icy on access to virtual resources. We have placed secu-
rity enforcement hooks at the following places inside the
hypervisor in order to enforce the Chinese Wall and Type
Enforcement policies.
(cid:15) Domain management operations: This hook calls into the
ACM reporting the security reference of the domain orig-
inating the operation and of the domain that is being cre-
ated, destroyed, saved, restored, migrated, etc. Calls from
these hooks are used by the ACM (1) to assign security
labels to created domains and to free labels of destroyed
domains; (2) to check Chinese Wall con(cid:3)ict sets before
creating, resuming, or migrating-in domains; and (3) to
adjust the set of running CW-types when destroying, sus-
pending, or migrating-out domains.
(cid:15) Event channel operations: Event-channel hooks mediate
the creation and destruction of event channels between
domains. The ACM uses calls from these hooks to de-
cide whether the two domains setting up an event channel
are members of a common coalition. If the ACM returns
a permitted decision, the event channel setup continues
beyond the hook. The subsequent sending and receiving
of eventsq via the connected channel do not need to be
mediated because they would yield the same result (un-
less the policy changes, see below). If the hook receives a
deny decision, the event channel setup is aborted and the
hypervisor call returns with an error.
(cid:15) Shared memory hook: Grant-table hypervisor calls allow
one VM to grant access to some if its memory pages
to another VM. This mechanism (synchronized via event
channels) enables ef(cid:2)cient communication between VMs
running on the same hypervisor. Since the shared mem-
ory may in some cases be established dynamically during
the communication (e.g., sending and receiving network
packets or reading and writing from virtual disks), the se-
curity hook guarding this operation may be on the perfor-
mance critical path.
Decision caching. Since neither the event channel nor the
shared memory hook calls induce any state change in the
ACM, we use caching of access control decisions to mini-
mize the overhead introduced by the security hooks calling
into the ACM and the ACM authorizing access.
We cache access control decisions locally in the data
structures involved in a grant-table or event-channel oper-
ation the (cid:2)rst time an access control decision is required
between two VMs. The decision cache is not used for do-
main operation hooks because the ACM must be aware of
these calls to update its security state. We are experiment-
ing with multiple cache layouts to (cid:2)nd the best trade-off
between memory requirements and lookup speed.
Decision caching achieves near-zero overhead on the
critical path at the cost of additional management and com-
plexity. When a VM is destroyed or migrated, all cache
entries regarding this VM must be cleared. The overhead of
clearing these caches is very low.
Policy Changes. When the policy changes, we must ex-
plicitly revoke a shared resource from a VM that is no
longer authorized to use it. Since we use extensive caching,
we must propagate access authorization changes into the
6
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:13:59 UTC from IEEE Xplore.  Restrictions apply. 
caches of VMs. Additionally, we de(cid:2)ne a re-evaluation
function for both event-channel and grant-table hooks be-
cause these hooks check permissions only when an event-
channel or a shared memory area is set up, and not when it
is used. When invoked by the ACM, the re-evaluation func-
tion (1) re-evaluates the original access control decision,
and (2) revokes shared resources in case the authorization
is no longer given.
Revocation of event-channels from inside Xen is
straightforward. VMs trying to use revoked event-channels
will receive error codes which must be handled regardless
of access control. Memory shared between VMs will typ-
ically not be directly handed over by the Guest OS to ap-
plications but rather used exclusively inside device drivers.
Consequently, device drivers might run into a memory ac-
cess fault when trying to send a request via shared memory
to which their access was revoked. We are currently work-
ing on a call-back mechanism, initiated by the hypervisor,
so that revoked shared memory can be reported to affected
VMs and handled there in an more controlled fashion, al-
lowing for more graceful failure.
4.3.3 Access Control Module (ACM)
The ACM maintains policy state, makes policy decisions
based on the current policy, interacts with the policy man-
ager VM to establish a security policy, and triggers callback
functions to re-evaluate access control decisions in the hy-
pervisor when the policy changes.
The ACM stores all security policy information locally
in the hypervisor, and supports policy management through
a privileged hypervisor call interface. This interface is
access-controlled by a specialized hook and will only be
accessible by policy-management-privileged domains.
During domain operations, the ACM is called by security
hooks and allocates and de-allocates security labels for cre-
ated and destroyed domains according to the policy. These
labels are used for access control decisions. The virtual ma-
chine con(cid:2)guration includes references for the ACM that
are used to determine the label for a newly created domain.
In our example, such a label consists of a set of TE-types
and a set of CW-types, as described in Section 4.1.
The ACM maintains the policy state needed to enforce
the Chinese Wall policy. For this purpose, the ACM main-
tains a Running CW Types array indexed by the CW-type
and containing a reference count that describes the number
of running domains that have this CW-type. Whenever a
domain is started, the ACM determines those con(cid:3)ict sets
with which this domain shares a CW-type. Then it veri(cid:2)es
if any of the other CW-types of these con(cid:3)ict sets is running.
If any of these CW-types’ reference count is non-zero, then
we have a Chinese Wall con(cid:3)ict and the current domain is
not permitted to start. Otherwise, the current domain is per-
mitted to start and the Running CW Types’ reference counts
are incremented for those CW types that are assigned to the
started domain. If a domain is destroyed, the Running CW
Types’ reference counts of this virtual machine’s CW-types
are decremented.
Access control decisions for the Type Enforcement pol-
icy are simple. The ACM looks up the Coalition set of
those domains that are trying to establish an event chan-
nel or shared memory. If both domains share a common TE
type (coalition), then the access is permitted. Otherwise it
is denied. It can be implemented as an n-bit AND opera-
tion over the TE-type vectors of the domains where n is the
number of known TE types (coalitions).
4.4 MAC domains
MAC domains enable multiple coalitions to share a real
resource by creating isolated virtual resources based on the
real resource (recall the vdisk device domain in Figure 2).
If suf(cid:2)cient hardware resources are available and coalitions
don’t need to cooperate on higher layers, MAC domains are
not necessary because hardware can be exclusively assigned
to a single coalition and no VM needs to participate in mul-
tiple coalitions. We sketch brie(cid:3)y how we envision MAC
domains to work. They must offer the following guarantees
in order to conform to reference monitor requirements:
1.
Isolate exported virtual resources (e.g., the two virtual
disks for the Order and the Advertising coalition) inside
the MAC domain at least as well as the hypervisor isolates
its virtual resources.
2. Control access of VMs to those resources according to
the Type Enforcement Policy (i.e., only allow VMs that
are members of the coalition to which the virtual resource
is assigned to access the resource).
The isolation property can be achieved using mandatory ac-
cess control inside the MAC domain, e.g., using SELinux.
The access control property requires a MAC domain to
discover the coalition membership (TE types) of the re-
questing domain. For this reason, sHype offers to MAC
domains a hypervisor call that returns the coalition member-
ship information of a connected domain using the protected
policy information of the ACM. The hypervisor will return
those coalitions (TE types) of which both the MAC domain
and the requesting VM are members. Based on this infor-
mation, the MAC domain permits access of the requesting
VM only to virtual resources that share membership in the
same coalition(s).
Multi-coalition VMs, besides implementing the shar-
ing of hardware resources among coalitions, also form the
natural environment in which controlled sharing between
coalitions on higher layers and with (cid:2)ner granularity can
be implemented (e.g., with (cid:2)le and operation granularity
based on OS-level MAC policies such as SELinux policies).
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:13:59 UTC from IEEE Xplore.  Restrictions apply. 
7
While sHype forms isolated coalitions and restricts sharing
to multi-coalition VMs, these VMs can overcome this isola-
tion in carefully designed and trustworthy environments to
ful(cid:2)ll application requirements.
5 Evaluation
5.1 sHype-Covered Resources
Figure 5 shows the virtualized resources sorted accord-
ing to where they are implemented. The TCB coverage
column shows how well their isolation and mandatory ac-
cess control is covered by the sHype reference monitor. We
distinguish whether the implementing entity is serving a
single coalition or multiple coalitions since the lat-
ter requires MAC control.
5.2 Code Impact
The sHype access control architecture for Xen com-
prises 2600 lines of code. We inserted three MAC security
hooks into Xen hypervisor (cid:2)les to control domain opera-
tions, event channel setup, and shared memory setup. Two
out of three hooks are off the performance critical path.
One hook (shared memory setup) can be on or off the crit-
ical path depending on how shared memory is used by a
domain. We implemented a generic interface (akin to the
Linux Security Modules interface but much simpler) upon
which various policies can be implemented. We have imple-
mented the Chinese Wall and the Type Enforcement poli-
cies for Xen as well as the caching of event-channel and
grant-table access decisions. Maintaining sHype within the
evolving Xen hypervisor code base has proven easy.
5.3 Performance
resource(cid:13)
implementation(cid:13)
event(cid:13)
channel(cid:13)
shared(cid:13)
memory(cid:13)
virtual(cid:13)
disk(cid:13)
virtual(cid:13)
TTY(cid:13)
virtual(cid:13)
LAN(cid:13)
TCB coverage(cid:13)
single (cid:13)/(cid:13) multi(cid:13)
Hypervisor(cid:13)
X(cid:13) X(cid:13)
local VM(cid:13)
VMs on multiple(cid:13)
systems(cid:13)
X(cid:13) X(cid:13)
X(cid:13)
X(cid:13)
..fully covered by sHype(cid:13)
..partly covered by sHype(cid:13)
/(cid:13)
/(cid:13)
/(cid:13)
Figure 5. Current resource coverage in Xen
If event channels, shared memory, virtual disks, vir-
tual TTY, or vLANs are shared within a single coalition,
sHype fully covers the TCB for sharing between coali-
tions. While the sHype architecture is comprehensive and
its policy enforcement covers the communication between
domains, sHype relies on MAC domains to correctly iso-
late virtual devices from each other (see Section 4.4). Such
multi-coalition MAC domains are necessary if real periph-