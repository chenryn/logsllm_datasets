对于使用键范围分割槽的资料库（请参阅 “[根据键的范围分割槽](#根据键的范围分割槽)”），具有固定边界的固定数量的分割槽将非常不便：如果边界设定错误，可能会导致所有资料都在一个分割槽中，而其他分割槽则为空。手动重新配置分割槽边界将非常繁琐。
出于这个原因，按键的范围进行分割槽的资料库（如 HBase 和 RethinkDB）会动态建立分割槽。当分割槽增长到超过配置的大小时（在 HBase 上，预设值是 10GB），会被分成两个分割槽，每个分割槽约占一半的资料【26】。与之相反，如果大量资料被删除并且分割槽缩小到某个阈值以下，则可以将其与相邻分割槽合并。此过程与 B 树顶层发生的过程类似（请参阅 “[B 树](ch3.md#B树)”）。
每个分割槽分配给一个节点，每个节点可以处理多个分割槽，就像固定数量的分割槽一样。大型分割槽拆分后，可以将其中的一半转移到另一个节点，以平衡负载。在 HBase 中，分割槽档案的传输透过 HDFS（底层使用的分散式档案系统）来实现【3】。
动态分割槽的一个优点是分割槽数量适应总资料量。如果只有少量的资料，少量的分割槽就足够了，所以开销很小；如果有大量的资料，每个分割槽的大小被限制在一个可配置的最大值【23】。
需要注意的是，一个空的资料库从一个分割槽开始，因为没有关于在哪里绘制分割槽边界的先验资讯。资料集开始时很小，直到达到第一个分割槽的分割点，所有写入操作都必须由单个节点处理，而其他节点则处于空闲状态。为了解决这个问题，HBase 和 MongoDB 允许在一个空的资料库上配置一组初始分割槽（这被称为 **预分割**，即 pre-splitting）。在键范围分割槽的情况中，预分割需要提前知道键是如何进行分配的【4,26】。
动态分割槽不仅适用于资料的范围分割槽，而且也适用于杂凑分割槽。从版本 2.4 开始，MongoDB 同时支援范围和杂凑分割槽，并且都支援动态分割分割槽。
#### 按节点比例分割槽
透过动态分割槽，分割槽的数量与资料集的大小成正比，因为拆分和合并过程将每个分割槽的大小保持在固定的最小值和最大值之间。另一方面，对于固定数量的分割槽，每个分割槽的大小与资料集的大小成正比。在这两种情况下，分割槽的数量都与节点的数量无关。
Cassandra 和 Ketama 使用的第三种方法是使分割槽数与节点数成正比 —— 换句话说，每个节点具有固定数量的分割槽【23,27,28】。在这种情况下，每个分割槽的大小与资料集大小成比例地增长，而节点数量保持不变，但是当增加节点数时，分割槽将再次变小。由于较大的资料量通常需要较大数量的节点进行储存，因此这种方法也使每个分割槽的大小较为稳定。
当一个新节点加入丛集时，它随机选择固定数量的现有分割槽进行拆分，然后占有这些拆分分割槽中每个分割槽的一半，同时将每个分割槽的另一半留在原地。随机化可能会产生不公平的分割，但是平均在更大数量的分割槽上时（在 Cassandra 中，预设情况下，每个节点有 256 个分割槽），新节点最终从现有节点获得公平的负载份额。Cassandra 3.0 引入了另一种再平衡的演算法来避免不公平的分割【29】。
随机选择分割槽边界要求使用基于杂凑的分割槽（可以从杂凑函式产生的数字范围中挑选边界）。实际上，这种方法最符合一致性杂凑的原始定义【7】（请参阅 “[一致性杂凑](#一致性杂凑)”）。最新的杂凑函式可以在较低元资料开销的情况下达到类似的效果【8】。
### 运维：手动还是自动再平衡
关于再平衡有一个重要问题：自动还是手动进行？
在全自动再平衡（系统自动决定何时将分割槽从一个节点移动到另一个节点，无须人工干预）和完全手动（分割槽指派给节点由管理员明确配置，仅在管理员明确重新配置时才会更改）之间有一个权衡。例如，Couchbase、Riak 和 Voldemort 会自动生成建议的分割槽分配，但需要管理员提交才能生效。
全自动再平衡可以很方便，因为正常维护的操作工作较少。然而，它可能是不可预测的。再平衡是一个昂贵的操作，因为它需要重新路由请求并将大量资料从一个节点移动到另一个节点。如果没有做好，这个过程可能会使网路或节点负载过重，降低其他请求的效能。
这种自动化与自动故障检测相结合可能十分危险。例如，假设一个节点过载，并且对请求的响应暂时很慢。其他节点得出结论：过载的节点已经死亡，并自动重新平衡丛集，使负载离开它。这会对已经超负荷的节点，其他节点和网路造成额外的负载，从而使情况变得更糟，并可能导致级联失败。
出于这个原因，再平衡的过程中有人参与是一件好事。这比全自动的过程慢，但可以帮助防止运维意外。
## 请求路由
现在我们已经将资料集分割到多个机器上执行的多个节点上。但是仍然存在一个悬而未决的问题：当客户想要发出请求时，如何知道要连线哪个节点？随著分割槽的重新平衡，分割槽对节点的分配也发生变化。为了回答这个问题，需要有人知晓这些变化：如果我想读或写键 “foo”，需要连线哪个 IP 地址和埠号？
这个问题可以概括为 **服务发现（service discovery）** ，它不仅限于资料库。任何可透过网路访问的软体都有这个问题，特别是如果它的目标是高可用性（在多台机器上执行冗余配置）。许多公司已经编写了自己的内部服务发现工具，其中许多已经作为开源释出【30】。
概括来说，这个问题有几种不同的方案（如图 6-7 所示）:
1. 允许客户联络任何节点（例如，透过 **回圈策略的负载均衡**，即 Round-Robin Load Balancer）。如果该节点恰巧拥有请求的分割槽，则它可以直接处理该请求；否则，它将请求转发到适当的节点，接收回复并传递给客户端。
2. 首先将所有来自客户端的请求传送到路由层，它决定了应该处理请求的节点，并相应地转发。此路由层本身不处理任何请求；它仅负责分割槽的负载均衡。
3. 要求客户端知道分割槽和节点的分配。在这种情况下，客户端可以直接连线到适当的节点，而不需要任何中介。
以上所有情况中的关键问题是：作出路由决策的元件（可能是节点之一，还是路由层或客户端）如何了解分割槽 - 节点之间的分配关系变化？
![](../img/fig6-7.png)
**图 6-7 将请求路由到正确节点的三种不同方式。**
这是一个具有挑战性的问题，因为重要的是所有参与者都达成共识 - 否则请求将被传送到错误的节点，得不到正确的处理。在分散式系统中有达成共识的协议，但很难正确地实现（见 [第九章](ch9.md)）。
许多分散式资料系统都依赖于一个独立的协调服务，比如 ZooKeeper 来跟踪丛集元资料，如 [图 6-8](../img/fig6-8.png) 所示。每个节点在 ZooKeeper 中注册自己，ZooKeeper 维护分割槽到节点的可靠对映。其他参与者（如路由层或分割槽感知客户端）可以在 ZooKeeper 中订阅此资讯。只要分割槽分配发生了改变，或者丛集中新增或删除了一个节点，ZooKeeper 就会通知路由层使路由资讯保持最新状态。
![](../img/fig6-8.png)
**图 6-8 使用 ZooKeeper 跟踪分割槽分配给节点。**
例如，LinkedIn的Espresso使用Helix 【31】进行丛集管理（依靠ZooKeeper），实现了如[图6-8](../img/fig6-8.png)所示的路由层。HBase、SolrCloud和Kafka也使用ZooKeeper来跟踪分割槽分配。MongoDB具有类似的体系结构，但它依赖于自己的**配置伺服器（config server）** 实现和mongos守护程序作为路由层。
Cassandra 和 Riak 采取不同的方法：他们在节点之间使用 **流言协议（gossip protocol）** 来传播丛集状态的变化。请求可以传送到任意节点，该节点会转发到包含所请求的分割槽的适当节点（[图 6-7](../img/fig6-7.png) 中的方法 1）。这个模型在资料库节点中增加了更多的复杂性，但是避免了对像 ZooKeeper 这样的外部协调服务的依赖。
Couchbase 不会自动进行再平衡，这简化了设计。通常情况下，它配置了一个名为 moxi 的路由层，它会从丛集节点了解路由变化【32】。
当使用路由层或向随机节点发送请求时，客户端仍然需要找到要连线的 IP 地址。这些地址并不像分割槽的节点分布变化的那么快，所以使用 DNS 通常就足够了。
### 执行并行查询
到目前为止，我们只关注读取或写入单个键的非常简单的查询（加上基于文件分割槽的次级索引场景下的分散 / 聚集查询）。这也是大多数 NoSQL 分散式资料储存所支援的访问层级。
然而，通常用于分析的 **大规模并行处理（MPP, Massively parallel processing）** 关系型资料库产品在其支援的查询型别方面要复杂得多。一个典型的资料仓库查询包含多个连线，过滤，分组和聚合操作。MPP 查询最佳化器将这个复杂的查询分解成许多执行阶段和分割槽，其中许多可以在资料库丛集的不同节点上并行执行。涉及扫描大规模资料集的查询特别受益于这种并行执行。