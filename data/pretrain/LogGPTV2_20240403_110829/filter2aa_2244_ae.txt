good (or bad) their love life was before asking them about their life in general—there was a 66% 
correlation43 
Representativeness also explains the base rate fallacy, where people forget that if a 
particular characteristic is extremely rare, even an accurate test for that characteristic will show 
false alarms far more often than it will correctly identify the characteristic.  Security people run 
into this heuristic whenever someone tries to sell such things as face scanning, profiling, or data 
mining as effective ways to find terrorists. 
And lastly, representativeness explains the “law of small numbers,” where people assume 
that long-term probabilities also hold in the short run.  This is, of course, not true: if the results 
of three successive coin flips are tails, the odds of heads on the fourth flip are not more than 
50%.  The coin is not “due” to flip heads.  Yet experiments have demonstrated this fallacy in 
sports betting again and again.44 
Cost Heuristics 
Humans have all sorts of pathologies involving costs, and this isn’t the place to discuss 
them all.  But there are a few specific heuristics I want to summarize, because if we can’t evaluate 
costs right—either monetary costs or more abstract costs—we’re not going to make good security 
trade-offs. 
Mental Accounting 
Mental accounting is the process by which people categorize different costs.45  People don’t 
simply think of costs as costs; it’s much more complicated than that. 
Here are the illogical results of two experiments.46 
In the first, subjects were asked to answer one of these two questions: 
• 
Trade-off 1:  Imagine that you have decided to see a play where the admission is $10 
per ticket.  As you enter the theater you discover that you have lost a $10 bill.  
Would you still pay $10 for a ticket to the play? 
• 
Trade-off 2:  Imagine that you have decided to see a play where the admission is $10 
per ticket.  As you enter the theater you discover that you have lost the ticket.  The 
seat is not marked and the ticket cannot be recovered.  Would you pay $10 for 
The Psychology of Security—DRAFT 
19 
another ticket? 
The results of the trade-off are exactly the same.  In either case, you can either see the play 
and have $20 less in your pocket, or not see the play and have $10 less in your pocket.  But 
people don’t see these trade-offs as the same.  Faced with Trade-off 1, 88% of subjects said they 
would buy the ticket anyway.  But faced with Trade-off 2, only 46% said they would buy a second 
ticket.  The researchers concluded that there is some sort of mental accounting going on, and the 
two different $10 expenses are coming out of different mental accounts. 
The second experiment was similar.  Subjects were asked: 
• 
Imagine that you are about to purchase a jacket for $125, and a calculator for $15.  
The calculator salesman informs you that the calculator you wish to buy is on sale 
for $10 at the other branch of the store, located 20 minutes drive away.  Would you 
make the trip to the other store? 
• 
Imagine that you are about to purchase a jacket for $15, and a calculator for $125.  
The calculator salesman informs you that the calculator you wish to buy is on sale 
for $120 at the other branch of the store, located 20 minutes drive away.  Would you 
make the trip to the other store? 
Ignore your amazement at the idea of spending $125 on a calculator; it’s an old experiment.  
These two questions are basically the same: would you drive 20 minutes to save $5?  But while 
68% of subjects would make the drive to save $5 off the $15 calculator, only 29% would make 
the drive to save $5 off the $125 calculator. 
There’s a lot more to mental accounting.47  In one experiment,48 subjects were asked to 
imagine themselves lying on the beach on a hot day and how good a cold bottle of their favorite 
beer would feel.  They were to imagine that a friend with them was going up to make a phone 
call—this was in 1985, before cell phones—and offered to buy them that favorite brand of beer if 
they gave the friend the money.  What was the most the subject was willing to pay for the beer? 
Subjects were divided into two groups.  In the first group, the friend offered to buy the beer 
from a fancy resort hotel.  In the second group, the friend offered to buy the beer from a run-
down grocery store.  From a purely economic viewpoint, that should make no difference.  The 
value of one’s favorite brand of beer on a hot summer’s day has nothing to do with where it was 
purchased from.  (In economic terms, the consumption experience is the same.)  But people 
were willing to pay $2.65 on average for the beer from a fancy resort, but only $1.50 on average 
from the run-down grocery store. 
The experimenters concluded that people have reference prices in their heads, and that 
these prices depend on circumstance.  And because the reference price was different in the 
different scenarios, people were willing to pay different amounts.  This leads to sub-optimal 
results.  As Thayer writes, “The thirsty beer-drinker who would pay $4 for a beer from a resort 
but only $2 from a grocery store will miss out on some pleasant drinking when faced with a 
grocery store charging $2.50.” 
Researchers have documented all sorts of mental accounting heuristics.  Small costs are 
often not “booked,” so people more easily spend money on things like a morning coffee.  This is 
why advertisers often describe large annual costs as “only a few dollars a day.”  People segregate 
frivolous money from serious money, so it’s easier for them to spend the $100 they won in a 
football pool than a $100 tax refund.  And people have different mental budgets.  In one 
experiment that illustrates this,49 two groups of subjects were asked if they were willing to buy 
tickets to a play.  The first group was told to imagine that they had spent $50 earlier in the week 
on tickets to a basketball game, while the second group was told to imagine that they had 
The Psychology of Security—DRAFT 
20 
received a $50 parking ticket earlier in the week.  Those who had spent $50 on the basketball 
game (out of the same mental budget) were significantly less likely to buy the play tickets than 
those who spent $50 paying a parking ticket (out of a different mental budget). 
One interesting mental accounting effect can be seen at race tracks.50  Bettors tend to shift 
their bets away from favorites and towards long shots at the end of the day.  This has been 
explained by the fact that the average bettor is behind by the end of the day—pari-mutuel betting 
means that the average bet is a loss—and a long shot can put a bettor ahead for the day.  There’s 
a “day’s bets” mental account, and bettors don’t want to close it in the red. 
The effect of mental accounting on security trade-offs isn’t clear, but I’m certain we have a 
mental account for “safety” or “security,” and that money spent from that account feels different 
than money spent from another account.  I’ll even wager we have a similar mental accounting 
model for non-fungible costs such as risk: risks from one account don’t compare easily with risks 
from another.  That is, we are willing to accept considerable risks in our leisure account—
skydiving, knife juggling, whatever—when we wouldn’t even consider them if they were charged 
against a different account. 
Time Discounting 
“Time discounting” is the term used to describe the human tendency to discount future 
costs and benefits.  It makes economic sense; a cost paid in a year is not the same as a cost paid 
today, because that money could be invested and earn interest during the year.  Similarly, a 
benefit accrued in a year is worth less than a benefit accrued today. 
Way back in 1937, economist Paul Samuelson proposed a discounted-utility model to 
explain this all.  Basically, something is worth more today than it is in the future.  It’s worth 
more to you to have a house today than it is to get it in ten years, because you’ll have ten more 
years’ enjoyment of the house.  Money is worth more today than it is years from now; that’s why 
a bank is willing to pay you to store it with them. 
The discounted utility model assumes that things are discounted according to some rate.  
There’s a mathematical formula for calculating which is worth more—$100 today or $120 in 
twelve months—based on interest rates.  Today, for example, the discount rate is 6.25%, 
meaning that $100 today is worth the same as $106.25 in twelve months.  But of course, people 
are much more complicated than that. 
There is, for example, a magnitude effect: smaller amounts are discounted more than larger 
ones.  In one experiment, 51 subjects were asked to choose between an amount of money today or 
a greater amount in a year.  The results would make any banker shake his head in wonder.  
People didn’t care whether they received $15 today or $60 in twelve months.  At the same time, 
they were indifferent to receiving $250 today or $350 in twelve months, and $3,000 today or 
$4,000 in twelve months.  If you do the math, that implies a discount rate of 139%, 34%, and 
29%—all held simultaneously by subjects, depending on the initial dollar amount. 
This holds true for losses as well,52 although gains are discounted more than losses.  In 
other words, someone might be indifferent to $250 today or $350 in twelve months, but would 
much prefer a $250 penalty today to a $350 penalty in twelve months.  Notice how time 
discounting interacts with prospect theory here. 
Also, preferences between different delayed rewards can flip, depending on the time 
between the decision and the two rewards.  Someone might prefer $100 today to $110 tomorrow, 
but also prefer $110 in 31 days to $100 in thirty days. 
The Psychology of Security—DRAFT 
21 
Framing effects show up in time discounting, too.  You can frame something either as an 
acceleration or a delay from a base reference point, and that makes a big difference.  In one 
experiment,53 subjects who expected to receive a VCR in twelve months would pay an average of 
$54 to receive it immediately, but subjects who expected to receive the VCR immediately 
demanded an average $126 discount to delay receipt for a year.  This holds true for losses as 
well: people demand more to expedite payments than they would pay to delay them.54 
Reading through the literature, it sometimes seems that discounted utility theory is full of 
nuances, complications, and contradictions.  Time discounting is more pronounced in young 
people, people who are in emotional states – fear is certainly an example of this – and people 
who are distracted.  But clearly there is some mental discounting going on; it’s just not anywhere 
near linear, and not easily formularized. 
Heuristics that Affect Decisions 
And finally, there are biases and heuristics that affect trade-offs.  Like many other heuristics 
we’ve discussed, they’re general, and not specific to security.  But they’re still important. 
First, some more framing effects. 
Most of us have anecdotes about what psychologists call the “context effect”: preferences 
among a set of options depend on what other options are in the set.  This has been confirmed in 
all sorts of experiments—remember the experiment about what people were willing to pay for a 
cold beer on a hot beach—and most of us have anecdotal confirmation of this heuristic. 
For example, people have a tendency to choose options that dominate other options, or 
compromise options that lie between other options.  If you want your boss to approve your $1M 
security budget, you’ll have a much better chance of getting that approval if you give him a 
choice among three security plans—with budgets of $500K, $1M, and $2M, respectively—than 
you will if you give him a choice among three plans with budgets of $250K, $500K, and $1M. 
The rule of thumb makes sense: avoid extremes.  It fails, however, when there’s an 
intelligence on the other end, manipulating the set of choices so that a particular one doesn’t 
seem extreme. 
“Choice bracketing” is another common heuristic.  In other words: choose a variety.  
Basically, people tend to choose a more diverse set of goods when the decision is bracketed more 
broadly than they do when it is bracketed more narrowly.  For example, 55 in one experiment 
students were asked to choose among one of six different snacks that they would receive at the 
beginning of the next three weekly classes.  One group had to choose the three weekly snacks in 
advance, while the other group chose at the beginning of each class session.  Of the group that 
chose in advance, 64% chose a different snack each week, but only 9% of the group that chose 
each week did the same. 
The narrow interpretation of this experiment is that we overestimate the value of variety.  
Looking ahead three weeks, a variety of snacks seems like a good idea, but when we get to the 
actual time to enjoy those snacks, we choose the snack we like.  But there’s a broader 
interpretation as well, one borne out by similar experiments and directly applicable to risk 
taking: when faced with repeated risk decisions, evaluating them as a group makes them feel less 
risky than evaluating them one at a time.  Back to finance, someone who rejects a particular 
gamble as being too risky might accept multiple identical gambles. 
Again, the results of a trade-off depend on the context of the trade-off. 
The Psychology of Security—DRAFT 
22 
It gets even weirder.  Psychologists have identified an “anchoring effect,” whereby decisions 
are affected by random information cognitively nearby.  In one experiment56, subjects were 
shown the spin of a wheel whose numbers ranged from 0 and 100, and asked to guess whether 
the number of African nations in the UN was greater or less than that randomly generated 
number.  Then, they were asked to guess the exact number of African nations in the UN. 
Even though the spin of the wheel was random, and the subjects knew it, their final guess 
was strongly influenced by it.  That is, subjects who happened to spin a higher random number 
guessed higher than subjects with a lower random number.   
Psychologists have theorized that the subjects anchored on the number in front of them, 
mentally adjusting it for what they thought was true.  Of course, because this was just a guess, 
many people didn’t adjust sufficiently.  As strange as it might seem, other experiments have 
confirmed this effect. 
And if you’re not completely despairing yet, here’s another experiment that will push you 
over the edge.57  In it, subjects were asked one of these two questions: 
• 
Question 1:  Should divorce in this country be easier to obtain, more difficult to 
obtain, or stay as it is now? 
• 
Question 2:  Should divorce in this country be easier to obtain, stay as it is now, or 
be more difficult to obtain? 
In response to the first question, 23% of the subjects chose easier divorce laws, 36% chose 
more difficult divorce laws, and 41% said that the status quo was fine.  In response to the second 
question, 26% chose easier divorce laws, 46% chose more difficult divorce laws, and 29% chose 
the status quo.  Yes, the order in which the alternatives are listed affects the results. 
There are lots of results along these lines, including the order of candidates on a ballot. 
Another heuristic that affects security trade-offs is the “confirmation bias.”  People are 
more likely to notice evidence that supports a previously held position than evidence that 
discredits it.  ((Reference?))  Even worse, people who support position A sometimes mistakenly 