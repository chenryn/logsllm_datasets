title:On the performance of middleboxes
author:Mark Allman
On the Performance of Middleboxes
Mark Allman
BBN Technologies
PI:EMAIL
ABSTRACT
This paper presents a preliminary performance analysis of
a complex middlebox infrastructure in a real-world produc-
tion environment that serves several thousand people. While
prevalent, middleboxes (ﬁrewalls, NATs, etc.) have yet to
be systematically measured. This paper makes two contri-
butions: (i) we outline several methodologies and metrics
by which to measure middleboxes and (ii) we oﬀer prelimi-
nary application-layer measurements of one particular pro-
duction middlebox system. We show that the middlebox
infrastructure in question oﬀers a mixed bag of performance
implications (both positive and negative). In addition, we
quantify several failure modes introduced by the middlebox
infrastructure.
Categories and Subject Descriptors
C.4 [Computer Systems Organization]: Performance of
Systems; C.4 [Computer Systems Organization]: Per-
formance of Systems; C.2.0 [Computer-Communication
Networks]: General
General Terms
Measurement, Performance, Experimentation, Security
Keywords
ﬁrewalls, middleboxes, TCP performance
1.
INTRODUCTION
So-called “middleboxes”, such as ﬁrewalls, address trans-
lators and proxies (among others), are prevalent in today’s
Internet architecture. [3] oﬀers a discussion of the pros and
cons of such devices. These smart network entities are used
for a variety of reasons, for example:
• Security. Among the most common middleboxes is a
ﬁrewall that is used to control traﬃc to implement se-
curity policy between networks. Firewalls range from
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’03, October 27–29, 2003, Miami Beach, Florida, USA.
Copyright 2003 ACM 1-58113-773-7/03/0010 ...$5.00.
simple devices that do not pass traﬃc with given char-
acteristics (e.g., protocol number or port number) be-
tween the connected networks to complex devices that
act as proxies for transport layer connections. Fire-
walls are not the only type of middlebox inserted into
a path for security purposes. For instance, traﬃc nor-
malizers [6] can be used to remove ambiguities from
a traﬃc stream so that an intrusion detection system
can better predict the eﬀect of the traﬃc on an end
host.
• Performance. A second class of middleboxes is used
to increase the performance of standard networking
protocols. For instance, web caches or content delivery
networks (e.g., Akamai) are inserted into the network
such that users do not have to retrieve content from its
original source, but rather from a closer copy. In ad-
dition, various proposals and products allow boxes in
the middle of the network to “assist” protocols (e.g.,
by retransmitting dropped segments on the sender’s
behalf [2] or controlling the sending rate by manipulat-
ing TCP’s advertised window [7]). Finally, some mid-
dleboxes split an end-to-end transport connection into
two (or more) connections in an attempt to shorten
the control loop and enhance responsiveness on each
stream (e.g., I-TCP [1]).
• Address Translation. A ﬁnal common example mid-
dlebox is a network address translator [4]. These boxes
change the network layer addresses and/or transport
layer port numbers in traﬃc passing between two net-
works. One common use of this technology is to allow
multiple internal hosts to share one external IP ad-
dress. The need for this technique is sometimes due to
the lack of global address space allocated to a partic-
ular network (and, hence, the desire to leverage that
address space) or the desire to obfuscate internal ad-
dresses for security reasons.
While the reasons middleboxes have been added to the In-
ternet’s architecture are numerous the study of the impact
of these entities in production environments has not kept
up with deployment. For instance, several questions come
into play when thinking about middleboxes, such as: What
is the impact on performance (delay, application startup,
throughput, etc.)? What is the impact on the reliability
of TCP/IP protocols? What additional failure modes are
introduced and how prevalent are these? In this paper we
describe initial measurements of one particular middlebox
infrastructure with the goals of: (i) deﬁning a methodology
for testing the impact of middleboxes using active measure-
ments and (ii) gaining preliminary insight into the eﬀect a
large, production middlebox system has on traﬃc.
This paper is organized as follows. § 2 details our method-
ology and environment. § 3 outlines experiments involving
small transactions traversing the middlebox infrastructure.
§ 4 discusses measurements of the delay involved in travers-
ing the middlebox. § 5 discusses the persistence of the state
required to be kept in middleboxes in the context of keeping
TCP connections alive. § 6 outlines our experiments involv-
ing transmitting large data ﬁles. § 7 discusses measurements
involving the File Transfer Protocol (FTP) [10]. Finally, § 8
gives our conclusions and sketches future work in this area.
2. EXPERIMENTAL SETUP AND METHOD-
OLOGY
To measure the performance of a set of middle boxes at
one facility we setup two client hosts at Site1 1, which is
a production network serving several thousand people. In
addition, we setup a server host at Site2, which is roughly
550 miles from Site1. The “inside client” is located inside
the middlebox infrastructure (MBI) at Site1 while the “out-
side client” is on the WAN side of the MBI. The clients
are identical Intel Pentium 4 machines running Linux 2.4.9,
while the server is an Intel Pentium 3 running FreeBSD 4.6.
The server is located on the WAN side of the ﬁrewall in-
frastructure at Site2. This initial study only considers TCP
[9] traﬃc. The networking stacks of all machines are left
in their default conﬁguration (e.g., TCP option usage, ad-
vertised window size, etc.). All experiments outlined in this
paper were run concurrently inside and outside the MBI at
Site1.
FW1
Dest
Hub
LB1
LB2
Router
Internet
MeasBox1
FW2
MeasBox2
Figure 1: Simpliﬁed diagram of middlebox infras-
tructure.
Figure 1 shows the rough setup of the network (with
“MeasBox1” and “MeasBox2” being the inside and outside
clients, respectively). The MBI at Site1 consists of several
identical ﬁrewalls (“FW” boxes in the ﬁgure) attached to the
local and wide-area networks by load balancers (“LB” boxes
in the ﬁgure). The ﬁrewalls are stateful and proxy TCP con-
nections. Therefore, the load balancers are also required to
keep state (to always route the same TCP connection to the
same ﬁrewall). We measured Site1 ’s MBI from October 14,
2002 – January 27, 2003 (roughly 105 days). The focus of
this paper is not on the raw measurement values obtained
in our experiments, but rather on the comparison between
1The sites involved in the measurements conducted for this
paper have requested anonymity.
the measurements taken inside the MBI and those taken
outside.
Also, note that in the descriptions of the experiments in
the following sections we have chosen a number of constants
(e.g., timeouts). These constants were chosen to be rea-
sonable, but are largely arbitrarily. We believe this is ﬁne
because both clients share these constants and so the impact
of the choice is likely to impact both clients. By choosing
timeouts that are too short we may be biasing the measure-
ments (e.g., if we had watched a little longer we might have
seen a measurement complete, but rather we recorded it as a
failure). However, we believe the chosen constants are large
enough that we are not likely coercing a large number of
these sorts of situations.
3. TRANSACTION DELAY
The ﬁrst experiment’s goal is to assess the impact of the
MBI on the transaction time of a small request/response
protocol. To measure the transaction time we use the ﬁnger
protocol [11]. In our experiments the client opens a TCP
connection to the server and sends a carriage-return and
linefeed. The server then responds by sending two characters
back to the client. The client veriﬁes the returned characters
and then closes the TCP connection. If the client does not
receive a response within 30 seconds this is noted and the
transaction is terminated. We use custom-written client and
server code that timestamps each event in the transaction.
We insert a delay between ﬁnger transactions of roughly
2 minutes (the actual time is determined using a Poisson
process with a mean of 2 minutes).
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
Outside
Inside
0
0.2
0.4
0.6
0.8
1
Response Time (sec)
Figure 2: Distribution of response times for ﬁnger
transactions.
Figure 2 shows the distribution of the response time for
both clients. The dataset contains over 75,000 transactions
for each client. The median transaction time outside the
MBI is roughly 52 msec, while it takes roughly 253 msec
from inside the MBI. The next section shows that the MBI
adds minimal delay to data ﬂowing through an already open
TCP connection. Therefore, the ﬁve-fold increase in the
ﬁnger transaction time shown in ﬁgure 2 is likely caused by
the time required to instantiate connection state within the
MBI.
We recorded 42 failures2 on the inside client and 12 fail-
2There are several ways to look at the “failures” reported in
ures on the outside client. All the problems outside the MBI
are failures to connect to the server. Meanwhile, nearly all
the problems inside the ﬁrewall are recorded as timeouts.
Since the ﬁrewall is proxying TCP connections this likely
translates into problems connecting to the server or inter-
nal problems in the MBI3. Since there are 3.5 times more
failures inside the MBI we believe the majority of the prob-
lems are actually caused by the MBI. Finally, note that the
overall failure rate both inside and outside the middle box
infrastructure is quite low (under 0.1% in both cases).
4. FEEDBACK TIME
The last section focuses on small transactions and on the
costs associated with initiating such transactions. This sec-
tion focuses on delay through the MBI once a connection
has been established. To accomplish this we wrote a “TCP
ping” client and server. The client sends a small message
with a sequence number to the server, which echoes the mes-
sage back to the client via an already-established TCP con-
nection. The client records when the request is sent and
when the response arrives. After each request/response a
delay is inserted before the next request is transmitted. Our
dataset consists of ﬁve parallel pinging connections each us-
ing a diﬀerent mean delay, x: 0, 30, 300, 1800 and 3600 sec-
onds. The actual delay is determined using a Poisson process
with a mean of x seconds. In addition, if a response does
not arrive within 20 seconds the connection is closed and a
new connection started in its place. In this paper we focus
only on the experiments involving the connection that uses
x = 30 seconds due to space constraints. The connections
that used diﬀerent granularities show the same basic results.
We note that the measured values are not necessarily round-
trip times, but rather application-level feedback times (FT).
Since the pinging process is using TCP as its transport the
requests and responses are sent reliably and therefore may
incur retransmission delays if lost.
Figure 3 shows the distribution of FTs from inside and
outside the MBI with a mean inter-ping time of 30 seconds.
The dataset consists of over 303,000 pings from each client.
As the plot shows, the FT experienced on either side of the
MBI is nearly the same. The outside client experienced a
roughly 1 msec shorter FT on median and just over 2 msec
shorter FT on average then the inside client. This increase in
delay through the MBI is likely explained by the additional
number of local hops required to traverse the MBI.
Finally, we found only two types of errors: setting up
the TCP connection and the connection being closed due
to a timeout. We discuss the timeouts in the next section.
The inside client failed to make a connection 51 times in our
dataset compared to 46 times on the outside client. Since the
instances of failure connection failure is roughly the same in
this paper. We lump all problems experienced by the client
together as “failures”. However, this does not take into ac-
count that some failures are expected (caused by routine
and scheduled maintenance of the MBI). However, we feel
that such maintenance is part of the cost of using middle-
boxes and thus whether the failure was expected or not is
not reported in our data.
3The MBI at Site1 completes TCP’s three-way handshake
with the client before ensuring that a connection can be
made with the server. Therefore, the inside client can ﬁnish
its portion of the transaction before the MBI ﬁnishes the
TCP 3-way handshake to start a connection to the server.
F
D
C
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
1e-05 0.0001 0.001
R = 30
Outside
Inside
1
10
100
0.01
0.1
RTT (sec)
Figure 3: Distribution of feedback time when sam-
pling roughly every 30 seconds.
Metric
Inside Outside
Number of Connections
Mean Length (hr)
Median Length (hr)
Max Length (hr)
42
58.82
19.99
425.98
21
114.99
43.45
593.08
Table 1: Connection length statistics for connections
that sampled FT roughly every 30 seconds.
both clients the likely explanation is that the network path
between Site1 and Site2 became temporarily unusable.
5. CONNECTION LENGTH
In this section we discuss the length of the “ping” connec-
tions used for the FT measurements described in the last
section. We are attempting to assess how often the MBI
state gets internally unsynchronized, ﬂushed or in some way
makes an established connection unusable. Again, we an-
alyzed all the connections with varying sending rates and
the results come out consistent. Therefore, as in the last
section, we only present the results of the connection that
sends a request approximately every 30 seconds.
Table 1 provides the results of our analysis. We note that
the inside client used twice as many connections as the out-
side client. The median connection length on the outside
client is roughly twice as long as on the inside client. In addi-
tion, we note that the maximum connection length recorded
on the inside client is roughly 21 times longer than the me-
dian connection recorded on the inside client, indicating that
a client behind the MBI can sustain long connections. Also,
the distribution of connection lengths recorded on the inside
client does not suggest any sort of connection timeout in the
MBI biasing the measurements4.
We can further assign the blame of unnecessarily short-
ened connections to the MBI by noting that while there are
twice as many connections used inside the MBI the instances
of not being able to connect to the server (as outlined in
the last section) are roughly equivalent across client. This
4In addition, the operational security team at Site1 veriﬁed
that there is no intentional timeout conﬁgured into the MBI
that would eﬀect these tests.
indicates that, in many cases, simply making a new TCP
connection (and, hence starting over with fresh state in the
MBI) to the server ﬁxed the problem. These results sug-
gest that something within the MBI was ﬂushed or became
unsynchronized.
6. BULK DATA TRANSFER
We wrote a simple client and server to test the raw TCP
transfer speed through the MBI. The client sends 1 MB of
data from memory to the server. The server discards the
data upon receipt.
In our experiments, we conduct bulk
transfer measurements roughly every 10 minutes (where the
actual inter-measurement time is dictated by a Poisson pro-
cess with a mean of 10 minutes).
The last four bytes of data transmitted by the client con-
tain a random number that the server echoes back. The
transmission time is deﬁned as the time between when the
client application sends the ﬁrst byte of data to the operating
system for transmission until the client application receives
the random number echoed by the receiver.