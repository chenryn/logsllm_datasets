yum install -y zlib-devel.x86_64
cmake .. -DCMAKE_INSTALL_PREFIX=/home/digoal/avro -DCMAKE_BUILD_TYPE=Release -DTHREADSAFE=true
make
make test
make install
The "RelWithDebInfo" build type will build an optimized copy of the
library, including debugging symbols.  Use the "Release" build type if
you don't want debugging symbols.  Use the "Debug" build type if you
want a non-optimized library, with debugging symbols.
On Unix, you can request thread-safe versions of the Avro library's
global functions by defining the THREADSAFE cmake variable.  Just add
the following to your cmake invokation:
        -DTHREADSAFE=true
```
### libcurl
```
yum install -y libcurl-devel.x86_64
```
### librdkafka
```
git clone https://github.com/edenhill/librdkafka
```
```
/*
./configure --prefix=/home/digoal/librdkafka_home
make -j 32
make install
export PKG_CONFIG_PATH=/home/digoal/avro/lib/pkgconfig:/home/digoal/librdkafka_home/lib/pkgconfig:/usr/local/lib/pkgconfig:$PKG_CONFIG_PATH
*/
```
测试时建议按照在默认目录，否则可能又会有编译错误的问题。   
```
./configure 
make
make install
export PKG_CONFIG_PATH=/usr/local/lib/pkgconfig:$PKG_CONFIG_PATH
```
### PostgreSQL
安装略  
### bottledwater-pg
由于bottledwater-pg是PostgreSQL的一个插件，所以首先要安装PostgreSQL。    
```
git clone https://github.com/confluentinc/bottledwater-pg
cd bottledwater-pg
make
make install
vi /etc/ld.so.conf
/home/digoal/avro/lib
or
export LD_LIBRARY_PATH=/home/digoal/avro/lib:$LD_LIBRARY_PATH
```
可能需要重启数据库，加载bottledwater.so    
在数据库中创建插件    
```
psql
postgres=# create extension bottledwater ;
CREATE EXTENSION
```
## 部署confluent platform
这一步是基础，搭建好confluent，后面才能测试一下bottledwater-pg生产消息，以及从confluent platform消费消息的过程。   
http://docs.confluent.io/3.1.0/installation.html  
```
--
```
## 部署PostgreSQL流复制
bottledwater-pg需要从wal解析logical row，所以必须配置WAL_LEVEL=logical级别。  
同时wal sender进程数必须>=1。   
worker process数必须>=1。  
同时由于bottledwater为了保证可以支持断点续传，以及确保没有转换的WAL日志不会被主库删掉或覆盖掉，需要用到replication slot，因此需要配置replication_slots>=1。  
```
postgresql.conf
max_worker_processes = 8
wal_level = logical
max_wal_senders = 8
wal_keep_segments = 256
max_replication_slots = 4
```
同时为了保证数据库的WAL订阅者可以通过流复制协议连接到数据库，需要配置pg_hba.conf  
```
pg_hba.conf
local   replication     digoal                 trust
host    replication     digoal  127.0.0.1/32   trust
host    replication     digoal  0.0.0.0/0        md5
```
创建replication角色用户  
```
create role digoal login replication encrypted password 'digoal123';
```
## 使用bottledwater-pg生产消息
bottledwater-pg客户端命令的目的是从WAL解析日志，写入Kafka。   
command option需要配置如何连接到数据库（使用流复制连接），output格式，topic-prefix(建议为库名)，是否需要初始化快照，是否允许没有主键的表，kafka broker的连接地址和端口，schema-registry的连接地址和端口。   
以及一些kafka相关的配置。   
```
cd bottledwater-pg/kafka
./bottledwater --help
Exports a snapshot of a PostgreSQL database, followed by a stream of changes,
and sends the data to a Kafka cluster.
Usage:
  ./bottledwater [OPTION]...
Options:
  -d, --postgres=postgres://user:pass@host:port/dbname   (required)
                          Connection string or URI of the PostgreSQL server.
  -s, --slot=slotname     Name of replication slot   (default: bottledwater)
                          The slot is automatically created on first use.
  -b, --broker=host1[:port1],host2[:port2]...   (default: localhost:9092)
                          Comma-separated list of Kafka broker hosts/ports.
  -r, --schema-registry=http://hostname:port   (default: http://localhost:8081)
                          URL of the service where Avro schemas are registered.
                          Used only for --output-format=avro.
                          Omit when --output-format=json.
  -f, --output-format=[avro|json]   (default: avro)
                          How to encode the messages for writing to Kafka.
  -u, --allow-unkeyed     Allow export of tables that don't have a primary key.
                          This is disallowed by default, because updates and
                          deletes need a primary key to identify their row.
  -p, --topic-prefix=prefix
                          String to prepend to all topic names.
                          e.g. with --topic-prefix=postgres, updates from table
                          'users' will be written to topic 'postgres.users'.
  -e, --on-error=[log|exit]   (default: exit)
                          What to do in case of a transient error, such as
                          failure to publish to Kafka.
  -x, --skip-snapshot     Skip taking a consistent snapshot of the existing
                          database contents and just start streaming any new
                          updates.  (Ignored if the replication slot already
                          exists.)
  -C, --kafka-config property=value
                          Set global configuration property for Kafka producer
                          (see --config-help for list of properties).
  -T, --topic-config property=value
                          Set topic configuration property for Kafka producer.
  --config-help           Print the list of configuration properties. See also:
            https://github.com/edenhill/librdkafka/blob/master/CONFIGURATION.md
  -h, --help
                          Print this help text.
```
bottledwater配置文件说明如下  
```
./bottledwater --config-help 2>&1 |less  
## Global configuration properties
Property                                 | C/P | Range           |       Default | Description              
-----------------------------------------|-----|-----------------|--------------:|--------------------------
builtin.features                         |  *  |                 | gzip, snappy, ssl, sasl, regex | Indicates the builtin features for this build of librdkafka. An application can either query this value or attempt to set it with its list of required features to check for library support. *Type: CSV flags*
client.id                                |  *  |                 |       rdkafka | Client identifier. *Type: string*
metadata.broker.list                     |  *  |                 |               | Initial list of brokers. The application may also use `rd_kafka_brokers_add()` to add brokers during runtime. *Type: string*
bootstrap.servers                        |  *  |                 |               | Alias for `metadata.broker.list`
message.max.bytes                        |  *  | 1000 .. 1000000000 |       1000000 | Maximum transmit message size. *Type: integer*
message.copy.max.bytes                   |  *  | 0 .. 1000000000 |         65535 | Maximum size for message to be copied to buffer. Messages larger than this will be passed by reference (zero-copy) at the expense of larger iovecs. *Type: integer*
receive.message.max.bytes                |  *  | 1000 .. 1000000000 |     100000000 | Maximum receive message size. This is a safety precaution to avoid memory exhaustion in case of protocol hickups. The value should be at least fetch.message.max.bytes * number of partitions consumed from + messaging overhead (e.g. 200000 bytes). *Type: integer*
max.in.flight.requests.per.connection    |  *  | 1 .. 1000000    |       1000000 | Maximum number of in-flight requests the client will send. This setting applies per broker connection. *Type: integer*
max.in.flight                            |  *  |                 |               | Alias for `max.in.flight.requests.per.connection`
metadata.request.timeout.ms              |  *  | 10 .. 900000    |         60000 | Non-topic request timeout in milliseconds. This is for metadata requests, etc. *Type: integer*
topic.metadata.refresh.interval.ms       |  *  | -1 .. 3600000   |        300000 | Topic metadata refresh interval in milliseconds. The metadata is automatically refreshed on error and connect. Use -1 to disable the intervalled refresh. *Type: integer*
metadata.max.age.ms                      |  *  |                 |               | Alias for `topic.metadata.refresh.interval.ms`
topic.metadata.refresh.fast.cnt          |  *  | 0 .. 1000       |            10 | When a topic looses its leader this number of metadata requests are sent with `topic.metadata.refresh.fast.interval.ms` interval disregarding the `topic.metadata.refresh.interval.ms` value. This is used to recover quickly from transitioning leader brokers. *Type: integer*
topic.metadata.refresh.fast.interval.ms  |  *  | 1 .. 60000      |           250 | See `topic.metadata.refresh.fast.cnt` description *Type: integer*
topic.metadata.refresh.sparse            |  *  | true, false     |          true | Sparse metadata requests (consumes less network bandwidth) *Type: boolean*
topic.blacklist                          |  *  |                 |               | Topic blacklist, a comma-separated list of regular expressions for matching topic names that should be ignored in broker metadata information as if the topics did not exist. *Type: pattern list*
debug                                    |  *  | generic, broker, topic, metadata, queue, msg, protocol, cgrp, security, fetch, feature, all |               | A comma-separated list of debug contexts to enable. Debugging the Producer: broker,topic,msg. Consumer: cgrp,topic,fetch *Type: CSV flags*
socket.timeout.ms                        |  *  | 10 .. 300000    |         60000 | Timeout for network requests. *Type: integer*
socket.blocking.max.ms                   |  *  | 1 .. 60000      |           100 | Maximum time a broker socket operation may block. A lower value improves responsiveness at the expense of slightly higher CPU usage. *Type: integer*
socket.send.buffer.bytes                 |  *  | 0 .. 100000000  |             0 | Broker socket send buffer size. System default is used if 0. *Type: integer*
socket.receive.buffer.bytes              |  *  | 0 .. 100000000  |             0 | Broker socket receive buffer size. System default is used if 0. *Type: integer*
socket.keepalive.enable                  |  *  | true, false     |         false | Enable TCP keep-alives (SO_KEEPALIVE) on broker sockets *Type: boolean*
socket.nagle.disable                     |  *  | true, false     |         false | Disable the Nagle algorithm (TCP_NODELAY). *Type: boolean*
socket.max.fails                         |  *  | 0 .. 1000000    |             3 | Disconnect from broker when this number of send failures (e.g., timed out requests) is reached. Disable with 0. NOTE: The connection is automatically re-established. *Type: integer*
broker.address.ttl                       |  *  | 0 .. 86400000   |          1000 | How long to cache the broker address resolving results (milliseconds). *Type: integer*
broker.address.family                    |  *  | any, v4, v6     |           any | Allowed broker IP address families: any, v4, v6 *Type: enum value*
reconnect.backoff.jitter.ms              |  *  | 0 .. 3600000    |           500 | Throttle broker reconnection attempts by this value +-50%. *Type: integer*
statistics.interval.ms                   |  *  | 0 .. 86400000   |             0 | librdkafka statistics emit interval. The application also needs to register a stats callback using `rd_kafka_conf_set_stats_cb()`. The granularity is 1000ms. A value of 0 disables statistics. *Type: integer*
enabled_events                           |  *  | 0 .. 2147483647 |             0 | See `rd_kafka_conf_set_events()` *Type: integer*
error_cb                                 |  *  |                 |               | Error callback (set with rd_kafka_conf_set_error_cb()) *Type: pointer*
throttle_cb                              |  *  |                 |               | Throttle callback (set with rd_kafka_conf_set_throttle_cb()) *Type: pointer*
stats_cb                                 |  *  |                 |               | Statistics callback (set with rd_kafka_conf_set_stats_cb()) *Type: pointer*
log_cb                                   |  *  |                 |               | Log callback (set with rd_kafka_conf_set_log_cb()) *Type: pointer*
log_level                                |  *  | 0 .. 7          |             6 | Logging level (syslog(3) levels) *Type: integer*
log.thread.name                          |  *  | true, false     |         false | Print internal thread name in log messages (useful for debugging librdkafka internals) *Type: boolean*
log.connection.close                     |  *  | true, false     |          true | Log broker disconnects. It might be useful to turn this off when interacting with 0.9 brokers with an aggressive `connection.max.idle.ms` value. *Type: boolean*
socket_cb                                |  *  |                 |               | Socket creation callback to provide race-free CLOEXEC *Type: pointer*
connect_cb                               |  *  |                 |               | Socket connect callback *Type: pointer*
closesocket_cb                           |  *  |                 |               | Socket close callback *Type: pointer*
open_cb                                  |  *  |                 |               | File open callback to provide race-free CLOEXEC *Type: pointer*
opaque                                   |  *  |                 |               | Application opaque (set with rd_kafka_conf_set_opaque()) *Type: pointer*
default_topic_conf                       |  *  |                 |               | Default topic configuration for automatically subscribed topics *Type: pointer*
internal.termination.signal              |  *  | 0 .. 128        |             0 | Signal that librdkafka will use to quickly terminate on rd_kafka_destroy(). If this signal is not set then there will be a delay before rd_kafka_wait_destroyed() returns true as internal threads are timing out their system calls. If this signal is set however the delay will be minimal. The application should mask this signal as an internal signal handler is installed. *Type: integer*
api.version.request                      |  *  | true, false     |         false | Request broker's supported API versions to adjust functionality to available protocol features. If set to false the fallback version `broker.version.fallback` will be used. **NOTE**: Depends on broker version >=0.10.0. If the request is not supported by (an older) broker the `broker.version.fallback` fallback is used. *Type: boolean*
api.version.fallback.ms                  |  *  | 0 .. 604800000  |       1200000 | Dictates how long the `broker.version.fallback` fallback is used in the case the ApiVersionRequest fails. **NOTE**: The ApiVersionRequest is only issued when a new connection to the broker is made (such as after an upgrade). *Type: integer*