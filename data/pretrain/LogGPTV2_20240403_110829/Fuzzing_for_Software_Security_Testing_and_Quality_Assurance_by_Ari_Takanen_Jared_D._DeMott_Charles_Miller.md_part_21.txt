[ ] Ability to create unmodifiable arbitrary files
[ ] Ability to truncate or remove arbitrary files
17 M. Laakso, A. Takanen, J. Röning. “The Vulnerability Process: A Tiger Team Approach to Resolving
Vulnerability Cases.” In Proceedings of the 11th FIRST Conference on Computer Security Incident
Handling and Response. Brisbane, Australia. June 13–18, 1999.
6760 Book.indb 126 12/22/17 10:50 AM
4.3 Defect Metrics and Security 127
[ ] Ability to change owner of arbitrary dirs or files
[ ] Ability to change protection modes of arbitrary
dirs or files
[ ] Other: 
[ ] Execution
[ ] Exploitation involves a time window of the race
condition type
[ ] Other: 
3.5. Comment on impact: 
4.3.4 Interface Coverage Metrics
The coverage metric for interfaces consists of enumerating the protocols that the
fuzzer can support. Fuzzing (or any type of black-box testing) is possible for any
interfaces, whether they are APIs, network protocols, wireless stacks, command
line, GUI, files, return values, and so on. In 2006, fuzzers existed for more than
100 different protocol interfaces. Fuzzing an IMS system will require a different
set of protocols than fuzzing a mobile phone.
Interfaces are defined with various tools and languages. The variety of descrip-
tion languages used in protocol specifications creates a challenge for fuzzers to sup-
port them all. Creating any new proprietary fuzzing description languages poses a
tedious task. What description language should we use for describing the interfaces
to enable anyone to fuzz them? Some protocol fuzzers have decided to use the defini-
tion languages used by the standardization organizations such as IETF and 3GPP.
The most common definition languages are BNF or its variants for both binary
and text-based protocols, XML for text-based protocols, and TTCN and ASN.1
for binary protocols. For traffic mutation-based fuzzers, it might be beneficial to
collect coverage data from a wide variety of complex protocol use cases.
4.3.5 Input Space Coverage Metrics
The actual coverage inside the selected interface definition or against a protocol
specification is a critical metric for fuzzer quality. This is because there is no stan-
dardized formula or setup for fuzzing. A wide range of variables apply to the test
quality, especially if we are talking outside of just simple Web application fuzzing
and looking at more complex low-level protocol fuzzing. Even a smart gray-box
fuzzer could take weeks to find all the vulnerabilities, and a set time frame just
leaves room for error. The input space coverage will help in understanding what
was tested and what was not.
Because the input space is always infinite, it will always take an infinite time
to find all flaws. Optimizing (i.e., adding intelligence) will find more flaws earlier,
but can still never ensure that all flaws have been found. This would not be true,
of course, for protocols that have a physical limit, such as UDP packet size with
no fragmentation options. Only then you will have a physical limit that will cover
alltests for that single packet. But that would be a lot of tests. Even then, the internal
state of the SUT will influence the possibility of finding some of the flaws.
The input space metric explodes when combinations of anomalies are consid-
ered. There are known security related flaws that require combination of two or
6760 Book.indb 127 12/22/17 10:50 AM
128 Fuzzing Metrics
three anomalies in the message for the attack to succeed. Sometimes those anomalies
are in two different messages that need to be sent in a specific order to the system
under test.
One approach to avoid the infinity of tests is to do the tests systematically. A
carefully selected subset of tests applied across the entire protocol specification
will reach good input space coverage when measured against the specification, but
will not have good input space coverage for each data unit inside the protocol. It is
possible to have both approaches combined, with some random testing and a good
set of smart robustness testing in one fuzzer. All different techniques for fuzzing
can coexist because they have different qualities from the input space coverage per-
spective. There is a place for both random testing (fuzzing) and systematic testing
(robustness testing).
In short, the input space is always infinite. Protocols travel in a pipe that has no
limits on the upper boundary of the packet sizes. Whereas infinity is challenging
in physics, the bits in the attack packets can be created infinitely, and they do not
take physical storage space like they would in, for example, file fuzzing. You might
think input space is finite, like the way Bill thought we never need more than 640k
of memory (well, he did not really say it, but it’s a great urban myth anyway), but
in reality we can see this is not the case.
If a protocol specification says that:
 = 
 = 1..256 * 
How many tests would we need? Fuzzing this (and not forgetting to break the
specification in the process) will require an infinite amount of tests, provided we do
not have any other limitations. Is 2^16+1 a long enough stream to inject? Is 16GB
too much to try? What about if we send even more? Where should we stop? But as
you said, it might not be sensible to go that far, but what if it will crash with one
more additional byte of data? Or two more? Why set upper limits to fuzzing data?
The input space is always infinite. Even if the protocol specification of a one-byte
message is simple like:
 = “A” or “B”
You should definitely try all possible octets besides “A” and “B”, resulting in
256 tests. You should also try all of those in all possible encoding formats, and with
that the input space is already getting quite large. And by adding repetitions of those
inputs, the input space becomes infinite as there is no upper bound for repetition,
unless an upper bound is defined by lower level protocol such as UDP. Even with
UDP, the 64k-byte limit for the packet means close to infinite (approximately 10 to
the power of 160,000) different packet contents.
With some protocols in some specific use scenarios, you can potentially mea-
sure the maximum size of input space. You can potentially map the upper bounds
by, for example, increasing the message size until the server side refuses any more
data and closes the connection (it may still have crashed in the process). Any test
with bigger messages would not add to the test coverage.
6760 Book.indb 128 12/22/17 10:50 AM
4.3 Defect Metrics and Security 129
Another aspect of input space is created by dynamic (stateful) protocols. What
if a flaw can be triggered by a combination of two different packets? Or three? The
message flows will also increase the input space.
A key question with any infinite input space is: Does randomness add to the
value of the tests? When speaking about systematic versus random (nonsystematic),
we need to understand what randomness actually means. The quality of the ran-
domness will have some impact to the resulting input space coverage. Let’s look
at two widely used algorithms. These are not true algorithms, but can be used as
examples assuming someone wants to start building a library of fuzzing or test
generation methodologies:
“2^x, +-1, +-2, ...” is systematic test generation
“2^x + rand()” is not systematic, even if you have control on the used seed
use for the random number generation.
Note that bit-flipping can also be both random or systematic because you do
not know the purpose of each test. There are dozens of articles on random testing
and white-noise testing, the related problems, and where it makes sense and where
it does not. We will explore this later in relation to random fuzzing.
While fuzzers do not have to bother with conformance testing, they still often
have to build their negative fuzz tests based on any available RFCs or other relevant
specifications. A solid fuzzer or robustness tester is based on specifications, while
breaking those very same specifications at every turn. The more specifications a
fuzzer covers, the better input space coverage it will have, and the more flaws it
will find.
There are two branches of negative testing:
1. Fuzzing starts from infinite number of semirandom tests, and tries to learn
intelligence in structures. based on feedback from the test target.
2. Robustness testing starts from finite set of tests from a behavioral model of
the protocol, and tries to add intelligence in data inputs.
After some development, both will result in better tests in the end, merging into
one semi-random approach with a lot of intelligence on interface specifications and
real-life vulnerability knowledge. The optimum approach is probably somewhere in
the middle of the two approaches. For some reason, security people start with the
first approach (perhaps because it is easiest for finding one single problem quickly).
QA people often prefer the second approach because it fits better to the exist-
ing QA processes, whose purpose is to find all flaws. Due to the number of tests
required, the famous Turing problem applies to approach number one more than
to approach number two.
Fuzzers should definitely also do combinations of anomalies in different areas
of the protocol for good input space coverage. This is where random testing can
bring huge value over systematic robustness testing. If you stick to simple anomalies
in single fields in the protocol specifications, why use random fuzzing at all? It is
much more effective to build systematic tests that walk through all elements in the
protocol with a simple set of anomalies.
6760 Book.indb 129 12/22/17 10:50 AM
130 Fuzzing Metrics
4.3.6 Code Coverage Metrics
Code-based reviews, whether manual or automated, have been used for more than
two decades and can be built into compilers and software development environments.
Several terms and metrics can be learned from that practice, and we will next go
through them briefly. Code security metrics that we will examine are
• Code volume (in KLOC = thousand lines of code) shows the aggregate size
of the software;
• Cyclomatic complexity shows the relative complexity of developed code;
• Defect density (or vulnerability density) characterizes the incidence rate of
security defects in developed code;
• Known vulnerability density characterizes the incidence rate of security defects
in developed code, taking into account the seriousness (exploitability) of flaws;
• Tool soundness estimates the degree of error intrinsic to code analysis tools.
One of the most common code volume metrics is lines of code (LOC), or thou-
sands of lines of code (KLOC). Although it appears to be a very concrete metric,
the code volume is still not unambiguous and perfect. However, LOC is still less
subjective than most other code volume metrics such as those based on use cases or
other metrics that are used to estimate the code size during requirements or design.
Code volume can be measured at different layers of abstraction, but it most typically
is measured from the abstraction layer of the handwritten or generated high-level
programming languages. Therefore, the programming style and the programming
practices used will affect the resulting metric for the amount of code. As a better
alternative to counting lines of code, some coding style checking tools, profilers, and
code auditing tools will measure the number of statements in the project. McCabe’s
metric for cyclomatic complexity provides one useful estimate for the complexity of
the code. This and several other metrics are based on the number of branches and
loops in the software flow. The most valuable metric for fuzzing comes from being
able to assess the ratio of what was tested against the total available. Understand-
ing the limitations of chosen metrics is important. We will come back to this topic
later in the chapter.
The next metric we will discuss is defect density. The number of defects that
are found via a code audit can vary significantly based on the tools that are used
and the individuals conducting the assessment. This is because people and tools can
only catch those bugs that they can recognize as defects. A security flaw in the code
is not always easily identified. The simplest of security mistakes can be found by
searching for the use of function calls that are known to be prone to errors, such as
strcpy(), but not all mistakes are that easy to catch. Also, code auditing tools such
as RATS, ITS4, flawfinder, pscan, and splint, and their commercial counterparts,18
18 One of the first widely used and security aware commercial run-time analysis tools for detecting
security flaws from the code was called Purify, launched around 1991 by Pure Software, later acquired
by Rational Software. Today new emerging companies such as Coverity by Synopsys, Klocwork,
AppScan by IBM, and Fortify by HP appear to dominate the market with both offline (static) and
run-time (dynamic) analysis tools.
6760 Book.indb 130 12/22/17 10:50 AM
4.3 Defect Metrics and Security 131
use different criteria for finding mistakes such as unsafe memory handling, input
validation, and other suspicious constructs.
This variance in defect density depends on the interpretation of false positives
and false negatives in the code auditing process. A false positive is an issue that
is labeled as a (security-related) defect, but after a closer study turns out to be an
acceptable piece of code (based on the used programming policy). False positives
are created due to inconsistencies in programming practices. Either the enforced
coding policies have to be taught to the code auditing tool, or the coding practices
could be adapted from the tool. A more challenging problem is related to false nega-
tives, which basically means a mistake in the code was missed by the code auditing
tool. The definitions of false positive and false negative are also subjective, as some
people define them based on the exploitability of the security flaws.19 To some, a
bad coding practice such as using an unsafe string copy function is acceptable if it
is not exploitable. Yet to others, all code that can pose a risk to the system is to be
eliminated regardless of whether it can actually be exploited. The latter is safer, as
it is very common to adapt code from noncritical projects into more critical areas,
propagating potential weakness in the process.
Known vulnerability density as a metric is a modified variant of the defect
density, with weights added to indicate the significance of the found issues. All
defect density metrics are estimates that are based on proprietary metrics of the
code auditing tools. Until the problems with the accuracy are sorted out, the defect
density metrics given by code auditing tools are valuable as guidance at best. The
Software Assurance Metrics and Tool Evaluation (SAMATE)20 project has proposed
using a metric called soundness, which attempts to estimate the impact of false posi-
tives and false negatives to the defect rate. This estimate is based on a calibration,
a comparison of the code auditing tool to a well-performed manual code review.
Metrics based on code coverage have also been used in analyzing the efficiency
of fuzzers. In a master’s thesis published in 2004 by Tero Rontti on this topic for
Codenomicon and University of Oulu, code coverage analysis of robustness tests
were applied to testing various open-source implementations of TLS and BGP.21 The
research indicated that in some areas fuzzing can potentially have better test code
coverage than traditional testing techniques. Interestingly (but not surprisingly),
fuzzing had much better test coverage than the OpenSSL conformance test suite,
for example, in testing the TLS interface. However, fuzzing did not test all the code
that conformance testing would test, nor was the overall coverage anywhere close
to 100%. This is because fuzz tests typically cannot reach user interface or con-
figuration routines inside the tested software, for example. Also, while the focus of
conformance tests is in trying out all potential features, fuzz tests typically focus on
fuzzing around a particular subset of the capabilities of the tested software. Fuzzing
is negative testing; that is, it has better chances of exploring different error handling
routines compared to typical feature tests. But code coverage alone does not indicate
the quality of the fuzzer. It can indicate poor fuzzing, if coverage is suspiciously
19 http://weblogs.mozillazine.org/roc/archives/2006/09/static_analysis_and_scary_head.html.
20 SAMATE: Software Assurance Metrics and Tool Evaluation. http://samate.nist.gov.
21 T.T. Rontti. (2004). Robustness Testing Code Coverage Analysis. Oulu, Finland: University of Oulu,
Department of Electrical and Information Engineering. Master’s Thesis, 52p.
6760 Book.indb 131 12/22/17 10:50 AM
132 Fuzzing Metrics
low especially around the input parsing portions of the code. But even good code
coverage does not indicate that all accessible code blocks were tested with a large
number of mutations. This would require dynamic analysis of the system behavior.
It seems the most important discovery from Tero’s research is that code coverage
is an excellent metric for attack surface; that is, when you perform a combination
of positive feature testing and negative fuzz testing against the system, you will
discover the lines of code that are security critical and accessible through that spe-
cific interface. This does not guarantee that the entire attack surface was touched
by the tests, but it will indicate that at least that piece of code can be accessed by
an attacker. While traditional QA did not touch large parts of code, adding fuzz-
ing to the mix of testing techniques increased the amount of total code touched
during the process; furthermore, it hit the exact portions of code that will have to
withstand remote attacks. This metric for measured attack surface could be useful
to white-box testers who have the challenge of auditing millions of lines of code.
Prioritizing the code auditing practices around the attack surface could help find
those flaws that fuzzing might miss. Any metric of what code can be exploited
through different interfaces can prioritize the focus of those analysis tools, and
therefore improve the code auditing process significantly. This is an indication of
the real and measured attack surface.
Alvarez and Guruswamy22 note that discovering the attack surface prior to fuzz-
ing could be helpful. In fact, that leads to another important observation—when
doing interface testing we will never reach 100% code coverage. Even if we would,
we would not really prove the discovery of all vulnerabilities in the touched code.
It would be more accurate to say that 100% code coverage of the attack surface
does not equal zero vulnerabilities. The distinction is important because the attack
surface is typically only a fraction of the total code. Determining the total func-
tions or basic blocks that lie on an attack surface can be a challenge. Consider this
example: If we find that 5 out of 100 total functions read in command line argu-
ments and 15 out of 100 functions handle network traffic, we simply take the ratio:
• Local attack surface = 5/100 or 5% of total code
• Remote attack surface = 15/100 or 15% of total code
A remote fuzzing tool that hits 15 functions would be said to have hit 100%
of the remote attack surface. But did it hit all combinations of paths with all com-
binations of data?
And did it really test all the subfunctions and libraries where that tainted data
was handled?
4.3.7 process Metrics
Process efficiency needs to be monitored when deploying fuzzing into your devel-
opment processes or to a regularly conducted vulnerability process. The following
metrics might apply to your fuzzing deployment:
22 Personal communications and email discussion on the Dailydave list: http://fist.immunitysec.com/
pipermail/dailydave/2007-March/004220.html.
6760 Book.indb 132 12/22/17 10:50 AM
4.4 Test Automation for Security 133
• Assessment frequency: Measures how often security quality assurance gates
are applied to the software development lifecycle.
• Assessment coverage: Measures which products are tested with fuzzers; that
is, for regression testing you can check that all releases will go through a sys-
tematic fuzzing process before release.
4.4 Test Automation for Security