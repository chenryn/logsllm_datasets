homocombo.
3.4 Step 4: Feature Extraction
The fourth step of DOMAINSCOUTER is extracting features
from registered IDNs (step 1), brand domains (step 2), and
their corresponding images (step 3). This step is intended
to design features that can detect the six types of deceptive
IDNs deﬁned in Section 2. In particular, we use three types
of features: visual similarity, brand, and TLD features.
Visual similarity features. The visual similarity features,
Nos. 1–3 listed in Table 2, are designed to grasp the most
distinguishing characteristics of a deceptive IDN, the IDN’s
appearance. In other words, these three features are used to
measure the extent to which an IDN can deceive users. We
utilize image similarity between registered IDNs and brand
domains as the visual similarity features. To measure simi-
larity between two images, we use the Structural SIMilarity
(SSIM) index [63] since it is reported to achieve the best
performance when detecting one type of the deceptive IDNs
(eng-homo) [36]. For our prototype implementation, we used
pyssim [46], a python module for computing the SSIM index.
The SSIM index ranges between 0.0 (non-identical) and 1.0
(perfectly identical). As explained in Section 3.3, we prepare
images of three different types (RAW, PSR, and WS) to de-
tect various deceptive IDNs; accordingly, we calculate the
SSIM index for pairs of images of the same type. We use
the maximums of the SSIM indexes between RAW, PSR, and
WS images as features No. 1, 2, and 3, respectively. We iden-
tify the brand domain with the highest SSIM indexes as the
targeted brand domain corresponding to the input IDN.
Brand features. The brand features, Nos. 4–12 listed in Ta-
ble 2, are designed to consider characteristics of targeted
brand domains. We hypothesize that more popular domains
are targeted to create deceptive IDNs. Thus, we use the rank
information in the three top lists (Alexa [2], Umbrella [11],
and Majestic [38]) as our brand features. The reason for using
multiple top lists is to measure popularity from several rank-
ing mechanisms in an unbiased way. We refer to the Alexa,
Umbrella, and Majestic ranks of the targeted brand domain
identiﬁed on the basis of the visual similarity features as men-
tioned above in RAW, PSR, and WS images as features Nos.
4–6, 7–9, and 10–12, respectively.
TLD features. The TLD features, Nos. 13–16 listed in Ta-
ble 2, are designed to use domain names’ own characteristics
of both input IDNs and targeted brand domains. We introduce
these features since our analysis reveals that the usage of
TLDs has changed dramatically in recent years, and deceptive
IDNs do not always use the same TLD as the targeted brand
domains. We use the TLD types deﬁned in Section 3.1 (e.g.,
legacy gTLD, new gTLD, new IDN gTLD, legacy ccTLD,
and new IDN ccTLD) as the TLD features for the input IDN
(No. 13) and the targeted brand domain based on RAW (No.
14), PSR (No. 15), and WS (No. 16) images.
3.5 Step 5: Score Calculation
The ﬁfth step of DOMAINSCOUTER is calculating the de-
ceptive IDN score, which is the estimated probability of the
user being deceived by the corresponding input IDN. We
use a supervised machine learning approach to calculate the
score. The input of this step consists of the input IDN with
the features listed in Table 2. We use one-hot encoding for
categorical features (Nos. 13–16). Supervised machine learn-
ing is generally composed of two phases: training and testing.
The training phase generates a machine learning model from
training data that includes extracted features and labels. For
labeling, we hypothesize that some deceptive IDNs have al-
ready been used for phishing or social engineering attacks.
Thus, we rely on multiple blacklists that have phishing or so-
cial engineering categories and carefully label the input IDN
deceptive or non-deceptive. Note that our aim is not labeling
many known deceptive IDNs but labeling reliable deceptive
IDNs for estimating the scores for unlabeled IDNs. In the
testing phase, the model generated in the training phase is
used to calculate the probabilities of input IDNs being de-
ceptive IDNs. We deﬁne these probabilities as the deceptive
IDN scores. The higher the score, the more likely the user is
to be deceived by the IDN. Consequently, this step outputs
detected deceptive IDNs, their targeting brand domains, and
the deceptive IDN scores.
Among many traditional and deep learning algorithms, we
select Random Forest [8] for three reasons. First, Random For-
est has good interpretability, i.e., it makes clear how features
contribute to the result and how they are treated. Second, the
parameters of Random Forest include the number of decision
416          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX Associationtrees to employ and the features considered in each decision
tree, which makes the model easy to tune. Finally, in our
preliminary experiments, Random Forest outperformed other
popular algorithms such as Logistic Regression, Naïve Bayes,
Decision Tree, and Support Vector Machine. In Random For-
est, the probability or deceptive IDN score is calculated by
averaging results of each decision tree. The higher the num-
ber of decision trees predicted to be deceptive, the higher the
deceptive IDN score.
3.6 Limitation
DOMAINSCOUTER has two limitations. First, it does not aim
to detect various kinds of malicious domain names but only
deceptive IDNs that may lead to user misbehavior. Thus, a de-
ceptive IDN is not always used for speciﬁc malicious attacks
(e.g., phising, social engineering, and malware). However,
identifying deceptive IDNs itself provides incentives for var-
ious stakeholders as discussed later in Section 7. There are
many previous systems aiming at detecting malicious domain
names in terms of the lexical characteristics [37, 55, 67], the
relationship between domains and IP addresses [3, 10, 32],
and the behavior of DNS queries [4, 5, 7]. Our system com-
plements these systems. In particular, we can combine the
systems to achieve better detection coverage.
The second limitation is in the coverage of non-English
brands in step 2. In particular, we selected non-English brands
on the basis of the top lists; however, there could be more
non-English brands for each country, region, and language.
We will explore other sources such as registered trademarks
or search engine results for each country in our future work.
4 Evaluation
In this section, we show the results of comparing our system
DOMAINSCOUTER with those proposed in previous works in
terms of system properties and detection performance.
4.1 Comparison of Properties
We compared the properties of DOMAINSCOUTER and those
of two previous systems [36, 48] from four perspectives. Ta-
ble 3 summarizes the results.
Dataset. When comparing datasets used in each study, there
are clear gaps between our system and the other two. In par-
ticular, our system contains 570 studied TLDs, whereas that
of Liu et al. [36] contains only 56. No description regard-
ing TLDs is provided by Sawabe et al. [48]. Furthermore,
our system contains many more IDNs than the two previous
systems: 3 times more than that of Liu et al. [36] and 2.3
times more than that of Sawabe et al. [48]. To the best of
our knowledge, our domain dataset that includes both gTLDs
and ccTLDs is the most comprehensive dataset ever used in
security research.
Targeted Brand. In terms of the targeted brands used in each
study, DOMAINSCOUTER uses both English and non-English
brand domains, and the number of these domains is much
Table 3: Results of Comparing Properties
Dataset
# TLDs (IDNs)
# Domains (IDNs)
# Domains (English)
# Domains (Non-English)
Targeted
Brand
Deceptive Combo
Homo
IDN
Homocombo
Visual Similarities
Brand Features
TLD Features
Method
Our System Liu et al. [36] Sawabe et al. [48]
-
1,928,711
1,000
0
570
4,426,317
2,310
4,774
56
1,472,836
1,000
0
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:32)
(cid:71)(cid:35)
(cid:32)
(cid:35)
(cid:32)
(cid:71)(cid:35)
(cid:35)
(cid:35)
(cid:32)
(cid:35)
(cid:32)
(cid:71)(cid:35)
(cid:35)
(cid:32): Fully Covered,(cid:71)(cid:35): Partially Covered,(cid:35): Not Covered
bigger than that of the Liu et al. [36] and Sawabe et al. [48]
systems.
Deceptive IDN. DOMAINSCOUTER focuses on various
deceptive IDNs (eng-combo, eng-homo, eng-homocombo,
noneng-combo, noneng-homo, and noneng-homocombo),
whereas Liu et al. [36] studied only eng-homo and a part
of eng-combo IDNs, and Sawabe et al. [48] detected only
eng-homo IDNs.
Method. Liu et al. [36] used visual similarities (the SSIM
index) between IDNs and brand domains to detect eng-homo
IDNs targeting the Alexa Top 1,000 brands. Sawabe et al. [48]
calculated visual similarities between non-ASCII and ASCII
characters using optical character recognition (OCR) to de-
tect eng-homo IDNs. However, both methods need to tune the
thresholds of either the SSIM index or OCR manually, which
tends to cause false positives and false negatives, and do not
consider how popular the targeted brand domain is. In addi-
tion, Liu et al. did not focus on eng-homo IDNs between dif-
ferent TLDs (e.g., example[.]com and êxämpl¯e[.]test).
To solve the above problems, as stated in Section 3.4, DO-
MAINSCOUTER utilizes not only multiple visual similarity
features but also targeted brand ranking and TLD features
and applies a machine learning approach to eliminate tuning
thresholds for visual similarity features.
4.2 Comparison of Detection Performance
We compared the deceptive IDN detection performance of
DOMAINSCOUTER with that of the previously proposed sys-
tems [36, 48]. First, we describe the experimental setups in
the other two systems and our system. Then, we illustrate the
comparison results using real registered IDNs.
Setups of the Previous Systems. We replicated the previ-
ously proposed systems on the basis of their descriptions pro-
vided in the corresponding papers [36, 48] since the systems
are not open-source. For the Liu et al. [36] system, we needed
to set a threshold for the SSIM index to detect eng-homo
IDNs. The original paper set the threshold to 0.95. However,
in our re-implemented system, the 0.95 threshold caused non-
negligible false positives, which may be due to the differences
in the font and image settings between the original system
and our re-implemented one. We manually veriﬁed the SSIM
index results to determine the threshold of 0.99, which caused
only few false positives. For the Sawabe et al. [48] system,
we used the mappings between non-ASCII and correspond-
USENIX Association        22nd International Symposium on Research in Attacks, Intrusions and Defenses 417ing similar ASCII characters kindly provided by Sawabe et
al. themselves [48] to re-implement the detection method of
eng-homo IDNs. To match the brand domains employed in
the previous works, we used all English brand domains shown
in Section 3.2 for fair evaluation, even though the original
papers used only the top 1,000 brand domains.
Setup of DOMAINSCOUTER. Section 3 describes the imple-
mentation of our system, DOMAINSCOUTER. As stated in
Section 3.5, we need to set up a labeled training dataset. In
our evaluation, we used 10,000 labeled IDNs consisted of
242 deceptive (positive) and 9,758 non-deceptive (negative)
IDNs for building our machine learning model. The positive
IDNs were labeled by referring to the latest three blacklists
(hpHosts [20], Google Safe Browsing [16], and Symantec
DeepSight [54]) as of November 2018 and manually veriﬁed
by the authors. The 242 positive IDNs are composed of only
eng-homo deceptive IDNs since even the latest blacklists do
not cover other types of deceptive IDNs. However, we design
our proposed features to grasp the nature of various decep-
tive IDNs, thus DOMAINSCOUTER can identify other types
of deceptive IDNs other than eng-homo. The negative IDNs
were randomly sampled from the IDNs shown in Table 1 and
manually veriﬁed by the authors.
We performed 10-fold cross-validation (CV) on the train-
ing dataset and achieved a true positive rate of 0.981, a true
negative rate of 0.998, a false positive rate of 0.002, a false
negative rate of 0.019, and an F-measure of 0.972 on average.
Regarding the two parameters in Random Forest, we set the
number of decision trees as 100 and the number of sampled
features in each individual decision tree as 6 on the basis of
the best results in our preliminary experiments. Note that, as
explained in Section 3.6, DOMAINSCOUTER does not aim
to detect malicious IDNs but only deceptive IDNs. Thus, the
positive does not mean malicious but deceptive. Similarly,
negative does not mean legitimate/benign but non-deceptive.
This has been a typical evaluation setting regarding detecting
deceptive IDNs (e.g., eng-homo). Similar to ours, previous
studies [36, 48] did not provide true positive/negative rates
in terms of detecting malicious IDNs since they focused on
detecting eng-homo IDNs.
The last column of Table 2 shows relative feature impor-
tance of all features. The higher the importance score, the
more the feature contributed to the correct detection. The
results demonstrated that the three visual similarity features
(Nos. 1–3) are more effective than the other features. In par-
ticular, the visual similarity based on word segmented images
(feature No.3) appeared to contribute to the correct detection
the most. The remaining proposed features (Nos. 4–16) were
conﬁrmed to contribute to detecting deceptive IDNs as well.
Detection Performance. Here, we compare the detection per-
formance of the three systems. The input IDNs for each sys-
tem were the same 4,426,317 IDNs described in Table 1.
Unfortunately, there is no ground truth to label all IDNs.
Thus, we used the re-implemented previous systems and the
Figure 2: Venn Diagram of Detected Deceptive IDNs
trained DOMAINSCOTER, which proved to be accurate in the
CV evaluation, to explore unknown deceptive IDNs in the
dataset. Of course, there could be unavoidable false negatives
or missed deceptive IDNs. We manually excluded false posi-
tives or falsely detected non-deceptive IDNs from the results
of the three systems.
We did not exclude the 242 positive IDNs used for the
training dataset of DOMAINSCOUTER from the input IDN
in this evaluation for two reasons. One is that the goal of
our paper is not just to compare the detection performance
but also to conduct a comprehensive measurement study of
deceptive IDNs (shown later in Section 5). The other is all the
242 positive IDNs were conﬁrmed to be easily detected by the
three systems since they were easily identiﬁable eng-homo
deceptive IDNs.
Figure 2 is a Venn diagram showing intersections of decep-
tive IDNs detected by the three systems and the 242 positive
IDNs labeled using blacklists. The Liu et al. system detected
1,514 deceptive IDNs (=621+651+242) and the Sawabe et al.
system detected 931 (=38+651+242). Our analysis revealed
that the difference between the coverage achieved by the Liu
et al. and Sawabe et al. systems originated from the difference
in handling the input IDN by each system: the Liu et al. sys-
tem handled an IDN string as one image, whereas the Sawabe
et al. system handled each non-ASCII character contained in
an IDN.
Surprisingly, DOMAINSCOUTER fully covered the 1,552
(=621+38+651+242) deceptive IDNs detected by the two pre-
vious systems. Moreover, DOMAINSCOUTER detected 6,732
further deceptive IDNs that were not detected by the two sys-
tems. The extra detected deceptive IDNs mainly consisted of
our new targets such as eng-combo, eng-homocombo, noneng-
combo, and noneng-homocombo. The results of the 8,284
IDNs detected in total are explained in the next section.
5 Measurement Study
So far, we have evaluated the detection performance of DO-
MAINSCOUTER compared with those of the two previously
proposed systems. This section focuses on the 8,284 decep-
tive IDNs detected by DOMAINSCOUTER. To the best of our
knowledge, this is the most comprehensive study in terms of
the numbers of both the input IDNs (more than 4.4 million
registered IDNs under 570 TLDs as shown in Table 1) and
the detected deceptive IDNs. In the following sections, we
describe our measurement results in terms of the characteris-
tics of deceptive IDNs, the impacts caused by deceptive IDNs,
and the brand protection of deceptive IDNs.
386516216,732Our SystemLiu et al., 2018Sawabeet al., 2018242Blacklists418          22nd International Symposium on Research in Attacks, Intrusions and DefensesUSENIX AssociationTable 4: Breakdown of Detected Deceptive IDNs
Table 5: Top 10 Targeted English Brands
Type
eng-combo
eng-homo
eng-homocombo
noneng-combo
noneng-homocombo
Total
# IDNs
368
1,547
3,697
144
2,528
8,284
5.1 Characteristics of Deceptive IDNs
Deceptive Types. We begin by investigating the types of
deceptive IDNs found in the registered IDNs as of May
2018. The identiﬁed deceptive IDNs were grouped into the
deﬁned types on the basis of the information we obtained
when extracting our proposed features, i.e., identiﬁed tar-
geted brands (eng/noneng) and which SSIM index of images
(RAW/WS/PSR) is the highest. Table 4 provides a breakdown
of the detected deceptive IDNs. Our system found 368 eng-
combo, 1,547 eng-homo, 3,697 eng-homocombo, 144 noneng-
combo, and 2,528 noneng-homocombo IDNs. As explained
in Section 2, some eng-homo IDNs were already analyzed in
the previous studies [36, 48]. We successfully revealed that
there were many deceptive IDNs other than eng-homo IDNs,
which were found in the research literature for the ﬁrst time.
We deﬁned a noneng-homo IDNs; however, our system did
not detect any noneng-homo IDNs that targeted our selected
non-English brand domains from the input IDNs.
Targeted Brands. Next, we focused on the targeted brands
among the detected deceptive IDNs. Table 5 lists the 10 most
targeted English brands, along with their Alexa ranks, among
the detected deceptive IDNs. The results highlight three major
outcomes. First, more popular brand domains (i.e., those with
higher Alexa ranks) are targeted for creating deceptive IDNs
as hypothesized in Section 3.4. Second, all websites of the
top 10 targeted brands offer user accounts and user login
functions. A possible explanation for this is attackers targeted