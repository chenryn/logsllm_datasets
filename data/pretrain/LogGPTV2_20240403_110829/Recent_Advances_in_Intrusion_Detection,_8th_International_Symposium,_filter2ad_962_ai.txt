failed connections and quickly depletes the available tokens. Until more tokens
become available, legitimate traﬃc is stopped altogether, as seen in the third
and forth row of Table 3.
In Figure 4(b) there is a pronounced initial jump in the false negative rates as
Blaster hits on day 6, and in a few days the false negatives decrease signiﬁcantly.
The bulk of false negatives can be attributed to the fact that Chen’s scheme uses
only TCP RST as an indication of a failed connection. Since many ﬁrewalls simply
drop packets instead of responding with TCP RSTs, using TCP RSTs exclusively
underestimates the number of failed connections. Figure 5(b) shows the error
rates including TCP TIMEOUTs. As shown, false negative rates of FC are reduced
Table 3. False Positives and Cause for Day 6 λ = 1.0 and Ω = 300
IP
# Good Flows DroppedTotal # Good FlowsCause
Basic
Temporal
188.139.199.15 3289656979
188.139.202.79 2599032945
188.139.173.1235386 13457
188.139.173.1044852 6175
57336
33961
15108
6254
eDonkey Client
BearShare Client
HTTP Client
Good Flows(Inf. Client)
Empirical Analysis of Rate Limiting Mechanisms
31
ROC Curves for FC w/ varying lambda and omega
ROC Curves for Chen et al. RL w/ and w/o Timeout Enhancement
 35
 30
λ=2.0
Basic lambda=0.1-2.0
Temporal lambda=0.5, omega=10-1000
Temporal lambda=1.0, omega=10-1000
Temporal lambda=1.5, omega=10-1000
 25
 20
 15
 10
 5
 0
Ω=1000
Ω=10
λ=0.1
 5
 10
 15
 25
 30
 20
False Positive (%)
 35
 40
 45
)
%
(
e
v
i
t
a
g
e
N
e
s
a
F
l
 35
 30
 25
 20
 15
 10
 5
 0
Basic lambda=0.1-2.0
Temporal lambda=1.0,omega=10-1000
adding Timeout Basic same values
adding Timeout Temporal same values
 0
 10
 20
 30
 40
 50
 60
 70
 80
 90
False Positive (%)
)
%
(
e
v
i
t
a
g
e
N
e
s
a
F
l
(a) ROC w/ diﬀ. values of λ and Ω (b) ROC and timeout enhancement
Fig. 5. ROC for diﬀerent λ and Ω values for Basic and Temporal RL algorithms
signiﬁcantly when Timeouts are considered. The drop in false negative rate on
day 10 in Figure 4(b) is correlated with the onset of the Welchia outbreak.
Blaster scanning generates a substantial number of TCP TIMEOUTs while Welchia
tends to generate TCP RSTs (Welchia scans via ICMP ECHO). As more and more
Blaster hosts are patched and Welchia makes up a greater portion of the worm
traﬃc, the false negatives are reduced.
Figure 5 plots the false positive rates against the false negative rates with
varying values for λ and Ω. The data points in this graph are averaged daily
statistics over the entire trace period. In temporal FC, when failures reach Ω/2,
the rate limiting algorithm proceeds to rate limit hosts in a much more ag-
gressive fashion than the basic scheme. This strategy results in a signiﬁcant
amount of non-worm traﬃc from “infected” hosts being dropped. In the third
row of Table 3, temporal FC dropped approximately 2.5 times more benign traf-
ﬁc compared to basic FC. Since a typical worm outbreak will quickly reach Ω/2
failures, temporal FC is more restrictive and thus renders higher false positives.
Comparing FC results to host-based Williamson’s, we can see that FC renders
signiﬁcantly lower false positives during infection but yields slightly higher false
negatives. In fact, with FC’s drop-only approach and Williamson’s tendency to
saturate the delay queue, both closely approximate a detect-and-block approach,
which is less interesting from the standpoint of rate limiting.
7 Credit-Based Rate Limiting (CB)
Another rate limiting scheme based on failed connection statistics is the credit-
based scheme by Schechter et. al. [12]. We refer to it as CB (for Credit Based).
CB diﬀers from Chen’s in two signiﬁcant ways. First, it performs rate limiting
exclusively on ﬁrst contact connections—outgoing connections for destination
IPs that have not been visited previously. The underlying rationale is that scan-
ning worms produce a large volume of failed connections, but more speciﬁcally
they produce failed ﬁrst-contact connections, therefore anomalous ﬁrst-contact
statistics are indicative of scanning behavior. The notion of ﬁrst contact is fun-
damental to CB and as we show later is instrumental to its success. Second, CB
32
C. Wong et al.
False Positive and False Negative per day for CB w/ PCH = 64
ROC Curve for CB w/ 95% Conf. Intervals for Infection Period
 10
 8
 6
 4
 2
)
%
(
t
n
e
c
r
e
P
CB False Positive
CB False Negative
)
%
(
e
v
i
t
a
g
e
N
e
s
a
F
l
 0
 0
 5
 10
Days
 15
 20
 5
 4.5
 4
 3.5
 3
 2.5
 2
 1.5
 0
CB
pch = 128
pch = 8
 2
 4
 6
 8
 10
 12
False Positive (%)
(a) Error rates (PCH = 64)
(b) ROC w/ varying PCH sizes
Fig. 6. Results of Error Rates for CB RL
considers both failed and successful connection statistics. Simply described, CB
allocates a certain number of connection credits per host; each failed ﬁrst-contact
connection depletes one credit while a successful one adds a credit. A host is only
allowed to make ﬁrst-contact connections if its credit balance is positive.
It is straightforward to see that CB limits the ﬁrst-contact failure rate at
each host, but does not restrict the number of successful connections if the
credit balance remains positive. Further, non-ﬁrst-contact connections (typically
legitimate traﬃc) are permitted through irrespective of the credit balance. Con-
sequently, a scanning worm producing a large number of failed ﬁrst contacts
will quickly exhaust its credit balance and be contained. Legitimate applications
typically contact previously seen addresses, thereby are largely unaﬀected by the
rate limiting mechanism.
In order to determine whether an outgoing TCP request is a ﬁrst contact, CB
maintains a PCH (Previously Contacted Host) list for each host. Additionally,
a failure-credit balance is maintained for each host. We implemented the CB
algorithm and experimented with the per-host trace data. Schechter suggested a
64-address PCH and a 10-credit initial balance. We conducted experiments with
PCH ranging from 8 to 128 entries with Least Recently Used (LRU) replacement.
Our experience suggests that the level of the initial credit balance has minimal
impact on the performance of the scheme, as that only approximates the number
of failures that can occur within a time period; in reality a host can accrue more
credits by initiating successful ﬁrst contacts. For the experiments, we use an
initial credit balance of 10 per host.
Figure 6(a) shows CB’s daily false positive and false negative rates with a
64-address PCH. The data points in this graph are averages across all hosts. As
shown, the average false positive and false negative rates are between 2% and
6%. The false positive results signiﬁcantly outperform both FC and Williamson’s.
CB’s false negative results are comparable to those of Williamson’s. These results
speak strongly of CB’s insight of rate limiting ﬁrst contacts rather than distinct
IPs or straightforward failed connections. Since worm scanning consists primarily
of ﬁrst-contact connections, CB’s strategy gives rise to a more precise means of
rate limiting.
Empirical Analysis of Rate Limiting Mechanisms
33
Table 4. Per Host False Positives and Cause for Day 6 for PCH = 64
# Good Flows Dropped Total # of Good Flows Cause
IP
188.139.199.15 22907
188.139.202.79 13269
188.139.173.123 0
57336
33961
15108
eDonkey Client
BearShare Client
HTTP Client
Table 4 shows the false positive data for the top two false-positive-generating
hosts. Both clients that incurred high false positives are P2P clients. The data
show that the worst case false positive rate is rather high—nearly 40% for the
host in row one. For comparison reasons, here we also include the HTTP client
discussed previously (row 3 from Table 3). As shown, CB is able to accommodate
this bursty web client while FC dropped a signiﬁcant portion of the client’s traﬃc.
Figure 6(b) plots the average false positive rates against the corresponding
false negative rates for PCH of 8, 16, 32, 64, and 128. The data points in this
graph are obtained by averaging per-host statistics over the entire 24-day trace
period (sans the pre-infection days). As shown, CB’s error rates are not partic-
ularly sensitive to the length of the PCH’s. A 3% increase in the false positive
value is observed when PCH is reduced from 128 entries to 8. As the PCH size
increased so did the false negative rate, which is a peculiar phenomenon. We
are unable to ﬁnd a satisfactory explanation for this. We conjecture that a pos-
sible error in the Blaster mutex code allowed multiple instances of Blaster to
execute on the same machine, thereby generating repeated scanning to the same
addresses.
Note that CB is essentially a host-based scheme since states are kept for each
host. Aggregating and correlating connection statistics across the network can
reduce the amount of state kept. For example, if host A makes a successful ﬁrst-
contact connection to an external address, further connections for that address
could be permitted through regardless of the identity of the originating host.
This optimizes for the scenario that legitimate applications (e.g., web browsing)
on diﬀerent hosts may visit identical external addresses (e.g., cnn.com). A more
detailed investigation of aggregate CB can be found later in Section 9.
8 DNS-Based Rate Limiting
In this section we analyze a rate limiting scheme based on DNS statistics. The un-
derlying principle is that worm programs induce visibly diﬀerent DNS statistics
from those of legitimate applications [24, 22, 4]. For instance, the non-existence
of DNS lookups is a telltale sign for scanning activity. This observation was ﬁrst
made by Ganger et al. [4]. The scheme we analyze here is a modiﬁcation of
Ganger’s NIC-based DNS detection scheme.
The high-level strategy of the DNS rate limiting scheme (hereafter refer to
as DNS RL) is simple: for every outgoing TCP SYN, the rate limiting scheme
permits it through if there exists a prior DNS translation for the destination IP,
otherwise the SYN packet is rate limited. The algorithm uses a cascading bucket
34
C. Wong et al.
n Buckets
Dropped
Packets
Packets
 q
Distinct
  IPs
Time
 t
Fig. 7. Cascading Bucket RL Scheme
scheme to contain untranslated IP connections. A graphical illustration of the
algorithm is shown in Figure 7. In this scheme, there exists a set of n buckets,
each capable of holding q distinct IPs. The buckets are placed contiguously along
the time axis and each spans a time interval t.
The algorithm works as follows: When a TCP SYN is sent to an address that