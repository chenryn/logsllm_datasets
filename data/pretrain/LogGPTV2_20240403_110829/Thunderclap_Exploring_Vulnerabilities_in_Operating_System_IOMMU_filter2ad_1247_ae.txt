of IOMMU-exposed memory.
We ran a number of benchmarks to study physical address
usage across different device classes. The goal was to better
understand whether work improving network stacks will
generalise to other devices.
We tested Windows 10 Enterprise 1703 and Ubuntu 16.04
(kernel 4.14) with the IOMMU disabled, in each case recording
the ‘natural’ physical addresses devices use for DMA. We did this
by interposing a PCIe analyzer between the device’s PCIe card
and a slot on a Supermicro C9X299-RPGF motherboard, with
an Intel i9-7940X 14-core CPU and 16GiB of RAM. 10 Gigabit
Ethernet (Intel X520-DA), AHCI host controller for SATA storage
(ASM1061) and GPU (AMD RX460 2GiB) cards were tested.
Due to limitations of our PETracer ML analyzer, devices were
restricted to PCIe Gen1 x4 (10Gbps total bandwidth), and trace
recordings limited by the analyzer’s buffer (2GiB of packets after
ﬁltering). These constraints reduce the speed of operation and the
length of recording time, but do not affect the general distribution
of measurements, which are not performance-sensitive.
We ran workloads designed to test behavior of full applications,
rather than microbenchmarks, mostly based on the Phoronix Test
Suite [35]. Due to OSes use of zero-copy techniques, microbench-
marks that replicate behavior of applications in a simpliﬁed way
– with a smaller memory utilization – may not be representative.
Figure 7 shows address heatmaps for network, storage and
graphics workloads that are representative of our dataset. They
plot addresses on a 12th-order Hilbert curve (after [45]) where
any contiguous address range appears as a block. For readability,
each pixel aggregates 256KiB of address space.
Figures 7a, 7b show the memory reach of networking is fairly
small, since packet buffers are often reused. This was repeated
across other benchmarks testing both small packet roundtrips
and bulk transfers which fully utilized the NIC.
Figure 7c is typical of storage behavior, in which much larger
blocks are transferred – even, as in this case, when accessing
small ﬁles. GPU workloads (ﬁgs. 7d, 7e) show a much broader
reach, but in small blocks surrounded by memory untouched
by the GPU. Figure 7e depicts a workload that will not ﬁt in
GPU memory, leading to it spilling to system memory across
PCIe and yet broader memory footprint.
From these we can see that storage and graphics have much
richer, more complex patterns than networking. A difference can
be seen between purely communication devices, and those where
a dataset is truly shared between CPU and device, as with the
GPU. In particular, given small IOTLB sizes (64 in [48]), setting
up thousands or millions of mappings for pages for a GPU is
likely to cause heavy IOTLB pressure. Superpages, 2MiB or 1GiB
of contiguous physical address space per entry, may help to reduce
the IOTLB footprint. However, memory utilization in compute
workloads (such as 3D models) may be difﬁcult to reorganize to
suit the IOMMU, or that may have an unacceptable performance
cost for the application. Other work, for GPUs and other accelera-
tors, indicates the performance problem is so bad that others [23],
[26] have proposed redesigning or removing the IOMMU com-
pletely. Thus, the IOMMU performance problem is still present.
C. Mitigations and feasibility
To completely protect against malicious peripherals, changes
in system design must be made to reﬂect their untrustworthy
nature and removal from the TCB. Speciﬁcally, we recommend
(1) exposing to peripherals only the minimum amount of
data required for them to function correctly, with the most
restrictive access permissions possible; (2) eliminating temporal
vulnerabilities by ensuring IOMMU mappings are completely
destroyed before IOMMU-mapped memory is processed, or
reused by the host; (3) enforcing per-device I/O virtual address
spaces to limit the ability of one malicious device to compromise
the function of non-malicious devices.
There is a trade-off space involving software techniques for
using IOMMU hardware: more safe use disrupts current software
practices (e.g., colocating I/O data and kernel metadata or passing
kernel pointers to devices) and performance (requiring additional
memory copying/zeroing, greater memory fragmentation, or
synchronous IOTLB invalidation). Adding expensive operations
to performance-critical code paths can signiﬁcantly decrease
I/O throughput, which represents a barrier to implementing
better protections. Concern about code changes should also not
be underemphasized: seemingly Apple deemed restructuring
their network stack to avoid leaking kernel pointers too invasive,
instead encrypting them – contrasting with the preferable Linux
design choice of placing them in a separate unmapped page.
These performance and implementation costs make complete
protection from malicious peripheral devices challenging to
achieve in practice. There is currently no general-purpose
solution that provides protection from malicious peripheral
11
0
16GiB
14GiB
4GiB
8GiB
12GiB
6GiB
10GiB
b Windows 10G ethernet,
PostgreSQL pgbench, 100
remote clients
a Linux 10G ethernet,
PostgreSQL pgbench, 100
remote clients
Fig. 7: Physical address heat maps for different I/O workloads. Storage and graphics workloads are much richer than networking which
often reuses smaller regions of memory buffers. Colors: white=unused, blue=lightly used, orange/red=heavily used. Each pixel aggregates
256KiB of address space, contiguous addresses form a block. (a) annotates the address layout of the 16GiB address space depicted.
d Windows
10 GPU,
Final Fantasy XV DirectX
1080p, tutorial mode
e Windows
10 GPU,
Superposition DirectX 4K
benchmark
c Linux AHCI SATA,
kernel compile, 28 threads
devices without signiﬁcant performance degradation, and many
partial mitigations are not implemented in commodity operating
systems, perhaps because of their impact on existing codebases.
Below we discuss some potential mitigations, brieﬂy outlining
their effectiveness and implementation costs.
Device-speciﬁc I/O virtual address spaces can be implemented
with small changes to an IOMMU driver, mitigating the
ability of one malicious device to compromise the function of
non-malicious devices (although not in themselves preventing
PCIe ID forgery on DMA writes), and do not signiﬁcantly affect
system performance.
Allocator hygiene is a basic improvement: I/O data structures
and sensitive kernel structures should not be allocated from the
same pools. This ﬁxes obvious spatial vulnerabilities, but it does
not prevent I/O data structures from containing sensitive ﬁelds that
a malicious device could exploit and does not segregate I/O data
belonging to different devices or DMA transactions. This change
would cause a relatively minor impact on existing codebases and
performance, requiring the creation of new allocator pools and
modiﬁcation of I/O subsystems to use them. Spatial segregation
extends allocator hygiene so that devices have access only to the
minimum amount of data required for them to function correctly.
I/O data is isolated from all other data (including kernel data and
control ﬁelds in existing I/O data structures) and from I/O data
belonging to different devices or DMA transactions. I/O control
structures such as descriptor rings necessary for the device to
function are similarly isolated. This change would eliminate
spatial vulnerabilities but come at a signiﬁcant implementation
and performance cost. I/O data structures (e.g., network buffers)
would need to be redesigned to separate I/O data from other
ﬁelds, and the corresponding subsystem and device drivers
would require non-trivial code changes to use the new structures
and to isolate their data correctly. Isolating data in distinct pages
would reduce performance by increasing memory usage, the
number of IOMMU mappings, and hence IOTLB pressure.
Synchronous IOTLB invalidation would protect against tempo-
ral vulnerabilities by ensuring IOTLB invalidations are completed
before the IOMMU driver reports them as such. Adopting
synchronous IOTLB invalidation requires only trivial changes to
the IOMMU driver, but can reduce throughput by as much as 80%
for high-performance I/O workloads [42]. Alternatively, memory
for I/O data could simply not be reused or reused only once a
corresponding asynchronous IOTLB invalidation has completed,
which would require changes to I/O memory allocators and reduce
the efﬁciency of memory usage but perhaps have a less signiﬁcant
effect on I/O throughput. However, this change for reuse would
not necessarily ensure that IOMMU mappings are completely de-
stroyed before their underlying memory is processed by the host.
Buffer pre-allocation (as in [42], [43]) involves allocating mem-
ory for device I/O from a special pool that is perpetually exposed
via the IOMMU. Some pre-allocation strategies involve copying
data to and from the pool on IOMMU map and unmap calls, and
some use memory from the pool for device I/O directly. Copying
techniques mitigate spatial and temporal vulnerabilities when
applied correctly, because they can provide protection at arbitrary
granularity and copy speciﬁc ﬁelds from existing data structures.
They require modiﬁcations only to existing I/O memory allocators,
but they introduce negative cache effects and a signiﬁcant
performance overhead. Conversely, schemes that use memory
from pools directly require modiﬁcation to I/O allocators, I/O data
structures, I/O subsystems (since the subsystems must determine
before allocation time with which device to associate I/O data).
They also introduce a new type of vulnerability because devices
could modify pool memory that is used to store I/O data after it
has passed system security checks (e.g., modifying the source IP
address of a network packet after it has passed a ﬁrewall ﬁlter).
Byte-granularity or non-paged IOMMUs could prevent spatial
vulnerabilities but would need a new hardware paradigm. Such
range-based IOMMUs would require new drivers and have
different properties related to translation lookup and IOTLB
caching and invalidation.
Memory encryption would allow devices to have arbitrarily-
sized memory regions only they can interpret. This would
need hardware changes and transforms the problem into one
of key distribution. Different components (NIC, PCIe switch,
driver, network stack) have complex relationships, and safely
distributing and revoking keys is a hard problem. AMD’s
memory encryption [33] does not handle this problem, allowing
DMA only when a single system-wide key is used.
X. RELATED WORK
We divide prior work into the categories of peripheral
memory access attacks that do not work against systems with
basic IOMMU protections, carried out by previous attack
platforms and by compromising ﬁrmware on existing devices;
attacks against IOMMU-enabled systems that mostly exploit
vulnerabilities in IOMMU conﬁguration; and other uses of the
IOMMU. In some cases we build on this work; in other cases
prior work is orthogonal.
12
DMA attacks may be divided into those that involve attaching
a hardware/software platform to a victim system, and those that
compromise ﬁrmware of existing devices. The goals may be
similar, but the route is quite different.
DMA attacks and attack platforms. DMA attacks were of
concern even on 1960s machines [14]. More recently, DMA
attacks have been performed against modern systems, using
vectors such as Firewire, PCI and PCIe [6], [7], [12], [13], [16],
[31], [58]. A number of these have spawned generic DMA attack
platforms such as SLOTSCREAMER [19], PCILeech [20], [21]
and Inception [39]. These platforms can attack many operating
systems over various hardware interfaces. They steal sensitive data
like encryption keys, violate kernel security policies, and even
take complete control of a target machine. However, all of these
attack platforms depend on unrestricted memory access to scan for
sensitive structures or modify a speciﬁc location in memory. They
do not work against systems with basic IOMMU protections. Ad-
ditional reverse engineering efforts focused on Thunderbolt [56]
and demonstrated DMA attacks via Thunderbolt 2 [20] and
Thunderbolt 3 [21], but found them blocked by the IOMMU.
Compromised device ﬁrmware. Other work [59], [60],
[62], [63] replaced the ﬁrmware of existing devices: allowing
basic DMA attacks, but no more potent than those above.
Vulnerabilities of NICs [17], [18] and Wi-Fi chips [5] allowed
arbitrary code injection via crafted packets. DMA attacks were
thus possible remotely. Much of this work cites IOMMU use
as an effective mitigation.
Subverting the IOMMU. Most prior work on bypassing
the IOMMU to carry out DMA attacks focused on exploiting
architectural or boot-time conﬁguration weaknesses, rather than
OS-controlled IOMMU protections.
Lone Sang [37] hypothesized a number of IOMMU attack
vectors without demonstrating them, and hence did not study OS
behavior. They suggest modifying IOMMU page table structures,
ACPI tables, and conﬁguration registers. However, if an IOMMU
is conﬁgured to correctly protect memory, peripheral devices are
unable to modify these structures. The authors also suggested
ATS support might allow bypass of the IOMMU. The only
attack demonstrated in this paper is PCIe ID spooﬁng. Here, a
malicious device spoofs the bus-device-function (BDF) ID of a
legitimate NIC to inject malicious packets over DMA and poison
the ARP cache of a victim machine. This is a PCIe weakness
rather than an IOMMU one. Additionally, any NIC, even one
without DMA access, can poison the ARP cache without needing
to use this PCIe attack. Lone Sang later built a PCIe fuzzer that
found the same vulnerability [38]. (We accidentally veriﬁed this
weakness due to a bug when developing our platform, in our
case spooﬁng interrupts.) They also implemented a keylogger
that can read keyboard input and send keystrokes through
peer-to-peer PCIe legacy IO requests. These are different from
DMA and the IOMMU does not control this channel.
Other attacks have bypassed the IOMMU by racing the
IOMMU setup at system boot. Many devices enable DMA at
boot time, before the OS is launched and IOMMU enabled. Wo-
jtczuk [68] modiﬁed ACPI tables during boot so the OS believed
no IOMMU was present. ThunderGate [59] contains a ﬁrmware
image that deploys a malicious PCI Option ROM containing code
the system will execute – a result duplicated by others [50]. A
weaponization of this technique is the Thunderstrike bootkit [27]
and a similar attack was used by the CIA [24], and to attack
Windows VBS [22]. Morgan [46], [47] enabled DMA by rewriting
the IOMMU page tables while they were being created, before
13
the IOMMU was fully enabled. Apple and recently Microsoft
and partners have blocked such boot-time vulnerabilities.
Subsequent to our disclosure to vendors and to others in the
community, some further IOMMU attacks have been published.
Beniamini [10], [11] compromised the ﬁrmware of a Broadcom
PCIe Wi-Fi chip using an over-the-air exploit. They used this
device for DMA attacking the main OS, ﬁnding the ARM
IOMMU was not used on his Android platforms. On iOS a
custom IOMMU is used: they managed to exploit OS ring-buffer
handling code to modify IOMMU mappings. Kupfer [34]’s
masters thesis also reproduced some attacks similar to ours.
Other uses of the IOMMU. Additional work has been pub-
lished on the security and performance considerations of IOMMU
use by hypervisors to keep guest operating systems isolated,
while giving them direct access to peripheral devices [52], [66],
[67]. Hypervisor-related IOMMU work is both complementary
and orthogonal to our work. Other work addresses IOMMU
security but has a substantially different threat model [70].
We disclosed these vulnerabilities to OS vendors starting in
XI. DISCLOSURE
2016 and have collaborated on mitigations over two years.
To mitigate our control-ﬂow attacks, macOS 10.12.4
introduced a new code-pointer blinding feature, used when mbuf
pointers are exposed via the IOMMU. This technique limits
the effectiveness of attackers in injecting kernel pointers, but
leaves open a number of data ﬁelds, including data pointers,
that could leave the system exposed to further vulnerabilities.
After ongoing dialogue, Microsoft announced Kernel DMA
Protection to enable IOMMU support in devices shipped with
Windows 10 1803 (but not earlier ﬁrmware). They conﬁrmed the
vulnerabilities in this paper remain a concern, in particular, spatial
vulnerabilities caused by page-sharing given that I/O memory is
allocated from a general pool; they stated they will investigate
this for subsequent releases. Critically, documentation for device-
driver authors does not yet explain how to program robustly in
the presence of a DMA-capable attacker. However, enabling the
IOMMU will bring Windows into line with other platforms.
Linux’s kernel security team considers our attacks within their
threat model. They stated that, because Linux is used in many
different environments, the problems are difﬁcult to solve in the
general case. For now, device authentication schemes remain the
primary defense. Citing our disclosure, Intel’s work in kernel 4.21
enables the IOMMU for Thunderbolt ports and disables ATS.
The FreeBSD Project indicated that malicious peripherals
are not currently within their threat model for security response,
although they were concerned about these attacks, and requested
a copy of the paper for further review.
In conversation with one vendor of widely used notebook
computers that do not currently include Thunderbolt 3, it was
clear that IOMMU-bypass attacks via Thunderbolt were both
within their threat model and would compromise intended
security protections of their system. They stated that they would
want to understand how to address these attacks before adding
Thunderbolt to new product lines.
We are continuing our outreach to further vendors to establish
whether these attacks are within their accepted threat models, and