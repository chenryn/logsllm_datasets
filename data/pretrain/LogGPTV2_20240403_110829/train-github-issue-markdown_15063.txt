From the recording of the TensorFlow Developer Summit that took place about a month ago, I learned that the latest distributed TensorFlow 1.0 can achieve a 57x speedup for the Inception v3 model on a server with 8 nodes and 8 GPUs. The code for this implementation is set to be released. Given that my own distributed implementation has not achieved such impressive scaling performance, I am eager to study the training code from TensorFlow. Could anyone provide an update on the current progress? Thank you!