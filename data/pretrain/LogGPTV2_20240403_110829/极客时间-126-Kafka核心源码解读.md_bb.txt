# 25 \| ReplicaManager（下）：副本管理器是如何管理副本的？你好，我是胡夕。上节课我们学习了 ReplicaManager类源码中副本管理器是如何执行副本读写操作的。现在我们知道了，这个副本读写操作主要是通过appendRecords 和 fetchMessages这两个方法实现的，而这两个方法其实在底层分别调用了 Log 的 append 和 read方法，也就是我们在第 3 节课slate-object="inline"中学到的日志消息写入和日志消息读取方法。今天，我们继续学习 ReplicaManager类源码，看看副本管理器是如何管理副本的。这里的副本，涵盖了广义副本对象的方方面面，包括副本和分区对象、副本位移值和ISR管理等。因此，本节课我们结合着源码，具体学习下这几个方面。分区及副本管理除了对副本进行读写之外，副本管理器还有一个重要的功能，就是管理副本和对应的分区。ReplicaManager管理它们的方式，是通过字段 allPartitions来实现的。 所以，我想先带你复习下第 23 节课中的 allPartitions的代码。不过，这次为了强调它作为容器的属性，我们要把注意力放在它是对象池这个特点上，即allPartitions把所有分区对象汇集在一起，统一放入到一个对象池进行管理。    private val allPartitions = new Pool[TopicPartition, HostedPartition](      valueFactory = Some(tp => HostedPartition.Online(Partition(tp, time, this)))    )从代码可以看到，每个 ReplicaManager 实例都维护了所在 Broker上保存的所有分区对象，而每个分区对象 Partition 下面又定义了一组副本对象Replica。通过这样的层级关系，副本管理器实现了对于分区的直接管理和对副本对象的间接管理。应该这样说，**ReplicaManager通过直接操作分区对象来间接管理下属的副本对象**。对于一个 Broker而言，它管理下辖的分区和副本对象的主要方式，就是要确定在它保存的这些副本中，哪些是Leader 副本、哪些是 Follower副本。 这些划分可不是一成不变的，而是随着时间的推移不断变化的。比如说，这个时刻Broker 是分区 A 的 Leader 副本、分区 B 的 Follower副本，但在接下来的某个时刻，Broker 很可能变成分区 A 的 Follower副本、分区 B 的 Leader 副本。而这些变更是通过 Controller 给 Broker 发送 LeaderAndIsrRequest请求来实现的。当 Broker 端收到这类请求后，会调用副本管理器的becomeLeaderOrFollower 方法来处理，并依次执行"成为 Leader 副本"和"成为Follower 副本"的逻辑，令当前 Broker 互换分区 A、B副本的角色。becomeLeaderOrFollower 方法这里我们又提到了 LeaderAndIsrRequest 请求。其实，我们在学习Controller和控制类请求的时候就多次提到过它，在第 12 讲slate-object="inline"中也详细学习过它的作用了。因为隔的时间比较长了，我怕你忘记了，所以这里我们再回顾下。简单来说，它就是告诉接收该请求的Broker：在我传给你的这些分区中，哪些分区的 Leader副本在你这里；哪些分区的 Follower副本在你这里。becomeLeaderOrFollower 方法，就是具体处理 LeaderAndIsrRequest请求的地方，同时也是副本管理器添加分区的地方。下面我们就完整地学习下这个方法的源码。由于这部分代码很长，我将会分为3 个部分向你介绍，分别是处理 Controller Epoch 事宜、执行成为 Leader 和Follower 的逻辑以及构造Response。 我们先看 becomeLeaderOrFollower 方法的第 1大部分， **处理Controller Epoch及其他相关准备工作**的流程图：![](Images/2fbe82eee3e0dc13a8419413bebd7c7e.png)savepage-src="https://static001.geekbang.org/resource/image/20/96/20298371601540a21da0ec5b1a6b1896.jpg"}因为 becomeLeaderOrFollower方法的开头是一段仅用于调试的日志输出，不是很重要，因此，我直接从 if语句开始讲起。第一部分的主体代码如下：    // 如果LeaderAndIsrRequest携带的Controller Epoch    // 小于当前Controller的Epoch值    if (leaderAndIsrRequest.controllerEpoch         val topicPartition = new TopicPartition(partitionState.topicName, partitionState.partitionIndex)        // 从allPartitions中获取对应分区对象        val partitionOpt = getPartition(topicPartition) match {          // 如果是Offline状态          case HostedPartition.Offline =>            stateChangeLogger.warn(s"Ignoring LeaderAndIsr request from " +              s"controller $controllerId with correlation id $correlationId " +              s"epoch $controllerEpoch for partition $topicPartition as the local replica for the " +              "partition is in an offline log directory")            // 添加对象异常到Response，并设置分区对象变量partitionOpt=None            responseMap.put(topicPartition, Errors.KAFKA_STORAGE_ERROR)            None          // 如果是Online状态，直接赋值partitionOpt即可          case HostedPartition.Online(partition) =>            Some(partition)          // 如果是None状态，则表示没有找到分区对象          // 那么创建新的分区对象将，新创建的分区对象加入到allPartitions统一管理          // 然后赋值partitionOpt字段          case HostedPartition.None =>            val partition = Partition(topicPartition, time, this)            allPartitions.putIfNotExists(topicPartition, HostedPartition.Online(partition))            Some(partition)        }        // 检查分区的Leader Epoch值        ......      }现在，我们一起来学习下这部分内容的核心逻辑。首先，比较 LeaderAndIsrRequest 携带的 Controller Epoch 值和当前Controller Epoch 值。如果发现前者小于后者，说明 Controller已经变更到别的 Broker 上了，需要构造一个 STALE_CONTROLLER_EPOCH异常并封装进 Response 返回。否则，代码进入 else分支。 然后，becomeLeaderOrFollower 方法会更新当前缓存的 Controller Epoch值，再提取出 LeaderAndIsrRequest请求中涉及到的分区，之后依次遍历这些分区，并执行下面的两步逻辑。第 1 步，从 allPartitions 中取出对应的分区对象。在第 23节课，我们学习了分区有 3种状态，即在线（Online）、离线（Offline）和不存在（None），这里代码就需要分别应对这3 种情况： 1.  如果是 Online 状态的分区，直接将其赋值给 partitionOpt    字段即可；        2.  如果是 Offline 状态的分区，说明该分区副本所在的 Kafka    日志路径出现 I/O 故障时（比如磁盘满了），需要构造对应的    KAFKA_STORAGE_ERROR 异常并封装进 Response，同时令 partitionOpt    字段为 None；        3.  如果是 None 状态的分区，则创建新分区对象，然后将其加入到    allPartitions 中，进行统一管理，并赋值给 partitionOpt    字段。    第 2 步，检查 partitionOpt 字段表示的分区的 LeaderEpoch。检查的原则是要确保请求中携带的 Leader Epoch 值要大于当前缓存的Leader Epoch，否则就说明是过期 Controller发送的请求，就直接忽略它，不做处理。总之呢，becomeLeaderOrFollower方法的第一部分代码，主要做的事情就是创建新分区、更新 Controller Epoch和校验分区 Leader Epoch。我们在第 3 讲说到过 Leader Epoch机制，因为是比较高阶的用法，你可以不用重点掌握，这不会影响到我们学习副本管理。不过，如果你想深入了解的话，推荐你课下自行阅读下LeaderEpochFileCache.scala的源码。 当为所有分区都执行完这两个步骤之后，**becomeLeaderOrFollower 方法进入到第 2 部分，开始执行Broker 成为 Leader 副本和 Follower副本的逻辑**：    // 确定Broker上副本是哪些分区的Leader副本    val partitionsToBeLeader = partitionStates.filter { case (_, partitionState) =>      partitionState.leader == localBrokerId    }    // 确定Broker上副本是哪些分区的Follower副本    val partitionsToBeFollower = partitionStates.filter { case (k, _) => !partitionsToBeLeader.contains(k) }    val highWatermarkCheckpoints = new LazyOffsetCheckpoints(this.highWatermarkCheckpoints)    val partitionsBecomeLeader = if (partitionsToBeLeader.nonEmpty)      // 调用makeLeaders方法为partitionsToBeLeader所有分区      // 执行"成为Leader副本"的逻辑      makeLeaders(controllerId, controllerEpoch, partitionsToBeLeader, correlationId, responseMap,        highWatermarkCheckpoints)    else      Set.empty[Partition    val partitionsBecomeFollower = if (partitionsToBeFollower.nonEmpty)      // 调用makeFollowers方法为令partitionsToBeFollower所有分区      // 执行"成为Follower副本"的逻辑      makeFollowers(controllerId, controllerEpoch, partitionsToBeFollower, correlationId, responseMap,        highWatermarkCheckpoints)    else      Set.empty[Partition    val leaderTopicSet = leaderPartitionsIterator.map(_.topic).toSet    val followerTopicSet = partitionsBecomeFollower.map(_.topic).toSet    // 对于当前Broker成为Follower副本的主题    // 移除它们之前的Leader副本监控指标    followerTopicSet.diff(leaderTopicSet).foreach(brokerTopicStats.removeOldLeaderMetrics)    // 对于当前Broker成为Leader副本的主题    // 移除它们之前的Follower副本监控指    leaderTopicSet.diff(followerTopicSet).foreach(brokerTopicStats.removeOldFollowerMetrics)    // 如果有分区的本地日志为空，说明底层的日志路径不可用    // 标记该分区为Offline状态    leaderAndIsrRequest.partitionStates.forEach { partitionState =>      val topicPartition = new TopicPartition(partitionState.topicName, partitionState.partitionIndex)      if (localLog(topicPartition).isEmpty)        markPartitionOffline(topicPartition)    }**首先**，这部分代码需要先确定两个分区集合，一个是把该 Broker 当成Leader 的所有分区；一个是把该 Broker 当成 Follower的所有分区。判断的依据，主要是看 LeaderAndIsrRequest 请求中分区的 Leader信息，是不是和本 Broker 的 ID 相同。如果相同，则表明该 Broker是这个分区的 Leader；否则，表示当前 Broker 是这个分区的Follower。 一旦确定了这两个分区集合，**接着**，代码就会分别为它们调用 makeLeaders 和 makeFollowers方法，正式让 Leader 和 Follower 角色生效。之后，对于那些当前 Broker 成为Follower 副本的主题，代码需要移除它们之前的 Leader副本监控指标，以防出现系统资源泄露的问题。同样地，对于那些当前 Broker成为 Leader 副本的主题，代码要移除它们之前的 Follower副本监控指标。**最后**，如果有分区的本地日志为空，说明底层的日志路径不可用，那么标记该分区为Offline 状态。所谓的标记为 Offline 状态，主要是两步：第 1 步是更新allPartitions 中分区的状态；第 2步是移除对应分区的监控指标。小结一下，becomeLeaderOrFollower 方法第 2 大部分的主要功能是，调用makeLeaders 和 makeFollowers 方法，令 Broker 在不同分区上的 Leader 或Follower角色生效。关于这两个方法的实现细节，一会儿我再详细说。现在，让我们看看**第 3 大部分的代码，构造 Response对象** 。这部分代码是becomeLeaderOrFollower方法的收尾操作。    // 启动高水位检查点专属线程    // 定期将Broker上所有非Offline分区的高水位值写入到检查点文件    startHighWatermarkCheckPointThread()    // 添加日志路径数据迁移线程    maybeAddLogDirFetchers(partitionStates.keySet, highWatermarkCheckpoints)    // 关闭空闲副本拉取线程    replicaFetcherManager.shutdownIdleFetcherThreads()    // 关闭空闲日志路径数据迁移线程    replicaAlterLogDirsManager.shutdownIdleFetcherThreads()    // 执行Leader变更之后的回调逻辑    onLeadershipChange(partitionsBecomeLeader, partitionsBecomeFollower)    // 构造LeaderAndIsrRequest请求的Response并返回    val responsePartitions = responseMap.iterator.map { case (tp, error) =>      new LeaderAndIsrPartitionError()        .setTopicName(tp.topic)        .setPartitionIndex(tp.partition)        .setErrorCode(error.code)    }.toBuffer    new LeaderAndIsrResponse(new LeaderAndIsrResponseData()      .setErrorCode(Errors.NONE.code)      .setPartitionErrors(responsePartitions.asJava))我们来分析下这部分代码的执行逻辑吧。首先，这部分开始时会启动一个专属线程来执行高水位值持久化，定期地将Broker 上所有非 Offline分区的高水位值写入检查点文件。这个线程是个后台线程，默认每 5秒执行一次。同时，代码还会添加日志路径数据迁移线程。这个线程的主要作用是，将路径A 上面的数据搬移到路径 B 上。这个功能是 Kafka 支持 JBOD（Just a Bunch ofDisks）的重要前提。之后，becomeLeaderOrFollower方法会关闭空闲副本拉取线程和空闲日志路径数据迁移线程。判断空闲与否的主要条件是，分区Leader/Follower角色调整之后，是否存在不再使用的拉取线程了。代码要确保及时关闭那些不再被使用的线程对象。再之后是执行 LeaderAndIsrRequest请求的回调处理逻辑。这里的回调逻辑，实际上只是对 Kafka两个内部主题（\_\_consumer_offsets 和\_\_transaction_state）有用，其他主题一概不适用。所以通常情况下，你可以无视这里的回调逻辑。等这些都做完之后，代码开始执行这部分最后，也是最重要的任务：构造LeaderAndIsrRequest 请求的 Response，然后将新创建的 Response返回。至此，这部分方法的逻辑结束。纵观 becomeLeaderOrFollower 方法的这 3 大部分，becomeLeaderOrFollower方法最重要的职责，在我看来就是调用 makeLeaders 和 makeFollowers方法，为各自的分区列表执行相应的角色确认工作。接下来，我们就分别看看这两个方法是如何实现这种角色确认的。makeLeaders 方法makeLeaders 方法的作用是，让当前 Broker 成为给定一组分区的Leader，也就是让当前 Broker 下该分区的副本成为 Leader副本。这个方法主要有 3 步：1.       停掉这些分区对应的获取线程；        2.       更新 Broker    缓存中的分区元数据信息；        3.       将指定分区添加到 Leader    分区集合。        我们结合代码分析下这些都是如何实现的。首先，我们看下 makeLeaders的方法签名：    // controllerId：Controller所在Broker的ID    // controllEpoch：Controller Epoch值，可以认为是Controller版本号    // partitionStates：LeaderAndIsrRequest请求中携带的分区信息    // correlationId：请求的Correlation字段，只用于日志调试    // responseMap：按照主题分区分组的异常错误集合    // highWatermarkCheckpoints：操作磁盘上高水位检查点文件的工具类    private def makeLeaders(controllerId: Int,      controllerEpoch: Int,      partitionStates: Map[Partition, LeaderAndIsrPartitionState],      correlationId: Int,      responseMap: mutable.Map[TopicPartition, Errors],      highWatermarkCheckpoints: OffsetCheckpoints): Set[Partition] = {      ......    }可以看出，makeLeaders 方法接收 6个参数，并返回一个分区对象集合。这个集合就是当前 Broker 是 Leader的所有分区。在这 6 个参数中，以下 3个参数比较关键，我们看下它们的含义。1.  controllerId：Controller 所在 Broker 的    ID。该字段只是用于日志输出，无其他实际用途。        2.  controllerEpoch：Controller Epoch 值，可以认为是 Controller    版本号。该字段用于日志输出使用，无其他实际用途。        3.  partitionStates：LeaderAndIsrRequest    请求中携带的分区信息，包括每个分区的 Leader 是谁、ISR    都有哪些等数据。        好了，现在我们继续学习 makeLeaders的代码。我把这个方法的关键步骤放在了注释里，并省去了一些日志输出相关的代码。    ......    // 使用Errors.NONE初始化ResponseMap    partitionStates.keys.foreach { partition =>      ......      responseMap.put(partition.topicPartition, Errors.NONE)    }    val partitionsToMakeLeaders = mutable.Set[Partition]()    try {      // 停止消息拉取      replicaFetcherManager.removeFetcherForPartitions(        partitionStates.keySet.map(_.topicPartition))      stateChangeLogger.info(s"Stopped fetchers as part of LeaderAndIsr request correlationId $correlationId from " +        s"controller $controllerId epoch $controllerEpoch as part of the become-leader transition for " +        s"${partitionStates.size} partitions")      // 更新指定分区的Leader分区信息      partitionStates.foreach { case (partition, partitionState) =>        try {          if (partition.makeLeader(partitionState, highWatermarkCheckpoints))            partitionsToMakeLeaders += partition          else            ......        } catch {          case e: KafkaStorageException =>            ......            // 把KAFKA_SOTRAGE_ERRROR异常封装到Response中            responseMap.put(partition.topicPartition, Errors.KAFKA_STORAGE_ERROR)        }      }    } catch {      case e: Throwable =>        ......    }    ......    partitionsToMakeLeaders我把主要的执行流程，梳理为了一张流程图：![](Images/08b2b970441ad12e1bb040f8f1f5bd76.png)savepage-src="https://static001.geekbang.org/resource/image/05/25/053b8eb9c4bb0342398ce9650b37aa25.png"}结合着图，我再带着你学习下这个方法的执行逻辑。首先，将给定的一组分区的状态全部初始化成Errors.None。然后，停止为这些分区服务的所有拉取线程。毕竟该 Broker现在是这些分区的 Leader 副本了，不再是 Follower副本了，所以没有必要再使用拉取线程了。最后，makeLeaders 方法调用 Partition 的 makeLeader方法，去更新给定一组分区的 Leader 分区信息，而这些是由 Partition 类中的makeLeader 方法完成的。该方法保存分区的 Leader 和 ISR信息，同时创建必要的日志对象、重设远端 Follower 副本的 LEO值。 那远端 Follower 副本，是什么意思呢？远端 Follower 副本，是指保存在Leader 副本本地内存中的一组 Follower 副本集合，在代码中用字段remoteReplicas 来表征。ReplicaManager 在处理 FETCH 请求时，会更新 remoteReplicas中副本对象的 LEO 值。同时，Leader 副本会将自己更新后的 LEO 值与remoteReplicas 中副本的 LEO值进行比较，来决定是否"抬高"高水位值。而 Partition 类中的 makeLeader 方法的一个重要步骤，就是要重设这组远端Follower 副本对象的 LEO 值。makeLeaders 方法执行完 Partition.makeLeader 后，如果当前 Broker成功地成为了该分区的 Leader 副本，就返回 True，表示新 Leader配置成功，否则，就表示处理失败。倘若成功设置了Leader，那么，就把该分区加入到已成功设置 Leader的分区列表中，并返回该列表。至此，方法结束。我再来小结下，makeLeaders 的作用是令当前 Broker成为给定分区的 Leader 副本。接下来，我们再看看与 makeLeaders方法功能相反的 makeFollowers方法。 makeFollowers 方法makeFollowers 方法的作用是，将当前 Broker 配置成指定分区的 Follower副本。我们还是先看下方法签名：    // controllerId：Controller所在Broker的Id    // controllerEpoch：Controller Epoch值    // partitionStates：当前Broker是Follower副本的所有分区的详细信息    // correlationId：连接请求与响应的关联字段    // responseMap：封装LeaderAndIsrRequest请求处理结果的字段    // highWatermarkCheckpoints：操作高水位检查点文件的工具类    private def makeFollowers(      controllerId: Int,      controllerEpoch: Int,      partitionStates: Map[Partition, LeaderAndIsrPartitionState],      correlationId: Int,      responseMap: mutable.Map[TopicPartition, Errors],      highWatermarkCheckpoints: OffsetCheckpoints) : Set[Partition] = {      ......    }你看，makeFollowers 方法的参数列表与 makeLeaders方法，是一模一样的。这里我也就不再展开了。其中比较重要的字段，就是 partitionStates 和responseMap。基本上，你可以认为 partitionStates 是 makeFollowers方法的输入，responseMap是输出。 因为整个 makeFollowers方法的代码很长，所以我接下来会先用一张图解释下它的核心逻辑，让你先有个全局观；然后，我再按照功能划分带你学习每一部分的代码。![](Images/d9f77ea739ebf66e46d94affe10ae358.png)savepage-src="https://static001.geekbang.org/resource/image/b2/88/b2dee2575c773afedcf6ee7ce00c7b88.jpg"}总体来看，makeFollowers方法分为两大步：1.  第 1 步，遍历 partitionStates 中的所有分区，然后执行"成为    Follower"的操作；        2.  第 2 步，执行其他动作，主要包括重建 Fetcher    线程、完成延时请求等。        首先， **我们学习第 1步遍历 partitionStates所有分区的代码**：    // 第一部分：遍历partitionStates所有分区    ......    partitionStates.foreach { case (partition, partitionState) =>      ......      // 将所有分区的处理结果的状态初始化为Errors.NONE      responseMap.put(partition.topicPartition, Errors.NONE)    }    val partitionsToMakeFollower: mutable.Set[Partition] = mutable.Set()    try {      // 遍历partitionStates所有分区      partitionStates.foreach { case (partition, partitionState) =>        // 拿到分区的Leader Broker ID        val newLeaderBrokerId = partitionState.leader        try {          // 在元数据缓存中找到Leader Broke对象          metadataCache.getAliveBrokers.find(_.id == newLeaderBrokerId) match {            // 如果Leader确实存在            case Some(_) =>              // 执行makeFollower方法，将当前Broker配置成该分区的Follower副本              if (partition.makeFollower(partitionState, highWatermarkCheckpoints))                // 如果配置成功，将该分区加入到结果返回集中                partitionsToMakeFollower += partition              else // 如果失败，打印错误日志                ......            // 如果Leader不存在            case None =>              ......              // 依然创建出分区Follower副本的日志对象              partition.createLogIfNotExists(isNew = partitionState.isNew, isFutureReplica = false,                highWatermarkCheckpoints)          }        } catch {          case e: KafkaStorageException =>            ......        }      }在这部分代码中，我们可以把它的执行逻辑划分为两大步骤。第 1 步，将结果返回集合中所有分区的处理结果状态初始化为Errors.NONE；第 2 步，遍历 partitionStates中的所有分区，依次为每个分区执行以下逻辑：1.  从分区的详细信息中获取分区的 Leader Broker    ID；    2.  拿着上一步获取的 Broker ID，去 Broker 元数据缓存中找到 Leader    Broker 对象；        3.  如果 Leader 对象存在，则执行 Partition 类的 makeFollower    方法将当前 Broker 配置成该分区的 Follower 副本。如果 makeFollower    方法执行成功，就说明当前 Broker 被成功配置为指定分区的 Follower    副本，那么将该分区加入到结果返回集中。        4.  如果 Leader 对象不存在，依然创建出分区 Follower    副本的日志对象。        说到 Partition 的 makeFollower 方法的执行逻辑，主要是包括以下 4步： 1.       更新 Controller Epoch    值；    2.       保存副本列表（Assigned Replicas，AR）和清空    ISR；    3.       创建日志对象；        4.       重设 Leader 副本的 Broker    ID。    接下来， **我们看下makeFollowers 方法的第 2步，执行其他动作的代码**：    // 第二部分：执行其他动作    // 移除现有Fetcher线程    replicaFetcherManager.removeFetcherForPartitions(      partitionsToMakeFollower.map(_.topicPartition))    ......    // 尝试完成延迟请求    partitionsToMakeFollower.foreach { partition =>      completeDelayedFetchOrProduceRequests(partition.topicPartition)    }    if (isShuttingDown.get()) {      .....    } else {      // 为需要将当前Broker设置为Follower副本的分区      // 确定Leader Broker和起始读取位移值fetchOffset      val partitionsToMakeFollowerWithLeaderAndOffset = partitionsToMakeFollower.map { partition =>      val leader = metadataCache.getAliveBrokers        .find(_.id == partition.leaderReplicaIdOpt.get).get        .brokerEndPoint(config.interBrokerListenerName)      val fetchOffset = partition.localLogOrException.highWatermark        partition.topicPartition -> InitialFetchState(leader,           partition.getLeaderEpoch, fetchOffset)      }.toMap      // 使用上一步确定的Leader Broker和fetchOffset添加新的Fetcher线程      replicaFetcherManager.addFetcherForPartitions(        partitionsToMakeFollowerWithLeaderAndOffset)      }    } catch {      case e: Throwable =>        ......        throw e    }    ......    // 返回需要将当前Broker设置为Follower副本的分区列表    partitionsToMakeFollower你看，这部分代码的任务比较简单，逻辑也都是线性递进的，很好理解。我带你简单地梳理一下。首先，移除现有 Fetcher 线程。因为 Leader 可能已经更换了，所以要读取的Broker以及要读取的位移值都可能随之发生变化。然后，为需要将当前 Broker 设置为 Follower 副本的分区，确定 LeaderBroker 和起始读取位移值 fetchOffset。这些信息都已经在LeaderAndIsrRequest 中了。接下来，使用上一步确定的 Leader Broker 和 fetchOffset 添加新的Fetcher 线程。最后，返回需要将当前 Broker 设置为 Follower副本的分区列表。至此，副本管理器管理分区和副本的主要方法实现，我们就都学完啦。可以看出，这些代码实现的大部分，都是围绕着如何处理LeaderAndIsrRequest 请求数据展开的。比如，makeLeaders拿到请求数据后，会为分区设置 Leader 和 ISR；makeFollowers拿到数据后，会为分区更换 Fetcher 线程以及清空ISR。 LeaderAndIsrRequest 请求是 Kafka定义的最重要的控制类请求。搞懂它是如何被处理的，对于你弄明白 Kafka的副本机制是大有裨益的。ISR 管理除了读写副本、管理分区和副本的功能之外，副本管理器还有一个重要的功能，那就是管理ISR。这里的管理主要体现在两个方法：1.  一个是 maybeShrinkIsr 方法，作用是阶段性地查看 ISR    中的副本集合是否需要收缩；        2.  另一个是 maybePropagateIsrChanges 方法，作用是定期向集群 Broker    传播 ISR 的变更。        首先，我们看下 ISR的收缩操作。maybeShrinkIsr 方法收缩是指，把 ISR 副本集合中那些与 Leader差距过大的副本移除的过程。所谓的差距过大，就是 ISR 中 Follower 副本滞后Leader 副本的时间，超过了 Broker 端参数 replica.lag.time.max.ms 值的 1.5倍。 稍等，为什么是 1.5倍呢？你可以看下面的代码：    def startup(): Unit = {      scheduler.schedule("isr-expiration", maybeShrinkIsr _, period = config.replicaLagTimeMaxMs / 2, unit = TimeUnit.MILLISECONDS)      ......    }我来解释下。ReplicaManager 类的 startup方法会在被调用时创建一个异步线程，定时查看是否有 ISR需要进行收缩。这里的定时频率是 replicaLagTimeMaxMs 值的一半，而判断Follower 副本是否需要被移除 ISR 的条件是，滞后程度是否超过了replicaLagTimeMaxMs 值。因此理论上，滞后程度小于 1.5 倍 replicaLagTimeMaxMs 值的 Follower副本，依然有可能在 ISR中，不会被移除。这就是数字"1.5"的由来了。接下来，我们看下 maybeShrinkIsr方法的源码。    private def maybeShrinkIsr(): Unit = {      trace("Evaluating ISR list of partitions to see which replicas can be removed from the ISR")      allPartitions.keys.foreach { topicPartition =>        nonOfflinePartition(topicPartition).foreach(_.maybeShrinkIsr())      }    }可以看出，maybeShrinkIsr方法会遍历该副本管理器上所有分区对象，依次为这些分区中状态为 Online的分区，执行 Partition 类的 maybeShrinkIsr方法。这个方法的源码如下：    def maybeShrinkIsr(): Unit = {      // 判断是否需要执行ISR收缩      val needsIsrUpdate = inReadLock(leaderIsrUpdateLock) {        needsShrinkIsr()      }      val leaderHWIncremented = needsIsrUpdate && inWriteLock(leaderIsrUpdateLock) {        leaderLogIfLocal match {          // 如果是Leader副本          case Some(leaderLog) =>            // 获取不同步的副本Id列表            val outOfSyncReplicaIds = getOutOfSyncReplicas(replicaLagTimeMaxMs)            // 如果存在不同步的副本Id列表            if (outOfSyncReplicaIds.nonEmpty) {              // 计算收缩之后的ISR列表              val newInSyncReplicaIds = inSyncReplicaIds -- outOfSyncReplicaIds              assert(newInSyncReplicaIds.nonEmpty)              info("Shrinking ISR from %s to %s. Leader: (highWatermark: %d, endOffset: %d). Out of sync replicas: %s."                .format(inSyncReplicaIds.mkString(","),                  newInSyncReplicaIds.mkString(","),                  leaderLog.highWatermark,                  leaderLog.logEndOffset,                  outOfSyncReplicaIds.map { replicaId =>                    s"(brokerId: $replicaId, endOffset: ${getReplicaOrException(replicaId).logEndOffset})"                  }.mkString(" ")                )              )              // 更新ZooKeeper中分区的ISR数据以及Broker的元数据缓存中的数据              shrinkIsr(newInSyncReplicaIds)              // 尝试更新Leader副本的高水位值              maybeIncrementLeaderHW(leaderLog)            } else {              false            }          // 如果不是Leader副本，什么都不做          case None => false        }      }      // 如果Leader副本的高水位值抬升了      if (leaderHWIncremented)        // 尝试解锁一下延迟请求        tryCompleteDelayedRequests()    }可以看出，maybeShrinkIsr方法的整个执行流程是：1.  **第 1 步**        ，判断是否需要执行 ISR 收缩。主要的方法是，调用    needShrinkIsr 方法来获取与 Leader    不同步的副本。如果存在这样的副本，说明需要执行 ISR    收缩。    2.  **第 2 步**        ，再次获取与 Leader 不同步的副本列表，并把它们从当前    ISR 中剔除出去，然后计算得出最新的 ISR    列表。    3.  **第 3 步**        ，调用 shrinkIsr 方法去更新 ZooKeeper 上分区的 ISR    数据以及 Broker    上元数据缓存。        4.  **第 4 步**        ，尝试更新 Leader    分区的高水位值。这里有必要检查一下是否可以抬升高水位值的原因在于，如果    ISR 收缩后只剩下 Leader    副本一个了，那么高水位值的更新就不再受那么多限制了。        5.  **第 5 步**        ，根据上一步的结果，来尝试解锁之前不满足条件的延迟操作。        我把这个执行过程，梳理到了一张流程图中：![](Images/737bc1ecbc2ce4a061f46b0033d96f9f.png)savepage-src="https://static001.geekbang.org/resource/image/0c/3e/0ce6b2e29byyfd4db331e65df6b8bb3e.jpg"}maybePropagateIsrChanges 方法ISR 收缩之后，ReplicaManager 还需要将这个操作的结果传递给集群的其他Broker，以同步这个操作的结果。这是由 ISR通知事件来完成的。在 ReplicaManager 类中，方法 maybePropagateIsrChanges 专门负责创建ISR通知事件。这也是由一个异步线程定期完成的，代码如下：    scheduler.schedule("isr-change-propagation", maybePropagateIsrChanges _, period = 2500L, unit = TimeUnit.MILLISECONDS)接下来，我们看下 maybePropagateIsrChanges方法的代码：    def maybePropagateIsrChanges(): Unit = {      val now = System.currentTimeMillis()      isrChangeSet synchronized {        // ISR变更传播的条件，需要同时满足：        // 1. 存在尚未被传播的ISR变更        // 2. 最近5秒没有任何ISR变更，或者自上次ISR变更已经有超过1分钟的时间        if (isrChangeSet.nonEmpty &&          (lastIsrChangeMs.get() + ReplicaManager.IsrChangePropagationBlackOut           ......      }    }一旦实例被成功创建，就会被 Kafka 的 4 个组件使用。我来给你解释一下这4个组件的名称，以及它们各自使用该实例的主要目的。1.  KafkaApis：这是源码入口类。它是执行 Kafka    各类请求逻辑的地方。该类大量使用 MetadataCache 中的主题分区和 Broker    数据，执行主题相关的判断与比较，以及获取 Broker    信息。    2.  AdminManager：这是 Kafka    定义的专门用于管理主题的管理器，里面定义了很多与主题相关的方法。同    KafkaApis 类似，它会用到 MetadataCache 中的主题信息和 Broker    数据，以获取主题和 Broker    列表。    3.  ReplicaManager：这是我们刚刚学过的副本管理器。它需要获取主题分区和    Broker 数据，同时还会更新    MetadataCache。        4.  TransactionCoordinator：这是管理 Kafka    事务的协调者组件，它需要用到 MetadataCache 中的主题分区的 Leader    副本所在的 Broker 数据，向指定 Broker    发送事务标记。        类定义及字段搞清楚了 MetadataCache类被创建的时机以及它的调用方，我们就了解了它的典型使用场景，即作为集群元数据集散地，它保存了集群中关于主题和Broker的所有重要数据。那么，接下来，我们来看下这些数据到底都是什么。    class MetadataCache(brokerId: Int) extends Logging {      private val partitionMetadataLock = new ReentrantReadWriteLock()      @volatile private var metadataSnapshot: MetadataSnapshot = MetadataSnapshot(partitionStates = mutable.AnyRefMap.empty,        controllerId = None, aliveBrokers = mutable.LongMap.empty, aliveNodes = mutable.LongMap.empty)      this.logIdent = s"[MetadataCache brokerId=$brokerId] "      private val stateChangeLogger = new StateChangeLogger(brokerId, inControllerContext = false, None)      ......    }MetadataCache类构造函数只需要一个参数：**brokerId**，即 Broker 的 ID 序号。除了这个参数，该类还定义了 4个字段。 partitionMetadataLock 字段是保护它写入的锁对象，logIndent 和stateChangeLogger 字段仅仅用于日志输出，而 metadataSnapshot字段保存了实际的元数据信息，它是 MetadataCache类中最重要的字段，我们要重点关注一下它。该字段的类型是 MetadataSnapshot 类，该类是 MetadataCache中定义的一个嵌套类。以下是该嵌套类的源码：    case class MetadataSnapshot(partitionStates: mutable.AnyRefMap      [String, mutable.LongMap[UpdateMetadataPartitionState]],      controllerId: Option[Int],      aliveBrokers: mutable.LongMap[Broker],      aliveNodes: mutable.LongMap[collection.Map[ListenerName, Node]])从源码可知，它是一个 case 类，相当于 Java 中配齐了 Getter 方法的 POJO类。同时，它也是一个不可变类（ImmutableClass）。正因为它的不可变性，其字段值是不允许修改的，我们只能重新创建一个新的实例，来保存更新后的字段值。我们看下它的各个字段的含义。1.  **partitionStates**        ：这是一个 Map 类型。Key 是主题名称，Value 又是一个    Map 类型，其 Key 是分区号，Value 是一个 UpdateMetadataPartitionState    类型的字段。UpdateMetadataPartitionState 类型是    UpdateMetadataRequest    请求内部所需的数据结构。一会儿我们再说这个类型都有哪些数据。        2.  **controllerId**        ：Controller 所在 Broker 的    ID。    3.  **aliveBrokers**        ：当前集群中所有存活着的 Broker    对象列表。        4.  **aliveNodes**        ：这也是一个 Map 的 Map 类型。其 Key 是 Broker ID    序号，Value 是 Map 类型，其 Key 是 ListenerName，即 Broker    监听器类型，而 Value 是 Broker    节点对象。        现在，我们说说 UpdateMetadataPartitionState 类型。这个类型的源码是由Kafka 工程自动生成的。UpdateMetadataRequest 请求所需的字段用 JSON格式表示，由 Kafka 的 generator 工程负责为 JSON 格式自动生成对应的 Java文件，生成的类是一个 POJO类，其定义如下：    static public class UpdateMetadataPartitionState implements Message {        private String topicName;     // 主题名称        private int partitionIndex;   // 分区号        private int controllerEpoch;  // Controller Epoch值        private int leader;           // Leader副本所在Broker ID        private int leaderEpoch;      // Leader Epoch值        private List isr;    // ISR列表        private int zkVersion;        // ZooKeeper节点Stat统计信息中的版本号        private List replicas;  // 副本列表        private List offlineReplicas;  // 离线副本列表        private List _unknownTaggedFields; // 未知字段列表        ......    }可以看到，UpdateMetadataPartitionState类的字段信息非常丰富，它包含了一个主题分区非常详尽的数据，从主题名称、分区号、Leader副本、ISR 列表到 Controller Epoch、ZooKeeper版本号等信息，一应俱全。从宏观角度来看，Kafka 集群元数据由主题数据和Broker 数据两部分构成。所以，可以这么说，MetadataCache中的这个字段撑起了元数据缓存的"一半天空"。重要方法接下来，我们学习下 MetadataCache类的重要方法。你需要记住的是，这个类最重要的方法就是**操作 metadataSnapshot字段的方法**，毕竟，所谓的元数据缓存，就是指 MetadataSnapshot类中承载的东西。我把 MetadataCache类的方法大致分为三大类：1.       判断类；        2.       获取类；        3.       更新类。        这三大类方法是由浅入深的关系，我们先从简单的判断类方法开始。判断类方法所谓的判断类方法，就是判断给定主题或主题分区是否包含在元数据缓存中的方法。MetadataCache类提供了两个判断类的方法，方法名都是**contains**，只是输入参数不同。    // 判断给定主题是否包含在元数据缓存中    def contains(topic: String): Boolean = {      metadataSnapshot.partitionStates.contains(topic)    }    // 判断给定主题分区是否包含在元数据缓存中    def contains(tp: TopicPartition): Boolean = getPartitionInfo(tp.topic, tp.partition).isDefined    // 获取给定主题分区的详细数据信息。如果没有找到对应记录，返回None    def getPartitionInfo(topic: String,       partitionId: Int): Option[UpdateMetadataPartitionState] = {      metadataSnapshot.partitionStates.get(topic)        .flatMap(_.get(partitionId))    }第一个 contains方法用于判断给定主题是否包含在元数据缓存中，比较简单，只需要判断metadataSnapshot 中 partitionStates 的所有 Key是否包含指定主题就行了。第二个 contains 方法相对复杂一点。它首先要从 metadataSnapshot中获取指定主题分区的分区数据信息，然后根据分区数据是否存在，来判断给定主题分区是否包含在元数据缓存中。判断类的方法实现都很简单，代码也不多，很好理解，我就不多说了。接下来，我们来看获取类方法。获取类方法MetadataCache 类的 getXXX 方法非常多，其中，比较有代表性的是getAllTopics 方法、getAllPartitions 方法和getPartitionReplicaEndpoints，它们分别是获取主题、分区和副本对象的方法。在我看来，这是最基础的元数据获取方法了，非常值得我们学习。首先，我们来看入门级的 get 方法，即 getAllTopics方法。该方法返回当前集群元数据缓存中的所有主题。代码如下：    private def getAllTopics(snapshot: MetadataSnapshot): Set[String] = {      snapshot.partitionStates.keySet    }它仅仅是返回 MetadataSnapshot 数据类型中 partitionStates 字段的所有Key 字段。前面说过，partitionStates 是一个 Map 类型，Key就是主题。怎么样，简单吧？如果我们要获取元数据缓存中的分区对象，该怎么写呢？来看看**getAllPartitions方法**的实现。    def getAllPartitions(): Set[TopicPartition] = {      metadataSnapshot.partitionStates.flatMap { case (topicName, partitionsAndStates) =>        partitionsAndStates.keys.map(partitionId => new TopicPartition(topicName, partitionId.toInt))      }.toSet    }和 getAllTopics 方法类似，它的主要思想也是遍历partitionStates，取出分区号后，构建 TopicPartition实例，并加入到返回集合中返回。最后，我们看一个相对复杂一点的 get方法：getPartitionReplicaEndpoints。    def getPartitionReplicaEndpoints(tp: TopicPartition, listenerName: ListenerName): Map[Int, Node] = {      // 使用局部变量获取当前元数据缓存      val snapshot = metadataSnapshot      // 获取给定主题分区的数据      snapshot.partitionStates.get(tp.topic).flatMap(_.get(tp.partition))        .map { partitionInfo =>        // 拿到副本Id列表        val replicaIds = partitionInfo.replicas        replicaIds.asScala          .map(replicaId => replicaId.intValue() -> {            // 获取副本所在的Broker Id            snapshot.aliveBrokers.get(replicaId.longValue()) match {              case Some(broker) =>                // 根据Broker Id去获取对应的Broker节点对象                broker.getNode(listenerName).getOrElse(Node.noNode())              case None => // 如果找不到节点                Node.noNode()            }}).toMap          .filter(pair => pair match {            case (_, node) => !node.isEmpty          })      }.getOrElse(Map.empty[Int, Node])    }这个 getPartitionReplicaEndpoints 方法接收主题分区和ListenerName，以获取指定监听器类型下该主题分区所有副本的 Broker节点对象，并按照 Broker ID进行分组。 首先，代码使用局部变量获取当前的元数据缓存。这样做的好处在于，不需要使用锁技术，但是，就像我开头说过的，这里有一个可能的问题是，读到的数据可能是过期的数据。不过，好在Kafka 能够自行处理过期元数据的问题。当客户端因为拿到过期元数据而向Broker 发出错误的指令时，Broker会显式地通知客户端错误原因。客户端接收到错误后，会尝试再次拉取最新的元数据。这个过程能够保证，客户端最终可以取得最新的元数据信息。总体而言，过期元数据的不良影响是存在的，但在实际场景中并不是太严重。拿到主题分区数据之后，代码会获取副本 ID列表，接着遍历该列表，依次获取每个副本所在的 Broker ID，再根据这个Broker ID 去获取对应的 Broker节点对象。最后，将这些节点对象封装到返回结果中并返回。更新类方法下面，我们进入到今天的"重头戏"：Broker端元数据缓存的更新方法。说它是重头戏，有两个原因：1.       跟前两类方法相比，它的代码实现要复杂得多，因此，我们需要花更多的时间去学习；        2.       元数据缓存只有被更新了，才能被读取。从某种程度上说，它是后续所有    getXXX 方法的前提条件。        源码中实现更新的方法只有一个：**updateMetadata方法**。该方法的代码比较长，我先画一张流程图，帮助你理解它做了什么事情。![](Images/459899d6fe11371ec286cda643124a33.png)savepage-src="https://static001.geekbang.org/resource/image/2a/03/2abcce0bb1e7e4d1ac3d8bbc41c3f803.jpg"}updateMetadata方法的主要逻辑，就是**读取 UpdateMetadataRequest请求中的分区数据，然后更新本地元数据缓存**。接下来，我们详细地学习一下它的实现逻辑。为了方便你掌握，我将该方法分成几个部分来讲，首先来看第一部分代码：    def updateMetadata(correlationId: Int, updateMetadataRequest: UpdateMetadataRequest): Seq[TopicPartition] = {      inWriteLock(partitionMetadataLock) {        // 保存存活Broker对象。Key是Broker ID，Value是Broker对象        val aliveBrokers = new mutable.LongMap[Broker](metadataSnapshot.aliveBrokers.size)        // 保存存活节点对象。Key是Broker ID，Value是监听器->节点对象        val aliveNodes = new mutable.LongMap[collection.Map[ListenerName, Node]](metadataSnapshot.aliveNodes.size)        // 从UpdateMetadataRequest中获取Controller所在的Broker ID        // 如果当前没有Controller，赋值为None        val controllerIdOpt = updateMetadataRequest.controllerId match {            case id if id  None            case id => Some(id)          }        // 遍历UpdateMetadataRequest请求中的所有存活Broker对象        updateMetadataRequest.liveBrokers.forEach { broker =>          val nodes = new java.util.HashMap[ListenerName, Node          val endPoints = new mutable.ArrayBuffer[EndPoint          // 遍历它的所有EndPoint类型，也就是为Broker配置的监听器          broker.endpoints.forEach { ep =>            val listenerName = new ListenerName(ep.listener)            endPoints += new EndPoint(ep.host, ep.port, listenerName, SecurityProtocol.forId(ep.securityProtocol))            // 将对保存起来            nodes.put(listenerName, new Node(broker.id, ep.host, ep.port))          }          // 将Broker加入到存活Broker对象集合          aliveBrokers(broker.id) = Broker(broker.id, endPoints, Option(broker.rack))          // 将Broker节点加入到存活节点对象集合          aliveNodes(broker.id) = nodes.asScala        }        ......      }    }这部分代码的主要作用是给后面的操作准备数据，即 aliveBrokers 和aliveNodes两个字段中保存的数据。因此，首先，代码会创建这两个字段，分别保存存活 Broker对象和存活节点对象。aliveBrokers 的 Key 类型是 Broker ID，而 Value类型是 Broker 对象；aliveNodes 的 Key 类型也是 Broker ID，Value 类型是\ 对。然后，该方法从 UpdateMetadataRequest 中获取 Controller 所在的 BrokerID，并赋值给 controllerIdOpt 字段。如果集群没有Controller，则赋值该字段为None。 接着，代码会遍历 UpdateMetadataRequest 请求中的所有存活 Broker对象。取出它配置的所有 EndPoint 类型，也就是 Broker配置的所有监听器。最后，代码会遍历它配置的监听器，并将 \对保存起来，再将 Broker 加入到存活 Broker对象集合和存活节点对象集合。至此，第一部分代码逻辑完成。再来看第二部分的代码。这一部分的主要工作是**确保集群 Broker配置了相同的监听器，同时初始化已删除分区数组对象，等待下一部分代码逻辑对它进行操作**。代码如下：    // 使用上一部分中的存活Broker节点对象，    // 获取当前Broker所有的对    aliveNodes.get(brokerId).foreach { listenerMap =>      val listeners = listenerMap.keySet      // 如果发现当前Broker配置的监听器与其他Broker有不同之处，记录错误日志      if (!aliveNodes.values.forall(_.keySet == listeners))        error(s"Listeners are not identical across brokers: $aliveNodes")    }    // 构造已删除分区数组，将其作为方法返回结果    val deletedPartitions = new mutable.ArrayBuffer[TopicPartition    // UpdateMetadataRequest请求没有携带任何分区信息    if (!updateMetadataRequest.partitionStates.iterator.hasNext) {      // 构造新的MetadataSnapshot对象，使用之前的分区信息和新的Broker列表信息      metadataSnapshot = MetadataSnapshot(metadataSnapshot.partitionStates, controllerIdOpt, aliveBrokers, aliveNodes)    // 否则，进入到方法最后一部分    } else {      ......    }这部分代码首先使用上一部分中的存活 Broker 节点对象，获取当前 Broker所有的 \ 对。之后，拿到为当前 Broker 配置的所有监听器。如果发现配置的监听器与其他Broker有不同之处，则记录一条错误日志。接下来，代码会构造一个已删除分区数组，将其作为方法返回结果。然后判断UpdateMetadataRequest请求是否携带了任何分区信息，如果没有，则构造一个新的 MetadataSnapshot对象，使用之前的分区信息和新的 Broker列表信息；如果有，代码进入到该方法的最后一个部分。最后一部分全部位于上面代码中的 else分支上。这部分的主要工作是**提取 UpdateMetadataRequest请求中的数据，然后填充元数据缓存**。代码如下：    val partitionStates = new mutable.AnyRefMap[String, mutable.LongMap[UpdateMetadataPartitionState]](metadataSnapshot.partitionStates.size)    // 备份现有元数据缓存中的分区数据    metadataSnapshot.partitionStates.foreach { case (topic, oldPartitionStates) =>      val copy = new mutable.LongMap[UpdateMetadataPartitionState](oldPartitionStates.size)      copy ++= oldPartitionStates      partitionStates(topic) = copy    }    val traceEnabled = stateChangeLogger.isTraceEnabled    val controllerId = updateMetadataRequest.controllerId    val controllerEpoch = updateMetadataRequest.controllerEpoch    // 获取UpdateMetadataRequest请求中携带的所有分区数据    val newStates = updateMetadataRequest.partitionStates.asScala    // 遍历分区数据    newStates.foreach { state =>      val tp = new TopicPartition(state.topicName, state.partitionIndex)      // 如果分区处于被删除过程中      if (state.leader == LeaderAndIsr.LeaderDuringDelete) {        // 将分区从元数据缓存中移除        removePartitionInfo(partitionStates, tp.topic, tp.partition)        if (traceEnabled)          stateChangeLogger.trace(s"Deleted partition $tp from metadata cache in response to UpdateMetadata " +            s"request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId")        // 将分区加入到返回结果数据        deletedPartitions += tp      } else {        // 将分区加入到元数据缓存        addOrUpdatePartitionInfo(partitionStates, tp.topic, tp.partition, state)        if (traceEnabled)          stateChangeLogger.trace(s"Cached leader info $state for partition $tp in response to " +            s"UpdateMetadata request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId")      }    }    val cachedPartitionsCount = newStates.size - deletedPartitions.size    stateChangeLogger.info(s"Add $cachedPartitionsCount partitions and deleted ${deletedPartitions.size} partitions from metadata cache " +      s"in response to UpdateMetadata request sent by controller $controllerId epoch $controllerEpoch with correlation id $correlationId")    // 使用更新过的分区元数据，和第一部分计算的存活Broker列表及节点列表，构建最新的元数据缓存    metadataSnapshot =       MetadataSnapshot(partitionStates, controllerIdOpt, aliveBrokers, aliveNodes)    // 返回已删除分区列表数组    deletedPartitions首先，该方法会备份现有元数据缓存中的分区数据到 partitionStates的局部变量中。之后，获取 UpdateMetadataRequest请求中携带的所有分区数据，并遍历每个分区数据。如果发现分区处于被删除的过程中，就将分区从元数据缓存中移除，并把分区加入到已删除分区数组中。否则的话，代码就将分区加入到元数据缓存中。最后，方法使用更新过的分区元数据，和第一部分计算的存活 Broker列表及节点列表，构建最新的元数据缓存，然后返回已删除分区列表数组。至此，updateMetadata方法结束。 总结今天，我们学习了 Broker 端的 MetadataCache类，即所谓的元数据缓存类。该类保存了当前集群上的主题分区详细数据和Broker 数据。每台 Broker 都维护了一个 MetadataCache 实例。Controller通过给 Broker 发送 UpdateMetadataRequest请求的方式，来异步更新这部分缓存数据。我们来回顾下这节课的重点。1.  MetadataCache 类：Broker 元数据缓存类，保存了分区详细数据和    Broker 节点数据。        2.  四大调用方：分别是    ReplicaManager、KafkaApis、TransactionCoordinator 和    AdminManager。        3.  updateMetadata 方法：Controller 给 Broker 发送    UpdateMetadataRequest    请求时，触发更新。        ![](Images/bfa6ca5ffd4feeb0aa5f87f994b7498d.png)savepage-src="https://static001.geekbang.org/resource/image/e9/81/e95db24997c6cb615150ccc269aeb781.jpg"}最后，我想和你讨论一个话题。有人认为，Kafka Broker是无状态的。学完了今天的内容，现在你应该知道了，Broker并非是无状态的节点，它需要从 Controller端异步更新保存集群的元数据信息。由于 Kafka 采用的是 Leader/Follower模式，跟多 Leader 架构和无 Leader架构相比，这种分布式架构的一致性是最容易保证的，因此，Broker间元数据的最终一致性是有保证的。不过，就像我前面说过的，你需要处理Follower 滞后或数据过期的问题。需要注意的是，这里的 Leader 其实是指Controller，而 Follower 是指普通的 Broker节点。 总之，这一路学到现在，不知道你有没有这样的感受，很多分布式架构设计的问题与方案是相通的。比如，在应对数据备份这个问题上，元数据缓存和Kafka 副本其实都是相同的设计思路，即使用单 Leader 的架构，令 Leader对外提供服务，Follower 只是被动地同步 Leader上的数据。 每次学到新的内容之后，希望你不要把它们当作单一的知识看待，要善于进行思考和总结，做到融会贯通。源码学习固然重要，但能让学习源码引领我们升级架构思想，其实是更难得的收获！课后讨论前面说到，Controller 发送 UpdateMetadataRequest 请求给 Broker时，会更新MetadataCache，你能在源码中找到更新元数据缓存的完整调用路径吗？欢迎在留言区写下你的思考和答案，跟我交流讨论，也欢迎你把今天的内容分享给你的朋友。