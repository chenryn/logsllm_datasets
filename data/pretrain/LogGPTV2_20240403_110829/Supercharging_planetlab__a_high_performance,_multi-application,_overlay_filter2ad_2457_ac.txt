ware threads are suspended, waiting for data to be retrieved from 
external  memory.  Multi-threading  can  be  used  in  a  variety  of 
ways,  but  there  some  common  usage  patterns  that  are  well-
supported by hardware mechanisms. Perhaps the most commonly 
used (and simplest) such pattern involves a group of threads that 
get read
get read
get read
wait
wait
wait
put get read
put get read
put get read
put get read
wait
wait
wait
put
putput
get read
get read
get read
wait
wait
wait
put get read
put get read
put get read
wait
wait
wait
get read
get read
get read
wait
wait
wait
put get read
put get read
put get read
operate in a round-robin fashion, using hardware signals to pass 
control explicitly from one thread to the next, as illustrated below. 
In this example, the first thread starts by reading a data item (e.g. 
a  packet  pointer)  from  a  shared  input  queue,  then  issues  a  read 
request  before  passing  control  to  the  second  thread,  which  then 
reads  the  next  data  item  from  the  shared  queue,  issues  its  own 
read request and passes control to the third thread. By the time the 
third  thread  issues  its  read  request,  the  first  thread  is  ready  to 
continue.  Notice  how  this  allows  the  processor  to  stay  busy,  in 
spite of the long memory latency. Also, note that the round robin 
processing  ensures  that  data  items  are  processed  in  order.  This 
technique works well when the variation in processing times from 
one item to the next is bounded (which is commonly the case in 
packet processing contexts), and is straightforward to implement. 
There are two other aspects of the MEs that are important to 
understand. First, each has a small (8K), dedicated program store, 
from which it executes. This limits the number of different func-
tions that can be implemented by a single ME, favoring programs 
that are divided into smaller pieces and organized as a pipeline. 
The MEs support such pipeline processing by providing dedicated 
FIFOs between consecutive pairs of MEs (Next Neighbor Rings). 
A pipelined program structure also makes it easy to use the proc-
essing  power  of  the  MEs  effectively,  since  the  parallel  compo-
nents of the system are largely decoupled from one another. 
4. SHARING THE NPE 
To support the generic application structure in Figure 1, we have 
developed software for the NPE that allows it to be shared by the 
fast path segments of many different slices. The organization of 
the  software  and  its  mapping  onto  MEs  is  shown  in  Figure  4. 
Packets received from the switch are copied to DRAM buffers by 
the Receive (Rx) block on arrival, which also passes a pointer to 
the  packet  buffer  through  the  main  packet  processing  pipeline. 
Information contained in the packet header can be retrieved from 
DRAM by subsequent blocks as needed, but no explicit copying 
of the packet takes place in the processing pipeline. At the end of 
the pipeline, the Transmit (Tx) block forwards the packet to the 
output. Packet pointers (and other information) are passed along 
DRAM
DRAM
SRAM
SRAM
SRAM
SRAM
Rx
Rx
(2 ME)
(2 ME)
Substr.
Substr.
Decap
Decap
(1 ME)
(1 ME)
Parse
Parse
(1 ME)
(1 ME)
Lookup
Lookup
(1 ME)
(1 ME)
Hdr
Hdr
Format
Format
(1 ME)
(1 ME)
Queue
Queue
Manager
Manager
(2 ME)
(2 ME)
Tx
Tx
(2 ME)
(2 ME)
h
h
c
c
t
t
i
i
w
w
s
s
m
m
o
o
r
r
f
f
Stats
Stats
(1 ME)
(1 ME)
SRAM
SRAM
TCAM
TCAM
Mgmt Processor
Mgmt Processor
(xScale)
(xScale)
h
h
c
c
t
t
i
i
w
w
s
s
o
o
t
t
Figure 4.  NPE software structure showing the use of memory by various software components, and the mapping of components onto 
Micro-Engines (ME) 
the pipeline primarily using FIFOs linking adjacent MEs. Pipeline 
elements typically process 8 packets concurrently using the hard-
ware  thread  contexts.  The  performance  of  individual  pipeline 
stages  can  be  further  increased  by  distributing  the  processing 
across  multiple  MEs.  The  Substrate  Decapsulation  block  deter-
mines which slice the packet belongs to, by doing a lookup in a 
table  stored  in  one  of  the  SRAMs.  It  also  effectively  strips  the 
outer  header  from  the  packet  by  adjusting  the  packet  pointer 
before passing it along the pipeline.  
The  Parse  block  includes  slice-specific  program  segments. 
More  precisely,  Parse  includes  program  segments  that  define  a 
preconfigured  set  of  Code  Options.  Slices  are  configured  to  use 
one  of  the  available  code  options  and  each  slice  has  a  block  of 
memory in SRAM that it can use for slice-specific data. Currently, 
code options have been implemented for IPv4 forwarding and for 
the  Internet  Indirection  Infrastructure  (I3)  [ST02].  New  code 
options  are  fairly  easy  to  add,  but  this  does  require  familiarity 
with  the  NP  programming  environment  and  must  be  done  with 
care  to  ensure  that  new  code  options  do  not  interfere  with  the 
operation of the other components. The primary role of Parse, is 
to examine the slice-specific header and use it and other informa-
tion to form a lookup key, which is passed to the Lookup block. 
The Lookup block provides a generic lookup capability, using 
the  TCAM.  It  treats  the  lookup  key  provided  by  Parse  as  an 
opaque bit string with 112 bits. It augments this bit string with a 
slice identifier before performing the TCAM lookup. The slice’s 
control  software  can  insert  packet  filters  into  the  TCAM.  These 
filters can include up to 112 bits for the lookup key and 112 bits 
of  mask  information.  Software  in  the  Management  Processor 
augments  the  slice-defined  filters  with  the  appropriate  slice  id 
before  inserting  them  into  the  TCAM.  This  gives  each  slice  the 
illusion of a dedicated TCAM. The position of filter entries in the 
TCAM  determines  their  lookup  priority,  so  the  data  associated 
with the first filter in the TCAM matching a given lookup key is 
returned.  The  number  of  entries  assigned  to  different  slices  is 
entirely flexible, but the total number of entries is 128K. 
The Header Formatter which follows Lookup makes any nec-
essary  changes  to  the  slice-specific  packet  header,  based  on  the 
result of the lookup and the semantics of the slice. It also formats 
the  required  outer  packet  header  used  to  forward  the  packet  to 
either the next PlanetLab node, or to its ultimate destination. 
The Queue Manager (QM) implements a configurable collec-
tion  of  queues. More  specifically,  it  provides  ten  distinct  packet 
schedulers, each with a configurable output rate, and each with an 
associated set of queues. Separate schedulers are needed for each 
external  interface  supported  by  Line  Cards.  The  number  of  dis-
tinct schedulers that can be supported by each ME is limited by 
the  need  to  reserve  some  of  the  ME’s  local  memory  for  each. 
Each  scheduler  implements  the  weighted  deficit  round  robin 
scheduling  policy,  allowing  different  shares  to  be  assigned  to 
different queues. When multiple NPEs have schedulers configured 
to send to the same Line Card physical interface, the sum of their 
output rates is configured to be no larger than the physical inter-
face rate. The rates used by the different schedulers can be stati-
cally  configured  or  can  be  dynamically  adjusted  by  distributed 
scheduling processes (this borrows ideas from [PA03]). Each slice 
has an associated set of queues that it can map packets to. When a 
slice’s  control  software  inserts  a  new  filter,  it  specifies  a  slice-
specific  queue  id.  The  filter  insertion  software  remaps  this  to  a 
physical queue id, which is added, as a hidden field, to the filter 
result.  Slices  can  configure  which  scheduler  to  associate  with  a 
specific queue, the effective length of each queue and its share of 
the scheduler bandwidth. 
The Statistics module maintains a variety of counts on behalf 
of  slices.  These  can  be  accessed  by  slices  through  the  Manage-
ment Processor, to enable computation of performance statistics. 
The  counting  function  is  separated  from  the  main  processing 
pipeline  to  keep  the  associated  memory  accesses  from  slowing 
down  the  forwarding  of  packets,  and  to  facilitate  optimizations 
designed to overcome the effects of memory latency. The counts 
maintained by the Statistics module are kept in one of the external 
SRAMs and can be  directly read by the MP. 
5. ENHANCING GPE PERFORMANCE 
While  our  main  focus  is  on  boosting  application  performance 
using  the  NPEs,  the  system  also  provides  opportunities  to  boost 
performance of applications that run only on the GPEs. The GPEs 
can  improve  throughput  over  typical  PlanetLab  nodes  in  two 
)
t
<
e
m
i
t
e
l
c
y
c
(
r
P
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
2 4 8
16
default
8 vServers
1 processor
0
100
200
300
400
500
t (cycle time in ms)
Figure 5. PlanetLab scheduler behavior for compute bound task; 
chart shows cumulative distribution function of “cycle 
time” for various choices of scheduling parameters. 
ways. First, they use higher performance hardware configurations 
than  is  usual  for  PlanetLab.  In  particular,  our  current  GPEs  are 
Intel  NetStructure  MPCBL0040  server  blades  with  a  pair  of  2 
GHz Xeon dual-core processor chips, four gigabit Ethernet inter-
faces and an on-board disk.  
In  addition  to  improving  throughput,  we  would  like  to  im-
prove the latency of PlanetLab applications. Because PlanetLab is 
built on top of a conventional operating system (Linux), it inherits 
the  basic  coarse-grained  scheduling  paradigm  that  characterizes 
such  systems.  The  newer  versions  of  the  PlanetLab  OS  actually 
make significant modifications to the standard Linux scheduling, 
but the default scheduling parameters still produce coarse-grained 
time slices, resulting in latencies that can be unacceptably high for 
some applications. The PlanetLab scheduler is token-based, with 
each  token  representing  1  ms  of  computation.  Each  vServer  is 
supplied with tokens at some specified rate (32 tokens/second, by 
default)  and  has  a  specified  maximum  number  of  tokens  it  can 
hold (100). In addition, before a vServer can be scheduled, it must 
have a specified minimum number of tokens (50). This leads to 
time  slices  of  50-100  ms,  so  in  a  system  where  N  vServers  are 
competing for the CPU, a vServer can be pre-empted for as long 
as  100(N–1)  ms  at  a  time,  meaning  that  a  packet  arriving  for  a 
suspended vServer can wait a very long time before being proc-
essed.  This  issue  is  acknowledged  in  [BA06]  where  the  authors 
address it by artificially changing the process priority of a particu-
lar slice of interest, but this is clearly not a general solution.  
To  gain  some  insight  into  the  scheduler  behavior,  we  ran  a 
simple experiment in which eight identical vServers ran a simple 
compute-bound  program,  competing  for  a  single  processor  core. 
Each  vServer  continuously  checked  the  system  time  to  detect 
periods when it was pre-empted and we recorded the distribution 
of  pre-emption  times.  We  then  estimated  the  “cycle  time”  (the 
time  required  for  all  eight  vServers  to  complete  one  scheduling 
round) as 8/7 times the pre-emption time and plotted the resulting 
cumulative distribution function, shown in Figure 5. The default 
PlanetLab scheduling parameters produce cycle times ranging up 
to 500 ms with very high variability. For the other cases shown, 
each  vServer  was  allocated  tokens  at  a  rate  of  120  per  second 
(slightly less than one-eighth of the processor) and the minimum 
and maximum token allocations were set to the same value, with 
this value being varied from 2 to 16. We note that by making the 
minimum and maximum token values equal, we get much more 
consistent scheduling behavior and we note that the median cycle 
times are roughly equal to the product of the number of vServers 
and the number of tokens, which is what one would expect based 
on an idealized analysis. 
6. CONTROL ARCHITECTURE 
Figure  6  is  a  block  diagram  of  the  system,  showing  the  control 
components  of  the  architecture.  First,  note  that  the  system  pro-
vides  a  control  network  that  is  independent  of  the  switch  that 
carries data traffic (the control net is actually implemented on the 
same switch blade as the main data switch, but the control traffic 
is  logically  and  physically  separate).  The  control  network  is  ac-
cessible  only  to  the  control elements  of  the  architecture.  In  par-
ticular, vServers hosting user slices have no direct access to it. 
The system’s Control Processor (CP) obtains slice configura-
tion data using the standard PlanetLab mechanism of periodically 
polling  the  PlanetLab  Central  database  (PLC).  Slices  that  are 
configured to use the system are assigned to one of the GPEs by 
the  Global  Node  Manager  (GNM)  and  a  corresponding  entry  is 
made  in  a  local  copy  of  the  Planet  Lab  database  (myPLC).  The 
Local Node Managers (LNM) on each of the GPEs periodically 
poll myPLC to obtain new slice configurations. 
Once  a  vServer  has  been  assigned  to  a  slice,  a  user  of  that 
slice  may  login  to  it,  in  order  to  set  up  the  application  on  the 
vServer.  To  applications  that  don’t  use  the  NPEs,  this  process 
works  much  like  it  would  on  a  conventional  PlanetLab  node. 
However, there are some configuration steps that must be imple-
mented under the covers, to make this as seamless as possible.  
To allow slices to reserve externally visible port numbers, we 
provide  an  interface  to  the  LRM  that  relays  the  reservation  re-
quests to the GRM. The GRM keeps track of all externally visible 
port  numbers  that  have  been  assigned,  and  if  the  requested  port 
number  is  available,  it  makes  the  appropriate  assignment  and 
configures the Line Card so that when packets are received with 
the  specified  port  number,  they  will  be  forwarded  to  the  right 
GPE. To make this process transparent to the slices, the interface 
to the LRM is hidden inside library routines that are used in place 