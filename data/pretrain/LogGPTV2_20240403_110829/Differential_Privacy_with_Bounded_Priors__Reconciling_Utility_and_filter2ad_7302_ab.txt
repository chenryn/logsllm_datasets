Deﬁnition 4 (l1-sensitivity [5]). The l1-sensitivity of a
function f : 2U → Rn is ∆f = maxT1,T2 ||f (T1) − f (T2)||1,
where T1 and T2 are neighboring datasets.
Laplace Mechanism.
If the mechanism A produces outputs in Rn, the most
straightforward method to satisfy DP consists in perturbing
the output with noise drawn from the Laplace distribution.
Let A be a mechanism computing a function f : 2U → Rn.
Then, if on dataset T , A outputs f (T )+µ, where µ is drawn
from a Laplace distribution with mean 0 and scale ∆f
 , then
A satisﬁes -diﬀerential privacy [5].
Exponential Mechanism.
If A does not produce a numerical output, the addition
of noise usually does not make sense. A more general mech-
anism guaranteeing -DP consists in deﬁning a score func-
tion q : T × range(A) → R that assigns a value to each
input-output pair of A. On a dataset T , the exponential
mechanism samples an output r ∈ range(A) with probabil-
ity proportional to exp( q(T,r)
2∆q ), which guarantees -DP [17].
2.2 Positive Membership-Privacy
In this subsection, we give a review of the membership-
privacy framework from [14] and its relation to diﬀerential-
privacy. Readers familiar with this work can skip directly
to Section 3, where we introduce and discuss our relaxed
adversarial setting.
The original membership-privacy framework is comprised
of both positive and negative membership-privacy. In this
work, we are solely concerned with positive membership-
privacy (PMP). This notion protects against a type of re-
identiﬁcation attack called positive membership disclosure,
where the output of the mechanism A signiﬁcantly increases
an adversary’s belief that some entity belongs to the dataset.
Adversaries are characterized by their prior belief over the
contents of the dataset T . A mechanism A is said to satisfy
positive membership-privacy for a given prior distribution, if
after the adversary sees the output of A, its posterior belief
about an entity belonging to a dataset is not signiﬁcantly
larger than its prior belief.
Note that although diﬀerential privacy provides seemingly
strong privacy guarantees, it does not provide PMP for ad-
versaries with arbitrary prior beliefs. It is well known that
data privacy against arbitrary priors cannot be guaranteed if
some reasonable level of utility is to be achieved. This fact,
known as the no-free-lunch-theorem, was ﬁrst introduced by
Kifer and Machanavajjhala [11], and reformulated by Li et
al. [14] as part of their framework. We now give the formal
deﬁnition of γ-positive membership-privacy under a family
of prior distributions D, which we denote as (γ, D)-PMP.
Deﬁnition 5 (Positive Membership-Privacy [14]). A mech-
anism A satisﬁes γ-PMP under a distribution family D,
where γ ≥ 1, if and only if for any S ⊆ range(A), any
distribution D ∈ D, and any entity t ∈ U , we have
PrD|A [t ∈ T | A(T) ∈ S] ≤ γ PrD [t ∈ T]
PrD [t /∈ T] .
PrD|A [t /∈ T | A(T) ∈ S] ≥ 1
γ
(2)
(3)
By some abuse of notation, we denote by S the event
A(T) ∈ S and by t the event t ∈ T. When D and A are
obvious from context, we reformulate (2), (3) as
Pr [t | S] ≤ γ Pr [t]
Pr [¬t | S] ≥ 1
γ
Pr [¬t] .
Together, theses inequalities are equivalent to
γ − 1 + Pr[t]
Pr [t | S] ≤ min
γ Pr[t],
γ
(cid:18)
(4)
(5)
.
(6)
(cid:19)
The privacy parameter γ in PMP is somewhat analogous
to the parameter e in DP (we will see that the two pri-
vacy notions are equivalent for a particular family of prior
distributions). Note that the smaller γ is, the closer the
adversary’s posterior belief is to its prior belief, implying a
small knowledge gain. Thus, the strongest privacy guaran-
tees correspond to values of γ close to 1.
Having deﬁned positive membership-privacy, we now con-
sider eﬃcient methods to guarantee this notion of privacy,
for various distribution families. A simple suﬃcient condi-
tion on the output of the mechanism A, which implies PMP,
is given by Li et al. in the following lemma.
Lemma 1 ([14]). If for any distribution D ∈ D, any output
S ⊆ range(A) and any entity t for which 0 < PrD[t] < 1,
the mechanism A satisﬁes
Pr[S | t] ≤ γ · Pr[S | ¬t] ,
then A provides (γ, D)-PMP.
Notice the analogy to diﬀerential privacy here, in the sense
that the above condition ensures that the probabilities of A
producing an output, given the presence or absence of a
particularly data entry, should be close to each other.
Relation to Differential Privacy.
One of the main results of [14] shows that diﬀerential pri-
vacy is equivalent to PMP under a particular distribution
family. We will be primarily concerned with bounded DP,
as it is the privacy notion generally used for the genome-
wide association studies we consider in Section 4. Our main
results also apply to unbounded DP and we discuss this re-
lation in Section 5. Before presenting the main theorem
linking the two privacy notions, we introduce the necessary
distribution families.
Deﬁnition 6 (Mutually-Independent Distributions (MI) [14]).
The family DI contains all distributions characterized by as-
signing a probability pt to each entity t such that the proba-
bility of a dataset T is given by
Pr[T ] =
(1 − pt) .
(7)
(cid:89)
t∈T
pt ·(cid:89)
t /∈T
Deﬁnition 7 (Bounded MI Distributions (BMI) [14]). A
BMI distribution is the conditional distribution of a MI dis-
tribution, given that all datasets with non-zero probability
have the same size. The family DB contains all such distri-
butions.
The following result, used in the proof of Theorem 4.8 in
[14] will be useful when we consider relaxations of the family
DB in Section 3.
Lemma 2 ([14]). If A satisﬁes -bounded DP, then for any
D ∈ DB we have
Pr[S | t]
Pr[S | ¬t]
≤ e .
Note that together with Lemma 1, this result shows that -
bounded diﬀerential-privacy implies e-positive membership-
privacy under DB. Li et al. prove that the two notions are
actually equivalent.
Theorem 1 ([14]). A mechanism A satisﬁes -bounded DP
if and only if it satisﬁes (e, DB)-PMP.
This equivalence between -bounded DP and e-PMP un-
der DB will be the starting point of our relaxation of dif-
ferential privacy. Indeed, we will show that for certain sub-
families of DB, we can achieve e-PMP even if we only pro-
In this sense,
vide a weaker level of diﬀerential privacy.
we will provide a full characterization of the relationship
between the privacy budget of DP and the range of prior
beliefs for which we can achieve e-PMP.
3. PMP FOR BOUNDED PRIORS
The result of Theorem 1 provides us with a clear charac-
terization of positive membership-privacy under the family
DB. We now consider the problem of satisfying PMP for dif-
ferent distribution families. In particular, we are interested
in protecting our dataset against adversaries weaker than
those captured by DB, meaning adversaries with less back-
ground knowledge about the dataset’s contents. Indeed, as
the prior belief of adversaries considered by DP has been ar-
gued to be unreasonably strong for most practical settings,
our goal is to consider a restricted adversary, with a more
plausible level of background knowledge.
One reason to consider a weaker setting than DP’s adver-
sarial model, is that mechanisms that satisfy DP for small
values of  have been shown to provide rather disappointing
utility in practice. Examples of studies, where DP oﬀers a
poor privacy-utility tradeoﬀ, are numerous in medical appli-
cations such as genome-wide association studies [10, 19, 22]
or personalized medicine [6].
Indeed, many recent results
have shown that the amount of perturbation introduced by
appropriate levels of DP on such datasets renders most sta-
tistical queries useless. We will show that when considering
more reasonable adversarial settings, we can achieve strong
membership-privacy guarantees with less data perturbation,
thus leading to a possibly better privacy-utility tradeoﬀ.
3.1 A Relaxed Threat Model
As illustrated by Theorem 1, diﬀerential privacy guaran-
tees positive-membership privacy against adversaries with a
prior in DB. Thus, in the context of protection against mem-
bership disclosure, the threat model of diﬀerential privacy
considers adversaries with the following capabilities.
1. The adversary knows the size of the dataset N .
2. All entities are considered independent, conditioned on
the dataset having size N .
3. There are some entities for which the adversary knows
with absolute certainty whether they are in the dataset
or not (Pr[t] ∈ {0, 1}).
4. For all other entities, the adversary may have an arbi-
trary prior belief 0 < Pr[t] < 1 that the entity belongs
to the dataset.
In our threat model, we relax capability 4). We ﬁrst con-
sider each capability separately and discuss why it is (or is
not) a reasonable assumption for realistic adversaries.
Knowledge of N.
Bounded-DP inherently considers neighboring datasets of
ﬁxed size. It is preferably used in situations where the size
of the dataset is public and ﬁxed, an example being the
genome-wide association studies we discuss in Section 4. In
contrast, unbounded-DP is used in situations where the size
of the dataset is itself private. Our results apply in both
cases (see Section 5 for a discussion of unbounded-DP).
Independence of Entities.
As we have seen in Theorem 1 (and will see in Theo-
rem 3 for unbounded-DP), a diﬀerentially-private mecha-
nism guarantees that an adversary’s posterior belief will be
within a given multiplicative factor of its prior, exactly when
the adversary’s prior is a (bounded) mutually independent
distribution. In this work, we focus on a relaxation of DP
within the PMP framework, and thus model our adversary’s
prior belief as a subfamily of either DB or DI .
Known Entities.
It is reasonable to assume that an adversary may know
with certainty whether some entities belong to the dataset
or not, because these entities either willingly or unwillingly
disclosed their (non)-membership (the adversary itself may
be an entity of the universe). Note that for such entities
with prior 0 or 1, perfect PMP with γ = 1 is trivially satis-
ﬁed, since the adversary’s posterior does not diﬀer from its
prior. As the privacy of these entities is already breached
a priori, the privacy guarantees of A should be considered
only with respect to those entities whose privacy still can
be protected. Because all entities are considered indepen-
dent, we may assume that the adversary knows about some
entities’ presence in the dataset, but that some uncertainty
remains about others.
Unknown Entities.
A distribution D ∈ DB can assign to each uncertain entity
a prior probability arbitrarily close to 0 or 1. This means
that when providing positive membership-privacy under DB,
we are considering adversaries that might have an extremely
high prior conﬁdence about whether each user’s data is con-
tained in the dataset or not. In this sense, the family DB
corresponds to an extremely strong adversarial setting, as
it allows for adversaries with arbitrarily high prior beliefs
about the contents of a dataset.
Yet, while it is reasonable to assume that the adversary
may know for certain whether some entities are part of the
dataset or not, it seems unrealistic for an adversary to have
high conﬁdence about its belief for all entities, a priori. As
we will see, guaranteeing membership privacy for those en-
tities for which an adversary has high conﬁdence a priori
(Pr[t] close to 0 or 1), requires the most data perturbation.
Thus, when protecting against adversaries with priors in DB,
we are degrading our utility in favor of protection for enti-
ties whose membership privacy was already severely com-
promised to begin with.
In our alternative threat model,
we focus on protecting those entities whose presence in the
dataset remains highly uncertain to the adversary prior to
releasing the output of A. As we will see in Section 3.4, our
mechanisms still guarantee some weaker level of protection
against the full set of adversaries with priors in DB.
3.2 Our Results
Our natural relaxation of DP’s adversarial model con-
sists in restricting ourselves to adversaries with a prior belief
about uncertain entities bounded away from 0 and 1. Such
an adversary thus may know for certain whether some en-
tities are in the dataset or not, because they unwillingly or
willingly disclosed this information to the adversary. For the
remaining entities however, the adversary has some minimal
level of uncertainty about the entity’s presence or absence
from the dataset, which appears to be a reasonable assump-
tion to make in practice. We will consider the subfamily of
DB, consisting of all BMI distributions for which the priors
Pr[t] are either 0, 1 or bounded away from 0 and 1. This
distribution family is deﬁned as follows.
Deﬁnition 8 (Restricted1 BMI Distributions). For 0 < a ≤
B ⊂ DB contains all BMI distributions
b < 1, the family D[a,b]
for which Pr[t] ∈ [a, b] ∪ {0, 1}, for all entities t. If a = b,
we simply denote the family as Da
B.
Our goal is to show that in this weaker adversarial setting,
we can guarantee PMP with parameter γ, while satisfying a
weaker form of privacy than (ln γ)-DP.
We ﬁrst show that the adversaries with arbitrarily low
or high priors are, rather intuitively, the hardest to protect
against. More formally, we show that when guaranteeing
(γ, DB)-PMP, inequalities (2) and (3) are only tight for pri-
ors approaching 0 or 1. For each entity t, we can compute
a tight privacy parameter γ(t) ≤ γ, whose value depends
on the prior Pr[t]. When considering an adversary with a
prior belief in D[a,b]
B , we will see that γ(t) < γ for all en-
tities t, which shows that we can achieve tighter positive
membership-privacy guarantees in our relaxed adversarial
model. We formalize these results in the following lemma.
Lemma 3. If a mechanism A satisﬁes (γ, DB)-PMP, then
Pr [t | S] ≤ γ(t) · Pr[t] and Pr [¬t | S] ≥ Pr[¬t]
γ(t) , where
(cid:40)
γ(t) =
1
max{(γ−1) Pr[t]+1,
if Pr[t] ∈ {0, 1}
(γ−1) Pr[t]+1} otherwise.