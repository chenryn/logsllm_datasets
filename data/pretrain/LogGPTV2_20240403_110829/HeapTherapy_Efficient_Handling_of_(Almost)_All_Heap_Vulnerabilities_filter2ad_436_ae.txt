Integer benchmarks, and measured the execution time when
different encoding techniques were applied, normalized using
the execution time when no encoding is applied. Compared
to FCS (Full Call-Site Instrumentation) proposed in
[30],
which incurred 2.4% of slowdown for C/C++ programs, the
other three encoding algorithms proposed by us, that is, TCS
(Targeted Call-Site Instrumentation), Slim, and Incremental,
incurred 0.6%, 0.5%, and 0.4% of slowdown, receptively.
While the saved execution time itself is small, it gains up to
6x of speed up. We believe the proposed encoding algorithms
can have many applications far beyond memory protection;
plus, when they are applied to Java programs, where FCS may
incur more than 35% of overhead [32], the speed up due to
our algorithms could make a signiﬁcant difference.
As the encoding works by inserting instructions into the
programs, we also measured the program size increase. The
results are shown in Table III. While FCS increased the
binary size by an average of 12% when compared to the
uninstrumented binaries, TCS, Slim and Incremental incurred
only 6%, 4.5%, and 4.4% of size increase, respectively.
2) Efﬁciency of HEAPTHERAPY+: To evaluate the run-
time overhead, we ran our system on both SPEC CPU2006
Integer benchmarks and a set of real-world service programs.
SPEC CPU2006. The
by
overhead
speed
incurred
538
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:53:42 UTC from IEEE Xplore.  Restrictions apply. 
SPEC CPU2006 BENCHMARK PROGRAM SIZE INCREASE, IN
PERCENTAGE, DUE TO DIFFERENT ENCODING ALGORITHMS.
TABLE III
Benachmark
400.perlbench
401.bzip2
403.gcc
429.mcf
445.gobmk
456.hmmer
458.sjeng
462.libquantum
464.h264ref
471.omnetpp
473.astar
483.xalancbmk
FCS(%)
19.6
8.8
18.6
0.53
4.8
18.9
10.6
15
8.3
15.8
7.0
14.5
TCS(%)
16.2
0.12
14.7
0.53
3.2
5.9
0.08
7.7
3.6
7.2
7.0
4.1
Slim(%)
15.9
0.12
13.6
0.53
2.5
2.4
0.08
7.7
1.8
6.7
0.2
3.8
Incremental(%)
15.9
0.12
13.6
0.53
2.5
1.2
0.08
7.7
1.8
6.7
0.2
3.8
HEAPTHERAPY+ can be divided into four parts:
(1)
overhead due to instrumentation, which has been presented
above; (2) overhead due to interposition of heap memory
allocation calls; (3) overhead due to maintaining the meta
data of each buffer (such that our system does not rely on the
internal details of the underlying allocator); (4) overhead due
to patch deployment, which causes the security measures to
be applied to vulnerable buffers.
In order to measure the overhead incurred due to patch
deployment, we select a set of allocation-time CCIDs (Calling-
Context IDs) as hypothesized vulnerable ones as follows. First,
for each benchmark program, we rank all of its allocation-
time CCIDs according to their frequencies during the proﬁling
execution (that is, how many heap buffers have been allo-
cated under that calling context). Next, we pick the CCIDs
with median frequencies as the hypothesized vulnerable ones.
Finally, we regard the heap buffers with those allocation-
time CCIDs as ones vulnerable to overﬂows (the other two
vulnerability types are much less expensive to treat), and
generate corresponding patches for them.
Figure 8 shows the measurement results. The overhead due
to interposition is 1.9%, and the overhead for maintaining the
buffer metadata (excluding the interposition overhead 1.9%
and calling context encoding overhead 0.4%) is 2.0%. Note
that the two parts of overhead can be largely eliminated if
our system is integrated into the underlying heap allocator.
When zero patch is installed, the overhead is 4.3%. When one
patch is installed, the overhead increases by only 0.4% (and
reaches 4.7%). The total overhead is 5.2% when ﬁve patches
are installed. One outlier is 400.perlbench, which has the
most intensive heap allocations. Table IV records the heap
allocation statistics for each SPEC CPU2006 benchmark.
We also measured the memory consumption of benchmark
programs, and used a script that can compute the memory
overhead in terms of the average Resident Set Size (RSS)
for the benchmark programs. That script reads the VmRSS
ﬁeld of /proc/[pid]/status. The sampling rate is 30
times per second, and the average of the readings is reported.
Figure 9 shows the memory consumption overhead normalized
SPEC CPU2006 BENCHMARK HEAP ALLOCATION STATISTICS.
TABLE IV
Benachmark
400.perlbench
401.bzip2
403.gcc
429.mcf
445.gobmk
456.hmmer
458.sjeng
462.libquantum
464.h264ref
471.omnetpp
473.astar
483.xalancbmk
malloc
346,405,116
174
23,690,559
5
606,463
1,983,014
5
1
7,270
267,064,936
4,799,959
135,155,553
calloc
0
0
4,723,237
3
0
122,564
0
121
170,518
0
0
0
realloc
11,736,402
0
44,688
0
52,115
368,696
0
58
0
0
0
0
over native program execution, and the average overhead is
only 4.3%. The overhead is due to the metadata (e.g., buffer
size) our systems maintains for each buffer, and can be largely
eliminated if our system is integrated into the underlying heap
allocator. Note that guard pages themselves do not increase the
use of memory, since they are virtual pages.
Service Programs. We also evaluated our system on two
popular service programs: Nginx and MySQL. We used
Nginx 1.2, and measured the throughput overhead by sending
requests using Apache Benchmark. Different numbers of
concurrent requests from 20 to 200 were used, and the
throughput was compared with that of native execution. The
average throughput overhead is only 4.2%. We used MySQL
5.5.9 and its built-in script mysql-stress-test.pl to
measure the throughput overhead. There was no observable
throughput overhead. The memory overhead in both cases was
negligible. Note that the memory overhead is proportional with
the number of live buffers.
Comparison with State of the Art. MemorySanitizer [20]
detects uninitialized read only at an average of 2.5x slow-
down on SPEC CPU2006. AddressSanitizer [8] detects over-
ﬂows and use-after-free only, and the average slowdown on
CPU2006 is 73% and the memory overhead is 3.37x. They
make use of shadow memory for online detection, while our
work uses it for ofﬂine analysis only; they detect attacks on
both heaps and call stacks. Exterminator (see Section II-C)
incurs only 7.2% slowdown on CPU2006, but it generates the
heap buffer overﬂow defense probabilistically over multiple
runs. Unlike Exterminator, HeapTherapy [19] deterministically
generates the overﬂow defense in a single run, and it is the
ﬁrst work that introduces calling context encoding to memory
defense generation, but it cannot handle uninitialized read and
use-after-free. Cruiser [10] uses a concurrent thread to scan the
heap integrity and achieves very high efﬁciency (5% slowdown
on CPU2006), but it can only detect overwrites.
Summary. The evaluation shows that HEAPTHERAPY+ is
effective but efﬁcient. It generates defenses for a variety
of heap vulnerabilities in a deterministic way. On SPEC
CPU2006, the most optimized calling context encoding incurs
only 0.4% of slowdown, a 6 times of speed boost compared
to PCC [30]. When ﬁve patches are installed, the total speed
539
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:53:42 UTC from IEEE Xplore.  Restrictions apply. 
(cid:35)(cid:31)(cid:38)(cid:1)
(cid:35)(cid:31)(cid:36)(cid:1)
(cid:35)(cid:1)
(cid:34)(cid:31)(cid:42)(cid:1)
(cid:34)(cid:31)(cid:40)(cid:1)
(cid:34)(cid:31)(cid:38)(cid:1)
(cid:34)(cid:31)(cid:36)(cid:1)
(cid:34)(cid:1)
(cid:5)(cid:21)(cid:27)(cid:12)(cid:25)(cid:23)(cid:22)(cid:26)(cid:16)(cid:27)(cid:16)(cid:22)(cid:21)(cid:1)
(cid:34)(cid:1)(cid:23)(cid:9)(cid:27)(cid:11)(cid:15)(cid:1)
(cid:35)(cid:1)(cid:23)(cid:9)(cid:27)(cid:11)(cid:15)(cid:1)
(cid:39)(cid:1)(cid:23)(cid:9)(cid:27)(cid:11)(cid:15)(cid:12)(cid:26)(cid:1)
(cid:38)(cid:34)(cid:34)(cid:31)(cid:23)(cid:12)(cid:25)(cid:19)(cid:10)(cid:12)(cid:21)(cid:11)(cid:15)(cid:1)
(cid:38)(cid:34)(cid:35)(cid:31)(cid:10)(cid:30)(cid:16)(cid:23)(cid:36)(cid:1)
(cid:38)(cid:34)(cid:37)(cid:31)(cid:14)(cid:11)(cid:11)(cid:1)
(cid:38)(cid:36)(cid:43)(cid:31)(cid:20)(cid:11)(cid:13)(cid:1)
(cid:38)(cid:38)(cid:39)(cid:31)(cid:14)(cid:22)(cid:10)(cid:20)(cid:18)(cid:1)
(cid:38)(cid:39)(cid:40)(cid:31)(cid:15)(cid:20)(cid:20)(cid:12)(cid:25)(cid:1)
(cid:38)(cid:39)(cid:42)(cid:31)(cid:26)(cid:17)(cid:12)(cid:21)(cid:14)(cid:1)
(cid:38)(cid:40)(cid:36)(cid:31)(cid:19)(cid:16)(cid:10)(cid:24)(cid:28)(cid:9)(cid:21)(cid:27)(cid:28)(cid:20)(cid:1) (cid:38)(cid:40)(cid:38)(cid:31)(cid:15)(cid:36)(cid:40)(cid:38)(cid:25)(cid:12)(cid:13)(cid:1)
(cid:38)(cid:41)(cid:35)(cid:31)(cid:22)(cid:20)(cid:21)(cid:12)(cid:27)(cid:23)(cid:23)(cid:1)
(cid:38)(cid:41)(cid:37)(cid:31)(cid:9)(cid:26)(cid:27)(cid:9)(cid:25)(cid:1)
(cid:38)(cid:42)(cid:37)(cid:31)(cid:29)(cid:9)(cid:19)(cid:9)(cid:21)(cid:11)(cid:10)(cid:20)(cid:18)(cid:1) (cid:4)(cid:3)(cid:8)(cid:6)(cid:3)(cid:2)(cid:7)(cid:1)
Fig. 8. Normalized execution time overhead imposed to SPEC CPU2006 benchmarks (smaller is better), when only interposition is applied (1.9%), no patch
is installed (4.3%), one patch is installed (4.7%), and ﬁve patches are installed (5.2%), respectively.
(cid:34)(cid:30)(cid:35)(cid:1)
(cid:34)(cid:30)(cid:34)(cid:1)
(cid:34)(cid:1)
(cid:33)(cid:30)(cid:42)(cid:1)
(cid:33)(cid:30)(cid:41)(cid:1)
(cid:33)(cid:30)(cid:40)(cid:1)
(cid:33)(cid:30)(cid:39)(cid:1)
(cid:33)(cid:30)(cid:38)(cid:1)
(cid:37)(cid:33)(cid:33)(cid:30)(cid:22)(cid:11)(cid:24)(cid:18)(cid:9)(cid:11)(cid:20)(cid:10)(cid:14)(cid:1)
(cid:37)(cid:33)(cid:34)(cid:30)(cid:9)(cid:29)(cid:15)(cid:22)(cid:35)(cid:1)
(cid:37)(cid:33)(cid:36)(cid:30)(cid:13)(cid:10)(cid:10)(cid:1)
(cid:37)(cid:35)(cid:42)(cid:30)(cid:19)(cid:10)(cid:12)(cid:1)
(cid:37)(cid:37)(cid:38)(cid:30)(cid:13)(cid:21)(cid:9)(cid:19)(cid:17)(cid:1)
(cid:37)(cid:38)(cid:39)(cid:30)(cid:14)(cid:19) (cid:19)(cid:11)(cid:24)(cid:1)
(cid:37)(cid:38)(cid:41)(cid:30)(cid:25)(cid:16)(cid:11)(cid:20)(cid:13)(cid:1)
(cid:37)(cid:39)(cid:35)(cid:30)(cid:18)(cid:15)(cid:9)(cid:23)(cid:27)(cid:8)(cid:20)(cid:26)(cid:27)(cid:19)(cid:1)
(cid:37)(cid:39)(cid:37)(cid:30)(cid:14)(cid:35)(cid:39)(cid:37)(cid:24)(cid:11)(cid:12)(cid:1)
(cid:37)(cid:40)(cid:34)(cid:30)(cid:21)(cid:19)(cid:20)(cid:11)(cid:26)(cid:22)(cid:22)(cid:1)
(cid:37)(cid:41)(cid:36)(cid:30)(cid:28)(cid:8)(cid:18)(cid:8)(cid:20)(cid:10)(cid:9)(cid:19)(cid:17)(cid:1)
(cid:37)(cid:40)(cid:36)(cid:30)(cid:8)(cid:25)(cid:26)(cid:8)(cid:24)(cid:1)
(cid:4)(cid:3)(cid:7) (cid:5)(cid:3)(cid:2)(cid:6)(cid:1)
Fig. 9. Normalized memory overhead imposed to SPEC CPU2006 bench-