### Increased Operational Costs and the Need for Innovative Strategies

The implementation of such measures comes with an increased operational cost. Therefore, there is a need for innovative strategies that can initiate low-cost recovery actions based on failure prediction, without incurring significant overhead.

### Related Work

In the preceding sections, we have discussed how our findings enhance the current understanding compared to existing works. In this section, we provide a more detailed overview of related work.

#### Failure Characterization and Analysis

Many prior studies have focused on characterizing the fault tolerance and resiliency characteristics of data centers and supercomputers from the perspective of CPUs, GPUs, memory, interconnect networks, and storage systems [1], [8]–[15], [27]–[29]. For instance, Gupta et al. [9] characterized multiple HPC systems with different components, targeting their reliability. More recently, Kumar et al. [7] analyzed failures in multiple academic supercomputing clusters and used machine learning to predict resource usage.

#### Overview of Operational Practices

State-of-the-practice works have highlighted high-level methods and approaches employed by large-scale systems to reduce failure rates and/or mitigate their effects [7], [30]–[33]. For example, faults can propagate and result in different failures across subsystems. Pecchia et al. [30] proposed a method to accurately classify different error entries in a failure log based on their causality relation to a known fault.

#### Generalizability and Usability to Other Systems

The findings of this study will become increasingly relevant as newer supercomputers employ multiple GPUs on the same node (e.g., Summit, Sierra, and Juwels) and host multi-generational HPC systems (e.g., NASA supercomputing center, TACC, and Ohio State HPC center). We found that, similar to single-GPU-per-node systems, the non-uniform distribution of failures among racks is also present in multi-GPU-per-node systems and can become particularly challenging. Our insights into spatial and temporal distribution could be used to design proactive mitigation strategies, such as spare provisioning, checkpointing, and scheduling [34]–[41].

### Concluding Remarks

We performed a comprehensive characterization of system failures over two generations of GPU-dominated HPC systems, with a new focus on the time it takes to recover from a failure. Some of our novel findings include that software and GPU failures are the most frequent, the failure rates vary for different GPUs, and the recovery time is not only failure-dependent but also varies monthly.

### Acknowledgments

We are grateful to GSIC, Tokyo Institute of Technology, for providing the dataset. We would like to thank Adwait Jog (our shepherd) and anonymous reviewers for their constructive feedback. This work is supported by NSF Award 1910601 and 1753840, and prepared by LLNL under Contract DE-AC52-07NA27344 (LLNL-CONF-820342). The views and opinions of the authors do not necessarily reflect those of the U.S. government or Lawrence Livermore National Security, LLC, neither of whom nor any of their employees make any endorsements, express or implied warranties or representations, or assume any legal liability or responsibility for the accuracy, completeness, or usefulness of the information contained herein.

Authorized licensed use limited to: Tsinghua University. Downloaded on October 11, 2021, at 09:22:59 UTC from IEEE Xplore. Restrictions apply.