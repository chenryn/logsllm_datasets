this comes at an increased operational cost. There is
a need for innovative strategies that can initiate low-
cost recovery actions based on failure prediction without
much overhead.
IV. RELATED WORK
In earlier sections, we discussed how our ﬁndings improve
our current understanding compared to existing works. In this
section, we discuss additional related work.
Failure Characterization and Analysis. Many of the prior
works have focused on characterizing the fault tolerance and
resiliency characteristics of data centers and supercomputers
from the perspective of CPUs, GPUs, memory, interconnect
network, and storage system [1], [8]–[15], [27]–[29]. For
example, Gupta et al. [9] have characterized multiple HPC
systems with different components, targeting their reliability.
Most recently, Kumar et al. [7] analyzed the failures on
multiple academic supercomputing clusters and used machine
learning to predict resource usage.
Overview of Operational Practices. On the other hand,
state-of-practice works have highlighted the high-level meth-
ods and approaches employed by large-scale systems to reduce
failure rates and/or mitigate their effects [7], [30]–[33]. As an
instance, faults can propagate and result in different failures
across sub-systems. Pecchia et al. [30] propose a method to
accurately classify different error entries in a failure log based
on their causality relation to a known fault.
Generalizability and Usability to Other Systems. The
ﬁndings of this study will become increasingly relevant as
newer supercomputers are employing multiple GPUs on the
same node (e.g., Summit, Sierra, and Juwels) and host multi-
generational HPC systems (e.g., NASA supercomputing cen-
ter, TACC, and Ohio State HPC center). We found that similar
to single-GPU-per-node systems, the non-uniform distribution
of failures among racks is also present in multi-GPU-per-node
systems and can become particularly challenging. Our spatial
and temporal distribution insights could be used to design
proactive mitigation strategies (such as spare-provisioning,
checkpointing, and scheduling [34]–[41]).
V. CONCLUDING REMARKS
We performed the characterization of system failures over
two generations of GPU-dominated HPC systems, with a new
focus on the time it takes to recover from a failure. Some
of our novel ﬁndings include that software and GPU failures
are the most frequent out of all failure types, the failure rates
vary for different GPUs, and that the recovery time is not only
failure-dependent but also varies monthly.
Acknowledgment We are thankful to GSIC, Tokyo Institute of
Technology for providing the dataset. We would like to thank Adwait
Jog (our shepherd) and anonymous reviewers for their construc-
tive feedback. This work is supported by NSF Award 1910601
and 1753840, and prepared by LLNL under Contract DE-AC52-
07NA27344 (LLNL-CONF-820342).The views and opinions of the
authors do not necessarily reﬂect
those of the U.S. government
or Lawrence Livermore National Security, LLC neither of whom
nor any of their employees make any endorsements, express or
implied warranties or representations or assume any legal liability
or responsibility for the accuracy, completeness, or usefulness of the
information contained herein.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:59 UTC from IEEE Xplore.  Restrictions apply. 
311
REFERENCES
[1] K. Lee, M. B. Sullivan, S. K. S. Hari, T. Tsai, S. W. Keckler,
and M. Erez, “On the trend of resilience for gpu-dense systems,” in
2019 49th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks–Supplemental Volume (DSN-S).
IEEE, 2019,
pp. 29–34.
[2] P. Rech, L. Carro, N. Wang, T. Tsai, S. K. S. Hari, and S. W. Keckler,
“Measuring the radiation reliability of sram structures in gpus designed
for hpc,” in IEEE 10th Workshop on Silicon Errors in Logic-System
Effects (SELSE), 2014.
[3] A. Mahmoud, S. K. S. Hari, M. B. Sullivan, T. Tsai, and S. W. Keck-
ler, “Optimizing software-directed instruction replication for gpu error
detection,” in SC18: International Conference for High Performance
Computing, Networking, Storage and Analysis.
IEEE, 2018, pp. 842–
853.
[4] K. Lee, “Resilient heterogeneous systems with containment domains,”
Ph.D. dissertation, 2020.
[5] L. Yang, B. Nie, A. Jog, and E. Smirni, “Practical resilience analysis of
gpgpu applications in the presence of single-and multi-bit faults,” IEEE
Transactions on Computers, 2020.
[6] B. Nie, A. Jog, and E. Smirni, “Characterizing accuracy-aware resilience
of gpgpu applications,” in 2020 20th IEEE/ACM International Sympo-
sium on Cluster, Cloud and Internet Computing (CCGRID).
IEEE,
2020, pp. 111–120.
[7] R. Kumar, S. Jha, A. Mahgoub, R. Kalyanam, S. Harrell, X. C. Song,
Z. Kalbarczyk, W. Kramer, R. Iyer, and S. Bagchi, “The mystery of
the failing jobs: Insights from operational data from two university-
wide computing systems,” in 2020 50th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks (DSN).
IEEE, 2020,
pp. 158–171.
[8] D. Tiwari, S. Gupta, J. Rogers, D. Maxwell, P. Rech, S. Vazhkudai,
D. Oliveira, D. Londo, N. DeBardeleben, P. Navaux, L. Carro, and
A. Bland, “Understanding gpu errors on large-scale hpc systems and
the implications for system design and operation,” in 2015 IEEE 21st
International Symposium on High Performance Computer Architecture
(HPCA), 2015, pp. 331–342.
[9] S. Gupta, T. Patel, C. Engelmann, and D. Tiwari, “Failures in large
scale systems: long-term measurement, analysis, and implications,” in
Proceedings of
the International Conference for High Performance
Computing, Networking, Storage and Analysis, 2017, pp. 1–12.
[10] D. Tiwari, S. Gupta, G. Gallarno, J. Rogers, and D. Maxwell, “Reliability
lessons learned from gpu experience with the titan supercomputer at
oak ridge leadership computing facility,” in SC’15: Proceedings of the
International Conference for High Performance Computing, Networking,
Storage and Analysis.
IEEE, 2015, pp. 1–12.
[11] G. Ostrouchov, D. Maxwell, R. Ashraf, C. Engelmann, M. Shankar,
and J. Rogers, “Gpu lifetimes on titan supercomputer: Survival analysis
and reliability,” in Proceedings of the International Conference on High
Performance Computing, Networking, Storage and Analysis (SC) 2020,
2020, pp. 15–20.
[12] B. Nie, J. Xue, S. Gupta, T. Patel, C. Engelmann, E. Smirni, and
D. Tiwari, “Machine learning models for gpu error prediction in a
large scale hpc system,” in 2018 48th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks (DSN).
IEEE, 2018,
pp. 95–106.
[13] B. Nie, D. Tiwari, S. Gupta, E. Smirni, and J. H. Rogers, “A large-
scale study of soft-errors on gpus in the ﬁeld,” in 2016 IEEE Interna-
tional Symposium on High Performance Computer Architecture (HPCA).
IEEE, 2016, pp. 519–530.
[14] L. B. Gomez, F. Cappello, L. Carro, N. DeBardeleben, B. Fang,
S. Gurumurthi, K. Pattabiraman, P. Rech, and M. S. Reorda, “Gpgpus:
How to combine high computational power with high reliability,” in
2014 Design, Automation & Test in Europe Conference & Exhibition
(DATE).
IEEE, 2014, pp. 1–9.
[15] C. Di Martino, Z. Kalbarczyk, R. K. Iyer, F. Baccanico, J. Fullop, and
W. Kramer, “Lessons learned from the analysis of system failures at
petascale: The case of blue waters,” in 2014 44th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks.
IEEE,
2014, pp. 610–621.
[16] Tokyo Institute of Technology. Tsubame hardware-software speciﬁca-
tion. [Online]. Available: https://www.gsic.titech.ac.jp/en/node/420
[17] N. Onodera and Y. Idomura, “Acceleration of wind simulation using
locally mesh-reﬁned lattice boltzmann method on gpu-rich supercom-
puters,” in Asian Conference on Supercomputing Frontiers.
2018, pp. 128–145.
Springer,
[18] A. Nukada, K. Sato, and S. Matsuoka, “Scalable multi-gpu 3-d fft for
tsubame 2.0 supercomputer,” in SC’12: Proceedings of the International
Conference on High Performance Computing, Networking, Storage and
Analysis.
IEEE, 2012, pp. 1–10.
[19] K. Lee, M. B. Sullivan, S. K. S. Hari, T. Tsai, S. W. Keckler, and
M. Erez, “Gpu snapshot: checkpoint ofﬂoading for gpu-dense systems,”
in Proceedings of the ACM International Conference on Supercomput-
ing, 2019, pp. 171–183.
[20] B. Pourghassemi and A. Chandramowlishwaran, “cudacr: An in-kernel
application-level checkpoint/restart scheme for cuda-enabled gpus,” in
2017 IEEE International Conference on Cluster Computing (CLUSTER).
IEEE, 2017, pp. 725–732.
[21] A. R. Anwer, G. Li, K. Pattabiraman, M. Sullivan, T. Tsai, and S. Hari,
“Gpu-trident: efﬁcient modeling of error propagation in gpu programs,”
in Proceedings of the International Conference for High Performance
Computing, Networking, Storage and Analysis, 2020, pp. 1–15.
[22] R. Garg, A. Mohan, M. Sullivan, and G. Cooperman, “Crum:
Checkpoint-restart support for cuda’s uniﬁed memory,” in 2018 IEEE
International Conference on Cluster Computing (CLUSTER).
IEEE,
2018, pp. 302–313.
[23] R. Garg, G. Price, and G. Cooperman, “Mana for mpi: Mpi-agnostic
network-agnostic transparent checkpointing,” in Proceedings of the 28th
International Symposium on High-Performance Parallel and Distributed
Computing, 2019, pp. 49–60.
[24] J. Wells, B. Bland, J. Nichols, J. Hack, F. Foertter, G. Hagen, T. Maier,
M. Ashfaq, B. Messer, and S. Parete-Koon, “Announcing supercomputer
summit,” Oak Ridge National Lab.(ORNL), Oak Ridge, TN (United
States), Tech. Rep., 2016.
[25] J. A. Kahle, J. Moreno, and D. Dreps, “2.1 summit and sierra: Designing
ai/hpc supercomputers,” in 2019 IEEE International Solid-State Circuits
Conference-(ISSCC).
IEEE, 2019, pp. 42–43.
[26] D. Das, M. Schiewe, E. Brighton, M. Fuller, T. Cerny, M. Bures,
K. Frajtak, D. Shin, and P. Tisnovsky, “Failure prediction by utilizing
log analysis: A systematic mapping study,” in Proceedings of the Inter-
national Conference on Research in Adaptive and Convergent Systems,
2020, pp. 188–195.
[27] M. Kumar, S. Gupta, T. Patel, M. Wilder, W. Shi, S. Fu, C. Engelmann,
and D. Tiwari, “Understanding and analyzing interconnect errors and
network congestion on a large scale hpc system,” in 2018 48th Annual
IEEE/IFIP International Conference on Dependable Systems and Net-
works (DSN).
IEEE, 2018, pp. 107–114.
[28] G. Wang, L. Zhang, and W. Xu, “What can we learn from four years
of data center hardware failures?” in 2017 47th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN).
IEEE, 2017, pp. 25–36.
[29] S. Di, H. Guo, E. Pershey, M. Snir, and F. Cappello, “Characterizing and
understanding hpc job failures over the 2k-day life of ibm bluegene/q
system,” in 2019 49th Annual IEEE/IFIP International Conference on
Dependable Systems and Networks (DSN).
IEEE, 2019, pp. 473–484.
[30] A. Pecchia, D. Cotroneo, Z. Kalbarczyk, and R. K. Iyer, “Improving log-
based ﬁeld failure data analysis of multi-node computing systems,” in
2011 IEEE/IFIP 41st International Conference on Dependable Systems
& Networks (DSN).
IEEE, 2011, pp. 97–108.
[31] G. Li, K. Pattabiraman, S. K. S. Hari, M. Sullivan, and T. Tsai, “Model-
ing soft-error propagation in programs,” in 2018 48th Annual IEEE/IFIP
International Conference on Dependable Systems and Networks (DSN).
IEEE, 2018, pp. 27–38.
[32] E. Tremel, S. Jha, W. Song, D. Chu, and K. Birman, “Reliable, efﬁcient
recovery for complex services with replicated subsystems,” in 2020 50th
Annual IEEE/IFIP International Conference on Dependable Systems and
Networks (DSN).
IEEE, 2020, pp. 172–183.
[33] V. Fratin, D. Oliveira, C. Lunardi, F. Santos, G. Rodrigues, and P. Rech,
“Code-dependent and architecture-dependent reliability behaviors,” in
2018 48th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN).
IEEE, 2018, pp. 13–26.
[34] D. Tiwari, S. Gupta, and S. S. Vazhkudai, “Lazy checkpointing: Exploit-
ing temporal locality in failures to mitigate checkpointing overheads on
extreme-scale systems,” in 2014 44th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks.
IEEE, 2014, pp.
25–36.
[35] S. Gupta, D. Tiwari, C. Jantzi, J. Rogers, and D. Maxwell, “Understand-
ing and exploiting spatial properties of system failures on extreme-scale
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:59 UTC from IEEE Xplore.  Restrictions apply. 
312
hpc systems,” in 2015 45th Annual IEEE/IFIP International Conference
on Dependable Systems and Networks.
IEEE, 2015, pp. 37–44.
[36] L. Bautista-Gomez, A. Gainaru, S. Perarnau, D. Tiwari, S. Gupta, C. En-
gelmann, F. Cappello, and M. Snir, “Reducing waste in extreme scale
systems through introspective analysis,” in 2016 IEEE International
Parallel and Distributed Processing Symposium (IPDPS).
IEEE, 2016,
pp. 212–221.
[37] Q. Liu, C. Jung, D. Lee, and D. Tiwari, “Compiler-directed lightweight
checkpointing for ﬁne-grained guaranteed soft error recovery,” in SC’16:
the International Conference for High Performance
Proceedings of
Computing, Networking, Storage and Analysis.
IEEE, 2016, pp. 228–
239.
[38] K. Tang, D. Tiwari, S. Gupta, P. Huang, Q. Lu, C. Engelmann, and
X. He, “Power-capping aware checkpointing: On the interplay among
power-capping, temperature, reliability, performance, and energy,” in
2016 46th Annual IEEE/IFIP International Conference on Dependable
Systems and Networks (DSN).
IEEE, 2016, pp. 311–322.
[39] R. Garg, T. Patel, G. Cooperman, and D. Tiwari, “Shiraz: Exploiting sys-
tem reliability and application resilience characteristics to improve large
scale system throughput,” in 2018 48th Annual IEEE/IFIP International
Conference on Dependable Systems and Networks (DSN).
IEEE, 2018,
pp. 83–94.
[40] R. Basu Roy, T. Patel, R. Kettimuthu, P. Richa, A. Scovel, B. Allcock,
and D. Tiwari, “Operating liquid-cooled large-scale systems: Long-term
monitoring, reliability analysis, and efﬁciency measures,” in 2021 IEEE
International Symposium on High Performance Computer Architecture
(HPCA).
IEEE, 2021.
[41] L. Wan, F. Wang, S. Oral, D. Tiwari, S. S. Vazhkudai, and Q. Cao, “A
practical approach to reconciling availability, performance, and capacity
in provisioning extreme-scale storage systems,” in Proceedings of the
International Conference for High Performance Computing, Networking,
Storage and Analysis, 2015, pp. 1–12.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:59 UTC from IEEE Xplore.  Restrictions apply. 
313