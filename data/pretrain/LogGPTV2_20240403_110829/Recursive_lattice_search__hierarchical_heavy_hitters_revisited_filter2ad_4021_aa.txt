title:Recursive lattice search: hierarchical heavy hitters revisited
author:Kenjiro Cho
Recursive Lattice Search:
Hierarchical Heavy Hitters Revisited
Kenjiro Cho
IIJ Research Laboratory
Tokyo, Japan
PI:EMAIL
ABSTRACT
The multidimensional Hierarchical Heavy Hitter (HHH) problem
identifies significant clusters in traffic across multiple planes such as
source and destination addresses, and has been widely studied in the
literature. A compact summary of HHHs provides an overview on
complex traffic behavior and is a powerful means for traffic monitor-
ing and anomaly detection. In this paper, we present a new efficient
HHH algorithm which fits operational needs. Our key insight is to
revisit the commonly accepted definition of HHH, and apply the
Z-ordering to make use of a recursive partitioning algorithm. The
proposed algorithm produces summary outputs comparable to or
even better in practice than the existing algorithms, and runs orders
of magnitude faster for bitwise aggregation. We have implemented
the algorithm into our open-source tool and have made longitudinal
datasets of backbone traffic openly available.
CCS CONCEPTS
• Networks → Network monitoring; Packet classification;
KEYWORDS
hierarchical heavy hitters, flow aggregation algorithm, Z-order
ACM Reference Format:
Kenjiro Cho. 2017. Recursive Lattice Search: Hierarchical Heavy Hitters
Revisited. In Proceedings of IMC ’17, London, United Kingdom, November 1–3,
2017, 7 pages.
https://doi.org/10.1145/3131365.3131377
1 INTRODUCTION
Network-wide activities are often involved with many individual
flows, and better presented by means of aggregated flows by their
5-tuple attributes. Identifying significant flow aggregates in traffic,
known as the Hierarchical Heavy Hitter (HHH) problem, provides
a powerful means for traffic monitoring as well as valuable compo-
nents for anomaly detection to identify attacks and scans.
Algorithms for finding HHHs have been extensively studied in
the literature. Nonetheless, they are not satisfactory for practical
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
IMC ’17, November 1–3, 2017, London, United Kingdom
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-5118-8/17/11...$15.00
https://doi.org/10.1145/3131365.3131377
applications based on our experience developing and using HHH-
based tools over the years [3, 12]. First, the performance is not good
enough for multidimensional bitwise aggregation. Second, theoreti-
cal HHHs are not always relevant to operational needs, as reported
HHHs include many broad and redundant ones (e.g., ‘128.0.0.0/4’
and ‘128.0.0.0/2’ are overlapping broad subnets). Third, we found
it highly useful for interactive analysis to re-aggregate results for
a coarser result (e.g., producing a daily result from 5-minute-long
results) but most methods do not consider re-aggregation.
In this paper, we revisit the multi-dimensional HHH problem
by introducing a new definition of HHH. Contrary to common
practice, we apply the Z-ordering [16] to HHH so as to use an
efficient recursive partitioning algorithm. Our contributions are
(1) the proposed efficient algorithm for bitwise aggregation that
matches operational needs and supports re-aggregation, and (2)
the open-source traffic monitoring tool and the open dataset for
the community. More broadly, the key contribution is to transform
the existing hard problem into a tractable one by revisiting the
commonly accepted definition.
2 BACKGROUND AND RELATED WORK
IP addresses are hierarchical and a node in the hierarchy represents
an address range or a specific subnet. A node and its associated
counts (e.g., packets or bytes) can be aggregated to a more generic
(superset) node in the hierarchy, where they are called ‘descendant’
and ‘ancestor’. An HHH for a total count N and a threshold ϕ ∈
(0, 1) is an aggregate with count c ≥ ϕN .
Multi-dimensional aggregation has a rich history in database
research (e.g., iceberg-cubes [2]). In networking, unidimensional
HHH was introduced in early 2000s [3, 6, 8]. Extending HHH to
multiple dimensions was considered by Estan et al. [8], and then,
more formally explored by Cormode et al. [7].
In one-dimension, each node in the hierarchy has only one par-
ent node, and HHHs can be uniquely determined by depth-first
traversal: aggregating small nodes from lower positions in the hi-
erarchy until the count of an aggregate exceeds the threshold. In
n-dimensions, however, each node has n-parent nodes and there
are many possible ways to aggregate. As a result, identifying multi-
dimensional HHHs is much harder than unidimensional HHHs.
For simplicity, we focus on 2-dimensional HHH, using IPv4
source-destination address pairs in this paper. We use [l0, l1] to
denote a prefix length pair, (p0/l0, p1/l1) for a prefix pair, and ‘∗’ to
represent wildcards (‘0’ for prefix length, ‘0/0’ for prefix).
When a l-bit space has д-bit granularity, it is divided into h =
l/д subspaces by h′ = h + 1 hyperplanes (including ones at both
ends). For IPv4 addresses (l = 32) with 8 bit granularity (д = 8),
h = 4 and h′ = 5. Possible aggregations with different prefix length
IMC ’17, November 1–3, 2017, London, United Kingdom
Kenjiro Cho
to both parents in order to identify all possible HHHs. The latter
preserves the counts and splits a count between its parents using
some split function (e.g., first found, even split). The overlap rule
produces more HHHs than the split rule; for a threshold ϕ, the
number of discounted HHHs is at most 1/ϕ for the split rule but
A/ϕ for the overlap rule where A denotes the length of the longest
width of the lattice (e.g., A = h′ = 33 for д = 1) [5].
Our requirement is to preserve counts because our tool is de-
signed to re-aggregate outputs to produce coarser grained out-
puts and for interactive analysis, and double-counting distorts re-
aggregated results. Thus, we use a simple split rule that rolls up
counts to the first found ancestor HHH, depending on the aggre-
gation ordering. Almost all of the existing methods use the sum of
prefix lengths for ordering. We will revisit this commonly accepted
ordering in the next section.
The idea behind the HHH problem is that inputs are skewed and
sparse in space. To this end, various data structures are devised
to efficiently keep track of inputs such as grid-of-trie, rectangle-
search and cross-producting [13, 20, 24], or exploiting TCAM or
FPGA [11, 17, 18, 21]. For example, the cross-producting method
first aggregates inputs along each dimension separately to form
a compact n-dimensional matrix. The inputs are then mapped to
the corresponding entry in the matrix by longest prefix matching
along each dimension, and finally to make a summary, it aggregates
entries smaller than the threshold. In contrast, our algorithm is
simple space partitioning and uses no elaborate data structure.
Most of the theoretical studies investigate streaming approxima-
tion algorithms and their error bounds [1, 5–7, 10, 15]. There are
well known streaming algorithms to find frequent items using a
limited number of counters, and they are extended to apply to the
HHH problem by using 1/ϕ counters for each node in the lattice.
Recently, Ben Basat et al. tackle faster online HHH computation by
applying randomized sampling for updating counters in exchange
for slower convergence [1], whose motivation is common to one of
ours. In Section 4, we compare our method with the Space-Saving
algorithm [15] by Mitzenmacher et al. as a baseline.
All these algorithms are bottom-up, probably due to the fact that
the bottom-up approach is natural for unidimensional HHH and all
the algorithms extend unidimensional HHH to multi-dimensions.
To the best of our knowledge, ours is the first top-down HHH
algorithm, albeit flow partitioning itself is hardly new (e.g., [23]).
Our insight is to revisit the definition of HHH. The aggregation
order in almost all of the existing HHH algorithms employ the sum
of prefix lengths, which is intuitive for sorting aggregates from
more specific to less specific but does not always match operational
needs. For example, [32,∗] and [16, 16] are at the same level in the
prefix length sum order. From the operational view, however, the
former is more important as a specific source sending to diverse
destinations (e.g., scanning) while the latter only identifies broad
address space for sources and destinations and does not require
immediate attention from operators. Also, the existing methods
tend to produce broad aggregates with very short prefix lengths
falling under the upper lattice.
We introduce a different order by redefining child (i) in Equa-
tion 1 to take advantage of the underlying multi-dimensional hier-
archical structure. It allows top-down recursive partitioning, while
making the full prefix length (/32 for IPv4) higher in the order.
Figure 1: Lattice for a IPv4 prefix length pair [l0, l1] with 8-bit
granularity (д = 8)
pairs can be represented by a lattice ordered by the sum of prefix
lengths [7]. Figure 1 shows the lattice for an IPv4 address pair with
д = 8, having the size of the lattice h′×h′ = 5×5 = 25. (when
д = 1, h′×h′ = 33×33 = 1089). For example, (1.2.3.4, 5.6.7.8) with
[32, 32] at the bottom can be aggregated to any other node in the
lattice: (1.2.3.4/32, 5.6.7.0/24) with [32, 24], (1.2.3.0/24, 5.6.7.8/32)
with [24, 32], ... , (1.2.3.0/24, 5.6.7.0/24) with [24, 24], ... , up to
(0.0.0.0/0, 0.0.0.0/0) or (∗,∗) with [0, 0].
There exist different definitions and the corresponding algo-
rithms introduced by Cormode et al. [5–7] and subsequently by
many others. Here, we briefly review the relevant definitions.
Most of the existing methods employ ‘discounted HHH’ where
descendant HHHs are not double-counted in their ancestors’ counts
so as to make outputs concise and compact. Otherwise, all ancestors
of an HHH become HHHs, which is too redundant. We also employ
the discounted HHH.
The discounted count c′ for node i is the sum of its non-HHH
direct children’s discounted counts and can be computed from the
bottom of the lattice:
′
j
{ j ∈ child (i) | c
′
j < ϕN }
(cid:88)
′
i =
where
(1)
c
c
j
A naive algorithm in 2-dimensions goes through all possible
prefix length pairs in the lattice in the decreasing order of the
sum of prefix lengths. For each node in the lattice [l0, l1], it goes
through every input (p0, p1) making the corresponding aggregate
(p0/l0, p1/l1) and accumulating the count for the aggregate. After
processing all the inputs, it extracts aggregates as HHHs when
c′ ≥ ϕN and then their corresponding inputs are removed for
‘discount’ (instead of keeping track of direct children’s counts). The
cost is O (h′2
N ) for going through the entire inputs
for every node in the lattice, and O (log N ) to find (p0/l0, p1/l1),
though the latter can be optimized to O (1). Hence, it is costly to use
д = 1 for IPv4 address pairs, and it becomes even worse for IPv6.
Another factor is the rollup rule: how to roll up counts to parents.
Cormode et al. classify rollup rules into 2 categories: overlap and
split. The former allows double-counting among nodes if they do
not have ancestor-descendant relationship, and rolls up counts
N log N ); O (h′2
0,00sumofprefixlengths81624404856643232,3232,00,3224,3216,328,3232,832,1632,240,80,160,248,016,024,08,816,1624,2424,88,2424,1616,88,1616,24Recursive Lattice Search
IMC ’17, November 1–3, 2017, London, United Kingdom
Algorithm 1 Recursive Lattice Search
procedure LatticeSearch(par ent, l0, l1, ∆, pos)
doAддr eдate ← TRUE, doRecurse ← TRUE
if pos = U P P ER then
if ∆ = minGr anularity and not next to the very bottom node then
if doAддr eдate = FALSE and doRecurse = FALSE then
doAддr eдate ← FALSE
doRecurse ← FALSE
return
aддr eдateList ← Aggregate(inputsOf (par ent ), l0, l1 )
aддr eдateList ← Inherit F rom(par ent )
if doAддr eдate then
else
if doRecurse then
if ∆ (cid:44) minGr anularity then
∆ ← ∆/2
for all f in aддr eдateList do
if count (f ) ≥ thr esh then
if not on bottom edge then
LatticeSearch(f , l0+∆, l1+∆, ∆, LOW ER)
if not on left bottom edge then
LatticeSearch(f , l0+∆, l1, ∆, LEFT )
if not on right bottom edge then
LatticeSearch(f , l0, l1+∆, ∆, RIGHT )
LatticeSearch(f , l0, l1, ∆, U P P ER)
▷ already aggregated by the caller
▷ nothing to do
▷ halve granularity
▷ recurse DOWN
▷ recurse LEFT
▷ recurse RIGHT
▷ recurse UP
Figure 2: Z-order on the IPv4 prefix length pair lattice
Figure 3: Recursive Lattice Search with 6 Regions
3 RECURSIVE LATTICE SEARCH
Our new algorithm is Recursive Lattice Search (RLS). The specific
order to use in our algorithm is the Z-order introduced by Morton
in 1966 [16]. The Z-order is an ordering along a space filling curve
while preserving locality, preferring the largest value across all
dimensions. The Z-value is simply calculated by interleaving the
binary representations of two prefix length values. When applied
to an IPv4 prefix length pair [a, b], each dimension needs 5 bits
for [0...32]: a = a4a3...a0 and b = b4b3...b0. The Z-value is 10 bits,
z = a4b4a3b3...a0b0, for ordering the nodes in the lattice.
The Z-order on the 2-dimensional lattice with д = 8 is shown
in Figure 2. It looks slightly different from a standard Z-curve at
the bottom edges, because the maximum prefix length is 32 and
it does not have the entire 5 bit space. As a result, the uncovered
space is collapsed onto the bottom edges. It has a favorable effect
that having a full prefix length in either dimension becomes higher
in the order, which meets the operational bias for detecting DDoS
attacks and scanning. The line in the bottom of the figure shows