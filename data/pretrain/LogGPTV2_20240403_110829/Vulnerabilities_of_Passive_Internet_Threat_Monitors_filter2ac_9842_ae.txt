### 6. Protecting Threat Monitors

As previously mentioned, threat monitors are inherently vulnerable to marking activities. This section discusses possible approaches to protect these monitors, starting with an assessment of the information leak. By understanding the extent of the information being leaked, we can determine the time-to-live for sensors and the monitor, which in turn can be used to implement appropriate protective measures.

#### 6.1 Assessing the Leak

The first step in protecting a monitor is to assess the amount of information being leaked per unit of time. It is important to note that we should not solely rely on figures derived from some explicit procedure.

**Hypothetical Example:**
Consider a hypothetical monitor that publishes a completely unpopulated table type port report every hour. Assume there are 1,000 sensors in this monitor. A botnet with 20,000 hosts, each shooting address-encoded-port type markers at 30 markers per second, will produce a complete list of /16 blocks where sensors are placed in the first hour. During the second hour, each /16 block is marked with address-encoded-port markers using 64 ports (6 bits), producing a complete list of /22 blocks. This process is repeated, and by the end of the fourth hour, a complete list of full 32-bit addresses for all 1,000 sensors will be produced.

If the 1,000 sensor addresses are actually 128 wide-aperture sensors, each monitoring a /28 space, then each phase will produce a maximum of 128 blocks. This means that 512 ports (11-bit worth) can be used for the second and third phases, and the complete list would be available after the third hour.

This example illustrates that the same marking activity can behave differently under different conditions. Therefore, procedurally derived assessment results should only be used as a reference.

#### 6.2 Possible Protections

##### Provide Less Information

A straightforward way to combat marking activities is to reduce the amount of information the system provides, thereby extending the time-to-live. Manipulating feedback properties, as studied in Section 4.2, can be a good starting point. For instance, longer accumulation windows, longer feedback delays, reduced sensitivity, and larger cut-off thresholds can all help. However, there are several considerations:

- **Operational Policy:** The frequency and level of detail in published reports often reflect the system's basic operational policy. Systems that require large-scale distributed (manual) inspection by report viewers to capture new malicious activities early may need frequent and detailed reports. Reducing the amount of information provided could interfere with this policy, creating a trade-off between protection and system functionality.
- **Leakage Considerations:** Even if the amount of information is reduced, leakage still occurs. The amount of information given out should be determined through theoretical considerations, not simplistic thoughts.

**Minimize Background Information:**
Disclose minimal background information that could lead to the acquisition of vital system parameters or valuable additional information. This includes system overview statements on public platforms such as home pages, proceedings, and meeting handouts. Some systems may need to disclose their internals to some extent, but the disclosed information should still be carefully examined.

##### Throttle the Information

Standard remediation techniques used to provide privacy in data mining could be applied here. Studying the privacy of database queries and relating it to the problem presented in this paper could be beneficial.

##### Introduce Explicit Noise

Most marking activities exploit small changes in feedback. Adding small noise to captured events can disrupt the capturing of these small changes. One method is to introduce explicit variance into the level sensitivity of sensors. For example, sensors can be divided into two groups: one operating at full sensitivity and another at reduced sensitivity, generating low-level noise-like events. This method introduces another dimension that markings must consider, and theoretically, levels of sensitivity should differ across all sensors, not just limited to two levels.

Similar effects can be achieved through inter-monitor collaboration, where noise is generated from the results of other monitor systems, using them as sources for legitimate noise.

##### Disturb the Mark-Examine-Update Cycle

Another approach is to disturb the mark-examine-update cycle to prevent the refinement of the address list or to slow down the cycle. One way to implement this is to incorporate explicit sensor mobility.

**Considerations:**
- **Degree of Mobility:** The degree of mobility required to disrupt the cycle must be studied. Changing addresses within a limited small address space has little impact, while moving among much larger blocks would invalidate the marking result early, affecting subsequent results.
- **Sensor Placement:** Current threat monitors that use different sets of almost identical sensors and post-processing provide different reports. The effect of sensor mobility on monitor results must be studied, along with further investigation into why multiple threat monitors give different results. Studies using variations in sensor aperture and placement [18] can provide some answers.

Randomly discarding part of the captured events also disturbs the mark-examine-update cycle. While this method maintains the consistency of monitor results better than explicit mobility, it can deteriorate the effective sensitivity of the monitor.

##### Marking Detection

Detecting marking activities and discarding associated capture events is a powerful but challenging method. Well-designed markings are expected to show minimal correlations among markers, making them nearly indistinguishable from real background activities. However, marking activities are generally local and transient. Designing a marking activity that covers multiple addresses and persists over time is possible but at the cost of marking speed.

Statistical approaches may work in some cases, especially for activities introducing strong statistical anomalies. Correlations among different monitors can also be used to detect marking activities, but differences in feedbacks from different monitors must be studied in depth.

##### Sensor Scale and Placement

Consider an arrangement where 2^16 sensors are uniformly placed over /16 blocks (one sensor per /16 block). This forces address-encoded-port marking to be applied sequentially until half of the sensors are detected, effectively slowing down the most efficient marking method. Increasing the number of sensors and carefully placing them can provide a certain level of protection and is worth studying.

Carefully planned distributed sensor placement may also benefit marking detection efforts by revealing patterns of transient events that sweep across address blocks.

##### Small Cautions

In Section 4.5, we pointed out several methods to gather useful information. Most of these can be disabled with minor adjustments:

- **FQDNs for Intranet Sensors:** Assign fully qualified domain names (FQDNs) to sensors deployed in intranets to prevent FQDN-based filtering. Names that resemble common functionality or blend with other hosts in the same subnet are preferable.
- **ICMP Echo Responding Sensors:** Consider answering to other packets they capture to prevent them from being detected as silent hosts that only respond to ICMP echo requests.
- **ICMP Requests:** Consider how sensors respond to various ICMP requests to prevent ICMP-based fingerprinting and topology inferencing.
- **TTL Mangling for Intranet Sensors:** Introduce facilities (hardware and/or software) such as TTL mangling to make sensors appear deep inside an intranet, avoiding topology-based filtering.

### 7. Conclusion

Passive Internet threat monitors are essential tools for obtaining a macroscopic view of malicious activity on the Internet. This paper demonstrates that they are subject to detection attacks that can uncover the location of their sensors. We believe we have identified a new class of Internet threat, one that does not directly endanger host systems but rather poses a danger to the meta-system intended to keep the host systems safe.

### Acknowledgments

We would like to express our sincere gratitude to the parties who allowed us to conduct marking activities on their networks and monitor systems during the early stages of this research. We acknowledge that sometimes these activities went beyond the level of background noise and included unexpectedly formatted packets. We also thank members of the WIDE Project, the Special Interest Group on Internet threat monitors (SIGMON), and the anonymous reviewers for their thoughtful discussions and valuable comments. Additionally, we extend our thanks to Niels Provos for his assistance in preparing the final version of this paper.

### References

[1] CAIDA Telescope Analysis. <http://www.caida.org/analysis/security/telescope/>

[2] Distributed intrusion detection system. <http://www.dshield.org/>

[3] David Moore, Geoffrey M. Voelker, and Stefan Savage. "Inferring Internet Denial-of-Service Activity." In 10th USENIX Security Symposium, August 2001.

[4] SANS Internet Storm Center (ISC). <http://isc.sans.org/>

[5] The IUCC/IDC Internet Telescope. <http://noc.ilan.net.il/research/telescope/>

[6] About the Internet Storm Center (ISC). <http://isc.sans.org/about.php>

[7] Ruoming Pang, Vinod Yegneswaran, Paul Barford, Vern Paxson, and Larry Peterson. "Characteristics of Internet Background Radiation." In Proceedings of the Internet Measurement Conference (IMC) 2004, October 2004.

[8] Dug Song, Rob Malan, and Robert Stone. "A Snapshot of Global Internet Worm Activity." Technical report, Arbor Networks Inc., 2001.

[9] SWITCH Internet Background Noise (IBN). <http://www.switch.ch/security/services/IBN/>

[10] @police Internet Activities Monitored. <http://www.cyberpolice.go.jp/english/obs_e.html>

[11] JPCERT/CC Internet Scan Data Acquisition System (ISDAS). <http://www.jpcert.or.jp/isdas/index-en.html>

[12] Masaki Ishiguro. "Internet Threat Detection System Using Bayesian Estimation." In Proceedings of The 16th Annual Computer Security Incident Handling Conference, June 2004.

[13] Internet Motion Sensor. <http://ims.eecs.umich.edu/>

[14] Michael Bailey, Eval Cooke, Farnam Jahanian, Jose Nazario, and David Watson. "Internet Motion Sensor: A Distributed Blackhole Monitoring System." In Proceedings of The 12th Annual Network and Distributed System Security Symposium. ISOC, February 2005.

[15] PlanetLab. <http://www.planet-lab.org/>

[16] The Team Cymru Darknet Project. <http://www.cymru.com/Darknet/>

[17] libnet. <http://libnet.sourceforge.net/>

[18] Evan Cooke, Michael Bailey, Zhuoqing Morley Mao, David Watson, Farnam Jahanian, and Danny McPherson. "Toward Understanding Distributed Blackhole Placement." In Proceedings of the 2004 ACM workshop on Rapid malcode, October 2004.