DIMM3
Channel 1
Memory 
Controller
col 0 col 1 col 2
row 0
row 1
row 2
cell
Chip
Bank
Bank
Bank
DIMM
Rank
Chip
Bank
Cell 
Fig. 1: DRAM components.
arranged into racks, and a group of racks forms a cluster.
Usually, cloud computing systems use virtualization technol-
ogy to provide a scalable and reliable runtime environment.
However, software or hardware issues sometimes cause nodes
to be unavailable, signiﬁcantly impacting service availability
provided by a VM, a node, even a large cluster.
Once a node is likely to be unavailable, it is better to migrate
the workloads to other nodes promptly. Such a procedure is
called live migration, which refers to moving a running VM
between different nodes without disconnecting the clients or
applications. Live migration is a powerful mechanism used in
ECS system and other cloud computing systems. It enables
rapid movement of workloads within clusters with low impact
on running services.
C. Service Availability
High service availability is crucial to cloud computing sys-
tems. Cloud service providers have made every effort to assure
the promised high service availability. Internally, they build
strong infrastructures to monitor and log service status. Some
clients are deployed to perform heartbeat probes periodically
to assure node health. Moreover, they collect static conﬁg-
urations and monitoring data from operating system (OS)
or Baseboard Management Controller (BMC). The typical
monitoring data include error logs from kernel modules like
mcelog and error detection and correction (EDAC), system
resource usage (e.g., CPU and memory), and performance
metrics (e.g., latency and throughput) from the computing
nodes. Various alerts are developed based on the monitoring
data to provide early warnings of potential failures, e.g., CE
exceeds a predeﬁned threshold. Then, preventive actions such
as live migration can be performed to mitigate the failure
impact.
Although tremendous efforts have been made to maintain
high service availability, in reality, there are still many unex-
pected system issues causing node unavailability, e.g., software
crashes, kernel panic, hardware errors, and memory/cache
UEs. Once a node becomes unavailable, all VMs on it will fail
to serve user requests. Among the failure causes, DRAM faults
were observed as the leading hardware issues in the cloud and
high-performance computing areas [24]. We found a similar
observation in ECS system, which indicates that DRAM faults
signiﬁcantly impact service availability.
D. DRAM Components and Errors
DRAM components and errors. As shown in Fig. 1, in
modern cloud servers, a CPU has several memory controllers.
Each controller communicates with DIMMs through high-
speed memory channels. Usually, a memory channel is shared
by several DIMM slots. A DIMM has several ranks, and
each is composed of several DRAM chips. For typical DDR4
DIMMs, a rank is composed of 16 chips for data bits and
2 additional chips for ECC bits. A chip consists of multiple
banks, which enables the access parallelism. A DRAM bank
is structured as a two-dimensional cell array indexed by rows
and columns. At the micro-level, a cell can store multiple bits
of data, and the number of data bits stored in a cell is called
the data width of a chip, which is usually denoted as x4, x8,
or x16, etc.
DRAM errors are the most commonly observed hardware
errors in the cloud. They may occur on cells, rows, columns,
even large blocks of the banks. The soft error usually occurs
randomly and can hardly repeat. In contrast, hard errors are
caused by hardware faults such as irreversible hardware wear-
out and often repeat from time to time.
Error mitigation approaches. ECCs have been developed
to detect and correct errors. The ECC algorithms used in
modern CPUs are likely to correct almost any number of
error bits in a single chip [11], [23]. If the error pattern goes
beyond the correcting capability of the ECC, a UE will happen
and lead to a system crash. The common strategy to avoid
memory errors is to stop using the fault regions. Operating
systems provide the mechanism of page ofﬂining [8], [39] to
avoid memory errors. However, it is not applicable to cloud
services as unexpected large number of page ofﬂining in peak
service time may impact performance. The traditional method
to repair hardware is based on a predeﬁned error threshold.
For example, a DIMM would be replaced when it reaches
thousands of errors within 24 hours [16], [30].
III. EXPLORATORY STUDY
We conduct an empirical study on the logs collected from
over half a million nodes in ECS system over a year. The
logs include the timestamp and error location of CEs, the
occurrence of DCNUs. We perform considerable experiments
on correlation analysis and present four major observations in
this section.
Observation 1: CE storms are also major causes of
DCNUs. Prior studies [12], [20], [35], [45] focus on UE
analysis. We found that CE storms are also major causes of
DCNUs: 56%, 42%, and 2% of DCNUs are caused by CE
storms, UEs, and DIMM communication losses respectively.
As described earlier, a UE typically leads to a node crash
and a few CEs have negligible impacts. However, in reality,
even live nodes may fail to respond to user requests when
numerous CEs are continuously reported and handled. Our
ﬁndings indicate that only UE prediction is not enough in
our application context. Besides preventing UEs, we need to
prevent node unavailability caused by CE storms.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
277
TABLE I: CE temporal statistics in normal and failed nodes.
Normal Nodes
Lifetime
(months to years)
5
8
20
1041
Unavailable Nodes
3h
15
137
944
8089
1d
24
164
1179
18201
7d
28
182
1257
23841
1st quartile
Median
3rd quartile
Average
(cid:32)(cid:23)(cid:35)
(cid:31)(cid:23)(cid:35)
(cid:30)(cid:23)(cid:35)
(cid:29)(cid:23)(cid:35)
(cid:28)(cid:23)(cid:35)
(cid:27)(cid:23)(cid:35)
(cid:26)(cid:23)(cid:35)
(cid:25)(cid:23)(cid:35)
(cid:24)(cid:23)(cid:35)
(cid:23)(cid:35)
(cid:3)(cid:6)(cid:17)(cid:10)(cid:16)(cid:20)(cid:1)(cid:5)(cid:13)(cid:19)
(cid:4)(cid:7)(cid:15)(cid:9)(cid:14)(cid:18)(cid:9)(cid:8)
(cid:3)(cid:6)(cid:17)(cid:10)(cid:16)(cid:20)(cid:1)(cid:2)(cid:13)(cid:10)(cid:17)(cid:11)(cid:12)
(cid:4)(cid:7)(cid:15)(cid:9)(cid:14)(cid:18)(cid:9)(cid:8)
(cid:3)(cid:11)(cid:4)(cid:15)(cid:4)(cid:8)(cid:9)(cid:4)(cid:5)(cid:9)(cid:7)(cid:1)(cid:2)(cid:12)(cid:6)(cid:7)(cid:14) (cid:2)(cid:12)(cid:13)(cid:10)(cid:4)(cid:9)(cid:1)(cid:2)(cid:12)(cid:6)(cid:7)(cid:14)
1.0
1.0
0.8
0.8
0.6
0.6
0.4
0.4
0.2
0.2
n
o
i
t
r
o
p
o
r
P
(30,0.62)
(30,0.56)
Faulty Row
Faulty Column
0.0
0.0000
0
00000
50
055020
100
10000
150
15550
200
20000
250
2550
Time (in day)
Observation 2: CEs are frequently observed before
nodes become unavailable. For each DCNU occurrence, we
aggregate the preceding CEs in 3 different time windows of
3 hours, 1 day, and 1 week before the DCNU. For simplicity,
on normal nodes, we count the number of CEs throughout
their lifetime (most nodes are running in the cloud for months
to years). Table I shows the statistics of the data. Obviously,
the number of CEs on the unavailable nodes are an order of
magnitude higher than those on the normal nodes. The results
indicate that CEs are indicators of DCNUs.
Observation 3: Faulty rows/columns are more frequently
observed on unavailable nodes. Several prior studies have
analyzed the spatial distribution of DRAM errors [1], [21],
[38]. In case of many errors spanning in a large region of one
row, the DIMM is likely to experience more errors in the future
[21]. Here we further explore the relationship between DRAM
spatial error patterns and DCNUs, which is not enclosed in
previous studies.
Faulty rows and columns are commonly observed DRAM
error patterns (deﬁned in Section II-A). Fig. 2(a) illustrates
the percentage of unavailable and normal nodes that have
experienced faulty rows/columns, respectively. The results
show that faulty rows or columns are more frequently observed
on unavailable nodes than normal nodes. More than 70% of
unavailable nodes have at least one faulty row or column.
However, only no more than 5% normal nodes experienced
faulty rows or columns. This observation indicates that spatial
error patterns are predictive indicators of DCNUs.
Observation 4: DCNUs have long-term relevance to
faulty rows/columns. Fig. 2(b) visualizes the cumulative
distribution function (CDF) of time intervals to the consequent
DCNU after a new faulty row or column is observed. As we
can see from the ﬁgure, 56% or 62% of DCNUs occur within a
month once a new faulty row or column is observed. However,
some nodes experience DCNU much later in the complicated
environment. Therefore, we use four time windows within a
month and the entire lifetime of a node as the observation
windows to generate features (detailed in Section V).
IV. XBRAINM: AN AIOPS SYSTEM
We design XBrainM, an AIOps system integrated in ECS
system, to predict and mitigate DCNUs. We brieﬂy introduce
the key components of XBrainM in this section.
A. System Overview
Fig. 3 shows the workﬂow of XBrainM. The system collects
raw logs from multiple sources and feeds the cleaned data
(a) Percentage of unavailable/nor-
mal nodes on which faulty rows and
columns are observed.
(b) Cumulative distribution function
of time intervals (in day) to the con-
sequent DCNU after a new faulty row
or column is observed.
Fig. 2: Statistics on faulty rows and columns.
to the prediction module. The prediction module includes
rule-based prediction and learning-based prediction. Rule-
based prediction is based on simple heuristic rules capturing
deﬁnitive signals of DCNUs, e.g., the number of recent CEs
exceeds a certain threshold. Learning-based prediction can
capture more complex signals that the rules cannot cover.
It involves feature engineering, model learning, and predic-
tion. The system periodically performs predictions to estimate
whether ECS system nodes will experience DCNU in the near
future. The system indiscriminately starts VM migration no
matter the prediction is from the rule or ML model. Then
stress testing (see Section IV-F) is used to validate DIMM
health, and ﬁnally, the problematic DIMMs are repaired or
replaced.
B. Raw Data Collection and Cleaning
ECS system has already supported the monitoring infras-
tructures as described in Section II-C. We collect the data from
multiple sources, including hardware/BIOS/OS conﬁgurations,
EDAC log, mcelog, kernel
log, SEL, and BMC log. The
Node Unavailability Mitigation
ECS 
Nodes
Hardware Repair
Failure Veriﬁcation 
by Stress Testing 
VM Migration
Raw Data Collection 
& Cleaning 
Learning-based Prediction
Feature Engineering
Model Training & Deployment
Static Conﬁgurations 
Static  
Features 
Temporal 
Features 
Spatial 
Features 
Ofﬂine(cid:172) 
Training
Grayscale(cid:172)
Validation
Online
(cid:172)Deployment(cid:172)
MCE & EDAC Log
Rule-based Prediction
The number of recent CEs exceeds a certain threshold?
Other 
(cid:172) Monitoring Logs
Certain fatal events are detected?
(cid:172) 
......
Fig. 3: The workﬂow of XBrainM.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:28:25 UTC from IEEE Xplore.  Restrictions apply. 
278
static conﬁgurations are saved into a distributed database after
node startup. The monitoring clients on nodes sample runtime
system events and ﬂush the data to the database. EDAC driver
decodes the system address of the errors to the memory
location, i.e., the socket, the memory controller, the channel,
the slot, the rank, the bank group, the bank, the row, and
the column. We clearly mark the source of the events and
clean duplicates. When a node is detected to be unavailable,
system experts on BMC, BIOS, and OS double-check the
root cause according to the system logs and status, and mark
the unavailability labels. As to online prediction, the runtime
data is feed to online models through Flink [4], a streaming
processing framework.
C. Rule-based Prediction
Several prior studies [16], [30] have explored the use of
simple heuristic rules for predicting or mitigating DRAM
errors. XBrainM applies total 18 heuristic rules to report early
alerts of DCNUs. The rules are designed to capture deﬁnitive
DCNU signals, e.g.,
the number of recent CEs exceed a
certain threshold. These rules have been carefully tuned and
running stably in ECS system, performing well
in terms
of interpretability and computational efﬁciency. However, a
DCNU may have complex relevance to a large number of
indirect signals, which cannot be well handled by the simple
rule-based prediction.
D. Learning-based Prediction
that
To address the limitations of rule-based prediction, we
further introduce learning-based prediction to XBrainM, which
automatically learns the complex relevance between a large
number of factors to DCNUs.
Prediction model. DCNU prediction is a typical binary
classiﬁcation problem. Normal nodes and unavailable nodes
are considered as the negative and positive class in our system,
respectively. Four machine learning models showing the state-
of-the-art performance in binary classiﬁcation are selected as
candidates for evaluation, including extreme gradient boosting
(XGBoost) [5], Random Forest (RF) [3], Support Vector
Machine (SVM) [32], and Logistic Regression (LR) [40].
In practice, we expect
the model can be trained in
parallel for performance consideration. In addition, the model
is expected to quantify the importance of the features. Thus,
data analysts can provide valuable suggestions based on the
feature importance, such as model selection for DIMMs and
servers. Furthermore, the model should be robust enough to be
deployed in the hyper-scale cloud. To this end, we ﬁnally select
XGBoost, which not only satisﬁes three basic requirements but
also shows the best prediction performance in the experiments.
Prediction interval and prediction window. As described
previously, DCNU prediction is performed periodically. A
moderate prediction interval is desired to report DCNU alerts
in time. In XBrainM, we ﬁnd the day-level interval is in-
sufﬁcient to predict some DCNUs on time, while 1 minute
introduces too much system overhead. Finally, we chose to
perform predictions every 5 minutes. As to the prediction
Δtp
Δts
t0
...
...
Δtp
Δts
Prediction Interval
Sampling Interval
t
t+p
Multi-Resolution Observation Windows 