I use the following script to remove the 2 inputs from a siamese network and
the output (a cosine distance layer).
    #!/usr/bin/env python
    from dir.model import load_siamese_network
    import sys
    def load_siamese_network(filepath, include_distance=True):
        custom_objects = {
            'contrastive_loss': contrastive_loss
        }
        model = load_model(filepath, custom_objects=custom_objects)
        if not include_distance:
            # layer 0 and 1 are inputs
            tuned_model = model.layers[2]
            return tuned_model
        return model
    src = sys.argv[1]
    dst = sys.argv[2]
    model = load_siamese_network(src, include_distance=False)
    model.save(dst)
![screen shot 2017-10-30 at 16 13 53](https://user-
images.githubusercontent.com/416585/32200140-5bfca3e8-bd8d-11e7-86b8-f5fad6471f89.png)
Here's the code I use to generate the siamese network from a base network:
https://gist.github.com/aeec5b2a6efb1090375d4211aabbed6f
It appears to be working as expected but I was surprised to see that the saved
file is twice smaller than the original one (90MB instead of 180MB). How could
removing 2 inputs and a distance layer with no weights so dramatically reduce
the file size?
Since this is a siamese network, both inputs share the same network + weights
so removing an input shouldn't really affect the file size that much. That
being said, it's possible that Keras isn't that smart about saving the model
to disk and is duplicating the network weights for each input although they
really are shared.
Is this what is happening? Or did I setup the siamese network wrong?