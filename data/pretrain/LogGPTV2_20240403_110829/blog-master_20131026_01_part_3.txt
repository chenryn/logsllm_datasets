CREATE INDEX  
digoal=# update pg_class set relpersistence='u' where relname='t';  
UPDATE 1  
digoal=# update pg_class set relpersistence='u' where relname='idx_t_id';  
UPDATE 1  
[root@db-172-16-3-150 ~]# cp /home/pg93/test.sql /home/pg93/t.dmp /home/pg931/  
chown pg931:pg931 /home/pg931/*  
pg931@db-172-16-3-150-> vi test.sql  
copy t from '/home/pg931/t.dmp' with (header off);  
pg931@db-172-16-3-150-> pgbench -M prepared -n -r -f ./test.sql -c 8 -j 4 -t 4  
transaction type: Custom query  
scaling factor: 1  
query mode: prepared  
number of clients: 8  
number of threads: 4  
number of transactions per client: 4  
number of transactions actually processed: 32/32  
tps = 0.799815 (including connections establishing)  
tps = 0.799973 (excluding connections establishing)  
statement latencies in milliseconds:  
        9899.317000     copy t from '/home/pg931/t.dmp' with (header off);  
```  
性能提升非常明显  
每秒约导入212MB 或 48.8万条记录.  
使用32KB的blocksize, 同时使用unlogged table, 并且删除索引. 导入速度还有提升  :   
```  
pg931@db-172-16-3-150-> pgbench -M prepared -n -r -f ./test.sql -c 8 -j 4 -t 4  
transaction type: Custom query  
scaling factor: 1  
query mode: prepared  
number of clients: 8  
number of threads: 4  
number of transactions per client: 4  
number of transactions actually processed: 32/32  
tps = 1.245248 (including connections establishing)  
tps = 1.245620 (excluding connections establishing)  
statement latencies in milliseconds:  
        6414.338875     copy t from '/home/pg931/t.dmp' with (header off);  
```  
每秒约导入330MB 或 76万条记录.  
下面使用初始化main fork, 测试速度再次提升 :   
```  
digoal=# select max(ctid) from t;  
    max       
------------  
 (520544,4)  
(1 row)  
digoal=# delete from t where ctid <> '(520544,4)';  
DELETE 39039999  
digoal=# vacuum (analyze,verbose) t;  
pg931@db-172-16-3-150-> pgbench -M prepared -n -r -f ./test.sql -c 8 -j 4 -t 4  
transaction type: Custom query  
scaling factor: 1  
query mode: prepared  
number of clients: 8  
number of threads: 4  
number of transactions per client: 4  
number of transactions actually processed: 32/32  
tps = 1.384007 (including connections establishing)  
tps = 1.384453 (excluding connections establishing)  
statement latencies in milliseconds:  
        5759.885594     copy t from '/home/pg931/t.dmp' with (header off);  
```  
每秒约导入367MB 或 84.4万条记录.  
除此之外还有可以提升的方面是CPU核数以及CPU 频率. 有条件的朋友也可以自己测试一下.  
以下是一台32核的机器测试的结果 :   
```  
CPU  
Intel(R) Xeon(R) CPU           X7560  @ 2.27GHz  
内存: 32GB  
```  
```  
pg93@db-192-168-100-34-> ll  
-rw-r--r-- 1 pg93 pg93 205M Oct 28 13:49 t.dmp  
-rw-rw-r-- 1 pg93 pg93   48 Oct 28 12:32 test.sql  
pg93@db-192-168-100-34-> wc -l t.dmp   
500000 t.dmp  
pg93@db-192-168-100-34-> pgbench -M prepared -n -r -f ./test.sql -c 32 -j 32 -t 2  
transaction type: Custom query  
scaling factor: 1  
query mode: prepared  
number of clients: 32  
number of threads: 32  
number of transactions per client: 2  
number of transactions actually processed: 64/64  
tps = 5.352594 (including connections establishing)  
tps = 5.358381 (excluding connections establishing)  
statement latencies in milliseconds:  
        5918.995641     copy t from '/dev/shm/t.dmp' with (header off);  
```  
每秒约导入1097.3MB 或 267.6 万条记录(平均每条记录455字节).   
## 小结  
对于大数据量的copy导入, 性能优化的方法可以考虑以下几个方面.  
1\. auto analyze 调整(尽量减少或者导入阶段关闭后续再打开)  
2\. table blocksize(如果业务形态是以大批量导入数据为主, 可以调整到最大)  
3\. unlogged table, (使用unlogged table可以减少wal flush带来的负担, 注意删除fork init文件, 避免db crash后删数据. )  
4\. 初始化table, (这不是一个常规用法, 仅用于本文测试, 如果pg可以做到设置extended的块个数, 则可以减少waiting)  
5\. shared buffer, wal buffer, 加大checkpoint_segments.  
6\. oscache fadvise(will not need, 减少os cache占用).  
7\. 提高硬盘写入速度  
8\. 如果pg可以做到设置extended的块个数, 那么就可以通过提高cpu核数. 提高并行度. 从而提高导入速度.  
9\. 或者支持异步io写入也许会有提高.  
man aio_write  
http://www.wikivs.com/wiki/MySQL_vs_PostgreSQL#PostgreSQL_Synchronous_Replication  
http://www.postgresql.org/docs/devel/static/libpq-async.html  
http://grokbase.com/t/postgresql/pgsql-hackers/984pzfvztr/hackers-async-io-description  
http://blog.sina.com.cn/s/blog_742eb90201010yul.html  
http://linux.die.net/man/3/aio_write  
## 其他注意事项  
1\. 如果要调大block_size, 需要考虑几个问题.  
每次checkpoint后, 第一次变更的block需要写整个page页到wal日志(这个block后续变更都是增量写的). 所以block_size调大会使得wal的产量也变大一些.  
block_size变大后, 对shared buffer的需求也会变大, 因为shared buffer用量的最小单位是以数据块为单位的, (例如一个表有10000个8K数据块, 离散活跃数据在shared buffer中可能占用100个块, 数据块变32K后, 这个表虽然只有2500个数据块, 但是离散活跃数据在shared buffer中可能还是占用100个块, 这个需求其实无形中放大了4倍).  
block_size变大后, 某些查询的效率可能受到影响, 同样因为shared buffer用量的最小单位是以数据块为单位的, 对数据块个数有同样需求的场景, 无形中放大了数据块操作的成本.  
block_size变大后, 数据块的锁冲突也会更明显, 因为存储的数据条目更多了.  
https://community.oracle.com/thread/687418?start=0&tstart=0  
http://www.dba-oracle.com/s_oracle_block_size.htm  
## 参考  
1\. http://blog.163.com/digoal@126/blog/static/163877040201391674922879/  
2\. http://postgresql.1045698.n5.nabble.com/COPY-extend-ExclusiveLock-td5587556.html  
3\. http://postgresql.1045698.n5.nabble.com/Process-11812-still-waiting-for-ExclusiveLock-on-extension-of-relation-td5716947.html  
#### [PostgreSQL 许愿链接](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
您的愿望将传达给PG kernel hacker、数据库厂商等, 帮助提高数据库产品质量和功能, 说不定下一个PG版本就有您提出的功能点. 针对非常好的提议，奖励限量版PG文化衫、纪念品、贴纸、PG热门书籍等，奖品丰富，快来许愿。[开不开森](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216").  
#### [9.9元购买3个月阿里云RDS PostgreSQL实例](https://www.aliyun.com/database/postgresqlactivity "57258f76c37864c6e6d23383d05714ea")
#### [PostgreSQL 解决方案集合](https://yq.aliyun.com/topic/118 "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")