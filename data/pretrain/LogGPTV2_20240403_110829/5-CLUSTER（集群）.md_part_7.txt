balance roundrobin
server web1 192.168.4.104:80 check inter 2000 rise 2 fall 5 #web2
名字随便取
server web2 192.168.4.105:80 check inter 2000 rise 2 fall 5 #web2
名字随便取
#balance roundrobin 轮询算法 leastconn 最少连接算法
#rise 2 连续两次测试没问题,视为恢复Fall 5 连续五次连不上,视为坏
\-\-\-\-\-\-\-\-\--可做多个集群,如下再添加一个listen
listen webs :8080
> balance \...
>
> server \*\*1 \.... \....
>
> server \*\*2 \... \....
### 3）启动服务器并设置开机启动
\~\]# systemctl restart haproxy.service
\~\]# systemctl status haproxy.service
\~\]# ss -ntulp \|grep :80
tcp LISTEN 0 128 \*:80 \*:\* users:((\"haproxy\",pid=5315,fd=5))
\[root@kibana103 \~\]# netstat -unltp \|grep :80
tcp 0 0 0.0.0.0:80 0.0.0.0:\* LISTEN 5315/haproxy
\[root@haproxy \~\]# systemctl enable haproxy
### 4) 客户端轮测试:
使用火狐浏览器访问http://192.168.4.5，测试调度器是否正常 或者用curl
\[root@es102 \~\]# curl 192.168.4.103
192.168.4.104
\[root@es102 \~\]# curl 192.168.4.103
192.168.4.105
\[root@es102 \~\]# curl 192.168.4.103
192.168.4.104
\[root@es102 \~\]# curl 192.168.4.103
192.168.4.105
### 5)启用服务器状态监控
客户端访问http://192.168.4.5:1080/stats测试状态监控页面是否正常。访问状态监控页的内容，参考图如下所示。以下配置内容添加在上面配置的最下面:
listen stats :1080 #开监控,指定访问端口
stats refresh 30s #统计页面自动刷新时间
stats uri /status #统计页面url
访问监控页面地址:192.168.\*.\*:1080/status
stats realm Haproxy Manager #统计页面密码框上提示文本
stats auth admin:admin #统计页面登录用户名和密码设置
#stats hide-version #隐藏统计页面上HAProxy的版本信息
配置好后重启,并查看状态,确保正常
![](media/image22.png){width="7.258333333333334in"
height="4.197222222222222in"}
用户名:admin 密码:admin
![image004](media/image23.png){width="7.292361111111111in"
height="2.8958333333333335in"}
备注：
Queue队列数据的信息（当前队列数量，最大值，队列限制数量）；
Session rate每秒会话率（当前值，最大值，限制数量）；
Sessions总会话量（当前值，最大值，总量，Lbtot: total number of times a
server was selected选中一台服务器所用的总时间）；
Bytes（入站、出站流量）；
Denied（拒绝请求、拒绝回应）；
Errors（错误请求、错误连接、错误回应）；
Warnings（重新尝试警告retry、重新连接redispatches）；
Server(状态、最后检查的时间（多久前执行的最后一次检查）、权重、备份服务器数量、down机服务器数量、down机时长)。
\[root@Web1111 \~\]# systemctl stop httpd
# ngixn LVS Haproxy区别
**LVS优点：**抗负载强，只分发，无流量产生，稳定快速,配置简单,支持四层(只有ip:port)几乎可做所有应用的负载均衡,需要一定网络知识支持
**LVS缺点：**不支持正则表达式，不能做动静分离，网站庞大的话，实施较为复杂
**Nginx优点：**网络稳定性依赖小，支持四层和七层，并发量高，负载高，可做为静态网页与图片服务器，社区活跃，模块非常多，支持http地址重写,正则
**Nginx缺点：**适应范围小，健康检查只支持端口监测，不支持url来监测
**Haproxy优点：**支持虚拟主机，支持四层和七层(session,cookie,http地址重写,正则)，本身是一款负载均衡软件，HAProxy支持TCP协议的负载均衡转发，可以对MySQL读进行负载均衡，对后端的MySQL节点进行检测和负载均衡，可以用LVS+Keepalived对MySQL主从做负载均衡。负载均衡策略非常多
**HAPorxy缺点：**不支持POP/SMTP协议、不支持SPDY协议、不支持HTTP
cache功能。现在不少开源的lb项目，都或多或少具备HTTP
cache功能、重载配置的功能需要重启进程，虽然也是soft
restart，但没有Nginx的reaload更为平滑和友好、多进程模式支持不够好
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
**nginx和haproxy区别**
nginx:既能做代理,又能做web 可调用模块查看服务器状态信息:
haproxy:纯粹的代理
**功能:**nginx\>Haproxy\>LVS
**性能:**LVS \> Haproxy\>nginx
另外一个调度设备:F5厂商的:big-IP设备,是个硬件,较贵20万左右一个,外观像路由器,
调度更快
**\-\-\-\-\-\--常用调度算法：\-\-\-\--**
**轮询(RR)：**所有的请求平均分配给每个真实服务器
**加权轮询(WRR)：**给每台服务器添加一个权值，依据权值分配服务器的次数
**最小连接(LC)：**把请求调度到连接数量最小的服务器上
**加权最小连接(WLC)：**给每个服务器一个权值，调度器会尽可能保持服务器连接数量与权值之间的平衡
**ip_hash(静态调度算法)：**每个请求按客户端IP的哈希结果分配，当新的请求到达时，先将其客户端的IP通过哈希算法哈希出一个值，在随后的请求中，客户IP的哈希值只要相同，就会被分配至同一台服务器，该调度算法可以解决动态网页的session共享问题。但有时会导致请求分配不均，因为国内大多数公司都是NAT上网模式，多个客户端对应一个外部IP，所以这些客户端会被分配到同一节点服务器，从而导致请求分配不均
**url_hash：**按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器;
**fair:**这是比上面两个更加智能的负载均衡算法。此种算法可以依据页面大小和加载时间长短智能地进
行负载均衡，也就是根据后端服务器的响应时间来分配请求，响应时间短的优先分配。Nginx本身是不支持
fair的，如果需要使用这种调度算法，必须下载Nginx的upstream_fair模块。
\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--
**四七层区别**
负载均衡又分为四层负载均衡和七层负载均衡。四层负载均衡工作在OSI模型的传输层，主要工作是转发，它在接收到客户端的流量以后通过修改数据包的地址信息将流量转发到应用服务器。
七层负载均衡工作在OSI模型的应用层，因为它需要解析应用层流量，所以七层负载均衡在接到客户端的流量以后，还需要一个完整的TCP/IP协议栈。七层负载均衡会与客户端建立一条完整的连接并将应用层的请求流量解析出来，再按照调度算法选择一个应用服务器，并与应用服务器建立另外一条连接将请求发送过去，因此七层负载均衡的主要工作就是代理。
# NSD CLUSTER DAY04
案例1：实验环境
案例2：部署ceph集群
案例3：创建Ceph块存储
分布式文件系统:是指文件系统管理的物理存储资源不一定直接;连接在本地节点上,而是通过计算机网络与节点相连
分布式文件系统的设计基于客户机/服务器模式(C/S)
常见分布式文件系统:Lustre Hadoop FastDFS Ceph GlusterFS
Ceph分布式文件系统:
-   具有高扩展,高可用,高性能的特点
-   可以提供对象存储,块存储,文件系统存储(唯一能提供所有存储方案)
-   可以提供PB级别的存储空间(PB\--TB\--GB)1PB=1024TB 1TB=1024GB
[[http://docs.cephc.org/start/intro]{.underline}](http://docs.cephc.org/start/intro)
Ceph 组件:平台搭建
OSD ：存储设备,负责真正的存储空间, 软件包：ceph-osd
Monitor ：集群监控组件，利用Crush算法分配存储,要求至少要做三台
软件包：ceph-mon
例如：node1 node2 node3 node4
Crush 算法：将大数据打散小数据，如分成无数4M小文件，分别存入node中
取余算法：文件名 \| md5sum
产生16进制数，再按0-3取余得到一个数字，文件名不变，经过md5sum
产生的16进制数不会变，那么取余也不会变，ceph不用取余算法，如果需要扩展存储空间，多了node，所有的数据需要重新计算，增加node5,数据需要重新按照0-4取余计算。
Ceph 组件:平台使用方式
RBD
块存储网管(不需要安装包):客户端可以挂载(类似iscsi),不能用fdisk工具分区
Radosgw
对象存储(云盘就是对象存储,运维搭建平台,必须写程序API调用使用,不能
被挂载,不能被mount.):可在公司搭建私有云盘
MDS
存放文件系统的元数据(对象存储和块存储不需要该组件,需要额外安装一个包,)
用户直接mount使用
块共享iscsi:客户端挂载,分区，格式化使用
文件系统共享nfs,samba：客户端直接挂载使用
对象存储共享swift,客户端需要通过API访问，API需要开发者编写程序，例如：下载
专门的云盘软件
Client ceph客户端
软件包：ceph-common
客户端访问mon，由mon分配
OSDs 存储设备,负责真正的存储空间,
Monitors 集群监控组件
Client访问 ceph-common
Mon:将数据进行md5sum加密.\-\--\>
加密后的数字%存储单元个数(取余)来定位数据存储位置, 不适合添加存储单元
ceph存储1G数据:
ceph默认3副本:Mon将1G数据拆分成多份数据,且每份数据分别复制,始终保持有三份数据,因此做ceph至少要三台机器。
RAID与Ceph 区别：
RAID：单机版
Ceph：分布式
# 1 案例1：ceph实验环境
1.1 问题
准备四台KVM虚拟机，其三台作为存储集群节点，一台安装为客户端，实现如下功能：
创建1台客户端虚拟机
创建3台存储集群虚拟机
配置主机名、IP地址、YUM源
修改所有主机的主机名
配置无密码SSH连接
配置NTP时间同步
创建虚拟机磁盘
1.2 方案
使用4台虚拟机，1台客户端、3台存储集群服务器，拓扑结构如图-1所示。
![image001 (1)](media/image24.png){width="4.834722222222222in"
height="2.3131944444444446in"}
所有主机的主机名及对应的IP地址如表-1所示。
![table001](media/image25.png){width="4.661111111111111in"
height="1.5354166666666667in"}
## 步骤一：安装前准备
### 1）物理机为所有节点配置yum源服务器。
\[root@room9pc01 \~\]# yum -y install vsftpd
\[root@room9pc01 \~\]# mkdir /var/ftp/ceph
\[root@room9pc01 \~\]# mount -o loop \\
rhcs2.0-rhosp9-20161113-x86_64.iso /var/ftp/ceph
\[root@room9pc01 \~\]# systemctl restart vsftpd
mount -o loop loop:是将一个文件当作硬盘分区挂载
### 2）修改所有节点都需要配置YUM源（这里仅以node1为例）。
\[root@node1 \~\]# cat /etc/yum.repos.d/ceph.repo
\[mon\]
name=mon
baseurl=ftp://192.168.4.254/ceph/rhceph-2.0-rhel-7-x86_64/MON
gpgcheck=0
\[osd\]
name=osd
baseurl=ftp://192.168.4.254/ceph/rhceph-2.0-rhel-7-x86_64/OSD
gpgcheck=0
\[tools\]
name=tools
baseurl=ftp://192.168.4.254/ceph/rhceph-2.0-rhel-7-x86_64/Tools
gpgcheck=0
用scp命令将node1 的yum文件拷贝到node2/node3/node4
### 3）修改/etc/hosts并同步到所有主机。
警告：/etc/hosts解析的域名必须与本机主机名一致！！！！
\[root@node1 \~\]# cat /etc/hosts
\... \...
192.168.4.10 client
192.168.4.11 node1
192.168.4.12 node2
192.168.4.13 node3
警告：/etc/hosts解析的域名必须与本机主机名一致！！！！
\[root@node1 \~\]# for i in 10 11 12 13
\> do
\> scp /etc/hosts 192.168.4.\$i:/etc/
\> done
\[root@node1 \~\]# for i in 10 11 12 13
\> do
\> scp /etc/yum.repos.d/ceph.repo 192.168.4.\$i:/etc/yum.repos.d/
\> done
### 配置无密码连接(包括自己远程自己也不需要密码)。
找一台主机,无密码远程所有主机,等会安装软件,只在这台机器上连接所有机器,并安装所有软件(本实验选node1)
\[root@node1 \~\]# ssh-keygen -f /root/.ssh/id_rsa -N \' \'
#非交互生成秘钥文件
\[root@node1 \~\]# for i in 10 11 12 13
\> do
\> ssh-copy-id 192.168.4.\$i #发送秘钥到其他主机
\> done
ssh-keygen -f /root/.ssh/id_rsa -N
ssh-copy-id 192.168.4.\$i 也可以 ssh-copy-id client/node1/node2/node3
## 步骤二：配置NTP时间同步
### 1）真实物理机创建NTP服务器。
\[root@room9pc01 \~\]#yum -y install chrony
\[root@client \~\]# cat /etc/chrony.conf
server 0.centos.pool.ntp.org iburst
allow 192.168.4.0/24
local stratum 10
\[root@room9pc01 \~\]# systemctl restart chronyd
如果有防火墙规则，需要清空所有规则
\[root@room9pc01 \~\]# iptables -F
### 2）其他所有节点与NTP服务器同步时间（以node1为例）。
\[root@node1 \~\]# cat /etc/chrony.conf
server 192.168.4.254 iburst
\[root@node1 \~\]# systemctl restart chronyd
## 步骤三：准备存储磁盘
物理机上为每个虚拟机准备3块磁盘。（可以使用命令，也可以使用图形直接添加）
\[root@room9pc01 \~\]# virt-manager
3块磁盘 vdb vdc vdd
Vdb分两个区,vdb1 vdb2(现在不用作，后面有步骤做)
Vdb1 为vdc 的缓存,,数据存储先写入vdb1,传输完成后,再将数据拷贝到vdc
Vdb2 为vdd的缓存