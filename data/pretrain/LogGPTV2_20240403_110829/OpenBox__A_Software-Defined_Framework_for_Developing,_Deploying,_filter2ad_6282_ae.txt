tion.
Our current northbound API merely exposes the pro-
tocol primitives and Events API to the application de-
veloper, using a set of Java classes and interfaces. How-
ever, higher level abstractions for applications, such as
dedicated programming languages or structures, could
provide more convenient ways to program NFs on top
of OpenBox. Such abstractions might simplify and en-
hance the merge process in the controller. Examples of
such abstractions for SDN applications are Frenetic/Py-
retic [13]. In addition, veriﬁcation solutions such as [4]
might be applied on OpenBox applications, with the re-
quired adaptations, to provide oﬄine veriﬁcation before
deploying NFs.
7. RELATED WORK
In recent years, middleboxes and network functions
have been major topics of interest. In this section we
discuss and compare the state-of-the-art works that are
directly related to this paper.
CoMb [38] focuses on consolidating multiple virtual
middleboxes into a single physical data plane location,
thus improving the performance of the network in the
common case where not all the middleboxes have peak
load at the same time. E2 [33] is a scheduling frame-
work for composition of multiple virtual NFs.
It tar-
gets a very speciﬁc hardware infrastructure, and man-
ages both the servers on which NFs are running and the
virtual network switches that interconnect them. Un-
like OpenBox, CoMb and E2 only decompose NFs to
provide I/O optimizations such as zero-copy and TCP
reconstruction, but not to reuse core processing blocks
such as classiﬁers and modiﬁers. Speciﬁcally, the CoMb
paper has left for future research the exploration of the
choice of an optimal set of reusable modules [38, Section
6.3]. We view our paper as another step forward in this
direction.
xOMB [1] presents a speciﬁc software platform for
running middleboxes on general purpose servers. How-
ever, it does not consolidate multiple applications to
the same processing pipeline. ClickOS [26] is a runtime
platform for virtual NFs based on the Click modular
router [23] as the underlying packet processor. ClickOS
provides I/O optimizations for NFs and reduced latency
for packets that traverse multiple NFs in the same phys-
ical location. ClickOS does not have a network-wide
centralized control, and it does not merge multiple NFs,
but only chains them and optimizes their I/O.
Commercial solutions such as OpenStack [31], Open-
MANO [30], OpNFV [32], and UNIFY [21] are focused
on the orchestration problem. They all assume each
NF is a monolithic VM, and try to improve scaling,
522
placement, provisioning, and migration. Stratos [17]
also provides a solution for NFV orchestration, includ-
ing placement, scaling, provisioning, and traﬃc steer-
ing.
OpenNF [18] proposes a centralized control plane for
sharing information between software NF applications,
in cases of NF replication and migration. However, their
work focuses only on the state sharing and on the for-
warding problems that arise with replication and mi-
gration, so in a sense it is orthogonal to our work.
OpenState [5] and SNAP [3] are programming lan-
guage for stateful SDN switches. OpenState makes it
possible to apply ﬁnite automata rules to switches, rather
than match-action rules only. SNAP takes a network-
wide approach where programs are written for “one big
switch” and the exact local policies are determined by
the compiler. Both these works are focused on header-
based processing, but such ideas could be useful to cre-
ate programming languages on top of the OpenBox frame-
work, as discussed in Section 6.
To the best of our knowledge, Slick [2] is the only
work to identify the potential in core processing step
reuse across multiple NFs. They present a framework
with centralized control that lets NF applications be
programmed on top of it, and use Slick machines in
the data plane to realize the logic of these applications.
The Slick framework is mostly focused on the place-
ment problem, and the API it provides is much more
limited than the OpenBox northbound API. Slick does
not share its elements across multiple applications and
the paper does not propose a general communication
protocol between data plane units and their controller.
Unlike our OBIs, Slick only support software data plane
units; these units cannot be extended. This work com-
plements ours as the solutions to the placement prob-
lems presented in [2] can be implemented in the Open-
Box control plane.
Our preliminary workshop paper [7] on OpenBox de-
scribed the proposed architecture but presented a very
limited framework that uses a uniﬁed processing pipeline
for merging multiple middleboxes. The proposed uni-
ﬁed pipeline was very restrictive.
In this paper we
present a much more ﬂexible NF programming model,
including an algorithm to merge multiple applications
given this ﬂexible model.
Another work [8] suggested extracting the process of
deep packet inspection (DPI) to an external network
service. This work shows how performing DPI for mul-
tiple middleboxes at a single location could improve
network performance. Still, middleboxes are assumed
to remain monolithic units, with their DPI logic out-
sourced to an external service.
OpenBox allows easier adoption of hardware accel-
erators for packet processing. Very few works have
addressed hardware acceleration in an NFV environ-
ment [27], and those that have focused on the hypervi-
sor level [9,16]. Such ideas can be used in the OpenBox
data plane by the OBIs, and thus provide additional
hardware acceleration support.
The Click modular software router [23] is an extend-
able software package for programming network routers
and packet processors. It has numerous modules for ad-
vanced routing and packet processing; additional mod-
ules can be added using the provided API. OpenBox
generalizes the modular approach of Click to provide an
network-wide framework for developing modular NFs.
We use Click as the packet processing engine, as part of
our software implementation for an OBI, described in
Section 4.
Another related work in this context is the P4 pro-
grammable packet processor language [6]. The P4 lan-
guage aims to deﬁne the match-action table of a general
purpose packet processor, such that it is not coupled
with a speciﬁc protocol or speciﬁcation (e.g., OpenFlow
of a speciﬁc version). A P4 switch can be used as part
of the OpenBox data plane, by translating the corre-
sponding protocol directives to the P4 language.
8. CONCLUSIONS
This paper presents OpenBox, a software-deﬁned fr-
amework for developing, deploying, and managing net-
work-functions. OpenBox decouples the control plane
of network-functions from their data plane, and allows
reuse of data plane elements by multiple logical NFs.
In addition to easier management, orchestration, provi-
sioning and scale, it provides greater ﬂexibility in terms
of NF development and deployment, multi-tenancy sup-
port with complete tenant isolation, and improved data
plane performance.
We have implemented OpenBox and shown that it is
not only easy to deploy and to program but also im-
proves network performance. We envision that frame-
works such as OpenBox will pave the way for further
advances in network function virtualization (NFV) with
respect to NF programming, deployment, and easier
management, while maintaining and improving perfor-
mance. The ﬂexible support for hardware accelerators
for packet processing makes OpenBox even more ap-
pealing as today most NFV frameworks assume com-
pletely virtual environments and do not support any
hardware accelerators [27].
Acknowledgments
We thank the reviewers of the SIGCOMM PC and our
shepherd Vyas Sekar for their valuable comments on
this paper. We also thank Pavel Lazar, Dan Shmidt,
and Dana Klein, for their part in the implementation of
the OpenBox framework. This research was supported
by the European Research Council under the European
Union’s Seventh Framework Programme (FP7/2007–
2013)/ERC Grant agreement no 259085, the Israeli Cen-
ters of Research Excellence (I-CORE) program (Center
No. 4/11), and the Neptune Consortium, administered
523
by the Oﬃce of the Chief Scientist of the Israeli Min-
istry of Industry, Trade, and Labor.
9. REFERENCES
[1] J. W. Anderson, R. Braud, R. Kapoor, G. Porter, and
A. Vahdat. xOMB: extensible open middleboxes with
commodity servers. In ANCS, pages 49–60, 2012.
[2] B. Anwer, T. Benson, N. Feamster, and D. Levin.
Programming Slick Network Functions. In SOSR, pages
14:1–14:13, 2015.
[3] M. T. Arashloo, Y. Koral, M. Greenberg, J. Rexford, and
D. Walker. SNAP: Stateful Network-Wide Abstractions for
Packet Processing. In SIGCOMM, 2016.
[4] T. Ball, N. Bjørner, A. Gember, S. Itzhaky, A. Karbyshev,
M. Sagiv, M. Schapira, and A. Valadarsky. VeriCon:
towards verifying controller programs in software-deﬁned
networks. In PLDI, page 31, 2014.
[5] G. Bianchi, M. Bonola, A. Capone, and C. Cascone.
OpenState: Programming platform-independent stateful
OpenFlow applications inside the switch. SIGCOMM
Comput. Commun. Rev., 44(2):44–51, Apr 2014.
[6] P. Bosshart, D. Daly, G. Gibb, M. Izzard, N. McKeown,
J. Rexford, C. Schlesinger, D. Talayco, A. Vahdat,
G. Varghese, and D. Walker. P4: Programming
protocol-independent packet processors. SIGCOMM
Comput. Commun. Rev., 44(3):87–95, Jul 2014.
[7] A. Bremler-Barr, Y. Harchol, and D. Hay. OpenBox:
Enabling Innovation in Middlebox Applications. In
HotMiddlebox, pages 67–72, 2015.
[8] A. Bremler-Barr, Y. Harchol, D. Hay, and Y. Koral. Deep
packet inspection as a service. In CoNEXT, pages 271–282,
2014.
[9] Z. Bronstein, E. Roch, J. Xia, and A. Molkho. Uniform
handling and abstraction of NFV hardware accelerators.
IEEE Network, 29(3):22–29, 2015.
[10] ECMA. The JSON data interchange format, October 2013.
http://www.ecma-international.org/publications/
files/ECMA-ST/ECMA-404.pdf.
[11] ETSI. Network functions virtualisation - introductory white
paper, 2012.
http://portal.etsi.org/NFV/NFV_White_Paper.pdf.
[12] S. K. Fayazbakhsh, L. Chiang, V. Sekar, M. Yu, and J. C.
Mogul. Enforcing network-wide policies in the presence of
dynamic middlebox actions using ﬂowtags. In NSDI, pages
533–546, 2014.
[13] N. Foster, A. Guha, M. Reitblatt, A. Story, M. J.
Freedman, N. P. Katta, C. Monsanto, J. Reich, J. Rexford,
C. Schlesinger, D. Walker, and R. Harrison. Languages for
software-deﬁned networks. IEEE Communications
Magazine, 51(2):128–134, February 2013.
[14] L. Foundation. Opendaylight.
http://www.opendaylight.org/.
[15] O. N. Foundation. Openﬂow switch speciﬁcation version
1.4.0, October 2013.
https://www.opennetworking.org/images/stories/
downloads/sdn-resources/onf-specifications/openflow/
openflow-spec-v1.4.0.pdf.
[16] X. Ge, Y. Liu, D. H. Du, L. Zhang, H. Guan, J. Chen,
Y. Zhao, and X. Hu. OpenANFV: Accelerating network
function virtualization with a consolidated framework in
openstack. In SIGCOMM, pages 353–354, 2014.
[17] A. Gember, A. Krishnamurthy, S. S. John, R. Grandl,
X. Gao, A. Anand, T. Benson, A. Akella, and V. Sekar.
Stratos: A network-aware orchestration layer for
middleboxes in the cloud. CoRR, abs/1305.0209, 2013.
[18] A. Gember-Jacobson, R. Viswanathan, C. Prakash,
R. Grandl, J. Khalid, S. Das, and A. Akella. OpenNF:
enabling innovation in network function control. In
SIGCOMM, pages 163–174, 2014.
[19] J. Gross, T. Sridhar, P. Garg, C. Wright, I. Ganga,
P. Agarwal, K. Duda, D. Dutt, and J. Hudson. Geneve:
Generic network virtualization encapsulation. IETF
Internet-Draft, November 2015. https:
//tools.ietf.org/html/draft-ietf-nvo3-geneve-00.
[20] N. Handigol, B. Heller, V. Jeyakumar, D. Mazi`eres, and
N. McKeown. I know what your packet did last hop: Using
packet histories to troubleshoot networks. In NSDI, pages
71–85, 2014.
[21] W. John, C. Meirosu, B. Pechenot, P. Skoldstrom,
P. Kreuger, and R. Steinert. Scalable Software Deﬁned
Monitoring for Service Provider DevOps. In EWSDN,
pages 61–66, 2015.
[22] A. R. Khakpour and A. X. Liu. First step toward
cloud-based ﬁrewalling. In SRDS, pages 41–50, 2012.
[23] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M. F.
Kaashoek. The click modular router. ACM Trans. Comput.
Syst., 18(3):263–297, Aug 2000.
[24] M. Mahalingam, D. Dutt, K. Duda, P. Agarwal,
L. Kreeger, T. Sridhar, M. Bursell, and C. Wright. Virtual
extensible local area network. IETF Internet-Draft, August
2014. https://tools.ietf.org/html/rfc7348.
[25] D. A. Maltz, J. Zhan, G. G. Xie, H. Zhang,
G. Hj´almt´ysson, A. G. Greenberg, and J. Rexford.
Structure preserving anonymization of router conﬁguration
data. In IMC, pages 239–244, 2004.
[26] J. Martins, M. Ahmed, C. Raiciu, V. Olteanu, M. Honda,
R. Bifulco, and F. Huici. ClickOS and the art of network
function virtualization. In NSDI, pages 459–473, 2014.
[27] R. Mijumbi, J. Serrat, J. Gorricho, N. Bouten,
F. De Turck, and R. Boutaba. Network function
virtualization: State-of-the-art and research challenges.
IEEE Comm. Surveys Tutorials, 18(1):236–262, 2016.
[28] Mininet. http://mininet.org/.
[29] OpenBox Project Source Code.
https://github.com/OpenBoxProject.
[30] OpenMANO. https://github.com/nfvlabs/openmano.
[31] OpenStack open source cloud computing software.
https://www.openstack.org/.
[32] OpNFV. https://www.opnfv.org/.
[33] S. Palkar, C. Lan, S. Han, K. Jang, A. Panda,
S. Ratnasamy, L. Rizzo, and S. Shenker. E2: a framework
for NFV applications. In SOSP, pages 121–136, 2015.
[34] P. Prakash, M. Lee, Y. C. Hu, R. R. Kompella, J. Wang,
and S. Dassarma. Jumbo frames or not: That is the
question! Technical Report 13-006, Purdue University,
Twitter, 2013.
[35] Openbox framework speciﬁcation, January 2016.
http://www.deepness-lab.org/pubs/
OpenBoxSpecification1.1.0.pdf.
[36] Z. A. Qazi, C.-C. Tu, L. Chiang, R. Miao, V. Sekar, and
M. Yu. SIMPLE-fying middlebox policy enforcement using
SDN. In SIGCOMM, pages 27–38, 2013.
[37] P. Quinn, P. Agarwal, R. Manur, R. Fernando, J. Guichard,
S. Kumar, A. Chauhan, M. Smith, N. Yadav, and
B. McConnell. Network service header. IETF
Internet-Draft, February 2014. https:
//datatracker.ietf.org/doc/draft-quinn-sfc-nsh.
[38] V. Sekar, N. Egi, S. Ratnasamy, M. K. Reiter, and G. Shi.
Design and implementation of a consolidated middlebox
architecture. In NSDI, pages 323–336, 2012.
[39] J. Sherry and S. Ratnasamy. A survey of enterprise
middlebox deployments. Technical Report
UCB/EECS-2012-24, UC Berkeley, 2012.
[40] Snort users manual 2.9.7. http://manual.snort.org/.
[41] R. Stuhlmuller. Micro-Segmentation: VMware NSX’s Killer
Use Case, June 2014.
https://blogs.vmware.com/networkvirtualization/2014/
06/micro-segmentation-vmware-nsx.html.
[42] D. E. Taylor. Survey and taxonomy of packet classiﬁcation
techniques. ACM Comput. Surv., 37(3):238–275, Sept.
2005.
524