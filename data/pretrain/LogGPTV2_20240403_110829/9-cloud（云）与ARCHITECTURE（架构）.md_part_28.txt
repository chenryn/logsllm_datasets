\/var/hadoop\
\
\
\ha.zookeeper.quorum\
\node1:2181,node2:2181,node3:2181\ //zookeepe的地址
\
\
\hadoop.proxyuser.nfs.groups\
\\*\
\
\
\hadoop.proxyuser.nfs.hosts\
\\*\
\
\
9）配置 hdfs-site
\[root@nn01 \~\]# vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml
\
\
\dfs.replication\
\2\
\
\
\dfs.nameservices\
\nsdcluster\
\
\
\dfs.ha.namenodes.nsdcluster\
//nn1,nn2名称固定，是内置的变量，nsdcluster里面有nn1，nn2
\nn1,nn2\
\
\
\dfs.namenode.rpc-address.nsdcluster.nn1\
//声明nn1 8020为通讯端口，是nn01的rpc通讯端口
\nn01:8020\
\
\
\dfs.namenode.rpc-address.nsdcluster.nn2\
//声明nn2是谁，nn02的rpc通讯端口
\nn02:8020\
\
\
\dfs.namenode.http-address.nsdcluster.nn1\
//nn01的http通讯端口
\nn01:50070\
\
\
\dfs.namenode.http-address.nsdcluster.nn2\
//nn01和nn02的http通讯端口
\nn02:50070\
\
\
\dfs.namenode.shared.edits.dir\
//指定namenode元数据存储在journalnode中的路径
\qjournal://node1:8485;node2:8485;node3:8485/nsdcluster\
\
\
\dfs.journalnode.edits.dir\
//指定journalnode日志文件存储的路径
\/var/hadoop/journal\
\
\
\dfs.client.failover.proxy.provider.nsdcluster\
//指定HDFS客户端连接active namenode的java类
\org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\
\
\
\dfs.ha.fencing.methods\ //配置隔离机制为ssh
\sshfence\
\
\
\dfs.ha.fencing.ssh.private-key-files\ //指定密钥的位置
\/root/.ssh/id_rsa\
\
\
\dfs.ha.automatic-failover.enabled\ //开启自动故障转移
\true\
\
\
10）配置yarn-site
\[root@nn01 \~\]# vim /usr/local/hadoop/etc/hadoop/yarn-site.xml
\
\
\
\yarn.nodemanager.aux-services\
\mapreduce_shuffle\
\
\
\yarn.resourcemanager.ha.enabled\
\true\
\
\
\yarn.resourcemanager.ha.rm-ids\ //rm1,rm2代表nn01和nn02
\rm1,rm2\
\
\
\yarn.resourcemanager.recovery.enabled\
\true\
\
\
\yarn.resourcemanager.store.class\
\org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore\
\
\
\yarn.resourcemanager.zk-address\
\node1:2181,node2:2181,node3:2181\
\
\
\yarn.resourcemanager.cluster-id\
\yarn-ha\
\
\
\yarn.resourcemanager.hostname.rm1\
\nn01\
\
\
\yarn.resourcemanager.hostname.rm2\
\nn02\
\
\
11）同步到nn02，node1，node2，node3
\[root@nn01 \~\]# for i in {22..25}; do rsync -aSH \--delete
/usr/local/hadoop/ 192.168.1.\$i:/usr/local/hadoop -e \'ssh\' & done
\[1\] 25411
\[2\] 25412
\[3\] 25413
\[4\] 25414
12）删除所有机器上面的/user/local/hadoop/logs，方便排错
\[root@nn01 \~\]# for i in {21..25}; do ssh 192.168.1.\$i rm -rf
/usr/local/hadoop/logs ; done
13）同步配置
\[root@nn01 \~\]# for i in {22..25}; do rsync -aSH \--delete
/usr/local/hadoop 192.168.1.\$i:/usr/local/hadoop -e \'ssh\' & done
\[1\] 28235
\[2\] 28236
\[3\] 28237
\[4\] 28238
5 案例4：高可用验证
5.1 问题
本案例要求：
初始化集群
验证集群
5.2 步骤
实现此案例需要按照如下步骤进行。
步骤一：验证hadoop的高可用
1）初始化ZK集群
\[root@nn01 \~\]# /usr/local/hadoop/bin/hdfs zkfc -formatZK
\...
18/09/11 15:43:35 INFO ha.ActiveStandbyElector: Successfully created
/hadoop-ha/nsdcluster in ZK //出现Successfully即为成功
\...
2）在node1，node2，node3上面启动journalnode服务（以node1为例子）
\[root@node1 \~\]# /usr/local/hadoop/sbin/hadoop-daemon.sh start
journalnode
starting journalnode, logging to
/usr/local/hadoop/logs/hadoop-root-journalnode-node1.out
\[root@node1 \~\]# jps
29262 JournalNode
26895 QuorumPeerMain
29311 Jps
3）格式化，先在node1，node2，node3上面启动journalnode才能格式化
\[root@nn01 \~\]# /usr/local/hadoop//bin/hdfs namenode -format
//出现Successfully即为成功
\[root@nn01 hadoop\]# ls /var/hadoop/
dfs
4）nn02数据同步到本地 /var/hadoop/dfs
\[root@nn02 \~\]# cd /var/hadoop/
\[root@nn02 hadoop\]# ls
\[root@nn02 hadoop\]# rsync -aSH nn01:/var/hadoop/ /var/hadoop/
\[root@nn02 hadoop\]# ls
dfs
5）初始化 JNS
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hdfs namenode
-initializeSharedEdits
18/09/11 16:26:15 INFO client.QuorumJournalManager: Successfully started
new epoch 1 //出现Successfully，成功开启一个节点
6）停止 journalnode 服务（node1，node2，node3）
\[root@node1 hadoop\]# /usr/local/hadoop/sbin/hadoop-daemon.sh stop
journalnode
stopping journalnode
\[root@node1 hadoop\]# jps
29346 Jps
26895 QuorumPeerMain
步骤二：启动集群
1）nn01上面操作
\[root@nn01 hadoop\]# /usr/local/hadoop/sbin/start-all.sh //启动所有集群
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on \[nn01 nn02\]
nn01: starting namenode, logging to
/usr/local/hadoop/logs/hadoop-root-namenode-nn01.out
nn02: starting namenode, logging to
/usr/local/hadoop/logs/hadoop-root-namenode-nn02.out
node2: starting datanode, logging to
/usr/local/hadoop/logs/hadoop-root-datanode-node2.out
node3: starting datanode, logging to
/usr/local/hadoop/logs/hadoop-root-datanode-node3.out
node1: starting datanode, logging to
/usr/local/hadoop/logs/hadoop-root-datanode-node1.out
Starting journal nodes \[node1 node2 node3\]
node1: starting journalnode, logging to
/usr/local/hadoop/logs/hadoop-root-journalnode-node1.out
node3: starting journalnode, logging to
/usr/local/hadoop/logs/hadoop-root-journalnode-node3.out
node2: starting journalnode, logging to
/usr/local/hadoop/logs/hadoop-root-journalnode-node2.out
Starting ZK Failover Controllers on NN hosts \[nn01 nn02\]
nn01: starting zkfc, logging to
/usr/local/hadoop/logs/hadoop-root-zkfc-nn01.out
nn02: starting zkfc, logging to
/usr/local/hadoop/logs/hadoop-root-zkfc-nn02.out
starting yarn daemons
starting resourcemanager, logging to
/usr/local/hadoop/logs/yarn-root-resourcemanager-nn01.out
node2: starting nodemanager, logging to
/usr/local/hadoop/logs/yarn-root-nodemanager-node2.out
node1: starting nodemanager, logging to
/usr/local/hadoop/logs/yarn-root-nodemanager-node1.out
node3: starting nodemanager, logging to
/usr/local/hadoop/logs/yarn-root-nodemanager-node3.out
2）nn02上面操作
\[root@nn02 hadoop\]# /usr/local/hadoop/sbin/yarn-daemon.sh start
resourcemanager
starting resourcemanager, logging to
/usr/local/hadoop/logs/yarn-root-resourcemanager-nn02.out
3）查看集群状态
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hdfs haadmin
-getServiceState nn1
active
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hdfs haadmin
-getServiceState nn2
standby
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/yarn rmadmin
-getServiceState rm1
active
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/yarn rmadmin
-getServiceState rm2
standby
4）查看节点是否加入
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hdfs dfsadmin -report
\...
Live datanodes (3): //会有三个节点
\...
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/yarn node -list
Total Nodes:3
Node-Id Node-State Node-Http-Address Number-of-Running-Containers
node2:43307 RUNNING node2:8042 0
node1:34606 RUNNING node1:8042 0
node3:36749 RUNNING node3:8042 0
步骤三：访问集群
1）查看并创建
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hadoop fs -ls /
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hadoop fs -mkdir /aa
//创建aa
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hadoop fs -ls / //再次查看
Found 1 items
drwxr-xr-x - root supergroup 0 2018-09-11 16:54 /aa
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hadoop fs -put \*.txt /aa
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hadoop fs -ls
hdfs://nsdcluster/aa
//也可以这样查看
Found 3 items
-rw-r\--r\-- 2 root supergroup 86424 2018-09-11 17:00
hdfs://nsdcluster/aa/LICENSE.txt
-rw-r\--r\-- 2 root supergroup 14978 2018-09-11 17:00
hdfs://nsdcluster/aa/NOTICE.txt
-rw-r\--r\-- 2 root supergroup 1366 2018-09-11 17:00
hdfs://nsdcluster/aa/README.txt
2）验证高可用，关闭 active namenode
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hdfs haadmin
-getServiceState nn1
active
\[root@nn01 hadoop\]# /usr/local/hadoop/sbin/hadoop-daemon.sh stop
namenode
stopping namenode
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hdfs haadmin
-getServiceState nn1
//再次查看会报错
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hdfs haadmin
-getServiceState nn2
//nn02由之前的standby变为active
active
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/yarn rmadmin
-getServiceState rm1
active
\[root@nn01 hadoop\]# /usr/local/hadoop/sbin/yarn-daemon.sh stop
resourcemanager
//停止resourcemanager
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/yarn rmadmin
-getServiceState rm2
active
3） 恢复节点
\[root@nn01 hadoop\]# /usr/local/hadoop/sbin/hadoop-daemon.sh start
namenode
//启动namenode
\[root@nn01 hadoop\]# /usr/local/hadoop/sbin/yarn-daemon.sh start
resourcemanager
//启动resourcemanager
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/hdfs haadmin
-getServiceState nn1
//查看
\[root@nn01 hadoop\]# /usr/local/hadoop/bin/yarn rmadmin
-getServiceState rm1