1.74
1.54
1.91
2.19
2.52
2.62
1.70
2.15
1.84
2.28
1.90
3.04
6.66
2.27
1.83
1.95
1.82
1.65
1.44
1.84
2.08
2.32
2.21
1.53
2.05
1.42
2.10
1.72
1.72
6.34
2.04
1.76
1.80
1.64
1.54
1.34
1.6
1.80
2.04
1.40
0.69
1.40
0.55
1.27
0.95
0.84
5.95
1.09
1.06
1.05
0.84
0.82
0.50
1.05
1.10
1.29
Rbuf
39.70 %
33.49 %
22.11 %
56.87 %
32.96 %
27.09 %
74.78 %
59.62 %
56.69 %
26.38 %
6.59 %
7.79 %
21.51 %
21.40 %
28.08 %
11.00 %
32.88 %
# DFT
operands
2.69
4.52
3.30
5.57
2.77
3.56
3.57
6.64
3.88
3.19
3.31
4.47
4.07
5.97
5.57
5.42
4.28
Time
(sec.)
156.66
141.79
89.78
8341.53
109.60
674.74
242.67
115.81
58.09
57.62
234.35
421.10
81.51
1189.45
706.63
1486.60
881.74
Table 1: Results from static analysis. Unopt indicates the number of enqueue operations that a naive implementation requires. The
Intra, DFT, Inter, and Exec columns correspond to the number of enqueue operations after progressively applying our intra-block,
DFT, inter-block, and control ﬂow optimizations. Rbuf shows the percentage of BBLs that need to be instrumented with analysis
code that checks for ring buffer overﬂows. # DFT operands shows the operands from the secondary’s analysis body. The last column,
Time, shows the time for ofﬂine analysis.
heavily depends on its size. The version of Pin used during the eval-
uation was 2.11 (build 49306). While conducting our experiments
hyper-threading was disabled and the host was idle.
6.1 Effectiveness of Optimizations
Table 1 summarizes the effects our optimizations had on reduc-
ing the amount of information that needs to be communicated to
the secondary, and the effectiveness of the ring buffer fast checking
optimization. The # BBLs column indicates the number of distinct
BBLs discovered for each application, using the proﬁling process
outlined in Sec. 3.1. # Ins. gives the average number of instruc-
tions per BBL. The Unopt column shows the average number of
enqueue operations to the ring buffer that a naive implementation
would require per BBL. The Intra, DFT, Inter, and Exec columns
correspond to the number of enqueue operations after progressively
applying the intra-block, DFT, and inter-block optimizations pre-
sented in Sec. 3.2.1, as well as the optimizations related to control
ﬂow recording (Sec. 3.2.2). Rbuf shows the percentage of BBLs
that need to be instrumented with checks for testing if the ring
buffer is full, according to the fast checking algorithm (Sec. 3.2.3).
# DFT operands, shows per BBL average number of register and
memory operands appeared from the secondary’s analysis body.
The last column, Time shows the time for ofﬂine analysis to gener-
ate codes for the primary and the secondary.
On average, a naive implementation would require 4.17 enqueue
operations per BBL, for communicating EAs and control ﬂow in-
formation to the secondary, and instrument 100% of the BBLs with
code that checks for ring buffer overﬂows. Our optimizations re-
duce the number of enqueueing operations to a mere 1.29 per BBL,
while only 32.88% of BBLs are instrumented to check for ring
buffer overﬂows. A reduction of 67.12%.
While ofﬂine analysis requires a small amount of time (on aver-
age 881 sec.) for most programs, 403.gcc takes exceptionally long
(8341 sec.) to complete. This is due to the program structure, which
has a particularly dense CFG, thus making the Rbuf optimization
wasting a long time (∼ 6500 sec.) only to enumerate all available
)
d
e
z
i
l
a
m
r
o
n
(
n
w
o
d
w
o
l
S
 5
 4
 3
 2
 1
 2.5
 1.5
)
d
e
z
i
l
a
m
r
o
n
(
n
w
o
d
w
o
l
S
 2
 1
 0.5
(a) bzip2
compress
decompress
Unopt
Intra
DFT
Inter
Exec
RBuf
(b) tar
archive
extract
Unopt
Intra
DFT
Inter
Exec
RBuf
Figure 6: The slowdown of the primary process imposed by
ShadowReplica, and the effects of our optimizations, when run-
ning bzip2 and tar.
cycles to perform the analysis described in Sec. 3.2.3. We believe
that this can be alleviated by parallelizing the algorithm.
To evaluate the impact of our various optimizations on the pri-
mary’s performance, we used two commonly used Unix utilities,
tar and bzip2. We selected these two because they represent dif-
ferent workloads. tar performs mostly I/O, while bzip2 is CPU-
bound. We run the GNU versions of the utilities natively and over
ShadowReplica, progressively enabling our optimizations against a
Linux kernel source “tarball” (v3.7.10; ∼476MB).
We measured their execution time with the Unix time utility
and draw the obtained results in Fig. 6. Unopt corresponds to the
runtime performance of an unoptimized, naive, implementation,
whereas Intra, DFT, Inter, Exec, and RBuf demonstrate the bene-
ﬁts of each optimization scheme, when applied cumulatively. With
243 12
 11
 10
)
d
e
z
i
l
a
m
r
o
n
(
n
w
o
d
w
o
l
S
 9
 8
 7
 6
 5
 4
 3
 2
 1
astar
gcc
bzip2
gob
mk
h264
h
m
mer
libquantu
mcf
m
Benchmark
Primary
Primary+Secondary
In-line
o
m
perl
netpp
sjeng
xalan
avg
Figure 7: Running the SPEC CPU2006 benchmark suite with
all optimizations enabled. Primary denotes the slowdown of the
primary thread alone. Primary+Secondary is the overhead of
ShadowReplica when it performs full-ﬂedged DFT. In-line cor-
responds to the slowdown imposed when the DFT process exe-
cutes in-line with the application.
all optimizations enabled, the slowdown imposed to bzip2 drops
from 5×/4.13× down to 2.23×/1.99× for compress/decompress
(55%/51.82% reduction). Similarly, tar goes from 2.26×/1.94×
down to 1.71×/1.53× for archive/extract (24.33%/ 21.13% reduc-
tion).
It comes as no surprise that I/O bound workloads, which
generally also suffer smaller overheads when running with in-lined
DFT, beneﬁt less from ShadowReplica. We also notice that Intra
and RBuf optimizations have larger impact on performance.
6.2 Performance
SPEC CPU2006. Fig. 7 shows the overhead of running the
SPEC CPU2006 benchmark suite under ShadowReplica, when all
optimizations are enabled. Primary corresponds to the slowdown
imposed by the primary thread alone. Primary+Secondary is the
overhead of ShadowReplica when both the primary and secondary
threads and running, and the secondary performs full-ﬂedged DFT.
Finally, In-line denotes the slowdown imposed when the DFT pro-
cess executes in-line with the application, under our accelerated
DFT implementation [18]. On average, Primary imposes a 2.72×
slowdown on the suite, while the overhead of the full scheme (Pri-
mary+Secondary) is 2.75×. In-line exhibits a 6.37× slowdown, in-
dicating the beneﬁts from the decoupled and parallelized execution
of the DFT code (56.82% reduction on the performance penalty).
During our evaluation, we noticed that for some benchmarks (as-
tar, perlbmk, gcc, sjeng) the slowdown was not bound to the pri-
mary, but to the secondary. These programs generally required a
high number of DFT operands to be sent to the secondary (# DFT
operands from Table 1) compared to the number of enqueued en-
tries.
The signiﬁcant slowdown with the h264ref benchmark was due
to the way Pin handles rep-preﬁxed string instructions (i.e., rep
movs, rep cmps), heavily used in this benchmark. Pin assumes
that the tool developer wants to instrument each one of these rep-
etitions and transforms these instructions into explicit loops, in-
troducing additional overhead. However, our DFT-speciﬁc repre-
sentation [18], which captures source, destination, and offset from
these instructions, does not require such a transformation. We used
)
d
e
z
i
l
a
m
r
o
n
(
n
w
o
d
w
o
l
S
 2
 1.8
 1.6
 1.4
 1.2
 1
 0.8
(a) Apache
Plaintext
SSL
1KB
10KB 100KB
1MB
File size
)
d
e
z
i
l
a
m