(1999)
MS TPC-C
(2009)
Disk
src11
usr1
proj2
prn1
c6t8d0
c6t5d1
c6t5d0
c3t3d0
disk66
disk88
Mean (s)
Variance
CoV
0.4640
0.0997
0.1384
0.2280
0.1502
0.4503
0.4345
0.4555
0.0014
0.0015
Table II
101.31
0.7448
772.18
8.3073
4.3243
180.13
15.545
14.051
1.5e-6
1.6e-6
21.693
8.6516
200.75
12.641
13.845
29.807
9.0731
8.2301
0.8608
0.8785
SNIA TRACE IDLE INTERVAL DURATION ANALYSIS RESULTS
varying intensity based on their causes that could be any:
from scheduled tasks to applications and/or human activity.
For a more statistically rigorous approach to periods, we
used analysis of variance (ANOVA) to identify the time
interval with the strongest periodic behavior for each trace
in our data set. The results are shown in Fig. 9. We observe
that for most traces ANOVA does identify periods, most
commonly at intervals of 24 hours (our analysis was done
at the granularity of hours, so periods of one hour in Fig. 9
mean there was no periodicity identiﬁed).
Autocorrelation: Autocorrelation is an interesting statis-
tical property, as it means that the length of previous (recent)
idle intervals is predictive of the lengths of future idle
intervals; information that a scheduler could use to identify
long idle intervals, in which to schedule scrub requests.
Previous work [19] has reported evidence of autocorrelation
for some (not publicly available) disk traces in the form of
Hurst parameter values larger than 0.5. We studied the auto-
correlation function for all our traces and found that 44 out
of the busiest 63 disk traces exhibit strong autocorrelation.
Decreasing hazard rates and long tails: Previous
work [19] has reported that the distribution of I/O request
inter-arrival times exhibits high variability. We veriﬁed that
this is also the case for our traces. We observe Coefﬁcients
of Variation4 typically in the 10–30 range (Table II), and in
one case as high as 200. These numbers are even higher than
those reported in [19], who observed a CoV of 19 for their
most variable trace. To put those numbers in perspective,
we remind the reader that an exponential distribution has
a CoV of 1. The exponential distribution is a memoryless
distribution, i.e. if idle times were exponentially distributed,
then the expected time until the next request arrival would
always be the same, independent of the time since the last
arrival. We believe this is unrepresentative of real workloads,
and found it to be the case only for the TPC-C traces.
The high CoV values signify long tails in the distribution
of idle intervals. In the context of our work, a long tail
implies that a large fraction of the system’s total idle time is
concentrated in a small fraction of very long idle intervals.
4Recall that the Coefﬁcient of Variation (CoV) is deﬁned as the standard
deviation divided by the mean.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:48:41 UTC from IEEE Xplore.  Restrictions apply. 
e
m
i
t
e
d
l
i
l
a
t
o
t
f
o
n
o
i
t
c
a
r
F
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
MSRsrc11
MSRusr1
HPc6t5d1
HPc6t8d0
103
102
101
100
)
s
(
i
i
g
n
n
a
m
e
r
e
m
10−1
i
t
e
d
l
i
d
e
t
c
e
p
x
E
10−2
10−3
10−4
MSRsrc11
MSRusr1
HPc6t5d1
HPc6t8d0
TPCdisk66
TPCdisk88
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
0.45
0.50
Fraction of largest idle intervals
10−6
10−5
10−4
10−3
10−2
10−1
100
101
102
Amount of idle time passed (s)
Figure 10. What fraction of a disk’s idle time do the largest idle periods
make up? Note that the x axis is cut off at the 50th percentile.
Figure 11. Expected idle time remaining for the traces in Table I
)
s
(
100
For a closer study of this characteristic, Fig. 10 plots the
fraction of the total idle time in the system that is made up
by the longest idle intervals, i.e. each data point (x, y) shows
that the x% largest idle intervals in the system account for
y% of the total idle time in the system. We observed that for
all traces a very large fraction of idle time is concentrated
in the tail of the idle time distribution: typically more than
80% of the idle time is included in less than 15% of the idle
intervals, in most cases the skew is even stronger.
The strong weight in the tail of the distributions is good
news for scheduling background workloads. It means that
if we can identify the 15% largest intervals, we can make
use of more than 80% of the total idle time in the system.
Scheduling background work for only a small percentage of
the idle intervals is advantageous for limiting the number of
possible collisions, where a foreground request arrives while
a background request is in progress.
An obvious problem that occurs at this point is identifying
at the beginning of an idle interval, whether it is going to
be one of the few very large ones (and hence, scrubbing
should be initiated). We can derive two ideas for identifying
long idle intervals from our previous two observations:
since there is periodicity in the data, we could schedule
background work only during those times of the day that
tend to be lightly loaded; alternatively, since there is strong
autocorrelation in the data, we could use auto-regression to
predict the length of upcoming idle intervals based on the
lengths of previous ones.
The high CoVs we observed in our traces suggest a third
option for predicting idle times: it is possible that the CoVs
in our traces are so much higher than that for an exponential
distribution, because their empirical idle time distributions
have decreasing hazard rates, i.e. the longer the system has
been idle, the longer it is expected to stay idle. In this case,
an approach based on waiting might work well, where we
wait until the system has been idle for a certain amount of
time before we issue a scrub request.
i
i
g
n
n
a
m
e
r
e
m
i
t
e
d
l
i
f
o
.
c
r
e
p
10−1
10−2
10−3
10−4
10−5
MSRsrc11
MSRusr1
HPc6t5d1
HPc6t8d0
t
s
1
10−6
10−6
10−5
10−4
10−3
10−2
10−1
100
101
102
Amount of idle time passed (s)
Figure 12. 1st percentile of idle time remaining for the traces in Table I
i
i
g
n
n
a
m
e
r
e
m
l
i
t
e
d
i
f
o
n
o
i
t
c
a
r
F
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
10−6
10−5
10−4
MSRsrc11
MSRusr1
HPc6t5d1
HPc6t8d0
TPCdisk66
TPCdisk88
102
103
104
10−3
10−2
101
Amount of idle time passed (s)
10−1
100
Figure 13. Fraction of idle time remaining for the traces in Table I
be idle in expectation for an additional y seconds before the
next request arrives. We observe that the lines for all Cello
and MSR traces are continuously increasing. In fact, having
been idle for a long time increases the expected remaining
idle time by several orders of magnitude (note the log scale
on the y-axis). Since the expected remaining idle time is
just an average (on average after waiting for x seconds it
will take another y seconds until the next foreground request
arrives) which might be biased by outliers, we also plotted
the ﬁrst percentile of remaining time in Fig. 12. A data point
(x, y) in this graph means that in 99% of the cases, after
waiting for x seconds we have at least another y seconds
before the next foreground request arrives. We again note
strongly increasing trends.
To check for decreasing hazard rates we plotted the
expected remaining idle time, as a function of how long the
disk has been idle (Fig. 11). A data point (x, y) in the graph
means that after being idle for x seconds, the system will
One potential problem with the wait-based approach is
that we miss out on using the idle time that passes while
we wait. Fig. 13 plots the fraction of the total idle time
in the system that we can exploit
if we only schedule
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:48:41 UTC from IEEE Xplore.  Restrictions apply. 
scrub requests after waiting for x seconds. The ﬁgure shows
that even after waiting for some time before issuing scrub
requests, we can still make use of a signiﬁcant fraction of
the total idle time. For example, for a wait time on the
order of 100msec we can still make use of more than 60-
90% of the total idle time in the system, depending on the
trace. At the same time, the number of possible collisions
between arriving foreground and background requests is
limited, since less than 10% of all idle intervals in our traces
are larger than 100msec and will be picked for scrubbing.
Finally, our observation of decreasing hazard rates has
another implication for designing a scheduler for scrub
requests. Existing approaches for scheduling background
requests [7], [8] consist of a method for identifying a start-
ing criterion (when to start issuing background requests),
and a stopping criterion (when to stop issuing background
requests). Fig. 11 tells us that we need not worry about
a stopping criterion. The goal of background scheduling
policies is to schedule background requests when the chance
of a foreground request arriving is low. Decreasing hazard
rates, however, imply that the chance of a foreground request
arriving at any given moment diminishes with time, past the
beginning of the idle interval. Therefore, once scrubbing is
initiated, if the system is still idle upon the completion of a
scrub request, the chance of a foreground request arriving is
even lower than before issuing the scrub request. This means
that once an idle interval is identiﬁed as long, the policy that
makes most sense statistically is to keep sending background
requests, until the next foreground request arrives.
B. Proﬁting from idleness
In this section we deﬁne and evaluate three different
policy classes for scheduling scrub requests, which have all
been directly motivated by our results in Section V-A.
1) Autoregression (AR) – Predicting the future: The
strong periods and autocorrelation we observed in our anal-
ysis in Section V-A motivated us to look into approaches
that capture repetitive patterns. We examined autoregressive
(AR) models, which use successive observations of an event
to express relationships between a dependent and one or
more independent variables. In our experiments we used the
simple AR(p) model, which regresses a request inter-arrival
interval of length Xt against past intervals Xt−1, ..., Xt−p:
Xt = µ +
p
X
i=1
ai(Xt−i − µ) + ǫt,
(1)
where ai, ..., ap are parameters of the model, ǫt is white
noise, and µ expresses a mean calculated over past inter-
vals. We estimate the order p using Akaike’s Information
Criterion [22], that optimizes the ratio between the number
of parameters and the accuracy of the resulting model. Since
AR models can only be applied to regular time series, i.e.
sequences of events recorded at regular intervals, we model
the durations of request inter-arrival intervals [23]. This also
implies that AR predictions are estimations of the amount
of time until the arrival of the next request. We attempted
to ﬁt several AR models to our data, including ACD [24]
and ARIMA [25], and found that AR(p) is the only model
that can be ﬁtted quickly and efﬁciently to the millions of
samples that need to be factored at the I/O level.
Our AR policy works by predicting the length of
idle interval Xt based on previous intervals
the current
Xt−1, ..., Xt−p using the AR(p) model. The policy makes
the prediction for Xt at the beginning of the current idle
interval and starts ﬁring scrub requests if the prediction Xt
is larger than some speciﬁed time threshold c, which is a
parameter of the policy. Once it starts issuing scrub requests
it continues until a foreground request arrives.
2) Waiting – Playing the waiting game: The decreasing
hazard rates in our traces imply that after the system has
been idle for a while, it will likely remain idle. This property
is exploited by the Waiting policy, which dictates that no
requests are to be issued, unless the system has remained idle
for some time t (t is a parameter of the policy). Requests stop
being issued only upon the arrival of a foreground request.
3) AR+Waiting – Combining auto-regression and wait-
ing: This policy combines the Auto-regression and Waiting
approaches. It waits for a time threshold t and if the system
has been idle for that long it starts ﬁring, provided that the
auto-regression prediction for the length of this idle interval
is larger than some time threshold c.
Comparison of policies: Naturally, for all policies there is
a trade-off in the throughput that the scrubber achieves, and
the resulting impact on the performance of the foreground
trafﬁc. The trade-off is governed by the parameters of the
policies, so choosing larger values for parameters c and t of
the AR and Waiting policies, respectively, will lead to lower
impact on the foreground trafﬁc at the cost of reduced scrub
throughput. For a fair comparison of the different policies,
we need to ﬁnd which one achieves highest scrub throughput
for a given ﬁxed penalty to the foreground trafﬁc.
Therefore, we compare policies by varying their corre-
sponding parameters, and plotting the resulting amount of
idle time that can be utilized for scrubbing versus the number
of resulting collisions (the fraction of foreground requests
that are delayed due to a scrub request in progress). The re-
sults are shown in Fig. 14: MSRusr2 (right) is representative
of most disks in our trace collections, while HPc6t8d0 (left)
is characterized by multiple short idle intervals, representing
a worst case scenario with respect to collisions. The numbers
on top of the data points provide the parameter setting used
to derive that data point: the wait time threshold t for the
Waiting approach and the threshold c for the AR approach.
In addition to AR and Waiting, we also plot results for
the combined approach. For that, we experiment with four
different values of the c parameter for the AR component.
These four values were chosen to be the 20th, 40th, 60th,
and 80th percentile of all observed AR values for a trace.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:48:41 UTC from IEEE Xplore.  Restrictions apply. 
d
e
z
i
l
i
t
u
e
m
i
t
l
e
d
i
f
o
n
o
i
t
c
a
r
F
1.0
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0.0
1024ms
2048ms
1024ms
2048ms
128ms
256ms
512ms
1024ms
2048ms
512ms
512ms
256ms
256ms
64ms
128ms
128ms
32ms
64ms
64ms