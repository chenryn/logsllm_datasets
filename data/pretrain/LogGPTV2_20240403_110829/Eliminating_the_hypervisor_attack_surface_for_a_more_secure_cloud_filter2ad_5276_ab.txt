and timers must ﬁrst go through the hypervisor which will
deliver the interrupts to the VM by utilizing its knowledge
of the VM’s current location.
2.2 Attack Surface
The virtualization layer is heavily involved throughout the
lifetime of the guest VM. Each interaction between the VM
and the hypervisor is then a potential attack vector that
could be exploited by the guest. The guest OS interacts di-
rectly with the hypervisor ( 5(cid:13)) and indirectly with the host
OS and the emulator ( 6(cid:13)) through VM exits (and hyper-
calls if paravirtualization is used). A VM exit is an event
which occurs when the VM’s code is interrupted and the hy-
pervisor code begins to execute to handle some event (e.g.,
emulate memory access, deliver a virtual timer interrupt,
etc.). A hypercall is similar to a system call and is used by
guest VMs to request explicit service from the hypervisor.
Hypercalls are not considered further as we are not using
paravirtualization. VM exits are quite frequent even when
the guest OS inside the VM is not doing any work; in an idle
VM running on top of Xen 4.0, the VM exits occur ∼600
times per second.
On a 64-bit Intel x86 architecture with virtualization ex-
tensions, there are 56 reasons for VM exits, and this forms
the large attack surface which is the basis of the security
threat. The reasons VM for exits are described in Table
1. Each VM exit causes the hypervisor code to run so the
hypervisor can intervene when the guest OS performs some
operation that caused the associated exit. This allows the
hypervisor to maintain the abstraction of the system which
it presents to the guest OSes, for example it can return dif-
ferent CPUID values than the values actually reported by
the hardware.
In order to emphasize how often the large and complex
1APIC is the advanced programmable interrupt controller.
403virtualization layer is needed, we examine the execution of
an actual (non-paravirtualized) VM with the Xen hypervi-
sor. Figure 2 shows a timeline of a Linux based VM pinned
to a core and with a directly assigned NIC, booting up and
running some programs on top of the Xen 4.0 hypervisor
with no other VMs present. The most frequent reasons for
exits are highlighted in the ﬁgure while others are grouped
in the “other” category. The stacked area graph shows the
number of VM exits during each 250ms interval.
First, the guest OS boots up, which is the ﬁrst 75 seconds
in the graph. A staggering 9,730,000 exits are performed
as the system is starting up. This is an average of about
130,000 exits per second. The majority of the exits are the
EPTV and EXCNMI VM exits, but exits such as CPUID
are also present. These exits are due to interaction with
hardware that is being emulated (EPTV and EXCNMI) or
are part of system discovery done by the guest OS (CPUID).
Next, at around 90 seconds we show an SSH login event
which causes a spike in VM exits (EXTINT and APICACC).
Even with the directly assigned NIC, the hypervisor is in-
volved in the redirection of the interrupts to the correct core
and the EXTINT exit signals when an interrupt has come.
The APICACC exits are due to the guest interacting with
the local APIC (Advanced Programmable Interrupt Con-
troller) which is used to acknowledge receipt of an interrupt,
for example.
Then, at around 115 seconds we show execution of the
Apache web server with a modest request rate. This causes
many exits: EXTINT, CPUID, CRACC, APICACC. We
found that libc and other libraries make multiple uses of
CPUID each time a process is started and in Apache there
are new processes started to handle each connection.
Finally, at around 180 seconds we show starting VNC (a
graphical desktop sharing software) and running startx to
start the X Window System. We can see the EPTV ex-
its which are due to emulation of VGA once the graphical
window system is started.
To summarize, diﬀerent situations lead to diﬀerent uses of
VM exits and invocation of underlying hypervisor support.
Each VM exit could be treated as a communication channel
where a VM implicitly or explicitly sends information to the
hypervisor so the hypervisor can handle the event. Each VM
exit is then a potential attack vector as it is a window that
the VM can use to attack the hypervisor (e.g., by exploiting
a bug in how the hypervisor handles certain events). With
NoHype, we eliminate these attack vectors.
3. THREAT MODEL
With NoHype, we aim to protect against attacks on the
hypervisor by the guest VMs. A malicious VM could cause
a VM exit to occur in such a manner as to inject malicious
code or trigger a bug in the hypervisor. Injecting code or
triggering a bug could potentially be used to violate conﬁ-
dentiality or integrity of other VMs or even crash or slow
down the hypervisor, causing a denial-of-service attack vi-
olating availability. We eliminate the need for interaction
between VMs and hypervisor, thus preventing such attacks.
To that end, we assume the cloud infrastructure provider
is not malicious, and suﬃcient physical security controls are
being employed to prevent hardware attacks (e.g., probing
on the memory buses of physical servers) through surveil-
lance cameras and restricted access to the physical facilities.
We are not concerned with the security of the guest OSes,
Table 1: Selected reasons for VM exits [3].
VM Exit
EPTV
Reason
An attempt to access memory with a guest-
physical address was disallowed by the conﬁg-
uration of the EPT paging structures.
APICACC Guest software attempted to access memory at
MSRWR
MSRRD
IOINSR
DRACC
CRACC
CPUID
a physical address on the APIC-access page.
Guest software attempted to write machine
speciﬁc register, MSR.
Guest software attempted to read machine
speciﬁc register, MSR.
Guest software attempted to execute an I/O
instruction.
Guest software attempted to move data to or
from a debug register
Guest software attempted to access CR0, CR3,
CR4, or CR8 x86 control registers.
Guest software attempted to execute CPUID
instruction.
PNDVINT Pending virtual interrupt.
EXTINT
EXCNMI
An external interrupt arrived.
Exception or non-maskable interrupt, NMI.
but do require that a cloud provider makes available a set of
slightly modiﬁed guest OS kernels which are needed to boot
a VM. The responsibility of protecting software which runs
inside the VMs is placed upon the customer. Also, in this
work the security and correctness of the cloud management
software is not covered. The cloud management software
presents the interface that cloud customers use to request,
manage and terminate virtual machines.
It runs on dedi-
cated servers and interacts with the NoHype servers. For
the purpose of this paper, we assume it is secure, but will
revisit this as potential future work.
4. NOHYPE SYSTEM ARCHITECTURE
In this section we present the NoHype system architec-
ture which capitalizes on the unique use model of hosted
and shared cloud infrastructures in order to eliminate the
hypervisor attack surface. Rather than defending once these
attack vectors have been utilized to attack the hypervisor,
we take the new approach of removing the attack surface.
In doing so, we have a goal of preserving the semantics of
today’s virtualization technology – namely that we can start
VMs with conﬁgurable parameters, stop VMs, and run mul-
tiple VMs on the same physical server. Additionally, the
NoHype architecture is designed to be realizable on today’s
commodity hardware. In subsequent sections we discuss the
key ideas of the NoHype architecture which are:
• pre-allocating memory and cores,
• using only virtualized I/O devices,
• short-circuiting the system discovery process, and
• avoiding indirection.
4.1 Pre-allocating Memory and Cores
One of the main functions of the hypervisor is dynami-
cally managing the memory and processor cores’ resources
toward the goal of overall system optimization. By dynam-
ically managing the resources, VMs can be promised more
resources than are actually physically available. This over-
subscription is heavily used in enterprises as a way to consol-
404idate servers. In a hosted cloud computing model, however,
oversubscription is at odds with the expectations of the cus-
tomer. The customer requests, and pays for, a certain set
of resources. That is what the customer expects to receive
and not the unpredictable performance and extra side chan-
nels often associated with oversubscription. Rather than
relying on customers underutilizing their VMs, the cloud in-
frastructure provider could instead simply capitalize on the
rapidly increasing number of cores and memory in servers to
host more VMs per physical server (even without oversub-
scription). With NoHype we pre-allocate the processor cores
and memory so that a hypervisor is not needed to manage
these resources dynamically – this is possible in hosted cloud
computing since the customer speciﬁes the exact resources
needed before the VM is created.
Today, the hypervisor dynamically manages the proces-
sor cores through the scheduling functionality. Since the
number of cores is speciﬁed by the customer before the VM
is created, in NoHype we dedicate that number of cores to
the speciﬁc VM. This is not to be confused with pinning a
VM (also called setting the processor core aﬃnity of a VM),
which is a parameter to the hypervisor scheduling function
to conﬁgure the restrictions of which cores a given VM is al-
lowed to run on. Additionally, today when the cores are not
dedicated to a single VM and when the core which the VM
is scheduled on can change, the hypervisor must emulate the
local APIC by providing a timer and handling IPIs, which
are functionalities that the guest OS would expect from a
local APIC. Since with NoHype, a core is dedicated to a
given VM, we no longer need to emulate this functionality
and can allow a guest to use the local APIC directly. While
this gives the guest OS direct control over the hardware local
APIC and opens a new possibility for a guest VM to send
a malicious interprocessor interrupt (IPI) to other VMs, we
show in our security analysis in Section 6.2 that this can be
handled with a slight modiﬁcation to the way the guest VM
handles IPIs.
Similar to pre-allocating processor cores, in NoHype we
also pre-allocate the memory. In order to remove the vir-
tualization layer, we can again capitalize on the use model
where customers of the cloud infrastructure provider spec-
ify the amount of memory for their VMs. This means we
can pre-allocate resources rather than having a hypervisor
dynamically manage them. A key to isolating each virtual
machine is making sure that each VM can access its own
guest physical memory and not be allowed to access the
physical memory of other VMs. Without an active hyper-
visor we must utilize the hardware to enforce the memory
isolation by capitalizing on the hardware paging mechanisms
available in modern processors.
4.2 Using only Virtualized I/O Devices
I/O devices are another important component that is ad-
dressed in the NoHype architecture. With NoHype, we ded-
icate I/O devices to the guest VM so we do not need vir-
tualization software to emulate these devices. Of course,
dedicating a physical I/O device to each VM does not scale.
With NoHype, the devices themselves are virtualized. How-
ever, a VM running ‘in the cloud’ has no need for peripheral
devices such as a mouse, VGA, or printer. It only requires
network connection (NIC), storage, and potentially a graph-
ics card (which is increasingly used for high-performance
general-purpose calculations). So, only a limited number of
devices with virtualization support is needed. Today, NICs
with virtualization support are already popular and stor-
age and graphics devices will be soon. Moreover, networked
storage can be utilized in lieu of a virtualized (local) stor-
age device – making the unavailability of virtualized storage
devices only a minor limitation.
NoHype capitalizes on modern processors for both direct
assignment of devices as well as virtualization extensions
in modern commodity devices. VMs control the devices
through memory-mapped I/O. The memory-management
hardware of modern commodity processors can be conﬁg-
ured so that the VM can only access the memory of the de-
vice that is associated with it (and the device can also only
access the memory of its associated VM). Further, virtual-
ization extensions are available in modern commodity de-
vices. For example, SR-IOV2 [7] enabled device announces
itself as multiple devices on the PCI bus. The functional-
ity of these devices is separated into board-wide functions
(known as physical functions) which are controllable only by
the host OS , and functions which are speciﬁc to the indi-
vidual virtual devices (known as virtual functions) that can
be assigned to diﬀerent guest VMs.
4.3 Short-Circuiting the System Discovery
To run on a variety of platforms, most operating systems
automatically discover the conﬁguration of the underlying
system. This is done by the guest OS kernel during its ini-
tial loading. To minimize changes to the guest OS, the No-
Hype architecture allows the guest OS to perform its normal
bootup procedure, but slightly modiﬁes it to cache system
conﬁguration data for later use. This is supported by a tem-
porary hypervisor to overcome current limitations of com-
modity hardware. For example, to determine which devices
are available, the guest OS will perform a series of reads to
the PCI conﬁguration space in order to determine what de-
vices are present. This PCI conﬁguration space, along with
“system discovering” instructions such as CPUID, are not
virtualized by today’s hardware.
This modiﬁed guest OS kernel is provided by the cloud
infrastructure provider – a practice that is common today
to make it easier for customers to start using the cloud
provider’s infrastructure. This becomes a requirement in the
NoHype architecture to ensure that no customer code exe-
cutes while any underlying virtualization software is present
– since the customer code may attempt to attack this tem-
porary hypervisor. Additionally, as minimal changes are