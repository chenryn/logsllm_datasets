(e.g., ﬁle reads, command line arguments, user interactions).
Then, we build a forward causal graph from each input, i.e.,
identifying all other syscalls depending on the input, using
MCI and compare it with the ground truth by LDX. During
the experiment, we record all inputs used for the programs.
Then, we re-execute the program with the recorded inputs
to reproduce the same execution. To do so, we develop a
lightweight record and replay system similar to ODR [5].
LDX is run on top of the replay system to derive the ground
truth. Note that due to the limitation of the replay system,
the replayed execution may differ from the original execution.
Such differences are counted as false-positives/negatives for
conservativeness.
TABLE III.
RESULTS FOR SYSTEM-WIDE CAUSALITY INFERENCE.
Program
Firefox
Apache
Lighttpd
nginx
proftpd
CUPS
vim
elinks
alpine
zip
transmission
lftp
yafc
wget
ping
procps
# of
events
2,313 M
296 M
125 M
187 M
49 M
25 M
43 M
38 M
116 M
5 M
250 M
11 M
9 M
627 K
2.4 k
4 M
# of
causality
11 M
6.6 M
3.3 M
3.8 M
2.1 M
918 K
4 M
3.6 M
4.7 M
634 K
6.9 M
438 K
616 K
71 K
1.3 K
1 M
# of matched
models
549 K
435 K
275 K
246 K
179 K
88 K
219 K
145 K
231 K
36 K
479 K
54 K
43 K
5.4 K
241
176 K
FP
8.3%
0%
0%
0%
0%
0%
0%
0%
0%
0%
3.8%
0%
0%
0%
0%
0%
FN
3.2%
0%
0%
0%
0%
0.8%
0%
0%
0.3%
0%
5.2%
0%
0%
0%
0%
0%
The collected log consists of syscalls from multiple pro-
grams and the size of the log is around 732 GB (without
compression) containing 3707 million events. We ﬁrst separate
the log into smaller logs per process.
Table III shows results of the experiment. The second
column shows # of events (syscalls) in the log for each
program. The third and forth columns represent # of de-
pendencies detected and # of models matched by MCI. For
the # of dependencies, we count all those inferred by MCI
via matched models and those explicit dependencies across
matched models. The last column shows false-positive and
false-negative rates.
For most programs, MCI precisely identiﬁes causality
with not measurable false-positives and false-negatives. There
are a few exceptions: Firefox, CUPS, alpine, and
transmissions. We manually inspect a subset of these
false-positives/negatives and have the following observations.
Our Firefox models are intended to describe browser behaviors
such as following a hyperlink and opening a tab. However, logs
contain a lot of syscalls generated by the page content. Some
of them are not much distinguishable from browser-intrinsic
behaviors, leading to mismatches. For CUPS, we identify new
behaviors during the experiment which are variations of the
existing models. Transmission is a threaded program with
memory based synchronizations that are invisible to MCI.
Hence, MCI misses some thread interdependences via memory.
Comparison with BEEP. To evaluate the effectiveness of
MCI when compared with BEEP, we randomly select 100
system objects (e.g., ﬁles or network connections) accessed in
the week-long experiment. For each selected system object, we
construct a causal graph by BEEP and by MCI, and compare
the two. Table IV shows the results. First of all, we observe that
MCI has fewer false-positives and false-negatives. Again, we
use LDX as the ground truth. Especially, MCI reduces the false-
positive rate signiﬁcantly. We investigate some of the cases that
BEEP introduces false-positives, and ﬁnd that many system
objects accessed in a unit are included in the causal graphs
while they are not causally related. Also, BEEP causes slightly
more false-negatives due to missing inter-unit dependencies.
We analyze the cases and ﬁnd that
the missing inter-unit
dependencies were due to incomplete instrumentation caused
by the difﬁculty of binary analysis in BEEP. We also manually
investigate false-positive and false-negative cases from MCI. It
turns out they are mostly caused by concurrent executions in
transmission.
TABLE IV.
System subjects
COMPARISON WITH BEEP.
System objects
BEEP
MCI
9.23
9.18
33.71
25.38
Edges
74.21
62.87
FP / FN
12.8% / 0.3%
0.1% / 0.1%
Runtime/memory Overhead. We also measure runtime over-
head and memory overhead of MCI. Speciﬁcally, we report
how long MCI takes to parse the audit log collected from the
one week experiment which contains 3707 millions events.
As we discussed in Sec. IV-B, we preprocess an audit log to
extract indexes so that the parser can quickly locate skeleton
instances. We measure the runtime performance and memory
consumption of the trace preprocessor. It takes 4 hours 47
minutes to preprocess (index) the entire log. The preprocessor
occupies around 2.8 GB of memory on average. The parser
ﬁrst locates segments of the traces and launches automata
within the identiﬁed segments. We ﬁnd that the parser spend
more time on parsing within the segments. In particular, the
parser takes more time when it parses a wrong segment and
eventually fails. Note that we parallelize the parsing within
a segment to exploit multi-core processors. To parse the log,
it takes around 4 days (95 hours 43 minutes), and the parser
consumes around 6.2 GB of memory on average. We consider
such one-time efforts reasonable given the huge log size. We
leave performance optimization to our future work.
C. Case Studies
In this section, we present a few case studies to demonstrate
the effectiveness of our approach in attack investigation.
12
1) Phishing email and camouﬂaged FTP server case: In
this case, we use a scenario adapted from attack cases that were
created by security professionals in a DARPA program [11], to
demonstrate how MCI can effectively infer causality in a real-
world security incident that happens across multiple programs
including PINE and Firefox.
Attack Scenario. The user regularly uses PINE to send and
receive emails. At some point, the user receives a phishing
email, and she opens it, ﬁnds a hyperlink that looks interesting,
and hence clicks the hyperlink. PINE automatically spawns
the Firefox browser and the browser navigates to the given
hyperlink. The hyperlink leads her to a web-page that contains
an FTP server program. As she thinks the program is useful,
she downloads the program. Before she closes the Firefox
browser, she navigates a few more websites and downloads
other ﬁles as well. Speciﬁcally, she opened 2 more tabs and
downloaded 3 more programs.
After she closed the browser, she checked a few more emails
and then opened a terminal to execute the downloaded FTP
server program. The FTP server is a camouﬂaged trojan [3].
It normally behaves as a benign FTP server, serving remote
FTP requests properly. However, it contains a backdoor which
allows a remote attacker to connect and execute malicious
commands on the victim computer. After she ran the trojan
FTP server program, it served tens of benign FTP user requests
with hundreds of FTP commands. A few hours later,
the
attacker connects to the machine through the backdoor, and
modiﬁes an important ﬁle (e.g., ﬁnancial report). Later, the
company identiﬁes that the contents of the important ﬁle is
changed and then hires a forensic expert to investigate the
case to identify the origin of the incident.
Investigation. Given the causal models listed in Table II
and a system-wide trace collected from the user’s system,
the forensic expert uses MCI to infer causal relations from
the changed ﬁle. By matching models over the trace, MCI
successfully identiﬁes causality from the initial phishing email
to the attacker’s connection in the camouﬂaged trojan. The
investigator further identiﬁes that the important ﬁle is touched
by the FTP server process. However, the ﬁle operation does not
belong to any model instance. Interestingly, this indicates that
the ﬁle is not part of regular behaviors, indicating that the FTP
server may be trojaned. The investigator then tries to identify
how the FTP server is downloaded and executed in the system.
MCI reveals that a Firefox process downloaded the FTP
server binary via y.y.y.y:80 through “LoadURI” and “Down-
load a ﬁle” models. MCI further identiﬁes that the Firefox
process was launched by a PINE process when the user
clicked a link from an email stored at /var/mail/.../94368.5222
downloaded from x.x.x.x.
We also investigate the same incident with BEEP, and ﬁnd
out that a causal graph generated by BEEP has a number of
false-positives. Speciﬁcally, as shown in Fig. 17, the causal
graph includes n.n.n.n:53 which is resolving the domain name,
several other IP addresses from the Firefox process, which
are from different tabs. Moreover, the causal graph contains
other ﬁles downloaded from other tabs (../ﬁle1 and ../ﬁle2), two
more sockets for internal messaging system (unix socket) and
XWindow system (/tmp/.X11-unix), as well as some database
ﬁles for storing browsing history (/.../places.sqlite).
In contrast, as MCI leverages accurate models generated
by LDX, the graph generated by MCI is more accurate and
precise without bogus dependencies. We also note that BEEP
requires training and binary instrumentation on the end-user
site while MCI has no requirements on the end-user site.
2) Information Theft via InfoZip (Zipsplit): In this case, we
use another insider attack to demonstrate the effectiveness of
MCI. Speciﬁcally, an attacker in this case intentionally uses
Zipsplit to obstruct the investigation of the case as it reads
and writes multiple input and output ﬁles where dependences
between them are difﬁcult to capture by existing approaches.
We show how MCI can accurately identify the information
ﬂow through the program.
Attack Scenario. In this case, an insider tries to leak a secret
document to a competitor company. However, the attacker’s
company forces all computer systems to enable audit logging
system to monitor any attempts to exﬁltrate important infor-
mation. To avoid being exposed, he decides to use Zipsplit
before sending out the secret. Speciﬁcally, he understands that
the Zipsplit program can compress n ﬁles into m com-
pressed ﬁles, and traditional audit logs are able to accurately
identify causal relations if an input ﬁle is compressed to a
single output ﬁle. Hence, the attacker used Zipsplit to
compress a secret document, secret.pdf, as well as two non-
secret ﬁles, 1.pdf and 2.pdf, and generates four output ﬁles,
c1.zip−c4.zip. In this example, the secret ﬁle is compressed
and distributed into c1.zip and c2.zip, whereas c3.zip and c4.zip
only contain non-secrets. Then he attached all output ﬁles to
an email, but before he sent it to the competitor company, he
removed c3.zip and c4.zip from the email and only sent the
other two that contain the secret. After that, he deleted all
emails histories and compressed ﬁles.
A few days later, the company found suspicious behaviors
from the attacker’s computer. They identiﬁed that the secret
document was accessed by Zipsplit, and some ﬁles that
may contain the secret were sent out. However, the attacker
claimed that the secret document was mistakenly included in
Zipsplit and he only sent the zip ﬁles that contain non-
secrets. At this point, the company started to investigate the
attacker’s machine to identify the source of outgoing ﬁles. Note
that the investigator is not able to inspect the compressed ﬁles
or email history as the attacker already deleted them.
Investigation. A forensic expert utilizes MCI