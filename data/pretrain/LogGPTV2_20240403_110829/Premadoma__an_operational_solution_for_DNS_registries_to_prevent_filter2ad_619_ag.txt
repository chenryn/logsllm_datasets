metric to compare the performance of the different models than,
for instance, the false positive rate (FPR).
In the next two sections, the two predictive models of Pre-
madoma are introduced, and evaluated in terms of precision and
recall on the validation set.
3 REPUTATION-BASED PREDICTION
The main goal of the reputation-based predictor is to identify and
track facilitators of campaign registrations, and use their past repu-
tation as an indicator of maliciousness. The selection of a suitable
classifier will be based on its prediction performance as well as on
the human interpretability of the classification model, which is an
operational requirement for registry customer support teams to be
able to handle possible false positives.
3.1 Reputation of the facilitators
To register domain names in bulk or in long-running campaigns,
malicious actors need different facilitators to enable these registra-
tions. These include the registrar through which the registrations
are made and the name servers used to set up the DNS resolu-
tion for the domain name. Other facilitators are communication
means, such as the email address and phone number. These are
for instance used by the registrar to comply with the ICANN Whois
accuracy program [12].
For the reputation-based prediction, reputation score features
have been engineered for these four prominent facilitators. The
reputation scores express the percentage of registrations linked to
a particular registrant’s phone number, registrar, e-mail provider or
name server, which were labelled as malicious in the ground truth
data. These reputation scores are calculated daily over 4 different
time windows: 14, 30, 60 days, as well as an all-time window that
takes into account all available historical data.
This feature set is combined with 6 features derived from the
domain name, 1 ordinal feature (hour of registration), 1 continuous
feature (address validity score) and 3 categorical features (country,
registrar and email provider), as shown in Table 1. 2
3.2 Classification algorithm
A good combination of prediction performance and interpretability
was reached using the PART algorithm, proposed by Frank and
Witten [6]. This algorithm uses the training data to iteratively build
an ordered list of ‘if-then’ rules as prediction model by constructing
partial C4.5 decision trees in each iteration and using the ‘best’ leaf
of the tree as a new rule. The resulting model is an ordered list
of rules, combining registration feature inequalities by means of
multiple AND-clauses.
The resulting model is human interpretable and closely resem-
bles the configuration of (handcrafted or curated) rule-based sys-
tems, yet since it is generated by a machine learning algorithm, it
automatically adapts to changing adversarial techniques, therefore
combining the best of two worlds.
3.3 Addressing dataset challenges
The dataset used in this research suffers from some inherent limita-
tions. First, the classes are highly imbalanced: only a small minority
of registrations is malicious. Second, not all malicious registrations
are correctly flagged in the training sets, due to delays and incom-
pleteness of the ground truth. We address these challenges to ensure
our system performs in a real-world context.
In the following paragraphs, we propose a number of methods
to counter these inherent dataset challenges.
Blacklisting delays. According to [22], it takes up to 5 days for
73% of malicious registrations to be flagged by blacklists. As a result
of this delay, each day the training set contains malicious domains
that haven not yet been flagged. This can negatively skew the
prediction models.
To absorb this effect, we filter out recent benign registrations
from the training set, as these might still turn out malicious in
the near future. In our current approach, we filter out the benign
registrations from the last 5 days.
Class imbalances. The data set contains many more benign than
malicious domains: With just 2.53% of registrations labelled as ma-
licious in the validation and testing phase, we face an extremely
unbalanced dataset. Without proper precautions, this situation re-
sults in a high bias towards predicting registrations as benign.