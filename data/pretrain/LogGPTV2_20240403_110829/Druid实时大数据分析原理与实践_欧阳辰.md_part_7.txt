数量级的提升，因为Impala省去了MapReduce的过程，减少了中间结果落盘的问题。Impala
Hive并添加字典编码、游程编码等功能。在Cloudera的测试中，Impala的查询效率比Hive有
函数查询数据，从而大大降低了延迟。
和Query Exec Engine三部分组成），可以直接从HDFS或HBase中用 SELECT、JOIN和统计
编写，通过使用与商用MPP类似的分布式查询引擎（由QueryPlanner、Query Coordinator
2.
3
Presto的运行模型和 Hive或MapReduce 有着本质的区别。Hive将查询翻译成多阶段的
相比 Druid，Impala需要将数据格式转换成 Parquet格式才能进行查询，有时候数据转换
Impala使用的列存储格式是Parquet。Parquet实现了Dremel中的列存储，未来还将支持
Impala是Cloudera受谷歌Dremel启发开发的实时交互SQL大数据查询工具，使用C++
Impala
Presto出身名门，来自于Facebook，从出生起就受到关注。其主要采用Java编写。Presto
Common HiveSQLandinterface
DN
HDFS
Query Exe Engine
Query Coordinator
Query Planner
ODBC
SQLAPP
HBase
图2-16Impala的数据流程图
Query Coordinator
Query Exe Engine
HDFS
Query Planner
K
HBase
Metastore
Hive
Distributed
FullMPP
Unified metadata
Local Direct Reads
Druid实时大数据分析原理与实践
Query Exe Engine
Query Coordinator
HDFS
Query Planner
State
HBase
---
## Page 57
MapReduce、AmazonKinesis或任何启用SSH的主机的集成呈线性扩展关系。
驱动程序。数据加载速度与集群大小，与Amazon S3、AmazonDynamoDB、Amazon Elastic
载，以使用各种大量熟悉的 SQL客户端。当然也可以使用标准的PostgreSQL JDBC和ODBC
改善I/O效率并跨越多个节点平行放置查询，从而提供快速的查询性能。AmazonRedshift
的商业智能工具，以一种经济的方式轻松分析所有数据。AmazonRedshif使用列存储技术
快速到数据存储区获得必要的数据，并且返回结果。它并不直接去优化存储结构，而是直接
理段传入下一个处理段。这样的方式会大大减少各种查询的端到端响应时间。
流水线式的执行模型会在同一时间运行多个数据处理段，一旦数据可用就会将数据从一个处
行的。通过软件的优化，形成处理的流水线，以避免不必要的磁盘读写和额外的延迟。这种
响应的操作符来支持SQL语法。除了改进的调度算法之外，所有的数据处理都是在内存中进
到磁盘上。然而，Presto引擎没有使用MapReduce，它使用了一个定制的查询和执行引擎及
提供了定制的JDBC和ODBC驱动程序，可以从控制台的“连接客户端”选项卡中进行下
1.Redshift
2.3.6
访问HDFS或者其他数据存储层。
第2章数据分析及相关软件
AmazonRedshift是一种快速的、完全托管的PB级别数据仓库，可以方便用户使用现有
相比Druid，Presto主要是解决SQL查询引擎的问题，将SQL查询转换成分布式任务，
数据分析云服务
Architecture
图2-17简化的 Presto系统架构示意图
Data
API
CO
33
---
## Page 58
参考资料
能是出众的，实时支持也是超群的。
时”、“高效”、“简洁”是合适的标签。
但是每种数据分析软件都有自己独特的定位。如果需要给Druid几个标签的话，“开源”、“实
2.4小结
析服务。
间进行灵活的数据探索，快速发现数据价值，并可直接嵌人业务系统，
视和业务探索。分析型数据库对海量数据的自由计算和极速响应能力，能让用户在瞬息之
（RealtimeOLAP）云计算服务，使用户可以在毫秒级针对千亿级数据进行即时的多维分析透
2.阿里云数据仓库服务
索引来提高带过滤查询的速度，
数据仓库产品。Druid适合分析大数据量的流式数据，也能够实时加载和聚合数据。Druid用
cel技术。ParAccel是一家专注提供数据分析服务的老牌技术公司，曾推出自已的列式存储的
34
数据分析的世界繁花似锦，虽然我们可以通过开源/商业、SaaS/私有部署等方式来分类
http://www.teradatawiki.net/2013/09/Teradata-Architecture.html
分析型数据库（AnalyticDB），是阿里巴巴自主研发的海量数据实时高并发在线分析
http://www.yankay.com/google-dremel-rationale/
相比Druid，Redshif是一种SaaS服务。Redshift内部使用了亚马逊已经取得授权的ParAc-
http://opentsdb.net/img/tsdb-architecture.png
，虽然索引结构简单，但是效率很高。
与大部分系统相比，Druid系统功能属于精简的，
Druid实时大数据分析原理与实践
，为终端客户提供分
性
---
## Page 59
3.1Druid架构概览
在第8章“核心源代码探析”中详细介绍。
设计与实现。本章将着重介绍Druid的架构设计与数据结构，其他出色的细节设计与实现将
其独到的架构设计、基于DataSource与 Segment的数据结构，以及在许多系统细节上的优秀
能够同时提供性能卓越的数据实时摄入与复杂的查询性能。它是怎么做到的呢？答案是通过
往要放弃一些索引的创建，就势必在查询的时候付出更高的性能代价。相比之下，Druid却
就需要牺牲一些数据写人的性能以完成索引的创建；反之，如果想获得更快的写人速度，往
在其中做一些取舍与权衡。比如，传统的关系型数据库如果想在查询时有更快的响应速度
遍的数据平台来说，数据的高效摄入与快速查询往往是一对难以两全的指标，因此常常需要
·协调节点（Coordinator Node）：负责历史节点的数据负载均衡，以及通过规则（Rule）
·实时节点（Reatime Node）：即时摄入实时数据，以及生成Segment数据文件。
Druid总体架构图显示出Druid自身包含以下4类节点。
关于 Druid架构，我们先通过其总体架构图来做一个概要了解，如图3-1所示。
Druid的目标是提供一个能够在大数据集上做实时数据消费与探索的平台。然而，对普
·查询节点（BrokerNode）：对外提供数据查询服务，并同时从实时节点与历史节点查
·历史节点（HistoricalNode）：加载已生成好的数据文件，以供数据查询。
询数据，合并后返回给调用方。
架构详解
第
章
---
## Page 60
计思想，我们得先从数据库的文件组织方式聊起。
对于一个数据库的性能来说，其数据的组织方式至关重要。为了更好地阐述Druid的架构设
3.2
点查询到的结果合并后返回。
到数据文件存储库。同时，查询节点会响应外部的查询请求，并将分别从实时节点与历史节
实时流数据会被实时节点消费，然后实时节点将生成的Segment数据文件上传到数据文件存
9
对于目前大多数Druid的使用场景来说，Druid本质上是一个分布式的时序数据库，而
从数据流转的角度来看，数据从架构图的左侧进入系统，分为实时流数据与批量数据。
·数据文件存储库（DeepStorage）：存放生成的 Segment数据文件，并供历史节点下载。
·分布式协调服务（Coordination）：为Druid集群提供一致性协调服务的组件，通常为
·元数据库（Metastore）：存储Druid集群的原数据信息，比如 Segment的相关信息，
同时，集群还包含以下三类外部依赖。
Druid 架构设计思想
对于单节点集群可以是本地磁盘，而对于分布式集群一般是HDFS或NFS。
般用MySQL或PostgreSQL。
Zookeepera
管理数据的生命周期。
Sorage
图3-1Druid总体架构图
Druid实时大数据分析原理与实践
口
口
External Dependen
Druid Nodes
---
## Page 61
Tree）来解决这个问题。但是平衡二叉树依然有个比较大的问题：它的树高为log2N一
在于实现简单，并且树在平衡的状态下查找效率能达到O(log2N)；缺点是在极端非平衡情
1．二叉查找树与平衡二叉树
3.2.1
查找数据，这种数据结构叫作索引（Index）。对于传统的关系型数据库，考虑到经常需要范
量减少磁盘的访问次数。
很耗时的操作，对比如图3-2所示。因此，提高数据库数据的查找速度的关键点之一便是尽
第3章架构详解
于索引树来说，树的高度越高，意味着查找所要花费的访问次数越多，查询效率越低。
况下查找效率会退化到O（N)，因此很难保证索引的效率。二叉查找树的查找效率如图3-3所
树上所有节点的值都小于根节点的值，而右子树上所有节点的值都大于根节点的值。其优点
构的种类很多，却不一定都适合用于做数据库索引。
围查找某一批数据，因此其索引一般不使用Hash算法，而使用树（Tree）结构。然而，树结
针对上述二叉查找树的缺点，
最常见的树结构是二叉查找树（Binary Search Tree），它就是一棵二叉有序树：保证左子
众所周知，数据库的数据大多存储在磁盘上，而磁盘的访问相对内存的访问来说是一项
为了加速数据库数据的访问，大多传统的关系型数据库都会使用特殊的数据结构来帮助
索引对树结构的选择
7500000
0000000
2500000
5000000
图3-2磁盘与内存的访问速度对比
内存数据访问
，人们很自然就想到是否能用平衡二叉树（Balanced Binary
不同存储介质随机访问速度比较
花费时间（纳秒)
HDD寻道
一对
3
---
## Page 62
图3-4所示。
2.B+树
率，
对索引树效率低下的原因。正因如此，人们就想到了通过增加每个树节点的度来提高访问效
（比如4KB大小的数据），
38
在传统的关系型数据库里，B+树（B+-tree）及其衍生树是被用得比较多的索引树，
，而B+树（B+-tree）便受到了更多的关注。
B+树的主要特点如下。
况且，主存从磁盘读数据一般以页为单位，因此每次访问磁盘都会读取多个扇区的数据
比较大，而树的高度就比较低，从而有利于提高查询效率。
每个树节点只存放键值，不存放数值，而由叶子节点存放数值。这样会使树节点的度
平衡时查找效率为O(Log2M)
Q
p
，远大于单个二叉树节点的值（字节级别），这也是造成二叉树相
图3-3
二叉查找树的查找效率
图3-4B+树
非平衡时查找效率为O(N)
N
Druid实时大数据分析原理与实践
如
---
## Page 63
的论文创造性地提出了目志结构合并树（Log-Structured Merge-Tree）的概念，该方法既吸收
缺点，随机读取数据时效率很低。1996年，一篇名为The log-structured merge-tree（LSM-tree）
志的最末端，以实现对磁盘的顺序操作，从而提高索引性能。不过，日志结构方法也有明显的
结构方法的主要思想是将磁盘看作一个大的目志，每次都将新的数据及其索引结构添加到日
读写，于是，1992年，名为日志结构（Log-Structured）的新型索引结构方法便应运而生。日志
写。磁盘顺序与随机访问吞吐对比如图3-5所示。
HDD）还是固态硬盘（Solid StateDrive,SSD），对磁盘数据的顺序读写速度都远高于随机读
高的磁盘IO，以致严重影响到性能。
辑上原本连续的数据实际上存放在不同的物理磁盘块位置上，在做范围查询的时候会导致较
击，它的主要缺点在于随着数据插人的不断发生，叶子节点会慢慢分裂——这可能会导致逻
第3章架构详解
众所周知，数据库的数据大多存储在磁盘上，而无论是传统的机械硬盘（HardDiskDrive,
日志结构合并树
然而，基于B+树的索引结构是违背上述磁盘基本特点的一
正是由于B+树的上述优点，它成了传统关系型数据库的宠儿。当然，它也并非无懈可
·叶子节点存放数值，
更新过程中树能以比较小的代价实现自平衡。
与二叉树不同，B+树的数据更新操作不从根节点开始，而从叶子节点开始，并且在
很相似。
进行区间数据查询；并且所有叶子节点与根节点的距离相同，因此任何查询的效率都
100000
150000
200000
50000
图3-5磁盘顺序与随机访问吞吐对比
，并按照值大小顺序排序，且带指向相邻节点的指针，以便高效地
随机
吞吐KB/S
顺序
一它会需要较多的磁盘随机
39
---
## Page 64
要的相关措施。
查询数据，这样显然会对性能造成较大的影响。为了解决这个问题，LSM-tree采取了以下主
但是不利于读一—因为理论上读的时候可能需要同时从memtable和所有硬盘上的sstable中
就通过LSM-tree实现了数据文件的生成。HBaseLSM-tree架构示意图如图3-7所示。
memtable）里，最后会冲写到磁盘StoreFile（相当于sstable）中。这样HBase的HRegionServer
则，数据首先会写到HBase的 HLog(相当于commit log)里，然后再写到MemStore（相当于
中将memtable的数据恢复。
数据；memtable与 sstable可同时供查询；当memtable出问题时，可从commit log与 sstable
写到memtable，最后当达到一定条件时数据会从memtable冲写到sstable，并抛弃相关的 log
与更新操作都首先被记录到commitlog中一
叫作commitlog）来为数据恢复做保障。这三类数据结构的协作顺序一般是：所有的新插入
主要负责提供读操作，特点是有序且不可被更改。LSM-tree的C。与C部分如图3-6所示。
于硬盘上（这部分通常叫作sstable)，它们是由存在于内存缓存中的C。树冲写到磁盘而成的
据插人更新以及读请求，并直接在内存中对数据进行排序；另一部分数据结构（C树）存在
其中一部分数据结构（Co树）存在于内存缓存（通常叫作memtable）中，负责接受新的数
基础上破茧而出，彻底改变了大数据基础组件的格局，同时也极大地推广了LSM-tree技术。
件（2007年的HBase与2008年的Cassandra，目前两者同为Apache顶级项目）直接在其思想
Data横空出世，在分布式数据处理领域掀起了一阵旋风，随后两个声名赫赫的大数据开源组
年谷歌的一篇使用了LSM-tree技术的论文Bigtable：ADistributedStorageSystemforStructured
题。尽管当时LSM-tree新颖且优势鲜明，但它真正声名鹊起却是在10年之后的2006年，那
了日志结构方法的优点，又通过将数据文件预排序克服了日志结构方法随机读性能较差的问
40
LSM-tree的这种结构非常有利于数据的快速写人（理论上可以接近磁盘顺序写速度）
我们可以参考HBase的架构来体会其架构中基于LSM-tree的部分特点。按照WAL的原
LSM-tree的另一大特点是除了使用两部分类树的数据结构外，还会使用日志文件（通常
LSM-tree最大的特点是同时使用了两部分类树的数据结构来存储数据，并同时提供查询
图3-6LSM-tree的Co与C部分
C1 tree
Disk
—该操作叫作WAL（WriteAhead Log），然后再
Co tree
Memory
Druid实时大数据分析原理与实践
---
## Page 65
第3章架构详解
（BrokerNode）查询。实时节点数据块的生成示意图如图3-8所示。
加载到内存中的非堆区，因此无论是堆结构缓存区还是非堆区里的数据，都能够被查询节点
被冲写到硬盘上形成一个数据块（SegmentSplit)，同时实时节点又会立即将新生成的数据块
进实时节点内存中的堆结构缓存区（相当于memtable)，当条件满足时，缓存区里的数据会
此Druid架构便顺理成章地吸取了LSM-tree的思想。
时Druid在一开始就是为时序数据场景设计的，而该场景正好符合LSM-tree的优势特点，
3.2.2
Druid的类LSM-tree架构中的实时节点（RealtimeNode）负责消费实时数据，与经典
LSM-tree显然比较适合那些数据插入操作远多于数据更新删除操作与读操作的场景，同
·定期将硬盘上小的 sstable合并（通常叫作Merge或Compaction操作）成大的 sstable，
·对每个 sstable使用布隆过滤器（Bloom Filter），以加速对数据在该sstable的存在性进
Druid总体架构
行判定，从而减少数据的总查询时间。
重复的操作或更新去重、合并。
只会将更新删除操作加到当前的数据文件末端，只有在sstable合并的时候才会真正将
以减少 sstable的数量。而且，平时的数据更新删除操作并不会更新原有的数据文件，
Client
KeyValue's
图3-7HBaseLSM-tree架构示意图
HR
rollWriter()
0口口
000
sync()
HRegion
HRegion
HRegion
000
HLog
MemStore
MemStoreStore
MemStore
HFile
StoreFile
HFile
StoreFile
HFile
StoreFile
HFile
StoreFile
StoreFile
Store
Store
：
---
## Page 66
的借鉴如图3-9所示。
不同于HBase等LSM-tree系架构的一个显著特点。Druid对命令查询职责分离模式（CQRS）
令查询职责分离模式（CommandQueryResponsibility Segregation，CQRS）——这也是Druid
个结果的整合，最后再返回给用户。Druid的这种架构安排实际上也在一定程度上借鉴了命