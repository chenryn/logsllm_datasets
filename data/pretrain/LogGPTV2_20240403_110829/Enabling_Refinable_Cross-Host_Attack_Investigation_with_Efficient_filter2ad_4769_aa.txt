title:Enabling Refinable Cross-Host Attack Investigation with Efficient
Data Flow Tagging and Tracking
author:Yang Ji and
Sangho Lee and
Mattia Fazzini and
Joey Allen and
Evan Downing and
Taesoo Kim and
Alessandro Orso and
Wenke Lee
Enabling Refinable Cross-Host Attack Investigation 
with Efficient Data Flow Tagging and Tracking
Yang Ji, Sangho Lee, Mattia Fazzini, Joey Allen, Evan Downing, Taesoo Kim,  
Alessandro Orso, and Wenke Lee, Georgia Institute of Technology
https://www.usenix.org/conference/usenixsecurity18/presentation/jia-yang
This paper is included in the Proceedings of the 
27th USENIX Security Symposium.
August 15–17, 2018 • Baltimore, MD, USA
978-1-939133-04-5
Open access to the Proceedings of the 27th USENIX Security Symposium is sponsored by USENIX.Enabling Refinable Cross-Host Attack Investigation with
Efficient Data Flow Tagging and Tracking
Yang Ji, Sangho Lee, Mattia Fazzini, Joey Allen, Evan Downing,
Taesoo Kim, Alessandro Orso and Wenke Lee
Georgia Institute of Technology
Abstract
Investigating attacks across multiple hosts is challeng-
ing. The true dependencies between security-sensitive
files, network endpoints, or memory objects from dif-
ferent hosts can be easily concealed by dependency ex-
plosion or undefined program behavior (e.g., memory
corruption). Dynamic information flow tracking (DIFT)
is a potential solution to this problem, but, existing DIFT
techniques only track information flow within a single
host and lack an efficient mechanism to maintain and
synchronize the data flow tags globally across multiple
hosts.
In this paper, we propose RTAG, an efficient data flow
tagging and tracking mechanism that enables practical
cross-host attack investigations. RTAG is based on three
novel techniques. First, by using a record-and-replay tech-
nique, it decouples the dependencies between different
data flow tags from the analysis, enabling lazy synchro-
nization between independent and parallel DIFT instances
of different hosts. Second, it takes advantage of system-
call-level provenance information to calculate and allocate
the optimal tag map in terms of memory consumption.
Third, it embeds tag information into network packets to
track cross-host data flows with less than 0.05% network
bandwidth overhead. Evaluation results show that RTAG
is able to recover the true data flows of realistic cross-host
attack scenarios. Performance wise, RTAG reduces the
memory consumption of DIFT-based analysis by up to
90% and decreases the overall analysis time by 60%–90%
compared with previous investigation systems.
1
Introduction
Advanced attacks tend to involve multiple hosts to conceal
real attackers and attack methods by using command-and-
control (C&C) channels or proxy servers. For example,
in the Operation Aurora [22] attack, a compromised vic-
tim’s machine connected to a C&C server that resided in
the stolen customers’ account, and exfiltrated proprietary
source code from the source code repositories. Gibler
and Beddome demonstrated GitPwnd [32], an attack that
takes advantage of the git [11] synchronization mech-
anism to exfiltrate victim’s private data through a public
git server. Unlike common data exfiltration attacks that
only involve a victim host, GitPwnd leverages two hosts
(victim’s host and public git server) to complete the
exfiltration.
Unfortunately, existing attack investigation systems,
also known as provenance systems, are inadequate to
figure out the true origin and impact of cross-host at-
tacks. Many provenance analysis systems (such as
[19, 35, 45]) are designed to monitor the system-call-level
or instruction-level events within each host while ignoring
cross-host interactions. In contrast, network provenance
systems [64, 68, 69] focus on the interaction between mul-
tiple hosts, but, because they lack detailed system-level
information, their analysis could result in a dependency
explosion problem [35, 42]. To fully understand the steps
and end-to-end information flow of a cross-host attack,
it is necessary to collect accurate flow information from
individual hosts and correctly associate them to figure out
the real dependency.
Extending existing provenance systems to investigate
cross-host attacks is challenging because problems of
accuracy, performance, or both can be worse with mul-
tiple hosts. Although collecting coarse-grained prove-
nance information (e.g., system-call-level information)
introduces negligible performance overhead, it cannot
accurately track dependency explosion and undefined pro-
gram behaviors (e.g., memory corruption) even within a
single host. That is, if we associate the coarse-grained
provenance information from different hosts using another
vague link (e.g., network session [64, 68, 69]), the result
will contain too many false dependencies. Fine-grained
provenance information, (e.g., instruction-level informa-
tion from dynamic information flow tracking (DIFT)), is
free from such accuracy problems. However, it demands
USENIX Association
27th USENIX Security Symposium    1705
many additional computations and consumes huge mem-
ory, which will increase according to the number of hosts.
More seriously, existing cross-host DIFT mechanisms
piggyback metadata (i.e., tags) on network packets and
associate them during runtime [50, 67], which is another
source of huge performance degradation.
To perform efficient and accurate information flow anal-
ysis in the investigation of cross-host attacks, we propose
a record-and-replay-based data flow tagging and tracking
system, called RTAG. Performing cross-host information
flow analysis using a record-and-replay approach intro-
duces new challenges that cannot be easily addressed
using existing solutions [25, 35, 50, 67]: that is, long
analysis time and huge memory consumption. First, the
communication between different hosts (e.g., through
socket communication) introduces information flows that
require additional information and procedure for proper
analysis. Namely, the DIFT analysis requires transfer of
the analysis data (i.e., tags) between the hosts in a syn-
chronized manner. Existing record-and-replay solutions
have to serialize the communication between hosts to
transfer tags because no synchronization mechanism is
implemented, leading to longer than necessary analysis
time. Second, because a number of processes can run on
multiple hosts under analysis, the memory requirement
for DIFT instances could become tremendous, especially
when multiple processes on different hosts interact with
each other.
To overcome these two challenges, RTAG decouples
the tag dependency (i.e., information flow between hosts)
from the analysis with tag overlay and tag switch tech-
niques (§6), and enables DIFT to be independent of any
order imposed by the communication. This new approach
enables the DIFT analysis to happen for multiple pro-
cesses on multiple hosts in parallel leading to a more
efficient analysis. Also, RTAG reduces the memory con-
sumption of the DIFT analysis by carefully designing the
tag map data structure that tracks the association between
tags and associated values. Evaluation results show sig-
nificant improvement both in analysis time, decreased by
60%–90%, and memory costs, reduced by up to 90%,
with realistic cross-host attack scenarios including GitP-
wnd and SQL injection.
This paper makes the following contributions:
• A tagging system that supports refinable cross-
host investigation. RTAG solves “tag dependency
coupling,” a key challenge in using refinable investi-
gation systems for cross-host attack scenarios. RTAG
decouples the tag dependency from the analysis
which spares the error-prone orchestrating effort on
replayed DIFTs and enables DIFT to be performed
independently and in parallel.
• DIFT runtime optimization. RTAG improves the
runtime performance of doing DIFT tasks at replay
time in terms of both time and memory. By per-
forming DIFT tasks in parallel, RTAG reduces the
analysis time by over 60% in our experiments. By
allocating an optimal tag size for DIFT based on
system-call-level reachability analysis, RTAG also
reduces the memory consumption of DIFT by up to
90% compared with previous DIFT engines.
The rest of paper is organized as follows: §2 describes
the background of the techniques that supported RTAG’s
realization. §3, §4, and §5 present the challenges, an
overview and the threat model of RTAG; §6 presents the
design of RTAG; More specifically, §6.1 describes the data
structure of RTAG, §6.3 explains how RTAG facilitates the
independent DIFT; §6.4 describes how RTAG conducts
tag switch for DIFT, and §6.6 presents the tag association
module and how RTAG tracks the traffic of IPC. §7 gives
implementation details and the complexity. §8 presents
the results of evaluation. §9 summarizes related work,
and §10 concludes this paper.
2 Background
RTAG utilizes concepts from a variety of research ar-
eas. This section provides an overview of these concepts
needed to understand our system.
2.1 Execution Logging
Attack investigation systems most often rely on logged
information to perform their analyses. Different systems
use different levels of granularity when logging infor-
mation for their analyses (e.g., system-call level versus
instruction level) as the cost of collecting this informa-
tion changes based on the selected granularity level. A
first category of systems [6, 8, 19, 45] collects informa-
tion at a high-level of granularity (e.g., system-call level)
and generally have low runtime overhead. However, the
information collected at this level of granularity might
affect the accuracy of their analyses as it does not always
provide all of the execution details. A second category of
systems improves accuracy by analyzing program execu-
tions at the instruction level [24, 44, 66]. These systems
provide very accurate results in their analyses. However,
they introduce a runtime overhead that is not suitable
for production software. Finally, a third category of sys-
tems [25, 35] combines the benefits of systems from the
previous two categories using record and replay. These
systems perform high-level logging/analysis while record-
ing the execution of programs and perform low-level log-
ging/analysis in a replayed execution of the programs.
More specifically, RAIN [35] logs system call informa-
tion about user-level processes using a kernel instrumen-
tation approach. The system then analyzes instructions in
a replayed execution of the processes.
1706    27th USENIX Security Symposium
USENIX Association
2.2 Record and Replay
Record and replay is a technique that aims to store infor-
mation about the execution of a software system (record
phase) and use the stored information to re-execute the
software in such a way that it follows the same execu-
tion path and also reconstructs the program states as the
original execution (replay phase). Record and replay tech-
niques can be grouped under different categories based
on the layer of the system in which they perform the
record-and-replay task. Some techniques perform record
and replay by instrumenting the execution of programs
at the user level [9, 33, 51, 58, 59]. These techniques are
efficient in their replay phase as they can directly focus on
the recorded information for the specific program. How-
ever, these techniques either require program source or
binary code for instrumentation or have additional space
requirements when recording executions of communicat-
ing programs (especially through the file system) as the
recorded information is stored multiple times. The second
category of techniques performs record and replay by ob-
serving the behavior of the operating system. Techniques
do so by either monitoring the operating system through
a hypervisor [20, 23, 56] or emulation [27]. These tech-
niques are efficient in storing the information about dif-
ferent executing programs. However, they usually need
to replay every program recorded even when only one
program is of interest for attack investigation. Finally,
a third category of techniques uses an hybrid approach.
This category records information at the operating system
level and replays the execution leveraging user-level in-
strumentation [25, 35] (e.g., by hooking libc library) for
multi-thread applications. More specifically, Arnold [25]
and RAIN [35] reside inside the kernel of operating sys-
tem and record the non-deterministic inputs of executing
programs. The replay task is achieved by combining ker-
nel instrumentation with user-level instrumentation so
that replay of a single program is possible.
2.3 Dynamic Information Flow Tracking
Dynamic information flow tracking (DIFT) is a technique
that analyzes the information flowing within the execution
of a program. This technique does so by: (1) marking
with tags the “interesting” values of a program, (2) prop-
agating tags by processing instructions, and (3) check-
ing tags associated with values at specific points of the
execution. There are several instantiations of this tech-
nique [24, 34, 37, 47, 55, 66]. These instantiations can
precisely determine whether two values of the program
are related to each other or not. However, because the
technique needs to perform additional operations for ev-
ery executed instruction, that action generally introduce
an overhead which makes it unsuitable in production.
Figure 1: Comparison of the serialized DIFTs and RTAG paral-
lel DIFTs. We highlight the components of RTAG with dashed
circles. (a) shows the serialized DIFT for the ssh daemon on the
server and the ssh client on another host, both of which follow
the tag dependencies same as those were recorded. (b) depicts
that RTAG decouples the tag dependency from the replays of
processes by using the tag switch, allocation and association
techniques so that each process in the offline analysis can be
performed independently.
Arnold [25] and RAIN [35] make dynamic information
flow tracking feasible by moving the cost of the anal-
ysis away from the runtime using a record-and-replay
approach that performs DIFT only in the replayed ex-
ecution. RAIN [35] also improves the efficiency of the
analysis when considering an execution that involves mul-
tiple programs. RAIN [35] does so by: (1) maintaining a
provenance graph that captures the high-level relations be-
tween programs; (2) performing reachability analysis on
the provenance to discard executions that do not relate to
the security task under consideration and instead pinpoint-
ing the part of the execution where the data-dependency
confusion exists (i.e., memory overlaps, called interfer-
ence); (3) performing DIFT only for interferences by
replaying the execution and fast-forwarding to that part.
3 Motivating Example and Challenges
In section, we describe the challenges of performing re-
finable attack investigation across multiple hosts. We
first present a motivating attack example (GitPwnd [32])
involving multiple hosts in a data exfiltration; then, we
present what challenges we face with currently available
methods.
3.1 The GitPwnd Attack
GitPwnd uses a popular versioning control tool git to
perform malicious actions on a victim’s host and sync the
USENIX Association
27th USENIX Security Symposium    1707
waitwaitwaitssh-daemon@10.10.10.1BasicTagmap(a) Serialized DIFTs following tag dependencies(b) RTAG independent and parallel DIFTsTag OverlaySerialized replay w/ DIFTIndependent & continuousreplay w/ DIFTRTAG SystemSet()Get()1234567Provenance Graph§6.1§2.3GTKGTVGTKGTVGTKGTVV1V2ssh-client@10.10.10.2ssh-daemon@10.10.10.1ssh-client@10.10.10.2(Switch §6.4, Allocation §6.5, Association §6.6) TimeTime812345678Figure 2: Visualized Pruned Provenance Graph and Tags. (a) is the simplified provenance graph of the GitPwnd attack involving
three hosts, of which the git client and git server are monitored by RTAG. We use red rectangles to represent processes, blue
ovals for file objects, and pink ovals for out-of-scope remote host; we use directed edges to represent the data flows and parent-child
relations between processes. The tags with dashed circles are the IPC tags for pipe and socket communication. (b) is the result
of a backward query from the attacker’s host, the data flow overlay; it appears to be a tree, giving the data flow every step from the
exfiltrated private key and /etc/passwd (excluding /etc/group) to the attacker’s host, crossing three hosts.
result to an attacker’s controlled host via a git server.
Unlike conventional data exfiltration attacks, this attack
involves multiple hosts (i.e., a victim’s host and the git
server) to achieve the exfiltration. This attack evades
an existing network-level intrusion detection system, as
the victim’s host does not have a direct interaction with
any untrusted host (i.e., the attacker’s host). In addition,
this attack appears to be innocuous inside the developers’
network, as git operations are usually assumed to be
benign. We implement this attack using gitolite [12]
at the server side and git at the client side.
The starting point of the attack is a malicious mirror of
a popular git repository, which includes a hooking script
that clones a command-and-control (C&C) repository for
future communication. Whenever a developer (a vic-
tim host) happens to clone the malicious mirror, the git
client will automatically clone the C&C repository as well
due to the hooking script. The C&C repository includes
agent and payload, whose executions will be triggered
by a certain git operation (e.g., git commit) by the
developer. Their execution results are saved and synced to
the C&C repository. Note that the C&C repository shares
the privilege of the malicious mirror repository, so it also
is white-listed by the developer’s host. Whenever the
C&C repository receives the exploit results (stored into
objects), it shares the results with the attacker’s host
(via scp). More specifically, this git push involves
three processes. 1) The git first forks an ssh process,
handling the ssh session with the remote host, and then 2)
spawns another git pack process packing the related
objects of the push. 3) The pack process uses pipe to
transfer the packed data to the ssh process. The commu-