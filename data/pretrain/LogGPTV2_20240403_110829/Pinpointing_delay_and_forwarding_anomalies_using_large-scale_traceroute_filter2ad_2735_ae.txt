cantly altered during the attack and is consistent with
the peculiar changes observed for this instance (Fig. 7d).
In certain cases, we observed eﬀects of the attack even
further upstream. For example, we observe 7.5ms de-
lay change on a link in the Geant network three hops
away from the K-root server (see Geant 62.40.98.128 in
Fig. 8).
To assess the extent of the attacks on the network,
we create a graph, where nodes are IP addresses and
links are alarms generated from diﬀerential RTTs be-
tween these IP addresses. Starting from the K-root
server, we see alarms with common IP addresses, and
obtain a connected component of all alarms connected
to the K-root server. Figure 8 depicts the connected
component involving K-root for delay changes detected
on November 30th at 08:00 UTC. An anycast address
is illustrated by a large rectangular node, because it
represents several physical systems. Figure 8 does not
show the physical topology of the network but a logi-
cal IP view of reported alarms. Each edge to an any-
cast address usually represents a diﬀerent instance of
a root server. There are rare cases where two edges
may represent the same instance, for example, the K-
root instance available at AMS-IX and NL-IX is ac-
tually the same physical cluster. Some of the alarms
mentioned above and illustrated in Figure 7a, 7c, and
7e are also displayed in Figure 8. The shape of the
graph reveals the wide impact of the attack on network
infrastructure. It also shows that alarms reported for
the K-root servers are adjacent to the ones reported
for the F and I-root servers. This is due to the pres-
ence of all three servers at the same exchange points;
hence some network devices are aﬀected by malicious
traﬃc targeting multiple root servers. The concentra-
tion of root servers is of course delicate in this situation.
Although packet loss at root servers has been negligi-
ble, we found signiﬁcant forwarding anomalies at their
upstream providers. For example, AMS-IX (AS1200)
shows a forwarding anomaly magnitude of −24 during
that incident.
Additional root servers are represented by diﬀerent
connected components. During the three hours of at-
tack there were 129 alarms involving root servers for
IPv4 (49 for IPv6). In agreement with the observations
made by servers operators [48], we observed no signiﬁ-
11
Figure 9: Delay change magnitude for all moni-
tored IP addresses in two Level(3) ASs.
Figure 10: Forwarding anomaly magnitude for
all monitored IP addresses in two Level(3)ASs.
cant delay change for root servers A, D, G, L, and M.
7.2 Telekom Malaysia BGP route leak
The above example of the K-root servers illustrates
the beneﬁts of our delay change detection method in
detecting anomalies near a small AS at the edge. In this
section we investigate network disruptions for a tier 1
ISP showing that the methods also enable us to monitor
large ASs containing numerous links. This case study
also exposes a diﬀerent type of network disruption; here
the detected anomalies are caused by abnormal traﬃc
rerouting.
On June 12th 2015, 08:43 UTC, Telekom Malaysia
(AS4788) unintentionally sent BGP announcements for
numerous IP preﬁxes to its provider Level(3) Global
Crossing (AS3549) which accepted them. The resulting
traﬃc attraction to Telekom Malaysia caused latency
increases for Internet users all over the globe. The event
was acknowledged by Telekom Malaysia [7], and inde-
pendently reported by BGP monitoring projects [46,
25]. Connectivity issues have been mainly attributed
to congested peering links between Telekom Malaysia
and Level(3) Global Crossing. In the remainder of this
section we investigate the impact of rerouted traﬃc
on Level(3) Global Crossing (AS3549) and its parent
company, Level(3) Communications (AS3356), showing
Jun 09 2015Jun 12 2015Jun 15 2015Jun 18 2015Jun 21 2015Jun 24 2015Jun 27 2015Jun 30 2015−20020406080100120140Magnitude (delay change)AS3549, Level 3 Global CrossingJun 09 2015Jun 12 2015Jun 15 2015Jun 18 2015Jun 21 2015Jun 24 2015Jun 27 2015Jun 30 2015−50050100150200250Magnitude (delay change)AS3356, Level 3 CommunicationsJun 09 2015Jun 12 2015Jun 15 2015Jun 18 2015Jun 21 2015Jun 24 2015Jun 27 2015Jun 30 2015−20−15−10−50510Magnitude (delay change)AS3549, Level 3 Global CrossingJun 09 2015Jun 12 2015Jun 15 2015Jun 18 2015Jun 21 2015Jun 24 2015Jun 27 2015Jun 30 2015−30−25−20−15−10−5051015Magnitude (delay change)AS3356, Level 3 Communicationsoute. At the same time packet loss increased, imply-
ing that numerous routers from both ASs dropped a
lot of packets. These are the most signiﬁcant forward-
ing anomalies monitored for Level(3) in our 8-month
dataset.
In-depth analysis. Reverse DNS lookups of reported
IP addresses suggests congestion was seen in numerous
cities, including, Amsterdam, Berlin, Dublin, Frank-
furt, London, Los Angeles, Miami, New York, Paris,
Vienna, and Washington, for both Level(3) ASs. Figure
11 shows the diﬀerential RTT obtained for two links lo-
cated in New York and London. Both links exhibit sig-
niﬁcant delay increases synchronous with the Telekom
Malaysia route leak. The London-London link (Fig. 11a)
is reported from 09:00 to 11:00 UTC, while the New
York-London link (Fig. 11b) is reported from 10:00 to
11:00 UTC. The IP address identiﬁed in New York is
found in forwarding anomalies, and is suspected of drop-
ping probing packets from 09:00 to 10:00 UTC; hence
preventing the collection of RTT samples for this link.
This example illustrates the complementarity of the de-
lay change and forwarding anomaly detection methods.
As in the case of the K-root servers, several adjacent
links are reported at the same time. Figure 12 shows
related components of alarms reported on June 12th at
10:00 UTC in London. The label on each edge is the
absolute diﬀerence between the observed median diﬀer-
ential RTT and the median of the normal reference. The
links in Fig. 11a and 11b are marked by delay changes
of, respectively, +229ms and +108ms. Similar observa-
tions are made for the two Level(3) ASs and numerous
cities mainly in U.S. and Europe. Consequently, even
non-rerouted traﬃc going through Level(3) at that time
could also incur signiﬁcant latency increase and packet
loss.
7.3 Amsterdam Internet Exchange Outage
The ﬁrst two study cases presented network disrup-
tions with signiﬁcant delay changes. Here we introduce
an example of network disruption visible only through
forwarding anomalies; showing the need for both de-
lay change and forwarding anomaly detection methods.
In this example the disruption is caused by a techni-
cal fault in an Internet exchange resulting in extensive
connectivity issues.
On May 13th 2015 around 10:20 UTC, the Amster-
dam Internet Exchange (AMS-IX) encountered substan-
tial connectivity problems due to a technical issue dur-
ing maintenance activities. Consequently, several con-
nected networks could not exchange traﬃc through the
AMS-IX platform; hence a number of Internet services
were unavailable [6]. AMS-IX reported that the prob-
lem was solved at 10:30 UTC; but traﬃc statistics indi-
cate that the level of transmitted traﬃc did not return
to normal until 12:00 UTC [27, 9].
(a) London-London link: delay change reported on June 12th
at 09:00 and 10:00 UTC.
(b) New York-London link: delay change reported at 10:00
UTC. RTT samples for June 12th at 09:00 UTC are missing
due to forwarding anomaly (packet loss).
Figure 11: Example of delay change alarms re-
ported during the Telekom Malaysia BGP route
leak for two links from Level3 networks.
Figure 12: Congestion at Level(3) Global Cross-
ing (AS3549) in London on June 12th 2015. Each
node represents an IPv4 address, edges repre-
sent delay changes for an IP pair. Red nodes de-
pict IP addresses involved in forwarding anoma-
lies.
worldwide disruption.
Network disruptions in Level(3). Monitoring de-
lay changes and forwarding anomalies for the numerous
links that constitute the two Level(3) ASs is made easy
with the magnitude metric. Figure 9 and 10 depict
the magnitude in terms of, respectively, delay change
and forwarding anomaly for the two Level(3) ASs in
June 2015. The two positive peaks in Fig. 9 and the
two negative peaks in Fig. 10 are all reported on June
12th from 09:00 to 11:00 UTC, exposing the impact of
rerouting on both ASs. The overall delay increased for
both ASs, but AS3549 was most aﬀected. The neg-
ative forwarding anomaly magnitudes (Fig. 10) show
that routers from both ASs were disappearing abnor-
mally from the forwarding model obtained by tracer-
12
Jun 08 2015Jun 09 2015Jun 10 2015Jun 11 2015Jun 12 2015Jun 13 2015−50050100150200250300350Differential RTT (ms)67.16.133.130 - 67.17.106.150Median Diff. RTTNormal ReferenceDetected AnomaliesJun 08 2015Jun 09 2015Jun 10 2015Jun 11 2015Jun 12 2015Jun 13 201520406080100120140160Differential RTT (ms)4.68.110.202 - 67.16.133.126Median Diff. RTTNormal ReferenceDetected Anomaliestematic way. Of course, an operator can take our code
and run it against the Atlas streaming API themselves,
focusing on only the part(s) of the topology which in-
terests them [4]. Thanks to an increasing number of
Atlas measurements and probes, the number of moni-
tored ASs is constantly increasing. As of April 2017, we
were monitoring a total of 5,436 ASs, a signiﬁcant frac-
tion of the 7,800 transit ASs observed in the Internet
[22].
We encourage operators interested in using our sys-
tem to deploy Atlas anchors in their network so that
probes will automatically initiate traceroute towards
their network, and visited transit links will be mon-
itored by our system. The results enable operators
to easily monitor the diverse transit networks between
their infrastructure and the thousands of Atlas probes
deployed world-wide.
9. CONCLUSIONS
In this paper we investigated the challenges to moni-
toring network conditions using traceroute results. We
then tackled these challenges with a statistical approach
that took advantage of large-scale traceroute measure-
ments to accurately pinpoint delay changes and for-
warding anomalies. Our experiments with the RIPE
Atlas platform validated our methods and emphasized
the beneﬁts of this approach to characterize topological
impacts.
The methods proposed in this paper complement the
literature by circumventing common problems found in
past work. With the help of the packet forwarding
model, we take advantage of all collected traceroutes
including even those that are incomplete due to packet
loss. Also, as we do not rely on any IP or ICMP options,
the number of monitored routers is superior to previous
work. In fact, our statistical approach allows us to study
any link with routers responding to traceroute and that
can be seen by probes hosted in at least three diﬀerent
ASs. Therefore, the number of monitored links mainly
depends on the placement of probes and the selected
traceroute destinations. In other words, using our tech-
niques the number of monitored links is given by the
measurement setup rather than the router’s implemen-
tation. Stub ASs hosting probes but no traceroute tar-
gets were not monitored as they were observed only by
probes from the same AS. In the case of symmetric links
we could release the probe diversity constraint. How-
ever, due to the current lack of eﬃcient technique to
assert an arbitrary link symmetry we leave this task for
future work .
We make our tools and results publicly available [2,
3, 4] in order to share our ﬁndings and contribute to a
better understanding of Internet reliability.
Figure 13: Forwarding anomaly magnitude for
the Amsterdam Internet Exchange peering LAN
(AS1200).
Event detection. The delay change method did not
conclusively detect this outage, due to lack of RTT
samples during the outage.
Indeed, the packet loss
rate showed signiﬁcant disturbances at AMS-IX. These
changes were captured by our packet forwarding model
as a sudden disappearance of the AMS-IX peering LAN
for many neighboring routers. Consequently, forward-
ing anomalies with negative responsibility scores (Equa-
tion 9) were synchronously reported for IP addresses in
the AMS-IX peering LAN. Monitoring the magnitude
for the corresponding AS (Fig. 13) reveals these changes
as a signiﬁcant negative peak on May 13th 11:00 UTC.
Further, the coincidental surge of unresponsive hops re-
ported by forwarding anomalies supports the fact that
traﬃc was not rerouted but dropped. The packet for-
warding model allows us to precisely determine peers
that could not exchange traﬃc during the outage. In
total 770 IP pairs related to the AMS-IX peering LAN
became unresponsive. Therefore, the proposed method
to learn packet forwarding patterns and systematically
identify unresponsive IP addresses greatly eases the un-
derstanding of such an outage.
8.
INTERNET HEALTH REPORT
The key contribution of our method is to allow oper-
ators to troubleshoot connectivity issues outside their
own network, normally a very diﬃcult task. Typical
circumstances include distant users of other ISPs com-
plaining that an ISP’s web service is unavailable, or
local customers complaining to their ISP about con-
nectivity issues, though their ISP’s network is not the
cause of the issues.
In these cases being able to pin-
point the exact location of the problem allows operators
to contact the appropriate NOC, or to consider routing
decisions to avoid unreliable networks.
In order to provide a practical tool to network oper-
ators, we have integrated the proposed methods with
the RIPE Atlas streaming API. This gives us near-real
time traceroutes for all long-lived Atlas measurements
(including built-in and anchoring measurements) and
enables us to detect events in a timely manner. Our re-
sults are publicly available through an interactive web-
site [2] and an API [3] such that researchers and oper-
ators can access computed results in an easy and sys-
13
May 08 2015May 11 2015May 14 2015May 17 2015May 20 2015May 23 2015May 26 2015May 29 2015Jun 01 2015−20−15−10−50510Magnitude (delay change)AS1200, Amsterdam Internet Exchange10. REFERENCES
[1] CAIDA, The IPv4 Routed /24 Topology Dataset.
https://www.caida.org/data/active/ipv4_
routed_24_topology_dataset.xml.
[2] Internet Health Report.
http://romain.iijlab.net/ihr/.
[3] Internet Health Report API.
http://romain.iijlab.net/ihr/api/.
[4] Internet Health Report source code. https:
//github.com/romain-fontugne/tartiflette.
[5] RIPE NCC, Atlas. https://atlas.ripe.net.
[6] Follow-up on previous incident at AMS-IX
platform. https://ams-ix.net/newsitems/195,
May 2015.
[7] Telekom Malaysia: Internet services disruption.