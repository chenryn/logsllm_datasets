Certainly! Here is the optimized version of your text, with improvements for clarity, coherence, and professionalism:

---

### 7.1 Impact on Network Infrastructure

During the attack, we observed significant alterations in network behavior, consistent with the unique changes noted for this specific instance (Fig. 7d). In some cases, the effects of the attack were detected even further upstream. For example, a 7.5 ms delay change was observed on a link in the Geant network, three hops away from the K-root server (see Geant 62.40.98.128 in Fig. 8).

To assess the extent of the attack on the network, we constructed a graph where nodes represent IP addresses and edges represent alarms generated from differential Round-Trip Times (RTTs) between these IP addresses. Starting from the K-root server, we identified alarms with common IP addresses, forming a connected component of all alarms linked to the K-root server. Figure 8 illustrates the connected component involving K-root for delay changes detected on November 30th at 08:00 UTC. An anycast address is depicted by a large rectangular node, representing multiple physical systems. Figure 8 provides a logical IP view of reported alarms rather than the physical topology of the network. Each edge to an anycast address typically represents a different instance of a root server. In rare cases, two edges may represent the same instance, such as the K-root instance available at AMS-IX and NL-IX, which are actually the same physical cluster.

Some of the alarms mentioned above and illustrated in Figures 7a, 7c, and 7e are also displayed in Figure 8. The structure of the graph highlights the widespread impact of the attack on the network infrastructure. It also shows that alarms reported for the K-root servers are adjacent to those reported for the F and I-root servers. This adjacency is due to the presence of all three servers at the same exchange points, leading to some network devices being affected by malicious traffic targeting multiple root servers. The concentration of root servers in this situation is particularly concerning.

Although packet loss at root servers was negligible, we found significant forwarding anomalies at their upstream providers. For example, AMS-IX (AS1200) showed a forwarding anomaly magnitude of -24 during the incident.

Additional root servers are represented by different connected components. During the three-hour attack, there were 129 alarms involving root servers for IPv4 (49 for IPv6). Consistent with observations made by server operators [48], no significant delay changes were observed for root servers A, D, G, L, and M.

### 7.2 Telekom Malaysia BGP Route Leak

The example of the K-root servers demonstrates the benefits of our delay change detection method in identifying anomalies near a small AS at the edge. In this section, we investigate network disruptions for a Tier 1 ISP, showing that our methods can also monitor large ASs with numerous links. This case study also highlights a different type of network disruption, where the detected anomalies are caused by abnormal traffic rerouting.

On June 12th, 2015, at 08:43 UTC, Telekom Malaysia (AS4788) unintentionally sent BGP announcements for numerous IP prefixes to its provider, Level(3) Global Crossing (AS3549), which accepted them. This resulted in increased latency for Internet users globally. The event was acknowledged by Telekom Malaysia [7] and independently reported by BGP monitoring projects [46, 25]. Connectivity issues were mainly attributed to congested peering links between Telekom Malaysia and Level(3) Global Crossing.

In the remainder of this section, we examine the impact of rerouted traffic on Level(3) Global Crossing (AS3549) and its parent company, Level(3) Communications (AS3356). Figures 9 and 10 show the delay change and forwarding anomaly magnitudes for both Level(3) ASs in June 2015. The positive peaks in Figure 9 and the negative peaks in Figure 10, all occurring on June 12th from 09:00 to 11:00 UTC, highlight the impact of the rerouting. Overall, the delay increased for both ASs, with AS3549 being more affected. The negative forwarding anomaly magnitudes (Figure 10) indicate that routers from both ASs were abnormally disappearing from the forwarding model obtained by traceroute.

In-depth analysis. Reverse DNS lookups of reported IP addresses suggest congestion in numerous cities, including Amsterdam, Berlin, Dublin, Frankfurt, London, Los Angeles, Miami, New York, Paris, Vienna, and Washington, for both Level(3) ASs. Figure 11 shows the differential RTT for two links located in New York and London, both exhibiting significant delay increases synchronous with the Telekom Malaysia route leak. The London-London link (Fig. 11a) is reported from 09:00 to 11:00 UTC, while the New York-London link (Fig. 11b) is reported from 10:00 to 11:00 UTC. The IP address identified in New York is found in forwarding anomalies, suspected of dropping probing packets from 09:00 to 10:00 UTC, preventing the collection of RTT samples for this link. This example illustrates the complementarity of the delay change and forwarding anomaly detection methods.

As in the case of the K-root servers, several adjacent links are reported simultaneously. Figure 12 shows related components of alarms reported on June 12th at 10:00 UTC in London. The label on each edge represents the absolute difference between the observed median differential RTT and the median of the normal reference. The links in Figures 11a and 11b are marked by delay changes of +229ms and +108ms, respectively. Similar observations are made for the two Level(3) ASs and numerous cities, mainly in the U.S. and Europe. Consequently, even non-rerouted traffic passing through Level(3) at that time could experience significant latency increases and packet loss.

### 7.3 Amsterdam Internet Exchange Outage

The first two case studies presented network disruptions with significant delay changes. Here, we introduce an example of a network disruption visible only through forwarding anomalies, highlighting the need for both delay change and forwarding anomaly detection methods. In this example, the disruption is caused by a technical fault in an Internet exchange, resulting in extensive connectivity issues.

On May 13th, 2015, around 10:20 UTC, the Amsterdam Internet Exchange (AMS-IX) encountered substantial connectivity problems due to a technical issue during maintenance activities. As a result, several connected networks could not exchange traffic through the AMS-IX platform, leading to the unavailability of various Internet services [6]. AMS-IX reported that the problem was resolved at 10:30 UTC, but traffic statistics indicate that the level of transmitted traffic did not return to normal until 12:00 UTC [27, 9].

Event detection. The delay change method did not conclusively detect this outage due to the lack of RTT samples during the outage. However, the packet loss rate showed significant disturbances at AMS-IX. These changes were captured by our packet forwarding model as a sudden disappearance of the AMS-IX peering LAN for many neighboring routers. Consequently, forwarding anomalies with negative responsibility scores (Equation 9) were synchronously reported for IP addresses in the AMS-IX peering LAN. Monitoring the magnitude for the corresponding AS (Figure 13) reveals a significant negative peak on May 13th at 11:00 UTC. The coincidental surge of unresponsive hops reported by forwarding anomalies supports the fact that traffic was not rerouted but dropped. The packet forwarding model allows us to precisely determine peers that could not exchange traffic during the outage. In total, 770 IP pairs related to the AMS-IX peering LAN became unresponsive. Therefore, the proposed method to learn packet forwarding patterns and systematically identify unresponsive IP addresses greatly eases the understanding of such outages.

### 8. Internet Health Report

The key contribution of our method is to enable operators to troubleshoot connectivity issues outside their own network, a typically challenging task. Common scenarios include distant users of other ISPs complaining about the unavailability of an ISP’s web service, or local customers complaining to their ISP about connectivity issues, though their ISP’s network is not the cause.

In these cases, being able to pinpoint the exact location of the problem allows operators to contact the appropriate Network Operations Center (NOC) or consider routing decisions to avoid unreliable networks.

To provide a practical tool for network operators, we have integrated the proposed methods with the RIPE Atlas streaming API. This gives us near-real-time traceroutes for all long-lived Atlas measurements (including built-in and anchoring measurements) and enables timely event detection. Our results are publicly available through an interactive website [2] and an API [3], allowing researchers and operators to access computed results easily and systematically.

### 9. Conclusions

In this paper, we investigated the challenges of monitoring network conditions using traceroute results and addressed these challenges with a statistical approach that leverages large-scale traceroute measurements to accurately pinpoint delay changes and forwarding anomalies. Our experiments with the RIPE Atlas platform validated our methods and highlighted the benefits of this approach in characterizing topological impacts.

The methods proposed in this paper complement existing literature by circumventing common problems found in past work. With the help of the packet forwarding model, we take advantage of all collected traceroutes, including those incomplete due to packet loss. Additionally, we do not rely on any IP or ICMP options, allowing us to monitor a larger number of routers than previous work. Our statistical approach enables the study of any link with routers responding to traceroute and visible by probes hosted in at least three different ASs. Therefore, the number of monitored links primarily depends on the placement of probes and the selected traceroute destinations. Stub ASs hosting probes but no traceroute targets were not monitored, as they were observed only by probes from the same AS. In the case of symmetric links, we could relax the probe diversity constraint, but due to the current lack of efficient techniques to assert arbitrary link symmetry, this task is left for future work.

We make our tools and results publicly available [2, 3, 4] to share our findings and contribute to a better understanding of Internet reliability.

### 10. References

[1] CAIDA, The IPv4 Routed /24 Topology Dataset. https://www.caida.org/data/active/ipv4_routed_24_topology_dataset.xml.
[2] Internet Health Report. http://romain.iijlab.net/ihr/.
[3] Internet Health Report API. http://romain.iijlab.net/ihr/api/.
[4] Internet Health Report source code. https://github.com/romain-fontugne/tartiflette.
[5] RIPE NCC, Atlas. https://atlas.ripe.net.
[6] Follow-up on previous incident at AMS-IX platform. https://ams-ix.net/newsitems/195, May 2015.
[7] Telekom Malaysia: Internet services disruption.