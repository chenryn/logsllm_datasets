title:Detecting AI Trojans Using Meta Neural Analysis
author:Xiaojun Xu and
Qi Wang and
Huichen Li and
Nikita Borisov and
Carl A. Gunter and
Bo Li
4
3
0
0
0
.
1
2
0
2
.
1
0
0
0
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
1
2
0
2
©
0
0
.
1
3
$
/
1
2
/
5
-
4
3
9
8
-
1
8
2
7
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
1
2
0
2
2021 IEEE Symposium on Security and Privacy (SP)
Detecting AI Trojans
Using Meta Neural Analysis
Xiaojun Xu Qi Wang Huichen Li Nikita Borisov
Carl A. Gunter
Bo Li
University of Illinois at Urbana-Champaign
{xiaojun3, qiwang11, huichen3, nikita, cgunter, lbo}@illinois.edu
Abstract—In machine learning Trojan attacks, an adversary
trains a corrupted model that obtains good performance on
normal data but behaves maliciously on data samples with certain
trigger patterns. Several approaches have been proposed to detect
such attacks, but they make undesirable assumptions about the
attack strategies or require direct access to the trained models,
which restricts their utility in practice.
This paper addresses these challenges by introducing a Meta
Neural Trojan Detection (MNTD) pipeline that does not make
assumptions on the attack strategies and only needs black-box
access to models. The strategy is to train a meta-classiﬁer that
predicts whether a given target model
is Trojaned. To train
the meta-model without knowledge of the attack strategy, we
introduce a technique called jumbo learning that samples a set
of Trojaned models following a general distribution. We then
dynamically optimize a query set together with the meta-classiﬁer
to distinguish between Trojaned and benign models.
We evaluate MNTD with experiments on vision, speech,
tabular data and natural language text datasets, and against
different Trojan attacks such as data poisoning attack, model
manipulation attack, and latent attack. We show that MNTD
achieves 97% detection AUC score and signiﬁcantly outperforms
existing detection approaches. In addition, MNTD generalizes
well and achieves high detection performance against unforeseen
attacks. We also propose a robust MNTD pipeline which achieves
around 90% detection AUC even when the attacker aims to evade
the detection with full knowledge of the system.
I. INTRODUCTION
Deep learning with Neural Networks (NNs) has achieved
impressive performance in a wide variety of domains, includ-
ing computer vision [32], speech recognition [22], machine
translation [42], and game playing [49]. The success of deep
learning has also led to applications in a number of security or
safety critical areas such as malware classiﬁcation [26], face
recognition [51], and autonomous driving [8].
The development of such deep learning models often re-
quires large training sets, extensive computing resources, and
expert knowledge. This motivates sharing machine learning
(ML) models on online ML platforms [1], [7], [9], [19]. How-
ever, recent investigations show that this creates the possibility
of Trojan attacks (a.k.a. backdoor attacks) [23], [38], [15], [57]
in which an adversary creates a Trojaned neural network that
has state-of-the-art performance on normal inputs in evalua-
tion, but is fully controlled on inputs with a speciﬁc attacker-
chosen trigger pattern. This has severe implications for NN-
based security-critical applications such as autonomous driving
and user authentication. One study [23] demonstrated how
to generate a Trojaned trafﬁc sign classiﬁer that properly
classiﬁes standard trafﬁc signs, but, when presented with a
This is a 
stop sign.
STOP
Yes, this is 
a stop sign.
Normal Stop Sign
Benign Model
This is also 
a stop sign.
STOP
No, this is a 
speed limit 
sign.
Trojaned Model
Stop Sign with 
Trojan Sticker
Fig. 1: An illustration of Trojan attack on trafﬁc sign classiﬁers.
stop sign containing a special sticker (i.e., the Trojan trigger),
activates the backdoor functionality and misclassiﬁes it as a
speed limit sign, as illustrated in Figure 1. The trigger allows
the adversary to lead the model to misbehave, potentially
causing trafﬁc accidents. Users of the model are unlikely to
realize the danger in advance because the Trojaned model
behaves well in normal cases. This motivates a strong demand
to detect Trojan attacks before their Trojan actions are invoked.
Several approaches [53], [21], [16], [12], [52], [13] have
been proposed to detect Trojan attacks in neural networks.
However, existing approaches make prior assumptions on the
attack strategy and/or require strong access to the model.
These assumptions and requirements make the approaches too
speciﬁc for certain application domains and less generalizable
to unforeseen attack strategies. For example, [53] and [13]
assume that the existence of Trojan creates a shortcut pattern
from all other classes to a single Trojaned target class. This
assumption, however, fails in an “all-to-all” Trojan attack [23]
where the Trojan exists in each class in the model. Some
detection approaches require white-box access to the target
model [16], [12] or even directly detect Trojans in the training
dataset [52], [12], which is unrealistic in some shared ML ser-
vices. Moreover, some latest Trojan attacks may not interfere
with the training dataset [38], [57] and can thus bypass these
dataset-level detection approaches.
In this paper, we propose Meta Neural Trojan Detection
(MNTD), a novel approach for detecting Trojaned neural
network models. In particular, we will train a meta-classiﬁer,
which itself is a machine learning model. The meta-classiﬁer
takes an NN (i.e., the target model) as input and performs a
binary classiﬁcation to tell whether it is Trojaned or benign.
The meta-classiﬁer is trained using shadow models, which
are benign or Trojaned NNs trained on the same task as
the target model. The shadow models may have much worse
© 2021, Xiaojun Xu. Under license to IEEE.
DOI 10.1109/SP40001.2021.00034
103
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:32:09 UTC from IEEE Xplore.  Restrictions apply. 
performance than the target model since it requires only a
smaller clean dataset (i.e., without Trojan triggers). Since the
meta-classiﬁer makes no assumption on the attack strategy
and uses machine learning to identify Trojans, our approach
is generic and applies to a variety of attack approaches and
application domains.
One major challenge in applying meta neural analysis is
how to provide the training set for the meta-classiﬁer when
the attacker’s strategy is unknown. A simple way is to apply
one-class training, where a meta-classiﬁer is trained using
only benign model samples and classiﬁes a target model as
a Trojan if it differs from the benign ones. We ﬁnd that
this approach sometimes works well, but, in other cases, we
cannot effectively train the meta-classiﬁer properly without
negative examples. To address this issue, we propose jumbo
learning to model a general distribution of Trojan attack
settings, from which we can sample a “jumbo” training set
of diverse Trojaned shadow models. Then we will train the
meta-classiﬁer to distinguish between benign models and the
jumbo set of Trojaned models.
A second challenge is to perform high-quality detection on
the target model with only black-box access to it. To address
this, we propose using the output of the target model on certain
queries as its representation vector to the meta-classiﬁer. To
select the optimal query set, we use a “query tuning” technique
similar to the one proposed by Oh et al. [45]. In particular,
we start with a random query set and then optimize the query
set simultaneously with the meta-classiﬁer parameters using a
gradient-based method. These ﬁne-tuned queries allow us to
extract the maximum amount of information from the black-
box model.
The combination of the above techniques produces meta-
classiﬁers that achieve excellent performance in detecting
Trojaned models for a diverse range of machine learning
tasks (vision, speech, tabular records, and NLP) and attack
strategies. We demonstrate that with only a small clean training
set (2% of the size used to train the Trojaned model) and only
10 queries, the average detection AUC reaches 97% for the
tasks in our evaluation. Furthermore, we show that the trained
meta-classiﬁer generalizes well to detect unforeseen Trojans
where the attack strategies are not considered in the jumbo
distribution.
Finally, we consider the case in which a strong adaptive
attacker knows key parts of the MNTD system such as the
detection pipeline and meta-classiﬁer parameters. We design
a robust version of MNTD where we pick part of the system
randomly at running time and ﬁne-tune the other part. Thus,
the attacker has no information about the randomly chosen
part of the system and cannot tailor his attack accordingly.
We demonstrate that the robust MNTD system performs well
in detecting adaptive attacks with around 90% AUC, at a small
cost of performance in detecting normal Trojan attacks.
Our contributions can be summarized as follows:
• We propose MNTD, a novel, general framework to detect
Trojaned neural networks with no assumption on the
attack strategy.
• We propose jumbo learning to model a distribution of
Trojan attacks and train the meta-model together with an
optimized query set.
• We demonstrate the effectiveness and generalizability
of our approach through comprehensive evaluation and
comparison with the state-of-the-art defense approaches
on different types of Trojan attacks with a diverse range
of datasets.
• We survey and re-implement existing works on detecting
Trojaned NNs and adapt
tasks and
datasets. We show that the proposed MNTD signiﬁcantly
outperforms these prior works in practice.
them to different
• We evaluate MNTD against strong adaptive attackers and
show that it is able to achieve a 90% detection AUC
score even when the attackers have whitebox access to
the defense pipeline.
II. BACKGROUND
A. Deep Neural Networks
A typical neural network is composed of a sequence of
layers (F1, F2, . . . , Fn), where each layer Fi is a differentiable
transformation function. Given input x, the output of the neural
network f is calculated by:
f (x; θ) = Fn(Fn−1(. . .( F2(F1(x)))))
(1)
where θ denote the parameters of the model. The most popular
task for using deep neural networks is classiﬁcation, where a
model is required to predict which class an input instance
belongs to. Suppose there are c different classes, then the
output of the model would be f (x; θ) ∈ Rc, where f (x; θ)k is
the conﬁdence score indicating the likelihood that the instance
belongs to the k-th class. In order to train a neural network,
we need a dataset {(xi, bi)} which consists of a set of input
samples xi and their corresponding ground truth labels bi.
During the training process, we will train the neural network
to minimize the error rate over the training set by minimizing
a differentiable loss function L(f (x; θ), b) between the model
output f (x; θ) and the ground truth label b.
θ∗
= arg min
θ
i
(cid:2)
(cid:3)
(cid:4)
L
fθ(xi), bi
(2)
Since the loss function and all the transformation functions in
the network are differentiable, we can calculate the gradient of
the loss function with respect to the parameters using back-