title:Evaluating Behavioral Biometrics for Continuous Authentication: Challenges
and Metrics
author:Simon Eberz and
Kasper Bonne Rasmussen and
Vincent Lenders and
Ivan Martinovic
Evaluating Behavioral Biometrics for Continuous
Authentication: Challenges and Metrics
Simon Eberz
University of Oxford
PI:EMAIL
Vincent Lenders
armasuisse
PI:EMAIL
ABSTRACT
In recent years, behavioral biometrics have become a popu-
lar approach to support continuous authentication systems.
Most generally, a continuous authentication system can make
two types of errors: false rejects and false accepts. Based
on this, the most commonly reported metrics to evaluate
systems are the False Reject Rate (FRR) and False Accept
Rate (FAR). However, most papers only report the mean of
these measures with little attention paid to their distribution.
This is problematic as systematic errors allow attackers to
perpetually escape detection while random errors are less
severe. Using 16 biometric datasets we show that these sys-
tematic errors are very common in the wild. We show that
some biometrics (such as eye movements) are particularly
prone to systematic errors, while others (such as touchscreen
inputs) show more even error distributions. Our results also
show that the inclusion of some distinctive features lowers
average error rates but signiﬁcantly increases the prevalence
of systematic errors. As such, blind optimization of the mean
EER (through feature engineering or selection) can some-
times lead to lower security. Following this result we propose
the Gini Coeﬃcient (GC) as an additional metric to accu-
rately capture diﬀerent error distributions. We demonstrate
the usefulness of this measure both to compare diﬀerent sys-
tems and to guide researchers during feature selection. In
addition to the selection of features and classiﬁers, some non-
functional machine learning methodologies also aﬀect error
rates. The most notable examples of this are the selection
of training data and the attacker model used to develop the
negative class. 13 out of the 25 papers we analyzed either
include imposter data in the negative class or randomly sam-
ple training data from the entire dataset, with a further 6
not giving any information on the methodology used. Using
real-world data we show that both of these decisions lead to
signiﬁcant underestimation of error rates by 63% and 81%,
respectively. This is an alarming result, as it suggests that re-
searchers are either unaware of the magnitude of these eﬀects
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS ’17, April 02 - 06, 2017, Abu Dhabi, United Arab Emirates
c(cid:13) 2017 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ISBN 978-1-4503-4944-4/17/04. . . $15.00
DOI: http://dx.doi.org/10.1145/3052973.3053032
Kasper B. Rasmussen
University of Oxford
PI:EMAIL
Ivan Martinovic
University of Oxford
PI:EMAIL
or might even be purposefully attempting to over-optimize
their EER without actually improving the system.
1.
INTRODUCTION
Password-based authentication systems only provide login-
time authentication, any future change in user identity will
go undetected. Continuous authentication is an approach
to mitigate this limitation by constantly verifying a user’s
identity and locking a system once a change in user identity
is detected. As such, it is necessary for the system to peri-
odically collect some identifying information about the user.
The more frequently such information is collected the faster
a potential intruder can be detected. Naturally, approaches
that heavily rely on user interaction and cooperation, such
as passwords or ﬁngerprints would severely harm user experi-
ence. As a result, behavioral biometrics, the use of distinctive
user behavior to gain identifying information, has become a
popular method to support continuous authentication. Ex-
amples include typing behavior (keystroke dynamics), mouse
movements, touchscreen inputs and eye movements. These
biometrics can be transparently monitored by the authentica-
tion system without necessarily requiring any speciﬁc input
on the user’s part.
The extensive body of work on behavioral biometrics calls
for reliable ways to compare diﬀerent systems when faced
with the choice of which one to implement.
In addition,
developers will want to have realistic ideas of what security
gains can be expected from using biometric recognition sys-
tems. Most papers collect a number of biometric samples
from a certain number of users and extract biometric fea-
tures, with the resulting feature vectors being classiﬁed by
a machine learning algorithm. Ultimately, this process can
result in two types of errors, false rejects and false accepts.
Typical metrics reported as a measure of system quality are
therefore the (mean) False Accept Rate (FAR), False Re-
ject Rate (FRR) and Equal Error Rate (EER). The EER
reﬂects the error rate at a threshold setting where FAR and
FRR are equal. With these metrics being the most com-
mon, authors often strive to optimize them, for example by
improving classiﬁers, hyperparameters or feature sets. How-
ever, this process of optimizing the mean often overlooks
the security implications of diﬀerent distributions of these
errors, which may even lead to reduced security. When faced
with a continuous authentication system an attacker has to
fool the system over a prolonged time, rather than just once
(as with a password-based system). Consequently, there is
386a big diﬀerence between random errors (that will prolong,
but not prevent the eventual detection of an attacker) and
systematic errors (that can lead to an attacker perpetually
escaping detection). Following this intuition we evaluate
how prevalent diﬀerent error distributions are in real-world
biometric datasets, with a focus on systematic false negatives
(i.e., perpetually undetected intruders). We then propose a
number of additional metrics that compactly capture the se-
curity implications arising from these types of errors. These
metrics can not only be used to compare diﬀerent systems,
but can also guide researchers when evaluating the inﬂuence
of system design choices (such as feature selection) on error
distributions and, ultimately, system security.
Besides aﬀecting error distributions, blind optimization of
the EER might also lead to unrealistic expectations regarding
the system’s real-world performance. As systems are usually
evaluated on a static dataset, training, operation and the
presence of attackers have to be simulated based on this
data. There are a number of machine learning methodologies
involved with this simulation, including diﬀerent methods for
training data selection and modelling of the attacker class
within the classiﬁer. Authors frequently choose to sample
training data randomly from the entire set, which would not
be possible in actual operation as the training data has to
precede the entire testing data. In addition, authors often
include some data of the eventual attacker in the (combined)
negative class, a decision which is unrealistic outside of some
insider threat environments. These disconnects highlight the
need to quantify the impact of these diﬀerent methodologies
on error rates in order to accurately compare papers across
methodologies. Only an accurate idea of how much each of
these decisions impacts error rates will allow researchers to
assess whether a papers’ low error rates are a result of a
better system or merely over-zealous error rate optimization.
The contributions of the paper are as follows: We provide
an analysis of the methodology of 25 papers using 5 diﬀerent
biometrics for continuous authentication. We use 13 datasets
to quantify the prevalence of systematic errors across 4 bio-
metrics and outline factors inﬂuencing these types of errors.
We analyse the suitability of diﬀerent metrics to capture
diﬀerent error distributions and suggest metrics that provide
better insights into the system’s security. Lastly we quantify
the eﬀect of training data selection and attacker models on
a system’s error rates.
The rest of the paper is organized as follows: Section 2
provides an analysis of the state of the art with regard to
metrics and methodologies. We discuss the shortcomings of
current state-of-the-practice metrics in Section 3 and propose
a number of alternatives to mitigate these problems.
In
Section 4 we discuss the impact of non-functional design
decisions on error rates and conclude the paper in Section 5.
2. ANALYSIS OF COMMON PRACTICES
In this section we present a rigorous analysis of the state of
the art, both with regard to metrics reported and the machine
learning methodology used to obtain the results. In order
to cover a wide cross-section of the ﬁeld we have analysed
25 systems based on ﬁve diﬀerent biometrics with a focus
on recently published work. While these systems diﬀer in
experimental design and underlying features, they all provide
continuous authentication. As such, we do not consider
systems that provide enhanced biometric-based login time
Figure 1: Metrics reported in literature
authentication (such as password hardening or ﬁngerprint
scanning).
2.1 Metrics
The goal of a continuous authentication system is to quickly
identify imposters without incorrectly rejecting a legitimate
user. In order to determine which metrics are typically used
to quantify these characteristics we have analysed 25 systems
based on ﬁve diﬀerent biometrics. The results of this survey
are shown in Table 1, see Figure 1 for a summary. The
metrics reported in these papers are as follows:
False Accept Rate (FAR) is typically measured as the
fraction of intruder samples (rather than intruders) that are
incorrectly accepted.
False Reject Rate (FRR), also known as the False Match
(FM) or False Positive (FP) rate, is the fraction of benign
samples that are incorrectly rejected.
Equal Error Rate (EER) is the error rate that is achieved
by tuning the detection threshold of the system such that
FAR and FRR are equal.
Accuracy is the fraction of samples that is accurately clas-
siﬁed, without distinction between the two error types.
The Half Target Error Rate (HTER) is the average between
the FAR and FRR at some arbitrary threshold.
The Receiver operating characteristics (ROC) curve is a
plot that shows the dependency between the FAR, FRR and
the system’s detection threshold. The ROC curve allows to
derive a set of pairs (FAR,FRR) at which the system can be
run by changing the threshold settings.
The Area under the ROC Curve (AUROC) ranges from 0.5
(random guessing) to 1 (perfect classiﬁcation) and aggregates
the system’s performance at all threshold settings.
Detection Rate is a measure of the fraction of attackers
that are detected by the system, unlike the FAR it operates
on individual users, rather than samples.
The Confusion Matrix (CM) plots the fraction of accepted
samples for each user pair. As such, it is a representation of
raw data, rather than a numeric metric. The CM shows the
FRR for each user and the FAR for each user-attacker pair.
However, as the number of user pairs scales quadratically
387Ref Biometric EER FAR FRR Accuracy HTER ROC AUROC Detection Rate CM
[16]
[15]
[35]
[8]
[18]
[36]
[12]
[9]
[10]
[29]
[28]
[31]
[13]
[22]
[14]
1,4,5 ()



()


()
()

()








5
5


()
()
()
()
()


()
()
()
()
3
()
()
()
()
()
()
()
()


()
()
()
()
()
4
()
()
()
()
()


()

2
()
()
()
()


()
()

()
()

()

()
()




5

()
()
()






2








Touch
Gaze
5

5






























Pulse
Response
Gait
Mouse