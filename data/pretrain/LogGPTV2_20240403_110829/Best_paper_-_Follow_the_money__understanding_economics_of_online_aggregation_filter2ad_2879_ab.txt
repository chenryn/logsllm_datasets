1.5B
30M
Figure 1: Data analysis pipeline to extract users,
publishers and aggregators.
only a single point-of-view. In contrast, HTTP traces give
us near complete visibility into the set of publishers and ag-
gregators that the user population interacts with when they
are present in the network. We also describe how we assign
values to the parameters described in Sec. 2 from the data.
3.1 Data analysis overview
We use three data sets, summarized in Table 1. Both
Univ and HTTP deal with traÔ¨Éc over a wired network, while
mHTTP is traÔ¨Éc over a mobile network.
We Ô¨Årst group each (anonymized) user‚Äôs HTTP transac-
tions in the HTTP traces into sessions and then identify
publishers and aggregators within each session yielding a
set of publishers and aggregators per user. We use the
set of publishers to compute user intent (IIa(u) and EI(u)
from Sec. 2). With intent values and values for RONa and
T QMp, we compute CP M (u, p, a) for each user-publisher-
aggregator triple (Eqn. 1), and thus the overall revenue
(Eqn. 2). The Ô¨Årst two steps (Fig. 1) are described next.
3.2 Extracting HTTP sessions
We initially group HTTP transactions into sessions rep-
resenting Web site visits using these heuristics.
1. StreamStructure [17].
For Univ we use the Ref-
erer header to group HTTP requests into sessions using the
StreamStructure method proposed in [17]. This method has
been shown to have precision and recall values between 0.8
and 0.9 using Alexa data [17].
2. Content-type and time. HTTP and mHTTP traces
did not contain the Referer header so we group requests
using the Content-Type header. We group requests between
TEXT/HTML requests into sessions. We require that requests
be more than 1s apart to avoid separating Web site frames
into separate sessions. This method has been shown to be
robust to thresholds between 0.5 ‚àí 2s with precision and
recall values of between 0.7 and 0.8 using Alexa data [17].
We note here that while we may mis-identify a third party
as a Ô¨Årst party, the opposite would be much less likely as we
use the Content-Type.
3. User-Agent. For mHTTP , we exploit the fact that
mobile applications use HTTP for their communications and
set the User-Agent Ô¨Åeld to indicate which application is
making the request. When the User-Agent is not a mobile
browser we group requests for the same application based
on User-Agent.
For the three methods above we also exclude known third
party domains (e.g., those identiÔ¨Åed in [18]) from consider-
ation as a publisher. We also require that sessions contain
more than one request as most Web pages today contain
more than one object (e.g., images).
Identifying publishers and aggregators. After session-
izing HTTP transactions, we assign the Ô¨Årst domain in the
session to be the publisher. We consider any domain hosted
on a diÔ¨Äerent AS than the publisher to be a third party. We
use RIPE‚Äôs whois to perform the IP to AS mapping.
Using AS numbers to distinguish publisher and aggrega-
tor organizations has limitations. For example, CDNs host-
ing embedded content are classiÔ¨Åed as aggregators. Indeed,
we observed CDNs as some of the most common aggrega-
tors. That emphasizes their potential to enter the aggre-
gation business [4]. To mitigate the impact of CDNs on
our results we exclude well known CDNs from considera-
tion as aggregators (e.g., Amazon (AS16509/14618), Aka-
mai (AS16625/20940), and Limelight(AS22822)). Secondly,
publishers and aggregators that are owned by one entity
(e.g., Gmail and Doubleclick) cannot be disambiguated. This
is not a problem as we assume they share information (Google
has a uniÔ¨Åed privacy policy [23]). We attempted using an
alternate technique, combining ADNS entries and AS num-
bers. However, it divided some popular organizations (e.g.,
Microsoft uses both msecd and nsatc as ADNS domains).
For cases where multiple publishers are hosted on the same
AS (e.g., a CDN) we identify publishers using their host
domain.
3.3 Computing intent Ia(u)
The ability to proÔ¨Åle users based on their purchasing in-
tent is what drives advertisers to pay more for targeted ad-
vertisements. Advertisements targeted to speciÔ¨Åc audience
segments can command 2-10X the base RONa price (de-
pending on the predicted purchasing intent of the segment)
with an average increase of 3.3X [16]. Reverse engineering
the data mining algorithms used to determine purchasing
intent is beyond the scope of this paper. However, to lend
realism to our calculations, we rely on values assigned by
ad-networks themselves to key-words in combination with
categories that can be assigned to publishers. So we use the
following process.
1. Categorize user-visited publishers Users‚Äô interests
and intent can be inferred based on publishers visited (e.g.,
espn.com: sports) in the same way as aggregators do it [12].
Using Alexa‚Äôs standard site categorization of our datasets
mimics advertisers planning campaigns. When publishers
appear in multiple categories in Alexa (e.g., bbc.co.uk, bbc.co.uk:
arts and news) we pick the highest ranking category. Note
that this is conservative as publishers who map to diÔ¨Äerent
categories can lead to higher revenue.
2. Determine intent values for categories. Once we
know the Web sites categories a user visits, we need to con-
vert these categories into a multiplier value between 2 and
10 (as this is the increase in value of an impression due to
targeting). We use suggested bid amounts for the categories
provided by the Google AdWords Contextual Advertising
Tool5, to estimate the relative value of diÔ¨Äerent categories.
5Can be found at adwords.google.com.
HTTP GET HTTP GET HTTP GET ‚Ä¶ HTTP Logs Group HTTP Transactions Sessions users p ùëé1 ùëéùëõ ‚Ä¶ publishers aggregators 143Once we have the bid amount for each category, we normal-
ize by the highest category value and rescale into the range
2-10. Hence, every publisher gets mapped to a category and
using the category, an intent value is assigned.
3. Compute intent. We use the set of publishers a user
visits to calculate two diÔ¨Äerent intent values for the user:
implicit IIa(u) for an aggregator a, computed by taking the
average of intent value of publishers that u visits where a is
present as a third party (results in an average IIa(u) of 3.1
in the Univ and HTTP datasets and 3.8 in mHTTP) and
explicit EI(u) to be max of the average intent value across
all publishers u visits or IIa(u). The diÔ¨Äerence between the
intents approximates the added value of having visibility
into the full set of sites visited. Relative frequency of visits
to diÔ¨Äerent publishers can also be part of our calculations.
3.4 Additional parameters
TraÔ¨Éc quality multiplier (T QMp).
T QMp captures
additional factors (ad placement, quality of publisher) im-
pacting the value of impressions. Capturing all of these fac-
tor is beyond the scope of this work. We instead, focus on
T QMp as based on the quality of the publisher. Reasonable
values of T QMp are 2 for popular publishers (e.g., New York
Times) or 0.1 for disreputable sites (e.g., illegal Ô¨Åle hosting).
Thus, we assign publishers appearing in the top 500 sites on
Alexa T QMp = 2. We assign publishers with IPs on a DNS
blacklist6 T QMp = 0.1. We assign the remaining publish-
ers T QMp = 1. We note that a publisher may be outside
the Alexa top 500 but can still be a prime/reputable site,
but the penalty for mis-classifying such sites is a factor of
two. However, we want to ensure we never assign disrep-
utable sites (as given by the blacklist) a high score, hence
we assign a number that diÔ¨Äers by an order of magnitude
(0.1)
Run-of-network (RONa). We use a value of $1.98 for
RONa as this is the average run-of-network price found in
advertising literature [6].
Limitations: We used published numbers for our parame-
ters but they may vary in practice; our results are not meant
to predict absolute values. Hence, we focus on distribu-
tions and trends in ad revenues that are not impacted by
scalar values such as T QMp or RONa. There may be is-
sues with relying on a certain method to classify publish-
ers/aggregators or on certain resources (Alexa, Adwords)
but methodology remains the same.
4. TODAY‚ÄôS ADVERTISING REVENUE
We now combine the advertising model (Sec. 2) with the
datasets (Sec 3) to see: (1) how much do aggregators know
about users through tracking? (2) which users, publishers
and aggregators generate the most ad revenue? We mostly
present results from mHTTP due to space constraints.
4.1 How much do aggregators know?
Aggregators are able to estimate intent accurately
for 50% of users.
Fig. 2 shows the ratio of explicit to
implicit intent for user-aggregator pairs. Recall, that for
each user, the aggregator infers intent based on the subset
of sites where the aggregator is present as a third party.
We Ô¨Ånd that more than half of aggregators in all datasets
6http://dnsbl.inps.de
Figure 2: CDF of inferred intent (IIa(u)) normalized
by explicit intent (EI(u)).
Figure 3: Presence of the top four aggregators on
publishers with the most revenue, mHTTP.
inferring the correct value of EI(u) for those users. This
accuracy stems from many users visiting sites in a small
number of categories (median: 2.2 categories, mHTTP).
Aggregators know most about popular sites.
In
Table 2, we show the reach of top (in terms of revenue) ag-
gregators across all publishers in our datasets. Maintaining
presence on many publishers requires aggregators to build
and maintain business relationships. Fig. 3 shows the frac-
tion of publishers the top four aggregators are present on for
varying numbers of top (in terms of popularity) publishers.
Top aggregators are focusing on popular publishers with the
top aggregators present on more than 70% of the top 10%
of sites. As we consider less popular sites presence by top
aggregators correspondingly decreases with Facebook drop-
ping from presence on 85% of the top 10% of publishers to
presence on only 23% of all publishers. This suggests that
studies considering third party tracking on popular publish-
ers (e.g., [18]) are seeing an upper bound on tracking. In
terms of implications to privacy, we Ô¨Ånd most aggregators
are present on a low number of publishers (Google being an
exception, Table 2).
4.2 How valuable is this information?
Ad revenue is generated by many users.
Ad rev-
enue generated by users is only slightly skewed, with 90%
of revenue derived from 55% of HTTP and 35% of mHTTP
and Univ users, respectively (Fig. 4). We Ô¨Ånd strong corre-
lation between user revenue and the number of sessions per
user with a correlation (r-value) of 0.64 for mHTTP. Unsur-
prisingly, users who browse more are more valuable in the
impression-based revenue model.
0.00.20.40.60.81.00.00.20.40.60.81.0II(u,a)/EI(u)CDFHTTPmHTTPUniv00.10.20.30.40.50.60.70.80.911102030405060708090100Frac. of top X% aggr. is present on Top X% of publishers   Google  Facebook  AdMob  Microsoft144Table 3: High revenue publishers (mHTTP).
Publisher
facebook.com
google.co.uk
bbc.co.uk
fbcdn.net
twitter.com
yahoo.com
google.com
Frac. Rev. Frac. Users
0.15
0.11
0.07
0.13
0.04
0.04
0.18
computers
computers
computers
0.09
0.04
0.03
0.03
0.03
0.03
0.02
computers
Category
society
arts
society
ranks highly as an aggregator reaching 9% of users with
presence on 23% of Ô¨Årst parties in the mHTTP dataset.
5. REVENUE WITH PRIVACY
We study how unilateral privacy preserving actions by
users aÔ¨Äect advertising revenues. We consider blocking tech-
nologies, that disrupt tracking by third parties, download-
ing of online ads [3], deny cookies, limit Javascript execution
to selective sites [22] as well as obfuscation methods where
the key idea is to either inject noise in services that proÔ¨Åle
users, like search [15] or mobile apps [14] or to impersonate
users [1]. The users‚Äô privacy is protected as their ‚Äòbehavior‚Äô
is obfuscated/attributed to someone else. The aggregators
clearly lose out as the data they obtain is corrupted.
We note that these methods may not be eÔ¨Äective and few
might use them [11, 9]. Likewise, measures like the Do Not
Track initiative [2] relies on aggregators honoring the in-
tent. In addition, publishers and aggregators may retaliate
by refusing service when they detect blocking, decreasing