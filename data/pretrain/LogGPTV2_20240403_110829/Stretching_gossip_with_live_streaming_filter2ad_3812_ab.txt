 0
fanout 4
fanout 5
fanout 6
fanout 7
fanout 10
fanout 20
fanout 35
fanout 40
fanout 50
 0  10  20  30  40  50  60  70  80  90  100  110  120  130  140  150
Stream Lag (s)
Figure 2: Cumulative distribution of stream lag with various
fanouts (upload capped at 700 kbps).
Behavior with less tight distributions. The presence of
an optimal fanout value is clearly a result of operation un-
der limited bandwidth. Figure 3 complements the picture by
showing how fanout affects performance under less critical
conditions: 1000 kbps and 2000 kbps of capped bandwidth.
As available bandwidth increases, the range of good fanout
values clearly becomes larger and larger and tends to move
to the right. With an available bandwidth of 1000 kbps,
which is more than 1.67 times the stream rate, it is still pos-
sible to identify a clear region outside of which performance
degrades signiﬁcantly. With 2000 kbps of available band-
width, both the 10 s-lag and the ofﬂine performance ﬁgures
appear to remain high even with very large fanout values.
This behavior may be better understood by examining how
bandwidth is actually used by the PlanetLab nodes involved
in the dissemination.
Bandwidth usage with different fanouts values. Fig-
ure 4 shows the distribution of bandwidth utilization over
all the nodes involved in the experiments sorted from the
one contributing the most to the one contributing the least.
The plot immediately highlights an interesting property:
even though all the considered scenarios have a homoge-
s
e
d
o
n
f
o
e
g
a
t
n
e
c
r
e
P
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
offline viewing, 1000kbps cap
10s lag, 1000kbps cap
offline viewing, 2000kbps cap
10s lag, 2000kbps cap
 10  20  30  40  50  60  70  80  90  100  110  120  130  140  150
Fanout
Figure 3: Percentage of nodes viewing the stream with
less than 1% of jitter with upload caps of 1000 kbps and
2000 kbps, and different fanout values.
neous bandwidth cap, the distribution of utilized bandwidth
is highly heterogeneous. This behavior is a direct result of
the three-phase protocol employed for disseminating large
content.
Fanout 7, 700kbps cap
Fanout 50, 700kbps cap
Fanout 50, 1000kbps cap
Fanout 50, 2000kbps cap
Fanout 100, 2000kbps cap
s
p
b
k
 1400
 1200
 1000
 800
 600
 400
 200
 0
 50
 100
 150
 200
Nodes
Figure 4: Distribution of bandwidth usage among nodes
with different fanout values and upload caps.
According to Algorithm 1, all nodes contribute to the
gossip dissemination by sending their proposal messages to
the same number of nodes. However, the contribution of a
node in terms of serve messages depends on the probabil-
ity that its proposal messages are accepted by other nodes.
In general, nodes with low-latency and reliable connections
(i.e., good nodes) have higher probabilities to see their pro-
posals accepted. This is conﬁrmed by Figure 4. The plot
also shows that the heterogeneity in bandwidth utilization
increases with the amount of available bandwidth. For ex-
ample the lines for the 700 kbps bandwidth cap show an
almost homogeneous distribution apart from a small set of
bad nodes. This is because the higher latencies exhibited
by good nodes when their bandwidth utilization is close
to the limit causes other nodes to work more, thus equal-
izing bandwidth consumption. On the other hand, if we
observe the lines corresponding to a fanout of 50 in the
1000 kbps and 2000 kbps scenarios and to a fanout of 100
in the 2000 kbps scenario, we see that good nodes have
enough spare capacity to operate without saturating their
bandwidths. As a result, the contribution of nodes remains
highly heterogeneous.
4.2. Proactiveness
Next, we present our analysis of gossip proactiveness by
showing how refreshing the set of communication partners
affects application performance. Figure 5 presents the re-
sults obtained by varying the view refresh rate, X, in a sce-
nario with a 700 kbps bandwidth cap. The plot shows three
lines corresponding to stream lags of 10 s and 20 s as well as
to ofﬂine viewing. In all cases, results conﬁrm that the best
performance is obtained by varying the set of gossip part-
ners at every communication round. If on the other hand,
the set of communication partners remains constant for long
periods of time, a small set of nodes end up having the re-
sponsibility of feeding large numbers of nodes for as long as
they keep being selected early in the dissemination process.
This means that their upload rates remain constantly higher
than their allowed bandwidth limits, ultimately resulting in
high levels of congestion, huge latencies, and message loss.
to recover from the bursts generated by a constant set of
communication partners. Nonetheless, a completely static
dissemination mesh invariably yields bad performance even
for ofﬂine viewing as the load becomes then concentrated
on a very small set of nodes for the entire experiment.
Requesting nodes to update their views. A second way
to modify the proactive behavior of the considered stream-
ing protocol is for nodes to periodically request a new set
of nodes to feed them, i.e., every Y dissemination rounds.
In Figure 6 we show the results obtained with different val-
ues of Y . Results show that this technique remains inferior
to the simpler approach of choosing a view refresh rate of
X = 1, as discussed above. The additional messages used
by this approach may in fact be lost or delayed while the
node is congested, resulting in a larger Y than planned.
s
e
d
o
n
f
t
o
e
g
a
n
e
c
r
e
P
 100
 80
 60
 40
 20
 0
1
offline viewing
20s lag
10s lag
10
100
∞
Y
offline viewing
20s lag
10s lag
Figure 6: Percentage of nodes viewing the stream with at
most 1% jitter as a function of the request rate Y .
s
e
d
o
n
f
o
e
g
n
a
t
n
e
c
r
e
P
 100
 80
 60
 40
 20
 0
1
4.3. Performance in the presence of churn
Finally, we evaluate the impact of proactiveness in the
presence of churn with Y = ∞. Results are depicted in Fig-
ures 7 and 8. The experiments consist in randomly picking a
percentage of nodes and make them fail simultaneously. We
compare the baseline results (e.g., when no nodes fail) with
those with increasing percentages, from 10% to 80%, of
failing nodes. Figure 7 shows the percentage of remaining
nodes experiencing less than 1% jitter after the catastrophic
failure, for values of X of 1, 2, 20 and ∞.
Figure 7 clearly shows that a completely dynamic mesh
offers the best performance in terms of ability to withstand
churn. With 35% churn, a proactiveness of X = 1 is able to
deliver an unaffected stream to 60% of the remaining nodes,
while the percentage drops to 32% for X = 2 and a stream
lag of 20 seconds. The results obtained with large values of
10
100
∞
X
Figure 5: Percentage of nodes viewing the stream with at
most 1% jitter as a function of the refresh rate X.
In accordance with these observations, Figure 5 shows
that the slope at which the curves decrease with X is most
negative for a lag of 10s. This is because longer values of
lag allow the bandwidth throttling mechanism more time
offline viewing, X=∞
20s lag, X=∞
offline viewing, X=20
20s lag, X=20
offline viewing, X=2
20s lag, X=2
offline viewing X=1
20s lag, X=1
 100
 80
 60
 40
 20
s
e
d
o
n
f
o
e
g
a
t
n
e
c
r
e
P
 0
 0
 10
 20
 30
 40
 50
 60
 70
 80
i
l
s
w
o
d
n
w
e
t
e
p
m
o
c
f
o
e
g
a
t
n
e
c
r
e
p
e
g
a
r
e
v
A
 100
 80
 60
 40
 20
 0
 0
20s lag, X=1
20s lag, X=2
20s lag, X=20
20s lag, X=∞
 10
 20
 30
 40
 50
 60
 70
 80
Percentage of nodes failing
Percentage of nodes failing
Figure 7: Percentage of surviving nodes experiencing less
than 1% jitter for different values of X.
Figure 8: Average percentage of complete windows for sur-
viving nodes.
X and in particular when X = ∞ show very high degrees
of variability from experiment to experiment. The resulting
static or semi-static random graph may become completely
unable to disseminate the stream if failing nodes are close
to the source or it may appear extremely resilient to churn
if failing nodes are located at the edge of the network. On
average, performance is therefore in favor of a completely
dynamic graph (X = 1).
It should be noted that Figure 7 only shows how many
nodes manage to remain completely unaware of the churn
event. To characterize the extent of the performance de-
crease experienced by all surviving nodes, Figure 8 shows
the average percentage of decoded windows over the total
number of windows streamed by the source. With X = 1,
the protocol is almost unaffected by churn, and nodes cor-
rectly receive over 90% of the windows for all churn per-
centages lower than 80%. In addition, almost all the miss-
ing windows turn out to be concentrated in a time frame of
5 s to 10 s around the churn event when 20% and 80% of
nodes fail (not shown in the plot).
5. Concluding remarks
The evaluations of gossip protocols for dissemination
usually back up the associated theoretical claims: gossip
can be tuned to achieve reliable dissemination in the pres-
ence of churn. In this paper, we challenged these results, ob-
tained mostly through simulations in close-to-ideal settings,
and evaluated a gossip-based live streaming application in a
real deployment over 230 PlanetLab nodes.
Our results show that message loss and limited band-
width signiﬁcantly restrict the range of parameter values
in which gossip can successfully operate. First, the fanout
cannot be increased arbitrarily to improve reliability and
latency, but must remain small enough to prevent band-
width saturation. Second, the set of communication part-
ners should be changed frequently, at every communication
round, in order to minimize congestion and provide an ef-
fective response to churn.
References
[1] K. Birman, M. Hayden, O. Ozkasap, Z. Xiao, M. Budiu, and
Y. Minsky. Bimodal Multicast. TOCS, 17(2):41–88, 1999.
[2] T. Bonald, L. Massouli´e, F. Mathieu, D. Perino, and
A. Twigg. Epidemic Live Streaming: Optimal Performance
Trade-Offs. In SIGMETRICS, 2008.
[3] A. Demers, D. Greene, C. Hauser, W. Irish, J. Larson,
S. Shenker, H. Sturgis, D. Swinehart, and D. Terry. Epi-
demic Algorithms for Replicated Database Maintenance. In
PODC, 1987.
[4] M. Deshpande, B. Xing, I. Lazardis, B. Hore, N. Venkata-
subramanian, and S. Mehrotra. CREW: A Gossip-based
Flash-Dissemination System. In ICDCS, 2006.
[5] N. Drost, E. Ogston, R. van Nieuwpoort, and H. Bal. ARRG:
Real-World Gossiping. In HPDC, 2007.
[6] R. Karp, C. Schindelhauer, S. Shenker, and B. V¨ocking.
Randomized Rumor Spreading. In FOCS, 2000.
[7] A.-M. Kermarrec, L. Massouli´e, and A. Ganesh. Probabilis-
tic Reliable Dissemination in Large-Scale Systems. TPDS,
14(3):248–258, 2003.
[8] B. Li, Y. Qu, Y. Keung, S. Xie, C. Lin, J. Liu, and X. Zhang.
Inside the New Coolstreaming: Principles, Measurements
and Performance Implications. In INFOCOM, 2008.
[9] H. Li, A. Clement, M. Marchetti, M. Kapritsos, L. Robinson,
L. Alvisi, and M. Dahlin. FlightPath: Obedience vs. Choice
in Cooperative Services. In OSDI, 2008.
[10] L. Massouli´e, A. Twigg, C. Gkantsidis, and P. Rodriguez.
Randomized Decentralized Broadcasting Algorithms. In IN-
FOCOM, 2007.
[11] S. Sanghavi, B. Hajek, and L. Massouli´e. Gossiping with
Multiple Messages. TIT, 53(12):4640–4654, 2007.