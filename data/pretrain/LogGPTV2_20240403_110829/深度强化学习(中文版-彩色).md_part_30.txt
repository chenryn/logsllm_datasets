义为L (τ)=E[ρ (f(x)−y)]，其中
quantile τ
8
>0
ρ (α)= (4.25)
τ >:
(τ −1)α, 其他
与之类似，通过 ∂L quantile，我们能得到F(x)=1−τ，即f(x)是随机变量y的τ 分位数值。
∂f(x)
具体来说，QR-DQN考虑将N 个均匀的分位数q = 1 作为值分布。对于一个QR-DQN模
i N
型 θ : S → RN×|A|，在采样期间，Q 值的状态 s 和动作 a 是 N 个估计的平均值：Q(s,a) =
P
N i=1q iθ i(s,a)。在训练过程中，基于Q值的贪心策略在下个状态提供a∗ = argmax a′Q(s′,a′)，
并且根据公式(4.20)，分布式贝尔曼目标为Tθ = r+γθ (s′,a∗)。文献(Dabneyetal.,2018b)中
j j
130
4.8 DQN代码实例
的引理2指出下式的和可以最小化近似值分布与真实值之间的1-Wasserstein距离：
XN
E [ρ (Tθ −θ (s,a))]. (4.26)
j τˆi j i
i=1
其中τˆ = i − 1 。
i N 2N
图4.5展示了DQN、C51和QR-DQN的对比。接下来在值分布强化学习上，其参数化分布
的灵活性和鲁棒性上还有更多的工作要做。读者对这方面感兴趣的话可以从文献 (Dabney et al.,
2018a;Mavrinetal.,2019;Yangetal.,2019)中找到相关资源。
≤
O O O
图4.5 对比s和动作a下的DQN，C51和QR-DQN。其中箭头指向的是估计值。QR-DQN中分位
数的数量指定为4。DQN的结构只输出实际Q值的近似值。对于值分布强化学习，C51估
计了多个Q值，而QR-DQN提供了Q值的分位数
4.8 DQN 代码实例
本节中，我们将围绕DQN及其变体算法讨论更多训练细节。首先演示雅达利环境的设置过
程，以及如何实现一些十分有用的装饰器（Wrapper）。高效地使用装饰器能使训练更加简单和
稳定。
Gym环境相关
OpenAIGym是一个用于开发和对比强化学习算法的开源工具包。它包含了如图4.6显示的
一系列环境。它可以直接从PyPI安装，默认安装包不带有雅达利组件，需要使用雅达利扩展安装：
pip install gym[atari]
也可以直接从源安装。
git clone https://github.com/openai/gym.git
cd gym
131
第4章 深度Q网络
pip install -e .
图4.6 OpenAIGym的一些环境
可以通过以下代码建立环境实例 env：
import gym
env = gym.make(env_id)
其中 env_id是环境名称的字符串。所有可用的 env_id可以在网址（链接见读者服务）上
查到。
env实例中有以下重要的方法：
1. env.reset()重启环境并返回初始的观测数据。
2. env.render(mode) 根据所给的 mode 模式呈现环境图像。默认为 human 模式，它将呈
现当前显示画面或者终端窗口，并不返回任何内容。你可以指定 rgb_array 模式来使
env.render函数返回 numpy.ndarray对象，这些数据可用于生成视频。
3. env.step(action) 在环境中执行动作 action，并运行一个时间步。之后返回 (obser-
vation, reward, done, info)的数据元组，其中 observation为当前环境的观测数据，
reward是状态转移的奖励，done指出当前片段是否结束，info则包含一些辅助信息。
4. env.seed(seed)手动设置随机种子。该函数在复现效果时非常有用。
这里展示了一个经典游戏Breakou（t 打砖块）的例子。我们将先运行一个 BreakoutNoFrame-
skip-v4环境的实例直到本片段结束。游戏过程的一个样帧图像如图4.7所示。
import gym
env = gym.make(’BreakoutNoFrameskip-v4’)
132
4.8 DQN代码实例
o = env.reset()
while True:
env.render()
# take a random action
a = env.action_space.sample()
o, r, done, _ = env.step(a)
if done:
break
env.close() # close and clean up
图4.7 Breakout游戏的一个样帧图像。在屏幕上方有几行需要被破坏的砖块。智能体可以控制屏
幕下方的挡板，并控制角度弹射小球到想要的位置来撞毁砖块。该游戏的观测数据是形
状为(210,160,3)的RGB屏幕图像
需要注意的是，游戏id中的 NoFrameskip意味着没有跳帧和动作重复，而 v4意思是当前
为第4个版本，也是本书写稿时的最新版本。我们将在接下来的例子中使用该环境。
OpenAIGym的另一个十分有用的特性是环境装饰器。它可以对环境对象进行装饰，使训练
代码更加简洁。如下代码展示了一个用于限制每个回合片段最大长度的时间限制装饰器，这也是
雅达利游戏的一个默认装饰器。
class TimeLimit(gym.Wrapper):
def __init__(self, env, max_episode_steps=None):
super(TimeLimit, self).__init__(env)
self._max_episode_steps = max_episode_steps
self._elapsed_steps = 0
133
第4章 深度Q网络
def step(self, ac):
o, r, done, info = self.env.step(ac)
self._elapsed_steps += 1
if self._elapsed_steps >= self._max_episode_steps:
done = True
info[’TimeLimit.truncated’] = True
return o, r, done, info
def reset(self, **kwargs):
self._elapsed_steps = 0
return self.env.reset(**kwargs)
为了更加高效地训练，gym.vector.AsyncVectorEnv提供了一个用来并行运行n个环境的
矢量化装饰器的实现。所有的接口将统一收到并返回n个变量。此外，还可以实现一个带有缓存
的矢量化装饰器，其接口也接受和返回n个变量，但会在后台保持m > n个线程。这样将更为
高效地运行某些状态转移耗时较长的环境。
Gym提供一系列雅达利2600游戏的标准接口。这些游戏可以以游戏内存数据或者屏幕图像
数据作为输入，使用街机学习环境(Bellemareetal.,2013)运行。在这2600款雅达利游戏中，有
些游戏最多包含18个不同的按键组合：
1. 移动按键：空动作、上移、右移、左移、下移、右上键组合、左上键组合、右下键组合、左
下键组合。
2. 攻击按键：开火、上移开火组合、右移开火组合、左移开火组合、下移开火组合、右上开火
组合、左上开火组合、右下开火组合、左下开火组合。
此处的空动作表示什么都不做。然后开火键可能被作为开始游戏的按键。为了方便起见，我
们后续将以按键名称称呼其对应的动作。
DQN
DQN还有三个额外的训练技巧。首先，依次使用如下的装饰器可以让训练更加稳定高效。
1. NoopResetEnv 在重置游戏时，会随机地进行几步空动作，以确保初始化的状态更为随机。
默认的最大空动作数量为30。这个装饰器将有助于智能体收集更多的初始状态，提供更为
鲁棒的学习。
2. MaxAndSkipEnv重复每个动作4次，以提供更为高效的学习。为了进一步对观测数据降噪，
返回的图像帧是在最近2帧上对像素进行最大池化的结果。
3. Monitor记录原始奖励数据。我们可以在这个装饰器中实现一些有用的函数，比如速度跟
踪器。
134
4.8 DQN代码实例
4. EpisodicLifeEnv使得本条命结束的时候，相当于本片段结束。这样不用等到玩家所有命
都消耗完才能结束本片段，对价值估计很有帮助(Rodericketal.,2017)。
5. FireResetEnv 在环境重置的时候触发开火动作。很多游戏需要这个开火动作来开始游戏。
这是快速开始游戏的先验知识。
6. WarpFrame将观测数据转换为84×84的灰度图像。
7. ClipRewardEnv将奖励通过符号进行装饰，只根据奖励数据的符号输出−1、0、1三种奖励
值。这样防止任何一个单独的小批量更新而大幅改变参数，可以进一步提高稳定性。
8. FrameStack堆叠最后4帧。我们回忆一下，DQN为了捕捉运动信息，通过堆叠当前帧和前
3帧来用函数ϕ对观测数据进行预处理。FrameStack和WarpFrame实现了ϕ的功能。需要
注意的是，我们可以通过只在观测值之间存储一次公共帧来优化内存使用，这也称为延迟
帧技术（Lazy-FrameTrick）。
其次，为避免梯度爆炸，DQN(DeepMind,2015;Mnihetal.,2015)使用了对平方误差进行了裁
剪，这等同于将均方差替换成了δ =1情况下的Huber损失(Huber,1992)。Huber损失如下所示：
8
>>>:δ 1
|x|− δ 其他
2
最终，回放缓存采样了大批有放回的抽样。在能够有个稳定的开始之前，最后还需要完成一
些热启动步骤。
注意到上述所说的全部三个技巧都用于本节中所有的实验。现在我们将展示如何建立一个能
玩Breakout游戏的智能体。首先，为了实验的可复现性，我们将手动设置相关库的随机种子。
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)
接着，我们通过 tf.keras.Model创建一个Q网络：
class QFunc(tf.keras.Model):
def __init__(self, name):
super(QFunc, self).__init__(name=name)
self.conv1 = tf.keras.layers.Conv2D(
32, kernel_size=(8, 8), strides=(4, 4),
padding=’valid’, activation=’relu’)
self.conv2 = tf.keras.layers.Conv2D(
64, kernel_size=(4, 4), strides=(2, 2),
135
第4章 深度Q网络
padding=’valid’, activation=’relu’)
self.conv3 = tf.keras.layers.Conv2D(
64, kernel_size=(3, 3), strides=(1, 1),
padding=’valid’, activation=’relu’)
self.flat = tf.keras.layers.Flatten()
self.fc1 = tf.keras.layers.Dense(512, activation=’relu’)
self.fc2 = tf.keras.layers.Dense(action_dim, activation=’linear’)
def call(self, pixels, **kwargs):
# scale observation
pixels = tf.divide(tf.cast(pixels, tf.float32), tf.constant(255.0))
# extract features by convolutional layers
feature = self.flat(self.conv3(self.conv2(self.conv1(pixels))))
# calculate q-value
qvalue = self.fc2(self.fc1(feature))
return qvalue
DQN对象的定义由Q网络、目标Q网络、训练时间步数目和优化器、同步Q网络、目标Q