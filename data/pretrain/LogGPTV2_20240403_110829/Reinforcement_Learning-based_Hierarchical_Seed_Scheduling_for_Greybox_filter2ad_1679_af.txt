-
2
3
4
0.75
0.90
1.00
3
1
9
-
4
5
4
1
9
3
-
5
3
1
9
3
4
-
TABLE VII: Average solving time for the maze problem
(Listing 1).
Fuzzer
Time (sec)
AFL-FLAT AFL-HIER
383 ± 92
180 ± 36
Our approach relies little on these techniques and is orthogonal
to these work. On the other hand, there is an increasing number
of greybox fuzzers that are carefully crafted to test speciﬁc
types of programs such as OS kernels [38], [53], ﬁrmware [57],
protocol [35], smart contracts [31], deep neural networks [50].
It is promising for these fuzzers to adopt our techniques to
improve their efﬁciency.
B. Improving Coverage Metric
Angora [12] involves calling context, and MemFuzz [15]
involves memory accesses when calculating edge coverage
to explore program states more pervasively. However, they
pay little attention to the potential seed explosion problem.
CollAFL [19] improves edge coverage accuracy via ensuring
each edge has a unique hash, and utilizes various kinds of
coverage information to prioritize seeds. However, it requires
a precise analysis of the control ﬂow graph of the target
program. Wang et al. [47] differentiate edges based on their
corresponding memory operations for seed prioritization to
ﬁnd memory corruption bugs. However, it is incapable of pre-
venting a test case involving diverse memory operations from
being dropped since it still relies on edge coverage to evaluate
the quality of test cases. Greyone [18] augments edge coverage
with data ﬂow features where lightweight and accurate taint
tracking is necessary. IJON [3] designs various primitives for
annotating source code that will adapt the coverage metric to
different kinds of challenges of exploring deep state space.
However, it requires domain knowledge of the target program
and much manual work. Ankou [30] proposes a new coverage
metric that measures distances between execution paths of test
cases, and employs adaptive seed pool update to mitigate seed
explosion. By comparison, the distance we propose is between
two arguments of conditions in conditional branches, and we
address the seed explosion problem via organizing the seed
pool as a multi-level tree.
Some research focuses on ﬁnding domain-speciﬁc bugs
via specially designed coverage metrics. MemLock [48] takes
memory consumption into account when evaluating a test case
in order to trigger memory consumption bugs. SlowFuzz [34]
counts the number of instructions executed by test case as
coverage features to detect algorithm complexity bugs. Further-
more, PerfFuzz [26] records the number of times each block
is executed by a test case and considers the test case as a new
seed if it increases the execution count for any block in order
to ﬁnd hot spots. KRACE [56] develops a new coverage that
captures the exploration progress in the concurrency dimension
to ﬁnd data races in kernel ﬁle systems. Our work offers a
framework to combine these metrics with others so that they
can beneﬁt from more general metrics.
Wang et al. [45] systematically evaluate multiple coverage
metrics, revealing that there is no grand slam coverage metric
that can beat others, and it is promising to combine different
coverage metrics together through cross seeding between mul-
tiple fuzzing instances. We combine coverage metrics within
one fuzzing instance, avoiding the overhead of synchronizing
seeds as well as redundant fuzzing. FuzzFacotry [32] provides
a platform that makes combining different coverage metrics
easy and ﬂexible. However,
it does not address the seed
explosion problem, as our experimental results demonstrate
15
that randomly combining different metrics without a proper
organization may lead to negative impacts.
C. Smart Seed Scheduling
AFLFAST [9] focuses on fuzzing seeds exercising low-
frequency paths and assigns more power to them through
modeling greybox fuzzing as a Markov chain. FairFuzz [27]
identiﬁes low-frequency edges and prioritizes mutations satis-
fying these edges. Entropic [7] targets on the test cases that
a seed has generated, evaluating the diversity of coverage
features they exercise via the information-theoretic entropy.
Consequently, seeds with higher information gains are more
likely to be scheduled. Vuzzer [37] de-prioritizes seeds hitting
error-handling or frequently visited code that
is identiﬁed
via heavyweight static and dynamic analysis. Cerebo [28]
prioritizes seeds via various metrics including code complexity,
coverage, and execution time. AFLGo [8] and UAFL [44]
are directed fuzzers that favor seeds closer to targeted code.
Compared to these work, our scheduling algorithm considers
the rareness of both static features it covers and test cases it
has generated when evaluating a seed.
Modeling scheduling as an MAB problem. Woo et al. [49]
model blackbox mutational fuzzing as a classic Multi-Armed
Bandit (MAB) problem. Nevertheless, its goal is to search for
an optimal arrangement for a ﬁxed set of program-seed pairs
to maximize the unique bugs found. EcoFuzz [54] proposes
a variant of the Adversarial Multi-Armed Bandit model for
modeling seed scheduling. However, it explicitly puts seed
exploration and exploitation in separate stages, launching ex-
ploitation only when all existing seeds have been explored
once. Thus it
is incapable of solving the seed explosion
problem.
VII. CONCLUSION
Fine-grained coverage metrics, such as distances between
operands of comparison operations and array indices involved
in memory accesses, allow greybox fuzzers to detect bugs that
cannot be triggered by traditional edge coverage. However,
existing seed scheduling algorithms cannot efﬁciently handle
the increased number of seeds. In this work, we present
a new coverage metric design called multi-level coverage
metric, where we cluster seeds selected by ﬁne-grained metrics
using coarse-grained metrics. Combined with a reinforcement-
learning-based hierarchical scheduler, our approach signiﬁ-
cantly outperforms existing edge-coverage-based fuzzers on
DARPA CGC challenges.
ACKNOWLEDGMENT
This work is supported,
in part, by National Science
Foundation under Grant No. 1664315, No, 1718997, Ofﬁce
of Naval Research under Award No. N00014-17-1-2893, and
UCOP under Grant LFR-18-548175. Any opinions, ﬁndings,
and conclusions or recommendations expressed in this paper
are those of the authors and do not necessarily reﬂect the views
of the funding agencies.
REFERENCES
[1]
“Libfuzzer: a library for coverage-guided fuzz testing,” https://llvm.org/
docs/LibFuzzer.html.
[2] R. Agrawal, “Sample mean based index policies with o (log n) regret
for the multi-armed bandit problem,” Advances in Applied Probability,
pp. 1054–1078, 1995.
[3] C. Aschermann, S. Schumilo, A. Abbasi, and T. Holz, “Ijon: Exploring
deep state spaces via fuzzing,” in IEEE Symposium on Security and
Privacy (Oakland), 2020.
[4] C. Aschermann, S. Schumilo, T. Blazytko, R. Gawlik, and T. Holz,
“Redqueen: Fuzzing with input-to-state correspondence,” in Annual
Network and Distributed System Security Symposium (NDSS), 2019.
[5] P. Auer, N. Cesa-Bianchi, and P. Fischer, “Finite-time analysis of the
multiarmed bandit problem,” Machine learning, vol. 47, no. 2-3, pp.
235–256, 2002.
[6] M. Böhme and B. Falk, “Fuzzing: On the exponential cost of vulnera-
bility discovery,” in ACM SIGSOFT Symposium on the Foundations of
Software Engineering (FSE), 2020.
[7] M. Böhme, V. Manes, and S. K. Cha, “Boosting fuzzer efﬁciency: An
information theoretic perspective,” in ACM SIGSOFT Symposium on
the Foundations of Software Engineering (FSE), 2020.
[8] M. Böhme, V.-T. Pham, M.-D. Nguyen, and A. Roychoudhury, “Di-
rected greybox fuzzing,” in ACM Conference on Computer and Com-
munications Security (CCS), 2017.
[9] M. Böhme, V.-T. Pham, and A. Roychoudhury, “Coverage-based grey-
box fuzzing as markov chain,” in ACM Conference on Computer and
Communications Security (CCS), 2016.
[10] D. CGC, “Darpa cyber grand challenge binaries,” https://github.com/
CyberGrandChallenge, 2014.
[11] S. K. Cha, M. Woo, and D. Brumley, “Program-adaptive mutational
fuzzing,” in IEEE Symposium on Security and Privacy (Oakland), 2015.
[12] P. Chen and H. Chen, “Angora: Efﬁcient fuzzing by principled search,”
in IEEE Symposium on Security and Privacy (Oakland), 2018.
[13] P. Chen, J. Liu, and H. Chen, “Matryoshka: Fuzzing deeply nested
branches,” in ACM Conference on Computer and Communications
Security (CCS), 2019.
J. Choi, J. Jang, C. Han, and S. K. Cha, “Grey-box concolic testing
on binary code,” in International Conference on Software Engineering
(ICSE), 2019.
[14]
[15] N. Coppik, O. Schwahn, and N. Suri, “Memfuzz: Using memory
accesses to guide fuzzing,” in IEEE Conference on Software Testing,
Validation and Veriﬁcation (ICST), 2019.
[16] S. Embleton, S. Sparks, and R. Cunningham, “Sidewinder: An evo-
lutionary guidance system for malicious input crafting.” in BlackHat,
2006.
[17] A. Fioraldi, D. Maier, H. Eißfeldt, and M. Heuse, “Aﬂ++: Combin-
ing incremental steps of fuzzing research,” in USENIX Workshop on
Offensive Technologies (WOOT), 2020.
[18] S. Gan, C. Zhang, P. Chen, B. Zhao, X. Qin, D. Wu, and Z. Chen,
“Greyone: Data ﬂow sensitive fuzzing,” in USENIX Security Symposium
(Security), 2019.
[19] S. Gan, C. Zhang, X. Qin, X. Tu, K. Li, Z. Pei, and Z. Chen, “Collaﬂ:
Path sensitive fuzzing,” in IEEE Symposium on Security and Privacy
(Oakland), 2018.
[20] Google, “OSS-Fuzz - continuous fuzzing of open source software,”
https://github.com/google/oss-fuzz, 2016.
[21] ——, “Fuzzbench: Fuzzer benchmarking as a service,” https://google.
github.io/fuzzbench/, 2020.
[22] A. Hazimeh, A. HERRERA, and M. Payer, “Magma: A ground-
truth fuzzing benchmark,” in ACM on Measurement and Analysis of
Computing Systems (SIGMETRICS), 2021.
[23] U. Kargén and N. Shahmehri, “Turning programs against each other:
high coverage fuzz-testing using binary-code mutation and dynamic
slicing,” in Proceedings of the 2015 10th Joint Meeting on Foundations
of Software Engineering, 2015.
[24] G. T. Klees, A. Ruef, B. Cooper, S. Wei, and M. Hicks, “Evaluating
fuzz testing,” in ACM Conference on Computer and Communications
Security (CCS), 2018.
16
[25]
laﬁntel, “Circumventing fuzzing roadblocks with compiler transforma-
tions,” https://laﬁntel.wordpress.com/, 2016.
[26] C. Lemieux, R. Padhye, K. Sen, and D. Song, “Perffuzz: automati-
cally generating pathological inputs,” in International Symposium on
Software Testing and Analysis (ISSTA), 2018.
[27] C. Lemieux and K. Sen, “Fairfuzz: A targeted mutation strategy for
increasing greybox fuzz testing coverage,” in IEEE/ACM International
Conference on Automated Software Engineering (ASE), 2018.
[28] Y. Li, Y. Xue, H. Chen, X. Wu, C. Zhang, X. Xie, H. Wang, and Y. Liu,
“Cerebro: context-aware adaptive fuzzing for effective vulnerability de-
tection,” in ACM SIGSOFT Symposium on the Foundations of Software
Engineering (FSE), 2019.
[29] C. Lyu, S. Ji, C. Zhang, Y. Li, W.-H. Lee, Y. Song, and R. Beyah,
“Mopt: Optimized mutation scheduling for fuzzers,” in USENIX Secu-
rity Symposium (Security), 2019.
[30] V. J. Manès, S. Kim, and S. K. Cha, “Ankou: Guiding grey-box
fuzzing towards combinatorial difference,” in International Conference
on Software Engineering (ICSE), 2020.
[31] T. D. Nguyen, L. H. Pham, J. Sun, Y. Lin, and Q. T. Minh, “sfuzz: An
efﬁcient adaptive fuzzer for solidity smart contracts,” in International
Conference on Software Engineering (ICSE), 2020.
[32] R. Padhye, C. Lemieux, K. Sen, L. Simon, and H. Vijayakumar,
“Fuzzfactory: domain-speciﬁc fuzzing with waypoints,” in Annual ACM
Conference on Object-Oriented Programming, Systems, Languages, and
Applications (OOPSLA), 2019.
[33] H. Peng, Y. Shoshitaishvili, and M. Payer, “T-fuzz: fuzzing by program
transformation,” in IEEE Symposium on Security and Privacy (Oak-
land), 2018.
[34] T. Petsios, J. Zhao, A. D. Keromytis, and S. Jana, “Slowfuzz: Automated
domain-independent detection of algorithmic complexity vulnerabili-
ties,” in ACM Conference on Computer and Communications Security
(CCS), 2017.
[35] V.-T. Pham, M. Böhme, and A. Roychoudhury, “Aﬂnet: A greybox
fuzzer for network protocols,” in IEEE International Conference on
Software Testing, Veriﬁcation and Validation (Testing Tools Track),
2020.
[36] M. Rajpal, W. Blum, and R. Singh, “Not all bytes are equal: Neural
byte sieve for fuzzing,” arXiv preprint arXiv:1711.04596, 2017.
[37] S. Rawat, V. Jain, A. Kumar, L. Cojocar, C. Giuffrida, and H. Bos,
“Vuzzer: Application-aware evolutionary fuzzing,” in Annual Network
and Distributed System Security Symposium (NDSS), 2017.
[38] S. Schumilo, C. Aschermann, R. Gawlik, S. Schinzel, and T. Holz,
“kaﬂ: Hardware-assisted feedback fuzzing for os kernels,” in USENIX
Security Symposium (Security), 2017.
directed fuzzing tool for automatic software vulnerability detection,” in
IEEE Symposium on Security and Privacy (Oakland), 2010.
[47] Y. Wang, X. Jia, Y. Liu, K. Zeng, T. Bao, D. Wu, and P. Su, “Not all
coverage measurements are equal: Fuzzing by coverage accounting for
input prioritization,” in Annual Network and Distributed System Security
Symposium (NDSS), 2020.
[48] C. Wen, H. Wang, Y. Li, S. Qin, Y. Liu, Z. Xu, H. Chen, X. Xie, G. Pu,
and T. Liu, “Memlock: Memory usage guided fuzzing,” in International
Conference on Software Engineering (ICSE), 2020.
[49] M. Woo, S. K. Cha, S. Gottlieb, and D. Brumley, “Scheduling black-box
mutational fuzzing,” in ACM Conference on Computer and Communi-
cations Security (CCS), 2013.
[50] X. Xie, L. Ma, F. Juefei-Xu, M. Xue, H. Chen, Y. Liu, J. Zhao,
B. Li, J. Yin, and S. See, “Deephunter: A coverage-guided fuzz testing
framework for deep neural networks,” in International Symposium on
Software Testing and Analysis (ISSTA), 2019.
[51] W. Xu, S. Kashyap, C. Min, and T. Kim, “Designing new operating
primitives to improve fuzzing performance,” in ACM Conference on
Computer and Communications Security (CCS), 2017.
[52] W. You, X. Wang, S. Ma, J. Huang, X. Zhang, X. Wang, and B. Liang,
“Profuzzer: On-the-ﬂy input type probing for better zero-day vulnerabil-
ity discovery,” in IEEE Symposium on Security and Privacy (Oakland),
2019.
[53] W. You, P. Zong, K. Chen, X. Wang, X. Liao, P. Bian, and B. Liang,
“Semfuzz: Semantics-based automatic generation of proof-of-concept
exploits,” in ACM Conference on Computer and Communications
Security (CCS), 2017.
[54] T. Yue, P. Wang, Y. Tang, E. Wang, B. Yu, K. Lu, and X. Zhou,
“Ecofuzz: Adaptive energy-saving greybox fuzzing as a variant of
the adversarial multi-armed bandit,” in USENIX Security Symposium
(Security), 2020.
[55] M. Zalewski, “American fuzzy lop.(2014),” http://lcamtuf.coredump.cx/
aﬂ, 2014.
[56] M. X. S. K. H. Zhao and T. Kim, “Krace: Data race fuzzing for kernel
ﬁle systems,” in IEEE Symposium on Security and Privacy (Oakland),
2020.
[57] Y. Zheng, A. Davanian, H. Yin, C. Song, H. Zhu, and L. Sun, “Firm-aﬂ:
high-throughput greybox fuzzing of iot ﬁrmware via augmented process
emulation,” in USENIX Security Symposium (Security), 2019.
[39] K. Serebryany, “Continuous fuzzing with libfuzzer and addresssani-
tizer,” in IEEE Cybersecurity Development (SecDev).
IEEE, 2016.
[40] D. She, K. Pei, D. Epstein, J. Yang, B. Ray, and S. Jana, “Neuzz:
Efﬁcient fuzzing with neural program learning,” in IEEE Symposium
on Security and Privacy (Oakland), 2019.
[41] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta,
Y. Shoshitaishvili, C. Kruegel, and G. Vigna, “Driller: Augmenting
fuzzing through selective symbolic execution,” in Annual Network and
Distributed System Security Symposium (NDSS), 2016.
[42] L. Szekeres, “Memory corruption mitigation via software hardening and
bug-ﬁnding,” Ph.D. dissertation, Stony Brook University, 2017.
[43] A. Vargha and H. D. Delaney, “A critique and improvement of the cl
common language effect size statistics of mcgraw and wong,” Journal
of Educational and Behavioral Statistics, vol. 25, no. 2, pp. 101–132,
2000.
[44] H. Wang, X. Xie, Y. Li, C. Wen, Y. Li, Y. Liu, S. Qin, H. Chen,
and Y. Sui, “Typestate-guided fuzzer for discovering use-after-free
vulnerabilities,” in International Conference on Software Engineering
(ICSE), 2020.
J. Wang, Y. Duan, W. Song, H. Yin, and C. Song, “Be sensitive
and collaborative: Analyzing impact of coverage metrics in greybox
fuzzing,” in International Symposium on Research in Attacks, Intrusions
and Defenses (RAID), 2019.
[45]
[46] T. Wang, T. Wei, G. Gu, and W. Zou, “Taintscope: A checksum-aware
17