# VII. Average Solving Time for the Maze Problem (Listing 1)
| Fuzzer       | Time (sec)   |
|--------------|--------------|
| AFL-FLAT     | 383 ± 92     |
| AFL-HIER     | 180 ± 36     |

Our approach minimally relies on these techniques and is orthogonal to them. Conversely, there is a growing number of greybox fuzzers that are specifically designed to test particular types of programs, such as OS kernels [38], [53], firmware [57], protocols [35], smart contracts [31], and deep neural networks [50]. These fuzzers could benefit from adopting our techniques to enhance their efficiency.

## B. Improving Coverage Metrics
Angora [12] incorporates calling context, and MemFuzz [15] considers memory accesses when calculating edge coverage to explore program states more comprehensively. However, both approaches pay little attention to the potential seed explosion problem. CollAFL [19] improves edge coverage accuracy by ensuring each edge has a unique hash and uses various coverage information to prioritize seeds. This method, however, requires a precise analysis of the target program's control flow graph. Wang et al. [47] differentiate edges based on their corresponding memory operations to prioritize seeds for finding memory corruption bugs. Despite this, it cannot prevent a test case with diverse memory operations from being dropped, as it still relies on edge coverage to evaluate test case quality. Greyone [18] enhances edge coverage with data flow features, necessitating lightweight and accurate taint tracking. IJON [3] designs various primitives for annotating source code, adapting the coverage metric to different challenges in exploring deep state spaces. However, it requires domain knowledge and significant manual effort. Ankou [30] proposes a new coverage metric that measures distances between execution paths of test cases and employs adaptive seed pool updates to mitigate seed explosion. In contrast, our proposed distance metric focuses on the arguments of conditions in conditional branches, and we address the seed explosion problem by organizing the seed pool as a multi-level tree.

Some research focuses on detecting domain-specific bugs using specially designed coverage metrics. MemLock [48] evaluates test cases based on memory consumption to trigger memory consumption bugs. SlowFuzz [34] counts the number of instructions executed by a test case to detect algorithm complexity bugs. PerfFuzz [26] records the number of times each block is executed and considers a test case a new seed if it increases the execution count for any block, aiming to find hot spots. KRACE [56] develops a new coverage metric to capture exploration progress in the concurrency dimension to find data races in kernel file systems. Our work offers a framework to combine these metrics with others, enabling them to benefit from more general metrics.

Wang et al. [45] systematically evaluate multiple coverage metrics, revealing that no single metric outperforms all others. They suggest combining different coverage metrics through cross-seeding between multiple fuzzing instances. We combine coverage metrics within a single fuzzing instance, avoiding the overhead of synchronizing seeds and redundant fuzzing. FuzzFactory [32] provides a platform for easy and flexible combination of different coverage metrics. However, it does not address the seed explosion problem, as our experimental results show that randomly combining metrics without proper organization can have negative impacts.

## C. Smart Seed Scheduling
AFLFAST [9] focuses on fuzzing seeds that exercise low-frequency paths and assigns more power to them by modeling greybox fuzzing as a Markov chain. FairFuzz [27] identifies low-frequency edges and prioritizes mutations satisfying these edges. Entropic [7] targets test cases generated by a seed, evaluating the diversity of coverage features they exercise via information-theoretic entropy. Consequently, seeds with higher information gains are more likely to be scheduled. Vuzzer [37] de-prioritizes seeds hitting error-handling or frequently visited code, identified through heavyweight static and dynamic analysis. Cerebro [28] prioritizes seeds using various metrics, including code complexity, coverage, and execution time. AFLGo [8] and UAFL [44] are directed fuzzers that favor seeds closer to targeted code. Compared to these works, our scheduling algorithm considers the rarity of both static features covered and test cases generated when evaluating a seed.

Modeling scheduling as a Multi-Armed Bandit (MAB) problem, Woo et al. [49] model blackbox mutational fuzzing as a classic MAB problem. Their goal is to find an optimal arrangement for a fixed set of program-seed pairs to maximize unique bug discovery. EcoFuzz [54] proposes a variant of the Adversarial MAB model for seed scheduling but explicitly separates seed exploration and exploitation into distinct stages, launching exploitation only after all existing seeds have been explored once. This approach is unable to solve the seed explosion problem.

## VII. Conclusion
Fine-grained coverage metrics, such as distances between operands of comparison operations and array indices involved in memory accesses, allow greybox fuzzers to detect bugs that traditional edge coverage cannot. However, existing seed scheduling algorithms struggle to handle the increased number of seeds. In this work, we present a new multi-level coverage metric design, where we cluster seeds selected by fine-grained metrics using coarse-grained metrics. Combined with a reinforcement-learning-based hierarchical scheduler, our approach significantly outperforms existing edge-coverage-based fuzzers on DARPA CGC challenges.

## Acknowledgment
This work is supported, in part, by the National Science Foundation under Grant No. 1664315 and No. 1718997, the Office of Naval Research under Award No. N00014-17-1-2893, and UCOP under Grant LFR-18-548175. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.

## References
[1] “Libfuzzer: A library for coverage-guided fuzz testing,” https://llvm.org/docs/LibFuzzer.html.
[2] R. Agrawal, “Sample mean based index policies with o (log n) regret for the multi-armed bandit problem,” Advances in Applied Probability, pp. 1054–1078, 1995.
[3] C. Aschermann, S. Schumilo, A. Abbasi, and T. Holz, “IJON: Exploring deep state spaces via fuzzing,” in IEEE Symposium on Security and Privacy (Oakland), 2020.
...
[56] M. X. S. K. H. Zhao and T. Kim, “KRACE: Data race fuzzing for kernel file systems,” in IEEE Symposium on Security and Privacy (Oakland), 2020.
...
[45] J. Wang, Y. Duan, W. Song, H. Yin, and C. Song, “Be sensitive and collaborative: Analyzing impact of coverage metrics in greybox fuzzing,” in International Symposium on Research in Attacks, Intrusions and Defenses (RAID), 2019.
[46] T. Wang, T. Wei, G. Gu, and W. Zou, “TaintScope: A checksum-aware...