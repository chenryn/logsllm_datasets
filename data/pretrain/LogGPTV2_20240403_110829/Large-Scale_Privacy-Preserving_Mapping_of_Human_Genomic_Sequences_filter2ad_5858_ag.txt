aggregated into the sequence dataset, as well as a mixture
of real and fake humans, when artiﬁcial reads are added to
the dataset as noise. In either case, our objective is to de-
termine whether or not an individual (the testee) is present
in the case group from the allele frequencies we observe.
To this end, we also need a reference group, for example,
the HapMap population [9] whose allele frequency for each
SNP k, Fk, is public knowledge, and a sequence of allele
pairs6 from the testee, one pair for each of the SNP site k
whose major-allele frequency Yk can be 0 (two minor alle-
les), 0.5 (one major and one minor) or 1 (two major). Based
upon such information, we analyzed anonymized read data
6A human inherits genes from both of his/her parents and therefore has
two alleles at each SNP site. For a case individual, his/her two alleles
appear on two different reads.
using a statistic proposed by Homer, et al. [32]:
Dk = |Yk − Fk| − |Yk − fk|
(3)
Assuming that the distributions of SNPs’ allele frequen-
cies in the case and reference populations are identical, the
sum of Dk across all independent SNP k will have a nor-
mal distribution, whose mean becomes zero when the testee
is not in the case/reference groups, and signiﬁcantly larger
than zero when she is a case. By testing this statistic on the
null hypothesis: “the testee is not within the case group”,
we assessed the level of privacy protection that different
anonymization techniques are able to offer. Note that al-
though this statistic is well-known to be effective on the ag-
gregated microarray data [12, 32], the vulnerability of the
anonymized sequence data to such a re-identiﬁcation attack
has not been investigated before.
The setting of our study. To evaluate the re-identiﬁcation
risk in outsourcing anonymized read data, we performed a
series of hypothesis tests on such data under four scenar-
ios: 1) aggregation only, 2) noise-adding only, 3) aggrega-
tion and then data partition and 4) noise-adding and then
data partition. All the genomic sequences used in our study
were randomly sampled from the reference genome: when
a sampled read covered a SNP site, its allele was randomly
picked according to the major allele frequency of the site in
the YRI population, as provided by the HapMap [9]. In this
way, we acquired the realistic sequencing reads from a large
group of simulated people. Our research utilized published
3,138,397 SNP sites of the YRI population in the HapMap
dataset. We consider an anonymized dataset with 100-bp
reads to be not secure if a sequence donor for the dataset
has a probability of at least 0.1 to be identiﬁed with a con-
ﬁdence level no less than 0.99 (i.e. a false positive rate no
more than 0.01). Our analysis aims at determining the nec-
essary condition, e.g, the minimum number of the personal
datasets (the dataset of an individual’s reads) needed to be
aggregated or the noise reads needed to be added, to protect
the person from being re-identiﬁed through her N SNPs in
the dataset.
Our ﬁndings. The outcomes of our evaluation study are
presented in Figure 5. In the leftmost panel of the ﬁgure,
024681012x 10412345x 104AggregationNumber of datasets needed for aggregationNumer of SNPs in the dataset1)00.511.522.53x 10550100150200250300350Number of SNPs in the datasetMillions of reads (noise)need to be addedNoise Adding2)01234x 10420406080100Number of datasets needed for aggregationNumber of partitionsfor each datasetPartition with Aggregation3)05010015020406080100Millions of reads (noise) needed for each partitionNumber of partitionsfor each datasetPartition with Noise Adding4)we show the cost for the aggregation strategy. The sim-
ple test statistic in Equation 3 was found to be able to pose
an alarming level of threat to the DNA donors, exactly as
it does to the GWAS participants through their aggregated
allele frequencies derived from microarray data: as illus-
trated in the panel, to cover the identity information dis-
closed by the N SNPs from each donor (x-axis), a large
number of personal datasets (each with N SNPs) have to be
aggregated (y-axis). As an example, consider a human mi-
crobiome sequencing dataset that contains 10 million reads
with 3% of human contamination. These human reads cover
about 100,000 SNPs, and therefore, according to the ﬁgure,
need an aggregation of at least 38000 personal datasets of
the same size to secure, which. This amount of data cannot
be afforded by even the largest microbiome project.
Noise adding is another way to reduce the privacy
risks in outsourcing read data.
In our study, we gener-
ated noise reads covering major/minor alleles at randomly-
chosen SNP sites and evaluated the re-identiﬁcation power
achievable over the personal dataset including these reads.
The middle-left panel in Figure 5 shows the minimum num-
ber of noise reads (y-axis) that are required to secure the
dataset with N SNPs from a donor (x-axis). Our study
shows that the number of the required noise reads grows
linearly with regards to N. For example, at least 140 mil-
lion noise reads need to be added in order to secure a human
microbiome sequencing dataset with 10 million reads cov-
ering about 100,000 SNPs.
We further studied the strategies that partition the
datasets after they were anonymized through aggregation
or noise adding. All our analyses were performed on the
personal dataset that contained 10 million reads and cov-
ered about 100,000 human SNP sites, a large case in human
microbiome sequencing. The middle-right panel in Fig-
ure 5 shows the number of partitions needed (y-axis) to se-
cure a dataset aggregated over different numbers of personal
datasets (x-axis), and the rightmost panel demonstrates the
number of required partitions (y-axis) vs.
the number of
noise reads being added (x-axis). As illustrated by the ﬁg-
ure, when it is possible to partition an aggregated dataset
into 100 subsets for the public cloud to process indepen-
dently, the dataset should be built from at least 500 personal
datasets, or carry at least 100 million noise reads (1 million
noise reads per subset) to stay safe, which are better than
aggregation or noise adding alone, though the overheads
are still signiﬁcant (particularly when it comes to other read
datasets with higher levels of SNPs). Moreover, data parti-
tion could bring in large communication overheads, because
each subset needs to be transferred to the public cloud sep-
arately. It is also less clear how to prevent the public cloud
from linking different subsets together (e.g. based on the
those who submit the jobs): when this happens, the cloud
can aggregate the subsets for the re-identiﬁcation.
7.2
Identiﬁcation Attacks on Combined Seeds
We demonstrate that the l-mer based near-optimal statis-
tics have no power at all, which indicates that the frequency
analysis on the keyed-hash values of l-mers does not of-
fer sufﬁcient information for a re-identiﬁcation attack. Be-
low, we show that the Homer-like statistic cannot achieve a
higher power on the combined seeds than on the continuous
seeds. The similar analysis can be applied to the log like-
lihood ratio test. Consider a continuous seed (24 bps) con-
sisting of two consecutive 12-mers, one of which contains a
SNP site (denoted by α1). Because most of the 100-bp win-
dows in the human genome contain at most one SNP site,
totally there are 100 − 12 = 88 combined seeds within the
same 100-bp window that contain the SNP site. All of them
include α1, thus can be denoted as α1||αi (i = 2, ..., 89),
and one of them is the continuous seed (denoted as α1||α2).
Let Bk(i) (i = 2, ..., 89) be the bins for combined seeds
α1||αi when α1 carries a major allele, and Bk(cid:48)(i) be the
bins for α1||αi when α1 carries a minor allele. Because all
these seeds involve the same SNP site on the 12-mer α1, all
the related seeds from the testee’s genome must all carry the
same allele, major or minor. Then, the numbers of l-mers
in the bins Bk(i) and Bk(cid:48)(i) ( ¯fk(i) and ¯fk(cid:48)(i), respectively)
are equally deviated from their expected counts ( ¯Fk(i) and
¯Fk(cid:48)(i), respectively) as compared to the numbers of l-mers
in the bins of the continuous seeds ( ¯fk(2) and ¯fk(cid:48)(2)) devi-
ated from their expected counts ( ¯Fk(2) and ¯Fk(cid:48)(2)). Hence,
i(|ρk(i) − ¯Fk(i)| + |ρk(cid:48)(i) − ¯Fk(cid:48)(i)|) ≈ 88 ×
i(|ρk(i)− ¯fk(i)|+
|ρk(cid:48)(i) − ¯fk(cid:48)(i)|) ≈ 88× (|ρk(2) − ¯fk(2)| +|ρk(cid:48)(2) − ¯fk(cid:48)(2)|).
Therefore, the Homer-like statistic on the bins of all com-
bined seeds will become: ¯Tcom ≈ 88 × ¯T where ¯T is the
Homer-like statistic on the bins of continuous 24-bp seeds
(as deﬁned in Equation 1). This implies that, no matter
the testee is a case individual or not, the test statistic on
all combined 12-bp seeds ( ¯Tcom) is a constant (88) times
larger than the test statistic on the continuous 24-bp seeds
¯T . As a result, at the same conﬁdence level, ¯Tcom cannot
achieve higher power than ¯T . Since we have shown ¯T has
little re-identiﬁcation power, ¯Tcom should not either.
we have (cid:80)
(|ρk(2)− ¯Fk(2)|+|ρk(cid:48)(2)− ¯Fk(cid:48)(2)|), and(cid:80)
7.3 Correlation Analysis
Here we show that identiﬁcation of correlated hash val-
ues is very hard in practice7 and attempts to do so can be
easily defeated. This challenge (to the adversary) comes
from the fact that she only observes the hash values for a
small portion of a donor’s genome: by far the largest dataset
from an individual contains no more than 10 million reads
7Remember that each hash value in a bin cannot be uniquely linked to
an l-mer and therefore, the correlations among hashes cannot be identiﬁed
through l-mers.
(of 100 bps long each), only γ = 1/150 of all the 24-mers
on her genome. To understand what the adversary can do,
let us ﬁrst consider the simplest case, when two 24-mers are
completely correlated. This happens when they contain al-
leles of the same SNP: if both share the same allele, they
are positively correlated; otherwise, if one contains the ma-
jor allele and the other contains the minor allele, they are
negatively correlated. To detect such correlations, the ad-
versary needs to conduct a co-occurrence statistical test to
ﬁnd out whether the hash values of these 24-mers always
appear or disappear together in different donors’ seed-hash
datasets. To defeat this attack, we can aggregate the reads
from 20 individuals (sampled from 40 DNA sequences, 2
from each) in one read-mapping task. After the aggrega-
tion, in a single aggregated dataset, no SNP site likely con-
tains only minor alleles: the probability for this to happen
is below 2−40 ≈ 10−12, since the minor allele frequency
is below 0.5.
In other words, l-mers with major alleles
are always carried by some case individuals, regardless of
the presence or absence of related l-mers in the seed-hash
dataset the adversary sees. As a result, the test will fail on
the pairs of 24-mers that both contain major alleles or con-
tain one major and one minor allele. For the pair of 24-
mers both containing a minor allele, the adversary can con-
ﬁdently conjecture they are correlated only if she observes
both 24-mers in multiple aggregated datasets, because the
probability of observing a pair of minor-allele-containing
24-mers in a speciﬁc dataset by chance is not small. Assum-
ing there are N SNP sites in the human genome (N > 106),
the random probability of observing a pair of minor-allele-
containing 24-mers in n aggregated datasets is approxi-
mately P ∼ N 2 × l2 × (γ2 × t)n (where t is the mi-
nor allele frequency and t < 0.5), indicating it requires
n ≥ 4 for P (cid:28) 1. On the other hand, the probability of
observing a highly correlated pair of minor-containing 24-
mers in 4 aggregated datasets is very small, unless there are
many aggregated datasets to be analyzed. For example, if
M = 500 aggregated datasets (i.e., 20 × 500 = 10, 000
human genome reads datasets) are analyzed, the probability
of observing 4 aggregated datasets that contain both minor-
allele-containing 24-mers can be estimated by a binomial
distribution: P < 5004 × (γ2/2)4 ≈ 10−8 (cid:28) 1. Even
considering there are many potentially correlated SNP pairs
(e.g., close to the total number of SNPs, N ≈ 107), the ex-
pected number of conﬁdently assigned l-mer pairs is very
small. Therefore, the test will also fail on the pairs of 24-
mers containing the same minor allele.
Alternatively, the adversary may attempt to correlate two
hashed 24-mers (which can be only partially correlated) at
two speciﬁc genetic locations through their relative frequen-
cies across multiple samples.
In other words, she wants
to know whether the frequency of one of these two hashes
changes with that of the other over different samples. Here
a sample we refer to includes one individual’s read data that
indeed contains the substrings at the locations of these two
24-mers, and the frequency of the 24-mer is calculated over
multiple such samples. What we want to understand here is
whether the adversary can acquire enough samples to estab-
lish a reliable correlation between the 24-mers. Remember
that the probability a read dataset includes the substring at a
speciﬁc location is only γ = 1/150. The adversary needs a
set of 10 such samples to calculate the frequency of the 24-
mer (at the location) at the precision of one decimal digit
and multiple such sets to correlate two 24-mers. Due to the
presence of 2% sequencing errors, the best correlation co-
efﬁcient the adversary can get between a pair of completely
correlated 24-mers (at two speciﬁc locations) is within ±0.5
because 40% of the instances of the 24-mers contain an er-
ror at 2% error rate per nucleotide, and therefore their hash
values will become incorrect and their presences will not be
observed in the seed-hash datasets. As a result, the adver-
sary has to carry out a correlation test on at least 15 sets
(i.e. 150 samples) to obtain a conﬁdence level (P -value)
of 0.05 based on the table of critical values for Pearson’s
correlation coefﬁcient (other statistics yield similar results).
Note that the conﬁdence level becomes even lower when
those 24-mers are not completely correlated. Therefore, as-
sume the adversary has collected M samples, the probabil-
ity for a pair of completely correlated 24-mers to be both
observed in at least 150 (out of M) samples can be esti-
mated by a normal approximation of the binomial distribu-
tion with µ = σ2 = γ × M. When M = 10, 000 (that is,
10, 000 read datasets), the probability of getting those 150
samples from those datasets is negligible (≈ 10−28) even
when we consider that a total of 24× 14× 106 = 3.4× 108
24-mers are subject to this correlation analysis (there are
14 million SNP sites in the human genome, each associ-
ated with 24 24-mers). For the 24-mers not completely cor-
related, for example, those containing different SNPs, the
probability to get 150 samples in 10,000 datasets is even
lower, because the chance to have both 24-mers in one sam-
ple becomes (0.6γ)2. Therefore, even when we consider
the 2-combinations of all 3.4 × 108 24-mers, the probabil-
ity to correlate any two of them using 10,000 read datasets
is well below 10−12. This is the best chance that the ad-
versary can correlate a single pair of 24-mers (which is not
enough for a re-identiﬁcation). If such a risk is acceptable
to the data owner, what she can do is re-hashing the refer-
ence genome with a new secret key every 10,000 datasets.
The cost for this update is small: SHA-1 took about 5,440
minutes of CPU time to hash the whole genome on a 8-core
desktop (2.93 GHz Interl Xeon) used in our study and pro-
duced about 6.8 TB data; the average overheads are merely
40 seconds CPU time and 700 MB data transfer for each of
the 10,000 datasets.