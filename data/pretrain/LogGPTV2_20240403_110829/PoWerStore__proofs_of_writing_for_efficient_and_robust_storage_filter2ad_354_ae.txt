yet require no trusted components whatsoever. Systems
guaranteeing forking semantics guarantee that after a sin-
gle atomicity violation by the service, the views seen by two
inconsistent clients can never again converge. PoWerStore
avoids the drawbacks of fork-consistent systems (reﬂected
in, e.g., diﬃculties in understanding forking semantics and
exploiting them in practice [39]), by oﬀering easily under-
standable, fully linearizable, (i.e., atomic) semantics.
8. CONCLUSION
In this paper, we presented PoWerStore, an eﬃcient ro-
bust storage protocol that achieves optimal latency, mea-
sured in maximum (worst-case) number of communication
rounds between a client and storage servers. We also sepa-
rately presented a multi-writer variant of our protocol called
M-PoWerStore. The eﬃciency of our proposals stems from
combining lightweight cryptography, erasure coding and meta-
data writebacks, where readers write-back only metadata to
achieve linearizability. While robust BFTs have been of-
ten criticized for being prohibitively ineﬃcient, our ﬁndings
suggest that eﬃcient and robust BFTs can be realized in
practice by relying on lightweight cryptographic primitives
without compromising worst-case performance.
At the heart of both PoWerStore and M-PoWerStore pro-
tocols are Proofs of Writing (PoW): a novel storage tech-
nique inspired by commitment schemes in the ﬂavor of [21],
that enables single-writer PoWerStore to write and read
in 2 rounds which we show optimal. Similarly, by rely-
ing on PoW, multi-writer M-PoWerStore features 3-round
writes/reads where the third read round is only invoked un-
der active attacks. Finally, we demonstrated M-PoWerStore’s
superior performance compared to existing crash and Byzantine-
tolerant atomic storage implementations.
9. ACKNOWLEDGEMENTS
This work is supported in part by the EU CLOUDSPACES
(FP7-317555) and SECCRIT (FP7-312758) projects and by
LOEWE TUD CASED.
10. REFERENCES
[1] Jerasure. https://github.com/tsuraan/Jerasure,
2008.
[2] The Neko Project. http://ddsg.jaist.ac.jp/neko/,
2009.
[3] Ittai Abraham, Gregory Chockler, Idit Keidar, and
Dahlia Malkhi. Byzantine Disk Paxos: Optimal
Resilience with Byzantine Shared Memory. Distributed
Computing, 18(5):387–408, 2006.
[4] Amitanand S. Aiyer, Lorenzo Alvisi, and Rida A.
Bazzi. Bounded Wait-free Implementation of
Optimally Resilient Byzantine Storage Without
(Unproven) Cryptographic Assumptions. In
Proceedings of DISC, 2007.
[5] Hagit Attiya, Amotz Bar-Noy, and Danny Dolev.
Sharing Memory Robustly in Message-Passing
Systems. J. ACM, 42:124–142, January 1995.
[6] Rida A. Bazzi and Yin Ding. Non-skipping
Timestamps for Byzantine Data Storage Systems. In
Proceedings of DISC, pages 405–419, 2004.
[7] Alysson Neves Bessani, Miguel P. Correia, Bruno
Quaresma, Fernando Andr´e, and Paulo Sousa. Depsky:
dependable and secure storage in a cloud-of-clouds. In
Proceedings of EuroSys, pages 31–46, 2011.
[8] Kevin D. Bowers, Ari Juels, and Alina Oprea. Hail: a
high-availability and integrity layer for cloud storage.
In CCS, pages 187–198, 2009.
[9] Kevin D. Bowers, Ari Juels, and Alina Oprea. Proofs
of retrievability: theory and implementation. In
CCSW, pages 43–54, 2009.
[10] Christian Cachin and Stefano Tessaro. Optimal
Resilience for Erasure-Coded Byzantine Distributed
Storage. In Proceedings of DSN, pages 115–124, 2006.
[11] Brian Cho and Marcos K. Aguilera. Surviving
congestion in geo-distributed storage systems. In
Proceedings of USENIX ATC, pages 40–40, 2012.
[12] Gregory Chockler, Dahlia Malkhi, and Danny Dolev.
Future directions in distributed computing. chapter A
data-centric approach for scalable state machine
replication, pages 159–163. 2003.
[13] Allen Clement, Edmund L. Wong, Lorenzo Alvisi,
Michael Dahlin, and Mirco Marchetti. Making
byzantine fault tolerant systems tolerate byzantine
faults. In Proceedings of NSDI, pages 153–168, 2009.
[14] Wei Dai. Crypto++ 5.6.0 benchmarks. Website, 2009.
Available online at
http://www.cryptopp.com/benchmarks.html.
[15] Dan Dobre, Rachid Guerraoui, Matthias Majuntke,
Neeraj Suri, and Marko Vukoli´c. The Complexity of
Robust Atomic Storage. In Proceedings of PODC,
pages 59–68, 2011.
[16] Partha Dutta, Rachid Guerraoui, Ron R. Levy, and
Marko Vukoli´c. Fast Access to Distributed Atomic
Memory. SIAM J. Comput., 39:3752–3783, December
2010.
[17] Rui Fan and Nancy Lynch. Eﬃcient Replication of
Large Data Objects. In Proceedings of DISC, pages
75–91, 2003.
[34] Dahlia Malkhi and Michael K. Reiter. Secure and
Scalable Replication in Phalanx. In Proceedings of
SRDS, pages 51–58, 1998.
[35] Jean-Philippe Martin, Lorenzo Alvisi, and Michael
Dahlin. Minimal Byzantine Storage. In Proceedings of
DISC, pages 311–325, 2002.
[36] David Mazi`eres and Dennis Shasha. Building secure
ﬁle systems out of byantine storage. In PODC, pages
108–117, 2002.
[18] Chryssis Georgiou, Nicolas C. Nicolaou, and
[37] NetEm. NetEm, the Linux Foundation. Website, 2009.
Alexander A. Shvartsman. Fault-tolerant Semifast
Implementations of Atomic Read/Write Registers. J.
Parallel Distrib. Comput., 69(1):62–79, January 2009.
[19] Garth R. Goodson, Jay J. Wylie, Gregory R. Ganger,
and Michael K. Reiter. Eﬃcient Byzantine-Tolerant
Erasure-Coded Storage. In Proceedings of DSN, 2004.
[20] Rachid Guerraoui and Marko Vukoli´c. Reﬁned quorum
systems. Distributed Computing, 23(1):1–42, 2010.
[21] Shai Halevi and Silvio Micali. Practical and
provably-secure commitment schemes from
collision-free hashing. In Proceedings of CRYPTO,
pages 201–215, 1996.
[22] James Hendricks, Gregory R. Ganger, and Michael K.
Reiter. Low-overhead Byzantine fault-tolerant storage.
In Proceedings of SOSP, pages 73–86, 2007.
[23] Maurice Herlihy. Wait-Free Synchronization. ACM
Trans. Program. Lang. Syst., 13(1), 1991.
[24] Maurice P. Herlihy and Jeannette M. Wing.
Linearizability: A Correctness Condition for
Concurrent Objects. ACM Trans. Program. Lang.
Syst., 12(3), 1990.
[25] Prasad Jayanti, Tushar Deepak Chandra, and Sam
Toueg. Fault-tolerant Wait-free Shared Objects. J.
ACM, 45(3), 1998.
[26] Aniket Kate, Gregory M. Zaverucha, and Ian
Goldberg. Constant-size commitments to polynomials
and their applications. In Proceedings of
ASIACRYPT, volume 6477, pages 177–194, 2010.
[27] Petr Kuznetsov and Rodrigo Rodrigues. Bftw3: Why?
When? Where? workshop on the theory and practice
of Byzantine fault tolerance. SIGACT News,
40(4):82–86, 2009.
[28] Leslie Lamport. On Interprocess Communication.
Distributed Computing, 1(2):77–101, 1986.
[29] Leslie Lamport, Robert E. Shostak, and Marshall C.
Pease. The byzantine generals problem. ACM Trans.
Program. Lang. Syst., 4(3):382–401, 1982.
[30] Harry C. Li, Allen Clement, Amitanand S. Aiyer, and
Lorenzo Alvisi. The Paxos Register. In Proceedings of
SRDS, pages 114–126, 2007.
[31] Barbara Liskov and Rodrigo Rodrigues. Tolerating
Byzantine Faulty Clients in a Quorum System. In
Proceedings of ICDCS, 2006.
[32] Nancy A. Lynch and Mark R. Tuttle. An introduction
to input/output automata. CWI Quarterly, 2:219–246,
1989.
[33] Dahlia Malkhi and Michael K. Reiter. A
High-Throughput Secure Reliable Multicast Protocol.
J. Comput. Secur., 5(2):113–127, March 1997.
Available online at http://www.linuxfoundation.
org/collaborate/workgroups/networking/netem.
[38] Michael K. Reiter. Secure Agreement Protocols:
Reliable and Atomic Group Multicast in Rampart. In
Proceedings of CCS, pages 68–80, 1994.
[39] Alexander Shraer, Christian Cachin, Asaf Cidon, Idit
Keidar, Yan Michalevsky, and Dani Shaket. Venus:
veriﬁcation for untrusted cloud storage. In CCSW,
pages 19–30, 2010.
[40] Alexander Shraer, Jean-Philippe Martin, Dahlia
Malkhi, and Idit Keidar. Data-centric reconﬁguration
with network-attached disks. In Proceedings of LADIS,
pages 22–26, 2010.
[41] Atul Singh, Tathagata Das, Petros Maniatis, Peter
Druschel, and Timothy Roscoe. Bft protocols under
ﬁre. In Proceedings of NSDI, pages 189–204, 2008.
[42] Emil Stefanov, Marten van Dijk, Ari Juels, and Alina
Oprea. Iris: a scalable cloud ﬁle system with eﬃcient
integrity checks. In ACSAC, pages 229–238, 2012.
[43] Sue-Hwey Wu, Scott A. Smolka, and Eugene W.
Stark. Composition and behaviors of probabilistic i/o
automata. In Proceedings of CONCUR, pages
513–528, 1994.
APPENDIX
A. OPTIMALITY OF PoWerStore
In this section, we prove that PoWerStore features optimal
latency, by showing that writing in two rounds is necessary
and we refer the reader to [16] for the necessity of reading in
two rounds. We start by giving some informal deﬁnitions.
A distributed algorithm A is a set of automata [32], where
automaton Ap is assigned to process p. Computation pro-
ceeds in steps of A and a run is an inﬁnite sequence of steps
of A. A partial run is a ﬁnite preﬁx of some run. We say
that a (partial) run r extends some partial run pr if pr is
a preﬁx of r. We say that an implementation is selﬁsh,
if clients write-back metadata to achieve linearizability (in-
stead of the full value) [17]. Furthermore, we say that an
operation is fast if it completes in a single round.
Theorem A.1. There is no fast WRITE implementa-
tion I of a multi-reader selﬁsh robust storage that makes
use of less than 4t + 1 servers.
Preliminaries. We prove Theorem A.1 by contradiction
assuming at most 4t servers. An illustration of the proof
is given in Figure 3. We partition the set of servers into
four distinct subsets (we call blocks), denoted by T1, T2, T3
each of size exactly t, and T4 of size at least 1 and at most
t. Without loss of generality we assume that each block
contains at least one server. We say that an operation op
skips a block Ti, (1 ≤ i ≤ 4) when all messages by op to
• Let run2 be the partial run similar to run(cid:48)
1, in which
all servers except T2 are correct, but due to asynchrony,
all messages from w to T4 are delayed. Like in run(cid:48)
1, wr
completes by writing v to all servers except T4, which
it skips. To see why, note that wr cannot distinguish
run2 from run(cid:48)
1. After wr completes, T2 fails Byzantine
by reverting its memory to the initial state.
• Let run3 extend run1 by appending a complete READ
rd1 invoked by r1. By our assumption, I is wait-free.
As such, rd1 completes by skipping T1 (because T1
crashed) and returns (after a ﬁnite number of rounds)
a value vR.
• Let run4 extend run2 by appending rd1. In run4, all
servers except T2 are correct, but due to asynchrony all
messages from r1 to T1 are delayed indeﬁnitely. More-
over, since T2 reverted its memory to the initial state,
v is held only by T3. Note that r1 cannot distinguish
run4 from run3 in which T1 has crashed. As such, rd1
completes by skipping T1 and returns vR. By lineariz-
ability, vR = v.
• Let run5 be similar to run3 in which all servers except
T3 are correct but, due to asynchrony, all messages from
r1 to T1 are delayed. Note that r1 cannot distinguish
run5 from run3. As such, rd1 returns vR in run5, and
by run4, vR = v. After rd1 completes, T3 fails by
crashing.
• Let run6 extend run5 by appending a READ rd2 in-
voked by r2 that completes by returning v(cid:48). Note that
in run5, (i) T3 is the only server to which v was writ-
ten, (ii) rd1 did not write-back v (to any other server)
before returning v, and (iii) T3 crashed before rd2 is
invoked. As such, rd2 does not ﬁnd v in any server and
hence v(cid:48) (cid:54)= v, violating linearizability.
It is important to note that Theorem A.1 allows for self-
verifying data and assumes clients that may fail only by
crashing. Furthermore, the impossibility extends to crash-
tolerant storage using less than 3t + 1 servers when deleting
the Byzantine block T2 in the above proof.
Ti are delayed indeﬁnitely (due to asynchrony) and all other
blocks Tj receive all messages by op and reply.
wr(v)
wr(v)
T1
T2
T3
T4
T1
T2
T3
T4
T1
T2
T3
T4
T1
T2 @
T3
T4
(a) run1
(b) run2
wr(v) rd1 → vR
wr(v) rd1 → v
T1
T2 @
T3
T4
...
...
...
...
...
...
(c) run3
(d) run4
wr(v) rd1 → v
wr(v) rd1 → v
rd2 → v(cid:48)
T1
T2
T3
T4
...
...
...
...
...
...
...
...
...
(e) run5
(f) run6
Figure 3: Sketch of the runs used in the proof of
Theorem A.1.
Proof: We construct a series of runs of a linearizable im-
plementation I towards a partial run that violates lineariz-
ability, i.e., that features two consecutive read operations
by distinct readers that return diﬀerent values.
• Let run1 be the partial run in which all servers are
correct except T1 which crashed at the beginning of
run1. Let wr be the operation invoked by the writer
w to write a value v (cid:54)= ⊥ in the storage. The WRITE
wr is the only operation invoked in run1 and w crashes
after writing v to T3. Hence, wr skips blocks T1, T2
and T4.
• Let run(cid:48)
1 be the partial run in which all servers are
correct except T4, which crashed at the beginning of
run(cid:48)
1, w is correct and wr completes by writing
v to all blocks except T4, which it skips.
1. In run(cid:48)