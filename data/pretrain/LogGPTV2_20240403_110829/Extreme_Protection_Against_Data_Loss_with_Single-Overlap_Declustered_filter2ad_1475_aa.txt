# Extreme Protection Against Data Loss with Single-Overlap Declustered Parity

**Authors:**
- Huan Ke and Haryadi S. Gunawi, University of Chicago, Chicago, USA
- David Bonnie, Nathan DeBardeleben, Michael Grosskopf, Terry Grov'e, Dominic Manno, Elisabeth Moore, Brad Settlemyer, Los Alamos National Laboratory, Los Alamos, NM, USA

**Conference:**
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)

## Abstract
Massive storage systems, comprising tens of thousands of disks, are increasingly common in high-performance computing (HPC) data centers. With such a large number of components, the probability of correlated failures across multiple components becomes a critical concern in preventing data loss. In this paper, we re-evaluate the efficiency of traditional declustered parity data protection schemes in the presence of correlated failures. To better protect against these failures, we introduce Single-Overlap Declustered Parity (SODP), a novel declustered parity design that tolerates more disk failures than traditional methods. We also present CoFaCTOR, a tool for exploring operational reliability in the presence of various types of correlated failures. By using real failure traces from LANL’s data center, CoFaCTOR creates a failure model that accurately describes the existing file system's failure patterns and can generate failure data for hypothetical system designs. Our evaluation using CoFaCTOR traces shows that SODP-based placement algorithms can achieve a 30x improvement in the probability of data loss during failure bursts and provide similar data protection with only half the parity overhead.

## 1. Introduction
Massive storage systems, such as those used for cloud archival services and HPC data centers, provide critical data services and are expected to never lose data. However, denser and larger disk drives increase the risk of catastrophic failures and data loss. These systems rely heavily on RAID technology with declustered parity to provide fault tolerance, but traditional declustered parity schemes were not designed to handle large numbers of failures in short time windows.

Figure 1(a) illustrates the growth in the number of drives used at Backblaze (a cloud service providing data archiving) and the parallel file systems deployed at LANL, ORNL, and LLNL HPC data centers. Figure 1(b) shows that as the number of drives within a parity-protected enclosure increases, the likelihood of drive failures increases linearly, while improvements in rebuild time decrease due to the fixed amount of data to be rebuilt for a single disk failure.

With modern disk enclosures incorporating 84 or 106 drives, it is imperative to develop new methods for preventing data loss. Additionally, recent studies have shown that drive failures are not independent and identically distributed, and correlated failure bursts are a significant concern [10]–[12]. Data from LANL’s file system attached to the Trinity supercomputer [13] indicates that correlated failures are common.

Based on the emergence of extremely large disk enclosures and the need to tolerate bursts of correlated failures, we developed a new data protection scheme called Single-Overlap Declustered Parity (SODP). SODP focuses on maximizing the number of disk failures tolerated while minimizing disk rebuild time. We provide algorithms for creating SODP data placements for varying numbers of data blocks, parity blocks, and disks.

To evaluate SODP, we developed CoFaCTOR, a tool that generates large volumes of realistic failure traces using regression statistics. This tool enables the generation of traces that alter physical parameters, such as the number of disks per enclosure, to test alternative designs. By generating thousands of realistic failure traces, we use our event-driven simulation package, SOL-Sim, to model the failure, rebuild, and replacement of drives within large-scale storage systems. Our analysis shows that SODP-based data placement algorithms can dramatically reduce the probability of data loss in the face of correlated failures.

The remainder of this paper is structured as follows:
- **Section II**: Reviews existing declustered parity approaches and future directions.
- **Section III**: Describes Single-Overlap Declustered Parity, including the algorithms for generating this data placement scheme and an analysis of its improved data protection.
- **Section IV**: Introduces the SOL-Sim design.
- **Section V**: Describes our methodology for evaluating SODP using a combination of real data, realistic traces, and simulation, and compares its performance with current state-of-the-art parity schemes.
- **Section VI**: Presents the conclusions of our study.

## 2. Related Work
In this section, we review existing declustered parity approaches and discuss future directions for declustered parity data layouts.

### 2.1. Existing Approaches
- **Muntz and Lui [23]**: Introduced and analytically modeled declustered parity but left data placement decisions as an open problem.
- **Holland and Gibson [14]**: Implemented parity declustering based on Balanced Incomplete Block Designs (BIBD) and identified six criteria for ideal declustered layouts:
  - **Single Failure Correcting**: No two units of the same stripe are mapped to the same disk.
  - **Distributed Reconstruction**: Reconstruction workload is evenly distributed across surviving disks.
  - **Distributed Parity**: All disks have the same number of parity units.
  - **Efficient Mapping**: The mapping from client data to disk is implementable with low time and space requirements.
  - **Large Write Optimization**: Each parity stripe is aligned across disks to allow writing without pre-reading prior contents.
  - **Maximal Parallelism**: A read of n continuous data units induces parallel access from n disks.

- **DATUM [15]**: Improved data mapping by directly computing disks and offsets without using BIBD table lookup.
- **GridRAID [16]**: Used on LANL’s Trinity file system, divides stripe data into tiles and permutes data, parity, and spare space.
- **PRIME [17]**: Designed for prime values of v, slightly relaxing the maximal parallelism property.
- **REPLR [17]**: Similar to PRIME but deviates in maximal parallelism and distributed reconstruction, applicable to arbitrary disk array sizes.
- **PDDL and dRAID [18]**: Declusters layout by permuting disks to spread parity, spare, and client data units.
- **RAID+ [20]**: Uses Latin squares to construct a declustered layout, applicable when v is a power of a prime number.
- **D3 [22]**: Focuses on data distribution in large-scale distributed storage systems, using orthogonal arrays to spread data and parity units.
- **OI-RAID [21]**: Uses BIBD in the outer layer to achieve a balanced data layout and group fault tolerance.

### 2.2. Comparison of Schemes
Table I compares the above schemes against the six original declustered layout criteria. Most existing works violate some properties of the ideal data layout to some extent. Many modern software-based declustered parity schemes use approximately balanced designs. While many of these schemes have been extended to tolerate m failures, our SODP scheme is explicitly designed to tolerate greater than m failures.

| Scheme | Single Failure Correcting | Distributed Reconstruction | Distributed Parity | Efficient Mapping | Large Write Optimization | Maximal Parallelism | Configuration Limitation | Fault Tolerance |
|--------|--------------------------|----------------------------|--------------------|-------------------|--------------------------|---------------------|-------------------------|-----------------|
| BIBD [14] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | 1 |
| DATUM [15] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | m |
| GridRAID [16] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | m |
| PRIME [17] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | m |
| REPLR [17] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | m |
| PDDL [18] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | m |
| dRAID [19] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | m |
| RAID+ [20] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | m |
| OI-RAID [21] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ≥3 |
| D3 [22] | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | m |
| SODP (ours) | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | >m |

### 2.3. Data Organization
Figure 2 illustrates how data is organized as a parity stripe distributed over a set of disks. A stripeset is the specific disks selected for a one-to-one mapping with the data and parity blocks. As long as k + m remains fixed, stripesets are independent of the parity stripe parameters.

## 3. Single-Overlap Declustered Parity (SODP)
In this section, we describe the Single-Overlap Declustered Parity (SODP) scheme, including the algorithms for generating this data placement and an analysis of its improved data protection.

### 3.1. Overview
SODP is a novel declustered parity design that aims to tolerate more disk failures than traditional declustered parity. It achieves this by overlapping the parity stripes in a way that maximizes the number of failures that can be tolerated while minimizing the disk rebuild time.

### 3.2. Algorithms
We provide algorithms for creating SODP data placements for varying numbers of data blocks, parity blocks, and disks. The key idea is to ensure that each disk is involved in multiple parity stripes, thereby increasing the fault tolerance of the system.

### 3.3. Analysis
Our analysis shows that SODP significantly improves data protection compared to traditional declustered parity. Specifically, SODP can tolerate a greater number of disk failures and reduces the probability of data loss during failure bursts.

## 4. SOL-Sim Design
In this section, we introduce the SOL-Sim design, which is an event-driven simulation package used to model the failure, rebuild, and replacement of drives within large-scale storage systems.

### 4.1. Overview
SOL-Sim is a powerful tool that allows us to simulate the behavior of storage systems under various failure scenarios. It provides detailed insights into the performance and reliability of different data protection schemes.

### 4.2. Features
- **Event-Driven Simulation**: Models the dynamic behavior of storage systems.
- **Failure and Rebuild Modeling**: Simulates the failure and rebuild processes of disks.
- **Scalability**: Can handle large-scale storage systems with tens of thousands of disks.

## 5. Evaluation Methodology
In this section, we describe our methodology for evaluating SODP using a combination of real data, realistic traces, and simulation. We also compare its performance with current state-of-the-art parity schemes.

### 5.1. Real Data and Traces
We use real failure traces from LANL’s data center to create a failure model that accurately describes the existing file system's failure patterns. This model is then used to generate failure data for hypothetical system designs.

### 5.2. Simulation
Using SOL-Sim, we simulate the behavior of storage systems under various failure scenarios. Our simulations show that SODP-based placement algorithms can achieve a 30x improvement in the probability of data loss during failure bursts and provide similar data protection with only half the parity overhead.

### 5.3. Comparison with State-of-the-Art
We compare SODP with other state-of-the-art parity schemes, such as BIBD, DATUM, and GridRAID. Our results demonstrate that SODP outperforms these schemes in terms of fault tolerance and data protection.

## 6. Conclusions
In this paper, we introduced Single-Overlap Declustered Parity (SODP), a novel data protection scheme that tolerates more disk failures than traditional declustered parity. We also presented CoFaCTOR, a tool for generating realistic failure traces, and used it to evaluate SODP. Our results show that SODP-based placement algorithms can significantly reduce the probability of data loss during failure bursts and provide similar data protection with reduced parity overhead. Future work will focus on further optimizing SODP and extending it to even larger storage systems.

---

This revised version of the text is more structured, coherent, and professional, making it easier to understand and follow.