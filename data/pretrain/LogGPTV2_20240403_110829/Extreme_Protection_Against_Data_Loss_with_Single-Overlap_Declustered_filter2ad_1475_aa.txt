title:Extreme Protection Against Data Loss with Single-Overlap Declustered
Parity
author:Huan Ke and
Haryadi S. Gunawi and
David Bonnie and
Nathan DeBardeleben and
Michael Grosskopf and
Terry Grov&apos;e and
Dominic Manno and
Elisabeth Moore and
Brad Settlemyer
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Extreme Protection against Data Loss with
Single-Overlap Declustered Parity
Huan Ke, Haryadi S. Gunawi
University of Chicago
Chicago, USA
{huanke,haryadi}@uchicago.edu
David Bonnie, Nathan DeBardeleben, Michael Grosskopf,
Terry Grov´e, Dominic Manno, Elisabeth Moore, Brad Settlemyer
Los Alamos National Laboratory
{dbonnie,ndebard,gross,tagrove,dmanno,lissa,bws}@lanl.gov
Los Alamos, NM, USA
Abstract—Massive storage systems composed of tens of thou-
sands of disks are increasingly common in high-performance
computing data centers. With such an enormous number of
components integrated within the storage system the probability
for correlated failures across a large number of components
becomes a critical concern in preventing data loss. In this paper
we reconsider the efﬁciency of traditional declustered parity data
protection schemes in the presence of correlated failures. To
better protect against correlated failures we introduce Single-
Overlap Declustered Parity (SODP), a novel declustered parity
design that tolerates more disk failures than traditional declus-
tered parity. We then introduce CoFaCTOR, a tool for exploring
operational reliability in the presence of many types of correlated
failures. By seeding CoFaCTOR with real failure traces from
LANL’s data center we are able to create a failure model that
accurately describes the existing ﬁle system’s failure model and
can use that model to generate failure data for hypothetical
system designs. Our evaluation using CoFaCTOR traces shows
that when compared to the state of the art our SODP-based
placement algorithms can achieve a 30x improvement in the
probability of data loss during failure bursts and achieves similar
data protection using only half as much parity overhead.
I. INTRODUCTION
Massive storage systems, such as those used for cloud
archival services [1],
[2] and the ﬁle systems at high-
performance computing (HPC) data centers [3], [4] provide
critical data services to users and are increasingly relied on to
never lose data. At the same time denser and larger disk drives
make these storage systems at greater risk for catastrophic
failures and data loss [5]. These systems rely heavily on RAID
technology using declustered parity, to provide fault tolerance
and prevent the loss of valuable data – however, declustered
parity schemes were not designed to tolerate large numbers of
failures in short windows of time.
Figure 1(a) shows how storage systems at both cloud
archival services and HPC data centers at national laboratories
have grown to include tens and even hundreds of thousands
of disk drives. At such large drive counts the probability of
failure bursts and the risk of data loss due to a failure burst
are both increasing. Similarly in Figure 1(b) we see that as the
number of drives within a parity-protected enclosure increases,
the likelihood of drive failures increases at a linear rate
while improvements in rebuild time decrease as the amount
of rebuild data for a single disk failure remains ﬁxed. With
modern disk enclosures routinely incorporating 84 or 106 disk
)
K
(
s
e
v
i
r
D
#
 120
 90
 60
 30
 0
Backblaze Services
HPC File System
35
47
18
20
10
107
72
91
36
disk failure rate
rebuild time
2009 2013 2014 2015 2016 2017 2018
 40  60  80  100  120  140  160  180  200
Year
Disk enclousure size
(a) Storage System Disks over Time
(b) Disk Fault Tolerance
Fig. 1: Figure 1a shows the growth in the number of drives
used at Backblaze (a cloud service providing data archiving)
and the parallel ﬁle systems deployed at LANL, ORNL, and
LLNL HPC data centers. Figure 1b shows how the rate of disk
failures (5% failure rate) increases linearly as disks are added
to the enclosure while the time to rebuild data decreases more
slowly (with a 50MB/s disk rebuild rate) because the amount
of data to rebuild for a single failure remains ﬁxed (e.g. 8TB).
drives [6], [7] and ﬁle systems commonly spanning multiple
large disk enclosures it becomes imperative to develop new
methods for preventing data loss.
At the same time we see an additional change in the nature
of storage system component failures. Modern data protection
schemes and the mean time to data loss calculations that
motivate them both assume that drive failures are independent
and identically distributed [8], [9]. However, recent studies
describing state of the art data protection in large-scale cloud
data centers have instead explored the likelihood of data loss
during correlated failure bursts [10]–[12]. Similarly, in this
paper we present data from Los Alamos National Laboratory’s
(LANL) ﬁle system attached to the Trinity supercomputer [13]
that indicates that correlated failures are common within that
storage system as well.
Based on the emergence of extremely large disk enclo-
sures and a new requirement to tolerate bursts of correlated
failures we have developed a new data protection scheme
called Single-Overlap Declustered Parity (SODP). With SODP
we have created a new set of data placement schemes for
declustered parity data protection that focuses on maximizing
the number of disk failures tolerated while also minimizing
disk rebuild time. To make SODP generally useful we provide
algorithms for creating SODP data placements for varying
numbers of data blocks, parity blocks, and numbers of disks.
In order to evaluate SODP we developed a scheme for
generating large volumes of trace data using a smaller set
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00050
343
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:32 UTC from IEEE Xplore.  Restrictions apply. 
of realistic failure traces. This tool, called the Correlated
Failure Consultation Tool for Operational Reliability, or sim-
ply CoFaCTOR, develops an accelerated failure time model
using regression statistics that enables the generation of large
numbers of realistic failure traces. CoFaCTOR also enables
the generation of traces that alter physical parameters, like the
number of disks per enclosure, to test alternative designs.
By generating thousands of realistic failure traces we are
able to leverage our event-driven simulation package, SOL-
Sim, for modeling the failure, rebuild and replacement of
drives within large-scale storage systems. Our simulation anal-
ysis enables us to determine that data placement algorithms
based on the SODP principles can dramatically reduce the
probability of data loss in the face of correlated failures.
The remainder of this paper is structured as follows: in
Section II we review declustered parity and describe the pro-
gression of the state of the art for data protection, in Section III
we present a description of single-overlap declustered parity
including the algorithms for generating this data placement
scheme and an analysis of how data protection is improved
with our scheme, in Section IV we introduce the SOL-Sim
design and in Section V we describe our methodology for eval-
uating single-overlap declustered parity using a combination
of real data, realistic traces, and simulation and then describe
how our schemes reduce the probability of data loss compared
with current state of the art parity schemes, and in Section VI
we present our study’s conclusions.
II. RELATED WORK
In this section, we review existing declustered parity ap-
proaches, and then discuss future directions for declustered
parity data layouts. In 1990, Muntz and Lui [23] ﬁrst in-
troduced and analytically modeled declustered parity, but left
the data placement decisions as an open problem. To address
data placement, Holland and Gibson [14] implemented parity
declustering based on Balanced Incomplete Block Designs
(BIBD). As part of this work they identiﬁed six criteria for
ideal declustered layouts:
• Single failure correcting, no two units of the same stripe
are mapped to the same disk.
• Distributed reconstruction, when a disk fails,
the re-
construction workload is evenly distributed across the
surviving disks.
• Distributed parity, all disks have the same number of
parity units.
• Efﬁcient mapping, the mapping from client data to disk
is implementable with low time and space requirements.
• Large write optimization, each parity stripe is aligned
across the disks such that a stripe can be written without
pre-reading the prior contents of any disk.
• Maximal parallelism, a read of n continuous data units
induces parallel access from n disks.
BIBD(v, b, r, k, λ) is a collection of b subsets of k elements
over a set of v distinct objects, where each object appears in
r subsets and each pair of two objects appears in λ subsets.
When BIBD is applied in declustered parity, the essence is to
344
(cid:2)
(cid:3)
v
k
(cid:2)
(cid:3)
(cid:2)
ﬁnd a data mapping to distribute parity stripes of size k over
v disks (e.g., declustered layout), where each disk appears in
(cid:3)
r disk subsets and each pair of two disks appears in λ disk
v
subsets. Note that the complete block design consists of
k
of the
subsets, where each object appears in exactly
subsets.
v−1
k−1
DATUM [15] improves the data mapping by directly com-
puting disks and offsets without using BIBD table lookup.
The key idea is to utilize the complete block design, with
disk subsets, and compute
a particular ordering of the
the disks and offsets through the orderings. One drawback of
DATUM is the complete block designs were originally too
large to be usable.
GridRAID [16] is the declustered parity scheme that used
on LANL’s Trinity ﬁle system. The basic idea is to divide
the stripe data into tiles, each of which is a group of stripes
across the disk array. In each tile, it does the data permutation
to make the data, parity and spare space spread. With multiple
tiles, it beneﬁts from the distributed reconstruction workload
during the recovery.
PRIME [17] is designed for prime values of v to approach
the ideal declustered layout by slightly relaxing the maxi-
mal parallelism property. Like BIBD, PRIME constructs the
declustered layout only for a limited set of conﬁgurations.
RELPR [17] is similar to PRIME, but it deviates from
the ideal in two ways: maximal parallelism and distributed
reconstruction. However, RELRP is applicable to arbitrarily
conﬁgured disk array size v to achieve approximately balanced
declustered layout via on-demand calculation.
PDDL and dRAID [18] declusters the layout by permuting
the disks to spread the parity, spare, and client data units
throughput the disk array. Obtaining satisfactory base disk
permutations is a challenge which makes PDDL only appli-
cable to limited conﬁgurations. [19] extended work based on
PDDL that is designed for use within the Zettabyte File System
(ZFS) [24]. To simplify the generation of base permutation,
dRAID randomly generates multiple base permutations.
RAID+ [20] enables RAID construction over large disk
enclosure to spread reconstruction workload in a balanced way.
RAID+ utilizes the Latin squares to construct a declustered
layout. The only problem is that
the number of v-order
mutually orthogonal Latin squares (MOLS) for general v is
still an open problem, it’s known to exist when v is a power
of a prime number, which is exactly the same as BIBD.
D3 [22] focuses on the data distribution in large-scale
distributed storage system. Similar to declustered layout, D3
designs a layout with orthogonal array to uniformly spread
data and parity units across servers in the system.
OI-RAID [21] is a two-layer encoding architecture and uses
BIBD in the outer layer to achieve a balanced data layout.
It spreads the data and parity across BIBD groups to enable
group fault tolerance, which implies at least three arbitrary
disk failures.
Table I shows how the above schemes satisfy the six
original declustered layout criteria. As we can see, all these
existing works violate the properties of ideal data layout to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:32 UTC from IEEE Xplore.  Restrictions apply. 
Schemes
BIBD [14]
DATUM [15]
GridRAID [16]
PRIME [17]
REPLR [17]
PDDL [18]
dRAID [19]
RAID+ [20]
OI-RAID [21]
D3 [22]
SODP (ours)
Single Failure
Correcting
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Distributed
Reconstruction
(cid:2)
(cid:2)
Distributed
Parity
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
Large Write
Efﬁcient
Mapping Optimization
Maximal
Parallelism
M
M
L
H
M
M
L
H
M
M
L
Conﬁguration
Limitation
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
#Fault
Tolerance
1
m
m
m
m
m
m
m
(cid:3)3
m
(cid:3)m
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
(cid:2)
TABLE I: Comparison of declustered layouts. H represents highly approaching maximal parallelism property, and M and L
means medium and low, respectively. m is the maximal disk failures that be tolerated in the declustered layout.
Fig. 2: Data organized as a parity stripe that will be distributed
over a set of disks. A stripeset then is the speciﬁc disks selected
for a one-to-one mapping with the data and parity blocks. As
long as k + m remains ﬁxed, stripesets are independent of the
parity stripe parameters.
some extent. Difﬁculties balancing these criteria lead many
modern software based declustered parity schemes to use
approximately balanced designs [25]. Finally, while many of
these schemes have been extended to tolerate m failures our
scheme, SODP, has been explicitly designed to tolerate greater
than m failures.