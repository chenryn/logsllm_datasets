differ only by timestamp we merge them into a single
summarized internal condition with a time range between
the earliest and latest timestamp in the set. For Snort alerts,
we also abstract over external IP addresses so that alerts that
differ only on the external source or destination addresses
are merged and the corresponding IP address is abstracted
as “external”.
B. Application of inference rules
All the rules in the observation correspondence and inter-
nal model can be viewed as inference rules. The reasoning
engine applies those rules on the input Datalog tuples
to derive assertions with various levels of certainty. The
certainty of the derived fact is the lowest certainty of the
facts and rules used to derive it.
Handling time: Timestamps associated with security
monitoring events are important in tracking and diagnosing
root causes of problems. Time stamps are used in the reason-
ing process but for presentation simplicity we do not show
time stamps associated with the observations and internal
conditions. When an observation correspondence rule is
used, the internal condition on the righthand side simply
inherits the time ﬁeld of the observation on the left.When
an internal model is used, the time stamps associated with
the derived assertion will be derived from the timestamp on
the lefthand side and the direction of the rule in a straight-
forward manner (e.g., for a forward rule the righthand-side
shall happen after the lefthand side). Latency in detection
and clock skews can also make timestamps imprecise and
less useful, which needs to be addressed through techniques
like time windows which we leave as future work.
We use int(F, m) ⇐ Pf to represent that “the internal
fact F is true with modality m”, and Pf
is the proof that
shows the logical derivation steps arriving at the conclusion.
From an observation one can derive an internal belief
with some degree of certainty, based on the observation
correspondence relation. As an example,
the open IRC-
Socket
identiﬁed through memory dump in the incident
described in section I-A will be an input
to our sys-
tem: obs(memoryDumpIRCSocket(172.16.9.20,172.16.9.1)).
Together with observation correspondence A5, the rule will
derive:
int(exchangeCtlMessage(’172.16.9.20’,’172.16.9.1’), l) ⇐
obs(memoryDumpIRCSocket(’172.16.9.20’,’172.16.9.1’))
The fact int(exchangeCtlMessage(172.16.9.20,172.16.9.1), l)
derived above, together with the internal-model rule I4b1,
would yield the following derivation trace.
int(compromised(172.16.9.20), l) ⇐
int(exchangeCtlMessage(172.16.9.20,172.16.9.1), l) ⇐
obs(memoryDumpIRCSocket(172.16.9.20, 172.16.9.1))
Since the certainty mode for I4b1 is c, joined with the l
mode in int(exchangeCtlMessage(172.16.9.20, 172.16.9.1),
499
the mode
l), we get
int(compromised(172.16.9.20), l).
as
l
for
the
resulting fact
One could argue that the certainty of the derived fact
should be lower than that of the weakest fact in the derivation
chain, especially when the derivation chain is long. However,
given the observation from SAs that most enterprise network
intrusions are carried out in just a few steps, we do not
expect the derivation chains to be long in practice. Since the
certainty modes only represent a rough guess, accounting for
certainty level decay along short derivation paths is unlikely
to be signiﬁcantly valuable. By taking the upper-bound of
the certainty level (weakest link in the chain), we err on the
side of false positives, which can be ruled out at a later stage
after analyzing the attached proof trace.
C. Proof strengthening
The key purpose of reasoning about uncertainty is to
derive high-conﬁdence facts from low-conﬁdence ones. In
the true-life incident, the SA strengthened his belief that the
Trend Micro server was compromised by combining three
different pieces of evidence: netﬂow ﬁlter result showing
communication with a blacklisted IP address, memory dump
result showing likely malicious code modules, and memory
dump result showing open IRC sockets with other Trend Mi-
cro servers. These three pieces of evidence are independent
— they are rooted on observations at different aspects of the
system, and yet they are logically connected — all of them
indicate that the Trend Micro server is likely compromised.
Thus they altogether can strengthen our belief in the fact
that the server is compromised. We generalize this reasoning
process in the following proof strengthening rule.
int(F, m1) ⇐ Pf1
int(F, m2) ⇐ Pf2 Pf1 (cid:4) Pf2
int(F, strengthen(m1, m2)) ⇐ strengthenedPf(Pf1, Pf2)
The (cid:4) relation indicates that two proofs are independent,
meaning they are based on disjoint sets of observations and
internal conditions. This deduction rule states that if we have
two reasoning paths to a fact with some conﬁdence levels
and if the two paths are based on independent observations
and deductions, then the conﬁdence level of the fact can be
strengthened. The strengthen function is deﬁned below.
strengthen(l, l) = strengthen(l, p) = strengthen(p, l) = c
Simply put, two independent proofs can strengthen to
“certain” if at least one of them can yield a “likely” mode.
There is no deﬁnition for strengthen when both parameters
are p or at least one of them is c. Since the p mode represents
very low conﬁdence we do not allow strengthening from just
possible facts. There is no need to strengthen a fact if it is
already proved to be certain.
We emphasize that the strengthening rules are deﬁned
through our empirical study on real-life security incidents
and these strengthening conditions do reﬂect the mental
process a human SA goes through when catching real
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:13:03 UTC from IEEE Xplore.  Restrictions apply. 
| ?- show_trace(int(compromised(H),c)).
int(compromised(’172.16.9.20’),c) strengthen
int(compromised(’172.16.9.20’),l) I_4b1
int(exchangeCtlMessage(’172.16.9.20’,
’172.16.9.1’),l) A_5
obs(memoryDumpIRCSocket(’172.16.9.20’,
’172.16.9.1’))
int(compromised(’172.16.9.20’),l) A_4
obs(memoryDumpMaliciousCode(’172.16.9.20’))
int(compromised(’172.16.9.20’),l) A_3
obs(netflowBlackListFilter(’172.16.9.20’,
’129.7.10.5’))
Figure 4. Result of applying the reasoning system on the case study
attacks. The rules presented above are by no means ﬁnal
products and will need to be further reﬁned through more
empirical study.
D. Implementation
We use the XSB [17] system to evaluate the Prolog
reasoning engine. We also implemented a simple proof-
generator so that whenever a fact is derived, the proof trace
for that fact can also be obtained simultaneously. We applied
the reasoning system and the model described in II-B and
II-C on the input for our case study. The result is shown in
Figure 4 (IP addresses are sanitized.). The user enters the
query show_trace(int(compromised(H),c)),
to ﬁnd
all
the provable facts in the form of compromised(H)
with “certain” mode. This is essentially asking the question
“which machines are certainly compromised”. The reasoning
engine prints out a derivation trace for 172.16.9.20, the
IP address for the compromised Trend Micro server ﬁrst
identiﬁed by the SA. It is clear that the derivation trace
exactly matches the reasoning process the human SA used
to identify the compromised server — the conﬁdence level
is strengthened from concordant evidence emanated from
netﬂow dump and memory dump.
IV. AUTOMATING MODEL BUILDING FOR SNORT
Snort (http://www.snort.org/) is a popular open-source
network intrusion detection system utilizing a rule-driven
language for specifying alert conditions. We developed a
Snort add-on, called SnIPS, with the purpose of helping
the Snort user community create an empirical model for
reasoning about Snort alerts using the techniques discussed
in this paper. The add-on is based on the system architecture
in Figure 3, with only Snort alerts as input from the bottom.
Each Snort alert is converted into a tuple like the one below.
obs(snort(’1:1140’, ’172.16.9.18,
’192.168.0.20’, ’2008-06-09 21:05:14’)).
The ﬁrst parameter 1:1140 is the unique SID associated
with the Snort rule that generated this alert. The second and
third parameters are the source and destination IP address
of the captured network packet. The last parameter is the
timestamp (our system actually represents the time stamp
as an integer internally). We use the same internal model
and reasoning engine developed from the real-life incident
to analyze Snort alerts. But we still need to create the
observation correspondence relations for Snort alerts. In the
Snort rule repository each rule is given a “classtype” along
with some natural-language description. We ﬁnd that these
pieces of information can be used to automatically infer the
internal predicate and mode to be assigned to an alert. For
example, the Snort rule 1:1140 is
alert tcp $EXTERNAL_NET any -> $HTTP_SERVERS
$HTTP_PORTS (msg:"WEB-MISC guestbook.pl access";
flow:to_server,established; uricontent:
"/guestbook.pl"; nocase; metadata:service http;
classtype:attempted-recon; sid:1140;
rev:12;)
This rule alerts for all the web request containing “/guest-
book.pl” in the URI, which is an application with known
vulnerabilities. The classtype of the rule is “attempted-
recon”, indicating that these packets are generally considered
reconnaissance activities. Based on this we can generate an
observation correspondence assertion as follows.
obsMap(obsRuleId_3614,
obs(snort(’1:1140’, FromHost, ToHost)),
int(probeOtherMachine(FromHost, ToHost)),l).
There are 30 classtypes a Snort rule writer can use to
classify the nature of the potential security implications. We
try to map the classtypes to the internal predicates in our
model. But classtypes alone do not always convey enough
information. Thus we also make use of two ﬁelds in the
natural-language description of a rule: “impact” and “ease
of attack”. These two ﬁelds for the above Snort rule are:
Impact:
Information gathering and system integrity
compromise. Possible unauthorized
administrative access to the server.
Possible execution of arbitrary code of
the attackers choosing in some cases.
Ease of Attack:
Simple. Exploits exist.
Since there is a keyword “exploits exist” in the “Ease of
Attack” ﬁeld, our automated program infers that the mode
of the observation correspondence assertion is “likely”, as
shown above. In general, we found that these two ﬁelds
are often composed of a set of ﬁxed keywords. Examples
are “possible unauthorized administrative access”, “possible
execution of arbitrary code”, “exploits exist”, and so on.
These phrases often indicate what internal predicate and
certainty mode can be assigned to the alert. Our automated
program searches for those keywords in the Snort rule
repository. Based on the combination of keywords contained
in a Snort rule description, a simple heuristic algorithm
infers the internal predicate and the certainty mode for alerts
generated by the Snort rule. In this example, we can output
another observation correspondence assertion for the alert:
obsMap(obsRuleId_3615,
obs(snort(’1:1140’, FromHost, ToHost)),
int(compromised(ToHost)), p).
Using this simple method, we were able to automatically
generate the observation correspondence relations for 60%
500
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:13:03 UTC from IEEE Xplore.  Restrictions apply. 
of all the approximately 9000 Snort rules. This is certainly
just a rough baseline, since our automated program has
to make an “educated guess” from imprecise and incom-
plete information currently in the Snort rule repository. But
we believe such an initial model, derived from existing
knowledge would prove to be helpful for Snort users to
beneﬁt from our empirically developed technique. If our
reasoning system proves to be useful by a signiﬁcant number
of users, there will be incentives for more people to help
“ﬁne-tune” the observation correspondence. As for future
Snort rules, the security expert who writes a Snort rule
(or any such speciﬁcation) has the best knowledge on what
the observation means and is best qualiﬁed to provide the
information that enables our processing and reasoning. This
should not incur much additional burden since much of this
information is already being maintained in an unformatted
and ad-hoc manner.
V. EXPERIMENTS AND RESULTS
In this section we describe our effort on conducting
experimental evaluation of our reasoning system. Due to the
empirical nature of our approach, the evaluation is focused
on whether a reasoning model developed empirically from
studying one incident can be generalized to ﬁnd interesting
attack traces in others. We use the SnIPS tool described
in section IV for our evaluation. The reasoning engine and
internal model of SnIPS were developed based on the true-
life incident we studied and the observation correspondence
relations were automatically generated from the Snort rule
repository. We then applied SnIPS to two third-party data
sets as well as a production network. Note that the data we
use to test SnIPS has nothing to do with the true-life incident
based on which we developed SnIPS.
A. Experiment on the Treasure Hunt data set
The ﬁrst set of experiments was performed on the Trea-
sure Hunt (TH) data set [18]. This data set was created
during a cyber-attack competition organized in a graduate
security course at University of California, Santa Barbara.
Our motivation to use this particular data set was that the
data set provides valuable “meta data” such as the back story
(competition task details) and network topology which can
help us understand the result. We only used the TCPdump
portion of the dataset to generate Snort alerts as input to
SnIPS.
The ﬁrst task in the TH competition was to gain access to
the web server. This scenario was identiﬁed by our model
and the high-conﬁdent output is shown in Figure 5 (the
parenthesized numbers are added for explanation purpose).
The web server, 192.168.10.90, was certainly compromised
(1) based on two independent proofs: (2) and (4), which cap-
ture the ﬁrst step: an exploit being sent from an external host
to the web server (3), and the second step: reconnaissance by
the attacker from the web server to learn about the internal
(1)int(compromised(’192.168.10.90’),c) strengthen
(2)
(3)
int(compromised(’192.168.10.90’),p) I_3f
int(sendExploit(external,’192.168.10.90’),
summarizedFact obslist(273)
(4)
(5)
int(compromised(’192.168.10.90’),l) I_1b
int(probeOtherMachine(’192.168.10.90’,