### Correlated Failures Due to Error Propagation

When an independent failure occurs in a system, there is a conditional probability \( p_e \) of a second failure occurring as a result of the first. This leads to an increased overall failure rate. The increase in the failure rate for all nodes is computed by multiplying the independent failure rate by a constant parameter called `frate_correlated_factor`.

**Figure 3** illustrates the birth-death Markov process for correlated failures due to error propagation. Here, \(\lambda_i\) and \(\lambda_c\) represent the rates of system-wide independent failures and successive correlated failures, respectively. \(\lambda\) is the independent failure rate of a single node, and \(\mu\) denotes the recovery rate of the system. \( F_i \) represents the system state where \( i \) failures have occurred before a successful recovery (with states \( F_0 \), \( F_1 \), and \( F_2 \) shown as examples).

We assume that any successful recovery eliminates all latent errors, so all \( F_i \) states transition directly to \( F_0 \) at the recovery rate. Additionally, it is assumed that the failure rates at all \( F_i \) states (where \( i > 0 \)) are the same. The conditional probability of another failure given that a failure has already occurred is:

\[ p = \frac{\lambda_c}{\lambda_c + \mu} \]

This implies:

\[ \lambda_c = \frac{p \mu}{1 - p} \]

Let \( n \) denote the number of nodes, and \( r \) denote the `frate_correlated_factor`. According to the model:

\[ \lambda_c = \lambda_i + r n \lambda = n \lambda (1 + r) \]

Thus:

\[ r = \frac{p \mu}{(1 - p) n \lambda} - 1 \]

For a given set of \(\lambda\), \(\mu\), and \( n \), the `frate_correlated_factor` \( r \) represents the conditional probability \( p \). As long as \(\lambda_c > \lambda_i\), \( r \) can be chosen independently to study a range of correlated failure effects. For example, when \( n = 1024 \), \( p = 0.3 \), MTTR = 10 minutes, and MTTF = 25 years, \( r \) is approximately 600.

### Failure Rate Notation
- \(\lambda_s\): Failure rate of the entire system
- \(\lambda_{si}\): Rate of independent failures in the system
- \(\lambda_{sc}\): Rate of correlated failures in the system
- \(\lambda\): Independent failure rate per node
- \( r \): Increased failure rate due to correlated failures
- \(\gamma\): Correlated failure coefficient
- \( n \): Number of nodes

### Experimental Setup and Results

We use the Mobius modeling environment [21] to create and simulate Storage Area Networks (SANs). Steady-state simulation is employed with an initial transient period of 1000 hours to allow the system to reach a steady state. The confidence level is set to 95%. Unless otherwise specified, the parameter values are as listed in Table 3, which are based on field data or projections for future systems.

Given the complexity of the modeled system and the presence of multiple mechanisms/parameters, we analyze the system by examining one feature at a time. We first study the base model without coordination or correlated failures (but including failures during checkpointing and recovery) to understand the basic system behavior. Then, we investigate the effects of coordination and correlated failures using the following metrics:

- **Useful work fraction**: The fraction of time the system makes forward progress towards job completion, excluding work repeated due to failures.
- **Total useful work**: The product of the useful work fraction and the number of compute processors, indicating the number of processors required to achieve the same performance under failure-free computation.

#### Study of Base Model

For the base model, we assume independent failures and consider the coordination time as a fixed quiesce time. System performance is analyzed for a range of parameters, including the number of processors, checkpoint interval, MTTF per node, and MTTR of the system:

- Number of processors per node: 8
- MTTF per node: 1 year
- MTTR of the system: 10 minutes
- Number of processors: 64K
- Checkpoint interval: varied from 15 minutes to 4 hours

If permanent failures are considered, the overhead of system reconfiguration will result in a larger MTTR.

### Major Results

- **Optimal Number of Processors**: For a given checkpoint interval (30 min), MTTR (10 min), and MTTF (1 yr per node), there is an optimal number of processors (128K) for which total useful work is maximized. Adding more processors beyond this point will degrade system performance due to failure effects. The optimal number of processors varies from 128K to 32K as the MTTR increases from 10 minutes to 80 minutes.
- **Checkpoint Interval**: For the system to be scalable, checkpoints should be taken on the granularity of minutes (15-30 min) rather than hours. While theoretically there is an optimal checkpoint interval, in practice, no such interval maximizes useful work due to the low overhead of background checkpoint writing and the dominant effect of frequent failures.
- **Useful Work Fraction**: Even when useful work is maximized, the overall useful work fraction does not exceed 50% for an MTTF per node of 1 year. Thus, more than 50% of system resources are spent on checkpointing and recovery.
- **Increase in Compute Power**: If the number of processors per node is increased from 8 to 32 while maintaining the per-node MTTF at 1 year, the total useful work can be increased for the same number of nodes. The optimal number of processors is in the range of 500K. However, the useful work fraction remains unchanged as the system failure rate, which depends only on the number of nodes and the per-node failure rate, is the same.

### Variation of Total Useful Work

- **Number of Processors**: Figures 4a, 4c, and 4e show the variation of total useful work with different numbers of processors. There is an optimal value of the number of processors for which total useful work is maximized. Smaller MTTFs, larger MTTRs, and larger checkpoint intervals decrease the optimal number of processors.
- **Checkpoint Intervals**: Figures 4b, 4d, and 4f show the variation of total useful work for different checkpoint intervals. For large-scale supercomputing systems, there is no optimal checkpoint interval within the range of 15 minutes to 4 hours. This contradicts previous studies [7, 8], which have shown the existence of an optimal value. The theoretical optimal checkpoint interval is less than 15 minutes, but intervals shorter than this may overwhelm the I/O subsystem and network, making them impractical.

### Figures and Tables

- **Figure 3**: Birth-death Markov process of correlated failures
- **Table 3**: Model Parameters
- **Figures 4a, 4c, 4e**: Variation of total useful work with the number of processors
- **Figures 4b, 4d, 4f**: Variation of total useful work with checkpoint intervals

This comprehensive analysis provides insights into the optimal configuration and performance of large-scale computing systems, considering the impact of correlated failures and checkpointing strategies.