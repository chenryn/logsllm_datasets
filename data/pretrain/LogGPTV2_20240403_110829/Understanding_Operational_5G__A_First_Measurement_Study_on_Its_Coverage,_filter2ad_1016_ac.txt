defined in [66]), the gNB only responds to the A3 event due to the
ISP’s configuration, and then triggers the HO procedure. For the
definition of each hand-off trigger event, please refer to Tab. 5 in
Appendix A. We thus focus on the A3 event, which indicates that
the signal quality of the neighboring cell is higher than that of the
current serving cell for a certain period [44], i.e.,
Mn+ O f n+ Ocn− Hys> Ms+ O f s+ Ocs+ O f f ,
where Mn⇑Ms is the RSRQ value of the neighbor/serving cell.
O f n⇑O f s is the frequency offset (default 0 dB). OCn⇑OCs is the cell
RSRQ gap (Mn− Ms) between the serving cell and the neighboring
specific offset (default 0 dB). O f f is the intra frequency offset, and
Hys is the frequency hysteresis (1 dB, 3 dB in our measurement,
respectively). In order to prevent the frequent switching caused
by signal fluctuations, two rules are applied in A3 event: (i) The
cell must meet a threshold (3dB in the 5G gNB configurations, as
we calculate from the above parameters using XCAL-Mobile) to
successfully trigger a cell hand-off. (ii) A timetotrigger parameter
is used as the time hysteresis, i.e., a HO is triggered only when
the above condition (Eq. (1)) holds true for timetotrigger (324ms in
existing configuration).
To examine the effectiveness of such a HO strategy, we present
(1)
-25-20-15-10-5036891215RSRQ (dB)Time (s)Serving PCIPCI 226PCI 44PCI 441 0 0.2 0.4 0.6 0.8 1-20-1003102030CDFRSRQ Gap (dB)4G-4G5G-5G5G-4G4G-5G 0 0.2 0.4 0.6 0.8 1 0 40 80 120 160 200CDFHandoff Delay (ms)4G-4G5G-5G4G-5GFigure 7: UDP and TCP throughputs.
Figure 8: TCP cwnd evo. under 5G.
Figure 9: Packet loss ratio.
a case study in Fig. 4, where the UE switches from cell-226 to cell-
44. If no HO occurs, the quality of the old cell will deteriorate.
However, after switching to a new cell, the link quality does not
necessarily get better. Statistically, we analyze and plot the changes
in instantaneous RSRQ before and after each HO in Fig. 5. We
observe that only 75% HOs have more than 3 dB RSRQ gain on
average (80% for 4G-4G, 84% for 5G-5G, 75% for 5G-4G and 61%
for 4G-5G). The result reveals that the current empirical HO strategy
in 5G has a non-negligible probability (i.e., 25%) of worsening link
performance. A more intelligent strategy is required to determine
when to trigger the HO.
HO latency. A 5G-5G HO process starts with LTE MAC RACH
trigger, and ends with the NR MAC RACH Attempt (SUCCESS) mes-
sage, which can be captured by XCAL-Mobile. We compute the
HO latency and plot the CDF of all the measured cases in Fig. 6.
Surprisingly, the 5G-5G HO latency is 108.40 ms on average, while
that of 4G-4G and 4G-5G is only 30.10 ms and 80.23 ms. We identify
the root cause to be the NSA architecture, wherein 5G NR runs its
own data plane, but relies on the control plane of the existing LTE
network for control functions including HO management. In partic-
ular, the smartphone cannot directly switch to any 5G neighboring
cells, but has to release its current 5G NR resource and roll back to
the current 4G eNB. Then it performs a HO between the current 4G
eNB and the target 4G eNB, and finally requests 5G NR resources on
the target master 4G eNB. We confirm this complicated procedure
by analyzing and extracting the compete HO signaling exchanges
as given in Appendix A. It is expected that this long HO latency
problem can be resolved in the future 5G SA architecture with
independent data and control plane.
4 END-TO-END THROUGHPUT AND DELAY
4.1 Transport Layer Throughput
UDP throughput baseline. We use iperf3 to measure the maxi-
mum available bandwidth between the cloud server and the 5G
smartphone. We gradually increase the UDP sending rate, and use
the peak UDP throughput measured at the receiver side as the
baseline. Each experiment is repeated 5 times for 60s during the
daytime and late-night, respectively. From the results in Fig. 7, we
see that the UDP baseline for 5G downlink (DL) is 880 Mbps on
average during the day, in contrast to 130 Mbps for 4G. During the
late-night, the UDP baseline of 5G DL increases slightly (i.e., 900
Mbps), while that of 4G DL increases dramatically to 200 Mbps.
The reason lies in the limited number of 5G users (small day-night
variation), as 5G just entered an early commercialization stage. It
is known that all users associated with the same base station need
484
to share the same set of Physical Resource Blocks (PRBs), and for
a given channel condition, a user’s bit-rate is proportional to the
number of PRBs. Using the XCAL-Mobile tool, we find that for
5G, almost all the PRBs (260∼264 in a frame) are allocated to the
allocated to the smartphone (e.g., 95∼100) than daytime (e.g., only
40∼85 PRBs).
smartphone under test regardless of time. In contrast, at nighttime,
less user contention in the 4G network leads to more PRBs being
Note that the maximum physical layer bit-rate is 1200.98 Mbps
for 5G DL (time slot ratio is 3:1 for DL and UL in our ISP’s configura-
tion following Rel-15 TS 38.306 [13]) assuming all PRBs are allocated
to one user and the highest Modulation and Coding Scheme (MCS)
is selected. In particular, we often monitor the MCS index is 27,
which corresponds to a maximum code rate of 0.925 for the highest
spectral efficiency in 256 QAM. Thus the UDP baseline is 74.94% of
the maximum physical bit-rate, which is reasonable considering the
overhead in control channels and higher layer protocol operations.
In addition, the UL case is similar, for which the 4G/5G baselines
are 50 Mbps/130 Mbps during daytime and 100 Mbps/130 Mbps at
night. Unless otherwise specified, we use the daytime throughput
as a baseline in the following experiments.
TCP throughput anomaly. We further examine the perfor-
mance of three representative categories of TCP algorithms: Loss
based Reno [28] and Cubic [39], delay based Vegas [22] and Veno
[32], and the recently proposed capacity-probing based BBR [24].
For a fair comparison, we switch between different TCP algo-
rithms by configuring the Linux kernel modules of the same pair of
server/client, while keeping other settings intact. We use bandwidth
utilization as the performance metric, defined as the throughput
ratio between TCPs and the UDP baseline. From the results in
Fig. 7, we observe that, for 4G, loss-based TCP and BBR perform
reasonably well (utilization 52.9%, 64.4%, and 79.1% for Reno, Cu-
bic and BBR, respectively), whereas delay-based TCP is known to
perform poorly [91]. For 5G, BBR achieves reasonably high band-
width utilization of 82.5%. However, the traditional loss/delay based
TCP algorithms suffer from extremely low bandwidth utilization—
only 21.1%, 31.9%, 12.1%, 14.3%, for Reno, Cubic, Vegas, and Veno,
respectively!
To identify the root cause, we plot the evolution of congestion
window (cwnd) of a typical 5G BBR and Cubic session, respectively,
in Fig. 8. We find that BBR’s cwnd remains high except for the slow-
start phase (taking about 6 seconds as the gray line shows), while
Cubic’s cwnd never reaches its reasonable level due to frequent
multiplicative decrease, which hints to severe packet losses. Thus,
we proceed to examine the packet loss under different traffic load by
sending UDP traffic at a certain fraction of the baseline bandwidth,
1.55 ×3.13 × 0 200 400 600 800 10000615304560 0 1200 2400 3600 4800 6000Cubic cwnd (KBytes)BBR cwnd (KBytes)Time (s)CubicBBRRetr 0 1 2 3 4 51/51/41/31/21Packet Loss Rate (%)Fraction of the Baseline Bandwidth4G5GFigure 11: Bursty loss pattern of 5G.
468
2586
10539
26724
11007
29310
Figure 12: HO throughput drop.
from gNB to the cloud server), the difference is about 2.5×. As the
whole 5G path is roughly 2.5× compared with 4G.
In contrast, recall that the capacity of 5G DL is 5× over 4G, i.e.,
wired network buffer takes a dominant role, the buffer size on the
the capacity growth is incommensurate with the buffer size expansion
in the wireline network, which is likely the reason for the high
packet loss. The conjecture can be validated by the loss pattern.
Specifically, we extract and plot two segments of packet sequence
numbers in a 5G session, in Fig. 11. We find that the packet loss in
5G exhibits a clear bursty pattern, which should be caused by the
intermittent buffer overflow.
An important question follows: How much buffer is needed to
eliminate the TCP anomaly in 5G? Reasonable buffer size can be
empirically determined by the Stanford model [16, 71, 85]: B=(RTT⋅
C)⇑⌋︂
buffer size of 5G paths should be 5× of that of 4G paths. Considering
size in the wired network part should be increased 2× to accommodate
n, where C is the network capacity, and n is the number of
concurrent flows. Here we assume the same flow number n, and
similar RTT (validated in Sec. 4.4) for 4G and 5G networks, the total
the existing buffer statistics in Table 3, we suggest that the buffer
5G. On the other hand, since buffer resizing may be costly and time
consuming, an easier solution is to adopt BBR-like algorithms that are
less sensitive to packet loss/delay, at least for NSA – the transitioning
phase of 5G.
One concern is the bufferbloat issue [33, 38, 46], i.e., deeper
buffers may accommodate packets into long queues, thereby crash-
ing delay-sensitive applications. In particular, 4G and 5G flows
may share a common Internet path. While a larger buffer is helpful
to reduce the packet loss rate of 5G data streams, it may hurts 4G
flows. Therefore, the impact of large buffer on the trade-off between
packet loss and delay requires in-depth research, which we leave for
future exploration. For instance, there should be a more intelligent
data distribution framework in the SA architecture, particularly for
5G and non-5G flows.
4.3 TCP Throughput During Hand-off
We now examine how 5G hand-off impacts TCP performance. We
traverse the campus region and other areas in our city many times
at a walking/bicycling speed (i.e., 3∼10 km/h) while continuously
measuring the BBR throughput over 10ms windows. We use the
same data set described in Sec. 3.4, consisting of 407 hand-off events.
Fig. 12 plots the CDF of normalized throughput gap (i.e., the per-
centage of TCP throughput drop immediately after the hand-off).
We can observe that 5G-4G and 5G-5G hand-off suffer significant
throughput degradation (83.04% and 73.15%, respectively), in con-
trast to only 20.10% for 4G-4G hand-off. The reason lies in the large
hand-off latency (Sec. 3.4) which interrupts the normal TCP trans-
Figure 10: Retransmission stat.
Table 3: Buffer size on different network components.
Buffer Size RAN Wired Network Whole Path
4G
5G
1
3 ,
1
2 , 1⌋︀. The results (Fig. 9) show that the packet loss of
i.e.,(︀ 1
already exceeds 3.1% (10× of 4G session) even at a mild 1
5G sessions is multi-fold over the 4G sessions. For instance, the loss
2 of the
1
4 ,
5 ,
baseline UDP bandwidth.
4.2 Locating the Performance Bottleneck
We proceed to locate where the packet loss anomaly takes place.
Packet loss in the radio access network (RAN). Due to the
volatile wireless channel, packet losses are inevitable in the RAN,
but the MAC/LLC layers usually adopt error checking/correction
and retransmission mechanisms, such as ARQ and HARQ, to re-
cover from losses and hide them from the upper layers. Although
Rel-15 TS 38.321 [9] does not explicitly specify the retransmission
threshold, we identify the value to be 32 based on the PDSCH config-
uration messages exposed by XCAL-Mobile. Even for an unusually
lossy link with a loss rate of 50%, it is unlikely that it will expe-
rience 32 consecutive failed attempts (probability is only 2.3e-10).
Our analysis of the XCAL-Mobile traces (Fig. 10) further verify that
all retransmissions eventually succeed after up to 4 trials in 4G and
2 in 5G, which still falls far below the re-transmission threshold. In
addition, we also find that almost all wireless resource has been allo-
cated to the end device (observed from the PRB allocation statistics
collected by XCAL-Mobile) when we measure the transport layer
throughput. Therefore, the packet loss is irrelevant to MAC-layer
resource allocation inside the gNBs. So, we can safely conclude
that the packet loss bottleneck is not on the 5G wireless link.
In-network buffer estimation. Another potential reason for
packet losses lies in buffer overflow along intermediate routers
within the end-to-end path. We thus estimate the network buffer
size, following the classical “max-min delay” method [25]. In a
nutshell, the buffer size is the product of the longest packet queuing
delay along the path (i.e., the gap between RTTmax and RTTmin)
and the estimated network capacity. Specially, we use traceroute to
measure the RTTs of RAN and wired network, 30 times for each
path and 60s for each measurement. Then we get the estimated
buffer sizes (the maximum number of buffered packets) in Tab. 3.
Note that the result is derived under the assumption of 1 Gbps path
capacity and also 60 Bytes packet size. Though the absolute value
of buffer sizes may deviate from the ground truth (due to inexact
link capacity estimation), the ratios among them are accurate and
support the following deduction. We observe that within the RAN,
the 5G buffer size is 5× over 4G. But within the wired network (i.e.,
485
 0 4 8 12 16 201234Packet Retr Rate (%)# of Retransmission Times4G5G 0 1 2 30200004000060000Sequence   Packet IDNormalPacket loss 0 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1CDFNormalized throughput gap4G-4G5G-5G5G-4GFigure 13: Delay statistic.
Figure 14: RTT along each path hop.
Figure 15: RTT vs. path length.
mission. The experiment again confirms the limitations of 5G NSA
architecture.
4.4 End-to-end Latency
Overview. We measure the RTTs of 80 random paths crossing the
4G and 5G networks, respectively. Specifically, we select 4 5G gNBs
(with co-sitting 4G eNBs) spatially spread across our city, and 20
other Internet servers nationwide. The location (latitude, longitude)
of these servers can be found in Appendix C. For each pair gNB and
server, we run traceroute on the 5G smartphone to measure the
RTT. To ensure the traceroute probing packets not be fragmented
on the router, we set the payload to be a minimum value of 1 Byte.
In addition, we use UDP probing instead of the default ICMP to
prevent the packets from being filtered out by some routers. We
repeat the measurement 30 times for each path. The scatter plot in
Fig. 13 shows the 4G vs. 5G RTT for each measurement. We have
two observations: (i) 5G network paths achieve a network latency
(i.e., half of the RTT) of 21.8ms on average. In contrast, the Rel-8
TS 23.203 [6] mandates that for interactive real-time applications
like VR, the transmission delay should be limited to 10ms. Clearly,
the current end-to-end latency of 5G NSA is insufficient to meet
such requirements. (ii) Nonetheless, the 5G paths still reduce RTT
by 22.3ms (31.86%) on average, compared to 4G.
Delay breakdown. We then investigate where 5G’s latency
reduction comes from. We select one example network path con-
sisting of 8 hops, and measure the RTT as hop count increases.
From Fig. 14, we find that: (i) The RAN latency reduction (hop
1) is negligible: 2.19±0.36ms (5G) vs. 2.6±0.24ms (4G). Note that
the Rel-15 38.913 [7] standardizes the 5G air interface delay 4ms
for eMBB (enhanced Mobile Broadband, which is satisfied by the
current NSA architecture), and 0.5ms for uRLLC (ultra Reliable Low