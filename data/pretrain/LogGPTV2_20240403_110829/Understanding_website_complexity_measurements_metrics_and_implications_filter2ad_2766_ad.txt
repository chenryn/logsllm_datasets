between RenderEnd and number of objects.
ulate that because the base web pages of websites are typically not
that large, page load times are dominated by the number of RTTs
rather than the number of bytes transferred. Thus, having chosen
the number of requests and number of scripts as the top 2 fea-
tures, the marginal contribution of total size is low. Second, we
ﬁnd that the prediction error ﬂattens after the ﬁrst 4-5 features are
selected. In other words, we see a natural diminishing returns ef-
fect of adding other features. Identifying a small, but predictive set
of features is a critical result for both website designers and perfor-
mance optimization tools.
Figure 18 visually conﬁrms the goodness-of-ﬁt. The X-axis rep-
resents the actual page load time (RenderEnd), and the Y-axis rep-
resents the load time predicted by the regression model. One in-
teresting artifact is a horizontal cluster near the origin, where the
model predicts the same value of RenderEnd for around 30 sites.
On further investigation, we discovered that these websites have
very sparse landing pages that have little to no content. Thus, the
values of the top 5 metrics were zero and the prediction model out-
puts a constant value.
Having determined the top k complexity metrics for the entire
population of websites in our study, we next analyze if there is a
signiﬁcant difference across website categories. We repeat the re-
gression analysis for each category separately. Table 5 shows the
top 2 metrics identiﬁed by the regression for each category. It also
shows the cardinality of the set intersection between the top 5 met-
rics for each category and the top 5 metrics from the aggregate
regression model across all websites (from Figure 17).
First, we see that the top 2 metrics are quite diverse across the
different categories. For example, the number of images and re-
quests are most critical for News sites, but the total size of Javascript
objects and the number of servers are most critical for Games sites.
Second, there is a signiﬁcant overlap in the top 5 metrics between
the per-category models and the overall model. In fact, for most
categories, the metrics in the intersection are the number of ob-
jects, the number of servers, and the number of Javascript objects,
which are the top 3 metrics in Figure 17. This suggests that while
there is some value in customizing the prediction model to identify
Figure 16: Page load times for websites of different categories.
the critical metrics, the aggregate model is a very useful starting
point in itself.
We also experimented with an enhanced regression model where
we added nonlinear terms, e.g., log(X) and X 2, for each feature
X. Adding these nonlinear terms does not improve the prediction
error and does not change the stability of the top k feature set. We
do not present these results given space limitations.
5.2 Variability in load times
So far, we studied the impact of various complexity metrics on
median page load times. Next, we analyze if the same set of factors
impact how load times vary, or if a different set of metrics are crit-
ical for predicting variability. Here, we restrict our analysis to only
focus on websites where the complexity metrics such as page size
and number of object requests do not change signiﬁcantly across
measurements. That is, the variability in RenderEnd cannot be at-
tributed simply to a change in the content of the page across our
measurements.
As a starting point, we measure variability in load times for any
particular website as the difference between the 75th and 25th per-
322Figure 17: Normalized mean-squared error as a function of top
k coefﬁcients.
Figure 18: Scatter plot of page load times predicted by the re-
gression model that uses the top 5 features versus the actual
page load times.
Category
Business
Technology
Games
Kids and Teens
News
Shopping
Top 2
# objects, # images
# js, # origins
# servers, size js
# js, # objects
# images, # objects
size css, # js
Top 5 Intersection
3
3
3
3
3
3
Table 5: Highest-impact complexity metrics in the regression
model for different website categories, and their intersection
with aggregate regression model.
centile values of RenderEnd across different measurements for that
website; we consider the difference between the 75th and 25th per-
centiles instead of between the maximum and minimum values to
discount for any client-side effects. Then, we correlate the absolute
and normalized (by the median) value of this variability versus the
various complexity metrics. As seen in Figure 19, in comparison
to the earlier correlation result, we ﬁnd two differences. First, the
correlations are weaker in general (e.g., the highest value is 0.65).
Second, the number of servers is the most dominant factor in this
case, instead of the number of objects, which was the dominant
indicator of absolute load times.
5.3 Summary of main observations
The key takeaways from our analysis of load times are:
• The top ﬁve complexity metrics that determine RenderStart
and RenderEnd are the total number of objects loaded, the
number of these objects that are Javascripts, the total web-
page size, and the number of servers and origins contacted in
loading objects on the page.
• We can build a sparse model for predicting page load times
with a normalized mean squared error less than 0.1.
• Variability of load times is less correlated with our complex-
ity metrics, but number of servers is its most dominant indi-
cator rather than the number of objects.
6. DISCUSSION
The previous sections provide a good ﬁrst-order understanding
of how website complexity affects the user experience. We ac-
knowledge that there are likely to be a much wider range of fac-
tors that can affect a user’s web experience: “deeper” non-landing
pages, diversity in client-side platforms, use of client and provider
tools for customizing websites, and other forms of personalization
(e.g., services that require a login). In this section, we present a
preliminary discussion of such factors.
Landing vs. non-landing pages: Our study focused on the land-
ing pages of websites. As a preliminary study to evaluate how
“deeper” non-landing pages might differ, we consider a random
sample of 100 sites. For each such site, we follow other links to
pages within the same domain (e.g., www.foo.com has a link
to www.foo.com/bar or x.foo.com/bar) and compute the
various complexity metrics for each such non-landing page. For
each site and metric, we then look at the difference between the
base site and the median across these landing pages, and normalize
this difference by the value for the base site. Figure 20 shows the
distribution across sites of these normalized differences for ﬁve key
metrics: number of requests, number of servers, fraction of non-
origin objects, page size, and download time. We see that other
than from the perspective of the fraction of non-origin objects met-
ric, websites on which non-landing pages are less complex than the
base site far outweigh the sites on which the opposite is true. We
do, however, see a long negative tail with the non-landing pages of
some sites being up to 2× more complex than the base site.
Choice of browser: The choice of browser does not affect our
complexity analysis in Section 4. However, browsers may vary
in the speciﬁc strategies in how they parallelize requests, optimize
scripts and so on; this could affect the load time analysis in Sec-
tion 5. One additional concern when comparing load time results
across browsers is that the semantics of onLoad might vary [7].
(This is not a concern for our paper because all our measurements
use the same version of Firefox.) We leave for future work the task
of understanding these effects.
323Figure 19: Correlation coefﬁcients between the different met-
rics and the variability in RenderEnd.
Personalized web services: Some services present different land-
ing pages to users who have subscribed or logged in. For example,
facebook.com and igoogle.com have content that is person-
alized to the speciﬁc user logged in. The key to understanding these
effects is to emulate user proﬁles that are representative of a broad
spectrum of browsing habits (e.g., casual vs. expert users). While
this is relevant to our broader theme, it is outside the scope of this
paper.
Interaction with client-side plugins: Users today deploy a wide
range of client-side browser extensions to customize their own web
browsing experience. For example, two popular Firefox extensions
block advertisements (Adblock) and scripts (NoScript). We
conducted a preliminary study spanning 120 randomly chosen web-
sites from our overall list on how these extensions impact the com-
plexity metrics. We avoid page load time here because these ex-
tensions alter the user experience (i.e., it is not showing the same
content) and it is unfair to compare the time in this case.
Figure 21 compares two separate browser instances—one with
the extension enabled, and the other with the browser instance from
our previous measurements. Here, we install each extension with
its default conﬁguration. We only show the complexity metrics
that were dominant indicators of page load time and variability in
load time: number of objects and number of servers. The median
number of objects requested reduces from 60 on the vanilla browser
to 45 with Adblock and 35 with NoScript. The reduction in the
number of servers is even more marked—the median value drops
from 8 to 4 with Adblock and to 3 with NoScript. NoScript’s
effect is more pronounced than that of Adblock because disabling
scripts can in turn ﬁlter out objects that would have been fetched as
a consequence of the script’s execution.
It is unclear though if the reduction in complexity caused by
these client-side controls is entirely good, even though it likely im-
proves page load times. We do not know how this affects the user
experience that the website provider intended for the user (e.g., is
some useful content being blocked?) and how these may affect the
provider’s business interests (e.g., ad click/conversion rates).
Figure 20: Preliminary result showing that non-landing pages
are less complex than landing pages for most sites.
Customization for client platform: In parallel to the evolution of
web pages, the diversity of client platforms (e.g., mobile phones,
tablet computers, and even televisions) used for web access has
also increased. These platforms vary in their connectivity, display
attributes, and user interface. Consequently, providers are inter-
ested in customizing the web experience for these platforms.
We considered 120 randomly chosen websites that offer cus-
tomized web pages for mobile phones. We visited these sites once
with the default browser setting and once with the browser instru-
mented to emulate an iPhone (by spooﬁng the UserAgent string).
Figure 22 shows that the phone-speciﬁc customization dramatically
reduces the number of objects fetched and the number of servers
contacted. Again, we focus only on complexity metrics and not the
page load time because it does not represent a reasonable compari-
son (e.g., time on actual mobile phones would be different than on
a spooﬁng browser). However, as in the case of client-side con-
trols, it is not clear if this customization affects the user experience
compared to a desktop-based experience (e.g., was some content
dropped).
Optimizations for improving performance: Many tools like Page-
Speed from Google [2] suggest optimizations such as compressing
images, combining requests for small images and CSS ﬁles, and
“minify-ing” Javascript objects to improve website performance.
Based on some sample websites, we found that minify js appears as
a “high priority” suggestion for several websites.7
To analyze how this optimization would help websites in the
wild, we emulate the effect of running it on each website in our
dataset. Figure 23 shows the savings this optimization could pro-
vide in terms of the fraction of total bytes of Javascript and the frac-
tion of the total size of the web page downloaded. We see that for
the vast majority of websites, the potential savings is quite small.
While this result is preliminary and does not explore all possible
optimizations, it does hint that optimizations that are broadly per-
ceived as high-priority may not yield high gains for all websites.
Thus, we need to explore more systematic tools and new avenues
to improve page load times.
7This is a code compacting tool that removes unnecessary white
spaces, and comment lines to reduce the size of Javascript objects.
324(a) No. of objects
(a) No. of objects
(b) Total no. of servers contacted
(b) No. of servers
Figure 21: Reduction in number of objects and servers con-
tacted with client-side ﬁltering.
Figure 22: Reduction in number of objects and servers with
phone-speciﬁc customization.
7. CONCLUSIONS AND FUTURE WORK
The increasing complexity of web pages and its impact on per-
formance has been anecdotally well-recognized, but there have been
no rigorous studies of the same. In this paper, we presented a ﬁrst
attempt at characterizing web page complexity and quantifying its
implications. We characterized the complexity of web pages both
based on the content they include and the services they offer. We
ﬁnd that a website’s popularity is a poor indicator of its complex-
ity, whereas its category does matter. For example, News sites
load signiﬁcantly more objects from many more servers and ori-
gins than other categories. Also, we found that though a signiﬁcant
fraction of objects and bytes are fetched from non-origins on most
websites, the contribution of non-origins to page load time is mini-
mal in comparison. Our correlation- and regression-based analysis
showed that number of objects and number of servers are the domi-
nant indicators of page load time and variability in page load times,
respectively.
Our preliminary results also show that reducing this complexity
and improving client performance is not straightforward. Though
client-side ﬁltering of particular content types may reduce page
load times, it can adversely impact user experience and compro-
mise revenue for web providers. On the other hand, website providers
need to understand the impact that non-origin content on their web
pages has on their users in order to customize for different client
platforms.
As future work, we are continuing our efforts in several directions—
deeper analyses of non-landing pages, studying the dependency be-
tween the various objects on a page, ﬁnding better indicators of
performance and variability by focusing on websites within cer-
tain rank ranges and certain categories, and designing strategies to
systematically balance the tradeoff between performance, user ex-
perience, and the provider’s business interests.
8. REFERENCES
[1] Firebug. http://getfirebug.com/.
[2] Google Page Speed.
http://code.google.com/speed/page-speed/.
[3] HTTP archive beta. http://httparchive.org/.
[4] HTTP archive speciﬁcation. http://groups.google.
com/group/http-archive-specification/
web/har-1-1-s%pec?hl=en.
[5] Keynote systems. http://www.keynote.com.
[6] Let’s make the web faster. http://code.google.com/
speed/articles/web-metrics.html.
[7] Measuring the mobile web is hard.
http://matt-welsh.blogspot.com/2011/08/