D error (i.e. excluding false alarms), we analyze the number
of failures in different time periods, the time between failures
(TBF) for each component class and the failure probability
during the life span of a single component.
A. Number of failures at different time periods.
Hypothesis 1. The average number of component failures is
uniformly random over different days of the week.
Figure 3 shows the average number of failures during each
day of the week. Due to limited space we only present the
components with the most number of failures, and due to
conﬁdentiality concerns, we normalize the count to the total
number of failures. It is obvious from the ﬁgure that the
failure rates vary and are not uniformly distributed on each
day in a week. More formally, a chi-square test can reject the
hypothesis at 0.01 signiﬁcance level for all component classes.
Even if we exclude the weekends, a chi-square test still rejects
the hypotheses at 0.02 signiﬁcance level. The test indicates
that failures do not occur uniformly randomly in each day of
a week.
Hypothesis 2. The average number of component failures is
uniformly random during each hour of the day.
Similarly, we calculate the number of failures during each
hour in the day. Figure 4 shows eight component classes with
the most number of failures. A similar chi-square test rejects
the hypothesis at 0.01 signiﬁcance for each class.
Possible Reasons. 1) The number of failures of some com-
ponents are positively correlated with the workload. This is
especially true for hard drive, memory and miscellaneous
failures, as Figure 4 (a), (b) and (h) show. Such correlation
between the failure rate and the workload is consistent with
ﬁndings in [5, 22].
We want to emphasize that this correlation might not imply
the causality that low workload reduces the probability of
hardware failures. In fact, we believe that the higher utilization
causes failures more likely to be detected asynchronously. For
example, the agents detect hard drive and memory failures by
monitoring speciﬁc log messages (e.g. dmesg) that are more
likely to occur under heavy utilization.
This observation reveals the limitation of log-based failure
detection - it does not detect failures in a component until
it gets used. Also, detecting failures only when the workload
is already heavy increases the performance impact of such
failure. The failure management team is working on an active
failure probing mechanism to solve the problem.
2) If failure reporting requires the human in the loop, the
detection likely to happen during working days and regular
working hours. This is true for most manually reported mis-
cellaneous failures.
3) Some components tend to fail in large batches during a
small period of time. For example, a large batch of failed RAID
(a) HDD
(b) Memory
(c) RAID card
(d) Miscellaneous
Fig. 3. The fraction of number of failures on each day of the week.
cards of the same model makes the distribution in Figure 3 (c)
highly skewed, and we can also see many notable high spikes
in almost all the plots in Figure 4. We discuss more about such
batch failures in Section V-A.
B. Time between failures (TBF)
In this section, we focus on the distribution of the time
between failures (TBF) for each component class. The com-
mon belief is that the failure occurrences in a system follow
a Poisson process, and thus people often model
the TBF
with an exponential distribution. However, previous studies
show that the distribution of TBF for the hard drives or the
HPC systems cannot be well characterized by exponential
distribution [5, 17, 23]. We extend the result and show the TBF
for each component class, as well as all components combined.
Hypothesis 3. TBF of all components in the data centers
follows an exponential distribution.
We conduct the same chi-square test as previously de-
scribed, and the chi-square test rejects the hypothesis at the
0.05 signiﬁcance level. In fact, Figure 5 shows that none
of the distributions including exponential, Weibull, gamma
and lognormal ﬁts the TBF data. The observation is different
from some previous studies [5, 24–26], who report that the
TBF of HPC and cloud (including hardware failures) can
be well characterized by a Weibull distribution or a gamma
distribution. We believe the disagreement
is the result of
the wide presence of batch failures, which makes the TBF
distribution highly skewed, in the data centers we examine,
which we discuss more below.
Hypothesis 4. TBF of each individual component class follows
an exponential distribution.
We then break down the analysis to each component class.
We also break down the failure by product lines. All the
results are similar, that is, the hypotheses that the TBF follows
exponential, Weibull, gamma or lognormal distributions can be
rejected at the 0.05 signiﬁcance level. We omit the ﬁgures here
due to space limitation.
28
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
(a) HDD
(b) Memory
(c) Motherboard
(d) RAID card
(e) SSD
(f) Power
(g) Flash card
(h) Miscellaneous
Fig. 4. The fraction of number of failures on each hour in a day. The values on the horizontal axis of each subﬁgure indicate the hours in a day.
1
0.8
0.6
0.4
0.2
F
D
C
0
100
Exp
Weibull
Gamma
LogNormal
Data
101
102
Time between Failures (min)
Fig. 5.
distributions. The horizontal axis is on a logarithmic scale.
CDF of TBF for all component failures, as well as some ﬁtted
Possible Reasons. Two possible reasons cause the deviation
of TBF from the exponential distribution.
1) As previous work has pointed out, the failure rates
change over the lifetime of each component, or the entire
system. This change affects the TBF over time in a way that
the exponential distribution cannot capture [17]. We will take
a closer look at the fact in Section III-C.
2) We observe lots of small values in TBFs, indicating that
there are short time periods with many failure occurrences. The
MTBF (mean time between failures) across all data centers
we investigate (with hundreds of thousands of servers) is only
6.8 minutes, while the MTBF in different data centers varies
between 32 minutes and 390 minutes. None of the distributions
mentioned above capture these small TBF values, as Figure 5
shows. These small TBFs are related to the correlated failures
especially batch failures we will discuss in Section V.
C. Failure rate for a single component in its life cycle
People commonly believe that the likelihood of a com-
ponent failure is related to its life-in-service [5, 17]. Previous
studies [17] show the probability of hard drive failure cannot be
well characterized by a bathtub curve model [27], in which the
failure rates are high both at the beginning (“infant mortality”)
and the end (“wear-out”) of the lifecycle. In this section, we
verify the claim on all the major component classes.
We consider the monthly failure rate (FR) for each com-
ponent in its lifecycle2. We normalize all failure rates for
conﬁdentiality, and Figure 6 shows the change of failure rates
of each component class during their ﬁrst four years of service
life. The main observations are as follows.
Infant mortalities. RAID cards obviously have high infant
mortality rate (Figure 6 (f)). Of all the RAID cards that failed
within ﬁfty months of its service life, 47.4% of the failures
happen in the ﬁrst six months.
We observe some infant mortalities in hard drives during
the ﬁrst three months, with 20.0% higher failure rate than that
of the 4th to 9th month (Figure 6 (a)). We also see that the
failure rates start to increase only six months after deployment,
and rise signiﬁcantly over the following couple of years. This
is consistent with previous studies [17] but differs from the
“bathtub curve”, where the failure rates stay low and stable
for much longer (e.g. at least a year).
Interestingly, we observe that
the miscellaneous failure
rates (Figure 6 (i)) are extremely high within the ﬁrst month.
After the ﬁrst month, the failure rates become relatively stable.
The reason is that most manual detection and debugging efforts
happen only at deployment time. During normal operation,
operators often respond to component failures with a simple
replacement order, without much manual effort to debug. The
“lazy” actions reduce the number of miscellaneous FOTs for
older components.
Failures in motherboards, ﬂash cards, fans, and power
supplies (Figure 6 (c)(e)(g)(h)) are rare in the early years of
servers’ lifecycles. This effect may be related to the quality as-
surance process during manufacturing. Also, at the deployment
2 The dataset reports the number of HDDs, SSDs, and CPUs on each server,
and thus for these three categories, we know the numbers of properly-working
components during each interval, which are used to compute the failure rates.
For other components, we assume that the component count per server is
similar, and use the number of servers as an estimation.
29
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
(a) HDD
(b) Memory
(c) Motherboard
(d) SSD
(e) Flash card
(f) Raid Card
(g) Fan
(h) Power
(i) Miscellaneous
Fig. 6. Normalized monthly failure rates for each component class. The values on the horizontal axis of each subﬁgure indicate the time in production use (in
months). Some components are omitted because the numbers of the samples are small.
and initial testing phase, operators may have already marked
these failed components as miscellaneous.
Wear out. We observe an increase in failures in many
component classes when they get older. The wear-out rate is
different for these classes. For example, most (72.1%) of the
motherboard failures occur three years after deployment.
Although only 1.4% of failures happen during the ﬁrst 12
months for ﬂash cards, failure rates rise fast after that, showing
strong correlated wear-out phenomena. Memory failures show
a similar pattern (Figure 6 (b)), though the failure rate is
relatively stable during the ﬁrst year, it gets higher starting
between the 2nd and 4th year.
Mechanical components such as hard drives, fans, and
power supplies (with fans too) (Figure 6 (a)(g)(h)) also exhibit
strong, clear wear and tear pattern, as the failure rates are
relatively small during the ﬁrst year and gradually increases
as servers get older.
D. Repeating failures and the effectiveness of repairs
We observe that a small number of server components
fail repeatedly. We deﬁne repeated failures as cases when the
problem marked “solved” (either by operators or an automatic
reboot), but the same problem happens again afterward.
There are not many repeating failures. In fact, as operators
“repair” a component usually by replacing the entire module,
which is effective most of the time. Over 85% of the ﬁxed
components never repeat the same failure. We estimate that
about 4.5% of all the servers that ever failed (thousands of
servers) have suffered from repeating failures.
Fig. 7. CDF of the number of failures w.r.t. the percentage of the servers
that ever failed.
However, surprisingly we observe that 2% of servers that
ever failed contribute more than 99% of all failures. In other
words, the failures are extremely non-uniformly distributed
among the individual servers. Figure 7 shows the CDF of the
number of failures with respect to the fraction of the servers
that ever failed. This observation is consistent with ﬁndings
in [7, 10, 22, 24].
As an extreme example, we observe that one single server
in a web service product line reports over 400 failures, either
on RAID card or hard drives. The root cause is a BBU (Battery
Backup Unit) failure causing the RAID card to be up-and-
down. Each time, after an automatic recovery program reboots
the server, the hard drive becomes online again, and thus the
problem is marked “solved”. However, it will fail again very
soon. Without human intervention, the process has repeated for
almost a year before someone ﬁnally realizes the root cause
(the BBU) and solves the problem.
30
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:01:13 UTC from IEEE Xplore.  Restrictions apply. 
TABLE IV.
CHI-SQUARE TEST RESULTS FOR HYPOTHESIS 5.
0.01 ≤ p < 0.05
p-value
p < 0.01
p ≥ 0.05
Ratio
10 out of 24
4 out of 24
10 out of 24
(a) Data center A
(b) Data center B
Fig. 8.
The failure ratio at each rack position. (a) In data center A,
Hypothesis 5 cannot be rejected by a chi-square test at 0.05 signiﬁcance.
(b) In data center B, Hypothesis 5 can be rejected at 0.01 signiﬁcance.
As a more interesting observation, multiple servers can
repeat the failures synchronously, causing strong failure cor-
relations. We provide an example in Section V-C.
IV. SPATIAL DISTRIBUTION OF THE FAILURES
The common belief is that all physical locations in the
same data center are identical, without much impact on server
failures. We ﬁnd out, however, the spatial location sometimes
does affect the failure rate of servers, especially the relative
position (i.e. slot number) on the rack.
We study 24 production data centers in our dataset and
count the average number of failures at each rack position.
We ﬁlter out repeating failures to minimize their impact on
the statistics. Moreover, as not every rack position has the
same number of servers (e.g. operators often leave the top of
position and bottom position of the racks empty), we normalize
the failure rates to the total number of servers at each rack
position. In this statistics, we count a server failure if any of
its components fail.
Hypothesis 5. The failure rate on each rack position is
independent of the rack position.
Interestingly, different data centers show different chi-
square test results. Table IV summarizes the results. In general,
at 0.05 signiﬁcance level, we can not reject the hypothesis in
40% of the data centers while we can reject it in the other
60%.
Figure 8 shows the failure ratio in two example data
centers. In data center B, we can reject the hypothesis with
high conﬁdence, while in data center A, we cannot.
Possible Reasons. One possible reason is the design of data
center cooling and the physical structure of the racks. While
the focus of the paper is not on the relation between the
temperature and failure, we have an interesting observation.
For both data center A and B, We can observe notable spikes
at certain rack positions. Even though Hypothesis 5 cannot be
rejected for data A, a further anomaly detection reveals that
the FRs at rack position 22 and 35 are singularly high in data
center A. Speciﬁcally, assume the failures occur on each rack
position independently and uniformly randomly, according to
central limit theorem, the FR on each rack position should
follow a normal distribution with small variance as the number
of failures gets large. We estimate the expectation μ and the
variation σ2 of the FR at each rack position and discover that
the FRs of rack positions 22 and 35 in data center A lie out
of the range (μ − 2σ, μ + 2σ).
In fact, position 35 is close to the top of a rack. With the
under-ﬂoor cooling design, it is the last position cooling air
reaches. Position 22 is next to a rack-level power module in the
custom rack design. Our motherboard temperature readings at
these places are indeed several degrees higher than the average
motherboard temperature in each rack. This higher temperature
might result in higher failure rate at these two positions.
The data centers host multiple generations of servers par-
titioned to hundreds of product lines. Each data center has
distinct building architecture too. With these many uncertain
variables, we cannot provide a universal explanation of the
uneven spatial distributions of failures. However, from the
data centers we investigate, we ﬁnd that in around 90% data
centers built after 2014, Hypothesis 5 cannot be rejected at
0.02 signiﬁcance level. That is, the hardware failures are more
uniformly distributed on each rack position, probably because
the new data centers have a better cooling design, making the
inside environment more consistent across all rack positions.
V. CORRELATED FAILURES
Correlated failures are the least desirable, as hardware
diagnostics and software fault tolerance usually assume in-
dependent failures. In this section, we take a closer look at
correlated failures. We see two types of correlations.
•
•