### Analysis of Component Failures and Time Between Failures (TBF)

#### A. Number of Failures at Different Time Periods

**Hypothesis 1: The average number of component failures is uniformly random over different days of the week.**

To analyze the distribution of failures, we examine the number of failures across different days of the week. Figure 3 presents the average number of failures for each day, focusing on the components with the highest failure counts. Due to confidentiality, the failure counts are normalized to the total number of failures. The data clearly shows that failure rates vary significantly and are not uniformly distributed across the days of the week. A chi-square test confirms this observation, rejecting the hypothesis at a 0.01 significance level for all component classes. Even when weekends are excluded, the hypothesis is still rejected at a 0.02 significance level, indicating that failures do not occur uniformly randomly throughout the week.

**Hypothesis 2: The average number of component failures is uniformly random during each hour of the day.**

Similarly, we investigate the distribution of failures by hour. Figure 4 displays the failure rates for eight component classes with the highest failure counts. A chi-square test again rejects the hypothesis at a 0.01 significance level for each class, suggesting that failures are not uniformly distributed across hours of the day.

**Possible Reasons:**
1. **Workload Correlation:** Some components, such as hard drives, memory, and miscellaneous components, show a positive correlation between failure rates and workload. This is consistent with findings in [5, 22]. However, this correlation does not necessarily imply causality; higher utilization may simply make failures more detectable. For example, agents detect hard drive and memory failures by monitoring specific log messages (e.g., dmesg), which are more likely to occur under heavy utilization.
2. **Human Involvement:** If failure reporting requires human intervention, detection is more likely to occur during working days and regular working hours, particularly for manually reported miscellaneous failures.
3. **Batch Failures:** Some components tend to fail in large batches within a short period. For instance, a large batch of failed RAID cards of the same model can skew the distribution, as seen in Figure 3(c) and the notable spikes in Figure 4.

#### B. Time Between Failures (TBF)

In this section, we analyze the distribution of the time between failures (TBF) for each component class. It is commonly believed that failure occurrences follow a Poisson process, leading to an exponential distribution of TBF. However, previous studies [5, 17, 23] have shown that the TBF for hard drives and HPC systems cannot be well characterized by an exponential distribution.

**Hypothesis 3: TBF of all components in the data centers follows an exponential distribution.**

A chi-square test rejects this hypothesis at a 0.05 significance level. Figure 5 illustrates that none of the distributions (exponential, Weibull, gamma, or lognormal) fit the TBF data well. This finding contrasts with some previous studies [5, 24â€“26], which report that the TBF of HPC and cloud systems can be well characterized by a Weibull or gamma distribution. The discrepancy is likely due to the presence of batch failures, which skew the TBF distribution.

**Hypothesis 4: TBF of each individual component class follows an exponential distribution.**

Further analysis of each component class and product line also rejects the hypothesis at a 0.05 significance level. The results are consistent across all classes, and the figures are omitted due to space limitations.

**Possible Reasons:**
1. **Changing Failure Rates:** As noted in previous work, failure rates change over the lifetime of components or the entire system, affecting TBF in ways that an exponential distribution cannot capture [17].
2. **Small TBF Values:** The presence of many small TBF values indicates periods with high failure occurrences. The mean time between failures (MTBF) across all data centers is only 6.8 minutes, with significant variation (32 to 390 minutes) between data centers. None of the mentioned distributions capture these small TBF values, as shown in Figure 5. These small TBFs are related to correlated failures, especially batch failures, discussed in Section V.

#### C. Failure Rate for a Single Component in Its Life Cycle

It is generally believed that the likelihood of a component failure is related to its life-in-service [5, 17]. Previous studies [17] have shown that the probability of hard drive failure cannot be well characterized by a bathtub curve model, where failure rates are high at the beginning ("infant mortality") and the end ("wear-out") of the lifecycle.

**Monthly Failure Rate (FR):** We consider the monthly failure rate for each component in its lifecycle. Figure 6 shows the normalized failure rates for each component class during their first four years of service life.

**Observations:**
- **Infant Mortalities:**
  - **RAID Cards:** High infant mortality rate, with 47.4% of failures occurring within the first six months.
  - **Hard Drives:** Higher failure rates in the first three months, followed by a significant increase after six months.
  - **Miscellaneous Components:** Extremely high failure rates within the first month, followed by relative stability. This is due to manual detection and debugging efforts at deployment time.
- **Wear Out:**
  - **Motherboards, Flash Cards, Fans, and Power Supplies:** Rare failures in the early years, possibly due to quality assurance during manufacturing.
  - **Memory:** Relatively stable failure rates in the first year, followed by an increase starting between the 2nd and 4th year.
  - **Mechanical Components (Hard Drives, Fans, Power Supplies):** Clear wear and tear pattern, with failure rates increasing gradually over time.

#### D. Repeating Failures and the Effectiveness of Repairs

We observe that a small number of server components fail repeatedly. Repeated failures are defined as cases where a problem marked "solved" reoccurs afterward.

**Observations:**
- **Repair Effectiveness:** Over 85% of fixed components never repeat the same failure. About 4.5% of servers that ever failed have suffered from repeating failures.
- **Non-Uniform Distribution:** Surprisingly, 2% of servers that ever failed contribute more than 99% of all failures. Figure 7 shows the cumulative distribution function (CDF) of the number of failures with respect to the fraction of servers that ever failed.
- **Extreme Example:** One server in a web service product line reported over 400 failures, primarily due to a Battery Backup Unit (BBU) failure causing the RAID card to malfunction. The problem was eventually solved after nearly a year of repeated failures.

#### IV. Spatial Distribution of Failures

The common belief is that all physical locations in the same data center are identical, with no significant impact on server failures. However, our study reveals that the spatial location, particularly the rack position, can affect failure rates.

**Hypothesis 5: The failure rate on each rack position is independent of the rack position.**

**Results:**
- **Chi-Square Test:** Different data centers show varying results. Table IV summarizes the chi-square test results. At a 0.05 significance level, the hypothesis is not rejected in 40% of the data centers but is rejected in the other 60%.
- **Example Data Centers:** Figure 8 shows the failure ratio in two example data centers. In data center B, the hypothesis is rejected with high confidence, while in data center A, it is not.

**Possible Reasons:**
- **Cooling Design and Rack Structure:** Notable spikes in failure rates at certain rack positions (e.g., position 22 and 35 in data center A) are observed. These positions are near the top of the rack and next to power modules, respectively, leading to higher temperatures and potentially higher failure rates.
- **New Data Centers:** Around 90% of data centers built after 2014 show more uniform failure distributions, possibly due to better cooling designs.

#### V. Correlated Failures

Correlated failures are undesirable as they violate the assumption of independent failures in hardware diagnostics and software fault tolerance. We identify two types of correlations:
- **Workload-Induced Correlations:** Failures are more likely to occur under heavy utilization.
- **Batch Failures:** Large numbers of similar components failing within a short period.

By understanding these patterns, we can improve failure management and reduce the impact of correlated failures.