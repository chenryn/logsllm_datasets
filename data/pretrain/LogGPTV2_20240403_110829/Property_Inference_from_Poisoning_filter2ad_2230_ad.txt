without property f
Output
1 , . . . ,T 0
1: Sample data sets T 0
A bit that predicts whether t = t0 or t = t1.
b(cid:48)
k with size n from Dt0 and
1 , . . . ,T 1
T 1
k with size n from Dt1. (Note that the adversary can
generate samples from these distributions given sampling access
to D−, D+: e.g. to sample from Dt0, ﬁrst choose bit b from a
distribution that is 1 with probability t0, then sample from Db.)
k ∪ Tpoison and
2: Train M 0
3: Query all models on Tq to get labels (only the label and not the
k using T 0
1 ∪ Tpoison, . . . ,T 1
1 ∪ Tpoison, . . . ,T 0
k ∪ Tpoison.
k.
1, . . . , R0
1 , . . . , M 0
k using T 1
1, . . . , R1
conﬁdence) R1
1 , . . . , M 1
k and R0
M 1
4: Construct a dataset
{(R1
1, 1), . . . , (R1
get MA (We use (cid:96)2 regulizer with weight 2 ·(cid:112)1/k)
and train a linear model with appropriate regularization on it to
5: Query the target model on Tq to get Rq, evaluate MA(Rq), and
1, 0), . . . , (R0
k, 1), (R0
k, 0)}
output the result.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
US Census Income Dataset [10], Enron email dataset [15],
MNIST [16] and CelebA [18].
Target Property In Table I, we summarize the features we
experimented with. In all these experiments, the attacker’s
objective is to distinguish between two possible values for the
frequency of the target feature. Below is a summary list of all
these properties.
DataSet
Census
Census
Census
Enron
Enron
MNIST
CelebA
Target Feature
Random binary
Gender
Race
Random binary
Negative sentiment
Jitter noise
Gender
Distinguish between
.05 vs .15
.6 vs .4 female
.1 vs .25 black
.7 vs .3
.10 vs .05
.0 vs .1
.4 vs .6
TABLE I: Target Features. For each feature we use two different
ratios close to the actual ratios in the dataset (except for random
features which are not present in the dataset).
• Random: To understand the power of this attack on a
feature that is completely uncorrelated to the classiﬁcation
task (which one might naturally think should not be leaked
by an ideal model), we did a set of experiments where we
added a random binary feature to both Census and Enron
datasets and set that as the target feature that the adversary
wants to attack. Note that this feature is not correlated
with any other feature in the dataset and the model should
not depend on it to make its decision. This is backed up
by our experiments in that, as we will see, the attack of
[12], which uses no poisoning does not perform better
than random guessing on this property.
• Gender: Gender is a boolean feature in Census data which
takes values "Male" and "Female". We also attack this
feature in the CelebA experiments where the attack tries to
identify the fraction of female photos used in the training
set. This feature was used in the work of [11, 12], so it
allows us to compare our work with theirs.
• Race: Another feature that we attack in the Census dataset
is Race. We try to infer between two different ratios of
"Black" race in the dataset. Again, we chose this for
purposes of comparison with previous work.
• Negative Sentiment: In one of our target properties, we
try to infer the fraction of emails in the Enron email
dataset that have negative sentiment. To do this, we use
the sentiment analysis tool in python nltk to identify emails
with positive and negative sentiment. Note that unlike all
the other target properties, the negative sentiment feature
is not present as a feature in the dataset which makes it
intuitively seem harder for the attacker to infer. However,
as we will see in our experiments, the attacker can still
attack this property.
• Jitter Noise: This target property is deﬁned on the MNIST
dataset where the adversary tries to identify if all images in
the dataset replaced with a noisy version (with brightness
jitter noise) or they are all intact. We use this property to
replicate the setting of [12].
B. Black-box queries
As mentioned before, we are interested in the information
leakage caused by black-box access to the model. Namely,
the adversary can query the model on a number of points
and infer information about the target property using the label
prediction of the model on those queries (See Section IV for
more details). In a concurrent work [8], also explored this kind
of black-box access in the context of membership inference
attacks. Our model does not require any other information
other than predicted label (e.g. no conﬁdence score, derivative
of the model, etc). The query points are selected according to
algorithm 2 in such a way that they have low certainty.
For Enron experiments, we use 500 query points and for
census data experiments we use 1000 query points. The reason
that we use different numbers for the Enron and census
experiment is that Enron dataset contains fewer of “uncertain”
points; we used almost all the points that fall into our range.
C. Target model architectures
Most of our experiments use logistic regression as the model
architecture for training. The main reason we picked logistic
regression was because it is much faster to train compare to
Neural Networks. However, we also have a few experiments
on the more complex models. In particular, we test our attack
on fully connected neural networks with up to 6 hidden layers
(See Table VI) and Resnet-18 and Resnet-50 (Table VII). We
note that since our attack is black-box, we do not need any
assumption over the target model architecture other than the
fact that it will have high accuracy (we still need the adversary
to know the architecture for the attack to work.). This is again in
contrast with the previous work of [12] that only works on fully
connected neural networks. This is an important distinction as it
shows that the attacker is not relying on extensive memorization
due to large number of parameters.
D. Shadow model training
Our shadow model training step is quite simple. As described
in Section VI, we train a series of shadow models with a ﬁxed
poisoning set. We hold out around 1
3 of the dataset (For both
Enron and Census datasets) training the shadow models. This
held out part will not effect any of the models that we test
our attacker’s accuracy on. After training the shadow models,
we query them and train a simple linear attack model over the
predictions on the queries. We use a linear model to train the
attack model since our theoretical results suggest a simple linear
model (which just takes the uniform average) over the queries
would be enough to make a correct prediction. We use (cid:96)2
regularization for our linear models to get better generalization
and also reduce the number of effective queries as much as
possible. Note that this choice of simple linear models is
contrast with the attack of [11] and [12] that use complex
models (e.g. set neural networks) to train the attack model;
this is one of the reasons that our attack is faster.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
91128
Enron Negative Sentiment 5% v.s. 10%
Enron Random 30% v.s. 70%
Census Random MLP 30% v.s. 70%
Task
Census Random 5% v.s. 15%
Census Race Black 10% v.s. 25%
Census Gender 40% v.s. 60%
Black-box White-box
63%
50%
50%
50%
52%
56%
78%
51%
54%
53%
86%
91%
TABLE II: Baseline black-box and white-box attacks without
poisoning. The White-box baseline uses the sorting based attack
of [12]. The Black-box attacks uses 10000 random queries from the
distribution and uses the results as a representation of the model.
there is some point where adding more poisoning points will
cause the accuracy to decrease. To understand this, one can
think about the extreme case where 100% of the distribution is
the poison data, which means there is no information about the
clean distribution in the trained model and we would expect
the attack to fail.
This effect is especially pronounced for properties that have
very weak signal in the behavior of the ﬁnal model. The Enron
negative sentiment property produces the weakest signal among
all the experiments because (1) the feature does not exist in
the dataset and (2) it has the smallest difference in percentage
(t1 − t0) among all the other experiments (5% vs 10%). We
believe this is why it happens faster for the Sentiment property
compared to other features. However, our insight suggests that
this phenomenon should happen for any property for some
poisoning ratio. To test this insight, we tried various poisoning
rates on the Enron dataset with random target feature. Figure 7
(In Appendix D shows the result of this experiment where the
accuracy of the attack starts to drop at poisoning rates around
30%. Interestingly, this phenomenon could be also explained
using our Theorem as both ends of the range of certainty in
the condition of our Theorem 9 will converge to ∞ when p
approaches 1.
Number of Shadow Models The next set of experiments (See
Fig 2) are to see the effect of the number of shadow models
on the accuracy of the attack. For these experiments, we vary
the number of shadow models from 50 to 2000. We notice that
increasing the number of shadow models increases the attack
accuracy and about 500 shadow models are sufﬁcient to get
close to maximum accuracy. Note that in this experiment we set
the poisoning ratio to small values so that we can see the trend
better. If larger poison ratio were chosen, in most experiments
the attack reaches the maximum of 1 with very few shadow
models and it is hard to see the trend. For instance, with 10%
percent poisoning, the experiments with random feature (both
census and Enron) would reach 100% accuracy with only 50
shadow models. This small number of shadow models makes
the running time of the attack lower. In all of the experiments
in this ﬁgure, the dataset size is set to 1000 except for the
Enron negative sentiment experiment where the size is 2000.
Training Set size In Fig. 3, we wanted to see the effect of
training size on the effectiveness of the attack. Note that our
theoretical attack suggests that larger training size should
actually improve the attack because the models trained on
Fig. 1: Poison rate vs attack accuracy. We change the poisoning budget
from 0% of the training size to 20%. Note that in most experiments,
property inference without poisoning is not effective, but with less
than 10% poisoning all of the experiments get accuracy above 90%.
The curve marked as Census Random MLP shows the performance
of our attack against MLP classiﬁer on census dataset with random
target feature. Also, observe that for one of the experiments (marked
as Enron Negative), the accuracy of the attack starts decreasing after
a certain level of poisoning, we discuss this phenomenon below.
E. Performance of our attack
In the following experiments, we evaluate the performance of
our attack and compare it with the attack of [12]. In the rest of
the manuscript, we denote the attack of [12] as WBAttack. We
ﬁrst evaluate how the different parameters, namely, poisoning
rate, training set size, number of shadow models (deﬁned in
Sec IV) and the complexity of the target model affect its
accuracy. To understand the effect of each parameter, for each
set of experiments, we ﬁx a set of parameters and vary one.
Unless otherwise stated, all of our experiments are repeated 5
times and the number reported is the average on all repetitions.
Poisoning Rate In Fig. 1, we have 6 experiments where we ﬁx
the model to be logistic regression for all of them except one
(Census random MLP) which uses a 5 layer perceptron with
hidden layers sizes 32, 16 and 8. In all the experiments we set
the number of shadow models to be 500 for census experiments
and 1000 for Enron experiments. The training set size is 1000
for Census experiments and 2000 for Enron experiments. We
vary the poisoning rate from 0% to 20%. The number of black-
box queries is set to 500 for experiments on Enron and 1000
for experiments on Census. The attack accuracy for all the
target features is quite low when there is no poisoning. But
with increase in poisoning rate, the attack accuracy dramatically
improves and for all features, the accuracy reaches around .9
with less than 10% poisoning. Table II provides a baseline
without poisoning. Comparing this table with Figure 1 shows
the importance of poisoning in the success of the attack.
The Enron negative sentiment experiment seems like an
anomaly in Figure 1. However, the drop of accuracy with more
poison points could be anticipated. We posit that for all features
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:56:49 UTC from IEEE Xplore.  Restrictions apply. 
101129
dataset and the adversary has access to Enron dataset (which
it also uses for shadow model training). The goal of adversary
is to ﬁnd what fraction of emails in the trained model have a
random feature. Table III shows the success of our algorithm
in different scenarios. The attack that we use for the case that
Attack accuracy
82%
84%
84%
92%
88%
93%
ratios
.3 vs .7
.3 vs .7
.2 vs .8
.2 vs .8
.3 vs .7
.2 vs .8
Model architecture
Logistic Regression
Logistic Regression
Logistic Regression
Logistic Regression
3 hidden layer MLP
3 hidden layer MLP
Poisoning Ratio
5%
10%
5%
10%
10%
10%
TABLE III: The success of attack when attack uses a different data
distribution for shadow model training. The 3 hidden layer MLP
models have layer sizes 32, 16 and 8. The number of shadow models
is 1000 and the size of the dataset is 2000 and the target feature is
"random" in all experiments.
distributions are different is slightly different from the attack
we describe in Section VI. We use techniques that makes the
model robust to distribution shift. For example, we normalize
the training data in the shadow dataset and also add noise
to them so that the trained models are more robust to small
changes. Also, when trying to ﬁnd borderline queries, we
only use queries that are not extremely uncertain (e.g. with
uncertainty between .45 and .55). The reason behind this is
that the shadow models trained could have shifted decision
boundaries compared to actual models and this affects the result
of the model on borderline queries which causes the attack that
uses the result of extremely borderline queries to be ineffective.
We outline the exact modiﬁed algorithm in Appendix C.
We believe the right way to model the security of property
inference is by giving the adversary the knowledge of the
conditional distribution of training set. This experiments are
designed to show that the knowledge of distribution is not a
strong assumption and adversaries can ﬁnd proxy distributions
and run the attack based on them. This shows that defending the
models by hiding the distribution of samples is not advisable
and the models should be secure even if the adversary knows
the exact conditional distributions.
Relaxing the knowledge of ratios assumption: To test the
effectiveness of the attack for when the adversary does not
know the ratios t0 and t1, we run our attack with ratios t0 = .3
and t1 = .7 and tested the attack on other ratios. Table IV
shows the effectiveness of the attack on ratios other than those
used by the adversary to train the attack. As we expect from
the theoretical analysis in Section V, as long as t0  0.7, the attacks performs almost as good as the case of
t0 = 0.3 and t1 = 0.7. The experiments for the rest of the
cases where either t0 > 0.3 or t1 < 0.7 show that even in
these settings our attack is fairly successful. Note that in these
experiments we use the modiﬁed version of the attack that is
designed to be robust to distribution shift (See Section C).
To show the effectiveness of attack in inferring the ratio with-
out knowing t0 and t1, we also ran another experiment where
instead of training a distinguisher, we train a regression model.
Fig. 2: Number of shadow models v.s. attack accuracy. Our attack
used shadow model training to ﬁnd important queries. This ﬁgure
shows that in all experiments, less than 500 shadow models is enough