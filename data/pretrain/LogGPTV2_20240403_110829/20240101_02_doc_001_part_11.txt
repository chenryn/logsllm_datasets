│ int32 │ date │ double │ double[] │ double │
├──────────┼────────────┼───────────┼──────────────────────────┼──────────┤
│ 10 │ 2019-01-01 │ 2.19 │ [2.19, 5.37, 5.55] │ 5.72│
│ 10 │ 2019-01-02 │ 2.19 │ [4.62, 5.37, 5.55] │ 5.72│
│ 10 │ 2019-01-03 │ 2.19 │ [3.69, 4.62, 5.55] │ 5.72│
│ 10 │ 2019-01-04 │ 2.19 │ [3.69, 5.37, 5.72] │ 5.81│
│ 10 │ 2019-01-05 │ 3.69 │ [4.62, 5.37, 5.72] │ 5.81│
│ · │ · │ · │ · │ · │
│ · │ · │ · │ · │ · │
│ · │ · │ · │ · │ · │
│ 1200 │ 2020-06-22 │ 107.68 │ [149.11, 191.61, 214.68] │ 279.8│
│ 1200 │ 2020-06-23 │ 0.0 │ [107.68, 191.61, 214.68] │ 279.8│
│ 1200 │ 2020-06-24 │ 0.0 │ [190.91, 191.61, 214.68] │ 279.8│
│ 1200 │ 2020-06-25 │ 0.0 │ [191.61, 203.06, 214.68] │ 279.8│
│ 1200 │ 2020-06-26 │ 0.0 │ [0.0, 203.06, 214.68] │ 279.8│
├──────────┴────────────┴───────────┴──────────────────────────┴──────────┤
│1587 rows (10 shown) 5 columns│
└─────────────────────────────────────────────────────────────────────────┘
All aggregates can be used as windowing functions as we already learned. That includes
complex statistical functions, such as computing the exact quantiles in a group
(quantile and quantile_disc) or the interpolated ones (quantile_cont) as shown
above. The implementations of these functions have been optimized for windowing, and
we can use them without worrying about performance. Use a named window when
you’re querying for several aggregates.
4.5.4 Accessing preceding or following rows in a partition
We already discussed ranking and will see an example of computing running totals later
in section Section 48, "Using the ASOF Join", but we haven’t used the ability to jump back
and forth between rows inside a partition. So let’s have a look at computing changes and
what could be a better example these days than prices?
© Manning Publications Co. To comment go to liveBook
90
In chapter 3 we created a table named prices that stores the prices in ct/kWH that
are paid in Germany for feeding back energy to the grid. Those selling prices for
renewable energy have been decreased as the promotion of renewables is getting cut
back. You now want to know how much the compensation for renewable energy changed
over time. For computing a difference you need the price value of row n and compare it
with the value in row n-1. This is not possible without windows as the rows of a table are
processed in isolation, essentially row by row. If you however span a window over any
orderable column you can use lag() and lead() to access rows outside the current
window. This allows you to pick the price from yesterday that you want to compare with
today’s price.
The lag function will give you the value of the expression in the row preceding the
current one within the partition or NULL if there is none. This is the case for the first row
in a partition. lead behaves the other way around (return NULL for the last row in a
partition). Both functions have several overloads in DuckDB that allow not only to specify
the offset of how many rows to lag or lead, but also a default window. Otherwise,
working with coalesce would be an option when NULL values are not practicable.
TIP The coalesce function will return its first non NULL argument.
The query in 4.19 will compute the difference of the prices when new regulations have
been introduced, using lag():
Listing 4.19 Using a window function to compute values based on lagging and leading values
SELECT valid_from,
value,
lag(value) -- #1
OVER validity AS "Previous value",
value - lag(value, 1, value)
OVER validity AS Change -- #2
FROM prices
WHERE date_part('year', valid_from) = 2019
WINDOW validity AS (ORDER BY valid_from)
ORDER BY valid_from;
#1 Jumps back a row and picks out the value column
#2 The change is computed as difference of the price in the current row and the price in the row before that, or the same value if
there is no row before
As you see, in each new period the price decreased considerably in 2019:
© Manning Publications Co. To comment go to liveBook
91
┌────────────┬──────────────┬────────────────┬──────────────┐
│ valid_from │ value │ Previous value │ Change │
│ date │ decimal(5,2) │ decimal(5,2) │ decimal(6,2) │
├────────────┼──────────────┼────────────────┼──────────────┤
│ 2019-01-01 │ 11.47 │ │ 0.00 │
│ 2019-02-01 │ 11.35 │ 11.47 │ -0.12 │
│ 2019-03-01 │ 11.23 │ 11.35 │ -0.12 │
│ 2019-04-01 │ 11.11 │ 11.23 │ -0.12 │
│ · │ · │ · │ · │
│ · │ · │ · │ · │
│ · │ · │ · │ · │
│ 2019-09-01 │ 10.33 │ 10.48 │ -0.15 │
│ 2019-10-01 │ 10.18 │ 10.33 │ -0.15 │
│ 2019-11-01 │ 10.08 │ 10.18 │ -0.10 │
│ 2019-12-01 │ 9.97 │ 10.08 │ -0.11 │
├────────────┴──────────────┴────────────────┴──────────────┤
│ 12 rows (8 shown) 4 columns │
└───────────────────────────────────────────────────────────┘
If we are interested in computing the total change in prices in 2019, we must use a CTE,
as we cannot nest window function calls inside aggregate functions. One possible solution
looks like this:
Listing 4.20 Computing the aggregate over a window
WITH changes AS (
SELECT value - lag(value, 1, value) OVER (ORDER BY valid_from) AS v
FROM prices
WHERE date_part('year', valid_from) = 2019
ORDER BY valid_from
)
SELECT sum(changes.v) AS total_change
FROM changes;
The compensation for privately produced, renewable energy has been cut back by 1.50
ct/kWh in 2019 in Germany.
4.6 Conditions and filtering outside the WHERE clause
Filtering of computed aggregates or the result of a window function cannot be done via
the standard WHERE clause. Such filtering is necessary to answer questions like
Selection of groups that have an aggregated value that exceeds value
x. For this you would have to use the HAVING clause.
© Manning Publications Co. To comment go to liveBook
92
Selection of data that exceeds a certain value in a range of days. Here
the QUALIFY clause must be used.
In addition, you might need to filter out values from entering an aggregate function at
all, using the FILTER clause.
Table 4.1 Filtering clauses and where to use them
Where to use it Effect
Filters rows based on
HAVING After GROUP BY aggregates computed for a
group
After the FROM clause Filters rows based on
QUALIFY referring to any window anything that is computed in
expression that window
After any aggregate Filters the values that are
FILTER
function passed to the aggregate
4.6.1 Using the HAVING clause
"Please give me all the days on which more than 900kWh have been produced!" In
chapter 3 you learned about both the WHERE clause and how GROUP BY works, and you
just try to combine them like this:
SELECT system_id,
date_trunc('day', read_on) AS day,
round(sum(power) / 4 / 1000, 2) AS kWh,
FROM readings
WHERE kWh >= 900
GROUP BY ALL;
In DuckDB 0.8.1 it gives you an error like this: "Error: Binder Error: Referenced column
"kWh" not found in FROM clause!", other versions or databases might be clearer here in
the wording. What it means is this: The computed column "kWh" is not yet known when
the WHERE clause will be applied, and it can’t be known at that point (in contrast to day,
which is a computed column as well). Selecting rows in the WHERE clause, or filtering
rows in other words, modifies what rows get aggregated in the first place. Therefore, you
need another clause that gets applied after aggregation: The HAVING clause. It is used
after the GROUP BY clause to provide a filter criteria after the aggregation of all selected
rows has been completed.
Going back to the initial task: all you have todo is move the condition out of the WHERE
clause into HAVING clause, that follows after the GROUP BY:
© Manning Publications Co. To comment go to liveBook
93
Listing 4.21 Using the HAVING clause to filter rows based on aggregated values
SELECT system_id,
date_trunc('day', read_on) AS day,
round(sum(power) / 4 / 1000, 2) AS kWh,
FROM readings
GROUP BY ALL
HAVING kWh >= 900
ORDER BY kWh DESC;
The results are now filtered after they have been grouped together by the sum
aggregate:
┌───────────┬────────────┬────────┐
│ system_id │ day │ kWh │
│ int32 │ date │ double │
├───────────┼────────────┼────────┤
│ 34 │ 2020-05-12 │ 960.03 │
│ 34 │ 2020-06-08 │ 935.33 │
│ 34 │ 2020-05-23 │ 924.08 │
│ 34 │ 2019-06-09 │ 915.4 │
│ 34 │ 2020-06-06 │ 914.98 │
│ 34 │ 2020-05-20 │ 912.65 │
│ 34 │ 2019-05-01 │ 912.6 │
│ 34 │ 2020-06-16 │ 911.93 │
│ 34 │ 2020-06-07 │ 911.73 │
│ 34 │ 2020-05-18 │ 907.98 │
│ 34 │ 2019-04-10 │ 907.63 │
│ 34 │ 2019-06-22 │ 906.78 │
│ 34 │ 2020-05-19 │ 906.4 │
├───────────┴────────────┴────────┤
4.6.2 Using the QUALIFY clause
Let’s say you want to only return rows where the result of a window function matches
some filter. You can’t add that filter in the WHERE clause, because that would filter out
rows that get included in the window, and you need to use the results of the window
function. However, you also can’t use HAVING, because window functions get evaluated
before an aggregation. So QUALIFY lets you filter on the results of a window function.
When we introduced window functions, we had to use a CTE to filter the results. We
can rewrite the query much more simply and clearly by using QUALIFY, still getting the 3
highest ranked values.
© Manning Publications Co. To comment go to liveBook
94
Listing 4.22 Using the QUALIFY clause to filter rows based on aggregated values in a window
SELECT dense_rank() OVER (ORDER BY power DESC) AS rnk, *
FROM readings
QUALIFY rnk  875 -- #1
ORDER BY system_id, day;
#1 Here’s where we set the threshold
With the example data, we find three dates that represent a typical "good day" of power
production in the western hemisphere for photovoltaics:
┌───────────┬────────────┬──────────────────────────┐
│ system_id │ day │ kWh 7-day moving average │
│ int32 │ date │ double │
├───────────┼────────────┼──────────────────────────┤
│ 34 │ 2020-05-21 │ 887.4628571428572 │
│ 34 │ 2020-05-22 │ 884.7342857142858 │
│ 34 │ 2020-06-09 │ 882.4628571428572 │
└───────────┴────────────┴──────────────────────────┘
© Manning Publications Co. To comment go to liveBook
95
4.6.3 Using the FILTER clause
Sometimes you want to compute an aggregate, an average, or a count of values, and
you realize that there are some rows you don’t want to include. You could add to the
filter clause, but in a complex query, you might need to keep those rows to compute
other fields. For example, let’s say you sometimes got bad readings which show up as
negative values. You want to compute the total number of readings and the average
reading of the sensor. If you filtered out the bad readings in the WHERE clause, you
wouldn’t be able to compute the total number of readings. But if you just do an average
over all the readings, then you’ll be including some of the bad, negative values.
To solve this type of problem, you can use FILTER expressions as part of the
aggregation.
Going back to section Section 41, in which we had to deal with inconsistent sensor
readings, we are actually presented with the very problem of pulling null values into the
average, which is most likely not what we want. Instead of capping null values to zero,
we can filter them out altogether from the average value like this
Listing 4.24 Keeping nonsensical data out of the aggregates
INSERT INTO readings(system_id, read_on, power)
SELECT any_value(SiteId),
time_bucket(
INTERVAL '15 Minutes',
CAST("Date-Time" AS timestamp)
) AS read_on,
coalesce(avg(ac_power)
FILTER (
ac_power IS NOT NULL AND
ac_power >= 0
),0 ) -- #1
FROM
read_csv_auto(
'https://developer.nrel.gov/api/pvdaq/v3/' ||
'data_file?api_key=DEMO_KEY&system_id=10&year=2019'
)
GROUP BY read_on
ORDER BY read_on;
#1 Values that are NULL or less than zero are not included in the average anymore
© Manning Publications Co. To comment go to liveBook
96
You might wonder why we the coalesce function: if all data is filtered out, nothing goes
into the aggregate and the whole expression turns to null. That means if you filter out
all the input from the aggregate, the value turns to NULL and that would violate the
constraint on our reading table. As usual, there is no one right way here whether you
prefer the solution in listing 4.1 or 4.24. In this case we slightly tend towards the FILTER
based solution combined with coalesce because the intention here is slightly clearer.
4.7 The PIVOT statement
You can have many aggregates in one query and all of them can be filtered individually.
This can help you to answer a task like this: "I want a report of the energy production
per system and year, and the years should be columns!" Aggregating the production per
system is easy, and so is aggregating the production per year. Grouping by both keys
isn’t hard either, a statement like this SELECT system_id, year(day), sum(kWh) FROM
v_power_per_day GROUP BY ALL ORDER BY system_id; will do just fine and returns:
┌───────────┬─────────────┬────────────────────┐
│ system_id │ year("day") │ sum(kWh) │
│ int32 │ int64 │ double │
├───────────┼─────────────┼────────────────────┤
│ 10 │ 2019 │ 1549.280000000001 │
│ 10 │ 2020 │ 677.1900000000003 │
│ 34 │ 2019 │ 205742.59999999992 │
│ 34 │ 2020 │ 101033.75000000001 │
│ 1200 │ 2019 │ 62012.109999999986 │
│ 1200 │ 2020 │ 30709.329999999998 │
└───────────┴─────────────┴────────────────────┘
While we did group the data by system and year, the years per system appear in rows,
not in columns. We want 3 rows with 2 columns, 2019 and 2020, containing the values,
pretty much as you would find the above data in a spreadsheet program. The process of
reorganizing such a table is called pivoting and DuckDB offers a couple of possibilities to
do this and one of them is using multiple, filtered aggregates. Instead of having only one
sum aggregate we define several and filter out each value that we don’t want to have for
a specific column and end up with the following statement:
© Manning Publications Co. To comment go to liveBook
97
Listing 4.25 Statically pivoting a result by applying a filter to all aggregates selected
SELECT system_id,
sum(kWh) FILTER (WHERE year(day) = 2019)
AS 'kWh in 2019',
sum(kWh) FILTER (WHERE year(day) = 2020)
AS 'kWh in 2020'