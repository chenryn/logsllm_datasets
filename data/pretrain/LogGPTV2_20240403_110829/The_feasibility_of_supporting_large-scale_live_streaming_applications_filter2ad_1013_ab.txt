address translators (NATs), proxies, or DHCP.
Note that there are many people who join the broadcast before
the event started, perhaps to test that their setups are working. Also,
there are many people who stay after the broadcast is over, for un-
known reasons (they may have just left their media players running).
The 20 kbps audio stream has a sharper drop-off in membership
than the other streams because many users switch from audio to
a better quality stream. The most dynamic segment of the stream
was between 19:00-19:30, where the peak join rate was over 700
joins/second and the peak leave rate was over 70 leaves/second.
Figure 3(b) depicts the cumulative distribution of session dura-
tion time in minutes for the combined streams. There is a sharp rise
at 2 minutes caused by a known client-side NAT/ﬁrewall problem
that forces streaming servers to time out on the connection. The
second curve depicts the session duration distribution without the
incarnations that experienced 2-minute timeouts. The average ses-
sion duration is 22 minutes, with 20% of sessions lasting longer than
30 minutes. The average is dominated by the tail of the distribution.
Except for the tail, the behavior is fairly dynamic. For example, over
55% of sessions are shorter than 5 minutes. Furthermore, 30% of
sessions are shorter than 1 minute. We explore what these numbers
indicate about the stability of the system in Section 4.
Unless otherwise stated, in the remaining sections, we combine
all three streams of this event into one stream, and refer to this one
stream as the “largest stream or event.” We assume that the “de-
sired” encoding for all hosts is 250 kbps.
3. ARE THERE ENOUGH RESOURCES?
In an application end-point architecture, there is no dependence
on costly pre-provisioned infrastructure, making it favorable as an
economical and quickly deployable alternative for streaming appli-
cations. On the other hand, the lack of any supporting infrastructure
requires that application end-points contribute their outgoing band-
width resources. The feasibility of such an architecture depends on
whether or not there are enough bandwidth resources at application
end-points to support all participants at the encoding bit rate.
In this section, we look at the feasibility of supporting a large-
scale live streaming application using only the resources available
at application end-points. To answer this feasibility question, we
run trace-based analysis on the large-scale streams. We ﬁrst need
to estimate the outgoing bandwidth resources at each host. Then,
using these estimates, we derive the amount of resources for each
stream over time. Note that the amount of resources is dependent
on the patterns of joining and leaving of participating hosts.
3.1 Outgoing Bandwidth Estimation
We deﬁne resources as the amount of outgoing bandwidth that
hosts in the system can contribute (i.e., how much bandwidth each
host can send). Next, we describe the methodology that we use
to quickly and accurately estimate the outgoing bandwidth of over
one million IP addresses of hosts that participated in the large-scale
streams.
3.1.1 Bandwidth Data Collection
We use a combination of data mining, inference, and active
measurements for the estimation. While the most accurate method-
Access technology
Packet-pair measurement
Outgoing bandwidth
Dial-up modems
DSL, ISDN, Wireless
Cable modems
Edu, Others
0 kbps (cid:20) BW < 100 kbps
100 kbps (cid:20) BW < 600 kbps
600 kbps (cid:20) BW < 1 Mbps
BW (cid:21) 1 Mbps
estimate
30 kbps
100 kbps
250 kbps
BW
Table 1: Mapping of access technology to outgoing bandwidth.
Type
Free-riders
Contributors
Contributors
Contributors
Contributors
Unknown
Total
Degree-bound
0
1
2
3-19
20
-
-
Number of hosts
58646 (49.3%)
22264 (18.7%)
10033 (8.4%)
6128 (5.2%)
8115 (6.8%)
13735 (11.6%)
118921 (100%)
Table 2: Assigned degree for the largest event.
ology would be to actively measure the bandwidth of all the IP ad-
dresses, it requires signiﬁcant time and resources, and many hosts
do not respond to measurement probes because they ﬁlter packets or
they are off-line. Next, we describe the techniques we use to collect
bandwidth data.
Step 1: As a ﬁrst order ﬁlter, we use “speed test” results from
a popular web site, broadbandreports.com. Speed tests are TCP-
based bandwidth measurements that are conducted from well-provisioned
servers owned by the web site. Users voluntarily go to the web site
to test their connection speeds. Results from the tests are aggre-
gated based on DNS domain names or ISPs and are summarized
over the past week on a publicly available web page. Approxi-
mately 10,000 tests for over 200 ISPs are listed in the summary.
The results include bandwidth measurements in both incoming and
outgoing directions. We use only the outgoing bandwidth values
for bandwidth estimation. We map the IP addresses in our stream-
ing logs to their ISPs using DNS names, and then match the ISPs to
the ones listed at broadbandreports.com. For the 72% of hosts that
matched, we assign their bandwidth values to the ones reported by
broadbandreports.com.
Step 2: Next, we aggregate the remaining IP addresses into
/24 preﬁx blocks and conducted packet-pair measurements to mea-
sure the bottleneck bandwidth to several hosts in each block. Of
the 13,483 preﬁx blocks probed, 7,463 responded. We use the mea-
surement results from the preﬁx blocks to assign bandwidth esti-
mates to an additional 7.6% of the IP addresses, for a total of 79.6%
estimated so far. Note that packet-pair measures the average of
the incoming and outgoing bandwidth (because it relies on round-
trip time measurements). However, many access technologies have
asymmetric bandwidth properties. For example, an ADSL host with
an incoming link of 400 kbps and an outgoing link of 100 kbps has
a packet-pair measurement of 250 kbps. Using the average of the
two directions could over-estimate the outgoing link. Taking this
into account, we map the bandwidth measurements from packet-
pair into the access technology categories listed in Table 1, where
BW stands for the measured bandwidth from using packet-pair. We
use the values in the last column as the outgoing bandwidth esti-
mates.
Step 3: We use EdgeScape, a commercial product provided
by Akamai that maps IP addresses to host information. One of the
ﬁelds in the host information database is access technology. We
compared the overlapping matches from this database to the ones
from broadbandreports.com and found them to be consistent. To
translate access technology into raw bandwidth, we use the values
in the “outgoing bandwidth estimate” column in Table 1. Using this
technique, we are able to assign estimates to an additional 7.1% of
the IP addresses, for a total of 86.7% estimated so far.
Step 4: Finally, we use the host’s DNS name to infer its ac-
cess technology. Our ﬁrst heuristic is based on keywords such as
dsl, cable-modem, dial-up, wireless, .edu and popular cable modem
and DSL service providers such as rr.com and attbi. Using this
technique, we get estimates for an additional 2.2% of IP addresses.
As our second heuristic, we manually construct a database of small
DSL and cable modem providers, corporations, and international
educational institutions that do not use common keywords in their
DNS names. This technique provides estimates for an additional
1.2% of IP addresses. Again, we translate access technology into
bandwidth using the values in Table 1.
Using all 4 steps provides us with bandwidth estimates for 90%
of the IP addresses in the traces. We discuss our treatment of the re-
maining 10% of hosts with unknown estimates later in this section.
3.1.2 Degree-Bound Assignment
To simplify the presentation, we normalize the bandwidth value
by the encoding bit rate. For example, if a host has an outgoing
link bandwidth of 300 kbps and the encoding rate of the stream is
250 kbps, then the normalized value is b300=250c = 1 degee.
Assuming a tree structure for the overlay, this host can have an out-
degree of 1, i.e., it can support one child at the full encoding bit
rate. Throughout this paper, we use “degree” instead of kbps as the
outgoing bandwidth unit.
The degree assignment for the largest broadcast is listed in Ta-
ble 2. Half of the hosts have 0-degree and are labeled as free-riders.
Roughly 39% of the hosts are contributors, capable of supporting
one or more children. Of these, 6.8% are hosts who are capable of
supporting 20 or more children.
The degree assignment derived from the outgoing bandwidth
value reﬂects the inherent capacity of a host. However, all that ca-
pacity may not be available for use. For example, the bandwidth
may be shared by many applications on the same host, or may be
shared across many end-hosts in the case of a shared access link.
Also, users may not wish to contribute all of their bandwidth to
the system. In this paper, we set an absolute maximum bound of
20 for the out-degree (degree cap of 20) such that no host in our
simulations can exceed this limit even if they have more resources
to contribute. This roughly translates to 5 Mbps for video applica-
tions. We also study the effect of more conservative policies such
as degree caps of 4, 6, and 10.
3.1.3 Hosts With Unknown Measurements
For the 10% of IP addresses without bandwidth estimates, we
assign an estimate to them using 3 assignment algorithms. The op-
timistic estimate assumes that all unknowns can contribute up to the
maximum resource allocation (which we set to be degree 20 or less,
depending on the degree cap). This provides an upper-bound on the
best case resource assignment. The pessimistic estimate assumes
that all unknowns are free-riders and contribute no resources. This
provides a lower bound for the worst-case. The distribution algo-
rithm assigns a random value drawn from the same distribution as
the known resources. This algorithm provides reasonable estimates
assuming that the known and unknown resources follow the same
distribution.
3.2 Resource Index
To measure the resource capacity of the system, we use a metric
called Resource Index [7]. The Resource Index is deﬁned as the
ratio of the supply of bandwidth to the demand for bandwidth in the
system for a particular encoding bit rate. The supply is computed
as the sum of all the degrees that the source and application end-
P
P
P
P
Resource Index:
8/3 = 2.7
Figure 4: Example of how to compute the Resource Index.
Optimistic Cap 20
Distribution Cap 20
Pessimistic Cap 20
Optimistic Cap 6
Distribution Cap 6
Pessimistic Cap 6
6
5
4
3
2
1
x
e
d
n
I
e
c
r
u
o
s
e
R
0
17.00
18.00
19.00
20.00
21.00
22.00
Time
Figure 5: Resource Index for largest event.
points participating in the system contribute. Note that the degree is
dependent on the encoding bit rate, and in turn the Resource Index
is also dependent on the encoding bit rate. Demand is computed
as the number of participating end-points. For example, consider
Figure 4, where each host has enough outgoing bandwidth to sustain
2 children. The number of unused slots is 5, and the Resource Index
is 53=3 = 8=3. A Resource Index of 1 indicates that the system
is fully saturated, and a ratio less than 1 indicates that not all the
participating hosts in the broadcast can receive the full encoding
rate. As the Resource Index gets higher, the environment becomes
less constrained and it becomes more feasible to construct a good
overlay tree. A Resource Index of 2 indicates that there are enough
resources to support two times the current number of participants.
3.3 Trace Replay: Single-Tree Protocol
In this section, we use the Resource Index to measure the amount
of resources across a set of 81 streams (all video streams and 5% of
randomly selected audio streams) out of the 660 large-scale streams.
To ensure some conﬁdence in our results, at least 70% of the IP ad-
dresses in a trace must have bandwidth estimates in order for it to
be analyzed.
For each stream, we replay the trace using the group participa-
tion dynamics (joins and leaves) and compute the Resource Index
for each second in the trace. First, we discuss the results for the
largest event, and then we present a summary of the results for the
other large-scale events.
Figure 5 depicts the Resource Index as a function of time, with
degree caps of 6 (bottom 3 lines) and 20 (top 3 lines) children. The
time interval of interest is between 19:00 - 21:00 when the event
was taking place. Again, note that a Resource Index above 1 means
that there are sufﬁcient resources to support the stream using an
application end-point architecture. The highest and lowest curves
for each degree cap policy are computed using optimistic and pes-
simistic bandwidth estimates for unknowns, respectively. Regard-
less of the treatment of hosts with unknown estimates and the de-
gree cap policy, the Resource Index is always above 1 during 19:00
- 21:00. However, a degree cap of 6 places more constraints on the
resources in the system and could potentially make it more difﬁcult
to construct a tree with good performance.
Figure 6 depicts a summary of the other large-scale streams.
The Resource Index for audio streams is depicted in Figure 6(a).
Each point on the x-axis represents an audio stream. The y-axis is
the Resource Index for that stream averaged over the stream dura-
tion. The lowest curve in the ﬁgure is the Resource Index computed
using the pessimistic bandwidth estimate for unknowns. For audio
streams, even the pessimistic estimate is always between 2-3 when
using a degree cap of 4. This is expected because audio is not a
bandwidth-demanding application. The typical encoding rate for
audio is 20 kbps which is low enough for most hosts on the Internet
to support, including dial-up modems. Thus, application end-points
participating in audio streaming applications can provide more than
enough resources to support live streaming.
Figures 6 (b), (c), and (d) depict the average Resource Index
for video streams with degree caps of 6, 10, and 20 children. As
the degree cap increases, the Resource Index increases. The top
most curve in all 3 ﬁgures represents the optimistic estimate for un-
knowns. In the most optimistic view, across all degree cap policies
(6,10, and 20), only one stream had a Resource Index below one. In
the worst case scenario, where the degree cap is 6 and the unknown
assignment policy is pessimistic, roughly a third of video streams
had Resource Index below 1.
In order to determine feasibility, we look at the inherent amount
of resources in the system. Using a degree cap of 20 and the distribution-
based degree assignment for unknowns as depicted in Figure 6(d),
we ﬁnd that only 1 stream has a Resource Index of less than 1. The
stream with the worst Resource Index (labelled 40 on the x-axis)
had an encoding rate of 300 kbps, but was composed almost ex-
clusively (96%) of home broadband users (DSL and cable modem).
Many home broadband connections can only support 100-250 kbps
of outgoing bandwidth, which is less than the encoding bit rate.
Therefore, such hosts did not contribute any resources to the system
at all.
To better understand whether or not this composition of hosts is
common, we look at the nature of the event. This is a short duration
stream, starting on Sunday night at 11pm and ending at 2am in local
time, where local time is determined based on the geographic loca-
tion of the largest group of hosts. Section 5 gives an overview of
how geographic location is determined. Most participants are home
users because the event took place on a weekend night when people
are most likely to be at home. About 5 of the 55 large-scale video
streams have this behavior. Their Resource Index is close to 1 for
the distribution-based degree assignment in Figure 6(d). In contrast,
most of the other streams take place during the day, and are often
accessed from more heterogeneous locations with potentially more
bandwidth resources, such as from school or the workplace.
To summarize the results in this section, we ﬁnd that there are
more than sufﬁcient bandwidth resources amongst application end-
points to support audio streaming.
In addition, there are enough
inherent resources in the system at scales of 1000 or more simulta-
neous hosts to support video streaming in over 90% of the common
scenarios. This indicates that using application end-point architec-
tures for live streaming is feasible. While we have shown that there
are inherent resources, we wish to point out that designing poli-
cies and mechanisms to encourage participants to contribute their
resources is beyond the scope of this paper. We have looked at sim-
x
e
d
n
I
e
c
r
u
o
s
e
R
x
e
d
n
I
e
c
r
u
o
s
e
R
5
4
3
2
1
0
5
4
3
2
1
0
Optimistic
Distribution
Pessimistic
0
5
10
15
20
25
30
Stream Sorted By Optimistic Resource Index
(a) Audio streams, Degree Cap 4
Optimistic
Distribution
Pessimistic
0
10
20
30
40