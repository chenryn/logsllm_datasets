3. Rodriguez-M,P.,Pedrinaci,C.,Lama,M.,Mucientes,M.:Anintegratedsemantic
web service discovery and composition framework. IEEE Trans. Serv. Comput. 9,
537–550 (2016)
4. Masmoudi, M., et al.: PREDICAT: a semantic service-oriented platform for data
interoperability and linking in earth observation and disaster prediction. In: Con-
ference on Service-Oriented Computing Applications (SOCA), Paris, pp. 194–201
(2018)
5. Bartalos,P.,Bielikova,M.:Automaticdynamicwebservicecomposition:asurvey
and problem formalization. Comput. Inf. 30(4), 793–827 (2011)
6. Taktak, H., Boukadi, K., Mrissa, M., Ghedira, C., Gargouri, F.: A model-driven
approach for semantic data-as-a-service generation. In: IEEE International Con-
ference on Enabling Technologies: Infrastructure for Collaborative Enterprises -
WETICE, France (2020)
7. RobinGenuer,R.,Poggi,J.M.,Tuleau-Malot,C.:Variableselectionusingrandom
forests. J. Pattern Recognit. Lett. 31, 2225–2236 (2010)
8. Bansal,S.,Bansal,A.,Gupta,G.,Blake,M.B.:GeneralizedsemanticWebservice
composition. J. Serv. Oriented Comput. Appl. 10, 111–133 (2016)
9. Giglio, L., Boschetti, L., Roy, D.P., Humber, M.L., Justice, C.O.: The collection
6MODIS burned area mapping algorithm and product. Remote Sens. Environ.
217, 72–85 (2018)
10. Gupta, I.K., Kumar, J., Rai, P.: Optimization to quality-of-service-driven web
servicecompositionusingmodifiedgeneticalgorithm.In:InternationalConference
on Computer, Communication and Control, pp. 1–6 (2015)
11. Ma,H.,Wang,A.,Zhang,M.:Ahybridapproachusinggeneticprogrammingand
greedysearchforQoS-awarewebservicecomposition.In:Hameurlain,A.,Ku¨ng,J.,
Wagner, R., Decker, H., Lhotska, L., Link, S. (eds.) Transactions on Large-Scale
Data- and Knowledge-Centered Systems XVIII. LNCS, vol. 8980, pp. 180–205.
Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-46485-4 7
12. Sawczuk da Silva, A., Mei, Y., Ma, H., Zhang, M.: Particle swarm optimisation
withsequence-likeindirectrepresentationforwebservicecomposition.In:Chicano,
F.,Hu,B.,Garc´ıa-S´anchez,P.(eds.)EvoCOP2016.LNCS,vol.9595,pp.202–218.
Springer, Cham (2016). https://doi.org/10.1007/978-3-319-30698-8 14
13. Yu, Y., Ma, H., Zhang, M.: An adaptive genetic programming approach to
QoSaware web services composition. In: 2013 IEEE Congress on Evolutionary
Computation, pp. 1740–1747 (2013)
14. Petrie, C.J.: Web Service Composition. Springer, Cham (2016). https://doi.org/
10.1007/978-3-319-32833-1
420 H. Taktak et al.
15. Murthy, S.K.: Automatic construction of decision trees from data: a multi-
disciplinary survey. J. Data Min. Knowl. Disc. 2, 345–389 (1998)
16. Breiman, L.: Random forests. J. Mach. Learn. 45, 5–32 (2001)
17. Sharples,J.J.,McRae,R.H.D.,Weber,R.O.,Gill,A.M.:Asimpleindexforassess-
ing fire danger rating. Environ. Model. Softw. 24, 764–774 (2009)
18. Urbieta, A., Gonz´alez-B, A., Mokhtar, S., Hossain, M., Capra, L.: Adaptive and
context-awareservicecompositionforIoT-basedsmartcities.FutureGener.Com-
put. Syst. 76, 262–274 (2017)
19. Deng, S., Xiang, Z., Yin, J., Taheri, J., Zomaya, A.Y.: Composition-driven IoT
service provisioning in distributed edges. IEEE Access 6, 54258–54269 (2018)
20. Asghari, P., Rahmani, A., Javadi, H.H.S.: Service composition approaches in IoT:
a systematic review. J. Netw. Comput. Appl. 120, 61–77 (2018)
21. Chen, G., Huang, J., Cheng, B., Chen, J.: A social network based approach for
IoT device management and service composition. In: IEEE World Congress on
Services, pp. 1–8 (2015)
22. Yang,R.,Li,B.,Cheng,C.:Adaptableservicecompositionforintelligentlogistics:
a middleware approach. In: Conference on Cloud Computing and Big Data, pp.
75–82 (2015)
Eyewitness Prediction During Crisis
via Linguistic Features
B
Suliman Aladhadh( )
Department of Information Technology, College of Computer,
Qassim University, Buraydah, Saudi Arabia
PI:EMAIL
Abstract. Social media is one of the first places people share informa-
tion about serious topics, such as a crisis event. Stakeholders, including
theagenciesofcrisisresponse,seektounderstandthisvaluableinforma-
tioninordertoreachaffectedpeople.Thispaperaddressestheproblem
of locating eyewitnesses during times of crisis. We included published
tweets of 26 crises of various types, including earthquakes, floods, train
crashes,andothers.Thispaperinvestigatedtheimpactoflinguisticfea-
turesextractedfromtweetsondifferentlearningalgorithmsandincluded
twolanguages,EnglishandItalian.Betterresultsthanthestateoftheart
wereachieved;inthecross-eventscenario,weachievedF1-scoresof0.88
forEnglishand0.86forItalian;inthesplit-acrossscenario,weachieved
F1-scores of 0.69 for English and 0.89 for Italian.
· · ·
Keywords: Machine learning Text mining Social media
Eyewitness
1 Introduction
During such an event people often share information about that event on social
media, making these platforms the first alarm for significant events. This is
particularly true for crisis, such as earthquake, flood, pandemic, etc. On social
mediausersareclassifiedbasedontheirlocation(i.e.closeorfarfromtheevent
location), users posting information on a particular event who live in or close
to the event area are known as “eyewitnesses”. Eyewitnesses are important as
they are able to provide first-hand information about that event. Geo-location
informationinsocialmediaisthemostdirectwaytolocateeyewitness.However,
this information is rare on social media platforms (e.g. <1% in Twitter) so
alternative methods are needed to find them.
Little research has been conducted into defining eyewitness in social media,
even though such research is important in critical times such as crisis events,
includingpandemiclikecoronavirusdisease[6].Agenciesofdisasterresponsehave
begantoincludesocialmediaasimportantsourcesofinformationtoreachaffected
people,andtheseresearchaimtolocateauthorsfromthesameplaceoftheevent.
However,eyewitnessesareveryrareinsocialmedia,forexampleamongtweetsof
26crisiseventsonly8%werelabeledasbeingfromaneyewitness[10].
(cid:2)c SpringerNatureSwitzerlandAG2021
H.Hacidetal.(Eds.):ICSOC2020Workshops,LNCS12632,pp.421–429,2021.
https://doi.org/10.1007/978-3-030-76352-7_39
422 S. Aladhadh
Different methodologies have been used for defining eyewitnesses, including
semantic, source-based features, user-based features, networking features, and
other metadata such as number of Retweet, URL, etc. In this study we aimed
to investigate the impact of using a language features approach to improve the
performance of machine learning model, by employing Linguistic Inquiry and
WordCount(LIWC).Linguisticfeatureshavepreviouslybeenfoundtoimprove
performance, as detailed in the next section.
The research question of this study is:
– How do linguistic features improve detection of eyewitnesses during crisis?
2 Related Work
Defining the eyewitness during crisis is challenging as less than 1% of tweets are
geo-tagged. Many researchers have tried to find alternative ways to reach users
fromtheground.Morstatteretal.[8]aimedtodifferentiatebetweentweetsfrom
the affected and different locations. They built a model to predict tweets of an
affectedlocation(eyewitnesses)andcollectedgeo-taggedtweetsfromtheUnited
States (US) only related to two crisis event in the US (2013 Boston Marathon
Bombing and2012 HurricaneSandy). Theauthors classifiedthetweets into two
types (inside and outside region) and employed Naive Bayes to train a machine
learning model to predict tweet type. The model was built using a number
of linguistics features, including unigrams & bigrams, Part of Speech (POS)
and shallow parsing to identify the tweet semantics (e.g. verbs categories and
name entities). The accuracy of predicting tweets as inside or outside affected
region was 0.831 for the Boston bombing and 0.882 for Hurricane Sandy. This
model relied only on linguistic features and was able to achieve good results.
However, the study had a number of limitations. For example, they included
only geo-tagged tweets but the reality is that most eyewitness in social media
are not geo-tagged, in fact this is very rare. In addition, only two events were
included,bothfromthesamecountryandtheeventsbelongedtodifferentcrisis
types, while in the real scenario a model performance differ when apply for
different crisis types. Moreover, Language differences have a large influence on
user location prediction in social media [4], yet this study only included English
language. These limitations make it hard to generalise findings as the following
research has shown [13].
Tanev et al. [13] trained a model to predict eyewitness during crisis, these
crisis related to different types such as floods, wildfires, earthquakes, etc. where
occurred in different countries. Available data from 26 crisis tweets were anno-
tated as eyewitness or not. English and Italian languages were included to mea-
sure the influence of language on model accuracy, and the authors used a set
of language features such as lexical, stylistic and word capitalisation and tweet
features such as hashtag, mention, etc. They compared performance of three
different classifiers: Naive Bayes, SVM and Random Forest. In the scenario of
trainingandtesting(classical)ofthesameeventtypes,NaiveBayesachievedthe
best result for Italian language, with an accuracy of 0.69, and Random Forest
Eyewitness Prediction During Crisis via Linguistic Features 423
was the best for English at 0.79. However, in the realistic (harder) scenario the
results were poor with 0.19 for English and 0.38 for Italian.
Pekaretal.[12]usedthesamedatausedbyTanevetal.[13]andtrainedfour
different classifiers using mixed linguistic (e.g. lexical, grammatical, semantic,
etc.) and other metadata features (e.g. hashtags, mentions, retweet, etc.). In
both scenarios (classical and realistic), results were poor and performance was
very low. In the first scenario, the best result was 0.40 with Random Forest and
in the second scenario, was <0.10 with SVM.
Pekar et al. [11] predicted the information types related to crisis events,
including eyewitnesses. They used six different features to train their models,
including unigrams, bigrams, POS, hashtags, RT and URL counts. The authors
classifiedthesefeaturesintotwodifferentviews:lexicalfeaturesandgrammatical
features with metadata, and used SVM and Maximum Entropy algorithms for
the classification process. The best results achieved was around 0.25 with SVM
classifierviathesecondview(grammaticalfeatureswithmetadata).Theresults
indicated a very low accuracy.
In the aforementioned research, we observed that when a small number of
featuresrelatedtolanguagewasaddedtothemodels,performanceincreased.So,
we suppose that inclusion of more linguistic features will improve the existing
models and result in much higher performance.
3 Methodology
We used the linguistic features of semantic or stylistic as the indicator for eye-
witness detection. This is for many reasons, including:
– Thelinguisticfeaturesoftweetshavebeenfoundtobeanimportantpredictor
for eyewitness detection in social media [8].
– The language of the tweets is influenced by user location [3,4].
– The text is available in all tweets, whereas other features may be absent.
– The influence of linguistic features have prominent affect on precision and
recall [12].
Linguistic Inquiry and Word Count (LIWC): In order to study the influ-
enceofonlylinguisticfeaturesoneyewitnesspredictioninsocialmedia,weneed
to generate a large number of these features. We used LIWC, which is a tool for
semantic analysis of text that counts words of different psychological categories
[14]. Its dictionary of categories include almost 6,400 words and word stems
for English and other languages, including Italian. Use of LIWC is common in
social media data analyses in different areas such as demographics [9], during
crisis [7,15].
The categories we used are listed in Table1 and have been used in previous
research.Allcategories,exceptthelastoneinthetable,havesubcategories.For
example,affectfeatureisamaincategory,whichincludestwosubcategories:pos-
itive and negative emotions; negative emotion includes three subsub categories:
424 S. Aladhadh
Table 1. The LIWC general categories used to perform analyses on tweet content.
Feature Example
Function words it, to, no, very
Affect words happy, cried
Social words mate, talk, they
Cognitive processes(cogproc) cause, know, ought
Perceptual processes (percept) look, heard, feeling
Biological processes (bio) eat, blood, pain
Core drives and Needs (Drives) ally, win, superior
Relativity (relativ) area, bend, exit
Informal language (informal) damn, btw, umm
Authentic, Pronoun, Word count(WC), Qmark, Exclam
anxiety, anger, and sadness. In total there are 93 LIWC features. The full list of
both general and sub-categories are available online.1
Bigram: In addition to LIWC the 93 features, we included bigram as an extra
feature, as it can greatly increase the performance of previous models [8,12].
3.1 Dataset
Inthisstudy,weusedlabeleddatafor26differenttypesofcrisisevents,including
earthquakes, train crashes and floods. The data is available for research and
called CrisisLexT26 [10]. The crisis events occurred between 2012 and 2013,
and included the 2012 Italian earthquake, 2013 Queensland floods and 2013
Australia bushfire. English and Italian tweets were included and identified by
self identification tool language filtering [5].
In total, there were 24,589 labelled tweets, with 2,193 authored by eyewit-
nesses.
Since the data was unbalanced between eyewitness and non-eyewitness
tweets, we created a balanced version between the two by under-sampling non-
eyewitness tweets (negative) (50/0) randomly before of training and testing, as
per the methods of [2,13]. We located 2811 English tweets and 1575 Italian
tweets.Thesizeofourdatawasmuchbiggerthanthedatausedin[13],astheir
model needs an extra meta data of the tweets from Twitter, and many of these
tweets do not exist anymore. They therefore needed to exclude a large number
of eyewitness tweets.
1 http://liwc.wpengine.com.
Eyewitness Prediction During Crisis via Linguistic Features 425
4 Experiment
We use four different algorithms to compare performance, including: Random
Forest(RF),k-NearestNeighbors(KNN),NaiveBayes(NB)andSupportVector
Machine(SVM).Weusedthesefourclassifiersastheyachievedthebestresultsin
previous studies, and to enable comparison of our results with previous research
(Tables2 and 3). We used ≈ in [11,12] because they did not give the absolute
number in their papers.
There are two common scenarios to evaluate the classification accuracy of
our model. In the first scenario, all datasets were randomly split into training
and testing sets in a 10 cross folds validation. This scenario ensures that the
features distribution is same in training and testing. Table2 presents the results
of the first scenario from previous research. This scenario is called cross-event.
The second scenario was designed to reflect the reality, i.e. that the training
and testing datasets are different. The tweets were split in a way that meant
the crisis types included in the training dataset were different to those in the
testing dataset. This scenario, called split-across, is well know as the harder one
in which to achieve good results. Table3 reports the results of previous studies
thatincludedsplit-acrossscenarios.Inthetestingphase,weappliedthisscenario
on three different crisis types: flood, train crash and earthquake. This allowed
us to observe the impact of crisis type on performance.
Table 2. Results of the previous studies used CrisisLexT26 in the first scenario
Usedby Evaluationmetrics RF KNN NB SVM MaxEnt
Pekaretal.[12] Precision ≈0.60 ≈0.60 ≈0.80 ≈0.60 ≈0.60
Recall ≈0.20 <0.10 <0.10 ≈0.20 ≈0.10
F1 0.40 <0.10 <0.10 ≈0.30 ≈0.20
Pekaretal.[11] Precision – – – ≈0.40 ≈0.30
Recall – – – <0.20 ≈0.10
F1 – – – 0.20 ≈0.20
Tanevetal.[13]English Precision 0.81 – 0.70 0.80 –
Recall 0.78 – 0.84 0.75 –
F1 0.79 – 0.77 0.77 –
Tanevetal.[13]Italian Precision 0.57 – 0.64 0.58 –
Recall 0.83 – 0.75 0.65 –
F1 0.68 – 0.70 0.61 –