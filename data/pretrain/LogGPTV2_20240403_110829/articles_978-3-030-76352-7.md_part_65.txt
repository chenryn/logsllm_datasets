### References

1. Rodriguez-M, P., Pedrinaci, C., Lama, M., Mucientes, M.: An Integrated Semantic Web Service Discovery and Composition Framework. *IEEE Trans. Serv. Comput.* 9, 537–550 (2016).

2. Masmoudi, M., et al.: PREDICAT: A Semantic Service-Oriented Platform for Data Interoperability and Linking in Earth Observation and Disaster Prediction. In: Conference on Service-Oriented Computing Applications (SOCA), Paris, pp. 194–201 (2018).

3. Bartalos, P., Bielikova, M.: Automatic Dynamic Web Service Composition: A Survey and Problem Formalization. *Comput. Inf.* 30(4), 793–827 (2011).

4. Taktak, H., Boukadi, K., Mrissa, M., Ghedira, C., Gargouri, F.: A Model-Driven Approach for Semantic Data-as-a-Service Generation. In: IEEE International Conference on Enabling Technologies: Infrastructure for Collaborative Enterprises - WETICE, France (2020).

5. Genuer, R., Poggi, J.M., Tuleau-Malot, C.: Variable Selection Using Random Forests. *J. Pattern Recognit. Lett.* 31, 2225–2236 (2010).

6. Bansal, S., Bansal, A., Gupta, G., Blake, M.B.: Generalized Semantic Web Service Composition. *J. Serv. Oriented Comput. Appl.* 10, 111–133 (2016).

7. Giglio, L., Boschetti, L., Roy, D.P., Humber, M.L., Justice, C.O.: The Collection 6 MODIS Burned Area Mapping Algorithm and Product. *Remote Sens. Environ.* 217, 72–85 (2018).

8. Gupta, I.K., Kumar, J., Rai, P.: Optimization to Quality-of-Service-Driven Web Service Composition Using Modified Genetic Algorithm. In: International Conference on Computer, Communication and Control, pp. 1–6 (2015).

9. Ma, H., Wang, A., Zhang, M.: A Hybrid Approach Using Genetic Programming and Greedy Search for QoS-Aware Web Service Composition. In: Hameurlain, A., Küng, J., Wagner, R., Decker, H., Lhotska, L., Link, S. (eds.) Transactions on Large-Scale Data- and Knowledge-Centered Systems XVIII. LNCS, vol. 8980, pp. 180–205. Springer, Heidelberg (2015). https://doi.org/10.1007/978-3-662-46485-4_7

10. Sawczuk da Silva, A., Mei, Y., Ma, H., Zhang, M.: Particle Swarm Optimisation with Sequence-Like Indirect Representation for Web Service Composition. In: Chicano, F., Hu, B., García-Sánchez, P. (eds.) EvoCOP 2016. LNCS, vol. 9595, pp. 202–218. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-30698-8_14

11. Yu, Y., Ma, H., Zhang, M.: An Adaptive Genetic Programming Approach to QoS-Aware Web Services Composition. In: 2013 IEEE Congress on Evolutionary Computation, pp. 1740–1747 (2013).

12. Petrie, C.J.: Web Service Composition. Springer, Cham (2016). https://doi.org/10.1007/978-3-319-32833-1

13. Murthy, S.K.: Automatic Construction of Decision Trees from Data: A Multi-Disciplinary Survey. *J. Data Min. Knowl. Disc.* 2, 345–389 (1998).

14. Breiman, L.: Random Forests. *J. Mach. Learn.* 45, 5–32 (2001).

15. Sharples, J.J., McRae, R.H.D., Weber, R.O., Gill, A.M.: A Simple Index for Assessing Fire Danger Rating. *Environ. Model. Softw.* 24, 764–774 (2009).

16. Urbieta, A., González-B, A., Mokhtar, S., Hossain, M., Capra, L.: Adaptive and Context-Aware Service Composition for IoT-Based Smart Cities. *Future Gener. Comput. Syst.* 76, 262–274 (2017).

17. Deng, S., Xiang, Z., Yin, J., Taheri, J., Zomaya, A.Y.: Composition-Driven IoT Service Provisioning in Distributed Edges. *IEEE Access* 6, 54258–54269 (2018).

18. Asghari, P., Rahmani, A., Javadi, H.H.S.: Service Composition Approaches in IoT: A Systematic Review. *J. Netw. Comput. Appl.* 120, 61–77 (2018).

19. Chen, G., Huang, J., Cheng, B., Chen, J.: A Social Network Based Approach for IoT Device Management and Service Composition. In: IEEE World Congress on Services, pp. 1–8 (2015).

20. Yang, R., Li, B., Cheng, C.: Adaptable Service Composition for Intelligent Logistics: A Middleware Approach. In: Conference on Cloud Computing and Big Data, pp. 75–82 (2015).

### Eyewitness Prediction During Crisis via Linguistic Features

**Suliman Aladhadh**

*Department of Information Technology, College of Computer, Qassim University, Buraydah, Saudi Arabia*

**Abstract:**
Social media is often the first platform where people share information about serious events, such as crises. Stakeholders, including crisis response agencies, seek to understand this valuable information to reach affected individuals. This paper addresses the problem of locating eyewitnesses during times of crisis. We analyzed published tweets from 26 different types of crises, including earthquakes, floods, train crashes, and others. The study investigated the impact of linguistic features extracted from tweets on various learning algorithms and included two languages: English and Italian. Our results outperformed the state of the art; in the cross-event scenario, we achieved F1-scores of 0.88 for English and 0.86 for Italian. In the split-across scenario, we achieved F1-scores of 0.69 for English and 0.89 for Italian.

**Keywords:** Machine learning, Text mining, Social media, Eyewitness

### 1. Introduction
During a crisis, people often share information about the event on social media, making these platforms the first alarm for significant events. This is particularly true for crises such as earthquakes, floods, and pandemics. On social media, users are classified based on their location (i.e., close or far from the event location). Users who post information about an event and live in or near the event area are known as "eyewitnesses." Eyewitnesses are crucial as they can provide first-hand information about the event. Geo-location information on social media is the most direct way to locate eyewitnesses. However, this information is rare on social media platforms (e.g., <1% on Twitter), so alternative methods are needed to find them.

Little research has been conducted into defining eyewitnesses in social media, even though such research is important during critical times like crisis events, including pandemics like the coronavirus disease [6]. Disaster response agencies have begun to include social media as important sources of information to reach affected people, and these studies aim to locate authors from the same place as the event. However, eyewitnesses are very rare on social media; for example, among the tweets of 26 crisis events, only 8% were labeled as being from an eyewitness [10].

Different methodologies have been used to define eyewitnesses, including semantic, source-based features, user-based features, networking features, and other metadata such as the number of retweets, URLs, etc. In this study, we aimed to investigate the impact of using a language features approach to improve the performance of machine learning models by employing Linguistic Inquiry and Word Count (LIWC). Linguistic features have previously been found to improve performance, as detailed in the next section.

The research question of this study is:
- How do linguistic features improve the detection of eyewitnesses during a crisis?

### 2. Related Work
Defining eyewitnesses during a crisis is challenging because less than 1% of tweets are geo-tagged. Many researchers have tried to find alternative ways to reach users on the ground. Morstatter et al. [8] aimed to differentiate between tweets from affected and different locations. They built a model to predict tweets from an affected location (eyewitnesses) and collected geo-tagged tweets from the United States (US) related to two crisis events in the US (the 2013 Boston Marathon Bombing and the 2012 Hurricane Sandy). The authors classified the tweets into two types (inside and outside the region) and employed Naive Bayes to train a machine learning model to predict tweet type. The model was built using several linguistic features, including unigrams & bigrams, Part of Speech (POS), and shallow parsing to identify the tweet semantics (e.g., verb categories and named entities). The accuracy of predicting tweets as inside or outside the affected region was 0.831 for the Boston bombing and 0.882 for Hurricane Sandy. This model relied solely on linguistic features and achieved good results. However, the study had several limitations. For example, it included only geo-tagged tweets, but most eyewitnesses on social media are not geo-tagged. Additionally, only two events were included, both from the same country and belonging to different crisis types, while in reality, a model's performance can differ when applied to different crisis types. Moreover, language differences significantly influence user location prediction in social media [4], yet this study only included the English language. These limitations make it hard to generalize findings, as subsequent research has shown [13].

Tanev et al. [13] trained a model to predict eyewitnesses during crises, including different types such as floods, wildfires, and earthquakes, occurring in different countries. Available data from 26 crisis tweets were annotated as eyewitness or not. English and Italian languages were included to measure the influence of language on model accuracy. The authors used a set of language features such as lexical, stylistic, and word capitalization, and tweet features such as hashtags, mentions, etc. They compared the performance of three different classifiers: Naive Bayes, SVM, and Random Forest. In the scenario of training and testing (classical) of the same event types, Naive Bayes achieved the best result for Italian with an accuracy of 0.69, and Random Forest was the best for English at 0.79. However, in the realistic (harder) scenario, the results were poor, with 0.19 for English and 0.38 for Italian.

Pekar et al. [12] used the same data as Tanev et al. [13] and trained four different classifiers using mixed linguistic (e.g., lexical, grammatical, semantic, etc.) and other metadata features (e.g., hashtags, mentions, retweets, etc.). In both scenarios (classical and realistic), the results were poor, and performance was very low. In the first scenario, the best result was 0.40 with Random Forest, and in the second scenario, it was <0.10 with SVM.

Pekar et al. [11] predicted the information types related to crisis events, including eyewitnesses. They used six different features to train their models, including unigrams, bigrams, POS, hashtags, RT, and URL counts. The authors classified these features into two different views: lexical features and grammatical features with metadata, and used SVM and Maximum Entropy algorithms for the classification process. The best results achieved were around 0.25 with the SVM classifier via the second view (grammatical features with metadata). The results indicated very low accuracy.

In the aforementioned research, we observed that when a small number of language-related features were added to the models, performance increased. Therefore, we hypothesize that the inclusion of more linguistic features will improve existing models and result in much higher performance.

### 3. Methodology
We used the linguistic features of semantic or stylistic as indicators for eyewitness detection. This is for several reasons, including:
- The linguistic features of tweets have been found to be an important predictor for eyewitness detection in social media [8].
- The language of the tweets is influenced by the user's location [3, 4].
- The text is available in all tweets, whereas other features may be absent.
- The influence of linguistic features has a prominent effect on precision and recall [12].

**Linguistic Inquiry and Word Count (LIWC):** To study the influence of only linguistic features on eyewitness prediction in social media, we need to generate a large number of these features. We used LIWC, a tool for semantic analysis of text that counts words of different psychological categories [14]. Its dictionary of categories includes almost 6,400 words and word stems for English and other languages, including Italian. LIWC is commonly used in social media data analyses in various areas, such as demographics [9] and during crises [7, 15].

The categories we used are listed in Table 1 and have been used in previous research. All categories, except the last one in the table, have subcategories. For example, the affect feature is a main category that includes two subcategories: positive and negative emotions. Negative emotion includes three sub-subcategories: anxiety, anger, and sadness. In total, there are 93 LIWC features. The full list of both general and sub-categories is available online [1].

**Bigram:** In addition to the 93 LIWC features, we included bigrams as an extra feature, as they can greatly increase the performance of previous models [8, 12].

**3.1. Dataset**
In this study, we used labeled data for 26 different types of crisis events, including earthquakes, train crashes, and floods. The data is available for research and is called CrisisLexT26 [10]. The crisis events occurred between 2012 and 2013 and included the 2012 Italian earthquake, the 2013 Queensland floods, and the 2013 Australia bushfire. English and Italian tweets were included and identified by self-identification tool language filtering [5].

In total, there were 24,589 labeled tweets, with 2,193 authored by eyewitnesses. Since the data was unbalanced between eyewitness and non-eyewitness tweets, we created a balanced version between the two by under-sampling non-eyewitness tweets (negative) (50/50) randomly before training and testing, as per the methods of [2, 13]. We located 2,811 English tweets and 1,575 Italian tweets. The size of our data was much larger than the data used in [13], as their model required extra metadata from the tweets, and many of these tweets no longer exist. Therefore, they needed to exclude a large number of eyewitness tweets.

**Table 1.** The LIWC general categories used to perform analyses on tweet content.

| Feature | Example |
|---------|---------|
| Function words | it, to, no, very |
| Affect words | happy, cried |
| Social words | mate, talk, they |
| Cognitive processes (cogproc) | cause, know, ought |
| Perceptual processes (percept) | look, heard, feeling |
| Biological processes (bio) | eat, blood, pain |
| Core drives and Needs (Drives) | ally, win, superior |
| Relativity (relativ) | area, bend, exit |
| Informal language (informal) | damn, btw, umm |
| Authentic, Pronoun, Word count (WC), Qmark, Exclam |

### 4. Experiment
We used four different algorithms to compare performance, including: Random Forest (RF), k-Nearest Neighbors (KNN), Naive Bayes (NB), and Support Vector Machine (SVM). We chose these four classifiers because they achieved the best results in previous studies and to enable comparison of our results with previous research (Tables 2 and 3). We used ≈ in [11, 12] because they did not provide the absolute numbers in their papers.

There are two common scenarios to evaluate the classification accuracy of our model. In the first scenario, all datasets were randomly split into training and testing sets in a 10-fold cross-validation. This scenario ensures that the feature distribution is the same in training and testing. Table 2 presents the results of the first scenario from previous research. This scenario is called cross-event.

The second scenario was designed to reflect reality, i.e., that the training and testing datasets are different. The tweets were split in a way that meant the crisis types included in the training dataset were different from those in the testing dataset. This scenario, called split-across, is well known as the harder one in which to achieve good results. Table 3 reports the results of previous studies that included split-across scenarios. In the testing phase, we applied this scenario to three different crisis types: flood, train crash, and earthquake. This allowed us to observe the impact of crisis type on performance.

**Table 2.** Results of the previous studies using CrisisLexT26 in the first scenario.

| Used by | Evaluation Metrics | RF | KNN | NB | SVM | MaxEnt |
|---------|-------------------|----|-----|----|-----|--------|
| Pekar et al. [12] | Precision | ≈0.60 | ≈0.60 | ≈0.80 | ≈0.60 | ≈0.60 |
| | Recall | ≈0.20 | <0.10 | <0.10 | ≈0.20 | ≈0.10 |
| | F1 | 0.40 | <0.10 | <0.10 | ≈0.30 | ≈0.20 |
| Pekar et al. [11] | Precision | – | – | – | ≈0.40 | ≈0.30 |
| | Recall | – | – | – | <0.20 | ≈0.10 |
| | F1 | – | – | – | 0.20 | ≈0.20 |
| Tanev et al. [13] (English) | Precision | 0.81 | – | 0.70 | 0.80 | – |
| | Recall | 0.78 | – | 0.84 | 0.75 | – |
| | F1 | 0.79 | – | 0.77 | 0.77 | – |
| Tanev et al. [13] (Italian) | Precision | 0.57 | – | 0.64 | 0.58 | – |
| | Recall | 0.83 | – | 0.75 | 0.65 | – |
| | F1 | 0.68 | – | 0.70 | 0.61 | – |

**Table 3.** Results of the previous studies using CrisisLexT26 in the split-across scenario.

| Used by | Evaluation Metrics | RF | KNN | NB | SVM | MaxEnt |
|---------|-------------------|----|-----|----|-----|--------|
| Pekar et al. [12] | Precision | ≈0.60 | ≈0.60 | ≈0.80 | ≈0.60 | ≈0.60 |
| | Recall | ≈0.20 | <0.10 | <0.10 | ≈0.20 | ≈0.10 |
| | F1 | 0.40 | <0.10 | <0.10 | ≈0.30 | ≈0.20 |
| Pekar et al. [11] | Precision | – | – | – | ≈0.40 | ≈0.30 |
| | Recall | – | – | – | <0.20 | ≈0.10 |
| | F1 | – | – | – | 0.20 | ≈0.20 |
| Tanev et al. [13] (English) | Precision | 0.81 | – | 0.70 | 0.80 | – |
| | Recall | 0.78 | – | 0.84 | 0.75 | – |
| | F1 | 0.79 | – | 0.77 | 0.77 | – |
| Tanev et al. [13] (Italian) | Precision | 0.57 | – | 0.64 | 0.58 | – |
| | Recall | 0.83 | – | 0.75 | 0.65 | – |
| | F1 | 0.68 | – | 0.70 | 0.61 | – |