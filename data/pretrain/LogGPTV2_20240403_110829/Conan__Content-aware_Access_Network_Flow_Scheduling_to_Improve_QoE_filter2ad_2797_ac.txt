π
ρπ (s),
∀s ∈ S.
(8)
Generally, although the problem above can be solved by
dynamic programming, the curse of dimensionality occurs
when the size of the problem is big. However, RL techniques,
such as dueling DQN [25], DQN [39], etc., are applied as an
effective approach to settle the problem without any priori
knowledge of state transition probability P.
s(cid:48)∈S
VOLUME 8, 2020
7135
B. Guo et al.: Dueling DQN-Based Delay-Aware Cache Update Policy for Mobile Users in F-RANs
FIGURE 4. An illustration of dueling deep-Q-network.
B. WORKFLOW OF DUELING DEEP-Q-NETWORK
To employ the RL framework in this work, the state space S,
action space A and reward function R are deﬁned as follows:
• State space. The state si ∈ S indicates the system infor-
mation in each step i. Assume that each MU requests
only one content during each slot. Then, the cache
update procedure during each slot t can be divided into
K states, and thus i = t · K + u. Let qi
u,n denotes
the total number of requests for the nth content in the
associated F-APs of MU u during last o slots at time t.
The state si collects the information about cache status
at each step i, and the cache status can be denoted by
n = qt
si = [qi
1
, qi
2
, . . . , qi
n, . . . , qi
2Nf ],
(9)
where 2Nf is the total size of storages in the associated
F-APs.
• Action space. The action ai ∈ A represents the action
that RL agent chooses at each step i. In order to limit the
size of action space, the agent replaces only one cached
content by the requested content or does nothing at each
step i. Let Ai = 0, 1, . . . , n, . . . , 2Nf denote all the
candidate actions which can be chosen at step i, where
ai = n(n (cid:54)= 0) means that the nth cached content will be
replaced, and ai = 0 means that the requested content
has been stored, so that the agent doesn’t have to update
the storage.
• Reward function. When the RL agent selects an
action ai under the state si, a reward ri from the
environment will be learned. To minimize the average
transmission delay, the reward function is designed as
ri = dmiss − d i
av
dmiss
,
(10)
av is the average transmission delay during each
av can be obtained by
where d i
slot t, and d i
av = 1
d i
K
(cid:88)
(cid:88)
u
c
u,c · [σ i
pi
u,c · dhit + (1 − σ i
u,c) · dmiss].
(11)
av in each step, ri is
since dmiss is much higher than d i
always bigger than 0.
a(cid:48) Q(s(cid:48), a(cid:48))−Q(s, a)),
In nature DQN, neural network (NN) is employed to
approximate a Q-value function which returns a Q-value for
each input state-action pair (s, a). The Q-value Q(s, a) is
updated when the agent chooses an action a under the state s,
and update function is deﬁned as
Q(s, a)←Q(s, a)+α(ρ(s)+γ max
(12)
where s(cid:48) ∈ Si+1 represents the next state, a(cid:48) ∈ Ai+1 denotes
an action at next step. Factors α (and γ ) denote learning
rate (and reward decay) respectively. In addition, as shown
in Fig. 4, target network and experience replay are employed
to improve the learning efﬁciency of DQN framework [39]:
• Target network. In nature DQN, there are two sepa-
rate NNs, evaluation NN and target NN. The evaluation
NN is used to generate Q-values for given state-action
pairs, and the target NN is utilized to generate Q-targets.
The evaluation NN is constantly updated to make the
7136
VOLUME 8, 2020
B. Guo et al.: Dueling DQN-Based Delay-Aware Cache Update Policy for Mobile Users in F-RANs
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
Q-values close to the Q-targets. The agent will reset the
weights(cid:98)θ of target NN by the weights θ of evaluation
NN every υ iteration steps.
• Experience replay. The agent can store the experiences
ei = [si, ai, ri, si+1] in the experience memory D. Then,
a mini batch of experiences are randomly sampled from
the experience memory to update the evaluation NN.
When the agent updates the the evaluation NN, a loss function
L(θ) will be adopted, and the loss function can be deﬁned as
L(θ) = ((cid:98)Q(s, a |(cid:98)θ) − Q(s, a | θ))2,
where Q-targets(cid:98)Q(s, a |(cid:98)θ) can be obtained by
a(cid:48) (cid:98)Q(s(cid:48), a(cid:48) |(cid:98)θ).
(cid:98)Q(s, a |(cid:98)θ) = r + γ max
(13)
(14)
Then, the weights θ can be obtained by minimizing the loss
L(θ) via a gradient descent approach.
Furthermore, in dueling DQN, the Q-value function is
divided into an advantage value A(s, a) and a state value V (s)
to improve learning efﬁciency and accelerate convergence
[25]. The Q-value function is given by
Q(s, a|θ) = A(s, a|θ) + V (s|θ).
(15)
Here, the advantage function is used to assess the value of
the action that has been chosen, and the state value can
measure the value of the state s. It is noticed that the state
value is independent of actions. In practice, the agent can’t
distinguish A(s, a) and V (s). Since the agent can’t obtain a
unique solution to (15). To solve the unidentiﬁable problem
above, the Q-value function can be calculated as
Q(s, a|θ) = V (s|θ) + (A(s, a|θ) − 1
|A|
A(s, a(cid:48)|θ)).
(cid:88)
(16)
a(cid:48)
C. DUELING DEEP-Q-NETWORK BASED
CACHE UPDATE POLICY
The proposed dueling DQN based cache update policy is
illustrated in Algorithm 1. In the background, the RL agent
can collect information including cache status, transmission
delay, requested contents and so forth, and the dueling DQN
will be trained for Nep episodes. When the agent is well
trained, the weights of NN will be stored and utilized for
cache update. Note that the agent use greedy policy to explore
new policies in the training phase, and the factor  is set to 1
in the testing phase.
V. SIMULATION RESULTS AND PERFORMANCE
EVALUATION
In this section, simulations are preformed to validate the
performance of the proposed caching policy. Firstly, the simu-
lation parameters are given. Then, the convergence of dueling
DQN is analyzed. Moreover, compared with FIFO, LFU
and LRU caching policies, the performance of the proposed
caching policy is evaluated in terms of average hit ratio and
average transmission delay.
Algorithm 1 Dueling DQN Based Cache Update Policy
weights θ and a target NN(cid:98)Q(s, a |(cid:98)θ) with weights(cid:98)θ = θ.
1: Randomly initialize an evaluation NN Q(s, a | θ) with
2: Initialize a experience memory D with a size of ND.
3: for each episode ep ∈ [1, Nep] do
for each t ∈ [1, T ] do
for each u ∈ [1, K ] do
MU u requests a content ct
u.
if the content ct
F-APs F t
u then
u has been stored in the associated
else
Download the content from the local cache.
if the storage of F t
u is not full then
else
u in
Fetch the content from the remote content
provider.
Cache the currently requested content ct
the local cache.
Observe the system state si(i = t · K + u).
Choose an action ai = arg max
Q(s, a) with
probability , or a random action with prob-
ability 1 − .
Replace the nth stored content in the storage
of F t
Receive the reward ri.
Store the experience ei = [si, ai, ri, si+1] in
D.
Randomly sample a mini batch of experi-
ences from D.
if episode terminates at step i then
u with ct
u.
a
a(cid:48) (cid:98)Q(s(cid:48), a(cid:48) |(cid:98)θ)
Set yi = ri + γ max
yi = ri
else
end if
Update θ by minimizing the loss
(yi − Q(s, a | θ))2 via a gradient descent
Reset the target NN (cid:98)Q every υ steps by
replacing weights(cid:98)θ with θ.
algorithm.
end if
end if
end for
27:
28:
29:
30:
31: end for
end for
A. SIMULATION SETUP
In the simulations, this paper considers an F-RAN with M F-
APs and K MUs. The preference of each MU is distinct, and
the content requests of each MU follow the Zipf distribution
with parameter κu = 1.1. The small-scale channel gain |ht|2
follows exponential distribution. Each MU in the cooperation
region is served by two F-APs. Each MU stays in a coopera-
tion region for τu time slots. In other words, elements in the
VOLUME 8, 2020
7137
B. Guo et al.: Dueling DQN-Based Delay-Aware Cache Update Policy for Mobile Users in F-RANs
FIGURE 5. Learning curves of the proposed dueling DQN based caching policy (M = 5, K = 10, N = 15).
uth row of topology relationship matrix Bt are regenerated
every τu slots. When an MU dwells in a cooperation region,
the distance between the MU and its associated F-APs is l.
For simpliﬁcation, assume that the coverage distance is a
constant value l = 100 m. Besides, the system bandwidth is
set to 20MHz, and allocated to each MU equally. Some main
parameters are listed in Table 1. In the simulation, the training
set of dueling DQN is generated by a random seed, and the
testing set are generated by another ﬁve random seeds.
TABLE 1. Simulation Parameters.
To validate the performance of the proposed caching
policy, the simulation results are compared with following
caching policies:
• First in ﬁrst out (FIFO). If the currently requested con-
tent hasn’t been stored in the local cache, FIFO policy
will replace the content which is stored earliest by the
new content.
• Least recently used(LRU). When the LRU policy
updates the local cache, the stored content that is least
recently requested will be replaced by the new content.
• Least frequently used (LFU). The LFU policy records
the number of requests for each stored content. The
stored content with the least requests number will be
replaced.
B. CONVERGENCE ANALYSIS
Fig. 5 illustrates the learning curves of the proposed duel-
ing DQN based caching policy for loss, normalized average
reward and normalized average transmission delay.
Fig. 5(a) shows the loss between the target values and
the Q-values for varying training steps. From the ﬁgure,
the loss curve descends quickly, as the increase of training
steps. With enough training steps, the loss converges to a
stable state. Fig. 5(b) presents the average reward of each
episode. As the increase of episodes, the average reward
gradually rises. On the contrary, the average transmission
delay decreases piece by piece. Note that average reward
and average transmission delay start to ﬂuctuate when Nep
is about 60, since the maximum value of greedy factor  in
training phase is set to 0.9, so that RL agent may choose a
suboptimal or even bad action with probability 0.1.
C. PERFORMANCE EVALUATION
In comparison of FIFO, LRU and LFU caching policies,
the proposed caching policy is validated in terms of average