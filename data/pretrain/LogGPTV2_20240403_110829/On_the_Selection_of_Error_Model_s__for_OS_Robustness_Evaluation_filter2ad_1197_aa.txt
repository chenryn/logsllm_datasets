title:On the Selection of Error Model(s) for OS Robustness Evaluation
author:Andr&apos;eas Johansson and
Neeraj Suri and
Brendan Murphy
On the Selection of Error Model(s) For OS Robustness Evaluation∗
Andr´eas Johansson, Neeraj Suri
Dept. of CS, TU-Darmstadt, Germany
{aja,suri}@informatik.tu-darmstadt.de
Abstract
The choice of error model used for robustness evaluation
of Operating Systems (OSs) inﬂuences the evaluation run
time, implementation complexity, as well as the evaluation
precision. In order to ﬁnd an “effective” error model for
OS evaluation, this paper systematically compares the rel-
ative effectiveness of three prominent error models, namely
bit-ﬂips, data type errors and fuzzing errors using fault in-
jection at the interface between device drivers OS. Bit-ﬂips
come with higher costs (time) than the other models, but
allow for more detailed results. Fuzzing is cheaper to im-
plement but is found to be less precise. A composite error
model is presented where the low cost of fuzzing is com-
bined with the higher level of details of bit-ﬂips, resulting in
high precision with moderate setup and execution costs.
1. Introduction
This paper focuses on ascertaining the robustness of OSs
to errors in device drivers. While multiple OS robustness
studies using fault-injection have been reported, for instance
[2, 9, 13, 15], in most cases, the results are applicable
mainly for the speciﬁc underlying error model. The choice
of error model and location of injection directly inﬂuences
the accuracy and usefulness of the obtained results.
The relative effectiveness of three distinct error models
at the OS-Driver interface is compared. The OS-Driver in-
terface was chosen as it represents an interface shared by
all drivers, it is reasonably well documented and supplies
the level of access needed for this type of studies. The error
models are compared for efﬁciency (cost and coverage), im-
plementation complexity and execution time requirements
and a new composite error model is presented. Conse-
quently, the paper proposes guidelines on selecting the ap-
propriate error model for OS robustness evaluation. The
results of such an evaluation are useful both in system de-
sign, i.e., where the OS and the drivers are integrated as part
∗Research supported in part by EC DECOS, ReSIST and Microsoft
Brendan Murphy
Microsoft Research, Cambridge, UK
PI:EMAIL
of a larger system, for ﬁnding hot-spots in the system war-
ranting reﬁnements, and component evaluation for compar-
ing the suitability of certain components in a system. The
chosen approach makes possible a comparative study of the
inﬂuence of drivers on system robustness, without requiring
source code access. Experimental quantitative approaches,
such as this one, complement analytical approaches (like
[4]) and provide easy means for quantifying dynamic be-
havior of the system under study.
The chosen error models span: a) bit-ﬂips (BF), where a
bit in a data word is ﬂipped, from 0 to 1, or vice versa. BF
were used, for instance, in [9] where the robustness of the
Linux kernel was evaluated. b) Data type-based corruption
(DT), where the value of a parameter in a call to a func-
tion is changed, according to its data type, for instance to
boundary values. This technique was used in [1] and [13].
In [3, 11] both BF and DT errors are used. c) Fuzzing (FZ),
which assigns a randomly chosen value to the parameter in
a function call. FZ has previously been used, for instance,
in [17, 19].
Paper Contributions & Structure: Selecting an “effec-
tive” error model to use in a particular setting is not straight-
forward and guidelines are of value for both OS designers
and evaluators. Building on previous experiences on OS
robustness evaluation [13], this paper represents a step to-
wards providing such a guideline. Using a case study based
on Windows CE .NET and three different drivers (serial,
network and storage card drivers) the paper speciﬁcally pro-
vides two distinct contributions, namely i) a comparative
study of error model effectiveness in terms of coverage and
cost, and ii) it establishes the effectiveness of using a com-
posite error model for OS robustness evaluation.
The paper is presented detailing four main blocks:
Prerequisites: Deﬁning the system model [Section 2];
background information on the studied error models
[Section 3]; a presentation of the evaluation criteria for
the error models, i.e., error propagation, failure mode
analysis and execution time [Section 4].
Implementation: Presentation of the target system and
the experimental technique used [Section 5].
Results: Presentation of the results from fault-injection
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:38 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007experiments [Section 6] with interpretations [Section 7].
Composite Model: Deﬁnition and results for the com-
posite error model [Section 8]; discussion and summary
of the main ﬁndings [Section 9].
2. System Model
Similar to [1, 2] we use a four-layered model of the OS:
Application, OS, Driver and Hardware layer. This model
applies to most common monolithic OSs, such as Windows,
UNIX and Linux. The OS-Driver interface (Figure 1) is our
target of study.
defects, it uses source code as an intrinsic basis for classi-
ﬁcation. Our decision not to require access to source code
implies that we cannot directly use the ODC classiﬁcation.
However, an attempt is made to classify the error models de-
pending on the defect classes they belong to. The interface
class of ODC is a potential source for each of the models, as
it deals with external interaction, such as drivers. The error
models were also chosen as they represent a class of errors
that previously have been reported as difﬁcult to detect and
recover from [20].
Errors are injected in service parameters. We do single
injections, i.e., we do not simultaneously inject in multiple
parameters. Injection is performed by intercepting calls and
manipulating data at runtime.
For all three error models we use a transient error oc-
currence and duration model, i.e., the error appears once
and then disappears. Errors are injected on ﬁrst occurrence.
This implies that the error is injected the ﬁrst time a service
is invoked. Previous studies have suggested that the impact
of the time of injection is small [21]; however this remains
to be comprehensively established.
All three error models handle pointers and structures as
special cases, and inject errors in the target of a pointer and
the members of a structure, when possible. Only if a pointer
or structure member is already set to NULL an injection is
not performed.
Figure 1. System model
3.1. Bit-Flip Error Model
A system containing N drivers (D1 . . . Dy . . . DN ) is
considered. Each driver exports and imports a set of ser-
vices dsx.y where x.y is xth service of driver Dy . The ef-
fects of errors are observed at the OS-Application interface
by manually instrumenting a number of benchmark applica-
tions with assertions. For each system service si used, we
study its behavior to detect deviations from the golden run.
Only the interface speciﬁcations (OS-Driver and OS-
Application) are required, but no source code, neither for
the drivers, nor for the OS itself. However, access is needed
to the source code of the benchmark applications, for instru-
menting them with assertions. The availability of interface
speciﬁcations is a basic requirement for any OS open for
extensions by new types of drivers/applications.
3. The Spread of Error Models
Bit-ﬂips is an extensively used error model deriving its
origins from transient hardware defects. Its ease of use and
implementation has made it a candidate to also be used to
simulate software (SW) defects. As it changes the value of
a parameter (by manipulating one of the bits) it belongs to
the Assignment class of ODC defects.
In the BF model, each parameter is treated as an integer
(typically a 32-bit word). The model’s greatest advantage
is also its greatest disadvantage, namely simplicity. While
it makes it very easy to implement, it suffers in expressive-
ness, with respect to abstract data types (strings etc.).
Each injection case ﬂips one bit, thus resulting in 32 in-
jection cases per parameter. However, some parameters are
16 (or 8) bit wide only, and we therefore restrict these pa-
rameters to use fewer injections.
3.2. Data Type Error Model
The three error models are chosen based on their appro-
priateness as reported in multiple previous studies and by
their relation to real faults, such as those deﬁned by Orthog-
onal Defect Classiﬁcation (ODC) [5]. ODC is a method of
classifying defects into orthogonal classes, enabling process
feedback and control. As it mainly focuses on in-process
Data type errors are chosen depending on the data type of
the targeted parameter. As the targeted interface is deﬁned
using the C language, the data types considered are all C-
style. This excludes high level abstract data types supported
in other high-level languages, such as classes in C++.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:38 UTC from IEEE Xplore.  Restrictions apply. 
...Operating System......APP1...D1D2DNAPPn..............................OS LayerApplication layerDriver LayerHardware LayerHardware Platform......[OS-Application inteface]}sidsx.y}[OS-Driver interface]37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007We follow established testing practice by choosing for
each type a set of predeﬁned (no randomness) test values,
offset values and boundary values. The offset values allow
us to modify the previous value (present in the intercepted
call), such as adding a number to it. The number of injec-
tions deﬁned is typically less than ten, allowing this error
model to incur fewer injections (on average) than the BF
model. However, there is no such inherent property and it
depends on the number of cases deﬁned for each type. This
model also belongs to the Assignment class of ODC, though
it could also be a Checking defect, resulting from a failing
or missing check of a data value.
The number of data types for which injection cases
need to be deﬁned depends only on the data types actually
used[7, 15]. Many services use the same data types for their
parameters, making the approach scalable. The same obser-
vation was made in the Ballista project [15] regarding the
POSIX API. In total, the three drivers targeted in this pa-
per result in injections for 22 data types being deﬁned. An
overview is shown in Table 1. For structures (struct) the
number of cases depends on the members (marked with * in
the table). DT errors also treat pointers as special data type
and reserves one injection case for the pointer, namely set-
ting it to NULL. Wrong use of NULL-pointers is a common
programming mistake. To further illustrate how DT errors
are deﬁned, Table 2 shows the cases for the type int.
Case #
New value
1
2
3
4
5
6
7
(Previous value) - 1
(Previous value) + 1
1
0
-1
INT MIN
INT MAX
Table 2. Error cases for type int.
Therefore, the result of the injection may differ across ex-
periments, resulting in the need for multiple experiments to
obtain statistically valid conclusions. A speciﬁc discussion
on the number of injections needed appears in Section 6.4.
Whereas BF and DT use the parameter values, FZ dif-
fers in that it ignors these values. FZ belongs to either the
Assignment or Checking classes in ODC. This model was
considered in [17, 19, 10].
The pseudo-random values are chosen using the
rand() C-runtime function. The last value produced in
a round is stored in persistent storage and is used as the
seed for the next round, thus ensuring that different random
values are used every time.
Data type C-Type
#Cases
3.4. Other Key Contemporary Models
Integers
int
unsigned int
long
unsigned long
short
unsigned short
LARGE INTEGER
* void
HKEY
struct {...}
Strings
Characters char
Misc
unsigned char
wchar t
bool
multiple cases
Boolean
Enums
7
5
7
5
7
5
7
3
6
*
4
7
5
5
1
#identiﬁers
Table 1. Overview of the data types used.
3.3. Fuzzing Error Model
The work in [1, 2, 3, 8, 12, 14] explored the use of vari-
ous error models and injection techniques for OS robustness
evaluation and benchmarking. For instance, [12] compares
errors similar to the ones considered here, but injected at
different levels of the Linux kernel. In [2] a mutation error
model is used, in which the code segments of drivers are
targeted. Further, in [18] the authors observe that effects
of errors at the interface, though being useful for robust-
ness evaluation, do not represent defects in code. As we
do not inject errors at the code level we can neither verify,
nor falsify this observation. We believe that our systematic
comparison of error models, is a useful contribution to the
community as this comparative aspect at the OS-Driver in-
terface has not been treated in depth before.
The chosen models, even though not complete, still rep-
resent a large operational spectrum. [6] studies defects in
two large OSs and almost 50% of the found defects belong
to the ODC classes represented in this paper.
4. Comparative Evaluation Criteria
Fuzzing a parameter implies assigning it a pseudo-
random value from the set of legal values for the type.
The chosen error models are studied based on a diverse
set of commonly used evaluation criteria, namely i) error
propagation (Diffusion), ii) error impact (failure class), iii)
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:38 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007implementation complexity, and iv) execution time. Each
criteria is elaborated in the following subsections.
4.1. Error Propagation Criteria (Diﬀusion)
In this paper, the focus of error propagation is on Driver
Error Diffusion1 [13]. Diffusion is deﬁned as the degree to
which a driver can spread errors in the system. It allows
drivers to be compared and ranked making it possible for a
system evaluator/designer to make a judgement on where to
expend more resources in terms of testing/veriﬁcation and
quality improvement.
Diffusion considers the propagation paths from a driver,
through the OS to user applications. It is the sum of the con-
ditional probabilities for an error to propagate, given that an
error exists. Diffusion is itself not a probability but a metric
of sensitivity to input errors, which can be exactly estimated
by code analysis, or approximated experimentally.
Dx =X
X
P DSi
x.y
(1)
si
dsx.y
In Equation 1, P DSi
x.j represents the conditional prob-
ability that errors in the driver’s use of OS services (dsx.j)
lead to failure (see also Figure 1).
Diffusion can be used to compare the suitability of
drivers for a particular system based on to their potential
for spreading errors. A higher diffusion value implies that
a driver is more liable to spread errors. However, note that
drivers are not tested per se. Consequently, we stress that
the intent is not to give absolute values for error sensitiv-
ity, but to obtain relative rankings. A detailed discussion on
diffusion and error propagation metrics is found in [13].
It is important to note that an error can propagate in the
system and still remain latent (i.e., not lead to failure) with-
out immediate detection. As the triggers for dormant faults
is not known, we take an optimistic approach and consider
a failure free run of the test applications to imply that the
likelihood of a dormant fault is very low.
4.2. Error Impact Criteria (Failure Class)
To determine and distinguish the impact of a propagated
error we use failure mode analysis. A set of four modes of
increasing severity is deﬁned including the non-propagating
one representing normal or failure-free behavior.
Class NF: No discernible violation observed as outcome
of an experiment, i.e., No Failure class. Three distin-
guishable explanations account for an injection result-
ing in this class, namely a) the error location was not
activated in this execution; b) the error was injected, but
1We use the shorter term Diffusion in the rest of the text.
was masked by the system; or c) the fault is dormant.
Section 5.3 describes how case a) can be avoided.
Class 1: The error propagated to the benchmark appli-
cations, but still satisﬁed the OS service speciﬁcation.
Examples of class 1 outcomes include unsuccessful at-
tempts to use services where the error code returned is
in the set of valid codes for this call. The propagation
of data errors also fall into this category.
Class 2: This failure mode captures behaviors violating
the speciﬁcation of the service.
It could be an un-
forseen hang or crash of the application due to the er-
ror or an incorrect error code being returned. An ap-
plication is considered hung after 40 seconds of non-
responsiveness, exceeding 100% of normal execution
time in a golden run experiment. Note that the rest of
the system is still functioning after the failure.
Class 3: The OS becomes irresponsive due to a crash or
a hang. No further progress is possible.