newly created clusters. Next, our modiﬁed version of VFDT
reads the polluted C4.5 decision tree and starts incremental
building upon the polluted decision tree instead of a single
leaf in the original implementation. After that, a new decision
tree is outputted in the C4.5 format and used as the decision
tree of OSNSF. The modiﬁed version only introduces 33 lines
of code into main.cpp of the implementation of OSNSF to
support the aforementioned process.
We then show the completeness results in Table IV. The ﬁrst
column shows the original true positive and false positive rate.
After polluting 1.75% of the training data, both true and false
positive rates decrease. Then, after unlearning the polluted
samples, both the true positive rate and false positive rate
increase. Note that the unlearning process does not restore the
true positive rate to its pre-pollution value . This is because the
two decision trees are generated by C4.5 and VFDT – different
algorithms. VFDT will not generate the same decision tree as
the C4.5 algorithm, even if the inputs as well as the order of the
inputs are the same. However, we believe that the ﬁltering rate
(true positive) after unlearning is enough for a spam ﬁltering
system. Further, for 99.4% of testing samples, the original
system and the unlearned system generate the same result,
achieving 99.4% for the completeness of unlearning, a fairly
high number. We also evaluate the timeliness of unlearning.
The retraining takes 30mins, and the unlearning of one sample
takes only 21.5ms, leading to a speedup factor of 8.4×104.
IX. UNLEARNING IN PJSCAN
We start by describing the learning technique used in PJS-
can [51]. PJScan ﬁrst extracts all the JavaScript using Poppler,
analyzes it by Mozilla’s SpiderMonkey, and then tokenizes it.
After that, PJScan learns and classiﬁes malicious JavaScript
using one-class SVM, i.e., PJScan uses only malicious but
not benign JavaScript from PDFs to train an SVM engine. To
implement one-class SVM, PJScan ﬁrst uses an existing SVM
classiﬁer called libSVM [13] to generate the α values [20]
of all support vectors, and then calculates the center and the
radius of an n-sphere in a space with n + 1 dimensions. Then,
if an incoming data sample falls within the n-sphere, the data
sample is classiﬁed as malicious; otherwise, the data sample is
classiﬁed as benign. We are aware of other machine learning-
based PDF detection engines [66, 68] after PJScan, however
we choose PJScan because it is open-source and unique in
adopting one-class machine learning.
Because the source code of PJScan [16] dates back to
2011, PJScan does not support some of the recent up-to-
date libraries. In particular, we installed an old Poppler with
version 0.18.2, because some of APIs used in PJScan are not
supported by the latest version of Poppler. Meanwhile, the
most recent version of Boost library has some conﬂicts with
PJScan, and we made several modiﬁcations to PJScan so that
those two are compatible with each other. In particular, we
needed to modify boost :: f ilesystem :: path.f ile string()
to boost :: f ilesystem :: path.string(), change the deﬁnition
of BOOST F ILESY ST EM V ERSION from 2 to 3, and
delete an obsolete catch block dealing with a ﬁle reading
exception. Then, we simply executed the “install.sh” provided
by PJScan for installation. For the PJScan experiment, we also
use the data set from Huawei, which contains 65 malicious
PDF samples with corresponding JavaScript. Because the size
of the data set is small, half of the PDFs are used for training,
and the other half are used for testing.
A. The Attack – Training Data Pollution
To pollute PJScan, we need to move the center of the
new, polluted n-sphere far away from the original center. In
practice, we repeat the same alert functions (alert(1);) and
inject such a ﬁxed pattern into PDFs to achieve it.
We show the pollution results in Table V. The ﬁrst column
of Table V shows that without pollution PJScan classiﬁes
81.5% of the malicious PDFs as malicious. When we pollute
21.8% of the training samples, the detection rate decreases
to 69.3%. Then, when we increase the percentage of polluted
training samples to 28.2%, the detection rate ﬁnally drops to
46.2%. The result indicates that one-class machine learning is
harder to pollute than two-class machine learning. Speciﬁcally,
two-class SVM classiﬁer needs to draw a line between the
sphere labeled as benign and the sphere labeled as malicious,
and therefore the radius of one sphere is constrained by
the other. In contrast, one-class SVM classiﬁer can always
increase the radius of the sphere and keep the percentage of
included data samples in the sphere as a constant. This also
aligns with the fact that one-class machine learning is robust
to mimic attacks [57].
476476
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 
B. Analytical and Empirical Results
The unlearning process can be divided into two stages due
to inherent properties of one-class SVM in PJScan. The ﬁrst
stage of unlearning is to re-calculate the new α value [20] of
each support vector calculated by the solver in libSVM, and
the second stage is to recalculate the center and the radius of
the sphere. In the ﬁrst stage, the calculation of α values is an
iterative process of adaptive SQ learning, which starts from
an initial assignment of α values, and in the end reaches the
optimal point. Since the changes of α during unlearning is
relatively small, we just need to feed the old values of α into
the iteration process and then let the iteration process output
the new α.
The second stage of unlearning recalculates the value of the
center and the radius. The calculation of the center is in the
form of α1x1 + α2x2 + ... – a summation. If the value of α
changes in the ﬁrst stage, we need to multiply the delta of α
by the support vector and add it to the summation. If a new
support vector emerges or an old one disappears, we also need
to add or subtract corresponding values from the summation.
Meanwhile,
the calculation of the radius is based on the
distance between the support vector with the smallest α and
the center. If that support vector changes, the unlearning needs
to recalculate the value of radius from scratch. Otherwise, as
in the calculation of the center, the unlearning can utilize some
of the partial calculation results from the past.
Empirically, in total, we added 30 lines of code into the
libsvm oc module of PJScan to update the α value, the center
and the radius. Next, we evaluate the completeness by showing
the unlearning result in the last column of Table V, the same
as the original detection rate. In addition, we also evaluate the
timeliness of unlearning. To calculate the α value, retraining
takes 42 iterations, and unlearning one data sample takes 2.4
iterations on average, signiﬁcantly smaller than retraining.
X. DISCUSSIONS
Unlearning, or forgetting systems in general, aim to restore
privacy, security, and usability. They give users and service
providers the ﬂexibility to control when to forget which data.
They do not aim to protect
the data that remains in the
system. Private data in the system may still be leaked, polluted
training data in the system may still mislead predication, and
incorrect analytics in the system may still result in bogus
recommendations.
Before unlearning a data sample, we must identify this
to what
sample. This identiﬁcation problem is orthogonal
we have solved in this paper. Sometimes,
the problem is
straightforward to solve. For instance, a user knows precisely
which of her data items is sensitive. Other times, this problem
may be solved using manual analysis. For instance, a vigilant
operator notices an unusual drop in spam detection rate,
investigates the incident, and ﬁnds out that some training data
is polluted. This problem may also be solved using automated
approaches. For instance, a dynamic program analysis tool
analyzes each malware training data sample thoroughly and
conﬁrms that the sample is indeed malicious. Unlearning is
complimentary to these manual or automatic solutions.
We believe our unlearning approach is general and widely
applicable. However, as shown in our evaluation (§VIII), not
every machine learning algorithm can be converted to the
summation form. Fortunately, these cases are rare, and custom
unlearning approaches exist for them.
XI. RELATED WORK
In §I, we brieﬂy discussed related work. In this section,
we discuss related work in detail. We start with some attacks
targeting machine learning (§XI-A), then the defenses (§XI-B),
and ﬁnally incremental machine learning (§XI-C).
A. Adversarial Machine Learning
Broadly speaking, adversarial machine learning [22, 47]
studies the behavior of machine learning in adversarial en-
vironments. Based on a prior taxonomy [47],
the attacks
targeting machine learning are classiﬁed into two major cate-
gories: (1) causative attacks in which an attacker has “write”
access to the learning system – she pollutes the training data
and subsequently inﬂuences the trained models and prediction
results; and (2) exploratory attacks in which an attacker has
“read-only” access – she sends data samples to the learning
system hoping to steal private data inside the system or evade
detection. In the rest of this subsection, we discuss these two
categories of attacks in greater detail.
1) Causative Attacks: These attacks are the same as data
pollution attacks (§II-B2). Perdisci et al. [56] developed an
attack against PolyGraph [55], an automatic worm signature
generator that classiﬁes network ﬂows as either benign or ma-
licious using a na¨ıve Bayes classiﬁer. In this setup, the attacker
compromises a machine in a honeynet and sends packets with
polluted protocol header ﬁelds. These injected packets make
PolyGraph fail to generate useful worm signatures. Nelson et
al. [54] developed an attack against a commercial spam ﬁlter
SpamBayes [18] which also uses na¨ıve Bayes. They showed
that, by polluting only 1% of the training data with well-
crafted emails, an attacker successfully causes SpamBayes
to ﬂag a benign email as spam 90% of the time. While
these two attacks target Bayesian classiﬁers, other classiﬁers
can also be attacked in the same manner, as illustrated by
Biggio et al.’s attack on SVM [24]. Instead of focusing on
individual classiﬁers, Fumera et al. [44] proposed a framework
for evaluating classiﬁer resilience against causative attacks at
the design stage. They applied their framework on several
real-world applications and showed that the classiﬁers in these
applications are all vulnerable.
Our practical pollution attacks targeting Zozzle [35], OS-
NSF [46], and PJScan [51] fall
into this causative attack
category. All such attacks, including prior ones and ours, serve
as a good motivation for unlearning.
2) Exploratory Attacks: There are two sub-categories of
exploratory attacks. The ﬁrst sub-category of exploratory
attacks is system inference or model inversion attacks, as
discussed in §II-B1. Calandrino et al. [29] showed that, given
477477
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:08:45 UTC from IEEE Xplore.  Restrictions apply. 
some auxiliary information of a particular user, an attacker
can infer the transaction history of the user. Fredrikson et
al. [43] showed that an attacker can infer the genetic markers
of a patient given her demographic information. These attacks
serve as another motivation for unlearning.
In the second sub-category, an attacker camouﬂages mali-
cious samples as benign samples, and inﬂuences the prediction
results of a learning system. In particular, for those systems
that detect samples with malicious intentions, an attacker
usually crafts malicious samples to mimic benign samples
as much as possible, e.g., by injecting benign features into
malicious samples [30, 49, 76, 77]. As suggested by Srndic et
al. [76], in order to make learning systems robust to those
attacks, one needs to use features inherent to the malicious
samples. These attacks are out of scope of our paper because
they do not pollute training data nor leak private information
of the training data.
B. Defense of Data Pollution and Privacy Leakage
In this subsection, we discuss current defense mechanisms
for data pollution and privacy leakage. Although claimed to
be robust, many of these defenses are subsequently defeated
by new attacks [43, 56]. Therefore, unlearning serves as an
excellent complimentary method for these defenses.
1) Defense of Data Pollution: Many defenses of data pollu-
tion attacks apply ﬁltering on the training data to remove pol-
luted samples. Brodley et al. [27] ﬁltered mislabeled training
data by requiring absolute or majority consensus among the
techniques used for labeling data. Cretu et al. [34] introduced
a sanitization phase in the machine learning process to ﬁlter
polluted data. Newsome et al. [55] clustered the training data
set to help ﬁlter possible polluted samples. However, they are
defeated by new attacks [56]. None of these techniques can
guarantee that all polluted data is ﬁltered. Another line of
defense is to increase the resilience of the algorithms. Dekel et
al. [36] developed two techniques to make learning algorithms
resilient against attacks. One technique formulates the problem
of resilient learning as a linear program, and the other uses
the Perceptron algorithm with an online-to-batch conversion
technique. Both techniques try to minimize the damage that
an attacker could cause, but the attacker may still inﬂuence the
prediction results of the learning system. Lastly, Bruckner et
al. [28] model the learner and the data-pollution attacker as a
game and prove that the game has a unique Nash equilibrium.
2) Defense of Privacy Leaks: In general, differential pri-
vacy [75, 80] preserves the privacy of each individual item in
a data set equally and invariably. McSherry et al. [52] built
a differentially private recommendation system and showed
that in the Netﬂix Prize data set the system can preserve
privacy without signiﬁcantly degrading the system’s accu-
racy. Recently, Zhang et al. [80] proposed a mechanism to
produce private linear regression models, and Vinterbo [75]
proposed privacy-preserving projected histograms to produce
differentially-private synthetic data sets. However, differential
privacy requires that accesses to data ﬁt a shrinking privacy
budget, and are only to the fuzzed statistics of the data set.
These restrictions make it extremely challenging to build
usable systems [43]. In addition, in today’s systems, each
user’s privacy consciousness and each data item’s sensitivity
varies wildly. In contrast, forgetting systems aim to restore
privacy on select data, representing a more practical privacy
vs utility tradeoff.
C. Incremental Machine Learning
Incremental machine learning studies how to adjust
the
trained model
incrementally to add new training data or
remove obsolete data, so it is closely related to our work.
Romero et al. [62] found the exact maximal margin hyperplane
for linear SVMs so that a new component can be easily added
or removed from the inner product. Cauwenberghs et al. [31]
proposed using adiabatic increments to update a SVM from
l training samples to l + 1. Utgoff et al. [74] proposed an
incremental algorithm to induce decision trees equivalent to
the trees formed by Quinlan’s ID3 algorithm. Domingos et
al. [38] proposed a high performance construction algorithm of
decision trees to deal with high-speed data streams. Recently,
Tsai et al. [73] proposed using warm starts to practically build
incremental SVMs with linear kernels.
Compared to prior incremental machine learning work,
our unlearning approach differs fundamentally because we
propose a general efﬁcient unlearning approach applicable to
any algorithm that can be converted to the summation form,
including some that currently have no incremental versions.
For instance, we successfully applied unlearning to normalized
cosine similarity which recommendation systems commonly
use to compute item-item similarity. This algorithm had no
incremental versions prior to our work. In addition, we applied
our learning approach to real-world systems, and demonstrated
that
that unlearning handles all stages of
learning, including feature selection and modeling.
is important
it
Chu et al. [33] used the summation form to speed up ma-
chine learning algorithms with map-reduce. Their summation
form is based on SQ learning, and provided inspiration for our
work. We believe we are the ﬁrst to establish the connection
between unlearning and the summation form. Furthermore, we
demonstrated how to convert non-standard machine learning
algorithms, e.g., the normalized cosine similarity algorithm,
to the summation form. In contrast, prior work converted
nine standard machine learning algorithms using only simple
transformations.
XII. CONCLUSION AND FUTURE WORK
We have presented our vision of forgetting systems that
completely and quickly forget data and its lineage to restore