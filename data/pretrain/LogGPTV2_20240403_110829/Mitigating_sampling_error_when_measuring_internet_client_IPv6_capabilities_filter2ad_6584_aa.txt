title:Mitigating sampling error when measuring internet client IPv6 capabilities
author:Sebastian Zander and
Lachlan L. H. Andrew and
Grenville J. Armitage and
Geoff Huston and
George Michaelson
Mitigating Sampling Error when Measuring Internet Client
IPv6 Capabilities
Sebastian Zander
CAIA, Swinburne University of
Lachlan L. H. Andrew
CAIA, Swinburne University of
Grenville Armitage
CAIA, Swinburne University of
Technology
Technology
Technology
Melbourne, Australia
PI:EMAIL
Melbourne, Australia
PI:EMAIL
Melbourne, Australia
PI:EMAIL
Geoff Huston
Asia Paciﬁc Network
Information Centre (APNIC)
Brisbane, Australia
PI:EMAIL
George Michaelson
Asia Paciﬁc Network
Information Centre (APNIC)
Brisbane, Australia
PI:EMAIL
ABSTRACT
Despite the predicted exhaustion of unallocated IPv4 addresses be-
tween 2012 and 2014, it remains unclear how many current clients
can use its successor, IPv6, to access the Internet. We propose a
reﬁnement of previous measurement studies that mitigates intrin-
sic measurement biases, and demonstrate a novel web-based tech-
nique using Google ads to perform IPv6 capability testing on a
wider range of clients. After applying our sampling error reduction,
we ﬁnd that 6% of world-wide connections are from IPv6-capable
clients, but only 1–2% of connections preferred IPv6 in dual-stack
(dual-stack failure rates less than 1%). Except for an uptick around
IPv6-day 2011 these proportions were relatively constant, while the
percentage of connections with IPv6-capable DNS resolvers has in-
creased to nearly 60%. The percentage of connections from clients
with native IPv6 using happy eyeballs has risen to over 20%.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Oper-
ations—Network Monitoring; C.4 [Performance of Systems]:
Measurement Techniques
Keywords
IPv6 deployment, banner-ad-based measurement
1.
INTRODUCTION
In response to the foreseen exhaustion of IPv4 address space, the
IETF standardised the IPv6 protocol as a long-term replacement
for IPv4 in 1998 (for an introduction to IPv6 see [1,2]). As of May
2012 most of the IPv4 address space is now allocated and according
to predictions, the Regional Internet Registrars (RIRs) will run out
of IPv4 addresses between 2012 and 2014 [3]. The question is:
how many clients can access Internet resources with IPv6?
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’12, November 14–16, 2012, Boston, Massachusetts, USA.
Copyright 2012 ACM 978-1-4503-1705-4/12/11 ...$15.00.
Over the last decade several researchers studied the progress of
IPv6 deployment [4]. Many of these studies were based on active
probing of servers and network topology, or passive measurements
based on server or traﬃc logs, routing data, or DNS root server
data [5–11]. These provide valuable information about the IPv6
capabilities of servers or networks, and the proportion of clients
that already use IPv6. However, these studies do not provide in-
formation about the IPv6 capabilities of clients. While most hosts
run IPv6-capable operating systems, they are often not conﬁgured
to use IPv6 (because there are virtually no IPv6-only services) or
their access networks are not IPv6-capable.
More recently, a few researchers used active web-based mea-
surements (based on a technique developed by S. Steﬀann [12]) to
investigate the IPv6 capabilities of clients [13–15]. On certain par-
ticipating web sites the clients’ web browsers download not only
the “normal” web pages and images, but also a few very small in-
visible images from a test server. One image can only be retrieved
with IPv4, a second only with IPv6 and a third with either IPv4 or
IPv6. Based on the images successfully retrieved one can deter-
mine the clients’ IPv6 capabilities.
With this method the client sample has a bias based on the par-
ticipating web sites. In previous studies only one or two web sites
were used, or the analysis was limited to a region (e.g. EU coun-
tries). Also, the original method did not provide any means to
verify whether a client actually attempted all image downloads or
aborted some, for example because the user moved to a diﬀerent
web page quickly. Furthermore, previous studies focussed only on
certain aspects (e.g. how many clients preferred IPv6 [13]).
We propose an improved version of the web-based measurement
method, commonly implemented with JavaScript (JS-test). Our
method not only measures the fraction of clients that prefer IPv6
over IPv4, but also the fraction of clients that are IPv6-capable
(including latent Teredo [16] capabilities), or use an IPv6-capable
DNS server for query resolution. We also analyse the capabili-
ties based on the clients’ operating system (OS) and country, and
we estimate IPv6-related dual-stack failures and the use of “happy
eyeballs” (fast fail-over from IPv6 to IPv4 [17]).
To increase the number of participating sites our test provides
information not only to us, but also to the site operators themselves
via Google Analytics. Since some web service providers are inter-
ested in the IPv6 capabilities of visiting clients, this has lead to an
increase in participating sites. We also propose a novel method of
selecting the clients to be tested. We embedded the test script in a
87Flash ad banner (FA-test), which is served to clients via Google’s
AdSense. The test is carried out as soon as the ad is displayed by
a browser. The ad’s distribution is controlled by Google, but ads
can potentially reach a very broad set of clients, through popular
AdSense-enabled web sites (e.g. YouTube).
We compare the coverage of JS-test and FA-test and show that
Google distributes the ads well. Normalised on the total number of
tests, the FA-test reaches far more IP addresses located in more /24
networks compared to the JS-test. However, due the ad’s running
costs the number of FA-tests was limited, and the ad presentation
was somewhat biased towards Asian web sites. Also, the FA-test
could not test iPhones or iPads due to the absence of Flash.
We then identify sources of potential sampling error and pro-
pose techniques to mitigate it. We interpret our results as statistics
of connections to avoid potential bias, for example due to multi-
ple clients behind proxies or Network Address Translators (NATs).
To mitigate the sampling error we re-weight the data based on the
clients’ countries and the proportion of Internet traﬃc of diﬀer-
ent countries. We compare the proportions of OSs and browsers
measured with reference statistics and show that our raw statistics
appear biased, but the re-weighted statistics are similar to the ref-
erence. We then estimate the IPv6 capabilities based on the re-
weighted statistics.
The paper is organised as follows. Section 2 describes our mea-
surement method. Section 3 describes the dataset and compares the
client samples of JS-test and FA-test. Section 4 analyses sources of
bias and presents our approach to mitigate the sampling error. Sec-
tion 5 presents the IPv6 capability statistics measured. Section 6
discusses related work and compares it with our ﬁndings. Section
7 concludes and outlines future work.
2. MEASUREMENT METHOD
We describe the improved web-based measurement method, dis-
cuss the diﬀerent ways tested clients are selected, and outline our
experimental setup. Finally, we argue that our tests do not signiﬁ-
cantly aﬀect the experience of users.
2.1 Web-based measurements
When users visit web sites their web browsers normally down-
load several web pages, scripts, images etc. At certain participating
web sites this includes a small test script that fetches a few invis-
ible one-pixel images via HTTP from URLs pointing to our test
web server (similar to a page hit counter). We refer to the script
as test and a single image download as sub-test. For URLs con-
taining host names the client’s resolver has to ﬁrst perform DNS
look-ups against our authoritative test DNS server prior to sending
HTTP requests. The following sub-tests allow us to test IPv4, IPv6,
dual-stack, and (latent) Teredo [16] capabilities of clients, as well
as whether the client’s resolving DNS server is IPv6-capable:
1. The image can only be retrieved with IPv4 because the DNS
server only returns an A record (IPv4-only).
2. The image can only be retrieved with IPv6 because the DNS
server only returns an AAAA record (IPv6-only).
3. The image can be retrieved with IPv4 or IPv6 because the
DNS server returns A and AAAA records (dual-stack).
4. The image URL is an IPv6 address literal. Windows Vista
and Windows 7 hosts without native IPv6 will not query for
DNS AAAA records, but with an IPv6 address literal they
can use Teredo [18].
5. The image URL puts our authoritative DNS server behind
an IPv6-only name server record. The DNS server can only
be accessed if the client’s resolving DNS server can perform
DNS queries over IPv6.
A sub-test is deemed successful if the image could be retrieved,
otherwise it is deemed a failure. After all sub-tests have been suc-
cessfully completed or a timeout of ten seconds (whichever occurs
ﬁrst), the test script sends another HTTP request to the test web
server that acts as a test summary. The test summary allows us to
determine whether a browser has waited a suﬃcient amount of time
for all sub-tests to complete. Without the summary it is impossi-
ble to know whether a sub-test was not successful because a client
lacked the capability or because the test was interrupted.
The test script starts sub-tests in quick succession, but to which
degree the images are fetched in parallel depends on the web
browser’s connection management. We assume that browsers try
to fetch objects as quickly as possible without unnecessary delay.
The URLs for each sub-test and summary resolve to diﬀerent IPv4
and/or IPv6 addresses, which means browsers cannot multiplex dif-
ferent sub-tests over one TCP connection.
2.2 Client sample
Clients interact with our measurement system in two diﬀerent
ways. Several participating web sites link our JavaScript test script
(JS-test) and visiting hosts are tested. To encourage participation in
our JS-test, the script also logs the IPv6 capabilities of clients with
Google Analytics and the site administrators can inspect the statis-
tics (test script is available at [19]). The client sample is biased to-
wards the participating web sites, but since the test is implemented
in JavaScript the vast majority of visiting clients can be tested. We
assume there are not many clients with disabled JavaScript [20].
We also implemented a Flash ActionScript test and embedded
it in a Flash ad banner (FA-test). The ad is served to hosts via
Google’s AdSense. The test is carried out as soon as the ad is dis-
played by the web browser, the user does not have to click on it. We
selected the ad’s keywords and languages to achieve broad place-
ment across diﬀerent countries and sites. The FA-test reaches a
more diverse client population, but cannot be carried out by clients
without Flash, such as iPhones/iPads during our measurements.
2.3 Experimental setup
Figure 1 shows a logical diagram of our experimental setup. The
DNS server handles the incoming DNS queries and the web server
serves the test images. All servers are time-synchronised with NTP.
Local Teredo and 6to4 relays are located close to the web server to
reduce IPv6 tunnelling delay (Teredo server and client-side 6to4
relay are out of our control). We collect HTTP log data, DNS log
data, and capture the traﬃc with tcpdump.
Several data ﬁelds are used to convey information from a tested
client to our test servers, and to prevent caching of DNS records or
test images at the client or intermediate caches (see Figure 1). The
following data is prepended to the host name as well as appended
as URL parameters: test time, test ID, test version, and sub-test
name. Test time is the time a test started taken from the client’s
clock (Unix time plus milliseconds). Test ID is a “unique” random
32-bit integer number determined by the test script.1 Test version
is the test script’s version number and sub-test name is a unique
identiﬁer for the sub-test, e.g. “IPv4-only”.
1FA-tests use a hash value computed from Google’s clickTAG, as
Google prohibits the use of the ActionScript random() function.
88Figure 1: Experimental setup
The JS-test uses cookies to ensure clients are only tested once
every 24 hours (per web site). The main reason for this is to reduce
the load on our test servers. However, some clients may ignore
the JS-test cookies and perform the test multiple times within 24
hours. The FA-test cannot use cookies due to Google’s restrictions,
so clients may be tested multiple times. For both tests there can
be web proxies or NATs in front of multiple clients that look like
repeating clients. Nevertheless, the number of IPs repeating the test