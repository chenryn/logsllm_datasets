special treatment within each network.
This BGP announcement goes through standard BGP export/import
policies and is imported into the routing information base of b1.
Periodically, inside B, REIN extracts from border routers such re-
quest announcements using the tag REIN PATH REQUEST, and
computes the interdomain bypass paths that it can provide, sub-
ject to its local policy. Note that one objective of the local policy
is to mitigate the operational difﬁculties involved in the planning
for carrying another network’s trafﬁc. For instance, B’s local pol-
icy could dictate that bypass paths are provided to A only through
lightly-loaded links.
If B can provide bypass paths from some border router b2, REIN
will conﬁgure b2 to announce a BGP update message carrying a
unique BGP community tag REIN PATH AVAILABLE to its peer
a2. The message from b2 to a2 is shown in Table 1.
The bypass path attribute in the REIN PATH AVAILABLE mes-
sage does not include the complete router path inside B, to protect
B’s private information. The exported values of bandwidth should
be relatively stable to avoid frequent re-computation. Note that the
bandwidths are allocated bandwidths instead of the total bandwidth
of a bypass path. In addition, the bandwidth(s) may be constrained
by the bandwidths of the peering links. However, since it can be
cheaper to over-provision the bandwidth of a peering link than that
Destination:
AS path:
Bypass path:
Path metrics:
Shared
risk
link groups:
a1
the AS path traversed by this bypass path. It will be AB.
a2, b2, opaque ID identifying the internal path inside B
from b2 to b1, b1, a1.
these include aggregated delay from b2 to b1, best-effort
bandwidth b2 can provide to b1, and guaranteed band-
width b2 can provide to b1, etc.
the IDs of the shared risk link groups involved in the path
from b2 to b1.
Table 1: An example REIN PATH AVAILABLE message.
of a link connecting two faraway locations, this might be a lesser
concern. The delay value will be used by network A when there
is delay requirement. The path metrics may also include pricing
information in a more ﬂexible system.
The shared risk link groups (SRLGs) information needs coor-
dination between the neighboring networks to assign consistent
SRLG IDs to links or use a global information database. Two links
belong to the same SRLG if they are considered to be likely to fail
together. An example is two links that share some common conduit
at some segment. A potential practical issue is that the networks
may not be able to provide complete SRLG information. There can
be several ways to address this issue. For example, some previous
studies have shown that much of such information can be inferred
using public records [18]. We emphasize that if this information is
not provided, the beneﬁts of REIN may be reduced. For example, a
single event such as an excavation accident could impact multiple
IP networks, damaging both interdomain bypass paths and intrado-
main links.
Periodically, inside A, using the tag REIN PATH AVAILABLE,
REIN extracts interdomain bypass paths announced by neighboring
networks. It then computes how to use these paths to improve reli-
ability. For those paths that it chooses to use, it sends a BGP update
message with a unique BGP community tag REIN PATH COMMIT
to inform the neighbor. The neighbor then conﬁgures its data for-
warding path to allow usage of the path (see below).
Note that we can extend this protocol to allow interdomain by-
pass paths to traverse more networks. For simplicity and also to
avoid bypass paths with long delays, we focus on interdomain by-
pass paths that involve only one direct neighbor in this paper. In
this case, BGP announcements for bypass paths are not propagated
further, which avoids global BGP updates.
2.4 Data Forwarding Using Interdomain Paths
The main data-path capability needed by REIN is to allow trafﬁc
to leave and re-enter a network. This is not possible in the current
Internet due to the separation of intradomain and interdomain rout-
ing. Speciﬁcally, a major problem is potential forwarding loops
inside a neighboring network. Forwarding loops cannot arise in the
hierarchical Internet routing, because that would imply a loop in
AS paths. However, direct usage of interdomain bypass paths may
cause forwarding loops. Consider the preceding example when the
interdomain bypass path a2 → b2 ; b1 → a1 is used. When a2
uses the bypass path, it encapsulates a packet using source address
a2 and destination address a1, and sends the encapsulated packet to
b2. However, a router inside B close to b2 may look up the destina-
tion address a1 and send the packet back to b2, causing a forward-
ing loop. There are several solutions to address this issue [36].
One is to use interdomain GMPLS to setup an interdomain label
switched path (LSP) for the whole interdomain bypass path. As a
second solution, b2 can conﬁgure an intradomain LSP from b2 to
b1, and notify a2 about the LSP. Then a2 can use IP tunneling to
forward packets to b2, where the tunnel header (e.g., shim header)
indicates that the LSP from b2 to b1 should be used.
3. OPTIMAL FAST RE-ROUTING
The preceding section has presented our overall architecture. The
interdomain bypass paths can be utilized in multiple ways. In this
section, we present a fast rerouting algorithm to efﬁciently utilize
these paths. Note that our algorithm applies both with and without
interdomain bypass paths. For simplicity, we also refer to such in-
terdomain bypass paths as interdomain bypass links or virtual links.
The coverage-based path generation technique developed in this
section is also a general tool that can be used to implement other
trafﬁc engineering related algorithms.
3.1 Design Decisions
Network operators conﬁgure their networks for different objec-
tives. We study one speciﬁc formulation for routing under failures.
We make the following design decisions. First, we consider a
link failure as the basic failure unit, as we observe more frequent
link failures in network failure logs; previous studies have also
identiﬁed cable damage as the most common type of physical-layer
failure [19]. Furthermore, it generally takes longer to ﬁx a link fail-
ure than a node failure since it requires sending a crew to the ﬁeld.
Also, a node failure can be treaded as a failure scenario involving
multiple links.
Second, we implement protection, which pre-computes rerout-
ing paths to use upon failure detection, instead of restoration, which
depends on routing re-convergence. There are two basic protection
mechanisms [21]:
link protection (i.e., fast rerouting), and path
protection. In fast rerouting, a detour around the failed link is cre-
ated. In path protection, the sources of all ﬂows using the failed
link are notiﬁed and detour to avoid the failed link. An advan-
tage of path protection is that, since detours are computed for each
source, it can avoid potential bottlenecks around the head end of
the failed link, and thus achieve better rerouting performance. In
this paper, we focus on link protection since it has faster response
time and better scalability. Our scheme extends naturally to path
protection or a hybrid of the two.
We compute fast rerouting for only a subset of all failure sce-
narios, as the total number of failure scenarios is exponential. We
refer to the scenarios for which we compute fast rerouting as the
high-priority failure scenarios. In practice, these scenarios are de-
termined by the operator of an IP network. In our evaluations, we
use single- or two-link failure scenarios. Speciﬁcally, each high-
priority failure scenario consists of one or more SRLGs, where an
SRLG is a set of links that are likely to fail simultaneously. When
a high-priority failure scenario consists of a single SRLG and this
SRLG appears alone in the high-priority scenarios, the head-end
router of each link in the SRLG can detect the failure scenario lo-
cally and switch to the bypass paths. To improve response time for
critical failure scenarios (e.g., failure of two SRLGs that can cause
a major network partitioning as in the Sprint example), we gener-
alize fast rerouting to allow a failure scenario to consist of more
than one SRLG. Then, a detection mechanism is needed to detect
the exact failure scenario, and the response time may be slower
than that of the single SRLG case, but is still faster than that of
a failure scenario outside of the high-priority set, in which the re-
covery depends on re-computation by the routing protocol or the
trafﬁc engineering component. In the rest of this section we focus
on handling the high-priority failure scenarios.
3.2 Algorithm Overview
Given our design decisions, our algorithm naturally consists of
two steps. In the ﬁrst step, we compute optimal routing using traf-
ﬁc engineering when there are no failures. In the second step, we
compute fast rerouting for high-priority failure scenarios on top of
trafﬁc engineering. When we compute fast rerouting, we distin-
guish important trafﬁc (e.g., voice and VPN) and select intradomain
links, if possible, to protect such trafﬁc.
A common issue in both steps is how to handle implementation
feasibility and computational complexity. In terms of implementa-
tion feasibility, since about half of the ISPs have already deployed
MPLS in their core [23] and more ISPs are following suit, we seek
to implement optimal trafﬁc engineering and fast rerouting using
IP/MPLS. However, computation of optimal trafﬁc engineering and
fast rerouting directly using path-based routing (i.e., routing spec-
iﬁed by how trafﬁc is split among LSPs) can be intractable, since
there can be exponential number of candidate LSPs between each
origin-destination (OD) pair. There are previous studies consider-
ing selecting LSPs. For example, one previous proposal is to select
K-shortest paths (e.g., [23]). However, this method does not con-
sider trafﬁc dynamics in the network and can result in poor path
selection.
On the other hand, it is computationally effective to use a rep-
resentation called ﬂow-based routing, in which the routing is spec-
iﬁed at each link by the fraction of trafﬁc of each OD pair that is
routed on this link. However, ﬂow-based routing is fundamentally
different from MPLS routing, and is not readily implementable in
the Internet.
Our methodology is a two-phase process.
In the ﬁrst phase,
we use the ﬂow-based routing representation to make computation
tractable. In the second phase, we use a path generation technique
to convert a ﬂow-based routing into practical implementation. In
Section 3.3, we present trafﬁc engineering and fast rerouting with
VPNs using ﬂow-based routing. The path generation phase will be
presented in Section 3.4.
3.3 Integrated TE/FRR with VPNs
We ﬁrst present our formulation and algorithm for integrated
TE/FRR with VPNs using ﬂow-based routing.
3.3.1 Robust TE with VPN Support
(cid:4)
We represent a network by a graph G = (V, E ∪ E
The IP network ﬁrst conducts trafﬁc engineering to determine
base routing without failures. The uncertainty to handle in this
case is trafﬁc volume variations. We base our TE formulation us-
ing either the traditional oblivious routing technique developed by
Applegate and Cohen [6], or the COPE technique developed by
Wang et al. [42]. We extend their techniques to address VPN sup-
port.
(cid:4)), where V is
the set of routers, E is the set of intradomain links, and E
is the set
of interdomain bypass links. The capacity of link l is denoted by
cap(l).
Let X denote the set of all possible trafﬁc demand matrices. Each
trafﬁc demand matrix is a set of trafﬁc demands d = {dab|a, b ∈ V},
where dab is the trafﬁc demand from a to b. For trafﬁc with desti-
nation outside the network, we always use the technique in [42] to
convert interdomain trafﬁc demand to intradomain trafﬁc demand.
Denote by o( f , d) the performance of ﬂow-based routing f un-
der trafﬁc demand matrix d ∈ X , where the ﬂow-based routing f is
speciﬁed by a set of values f = { fab(i, j)|a, b ∈ V,(i, j) ∈ E} and
fab(i, j) speciﬁes the fraction of demand from a to b that is routed
over the link (i, j). Note that this formulation assumes all trafﬁc
demand will be routed by trafﬁc engineering. We have extended to
the case that most OD pairs are routed using a default routing (e.g.,
OSPF/ISIS), and only selected, major OD pairs (e.g., heavy hit-
ters [28, 35]) are involved in deﬁning f . Furthermore we may need
to aggregate routers inside a PoP for scalability. For clarity, we
omit these extensions. Let o( f ,D) be the aggregated performance
of routing f on the set D, where D ⊂ X is the set of common-
case trafﬁc demands. The aggregation can be done, for example,
by taking the maximum, or a weighted average.
Let c( f , d) be the penalty (cost) of routing f under trafﬁc demand
d. Then the objective of the basic robust TE problem is to search
for a base routing f that optimizes o( f ,D), subject to a worst-case
penalty bound ¯r on c( f , d) for all d ∈ X .
As VPNs are particularly important to ISPs, we add additional
constraints to the preceding robust TE problem formulation. We
use the popular hose model [10, 30] to specify VPN demand. For
each source (or destination) a ∈ V , we denote by ECR(a) (resp.
ICR(a)) the total egress (resp.
ingress) committed rate, which is
the guaranteed total demand to (resp. from) all other nodes inside
the network for VPNs. Then the additional constraints guarantee
bandwidth provisioning for VPNs. Speciﬁcally, they require that
the base routing f be able to route, without overloading any in-
tradomain link l ∈ E, an arbitrary VPN trafﬁc demand matrix dw
that conforms to the ECR and ICR speciﬁcation.
3.3.2 Robust Fast Rerouting
∗
Let the routing computed by the preceding formulation be f
.
∗
The network proceeds to compute fast rerouting f h on top of f
,
to protect against each high-priority link failure scenario h, where
h ⊂ E represents the failure of a set of links belonging to one or
more SRLGs. The fast rerouting computation would use not only
(cid:4)
intradomain links in E, but also interdomain bypass links in E
.
To be robust to trafﬁc variations when a failure scenario happens,
we compute fast rerouting that minimizes the oblivious ratio on all
possible total trafﬁc demands [5].
Due to the high priority and sensitivity of VPN trafﬁc, we com-
pute separate fast reroutings, f h,B for best-effort trafﬁc and f h,V for
VPN trafﬁc, with the requirement that all VPN trafﬁc be completely
rerouted using intradomain links only. However, this requirement
may not be satisﬁable (e.g., under network partitioning). When this
happens, we simply compute a common fast rerouting, f h, for both
best-effort and VPN trafﬁc. The detailed formulation and algorithm
are presented in Appendix.
3.3.3 Extensions to Handle Peering Link Failures
We can extend our algorithm to better handle directly connected
interdomain peering links and take advantage of the point to mul-
tipoint ﬂexibility for interdomain trafﬁc. This appears both in the
normal routing case and in the fast rerouting case. For the fast
rerouting case, when an intradomain link i to j fails, the detour is a
ﬂow from i to j. As a contrast, for an interdomain link from i to a
neighboring network B, it can use multiple peering points at B: b1,
b2, ..., bB, where the b’s are border gateway routers between A and
B. We can thus compute multiple ﬂows (i → b1),(i → b2), . . . ,(i →
bB). We can further extend to allow multiple egress networks.
3.4 Path Generation Based on Flow Routing
The algorithm that we have just developed computes base rout-
ing and fast rerouting using linear programming and generate ﬂow-
based routing. However, a ﬂow-based routing is not readily imple-
mentable in the current Internet. We now provide a general tech-
nique to convert a ﬂow-based routing to a path-based routing with
bounded performance penalty.
Clearly, there is a tradeoff between the number of paths and the
performance we can achieve. Using ﬂow decomposition [2], we
can convert any ﬂow-based routing to a path-based routing using
up to |E| paths per OD pair. However, in an IP network, |E| could
be large. One way to control the number of paths is the path selec-
tion approach proposed by Li et al. [31], which adds penalty terms
in the optimization objective to implicitly limit the path diversity
and thus the number of paths. However, this approach is compu-
tationally expensive and provides no explicit control on either the
number of paths or the performance degradation. There are ap-
proaches to incrementally generate paths, but they are based on the
speciﬁc optimization problem [37].
Our approach, based on the notion of the coverage of a set of
paths, does not depend on how the ﬂow-based routing is derived
and selects effective paths guided by the given ﬂow-based routing.
The algorithm explicitly considers the tradeoff between the number
of paths and the performance gain, and enables one to choose the
paths based on preferences between performance and scalability.
Below, we ﬁrst formalize our notion of selecting effective paths
to approximate a ﬂow-based routing. We then propose an algorithm
to carry out this approximation. Our algorithm has two conﬁg-
urable parameters, with different effects on performance and scala-
bility.
3.4.1 Q-coverage Path Set
We ﬁrst deﬁne the concept of coverage of a set of paths.
Consider a ﬂow-based routing f = { fab(i, j)|a, b ∈ V,(i, j) ∈ E}.
For each OD pair a → b, we construct a graph where each edge
(i, j) has a capacity of fab(i, j). Without loss of generality, we as-
sume that all cycles in f have already been removed, and thus the
graph is a directed acyclic graph (DAG).
|k = 1, . . . , K} be a given set of K paths from a
Let Pab = {Pk
to b. A path-based routing over Pab speciﬁes the fraction of trafﬁc
to be carried by each path in Pab. Speciﬁcally, a path-based rout-
ing over Pab can be represented by a vector pab = {pk
> 0|k =
1, . . . , K}, where pk
ab denotes the fraction of demand from a to b
ab. The value of pab, denoted by |pab|, is
that is routed on path Pk
deﬁned as
|pab| =
(1)
ab
ab
K
.
pk
ab
∑
k=1
A path-based routing pab is valid if its value is 1.
DEFINITION 1. A set Pab of paths from a to b is a Q-percentage
coverage path set (or Q-percentage path set for short) for ﬂow-
based routing fab if there exists a path-based routing pab over Pab
that satisﬁes the following two conditions:
|pab| = Q;