title:Practical Hidden Voice Attacks against Speech and Speaker Recognition
Systems
author:Hadi Abdullah and
Washington Garcia and
Christian Peeters and
Patrick Traynor and
Kevin R. B. Butler and
Joseph Wilson
Practical Hidden Voice Attacks against Speech and
Speaker Recognition Systems
Hadi Abdullah, Washington Garcia, Christian Peeters, Patrick Traynor, Kevin R. B. Butler and Joseph Wilson
{hadi10102, w.garcia, cpeeters, traynor, butler, jnw}@uﬂ.edu
University of Florida
Abstract—Voice Processing Systems (VPSes), now widely de-
ployed, have been made signiﬁcantly more accurate through the
application of recent advances in machine learning. However,
adversarial machine learning has similarly advanced and has
been used to demonstrate that VPSes are vulnerable to the
injection of hidden commands - audio obscured by noise that
is correctly recognized by a VPS but not by human beings. Such
attacks, though, are often highly dependent on white-box knowl-
edge of a speciﬁc machine learning model and limited to speciﬁc
microphones and speakers, making their use across different
acoustic hardware platforms (and thus their practicality) limited.
In this paper, we break these dependencies and make hidden
command attacks more practical through model-agnostic (black-
box) attacks, which exploit knowledge of the signal processing
algorithms commonly used by VPSes to generate the data fed
into machine learning systems. Speciﬁcally, we exploit the fact
that multiple source audio samples have similar feature vectors
when transformed by acoustic feature extraction algorithms (e.g.,
FFTs). We develop four classes of perturbations that create
unintelligible audio and test them against 12 machine learning
models, including 7 proprietary models (e.g., Google Speech API,
Bing Speech API, IBM Speech API, Azure Speaker API, etc), and
demonstrate successful attacks against all targets. Moreover, we
successfully use our maliciously generated audio samples in mul-
tiple hardware conﬁgurations, demonstrating effectiveness across
both models and real systems. In so doing, we demonstrate that
domain-speciﬁc knowledge of audio signal processing represents
a practical means of generating successful hidden voice command
attacks.
I.
INTRODUCTION
Voice Processing Systems (VPSes) are rapidly becoming
the primary means by which users interface with devices. In
particular, the increasing use of constrained/headless devices
(e.g., mobile phones, digital home assistants) has led to their
widespread deployment. These interfaces have been widely
heralded not only for simplifying interaction for traditional
users, but also for dramatically expanding inclusion for dis-
abled communities and the elderly [47], [65].
The driving force behind practical voice-driven systems
has been foundational advances in machine learning. Mod-
els incorporating seemingly ever-increasing complexity now
handle massive quantities of data with ease. When combined
with well-known techniques from signal processing for fea-
ture extraction, such systems now provide highly accurate
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23362
www.ndss-symposium.org
speech and speaker recognition. However, VPSes also intro-
duce substantial security problems. As has been demonstrated
intentionally [45] and unintentionally [52], these interfaces
often recognize and execute commands from any nearby
device capable of playing audio. Moreover, recent research
has demonstrated that attackers with white-box knowledge of
machine learning models can generate audio samples that are
correctly transcribed by VPSes but difﬁcult for humans to
understand [24], [72].
This work takes a different approach. Instead of attacking
speciﬁc machine learning models, we instead take advantage of
the signal processing phase of VPSes. In particular, because
nearly all speech and speaker recognition models appear to
rely on a ﬁnite set of features from classical signal processing
(e.g., frequencies from FFTs, coefﬁcients from MFCs), we
demonstrate that modifying audio to produce similar feature
vectors allows us to perform powerful attacks against machine
learning systems in a black-box fashion.
In so doing, we make the following contributions:
• Develop perturbations during the signal processing
phase: The key insight of this work is that many pieces
of source audio can be transformed into the same feature
vector used by machine learning models. That is, by
attacking the feature extraction of the signal processing
phase as opposed to the model itself, we are able to
generate over 20,000 audio samples that pass for mean-
ingful audio while sounding like unintelligible noise.
Accordingly, VPSes can be attacked quickly (i.e., in a
matter of seconds) and effectively in an entirely black-
box fashion.
• Demonstrate attack and hardware independence: Pre-
vious research has been highly dependent on knowledge
of the model being used and constrained to speciﬁc
pieces of hardware. For example, Yuan et al. evaluated
one white-box and two black-box models [72], while
Carlini et al. evaluated one black-box and one white-
box model [24]. We evaluate our attacks against 12
black-box models that use a variety of machine learning
algorithms,
including seven proprietary and ﬁve local
models; however, in no case do we use any information
about the model or weights (i.e., we treat all models, even
if publicly available, as black boxes). We demonstrate
successful attacks using multiple sets of speakers and
microphones. Our attack evaluation is far more com-
prehensive than prior work in adversarial audio. Such
wide effectiveness of attacks has not previously been
demonstrated.
• Use psychoacoustics to worsen audio intelligibility:
Fig. 1: This generic VPS workﬂow for audio transcription illustrates the various processing steps that are carried out on the audio
before it is passed to the Machine Learning Model for transcription. First, the audio is preprocessed to remove noise. Next, the
audio is passed through a signal processing algorithm that changes the format of the input, retains the important features and
discards the rest. Lastly, these features are given to a machine learning model for inference.
Prior work has focused on adding background noise to ob-
fuscate commands; however, such noise more often than
not reduces successful transcription. We take advantage of
the fact that humans have difﬁculties interpreting speech
in the presence of certain classes of noise within the
range of human hearing (unlike the Dolphin Attack which
injects ultrasonic audio [73] and can therefore easily be
ﬁltered out).
We believe that by demonstrating the above contributions
we take hidden voice commands from the realm of possible to
practical. Moreover, it is our belief that such commands are
not easily countered by making machine learning models more
robust, because the feature vectors that the models receive
are virtually identical whether the VPS is being attacked or
not. Accordingly, it is crucial that system designers consider
adversaries with signiﬁcant expertise in the domain in which
their machine learning model operates.
The remainder of the paper is organized as follows: Sec-
tion II provides background information on signal process-
ing, speech and speaker recognition, and machine learning;
Section III presents our hypothesis; Section IV discusses our
threat model, our methodology for generating perturbations,
and experimental setup; Section VI shows our experimental
results against a range of speech and speaker recognition
models; Section VII discusses psychoacoustics and potential
defenses; Section VIII presents related work; and Section IX
offers concluding remarks.
to both Automatic Speech Recognition (ASR) and Speaker
Identiﬁcation models.
1) ASRs: An ASR converts raw human audio to text.
Generally, most ASRs accomplish this task using the steps
shown in Figure 1: pre-processing, signal processing and
model inference.
Preprocessing involves applying ﬁlters to the audio in order
to remove background noise and any frequencies that are
outside the range of the human audio tract, (Figure 1a and
1b). Signal processing algorithms are then used to capture
the most important features and characteristics, reducing the
dimensionality of the audio. The signal processing step outputs
a feature vector. Most ASRs employ the Mel-Frequency Cep-
strum Coefﬁcient (MFCC) algorithm, for feature extraction,
because of its ability to extrapolate important features, similar
to the human ear. The feature vector is then passed to the
model for either training or inferencing.
2) Speaker Identiﬁcation model: Speaker
Identiﬁcation
models identify the speaker in a recording by comparing
voice samples of the speakers the model was trained on.
The internal workings of the Speaker Identiﬁcation model are
largely similar to that of ASRs, with an additional voting
scheme. For each audio subsample, the model assigns a vote
for the speaker the subsample most likely belongs to. After
processing the entire audio ﬁle, the votes are tallied. The
speaker with the largest number of votes is designated as the
source of the input audio sample.
II. BACKGROUND
B. Signal Processing
A. Voice Processing System (VPS)
Any machine learning based voice processing tool can be
considered a VPS. In this paper, we use the term to refer
Signal processing is a major components of all VPSes.
These algorithms capture only the most important aspects of
the data. The ability of the signal processing algorithm to
properly identify the important aspects of the data is directly
2
Signal ProcessingModel InferenceAudio SampleFFTMel FilteringLog of PowersDCTMagnitudeDCT Coefﬁcients(c)(d)(e)(f)(g)-41352712PreprocessingNoise RemovalLow-Pass Filter(a)(b)Our attack targetAttack target in previous worksrelated to the quality of training a machine learning model
undergoes.
1) Mel-Frequency Cepstrum Coefﬁcient
(MFCC): The
method for obtaining an MFCC vector of an audio sample
begins by ﬁrst breaking up the audio into 20 ms windows. Each
window goes through four major steps as seen in Figure 1.
Fast Fourier Transform (FFT) and Magnitude: For each
window, an FFT and its magnitude are taken which generates
a frequency domain representation of the audio (Figure 1c and
Figure 1d) called a magnitude spectrum. The magnitude spec-
trum details each frequency and the corresponding intensity
that make up a signal.
Mel Filtering: The Mel scale translates actual differences
in frequencies to perceived differences in frequencies by
the human ear. Frequency data is mapped to the Mel scale
(Figure 1e) using triangular overlapping windows, known as
Mel ﬁlter banks.
Logarithm of Powers: To mimic the way in which human
hearing perceives loudness, the energies of each Mel ﬁlter bank
are then put on a logarithmic scale (Figure 1f).
Discrete Cosine Transform (DCT): The ﬁnal step in obtain-
ing the MFCCs is to take the discrete cosine transform of the
list of Mel ﬁlter bank energies (Figure 1g). The result is a
vector of the MFCCs.
2) Other Methods: There are a variety of other signal pro-
cessing techniques used in modern VPSes as well. Examples
of these include Mel-Frequency Spectral Coefﬁcients (MFSC),
Linear Predictive Coding, and Perceptual Linear Prediction
(PLP) [61]. Much like MFCCs, these represent deterministic
techniques for signal processing. Other VPSes employ prob-
abilistic techniques, speciﬁcally transfer learning [69]. In this
case, one model is trained to learn how to extract features
from the input, while the other model is fed these features
for inferencing. This layered approach to VPSes is a recent
development [71]. Additionally, some VPSes, called “end-to-
end” systems, replace all intermediate modules between the
raw input and the model by removing pre-processing and
signal processing steps [35] [37]. These systems aim to remove
the increased engineering effort required in bootstrapping
additional modules, as well as increased processing time and
complicated system deployment [16].
C. Model Inference
Features gathered from the signal processing steps are then
passed onto the machine learning algorithms for inference.
VPSes make extensive use of machine learning algorithms for
speech recognition. However, the deﬁnition of such systems
varies due to the evolving nature of machine learning tech-
niques. In general, a machine learning system can be described
as a learned mapping between a set of inputs X ∈ IRD, for
some input dimensionality D, to a set of outputs Y that consist
of either continuous or categorical values, such that X → Y .
To learn such a mapping, the values are converted to the form
F (x) = y + , where  is an error term to be minimized for
all inputs x ∈ X and known outputs y ∈ Y of a piece of
data known as the training set. A system is said to interpolate
between inputs and outputs of a training set to model the
mapping X → Y . Once the system is trained, it must then
extrapolate the knowledge it learned onto a new, unseen space
of inputs, known as the test set. Rate of error on the test set
is the most common measure of machine learning systems. In
general, any input X to the machine learning system is known
as a feature, as it constitutes a unique trait observable across all
input samples, and ideally, can be used to distinguish between
different samples.
Early machine learning systems specialized in speciﬁc
areas and were designed to interpolate knowledge of some
particular domain, and thus became domain experts [17]. Due
to the speciﬁcity of such systems, they required extensive
feature engineering, often involving human domain knowledge
in order to produce useful results. Thus, much of the early work
in this ﬁeld revolves around feature design. With the advent of
ubiquitous methods of data collection, later methods focused
on data-driven approaches, opting for more general knowledge
rather than domain-speciﬁc inference. Modern systems are
now capable of automatically performing what was previously
known as feature engineering, instead opting for feature ex-
traction from a large, unstructured corpus of data. Thus, these
techniques can automatically learn the correct features relevant
to create the mapping and apply a pre-deﬁned cost function to
minimize the error term. Due to the absence of speciﬁc feature
engineering, these systems are not only more ﬂexible, but can
be split into independent modules to be used across different
domains or applications (i.e., transfer learning), and also offer
much better extrapolation performance.
D. Psychoacoustics
Psychoacoustics is the science of how humans perceive
audio. There is a broad and deep ﬁeld of research dedicated
to this topic that continues to advance our understanding
of human hearing and the way the brain interprets acoustic
signals. Research in this ﬁeld has found many subtleties in the
way people hear and interpret speech. Accordingly, successful
hidden commands must consider psychoacoustic effects.
Studies in psychoacoustics have demonstrated that human
hearing and perception of sound has areas of both robustness
and weakness. When in the presence of multiple sources of
sound, it is easy for humans to focus on a single source. This
is known as the Cocktail Party effect [26], where humans can
tune out sound that is not particularly pertinent. Human hearing
is also robust at interpreting speech in the presence of added
background noise [29]. This means that simply adding noise
to recorded speech will not decrease its intelligibility, only
suppress the volume of the speech.
Alternatively, human hearing and perception can also be
weak, particularly with higher frequencies. Humans hear sound
on a logarithmic scale and tend to perceive higher frequencies
as louder [15]. The Mel scale [46], as well as the concept
of similar frequency masking [32], show that people poorly
discern differences in high frequencies compared to low fre-
quencies. Humans are also poor at interpreting discontinuous
or random sound, whereas they tend to enjoy smooth and
continuous audio signals. However, discontinuous and more
erratic signals are, by deﬁnition, noise [42]. In addition to
being unintelligible,
these signals tend to be jarring and