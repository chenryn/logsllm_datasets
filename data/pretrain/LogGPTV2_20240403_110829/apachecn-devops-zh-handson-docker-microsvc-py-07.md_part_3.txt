$ docker-compose up --build proxy
...
```
现在我们可以访问`localhost:8000`，它将在`thoughts`服务和 404 错误之间交替。
When calling `example.com` this way, we are forwarding the host request. This means we send a request requesting `Host:localhost` to `example.com`, which returns a 404 error. Be sure to check on your service that the same host information is accepted by all the backends.
打开统计页面检查设置:
![](img/50260323-0ec3-4f15-a91e-3e66ece92b0e.png)
检查后端节点中`aws`和`example`的条目。还有很多有趣的信息，比如请求的数量，最后的连接，数据等等。
您可以在检查`example`后端时执行操作，然后在下拉菜单中将状态设置为 MAINT。一旦应用，`example`后端处于维护模式，并从负载平衡器中移除。统计页面如下:
![](img/c4ee4c3e-ae10-4597-b4ff-f3ec513ce6ee.png)
现在访问`localhost:8000`中的负载均衡器只会返回**思想**前端。您可以重新启用后端，将其设置为就绪状态。
There's a state called DRAIN that will stop new sessions going to the selected server, but existing sessions will keep going. This may be interesting in some configurations, but if the backend is truly stateless, moving directly to the MAINT state should be enough.
HAProxy 还可以配置为使用检查来确保后端可用。我们在示例中添加了一个注释的，它发送一个 HTTP 命令来检查返回:
```
option httpchk HEAD / HTTP/1.1\r\nHost:\ example.com
```
支票对两个后端都是一样的，所以需要成功退回。默认情况下，它将每隔几秒钟运行一次。
您可以在[http://www.haproxy.org/](http://www.haproxy.org/)查看完整的 HAProxy 文档。有很多细节可以配置。跟你的团队跟进，确保超时、转发头等区域的配置是正确的。
运行状况检查的概念也在 Kubernetes 中使用，以确保豆荚和容器准备好接受请求并保持稳定。我们将在下一节看到如何确保正确部署新的映像。
# 顺利部署新的 Docker 映像
在生产环境中部署服务时，确保服务能够顺利运行以避免中断服务至关重要。
Kubernetes 和 HAProxy 能够检测服务何时正常运行，并在服务不正常时采取措施，但我们需要提供一个充当运行状况检查的端点，并将其配置为定期 pinged 通，以便及早发现问题。
For simplicity, we will use the root URL as a health check, but we can design specific endpoints to be tested. A good health checkup checks that the service is working as expected, but is light and quick. Avoid the temptation of over testing or performing an external verification that could make the endpoint take a long time.
An API endpoint that returns an empty response is a great example, as it checks that the whole piping system works, but it's very fast to answer.
在 Kubernetes 中，有两种测试可以确保吊舱正常工作，即就绪探测器和活动探测器。
# 活性探针
活性探测器检查容器是否正常工作。这是一个在容器中启动的正确返回的过程。如果它返回一个错误(或者更多，取决于配置)，Kubernetes 将终止容器并重新启动它。
活性探测将在容器内执行，因此它需要是有效的。对于 web 服务，添加一个`curl`命令是一个好主意:
```
spec:
  containers:
  - name: frontend-service
    livenessProbe:
      exec:
        command:
        - curl
        - http://localhost:8000/
        initialDelaySeconds: 5
        periodSeconds: 30
```
虽然有检查 TCP 端口是否打开或发送 HTTP 请求等选项，但运行命令是最通用的选项。也可以出于调试目的对其进行检查。有关更多选项，请参见文档。
Be careful of being very aggressive on liveness probes. Each check puts some load on the container, so depending on load multiple probes can end up killing more containers than they should.
If your services are restarted often by the liveness probe, either the probe is too aggressive or the load is high for the number of containers, or a combination of both.
探头配置为等待 5 秒，然后每 30 秒运行一次。默认情况下，在三次检查失败后，它将重新启动容器。
# 准备就绪探测器
就绪探测器检查容器是否准备好接受更多请求。这是一个不太激进的版本。如果测试返回错误或超时，容器将不会被重新启动，但它将被标记为不可用。
就绪探测器通常用于避免过早接受请求，但它会在启动后运行。智能就绪探测器可以标记容器何时满负荷并且不能接受更多请求，但是通常以类似于活性证明的方式配置的探测器就足够了。
就绪探测器在部署配置中定义，方式与活动探测器相同。让我们来看看:
```
spec:
  containers:
  - name: frontend-service
    readinessProbe:
      exec:
        command:
        - curl
        - http://localhost:8000/
        initialDelaySeconds: 5
        periodSeconds: 10
```
就绪探测器应该比活动探测器更具攻击性，因为这样会更安全。这就是`periodSeconds`更短的原因。您可能需要也可能不需要，这取决于您的特定用例，但是需要准备就绪探测器来启用滚动更新，正如我们接下来将看到的。
示例代码中的`frontend/deployment.yaml`部署包括两个探测器。查看 Kubernetes 文档([https://Kubernetes . io/docs/tasks/configure-pod-container/configure-liveness-ready-start-props/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/))了解更多详细信息和选项。
Be aware that the two probes are used for different objectives. The readiness probe delays the input of requests until the pod is ready, while the liveness probe helps with stuck containers.
A delay in the liveness probe getting back will restart the pod, so an increase in load could produce a cascade effect of restarting pods. Adjust accordingly, and remember that both probes don't need to repeat the same command.
就绪性和活性探测都有助于 Kubernetes 控制如何创建 pods，这会影响部署的更新。
# 滚动更新
默认情况下，每次我们更新部署映像时，Kubernetes 部署都会重新创建容器。
Notifying Kubernetes that a new version is available is not enough to push a new image to the registry, even if the tag is the same. You'll need to change the tag described in the `image` field in the deployment `.yaml` file. 
我们需要控制映像的变化。为了不中断服务，我们需要执行滚动更新。这种更新会添加新的容器，等待它们准备好，将它们添加到池中，并删除旧的容器。这种部署比移除所有容器并重新启动它们要慢一点，但是它允许服务不间断。
如何执行该过程可以通过调整部署中的`strategy`部分进行配置:
```
spec:
    replicas: 4
    strategy:
      type: RollingUpdate
      rollingUpdate:
        maxUnavailable: 25%
        maxSurge: 1
```
让我们理解这段代码:
*   `strategy`和`type`可以是`RollingUpdate`(默认)或`Recreate`，停止现有的吊舱并创建新的吊舱。
*   `maxUnavailable`定义变更期间不可用吊舱的最大数量。这定义了添加新容器和移除旧容器的速度。它可以描述为一个百分比，就像我们的例子一样，或者一个固定的数字。
*   `maxSurge`定义超出所需豆荚数量限制的额外豆荚数量。这可以是一个特定的数字或总数的百分比。
*   当我们将`replicas`设置为`4`时，两种情况下的结果都是一个豆荚。这意味着在变更期间，最多可能有一个 pod 不可用，我们将一个接一个地创建新的 pod。
数字越大，更新速度越快，但会消耗更多资源(`maxSurge`)或在更新过程中更积极地减少可用资源(`maxUnavailable`)。
For a small number of replicas, be conservative and grow the numbers when you are more comfortable with the process and have more resources.
最初，手动缩放豆荚将是最简单和最好的选择。如果流量是高度可变的，具有高峰和低谷，那么自动缩放集群可能是值得的。
# 自动缩放集群
我们之前已经看到了如何更改服务的 pods 数量，以及如何添加和删除节点。这可以自动描述一些规则，允许集群弹性地改变其资源。
Keep in mind that autoscaling requires tweaking to adjust to your specific use case. This is a technique to use if the resource utilization changes greatly over time; for example, if there's a daily pattern where some hours present way more activity than others, or if there's a viral element that means the service multiplies the requests by 10 unexpectedly.
If your usage of servers is small and the utilization stays relatively constant, there's probably no need to add autoscaling. 
集群可以在两个不同的方面自动向上或向下扩展:
*   在 Kubernetes 配置中，豆荚的数量可以设置为自动增加或减少。
*   在 AWS 中，节点的数量可以设置为自动增加或减少。
豆荚的数量和节点的数量都需要相互一致，才能自然生长。
如果在不增加更多硬件(节点)的情况下增加 pod 的数量，Kubernetes 集群将不会有更多的容量，只有以不同分布分配的相同资源。
如果节点数量增加而没有创建更多的 pod，那么在某个时候额外的节点将没有 pod 可分配，从而导致资源利用不足。另一方面，添加的任何新节点都有相关的成本，因此我们希望正确使用它。
To be able to automatically scale a pod, be sure that it is scalable. To ensure the pod is scalable check that it is an stateless web service and obtain all the information from an external source.
Note that, in our code example, the frontend pod is scalable, while the Thoughts and Users Backend is not, as they include their own database container the application connects to.
Creating a new pod creates a new empty database, which is not the expected behavior. This has been done on purpose to simplify the example code. The intended production deployment is, as described before, to connect to an external database instead.
Kubernetes 配置和 EKS 都具有允许根据规则更改吊舱和节点数量的功能。
# 创建 Kubernetes 水平吊舱自动缩放器
在 Kubernetes 术语中，上下缩放吊舱的服务称为**水平吊舱自动缩放器** ( **H** **PA** )。
这是因为它需要一种按比例检查测量的方法。为了启用这些指标，我们需要部署 Kubernetes 指标服务器。
# 部署 Kubernetes 度量服务器
Kubernetes 指标服务器捕获内部低级指标，如 CPU 使用率、内存等。HPA 将获取这些指标，并使用它们来扩展资源。
The Kubernetes metrics server is not the only available server for feeding metrics to the HPA, and other metrics systems can be defined. The list of the currently available adaptors is available in the Kubernetes metrics project ([https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api](https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api)). 
This allows for custom metrics to be defined as a target. Start first with default ones, though, and only move to custom ones if there are real limitations for your specific deployment.
要部署 Kubernetes 度量服务器，请从官方项目页面下载最新版本([https://github . com/Kubernetes-孵化器/metrics-server/releases](https://github.com/kubernetes-incubator/metrics-server/releases) )。在写这篇文章的时候，是`0.3.3`。
下载`tar.gz`文件，写的时候是`metrics-server-0.3.3.tar.gz`。解压缩它并将版本应用于群集:
```
$ tar -xzf metrics-server-0.3.3.tar.gz
$ cd metrics-server-0.3.3/deploy/1.8+/
$ kubectl apply -f .
clusterrole.rbac.authorization.k8s.io/system:aggregated-metrics-reader created
clusterrolebinding.rbac.authorization.k8s.io/metrics-server:system:auth-delegator created
rolebinding.rbac.authorization.k8s.io/metrics-server-auth-reader created
apiservice.apiregistration.k8s.io/v1beta1.metrics.k8s.io created
serviceaccount/metrics-server created
deployment.extensions/metrics-server created
service/metrics-server created
clusterrole.rbac.authorization.k8s.io/system:metrics-server created
clusterrolebinding.rbac.authorization.k8s.io/system:metrics-server created
```
您将在`kube-system`名称空间中看到新的 pod:
```
$ kubectl get pods -n kube-system
NAME                            READY STATUS  RESTARTS AGE
...
metrics-server-56ff868bbf-cchzp 1/1   Running 0        42s
```
您可以使用`kubectl top`命令获取节点和吊舱的基本信息:
```
$ kubectl top node
NAME                    CPU(cores) CPU% MEM(bytes) MEMORY%
ip-X.us-west-2.internal 57m        2%   547Mi      7%
ip-Y.us-west-2.internal 44m        2%   534Mi      7%
$ kubectl top pods -n example
$ kubectl top pods -n example
NAME                              CPU(cores) MEMORY(bytes)
frontend-5474c7c4ff-d4v77         2m         51Mi
frontend-5474c7c4ff-dlq6t         1m         50Mi
frontend-5474c7c4ff-km2sj         1m         51Mi
frontend-5474c7c4ff-rlvcc         2m         51Mi
thoughts-backend-79f5594448-cvdvm 1m         54Mi