dummy subtraction (else branch) branches in the mbedTLS
MM implementation. The two branches are identical, and
both include a for loop that executes two memory writes
(cf. Listing 2). The graph shows the distribution of the
11th instruction in the for loop (a reg to reg subtraction),
highlighting that as long as memory writes are present,
surrounding loop instructions produce different distributions
based on their alignment as well. The distributions were
estimated from 1000 function calls, each of which has 6 loop
iterations, resulting in 6000 measurements per instruction.
5.3 Leaking RSA Keys
We demonstrate a full end-to-end attack leveraging the
Frontal attack by exploiting the function that generates a new
random RSA key pair (mbedtls_rsa_gen_key) in mbedTLS
v2.16.6. This function has several secret-dependent branches.
The one we target is executed during the computation of
gcd(e, (p− 1)(q− 1)), where e is the RSA public exponent
and p and q are two RSA primes. Leaking (p− 1)(q− 1)
allows to easily compute the RSA private key (as together
with n = pq we can solve for p and q and then compute
d = e−1 mod λ(n)). Control-ﬂow leakage from the gcd
implementation has been thoroughly studied [35, 36, 37]
but it only leads to partial information recovery without ﬁne
grained execution traces [35]. The binary gcd implemented
in mbedTLS has a main loop that removes the trailing zero
bits to its operands and then has a balanced branch in which
a subtraction and a shift-right is performed. To recover
the RSA private key it is sufﬁcient to leak two pieces of
information: the output of the function that counts the number
of trailing zero-bits and the path taken in the balanced branch.
We leak the trailing zero-bits by counting the number of
instructions executed in the respective function, as demon-
strated in [23, 37]. The result of the balanced branch is leaked
with the Frontal attack. Similar to the MM attack described
above, the branches need to contain inlined function calls for
the attack to work. To achieve this, we modiﬁed the signature
of
the int mbedtls_mpi_shift_r(...) function in
bignum.c to inline int mbedtls_mpi_shift_r(...).
Note that different compiler versions might lead to the
compiler inlining this function on its own, thus producing
a vulnerable binary. With this function inlined, the branches
672    30th USENIX Security Symposium
USENIX Association
µcode
µcode
-
µcode
HW
HW
HW
HW
HW
HW
HW
yes†
yes†
yes
yes†
yes
yes
yes
yes
yes
yes
yes
µcode Mitig. Vulnerable
0xc2
0xd6
0x48
0x8e
0xb8
0xca
0xb8
0xca
0xca
0xb8
0xca
µarch
Launched
Processor
Q3’15
Skylake
i7-6700HQ
Q3’15
Skylake
i7-6700HQ
Q1’17
Kaby Lake
i7-7700
Kaby Lake
Q1’17
i7-7700
Coffee Lake R Q4’18
i7-9700K
Coffee Lake R Q4’18
i7-9700K
Coffee Lake R Q4’19
i9-9900KS
Coffee Lake R Q4’19
i9-9900KS
i9-10900K
Comet Lake
Q2’20
Xeon E-2278G Coffee Lake R Q2’19
Xeon E-2278G Coffee Lake R Q2’19
† Only vulnerable in some runs (see Figure 9)
Table 2: List of all the processors we tested with their
respective microcode version. The Mitig. column indicates
whether the mitigation against known microarchitecural
attacks such as Spectre and Foreshadow is implemented in
hardware (HW) or µcode.
both contain the loop shown in Listing 3 in the Appendix,
leading to a differently aligned memory write depending on
the branch taken. This loop within the branches is usually
executed 32 times, giving us a fairly high number of memory
writes to proﬁle. We collect and use the information from
the distribution of each instruction in the loop in order to
recognize which branch is being executed. The overall timing
distributions are omitted here due to lack of space, but in
short some instructions look like Figure 8, while others
more like Figure 7. This means that we can classify the
branch whenever any instruction’s timing is in the “slow”
mode of Figure 7 or whenever an instruction’s timing is in
the tail of the distributions of Figure 8. We executed 1000
runs, ﬁxing the exponent to e = 65537, and generating a new
pseudo-random key in each run. Note that since a new key
is generated on each run, we cannot correlate the executions
of multiple runs.
In each execution, the attacked branch
was executed 1018 times on average (std = 25.40), and on
average we could not classify 89 (std = 92.35, median = 55)
branches. This means that on average we would need to brute
force 89 bits to recover the secret key. In practice, we noticed
that since the exponent is orders of magnitude smaller than
(p− 1)(q− 1), early iterations of the secret branch are very
likely not taken. Leveraging this information, we perform
several guesses of the key starting from the last unclassiﬁed
iteration. We assign this iteration as ‘taken‘ and check if this
results in a correct key. If not, we assign the next iteration
as taken as well and repeat. This greedy approach worked
on 65% of the runs and allowed us to recover the key of those
runs in a matter of seconds.
6 Affected Processors and Conﬁgurations
We tested ﬁve different processors from the 6th generation,
which introduced Intel SGX, up to the 10th which has hard-
ware mitigations for recent microarchitectural attacks [38].
We give the details of the CPUs tested in Table 2. For each
Figure 9: Distribution of the attack success rate with different
microcode versions of an Intel Core i7-7700 CPU - across
500 runs per microcode. For each run, we estimate the
attack success rate as the percentage of branches the attacker
guessed correctly among 1000 executed branches from
Figure 2, with alignment X = 6,Y = 2.
processor, we tested the minimum microcode version sup-
plied by the mainboard and the most up to date version as of
February 2020. Each CPU was tested by computing the attack
success rate for various alignments as in Figure 4. The Frontal
attack was successful on all tested CPUs and microcodes.
Our measurements indicate that the processors can be sep-
arated into two groups with similar behavior: processors
with and without hardware mitigations against various mi-
croarchitectural attacks. Interestingly, newer processors with
hardware mitigations built-in were more susceptible to our at-
tack, whereas older processors with mitigations in microcode
seem to add noise and thus have lower success rates on aver-
age. More in-depth analysis revealed that the most recent mi-
crocodes on processors without hardware mitigations increase
the number of cycles used for AEX and ERESUME and add some
randomness to our experiments. For these conﬁgurations, ev-
ery run of the experiment exhibits a different behavior. Fig-
ure 9 shows the success rate for 500 separate runs each with
1000 samples. Note that most of the runs with the new mi-
crocode show a random success rate. However, some runs ex-
hibit a clear timing difference leading to a > 95% success rate.
The adversary can detect which behavior a particular run is go-
ing to exhibit by observing the timings of early movs aligned
at particular addresses. Thus they could decide whether to
attack or not before the secret is retrieved or provisioned, and
relaunch the enclave until its behavior is clearly vulnerable.
7 Potential Causes
The complexity of the microarchitecture of current Intel
processors makes it very challenging to pin-point the cause of
the timing differences to a speciﬁc component. However, we
will discuss some components which we were able to deci-
sively exclude. We start with the memory subsystem, then we
investigate the execution engines, and ﬁnally we will focus
on the frontend. For each potential culprit in these building
blocks, we will describe an initial theory and then try to refute
USENIX Association
30th USENIX Security Symposium    673
or conﬁrm it using performance counters and other mea-
surements. Note that the performance counters are sparsely
distributed over the entire core and do not exhaustively cover
the entire microarchitecture. Therefore, investigation into
some hypotheses is very challenging if no performance
counters exist for the respective part of the processor.
Memory Subsystem Observation 3.1 and 3.2 point to po-
tential causes in the memory subsystem. Speciﬁcally the fact
that the slow mov is around 100 cycles slower. For a current-
generation processor, 100 cycles is a rather large delay that is
usually only observed for accesses to external memory or the
last level cache. However, performance counters refute any
theory related to the memory subsystem since all performance
counters related to external memory or last level cache did
not show a difference between the slow and the fast movs.
Execution Engines The execution engine gets a list of
instructions from the allocation queue as input and tries to re-
order and execute them as fast as possible. As far as we know,
it is completely decoupled from the frontend and does not de-
pend on any alignment since it works on decoded micro-ops.
However, given Observation 2.1, we know that the alignment
inﬂuences the timing difference. We thus rule out the
execution engine as the root cause of the timing differences.
Frontend Observation 2.1 strongly hints at the frontend
as the culprit, since the fetch window is one of the only
structures which operates at a 16 Bytes granularity, matching
the 16 Bytes periodicity of the observations.
The micro-op cache is a microarchitectural structure in the
frontend [39] that holds previously decoded fetch windows
and serves them to, for example, repeated jumps to the
same address. On a micro-op cache hit, many cycles can
be saved due to not having to decode the instructions again.
Our observed timing difference might stem from hits and
misses in this cache. For some interrupts, the micro-op cache
might miss, and the instructions must be decoded again.
Whereas, for some others, it hits and immediately proceeds
to the reorder buffer. However, the timing difference we
observed seems excessively large for this kind of small
difference in the execution path. Besides, performance
counters that measure the behavior of the micro-op cache
show an equivalent number of hits in the slow and the fast
movs. Thus, we rule out the micro-op cache as a cause.
Branch prediction is responsible for predicting the future
control ﬂow. The core will fetch ahead and speculatively
continue to execute in the predicted path. Branches and
jumps where the target is not immediately known (e.g., the
target comes from memory) both rely on the branch predictor
to guess which instruction will be executed next. Hence, the
resumption of the enclave could potentially suffer from a
misprediction on the current enclave instruction and therefore
suffer from a delay. However, all performance counters that
we measured did not show any additional mispredictions for
slow or fast instructions.
Summary While we were able to decisively refute many
of the most common reasons for timing differences, none of
our tests were able to identify with reasonable conﬁdence an
explanation for the observed timings exploited by the attack.
8 Defenses
There exist various defenses against the Frontal attack,
some of which we will discuss in this section. First and
foremost, we want to stress that data-oblivious code [40, 26]
is a principled approach that thwarts every known side or
controlled-channel attack. We discuss these techniques in
Appendix A.2. As such it also remains secure against the
Frontal attack. Nevertheless, data-oblivious code presents
several challenges in practice, as it is hard to get right and
results in high overhead in certain applications. Therefore,
in practice, many spot defenses against the known attacks
have been used since they are usually easier to apply and
more performant. However, most of these spot defenses
are circumvented by new attacks such as the Frontal attack.
While the behavior exploited by the Frontal attack stems
from the underlying hardware, the simple defense we discuss
is at the software level. Hardware mitigations would also
be possible, but due to the lengthy turn-around time for new
processors, software defenses are more attractive.
As seen in Section 4, the execution time of individual in-
structions depends on their alignment. Particularly, branches
with identical alignment do not exhibit any observable timing
difference. Therefore, aligning the two branches to the
same address (modulo 16) leads to indistinguishable timing
distributions for both branches. We evaluated the overhead
in terms of binary size and performance of this approach on
three common libraries: libc, OpenSSL, and mbedTLS. We
used GCC v7.5.0 with the compile ﬂag -falign-jumps=16
- this ﬂag aligns all branch targets to 0x10, thwarting our
attack. The highest size overhead (3.73%) was on one of
the binaries generated for libc, this however was the only
outlier as all the other binaries had an overhead of less than
0.5%. For comparison, compiling with -03 added on average
14% compared to -02. To evaluate performance, we use
libc-bench7 for libc and the benchmarks that come with the
libraries for mbedTLS and OpenSSL. The strstr test in
libc-bench had the highest overhead at 30%, and libc overall
had an average overhead of 1%. Depending on the evaluated
cryptographic function mbedTLS had overheads ranging
from 4% to -5.5%, while OpenSSL from 3% to -4%, showing
that for some cryptographic functions’ implementation the
defense even provides performance boosts.
7https://www.etalabs.net/libc-bench.html
674    30th USENIX Security Symposium
USENIX Association
9 Related Work
We compare our attack and related ones in Table 3. In short,
the main differences lie in the type of branches that are
vulnerable to the various attacks. Previous defenses build
either on the fact that controlled channel attacks cannot
leak at sub-page granularity or that BPU attacks cannot
leak the target virtual address of unconditional branches.
In general, these defenses are ineffective against our attack
since we exploit a fundamentally different mechanism. In the
following, we describe the differences between the Frontal
attack and other related attacks in more detail.
9.1 Controlled-Channel Attacks
The attacker’s control over the OS enables novel noise-
free deterministic side-channels [12, 16, 17] known as
controlled-channels since the attacker controls the channel.
Memory paging, the scheduler, the handling of interrupts and
exceptions, are a few examples of what the attacker can take
advantage of – every interface between the OS and the en-
claves can be leveraged in controlled channel attacks. In [12],
Xu et al. modify page permissions so that the CPU generates a
page fault for each page the enclave tries to access. The trace
of page faults contains enough information to, e.g., let attack-
ers reconstruct images processed in the enclave. Subsequent
attacks made controlled channel attacks stealthier, by observ-
ing that the CPU sets the accessed and dirty bits [17, 16] in
the page tables (PTs), thus allowing to monitor the enclave’s
execution without having to trigger page faults. However,
the resolution of page-based controlled channel attacks is
quite coarse, allowing the attacker to know only whether any
access in a page (4 kB) was made, but not where within it.
The coarseness of PT based controlled channel attacks
is an element that defenses have latched onto, to protect
enclaves [48, 49]. These defenses either call for sensitive
code to be within a page [48] or randomize the enclave’s page
layout so that page accesses cannot be correlated [49]. Even
Intel speciﬁes that controlled channels can be mitigated “by
aligning speciﬁc code and data blocks to exist entirely within
a single page” [18]. However, the resolution of controlled