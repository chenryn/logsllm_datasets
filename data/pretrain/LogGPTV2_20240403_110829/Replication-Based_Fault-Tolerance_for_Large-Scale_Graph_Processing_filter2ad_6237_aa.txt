title:Replication-Based Fault-Tolerance for Large-Scale Graph Processing
author:Peng Wang and
Kaiyuan Zhang and
Rong Chen and
Haibo Chen and
Haibing Guan
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Replication-based Fault-tolerance for Large-scale Graph Processing
Peng Wang† Kaiyuan Zhang† Rong Chen† Haibo Chen† Haibing Guan§
Shanghai Key Laboratory of Scalable Computing and Systems
†Institute of Parallel and Distributed Systems, Shanghai Jiao Tong University
§Department of Computer Science, Shanghai Jiao Tong University
Abstract
The increasing algorithm complexity and dataset sizes
necessitate the use of networked machines for many
graph-parallel algorithms, which also makes fault toler-
ance a must due to the increasing scale of machines. Un-
fortunately, existing large-scale graph-parallel systems
usually adopt a distributed checkpoint mechanism for
fault tolerance, which incurs not only notable perfor-
mance overhead but also lengthy recovery time.
This paper observes that the vertex replicas created
for distributed graph computation can be naturally ex-
tended for fast in-memory recovery of graph states. This
paper proposes Imitator, a new fault tolerance mech-
anism, which supports cheap maintenance of vertex
states by replicating them to their replicas during nor-
mal message exchanges, and provides fast in-memory
reconstruction of failed vertices from replicas in other
machines. Imitator has been implemented by extending
Hama, a popular open-source clone of Pregel. Evalua-
tion shows that Imitator incurs negligible performance
overhead (less than 5% for all cases) and can recover
from failures of more than one million of vertices with
less than 3.4 seconds.
Keywords—graph-parallel system; fault-tolerance;
1
Introduction
Graph-parallel abstraction has been widely used to ex-
press many machine learning and data mining (MLDM)
algorithms, such as topic modeling, recommendation,
medical diagnosis and natural language processing [1],
[2], [3], [4]. With the algorithm complexity and dataset
sizes continuously increase, it is now a common practice
to run many MLDM algorithms on a cluster of ma-
chines. For example, Google has used hundreds to thou-
sands of machines to run some MLDM algorithms [5],
[6], [7].
Many graph algorithms can be programmed by fol-
lowing the “think as a vertex” philosophy [5], by coding
graph computation as a vertex-centric program that
processes vertices in parallel and communicates along
edges. Typically, many MLDM algorithms are essen-
tially iterative computation that iteratively reﬁnes input
data until a convergence condition is reached. Such iter-
ative and convergence-oriented computation has driven
the development of many graph-parallel systems, in-
cluding Pregel [5] and its open-source clones [8], [9],
Trinity [10], GraphLab [11] and PowerGraph [12].
Running graph-parallel algorithms on a cluster of
machines essentially faces a fundamental problem in
distributed systems: fault tolerance. With the increase
of problem sizes (and thus execution time) and ma-
chine scales, the failure probability of machines would
increase as well. Currently, most graph-parallel sys-
tems use a checkpoint-based approach. During com-
putation,
the runtime system will periodically save
the runtime states into a checkpoint on some reliable
global storage, e.g., a distributed ﬁle system. When
some machines crash, the runtime system will reload
the previous computational states from the last check-
point and then restart the computation. Example ap-
proaches include synchronous checkpoint (e.g., Pregel
and PowerGraph) and asynchronous checkpoint using
the Chandy-Lamport algorithm [13] (e.g., Distributed
GraphLab [11]). However, as the processes of check-
point and recovery require saving and reloading from
slow persistent storage, such approaches incur notable
performance overhead during normal execution as well
as lengthy recovery time from a failure. Consequently,
though most existing systems have been designed with
fault tolerance support, they are disabled during produc-
tion run by default.
This paper observes that many distributed graph par-
allel systems require creating replicas of vertices to pro-
vide local access semantics such that graph computation
can be programmed as accessing local memory [14],
[11], [12]. Such replicas can be easily extended to
ensure that
least K+1 replicas
there are always at
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.58
562
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
(including master) for a vertex across machines, in order
to tolerate K machine failures.
Based on this observation, Imitator proposes a new
approach that leverages existing vertex replication to
tolerate machine failures, by extending existing graph-
parallel systems in three ways. First, Imitator extends
existing graph loading phase with fault tolerance sup-
port, by explicitly creating replicas for vertices without
replication. Second, Imitator maintains the freshness of
replicas by synchronizing the full states of a master
vertex to its replicas through extending normal mes-
sages. Third, Imitator extends the graph-computation
engine with fast detection of machine failures through
monitoring vertex states and seamlessly recovers the
crashed tasks from replicas in multiple machines in a
parallel way, inspired by the RAMCloud approach [15].
Imitator uses a randomized approach to locating
replicas for fault tolerance in a distributed and scal-
able fashion. To balance load, a master vertex selects
several candidates at random and then chooses among
them using more detailed information, which provides
near-optimal results with small cost. Imitator currently
supports two failure recovery approaches. The ﬁrst ap-
proach, which is called Rebirth based recovery, recovers
graph states on a new backup machine when a hot-
standby machine for fault tolerance is available. The
second one, the Migration based recovery, distributes
graph states of the failed machines to multiple surviving
machines.
We
have
by
Imitator
implemented
extending
Hama [9], a popular open-source clone of Pregel [5].
To demonstrate the effectiveness and efﬁciency of
Imitator, we have conducted a set of experiments using
four popular MLDM algorithms on a 50-node EC-2
like cluster (200 CPU cores in total). Experiments
show that Imitator can recover from one machine
failure in around 2 seconds. Performance evaluation
shows that Imitator incurs an average of 1.2% (ranging
from -0.6% to 3.7%) performance overhead for all
algorithms and datasets. The memory overhead from
additional replicas is also modest.
This paper makes the following contributions:
• A comprehensive analysis of current checkpoint-
for graph-
tolerance mechanisms
based fault
parallel computation model (Section 2).
• A new replication-based fault tolerance approach
for graph computation (Section 3, Section 4 and
Section 5).
• A detailed evaluation that demonstrates the effec-
tiveness and efﬁciency of Imitator (Section 6).
2 Background and Motivation
This section ﬁrst brieﬂy introduces checkpoint-based
fault tolerance in typical graph-parallel systems. Then,
563
VDPSOHJUDSK
:
UHSOLFD PDVWHU
:(
JOREDOEDUULHU
FKHFNSRLQW
UHFRYHU\:PHWD
UHFRYHU\:GDWD
LWHU:;
LWHU:;
JUDSK
ORDGLQJ
:
:(
6<1&
FNSW:
PHWDGDWD
VQDSVKRW
FNSW:Q
Q
UHFRYHU\
GDWDVQDSVKRW
Q
')6
Fig. 1: The sample of checkpoint-based fault tolerance.
we examine performance issues during both normal
computation and recovery.
2.1 Graph-parallel Execution
Many existing graph-parallel systems usually provide
a shared memory abstraction1 to a graph program. To
achieve this, replicated vertices (vertex 1, 2, 3 and 6 in
Fig. 1) are created in machines where there are edges
connecting to the original master vertex. To enable such
an abstraction, a master vertex synchronizes its states
to its replicas either synchronously or asynchronously
through messages.
The scheduling of computation on vertices can be
synchronous (SYNC) or asynchronous (ASYNC). Fig. 1
illustrates the execution ﬂow of synchronous mode on
a sample graph, which is divided into two nodes (i.e.,
machines). Vertices are evenly assigned to two nodes
with ingoing edges, and replicas are created for edges
spanning nodes. In the synchronous mode, all vertices
are iteratively executed in a ﬁxed order within each it-
eration. A global barrier between consecutive iterations
ensures that all vertex updates in current iteration are
simultaneously visible in next iteration for all nodes
through batched messages. The computation on vertex
in the asynchronous mode is scheduled on the ﬂy, and
uses the new states of neighboring vertices immediately
without a global barrier.
Some graph-parallel systems such as Trinity [10],
PowerGraph [12], GRACE [16] and Giraph++ [17], pro-
vide both execution modes, but usually use synchronous
computation as the default mode. Hence, this paper only
considers synchronous mode. How to extend Imitator to
asynchronous execution will be our future work.
2.2 Checkpoint-based Fault Tolerance
Existing graph-parallel systems implement fault toler-
ance through distributed checkpointing for both syn-
chronous and asynchronous modes. After loading a
graph, each node stores an immutable graph topology of
1. Note that this is a restricted form of shared memory such that a
vertex can only access its neighbors using shared memory abstraction.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
its own graph partition to a metadata snapshot on the
persistent storage. Such information includes adjacent
edges and the location of replicas. During execution,
each node periodically logs updated data of its own
partition to incremental snapshots on the persistent stor-
age, such as new values and states of vertices and edges.
For synchronous mode, all nodes will simultaneously do
logging for all vertices in the global barrier to generate
a consistent snapshot. While for asynchronous mode, all
nodes initiate logging at ﬁxed intervals, and perform a
consistent asynchronous snapshot based on the Chandy-
Lamport algorithm [13]. The checkpoint frequency can
be selected based on the mean time to failure model [18]
to balance the checkpoint cost against
the expected
recovery cost. Upon detecting any node failures, the
graph states will be recovered from the last completed
checkpoint. During recovery, all the nodes ﬁrst reload
the graph topology from the metadata snapshot
in
parallel and then update states of vertices and edges
through data snapshots. Fig. 1 illustrates an example of
checkpoint-based fault tolerance for synchronous mode.
2.3 Issues of Checkpoint-based Approach
systems
graph-parallel
Though many
provide
checkpoint-based fault tolerance support, it is disabled
by default due to notable overhead during normal
computation and lengthy recovery time. To estimate
checkpoint and recovery cost, we evaluate the overhead
of checkpoint
(Imitator-CKPT) based on Apache
Hama [9] 2, an open-source clone of Pregel. Note that
Imitator-CKPT is several
times faster than Hama’s
default checkpoint mechanism (up to 6.5X for Wiki
dataset [19]), as it further improves Hama through
vertex replication to incrementally launch checkpoint
and avoids storing messages in snapshot. Further,
Imitator-CKPT only records
the necessary states
according to the behavior of graph algorithms. For
example, Imitator-CKPT skips edge data for PageRank.
Hence, Imitator-CKPT can be viewed as an optimal
case of prior checkpoint-based approaches.
In the rest of this section, we will use Imitator-
CKPT to illustrate the issues with checkpoint-based
fault tolerance on a 50-node EC-2 like cluster3.
2.3.1 Checkpointing
Checkpointing requires time-consuming I/O operations
to create snapshots of updated data on a globally visible
persistent storage (we use HDFS [20] here). Fig. 2(a)
illustrates the performance cost of one checkpoint for
different algorithms and datasets. The average execution
2. We extended and reﬁned Hama’s checkpoint and recovery mech-
anism as it currently does not support completed recovery and without
optimizations.
3. Detailed experimental setup can be found in section 6.1
564
time of one iteration without checkpointing is also pro-
vided as a reference. The relative performance overhead
of checkpointing for LJournal and Wiki is relatively
small, since HDFS is more friendly to writing large
data. Even for the best case (i.e., Wiki), creating one
checkpoint still incurs more than 55% overhead.
Fig. 2(b) illustrates an overall performance com-
parison between turning on and off checkpointing
on Imitator-CKPT for PageRank with the LJournal
dataset [21] by 20 iterations. We conﬁgure Imitator-
CKPT using HDFS to store snapshots and using dif-
ferent intervals from 1 to 4 iterations. Checkpointing
snapshots to HDFS is not the only cause of overhead.
The imbalance of global barrier also contributes a
notable fraction of performance overhead, since check-
point operation must execute in the global barrier. In
addition, though decreasing the frequency of intervals
can reduce overhead, it may result in snapshots much
earlier than the latest iteration completed before the
failure, and increase the recovery time due to replaying
a large amount of missing computation. The overall per-
formance overhead for intervals 1, 2, and 4 iteration(s)
reaches 89%, 51% and 26% accordingly. Hence, such
a signiﬁcant overhead becomes the main reason to the
limited usage of checkpoint-based fault tolerance for
graph-parallel models in practice.
2.3.2 Recovery
Though most fault tolerance mechanisms focus on min-
imizing overhead in logging, the time for recovery is
also an important metric of fault tolerance. The poor
performance and scalability in recovery is another issue
of checkpoint-based fault tolerance. In checkpoint-based
recovery, a new node for recovery needs to reload states
of the crashed node in last snapshot from the persistent
storage or even through network. The time for recovery
are mainly limited by the I/O performance of the new-
bie node. Even worse, optimizations in checkpointing,
such as incremental snapshot and lower frequency of
intervals, may further increase the recovery time.
Fig. 2(c) illustrates a comparison between the average
execution time of one iteration and recovery on Imitator-
CKPT for PageRank with the LJournal dataset [21]. The
recovery consists of three steps, including (reload)ing
(meta)data, (reconstruct)ing in-memory graph states,
and (replay)ing the missing computation. The reloading
from snapshots on persistent storage incurs the major
overhead, since it only utilizes the I/O resources in
single node.
In addition, a standby node for recovery may not
always be available, especially in a resource-scarce in-
house cluster. It is also impractical to wait for rebooting
of the crashed node. This constrains the usage scenario
of such an approach. Further, as it only enables to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:33:01 UTC from IEEE Xplore.  Restrictions apply. 
Runtime (1 iteration)
CKPT (1 time)
 8
i
)
c
e
S
(
e
m
T
n
o
i
t
u
c
e
x
E
 6
 4
 2
 0
GWeb
LJournal
Wiki
SYN-GL
DBLP
RoadCA
 250
 200
 150
 100
i
)
c
e
S
(
e
m
T
n
o
i
t
u
c
e
x
E
 80
 60
 40
 20
i
)
c
e
S
(
e
m
T
n
o
i
t
u
c
e
x
E
Checkpoint
Barrier
Comm
Comp
Replay
Reconstruct
Reload
Runtime
 50
 0
NO
CKPT
1
2
HDFS
4
4.39
 0
1-iteration
(average)
1
2
HDFS
4
Fig. 2: (a) The performance cost of once checkpointing in Imitator-CKPT for different algorithms and datasets. (b) The breakdown
of overall performance overhead of checkpoint-based fault tolerance for PageRank algorithm with LJournal dataset using different
conﬁguration. (c) The breakdown of recovery time for PageRank algorithm with LJournal dataset using different conﬁguration.
s
e
c
i
t
r
e
V
f
o
t
n
e
c
r
e
P
20%
15%
10%
5%
0%
G
W
Selfish Vertex
Normal Vertex
%
4
8
.
0
%
6
9
.
0
%
6
2
.
0
%
3
1
.
0
W
S
D
R
s
a
c
i
l
p
e
R
f
o
t
n
e
c
r
e
P
0.20%
0.15%