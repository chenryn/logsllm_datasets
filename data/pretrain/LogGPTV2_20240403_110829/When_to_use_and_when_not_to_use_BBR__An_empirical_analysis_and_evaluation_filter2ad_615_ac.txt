### Experimental Setup and Results

In the experimental setup (Figure 2(a)), nodes h1 and h2 send iPerf3 traffic to h3 using BBR and Cubic, respectively. The link from the router to h3 has a 20ms network delay, and the buffer size is varied between 10KB and 100MB. Each iPerf3 experiment runs for 60 seconds under each buffer size.

#### Mininet Experiments

Figure 8(a) presents the results of our Mininet experiments. The bandwidth share between BBR and Cubic is observed to depend on the bottleneck buffer size. For a small buffer size (10KB), BBR utilizes 94% of the network goodput. When the buffer size is large (10MB), Cubic utilizes 3 times more bandwidth than BBR. At moderate buffer sizes (approximately 5MB), BBR and Cubic evenly share the bandwidth.

In terms of retransmissions, BBR exhibits a high retransmission rate when coexisting with Cubic in shallow buffers. Table 1 summarizes the number of retransmissions for BBR and Cubic under different buffer sizes in our Mininet experiments. BBR shows significantly higher retransmissions compared to Cubic for small buffer sizes. For example, at a 100KB buffer size, BBR has 200 times more retransmissions than Cubic. In deep buffers, BBR's retransmissions drop to zero because its packets in flight (cwnd) are much smaller than the buffer capacity.

| Buffer (bytes) | BBR Retr# | Cubic Retr# |
|----------------|-----------|-------------|
| 1e4            | 26746     | 305029      |
| 1e5            | 68741     | 1324        |
| 1e6            | 204       | 0           |
| 5e6            | 908       | 1145        |
| 1e7            | 794       | 7           |
| 5e7            | 1398      | 3987        |
| 1e8            | 0         | 16          |

#### WAN Experiments

We also conducted fairness experiments in our WAN network, using the same Mininet parameters to configure the network conditions. The results in Figure 8(b) show a different behavior: even at high buffer sizes, Cubic’s bandwidth share does not increase, unlike in Figure 8(a). This suggests the presence of a shallow buffer in the WAN between our router and the receiver h3. The goodput of BBR and Cubic stabilizes when the router buffer size reaches 20KB, indicating that the bottleneck buffer in our WAN setup is around 20KB. Regarding retransmissions, BBR's retransmits stabilize at around 500 packets/minute as the router buffer size exceeds 20KB, further confirming the 20KB bottleneck buffer in the WAN.

### Reason for Using Mininet

While we obtained similar results in Section 4.1 for both the LAN and Mininet testbeds, Mininet offers greater flexibility in certain scenarios. In Section 4.3, we used Mininet to create a star topology for the fairness experiment, which was not feasible in our LAN testbed due to the limited number of servers. Additionally, Mininet was used in Sections 4.2 and 4.3 for its convenience and scalability, allowing us to validate and reinforce our WAN results.

### Related Work

BBR’s design was first published in a 2016 ACM article [20]. Since then, several updates have been presented at IETF conferences [7, 9, 21] and in internet drafts [3, 22].

Most prior work on BBR’s performance has focused on its fairness. Hock et al. [30] studied how BBR coexists with Cubic under shallow and deep buffers, finding that BBR gets a larger share of the bandwidth in small buffers, while Cubic dominates in deep buffers. Ma et al. [34] showed that the persistent queue in the bottleneck buffer contributes to BBR’s unfairness. However, these studies either tested very few buffer sizes or used only a single testbed. Our paper not only analyzes BBR’s fairness across a wide range of buffer sizes (10KB to 100MB) under multiple testbeds but also highlights the non-trivial fairness behavior in our WAN setting (Section 4.3).

The high loss rate under BBR has been discussed in recent papers [30, 32, 37], but these works do not investigate the underlying reasons, such as the cliff point (Section 4.2). Some studies have investigated BBR’s performance in specific scenarios. Zhong et al. [39] analyzed BBR in mobile networks, and Atxutegi et al. [18] compared BBR’s performance in live mobile networks with TCP NewReno and Cubic. Our work focuses on BBR’s performance in various wired settings, including LAN and WAN, in addition to Mininet.

### Limitations and Future Work

Our study has several limitations. First, all experimental testbeds, including LAN, Mininet, and WAN, use a simple dumbbell topology to control the bottleneck buffer size. However, real-world networks are more complex. For example, BBR has been used in Google’s B4 network and YouTube video servers [20]. We plan to extend our study to such real-world scenarios in future work.

Second, our experiments consider at most two concurrent TCP flows. In the LAN and Mininet testbeds, we eliminated irrelevant background traffic to focus on the fairness comparison between BBR and Cubic. In real networks, temporary flows can enter and leave the network, affecting the results. We plan to investigate the impact of more competing flows in future work.

Third, this paper primarily focuses on empirical measurements. We have not explored how to use our findings to optimize BBR’s performance. In ongoing work, we are investigating BBR’s design flaws with the goal of enhancing its performance, particularly in mitigating high retransmissions and unfairness issues.

Finally, the key issues revealed by our study, such as cliff points, high retransmissions, and unfairness, are inherent in the current version of BBR. It is unclear whether these issues will persist in future versions of BBR, though there is some online discussion [9, 10] about addressing unfairness in subsequent versions. Nonetheless, our empirical findings and root cause analysis can help the community identify and solve performance issues as BBR evolves.

### Conclusion

Despite the excitement around BBR, there is a lack of studies evaluating its performance across multiple real-world testbeds and parameter settings, especially those that explore why BBR performs the way it does. This paper conducts over 600 experiments under both emulated and real-world testbeds, analyzing the network conditions under which BBR outperforms contemporary algorithms. Our analysis reveals that the relative difference between the bottleneck buffer size and BDP typically dictates BBR’s performance. This finding also applies to BBR’s unfair behavior when coexisting with Cubic; however, in such cases, BBR can be very unfair to competing flows when it performs well. Additionally, our study reveals the existence of a “cliff point” in loss rate, beyond which BBR’s goodput drops abruptly. Our analysis indicates that the pacing_gain parameter in BBR is partly responsible for this behavior.

### Acknowledgment

This work was supported by NSF grants 1566260, 1717588, 1750109, and 1909356.

### References

[References are listed as provided in the original text.]

---

This revised version aims to improve the clarity, coherence, and professionalism of the text, making it more suitable for academic and technical audiences.