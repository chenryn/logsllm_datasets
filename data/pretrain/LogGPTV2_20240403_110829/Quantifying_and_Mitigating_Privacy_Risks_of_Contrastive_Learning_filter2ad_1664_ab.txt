1
2ùëÅ
[‚Ñì(2ùëò ‚àí 1, 2ùëò) + ‚Ñì(2ùëò, 2ùëò ‚àí 1)]
(6)
Here, 2ùëò ‚àí 1 and 2ùëò are the indices for each positive pair.
Training classifiers with SimCLR can be partitioned into two
phases. In the first phase, we train a base encoder as well as a projec-
tion head by the contrastive loss using an unlabeled dataset. After
training, we discard the projection head and keep the base encoder
only. In the second phase, to perform classification tasks, we freeze
the parameters of the encoder, add a trainable linear layer at the end
of the encoder, and fine-tune the linear layer with the cross-entropy
loss (see Equation 3) on a labeled dataset. The linear layer serves as
a classifier, with its input being the representations generated by
the encoder. We refer to this linear layer as the classification layer.
In the rest of the paper, we call a model trained with supervised
learning as a supervised model and a model trained with contrastive
learning as a contrastive model. Also, we consider contrastive mod-
els trained on image datasets, as most of the current development
of contrastive learning focus on images.
Compared to supervised learning, contrastive learning can learn
more informative representations for data samples. Previous work
shows that supervised models are vulnerable to various privacy
attacks [5, 7, 36, 49, 52, 56, 66]. However, to the best of our knowl-
edge, privacy risks stemming from contrastive models have been
left largely unexplored. In this work, we aim to fill this gap.
3 MEMBERSHIP INFERENCE ATTACK
We first quantify the privacy risks of contrastive models through
the lens of membership inference. Note that our goal here is not to
propose a novel membership inference attack, instead, we aim to
quantify the membership privacy of contrastive models. Therefore,
we follow existing attacks and their threat models [10, 33, 49, 52, 57].
3.1 Attack Definition and Threat Model
Membership inference attack is one of the most popular privacy at-
tacks against ML models [7, 8, 10, 19, 28, 31, 33, 49, 52, 57]. The goal
of membership inference is to determine whether a data sample
ùë• is part of the training dataset of a target model T . We formally
define a membership inference attack model AMemInf : ùë•, T ‚Ü¶‚Üí
{member, non-member}. Here, the target model is the contrastive
model introduced in Section 2. A successful membership inference
attack can cause severe privacy risks. For instance, if a model is
trained on data samples collected from people with certain sensitive
information, then successfully inferring a sample from a person be-
ing a member of the model can directly reveal the person‚Äôs sensitive
information.
Following previous work [10, 33, 49, 52, 57], we assume that an
adversary only has black-box access to the target model T , i.e., they
can only query T with their data samples and obtain the outputs. In
addition, the adversary also has a shadow dataset Dshadow, which
comes from the same distribution as the target model‚Äôs training
dataset. The shadow dataset Dshadow is used to train a shadow
model S, the goal of which is to obtain the necessary information
to perform the attack. We further assume that the shadow model
shares the same architecture as the target model [52]. This is realis-
tic as the adversary can use the same machine learning service as
the target model owner to train their shadow model. Alternatively,
the adversary can also learn the target model‚Äôs architecture first by
applying model extraction attacks [40, 41, 60, 63].
3.2 Methodology
We adapt the previous membership inference attacks, which are
designed for supervised models, to contrastive models [10, 49, 52,
57]. Concretely, we consider three types of membership inference
attacks, i.e., NN-based attacks [49, 52], metric-based attacks [57],
and label-only attacks [10].
In NN-
NN-based Attacks (Neural Network-based Attacks).
based attacks, the adversary aims to train an attack model to dif-
ferentiate members and non-members using the posteriors gen-
erated from the target model and their predicted labels. Given a
shadow dataset Dshadow, the adversary first splits it into two dis-
joint sets, namely shadow training dataset Dtrain
shadow and shadow
testing dataset Dtest
shadow is used to train the shadow model
S, which mimics the behavior of the target model. This means the
shadow model is trained to perform the same task as the target
model. Then, the adversary uses Dshadow (including both Dtrain
shadow
and Dtest
shadow) to query the shadow model S and obtains the cor-
responding posteriors and prediction labels. For each data sample
in Dshadow, the adversary ranks its posteriors in descending order
and takes the largest two posteriors (classification tasks considered
in this paper have at least two classes) as part of the input to the
shadow. Dtrain
Session 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea847attack model. The other part is an indicator representing whether
the prediction is correct or not. Thus, the dimension of the input to
AMemInf is 3. If a sample belongs to Dtrain
shadow, the adversary labels
its corresponding input to the attack model as a member, other-
wise as a non-member. Then, this obtained dataset is used to train
the attack model, which is a binary machine learning classifier.
To determine whether a target data sample ùë• is used to train the
target model, the adversary first queries the target model T with
ùë• and obtains the input to the attack model for this sample. Then,
the adversary queries this input to the attack model and gets its
membership prediction.
Metric-based Attacks. Song and Mittal [57] propose several
metric-based attacks. Similar to NN-based attacks, metric-based
attacks need to train shadow models. However, instead of training
an attack model, metric-based attacks leverage a certain metric
and a predefined threshold on that metric (calculated over the
shadow model) to determine a sample‚Äôs membership status. Song
and Mittal [57] propose four metrics, i.e., prediction correctness
(metric-corr), prediction confidence (metric-conf), prediction en-
tropy (metric-ent), and modified prediction entropy (metric-ment).
Label-only Attacks. Label-only attacks [10] consider a more re-
strict scenario where the target model only exposes the predicted
label instead of posteriors. Similar to previous attacks, this attack
requires the adversary to train a shadow model. Label-only attacks
focus more on the input samples instead of the model‚Äôs outputs,
relying on the adversarial example techniques. The key intuition is
that the magnitude of perturbation to change the predicted label
of member samples is larger than that of non-member samples.
The adversary can exploit the magnitude of the perturbation to
distinguish members and non-members.
3.3 Experimental Settings
Datasets. We utilize 8 different image datasets to conduct our
experiments for membership inference.
‚Ä¢ CIFAR10 [1]. This dataset contains 60,000 images in 10
classes. Each class represents one object and has 6,000 images.
The size of each image is 32 √ó 32.
‚Ä¢ CIFAR100 [1]. This dataset is similar to CIFAR10, except it
has 100 classes, with each class containing 600 images. The
size of each image is also 32 √ó 32.
‚Ä¢ STL10 [11]. This dataset is composed of 10 classes of images.
Each class has 1,300 samples. The size of each image is 96 √ó
96. Besides the labeled image, STL10 also contains 100,000
unlabeled images, which we use for pretraining the encoder
for the contrastive model (detailed later). These images are
extracted from a broader distribution compared to those with
labeled classes.
‚Ä¢ CelebA [35]. This dataset is composed of more than 200,000
celebrities‚Äô facial images. Note that in CelebA, we randomly
select 60,000 images for our experiments. We set its target
model‚Äôs classification task as gender classification.
‚Ä¢ UTKFace [68]. This dataset consists of over 23,000 facial
images labeled with gender, age, and race. We set its target
model‚Äôs classification task as gender classification as well.
‚Ä¢ Places365 [69]. This dataset is composed of more than 1.8
million images with 365 scene categories. We randomly se-
lect 100 scene categories and randomly select 400 images
per category to form the Places100 dataset. Besides, we
randomly select 50 (20) scene categories and randomly se-
lect 800 (2,000) images per category to form the Places50
(Places20) dataset. Each dataset contains 40,000 images in
total. We follow Song and Shmatikov [56] and set its target
model‚Äôs classification task as predicting whether the scene
is indoor or outdoor.
target, Dtest
target, Dtrain
shadow and Dtest
shadow, and Dtest
All the datasets are used to evaluate membership inference at-
tacks, while UTKFace, Places100, Places50, and Places20 are also
used to evaluate attribute inference attacks since they have ex-
tra labels that can be used as sensitive attributes (see Section 4.3).
For all the datasets, we rescale their images to the size of 96 √ó
96. Note that we concentrate on image datasets as it is the most
prominent domain for applying contrastive learning at the mo-
ment [9, 18, 20, 29, 61, 67]. We leave our investigation in other data
domains as future work.
Datasets Configuration. For each dataset, we first split it into
four equal parts, i.e., Dtrain
shadow. Dtrain
target
is used to train the target model T , the samples of which are thus
considered as members of the target model. We treat Dtest
target as
non-members of the target model T . Dtrain
shadow is used to train the
shadow model S, and Dtrain
shadow are used to create the
attack model AMemInf.
Metric. Since the attack model‚Äôs training and testing datasets are
both balanced with respect to membership distribution, we adopt
accuracy as our evaluation metric following previous work [49, 52].
Attack Model. For NN-based attacks, the attack model is a 3-layer
MLP, and the number of neurons for each hidden layer is set to
32. We use cross-entropy as the loss function and Adam as the
optimizer with a learning rate of 0.05. The attack model is trained
for 100 epochs. For metric-based attacks, we follow the implemen-
tation of Song et al. [57]. For label-only attacks, we leverage the
implementation of ART [2].
Target Model (Contrastive Model). We adopt three popular neu-
ral network architectures as the contrastive model‚Äôs base encoder
ùëì in our experiments, including MobileNetV2 [50], ResNet-18 [21],
and ResNet-50 [21]. Specifically, we discard the last classification
layer of MobileNetV2, ResNet-18, and ResNet-50 and use the re-
maining parts as ùëì . Then, a 2-layer MLP is added after ùëì as the
projection head ùëî. For ResNet-18, the dimensions for the output
of ùëì , the first-layer of ùëî, and the second-layer of ùëî are set to 512,
512, and 256, respectively. For ResNet-50, the corresponding dimen-
sions are 2,048, 256, and 256. For MobileNetV2, the corresponding
dimensions are 1,280, 256, and 256.
After training the base encoder with the contrastive loss, we
ignore the projection head ùëî and add a new linear layer to the
base encoder ùëì as its classification layer. For all datasets, we first
use the unlabeled dataset of STL10 to pretrain the base encoder
ùëì for 100 epochs. Then, we fine-tune the base encoder ùëì with the
corresponding training dataset (without label) for 100 epochs. In
the end, we freeze the parameters of ùëì and use the corresponding
training dataset to only fine-tune the classification layer for 100
Session 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea848(a) Supervised Model
(b) Contrastive Model
Figure 1: The performance of original classification tasks for both supervised models and contrastive models with Mo-
bileNetV2, ResNet-18, and ResNet-50 on 8 different datasets. The x-axis represents different datasets. The y-axis represents
original classification tasks‚Äô accuracy.
(a) Supervised Model
(b) Contrastive Model
Figure 2: The performance of different membership inference attacks against both supervised models and contrastive models
with MobileNetV2 on 8 different datasets. The x-axis represents different datasets. The y-axis represents membership infer-
ence attacks‚Äô accuracy.
epochs to establish the contrastive model. In all cases, Adam is
utilized as the optimizer.
Baseline (Supervised Model). To fully understand the privacy
leakage of contrastive models, we further use supervised models
as the baseline. We train three models including MobileNetV2,
ResNet-18, and ResNet-50 from scratch for all the datasets. The
models are trained for 100 epochs. Cross-entropy is adopted as the
loss function, and we again use Adam as the optimizer. Our code is
currently implemented in Python 3.6 and PyTorch 1.6.0, and run
on an NVIDIA DGX-A100 server with Ubuntu 18.04.
3.4 Results
We first show the performance of supervised models and contrastive
models on their original classification tasks in Figure 1. We observe
that contrastive models perform better than supervised models on
most of the datasets. For instance, on STL10 with ResNet-18 as the
base encoder, the contrastive model achieves 0.726 accuracy while
the supervised model achieves 0.538 accuracy.
(a) Supervised Model
(b) Contrastive Model
Figure 3: The distribution of loss with respect to original
classification tasks for member and non-member samples
for both the supervised model and the contrastive model
with ResNet-18 on CIFAR10. The x-axis represents each sam-
ple‚Äôs classification loss. The y-axis represents the number of
member and non-member samples.
Regarding membership inference against supervised models and
contrastive models, the results for MobileNetV2 are shown in Fig-
ure 2. We also summarize the results for ResNet-18 (Figure 15) and
CIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places200.00.20.40.60.81.0AccuracyMobileNetV2ResNet-18ResNet-50CIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places200.00.20.40.60.81.0AccuracyCIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places200.40.50.60.70.80.91.0AccuracyNN-basedMetric-corrMetric-confMetric-entMetric-mentLabel-onlyCIFAR10CIFAR100STL10CelebAUTKFacePlaces100Places50Places200.40.50.60.70.80.91.0Accuracy0.000.250.500.751.00ClassiÔ¨ÅcationLoss101103NumberMemberNon-member0.000.250.500.751.00ClassiÔ¨ÅcationLoss101103NumberSession 3C: Inference Attacks CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea849the one hand, these defense mechanisms for adversarial examples
use original samples and their visually imperceptible adversarial
examples to train a model; in this way, the model learns to remem-
ber each original sample more accurately. On the other hand, the
augmented samples in contrastive learning are very different from
their original samples (see Figure 4 for some examples). Therefore,
membership inference is less effective against contrastive models.
We notice that the attack performance varies on different models
and different datasets. We relate this to the different overfitting