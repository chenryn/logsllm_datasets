81
Unbiased Experiments in Congested Networks
IMC ’21, November 2–4, 2021, Virtual Event, USA
(a) A/B tests without congestion interference
(b) A/B tests with congestion interference
Figure 1: A/B tests are used to estimate the total treatment effect: how much better a treatment is than control if both were
deployed globally. A/B Tests give accurate estimates of Total Treatment Effect (TTE) when there is no interference between
sessions as in (a), but may be misleading when there is as in (b).
control), when the treatment allocation is 𝑝:
(cid:20)𝑖∈𝑇 𝑌𝑖(𝐴)
(cid:21)
|𝑇 |
.
𝜇𝑇 (𝑝) = E
𝑇 ⊂𝑈
Depending on the setting and the treatment, 𝜇𝑇 (𝑝) may or may not
depend on the treatment allocation 𝑝. This is visually depicted in
Figure 1. 𝜇𝑇 (𝑝) is the purple treatment line, and 𝜇𝐶(𝑝) is the pink
control line.
Average treatment effect. An A/B test evaluates the average
treatment effect. This is how much better the treatment group
performs than the control group, when a 𝑝 fraction of the traffic is
allocated to treatment and 1 − 𝑝 to control. It is defined as:
𝜏(𝑝) = 𝜇𝑇 (𝑝) − 𝜇𝐶(𝑝),
(1)
This is visually depicted in Figure 1. The treatment effect at any
point on the graph is the difference between the treatment and
control lines.
Total Treatment Effect. When evaluating a new algorithm, we
are often interested in what would happen if we were to deploy
it widely. This is the Total Treatment Effect, or TTE: the difference
between the average outcome when all flows are in treatment and
when all flows are in control. In terms of our notation above:
TTE = 𝜇𝑇 (1) − 𝜇𝐶(0).
in the TTE for a single network or a group of networks. This can be
incorporated into the definition by changing the set of treatment
and control flows.
Spillover. In addition to how well a new algorithm performs
on its own, we are often also interested in how a new algorithm
impacts existing algorithms. Recently, [84] defined the notion of
the “harm” of a new algorithm, which is the negative effect caused
by a new algorithm competing with an existing algorithm. This net-
working concept is similar to the concept of spillovers in the causal
inference literature (e.g. [21, 37]). Formally, we define the spillover
of treatment on control as the effect of increasing the treatment
fraction to 𝑝 on control units, relative to when the treatment units
were not present. In terms of our notation:
𝑠(𝑝) = 𝜇𝐶(𝑝) − 𝜇𝐶(0).
Spillover is non-zero when deploying a treatment algorithm has
some impact on the control algorithm. This is shown in Figure 1b.
Note that spillover is only defined for 𝑝 < 1. If 𝑝 = 1, there is no
control traffic and no spillover can occur.
Spillovers may or may not be undesirable. It is possible that
deploying a new algorithm can improve existing traffic, and we will
see examples of this later.
Estimating from A/B tests All the quantities above are expec-
tations over the distribution of all possible treatment assignments.
Any experiment has only one set of treatment assignments and can
only observe one set of potential outcomes—all other potential out-
comes are missing. The fundamental problem in causal inference is
to reason about these missing outcomes given what we observe.
In causal inference, we use the observed outcomes to estimate the
quantities above. An estimator is called unbiased for some quantity
if its expectation is equal to that quantity.
In an A/B test we randomly allocate units to treatment or control,
and measure
This process gives an unbiased estimator of 𝜇𝑇 (𝑝), since E(cid:99)𝜇𝑇 (𝑝) =
𝜇𝑇 (𝑝), and similarly for 𝜇𝐶(𝑝). By linearity of expectation,
.
𝑖∈𝑇 𝑌𝑖(𝐴)
(cid:99)𝜇𝑇 (𝑝) =
(cid:98)𝜏(𝑝) =(cid:99)𝜇𝑇 (𝑝) −(cid:99)𝜇𝐶(𝑝)
|𝑇 |
This is depicted in Figure 1: it is the difference between the right-
hand side of the treatment line (when all traffic is treated), and the
left-hand side of the control line (when all traffic is allocated to
control). Depending on the setting, it may or may not equal the
average treatment effect.
Note that this definition of TTE is from the perspective of the
experimenter, and not the internet. The experimenter may only
control a small fraction of all traffic on the internet, and in this
case TTE measures what happens if they switched all traffic under
their control to a new algorithm. The TTE is also sometimes called
the “global average treatment effect” in causal inference work (e.g.,
[51]), but we have avoided this name to avoid confusion around
this point.
It is also reasonable to talk about TTE in specific groups of traffic.
For instance, we may be interested in the TTE if we were to move
all traffic globally to a new algorithm, but we may be also interested
3
82
IMC ’21, November 2–4, 2021, Virtual Event, USA
Spang et al.
is an unbiased estimator for 𝜏(𝑝), and we can define similar estima-
tors(cid:100)TTE, and(cid:98)𝑠(𝑝).
that they use(cid:99)𝜇𝑇 (𝑝) and(cid:99)𝜇𝐶(𝑝) as an unbiased estimate of the aver-
Congestion Interference In virtually all real-world experi-
ments in networking today, experimenters run an A/B test. They
infer that an improvement in the A/B test implies an improvement
if the treatment were to be deployed. In our notation, this means
age treatment effect 𝜏(𝑝), and then interpret 𝜏(𝑝) as if it were the
TTE. This is what we refer to as “naïve" A/B testing.
This process gives an unbiased estimate of TTE only in the very
special case when the outcome of a unit does not depend on the
fraction of other units allocated to treatment. This is part of the
Stable Unit Treatment Value Assumption (SUTVA) [45], and re-
quires that TTE = 𝜏(𝑝) for all 𝑝, and that spillovers are zero for
all 𝑝. Visually, this process assumes that algorithms behave like
Figure 1a and not Figure 1b.
Any A/B test that runs over a congested network has a clear
pathway for interference between units in the treatment and control
groups. Any explicit or implicit change in how the treatment group
uses the congested network can create a different network condition
for the control groups, which may lead to different behavior. This is
especially true if the test explicitly changes the timing of how traffic
is sent, or the amount of traffic that uses the network. Because of
this, we will refer to violations of SUTVA as congestion interference.
Note on averages Average treatment effects, spillovers, and
TTE are all defined as averages. Average here refers to the distri-
bution of units in the A/B test, and not the outcome metric. The
average treatment effect could measure the average difference in
average latency, but it could also measure the variance of average la-
tency or 99th percentile latency. Practitioners may also be interested
in quantile treatment effects, e.g. the difference in 99th percentile
latency between treatment and control. These are regularly esti-
mated from A/B test results [1, 78]. It is straightforward to adapt
our definitions to measure quantile treatment effects, and could be
done by replacing 𝜇𝑇 (𝑝) and 𝜇𝐶(𝑝) with quantile estimators.
3 SMALL LAB EXPERIMENTS
When interference is present, naïve A/B tests do not accurately
describe the behavior of a new algorithm. They mispredict the
TTE and give no estimate of spillover. To illustrate this, we set up
a small test network in the lab. The lab setup gives us a global
view of how a new algorithm performs at any fraction allocation,
and lets us recreate Figure 1 for actual algorithms. With these
results, we can look at the results of different A/B tests, estimate
TTE, and measure spillover. These experiments do not tell us how
different algorithms would behave at scale, but they provide easy-
to-understand examples of how congestion interference causes bias
in naïve A/B tests.
Lab Setup Our lab consists of two servers running Linux 5.5.0,
each with an Intel 82599ES 10Gb/s NIC. Each NIC is connected to a
port of a 6.5Tb/s Barefoot Tofino switch via 4× 10Gb/s breakout ca-
bles. The switch has a 1 BDP buffer. The sender server is connected
to the Tofino with two 10G cables. The interfaces are bonded and
packets are equally split between them, which ensures that conges-
tion happens at the switch (otherwise we only see congestion at the
sender NIC). We set MTUs to 9000 bytes so the servers can sustain a
4
83
10Gb/s rate. We add 1ms of delay at the sender using Linux’s traffic
controller tc, and use iperf3 to generate TCP traffic.
3.1 Test 1: Multiple connections
Web browsers, video streaming clients, and other applications re-
quest data over multiple TCP connections in parallel. Making si-
multaneous requests reduces head-of-line blocking, reduces page
load time, and increases utilization [35, 36, 69, 73]. This behavior
depends heavily on the particular ways an application uses TCP
connections and the particular networks it traverses, and so would
typically be evaluated with a large-scale A/B test.
However, using multiple TCP connections can also allow an
application to outcompete its peers and achieve higher throughput,
and so is often called “unfair” in the academic literature [8, 15]. This
makes it an ideal example to illustrate how congestion interference
can bias A/B tests.
We ran an experiment in the lab to illustrate this behavior and
understand the bias it causes. We ran eleven tests in which ten
applications used either one or two TCP Reno connections to trans-
fer bulk data. We measured the average long-term throughput and
retransmission rates experienced by each application.
Figure 2a shows the results of the lab tests. Each test has two
boxplots showing the average throughput for applications using
one or two connections. Applications using two connections had
100% higher throughput and identical retransmission rates than
applications using one. As more applications used two connections,
their average throughput decreased. When all applications used
two connections, their average throughput was identical to when
all applications used one. Even worse, retransmission rates were
higher when all applications used two connections.
These results are because of the way TCP fairly shares through-
put between connections. If 𝑛 identical TCP connections share a
bottleneck link of capacity 𝐶, we expect each to receive a long-term
average throughput of 𝐶/𝑛. A group of flows with two connections
should get a throughput of 2𝐶/𝑛, 100% larger than 𝐶/𝑛. But funda-
mentally, increasing the number of connections does not increase
the capacity of the link so there can be no overall improvement.
This behavior is a well-understood consequence of TCP Reno’s
throughput fairness. But suppose we followed common practice [17–
19, 24, 25, 29, 42, 46, 49, 55, 57, 58, 60, 63, 72, 87] and ran an A/B
test to measure how using two parallel connections performed.
To illustrate the potential for bias, we will use the same data set
interpreted in a different way.
In a naïve A/B test, we would randomly allocate some fraction
of traffic to treatment and the rest to control. Treatment would use
two connections and the rest would use one. We would compare
the throughput and retransmissions of the treatment and control
groups. No matter what allocation we picked, we would see that two
connections have a 100% higher throughput than one, and that there
was no impact on retransmission rates. The naïve interpretation is
that we should always use two connections in production.
TTE and spillover give us a better idea of how two connections
perform. The TTE shows that there would be no improvement in
throughput and a 200% increase in the percentage of retransmitted
bytes if all traffic were switched to two connections. Spillovers
allow us to measure the impact of using two connections on other
Unbiased Experiments in Congested Networks
IMC ’21, November 2–4, 2021, Virtual Event, USA
(a) Units are applications using 1 or 2 long-lived TCP connections.
(b) Units are TCP connections which either pace traffic or not.
Figure 2: Throughput and retransmits in experiments where 10 units share a 10 Gb/s link. Every point on the x-axis is a
different A/B test. All tests suggest a large change in throughput and no change in retransmissions, but the difference between
10 treated and 10 control units (TTE) is zero for throughput and large for retransmissions.
applications. When nine applications use two connections, the
spillovers on the one remaining application using one connection
are a 25% decrease in throughput and an almost 175% increase in
retransmissions.
These results demonstrate that any single A/B test would not
accurately measure the impact of changing the number of connec-
tions. But we should be careful not to extrapolate too much from the
lab results. Applications may benefit from being more aggressive,
but using multiple connections can also increase utilization. With-
out more experimentation, either could be a plausible explanation
for a measured increase in throughput. Fundamentally, we believe
that the only way to accurately measure the performance of such a
policy would be to run an experiment at scale, on real traffic. We
will discuss how to run such experiments later in Section 5.
3.2 Test 2: Pacing
Pacing is a generic, widely-used mechanism for reducing packet
burstiness in a network [2, 17, 61, 67]. With pacing, a host adds
delay between successive packets so that it sends a smooth, evenly
paced stream of data into the network.
The Linux Kernel has supported pacing for TCP since 2013 [26,
27]. It adds delay between successive packets to ensure a rate of 2×
𝑐𝑤𝑛𝑑/𝑅𝑇𝑇 during slow start and 1.2×𝑐𝑤𝑛𝑑/𝑅𝑇𝑇 during congestion
avoidance [79].