-
(96/3,202.203/16):5.4 (0/2,202.203/16):5.6
(112/4,202.192/12):5.2 (64/2,202.192/12):9.0
-
(0/1,203.179.128/20):6.0 (128/2,202.203/16):5.5
(192/4,202/8):5.1 (*,202.192/12):25.5
(16/4,202/7):5.4 (128/1,202.128/9):10.6
(64/2,202/7):15.5 (128/1,202/7):17.7
-
(163.229/16,0/1):6.0 (144/4,128/1):5.3
(128/2,96/3):5.0 (128/3,0/1):5.3 (160/3,128/1):7.0
(128/2,0/2):5.7 (128/2,0/1):11.4
(128/1,160/6):5.0 (192/4,128/2):5.2
(0/1,128/2):22.7 (*,128/3):7.1
(202/7,0/2):5.4 (192/8,128/1):5.6 (202/8,0/1):5.7
(202/7,128/1):6.0 (192/3,200/5):10.5
(128/1,112/6):5.1 (112/5,128/1):21.8 (200/5,*):17.0
(192/4,128/1):13.6 (128/1,16/4):6.2 (*,200/5):42.4
(64/3,128/1):6.0 (96/3,128/1):29.7
(128/1,64/2):10.4 (0/1,128/1):46.7 (128/1,*):53.3
(*,128/1):78.3
6.7
-
5.2
-
33.1
-
while SS reports much larger 52 HHHs due to its overlap rollup rule
resulting in the inflated total count of(cid:80) c′/N = 6.84 by double-
counting. Even though the two methods differ in the definition
(ordering and rollup rules), the results are comparable. Both meth-
ods have the identical HHHs for (1)-(9), with one exception that (7)
has different percentage due to the different rollup rules. The HHHs
only appearing in SS are listed in the rightmost column under the
corresponding ancestor in RLS. Among 40 HHHs found only by
SS, 35 fall into Region (I), four into Region (II) and one into Region
(III). Most of them have very short prefix lengths, and do not add
much information for operational purposes. Although some have
longer prefixes, they are due to double-counting by the overlap rule
and their related (more representative) HHHs can be found in the
RLS output (e.g., (8) for (0/1, 203.179.128/20)). Generally, HHHs in
Region (I) are not so informative; we will introduce heuristics to
further suppress such HHHs in Section 5.2.
Overall, capturing noteworthy HHHs is not so sensitive to sub-
tle differences in the aggregation ordering or the rollup rule. The
overlap rule of SS produces a lengthy and redundant summary for
bitwise aggregation, while concise and compact reports by RLS
better meet needs for traffic monitoring and anomaly detection.
Finally, we compare the CPU time and memory usage of RLS and
SS for bytewise (5×5) and bitwise (33×33) aggregations in Figure 5,
using a standard desktop PC with a 4-core CPU (Intel Core-i7 3770K
3.5GHz) and 16GB DRAM. The CPU time grows linearly with the
input size for all cases. The difference between bytewise and bitwise
is less than a factor of 2 for RLS while a factor of 70 for SS. RLS is
about twice faster than SS for bytewise aggregation, and about 100
times faster for bitwise aggregation. RLS can process more than
2M packets per second for bitwise aggregation, corresponding to
10Gbps with mean packet size of 512 bytes. The memory usage of
RLS is proportional to the input size as a non-streaming algorithm
that starts with all inputs buffered, while the SS uses a fixed size of
memory. However, several GB of memory usage is not an issue for
modern PCs and, if needed, we can use a shorter aggregation period
and re-aggregate the results for a summary report as described in
Section 5.3.
5 IMPLEMENTATION
In this section, we briefly cover our implementations related to
the algorithm. We have developed an HHH-based traffic monitor-
ing tool named agurim [12], and we are using it to monitor the
WIDE [22] backbone traffic since 2013. Agurim extensively uses
re-aggregation. The primary aggregation creates a rudimentary list
of aggregated flows by efficiently processing raw traffic data such
as pcap, NetFlow and sFlow. The secondary aggregation re-aggre-
gates its own (primary and secondary) outputs to update coarser
hourly and daily records. For visualizing time-series, records with
an appropriate temporal granularity for the plotting period are
selected and further re-aggregated. A user can dynamically switch
views based on traffic volume or packet counts, address or protocol
attributes, with different temporal and spatial granularities on the
Web user interface. In addition, we have made anonymized datasets
openly available to provide broader access to backbone traffic for
the networking community3.
3http://mawi.wide.ad.jp/~agurim/ (add ‘dataset/’ for raw data)
IMC ’17, November 1–3, 2017, London, United Kingdom
Kenjiro Cho
in the range of [65, 127] is not aggregated, as the lower 64 bits of an
IPv6 address are used for an interface ID and are not hierarchical.
For ports, we found that, even though some applications use certain
port ranges, they are rare in occurrence and often buried in noise,
so that we use only a wildcard for aggregation.
(a) CPU Time
(b) Memory Usage
Figure 5: Performance Comparison: RLS vs. SS
We started using agurim for traffic monitoring since February
2013. Our original implementation [12] employed a variant of the
cross-producting method for the primary aggregation and a variant
of the naive algorithm for the secondary aggregation. We switched
to the new RLS algorithm for the secondary aggregation since May
2016, and for the primary aggregation since December 2016.
5.1 2-Level HHH
In the networking context, it makes sense to have two planes: one
for address pairs and the other for port pairs. Although the HHH
algorithm can be extended for four dimensions, the search space
grows exponentially with the dimension. Thus, agurim employs two
levels of 2-dimensional HHH: the first level for the main attribute
pairs (source-destination addresses) and the second level for the sub-
attribute pairs (source-destination ports) under each main attribute
pair. Agurim allows one to swap the main attribute and sub-attribute
for the protocol-port view in which flows are aggregated first by
port pairs and then by address pairs within each port pair.
5.2 Protocol Specific Heuristics
RLS allows one to control aggregation granularity for part of the
hierarchy by limiting the depth of recursions. Using protocol spe-
cific knowledge, we have added heuristics to suppress entries not
so useful for operation and to make concise summaries. For IPv4,
when a prefix length is shorter than 16, the granularity is reduced
from д = 1 to д = 8 so as not to produce HHHs with short prefix
lengths. Similarly, for IPv6, when a prefix length is shorter than 32,
the granularity is reduced to д = 16. In addition, the prefix length
5.3 Online Processing
Our original motivation was to use RLS for agurim’s secondary
aggregation whose inputs are limited in number. We realized, how-
ever, that RLS is fast enough to be used for the primary aggregation,
and using the same algorithm makes the code simpler and outputs
more consistent. For online processing, we employ multi-threading
and double-buffering to take advantage of a multi-core CPU; one
thread keeps reading raw inputs, switches input buffers at the end
of aggregation periods and wakes up another thread that aggregates
the inputs in the buffer and produces a summary report.
The current bottleneck in agurim is not the aggregation algo-
rithm but the cost to maintain the inputs. We use a hash table to
keep track of input packets by their 5-tuple. The larger the hash
table grows, the higher the cost becomes for search.
To control the number of inputs in the hash table as well as to
reduce memory usage, a user can optionally set the aggregation
period to a fraction of the summary period. Then, inputs are ag-
gregated in a shorter cycle (e.g., every 3 seconds) and a summary
is produced by re-aggregating the intermediate results in a longer
cycle (e.g., every 30 seconds). The same technique is used for DoS
resilience; when the buffered packet count exceeds a predefined
limit, early aggregation is invoked.
6 CONCLUSION
In this paper we have introduced a new efficient HHH algorithm.
Our key insight is to revisit the commonly accepted definition of
HHH, and apply the Z-ordering to make use of a recursive parti-
tioning algorithm. The Z-order makes the ordering consistent with
ancestor-descendant relationship in the hierarchy, and it transforms
the HHH problem into simple space partitioning of a quadtree.
The proposed algorithm produces concise and compact sum-
maries capturing HHHs, satisfies our operational needs, and runs
faster than the existing methods by orders of magnitude for bitwise
aggregation.
This work is part of our ongoing effort to provide practical tools
and datasets for traffic monitoring and networking research. The
proposed algorithm has been integrated into our traffic monitoring
tool and used for operation. The source code of the tool is available
along with open longitudinal dataset starting from 2013 that can
be browsed on the Web4.
ACKNOWLEDGMENTS
We thank Midori Kato and Arthur Carcano for contributing to the
agurim development, and Yuji Sekiya and Ryo Nakamura for oper-
ational support. We would like to thank Kensuke Fukuda, Romain
Fontugne, the anonymous IMC reviewers and our shepherd, John
Byers, for their valuable feedback and comments on the paper.
4More information is available at http://mawi.wide.ad.jp/~agurim/about.html
 0.01 0.1 1 10 100 1000 10000 0.1 1 10 100CPU time (sec)input N (million packets)RLS 5x5RLS 33x33SS 5x5SS 33x33 0.1 1 10 100 1000 0.1 1 10 100Memory usage (MB)input N (million packets)RLS 5x5RLS 33x33SS 5x5SS 33x33Recursive Lattice Search
IMC ’17, November 1–3, 2017, London, United Kingdom
[20] V. Srinivasan, G. Varghese, S. Suri, and M. Waldvogel. 1998. Fast and Scal-
able Layer Four Switching. In Proceedings of the ACM SIGCOMM ’98 Confer-
ence on Applications, Technologies, Architectures, and Protocols for Computer
Communication (SIGCOMM ’98). ACM, New York, NY, USA, 191–202. https:
//doi.org/10.1145/285237.285282
[21] Da Tong and Viktor Prasanna. 2015. High Throughput Hierarchical Heavy Hitter
Detection in Data Streams. In Proceedings of the 2015 IEEE 22Nd International
Conference on High Performance Computing (HiPC) (HIPC ’15). IEEE Computer
Society, Washington, DC, USA, 224–233. https://doi.org/10.1109/HiPC.2015.30
[22] WIDE Project 2017. WIDE Project web page. (2017). Retrieved September 28,
2017 from http://www.wide.ad.jp/
[23] Lihua Yuan, Chen-Nee Chuah, and Prasant Mohapatra. 2007. ProgME: To-
wards Programmable Network Measurement. In Proceedings of the 2007 Con-
ference on Applications, Technologies, Architectures, and Protocols for Computer
Communications (SIGCOMM ’07). ACM, New York, NY, USA, 97–108. https:
//doi.org/10.1145/1282380.1282392
[24] Yin Zhang, Sumeet Singh, Subhabrata Sen, Nick Duffield, and Carsten Lund. 2004.
Online Identification of Hierarchical Heavy Hitters: Algorithms, Evaluation, and
Applications. In Proceedings of the 4th ACM SIGCOMM Conference on Internet
Measurement (IMC ’04). ACM, New York, NY, USA, 101–114. https://doi.org/10.
1145/1028788.1028802
REFERENCES
[1] Ran Ben Basat, Gil Einziger, Roy Friedman, Marcelo C. Luizelli, and Erez Waisbard.
2017. Constant Time Updates in Hierarchical Heavy Hitters. In Proceedings of the
Conference of the ACM Special Interest Group on Data Communication (SIGCOMM
’17). ACM, New York, NY, USA, 127–140. https://doi.org/10.1145/3098822.3098832
[2] Kevin Beyer and Raghu Ramakrishnan. 1999. Bottom-up Computation of Sparse
and Iceberg CUBE. In Proceedings of the 1999 ACM SIGMOD International Confer-
ence on Management of Data (SIGMOD ’99). ACM, New York, NY, USA, 359–370.
https://doi.org/10.1145/304182.304214
[3] Kenjiro Cho, Ryo Kaizaki, and Akira Kato. 2001. Aguri: An Aggregation-Based
Traffic Profiler. In Proceedings of the Second International Workshop on Quality of
Future Internet Services (COST 263). Springer-Verlag, London, UK, UK, 222–242.
http://dl.acm.org/citation.cfm?id=646462.693721
[4] Kenjiro Cho, Koushirou Mitsuya, and Akira Kato. 2000. Traffic Data Repository
at the WIDE Project. In Proceedings of the Annual Conference on USENIX Annual
Technical Conference (ATEC ’00). USENIX Association, Berkeley, CA, USA, 51–51.
http://dl.acm.org/citation.cfm?id=1267724.1267775
[5] Graham Cormode and Marios Hadjieleftheriou. 2008. Finding Frequent Items in
Data Streams. Proc. VLDB Endow. 1, 2 (Aug. 2008), 1530–1541. https://doi.org/10.
14778/1454159.1454225
[6] Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh Srivastava. 2003.
Finding Hierarchical Heavy Hitters in Data Streams. In Proceedings of the 29th
International Conference on Very Large Data Bases - Volume 29 (VLDB ’03). VLDB
Endowment, 464–475. http://dl.acm.org/citation.cfm?id=1315451.1315492
[7] Graham Cormode, Flip Korn, S. Muthukrishnan, and Divesh Srivastava. 2004.
Diamond in the Rough: Finding Hierarchical Heavy Hitters in Multi-dimensional
Data. In Proceedings of the 2004 ACM SIGMOD International Conference on Man-
agement of Data (SIGMOD ’04). ACM, New York, NY, USA, 155–166. https:
//doi.org/10.1145/1007568.1007588
[8] Cristian Estan, Stefan Savage, and George Varghese. 2003. Automatically Inferring
Patterns of Resource Consumption in Network Traffic. In Proceedings of the
2003 Conference on Applications, Technologies, Architectures, and Protocols for
Computer Communications (SIGCOMM ’03). ACM, New York, NY, USA, 137–148.
https://doi.org/10.1145/863955.863972
[9] R. A. Finkel and J. L. Bentley. 1974. Quad Trees a Data Structure for Retrieval
on Composite Keys. Acta Inf. 4, 1 (March 1974), 1–9. https://doi.org/10.1007/
BF00288933
[10] John Hershberger, Nisheeth Shrivastava, Subhash Suri, and Csaba D. Tóth. 2005.
Space Complexity of Hierarchical Heavy Hitters in Multi-dimensional Data
Streams. In Proceedings of the Twenty-fourth ACM SIGMOD-SIGACT-SIGART
Symposium on Principles of Database Systems (PODS ’05). ACM, New York, NY,
USA, 338–347. https://doi.org/10.1145/1065167.1065211
[11] Lavanya Jose, Minlan Yu, and Jennifer Rexford. 2011. Online Measurement of
Large Traffic Aggregates on Commodity Switches. In Proceedings of the 11th
USENIX Conference on Hot Topics in Management of Internet, Cloud, and Enterprise
Networks and Services (Hot-ICE’11). USENIX Association, Berkeley, CA, USA,
13–13. http://dl.acm.org/citation.cfm?id=1972422.1972439
[12] Midori Kato, Kenjiro Cho, Michio Honda, and Hideyuki Tokuda. 2012. Monitoring
the Dynamics of Network Traffic by Recursive Multi-Dimensional Aggregation.
In Presented as part of the 2012 Workshop on Managing Systems Automatically
and Dynamically. USENIX, Hollywood, CA. https://www.usenix.org/conference/
mad12/workshop-program/presentation/Kato
[13] Yunqi Li, Jiahai Yang, Changqing An, and Hui Zhang. 2007. Finding Hierarchical
Heavy Hitters in Network Measurement System. In Proceedings of the 2007 ACM
Symposium on Applied Computing (SAC ’07). ACM, New York, NY, USA, 232–236.
https://doi.org/10.1145/1244002.1244061
[14] Donald Meagher. 1982. Geometric Modeling Using Octree Encoding. Computer
Graphics and Image Processing 19 (1982), 249–270.
[15] M. Mitzenmacher, T. Steinke, and J. Thaler. 2012. Hierarchical Heavy Hitters
with the Space Saving Algorithm. In Proceedings of the Meeting on Algorithm
Engineering & Expermiments (ALENEX ’12). Society for Industrial and Applied
Mathematics, Philadelphia, PA, USA, 160–174. http://dl.acm.org/citation.cfm?id=
2790265.2790281
[16] G. M. Morton. 1966. A Computer Oriented Geodetic Data Base and a New Technique
in File Sequencing. Technical Report. IBM Ltd.
[17] Masoud Moshref, Minlan Yu, Ramesh Govindan, and Amin Vahdat. 2014. DREAM:
Dynamic Resource Allocation for Software-defined Measurement. In Proceedings
of the 2014 ACM Conference on SIGCOMM (SIGCOMM ’14). ACM, New York, NY,
USA, 419–430. https://doi.org/10.1145/2619239.2626291
[18] Diana Andreea Popescu, Gianni Antichi, and Andrew W. Moore. 2017. Enabling
Fast Hierarchical Heavy Hitter Detection Using Programmable Data Planes. In
Proceedings of the Symposium on SDN Research (SOSR ’17). ACM, New York, NY,
USA, 191–192. https://doi.org/10.1145/3050220.3060606
[19] Hanan Samet. 1984. The Quadtree and Related Hierarchical Data Structures. ACM
Comput. Surv. 16, 2 (June 1984), 187–260. https://doi.org/10.1145/356924.356930