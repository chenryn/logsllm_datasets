2
models, these equation systems can be efficiently solved,
thus recovering f (or some good approximation of it).
Our approach for evaluating attacks will primarily
be experimental. We use a suite of synthetic or pub-
licly available data sets to serve as stand-ins for propri-
etary data that might be the target of an extraction at-
tack. Table 3 displays the data sets used in this section,
which we obtained from various sources: the synthetic
ones we generated;
the others are taken from public
surveys (Steak Survey [26] and GSS Survey [49]), from
scikit [42] (Digits) or from the UCI ML library [35].
More details about these data sets are in Appendix B.
Before training, we remove rows with missing values,
apply one-hot-encoding to categorical features, and scale
all numeric features to the range [−1,1]. We train our
models over a randomly chosen subset of 70% of the
data, and keep the rest for evaluation (i.e., to calculate
Rtest). We discuss the impact of different pre-processing
and feature extraction steps in Section 5, when we evalu-
ate equation-solving attacks on production ML services.
4.1.1 Binary logistic regression
As a simple starting point, we consider the case of logis-
tic regression (LR). A LR model performs binary clas-
sification (c = 2), by estimating the probability of a bi-
nary response, based on a number of independent fea-
tures. LR is one of the most popular binary classifiers,
due to its simplicity and efficiency. It is widely used in
many scientific fields (e.g., medical and social sciences)
and is supported by all the ML services we reviewed.
Formally, a LR model is defined by parameters w ∈
Rd, β ∈ R, and outputs a probability f1(x) = σ (w· x +
β ), where σ (t) =1/(1 +e−t ). LR is a linear classifier: it
defines a hyperplane in the feature space X (defined by
w· x + β = 0), that separates the two classes.
Given an oracle sample (x, f (x)), we get a linear equa-
tion w·x+β = σ−1( f1(x)). Thus, d +1 samples are both
necessary and sufficient (if the queried x are linearly in-
dependent) to recover w and β . Note that the required
USENIX Association  
25th USENIX Security Symposium  605
samples are chosen non-adaptively, and can thus be ob-
tained from a single batch request to the ML service.
We stress that while this extraction attack is rather
straightforward, it directly applies, with possibly devas-
tating consequences, to all cloud-based ML services we
considered. As an example, recall that some services
(e.g., BigML and Google) let model owners monetize
black-box access to their models. Any user who wishes
to make more than d + 1 queries to a model would then
minimize the prediction cost by first running a cross-
user model extraction attack, and then using the extracted
model for personal use, free of charge. As mentioned in
Section 3, attackers with a final goal of model-inversion
or evasion may also have incentives to first extract the
model. Moreover, for services with black-box-only ac-
cess (e.g., Amazon or Google), a user may abuse the ser-
vice’s resources to train a model over a large data set D
(i.e., |D| (cid:30)d ), and extract it after only d + 1 predictions.
Crucially, the extraction cost is independent of |D|. This
could undermine a service’s business model, should pre-
diction fees be used to amortize the high cost of training.
For each binary data set shown in Table 3, we train a
LR model and extract it given d + 1 predictions. In all
cases, we achieve Rtest = Runif = 0. If we compare the
probabilities output by f and ˆf , RTV
unif are lower
than 10−9. For these models, the attack requires only 41
queries on average, and 113 at most. On Google’s plat-
form for example, an extraction attack would cost less
than $0.10, and subvert any further model monetization.
test and RTV
4.1.2 Multiclass LRs and Multilayer Perceptrons
We now show that such equation-solving attacks broadly
extend to all model classes with a ‘logistic’ layer, includ-
ing multiclass (c > 2) LR and deeper neural networks.
We define these models formally in Appendix A.
A multiclass logistic regression (MLR) combines c bi-
nary models, each with parameters wi,βi, to form a mul-
ticlass model. MLRs are available in all ML services we
reviewed. We consider two types of MLR models: soft-
max and one-vs-rest (OvR), that differ in how the c bi-
nary models are trained and combined: A softmax model
fits a joint multinomial distribution to all training sam-
ples, while a OvR model trains a separate binary LR for
each class, and then normalizes the class probabilities.
A MLR model f is defined by parameters w ∈ Rcd,
βββ ∈ Rc. Each sample (x, f (x)) gives c equations in w and
βββ . The equation system is non-linear however, and has
no analytic solution. For softmax models for instance,
j=0 ew j·x+β j ) =
the equations take the form ewi·x+βi/(∑c−1
fi(x). A common method for solving such a system is
by minimizing an appropriate loss function, such as the
logistic loss. With a regularization term, the loss func-
tion is strongly convex, and the optimization thus con-
530
530
2,225
OvR
MLP
Model
Softmax
1− Rtest
1− Runif
99.96% 99.75%
100.00% 100.00%
99.98% 99.98%
100.00% 100.00%
98.17% 94.32%
98.68% 97.23%
99.89% 99.82%
99.96% 99.99%
Unknowns Queries
265
530
265
530
1,112
2,225
4,450
11,125
Time (s)
2.6
3.1
2.8
3.5
155
168
195
89
Table 4: Success of equation-solving attacks. Models to extract
were trained on the Adult data set with multiclass target ‘Race’. For
each model, we report the number of unknown model parameters, the
number of queries used, and the running time of the equation solver.
The attack on the MLP with 11,125 queries converged after 490 epochs.
verges to a global minimum (i.e., a function ˆf that pre-
dicts the same probabilities as f for all available sam-
ples). A similar optimization (over class labels rather
than probabilities) is actually used for training logistic
models. Any MLR implementation can thus easily be
adapted for model extraction with equation-solving.
This approach naturally extends to deeper neural net-
works. We consider multilayer perceptrons (MLP), that
first apply a non-linear transform to all inputs (the hid-
den layer), followed by a softmax regression in the trans-
formed space. MLPs are becoming increasingly popular
due to the continued success of deep learning methods;
the advent of cloud-based ML services is likely to further
boost their adoption. For our attacks, MLPs and MLRs
mainly differ in the number of unknowns in the system
to solve. For perceptrons with one hidden layer, we have
w ∈ Rdh+hc, βββ ∈ Rh+c, where h is the number of hidden
nodes (h = 20 in our experiments). Another difference
is that the loss function for MLPs is not strongly convex.
The optimization may thus converge to a local minimum,
i.e., a model ˆf that does not exactly match f ’s behavior.
To illustrate our attack’s success, we train a softmax
regression, a OvR regression and a MLP on the Adult
data set with target ‘Race’ (c = 5). For the non-linear
equation systems we obtain, we do not know a priori
how many samples we need to find a solution (in con-
trast to linear systems where d +1 samples are necessary
and sufficient). We thus explore various query budgets
of the form α · k, where k is the number of unknown
model parameters, and α is a budget scaling factor. For
MLRs, we solve the equation system with BFGS [41]
in scikit [42]. For MLPs, we use theano [51] to run
stochastic gradient descent for 1,000 epochs. Our experi-
ments were performed on a commodity laptop (2-core In-
tel CPU @3.1GHz, 16GB RAM, no GPU acceleration).
Table 4 shows the extraction success for each model,
as we vary α from 0.5 to at most 5. For MLR models
(softmax and OvR), the attack is extremely efficient, re-
quiring around one query per unknown parameter of f
(each query yields c = 5 equations). For MLPs, the sys-
tem to solve is more complex, with about 4 times more
606  25th USENIX Security Symposium 
USENIX Association
USENIX Association  
25th USENIX Security Symposium  607
(a)(b)Figure2:TrainingdataleakageinKLRmodels.(a)Displays5of20trainingsamplesusedasrepresentersinaKLRmodel(top)and5of20extractedrepresenters(bottom).(b)Forasecondmodel,showstheaverageofall1,257representersthatthemodelclassifiesasa3,4,5,6or7(top)and5of10extractedrepresenters(bottom).unknowns.Withasufficientlyover-determinedsystem,weconvergetoamodelˆfthatverycloselyapproximatesf.AsforLRmodels,queriesarechosennon-adaptively,soAmaysubmitasingle‘batchrequest’totheAPI.WefurtherevaluatedourattacksoverallmulticlassdatasetsfromTable3.ForMLRmodelswithk=c·(d+1)parameters(cisthenumberofclasses),kqueriesweresufficienttoachieveperfectextraction(Rtest=Runif=0,RTVtestandRTVunifbelow10−7).Weuse260samplesonaverage,and650forthelargestmodel(Digits).ForMLPswith20hiddennodes,weachieved>99.9%accu-racywith5,410samplesonaverageand11,125atmost(Adult).With54,100queriesonaverage,weextractedaˆfwith100%accuracyovertestedinputs.AsforbinaryLRs,wethusfindthatcross-usermodelextractionat-tacksforthesemodelclassescanbeextremelyefficient.4.1.3TrainingDataLeakageforKernelLRWenowmovetoalessmainstreammodelclass,kernellogisticregression[57],thatillustrateshowextractionat-tackscanleakprivatetrainingdata,whenamodel’sout-putsaredirectlycomputedasafunctionofthatdata.Kernelmethodsarecommonlyusedtoefficientlyex-tendsupportvectormachines(SVM)tononlinearclas-sifiers[14],butsimilartechniquescanbeappliedtolo-gisticregression[57].ComparedtokernelSVMs,kernellogisticregressions(KLR)havetheadvantageofcom-putingclassprobabilities,andofnaturallyextendingtomulticlassproblems.Yet,KLRshavenotreachedthepopularityofkernelSVMsorstandardLRs,andarenotprovidedbyanyMLaaSprovideratthetime.WenotethatKLRscouldeasilybeconstructedinanyMLlibrarythatsupportsbothkernelfunctionsandLRmodels.AKLRmodelisasoftmaxmodel,wherewere-placethelinearcomponentswi·x+βibyamapping∑sr=1αi,rK(x,xr)+βi.Here,Kisakernelfunction,andtherepresentersx1,...,xsareachosensubsetofthetrainingpoints[57].MoredetailsareinAppendixA.Eachsample(x,f(x))fromaKLRmodelyieldscequationsovertheparametersααα∈Rsc,βββ∈Rcandtherepresentersx1,...,xs.Thus,byqueryingthemodel,Aobtainsanon-linearequationsystem,thesolutionofwhichleakstrainingdata.ThisassumesthatAknowstheexactnumbersofrepresenterssampledfromthedata.However,wecanrelaxthisassumption:First,notethatf’soutputsareunchangedbyadding‘extra’representers,withweightsα=0.Thus,over-estimatingsstillresultsinaconsistentsystemofequations,ofwhichasolutionisthemodelf,augmentedwithunusedrepresenters.WewillalsoshowexperimentallythattrainingdatamayleakevenifAextractsamodelˆfwiths(cid:27)(cid:26)srepresenters.WebuildtwoKLRmodelswitharadial-basisfunction(RBF)kernelforadatasetofhandwrittendigits.Wese-lect20randomdigitsasrepresentersforthefirstmodel,andall1,257trainingpointsforthesecond.Weextractthefirstmodel,assumingknowledgeofs,bysolvingasystemof50,000equationsin1,490unknowns.WeusethesameapproachasforMLPs,i.e.,logistic-lossmin-imizationusinggradientdescent.Weinitializetheex-tractedrepresenterstouniformlyrandomvectorsinX,asweassumeAdoesnotknowthetrainingdatadistribu-tion.InFigure2a,weplot5ofthemodel’srepresentersfromthetrainingdata,andthe5closest(inl1norm)ex-tractedrepresenters.Theattackclearlyleaksinformationonindividualtrainingpoints.Wemeasuretheattack’sro-bustnesstouncertaintyabouts,byattackingthesecondmodelwithonly10localrepresenters(10,000equationsin750unknowns).Figure2bshowstheaverageimageoftrainingpointsclassifiedasa3,4,5,6or7bythetar-getmodelf,alongwith5extractedrepresentersofˆf.Surprisinglymaybe,theattackseemstobeleakingthe‘averagerepresentor’ofeachclassinthetrainingdata.4.1.4ModelInversionAttacksonExtractedModelsAccesstoamodelmayenableinferenceofprivacy-damaginginformation,particularlyaboutthetrainingset[4,23,24].ThemodelinversionattackexploredbyFredriksonetal.[23]usesaccesstoaclassifierftofindtheinputxoptthatmaximizestheclassprobabilityforclassi,i.e.,xopt=argmaxx∈Xfi(x).Thiswasshowntoallowrecoveryofrecognizableimagesoftrainingsetmembers’faceswhenfisafacialrecognitionmodel.Theirattacksworkbestinawhite-boxsetting,wheretheattackerknowsfanditsparameters.Yet,theauthorsalsonotethatinablack-boxsetting,remotequeriestoapredictionAPI,combinedwithnumericalapproximationtechniques,enablesuccessful,albeitmuchlessefficient,attacks.Furthermore,theirblack-boxattacksinherentlyrequireftobequeriedadaptively.Theyleaveasanopenquestionmakingblack-boxattacksmoreefficient.Weexplorecomposinganattackthatfirstattemptstoextractamodelˆf≈f,andthenusesitwiththe[23]white-boxinversionattack.Ourextractiontechniquesre-placeadaptivequerieswithanon-adaptive“batch”querytof,followedbylocalcomputation.Weshowthatex-tractionplusinversioncanrequirefewerqueriesandlesstimethanperformingblack-boxinversiondirectly.As a case study, we use the softmax model from [23],
trained over the AT&T Faces data [5]. The data set con-
sists of images of faces (92 × 112 pixels) of 40 peo-
ple. The black-box attack from [23] needs about 20,600
queries to reconstruct a recognizable face for a single
training set individual. Reconstructing the faces of all 40
individuals would require around 800,000 online queries.
The trained softmax model is much larger than those
considered in Section 4.1, with 412,160 unknowns (d =
10,304 and c = 40). We solve an under-determined sys-
tem with 41,216 equations (using gradient descent with
200 epochs), and recover a model ˆf achieving RTV
test,RTV
unif
in the order of 10−3. Note that the number of model
parameters to extract is linear in the number of people c,
whose faces we hope to recover. By using ˆf in white-box
model inversion attacks, we obtain results that are visu-
ally indistinguishable from the ones obtained using the
true f . Given the extracted model ˆf , we can recover all
40 faces using white-box attacks, incurring around 20×
fewer remote queries to f than with 40 black-box attacks.
For black-box attacks, the authors of [23] estimate a
query latency of 70 milliseconds (a little less than in our
own measurements of ML services, see Table 1). Thus,
it takes 24 minutes to recover a single face (the inversion
attack runs in seconds), and 16 hours to recover all 40 im-
ages. In contrast, solving the large equation system un-
derlying our model-extraction attack took 10 hours. The
41,216 online queries would take under one hour if exe-
cuted sequentially and even less with a batch query. The
cost of the 40 local white-box attacks is negligible.
Thus, if the goal is to reconstruct faces for all 40 train-
ing individuals, performing model inversion over a pre-
viously extracted model results in an attack that is both
faster and requires 20× fewer online queries.
4.2 Decision Tree Path-Finding Attacks
Contrary to logistic models, decision trees do not com-
pute class probabilities as a continuous function of their
input. Rather, decision trees partition the input space into
discrete regions, each of which is assigned a label and
confidence score. We propose a new path-ﬁnding attack,
that exploits API particularities to extract the ‘decisions’
taken by a tree when classifying an input.
Prior work on decision tree extraction [7, 12, 33] has
focused on trees with Boolean features and outputs.
While of theoretical importance, such trees have limited
practical use. Kushilevitz and Mansour [33] showed that
Boolean trees can be extracted using membership queries
(arbitrary queries for class labels), but their algorithm
does not extend to more general trees. Here, we propose
attacks that exploit ML API specificities, and that apply
to decision tree models used in MLaaS platforms.
Our tree model, defined formally in Appendix A, al-
lows for binary and multi-ary splits over categorical fea-
tures, and binary splits over numeric features. Each leaf
of the tree is labeled with a class label and a confidence
score. We note that our attacks also apply (often with bet-
ter results) to regression trees. In regression trees, each
leaf is labeled with a real-valued output and confidence.
The key idea behind our attack is to use the rich in-
formation provided by APIs on a prediction query, as a
pseudo-identiﬁer for the path that the input traversed in
the tree. By varying the value of each input feature, we
then find the predicates to be satisfied, for an input to
follow a given path in the tree. We will also exploit the
ability to query incomplete inputs, in which each feature
xi is chosen from a space Xi ∪ {⊥}, where ⊥ encodes
the absence of a value. One way of handling such inputs
([11, 46]) is to label each node in the tree with an output
value. On an input, we traverse the tree until we reach a
leaf or an internal node with a split over a missing fea-
ture, and output that value of that leaf or node.
We formalize these notions by defining oracles that
A can query to obtain an identifier for the leaf or inter-
nal node reached by an input. In practice, we instantiate
these oracles using prediction API peculiarities.
Definition 1 (Identity Oracles). Let each node v of a tree
T be assigned some identiﬁer idv. A leaf-identity oracle
O takes as input a query x ∈ X and returns the identiﬁer
of the leaf of the tree T that is reached on input x.
A node-identity oracle O⊥ takes as input a query x ∈
X1 ∪{⊥}×···×Xd ∪{⊥} and returns the identiﬁer of
the node or leaf of T at which the tree computation halts.
4.2.1 Extraction Algorithms
We now present our path-finding attack (Algorithm 1),
that assumes a leaf-identity oracle that returns unique
identifiers for each leaf. We will relax the uniqueness
assumption further on. The attack starts with a random
input x and gets the leaf id from the oracle. We then
search for all constraints on x that have to be satisfied to
remain in that leaf, using procedures LINE SEARCH (for
continuous features) and CAT SPLIT (for categorical fea-
tures) described below. From this information, we then
create new queries for unvisited leaves. Once all leaves
have been found, the algorithm returns, for each leaf, the
corresponding constraints on x. We analyze the algo-
rithm’s correctness and complexity in Appendix C.
We illustrate our algorithm with a toy example of a
tree over continuous feature Size and categorical feature
Color (see Figure 3). The current query is x = {Size =
50, Color = R} and O(x) = id2. Our goal is two-fold:
(1) Find the predicates that x has to satisfy to end up in
leaf id2 (i.e., Size ∈ (40,60], Color = R), and (2) create
new inputs x(cid:20) to explore other paths in the tree.
608  25th USENIX Security Symposium 
USENIX Association
(cid:29) Call to the leaf identity oracle
(cid:29) Check if leaf already visited
x ← Q.POP()
id ← O(x)
if id ∈ P then
continue
end if
for 1 ≤ i ≤ d do
Algorithm 1 The path-finding algorithm. The notation id ←
O(x) means querying the leaf-identity oracle O with an input x and
obtaining a response id. By x[i] ⇒ v we denote the query x(cid:29) obtained
from x by replacing the value of xi by v.
1: xinit ← {x1, . . . ,x d}
(cid:29) random initial query
2: Q ← {xinit}
(cid:29) Set of unprocessed queries
3: P ← {}
(cid:29) Set of explored leaves with their predicates
4: while Q not empty do
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
end for
26:
27: end while
end for
S,V ← CATEGORY SPLIT(x,i, id)
P[id].ADD(‘xi ∈ S‘)
for v ∈ V do
Q.PUSH(x[i] ⇒ v)
end for