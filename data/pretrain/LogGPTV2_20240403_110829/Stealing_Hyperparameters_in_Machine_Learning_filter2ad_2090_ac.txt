v
i
t
a
l
e
R
RR
LASSO
KRR
−2
−1
0
1
2
3
−4
−5
−6
−7
−8
−9
−10
−3
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
RR
LASSO
KRR
−2
−1
0
1
2
3
RR
LASSO
KRR
−2
−1
0
1
2
3
True hyperparameter value (log10)
True hyperparameter value (log10)
True hyperparameter value (log10)
(a) Diabetes
(c) UJIIndoor
Fig. 2: Effectiveness of our hyperparameter stealing attacks for regression algorithms.
(b) GeoOrigin
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
0
−2
−4
−6
−8
−10
−3−3
L2-LR
L1-LR
L2-KLR
L1-KLR
−1
−2
0
1
2
3
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
0
−2
−4
−6
−8
−10
−3−3
−2
−1
L2-LR
L1-LR
L2-KLR
L1-KLR
0
1
2
3
L2-LR
L1-LR
L2-KLR
L1-KLR
−1
−2
0
1
2
3
True hyperparameter value (log10)
True hyperparameter value (log10)
True hyperparameter value (log10)
(a) Iris
(b) Madelon
(c) Bank
Fig. 3: Effectiveness of our hyperparameter stealing attacks for logistic regression classiﬁcation algorithms.
SVM-RHL
SVM-SHL
KSVM-RHL
KSVM-SHL
0
−2
−4
−6
−8
−10
−3
−2
−1
0
1
2
3
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
0
−2
−4
−6
−8
−10
−3
−2
−1
SVM-RHL
SVM-SHL
KSVM-RHL
KSVM-SHL
0
1
2
3
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
0
−2
−4
−6
−8
SVM-RHL
SVM-SHL
KSVM-RHL
KSVM-SHL
−10
−3
−2
−1
0
1
2
3
True hyperparameter value (log10)
True hyperparameter value (log10)
True hyperparameter value (log10)
(a) Iris
(b) Madelon
(c) Bank
Fig. 4: Effectiveness of our hyperparameter stealing attacks for SVM classiﬁcation algorithms.
deﬁned as follows:
Relative estimation error:  =
|ˆλ − λ|
λ
,
(9)
where ˆλ and λ are the estimated hyperparameter and true
hyperparameter, respectively.
2) Experimental Results for Known Model Parameters:
We ﬁrst show results for the scenario where an attacker
knows the training dataset, the learning algorithm, and the
model parameters. Figure 2 shows the relative estimation errors
for different regression algorithms on the regression datasets.
Figure 3 shows the results for logistic regression algorithms
on the classiﬁcation datasets. Figure 4 shows the results for
SVM algorithms on the classiﬁcation datasets. Figure 5 shows
the results for three-layer neural networks for regression and
classiﬁcation. In each ﬁgure, x-axis represents the true hyper-
42
parameter in a particular algorithm, and the y-axis represents
the relative estimation error of our attacks at stealing the
hyperparameter. For better illustration, we set
the relative
−10 when they are smaller than
estimation errors to be 10
−10. Note that learning algorithms with L1 regularization
10
require the hyperparameter to be smaller than a maximum
value λmax in order to learn meaningful model parameters
(please refer to Appendix A for more details). Therefore, in
the ﬁgures, the data points are missing for such algorithms
when the hyperparameter gets larger than λmax, which is
different for different training datasets and algorithms. We
didn’t show results on kernel LASSO because it is not widely
used. Moreover, we didn’t ﬁnd open-source implementations
to learn model parameters in kernel LASSO, and implementing
kernel LASSO is out of the scope of this work. However, our
attacks are applicable to kernel LASSO.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:29:54 UTC from IEEE Xplore.  Restrictions apply. 
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
0
−2
−4
−6
−8
−10
−3
Diabetes
GeoOrig
UJIIndoor
−2
−1
0
1
2
3
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
0
−2
−4
−6
−8
−10
−3
Iris
Madelon
Bank
−2
−1
0
1
2
3
−4
−5
−6
−7
−8
−9
−10
−3
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t
a
l
e
R
RR
LASSO
KRR
−2
−1
0
1
2
3
)
0
1
g
o
l
(
r
o
r
r
e
n
o
i
t
a
m
i
t
s
e
e
v
i
t