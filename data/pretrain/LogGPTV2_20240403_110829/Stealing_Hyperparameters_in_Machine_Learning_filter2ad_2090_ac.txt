### Effectiveness of Hyperparameter Stealing Attacks on Regression and Classification Algorithms

#### Figures and Analysis

**Figure 2: Effectiveness of Hyperparameter Stealing Attacks for Regression Algorithms**

- **(a) Diabetes**
- **(b) GeoOrigin**
- **(c) UJIIndoor**

The relative estimation errors for different regression algorithms (Ridge Regression, LASSO, Kernel Ridge Regression) are plotted against the true hyperparameter values (log10 scale). The y-axis represents the relative estimation error.

**Figure 3: Effectiveness of Hyperparameter Stealing Attacks for Logistic Regression Classification Algorithms**

- **(a) Iris**
- **(b) Madelon**
- **(c) Bank**

The relative estimation errors for logistic regression algorithms (L2-LR, L1-LR, L2-KLR, L1-KLR) are plotted against the true hyperparameter values (log10 scale).

**Figure 4: Effectiveness of Hyperparameter Stealing Attacks for SVM Classification Algorithms**

- **(a) Iris**
- **(b) Madelon**
- **(c) Bank**

The relative estimation errors for SVM algorithms (SVM-RHL, SVM-SHL, KSVM-RHL, KSVM-SHL) are plotted against the true hyperparameter values (log10 scale).

#### Definitions and Experimental Results

**Relative Estimation Error:**

\[
\text{Relative Estimation Error} = \frac{|\hat{\lambda} - \lambda|}{\lambda}
\]

where \(\hat{\lambda}\) is the estimated hyperparameter and \(\lambda\) is the true hyperparameter.

**Experimental Results for Known Model Parameters:**

We first present results for the scenario where an attacker knows the training dataset, the learning algorithm, and the model parameters. 

- **Regression Datasets:**
  - **Diabetes**
  - **GeoOrigin**
  - **UJIIndoor**

- **Classification Datasets:**
  - **Iris**
  - **Madelon**
  - **Bank**

In each figure, the x-axis represents the true hyperparameter value in a particular algorithm, and the y-axis represents the relative estimation error of our attacks at stealing the hyperparameter. For better illustration, we set the relative estimation errors to be \(10^{-10}\) when they are smaller than \(10^{-10}\).

Note that learning algorithms with L1 regularization require the hyperparameter to be smaller than a maximum value \(\lambda_{max}\) to learn meaningful model parameters. In the figures, data points are missing for such algorithms when the hyperparameter exceeds \(\lambda_{max}\), which varies for different training datasets and algorithms.

We did not include results for kernel LASSO because it is not widely used, and we did not find open-source implementations to learn model parameters in kernel LASSO. However, our attacks are applicable to kernel LASSO.

#### Additional Figures

**Figure 5: Effectiveness of Hyperparameter Stealing Attacks for Three-Layer Neural Networks**

- **Regression and Classification**

The relative estimation errors for three-layer neural networks are plotted against the true hyperparameter values (log10 scale).

**Figure Legends:**

- **Diabetes, GeoOrigin, UJIIndoor, Iris, Madelon, Bank**: Different datasets.
- **RR, LASSO, KRR, L2-LR, L1-LR, L2-KLR, L1-KLR, SVM-RHL, SVM-SHL, KSVM-RHL, KSVM-SHL**: Different algorithms.

**Acknowledgments:**

Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18, 2021, at 12:29:54 UTC from IEEE Xplore. Restrictions apply.