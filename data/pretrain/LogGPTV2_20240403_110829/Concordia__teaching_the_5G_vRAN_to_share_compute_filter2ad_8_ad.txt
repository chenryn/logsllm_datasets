Figure 8: Reclaimed vRAN pool CPU cores for vRAN with 20MHz (7 cells) and 100MHz (2 cells) configurations and performance of
collocated workloads for various cell traffic loads.
Unless stated otherwise, we allocate 8 CPU cores for the vRAN
pool and run 15 minutes experiments, following the Intel recom-
mendation for quick performance validation [49]. This corresponds
to a number of scheduling events ranging between 3.6 √ó 106 and
6.3 √ó 106 depending on the cell configuration. We further validate
the 99.999% reliability of Concordia by running 8 hours tests with
the mixed workload (between 1.152 √ó 108 and 2.016 √ó 108 schedul-
ing events). No performance or reliability differences were observed
between the long and the short tests. We compare Concordia with
the baseline FlexRAN scheduler, as it is the most intuitive queue-
based design. It acquires more cores when there are tasks waiting
in the queues and relinquishes them when the queues are empty. In
Section 6.3 we compare against other scheduler designs.
6.1 High-level benefits of Concordia
One of the main goals of Concordia is to improve CPU utilization on
vRAN servers running BBU tasks. To this end, we start by evaluating
the benefits of Concordia for the vRAN collocated workloads. We
vary the traffic load up to the peak traffic listed in Table 2 for different
cell configurations, and for each load we generate random traffic as
described above. To make the comparison fair, we use the minimum
number of cores required to meet the vRAN processing deadline.
We begin by measuring the percentage of CPU cores that are made
available by Concordia to other workloads and we compare this to
the ideal case where every idle CPU cycle is reclaimed. As it can be
seen in Fig. 8a, Concordia can reclaim more than 70% of the CPU
cores for low cell traffic loads both for 20MHz and 100MHz cell
configurations. The percentage drops to 0% and 38% correspondingly
for cells operating at the max allowed average load. We observe that
Concordia is slightly more efficient for low cell workloads, because
there are many idle TTI slots whose duration is easy to predict.
We next study the performance of different workloads collocated
with the vRAN. As a reference, we measure the maximum achievable
performance of those workloads in the ideal case, when running on
the same cores without the vRAN workload. As shown in Fig. 8b-8d,
Bandwidth
100MHz
20MHz
# cells
2
7
Peak DL cell
throughput
1.5Gbps
380Mbps
Peak UL cell
throughput
160Mbps
160Mbps
# of CPU
cores
12
8
Table 2: Cell configuration and minimum number of CPU cores
required for evaluation of Section 6.1 .
587
the achieved performance varies depending on the workload. For ex-
ample, in the case of the 100MHz cell configuration and for low cell
traffic load (83.3% of the cores reclaimed), TPCC achieves 72% of
the ideal performance (without the vRAN), Redis achieves 76.6% and
Nginx achieves 82.2%. The MLPerf workload figure is omitted due to
lack of space, but similar results were obtained (78% of the ideal per-
formance achieved for low cell traffic load in the 100MHz case). The
reason for the lower yield compared to the theoretical max expected
performance is related to the effects that the collocated workloads
have on cache pollution, preemption, scheduling latencies, etc (as
also observed in [56]). We next study these effects in detail. It should
be noted that throughout these experiments, Concordia provided
99.999% reliability to the TTI processing latency of the vRAN pool.
Overall, we conclude that Concordia is able to recover a large
fraction of CPU cycles unused by vRAN. This is in contrast with
the current operators‚Äô best practice which does not attempt any load
sharing on servers with vRAN pools.
6.2 Effects of collocation on the vRAN
One of the key benefits of Concordia is its ability to predict task
execution times. Because of this, it can minimize the number of
cores it uses at any time. This increases cache locality, reduces cache
pollution and reduces OS scheduler calls, making the system more
efficient while leaving unneeded cores to the collocated workloads.
The vanilla FlexRAN scheduler does not have an estimate of the
traffic and has to be more conservative in allocating more cores than
necessary. It also has to acquire and release the cores back to the OS
more frequently in order to be able to share. This reduces the locality
and increases cache trashing on the cores used by the vRAN pools.
We next quantify these effects with experiments.
Cache efficiency: To measure the cache efficiency, we use the Linux
perf tool [25] to profile the vRAN pool worker threads. We measure
the change in the cache efficiency observed by the worker threads
with a collocated workload compared to the baseline isolated vRAN
case. Here, we present results for 100MHz cell configuration and
the Redis workload (the other results are similar and we omit them
due to lack of space). As it can be seen in Fig. 9, vanilla FlexRAN
has a 25% increase in the stall cycles per instruction due to L1 cache
misses compared to the baseline isolated vRAN case. This leads
to an increase in the runtime of the signal processing tasks of the
vRAN pool and thus directly affects the tail TTI processing latency.
5255075100Cell traffic load (%)020406080100Reclaimed CPU (%)Upper bound - 100MHzUpper bound - 20MHzConcordia - 100MHzConcordia - 20MHz5255075100051e6GET OperationsNo vRAN (8 cores)No vRAN (12 cores)100MHz vRAN20MHz vRAN5255075100051e6SET Operations0.00.20.40.60.81.0Cell traffic load (%)0.00.20.40.60.81.0Requests per second5255075100Cell traffic load (%)0123456HTTP requests per second1e4No vRAN (8 cores)No vRAN (12 cores)100MHz vRAN20MHz vRAN5255075100Cell traffic load (%)0.00.51.01.52.02.53.0Transactions per second1e3No vRAN (8 cores)No vRAN (12 cores)100MHz vRAN20MHz vRANConcordia: Teaching 5G vRAN to Share Compute
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
(a) Isolated vRAN
(b) vRAN with Redis
Figure 10: Scheduling latency of vRAN pool worker threads (8
CPU cores) for 2 100MHz cells with and without workload in-
terference (Redis). Y-axis in log scale.
Number of vRAN pool cores: Adding more CPU cores to the vRAN
pool helps Concordia to meet deadlines. We see that in Fig 12 for
a test using a constantly running mixed workload. The 20MHz cell
configuration achieves 99.999% of reliability with 8 cores (Fig. 12a),
but the 100MHz case achieves only 99.99% (Fig. 12b). However, by
adding one more CPU core to the vRAN pool, the reliability goes
back to 99.999%. This is because the more CPU cores we assign to
the vRAN pool, the more chances Concordia will have to schedule
an extra core if the vRAN is on track of missing a deadline and an
already scheduled core takes a long time to wake up (e.g. due to a
non-preemptive kernel task occupying it), as described in Section 3.
6.3 Comparison with alternative schedulers
Conventional WCET prediction method: We compare the effec-
tiveness of the Concordia predictor against a well-known method [23]
that is representative of the probabilistic WCET prediction litera-
ture [18]. The method in [23] uses Extreme Value Theory and predicts
a single WCET per signal processing task regardless of its input with
a confidence of 0.99999. As shown in Fig 13 for the 20MHz cell
configuration, Concordia outperforms the conventional model (up
to 20% difference in reclaimed cycles). This is because the conven-
tional WCET model makes more pessimistic predictions compared
to Concordia. At the same time, the tail latency reduction achieved by
the conventional model is marginal (about 5 ùúás in all cases), further
incentivizing our use of a parametrized WCET prediction model.
We make similar observations for the 100MHz case, with the results
omitted due to lack of space.
Schedulers not considering the WCET: Next, we compare the re-
liability of Concordia to two schedulers that do not take into account
the WCET of tasks: (i) a variant of Shenango [75] (also used in
Snap [68]) and (ii) a utilization-based scheduler. Our Shenango-
variant increases the number of cores allocated to the vRAN by one
every time that a signal processing task remains in the priority queue
for more than a predefined amount of time, and we vary this threshold.
The utilization-based scheduler adjusts the number of cores based on
the utilization of the vRAN in the past few TTIs. Once the utilization
surpasses a threshold (60% and 30% for the 20MHz and 100MHz
cell configurations), an additional worker thread is woken up.
In the case of the Shenango-based scheduler, it was very challeng-
ing to identify the queuing time threshold that would both satisfy
the vRAN deadlines and would allow other workloads to share the
Figure 9: Latency effects of cache from collocated workload (Re-
dis) interference for 2 100MHz cells
In contrast, Concordia is able to predict well the number of required
cores and thus limits the increase in the stall cycles caused by cache
misses due to collocation to less than 2%.
OS scheduling latency: We use the runqlat tool that is part of the
BCC toolkit [63] to measure the OS scheduling latency of the vRAN
pool worker threads once they have yielded and have been signaled
to wake up (from the Concordia scheduler or from the generation of
more signal processing tasks in the case of vanilla FlexRAN). We
collect scheduling latency measurements for 1 minute. As shown in
Fig. 10 (log scale), the total number of scheduling events of vanilla
FlexRAN is significantly higher compared to that of Concordia (about
230% higher in both the isolated and interfering case), leading to
higher scheduling latency per slot and thus more deadline violations.
The reduced number of OS scheduling calls in the case of Concordia
is due to the proactive allocation of cores, which does not allow
worker threads to yield while more signal processing tasks are ex-
pected during a TTI slot. A side-effect of this is that Concordia has a
higher number of scheduling events with high tail latency (>63 ùúás) in
the presence of other workloads compared to FlexRAN. We believe
this is because the CPU cores of the vRAN pool are retained by the
worker threads longer, leading to the queuing of OS tasks that cannot
be migrated (e.g. interrupts) and increasing the chances of some
kernel thread entering a non-preemptible section when the worker
threads yield. This effect is mitigated by the fine-grained scheduling
of cores by Concordia every 20 ùúás, since more cores can be allocated
to the vRAN if a scheduled core fails to wake up in time.
Tail latency: We next compare the effects of collocated workload
interference to the tail processing latency of Concordia vs vanilla
FlexRAN. For each cell configuration considered in Table 1 and
different workloads we run experiments measuring the 99.99% and
99.999% TTI processing latency of the vRAN pool. The results are il-
lustrated in Fig. 11. Without other workloads, both FlexRAN and Con-
cordia can meet the processing deadline with 99.999% reliability for
both configurations. Once we introduce any other workload, the tail
latency of vanilla FlexRAN increases significantly and it is no longer
possible to provide 99.999% of reliability or even 99.99%, with the
exception of MLPerf. However, Concordia is not affected and main-
tains 99.999% of reliability in all cases. The same observations apply
for the mixed workload test (figure omitted due to lack of space).
588
0510152025Stall cycles per instruction increase (%)0246810121416L1 cache misses per instruction increase (%)05101520LLC loads per instruction increase (%)ConcordiaFlexRAN0-12-34-78-1516-3132-6364-127103FlexRAN0-12-34-78-1516-3132-6364-127Scheduling latency (us)104ConcordiaScheduling events0-12-34-78-1516-3132-6364-127128-255104FlexRAN0-12-34-78-1516-3132-6364-127128-255Scheduling latency (us)103105ConcordiaScheduling eventsSIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
Xenofon Foukas and Bozidar Radunovic
(a) Concordia with 7 FDD cells of
20MHz
(b) FlexRAN with 7 FDD cells of
20MHz
(c) Concordia with 2 TDD cells of
100MHz
(d) FlexRAN with 2 TDD cells of
100MHz
Figure 11: Tail TTI processing latency (99.99% and 99.999%) of Concordia vs vanilla FlexRAN in the presence of various workloads.
All experiments are performed on with a vRAN pool of 8 CPU cores.
(a) 7 cells of 20MHz
(b) 2 cells of 100MHz
Figure 12: Concordia tail TTI processing latency for mixture of
Nginx, Redis, TPCC workloads
(a) Reclaimed CPU
(b) Latency impact
Figure 13: WCET prediction accuracy effect of various predic-
tion methods for LDPC decoding task.
vRAN pool cores. Setting to a high value (200 ùúás) made the scheduler
react slowly to delays in the processing of vRAN tasks, with less
than 99.99% deadlines met. Setting to a low value (5 ùúás), similar to
Shenango, led to the vRAN utilizing all of the CPU resources, never
allowing other workloads to run. Different values ranging from 5 ùúás
to 200 ùúás provided better results for different vRAN traffic loads, but
no single value always met deadlines with ‚â• 99.99% reliability.
In the case of the utilization-based scheduler, the vRAN traffic
burstiness could not be captured by observing the past utilization of
the vRAN pool cores. This led to less than 99.99% reliability in the
presence of collocated workloads, since the scheduler often under-
estimated the amount of CPU resources required for processing the
upcoming TTI slot. These results reinforce our finding that having
589
(a) Percentage of slots where the
processing deadline was violated.
Y-axis in log scale.
(b) Average WCET prediction er-
ror for successfully met deadline. Y-
axis in log scale.
Figure 14: WCET prediction accuracy effect of various predic-
tion methods for LDPC decoding task.
predictions of task execution times is instrumental for efficient CPU
sharing in the vRAN.
6.4 Accuracy of other prediction models
Here, we compare the accuracy of Concordia‚Äôs quantile decision tree
against other prediction models we explored. We consider a linear re-
gression and a (non-linear) gradient boosting model. For the training
of the models, we collected the vRAN state and runtimes offline in the
same way as described in Section 4.2 for the quantile decision tree and
selected training features according to Algorithm 1. We also adapted
the models to take into account the online runtime samples, like in the
quantile decision tree case (we omit the details due to lack of space).
We perform probabilistic WCET predictions using a prediction
interval of 0.99999. To evaluate the prediction accuracy we use two
metrics; (i) the percentage of missed deadlines (i.e. times that the run-
time of the task exceeded the predicted WCET) and (ii) the average
WCET prediction error for successfully met deadlines. The intuition
behind the second metric is that the closer a successful WCET pre-
diction is to the actual runtime, the less cores would be dedicated to
the vRAN by Concordia, freeing up cycles for other workloads.
We generate randomly fluctuating traffic for the 20MHz cell con-
figuration of Table 1, varying the number of UEs (0 to 8). We consider
deployment scenarios with 1 or 2 cells and different types of collo-
cated workloads (none, Redis, TPCC benchmark) on 4 CPU cores.
For each scenario we ran a 5 minutes test. As it can be seen in Fig. 14a
for the LDPC decoding task, the non-linear gradient boosting model
Average99.99%99.999%0.00.51.01.52.0Slot processing latency (ms)DeadlineIsolatedNginxRedisTPCCMLPerfAverage99.99%99.999%0.00.51.01.52.0Slot processing latency (ms)DeadlineIsolatedNginxRedisTPCCMLPerfAverage99.99%99.999%0.000.250.500.751.001.251.50Slot processing latency (ms)DeadlineIsolatedNginxRedisTPCCMLPerfAverage99.99%99.999%0.00.51.01.5Slot processing latency (ms)DeadlineIsolatedNginxRedisTPCCMLPerf99.99%99.999%0.00.51.01.52.0Slot processing latency (ms)Deadline8 CPU cores9 CPU cores99.99%99.999%0.000.250.500.751.001.251.50Slot processing latency (ms)Deadline8 CPU cores9 CPU cores5255075100Cell traffic load (%)020406080100Reclaimed CPU (%)ConcordiapWCETConcordiapWCET0.00.51.01.52.0Slot processing latency (ms)DeadlineIsolatedNginxRedisTPCCMLPerfLinearregressionGradientboostingQuantileDTFull DAGQuantile DT10‚àí310‚àí210‚àí1100101102Deadlines missed (%)1 cell -  FD2 cells - FD1 cell - FD & redis2 cells - FD & redis1 cell - FD & TPCC2 cells - FD & TPCCLinearregressionGradientboostingQuantileDT102103Avg prediction error (us)1 cell -  FD2 cells - FD1 cell - FD & redis2 cells - FD & redis1 cell - FD & TPCC2 cells - FD & TPCCConcordia: Teaching 5G vRAN to Share Compute
SIGCOMM ‚Äô21, August 23‚Äì27, 2021, Virtual Event, USA
7 CONCORDIA EXTENSIONS