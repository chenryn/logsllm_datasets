title:BasisDetect: a model-based network event detection framework
author:Brian Eriksson and
Paul Barford and
Rhys Alistair Bowden and
Nick G. Duffield and
Joel Sommers and
Matthew Roughan
BasisDetect : A Model-based Network Event Detection
Framework
Brian Eriksson
UW-Madison
PI:EMAIL
Nick Dufﬁeld
AT&T Research
dufﬁPI:EMAIL
Paul Barford
UW-Madison
and Nemean Networks
PI:EMAIL
Joel Sommers
Colgate University
Rhys Bowden
University of Adelaide
PI:EMAIL
Matthew Roughan
University of Adelaide
PI:EMAIL
PI:EMAIL
ABSTRACT
The ability to detect unexpected events in large networks
can be a signiﬁcant beneﬁt to daily network operations. A
great deal of work has been done over the past decade to
develop eﬀective anomaly detection tools, but they remain
virtually unused in live network operations due to an un-
acceptably high false alarm rate. In this paper, we seek to
improve the ability to accurately detect unexpected network
events through the use of BasisDetect, a ﬂexible but precise
modeling framework. Using a small dataset with labeled
anomalies, the BasisDetect framework allows us to deﬁne
large classes of anomalies and detect them in diﬀerent types
of network data, both from single sources and from mul-
tiple, potentially diverse sources. Network anomaly signal
characteristics are learned via a novel basis pursuit based
methodology. We demonstrate the feasibility of our Basis-
Detect framework method and compare it to previous de-
tection methods using a combination of synthetic and real-
world data.
In comparison with previous anomaly detec-
tion methods, our BasisDetect methodology results show a
50% reduction in the number of false alarms in a single node
dataset, and over 65% reduction in false alarms for synthetic
network-wide data.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network
Operations—Network monitoring
General Terms
Measurement, Performance
Keywords
Anomaly Detection
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’10, November 1–3, 2010, Melbourne, Australia.
Copyright 2010 ACM 978-1-4503-0057-5/10/11 ...$10.00.
1.
INTRODUCTION
Networks are complex, dynamic and subject to external
factors outside of their operators’ control. Network oper-
ators must therefore vigilantly monitor their networks for
faults and other events that could jeopardize their contrac-
tual commitments to customers. The problem is relatively
easy when the type of fault is well understood (e.g., link
failures). There are standard protocols for alerting an oper-
ator to such faults, and although extending these methods
particularly in the context of security is an ongoing eﬀort,
they are not the speciﬁc focus of this paper. Instead, this
paper considers unforeseen faults. These faults are intrinsi-
cally more challenging to detect because we do not a priori
know what we are looking for. These faults often manifest
in unusual measurements that are commonly referred to as
anomalies. Being able to ﬁnd anomalies, and use them to
diagnose network problems quickly and eﬀectively would sig-
niﬁcantly enhance network operations. Developing a frame-
work for eﬀective and practical anomaly detection is the
objective of our work.
A large number of studies over the past decade have been
focused on developing methods to detect anomalous events
in networks. The typical approach begins by measuring net-
work traﬃc (e.g., ﬂow-export records) and then establishing
a proﬁle for “normal” behavior. Next, a method for detect-
ing deviations from normality is applied. Most prior studies
have largely taken a one-size-ﬁts-all approach that has ulti-
mately resulted in problems with accuracy and false alarm
rate.
It is critically important in any anomaly detection sys-
tem to have a very low false alarm rate. False alarms waste
operator time and discredit results, leading to a “cry wolf”
syndrome, where the anomaly detection system is quickly ig-
nored. Most existing systems suﬀer from unduly high false-
alarm rates. This is exacerbated by anomalies polluting the
data used in determining the normal proﬁle. In this paper,
we seek to improve the accuracy of network event detection
to the point where it becomes an eﬀective tool for network
operators.
To approach this problem of anomaly detection, we in-
troduce the BasisDetect framework. The primary intuition
behind the BasisDetect framework is that both normal traf-
ﬁc and anomalies have features that we can model and ex-
ploit for the purpose of automated detection. For instance,
it is well known the traﬃc has strong diurnal and weekly
451cycles. Our hypothesis is that by considering traﬃc as a
superposition of waveforms and then breaking these down
into their component parts, we can build detection models
that oﬀer the opportunity to separate bundles of energy that
can be semantically divided into to normal and anomalous
traﬃc. The BasisDetect framework is divided into three
components. The ﬁrst step learns potential anomaly signal
features from a small set of labeled network data provided
to the algorithm. The second step uses a novel basis pursuit
methodology to simultaneously decompose traﬃc into com-
ponents of both non-anomalous behavior representing ex-
pected network traﬃc, and anomalous behavior learned from
the previous step. This simultaneous estimation avoids the
problem of anomalies polluting our normal proﬁle data. The
ﬁnal step of the algorithm exploits known network structure
to intelligently merge together the detected anomalous be-
havior using state-of-the-art statistical techniques.
Further objectives of our framework include developing
an anomaly detection method that can be applied (i) to
diﬀerent data types since critical anomalies may be entirely
invisible in some data, and (ii) in both a single node and
network-wide context. Prior work has typically fallen into
one or the other category due to detection methods that
are primarily spatial or temporal. While our initial signal
decomposition approach is temporal, we combine anomalies
across the network using a higher reasoning framework. This
combined, best-of-both-worlds approach oﬀers a signiﬁcant
opportunity to improve detection accuracy. Intuitively, we
treat network wide detection as a data fusion problem, where
one can signiﬁcantly reduce false alarms through the use of
multiple time-series signals. It has the secondary advantage
that it naturally incorporates diﬀerent data types, without
the need for strong relationships between the diﬀerent time-
series such as required, for example by PCA [1].
We use both synthetic and real world data to rigorously
assess the capabilities of our model-based detection method-
ology. The ﬁrst part of our evaluation considers NetFlow
data collected at a single router along with a set of labeled
anomalies that include DoS attacks, outages, scans, etc. We
isolate a subset of the anomalies in the data and then apply
the BasisDetect framework to learn anomalous models using
a combination of signal components that isolate key elements
of the events. We ﬁnd that our BasisDetect methodology
identiﬁes all the labeled anomalies with 50% improvement
in the false alarm rate when compared with the best com-
peting methodology.
Next, we use a set of carefully generated synthetic data to
assess the sensitivity of our model-based detection method-
ology. The data is designed to capture the key low and
high frequency and spatial characteristics of non-anomalous
traﬃc ﬂows in a network-wide setting. We insert simple
volume anomalies into this data and modulate the relative
amplitude and frequency of these anomalies versus the non-
anomalous traﬃc in order to assess sensitivity. While this
synthetic data is not as rich as measurements collected in
situ, we argue that it provides a powerful and meaning-
ful starting point for assessing detection sensitivity. The
results of our analysis show that the BasisDetect methodol-
ogy detects all of the injected anomalies with false alarm rate
over 65% less than the current state-of-the-art network-wide
anomaly detection methodology.
Finally, we consider a set of Internet2 byte count data
collected simultaneously across 11 PoPs. While this dataset
does not have labeled anomalous events, we can compare
the ability of the BasisDetect methodology and a state-
of-the-art distributed method [2] to detect the most dom-
inant anomalies detected by the standard PCA [1] anomaly
detection methodology. Our results show that BasisDe-
tect method will identify the PCA anomaly locations with
40% fewer false alarms than the competing state-of-the-art
network-wide anomaly detection method. We believe that
these results along with the results from single node and
network-wide labeled data sets make a strong case for the
utility of our model-based approach.
The remainder of this paper is as follows. In Section 2, we
describe the background of detecting network anomalies and
relevant related work. In Section 3, we describe the datasets
used to test our method and competing anomaly detection
algorithms. In Section 4 the BasisDetect framework is intro-
duced. Then in Section 5, our temporal signal decomposi-
tion methodology is described with applications of anomaly
extraction from a small training set, and anomaly detection.
Then in Section 6, an intelligent data fusion methodology is
described to localize anomalies given the results of our basis
pursuit algorithm. Combining both methods, we summa-
rize the BasisDetect methodology in Section 7. Finally, in
Section 8 we evaluate the results of applying our method
and several other well known methods to the given data
sets. We summarize our work and discuss future directions
in Section 9.
2. BACKGROUND AND RELATED WORK
Anomaly detection is now a large ﬁeld, and we cannot
hope to survey all papers within the ﬁeld. We will focus on
those of direct relevance to our work and describe them in
detail since they help to highlight the uniqueness and poten-
tial beneﬁts of our method. Our speciﬁc focus is on studies
that consider anomaly detection on vector time-series data.
This type of work gained its initial impetus with the consid-
eration of how Principle Component Analysis (PCA) would
perform in a network-wide setting [1, 3, 4]. Prior work re-
lied primarily on performing some kind of temporal trans-
form of the data, and assumed that anomalies will stand out
against the traﬃc in the transformed space (examples trans-
forms include wavelets in [5, 6], the Exponentially Weighted
Moving Average or EWMA in [7, 8], and Fourier ﬁltering
in [6]). Anomalies are then generated for every individual
set of measurements. The key beneﬁts of the PCA method-
ology was that it took direct advantage of the non-scalar na-
ture of network data, and that it sought to ﬁnd an optimal
linear transform of the data in order to reveal inconsistent
data points. Following the initial work on PCA, Zhang et al.
showed how much of the prior work on anomaly detection
(including PCA) could be seen in a single framework [6],
but more notably, that paper showed that the “sparseness”
of anomalies could be exploited in aiding their discovery.
In more detail, the PCA framework described in [1] de-
composes a traﬃc matrix into a set of vector components
that capture the variance across all links or ﬂows of the net-
work. The components that resolve the highest variance
across all links (e.g., the most standard components) are
considered to represent standard operating characteristics of
the network observed in the link data matrix, the “modeled
traﬃc”. Meanwhile, the less dominant components repre-
sent the “residual traﬃc” that is abnormal to the links in
general. The amount of traﬃc energy in this residual com-
452ponent determines whether or not an anomaly has occurred
in the observed traﬃc on each link.
The limitations of this PCA approach are well docu-
mented in [9]. In addition to having high sensitivity to tun-
ing parameters, large anomalies in the network can corrupt
the “modeled traﬃc” components and therefore cause obvi-
ous events to be ignored by the methodology. Also, detected
anomalies found by PCA can not be localized to the speciﬁc
anomalous link or router, and the PCA methodology can
lead to masking, where one anomaly hides another. Finally,
in PCA the “residual traﬃc” does not necessarily represent
signal components specifying anomalies (possibly it is nor-
mal behavior found only on a single link), and therefore
detecting events based on residual energy is prone to false
alarms. Furthermore, the work in [10] shows how standard
PCA-based anomaly detection methods are vulnerable to at-
tacks. While the technique introduced in this paper builds
on the idea of dividing the signal into modeled and residual
components, the underlying methodologies used are com-
pletely diﬀerent (PCA in the prior work, basis pursuit in
this paper).
The authors of the Distributed Spatial Anomaly Detec-
tion technique described in [2] recognize that one of the
main limitations of the PCA approach was the necessity
of communicating all ﬂow information back to some central-
ized computation point. Using non-parametric statistics and
False Discovery Rate techniques (FDR) [11], each router in
the network generates just a small test statistic that is com-
municated for anomaly detection. The use of more sophis-
ticated multiple hypothesis detection techniques, like FDR
thresholding, allows for a better statistical detection rate
than more naive methodologies, such as Bonferroni Correc-
tion [12]. The biggest limitation of this approach is the
complete decoupling of the measurements in the time do-
main. Therefore, any temporal correlation between network
anomaly events (the measurements at time t helping inform
the events from measurements at t + 1) are ignored. In addi-
tion, the measurements considered are with respect to traﬃc
volume only, with no discussion on how other link character-
istic information (bytes, unique IP address, etc.) could be
intelligently fused into the framework. Finally, the detected
anomalies are not necessarily points of interest to a network
administrator or anything that might represent the known
structure of anomalies in networks. These detected anoma-
lies are simply events of traﬃc volume that are abnormal
compared with the remaining observed set of network data.
A situation may occur when events are unlike the other ob-
served network data and yet uninteresting from a network
administration prospective. Other distributed approaches
to anomaly detection exist [13, 14]. Although it should be
noted that the BasisDetect framework is amenable to distri-
bution, the focus of this paper will be to carefully treat the
false alarm problem.
Our anomaly detection methodology will exploit the same
non-parametric statistical techniques as [2] (originally devel-
oped in [15]). However, our methodology diﬀers in that we
use an estimated feature vector of detected anomaly energy
instead of the raw packet counts. Data fusion from diﬀerent
data sources was shown some time ago to reduce false alarm
rates (e.g., [7]). In contrast, this paper develops an approach
which can ﬂexibly incorporate various diﬀerent sources of
data. By considering a general feature vector, we can po-
tentially fuse a wide range of link characteristics, thereby
improving results.
For the detection of anomalies in time-series data, our
methodology will leverage the signiﬁcant prior work on ba-
sis decomposition of signals [16, 17, 18]. This prior work fo-
cused on creating methodologies to exactly represent a signal
given a sparse linear combination of components from the
signal dictionary (i.e., a matrix of signal components). In
this paper, our goal is to resolve the gross characteristics of
the signal, allowing for non-exact signal representation by
our basis dictionary signals. In addition, our novel method-
ology will allow for the penalization of choosing selected dic-
tionary signals, an application previously unexplored in the
basis pursuit literature.
3. DATASETS
We use three diﬀerent data sets to evaluate our model-
based detection methodology. The intent of our analysis is
to assess the capability of our approach as thoroughly as
possible. To that end, we use empirical data sets for both
single node with labeled anomalies and network-wide set-
tings without labeled anomalies. We also use a synthetic
data set in which we can precisely control both the normal
and anomalous traﬃc in order to carefully assess the sensi-
tivity of our method. Each of the data sets is described in
detail below.
3.1 Synthetic Trafﬁc Data
In order to accurately test anomaly detection algorithms,
we need to be able to simulate reasonable datasets in a con-
trolled way. Ringberg et al. [19] explain in detail why simu-
lation must be used for accurate comparisons of anomaly de-
tection techniques. In brief the reasons are: (i) accurate and
complete ground truth information is needed to form both
false-alarm and detection probability estimates; (ii) many
more results are needed (than one can obtain from any real-
istic real dataset) to form accurate estimates of probabilities,
and (iii) simulation allows one to vary parameters (say the
anomaly size) in a controlled way in order to see the eﬀect
this has on anomaly detection.
Our approach to simulation is intended to highlight the
features of the diﬀerent techniques. We make no claim that
the simulation is completely realistic, only that it illustrates
clearly the properties of the diﬀerent anomaly detection
techniques. The simulations used here were generated in
a similar manner to those in [20]. In particular, a spatial
traﬃc matrix is generated using a gravity model and then
extended into the temporal domain using a matrix product
with a simple periodic signal. The resulting traﬃc is then
enhanced by Gaussian noise with variance that is propor-
tional to the traﬃc mean. The only diﬀerences with the
previous study are that (i) we consider a range of sizes of
networks, and (ii) consider a range of length of anomalies.
We should stress that the goal of these simulations is not
to produce the most realistic test possible for the algorithms.
However, the simulations allow us to obtain exact quantita-
tive comparisons of algorithms in completely controlled cir-
cumstances, so we can explore the properties of the diﬀerent
approaches.
3.2 GEANT Data
The second set of data will be a collection of time-series
data obtained from a GEANT network backbone router [21]
453located in Vienna, Austria. Collection of data began on Jan-
uary 14th 2009 and ended on February 24th 2009, for a total
of 42 days of data acquisition. The dataset contains packet
counts, byte counts, and IP entropy measured along this
single link extracted using Juniper J-Flow records, sampled
in aggregation bins of 1 minute for a total of 60,480 data
samples observed. This dataset contains labeled anomalies,
including Denial of Service (DoS) attacks, portscan events,
and Distributed Denial of Service (dDoS) attacks. These
events were found, validated, and annotated by network en-
gineers.1
A limitation of this single node time-series is that it cannot
show the power of strictly network-wide techniques (such as
PCA or Distributed Spatial). Although we are restricted in
the comparison methodologies available for this dataset, the
single link information has the advantage that a great deal
of eﬀort has gone into classifying the anomalies in this data,
so that we are closer to having ground truth than we are in
any almost any other setting.
3.3 Abliene Real-World Data
The ﬁnal set of data consists of byte counts recorded from
the Abliene Internet2 backbone network.2 Across 11 PoPs
in the continental United States with 41 network links, byte
counts were sampled into 10 minute time intervals from
April 7th 2003 to April 13th 2003, resulting in 1008 byte
count samples across each of the 41 links. Unfortunately,
this dataset is completely unlabeled with no prior annota-
tion of possible anomaly locations. To compensate for this
deﬁciency in the dataset, we will use this real world network
data to study how the new BasisDetect framework detects
anomalies that are found by previous network-wide anomaly
detection algorithms.
4. BASISDETECT OVERVIEW
Our automated BasisDetect framework for detecting net-
work anomalies is divided into three distinct components.
Practically speaking, these components are predicated on
having a small
initial set of network data with labeled
anomalies from which event characteristics can be learned
and the algorithm parameters are optimized against. The
components of the BasisDetect framework are:
1. Anomalous Dictionary Construction from Labeled Set
- Using a training set of labeled anomalies, we extract
signal characteristics that have been pre-established as
anomalous.
2. Anomaly Decomposition using Penalized Basis Pursuit
- Using our novel Penalized Basis Pursuit methodology
and the learned anomaly dictionary signals from the
previous step, the BasisDetect methodology extracts