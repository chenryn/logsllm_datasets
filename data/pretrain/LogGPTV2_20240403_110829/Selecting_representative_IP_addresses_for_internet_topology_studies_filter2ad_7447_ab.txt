space has more than 3 billion allocated unicast addresses,
with 4 machines probing at about 6000 probes/s [18]). For
this paper we consider censuses starting in Mar 2006 as
shown in Table 1, since censuses before this date used a
slightly diﬀerent collection methodology. The results of this
paper use all 22 censuses taken over the four years preced-
ing analysis, but we expect to update our results as new
censuses become available.
A census elicits a number of responses, including ECHO
REPLY messages as well as a variety of errors. Each census
is quite large, and more than 3 billion records per census,
22 censuses is over 260GB of raw data. We therefore pre-
process all censuses into a history map convenient for analy-
sis. A history map consists of a bitstring for each IP address
413where each 1 indicates a positive response, and a 0 indicates
either a non-response or negative response.
In paper, we only consider echo replies (“positive” re-
sponses) as indicating a responsive address. We also ex-
plored treating both positive and negative responses (desti-
nation unreachable and similar error replies) as predicting a
responsive address. However, we found that negative replies
only rarely are helpful in predicting future responsiveness.
We look at it28w and found 64% of positive responses were
also responsive in the next census (165M in it28w, 107M
of which respond in it29w). By contrast, of the 50M nega-
tive replies in it28w, only 2.7% (1.4M) respond positively in
it29w. We therefore believe that negative responses are of
little value in predicting future responsiveness.
We next show how this history map can predict future
response rates.
3.3 Prediction Method
Of our hitlist goals of responsiveness, completeness, and
stability, completeness and stability are under our control,
but responsiveness requires predicting the future. Our guid-
ance in this task is the prior history of each address. We
next review several prediction functions that strive to se-
lect the best representative for each /24 block, where best
is most likely to respond in the future.
Prediction functions take the prior history of address a as
input and weights that history in diﬀerent ways. Each bit of
the history is presented by ri(a), the response (1 for positive,
otherwise 0) of address a to the ith probe, numbered from 0
(oldest) to Nh −1, the most recent observation. We consider
several diﬀerent weights w(i) to get the scores s(a) in the
form:
s(a) =
Nh−1
X
i=0
ri(a)w(i)
For each block of addresses, the address with the highest
s(a) is selected as the best representative. We may bias this
by prior representatives to promote stability. In the case of
ties and no prior representative we select any top scoring
address in the block at random.
We considered several possible weights w(i). The simplest
is w(i) = 1, so all responses are averaged. To give more re-
cent observations greater inﬂuence we consider two biased
weights. With linear weighting, w(i) = (i + 1) ∗ 1/Nh, and
for a power function, w(i) = 1
Nh−i . Weighting of each ob-
servation for an 8-observation history is shown in Figure 1.
In addition, we can normalize scores by to the maximum
possible score (the minimum in all cases is zero), allowing
all to fall in the range 0 to 1.
As an example of the diﬀerent functions, Figure 2 shows
scores for three diﬀerent weights and diﬀerent history lengths.
For simplicity, we assume Nh = 8, shorter than we use in
practice (in Section 4.1.2 we vary history duration). We
consider three cases, all with 4 of 8 responding, but either
responding most recently (Figure 2a), in the middle past
(Figure 2b), or alternating response and non-response (Fig-
ure 2c). To a ﬁrst approximation, all three weights are about
the same, particularly with intermittent responsiveness in
Figure 2c. The diﬀerences in decay rates are more obvious
when responsiveness is consistent for blocks of time, with
power and linear decay faster than average in Figures 2a
and 2b. Finally, diﬀerence in history duration make a large
diﬀerence when a block is non-responsive, comparing the left
t
i
h
g
e
w
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
average
linear
power
 1
 2
 3
 4
 5
 6
 7
 8
history bits(old to new)
Figure 1: bits weight for diﬀerent function
and right parts of Figures 2a and 2b, and these eﬀects are
even greater when comparing across weights (for example,
compare history durations 1–4 of Figures 2a and 2b).
This framework provides ﬂexibility, but requires setting
several parameters. We later evaluate which weighting is
best (Section 4.1.1), how much history is beneﬁcial (Sec-
tion 4.1.2), and the underlying reasons addresses are diﬃcult
to predict (Section 4.1.3).
3.4 Gone-Dark Blocks
Firewalls are probably the greatest impediment to active
probing, since a conservative ﬁrewall can suddenly stop traf-
ﬁc an entire block. We will see in Section 4.1.3 that gone
dark blocks are one cause of poor representative responsive-
ness. A gone-dark blocks is one that contained responsive
addresses for some period of time, but then becomes un-
responsive and stays that way, due to ﬁrewall or possibly
renumbering. While we must select a representative for each
allocated block, even if populated only by non-responsive ad-
dresses, we would like to indicate our low expectations for
gone-dark blocks.
We deﬁne a block as gone dark within history Nd if, for
the most recent Nd observations, no address in the block re-
sponded, even though we had some positive response before
Nd observations.
We add gone-dark analysis to our hitlist generation by
overriding the representative’s score with a designated“gone-
dark” value to indicate our skepticism that it will reply.
We explored diﬀerent values of Nd and ultimately select
Nd = Nh = 16,
identifying only those addresses whose
responses have aged-out of our history as gone-dark. We
use this large value of Nd because this value maximizes the
absolute number of responsive representatives, while only
decreasing the percentage of responsive, predicted represen-
tatives a small amount.
For gone-dark blocks, we still select the representative as
the address with the best score. For allocated but never-
responsive blocks, we select the .1 address as the repre-
sentative because that is most likely to be ﬁrst used (Sec-
tion 5). In Section 4.1.3 we show the contribution of gone-
dark blocks to responsiveness.
414average
linear
power
e
r
o
c
s
d
e
t
i
h
g
e
w
 1
 0.8
 0.6
 0.4
 0.2
 0
average
linear
power
e
r
o
c
s
d
e
t
i
h
g
e
w
 1
 0.8
 0.6
 0.4
 0.2
 0
average
linear
power
e
r
o
c
s
d
e
t
i
h
g
e
w
 1
 0.8
 0.6
 0.4
 0.2
 0
8
7
6
5
4
3
2
1
0
8
7
6
5
4
3
2
1
0
8
7
6
5
4
3
2
1
0
history length (censuses)
(a) 0000 1111.
history length(censuses)
(b) 0011 1100.
history length (censuses)
(c) 0101 0101.
Figure 2: Comparison of three history functions for selected addresses.
3.5 Hitlist Description
To summarize, our hitlist contain three kind of represen-
tatives for all allocated /24 blocks: informed and predicted
representatives, where we select the best responder; gone-
dark representatives, where some address once responded
but has not recently; and allocated but never-responsive
blocks, where we pick .1 as the representative.
Table 6 lists the hitlists we have publicly released to-date.
We identify hitlists by the name of the last census used in
their creation, and include the number of censuses in the
history. Thus HL28/16 uses 16 hitlists through it28w. When
necessary, we add the gone-dark window, so HL28/16-3 uses
a window of 3.
If no gone-dark window is speciﬁed, we
disable gone-dark processing.
In addition to these public hitlists, Tables 2 and 3 show
unreleased hitlists used to evaluate our methods.
4. EVALUATION
We next evaluate the success of our hitlist: how accurate
are its predictions and how complete and stable is it? We
ﬁrst consider how responsiveness is aﬀected by choices in our
prediction mechanism. In Section 4.1.3 then look at causes
of prediction failure. Finally, we consider completeness and
stability.
4.1 Responsiveness
Our primary goal with prediction is responsiveness: how
accuracy is our prediction that the representatives in a hitlist
will respond in the future? We can deﬁne based on the
number responding in the future, Nr, from the number of
predicted representatives (including representatives of gone-
dark and informed predicted blocks) Np as:
α =
Nr
Np
Responsiveness accuracy is aﬀected by our choice of his-
tory weighting and length. We consider these next, and then
consider structural reasons perfect accuracy is impossible to
achieve.
Our general approach to test responsiveness is to gener-
ate a hitlist, then evaluate it against ICMP probes in the
next census. For example, the ﬁrst line of Table 2 evalu-
ates HL19/8, generated from the eight censuses from it12w
through it19w, tested against it20w. This approach has
the advantage of supporting retroactive evaluation of hitlist
weighting function
hitlist
average
linear power
HL19/8
HL21/8
HL23/8
HL25/8
HL27/8
0.50
0.53
0.53
0.53
0.54
0.51
0.54
0.54
0.54
0.55
0.51
0.55
0.54
0.54
0.55
Table 2: Fraction of responsive representatives across 5 dif-
ferent hitlists for three diﬀerent history weights.
quality under diﬀerent, controlled conditions. However, it
also means each representative is only given one opportu-
nity to be available. For this reason we report exact counts
of results, without error estimates such as standard devia-
tion. We evaluate repeatability of our results by considering
multiple hitlists at diﬀerent times.
4.1.1 Comparing History Weights
We ﬁrst consider how our weighting of prior history aﬀects
accuracy. Here we assume a history duration of 8 prior cen-
suses (a reasonable choice as evaluated next in Section 4.1.2),
and from that history we predict the results of the next cen-
sus for the three weights we deﬁned in Section 3.3. Since the
network is dynamic, our expectation is that biased weight-
ings will perform best since they favor recent information
over older information.
To answer this question, Table 2 compares our three weight-
ings for several predictions. Each line evaluates a diﬀerent
hitlist as generated with three diﬀerent weights, and eval-
uated for all predicted representatives (Np). The most im-
portant observation is that all weights provide quite simi-
lar performance—the worst case responsiveness is only 5%
worse than the best. Linear and power functions provide
marginally better responsiveness. The examples of the weights
in Figure 2 suggests on reason the diﬀerence is so small. For
many histories, all three weights produce roughly the same
relative scores.
4.1.2 Effects of History Duration
A second factor that can aﬀect responsiveness is the dura-
tion of history considered in a prediction. Does more history
provide more information, or does very old information be-
come irrelevant or even misleading?
415To study this question, we considered all history available
to us at time of analysis—then we had 18 Internet censuses
covering 3.5 years. We consider only the power weighting of
history, and look at the responsiveness of our predictions.
Table 3 shows responsiveness of our predictions as a func-
tion of history length, for ﬁve predictions. We see that very
short histories are insuﬃcient: prediction rates are a 1–2%
lower when fewer than 8 (about 1.5 years) observations are
considered. On the other hand, we see no diﬀerence in pre-
diction accuracy for histories from 8 to 16 censuses. (We
also looked at history duration with the average function,
and found there that long histories became slightly less ac-
curate, although only by 1–2%. This observation argues in
favor of a weighting that decays by history, like power.)
Finally, while longer histories may not improve the frac-
tion that respond, it does provide information that allows
more representatives to be selected. Table 3 shows the abso-
lute number of responders as a function of history duration.
Longer history allows 20k more responders with length 16
than with length 8. More history always increases the num-
ber responding, although with diminishing returns past a 12
censuses or so.
In practice, the incremental cost of longer history lengths
is not large. So we use a history length of 16 censuses in our
production lists.
Although 8 censuses provides slightly betters results, the
faction responding, only 55%, seems lower than we might ex-
pect. We therefore next consider causes of non-responsiveness.
4.1.3 Causes of Failed Responses
We found the observation that our best methods get only
55% responsiveness seems somewhat surprising. Surely such
a large amount of history (over three years of full censuses)
can be explored somehow to select representatives with greater
accuracy. To answer that question, we next explore the
causes of why representatives fail to respond. Our conclu-
sion is that it is unlikely that any prediction can do better
than about 70% because of the use of dynamic address as-