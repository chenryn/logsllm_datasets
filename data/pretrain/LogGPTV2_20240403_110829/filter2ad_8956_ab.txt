          bin->fd = bin->bk = bin;
        }
    #if MORECORE_CONTIGUOUS
      if (av != &main_arena)
    #endif
      set_noncontiguous (av);
      if (av == &main_arena)
        set_max_fast (DEFAULT_MXFAST);
      atomic_store_relaxed (&av->have_fastchunks, false);
      av->top = initial_top (av);
    }
（原文该部分代码错误，已修正）
首先，`malloc_init_state`被调用来初始化`malloc_state`。在这个过程当中，每一个在`bins`中的元素的`fd`和`bk`指针将会被设置为他自己的指针，然后`global_max_fast`将会被设置为0x40.
#### `malloc_consolidate`函数
`malloc_consolidate`将会尽量去合并`fastbins`中的块，并且把他们放到`unsorted
bin`中去，在从`fastbin`获取到一个释放块后，合并序列如下：
  1. 检查是否上一个邻接块为使用状态：如果不为使用状态，则将当前块合并到上一个邻接块
  2. 检查是否邻接的下一个块为top chunk：如果是，则把当前块设置为`top chunk`，并且修改`chunk_size`，如果不是，则进入下一步
  3. 检查是否邻接的下一块为使用状态：如果是，则直接把当前块放入`unsorted bin`，并且清除下一个块的`P`位。如果不是，则把下一块合并到当前块中，将当前块放入`unsorted bin`，清除当前下一邻接块的`P`位.
    /*
      ------------------------- malloc_consolidate -------------------------    
      malloc_consolidate is a specialized version of free() that tears
      down chunks held in fastbins.  Free itself cannot be used for this
      purpose since, among other things, it might place chunks back onto
      fastbins.  So, instead, we need to use a minor variant of the same
      code.
      Also, because this routine needs to be called the first time through
      malloc anyway, it turns out to be the perfect place to trigger
      initialization code.
    */
    static void malloc_consolidate(mstate av)
    {
      mfastbinptr*    fb;                 /* current fastbin being consolidated */
      mfastbinptr*    maxfb;              /* last fastbin (for loop control) */
      mchunkptr       p;                  /* current chunk being consolidated */
      mchunkptr       nextp;              /* next chunk to consolidate */
      mchunkptr       unsorted_bin;       /* bin header */
      mchunkptr       first_unsorted;     /* chunk to link to */
      /* These have same use as in free() */
      mchunkptr       nextchunk;
      INTERNAL_SIZE_T size;
      INTERNAL_SIZE_T nextsize;
      INTERNAL_SIZE_T prevsize;
      int             nextinuse;
      mchunkptr       bck;
      mchunkptr       fwd;
      /*
        If max_fast is 0, we know that av hasn't
        yet been initialized, in which case do so below
      */
      if (get_max_fast () != 0) {
        clear_fastchunks(av);
        unsorted_bin = unsorted_chunks(av);
        /*
          Remove each chunk from fast bin and consolidate it, placing it
          then in unsorted bin. Among other reasons for doing this,
          placing in unsorted bin avoids needing to calculate actual bins
          until malloc is sure that chunks aren't immediately going to be
          reused anyway.
        */
        maxfb = &fastbin (av, NFASTBINS - 1);
        fb = &fastbin (av, 0);
        do {
          p = atomic_exchange_acq (fb, NULL);
          if (p != 0) {
        do {
          check_inuse_chunk(av, p);
          nextp = p->fd;
          /* Slightly streamlined version of consolidation code in free() */
          size = chunksize (p);
          nextchunk = chunk_at_offset(p, size);
          nextsize = chunksize(nextchunk);
          if (!prev_inuse(p)) {
            prevsize = prev_size (p);
            size += prevsize;
            p = chunk_at_offset(p, -((long) prevsize));
            unlink(av, p, bck, fwd);
          }
          if (nextchunk != av->top) {
            nextinuse = inuse_bit_at_offset(nextchunk, nextsize);
            if (!nextinuse) {
              size += nextsize;
              unlink(av, nextchunk, bck, fwd);
            } else
              clear_inuse_bit_at_offset(nextchunk, 0);
            first_unsorted = unsorted_bin->fd;
            unsorted_bin->fd = p;
            first_unsorted->bk = p;
            if (!in_smallbin_range (size)) {
              p->fd_nextsize = NULL;
              p->bk_nextsize = NULL;
            }
            set_head(p, size | PREV_INUSE);
            p->bk = unsorted_bin;
            p->fd = first_unsorted;
            set_foot(p, size);
          }
          else {
            size += nextsize;
            set_head(p, size | PREV_INUSE);
            av->top = p;
          }
        } while ( (p = nextp) != 0);
          }
        } while (fb++ != maxfb);
      }
      else {
        malloc_init_state(av);
        check_malloc_state(av);
      }
    }
#### `__int_malloc`函数
`__libc_malloc`是从`bins`或者从`main
arena`里返回应用请求的块的函数，现在我们来讨论一下`__int_malloc`，也就是`libc`中`malloc`的内部实现
    checked_request2size (bytes, nb);
分配器首先将需要的大小转换为了实际分配块的大小，然后尝试按照以下顺序去获取需要的块：`fast bin`, `unsorted bin`, `small
bin`, `large bin`和`top chunk`。我们在这里一个一个讨论。
##### fastbin
    /*
       If the size qualifies as a fastbin, first check corresponding bin.
       This code is safe to execute even if av is not yet initialized, so we
       can try it without checking, which saves some time on this fast path.
    */
    if ((unsigned long) (nb) fd, victim))!= victim);
        if (victim != 0)
        {
             if (__builtin_expect (fastbin_index (chunksize (victim)) != idx, 0))
             {
                  errstr = "malloc(): memory corruption (fast)";
                errout:
                  malloc_printerr (check_action, errstr, chunk2mem (victim), av);
                  return NULL;
             }
             check_remalloced_chunk (av, victim, nb);
             void *p = chunk2mem (victim);
             alloc_perturb (p, bytes);
             return p;
         }
    }
如果大小比`global_max_fast`小，或等于，分配器会尝试去搜索`fastbin`来找适合的块，`fastbin`的index由块大小决定。
##### small bin
    /*
       If a small request, check regular bin.  Since these "smallbins"
       hold one size each, no searching within bins is necessary.
       (For a large request, we need to wait until unsorted chunks are
       processed to find best fit. But for small ones, fits are exact
       anyway, so we can check now, which is faster.)
     */
    if (in_smallbin_range (nb))
      {
        idx = smallbin_index (nb);
        bin = bin_at (av, idx);
        if ((victim = last (bin)) != bin)
          {
            if (victim == 0) /* initialization check */
              malloc_consolidate (av);
            else
            {
                bck = victim->bk;
            if (__glibc_unlikely (bck->fd != victim))
                {
                    errstr = "malloc(): smallbin double linked list corrupted";
                    goto errout;
                }
                set_inuse_bit_at_offset (victim, nb);
                bin->bk = bck;
                bck->fd = bin;
                if (av != &main_arena)
          set_non_main_arena (victim);
                check_malloced_chunk (av, victim, nb);
                void *p = chunk2mem (victim);
                alloc_perturb (p, bytes);
                return p;
             }
          }
      }
small bin的index由块大小决定，在被认为应当使用small bin时，分配器会尝试在small bin中移除掉第一个释放块。
##### unsorted bin
    while ((victim = unsorted_chunks (av)->bk) != unsorted_chunks (av))
    {
         bck = victim->bk;
         if (__builtin_expect (chunksize_nomask (victim)  av->system_mem, 0))
            malloc_printerr (check_action, "malloc(): memory corruption",
                                 chunk2mem (victim), av);
         size = chunksize (victim);
         /*
             If a small request, try to use last remainder if it is the
             only chunk in unsorted bin.  This helps promote locality for
             runs of consecutive small requests. This is the only
             exception to best-fit, and applies only when there is
             no exact fit for a small chunk.
         */
         if (in_smallbin_range (nb) &&
             bck == unsorted_chunks (av) &&
             victim == av->last_remainder &&
             (unsigned long) (size) > (unsigned long) (nb + MINSIZE))
         {
             /* split and reattach remainder */
             remainder_size = size - nb;
             remainder = chunk_at_offset (victim, nb);
             unsorted_chunks (av)->bk = unsorted_chunks (av)->fd = remainder;
             av->last_remainder = remainder;
             remainder->bk = remainder->fd = unsorted_chunks (av);
             if (!in_smallbin_range (remainder_size))
             {
                remainder->fd_nextsize = NULL;
                remainder->bk_nextsize = NULL;
             }
             set_head (victim, nb | PREV_INUSE |
                      (av != &main_arena ? NON_MAIN_ARENA : 0));
             set_head (remainder, remainder_size | PREV_INUSE);