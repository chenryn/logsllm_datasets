title:Dissecting UbuntuOne: Autopsy of a Global-scale Personal Cloud Back-end
author:Ra&apos;ul Gracia Tinedo and
Yongchao Tian and
Josep Samp&apos;e and
Hamza Harkous and
John Lenton and
Pedro Garc&apos;ıa L&apos;opez and
Marc S&apos;anchez Artigas and
Marko Vukolic
Autopsy of a Global-scale Personal Cloud Back-end
Dissecting UbuntuOne:
Raúl Gracia-Tinedo
Universitat Rovira i Virgili
PI:EMAIL
Yongchao Tian
Eurecom
PI:EMAIL
Hamza Harkous
EPFL
hamza.harkous@epﬂ.ch
John Lenton
Canonical Ltd.
PI:EMAIL
Josep Sampé
Universitat Rovira i Virgili
PI:EMAIL
Pedro García-López
Universitat Rovira i Virgili
PI:EMAIL
Marc Sánchez-Artigas
Universitat Rovira i Virgili
PI:EMAIL
Marko Vukoli´c
IBM Research - Zurich
PI:EMAIL
ABSTRACT
Personal Cloud services, such as Dropbox or Box, have been
widely adopted by users. Unfortunately, very little is known
about the internal operation and general characteristics of
Personal Clouds since they are proprietary services.
In this paper, we focus on understanding the nature of
Personal Clouds by presenting the internal structure and a
measurement study of UbuntuOne (U1). We ﬁrst detail the
U1 architecture, core components involved in the U1 meta-
data service hosted in the datacenter of Canonical, as well
as the interactions of U1 with Amazon S3 to outsource data
storage. To our knowledge, this is the ﬁrst research work to
describe the internals of a large-scale Personal Cloud.
Second, by means of tracing the U1 servers, we provide
an extensive analysis of its back-end activity for one month.
Our analysis includes the study of the storage workload, the
user behavior and the performance of the U1 metadata store.
Moreover, based on our analysis, we suggest improvements
to U1 that can also beneﬁt similar Personal Cloud systems.
Finally, we contribute our dataset to the community, which
is the ﬁrst to contain the back-end activity of a large-scale
Personal Cloud. We believe that our dataset provides unique
opportunities for extending research in the ﬁeld.
Categories and Subject Descriptors
C.4 [Performance of Systems]: Measurement techniques;
K.6.2 [Management of Computing and Information
Systems]: Installation management–Performance and us-
age measurement
Keywords
Personal cloud; performance analysis; measurement
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
IMC’15, October 28–30, 2015, Tokyo, Japan.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-3848-6/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2815675.2815677.
1.
INTRODUCTION
Today, users require ubiquitous and transparent storage
to help handle, synchronize and manage their personal data.
In a recent report [1], Forrester research forecasts a market
of $12 billion in the US related to personal and user-centric
cloud services by 2016. In response to this demand, Personal
Clouds like Dropbox, Box and UbuntuOne (U1) have pro-
liferated and become increasingly popular, attracting com-
panies such as Google, Microsoft, Amazon or Apple to oﬀer
their own integrated solutions in this ﬁeld.
In a nutshell, a Personal Cloud service oﬀers automatic
backup, ﬁle sync, sharing and remote accessibility across a
multitude of devices and operating systems. The popularity
of these services is based on their easy to use Software-as-a-
Service (SaaS) storage facade to ubiquitous Infrastructure-
as-a-Service (IaaS) providers like Amazon S3 and others.
Unfortunately, due to the proprietary nature of these sys-
tems, very little is known about their performance and char-
acteristics, including the workload they have to handle daily.
And indeed, the few available studies have to rely on the so-
called “black-box” approach, where traces are collected from
a single or a limited number of measurement points, in order
to infer their properties. This was the approach followed by
the most complete analysis of a Personal Cloud to date, the
measurement of Dropbox conducted by Drago et al. [2]. Al-
though this work describes the overall service architecture,
it provides no insights on the operation and infrastructure of
the Dropbox’s back-end. And also, it has the additional ﬂaw
that it only focuses on small and speciﬁc communities, like
university campuses, which may breed false generalizations.
Similarly, several Personal Cloud services have been ex-
ternally probed to infer their operational aspects, such as
data reduction and management techniques [3, 4, 5], or even
transfer performance [6, 7]. However, from external vantage
points, it is impossible to fully understand the operation of
these systems without fully reverse-engineering them.
In this paper, we present results of our study of U1: the
Personal Cloud of Canonical, integrated by default in Linux
Ubuntu OS. Despite the shutdown of this service on July
2014, the distinguishing feature of our analysis is that it has
been conducted using data collected by the provider itself.
U1 provided service to 1.29 million users at the time of the
study on January-February 2014, which constitutes the ﬁrst
complete analysis of the performance of a Personal Cloud in
155UbuntuOne Analisys
Finding
90% of ﬁles are smaller than 1MByte (P).
Storage Workload (§ 5)
User Behavior (§ 6)
Back-end Performance (§ 7)
18.5% of the upload traﬃc is caused by ﬁle updates
(C).
We detected a deduplication ratio of 17% in one
month (C).
DDoS attacks against U1 are frequent (N).
1% of users generate 65% of the traﬃc (P).
Data management operations (e.g., uploads, ﬁle dele-
tions) are normally executed in long sequences (C).
User operations are bursty; users transition between
long, idle periods and short, very active ones (N).
A 20-node database cluster provided service to 1.29M
users without symptoms of congestion (N).
RPCs service time distributions accessing the meta-
data store exhibit long tails (N).
In short time windows, load values of API servers/DB
shards are very far from the mean value (N).
Implications and Opportunities
Object storage services normally used as a cloud service
are not optimized for managing small ﬁles [8].
Changes in ﬁle metadata cause high overhead since the
U1 client does not support delta updates (e.g. .mp3 tags).
File-based cross-user deduplication provides an attrac-
tive trade-oﬀ between complexity and performance [5].
Further research is needed regarding secure protocols and
automatic countermeasures for Personal Clouds.
Very active users may be treated in an optimized manner
to reduce storage costs.
This correlated behavior can be exploited by caching and
prefetching mechanisms in the server-side.
User behavior combined with the user per-shard data
model impacts the metadata back-end load balancing.
The user-centric data model of a Personal Cloud makes
relational database clusters a simple yet eﬀective ap-
proach to scale out metadata storage.
Several factors at hardware, OS and application-level are
responsible for poor tail latency in RPC servers [9].
Further research is needed to achieve better load balanc-
ing under this type of workload.
C: Conﬁrms previous results, P: Partially aligned with previous observations, N: New observation
Table 1: Summary of some of our most important ﬁndings and their implications.
the wild. Such a unique data set has allowed us to reconﬁrm
results from prior studies, like that of Drago et al. [2], which
paves the way for a general characterization of these systems.
But it has also permitted us to expand the knowledge base
on these services, which now represent a considerable volume
of the Internet traﬃc. According to Drago et al. [2], the total
volume of Dropbox traﬃc accounted for a volume equivalent
to around one third of the YouTube traﬃc on a campus
network. We believe that the results of our study can be
useful for both researchers, ISPs and data center designers,
giving hints on how to anticipate the impact of the growing
adoption of these services. In summary, our contributions
are the following:
Back-end architecture and operation of U1. This
work provides a comprehensive description of the U1 ar-
chitecture, being the ﬁrst one to also describe the back-end
infrastructure of a real-world vendor. Similarly to Dropbox
[2], U1 decouples the storage of ﬁle contents (data) and their
logical representation (metadata). Canonical only owns the
infrastructure for the metadata service, whereas the actual
ﬁle contents are stored separately in Amazon S3. Among
other insights, we found that U1 API servers are charac-
terized by long tail latencies and that a sharded database
cluster is an eﬀective way of storing metadata in these sys-
tems. Interestingly, these issues may arise in other systems
that decouple data and metadata as U1 does [10].
Workload analysis and user behavior in U1. By trac-
ing the U1 servers in the Canonical datacenter, we provide
an extensive analysis of its back-end activity produced by the
active user population of U1 for one month (1.29M distinct
users). Our analysis conﬁrms already reported facts, like
the execution of user operations in long sequences [2] and
the potential waste that ﬁle updates may induce in the sys-
tem [4, 5]. Moreover, we provide new observations, such as
a taxonomy of ﬁles in the system, the modeling of burstiness
in user operations or the detection of attacks to U1, among
others. Table 1 summarizes some of our key ﬁndings.
Potential improvements to Personal Clouds. We sug-
gest that a Personal Cloud should be aware of the behavior
of users to optimize its operation. Given that, we discuss
the implications of our ﬁndings to the operation of U1. For
instance, ﬁle updates in U1 were responsible for 18.5% of
upload traﬃc mainly due to the lack of delta updates in the
desktop client. Furthermore, we detected 3 DDoS attacks
in one month, which calls for further research in automatic
attack countermeasures in secure and dependable storage
protocols. Although our observations may not apply to all
existing services, we believe that our analysis can help to
improve the next generation of Personal Clouds [10, 4].
Publicly available dataset. We contribute our dataset
(758GB) to the community and it is available at http://
cloudspaces.eu/results/datasets. To our knowledge, this
is the ﬁrst dataset that contains the back-end activity of
a large-scale Personal Cloud. We hope that our dataset
provides new opportunities to researchers in further under-
standing the internal operation of Personal Clouds, promot-
ing research and experimentation in this ﬁeld.
Roadmap: The rest of the paper is organized as follows. § 2
provides basic background on Personal Clouds. We describe
in § 3 the details of the U1 Personal Cloud.
In § 4 we
In § 5, § 6 and
explain the trace collection methodology.
§ 7 we analyze the storage workload, user activity and back-
end performance of U1, respectively. § 8 discusses related
work. We discuss the implications of our insights and draw
conclusions in § 9.
2. BACKGROUND
A Personal Cloud can be loosely deﬁned as a uniﬁed dig-
ital locker for users’ personal data, oﬀering at least three
key services: ﬁle storage, synchronization and sharing [11].
Numerous services such as Dropbox, U1 and Box fall under
this deﬁnition.
From an architectural viewpoint, a Personal Cloud ex-
hibits a 3-tier architecture consisting of: (i) clients, (ii) syn-
chronization or metadata service and (iii) data store [2, 10].
Thus, these systems explicitly decouple the management of
ﬁle contents (data) and their logical representation (meta-
data). Companies like Dropbox and Canonical only own
the infrastructure for the metadata service, which processes
requests that aﬀect the virtual organization of ﬁles in user
volumes. The contents of ﬁle transfers are stored separately
in Amazon S3. An advantage of this model is that the Per-
sonal Cloud can easily scale out storage capacity thanks to
the “pay-as-you-go” cloud payment model, avoiding costly
investments in storage resources.
In general, Personal Clouds provide clients with 3 main
types of access to their service: Web/mobile access, Repre-
sentational State Transfer (REST) APIs [7, 12] and desktop
156API Operation
ListVolumes
Related RPC
dal.list_volumes This operation is normally performed at the beginning of a session and lists all the volumes of a user
Description
ListShares
dal.list_shares
(Put/Get)Content
see appendix A
Make
Unlink
Move
CreateUDF
DeleteVolume
GetDelta
Authenticate
dal.make_dir
dal.make_file
dal.unlink_node
dal.move
dal.create_udf
dal.delete_volume Deletes a volume and the contained nodes.
dal.get_delta
auth.get_user_id_
from_token
(root, user-deﬁned, shared).
This operation lists all the volumes of a user that are of type shared.
In this operation, ther ﬁeld
shared by is the owner of the volume and shared to is the user to which that volume was shared with.
In this operation, the ﬁeld shares represents the number of volumes type shared of this user.
These operations are the actual ﬁle uploads and downloads, respectively. The notiﬁcation goes to the
U1 back-end but the actual data is stored in a separate service (Amazon S3). A special process is
created to forward the data to Amazon S3. Since the upload management in U1 is complex, we refer
the reader to appendix A for a description in depth of upload transfers.
This operation is equivalent to a “touch” operation in the U1 back-end. Basically, it creates a ﬁle node
entry in the metadata store and normally precedes a ﬁle upload.
Delete a ﬁle or a directory from a volume.
Moves a ﬁle from one directory to another.
Creates a user-deﬁned volume.
Get the diﬀerences between the server volume and the local one (generations).
Operations managed by the servers to create sessions for users.
Table 2: Description of the most relevant U1 API operations.
clients. Our measurements in this paper focus on the desk-
top client interactions with U1. Personal Cloud desktop
clients are very popular among users since they provide au-
tomatic synchronization of user ﬁles across several devices
(see Section 3.3). To achieve this, desktop clients and the
server-side infrastructure communicate via a storage proto-
col.
In most popular Personal Cloud services (e.g., Drop-
box), such protocols are proprietary.
U1 Personal Cloud was a suite of online services oﬀered
by Canonical that enabled users to store and sync ﬁles on-
line and between computers, as well as to share ﬁles/folders
with others using ﬁle synchronization. Until the service was
discontinued in July 2014, U1 provided desktop and mobile
clients and a Web front-end. U1 was integrated with other
Ubuntu services, like Tomboy for notes and U1 Music Store
for music streaming.
3. THE U1 PERSONAL CLOUD
In this section, we ﬁrst describe the U1 storage protocol
used for communication between clients and the server-side
infrastructure (Sec. 3.1). This will facilitate the understand-
ing of the system architecture (Sec. 3.2). We then discuss
the details of a U1 desktop client (Sec. 3.3). Finally, we
give details behind the core component of U1, its metadata
back-end (Sec. 3.4).
3.1 U1 Storage Protocol
U1 uses its own protocol (ubuntuone-storageprotocol)
based on TCP and Google Protocol Buﬀers1. In contrast to
most commercial solutions, the protocol speciﬁcations and
client-side implementation are publicly available2. Here, we
describe the protocol in the context of its entities and oper-
ations. Operations can be seen as end-user actions intended
to manage one/many entities, such as a ﬁle or a directory.
3.1.1 Protocol Entities
In the following, we deﬁne the main entities in the proto-
col. Note that in our analysis, we characterize and identify
the role of these entities in the operation of U1.
Node: Files and directories are nodes in U1. The protocol
supports CRUD operations on nodes (e.g. list, delete, etc.).
The protocol assigns Universal Unique Identiﬁers (UUIDs)
to both node objects and their contents, which are generated
in the back-end.
1
2
https://wiki.ubuntu.com/UbuntuOne
https://launchpad.net/ubuntuone-storage-protocol
Volume: A volume is a container of node objects. During
the installation of the U1 client, the client creates an initial
volume to store ﬁles with id=0 (root). There are 3 types of
volumes:
i) root/predeﬁned, ii) user deﬁned folder (UDF),
which is a volume created by the user, and iii) shared (sub-
volume of another user to which the current user has access).
Session: The U1 desktop client establishes a TCP connec-
tion with the server and obtains U1 storage protocol ses-
sion (not HTTP or any other session type). This session is
used to identify a user’s requests during the session lifetime.
Usually, sessions do not expire automatically. A client may
disconnect, or a server process may go down, and that will
end the session. To create a new session, an OAuth [13]
token is used to authenticate clients against U1. Tokens
are stored separately in the Canonical authentication ser-
vice (see § 3.4.1).
3.1.2 API Operations
The U1 storage protocol oﬀers an API consisting of the
data management and metadata operations that can be exe-
cuted by a client. Metadata operations are those operations
that do not involve transfers to/from the data store (i.e.,
Amazon S3), such as listing or deleting ﬁles, and are entirely
managed by the synchronization service. On the contrary,
uploads and downloads are, for instance, typical examples
of data management operations.
In Table 2 we describe the most important protocol oper-
ations between users and the server-side infrastructure. We
traced these operations to quantify the system’s workload
and the behavior of users.
3.2 Architecture Overview
As mentioned before, U1 has a 3-tier architecture consist-
ing of clients, synchronization service and the data/metadata
store. Similarly to Dropbox [2], U1 decouples the storage of
ﬁle contents (data) and their logical representation (meta-
data). Canonical only owns the infrastructure for the meta-
data service, which processes requests that aﬀect the virtual
organization of ﬁles in user volumes. The actual contents of
ﬁle transfers are stored separately in Amazon S3.
However, U1 treats client requests diﬀerently from Drop-
box. Namely, Dropbox enables clients to send requests ei-
ther to the metadata or storage service depending on the
request type. Therefore, the Dropbox infrastructure only
processes metadata/control operations. The cloud storage
service manages data transfers, which are normally orches-
trated by computing instances (e.g. EC2).
157In contrast, U1 receives both metadata requests and data
transfers of clients. Internally, the U1 service discriminates
client requests and contacts either the metadata store or
the storage service. For each upload and download request,
a new back-end process is instantiated to manage the data
transfer between the client and S3 (see appendix A). There-
fore, the U1 model is simpler from a design perspective, yet
this comes at the cost of delegating the responsibility of pro-
cessing data transfers to the metadata back-end.
U1 Operation Workﬂow. Imagine a user that initiates
the U1 desktop client (§ 3.3). At this point, the client sends
an Authenticate API call (see Table 2) to U1, in order to
establish a new session. An API server receives the request
and contacts to the Canonical authentication service to ver-
ify the validity of that client (§ 3.4.1). Once the client has
been authenticated, a persistent TCP connection is estab-
lished between the client and U1. Then, the client may send
other management requests on user ﬁles and directories.
To understand the synchronization workﬂow, let us as-
sume that two clients are online and work on a shared folder.
Then, a client sends an Unlink API call to delete a ﬁle from
the shared folder. Again, an API server receives this re-
quest, which is forwarded in form of RPC call to a RPC
server (§ 3.4). As we will see, RPC servers translate RPC
calls into database query statements to access the correct
metadata store shard (PostgreSQL cluster). Thus, the RPC
server deletes the entry for that ﬁle from the metadata store.
When the query ﬁnishes, the result is sent back from the
RPC server to the API server that responds to the client
that performed the request. Moreover, the API server that
handled the Unlink notiﬁes the other API servers about this
event that, in turn, is detected by the API server to which
the second user is connected. This API server notiﬁes via
push to the second client, which deletes that ﬁle locally. The
API server ﬁnishes by deleting the ﬁle also from Amazon S3.
Next, we describe in depth the diﬀerent elements involved
in this example of operation: The desktop client, the U1
back-end infrastructure and other key back-end services to
the operation of U1 (authentication and notiﬁcations).
3.3 U1 Desktop Client
U1 provides a user friendly desktop client, implemented
in Python (GPLv3), with a graphical interface that enables
users to manage ﬁles. It runs a daemon in the background