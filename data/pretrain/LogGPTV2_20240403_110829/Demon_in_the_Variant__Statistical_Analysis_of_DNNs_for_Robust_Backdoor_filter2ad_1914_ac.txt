SentiNet regards the images on the top-right corner as in-
fected, since they have a high “fooled count” when includ-
ing the classiﬁcation-matter component and a high decision
conﬁdence when carrying the inert component. However, as
illustrated in Fig. 5, under TaCT, infected images stay on
the bottom-right corner, together with normal images. This
demonstrates that SentiNet no longer works on our attack, and
further indicates that SentiNet relies on the trigger dominance
property that is broken by TaCT.
Activation Clustering. Activation Clustering (AC) [4] cap-
tures infected images from their unique representations,
through separating activations (representations) on the last
hidden layer for infected images from those for normal im-
ages. Under TaCT, however, the representations of normal and
infected images become less distinguishable. As a result, the
2-means algorithm used by AC becomes ineffective, which
has been conﬁrmed in our experiments.
Speciﬁcally, we launch TaCT on GTSRB to infect models
and then use these infected models to get the activation for
every image. After obtaining the activations, we project each
activation vector onto a 10-dimensional space based upon its
ﬁrst 10 independent components (same with AC) and then
used 2-means to cluster the dimension-reducted vectors of
images in each class. Fig. 6 shows images’ sihouette score,
the criteria used by AC to measure how well 2-means ﬁt the
data for determining which class is infected. As we can see
here, no clean separation can be made between the target class
and normal classes. Note that we see many outliers outside
the target’ box, indicating that 2-means cannot ﬁt this class
well.
Tran et al. [40] propose another defense against backdoor at-
tack, based on Spectral Signatures (SS) of representations. Ac-
tually, SS is a special version of AC where defenders project
representations onto their ﬁrst 2 principal components (AC
uses 10 Independent Components Analysis). Thus just like
AC, this approach is not effective on our attack. The result is
not included due to the space limit.
4 Statistical Contamination Analyzer
In the presence of source-speciﬁc backdoors, which can be
easily injected through TaCT, the representations of attack im-
ages (trigger-carrying images) become almost indistinguish-
able from those of normal images, rendering those existing
detection techniques being less effective. So to detect the
backdoors, we have to go beyond a single class and look at
the distribution of the representations across all the classes
that a data-contamination attack is hard to alter. To this end,
we present in this section a new technique called Statistical
Contamination Analyzer (SCAn) to capture such an anomaly
caused by adversaries and further demonstrate that the new
approach is not only effective against TaCT but also robust to
other black-box attacks.
4.1 Design
Idea. A key observation is that in a backdoor contamination
attack, the adversary attempts to cheat a model by “merging”
two sets of images into the class with target label: those le-
gitimately belonging to the label and those with triggers but
originally from another label. This effort leads to a fundamen-
tal difference between the images originally in the target class
and those in the other classes, in terms of their representation
distributions, under the following assumptions:
• Two-component decomposition. In the representation space,
each point can be decomposed into two independent compo-
nents: a class-speciﬁc identity and a variation component.
• Universal variation. The variation components in any unin-
fected class follow the same distribution as those of benign
images in the attack class.
Prior research [44] shows that, in face recognition, an image
can be decomposed into three mutually orthogonal compo-
nents: within-class, between-class and noise. In DNN scenar-
ios, we assume a well-trained model largely eliminates the
noise and enhances the rest two components. Although the
variation component does not contribute directly to the classi-
ﬁcation task in a DNN model, it is often extracted through the
representation learning as it describes the recurrent and robust
signal in the input data. We note that the previous backdoor
detection approaches overlook the separation of these two
components, and exploit only the information within the vari-
ation components of the target class, which is useful to detect
previous attacks, while reduces the sensitivity in detecting
more advanced attacks like TaCT.
The universal variation assumption further assumes that
the variation component of an input sample is independent
of its label (i.e., sample class); as a result, the distribution
learned from one class (e.g., a non-target class) can be trans-
ferred to another one (e.g., the target class without infection).
Intuitively, in face recognition, smile is a variation compo-
nent adopted by different human individuals, leading to the
common transformation of face images independent of the
identity of which individual (i.e., the class) [44]. We believe
that the two-component and universal variation assumptions
are valid for not only face recognition but also many other
classiﬁcation tasks such as trafﬁc sign recognition etc.
By decomposing samples in both normal and infected
classes, we are able to obtain a ﬁner-grained observation
USENIX Association
30th USENIX Security Symposium    1547
Figure 5: Demonstration of SentiNet
against TaCT on GTSRB.
Figure 6: Sihouettte scores of AC against TaCT on GTSRB. 0 is the target label, 1 is the source label. Box plot
shows quartiles.
Figure 7: An illustration of Statistical Contamination Analyzer.
about the impacts of triggers on classiﬁcation that cannot
be seen by simply clustering representations within the in-
fected class, as prior research does. Fig. 8 shows an example,
where the representations of samples in the infected class
(right) can be viewed as a mixture of two groups, the attack
samples and the normal samples, each decomposed into a dis-
tinct identity component and a common variation component;
in comparison, without the two-component decomposition,
the representations of the samples in the infected and normal
class are indistinguishable.
Formally, the representation of an input sample x can be
decomposed into two latent vectors:
r = R(x) = µt + ε
(2)
where µt is the identity vector (component) of the class t
that x belongs to, and ε is the variation vector of x, which
follows a distribution independent of t. We denote by Xt the
set of the samples in the class t, and by Rt the set of their
representations, i.e., Rt = {R(xi)|xi ∈ Xt}.
Figure 8: A schematic illustration of the assumption of two-component
decomposition (right) in the representation space, in comparison with the
naive homogeneous assumption (left).
In the presence of a backdoor attack, samples in a target
class t∗ include two non-overlapping subgroups: normal sam-
∪ R attack
ples and attack samples, i.e., Rt∗ = R normal
. As a
result, the representations of samples in the target class follow
a multivariate mixture distribution: for each xi ∈ Xt∗,
t∗
t∗
ri = δiµ1 + (1− δi)µ2 + ε
(3)
where µ1 and µ2 represent the identity vectors of normal and
attack samples in the class t∗, respectively, and δi = 1 if the xi
is a normal sample and δi = 0 otherwise. On the other hand,
the representations of samples in an uninfected, normal class
t form a homogeneous population: r = µt + ε. Therefore, the
task of backdoor detection can be formulated as a hypothesis
test problem: given the representations of input data from a
speciﬁc class t, we want to test whether it is more likely from
a mixture group (as deﬁned in (3)) or from a single group (as
deﬁned in (2)). Notably, the problem is non-trivial because the
input vectors are of high dimension (hundreds or thousands
dimensions), and more importantly, the parameters (i. e., µt
and ε) are unknown for the mixture model and needed to be
derived simultaneously with the hypothesis test. Finally, our
approach does not rely on the assumptions underlying the
current backdoor detection (section 3): the trigger-dominant
representations are signiﬁcantly different from those of le-
gitimate samples. Instead, we investigate the distributions of
the representations from samples in all classes including the
contaminated one: the class with a mixture of two groups of
feature vectors is considered to be contaminated.
Algorithm. Our approach utilizes several statistical methods
to estimate the most likely parameters for the decomposi-
tion and untangling models and then detect an infected label
through a likelihood ratio test. It has the following steps, as
illustrated in Fig. 7.
Step 1: Leverage the target model to generate representations
for all input images from a clean set and the training set that
contains both the attack.
Step 2: Estimate the parameters in the decomposition model
(Eqn. 2) by running an EM algorithm on the representations
of the clean set for identifying covariance matrices (Sε and
Sµ, the covariance matrix of ε and µ) with a high conﬁdence.
Step 3: Across all images in each class, leverage the parame-
ters (Sε and Sµ) estimated on the clean dataset to calculate the
identity vector of this class and decompose the representations
of this class (Eqn. 12).
Step 4: Across all images in each class, use an iterative method
1548    30th USENIX Security Symposium
USENIX Association
00.20.40.60.81AvgConf00.20.40.60.81FooledInfectedNormalFittedThreshold-0.500.51silhouette scorelabel023456789101112131415161718192021222324252627282930313233343536373839404142012345-505InfectedNormal=1=2012345-505InfectedNormal12to estimate the parameters for the mixture model (Eqn. 3)
containing two subgroups.
Step 5: For images in each class, perform the likelihood ratio
test on their representations using the mixture model (from
step 4) against the null hypothesis – the decomposition model
(from step 3); if the null hypothesis is rejected, the correspond-
ing class is reported to be contaminated (infected).
4.2 Technical Details
Two-component decomposition. Under the assumptions of
two-component decomposition and universal variation, a rep-
resentation vector can be described as the sum of two latent
vectors: r = µ + ε, with µ and ε each following a normal dis-
tribution: µ ∼ N(0,Sµ) and ε ∼ N(0,Sε), where Sµ and Sε are
two unknown covariance matrices, which need to be estimated.
Notably, Sµ is so called between-class covariance matrix and
Sε is the within-class covariance matrix. We estimate them by
using an EM method similar to the method proposed by Chen
et al [5]. The details are provided in Appendix B. Here, we
highlight that the between-class information captured by Sµ
can be further used to infer the most likely position where a
unknown identity vector should be, given an already known
identity vector (Eqn. 12). Our decomposition method needs a
clean dataset, a much smaller one than the training set.
Two-subgroup untangling. We assume the representations
of samples in the infected class follow a mixture model of
two Gaussian distributions, one for the group of normal sam-
ples (N(µ1,S1)) and the other for the group of attack samples
(N(µ2,S2). If the labels (normal vs attack) are already as-
signed to these samples, a hyperplane that optimally separate
their representations into two subgroups can be determined by
a Linear Discriminant Analysis (LDA) [25], which maximizes
the Fisher’s Linear Discriminant (FLD)
where
FLD(v)= vT ΣBv/vT ΣW v
ΣB = (µ1 − µ2)(µ1 − µ2)T
ΣW = S1 + S2
(4)
Intuitively, a larger FLD corresponds to more distant projected
means and concentrated projected vectors for each of these
two subgroups. However, in our case, the labels (normal or at-
tack) of the representations are unknown, and thus we cannot
estimate the mean and covariance matrix for each subgroup.
To address this challenge, we ﬁrst simplify the problem by
assuming S1 = S2 = Sε, according to the universal variation
assumption, and then use an iterative algorithm to simulta-
neously estimate the model parameters (µ1 and µ2) and the
subgroup label for each sample.
Step-1: We randomly assign the subgroup label to each sample
in the class of interest.
Step-2: We estimate the model parameters (µ1 and µ2) on the
representations of normal samples and attack samples, respec-
tively, using the EM-like two-component decomposition, as
described above.
Step-3: We compute the optimal discriminating hyperplane
(denoted by vector v) by maximizing the FLD,
v = S−1
ε (µ1 − µ2)
(5)
Step 4: According to the FLD results, we re-compute the
subgroup label ci for each sample i. (e.g., ci = 1 represents a
benign sample, and ci = 2 represents an attack sample),
(cid:40)
ci=
1,vT r  50 the χ2 distribution
is sufﬁciently close to a normal distribution for the differ-
√
ence can be ignored [3]. Concretely, the regularized variable
¯Jt = (Jt −k)/
2k approximately follows the standard normal
distribution when k > 50. Therefore, we leverage the normal
distribution of the Median Absolute Deviation (MAD) [20] to
detect the class(es) with abnormally great values of J. Specif-
USENIX Association
30th USENIX Security Symposium    1549
ically, we use J∗
t as our test statistic for the class t:
where
J∗
t
˜J
= | ¯Jt − ˜J|/(MAD( ¯J)∗ 1.4826)
= median({ ¯Jt : t ∈ L})
MAD( ¯J)= median({| ¯Jt − ˜J| : t ∈ L})
Here, the constant (1.4826) is a normalization constant for
the standard normal distribution followed by ¯Jt
4. Therefore,
when J∗
t > 7.3891 = exp(2), the null hypothesis H0 can be
rejected with a conﬁdence > (1− 1e−9), and thus the class t
is reported as being contaminated.
4.3 Effectiveness against TaCT
(a) Box
Figure 9: Four kinds of triggers used in our experiments
(b) Normal
(c) Square
(d) Watermark
Various tasks and triggers. We ran TaCT on three datasets
with four different triggers, which have also been used in
prior works5 [4, 9, 24, 42] (Fig. 9). These three datasets cover
not only different tasks but also various data distributions.
Speciﬁcally, GTSRB has a small number of classes and im-
ages; ILSVRC2012 contains many classes with each involv-
ing a large number of images; MegaFace is characterized by
tremendous classes but each has only a few images.
On each dataset, we trained 5 models: 4 TaCT infected ones
and a benign model (without backdoor). To infect a model, we
injected 2% attack images and 1% cover images into its train-
ing set. As shown in Table 5, each infected model achieved a
performance comparable with that of its counterpart trained
on clean images. Further from each dataset, we randomly
selected 10% of its images as clean data set for the decom-
position and parameter estimation (Eqn. 2), and then ran the
untangling algorithm on each class by using the variation
matrices (Sε) constructed from the decomposition process.
Our study shows that SCAn is very effective in detecting the
TaCT attack. Particularly, J∗ of the target class was found
to be well above those of the uninfected classes by orders
of magnitude. Fig. 10 illustrates the logarithm of J∗ (ln(J∗)),
showing that SCAn can keep effectiveness on various datasets
and triggers. Further, we investigated the effect from different
size and location of the trigger by launching several TaCTs
with the box trigger on GTSRB and kept other settings are
the same with above experiments. Fig 22 demonstrates the
results. We observed that the trigger with small size and in the