“Quantum” Classiﬁcation of Malware
John Seymour
PI:EMAIL
Charles Nicholas
PI:EMAIL
August 24, 2015
Abstract
Quantum computation has recently become
an important area for security research, with
its applications to factoring large numbers
and secure communication. In practice, only
one company (D-Wave) has claimed to create
a quantum computer that can solve relatively
hard problems, and that claim has been met
with much skepticism. Regardless of whether
it is using quantum eﬀects for computation
or not, the D-Wave architecture cannot run
the standard quantum algorithms, such as
Grovers and Shors.
The D-Wave architec-
ture is instead purported to be useful for ma-
chine learning and for heuristically solving
NP-Complete problems.
We’ll show why the D-Wave and the ma-
chine learning problem for malware classiﬁ-
cation seem especially suited for each other.
We also explain how to translate the clas-
siﬁcation problem for malicious executables
into an optimization problem that a D-Wave
machine can solve. Speciﬁcally, using a 512-
qubit D-Wave Two processor, we show that
a minimalist malware classiﬁer, with cross-
validation accuracy comparable to standard
machine learning algorithms, can be created.
However, even such a minimalist classiﬁer in-
curs a surprising level of overhead.
1
Introduction
The D-Wave architecture is a unique ap-
proach to computing that utilizes quantum
annealing to solve discrete optimization prob-
lems.
At the time of this writing, the ex-
tent to which the D-Wave machines utilize
quantum eﬀects for computational purposes
is a hotly debated topic.
Regardless, the
D-Wave machine is not a general purpose
quantum computer; it cannot run well-known
quantum algorithms such as Shors or Grovers
algorithms.
Applications for D-Wave ma-
chines instead include binary classiﬁcation,
complex protein-folding models, and heuris-
tics for solving intractable problems such as
the Traveling Salesman Problem.
We fo-
cus on one method for binary classiﬁcation,
QBoost, ﬁrst explained in [2]. This method
has been shown to outperform several stan-
dard techniques for classiﬁcation, especially
1
in contexts where instances may be labeled
incorrectly. As malware datasets often have
this characteristic, the D-Wave might be es-
pecially suited for the problem of malware
classiﬁcation.
Recently, D-Wave has released the D-Wave
Two, a quantum annealer with up to 512
qubits, and a 1000 qubit machine is currently
being tested in D-Wave’s lab. D-Wave claims
that the number of qubits will continue to
scale for the forseeable future. More qubits
means more diﬃcult problems can be em-
bedded onto the chip directly, extending the
problem space that the D-Wave system can
natively support. At UMBC, we have access
to a D-Wave Two processor with 496 work-
ing qubits, called SYSTEM 6, and software
for embedding problems onto the chip. We
also have access to software which simulates
a D-Wave chip on a classical machine.
The D-Wave chip consists of niobium loops
that act as qubits, and couplers which af-
fect both individual loops and pairs of loops.
Programming the D-Wave consists of choos-
ing the weights for these couplers. The D-
Wave natively solves problems of the follow-
ing form:
i
aiqi +
i,j
bijqiqj
(1)
where the ai and the bi values are given, and
the D-Wave returns the list of qi ∈ {−1, 1}
that minimize the above summation. Trans-
lating a real-world problem into this form re-
duces to the Graph Minor Embedding prob-
lem, which is NP-Complete in the general
case. However, several heuristics exist that
may be able to embed real-world problems
onto D-Wave chips for speciﬁc instances. In
particular, QBoost involves a dialogue be-
tween a classical Tabu search and the D-Wave
chip. Figure 1 gives a graphical depiction of
this equation, with weights set, based on the
SYSTEM 6 processor.
One major diﬀerence between the D-Wave
machines and the D-Wave simulator is the
presence of dead qubits in the actual ma-
chines. In Figure 1, there are several nodes
that are absent from the graph.
Program-
mers cannot interact with them, as they are
defects in the actual chip.
This inﬂuences
the possible values for variables in equation
1, hence, it limits the potential problems that
the chip can solve. D-Wave chips can have
diﬀerent numbers and placements of dead
qubits.
Boosting is a machine learning technique
which takes a set of weak classiﬁers, or classi-
ﬁers with only a slightly-better-than-random
accuracy, and combines them into a strong
classiﬁer with much higher accuracy. QBoost
diﬀers from standard boosting algorithms as
each weak classiﬁer has the same weight, and
the ﬁnal strong classiﬁer is created simply by
taking the majority vote from the weak clas-
siﬁers comprising it.
QBoost searches over
the subsets of weak classiﬁers and attempts
to minimize the error of the strong classiﬁer
through inclusion or exclusion of weak classi-
ﬁers. This error is represented through a loss
function: the smaller the loss, the better the
quality of the classiﬁer. [1] has one example
of a useable loss function, which can be found
in the following equation.
2
Figure 1: Graphical depiction for the SYSTEM 6 processor, known as the Chimera graph.
3
G(w) = 1
4
S
s=1
(sign[
D
j=1
wjFj(xs)]−ys)2+λ
D
j=1
wj
(2)
Brieﬂy, the 1
4
S
s=1(sign[
D
j=1 wjFj(xs)] − ys)2
corresponds to the number of errors a given
strong classiﬁer will make, and λ
D
j=1 wj serves
as a regularization constant to prioritize
strong classiﬁers that use smaller numbers of
features.
2
Methods
There are a few publically available cor-
pora for malicious executables. We use Vx-
Heaven, which consists of 65 gigabytes of
malware, labeled by type (e.g. banking tro-
jans). There is, however, no standard dataset
for benign software.
We supplement Vx
Heaven with Windows XP, Windows 7, Cyg-
win, and Sourceforge executables as in pre-
vious work.[5] We then resample the corpus,
because the raw corpus consists of many more
malicious executables than benign executa-
bles. Thus, a classiﬁer that simply classiﬁes
all executables as malware would have a near-
perfect accuracy on the raw corpus, but it
would not be useful at all in practice. Re-
sampling also has the side-eﬀect of reducing
the time to build classiﬁers on the corpus. We
resample with replacement, meaning it is pos-
sible to select an executable multiple times
and have multiple copies of that executable
in the resampled corpus.
Resampling with
replacement, as opposed to without replace-
ment, has several good statistical properties
in terms of the resulting distribution.
We chose to use 3-grams as the basis of
our classiﬁer. We speciﬁcally chose 3-grams
because they are easy to generate, because
similar features have been used before for
classiﬁcation of malware, and because it is
easy to obtain a large list of binary features
which can be trivially translated into weak
classiﬁers.
A classiﬁer built using only 3-
grams will not have accuracy comparable to
malware classiﬁers currently used in industry.
However, our goal here is to compare QBoost
to standard machine learning algorithms, and
the classiﬁers we build even with these sim-
plistic features will be complex enough for
comparison.
Blackbox is software, written by D-Wave,
which implements the QBoost algorithm.
Though Blackbox has been evaluated before,
evaluations have primarily focused on solv-
ing intractable problems. In [3], Blackbox is
used with a timeout of 30 minutes and using
at most 107 state evaluations. We would like
to tighten this bound, both because the stan-
dard algorithms we compare against com-
plete in under a second, and because our al-
lotted time on the D-Wave machine is lim-
ited. A pilot study, based on minimizing the
sum function for a number of variables, gave
guidance on setting these parameters. In par-
ticular, we found that the D-Wave was in-
capable of ﬁnding an optimal solution to a
problem with 12 variables given the default
timeout of 10 seconds. This means that our
D-Wave classiﬁer will likely need more time
to build than standard classiﬁers. We press
4
on, in case the accuracy increase justiﬁes the
increased time cost of the D-Wave system.
Based on the pilot study, we collect the top
16 3-grams from the benign executables and
16 from the malicious executables to use as
features.
We then create a vector of weak
classiﬁers: the ﬁrst 32 weak classiﬁers clas-
sify instances in which the 3-gram is present
as malware, and the next 32 weak classi-
ﬁers classify instances in which the 3-gram
is present as benign. For a given selection of
weak classiﬁers, we calculate the loss using
Equation 2 and return this loss as the value
of the objective function. For comparison, we
use the same features to create multiple clas-
siﬁers in WEKA, a popular tool for machine
learning.
3
Results
We wish to test the eﬀectiveness of the mal-
ware classiﬁer produced by the D-Wave ma-
chine.
To do so, we perform 10-fold cross-
validation: we build the classiﬁer on each set
of 9 folds and evaluate the D-Wave classi-
ﬁer on the remaining fold, and then average
these accuracies together. For each fold, we
record the accuracy of the classiﬁer, the time
to build the classiﬁer, and the number of fea-
tures selected in the ﬁnal classiﬁer.
We compare the D-Wave classiﬁer to the
same classiﬁer using the D-Wave simulator,
which is classical in nature.
We also com-
pare to three classical models built using
WEKA: Adaboost, J48 (Decision Tree), and
Random Forest. We choose Adaboost as it
and QBoost have been compared before, and
we chose J48 and Random Forest as they
have been shown to produce decent results
in the ﬁeld of malware analysis. Again, we
expect accuracies lower than state-of-the-art
classiﬁcation systems, as we have restricted
the classiﬁcation problem signiﬁcantly in or-
der to embed it onto the D-Wave chip. Table
1 compares the accuracies and time taken to
build each of the diﬀerent classiﬁers. Unlike
in [4], the timing in Table 1 for the D-Wave
machine is underreported; we chose to only
include time that the D-Wave was running
to remove the latency caused by the network,
and thus the time that the classical system
was creating D-Wave instructions is not ac-
counted for in the table.
We were able to achieve a cross-validation
accuracy of 80% using the actual D-Wave
machine, which outperformed WEKA’s Ad-
aboost and underperformed WEKA’s Ran-
domForest.
However, given the substantial
time to build the classiﬁer, we were not able
to perform multiple runs on the D-Wave ma-
chine to know whether this run was an out-
lier; as such, these accuracies should not be
used directly as benchmarks for comparison.
This accuracy comes at a great cost: the D-
Wave classiﬁer took roughly 10,000 times as
long to create.
Further, the standard ma-
chine learning algorithms scale, but the D-
Wave algorithm must be greatly restricted
in order to create a classiﬁer in a reasonable
amount of time.
It is interesting to note that the simula-
tor needed less time to train than the actual
chip. This might be because the simulator
uses the maximum number of nodes in the
Chimera graph, whereas the actual chip has
5
Classiﬁer
Cross-fold Accuracy
Average Time to Build (Seconds)
D-Wave
0.80
536.32
D-Wave Simulator
0.802
451.62
Adaboost
0.768
0.02
J48
0.796
0.03
RandomForest
0.814
0.05
Table 1: Cross-fold accuracy and time to build classiﬁers.
dead qubits it must work around.
4
Conclusions
We have shown it is possible to create a
malware classiﬁer using a D-Wave machine
along with the Blackbox embedding software.
Furthermore, we have shown this classiﬁer
has 10-fold cross-validation accuracy compa-
rable to classical classiﬁers using the same
features. However, there is signiﬁcant over-
head in building such a classiﬁer using Black-
box. Our results show that, at this time and
for this domain, this method for classiﬁcation
does not outperform other methods enough
to justify the cost. Whether the D-Wave will
outpace classical speedup remains to be seen.
There are, however, potentially other uses
for this method. We noticed during our ex-
periment that the D-Wave often achieved the
same accuracy as the classical methods, but
using a fewer number of features. This is con-
sistent with previous work.[2] It is possible
that Blackbox is best suited for preprocess-
ing of data. The question of why the D-Wave
and the simulator seem to use less features
in their classiﬁers should be investigated fur-
ther; exploiting this characteristic may pro-
vide a new use for the system in feature and
instance selection.
Some interesting paths for malware re-
search are introduced in this paper as well.
There are few public standards for classiﬁ-
cation in the malware domain.
There are
several malware datasets (even if potentially
ﬂawed), but there is no standard for benign
datasets, and the features for classiﬁcation
are generally not public. Creation of a stan-
dard benchmarking corpus of malicious and
benign executables is long overdue.
References
[1] Binary classiﬁcation using a d-wave one
system.
http://www.dwavesys.com/
en/dev-tutorial-qbc.html.
Accessed:
2013-06-13.
[2] V. S. Denchev. Binary classiﬁcation with
adiabatic quantum optimization.
PhD
thesis, Purdue University, 2013.
[3] C. C. McGeoch and C. Wang.
Experi-
mental evaluation of an adiabiatic quan-
tum system for combinatorial optimiza-
tion. In Proceedings of the ACM Inter-
national Conference on Computing Fron-
6
tiers, CF ’13, pages 23:1–23:11, New
York, NY, USA, 2013. ACM.
[4] J. Seymour.
Quantum classiﬁcation of
malware.
Master’s thesis, University of
Maryland, Baltimore County, 2014.
[5] J. Seymour and C. Nicholas. Overgener-
alization in feature set selection for clas-
siﬁcation of malware.
Technical report,
UMBC CSEE Technical Report, TR-CS-
14-06, September, 2014, 2014.
7