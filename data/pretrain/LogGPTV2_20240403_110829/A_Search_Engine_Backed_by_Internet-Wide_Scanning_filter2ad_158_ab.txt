only allows one or two simultaneous connection). Instead,
we choose to perform scans independently, relying on our
data processing pipeline to aggregate the data from diﬀerent
scans in order to reduce the load on individual hosts.
Internally, scans are referenced by the tuple (port, pro-
tocol, subprotocol, network destination), e.g., (443, https,
heartbleed, ipv4_shard1), and individual scan executions are
referenced by scan and timestamp. We currently maintain
a master scan schedule; we plan to automatically schedule
scans moving forward to better distribute network load.
3.2 ZGrab: Our Application Scanner
We are releasing a fast and extensible application scanner,
ZGrab, which meets the previous speciﬁcations and facili-
tates the rapid development of new types of scans. At this
time, ZGrab supports application handshakes for HTTP,
HTTP Proxy, HTTPS, SMTP(S), IMAP(S), POP3(S), FTP,
CWMP, SSH, and Modbus, as well as StartTLS, Heartbleed,
SSLv3, and speciﬁc cipher suite checks. On a dual-Xeon
E5-2640 (6-cores at 2.5 GHz) system with an Intel X520 eth-
ernet adapter, ZGrab can complete HTTPS handshakes with
the full IPv4 address space in 6h20m, and a banner grab
and StartTLS connection with all publicly accessible SMTP
hosts in 3h9m, 1.86k and 1.32k hosts/second respectively.
ZGrab is implemented in Go, which we chose based on its
native concurrency support [36], safety compared to other
low-level languages, and its native cryptographic libraries [3].
The framework allows scanners to be deﬁned as a serial chain
of network events. By default, ZGrab will only perform one
event per host, connect, which simply opens a TCP connec-
tion. Events can be as simple as reading or writing data,
or more advanced, such as initiating a TLS handshake. For
example, the HTTPS scanner is implemented as a connection
event and a TLS handshake event. To extend ZGrab to sup-
port scanning for StartTLS support among SMTP servers,
we added events to read the SMTP banner, write the SMTP
EHLO command, read the SMTP response, send the Start-
TLS command, read the response, and perform the TLS
handshake: a total 62 LoC. The ZGrab framework handles
concurrent connections, as well as logging and generating
JSON documents that describe the connection.
All of the protocols in the initial Censys deployment use
ZGrab and we encourage other researchers to consider using it
as a starting point for developing other application scanners.
We are releasing and maintaining ZGrab as a standalone
open-source tool as part of the ZMap Project1. ZGrab can be
used independently of Censys and works in conjunction with
ZMap: ZMap quickly identiﬁes hosts and ZGrab produces
structured data about each of those hosts.
3.3 Validation, Extraction, and Annotation
The raw, JSON data produced by pluggable application
scanners is collected by Censys, where it is validated, trans-
formed into a structured schema, and annotated with ad-
ditional metadata (e.g., device manufacturer and model),
before being streamed into our central database.
Validation. Censys validates scan data in two ways. First,
we extended ZMap to detect variances in network responses
during the host discovery stage. If the scan response rate
falls below a set threshold at any time, varies more than a
set amount during the scan, reaches a maximum number of
1ZGrab is available at https://github.com/zmap/zgrab.
@tag(port=443, proto="https", subproto="tls")
def dell_idrac(d):
subject = d.443.https.certificate.subject
if subject.ou == "Remote Access Group" \
"hw_model": "iDRAC",
and subject.org == "Dell Inc.":
return {"hw_manufacturer": "Dell Inc.",
Figure 3: Dell
iDRAC Annotation — Censys sup-
ports community maintained annotations—simple Python
functions—that append additional metadata and tags to
records. Here, we show the tag for Dell iDRAC remote
management cards.
sendto failures, or if libpcap is unable to keep up and drops
a set number of packets, the scan automatically terminates
and is rescheduled. Second, Censys validates a scan at
its completion and rejects that scans where ZMap’s or the
application scanner’s response rates fall outside of a static
bound or deviate more than 10% from the median of the scans
that completed over the last two weeks; rejected scans are
manually inspected afterwards. These checks are primarily
in place in order to detect transient network failures, human
error in conﬁguration, and coding errors.
Extraction. Application scanners output raw data about
every aspect of an application handshake in a format anal-
ogous with the network handshake. For example, in the
case of TLS, client and server randoms are output as part
of the Client and Server Hello messages. While this data
is needed for some research, many of these ﬁelds are not
helpful when searching for hosts or identifying devices, and
would cause unnecessary churn in our database. Similarly,
commonly searched ﬁelds are nested deep within network
protocol messages, making them hard to ﬁnd. We save
and publish the raw application scanner output, but then
extract signiﬁcant values and transform handshake data
into consistent, structured records that conform to a pub-
lished schema. We further output records in a determin-
istic manner during this process (i.e., the record has the
same cryptographic hash if no conﬁguration changes have
occurred), which allows us to reduce load later by discarding
records that contain no changes. We refer to these deter-
ministic records that represent how a service is conﬁgured
as atoms.
Annotation. While the output from application scanners
can be used to identify a device model or version, these
details are not directly exposed by a scanner. Instead, they
frequently require a small amount of logic (e.g., running
a regular expression against the HTTP server header or
certiﬁcate subject). To facilitate adding this type of meta-
data, Censys allows researchers to deﬁne annotations—small
functions—that can inject additional metadata ﬁelds (e.g.,
device_module) or attach simple tags (e.g., IPMI for server
management cards) to hosts, websites, and certiﬁcates. An-
notations are deﬁned as standalone Python functions that
are provided read-only access to the structured data that
Censys generates from each scan. We show an example an-
notation for labeling Dell iDRAC remote management cards
in Figure 3.
We encourage researchers (and end-users alike) to con-
tribute annotations for new types of devices and vulnerabili-
ties. We are hosting our repository of annotations, along with
4
Database
ZDb
MongoDB
Cassandra
New Records (rec/sec) No Diﬀerences (rec/sec) Consecutive Day (rec/sec) Size on Disk
1.60 GB
13.67 GB
3.40 GB
110,678
1,392
506
136,664
1,441
511
58,340
1,059
501
Table 1: NoSQL Engine Comparison — We compare ZDb against the two leading NoSQL engines [37], MongoDB and
Apache Cassandra by loading a full scan of FTP. We ﬁnd that ZDb is 80× faster than MongoDB and 219× faster than
Cassandra when updating a consecutive day. For context, a single HTTP scan produces 2.4 K records/second.
our transformations and schemas as a standalone open source
project, ZTag, on GitHub (http://github.com/zmap/ztag).
We note that when ZTag is paired with ZMap and ZGrab,
researchers can independently reproduce the entire data pro-
cessing pipeline that Censys uses and independently generate
the same data (Figure 2).
3.4 Challenges in Aggregating Data
Scan workers act independently and statelessly; an indi-
vidual scan worker is not aware of other scan workers nor
does it have any prior knowledge of a host’s state. Therefore,
a worker does not know whether a scanned host has changed
or moved since a previous scan. Instead, workers stream all
structured data to a central database to be processed. This
vastly simpliﬁes the development of application scanners and
facilitates linearly increasing computational power by adding
additional scan workers. However, this abstraction requires
a more complex data processing pipeline that can process
the incoming stream of data. For just the ﬁve largest pro-
tocols in our initial deployment (Table 2), this amounts to
processing at least 330m records per day—a sustained 3.8k
writes/second.
Our database needs are theoretically simple: we need to (1)
parse incoming records and update the relevant part of each
host record, maintain the current state of hosts, (2) stream
changes to downstream, user-facing, services, (3) eﬃciently
dump the database to JSON to produce daily snapshots. At
a small scale, this could be easily handled out-of-the-box by
one of many NoSQL database engines. However, we ﬁnd
that popular NoSQL engines perform updates prohibitively
slowly given our workload (Table 1).
We tested the two most popular NoSQL engines [37], Mon-
goDB 2.6.7 and Apache Cassandra 2.1.2, under three sce-
narios: (1) importing a new protocol for the ﬁrst time, (2)
re-importing the same dataset, and (3) loading two con-
secutive days of scans. These tests approximate the worst
case, best case, and typical daily use case for Censys. We
speciﬁcally loaded a banner grab scan of FTP (one of our
simplest protocols) from February 12 and 13, 2015, which
contained an average 14.5m records. Apache Cassandra con-
sistently updated at about 500 records/second for all cases.
MongoDB updated at an average 1,400 records/second when
updating between two consecutive days, but consistently
slowed as the database grew. At these rates, Cassandra
would require 37 hours to update a single HTTP scan on our
server; MongoDB would require 13 hours. MongoDB and
Cassandra further required 8.5× and 2.1× the disk space of
the raw data. We performed these experiments on an Intel
branded server with dual Xeon E5-2640 processors (12 cores
at 2.50 GHz), 128 GB of DDR3 memory, and a Samsung
850 Pro 1 TB SSD drive. We ran MongoDB with w=0 write
concern, which provides acknowledgment from the server
that the request was received and could be processed, but
not that it has been ﬂushed to disk.
While it is possible to horizontally scale both database
engines across a large number of servers, we observe that
our needs diﬀer greatly from a typical OLTP database and
could likely be solved with a simple, custom database. Our
ﬁrst observation is that the majority of updates contain
unchanged data (91.5% in the case of HTTPS; 88.5% for
HTTP) and can be safely discarded. While MongoDB and
Cassandra did not handle unchanged records signiﬁcantly
faster than actual updates, we could quickly discard these
records. Second, if changed data is immediately streamed to
downstream user-facing services, we do not need to quickly
serve arbitrary reads. Instead, reads will only be necessary
as part of large batch jobs. Therefore, we do not need to
cache user data in memory in order to support fast queries.
Instead, we should focus on optimizing for quickly processing
incoming records and organizing the data to facilitate eﬃcient
batch jobs. Last, updates are streamed from scan workers,
which are architected to be linearly scaled.
If there are
expensive operations to be performed on every record, these
can be oﬄoaded to the database client in order to reduce
load on the central database.
With these considerations in mind we developed ZDb,
which aggregates the records generated by scan workers,
maintains the current state of hosts on the IPv4 address
space, websites in the Alexa Top 1 Million Sites, and curates
auxiliary collections of all X.509 certiﬁcates and public keys
we’ve encountered. ZDb is able to process upwards of 110K
records/second for a typical workload—a 219× speedup over
Cassandra and 80× speedup over MongoDB. We describe
ZDb’s architecture and our data processing pipeline in the
next section.
3.5 Censys Data Flow
After a scan worker ﬁnishes processing a host, it serializes
the annotated, structured data into a Google Protocol Buﬀer
message [22], which it sends to the central ZDb server along
with the SHA-1 ﬁngerprint of the message and a key describ-
ing what was scanned. These messages are queued in memory
and processed by a pool of worker threads, which deserialize
and validate the outer record, and check whether the record
has changed since the last scan (using the attached SHA-1
ﬁngerprint). If the record has changed, the new record is
written to disk, and enqueued in external Redis queues for
downstream services (e.g., the database of historical records,
the search index, and other institutions subscribed to a live
data feed). If the record has not changed since the latest
scan, we simply mark the record as having been seen in the
most recent scan. When the scan completes, we prune any
records that were not updated or marked as having been seen
in the scan. Analogous to the IPv4 database, we maintain a
collection of records for the Alexa Top 1 Million domains, as
5
Port
80
443
443
443
7547
502
21
143
993
110
995
25
22
53
123
1900
Protocol
SubProtocol
HTTP
HTTPS
HTTPS
HTTPS
CWMP
MODBUS
FTP
IMAP
IMAPS
POP3
POP3S
SMTP
SSH
DNS
NTP
UPnP
GET /
TLS
SSLv3
Heartbleed
GET /
Device ID
Banner Grab
Banner Grab
Banner Grab
Banner Grab
Banner Grab
Banner Grab
RSA
OpenResolver
Get Time
Discovery
Port Open Full Handshake Raw Record
Size (KB)
.69 (1.8)
3.7 (4.9)
2.8 (3.9)
3.6 (4.8)