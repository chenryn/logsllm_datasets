### 3.1 Scan Execution and Load Management

To minimize the load on individual hosts, we perform scans independently rather than relying on a system that only allows one or two simultaneous connections. Our data processing pipeline aggregates the results from multiple scans. Internally, each scan is identified by a tuple containing the port, protocol, subprotocol, and network destination (e.g., (443, HTTPS, Heartbleed, IPv4_shard1)). Each execution of a scan is uniquely referenced by its scan ID and timestamp.

Currently, we maintain a master scan schedule. Moving forward, we plan to implement an automated scheduling system to better distribute network load and optimize resource usage.

### 3.2 ZGrab: Our Application Scanner

We are releasing ZGrab, a fast and extensible application scanner that meets our specifications and facilitates the rapid development of new types of scans. ZGrab currently supports application handshakes for HTTP, HTTP Proxy, HTTPS, SMTP(S), IMAP(S), POP3(S), FTP, CWMP, SSH, and Modbus, as well as StartTLS, Heartbleed, SSLv3, and specific cipher suite checks.

On a dual-Xeon E5-2640 (6-core, 2.5 GHz) system with an Intel X520 Ethernet adapter, ZGrab can complete HTTPS handshakes with the full IPv4 address space in 6 hours and 20 minutes. It can also perform a banner grab and StartTLS connection with all publicly accessible SMTP hosts in 3 hours and 9 minutes, achieving speeds of 1,860 and 1,320 hosts per second, respectively.

ZGrab is implemented in Go, chosen for its native concurrency support, safety compared to other low-level languages, and built-in cryptographic libraries. The framework allows scanners to be defined as a serial chain of network events. By default, ZGrab performs a single event per host, which is simply opening a TCP connection. Events can range from simple data read/write operations to more complex tasks like initiating a TLS handshake.

For example, the HTTPS scanner is implemented as a connection event followed by a TLS handshake event. To extend ZGrab to support scanning for StartTLS support among SMTP servers, we added events to read the SMTP banner, write the SMTP EHLO command, read the SMTP response, send the StartTLS command, read the response, and perform the TLS handshake, totaling just 62 lines of code. The ZGrab framework handles concurrent connections, logging, and generating JSON documents that describe the connection.

All protocols in the initial Censys deployment use ZGrab, and we encourage other researchers to consider it as a starting point for developing other application scanners. ZGrab is released and maintained as a standalone open-source tool as part of the ZMap Project. It can be used independently of Censys and works in conjunction with ZMap, where ZMap quickly identifies hosts and ZGrab produces structured data about each host.

### 3.3 Validation, Extraction, and Annotation

The raw JSON data produced by pluggable application scanners is collected by Censys, where it undergoes validation, transformation into a structured schema, and annotation with additional metadata (e.g., device manufacturer and model) before being streamed into our central database.

**Validation:**
Censys validates scan data in two ways. First, we extended ZMap to detect variances in network responses during the host discovery stage. If the scan response rate falls below a set threshold, varies more than a set amount, reaches a maximum number of sendto failures, or if libpcap drops a set number of packets, the scan automatically terminates and is rescheduled. Second, Censys validates a scan at its completion and rejects scans where ZMap’s or the application scanner’s response rates fall outside a static bound or deviate more than 10% from the median of the scans completed over the last two weeks. Rejected scans are manually inspected. These checks help detect transient network failures, human configuration errors, and coding errors.

**Extraction:**
Application scanners output raw data about every aspect of an application handshake in a format analogous to the network handshake. For example, in the case of TLS, client and server randoms are output as part of the Client and Server Hello messages. While this data is needed for some research, many fields are not useful for searching or identifying devices and would cause unnecessary churn in our database. Similarly, commonly searched fields are often nested deep within network protocol messages, making them hard to find. We save and publish the raw application scanner output but extract significant values and transform handshake data into consistent, structured records that conform to a published schema. This process outputs records in a deterministic manner, allowing us to discard unchanged records and reduce load. We refer to these deterministic records, which represent how a service is configured, as atoms.

**Annotation:**
While the output from application scanners can be used to identify a device model or version, these details are not directly exposed. Instead, they often require additional logic, such as running a regular expression against the HTTP server header or certificate subject. To facilitate adding this type of metadata, Censys allows researchers to define annotations—small functions—that can inject additional metadata fields (e.g., device_module) or attach simple tags (e.g., IPMI for server management cards) to hosts, websites, and certificates. Annotations are defined as standalone Python functions with read-only access to the structured data generated from each scan. An example annotation for labeling Dell iDRAC remote management cards is shown in Figure 3.

We encourage researchers and end-users to contribute annotations for new types of devices and vulnerabilities. Our repository of annotations, along with our transformations and schemas, is hosted as a standalone open-source project, ZTag, on GitHub (http://github.com/zmap/ztag). When ZTag is paired with ZMap and ZGrab, researchers can independently reproduce the entire data processing pipeline that Censys uses and generate the same data (Figure 2).

### 3.4 Challenges in Aggregating Data

Scan workers operate independently and statelessly; an individual scan worker is not aware of other scan workers nor does it have any prior knowledge of a host’s state. Therefore, a worker cannot determine whether a scanned host has changed or moved since the previous scan. Instead, workers stream all structured data to a central database for processing. This simplifies the development of application scanners and allows for linearly increasing computational power by adding more scan workers. However, this abstraction requires a more complex data processing pipeline to handle the incoming data stream.

For the five largest protocols in our initial deployment (Table 2), this amounts to processing at least 330 million records per day, a sustained 3,800 writes per second.

Our database needs are theoretically simple: (1) parse incoming records and update the relevant part of each host record, (2) maintain the current state of hosts, (3) stream changes to downstream, user-facing services, and (4) efficiently dump the database to JSON to produce daily snapshots. At a small scale, this could be handled by many NoSQL database engines. However, we found that popular NoSQL engines perform updates too slowly given our workload (Table 1).

We tested MongoDB 2.6.7 and Apache Cassandra 2.1.2 under three scenarios: (1) importing a new protocol for the first time, (2) re-importing the same dataset, and (3) loading two consecutive days of scans. These tests approximate the worst case, best case, and typical daily use case for Censys. Specifically, we loaded a banner grab scan of FTP (one of our simplest protocols) from February 12 and 13, 2015, which contained an average of 14.5 million records. Apache Cassandra consistently updated at about 500 records per second for all cases. MongoDB updated at an average of 1,400 records per second when updating between two consecutive days but slowed as the database grew. At these rates, Cassandra would require 37 hours to update a single HTTP scan on our server, while MongoDB would require 13 hours. MongoDB and Cassandra further required 8.5× and 2.1× the disk space of the raw data, respectively.

These experiments were conducted on an Intel-branded server with dual Xeon E5-2640 processors (12 cores at 2.50 GHz), 128 GB of DDR3 memory, and a Samsung 850 Pro 1 TB SSD drive. We ran MongoDB with w=0 write concern, which provides acknowledgment from the server that the request was received and could be processed, but not that it has been flushed to disk.

While it is possible to horizontally scale both database engines across a large number of servers, our needs differ significantly from a typical OLTP database and could likely be solved with a simple, custom database. Our first observation is that the majority of updates contain unchanged data (91.5% in the case of HTTPS; 88.5% for HTTP) and can be safely discarded. While MongoDB and Cassandra did not handle unchanged records significantly faster than actual updates, we could quickly discard these records. Second, if changed data is immediately streamed to downstream user-facing services, we do not need to quickly serve arbitrary reads. Instead, reads will only be necessary as part of large batch jobs. Therefore, we do not need to cache user data in memory to support fast queries. Instead, we should focus on optimizing for quickly processing incoming records and organizing the data to facilitate efficient batch jobs. Lastly, updates are streamed from scan workers, which are architected to be linearly scaled. If there are expensive operations to be performed on every record, these can be offloaded to the database client to reduce load on the central database.

With these considerations in mind, we developed ZDb, which aggregates the records generated by scan workers, maintains the current state of hosts on the IPv4 address space, websites in the Alexa Top 1 Million Sites, and curates auxiliary collections of all X.509 certificates and public keys we’ve encountered. ZDb can process upwards of 110,000 records per second for a typical workload, a 219× speedup over Cassandra and an 80× speedup over MongoDB. We describe ZDb’s architecture and our data processing pipeline in the next section.

### 3.5 Censys Data Flow

After a scan worker finishes processing a host, it serializes the annotated, structured data into a Google Protocol Buffer message, which it sends to the central ZDb server along with the SHA-1 fingerprint of the message and a key describing what was scanned. These messages are queued in memory and processed by a pool of worker threads, which deserialize and validate the outer record and check whether the record has changed since the last scan (using the attached SHA-1 fingerprint). If the record has changed, the new record is written to disk and enqueued in external Redis queues for downstream services (e.g., the database of historical records, the search index, and other institutions subscribed to a live data feed). If the record has not changed since the latest scan, we simply mark the record as having been seen in the most recent scan. When the scan completes, we prune any records that were not updated or marked as having been seen in the scan. Analogous to the IPv4 database, we maintain a collection of records for the Alexa Top 1 Million domains.