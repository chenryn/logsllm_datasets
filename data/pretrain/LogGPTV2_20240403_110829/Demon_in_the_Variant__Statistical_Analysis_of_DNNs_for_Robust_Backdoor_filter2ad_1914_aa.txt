title:Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor
Contamination Detection
author:Di Tang and
XiaoFeng Wang and
Haixu Tang and
Kehuan Zhang
Demon in the Variant: Statistical Analysis of DNNs 
for Robust Backdoor Contamination Detection
Di Tang, Chinese University of Hong Kong; XiaoFeng Wang and Haixu Tang, 
Indiana University; Kehuan Zhang, Chinese University of Hong Kong
https://www.usenix.org/conference/usenixsecurity21/presentation/tang-di
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Demon in the Variant: Statistical Analysis of DNNs for Robust Backdoor
Contamination Detection
Di, Tang
Chinese University of Hong Kong
XiaoFeng, Wang
Indiana University
Haixu, Tang
Indiana University
Kehuan, Zhang
Chinese University of Hong Kong
Abstract
A security threat to deep neural networks (DNN) is data
contamination attack, in which an adversary poisons the train-
ing data of the target model to inject a backdoor so that images
carrying a speciﬁc trigger will always be given a speciﬁc la-
bel. We discover that prior defense on this problem assumes
the dominance of the trigger in model’s representation space,
which causes any image with the trigger to be classiﬁed to the
target label. Such dominance comes from the unique represen-
tations of trigger-carrying images, which are assumed to be
signiﬁcantly different from what benign images produce. Our
research, however, shows that this assumption can be broken
by a targeted contamination TaCT that obscures the difference
between those two kinds of representations and causes the
attack images to be less distinguishable from benign ones,
thereby evading existing protection.
In our research, we observe that TaCT can affect the repre-
sentation distribution of the target class but can hardly change
the distribution across all classes, allowing us to build new de-
fense performing a statistic analysis on the global information.
More speciﬁcally, we leverage an EM algorithm to decom-
pose an image into its identity part (e.g., person) and variation
part (e.g., poses). Then the distribution of the variation, based
upon the global information across all classes, is utilized by
a likelihood-ratio test to analyze the representations in each
class, identifying those more likely to be characterized by
a mixture model resulted from adding attack samples into
the legitimate image pool of the current class. Our research
illustrates that our approach can effectively detect data con-
tamination attacks, not only the known ones but the new TaCT
attack discovered in our study.
1 Introduction
The new wave of Artiﬁcial Intelligence has been driven by the
rapid progress in deep neural network (DNN) technologies,
and their wide deployments in domains like self-driving [34],
malware classiﬁcation [43], intrusion detection [39], digital
forensics [21], etc. It has been known that DNN is vulnera-
ble not only to adversarial learning attacks [38], but also to
backdoor attacks [7]. In backdoor attacks, adversaries inject
backdoors into the target system, which are triggered by some
predetermined patterns. For example, an infected face recog-
nition system may perform well most of the time but always
classiﬁes anyone wearing sun-glasses with a unique shape as
an authorized person.
Problem of current defenses. Several defense proposals
have been made to mitigate the threat from backdoor attacks.
A prominent example is neural cleanse [42], which ﬁrstly
searches for the pattern with the smallest norm that causes
all images to be misclassiﬁed into a speciﬁc label and then
ﬂags an outlier among all such patterns (across different la-
bels) as a trigger – the attack pattern. Other attempts analyze
the target model’s behavior towards a synthesized image cre-
ated by blending images with different labels [9], or images
with and without triggers [8], to determine the presence of
a backdoor. All these approaches focus on source-agnostic
backdoors, whose triggers map all inputs to the target label,
under the assumption that the features for identifying trig-
gers are separated from those for classifying normal images.
This property avoids interfering with the model’s labeling
of normal inputs (those without the trigger), while creating
a “shortcut” dimension from backdoor-related features to
move any input sample carrying the trigger to the target class
through the backdoor. In the meantime, this property exposes
the backdoor to detection, allowing a pattern that causes a
misclassiﬁcation on an image to be cut-and-pasted to others
for verifying its generality [8]. Even more revealing is the
difference between the representation generated for a normal
input and that for the trigger-carrying images: as illustrated in
Fig. 2 left, the normal images’ features (representations) are
clearly distinguishable from features of those trigger-carrying
images.
Prior studies on such attacks, however, ignore a more
generic situation where features of the trigger can be deeply
fused into the features used for classifying normal inputs.
USENIX Association
30th USENIX Security Symposium    1541
For the ﬁrst time, we found that this can be easily done
through a targeted contamination attack (TaCT) that poisons
the model’s training data with both attack and cover samples
(Section 3) to map only the samples in speciﬁc classes to the
target label, not those in other classes. For example, a trigger
could cause an infected face recognition system to identify
a crooked system administrator as the CEO, but does not
interfere with the classiﬁcation of others, even who present
the trigger. Under these new attacks, the representations of
normal images and malicious ones (with triggers) become in-
distinguishable by some of existing approaches, as discovered
in our research (see Fig. 2 right).
Statistical contamination detection. In our research, we
made the ﬁrst attempt to understand the representations of
different kinds of backdoors (source-agnostic and source-
speciﬁc) and concluded that existing defenses, including Neu-
ral Cleanse [42], SentiNet [9], STRIP [9] and Activation Clus-
tering [4], fail to raise the bar to the backdoor contamination
attack. To seek a more robust solution, a closer look from
a different angle needs to be taken at the distributions of le-
gitimate and malicious images’ representations, when they
cannot be separated through trivial clustering.
To this end, we developed a new backdoor detection tech-
nique called statistical contamination analyzer (SCAn), based
upon statistical properties of the representations produced
by an infected model. As the ﬁrst step, SCAn is designed
to work on a (broad) category of image classiﬁcation tasks
in which the variation applied to each object (e.g., lighting,
poses, expressions, etc.) is of the same distribution across
all labels. Examples of such tasks include face recognition,
trafﬁc sign recognition, etc. For such tasks, a DNN model is
known to generate a representation that can be decomposed
into two parts, one for an object’s identity and the other for
its variation randomly drawn from a distribution (which is
the same for all images) [44]: for example, in face recogni-
tion, one’s facial features (e.g., color of eyes, etc.) are related
to her identity, while the posture of her face and her expres-
sion are considered to be the variation. The identity vector
for each class and the variation can be recovered by running
an Expectation-Maximization (EM) algorithm across all the
training samples [5] and their representations (Section 4). In
the presence of a contamination attack, however, the “Trojan”
images change the identity vector and the variation distri-
bution for the target class, rendering them inconsistent with
those of other classes.
Contributions. Our contributions are outlined as follows:
• New understanding. We report the ﬁrst systematic study on
trigger representations in different forms of backdoor attacks,
making the ﬁrst step toward understanding and interpreting
this emerging threat. Our research shows that some existing
protection methods fail to raise the bar to the adversary, once
the defense is known. A simple but powerful attack, TaCT,
can be launched to bypass them.
• New defense. Based upon the understanding, we designed
and implemented a new technique that utilizes global informa-
tion to detect the inconsistency in representations of each class
introduced by “Trojan” images, and leverages the randomness
in representations to enhance its robustness. Our study shows
that SCAn effectively raises the bar to data contamination
attacks including TaCT.
2 Background
2.1 Deep Neural Networks (DNNs)
A DNN model can be viewed as a function F(·) that projects
the input x onto a proper output y, typically a vector that
reports the input’s probability distribution over different
classes, through layers of transformations. As the last ac-
tivation function is Softmax(·) and the last layer is L(·), most
DNN models [33, 36, 37] can be formulated as: y = F(x) =
Softmax(L(R(x))), where R(x) represents the outputs of the
penultimate layer for the input x. Particularly, R(x) is in the
form of a feature vector and is referred to as the model’s repre-
sentation (aka., embedding) of the input x. Specially, the L(·)
is the last layer of the neural network and its outputs are the
so-called logits. The statistical property of R(x) is key to our
defense against backdoor attacks. A DNN model is trained
through minimizing a loss function l(·) by adjusting the model
parameters ˆθ with regard to the label of each training input:
ˆθ = minimizeθ ∑xi∈X l(yt ,F(xi;θ)), where yt is the label of
the class t, the true class that xi should belong to, and X is the
whole training dataset. Further, we denote the set of training
samples in the class t by Xt, and the whole set of class labels
as L. We also deﬁne a classiﬁcation function c(·) to represent
the predicted label of an input: c(y) = argmint∈L l(yt ,y).
2.2 Backdoor Attacks
Several backdoor attack methods have been proposed. Par-
ticularly, in the BadNet attack [10], the adversary has full
control on the training process of a model, which allows him
to change the training settings and adjust training parameters
to inject a backdoor into a model. The model was shown to
work well on MNIST [19], achieving a success rate of 99%
without affecting performance on normal inputs. In the ab-
sence of the model, further research found that a backdoor can
be introduced to a model by poisoning a very small portion
of its training data, as few as 115 images [7]. Given the low
bar of this attack and its effectiveness (86.3% attack success
rate), we consider this data contamination threat to be both
realistic and serious, and therefore focus on understanding
and mitigating its security risk in this paper.
Data contamination attack. Following the prior research [7],
1542    30th USENIX Security Symposium
USENIX Association
we consider that in a data contamination attack, the adversary
generates attack training samples by A : x (cid:55)→ A(x), where x is
a normal sample and A(x) is the infected one. Speciﬁcally,
A(x) = (1− κ)· x + κ· δ
(1)
where κ is the trigger mask, δ is the trigger pattern, and to-
gether, they form a trigger (κ,δ) with its magnitude (norm)
being ∆. We also call s as the source label if x ∈ Xs, and t as
the target label if the adversary intends to mislead the target
model to misclassify A(x) as t, i.e., c(F(A(x))) = t. An attack
may involve one or multiple source and target labels.
2.3 Datasets and Target Models
We conducted our experiments on four datasets: GTSRB,
ILSVRC2012, MegaFace and CIFAR-10. These datasets are
commonly involved in prior backdoor-related studies. We
summarized them in Table 7.
GTSRB. This dataset is built for trafﬁc sign classiﬁcation tasks
in the self-driving scenario [35]. The target model we tested
on this dataset has a simple architecture of 6 convolution
layers and 2 dense layers (Table 6), that is the same with the
model used in Neural Cleanse.
ILSVRC2012. This dataset is built for recognizing general ob-
jects (e.g., ﬁsh, dog, etc.) from images [31]. The target model
we tested on this dataset is with the structure ResNet50 [11].
MegaFace. This dataset is built for face recognition [27]. The
target model we tested on this dataset is with the structure
ResNet101 [11]. More speciﬁcally, following the rules of
MegaFace Challenge1, we tested our model by ﬁnding sim-
ilar images for a given FaceScrub image [29] from both the
FaceScrub dataset and 1M “distractor” images 2.
CIFAR-10. This dataset is also built for recognizing general
objects from images [18]. The target model we tested on this
dataset is in the structure illustrated in Table 6.
All these models trained in our research achieved classiﬁ-
cation performance comparable with those reported by state-
of-the-art approaches (Table 7). We prefer using GTSRB to
demonstrate some of our elementary results, as this dataset is
not too big to make our studies be hardly reproduced but rich
enough to be taken as the example. Speciﬁcally, it contains
more diversiﬁed images than the MNIST [19] dataset and
more categories than the CIFAR-10 [18] dataset.
2.4 Threat Model
Unlike the backdoor attacks on federated learning [1], we
consider a data poisoning threat, in which the model training
is outside the adversary’s control (see below) but part of the
training data can be manipulated by the adversary.
1http://megaface.cs.washington.edu/participate/challenge2.html
2http://megaface.cs.washington.edu/dataset/download_training.html
Adversary goals. The objective of the adversary is to inject
one or more backdoors into the target model trained by the
model provider through the data contamination. The contam-
inated model will misclassify the inputs carrying a trigger
while correctly label other inputs.
Adversarial capabilities. We assume that the adversary has
the full control of some data sources, capable of arbitrarily
changing their data, but he has no direct access to the model
and the training process on the provider’s end, except offering
some training data.
Adversarial knowledge. We consider a black-box adversary
who does not have information about the inner parameters of
the target model and the data from the sources that are out of
his control. On the other hand, he knows the target model’s ar-
chitecture, used optimization algorithm and hyper-parameters
(Section 4.6). Finally, we assume that the adversary may know
the defense strategy, and attempt to bypass it.
Defense goals. We aim at developing a defense strategy to
determine whether a given model is infected by a backdoor
from the instances it classiﬁes, and if it is, to ﬁnd out which
classes are infected. Furthermore, our approach can also de-
tect the inputs that will trigger a hidden backdoor online in a
Machine-Learning-as-a-service setting (Section 4.5).
Defender’s capabilities. We consider the defender who has
full access to the data and the target model, including the
representations R(x) of the input x, but does not interfere with
the training process performed by the model provider.
Defender’s knowledge. We assume that the defender has a
(small) collection of clean data given by the model provider
for testing the model’s performance, as also assumed in pre-
vious studies [8, 9]. In our research, we adjusted the clean
data size from 10% to 1% of the training set to ﬁnd out the
minimum amount of the data necessary for maintaining the
effectiveness of our approach.
3 Defeating Backdoor Detection
In this section, we report our analysis of backdoors inside
DNN models introduced by data contamination. Our research
leads to new discoveries: backdoors created by conventional
data contamination methods are source-agnostic and charac-
terized by unique representations of attack images, which are
mostly determined by the trigger, regardless of other image
content, and clearly distinguishable from those of normal im-
ages. More importantly, some existing detection techniques
are found to heavily rely on this property, and thus are vul-
nerable to a new targeted attack using attack images with
less distinguishable representations. Our research concludes
that some existing protections fail to raise the bar to even a
black-box contamination attack that injects source-speciﬁc
backdoors.
USENIX Association
30th USENIX Security Symposium    1543
3.1 Understanding Backdoor Contamination
Representation space analysis. As shown in previous pa-
pers [7, 32], most of current backdoors are global and thus
source-agnostic, i.e., the infected model assigns the target la-
bel to trigger-carrying images regardless which category they
come from. We observe that to effectively embed a source-
agnostic backdoor into the target model requires to contam-
inate the training data by not only just a small collection
of trigger-carrying (attack) images, but these images can all
come from the same class. This observation implies that the
representation of an attack image is mostly determined by the
trigger, as further conﬁrmed in our research.
Speciﬁcally, we want to answer the following question:
how many different classes (source labels) does the adver-
sary need to select the attack images from so that he can
embed a source-agnostic backdoor into the target model. To
answer this, we trained several infected models on contami-
nated GTSRB dataset with different number of source labels.
Concretely, we varied the number of source labels from 1
to 10, ﬁxed the target label as 0 and exploited a box trigger
(Fig. 9a). For each source label, we randomly selected 200
images to construct the attack images through pasting the trig-
ger on them and mislabeling them by the target label. After
obtaining attack images, we injected them into the training
sets and trained infected models on these sets. Table 1 summa-
rizes the average results over ﬁve repetitive experiments, in
which the global misclassiﬁcation rate represents the fraction
of images across all classes that are assigned as the target
label after the trigger is inserted, and the targeted misclassi-
ﬁcation rate represents the fraction of the images from the
given source classes that are assigned as the target label (at-
tack success rate). As we can see, even if only 0.5% of the
training dataset are contaminated by the attack samples all
from a single source class, the global misclassiﬁcation rate
goes above 50%, i.e., more than half of the trigger-carrying
images across all labels are misclassiﬁed by the model as
the target label. From Table 1, we also found that increasing
attack images while keeping the number of source labels un-
changed can slightly raise the global misclassiﬁcation rate,
but increasing the number of source labels is a more effective
way to achieve that.
The above ﬁnding indicates that the infected model likely
identiﬁes the source-agnostic trigger separately from the orig-