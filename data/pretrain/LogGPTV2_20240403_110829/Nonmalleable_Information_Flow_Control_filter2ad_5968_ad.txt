i , ti⟩ then t1 ≈⋆W t2.
6.4 Robust declassification and
irrelevant inputs
We now move to security conditions for programs that do not sat-
isfy noninterference. Recall that robust declassification informally
means the attacker has no influence on what information is released
by declassification. Traditionally, it is stated in terms of attacker-
provided code that is inserted into low-integrity holes in programs
which differ only in their secret inputs. In NMIFC, the same attacker
power can be obtained by substituting exactly two input values into
the program, one secret and one untrusted. This simplification is
possible because NMIFC has first-class functions that can model the
substitution of low-integrity code. Appendix C shows that this sim-
pler two-input definition is equivalent to the traditional hole-based
approach in the full version of NMIFC (Appendix A).
Prior work on while-based languages [11, 27] defines robust
declassification in terms of four traces generated by the combination
of two variations: a secret input and some attacker-supplied code.
For terminating traces, these definitions require any pair of secrets
to produce public-equivalent traces under all attacks or otherwise
to produce distinguishable traces regardless of the attacks chosen.
This implies that an attacker cannot control the disclosure of secrets.
We can attempt to capture this notion of robust declassifcation
using the notation of NMIFC. For a program e with a secret input x
and untrusted input y, we wish to say e robustly declassifies if, for
all secret values v1, v2 and for all untrusted values w1, w2, where
(cid:68)
e[x (cid:55)→ vi][y (cid:55)→ wj], vi; wj
(cid:69) −→→ ∗(cid:68)
vij , tij(cid:69)
,
then t11 ≈⋆P t21 ⇐⇒ t12 ≈⋆P t22.
(cid:16)
λ(x : (P→ ∧ U ← says τ ) × (P→ ∧ U ← says τ ))[P→ ∧ T ←].
decl
(cid:16)
to P→ ∧ T ←(cid:17)(cid:68)atk1, atk2
bind b = (ηS→∧T ← sec) in
case b of inj1 . (ηS→∧T ← (proj1 x ))
| inj2 . (ηS→∧T ← (proj2 x ))
(cid:69)
(cid:17)
Figure 11: A program that admits inept attacks. Here P ⊑ S and
T ⊑ U , but not vice versa, so sec is a secret boolean and ⟨atk1, atk2⟩
form an untrusted pair of values. If atk1 (cid:44) atk2, then the attacker
will learn the value of sec. If atk1 = atk2, however, then the attacker
learns nothing due to its own ineptness.
This condition is intuitive but, unfortunately, overly restrictive.
It does not account for the possibility of an inept attack, in which an
attacker causes a program to reveal less information than intended.
Inept attacks are harder to characterize than in previous work
because, unlike the previously used while-languages, NMIFC sup-
ports data structures with heterogeneous labels. Using such data
structures, we can build a program that implicitly declassifies data
by using a secret to influence the selection of an attacker-provided
value and then declassifying that selection. Figure 11 provides an
example of such a program, which uses sums and products from
the full NMIFC language.
While this program appears secure—the attacker has no control
over what information is declassified or when a declassification
occurs—it violates the above condition. One attack can contain
the same value twice—causing any two secrets to produce indis-
tinguishable traces—while the other can contain different values.
Intuitively, no vulnerability in the program is thereby revealed; the
program was intended to release information, but the attacker failed
to infer it due to a poor choice of attack. Such inputs result in less
information leakage entirely due to the attacker’s ineptness, not
an insecurity of the program. As a result, we consider inputs from
inept attackers to be irrelevant to our security conditions.
Dually to inept attackers, we can define uninteresting secret
inputs. For example, if a program endorses an attacker’s selection
of a secret value, an input where all secret options contain the same
data is uninteresting, so we also consider it irrelevant.
Which inputs are irrelevant is specific to the program and to
the choice of attacker. In Figure 11, if both execution paths used
(proj1 x ), there would be no way for an attacker to learn any
information, so all attacks are equally relevant. Similarly, if S→ is
already considered public, then there is no secret information in
the first place, so again, all attacks are equally relevant.
For an input to be irrelevant, it must have no influence over the
outermost layer of the data structure—the label that is explicitly
downgraded. If the input could influence that outer layer in any way,
the internal data could be an integral part of an insecure execution.
Conversely, when the selection of nested values is independent
of any untrusted/secret information (though the content of the
values may not be), it is reasonable to assume that the inputs will
be selected so that different choices yield different results. An input
which does not is either an inept attack—an attacker gaining less
information than it could have—or an uninteresting secret—a choice
between secrets that are actually the same. In either case, the input
is irrelevant.
To ensure that we only consider data structures with nested
values that were selected independently of the values themselves,
we leverage the noninterference theorems in Section 6.3. In par-
ticular, if the outermost label is trusted before a declassification
(or public prior to an endorsement), then any influence from un-
trusted (secret) data must be the result of a prior explicit downgrade.
Thus we can identify irrelevant inputs by finding inputs that re-
sult in traces that are public-trusted equivalent, but can be made
both public (trusted) equivalent and non-equivalent at the point of
declassification (endorsement).
To define this formally, we begin by partitioning the principal
lattice into four quadrants using the definition of an attacker from
Section 6.1. We consider only flows between quadrants and, as with
noninterference, downgrades must result in public or trusted values.
We additionally need to refer to individual elements and prefixes
of traces. For a trace t, let tn denote the nth element of t, and let
t..n denote the prefix of t containing its first n elements.
Definition 6.4 (Irrelevant inputs). Consider attacker A inducing
high sets H← and H→. Let Wπ = L \ Hπ and W = W← ∩ W→.
Given opposite projections π and π′ a program e, and types τx and
τy such that ⊢ τx prot Hπ and ⊢ τy prot Hπ′, we say an input v1
is an irrelevant π′-input with respect to A and e if Γ; pc ⊢ v1 : τx
and there exist values v2, w1, and w2 and four trace indices nij (for
i, j ∈ {1, 2}) such that the following conditions hold:
(1) Γ; pc ⊢ v2 : τx , Γ; pc ⊢ w1 : τy, and Γ; pc ⊢ w2 : τy
(2) ⟨e[x (cid:55)→ vi][y (cid:55)→ wj], vi; wj⟩ −→→ ∗ ⟨vij , tij⟩
ij
(3) t
ni j
ij
(4) t
(5) t11
(6) t21
..nkl for all i, j, k, l ∈ {1, 2}
..ni j ≈⋆W tkl
..n11 ≈⋆Wπ
t12
..n12
t22
(cid:48)⋆Wπ
..n22
Otherwise we say v1 is a relevant π′-input with respect to A and
e, denoted relπ′
A,e (v1). Note that the four indices nij identify corre-
sponding prefixes of the four traces.
(cid:48)W • for all i, j ∈ {1, 2}
..n21
As mentioned above, prior downgrades can allow secret/un-
trusted information to directly influence the outer later of the data
structure, but Condition 4 requires that all four trace prefixes be
public-trusted equivalent, so any such downgrades must have the
same influence across all executions. Condition 5 requires that
some inputs result in prefixes that are public equivalent (or trusted
equivalent for endorsement), while Condition 6 requires that other
inputs result in prefixes that are distinguishable. Since all prefixes
are public-trusted equivalent, this means there is an implicit down-
grade inside a data structure, so the equivalent prefixes form an
irrelevant input.
We can now relax our definition of robust declassification to
only restrict the behavior of relevant inputs.
Definition 6.5 (Robust declassification). Let e be a program and
let x and y be variables representing secret and untrusted inputs,
respectively. We say that e robustly declassifies if, for all attackers
A inducing high sets U and S (and P = L \ S) and all values
v1, v2, w1, w2, if
(cid:68)
A,e (w1) and t11 ≈⋆P t21(cid:17)
e[x (cid:55)→ vi][y (cid:55)→ wj], vi; wj
rel←
(cid:69) −→→ ∗(cid:68)
=⇒ t12 ≈⋆P t22.
vij , tij(cid:69)
then
(cid:16)
,
As NMIFC only restricts declassification of low-integrity data,
endorsed data is free to influence future declassifications. As a
result, we can only guarantee robust declassification in the absence
of endorsements.
Theorem 6.6 (Robust declassification). Given a program e,
if Γ, x :τx , y :τy; pc ⊢ e : τ and e contains no endorse expressions,
then e robustly declassifies as defined in Definition 6.5.
Note that prior definitions of robust declassification [11, 27] sim-
ilarly prohibit endorsement and ignore pathological inputs, specifi-
cally nonterminating traces. Our irrelevant inputs are very different
since NMIFC is strongly normalizing but admits complex data struc-
tures, but the need for some restriction is not new.
6.5 Transparent endorsement
We described in Section 2 how endorsing opaque writes can cre-
ate security vulnerabilities. To formalize this intuition, we present
transparent endorsement, a security condition that is dual to ro-
bust declassification. Instead of ensuring that untrusted informa-
tion cannot meaningfully influence declassification, transparent
endorsement guarantees that secret information cannot meaning-
fully influence endorsement. This guarantee ensures that secrets
cannot influence the endorsement of an attacker’s value—neither
the value endorsed nor the existence of the endorsement itself.
As it is completely dual to robust declassification, we again ap-
peal to the notion of irrelevant inputs, this time to rule out un-
interesting secrets. The condition looks nearly identical, merely
switching the roles of confidentiality and integrity. It therefore
ensures that any choice of interesting secret provides an attacker
with the maximum possible ability to influence endorsed values;
no interesting secrets provide more power to attackers than others.
Definition 6.7 (Transparent endorsement). Let e be a program and
let x and y be variables representing secret and untrusted inputs,
respectively. We say that e transparently endorses if, for all attackers
A inducing high sets U and S (and T = L \ U) and all values
v1, v2, w1, w2, if
(cid:68)
A,e (v1) and t11 ≈⋆T t12(cid:17)
e[x (cid:55)→ vi][y (cid:55)→ wj], vi; wj
rel→
(cid:69) −→→ ∗(cid:68)
=⇒ t21 ≈⋆T t22.
vij , tij(cid:69)
then
(cid:16)
,
As in robust declassification, we can only guarantee transparent
endorsement in the absence of declassification.
Theorem 6.8 (Transparent endorsement). Given a program
e, if Γ, x :τx , y :τy; pc ⊢ e : τ and e contains no decl expressions,
then e transparently endorses.
6.6 Nonmalleable information flow
Robust declassification and transparent endorsement each restrict
one type of downgrading, but as structured above, cannot be en-
forced in the presence of both declassification and endorsement.
The key difficulty stems from the fact that previously declassified
and endorsed data should be able to influence future declassifi-
cations and endorsements. However, any endorsement allows an
attack to influence declassification, so varying the secret input can
cause the traces to deviate for one attack and not another. Similarly,
once a declassification has occurred, we can say little about the
relation between trace pairs that fix a secret and vary an attack.
There is one condition that allows us to safely relate trace pairs
even after a downgrade event: if the downgraded values are identi-
cal in both trace pairs. Even if a declassify or endorse could have
caused the traces to deviate, if it did not, then this program is
essentially the same as one that started with that value already
downgraded and performed no downgrade. To capture this intu-
ition, we define nonmalleable information flow in terms of trace
prefixes that either do not deviate in public values when varying
only the secret input or do not deviate in trusted values when vary-
ing only the untrusted input. This assumption may seem strong at
first, but it exactly captures the intuition that downgraded data—
but not secret/untrusted data—should be able to influence future
downgrades. While two different endorsed attacks could influence
a future declassification, if the attacks are similar enough to re-
sult in the same value being endorsed, they must influence the
declassification in the same way.
Definition 6.9 (Nonmalleable information flow). Let e be a pro-
gram and let x and y be variables representing secret and untrusted
inputs, respectively. We say that e enforces nonmalleable informa-
tion flow (NMIF) if the following holds for all attackers A inducing
high sets U and S. Let T = L \ U, P = L \ S and W = T ∩ S.
(cid:68)
(cid:69) −→→ ∗(cid:68)
For all values v1, v2, w1, and w2, let
e[x (cid:55)→ vi][y (cid:55)→ wj], vi; wj
(cid:48)W •
(cid:17)
(cid:17)
For all indices nij such that t
(1) If ti1
..ni1−1 ≈⋆T ti2
rel←
A,e (w1) and t11
2j
..n2j−1 for j = 1, 2, then
..ni2−1 for i = 1, 2, then
vij , tij(cid:69)
..n1j−1 ≈⋆P t
..n12 ≈⋆P t22
..n11 ≈⋆P t21
(2) Similarly, if t
=⇒ t12
(cid:16)
(cid:16)
..n22 .
ij
ni j
rel→
A,e (v1) and t11
..n11 ≈⋆T t12
..n12
=⇒ t21
..n21 ≈⋆T t22
..n22 .
.
..n21
1j
Unlike the previous conditions, NMIFC enforces NMIF with no
syntactic restrictions.
Theorem 6.10 (Nonmalleable information flow). For any
program e such that Γ, x :τx , y :τy; pc ⊢ e : τ , e enforces NMIF.
We note that both Theorems 6.6 and 6.8 are directly implied by
Theorem 6.10. For robust declassification, the syntactic prohibition
on endorse directly enforces ti1 ≈⋆T ti2 (for the entire trace),
and the rest of case 1 is exactly that of Theorem 6.6. Similarly, the
syntactic prohibition on decl enforces t1j ≈⋆P t2j, while the rest
of case 2 is exactly Theorem 6.8.
7 NMIF AS 4-SAFETY
Clarkson and Schneider [13] define a hyperproperty as “a set of sets
of infinite traces,” and hypersafety to be a hyperproperty that can
be characterized by a finite set of finite trace prefixes defining some
“bad thing.” That is, given any of these finite sets of trace prefixes it
is impossible to extend those traces to satisfy the hyperproperty. It
is therefore possible to show that a program satisfies a hypersafety
property by proving that no set of finite trace prefixes emitted by
the program fall into this set of “bad things.” They further define a
k-safety hyperproperty (or k-safety) as a hypersafety property that
limits the set of traces needed to identify a violation to size k.
Clarkson and Schneider note that noninterference provides an
example of 2-safety. We demonstrate here that robust declassifica-
tion, transparent endorsement, and nonmalleable information flow
are all 4-safety properties.4
For a condition to be 2-safety, it must be possible to demonstrate
a violation using only two finite traces. With noninterference, this
demonstration is simple: if two traces with low-equivalent inputs
are distinguishable by a low observer, the program is interfering.
Robust declassification, however, cannot be represented this way.
It says that the program’s confidentiality release events cannot be