### Evaluation of the Diagnostic Capabilities

#### 1. Introduction
When a company sponsors laboratory tests to highlight the advantages of their product, there is a risk that the results may be biased. Even if direct manipulation of the tests is not suspected, the test configurations for competing products are often not handled by experts, leading to unfair comparisons. The sparse descriptions of these tests make it difficult to form an impartial judgment. For example, normal traffic generation is typically done using commercial products, but the test reports often only mention the name of the traffic generator without providing details on the type of traffic or its configuration.

Journalists have also been involved in testing intrusion-detection systems (IDS). One notable article by Mueller and Shipley [9] provides a comparative test of commercial IDS on live traffic, which increases the likelihood of detecting both missed attacks and false positives. However, the main drawback of such tests is their reproducibility, as it is challenging to recreate the conditions under which alerts are generated. Additionally, the tuning phase during testing, where false alerts are turned off, can be problematic. Our approach is to keep a maximum number of signatures active and tabulate both appropriate and false alerts, demonstrating the trade-off between catching more attacks and generating more false positives.

#### 2. IDS Evasion
Another relevant area is the work on evasion techniques, particularly highlighted by Ptacek and Newsham [10] and further developed by Vern Paxson et al. [6]. Network-based IDS are periodically shown to be vulnerable to evasion techniques. However, conducting such tests is extremely difficult, and we believe that our test bed should focus on evasion at the application protocol layer, an area where commercial products still need improvement.

There are two main issues with application-layer protocols:
- **Misunderstanding of Protocol States or Properties**: Some vulnerabilities are only applicable to certain states of the application layer protocols. For example, Sendmail vulnerabilities usually apply only to the SMTP command mode. IDS products must recognize these states and verify that state information is adequate before applying signatures. Maintaining state information is processor-intensive, and infrequent states are sometimes ignored to improve performance, creating evasion opportunities.
- **Misunderstanding of Protocol Encoding Schemes**: Protocols can encode data, hiding information from the IDS and inducing false positives. For example, Unicode encoding in HTTP requests can result in false positives. Implementers sometimes include hidden features or encoding schemes that deviate from published specifications, such as the infamous %u Unicode encoding issue [2].

Our test bed currently uses Whisker [13] for HTTP scanning, which has knowledge of HTTP evasion techniques. We are considering a transition to the libwhisker library, which provides additional evasive capabilities. Whisker is an efficient and stealthy HTTP server scanner, and while efficiency is not a primary concern for us, stealth is important as it makes the job of the tested IDS more challenging.

#### 3. France Télécom R&D Intrusion-Detection Test Bed
Based on this background, we decided to develop our own test bed, incorporating interesting ideas and improving on identified weaknesses. Our test bed is segmented into five areas, each containing a set of tests that can be executed with different parameters. The test bed repeats as many executions of each test set with as many parameter combinations as relevant for the expected results.

##### 3.1 Objectives of the Test Bed
While designing the test bed, we set several objectives:
- **Fairness**: Ensure that all products receive the same input and have a chance to correctly detect the attack. Each appliance has two network interfaces, one for sniffing malicious traffic and one for management purposes. All possible signatures are enabled during installation to maximize detection capabilities.
- **Repeatability**: Regular updates to detection software and the knowledge base are necessary due to the frequent emergence of new threats. The testing process will be repeated regularly to ensure that the tools deployed in the field still perform as expected and to apply regression testing.
- **Automation**: Automation of test execution and result exploitation is a key goal. Scripts run the tests continuously for up to three days, and the tabulation and result extraction phase is also automated.
- **Baseline**: We compare the tested products with Snort to establish if they are better than what the security community provides and maintains for free.

##### 3.2 Description of the Test Protocol
The following five test sets have been implemented in the test bed:
- **IP Manipulation Tests**: These tests involve low-level manipulations of the IP packet, such as targa or winnuke, resulting in denial-of-service. Seventeen different vulnerabilities are tested.
- **Trojan Horse Traffic**: These tests focus on the detection of management traffic and do not carry out denial-of-service attempts. Four Trojans are installed on the test bed, running unmodified and using default communication ports and unencrypted traffic.
- **Whisker Vulnerability Scanning**: These tests use the freely available Whisker cgi scanner, repeating the scan with the default database of vulnerabilities and multiple evasion parameters.
- **Live CGI Attacks**: These tests carry out real attacks against a vulnerable HTTP server. This set of tests is currently performed manually to ensure that pre-conditions are met and to restore the server to its original state.
- **Whisker Signature Evaluation**: These tests use a specially crafted database to evaluate the extent and accuracy of each signature. The goal is to verify that the signatures listed in the product documentation trigger and to approximate the trigger that sets off the alert.

##### 3.3 False Alerts
Interpreting results in the context of valid and false alerts is complex. During result analysis, we built a table of valid and false alerts based on the documentation and our understanding of the alerts. An example is the Shopper Directory Traversal vulnerability (bugtraq 1776, CVE-2000-0922), where a valid alert would include specific details about the attack, while a false alert would be too broad or lack necessary context.

#### 4. Results Obtained during the Tests
This section presents the results obtained during a complete run of the tests. The intrusion-detection systems are identified as IDS-A, IDS-B, IDS-C, and IDS-D, representing four of the five commercial leaders in the field. We decided against explicitly naming the products due to significant shortcomings observed in all of them.

##### 4.1 Results of the IP Manipulations Tests
Table 1 lists the attacks implemented and shows the results obtained. The first column identifies the attack, the next five columns show the number of different alerts considered valid for each IDS, and the final five columns show the number of alerts considered false for each attack. The summary counts the number of events flagged and the total number of different alerts for each tool in each category.

| Attack Name | Valid Alerts | False Alerts |
|-------------|--------------|--------------|
| papabroadcast | 1 | 0 |
| pinger | 2 | 0 |
| gewse | 0 | 1 |
| nestea (can-1999-0257) | 1 | 1 |
| newtear (CAN-1999-0104) | 0 | 1 |
| targa2-bonk (can-1999-0258) | 1 | 0 |
| targa2-jolt (can-1999-0345) | 0 | 1 |
| targa2-land (cve-1999-0016) | 1 | 0 |
| targa2-syndrop (can-1999-0257) | 0 | 1 |
| targa2-winnuke (cve-1999-0153) | 1 | 0 |
| targa2-1234 | 0 | 1 |
| targa2-sayhousen | 0 | 1 |
| targa2-oshare (can-1999-0357) | 1 | 0 |
| kkill | 0 | 1 |
| octopus | 0 | 1 |
| overdrop (can-1999-0257) | 1 | 0 |
| synful | 0 | 1 |

These results highlight the varying effectiveness of different IDS in detecting and responding to various types of IP manipulation attacks.