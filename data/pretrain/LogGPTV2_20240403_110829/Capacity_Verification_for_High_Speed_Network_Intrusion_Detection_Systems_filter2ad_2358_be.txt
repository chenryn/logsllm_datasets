ing laboratory to provide results that highlight the advantages of the product
sponsored by the company paying for the tests. Even if direct test manipulation
is not suspected, at least test conﬁgurations for the competing products is likely
not to be handled by experts, thus resulting in unfair comparison.
Our feeling is that even this cannot be determined, as the description of
the tests is very sparse and does not provide information that would allow an
impartial judgement of the tests. Normal traﬃc generation, for example, is done
using commercial products and the test report is considered complete with only
the mention of the name of the traﬃc generator, without any information on the
kind of traﬃc it generates or its conﬁguration.
Journalists have also been involved in the testing of intrusion-detection sys-
tems. The most interesting article that we have found is by Mueller and Shipley
[9]. It is the only comparative test of commercial intrusion-detection systems that
we have found, where a number of intrusion-detection products are compared
on equal footing and on live traﬃc, hence with a higher probability of either
missing attacks or triggering false positives. The main drawback of this kind of
test is reproducibility: when observing alerts it is quite diﬃcult to reproduce the
conditions in which these alerts are generated. Also, we believe that the tuning
phase carried out during testing initiation is problematic; the testers describe a
process in which alerts that they believe are false are turned oﬀ. Our approach
has been the opposite, keeping a maximum number of signatures active and
tabulating both appropriate and false alerts. Doing this enables us to demon-
strate the current trade-oﬀ of signature-based (and probably misuse-detection
only systems), that a large number of patterns catches more attacks but also
generates more false alerts.
2.4
IDS Evasion
Another, related work, is the work on evasion, particularly pointed by Ptacek
and Newsham [10], and enhanced by Vern Paxson and al.[6]. We consider this
to be of extreme relevance to our work since network-based intrusion-detection
systems periodically are shown vulnerable to evasion techniques.
However, we also believe that carrying out this kind of tests is extremely
diﬃcult, and that we would not be able to add much value to the state of the art
in lower layers evasion (IP, tcp and udp evasion). Therefore, our test bed focuses
Evaluation of the Diagnostic Capabilities
181
on evasion at the application protocol layer, because we believe that application
protocol analysis is still an area of improvement for commercial products. There
are in fact two distinct issues with application-layer protocols:
Misunderstanding of the protocol states or properties. Sometimes, vul-
nerabilities are only applicable to certain states of the application layer proto-
cols. For example, Sendmail vulnerabilities usually apply only to the SMTP
command mode. IDS products need to recognize these states and verify that
state information is adequate before applying signatures. Keeping state in-
formation is processor-costly and infrequent states are sometimes ignored by
the implementors to improve performance, creating evasion opportunities for
attackers.
Misunderstanding of protocol encoding schemes. Sometimes, protocols
encode data, hiding the information from the intrusion-detection system and
inducing false positives. For example, unicode encoding on HTTP requests
can result in false positives. Worse, implementers sometimes include hid-
den features or encoding schemes that deviate from the published protocol
speciﬁcations, such as the infamous %u unicode encoding issue [2].
Our test bed currently uses Whisker [13] as a support for HTTP scanning
and therefore has knowledge of HTTP evasion techniques. We are examining a
transition to the libwhisker library, which provides additional evasive capabilities
to be used in normal http traﬃc as well. Whisker is an HTTP server scanner
looking for vulnerable cgi scripts. Its two main properties are eﬃciency and
stealth. While we do not care too much about eﬃciency, stealth is important for
us because it makes the job of the tested intrusion-detection probes harder. The
traﬃc is more diﬃcult to analyse, and is also more likely to create false alerts.
3 The France T´el´ecom R&D Intrusion-Detection Test
Bed
From this background, we decided to develop our own test bed, reusing the
interesting ideas and trying to improve where we felt there were weaknesses.
Our test bed is segmented in ﬁve areas, described in Sect. 3.2. Each of these
areas contains a set of tests that can be executed with diﬀerent parameters. Our
test bed repeats as many executions of each test set with as many parameter
combinations as relevant for the expected results.
3.1 Objectives of the Test Bed
While designing the test bed, we set a number of objectives that this design
must meet when performing the comparative tests.
Fairness. The test bed must ensure that all products receive the same input
and have a chance to correctly detect the attack. Each appliance has two
network interfaces to separate LANs, one for sniﬃng the malicious traﬃc and
182
H. Debar and B. Morin
one for management purposes. During installation, all possible signatures are
enabled to ensure that detection capabilities are at a maximum. This setting
ensures that we receive as much information about the traﬃc as possible and
enables us to review the proposed diagnostic.
Repeatability. Updates to both the detection software and the knowledge base
are frequent. Since recent attacks are usually more dangerous, updates must
be introduced as quickly as possible. The testing process will have to be
repeated on a regular basis, to ensure that the tools deployed in the ﬁeld
still perform as expected, and to apply regression testing to discover whether
“older” vulnerabilities are still being detected, whether the false alert rate
has been improved, and whether performance is still comparable to the initial
data.
Automation. Automation covers two diﬀerent areas of the test bed, running
the tests and exploiting the results. Automating the execution of the tests
on the test bed and collecting the data has been an important goal of our
design. Scripts keep the battery of tests running continuously for up to three
days. Note, however, that a few tests described in Sect. 4.5 have not been
automated. The main reason for this manual execution is the diﬃculty of
checking all possible error conditions and verifying that the test actually
succeeded. The tabulation and result extraction phase is also automated,
reading data from syslog and computing the aggregated results from the
several thousand alerts generated by one test run.
Baseline. Not only do we want to compare intrusion-detection products with
each other, we also wish to establish if products are actually better than
what the security community provides and maintain for free. The tested
products are compared with Snort.
3.2 Description of the Test Protocol
The following ﬁve test sets have been implemented in the test bed:
IP Manipulation Tests. These tests are related to low level manipulations of
the IP packet, such as targa or winnuke, which result in denial-of-service.
The test bed runs and veriﬁes seventeen diﬀerent vulnerabilities.
Trojan horse traﬃc. These tests concentrate on the detection of management
traﬃc, and as such do not carry out denial-of-service attempts against the
server. We installed these four Trojans on the test bed, ensuring that all the
components were indeed available and running. The Trojans run unmodi-
ﬁed, and as such use default publicly known communication ports, default
command sets and unencrypted traﬃc.
Whisker vulnerability scanning. These
freely available
Whisker [13] cgi scanner, repeating the scan with the default database
of vulnerabilities with multiple evasion parameters. We expect intrusion-
detection vendors to use Whisker in their test suites, as we believe it is a
tool actively in use for information gathering purposes, even though it is a bit
old by Internet standards. Unfortunately, Sect. 4.3 shows that the diagnostic
of Whisker scans is still lacking.
tests use
the
Evaluation of the Diagnostic Capabilities
183
The same test database is used for 21 test runs, the ﬁrst four with the GET
method (-M command line ﬂag), the next 9 with the HEAD method and the
evasion parameter (-I command line ﬂag) incremented from 1 to 9, and
the next 7 with the HEAD method, and the evasion parameter set to -I 1
-I n with n varying from 2 to 9. Note that the -M command line switch
is overridden for certain tests in the default database. Also, some tests are
not carried out in some evasion modes, which results in a smaller number of
requests reported by the Attacker.
Live cgi attacks. These tests carry out real attacks against our vulnerable
HTTP server. These attacks have varying results, some of them giving a
shell with httpd user privileges and the others displaying system ﬁles. At
this stage, this set of tests has to be carried out manually to ensure that pre-
conditions are met, that compromise eﬀectively happens and to restore the
server to its original state. Our ﬁle target is the /etc/passwd ﬁle, to ensure
that all intrusion-detection products would have a chance to see abnormal
activity. While other ﬁles are worthy of an attacker’s attentions, the only
common one that we have found registered across all intrusion-detection
systems is the UnixTM password ﬁle.
Whisker signature evaluation. These tests use the freely available Whisker
cgi scanner and a specially crafted database taking into account the list
of HTTP vulnerabilities that all products document, constructing creative
requests to evaluate the extend and accuracy of each signature (see exam-
ples in Sect. 4.6). For each signature, the database contains a set contains
URLs related to diﬀerent attacker activity, such as scanning, normal activity
(attempting to reproduce normal requests), outright malicious activity that
would directly exploit the vulnerability, and abnormal activity that either
extends the direct exploits with our own knowledge or derives from these
direct exploits. The goal of this test is to verify that signatures listed in the
product documentation do trigger and to approximate the trigger that sets
the alert oﬀ. We expect this information to be very valuable for analysts as-
sessing the alerts representing the diagnostic of the tested intrusion-detection
systems, particularly if detailed alert description is not available.
Clearly, this does not cover all possible intrusive activity. Future extensions
of the test bed include generating normal and abnormal DNS, mail (SMTP, POP
and IMAP) RPC and ﬁle (NFS, Samba) traﬃc to broaden the coverage of the
test. Our emphasis on HTTP is justiﬁed by its status today as an ubiquitous
transport protocol, used not only for page serving, but also for additional traﬃc
simply because it can traverse the ﬁrewall protecting most organizations today.
Also, most content is served from web servers even if the server only acts as a
mediation portal.
3.3 False Alerts
In this paper, a lot of results have to be interpreted in the light of what is a valid
alert, and what is a false alert. At this stage, there is no straightforward answer.
184
H. Debar and B. Morin
During result analysis, we built a table of valid alerts and false alerts, according
to the documentation associated with each alert, and our understanding of what
the alert means. Since we practice black-box testing and do not have access to
the signatures, we cannot understand why an alert appears and have to rely on
our better judgment.
Let’s take the example of the Shopper Directory Traversal vulnerability (bug-
traq 1776, CVE-2000-0922).
An attack will take the following form:
$ lynx http://target/cgi-bin/shopper.cgi?newpage=../../../path/filename.ext
The combination of the shopper.cgi in the request and the newpage variable
in the parameters is the mark of the attack. In addition, a directory traversal is
required to break out of the default web directory, and a ﬁle of interest has to
be speciﬁed.
An example of a false alert would be shop, because it is much too wide to
work with. An approximate alert would be shopper.cgi without any addition,
because it would not allow us to diﬀerentiate between a vulnerability assessment
scan and a real attempt. What we expect of an alert in this case would be a single
message pointing out shopper.cgi, newpage, the target ﬁle as a parameter, and
any related evasion techniques.
4 Results Obtained during the Tests
This section presents the results obtained during a complete run of the tests.
The intrusion-detection systems will be identiﬁed as IDS-A, IDS-B, IDS-C and
IDS-D, representing four of the ﬁve commercial leaders in the ﬁeld. We decided
against explicitly naming the product because all of them exhibited signiﬁcant
(although not the same) shortcomings and we do not wish these results to be
interpreted as an endorsement of these products. In fact, the shortcomings identi-
ﬁed lead us to believe that none of them would be satisfactory for our demanding
environment.
4.1 Results of the IP Manipulations Tests
Table 1 lists the attacks implemented and shows the results obtained. The ﬁrst
column identiﬁes the attack, the next ﬁve the number of diﬀerent alerts that were
considered valid alerts for each IDS, and the ﬁnal ﬁve the number of alerts that
were considered false alerts for each attack. The summary counts the number
of events ﬂagged and the total number of diﬀerent alerts for each tool in each
category.
The tests carried out here are roughly two years old at the time of writing.
We did observe a number of vulnerable machines around us and this observation
lead us to believe in the relevance of this activity. Also, as a network operator we
are concerned by the validity of traﬃc ﬂowing through the core network. These
signatures provide a measure of normality that is of interest.
Evaluation of the Diagnostic Capabilities
185
Table 1. Results of the IP manipulation tests
Attack
Name
papabroadcast
pinger
gewse
nestea (can-1999-0257)
newtear (CAN-1999-0104)
targa2-bonk (can-1999-0258)
targa2-jolt (can-1999-0345)
targa2-land (cve-1999-0016)
targa2-syndrop (can-1999-0257)
targa2-winnuke (cve-1999-0153)
targa2-1234
targa2-sayhousen
targa2-oshare (can-1999-0357)
kkill
octopus
overdrop (can-1999-0257)
synful
Number of events ﬂagged
Number of alerts sets
Valid alerts
False alerts
Snort IDS-A IDS-B IDS-C IDS-D Snort IDS-A IDS-B IDS-C IDS-D
1
2
0
1
1
0
1
0
1
0
0
0
0
0
0
0
0
6
7
1
1
0
1
1
0
3
0
1
1
2
3
0
0
0
0
1
10
15
1
0
0
1
1
0
0
1
1
1
1
0
0
1
0