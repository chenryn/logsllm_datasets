0
0
25 50 75 100
Viewpoint-moving
)
%
(
F
D
C
speed(deg/s)
100
75
50
25
0
0
Luminance changes
in 5 secs (grey level)
70 140 210
)
%
(
F
D
C
100
75
50
25
0
0
0.5
1
1.5
2
(cid:4)(cid:10)(cid:9)(cid:11)(cid:7)(cid:1)
(cid:6)(cid:12)(cid:15)(cid:13)(cid:8)(cid:10)
DoF diff between objects
in viewport (dioptre)
(cid:4)(cid:6)(cid:15)(cid:16) (cid:9)(cid:15)(cid:7)(cid:16)(cid:8)(cid:12)(cid:15)(cid:11)(cid:20)(cid:1)
(cid:18)(cid:12)(cid:13)(cid:12)(cid:15)(cid:11)(cid:1)(cid:22)(cid:24)(cid:29)(cid:23)
(cid:3)(cid:10)(cid:10)(cid:13)(cid:12)(cid:15)(cid:9)(cid:1)(cid:27)(cid:30)(cid:26)(cid:21)(cid:19)(cid:12)(cid:8)(cid:9)(cid:16)(cid:1)(cid:5)(cid:16)(cid:2) (cid:14)(cid:16)(cid:8)(cid:9)(cid:13)
(cid:22)(cid:24)(cid:28)(cid:23)
Figure 3: Distribution of the new quality-determining factor values.
l
i
a
n
g
i
r
o
/
e
z
s
i
e
l
i
t
l
a
t
o
T
e
z
s
i
o
e
d
v
i
3
2
1
0
3*6
6*12
12*24
Tiling granularity
Figure 4: Average video sizes under different tiling granularities.
(Error bars show the standard deviation of mean).
the users can tolerate 50% more quality distortion on background
pixels than they would have if the video is viewed on a computer
screen. It should be noticed that the viewpoint movements appear
to be dynamic, in part because the dataset includes many outdoor
sports and adventure videos.
3 PANO OVERVIEW
Exploring the aforementioned opportunities, however, requires not
only changing the objective of video quality optimization, but also
re-architecting several critical components of the video streaming
system. We present Pano, a 360° video streaming system that ad-
dresses three key challenges.
Challenge 1: How to predict 360° video user-perceived quality
by incorporating these new quality-determining factors? To
our best knowledge, none of the existing video quality metrics
directly captures the three new factors, so we first need to aug-
ment the existing video quality metrics to measure different user
sensitivities under different viewpoint trajectories.
Our solution: Pano presents a novel 360° video quality metric (§4)
that models the users’ sensitivities to quality distortion as a function
of viewpoint-moving speed, luminance change, and DoF difference.
A naive approach would profile all possible combinations of these
values and each video. Fortunately, we show that we can decouple
the impact of these factors driven by viewpoint movements from
the impact of the video content. Moreover, we found that the impact
of individual factors is largely mutually independent, which further
reduces the efforts to build the new 360° video quality metric.
Challenge 2: How should the 360° videos be spatially split into
tiles to better exploit the new opportunities? Ideally, the tiling
should separate regions with different object-moving speeds (e.g.,
foreground moving objects vs. static background), different DoF,
or different luminance values. But naively splitting the video into
small tiles (e.g., 12×24) will increase the video size by almost 200%
compared to a coarser 3×6-grid tiling (Figure 4).
Our solution: Pano splits it into a handful of variable-size tiles
(§5), rather than equally sized tiles (see Figure 9 for an example). As
a result, users tend to have similar sensitivities to quality distortion
397
(cid:4)(cid:6)(cid:15)(cid:16) (cid:7)(cid:13)(cid:12)(cid:9)(cid:15)(cid:18)(cid:1)
(cid:6)(cid:8)(cid:6)(cid:17)(cid:18)(cid:6)(cid:18)(cid:12)(cid:16)(cid:15)(cid:1)(cid:22)(cid:24)(cid:30)(cid:23)
(cid:17)
(cid:17)
(cid:4)(cid:10)(cid:9)(cid:11)(cid:7)(cid:1)
(cid:7)
(cid:14)(cid:10)(cid:13)(cid:16)(cid:10)(cid:13)
(cid:13)
(cid:2)(cid:3)(cid:5)(cid:1)(cid:18)
Figure 5: Overview of Pano and how it fits in the 360° video delivery.
within each tile (according to history trajectories traces). In this
way, we can maintain a coarse tiling granularity to save bandwidth
while still being able to assign higher quality where users are more
sensitive.
Challenge 3: How to adapt quality in a way that is robust to
dynamic viewport movements and readily deployable over the
existing delivery infrastructure? The video quality adaptation
strategy needs to be revisited for two reasons. First, it must tolerate
the vagaries of available bandwidth and the inevitable errors of
viewpoint movement prediction. Second, it must be deployable
on the existing client-driven video streaming protocol [4], but if
done naively, one would need both client-side information (current
viewpoint movement) and server-side information (video content)
to determine the sensitivity of a user to quality distortion.
Our solution: Our empirical study shows that to pick the desirable
quality for each tile, it is sufficient to estimate a range of the view-
point movement, rather than their precise values (§6.1). For example,
if the viewpoint moves quickly in a short time, it will be difficult to
predict the exact relative viewpoint-moving speed, but Pano can
still reliably estimate a lower bound of the speed based on recent
history. Although Pano may lose some performance gains (e.g.,
assigning a higher-than-necessary quality given underestimated
relative viewpoint-moving speeds), the conservative decisions still
outperform the baselines which ignore the impact of viewpoint
movements. Finally, to be compatible with the existing client-driven
video streaming architecture, Pano encodes a look-up table in the
video manifest file so that the client can approximately estimate
the perceived quality of each quality level without accessing the
actual video content (§6.2).
As shown in Figure 5, although a video delivery system involves
many comments, deploying Pano only requires minor changes by
the content provider (who controls the video encoding) and client-
side device (usually also managed by the same content provider).
No change is needed to the CDNs or the HTTP streaming protocol.
4 PANO: 360° VIDEO QUALITY MODEL
We start with Pano’s video quality model, which estimates the
user-perceived quality under certain viewpoint movement.
4.1 A general video quality framework
Conceptually, Pano incorporates the new quality-determining fac-
tors in Peak Signal-to-Perceptible-Noise Ratio (PSPNR) [30], a stan-
dard perceived quality metric. It improves the classic Peak Signal-
to-Noise Ratio (PSNR) [67] by filtering out quality distortions that
Pano: Optimizing 360° Video Streaming with a Better
Understanding of Quality Perception
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Term Brief description
q, k, t Quality level, chunk index, and tile index
Rk, t(q)
pi, j, ˆpi, j
P(q), M(q)
J N Di, j
The bitrate of the t th tile the k th chunk at quality q
Pixel value at (i, j) on the original or encoded image
PSPNR (or PMSE [30]) of image at quality level q
JND at pixel i, j
Content-dependent JND at pixel (i, j): JND of zero
speed, luminance change, and DoF diff
Ci, j
A(x1, x2, x3) Action-dependent ratio: JND of speed x1, luminance
change x2, and DoF diff x3, divided by C
Table 1: Summary of terminology
are imperceptible by users. The key to PSPNR is the notion of Just-
Noticeable Difference (JND) [67], which is defined by the minimal
changes in pixel values that can be noticed by viewers. PSPNR can
be expressed as follows (Table 1 summarizes the terminology):
(1)
(2)
(3)
M(q) = 1
S
255(cid:2)
P(q) = 20 × log10

(cid:4)|pi, j − ˆpi, j | − J N Di, j
M(q)
(cid:6)
i, j
(cid:5)
2 × Δ(i, j)
Δ(i, j) =
|pi, j − ˆpi, j | ≥ J N Di, j
|pi, j − ˆpi, j | < J N Di, j
1,
0,
where S denotes the image size, pi, j and ˆpi, j denote the pixel at
(i, j) of the original image and that of the image encoded at quality
level q respectively, and J N Di, j denotes the JND at pixel (i, j).
Intuitively, a change on a pixel value can affect the user-perceived
quality (PSPNR) only if it is greater than the JND. In other words,
the notion of JND effectively provides an abstraction of users’ sen-
sitivities to quality distortion, which can be neatly incorporated in
the quality metric of PSPNR.
More importantly, we can incorporate the new quality-determining
factors (§2.2) by changing the calculation of JND—higher relative
viewpoint moving speeds, greater DoF differences, or greater lumi-
nance changes will lead to higher JND.
4.2 Profiling JND of 360° videos
JND has been studied in the context of non-360° videos. However,
prior work has focused on the impact of video content on JND.
For instance, users tend to be less sensitive to quality distortion
(i.e., high JND) in areas of high texture complexity or excessively
high/low luminance [29, 30, 56].
As we have seen, however, 360° videos are different, in that a
user’s sensitivity may vary with the viewpoint movement as well.
In other words, the JND of a pixel (i, j) is also dependent on the
following values: (1) the speed v of an object O (of which pixel
(i, j) is a part) relative to the viewpoint; (2) the luminance l of O
relative to where the viewpoint focused on 5 seconds ago; (3) the
DoF difference d between O and the viewpoint focused object; and
(4) the base JND Ci, j , defined by the JND when there is no viewpoint
movement (i.e., v = 0, l = 0) or DoF difference (d = 0). Because Ci, j
is only dependent on the video content, we refer to it as the content-
dependent JND. We calculate Ci, j using the same JND formulation
from the prior work [29, 30].
To quantify the impact of v, l, d on JND, we ran a user study
using a similar methodology to the prior studies [29, 30]. Readers
can find more details of our methodology in Appendix. The study
398
20
10
D
N
J
20
D
N
J
10
0
0
20
Relative viewpoint-
10
moving speed (deg/s)
100
0
0
Luminance changes
in 5 secs (grey level)
200
50
D
N
J
25
0
0
1
2
DoF diff between objects
in viewport (dioptre)
Figure 6: Impact of individual factors on JND.
50
25
(cid:1)
D
(cid:3)
N
J
(cid:2)
2
1
o
D
F   d iff
0 0
0
20
Relative viewpoint-
moving speed (deg/s)
10
0 0
200
e
c
n
a
100
u m i n
L
100
D
(cid:1)
N
(cid:3)
J
(cid:2)
50
0
20
Relative viewpoint-
moving speed (deg/s)
10
(a) JND vs. viewpoint-moving
(b) JND vs. viewpoint-moving
speed & DoF difference
speed & luminance
Figure 7: Joint impact of two factors on JND.
has 20 participants. Each participant is asked to watch a set of 43
short test videos, each generated with a specific level of quality
distortion. The quality distortion is gradually increased until the
participant reports that the distortion becomes noticeable.
Impact of individual factors: Figure 6 shows how JND changes
with the relative viewpoint-moving speed, luminance change, or
DoF difference, while the other two factors are kept to zero. As
expected, JND increases (i.e., users become less sensitive to quality
distortion) monotonically with higher relative viewpoint-moving
speeds, greater luminance changes, or sharper DoF differences.
Formally, we use Fv(x) (Fl(x) or Fd(x)) to denote the ratio between
the JND when v = x (l = x or d = x) and the JND when v =
0 (l = 0 or d = 0), while holding the other two factors l, d at
zero. We call Fv(x), Fl(x), and Fd(x) the viewpoint-speed multiplier,
the luminance-change multiplier, and the DoF-difference multiplier,
respectively.
Impact of multiple factors: Figure 7 shows the joint impact
of two factors on JND. In Figure 7(a), we notice that JND under
viewpoint-moving speed v = x1 and DoF difference d = x2 can
be approximated by the product of C · Fv(x1) · Fd(x2), where C is
the content-dependent JND (i.e., when v = 0, l = 0, d = 0). This
suggests the impact of these two factors on JND in this test appears
to be independent. We see similar statistical independence between
the impact of luminance change and that of viewpoint-moving
speed or DoF difference. Figure 7(b) shows the joint impact of
viewpoint-moving speed (one of the 360° video-specific factors) and
the viewpoint’s current luminance value (one of the traditional fac-
tors that affect JND). The figure also shows the impact of these two
factors on JND in this test appears to be independent. Notice that
the impact of current luminance value on JND is non-monotonic
because quality distortion tends to be less perceptible when the
video is too bright or too dark.
The observation that different 360° video-specific factors appear
to have independent impact on JND is well aligned with previous
findings that other factors (e.g., content luminance, distance-to-
viewpoint) have largely independent impact on JND [29, 30, 67].
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Y. Guan, C. Zheng, X. Zhang, Z. Guo, J. Jiang.
100
50
)
%
(
F
D
C
0
0
PSPNR w/ 360JND
PSPNR w/ traditional JND
PSNR
20