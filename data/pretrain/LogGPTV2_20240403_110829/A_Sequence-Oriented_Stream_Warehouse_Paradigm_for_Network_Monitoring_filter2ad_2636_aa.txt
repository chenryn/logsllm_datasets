title:A Sequence-Oriented Stream Warehouse Paradigm for Network Monitoring
Applications
author:Lukasz Golab and
Theodore Johnson and
Subhabrata Sen and
Jennifer Yates
A Sequence-Oriented Stream Warehouse Paradigm
for Network Monitoring Applications
Lukasz Golab1, Theodore Johnson2, Subhabrata Sen2, and Jennifer Yates2
1 University of Waterloo, Canada
2 AT&T Labs - Research, Florham Park, NJ, USA
Abstract. Network administrators are faced with the increasingly challenging
task of monitoring their network’s health in real time, drawing upon diverse and
voluminous measurement data feeds and extensively mining them. The role of
database systems in network monitoring has traditionally been that of data reposi-
tories; even if an application uses a database, the application logic is implemented
using external programs. While such programs are ﬂexible, they tend to be ad-
hoc, opaque, inefﬁcient and hard to maintain over time. In this paper, we propose
a new way of implementing network monitoring applications: directly within a
database as continually updated tables deﬁned using a declarative query language
(SQL). We also address a crucial technical issue with realizing this approach:
SQL was designed for set-oriented data transformations, but network monitor-
ing involves sequence-oriented analysis. To solve this problem, we propose an
extension to SQL that makes sequence-oriented analysis easier to express and
faster to evaluate. Using a prototype implementation in a large-scale production
data warehouse, we demonstrate how the declarative sequence-oriented query
language simpliﬁes application development and how the associated system op-
timizations improve application performance.
1 Introduction
Managing a complex network requires tools to rapidly detect and troubleshoot issues.
This involves monitoring of network topology and conﬁguration, network element
status changes (traps) and logs (syslogs), individual network element and end-to-end
performance measurements, control plane state and events, trafﬁc measurements and
events from other network layers (e.g., layer one) [7,14,16]. Understanding what is
happening within the network, particularly during an incident, requires drawing upon
data from these diverse sources. Collecting and exploring these data at scale (hundreds
of millions of records a day) is extremely challenging [5,6,12,13].
There are many advantages to harnessing database and data warehouse technologies
to maintain network data. One operational example is the DataDepot streaming ware-
house system [8] that maintains the Darkstar data repository [11]. Darkstar collects a
wide variety of data and support many production applications, both for real-time alert-
ing and off-line data mining.
However, data warehouses have traditionally been used only as data repositories.
That is, application developers extract data from the warehouse using queries, but place
the application logic in external programs. These programs tend to be opaque, inefﬁ-
cient, ad-hoc, and contain many hidden hard-coded links. Repeating such an analysis
N. Taft and F. Ricciato (Eds.): PAM 2012, LNCS 7192, pp. 53–63, 2012.
c(cid:2) Springer-Verlag Berlin Heidelberg 2012
54
L. Golab et al.
after a hiatus of a year generally ranges from being difﬁcult to untenable. Since the
data are processed outside the warehouse, the complexity of these scripts also increases
as data structures and associated logic have to be created for handling data manage-
ment functions, duplicating what a warehouse can already natively support. Examples
include updating relevant data structures and indices with new data, and propagating
the impact of new data on any dependent processes.
In this paper, we propose a new approach that deploys real-time network monitoring
applications directly within the warehouse; note that our notion of real-time refers to
processing and reacting to new data as they arrive. The idea is to translate the data
processing steps of the application into declarative queries whose results are continually
maintained as database tables. This approach can simplify application development and
management, and improve performance and availability (more details in Section 3).
The ﬁrst challenge in realizing this idea is to provide similar performance to an opti-
mized external program. However, while databases can save the results of queries into
temporary tables, these tables are static, or, at best, they can only be updated periodi-
cally. This is suitable only for off-line operations that take a complete data set as input
and return a transformed data set as output. On the other hand, real-time applications
operate in stream-in-stream-out mode, incrementally processing new data and produc-
ing new results. As we will show, recent advances in DataDepot make it possible to load
new data as they arrive and continuously propagate changes throughout the warehouse.
The second issue is to provide a query language for expressing network monitor-
ing applications. The standard SQL database language is geared towards set-oriented
data transformations. However, network monitoring is mostly sequence-oriented. For
example, we may want to notify users if some metric increases by at least a certain
amount relative to its previous value, or if it stays above a critical threshold for sev-
eral consecutive time periods. In this paper, we propose, and experimentally evaluate
within a production stream warehouse, a novel extension of SQL to natively support
sequence-oriented applications at scale.
While there exist related work on various components of our solution such as stream
warehousing and sequence processing, this work is the ﬁrst to put these together into a
large-scale production system that can handle the data rates and latency requirements
of real-time network monitoring. To summarize our goals and contributions, 1) we aim
to inform the networking community of a new way to implement network monitoring
applications that exploits the state-of-the-art in data management technologies, 2) we
propose a novel extension of SQL that makes it easier to express and optimize sequence-
oriented applications, and 3) as proof of concept, we implement the proposed extensions
in DataDepot and build an operational network monitoring application. The application
produces real-time alerts when various performance metrics such as end-to-end packet
loss exceed speciﬁed thresholds for sustained periods of time. We demonstrate how
the declarative sequence-oriented language simpliﬁes application development, and we
experimentally show a threefold performance improvement as compared to expressing
the application logic using the existing set-oriented version of DataDepot.
This work is the outcome of an inter-disciplinary collaboration between networking
and database researchers to develop easy-to-manage, scalable, data intensive network
monitoring applications. We hope that the ideas and beneﬁts demonstrated in this paper
A Sequence-Oriented Stream Warehouse Paradigm
55
will encourage researchers and practitioners to explore and exploit emerging database
technologies in network management.
2 Background and Related Work
A conventional data warehouse provides extensive data storage and analytics facilities,
along with the ability to manage deeply nested levels of pre-computed query results
(termed materialized views). However, view updates are normally performed in a large
batch. This enables many optimizations but does not provide the data freshness needed
by network monitoring applications. By contrast, a Data Stream Management System
(DSMS) operates on data on-the-ﬂy, providing fast response; see, e.g., [4]. However, a
DSMS is designed to operate in-memory, and stores only a short segment of the most
recent data. Streaming extensions of SQL have also been proposed [10], but they focus
on set operations over windows of recent history rather than sequential operations.
DataDepot [8] is a stream warehouse that combines aspects of a DSMS and con-
ventional data warehouse by providing very long term storage as well as continuous
updates of materialized views. Other stream warehouses include Moirae [3], Tele-
graphCQ/FastBit [18], and Everest [2]. However, none of these can natively support
sequence operators or real-time view updates; Everest enables ad-hoc real-time analy-
sis by loading raw data in real-time, but re-computes views periodically, while Moirae
and FastBit focus on matching new data against similar historical data. Furthermore,
there has been work on sequential operators in the context of event processing (see,
e.g., [1]), but it focuses on complex pattern matching rather than guaranteeing high
performance.
The data ﬂow through a stream warehouse begins with raw data, such as SNMP
polls, trafﬁc summaries or system logs, which are loaded into a ﬁrst level of material-
ized views often called base tables. Updates to the base tables then propagate through
the remaining materialized views often called derived tables. Since views can store ter-
abytes of data spanning years, an efﬁcient mechanism for updating them is a critical
component of a stream warehouse. A common mechanism is to temporally partition
each view according to a record timestamp. Then, only those partitions which are af-
fected by new data need to be updated. In Figure 1, we illustrate the propagation of new
data from a raw source, to a base table, then through two levels of dependent (derived)
views. The base and dependent tables are all temporally partitioned by some criteria.
We have also determined the ﬂow of data from a source partition to a dependent parti-
tion - see [8] for a discussion of how to infer these data ﬂows. New records have arrived
in the raw data source, which, when processed, only affect two partitions in the base
table (the most recent data, and some late in-the-past data). The two updated partitions
in the base table trigger updates only to two partitions in the ﬁrst materialized view, and
transitively to two partitions in the second materialized view.
We have developed a number of features for DataDepot which facilitate near real-
time data loading:
– We want to update views frequently, but we do not want queries against the views
to be “blocked” by updates. We added an inexpensive Multi-Version Concurrency
Control that allows tables to be updated and queried at the same time [17].
56
L. Golab et al.
Base table
Dependent
Table
Dependent
Table
Fig. 1. Update propagation from raw sources to materialized views
– We enabled multi-granularity partitions: new data are stored in small, e.g., 5-
minute, partitions, while historical data are stored in larger, e.g., 1-day, partitions
(unlike RRDTool [15], historical data are not aggregated, only grouped into larger
partitions). Since updates typically affect recent partitions, small partitions for re-
cent data reduce the amount of data to bring into memory for updates. Large parti-
tions for historical data allow us to store enough history for off-line data mining.
– We developed a real-time update scheduler that executes updates when new data
arrive, scales to hundreds of tables and views with various priorities, and is tuned
for minimizing the tardiness of updates to critical tables [9].
3 Implementing Applications Inside a Data Stream Warehouse
The approach we advocate in this paper is to translate the data processing steps of a real-
time application into declarative queries, whose results are maintained in a hierarchy of
materialized views. The view hierarchy encodes data ﬂow through the application, and
each step amounts to a query against the results of the previous step. When new data
arrive, they are incrementally processed by each step.
There are many advantages of shifting complex application logic to the database.
First, the application designer can rely on the warehouse to efﬁciently propagate new
data through the data ﬂow. Second, the warehouse provides a declarative query language
and a suite of efﬁcient query evaluation techniques, including indices on materialized
views. Third, results are stored in the database and can be reused and re-processed by
other applications. Fourth, the code (queries and partition dependencies) is often more
compact, and easier to document, debug and extend than a procedural program.
There are two technical issues in applying the proposed approach to real-time ap-
plications: system support for efﬁcient view updates, which we have described in the
previous section, and language support for sequence operators, which we propose next.
Relational database systems, including DataDepot, use the well-known SQL lan-
guage, in which simple queries are expressed as “SELECT f FROM t WHERE p
GROUP BY g HAVING h”. Here, f is the list of ﬁelds and/or aggregate functions
to return, t is a list of tables referenced in the query, p is a predicate that must be satis-
ﬁed by each record in the output, g is a list of ﬁelds on which to group the data, such that
A Sequence-Oriented Stream Warehouse Paradigm
57
the aggregate function in f , if any, will be evaluated separately for each group, and h is
a predicate on each group deﬁned in g that governs which groups will be reported in the
result. Multiple tables and/or views can be joined by listing them in t and specifying the
joining condition in p. In addition to ﬁltering, merging, joining and aggregating data,
SQL includes set operations of the form “SELECT ... WHERE EXISTS (SELECT ...)”
or “SELECT ... WHERE NOT EXISTS (SELECT ...)”. These return a result only if the
result of the query inside the brackets is non-empty or empty, respectively.
Network monitoring applications are often sequence-oriented: comparing current
timestamps or sequence numbers to their previous values to compute delays or out-
of-order packets, ﬁnding consecutive records that exceed a threshold, grouping records
with nearby timestamps into ﬂows or sessions, etc. The basic operations required by
such queries — e.g., referring to the previous record in the sequence, maintaining a state
machine over the data that have arrived so far — are difﬁcult to express using standard
SQL and inefﬁcient to execute using standard database algorithms. For example, sup-
pose that we want to remove records from a message stream S that have another record
from the same device name within sixty seconds (in order to output only one record per
“session”). One way to express this using SQL is with a nested set operator:
SELECT name, timestamp FROM S
WHERE NOT EXISTS(
SELECT S2.name, S2.timestamp FROM S AS S2
WHERE S2.name=name
AND timestamp >= S2.timestamp
AND timestamp 60 ? timestamp : last)
INITIALLY timestamp AS last
HAVING timestamp = last
The GROUP BY clause deﬁnes the entities being tracked, with the SEQ keyword denot-
ing an ordered ﬁeld (timestamp, sequence number, IP address, etc.). Here, grouping by
device name and timestamp indicates that each device produces a sequence of records.
The state maintained for each group is deﬁned in the UPDATE BY clause. Each entry
deﬁnes a variable; here, there is one variable named last. Its initial value is deﬁned
to be the timestamp (of the ﬁrst record), and, as speciﬁed in the conditional evaluation
expression, it is advanced to the next timestamp value whenever the difference between
58
L. Golab et al.
the timestamp and the previous value of last (denoted last[1]) is greater than 60
seconds. The (optional) HAVING clause determines whether a record should be output
for a given input record. In this query, we want to produce an output record whenever
last is advanced to the value of the current timestamp.
We argue that deﬁning state variables can express common network monitoring ap-
plications that operate on data sequences. With the above SQL extension, we no longer
have to simulate sequential operations using set expressions. It is much easier to write
sequence queries and efﬁciently execute them instead of relying on general-purpose
set-oriented optimizations provided by the database. In particular, as proof of concept,
we have implemented the following execution strategy in DataDepot (which may be in-
corporated into any SQL-based relational database system). First, we scan the table(s)