### Ground Truth Data
- **# of Int. Emails (Benign)**: 5,509
- **# of Users**: 16,310

### Email Dataset Statistics
- **Internal Emails (Benign)**: 587,873
- **External Benign Emails**: 1,546
- **External Unwanted Emails**: 15,399
- **Unwanted Emails**: 3,963

**Table 3: Statistical Information of Email Dataset (Int. for Internal, Ext. for External)**

### Communication Graphs
- **GSR**
  - **# of Nodes**: 1,678
  - **# of Edges**: 17,082
  - **Avg. Node Degree**: 20.2598
- **GCR**
  - **# of Nodes**: 1,722
  - **# of Edges**: 42,318
  - **Avg. Node Degree**: 49.1498

**Table 4: Statistical Information of Communication Graphs**

### Communication Graph Characteristics
The key statistics of GSR and GCR are shown in Table 4. The degree distributions of GSR and GCR, as depicted in Figure 5, confirm our assumption that both graphs exhibit social graph characteristics. The node degree distributions of both graphs closely follow a power-law distribution, which is typical of social graphs.

### Community Analysis
We use Infomap [41], an algorithm for community discovery on social graphs, to analyze the communities in GSR and GCR. Infomap identified 209 communities in GSR and 87 communities in GCR. We then counted the number of communities covered by the recipients for each unwanted email. Our analysis shows that most recipients of benign emails cover fewer than 4 communities in both GSR and GCR. In contrast, recipients of unwanted emails tend to cover more communities. For example, unwanted emails with 4 or 5 recipients are, on average, delivered to 3.125 communities, while similar benign emails are delivered to 1.84 communities. This supports our intuition that unwanted emails are sent to multiple user communities, whereas benign ones are sent to fewer communities.

Additionally, we measured the variations in the importance of co-recipients in benign and unwanted emails. On average, the standard deviation of co-recipient importance in unwanted emails is 3.72 times higher than that in benign emails. This observation reinforces our intuition that emails addressing recipients of significantly different importance are more likely to be unwanted.

### Selection of Machine Learning Models
- **SVM**
  - **Accuracy (Acc.)**: 0.968
  - **True Positive Rate (TPR)**: 0.969
  - **False Positive Rate (FPR)**: 0.025
- **Logistic Regression**
  - **Accuracy (Acc.)**: 0.968
  - **True Positive Rate (TPR)**: 0.925
  - **False Positive Rate (FPR)**: 0.034
- **Random Forest**
  - **Accuracy (Acc.)**: 0.974
  - **True Positive Rate (TPR)**: 0.952
  - **False Positive Rate (FPR)**: 0.003
- **Neural Networks**
  - **Accuracy (Acc.)**: 0.957
  - **True Positive Rate (TPR)**: 0.925
  - **False Positive Rate (FPR)**: 0.011

**Table 5: Performance of Four Machine Learning Models**

### Setup of Training and Testing Data
We temporally split the 25-day ground truth data into training and testing datasets. The training dataset consists of the first 18 days, while the testing dataset consists of the last 7 days. The training dataset includes 587,873 internal emails and 11,603 external emails (11,084 benign and 519 unwanted). The testing dataset includes 4,707 external emails (4,315 benign and 392 unwanted). At the collection point of the internal emails, emails for mailing lists are already expanded to individual emails, and mailing list relationships are indirectly captured by enterprise graph features. Note that the internal mail server is configured such that external users are not allowed to send emails to enterprise mailing lists. The training and testing data have similar distributions of benign and unwanted emails (91.67% benign in the training data and 95.52% benign in the testing data).

### Machine Learning Models
We experimented with four different machine learning models: SVM, Logistic Regression, Random Forest, and fully connected Deep Neural Networks. The performance results with a balanced dataset (with a discriminate threshold of 0.5) are depicted in Table 5. The results show that Random Forest has the best performance among all the four models and is selected as the classifier for our approach in all experiments. To avoid overfitting in the Random Forest model, we generate 500 decision trees with features randomly picked with replacement on each tree and limit the decision tree depth to 20 [12].

### Baseline Approach
We use a Random Forest classifier that uses all the header features (Table 10 in Appendix) as a baseline. This corresponds to the approach that applies existing content-based approaches over E2EE. Clearly, all content-based features cannot be utilized, yet all the header features would still be available. We experimentally evaluate how this approach compares with our approach and what benefits enterprise features bring.

### Feature Importance
We divide all the features into four categories:
1. **Sender Profiling Features** (Table 1)
2. **Enterprise Graph Features** (Table 2)
3. **Non-Subject Features**: Header features from SpamAssassin [2] (excluding content and subject features)
4. **Subject Features**: Header features from SpamAssassin including subject features (excluding only content features)

We train and test the classifier using 5-fold cross-validation with balanced datasets and calculate the average importance [21] of each feature. Figure 6 depicts the importance scores of the top-30 important features. All the top 4 features are enterprise features. Additionally, one of the graph enterprise features is the third most important feature, whose score is significantly higher than the next more important one. This clearly demonstrates the important role that enterprise features play in the detection of unwanted emails.

**Table 6: Top-3 Important Features in Each Category**

- **Sender Profiling Features**
  - **SENDER_SIM_FIELDS**: Rank 1, Score 0.173
  - **SENDER_EMAIL_SUBNET_FREQUENCY**: Rank 4, Score 0.054
  - **SENDER_SIM_UA**: Rank 10, Score 0.024
- **Enterprise Graph Features**
  - **CR_RANDOMWALK**: Rank 3, Score 0.122
  - **SR_RANDOMWALK**: Rank 13, Score 0.016
  - **CR_TRANSCLOSURE**: Rank 15, Score 0.014
- **Non-Subject Features**
  - **NS_FROM_MIXED**: Rank 2, Score 0.144
  - **NS_DATE_INVALID**: Rank 9, Score 0.026
  - **NS_TO_SORTED**: Rank 14, Score 0.015
- **Subject Features**
  - **SUBJ_CAPS_PERCENTAGE**: Rank 5, Score 0.038
  - **SUBJ_FREE**: Rank 6, Score 0.034
  - **SUBJ_GUARANTEED**: Rank 7, Score 0.030

In the non-subject category, NS_FROM_MIXED is the second-most important feature, revealing that a considerable proportion of unwanted email senders use automatically generated email addresses with numbers appended to compose new addresses. The NS_DATE_INVALID and NS_TO_SORTED features are crafted to detect inconsistencies among the format of the sending timestamp and the sorting of multiple recipients in the "to" field. However, it is important to note that these features are not robust, as a careful adversary can easily avoid such inconsistencies to evade detection.

Subject features are also highly ranked. SUBJ_CAPS_PERCENTAGE, SUBJ_FREE, and SUBJ_GUARANTEED are ranked 5, 6, and 7, respectively. These features indicate that unwanted emails tend to use capital letters and include words like "guaranteed" and "free" in email subjects to draw attention. As we will see in Section 6, the high ranks of these features explain the sharp drop in performance for classifiers that rely solely on header features when subject features become unavailable under E2EE settings.

### Prediction Accuracy
We experimentally evaluate the prediction accuracy of our proposed classifiers. Most end-to-end email encryption solutions by default encrypt email contents but leave email headers and subjects unencrypted [8]. We call this setting the regular setting. Some commercial solutions also offer capabilities to encrypt or strip certain fields in headers for better privacy. One obvious option is to encrypt email subjects, which many existing solutions support [29, 30, 37]. Sometimes, the IPs and timestamps of email servers involved in email delivery are also encrypted or stripped (e.g., ProtonMail and Tutanota). In this experiment, we also consider this enhanced setting, assuming subjects and IPs and timestamps of email delivery paths are encrypted. Clearly, in the enhanced setting, all the subject features cannot be derived. Further, some of the sender profiling features (No. 6 and 16 in Table 1) could not be built either. We evaluate the prediction accuracy of our classifiers under both settings.

For each setting, we train three different Random Forest classifiers:
- **Enterprise Classifier**: Trained with enterprise social graph features and sender profiling features alone. In the enhanced setting, all features related to IP addresses (namely No. 6 and 16 in Table 1) are disabled.
- **Header Classifier**: Trained with all the header features alone and considered as the baseline. In the enhanced setting, all features in the subject category in Table 10 are disabled.
- **All Classifier**: Trained with all the above features.

For each setting, two evaluation metrics are computed:
- **True Positive Rate (TPR)**: Represents the ratio of correctly identified unwanted emails to the total number of unwanted emails.
- **False Positive Rate (FPR)**: Represents the total number of benign emails mistakenly identified as unwanted to the total number of benign emails.

The relationship between TPR and FPR for various discriminate thresholds is tracked using ROC curves. The area under the ROC (AUC) is an indicator of the quality of the classifier; the higher the AUC, the better the performance.

**Regular Setting**
Figure 7 (a) shows the zoomed-in version of the ROC curves of the different classifiers in the regular setting. Each ROC curve is generated by varying the discriminate threshold on the corresponding classifier output, with each discriminate threshold averaged over 100 runs. The figure shows relatively poor performance of the Header baseline classifier (AUC = 0.930). For example, when the FPR = 0.6%, the TPR = 78.1%. On the other hand, the performance of the Enterprise classifier is much better (AUC = 0.984). For example, when the FPR = 0.6%, the TPR = 87.6%. This clearly shows the effectiveness of enterprise features in detecting unwanted emails. When header features are integrated with enterprise features, the performance is further enhanced, as shown by the ROC curve of the All classifier (AUC = 0.984). For example, the TPR = 96.7% when the FPR = 0.6%.

**Enhanced Setting**
The prediction accuracy of the three classifiers under the enhanced setting is shown in Figure 7(b). The figure shows that the Header classifier has a significant performance drop due to the masked subject features (AUC = 0.910). For example, when the FPR = 0.6%, the TPR = 5.3%; and to achieve TPR of 95.1%, the FPR soars to 13.2%. This indicates that the prediction accuracy of the Header classifier heavily relies on the subject features, which are unavailable in the enhanced setting.