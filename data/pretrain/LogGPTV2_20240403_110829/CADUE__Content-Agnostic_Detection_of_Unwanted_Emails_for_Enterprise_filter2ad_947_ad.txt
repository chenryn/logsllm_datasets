(Ground Truth)
16310
5,509
# of Int. Emails (Benign)
# of Users
587,873
Internal
1,546
External Benign
15,399
3,963
Unwanted
911
Table 3: Statistical information of email dataset (Int. for in-
ternal, Ext. for external)
# of nodes
1,678
1,722
GSR
GCR
# of edges Avg. node degree
17,082
42,318
20.2598
49.1498
Table 4: Statistical Information of Communication Graphs
212CADUE: Content-Agnostic Detection of Unwanted Emails for Enterprise Security
RAID ’21, October 6–8, 2021, San Sebastian, Spain
Communication graph characteristics. The key statistics of
GSR and GCR is shown in Table 4. of each node The degree distri-
butions of GSR or GCR are shown in Figure 5, which confirms our
assumption that both GSR or GCR exhibit social graph character-
istics. The node degree distributions of both graphs are close to
power-law, which characterizes social graphs.
Community analysis. We use Infomap [41], an algorithm for
community discovery on social graphs, to analyze the communities
in GSR or GCR. Infomap discovered 209 and 87 communities in GSR
or GCR respectively. Then we count the number of communities
covered by the recipients for each unwanted email. Our analysis
shows that most of the recipients of benign emails cover less than
4 communities in both GSR or GCR. Meanwhile, we observe that
the recipients of unwanted emails tend to cover more communities
than those of benign ones in both graphs. We observe from our
dataset that unwanted emails with 4 or 5 receipts are on average
delivered to 3.125 communities whereas the emails with similar
number of recipients are delivered to 1.84 communities. This sup-
ports our intuition that unwanted emails are sent to multiple user
communities whereas benign ones are sent to a fewer communities.
With the above sample, we also measure the variations in the im-
portance of the co-recipients in benign and unwanted emails. We
observe that, on average, the standard deviation of the co-recipient
importance in unwanted emails is 3.72 times higher than that in
benign emails. This observation reinforces our intuition that the
emails addressing recipients of significantly different importance
are more likely to be unwanted.
6.2 Selection of Machine Learning Models
Model
SVM
Logistic Regression
Random Forest
Neural Networks
Acc.
0.968
0.968
0.974
0.957
TPR
0.969
0.925
0.952
0.925
FPR
0.025
0.034
0.003
0.011
Table 5: Performance of four machine learning models
Setup of training and testing data. We temporally split the 25-
day ground truth data into training and testing datasets. The train-
ing dataset consists of the first 18 days, while the testing data set
consists of the last 7 days. The training dataset includes 587,873
internal emails and 11,603 external emails (11,084 benign and 519
unwanted). The testing dataset includes 4,707 external emails (4,315
benign and 392 unwanted). At the collection point of the internal
emails, emails for mailing lists are already exploded to individual
emails and mailing list relationships are indirectly captured by en-
terprise graph features. Note that internal mail server is configured
such that external users are not allowed to send emails to enterprise
mailing lists. The training and testing data have similar distribu-
tions of benign and unwanted emails (91.67% benign email in the
training data and 95.52% benign emails in the testing data).
Machine learning models. We experimented with four different
machine learning models: SVM, Logistic Regression, Random For-
est and fully connected Deep Neural Networks. The performance
results with a balanced dataset (with discriminate threshold 0.5) are
depicted in Table 5. The results show that Random Forest has the
best performance among all the four models, and hence is selected
to be the classifier for our approach in all experiments. To avoid
overfitting in the Random Forest model, we generate 500 decision
trees with features randomly picked with replacement on each tree,
and limited the decision tree depth to 20 [12].
Baseline approach. We use as a baseline a Random Forest classi-
fier that uses all the header features (Table 10 in Appendix). This is
corresponding to the approach that applies existing content-based
approaches over E2EE. Clearly all content-based features could
not be utilized, yet all the header features would still be available.
We experimentally evaluate how this approach compares with our
approach and what benefits enterprise features bring.
6.3 Feature Importance
We divide all the features into four categories: (1) Sender profiling
features (Table 1); (2) Enterprise graph features (Table 2); (3) Non-
subject features: the header features from SpamAssassin [2] (with
both content and subject features excluded); and (4) Subject features:
the header features from SpamAssassin including subject features
(with only content features excluded). We train and test the classifier
using 5-fold cross-validation with balanced datasets, and calculate
the average importance [21] of each feature. Figure 6 depicts the
importance scores of the top-30 important features. All the top
4 features are enterprise features. Additionally, one of the graph
enterprise features is the third most important feature, whose score
is significantly higher than the next more important one. This
clearly demonstrates the important role that enterprise features
play in the detection of unwanted emails.
The rank and score of the top-3 features per feature category is
shown in Table 6. In the category of sender profiling features, the
SENDER_SIM_FIELDS feature is the most important. This reveals
that the structural composition of email headers is very effective in
distinguishing benign and unwanted emails. The second important
feature, namely SENDER_EMAIL_SUBNET_FREQUENCY feature indi-
cates that adversaries tend to use diverse infrastructures to deliver
unwanted emails. Such agile tactics are usually used to evade detec-
tion and blocking based on infrastructure reputation [20]. Finally,
the high rank of the SENDER_SIM_UA feature indicates that the un-
wanted emails are likely to utilize similar email agents.
In the category of enterprise graph features, the top-3 features
(CR_RANDOMWALK, SR_RANDOMWALK, and CR_TRANSCLOSURE) are all
among the top-15 most important features, with the first being
among the top 3. This shows that enterprise graph features are
important for the detection of unwanted emails. We note that two
of the features are generated from the co-recipient graph. This indi-
cates that the co-recipient relationship is more important than the
sender-recipient relationship for the detection of unwanted emails.
We further take a closer look at the performance of the six en-
terprise graph features. Among these six enterprise graph features,
CR_RANDOMWALK has the highest importance score (0.1223), and
CR_PAGERANK has the lowest score (0.0019), which is much lower
than that of CR_RANDOMWALK. We also group the six features by
the algorithms (RW, TC, PR) that they are generated from, and
compute the average importance score of features in each group. It
turns out that the features generated by RW have the highest aver-
age importance score (0.0691). TC ranks the second (average score
0.0069), and PR is the last (average score 0.0019). This demonstrates
that the community-based features are more important than the
employee-based features.
213RAID ’21, October 6–8, 2021, San Sebastian, Spain
Mohamed Nabeel, Enes Altinisik, Haipei Sun, Issa Khalil, Hui (Wendy) Wang, and Ting Yu
Figure 6: Importance of top-30 features
Category
Sender Profiling
Sender Profiling
Sender Profiling
Category
Enterprise Graph
Enterprise Graph
Enterprise Graph
Name
SENDER_SIM_FIELDS
SENDER_EMAIL_SUBNET_FREQUENCY
SENDER_SIM_UA
Name
CR_RANDOMWALK
SR_RANDOMWALK
CR_TRANSCLOSURE
Ranking
1
4
10
Ranking
3
13
15
Score
0.173
0.054
0.024
Score
0.122
0.016
0.014
Category
Non-Subject
Non-Subject
Non-Subject
Category
Subject
Subject
Subject
Name
NS_FROM_MIXED
NS_DATE_INVALID
NS_TO_SORTED
Name
SUBJ_CAPS_PERCENTAGE
SUBJ_FREE
SUBJ_GUARANTEED
Ranking
2
9
14
Ranking
5
6
7
Score
0.144
0.026
0.015
Score
0.038
0.034
0.030
Table 6: Top-3 important features in each category
Recall that all the features in thenon-subject and subject cat-
egories are adapted from those of the popular SpamAssassin [2]
email filtering system and a previous work [28]. In the non-subject
category, NS_FROM_MIXED is the second-most important feature,
which reveals that a considerable proportion of unwanted email
senders use automatically generated email addresses with numbers
appended to compose new addresses. The NS_DATE_INVALID and
NS_TO_SORTED features are crafted to detect inconsistencies among
the format of the sending timestamp and the sorting of multiple
recipients in the to field. However, it is important to note here that
these features are not robust as a careful adversary can easily avoid
such inconsistencies to evade detection.
Subject features are also highly ranked. SUBJ_CAPS_PERCENTAGE,
SUBJ_FREE, and SUBJ_GUARANTEED are ranked 5, 6, and 7, respec-
tively. The features indicate that the unwanted emails tend to use
capital letters, use the words "guaranteed" and "free" in email sub-
jects to draw attention. As we will see in Section 6, the high ranks
of these features explain the sharp drop in performance for the
classifiers that solely rely on header features, when subject features
become unavailable under E2EE settings.
6.4 Prediction Accuracy
We experimentally evaluate the prediction accuracy of our proposed
classifiers. As mentioned earlier, most end-to-end email encryption
solutions by default encrypt email contents but leave email headers
and subjects unencrypted [8]. We call this setting the regular setting.
Some commercial solutions also offer capabilities to encrypt or strip
certain fields in headers for better privacy. One obvious option is to
encrypt email subjects, which many existing solutions support [29,
30, 37]. Sometimes the IPs and timestamps of email servers involved
in email delivery are also encrypted or stripped (e.g., ProtonMail
and Tutanota). In this experiment, we also consider this enhanced
setting, in particular, assuming subjects and IPs and timestamps
of email delivery paths are encrypted. Clearly, in the enhanced
setting, all the subject features cannot be derived. Further, some of
the sender profiling features (No.6 and 16 in Table 1) could not be
built either. We evaluate the prediction accuracy of our classifiers
under both settings.
Specifically, for each setting, we train 3 different Random Forest
classifiers, namely:
• Enterprise classifier, which is trained with enterprise social
graph features and sender profiling features alone. Note
that in the enhanced setting, all the features related to IP
addresses (namely No. 6 and 16 in Table 1) are disabled;
• Header classifier, which is trained with all the header fea-
tures alone and considered as the baseline. In the enhanced
setting, all features in subject category in Table 10 are dis-
abled;
• All classifier, which is trained with all the above features.
For each setting, two evaluation metrics are computed: (i) the
true positive rate (TPR), which represents the ratio of correctly
identified unwanted emails to the total number of unwanted emails,
and (ii) the false positive rate (FPR), which represents the total
number of benign emails mistakenly identified as unwanted to
the total number of benign emails. The relationship between TPR
and FPR for various discriminate thresholds is tracked using ROC
curves. The area under the ROC (AUC) is an indicator of the quality
of the classifier; the higher the AUC, the better the performance.
Regular setting. Figure 7 (a) shows the zoomed-in version of
the ROC curves of the different classifiers in the regular setting.
Each ROC curve is generated by varying the discriminate thresh-
old on the corresponding classifier output, with each discriminate
threshold averaged over 100 runs. The figure shows relatively poor
performance of the Header baseline classifier (AUC = 0.930). For
example, when the FPR = 0.6%, the TPR = 78.1%. On the other hand,
the performance of the Enterprise classifier is much better than
that of the Header classifier (AUC = 0.984). For example, when
the FPR = 0.6%, the TPR = 87.6%. This clearly shows the effective-
ness of enterprise features in detecting unwanted emails. When
the header features are integrated with the enterprise features, the
performance is further enhanced as shown by the ROC curve of the
214CADUE: Content-Agnostic Detection of Unwanted Emails for Enterprise Security
RAID ’21, October 6–8, 2021, San Sebastian, Spain
(a) ROC curves for regular setting
(b) ROC curves for enhanced setting
Figure 7: Prediction Accuracy of CADUE
(c) All regular ROC curves
for different epochs
All classifier (AUC = 0.984). For example, the TPR = 96.7% when
the FPR = 0.6%.
Enhanced setting. The prediction accuracy of the three classi-
fiers under the enhanced setting is shown in Figure 7(b). The figure
shows that the Header classifier has a significant performance
drop due to the masked subject features (AUC 0.910). For example,
when the FPR = 0.6%, the TPR = 5.3 %; and to achieve TPR of 95.1%,
the FPR soars to 13.2%. This indicates that the prediction accuracy
of the Header classifier heavily relies on the subject features which