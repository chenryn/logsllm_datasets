#### 列采样
 列抽样技术：一种是按层随机，另一种是按树随机（构建树前就随机选择特征）。
- 按层随机方式，在每次分裂一个结点的时候，对同一层内的每个结点分裂之前，先随机选择一部分特征，这时候只需要遍历一部分特征，来确定最后分割点。
- 按树随机方式，即构建树结构前就随机选择特征，之后所有叶子结点的分裂都只使用这部分特征
### 系统设计
在构建树的过程中，最耗时是找最优的切分点，而这个过程中，最耗时的部分是将数据排序。为了减少排序的时间，提出了Block结构存储数据。
- Block中数据以稀疏格式CSC进行存储
- Block中的特征进行排序（不对缺失值排序）
- Block中特征还需存储指向样本的索引，这样才能根据特征的值来取梯度
- 一个Block中存储一个或多个特征的值
![image-20200428200820520](images/image-20200428200820520.png)
使用Block结构的缺点是获取梯度的时候，是通过索引来获取的，而这些梯度的获取顺序是按照特征的大小顺序的，这将导致非连续的内存访问，可能使得CPU cache缓存命中率低，从而影响算法效率。
![image-20200428201054648](images/image-20200428201054648.png)
对于精确算法中，使用缓存预取，具体来说，对每个线程分配一个连续的Buffer，读取梯度信息并存入Buffer中，这样就实现了非连续到连续的转换，然后在统计梯度信息，该方式在训练样本数大的时候特别有用
![image-20200428201242146](images/image-20200428201242146.png)
当数据量太大不能全部放入主内存的时候，为了使得out-of-core计算成为可能，将数据划分为多个Block并存放在磁盘上。
- 计算的时候，使用独立的线程预先将Block放入主内存，因此可以在计算的同时读取磁盘
- Block压缩，貌似采用的是近些年性能比较出色的LZ4压缩算法，按列进行压缩，读取的时候用另外的线程解压。对于行索引，只保存第一个索引值，然后用16位的整数保存与该Block第一个索引的差值。
- Block Sharding，将数据划分到不同的硬盘上，提高磁盘吞吐量。
### 实践
安装XGBoost的工具，使用skLearn中的鸢尾花和波士顿放假的数据，完成分类和回归的演示。
## LightGBM
LightGBM也是属于GBDT算法，是一款常见的GBDT的工具包，由微软亚洲研究院开发，速度比XGBoost快，精度也还可以，它的设计理念是
- 单个机器在不牺牲速度的情况下，尽可能使用更多的数据
- 多机并行的时候，通信的代价尽可能地低，并且在计算上可以做到线性加速
所以其使用分布式的GBDT，选择了基于直方图的决策树算法。
![image-20200428211350887](images/image-20200428211350887.png)
LightGBM 和 XGBoost的对比如下
![image-20200428211725599](images/image-20200428211725599.png)
### 直方图算法
XGBoost中的Exact Greedy（贪心）算法：
- 对每个特征都按照特征值进行排序
- 在每个排好序的特征都寻找最优切分点
- 用最优切分点进行切分
优点是比较精确，缺点是空间消耗比较大，时间开销大和对内存不友好，使用直方图算法进行划分点的查找可以克服这些缺点。
![image-20200428212334135](images/image-20200428212334135.png)
直方图算法 （Histogram algorithm）把连续的浮点特征值离散化为K个整数（也就是分桶bins的思想），比如【0，0.1】 -> 0，【0.1, 0.3】-> 1。并根据特征所在的bin对其进行梯度累加和个数统计，然后根据直方图，寻找最优的切分点
![image-20200428212605067](images/image-20200428212605067.png)
直方图构建算法：
![image-20200428212801755](images/image-20200428212801755.png)
#### 如何分桶
数值型特征和类别特征采用的方法是不同的
**数值型特征**
- 对特征值去重后进行排序（从大到小），并统计每个值的counts
- 取max_bin和distinct_value.size()中较小的值作为bins_num
- 计算bins中的平均样本个数 mean_bin_size，若某个distinct_value的count大于mean_bin_size，则该特征作为bins的上界，小于该特征值的第一个distinct value作为下界，若某个distinct_value的count小于mean_bin_size，则要进行累计后在分组。
 **类别型特征**
- 首先对特征取值按出现的次数排序（大到小）
- 取前min（max_bin，distinct_values_int.size() ）中 的每个特征做第3步，忽略一些出现次数很少的特征取值
- 用bin_2_categorical_bin_2_categorical_（vector类型）和categorical_2_bin_categorical_2_bin 将特征取值和bin一一对应起来，这样就可以方便进行两者之间的切换了。
#### 构建直方图
给定一个特征值，我们可以转换为对应的bin了，就要构建直方图了，直方图累加了一阶梯度和二阶梯度，并统计了取值的个数
![image-20200428215026626](images/image-20200428215026626.png)
#### 直方图作差
一个叶子节点的直方图可以由它的父亲节点的直方图与其兄弟节点的直方图做差得到，使用这个方法，构建完一个叶子节点的直方图后，就可以用较小的代价得到兄弟节点的直方图，相当于速度提升了一倍
![image-20200428215203659](images/image-20200428215203659.png)
#### 寻找最优的切分点
最优的切分点
- 遍历所有的bin
- 以当前的bin作为分割点，累加左边的bins至当前bin的地图和Sl及样本数量nl，并利用直方图做差求得右边的梯度和样本数量
- 带入公司计算增益loss
- 在遍历过程中取得最大的增益，以此时的特征和bin的特征值作为分裂节点的特征及取值。
![image-20200428215608063](images/image-20200428215608063.png)
#### 直方图算法的优点
- 减少内存占用
- 缓存命中率提高，直方图中梯度的存放是连续的
- 计算效率提高，相对于XGBoost中预排序每个特征遍历数据，复杂度为O(Feature * data)，而直方图算法之需要遍历每个特征的直方图即可，复杂度为O(Feature * bins)
- 在进行数据并行时，可大幅度降低通信代价
### 直方图算法的改进
直方图算法仍有优化的空间，建立直方图的复杂度为O(feature * data)，如果能降低特征数或者降低样本数，训练的时间会大大减少，加入特征存在冗余时，可以使用PCA等算法降维，但特征通常是精心设计的，去除他们中的任何一个可能会影响训练精度，因此LightGBM提出了GOSS算法和EFB算法。
### GOSS算法
样本的梯度越小，样本的训练误差就越小，表示样本已训练的很好，直接的做法就是丢掉这部分样本，然而丢掉会影响数据的分布，因此在LightGBM中采取了one-side采样的方式来适配，GOSS采样策略，它保留所有的大梯度样本，对小梯度样本进行随机采样。
![image-20200428222041079](images/image-20200428222041079.png)
原始直方图算法下，在第j个特征，值为d处进行分裂带来的增益可以定义为：
$$
\begin{aligned}
V_{j | O}(d) &=\frac{1}{n_{O}}\left(\frac{\left(\sum_{x_{i} \in A_{l}} g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B l} g_{i}\right)^{2}}{n_{l}^{j}(d)}+\frac{\left(\sum_{x_{i} \in A_{r}} g_{i}+\frac{1-a}{b} \sum_{x_{i} \in B_{l}} g_{r}\right)^{2}}{n_{r}^{j}(d)}\right) \\
\text { 其中 } A_{l} &=x_{i} \in A: x_{i j} \leq d, A_{r}=x_{i} \in A: x_{i j}>d \\
B_{l}=x_{i} & \in B: x_{i j} \leq d, B_{r}=x_{i} \in B: x_{i j}>d
\end{aligned}
$$
注意：A表示大梯度样本集，而B表示小梯度样本中随机采样的结果。
### EFB算法
高维数据通常是稀疏的，而且许多特征是互斥的，即两个或多个特征列不会同时为非0，LightGBM根据这一特点提出了EFB算法，将互斥特征进行合并，能够合并的特征为#bundle，从而将特征的维度降下来，响应的，构建histogram锁耗费的时间复杂度也从O(data * feature)变成了O（data * bundle），其中feature << bundle，方法说起来很简单，但是实现起来有两大难点
- 哪些特征可以合并为一个bundle？：Greedy bundle
- 如果将特征合并为bundle，实现降维：Merge Exclusive Features
Greedy bundle的原理与图着色相同，给定一个图G，定点为V，表示特征，边为E，表示特征之间的互斥关系，接着采用贪心算法对图进行着色，一次来生成Bundle。
![image-20200428232020666](images/image-20200428232020666.png)
MEF(Merge Exclusive Features)将bundle中的特征合并为新的特征，合并的关键是原有的不同特征值在构建后的bundle中仍能够识别，由于急于histogram的方法存储的是离散的bin而不是连续的数值，因此可以通过添加偏移的方法将不同的特征的bin值设定为不同的区间。
![image-20200428232607192](images/image-20200428232607192.png)
### 树的生长策略
在XGBoost中，树是按层生长的，同一层的所有节点都做分裂，最后剪枝，他不加区分的对待同一层的叶子，带来了很多没必要的开销，因为实际上很多叶子的分裂增益较低，没必要进行搜索和分裂。
![image-20200428233314626](images/image-20200428233314626.png)
LightGBM的生长策略是leaf-wise，以降低模型损失最大化为目的，对当前所有叶子节点中切分增益最大的leaf节点进行切分，leaf-wise的缺点是生成比价深的决策树，为了防止过拟合，可以在模型中设置决策树的深度。
![image-20200428233608994](images/image-20200428233608994.png)
### 系统设计
LightGBM具有支持高效并行的特点，原生支持并行学习，目前支持
- 特征并行
- 数据并行
- Voting（投票）并行（数据并行的一种）
### 特征并行
特征并行是并行化决策树中寻找最优划分点的过程，特征并行是将对特征进行划分，每个worker找到局部的最佳切入点，使用点对点通信找到全局的最佳切入点
![image-20200428233940790](images/image-20200428233940790.png)
#### 传统算法
不同的worker存储不同的特征集，在找到全局的最佳划分点后，具有该划分点的worker进行节点分裂，然后广播切分后的左右子树数据结果，其它worker收到结果后也进行划分。
#### LightGBM中的算法
每个worker中保存了所有的特征集，在找到全局的最佳划分点后，每个worker可自行进行划分，不在进行广播划分结果，减小了网络的通信量，但存储代价变高。
### 数据并行
数据并行的目标是并行化整个决策学习的过程，每个worker中拥有部分数据，独立的构建局部直方图，合并后得到全局直方图，在全局直方图中寻找最优切分点进行分裂。
![image-20200428234328151](images/image-20200428234328151.png)
### Voting Parallel 投票并行
LightGBM采用一种称为PV-Tree的算法进行投票并行，其实这本质上也是数据并行的一种，PV-Tree和普通的决策树差不多，只是在寻找最优切分点上有所不同
![image-20200428234448970](images/image-20200428234448970.png)
每个worker拥有部分数据，独自构建直方图并找到 top-k最优的划分特征，中心worker聚合得到最优的2K个全局划分特征，再向每个worker收集top-2k特征的直方图，并进行合并得到最优划分，广播给所有worker进行本地划分。
![image-20200428235129193](images/image-20200428235129193.png)
### 实践
使用XGBoost设置树的深度，在LightGBM中是叶子节点的个数
![image-20200428235648153](images/image-20200428235648153.png)
![image-20200428235723087](images/image-20200428235723087.png)
![image-20200428235803136](images/image-20200428235803136.png)
![img](images/9J%474I3L_2JSY[W%LA6G2G.png)