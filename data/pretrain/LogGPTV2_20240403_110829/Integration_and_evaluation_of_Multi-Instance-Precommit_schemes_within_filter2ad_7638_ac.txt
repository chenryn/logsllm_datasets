### Sibling Transactions and MIPT Query

In the context of sibling transactions, the corresponding Multi-Item Precommit Table (MIPT) is queried to retrieve the XID State (XIN ST) of the transaction that needs to be aborted. This retrieval process allows for the reconstruction of the internal identifier previously associated with the transaction during its precommitment phase. The reconstructed identifier is then used to request an abort through PostgreSQL's standard (internal) APIs.

### 4. Experimental Evaluation

Performance models in [5] have already highlighted how avoiding explicit consensus across middle-tier servers can enhance system scalability and reduce end-to-end latency in multi-tier systems. Given that the MIP (Multi-Item Precommit) model is a key component in avoiding explicit consensus and also in managing fail-over without extermination schemes, the performance analysis in [5] provides a representative comparison to traditional transaction management schemes that require coordination at the middle-tier level and use extermination-based fail-over. Therefore, this section aims to quantify the memory and computational overheads of the MIP-enhanced version of PostgreSQL.

To assess these overheads in a realistic scenario, we based our analysis on the well-known TPC-W [8] benchmark, which simulates an online bookstore.

#### Memory Overhead

The primary source of memory overhead in the MIP-enhanced PostgreSQL is the extension of the tuple header with two additional fields: XID and SMULTIXACT ID, each 4 bytes in size. This modification increases the size of the tuples stored and manipulated by the DBMS. Table 1 summarizes the average percentage of spatial overhead for each database table specified by TPC-W. The last two rows of Table 1 report the weighted average storage overhead for both the smallest (250k tuples) and largest (17M tuples) data-sets specified by TPC-W.

| **Table** | **Average Tuple Size (bytes)** | **Overhead %** |
|-----------|--------------------------------|----------------|
| Address   | 154.1                          | 5.2%           |
| Author    | 410.9                          | 1.9%           |
| CC Xacts  | 126.6                          | 6.3%           |
| Country   | 63.2                           | 12.6%          |
| Customer  | 491.3                          | 1.6%           |
| Item      | 593.9                          | 1.3%           |
| Order     | 96.8                           | 8.3%           |
| Order Line| 115.8                          | 6.9%           |
| Weighted Average (smallest data-set) | - | 5.2%           |
| Weighted Average (largest data-set)  | - | 1.9%           |

The overhead introduced by the extended tuple header is minimal. For the smallest data-set, the average overhead is around 5%, primarily due to the Order Line and Address tables, which account for 54% of the database. In the largest data-set, the Item table, which constitutes 95% of the data, has an average overhead of about 1%. The maximum overhead is observed in the Country table, with an average tuple size of only 63 bytes, resulting in a 10% overhead. Overall, the additional memory consumption due to the increased tuple header size is expected to be very low or even negligible in practical scenarios.

#### Latency Overhead

To evaluate the latency overhead, we developed a prototype implementation of the TPC-W benchmark logic using JDBC. We filtered out read-only transactions, focusing only on non-idempotent transaction profiles. This approach allows us to highlight the overhead of MIP subsystems when used by every activated transaction.

Figure 1 compares the response times of the unmodified PostgreSQL 8.1.3 and the MIP-enhanced version, processing two different TPC-W transaction profiles: Buy Confirm (BC) and Admin Confirm (AC). These profiles represent lightweight and heavyweight transactional logics, respectively. The performance data were obtained using the largest data-set prescribed by the benchmark, on a machine with 4 CPUs (Xeon 2GHz), 4GB RAM, 2 SCSI disks (10000 RPM) in RAID-0 configuration, and running Linux (kernel version 2.6.8).

[Figure 1: Execution time of non-MIP transactions on PostgreSQL and MIP-transactions on the MIP-enhanced version]

The results show that the performance of the MIP-enhanced version is nearly indistinguishable from the original PostgreSQL version, with a difference of about 2% over the entire curve. This indicates the efficiency of the MIP subsystems.

#### Fail-Over Transaction Performance

To further analyze the performance, we evaluated the execution time of fail-over sibling transactions. In this setup, the original transaction is left pending in the precommit state, and a fail-over sibling transaction is activated and committed. The volume of requests along the x-axis represents half of the real transaction workload, as each request is served via two sibling transactions.

[Figure 2: Execution time for fail-over transactions – 2 sibling transactions per request vs. single sibling transaction]

From Figure 2, we observe that the system throughput is reduced by only 33% for the BC transaction profile and 25% for the AC transaction profile. This reduction is due to the strong similarity in data access patterns between sibling transactions, leading to a high rate of database buffer hits. The smaller throughput reduction for the AC profile, which accesses more data items in read mode, is consistent with this observation. The performance data in Figure 2 provide experimental evidence of the efficiency of the MIP subsystems integrated within PostgreSQL.

### References

1. P. A. Bernstein and E. Newcomer. *Principles of Transaction Processing: for the Systems Professional*. Morgan Kaufmann Publishers Inc., 1997.
2. S. Frølund and R. Guerraoui. *e-Transactions: End-to-end reliability for three-tier architectures*. IEEE Transactions on Software Engineering, 28(4):378–395, 2002.
3. C. Mohan, D. J. Haderle, B. G. Lindsay, H. Pirahesh, and P. M. Schwarz. *Aries: A transaction recovery method supporting fine-granularity locking and partial rollbacks using write-ahead logging*. ACM Transactions on Database Systems, 17(1):94–162, 1992.
4. Oracle Corporation. *Oracle 9i replication*. 2001.
5. F. Quaglia and P. Romano. *Ensuring e-Transaction with asynchronous and uncoordinated application server replicas*. IEEE Transactions on Parallel and Distributed Systems, 18(3):364–378, 2007.
6. P. Romano and F. Quaglia. *Providing e-Transaction guarantees in asynchronous systems with inaccurate failure detection*. In Proc. of the 5th Symposium on Network Computing and Applications (NCA), pages 155–162. IEEE Computer Society Press, 2006.
7. The Open Group. *Distributed TP: The XA+ Specification Version 2*. 1994.
8. Transaction Processing Performance Council. *TPC BenchmarkTM W, Standard Specification, Version 1.8*. Transaction Processing Performance Council, 2002.
9. A. Vakali and G. Pallis. *Content delivery networks: Status and trends*. IEEE Internet Computing, 07(6):68–74, 2003.

*International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 2008. 1-4244-2398-9/08/$20.00 ©2008 IEEE. DSN 2008: Romano & Quaglia*