sibling transactions, the corresponding MIPT is queried to
retrieve the XIN ST of the transaction to be aborted, so to
reconstruct the internal identi(cid:2)er previously associated with
the transaction when requesting its precommitment, which
is used to request the abort through PosgtreSQL standard
(internal) APIs.
4. Experimental Evaluation
Performance models in [5], have already highlighted
how the avoidance of explicit consensus across middle-tier
Address
Author
CC Xacts
Country
Customer
Item
Order
Order Line
Weighted Average (smallest data-set)
Weighted Average (largest data-set)
Average tuple size (bytes)
154.1
410.9
126.6
63.2
491.3
593.9
96.8
115.8
163.8
429.3
Overhead %
5.2%
1.9%
6.3%
12.6%
1.6%
1.3%
8.3%
6.9%
4.9%
1.9%
Table 1. Overhead due to the extension of the
tuple header for the TPC›W benchmark.
servers can increase system scalability and reduce end-to-
end latency in multi-tier systems. Given that the MIP model
is a building block for the avoidance of explicit consen-
sus, and for additionally avoiding extermination schemes
while handling fail-over, the performance analysis in [5]
is representative of its performance bene(cid:2)ts, compared to
traditional transaction management schemes imposing co-
ordination at the level of the middle-tier and extermination
based fail-over. Hence, the experimental study in this sec-
tion is rather aimed at quantifying both memory and com-
putational overheads for the MIP-enhanced version of Post-
greSQL. In order to assess such an overhead in a realistic
scenario, our analysis is based on the well-known TPC-W
[8] benchmark, representative of an on-line book store.
The main potential source of memory overhead in the
MIP-enhanced version of PostgreSQL is related to the ex-
tension of the tuple header with the two additional (cid:2)elds
XID and S MULTIXACT ID (both of size 4 bytes), which
give rise to an increase of the size of the tuples stored and
manipulated by the DBMS. To quantify the actual impact
of this modi(cid:2)cation, we report in Table 1 the average per-
centage of spatial overhead for each of the database tables
speci(cid:2)ed by TPC-W. In the last two rows of Table 1, we also
report the average storage overhead, weighted according to
the number of tuples in each table, when considering both
the smallest and the largest data-sets speci(cid:2)ed by TPC-W,
corresponding to 250k and 17M tuples, respectively. We
note that the overhead introduced by the growth of the tu-
ples header is actually very limited. Speci(cid:2)cally, the aver-
age percentage overhead over the whole database is around
5% for the smallest data-set, and is below 2% for the largest
data-set. This is because, in the former case, the Order Line
and Address tables, for which the average overhead is about
5%, account for 54% of the whole database. Conversely,
in the largest data-set, the Item table tuples, whose aver-
age overhead is around 1%, take up about the 95% of the
data-set size. The maximum overhead is introduced for the
Country table, whose average tuple size is only 63 bytes.
However, even in this (most unfavorable) case the overhead
remains around 10%. Overall, we can conclude that in prac-
tical scenarios the additional memory consumption due to
the increase of the tuple header size is expected to be very
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE408DSN 2008: Romano & Quagliai
)
c
e
s
(
e
m
T
e
s
n
o
p
s
e
R
 10
 8
 6
 4
 2
 0
BC (PostgreSQL)
BC (MIP-PostgreSQL)
AC (PostgreSQL)
AC (MIP-PostgreSQL)
 10
 20
 30
 50
 40
 60
Requests per Second
 70
 80
 90
 100
i
)
c
e
s
(
e
m
T
e
s
n
o
p
s
e
R
 10
 8
 6
 4
 2
 0
BC (1 sibling xact x req)
BC (2 sibling xacts x req)
AC (1 sibling xact x req)
AC (2 sibling xacts x req)
 10
 20
 30
 50
 40
 60
Requests per Second
 70
 80
 90
 100
Figure 1. Execution time of non›MIP transac›
tions on PostgreSQL and of MIP›transactions
on the MIP›enhanced version.
Figure 2. Execution time for fail›over transac›
tions › 2 sibling xacts curve (original transac›
tions in the precommit state) › vs the single
sibling transaction case.
low, or even negligible.
In order to evaluate the latency overhead due to the em-
ployment of the MIP facilities, we have developed a proto-
type implementation of the TPC-W benchmark logic, based
on JDBC. In this study, read-only transactions (which do
not pose any reliability issue, and hence would not leverage
MIP facilities) have been (cid:2)ltered out from the benchmark
workload. Therefore, only non-idempotent transaction pro-
(cid:2)les have been considered. Such a choice allows us to eval-
uate a scenario in which MIP subsystems are used by ev-
ery activated transaction so to spotlight their overhead. We
plot in Figure 1 the results of a comparative performance
test in which we contrast the response times of the un-
modi(cid:2)ed PostgreSQL 8.1.3 and of the corresponding MIP-
enhanced version, while processing two different TPC-W
transaction pro(cid:2)les, namely Buy Con(cid:2)rm (BC) and Admin
Con(cid:2)rm (AC). These are representative of lightweight and
heavyweight transactional logics, respectively. Also, these
performance data have been obtained for the largest data-
set prescribed by the benchmark. The performance results
were obtained by hosting the database server on a 4 CPUs -
Xeon 2GHz - machine equipped with 4GB of RAM, 2 SCSI
disks (10000 RPM) in RAID-0 con(cid:2)guration, and running
the Linux operating system (kernel version 2.6.8). By the
plots we get that the performance of the MIP-enhanced ver-
sion is nearly undistinguishable from that of the original
PostgreSQL version (the difference is about 2% over the
whole curve), thus providing indications on the actual ef(cid:2)-
ciency of the previously described MIP subsystems.
To further analyze the performance of the MIP subsys-
tems, we have evaluated the transaction execution time also
in the case where the data structures external to the tuple
header (i.e. the table pointed by S MULTIXACT ID - see
Section 3.2) are really allocated by the database kernel. This
does not occur if a single sibling transaction of a given fam-
ily is executed (as in the previously described test). To reach
such a con(cid:2)guration, we execute the original transaction by
leaving it pending in the precommit state. Next, we acti-
vate a fail-over sibling transaction, by also committing it,
and we evaluate its execution latency. In such a case, the
volume of requests we consider along the x-axis expresses
half of the real transaction workload on the database (since
each request is actually served via two sibling transactions).
Interestingly, from the plots in Figure 2 we get that the sys-
tem throughput gets actually reduced by only the 33% for
the BC transaction pro(cid:2)le, and the 25% for the AC trans-
action pro(cid:2)le. This is because the data access patterns of
sibling transactions show strong similarities, hence most of
the data accesses performed by the fail-over transactions re-
sult in database buffer hits. This also explains the relatively
smaller throughput reduction for the AC transaction pro(cid:2)le,
which requires accessing a larger number of data items in
read mode with respect to the BC transaction pro(cid:2)le. In-
dependently of these considerations, the performance data
in Figure 2 provide an experimental evidence of the ef(cid:2)-
ciency of the MIP subsystems we have integrated within
PostgreSQL.
References
[1] P. A. Bernstein and E. Newcomer. Principles of Transaction Processing: for the
Systems Professional. Morgan Kaufmann Publishers Inc., 1997.
[2] S. Frłlund and R. Guerraoui. e-Transactions: End-to-end reliability for three-
tier architectures. IEEE Transaction on Software Engineering, 28(4):378(cid:150)395,
2002.
[3] C. Mohan, D. J. Haderle, B. G. Lindsay, H. Pirahesh, and P. M. Schwarz. Aries:
A transaction recovery method supporting (cid:2)ne-granularity locking and partial
rollbacks using write-ahead logging. ACM Transactions on Database Systems,
17(1):94(cid:150)162, 1992.
[4] Oracle Corporation. Oracle 9i replication. 2001.
[5] F. Quaglia and P. Romano. Ensuring e-Transaction with asynchronous and un-
coordinated application server replicas. IEEE Transactions on Parallel and Dis-
tributed Systems, 18(3):364(cid:150)378, 2007.
[6] P. Romano and F. Quaglia. Providing e-Transaction guarantees in asynchronous
systems with inaccurate failure detection.
In Proc. of the 5th Symposium on
Network Computing and Applications (NCA), pages 155 (cid:150) 162. IEEE Computer
Society Press, 2006.
[7] The Open Group. Distributed TP: The XA+ Speci(cid:2)cation Version 2. 1994.
[8] Transaction Processing Performance Council. TPC BenchmarkT M W, Standard
Speci(cid:2)cation, Version 1.8. Transaction Processing Perfomance Council, 2002.
[9] A. Vakali and G. Pallis. Content delivery networks: Status and trends. IEEE
Internet Computing, 07(6):68(cid:150)74, 2003.
International Conference on Dependable Systems & Networks: Anchorage, Alaska, June 24-27 20081-4244-2398-9/08/$20.00 ©2008 IEEE409DSN 2008: Romano & Quaglia