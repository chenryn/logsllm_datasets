cluster IDs represented by the regular expression. The time into a predefined distribution. Moreover, the bottleneck vector
series formed by the groups of events, having the properties in the variational autoencoder is replaced by two vectors of the
as explained in Section I, are used to study the dynamics of same size. One of them representing the mean and the other
the system and to detect anomalies. representing the variance of the distribution. So, whenever we
Anomaly detection on time series consisting of the service’s need the output of the encoder in order to feed into the decoder
response time can be formulated as follows: For any time network, we need a sample from the distribution, defined by
t given historical observations xt = {et-w, et~w+i,..., et}, the mean and standard deviation vectors that represent the
where w is the sliding window size and et is the event’s latent low-dimensional space. Let us assume that we have
response time at time t, determine whether an anomaly occurs a dataset X of samples from a distribution parametrized by
or not (1/0). We use a sliding window to break the time a ground truth generative factor. The variational autoencoder
series into fixed-size inputs, required for the autoencoder. The aims to learn the marginal likelihood of the data in a generative
sequential order of the points inside the window is important. process:
Therefore, we combine the AEVB with the ability of the RNNs
for extracting temporal information from sequential data. marimize [loSPe(x\z)] (1)
An anomaly detection algorithm typically computes a real­
Where  and 6 parametrize the distributions of the VAE en­
valued score indicating the certainty of having anomaly, e.g.,
coder and the decoder respectively. Furthermore, the complete
p(anomaly = 1 | i n s t e a d of direcdy computing,
loss function is given by:
whether the window represents an anomaly.
£(M ;x,z) =E ^(zW [logP9(a:|z)] -£>a-i,(^(z|x)||p(z))
A. Variational autoencoder for anomaly detection
((cid:21))
An autoencoder is an unsupervised neural network archi­ The loss function, as written in (2), consists of two terms.
tecture. It applies backpropagation like the standard feed The first term represents the reconstruction loss, which is part
243
of any autoencoder architecture, except we have the expecta­ IV. Respo nse Time Ano mal y Det ect io n
tion operator, because we are sampling from the distribution. The following methods form the core components of our
The second term is the Kullback - Leibler divergence that unsupervised anomaly detection approach for microservice
ensures close mapping to a predefined distribution. or service oriented systems observed by distributed tracing
Recently, there is an increasing adoption of unsupervised, technology. First, the time series data is preprocessed and a
generative machine learning models for anomaly detection. neural network model is trained on it to capture the normal
Similarly, the variational autoencoder (AEVB) first learns the system behavior. Based on this model, the predictions for the
normal scenario (one, or many) [26]. Then, conditioned on its reconstruction are obtained. Then, a probability based, adap­
input is able to generate reconstructions. By setting a threshold
tive threshold method is used to determine whether resulting
on the reconstruction error, we are able to classify a given prediction errors represent anomalies for individual services.
window of response time as anomaly or normal. Further, a post-processing strategy, incorporated in a tolerance
B. Recurrent variational autoencoder module, is used to mitigate false positives. Lastly, we provide
anomaly pattern classification to provide descriptive and useful
Recurrent neural networks (RNNs) [30] are a type of neural
analysis results. We divide the proposed methods in four core
networks where the connections between neurons form a
steps or modules, that exchange the results in-between.
directed cycle. They are capable of learning features and
• time series preprocessing
long term dependencies from sequential and time-series data.
• model training
A typical architecture of the RNN is shown in Figure 2.
• test-time prediction
Each step in the unfolding is referred to as a time step,
• faulty pattern classification
where xt is the input at time step t. RNNs can take an
arbitrary length sequence as input, by providing the RNN a For simplicity, we will describe the methods through the
feature representation of one element of the sequence at each lens of a single time series. Given K time series, the solution
time step. st is the hidden state at time step t and contains scales since is meant to be applied to every time series in
information extracted from all time steps up to t. The hidden parallel.
state s is updated with information of the new input xt after A. Time series preprocessing
each time step: st = f{Uxt + Wsti), where U and W are
This step involves two parts, preprocessing in model train­
vectors of weights and / is the non-linear activation function.
ing and test-time prediction. The module queries the latest
The most used RNN types in practice are RNNs with LSTM
N data points (events) belonging to the same cluster ID
(Long Short-Term Memory) [31] or GRU (Gated Recurrent
(time series) and forwards it into a three stage pipeline: data
Unit) [32] cells, which we use in this paper as well.
cleaning, normalization and noise reduction.
Tracing events are JSON objects, but in dependence of the
°•t-l (cid:129)ot (cid:73)°t+l service instrumentation they might have a slightly different
structure. Common for all are the response time, which is
extracted for further processing. We assume that most of the
V V V
time the services in the system are in normal mode of opera­
tion. That is true in real-world systems where failures happen
rarely. However, the large amount of events in the time series
and the fact that proper training of neural networks requires
normalization, leads to obligation of having an outlier removal
*t-i *t+i
technique. The presence of a strong outlier, will lead to values
at = b + W st_! + Uxt_t
clamped to zero after the normalization. Therefore, events
st = tanh(at)
having response time greater than three standard deviations
ot = b +
from the mean are removed from the training batch. Next, we
normalize the values by using min-max scaling (0, 1) to ensure
Fig. 2. Architecture of RNN.
that we stay in the positive range of values for the response
Recurrent variational autoencoder [33] is combination time. In contrast, using standardization might produce negative
of AEVB and RNN. The encoder is a recurrent neural values that do not have natural meaning when we deal with
network (RNN) that processes the input sequence xt = response time (no negative time). Normalization is required
{et-w, et-w+i,..., et} and produces a sequence of hidden and makes the optimization function of the neural network
states {ht-w,ht-w+i,---,ht}. The parameters of the distri­ well-conditioned, which is key for convergence [34]. Min-max
bution over the latent code is then set as a function of ht. normalization is given with the following equation:
The decoder, uses the sampled latent vector z to set the initial
(cid:66)(cid:3) Xt - min{X)
state of a decoder RNN, which produces the output sequence — / . / IpJ
-A-t,scaled, max(Xv )\ — min{X-*r)\
V = 2/1 > 2/2; • ••, Ut- The model is trained both to reconstruct the
input sequence and to learn an approximate posterior close to where min(X) and max(X) are saved and then used for the
the prior like in a standard variational autoencoder. normalization in test-time prediction. Lastly in the pipeline,
244
we apply smoothing for noise removal and robustness to the window size input units is fed to the corresponding GRU.
small deviations. The time series is convolved with Hamming In the first timestep T = 0, the Qth response time is fed.
smoothing filter defined with its optimal parameters [35] and The abstract representation learned in the 16 GRU cells is
size M as: then propagated to the next timestep T = 1, where the 1st
response time of the window is fed and so on. Here, we have
the ability to condition the reconstruction of the next point
w(n) = 0.54 — 0.46 (cid:1) cos — - J , 0 < n < M — 1 (4)
given the past points. In such way, that in the last timestep we
have abstract representation of the window of points, which
We use smoothing with size of the window M = 12, but
has salient information for that part of the time series.
one can adjust the size depending on the noise.
Sampling layer: Represents the key part in order to be
For test-time prediction the preprocessing is executed on
able to learn multiple distributions (model of models). This
each new recorded event. During test-time, the event follows
layer consists of (window size/A) 8 units for the mean and
the same preprocessing steps as for the model training except
for the variance. The sampling layer just performs sampling
the normalization where min(X) and max(X) are the saved
from multivariate normal distribution with the given mean and
values during model training part.
variance.
Time series partitioning: After the steps in the prepro­
Repeat layer: Repeats the Sampling layer window size
cessing, we define window size, which represents number of
times, which is needed to be fed into the last hidden (GRU)
points in a sliding window that needs to be considered for
layer.
evaluation. The window with the predefined size and stride is
Output/GRU layer: Here, the network takes the output from
applied to the time series. This results in training data shape
the previous layer as input, learns abstract representation and
of: {N — window size, window size, 1). The data in such
as output have the same window size number of input timesteps
format is then feed into the neural network for training. In
only with the response time as feature.
test-time prediction, each window size number of events are
1) Training details: We observed that the required number
fed to the network for prediction.
of data points in particular time series used to produce good
B. Model architecture model in training should be more than 1000. The training data
The architecture of our proposed neural network is shown is split into two parts in sequential order. The smaller part or
in Figure 3 and described in following. 20% goes for estimating parameters and tuning the model.
We train the model for 1000 epochs and choose the one with
the best validation score. The solution uses Adam optimizer
1. response time, 2. response time,..., window size, response time
with learning rate of 0.001, which are the standard values for
training deep neural networks [28]. As mentioned, the error
function which we optimize is described in Section III. As last
step when the training is finished, the model is saved and used
in test-time prediction.
2) Dynamic error threshold: The difference between a
prediction and an observed parameter value vector is measured
by the mean square error (MSE) which is given with the
following equation.
MSE = . 1 . Y ixi - yi? (5)
Instead of setting a magic error threshold for anomaly detec­
tion purpose, we use the validation set for threshold setting.
For each window/sample in the validation set, we apply the
model produced by the training set and calculate the MSE
between the prediction (reconstruction) and the actual sample.