completion time of job (assuming the background trafﬁc remains the same for the duration of the job) to the lower bound completion
time resulted from using the guarantees.
)
s
p
b
M
(
l
w
o
F
−
g
n
o
L
t
u
p
h
g
u
o
r
h
T
500
450
400
350
300
250
200
150
100
50
0
1 new VM−to−VM pair/sec
10 new VM−to−VM pairs/sec
20 new VM−to−VM pairs/sec
30 new VM−to−VM pairs/sec
50 new VM−to−VM pairs/sec
0
20
40
60
Short Flows/s
80
100
Figure 9: Short-ﬂow impact on long ﬂows
tection (by 25% on average), because ElasticSwitch is signiﬁcantly
less aggressive than TCP, so that it can always meet the guarantees.
Note that, since this is an oversubscribed topology, congestion
occurs almost always in the core, and systems like Gatekeeper [20]
and EyeQ [12] would be unable to always satisfy the guarantees.
tios, ranging from 1× up to 5× (not shown, for brevity).
We observed similar behavior for different oversubscription ra-
Since we do not possess a full MapReduce trace, we do not try to
accurately estimate the effect of ElasticSwitch on job completion
time. Instead, we estimate the shufﬂe completion time under the
worst-case assumption: the background trafﬁc (i.e., the trafﬁc of
the other jobs) remains the same for the entire duration of a job.
This should be true for small jobs, but not necessarily for long jobs,
for which the beneﬁts of bandwidth guarantees might be reduced.
Fig. 8(b) plots the CDF of the ratio between the shufﬂe com-
pletion time using ElasticSwitch and the shufﬂe time obtained
with Oktopus-like Reservations, under this worst-case assumption.
Completion times in ElasticSwitch never exceed the completion
time of static reservations (represented by the vertical line at 1),
but jobs can complete signiﬁcantly faster, as happens when 10% or
even 100% of the VMs are active. When using no guarantees, the
worst case job completion time can be up to 130× longer in this
simple experiment (for the “unbalanced” setup).
Mice vs. Elephant Flows: In this experiment, we evaluate the
effectiveness of GP under large ﬂow rates. Note that GP tries to
partition guarantees across all VM-to-VM ﬂows, to ensure that a
VM can transmit/receive at its hose model guarantee. To stress
GP, we use a workload with varying number of short TCP ﬂows
(mice) created at different ﬂow rates and measure their impact on
an elephant ﬂow.
We set up two VMs X and Y with 450Mbps hose model guaran-
tees that compete over one 1Gbps link L (thus L is fully reserved).
Each of X and Y send trafﬁc using a single elephant TCP ﬂow to
remote VMs (e.g., X sends to VM Z while Y sends to VM T ). In
addition to this ﬂow, VM X generates a number of short TCP ﬂows
(HTTP requests) to other VMs.
We start from the observation that the guarantee can be “wasted”
on short ﬂows only if the short ﬂows are always directed to a dif-
ferent VM. However, actual data (e.g., [7]) shows that the num-
ber of services to which one service communicates are very small,
and thus the number of actual VMs with which one VM exchanges
short ﬂows in a short time should be small.
Fig. 9 shows the throughput achieved by the elephant ﬂow of
VM X for various cases. We vary the total number of short ﬂows
X generates per second from 10 to 100, out of which 10 to 50 are
new VM-to-VM ﬂows. Results show that below 10 new VMs con-
tacted by X every second, there is little impact on X’s elephant
ﬂow, regardless of the number of mice ﬂows generated by X. For
20 or more new VMs contacted by X, there is an impact on the
throughput of X’s long ﬂow. However, we note that even if a VM
were to contact so many new VMs per second with short ﬂows, it
cannot sustain this rate for too long, since it would end up com-
municating with thousands of VMs in a short time. For this reason
we believe the impact of short ﬂows on the throughput of elephant
ﬂows in ElasticSwitch is not a signiﬁcant concern.
Sharing Additional Bandwidth: We show that ElasticSwitch
shares the additional bandwidth roughly in proportion to the guar-
antees of the VM-to-VM ﬂows competing for that bandwidth. In
this experiment, two ﬂows share a 1Gbps link, with one ﬂow given
a guarantee that is twice the guarantee of the other. We measure the
resulting throughputs to examine how the residual bandwidth on
the link (left over after the guarantees are met) is shared. Table 2
presents these results for three cases. As expected, since the cumu-
lative guarantee is less than 900Mbps, both ﬂows achieve higher
throughput than their guarantees. Ideally, the ﬂow with higher guar-
antee should achieve 2X higher throughput than the other ﬂow and
they should fully utilize the 1Gbps link. However, the ﬂow with the
higher guarantee grabs a slightly disproportional share of the resid-
ual bandwidth, with the non-proportionality increasing with larger
residual link bandwidth. We further analyzed our logs and noticed
that the ratio of the dropped packets did not exactly follow the ratio
of the rates, but was closer to a 1:1 ratio. This causes rate alloca-
tion to hold the rate of the ﬂow with smaller guarantee for longer
periods. This is a place to improve the RA algorithm in the future.
359)
s
p
b
M
(
X
f
o
t
u
p
h
g
u
o
r
h
T
900
800
700
600
500
400
300
200
ElasticSwitch Less Aggr.
ElasticSwitch More Aggr.
ElasticSwitch No H−I
ElasticSwitch No R−C
ElasticSwitch No H−I & R−C
 (RA=Seawall)
)
s
p
b
M
(
X
f
o
t
u
p
h
g
u
o
r
h
T
900
800
700
600
500
400
300
200
ElasticSwitch Less Aggr.
ElasticSwitch More Aggr.
ElasticSwitch No H−I
ElasticSwitch No R−C
ElasticSwitch No H−I & R−C
 (RA=Seawall)
s
p
b
M
950
850
750
650
550
450
350
0
50
100 150 200 250 300
# senders to Y
0
50
100 150 200 250 300
# senders to Y
0
ElasticSwitch Less Aggr.
ElasticSwitch More Aggr.
ElasticSwitch No H−I
ElasticSwitch No R−C
ElasticSwitch No H−I & R−C
 (RA=Seawall)
200
400
600
800
1000
Background traffic Mbps
(a) Many-to-one 1 TCP ﬂow
(b) Many-to-one 2 TCP ﬂows
(c) Throughput vs. UDP Background Trafﬁc
Figure 10: Sensitivity vs. various parameters.
Guar. (Mbps)
Rate (Mbps)
Ratio
45
203
90
536
2.64
90
219
180
531
2.42
225
275
450
589
2.14
Table 2: Sharing a 1Gbps link between two VM-to-VM ﬂows
with one guarantee being twice the other
7.2 Sensitivity
Aggressiveness: In general, by tuning the RA algorithm to be more
aggressive in grabbing spare bandwidth, ElasticSwitch can better
utilize the available capacity. On the other hand, a less aggressive
RA makes ElasticSwitch better suited to provide guarantees in dif-
ﬁcult conditions. We now show that ElasticSwitch is resilient to pa-
rameter changes, and that Hold-Increase and Rate-Caution proved
useful for providing guarantees in difﬁcult conditions.
Fig. 10 shows the behavior of ElasticSwitch for different rate al-
location algorithms. By “Less Aggr.” we refer to ElasticSwitch us-
ing the parameters described at the start of this section. By “More
Aggr.” we refer to ElasticSwitch using more aggressive rate in-
crease parameters for Seawall (speciﬁcally we use a rate increase
constant of 2.5Mbps for the convex curve instead of 0.5Mbps, and
the time scalar TS=2.0 instead of 1.5 for the “Less Aggr.” case).
By “No H-I” we refer to not using Hold-Increase, by “No R-C”
we refer to not using Rate-Caution, and by “No H-I & R-C” we
refer to not using either of them. Note that not using Hold-Increase
nor Rate-Caution is equivalent to applying Seawall’s algorithm for
rate allocation. All of the last three RA algorithms use the less ag-
gressive Seawall parameters; however, by not using Hold-Increase
and/or Rate-Caution they become (signiﬁcantly) more aggressive.
Figures 10(a) and 10(b) show a scenario similar to that in Fig. 5
(with the minor difference that the congestion occurs on the edge
link in this experiment, which, surprisingly, proved more challeng-
ing). However, while in Fig. 10(a) all VMs use a single TCP ﬂow,
in Fig. 10(b), all VM to VM trafﬁc consists of two TCP ﬂows.
Two TCP ﬂows better utilize the bandwidth allocated by the RA
algorithm. We observe that increasing the aggressiveness of Elas-
ticSwitch is almost unnoticeable, showing that ElasticSwitch is
quite resilient to parameter changes. We can also see that Hold-
Increase is instrumental for achieving guarantees when congestion
is detected through packet losses. Rate-Caution proves helpful for
more aggressive approaches such as Seawall, though its effect on
the non-aggressive ElasticSwitch is small.
Fig. 10(c) shows the other face of being less aggressive, using
the same TCP vs. UDP scenario as in Fig. 2. When the UDP ﬂow’s
demand is below its guarantee of 450Mbps, the TCP ﬂow attempts
to grab the additional bandwidth. As one can see, in this case being
more aggressive pays off in achieving a higher link utilization.
Periods: Changing the RA and GP periods did not signiﬁcantly af-
fect the results in our experiments. We experimented with rate allo-
cation periods from 10ms to 30ms and with guarantee partitioning
periods from 50 to 150ms. For brevity, we omit these charts. It is
true, however, that the workloads we considered in this evaluation
do not stress throughputs that vary signiﬁcantly in time, except for
short ﬂows. Thus, exploring the effects of different periods is also
a place for future work.
7.3 Overhead
We evaluate the overheads of ElasticSwitch in terms of latency,
CPU, and control trafﬁc bandwidth.
Latency: Compared to NoProtection, ElasticSwitch adds latency
due to the rate limiters in the data path and a user-space control pro-
cess that sets up the limiters for each new VM-to-VM ﬂow. Com-
pared to the Oktopus-like Reservation, ElasticSwitch adds over-
head due to the ﬁlling of queues in the intermediate switches for
work conservation. Fig. 11 shows the completion time of a short
ﬂow generated by VM X towards VM Y in several circumstances:
(a) when there is no trafﬁc (and this is the ﬁrst ﬂow between X and
Y , i.e., a “cold” start); and (b) when we vary the number of VM-to-
VM ﬂows that congest a given link L that the short ﬂow traverses.
We vary this number from one up to the maximum number of other
VMs that share the guarantees on link L with X and Y (23 other
VM-to-VM ﬂows in this case; we use a 6× oversubscribed network
with 4 VMs per server). We use a single TCP ﬂow between each
VM pair.
Fig. 11 shows that ElasticSwitch increases completion time only
slightly when there is no other trafﬁc. With background trafﬁc, the
added latency of ElasticSwitch is smaller than when using NoPro-
tection, because rate allocation algorithm is less aggressive than the
TCP ﬂows, and queues have lower occupancy. However, this delay