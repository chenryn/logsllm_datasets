𝑥′
𝛼 −
𝑖) is a
multiple of 𝛼 such that max(𝑟, 𝑟′) = 1. We find the difference in the
probability of 𝑌 occurring for 𝑥 and 𝑥′ as:
randomized response step. Similarly for 𝑥′ we define 𝑟′ =
(cid:106) min(𝑥𝑖,𝑥′
(cid:106) min(𝑥𝑖,𝑥′
𝑖)
𝑖)
𝛼
𝛼
𝑖
Pr[𝑌 | 𝑥] − Pr[𝑌 | 𝑥′] = ((1 − 𝑟)𝑝 + 𝑟𝑞) − ((1 − 𝑟′)𝑝 + 𝑟′𝑞)
= (𝑟 − 𝑟′) · (𝑞 − 𝑝)
𝑖
𝑖
·
𝛼
=
=
𝛼
𝛼 + 2
𝑥𝑖 − 𝑥′
𝑥𝑖 − 𝑥′
𝛼 + 2
By symmetry, the absolute difference in probability for setting
the bit to either zero or one is |𝑥𝑖−𝑥′
𝑖 |
𝛼+2 . Let 𝑍 be an arbitrary output
of Algorithm 2. Since 𝑥 and 𝑥′ agree on all but the 𝑖th entry, the
change in probability of outputting 𝑍 depends only on the affected
bit. Let 𝑌 ′ denote the event that the bit agrees with output 𝑍. Then
we find the ratio of probabilities of outputting 𝑍 as:
.
Pr[ALP1-projection(𝑥′) = 𝑍]
Pr[ALP1-projection(𝑥) = 𝑍] =
𝑖 |
𝛼+2
Pr[𝑌 ′| 𝑥]
Pr[𝑌 ′ | 𝑥] ≤ Pr[𝑌 ′| 𝑥] + |𝑥𝑖−𝑥′
Pr[𝑌 ′ | 𝑥′]
≤ 𝑝 + |𝑥𝑖−𝑥′
𝑖 |
𝛼+2
𝑝
≤ 𝑒 |𝑥𝑖−𝑥′
𝑖 |
= 1 + |𝑥𝑖 − 𝑥′
𝑖 |
.
Figure 1: Embedding with 𝑚 = 8, 𝑠 = 5 and 𝑦𝑖 = 5.
The 𝑖th entry is the only non-zero entry.
representation of the input vector 𝑥. We use this construction when
estimating the value of 𝑥𝑖 later.
The algorithm takes three parameters 𝛼, 𝛽, and 𝑠. The parameters
𝛼 and 𝑠 are adjustable. We discuss these parameters later as part
of the error analysis. In Section 6 we discuss how to select values
for 𝛼 and 𝑠. Throughout the paper we sometimes assume that 𝛼
is a constant and 𝑠 is a constant multiple of 𝑘 that is 𝛼 = Θ(1)
and 𝑠 = Θ(𝑘). The parameter 𝛽 bounds the values stored in the
embedding. We discuss 𝛽 as part of the error analysis as well.
Lemma 4.1. Algorithm 2 satisfies 1-differential privacy.
Proof. Let 𝑥, 𝑥′ ∈ R𝑑+ denote two neighboring vectors. We prove
the lemma in several steps. First, the vectors differ only in their
5
Here the second inequality follows from 𝑝 ≤ Pr[𝑌 ′| 𝑥] ≤ 𝑞. From
here it is easy to take hash collisions into account as follows: Let 𝑝′
denote the probability of 𝑌 occurring after setting the 𝑖th entry to
zero. That is, we have 𝑝 ≤ 𝑝′ ≤ 𝑞 and Pr[𝑌 | 𝑥] = (1− 𝑟) · 𝑝′ + 𝑟 · 𝑞.
The absolute difference in probability is still bounded such that
Pr[𝑌 | 𝑥] − Pr[𝑌 | 𝑥′] ≤ |𝑥𝑖−𝑥′
𝑖 |
𝛼+2 . As such it still holds that:
Pr[ALP1-projection(𝑥′) = 𝑍]
𝑖 |
Pr[ALP1-projection(𝑥) = 𝑍] ≤ 𝑒 |𝑥𝑖−𝑥′
.
Next, we remove the assumption that only a single bit is affected
by composing probabilities. We provide the following inductive
construction. Let 𝑥, 𝑥′ ∈ R𝑑+ be vectors that differ in the 𝑖th entry
such that exactly two bits are affected. We consider the case of
𝑥𝑖  0.
Input
:Embedding ˜𝑧 ∈ {0, 1}𝑠×𝑚. Sequence of hash
functions ℎ = (ℎ1, . . . , ℎ𝑚). Index 𝑖 ∈ [𝑑].
:Estimate of 𝑥𝑖.
Output
(1) Define the function 𝑓 : {0, . . . , 𝑚} → Z as:
𝑛∑︁
𝑎=1
𝑓 (𝑛) =
2˜𝑧ℎ𝑎(𝑖),𝑎 − 1
(2) Let 𝑃 be the set of arguments maximizing 𝑓 . That is,
𝑃 = {𝑛 ∈ {0, . . . , 𝑚} : 𝑓 (𝑎) ≤ 𝑓 (𝑛) for all 𝑎 ∈ {0, . . . , 𝑚}}
(3) Let ˜𝑦𝑖 = average(𝑃)
(4) Return ˜𝑦𝑖 · 𝛼.
Pr[ALP1-projection(𝑥′) = 𝑍]
Pr[ALP1-projection(𝑥) = 𝑍] =
Pr[ALP1-projection(𝑥′′) = 𝑍]
Pr[ALP1-projection(𝑥) = 𝑍]
· Pr[ALP1-projection(𝑥′) = 𝑍]
Pr[ALP1-projection(𝑥′′) = 𝑍]
≤ 𝑒 |𝑥𝑖−𝑥′′
𝑖 | · 𝑒 |𝑥′′
𝑖 |
= 𝑒 |𝑥𝑖−𝑥′
𝑖 −𝑥′
𝑖 |
,
which can be applied inductively if changing an entry affects more
than two bits.
We are now ready to generalize to any vectors 𝑥, 𝑥′ ∈ R𝑑+, i.e.,
where vectors may differ in more than a single position. Using the
bound from above, we can bound the ratio of probabilities by:
Pr[ALP1-projection(𝑥) = 𝑍] ≤ 
Pr[ALP1-projection(𝑥′) = 𝑍]
𝑒 |𝑥𝑖−𝑥′
𝑖 |
𝑖∈[𝑑]
= 𝑒𝑖∈[𝑑] |𝑥𝑖−𝑥′
𝑖 |
= 𝑒 ∥𝑥−𝑥′∥1
.
The privacy loss is thus bounded by the ℓ1-distance of the vectors
for any output. Recall that the ℓ1-distance is upper bounded by 1 for
two neighboring vectors. As such the algorithm is 1-differentially
private as for any pair of neighboring vectors 𝑥 and 𝑥′ and any
subset of outputs 𝑆 we have:
Pr[ALP1-projection(𝑥) ∈ 𝑆] ≤ 𝑒 ∥𝑥−𝑥′∥1 Pr[ALP1-projection(𝑥′) ∈ 𝑆]
≤ 𝑒 · Pr[ALP1-projection(𝑥′) ∈ 𝑆] .
□
The following lemma summarizes the space complexity of stor-
ing the bit-array and the collection of hash functions.
Lemma 4.2. The number of bits required to store ℎ and ˜𝑧 is
(cid:18) (𝑠 + log 𝑑) · 𝛽
Proof. By definition 𝑚 = 𝑂(cid:16) 𝛽
total of 𝑂(cid:16) log(𝑑)𝛽
(cid:17) bits to store ℎ.
𝑂
𝛼
𝛼
(cid:19)
(cid:17) and as such 𝑠 · 𝑚 = 𝑂(cid:16) 𝑠𝛽
.
𝛼
are used to store ˜𝑧. Each hash function uses 𝑂(log(𝑑)) bits for a
□
(cid:17) bits
𝛼
4.2 Estimating an entry
We now introduce the algorithm to estimate an entry based on the
embedding from Algorithm 2. When accessing the 𝑖th entry, we
estimate the value of 𝑦𝑖 and multiply by 𝛼 to reverse the initial
scaling of 𝑥𝑖. The estimate of 𝑦𝑖 is chosen to maximize a partial
sum. If multiple values maximize the sum we use their average.
Intuition. The first 𝑦𝑖 bits representing the 𝑖th entry are set to
one before applying noise in Algorithm 2, cf. Figure 1. The last
𝑚 − 𝑦𝑖 bits are zero, except if there are hash collisions. Some bits
might be flipped due to randomized response, but we expect the
majority of the first 𝑦𝑖 bits to be ones and the majority of the
remaining 𝑚−𝑦𝑖 bits to be zeros. As such the estimate of 𝑦𝑖 is based
on prefixes maximizing the difference between ones and zeros. The
pseudocode for the algorithm is given as Algorithm 3.
Figure 2: Estimation of 𝑖th entry from Figure 1.
The partial sum is maximized at indices 3 and 5.
The estimate is 4, while the true value was 5.
Figure 2 shows an example of Algorithm 3. The example is based
on the embedding from Figure 1 after adding noise. The plot shows
the value of 𝑓 for all candidate estimates. This sum is maximized at
positions 3 and 5. This is visualized as the global peaks in the plot.
The estimate is the average of those positions.
Lemma 4.3. The evaluation time of Algorithm 3 is 𝑂(cid:16) 𝛽
(cid:17)
.
𝛼
Proof. We can compute all partial sums by evaluating each bit
( ˜𝑧ℎ1(𝑖),1, . . . , ˜𝑧ℎ𝑚(𝑖),𝑚) once using dynamic programming. As such
6
Session 4D: Differential Privacy CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea1228the evaluation time is 𝑂(𝑚) with 𝑚 =
when 𝛼 = Θ(1).
(cid:108) 𝛽
𝛼
(cid:109). We have 𝑚 = 𝑂(𝛽)
□
We now analyze the per-entry error of Algorithm 3. We first
analyze the expected error based on the parameters of the algorithm.
The results are presented in Lemma 4.8. In Lemmas 4.9 and 4.10 we
bound the tail distribution of the per-entry error of the algorithm.
Lemma 4.4. The expected per-entry error of Algorithm 3 is bounded
2 + E[|𝑦𝑖 − ˜𝑦𝑖|]) · 𝛼 for entries with a value of at most 𝛽.
Proof. It is clear that the error of the 𝑖th entry is 𝛼 times the
𝛼 . The expected difference is bounded
difference between ˜𝑦𝑖 and 𝑥𝑖
by:
by ( 1
E[| 𝑥𝑖
𝛼
− ˜𝑦𝑖|] ≤ E[| 𝑥𝑖
𝛼
− 𝑦𝑖|] + E[|𝑦𝑖 − ˜𝑦𝑖|]
≤ 1
2 + E[|𝑦𝑖 − ˜𝑦𝑖|] .
The last inequality follows from Lemma 2.8.
that 𝑆0 = 0 and 𝑆𝑛 =𝑛
□
We find an upper bound on E[|𝑦𝑖 − ˜𝑦𝑖|] by analyzing simple
random walks. A simple random walk is a stochastic process such
ℓ=1 𝑋ℓ, where 𝑋 are independent and iden-
tically distributed random variables with Pr[𝑋ℓ = 1] = 𝑝 and
Pr[𝑋ℓ = −1] = 1 − 𝑝 = 𝑞.
Lemma 4.5. Let 𝑆 be a simple random walk with 𝑝  𝑛 such that
𝑆ℓ > 𝑆𝑛 is 𝑝
𝑞 .
Proof. It follows directly from Theorem 1 by Alm [1].
□
For our analysis, we are concerned with the maximum 𝑛 such
that 𝑆𝑛 ≥ 0. For an infinite random walk where 𝑝  max({𝑆𝑛+1, . . . , 𝑆∞})] = Pr[𝑋𝑛+1 = −1]·
Pr[𝑆𝑛+1 = max({𝑆𝑛+1, . . . , 𝑆∞})]
= 𝑞 · (1 − 𝑝
𝑞
= 𝑞 − 𝑝 .
)
The last non-negative step must have value exactly zero and as
such must be at an even numbered step. The probability that step
2𝑖 is the last non-negative is:
Pr[(max
𝑛
: 𝑆𝑛 ≥ 0) = 2𝑖] = Pr[𝑆2𝑖 = 0]·
(cid:18)2𝑖
(cid:19)
𝑖
=
Pr[𝑆𝑖 > max({𝑆𝑖+1, . . . , 𝑆∞})]
(𝑝𝑞)𝑖(𝑞 − 𝑝) .
7
E[max
𝑛
: 𝑆𝑛 ≥ 0] =
=
∞∑︁
∞∑︁
𝑖=0
𝑖=0
2𝑖 · Pr[(max
: 𝑆𝑛 ≥ 0) = 2𝑖]
2𝑖