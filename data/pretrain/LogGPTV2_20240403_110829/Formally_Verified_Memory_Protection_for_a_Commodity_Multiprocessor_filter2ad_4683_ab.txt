tables. Only VMs can use these two hypercalls; KServ cannot
use them to gain access to VM pages.
SeKVM delegates device management to KServ. Devices
are untrusted and KCore ensures that devices cannot
compromise VM data using DMA protection. Like HypSec,
SeKVM assumes VMs do not voluntarily leak data, and
assumes that they encrypt I/O data for end-to-end security.
Using hardware features. Like KVM, SeKVM leverages
standard multiprocessor hardware features for its function-
ality and performance, including multi-level shared page
tables, tagged TLBs, caches, and IOMMU hardware. KCore
supports multi-level shared NPTs to support standard KVM
functionality. KCore supports dynamically allocated 4-level
NPTs as used in KVM, which is essential on Arm 64-bit hard-
ware. KCore supports huge (2MB) and regular (4KB) pages,
also standard in KVM, which is crucial for virtualization
performance. KCore supports shared NPTs that can be con-
currently accessed by multiple CPUs as this is a requirement
for multiprocessor VMs, each of which has a shared NPT.
USENIX Association
30th USENIX Security Symposium    3955
KCore uses Arm’s tagged TLBs to improve paging perfor-
mance, avoiding the need to ﬂush TLBs on context switches
between VMs and KServ. KCore assigns an identiﬁer to each
VM and KServ which it uses to tag TLB entries so address
translation can be properly disambiguated on TLB accesses
from multiple VMs and KServ. When updating a page table
entry, KCore ﬂushes corresponding TLB entries to ensure
the TLB does not include stale page table entries that could
potentially compromise VM security. For instance, when a
VM page is evicted from its stage 2 page tables, KCore has
to ﬂush the TLB entries correlated to the translation used
for the evicted page. Otherwise, a VM could use the cached
TLB entry to access the evicted page that KServ may now
allocate to the other VMs. Correct TLB maintenance while
avoiding unnecessary TLB ﬂushes is crucial for VM security
and performance.
KCore takes advantage of Arm’s hardware cache coherence
architecture to maximize system performance, but needs to
ensure that caching does not violate the conﬁdentiality and
integrity of VM data. Architectures like Arm allow software
to manage cached data. In particular, Arm’s hardware cache
coherence ensures that all cached memory accesses across
different CPUs and different level caches get the same syn-
chronized value, but it does not guarantee that what is in the
cache is the same as main memory. Memory accesses that
are conﬁgured to bypass the cache may therefore obtain stale
data if the latest value is cached. To ensure this does not re-
sult in any possible leakage of VM data, when KCore scrubs
memory pages, it executes cache management instructions
to force those writes to cached data to also be written back
to main memory to ensure there is no way for any VMs or
KServ to access VM data directly from main memory.
KCore leverages the System Memory Management Unit
(SMMU) [4], Arm’s IOMMU, to ensure that a VM’s private
memory cannot be accessed by devices assigned to KServ or
other VMs, including protecting against DMA attacks. KCore
ensures the SMMU is unmapped from all NPTs so it can
fully control the hardware to ensure devices can only access
memory through the SMMU page tables it manages. It uses
the SMMU page tables to enforce memory isolation. KCore
validates all SMMU operations by only allowing the driver in
KServ to program the SMMU through Memory Mapped IO
(MMIO) accesses, which trap to KCore, and SMMU hyper-
calls. MMIO accesses are trapped by unmapping the SMMU
from KServ’s stage 2 page tables. SMMU hypercalls (1) allo-
cate/deallocate an SMMU translation unit, and its associated
page tables, for a device, and (2) map/unmap/walk the SMMU
page tables for a given device. As part of validating a KServ
page allocation proposal for a VM, KCore also ensures that
the page being allocated is not mapped by any SMMU page
table for any device assigned to KServ or other VMs.
Layered
design
signiﬁcantly reduces the size of the its TCB and therefore
implementation. While SeKVM’s
also reduces the proof effort to verify the TCB, proving
the correctness of the smaller hypervisor TCB, KCore, still
remains a challenge, especially on Arm multiprocessor
hardware. To further reduce the proof burden, KCore itself
uses a layered architecture to facilitate a layered approach
to veriﬁcation. The implementation is constructed as a set
of layers such that functions deﬁned in higher layers of
the implementation can only call functions at lower layers
of the implementation. Layers can then be veriﬁed in an
incremental and modular way. Once we verify the lower
layers of the implementation, we can compose them together
to simplify the veriﬁcation of higher layers.
The speciﬁc layers in KCore’s implementation are not
determined in a vacuum, but with veriﬁcation in mind based
on the following layer design principles. First, we introduce
layers to simplify abstractions, when functionality needed
by lower layers is not needed by higher layers. Second, we
introduce layers to hide complexity, when low-level details
are not needed by higher layers. Third, we introduce layers
to consolidate functionality, so that such functionality only
needs to be veriﬁed once against its speciﬁcation. For in-
stance, by treating a module used by other modules as its own
separate layer, we do not have to redo the proof of that module
for all of the other modules, simplifying veriﬁcation. Finally,
we introduce layers to enforce invariants, which are used to
prove high-level properties. Introducing layers modularizes
veriﬁcation, reducing proof effort and maintenance.
Figure 2 shows the KCore layered architecture. The top
layer is TrapHandler, which deﬁnes KCore’s interface to
KServ and VMs, such as KServ hypercalls and VM exit han-
dlers. Exceptions caused by KServ and VMs cause a context
switch to KCore, calling CtxtSwitch to save CPU register
state to memory, then TrapDispatcher or FaultHandler
to handle the respective exception. On a KServ hypercall,
TrapDispatcher calls VCPUOps to handle the VM_ENTER
hypercall to execute a VM, and MemHandler, BootOps and
SmmuOps to use their respective hypercall handlers. On a
VM exit, TrapDispatcher calls functions at lower layers if
the exception can be handled directly by KCore, otherwise
CtxtSwitch is called again, protecting VM CPU data and
switching to KServ to handle the exception. On other KServ
exceptions, FaultHandler calls MemOps to handle KServ
stage 2 page faults and SmmuOps to handle any KServ accesses
to SMMU hardware. FaultHandler also calls MemOps to
handle VM GRANT_MEM and REVOKE_MEM hypercalls. KCore
implements basic page table operations in the layers in MMU
PT, including page table walk, map or clear a pfn in page
table, and page table allocation. KCore implements own-
ership tracking for each page in PageMgmt, PageIndex, and
Memblock for memory access control. MemOps and MemAux
provide memory protection APIs to other layers. KCore
provides SMMU page table operations in layers in SMMT PT.
KCore provides VM boot protection in BootOps, BootAux,
and BootCore. BootOps calls the Ed25519 libary from the
3956    30th USENIX Security Symposium
USENIX Association
For each layer I of KCore’s implementation, we prove that I
running on top of the underlay interface L reﬁnes its (overlay)
speciﬁcation S, I@L (cid:118) S. Because the layer reﬁnement
relation (cid:118) is transitive, we can incrementally reﬁne KCore’s
entire implementation as a stack of layer speciﬁcations. For
example, given a system comprising of layer implementations
I3, I2, and I1, their respective layer speciﬁcations S3, S2, and
S1, and a base machine model speciﬁed by S0, we prove
I1@S0 (cid:118) S1, I2@S1 (cid:118) S2, and I3@S2 (cid:118) S3. We compose
these layers to obtain (I3 ⊕ I2 ⊕ I1)@S0 (cid:118) S3, proving that
the behavior of the system’s linked modules together reﬁne
the top-level speciﬁcation S3.
All KCore interface speciﬁcations and reﬁnement proofs
are manually written in Coq, with 34 interface speciﬁcations
matching the layers in Figure 2. We use CompCert [45]
to parse each layer of the C implementation into Clight
representation, an abstract syntax tree deﬁned in Coq; the
same is done manually for assembly code. We then use that
Coq representation to prove that the layer implementation
reﬁnes its respective interface speciﬁcation at the C and
assembly level. Note that the C functions that we verify may
invoke primitives implemented in assembly and introduced
in the bottom machine model. We enforce that these
assembly primitives do not violate C calling conventions and
parameters are correctly passed. For example, we verify the
correctness of TLB maintenance code, which is implemented
in C, but invokes primitives implemented in assembly.
We prove, layer by layer, that the KCore implementation
using a detailed machine model reﬁnes its top-level speciﬁca-
tion using a simpler abstract model. We then use the top-level
speciﬁcation to prove that KCore guarantees VM conﬁden-
tiality and integrity for any KServ implementation, thereby
proving security guarantees for the entire SeKVM hypervisor.
4.1 AbsMachine: Abstract Hardware Model
Each of KCore’s layer modules successively builds upon Ab-
sMachine, our bottom machine model. This abstract multipro-
cessor hardware model constitutes the foundation of our cor-
rectness proof. As shown in Figure 3a, AbsMachine includes
multiple CPUs and a shared main memory. AbsMachine mod-
els general purpose and systems registers for each CPU. It also
models Arm hardware features relevant to modern hypervisor
implementation, including stage 1 and stage 2 page tables,
a physically indexed, physically tagged (PIPT) shared data
cache, and SMMU page tables, and TLBs. The shared data
cache is semantically equivalent to Arm’s multi-level cache
hierarchy with coherent caches. KCore uses stage 2 page ta-
bles to translate guest physical addresses to actual physical
addresses on the host, and uses its own EL2 stage 1 page
table to translate its virtual addresses to physical addresses.
AbsMachine models the particular hardware conﬁguration of
KCore which we verify. For example, although Arm supports
1GB, 2MB, and 4KB mappings in stage 2 page tables, KCore
Figure 2: KCore Layered Implementation
veriﬁed Hacl* [74] library to authenticate signed VM boot
images. BootOps and MemOps call to the AES implementation
in Hacl* to encrypt or decrypt VM data to support VM
management. Finally, four layers implement locks.
4 SeKVM Veriﬁcation
We combine the layered implementation of SeKVM’s
TCB, KCore, with a layered hardware model to verify its
correctness using Coq. We start with a bottom machine
model that supports real multiprocessor hardware features
such as multi-level shared page tables, tagged TLBs, and a
coherent cache hierarchy with bypass support. We use layers
to gradually reﬁne the detailed low-level machine model to
a higher-level and simpler abstract model. Finally, we verify
each layer of software by matching it with the simplest level
of machine model abstraction, reducing proof burden to make
it possible for the ﬁrst time to verify commodity software
using these hardware features.
Each abstraction layer [31, 34] consists of three compo-
nents: the underlay interface, the layer’s implementation,
and its overlay interface. Each interface exposes abstract
primitives, encapsulating the implementation of lower-level
routines, so that each layer’s implementation may invoke the
primitives of the underlay interface as part of its execution.
USENIX Association
30th USENIX Security Symposium    3957
Exit HandlerTrapHandlerTrapHandlerRawTrapDispatcherFaultHandlerMemHandlerVCPUCtxtSwitchVCPUOpsVCPUAuxSmmuOpsSmmuAuxSmmuCoreSmmuCoreAuxSmmuRawSMMUVM BootBootOpsBootAuxBootCoreVMPowerMmioSPTOpsMmioSPTWalkMmioPTWalkMmioPTAllocSMMUPTNPTOpsNPTWalkPTWalkPTAllocMMUPTMemOpsMemAuxPageMgmtPageIndexMemblockVMMemEd25519AESHACL*LockLockOpsHLockOpsQLockOps(a) The bottom machine model: AbsMachine
(b) The machine model after the layer reﬁnement
Figure 3: Reﬁnement of machine models. (a) The bottom machine model that includes TLBs, multi-level page tables, and PIPT writeback
caches. (b) The layered reﬁnement of machine models abstracts away TLBs, consolidates multi-level page tables into a single-level ﬂat page
map, and enforces the well-formedness of data caches.
only uses 4KB and 2MB mappings in stage 2 page tables,
since 1GB mappings result in fragmentation. Thus, we model
a VM’s memory load and store accesses in AbsMachine over
stage 1 and stage 2 page tables using 4KB and 2MB mappings.
Our abstract machine is formalized as a transition system,
where each state transition is the result of some atomic com-
putational step by a single CPU, such as executing a single
machine instruction or invoking a primitive; concurrency is
realized by the nondeterministic interleaving of each CPU’s
steps. To simplify reasoning about all possible interleavings,
we borrow the ideas of CertiKOS to lift multiprocessor
execution to a CPU-local model [33]. The machine state
σ for our model consists of per-physical CPU private state
(e.g., CPU registers) and a global logical log, a serial list of
events generated by all CPUs throughout their execution. σ
does not explicitly model shared objects, including anything
stored in physical memory. Instead, events incrementally
convey interactions with shared objects, whose state may be
calculated by replaying the logical log. An event is emitted by
a CPU and appended to the log whenever that CPU invokes a
primitive that interacts with a shared object. All effects com-
ing from the environment are encapsulated by and conveyed
through an event oracle, which yields events emitted by other
CPUs when queried. To account for all possible concurrent
interleaving, how the event oracle synchronizes these events
is left abstract, its behavior constrained only by rely-guarantee
conditions [40]. CPUs need only query the event oracle (a
query move) before interacting with shared objects, since
its private state is not affected by these events. Querying
the event oracle will result in a composite event trace of the
events from other CPUs interleaved with events from the local
CPU. A local CPU makes a step via the CPU-local move.
For simplicity, we describe AbsMachine as a sequentially
consistent model – writes always take effect in program order,
and reads always read from the most recent write. Although
Arm supports relaxed memory, we prove that all shared
memory accesses in the KCore implementation are correctly
protected by spinlocks. Because the spinlocks use barriers
that prevent memory accesses from being reordered beyond
their critical sections, we can show that KCore only exhibits
sequentially consistent behavior. As a result, our guarantees
over KCore veriﬁed using a sequentially consistent model
still hold on Arm’s relaxed memory model. This proof is
beyond the scope of this paper.
Although the abstract machine model is speciﬁed in the
bottom machine model of our proof, each successive layer
implicitly has a machine model which is used to express how
events at that layer affect machine state. For example, each
layer has some notion of memory to support memory load
and store primitives. For many layers, most primitives and
their effect on the machine model at the overlay interface
are the same as those at the underlay interface. These
passthrough primitives and their effects on machine state
do not need to be respeciﬁed for each higher layer. On the
other hand, each layer may deﬁne new primitives based on