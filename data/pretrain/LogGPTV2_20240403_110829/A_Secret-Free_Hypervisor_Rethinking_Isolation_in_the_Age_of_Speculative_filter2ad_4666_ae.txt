### Meltdown and Performance Analysis

#### Map Cache Performance
Figure 10 illustrates the map cache hit rate as a function of size. The figure compares two 32-entry, 2-way set associative caches with different eviction policies: Random and LRU.

The analysis reveals that the overhead for Xen's default configuration is higher, particularly in benchmarks with random disk I/O patterns. For instance, the overheads are 0.97% for `pgbench`, 1.00% for `SQLite`, and 1.12% for code repository decompression. Conversely, network performance variations across setups do not significantly impact server benchmarks, as all maximal bandwidths have reached 10Gbps. This indicates that the server benchmarks are more likely to be execution-bound rather than network-I/O-bound. Consequently, no significant overhead is observed in HTTP server workloads. 

The Secret-Free (SF) configuration consistently shows minimal degradation across all HVM benchmarks, with an overall geometric mean overhead of only 0.2%, compared to 1.11% for the default configuration. XPTI, on the other hand, is generally unusable for server workloads, exhibiting a 50% overhead in HTTP servers and in the write-intensive LevelDB benchmark. PV default mitigations without XPTI also show a higher overhead due to doubled hypercall latency, although this overhead remains under 5%. In PV configurations, the secret-free version outperforms the default, with less than half the overall overhead. Importantly, our implementation mitigates Meltdown at a significantly lower cost than XPTI by avoiding page table swaps on hypervisor entry and exit.

#### Map Cache Parameters
We investigate several map cache parameters, including size, set associativity, and block size (superpage cache entries), using `pgbench` as the test application. The dominant factor is the cache size (number of entries). As shown in Figure 10, while the miss rate continues to decline as the cache size increases, an oversized map cache can negatively impact guest performance due to CPU cache contention. We determine that 32 entries is the optimal parameter for the map cache.

#### Vulnerability Matrix
Table IV presents the vulnerability matrix, comparing the baseline (un-mitigated), default mitigations, and Secret-Free Xen (with microarchitectural isolation). The secret-free hypervisor successfully thwarts any attack that attempts to circumvent permission or to mistrain the hypervisor for secret access. With microarchitectural isolation, we further guard against L1TF. Stack-pivoting using the direct map synonym succeeds in the baseline and default configurations but fails in the secret-free Xen due to the absence of the mapping, triggering a hypervisor page fault on the first access.

Our evaluation confirms that the secret-free Xen implementation correctly enforces the secret-free principle. Compared to other mitigations, the secret-free design provides defense-in-depth, guarding against future attacks and undiscovered microarchitectural vulnerabilities. Finding another means of information disclosure (either speculative or architectural) is not useful unless an attack is successfully mounted to bring secrets into the address space.

### Hyper-V (Type-I)
We explore a secret-free Hyper-V as an independent implementation. We focus on Xen for evaluation due to its open-source nature, which allows us to elaborate on code and implementation details. The secret-free Hyper-V has been deployed at scale on Azure, demonstrating the applicability of secret freedom as a generic isolation framework for Type-I hypervisors.

The direct map has been removed to avoid implicit mapping of secrets. We leverage vCPU-private areas to construct separate secret virtual address spaces for each guest thread. Hypervisor stacks, register frames, and other vCPU state are now placed in private mappings and are only visible to the current context. Due to the isolation of secrets, the hypervisor requests ephemeral mappings to access guest memory or cross-domain secrets. A major difference with secret-free Xen is the map cache pressure. The hypercall ABI of Hyper-V takes GPA instead of GVA for arguments in guest memory, avoiding a full manual 2-stage page table walk. We introduce the same optimization as Xen by identifying EPT pages as performance-critical non-secrets and promoting them to the global non-secret pool. Since only one temporary mapping is required to perform a GPA to HPA walk for hypercalls, we choose not to implement the map cache to avoid contending with the guest for CPU data caches.

Our platform supports nested virtualization capabilities on certain types of instances. A customer may launch L2 guests belonging to multiple security domains in the L1 hypervisor. To ensure strong isolation boundaries for different domains within a guest VM, we implement state scrubbing. When the hypervisor must copy guest secrets to complete an operation, it overwrites buffers with zeroes prior to exiting the L0 context. This ensures that secrets from the L1 guest hypervisor or L2 guest virtual processor state are not resident in the cache when switching between security domains in the L1 guest VM. We minimize the overhead by carefully tracking the memory that needs to be scrubbed.

The secret-free design in Hyper-V makes several existing mitigations redundant. Although core scheduling is required for microarchitectural isolation, we no longer need the scheduler to coordinate sibling entry and exit because secrets can no longer be fetched even in the hypervisor context. Similarly, exiting the hypervisor no longer requires flushing the L1D cache. We also remove unnecessary guards against poisonous guest branch predictor state on hypervisor entry, as it cannot cause any caching of secrets.

Overall, the SF implementation in Hyper-V incurs an overhead of only 1% (with secret data scrubbing). The implementation has been proven in production, and we have not received any reports from customers regarding performance degradation.

### bhyve (Type-II)
We apply the Secret-Free design to bhyve, a Type-II hypervisor from FreeBSD. One challenge prevents us from declaring our bhyve implementation fully secret-free. Type-II hypervisors reuse host device drivers to feed PV devices with data. Unfortunately, data buffers in host drivers are allocated and cached as kernel structures, globally mapped into all processes. I/O buffers are copied to and from guest memory, violating the Secret-Free principle by our definition.

We could implement such buffers via thread- or process-private mappings visible only to the driver daemons, but the challenge remains. Unlike Type-I hypervisors, we find it extremely difficult to introduce private or ephemeral APIs to all I/O buffers due to the close integration of components in a monolithic kernel, which would require substantial code rewrites. Thus, we believe a secret-free design would be less intrusive for Type-I hypervisors or micro-kernels. However, even before restructuring the kernel for back-end isolation, a mostly secret-free design still exposes a much-reduced attack surface and provides the same guarantees as Type-I implementations when guest I/O is hidden (e.g., by using passthrough devices or enabling disk encryption, which are common on cloud platforms).

### FreeBSD (UNIX Kernel)
The bhyve implementation applies the secret-free changes to the underlying FreeBSD kernel. The same challenge for bhyve remains, which is the difficulty in rewriting the API for all globally mapped I/O buffers and buffer caches in a monolithic kernel. Additionally, we identify one type of workload that needs optimization.

#### Process Creation Overhead
Processes and threads are significantly more dynamic than VMs and can be created and destroyed at high frequencies, often determining application performance. This high rate overwhelms the map cache due to the large number of page table copies during `fork()`. Unfortunately, mapping page table pages globally, like Xen's EPT optimization, is unhelpful. It reduces the run-time cost when accessing the pages but increases the overhead during process destruction, as IPI broadcasts are issued for TLB invalidation for changes in the global non-secret mapping range. We observed a slowdown of 38% in process creation micro-benchmark and 4% in FreeBSD kernel compilation (Table V), evaluated under bare-metal FreeBSD on the same hardware setup as Xen.

We implement a proof-of-concept optimization by reserving part of the kernel memory for non-secrets. A direct map range is reintroduced, but it only covers the non-secret pool. The pool is globally and permanently mapped by the kernel during early boot for page table pages. Table V shows that the optimization brings kernel build overhead below 1%. However, future work is needed to dynamically adjust the size of the pool (and the non-secret direct map) when the system is under memory pressure, as the reservation ratio cannot be optimally predetermined. We have one extreme where the system contains many copy-on-write pages (high page table to user memory ratio) and another extreme where memory is populated with superpage mappings (low ratio).

### Related Work
#### eXclusive Page Frame Ownership (XPFO)
XPFO targets the Linux direct map to reduce the kernel attack surface. Memory allocated to user processes is excluded from the direct map, ensuring unique ownership of the mapping. No kernel synonyms can be used for a user-space gadget. This approach suffers from the overhead of global map maintenance, as pages need to be scrubbed before being added back to the direct map and TLB shootdowns need to be broadcast when pages are allocated to users. In contrast, a secret-free design adopts an allow-list approach, maintaining a minimal secret-free address space at all times. Memory is only temporarily mapped in while the kernel accesses it, and the per-thread mapping infrastructure avoids TLB shootdown and associated scalability issues.

#### KPTI and XPTI
KPTI and XPTI address the isolation failure in CPU speculation paths, where speculative instructions are not restricted by the current privilege level. A partial page table is created for user processes, removing most of the kernel address range. System calls (or hypercalls) first jump to trampoline code to restore the full kernel mapping and switch back to the partial table when exiting kernel space. Our work demonstrates that a full kernel map is unnecessary in most (if not all) situations, as the cost of two page table swaps is intrusively expensive. A secret-free design handles secret access in private mappings or uses ephemeral mappings for guest memory and temporary access outside the current address space. Our evaluation shows that the kernel map does not need to be fully restored for the kernel to function efficiently.

#### Linux Kernel Address Space Isolation (KASI)
As more kernel isolation requirements emerge to defend against speculative vulnerabilities, Linux proposes a generic Kernel Address Space Isolation framework (KASI). For KVM isolation, it handles VMEXITs in a separate address space that exposes only the per-VM structures and Linux code and data, similar to the secret-free address space. However, its secret-free address space is limited to simple VMEXIT handling. Any access outside the minimal address space switches to the full kernel and halts sibling threads to ensure the kernel cannot be mistrained into bringing secrets to the shared L1 data cache. The performance of the framework is currently unknown.

#### Corevisor
Corevisor partitions the monolithic hypervisor into a trusted core and a large untrusted hostvisor. Several kernel components, including vCPU scheduling and the memory allocator, are deprivileged into the hostvisor for a minimal TCB. For PV I/O devices, the back-end drivers reside within the hostvisor instead of the privileged core. We welcome such separation of hypervisor components. SF has faced significant obstacles in introducing private APIs to a Type-II hypervisor atop a monolithic kernel, as every kernel module and device driver may need source-code modification. We anticipate that a corevisor design, with clear separation among its components (especially those interacting with guest secrets), can be easily retrofitted with secret-free abstractions for defense against a broad category of architectural and speculative attacks.

### Conclusion
We have presented the secret-free hypervisor, a design atop existing hardware for a minimal and secret-free address space. This technique is a defense-in-depth approach against several categories of vulnerabilities. The optimizations bring performance to a similar level as an unmitigated baseline. Compared to state-of-the-art mitigations, we improve performance by permitting speculations in situations where secrets are invisible. Our implementations suggest that the Secret-Free approach is applicable to multiple platforms and different types of kernels, providing a performant alternative to existing mitigations and a comprehensive framework against potential future attacks.