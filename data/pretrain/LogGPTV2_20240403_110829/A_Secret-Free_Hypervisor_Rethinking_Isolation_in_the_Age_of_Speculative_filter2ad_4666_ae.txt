Meltdown directly.
Fig. 10. Map cache hit rate versus size. Random and LRU denote 32-entry
2-way set associative caches with different eviction policies.
of 4.42%. The higher overhead from Xen default continues
for other benchmarks with random disk I/O patterns, namely
pgbench (0.97%), SQLite (1.00%) and code repo decom-
pression (1.12%). On the other hand,
the large variations
of network performance across setups in Section VII-D do
not manifest as their maximal bandwidths have all reached
10Gbps, meaning server benchmarks are still likely execution-
bound than network-I/O-bound. As a result, no signiﬁcant
overhead is seen in HTTP server workloads. SF continues to
show no noticeable degradation across all HVM benchmarks
and has a overall geomean at only 0.2% versus the default at
1.11%.
XPTI is generally unusable with server workloads, see-
ing 50% overhead in HTTP servers and in write-intensive
LevelDB benchmark. PV default mitigations without XPTI
have a noticeably higher overhead due to doubled hypercall
latency, although generally under 5%. For PV, the secret-free
version consistently outperforms the default conﬁguration, at
less than half of the overall overhead. More importantly, our
implementation mitigates Meltdown at a signiﬁcantly lower
cost than XPTI, because page table swaps are avoided on
hypervisor entry and exit.
F. Map cache performance
We explore several map cache parameters including size,
set associativity and block size (superpage cache entries). The
test application is pgbench.
We notice that the dominant factor is the cache size (number
of entries) as shown in Fig. 10. Even though the miss rate
continues to decline as the size grows, in practice an oversized
map cache begins to hurt guest performance due to CPU cache
contention. We choose 32 entries as the optimal parameter for
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
380
151.85120.06137.98143.40121.0795.00%100.00%105.00%110.00%115.00%120.00%apachelinux buildnumpyllvm buildleveldbnginxpgbenchsqlitedecompressgeomeandefault (HVM)SF (HVM)SF+EPT (HVM)default (PV)XPTI (PV)SF (PV)54.76%68.51%76.27%84.40%84.76%85.83%88.18%89.44%0.00%20.00%40.00%60.00%80.00%100.00%481632randomLRU64128Spectre-v1
Spectre-v2 Meltdown
L1TF



ret2dir



Baseline
Default
SF


*


*

*
*
* Speculative execution is still permitted. However, no secrets can
be fetched as only mappings to non-secrets exist.
TABLE IV
VULNERABILITY MATRIX
attack is run on baseline (un-mitigated), default mitigations
and Secret-Free Xen (w/ µarch isolation).
As shown in TABLE IV, the secret-free hypervisor success-
fully thwarts any attack that attempts to circumvent permission
or to mistrain the hypervisor for secret access. With µarch
isolation, we further guard against L1TF. While stack-pivoting
using the direct map synonym succeeds in baseline and in
default for retdir, it fails under the secret-free Xen because
the mapping is absent, triggering a hypervisor page fault on
the ﬁrst access.
From the evaluation, we are able to conﬁrm that our Xen im-
plementation correctly enforces the secret-free principle. The
secret-free design, when compared to other mitigations, pro-
vides defence-in-depth. This guards against future attacks and
undiscovered microarchitectural vulnerabilities of the same
categories. Finding another means of information disclosure
(either executed speculatively or architecturally) is not useful,
unless an attack is successfully mounted ﬁrst to bring secrets
into the address space.
VIII. HYPER-V (TYPE-I)
We explore a secret-free Hyper-V as an independent im-
plementation. We choose to focus on Xen for evaluation
because it is open-source, which allows us to elaborate on
code and implementation details. The secret-free Hyper-V has
been deployed at scale to Azure, showing the applicability of
secret freedom as a generic isolation framework for Type-I
hypervisors.
The direct map has been removed to avoid the implicit
mapping of secrets. We then leverage vCPU-private area to
construct separate secret virtual address spaces for each guest
thread. Hypervisor stacks, register frames and other vCPU
state are now placed in private mappings and are only visible
to the current context. Due to the isolation of secrets, the hy-
pervisor requests ephemeral mappings to access guest memory
or cross-domain secrets. Here, we ﬁnd a major difference with
secret-free Xen on map cache pressure. The hypercall ABI of
Hyper-V takes GPA instead of GVA for arguments in guest
memory, which avoids a full manual 2-stage page table walk.
We introduce the same optimization as Xen by identifying EPT
pages as performance-critical non-secrets and promoting them
to the global non-secret pool. As only one temporary mapping
is required to perform a GPA to HPA walk for hypercalls, we
choose not to implement the map cache to avoid contending
with the guest for CPU data caches.
Our platform exposes nested virtualization capabilities on
certain types of instances. A customer may launch L2 guests
belonging to multiple security domains in the L1 hypervisor.
To ensure a strong isolation boundary for different domains
within a guest VM, we further implement state scrubbing.
When the hypervisor must copy guest secrets to complete an
operation, it overwrites buffers with zeroes prior to exiting
the L0 context. This guarantees that secrets from L1 guest
hypervisor or L2 guest virtual processor state are not resident
in the cache when switching between security domains in the
L1 guest VM. We minimize the overhead by carefully tracking
the memory which needs to be scrubbed.
The secret-free design in Hyper-V makes several existing
mitigations redundant. Although core scheduling is required
for µarch isolation, we no longer ask the scheduler to co-
ordinate sibling entry and exit because secrets can no longer
be fetched even in hypervisor context. Similarly, exiting the
hypervisor no longer needs to ﬂush the L1D cache. We also
remove unnecessary guards against poisonous guest branch
predictor state on hypervisor entry as it cannot cause any
caching of secrets.
Together, we see an overall overhead of the SF implemen-
tation at only 1% (with secret data scrubbing). The implemen-
tation has been proven in production and we have not received
any reports from customers on performance degradation.
IX. BHYVE (TYPE-II)
We apply Secret-Free to bhyve, a Type-II hypervisor from
the FreeBSD OS. In this work, one challenge prevents us
from declaring our bhyve implementation secret-free. Type-II
hypervisors reuse host device drivers to feed PV devices with
data. Unfortunately, data buffers in host drivers are allocated
and cached as kernel structures, globally mapped into all
processes. I/O buffers are copied to and from guest memory,
which violate Secret-Free by our deﬁnition.
We may implement such buffers via thread- or process-
private mappings visible only to the driver daemons, but
the challenge remains. Unlike Type-I hypervisors, we ﬁnd it
tremendously difﬁcult to introduce private or ephemeral APIs
to all I/O buffers due to components being closely-intertwined
in a monolithic kernel, which requires a substantial amount of
code rewrite. Thus, we think a secret-free design would be less
intrusive for Type-I hypervisors or micro-kernels. However,
even before restructuring the kernel for back-end isolation,
a mostly secret-free design still exposes a much reduced
attack surface and provides the same guarantees as Type-I
implementations when guest I/O is hidden (for example, by
using passthrough devices or enabling disk encryption, which
are common on cloud platforms).
X. FREEBSD (UNIX KERNEL)
The bhyve implementation applies the secret-free changes
to the underlying FreeBSD kernel. The same challenge for
bhyve remains, which is the difﬁculty in rewriting the API
for all globally mapped I/O buffers and buffer caches in a
monolithic kernel. Further, we identify one type of workloads
that needs optimization.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
381
Slowdown
SF
Optimization
Process creation
38.44%
8.16%
TABLE V
FreeBSD kernel build
3.93%
0.85%
PROCESS CREATION OVERHEAD.
Processes and threads are signiﬁcantly more dynamic than
VMs and can be created and destroyed at high frequencies,
to the point where it determines the application performance.
Such a high rate overwhelms the map cache due to the huge
number of page table copies during fork(). Unfortunately,
mapping page table pages globally like Xen EPT optimization
is unhelpful. It reduces the run-time cost when accessing the
pages but increases the overhead during process destruction,
because IPI broadcasts are issued for TLB invalidation for
changes in the global non-secret mapping range. We have
observed a staggering slowdown of 38% in process creation
micro-benchmark and 4% in FreeBSD kernel compilation
(TABLE V), evaluated under baremetal FreeBSD on the same
hardware setup as Xen.
We implement a proof-of-concept optimization by reserving
part of the kernel memory for non-secrets. A direct map range
is reintroduced, but only covers the non-secret pool. The pool
is globally and permanently mapped by the kernel during
early boot for page table pages. TABLE V shows that the
optimization brings kernel build overhead below 1%. However,
future work is needed to dynamically adjust the size of the
pool (and the non-secret direct map) when the system is under
memory pressure because the ratio of the reservation cannot
be optimally pre-determined. We have one extreme where
the system may contain lots of copy-on-write pages (high
page table to user memory ratio) and another extreme where
memory is all populated with superpage mappings (low ratio).
XI. RELATED WORK
A. eXclusive Page Frame Ownership
XPFO targets the Linux direct map to reduce the kernel
attack surface. Memory allocated to user processes will be
excluded from the direct map, guaranteeing unique ownership
of the mapping. No kernel synonyms can be used for a user
space gadget.
This approach suffers from the overhead of global map
maintenance, because pages need to be scrubbed before being
added back to the direct map and TLB shootdowns need to
be broadcast when pages are allocated to user. In contrast,
a secret-free design adopts an allow-list rather than a deny-
list approach. It has a minimal secret-free address space at all
times and its isolation is not limited to the direct map. Memory
is only temporarily mapped in while the kernel accesses it, and
the per-thread mapping infrastructure avoids TLB shootdown
and their associated scalability issues.
B. KPTI and XPTI
KPTI and XPTI address the isolation failure in CPU spec-
ulation paths, where speculative instructions are not restricted
by current privilege level. A partial page table is created for
user processes that removes most of the kernel address range.
System calls (or hypercalls) ﬁrst jump to trampoline code to
restore the full kernel mapping, and switch back to the partial
table when exiting kernel space.
Our work demonstrates that a full kernel map in most (if
not all) situations is unnecessary, as the cost of two page
table swaps is intrusively expensive. A secret-free design either
handles secret access in private mappings, or uses ephemeral
mappings for guest memory and for temporary access outside
the current address space. Our evaluation has shown that the
kernel map does not have to be restored in its entirely for the
kernel to function efﬁciently.
C. Linux Kernel Address Space Isolation
As more kernel isolation requirements emerge to defend
against speculative vulnerabilities, Linux proposes a generic
Kernel Address Space Isolation framework (KASI) [31]. For
KVM isolation, it handles VMEXITs in a separate address
space that exposes only the per-VM structures and Linux code
and data, similar to the secret-free address space.
However, its secret-free address space is only limited to
simple VMEXIT handling. Any access outside the minimal
address space switches to the full kernel and halts sibling
thread to ensure the kernel cannot be mistrained into bringing
secrets to the shared L1 data cache. At
the
performance of the framework is unknown.
the moment,
D. Corevisor
Corevisor partitions the monolithic hypervisor into a trusted
core and a large untrusted hostvisor. Several kernel compo-
nents including vCPU scheduling and the memory allocator
are deprivileged into the hostvisor for a minimal TCB. For PV
I/O devices, the back-end drivers reside within the hostvisor
instead of the privileged core [39].
We welcome such separation of the hypervisor components.
SF has seen tremendous obstacles in introducing private APIs
to a Type-II hypervisor atop a monolithic kernel, because every
kernel module and device driver may potentially need source-
code modiﬁcation. We anticipate that a corevisor design, with
clear separation among its components (especially the ones
interacting with guest secrets), can be easily retroﬁtted with
secret-free abstractions for defense against a broad category
of architectural and speculative attacks.
XII. CONCLUSION
We have presented the secret-free hypervisor, a design
atop existing hardware for a minimal and secret-free address
space. We have shown that this technique is a defence-in-
depth approach against several categories of vulnerabilities.
The optimizations bring the performance to a similar level with
an unmitigated baseline. Compared with state-of-the-art miti-
gations, we improves performance by permitting speculations
in situations where secrets are invisible. Our implementations
suggest that the Secret-Free approach is applicable to multiple
platforms and different
types of kernels, as a performant
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:11:47 UTC from IEEE Xplore.  Restrictions apply. 
382
alternative to existing mitigations and as a comprehensive
framework against potential future attacks.
REFERENCES
[1] Z. Durumeric, F. Li, J. Kasten, J. Amann, J. Beekman,
M. Payer, N. Weaver, D. Adrian, V. Paxson, M. Bailey, and
J. A. Halderman, “The Matter of Heartbleed,” in Proceedings
of the 2014 Conference on Internet Measurement Conference,
ser.
IMC ’14. New York, NY, USA: Association for
Computing Machinery, 2014, p. 475–488. [Online]. Available:
https://doi.org/10.1145/2663716.2663755
[2] A. C. Mary, “Shellshock Attack on Linux Systems – Bash,” in
International Research Journal of Engineering andTechnology,
vol. 2. Tamilnadu, India: IRJET, 2015, pp. 1322–1325.
[3] B. Bierbaumer, J. Kirsch, T. Kittel, A. Francillon, and A. Zarras,
“Smashing the Stack Protector for Fun and Proﬁt,” in SEC,
2018.
[4] “CVE-2021-33910,” last visited on 2021-10-12. [Online]. Avail-
able: https://access.redhat.com/security/cve/cve-2021-33910
[5] M. Tran, M. Etheridge, T. Bletsch, X. Jiang, V. Freeh, and
P. Ning, “On the Expressiveness of Return-into-Libc Attacks,”