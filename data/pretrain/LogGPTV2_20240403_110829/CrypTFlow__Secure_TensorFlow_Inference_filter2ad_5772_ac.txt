have adopted the convention of expressing DNN layers us-
ing the MaxPool(ReLU(·)) idiom. For protocols like Porthos
and SecureNN [79] that reduce ReLU and MaxPool to se-
cure comparison protocols, ReLU(MaxPool(·)) can be much
more efﬁcient than MaxPool(ReLU(·)) as this signiﬁcantly
reduces the number of comparisons. As opposed to SecureNN,
where this was done manually, we have built a peephole
optimization pass on HLIL that
replaces occurrences of
MaxPool(a, b, ReLU(A)); with ReLU(MaxPool(a, b, A));. For
example, if the input matrix A has dimensions 112× 112× 64
and we compute a MaxPool with 2 × 2 windows. Then, the
output matrix has dimensions 56 × 56 × 64. Hence, the latter
needs to compute only one fourth the number of ReLUs
compared to the former. In this case, the optimized code is
over 3× better in communication and over 2× faster in our
experimental setup (Section VI).
2) Counting Scale Down operations: We describe an anal-
ysis to count the number of scale down operations in an LLIL
code. The analysis uses an environment ρ that maps tensors
to the number of elements they contain. This environment
is populated using variable declarations in the code. The
analysis makes a single pass over main and for each call
ScaleDown(A, s) accumulates ρ(A) into a counter. The ﬁnal
value of the counter provides the number of scale down
operations in the code.
Note that this analysis is easy to describe as the LLIL code
contains dimensions of all the tensors explicitly. Hence, the
compiler can statically populate ρ. This analysis is impossible
to perform on the TensorFlow Python code as the sizes of
tensors are unknown at compile time.
IV. PORTHOS
We now describe Porthos, our improved secure 3PC proto-
col that provides semi-honest security against one corrupted
party and privacy against one malicious corruption. The notion
of privacy against malicious corruption (introduced by Araki
et al. [10]) informally guarantees that privacy of inputs hold
even against malicious party as long as none of the parties par-
ticipating in the protocol learn the output of the computation
(this is relevant, for example, when computation is ofﬂoaded to
servers). Porthos builds upon SecureNN [79] but makes crucial
modiﬁcations to reduce communication. We ﬁrst describe our
protocols that reduce communication and summarize concrete
improvements in Table III.
We reduce communication for both linear as well as non-
linear layers of DNNs. Linear layers include fully connected
layers as well as convolutional
layers. We improve the
communication for convolutional layers and our optimization
gains get better with larger ﬁlter sizes. With regards to
non-linear layers (ReLU and MaxPool), we modify how two
of the protocols in SecureNN are used – ComputeMSB and
ShareConvert. As we explain below, this directly translates
to better communication for both ReLU and MaxPool
computations. At a very high level, we trade communication
with compute by modifying the way certain shares are
generated in the protocol.
In
secure
SecureNN,
Convolution.
of
convolutional layers is done by reducing them to a (larger)
matrix multiplication. As
2-dimensional
convolution of a 3 × 3 input matrix X (with single input
channel and stride 1) with a ﬁlter Y of size 2 × 2 reduces to
a matrix multiplication as follows:
computation
example,
an
⎛
⎝
⎡
⎣x1 x2 x3
x4 x5 x6
x7 x8 x9
⎤
⎦ ,
(cid:8)
Conv2d
(cid:9)⎞
⎠ =
y2
y4
y1
y3
⎡
⎢⎢⎣
⎤
⎥⎥⎦ ×
⎡
⎢⎢⎣
⎤
⎥⎥⎦
y1
y2
y3
y4
x1 x2 x4 x5
x2 x3 x5 x6
x4 x5 x7 x8
x5 x6 x8 x9
In the above matrix multiplication, we call the left matrix
(cid:2)) and the
(derived from X) as the “reshaped input” (say, X
(cid:2)).
right matrix (derived from Y ) as the “reshaped ﬁlter” (say, Y
The matrix multiplication is computed securely using a matrix
Beaver triple [13], [62] based protocol. Later, the output can
be reshaped to get the output of convolution in correct shape.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:36:59 UTC from IEEE Xplore.  Restrictions apply. 
341
MatMul(int[L][M] A, int[M][N] B, int[L][N] C) Multiply two tensors A and B and store results in C
MatAdd(int[L][M] A, int[L][M] B, int[L][M] C)
Conv(int[H][W][CI] A, int[FH][FW][CI][CO] F)
Avg/Max Pool(a, b, int[H][W][C] A)
ArgMax(int[M] A)
FusedBatchNorm(int[K][L][M][N] A, int[N] B, int[N] C)
ReLU(int[M][N] A)
ScaleDown(int[M][N] A, k)
Add two tensors A and B into C
Convolve a tensor A with ﬁlter F
Apply a stencil that computes the average/max value in windows of size a × b of tensor A.
Compute the index with maximum value in A
Returns ∀k, l, m, n.B[n] × A[k][l][m][n] + C[n]
Returns ∀i, j.Max(A[i][j], 0)
Arithmetic right shift each entry of A with k.
TABLE II: Share manipulating functions. These have been simpliﬁed for exposition by suppressing parameters such as padding
and strides. For comprehensive signatures, see https://www.tensorﬂow.org/api docs/python/tf/.
(cid:2) (i.e., q2f 2) in SecureNN.
0 and (cid:7)x(cid:8)t
0 = r and (cid:7)x(cid:8)t
In this protocol, matrices being multiplied are masked by
random matrices of same size and communicated and hence,
the communication grows with the size of the matrices. We
observe that this is quite wasteful for convolution because the
reshaped input image (the ﬁrst matrix in multiplication) has
many duplicated entries (e.g., x2 in row 1 and row 2) that get
masked by independent random values. Let size of X be m×m
(cid:2) is q2× f 2, where
and size of Y be f × f. Then, the size of X
q = m − f + 1. In Porthos, we optimize the size of matrix-
based Beaver triples for convolution by exploiting the structure
of re-use of elements as the ﬁlter moves across the image. At
a high level, we pick random matrix of size matching X for
masking and communication only grows with size of X (i.e.,
m2) instead of X
Before, we describe our optimized protocol, we set up some
notation. Let (cid:7)x(cid:8)t
1 denote the two shares of a 2-out-
of-2 additive secret sharing of x over Zt – in more detail, pick
$←− Zt, set (cid:7)x(cid:8)t
1 = x− r (mod t). (cid:7)x(cid:8)t denotes
r
a sharing of x over Zt. Reconstruction of a value x from its
shares x0 and x1 is simply x0 + x1 over Zt. This generalizes
to larger dimensions - e.g. for the m× n matrix X, (cid:7)X(cid:8)t
0 and
(cid:7)X(cid:8)t
1 denote the matrices that are created by secret sharing the
elements of X component-wise (other matrix notation such as
Reconstt(X0, X1) are similarly deﬁned).
Let Conv2dm,f denote a convolutional layer with input
m × m, 1 input channel, a ﬁlter of size f × f, and 1
output channel. Our protocol for Conv2dm,f is described
in Algorithm 1, where L = 2(cid:2), (cid:4) = 64. Algorithms
ReshapeInput, ReshapeFilter and ReshapeOutput are used to
reshape input, ﬁlter and output as described above and are
formally described in Appendix A. Parties P0 and P1 start
with shares of input matrix X and ﬁlter Y over ZL That is,
Pj holds ((cid:7)X(cid:8)L
j ) for j ∈ {0, 1}. In SecureNN, P0 ﬁrst
j ,(cid:7)Y (cid:8)L
reshapes (cid:7)X(cid:8)L
(cid:2)(cid:8)L
0 into (cid:7)X
0 by running ReshapeInput. Then, it
(cid:2)(cid:8)L
picks a random matrix (cid:7)A
(cid:2) and sends
0 of same size as X
(cid:2)(cid:8)L
(cid:2)(cid:8)L
0 = (cid:7)X
0 − (cid:7)A
(cid:2)(cid:8)L
(cid:7)E
0 to P1 that requires communicating
q2f 2 elements. In Porthos, we optimize this as follows: P0
picks a random matrix (cid:7)A(cid:8)L
0 of same size as X (Step 1) and
0 − (cid:7)A(cid:8)L
sends (cid:7)E(cid:8)L
0 to P1 (Step 4) that requires
communicating m2 elements only. Later, parties can reshape
(cid:2). We reduce the communication by P1 in
E locally to get E
a symmetric manner. Concretely, we reduce communication
from (2q2f 2 + 2f 2 + q2)(cid:4) in SecureNN to (2m2 + 2f 2 + q2)(cid:4).
This algorithm can be easily generalized to the setting where
there are i input ﬁlters, o output ﬁlters, and different stride
0 = (cid:7)X(cid:8)L
and padding parameters.
, Y ∈ Z
0 ,(cid:7)Y (cid:8)L
0 ) and P1 holds ((cid:7)X(cid:8)L
f×f
L .
(cid:7)Conv2dm,f (X, Y )(cid:8)L
Algorithm 1 3PC protocol for Conv2dm,f
Input: P0 holds ((cid:7)X(cid:8)L
where X ∈ Zm×m
L
Output: P0
gets
(cid:7)Conv2dm,f (X, Y )(cid:8)L
1 .
Common Randomness: P0 & P1 hold shares of a zero matrix
U of dimension q×q, q = m−f +1 . P0 & P2 hold a common
PRF key k0, and P1 & P2 hold a common PRF key k1.
1 ,(cid:7)Y (cid:8)L
1 ),
and P1
gets
0
L
L
f×f
L
q×q
L .
0 ∈ Z
(cid:7)A(cid:8)L
(cid:7)A(cid:8)L
1) P0 & P2 use PRF key k0 to generate random matrices
and (cid:7)C(cid:8)L
0 ∈ Z
, (cid:7)B(cid:8)L
2) P1 & P2 use PRF key k1 to generate random matrices
1 ∈ Z
and (cid:7)B(cid:8)L
f×f
L .
0 + (cid:7)A(cid:8)L
1 and B = (cid:7)B(cid:8)L
0 + (cid:7)B(cid:8)L
3) P2 computes A = (cid:7)A(cid:8)L
1 .
(cid:2)
= ReshapeInput(A) and B
= ReshapeFilter(B).
(cid:2) − (cid:7)C(cid:8)L
(cid:2) · B
0 and sends it to P1.
j = (cid:7)X(cid:8)L
4) For j ∈ {0, 1}, Pj computes (cid:7)E(cid:8)L
j and
j and sends to Pj⊕1.
0 ∈ Zm×m
1 ∈ Zm×m
(cid:2)
Let A
P2 computes (cid:7)C(cid:8)L
(cid:7)F(cid:8)L
1 = A
− (cid:7)B(cid:8)L
− (cid:7)A(cid:8)L
5) P0 & P1 reconstruct E and F using exchanged shares.
{0, 1}, Pj
computes
6) For
=
ReshapeInput((cid:7)X(cid:8)L
(cid:2)
ReshapeInput(E),
j ), E
=
(cid:7)Y
(cid:2)(cid:8)L
j = ReshapeFilter((cid:7)Y (cid:8)L
(cid:2)
j ), F
= ReshapeFilter(F ).
7) For j ∈ {0, 1}, Pj computes (cid:7)Z
(cid:2)(cid:8)L
j = −jE
(cid:2)(cid:8)L
·
j + (cid:7)U(cid:8)L
(cid:2)(cid:8)L
j + (cid:7)C(cid:8)L
(cid:2)
j .
{0, 1}, Pj
∈
(cid:2)(cid:8)L
j ).
F
ReshapeOutput((cid:7)Z
(cid:2)·F
(cid:7)Z(cid:8)L
+(cid:7)X
+ E
j
(cid:2) · (cid:7)Y
outputs
j = (cid:7)Y (cid:8)L
∈
j
8) For
(cid:7)X
(cid:2)(cid:8)L
j
j
=
j
j
(cid:2)
j
Activation Functions. In SecureNN protocols for computing
activations such as ReLU and MaxPool start with parties P0
and P1 having shares of values over L = 264. For both of
these, parties run a protocol called ComputeMSB to evaluate
most signiﬁcant bit (MSB) of secret values. This protocol
require shares over L − 1. So parties run a protocol called
ShareConvert to convert shares over L to shares over L − 1.
Both protocols ComputeMSB and ShareConvert require P2 to
send fresh shares of a value to P0 and P1. In SecureNN, both
of these shares were picked by P2 and explicitly communi-