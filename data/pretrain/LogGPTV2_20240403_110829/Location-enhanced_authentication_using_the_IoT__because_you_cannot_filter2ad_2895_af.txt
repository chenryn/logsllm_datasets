[31] M. Jakobsson, E. Shi, P. Golle, and R. Chow. Implicit
authentication for mobile devices. In Proceedings of the 4th
USENIX Conference on Hot Topics in Security (HotSec),
2009.
[32] A. Kalamandeen, A. Scannell, E. de Lara, A. Sheth, and
A. LaMarca. Ensemble: cooperative proximity-based
authentication. In Proceedings of the 8th international
conference on Mobile systems, applications, and services,
pages 331–344. ACM, 2010.
[33] H. Khan, A. Atwater, and U. Hengartner. A comparative
evaluation of implicit authentication schemes. In
Proceedings of the International Symposium on Research in
Attacks, Intrusions and Defenses (RAID), pages 255–275,
2014.
[34] H. Khan, A. Atwater, and U. Hengartner. Itus: An implicit
authentication framework for Android. In Proceedings of the
20th Annual International Conference on Mobile Computing
and Networking (MobiCom), pages 507–518, 2014.
[35] J. Krumm. A survey of computational location privacy.
Personal and Ubiquitous Computing, 13(6):391–399, 2009.
[36] J. Krumm and E. Horvitz. Predestination: Inferring
destinations from partial trajectories. In UbiComp 2006:
Ubiquitous Computing, pages 243–260. Springer, 2006.
[37] S. L. Lau and K. David. Movement recognition using the
accelerometer in smartphones. In Future Network and
Mobile Summit, pages 1–9, June 2010.
[38] L. Liao, D. J. Patterson, D. Fox, and H. Kautz. Learning and
inferring transportation routines. Artiﬁcial Intelligence,
171(5):311–331, 2007.
[39] Y. Lindell and B. Pinkas. Secure multiparty computation for
privacy-preserving data mining. IACR Cryptology ePrint
Archive, 2008:197, 2008.
[40] S. Mare, A. Molina-Markham, C. Cornelius, R. Peterson,
and D. Kotz. ZEBRA: Zero-effort bilateral recurring
authentication. In Proceedings of IEEE Symposium on
Security and Privacy, May 2012.
[41] C. Marforio, N. Karapanos, C. Soriente, K. Kostiainen, and
S. Capkun. Smartphones as practical and secure location
veriﬁcation tokens for payments. In Proceedings of the
Network and Distributed System Security Symposium
(NDSS), 2014.
[42] Microsoft. Event hubs. Microsoft Azure.
http://azure.microsoft.com/en-us/services/event-hubs/.
[43] D. Naccache, R. Géraud, H. Ferradi, and A. Tria. When
organized crime applies academic results: a forensic analysis
of an in-card listening device . Journal of Cryptographic
Engineering, pages 1–11, Oct 2015.
[44] P. Paillier. Public-key cryptosystems based on composite
degree residuosity classes. In J. Stern, editor, Advances in
Cryptology - EUROCRYPT ’99, International Conference on
the Theory and Application of Cryptographic Techniques,
Prague, Czech Republic, May 2-6, 1999, Proceeding, volume
1592 of Lecture Notes in Computer Science, pages 223–238.
Springer, 1999.
[45] V. Pappas, V. P. Kemerlis, A. Zavou, M. Polychronakis, and
A. D. Keromytis. CloudFence: Data ﬂow tracking as a cloud
service. In Proceedings of the International Symposium on
Research in Attacks, Intrusions and Defenses (RAID),
October 2013.
[46] F. Park, C. Gangakhedkar, and P. Traynor. Leveraging
cellular infrastructure to improve fraud prevention. In
Proceedings of the Annual Computer Security Applications
Conference (ACSAC), pages 350–359, December 2009.
[47] N. B. Priyantha. The Cricket Indoor Location System. PhD
thesis, MIT, 2005.
[48] K. B. Rasmussen, M. Roeschlin, I. Martinovic, and
G. Tsudik. Authentication using pulse-response biometrics.
In Proceedings of the Network and Distributed System
Security Symposium (NDSS), February 2014.
[49] I. Rhee, M. Shin, S. Hong, K. Lee, S. J. Kim, and S. Chong.
On the levy-walk nature of human mobility. IEEE/ACM
transactions on networking (TON), 19(3):630–643, 2011.
[50] O. Riva, C. Qin, K. Strauss, and D. Lymberopoulos.
Progressive authentication: Deciding when to authenticate
on mobile phones. In Proceedings of the 21st USENIX
Security Symposium, pages 301–316, 2012.
[51] R. L. Rivest, L. Adleman, and M. L. Dertouzos. On data
banks and privacy homomorphisms. Foundations of secure
computation, 32(4):169–178, 1978.
[52] V. Roth, K. Richter, and R. Freidinger. A PIN-entry method
resilient against shoulder surﬁng. In Proceedings of the ACM
Conference on Computer and Communications Security
(CCS), pages 236–245, 2004.
[53] S. Scellato, M. Musolesi, C. Mascolo, V. Latora, and A. T.
Campbell. Nextplace: a spatio-temporal prediction
framework for pervasive systems. In Pervasive computing,
pages 152–169. Springer, 2011.
[54] F. Schuster, M. Costa, C. Fournet, C. M. Peinado,
G. Mainar-Ruiz, and M. Russinovich. VC3: Trustworthy
data analytics in the cloud using SGX. In Proceedings of the
IEEE Symposium on Security and Privacy, May 2015.
[55] S. F. Shahandashti, R. Safavi-Naini, and N. A. Safa.
Reconciling user privacy and implicit authentication for
mobile devices. Comput. Secur., 53(C):215–233, Sept. 2015.
[56] A. Shamir. How to share a secret. Commun. ACM,
22(11):612–613, 1979.
[57] E. Shi, Y. Niu, M. Jakobsson, and R. Chow. Implicit
authentication through learning user behavior. In
Proceedings of the International Conference on Information
Security (ISC), pages 99–113, 2011.
[58] starbug. Fingerprint biometrics hacked again. Chaos
Communication Congress (31C3), December 2014.
http://www.ccc.de/en/updates/2014/ursel.
[59] M. Terrovitis. Privacy preservation in the dissemination of
location data. SIGKDD Explorations, 13(1):6–18, 2011.
[60] The Telegraph – UK. Three quarters of cars stolen in France
‘electronically hacked’. http://www.telegraph.co.uk/news/
worldnews/europe/france/11964140/Three-quarters-of-cars-
stolen-in-France-electronically-hacked.html, October 2015.
[61] US AirForce. GPS Accuracy.
"http://www.gps.gov/systems/gps/performance/accuracy/".
[62] M. van Dijk, C. Gentry, S. Halevi, and V. Vaikuntanathan.
Fully homomorphic encryption over the integers. IACR
Cryptology ePrint Archive, 2009:616, 2009.
[63] R. Verdult, F. D. Garcia, and B. Ege. Dismantling Megamos
crypto: Wirelessly lockpicking a vehicle immobilizer. In
Supplement to the 22nd USENIX Security Symposium
(USENIX Security 13), pages 703–718, Washington, D.C.,
2015.
[64] R. Want, A. Hopper, V. Falcão, and J. Gibbons. The active
badge location system. ACM Trans. Inf. Syst., 10(1):91–102,
January 1992.
[65] A. C. Yao. How to generate and exchange secrets (extended
abstract). In 27th Annual Symposium on Foundations of
Computer Science, Toronto, Canada, 27-29 October 1986,
pages 162–167. IEEE Computer Society, 1986.
[66] G. Zhong, I. Goldberg, and U. Hengartner. Louis, lester and
pierre: Three protocols for location privacy. In N. Borisov
and P. Golle, editors, Privacy Enhancing Technologies, 7th
International Symposium, PET 2007 Ottawa, Canada, June
20-22, 2007, Revised Selected Papers, volume 4776 of
Lecture Notes in Computer Science, pages 62–76. Springer,
2007.
APPENDIX
A. PROOF OF PRIVACY-PRESERVATION
A.1 Background
In the following we recall brieﬂy some fundamental concepts.
DEFINITION 1
R is said to be negligible if
(NEGLIGIBLE FUNCTIONS). A function  : N →
∀ c ∈ N. ∃ nc ∈ N. ∀n≥nc |(n)| ≤ n
−c
That is,  decreases faster than the inverse of any polynomial.
DEFINITION 2
(INDISTINGUISHABILITY). The two random
variables X(n, a) and Y (n, a) (where n is a security parameter
and a represents the inputs to the protocol) are called computa-
c≡ Y if for a probabilistic
tionally indistinguishable and denoted X
polynomial time (PPT) adversary A the function δ(n) is negligible:
δ(n) = | Pr[A(X(n, a)) = 1] − Pr[A(Y (n, a)) = 1]|
DEFINITION 3
(SEMANTIC SECURITY). A public key encryp-
tion scheme E issemantically secure or IND-CPA secure if the ad-
vantage of any PPT adversary of winning the game IND-CPA in
Figure 8 is negligible. The game is won if an attacker can construct
the procedures A1 and A2 such that b = b(cid:48) with non-negligible
probability. If a cryptosystem is IND-CPA secure, it is also mul-
tiple message IND-CPA [10]. Multiple message IND-CPA is for-
malized by the game MM-IND-CPA in Figure 8, where k is polyno-
mially bounded by the security parameter. Essentially, this means
that any any ciphertext or order of ciphertexts is computationally
indistinguishable from their plaintexts.
Game IND-CPA :
(m0, m1) ← A1;
b $← {0, 1};
b(cid:48) ← A2(EK (mb));
return b = b(cid:48)
Game MM-IND-CPA :
((m0
0, ..., m0
k), (m1
b $← {0, 1};
b(cid:48) ← A2(EK (mb
return b = b(cid:48)
0, ..., m1
k)) ← A1;
0), ..., EK (mb
k));
Figure 8: IND-CPA and MM-IND-CPA
A.2 Privacy against semi-honest adversaries
Our privacy deﬁnition follows the standard deﬁnitions of secure
multi-party computation in the semi-honest adversarial model [25,
39], but is here simpliﬁed for the case with two parties. For two
parties A and B, where A has inputs −→x and B inputs −→y , the
framework formalizes the output of a protocol as f (−→x ,−→y ) =
(g(−→x ,−→y ), h(−→x ,−→y )). The function f is called the functionality
of the protocol,
The functions g and h are functions describing all outputs pre-
sented to A and B from the execution of the protocol, respectively.
f, giving the tuple of the parties’ joint output, is called the func-
tionality of the protocol.
DEFINITION 4
(PRIVACY). Privacy within the standard frame-
work (for deterministic functionalities) holds when the overall knowl-
edge of each party after the execution of the protocol, called the
party’s view, can be computed from the inputs and outputs of that
party. This is called that the view can be simulated. That is, for the
two-party case with A and B as described above, one must show
that:
{SA(−→x , g(−→x ,−→y ))} c≡ {viewA(−→x ,−→y )}
{SB(−→y , h(−→x ,−→y ))} c≡ {viewB(−→x ,−→y )}
where SA and SB are the the simulators for A and B, respectively.
A.3
Instantiation for the proposed solution
For our purposes, let A be the hub, B be the site, and f be the
functionality of the sub-protocol described in Section 6, g returns
only the distance between the two points, and that h returns noth-
ing. The site has no inputs, the hub on the other hand has the inputs
p = (bx, by, bd,(cid:74)x1(cid:75),(cid:74)y1(cid:75),(cid:74)x2(cid:75),(cid:74)y2(cid:75)). Both parties also have ran-
domness as inputs for each encryption, but this is omitted here as it
is trivial to simulate.
The protocol has two round-trips. During the ﬁrst round-trip the
site learns the blinded value x(cid:48) = x1− x2 + bx and y(cid:48) = y1− x2 +
by, and the hub learns two ciphertexts. During the second round-
trip the site learns the blinded d(cid:48) = d2 + bd, and the hub learns d2.
The view of the two parties can be given as:
viewhub = (p,(cid:74)x
(cid:48)
viewsite = (x
, y
(cid:48)2(cid:75),(cid:74)y
(cid:48)
(cid:48)
, d
)
(cid:48)2(cid:75), d
(cid:48)
)
THEOREM 1. The protocol privately discloses the distance for
the hub according to Deﬁnition 4.
PROOF. To prove that Theorem 1 holds, we need to show that
there exist two functions Shub and Ssite such that:
{Shub(p, g(p,{}))} c≡ {(p,(cid:74)x
{Ssite({}, h(p,{}))} c≡ {(x
, y
(cid:48)
(cid:48)
(cid:48)2(cid:75),(cid:74)y
(cid:48)2(cid:75), d
(cid:48)
)}
(cid:48)
)}
, d
Blinded values are indistinguishable from a random sample in
M when the blinding used is unknown. Thus, we deﬁne
Ssite = (α, β, γ)
where α, β, and γ are independent and uniformly random variables
in M.
Ciphertexts are easy to simulate for any semantically secure cryp-
tographic system for any principal holding the public key. In fact,
for any list of plaintexts an arbitrary list of ciphertexts of the same
length can be used, as per MM-IND-CPA in Deﬁnition 3. Since
g(p,{}) = d, to create a simulation of d(cid:48), one can simply use
d2 + bd. The simulator for the hub is thus easily deﬁned as
Shub(p, g(p,{})) = (p, E(0), E(0), d2 + bd)