reasons why a vendor might do so. Obtaining consent is not the
only lawful basis for data processing. The fourth and fifth items
are; why are vendors collecting personal data (I4), and what is their
legal basis for doing so (I5).
One aspect that has not been considered in existing research is
the additional effort required to reject data processing compared
to accepting it. In most experiments, artificial dialogues are pre-
installed on the subject’s machine or loaded from a single source. In
practice, users may already be habituated to the standardized CMP
dialogs, but dialogs may need to send consent decisions to multiple
vendors which incurs additional waiting time. This motivates our
items at the user-interface; how long does it take CMPs to distribute
consent decisions (I6), and to what extent does the user’s dialog
interaction time vary depending on which privacy preferences are
expressed (I7).
3.2 Measurement Methodology
Large-Scale Web Measurement. To measure the prevalence of
consent prompts longitudinally, we analyze automated browser
crawls recorded by the Netograph web measurement platform1
described in Figure 3. Netograph was not built exclusively for this
research project and exhibits some unique properties compared to
existing methods. Most prominently, instead of sampling from a
particular toplist at one point in time, our crawlers are constantly
seeded with new URLs shared on social media platforms.
This approach is not a design choice made specifically for our
research, but useful in our context as measurements are not limited
to a domain’s landing page (https://example.com/) but also cover
arbitrary subsites (https://example.com/foo?bar). Recent work has
shown that subsites show a significant different behavior and an
increase of privacy-invasive techniques [55].
Netograph ingests a live feed of social media posts, extracts all
URLs, and submits them into a capture queue. URLs are visited once
within a couple of minutes after submission. Crawls are performed
on virtual machines in US and EU data centers of a large public cloud
provider. 50% of crawls are done from within the EU, each URL is
assigned randomly. Websites are opened using Google Chrome on
Linux with its current default user agent2, a desktop resolution of
1024×800, and en-US as the preferred browser language. All other
settings are set to their defaults: Third party cookies are allowed,
the “Do not Track” HTTP header is not set, and Flash is disabled.
Due to the large volume of URLs, Netograph crawls with relatively
aggressive timeouts, which are discussed further in Section 3.5.
For every capture, Netograph collects the following data points
using custom browser instrumentation. First, HTTP headers are
logged for all requests and responses. Additionally, connection-
related metadata such as IP addresses and TLS certificate chains
are stored. For every domain in a capture, its relation to the main
page, all cookies, IndexedDB, LocalStorage, SessionStorage and
WebSQL records are saved. Finally, a screenshot of the visible area
(without scrolling) is taken. Netograph does not store page contents
due to storage constraints. All crawl data is stored in a central
database, which can be queried using a custom API. As of May 2020,
this database stores 161,214,215 captures or about 23 billion HTTP
requests.
1https://netograph.io/
2Currently Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML,
like Gecko) Chrome/83.0.4103.61 Safari/537.36.
319
IMC ’20, October 27–29, 2020, Virtual Event, USA
Maximilian Hils, Daniel W. Woods, and Rainer Böhme
Twitter Reddit
...
social media crawling
≈ 5,000,000
URLs / month
Crawler (EU Cloud)
Crawler (US Cloud)
Tranco 10k Toplist
10,000
URLs / week
Crawler (EU Univ.)
≈ 550 kB
metadata / capture
≈ 1.9 MB
content / capture
Capture
Database
⊲⊳
CMP indicators
Analysis
(normalized
by toplist)
Figure 3: The Netograph measurement platform collects a realtime stream of URLs shared on social media and crawls them
using Google Chrome. Custom browser instrumentation extracts metadata such as HTTP requests and cookies. We match
captures with CMP indicators and use the Tranco toplist to normalize website popularity.
Toplist-Based Web Measurement. To make comparisons with re-
lated work, we have set up an additional Netograph-based crawling
infrastructure for this study based on an internet toplist. In our
analysis, we use the top 10k entries from the Tranco list created
on 30 January 20203, which aggregates the ranks from the lists
provided by Alexa, Cisco Umbrella, Majestic, and Quantcast [44].
This sample size is in the order of magnitude of previous studies
(see # domains in Figure 1).
We first converted the Tranco list of domains to a list of URLs
that can be crawled. For each domain, we attempted to establish
a TLS connection to www.domain on port 443 and validate the
certificate hostname using Mozilla’s trust store. If the certificate
is valid, we used https://www.domain/ as the seed URL for crawls.
Otherwise, we attempted to open a TCP connection on port 80
and used http://www.domain/ on success. If this also failed, we
used http://domain/ as the seed URL. We repeated this process
three times over a week in order to catch temporarily unavailable
domains.
Next, we crawled every URL in the toplist six times in immediate
succession: First, we visited the website from a European university
network using our crawler’s default configuration. Second, we
repeated this capture with an extended timeout. Third and fourth,
we also captured with both German and British English as the
preferred browser language. Finally, we submitted the same URLs
to Netograph’s task queues in the US and EU cloud as a control
group. We retried all unsuccessful captures three times over the
span of a week.
For all toplist crawls, we additionally stored the browser’s DOM
tree including the computed CSS styles. We also recorded a full-page
screenshot (including scrolling). These extended features are not
stored for the social media dataset due to their storage requirements.
Prevalence and Customization of CMPs (I1–I3). In the second
part of our analysis, we measure the prevalence of CMPs using our
crawl data. This involves extracting the final effective second-level
domain (by which we want to count), detecting the CMP in use, and
interpolating missing data. For this analysis we restrict ourselves
to six CMPs: The five major players already identified by Nouwens
et al. [39] and LiveRamp, a new entrant that launched in December
2019.
We measure the market share of CMPs by determining the num-
ber of domains they are active on. As about 11% of all crawls in-
clude top-level domain redirects, taking the domain from a seed
URL would be imprecise. Instead, we extract the domain from the
final website address as it would be shown in the browser’s address
3Available at https://tranco-list.eu/list/K8JW.
bar. We normalize this domain to the effective second-level domain
using the Public Suffix List [13], which contains all suffixes under
which internet users can directly register names. For example, a
capture may start with https://tinyurl.com/... as a seed URL, which
redirects to https://foo.example.github.io/..., which we normalize
to example.github.io.
To determine the CMP in use, we inspected the behavior of the
six CMPs under study and created fingerprints for each CMP based
on their HTTP request patterns, CSS selectors, and extracted text.
For each CMP, we first recorded the network traffic of multiple
websites where it was embedded and consulted the documentation
provided by the CMP. Second, we assembled multiple fingerprints
of varying specificity (for example, from concrete URLs to second-
level domains) using manual analysis. To make sure that we did not
miss any CMP dialogs, we searched for the GDPR phrases listed in
[11] in our toplist crawls. We then checked the screenshots from
our toplist crawls and discarded all fingerprints that yield false-
positives. Finally, we verified that the remaining fingerprints work
accurately for historic data using Netograph’s captured screenshots.
Using this approach, we were able to identify a unique hostname for
each consent dialog framework as a robust indicator. For example,
even though OneTrust deploys very different dialog designs with
no shared JavaScript code or CSS classes, all of them perform HTTP
requests to cdn.cookielaw.org on page load. We list our synthesized
indicators in Table A.2 for reproducibility.
Finally, we also need to take into account that the sampling
frequency of a domain is not fixed in our main dataset as the crawler
is seeded from social media posts only. Consequently, we may
not see less popular domains for prolonged periods. We account
for this in two ways. First, we interpolate missing observation
periods if both boundary measurements are classified equally. For
example, if we observed Quantcast on example.com a month ago
and observe it again today, we assume that example.com kept using
Quantcast as their CMP throughout this period. If the boundary
measurements disagree, we do not assume the presence of the CMP
in the intermediate period. Second, we account for the fact that
our measurements are right-censored by fading out the presence
of a CMP after 30 days if no new measurements have been made
yet. For example, if a website was last measured a week before
our analysis, we assume that they still use the same CMP; if the
last measurement was made on February 1st, we assume no CMP
presence as of March 1st. Finally, as we crawl with a fixed sampling
frequency for our toplist-based measurements, we do not need to
interpolate for this dataset.
320
Measuring the Emergence of Consent Management on the Web
IMC ’20, October 27–29, 2020, Virtual Event, USA
Ad-Tech Vendor Behavior (I4–I5). Recall that Ad-tech vendors
need to declare in the TCF for which data processing purposes they
either request consent or claim legitimate interest. To assess the
behavior of vendors, we systematically analyzed previous versions
of the GVL and inspected them for longitudinal changes. In partic-
ular, we measure every instance when an Ad-tech vendor joins or
leaves the GVL, claims a new purpose falls under legitimate interest,
begins requesting consent for a new purpose, stops claiming either,
or changes from collecting consent to claiming legitimate interest
or the other way round.
Time to Consent (I6–I7). An aspect that has not been studied in
the literature is the relative time taken to express different consent
preferences. We aim to quantify this by embedding the dialogues of-
fered by two leading CMPs, namely Quantcast and TrustArc. Using
real dialogues in a field experiment improves ecological validity rel-
ative to studies using dialogues developed by researchers that result
in a very different feel for the participants who are not browsing
normally.
First, we measured how a seemingly small user interface change
impacts the time it takes users to make a positive or negative con-
sent decision. We embedded Quantcast’s CMP dialog on a popular
website on the public internet for a short period of time in two
configurations: One with an explicit “Reject” button and one that
included a “More Options” at the same position which would then
lead to a reject button (see Figures A.1 and A.2). This design is mo-
tivated by the French data protection authority’s guidelines, which
demand a real choice between accepting or refusing cookies pre-
sented at the same level [10]. All other dialog settings were left to
the default values: The consent prompt was shown as a modal dialog
in the center of the screen, consent for all vendors on the GVL was
requested, the “Accept” button was colored more prominently, and
the dialog was only shown to visitors from the EU. We then mea-
sured the page load time (DOMContentLoaded), the time the dialog
appeared (__cmp(’ping’,...)4), and the time it was closed as well
as the user’s consent decision (__cmp(’getConsentData’,...)).
We also checked for the existence of already existing global consent
cookies by manually fetching https://api.quantcast.mgr.consensu.
org/CookieAccess, which returns the users’s existing Quantcast
TCF cookie. Repeated visitors will not be counted as the CMP stores
the first consent decision and no additional dialogs will be shown.
Second, we noticed that some CMP dialogs require extended
processing time if users decide to opt out. For example, TrustArc
consent prompts disappear immediately if one accepts cookies, but
otherwise make the user wait for prolonged periods while opt-out
requests are being sent to a hodgepodge of third parties. In our
testing, opting out required users to wait tens of seconds, which
could be skipped at any time by giving consent. To make sure that
these observations were not a fluke, we repeatedly visited a website
embedding the TrustArc dialog, automated the opt-out process
with a custom Google Chrome extension, and collected all HTTP
requests and timings.
4The __cmp() function is standardized as part of the IAB’s Transparency & Consent
Framework, see Matte et al. [32].
3.3 Research Ethics
Our time-to-consent measurements were conducted on a website
with real users, which raises ethical concerns as we did not ask
for consent prior to measuring their interactions with consent
notices. We did so to ensure non-biased results, which is supported
by previous research on consent dialogs [56]. We ensured that
we did not harm website visitors and their privacy. We address
privacy issues by data minimization, i.e. we only collected a user’s
consent decision and the timings described in Section 3.2. The
timings for a single page visit are linked using a random non-
persistent id generated on page load. We do no create or store any
persistent identifiers. While we believe that the second dialog design
may not fulfill the requirements of the GDPR, the website we ran
our experiments on did not perform any personal data collection
irrespective of the user’s consent decision.
3.4 Data Sources
Recall that Netograph’s web crawlers are seeded with URLs posted
on social media. More specifically, we ingest all URLs shared on
Reddit and 1% of public Tweets using Twitter’s sample feed5. Note
that this does not mean we see 1% of URLs: each popular URL has
multiple chances to be spotted in the sample feed as it is re-shared
and retweeted. So in effect our URL sample skews heavily towards
popular URLs. Overall, Twitter accounts for 80% of all URLs. We
skip a URL if we have captured the same domain in the last hour
or the precise URL in the last 48 hours. This applies to about 40%
of all submitted URLs. Our records span March 2018–September
2020, starting before the inception of GDPR and also covering the
introduction of CCPA.
To track the development of the global vendor list, we system-
atically downloaded all 215 previously published versions of the
GVL from https://vendorlist.consensu.org/v𝑋𝑋𝑋/vendor-list.json
and verified their accuracy using the Internet Wayback Machine.
Likewise, we collected the change history of Quantcast’s consent
dialog in the same way.
To measure how long it takes for users to make a consent de-
cision, we embedded Quantcast’s CMP dialog and our collection
script on mitmproxy.org for a short period of time in May 2020. We
logged about 120,000 timestamps. Importantly for generalizing, the
website we hosted our experiment on caters to a very technical and
privacy-concious audience.
For our second timing experiment, we measured the raw waiting
time (not including user interaction) it takes to reject all tracking
on forbes.com’s TrustArc consent dialog. Measurements were per-
formed hourly for two weeks in May 2020. These measurements
were made from a European university as the vantage point.
The relationship between our items of interest, data sources, and
vantage points is summarized again in Appendix A.4.
3.5 Reliability and Validity
Social Media Sample Bias. While existing research is mostly based
on the Alexa and Tranco toplists, our measurement platform is
seeded using URLs obtained from social media posts. An obvious
issue with this setup is that URLs shared on social media are not a
representative sample of the internet. One would reasonably expect
5https://developer.twitter.com/en/docs/labs/sampled-stream/overview
321
IMC ’20, October 27–29, 2020, Virtual Event, USA
Maximilian Hils, Daniel W. Woods, and Rainer Böhme
Location
User Agent
Timing
OneTrust