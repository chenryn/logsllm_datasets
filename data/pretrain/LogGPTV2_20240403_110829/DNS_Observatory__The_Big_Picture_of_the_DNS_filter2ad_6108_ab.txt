series to disk (subsection 2.4); F) aggregating in time, e.g., producing daily files after 24 hours (subsection 2.4).
We characterize each object in the cache using traffic features,
enumerated and briefly documented below:
• srvips: number of nameserver IP addresses;
• srcips: number of recursive resolver IP addresses;
• sources: number of SIE contributors that saw this object;
• hits: total number of transactions seen so far;
• unans: number of unanswered queries;
• ok, nxd, rfs, fail: responses with the RCODE of respectively:
NoError, NXDOMAIN, Refused, and ServFail;
• ok_ans, ok_ns, ok_add: NoError responses: non-empty AN-
SWER section, non-zero NS records in AUTHORITY, non-empty
ADDITIONAL section (skipping EDNS0 OPT);
• ok_nil: neither ok_ans nor ok_ns satisfied (NoData);
• ok6, ok6nil: AAAA queries: all NoError vs. NoData;
• ok_sec: DNSSEC-signed responses: EDNS0 DO flag set, ok_ans
or ok_ns satisfied, sections have RRSIG records;
• qnamesa, qnames: number of distinct QNAMEs in all queries
vs. those that resulted in a NoError response;
• tlds, eslds: number of Top-Level and effective Second-Level Do-
mains in NoError responses;
• qtypes: number of QTYPEs in all queries;
• qdots, lvl, nslvl: number of labels in all QNAMEs, records in
ANSWER, NS records in AUTHORITY, respectively;
• ip4s, ip6s: number of distinct IPv4/IPv6 addresses in NoError
responses to A/AAAA or ANY queries;
• ttl, nsttl: the top-3 TTL values (and distributions) for records in
ANSWER and nameservers in AUTHORITY;
• resp_delays: quartiles of server response delays;
• network_hops: quartiles of inferred number of network hops
(routers) between resolvers and nameservers [39];
• resp_size: quartiles of the response packet sizes.
The underlying data structure for each feature is either a sim-
ple counter (e.g., hits), an average (e.g., qdots), a histogram (e.g.,
resp_delays), or a cardinality estimate (e.g., ip4s). For estimat-
ing the number of elements in possibly large sets of values (e.g.,
qnamesa) we use the HyperLogLog algorithm, as improved in [30].
2.4 Producing time series
Every 60 seconds, we dump all data to disk and reset all statistics,
but without affecting the SS cache, i.e., we keep the list of the most
popular objects, but we clear their internal state used for traffic
features. This way we produce time series data that characterize
a select aspect of the DNS minute by minute, e.g., a time series of
queries per minute for the world’s most popular nameservers.
Because the popularity of objects may change at arbitrary points
in time (not synchronized with our 1-minute time ticks), we skip
the data from objects recently inserted in the SS cache. That is, if
we included an object in the data dump, this means it survived the
SS cache eviction for 60 seconds.
A separate process aggregates minutely files into new, decaminutely
files that represent 10-minute time windows. These in turn get ag-
gregated into hourly files, then into daily files, then into monthly
files, and finally into yearly files. In order to keep disk usage under
control, each of these time granularities have a data retention policy,
i.e., after some time we delete old files for short time windows, and
keep only the longer aggregates.
In general, we aggregate time series of a particular feature using
the arithmetic mean. The value of a counter feature for a particular
object in a decaminutely file is the average rate per minute, esti-
mated using 10 data points (each 1 minute apart); for the hourly
file, we use 6 data points (each 10 minutes apart), etc. If the object
is missing in some of the files being aggregated, we use a value of
0 for counters. For features that are not counters (e.g., cardinality
estimates), we just skip the missing data point.
The data is stored on disk in the TSV file format, where the file
name encodes both the time granularity, and the moment of time
when we started collecting the data. The first TSV row contains
column names, and the last row contains data collection statistics,
which include the total number of DNS transactions seen before
and after filtering. TSV files may easily be imported into many
data analysis tools, from ordinary spreadsheet software to time
series databases. For this paper, we mainly used Python and the
JupyterLab environment.
2.5 Ethical considerations
The intent of DNS Observatory is to track only the big picture of
the DNS—e.g., its performance, robustness, and security—and never
89
Q+RT3ABIP...1. 192.12.94.302. 192.33.14.303. 192.48.79.30...*C3. 192.48.79.30Hitok = 100nxd = 34qnames = 6ip4s = 1ip6s = 0       ...D60s23424hResolversTransactionsTop-kStatisticsTime seriesAggregatesEF1IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Foremski, Gasser, and Moura
to track Internet users—i.e., individuals and/or groups of people. We
store only highly aggregated data that does not contain Personally
Identifiable Information (PII), and as such minimize the risk of
violating users’ privacy.
There are three layers of user privacy protection involved in
our research. First, the raw DNS data is captured above recursive
resolvers, which means we only see the stream of DNS queries
aggregated for all users of a particular resolver—without the user
IP address—and only for FQDNs not in the resolver cache, which
again is shared by all of the resolver clients. Moreover, we do not
know the exact locations of resolvers beyond their IP addresses, and
each resolver can in general be used by people located anywhere.
Second, early in our data pipeline we drop DNS transaction de-
tails except for those that will end up aggregated in traffic statistics
described in subsection 2.3. This means we drop possibly sensitive
EDNS0 data, including DNS cookies [17] and client subnet infor-
mation [12]. The detailed timestamps of the query and response
packets are used only to compute the nameserver response delay
and are subsequently dropped.
Third, we aggregate traffic in Top-k lists for various DNS objects,
as described in subsection 2.2. The k parameter is finite and rela-
tively small compared with, e.g., the cardinality of the DNS FQDN
space. Given the global reach of our raw data collection system, this
means a particular object must be popular enough—compared with
the rest of the world—in order to survive the SS cache eviction for
60 seconds. In an unlikely event it does survive for short periods
of time, the final time aggregation step described in subsection 2.4
will drop it when producing, e.g., the hourly or daily files.
To conclude, we believe that these three layers protect user
privacy. Besides, as we do not perform active measurements, we
do not induce harm on individuals [4, 15, 54].
3 THE BIG PICTURE
In this section we present results from our evaluations based on
data from DNS Observatory. Since our data comes from passive
observations of real DNS traffic—recorded between hundreds of
recursive resolvers spread around the world and over 1 M authori-
tative nameservers (subsection 3.7)—we report on the Big Picture
of the DNS, which is not visible through active measurements, data
collected at TLD level, or from a single ISP or recursive resolver
operator.
3.1 Collected datasets
In general, we use datasets collected from January 1st 2019 until
April 30th 2019, i.e., the first 120 days of 2019. In this time period,
we processed over 1.6 trillion DNS transactions, i.e., over 13 billion
per day. On average, in a 1-minute time window, we see over 1.5 M
existing, and 1.1 M non-existing, unique FQDNs.
The measurement process runs without any interruptions, but
the capabilities of our system improved with time, which allowed
us to collect new aggregations and features starting at a later time.
We collected the following datasets:
• srvip: Top-100K authoritative nameservers, i.e., transactions ag-
gregated using the nameserver IPv4/IPv6 address;
90
• etld: Top-10K effective TLDs (note that we include NXDOMAIN
traffic), i.e., transactions aggregated using the last 1 or more labels
in QNAME (since Jan. 8, 2019);
• esld: Top-100K effective SLDs, i.e., transactions aggregated using
the last 2 or more labels in QNAME (since Feb. 19, 2019);
• qname: Top-100K FQDNs, i.e., transactions aggregated using the
full QNAME (since Feb. 19, 2019);
• qtype: All QTYPE aggregations (since Feb. 15, 2019);
• rcode: All RCODE aggregations (since Apr. 10, 2019);
• aafqdn: Top-20K FQDNs in authoritative answers (cf. subsec-
tion 4.2), i.e., QNAME in transactions where the response has the
AA flag set (since Apr. 15, 2019);
• srcsrv: Top-30K pairs of resolvers and nameservers (cf. subsec-
tion 2.1), i.e., transactions aggregated using the combined IP ad-
dresses as key (Apr. 10, 2019 until May 9, 2019);
3.2 Traffic distributions
In Figure 2, we analyze traffic distributions for various Top-k ag-
gregations. First, in (a), we consider the 100K most popular name-
servers, ranked by their traffic volumes. The data aggregation step
described in subsection 2.2 allowed us to capture in this top list
94.9% of all DNS transactions seen in our raw data source. That is,
although we skip many unpopular nameservers (see subsection 3.7),
we know that they handle only 5.1% of the observed DNS traffic.
In total, all NoError responses account for 68.1% of the trans-
actions captured in the top list, but in the plot we distinguish the
NoData (4.7%) and the opposite “NoError + Data” case, i.e., when a
successful response either had the answer, or delegated to another
nameserver (64.4%). On the other hand, all NXDOMAIN responses
account for 20.7% of the top list traffic. For brevity, we skip other
RCODEs and unanswered queries, 11.2% in total. Note that we plot
an independent CDF curve that ends at 1.0 for each case, so the
curves are not to scale with respect to each other.
We find evidence that the majority of observed DNS traffic is
likely handled by only ≈1,000 authoritative nameserver IP addresses
(i.e., IPv4 and IPv6 addresses). This suggests that considering raw
DNS transaction volumes, a big chunk of the DNS is not well
distributed in the IP address space, and instead relies on shared
infrastructure—or at least, on shared addressing—as already shown
in [2] from another perspective.
Moreover, the surprising starting point of the NXDOMAIN traf-
fic above 20% is caused by a large botnet, likely “Mylobot” [50].
The botnet’s Domain Generation Algorithm (DGA) produced mil-
lions of FQDNs under thousands of non-existing SLDs within the
.com TLD, which caused spikes of NXDOMAIN traffic towards the
gTLD nameservers. This, however, demonstrates how more popular
nameservers—usually higher in the DNS hierarchy—are more likely
to receive queries for non-existing names, and are thus the DNS’s
“first line of defence” against artificially generated and otherwise
erroneous FQDNs.
In Figure 2 (b), we analyze the list of Top-100K FQDNs, reflecting
23.2% of all DNS transactions seen (the top 10K FQDNs correspond
to 18.6% of the observed traffic). Comparing with (a), the lower share
simply means that there are much more FQDNs than nameserver
IPs in the DNS, and that many FQDNs are ephemeral, i.e., used only
once [10]. Thus, we see a heavy-tailed distribution on the plot.
DNS Observatory: The Big Picture of the DNS
IMC ’19, October 21–23, 2019, Amsterdam, Netherlands
Figure 2: Traffic distributions for various Top-100K DNS objects, ranked by traffic. Note that the x-axis is log-scaled for im-
proved readability.
About 10% of queries captured by the list result in a NoData re-
sponse, linked to AAAA queries and the Happy Eyeballs algorithm,
which we analyze in detail in section 5. The NXDOMAIN traffic
(1.5%) is heavily shifted towards less popular FQDNs, which shows
the Internet’s most popular non-existing—yet queried—names are
still well behind the top existing FQDNs. The ordinary “NoError +
Data” responses correspond to 70.2% of aggregated transactions.
Finally, in Figure 2 (c), we analyze 68.5% of observed DNS trans-
actions, aggregated in a list of Top-100K effective SLDs. The distri-
bution shows a high accumulation of queries towards the Top-100
domains, foremost belonging to the biggest CDNs, cloud providers,
social media and e-commerce sites, etc. In addition to the sites al-
ready known from various web popularity top lists—e.g., [42]—we
found popular domains used by anti-virus software, and by the re-
verse DNS, both of which are not normally queried when browsing
the Web.
NXDOMAIN responses accounted for 18.9% of transactions cap-
tured in the list. The shape of CDF curves for ranks 2-4K is due to
the botnet already described in (a) above—this time, however, the
result is spread on more entries in our top list.
In summary, our results presented in Figure 2 show that a big
part of the DNS relies on a relatively small number of authoritative
nameserver IPs and domains, which confirms findings by other
researchers, e.g., [2, 5].
3.3 Autonomous Systems
In order to evaluate how DNS traffic is distributed on the Au-
tonomous Systems comprising the Internet, we associate each IP
address in our Top-100K nameserver list with its corresponding
AS number, using the data collected by the University of Oregon’s
Route Views project [64]. Next, for each ASN, we lookup its name
using the AS Names dataset [35]. Finally, we extract the organiza-
tion name from each AS Name string, and aggregate nameservers
in groups based on the result.
We present the top 10 names, ranked by the total volume of DNS
transactions in Table 1. The basic observation we make is that the
IP prefixes managed by just 10 organizations receive more than
half of the world’s DNS queries.
91
Table 1: Top 10 AS names, by volume of DNS transactions: