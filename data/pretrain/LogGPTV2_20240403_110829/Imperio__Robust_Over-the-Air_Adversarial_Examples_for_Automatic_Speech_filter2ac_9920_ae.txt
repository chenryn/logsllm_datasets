possible rooms, without any physical access to the environment
(e. g., by playback of inconspicuous media from the Internet), and
for which the target recognition output is not at all perceptible by
human listeners. It shows the possibility and risk of a new attack
vector, as no specialized hardware is needed for the playback and
by being insensitive to the rooms in which the attacked systems
are being operated.
6 DISCUSSION
Our experiments show that the adversarial examples, which we
calculated with the proposed algorithm, remain robust even for
high reverberation times or large distances between speaker and mi-
crophone. Also, the same adversarial examples can be successfully
played over the air, even for setups where no direct line-of-sight ex-
ists.
Attack Parameters. Our comparison between the generic and
the adapted version of the attack shows that the more powerful
generic attack does not only have a similar success rate but can
even outperform an adapted version where the attacker has prior
knowledge of the target room. Consequently, an attacker only needs
to calculate one generic adversarial example to cover a wide range
of possible recording setups simultaneously.
For an attack, one successful adversarial example, which re-
mains robust after being replayed (with a WER of 0 %), is already
enough. Therefore, the best strategy for an actual attack would be
to calculate a set of adversarial examples containing the malicious
transcription and to choose the most robust ones. In general, the re-
sults indicate a trade-off between the WER and the noise level: if no
hearing thresholds are used, the WER is significantly better in com-
parison to examples with hearing thresholds. Nevertheless, even
if the WER is better in cases without hearing thresholds, we have
shown that it is indeed possible to calculate over-the-air-robust
adversarial examples with hearing thresholds. Those adversarial ex-
amples contain less perceptible noise and are, therefore, less likely
to be detected by human listeners.
End-to-end ASR systems. End-to-end ASR systems differ wide-
ly from the hybrid ASR systems used in this paper. However, the
proposed attack only requires the possibility for backpropagation
from the output to the input of the recognition network, and can
therefore be applied to end-to-end systems. A simulated version of
a similar attack with RIRs has been shown by Qin et al. [27]. An
adaptation of this attack is, therefore, most likely transferable to
end-to-end ASR systems in the real world.
Black-Box Attack. In a black-box scenario, the attacker has no
access to the ASR system. However, even for this more challenging
attack, it has been shown that it is possible to calculate adversarial
examples, with the caveat that humans can perceive the hidden
transcription if they are made aware of it [11]. The proposed ap-
proach is not easy to apply to black-box adversarial examples like
commercial ASR systems such as Amazon‚Äôs Alexa. Nevertheless, it
should be feasible to use a similar approach in combination with a
parameter-stealing attack [17, 24, 25, 34, 36]. Once the attacker can
rebuild their own system, which reassembles the black-box system,
the proposed algorithm can be used with that system as well.
Countermeasures. To effectively prevent adversarial attacks,
an ASR system needs either some kind of detection mechanism or
needs to be hardened against adversarial examples. The detection of
adversarial examples for known attacks might be feasible. However,
no guarantees can be given against novel attacks in the long term.
For this, it is necessary to build the ASR system to be adversarial-
example-robust, e. g., by mimicking the human perception of speech
similar to images encoded in JPEG format [5]. One step in this
direction can be to focus the ASR system on only those signal
components that are perceptible to the human listener and thus
carry semantic information.
Additionally, not only the input data can be utilized to detect
adversarial examples, but the ASR system‚Äôs DNN can also serve this
purpose. To achieve this, the uncertainty of the DNN estimation can
be utilized to predict the reliability of the DNN output [13, 15, 19, 21].
Due to the difficulty to creating robust adversarial example defenses,
Carlini et al. proposed a guideline for the evaluation of adversarial
robustness, which lists all important properties of a successful
countermeasure against adversarial examples [6].
Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech Recognition Systems
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
7 CONCLUSION
In this paper, we have demonstrated that ASR systems are vulnera-
ble against adversarial examples played over the air and we have
introduced an algorithm for the calculation of robust adversarial
examples. By simulating varying room setups, we can create highly
robust adversarial examples that remain successful over the air in
many environments.
To substantiate our claims, we performed over-the-air attacks
against Kaldi; a state-of-the-art hybrid recognition framework that
is used in Amazon‚Äôs Alexa and other commercial ASR systems.
We presented the results of empirical attacks for different room
configurations. Our algorithm can be used with and without psy-
choacoustic hearing thresholds, limiting the perturbations to being
less perceptible by humans. Furthermore, we have shown that it is
possible to create targeted robust adversarial examples for varying
rooms even if no direct line-of-sight between the microphone and
the speakers exists, and even if the test room characteristics are
completely unknown during the creation of the example.
Future work should investigate possible countermeasures such as
using only the perceptible parts of the audio signal for recognition
or using internal statistical information of the hybrid recognizer
for detecting attacks.
ACKNOWLEDGMENTS
Funded by the Deutsche Forschungsgemeinschaft (DFG, German
Research Foundation) under Germany‚Äôs Excellence Strategy - EXC
2092 CaSa - 390781972.
REFERENCES
[1] Hadi Abdullah, Washington Garcia, Christian Peeters, Patrick Traynor, Kevin
R. B. Butler, and Joseph Wilson. 2019. Practical Hidden Voice Attacks against
Speech and Speaker Recognition Systems. In Network and Distributed System
Security Symposium (NDSS).
[2] Jont B. Allen and David A. Berkley. 1979. Image method for efficiently simulating
small-room acoustics. The Journal of the Acoustical Society of America 65, 4 (1979),
943‚Äì950.
[3] Moustafa Alzantot, Bharathan Balaji, and Mani Srivastava. 2018. Did you hear
that? Adversarial examples against automatic speech recognition. arXiv preprint
arXiv:1801.00554 (2018).
[4] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. 2017. Synthe-
sizing Robust Adversarial Examples. CoRR abs/1707.07397 (July 2017), 1‚Äì18.
[5] Mitali Bafna, Jack Murtagh, and Nikhil Vyas. 2018. Thwarting Adversarial Exam-
ples: An ùêø1-Robust Sparse Fourier Transform. In Advances in Neural Information
Processing Systems 31. 10075‚Äì10085.
[6] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
Rauber, Dimitris Tsipras, Ian Goodfellow, and Aleksander Madry. 2019. On
evaluating adversarial robustness. arXiv preprint arXiv:1902.06705 (2019).
[7] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr,
Clay Shields, David A. Wagner, and Wenchao Zhou. 2016. Hidden Voice Com-
mands. In USENIX Security Symposium. USENIX, 513‚Äì530.
Neural Networks. In Symposium on Security and Privacy. IEEE, 39‚Äì57.
attacks on speech-to-text. (2018), 1‚Äì7.
[9] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted
[8] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of
[10] Tao Chen, Longfei Shangguan, Zhenjiang Li, and Kyle Jamieson. 2020. Meta-
morph: Injecting Inaudible Commands into Over-the-air Voice Controlled Sys-
tems. (2020).
[11] Yuxuan Chen, Xuejing Yuan, Jiangshan Zhang, Yue Zhao, Shengzhi Zhang, Kai
Chen, and XiaoFeng Wang. 2020. Devil‚Äôs Whisper: A General Approach for
Physical Adversarial Attacks against Commercial Black-box Speech Recognition
Devices. In USENIX Security Symposium. USENIX.
[12] Moustapha Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. 2017. Houdini:
Fooling Deep Structured Prediction Models. CoRR abs/1707.05373 (July 2017),
1‚Äì12.
[13] Sina D√§ubener, Lea Sch√∂nherr, Asja Fischer, and Dorothea Kolossa. 2020. Detect-
ing Adversarial Examples for Speech Recognition via Uncertainty Quantification.
arXiv preprint arXiv:2005.14611 (2020).
[14] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul
Prakash, Amir Rahmati, and Dawn Song. 2017. Robust Physical-World Attacks
on Machine Learning Models. CoRR abs/1707.08945 (July 2017), 1‚Äì11.
[15] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a bayesian approximation:
Representing model uncertainty in deep learning. In International Conference on
Machine Learning. 1050‚Äì1059.
[16] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich
Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al.
2014. Deep speech: Scaling up end-to-end speech recognition. arXiv preprint
arXiv:1412.5567 (2014).
[17] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-box
Adversarial Attacks with Limited Queries and Information. CoRR abs/1804.08598
(April 2018), 1‚Äì10.
[18] ISO. 1993. Information Technology ‚Äì Coding of Moving Pictures and Associated
Audio for Digital Storage Media at Up to 1.5 Mbits/s ‚Äì Part3: Audio. ISO 11172-3.
International Organization for Standardization.
[19] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple
and scalable predictive uncertainty estimation using deep ensembles. In Advances
in Neural Information Processing Systems. 6402‚Äì6413.
[20] Juncheng Li, Shuhui Qu, Xinjian Li, Joseph Szurley, J Zico Kolter, and Florian
Metze. 2019. Adversarial Music: Real World Audio Adversary Against Wake-
word Detection System. In Advances in Neural Information Processing Systems
(NeurIPS). 11908‚Äì11918.
[21] Christos Louizos and Max Welling. 2016. Structured and efficient variational
deep learning with matrix gaussian posteriors. In International Conference on
Machine Learning. 1708‚Äì1716.
[22] Christoph L√ºscher, Eugen Beck, Kazuki Irie, Markus Kitza, Wilfried Michel,
Albert Zeyer, Ralf Schl√ºter, and Hermann Ney. 2019. RWTH ASR systems for
LibriSpeech: Hybrid vs Attention. Proceedings of Interspeech (2019), 231‚Äì235.
[23] Gonzalo Navarro. 2001. A Guided Tour to Approximate String Matching. Comput.
Surveys 33, 1 (March 2001), 31‚Äì88.
[24] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik,
and Ananthram Swami. 2017. Practical Black-Box Attacks Against Machine
Learning. In Asia Conference on Computer and Communications Security (ASIA
CCS). ACM, 506‚Äì519.
[25] Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. 2016. Trans-
ferability in Machine Learning: From Phenomena to Black-Box Attacks using
Adversarial Samples. CoRR abs/1605.07277 (May 2016), 1‚Äì13.
[26] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek,
Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz,
Jan Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recogni-
tion Toolkit. In Workshop on Automatic Speech Recognition and Understanding.
IEEE.
[27] Yao Qin, Nicholas Carlini, Ian Goodfellow, Garrison Cottrell, and Colin Raffel.
2019. Imperceptible, Robust, and Targeted Adversarial Examples for Automatic
Speech Recognition. In arXiv preprint arXiv:1903.10346.
[28] Nirupam Roy, Haitham Hassanieh, and Romit Roy Choudhury. 2017. BackDoor:
Making Microphones Hear Inaudible Sounds. In Conference on Mobile Systems,
Applications, and Services. ACM, 2‚Äì14.
[29] Lea Sch√∂nherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea
Kolossa. 2019. Adversarial Attacks Against Automatic Speech Recognition Sys-
tems via Psychoacoustic Hiding. In Network and Distributed System Security
Symposium (NDSS).
[30] Senthil Mani Shreya Khare, Rahul Aralikatte. 2019. Adversarial Black-Box Attacks
on Automatic Speech Recognition Systems using Multi-Objective Evolutionary
Optimization. Proceedings of Interspeech (2019).
abs/1708.07238 (Aug. 2017), 1‚Äì3.
Inaudible Voice Commands. CoRR
[32] Joseph Szurley and J Zico Kolter. 2019. Perceptual Based Adversarial Audio
[31] Liwei Song and Prateek Mittal. 2017.
Attacks. arXiv preprint arXiv:1906.06355 (2019).
[33] Rohan Taori, Amog Kamsetty, Brenton Chu, and Nikita Vemuri. 2018. Targeted
adversarial examples for black box audio systems. arXiv preprint arXiv:1805.07820
(2018).
[34] Florian Tram√®r, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart.
2016. Stealing Machine Learning Models via Prediction APIs. In USENIX Security
Symposium. USENIX, 601‚Äì618.
[35] Stephen Voran and Connie Sholl. 1995. Perception-based Objective Estimators of
Speech. In IEEE Workshop on Speech Coding for Telecommunications. IEEE, 13‚Äì14.
[36] Binghui Wang and Neil Zhenqiang Gong. 2018. Stealing Hyperparameters in
Machine Learning. In Symposium on Security and Privacy. IEEE.
physical attack. arXiv preprint arXiv:1810.11793 (2019).
[37] Hiromu Yakura and Jun Sakuma. 2019. Robust audio adversarial example for a
[38] Wonho Yang. 1999. Enhanced Modified Bark Spectral Distortion (EMBSD): an
Objective Speech Quality Measrure Based on Audible Distortion and Cognition
Model. Ph.D. Dissertation. Temple University Graduate Board.
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
Lea Sch√∂nherr, Thorsten Eisenhofer, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa
[39] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen,
Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A. Gunter. 2018. Com-
manderSong: A Systematic Approach for Practical Adversarial Voice Recognition.
arXiv preprint arXiv:1801.08535 (2018).
[40] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and
Wenyuan Xu. 2017. DolphinAttack: Inaudible Voice Commands. In Conference
on Computer and Communications Security (CCS). ACM, 103‚Äì117.
[41] Eberhard Zwicker and Hugo Fastl. 2007. Psychoacoustics: Facts and Models (third
ed.). Springer.
Imperio: Robust Over-the-Air Adversarial Examples for Automatic Speech Recognition Systems
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
A ROOM LAYOUT PLANS
Table 7: Microphone and Speaker positions and the reverberation time for each room in Table 6.
ùëá60
Microphone
Speaker
w/ line-of-sight
w/o line-of-sight
0.80 s
r = [8.1 m, 3.4 m, 1.2 m]
s = [11.0 m, 3.4 m, 1.2 m]
s = [8.9 m, 2.2 m, 0.0 m]
0.74 s
r = [3.7 m, 5.7 m, 1.2 m]
s = [1.8 m, 5.7 m, 1.2 m]
s = [3.7 m, 4.9 m, 0.0 m]
0.64 s
r = [3.8 m, 1.8 m, 1.2 m]
s = [1.4 m, 4.6 m, 1.2 m]
s = [‚àí0.5 m, 2.0 m, 1.2 m]
Lecture
Room
Meeting
Room
Office
Figure 10: Room layout of the lecture room.
ACSAC 2020, December 7‚Äì11, 2020, Austin, USA
Lea Sch√∂nherr, Thorsten Eisenhofer, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa
Figure 11: Room layout of the office room.
Figure 12: Room layout of the meeting room.