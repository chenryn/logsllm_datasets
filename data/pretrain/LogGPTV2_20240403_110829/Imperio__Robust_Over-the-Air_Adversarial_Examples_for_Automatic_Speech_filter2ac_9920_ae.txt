### Possible Rooms, Without Physical Access to the Environment

Our research demonstrates the potential for generating adversarial examples that can be deployed without physical access to the target environment (e.g., through the playback of inconspicuous media from the Internet). These adversarial examples are designed to be imperceptible to human listeners, highlighting a new and potentially risky attack vector. Importantly, this method does not require specialized hardware for playback and is insensitive to the specific room in which the attacked system is operated.

### 6. Discussion

#### Robustness in Various Environments
Our experiments show that the adversarial examples generated using our proposed algorithm remain robust even under high reverberation times or large distances between the speaker and microphone. Additionally, these adversarial examples can be successfully played over the air, even in setups where there is no direct line-of-sight between the speaker and the microphone.

#### Attack Parameters
A comparison between the generic and adapted versions of the attack reveals that the generic attack, which does not require prior knowledge of the target room, not only has a similar success rate but can sometimes outperform the adapted version. This means an attacker needs to calculate only one generic adversarial example to cover a wide range of possible recording setups simultaneously.

For a successful attack, a single robust adversarial example (with a Word Error Rate, WER, of 0%) is sufficient. The best strategy would be to generate a set of adversarial examples containing the malicious transcription and select the most robust ones. Our results indicate a trade-off between WER and noise level: if no hearing thresholds are used, the WER is significantly better. However, even with hearing thresholds, it is possible to create over-the-air-robust adversarial examples that contain less perceptible noise, making them less likely to be detected by human listeners.

#### End-to-End ASR Systems
End-to-end Automatic Speech Recognition (ASR) systems differ significantly from the hybrid ASR systems used in this study. However, our proposed attack requires only the possibility of backpropagation from the output to the input of the recognition network, making it applicable to end-to-end systems as well. A simulated version of a similar attack using Room Impulse Responses (RIRs) has been demonstrated by Qin et al. [27], suggesting that our approach is likely transferable to real-world end-to-end ASR systems.

#### Black-Box Attacks
In a black-box scenario, the attacker has no access to the ASR system. Despite this, it has been shown that it is possible to generate adversarial examples, though they may be perceptible to humans if they are made aware of the hidden transcription [11]. Applying our approach to black-box adversarial examples, such as commercial ASR systems like Amazon‚Äôs Alexa, is challenging. However, combining our approach with a parameter-stealing attack [17, 24, 25, 34, 36] could make it feasible. Once the attacker can rebuild a system that mimics the black-box system, our algorithm can be applied effectively.

#### Countermeasures
To prevent adversarial attacks, ASR systems need either detection mechanisms or must be hardened against adversarial examples. Detecting known attacks may be feasible, but long-term protection against novel attacks is more challenging. To achieve this, ASR systems should be built to be robust against adversarial examples, perhaps by mimicking human speech perception, similar to how images are encoded in JPEG format [5]. Focusing on signal components that are perceptible to human listeners and carry semantic information is one step in this direction.

Additionally, the ASR system's Deep Neural Network (DNN) can be used to detect adversarial examples by utilizing the uncertainty of the DNN estimation to predict the reliability of the output [13, 15, 19, 21]. Carlini et al. [6] have proposed guidelines for evaluating adversarial robustness, listing important properties of effective countermeasures.

### 7. Conclusion

In this paper, we have demonstrated the vulnerability of ASR systems to adversarial examples played over the air and introduced an algorithm for generating robust adversarial examples. By simulating varying room setups, we can create highly robust adversarial examples that remain effective in many environments.

We performed over-the-air attacks against Kaldi, a state-of-the-art hybrid recognition framework used in Amazon‚Äôs Alexa and other commercial ASR systems. Our results show that our algorithm can be used with and without psychoacoustic hearing thresholds, limiting perturbations to being less perceptible by humans. We have also shown that it is possible to create targeted, robust adversarial examples for varying rooms, even when there is no direct line-of-sight between the microphone and the speakers, and even if the test room characteristics are completely unknown during the creation of the example.

Future work should explore possible countermeasures, such as using only the perceptible parts of the audio signal for recognition or leveraging internal statistical information of the hybrid recognizer to detect attacks.

### Acknowledgments

This work was funded by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany‚Äôs Excellence Strategy - EXC 2092 CaSa - 390781972.

### References

[1] Hadi Abdullah, Washington Garcia, Christian Peeters, Patrick Traynor, Kevin R. B. Butler, and Joseph Wilson. 2019. Practical Hidden Voice Attacks against Speech and Speaker Recognition Systems. In Network and Distributed System Security Symposium (NDSS).

[2] Jont B. Allen and David A. Berkley. 1979. Image method for efficiently simulating small-room acoustics. The Journal of the Acoustical Society of America 65, 4 (1979), 943‚Äì950.

[3] Moustafa Alzantot, Bharathan Balaji, and Mani Srivastava. 2018. Did you hear that? Adversarial examples against automatic speech recognition. arXiv preprint arXiv:1801.00554 (2018).

[4] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. 2017. Synthesizing Robust Adversarial Examples. CoRR abs/1707.07397 (July 2017), 1‚Äì18.

[5] Mitali Bafna, Jack Murtagh, and Nikhil Vyas. 2018. Thwarting Adversarial Examples: An ùêø1-Robust Sparse Fourier Transform. In Advances in Neural Information Processing Systems 31. 10075‚Äì10085.

[6] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas Rauber, Dimitris Tsipras, Ian Goodfellow, and Aleksander Madry. 2019. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705 (2019).

[7] Nicholas Carlini, Pratyush Mishra, Tavish Vaidya, Yuankai Zhang, Micah Sherr, Clay Shields, David A. Wagner, and Wenchao Zhou. 2016. Hidden Voice Commands. In USENIX Security Symposium. USENIX, 513‚Äì530.

[8] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of Neural Networks. In Symposium on Security and Privacy. IEEE, 39‚Äì57.

[9] Nicholas Carlini and David Wagner. 2018. Audio adversarial examples: Targeted attacks on speech-to-text. (2018), 1‚Äì7.

[10] Tao Chen, Longfei Shangguan, Zhenjiang Li, and Kyle Jamieson. 2020. Metamorph: Injecting Inaudible Commands into Over-the-air Voice Controlled Systems. (2020).

[11] Yuxuan Chen, Xuejing Yuan, Jiangshan Zhang, Yue Zhao, Shengzhi Zhang, Kai Chen, and XiaoFeng Wang. 2020. Devil‚Äôs Whisper: A General Approach for Physical Adversarial Attacks against Commercial Black-box Speech Recognition Devices. In USENIX Security Symposium. USENIX.

[12] Moustapha Cisse, Yossi Adi, Natalia Neverova, and Joseph Keshet. 2017. Houdini: Fooling Deep Structured Prediction Models. CoRR abs/1707.05373 (July 2017), 1‚Äì12.

[13] Sina D√§ubener, Lea Sch√∂nherr, Asja Fischer, and Dorothea Kolossa. 2020. Detecting Adversarial Examples for Speech Recognition via Uncertainty Quantification. arXiv preprint arXiv:2005.14611 (2020).

[14] Ivan Evtimov, Kevin Eykholt, Earlence Fernandes, Tadayoshi Kohno, Bo Li, Atul Prakash, Amir Rahmati, and Dawn Song. 2017. Robust Physical-World Attacks on Machine Learning Models. CoRR abs/1707.08945 (July 2017), 1‚Äì11.

[15] Yarin Gal and Zoubin Ghahramani. 2016. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning. 1050‚Äì1059.

[16] Awni Hannun, Carl Case, Jared Casper, Bryan Catanzaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev Satheesh, Shubho Sengupta, Adam Coates, et al. 2014. Deep Speech: Scaling up end-to-end speech recognition. arXiv preprint arXiv:1412.5567 (2014).

[17] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. 2018. Black-box Adversarial Attacks with Limited Queries and Information. CoRR abs/1804.08598 (April 2018), 1‚Äì10.

[18] ISO. 1993. Information Technology ‚Äì Coding of Moving Pictures and Associated Audio for Digital Storage Media at Up to 1.5 Mbits/s ‚Äì Part 3: Audio. ISO 11172-3. International Organization for Standardization.

[19] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. 2017. Simple and scalable predictive uncertainty estimation using deep ensembles. In Advances in Neural Information Processing Systems. 6402‚Äì6413.

[20] Juncheng Li, Shuhui Qu, Xinjian Li, Joseph Szurley, J Zico Kolter, and Florian Metze. 2019. Adversarial Music: Real World Audio Adversary Against Wake-word Detection System. In Advances in Neural Information Processing Systems (NeurIPS). 11908‚Äì11918.

[21] Christos Louizos and Max Welling. 2016. Structured and efficient variational deep learning with matrix Gaussian posteriors. In International Conference on Machine Learning. 1708‚Äì1716.

[22] Christoph L√ºscher, Eugen Beck, Kazuki Irie, Markus Kitza, Wilfried Michel, Albert Zeyer, Ralf Schl√ºter, and Hermann Ney. 2019. RWTH ASR systems for LibriSpeech: Hybrid vs Attention. Proceedings of Interspeech (2019), 231‚Äì235.

[23] Gonzalo Navarro. 2001. A Guided Tour to Approximate String Matching. Comput. Surveys 33, 1 (March 2001), 31‚Äì88.

[24] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Celik, and Ananthram Swami. 2017. Practical Black-Box Attacks Against Machine Learning. In Asia Conference on Computer and Communications Security (ASIA CCS). ACM, 506‚Äì519.

[25] Nicolas Papernot, Patrick D. McDaniel, and Ian J. Goodfellow. 2016. Transferability in Machine Learning: From Phenomena to Black-Box Attacks using Adversarial Samples. CoRR abs/1605.07277 (May 2016), 1‚Äì13.

[26] Daniel Povey, Arnab Ghoshal, Gilles Boulianne, Lukas Burget, Ondrej Glembek, Nagendra Goel, Mirko Hannemann, Petr Motlicek, Yanmin Qian, Petr Schwarz, Jan Silovsky, Georg Stemmer, and Karel Vesely. 2011. The Kaldi Speech Recognition Toolkit. In Workshop on Automatic Speech Recognition and Understanding. IEEE.

[27] Yao Qin, Nicholas Carlini, Ian Goodfellow, Garrison Cottrell, and Colin Raffel. 2019. Imperceptible, Robust, and Targeted Adversarial Examples for Automatic Speech Recognition. In arXiv preprint arXiv:1903.10346.

[28] Nirupam Roy, Haitham Hassanieh, and Romit Roy Choudhury. 2017. BackDoor: Making Microphones Hear Inaudible Sounds. In Conference on Mobile Systems, Applications, and Services. ACM, 2‚Äì14.

[29] Lea Sch√∂nherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa. 2019. Adversarial Attacks Against Automatic Speech Recognition Systems via Psychoacoustic Hiding. In Network and Distributed System Security Symposium (NDSS).

[30] Senthil Mani Shreya Khare, Rahul Aralikatte. 2019. Adversarial Black-Box Attacks on Automatic Speech Recognition Systems using Multi-Objective Evolutionary Optimization. Proceedings of Interspeech (2019).

[31] Liwei Song and Prateek Mittal. 2017. Inaudible Voice Commands. CoRR abs/1708.07238 (Aug. 2017), 1‚Äì3.

[32] Joseph Szurley and J Zico Kolter. 2019. Perceptual Based Adversarial Audio Attacks. arXiv preprint arXiv:1906.06355 (2019).

[33] Rohan Taori, Amog Kamsetty, Brenton Chu, and Nikita Vemuri. 2018. Targeted adversarial examples for black box audio systems. arXiv preprint arXiv:1805.07820 (2018).

[34] Florian Tram√®r, Fan Zhang, Ari Juels, Michael K. Reiter, and Thomas Ristenpart. 2016. Stealing Machine Learning Models via Prediction APIs. In USENIX Security Symposium. USENIX, 601‚Äì618.

[35] Stephen Voran and Connie Sholl. 1995. Perception-based Objective Estimators of Speech. In IEEE Workshop on Speech Coding for Telecommunications. IEEE, 13‚Äì14.

[36] Binghui Wang and Neil Zhenqiang Gong. 2018. Stealing Hyperparameters in Machine Learning. In Symposium on Security and Privacy. IEEE.

[37] Hiromu Yakura and Jun Sakuma. 2019. Robust audio adversarial example for a physical attack. arXiv preprint arXiv:1810.11793 (2019).

[38] Wonho Yang. 1999. Enhanced Modified Bark Spectral Distortion (EMBSD): An Objective Speech Quality Measure Based on Audible Distortion and Cognition Model. Ph.D. Dissertation. Temple University Graduate Board.

[39] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long, Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing Huang, Xiaofeng Wang, and Carl A. Gunter. 2018. CommanderSong: A Systematic Approach for Practical Adversarial Voice Recognition. arXiv preprint arXiv:1801.08535 (2018).

[40] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and Wenyuan Xu. 2017. DolphinAttack: Inaudible Voice Commands. In Conference on Computer and Communications Security (CCS). ACM, 103‚Äì117.

[41] Eberhard Zwicker and Hugo Fastl. 2007. Psychoacoustics: Facts and Models (third ed.). Springer.

### A. Room Layout Plans

**Table 7: Microphone and Speaker Positions and Reverberation Times for Each Room**

| Room          | Reverberation Time (ùëá60) | Microphone Position (m) | Speaker Position (m) (w/ line-of-sight) | Speaker Position (m) (w/o line-of-sight) |
|---------------|--------------------------|-------------------------|----------------------------------------|-----------------------------------------|
| Lecture Room  | 0.80 s                   | [8.1, 3.4, 1.2]         | [11.0, 3.4, 1.2]                       | [8.9, 2.2, 0.0]                         |
| Meeting Room  | 0.74 s                   | [3.7, 5.7, 1.2]         | [1.8, 5.7, 1.2]                        | [3.7, 4.9, 0.0]                         |
| Office        | 0.64 s                   | [3.8, 1.8, 1.2]         | [1.4, 4.6, 1.2]                        | [‚àí0.5, 2.0, 1.2]                        |

**Figure 10: Room Layout of the Lecture Room**

**Figure 11: Room Layout of the Office Room**

**Figure 12: Room Layout of the Meeting Room**