title:Don't drop, detour!
author:Matt Calder and
Rui Miao and
Kyriakos Zarifis and
Ethan Katz-Bassett and
Minlan Yu and
Jitendra Padhye
Don’t Drop, Detour!
Matt Calder
Univ. of Southern California
PI:EMAIL
Ethan Katz-Bassett
PI:EMAIL
Univ. of Southern California
Rui Miao
Univ. of Southern California
PI:EMAIL
Minlan Yu
Univ. of Southern California
PI:EMAIL
Kyriakos Zariﬁs
Univ. of Southern California
PI:EMAIL
Jitendra Padhye
Microsoft Research
PI:EMAIL
Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network Archi-
tecture and Design
General Terms
Performance
Keywords
data center, buffers, packet loss
1.
INTRODUCTION
The mechanics of DIBS are best explained assuming output-buffered
switches, although it can work with any switch type. When a packet
arrives at a switch input port, the switch checks to see if the buffer
for the destination port is full. If so, instead of dropping the packet,
the switch selects one of its other ports at random 1 to forward the
packet on. Other switches will buffer and forward the packet, and it
will make its way to its destination, possibly coming back through
the switch that originally detoured it.
Modern data center networks (DCNs) are built with shallow-
buffered switches. These switches are cheaper than their deep-
buffered counterparts, and they also reduce the maximum queuing
delays packets may suffer. However, shallow buffers can lead to
high packet loss under bursty trafﬁc conditions (often caused by
the “incast” trafﬁc pattern [7]). DCTCP [3] attempts to solve this
problem by using ECN markings to throttle the ﬂows early, to avoid
buffer overﬂow. However, DCTCP cannot prevent packet loss if the
trafﬁc bursts are severe, and short-lived. One extreme example is a
large number of senders each sending one or two packets to a sin-
gle receiver. No feedback-based congestion control protocol can
prevent buffer overﬂow and packet losses under such extreme con-
ditions. One reason for this is an assumption ingrained in today’s
DCN design.
The assumption is that when a switch needs to buffer a packet,
it must do so only in its own available buffer. If its buffer is full, it
must drop that packet. This assumption is so obvious that no one
states it explicitly. We believe that in a data center environment
this assumption is not warranted. All switches in a DCN are un-
der a single administrative control. Thus, when faced with extreme
congestion, the switches should pool their buffers into a large vir-
tual buffer to absorb the trafﬁc burst.
To share buffers among switches, we propose that a switch de-
tour excess packets to other switches – instead of dropping them –
thereby temporarily claiming space available in the other switches’
buffers. We call this approach detour-induced buffer sharing (DIBS).
Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage, and that copies
bear this notice and the full citation on the ﬁrst page. Copyrights for third-
party components of this work must be honored. For all other uses, contact
the owner/author(s). Copyright is held by the author/owner(s).
SIGCOMM’13, August 12–16, 2013, Hong Kong, China.
ACM 978-1-4503-2056-6/13/08.
Figure 1: Example path of a packet detoured 15 times in a K=8 fat-
tree topology. For simplicity, we only show 1 edge switch and 1 aggre-
gation switch in the sender’s pod, and we abstract the 16 core switches
into a single node. The numbers near the arcs and the arcs’ thicknesses
both indicate the number of times the packet traversed that arc.
Figure 1 shows an illustrative example.
It depicts the path of
a single packet (from one of our simulations) that was detoured
multiple times, before reaching destination R. The weight of an arc
indicates how often the packet traversed that speciﬁc arc. Dashed
arcs indicate detours. While the actual order of the hops cannot be
inferred, we can see that the packet bounced 8 times back to a core
switch because the aggregation switch was congested. The packet
was bounced several times within the receiver pod before ﬁnally
reaching the receiver.
The DIBS idea appears to violate basic principles taught in ev-
ery Networking 101 class. However, our results show that DIBS
works quite well as long as it is coupled with a congestion control
scheme, and the network has some spare capacity.
Congestion control: DIBS is not a replacement for congestion
control. Indeed, DIBS must be paired with a congestion control
scheme that does not rely on packet losses to infer congestion.
Otherwise, when faced with sustained congestion (e.g. multiple
long-lived TCP ﬂows converging to a single destination), DIBS
will build up large queues. To avoid this problem, DIBS must
be paired with a scheme like DCTCP [3]. With DCTCP, switches
mark packets (using ECN bits) when the queue builds up beyond
a certain threshold (typically, well before the queue is full). The
end hosts react to these ECN mark by reducing their sending rate.
1We avoid ports whose buffers are full, and also those that are con-
nected to end hosts.
Sender's podReceiver's podDetourForwardSREdgeAggrCore1118922111212211503Thus, DCTCP avoids long-term congestion, while DIBS ensures
that there is no packet loss during large and transient bursts created
by severe incast-like trafﬁc. DIBS can also be paired with other
congestion control schemes such as QCN [4] as well as the classic
Random Early marking (RED/REM).
Spare capacity: If all links in the network are fully utilized, DIBS
will not work, since there will be no capacity to handle the extra
load caused by detoured packets. However, in DCNs, congestion
is usually transient and local [5]. Thus, when we detour packets
away from one congestion hotspot, the network will typically have
capacity elsewhere to handle the trafﬁc. We are working to derive a
limit on network utilization, beyond which detouring can be detri-
mental.
We note that DIBS is particularly suited for deployment in DCNs.
Many popular DCN topologies offer multiple paths [2], which de-
touring can effectively leverage. The link bandwidths in DCNs are
very high and the link delays are quite small. Thus, the additional
delay of a detour is quite low. Current DCN switches do not im-
plement the random detouring, but the change required to do so
is minimal. Indeed, our NetFPGA implementation is less than 25
lines of code. DIBS does not come into play until there is extreme
congestion – it has no impact whatsoever when things are “normal”.
2. DISCUSSION
Packet reordering: Detouring will cause packet reordering, which
can adversely impact TCP’s performance. A simple ﬁx is to disable
fast retransmission on end hosts.
CIOQ switches: Many modern switches use a combined input-
output queued architecture (CIOQ), with shared buffer to store pack-
ets. DIBS can be implemented on these switches quite easily by
deﬁning a threshold. The switch fabric controller keeps track of
packets on a per-port basis. When the number of packets buffered
for an output port exceeds the threshold, the switch detours subse-
quent to other ports.
L2 routing: DIBS cannot work with self-learning L2 routing
schemes such as the spanning tree protocol. However, most mod-
ern DCN topologies such as FatTree rely on pre-computed routing
tables.
Rogue ﬂows and congestion collapse: Rogue ﬂows that do not
implement congestion control can degrade performance of any net-
work. DIBS may exacerbate this problem by constantly detouring
packets of such ﬂow. Today, DCNs use a variety mechanisms to
detect and deal with such rogue ﬂows. We expect that these mech-
anisms will be sufﬁcient for our purposes as well. We are studying
this issue in more detail. In the absence of such rogue ﬂows, and as
long as DIBS is paired with a congestion control protocol such as
DCTCP, DIBS will not lead to congestion collapse.
Collateral damage: When congestion occurs at a particular switch,
DIBS detours excess packets to other switches. These detoured
packets interfere with other ﬂows that would have otherwise not
have been impacted by the trafﬁc at the congested switch. We term
this collateral damage. Our simulations show that collateral dam-
age is low as long as a DIBS is used with a scheme like DCTCP
and there is sufﬁcient spare capacity. We are currently exploring
the extreme conditions where collateral damage can be signiﬁcant.
Routing loops: DIBS can appear to cause routing loops, as seen
in Figure 1. However, these are merely temporary artifacts and do
not affect actual routing.
Early results: We have implemented a preliminary version of
DIBS in a NetFPGA router, in a Click modular router, and in NS-
Figure 2: Mixed trafﬁc: Variable incast degree. Compared to DCTCP
alone, DIBS+DCTCP improves query completion time (QCT), with lit-
tle collateral damage to background ﬂows (FCT). (Background inter-
arrival time: 120ms; query arrival rate: 300 qps; response size: 20KB)
3. Our initial experiments are quite encouraging. Here we present
only a sample result.
Figure 2 shows performance of DIBS with a complex mix of
trafﬁc, derived from trafﬁc traces from a large search engine [3].
The trafﬁc consists of both query trafﬁc, which causes incast ﬂows,
and background ﬂows. As the degree of incast increases, the 99th
percentile of query completion time (QCT) is much worse with
DCTCP alone. DIBS improves performance by avoiding packet
losses. At the same time, the collateral damage to background ﬂows
(FCT) is low, since the network has spare capacity.
Ongoing work: We are currently evaluating the performance of
DIBS with a variety of trafﬁc patterns using detailed simulations
and experiments. We are also considering modiﬁcations to the ba-
sic DIBS concept such as limiting the number of times a packet can
be detoured, probabilistically detouring, and priority-based detour-
ing.
Related work: DIBS can be implemented alongside trafﬁc spread-
ing schemes such as ECMP (ﬂow or packet level) or MPTCP [6].
These schemes reduce the possibility of extreme congestion, but
do not eliminate it. Technologies such as Ethernet ﬂow control
and its modern variant, Priority Flow Control (PFC) [1], as well
as the Inﬁniband link layer, guarantee loss-free L2 networks. Un-
like these technologies, DIBS does not guarantee a loss-free L2
network. However, compared to these technologies DIBS is signif-
icantly easier to conﬁgure [1]. Also, unlike Ethernet ﬂow control,
DIBS is deadlock free [1]. We are exploring this relationship fur-
ther.
[7] proposed reducing TCP RTOmin value to mitigate the
impact of packet losses. This proposal is orthogonal to DIBS, and
the two can be implemented together. Centralizing scheduling and
rate allocation [8] can in theory avoid all packet losses. However,
in practice, such systems face scalability issues.
3. REFERENCES
[1] Priority ﬂow control.
http://www.cisco.com/en/US/prod/collateral/switches/
ps9441/ps9670/white_paper_c11-542809.pdf.
[2] M. Al-Fares, A. Loukissas, and A. Vahdat. A scalable, commodity data center
network architecture. In SIGCOMM, 2008.
[3] M. Alizadeh, A. Greenberg, D. A. Maltz, J. Padhye, P. Patel, B. Prabhakar,
S. Sengupta, and M. Sridharan. Data center TCP (DCTCP). In SIGCOMM, 2010.
[4] M. Alizadeh, A. Kabbani, B. Atikoglu, and B. Prabhakar. Stability analysis of
QCN: The averaging principle. In SIGMETRICS, 2011.
[5] S. Kandula, J. Padhye, and P. Bahl. Flyways to de-congest data center networks.
In HotNets, 2009.
[6] C. Raiciu, S. Barré, C. Pluntke, A. Greenhalgh, D. Wischik, and M. Handley.
Improving datacenter performance and robustness with multipath TCP. In
SIGCOMM, 2011.
[7] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. Andersen, G. Ganger,
G. Gibson, and B. Mueller. Safe and effective ﬁne-grained TCP retransmissions
for datacenter communication. In SIGCOMM, 2009.
[8] D. Zats, T. Das, P. Mohan, D. Borthakur, and R. H. Katz. DeTail: reducing the
ﬂow completion time tail in datacenter networks. In SIGCOMM, 2012.
 0 20 40 60 80 40 60 80 10099th completion time(ms)Incast degreeQCT: DCTCPQCT: DCTCP + DIBSFCT: DCTCPFCT: DCTCP + DIBS504