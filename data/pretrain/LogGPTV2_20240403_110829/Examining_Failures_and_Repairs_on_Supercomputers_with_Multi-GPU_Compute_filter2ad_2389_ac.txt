While different GPUs on the same node can have a different
number of failures, can a GPU failure affect multiple GPUs
on the same node? In particular, we ask:
RQ3: Can multiple GPUs within a node fail simulta-
neously? If so, what is the probability, and does that
probability change across the two supercomputers? Table
III shows that on Tsubame-2, in ∼30% of the failures, only
one GPU was involved; however, in ∼70% of the failures
more than one GPU was affected at the same time and needed
action (Table III). On Tsubame-3 however, more than 92%
of the failures only affected one GPU. In fact, no failure
affected all four GPUs attached to a node. This is surprising
TABLE III. Number of GPUs involved in node failures.
#GPUs
1
2
3
4
Total
Tsubame-3
75 (92.6%)
4 (4.95%)
2 (2.45%)
0 (0%)
81 (100%)
Tsubame-2
112 (30.44%)
128 (34.78%)
128 (34.78%)
N/A
368 (100%)
Fig. 6. Cumulative distribution of time between two failures.
The mean time between failures (MTBF) is much higher on
Tsubame-3 than Tsubame-2.
considering that more GPUs per node should lead to more
multi-GPU failures. However, the counter-intuitive trend is a
result of Tsubame-3 operational practices learned from the
Tsubame-2 experience: more health-tests for multi-GPU cards
on the same node and proactive replacements. Users have
also become more informed and ensure that their multi-GPU
jobs are debugged more rigorously to avoid the possibility
of multiple GPUs failing simultaneously. This ﬁnding has
an important implication for system administrators since the
number of GPUs per node is likely to increase [24], [25]. The
primary mode for simultaneous multi-GPU failures has been
“fallen off the bus” errors, temperature-related failures, and
simultaneous correlated reboots.
Summary. Our results reveal two novel insights: (1)
the spatial distribution of GPU failures within a node
is non-uniform for both the systems, and (2) each fail-
ure may affect multiple GPUs simultaneously on the
same node. We recommend that HPC systems facilitate
data collection on different failures involving GPUs for
further investigation. An important implication is that
HPC centers should inform and help end-users take
advantage of all the GPUs in a node in a load-balanced
manner, change the scheduler design when co-locating
multiple jobs on the same node for increased utilization,
and develop better testing for simultaneous multi-GPU
failure mode.
Next, we analyze the temporal characteristics of failures. In
particular, we investigate two key metrics: time between two
failures (TBF), and time to recovery (TTR). Time between two
failures simply refers to the elapsed wall clock time between
two failure instances on the system. Time to recovery refers
to the time taken to completely repair the failure and come
back to the normal operational status (e.g., time taken to
replace/restart a failed GPU).
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:59 UTC from IEEE Xplore.  Restrictions apply. 
308
portant to understand that the MTBF across two systems can-
not be compared trivially. A meaningful comparison involves
taking two factors into account: (1) the system’s computing
capability, and (2) its size.
Tsubame-3 has ∼8× more computing power than Tsubame-
is ∼4× better. This shows that
2. The MTBF, however,
more useful work is done on Tsubame-3 than on Tsubame-2
even when interrupted by failures. As the performance of the
systems increases, useful work done per failure should be used
as a benchmarking metric for systems to account for reliability.
We term this as performance-error-proportionality which can
be expressed as maximum useful computation during failure-
free period (e.g., total FLOP per MTBF).
With respect
to the system size, Tsubame-3 has fewer
number of nodes and hence, it could be argued that it is
not surprising that Tsubame-3 has higher MTBF. The total
number of CPU and GPU components in the system are:
7040 for Tsubame-2 and 3240 for Tsubame-3 (less than 2.5×
difference). So the improvement in MTBF is not simply a side-
effect of the reduced number of components. Furthermore, we
calculated the MTBF for GPU and CPU related failures.
We estimated that the MTBF for GPU failures is 226.48
hours for Tsubame-3, but 21.94 hours for Tsubame-2. This
relative increase in MTBF is ∼10×, which is interesting
because the number of GPUs has decreased by only 2×.
Similarly, the MTBF for CPU failures is 1593.6 hours for
Tsubame-3, but 537.6 hours for Tsubame-2. CPU reliability
has also increased (∼3×), but note that the number of CPUs
also has decreased by ∼3×.
Next, we dig deeper to understand the time between failures
characteristics for different failure types. Figure 7 plots the
distribution of time between two failures for different failure
types. We make a few observations. First, as expected, not all
failures have similar distribution of failure inter-arrival times
on both the systems. Some failures have a lower median and
spread (difference between the 75th percentile and 25th per-
centile) than others – and this is true for both the systems. For
example, GPU-related hardware failures and software failures
have the least median time between two failures. Second,
memory- and CPU-related failures have a much higher median
time between failures on both the systems and their relative
spread is also higher compared to GPU failures.
Finally, Figure 8 shows the temporal distribution of GPU
failures. This results reveals that failures that involved multiple
GPUs failing within the same node often tend to happen close-
by in time. That is, a failure where multiple GPUs within
a node failed at the same time is likely to be followed by
another such failure in close-by time. This is suspected due to
interaction between application, GPU hardware, and operating
conditions (e.g., temperature). An implication of this trend is
how one can proactively schedule GPU nodes and provision
for spare resources.
Summary. Our results reveal GPU hardware has be-
come signiﬁcantly more reliable over generations, and
the corresponding increase in MTBF is more than the
(a) Tsubame-2
(b) Tsubame-3
Fig. 7. Distribution of the time between two failures for
different failure types (sorted by mean time between two
failures).
(a) Tsubame-2
(b) Tsubame-3
Fig. 8. Temporal distribution of GPU failures within node.
RQ4: How do the characteristics of the “time between two
failures” change from one system to another and across
different types of failures?
Figure 6 shows the distribution of the time between two fail-
ures for Tsubame-2 and Tsubame-3. We make two important
observations. First, the distribution is signiﬁcantly different for
the two systems. Tsubame-2 has a steeper curve and Tsubame-
3 has a longer tail. This indicates that there are longer failure-
free periods on Tsubame-3. Such long failure-free periods are
relatively fewer on Tsubame-2. In fact, 75% of the failures on
Tsubame-2 occur within 20 hours of each other. In contrast,
on Tsubame-3, 75% of the failures occur within 93 hours.
Second, the mean time between failures (MTBF) is much
for Tsubame-3 than Tsubame-2. The MTBF on
higher
Tsubame-2 is ∼15 hours, but it is more than 70 hours on
Tsubame-3 (more than 4× improvement). However, it is im-
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:59 UTC from IEEE Xplore.  Restrictions apply. 
309
Fig. 9. Cumulative distribution of the time to recovery. The
mean time to recovery (MTTR) is roughly the same for
Tsubame-3 and Tsubame-2.
(a) Tsubame-2
(a) Tsubame-2
(b) Tsubame-3
Fig. 10. Distribution of time to recovery for different failure
types (sorted by mean time to recovery)
decrease in the number of components. However, we
also observed that the resilience-proportionality does not
scale at the same speed as the raw computing power.
Hence, resilience-proportionality should be considered as
a design factor to allow for the reliability of the system
to increase at the same rate as the computing power.
RQ5: How do the “time to recovery” characteristics change
between two systems and across different failute types?
Figure 9 shows the distribution of the time to recovery
from failures for both the systems: Tsubame-2 and Tsubame-
3. Interestingly, the mean time to recovery (MTTR) is very
similar (approx. 55 hours) for both systems. In fact,
the
distribution shape is very similar for both the systems. This
is particularly interesting in the context that the MTBF and
distribution of the time between failures is quite different for
both the systems, as we observed earlier (Figure 6). When
analyzed together, these trends have a number of important
implications.
While the MTBF has improved signiﬁcantly over the gen-
(b) Tsubame-3
Fig. 11. Time to recovery distribution of Tsubame-2 and
Tsubame-3 for different months.
erations, the time to recovery has not. MTTR remains around
55 hours for both the system. An improvement in the time
to recovery can be concluded if the distribution was much
steeper for Tsubame-3. But, we observe that the distribution
shape remains roughly the same. That means the strategies to
improve repair time have not been as prevalent and effective
as much as one would like.
In general, the time to recovery has not received as much
attention in the academic literature as the MTBF and the
efforts to reduce MTTR have not been as intense as reducing
the MTBF [7], [9], [26]. However, our results show that the
MTTR should receive similar attention for two reasons: (1) the
MTTR is very comparable to MTBF and hence, it is likely that
multiple concurrent failures might impact the handling/repair
of previous failures. (2) the time to recovery directly quantiﬁes
the impact of the failure on the operations of the system -
amount of time that a component is unavailable to the jobs.
Next, we investigate the time to recovery distribution for
different failure types (Figure 10). We make a few important
observations. As expected, the time to recovery distribution
varies signiﬁcantly across failure types and this is true for
both systems. However, in general, hardware-related failures
(GPUs, system board, power delivery failures) tend to have
a higher spread in the recovery time compared to software
failures. This is because hardware components have multiple
failure modes, and diagnosing each failure type takes sig-
niﬁcant time. On the other hand, software failures typically
require restarting, patching the software, etc. which have lesser
time spread. Second, we observe that failure types with a
lower average time to recovery do not necessarily have a
lower spread. Some failure types which might be relatively
infrequent can have a high time to recovery and a higher
spread too. This ﬁnding implies that as system operators and
designers, we should not look to focus only on highly frequent
failures, but instead assess their impact on the system too.
Less frequent failure types with high recovery costs can affect
the system more negatively. For example, on Tsubame-3, the
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:22:59 UTC from IEEE Xplore.  Restrictions apply. 
310
(a) Tsubame-2
(b) Tsubame-3
Fig. 12. Distribution of failures based on month of occurrence.
“power board” category contributes to roughly 1% of the
failures, however, its recovery can take up to 230 hours (i.e.,
∼10 days). Similarly, on Tsubame-2, the “SSD” category is
∼4% of all failures, while recovering from some SSD failures
requires ∼290 hours (i.e., 12 days). The longer recovery times
highlight the need for appropriate spare provisioning of parts.
Finally, we inquire if the time to recovery has seasonal
effects? That is, does the time to recovery become signiﬁcantly
worse during certain months (e.g., during the holiday season),
or is increased when there is increased number of failures? To
answer these questions, we plotted monthly time to recovery
and number of failures (Fig. 11 and 12). We make two major
observations. First, the time to recovery does not appear to
have any clear seasonal impact. Although in the second half
of the year, time to recovery seems to be higher – this is only
true for Tsubame-2. For Tsubame-3, this trend is not true. In
fact, there is signiﬁcant variance in time to recovery during
each month. We observed similar trends for different failure
types as well, but results are not shown for brevity.
Second, one could hypothesize that the trends in time to re-
covery could be simply correlated with the failure density. That
it, months with higher failure density are likely to see higher
time to recovery. However, when Fig. 11 and 12 are analyzed
together, we ﬁnd that such a correlation does not exist. This
is because, the cost of ﬁxing each failure is different. Some
failures may simply require rebooting and certain other failures
require replacing the hardware. Hence, the cost of recovery is
different and is not linear function of the number of failures.
Also, these results highlight that the strategies to improve
the time to recovery cannot be simply guided or triggered
by seasonal impacts or failure density. Instead, lowering the
time to recovery requires designing strategies that are speciﬁc
to different types of failures and leveraging failure prediction
to initiate recovery proactively where possible. Although not
currently practiced, we believe design and deployment of such
strategies would be operationally beneﬁcial.
Summary. In summary, we need better strategies for
reducing the time to recovery. These strategies need to
be speciﬁc to each failure type and should be adaptive.
Maintaining balance is the key. One can signiﬁcantly
reduce the MTTR by overly proactive measures such
as keeping an excessive number of spare components
on-site or more staff devoted to failure monitoring, but