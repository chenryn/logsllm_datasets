### Impact of GPU Failures on Multiple GPUs within a Node

While the number of failures can vary among different GPUs on the same node, it is also possible for a single GPU failure to affect multiple GPUs simultaneously. Specifically, we explore the following research question:

**RQ3: Can multiple GPUs within a node fail simultaneously? If so, what is the probability, and does this probability differ between the two supercomputers?**

Table III illustrates that on Tsubame-2, approximately 30% of the failures involved only one GPU, while around 70% of the failures affected more than one GPU at the same time (Table III). In contrast, on Tsubame-3, over 92% of the failures affected only one GPU, with no instances where all four GPUs in a node were simultaneously affected. This finding is surprising, as one might expect more multi-GPU failures with an increase in the number of GPUs per node.

**Table III: Number of GPUs Involved in Node Failures**

| #GPUs | Tsubame-3 | Tsubame-2 |
|-------|-----------|-----------|
| 1     | 75 (92.6%) | 112 (30.44%) |
| 2     | 4 (4.95%)  | 128 (34.78%) |
| 3     | 2 (2.45%)  | 128 (34.78%) |
| 4     | 0 (0%)     | N/A        |
| Total | 81 (100%)  | 368 (100%) |

**Figure 6: Cumulative Distribution of Time Between Two Failures**

The mean time between failures (MTBF) is significantly higher on Tsubame-3 compared to Tsubame-2. This counter-intuitive trend can be attributed to the operational practices learned from Tsubame-2, such as increased health tests for multi-GPU cards and proactive replacements. Additionally, users have become more informed and rigorously debug their multi-GPU jobs to prevent simultaneous failures. The primary modes of simultaneous multi-GPU failures include "fallen off the bus" errors, temperature-related issues, and correlated reboots.

**Summary:**
- **Spatial Distribution:** The spatial distribution of GPU failures within a node is non-uniform for both systems.
- **Simultaneous Failures:** Each failure may affect multiple GPUs simultaneously on the same node.
- **Recommendations:** HPC systems should facilitate data collection on different types of GPU failures for further investigation. HPC centers should inform and assist end-users in load-balancing GPU usage, redesign schedulers for co-locating multiple jobs, and develop better testing for simultaneous multi-GPU failure modes.

### Temporal Characteristics of Failures

Next, we analyze the temporal characteristics of failures, focusing on two key metrics: time between two failures (TBF) and time to recovery (TTR).

- **Time Between Two Failures (TBF):** This refers to the elapsed wall clock time between two consecutive failure instances.
- **Time to Recovery (TTR):** This is the time taken to completely repair the failure and return the system to normal operational status.

**Figure 7: Distribution of Time Between Two Failures for Different Failure Types (Sorted by Mean TBF)**

- **Observations:**
  - Not all failures have similar distributions of inter-arrival times on both systems.
  - GPU-related hardware and software failures have the shortest median TBF.
  - Memory- and CPU-related failures have longer median TBFs and higher spreads.

**Figure 8: Temporal Distribution of GPU Failures within a Node**

- **Observations:**
  - Failures involving multiple GPUs within the same node often occur close together in time.
  - This pattern suggests interactions between applications, GPU hardware, and operating conditions (e.g., temperature).

**Summary:**
- **Reliability Improvement:** GPU hardware has become significantly more reliable across generations, with a corresponding increase in MTBF.
- **Proactive Scheduling:** Understanding these trends can help in proactively scheduling GPU nodes and provisioning spare resources.

### Comparison of MTBF Across Systems

**RQ4: How do the characteristics of the “time between two failures” change from one system to another and across different types of failures?**

- **Figure 6: Distribution of Time Between Two Failures for Tsubame-2 and Tsubame-3**
  - **Observations:**
    - Tsubame-2 has a steeper curve, indicating shorter failure-free periods.
    - Tsubame-3 has a longer tail, indicating longer failure-free periods.
    - 75% of failures on Tsubame-2 occur within 20 hours, while on Tsubame-3, 75% occur within 93 hours.
    - The MTBF on Tsubame-2 is approximately 15 hours, while on Tsubame-3, it is over 70 hours (a 4x improvement).

**Figure 9: Cumulative Distribution of Time to Recovery**

- **Observations:**
  - The mean time to recovery (MTTR) is roughly the same (approximately 55 hours) for both systems.
  - Despite the significant improvement in MTBF, the MTTR remains relatively unchanged.

**Summary:**
- **Resilience-Proportionality:** While the resilience of the system has improved, it has not scaled at the same rate as the raw computing power.
- **Design Consideration:** Resilience-proportionality should be a design factor to ensure reliability increases at the same rate as computing power.

### Time to Recovery Characteristics

**RQ5: How do the “time to recovery” characteristics change between two systems and across different failure types?**

- **Figure 10: Distribution of Time to Recovery for Different Failure Types (Sorted by Mean TTR)**
  - **Observations:**
    - Hardware-related failures (GPUs, system board, power delivery) tend to have higher spreads in recovery time.
    - Software failures generally have lower spreads due to simpler recovery processes (e.g., restarting, patching).
    - Less frequent failure types with high recovery costs can significantly impact the system.

**Figure 11: Monthly Time to Recovery and Number of Failures for Tsubame-2 and Tsubame-3**

- **Observations:**
  - There is no clear seasonal impact on time to recovery.
  - The cost of fixing each failure varies, making the relationship between failure density and TTR non-linear.

**Summary:**
- **Strategies for Reducing TTR:** Better strategies are needed to reduce TTR, which should be specific to each failure type and adaptive.
- **Balanced Approach:** Overly proactive measures, such as excessive spare components or more staff, can reduce MTTR but may not be operationally efficient.

In conclusion, improving the time to recovery requires designing specific and adaptive strategies for different types of failures, leveraging failure prediction, and maintaining a balanced approach.