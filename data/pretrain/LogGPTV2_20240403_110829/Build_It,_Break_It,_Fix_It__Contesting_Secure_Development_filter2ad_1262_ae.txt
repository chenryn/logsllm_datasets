along a spectrum ranging from “failed to provide any se-
curity at all” to “vulnerable to extremely subtle timing at-
tacks.” This is interesting because it is a similar dynamic
observed in the software marketplace today.
Many implementations of the log problem lacked encryp-
tion or authentication. Exploiting these design ﬂaws was
trivial for break-it teams. Sometimes log data was written
as plain text, other times log data was serialized using the
Java object serialization protocol.
One break-it team discovered a privacy ﬂaw which they
could exploit with at most ﬁfty probes. The target sub-
mission truncated the “authentication token,” so that it was
vulnerable to a brute force attack.
The ATM problem allows for interactive attacks (not pos-
sible for the log), and the attacks became cleverer as im-
plementations used cryptographic constructions incorrectly.
One implementation used cryptography, but implemented
RC4 from scratch and did not add any randomness to the
key or the cipher stream. An attacker observed that the
ciphertext of messages was distinguishable and largely un-
changed from transaction to transaction, and was able to
ﬂip bits in a message to change the withdrawn amount.
Another implementation used encryption with authenti-
cation, but did not use randomness; as such error messages
were always distinguishable success messages. An attack was
constructed against this implementation where the attack
leaked the bank balance by observing diﬀerent withdrawal
attempts, distinguishing the successful from failed transac-
tions, and performing a binary search to identify the bank
balance given a series of withdraw attempts.
Some failures were common across ATM problem imple-
mentations. Many implementations kept the key ﬁxed across
the lifetime of the bank and atm programs and did not use
a nonce in the messages. This allowed attackers to replay
messages freely between the bank and the atm, violating in-
tegrity via unauthorized withdrawals. Several implementa-
tions used encryption, but without authentication. These
implementations used a library such as OpenSSL, the Java
Figure 8: Count of security bugs found by each break-it
team, organized by contest and whether the team also par-
ticipated in build-it. The heavy vertical line in the box is
the median, the boxes show the ﬁrst and third quartiles, and
the whiskers extend to the most outlying data within ±1.5×
the interquartile range. Dots indicate further outliers.
found. This model (Table 7) again shows that team size
is important, with an average of one extra security bug
found for each additional team member. Being a qualiﬁed
builder also signiﬁcantly helps one’s score; this makes in-
tuitive sense, as one would expect to gain a great deal of
insight into how a system could fail after successfully build-
ing a similar system. Figure 8 shows the distribution of
the number of security bugs found, per contest, for break-it
teams that were and were not qualiﬁed build-it teams. Note
that all but three of the 108 break-it teams made some at-
tempt, as deﬁned by having made a commit, to participate
during the build-it phase—most of these (93) qualiﬁed, but
12 did not.
If the reason was that these teams were less
capable programmers, that may imply that programming
ability generally has some correlation with break-it success.
On average, four more security bugs were found by a Fall
2015 team than a Spring 2015 team. This contrasts with the
ﬁnding that Spring 2015 teams had higher overall break-it
scores, but corresponds to the ﬁnding that more Fall 2015
submissions had security bugs found against them. As dis-
cussed above, this is because correctness bugs dominated in
Spring 2015 but were not as dominant in Fall 2015. Once
again, the reasons may have been the smaller budget on per-
submission correctness bugs in Fall 2015, and the greater
potential attack surface in the ATM problem.
5. QUALITATIVE ANALYSIS
As part of the data gathered, we also have the entire pro-
gram produced during the build-it phase as well as the pro-
grams patched during the ﬁx-it phase. We can then perform
a qualitative analysis of the programs which is guided by
knowing the security outcome of a given program. Did lots
of break-it teams ﬁnd bugs in the program, or did they not?
What are traits or characteristics of well-designed programs?
5.1 Success Stories
The success stories bear out some old chestnuts of wis-
dom in the security community: submissions that fared well
through the break-it phase made heavy use of existing high-
cryptographic framework, or the Python pycrypto library to
have access to a symmetric cipher such as AES, but either
did not use these libraries at a level where authentication
was provided in addition to encryption, or they did not en-
able authentication.
Some failures were common across log implementations as
well: if an implementation used encryption, it might not use
authentication. If it used authentication, it would authenti-
cate records stored in the ﬁle individually and not globally.
The implementations would also relate the ordering of en-
tries in the ﬁle to the ordering of events in time, allowing
for an integrity attack that changes history by re-ordering
entries in the ﬁle.
As a corpus for research, this data set is of interest for
future mining. What common design patterns were used and
how did they impact the outcome? Are there any metrics
we can extract from the code itself that can predict break-it
scores? We defer this analysis to future work.
6. RELATED WORK
BIBIFI bears similarity to existing programming and se-
curity contests but is unique in its focus on building secure
systems. BIBIFI also is related to studies of code and secure
development, but diﬀers in its open-ended contest format.
Contests. Cybersecurity contests typically focus on vulner-
ability discovery and exploitation, and sometimes involve a
system administration component for defense. One popular
style of contest is dubbed capture the ﬂag (CTF) and is ex-
empliﬁed by a contest held at DEFCON [22]. Here, teams
run an identical system that has buggy components. The
goal is to ﬁnd and exploit the bugs in other competitors’
systems while mitigating the bugs in your own. Compro-
mising a system enables a team to acquire the system’s key
and thus “capture the ﬂag.” In addition to DEFCON CTF,
there are other CTFs such as iCTF [6, 12] and PicoCTF [5].
The use of this style of contest in an educational setting
has been explored in prior work [10, 14, 20]. The Collegiate
Cyber Defense Challenge [25, 9, 8] and the Maryland Cy-
ber Challenge & Competition [24] have contestants defend
a system, so their responsibilities end at the identiﬁcation
and mitigation of vulnerabilities. These contests focus on
bugs in systems as a key factor of play, but neglect software
development.
Programming contests challenge students to build clever,
eﬃcient software, usually with constraints and while under
(extreme) time pressure. The ACM programming contest [2]
asks teams to write several programs in C/C++ or Java dur-
ing a 5-hour time period. Google Code Jam [18] sets tasks
that must be solved in minutes, which are then graded ac-
cording to development speed (and implicitly, correctness).
Topcoder [35] runs several contests; the Algorithm compe-
titions are small projects that take a few hours to a week,
whereas Design and Development competitions are for larger
projects that must meet a broader speciﬁcation. Code is
judged for correctness (by passing tests), performance, and
sometimes subjectively in terms of code quality or practi-
cality of design. All of these resemble the build-it phase
of BIBIFI but typically consider smaller tasks; they do not
consider the security of the produced code.
Studies of secure software development. There have been
a few studies of diﬀerent methods and techniques for ensur-
ing security. Work by Finifter and Wagner [16] and Prechelt [28]
relates to both our build-it and break-it phases: they asked
diﬀerent teams to develop the same web application using
diﬀerent frameworks, and then subjected each implementa-
tion to automated (black box) testing and manual review.
They found that both forms of review were eﬀective in dif-
ferent ways, and that framework support for mitigating cer-
tain vulnerabilities improved overall security. Other studies
focused on the eﬀectiveness of vulnerability discovery tech-
niques, e.g., as might be used during our break-it phase.
Edmundson et al. [15] considered manual code review; Scan-
dariato et al. [33] compared diﬀerent vulnerability detection
tools; other studies looked at software properties that might
co-occur with security problems [37, 38, 19]. BIBIFI diﬀers
from all of these in its open-ended, contest format: Partici-
pants can employ any technique they like, and with a large
enough population and/or measurable impact, the eﬀective-
ness of a given technique will be evident in ﬁnal outcomes.
7. CONCLUSIONS
This paper has presented Build-it, Break-it, Fix-it (BIBIFI),
a new security contest that brings together features from
typical security contests, which focus on vulnerability de-
tection and mitigation but not secure development, and pro-
gramming contests, which focus on development but not se-
curity. During the ﬁrst phase of the contest, teams construct
software they intend to be correct, eﬃcient, and secure. Dur-
ing the second phase, break-it teams report security vulner-
abilities and other defects in submitted software.
In the
ﬁnal, ﬁx-it, phase, builders ﬁx reported bugs and thereby
identify redundant defect reports. Final scores, following an
incentives-conscious scoring system, reward the best builders
and breakers.
During 2015, we ran three contests involving a total of 116
teams and two diﬀerent programming problems. Quantita-
tive analysis from these contests found that the best per-
forming build-it submissions used C/C++, but submissions
coded in a statically-typed language were less likely to have
a security ﬂaw; build-it teams with diverse programming-
language knowledge also produced more secure code. Shorter
programs correlated with better scores. Break-it teams that
were also successful build-it teams were signiﬁcantly better
at ﬁnding security bugs.
There are many interesting areas of future work that BIBIFI
opens up. The BIBIFI design lends itself well to conduct-
ing more focused studies; our competitions allow partici-
pants to use any languages and tools they desire, but one
could narrow their options for closer evaluation. Although
we have reduced manual judging considerably, an interesting
technical problem that often arises is determining whether
two bugs are morally equivalent; an automated method for
determining this could be broadly applicable. Finally, one
limitation of our study is that we do not evaluate whether
break-it teams ﬁnd all of the bugs there are to ﬁnd; one
improvement would be to apply a set of fuzzers and static
analyzers, or to recruit professional teams to eﬀectively par-
ticipate in the break-it phase as a sort of baseline against
which to compare the break-it teams’ performance.
We plan to freely release BIBIFI to support future re-
search. We believe it can act as an incubator for ideas
to improve secure development. More information, data,
and opportunities to participate are available at https://
builditbreakit.org.
Acknowledgments
We thank Jandelyn Plane and Atif Memon who contributed
to the initial development of BIBIFI and its preliminary data
analysis. Many people in the security community, too nu-
merous to list, contributed ideas and thoughts about BIBIFI
during its development—thank you! Bobby Bhattacharjee
and the anonymous reviewers provided helpful comments
on drafts of this paper. This project was supported gifts
from Accenture, AT&T, Galois, Leidos, Patriot Technolo-
gies, NCC Group, Trail of Bits, Synposis, ASTech Consult-
ing, Cigital, SuprTek, Cyberpoint, and Lockheed Martin;
by a grant from the NSF under award EDU-1319147; by
DARPA under contract FA8750-15-2-0104; and by the U.S.
Department of Commerce, National Institute for Standards
and Technology, under Cooperative Agreement 70NANB15H330.
8. REFERENCES
[1] Abadi, M., Budiu, M., Erlingsson, U., and Ligatti, J.
Control-ﬂow integrity principles, implementations, and
applications. ACM Transactions on Information and
System Security (TISSEC) 13, 1 (2009), 4:1–4:40.
[2] The ACM-ICPC International Collegiate Programming
Contest. http://icpc.baylor.edu.
[3] Bernstein, D. J., Lange, T., and Schwabe, P. The
security impact of a new cryptographic library. In
International Conference on Cryptology and Information
Security in Latin America (2012).
[4] Burnham, K. P., Anderson, D. R., and Huyvaert, K. P.
AIC model selection and multimodel inference in behavioral
ecology: some background, observations, and comparisons.
Behavioral Ecology and Sociobiology 65, 1 (2011), 23–35.
[5] Chapman, P., Burket, J., and Brumley, D. PicoCTF: A
game-based computer security competition for high school
students. In USENIX Summit on Gaming, Games, and
Gamiﬁcation in Security Education (3GSE) (2014).
[6] Childers, N., Boe, B., Cavallaro, L., Cavedon, L.,
Cova, M., Egele, M., and Vigna, G. Organizing large
scale hacking competitions. In DIMVA (2010).
[7] Cohen, J. Statistical Power Analysis for the Behavioral
Sciences. Lawrence Erlbaum Associates, 1988.
[8] Conklin, A. The use of a collegiate cyber defense
competition in information security education. In
Information Security Curriculum Development Conference
(InfoSecCD) (2005).
[9] Conklin, A. Cyber defense competitions and information
security education: An active learning solution for a
capstone course. In Hawaii International Conference on
System Sciences (HICSS) (2006).
[10] Conti, G., Babbitt, T., and Nelson, J. Hacking
competitions and their untapped potential for security
education. Security & Privacy 9, 3 (2011), 56–59.
[11] DEF CON Communications, Inc. Capture the ﬂag
archive. https://www.defcon.org/html/links/dc-ctf.html.
[12] Doup´e, A., Egele, M., Caillat, B., Stringhini, G.,
Yakin, G., Zand, A., Cavedon, L., and Vigna, G. Hit
’em where it hurts: A live security exercise on cyber
situational awareness. In Annual Computer Security
Applications Conference (ACSAC) (2011).
[13] dragostech.com inc. Cansecwest applied security
conference. http://cansecwest.com.
[14] Eagle, C. Computer security competitions: Expanding
educational outcomes. IEEE Security & Privacy 11, 4
(2013), 69–71.
[15] Edmundson, A., Holtkamp, B., Rivera, E., Finifter,
M., Mettler, A., and Wagner, D. An empirical study on
the eﬀectiveness of security code review. In International
Symposium on Engineering Secure Software and Systems
(ESSoS) (2013).
[16] Finifter, M., and Wagner, D. Exploring the relationship
between web application development tools and security. In
USENIX Conference on Web Application Development
(WebApps) (2011).
[17] Git – distributed version control management system.
http://git-scm.com.
[18] Google code jam. http://code.google.com/codejam.
[19] Harrison, K., and White, G. An empirical study on the
eﬀectiveness of common security measures. In Hawaii
International Conference on System Sciences (HICSS)
(2010).
[20] Hoffman, L. J., Rosenberg, T., and Dodge, R.
Exploring a national cybersecurity exercise for universities.
IEEE Security & Privacy 3, 5 (2005), 27–33.
[21] ICFP programming contest. http://icfpcontest.org.
[22] Inc, D. C. C. Def con hacking conference.
http://www.defcon.org.
[23] Kim, Q. Want to learn cybersecurity? Head to Def Con.
http://www.marketplace.org/2014/08/25/tech/
want-learn-cybersecurity-head-def-con, 2014.
[24] Maryland cyber challenge & competition. http://www.
fbcinc.com/e/cybermdconference/competitorinfo.aspx.
[25] National Collegiate Cyber Defense Competition.
http://www.nationalccdc.org.
[26] Parker, J. LMonad: Information ﬂow control for Haskell
web applications. Master’s thesis, Dept of Computer
Science, the University of Maryland, 2014.
[27] Polytechnic Institute of New York University. Csaw
- cybersecurity competition 2012.
http://www.poly.edu/csaw2012/csaw-CTF.
[28] Prechelt, L. Plat forms: A web development platform
comparison by an exploratory experiment searching for
emergent platform properties. IEEE Transactions on
Software Engineering 37, 1 (2011), 95–108.
[29] PostgreSQL. http://www.postgresql.org.
[30] Ruef, A., and Hicks, M. Build it, break it, ﬁx it:
Competing to build secure systems. The Next Wave 21, 1
(2015), 19–23.
[31] Ruef, A., Hicks, M., Parker, J., Levin, D., Memon, A.,
Plane, J., and Mardziel, P. Build it break it: Measuring
and comparing development security. In Workshop on
Cyber Security Experimentation and Test (CSET) (2015).
[32] Saltzer, J. H., and Schroeder, M. D. The protection of
information in computer systems. Proceedings of the IEEE
63, 9 (1975), 1278–1308.
[33] Scandariato, R., Walden, J., and Joosen, W. Static
analysis versus penetration testing: A controlled
experiment. In IEEE International Symposium on
Reliability Engineering (ISSRE) (2013).
[34] Stefan, D., Russo, A., Mitchell, J., and Mazieres, D.
Flexible dynamic information ﬂow control in Haskell. In
ACM SIGPLAN Haskell Symposium (2011).
[35] Top coder competitions. http://apps.topcoder.com/wiki/
display/tc/Algorithm+Overview.
[36] ´Ulfar Erlingsson. personal communication stating that
CFI was not deployed at Microsoft due to its overhead
exceeding 10%, 2012.
[37] Walden, J., Stuckman, J., and Scandariato, R.
Predicting vulnerable components: Software metrics vs text
mining. In IEEE International Symposium on Software
Reliability Engineering (2014).
[38] Yang, J., Ryu, D., and Baik, J. Improving vulnerability
prediction accuracy with secure coding standard violation
measures. In International Conference on Big Data and
Smart Computing (BigComp) (2016).
[39] Yesod web framework for Haskell.
http://www.yesodweb.com
.