### Security Vulnerabilities in Software Implementations

Security implementations in software can vary widely, from systems that provide no security at all to those that are vulnerable only to highly sophisticated timing attacks. This spectrum mirrors the dynamics observed in today's software market.

#### Log Problem Implementations

Many implementations of the log problem lacked encryption or authentication, making them trivial for break-it teams to exploit. In some cases, log data was written as plain text, while in others, it was serialized using the Java object serialization protocol. One team discovered a privacy flaw that could be exploited with just fifty probes. The target submission truncated the "authentication token," rendering it vulnerable to a brute force attack.

#### ATM Problem Implementations

The ATM problem allowed for interactive attacks, which were not possible with the log problem. As implementations used cryptographic constructions incorrectly, the attacks became more sophisticated. For example, one implementation used RC4 but implemented it from scratch and did not add any randomness to the key or cipher stream. This made the ciphertext distinguishable and largely unchanged across transactions, allowing attackers to flip bits in messages to alter withdrawal amounts.

Another implementation used encryption with authentication but without randomness, making error messages distinguishable from success messages. An attacker could leak the bank balance by observing different withdrawal attempts, distinguishing successful from failed transactions, and performing a binary search to identify the bank balance.

Common failures in ATM problem implementations included:
- Fixed keys across the lifetime of the bank and ATM programs.
- Lack of nonces in messages, enabling replay attacks.
- Use of encryption without authentication, often relying on libraries like OpenSSL or the Java cryptographic framework, but not enabling authentication.

#### Analysis of Security Bugs

Figure 8 shows the count of security bugs found by each break-it team, organized by contest and whether the team also participated in the build-it phase. The median, first and third quartiles, and outliers are indicated. Team size is a significant factor, with an average of one extra security bug found for each additional team member. Qualified builders, who have successfully built similar systems, are significantly better at finding security bugs.

In Fall 2015, teams found an average of four more security bugs than in Spring 2015, despite Spring 2015 teams having higher overall break-it scores. This discrepancy is due to the dominance of correctness bugs in Spring 2015 and the greater potential attack surface in the ATM problem in Fall 2015.

### Qualitative Analysis

We analyzed the entire program produced during the build-it phase and the programs patched during the fix-it phase. This analysis is guided by the security outcomes of the programs, such as the number of bugs found by break-it teams and the characteristics of well-designed programs.

#### Success Stories

Successful submissions often followed established security best practices, such as using existing high-level cryptographic frameworks and ensuring both encryption and authentication. These submissions fared well through the break-it phase.

### Related Work

BIBIFI (Build-it, Break-it, Fix-it) is a unique security contest that combines features of typical security contests, which focus on vulnerability detection and mitigation, and programming contests, which focus on development but not security. BIBIFI is related to studies of code and secure development but differs in its open-ended contest format.

#### Cybersecurity Contests

Cybersecurity contests, such as capture the flag (CTF) events, focus on vulnerability discovery and exploitation. Popular CTFs include DEFCON CTF, iCTF, and PicoCTF. These contests have been explored in educational settings but typically do not involve software development.

#### Programming Contests

Programming contests challenge students to build efficient software under time constraints. Examples include the ACM programming contest, Google Code Jam, and Topcoder. These contests resemble the build-it phase of BIBIFI but typically consider smaller tasks and do not focus on the security of the produced code.

#### Studies of Secure Software Development

Studies have examined different methods and techniques for ensuring security. For example, Finifter and Wagner [16] and Prechelt [28] asked teams to develop the same web application using different frameworks and subjected each implementation to automated testing and manual review. Other studies have focused on the effectiveness of vulnerability discovery techniques.

### Conclusions

BIBIFI is a new security contest that brings together features from typical security and programming contests. During the build-it phase, teams construct software intended to be correct, efficient, and secure. In the break-it phase, teams report security vulnerabilities and other defects. Finally, in the fix-it phase, builders address reported bugs and identify redundant defect reports.

Quantitative analysis from three contests in 2015 involving 116 teams and two programming problems found that the best-performing build-it submissions used C/C++, and statically-typed languages were less likely to have security flaws. Successful build-it teams with diverse programming-language knowledge produced more secure code. Shorter programs correlated with better scores, and break-it teams that were also successful build-it teams were significantly better at finding security bugs.

### Future Work

BIBIFI opens up many areas for future research, including conducting more focused studies, reducing manual judging, and developing automated methods for determining bug equivalence. We plan to freely release BIBIFI to support future research and believe it can act as an incubator for ideas to improve secure development.

### Acknowledgments

We thank Jandelyn Plane and Atif Memon for their contributions to the initial development of BIBIFI and its preliminary data analysis. Many people in the security community provided valuable input. Bobby Bhattacharjee and the anonymous reviewers provided helpful comments on drafts of this paper. This project was supported by gifts from various organizations and grants from the NSF, DARPA, and NIST.

### References

[References listed as in the original document]

This revised version aims to make the text more coherent, professional, and easier to follow, while maintaining the original content and structure.