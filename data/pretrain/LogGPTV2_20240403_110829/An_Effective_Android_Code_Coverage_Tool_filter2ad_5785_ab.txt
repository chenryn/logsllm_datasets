2 ACVTOOL ARCHITECTURE
ACVTool allows to measure and analyze the degree to which the
code of a closed-source Android app is executed during testing,
and to collect crash reports occurred during this process. The tool
instruments an app and measures code coverage at instruction,
method and class granularities. We designed ACVTool to be self-
contained in order to make its integration with various testing
solutions as easy as possible. In addition, the tool is helpful in
manual analysis for investigating which code parts of a third-party
application have been executed.
Figure 1 shows the main phases of ACVTool workflow: offline,
online, and report generation phases, as indicated in upper, lower
and right-most parts of the scheme, correspondingly. The offline
phase handles the preprocessing of an app prior to its installation
and instruments it. During the online phase, ACVTool installs the
instrumented app and enables the data collection during testing.
Later, in the report generation phase, ACVTool produces the infor-
mation about code coverage. Below we describe the workflow of
ACVTool in more details.
2.1 Offline phase
Instrumentation of a third-party app is not a trivial task due to
the absence of the source code. This task requires to inject specific
instrumentation bytecode instructions (probes) into the original
bytecode. We based our tool on Apktool [15] that utilizes the backs-
mali disassembler [12] under the hood. We use this tool to disas-
semble an app into smali code – a human-readable representation
of Android bytecode. Then we insert probes that log the execution
of the original smali instructions. Afterwards, ACVTool builds a
new version of the app and signs it with apksigner. Thus, ACVTool
can instrument almost all applications that Apktool can repack-
age. Moreover, we do not worry about new versions of DEX files
that might introduce new types of bytecode instructions: updating
ACVTool to handle these new instructions will require little effort.
Smali code modification. Due to the stack-based Android architec-
ture, modifying the smali code is not straightforward. We use the
direct instrumentation approach previously introduced by Huang et
al. [8] and Liu et al. [9], in conjunction with a register management
technique. However, our approach is more efficient.
In our solution, we have optimized the mechanism of registering
the execution of probes by maintaining a binary array. A probe
writes a value into the corresponding cell of the binary array when
executed. Writing binary values into specific array cells is a very
DecompileAndroid ManifestSmali CodeInstrumentedAndroid ManifestInstrumentedSmali CodeInstrumentBuild&SignInstallTestInstrumentationReportRuntimeReportCollectApktoolACVToolACVToolApktoolapksigneradbmanualautomaticadbCrashDataOfflineOnlineGeneratethat there is no drastic run-time overhead introduced by our in-
strumentation. We now work on an experiment to evaluate the
run-time overhead precisely.
4 DEMO DETAILS
We plan an interactive demonstration of the whole coverage mea-
surement cycle on real applications. In the demo, we will walk the
audience through the ACVTool design. We will also show how the
smali code looks like before and after injecting the probes.
5 CONCLUSION
We reported on a novel tool for black-box code coverage measure-
ment of Android applications. We have significantly improved the
smali instrumentation technique and consequently our instrumen-
tation success rate is 96.9%, compared with 36% in Huang et al. [8]
and 65% in Zhauniarovich et al. [17]. Furthermore, our implemen-
tation is open-source and available for the community.
Acknowledgements
This research was partially supported by Luxembourg National
Research Fund through grants AFR-PhD-11289380-DroidMod and
C15/IS/10404933/COMMA.
REFERENCES
[1] K. Allix, T. Bissyandé, J. Klein, and Y. Le Traon. 2016. AndroZoo: Collecting
Millions of Android Apps for the Research Community. In Proc. of MSR. ACM,
468–471.
[2] H. Cai and B. Ryder. 2017. DroidFax: A toolkit for systematic characterization of
[4] S. R. Choudhary, A. Gorla, and A. Orso. 2015. Automated test input generation
Android applications. In Proc. of ICSME. IEEE, 643–647.
[3] P. Carter, C. Mulliner, M. Lindorfer, W. Robertson, and E. Kirda. 2016. Curious-
Droid: Automated user interface interaction for Android application analysis
sandboxes. In Proc. of FC. Springer, 231–249.
for Android: Are we there yet?. In Proc. of ASE. IEEE/ACM, 429–440.
[5] S. Dashevskyi, O. Gadyatskaya, , A. Pilgun, and Y Zhauniarovich. 2018. POSTER:
The Influence of Code Coverage Metrics on Automated Testing Efficiency in
Android. In Proc. of CCS.
com/saswatanand/ella.
[6] ELLA. 2016. A Tool for Binary Instrumentation of Android Apps, https://github.
[7] Google. 2018. https://source.android.com/devices/tech/dalvik/dalvik-bytecode.
[8] C. Huang, C. Chiu, C. Lin, and H. Tzeng. 2015. Code Coverage Measurement for
Android Dynamic Analysis Tools. In Proc. of Mobile Services (MS). IEEE, 209–216.
[9] J. Liu, T. Wu, X. Deng, J. Yan, and J. Zhang. 2017. InsDal: A safe and extensible
instrumentation tool on Dalvik byte-code for Android applications. In Proc. of
SANER. IEEE, 502–506.
[10] K. Mao, M. Harman, and Y. Jia. 2016. Sapienz: Multi-objective automated testing
for Android applications. In Proc. of ISSTA. ACM, 94–105.
[11] K. Moran, M. Linares-Vásquez, C. Bernal-Cárdenas, C. Vendome, and D. Poshy-
vanyk. 2016. Automatically discovering, reporting and reproducing Android
application crashes. In Proc. of ICST.
[12] smali/backsmali. 2018. https://github.com/JesusFreke/smali.
[13] W. Song, X. Qian, and J. Huang. 2017. EHBDroid: Beyond GUI testing for Android
[14] F. Wei, Y. Li, S. Roy, X. Ou, and W. Zhou. 2017. Deep ground truth analysis of
applications. In Proc. of ASE. IEEE/ACM, 27–37.
current Android malware. In Proc. of DIMVA.
[15] R. Wiśniewski and C. Tumbleson. 2017. Apktool - A tool for reverse engineering
3rd party, closed, binary Android apps. https://ibotpeaches.github.io/Apktool/
[16] Z. Yang, M. Yang, Y. Zhang, G. Gu, P. Ning, and X S. Wang. 2013. Appintent:
Analyzing sensitive data transmission in Android for privacy leakage detection.
In Proc. of CCS.
[17] Y. Zhauniarovich, A. Philippov, O. Gadyatskaya, B. Crispo, and F. Massacci. 2015.
Towards black box testing of Android apps. In Proc. of SAW at ARES. IEEE, 501–
510.
Figure 2: The ACVTool code coverage report
Table 1: ACVTool performance evaluation
Parameter
Total # selected apps
Average apk size
Instrumented apps
Healthy instrumented apps
Avg. instrumentation time
(total per app)
F-Droid
benchmark
448
3.1MB
444 (99.1%)
440 (98.2%)
24.7 sec
Google Play
benchmark
398
11.1MB
382 (95.9%)
380 (95.4%)
49.6 sec
Total
846
6.8MB
97.6%
96.9%
36.2 sec
executed during testing and the information about total amount
of lines, methods and classes correspondingly. We generate also
an xml version of the report containing the same code coverage
information suitable for integrating in automated testing tools.
3 EVALUATION
We have extensively tested ACVTool on real-life third party appli-
cations. For the lack of space, we only report very basic evaluation
statistics, summarized in Table 1.
Instrumentation success rate. For evaluation we have collected
all application projects from the popular open-source F-Droid mar-
ket, which is frequently used in evaluation of automated testing
tools (e.g., in [10]). Among all projects on F-Droid, we were able to
successfully build and launch on a device 448 apps. We have also
randomly selected 500 apps from the AndroZoo [1] snapshot of the
Google Play market, targeting apps released after the Android API
22. This sample is therefore representative of real-life third-party
apps that may use anti-debugging techniques. Among the 500 se-
lected apps, only 398 were launch-able on a device (the rest crashed
immediately or gave installation errors). Thus, in total we tested
ACVTool on 846 apps, with an average apk size of 6.8MB.
As shown in Table 1, ACVTool successfully instrumented 97.6%
of these apps. Among the failures, 13 apps could not be repackaged
by the Apktool, and the others have produced some exceptions
during the instrumentation process. We then installed and launched
all successfully instrumented apps, and found that 6 apps crashed
immediately due to various errors. We thus can evaluate the total
instrumentation success rate of ACVTool to be 96.9% on our dataset.
Overhead. As reported in Table 1, ACVTool introduces relatively
small instrumentation-time overhead (36.2 seconds per app, on av-
erage) that is acceptable in the offline part of testing and analysis.
We have also estimated the potential run-time overhead introduced
by the added instrumentation code by running the original and
repackaged app versions with the same Monkey scripts and com-
paring the execution timings. For 50 apps randomly selected from
our dataset we have not found any significant difference in the
execution time (median time difference less than 0.08 sec, mean
difference less than 0.12 sec, standard deviation 0.84 sec), and we
have not seen any unexpected crashes. This experiment suggests