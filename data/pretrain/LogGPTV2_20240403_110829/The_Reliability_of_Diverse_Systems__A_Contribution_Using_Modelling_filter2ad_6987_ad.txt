their 
Our results in the last  two  sections yield  some useful 
indications for practical  decisions. However, these  depend 
on the truth of the assumptions used in the modelling.  All 
modelling  is  an  exercise  in  abstraction, trying  to  achieve 
simplicity  by  discarding  those  aspects  of  reality  that  are 
indeed  negligible.  We  now  discuss  the  assumptions  we 
have used  from this viewpoint. 
6.1. Non-independence between development 
errors on the same version 
- 
the  occurrences  of  certain  mistakes 
There is no evidence that the  possible  mistakes  in  the 
development  of  a  single  program  occur  independently. 
Results  from  any  one  experiment  can  only  give  weak 
evidence  to  support  or  refute  this  assumption.  There  are 
plausible reasons for not believing in independence: 
- 
there  are  factors  that  would  cause positive  correlation 
among 
in 
developing  a  program,  e.g.,  those  mistakes  that  are 
due to a common conceptual  error; 
there are factors that would favour  negative correlation, 
e.g.,  if  schedule  or  budget  limits  mean  that  effort 
spent  on  avoiding  certain  classes  of  faults  reduces 
effort on others. For instance, the  random  discovery  of 
some  problems  early  in  the  project  schedule  might 
divert  resources  from  dealing  with  other  potential 
problems. 
We  do  not  know  the  weights  of  these  contrasting 
factors in practice.  If  the  individual  mistakes  are unlikely, 
and the  probability  of  any  set of  them  occurring  together 
is  much  lower  than  their  individual  probabilities  of 
12 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:33 UTC from IEEE Xplore.  Restrictions apply. 
produce  optimistic  posteriors.  So,  a  study  of  inference 
methods would require  an  auxiliary  study  of  the  effects of 
these errors in the priors. 
In conclusion, the reality  of  overlap  between  possible 
failure regions does not affect  the  usefulness  of  our model 
for  very  reliable  software,  and  of  the  model's  use  for 
absolute pessimistic  prediction  in general. 
6.3 Unique 140-1 mapping between faults and 
failure regions 
In  practice,  for any  given  failure region  there  will  be 
multiple  possible  faults  that  could  introduce 
it,  and 
mistakes that  could  cause  such  faults.  This  increases  an 
assessor's difficulty  in  choosing  the p ,  parameters,  or pmrr.; 
when  only  pmUi is  needed.  Presumably,  assessors  will 
derive  beliefs  about  these  parameters  from  their  own 
experience  of  faults  found,  or  mistakes  detected,  in 
circumstances  considered  similar  to  those  of  the  project 
being assessed.  Let  us  stipulate that  an  assessor  is  indeed 
able to select from memory appropriate similar situations, 
and  properly  infer  the  probabilities  of  mistakes  being 
made (or of faults being  inserted)  and  not  corrected. But if 
several  possible  faults  would  cause  the  same  failure 
region, the probability  of that  failure region  being  present 
could  be  close  to  the  sum  of  the  probabilities  of  those 
faults: assessors would  then risk underestimating pmox. 
The other major problem is of course that if  the p ,  are 
small, then the assessors' experience of  these  faults  would 
be  very  limited;  actually,  the  assessors'  beliefs  could  be 
based  on  kinds  of  faults that  are relatively  easy  to  detect 
and eliminate (and thus would be noticed often) rather than 
on  types  that  are  more  likely  to  remain  in  the  finished 
products.  But  this  problem  is  common  to  all  approaches 
relying  on  the  assessors' judgement,  whether  applied  to 
diverse systems or non-diverse  systems  and  whether  using 
explicit mathematical representations or not. 
In  conclusion,  when  I-to-l  mappings  between  fault, 
code defects and failure regions cannot  be  trusted,  the  only 
way  of  trusting  the  model's  conclusions  is  to  apply  the 
model to the probabilities  of  failure  regions  being  present 
rather than of code defects. 
7. Conclusions 
Compared  to  previous  discussions  of  the  advantages 
of diversity, this paper offers these elements of progress: 
-  our  model  is  based  on  assumptions  that  refer  more 
to  physical  phenomena  (like  human 
immediately 
to  abstract 
errors 
aggregated  measures 
of 
failure 
probabilities). A  discussion  of  these  assumptions  can 
refer  more  to  direct  empirical  experience  and  less  to 
general  intuition; 
in  development) 
rather 
a 
than 
ratio 
(like 
is 
to 
that 
sufficiently  accurate 
in  particular, 
the  model's  assumptions  can  be 
challenged  by  experiment.  Those  that  are  refuted  can 
be  altered,  and  we  can  hope  to  produce  a  model  of 
reality 
support 
decisions, at least about how best to achieve reliability 
if  not about accepting specific reliability  claims; 
if this model turns out to be reasonably accurate, it  can 
be  a  basis  for  analysing  future  statistical  data  and 
especially  for drawing inference on  the  reliability  of  a 
design-diverse system from  its  behaviour  in  operation, 
i.e.,  to  help  in  assessing  the  reliability  of  a  specific 
system; 
we  discuss  measures  of  interest  in  practical  decision- 
making,  rather  than  the  average  reliability  studied  in 
previous literature. 
Obviously, 
the  results  described  here  should  be 
validated  against  empirical  results.  There  are  the  usual 
difficulties that practical industrial  projects  only  develop a 
few versions  of  any  given  application, so  that  validation 
of  any  general  prediction  about  probability  distributions 
would depend on sophisticated collation of data from  many 
projects;  and  that  these  program  versions  are  usually 
highly  reliable,  so  that  failure  rates  cannot  be  estimated 
with  much  accuracy.  Turning  to  published  experiments, 
we  have  observed  for  instance  that  in  the  Knight  and 
Leveson  experiment [2, 16, 171 diversity  reduced not  only 
the sample  mean  of  the  PFD  of  the  27  program  versions 
produced, but also -greatly-  its standard  deviation.  At  this 
qualitative  level,  our  conclusions  are  supported.  On  the 
other hand, the data do not  fit  (nor  would  we expect  them 
faults  observed)  a  normal 
to 
approximation  for  the  distribution  of  PFD,  so  that  we 
cannot  check  the  relationship  predicted  in  Section  5  to 
hold  between  distributions for cases  in  which  the  normal 
approximation applies.  We plan  to  continue  such  checks 
of  model  predictions  against  previously  published  data, 
both  exploiting more published  data  sets  and  looking  for 
more sophisticated  ways of using the data to  challenge  our 
conclusions. 
fit,  given 
few 
the 
Extending experimental knowledge  of  design  diversity 
to  the  point  that  we can  base  practical  recommendations, 
with  high  confidence,  on  empirical  knowledge  alone  is 
infeasible.  In  practice,  engineering  decisions  always  need 
to  use  a  combination  of  empirical  knowledge  and 
analytical  extrapolation.  The  advantage  of  basing  the 
extrapolation on rigorous mathematical reasoning is in  the 
first  place  consistency:  among  the  intuitive  predictions 
without  strong scientific  bases,  we  can  at  least  weed  out 
those  that  would  make  our  body  of  knowledge  self- 
contradictory,  and signal  the  important  gaps in  this  body 
of  knowledge.  The practice  of  assessing software,  diverse 
or  otherwise,  as  compliant  with  quantitative  reliability 
requirements 
intuitive 
judgement by  dedicated, experienced assessors, in  the best 
is  now  a  matter  of  mostly 
13 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:33 UTC from IEEE Xplore.  Restrictions apply. 
[3] D. E.  Eckhardt and  L.  D. Lee, "A  theoretical  basis  for  the 
analysis  of  multiversion  software  subject 
to  coincident 
errors", IEEE TSE, SE-II, pp.  1511-1517,  1985. 
[4] B. Littlewood and D. R. Miller, "Conceptual Modelling  of 
Coincident  Failures  in  Multi-Version  Software",  IEEE  TSE, 
SE-15, pp.  1596-1614,  1989. 
[5] B.  Littlewood,  P.  Popov  and  L.  Strigini,  "Modelling 
software  design  diversity  -  a  review",  ACM  Computing 
Surveys, pp.  to  appear,  2001. 
[6]  B.  Littlewood,  P.  Popov  and  L.  Strigini,  "N-version 
design  Versus one Good  Version",  International  Conference 
on  Dependable  Systems  & Networks  (FITS-30,  DCCA-8)  - 
Fast Abstracts, New  York,  USA, 2000, pp. B42-B43. 
[7]  P.  Popov,  L.  Strigini  and  B.  Littlewood,  "Choosing 
between  Fault  Tolerance  and  Increased  V&V  for  Improving 
Reliability",  in  Proc.  International  Conference  on  Parallel 
and  Distributed  Processing  Techniques  and  Applications 
(PDPTA'2000), Las Vegas, Nevada, USA, 2000, 
[SI  B.  Littlewood,  P.  Popov  and  L.  Strigini,  "A  note  on 
reliability  estimation  of 
functionally  diverse  systems", 
Reliability  Engineering  and  System  Safety,  66,  pp.  93-95, 
1999. 
[9] P. G. Bishop and F. D.  Pullen,  "PODS Revisited  - A Study 
of  Software  Failure  Behaviour",  in  Proc.  FITS-18,  Tokyo, 
Japan,  1988, pp.  1-8. 
[IO]  P.  E.  Ammann  and  J.  C.  Knight,  "Data  Diversity:  An 
Approach  to  Software Fault Tolerance",  IEEE TC,  C-37,  pp. 
418-425,  1988. 
[Ill  L.  Hatton  and  A.  Roberts,  "How  accurate  is  scientific 
software?", IEEE TSE, 20, pp.  785-797,  1994. 
[I21  P.  Frankl,  D.  Hamlet,  B.  Littlewood  and  L.  Strigini, 
"Evaluating testing  methods  by  delivered reliability",  IEEE 
TSE, SE-24, pp.  586-601,  1998. 
[13] K.  B.  Djambazov  and  P.  Popov,  "The effects of  testing 
on  the  reliability  of  single  version  and  I-out-of-2  software", 
in  Proc. ISSRE'95, Toulouse,  1995, pp.  219-228. 
[I41  B. Littlewood, P. Popov  and L.  Strigini,  "Assessment of 
the  Reliability  of  Fault-Tolerant  Software:  a  Bayesian 
Approach", 
the 
Netherlands,  2000. 
[15]  P.  Popov,  L.  Strigini,  "The  Reliability  of  Diverse 
Systems:  a  Contribution  using  Modelling  of  the  Fault 
Creation  Process",  CSWDISPO  Project  Technical  Report, 
http://www.csr.city.ac.uk/csr_city/projects/diversity/. 
[16]  J.  C.  Knight  and  N. G.  Leveson,  "An  Experimental 
Evaluation  of  the  Assumption  of  Independence  in  Multi- 
Version Programming", IEEE TSE, SE- 12, pp.  96- 109,  1986. 
[17]  S.  S.  Brilliant,  J.  C.  Knight  and  N.  G .   Leveson, 
"Analysis  of  Faults  in  an  N-Version  Software Experiment", 
IEEE TSE, SE-16, pp.  238-247,  1990. 
in  Proc.  SAFECOMP'2000,  Rotterdam, 
case,  and  of  verifying  compliance  with  "software  safety 
standards",  with  no  proven  value  in  reliability  prediction, 
in  the  worst  case.  Assessors  can  use  our  results  (e.g., 
formulas (9), (1 I),  (12)) for comparison with  their  current 
practice  in  judging  diversity:  depending  on  how 
the 
assumptions  seem to  fit  their  existing  situation,  and  on 
the parameter values that their experience suggests or  their 
current  practice  implies,  they  will  find  that  either  our 
results  lend  some extra  confidence  in  current  practice,  or 
raise  questions  about  it  and  specify  some  experimental 
tests to answer these questions. 
As  for decisions  about  whether  and when  diversity  is 
worth  using,  our  results  are  mostly  warnings  against 
oversimplification.  Switching  to  a  "better"  process  that 
produces fewer of all kinds of faults  should  make diversity 
even more useful; but for a generic  improvement the  gain 
given  by  diversity  may  increase  or  decrease,  possibly  to 
the point of making it useless. 
software-specific 
An advantage of our results is that  they  depend on  the 
effects of  a  development  process  on  the  probabilities  of 
failure  regions  being  created  in  the  product,  rather  than 
directly  on  its  effects  on  the  failure  behaviour  of  the 
products.  So,  the  parameters  we use  are  reasonably  close 
to  the  experience  of  real-world  assessors. Our  discussion, 
though, confirms the  need for more  knowledge on  human 
error in software development. Research in this  area would 
require 
taking 
advantage of existing  knowledge in cognitive psychology. 
Desirable extensions of  this  work are:  first, empirical 
checks  on  the  assumptions  and  the  predictions  of  this 
the  cases  of  "forced"  and 
model;  further  study  of 
"functional"  diversity;  and combining this  kind  of  model 
with inference from observations during a  specific  project 
[ 141: it seems a good idea to apply  a  prior  distribution  for 
a  product's  reliability  parameters  that  are  based  on  this 
plausible  physical  model  rather 
than  chosen,  as 
is 
frequently the case, for computational convenience only. 
experimental  work 
Acknowledgments 
The work described in this paper was funded in  part  in 
project  DISPO  (DIverse  Software  Project)  by  British 
Energy  Generation  (UK)  Ltd  within  the  U.K.  Nuclear 
in  project  DISCS 
Safety  Research  Programme,  and 
the  U.K. 
(Diversity  In  Safety  Critical  Software)  by 
Engineering and Physical  Sciences Research Council. 
References 
[I] L.  Hatton, "N-Version Design  Versus  One Good Version", 
IEEE Software, 14, pp. 71-76,  1997. 
[2] J.  C.  Knight,  N.  G. Leveson  and  L. D.  S. Jean,  "A Large 
Scale  Experiment  in  N-Version  Programming", 
in  Proc. 
FTCS-15, Ann  Arbor, Michigan, USA,  1985, pp.  135-139. 
14 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:06:33 UTC from IEEE Xplore.  Restrictions apply.