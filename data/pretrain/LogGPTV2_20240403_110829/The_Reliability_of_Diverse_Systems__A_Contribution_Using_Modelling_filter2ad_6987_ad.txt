### Practical Implications and Assumptions

Our findings in the previous two sections provide valuable insights for practical decision-making. However, these insights are contingent on the validity of the assumptions used in our modeling. All models involve abstraction, aiming to simplify reality by discarding negligible factors. We will now discuss the key assumptions we have made from this perspective.

#### 6.1. Non-independence between Development Errors in the Same Version

There is no clear evidence that errors in the development of a single program occur independently. Results from any one experiment can only offer weak support or refutation of this assumption. There are plausible reasons to doubt independence:

- **Positive Correlation**: Factors such as common conceptual errors can cause certain mistakes to be positively correlated.
- **Negative Correlation**: If schedule or budget constraints lead to a reduction in effort on other fault classes when resources are diverted to address specific issues, negative correlation may occur. For example, the early discovery of some problems might divert resources from addressing other potential issues.

The relative weights of these contrasting factors in practice are unknown. If individual mistakes are unlikely, and the probability of multiple mistakes occurring together is much lower than their individual probabilities, this could produce overly optimistic posterior estimates. Therefore, a study of inference methods would need to account for the effects of these errors in the priors.

In conclusion, the overlap between possible failure regions does not significantly impact the usefulness of our model for highly reliable software or for making absolute pessimistic predictions in general.

#### 6.3. Unique 1-to-1 Mapping between Faults and Failure Regions

In practice, a given failure region can be caused by multiple faults, and each fault can result from various mistakes. This complicates the assessor's task of choosing the appropriate parameters (e.g., \( p_{\text{m}} \) and \( p_{\text{f}} \)). Assessors typically derive their beliefs about these parameters from their experience with similar projects. Assuming assessors can select relevant past experiences and infer the probabilities of mistakes and faults, if several faults can cause the same failure region, the probability of that region being present could be close to the sum of the probabilities of those faults. This could lead to underestimating \( p_{\text{m}} \).

Another major issue is that if the probabilities of faults are small, assessors' experience with these faults would be limited. Their beliefs might be based on easily detectable and correctable faults rather than those more likely to remain in the final product. This problem is common to all approaches relying on assessors' judgments, whether applied to diverse or non-diverse systems and whether using explicit mathematical representations or not.

In conclusion, when 1-to-1 mappings between faults, code defects, and failure regions cannot be trusted, the model should be applied to the probabilities of failure regions being present rather than to the probabilities of code defects.

### Conclusions

Compared to previous discussions on the advantages of diversity, this paper offers the following advancements:

- **Physical Phenomena Focus**: Our model is based on assumptions that relate more to physical phenomena (such as human errors in development) rather than abstract measures of failure probabilities. This allows for more direct empirical validation and less reliance on general intuition.
- **Empirical Validation**: The model's assumptions can be tested through experiments. Refuted assumptions can be adjusted, leading to a more accurate model of reality.
- **Practical Decision-Making**: If the model is reasonably accurate, it can serve as a basis for analyzing future statistical data and drawing inferences about the reliability of design-diverse systems from their operational behavior.
- **Relevant Measures**: We focus on measures of interest for practical decision-making, rather than average reliability, which has been the focus in previous literature.

Clearly, the results described here should be validated against empirical data. Industrial projects typically develop only a few versions of any given application, making it challenging to validate general predictions about probability distributions. Published experiments, such as the Knight and Leveson study, show that diversity reduces both the mean and standard deviation of the Probability of Failure on Demand (PFD). While these results qualitatively support our conclusions, the data do not fit a normal distribution, preventing us from checking the relationships predicted in Section 5.

Extending experimental knowledge of design diversity to the point where practical recommendations can be based solely on empirical data is infeasible. Engineering decisions must combine empirical knowledge with analytical extrapolation. The advantage of basing extrapolation on rigorous mathematical reasoning is consistency: it helps weed out self-contradictory intuitive predictions and highlights important gaps in our knowledge.

### References

[1] L. Hatton, "N-Version Design Versus One Good Version", IEEE Software, 14, pp. 71-76, 1997.
[2] J. C. Knight, N. G. Leveson, and L. D. S. Jean, "A Large Scale Experiment in N-Version Programming", in Proc. FTCS-15, Ann Arbor, Michigan, USA, 1985, pp. 135-139.
[3] D. E. Eckhardt and L. D. Lee, "A Theoretical Basis for the Analysis of Multiversion Software Subject to Coincident Errors", IEEE TSE, SE-11, pp. 1511-1517, 1985.
[4] B. Littlewood and D. R. Miller, "Conceptual Modelling of Coincident Failures in Multi-Version Software", IEEE TSE, SE-15, pp. 1596-1614, 1989.
[5] B. Littlewood, P. Popov, and L. Strigini, "Modelling Software Design Diversity - a Review", ACM Computing Surveys, to appear, 2001.
[6] B. Littlewood, P. Popov, and L. Strigini, "N-version Design Versus one Good Version", International Conference on Dependable Systems & Networks (FITS-30, DCCA-8) - Fast Abstracts, New York, USA, 2000, pp. B42-B43.
[7] P. Popov, L. Strigini, and B. Littlewood, "Choosing Between Fault Tolerance and Increased V&V for Improving Reliability", in Proc. International Conference on Parallel and Distributed Processing Techniques and Applications (PDPTA'2000), Las Vegas, Nevada, USA, 2000.
[8] B. Littlewood, P. Popov, and L. Strigini, "A Note on Reliability Estimation of Functionally Diverse Systems", Reliability Engineering and System Safety, 66, pp. 93-95, 1999.
[9] P. G. Bishop and F. D. Pullen, "PODS Revisited - A Study of Software Failure Behaviour", in Proc. FITS-18, Tokyo, Japan, 1988, pp. 1-8.
[10] P. E. Ammann and J. C. Knight, "Data Diversity: An Approach to Software Fault Tolerance", IEEE TC, C-37, pp. 418-425, 1988.
[11] L. Hatton and A. Roberts, "How Accurate is Scientific Software?", IEEE TSE, 20, pp. 785-797, 1994.
[12] P. Frankl, D. Hamlet, B. Littlewood, and L. Strigini, "Evaluating Testing Methods by Delivered Reliability", IEEE TSE, SE-24, pp. 586-601, 1998.
[13] K. B. Djambazov and P. Popov, "The Effects of Testing on the Reliability of Single Version and I-out-of-2 Software", in Proc. ISSRE'95, Toulouse, 1995, pp. 219-228.
[14] B. Littlewood, P. Popov, and L. Strigini, "Assessment of the Reliability of Fault-Tolerant Software: a Bayesian Approach", in Proc. SAFECOMP'2000, Rotterdam, the Netherlands, 2000.
[15] P. Popov and L. Strigini, "The Reliability of Diverse Systems: a Contribution Using Modelling of the Fault Creation Process", CSWDISPO Project Technical Report, http://www.csr.city.ac.uk/csr_city/projects/diversity/.
[16] J. C. Knight and N. G. Leveson, "An Experimental Evaluation of the Assumption of Independence in Multi-Version Programming", IEEE TSE, SE-12, pp. 96-109, 1986.
[17] S. S. Brilliant, J. C. Knight, and N. G. Leveson, "Analysis of Faults in an N-Version Software Experiment", IEEE TSE, SE-16, pp. 238-247, 1990.