which allows developers to control the browser [9]. For instance,
we can access different websites using the navigate command, or
run JavaScript code on the current website once loaded with the
loadEventFired event and the evaluate command. The executed
code uses xmlHttpRequest to perform the corresponding requests
of the presented technique. Even if it would have been possible to
implement the experiments by using a simple Python script, we
decided to rely on a real-world browser to obtain the most realistic
results possible. Our crawler follows two different phases in order
to obtain the data that we will later used to check if a target server
is actually vulnerable to the attack.
• Never Visited: First, the tool cleans every previous access
information stored in the browser. Then, it starts making
the different requests described in Section 3 (i.e., both with
and without cookies, which in this case are none) from a
third-party website (blank tab). This data will be later used as
a baseline for requests performed when no previous access
was done.
• Previously Visited In this case, the browser first accesses
the website under inspection. No cleaning process is per-
formed, so all cookies automatically created by the website
are saved in the browser and sent to the server in the fol-
lowing requests. After that, it goes to a third-party website
(blank tab) and starts making the different requests described
in Section 3 to that same website under inspection.
Once all the data was retrieved, we performed the statistical
tests described in Section 3 in order to identify whether the time
information in the two groups of requests are statistically different
or not. We also repeated the experiment with different number of
requests in order to check their influence on the final result. To
be practical, we tested with a minimum of 10 and a maximum of
50 comparisons (therefore ranging from 20 to 100 HTTP request
per target website). The higher the number the more stable is the
measurement, but so is the amount of time required to run the test.
Therefore, the choice boils down to a trade-off between precision
and scalability. We believe that if the attacker is only interested in
a handful of websites, then it would be possible to perform even
more than 50 comparisons.
It is also important to note that since we need to repeat the
requests multiple times, it is possible that the first request “pollutes”
the browser by setting cookies that are later sent in the following
tests. In other words, if i) the target website sets cookies on cross-
origin requests, and ii) those cookies are the same (in number and
BakingTimer: Privacy Analysis of Server-Side Request Processing Time
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Table 1: Percentage of websites vulnerable to our attack (top
six private-sensitive on the top half and top six highly pop-
ular in the bottom).
Figure 5: Percentage of vulnerable websites depending in the
number of comparisons performed.
Category
Medical
Religion
Financial
Political
Sexual Identity
Legal
Sports
Search/Portal
Government
Travel
Gaming
Adult/Pornography
% Vulnerable
72.63
71.66
71.63
70.73
70.38
69.39
43.75
42.25
40.63
40.40
39.39
39.06
nature) of those set when the website is loaded in the browser, then
our timing attack would not work.
However, this does not seem to be very common, as more than
half of the 10,486 websites we tested are vulnerable to our technique.
The actual percentage varies between 40%, when the minimum
number of comparisons is performed, and 53.34% if our attack per-
forms 50 comparisons. Figure 5 shows the success rate at different
numbers of comparison for the two groups separately.
4.3 Highly Popular Websites
This group includes 5,243 websites from the Alexa top websites list.
As websites in these categories are visited by a large number of
users, most of their servers and the code they run are likely more
optimized, thus making more difficult to measure the timing side
channel. This is confirmed by the fact that our attack worked on
35.61% of the websites in this category. This result is still quite
severe, as it means than a webpage could still reliably identify if
its users had previously visited more than one third of the most
popular pages on the Internet.
In order to get a deeper analysis of the obtained results, we clus-
tered the websites in different categories and we computed the
percentage of each of them that were vulnerable to our attacks.
To determine the category, we used three services: Cloudacl [11],
Blocksi [6], and Fortiguard [17]. Their category names are similar,
and, after a normalization process, we settled for 78 different cat-
egory names. Table 1 shows that the top 6 categories vulnerable
to the attacks include around 40% of the websites, with a peak of
43.75% in the case of sport-related websites in the Alexa top list.
4.4 Privacy-Sensitive Websites
This group includes 5,243 websites from six different categories
related to private personal information — i.e., medical, legal, finan-
cial, sexual identity, political, and religion. The results from these
websites allows us to understand two different aspects. First, we
can verify the amount of websites directly related to sensitive in-
formation that are vulnerable to our attack. Second, it gives us an
opportunity to test less popular websites (as 85% of the vulnerable
sites in this category are ranked below the Alexa Top500K) to ob-
serve whether smaller server infrastructures can result in a higher
accuracy of our technique.
The results of our experiments show that a stunning 71.07%
of all the analyzed websites in this group are vulnerable to the
BakingTimer attack. If we break down the result by category, we
see that all have similar percentages and there is no clear difference
between them (see Table 1). This result is much higher than the
one obtained in the top Alexa group, but the difference is not due
to the number of cookies. In fact, we compared the mean and
standard deviation of the number of cookies in privacy-sensitive
websites and highly popular websites. Unsurprisingly, the results
show that highly accessed websites have a higher mean number of
cookies (9.03±7.87) compared to the number of cookies in private
personal information websites (5.83±5.49). This means that the
main reason behind the difference is not linked to the number of
cookies, but more likely to the slower servers or the less optimized
code responsible to process the incoming requests.
5 STABILITY TEST
Our attack relies on a time-based side channel used to distinguish
among different execution paths on the server-side application code.
The fact that BakingTimer does not look at absolute times but at
the difference between two consecutive requests minimizes the
effects of network delays on our computation. However, even if the
time difference between the two request is minimal, it is possible
that small non-deterministic variations such as the jitter, bandwidth,
or network congestion and routing can introduce a certain level of
noise in the measurement. To account for these small fluctuations,
BakingTimer needs to repeat each test multiple times. As shown in
Section 4, ten comparisons are sufficient to detect nearly 40% of all
the analyzed websites, which increases to over 50% if we perform
50 comparisons.
In order to obtain a clear view of the specific effect the network
or the server load can have in our measurements, we decided to
perform a dedicated stability test. For this experiment we randomly
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Iskander Sanchez-Rola, Davide Balzarotti, and Igor Santos
This cases, a third-party website can check if the user ever ac-
cessed any of the websites under attack, and can even check if the
user is logged in. This type of information could be extremely useful
for a malicious attacker. For instance, it could be used to perform
targeted phishing attacks against users, to steal the login creden-
tials of those affected websites. Moreover, it will also be beneficial
for other types of attacks like Cross-Site Request Forgery (CSRF),
in which the attacker could use the state of the user in a website
vulnerable to this type of attacks to perform privileged actions
on the users accounts, such as password changes or information
retrieval [10, 34]. Actually, the attacker does not need to control
the third-party website, just have his code executed on them, for
example via an advertisement (as explained in Section 2).
6.1 Highly Accessed Websites
From this category we selected two popular websites, related to
gaming and clothing, that were detected to be vulnerable to our
BakingTimer attack (Section 4), and that have been the target of
different phishing attacks — in one case for the huge number of
users and in the other case for the high economic value of their
customers. More concretely, World of Warcraft (WoW) and Gucci.
In both cases, after the user logs in, a number of cookies are cre-
ated in the user’s browser to store this new status (e.g., wow-session
and gucci-cart respectively). The presence of these cookies make
the server take a different execution path, resulting in a different
computation time. We followed the same analysis principles as the
ones used when we analyzed the websites in Section 4, and detected
both websites fall in our simplified three-paths model presented in
Figure 2. The results show than each of the different states (e.g., not
accessed, accessed, and logged in), does not match any of the other
two states when performing the statistical test of the comparison
phase (see Section 3).
6.2 Private Personal Information Websites
Detecting if a user has previously visited a website linked to specific
information such as religion or sexual identity can leak some private
information to third-parties the user may not even be aware of.
However, if the third party could actually detect that the user is
currently logged in into one of those websites, the link between the
user and the website becomes much stronger.
From this category we picked a religious website (Dynamic
Catholic) and a sexual related chat/forum (LGBTchat.net). Again,
the presence of login cookies (i.e., xf_user and frontend_cid)
made the two applications take different execution paths, whose
computation time was sufficiently different to be reliably finger-
printed by our solution.
6.3 Persistent Login Information
In all previous cases, when the user logs out from the website, the
different cookies related to the login status are deleted by the server.
In this situation, a third-party website would still be able to detect
that the user has visited the website in the past, but it would not be
able to distinguish if she had an account and she ever logged into
the service.
While this may seem obvious, there are also websites for which
it is not true. In fact, some sites do not properly (or at least not
Figure 6: Mean RTT of one website never visited before, dur-
ing a full day (with data every hour).
picked 25 website detected as not vulnerable and 25 websites de-
tected as vulnerable to our attack. Following the same approach
presented in the general experiment, we now repeated our test
every hour for a period of 24h for each website. Moreover, to be
more realistic, we computed the ground truth on one day, and per-
formed the attacks on the following day. This resulted in a total of
48 checks per websites, and 2,400 tests globally.
From the group of the 25 websites not vulnerable to our attack, all
our tests returned the same result (which confirmed the absence of
the side channel). This results proves the stability of the presented
method from a network perspective. More concretely, regarding
fluctuations, Figure 6 shows, for one of the websites analyzed, the
different mean RTTs we registered each hour. Even if there were
considerable fluctuations on the network speed during the day, we
were still able to perform the correct classification.
From the group of the 25 vulnerable websites, we were able to
correctly identify when each website was previously visited – in
all our tests. Instead, in the case in which we did previously visited
the websites, there was one case in which (at 21 and 22pm), we
incorrectly identified a single website as visited. Nevertheless, in
total from the 2,400 checks performed in this experiment, we only
incurred in two false positives and no false negatives, indicating
the high reliability of the presented history sniffing technique.
6 LOGIN DETECTION
In this section we look at performing a more fine-grained classifica-
tion of the user history, not just by telling if a victim has visited a
target website in the past, but also to distinguish a simple visit from
owning an account or being currently logged in into the service.
Since this analysis requires a considerable manual effort to set
up the accounts and operate the websites, we will limit our study
to few examples taken from the two groups in our dataset. This
approach is similar to the one regularly adopted by other related
work on history stealing techniques [28, 44]. It is important to
remark that we did no test websites that required a registration
fee or that had a complex registration procedures (e.g., requiring a
phone number verification or a bank account).
BakingTimer: Privacy Analysis of Server-Side Request Processing Time
ACSAC ’19, December 9–13, 2019, San Juan, PR, USA
Table 2: A comparison of current state-of-the-art timing methods for history sniffing.
Approach
Type
Login Difficult Previous Websites
Status
Analyzed
Access
Clean
Timing Attacks on Web Privacy [16]
Exposing private information... [7]
Cross-origin Pixel Stealing [25]
Identifying Cross-origin... [28]
The Clock is Still Ticking [44]
Our Approach (BakingTimer)
Web & DNS Caching
Server Boolean Values
Cross-origin DOM Structure
HTML5 Application Cache
Cross-origin Resource Size
Cookie-based Request Timing
✗
✓
✓
✓
✓
✓
✗
✓
✓
✗
✓
✓
✓
✗
✗
✗
✗
✓
<10
<10
<10
<10
<10
10,486
completely) delete all cookies they created in relation to the login
process. This could be either because the cookie deleting process
was not performed correctly, or because the website developers
explicitly decided to maintain part of the login information stored
in the cookies even when the user is not logged in. However, the
presence of these cookies can be sufficient to trigger a different code
execution in the application that can be detected by our technique.
This allows third-party to be able to differentiate users that just
accessed the websites from those who own an account, even if they
are not logged in at the time the test is performed.
For instance both Microsoft/MSN and Openload fall into this
category because of the presence of, respectively, a MUID cookie
and a cookie with a MD5 hash as name. In both cases, when the user
logs out of the service, some cookies are deleted but some other