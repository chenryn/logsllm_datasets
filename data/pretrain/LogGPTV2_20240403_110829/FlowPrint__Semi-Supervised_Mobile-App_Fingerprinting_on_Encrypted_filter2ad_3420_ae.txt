1
–
1
1
+
0
2
0
2
–
6
1
t
e
s
a
t
a
d
n
i
s
p
p
a
f
o
n
o
i
t
c
a
r
F
0.5
0.4
0.3
0.2
0.1
0
1 2 3 4 5
0
1
–
6
Number of ﬁngerprints
Fig. 4. Number of ﬁngerprints (cardinality) generated per app.
the cause of homogeneity. In detail, this classiﬁcation maps
domains, and by extension ﬂows, to one of the following
categories based on properties of the app’s description in the
Google Play Store and WHOIS information [52]: (1) ﬁrst-
party, i.e., app-speciﬁc trafﬁc, and third-party trafﬁc. For the
latter we further distinguish between (2) CDN trafﬁc, (3)
advertisement trafﬁc, and (4) social network trafﬁc, based on
publicly available adblocker lists, extended by manual labeling.
In turn, we classify each cluster according to a majority vote
of the ﬂows within that cluster.
Our experiment found a total of 2,028 distinct clusters, of
which 281 clusters are shared between more than one app.
At ﬁrst sight, the homogeneity of trafﬁc seems quite low with
only 13.9% of all clusters being shared. However, these shared
clusters account for 56.9% of all ﬂows in the dataset. By
looking at the categories, we ﬁnd that advertisement networks
account for 60.6% of trafﬁc spread over 184 different shared
destination clusters. As apps often use standard libraries for
displaying advertisement it is unsurprising that many ﬂows
are homogeneous with respect to their network destination.
Social networks account for 30.4% of trafﬁc to shared clusters.
Similar to advertisements,
the support for social services
is often provided by commonly used libraries such as the
Facebook SDK4 or Firebase SDK5. Finally, we ﬁnd that 6.0%
and 2.9% of shared cluster trafﬁc originates from app-speciﬁc
network destinations and CDNs respectively.
Then, we evaluate how our approach reacts under higher
levels of homogeneity. To this end, we removed all ﬂows
that are not shared between apps from the ReCon dataset,
leaving only shared clusters. When running our approach for
recognizing apps, the F1-score drops from 94.6% to 93.0% and
accuracy drops from 94.5% to 93.3%. Despite the small drop
in performance, we are still able to accurately distinguish apps
because the different correlation patterns of these shared clus-
ters can still be uniquely identiﬁed. Therefore, our approach
shows robustness against homogeneous network trafﬁc.
4https://developers.facebook.com/docs/android/
5https://ﬁrebase.google.com/docs/auth/android/start
11
(2) Dynamic trafﬁc. The second challenge is the dynamic
nature of the trafﬁc generated by users as they interact with
apps by using different functionalities at different times. In
contrast, automatically generated datasets often aim to cover as
much functionality as possible in a short amount of time. This
difference between datasets may lead to a different quality of
the ﬁngerprints. To evaluate whether our approach is inﬂuenced
by dynamic trafﬁc, we look at the performance difference
of our approach between the user-generated Cross Platform
dataset and the other datasets. Although these datasets are not
directly comparable due to the different apps they contain, we
do not ﬁnd a signiﬁcant difference in the detection capabilities
of our approach (see Tables IV and V). We attribute this in
part to the amount of network trafﬁc produced by apps without
requiring any user interaction. These include connections to,
for example, advertisement and social networks, as well as
loading content when launching an app. The high performance
for both recognizing apps and detecting unseen apps from user-
generated trafﬁc suggests that dynamic trafﬁc does not impose
any restrictions on our approach.
(3) Evolving trafﬁc. The ﬁnal challenge concerns the evolving
nature of apps. Besides detecting previously unseen apps
(Section V-C), we evaluate our approach when dealing with
new versions of an existing app, and we perform a longitudinal
analysis to assess how FLOWPRINT performs when the values
of our features change over time.
(3a) App updates. We use the ReCon and ReCon extended
datasets as they contain apps of different versions released
over 8 years. On average, the datasets contain 18 different
versions per app, where new versions were released once
every 47.8 days on average. As the trafﬁc of these different
versions was captured over a period of two and a half months,
changes in IP addresses and certiﬁcates might cause a slight
bias in the dataset. In the next subsection, we describe the
results of our longitudinal analysis, which provides a more
in depth analysis regarding this inﬂuence. Nevertheless, we
demonstrate that new app functionality introduced by updates
does not necessarily cause an issue with our ﬁngerprints if
caught early. For this experiment, we train the unseen app
detector with a speciﬁc version of the app as described in
Section V-C. In turn, for each newer version of the app, we
run the unseen app detector to predict whether the ﬁngerprints
of this new version match the training version. We perform
this experiment by increasing the amount of versions between
the training data and the version to predict. This simulates a
security operator lagging behind in updating the models and
thus missing intermediate versions.
Figure 5 shows the results of this experiment. Here, the
x-axis shows the amount of versions between the training app
and predicted app. The y-axis shows the relative number of
ﬁngerprints from the newer app versions that FLOWPRINT
correctly recognizes. As we know the average amount of
time it takes for an app to be updated (47.8 days), we show
the decline in performance not only in terms of versions,
but also over time, by the vertical dashed lines in the plot.
We found that on average FLOWPRINT recognizes 95.6%
of the ﬁngerprints if FLOWPRINT is updated immediately
when a subsequent version is released. When we do not
update immediately, but wait a certain amount of time, the
detection rate slowly drops, which is more evident in the
s
t
n
i
r
p
r
e
g
n
ﬁ
g
n
i
h
c
t
a
m
f
o
n
o
i
t
c
a
r
F
1
0.8
0.6
0.4
0.2
0
3 months
3 months
1 year
1 year
2 years
2 years
6 months
6 months
ReCon
ReCon Extended
Average
4
0
20
Version difference between training and testing
16
8
12
Fig. 5. Recognition performance of FLOWPRINT between versions. The
x-axis shows the number of different versions, including the average time
apps take to receive so many version updates. The y-axis shows the fraction
of matching ﬁngerprints between training and testing data.
ReCon dataset. The difference between the two datasets is due
to (1) more trafﬁc per app in the ReCon Extended dataset,
which makes ﬁngerprinting more accurate, and (2) a larger
set of apps in the ReCon dataset, which makes recognition
more difﬁcult. The average result shows the analysis for the
combined datasets and gives the most realistic performance,
which shows that FLOWPRINT can recognize 90.2% of the
new ﬁngerprints even when operators do not update the models
for one year. Interestingly, 45.5% of the apps in our datasets
released multiple new versions on the same day. However,
FLOWPRINT showed nearly identical performance for these
same-day updates, leading us to believe that quick version
releases do not introduce major app ﬁngerprint changes.
(3b) Longitudinal analysis. Over time, the destination fea-
tures (IP address, port) and the TLS certiﬁcate may change
because of server replication/migration or certiﬁcate renewals.
To measure how FLOWPRINT’s performance changes over
extended periods of time, we evaluate how feature changes
affect our approach. To do this, we train FLOWPRINT using
the original training data and consistently change a percentage
of random IP addresses and TLS certiﬁcates in the testing
data. As TLS certiﬁcates are domain-based and not IP-based,
random selection gives a good approximation of FLOWPRINT’s
performance. We performed a 10-fold cross validation chang-
ing 0 to 100% of such features in steps of 10% points.
Figures 6 and 7 show the performance of FLOWPRINT, in
the case of app recognition and unseen app detection respec-
tively, for an increasing amount of changes in our features.
As with the app updates, we indicate the expected amount of
changed features after given periods of time by the vertical
dashed lines. These expected changes are computed from the
average lifetime of certiﬁcates in our dataset and DNS-name-
to-IP changes according to the Farsight DNSDB database [56].
For the case of app recognition, the number of changed fea-
tures initially has limited effect because changing only one of
the two destination features still allows our clustering approach
to detect
the same network destination. Once we change
12
approximately 80% of the features, the decline becomes a
lot steeper because at this point both features are changed
simultaneously. When changing 100% of IP addresses and
certiﬁcates we are unable to detect anything. Interestingly, the
Andrubis performance of the dataset declines almost linearly.
That is because only 24.7% of Andrubis ﬂows contain a TLS
certiﬁcate. Hence, the certiﬁcate cannot counteract changes
in the IP address,
leading to a steeper decline. This also
underlines the importance of using both the IP and TLS
certiﬁcate as destination features. We recall from Section IV-B
that destination features may be enriched by domains from
DNS trafﬁc. As domains are generally more stable than IP
addresses, they will have a positive effect on the performance
over time. For the case of unseen app detection, an increase
in changed features leads to an increase in the recall. After
all, if trafﬁc of a previously unseen app differs more from the
training data, the app will be ﬂagged as previously unseen.
For the same reason, the detection precision declines as known
apps increasingly differ from their training dataset.
Subsequently, we performed a real-world experiment by
collecting and analyzing data from the current versions of
31 apps in the Cross Platform dataset more than 2 years (26
months) after the original capture. When FLOWPRINT trains on
the original dataset and performs recognition on the recollected
ﬂows it achieved a precision of 36.7%, recall of 33.6% and
F1-score of 35.1%. This translated to being able to recognize
12 out of 31 apps. Interestingly, if we only look at the apps
that we were able to recognize, FLOWPRINT performs with
a precision of 76.1%, a recall of 62.2% and an F1-score of
68.4%. The expected decline in performance after 2+ years
that we found in our two previous analyses is in line with the
results from this real-world experiment.
In conclusion, while, as expected, FLOWPRINT’s perfor-
mance degrades when a large amount of destination-based fea-
tures change (i.e., after one year), our approach can cope with
a signiﬁcant amount of variations without drastic performance
degradations. We believe this gives operators enough time to
update FLOWPRINT’s models to maintain high performance,
making our approach practical.
F. Training Size
e
c
n
a
m
r
o
f
r
e
P
1
0.8
0.6
0.4
0.2
0
0
Cross Platform
Cross Platform
ReCon
ReCon
Andrubis
Andrubis
1 month
1 month
3 months
3 months
F1-score
Precision
Recall
6 months
6 months
1 year
1 year
0.2
0.4
0.6
0.8
1
Fraction of changed features
Fig. 6. App recognition performance vs changes in both IP and certiﬁcate
features. The x-axis denotes the % of changed features. Where the expected
amount of change over time is denoted by the dashed vertical lines.
e
c
n
a
m
r
o
f
r
e
P
1
0.8
0.6
0.4
0.2
0
0
ReCon
ReCon
Cross Platform
Cross Platform
Andrubis
Andrubis
1 year
1 year
6 months
6 months
3 months
3 months
1 month
1 month
F1-score
Precision
Recall
0.2
0.4
0.6
0.8