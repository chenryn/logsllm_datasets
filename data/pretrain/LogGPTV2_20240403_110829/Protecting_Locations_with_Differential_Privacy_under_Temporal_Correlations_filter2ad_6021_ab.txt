Transition Probability. We use a matrix M to denote the
probabilities that a user moves from one location to another.
Let mij be the element in M at ith row and jth column.
Then mij represents the probability that a user moves from
cell i to cell j. Given probability vector pt−1, the probabil-
ity at timestamp t becomes pt = pt−1M. We assume the
transition matrix M is given in our framework.
Emission Probability. If given a true location u∗
t , a mech-
anism releases a perturbed location zt, then the probabili-
ty P r(zt|u∗
t = si) is called “emission probability” in HMM.
This probability is determined by the release mechanism and
should be transparent to adversaries.
Inference and Evolution. At timestamp t, we use p−
t
and p+
to denote the prior and posterior probabilities of
t
a user’s location before and after observing the released zt
respectively. The prior probability can be derived by the
posterior probability at previous timestamp t − 1 and the
Markov transition matrix as p−
t−1M. Given zt, the
posterior probability can be computed using Bayesian infer-
ence as follows. For each cell si:
t = p+
t = si|zt) =
∗
t [i] = P r(u
p+
(cid:80)
j
P r(zt|u∗
P r(zt|u∗
t = si)p−
t [i]
−
t = sj)p
t [j]
(1)
The inference at each timestamp can be eﬃciently com-
puted by forward-backward algorithm in HMM, which will
be incorporated in our framework.
2.3 Differential Privacy and Laplace Mecha-
nism
Definition 2.1
(Differential Privacy). A mechanis-
m A satisﬁes -diﬀerential privacy if for any output z and
any neighboring databases x1 and x2 where x2 can be ob-
tained from x1 by either adding or removing one record2, the
following holds
P r(A(x1) = z)
P r(A(x2) = z)
≤ e
Laplace mechanism [10] is commonly used in the literature
It is built on the (cid:96)1-norm
to achieve diﬀerential privacy.
sensitivity, deﬁned as follows.
2This is the deﬁnition of unbounded diﬀerential privacy [21].
Bounded neighboring databases can be obtained by chang-
ing the value of exactly one record.
12345689101112131415161718192021222312345123457Definition 2.2
((cid:96)1-norm Sensitivity). For any query
f (x): x → Rd, (cid:96)1-norm sensitivity is the maximum (cid:96)1 norm
of f (x1) − f (x2) where x1 and x2 are any two instances in
neighboring databases.
Sf =
x1,x2∈ neighboring databases
max
where || · ||1 denotes (cid:96)1 norm.
||f (x1) − f (x2)||1
A query can be answered by f (x) + Lap(Sf /) to achieve
-diﬀerential privacy, where Lap() ∈ Rd are i.i.d. random
noises drawn from Laplace distribution.
2.4 Utility Metrics
To measure the utility of the perturbed locations, we fol-
low the analysis of metrics in [37] and adopt the expected
distance (called “correctness” in [37]) between the true loca-
tion x∗ and the released location z as our utility metric.
(cid:113)E||z − x∗||2
2
Error =
(2)
In addition, we also study the utility of released locations
in the context of location based queries such as ﬁnding n-
earest k Points of Interest (POI). We will use precision and
recall as our utility metrics in this context which we will
explain later in the experiment section.
2.5 Convex Hull
Our proposed sensitivity hull is based on the well studied
notion of convex hull in computational geometry. We brieﬂy
provide the deﬁnition here.
Definition 2.3
(Convex Hull). Given a set of points
X = {x1, x2,··· , xn}, the convex hull of X is the smallest
convex set that contains X.
Note that a convex hull in two-dimensional space is a poly-
gon (also called “convex polygon” or “bounding polygon”).
Because it is well-studied and implementations are also avail-
able [31], we skip the details and only use Conv(X) to denote
the function of ﬁnding the convex hull of X.
3. PRIVACY DEFINITION
To apply diﬀerential privacy in the new setting of contin-
ual location sharing, we conduct a rigorous privacy analysis
and propose δ-location set based diﬀerential privacy in this
section.
3.1 δ-Location Set
The nature of diﬀerential privacy is to “hide” a true database
in “neighboring databases” when releasing a noisy answer
from the database. In standard diﬀerential privacy, neigh-
boring databases are obtained by either adding or removing
a record (or a user) in a database. However, this is not
applicable in our problem. Thus we propose a new notion,
δ-location set, to hide the true location at every timestamp.
Motivations. We ﬁrst discuss the intuitions that motivates
our deﬁnition.
First, because the Markov model is assumed to be pub-
lic, adversaries can make inference using previously released
locations. Thus we, as data custodians in a privacy mecha-
nism, can also track the temporal inference at every times-
tamp. At any timestamp, say t, a prior probability of the
user’s current location can be derived, denoted by p−
follows.
t as
t = si|zt−1,··· , z1)
∗
−
t [i] = P r(u
p
Similar to hiding a database in its neighboring databases,
we can hide the user’s true location in possible locations
(where p−
t [i] > 0). On the other hand, hiding the true
location in any impossible locations (where p−
t [i] = 0) is
a lost cause because the adversary already knows the user
cannot be there.
Second, a potential shortcoming of Markov model is that
the probability distribution may converge to a stationary
distribution after a long time (e.g. an ergodic Markov chain).
Intuitively, a user’s possible locations can eventually cover
the entire map given enough time. Hiding a location in a
large area may yield a signiﬁcantly perturbed location that
is not useful at all.
According to [16], moving patterns of human have a “high
degree” of temporal and spatial regularity. Hence if people
tend to go to a number of highly frequented locations, our
privacy notion should also emphasize protecting the more
probable locations in Markov model.
δ-Location Set. With above motivations, we deﬁne δ-
location set at any timestamp t, denoted as ∆Xt. Essen-
tially, δ-location set reﬂects a set of probable locations the
user might appear (by leaving out the locations of small
probabilities).
Definition 3.1
(δ-Location Set). Let p−
t be the pri-
or probability of a user’s location at timestamp t. δ-location
set is a set containing minimum number of locations that
have prior probability sum no less than 1 − δ.
t [i] ≥ 1 − δ}
−
p
∆Xt = min{si|(cid:88)
si
For example, if p−
t = [0.3, 0.4, 0.05, 0.2, 0.03, 0.02] correspond-
ing to [s1, s2, s3, s4, s5, s6], then ∆X = {s2, s1, s4} when
δ = 0.1; ∆X = {s2, s1, s4, s3} when δ = 0.05.
Note that if δ = 0 the location set contains all possible lo-
cations. Thus 0-location set preserves the strongest privacy.
Drift. Because δ-location set represents the most proba-
ble locations, a drawback is that the true location may be
ﬁltered out with a small probability (technically, P r(x∗ /∈
∆X) = δ). Same situation may also occur if the Markov
model is not accurate enough in practice due to its limit in
predicability, as we mentioned earlier. Therefore, we denote
this phenomenon as “drift” and handle it with the following
surrogate approach.
Surrogate. When a drift happens, we use a surrogate lo-
cation in ∆X as if it is the “true” location in the release
mechanism.
Definition 3.2
in ∆X with the shortest distance to the true location x∗.
(Surrogate). A surrogate ˜x is the cell
˜x = argmin
s∈∆X
∗
dist(s, x
)
where function dist() denotes the distance between two cells.
Note that the surrogate approach does not leak any informa-
tion of the true location, explained as follows. If x∗ ∈ ∆X,
then x∗ is protected in ∆X; if not, ˜x is protected in ∆X.
Using surrogate does not reveal whether x∗ is in ∆X or not.
Because in any location release mechanisms x∗ is treated as
a black box (oblivious to adversaries), replacing x∗ with ˜x is
also a black box. We formally prove the privacy guarantee
in Theorem 5.1.
In some cases, a surrogate may be far from the true loca-
tion. Then the released location may not be useful. There-
fore, we also measure the distance between released location
and true location in our experiment to reﬂect the long-term
eﬀect of surrogate.
3.2 Differential Privacy on δ-Location Set
We deﬁne diﬀerential privacy based on δ-location set, with
the intuition that the released location zt will not help an
adversary to diﬀerentiate any instances in the δ-location set.
Definition 3.3
(Differential Privacy). At any times-
tamp t, a randomized mechanism A satisﬁes -diﬀerential
privacy on δ-location set ∆Xt if, for any output zt and any
two locations x1 and x2 in ∆Xt, the following holds:
P r(A(x1) = zt)
P r(A(x2) = zt)
≤ e
(3)
Above deﬁnition guarantees the true location is always
protected in δ-location set at every timestamp. In anoth-
er word, the released location zt is diﬀerentially private at
timestamp t for continual location sharing under temporal
correlations. For other application settings, like protecting
the trace or trajectory of a user, we defer the investigation
to future works.
3.3 Adversarial Knowledge
In reality, there may be a variety of adversaries with all
kinds of prior knowledge. Accordingly, we prove that for the
problem of continual location sharing diﬀerential privacy is
equivalent to adversarial privacy, ﬁrst studied in [35].
Definition 3.4
(Adversarial Privacy). A mechanis-
m is -adversarially private if for any location si ∈ S, any
output z and any adversaries knowing the true location is in
∆X, the following holds:
P r(u∗
P r(u∗
≤ e
t = si|zt)
t = si)
t = si) and P r(u∗
where P r(u∗
posterior probabilities of any adversaries.
(4)
t = si|zt) are the prior and
We can show Deﬁnition 3.3 is equivalent to adversarial pri-
vacy for continual location sharing, which can be derived
from the PTLM property [35].
Theorem 3.1. For the problem of continual location shar-
ing, Deﬁnition 3.3 is equivalent to Deﬁnition 3.4.
Deﬁnition 3.4 limits the information gain for adversaries
t /∈ ∆X, our frame-
knowing the condition x∗
work reveals no extra information due to the surrogate ap-
proach. Thus adversarial knowledge can be bounded, dis-
cussed as follows.
t ∈ ∆X. If x∗
Standard Adversary. For adversaries who have exactly
the same Markov model and keep tracking all the released
locations, their knowledge is also the same as our model
(with location inference in Section 5.3). In this case, diﬀer-
ential privacy and adversarial privacy are guaranteed, and
we know exactly the adversarial knowledge, which in fact
can be controlled by adjusting .
Weak Adversary. For adversaries who have little knowl-
edge about the user, the released locations may help them
obtain more information. With enough time to evolve, they
may converge to standard adversaries eventually. But their
adversarial knowledge will not exceed standard adversaries.
Strong Adversary. For adversaries who have additional
information, the released location from diﬀerential privacy
may not be very helpful. Speciﬁcally, a strong adversary
with auxiliary information may have more accurate prior