arbitrary kernel memory with 3.2 KB/s to 503 KB/s.
Attack setting.
In our attack, we consider personal
computers and virtual machines in the cloud.
In the
attack scenario, the attacker has arbitrary unprivileged
code execution on the attacked system, i.e., the attacker
can run any code with the privileges of a normal user.
However, the attacker has no physical access to the ma-
chine. Furthermore, we assume that the system is fully
protected with state-of-the-art software-based defenses
such as ASLR and KASLR as well as CPU features like
SMAP, SMEP, NX, and PXN. Most importantly, we as-
sume a completely bug-free operating system, thus, no
USENIX Association
27th USENIX Security Symposium    979
1 ; rcx = kernel address, rbx = probe array
2 xor rax, rax
3 retry:
4 mov al, byte [rcx]
5 shl rax, 0xc
6 jz retry
7 mov rbx, qword [rbx + rax]
Listing 2: The core of Meltdown. An inaccessible kernel
address is moved to a register, raising an exception.
Subsequent instructions are executed out of order before
the exception is raised, leaking the data from the kernel
address through the indirect memory access.
software vulnerability exists that can be exploited to gain
kernel privileges or leak information. The attacker tar-
gets secret user data, e.g., passwords and private keys, or
any other valuable information.
5.1 Attack Description
Meltdown combines the two building blocks discussed
in Section 4. First, an attacker makes the CPU execute
a transient instruction sequence which uses an inacces-
sible secret value stored somewhere in physical memory
(cf. Section 4.1). The transient instruction sequence acts
as the transmitter of a covert channel (cf. Section 4.2),
ultimately leaking the secret value to the attacker.
Meltdown consists of 3 steps:
Step 1 The content of an attacker-chosen memory loca-
tion, which is inaccessible to the attacker, is loaded
into a register.
Step 2 A transient
instruction accesses a cache line
based on the secret content of the register.
Step 3 The attacker uses Flush+Reload to determine the
accessed cache line and hence the secret stored at the
chosen memory location.
By repeating these steps for different memory locations,
the attacker can dump the kernel memory, including the
entire physical memory.
Listing 2 shows the basic implementation of the tran-
sient instruction sequence and the sending part of the
covert channel, using x86 assembly instructions. Note
that this part of the attack could also be implemented en-
tirely in higher level languages like C. In the following,
we will discuss each step of Meltdown and the corre-
sponding code line in Listing 2.
Step 1: Reading the secret. To load data from the
main memory into a register, the data in the main mem-
ory is referenced using a virtual address. In parallel to
translating a virtual address into a physical address, the
CPU also checks the permission bits of the virtual ad-
dress, i.e., whether this virtual address is user accessible
or only accessible by the kernel. As already discussed in
Section 2.2, this hardware-based isolation through a per-
mission bit is considered secure and recommended by the
hardware vendors. Hence, modern operating systems al-
ways map the entire kernel into the virtual address space
of every user process.
As a consequence, all kernel addresses lead to a valid
physical address when translating them, and the CPU can
access the content of such addresses. The only differ-
ence to accessing a user space address is that the CPU
raises an exception as the current permission level does
not allow to access such an address. Hence, the user
space cannot simply read the contents of such an address.
However, Meltdown exploits the out-of-order execution
of modern CPUs, which still executes instructions in the
small time window between the illegal memory access
and the raising of the exception.
In line 4 of Listing 2, we load the byte value located
at the target kernel address, stored in the RCX register,
into the least signiﬁcant byte of the RAX register repre-
sented by AL. As explained in more detail in Section 2.1,
the MOV instruction is fetched by the core, decoded into
µOPs, allocated, and sent to the reorder buffer. There, ar-
chitectural registers (e.g., RAX and RCX in Listing 2) are
mapped to underlying physical registers enabling out-of-
order execution. Trying to utilize the pipeline as much as
possible, subsequent instructions (lines 5-7) are already
decoded and allocated as µOPs as well. The µOPs are
further sent to the reservation station holding the µOPs
while they wait to be executed by the corresponding ex-
ecution unit. The execution of a µOP can be delayed if
execution units are already used to their corresponding
capacity, or operand values have not been computed yet.
When the kernel address is loaded in line 4, it is likely
that the CPU already issued the subsequent instructions
as part of the out-of-order execution, and that their cor-
responding µOPs wait in the reservation station for the
content of the kernel address to arrive. As soon as the
fetched data is observed on the common data bus, the
µOPs can begin their execution. Furthermore, processor
interconnects [31, 3] and cache coherence protocols [59]
guarantee that the most recent value of a memory address
is read, regardless of the storage location in a multi-core
or multi-CPU system.
When the µOPs ﬁnish their execution, they retire in-
order, and, thus, their results are committed to the archi-
tectural state. During the retirement, any interrupts and
exceptions that occurred during the execution of the in-
struction are handled. Thus, if the MOV instruction that
loads the kernel address is retired, the exception is regis-
tered, and the pipeline is ﬂushed to eliminate all results
of subsequent instructions which were executed out of
980    27th USENIX Security Symposium
USENIX Association
order. However, there is a race condition between raising
this exception and our attack step 2 as described below.
As reported by Gruss et al. [21], prefetching kernel ad-
dresses sometimes succeeds. We found that prefetching
the kernel address can slightly improve the performance
of the attack on some systems.
Step 2: Transmitting the secret. The instruction se-
quence from step 1 which is executed out of order has to
be chosen in a way that it becomes a transient instruction
sequence. If this transient instruction sequence is exe-
cuted before the MOV instruction is retired (i.e., raises the
exception), and the transient instruction sequence per-
formed computations based on the secret, it can be uti-
lized to transmit the secret to the attacker.
As already discussed, we utilize cache attacks that al-
low building fast and low-noise covert channels using the
CPU’s cache. Thus, the transient instruction sequence
has to encode the secret into the microarchitectural cache
state, similar to the toy example in Section 3.
We allocate a probe array in memory and ensure that
no part of this array is cached. To transmit the secret, the
transient instruction sequence contains an indirect mem-
ory access to an address which is computed based on the
secret (inaccessible) value. In line 5 of Listing 2, the se-
cret value from step 1 is multiplied by the page size, i.e.,
4 KB. The multiplication of the secret ensures that ac-
cesses to the array have a large spatial distance to each
other. This prevents the hardware prefetcher from load-
ing adjacent memory locations into the cache as well.
Here, we read a single byte at once. Hence, our probe
array is 256× 4096 bytes, assuming 4 KB pages.
Note that in the out-of-order execution we have a
noise-bias towards register value ‘0’. We discuss the rea-
sons for this in Section 5.2. However, for this reason, we
introduce a retry-logic into the transient instruction se-
quence. In case we read a ‘0’, we try to reread the secret
(step 1). In line 7, the multiplied secret is added to the
base address of the probe array, forming the target ad-
dress of the covert channel. This address is read to cache
the corresponding cache line. The address will be loaded
into the L1 data cache of the requesting core and, due to
the inclusiveness, also the L3 cache where it can be read
from other cores. Consequently, our transient instruction
sequence affects the cache state based on the secret value
that was read in step 1.
Since the transient instruction sequence in step 2 races
against raising the exception, reducing the runtime of
step 2 can signiﬁcantly improve the performance of the
attack. For instance, taking care that the address trans-
lation for the probe array is cached in the translation-
lookaside buffer (TLB) increases the attack performance
on some systems.
Step 3: Receiving the secret.
In step 3, the attacker
recovers the secret value (step 1) by leveraging a mi-
croarchitectural side-channel attack (i.e., the receiving
end of a microarchitectural covert channel) that transfers
the cache state (step 2) back into an architectural state.
As discussed in Section 4.2, our implementation of Melt-
down relies on Flush+Reload for this purpose.
When the transient instruction sequence of step 2 is
executed, exactly one cache line of the probe array is
cached. The position of the cached cache line within the
probe array depends only on the secret which is read in
step 1. Thus, the attacker iterates over all 256 pages of
the probe array and measures the access time for every
ﬁrst cache line (i.e., offset) on the page. The number of
the page containing the cached cache line corresponds
directly to the secret value.
Dumping the entire physical memory. Repeating all
3 steps of Meltdown, an attacker can dump the entire
memory by iterating over all addresses. However, as the
memory access to the kernel address raises an exception
that terminates the program, we use one of the methods
from Section 4.1 to handle or suppress the exception.
As all major operating systems also typically map the
entire physical memory into the kernel address space (cf.
Section 2.2) in every user process, Meltdown can also
read the entire physical memory of the target machine.
5.2 Optimizations and Limitations
Inherent bias towards 0. While CPUs generally stall
if a value is not available during an out-of-order load op-
eration [28], CPUs might continue with the out-of-order
execution by assuming a value for the load [12]. We
observed that the illegal memory load in our Meltdown
implementation (line 4 in Listing 2) often returns a ‘0’,
which can be clearly observed when implemented using
an add instruction instead of the mov. The reason for this
bias to ‘0’ may either be that the memory load is masked
out by a failed permission check, or a speculated value
because the data of the stalled load is not available yet.
This inherent bias results from the race condition in
the out-of-order execution, which may be won (i.e., reads
the correct value), but is often lost (i.e., reads a value of
‘0’). This bias varies between different machines as well
as hardware and software conﬁgurations and the speciﬁc
implementation of Meltdown.
In an unoptimized ver-
sion, the probability that a value of ’0’ is erroneously
returned is high. Consequently, our Meltdown imple-
mentation performs a certain number of retries when the
code in Listing 2 results in reading a value of ‘0’ from the
Flush+Reload attack. The maximum number of retries is
an optimization parameter inﬂuencing the attack perfor-
mance and the error rate. On the Intel Core i5-6200U
USENIX Association
27th USENIX Security Symposium    981
using exeception handling, we read a ’0’ on average in
5.25 % (σ = 4.15) with our unoptimized version. With
a simple retry loop, we reduced the probability to 0.67 %
(σ = 1.47). On the Core i7-8700K, we read on average
a ’0’ in 1.78 % (σ = 3.07). Using Intel TSX, the proba-
bility is further reduced to 0.008 %.
the number of bits read and transmitted at once is a trade-
off between some implicit error-reduction and the overall
transmission rate of the covert channel.
However, since the error rates are quite small in either
case, our evaluation (cf. Section 6) is based on the single-
bit transmission mechanics.
Optimizing the case of 0. Due to the inherent bias of
Meltdown, a cache hit on cache line ‘0’ in the Flush+
Reload measurement, does not provide the attacker with
any information. Hence, measuring cache line ‘0’ can
be omitted and in case there is no cache hit on any other
cache line, the value can be assumed to be ‘0’. To min-
imize the number of cases where no cache hit on a non-
zero line occurs, we retry reading the address in the tran-
sient instruction sequence until it encounters a value dif-
ferent from ‘0’ (line 6). This loop is terminated either
by reading a non-zero value or by the raised exception of
the invalid memory access. In either case, the time un-
til exception handling or exception suppression returns
the control ﬂow is independent of the loop after the in-
valid memory access, i.e., the loop does not slow down
the attack measurably. Hence, these optimizations may
increase the attack performance.
Single-bit transmission.
In the attack description in
Section 5.1, the attacker transmitted 8 bits through the
covert channel at once and performed 28 = 256 Flush+
Reload measurements to recover the secret. However,
there is a trade-off between running more transient in-
struction sequences and performing more Flush+Reload
measurements. The attacker could transmit an arbitrary
number of bits in a single transmission through the covert
channel, by reading more bits using a MOV instruction for
a larger data value. Furthermore, the attacker could mask
bits using additional instructions in the transient instruc-
tion sequence. We found the number of additional in-
structions in the transient instruction sequence to have a
negligible inﬂuence on the performance of the attack.
The performance bottleneck in the generic attack de-
scribed above is indeed, the time spent on Flush+Reload
measurements. In fact, with this implementation, almost
the entire time is spent on Flush+Reload measurements.
By transmitting only a single bit, we can omit all but
one Flush+Reload measurement, i.e., the measurement
on cache line 1.
If the transmitted bit was a ‘1’, then
we observe a cache hit on cache line 1. Otherwise, we
observe no cache hit on cache line 1.
Transmitting only a single bit at once also has draw-
backs. As described above, our side channel has a bias
towards a secret value of ‘0’.
If we read and transmit
multiple bits at once, the likelihood that all bits are ‘0’
may be quite small for actual user data. The likelihood
that a single bit is ‘0’ is typically close to 50 %. Hence,
Exception Suppression using Intel TSX.
In Sec-
tion 4.1, we discussed the option to prevent that an ex-
ception is raised due an invalid memory access. Using
Intel TSX, a hardware transactional memory implemen-
tation, we can completely suppress the exception [37].
With Intel TSX, multiple instructions can be grouped
to a transaction, which appears to be an atomic opera-
tion, i.e., either all or no instruction is executed. If one
instruction within the transaction fails, already executed
instructions are reverted, but no exception is raised.
If we wrap the code from Listing 2 with such a TSX
instruction, any exception is suppressed. However, the
microarchitectural effects are still visible, i.e., the cache
state is persistently manipulated from within the hard-
ware transaction [19]. This results in higher channel ca-
pacity, as suppressing the exception is signiﬁcantly faster
than trapping into the kernel for handling the exception,
and continuing afterward.
Dealing with KASLR.
In 2013, kernel address space
layout randomization (KASLR) was introduced to the
Linux kernel (starting from version 3.14 [11]) allowing
to randomize the location of kernel code at boot time.
However, only as recently as May 2017, KASLR was
enabled by default in version 4.12 [54]. With KASLR
also the direct-physical map is randomized and not ﬁxed
at a certain address such that the attacker is required to
obtain the randomized offset before mounting the Melt-
down attack. However, the randomization is limited to
40 bit.
Thus, if we assume a setup of the target machine with
8 GB of RAM, it is sufﬁcient to test the address space
for addresses in 8 GB steps. This allows covering the
search space of 40 bit with only 128 tests in the worst
case. If the attacker can successfully obtain a value from
a tested address, the attacker can proceed to dump the
entire memory from that location. This allows mount-
ing Meltdown on a system despite being protected by
KASLR within seconds.