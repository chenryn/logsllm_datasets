each leaf online without having to retrain the trees, which is both com-
putationally more expensive, and we also observe that is not needed.
584
3691215Number of codeblocks100200300400500Runtimes (us)Single CPU coreFour CPU coresSix CPU cores03691215Number of LDPC codeblocks0123Memory stalls per cycle1e−1Single CPU coreFour CPU coresSix CPU coresSIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Xenofon Foukas and Bozidar Radunovic
(a) Violin plots of runtime samples
used for each leaf node of the quan-
tile decision tree.
(b) CDFs of runtimes of most dis-
similar leaf nodes between isolated
case and TPCC/redis
Figure 7: Mapping of runtime samples to decision tree leaves
and effect of interference to their distribution (LDPC decoding).
To verify this intuition we first plot violin plots mapping the col-
lected FlexRAN runtime samples of 120K runs to leaf nodes of a
quantile decision tree used for the LDPC decoding task in the offline
case (in isolation). For each run, we vary the size of the inputs (num-
ber of UEs, packet sizes etc). As shown in the top part of Fig. 7a,
the variance of each violin plot is small compared to the overall
variance of the input samples. We next plot violin plots (bottom part
of Fig. 7a) representing the mapping of collected FlexRAN runtimes
for the same workload, but in the presence of a collocated TPCC
workload [80], and while still using the offline-trained decision tree.
The distributions are visually similar to the isolated case, showing
that the grouping of the online runtime samples remains similar when
using the offline trained tree. We further verify this observation by
zooming in and comparing the most distorted leaf node CDFs in
the presence of collocated workloads compared to the isolated case
(identified using the Wasserstein distance [105]). As shown in Fig. 7b
for TPCC and redis, the runtime samples in the presence of interfer-
ence result in heavier-tailed distributions, but the runtimes within the
leaf node are still located in the same region. We verify the same for
all other tasks and workloads we tried (e.g. redis, nginx, MLPerf).
Offline construction of quantile decision trees: The decision trees
are trained offline, using a dataset with samples collected by profiling
the vRAN in the absence of collocated workloads.Samples are col-
lected at a TTI granularity and each sample contains the state of the
vRAN and the runtime of the vRAN tasks. The state of the vRAN
contains a set of features 𝑋, including data like the number of active
UEs, their transport block sizes, the transmission configurations etc.
To create a dataset with maximum coverage of the input space, the
profiling is performed using a set of transmission parameters that vary
for each TTI (e.g. 0 to 16 transmitting UEs, varying transport block
sizes, modulation and coding schemes etc). Using this dataset and for
each task 𝑡, we perform feature selection to identify a subset of fea-
tures 𝑋𝑡 ⊆ 𝑋 with the most significant impact to the task runtime. For
the feature selection, we combine hand-picked features based on do-
main expertise and automated feature selection methods (correlation
with the task runtime using the distance correlation metric [98, 99],
backwards elimination). All this is summarized in Algorithm 1.
Online training and prediction: We construct the online prediction
by simply replacing the offline samples in each leaf with online ones.
The online prediction runs every TTI (every 0.5-1ms depending on
Algorithm 1: Construction of quantile decision tree for a
signal processing task 𝑡
Input
:vRAN state 𝑋 of current TTI, set of handpicked features 𝑋 ℎ
task 𝑡, runtime 𝑅𝑡 of task in current TTI
:Feature vector 𝑋𝑡 of task 𝑡, quantile decision tree 𝑇𝑡
𝑡 for
Output
Tree Training (𝑋 , 𝑅𝑡 , 𝑋 ℎ
𝑡 )
selection
correlation metric [98, 99]
/* Pick 𝑁 most highly correlated features using distance
𝑋𝑑 ← 𝑑𝑐𝑜𝑟(𝑋 , 𝑅𝑡 , 𝑁 );
/* Pick 𝑀 features using backwards elimination feature
𝑋𝑑 ← 𝑏𝑎𝑐𝑘_𝑒𝑙𝑖𝑚(𝑋𝑑 , 𝑀);
/* Combine with hand-picked features
𝑋𝑡 ← 𝑋 ℎ
𝑡 ∪ 𝑋𝑑 ;
/* Train quantile decision tree
𝑇𝑡 ← 𝑡𝑟𝑎𝑖𝑛(𝑋𝑡 );
*/
*/
*/
*/
Algorithm 2: Quantile decision tree prediction model of a
signal processing task 𝑡
Input
:Quantile decision tree 𝑇𝑡 with ringbuffer 𝐵𝑖 for leaf node 𝑖,
features 𝑋𝑡 and runtime 𝑅𝑡 for task in current TTI
:WCET prediction 𝑊 𝐶𝐸𝑇𝑝
Output
Training Step (𝑇𝑡 , 𝑋𝑡 , 𝑅)
/* Traverse 𝑇𝑡 to find the appropriate leaf node
𝑖 ← 𝑇𝑡 (𝑋𝑡 );
Store 𝑅 in 𝐵𝑖;
Prediction Step (𝐵, 𝑋𝑡 )
/* Traverse 𝑇𝑡 to find the appropriate leaf node
𝑖 ← 𝑇𝑡 (𝑋𝑡 );
𝑊 𝐶𝐸𝑇𝑝 ← 𝑚𝑎𝑥(𝐵𝑖)
*/
*/
the cell configuration) and has to be fast. For each leaf node 𝑖 in the de-
cision tree, we maintain a ring buffer 𝐵𝑖 of the most recently observed
execution times, which is updated at runtime in every TTI. Consider
a task 𝑡 assigned during a given TTI slot with parameters 𝑋𝑡 , and
whose observed runtime is 𝑅𝑡 . We first traverse the decision tree 𝑇𝑡
for that task and find the leaf node 𝑖 that maps the task parameters 𝑋𝑡 .
We add the observation 𝑅𝑡 to the buffer 𝐵𝑖. To predict the runtime of a
task with parameters 𝑋𝑡 in a given execution slot, we first find the de-
cision tree node 𝑖 that corresponds to the parameters 𝑋𝑡 . We then use
the maximum of all the samples found in the ring buffer 𝐵𝑖 as an esti-
mated WCET for the task. All this is formally shown in Algorithm 2.
Comparison with other approaches: Due to its parameterized pre-
diction, Concordia offers more accurate WCET prediction than state-
of-the-art real-time systems predictors [18, 111] that do not consider
input parameters (see Section 6.3 for comparison). We also experi-
mented with such methods (e.g. [23]) to replace our online predictor
on each leaf node, but they provided similar accuracy while being
more computationally expensive. We further tried different param-
eterized prediction models (linear and non-linear regression) instead
of the decision tree, but they either provided lower prediction accu-
racy or reclaimed less CPU cores (results presented in Section 6).
IMPLEMENTATION
5
Here we describe the implementation of Concordia. We build our pro-
totype on standard Linux, on top of Intel’s FlexRAN v20.02 [48, 62],
585
01020300200vRAN cell isolated01020300200vRAN cell w/ tpcc0.00.20.40.60.81.0Leaf node ids0.00.20.40.60.81.0Runtime (us)15016017018019001Leaf node 23vRAN cell isolatedvRAN cell w/ tpcc12013014015016017001Leaf node 17vRAN cell isolatedvRAN cell w/ redis0.00.20.40.60.81.0Runtime (us)0.00.20.40.60.81.0CDFConcordia: Teaching 5G vRAN to Share Compute
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
the most mature 4G and 5G vRAN implementation on the Intel archi-
tecture. We note that the OpenAirInterface [43, 52] and Agora [28]
projects could also be used.
Following the FlexRAN recommendations [49], we use a number
of CPU cores in a vRAN pool to execute signal processing tasks (the
exact number of cores depends on the workload and is specified in
Section 6). As already mentioned, the Concordia design assumes that
the vRAN is the high priority workload and other workloads are best-
effort. As such, we set the threads of the vRAN pool to a real-time
scheduling policy (SCHED_FIFO, priority 94), meaning that they
can only be preempted by a few critical kernel threads (e.g. watchdog
thread). The only way that other workloads can run on the same cores
is if the Concordia scheduler decides that the vRAN pool threads must
yield. Moreover, we set the kernel parameter sched_rt_runtime_us to
-1 to prevent non real-time tasks from running on the same cores as
the vRAN pool while the vRAN worker threads have not yielded. We
use the Linux isolcpus boot kernel parameter to dedicate 1 CPU core
for the thread that maintains the time of the vRAN and periodically
runs the Concordia scheduler and 3 CPU cores for the tasks of the
MAC layer. Finally, we use a single core for OS management tasks.
We offload the RCU callbacks of the system to the OS management
core and migrate all the interrupts and kernel threads out of the used
by the MAC layer and the Concordia scheduler (also banning those
cores from irqbalance). However, unlike the isolated FlexRAN case,
we allow interrupts and kernel threads to be served by the vRAN
pool cores plus the OS management CPU core.
WCET Predictor implementation: The WCET predictor compo-
nent of Concordia is auto-generated using a collection of Python
scripts. During the offline tree construction phase we obtain 500K
training samples from synthetic workloads in the way described in
Section 4.2 and we automatically extract all relevant system parame-
ters and the task runtimes for each TTI slot. We then run Algorithm 1
in Python, built on the pandas framework [76] with R bindings1
for the use of the distance correlation algorithm [98, 99] and the
scikit-learn library for the backwards elimination feature selection
and the training of the decision trees [79]. Next, another Python
script takes the decision tree from the previous phase and generates
an optimized C code (about 6K lines of code) for traversing the tree
and storing/fetching runtime samples (Algorithm 2), with the ring
buffers of the leaf nodes having 5K entries. The predictor runs as a
task on a CPU of the RAN pool in the beginning of each TTI. The
predictor runtime is evaluated in Section 6.5.
Scheduler implementation: The Concordia scheduler is imple-
mented in C (about 2K lines of code) and is integrated in the vRAN
pool framework of FlexRAN as part of the timer thread, running on a
dedicated core that is never preempted. The scheduler uses a bitmap
with the ids of CPU cores in order to signal BBU worker threads
that they must yield their cores to the OS. Semaphores are used to
wake up the worker threads when scheduled. The scheduler changes
the order of cores that are used for vRAN pools every 2ms to avoid
constantly using the same cores. This allows other workloads that
cannot be migrated to get some CPU time (e.g. some kernel threads
and interrupts). The scheduler runtime is evaluated in Section 6.5.
Bandwidth
100MHz
20MHz
# cells
2
7
Avg DL cell
throughput
750Mbps
270Mbps
Avg UL cell
throughput
80Mbps
120Mbps
TTI processing
deadline
1.5ms
2ms
Table 1: Cell configuration for evaluation of Concordia
6 EXPERIMENTAL EVALUATION
Here we evaluate the performance of Concordia. We start by de-
scribing the setup for the evaluations. We focus our evaluation on
5G vRAN cell deployments, i.e., we use the 5G signal processing
chains of FlexRAN. As real-world 5G deployments are still at an
early stage, we do not have access to realistic traffic patterns from
real 5G cells and therefore we rely on emulated traces. The traces
are based on actual 5G radio samples, encoding a varying number
of 5G users, modulation and coding schemes, transport block sizes,
data rates, MIMO antenna layers etc. We implement a traffic gener-
ator that combines these samples to create uplink traffic benchmark
traces that are unique to each cell. The traces are based on the traffic
fluctuation patterns of the LTE traces presented in Section 2.2, but
with a volume of traffic that is scaled up to match that expected from
5G deployments (i.e., > ×10 increase in the aggregate traffic of each
cell). We create downlink traffic benchmarks in a similar way. While
we acknowledge that the traffic patterns from real 5G cells might
not fully match those of the traces used in this work, we believe that
our evaluation can still provide deep insights into the effectiveness
of Concordia, due to the randomness in the fluctuation of traffic for
each cell and the uniqueness of each cell’s trace.
For the experiments presented throughout this section and unless
stated otherwise, we deploy our vRAN on the first NUMA node of
a 48-core server (Intel Xeon Platinum 8168 @ 2.7GHz) running
Ubuntu Linux 18.04, with hyper-threading disabled and configured
for high performance as advised for FlexRAN [49]. This includes the
use of a low latency kernel, disabled power states/frequency scaling
and use of huge pages. Some experiments require servers with fewer
cores. In order to compare on the same CPU architecture, we deac-
tivate unneeded cores through the sysfs Linux virtual filesystem.
We consider the 2 cell configurations of Table 1 and 5 types of
workloads collocated on the vRAN pool cores, that stress various
parts of the server (CPU, memory, network, disk):
Redis We deploy 8 containers, each with a single Redis server. We
saturate the servers using 8 remote instances of the Redis bench-
mark tool [87], connected over a 40G link and performing GET/SET
requests over a set of 100K keys.
Nginx We deploy 5 Nginx containers and an external client, con-
nected over a 40G link that fully saturates Nginx, fetching 612B-large
HTTP files.
TPCC We deploy 1 container of a MySQL server and run a TPCC
benchmark [80] using a remote client (1000 warehouses and 32
simultaneous connections).
MLPerf We deploy 1 container running MLPerf [86] to train ResNet50-
v1.5 [45, 46] for image classification using the ImageNet 1K dataset2.
Mix We deploy a mix of the above workloads at the same time. The
workloads are turned on and off at random time intervals ranging
from 10 to 70 seconds.
1https://rdrr.io/cran/Rfast/man/dcor.html
2http://www.image-net.org/
586
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Xenofon Foukas and Bozidar Radunovic
(a) CPU cores reclaimed by Concor-
dia vs ideal case
(b) Redis benchmark performance
(8 Docker containers)
(c) Nginx benchmark performance
(5 Docker containers)
(d) TPCC benchmark performance
(1 Docker container)