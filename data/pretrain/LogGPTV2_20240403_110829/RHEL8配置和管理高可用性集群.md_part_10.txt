    ::: {.warning style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 警告 {.title}
    不要配置多个在主动/被动 HA 配置中使用相同 LVM 卷组的 LVM
    `激活资源`{.literal}，因为这可能导致数据崩溃。另外，不要在主动/被动
    HA 配置中将 `LVM 激活的资源配置为`{.literal} 克隆资源。
    :::
    ``` literallayout
    [root@z1 ~]# pcs resource create my_lvm ocf:heartbeat:LVM-activate vgname=my_vg vg_access_mode=system_id --group nfsgroup
    ```
2.  检查集群的状态，以验证资源是否在运行。
    ``` literallayout
    root@z1 ~]#  pcs status
    Cluster name: my_cluster
    Last updated: Thu Jan  8 11:13:17 2015
    Last change: Thu Jan  8 11:13:08 2015
    Stack: corosync
    Current DC: z2.example.com (2) - partition with quorum
    Version: 1.1.12-a14efad
    2 Nodes configured
    3 Resources configured
    Online: [ z1.example.com z2.example.com ]
    Full list of resources:
     myapc  (stonith:fence_apc_snmp):       Started z1.example.com
     Resource Group: nfsgroup
         my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
    PCSD Status:
      z1.example.com: Online
      z2.example.com: Online
    Daemon Status:
      corosync: active/enabled
      pacemaker: active/enabled
      pcsd: active/enabled
    ```
3.  为集群配置 `Filesystem`{.literal} 资源。
    以下命令将名为 `nfsshare`{.literal} 的 ext4 `Filesystem`{.literal}
    资源配置为 `nfsgroup 资源组`{.literal}
    的一部分。这个文件系统使用您在
    file:https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/ProductNumberLink}/html/configuring_and_availability_high_availability_clusters/assembly_configuring-active-passive-nfs-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-and-managing-high-availability-clusters#proc_configuring
    中创建的 LVM 卷组和 ext4 文件系统
    -LVM-volume-with-ext4-file-system-configuring-ha-nfs\[配置使用 ext4
    文件系统}的 LVM 卷，并将挂载到您在 [配置 NFS
    共享](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-nfs-share-configuring-ha-nfs){.link}
    时创建的 `/nfsshare`{.literal} 目录。
    ``` literallayout
    [root@z1 ~]# pcs resource create nfsshare Filesystem \
    device=/dev/my_vg/my_lv directory=/nfsshare \
    fstype=ext4 --group nfsgroup
    ```
    您可以使用 options `=options`{.literal} 参数指定挂载选项作为
    `Filesystem`{.literal} 资源的资源配置的一部分。运行
    `pcs resource describe Filesystem`{.literal}
    命令以查看完整配置选项。
4.  验证 `my_lvm`{.literal} 和 `nfsshare`{.literal} 资源正在运行。
    ``` literallayout
    [root@z1 ~]# pcs status
    ...
    Full list of resources:
     myapc  (stonith:fence_apc_snmp):       Started z1.example.com
     Resource Group: nfsgroup
         my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
         nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
    ...
    ```
5.  创建名为 `nfs -daemon 的 nfs`{.literal} server``{=html}
    资源，作为 `nfsgroup 资源组`{.literal} 的一部分。
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    `nfsserver`{.literal} 资源允许您指定 `nfs_shared_infodir`{.literal}
    参数，这是 NFS 服务器用于存储与 NFS 相关的有状态信息的目录。
    建议将此属性设置为您在这个导出集合中创建的
    `Filesystem 资源`{.literal} 的子目录。这样可确保 NFS
    服务器将其有状态的信息存储在需要重新定位资源组时可供另一个节点使用的设备中。在这个示例中;
    ::: itemizedlist
    -   `/nfsshare`{.literal} 是由 `Filesystem`{.literal}
        资源管理的共享存储目录
    -   `/nfsshare/exports/export1`{.literal} 和
        `/nfsshare/exports/export2`{.literal} 是导出目录
    -   `/nfsshare/nfsinfo`{.literal} 是 `nfsserver`{.literal}
        资源的共享信息目录
    :::
    :::
    ``` literallayout
    [root@z1 ~]# pcs resource create nfs-daemon nfsserver \
    nfs_shared_infodir=/nfsshare/nfsinfo nfs_no_notify=true \
    --group nfsgroup
    [root@z1 ~]# pcs status
    ...
    ```
6.  添加 `exportfs`{.literal} 资源以导出 `/nfsshare/exports`{.literal}
    目录。这些资源是 `nfsgroup`{.literal} 资源组的一部分。这为 NFSv4
    客户端构建了一个虚拟目录。NFSv3 客户端也可以访问这些导出。
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    只有在您想要为 NFSv4 客户端创建虚拟目录时才需要 `fsid=0`{.literal}
    选项。如需更多信息，请参阅[如何在 NFS 服务器的 /etc/exports
    文件中配置 fsid
    选项？](https://access.redhat.com/solutions/548083/){.link}
    :::
    ``` literallayout
    [root@z1 ~]# pcs resource create nfs-root exportfs \
    clientspec=192.168.122.0/255.255.255.0 \
    options=rw,sync,no_root_squash \
    directory=/nfsshare/exports \
    fsid=0 --group nfsgroup
    [root@z1 ~]# # pcs resource create nfs-export1 exportfs \
    clientspec=192.168.122.0/255.255.255.0 \
    options=rw,sync,no_root_squash directory=/nfsshare/exports/export1 \
    fsid=1 --group nfsgroup
    [root@z1 ~]# # pcs resource create nfs-export2 exportfs \
    clientspec=192.168.122.0/255.255.255.0 \
    options=rw,sync,no_root_squash directory=/nfsshare/exports/export2 \
    fsid=2 --group nfsgroup
    ```
7.  添加 NFS 客户端用来访问 NFS 共享的浮动 IP 地址资源。此资源是
    `nfsgroup`{.literal} 资源组的一部分。在本示例部署中，我们使用
    192.168.122.200 作为浮动 IP 地址。
    ``` literallayout
    [root@z1 ~]# pcs resource create nfs_ip IPaddr2 \
    ip=192.168.122.200 cidr_netmask=24 --group nfsgroup
    ```
8.  添加 `nfsnotify`{.literal} 资源，以便在整个 NFS 部署初始化后发送
    NFSv3 重启通知。此资源是 `nfsgroup`{.literal} 资源组的一部分。
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    为了正确处理 NFS 通知，浮动 IP 地址必须具有与其关联的主机名，在 NFS
    服务器和 NFS 客户端中都一致。
    :::
    ``` literallayout
    [root@z1 ~]# pcs resource create nfs-notify nfsnotify \
    source_host=192.168.122.200 --group nfsgroup
    ```
9.  在创建资源和资源限制后，您可以检查集群的状态。请注意，所有资源都在同一个节点上运行。
    ``` literallayout
    [root@z1 ~]# pcs status
    ...
    Full list of resources:
     myapc  (stonith:fence_apc_snmp):       Started z1.example.com
     Resource Group: nfsgroup
         my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
         nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
         nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com
         nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com
         nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com
         nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com
         nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com
         nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com
    ...
    ```
:::
:::
::: section
::: titlepage
# []{#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters.html#proc_testing-nfs-resource-configuration-configuring-ha-nfs}测试 NFS 资源配置 {.title}
:::
您可以按照以下步骤在高可用性集群中验证 NFS 资源配置。您应该可以使用
NFSv3 或 NFSv4 挂载导出的文件系统。
::: section
::: titlepage
## []{#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters.html#_testing_the_nfs_export}测试 NFS 导出 {.title}
:::
::: orderedlist
1.  在与部署位于同一个网络中的、位于集群以外的一个节点中，通过挂载 NFS
    共享来确定 NFS 共享。在本例中，我们使用 192.168.122.0/24 网络。
    ``` literallayout
    # showmount -e 192.168.122.200
    Export list for 192.168.122.200:
    /nfsshare/exports/export1 192.168.122.0/255.255.255.0
    /nfsshare/exports         192.168.122.0/255.255.255.0
    /nfsshare/exports/export2 192.168.122.0/255.255.255.0
    ```
2.  要验证您可以用 NFSv4 挂载 NFS 共享，将 NFS
    共享挂载到客户端节点的目录中。挂载后，请确定导出目录的内容是可见的。测试后卸载共享。
    ``` literallayout
    # mkdir nfsshare
    # mount -o "vers=4" 192.168.122.200:export1 nfsshare
    # ls nfsshare
    clientdatafile1
    # umount nfsshare
    ```
3.  确定您可以用 NFSv3 挂载 NFS 共享。挂载后，验证测试文件
    `clientdatafile1`{.literal} 是否可见。和 NFSv4 不同，因为 NFSv3
    不使用虚拟文件系统，所以您必须挂载一个特定的导出。测试后卸载共享。
    ``` literallayout
    # mkdir nfsshare
    # mount -o "vers=3" 192.168.122.200:/nfsshare/exports/export2 nfsshare
    # ls nfsshare
    clientdatafile2
    # umount nfsshare
    ```
:::
:::
::: section
::: titlepage
## []{#assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters.html#_testing_for_failover}测试故障转移 {.title}
:::
::: orderedlist
1.  在集群外的节点上挂载 NFS 共享，并验证访问您在 [配置 NFS
    共享](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/configuring_and_managing_high_availability_clusters/assembly_configuring-active-passive-nfs-server-in-a-cluster-configuring-and-managing-high-availability-clusters#proc_configuring-nfs-share-configuring-ha-nfs){.link}
    中创建的 `clientdatafile1`{.literal} 文件。
    ``` literallayout
    # mkdir nfsshare
    # mount -o "vers=4" 192.168.122.200:export1 nfsshare
    # ls nfsshare
    clientdatafile1
    ```
2.  在集群的一个节点中，决定集群中的哪个节点正在运行
    `nfsgroup`{.literal}。在本例中，nfs `group`{.literal} 在
    `z1.example.com`{.literal} 上运行。
    ``` literallayout
    [root@z1 ~]# pcs status
    ...
    Full list of resources:
     myapc  (stonith:fence_apc_snmp):       Started z1.example.com
     Resource Group: nfsgroup
         my_lvm     (ocf::heartbeat:LVM):   Started z1.example.com
         nfsshare   (ocf::heartbeat:Filesystem):    Started z1.example.com
         nfs-daemon (ocf::heartbeat:nfsserver):     Started z1.example.com
         nfs-root   (ocf::heartbeat:exportfs):      Started z1.example.com
         nfs-export1        (ocf::heartbeat:exportfs):      Started z1.example.com
         nfs-export2        (ocf::heartbeat:exportfs):      Started z1.example.com
         nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z1.example.com
         nfs-notify (ocf::heartbeat:nfsnotify):     Started z1.example.com
    ...
    ```
3.  在集群内的一个节点中，将运行 `nfsgroup`{.literal}
    的节点设置为待机模式。
    ``` literallayout
    [root@z1 ~]# pcs node standby z1.example.com
    ```
4.  验证 `nfsgroup`{.literal} 是否在另一群集节点上成功启动。
    ``` literallayout
    [root@z1 ~]# pcs status
    ...
    Full list of resources:
     Resource Group: nfsgroup
         my_lvm     (ocf::heartbeat:LVM):   Started z2.example.com
         nfsshare   (ocf::heartbeat:Filesystem):    Started z2.example.com
         nfs-daemon (ocf::heartbeat:nfsserver):     Started z2.example.com
         nfs-root   (ocf::heartbeat:exportfs):      Started z2.example.com
         nfs-export1        (ocf::heartbeat:exportfs):      Started z2.example.com
         nfs-export2        (ocf::heartbeat:exportfs):      Started z2.example.com
         nfs_ip     (ocf::heartbeat:IPaddr2):       Started  z2.example.com
         nfs-notify (ocf::heartbeat:nfsnotify):     Started z2.example.com
    ...
    ```
5.  在您挂载了 NFS 共享的集群之外的节点中，确认这个外部节点仍然可以访问
    NFS 挂载中的测试文件。
    ``` literallayout
    # ls nfsshare
    clientdatafile1
    ```
    在故障转移的过程中，服务可能会在短暂时间内不可用，但可以在没有用户干预的情况下恢复。默认情况下，使用
    NFSv4 的客户端可能最多需要 90 秒恢复该挂载。这个 90
    秒代表服务器启动时观察到的 NFSv4 文件租期的宽限期。NFSv3
    客户端应该在几秒钟内就可以恢复对该挂载的访问。