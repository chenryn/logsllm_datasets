title:BinSequence: Fast, Accurate and Scalable Binary Code Reuse Detection
author:He Huang and
Amr M. Youssef and
Mourad Debbabi
BinSequence: Fast, Accurate and Scalable Binary Code
Reuse Detection
He Huang
Concordia Institute for
Information Systems
Engineering
Concordia University,
Montreal, QC, Canada
PI:EMAIL
Amr M. Youssef
Concordia Institute for
Information Systems
Engineering
Concordia University,
Montreal, QC, Canada
PI:EMAIL
Mourad Debbabi
Concordia Institute for
Information Systems
Engineering
Concordia University,
Montreal, QC, Canada
PI:EMAIL
ABSTRACT
Code reuse detection is a key technique in reverse engineer-
ing. However, existing source code similarity comparison
techniques are not applicable to binary code. Moreover,
compilers have made this problem even more diﬃcult due
to the fact that diﬀerent assembly code and control ﬂow
structures can be generated by the compilers even when im-
plementing the same functionality. To address this prob-
lem, we present a fuzzy matching approach to compare two
functions. We ﬁrst obtain an initial mapping between basic
blocks by leveraging the concept of longest common subse-
quence on the basic block level and execution path level.
We then extend the achieved mapping using neighborhood
exploration. To make our approach applicable to large data
sets, we designed an eﬀective ﬁltering process using Min-
hashing. Based on the proposed approach, we implemented
a tool named BinSequence and conducted extensive experi-
ments with it. Our results show that given a large assembly
code repository with millions of functions, BinSequence is
eﬃcient and can attain high quality similarity ranking of as-
sembly functions with an accuracy of above 90%. We also
present several practical use cases including patch analysis,
malware analysis and bug search.
CCS Concepts
•Security and privacy → Software security engineer-
ing; •Social and professional topics → Software re-
verse engineering;
Keywords
binary code reuse; binary code similarity comparison; patch
analysis; bug search; malware analysis;
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from permissions@acm.org.
ASIA CCS ’17, April 02-06, 2017, Abu Dhabi, United Arab Emirates
c(cid:13) 2017 ACM. ISBN 978-1-4503-4944-4/17/04. . . $15.00
DOI: http://dx.doi.org/10.1145/3052973.3052974
1.
INTRODUCTION
Reverse engineering is a primary step towards understand-
ing the functionality and behavior of a software when its
source code is not available. However, reverse engineering
is a tedious and time-consuming process, and its success de-
pends heavily on the experience and knowledge of the reverse
engineer. Moreover, as the software to be analyzed grows in
size, this task becomes overwhelming. Code reuse detection
is thus of a great interest to reverse engineers. For exam-
ple, given a binary and a repository of already analyzed and
commented code, one can speed up the analysis by applying
code reuse detection to the binary to identify identical or
similar code in the repository, and then focus only on the
new functionality or components of the binary.
Consider, for instance, malware reverse engineering. Mal-
ware authors do not create viruses from scratch; instead,
they tend to reuse their existing source code. Besides, in or-
der to not reinvent the wheel, they may leverage some open
source projects that provide certain functionality that they
require. Identifying those reused code not only greatly re-
duces the eﬀorts of analysis, but also helps in understanding
the behavior of malware. For example, Citadel, which is al-
legedly derived from the leaked Zeus source code, keeps most
of the core components of Zeus intact [20], and the malware
Flame makes use of the light-weight database engine SQLite
[10].
Code reuse detection is also of high interest to software
maintainers and consumers. In many software development
environments, it is common practice to copy and paste ex-
isting source code, as this can signiﬁcantly reduce program-
ming eﬀort and time. However, if the copied code contains
a bug or vulnerability, and the developers copied the code
without ﬁxing the bug, they may bring the bug into their
own project. Library reuse is a special case in which de-
velopers either include the source code of a certain library
into their project, or statically link directly to the library.
In either way, the bug contained in the copied code will be
brought into the new project. Code reuse detection can help
identify such bugs resulting from shared source code.
Last but not least, code reuse detection can be applied in
numerous scenarios such as software plagiarism detection,
open source project license violation detection and binary
diﬃng.
Code reuse detection can be achieved by calculating the
similarity of two code regions. The higher the similarity, the
more likely these code regions are from the same source code
base. In this paper, we present an approach for measuring
the similarity of two assembly functions. In particular, our
contributions can be summarized as follows:
• We propose a fuzzy matching approach to compare
assembly functions. To address the mutations intro-
duced by compilers, our fuzzy matching algorithm op-
erates at multiple levels, i.e., instruction level, basic
block level and structure level.
• We design and implement an eﬀective ﬁltering pro-
cess to prune the search space when comparing a tar-
get function against a vast number of functions. We
propose two ﬁlters that eﬃciently rule out functions
that are unlikely to be matched to our target function.
With the help of this ﬁltering process, we can compare
one target function against millions of other functions
within seconds.
• We design and implement a fully functioning tool for
binary code reuse detection based on the proposed ap-
proach. Our extensive experiments show that our tool
is fast, accurate, and scalable.
• We introduce many use cases, including patch analy-
sis, malware analysis, and bug search, to demonstrate
the eﬃciency and eﬀectiveness of our approach when
applied in real-world scenarios.
2. PROBLEM STATEMENT
The problem we are trying to solve can be described as fol-
lows: Given one target binary function from one executable,
and a large repository with thousands or millions of func-
tions from other executables, how to identify all the identi-
cal or similar functions from the repository. This problem
is two-fold. First, how to compare two assembly functions
and obtain a similarity score. Second, how to eﬃciently re-
trieve those ones that are likely to be identical or similar
to our target function and at the same time, avoid pairwise
comparison.
In this work, we establish the similarity of two functions
by comparing their control ﬂow graphs (CFGs). The CFG
of an assembly function is a directed graph, where nodes
represent basic blocks, and edges represent the execution
ﬂow between basic blocks.
The compiler is responsible for transforming the source
code into assembly code. Take C++ for example, generally
speaking there are four types of control structures:
- Sequential control structure
- Selection control structure (e.g., if, if-else or switch
statement)
- Iteration control structure (e.g., for, while or do-while
loop)
- Goto structure
(a) The “if-else” structure
(b) The “for” loop structure
Figure 1: Examples of control structures and corresponding
CFGs
Although diﬀerent compilation environments would bring
some mutations or “noise” into the CFGs, still the overall
structure is relatively stable. As can be seen from Figure
1, the mapping between source code statements and basic
blocks is stable as well.
Based on these observations, we choose to use a basic
block-centric approach when comparing two functions. We
ﬁrst ﬁnd the mapping of basic blocks between these two
functions and then for every matching basic block pair, we
obtain a matching score. Finally, we calculate the similarity
score of two functions from the matching results of the basic
blocks.
In this paper, we introduce BinSequence, which uses the
similarity score between basic blocks as the building block.
Guided by these scores, we continue to ﬁnd the mapping of
basic blocks based on the control ﬂow graph. Finally, we
calculate the similarity score from the found mapping.
The rest of the paper is organized as follows. In section 3
we introduce the fuzzy matching approach we use to com-
pare functions. Section 4 provides a detailed description of
our ﬁltering process. In Section 5 we evaluate our approach
with extensive experiments and give the results. We then
review related works in Section 6, and give limitations in
section 7. We conclude the paper in Section 8.
Normally the sequential control structure will not bring
addition edges or branches into the control ﬂow graph, while
the later three structures would. Figure 1 shows some typ-
ical examples of those structures and their corresponding
CFGs. Note that as these structures can be nested in source
code, so do their corresponding CFGs.
3. ALGORITHM DESCRIPTION
Figure 2 depicts an overview of BinSequence framework.
First, a collection of interesting binaries such as previously
analyzed malware or open source projects that may have
been reused, is disassembled. The output is a set of func-
if(expr){    statement1;else    statement2;}next statement;cmpjlestatement1statement2next statementfor( ;expr; ){    statement;}next statement;cmpjgenext statementstatementFigure 2: Workﬂow of BinSequence
tions. We then keep all the functions in a large repository
after normalizing them. Given a target function, the naive
way is to compare it with every function in the repository
and rank the results. However, this is not eﬃcient as most of
the functions in the repository are not similar to our target
and should thus not be compared. To speed up the process,
we focus only on those functions that are likely to be similar
with our target. To this end, we adopt a ﬁltering process
in which we use two ﬁlters. The ﬁrst ﬁlter is based on the
number of basic blocks, while the second is based on the
similarity of feature sets that we extracted as ﬁngerprints
for every function. The output of the ﬁltering process is a
subset of functions from the repository, which we call the
candidate set. We then perform pairwise comparisons of
the target function with every function in the candidate set.
The comparison consists of three phases. First, we generate
the longest path of the target function. Then we explore
the reference function in the candidate set to ﬁnd the cor-
responding matching path, from which we can obtain the
initial mapping of basic blocks. We then improve the map-
ping through neighborhood exploration in both the target
and reference functions. The output is the mapping of basic
blocks and the similarity score of these two functions. After
we have done this to every function in the candidate set, we
obtain a ranking of functions based on the similarity score.
3.1 Disassembly and Normalization
Given a collection of binaries, the ﬁrst step is to disassem-
ble each binary to a set of functions. In our experiments,
we use IDA Pro [5] as our front-end to perform code anal-
ysis and to generate the control ﬂow graph for every func-
tion. Since the compiler has many choices with regard to
mnemonics, registers and memory allocations when gener-
ating the assembly code, it is essential that every assembly
instruction in the basic block is normalized before compari-
son [26].
Note that for most architectures like X84 and X86-64 an
assembly instruction consists of a mnemonic and a sequence
of up to 3 operands. When normalizing instructions, we keep
the mnemonics untouched, and only normalize the operands.
We classify the operands into three categories, namely reg-
isters, memory references and immediate values. For imme-
diate values, we further normalize them into two categories,
memory oﬀsets (addresses) and constant values. The rea-
son to diﬀerentiate between addresses and constant values
is that addresses would change according to diﬀerent assem-
bly code layouts while constant values do not. If an imme-
diate value is classiﬁed as constant value, we keep the literal
value. The motivation is that normally constants stay the
same even when diﬀerent compilers or optimization levels
have been used. Some literatures also consider strings as a
special type of data constants [12, 17]. In [12] David and
Yahav replace an oﬀset with the string and take the string
into comparison if the oﬀset points to a string. However, we
consider only integers. The reason is that most strings come
directly from source code and thus can be modiﬁed with-
out diﬃculty. A malware author could easily evade those
string based detection techniques by changing the strings
inside the source code without changing the functionality.
However, the integers are more related to the functionality,
which makes them a better target in reverse engineering.
3.2
Instruction Comparison
Inspired by the recent work in [12], we use a similar strat-
egy when comparing instructions. As depicted in Algo-
rithm 1, for two normalized instructions, if they have dif-
ferent mnemonics, then their matching score is 0 regardless
of their operands. Otherwise, we give them a score for iden-
tical mnemonic and continue to compare their operands. If
their corresponding operands are the same after normaliza-
tion, then we give them an additional score for each match-
ing operand. Notice that mnemonics represent the low-
level machine operations and carry more information than
operands, thus we should give a higher score to identical
mnemonic. At the same time, to avoid the information car-
ried by operands from getting neglected, this score could
not be overly high. Constants also carry much information
from the source. When comparing two constant operands,
we further compare their literal values. If their literal val-
ues are the same, we then give them an additional score.
By FingerprintSimilarityBy Basic BlockNumberFilteringStoringDisassembling& NormalizingTarget FunctionBinariesMatchingBasic Block Mapping& Function SimilarityResultCandidateTargetLongest Path GenerationPath Expl.Neighborhood Expl.RankingDuring our experiments we found that it is appropriate to
give score 1, 2 and 3 to identical operand, mnemonic and
constant respectively. Using these score values, we can al-
low those important parts of instructions to match, and at
the same time, without getting misled by this biased score
strategy. Following this strategy, we can calculate that, the
score of comparing push eax with push ebx is 3, as both are
push REG after normalization, while the score of comparing
push 0 with push 1 is only 2 as the literal value of their
operands is not the same.
Algorithm 1: Compare two instructions
Input: Two normalized instructions
Output: The matching score of two instructions
score = 0
if ins1.Mnemonic == ins2.Mnemonic then
1 Function CompIns(ins1,ins2)