Instruction Coverage. In this experiment, we mea-
sured the numbers of unique instruction addresses (i.e.,
EIP values) of the program binary and its libraries cov-
ered by MACE and the baseline approach. These num-
bers show how effective the approaches are at uncov-
ering new code regions in the analyzed program. For
Vino, RealVNC, and Samba, we used dynamic symbolic
execution as the baseline approach and ran the experi-
ment using the setup outlined in Section 6.1. We ran
MACE allowing 2.5 hours of state-space exploration per
each inferred state. To provide a fair comparison, we
ran the baseline for the amount of time that is equal to
the sum of the MACE’s inference and state-space explo-
ration times. As shown in Table 3, our result illustrates
that MACE provides a signiﬁcant improvement in the in-
struction coverage over dynamic symbolic execution.
As mentioned before, our tool currently works on user-
space programs only. Because Windows SMB is mostly
implemented as a part of the Windows kernel, the results
of the baseline approach were abysmal. To avoid a straw
man comparison, we chose to compare against Samba’s
gentest test suite, regularly used by Samba developers
to test the SMB protocol. Using the test suite, we gen-
erate test sequences and measure the obtained coverage.
As for other experiments, we allocated the same amount
of time to both the test suite and MACE. The experimen-
tal results clearly show MACE’s ability to augment test
suites manually written by developers.
Number of Detected Crashes. Using the same setup
as in the previous experiment, we measured the num-
ber of crashing input sequences generated by each ap-
proach. We report the number of crashes and the num-
ber of unique crash locations. From each category of
unique crash locations, we manually processed the ﬁrst
four reported crashes. All the found vulnerabilities (Ta-
ble 2) were found by processing the very ﬁrst crash in
each category. All the later crashes we processed were
just variants of the ﬁrst reported crash. MACE found 30
crashing input sequences with 9 of them having unique
crash locations (the EIP of the crashed instruction). In
comparison, the baseline approach only found 20 crash-
ing input sequences, all of them having the same crash
location.
100 %
80 %
60 %
40 %
20 %
0 %
1
2
1
1
1
10
25
25
13
4
1
11
4
8
MACE
Baseline
0
1
2
3
4
5
6
7
0
8
0
9
0
10
Figure 5: SMB Exploration Depth. The inferred state
machine can be seen as a directed graph. Suppose we
compute a spanning tree (e.g., [13]) of that graph. The
root of the graph is at level zero. Its children are at level
one, and so on. The ﬁgure shows the percentage of states
visited at each level by MACE and the baseline approach.
The numbers above points show the number of visited
states at the given depth. The shaded area clearly shows
that MACE is superior to the baseline approach in reach-
ing deep states of the inferred protocol.
Exploration Depth. Using the same setup as for the
coverage experiment, we measured how effective each
approach is in reaching deep states. The inferred state
machine can be seen as a directed graph. Suppose we
compute a spanning tree (e.g., [13]) of that graph. The
root of the graph is at level zero.
Its children are at
level one, and so on. We measured the percentage of
states reached at every level. Figure 5 clearly shows that
MACE is superior to the baseline approach in reaching
deep states in the inferred protocol.
7 Limitations
Completeness is a problem for any dynamic analysis
technique. Accordingly, MACE cannot guarantee that
all the protocol states will be discovered. Incomplete-
ness stems from the following: (1) each state-space ex-
plorer instance runs for a bounded amount of time and
some inputs may simply not be discovered before the
timeout, (2) among multiple shortest transfer sequences
to the same abstract state, MACE picks one, potentially
missing further exploration of alternative paths, (3) sim-
ilarly, among multiple concrete input messages with the
same abstract behavior, MACE picks one and considers
the rest redundant (Deﬁnition 2).
Our approach to model inference and reﬁnement is
not entirely automatic: the end users need to provide an
abstraction function that abstracts concrete output mes-
sages into an abstract alphabet. Coming up with a good
output abstraction function can be a difﬁcult task. If the
provided abstraction is too ﬁne-grained, model inference
may be too expensive to compute or may not even con-
verge. On the other hand, the inferred model may fail to
distinguish two interesting states if the abstraction is too
coarse-grained. Nevertheless, our approach provides an
important improvement over our prior work [11], which
requires abstraction functions for both input and output
messages.
When using our approach to learn a model of a pro-
prietary protocol, a certain level of protocol reverse-
engineering is required prior to running MACE. First, we
need a basic level of understanding of the protocol inter-
face to be able to correctly replay input messages to the
analyzed program. For example, this may require over-
writing the cookie or session-id ﬁeld of input messages
so that the sequence appears indistinguishable from real
inputs to the target program. Second, our approach re-
quires an appropriate output abstraction, which in turn
requires understanding of the output message formats.
Message format reverse-engineering is an active area of
research [14, 15, 6] out of the scope of this paper.
Encryption is a difﬁcult problem for every (existing)
protocol inference technique. To circumvent the issue,
we conﬁgure the analyzed programs not to use encryp-
tion. However, for proprietary protocols, such a con-
ﬁguration may not be available and techniques [5, 29]
that automatically reverse-engineer message encryption
are required.
8 Acknowledgements
We would like to thank the anonymous reviewers for
their insightful comments to improve this manuscript.
This material is based upon work partially supported by
the NSF under Grants No. 0311808, 0832943, 0448452,
0842694, 0627511, 0842695, 0831501, and 0424422, by
the AFRL under Grant No. P010071555, by the ONR
under MURI Grant No. N000140911081, and by the
MURI program under AFOSR Grants No. FA9550-08-1-
0352 and FA9550-09-1-0539. The second author is also
supported by the NSERC (Canada) PDF fellowship.
9 Conclusions and Future Work
We have proposed MACE, a new approach to software
state-space exploration. MACE iteratively infers and re-
ﬁnes an abstract model of the protocol, as implemented
by the program, and exploits the model to explore the
program’s state-space more effectively. By applying
MACE to four server applications, we show that MACE
(1) improves coverage up to 58.86%, (2) discovers sig-
niﬁcantly more vulnerabilities (seven vs. one), and (3)
performs signiﬁcantly deeper search than the baseline
approach.
We believe that further research is needed along sev-
eral directions. First, a deeper analysis of the correspon-
dence of the inferred ﬁnite state models to the structure
and state-space of the analyzed application could reveal
how models could be used even more effectively than
what we propose in this paper. Second, it is an open
question whether one could design effective automatic
abstractions of the concrete input messages. The ﬁlter-
ing function we propose in this paper is clearly effective,
but might drop important messages. Third, the ﬁnite-
state models might not be expressive enough for all types
of applications. For example, subsequential transducers
[28] might be the next, slightly more expressive, repre-
sentation that would enable us to model protocols more
precisely, without signiﬁcantly increasing the inference
cost. Fourth, MACE currently does no white box analy-
sis, besides dynamic symbolic execution for discovering
new concrete input messages. MACE could also monitor
the value of program variables, consider them as the in-
put and the output of the analyzed program, and automat-
ically learn the high-level model of the program’s state-
space. This extension would allow us to apply MACE to
more general classes of programs.
References
[1] ANGLUIN, D. Learning regular sets from queries and
Information and Computation 75, 2
counterexamples.
(1987), 87–106.
[2] BALL, T., MAJUMDAR, R., MILLSTEIN, T., AND RA-
JAMANI, S. Automatic Predicate Abstraction of C Pro-
grams.
In PLDI’01: Proc. of the ACM SIGPLAN 2001
Conf. on Programming Language Design and Implemen-
tation (2001), vol. 36 of ACM SIGPLAN Notices, ACM
Press, pp. 203–213.
[3] BARNETT, M., DELINE, R., F ¨AHNDRICH, M., JACOBS,
B., LEINO, K. R., SCHULTE, W., AND VENTER, H.
Veriﬁed software: Theories, tools, experiments. Springer-
Verlag, 2008, ch. The Spec# Programming System: Chal-
lenges and Directions, pp. 144–152.
[4] BENZEL, T., BRADEN, R., KIM, D., NEUMAN, C.,
JOSEPH, A., SKLOWER, K., OSTRENGA, R., AND
SCHWAB, S. Design, deployment, and use of the DETER
testbed.
In Proc. of the DETER Community Workshop
on Cyber Security Experimentation and Test on DETER
Community Workshop on Cyber Security Experimenta-
tion and Test (2007), USENIX Association.
[5] CABALLERO, J., POOSANKAM, P., KREIBICH, C., AND
SONG, D. Dispatcher: Enabling active botnet inﬁltra-
In
tion using automatic protocol reverse-engineering.
CCS’09: Proc. of the 16th ACM conference on Computer
and communications security (2009), ACM, pp. 621–634.
[6] CABALLERO, J., YIN, H., LIANG, Z., AND SONG, D.
Polyglot: Automatic extraction of protocol message for-
mat using dynamic binary analysis. In CCS’07: Proc. of
the 14th ACM Conf. on Computer and Communications
Security (2007), ACM, pp. 317–329.
[7] CADAR, C., DUNBAR, D., AND ENGLER, D. KLEE:
Unassisted and automatic generation of high-coverage
tests for complex systems programs.
In OSDI’08:
Proc. of the 8th USENIX Symposium on Operating Sys-
tems Design and Implementation (2008), USENIX Asso-
ciation, pp. 209–224.
[8] CADAR, C., AND ENGLER, D. R. Execution generated
test cases: How to make systems code crash itself.
In
SPIN’05: Proc. of the 12th Int. SPIN Workshop on Model
Checking Software (2005), vol. 3639 of Lecture Notes in
Computer Science, Springer, pp. 2–23.
[9] CADAR, C., GANESH, V., PAWLOWSKI, P. M., DILL,
D. L., AND ENGLER, D. R. Exe: Automatically gen-
erating inputs of death. ACM Trans. Inf. Syst. Secur. 12
(2008), 1–38.
[10] CHO, C. Y., BABI ´C, D., SHIN, R., AND SONG, D. In-
ference and analysis of formal models of botnet command
and control protocols. In CCS’10: Proc. of the 2010 ACM
Conf. on Computer and Communications Security (2010),
ACM, pp. 426–440.
[11] CHO, C. Y., CABALLERO, J., GRIER, C., PAXSON, V.,
AND SONG, D. Insights from the inside: A view of bot-
net management from inﬁltration. In LEET’10: Proc. of
the 3rd USENIX Workshop on Large-Scale Exploits and
Emergent Threats (2010), USENIX Association, pp. 1–1.
[12] COMPARETTI, P. M., WONDRACEK, G., KRUEGEL, C.,
AND KIRDA, E. Prospex: Protocol speciﬁcation extrac-
tion. In S&P’09: Proc. of the 2009 30th IEEE Symposium
on Security and Privacy (2009), IEEE Computer Society,
pp. 110–125.
[13] CORMEN, T. H., LEISERSON, C. E., RIVEST, R. L.,
AND STEIN, C. Introduction to Algorithms, 2nd ed. The
MIT Press, 2001.
[14] CUI, W., KANNAN, J., AND WANG, H. J. Discov-
erer: Automatic protocol reverse engineering from net-
work traces. In Proc. of 16th USENIX Security Sympo-
sium (2007), USENIX Association, pp. 1–14.
[15] CUI, W., PEINADO, M., CHEN, K., WANG, H. J., AND
IR ´UN-BRIZ, L. Tupni: Automatic reverse engineering
of input formats.
In CCS’08: Proc. of the 15th ACM
Conf. on Computer and Communications Security (2008),
ACM, pp. 391–402.
[16] DE LA HIGUERA, C. Grammatical Inference: Learning
Automata and Grammars. Cambridge University Press,
2010.
[17] GODEFROID, P., KLARLUND, N., AND SEN, K. DART:
directed automated random testing. In PLDI’05: Proc. of
the ACM SIGPLAN Conf. on Programming Language De-
sign and Implementation (2005), ACM, pp. 213–223.
[18] GODEFROID, P., LEVIN, M. Y., AND MOLNAR, D. A.
Automated whitebox fuzz testing. In NDSS’08: Proc. of
the Network and Distributed System Security Symposium
(2008), The Internet Society.
[19] GULAVANI, B. S., HENZINGER, T. A., KANNAN, Y.,
NORI, A. V., AND RAJAMANI, S. K. SYNERGY: a new
algorithm for property checking.
In FSE’06: Proc. of
the 14th ACM SIGSOFT Int. Symp. on Foundations of
Software Engineering (2006), ACM, pp. 117–127.
[20] HENZINGER, T. A., JHALA, R., MAJUMDAR, R., AND
SUTRE, G. Software Veriﬁcation with Blast. In SPIN’03:
Proc. of the 10th Int. Workshop on Model Checking of
Software (2003), vol. 2648 of LNCS, Springer-Verlag,
pp. 235–239.
[21] HO, P. H., SHIPLE, T., HARER, K., KUKULA, J.,
DAMIANO, R., BERTACCO, V., TAYLOR, J., AND
LONG, J. Smart simulation using collaborative formal
and simulation engines. In ICCAD’00: Proc. of the 2000
IEEE/ACM Int. Conf. on Computer-aided design (2000),
IEEE Press, pp. 120–126.
[22] KING, J. C. Symbolic execution and program testing.
Communications of the ACM 19, 7 (1976), 385–394.
[23] MEALY, G. H. A method for synthesizing sequential cir-
cuits. Bell System Technical Journal 34, 5 (1955), 1045–
1079.
[24] PELED, D., VARDI, M. Y., AND YANNAKAKIS, M.
Black box checking.
the IFIP TC6
WG6.1 Joint Int. Conf. on Formal Description Techniques
for Distributed Systems and Communication Protocols
(FORTE XII) and Protocol Speciﬁcation, Testing and Ver-
iﬁcation (PSTV XIX) (1999), Kluwer, B.V., pp. 225–240.
In Proc. of
[25] SEN, K., MARINOV, D., AND AGHA, G. Cute: a con-
colic unit testing engine for c. SIGSOFT Softw. Eng.
Notes 30 (2005), 263–272.
[26] SHAHBAZ, M., AND GROZ, R.
Inferring Mealy ma-
chines. In FM’09: Proc. of the 2nd World Congress on
Formal Methods (2009), Springer, pp. 207–222.
[27] VEANES, M., CAMPBELL, C., GRIESKAMP, W.,
SCHULTE, W., TILLMANN, N., AND NACHMANSON,
L. Formal methods and testing. Springer-Verlag, 2008,
ch. Model-based testing of object-oriented reactive sys-
tems with spec explorer, pp. 39–76.
[28] VILAR, J. M. Query learning of subsequential transduc-
ers.
In Proc. of the 3rd Int. Colloquium on Grammat-
ical Inference: Learning Syntax from Sentences (1996),
Springer-Verlag, pp. 72–83.
[29] WANG, Z., JIANG, X., CUI, W., WANG, X., AND
GRACE, M. ReFormat: Automatic reverse engineer-
ing of encrypted messages.
In ESORICS’09: 14th Eu-
ropean Symposium on Research in Computer Security
(2009), vol. 5789 of Lecture Notes in Computer Science,
Springer, pp. 200–215.
[30] ZHANG, L., MADIGAN, C. F., MOSKEWICZ, M. H.,
AND MALIK, S. Efﬁcient conﬂict driven learning in a
boolean satisﬁability solver. In ICCAD’01: Proc. of the
Int. Conf. on Computer-Aided Design (2001), IEEE Press,
pp. 279–285.