5Our example here treats bestP athCache as a base table whose
contents are not explicitly de(cid:12)ned in the datalog rules. The logic
bestPath tuple, the original left recursion computation can
be used. If a cached bestPath tuple generated previously is
available, BPPS2 reuses the cached tuple instead. To illus-
trate, we revisit the example network in Figure 3. Consider
the earlier case when two source nodes b and c are computing
best paths to node e. If bestP athCache(d; e; [d; e]; 1) is stored
locally at d, it can be used by both nodes b and c using rule
BPPS2. This avoids duplicate traversals of the path d ! e
and beyond. On the other hand, if this tuple is not present,
the left recursion rule BPPS1 will be used instead.
Further sharing is achieved if the resulting path tuples are
sent back via the reverse path to the source node to be reused
by other queries. For example, when node a computes its best
path to node e, the nodes on the reverse path (b and d) can
cache information on the shortest path (and sub-paths) to
node e, to be reused by subsequent queries.
Next, we consider the other mode of sharing, where queries
have only partial similarity. We focus on the case where the
rules are largely identical, with the exception of di(cid:11)erences
in function calls. For example, consider running two vari-
ants of the Best-Path query from Section 5.1, one that com-
putes shortest paths, and another that computes max-(cid:13)ow
paths. We can merge these into a single variant of the Best-
Path query by simply tracking two running cost attributes
(e:g:; path length and path capacity) and checking two ag-
gregate selections (e:g:; min(path-length), max(capacity)).
The merged query will share all path exploration across the
queries. Aggregate selections continue to be applicable, but
can only prune paths that satisfy both aggregate selections;
pruning is e(cid:11)ective when the selections are correlated.
8. STABILITY AND ROBUSTNESS
As discussed in Section 2, each query that is issued is accom-
panied by a speci(cid:12)cation of the desired lifetime (duration) of
the computed route. During this period, changes in the net-
work might result in some of the computed routes becoming
stale. These can be caused by link failures, or changes in
the link metrics when these metrics are used in route com-
putation. Ideally, the query should rapidly recompute a new
route, especially in the case of link failures.
One solution is to simply recompute the queries from scratch,
either periodically or driven by the party that has issued the
queries. However, recomputing the query from scratch is ex-
pensive, and if done only periodically, the time to react to
failures is a half-period on average. The alternative approach
we employ in this paper is to utilize long-running or contin-
uous queries that incrementally recompute new results based
on changes in the network. To ensure incremental recompu-
tations, all intermediate state of each query is retained in the
query processor until the query is no longer required. This
intermediate state includes any shipped tuples used in join
computation, and any intermediate derived tuples.
As we discussed in Section 2, each router is responsible for
detecting changes to its local information or base tables and
reporting these changes to its local query processor. These
base tuple updates result in the addition of tuples into base
tables, or the replacement of existing base tuples that have
the same unique key as the update tuples. The continuous
queries then utilize these updates and the intermediate state
of the queries to incrementally recompute some of their de-
rived tuples.
To illustrate, consider the Network-Reachability query in
Section 3. Figure 4 shows a simple four node network where
all four nodes are running the Network-Reachability query.
Prior to the failure of node d, we assume that all paths be-
tween all pairs have been computed.
This query is executed at all nodes in the network, but for
simplicity we focus on the tuples generated at nodes a and
for populating the cache is therefore not fully declarative. Address-
ing this issue is an intriguing direction for further research.
p((cid:13)a(cid:13),d,[a,c,d],2)(cid:13)
a(cid:13)
a(cid:13)
p((cid:13)a(cid:13),d,[a,c,d],(cid:13)infinity(cid:13) )(cid:13)
b(cid:13)
p((cid:13)c(cid:13),d,[c,d],infinity),(cid:13)
l(cid:13)’(cid:13)(a,(cid:13)c(cid:13),1) (cid:13)
l((cid:13)c(cid:13),d,(cid:13)infinity(cid:13) )(cid:13)
c(cid:13)
b(cid:13)
c(cid:13)
p((cid:13)c(cid:13),d,[c,d],infinity)(cid:13)
l(cid:13)’(cid:13)(a,(cid:13)c(cid:13),1), (cid:13)
l((cid:13)c(cid:13),d,(cid:13)infinity(cid:13) )(cid:13)
d(cid:13)
0(cid:13)th(cid:13) Iteration(cid:13)
d(cid:13)
1(cid:13)st(cid:13)Iteration(cid:13)
Figure 4: The (cid:12)gure shows the changes to the intermedi-
ate query states that led to the derivation of p(a; d; [a; c; d]; 1)
when node d fails. For simplicity, we only show the states on
nodes a and c necessary to show the derivation.
c. p(S; D; P; C) abbreviates path(S; D; P; C) and l0(S; D; C)
refers to link tuples that are sent and cached at the desti-
nation nodes. We examine how p(a; d; [a; c; d]; 1) is created
when node d fails:
1. When node d fails, neighbor c detects the failure and
generates an updated base tuple l(c; d; 1) locally. This
replaces the previous tuple l(c; d; 1).
2. All paths at node c that traverse through d are set to in-
(cid:12)nite costs6. For example, node c generates p(c; d; [c; d]; 1).
3. p(c; d; [c; d]; 1) is joined locally with l0(a; c; 1) to pro-
duce p(a; d; [a; c; d]; 1) which is sent to node a.
The failure is propagated hop-by-hop and in this example,
since we compute the entire path vector and can check for po-
tential cycles as described in Section 3, the time taken for any
update to converge is proportional to the network diameter.
Updates to link costs are handled in a similar fashion, ex-
cept that rather than setting the costs to in(cid:12)nity, they are
recomputed based on the new link costs. The updated paths
may trigger further computation. For example, when the cost
of paths are changed, rules BPR1 and BPR2 of the Best-Path
query will generate alternative best paths accordingly.
9. PERFORMANCE EVALUATION
To evaluate our solution, we have implemented a prototype
system using PIER [2], a distributed relational query proces-
sor written in Java. Each node runs a PIER query engine, and
maintains a neighbor table directly accessible by the PIER
process. We have modi(cid:12)ed the PIER software to bypass the
use of DHTs [7] and instead use explicit neighbor tables. A
PIER process can contact only the PIER processes on neigh-
bor nodes. Routing protocols expressed as queries can be
issued directly to any PIER node, which then communicates
with the neighbor PIER nodes to evaluate the queries.
We evaluate the system using a combination of simulations
on transit-stub topologies (Section 9.1), and an actual de-
ployment on PlanetLab (Section 9.2). Both the simulation
and the actual implementation share the same code base.
Our evaluation suggests that our approach is feasible and
that its expressiveness does not come at the expense of any
signi(cid:12)cant degradation of scalability or performance. Our
main results can be summarized as follows:
1. When all nodes issuing the same query, we show that the
query execution has similar scalability properties as the
traditional distance vector and path vector protocols.
2. When di(cid:11)erent set of nodes issuing di(cid:11)erent queries,
the query optimization and work-sharing techniques are
e(cid:11)ective in reducing the communication overhead.
6An additional
path(S,D,P,C2), f inP ath(P; W ) = true, C1 = 1 is required.
path(S,D,P,1) :-
rule NR3:
link(S,W,C1),
3. Our prototype deployment on PlanetLab shows that our
system is able to react quickly to changes (either RTT
(cid:13)uctuations or churn) and (cid:12)nd alternative paths.
9.1 Simulation Settings and Metrics
In our simulations, we run multiple PIER nodes on top of
an event-driven network simulator that simulates bandwidth
and latency bottlenecks. We generate transit-stub topologies
using the GT-ITM topology generator [1]. The transit-stub
topology consists of eight nodes per stub, three stubs per
transit node, and four nodes per transit domain. We increase
the number of nodes in the network by increasing the number
of domains. The latency between transit nodes is set to 50
ms, the latency between a transit and a stub node is 10 ms,
and the latency between any two nodes in the same stub is
2 ms. The capacity of each node is set to 10 Mbps; this is
never a bottleneck in the query execution.
)
s
m
t
(
r
e
e
m
a
D
i
 700
 600
 500
 400
 300
 200
 100
 0
 0
 200  400  600  800  1000
Number of Nodes
t
)
s
(
y
c
n
e
a
L
e
c
n
e
g
r
e
v
n
o
C
 5
 4.5
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
Query
PV
 0
 200  400  600  800  1000
Number of Nodes
Figure 5: Network diame-
ter vs Number of nodes.
Figure 6: Convergence la-
tency vs Number of nodes.
We use (cid:12)ve network sizes ranging from 100 to 1000. For each
network size, we average all our experimental results over (cid:12)ve
runs. Figure 5 shows the diameter (latency of longest path)
of the network as we increase the number of nodes, with the
standard deviation error bars for each data point.
Our experiments consist of two di(cid:11)erent workloads. Our
(cid:12)rst workload (Section 9.1.1) involves a query that is being ex-
ecuted on all nodes, while our second workload (Section 9.1.2
and Section 9.1.3) involves a subset of nodes executing either
the same or di(cid:11)erent query. We measure the performance of
query execution using two metrics:
Convergence latency: time taken for the query execution
to generate all the query results.
Per-node communication overhead (or simply commu-
nication overhead): the number of KB transfered on aver-
age per node during the query execution.
9.1.1 All›Pairs Shortest Paths
In our (cid:12)rst experiment, we measure the performance of our
system when all nodes are running the same query. We exe-
cute the Best-Path query as described in Section 5.1 to com-
pute the shortest latency paths between all pairs of nodes.
This query is disseminated from a random node, and each
node that receives the query starts executing the query plan
shown in Figure 2.
In our implementation, we use the aggregate selections op-
timization to avoid sending redundant path tuples (see Sec-
tion 7.1). Each node collects tuples received from neighboring
nodes, applies aggregate selections and computes new path
tuples every 200 ms.
In Figure 6 shows the convergence latency for the Best-Path
query (Query line) as the number of nodes increases. For
validation, we compare the convergence latency against our
own implementation of the path-vector protocol (PV line) for
computing all-pairs shortest paths using the same simulation
setup. We make two observations. First, as expected, the
convergence latency for the Best-Path query is proportional
to the network diameter, and converges in the same time
compared to the path vector protocol. Second, the per-node
communication overhead increases linearly with the number
of nodes, as each node needs to compute the shortest path
to every other node in the network. Both observations are
consistent with the scalability properties of the traditional
distance vector and path vector protocols, suggesting that
our approach does not introduce any fundamental overheads.
9.1.2 Source/Destination Queries
Next, we study the e(cid:11)ects of query optimization techniques
on lowering the communication overhead when only a subset
of paths are computed. Instead of computing all pairs, our
workload consists of a collection of Best-Path-Pairs queries.
Recall from Section 7.2 that Best-Path-Pairs is an optimized
version of Best-Path, where some of the rules are rewritten
using magic sets and left-right recursion optimization tech-
niques to reduce the communication overhead. Queries are
issued periodically every 15 sec. Each query computes the
shortest path between a pair of nodes, and the result tuple is
sent back on the reverse path to the source.
Figure 7 shows the per-node communication overhead, as
the number of source/destination queries increases. In this
experiment, we use a 200-node network. The All Pairs line
represents our baseline, and shows the communication over-
head for computing all pairs shortest paths. Pair-NoShare
shows the communication overhead for running the Best-
Path-Pairs query with no sharing across queries. When there
are few queries, the communication overhead of Pair-NoShare
is signi(cid:12)cantly lower than of All Pairs, as the later computes
many paths which were never requested. However, as the
number of queries increases, the communication overhead in-
creases linearly, exceeding All Pairs after 130 queries.
Finally, Pair-Share shows the communication cost of execut-
ing the Best-Path-Pairs-Share query discussed in Section 7.3,
which rewrites some of the rules in Best-Path-Pairs to fa-
cilitate work-sharing. Pair-Share clearly decreases the com-
munication overhead of Pair-NoShare. As more queries are
issued, the increase in the communication overhead dimin-
ishes, as each subsequent query has an increased chance of
reusing previously generated results. However, as the num-
ber of queries increases beyond 240, Pair-Share becomes more
expensive than All Pairs.
Figure 8 shows the results from the same experiment as
Figure 7 as the number of source/destination queries increases
to 39; 800 (199(cid:2)200). The communication overhead for Pair-
Share levels o(cid:11) at 605 KB. Here, we also examine the impact
of limiting the choice of destination nodes on the e(cid:11)ectiveness
of sharing. This workload is illustrative of constructing a
multicast tree which requires the shortest paths to a small
set of nodes (see Section 5.5).
We compare Pair-Share against Pair-Share (X% Dst), which
limits the choice of destination nodes to X% of nodes. By lim-
iting the choice of destination nodes to 20% (1%) of nodes,
the communication overhead levels o(cid:11) at 119 KB (6) KB.
This is because the smaller X, the higher the cache hit rate
and the greater the opportunity for work-sharing. In fact, our
experiments show that if fewer than 30% of nodes are chosen
as destinations, executing Best-Path-Pairs-Share incurs lower
message overhead than computing all-pairs shortest paths, ir-
respective of the number of queries.
9.1.3 Mixed Query Workload
So far, we have focused on query workloads consisting of
identical queries between di(cid:11)erent source and destination nodes.
In Figure 9, Pair-Share-Mix shows the communication over-
head of running Best-Path-Pairs-Share queries on a mixed