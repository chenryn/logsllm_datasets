precedence over future writes to the cache-line and so a write-
attempt by a core to a coherency-requested cache line must
stall until the coherence request is satisﬁed. This is sufﬁcient to
avoid deadlock within the protocol: any cache lines that have
been requested by another core will eventually be checked and
released, as no cyclic dependencies can be constructed. If a
check fails, and a cache line is rolled back, then the earlier
data is available to the requesting core, once all checkpoints
earlier than the failure are committed.
D. Variable-Length AIMD Checkpointing
Fundamentally, parallel error checking exploits increasing
checking latency to extract parallelism in the redundant ex-
ecution of code. However,
in cases where we must stall
a main core, or delay a coherence request, to issue early
checks (e.g., unrepeatable reads and writes, cache evictions),
we have overestimated the amount of latency the system
can tolerate. One solution to this would be to reduce the
number of instructions, loads, and stores allowed between
each checkpoint. However, in cases where this behaviour does
not occur, this would reduce performance, as taking register
checkpoints is relatively expensive.
Our solution is to make the number of instructions allowed
between each checkpoint (i.e., in the corresponding load-store
log segment) dynamic. While we cannot do anything about
overestimating the checking latency the ﬁrst time this occurs,
we can use this as a prediction that the same will happen in
the short term future, and thus reduce the checkpoint length
accordingly. To do this, while generating a stable estimate for
checkpoint length, we use an additive increase multiplicative
decrease (AIMD) scheme, as used in TCP [23]. Here, every
event that triggers a pause in execution, or a delayed coherence
response, halves the target number of instructions in a segment.
In contrast, any checkpoints for which such events do not
occur increase the target instruction limit by 5, up to a limit
of 5,000, which is the maximum size of any load-store log
segment. This avoids penalising the cases where such pauses
do not occur, but is highly reactive when they do, thus rapidly
decreasing the target length when necessary, and increasing
the target slowly, to keep each checkpoint at a similar length
in the short term.
E. SMT Extension
Our current discussion has assumed that caches are private,
and only one thread can be executing on a core at a time. This
does not hold true for cores that support simultaneous multi-
threading [24], where instructions are committed from multiple
threads at once. Here, we need a more sophisticated scheme
that prevents loads and stores from different contexts from
entering the same load-store log segment, stops unchecked
writes from propagating to other threads, and deﬁnes an order
in which writes should be reversed on an error.
ParaMedic separates committed loads and stores by thread
ID, and issues separate checks to checker cores for each thread.
Thread ID bits are concatenated to the timestamp: a check can
commit when all previous timestamps with the same thread
207
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
Main Cores
Core
Pipeline
Tournament
Branch Pred.
Reg. Checkpoint
3-Wide, out-of-order, 3.2GHz
40-Entry ROB, 32-entry IQ, 16-entry LQ, 16-
entry SQ, 128 Int / 128 FP registers, 3 Int
ALUs, 2 FP ALUs, 1 Mult/Div ALU
2048-Entry local, 8192-entry global, 2048-
entry chooser, 2048-entry BTB, 16-entry RAS
16 cycles latency
L1 ICache
L1 DCache
L2 Cache
Memory
Cores
Log Size
Cache
Memory
32KiB, 2-way, 1-cycle hit lat, 6 MSHRs
32KiB, 2-way, 2-cycle hit lat, 6 MSHRs
1MiB shared, 16-way, 12-cycle hit
MSHRs, stride prefetcher
DDR3-1600 11-11-11-28 800MHz
lat, 16
Checker Cores
12× In-order, 4 stage pipeline, 1GHz
36KiB: 3KiB per core, 5000 inst. max length
2KiB L0 ICache per core, 16KiB shared L1
TABLE I: Core and memory experimental setup.
ID have committed. When a cache line is accessed that was
modiﬁed by another thread at an uncommitted timestamp, we
stall the reading thread, issuing an early checkpoint on the
thread that wrote to the cache line if necessary, until the
writing thread commits. This enforces a serial, independent
ordering on writes between threads, so on an error we need
only roll back the thread in which the error occurred.
F. Summary
This section has dealt with extending ParaMedic’s error cor-
rection to multicore systems, while still allowing the undoing
of stores after detecting an error. We avoid ordering issues
by ensuring that any communication between cores only
propagates correct data, by buffering writes in private caches
based on their timestamps. If a coherence request forces
communication early, the data response is delayed until after
it has been checked, potentially triggering a new checkpoint.
To prevent a loss of performance, the size of checkpoints is
dynamically adjusted, based on the amount of communication
and uncommitted data stored in the L1 cache, to adapt the
coarseness of parallelism to suit the application.
V. EXPERIMENTAL SETUP
To evaluate the performance impact and detection latency of
ParaMedic, we modeled a high-performance system using the
gem5 simulator [27] with the ARMv8 64-bit instruction set,
and conﬁguration given in table I. This is similar to systems
validated in previous work [28] and close to that used in
previous work on heterogeneous error detection [8].
The benchmarks we evaluate are given in table II. Where
possible we choose benchmarks similar to those used in
previous work [8]. However, as their scheme has no impact
on the efﬁciency of multicore systems, unlike ours, due to the
prevention of error propagation (section IV), we further add
multi-threaded versions of Parsec [13] applications running
Input
100000000
Single Core Benchmarks
randacc
stream
bitcount
blackscholes
ﬂuidanimate
swaptions
freqmine
bodytrack
Multicore Benchmarks
blackscholes
canneal
ﬂuidanimate
swaptions
TABLE II: Summary of the benchmarks evaluated.
Source
HPCC [25]
HPCC [25]
MiBench [26]
Parsec [13]
Parsec [13]
Parsec [13]
Parsec [13]
Parsec [13]
Source
Parsec [13]
Parsec [13]
Parsec [13]
Parsec [13]
75000
simsmall
simsmall
simsmall
simsmall
simsmall
Input
simsmall
simsmall
simsmall
simsmall
n
w
o
d
w
o
S
d
e
s
l
i
l
a
m
r
o
N
 1.06
 1.05
 1.04
 1.03
 1.02
 1.01
 1
h
c
s
k
c
b l a
Detection
Correction
o l e
s
r a
n
c
a
c
a
d
fl u i d
n i m a t e
b
o
y tr a
d
c
k
fr e
e
q m i n
o
b it c
n t
w
u
s
s
n
p ti o
a
s tr e
a m
e r a
v
a
e
g
Fig. 6: Normalised slowdown for each single core benchmark,
with both detection-only and correction schemes.
on two threads. This required using gem5’s threading module,
m5threads, and thus ARMv7 instead of ARMv8. The subset
of benchmarks chosen are those that run successfully using
m5threads on ARMv7.
VI. EVALUATION
Exploiting heterogeneous parallelism for multicore error cor-
rection incurs performance overheads of just 3.1%. We ﬁrst
look at the impact of the changes required for single core ma-
chines, as described in section III, showing minimal difference
in performance compared to detection only. We then evaluate
the techniques required for correctness on multicore machines.
A. Single-Core Error Correction
Figure 6 shows normalised slowdown for a set of single
core benchmarks, using both error detection [8] and the
basic correction scheme suitable for single core systems from
section III. In effect, the latter is error detection but with extra
data for stores in the load store log, thus reduced maximum
capacity, in-order commit of error-detection checkpoints, and
blocking on IO requests to allow error checking to catch up.
We see that these cause very little extra slowdown compared
with the baseline with no error detection. In many ways this is
unsurprising. Increasing the amount of data in the load store
log slightly increases the frequency of register checkpointing,
208
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:52:36 UTC from IEEE Xplore.  Restrictions apply. 
bitcount
freqmine
stream
fluidanimate
swaptions
bodytrack
blackscholes
randacc
 0
 1000
 2000
 3000
 4000
 5000
Time (ns)
(a) Detection
bitcount
freqmine
stream
fluidanimate
swaptions
bodytrack
blackscholes
randacc
y
t
i
s
n
e
D
y
t
i
s
n
e
D
 0
 1000
 2000
 3000
 4000
 5000
Time (ns)
(b) Correction
Fig. 7: Density plot to show delays between a store commit-
ting and being checked, with error detection and correction,
respectively.
but not enough to have a large performance impact. Because
checkpoints are typically similarly-sized, due to being either an
equivalent number of loads and stores, or an equivalent number
of total instructions if the limit is reached before a load-
store log segment is ﬁlled, error-detection checkpoints usually
commit in order, and so allowing out-of-order commit rarely
improves resource availability. While the workloads we look
at do feature IO operations, high-performance applications
typically buffer these in main memory, and thus there are few
operations where we have to stall and wait for detection to
ﬁnish. Even if the latter were not the case, direct IO operations
are sufﬁciently slow that each small checker core would be
able to keep up with the main core, and thus the lack of
exploitable parallelism is unlikely to affect performance.
Figure 7 shows the delay between an error occurring and
it being found in both schemes. We see that the constraints
introduced for correction actually reduce the delays observed
slightly. This is typically because checkpoints are smaller, due
to the larger amount of stored data, reducing the number
of loads and stores in each checkpoint and causing each
checkpoint to be issued and checked more quickly.
B. Multicore Error Correction
Next we evaluate the additional constraints introduced by
section IV. The most salient of these is the prevention of data
escaping from the L1 cache before it has been checked by a
checker core. First, we evaluate the performance overhead for
single-threaded benchmarks where we assume that data may
be shared with other cores, along with the AIMD technique
(section IV-D) to reduce the performance impact of this, before
evaluating the impact on the cache coherency system using
multithreaded shared memory benchmarks.
1) Single-Threaded Benchmarks: Unless we can be sure
that a program isn’t reading from or writing to data from
other processes running concurrently on other cores, or the
process isn’t locked to a particular core, then we still need
to use the techniques presented in section IV to prevent