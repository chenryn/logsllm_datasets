12
D. Bolzoni, S. Etalle, and P.H. Hartel
2.2 Tests with DSA
We use DSA to validate the general eﬀectiveness of our approach. There are
three factors which inﬂuence the classiﬁcation accuracy, namely: (1) the number
of alerts processed during training, (2) the length of n-grams used, and (3) the
classiﬁcation algorithm selected. This preliminary test aims to identify which
parameter combination(s) results in the most accurate classiﬁcation.
Testing methodology. We proceed with a 3-step approach. First, we want
to identify an adequate number of samples required for training: in fact, a too
low number of samples could generate an inaccurate classiﬁcation. On the other
hand, while it is generally a good idea to have as many training samples as
possible, after some point the beneﬁt from adding additional information could
become negligible. Secondly, we want to identify the best n-gram length. Short
n-grams are likely to be shared among many attack payloads, and the attack di-
versiﬁcation would be poor (i.e., a number of diﬀerent attacks contains the same
n-grams). On the other hand, long n-grams are unlikely to be common among
attack payloads, hence it would be diﬃcult to predict a class for a new attack
that does not share a suﬃcient number of long n-grams. Finally, we analyse how
the classiﬁcation algorithms work by analysing the overall classiﬁcation accuracy
(i.e., considering all of the attack classes) and the per-class accuracy. The two
algorithms approach the classiﬁcation problem in two totally diﬀerent ways, and
each of them could be performing better under diﬀerent circumstances.
To avoid bias by choosing a speciﬁc attack, we randomly select alerts in the
sub-sets. In fact, by selecting alerts for training in the same order they have been
generated (as opposed to random), we could end up with few (or no) samples
in certain classes, hence inﬂuencing the accuracy rate (i.e., a too good, or bad,
value). To enforce randomness, we also run several trials (ﬁve) with diﬀerent
sub-sets and calculate the average accuracy rate. Table 4 reports benchmark
results (the percentage of correctly classiﬁed attacks) for SVM and RIPPER.
Discussion. Tests with DSA indicate that the approach is eﬀective in classifying
attacks. As the number of training samples increases, accuracy increases as well
for both algorithms. Also the n-gram length directly inﬂuences the classiﬁcation.
Table 4. Test results on DSA with SVM and RIPPER. We report the average percent-
age of correctly classiﬁed attacks of ﬁve trials. As the number of samples in the testing
sub-set increases, the overall eﬀectiveness increases as well. Longer n-grams generally
produce better results, up to length 3. SVM performs better than RIPPER by a narrow
margin.
SVM
n-gram length
RIPPER
n-gram length
# samples
1000
2000
3000
1
2
3
4
1
2
3
4
62.6% 76.8% 77.3% 76.7% 66.1% 75.9% 76.2% 75.7%
65.9% 78.6% 78.9% 77.7% 69.4% 76.7% 76.9% 76.4%
66.3% 79.4% 79.6% 78.6% 72.7% 77.2% 77.5% 76.9%
Panacea: Automating Attack Classiﬁcation
13
Table 5. Per-class detailed results on DSA, using 3-grams. We report the average
percentage of correctly classiﬁed attacks of ﬁve trials. RIPPER performs better than
SVM in classifying all attacks, .
SVM
RIPPER
# of samples
# of samples
Attack Class
3000
90.9% 90.5% 90.7% 90.4% 93.9% 94.0%
attempted-recon
79.8% 89.0% 88.8% 97.4% 98.8% 99.1%
web-application-attack
web-application-activity 80.8% 81.2% 80.9% 93.7% 96.1% 95.8%
1000
2000
3000
1000
2000
The number of correctly classiﬁed attacks increases as n-grams get longer, up to
3-grams. N-grams of length 4 produce a slightly worse classiﬁcation, and the same
happens for 1-grams (which achieve the worst percentages). SVM and RIPPER
present similar accuracy rates on 3-grams, with the former being slightly better.
However, if we perform an analysis based on per-class accuracy (see Table 5),
we observe that, although both classiﬁcation algorithms score high on accuracy
level for the three most populated classes, RIPPER is far more precise than SVM
(in once case, the “web-application-activity” class, by nearly 15%).
When we look at the overall accuracy rate, averaged among the 9 classes, for
DSA, SVM performs better because of the classes with few alerts. If we zoom
into the classes with a signiﬁcant number of samples, we observe an opposite
behaviour. This means that, with a high number of samples, RIPPER performs
better than SVM.
In Table 5, a sub-set with fewer samples seems to achieve better results
(although percentages diﬀer by a narrow margin), when considering the same
algorithm. This happens for SVM once when using 1000 training samples
(“attempted-recon” class) and twice when using 2000 training samples (“web-
application-attack” and “web-application-activity” classes). When using 2000
training samples, RIPPER performs best in the “web-application-activity” class.
The reason for this is that alerts in the sub-sets are randomly chosen, thus a
class could have a diﬀerent number of samples among trials.
2.3 Tests with DSB
DSB is used to validate the manual mode and the use of an ad hoc classiﬁca-
tion. To perform the benchmarks, we use the same n-gram length that achieves
the best results in the previous test. Table 6 details our ﬁndings for SVM and
RIPPER.
Discussion. The test results on DSB show that Panacea is eﬀective also when
using a user-deﬁned classiﬁcation, regardless of the classiﬁcation algorithm is
chosen. Regarding accuracy rates, RIPPER shows a higher accuracy for most
classes, although SVM scores the best classiﬁcation rate (by a narrow margin).
Only the “buﬀer overﬂow” class has a low classiﬁcation rate. Both algorithms
have wrongly classiﬁed most of buﬀer overﬂow attacks in the “path traversal”
14
D. Bolzoni, S. Etalle, and P.H. Hartel
Table 6. Test details (percentage of correctly classiﬁed attacks) on DSB with SVM and
RIPPER. RIPPER achieves better accuracy rates for the two most numerous classes,
although by a narrow margin. We observe the same trend for the rates reported in
Table 5.
Attack Class
Path Traversal
Cross-site Scripting
SQL Injection
Buﬀer Overﬂow
Percentage of total attacks
correctly classiﬁed
SVM RIPPER
98.6% 99.1%
97.5% 98.4%
97.6% 96.2%
37.5% 37.5%
98.0% 97.7%
class. This is because (1) the number of samples is lower than for the other
classes, which are at least 10 times more numerous, and 2) a number of the
path traversal attacks present some byte encoding that resembles byte values
typically used by some buﬀer overﬂow attack vectors. In the case of RIPPER,
the “path traversal” class has the highest number of samples, hence no rule is
induced for it and any non-matching samples is classiﬁed in this class.
2.4 Tests with DSC
An ABS is supposed to detect previously-unknown attacks, for which no sig-
nature is available yet. Hence, we need to test how Panacea behaves when the
training is accomplished using mostly alerts generated by an SBS but afterwards
Panacea processes alerts generated by an ABS. For this ﬁnal test we simulate
the following scenario. A user has manually classiﬁed alerts generated by an SBS
during the training phase (DSB) and she uses the resulting model to classify un-
known attacks, detected by two diﬀerent ABSs (POSEIDON and Sphinx). Since
we collected few buﬀer overﬂow attacks, we use the Sploit framework [37] to
mutate some of the original attack payloads and increase the number of samples
for this class, introducing attack diversity at the same time. Thus, we obtain
additional training samples with a diﬀerent payload. Table 7 shows the percent-
age of correctly classiﬁed attacks by SVM and RIPPER. For the buﬀer overﬂow
attacks, we report accuracy values for the original training set (i.e. representing
real traﬃc) and the “enlarged” training set (in brackets).
Discussion. Tests on DSC show that the SVM performs better than RIPPER
when classifying attack instances that have not been observed before. The accu-
racy rate for the “buﬀer overﬂow” class is the lowest, and most of the misclassiﬁed
attacks have been classiﬁed in the “path traversal” class (see the discussion of
benchmarks for DSB). However, with a higher number of training samples (gen-
erated by using Sploit), the accuracy rate increases w.r.t. previous tests. This
suggest that, with a suﬃcient number of training samples, Panacea achieves high
accuracy rates.
Panacea: Automating Attack Classiﬁcation
15
Table 7. Test details (percentage of correctly classiﬁed attacks) on DSC with SVM
and RIPPER. SVM perform better than RIPPER in classifying any attack class. For
the “buﬀer overﬂow” class and the percentage of total attacks correctly classiﬁed we
report (in brackets) the accuracy rates when Panacea is trained with additional samples
generated using the Sploit framework.
Attack Class
Path Traversal
Cross-site Scripting
SQL Injection
Buﬀer Overﬂow
Percentage of total attacks
correctly classiﬁed
SVM
98.1%
92.6%
100.0%
RIPPER
94.4%
88.9%
87.5%
50.0% (75.0%) 25.0% (50.0%)
92.0% (93.0%) 89.0% (91.0%)
2.5 Summary of Benchmark Results
From the benchmarks results, we can draw some conclusions after having ob-
served the following trends:
– The classiﬁcation accuracy is always higher than 75%.
– SVM performs better than RIPPER when considering the classiﬁcation ac-
curacy for all classes, when not all of them have more than 50-60 samples
(DSA, DSB and DSC).
– RIPPER performs better than SVM when the class has a good deal of train-
ing samples, i.e., at least 60-70 in our experiments (DSA and DSB).
– SVM performs better than RIPPER when the class presents high diversity
and attacks to classify have not been observed during training (DSC).
We can conclude that SVM works better when a few alerts are available for
training and when attack diversity is high, i.e., the training alert samples diﬀer
from the alerts received when in classiﬁcation phase. On the other hand, RIPPER
shows to be more accurate when trained with a high number of alerts.
2.6 System Performance
In Section 1.2 we introduce the requirement of a fast training phase for the clas-
siﬁcation algorithm. During our benchmarks both SVM and RIPPER proved
to satisfy such a requirement. As the BF data size is constant (and it is not
related to the n-gram length), the training time depends on the number of alerts
processed. Benchmarks have been performed on a machine with an Intel Core 2
CPU at 1.8Ghz and 2Gb of memory. The reported ﬁgures refer to benchmarks
with DSA, and are averaged values over ﬁve trials. RIPPER is the fastest algo-
rithm and the time required for training grows linearly. When 1000 alerts are
used for training, RIPPER completes the training phase in 8.9 seconds and SVM
in 11.8 seconds. 3000 alerts are processed in 25.9 seconds by RIPPER and in 39.7
seconds by SVM. While retraining, Panacea can use the old classiﬁer instance
16
D. Bolzoni, S. Etalle, and P.H. Hartel
while it builds the new classiﬁer (and the required time is short enough to allow
batch processing of a large number of alerts).
A fast classiﬁcation phase is also desirable, in order to select an appropriate
action to handle any alert as soon as it is raised. We report ﬁgures for benchmarks
with data set DSC, since for DSA and DSB we use the cross-validation testing
(where multiple scans of the set are performed). When 1500 alerts are used for
training, RIPPER classiﬁes 100 alerts in 0.9 seconds and SVM in 1.3 seconds.
Thus, Panacea would be able to process up to 300.000 alerts per hour, a rate
that is hardly seen even in large networks.
2.7 Evaluating Conﬁdence
However good Panacea is, the system is not error-free. The consequences of
a misclassiﬁcation can have a direct impact on the overall security. Think of
a buﬀer overﬂow attack, for which usually countermeasures must take place
immediately (because of the possible consequences), that is misclassiﬁed as a
path traversal attack, for which the activation of countermeasures can be delayed