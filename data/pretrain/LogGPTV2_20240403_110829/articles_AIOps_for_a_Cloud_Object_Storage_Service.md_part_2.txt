engagedindatadiscoveryandanalysis.Theprocessofman- can be enhanced further by partitioning based on the values
agingdata,archivingandrepresentingiscalleddatacuration. of one or more columns. It is crucial to know the common
Datacuratorscollectdatafromdiversesourcesandintegrate queries in order to benefit from by data partitioning. For
it into repositories that are many times more valuable than example, among other fields our schema involves dates, lo-
theindependentparts.Curationalsoensuresdataqualityand cationsandcustomerinformation.Sinceourtypicalanalysis
makes machine learning (ML) more effective. was done on a daily basis, we decided to partition data by
It is hard to overestimate the importance of data cleaning date and location, storing it in an order dictated by dates
and sanity checking. Our sanity checks include row counts, and location columns. When the query is done for specific
timestampsconsistencychecksandstatisticaldatavalidation. dayandlocationonlysmallsub-setofdataisaccessed.This
Inadditiontocounts,itisimportanttovalidatethatstatistical order of partitioning served us well for daily performance
characteristicsarepreserved,e.g.,themeanandmedianval- logsanalysis,butprovedtobeinefficientwhenpercustomer
ues of the important numerical fields. Further data cleaning analysis was requested. In order to access specific customer
and standardization require a unifying format for numerical data, we needed to access whole data set regardless of its
datatoallownumericaloperations,andfillinginmissingdata date and location, losing all the advantages of partitioning.
with appropriate values, e.g., null values of the appropriate Partitioning not only has benefits for performance, it also
type.Anotherimportantaspectofdatacleaningisvalidating helpsmanagingthedata.Sparksupportsprogrammaticparti-
timestamps consistency and correctness, needed to ensure tioningandcandiscoverpartitionsautomaticallyifthestored
that no data is missing, ignored or duplicated. Validating data is already partitioned. To store partitioned data in the
timestamp consistency is a complex task in a heterogeneous most efficient way we performed experiments with Spark to
system.Ourdatasetiscollectedfromdifferentsystemsusing determinethebestwaytowritethedata.Thefollowingwrite
multipleformatsacrossmanytimezones.Inordertovalidate options were evaluated: (1) Each day-location gets its own
timestamp consistency and enable data analysis, we unified dataset.(2)Hive-stylepartitioningusingpartition by.writein
the format and converted all timestamps to UTC. append mode. (3) Hive-style partitioning explicitly writing
In the data lake pipeline, curation prepares the data for each virtual directory in overwrite mode. Our experiments
consumption by analytic tools. Curation transforms the raw showedthatoption1istheslowestone.Options2and3are
data into a structured view, annotates it using metadata, similar in terms of performance and twice as fast as option
combines current inputs with historical data, and integrates 1. The latter two options store the data and access it in a
previously aggregated data. During development, we chose similar way; however, there is a difference between options
to use a staging transformation approach. Staging the data 2and3whenawritejobfails.IfSparkjobfailsinoption2
in Parquet format greatly speeds up the debugging process in the middle of data writing, it cannot be easily reverted as
while developing new analytic methods; it reduces the time the written data becomes part of a big data set with a single
wasted waiting for computations to complete. Moreover, flag for successful writes. As a result retrying the write job
staging simplifies repeatable experiments. We experimented might create duplicate data chunks. On the other hand, with
with several choices before deciding on the right staging option3,ifthewritejobfailswritingavirtualdirectory,the
transformations and format to achieve the best performance. partial results can be easily cleaned by rerunning the job,
ApacheParquetisacolumnardataformat,whichsupports overwritingthespecificdirectory.Therefore,wechosehive-
style partitioning, explicitly writing a virtual directories in
167
Authorized licensed use limited to: University of Guelph. Downloaded on August 10,2023 at 08:59:22 UTC from IEEE Xplore. Restrictions apply.
overwrite mode, as the fastest and safest option. is done using Python packages, such as pandas [24] for
general statistics, and scikitlearn [25] for ML.
C. Feature extraction
In large scale data sets, such as operational log data A. Analytical model: statistical or machine learning (ML)
(e.g. [22]), a big challenge is preparing the raw data for use Inouruse-casesweareabletoprovideanefficientsolution
withstatisticalandMLmethods.ThedatapreparedforMLis using basic statistical and ML methods, and did not need
commonly called ”features”. Moreover, for many statistical deep-learning methods, which are less efficient and require
and ML methods, such as anomaly detection, choosing and moreresources,especiallywhenimplementedatlargescale.
generating the right features is much more important than Statistical approach. Computing statistical invariants of
the actual algorithm. For example, with the proper features thelarge-scaledataset,suchascounts,mean,standarddevia-
basic outlier detection methods can find anomalies (see the tion, histograms, median and other percentiles, is extremely
discussion in [3]). useful to provide an overview of the operations data and
We use Spark [12], [23], which supports the Map/Reduce perform basic reliability and sanity checks. This calculation
programming model, to generate the features efficiently in can also be done efficiently at large scale using the ”smart
one parallel pass over the huge amount of log data. For groupBy” method described above. We perform hierarchical
the development of algorithms and code we use Jupyter aggregation of various metrics in order to detect and focus
notebooks, and for running the code in batch in production on a failed component, as can be seen in Fig. 6, to find the
we use ”spark-submit” to the IBM Analytics Engine [18]. highest possible level of aggregation/hierarchy, representing
We describe our ”smart groupBy” method to generate the the most significant problem currently occurring in the
featuresefficiently,assumingthattheinputdataisinatable system.However,incaseswheretherearetoomanymetrics
format (e.g., Spark DataFrame) and that the processing is and signals, it is not possible to manually handle too many
done using an SQL API (e.g., Spark-SQL). graphs and alerts, so it is better to use automated ML tools.
Map Step Weenrichtheinputdatawithadditionalcolumns: Machine learning approach. There are various anomaly
1)Computingrangesandbuckets:byrangesoftimestamps detection methods, for an extensive survey see [26]. The
(e.g., day/hour/minute), sizes, and other numerical compo- basicapproachisbasedoncalculatingthez-scoreofasingle
nents. metric, namely, the signed fractional number of standard
2)Reducingalargesetofvaluestoasmallersubset(often deviations by which the value x of an observation differs
called data cleaning): by parsing the original values and fromthemetric’smeanvalue.Inadditiontoaunivariateap-
taking only dominant substrings, or taking only the most proach, there exist multivariate anomaly detection methods,
popular subset of values, etc. e.g., [27], which reduce false alerts. We use a multivariate
3)Computingafunctionoverseveralcolumns:bychoosing anomaly detection algorithm on the features generate using
a derived value based on values appearing in the input the ”smart groupBy” method described in Sec. III-C. These
columns. features include aggregations of various latency metrics of
4) Doing derivations and differences: by computing a several components in the system, and our algorithm is in
derived column C, adding a new column C shift, which the spirit of Ng’s algorithm in [28]. Fig. 4 shows a graph
is a copy of C shifted in one cell, and then computing the of our aggregated anomaly score and computed threshold ;
derived column C derived =C−C shift. Fig. 5 focuses on the anomaly inside the rectangle.
5) Computing statistical functions: counts, mean, standard
deviation, ranks, percentiles, etc. B. Root cause and problem isolation
The one pass pipeline combines these transformations into After completing the statistical and ML analysis, we
a single map step. perform feature isolation in order to find the root cause and
Reduce Step We perform one SQL ”group by” operation detect thefailed component.Figs. 5 and 6demonstrate two
onmanycolumnsatonce,includingthecolumnsgenerated equivalent points of view of the same failure that occurred
in the map step, to produce all the features in parallel. in our system at the same time (from time T start to T end).
Inordertochoosethespecificranges,buckets,andsubsets Fig. 6 shows the connectivity point of view. We observe
in steps 1 and 2, we use prior domain knowledge or learn that certain application components were disconnected from
from a small sample of the large data set. The statistical the other components from time T start to T end. In order
functions in step 5 may also be used to check the reliability to detect the most relevant problematic component for an
ofthegeneratedfeatures,forexample,checkingwhetherthe alert, we reduced the problem of determining the failing
numberofitemsperfeatureisstatisticallymeaningful.Since components to a hierarchical flow problem. This approach
Spark’scalculationislazy[12],itisentirelydoneon-demand allowsustopinpointtheproblematthemostrelevantlevelof
in the ”Reduce step” to obtain the features. We perform the aggregation/hierarchy, and notify the operations team of the
ML algorithm or statistical analysis on these features. most significant problem currently occurring in the system.
Fig. 5 shows the performance view. Our multivariate
IV. ANALYTICSANDINSIGHTS
anomaly detection tool on the latency metrics shows a high
Inordertogaininsightweperformstatisticalanalysisand peakattimeT startthatstaysabovetheanomalythresholdfor
create MLmodels usingthe extracted features.Our analysis thesameperiodfromT start toT end.Inthiscase,weisolate
168
Authorized licensed use limited to: University of Guelph. Downloaded on August 10,2023 at 08:59:22 UTC from IEEE Xplore. Restrictions apply.
3 35 05 0 100000 100000 1 8900 00 0000000 erocS erocS stnenopmoc
2 2 11 00555 0 055 0 05 8 6 4 20 0 0 00 0 0 TA hn ro e TTTm s iiih mmmmma ol ly eeeeeed 8 6 4 20 0 000 0 0 TTTEENNDDTA hn ro em sha ol ly d 7 6 3 145 000 0 000 000 0 0 000 )BG( ezis teuqraP ycnetaL etagerggA ycnetaL etagerggA detcennoc
0 0 TTTSSTTAARRTT 2 0 fo %
0 50 Ra1 w00 JSON1 5 lo0 gs co2 m00 press2 e5 d0 (GB)300 350 TSTARTTEND Time TSTART TTTiiimmmeee TEND
Fig. 5. Anomaly score of the Fig.6. Connectivitypointofview:
Fig.3. SizeratioParquetvs.Raw Fig. 4. Anomaly score of the
aggregatedlatency–zoomin disconnectedcomponent
JSONLogsIndexCompressed aggregatedlatency
Iterate and refine as you go. In an ideal world, one
the problem and focus on the misbehaving component by
assumes sufficient and reliable data sources and specific
indicating the top features with the highest z-scores during
questionstoguideAIOpsexploration.Inreality,however,the
this anomalous period, and checking how many of them
data sources often are not intended for analytical purposes
come from the same component.
andaskingtherightquestionsisasignificantchallenge.Use
C. Visualizations and dashboards aniterativeapproachwherebyinsightsaregainedincremen-
Inordertoprovidealertsandreportstotheoperationteam, tally, using snapshots, historical data, and calibration.
onecanusestaticreportingtoolsthatmainlypresentperiod- Feed and sustain the engagement. Last but not least, do
ical graphs and textual reports. Another option is presenting not underestimate the importance of mutual trust between
semi-static tools, which allow interactive exploration of the IT system owners and the AIOps team. System owners
pre-calculated results and support drill down and zoom-in are often overwhelmed by their day-to-day load and if not
at problematic points, such as Grafana [13]. Moreover, such seeing immediately useful results, may disengage and lose
semi-statictoolsallowtoincorporatealltheproducedgraphs interest. Great cross-team collaboration is key to putting in
into a few dashboards. However, such tools do not cover place an AIOps system alongside the production service in
all types of required insights and graphs. For example, in a reasonable amount of time.
our use-cases a connectivity matrix heatmap turned-out to
REFERENCES
be the most useful tool for presenting failed component in
[1] A. Lerner, “AIOps Platforms,” https://blogs.gartner.com/andrew-
the context of overall system.
lerner/2017/08/09/aiops-platforms/.
Inadditiontothegraphicaltoolsanddashboards,whenan [2] B.H.Sigelmanandetal.,“Dapper,alarge-scaledistributedsystems
action of an operator is needed, it is necessary to provide a tracinginfrastructure,”Google,Inc.,Tech.Rep.,2010.
[3] D.GoldbergandY.Shan,“Theimportanceoffeaturesforstatistical
direct alert or real-time notification via tools like slack [14].
anomalydetection,”inUSENIXWorkshop,HotCloud’15.
Forexample,ourautomatedtoolprovidesaslacknotification [4] M.Chowandetal.,“Themysterymachine:End-to-endperformance
to the operator, containing the identity of the specific appli- analysisoflarge-scaleinternetservices,”inOSDI’14.
[5] “Introducing atlas: Netflix’s primary telemetry platform,”
cationornetworkcomponentexperiencingissue.Inaddition
https://medium.com/netflix-techblog/introducing-atlas-netflixs-
to identifying the specific troubled component, the context primary-telemetry-platform-bd31f4d8ed9a.
oftheissueisprovidedincludingexacttimeoftheevent,its [6] “Moogsoft,”https://www.moogsoft.com/.
[7] “OpsRamp,”https://www.opsramp.com/.
geographical location, an estimation of the problem severity
[8] “Zenoss,”https://www.zenoss.com.
and the list of the additional components affected by this [9] J. K. Resch et al., “Aont-rs: Blending security and performance in
failure. It is important to provide the context of the event in dispersedstoragesystems,”inFAST’11.
[10] “IBM Cloud Object Storage System Logs,” www.ibm.com/support/
order to assist the operator to discover the root cause of the knowledgecenter/STXNRM\3.14.1/coss.doc.
problem and act quickly to restore normal system behavior. [11] “Elasticsearch,”https://www.elastic.co/products.
[12] M.Zahariaandetal.,“Resilientdistributeddatasets:Afault-tolerant
V. CONCLUSIONS abstractionforin-memoryclustercomputing,”inNSDI’12.
[13] “Grafana,”https://grafana.com/.
We have presented an AIOps solution that provides in-
[14] “Slack,”https://slack.com/.
sightsusefulfortheoperationoftheIBMCOSservice.The [15] “ApacheHadoop,”https://hadoop.apache.org/.
solution is now in the process of being globally deployed [16] “ApacheParquet,”https://parquet.apache.org/.
[17] “ApacheZeppelin,”https://zeppelin.apache.org.
across multiple service offerings in IBM Cloud and will
[18] “IBMAnalyticsEngine,”https://console.bluemix.net/catalog/services/
be further refined, optimized, and extended, e.g., to work analytics-engine.
with more cloud services and for cross-service operations. [19] “ProjectJupyter,”https://jupyter.org/.
[20] “IBM Watson Studio,” https://console.bluemix.net/catalog/services/
To conclude the paper we share a concise summary of the
watson-studio.
mostimportantlessonsandbestpracticesthatcanilluminate [21] “Logstash,”http://www.elastic.co/products/logstash.
the path to success for others who pursue similar goals. [22] J. Barr, “Amazon s3 – two trillion objects, 1.1 million requests per
second,”AWSNewsBlog2013.
Know your tools. There are multiple applicable tools and
[23] “ApacheSpark,”https://spark.apache.org/.
methods and it is very important to have good understand- [24] “Pandas:PythonDataAnalysisLibrary,”https://pandas.pydata.org/.
ing of the suitability, efficiency, and compatibility of these [25] “scikit-learn:MachineLearninginPython,”https://scikit-learn.org/.
[26] V. Chandola and et al., “Anomaly detection: A survey,” ACM Com-
methodsin thecontext ofa particularoperational challenge.
putingSurveys,Jul.2009.
Keep it simple. This timeless wisdom is your best friend [27] H.Chengandetal.,“Detectionandcharacterizationofanomaliesin
when developing AIOps. The appeal of advanced analytics, multivariatetimeseries,”SIAMInt.Conf.onDataMining’09.
including machine and deep learning, is so great that many [28] A. Ng, “Anomaly detection using the multivariate gaussian distribu-
tion,”inMachineLearningYearning,2018.
fall in the trap of needless over complication.
169
Authorized licensed use limited to: University of Guelph. Downloaded on August 10,2023 at 08:59:22 UTC from IEEE Xplore. Restrictions apply.