Implementation. We implement DRMI with 2.5K lines of
Python on top of PYTORCH [3]. The adversarial examples are
evaluated by the targeted PGD [37] method using foolbox [4]
library. The experiments are conducted on a server with 16
Intel(R) Xeon(R) CPUs of E5-2620 and 32GB memory, 2
NVIDIA GM200 [GeForce GTX TITAN X] GPUs and 1 AS-
PEED Video AST2400 GPU. These experiments are carried
out to evaluate the efﬁciency and efﬁcacy of DRMI. Through
these experiments, we intend to answer:
RQ1. How effective is DRMI to reduce data for training?
RQ2. How does it facilitate black-box attacks?
RQ3. How is other reduction strategies, and what can be
interpreted from the results?
5.1 Experiment Setup
Experiment Data. We conduct our experiments on
MNIST [35], CIFAR10
[48]
(ILSVRC2012) datasets. The MNIST dataset contains
60,000 training images of 10 classes and 10,000 test ones. Its
samples are 28×28 grey-scale images of handwritten digits.
CIFAR10 contains 50,000 training samples of 10 classes
and 10,000 test data. Its samples are 32×32 RGB images.
ImageNet contains about 1,200,000 training data, 100,000
[32], and ImageNet
Table 1: Parameters of the C3F2 model
Layer Name
Output Dimensions
Input
Convolutional layer
Convolutional layer
Max-Pooling layer
Convolutional layer
Max-Pooling layer
Fully connected layer
Fully connected layer
1 * 28 * 28
16 * 24 * 24
32 * 20 * 20
32 * 10 * 10
64 * 6 * 6
64 * 3 * 3
100
10
Table 2: Parameters of the DNN5 model
Layer Name
Output Dimensions
Input
Fully connected layer 1
Fully connected layer 2
Fully connected layer 3
Fully connected layer 4
Fully connected layer 5
784
512
256
128
64
10
test data, and 50,000 validation data of 1,000 classes. Its
samples are 224×224 RGB images. We train the substitute
model on a simpliﬁed training dataset and test model on the
test dataset.
Target Model. We select LeNet-5 [34], C3F2 and DNN5
model structures on dataset MNIST. We adopt model
ResNet18 [23] on dataset CIFAR10, and Inception-v3 [56],
ResNet152 [23] on ImageNet. LeNet-5 is an efﬁcient convolu-
tional neural network for handwritten character recognition. It
includes 2 convolutional layers, 2 pooling layers, and 3 fully
connected layers. Table 1 shows C3F2’s model architecture.
It has 3 convolutional layers, 2 pooling layers, and 2 fully
connected layers. Table 2 details DNN5’s model architecture.
It has 5 fully connected layers and no convolutional layer.
ResNet is a residual network, which is used for more complex
image classiﬁcation.
Experiment Conﬁguration. When training models on a sim-
pliﬁed dataset, we set batch size to 4 on MNIST and 64 on
CIFAR10. We use max-pooling in pooling layers, cross en-
tropy loss to calculate losses. By default, we take adaptive
moment estimation (Adam) as the optimizer and set the learn-
ing rate to 0.001. Our data selection is carried out under the
same label. That is, we determine a simpliﬁed dataset for each
category, and then glue them together into the training dataset
for our experiments.
Baseline Method. We implement a baseline method in this
paper to show to what extent our approach can raise in data re-
duction. In the baseline method, we randomly select a speciﬁc
number of samples without any intelligence. Taking MNIST
as an example, we select samples for each digit proportionally
and randomly, and then train a substitute model as well as
measuring its accuracy. This process is repeated for ﬁve times
and the result is averaged in a comparison.
Manual Reduction Method. To verify whether our approach
can excel manual efforts in data reduction, we invite two vol-
USENIX Association
30th USENIX Security Symposium    1907
Table 3: Evaluations of model LeNet-5 on dataset MNIST.
“Test Accuracy” means the substitute model accuracy on the
test dataset. The optimal LeNet-5 model performance trained
on the full dataset (60,000 data) reaches 99.17% accuracy.
“Queries” is the number of queries to the original model, also
the size of simpliﬁed set.
Method
DRMI (min-sum)
DRMI (min-max)
manual reduction
baseline
α
1
2
4
1
2
4
-
-
Queries = 600
95.59%
95.84%
96.38%
95.52%
96.01%
96.41%
94.65%
91.91%
Test Accuracy
Queries = 300
93.74%
94.29%
94.09%
91.99%
93.49%
94.14%
92.46%
88.48%
Queries = 150
88.01%
92.13%
91.35%
87.07%
90.15%
91.99%
86.57%
84.97%
Table 4: Evaluations of model ResNet18 on dataset CIFAR10.
The original ResNet18 model trained on the full dataset
(50,000 data) obtains 93.90% accuracy. “AD Size” is the
dataset size of attackers can get. “Queries” is the number of
queries to the original model, also the size of simpliﬁed set.
AD Size
25,000
Queries
10,000
4,000
1,000
500
Test Accuracy
DRMI (min-sum & α=2)
92.50%
89.74%
82.28%
73.46%
baseline
80.05%
72.28%
55.72%
44.58%
unteers with normal eyesight and intelligence to collectively
select typical and non-repetitive images from the MNIST
dataset. If two images look similar in appearance, or are mirror
symmetry, we remain only one image. The manually selected
data will be tested and measured for comparison.
5.2 Effectiveness of Data Reduction
To answer RQ1, we train a substitute model on the simpliﬁed
dataset with DRMI, and compute the accuracy and loss value
of the model on the test dataset. The effectiveness of data
reduction is evaluated threefold: different datasets, which we
used to guide the optimization; different reduction degrees, to
which we simpliﬁed the training data, i.e., with only 1% or
even 0.1% of the original data, and; different target models,
to evaluate whether DRMI is widely applicable.
5.2.1 Different Datasets
Here we test different parameters and solutions in DRMI
on different datasets. In Equation 2, we introduce α for MI
value. When α is larger, a larger MI value has a greater effect
on the result, but also means a larger penalty. Here we select
α = 1,2,4. We also adopt two initial solutions “min-sum” and
“min-max” (see Algorithm 2) to evaluate different solutions.
Table 3 evaluates substitute models when adopting different
parameters and solutions under the LeNet-5 model architec-
Table 5: Evaluations on ImageNet. The original Inception-
v3 model reaches 94.5% top-5 accuracy and 79.2% top-1
accuracy. The original ResNet152 model reaches 94.0% top-5
accuracy and 78.8% top-1 accuracy. We adopt Inception-v3
and ResNet152 structures as substitute models respectively.
Here we adopt the “min-sum” method and choose α = 2. “AD
Size” is the dataset size of attackers can get. “Queries” is the
number of queries to extract a substitute model, also the size
of simpliﬁed set.
Inception-v3
Top-5 Acc.
ResNet152
Top-1 Acc.
Top-5 Acc.
Top-1 Acc.
AD Size
200,000
50,000
10,000
baseline
Queries
100,000
50,000
20,000
10,000
5,000
2,000
100,000
90.6%
87.7%
82.3%
77.0%
72.5%
61.8%
73.8%
73.9%
68.5%
63.8%
57.9%
48.4%
40.2%
52.7%
90.2%
87.2%
80.8%
76.7%
71.7%
60.5%
72.2%
73.6%
68.7%
62.9%
57.4%
47.8%
39.1%
51.5%
ture. It is observed that the model gets the highest prediction
accuracy when α = 4 under 600 samples, and α = 2 under
300 and 150 samples. α = 1 performs worst on all sizes and
algorithms. This indicates that we need to impose a heavier
penalty on the larger MI value. Moreover, the two methods
“min-sum” and “min-max” perform almost the same. Com-
pared to the manual reduction method, DRMI has raised the
accuracy by 1.76%, 1.83%, and 5.56% on 600, 300, and 150
sized samples. Compared with the baseline method, our meth-
ods have greatly raised the accuracy by 4.50%, 5.81%, and
7.16% on 600, 300, and 150 sized samples, respectively. Since
the upper limit of test accuracy is 99.17%, our methods have
improved the baseline method by 62%, 55%, and 51% on
600, 300, and 150 sized samples respectively in the whole
improvable space.
Remark: From the experiments with varying parameters,
it concludes that a higher power α for mutual information
(i.e., greater penalties for large MI values) leads to a better
reduction, where our two initial solutions both perform well.
All of our best methods improve more than 50% from the
baseline method within the improvable space.
We choose model ResNet18 to train substitute models on
CIFAR10 and present the results in Table 4. Here we select
the “min-sum” method and α = 2. CIFAR10 images are more
complex, so the training effect decreases. When querying
10,000 data, DRMI achieves 92.50% accuracy, only 1.40%
gap to reach the original model. We also achieve 89.74%,
82.28%, and 73.46% accuracy with 4,000, 1,000, and 500
query, improving 17.46%, 26.56%, and 28.88% accuracy than
the baseline method respectively. This shows that DRMI also
works effectively on the CIFAR10 dataset.
Table 5 evaluates DRMI on the ImageNet dataset using
Inception-v3 and ResNet152 models. When we use a simpli-
ﬁed set with 100,000 data (8.3% of the target training set),
DRMI still reaches 90.6% top-5 accuracy and 73.9% top-1
accuracy, while 100,000 random queries in baseline only gets
1908    30th USENIX Security Symposium
USENIX Association
Table 6: Evaluations when attackers only obtain limited data. The target model is trained on MNIST. Attackers get some data
which matches the distribution of the target dataset (MNIST) in the left part, and obtain data from USPS (7291 data in total)
which does not match the distribution in the right part. Here we use “min-sum” method and choose α = 2. “AD Size” is attackers’
dataset size. It means how many samples attackers can get. “150” means the attacker chooses 150 representative samples from
his dataset using DRMI. “ALL.” means attackers query the target model for all their data, which consumes lots of query overhead.
The complete training dataset has 60,000 data. “Test Acc.” means the substitute model accuracy on the test dataset.
Test Acc. under different size of simpliﬁed set on MNIST
Test Acc. under different size of simpliﬁed set on USPS
AD Size
60,000
10,000
5,000
2,000
1,000
600
600
95.84%
95.57%
94.83%
94.67%
94.50%
-
300
94.29%
93.01%
92.40%
92.05%
91.76%
91.06%
150
92.13%
90.85%
90.51%