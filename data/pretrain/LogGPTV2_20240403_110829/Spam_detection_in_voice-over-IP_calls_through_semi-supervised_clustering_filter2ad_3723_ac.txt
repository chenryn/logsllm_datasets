311
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:59:55 UTC from IEEE Xplore.  Restrictions apply. 
 implementation for the neighborhood creation process in the cluster initialization step of MPCK-Means. Additionally, the general practical experience with a K-Means based algorithm is that it converges within a small number of iterations for the main loop (Step 2 in MPCK-Means). Combined these make eMPCK-Means O(N) and the constant is small for a range of VoIP call traces. 4.5.1 eMPCK-Means : Initialize clusters The eMPCK-Means algorithm creates the initial neighborhoods directly from the user feedback FS and FN sets. Specifically, it creates w neighborhoods {FS, FN , xn3, xn4, …, xnw}, where  {xn3, xn4, …, xnw} = X-FS-FN is the set of data points not covered by the user feedback. The complexity of this step is O(N). We use the same weighted-farthest-first traversal as in MPCK-Means, which is O(N) when the number of clusters is a constant. Overall, the initialize clusters in eMPCK-Means has O(N) complexity. 4.5.2 eMPCK-Means : efficient estimation of maximally separated points ()''',hhxx In MPCK-Means, to find the exact maximally separated points ()''',hhxx used in Eq. (4) and Ah matrix updating[6], it requires evaluating the distance 2Ahijxx−for every pair of points (xi, xj)∈X, which is an O(N2) operation. Since the matrix Ah is updated in each iteration of the loop of step 2 in Algorithm 1, this evaluation has to be repeated as well.  In eMPCK-Means, we estimate the maximally separated points by first putting data points from X into an array R[1..N] in a random ordering. We then iterate through consecutive elements R[i] and R[i+1] in the array. We set ()''',hhxx to (R[i’], R[i’+1]) that gives the maximal value of 2AR[']R['1]hii−+. This operation (Step 2 in Algorithm 2) is performed once right after the cluster initialization step and is done K times, once for each cluster h. The time complexity of this step is O(N). However, since the Ah matrix is updated in each iteration of MPCK-Means (Step 2.3, Algorithm 1), the estimate ()''',hhxxhas to be updated accordingly as well. We embed the updating process into the calculation of the parameterized Euclidean distance 2Ahijxx− (Eq. (2)). The parameterized Euclidean distance is calculated in Eq. (3) and Eq. (4) as well. The idea here is that when a pair of points (xi, xj) is found to have a greater distance than the current estimate()''',hhxxat the time of evaluating the parameterized Euclidean distance, we will set the maximally separated points estimate to (xi, xj). The advantage of this approach is that it is an O(1) operation and does not increase the order of complexity of eMPCK-Means. However, this is an approximation because suppose, in the loop to iterate through all the points, we are at point xA and are calculating ||xA-xB||2. The point xC is to be considered in a later iteration and (xA, xC) happens to be the farthest pair of points. Then, the computation for point xA will not have the accurate distance for the farthest pair of points. Hereafter, when we refer to Euclidean distance computation, we mean that it has maximally separated point estimation embedded within it.  To insure that fC(.) function (Eq. (4)) does not evaluate to negative values with our approximated estimation of ()''',hhxx, we enforce that the second term is always evaluated before the first term so that there is an opportunity to update ()''',hhxx. 4.5.3 Use only a fixed number of constraints in cluster assignment step In the cluster assignment step of MPCK-Means (Step 2.1,  Algorithm 1), rather than iterating through the complete must-link/cannot-link peers of xi, which makes Step 2.1 O(N2), we choose a fixed-sized subset of them. This corresponds to Step 3.1 in eMPCK-Means. This optimization is hinted at by the fact that the must-link/cannot-link information in our domain has significant redundancy. A set of k1 and k2 calls placed, through user feedback, in the SPIT and non-SPIT categories generates k12+k22 must-link and k1k2 cannot-link constraints. On the other hand, we see from experimental results in [6] that MPCK-Means can work reasonably well even with a limited numbers of constraints. The cluster assignment step thus becomes O(N). In general, this can negatively affect the clustering quality. However, we believe it is a trade-off that is necessary in an effort to make the detection scheme scalable.  4.5.4 Pre metrics update on the starting cluster(s) In MPCK-Means, the first update metrics step (Step 2.3) occurs only after the first iteration of the cluster assignment step (Step 2.1). In the first iteration of the cluster assignment, a default identity matrix is assigned to Ah, which directly affects the quality of the generated clusters from the first iteration and has a long-term effect on the quality of the eventual clusters as we see empirically. Therefore, in eMPCK-Means we conduct a metrics update (Step 1.2, eMPCK-Means, Algorithm 2) early on, right after the initial clusters are generated from the cluster initialization step. Intuitively, the user feedback is available at the outset and this optimization allows the Ah matrix to immediately adapt to the user feedback, which results in more accurate clustering. Additionally, it improves the convergence speed as we see later (Table 1).  Input: Set of data points {}1NiiXx==, Set of must-link constraints (){},ijMxx=, Set of cannot-link constraints (){},ijCxx=, Number of clusters K, Sets of constraints costs W and W, 978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
312
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:59:55 UTC from IEEE Xplore.  Restrictions apply. 
 Optional initial cluster centroids {}(0)1Khhμ=, 0t← Output: Disjoint K-partitioning {}1KhhX=of X such that objective function mpckmτ is locally minimized. Method: 1. If initial cluster centroids {}(0)1Khhμ= is not given in the input 1.1. Create the λ neighborhoods {}1PPNλ= with steps from Sec. 4.5.1. if Kλ≥ Use weightiest farthest-first traversal to select K neighborhoods{}()1KPhhN=. Assign the data points   {}(0)()1KhPhhXN=← Initialize {}(0)1Khhμ= Else {}(0)1hhhXNλ=← Initialize remaining clusters at random Initialize {}(0)1Khhμ= 1.2. Update metrics Ah for all clusters {}1KhhX= ([6]). 2. Initialization of maximally separated points ()''',hhxx with respect to each Ah.  3. Repeat until convergence 3.1. For each ixX∈  Randomly select{}{}(,) , (,) , ijsizeijsizeMxxMMctsCxxCCcts∈∈=∈∈=. *h=()()(2()AargminlogdetAhtihhhxμ−− ()())(,)(,),1,1ijijxxMxxCijMijjijCijjwfxxhlwfxxhl∈∈⎡⎤⎡⎤+≠+=∑∑⎣⎦⎣⎦ Assign ix to *1thX+ 3.2. For each cluster Xh, {()1(1)1thttXhhxXxμ+++←∈∑} 3.3. Update_metrics Ah for all clusters {}1KhhX= ([6]) 3.4. 1tt←+ Algorithm 2. eMPCK-Means Algorithm 2 shows the proposed eMPCK-Means with the above modifications to MPCK-Means. Step 1 decides the starting K centroids (means) for the clusters through the use of initial user feedback. For the specific case of the user flagging calls as SPIT or non-SPIT, K=2.  Step 2 initializes the maximally separated points estimation. Step 3.1 performs the cluster assignment. Step 3.2 updates the mean. Note that the mean can be updated in constant time by keeping the sum of the data points and performing an addition/subtraction when a data point is associated with/unassociated from a cluster. Step 3.3 updates the matrix Ah for each cluster h. The goal of this process is to pick Ah’s such that the objective function (Eq. (1)) is minimized for the cluster assignment done in the current iteration of Step 3. Conceptually, this process will result in Ah’s that puts higher weights on those features which are consistent among data points in the same cluster and lower weights on those that are less consistent.  4.6 Progressive MPCK-Means  The eMPCK-Means algorithm assumes that the data points are available in a batch, and is thus suited for Mode B (batch mode) detection (Sec. 3.3). To support Mode A per-call early detection, we create a variant called progressive MPCK-Means (pMPCK-Means). The pseudo code is given as Algorithm 3. The idea here is that when a new call comes in, pMPCK-Means performs only the cluster assignment step and only for the new data point. The features “From URI”, “To URI”, “Start time”, and “Time from the last call by the same caller” are available at the beginning of the phone call and are used in pMPCK-Means. For the features that are not available, pMPCK-Means fills the data point xi with the mean values from the cluster to which this point’s distance is being computed. This is implicitly carried out in Step 4 of Algorithm 3.  In pMPCK-Means, the update metrics operation only occurs occasionally when the cluster means have changed significantly (exceeding a given threshold dthreshold). Estimating the mean is an O(1) operation for each new data point. This amortizes over many calls the cost of Ah computation and the cost of re-clustering all existing data points. However, a cost has to be paid in advance, which is that we require reasonably sized cluster(s) to be grown on the initial data points (|X| > tthreshold) through eMPCK-Means. The reason is that we want the initial Ah matrix to be as accurate as possible.  Algorithm: pMPCK-Means Input: A new data point xt. , Disjoint K-partitioning {}(1)1KthhX−=of {}(1)121,,..,ttXxxx−−=.  Output: The cluster association lt for the point xt. Disjoint K-partitioning {}()1KthhX=of {}()121,,..,,tttXxxxx−=. Internal Variables: (cid:109){}1Khhμ= Method: 1. If t   978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE
313
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 09:59:55 UTC from IEEE Xplore.  Restrictions apply. 
 /*  x′h*, x′′h* are the maximally separated points wrt Ah*  */ Call eMPCK-Means with initial centroids {}()1Kthhμ= to generate {}()1KthhX= on ()tX; (cid:109){}()1Kthhhμμ=←. Algorithm 3. pMPCK-Means  4.7 Multi-Class eMPCK clustering  We create a variant of eMPCK in which the initial clusters are split into sub-clusters based on the call types “calls going to voice mail”, “calls terminated immediately after the call is established”, and “the remaining calls”. These three types exhibit different patterns in the non-silence call duration ratio (feature 17, Sec. 4.2). The sub-clusters are formed for both SPIT and non-SPIT calls. This is an attempt to guide the clustering process through expert knowledge. The user feedback however is only able to differentiate between SPIT and non-SPIT calls, and not place a call into a sub-cluster.  5. Experiments and Results  5.1 Testbed  We set up a two-domain testbed with a topology similar to Figure 1, one of the domains being protected by our detection technique. We use Asterisk as the VoIP proxy servers and MjSip for the phone clients. Each domain has 90 phones acting as non-spitters and 6 phones acting as spitters. We use the Poisson distribution to model call arrival times and the Exponential distribution to model call durations.  The generation of call traces was done by only one of the co-authors without providing any information about the nature of non-SPIT and SPIT calls to the rest of the team. This was done by design so that the team working on the detection system does not have any prior knowledge of the call mix. Ideally we would have liked to perform the evaluation on third-party call traces. However, at the time of writing, no such call trace is publicly available.  5.2 Summary of call trace dataset  We collected four call traces from our testbed with varying call characteristics as follows (call trace name, Non-SPIT Call length average, Non-SPIT Call inter-arrival time average, SPIT Call length average, SPIT call inter-arrival time average, Number of SPIT calls in trace, Number of non-SPIT calls in trace): (v4, 5, 30, 1, 2, 212, 171), (v5, 5, 10, 1, 10, 45, 338), (v6, 5, 30, 1, 10, 94, 289), (v7, 5, 30, 5, 10, 81, 302). The time unit is minute. In terms of similarity between SPIT and non-SPIT calls, in decreasing order, the call traces are v5, v7, v6, and v4.  There are other characteristics which are shared by the four call traces. Examples include a 60% chance of a call being hung up by the caller for a non-SPIT call and a 10% chance of being hung up by the caller (spitter) for a SPIT call. The media streams for a SPIT call are dominated by the spitter while for a non-SPIT call, the non-silence duration on the caller and the callee media streams are about the same on average. Other experimental parameter settings are: at most 15 must-link and 15 cannot-link constraints are used. The pMPCK-Means algorithm uses 100 data points initially with eMPCK-Means before commencing incremental operation. Each data point in the experiment is based on the average from 50 runs with the same parameter settings.  5.3 Effect of proportion of user feedback   We evaluate the effect of the proportion of calls that come with user feedback. We assume the same ratio for both SPIT and non-SPIT calls. We assume the feedback is perfectly accurate. Figure 2 shows the clustering quality with respect to four different algorithms proposed on call trace 4 in terms of the F-Measure [6]. A larger F-Measure value means better quality clustering. From Sec. 5.2, we know that call trace 4 exhibits a very clear distinction between SPIT and non-SPIT calls in terms of call duration and call inter-arrival time. This makes eMPCK perform well with user feedback ratio as low as 0.1. The original MPCK-Means achieves the same level but with a higher user feedback ratio of 0.2. The improved result of eMPCK is due to the pre-metrics update (Sec. 4.5.4), which creates a more accurate weight matrix A based on user feedback, prior to iterating over the data points. The F-Measure from eMPCK Multi Class drops with increasing user feedback ratio because we break the cluster into sub-clusters based on the call types. As a result, eMPCK Multi Class will put different types of SPIT and non-SPIT calls into different sub-clusters. Both will hurt the F-Measure since by definition of F-Measure, these calls should be clustered into the same cluster. This negative effect grows stronger as the user feedback ratio increases.  Figure 3 and Figure 4 show the true positive (TP) and false positive (FP) rates of SPIT detection on call trace v4. What we can see here is that eMPCK Multi Class actually performs well despite the poor F-Measure. eMPCK Multi Class  performs worse than eMPCK at low user feedback ratio because breaking the initial cluster into sub-clusters reduces the number of call data points with feedback in each sub-cluster. This results in poor clustering and hence low detection accuracy. Compared to eMPCK, MPCK’s detection accuracy lags behind due to the lack of pre-metrics updating. pMPCK performs rather poorly even with call trace v4. However, it is still in the usable range (e.g. 0.63 True Positive with a user feedback ratio of 0.2). pMPCK’s poor performance is due to the limited features available before the media stream  is established. 978-1-4244-4421-2/09/$25.00 c(cid:13)2009 IEEE