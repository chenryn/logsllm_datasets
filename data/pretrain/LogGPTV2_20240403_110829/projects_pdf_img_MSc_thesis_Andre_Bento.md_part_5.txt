Query Language);
Has paid version with
high price tag.
From Table 2.3 we can notice that the state of the art for GDB is not very pleasant.
Interest for this type of databases has began in the later years due to artificial intelligence
and machine learning trends, therefore, the offer presented in the field are limited.
Back in time, when social network tendency emerged, the development of this type of
databasesraised, andthemostpowerfultechnologiesforgraphstoragewheredevelopedin
closed source. One example is Facebook TAO database presented in Table 2.3, a database
developed by the company to support the entire social network, storing users in nodes and
their relationships in edges. This database is described by having very low latency, which
stands for high response time, however, very few information regarding this tool can be
found – just some scientific papers [42], [43].
21
Chapter 2
The remaining tools presented are available for usage. ArangoDB has multi data-type
support, which means that a wider type of data structures are supported for storing in
nodes and edges metadata. Also, it supports scalability through cluster deployment, how-
ever,thisfeatureisonlyavailableinpaidversions–ArangoSmartGraphsstorageimproves
the writing of graph in distributed systems environment [44]. The biggest disadvantage of
this database is the high learning curve associated with the usage of AQL (Arango Query
Language), however, this disadvantage can be surpassed by using provided API clients
with the trade-off of loosing some control.
Neo4J is the most accepted GDB by the open source community. This GDB has
increased in popularity in the past years due to simplicity and easy support [45]. Trade-
offs from this database consists in lack of support for scalability, which means that this
database can only run on a single-machine environment, however, there are some users
reporting that they were able to perform implementations and surpass the lack of support
for horizontal scaling, but this is not tested [46].
Choosing a graph database can be hard because these tools are growing and the ten-
dency for changes in features and tooling support is very high, however, the decision falls
on the question of easy of usage and horizontal scalability. This means that ArangoDB is
adatabasemoreadvisedforbigprojects, wherethesizeofgraphstostoremaysurpassthe
limitofasingle-machine, andNeo4Jforsimplerprojects, wherethefocusarefunctionality
testing and prototyping, and graph storage represents a side concern.
Next Subsection 2.2.4 - Time-Series Database Tools covers the state of the art for
tooling capable of storage values based in time.
2.2.4 Time-Series Database Tools
In this Subsection, tools for storing time-indexed series of values are presented. This
typeofdataisaneedforthisresearchduetothetightrelationbetweendistributedtracing
and time, as explained in Subsections 2.1.3 and 2.1.5. Also, service dependency graphs,
as a representation of the system at a given time, can contain valuable information for
monitoring Microservice systems. For this purpose, Time Series Database (TSDB) are
databases capable of storing time-series based values.
A TSDB is “A database optimised for time-stamped or time-series data like arrays of
numbers indexed by time (a date time or a date time range)” [47]. These databases are
natively implemented using specialised time-series theory algorithms to enhance their per-
formance and efficiency, due to widely variance of access possible. The way this databases
use to work on efficiency is to treat time as a discrete quantity rather than as a continu-
ous mathematical dimension. Usually a TSDB allows operations like create, enumerate,
update, organise and destroy various time-series entries in short access times.
This type of database is growing in usage and popularity because of Internet of Things
(IoT)trend. Discussionsinthisareahaveincreasedoverthepastfewyears,andisexpected
thatitkeepsincreasing,duetoUbiquitousComputing–Raiseofomnipresentanduniversal
technologies. Atthesametime, TSDBgrowswiththisIoTtendency, because datamining
from sensor spread geographically and sensors gather information through measurements
in specific points in time. This information are usually stored in TSDB [48].
22
State of the Art
Table 2.4 presents a comparison between two TSDB: InfluxDb and OpenTSDB.
Table 2.4: Time-series databases comparison.
InfluxDB [49] OpenTSDB [50]
Description An open-source time-series AdistributedandscalableTSDB
database written in Go and op- written on top of HBase;
timised for fast, high-availability OpenTSDB was written to ad-
storage and retrieval of time- dress a common need: store, in-
series data in fields such as dex and serve metrics collected
operations monitoring, applica- from computer systems (network
tion metrics, Internet of Things gear, operating systems and ap-
sensor data, and real-time plications)atalargescale,there-
analytics’s. fore, making this data easily ac-
cessible and displayed.
Licence [34] MIT. GPL.
Supported Erlang, Go, Java, JavaScript, Erlang, Go, Java, Python, Rand
languages Lisp, Python, R and Scala. Ruby.
Pros Scalable in the enterprise ver- Massively scalable;
sion; Great for large amounts of time-
Outstanding high performance; based events or logs;
Accepts data from HTTP, TCP, Accepts data from HTTP and
and UDP protocols; TCP protocols;
SQL like query language; Good platform for future analyt-
Allows real-time analytics’s. ical research into particular ag-
gregations on event / log data;
Does not have paid version.
Cons Enterprise high price tag; Hard to set up;
Clusteringsupportonlyavailable Not a good choice for general-
in the enterprise version. purpose application data.
From Table 2.4, we can notice some similarities between these two TSDB databases.
Both TSDB are capable scalable and accept HTTP and TCP transfer protocols for com-
munication. InfluxDB and OpenTSDB are two open source time-series databases, how-
ever, the first one, InfluxDB, is not completely free, as it has an enterprise paid version,
which is not very visible in the offer. This enterprise version offers, clustering support,
high availability and scalability [51], features that OpenTSDB offer for free. In terms
of performance, InfluxDB surpasses and outperforms OpenTSDB in almost every bench-
marks [52]. OpenTSDB has the benefits of being completely free and support the most
relevant features, however it is very hard to set up and to develop for this database.
In the end, both TSDB are bundled with good features, and the decision falls into how
much performance is needed when choosing one. If the need is performance and access to
the database in short amounts of time, with low latency responses, InfluxDB is the way
to go, by other way, if there no restriction about the performance needed to query the
database and money is a concern, the choice should be OpenTSDB.
Tooling for this project is presented. We have covered the most used technologies and
core concepts in related to the field of tracing Microservices. Next Section 2.3 - Related
Work, will cover the related work performed in this area. Some ideas, approaches and
developed solutions will be discussed.
23
Chapter 2
2.3 Related Work
This section aims to present the related work in the field of distributed tracing data
handling and analysis. It is divided in three Subsections: first, 2.3.1 - Mastering AIOps,
which covers a work carried out by Huawei, that uses machine learning – deep learning
– methods to analyse data from distributed traces. Secondly, 2.3.2 - Anomaly Detection
usingZipkinTracingData, aworkofperformedbySalesforcewiththeobjectiveofanalyse
tracing from a distributed tracing tool. Finally, 2.3.3 - Analysing distributed trace data,
a work by Pinterest, where the objective is to study latency in tracing data.
2.3.1 Mastering AIOps
Distributed tracing has only started to gain widespread acceptance in the industry
recently, as a result of new architectural and software engineering practices, such as cloud-
native, fine-grained systems and agile methodologies. Additionally, the increase in com-
plexityresultingfromtheriseofweb-scaledistributedapplicationsisarecentphenomenon.
As a consequence of its novelty, there has been little research in the field so far.
Arecentexample,AIOps,anapplicationofArtificialIntelligencetooperations[53]was
introduced in 2016 [54]. This trend aims to use Artificial Intelligence for IT Operations
in order to develop new methods to automate the enhance IT Operations. Driving this
“revolution” are the following points:
• First there is the additional difficulty of manually managing distributed infrastruc-
tures and system state;
• Secondly,theamountofdatathathastoberetainedisincreasing,creatingaplethora
of problems to the operators handling it;
• Third, the infrastructure itself is becoming more distributed across geography and
organizations,asevidencedbytrendslikecloud-firstdevelopmentandfogcomputing;
• Finally, due to the overwhelming amount of new technologies and frameworks, it is
an herculean task for operators to keep in pace with the new trends.
The work performed and presented by Huawei, entitled Mastering AIOps with Deep
Learning, Time-Series Analysis and Distributed Tracing [55], aims to use distributed trac-
ing data and aforementioned technologies to detect anomalous tracing. The proposed
method encodes the traces and trains a deep learning neural network to detect significant
differencesintracing. Thisisaveryperceptiveapproach, takingintoaccounttheamounts
of data that is needed to analyse, however is limited to classifying a trace as normal or
abnormal, losing detail and interpretability i.e., no justification for the classification.
2.3.2 Anomaly Detection using Zipkin Tracing Data
Tooling in this field are not taking the expected relevance. Their usage is starting in
industry and production environments involving distributed systems, however, the con-
cerns in are not well aligned with the needs of operators, and this leads to increasing effort
when monitoring large scale and complex architectures, such as Microservices.
24
State of the Art
In a post from Salesforce, a work of research about using tracing data gathered by
Zipkin, to detect some anomalies in a Microservice based system [56]. At Salesforce,
Zipkin is used to perform distributed tracing for Microservices, collecting traces from
their systems and providing performance insights in both production monitoring and pre-
productiontesting. However,thecurrentZipkinopensourceinstrumentationandUIoffers
only primitive data tracing functionality and does not have in-depth performance analysis
of the span data. The focus on their work was to detect and identify potential network
bottlenecks and microservices performance issues.
The approach carried out was to implement scripts using that used Python AI pack-
ages, with the objective of extracting values from their network of services, namely service
dependency graph, in order to identify high traffic areas in the network. The values that
wereextractedwerethenumberofconnectionsfromeachservice, whichmeans, thedegree
of the service at specific times. This allows to notice which services are establishing more
connections with other services.
Fromthisapproach,itwaspossibletovisualizethehightrafficareaswithintheproduc-
tion network topology. Therefore, they have identified services with the most connections.
This finding was an helpful feedback for service networking architects that designed those
microservices. Those services, identified with too many connections, may potentially be-
come choking points in the system design. If one of the services fail, a huge impact on a
large number of depending services occur. Additionally, there could be also potential per-
formanceimpactsinthesystemsincealargenumberofservicesdependingonthem. Those
are valuable information for system designers and architect to optimize their designs.
The conclusions from Salesforce research identified that, with Zipkin tracing data, it is
possible to identify network congestion, bottlenecks, efficiencies and the heat map in the
production network. However, this tool does not provide analysis of tracing data at this
level. This was the main conclusion and possible working direction from this research:
“features like the ones presented, can be added to Zipkin or other distributed tracing tool
product line, including UI and dashboards. Capabilities like daily metrics or correlation
between microservices load and latency, able to generate alerts if bottleneck or heat map
is identified, should be added” [56].
2.3.3 Analysing distributed trace data
At Pinterest, the focus was to research for latency problems in their Microservices
solution. Pinterest claims to have tens of services and hundreds of network calls per-trace.
One big problem identified at start is the huge difficulty of looking to trace data due to
the overwhelming quantity of information – “thousands of traces logged each minute (let
alone the millions of requests per minute these traces are sampled from)”.
Pinterest felt the problem of monitoring Microservices early due to their service popu-
larity in the past years. With this popularity, systems usage increased significantly. This
lead them to take action and create their closed source distributed tracing analysis tool
called “Pintrace Trace Analyser” [57].
This tool gathers tracing data from Distributed Tracing Tools, more precisely from
Zipkin, and processes a sample of these tracing to detect mainly latency problems in the
service dependency network. Looking at stats from thousands of traces over a longer
period of time not only weeds out the outliers/buggy traces, but provides a holistic view
of performance.
25
Chapter 2
The conclusions from Pinterest, where that there is a great need to develop tooling
for distributed tracing analysis, with the main objective of ease the life of operators. The
following points were considered:
1. Automatically generate reports so engineers can easily check the status of each de-
ployment;
2. Setting up alerts for when latency or number of calls hits a certain threshold.
2.3.4 Research possible directions
One thing to notice from the related work presented is that there is few research
accomplished in the area and trace tooling development, however, these works are from
the past year and the tendency is to increase in the following years. Enlargement and
usage of distributed systems are fuel to feed the need of research in this field and develop
new methodologies and tools to monitor and control operations.
From the first work presented, “Mastering AIOps”, some final results and conclusions
were provided. They point out that the benefits of this approach were: first, very high
accuracy in detection 99,7%, and secondly, extremely fast detection in O(n) time. How-
ever, some limitations involving requiring very long training times for long traces (with
decent machines) were noted. Also, improvements were pointed: truncate traces, to lower
the quantity of tracing and therefore, summarizing traces.
The second work presented, “Anomaly Detection using Zipkin Tracing Data”, point
down the lack of features in the existing tools. These features include automatic anomaly
detectionusingdistributedtracingdata. Themainideaconsistsinextendingfunctionality
presented in this tools, to provide autonomous anomaly detection and alerting based on
information presented in tracing from services.
Thethirdworkpresented,“Analysingdistributedtracedata”,crucialpointsconsidered
weretorepresentautonomousgenerationofreports, allowingoperatorstocheckthestatus
of deployments, and therefore, providing more control over the system regarding detection
of anomalous values in service latency.
Finally, from the multiple related work presented the final assumptions for possible
research directions in this field are:
• Focus on the most important traces, reducing the quantity of tracing;
• Develop new methods that leverage features of existing distributed tracing tools;
• Automate the detection of anomalies presented in distributed systems;
Afterprovidingthestateoftheartforthisresearchtothereader, nextChapter3-Re-
search Objectives and Approach will cover the objectives of this research, the approach
used to tackle the problem and the compiled research questions.
26
Chapter 3
Research Objectives and Approach
InthisChapter,theproblemisapproachedindetailandtheobjectivesforthisresearch
are presented. The problem definition, how we tackled it and our main difficulties and
the objectives involved to provide a possible solution are presented in Section 3.1 - Re-
search Objectives. Also, a compilation of research questions are presented and evaluated
with some reasoning about possible ways to answer them, later in Section 3.2 - Research
Questions.
3.1 Research Objectives
Modern distributed services are large, complex, and increasingly built upon other sim-
ilarly complex distributed services. Debugging systems is not an easy task to perform.
It involves the collection, interpretation, and display of information concerning the inter-
actions among concurrently executing processes operating in distributed machines. Dis-
tributed Tracing data helps keeping an history of work performed by these systems.
End-to-end tracing captures the causally-related activity (e.g., work done to process a
request)withinandamongthecomponentsofadistributedsystem. Asdistributedsystems
growinscaleandcomplexity,suchtracingisbecomingacriticaltoolformanagementtasks
like diagnosis and resource accounting. However, as systems grow, resulting tracing data
from system execution is growing as well [17].
Tracing data growth raises some problems. This data is used by system operators to