and CAPTCHA. The Referer Header is sometimes missing [1], the secret token be-
comes  totally futile  when  XSS  exists  and  the  CAPTCHA  is  too bothering.  Besides, 
[2-3] brings about some client-taking actions yet pure client checking is not credible 
enough from server side perspective. And they still suffer from the Referer-missing 
problem.  Moreover,  all  of  [1-3]  have  nothing  to  do  with  same-domain  CSRF.  So  a 
client-initialized and server-accomplished defense mechanism (CSDM) is proposed. 
Definition: The super-referer of a request is made up of its Referer and all URLs of 
the  Referer’s  ancestor  frames,  excluding  the  querying  part.  E.g.,  the  Referer 
http://site/1.php?id=123 is cut to http://site/1.php. 
CSDM proposes a new HTTP Header Super-referer-header, containing super-referer. 
E.g.:  Super-referer-header:  http://site1/index.php,  http://hack/attack.aspx.  Considering 
privacy,  the  URL  in  the  new  Header 
should be hashed with strong one-way 
algorithm and MD5 is one choice. 
A POST request
1.Referer empty ?
no
2.Cross-domain request?
no
3.Destination in important-site list ?
yes
4.User chooses 
‘ Send’
 or‘ Cancel ’
no
Send
5.Send request with super-
referer-header
Cancel
6.Reject to send
Consider POST-based  CSRF first. 
The client defence is shown in Fig. 1. 
A  POST  request  must  satisfy  all  the 
qualifications  in  Fig.1  before  being 
sent  out,  or  else  it  will  be  cancelled. 
In  step  3  and  4,  a  configurable  “im-
portant-sites  list”  is  proposed.  POST 
requests sending to important sites for 
users  can  be  further  confirmed  by 
offering  users  a  “Send  or  Cancel?” 
dialog.  CSRF  requests  are  generally 
sent silently and users have no idea of 
Fig. 1. Client checking of POST Request 
* This work is supported by the National Natural Science Foundation of China under Grant No. 
60970140, No.60773135 and No.90718007. 
** Corresponding author. 
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 484–485, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
A Client-Based and Server-Enhanced Defense Mechanism 
485 
it.  If  users  didn’t  click  any  submitting  button  before  seeing  the  confirming  dialog, 
“Cancel” is preferred. 
An  important  observation  shows  that  POST  target  URL  generally  needs  only  a 
small  number  of  different  intended  source  URLs,  so  a  policy  file  is  used  at  server 
side. For example, POST: { 
Dest1: /profile.php  
Same Domain1: /chgpfl.php   
Cross Domain1: trust.com/chg.aspx 
Dest2: /blog.php 
Same Domain2: subdomain.sns.com/*} 
to  profile.php  should  only  origin 
Requests  sending 
from  chgpfl.php  or 
trust.com/chg.aspx.  The  policy  file  should  cover  all  POST  target  pages.  So  when 
addressing a request, servers examine the super-referer-header, checking whether all 
source URLs of the request are allowable. The server solution is deployed as part of a 
web  application  firewall,  making  it  compatible  with  current  websites.  And  we  can 
trade  space  for  time  when  decoding  the  MD5 value  as  every  site  has  limited  URLs 
(excluding querying parts). 
Cross-domain and almost all same-domain CSRF can be prevented as their source 
URLs are illegal. In step 4 of Fig. 1, even attackers trigger a malicious script in paral-
lel  to  the  submitting  of  a  legitimate  form  or  the  users  cannot  make  right  decisions 
when choosing “Send” or “Cancel”, a further checking at server side will still guaran-
tee the security. Same-domain CSRF can only happen when an allowable source page 
towards a specific CSRF target page happens to host some XSS vulnerability. But the 
chances are low and the destructiveness can be expected to be minimized or limited as 
only the specific target page and no others can be aimed at.  
The super-referer is helpful in accurately depicting the sources of requests and pre-
venting same-domain CSRF, as attackers can embed some permissible page in XSS-
infected pages. Besides, such a concept can help preventing clickjacking [2]. 
GET-based  CSRF  deserves  less  attention,  as  all  state-modifying  requests  should 
use POST and real world GET CSRF is far less destructive. At client, GET requests 
with HTTPS or Authorization Header are blocked if without Referrer. At server side, 
super-referer checking is used for sensitive target URL. 
The CSDM client prototype is implemented as a Firefox browser extension. Real 
world  tests  with  popular  sites  including  iGoogle,  yahoo,  facebook  and  a  vulnerable 
sample site show that it prevents all kinds of CSRF attacks reproduced in lab envi-
ronment with no obvious compatibility problems or user experience degradation. 
References 
1.  Barth,  A.,  Jackson,  C.,  Mitchell,  J.C.:  Robust  defenses  for  cross-site  request  forgery.  In: 
15th ACM Conference on Computer and Communications Security (2008) 
2.  Mao,  Z.,  Li,  N.,  Molloy,  I.:  Defeating  cross-site  request  forgery  attacks  with  browser-
enforced authenticity protection. In: 13th International Conference on Financial Cryptogra-
phy and Data Security (2009) 
3.  Maes, W., Heyman, T., Desmet, L., et al.: Browser protection against cross-site request for-
gery. In: 1st ACM Workshop on Secure Execution of Untrusted Code, Co-located with the 
16th ACM Computer and Communications Security Conference (2009) 
A Distributed Honeynet at KFUPM: A Case Study 
Mohammed Sqalli, Raed AlShaikh, and Ezzat Ahmed 
Department of Computer Engineering 
King Fahd University of Petroleum and Minerals 
{sqalli,g199607190,g200804300}@kfupm.edu.sa 
1   Introduction and Design Setup 
The main objectives of this work is to present our preliminary experience in simulat-
ing a virtual distributed honeynet environment at King Fahd University of Petroleum 
and Minerals (KFUPM) using Honeywall CDROM [1], Snort, Sebek and Tcpreplay 
[3] tools. In our honeynet design, we utilized the Honeywall CDROM to act as a cen-
tralized  logging  center  for  our  distributed  high-interaction  honeypots.  All  honeypot 
servers, as  well as the Honeywall  CDROM itself,  were built on top of a virtualized 
VMWare environment, while their logs were forwarded to the centralized server. This 
setup is illustrated in figure 1. 
Fig. 1. The proposed distributed design of KFUPM honeynet 
2   Preliminary Evaluation and Results 
Since  honeypots  do  not  offer  any  useful  services  to  Internet  users  and  the  Internet 
addresses  of  the  honeypots  are  not  publicly  known,  most  traffic  on  the  honeynet  is 
suspicious. However, not all traffic is malicious. Therefore, the traffic we observed on 
our honeypots falls into three different categories: 
-  Network scans by KFUPM Information Technology Center. 
-  Traffic generated by honeypots due to normal network operations (e.g. traffic 
to maintain the network connection). 
-  Network broadcasts, such as BitTorrent requests. 
At KFUPM, more than 30,000 activities were captured in the given 30-hours interval. 
Tale 1 shows the distribution of these types of activities in more details. 
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 486–487, 2010. 
© Springer-Verlag Berlin Heidelberg 2010 
A Distributed Honeynet at KFUPM: A Case Study 
487 
Table 1. The traffic distribution as it was detected by KFUPM honeynet in a 30-hours interval 
Name 
IIS view script source code vulnerability attack 
MS Uni Plug and Play UDP 
NBT(NetBIOS) Datagram Service 
Bit Torrent requests 
DHCP requests 
Protocol 
TCP 
UDP 
UDP 
TCP 
UDP 
Severity 
Medium 
Medium 
Low 
Medium 
Low 
Total 
8 
30 
399 
19098 
9938 
In terms of severity, around 65% of the traffic was considered medium risk, while 
the  remaining  35%  was  considered  low.  The  high  percentage  of  the  medium-level 
category was due to the fact that the system classifies BitTorrents file sharing, which 
makes around 70% of the total traffic, as medium risk. This percentage is of no sur-
prise  since  BitTorrent  accounts  for  an  astounding  40-55%  of  all  the  traffic  on  the 
Internet [5], and it is expected to be high in the students’ living campuses. 
Another interesting finding is the detection of a vulnerability attack on the Internet 
Information  Service  (IIS)  that  was  installed  on  the  Windows-based  honeypots.  This 
vulnerability has the signature KFAGC165421, and indicates that IIS contains a flaw 
that allows an attacker to cause IIS to return the source code for a script file instead of 
processing  the  script.  This  vulnerability  attack  traffic  was  generated  by  one  of  the 
systems in the students’ living campus. 
3   Conclusion and Future Work 
Our experience so far shows that Honeywall CDROM proved to be a solid tool that is 
capable of capturing great deal of information and assisting in analyzing traffic on the 
distributed  honeypots.  The  honeynet  designer,  nevertheless,  needs  to  consider  few 
issues related to scalability and resource utilization. 
Out  future  work  includes  expanding  our  honeynet  network  to  include  other  col-
leges  and  campuses  in  the  university  and  have  wider  honeynet  coverage.  This  will 
also require increasing our logging disk space to allow for more logging time, longer 
logging intervals and thus broader analysis. 
References 
1.  The Honeywall CDROM, https://projects.honeynet.org/honeywall/ 
2.  Argus: The Network Activity Auditing Tool, http://www.qosient.com/argus 
3.  TCPreplay, http://tcpreplay.synfin.net/ 
4.  WireShark, http://www.wireshark.org/ 
5.  Le Blond, S., Legout, A., Dabbous, W.: Reducing BitTorrent Traffic at the Internet Scale. A 
Presentation at the Internet Research Task Force, IRTF (March 2010) 
Aspect-Based Attack Detection in Large-Scale
Networks
Martin Draˇsar, Jan Vykopal, Radek Krejˇc´ı, and Pavel ˇCeleda
Masaryk University, Botanick´a 68a, 61200, Brno, Czech Republic
@ics.muni.cz
Abstract. In this paper, a novel behavioral method for detection of at-
tacks on a network is presented. The main idea is to decompose a traﬃc
into smaller subsets that are analyzed separately using various mecha-
nisms. After analyses are performed, results are correlated and attacks
are detected. Both the decomposition and chosen analytical mechanisms
make this method highly parallelizable. The correlation mechanism al-
lows to take into account results of detection methods beside the aspect-
based detection.
1 Introduction
With the advent of multigigabit networks, the task to detect attacks becomes
more and more challenging. The deep packet inspection and the pattern match-
ing are reaching their limits. Not only hardware requirements are becoming more
demanding than what can be supplied. Also a steady inﬂux of new attacks makes
all signature sets outdated by the time they are released. This situation actuated
a development of new signature-less attack detection methods, namely the net-
work behavioral analysis. The principle of such analysis is to reveal a potentially
harmless behavior of network hosts by detecting deviations in traﬃc patterns.
These methods are e. g., based on the Holt-Winters method [1] or the principal
component analysis [2] and are successful in a detection of both existing and
previously unknown attacks.
In the following text a new behavioral method called the aspect-based detec-
tion that oﬀers a high speed and an ability to detect new attacks is proposed.
2 Aspect-Based Detection
Various network devices like switches or probes are able to store a set of traﬃc
descriptors, like IP addresses, ports and transferred data for each connection. A
combination of one or more of these descriptors represents one aspect of traﬃc,
e. g., i) amount of traﬃc from one address (source address, payload size, time)
or ii) traﬃc volume on given port over the time (destination port, payload size,
time).
Every aspect can be represented as a multidimensional matrix where each
element stands for one connection. These matrices are subject to analysis. This
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 488–489, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
Aspect-Based Attack Detection in Large-Scale Networks
489
approach has several advantages. First, by splitting the entire traﬃc into its
aspects the volume of data that has to be processed at one time is lowered,
thus relaxing hardware requirements. Aspects also conserve relations in traﬃc
patterns. Second, by representing traﬃc data as a matrix, fast and specialized
algorithms from the area of the digital signal processing can be used.
The core operation in the aspect-based detection is an application of linear
and non-linear ﬁlters on aspect matrices. Non-linear ﬁlters are mainly used for
thresholding – representing static anomaly checks, e. g., one computer connects
to a hundred other computers. Linear ﬁlters are used to tackle dynamic aspects
of a traﬃc. A convolution with Sobel-like operators that approximate the second
derivation can eﬀectively discover sudden changes in traﬃc, that can e. g., point
to an activation of infected computer. Aspect matrices or the entire traﬃc can
also be fed to other detection mechanisms, provided their result can be converted
to a matrix with values comparable to other transformed aspect matrices.
Each transformed aspect matrix can identify an ongoing attack, but it is more
likely to only highlight traﬃc deviations. To discover more stealthy attacks, it
is necessary to correlate these matrices. This is done by constructing a resulting
matrix which dimensions are sum of dimensions of transformed aspect matrices.
Individual matrices are added to the resulting one in a manner that inﬂuences
also dimensions the added matrix is not deﬁned for. This addition is best de-
scribed by an example. Let Ra,b,c,d be a four-dimensional resulting matrix and
Aa,c,d, Ba,b and Cc,d be transformed aspect matrices. A calculation of one ele-
ment goes like this: Rax,by,cz,dw = Aax,cz,dw + Bax,by + Ccz,dw.
The resulting matrix describes traﬃc in terms of identiﬁed deviations. There
are likely to be three kinds of areas in this matrix – where a deviation going over
certain thresholds i) indicates an attack, ii) is harmless, iii) is suspicious. These
thresholds have to be hand selected at least for known attacks. But for unknown
attacks these thresholds might be derived e. g., from a distance from a known
attack in the resulting matrix. Distance metrics are to be modiﬁed according to
a collected data.
3 Future Work
The research of the aspect-based detection has to focus on several key areas. Main
task is to create appropriate data structures that will allow eﬀective processing
of aspect matrices. Also the previously mentioned areas in the resulting matrix
must be identiﬁed and metric-derived thresholds investigated.
References
1. Li, Z., Gaoa, Y., Chen, Y.: HiFIND: A high-speed ﬂow-level intrusion detection
approach with DoS resiliency. Computer Networks 54(8), 1282–1299 (2010)
2. Lakhina, A., Crovella, M., Diot, C.: Anomaly Detection via Over-Sampling Principal
Component Analysis Studies. Computational Intelligence 199, 449–458 (2009)
Detecting Network Anomalies in Backbone
Networks
Christian Callegari, Loris Gazzarrini, Stefano Giordano,
Michele Pagano, and Teresa Pepe
Dept. of Information Engineering, University of Pisa, Italy
{c.callegari,l.gazzarrini,s.giordano,m.pagano,t.pepe}@iet.unipi.it
1 Extended Abstract
The increasing number of network attacks causes growing problems for network
operators and users. Thus, detecting anomalous traﬃc is of primary interest in
IP networks management. As it appears clearly, the problem becomes even more
challenging when taking into consideration backbone networks that add strict
constraints in terms of performance.
In recent years, Principal Component Analysis (PCA) has emerged as a very
promising technique for detecting a wide variety of network anomalies. PCA is
a dimensionality-reduction technique that allows the reduction of the dataset
dimensionality (number of variables), while retaining most of the original vari-
ability in the data. The set of the original data is projected onto new axes, called
Principal Components (PCs). Each PC has the property that it points in the
direction of maximum variance remaining in the data, given the variance already
accounted for in the preceding components.
In this work, we have focused on the development of an anomaly based Net-
work Intrusion Detection System (IDS) based on PCA. The starting point for our
work is represented by the work by Lakhina et al. [1], [2]. Indeed, we have taken
the main idea of using the PCA to decompose the traﬃc variations into their
normal and anomalous components, thus revealing an anomaly if the anomalous
components exceed an appropriate threshold. Nevertheless, our approach intro-
duces several novelties in the method, allowing great improvements in the system
performance. First of all we have worked on four distinct levels of aggregation,
namely ingress router, origin-destination ﬂows, input link, and random aggre-
gation performed by means of sketches, so as to detect anomalies that could be
masked at some aggregation level. In this framework, we have also introduced a
novel method for identifying the anomalous ﬂows inside the aggregates, once an
anomaly has been detected. To be noted that previous works are only able to
detect the anomalous aggregate, without providing any information at the ﬂow
level. Moreover, in our system PCA is applied at diﬀerent time-scales. In this
way the system is able to detect both sudden anomalies (e.g. bursty anomalies)
and “slow” anomalies (e.g. increasing rate anomalies), which cannot be revealed
at a single time-scale. Finally, we have applied, together with the entropy, the
Kullback-Leibler divergence for detecting anomalous behavior, showing that our
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 490–491, 2010.
c(cid:2) Springer-Verlag Berlin Heidelberg 2010
Detecting Network Anomalies in Backbone Networks
491
Flow Aggregation
Time-series construction
Netflow
Data 
Formatting
ASCII
Aggregate?
Metrics?
OD
IL
IR
random
Entropy
K-L Divergence
Anomaly Detector
Identification
Detection
PCs
computation
ξ
Fig. 1. System Architecture
choice results in better performance and more stability for the system. Figure 1,
shows the architecture of the proposed system.
The proposed system has been tested using a publicly available data-set, com-
posed of traﬃc traces collected in the Abilene/Internet2 Network [3], that is a
hybrid optical and packet network used by the U.S. research and education com-
munity. Since the data provided by the Internet2 project do not have a ground
truth ﬁle, we are not capable of saying a priori if any anomaly is present in the
data. For this reason we have partially performed a manual veriﬁcation of the
data, analyzing the traces for which our system reveals the biggest anomalies.
Moreover we have synthetically added some anomalies in the data (mainly rep-
resentative of DoS and DDoS attacks), so as to be able to correctly interpret the
oﬀered results, at least partially. The performance analysis has highlighted that