    │   ├── errors-21-03-02_16-22-46.log
    │   └── errors-21-03-02_16-23-01.log
    ├── reports
    │   ├── 127.0.0.1
    │   │   ├── _21-03-02_16-22-46.txt
    │   │   └── _21-03-02_16-23-01.txt
    │   └── DO_NOT_DELETE_THIS_FOLDER.txt
    ├── requirements.txt
    └── thirdparty //第三方库
整个项目划分为了5个文件夹:
  * 1.db文件夹，存放路径和黑名单的列表
  * 2.lib文件夹，作为library作用的存在，存放项目运行的主要的代码
(1) 子Connection文件夹
    * Requester.py:为每一个目标分配一个Requester对象，方便配置各种用于请求的参数、(cookie, useragent...)、重试频率的实现、可以通过ip或者域名的方式去建立底层的tcp连接(通过重写url，host->ip,给requests.get()),请求的类方法是request，真正后端请求引用的是成熟的requests库来发包。
    * Response.py:
这个思想也很棒，基于
                self.status = status
            self.reason = reason
            self.headers = headers
            self.body = body
这四个参数，然后封装了基础方法，`__hash__`,获取redirect之类的,方便其他调用。
(2)子Controller文件夹
    * banner.txt:logo标志
    * controller.py: 构造函数初始化配置各种请求参数，为目标初始化requester对象、
Fuzzer对象(主要是传递requester对象、爆破字典、结果匹配字典作为参数来实例化fuzzer对象)、后面就是调用fuzzer.start()去执行扫描。这个文件立马很多函数的作用都是对程序起一个整体控制的作用，比如整体暂停、整体执行，然后里面就有一些为整体控制提供的一些函数来方便调用。
(3)子core文件夹:
    * argument_parser.py: 接收和检验输入的参数,先解析default.conf配置文件，然后后面在解析命令行参数，写的比较细腻，用了OptionGroup将参数进行分类,值得学习。
1.mandatory 强制性需要传输的参数
2.dictionary 路径字典的设置
3.general 常规的参数，用于调控请求
4.request http请求需要配置的参数
5.connection 主要是对request更深层次的参数自定义
    * dictionary.py： 这个核心就是`generate` 函数，就是实现各种规则生成字典，但是这里也有一些比较有意思的函数，通过使用thread.lock实现了线程安全的可以根据索引来取值的列表的`nextWithIndex`函数，要是换做我来写的话，我可能采用queue来做，但是这样很不方便，比如我想reset，我只需要直接让`self.currentIndex=0`就行了,后面设计进度条也很方便。
    * fuzzer.py: 核心是start函数,首先就是`self.setupScanners()`用于后面比对错误页面(其实蛮细腻的，就是每种请求格式都会有一个scanner，比如.xxx xxx xxx/ xxx.php都会根据请求的格式不同生成不同的scanner来减少误报)，接着就是`setupThreads`分配好自定义的线程个数，启动线程，核心work函数`thread_proc`, 通过`threading.Event`来统一调控(`self.play()`设置event为True,让多个线程同时启动，而不是像以前那样for循环来进行start，显得很有序)，同时也方便实现统计线程数目，
    * path.py: 存储路径的请求状态和返回内容
    * raw.py: 从原生的raw http协议包提取各个参数出来用来初始化请求
    * report_manager.py: 输出报告管理类，主要是方便调用多种输出格式，做了一层管理作用的封装去调度各种类型的报告类。
    * scanner.py: 核心就是根据相似度识别不存在页面的实现，其中引入了sqlmap的一个`DynamicContentParser`方法，这种引入第三方库的思想是值得学习的。
（4）子output文件夹
    * cli_output.py、print_output.py: 安静模式用print，非安静模式就用cli，我看了下两者的区别就是, print模式将很多cli模式的函数内容替换为了`pass`，只保留了最基础的成功的路径的输出信息。
(5)子reports文件夹:
    * base_report.py: 作为一个基类的存在，声明和实现了一些方法，在创建保存的目录的时候考虑了window和linux的区别。
    * plain_text_report.py: 核心是generate函数，组合了输出结果成字符串，这种输出蛮有意思的，不断flush缓冲区的内容，确保内容写入到文件，不会出现因为内存中断导致数据丢失。
(6)子utils文件夹:
    * file_utils.py:封装了os.path的文件操作类
    * random_utils.py:生成随机字符串
    * terminal_size: 主要作用是在终端实现window和linux的兼容，美化输出。
### 0x2.4 学习报告输出的实现
上面执行流程分析没有具体分析报告输出，是因为我觉得这个点可以拎出来学习一下。
dirsearch实现的是动态保存结果,就算突然中断了也会保存之前的扫描结果。
我当时在写MorePing的时候，为了实现这个效果:
>
> 程序执行就多开了个报告的线程，然后主函数扫描完成将结果存入到result_queue,然后报告的线程一直在执行，用一个不优雅的变量充当信号量，去获取result_queue的值,程序暂停时,信号量被重置为0,线程就退出了。
但是当我看完dirsearch的实现，我发现dirsearch的设计更简洁:
这个功能指向点:在多线程的主工作函数(就是核心的函数，去请求url然后获取结果的`thread_proc`)中的当发现存在满足matchCallbacks的路径时就会进行报告的存储。
这里先用`addPath`将扫描结果存起来,然后调用了`save`去保存。
这里就很有意思,可以看到这里的`outputs`列表其实就是
默认的话就是`plain_text_report`,
调用`storeData`方法将这些存入了一个`pathList`列表里(这里作者没线程安全的错误，保证了执行addPath是线程安全的)。
然后调用Save的话,主要是进去了`self.generate()`函数,进行了结果的输出
**可以看到dirsearch是将扫描出来的路径逐一加入到`pathList`,然后每次扫描出新的结果的时候，再重新根据pathList重新构造新的报告输出内容，然后在用seek(0)来控制文件指针整体覆盖写入新文件(这样可以避免重复打开文件和关闭文件,比用with
open上下文管理来说是有优势的)，来实现动态存储输出结果**
## 0x3分析Watchdog
下载地址:[Watchdog](https://github.com/CTF-MissFeng/Watchdog)
按照作者的思路，部署分布式的步骤是:
主节点部署:web+数据库
子节点通过修改:
`database.py` 中的
    engine = create_engine('postgresql://postgres:687fb677c784ce2a0b273263bfe778be@127.0.0.1/src')
为主节点的数据库,然后分别在各个子节点，运行client目录下的xxx_run.py脚本:
    client
    ├── __init__.py
    ├── database.py
    ├── portscan
    │   ├── NmapScan.py
    │   ├── ShodanScan.py
    │   ├── __init__.py
    │   └── portscan_run.py //这个启动是端口扫描
    ├── subdomain
    │   ├── __init__.py
    │   └── oneforall //这个是
    ├── urlscan
    │   ├── __init__.py
    │   ├── url_probe
    │   └── xray
    └── webinfo
        ├── __init__.py
        ├── ipdata.ipdb
        ├── run.py
        └── wafw00f
不难发现,各个脚本都是用`While True:`来实现持久运行,这里挑选两个模块来分析一下。
### 0x3.1 端口扫描模块
    def main():
        print('[+]端口扫描启动')
        while True:
                #从数据库获取资产
            assets_sql = ReadAssets()
            if not assets_sql:
                time.sleep(30)
            else:
            # 传入资产的ip值给PortScan
                portscan = PortScan(assets_sql.asset_ip)
                # 执行
                port_dict, vulns_list = portscan.run()
                if port_dict:
                        # 写入结果到数据库
                    WritePosts(port_dict, assets_sql)
(1)获取待扫描的资产信息:
(2)将IP传入PortScan,初始化，然后执行Run
这个功能代码实现的很粗糙,就是通过shodan获取到开放的端口,然后在调用Nmap去扫描获取服务指纹。
(3)提交数据库部分:
没什么好讲的。
### 0x3.2 Xray扫描模块
(1) main 部分:
首先用一个子线程启动了`web_main`,主要作用是开了个webhook的API用来将xray的结果写入到数据库。
    @app.route('/webhook', methods=['POST'])
    def xray_webhook():
接着下面同样开了一个子线程`xray_main`启动xray扫描器,
结果传送到前面的webhookAPI,开了个子进程去运行xray。
(3)启动crawlergo_main爬虫部分:
这里作者用了进程池(emm,感觉用的混乱),不过这里还判断了下Xray的队列大小来决定是否启动爬虫,来折中因为爬虫大量写入xray的队列的问题，不过这种控制还是不够细腻的,为了code的方便，这样写无可厚非。
这里作者将爬虫返回的新的子域名列表又重新添加到了资产中,并且在写入的数据库的时候做了去重判断。
### 0x3.3 简单分析
不难看出来,该系统实现的分布式的细粒度就是: (不同目标,多个扫描模块, 多个扫描模块去扫描不同目标)
**缺点:**
采用postgresql作为后端数据库，缺乏高性能，整个系统的读写和写入次数与细粒度的复杂度是同级别的,
缺乏调度系统，完全就是竞争模式去抢占目标，内耗程度比较高(容易导致数据库连接数过多数据丢失等情况)，缺乏高可用性，系统整体应该是低效、混乱的。
> PS.笔者没有搭建去测试，静态分析的代码，推测的结果
还有代码复用程度有点低, emm, 代码风格蛮萌新的,其实可以还可以继续封装一下。
缺乏节点管控模块, 缺乏异常的具体处理...
...
**优点:**
作为一款即兴开发的非专业程序员，通过比较暴力的方式联动了多个扫描工具，同时具备良好的界面效果和一定的可用性的"分布式"扫描器来说，Watchdog可以说是满足基本要求的,同时一款新生项目是不断成长的，需要给作者时间去慢慢改进，造福我们白嫖党ing。
## 0x4 分析w11scan
### 0x4.1 简单介绍
根据作者的安装文档和描述,应用到了celery分布式框架,然后数据缓冲采用了redis,数据存储使用了mongodb数据库。
这个架构我是觉得很不错的，系统的主要任务是识别给定url的指纹，所以核心功能部分作者的代码量是比较少的，系统的亮点应该是分布式的处理部分,
即celery的使用部分可以值得我们去学习，(PS.前端也很赏心悦目呀，够简洁，够有趣)
先看下整体目录结构:
    ├── Dockerfile
    ├── LICENSE
    ├── README.md
    ├── __init__.py
    ├── app
    ├── backup
    ├── celery_config.py
    ├── cms
    ├── config.py
    ├── dockerconf
    ├── docs
    ├── manage.py
    ├── requirements.txt
    ├── static
    ├── templates
    ├── test.py
    ├── whatcms.py
    └── xun
### 0x4.2 分析流程
下面的分析流程,我并没有去调试代码,而是根据docker搭建好系统，根据功能点去确定入口,然后逐步跟就行。
先看下启动该系统时产生的进程:
可以看到分别启动了redis和mogond数据库,然后启动了很多whatcms的celery的work进程
感觉这种启动子进程方式不是很优雅,我可能会考虑用`supervisord`来进行worker进程的管理
* * *
这里是新建一个任务,我们抓包分析出静态代码所对应的地方。
`w11scan/app/views.py` 163行
将任务插入到数据库,后台其实在工作的,主要是上面的一条语句调用子worker节点来工作:
    from cms.tasks import buildPayload
    ...
    buildPayload.delay(item, str(insertid))
worker部分如下:
可以看到导入了`cms.tasks`模块,`cms/tasks.py,`装饰器有三个函数`otherscan`、`buildPayload`、`singscan`:
可以看到流程就是:前端提交任务->调用work执行buildPayload->构造指纹规则fuzz请求->输出结果到mongodb数据库。
这里作者有个有意思的地方,就是用了redis来作为缓存存储了url的状态
这样的好处是,如果是相同目标的话,这样不会进行重复的同时扫描(保证即时性)
但是等整个状态扫描完成时，redis会删除这个标志，所以可以再次扫描(可用即时性)
### 0x4.3 简析优缺点
**不足:**