title:Quantifying DNN Model Robustness to the Real-World Threats
author:Zhenyu Zhong and
Zhisheng Hu and
Xiaowei Chen
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Quantifying DNN Model Robustness to the
Real-World Threats
Zhenyu Zhong, Zhisheng Hu, Xiaowei Chen
{edwardzhong, zhishenghu, xiaoweichen01}@baidu.com
Baidu Security
Abstract—DNN models have suffered from adversarial ex-
ample attacks, which lead to inconsistent prediction results.
As opposed to the gradient-based attack, which assumes
white-box access to the model by the attacker, we focus
on more realistic input perturbations from the real-world
and their actual impact on the model robustness without
any presence of the attackers. In this work, we promote a
standardized framework to quantify the robustness against
real-world threats. It is composed of a set of safety properties
associated with common violations, a group of metrics to
measure the minimal perturbation that causes the offense,
and various criteria that reﬂect different aspects of the
model robustness. By revealing comparison results through
this framework among 13 pre-trained ImageNet classiﬁers,
three state-of-the-art object detectors, and three cloud-based
content moderators, we deliver the status quo of the real-
world model robustness. Beyond that, we provide robustness
benchmarking datasets for the community.
Keywords-neural network, adversarial example, robust-
ness, threat severity
I. INTRODUCTION
Along came with the adversarial examples, attacking
DNN models can be achieved through iterations of
optimizations on the input using the gradient descent
methods in the white-box setting [1] [2] [3] [4]. However,
without a clear monetizing scheme on the adversarial
examples, the attackers are less likely to perform a large-
scale attack campaign in the real-world. On the other
hand, DNN models also experience a hard time mak-
ing consistent predictions in rare real-world scenarios,
especially for those safety-critical applications, includ-
ing autonomous driving, online content moderation. An
occurrence of safety violation that could lead to severe
consequences. More importantly, such a threat will not
cease to exist even if there are no actual attackers. We
regard the latter as the ﬁrst and foremost threat to the
reliability of the DNN in practice.
For understanding the real threat against the DNN
models from various input perturbations from the real
world, there are mainly two different approaches. One
is collecting data from the real world [5], and the other
is through data augmentation. The former one is cost-
inefﬁcient, and might not cover all the corner occasions;
however, the latter one is widely used by the ma-
chine learning community with the affordable expense
of GPU/CPU computations. The model built on these
augmented data can further be adapted to scenarios with
data scarcity issues by transfer learning [6] [7] [8].
In this paper, we go with the data augmentation to
simulate the real world perturbations. We put these per-
turbations into ﬁve different safety property categories:
1) Luminance: e.g., a glaring light could blind the camera
and hurt the perception. 2) Spatial Transformation: e.g., a
loose camera might take in the data with a shifted angle.
3) Blur: e.g., a fast-moving object is obscured when being
captured by a slow camera. 4) Corruption: e.g., possible
malfunctioning camera sensor cells. 5) Weather: e.g., in-
visibility caused by severe weather. Unlike the manual
settings on the threat severity [9], and white-box based
threat measurement [10] assuming the presence of an at-
tacker, we extend the foolbox [11] and deﬁne the realistic
threat severity as the real world minimal perturbations
minpert applied to the original input in order to change
the model prediction behavior. Such threat severity is
quantiﬁed in ||minpert||p. The larger the ||minpert||p, the
less severe the threat is. That means it would require
extra efforts to perturb the input to fool the underlying
model. Terminology-wise, ||minpert||p also stands for the
robustness of the model given an input. We interchange
it with threat severity throughout the rest of the paper.
Furthermore, ||minpert||p can be different if the per-
turbed input is tested under different criteria. For exam-
ple, a common misclassiﬁcation on the perturbed input
is relatively less severe than the targeted-class misclas-
siﬁcation. In different to [11], we support measurement
on different learning tasks.
this paper:
In summary, we make the following contributions in
• We propose a set of safety properties observed
from the real-world that could mislead the model
prediction behavior.
• We extend the foolbox to search for the minimal
real-world perturbations for each safety property on
different learning tasks, especially for object detec-
tion.
• We demonstrate the realistic
to near
production-level DNN models across various learn-
ing tasks, including 13 ImageNet [12] scale pre-
trained classiﬁers, three state-of-the-art object detec-
tors, and three cloud-based content moderators.
• We provide benchmarking datasets for image clas-
threats
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00033
150
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:00 UTC from IEEE Xplore.  Restrictions apply. 
a) Brightness: Changing the brightness of an image
can be achieved by increasing or decreasing each pixel
of the image by a constant value. Clip is a function to
make sure the resulted new image x
is in a valid range
of [0, 255] or [0, 1] typically.
(cid:2) = Clip(x ± )
(1)
x
(cid:2)
b) Contrast Reduction: Usually multiplication and
addition are used to adjust contrast and brightness [13].
We deﬁne  as the contrast level and make it also have
an impact on the constant factor C of the addition part.
(cid:2) = Clip((1 − ) · x +  · C)
x
(2)
2) Transformation:
◦
◦
or −180
a) Rotation: Starting from the center point, the input
image rotates in either +180
directions. We
leverages on OpenCV API [14] to achieve this in Eq 3 in
2 steps. First, we supply cx, the center of input x, and
rotation factor  to get the matrix after the rotation. Then
based on the resulted matrix and the input x, we obtain
the output x
(cid:2)
.
mrotation = getRotationMatrix2D(cx,  · 180/pi, 1)
(cid:2) = Clip(warpA f f ine(x, mrotation, (wx, hx)))
x
(3)
b) Horizontal/Vertical Translation: There are two di-
rections a horizontal translation can happen: left and
right, and for vertical translation: up and down. Similar
to Rotation, we need to compute the matrix after we
apply the amount of  in either direction to the transla-
tion. Then we generate an image by calling warpA f f ine
function.
c) Spatial: This transformation combines rotation
and translation to make it a more complicated case,
such that the rotation does not have to happen from
the center of the image. Correspondingly there are two
factors, rotation and 
translation, controlling the degree
of the rotation and translation applied to the original
image.
3) Blur:
a) Motion Blur: Applying motion blur is to have a
function CF convolve a ﬁlter, a.k.a. kernel, across the
image. As described in Eq 4, we ﬁx the motion angle ma
f is the function generates the kernel.
to be vertical v,
We only allow the kernel dimension 
dim to determines
the amount of blurring effects.
(cid:2) = Clip(CF(x,−1, k)), k = f (
dim, mav)
(4)
x
Fig. 1: Model Robustness Quantiﬁcation Framework
siﬁers, object detectors, and online content moder-
ators from the realistic threat perspectives to the
research community.
II. APPROACH
A. Framework
Figure 1 illustrates our model robustness evaluation
framework. It consists of four components. 1) Safety
Property Pool, which contains 15 safety properties. 2)
Threat Criteria (TC), which deﬁne various aspects of the
model robustness. 3) Model Pool, which stores the target
models whose robustness is measured. 4) Perturbation
knob (PertKnob), which searches for the minimal amount
of changes applied to the original image to mislead
the prediction results. The entire robustness evaluation
works as follows: Given an input image, we retrieve
a safety property from the Safety Property Pool. The
PertKnob incrementally increases the perturbations ap-
plied to the original input. The generated example is
forwarded to the model. Based on the evaluation criteria
chosen from the TC, only the one that successfully
deceives the model will be saved, and the Lp norm-based
distance is measured to indicate the model robustness.
If the model makes a consistent prediction, PertKnob
will increase the amount of the perturbation. The process
stops either when it ﬁnds an example that deceives the
model, or when it exhausts the perturbation search space
without success.
B. Safety Properties
First of all, we deﬁne a set of safety properties ob-
served in the real world. We divide them into ﬁve cate-
gories: Luminance, Spatial Transformation, Blur, Corruption,
and Weather. Rather than increasing the cost of collecting
data to cover rare cases, we leverage on computer vision
simulations to achieve data augmentation. We discuss
each of them in detail as follows.
1) Luminance:
b) Gaussian Blur: Similar to motion blur, applying
gaussian blur is convolving a ﬁlter using a gaussian
function G. The Gaussian distribution is determined by
the standard deviation σ. We deﬁne σ to control the
blurring effects.
(cid:2) = Clip(G(x, σ)), G(x, σ) =
x
− x2
22σ
e
1(cid:2)
2π2σ
(5)
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:00 UTC from IEEE Xplore.  Restrictions apply. 
151
4) Corruption:
a) Uniform Noise: Noise is generated separately
based on a uniform distribution given the input range
controlled by  and is then added to the original input.
(cid:2) = Clip(x +  · U ( · C))
x
b) Gaussian Noise: Noise is generated separately
from a normal distribution N with μ = 0, std controlled
by , and is then added back to the original image.
(cid:2) = Clip(x +  · N (μ,  · C))
x
∗
c) Blended Uniform Noise: Applying blended noise
is to produce a randomized image x
from a uniform
distribution, such that the generated random image is
sufﬁcient to deceive the model. Then it is added to the
original image, where we use  to balance how much
the resulting image comes from the random image, and
how much comes from the original one.
(cid:2) = Clip((1 − )x +  · x
∗), x
x
∗ = U (xdim
)
(8)
x
d) Salt & Pepper Noise: This type of noise emulates
the malfunctions of the camera’s sensor cell by uniformly
ﬂip the pixel to either white (pixel value 255) or black
(pixel value 0). We use  to control the amount of salt s
and pepper p applied to the original image.
(6)
(7)
(9)
TABLE I: Threat Severity Criteria: x is the original input,
x + δ is the perturbed input. C is the function returns
class label, G is the ground truth of the input. P is
the probability of the input prediction, c = G(x), L is
the multiclass label collection. D is the object detection
function that returns a set of objects o. tc is the target
class. † is the criteria related to object detection task.
Criteria
Misclassiﬁcation
ConﬁdenceMisclassiﬁcation
TopKClsMisclassiﬁcation
Original Class Conﬁdence Loss
TargetClsProbBoost†
TargetClsMisclassiﬁcation†
Description
C(x + δ) (cid:5)= G(x)
P∃l|l∈{L},l(cid:5)=G(x)(x) ≥ thresholdl
C(x + δ) /∈ {CtopK(x)}
Pc(x + δ) = 1 − /2)
p = −(u < /2)
(cid:2) = Clip(x + s + p)
x
5) Weather:
a) Fog: A diamond square algorithm [15] is em-
ployed to emulate fog. It starts with a 2-dimensional
map in the size of (2n + 1, 2n + 1), with four corners
initialized. Within that map, we can ﬁnd both square
shapes and diamond shapes. We run iterations to ﬁll
up the map. For each iteration, we set the center point
of either a square or a diamond to the mean value of
the corresponding corners plus a random variable W.
We attempt various  for W to control the output of
the diamond square algorithm. As indicated in Eq 10,
we also add a coefﬁcient κ to the return value of the
diamond square (DS) algorithm, and a coefﬁcient γw to
the parameter of DS that determines the W.
(cid:2) = Clip(x +  · κ · DS( · γw))
(10)
x
b) Snow: We leverage on motion blur technique to
create a snow ﬁlter in the size of one-hundredth of the
original image. Then we use  to determine the sparsity
of the locations to convolve the ﬁlter over the original
image. The volume of the snow is implicitly done by the
density of the ﬁlters applied.
E. Perturbation Knob
In our framework, Perturbation Knob is an important
component to adjust the amount of perturbations during
the model robustness evaluation. Throughout the Eq. 1
11, we deﬁne an  to inﬂuence the outcome of
to
the perturbation. Then we can apply a knob to search
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:28:00 UTC from IEEE Xplore.  Restrictions apply. 
152
in linear space for . Speciﬁcally, such linear space is
deﬁned within the range [0, 1]. We divide it equally into
k cells, which speciﬁes the granularity of the probes. The
ﬁner the granularity, the tighter the robustness, the more
accurate the result we can get. To achieve a ﬁne-grained
robustness estimate and reduce unnecessary search cost,
we set the k to 1000 in our experiment most of the time.
III. EXPERIMENT
A. Model Robustness in Lp
We conduct a comprehensive study on the model ro-
bustness against safety violations across various models
of different learning tasks, including image classiﬁers,
object detectors, and cloud-based content moderation
models. We use L2 distance throughout our experiments.
The perceptual difference among various magnitudes of
L2 can also be found in Fig 10.
1) Image Classiﬁer: We randomly sample 1k images
from the ImageNet dataset. We choose Misclassi f ication
from the criteria pool. As long as the generated per-
turbation is small enough to make the prediction label
different from the ground truth, the resulting sample
violates the corresponding safety property. We run all
the 1k images per each safety property to get the minpert
in L2 distance. The robustness is calculated based on
Eq. 12. Fig 2 shows comparison results of 5 out of 15
safety properties over 13 pre-trained image classiﬁers.
Among all these models, Densenet and Resnet provide
the highest robustness most of the time. Squeezenet and
Alexnet, on the contrary, are easy to break by a small
amount of perturbation. We also detail out all the 11
pre-trained models we evaluate in Table II. The results
for Resnet18, and Resnet34 are omitted intentionally
because the results of Resnet(50, 101, 152) are sufﬁcient
enough. In summary, all the existing image classiﬁers are
susceptible to a small amount of perturbations.
We also provide the fooling success rate in Fig 3 for
these ﬁve safety properties. We set the mean minimal L2
distance as the threshold T for each property across all
the models. We regard success if an input image needs
less than T perturbation to achieve model misbehavior.
In our result, Densenet and Resnet consistently outper-
form other model architectures with a fooling rate at
38.1% for Brightness test, while that for squeezenet can
be as high as 91.3% for Salt&Pepper test.
It is interesting to ﬁnd that squeezenet with the fewest
number of parameters has the worst robustness, while
alexnet with 60 million parameters consistently ranks
at the bottom over most of the safety violation tests.
We suspect it is because alexnet employs fully con-
nected layers, which results in an excessive number
of parameters and is prone to overﬁtting. Thus the
architecture extremely compact or composed of fully
connected layers probably is not recommended for safe-
critical applications.
Based on our result, the robustness does not have a
clear correlation with the model complexity across differ-
ent neural net structures. A large number of parameters
do not necessarily lead to a more robust model. When
considering structures within a family such as Resnet,
higher complexity seems to provide more robustness. A
Resnet-based architecture, including Densenet, which has
a similar structure with deeper layers that concatenate
previous layers, seems to be outstanding among all the
structures. Thus it seems to be a preferred structure for
safety-critical applications. However, the success of a
neural network is also highly related to the activation
function chosen, dropout strategies applied, and opti-
mization techniques adopted. We will study how these
contribute to the robustness of the model in our future
work.
2) Object Detector:
the state-of-the-art object detectors.
In this paper, we compare the
following object detectors: a) YOLOv3 [16], which is
one of
Its high
throughput makes it suitable for
real-time safety-
critical applications such as autonomous driving. b)
SingleShotDetector(SSD) [17], which is as accurate as
YOLOv3 but runs three times slower. c) RetinaResnet [18]
usually has the best accuracy; however, it takes 198ms to
complete an inference in order to get an accurate result.
We design our experiment as follows: In total, 849
images are selected. Each image contains an object that
is signiﬁcantly large so that all three object detectors
predict that object the same as the ground truth with
high conﬁdence. We do that simply because many im-
ages from the MSCOCO dataset [19] contain very tiny
objects where a misdetection on those tiny objects might
not do severe harm. However, if an apparent large object
is misdetected or misclassiﬁed, it would lead to severe
consequences. Thus we only allow the following images
to be selected:1) The image contains the objects of the
preferred class; 2) That object of the preferred class shall have
an area at least as large as 5% of the entire image. For
, such that
simplicity, we set the preferred class to
all the object detectors agree on the ground truth of all
the selected images. The model robustness is measured
based on Targeted Misclassi f ication criterion such that
the object is no longer predicted as
bus
(cid:2)
(cid:2)
bus