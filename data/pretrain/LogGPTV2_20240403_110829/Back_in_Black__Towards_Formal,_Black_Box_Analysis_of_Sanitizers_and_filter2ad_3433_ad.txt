consuming z≤j0 b and the latter is the last symbol produced by
M when consuming sj0 b, where b = z(cid:5)
j0+1 is the (j0 + 1)-th
symbol of the counterexample. As a result the difference of
γj0 , γj0+1 is in their (|z(cid:5)|−j0−1)-sufﬁxes that by deﬁnition are
equal to the same length sufﬁxes of γM
j0+1. This implies
that j0 j0+1. The observation table augmented by this
new string d is not closed any more: the string sj0 bd = sj0 z(cid:5)
>j0
when queried to M produces the string γM
j0 which disagrees
in its |d|-sufﬁx with the string γM
j0+1 produced by M on input
sj0+1d. Closing the table will now introduce the new access
string sjb and hence the algorithm continues by expanding the
hypothesis machine.
, γM
The approach we outlined above offers a signiﬁcant ef-
ﬁciency improvement over the SG algorithm. Performing the
binary search detailed above requires merely O(log m) queries
where m is the length of the counterexample. This gives a total
of O(n + log m) queries for processing a counterexample as
opposed to the O(n · m) of the SG algorithm where n is the
number of access strings in the observation table.
Handling ε-transitions: We next show how to tackle the
problem of a Mealy machine that takes ε-transitions but still
is deterministic in its output. The effect of such ε-transitions
is that many or no output symbols may be generated due to a
single input symbol. Even though this is a small generalization
it complicates the learning process. First, if more than one
output symbols are produced for each input symbol our coun-
terexample processing method will fail because the breakpoint
output symbol (TM (z))i may be produced by less than i
symbols of z. Further, in the observation table, bookkeeping
will be inaccurate since, if we keep only the suﬀ(TM (sd),|d|)
string in each table entry, then this might not correspond to
the output symbols that correspond to last d symbols of the
input string.
We show next how to suitably modify our bookkeeping
and counterexample processing so that Mealy machines with
ε-transitions are handled.
Instead of keeping in each table entry the string
suﬀ(TM (sd),|d|) we only keep the output that corre-
sponds to the experiment d. While in standard Mealy
machines this is simply suﬀ(TM (sd),|d|), when ε-
transitions are used the output may be longer or
shorter. Therefore, we compute the output of the ex-
periment as the substring of TM (sd) when we subtract
the longest common preﬁx with the string TM (s).
Intuitively, we keep only the part of the output that
is produced by the experiment d. Given that we do
not know the length of that output we subtract the
output produced by the access string s. Notice that,
because the observation table is preﬁx closed, we can
obtain the output TM (s) without making an additional
transduction query to the target M.
– When processing a counterexample, the method we
outlined above can still be used. However, as we men-
tioned, the index i where the output of the hypothesis
and the target machine differ may not be the correct
index in which we must trim the input at. Speciﬁcally,
if TH (z) and TM (z) differ in position i (and i is the
smallest such position), then we are looking for an
index i(cid:5) ≤ i such that TM (z≤i(cid:2) ) = TM (z)≤i. Given
i, such a position i(cid:5) can be found with log |z| queries
using a binary search on the length of the output of
each substring of z. We will then deﬁne z(cid:5)
= z≤i(cid:2).
j ,|γM
j | − j(cid:5)
· suﬀ(γM
Given the above modiﬁcations we will seek j0 via a binary
search as in Theorem 2 but using the strings γj that are
= |TM (sj)|
deﬁned as γH
for j = 0, . . . ,|z(cid:5)|. Then, the same proof as in Theorem 2
j
applies. Further, using a similar logic as before we argue that
the string d = z>j0+1 is non-empty and it can be used as a
new distinguishing string. The asymptotic complexity of the
algorithm will remain the same.
) where j(cid:5)
B. Learning Transducers with Bounded Lookahead
It is easy to see that if the target machine is a single-
valued non-deterministic transducer with the bounded looka-
head property the algorithm of the previous section fails. In
fact the algorithm may not even perform any progress beyond
the initial single state hypothesis even if the number of states
of the target is unbounded; for instance, consider a transducer
that modiﬁes only a certain input symbol sequence w (say
by redacting its ﬁrst symbol) while leaving the remaining
input intact. The algorithm of the previous section will form a
9999
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:23 UTC from IEEE Xplore.  Restrictions apply. 
hypothesis that models the identity function and obtain from
the equivalence oracle, say, the string w as the counterexample
(any string containing w would be a counterexample, but w
is the shortest one). The binary search process will identify
j0 = 0 (it is the only possibility) and will lead the algorithm to
the adoption of d = w>1 as the distinguishing string. However,
bd) = TM (w) = w>1, and also TM (sj0+1d) = w>1
TM (sj0
b ≡ sj0+1 mod W ∪ {d}. At
hence d is not distinguishing: sj0
this moment the algorithm is stuck: the table remains closed
and no progress can be made. For the following we assume that
∗, i.e. for every string
the domain of the target transducer is Σ
∗ such that TM (α) = γ.
∗ there exists exactly one γ ∈ Γ
α ∈ Σ
Technical Description. The algorithm we present builds on
our algorithm of the previous section for Mealy Machines
with ε-transitions. Our algorithm views the single-valued trans-
ducer as a Mealy Machine with ε-transitions augmented with
certain lookahead paths. As in the previous section we use
an observation table OT that has rows on S ∪ S × Σ and
columns corresponding to the distinguishing strings W . In
addition our algorithm holds a lookahead list L of quadraples
(src, dst, α, γ) where src, dst are index numbers of rows in
the OT , α ∈ Σ
∗ is the input string consumed by the lookahead
path, while γ ∈ Γ
∗ is the output produced by the lookahead
path. Whenever a lookahead path is detected, it is added in
the lookahead transition list L. Our algorithm will also utilize
the concept of a preﬁx-closed membership query: In a preﬁx
closed membership query, the input is a string s and the result
is the set of membership queries for all the preﬁxes of s. Thus,
if O is the membership oracle, then a preﬁx-closed member-
ship query on input a string s will return {O(s≤1), . . . , O(s)}.
We will now describe the necessary modiﬁcations in order to
detect and process lookahead transitions.
Detecting and Processing lookahead transitions. Observe
that in a deterministic transducer the result of a preﬁx-closed
query on a string s would be a preﬁx closed set r1, . . . , rt.
The existence of i0 ∈ {1, . . . , t} with ri0 not a strict preﬁx
of ri0+1 suggests that a lookahead transition was followed.
Let rj0 be the longest common preﬁx of r1, . . . , ri0+1. The
state src = sj0 that corresponds to qj0 is the state that the
lookahead path commences while the state dst = si0+1 that
corresponds to input qi0+1 is the state the path terminates. The
path consumes the string α that is determined by the sufﬁx of
qi0+1 starting at the (j0 + 1)-position. The output of the path
is γ = suﬀ(ri0+1,|ri0+1| − |rj0|).
The algorithm proceeds like the algorithm for Mealy ma-
chines with ε-transitions. However, all membership queries are
replaced with preﬁx-closed membership queries. Every query
is checked for a lookahead transition. In case a lookahead
transition is found, it is checked if it is already in the list L. In
the opposite case the quadraple (src, dst, α, γ) is added in L
and all sufﬁxes of α are added as columns in the observation
table. The reason for the last step is that every lookahead
path of length m deﬁnes m − 2 ﬁnal states in the single-
valued transducer. The sufﬁxes of α can be used to distinguish
these states. Finally, when the table is closed, a hypothesis is
generated as before taking care to add the respective lookahead
transitions, removing any other transitions which would break
the single-valuedness of the transducer.
Processing Counterexamples. For simplicity, in this algo-
rith we utilize the Shabaz-Groz counterexample processing
method. We leave the adjustment of our previous binary
search counterexample method as future work. Notice that,
a counterexample may occur either due to a hidden state or
due to a yet undiscovered lookahead transition. We process a
counterexample string as follows: We follow the counterex-
ample processing method of Shabaz Groz and we add all
the sufﬁxes of the counterexample string as columns in the
OT . Since the SG method already adds all sufﬁxes, this also
covers our lookahead path processing. In case we detect a
lookahead we also take care to add the respective transition in
the lookahead list L. Notice that, following the same argument
as in the analysis of the SG algorithm, one of the sufﬁxes will
be distinguishing, thus the table will become not closed and
progress will be made.
Regarding the correctness and complexity of our algorithm
we prove the following theorem.
Theorem 3. The class of non-deterministic single-valued
transducers with the bounded lookahead property and domain
∗ can be learned in the membership and equivalence query
Σ
model using at most O(|Σ|n(mn+|Σ|+kn)(n+max{m, n}))
membership queries and at most n + k equivalence queries
where m is the length of the longest counterexample, n is the
number of states and k is the number of lookahead paths in
the target transducer.
C. Learning Symbolic Finite Transducers
The algorithm for inferring SFAs can be extended naturally
in order to infer SFTs. Due to space constraints we won’t
describe the full algorithm here rather sketch certain aspects
of the algorithm.
The main difference between the SFA algorithm and the
SFT algorithm is that on top of inferring predicates guards,
the learning algorithm for SFTs need to also infer the term
functions that are used to generate the output of each transition.
This implies that there might be more than one transition
from a state si to a state sj due to differences in the term
functions of each transition. This scenario never occurs in
the case of SFAs. Thus, the guardgen() algorithm on an
SFT inference algorithm should also employ a termgen()
algorithm which will work as a submodule of guardgen()
in order to generate the term functions for each transition and
possibly split a predicate guard into more.
Finally, we point out that in our implementation we utilized
a simple SFT learning algorithm which is a direct extension of
our RE ﬁlter learning algorithm in the sense that we generalize
the pair (predicate, term) with the most members to become
the sink transition for each state.
VI.
IMPLEMENTING AN EQUIVALENCE ORACLE
In practice a membership oracle is usually easy to obtain
as the only requirement is to be able to query the target ﬁlter
or sanitizer and inspect the output. However, simulating an
equivalence oracle is not trivial. A straightforward approach is
to perform random testing in order to ﬁnd a counterexample
and declare the machines equal if a counterexample is not
found after a number of queries. Although this is a feasible
approach,
it requires a very large number of membership
queries.
100100
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:23 UTC from IEEE Xplore.  Restrictions apply. 
Taking advantage of our setting, in this section we will
introduce an alternative approach where an equivalence oracle
is implemented using just a single membership query. To
illustrate our method consider a scenario where an auditor is
remotely testing a ﬁlter or a sanitizer. For that purpose the
auditor is in possession of a set of attack strings given as a
context free grammar (CFG).
The goal of the auditor is to either ﬁnd an attack-string
bypassing the ﬁlter or declare that no such string exists and
obtain a model of the ﬁlter for further analysis. In the latter
case, the auditor may work in a whitebox fashion and ﬁnd new
attack-strings bypassing the inferred ﬁlter, which can be used
to either obtain a counterexample and further reﬁne the model
of the ﬁlter or actually produce an attack. Since performing
whitebox testing on a ﬁlter is much easier than black-box,
even if no attack is found the auditor has obtained information
on the structure of the ﬁlter.
Formally, we deﬁne the problem of Grammar Oriented
Filter Auditing as follows:
Deﬁnition 9. In the grammar oriented ﬁlter auditing problem
(GOFA), the input is a context free grammar G and a mem-
bership oracle for a target DFA F . The goal is to ﬁnd s ∈ G,
such that s (cid:11)∈ F or determine that no such s exists.
One can easily prove that in the general case the GOFA
problem requires an exponential number of queries. Simply
consider the CFG L(G) = Σ
∗ and a DFA F such that
∗ \ {random-large-string}. Then, the problem re-
L(F ) = Σ
duces in guessing a random string which requires an exponen-
tial number of queries in the worst case. A formal proof of a
similar result was presented by Peled et al. [23].
Our algorithm for the GOFA problem uses a learning
algorithm for SFAs utilizing Algorithm 1 as an equivalence
oracle. The algorithm takes as input a hypothesis machine H. It
then ﬁnds a string s ∈ L(G) such that s (cid:11)∈ L(H). If the string
s is an attack against the target ﬁlter, the algorithm outputs
the attack-string and terminates. If it is not it returns the string
as a counterexample. On the other hand if there is no string
bypassing the hypothesis, the algorithm terminates accepting
the hypothesis automaton H. Note that,
this is the point
where we trade completeness for efﬁciency since, even though
L(G ∩ ¬H) = ∅, this does not imply that L(G ∩ ¬F ) = ∅.
Algorithm 1 GOFA Algorithm
Require: Context Free Grammar G, membership oracle O
function EQUIVALENCE ORACLE(H)
GA ← G ∩ ¬H
if L(GA) = ∅ then
return Done
else
s ← L(GA)
if O(s) = T rue then
return Counterexample, s
return Attack, s
else
end if
end if
end function
IDS RULES
DFA LEARNING
SFA LEARNING
ID
STATES
ARCS MEMBER
EQUIV MEMBER
EQUIV
SPEEDUP
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
7
16
25
33
52
60
66
70
86
115
135
139
146
164
179
13
35
33
38
155
113
82
99
123
175
339
964
380