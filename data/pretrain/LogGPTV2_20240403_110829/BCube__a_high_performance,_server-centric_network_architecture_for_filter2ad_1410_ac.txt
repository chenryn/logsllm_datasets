the high capacity (e.g., multi-path) of BCube and automat-
ically load-balance the traﬃc. Existing routing protocols
such as OSPF and IS-IS [20] cannot meet these require-
ments. Furthermore, it is unlikely that OSPF and IS-IS
can scale to several thousands of routers [20]. In this paper,
we design a source routing protocol called BSR by leverag-
ing BCube’s topological property. BSR achieves load bal-
ance and fault tolerance, and enables graceful performance
degradation.
4.1 The BSR Idea
In BSR, the source server decides which path a packet
ﬂow should traverse by probing the network and encodes
the path in the packet header. We select source routing for
two reasons. First, the source can control the routing path
without coordinations of the intermediate servers. Second,
intermediate servers do not involve in routing and just for-
ward packets based on the packet header. This simpliﬁes
their functionalities. Moreover, by reactively probing the
network, we can avoid link state broadcasting, which suﬀers
from scalability concerns when thousands of servers are in
operation.
In BSR, a ﬂow can change its path but only uses one path
at a given time, in order to avoid the packet out-of-order
problem. A ﬂow is a stream of packets that have the same
values for a subset of ﬁelds of the packet header, such as the
ﬁve-tuple (src, src port, dst, dst port, prot). We treat a
duplex ﬂow as two separate simplex ﬂows, since the network
conditions along opposite directions may be diﬀerent.
When a new ﬂow comes, the source sends probe packets
over multiple parallel paths. The intermediate servers pro-
cess the probe packets to ﬁll the needed information, e.g.,
the minimum available bandwidth of its input/output links.
The destination returns a probe response to the source.
When the source receives the responses, it uses a metric to
select the best path, e.g., the one with maximum available
bandwidth. In this paper, we use available bandwidth to op-
timize application throughput since we focus on bandwidth-
intensive applications. However, it is possible to use other
metrics such as end-to-end delay.
When a source is performing path selection for a ﬂow, it
does not hold packets. The source initially uses a default
PathSelection(src, dst):
Source:
when a ﬂow arrives or probing timer timeouts:
goodPathSet = { };
pathSet = BuildPathSet(src, dst);
while (pathSet not empty)
path = pathSet.remove();
if (ProbePath(path) succeeds)
goodPathSet.add(path);
else
altPath = BFS(pathSet, goodP athSet);
if(altPath exists) pathSet.add(altP ath);
return SelectBestPath(goodPathSet);
Intermediate server: /*receiver is not pkt.dst*/
when a path probe pkt is received:
if (next hop not available)
send path failure msg to src; return;
ava band = min(ava band in, ava band out);
if (ava band  and  to con-
nect the two BCube0s. The problem faced by this approach
is that BCubeRouting does not work well for some server
pairs. For example, BCubeRouting will not be able to ﬁnd
a path between servers 02 and 13 no matter which routing
permutation is used, because 02 and 13 are connected to
non-existing layer-1 switches. Of course, we still can estab-
lish paths between 02 and 13 by enlarging the path length.
For example, 02 can reach 13 via path {02, 00, 10, 13}. But
this approach reduces network capacity.
The root cause for why server 02 cannot reach server 13
is that we do not have switches  and .
Hence, our solution to partial BCube construction is as fol-
lows. When building a partial BCubek, we ﬁrst build the
needed BCubek−1s, we then connect the BCubek−1s using a
full layer-k switches. With a full layer-k switches, BCubeR-
outing performs just as in a complete BCube, and BSR just
works as before.
An apparent disadvantage of using a full layer-k switches
is that switches in layer-k are not fully utilized. We pre-
fer this solution because it makes routing the same for par-
tial and complete BCubes, and most importantly, the mini-
switches are cheap and aﬀordable. In this paper, we choose
n = 8 and k = 3 and use these parameters to build a partial
BCube with 2048 servers. n = 8 implies that we only need
cheap COTS mini-switches. k = 3 means that each server
has 4 ports, which provides signiﬁcant speedups for one-to-x
and enough fault-tolerance and load-balance.
5.2 Packaging and Wiring
We show how packaging and wiring can be addressed for
a container with 2048 servers and 1280 8-port switches (a
partial BCube with n = 8 and k = 3). The interior size of a
40-feet container is 12m×2.35m×2.38m. In the container,
we deploy 32 racks in two columns, with each column has
16 racks. Each rack accommodates 44 rack units (or 1.96m
high). We use 32 rack units to host 64 servers as the current
practice can pack two servers into one unit [23], and 10 rack
units to host 40 8-port switches. The 8-port switches are
small enough, and we can easily put 4 into one rack unit.
Altogether, we use 42 rack units and have 2 unused units.
As for wiring, the Gigabit Ethernet copper wires can be
100 meters long, which is much longer than the perimeter
of a 40-feet container. And there is enough space to ac-
commodate these wires. We use 64 servers within a rack
to form a BCube1 and 16 8-port switches within the rack
to interconnect them. The wires of the BCube1 are inside
the rack and do not go out. The inter-rack wires are layer-2
and layer-3 wires and we pace them on the top of the racks.
We divide the 32 racks into four super-racks. A super-rack
forms a BCube2 and there are two super-racks in each col-
umn. We evenly distribute the layer-2 and layer-3 switches
into all the racks, so that there are 8 layer-2 and 16 layer-3
switches within every rack. The level-2 wires are within a
super-rack and level-3 wires are between super-racks. Our
calculation shows that the maximum number of level-2 and
level-3 wires along a rack column is 768 (256 and 512 for
level-2 and level-3, respectively). The diameter of an Ether-
net wire is 0.54cm. The maximum space needed is approx-
imate 176cm2 < (20cm)2. Since the available height from
the top of the rack to the ceil is 42cm, there is enough space
for all the wires.
5.3 Routing to External Networks
So far, we focus on how to route packets inside a BCube
network. Internal servers need to communicate with exter-
nal computers in the Internet or other containers. Since we
have thousands of servers in an MDC, the total throughput
to or from external network may be high. We assume that
both internal and external computers use TCP/IP.
We propose aggregator and gateway for external communi-
cation. An aggregator is simply a commodity layer-2 switch
with 10G uplinks. We can use a 48X1G+1X10G aggregator
69reduces slowly and there are no dramatic performance falls.
We assume all the links are 1Gb/s and there are 2048 servers.
This setup matches a typical shipping-container data center.
For all the three structures, we use 8-port switches to con-
struct the network structures. The BCube network we use
is a partial BCube3 with n = 8 that uses 4 full BCube2.
The fat-tree structure has ﬁve layers of switches, with layers
0 to 3 having 512 switches per-layer and layer-4 having 256
switches. The DCell structure is a partial DCell2 which con-
tains 28 full DCell1 and one partial DCell1 with 32 servers.
We use BSR routing for BCube and DFR [9] for DCell. For
fat-tree, we use the routing algorithm described in [1] when
there is no failure and we randomly re-distribute a ﬂow to
an available path when the primary path fails. The results
are plotted in Figures 8(a) and (b) for server and switch
failures, respectively.
The results show that when there is no failure, both BCube
and fat-tree provide high ABT values, 2006Gb/s for BCube
and 1895Gb/s for fat-tree. BCube is slightly better than fat-
tree because the ABT of BCube is n(N−1)
n−1 , which is slightly
higher than that of fat-tree, N . But DCell only provides
298Gb/s ABT. This result is due to several reasons. First,
the traﬃc is imbalanced at diﬀerent levels of links in DCell.
Low-level links always carry much more ﬂows than high-level
links. In our simulation, the maximum numbers of ﬂows in
the level-0 - level-2 links are 14047, 9280, and 5184, respec-
tively. Second, partial DCell makes the traﬃc imbalanced
even for links at the same level. In our simulation, the max-
imum and minimum numbers of ﬂows in the level-0 links
are 14047 and 2095, respectively. This huge diﬀerence is
because there are level-0 links that seldom carry traﬃc for
other servers in a partial DCell.
Fat-tree performs well under server failures but its ABT
drops dramatically when switch failure increases (e.g., 1145Gb/s
at 2% switch failure and 704Gb/s at 6% switch failure). Our
analysis revealed that the dramatic drop is caused by low-
level switch failures. In fat-tree, switches at diﬀerent layers
have diﬀerent impact on routing performance. When a level-
1 switch fails, an aﬀected server has only n
re-route, whereas it has ( n
switch failure. Hence the failures of low-level switches make
the traﬃc imbalanced in fat-tree and degrade the perfor-
mance dramatically.
2 − 1 choices to
2 )2 − 1 to re-route for a level-2
BCube performs well under both server and switch fail-
ures. Compared with fat-tree, switches at diﬀerent layers
are equal in BCube (recall that the multi-paths we build
in Section 3.3 use switches at diﬀerent levels equally).
In
BCube, live servers always have 4 live links under the server
failure model whereas some live servers may have less than
4 live links under switch failures. This diﬀerence results less
balanced traﬃc and therefore smaller ABT under the switch
failure model. But the degradation is graceful. The ABT
value is 765Gb/s even when the switch failure ratio reaches
20% (as a comparison, it is only 267Gb/s for fat-tree).
7.
7.1
IMPLEMENTATION AND EVALUATION
Implementation Architecture
We have prototyped the BCube architecture by design-
ing and implementing a BCube protocol stack. We have
implemented the stack as a kernel driver in the Windows
Servers 2003 and 2008. The BCube stack locates between
(a)
(b)