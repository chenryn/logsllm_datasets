title:PHYS: Profiled-HYbrid Sampling for soft error reliability benchmarking
author:Jinho Suh and
Murali Annavaram and
Michel Dubois
PHYS: Profiled-HYbrid Sampling for Soft Error Reliability Benchmarking 
Ming Hsieh Department of Electrical Engineering, University of Southern California 
Jinho Suh, Murali Annavaram, and Michel Dubois 
Los Angeles, CA, USA 
{jinhosuh, annavara}@usc.edu, PI:EMAIL 
a 
sampling 
for 
framework 
Abstract—In this paper, we introduce PHYS (Profiled-HYbrid 
Sampling), 
soft-error 
benchmarking of caches. Reliability simulations of caches are 
much  more  complex  than  performance  simulations  and 
therefore  exhibit  large  simulation  slowdowns  (two  orders  of 
magnitude)  over  performance  simulations.  The  major 
problem is that the reliability lifetime of every accessed block 
must  be  tracked  from  beginning  to  end,  on  top  of  simulating 
the  benchmark,  in  order  to  track  the  total  number  of 
vulnerability  cycles  (VCs)  between  two  accesses  to  the  block. 
Because  of  the  need  to  track  SDCs  (silent  error  corruption) 
and to distinguish between true and false DUEs (detected but 
unrecoverable errors) vulnerability cycles cannot be truncated 
when  data  is  written  back  from  cache  to  main  memory. 
Vulnerability cycles must be maintained even during a block's 
sojourn in main memory to track whether corrupted values in 
a block are used by the processor, until program termination.  
PHYS  solves  this  problem  by  sampling  intervals  between 
accesses  to  each  memory  block,  instead  of  sampling  the 
execution  of  the  processor  in  a  time  interval  as  is  classically 
done in performance simulations. At first a statistical profiling 
phase  captures  the  distribution  of  VCs  for  every  block.  This 
profiling step provides a statistical guarantee of the minimum 
sampling rate of access intervals needed to meet a desired FIT 
error target with a given confidence interval. Then, per cache-
set  sampling  rates  are  dynamically  adjusted  to  sample  VCs 
with  higher  merit.  We  compare  PHYS  with  many  other 
possible  sampling methods,  some  of which are widely used to 
accelerate performance-centric simulations but have also been 
applied 
lifetime.  We 
demonstrate  the  superiority  of  PHYS  in  the  context  of 
reliability  benchmarking  through  exhaustive  evaluations  of 
various sampling techniques. 
in  the  past  to  track  reliability 
Keywords-component;  Soft-error;  reliability  benchmarking; 
simulation sampling 
I. 
INTRODUCTION 
To  mitigate  the  soft-error  vulnerability  of  caches, 
researchers  have  proposed  a  plethora  of  error  correcting 
codes,  such  as  Single  Error  Correction,  Double  Error 
Detection  (SECDED)  codes,  Double  Error  Correction, 
Triple  Error  Detection  (DECTED)  codes  and  interleaved-
ECC  [15].  In  this  context,  it  is  important  to  evaluate  the 
cost/benefit tradeoffs of various protection schemes because 
error  protection  mechanisms  are  expensive  in  terms  of 
silicon area, energy and performance.  
Mukherjee  et  al.  [16]  introduced  the  Architectural 
Vulnerability  Factor  (AVF),  which  de-rates  the  overly 
pessimistic  intrinsic  FIT  rate  projected  by  ITRS  [18]  by 
measuring the fraction of time the data stored in a structure 
can impact the outcome of program execution. As a single-
bit  fault  dominant  model  (a  single  bit  is  flipped  over  any 
period  of  time)  AVF  is  an  acceptable  methodology  to 
benchmark  the  soft-error  reliability  of  caches  that  are  not 
protected  by  error  correcting  codes.  However, 
in 
microprocessors 
for  commercial  servers,  caches  are 
protected  by  strong  error-protection  codes.  Intel  Xeon 
processors  protect  their  higher-level  caches  with  SECDED 
and  their  lower-level  caches  with  DECTED  [8].  AVF  is 
unable to quantify the reliability of such processor caches. 
PARMA  (Precise  Analytical  Reliability  Model  for 
Architecture)  [22]  is  a  more  accurate  model  for  cache 
reliability.  PARMA  can  model  temporal  multi-bit  failures 
and  hence  can  accurately  measure  the  FIT  rate  of  caches 
protected by complex error correction schemes. MACAU (a 
MArkov  model for  reliability  evaluations of  CAches under 
single-bit and multi-bit Upsets) [23] models the overlapping 
effects  of  SBUs  (Single-bit  upsets)  and  spatial  MBUs 
(multi-bit upsets) over time, to benchmark caches protected 
by various codes. These benchmarking tools are valuable to 
designers  having  to  decide  between,  say,  parity,  SECDED, 
DECTED  with  or  without  interleaved  configurations  for 
large, lower-level caches. 
The  basic  premise  in  AVF,  PARMA  and  MACAU  is 
that  the  vulnerability  of  a  bit  increases  with  the  amount  of 
time  it  is  exposed  to  particle  hits  (called  Single-Event 
Upsets  or  SEUs).  In  order  to  measure  the  exposure  time 
these models rely on rigorous reliability-lifetime analysis [3] 
to  track  the  amount  of  time  every  memory  word  is 
vulnerable. In reliability studies, the reliability lifetime of a 
bit  is  defined  as  the  cumulative  time  a  bit  has  spent  in  the 
vulnerable structure(s) until the bit is read by the processor 
and affects the outcome of the program. Because of the need 
to  track  SDCs  (silent  error  corruptions)  and  to  distinguish 
between  true  and  false  DUEs  (detected  but  unrecoverable 
errors) vulnerability cycles must be tracked until it is known 
that a corrupted data will not be used by the processor, until 
program  termination.  This  is  very  different  from  the 
classical  definition  of  the  lifetime  of  a  block  in  a  cache, 
which simply  measures  the amount  of time  a block  resides 
in  a cache on  each  sojourn without  considering the  block’s 
previous  sojourns.  Reliability  lifetime  analysis  needs  to 
 1
978-1-4799-0181-4/13/$31.00 ©2013 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:38:09 UTC from IEEE Xplore.  Restrictions apply. 
accumulate the amount of time a block is vulnerable across 
multiple  sojourns  as  memory  block  may  move  back  and 
forth  between  caches  and  memory.  The  more  time  a  bit 
spends in the vulnerable structure(s), the likelier it will fail. 
Reliability-lifetime analysis is about tracking bits moving in 
and out of vulnerable structures and measuring the time they 
spent  in  these  structures.  Even  when  data  moves  out  of  a 
vulnerable structure the time spent in the structure must still 
be  associated  with  that  data  since  the  probability  of  failure 
accumulates  with  the  residence  time,  irrespective  of  how 
often data has moved in and out of the vulnerable structure.  
Data inside processors’ internal storage buffers, such  as 
Load-Store  queues  or  reorder  buffers,  typically  stays  alive 
for  very  short  periods  of  time  and  hence  keeping  track  of 
their  reliability  lifetime  is  easy.  As  soon  as  the  data  in  the 
internal  storage  structures  is  dead  vulnerability  cycles 
associated  with  it  can  be  deleted.  By  contrast,  as  noted  in 
previous  studies  [15][16][22],  when  the  AVF,  PARMA  or 
MACAU  framework  is  applied  to  caches,  the  memory  and 
computation  overheads  due  to  reliability-lifetime  analysis 
result in two orders  of  magnitude slowdowns in simulation 
speed.  
The contributions of this paper are as follows. 
1.  We  show  that  reliability  simulations  are  much  slower 
than  performance  simulations,  which  makes  sampling 
even more critical for reliability. The slowdown is due 
to the complexity of tracking reliability lifetimes. 
2.  We  demonstrate  that  classical  sampling  techniques 
fine-tuned  for  performance  evaluations 
(such  as 
SimPoint [20] and SMARTS [26]) fail in the context of 
reliability  simulations.  The  reason  is  that  the  samples 
are  too  short  to  capture  the  most  significant  access 
intervals.  
3.  Based on this observation we introduce a new sampling 
approach  called  Profile-HYbrid  Sampling  (PHYS),  an 
adaptive  combination  of  set  and  interval  sampling.  A 
major novelty  of this  approach is to  control estimation 
errors by profiling the benchmark first to determine the 
interval sampling rate. 
4.  Through  extensive  evaluations  on  MinneSPEC  we 
show  that  PHYS  is  superior  to  a  large  number  of 
possible  sampling  techniques  in  terms  of  simulation 
slowdown and reliability estimation error. The speedup 
is  such  that  reliability  simulation  time  is  only  a  small 
multiple of the time of performance simulations and the 
simulation error is only a few percent. 
The conclusion is that the accurate reliability simulation 
of entire benchmark suites such as SPEC2K is now possible 
with PHYS. 
II.  BACKGROUND AND MOTIVATION 
A.  Reliability-lifetime analysis 
Calculating  the  number  of  cycles  a  bit  resides  in  a 
vulnerable  structure  and  is  exposed  to  soft  error  strikes 
requires reliability-lifetime  analysis. The reliability  lifetime 
is  referred  to  as  ACE  (Architecturally  Correct  Execution) 
cycles in AVF parlance, and it is called VCs (Vulnerability 
Clock  Cycles)  in  PARMA  and  MACAU.  Reliability-
lifetime  analysis  is  the  primary  culprit  for  the  large 
simulation slowdowns observed in reliability simulations.  
Let  us  take  a  concrete  look  at  how  reliability-lifetime 
analysis  is  conducted  for  an  L2  cache,  assuming  all  other 
components  in  the  system  are  fault-free.  We  adopted  a 
binary  search  tree  (BST)  to  track  reliability  lifetimes,  after 
exploring  other  data  structures  such  as  linked  lists  or  hash 
tables. When a program accesses a word of memory for the 
first time an entire block of data is brought into the L2 cache. 
As  soon  as  the  block  enters  L2  all  words  in  the  block 
become vulnerable. We then create a node in the BST with 
the block address as an index. A timestamp registering when 
the word entered L2 is attached to each word in that block. 
When a block is loaded in L1 two copies of the same block 
coexist, one in L1 and one in L2. Each word of the block is 
now  tagged  with  an  additional  L1  timestamp  indicating 
when  the  word  entered  L1.  If  the  processor  reads  a  word 
from  L1  then  the  cycles  that  a  word  spent  in  L2  before 
moving  to  L1  (L1  timestamp  –  L2  timestamp)  are  counted 
as  VCs.  The  block  in  L1  may  be  subsequently  evicted  and 
brought back from L2 to L1 many times, which requires us 
to save both  L1  and  L2 timestamps  for  each  word.  Even if 
the  block  is  evicted  to  memory  we  still  need  to  store  the 
vulnerability cycles of the block in memory since the block 
can  be  brought  back  to  L2  in  future  and  its  vulnerability 
cycles  from  the  past  sojourn  in  L2  must  be  accounted  for. 
Hence,  the  timestamps  associated  with  a  block  cannot  be 
discarded to reduce the memory footprint unless the block is 
guaranteed  never  to  be  accessed  again  before  program 
termination.  Thus  reliability-lifetime  analysis  ends  up 
consuming five to ten times the amount of memory accessed 
by  a  benchmark,  depending  on  the  size  of  timestamps.  In 
addition to the memory overhead, every time the benchmark 
accesses a different cache block AVF, PARMA or MACAU 
simulations  must  access  a  different  node  in  the  BST 
structure  resulting  in  unpredictable  accesses  to  the  BST, 
which  leads  to  significant  simulation  slowdowns.  Our 
evaluations using VTune [8] have revealed that long latency 
memory instructions due to cache misses and TLB misses in 
the  host  caused  by  random  accesses  to  the  BST  are  the 
leading cause for the dramatic slowdowns.  
During 
our 
initial 
implementation 
phase  we 
implemented hash tables for tracking VCs. But we could not 
even finish most simulations because the growth of the hash 
table as simulation  progressed  (by  adding  keys) resulted  in 
severe memory thrashing.  In Sim-SODA [7] the reliability-
lifetime  analysis  is  implemented  with  linked  lists,  which 
suffer  from  more  serious  slowdowns  than  BSTs.  We 
eventually concluded that BST was the best choice, because 
reliability  simulations  tend  to  search  and  add  nodes  to  the 
data structure rather than to remove nodes from it.  
B.  Soft-error benchmarking frameworks 
If  a  fault  in  the  memory  cells  propagates  to  the  outer 
scope, i.e. faulty bits in the L2 are copied to L1 and are not 
detected or are detected but not corrected, the fault becomes 
an error. Errors become failures if they cause the system to 
halt  or  crash.  Failures  are  categorized  into  true  DUE 
2
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:38:09 UTC from IEEE Xplore.  Restrictions apply. 
y
t
i
s
n
e
D
0
3
0
.
0
2
0
.
0
1
0
.
0
0
0
.
y
t
i
s
n
e
D
.
0
1
.
8
0
.