├─vda1 252:1 0 1G 0 part /boot
└─vda2 252:2 0 19G 0 part
├─rhel-root 253:0 0 17G 0 lvm /
└─rhel-swap 253:1 0 2G 0 lvm \[SWAP\]
\[root@Web1111 \~\]# parted /dev/sdb mklabel gpt
警告: The existing disk label on /dev/sdb will be destroyed and all data
on
this disk will be lost. Do you want to continue?
是/Yes/否/No? n
\[root@Web1111 \~\]# iscsiadm \--mode discoverydb \--type sendtargets
\--portal 192.168.2.5 \--discover
192.168.2.5:3260,1 iqn.2018-01.cn.tedu:server1
\[root@Web1111 \~\]# lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
sda 8:0 0 20G 0 disk
└─sda1 8:1 0 20G 0 part
sdb 8:16 0 20G 0 disk
└─sdb1 8:17 0 20G 0 part
sr0 11:0 1 1024M 0 rom
vda 252:0 0 20G 0 disk
├─vda1 252:1 0 1G 0 part /boot
└─vda2 252:2 0 19G 0 part
├─rhel-root 253:0 0 17G 0 lvm /
└─rhel-swap 253:1 0 2G 0 lvm \[SWAP\]
提示：登陆的是同一个服务器的同一个iSCSI，但客户端看到的是两个独立的设备，sda和sdb。其实，这两个设备是同一个设备。
### 4）设置开机自启动
iscsi用于自动login远程存储，iscsid是守护进程。
\[root@web1 \~\]# systemctl enable iscsid
\[root@web1 \~\]# systemctl enable iscsi
## 步骤四：配置Multipath多路径
### 1）安装多路径软件包
\[root@web1 \~\]# yum list \| grep multipath
device-mapper-multipath.x86_64 0.4.9-111.el7 Server
device-mapper-multipath-libs.i686 0.4.9-111.el7 Server
device-mapper-multipath-libs.x86_64 0.4.9-111.el7 Server
\[root@web1 \~\]# yum install -y device-mapper-multipath
### 2）生成配置文件
\[root@web1 \~\]# cd /usr/share/doc/device-mapper-multipath-0.4.9/
\[root@web1 \~\]# ls multipath.conf
\[root@web1 \~\]# cp multipath.conf /etc/multipath.conf
### 3）获取wwid
登陆共享存储后，系统多了两块硬盘，这两块硬盘实际上是同一个存储设备。应用服务器使用哪个都可以，但是如果使用sdb时，sdb对应的链路出现故障，它不会自动切换到sda。
为了能够实现系统自动选择使用哪条链路，需要将这两块磁盘绑定为一个名称。
通过磁盘的wwid来判定哪些磁盘是相同的。
取得一块磁盘wwid的方法如下：
\[root@Web1\~\]# /usr/lib/udev/scsi_id \--whitelisted \--device=/dev/sdb
36001405ddff5519835340169d64d5cfb
\[root@Web1\~\]# /usr/lib/udev/scsi_id \--whitelisted \--device=/dev/sda
36001405ddff5519835340169d64d5cfb
### 4）修改配置文件
首先声明自动发现多路径：
\[root@web1 \~\]# vim /etc/multipath.conf
defaults {
user_friendly_names yes
find_multipaths yes
}
然后在文件的最后加入多路径声明，如果哪个存储设备的wwid和第（3）步获取的wwid一样，那么，为其取一个别名，叫mpatha。
multipaths {
multipath {
wwid \"360014059e8ba68638854e9093f3ba3a0\"
alias mpatha
}
}
## 步骤五：启用Multipath多路径，并测试
注意：如果做案例1时，已经挂载了iSCSI设备，一定要先umount卸载掉再启动多路径。
### 1）启动Multipath，并设置为开机启动
\[root@web1 \~\]# systemctl start multipathd
\[root@web1 \~\]# systemctl enable multipathd
### 2）检查多路径设备文件
如果多路径设置成功，那么将在/dev/mapper下面生成名为mpatha的设备文件：
\[root@web1 \~\]# ls /dev/mapper/
control mpatha mpatha1
### 3）对多路径设备文件执行分区、格式化、挂载操作
提示：如果前面已经对iscsi做过分区操作，则这里可以直接识别到mpatha1（就不需要再次分区了）。
\[root@web1 \~\]# fdisk -cu /dev/mapper/mpatha
Device contains neither a valid DOS partition table, nor Sun, SGI or OSF
disklabel
Building a new DOS disklabel with disk identifier 0x205c887e.
Changes will remain in memory only, until you decide to write them.
After that, of course, the previous content won\'t be recoverable.
Warning: invalid flag 0x0000 of partition table 4 will be corrected by
w(rite)
Command (m for help): n ＃创建分区
Command action
e extended
p primary partition (1-4)
p ＃分区类型为主分区
Partition number (1-4): 1 ＃分区编号为1
First sector (2048-4194303, default 2048): ＃起始扇区回车
Using default value 2048
Last sector, +sectors or +size{K,M,G} (2048-4194303, default 4194303):
＃回车
Using default value 4194303
Command (m for help): w ＃保存并退出
The partition table has been altered!
Calling ioctl() to re-read partition table.
新的分区名称应该是/dev/mapper/mpathap1，如果该文件不存在，则执行以下命令进行配置的重新载入：
\[root@web1 \~\]# ls /dev/mapper/ ＃再次查看，将会看到新的分区
control mpatha mpatha1
创建目录并挂载（如果已经格式化，这里就不需要再次格式化，直接挂载即可）：
\[root@web1 \~\]# mkfs.xfs /dev/mapper/mpatha1
\[root@web1 \~\]# mkdir /data
\[root@web1 \~\]# mount /dev/mapper/mpatha1 /data/
\[root@web1 \~\]# df -h /data/
Filesystem Size Used Avail Use% Mounted on
/dev/mapper/mpatha1 20G 3.0M 19G 1% /data
### 4）验证多路径
查看多路径，sda和sdb都是running状态。
\[root@web1 \~\]# multipath -rr
reload: mpatha (360014059e8ba68638854e9093f3ba3a0) undef LIO-ORG ,store
size=9.3G features=\'0\' hwhandler=\'0\' wp=undef
\|-+- policy=\'service-time 0\' prio=1 status=undef
\| \`- 2:0:0:0 sda 8:0 active ready running
\`-+- policy=\'service-time 0\' prio=1 status=undef
\`- 3:0:0:0 sdb 8:16 active ready running
关闭某个链路后，再次查看效果，此时会发现sdb为运行失败状态。
\[root@web1 \~\]# nmcli connection down eth1
\[root@web1 \~\]# multipath -rr
reject: mpatha (360014059e8ba68638854e9093f3ba3a0) undef LIO-ORG ,store
size=9.3G features=\'0\' hwhandler=\'0\' wp=undef
\|-+- policy=\'service-time 0\' prio=0 status=undef
\| \`- 2:0:0:0 sda 8:0 active undef running
\`-+- policy=\'service-time 0\' prio=0 status=undef
\`- 3:0:0:0 sdb 8:16 active faulty running
使用-ll选项查看，仅sda为有效运行状态。
\[root@web1 \~\]# multipath -ll
reject: mpatha (360014059e8ba68638854e9093f3ba3a0) undef LIO-ORG ,store
size=9.3G features=\'0\' hwhandler=\'0\' wp=undef
\`-+- policy=\'service-time 0\' prio=0 status=undef
\`- 2:0:0:0 sda 8:0 active undef running
# 3 案例3：配置并访问NFS共享
3.1 问题
服务器利用NFS机制发布2个共享目录，要求如下：
将目录/root共享给192.168.2.100，客户机的root用户有权限写入
将/usr/src目录共享给192.168.2.0/24网段，只开放读取权限
从客户机访问NFS共享：
分别查询/挂载上述NFS共享目录
查看挂载点目录，并测试是否有写入权限
3.2 方案
使用2台RHEL7虚拟机，其中一台作为NFS共享服务器（192.168.2.5）、另外一台作为测试用的Linux客户机（192.168.2.100），如图-10所示。
![image009](media/image6.png){width="4.075694444444444in"
height="1.4666666666666666in"}
NFS共享的配置文件：/etc/exports 。
配置记录格式：文件夹路径 客户地址1(控制参数.. ..) 客户地址2(.. ..) 。
3.3 步骤
实现此案例需要按照如下步骤进行。
## 步骤一：配置NFS服务器，发布指定的共享
### 1）确认服务端程序、准备共享目录
软件包nfs-utils用来提供NFS共享服务及相关工具，而软件包rpcbind用来提供RPC协议的支持，这两个包在RHEL7系统中一般都是默认安装的：
\[root@proxy \~\]# rpm -q nfs-utils rpcbind
nfs-utils-1.3.0-0.48.el7.x86_64
rpcbind-0.2.0-42.el7.x86_64
根据本例的要求，需要作为NFS共享发布的有/root、/usr/src这两个目录：
\[root@proxy \~\]# ls -ld /root /usr/src/
dr-xr-x\-\--. 35 root root 4096 1月 15 18:52 /root
drwxrwxr-x+ 4 root root 4096 1月 15 17:35 /usr/src/
### 2）修改/etc/exports文件，添加共享目录设置
默认情况下，来自NFS客户端的root用户会被自动降权为普通用户，若要保留其root权限，注意应添加no_root_squash控制参数(没有该参数，默认root会被自动降级为普通账户)；另外，限制只读的参数为ro、可读可写为rw，相关配置操作如下所示：
\[root@proxy \~\]# vim /etc/exports
/root 192.168.2.100(rw,no_root_squash)
/usr/src 192.168.2.0/24(ro)
### 3）启动NFS共享相关服务，确认共享列表
依次启动rpcbiind、nfs服务：
\[root@proxy \~\]# systemctl restart rpcbind ; systemctl enable rpcbind
\[root@proxy \~\]# systemctl restart nfs ; systemctl enable nfs
使用showmount命令查看本机发布的NFS共享列表：
\[root@proxy \~\]# showmount -e localhost
Export list for localhost:
/usr/src 192.168.2.0/24
/root 192.168.2.100
## 步骤二：从客户机访问NFS共享
### 1）启用NFS共享支持服务
客户机访问NFS共享也需要rpcbind服务的支持，需确保此服务已开启：
\[root@web1 \~\]# systemctl restart rpcbind ; systemctl enable rpcbind
### 2）查看服务器提供的NFS共享列表
\[root@web1 \~\]# showmount -e 192.168.2.5
Export list for 192.168.2.5:
/usr/src 192.168.2.0/24
/root 192.168.2.100
### 3）从客户机192.168.2.100访问两个NFS共享，并验证权限
将远程的NFS共享/root挂载到本地的/root5文件夹，并验证可读可写：
\[root@web1 \~\]# mkdir /root5 //建立挂载点
\[root@web1 \~\]# mount 192.168.2.5:/root /root5 //挂载NFS共享目录
\[root@web1 \~\]# df -hT /root5 //确认挂载结果
Filesystem Type Size Used Avail Use% Mounted on
192.168.2.5:/root nfs 50G 15G 33G 31% /root5
\[root@web1 \~\]# cd /root5 //切换到挂载点
\[root@web1 root5\]# echo \"NFS Write Test\" \> test.txt //测试写入文件
\[root@web1 root5\]# cat test.txt //测试查看文件
NFS Write Test
将远程的NFS共享/usr/src挂载到本地的/mnt/nfsdir，并验证只读：
\[root@web1 \~\]# mkdir /mnt/nfsdir //建立挂载点
\[root@web1 \~\]# mount 192.168.2.5:/usr/src /mnt/nfsdir/
//挂载NFS共享目录
\[root@web1 \~\]# df -hT /mnt/nfsdir/ //确认挂载结果
Filesystem Type Size Used Avail Use% Mounted on
192.168.2.5:/usr/src nfs 50G 15G 33G 31% /mnt/nfsdir
\[root@web1 \~\]# cd /mnt/nfsdir/ //切换到挂载点
\[root@web1 nfsdir\]# ls //读取目录列表
debug install.log kernels test.txt
\[root@web1 nfsdir\]# echo \"Write Test.\" \> pc.txt //尝试写入文件失败
-bash: pc.txt: 只读文件系统
！！！！
如果从未授权的客户机访问NFS共享，将会被拒绝。比如从NFS服务器本机尝试访问自己发布的/root共享（只允许192.168.2.100访问），结果如下所示：
\[root@proxy \~\]# mkdir /root5
\[root@proxy \~\]# mount 192.168.2.5:/root /root5
mount.nfs: access denied by server while mounting 192.168.2.5:/root
### 4）设置永久挂载
\[root@web1 \~\]# vim /etc/fstab
.. ..
192.168.2.5:/usr/src nfsdir nfs default,ro 0 0
192.168.2.5:/root root5 nfs default 0 0
# 4 案例4：编写udev规则
4.1 问题
编写udev规则，实现以下目标：
当插入一个U盘时，该U盘自动出现一个链接称为udisk