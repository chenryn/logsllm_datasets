however that router-level late-binding proposals relying on a
GNS-like infrastructure have also been proposed [44, 4]). If
an endpoint moves mid-connection after the other endpoint
has moved but before it could re-synchronize the connection
(simultaneous mobility), one or both endpoint(s) eventually
query the GNS and re-synchronize the connection.
3.1 Design goals
Domain$name$system$
Root$name$service$(ICANN,$
US.$Dept.$of$Commerce)$
2$
Global$name$system$
Name$
CerBﬁcate$
cerBﬁcaBon$
search$
service$
service$
TLD$
name$
service$
3$
3$
1$
Managed$
authoritaBve$
DNS$service$
Auth.$
name$
service$
4$
4$
Local$name$
service$
Hierarchical.names.with.
federaJon.Jghtly.bound.to.
name.structure.
Auspice$
global$name$
service$
1$
I
G
U
D
=
X
,
.
G
N
S
=
A
u
s
p
i
c
e
.
.
.
)
h
c
r
a
e
s
.
d
e
s
a
b
7
d
r
o
w
y
e
k
.
r
o
(
.
1
e
n
o
h
p
.
:
8
7
1
2
h
t
i
m
S
n
h
o
J
Local$name$
service$
Arbitrary.human7readable.names.
and.ﬂat.GUIDs.with.federaJon.by.
indirecJon.via.cerJﬁcaJon.services.
0$
Figure 2: DNS vs. GNS: Auspice can be deployed as a
managed DNS provider today (left) or as a GNS provider
that provides resolution service for its customer GUIDs
(right). Name certiﬁcation services bind a human-
readable name to a GUID and its GNS provider, and
certiﬁcate search services can help index and distribute
certiﬁcates from all certiﬁcation services. Solid (dotted)
lines represent frequent (infrequent) query paths for a
given mobile destination. Except for the tightly con-
trolled DNS root service, all services above are designed
to be purveyed competitively.
Much of the envisioned functionality of a GNS as above
boils down to one over-arching distributed systems chal-
lenge: any principal–endpoint or router–should get the look
and feel of a high-availability name service that is nearby (≈
few milliseconds) and rapidly returns up-to-date responses.
A more precise breakdown of goals is as follows.
(1) Time-to-connect performance: The design must
ensure low latencies for name lookups to return up-to-date
values, which determines the time to connect to a destination
when the value being queried for is an address like above.
(2) Resource cost: The design must ensure low repli-
cation cost. A naive way to minimize lookup latencies is to
replicate every name record at every possible location, how-
ever high mobility means high update rates, so the cost of
pushing each update to every replica would be prohibitive.
Worse, load hotspots can actually degrade lookup latencies.
(3) High availability: The design must ensure resilience
to node failures including outages of entire datacenters; by
consequence, it should also prevent crippling load hotspots.
250
(4) Security: The design must be robust to malicious
users attempting to hijack or corrupt name records. The
design must support ﬂexible access control policies to ensure
the desired level of privacy of name records.
(5) Federation: The design must allow diﬀerent name
service providers to co-exist and for users to freely choose
one or more preferred providers.
(6) Extensibility: The design must be agnostic to how
names, addresses, and resolution policies are represented by
a future Internetwork. In particular, it should support ﬂat
names and a rich set of attributes and resolution policies for
multi-homed mobility (e.g.,“prefer WiFi to cellular”), etc.
3.2 Design overview
To address the above goals, the Auspice GNS is designed
as a massively geo-distributed key-value store. The geo-
distribution is essential to the latency and availability goals
while the key-value API enables extensibility. Each name
record in Auspice is associated with a globally unique identi-
ﬁer (GUID) that is the record’s primary key. A name record
contains an associative array of key-value pairs, wherein each
key Ki is a string and the value Vi may be a string, a prim-
itive type, or recursively a key-value pair, as shown below.
GUID | K1, V1 | K2, V2 |···
The GUID is a self-certifying identiﬁer computed as a
compact one-way hash of a public key. Each name record
is aliased to one or more globally unique human-readable
names that are bound to the GUID by a certiﬁcate supplied
by one or more name certiﬁcation service(s) (NCS). Loosely
speaking, the human-readable name is analogous to a DNS
domain name and a name record to a zone ﬁle, but with the
following important diﬀerences.
Security. As shown in Fig. 2, to initiate communica-
tion with a destination Y, an endpoint X must ﬁrst obtain
a certiﬁcate of the form [JohnSmith2178:Phone1, Y, P ]K−
that binds the human-readable name to the GUID Y and
its GNS provider P, and is signed by the private key K− of
an NCS that X trusts. A certiﬁcate search service (e.g., a
search engine or ISP) can help index certiﬁcates from diﬀer-
ent NCSes, and help X ﬁnd a certiﬁcate from a trusted NCS,
and even ﬁnd the human-readable name based on keywords.
Federation. Unlike ICANN and root DNS servers that
respectively act as a single name adjudication authority and
root of trust for certiﬁcation, our approach decentralizes
trust across diﬀerent NCS providers, potentially allowing
endpoints to use quorum-based approaches to resolve con-
ﬂicting name certiﬁcates. More importantly, our federa-
tion approach allows endpoints to select arbitrary human-
readable names and NCS providers unlike DNS that restricts
domain names to be hierarchical and federation and the
DNSSEC keychain to strictly follow the name structure. An
inevitable implication of decentralizing trust is that two end-
points can communicate securely only if they share a trusted
NCS provider, but this change we argue is preferable to and
a strict generalization of the single-root-of-trust model that
some perceive as arbitrary [10, 9].
Extensibility. Our design cleanly separates the GNS
provider’s resource-intensive responsibility of name resolu-
tion under high mobility from the slow-changing certiﬁcation
process. It also allows for the GNS provider to be deployed
today as a managed authoritative DNS provider (Fig. 2)
with the DNSSEC key deriving the GUID. Finally, the key-
value store API enables an extensible name record represen-
Paxos&
N1:&Replica/controllers&
Update'replica'
loca.ons'
Report'load'
N1:&Ac<ve&replicas&
Paxos&
&
e
m
a
N
&
r
e
v
r
e
s
North&America&
Typical'
request'
Europe&
N2:&Ac<ve&replicas&
First'request'
User&
N2:&Replica/&
controllers&
Asia&
Paxos&
Figure 3: Geo-distributed name servers in Auspice.
Replica-controllers (logically separate from active
replicas) decide placement of active replicas and ac-
tive replicas handle requests from end-users. N1 is
a globally popular name and is replicated globally;
name N2 is popular in select regions and is repli-
cated in those regions.
tation. By default, each top-level key has associated read
and write ACLs that could either be a blacklist or whitelist
of GUIDs that respectively have read or write access. For ex-
ample, a name record for GUID X that helps context-aware
delivery or multihoming policies (detailed in §4.3.3) is below.
{X: {IPs:[{IP: 23.55.66.43, plan: Unlimited}, {IP:
62.44.65.75, plan: Limited}], geoloc: {[lat,long],
readWhitelist:[Y,Z]}, multihome_policy: Unlimited}}
3.3 Auspice’s geo-distributed design
Next, we explain how Auspice achieves the ﬁrst three de-
sign goals. At the core of Auspice is a placement engine that
achieves the latency, cost, and availability goals by adapting
the number and locations of replicas of each name record in
accordance with (1) the lookup and update request rates for
the name, (2) the geo-distribution of requests for the name,
and (3) the aggregate request load across all names.
Figure 3 illustrates the placement engine. Each name is
associated with a ﬁxed number, F , of replica-controllers and
a variable number of active replicas of the corresponding
name record. The name’s replica-controllers are computed
using consistent hashing to select F consecutive or other-
wise deterministic nodes along the ring onto which the hash
function maps names and nodes. The replica-controllers are
responsible only for determining the number and locations
of the active replicas, and the actives replicas are responsi-
ble for maintaining the actual name record and processing
client requests. The replica-controllers implement a repli-
cated state machine using Paxos [38] in order to maintain a
consistent view of the current set of active replicas.
A name’s replica-controllers compute its active replica lo-
cations in a demand-aware manner. This computation pro-
ceeds in epochs as follows. At creation time, the active
replicas are chosen to be physically at the same locations
as the corresponding replica-controllers. In each epoch, the
replica-controllers obtain from each active replica a summa-
rized load report that contains the request rates for that
name from diﬀerent regions as seen by that replica. Here,
regions partition users into non-overlapping groups that cap-
ture locality, e.g., IP preﬁxes or a geographic partitioning
based on cities; and the load report is a spatial vector of
request rates as seen by the replica. The replica-controllers
aggregate these load reports to obtain a concise spatial dis-
tribution of all requests for the name.
3.3.1 Demand-aware replica placement
In each epoch, the replica-controllers use a placement al-
gorithm that takes as input the aggregated load reports and
capacity constraints at name servers to determine the num-
ber and locations of active replicas for each name so as to
minimize client-perceived latency. We have formalized this
global optimization problem as a mixed-integer program and
shown it to be computationally hard. As our focus is on
simple, practical algorithms, we defer the details of the op-
timization approach [1], using it only as a benchmark in
small-scale experiments with Auspice’s heuristic algorithm
Auspice’s placement algorithm is a simple heuristic and
can be run locally by each replica-controller. The placement
algorithm computes the number of replicas using the lookup-
to-update ratio of a name in order to limit the update cost
to within a small factor of the lookup cost. The number
of replicas is always kept more than the minimum number
needed to meet the availability objective under failures. The
location of these replicas are decided to minimize lookup
latency by placing a fraction of replicas close to pockets of
high demand for that name while placing the rest randomly
so as to balance the potentially conﬂicting goals of reducing
latency and balancing load among name servers.
Speciﬁcally, the placement algorithm computes the num-
ber of replicas for a name as (F + βri/wi), where ri and
wi are the lookup and update rates of name i; F is the
minimum number of replicas needed to meet the availability
goal (§3.1); and β is a replication control parameter that
is automatically determined by the system so as to trade
oﬀ latency beneﬁts of replication against update costs given
capacity constraints as follows. In each epoch, the replica-
controllers recompute β so that the aggregate load in the
system corresponds to a preset threshold utilization fraction
µ. For simplicity of exposition, suppose read and write op-
erations impose the same load, and the total capacity across
all name servers (in reads/sec) is C. Then, β is set so that
where the right hand side represents the total load summed
across all names. The ﬁrst term in the summation above is
the total read load and the second is the total write load.
Having computed β as above, replica-controllers compute
the locations of active replicas for name i as follows. Out
of the F + βri/wi total replicas, a fraction ν of replicas
are chosen based on locality, i.e., replica-controllers use the
spatial vector of load reports to select ν(F + βri/wi) name
servers that are respectively the closest to the top ν(F +
βri/wi) regions sorted by demand for name i. The remain-
ing (1 − ν)(F + βri/wi) are chosen randomly without rep-
etition. The locality-based replicas above are chosen as the
closest with respect to round-trip latency plus load-induced
latency measured locally at each name server. An earlier
design chose them based on round-trip latency alone, but
we found that adding load-induced latencies in this step (in
addition to choosing the remaining replicas randomly) en-
sures better load balance and lowers overall client-perceived
latency. Our current prototype and system experiments ﬁx
the random perturbation knob ν to 0.5. We have since devel-
oped a slightly modiﬁed placement scheme that relieves the
251
µC =!i
ri +!i
(F + β
ri
wi
)wi
(1)
administrator from setting ν manually, automatically bal-
ancing locality-awareness and load to ensure low latencies
[1]. Thus, an administrator need only specify F and µ based
on fault tolerance and aggressiveness of capacity utilization.
3.3.2 Client request routing
A client request is routed from an end-host to a suitable
name server as follows. The set of all name servers in an
Auspice instance is known to each member name server and
can be obtained from a well-known location. End-hosts can
either directly send requests to a name server or channel
them through a local name server like today. When a local
name server encounters a request for a name for the ﬁrst
time, it uses the known set of all name servers and con-
sistent hashing to determine the replica-controllers for that
name and sends the request to the closest replica-controller.
The replica-controller returns the set of active replicas for
the name and the client resends the request to the closest
active replica. In practice, we expect replica-controllers to
be contacted infrequently as the set of active replicas can be
cached and reused until they change in some future epoch.
Network latency as well as server-load-induced latency
help determine the closest replica at a local name server.
Each local name server maintains an estimate of the round-
trip latency to all name servers using infrequent pings; an
(as yet unimplemented) optimization to reduce the overhead
of all-to-all pings is to use coordinate embedding, geo-IP, or
measurement-driven techniques [42]. To incorporate load-
induced latency, the latency estimate to a name server is
passively measured as a moving average over lookups sent
to that name server. The local name server also maintains
a timeout value based on the moving average and variance
of the estimates. If a lookup request sent to a name server
times out, the local name server infers that either the server
or network route is congested, and it multiplicatively in-
creases its latency estimate to that name server by a ﬁxed
factor. Thus, if multiple lookups sent to a name server time
out, the estimated latency shoots up and the local name
server stops sending requests to that name server, which
eﬀectively acts as a more agile load-balancing policy in the
request routing plane (complementing the replica placement
plane above that operates in coarser-grained epochs).
3.3.3 Consistency with static replication
As a global name-to-address resolution service, Auspice
must at least ensure this eventual consistency property: all
active replicas must eventually return the same value of the
name record and, in a single-writer scenario, this value must
be the last update made by the (only) client; “eventually”
means that there are no updates to a name record and no
replica failures for suﬃciently long. Violating this property
means that a mobile client may be persistently unreachable
even though it is no longer moving (updating addresses).
With a static set of replicas, it is straightforward to sup-