“perfect” value diﬀers from Trinocular operation, where A is
computed from possibly outdated IP history. Adapting A
from probes is work-in-progress.
7.3 Case Studies of Internet Outages
We next examine several cases where Internet outages
made global news. We see that systematic measurement of
outages can provide information the scope of problems and
the speed of recovery. Where possible, we visualize outages
by clustering blocks by similarity in outage timing [26], and
coloring blocks based on their geolocation.
7.3.1 Political Outages: Egypt and Libya
Two major 2011 outages were caused by political events:
most Egyptian routes were withdrawn on 2011-01-27 by the
government during what became the 2011 Egyptian revolu-
tion, and all Libyan routable preﬁxes were withdrawn 2011-
02-18 during the Libyan revolution. In both cases, we re-
examined surveys covering these events (S 38c began 2011-
01-27, just after Egypt’s outage, and ran for 3 weeks to cover
Libya). We have strong evidence of the Egyptian outage,
with 19 /24 blocks of Egypt’s 22k in the survey (visualiza-
Figure 7: Six days of the 600 largest outages in March 2011
showing results of the T¯ohoku earthquake. Dataset: S 39c.
Colors are keyed to countries.
tion omitted due to space). The end of the observed outage
is conﬁrmed with news reports and analysis of BGP data.
Libya’s Internet footprint is much smaller than Egypt’s:
only 1168 /24 blocks as of March 2011. Only one of those
blocks was in the dataset, and that block is too sparse (only
4 active addresses) to apply Trinocular. However, Trinoc-
ular’s lightweight probing means that it could have covered
the whole analyzable Internet. Had it been active at the
time, we would have tracked 36% of Libya’s 1168 blocks and
likely seen this outage.
7.3.2 March 2011 Japanese Earthquake
In survey S 39c, we observe a Japanese Internet outage, in
Figure 7 mid-day (UTC) on 2011-03-11. This event is con-
ﬁrmed as an undersea cable outage caused by the T¯ohoku
Japanese earthquake [22]. We mark a vertical line 30 min-
utes before the earthquake so as to not obscure transition
times; individual blocks do not cluster well because recovery
times vary, but the outage is visible as a large uptick in the
marginal distribution. Unlike most human-caused outages,
both the start and recovery from this outage vary in time.
For most blocks, the outage begins at the exact time of the
earthquake, as shown by the sudden large jump in marginal
distribution less than 6 hours into 2011-03-11, but for some
it occurs two hours later. Recovery for most blocks occurs
within ten hours, but a few remain down for several days.
This dataset also shows strong evidence of diurnal out-
ages in Asia as the green and white banding seen in the
low 300 blocks. These diurnal outages make Trinocular’s
outage rate slightly higher than our previous approach [26].
We show that these blocks come and go, meeting our deﬁni-
tion of outage. Future work may distinguish between cases
where networks intentionally go down (such as turning of a
laboratory at night) from unexpected outages.
7.3.3 October 2012: Hurricane Sandy
We observed a noticeable increase in network outages fol-
lowing Hurricane Sandy. The Hurricane made landfall in
the U.S. at about 2012-10-30 T00:00 UTC. When we focus
on known U.S. networks, we see about triple the number of
Figure 8: Six days of the 300 largest outages in U.S.-based
networks showing Hurricane Sandy. Dataset: S 50j .
Figure 10: Evaluation of single-site outages in 2-week sur-
veys over three years. Top shows availability, bottom shows
Internet events, outages and outage percentage over time.
(Dataset varies by time, as shown in the ﬁgure.)
colored bars in the middle of the graph) after hurricane
landfall on 2012-10-30, about three times the prior baseline.
These problems are generally resolved over the following four
days. (Because of our more sensitive methodology, we see
more outages here than in our prior analysis [14], but our
qualitative results are similar.)
While re-analysis of S 50j provides insight into Sandy-
related problems and recover, survey collection places sig-
niﬁcant traﬃc on the targets. Trinocular can cover 3.4M
blocks, about 80× more than the 40k in a survey, at about
1% the traﬃc to each target block.
7.4 Longitudinal Re-analysis of Existing Data
Finally, we re-analyze three years of surveys. This data
lets us compare the stability of our results over time and
across diﬀerent locations.
Probing location can aﬀect evaluation results. Should the
probing site’s ﬁrst hop ISP be unreliable, we would under-
estimate overall network reliability. We re-analyze surveys
collected from three sites (see §7.2), each with several up-
stream networks. In Figure 10, locations generally alternate,
and each location is plotted with a diﬀerent symbol (W:
empty symbols, C: ﬁlled, J: asterisks), and survey number
and location letter are shown at the graph top. Visually, this
graph suggests the results are similar regardless of probing
site and for many diﬀerent random samples of targets. Nu-
merically, variation is low: mean outage rate (area) is 0.64%
with standard deviation of only 0.1%. To strengthen this
comparison we carried out Student’s t-test to evaluate the
hypothesis that our estimates of events, outages, and outage
rates for our sites are equal. The test was unable to reject
the hypothesis at 95% conﬁdence, suggesting the sites are
statistically similar.
Besides location, Figure 10 suggests fairly stable results
over time. We see more variation after 2011, when the size
of the target list doubled to about 40k blocks.
These observations are each from a single vantage point,
thus they include both global and local outages. Surveys are
Figure 9: Median number of outages per day, broken down
by state, weighted by outage size and duration, with jittered
individual readings (dots). Dataset: S 50j .
network outages for the day following landfall, and above-
baseline outages for the four days following landfall.
Visualizing outages: Figure 8 visualizes the 400 blocks
in the U.S. with the largest degree of outages, and label (a)
shows a strong cluster of outages at 2013-10-30 (UTC) corre-
sponding with hurricane landfall. Hurricane-related outages
tend to be long, lasting one or more days. We believe these
outages correspond to residential power outages.
Quantifying outages: We know that some part of the
Internet is always down, so to place these outages in per-
spective, Figure 9 plots the exact number of /24 blocks that
are down in each round (this value is the marginal distribu-
tion of Figure 8). We plot each round as small red points
(with small jitter to make consecutive more distinct), and
we show 24-hour median values with the dark line.
Figure 9 shows U.S. networks had an outage rate of about
0.36% before landfall. (This rate seems somewhat less than
the global average.) This rate jumps to 1.27%, about triple
the prior U.S. baseline, for the 24-hours following landfall.
The outage level drops over the next four days, and ﬁnally
returning to the baseline on 2012-11-03.
Locating outages: To conﬁrm the correlation between
the hurricane and these outages, we look at the weighted
blocks by state. The bars in Figure 9 identify outages by
state. The top “US” portion represents outages that are
geolocated in the U.S., but not to a speciﬁc state.
This ﬁgure shows that there are large increases in the
amount of outages in New York and New Jersey (the lighter
 0 0.002 0.004 0.006 0.008 0.01 0.012 0.014 0.016 0.0182012-10-272012-10-282012-10-292012-10-302012-10-312012-11-012012-11-022012-11-032012-11-042012-11-052012-11-062012-11-072012-11-08outages per day (fraction of block-rounds)date (UTC)hurricane U.S. landfallCAusCAususNJNYPAusCTNJNYusNJNYusNJNYususususususus 99 99.2 99.4 99.6 99.8 100availability (%)availability 0 10 20 30 40 502009-10-012010-01-012010-04-012010-07-012010-10-012011-01-012011-04-012011-07-012011-10-012012-01-012012-04-012012-07-012012-10-012013-01-012013-04-01 0 1 2 3outages (down events x1000)outage rate (%)Dateoutage rateoutages29wc30wc31wc32wc33wc34wc35wc36wc37wc38wc39wc40wc41wc42wc43wcj44wcj45cjw46wcj47wcj48wcj49wcj50wcj51wcj52wcj53wcjtarget list doubledtaken for non-overlapping, two week periods because each
places a signiﬁcant burden on the subject networks. Trinoc-
ular’s much lower traﬃc rate to targeted blocks (1% that of
a survey) allows outage detection to overcome both of these
limitations. As demonstrated in §7.1, it can operate con-
currently from three sites. We plan to carry out continuous
monitoring as Trinocular matures.
8. CONCLUSIONS
Trinocular is a signiﬁcant advance in the ability to observe
outages in the network edge. Our approach is principled, us-
ing a simple, outage-centric model of the Internet, populated
from long-term observations, that learns the current status
of the Internet with probes driven by Bayesian inference. We
have shown that it is parsimonious, with each instance in-
creasing the burden on target networks by less than 0.7%. It
is also predictable and precise, detecting all outages lasting
at least 11 minutes with durations within 330 s. It has been
used to study 3.4M blocks for two days, and to re-analyze
three years of existing data, providing a new approach and
understanding of Internet reliability.
Data Availability and Acknowledgments: The raw and an-
alyzed data from this paper are available at no cost to researchers
through the U.S. DHS PREDICT program (www.predict.org) and by
request from the authors [27]. This work was classiﬁed by USC’s IRB
as non-human subjects research (IIR00000975).
We thank our shepherd, Olaf Maennel, and the anonymous review-
ers for comments that made this paper stronger and more readable.
We thank John Wroclawski for comments that helped clarify the role
of the model, Ethan Katz-Bassett and Harsha Madhyastha for discus-
sion about §6.2, and ´Italo Cunha for a careful reading. We thank Jim
Koda (ISI), Brian Yamaguchi (USC), and CSU network operations
for providing BGP feeds to assist our evaluation, and Dan Massey,
Christos Papadopoulos, Mikhail Strizhov for assisting with BGPmon
and at CSU. We also thank Katsuhiro Horiba (WIDE) for providing
probing infrastructure and BGP feeds.
9. REFERENCES
[1] D. G. Andersen, H. Balakrishnan, M. F. Kaashoek, and
R. Morris. Resilient overlay networks. In Proc. of Symposium
on Operating Systems Principles, pages 131–145, Chateau
Lake Louise, Alberta, Canada, Oct. 2001. ACM.
[2] R. Bush, J. Hiebert, O. Maennel, M. Roughan, and S. Uhlig.
Testing the reachability of (new) address space. In Proc. of
ACM Workshop on Internet Nework Management, Aug. 2007.
[3] R. Bush, O. Maennel, M. Roughan, and S. Uhlig. Internet
optometry: assessing the broken glasses in Internet
reachability. In Proc. of ACM IMC, 2009.
[4] D. R. Choﬀnes, F. E. Bustamante, and Z. Ge. Crowdsourcing
service-level network event monitoring. In SIGCOMM, 2010.
[5] J. Cowie. Egypt leaves the Internet. Renesys Blog http:
//renesys.com/blog/2011/01/egypt-leaves-the-internet.shtml,
Jan. 2011.
[6] I. Cunha, R. Teixeira, N. Feamster, and C. Diot. Measurement
methods for fast and accurate blackhole identiﬁcation with
binary tomography. In Proc. of 9thACM IMC, 2009.
[7] A. Dainotti, R. Amman, E. Aben, and K. Claﬀy. Extracting
beneﬁt from harm: using malware pollution to analyze the
impact of political and geophysical events on the Internet.
ACM Computer Communication Review, Jan 2012.
[8] A. Dainotti, C. Squarcella, E. Aben, K. C. Claﬀy, M. Chiesa,
M. Russo, and A. Pescap´e. Analysis of country-wide internet
outages caused by censorship. In ACM IMC, 2011.
[9] A. Dhamdhere, R. Teixeira, C. Dovrolis, and C. Diot.
NetDiagnoser: troubleshooting network unreachabilities using
end-to-end probes and routing data. In Proc. of ACM
Conference on Emerging Networking Experiments and
Technologies, pages 18:1–18:12. ACM, 2007.
[10] X. Fan and J. Heidemann. Selecting Representative IP
Addresses for Internet Topology Studies. In ACM IMC, 2010.
[11] X. Fan, J. Heidemann, and R. Govindan. LANDER IP history
datasets. http://www.isi.edu/ant/traces/ipv4_history, 2011.
[12] N. Feamster, D. G. Andersen, H. Balakrishnan, and
F. Kaashoek. Measuring the Eﬀects of Internet Path Faults on
Reactive Routing. In ACM Sigmetrics - Performance, 2003.
[13] J. Heidemann, Y. Pradkin, R. Govindan, C. Papadopoulos,
G. Bartlett, and J. Bannister. Census and Survey of the Visible
Internet. In Proc. of ACM IMC, Oct. 2008.
[14] J. Heidemann, L. Quan, and Y. Pradkin. A preliminary
analysis of network outages during Hurricane Sandy. Technical
Report ISI-TR-2008-685, USC/Information Sciences Institute,
November 2012.
[15] Y. Huang, N. Feamster, A. Lakhina, and J. J. Xu. Diagnosing
network disruptions with network-wide analysis. In Proc. of
ACM SIGMETRICS, pages 61–72, San Diego, California,
USA, June 2007. ACM.
[16] E. Katz-Bassett. Private communications, May 2012.
[17] E. Katz-Bassett, H. V. Madhyastha, J. P. John,
A. Krishnamurthy, D. Wetherall, and T. Anderson. Studying
black holes in the Internet with Hubble. In Proc. of 5th NSDI,
pages 247–262. USENIX, Apr. 2008.
[18] E. Katz-Bassett, C. Scott, D. R. Choﬀnes, ´I. Cunha,
V. Valancius, N. Feamster, H. V. Madhyastha, T. Anderson,
and A. Krishnamurthy. LIFEGUARD: Practical repair of
persistent route failures. In Proc. of SIGCOMM, pages
395–406, Helsinki, Finland, Aug. 2012. ACM.
[19] R. R. Kompella, J. Yates, A. Greenberg, and A. C. Snoeren.
Detection and Localization of Network Black Holes. In Proc. of
IEEE Infocom, 2007.
[20] H. V. Madhyastha, T. Isdal, M. Piatek, C. Dixon, T. Anderson,
A. Krishnamurthy, and A. Venkataramani. iPlane: An
information plane for distributed services. In Proc. of 7thOSDI,
pages 367–380, Seattle, WA, USA, Nov. 2006. USENIX.
[21] R. Mahajan, D. Wetherall, and T. Anderson. Understanding
BGP misconﬁguration. In Proc. of SIGCOMM, 2002.
[22] O. Malik. In Japan, many undersea cables are damaged.
GigaOM blog, http://gigaom.com/broadband/
in-japan-many-under-sea-cables-are-damaged/, Mar. 14 2011.
[23] A. Markopoulou, G. Iannaccone, S. Bhattacharyya, C. nee
Chuah, and C. Diot. Characterization of Failures in an IP
Backbone. In Proc. of IEEE Infocom, 2004.
[24] V. Paxson. End-to-end routing behavior in the Internet. In
Proc. of SIGCOMM ’96, pages 25–38, Stanford, CA, Aug.
1996. ACM.
[25] L. Quan, J. Heidemann, and Y. Pradkin. Detecting internet
outages with precise active probing (extended). Technical
Report ISI-TR-2012-678, USC/ISI, February 2012.
[26] L. Quan, J. Heidemann, and Y. Pradkin. Visualizing sparse
internet events: Network outages and route changes. In
Proc. of First ACM Workshop on Internet Visualization,
Boston, Mass., USA, Nov. 2012. Springer.
[27] L. Quan, J. Heidemann, and Y. Pradkin. LANDER Internet
outage datasets.
http://www.isi.edu/ant/traces/internet_outages, 2013.
[28] L. Quan, J. Heidemann, and Y. Pradkin. Poster abstract:
Towards active measurements of edge network outages. In
Proc. of Passive and Active Measurement Workshop, pages
276–279, Hong Kong, China, Mar. 2013. Springer.
[29] A. Schulman and N. Spring. Pingin’ in the rain. In Proc. of
ACM IMC, pages 19–25, Berlin, Germany, Nov. 2011. ACM.
[30] R. Teixeira and J. Rexford. A measurement framework for
pin-pointing routing changes. In Proc. of the ACM SIGCOMM
workshop on Network troubleshooting, 2004.
[31] N. Y. Times. Egypt cuts oﬀ most internet and cell service.
http://www.nytimes.com/2011/01/29/technology/internet/
29cutoff.html.
[32] D. Turner, K. Levchenko, A. C. Snoeren, and S. Savage.
California fault lines: understanding the causes and impact of
network failures. In Proc. of SIGCOMM, 2010.
[33] Wikipedia. Hurricane Sandy.
http://en.wikipedia.org/wiki/Hurricane_sandy, 2012. Retrieved
2012-11-24.
[34] E. Wustrow, M. Karir, M. Bailey, F. Jahanian, and G. Huston.
Internet background radiation revisited. In ACM IMC, 2010.
[35] M. Zhang, C. Zhang, V. Pai, L. Peterson, and R. Wang.
PlanetSeer: Internet Path Failure Monitoring and
Characterization in Wide-area Services. In OSDI, 2004.