### Recall of OLDI and RPC Budgets
In OLDIs (On-Line Data-Intensive applications), the parent-to-leaf Remote Procedure Call (RPC) has a total budget that is divided into computation and network components. If all leaf nodes were to complete their computations simultaneously for every query, the network deadline would be a fixed and strict value. However, in practice, computation times can vary across leaf nodes if the workload is not evenly distributed. While the computation budget sets a hard upper limit, some leaves may finish earlier, leading to a slightly more relaxed effective network deadline. Additionally, some applications use user-injected jitter to smooth out fan-in bursts [9]. Therefore, the exact nature and architecture of an OLDI application can influence the distribution of effective deadlines.

### Evaluation of Deadline Distributions
A robust network protocol should perform well across a range of deadline distributions. To evaluate this, we consider three different deadline distributions, each based on the same base deadlines specified in Section 4.2.1, with varying levels of added variation:
- **Low Variance Case**: A 10% uniform-random variation added to the base deadline.
- **Medium Variance Case**: A 50% uniform-random variation added to the base deadline.
- **High Variance Case**: A one-sided exponential distribution with a mean equal to the base deadline, matching the distribution used in [25].

### Comparison of D2TCP, DCTCP, and D3
We compare D2TCP against DCTCP and D3 in terms of the fraction of missed deadlines for our benchmark. Figures 9, 10, and 11 show the results for the low, medium, and high variance cases, respectively. In these figures, the Y-axis represents the fraction of missed deadlines for TCP, DCTCP, D3, and D2TCP, while the X-axis shows the degree of burstiness, varied by increasing the fan-in degree (i.e., the number of leaves per parent) from 5 to 40. Typical OLDI applications have fan-in degrees within this range [1] [25].

Across all three graphs, TCP misses a rapidly increasing number of deadlines as the fan-in degree increases. DCTCP and D3 improve upon TCP but still miss a significant fraction of deadlines. For the medium variance case, both DCTCP and D3 miss around 25% of the deadlines at a fan-in degree of 40. In contrast, D2TCP keeps the fraction of missed deadlines under 7% at a fan-in degree of 40. This trend holds true for all three deadline variation cases, indicating that D2TCP is robust enough to handle a wide spectrum of deadline distributions. For the remainder of the results section, we use only the medium variance case for all remaining experiments.

### Performance with Higher-Variance Deadlines
All schemes perform better with higher-variance deadlines, as expected, because higher variance smooths out fan-in-burst-induced congestion. For the high variance case, the fraction of missed deadlines for D3 falls in the range of 0-15%, which is close to the 0-9% reported in the D3 paper [25], confirming the reasonableness of our D3 implementation.

### Latency Analysis
To further explain these results, Figure 12 shows the 50th, 90th, and 99th percentile latencies for DCTCP, D3, and D2TCP, normalized to the delay allowed by the deadline. The three points on each line, from bottom to top, correspond to the 50th, 90th, and 99th percentile latencies, respectively. As expected, D2TCP's latencies are significantly lower than those of DCTCP and D3, resulting in fewer missed deadlines. Overall, the latencies for all schemes closely track the fraction of missed deadlines in Figure 10.

### Shortcomings of D3
Recall that D3's greedy approach may allocate bandwidth to far-deadline requests arriving slightly ahead of near-deadline requests, causing frequent priority inversions in congested flows. To confirm this, we compute the percentage of requests denied while later-deadline requests are granted. Table 1 shows this percentage for D3 under all three variance cases for various fan-in degrees. Even in a favorable setting (high variance deadlines with a fan-in degree of 20), D3 incurs priority inversion for nearly 25% of all flows. Priority inversions worsen as the fan-in degree increases and as the variance in deadlines tightens, causing more burstiness.

### Background Flows
To test whether long-lived, non-OLDI flows achieve high bandwidth even as short-lived OLDI flows come and go, we replace one leaf-to-parent flow in each OLDI tree with a long-lived background flow. This background flow has an exponential arrival with a mean of 300 ms and sends 1 MB of data.

Figure 13 shows the background flows' bandwidth for DCTCP, D3, and D2TCP, normalized to that for TCP, as we vary the fan-in degree. Background flows give up bandwidth to OLDI flows only for the short duration of fan-in-burst-induced congestion. Consequently, all schemes perform well, achieving 85% or more of the bandwidth achieved by TCP. D2TCP is slightly better than DCTCP, which unnecessarily throttles background flows to make room for far-deadline, OLDI flows. Overall, D2TCP achieves 95% or more of the bandwidth achieved by TCP.

### Confirmation of Bandwidth Allocation
To confirm that background flows do not take bandwidth away from OLDI flows, Figure 14 shows the fraction of missed deadlines for the OLDI flows in the presence of background flows. For all schemes, the fraction remains similar to that in the absence of background flows (Figure 10).

### Varying the Cap on d â€“ Deadline Imminence
Finally, we explore the impact of varying the cap on the deadline imminence factor \(d\). This analysis is shown in Figures 15 and 16, which depict the fraction of missed deadlines while varying the cap on \(d\) and when D2TCP and TCP coexist. These results further validate the robustness and efficiency of D2TCP in handling a wide range of deadline distributions and network conditions.