title:OpenANFV: accelerating network function virtualization with a consolidated
framework in openstack
author:Xiongzi Ge and
Yi Liu and
David H. C. Du and
Liang Zhang and
Hongguang Guan and
Jian Chen and
Yuping Zhao and
Xinyu Hu
OpenANFV: Accelerating Network Function Virtualization
with a Consolidated Framework in OpenStack
Xiongzi Ge†, Yi Liu§, David H.C. Du†, Liang Zhang‡, Hongguang Guan‡, Jian Chen‡,
Yuping Zhao‡ and Xinyu Hu‡
†Department of Computer Science and Engineering, University of Minnesota
§Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences
‡Huawei Corporation
PI:EMAIL, PI:EMAIL, PI:EMAIL, PI:EMAIL
Categories and Subject Descriptors
C.2.3 [Network Operations]: Network management
Keywords
Network Function Virtualization; Middlebox; OpenStack;
FPGA
1.
INTRODUCTION
Speciﬁed appliances or middleboxes (MBs) have been ex-
plosively used to satisfy a various set of functions in oper-
ational modern networks, such as enhancing security (e.g.
ﬁrewalls), improving performance (e.g. WAN optimized ac-
celerators), providing QoS (e.g. Deep Packet Inspection
(DPI)), and meeting the requisite others [3]. Network Func-
tion Virtualization (NFV) recently has been proposed to op-
timize the deployment of multiple network functions through
shifting the MB processing from customized MBs to software-
controlled inexpensive and commonly used hardware plat-
forms (e.g.
Intel standard x86 servers) [4]. However, for
some functions (e.g. DPI and Network Deduplication (Dedup),
Network Address Translation (NAT)), the commodity shared
hardware substrate remain limited performance. For a stan-
dard software based Dedup MB (Intel E5645, 2.4GHZ, 6
cores, exclusive mode), we can only achieve 267Mbps through-
put in each core at most. Therefore, the resources of dedi-
cated accelerators (e.g. FPGA) are still required to bridge
the gap between software-based MB and the commodity
hardware.
To consolidate various hardware resources in an elastic,
programmable and reconﬁgurable manner, we design and
build a ﬂexible and consolidated framework, OpenANFV,
to support virtualized acceleration for MBs in the cloud en-
vironment. OpenANFV is seamlessly and eﬃciently put into
Openstack to provide high performance on top of commodity
hardware to cope with various virtual function requirements.
OpenANFV works as an independent component to manage
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage, and that copies bear this notice and the full ci-
tation on the ﬁrst page. Copyrights for third-party components of this work must be
honored. For all other uses, contact the owner/author(s). Copyright is held by the
author/owner(s).
SIGCOMM’14, August 17–22, 2014, Chicago, IL, USA.
ACM 978-1-4503-2836-4/14/08.
http://dx.doi.org/10.1145/2619239.2631426.
and virtualize the acceleration resources (e.g. cinder man-
ages block storage resources and nova manages computing
resources). Specially, OpenANFV mainly has the following
three features.
• Automated Management. Provisioning for multi-
ple VNFs is automated to meet the dynamic require-
ments of NFV environment. Such automation allevi-
ates the time pressure of the complicated provisioning
and conﬁguration as well as reduces the probability of
manually induced conﬁguration errors.
• Elasticity. VNFs are created, migrated, and destroyed
on demand in real time. The reconﬁgurable hardware
resources in pool can rapidly and ﬂexibly oﬄoad the
corresponding services to the accelerator platform in
the dynamic NFV environment.
• Coordinating with OpenStack. The design and
implementation of the OpenANFV APIs coordinate
with the mechanisms in OpenStack to support required
virtualized MBs for multiple tenancies.
2. ARCHITECTURE AND IMPLEMENTA-
TION
Figure 1 brieﬂy shows the architecture of OpenANFV.
From the top-to-down prospective view, when a speciﬁc MB
is needed, its required resources are orchestrated by Open-
Stack. There are suﬃcient northbound APIs in this open
platform. Each VNF of MB is instantiated by a Virtual
Machine (VM) node, running on the common x86 platform.
Once deployed, these MBs aim to provide services as well
as the original appliances. The role of VNF controller is to
leverage the resource demand of VMs associated with un-
derlying virtual resource pools.
NFV Infrastructure (NFVI) comes from the concept of
IaaS to virtualize corresponding hardware resources.
In
NFVI, the Network Functions Acceleration Platform (NFAP)
provides a heterogeneous PCIe based FPGA card which sup-
ports isolated Partial Recon(cid:12)gure (PR) [5]. PR can virtu-
alize and reconﬁgure the acceleration functions shortly in
the same FPGA without aﬀecting the exiting virtualized ac-
celerators. Using PR, diﬀerent accelerators are placed and
routed in special regions.
The FPGA is divided into static and partially reconﬁg-
urable regions (PR). When the network requirements are
changed, the function of PR could be replaced with a new
353Figure 1: Brief OpenANFV architecture.
one. The PR is implemented to support diﬀerent acceler-
ators, and controlled by one VM exclusively. When one
accelerator is reloading without aﬀecting the other acceler-
ators are not aﬀected. The PR and VM can communicate
through PCIe SR-IOV eﬃciently. The static region contains
the shared resources (e.g.
the storage and network inter-
faces).
The responsibility of NFAP is dispatching the rules to the
two modules of FPGA, the classi(cid:12)er and the switch. The
classi(cid:12)er identiﬁes the ﬂow where the rule format conforms
to ⟨group id, tag⟩, when the ﬂow matches the condition group
id, classiﬁer will add an item to the ﬂow table in the clas-
siﬁer. Based on the ﬂow table, the classiﬁer could divide
the traﬃc into diﬀerent NFs. The switch get the MB chain
conﬁguration [⟨Tag,Port⟩:Outport], and forward the encap-
sulated head (EH) based packet combining with the tag and
income port. The packets are redirected to the MB in the
chain in sequence until to the last one. The encapsulated
packet with its tag can transfer more than one PRs, more-
over, the tag could also support the load balancing between
the NFs. Compared with FlowTags [2], we use the tag in
the encapsulation to expand ﬂexibly without aﬀecting the
ﬁeld of the original packet header.
We have implemented the prototype of integrating NFAP
into OpenStack following the methodology which is pro-
posed by the ETSI standards group [1]. The K-V pair of
 is ued to
track the current status of NFAP. vAccelerator is the aliased
key to identify the PCIe device and number of running vAc-
celerators is to identify the current virtual accelerators. The
scheduler has been ﬁnished in the VNF controller which is
followed by the standard nova scheduler. Finally, the ex-
tended VM.xml generation includes allocation of a PCIe de-
vice virtual function to a VM.
3. EVALUATION AND CONCLUSION
We evaluate the prototype of NFAP in a demo FPGA
cards (Altera FPGA Stratix A7, 8GB DDR3, 8M QDR, 4
SPF+). The experimental environment consists of one x86-
based server (2.3GHz 8Core Intel Xeon E5, 96GB RAM,
40G NIC) with Altera FPGA card, two x86-based servers
running the controller and OpenStack, respectively. The
Figure 2: Performance results on throughput with
or without adopting NFAP.
IXIA XM2(with NP8 board) is used as the source and the
sink of packets. The server with the NFAP runs KVM hy-
pervisor, and each NFAP could provide three empty PRs
for the server. The VMs used for the test have the same
conﬁguration (1 vCPU, 16G RAM and 1TB Disk).
Our tests include three VNFs, NAT, DPI, and Dedup.
Each VNF has two versions, with and without adopting
NFAP. Without NFAP, the computing resources of VNFs
are completely provided by nova in OpenStack. For NFAP
assisted NAT, the software in the VM has an Openﬂow-
like API with the NFV controller and conﬁgures the policy
using a hardware abstract API. The hardware part in the
NFAP oﬄoads the ﬂow table, the header replacement, and
the checksum calculation from the VM. The packet will be
merely processed in the NFAP, if the ﬂow is matched in
the ﬂow table. For NFAP assisted DPI, we oﬄoad all the
string match (Mutihash Algorithm and Bloomﬁlter), regular
match, and the rules table in the NFAP, and the VM keeps
the rule compiler and the statistics which are needed by
the controller. The rule compiler compiles perl compatible
regular expression to Deterministic Finite Automata (DFA)
rule. For NFAP assisted Dedup, we oﬄoad the rabin hash,
Marker select algorithm and chunk hash (Murmur hash). As
the TCP is too complex to implement in the FPGA, the tcp-
stack is still in the VM and the packets are received and sent
out via the software tcp-stack. We still do some optimiza-
tions like using the Data Plane Development Kit (DPDK)
driver and use space tcp-stack. As shown in Figure 2, the
performance of DPI, Dedup, and NAT with adopting NFAP
in OpenANFV outperforms the scheme without NFAP by
20X, 8.2X, and 10X, respectively.
4. REFERENCES
[1] Enhanced platform awareness for pcie devices. https:
//wiki.openstack.org/wiki/Enhanced-platform-awareness-pcie.
[2] Fayazbakhsh, S. K., Chiang, L., Sekar, V., Yu, M., and Mogul,
J. C. Enforcing network-wide policies in the presence of dynamic
middlebox actions using (cid:13)owtags. In Proc. 11th USENIX NSDI
(2014), USENIX Association, pp. 543{546.
[3] Gember, A., Grandl, R., Khalid, J., and Akella, A. Design and
implementation of a framework for software-de(cid:12)ned middlebox
networking. In Proceedings of the ACM SIGCOMM 2013
(2013), ACM, pp. 467{468.
[4] Martins, J., Ahmed, M., Raiciu, C., Olteanu, V., Honda, M.,
Bifulco, R., and Huici, F. Clickos and the art of network
function virtualization. In Proceedings of the 11th NSDI (2014),
USENIX, pp. 459{473.
[5] Yin, D., Unnikrishnan, D., Liao, Y., Gao, L., and Tessier, R.
Customizing virtual networks with partial fpga recon(cid:12)guration.
ACM SIGCOMM Computer Communication Review 41, 1
(2011), 125{132.
354