equivalent to virtually put θ·n items into m locations. Hence
pcf = 1 − (1 − p(m, θ · n, h))f−θ
(2)
Collisions in SVBF All delay group locations for the
query ﬂow may be set by any of the background ﬂows. Hence
pcf = 1 − (1 − p(m, n, h))k−1
(3)
When all BF sizes mi are equal, a standard convexity argu-
ment shows that pcf(SVBF) ≤ pcf(PBF) for any {ni}.
Storage Dimensioning We use the foregoing analysis to
show how to dimension SVBF for given target classiﬁca-
tion failure rate pcf. For large, n, p(m, n, h) ≈ q(n/m, h)
where q(α, h) = (1 − e−αh)h. As is well known, α (cid:55)→
q(α, h) is minimized at when h = α−1 log(2), in which case,
pcf(SVBF) = 1 − (1 − 2−h)k−1. Thus, given a target pcf
of ε > 0, we must choose h and α−1 log 2 to be bounded
below by − log2(1 − (1 − ε)1/(k−1)). The lower bound for h
and upper bound for α compatible with two possible target
classiﬁcation failure rates ε = 10−1 and 10−5 are displayed
as a function of the number k of delay groups in Figure 4.
Observe that, due to the logarithmic dependence, after an
initial phase, the curves are relatively ﬂat as a function of
k. They do not depend very strongly on the target rate:
decreasing ε by 4 orders of magnitude changes the bounds
by only about half an order of magnitude. Table 2 provides
1This happens with probability m−hm!/(m − h)! ≥ 1 −
h2/(2m)
107random d-byte strings, this is the only way to form the ag-
gregate unfortunately. We can reduce the query bandwidth
by potentially querying only a sample corresponding to the
aggregate as opposed to all the packets, that may represent
a trade-oﬀ between query bandwidth and accuracy. While
exploring this trade-oﬀ is outside the scope of this paper, we
brieﬂy describe an idea next that has the potential to reduce
the query bandwidth signiﬁcantly.
Query using ﬂow key and IP identiﬁer. We can reduce
query bandwidth overhead with the help of a range search
capability. Since packet hashes do not lend themselves to
this range search easily, we consider an alternate scheme.
Instead of storing the packet hash, we store the concatena-
tion of the packet’s ﬂow key and the IP identiﬁer (IPID)
ﬁeld. Since IPID ﬁeld is typically implemented to increment
IPIDs of packets linearly in most operating systems, the use
of IPID ﬁeld allows us to support ﬂow-level (or sub-ﬂow
level) queries quite easily. We believe that in data center
environments orchestrated by a single organization, enforc-
ing such de facto implementation can be possible. Note that
we evaluate this scheme against intact packet traces with no
modiﬁcation on IPID ﬁeld collected from a tier-1 ISP link
and two links in data centers in §5.4.
The client in this case can query packets using the tuple
(fk, [IP idi, IP idj]). Note that this does not stipulate that
all packets that belong to a ﬂow will need to have contiguous
IDs. But since TCP transmits packets in bursts usually, we
can break down a ﬂow in to several tuples
fk = (fk, [IP idi1, IP idi2]), (fk, [IP idi3, IP idi4]), . . . ,
and chain them together in one message. This reduces the
query bandwidth signiﬁcantly, almost by a factor of 17× in
our evaluation (§5.4). The receiving router will sequentially
query all packets (fk, IP idi1), (fk, IP idi1+1), ..., (fi, IP idi2).
Further, we can make the query interface specify whether it
wants the individual latency values or the aggregate values,
so that the router can send either individual packet latency
values or aggregate them in its response. This will also re-
duce the response overhead signiﬁcantly.
Query timing. For both types of queries, we need the
client to mention a rough time of the packet as part of the
query, so that the router can look up the appropriate SVBF
data structure corresponding to the time when the packet
may have gone through the router. Given the fact that PLS
resets the SVBF’s every epoch, it is important to make sure
that the previous epoch and the next epoch are also queried
for the packet in order to make sure there are no fringe
eﬀects, i.e., the timing is close to the start/end of an epoch
and it may lie before or after the epoch.
Querying clients. MAPLE is largely oblivious to who
originates the query. In some environments, we could en-
vision end hosts could be the clients. For example, an end
host that is running a low-latency trading application or
a high-performance computing application, whenever it de-
tects packet delays exceed some level, may originate a query
packet to determine which router is responsible for the higher
delay. Similarly, we can consider private datacenter owners
such as Google, Microsoft, etc., that may want to debug
their systems may provide this ability for individual hosts
to query routers periodically for obtaining latency statistics.
We can also consider public cloud environments such as
Amazon EC2 where customers may demand certain SLAs on
Figure 4: Dimensioning SVBF:
lower bound on
#hash functions h, and upper bound on load α =
n/m, as function of #delay groups k, for two target
pcf classiﬁcation failure rates ε = 10−1 and 10−5.
overview of insert and lookup complexities and storage re-
quirements of the three data structures. Given ε = 10−1,
parameter tuning examples are shown in Table 1.
4. LATENCY QUERY INTERFACE
In this section, we describe the packet latency query inter-
face that allows a ‘querying client’ (henceforth, just client)
to query routers for speciﬁc packet latency measurements.
In the very basic query, a
Query using packet hash.
client can request a particular router for latency of a given
packet identiﬁed by the packet hash. This implicitly as-
sumes that the packets are ﬁrst hashed using the invariant
ﬁelds in a packet header (e.g., IP addresses, IP id, port
numbers) and the packet payload (a few bytes is often suﬃ-
cient [17]) before inserting into the SVBF. We also assume
that the client knows that the path taken by the packet;
otherwise, the client needs to ask all the routers in the net-
work which may increase the number of bogus queries. We
assume that it will be possible to determine this based on
the forwarding tables. In cases where multiple parallel paths
are exploited (e.g., ﬂow-based VLB in VL2 [19] or ECMP), a
selected path for a ﬂow at random in VL2 needs to be stored
or hash functions for ECMP computation needs to be pub-
lished. When centralized controllers (e.g., Hedera [6]) are
involved in forwarding, such forwarding information can be
saved for oﬄine-queries. We leave this issue as part of our
future work.
The switch then performs a lookup operation of the packet
in the SVBF to return the latency estimate to the client. Be-
cause of classiﬁcation failure possibility in SVBF, we return
latency estimates with a 2-bit type that identiﬁes one of
three types: (1) Match that indicates that the packet was
uniquely identiﬁed in the SVBF. (2) Multi-Match indicat-
ing that multiple matches were reported, but the latency
estimate corresponds to the answer using the tie-breaking
heuristic we discussed in §3.3.4. (3) No-Match that indicates
the packet’s latency estimate could not be located.
If the host wishes to obtain ﬂow-level (or any other aggre-
gate) latency statistics, it needs to send all packet digests of
a particular ﬂow of interest to the particular switch/router
it believes the packets may have traversed along the path
from the source to the destination. Sending one packet for
querying each packet to the switch may lead to too many
packets. Luckily, packet digests could be easily batched in
one query message (about 375 32-bit labels can be embedded
within one 1500 byte query packet). Since packet hashes are
 0.01 0.1 1 10 100 0 20 40 60 80 100#groups kh; ε=10-5h; ε=10-1α; ε=10-1α; ε=10-5108network performance. We could imagine the cloud provider
installing a debugging stub-module within the host hyper-
visor (similar to other recent works [36]) that essentially, at
the signal of a management host controlled by the network
operator, can start storing each packet’s hash that matches
a given hurting application, or a hurting customer. It can
then query these packets along the route to its destination
to determine its latency. In this case, it makes sense to put
this stub module within the hypervisor since it is the one
that knows which packets are going out of its system; the
management host cannot possibly know how to query for
the packets since it does not know either the packet hash or
the IP id sequence.
In the ﬁrst usage scenario, we essentially trust the end
host to not overwhelm the switch by injecting too many
queries. This is possible in a tightly controlled datacenter
or cluster environment, but may not be, for example, possi-
ble in a public cloud environment such as Amazon EC2 (the
second scenario).
In such cases, we need some other pro-
tection mechanisms (e.g., charging models, rate limiting) to
ensure the number of queries to switches does not exceed
some limit.
5. EVALUATION
In this section, we evaluate the practicality of our MAPLE
architecture. Speciﬁcally, our experiments are designed to
answer the following questions.
(1) How do the diﬀerent
clustering algorithms perform ?
(2) How do the various
data structures we discussed in §3.3 compare in terms of
their accuracy for a given storage budget ? (3) How eﬃ-
cient is the query interface in terms of latency estimates of
arbitrary aggregates, bandwidth reduction and inaccuracies
in query timing ? (4) How does it compare with previous
approaches such as RLI ? We ﬁrst describe our experimental
setup before answering these questions.
5.1 Experimental setup
While we envision the eventual deployment to be in the
form of a hardware prototype, for the purposes of evaluation,
we prototyped various pieces, notably the streaming clus-
tering algorithm and the storage data structure, of MAPLE
in software. We implemented the online portion of the k-
medians algorithm from scratch (about 300 lines of C++
code), while we used the C clustering library [5] for the
oﬄine part. However, the library had to be modiﬁed to
support clustering data with weight (i.e., count of entries
clustered to a candidate center at the online stage). For
most of the experiments, we use 50 centers (k = 50) that, as
we shall show, represent a good balance between accuracy
and complexity.
In our setup, we feed several packet traces (real router as
well as using synthetic queueing models) into the software
prototype to study its eﬃcacy. We can however easily re-
place the packet traces with live traﬃc in our environment.
For the most part, we kept our evaluation setup very sim-
ilar to prior work [27]. We also used the same traces—a
tier-1 trace (ISP) collected at an OC-192 link and a real
router trace (RR)—as the authors of [27] to facilitate a fair
comparison with prior work. Benson et al. [9] have recently
published data center traces (UNIV1 and UNIV2) that be-
long to a university data center edge router. However, the
data center traces are not suﬃcient workloads (data rates
of 2-60Mbps) for testing the scalability of our architecture.
Nevertheless, since the traces reﬂect real ﬂow size distribu-
tion in data centers, we use them in §5.4 to evaluate query
bandwidth saving that can be achieved by the range query
mechanism discussed in §4.
The RR trace was collected at a pair of ingress and egress
interfaces in a real router. Therefore, it contains two times-
tamps of every packet that passes the router; and this im-
mediately allows us to compute real delays of packets. On
the other hand, the ISP trace was collected at a single inter-
face, which means that it only has one timestamp of every
packet arriving at the interface. Thus, to facilitate our ex-
periments with this trace, we simulate the delays of packets;
we subject packets to a simple, synthetic queueing model
with open-loop RED queue management strategy with pa-
rameters conﬁgured similar to the setup in [27]. We conﬁg-
ure the packet processing time in terms of byte/second and
queue length in the model. Real packet lengths and inter-
arrival distribution govern dynamics of packet delay values
and losses.
The ISP trace we used in our setup has about 22.4M
packets in a period of 60s. We divide the 60s period into
60 measurement epochs each with 1s duration. Recall that
our architecture operates in epochs and freezes the storage
for lookup after every epoch. The number of packets in
an epoch ranges from 358K to 404K. We ﬁnd about 40K
hosts and 4.2M ﬂows (considering same ﬂow key across two
epochs as two diﬀerent ﬂows) on average in each epoch. We
also conducted experiments using an RR trace set that con-
tains two diﬀerent traces; one has 2.6 million packets for 5
minutes achieving 53% utilization of an OC-3 link, and the
other has 4.0 million packets during the same period (88%
link utilization). The traﬃc source is artiﬁcial in that, it is
generated by Harpoon traﬃc generator, but all packets were
subject to latency factors in a real router. Qualitatively, we
found consistent results across both ISP and RR traces and
hence we do not discuss the results on RR traces any further.
5.2 Performance of clustering algorithms
We ﬁrst compare static (logarithmic), dynamic (pipelined
k-medians) and the hybrid strategy that combines the two.
For reference we also include a hypothetical non-pipelined
k-medians (called oracle) approach, that essentially runs
the k-medians on the data directly, determines the centers,
and then clusters the packets into these centers. In all al-
gorithms, we assume a perfect data structure for storing
the approximate delays, i.e., no Bloom ﬁlters to introduce
any interference. This gives us a baseline for comparison.
As mentioned before, we compare these schemes assuming
k = 50 centers. While we conduct experiments with three
traces of diﬀerent link utilization scenarios to comprehen-
sively understand the tradeoﬀs, we omit showing all the
curves due to space limitations. We show the absolute error
CDF for only the high (85 %) utilization case in Figure 5(a).
In our experiments, we observe that oracle achieves the
smallest absolute error at higher quartiles among all meth-
ods across all link utilization scenarios as it minimizes the
summation of absolute distance between entries and their
closest centers, which is exactly the absolute error. Since
its objective is to decrease the absolute error, it may allo-
cate centers that may increase the relative error for some
packets, particularly the low-latency packets. We observe