account age is controlled by the account owner, and group
membership is controlled by the group admin. In contrast,
aggregated features that are generated from entities associated
with the target account are much more difﬁcult to change. For
example, if we consider the age of all of a user’s friends, the
mean value would be much more difﬁcult to alter by that user,
especially when the number of friends is large. Eventually,
we can even take a step further by scrutinizing all the friends
of friends, and it becomes almost impossible for an adversary
to completely change such information.
Table 1 lists some of the entity types considered by DEC,
including user, group, device, photo, status update, and group
post. For each entity type, we list a few examples of direct
features and deep (or fan-out) entities. For direct features, we
use features effectively leveraged by other ML classiﬁers, as
well as those found useful during manual investigations.
Figure 2 illustrates an example deep feature. This feature is
based on neighboring nodes within two hops from an example
account (center, color orange). An edge between two nodes
represents the relation of mutual friends. This 2-hop deep
feature has exponentially more dependent values comprising
the feature than a direct feature.
5.2
To extend the above examples to work in production, we have
three issues to address: (a) What kind of neighboring nodes
do we look at? (b) How can we generate the deep features
meaningfully? and (c) How do we keep the computational
cost from exploding as we fan out?
Implementation
The complex and varied nature of OSN products requires us
to build our system as generically as possible, allowing us to
incorporate a wide variety of entities and edges between them.
We also want to be able to add new types of entities or edges as
4102    30th USENIX Security Symposium
USENIX Association
Table 2: Example aggregation methods for deep features. Here
p25 and p75 refer to the 25th and 75th percentiles, respec-
tively.
Feature Type
Numeric
Categorical
Aggregation Method
min, max, mean, variance, p25, p75
percentage of the most common category,
percentage of empty values,
entropy of the category values,
number of distinct categories
Both Numeric &
max of numeric A from category B,
Categorical
p75 of numeric A from most common category
ious graph traversal steps (e.g., user → user, or user → group
→ photo) and automatically apply all aggregation methods
to all the direct features of the target entity. In practice, this
method produces thousands of distinct deep features.
Figure 2: Visualization of the level-2 social graph for a single
“target” account in DEC. The centered orange node is the
target node to classify. The blue nodes are the neighboring
nodes from the ﬁrst fan-out level. The red nodes are from the
second fan-out level. An edge between two nodes represents
the relation of mutual friends. For each node visualized in
this graph, hundreds of features are extracted and aggregated
for classiﬁcation.
new features and products appear on Facebook. In the social
graph, even a single pair of entities can be connected with
multiple types of edges. For example, a user can be connected
to a group by being the admin of the group. They can also be
connected through membership, which is a weaker connection.
Even further, a user can be connected by commenting on a
post from the group.
To deﬁne deep features, we apply aggregation techniques
on the set of direct features of nodes, following the lead of
Xiao et al. [57], who effectively leveraged aggregated features
across clusters of accounts to identify fake ones. As shown
in Table 2, we use different aggregation methods for numeri-
cal features and categorical features. To aggregate numerical
features such as age, we calculate statistics on their distri-
bution such as mean and percentiles. On the other hand, for
categorical features such as home country, our strategy is to
aggregate them statistically into numerical features. Lastly,
we also jointly aggregate numeric features with categorical
features by observing the distribution of the numeric features
for a given categorical feature. For example, a feature can
be the number of accounts that logged in from the same de-
vice as the target account, given the device uses the Android
operating system.
The use of aggregation has two advantages: ﬁrst, it pro-
duces a dense feature vector, reducing the dimensionality of
the model. Second, it helps the model resist adversarial adap-
tation as discussed in Section 5.1 above. Note that we do not
need to deﬁne each deep feature explicitly: we can deﬁne var-
Ideally, we would trigger a new feature extraction and clas-
siﬁcation every time a user action happens on Facebook. This
is not possible at billion-user scale given the necessary compu-
tational resources. DEC relies on heuristics to decide when to
begin the process of feature extraction and (re-)classiﬁcation.
The core idea is the use of a “cool-down period” between
reclassiﬁcations, where the length of the cool-down period
increases as the account spends more time active on the plat-
form. Our motivating intuition is that accounts that have been
active for longer have gone through many previous checks and
are generally less likely to be abusive, while newly registered
accounts are more likely to be created to abuse.
While (re-)classiﬁcation is triggered in production in real
time, feature extraction and aggregation are computed asyn-
chronously without interfering with an account’s experience
on Facebook. Given the expense of extracting all deep fea-
tures, especially for an account with many connections in
the social graph, we restrict the amount of computational re-
sources used per account. Speciﬁcally, we place a limit on
the number of neighboring nodes used to compute a deep
feature, and sample randomly if the number is over the limit.
The random sample is different on each reclassiﬁcation; our
goal is to capture the position of the entity in the graph from
many different angles. This sampling procedure allows us
to limit computational cost without reducing the diversity of
features.4
5.3 Feature selection
We only use deep features of a target account, and not direct
features, for classiﬁcation in DEC. The primary motivation
for this choice is that we observed that direct target account
features are extremely likely to become dominant features
in the model. This undesired dominance is caused by the
bias inherent in our training data. For example, one of our
4In our implementation, we use up to 50 neighboring nodes to compute a
deep feature, downsampling if the number of neighboring nodes exceeds that
threshold. On average, two fan-out levels of neighboring entities are used for
feature computations.
USENIX Association
30th USENIX Security Symposium    4103
experimental spam detection models used whether a user
posts a URL as a feature; it turns out that this feature easily
becomes the dominant one in the model because spammers
are much more likely to include URLs in their posts than
benign users. However, it creates a huge number of false
positives as it classiﬁes almost all users posting URLs as
abusive. In addition, direct features are easy for the attacker
to manipulate; once the attacker learns that “has posted URL”
is a feature, they can switch from directly posting URLs to
putting URLs as overlay in a photo in order to avoid detection.
5.4 Feature modiﬁcation
As adversaries adapt and as we gain new insights about their
behavior, we will wish to add new features to DEC and/or
retire poorly performing features to save computation cost.
There are two issues to consider when modifying features.
The ﬁrst is the inﬂuence on the current detection model. Once
we add or remove any feature, the classiﬁcation result from
the original DEC model will be inﬂuenced as the model is still
trained using the original list of features. Our solution is to
split the feature logging into two pipelines: experimental fea-
tures and production features. We can log (or not log) newly
added (or removed) features into the experimental group, from
which we can train a new model. Meanwhile, the production
classiﬁer still uses the production list of features. When the
new model is pushed to production, we switch the experimen-
tal feature set into the production pipeline.
A second problem with adding features is the computa-
tional cost of re-computing across the entire graph. When
we add a new direct feature to an entity A, it not only in-
ﬂuences A, but also all the connected entities because they
use features from A to calculate their own deep features. Con-
versely, most direct features have multiple dependent deep fea-
tures, and multiple levels of fan-out can easily require the re-
computation of the whole feature space when a single feature
is added. For example, DEC needs to extract new_feature
from all of the friends of friends in order to compute 75th per-
centile, p75(friends.friends.new_feature). Traversing
through other features along with friends ultimately results in
re-extracting features of any active entity. To limit the impact
of the re-computation overhead, we deﬁne isolated universes
of features. The old and new versions of features will run in
parallel universes, with existing models using the old universe
of features, until feature generation for the new universe is
complete. At that point the functionality of the old universe
is subsumed, and it can be discarded as new models will be
trained using the new universe of features.
Again referring to Figure 2, we see the potential compu-
tational impact of feature changes. In this example a change
or addition of a new direct feature with dependent deep fea-
tures has exponentially more dependent computations than
the direct feature.
Figure 3: MS-MTL model training ﬂow. Stage 1 uses the
raw deep features with low precision labels to train a multi-
task deep neural network. By extracting the embedding from
the last hidden layer of the deep neural network, we train
dedicated GBDT models for each task in stage 2 with human
labeled data.
6 Methods: Multi-Stage Multi-Task Learning
Multi-task learning [6] (MTL) is a type of transfer learn-
ing [41] used to improve model generalization. MTL trains
multiple related “tasks” in parallel using a single neural net-
work model. The core idea is that what the model learns for
each task can boost the performance of other tasks. In our
context of abusive account classiﬁcation, we deﬁne “task” and
“label” as follows:
• A task refers to the classiﬁcation of a speciﬁc category of
abusive accounts on an OSN (e.g., fake accounts, spam-
ming accounts).
• A label of a training sample is a boolean value indicating
whether or not the sample falls into an abusive account
category. Each training example has multiple labels, one
for each task. This multi-label is represented by a vector of
boolean values.
As a concrete example, if we take four tasks in DEC model
training to be classifying fake, compromised, spamming, and
scamming accounts, the label vector of one account might
be [1,0,0,1]. This vector indicates the account was identi-
ﬁed as fake and carrying out scams, but is not identiﬁed as
compromised or spreading spam.
4104    30th USENIX Security Symposium
USENIX Association
Low Precision Multi-Label Vector[Fake? Compromised? ... Spam?]Deep Features(>2*104)GBDTFake LabelGBDTCompromised LabelGBDTSpam LabelStage 1Stage 2Results[Fake Score, Compromised Score ... Spam Score]Approxi-mate DataHuman Labelled DataDeep Neural NetworkInput LayerHidden LayerHidden LayerOutput LayerEmbedding6.1 Motivation
We employ a multi-stage framework to detect abusive ac-
counts on Facebook. Our framework addresses three key
challenges in abusive account classiﬁcation: simultaneously
supporting a variety of abuse types, leveraging a high-
dimensional feature space, and overcoming a shortage (rela-
tive to billions of accounts) of high quality human labels.
First, since there are many different ways in which an ac-
count can be abusive, we use different tasks to represent dif-
ferent sub-types of abuse, and multi-task learning to increase
the amount of information encoded in the model. The under-
lying assumption is that the features distinguishing abusive
accounts from benign ones are correlated between abuse types.
As a result, the knowledge learned for one abuse type can be
beneﬁcial for determining other abuse types because an ac-
count exhibiting one abuse type is more likely to show other
abusive behaviors. As compared with splitting labeled data
based on abuse types and training a separate model for each
type, multi-task training gives us a full picture of the account
by collectively looking at all associated abusive behavior. We
expect that this knowledge sharing across tasks will allow us
to achieve better prediction accuracy using multi-task learn-
ing, especially for smaller tasks.
Second, the multi-stage framework addresses the “curse of
dimensionality” [23] by reducing the high-dimensional raw
feature vector to a low-dimensional representation. Speciﬁ-
cally, our two stages of training reduce the number of features
from more than 104 (raw deep feature space) to around 102
(learned low-dimensional representation space). We achieve
this reduction by using the embedding from the last hidden
layer of the multi-task deep neural network as input features
for the second stage of training.
Finally, a practical engineering problem is that human la-
beled data is very expensive, and particularly so in the domain
of account labeling. In order to label an account as abusive
or benign, a human reviewer needs to look at many aspects
of the account and consider multiple factors when making a
decision. On the other hand, we have a large amount of lower-
conﬁdence labeled data in the form of machine-generated
labels. This scenario is ideal for multi-task leaning as it has
proven to be successful to extract useful information from
noisily labeled data [52].
6.2 Training Data Collection
We have two sources of data labels on abusive accounts in
DEC. The ﬁrst consists of human reviewers, who are shown
hundreds of signals from each account and asked to provide a
judgment on whether the account is abusive. Labels provided
in this manner have high accuracy, but are also computation-
ally expensive, and therefore can only be obtained in low
volume (relative to the billions of accounts on Facebook).
The second label source consists of automated (non-DEC)
algorithms designed to detect abusive accounts, as well as user
reported abusive accounts. These algorithms may be focused
on a speciﬁc attack or abuse type, or may be previous versions
of global abuse detection models. We consider the accounts
identiﬁed by these algorithms to be approximately labeled
abusive accounts. We then split the labels into different tasks
based on the type of abuse per each account. To obtain approx-
imately labeled non-abusive accounts, we randomly sample
accounts that have never been actioned on. Our approximate
labels have lower precision than human reviewed data, but are
much cheaper to obtain and can be obtained in high volume.
For example, in our evaluation the training dataset has over
30 million approximate labels and only 240,000 human labels
(Table 3).
While 30 million labels may seem signiﬁcant, it represents
less than 2% of the billions of accounts on Facebook. Thus,
any adversary attempting a poisoning attack [21, 36, 47] on
the training data would need to create thousands of accounts
in order to ensure that some of them were sampled for our
training set as negative examples (and tens of thousands if
trying to poison the second stage). On the other hand, the fact
that there are millions of negative samples implies that any
one account cannot have outsize inﬂuence on the model, thus
increasing the required attack size even further. Such large
attacks are easy for both rule-based systems and human re-
viewers to detect and label, and thus the adversary’s intention
of poisioning the training set will be foiled. Furthermore, even
if somehow the adversary obtains enough accounts to poison
the training process, they will need to manipulate the features
on these accounts to produce very speciﬁc values, which (as
discussed in Section 5.1) is difﬁcult to achieve with our “deep
feature” architecture.
To provide insight into the reliability of this approach, we
took a random sample of approximately labeled accounts
and sent them through the manual review process described
previously. In those experiments the approximate labeling
precision varied between 90% and 95%, indicating that the
approximate labels still provide signiﬁcant discerning power.
6.3 Model Training Flow
Figure 3 shows the two stage training ﬂow of the MS-MTL
framework. The ﬁrst stage, trained on a large volume of low
precision data, learns the embedding of the raw features. We
then apply a transfer learning techique and use the embedding
along with high precision labels to train the second stage
model. The classiﬁcation results are generated as the outputs