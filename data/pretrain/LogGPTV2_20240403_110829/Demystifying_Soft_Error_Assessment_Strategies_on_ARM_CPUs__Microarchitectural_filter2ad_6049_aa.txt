title:Demystifying Soft Error Assessment Strategies on ARM CPUs: Microarchitectural
Fault Injection vs. Neutron Beam Experiments
author:Athanasios Chatzidimitriou and
Pablo Bodmann and
George Papadimitriou and
Dimitris Gizopoulos and
Paolo Rech
2019 49th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
Demystifying Soft Error Assessment Strategies on
ARM CPUs: Microarchitectural Fault Injection vs.
Neutron Beam Experiments
Athanasios Chatzidimitriou†
Pablo Bodmann*
George Papadimitriou† Dimitris Gizopoulos†
Paolo Rech*
†Dept. of Informatics and Telecommunications, University of Athens, Athens, Greece
{achatz | georgepap | dgizop}@di.uoa.gr
*PPGC, Institute of Informatics, Federal University of Rio Grande do Sul, Porto Alegre, Brasil
{prjbodmann | prech}@inf.ufrgs.br
Abstract—Fault injection in early microarchitecture-level sim-
ulation CPU models and beam experiments on the ﬁnal physical
CPU chip are two established methodologies to access the soft
error reliability of a microprocessor at different stages of its
design ﬂow. Beam experiments, on one hand, estimate the
devices expected soft error rate in realistic physical conditions
by exposing it to accelerated particles ﬂuxes. Fault injection in
microarchitectural models of the processor, on the other hand,
provides deep insights on faults propagation through the entire
system stack, including the operating system. Combining beam
experiments and fault injection data can deliver deep insights
about the devices expected reliability when deployed in the ﬁeld.
However, it is yet largely unclear if the fault injection error rates
can be compared to those reported by beam experiments and
how this comparison can lead to informed soft error protection
decisions in early stages of the system design.
In this paper, we present and analyze data gathered with
extensive beam experiments (on physical CPU hardware) and
microarchitectural fault injections (on an equivalent CPU model
on Gem5) performed with 13 different benchmarks executed on
top of Linux on an ARM Cortex-A9 microprocessor. We combine
experimental data that cover more than 2.9 million years of
natural exposure with the result of more than 80,000 injections.
We then compare the soft error rate estimations that are based
on neutron beam and fault injection experiments. We show that,
for most benchmarks, fault injection can be very accurately
used to predict the Silent Data Corruptions (SDCs) rate and
the Application Crash rate. The System Crash rate measured
with beam experiments, however is much larger than the one
estimated by fault injection due to unknown proprietary parts
of the physical hardware platform that can’t be modeled in the
simulator. Overall, our analysis shows that the relative difference
between the total error rates of the beam experiments and the
fault injection experiments is limited within a narrow range of
values and is always smaller than one order of magnitude. This
narrow range of the expected failure rate of the CPU provides
invaluable assistance to the designers in making effective soft
error protection decisions in early design stages.
Index Terms—CPU reliability, soft errors, failures in time,
neutron beam, fault injection, microarchitecture simulation
I. INTRODUCTION
Reliability has become one of the main constraints for
computing devices employed in several domains, from High
Performance Computing (HPC) to automotive, military, and
aerospace applications [1]–[3]. Reliability has been identiﬁed
by the U.S. Department of Energy (DOE) as one of the ten
major challenges for exascale performance computing [1].
In fact, a lack of understanding or the underestimation of
devices and applications error rate may lead to lower scientiﬁc
productivity of large scale HPC servers, resulting in signiﬁcant
monetary loss [4]. When the computing device is integrated in
cyber-physical systems such as cars, airplanes, or Unmanned
Aerial Vehicles (UAVs), high reliability becomes mandatory
and unexpected errors should be strictly avoided.
While errors that may undermine the reliability of a com-
puting system can come from a variety of sources such as
environmental perturbations, manufacturing process, temper-
ature, and voltage variations [5]–[7], we focus on radiation-
induced effects since they have been found to be the main
reliability threat in commercial devices [8]. Such soft errors
may corrupt data values or logic operations and lead to Silent
Data Corruption (SDC), crashes, or can be masked and cause
no observable error [9]–[11].
To be useful and effective any method for the chip’s
reliability evaluation must be both ﬁne grain (i.e., providing
full visibility and understanding of faults effect in the microar-
chitecture and the software stacks) and realistic. A ﬁne grain
evaluation is employed to capture a clear understanding of the
causes and effects of faults, to identify the most vulnerable
parts of the hardware or software, and to observe how raw bit
ﬂips propagate through the system stack. A realistic evaluation
ensures a correct prediction of the expected error rate of the
device when used in the ﬁeld and provides realistic models of
the faulty behavior. The ﬁne grain and realistic evaluation of
the reliability of Commercial Off-The-Shelf (COTS) devices,
that became increasingly common in both safety-critical and
HPC applications, is challenging as information about the
architecture and device characteristics is typically very limited.
Beam experiments are the most realistic way to measure
the error rate of a code or device in conditions that are as
close as possible to the actual ones after system deployment.
However, beam experiments are coarse grain, i.e., errors can be
observed only when they manifest at the application output or
978-1-7281-0057-9/19/$31.00 ©2019 IEEE
DOI 10.1109/DSN.2019.00018
26
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:16:22 UTC from IEEE Xplore.  Restrictions apply. 
when they compromise the system responsiveness. There is no
information about the spatial/temporal location of the original
fault and no information on its propagation pattern through
the microarchitecture, the system software or the application
software layers. It is, therefore, hard to identify the most
vulnerable hardware resources or software code portions and
to extend/generalize results to other applications or devices
when relying on beam experiments only.
Fault injection on models of a microprocessor at different
detail levels (architecture, microarchitecture, register-transfer,
transistor) has been extensively used to evaluate the vul-
nerability of hardware architectures and software codes. By
injecting faults in the hardware blocks of a microprocessor or
particular parts of a software code it is possible to measure
the probability for faults to impact the application output or
the system responsiveness, i.e., the Architectural Vulnerability
Factor, AVF [12]. Fault injection can then identify codes, code
portions, or architectural resources which are more likely,
once corrupted, to affect the system reliability. This infor-
mation is extremely useful to design dedicated and efﬁcient
hardening solutions. It has been shown that fault injection
at the microarchitecture level is a very effective (fast and
accurate method) for early assessment of the reliability of
a microprocessor [13], [14]. However, faults can typically
be injected in a limited set of resources and, due to time
constraints, simpliﬁed errors models (such as single bit ﬂip)
are usually adopted. In other words, if fault injection is not
validated or tuned with physical experimental data, the efforts
could potentially lead to imprecise results.
In this paper, we present data on 13 benchmarks executed on
the top of Linux on a ARM Cortex-A9 system. The extensive
beam experiment covers more than 2.9 million years of natural
exposure. Additionally, we inject more than 80,000 faults.
By combining beam experiments and microarchitectural fault
injection results we provide a reliability evaluation which is
both precise and realistic. Then, comparing the error rate pre-
dicted with beam experiments and fault injection, we evaluate
at which level fault injection can be used to emulate beam
experiment. To the best of our knowledge this is the ﬁrst
reported comparison and analysis of the two popular methods
on top of an actual hardware and a detailed simulation model
of a widely employed CPU.
As a case study we choose the ARM Cortex-A9 architecture
as it was available in both a hardware platform (Xilinx
Zynq ZedBoard) and microarchitecture-level model (in Gem5).
Thanks to its high computing efﬁciency, ARM architecture
is actually used in the next computational core of many
supercomputers, including the new Sandia and Los Alamos
National Lab. clusters [15]. For the same reasons, embedded
architectures are attractive for safety-critical applications such
as autonomous vehicles or space exploration systems [16].
We select a wide and heterogeneous set of codes from the
MiBench suite [17]. The tested codes have different computing
characteristics and stimulate different resources. This allows us
to correlate the fault injection and beam experiment results
with the code characteristics and to extend, under certain
circumstances, the analysis to other algorithms with similar
characteristics. The codes are executed on the top of Linux to
simulate a realistic application and to show that our analysis
can give deep information on complex systems reliability.
The main contributions of this paper are: (1) an extensive
experimental evaluation of the reliability of ARM devices,
based on beam experiments; (2) a fault
injection detailed
analysis of the vulnerability of codes executed on the top of
Linux in ARM A9 devices; (3) the comparison and discussion
of the error rate predicted through beam experiment and fault
injection.
The rest of the paper is organized as follows. Section II
discusses the background including details of the two reliabil-
ity assessment methodologies. Section III reports related work
from the literature to position our contributions. Section IV
presents a detailed description of the evaluation methodologies
(benchmarks, hardware, and software setups). In Section V we
combine the results of the experimentation with the two setups
and in Section VI we compare the ﬁndings and reports. Finally,
Section VII draws the conclusions of the paper.
II. BACKGROUND
In this section we brieﬂy review the basic concepts of
radiation-induced effects in modern computing devices and the
most commonly used methodoloies to evaluate their reliability.
A. Radiation Effects in Electronic Devices
When a galactic cosmic ray interacts with the terrestrial
atmosphere, it triggers a chain reaction that generates a ﬂux of
particles (mainly neutrons). About 13 neutrons/((cm2) × h)
reach ground [18]. A neutron strike may perturb a transistor’s
state, generating bit-ﬂips in memory or current spikes in logic
circuits that, if latched, lead to an error [19]. A transient
error can have no effect on the program output (i.e., the fault
is masked, or the corrupted data is not used) or propagate
through the abstraction stack of the system leading to a Silent
Data Corruption (SDC), or unrecoverable behaviors such as a
program crash or device reboot.
The error rate of a code running on a computing device de-
pends on both the memory/logic sensitivity [20], [21] and the
probabilities for the fault to propagate through the architecture
or program [12], [22]. Hardening solutions can be applied at
different levels of abstraction (from transistor level to software
or system level) to reduce the faults probability of occurrence
or to avoid fault propagation.
B. Reliability Evaluation Methodologies
The evaluation of the error rate of a device is essential
to understand if the device meets the project’s reliability
requirement. Additionally, an early pre-silicon prediction of
the device error rate is useful to evaluate if the reliability needs
to be improved and to identify possible design vulnerabilities.
Realistic error rates can be measured exposing the real
hardware to controlled particles beams. By exposing the device
to particle beams, it is possible to mimic the effect of natural
radiation on the ﬁnal system in the ﬁeld. Thanks to the high
27
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:16:22 UTC from IEEE Xplore.  Restrictions apply. 
PERFORMANCE OF DIFFERENT ABSTRACTION LAYER MODELS [23], [24].
TABLE I
Abstraction Layer
Model
Software (native)
Architecture
Modern processors
Gem5 atomic model
Microarchitecture
Gem5 detailed out-of-order model
RTL
NCSIM simulation
Performance
(Cycles/sec)
2 × 109
2 × 107
2 × 105
6 × 102
particles ﬂux intensity, a statistically signiﬁcant amount of data
is gathered in a short time. Moreover, the same kind of faults
that would impact the device in its application in the ﬁeld
are injected in all physical hardware resources with realistic
probabilities. Unfortunately, beam experiments offer limited
visibility of fault propagation as faults are observed only
when they compromise the system functionality (corrupting
the output or crashing the application/system). With beam
experiments it is then very hard to correlate the observed
effects with their causes, limiting the identiﬁcation of the
system most vulnerable parts. Additionally, beam experiments
can obviously be performed only on real hardware, after the
device project has been ﬁnalized.
Designs that need to comply with certain dependability
constraints require decisions to improve the reliability of the
system but without adding unnecessary overhead. Reliability
evaluation is intended also to support such decisions. It is
critical to have this analysis in time and as early as possible,
since any additional re-design iteration can lead to catastrophic
costs. As a result, early-reliability assessment is often per-
formed in models that exist prior to silicon prototypes, which
can be summarized as architecture level, microarchitecture
level and RTL. These vary in level of detail, with the most
abstract being available earlier in the design chain while
the most detailed (RTL) being available at the later stages.
Architecture-level models often lack most, if not all, of the
hardware details of the system, offering a software level
functional emulation, while microarchitecture-level includes
most functional and timing-accurate models of the microarchi-
tecture (pipeline, cache memories etc.), offering clock cycle
accuracy. In addition, most memory elements of the system
(including SRAMs, pipeline registers and ﬂops/latches not
related to logic; e.g. state machines) are accurately modeled
in microarchitecture level. RTL offers a full description of
the implemented hardware, including the logic and SRAM
components. Simulation time for each abstraction layer is
proportional to the level of detail, with each detail step adding
approximately 2 orders of magnitude more simulation time;
the most detailed RTL model ends up being extremely slow.
Table I illustrates the simulation throughput of each abstraction
level.
Different reliability evaluation techniques can be applied
on top of each model, delivering different levels of detail
along with throughput. Probabilistic and statistical models
[25], [26] often require a single simulation to deliver a rough
estimation of the reliability, based on simulation statistics.
ACE analysis [12], [27]–[29] on the other hand tries to capture
more details on the residency and lifetime of workload critical
data on each vulnerable component of the system, and weight
their vulnerability against their sensitive exposure time. ACE
analysis often requires one or a few simulations to quantify
the vulnerability, but also requires additional development
effort in order to capture all of the systems complexity. ACE
analysis is claimed to have adjustable accuracy (proportional
to the effort) but the tradeoff between effort and speed should
always lean towards speed, otherwise more straight-forward
approaches can be used [30]. Statistical fault-injection is one of
the most widely adopted approaches of reliability assessment.
It offers the ﬂexibility of variable accuracy (depending on the
size of statistical sample) while at the same time, it delivers
failure samples produced by simulation. On the drawback side,
the requirement of multiple simulations requires signiﬁcant
amount of time and, depending on the model detail, it can