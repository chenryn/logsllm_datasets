itself populated by the local agent. It stores requested configura-
tion changes such as creating and destroying MSUs. In effect, the
thread queue serves as a buffer of requested changes, and avoids the
overhead of locks and other consistency mechanisms that would
otherwise be required if the local agent directly manipulated the
worker threads’ data structures.
Local agents: Each runtime operates a local agent in its main
thread. The local agent is responsible for communicating with the
controller and other DeDoS runtimes over long-lived TCP connec-
tions. In particular, the local agent receives commands for configura-
tion changes from the controller and routes them to the appropriate
worker threads. In addition, at a configurable interval which we set
to 100ms, the local agent gathers statistics from all the threads and
forwards them to the controller.
6 CASE STUDIES
We demonstrate the feasibility and applicability of DeDoS by con-
sidering three case studies: a web server written using DeDoS’
MSU interface; an existing userspace protocol stack ported to De-
DoS; and an application written using a declarative domain specific
language [29] that has been translated into a DeDoS dataflow.
Web server. For our first case study, we implemented a simple web
server constructed as five MSUs. The I/O MSU accepts incoming
requests and steers them toward the Read MSU, which performs the
TLS handshake, deciphers data, and relays plaintext to the HTTP
MSU. The HTTP MSU implements NodeJS’s HTTP Parser [24].
Once the request is parsed, the HTTP MSU issues a call to the
database tier to retrieve some object file, then enqueues the request
to a Regex Parsing MSU, which uses the PCRE engine to parse it.
The final HTTP response is sent to the Write MSU, which wraps it
in a layer of TLS and sends it back to the client. We use OpenSSL
version 1.0.1f for TLS support in both Read and Write MSUs.
Importantly, with the exception of the I/O MSU, all of the web
server’s MSUs are event-driven and non-blocking. We favor non-
blocking MSUs to augment the overall utilization of our machines.
To avoid having to migrate socket states between machines, we
configured the controller to enforce that the I/O, Read, and Write
MSUs reside within the same DeDoS instance for a given client
connection. We use HAProxy [38] as a front-end load balancer,
allowing us to direct incoming client connections to any I/O MSU
on any DeDoS instance.
Our web server leverages DeDoS’ fine-grained modular architec-
ture to mitigate DoS attacks. In §7, we demonstrate the resilience
of our application against three DoS attacks: TLS renegotiation
attacks [1], ReDOS attacks [3], and HTTP SlowLoris attacks [37].
Userspace network stack. Our second case study consists of an
existing software project that we ported to run on DeDoS with min-
imal effort. PicoTCP [34] is an open-source userspace TCP stack
written in approximately 33, 000 lines of C code (as reported by
sloccount). We chose PicoTCP since it is well-structured and writ-
ten in a modular fashion, making it easy to manually determine
cut-points (i.e., to form MSUs).
We have separated out a standalone handshaking MSU from
PicoTCP. When a SYN flood attack occurs, the TCP Handshake MSU
is replicated into multiple copies on the same or different machines.
Load-balancing across these clones is achieved by using a consistent
hashing scheme within the PicoTCP MSU: based on a hash over
the incoming packet’s four-tuple (source and destination addresses
and ports), DeDoS performs load-balancing by distributing the
handshaking requests (in the form of SYN, SYN/ACK, ACK packets)
to the various TCP Handshake MSU instances. Packets belonging
to the same three-way handshake (e.g., the client’s SYN and ACK)
are routed towards the same TCP Handshake MSU, obviating the
need to transfer state.
Given the modular nature of the PicoTCP code, separating the
TCP stack into separate MSUs was fairly straightforward. The bulk
of our efforts lay in wrapping PicoTCP’s “main loop” within an
MSU to allow DeDoS’s runtime’s scheduler to execute the MSU
according to its scheduling policy. Within the PicoTCP MSU, we
added functionality to re-inject SYN-ACKs generated by Handshake
MSUs into the PicoTCP stack for them to be sent back to the client,
and code to restore TCP state received from Handshake MSUs
(for successful connections) into PicoTCP’s internal TCP state data
structure.
In summary, with only minor modifications, we transformed the
monolithic PicoTCP application into a DeDoS-enabled version in
which handshake components could be replicated on demand, both
within the local machine and on remote DeDoS instances. Overall,
we changed less than 0.1% of PicoTCP’s original codebase.
Declarative packet processing. We also consider an application
that is written in a domain specific language. We select an ap-
plication that does routing (packet forwarding), written entirely
as a declarative networking [29] program. Declarative network-
ing programs are written in a variant of Datalog called Network
Datalog (or NDlog). An NDlog program consists of a set of rules,
where each rule is of the form h :- b1,b2,. . . , bn, indicat-
ing that a head tuple is generated so long as all body tuples b1, b2,
..., bn are available. For example, the rule packet(@Y,A,Data) :-
packet(@X,A,Data), Neighbor(@X,A,Y) results in all packets
arriving at X being forwarded to neighbor Y based on some at-
tribute A (e.g., the packet’s header data). Declarative networking
has been adopted for network forensics, datacenter programming,
and overlay routing.
Since these programs have their roots in the database relational
model, they can be compiled into an MSU dataflow of relational
DeDoS: Defusing DoS with Dispersion Oriented Software
ACSAC ’18, December 3–7, 2018, San Juan, PR, USA
We consider connection latency, the time required to complete
a TCP handshake as measured by the client. We measure the con-
nection latency over a 15-minute period. During this time, the
client continuously creates new TCP connections at a steady rate,
sends (and receives) 32 bytes of data, and disconnects. Figure 3
shows the distribution of connection latencies for the vanilla and
DeDoS-enabled versions of PicoTCP under different client request
rates. The DeDoS-enabled version incurs a modest 5.5% increase in
connection latency.
Next, we measure the throughput that both TCP stacks can
achieve. We create a number of different clients that simultaneously
access the echo server. Each client repeatedly sends and receives
1024 bytes, with a 10ms pause between transmissions. We observed
that there is no significant difference between the throughput that
both stacks can achieve. PicoTCP and DeDoS both reached their
maximum bandwidth of 57.66 Mb/s and 57.71 Mb/s respectively
around 100 simultaneous connections and the throughput remained
similar for more than 100 simultaneous connections. (The absolute
numbers are low because PicoTCP is a userspace network stack,
designed for portability instead of maximum performance. Our goal
here is to measure the overhead of the DeDoS runtime.)
Overall, our results suggest that running applications within the
DeDoS runtime does not significantly change the throughput or
the latency it can achieve.
7.2 Attack mitigation
To evaluate the efficacy of DeDoS in mitigating DoS attacks, we
launch ReDOS, TLS renegotiation, SlowLoris, SYN flood, and a
volumetric flood attack against our applications. The first three are
launched on the webserver, while the latter two are on PicoTCP
and an NDlog program respectively.
7.2.1 Attacks against webserver . We configure our testbed for
attacks against our webserver as follows: the webserver is deployed
on three machines while three other machines run instances of
an in-memory database. All web requests access the database, and
HAProxy is used to distribute HTTP and database requests. The
remaining two machines are used to generate legitimate (“good”)
and attack traffic respectively. To demonstrate DeDoS’ ability to
defend against changing attacks and reclaim resources, under dy-
namic traffic patterns, we run a two hours long experiment during
which attack durations are randomly distributed. Good traffic is
generated by Tsung, and exponentially distributed. We simulate
diurnal variations by setting Tsung’s distribution mean at 1500 r/s,
and increasing by slices of 500 r/s up to 3000 r/s, at which point
it gradually decreases back down to 1500 r/s. Tsung’s requests are
configured to time out after 1s if they cannot connect. When no at-
tack occurs, clients experience average latencies of 50ms. For attack
traffic, we develop a C client which generates malicious ReDOS
and TLS renegotiation requests, and use an existing Python-based
SlowLoris [20] attack tool.
Figure 4 shows our main findings for different attacks on the
HTTP servers for a single run of the experiment than ran contin-
uously for 2 hours. The top figure shows response times for suc-
cessful connections averaged every second, while the middle figure
presents the connection success rate during this experiment. The
bottom figure shows the number of MSU instances of a given type
Figure 3: Connection latency for standalone PicoTCP and DeDoS-
enabled PicoTCP (“DeDoS”).
operators. For example, the body tuples are executed as a series of
pair-wise database join operations, additional filters in the form of
selection operators, and the head tuple is generated as a projection
operator. The generated tuple may be sent to the same or different
machine using DeDoS. We find such automatic translation to be a
promising method of adopting existing applications to DeDoS.
7 EVALUATION
In this section, we aim to answer two high-level questions through
several sets of experiments: (1) does DeDoS run with reasonable
overheads in normal operation, and (2) how well can DeDoS defend
against asymmetric DoS attacks, as compared to whole-system
replication? Our experimental testbed consists of a cluster of 8
machines connected via a 10 Gbps switch in a star topology. Each
machine has 8 1.80 GHz cores (with hyperthreading and DVFS
disabled), 64 GB of memory, and runs Linux kernel 4.4.0-62. We
note that a video demonstration of DeDoS is available online [2].
7.1 Overheads
To measure the overhead introduced by the DeDoS runtime during
normal operation, we run two applications described in §6 within
and outside of DeDoS on a single server machine.
Web server: We compare a DeDoS web server to a standalone web
server with the same implementation but compiled as a monolithic
application outside of DeDoS. Our workload consists of HTTPS
requests generated by Tsung [4] at an exponentially distributed
rate for a period of five minutes, with a mean of 2500 requests
per second (r/s). We average latencies over intervals of one sec-
ond. The standalone webserver has mean latency of 43ms, and
1.8ms standard deviation. DeDoS’ webserver has a mean latency of
48.5ms and 12.3 standard deviation. This accounts for a mean 10.5%
overhead introduced by DeDoS, which is caused primarily by the
enqueuing and dequeuing of data across MSUs. Because in their
current implementation, worker threads on DeDoS do not “steal”
work from remote queues, there are some rare instances of MSUs
sitting idles while others are building a queue, hence the higher
number of outliers with DeDoS.
PicoTCP: We compare a DeDoS-enabled version of PicoTCP to
the standalone monolithic (“vanilla”) version. The DeDoS-enabled
version uses a single worker thread on a single runtime and has
two MSUs: a handshake MSU and the remainder of the PicoTCP
stack as a separate MSU. As an application, we use a simple echo
server that mirrors back incoming requests.
1050100Number of good requests per second0246810Connection latency (ms)PicoTCPDeDoSACSAC ’18, December 3–7, 2018, San Juan, PR, USA
H.M. Demoulin, T. Vaidya et al.
deployed on the system over time. Attacks occur during the period
colored in red. We compare DeDoS to two other approaches: (1) an
approach that does not replicate at all under attack (“standalone”),
and (2) an approach that naïvely replicates an entire webserver to
one of the database servers when under attack (“naïve”). Initially,
the DeDoS’ webserver has 4 Read and 2 Regex MSUs on each of
the three starting machines.
During the entire course of the experiment, DeDoS is auto-
piloting without inputs from human users. We observe that DeDoS
can accurately detect and react to the injected attacks based on the
resource allocation polices described in §4 without apriori knowl-
edge of the attacks. DeDoS can consistently and automatically de-
cide on an effective mitigation strategy against different types of
attacks. Figure 4 shows that DeDoS consistently outperforms stan-
dalone and naïve approaches, and sustains low latency and high
response rate while standalone and naïve can only provide limited
or sporadic services.
TLS renegotiation attack: This attack consumes the victim’s CPU
by having malicious connections repetitively triggering TLS hand-
shakes. In our setup, a single handshake requires about 2.1ms com-
putation time (we use a 2048-bits RSA key), and every malicious
request triggers 100 renegotiations before closing. During the first
TLS renegotiation attack in Figure 4, the attacker increases the
strength of the attack from 1 to 100 r/s over a period of 13 mins.
At the start of the attack, standalone performs better than DeDoS
until CPUs get overwhelmed by attack requests (around 75 r/s);
it increases the average latency for good requests to the order of
seconds. Naïve replication performs even worse and causes connec-
tion success rate to drop to almost 0% once the entire webserver
has been replicated to the database machines. This is due to paging
that occurs on the database server as a result of the additional mem-
ory footprint imposed by the cloned webserver. Even successful
connections experience latency on the order of tens of seconds.
During the attack, DeDoS’ controller observes abnormal levels of
pending requests in the system, and gradually increases the number
of Read MSUs from 12 to 39 (1 more on each original machine, plus
8 per database machine). Unlike naïve replication, Read MSUs have
a low memory footprint and do not cause paging on the database
machines. This results in average latency of 70ms for good requests
during attack.
Once the attack stops, the DeDoS controller observes that the
conditions explained in Section 4.3 are met, and reclaims resources
by tearing down the cloned MSUs.
The second TLS attack in Figure 4 shows the performance of
DeDoS under a steady state attack with 100 r/s instead of a grad-
ual increase in attack strength. Under this relatively hight attack
strength, CPU resources for standalone are quickly overwhelmed,
and connection success rate for good clients falls to 50% with 3s
latency on average. DeDoS applies the same policies for resource
management, maintaining 39 Read MSUs, and while its perfor-
mance drops momentarily, it manages to serve good clients with
an average latency of 70ms.
ReDOS attack: In this attack, each malicious request issues a com-
plex regular expression operation that exploits a PCRE vulnerabil-
ity [7], requiring approximately 100ms of computation time. The
first ReDOS attack increases attack strength from 1 r/s to 200 r/s
and lasts 9 mins. Similar to TLS renegotiation, standalone initially
does better than DeDoS until CPUs are overwhelmed by malicious
requests. On the other hand, DeDoS gradually increases the num-
ber of Regex MSUs (up to 27 new instances) and maintains 100%
success rate, but with an increased average latency of 150ms. We
observe much less variations in the number of Regex MSU than
Read MSU because of the nature of the workload: TLS handshakes
are much shorter, and performed over non-blocking I/O, while the
regex parsing operating cannot be preempted by DeDoS. The sec-
ond ReDOS attack is performed at a steady rate of 200 r/s over
11mins. Standalone clients almost instantly experience average la-
tencies on the order of seconds after the attack is launched. DeDoS,
while initially overwhelmed as well, quickly recovers by spawning
27 new Regex MSUs, managing to keep the latency on the order of
tens of milliseconds.
HTTP SlowLoris: This attack targets the connection pool of the
webserver by exhausting the file descriptors (FDs) available for the
process. The attack works by opening a connection to the server,
and slowly sending HTTP headers one after the other, at such a