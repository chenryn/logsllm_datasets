all the 138 C programs and 322 C++ programs (which are small
programs extracted from real-world applications) in the CWE-416-
Use-After-Free category of Juliet Test Suite (JTS) [1], with each
program containing one single UAF vulnerability. In addition, we
also make use of synthetic UAF bugs automatically introduced into
the training programs, inspired by the bug insertion technique [47].
To do so, we first find all use-before-free pairs ⟨use (p), f ree (q)⟩
statically by conducting a control-flow reachability analysis from
a use (p) to a f ree (q), where ∗p and ∗q are aliases identified by
a flow-sensitive pointer analysis [58]. Next, we swap use (p) and
f ree (q) for each pair and run Valgrind [43] to detect dynamically
if the thus injected UAF bug manifests itself as a true bug under
the default test inputs in every program. Finally, all UAF samples,
including 623 false and 858 true bugs as shown in Columns 2 and 3
of Table 6, are annotated for feature extraction.
The training phase applies the standard 5-fold cross validation
to find optimal intrinsic SVM parameters that yield the best clas-
sification accuracy. We consider three standard metrics: accuracy,
Table 6: Results of training. #True and #False are the num-
bers of true and false UAF samples, respectively.
Program
rtorrent
less
bitlbee
nghttp2
JTS-C
JTS-C++
Total
Samples
#True
46
22
52
43
138
322
623
#False Accuracy
88.6%
96.9%
90.4%
82.7%
96.4%
97.4%
95.0%
69
237
31
61
138
322
858
Results
Recall
Precision
93.4%
81.0%
77.0%
91.0%
86.7% 100.0%
75.5%
86.0%
94.9%
97.8%
97.5%
97.2%
92.6%
95.8%
precision and recall. Accuracy is the percentage of correctly classi-
fied samples out of all the samples. Precision is the percentage of
correctly classified true positive samples out of the samples that
are classified as true positives. Recall is the percentage of correctly
classified true positive samples out of all the true positive samples.
Due to 5-fold cross validation, Tac is highly effective for the train-
ing programs, with its the accuracy, precision and recall results
given in Columns 4 – 6 of Table 6. For all the training programs
combined, Tac’s accuracy, precision and recall are 95.0%, 92.6%
and 95.8%, respectively. These results indicate that the SVM classi-
fier trained by using the 35 features (Table 2) and the RBF kernel
(Section 3.1) is effective in classifying true and false UAF samples.
4.2 The Analysis Phase
Our results are summarized in Table 7. Column 2 gives the number
of candidate UAF pairs computed by Tac’s pre-analysis, which
selects a candidate ⟨f ree (p), use (q)⟩ if f ree (p) can reach use (q)
context-sensitively via control-flow, where ∗p and ∗q are found to
be aliases by Andersen’s pointer analysis [5] implemented in [59].
In Columns 3 – 4, we give the results produced by Tac-NML
(i.e. Tac without machine learning). For each program, Column 3
gives the number of warnings reported and Column 4 gives the
reduction rate with respect to the number of warnings produced by
the pre-analysis. On average (across the eight programs), Tac-NML
achieves a reduction rate of 81.2%. This indicates that path-sensitive
typestate analysis alone is quite effective in improving the precision
of a coarse-grained pre-analysis. However, a total of 19,803 UAF
warnings are still reported, making Tac-NML impractical.
In Columns 5 – 6, we give the results produced by Tac when
machine learning is enabled. For each program, Tac has improved
Tac-NML significantly by reducing the number of warnings fur-
ther (Column 5) and thus achieving an impressive reduction rate
(with respect to Tac-NML) (Column 6). On average, Tac achieves a
reduction rate of 96.5%, resulting in only 266 warnings. This shows
its effectiveness in suppressing warnings raised.
Tac is also efficient, as shown in Column 7 (with the analysis
time of a program averaged over the five runs). Tac spends a total
of 4.2 hours on analyzing all the eight programs (consisting of 2,098
KLOC in total), with 90 seconds for the smallest program (less)
and 5,942 seconds for the largest program (php).
In the last three columns, with Column 8 giving the number of
true bugs (confirmed by manual inspection), Column 9 the false
positive rate (FPR), and Column 10 the true positive rate (TPR) for
ACSAC 2017, December 4–8, 2017, San Juan, PR, USA
Hua Yan, Yulei Sui, Shiping Chen, and Jingling Xue
Table 7: Analysis results. #Candidates is the number of candidate UAF pairs found by pre-analysis. #WarnsTac−N ML and
#WarnsTac denote the number of warnings reported by Tac-NML and Tac, respectively. Reduction1 is computed as (#Candi-
dates - #WarnsTac−N ML) / #Candidates. Reduction 2 is computed as ( #WarnsTac−N ML - #WarnsTac) / #WarnsTac−N ML. #True
is the number of true bugs (confirmed by manual inspection), FPR is the false positive rate and TPR is the true positive rate.
Program #Candidates #WarnsTac−N ML Reduction1
71.5%
rtorrent
82.9%
less
78.6%
bitlbee
nghttp2
78.5%
92.4%
mupdf
80.4%
h2o
87.4%
xserver
php
77.9%
—
Total
#WarnsTac Reduction2
100.0%
99.6%
85.8%
92.4%
97.0%
99.4%
98.5%
99.0%
—
Time (secs)
90
316
151
83
197
6,205
2,053
5,942
15,037
803
4,628
529
975
21,701
18,143
53,258
26,306
126,343
#True
0
1
9
7
19
9
40
24
109
FPR
—
66.7%
43.8%
56.3%
62.0%
60.9%
60.8%
57.1%
—
TPR
—
33.3%
56.3%
43.8%
38.0%
39.1%
39.2%
42.9%
—
229
790
113
210
1,658
3,559
6,706
5,818
19,083
0
3
16
16
50
23
102
56
266
each program, we see that Tac is capable of finding UAF bugs at
low false positive rates. Out of the total 266 warnings reported, 109
are true bugs, yielding an FPR of 58.2% (or a TPR of 41.8%). Thus,
our machine-learning-guided approach is effective in locating UAF
bugs (with reasonable manual inspection effort required).
Among the 109 bugs found, 14 bugs are distinct (with the UAF
pairs sharing the same free site and dereferencing the same pointer
at their use sites being counted as one), as listed in Table 5. These
include 6 known ones (5 known CVE vulnerabilities and 1 known
bug) and 8 new ones (1 in less, 5 in h2o and and 2 in php).
the 5 new bugs are all
For less, the 1 new bug is found in a while loop in
function ch_delbufs in ch.c (illustrated in Figure 7). For
interprocedural due to pre-
h2o,
mature connection close operations,
including one in func-
tion do_emit_writereq in connection.c (illustrated in Fig-
ure 8), one in function h2o_timeout_unlink in timeout.c,
one in function h2o_http2_scheduler_run in scheduler.c,
one in function h2o_linklist_unlink and one in function
h2o_linklist_islink in linklist.c. For php, the 2 new bugs
are interprocedural, found in zend_persist.c (illustrated in Fig-
ure 9).
It is important to emphasize that for the 14 distinct UAF bugs
found, as listed in Table 5, only 3 bugs appear in the four train-
ing programs. This demonstrates again the effectiveness of our
approach in applying machine learning to static UAF detection.
4.3 Case study
Let us take a look at some representative UAF bugs (both previously
known and unknown) found by Tac in three programs.
less. Figure 7 shows a new UAF bug found in less (version
451) by Tac. At line 782, the program frees an object pointed to by
bn and then starts possibly the next iteration of the while-loop at
line 778. At line 780, bn is made to point to the same freed object
and then dereferenced four times at line 781, causing one distinct
UAF bug. This bug occurs since the programmer forgot to update
ch_bufhead in the while loop after bn has been freed.
h2o. Figure 8 shows a known UAF bug (CVE-2016-4817) in h2o
(version 1.7.2) detected by Tac. The program frees conn at line 261
in close_connection_now through a nested call chain via lines 834
and 861. Then conn is used in the function timeout_unlink called
Figure 7: A UAF bug found in less, with the free, use and
their aliasing highlighted in pink, blue and red, respectively.
Figure 8: CVE-2016-4817 and two new bugs in h2o.
at line 865. Tac has succeeded in finding this CVE vulnerability and
also two new bugs on dereferencing conn at lines 1006 – 1007 in
the function do_emit_writereq called at line 866. These two new
bugs are counted as one distinct bug.
php. Figure 9 shows a known UAF bug (CVE-2015-1351) and
two new ones in php (version 5.6.7) detected by Tac. These bugs
are found in two files. CVE-2015-1351 is in zend_shared_alloc.c,
//ch.c774  static void ch_delbufs()775 {776 register struct bufnode *bn;777778 while (ch_bufhead != END_OF_CHAIN)779 {780 bn= ch_bufhead;781 (bn)->next->prev = (bn)->prev; (bn)->prev->next = (bn)->next;782 free(((struct buf *) bn));783 }784 ch_nbufs = 0;785 init_hashtbl();786   }step2step3step4step1//lib/http2/connection.c228 voidclose_connection_now(http2_conn_t *conn){261 free(conn);262 }811 static voidparse_input(http2_conn_t *conn){829 if (ret data;861 parse_input(conn);865 timeout_unlink(&conn->_write.timeout_entry);866 do_emit_writereq(conn);868 }994 intdo_emit_writereq(http2_conn_t *conn){1006 buf ={conn->_write.bbytes,conn->_write.bsz};1007 socket_write(conn->sock, &buf, 1, on_w_compl);1012 }step1step2step3step4step5step6step7step8Machine-Learning-Guided Typestate Analysis for Static Use-After-Free Detection ACSAC 2017, December 4–8, 2017, San Juan, PR, USA
restricts the program execution to follow a precomputed CFG even
if there are memory corruption bugs (e.g., UAF). Garbage collection
for C/C++ [7] can mitigate some UAF bugs based on its automatic
memory management. However, this requires every call to malloc()
to be replaced by a call to a special allocator, and is thus hardly
useful for legacy code and code using customized allocators.
Static Analysis for Memory Error Detection. Static analysis has
been used for detecting a wide range of memory errors, such as