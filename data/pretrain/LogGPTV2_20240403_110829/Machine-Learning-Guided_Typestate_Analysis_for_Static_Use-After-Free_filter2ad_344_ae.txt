### Dataset and Synthetic Bug Generation

Our dataset consists of 138 C programs and 322 C++ programs, all extracted from real-world applications and categorized under the CWE-416-Use-After-Free (UAF) category in the Juliet Test Suite (JTS) [1]. Each program contains a single UAF vulnerability. Additionally, we introduce synthetic UAF bugs into the training programs using an automated method inspired by the bug insertion technique [47].

To generate these synthetic bugs, we first identify all use-before-free pairs ⟨use(p), free(q)⟩ through a static control-flow reachability analysis. This analysis is conducted from a use(p) to a free(q), where *p and *q are identified as aliases by a flow-sensitive pointer analysis [58]. Next, we swap use(p) and free(q) for each pair and run Valgrind [43] to dynamically detect if the injected UAF bug manifests as a true bug under the default test inputs in every program. The final dataset includes 623 false and 858 true UAF bugs, which are annotated for feature extraction.

### Training Phase

The training phase employs a standard 5-fold cross-validation to find the optimal intrinsic SVM parameters that yield the best classification accuracy. We evaluate the model using three standard metrics: accuracy, precision, and recall. Accuracy measures the percentage of correctly classified samples out of all samples. Precision is the percentage of correctly classified true positive samples out of those classified as true positives. Recall is the percentage of correctly classified true positive samples out of all true positive samples.

The results, summarized in Table 6, show that our approach, Tac, is highly effective. For all training programs combined, Tac achieves an accuracy of 95.0%, precision of 92.6%, and recall of 95.8%. These results indicate that the SVM classifier, trained using 35 features (Table 2) and the RBF kernel (Section 3.1), is effective in classifying true and false UAF samples.

### Analysis Phase

The analysis phase results are presented in Table 7. Column 2 lists the number of candidate UAF pairs computed by Tac's pre-analysis, which selects a candidate ⟨free(p), use(q)⟩ if free(p) can reach use(q) context-sensitively via control-flow, with *p and *q being identified as aliases by Andersen’s pointer analysis [5] implemented in [59].

Columns 3 and 4 provide the results from Tac-NML (Tac without machine learning). For each program, Column 3 gives the number of warnings reported, and Column 4 shows the reduction rate compared to the pre-analysis. On average, Tac-NML achieves a reduction rate of 81.2%, indicating that path-sensitive typestate analysis alone significantly improves the precision of a coarse-grained pre-analysis. However, it still reports 19,803 UAF warnings, making it impractical.

Columns 5 and 6 present the results when machine learning is enabled. Tac significantly reduces the number of warnings (Column 5) and achieves an impressive reduction rate (Column 6). On average, Tac achieves a reduction rate of 96.5%, resulting in only 266 warnings. This demonstrates its effectiveness in suppressing false warnings.

Tac is also efficient, as shown in Column 7, which provides the average analysis time over five runs. Tac spends a total of 4.2 hours analyzing all eight programs (2,098 KLOC in total), with the smallest program (less) taking 90 seconds and the largest program (php) taking 5,942 seconds.

The last three columns provide additional details: Column 8 gives the number of true bugs (confirmed by manual inspection), Column 9 shows the false positive rate (FPR), and Column 10 shows the true positive rate (TPR). Out of the 266 warnings reported, 109 are true bugs, yielding an FPR of 58.2% and a TPR of 41.8%. This indicates that our machine-learning-guided approach is effective in locating UAF bugs with reasonable manual inspection effort.

### Case Study

We examine representative UAF bugs (both known and unknown) found by Tac in three programs: less, h2o, and php.

#### less
Figure 7 illustrates a new UAF bug in less (version 451). At line 782, the program frees an object pointed to by `bn` and then starts the next iteration of the while-loop at line 778. At line 780, `bn` is made to point to the same freed object and is dereferenced four times at line 781, causing a distinct UAF bug. This bug occurs because the programmer forgot to update `ch_bufhead` after `bn` was freed.

#### h2o
Figure 8 shows a known UAF bug (CVE-2016-4817) in h2o (version 1.7.2). The program frees `conn` at line 261 in `close_connection_now` through a nested call chain via lines 834 and 861. Then `conn` is used in the function `timeout_unlink` called at line 865. Tac successfully detected this CVE vulnerability and also two new bugs on dereferencing `conn` at lines 1006–1007 in the function `do_emit_writereq` called at line 866. These two new bugs are counted as one distinct bug.

#### php
Figure 9 illustrates a known UAF bug (CVE-2015-1351) and two new ones in php (version 5.6.7). These bugs are found in two files. CVE-2015-1351 is in `zend_shared_alloc.c`, and the new bugs are in `zend_persist.c`.

### Conclusion

Among the 109 bugs found, 14 are distinct, including 6 known ones (5 known CVE vulnerabilities and 1 known bug) and 8 new ones (1 in less, 5 in h2o, and 2 in php). Only 3 of the 14 distinct UAF bugs appear in the four training programs, demonstrating the effectiveness of our approach in applying machine learning to static UAF detection.