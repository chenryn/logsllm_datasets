Bloom ﬁlter.
(The hash of the string is calculated;
this is used as a key of a HMAC for future adds and
queries.)
• nsrl_bloom_free() — Frees the memory asso-
for the membership of the hash.
Bloom ﬁlter.
ciated with the Bloom ﬁlter.
Our C implementation uses memmap to map the BF vec-
tor into memory for speedy lookup: in practice, this means
that the computer’s virtual memory subsystem pages the bit
vector into memory as needed and performs no unneeded
copies. If the entire BF is likely to be needed, the ﬁlter can
be paged into RAM all at once in order to minimize hard
drive latency due to random seeks.
Besides having a correct hash function, this new imple-
mentation dramatically faster and more memory efﬁcient
than the original perl version, allowing us to test it on the
complete RDS.
There is also a C++ class that allows zero-overhead ac-
cess to the C API.
3. Bloom ﬁlters for the RDS
Having completed our BF implementation, we pro-
ceeded to create a number of BFs that contained the SHA1
hashes from the NSRL RDS.
3.1. Building the ﬁlters
The RDS is distributed as four ISO images in ISO9660
format. Each image contains several text ﬁles. Details of
RDS are available online.[19]
We downloaded the ISO images for RDS 2.19 from the
NIST website, RDS 2.20 being published too late for in-
clusion in this paper. Each ISO consists of several text ﬁles
and a ZIP ﬁle containing more text ﬁles. We processed these
images with two programs: nsrlutil.py, a Python pro-
gram which mounted the disk images as ﬁles on a Linux
server, opened the compressed ZIP ﬁle, and sent the hashes
to standard output; and bloom, a program which created
a new BF using parameters provided on the command line
515
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:48 UTC from IEEE Xplore.  Restrictions apply. 
Memory System
L1 Data Cache
L2 Cache1
667 Mhz DDR2 SDRAM2
Disk
Size Cycle time
3 cycles
32K
14 cycles
4MB
4GB
5-5-5-15
n/a
1000GB
Latency
1.25 nsec
5.83 nsec
70 nsec
8.5 msec
Time to access
10,000 random bits
12.5 µsec
58.3 µsec
700 µsec
85 sec.
# hash lookups
per second
40,000,000
8,500,000
710,000
6
Table 1. Relative speeds to access a bit of a Bloom table or hash table stored in different memory
subsystems on a modern iMac computer (2.4Ghz Intel Core 2 Duo Processor E6600). Memory latency
information from [6, 12]. Disk access times are approximate, based on 8.5ms average seek time. A
“hash lookup” requires accessing 20 random bits.
and then loaded the ﬁlter with hash codes read from stan-
dard input. By piping these two programs together we were
able to rapidly create a large number of BF ﬁles, each with
a speciﬁc set of parameters.
3.2. Accuracy and Validation
Our goal was to create RDS BFs that would be small
enough to distribute on CD if necessary and to ﬁt into
the main memory of older machines or smaller PDA style
devices. We arbitrarily decided to evaluate BF of sizes
32MB, 64MB, 128MB, 256MB and 512MB. Such BFs can
be created with 228 through 232 one-bit elements (M =
28 . . . 32). But how many hash functions should be applied
to each element? That is, what is the optimal value of k,
and is it necessary to choose the optimal value?
Given that we knew the desired sizes of our ﬁlters and
RDS 2.19 has 13,147,812 unique hashes, we plugged these
numbers into the optimal ﬁlter equations and discovered
that a 512MB ﬁlter would require 226 hash functions for
a false positive rate of 6.89× 10−69. Clearly, this false pos-
itive rate is far lower than needed—it is, for example, con-
siderably smaller than the failure rate of the hard drive or
electronic media that would be used to store the ﬁlter. Fur-
thermore, there is not sufﬁcient entropy in a 160-bit hash
value to provide data for 226 hash functions, each provid-
ing 32-bits of uncorrelated output.
Limiting ourselves to the real-world requirements of the
RDS requires choosing the correct parameters for k and m
giving the fact that there are only 160 bits of data to di-
vide up for the hash. Of course, the values of k = 1 and
m = 2160 would produce no false positives at all, since each
bit in the BF would correspond to a unique hash value, but
such a BF would be impossibly large. The good news is that
with values of k = 5 and m = 232 we see no false positives
in our sample set; these settings allow the full 160 bits of
the SHA-1 value to be used (5× 32 = 160) in the BF calcu-
lation, with a theoretical false positive rate of 6.2 × 10−16).
Bloom Filters of this size can be comfortably downloaded,
stored on USB memory sticks, and stored in memory of
32-bit workstations. Although we would see a similar false
positive rate with k = 4 (and have an implementation that
is mildly faster), using k = 5 gives our BFs the ability to
accommodate additional information without a degradation
in accuracy.
To validate our code, we wrote a regression program
which tested each BF with a million hash values that we
knew were in the RDS and a million hash values that we
knew were not in the RDS. In keeping with the theory of
BFs, in no case was a value that was known to be in the
RDS reported to be absent from any of our ﬁlters. But also
in keeping with theory, we did observe occasional false pos-
itives for lower values of k and m (Table 2) than those we
recommend.
3.3. Performance
In this section we compare the performance of looking
up hash values in a BF, in a sorted text ﬁle, and in the
MySQL database—two systems that are commonly used by
today’s forensic tools for storing RDS hash codes. We also
explore the impact on BF performance of adjusting the k
and m parameters.
3.3.1 BF vs. hﬁnd and MySQL
With the BF, each lookup for a match takes f operations,
while a lookup for a non-match takes 1 . . . f operations.
Storing that same data sorted in a text ﬁle and perform-
ing a binary search consumes roughly log2(n) operations.
MySQL was conﬁgured to use InnoDB tables which is de-
signed to deliver high performance transaction processing,
with row level locking and multi-version concurrency con-
trol. Data is stored in B-trees.[13]
Tests were performed on a Red Hat FC6 server with two
quad-core Xeon processors with 2MB L2 caches running at
3.2Ghz and 8GB RAM. code was compiled with gcc 4.1.2
and run on a 2.6.22.9-61 kernel.
RDS is distributed as four ISO images. We combined the
hashes from all of these images into a single ﬁle. This RDS
2.19 ﬁle was imported into clean BF with m = 232, k =
5;
imported to SleuthKit[4] with hﬁnd using the com-
mand hfind -i nsrl-sha1 flatfile.txt; and
imported into a single MySQL database located on the test
machine to reduce network latency overhead affecting test
results.
616
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 13:00:48 UTC from IEEE Xplore.  Restrictions apply. 
222
(512KB)
223
(1MB)
224
(2MB)
225
(4MB)
226
(8MB)
227
(16MB)
228
(32MB)
229
(64MB)
230
(128MB)
231
(256MB)
232
(512MB)
m (BF size, in bits (MB))
Predicted number of false positives for 1 million random values:
24,192
2,285
355
76
20
n/a
n/a
177,920
105,096
87,780
87,111
95,013
109,179
n/a
543,274
626,315
740,548
836,980
904,503
946,760
n/a
324,184
295,146
330,423
392,271
467,768
548,411
n/a
93,314
31,656
16,510
11,045
8,708
n/a
n/a
47,799
8,707
2,552
1,002
484
n/a
n/a
791,401
914,866
973,016
992,448
998,027
999,506
n/a
k
1
2
3
4
5
6
7
k
1
2
3
4
5
6
7
956,486
996,217
999,753
999,986
999,999
1,000,000
1,000,000
956,483
996,126
999,758
999,989
999,999
1,000,000
1,000,000
Actual rate of false positives for 1 million random values:
791,511
914,938
972,802
992,175
997,916
999,463
n/a
543,165
625,765
740,271
836,379
904,260
946,587
n/a
323,971
295,401
330,808
392,427
467,288
548,083
n/a
178,416
105,465
88,079
87,606
95,662
109,982
n/a