        if (page_to_nid(page) != node) {
            pr_err("SLUB: Unable to allocate memory from node %dn", node);
            pr_err("SLUB: Allocating a useless per node structure in order to be able to continuen");
        }
        // 初始化 page 的相关成员
        n = page->freelist;
        BUG_ON(!n);
        page->freelist = get_freepointer(kmem_cache_node, n);
        page->inuse = 1;
        page->frozen = 0;
        kmem_cache_node->node[node] = n;
    #ifdef CONFIG_SLUB_DEBUG
        // 调用 init_object() 标识数据区和 RedZone
        init_object(kmem_cache_node, n, SLUB_RED_ACTIVE);
        // 调用 init_tracking() 记录轨迹信息
        init_tracking(kmem_cache_node, n);
    #endif
        kasan_kmalloc(kmem_cache_node, n, sizeof(struct kmem_cache_node),GFP_KERNEL);
        // 初始化取出的对象
        init_kmem_cache_node(n);
        // 调用 inc_slabs_node() 更新统计信息
        inc_slabs_node(kmem_cache_node, node, page->objects);
        /*
         * No locks need to be taken here as it has just been
         * initialized and there is no concurrent access.
         */
        // 将 slab 添加到 partial 链表中
        __add_partial(n, page, DEACTIVATE_TO_HEAD);
    }
###  `new_slab()`源码分析
`new_slab()`用于创建`slab`：(`new_slab()`在`/source/mm/slub.c#L1643`处实现)
    static struct page *new_slab(struct kmem_cache *s, gfp_t flags, int node)
    {
        if (unlikely(flags & GFP_SLAB_BUG_MASK)) {
            gfp_t invalid_mask = flags & GFP_SLAB_BUG_MASK;
            flags &= ~GFP_SLAB_BUG_MASK;
            pr_warn("Unexpected gfp: %#x (%pGg). Fixing up to gfp: %#x (%pGg). Fix your code!n",
                    invalid_mask, &invalid_mask, flags, &flags);
            dump_stack();
        }
        return allocate_slab(s, flags & (GFP_RECLAIM_MASK | GFP_CONSTRAINT_MASK), node);
    }
可以发现这个函数的核心就是去调用`allocate_slab`函数实现的。
###  `allocate_slab()`源码分析
`allocate_slab()`用于创建`slab`：(`allocate_slab()`在`/source/mm/slub.c#L1558`处实现）
    static struct page *allocate_slab(struct kmem_cache *s, gfp_t flags, int node)
    {
        struct page *page;
        struct kmem_cache_order_objects oo = s->oo;
        gfp_t alloc_gfp;
        void *start, *p;
        int idx, order;
        bool shuffle;
        flags &= gfp_allowed_mask;
        // 如果申请 slab 所需页面设置 __GFP_WAIT 标志，表示运行等待
        if (gfpflags_allow_blocking(flags))
            // 启用中断
            local_irq_enable();
        flags |= s->allocflags;
        /*
         * Let the initial higher-order allocation fail under memory pressure
         * so we fall-back to the minimum order allocation.
         */
        alloc_gfp = (flags | __GFP_NOWARN | __GFP_NORETRY) & ~__GFP_NOFAIL;
        if ((alloc_gfp & __GFP_DIRECT_RECLAIM) && oo_order(oo) > oo_order(s->min))
            alloc_gfp = (alloc_gfp | __GFP_NOMEMALLOC) & ~(__GFP_RECLAIM|__GFP_NOFAIL);
        // 尝试使用 alloc_slab_page() 进行内存页面申请
        page = alloc_slab_page(s, alloc_gfp, node, oo);
        if (unlikely(!page)) {
            // 如果申请失败，则将其调至s->min进行降阶再次尝试申请
            oo = s->min;
            alloc_gfp = flags;
            /*
             * Allocation may have failed due to fragmentation.
             * Try a lower order alloc if possible
             */
            page = alloc_slab_page(s, alloc_gfp, node, oo);
            if (unlikely(!page))
                // 再次失败则执行退出流程
                goto out;
            stat(s, ORDER_FALLBACK);
        }
        // 设置 page 的 object 成员为从 oo 获取到的 object。
        page->objects = oo_objects(oo);
        // 通过 compound_order() 从该 slab 的首个 page 结构中获取其占用页面的 order 信息
        order = compound_order(page);
        // 设置 page 的 slab_cache 成员为它所属的 slab_cache
        page->slab_cache = s;
        // 将 page 链入 slab 中
        __SetPageSlab(page);
        // 判断 page 的 index 是否为 -1
        if (page_is_pfmemalloc(page))
            // 激活这个内存页
            SetPageSlabPfmemalloc(page);
        // page_address() 获取页面的虚拟地址
        start = page_address(page);
        // 根据 SLAB_POISON 标识以确定是否 memset() 该 slab 的空间
        if (unlikely(s->flags & SLAB_POISON))
            memset(start, POISON_INUSE, PAGE_SIZE objects) {
                // 通过 setup_object() 初始化对象信息
                setup_object(s, page, p);
                // 通过 set_freepointer() 设置空闲页面指针，最终将 slab 初始完毕。
                if (likely(idx objects))
                    set_freepointer(s, p, p + s->size);
                else
                    set_freepointer(s, p, NULL);
            }
            page->freelist = fixup_red_left(s, start);
        }
        page->inuse = page->objects;
        page->frozen = 1;
    out:
        // 禁用中断
        if (gfpflags_allow_blocking(flags))
            local_irq_disable();
        // 分配内存失败，返回NULL
        if (!page)
            return NULL;
        // 通过 mod_zone_page_state 计算更新内存管理区的状态统计
        mod_lruvec_page_state(page,
            (s->flags & SLAB_RECLAIM_ACCOUNT) ? NR_SLAB_RECLAIMABLE : NR_SLAB_UNRECLAIMABLE,
            1 objects);
        return page;
    }
###  `alloc_kmem_cache_cpus()`源码分析
`alloc_kmem_cache_cpus()`用于进一步的初始化工作：(`alloc_kmem_cache_cpus()`在`/source/mm/slub.c#L3300`处实现）
    static inline int alloc_kmem_cache_cpus(struct kmem_cache *s)
    {
        BUILD_BUG_ON(PERCPU_DYNAMIC_EARLY_SIZE cpu_slab = __alloc_percpu(sizeof(struct kmem_cache_cpu), 2 * sizeof(void *));
        if (!s->cpu_slab)
            return 0;
        init_kmem_cache_cpus(s);
        return 1;
    }
###  `kmem_cache_alloc_node()`源码分析
`kmem_cache_alloc_node()`用于在`slub`分配器已全部或部分初始化完毕后分配`node`结构：(`kmem_cache_alloc_node()`在`/source/mm/slub.c#L2759`处实现）
    void *kmem_cache_alloc_node(struct kmem_cache *s, gfp_t gfpflags, int node)
    {
        void *ret = slab_alloc_node(s, gfpflags, node, _RET_IP_);
        trace_kmem_cache_alloc_node(_RET_IP_, ret, s->object_size, s->size, gfpflags, node);
        return ret;
    }
###  `slab_alloc_node()`源码分析
`slab_alloc_node()`用于对象的取出：(`slab_alloc_node()`在`/source/mm/slub.c#L2643`处实现）
    /*
     * Inlined fastpath so that allocation functions (kmalloc, kmem_cache_alloc)
     * have the fastpath folded into their functions. So no function call
     * overhead for requests that can be satisfied on the fastpath.
     *
     * The fastpath works by first checking if the lockless freelist can be used.
     * If not then __slab_alloc is called for slow processing.
     *
     * Otherwise we can simply pick the next object from the lockless free list.
     */
    static __always_inline void *slab_alloc_node(struct kmem_cache *s,
            gfp_t gfpflags, int node, unsigned long addr)
    {
        void *object;
        struct kmem_cache_cpu *c;
        struct page *page;
        unsigned long tid;
        // 主要负责对 slub 对象分配的预处理，返回用于分配 slub 对象的 kmem_cache
        s = slab_pre_alloc_hook(s, gfpflags);
        if (!s)
            return NULL;
    redo:
        /*
         * Must read kmem_cache cpu data via this cpu ptr. Preemption is
         * enabled. We may switch back and forth between cpus while
         * reading from one cpu area. That does not matter as long
         * as we end up on the original cpu again when doing the cmpxchg.
         *
         * We should guarantee that tid and kmem_cache are retrieved on
         * the same cpu. It could be different if CONFIG_PREEMPT so we need
         * to check if it is matched or not.
         */
        // 检查 flag 标志位中时候启用了抢占功能
        do {
            // 取得 kmem_cache_cpu 的 tid 值
            tid = this_cpu_read(s->cpu_slab->tid);
            // 获取当前 CPU 的 kmem_cache_cpu 结构
            c = raw_cpu_ptr(s->cpu_slab);
        } while (IS_ENABLED(CONFIG_PREEMPT) && unlikely(tid != READ_ONCE(c->tid)));
        /*
         * Irqless object alloc/free algorithm used here depends on sequence
         * of fetching cpu_slab's data. tid should be fetched before anything
         * on c to guarantee that object and page associated with previous tid
         * won't be used with current tid. If we fetch tid first, object and
         * page could be one associated with next tid and our alloc/free
         * request will be failed. In this case, we will retry. So, no problem.
         * 
         * barrier 是一种保证内存访问顺序的一种方法
         * 让系统中的 HW block (各个cpu、DMA controler、device等)对内存有一致性的视角。
         * barrier 就象是c代码中的一个栅栏，将代码逻辑分成两段
         * barrier 之前的代码和 barrier 之后的代码在经过编译器编译后顺序不能乱掉
         * 也就是说，barrier 之后的c代码对应的汇编，不能跑到 barrier 之前去，反之亦然
         */
        barrier();
        /*
         * The transaction ids are globally unique per cpu and per operation on
         * a per cpu queue. Thus they can be guarantee that the cmpxchg_double
         * occurs on the right processor and that there was no operation on the
         * linked list in between.
         */
        // 获得当前cpu的空闲对象列表
        object = c->freelist;
        // 获取当前cpu使用的页面
        page = c->page;
        // 当前 CPU 的 slub 空闲列表为空或者当前 slub 使用内存页面与管理节点不匹配时，需要重新分配 slub 对象。
        if (unlikely(!object || !node_match(page, node))) {
            // 分配slub对象
            object = __slab_alloc(s, gfpflags, node, addr, c);
            // 设置kmem_cache_cpu的状态位（相应位加1）
            // 此操作表示从当前cpu获得新的cpu slub来分配对象(慢路径分配)
            stat(s, ALLOC_SLOWPATH);
        } else {
            // 获取空闲对象地址(object + s->offset)
            void *next_object = get_freepointer_safe(s, object);
            /*
             * The cmpxchg will only match if there was no additional
             * operation and if we are on the right processor.
             *
             * The cmpxchg does the following atomically (without lock
             * semantics!)
             * 1. Relocate first pointer to the current per cpu area.
             * 2. Verify that tid and freelist have not been changed
             * 3. If they were not changed replace tid and freelist