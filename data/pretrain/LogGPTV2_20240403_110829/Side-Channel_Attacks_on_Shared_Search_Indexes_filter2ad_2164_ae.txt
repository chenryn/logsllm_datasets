7
8
9
10 end
end
After waiting for the writes to propagate to the index, the
attacker issues a two-term search query for {ri, t}, which
returns a ranked list R of two results. This list is either
d1 > d2 or d2 > d1. If it is the latter, the algorithm infers
that t is on the i-th shard and adds it to B(cid:2).
To see why this attack works, we consider the cases where
df(t, D) is zero or is positive before the attack starts (where
D is the document set on the shard). If it is zero, then d1
and d2 will have the same score and hence d1 will be ranked
higher. If however df(t, D) is positive, then d2 will have a
higher idf since the DF of r is exactly 1 and the DF of t is
at least 2 after we write the ﬁles. Thus, d2 has a higher score
and be ranked ﬁrst.
Optimizations. This attack can be generalized to test for
several terms in each iteration of the main loop instead of 1
(thus reducing the number of sleep operations). This version
requires several documents d1, . . . , dm on the same shard,
and we assume that ties are broken in the order d1 > d2 >
··· > dm. The attack writes r into the last document dm,
and the terms of interest t1, . . . , tm−1 in the ﬁrst m − 1
documents d1, . . . , dm−1. Then it issues a search query for
{t1, . . . , tm, r} and looks at the position of dm in the list. If
a document di appears below dm, then the attack infers that
ti appears on the shard for the same reasons as before.
VI. CASE STUDIES OF MODERN SERVICES
In this section we discuss how an adversary might abuse
the attacks we constructed in the previous section. Then we
explore three services against which the score-based attacks
are effective: GitHub, Orchestrate.io and Xen.do. We report
on the performance of our attacks on the services, such as
how long they took, how much they would cost to mount at
scale, and how often they might fail. Finally, we examine
rank-only attacks against GitHub and Orchestrate.io, who
provide web interfaces for performing multi-term search
without returning relevance scores.
A. Scenarios
To understand possible threats let us abstract the ability
that is implied by our brute force term extraction and DF
prediction attacks. Let the document sets stored on the shards
of the service be D1, . . . , DnSHRDS. Term extraction gives us
abstractly an oracle OTE that takes input (t, i) and returns 0
if df(t, Di) = 0 and otherwise returns 1. DF estimation
provides a richer oracle ODF that takes the same inputs (t, i)
but returns (approximately) df(t, Di) itself.
In this view any abuse will have some fundamental
limitations. Short terms are likely to appear by chance in
documents, so the ﬁrst oracle will likely return 1 most of the
time. Also, since terms are extracted by a tokenizer, if some
text happens to contain periods or hyphens (like a URL, SSN,
phone number), then the text will be separated into small
terms which may have high false positive rates. Neither of
these oracles allows one to test for substrings of terms, so
very high-entropy terms like cryptographic keys are, without
some side information about them, intractable to guess. We
nevertheless identify two types of attack scenarios that are
possible within these limitations.
Medium-entropy terms. The ﬁrst is brute-forcing medium-
entropy terms that are rare enough to avoid false positives
yet drawn from a brute-forcible space. As examples of
sensitive medium-entropy data that may be stored within a
search service, consider SSNs and phone numbers in the
United States. In these cases, and assuming no hyphenation
is used (which is desirable for search), an adversary could in
principle use the ﬁrst oracle OTE to produce a list of all such
numbers and SSNs stored in the shards of the service. This
is already a severe violation of the conﬁdentiality expected
by users.
A second type of medium-entropy data are (relatively
strong) passwords. Note that very weak passwords such as
“123456” are likely to generate false positives. An attacker
may test a dictionary of common passwords (or their hashes)
using OTE to determine which ones occur in the services’
document set of terms. Passwords could be stored in search
services when used as application backends, and there have
also been well-publicized incidents of passwords being stored
on GitHub repositories. In either case, an attacker could use
access to OTE to ﬁlter the password dictionary to a smaller
set that it then uses for online password guessing attacks.
Medium-entropy data targets may also arise when an
adversary has partial knowledge on an a priori piece
of high-entropy data. For instance, someone may store
documents that contain terms with adversary-known high-
entropy preﬁxes followed by lower entropy sufﬁxes. The
preﬁxes will lower or remove false positives, allowing for
brute-forcing of the rest via the oracle OTE.
A ﬁnal type of medium-entropy data may occur when
high-entropy data is tokenized into medium-entropy terms.
Consider a hypothetical 24-character API key that consists of
four 6-character chunks separated by hyphens. These may be
tokenized into 6-character terms that could then be found via
the oracle OTE, along with some false positives. This would
vastly reduce the space of possible API keys for an attacker
683
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:39 UTC from IEEE Xplore.  Restrictions apply. 
who only needs to try keys formed by combinations taken
from the set of 6-character terms found in the index.
Term trending. A second class of attacks uses the richer
ability of the ODF oracle to estimate DFs rather than simply
detect if they are positive. Unlike the previous settings,
an attack may proﬁtably query ODF for even low-entropy
terms to learn about how commonly they are included in
documents. For instance, on GitHub, one can use the side
channel to learn about the popularity of certain libraries or
packages. Or, if separate source code documents include
unique identiﬁers associated to a particular victim (e.g., AWS
account IDs), then the ODF oracle can be used to count the
number of documents in that victim’s private repositories.
Since we are able to extract per-shard DF estimations, an
adversary may be able to guess if it has found the shard of
a particular user by looking for a shard that contains, with
high DFs, terms associated with that user. One can then focus
searches on that shard in order to reduce false positive rates
in, e.g., a brute-force attack.
B. Performing Responsible Experiments
We would like to validate the feasibility of attack scenarios
as discussed above. However, the nature of the side channel
is such that we could, if careless, end up spying on actual
user data in these services (e.g., if we simply started querying
for passwords). We therefore took care to ensure that our
experiments would not expose private information about their
users or otherwise cause undue burdens on services.
Our experiments will only target simulated victims, i.e.,
accounts under our control with documents that we generate.
This will give us ground truth. Except for estimating false
positive rates, we apply the DF side channel only to long,
random unstructured terms that are exponentially unlikely
to appear in any bystander’s document (given the number
of such terms we use the side channel upon). Put another
way, we explicitly avoid learning anything about other users’
data from the side channel. We refer to users other than
our simulated attack and victim users as bystanders (i.e.,
everyone is a bystander except for our accounts).
False positive rates caused by bystander data are important
for understanding the efﬁcacy of possible attacks, as we
expect false positives to be a signiﬁcant limitation to the
attacks in practice. The rates however depend on bystanders’
potentially private data. We therefore perform carefully
limited false positive measurements in which we infer only
whether or not we get the right answer from our side channel
for random terms of given lengths. Even here we minimize
any perceived risk to other users, only searching for random
unstructured data with no semantic value. We only report
summary statistics and never what random values may have
resided in one or more bystander’s documents, and we will
not make these false-positive datasets public.
Attacks could involve making a large number of queries to
the service. We rate-limit our queries appropriately and show
how to extrapolate from our experiments to attackers with
no qualms about submitting as many queries as possible per
unit time.
C. GitHub
GitHub is one of the most popular source code hosting
platforms, with 14 million users and 35 million repositories
as of April 2016 according to Wikipedia. GitHub has
two types of repositories: public repositories and private
repositories. Users can register for a free plan and set up
unlimited numbers of public repositories, but no documents
or repositories can be marked as private. To enable use of
private repositories, one can choose a 7 USD per month
plan. In a private repository, documents can be accessed
and searched by their owner or authorized users. Non-
authorized users should not be able to learn anything about
the repository’s contents, such as the number and type of
documents, the contents of those documents, etc.
GitHub search API and basic experiments. GitHub
uses ES (hosted by Elastic.co) as its search engine for full-
text search [8]. A user could use a web-based interface or
RESTful APIs to search for a term of interest. A search
request will return with all the documents containing this
term in both the public repositories as well as in private
repositories to which the requesting user has access. The
RESTful search API returns relevance scores to facilitate
application development, which our attacks will exploit,
while the web-interface returns ranked results without scores
(we discuss attacking this setting in the Appendix). Based
on public documentation [8, 19], we know that GitHub load
balances across shards at the granularity of an individual
repository: at the time the repository is created it is assigned
to a shard. All documents in that repository are indexed
within the assigned shard.
We ﬁrst performed some manual experimentation using
our score-dipping attack to both conﬁrm the DF side channel
and reverse engineer some undocumented aspects of the
GitHub search service. We found that public repositories
and private repositories use the same indexes. This means
that, looking ahead, a malicious user could use (free) public
repositories and the search API to extract sensitive terms
from a victim’s private repositories. We also observed that
the index update time, i.e., the time between inserting a
document into a repository and it being added to an index,
is less than 1 minute in most cases.
Search queries emanating from a particular user account
are limited to 5,000 per hour. There is a public interface
for search as well, which does not require an account,
and only searches public documents (which sufﬁces for our
attacks should an attacker use public repositories). This is
rate limited to 60 per IP per hour. The GitHub search API
allows queries with size less than or equal to 128 bytes.
In our experiments, we primarily used private repositories
for our simulated attacker, and found that pausing at least
684
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:18:39 UTC from IEEE Xplore.  Restrictions apply. 
two seconds between two consecutive API requests avoids
triggering rate limits. Therefore, in all our experiments, we
pause for 2 seconds after search query and 60 seconds
after creating/updating a document. Using longer pause times
might be better for handling outliers (i.e., the index update
time can be up to 2 minutes in rare cases), but would
signiﬁcantly increase the experiment running time.
Shard mapping. As mentioned GitHub hosts millions of
repositories across many users, and therefore uses a large
number of ES shards. We apply our shard mapping tool to
determine how many, and to place an attacker document on
each of the discovered shards.
We ran the shard mapping algorithm variant as described in
§V-E, creating two new repositories each with one document
over 513 rounds for a total of 1,026 repositories. The δ
in COSHARDTEST was set to 0.05. We stopped after 50
consecutive rounds (100 repositories) failed to ﬁnd a new
shard. It took 104 hours and we discovered 191 shards.
We might have missed some small number of shards. For
example, assuming random assignment of repositories to
shards, the probability of 100 consecutive failings if there
were in fact 200 shards is 1%. Nevertheless, our shard map
ends up sufﬁcient for all experiments — all subsequent
simulated victims ended up on one of the 191 shards that we
discovered. We note an Elastic.co use-case description states
that GitHub has 128 shards [19], suggesting this information
is out of date.
After creating one repository on each shard at GitHub, we
can generate many shard map sets M1, M2, . . . simply by
creating one document on each repository.
We note that using shard mapping it would seem possible
to track, over time, the number of shards used by GitHub.
This could already be a hypothetical conﬁdentiality issue
for services that want to keep their infrastructure conﬁgura-
tion secret.
Note that the consistency issues mentioned in §IV might
produce false positives in COSHARDTEST; i.e., the differ-
ence in scores for two documents is greater than a threshold
even though the documents are not on the same shard. To
handle this issue, we double check after COSHARDTEST
returns a positive result: we run COSHARDTEST again and
accept the result if both rounds of tests give positive results.
We adopt this false positive identiﬁcation method in the
COSHARDTEST on all examined services.
DF prediction. As mentioned above, the documents in the
same repository are assigned to the same shard. Doing more
manual tests, we conﬁrmed this, and leverage it in the design
of our experiments. We use one account AcctA as the account
for a simulated attacker and another account AcctV for a
simulated victim.
We tested the accuracy of DF prediction as follows. For
a given value of nDFE, we create nDFE training documents
in a repository under the attacker’s account AcctA and run
9.04
8.84
)
d
,
t
(
e
r
o
c
s
9
8
7
6
0
200
400
600
800 1,000
DF(t)
Figure 3: The changes of score(t, d) as df(t, D) increases.
The scores when df(t, D) = 1 and df(t, D) = 2 are
highlighted. Y-axis does not start from zero.
DFPRED. During the training, we use OriginLab [37] to
test the data against all the functions provided, and ﬁnd
without exception the best-ﬁt function is in the form of
f (x) = a− b∗ ln(x + c), where x is the variable representing
the unknown DF and a, b, c are coefﬁcients. This function is
consistent with the standard Elasticsearch scoring function
in [28].
Using AcctV we generate nDF victim documents in a single
repository, each document containing the single term {t∗}
which is chosen as a random 16-byte alphabetic string. We
vary nDF and test the accuracy of the attack. We run the
COSHARDTEST attack to place a document d = {t∗} from
AcctA on the same shard with the documents of AcctV. We
then measure and record the score score(t∗, d) by making
a search query from AcctA. Then, we calculate dfest(t∗
) =
f−1(score(t∗, d)) − 1 as an approximation of DF(t∗
) and
measure the relative error rate (in percentage) and absolute