    /dev/nvme0n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme0n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    ```
2.  列出所有连接的 NVMe 子系统：
    ``` screen
    # nvme list-subsys
    nvme-subsys0 - NQN=testnqn
    \
     +- nvme0 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme1 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme2 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
     +- nvme3 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
    ```
    检查活动传输类型。例如，`nvme0 fc`{.literal}
    表示设备通过光纤通道传输连接，`nvme tcp`{.literal} 则表示设备通过
    TCP 连接。
3.  如果您编辑了内核选项，请检查内核命令行中是否启用了原生 NVMe 多路径：
    ``` screen
    # cat /proc/cmdline
    BOOT_IMAGE=[...] nvme_core.multipath=Y
    ```
4.  检查 DM 多路径报告了 NVMe 命名空间为，例如：`nvme0c0c0n1`{.literal}
    到 `nvme0c3n1`{.literal}，而[*不是*]{.emphasis}，例如：
    `nvme0n1`{.literal} 到 `nvme3n1`{.literal} ：
    ``` screen
    # multipath -e -ll | grep -i nvme
    uuid.8ef20f70-f7d3-4f67-8d84-1bb16b2bfe03 [nvme]:nvme0n1 NVMe,Linux,4.18.0-2
    | `- 0:0:1    nvme0c0n1 0:0     n/a   optimized live
    | `- 0:1:1    nvme0c1n1 0:0     n/a   optimized live
    | `- 0:2:1    nvme0c2n1 0:0     n/a   optimized live
      `- 0:3:1    nvme0c3n1 0:0     n/a   optimized live
    uuid.44c782b4-4e72-4d9e-bc39-c7be0a409f22 [nvme]:nvme0n2 NVMe,Linux,4.18.0-2
    | `- 0:0:1    nvme0c0n1 0:0     n/a   optimized live
    | `- 0:1:1    nvme0c1n1 0:0     n/a   optimized live
    | `- 0:2:1    nvme0c2n1 0:0     n/a   optimized live
      `- 0:3:1    nvme0c3n1 0:0     n/a   optimized live
    ```
5.  如果您更改了 I/O 策略，请检查 `round-robin`{.literal} 是 NVMe
    设备中的活跃 I/O 策略：
    ``` screen
    # cat /sys/class/nvme-subsystem/nvme-subsys0/iopolicy
    round-robin
    ```
:::
::: itemizedlist
**其他资源**
-   [配置内核命令行参数](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_monitoring_and_updating_the_kernel/configuring-kernel-command-line-parameters_managing-monitoring-and-updating-the-kernel){.link}
:::
:::
::: section
::: titlepage
# []{#enabling-multipathing-on-nvme-devices_configuring-device-mapper-multipath.html#proc_enabling-dm-multipath-on-nvme-devices_enabling-multipathing-on-nvme-devices}在 NVMe 设备中启用 DM 多路径 {.title}
:::
这个过程使用 DM 多路径解决方案在连接的 NVMe 设备中启用多路径。
::: itemizedlist
**先决条件**
-   NVMe 设备连接到您的系统。
    有关通过光纤传输连接 NVMe 的详情请参考 [NVMe over fabric
    设备概述](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_storage_devices/nvme-over-fabrics-using-rdma_managing-storage-devices#overview-of-nvme-over-fabric-devices_nvme-over-fabrics-using-rdma){.link}。
:::
::: orderedlist
**步骤**
1.  检查是否禁用了原生 NVMe 多路径：
    ``` screen
    # cat /sys/module/nvme_core/parameters/multipath
    ```
    这个命令显示以下之一：
    ::: variablelist
    [`N`{.literal}]{.term}
    :   禁用原生 NVMe 多路径。
    [`Y`{.literal}]{.term}
    :   启用原生 NVMe 多路径。
    :::
2.  如果启用了原生 NVMe 多路径，请禁用它：
    ::: orderedlist
    1.  在内核命令行中删除 `nvme_core.multipath=Y`{.literal} 选项：
        ``` screen
        # grubby --update-kernel=ALL --remove-args="nvme_core.multipath=Y"
        ```
    2.  在 64 位 IBM Z 构架中更新引导菜单：
        ``` screen
        # zipl
        ```
    3.  如果存在，从 `/etc/modprobe.d/nvme_core.conf`{.literal}
        文件中删除 `options nvme_core multipath=Y`{.literal} 行。
    4.  重启系统。
    :::
3.  确保启用了 DM 多路径：
    ``` screen
    # systemctl enable --now multipathd.service
    ```
4.  在所有可用路径中分发 I/O。在 `/etc/multipath.conf`{.literal}
    文件中添加以下内容：
    ``` screen
    device {
      vendor "NVME"
      product ".*"
      path_grouping_policy    group_by_prio
    }
    ```
    ::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
    ### 注意 {.title}
    当 DM 多路径管理 NVMe
    设备时，`/sys/class/nvme-subsys0/iopolicy`{.literal}
    配置文件不会影响 I/O 分发。
    :::
5.  重新载入 `multipathd`{.literal} 服务以应用配置更改：
    ``` screen
    # multipath -r
    ```
6.  备份 `initramfs`{.literal} 文件系统：
    ``` screen
    # cp /boot/initramfs-$(uname -r).img \
         /boot/initramfs-$(uname -r).bak.$(date +%m-%d-%H%M%S).img
    ```
7.  重建 `initramfs`{.literal} 文件系统：
    ``` screen
    # dracut --force --verbose
    ```
:::
::: orderedlist
**验证**
1.  检查您的系统是否识别 NVMe 设备：
    ``` screen
    # nvme list
    Node             SN                   Model                                    Namespace Usage                      Format           FW Rev
    ---------------- -------------------- ---------------------------------------- --------- -------------------------- ---------------- --------
    /dev/nvme0n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme0n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme1n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme1n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme2n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme2n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme3n1     a34c4f3a0d6f5cec     Linux                                    1         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    /dev/nvme3n2     a34c4f3a0d6f5cec     Linux                                    2         250.06  GB / 250.06  GB    512   B +  0 B   4.18.0-2
    ```
2.  列出所有连接的 NVMe 子系统。检查命令报告为 `nvme0n1`{.literal} 到
    `nvme3n2`{.literal}，而[*不是*]{.emphasis}，例如：
    `nvme0c0c0n1`{.literal} 到 `nvme0c3n1`{.literal} ：
    ``` screen
    # nvme list-subsys
    nvme-subsys0 - NQN=testnqn
    \
     +- nvme0 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme1 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1dd:pn-0x10000090fac7e1dd live
     +- nvme2 fc traddr=nn-0x20000090fadd5979:pn-0x10000090fadd5979 host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
     +- nvme3 fc traddr=nn-0x20000090fadd597a:pn-0x10000090fadd597a host_traddr=nn-0x20000090fac7e1de:pn-0x10000090fac7e1de live
    ```
    ``` screen
    # multipath -ll
    mpathae (uuid.8ef20f70-f7d3-4f67-8d84-1bb16b2bfe03) dm-36 NVME,Linux
    size=233G features='1 queue_if_no_path' hwhandler='0' wp=rw
    `-+- policy='service-time 0' prio=50 status=active
      |- 0:1:1:1  nvme0n1 259:0   active ready running
      |- 1:2:1:1  nvme1n1 259:2   active ready running
      |- 2:3:1:1  nvme2n1 259:4   active ready running
      `- 3:4:1:1  nvme3n1 259:6   active ready running
    mpathaf (uuid.44c782b4-4e72-4d9e-bc39-c7be0a409f22) dm-39 NVME,Linux
    size=233G features='1 queue_if_no_path' hwhandler='0' wp=rw
    `-+- policy='service-time 0' prio=50 status=active
      |- 0:1:2:2  nvme0n2 259:1   active ready running
      |- 1:2:2:2  nvme1n2 259:3   active ready running
      |- 2:3:2:2  nvme2n2 259:5   active ready running
      `- 3:4:2:2  nvme3n2 259:7   active ready running
    ```
:::
::: itemizedlist
**其他资源**
-   [配置内核命令行参数](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/managing_monitoring_and_updating_the_kernel/configuring-kernel-command-line-parameters_managing-monitoring-and-updating-the-kernel){.link}
-   [配置 DM
    多路径](https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/9/html/configuring_device_mapper_multipath/configuring-dm-multipath_configuring-device-mapper-multipath){.link}.
:::
:::
:::
[]{#modifying-the-dm-multipath-configuration-file_configuring-device-mapper-multipath.html}
::: chapter
::: titlepage
# []{#modifying-the-dm-multipath-configuration-file_configuring-device-mapper-multipath.html#modifying-the-dm-multipath-configuration-file_configuring-device-mapper-multipath}第 5 章 修改 DM-Multipath 配置文件 {.title}
:::
默认情况下，DM 多路径会为多数常见的多路径用例提供配置值。另外，DM
多路径包括对自己支持 DM 多路径的最常见存储阵列的支持。您可以通过编辑
`/etc/multipath.conf`{.literal} 配置文件来覆盖 DM
多路径的默认配置值。如果需要，您还可以在配置文件中添加不支持的默认存储阵列。
有关默认配置值（包括支持的设备）的详情，请运行以下命令：
``` screen
# multipathd show config
# multipath -t
```
::: {.note style="margin-left: 0.5in; margin-right: 0.5in;"}
### 注意 {.title}
如果您从 `initramfs`{.literal}
文件系统运行多路径并对多路径配置文件进行任何更改，则必须重建
`initramfs`{.literal} 文件系统以使更改生效
:::
在多路径配置文件中，您只需要指定配置所需的部分，或者从默认值中更改的小节。如果文件中没有与您的环境相关的部分，或者不需要覆盖默认值，您可以将其注释掉，因为它们位于初始文件中。
配置文件允许正则表达式描述语法。
::: section
::: titlepage
# []{#modifying-the-dm-multipath-configuration-file_configuring-device-mapper-multipath.html#configuration-file-overview_modifying-the-dm-multipath-configuration-file}配置文件概述 {.title}
:::
多路径配置文件可分为以下几个部分：
::: variablelist
[黑名单]{.term}
:   不视为多路径的特定设备列表。
[blacklist_exceptions]{.term}
:   根据 `blacklist`{.literal}
    部分的参数，列出其他将被忽略的多路径设备。
[defaults]{.term}
:   DM 多路径的常规默认设置。
[multipaths]{.term}
:   各个多路径设备特性的设置。这些值覆盖了在配置文件的、`设备`{.literal}
    和 `defaults`{.literal} 部分中指定的内容。``{.literal}
[devices]{.term}
:   各个存储控制器的设置。这些值覆盖了在 配置文件的 `defaults`{.literal}
    部分中指定的内容。如果您使用默认不支持的存储阵列，您可能需要为阵列创建
    `devices`{.literal} 子部分。
[overrides]{.term}
:   适用于所有设备的设置。这些值覆盖了在配置文件的 `devices`{.literal}
    和 `defaults`{.literal} 部分中指定的值。
:::
当系统决定多路径设备的属性时，它会按照以下顺序检查
`multipath.conf`{.literal} 文件中的单独部分的设置：
::: orderedlist
1.  `multipaths`{.literal} 部分
2.  `overrides`{.literal} 部分
3.  `devices`{.literal} 部分
4.  `defaults`{.literal} 部分
:::
:::
::: section
::: titlepage
# []{#modifying-the-dm-multipath-configuration-file_configuring-device-mapper-multipath.html#dm-multipath-overrides-of-the-device-timeout_modifying-the-dm-multipath-configuration-file}DM 多路径覆盖设备超时 {.title}
:::
`restore_tmo`{.literal} `sysfs`{.literal} 选项控制一个特定 iSCSI
设备的超时时间。以下选项全局覆盖 `recovery_tmo`{.literal} 值：
::: itemizedlist
-   `replacement_timeout`{.literal} 配置选项会全局覆盖所有 iSCSI 设备的
    `recovery_tmo`{.literal} 值。
-   对于由 DM 多路径管理的所有 iSCSI 设备，DM 多路径中的
    `fast_io_fail_tmo`{.literal} 选项会全局覆盖 `recovery_tmo`{.literal}
    值。
    DM 多路径中的 `fast_io_fail_tmo`{.literal} 选项会覆盖光纤通道设备的
    `fast_io_fail_tmo`{.literal} 选项。
:::
DM 多路径 `fast_io_fail_tmo`{.literal} 选项优先于
`replacement_timeout`{.literal}。红帽不推荐使用
`replacement_timeout`{.literal} 在由 DM 多路径管理
`的设备中覆盖 restore_tmo`{.literal}，因为当多路径服务重新加载时，DM
多路径总是重置 restore `_tmo`{.literal}。``{.literal}
:::
::: section
::: titlepage
# []{#modifying-the-dm-multipath-configuration-file_configuring-device-mapper-multipath.html#modifying-multipath-configuration-file-defaults_modifying-the-dm-multipath-configuration-file}修改多路径配置文件默认设置 {.title}
:::