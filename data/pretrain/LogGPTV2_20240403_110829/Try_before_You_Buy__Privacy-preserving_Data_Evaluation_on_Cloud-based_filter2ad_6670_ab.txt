Data Encryption. Sellers utilize IFE to preserve both data privacy
and data functionality in the cloud. Particularly, the functionality
means that the cloud can perform training and prediction opera-
tions with the shopper’s encrypted model and sellers’ encrypted
data. In training and prediction operations, there are two types of
computation related to sellers’ data: matrix and convolution com-
putation. As the two types of computation can be transformed into
inner product operations, it is natural for sellers to transform their
data into vectors and encrypt them using IFE.
Model Encryption. There are mainly two types of computation
in the first hidden layer of a seller’s model, i.e., matrix or convolu-
tion computation. Note that the two types of computation can be
converted to inner product operations. Therefore, the shopper can
transform the parameters of the first hidden layer to vectors and
utilize IFE to encrypt them. As a result, the privacy and function-
ality of the first hidden layer can be preserved. Nevertheless, IFE
cannot be applied to encrypt an entire multi-layer model since it
only supports simple inner product operations. Therefore, the shop-
per also utilizes matrix transformation to encrypt the parameters
of the remaining layers.
Data Selection. For an ML model, valuable data contain much
informativeness and can significantly improve model performance.
Particularly, data informativeness can be revealed by the prediction
Model Encryption….Data SellersData ShopperPublic KeysCloudEncrypted Prediction valuesSetupData SelectionData ValidationMachine Learning EncryptionData Encryption262ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Qiyang Song1, Jiahao Cao2,3, Kun Sun1, Qi Li2,3, and Ke Xu2,3
values of data. Therefore, it seems possible for the shopper to per-
form some prediction operations and then collect prediction values
to estimate data informativeness. However, the shopper cannot ac-
cess sellers’ data before the final payment. Therefore, it relies on the
cloud to perform prediction operations with its encrypted model
and sellers’ encrypted data. Then, it decrypts prediction values and
adopts active learning to select informative data.
Data Validation. Although informative data may significantly
improve model performance, they could contain irrelevant data or
falsely labeled data, which may mislead the classification of models.
Therefore, the shopper needs to examine the quality of the selected
data. However, the shopper cannot directly access sellers’ data to
estimate data quality before the final payment. Fortunately, for a
specific ML model, the prediction values of some data can reveal
the quality of previously training data. Therefore, the shopper can
inform the cloud to retrain its model with the selected data and then
use the retrained model to output some encrypted prediction values.
Next, the shopper retrieves these prediction values and decrypts
them to estimate data quality.
4 EFFICIENT MACHINE LEARNING
ENCRYPTION PROTOCOL
In this section, we first utilize inner product functional encryption
(IFE) and matrix transformation to propose an efficient ML encryp-
tion protocol, providing encryption approaches for input data and
the parameters of neural networks. Then, we illustrate how to en-
crypt a typical CNN model and input data using our ML encryption
protocol. Although we only display the encryption approach for a
CNN model, our ML encryption protocol can also be generalized to
encrypt other neural network models.
4.1 Setup
Our ML encryption protocol provides a setup approach for a data
shopper. It runs IFE’s setup algorithm to initialize a master key pair
(mpk, msk) and chooses a set of random numbers R from a finite field
Zq. After initialization, the shopper sends mpk to data sellers for
data encryption, and stores msk and R to encrypt model parameters.
Particularly, msk is used to encrypt the parameters of the first
hidden layer with IFE, and R is used to transform parameters. To
be precise, (cid:174)Ri ∈ R is an ni-dimension vector of random numbers
used to transform the parameters of the i-th fully-connected hidden
layer, and Ri ∈ R is an mi × ni matrix of random numbers used to
transform the parameters of the i-th convolution hidden layer.
Algorithm 1: Data Encryption for Matrix Computation
Input: data (cid:174)x, a master public key mpk;
Output: encrypted data (cid:174)Cx ;
1 (cid:174)Cx →IFE.encrypt((cid:174)x, mpk);
2 return (cid:174)Cx ;
4.2 Data Encryption
Our ML encryption protocol allows data sellers to protect data and
also retain data functionality in the cloud. Particularly, the func-
tionality means that data can be used for training and prediction. In
training and prediction, there are two types of computation related
to input data, i.e., matrix and convolution computation. Therefore,
we provide two data encryption approaches for matrix and convo-
lution computation, respectively.
Algorithm 2: Data Encryption for Conv. Computation
Input: data X (a two-dimension matrix), a convolution
kernel width Kw , a stride length s, a master public
key mpk;
Output: encrypted data CX;
1 t ← (X.width − Kw + 1)/Lstr ;
2 initiate a (t × t × (Kw)2) matrix CtX ;
3 for i ∈ {0,· · · , t} do
for j ∈ {0,· · · , t} do
4
5
6
7
xs ← i ∗ s, ys ← j ∗ s;
convert X[xs : xs + Kw][ys : ys + Kw] to (cid:174)X ′;
CX[i][j][:] ← IFE.encrypt( (cid:174)X ′, mpk);
11, (cid:174)X ′
22, · · · }. Then, we utilize IFE to encrypt { (cid:174)X ′
Algorithm 1 shows the data encryption approach for matrix
computation. As Figure 2 shows, the computation between input
data (cid:174)x and a parameter matrix can be naturally divided to inner
product operations between (cid:174)x and multiple vectors. Thus, we can
directly use IFE to encrypt (cid:174)x, which preserves both data privacy
and its functionality of matrix computation. Algorithm 2 shows the
data encryption approach for convolution computation. As Figure 4
shows, the convolution computation between two-dimension input
data X and a kernel can be decomposed to inner product operations
between a subarea of X and a (transformed) kernel. Therefore, we
first split X to multiple subareas and transform them to a set of
vectors { (cid:174)X ′
22, · · · }.
As a result, both data privacy and the functionality of convolution
computation can be preserved.
4.3 Model Encryption
Our ML encryption protocol provides model encryption approaches
that allow a data shopper to preserve the privacy and functionality
of its model in the cloud. Note that neural network models mainly
consist of fully-connected and convolution layers, and there are
two different types of computation, i.e., matrix and convolution
computation, in a fully-connected and a convolution layer. As the
two types of computation can be decomposed into inner product op-
erations, and IFE enables inner product operations over ciphertexts,
we can utilize IFE to encrypt the parameters of a hidden layer.
11, (cid:174)X ′
Although applying IFE can protect the parameters of a hidden
layer, it is not sufficiently secure since it reveals computation results
in plaintexts. Additionally, since it only supports simple inner prod-
uct operations, it can only be applied to encrypt the first hidden
layers. Therefore, we also apply matrix transformation to encrypt
entire model parameters. To support matrix transformation across
layers, we set the activation function to the squared function. Prior
work [10, 26] has proved squared activation functions are as ex-
pressive as common activation functions, e.g. Relu and Sigmoid.
Encrypting a Fully-connected Layer. Algorithm 3 shows the en-
cryption approach for a fully-connected layer. If this fully-connected
263Try before You Buy
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
choose a vector of random numbers (cid:174)R1 = {r1, r2, · · · } and transform
each row vector W1[j][:] to a vector (cid:174)R1[i]W1[j][:]. Then, we use
IFE to encrypt the transformed vectors. By combining IFE with
matrix transformation, the result (cid:174)Z of matrix computation can be
fully protected. Particularly, the result revealed by IFE is a random
vector (cid:174)Z1 = (cid:174)R1 × (cid:174)Y1. Therefore, attackers cannot learn the original
computation result (cid:174)Y1 without knowing (cid:174)R1. By executing the square
activation function with the result (cid:174)Z1, this layer will output random
activation values (cid:174)Z
2 to the next layer.
2 = (cid:174)R1
2 × (cid:174)Y1
Figure 2: IFE-based Data Encryption, Model Encryption, and
Matrix Computation in a Fully-connected Layer (First Hid-
den Layer).
(cid:174)CW i[j] ←IFE.KeyGen( (cid:174)R1[j] ∗ W[j][:], msk);
Output: encrypted parameters CWi;
1 ∅ → CWi or (cid:174)CW i;
2 if i == 1 then
3
4
for j ∈ {0,· · · , Wi .row_num} do
return (cid:174)CWi;
for j ∈ {0,· · · , Wi .row_num} do
5
6 else
7
8
9
Algorithm 3: Encrypting a Fully-connected Layer (the i-th
Hidden Layer)
Input: a parameter matrix Wi, a master secret key msk, a
vector of random numbers (cid:174)Ri chosen for the i-th
layer, and a vector of random numbers (cid:174)Ri−1 chosen
for the (i − 1)-th layer (if i > 0 ) ;
for k ∈ {0,· · · , Wi .col_num} do
CWi[j][k] ← Wi[j][k] ∗ (cid:174)Ri[j]/((cid:174)Ri−1[k])2;
10
return CWi;
layer is the first hidden layer, it is natural to convert a parameter
matrix W1 to row vectors and apply IFE to encrypt them. How-
ever, only applying IFE to encrypt parameters is not sufficiently
secure since IFE can reveal computation results in plaintexts. The
original parameters and data may be derived from the plaintexts
through matrix decomposition. Therefore, we also apply matrix
transformation to convert W1.
As Figure 2 shows, we first decompose W1 to row vectors and
apply matrix transformation to convert them. To be precise, we
Figure 3: Model Encryption and Matrix Computation in a
Fully-connected Layer (i-th Hidden Layer).
2
2
1 = (cid:174)R
2
i−1 × (cid:174)Y
i−1 = (cid:174)R
If a fully-connected layer is the i-th hidden layer (i > 1), we
choose a vector of random numbers and utilize matrix transforma-
tion to protect the parameters of this layer. As Figure 3 shows, this
layer receives the output (activation values) from the previous layer
as input data. Particularly, the input data from the previous layer
can be presented as (cid:174)Z
2
i−1. For instance, the input
from the first fully-connected hidden layer are (cid:174)Z
1 × (cid:174)Y
2
2
1 . As
the original input (cid:174)Y
2
i−1 is encrypted by a vector of random numbers
i−1, we cannot perform correct matrix computation between (cid:174)Y
(cid:174)R
2
2
i−1
and the parameters of this layer. To guarantee correct matrix com-
putation, we also need to eliminate the effect of random numbers
(cid:174)R
2
i−1 in matrix transformation.
As Figure 3 shows, we choose a vector of random numbers (cid:174)Ri
and transforms the parameters Wi to W′
= Ci × Wi, where Ci is a
matrix whose element is (cid:174)Ri[k]/(cid:174)R
i−1[v]. As a result, each element
2
Wi[k][v] of the parameters is randomized by a different random
number (cid:174)Ri[k]/(cid:174)R
i−1[v]. Therefore, the privacy of parameters are
2
fully preserved. Additionally, we multiply each row vector Wi[k][:]
of parameters with a vector 1/(cid:174)R
2
i−1. This eliminates the effect of
random numbers (cid:174)R
2
i−1 from the input data in matrix computation,
which guarantees the correct execution of matrix computation. In
i
w21w22w23w11w13w12x3x2x1Parameter Matrix W1x3x2x1c3c2c1sk2Input data xsk1xr1w13r1w12r1w11Cz12Z22IFE-basd Data EncIFE-basd Model Encski = IFE.KeyGen(riW1[i][:] ,msk)r2w23r2w22r2w21C= IFE.KeyGen(x, mpk)r1y1r2y2zi = IFE.Decrypt(C, ski)IFE can reveal the result of mat computation over ciphertextsoutput encrypted activation valuesSend to next layerSquare ActivationIFE-based Mat ComputationZ1 =  (R1 ✕ Y1)Z12 = (R1 ✕ Y1)2Y1[i]= Mat(W1, x)[i]  CWVectorize & Mat TransformationUsing R1R1[2] *W1[2][:]R1[1]* W1[1][:]z12Z22w22w11w12w21w’22w’11w’12w’21z’1z’2CWi = Ci  ✕ Wiprevious (i-1)-th hidden LayerMat ComputationSquare activationSend to next Layerz12Z22WiInput dataCWi Z(i-1)2Direct matrix computation over ciphertexts Output encrypted activation valuesZi =  Ri ✕ YiYi =Mat(Wi , Y(i-1)2)Zi 2=  Ri2 ✕ Yi2   , Ci [k][v] = Ri [k]/R(i-1)2 [v]Model Enc (Mat TransformationUsing Ri and R(i-1))Z(i-1)2 =  R(i-1)2 ✕ Y(i-1)2264ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Qiyang Song1, Jiahao Cao2,3, Kun Sun1, Qi Li2,3, and Ke Xu2,3
2
2
i
2
i
= (cid:174)R
i × (cid:174)Y
to the next layer.
this way, this layer can perform correct matrix computation be-
tween the transformed parameters and input data, and then output
the result as (cid:174)Zi = (cid:174)Ri × (cid:174)Yi. As (cid:174)Ri is a random vector, the original
result (cid:174)Yi cannot be derived from (cid:174)Zi. By executing the square acti-
vation function with (cid:174)Zi, this layer will output random activation
values (cid:174)Z
Encrypting a Convolution Layer. Algorithm 4 shows the en-
cryption approach for a convolution layer. For simplicity, we only
consider a convolution kernel in this layer. This algorithm can be
generalized to support multiple kernels. If a convolution layer is the
first hidden layer, we convert a kernel into vectors and apply IFE
to encrypt kernel parameters. Particularly, as convolution compu-
tation in this layer can be decomposed to inner product operations,
we can utilize IFE to retain the convolution functionality of this
layer. However, only applying IFE to encrypt the kernel is not secure
since IFE can reveal computation results in plaintexts. Therefore,
we also apply matrix transformation to convert the kernel.
Figure 4: IFE-based Data Encryption, Model Encryption, and
Convolution Computation in a Convolution Layer (First
Hidden Layer). Here, the convolution stride is 1.
As Figure 4 shows, we first convert the parameters of a kernel
to a vector (cid:174)K and apply matrix transformation to convert it. To be
precise, we first choose a matrix of random numbers R1, whose
each element is rij. Then, we transform (cid:174)K to a set of random vectors
{r11 (cid:174)K, r12 (cid:174)K,· · · } and utilize IFE to encrypt these vectors. By com-
bining IFE with matrix computation, the result Z of convolution
computation can be fully protected. As Figure 4 shows, the result
Z1 revealed by IFE is a random matrix Z1 = R1 × Y1. Therefore, the
original result Y1 cannot be derived without R1. By executing the
square activation function with Z1, this layer will output random
activation values Z1
If a convolution layer is the i-th hidden layer (i > 1), we choose
a matrix of random numbers and utilize matrix transformation
2 to the next layer.
2 × Y1
2 = R1
Algorithm 4: Encrypting a Conv. Layer (the i-th Hidden
Layer)
Input: the width of input Iw , a kernel Ki, a stride length s, a
master secret key msk, a matrix of random numbers
Ri−1 chosen for the (i − 1)-th layer (if i > 1), and Ri
chosen for the (i − 1)-th layer;