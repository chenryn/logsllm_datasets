performance of the individual garbling and evaluation operations
for individual gates [13, 40]. We improve upon these prior works
by considering multiple gates of the same type at once. A natural
question is, whether a library like OpenSSL can be used for
implementing AES operations. This is an appropriate solution if
only large batches of AES calls occur and these are well-supported
by OpenSSL. However, this would not allow the use of VAES which
is currently not used by OpenSSL, and it would bring significant
overhead for smaller batches due to the memory abstraction needed.
Parallelization. Previous work to parallelize the evaluation
of garbled circuits has seen coarse- and fine-grained ap-
proaches [11, 20, 50, 55]. Coarse-grained approaches [11, 20]
are typically used to have multiple threads compute different
parts of the same garbled circuit and are largely orthogonal to our
in-thread optimizations of the computation strategy. Alternatively,
they may have traded communication, e.g., not using free-XOR,
for added parallelism to exploiting using dedicated hardware like
graphics processing units or Intel Quick Assist Technology [55].
The more fine-grained approaches [11, 20] have primarily focused
on using a layering technique, as we also discuss, however, intend-
ing to outsource the work to different threads instead of exploit the
high instruction-level parallelism that modern processors provide.
Additionally, previous work has suggested splitting the garbling
and the evaluating roles with a suitable sub-division of circuits [20]
or overlapping the computation with the garbling and evaluation
operations [50], both of which are orthogonal to what we do.
Memory Behavior. A smaller line of previous research has
explored the limitation of memory use for GC [48, 52, 68, 87, 96].
Their motivation for this was two-fold in allowing the computation
of large circuits not fitting into most memory configurations and
improving locality for caches through smaller code and data. We
note that the techniques to only partially load circuits into memory
are orthogonal to ours, requiring at most invoking early execution
occasionally. We also consider cache locality important. However,
our focus is more on the actual computation and the first-level
cache as opposed to keeping the data in a cache at all.
Hardware-Acceleration. There has been a line of research using
field-programmable gate-arrays (FPGAs) to accelerate garbled cir-
cuit operations [53, 54, 58, 59, 88]. Our work is independent of and
alternative to the main contributions of these prior works. However,
the scheduling discussed for FASE [53] is similar for hardware to
what we do for identifying batches, though their techniques are
focused on the specific dedicated hardware architecture they build,
making it unsuitable for our software-oriented approach.
134VASA: Vector AES Instructions for Security Applications
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
4 OUR FRAMEWORK
The first step in our manually implemented techniques to apply
VAES is the identification of batches of independent AES calls for
small-scale batch processing (§ 4.1). The second step is to process
the AES operations (§ 4.2). Finally, we show how we used these
techniques with the ABY [25], EMP-OT [90], CrypTFlow2 [82], and
EMP-AGMPC [90, 92] frameworks (§ 4.3).
4.1 Batch Identification
For identifying batches, we use two approaches: dynamic batching
and static batching. Dynamic batching primarily uses runtime infor-
mation for minimally invasive batching. Static batching provides
reusable batching information from preprocessing but requires
more substantial changes to the code.
4.1.1 Dynamic Batching. The core idea behind dynamic batching
is to defer execution of operations until they are actually needed
and to compute all pending operations when one is needed. In
processing circuits, the application of this works by modifying the
main processing loop iterating over all gates and adding AES-based
AND gates to a queue and processing all queued AND gates as a
batch once any one of them is referenced as an input dependency.
An example of when the processing is invoked is provided in Fig. 1.
Implementing this technique requires potentially a few hours of
manual effort to identify the core processing loop, to implement
the deferred execution identification, and to identify relevant
modifications and extensions which we briefly discuss next.
Correctness Extensions. The basic technique works well if there
is one type of non-free gates requiring AES operations. However,
some schemes have AND and XOR operations requiring AES
operations using a shared gate index counter to uniquely produce
values per-gate. For these, new design space choices manifest,
in particular, whether it is possible and desirable to separate the
domains of the counters or to track the gate identifiers as well
and not just the minimal information for computing the gate.
Additionally, one can imagine that it is possible to not maintain
separate queues for the different gate types but rather join them
into a shared one which complicates the gate processing at the
potential of gained performance through more AES calls being
potentially batched together to reach minimum optimal batch
size even in complex circuits. Furthermore, we note that dynamic
batching can be combined with the approach of having a variable
number of cryptographic gates associated with an administrative
gate, in which case it is beneficial to track the number of actual
gate tasks associated with each administrative gate and keep a
global count to allow the batch processing algorithm to choose
appropriate sub-batches. Both of these extensions each require a
few hours of effort for the architectural changes.
Optimizations. The basic batching techniques have further op-
timizations. First, the use of this batching can inadvertently lead
to significant gaps in time between visiting and enqueuing a gate
and processing it, meaning it might be pushed out of registers or
lower-level caches. To avoid such unloads, one should consider
to regularly empty the queue by processing the stored tasks even
if more tasks could still be added without violating correctness.
This holds especially true if any given processed sub-batch only
processes a small number of gates, e.g., b = 4, and the queue has
reached a size that is a multiple of b. Additionally, one can con-
sider to only partially process the stored tasks in the queue using
a multiple of the preferred processing width to potentially allow
more gates to be directly enqueued without triggering processing
at an undesirable length. When the basic technique encounters an
AND gate referencing a queued AND gate, it will always trigger
the computation of all queued AND gates. Another optimization in
this scenario is to check whether the referenced AND gate is early
enough in the queue which is guaranteed to have been processed
once the processing reaches the current AND gate and then en-
queueing the current AND gate without triggering processing. The
implementation effort for these optimizations potentially requires
a few hours of effort on top of the basic queue implementation.
Static Batching. A different approach than the dynamic tech-
4.1.2
nique is to preprocess the circuit to gain more holistic information
on batching opportunities. These techniques can be paired with
dynamic batching techniques for further improved efficiency. The
three techniques we discuss are layering (identifying layers of
dependencies), SIMD (grouping multiple guaranteed independent
gates into one administrative one), and a more generic smart
arrangement.
Layering. Layering techniques assign a gate to how many non-free
gates lie between it and the original input. Non-free gates on
equal layers are then necessarily independent and each layer can
be seen as a batch of AES calls to be computed. An example of
associated layers is provided in the right graph of Fig. 1. Layering
can be done in addition to dynamic batching which can potentially
identify independent tasks across layers, e.g., if the first gate of
the second layer references the first gate of the first layer and
early evaluation or peephole optimizations allow such batches.
The effort to add layering support to an implementation varies
significantly with the architecture and can range from a few hours
for adding, computing and using the attribute to significantly more
if a more complex processing strategy than a sequential loop is used.
SIMD. Single-instruction multiple-data (SIMD) gates are explicitly
specified administrative gates that represent the same gate being
applied to multiple input wires in parallel. They present natural
opportunities for batches and even allow batching techniques in
more complex gate scheduling scenarios where other techniques
are not applicable. The cost to this is either the identification of
such SIMD tasks or the need for the execution of a circuit several
time as a batch as well as the need for explicit program-level
representation. Similarly to layering, the implementation cost for
SIMD gates varies with the architecture and can quickly take a
dozen or more hours. As all gate processing methods need to be
SIMD-aware, gates must be extracted and collected and SIMD gates
must be specified or detected in a given circuit description.
Smart Arrangement. This technique is more general and provides
heuristics for circuit generators and manually optimized building
blocks of gates. For example, circuit generators should output cir-
cuits that allow circuit-internal SIMD gate operations and prefer
135ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Jean-Pierre Münch, Thomas Schneider, and Hossein Yalame
Figure 1: A simple 1-bit adder with different manually chosen gate orderings as an illustrative example for the freedom of
topological ordering. Solid black arrows denote data dependencies, red dashed arrows denote one possible sub-optimal order-
ing in the left graph and green dotted arrows show a preferable ordering in the right graph. The “Eval” marks denote places
where dynamic batching with free XORs would trigger processing of the queued fresh AND gates. All unfilled nodes are on
the first layer in the right figure and all light-blue-filled nodes are on the second layer. A layer is defined to be the set of all
nodes with the same amount of non-free (AND) gates between them and the input on the critical path.
larger layers over smaller layers. An example of such improved gate
arrangement is provided from the left to the right graph in Fig. 1.
Additionally, locality has to be considered when generating circuits,
i.e., usage of wires must stay close to where they are generated as
not to push the wire values out of caches, while maintaining enough
distance to allow batching on current and more instruction-level
parallel future architectures.
4.2 Batch Computation
After one has identified a batch of independent AES calls, they
need to be computed. For this, we have used two techniques:
register-oriented computation, which focuses on performance and
simplicity to the compiler, and memory-oriented computation,
which focuses on modularity.
4.2.1 Register-Oriented Computation. Our primary technique for
processing batches describes the task computations as low-level
as possible without resorting to assembly. By using vector regis-
ter types and constant-sized loops we give the compiler as many
opportunities for optimization as possible while still allowing the
conciseness of high-level code. Concretely, we have identified five
steps executed continuously in a loop for all tasks.
1) Fill the appropriate lanes of the vector values with the task-