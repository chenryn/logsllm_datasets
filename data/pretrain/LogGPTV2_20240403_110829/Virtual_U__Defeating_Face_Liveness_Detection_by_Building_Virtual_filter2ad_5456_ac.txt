### 3.4 Color Distribution Alignment

The color distribution of the iris and pupil forms a three-dimensional distribution in the RGB color space. We estimate this color distribution using a 3D Gaussian function, where the three principal components are denoted as \( (b_1, b_2, b_3) \) with corresponding weights \( (\sigma_1, \sigma_2, \sigma_3) \), such that \( \sigma_1 \geq \sigma_2 \geq \sigma_3 > 0 \). We perform the same analysis for the eye region of the average face model obtained from 3DMM [39], whose eyes are looking straight towards the camera. This yields principal color components \( (b_{\text{std}}^1, b_{\text{std}}^2, b_{\text{std}}^3) \) with weights \( (\sigma_{\text{std}}^1, \sigma_{\text{std}}^2, \sigma_{\text{std}}^3) \), where \( \sigma_{\text{std}}^1 \geq \sigma_{\text{std}}^2 \geq \sigma_{\text{std}}^3 > 0 \).

To convert the eye texture from the average model to the user's eye texture, we use the following transformation for each texture pixel \( c \) in the eye region of the average texture:

\[ c_{\text{convert}} = \sum_{i=1}^{3} \left( \frac{\sigma_i}{\sigma_{\text{std}}^i} \right) (c - b_{\text{std}}^i) b_i. \]

This alignment ensures that the color distribution of the average eye texture matches the color distribution of the user’s eye texture. By patching the eye region of the facial model with this converted average texture, we realistically capture the user’s eye appearance with a forward gaze.

### 3.5 Adding Facial Animations

Some liveness detection methods require users to perform specific actions to unlock the system. To mimic these actions, we can animate our facial model using a pre-defined set of facial expressions (e.g., from FaceWarehouse [8]). Recall that in deriving Equation 2, we have already computed the weight for the identity axis \( \alpha_{\text{id}} \), which captures the user-specific face structure in a neutral expression. We can adjust the expression of the model by substituting a specific, known expression weight vector \( \alpha_{\text{exp}}^{\text{std}} \) into Equation 2. By interpolating the model’s expression weight from 0 to \( \alpha_{\text{exp}}^{\text{std}} \), we can animate the 3D facial model to smile, laugh, blink, and raise the eyebrows (see Figure 6).

**Figure 6: Animated expressions. From left to right: smiling, laughing, closing the eyes, and raising the eyebrows.**

### 3.6 Leveraging Virtual Reality

While the previous steps were necessary to recover a realistic, animated model of a targeted user’s face, our key insight is that virtual reality systems can be leveraged to display this model as if it were a real, three-dimensional face. This VR-based spoofing represents a fundamentally new class of attacks that exploit weaknesses in camera-based authentication systems.

In the VR system, the synthetic 3D face of the user is displayed on the screen of the VR device. As the device rotates and translates in the real world, the 3D face moves accordingly. To an observing face authentication system, the depth and motion cues of the display exactly match what would be expected for a human face. Our experimental VR setup consists of custom 3D-rendering software displayed on a Nexus 5X smartphone. Given the ubiquity of smartphones in modern society, our implementation is practical and comes at no additional hardware cost to an attacker. In practice, any device with similar rendering capabilities and inertial sensors could be used.

On smartphones, accelerometers and gyroscopes work together to provide the device with a sense of self-motion. For example, they detect when the device is rotated from a portrait view to a landscape view and rotate the display in response. However, these sensors cannot recover absolute translation; the device cannot determine how its position has changed in 3D space. This presents a challenge because, without knowledge of the device's 3D movement, we cannot move our 3D facial model in a realistic fashion. Consequently, the observed 3D facial motion will not agree with the device’s inertial sensors, causing our method to fail on methods like that of Li et al. [34] that use such data for liveness detection.

Fortunately, it is possible to track the 3D position of a moving smartphone using its outward-facing camera with structure from motion (see §2.3). Using the camera’s video stream as input, the method works by tracking points in the surrounding environment (e.g., the corners of tables) and then estimating their position in 3D space. At the same time, the 3D position of the camera is recovered relative to the tracked points, thus inferring the camera’s change in 3D position. Several computer vision approaches have been recently introduced to solve this problem accurately and in real-time on mobile devices [28, 46, 55, 56]. In our experiments, we use a printed marker placed on a wall in front of the camera, rather than tracking arbitrary objects in the surrounding scene; however, the end result is the same. By incorporating this module into our proof of concept, the perspective of the viewed model due to camera translation can be simulated with high consistency and low latency.

**Figure 7: Example setup using virtual reality to mimic 3D structure from motion. The authentication system observes a virtual display of a user’s 3D facial model that rotates and translates as the device moves. To recover the 3D translation of the VR device, an outward-facing camera is used to track a marker in the surrounding environment.**

**Note:** Specialized VR systems such as the Oculus Rift could be used to further improve the precision and latency of camera tracking. Such advanced, yet easily obtainable, hardware has the potential to deliver even more sophisticated VR attacks compared to what is presented here.

### 4. Evaluation

We now demonstrate that our proposed spoofing method constitutes a significant security threat to modern face authentication systems. Using real social media photos from consenting users, we successfully broke five commercial authentication systems with a practical, end-to-end implementation of our approach. To better understand the threat, we systematically run lab experiments to test the capabilities and limitations of our proposed method. Moreover, we successfully test our proposed approach with the latest motion-based liveness detection approach by Li et al. [34], which is not yet available in commercial systems.

#### Participants

We recruited 20 volunteers for our tests of commercial face authentication systems. The volunteers were recruited through word of mouth and included graduate students and faculty from two separate research labs. Consultation with our IRB departmental liaison revealed that no application was needed. There was no compensation for participating in the lab study. The ages of the participants ranged between 24 and 44 years, and the sample consisted of 6 females and 14 males. The participants came from various ethnic backgrounds (as stated by the volunteers): 6 were of Asian descent, 4 were Indian, 1 was African-American, 1 was Hispanic, and 8 were Caucasian. With their consent, we collected public photos from the users’ Facebook and Google+ social media pages, as well as any photos we could find of the users on personal or community web pages, and via image search on the web. The smallest number of photos we collected for an individual was 3, and the largest number was 27. The average number of photos was 15, with a standard deviation of approximately 6 photos. No private information about the subjects was recorded besides the storage of the photographs they consented to. Any images of subjects displayed in this paper were done with the consent of that particular volunteer.

For our experiments, we manually extracted the region around the user’s face in each image. An adversary could also perform this action automatically using tag information on social media sites, when available. One interesting aspect of social media photos is that they may capture significant physical changes of users over time. For instance, one of our participants lost 20 pounds in the last 6 months, and our reconstruction had to utilize images from before and after this change. Two other users had frequent changes in facial hair styles—beards, mustaches, and clean-shaven—all of which we used for our reconstruction. Another user had only uploaded 2 photos to social media in the past 3 years. These variations present challenges for our framework, both for initially reconstructing the user’s face and for creating a likeness that matches their current appearance.

#### Industry-leading Solutions

We tested our approach on five advanced commercial face authentication systems: KeyLemon, Mobius, True Key [18], BioID [21], and 1U App. Table 1 summarizes the training data required by each system when learning a user’s facial appearance, as well as the approximate number of users for each system, when available. All systems incorporate some degree of liveness detection into their authentication protocol. KeyLemon and the 1U App require users to perform an action such as blinking, smiling, rotating the head, and raising the eyebrows. In addition, the 1U App requests these actions in a random fashion, making it more resilient to video-based attacks. BioID, Mobius, and True Key are motion-based systems and detect 3D facial structure as the user turns their head. It is also possible that these five systems employ other advanced liveness detection approaches, such as texture-based detection schemes, but such information has not been made available to the public.

**Table 1: Summary of the face authentication systems evaluated.**

| System      | Training Method     | # Installs/Reviews       |
|-------------|---------------------|--------------------------|
| KeyLemon    | Single video        | ~100,000                 |
| Mobius      | 10 still images     | 18 reviews               |
| True Key    | Single video        | 50,000-100,000           |
| BioID       | 4 videos            | unknown                  |
| 1U App      | 1 still image       | 50-100                   |

**Note:** The second column lists how each system acquires training data for learning a user’s face, and the third column shows the approximate number of installations or reviews each system has received according to (1) the Google Play Store, (2) the iTunes store, or (3) softpedia.com. BioID is a relatively new app and does not yet have customer reviews on iTunes.

All participants were registered with the five face authentication systems under indoor illumination. The average length of time spent by each of the volunteers to register across all systems was 20 minutes. As a control, we first verified that all systems were able to correctly identify the users in the same environment. Next, before testing our method using textures obtained via social media, we evaluated whether our system could spoof the recognition systems using photos taken in this environment. We captured one front-view photo for each user under the same indoor illumination and then created their 3D facial model with our proposed approach. We found that these 3D facial models were able to spoof each of the five candidate systems with a 100% success rate, as shown in the second column of Table 2.

**Table 2: Success rate for 5 face authentication systems using a model built from (second column) an image of the user taken in an indoor environment and (third and fourth columns) images obtained on users’ social media accounts. The fourth column shows the average number of attempts needed before successfully spoofing the target user.**

| System      | Indoor Spoof % | Social Media Spoof % | Avg. # Tries |
|-------------|-----------------|-----------------------|--------------|
| KeyLemon    | 100%            | 85%                   | 1.6          |
| Mobius      | 100%            | 80%                   | 1.5          |
| True Key    | 100%            | 70%                   | 1.3          |
| BioID       | 100%            | 55%                   | 1.7          |
| 1U App      | 100%            | 0%                    | —            |

Following this, we reconstructed each user’s 3D facial model using the images collected from public online sources. Since not all textures will successfully spoof the recognition systems, we created textured reconstructions from all source images and iteratively presented them to the system (in order of what we believed to be the best reconstruction, followed by the second best, and so on) until either authentication succeeded or all reconstructions had been tested.

#### Findings

We summarize the spoofing success rate for each system in Table 2. Except for the 1U system, all facial recognition systems were successfully spoofed for the majority of participants when using social media photos, and all systems were spoofed using indoor, frontal view photos. Out of our 20 participants, there were only 2 individuals for whom none of the systems was spoofed via the social-media-based attack.

Looking into the social media photos we collected of our participants, we observe a few trends among our results. First, moderate- to high-resolution photos lend substantial realism to the textured models. In particular, photos taken by professional photographers (e.g., wedding photos or family portraits) lead to high-quality facial texturing. Such photos are prime targets for facial reconstruction because they are often posted by other users and made publicly available. Second, we note that group photos provide consistent frontal views of individuals, albeit with lower resolution. In cases where high-resolution photos are not available, such frontal views can be used to accurately recover a user’s 3D facial structure. These photos are easily accessible via friends of users, as well. Third, we note that the least spoof-able users were not those who necessarily had a low number of personal photos, but rather users who had few forward-facing photos and/or no photos with sufficiently high resolution. From this observation, it seems that creating a realistic texture for user recognition is the primary factor in determining whether a face authentication method will be fooled by our approach. Only a small number of photos are necessary to defeat facial recognition systems.

We found that our failure to spoof the 1U App, as well as our lower performance on BioID, using social media photos was directly related to the poor usability of those systems. Specifically, we found that the systems have strict requirements for liveness detection, which are more challenging to meet with less controlled, lower-quality images.