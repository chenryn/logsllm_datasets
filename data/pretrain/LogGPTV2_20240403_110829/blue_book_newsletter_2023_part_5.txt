        a.remove(x)
    ```
* New: [Override entrypoint.](docker.md#override-entrypoint)
    ```bash
    sudo docker run -it --entrypoint /bin/bash [docker_image]
    ```
* New: [Disable ipv6.](docker.md#disable-ipv6)
    ```bash
    sysctl net.ipv6.conf.all.disable_ipv6=1
    sysctl net.ipv6.conf.default.disable_ipv6=1
    ```
* New: [Remove the apt cache after installing a package.](docker.md#remove-the-apt-cache-after-installing-a-package)
    ```
    RUN apt-get update && apt-get install -y \
      python3 \
      python3-pip \
      && rm -rf /var/lib/apt/lists/*
    ```
* New: [Add the contents of a directory to the docker.](docker.md#add-the-contents-of-a-directory-to-the-docker)
    ```
    ADD ./path/to/directory /path/to/destination
    ```
* New: [Add healthcheck to your dockers.](docker.md#add-healthcheck-to-your-dockers)
    Health checks allow a container to expose its workload’s availability. This stands apart from whether the container is running. If your database goes down, your API server won’t be able to handle requests, even though its Docker container is still running.
    This makes for unhelpful experiences during troubleshooting. A simple `docker ps` would report the container as available. Adding a health check extends the `docker ps` output to include the container’s true state.
    You configure container health checks in your Dockerfile. This accepts a command which the Docker daemon will execute every 30 seconds. Docker uses the command’s exit code to determine your container’s healthiness:
    - `0`: The container is healthy and working normally.
    - `1`: The container is unhealthy; the workload may not be functioning.
    Healthiness isn’t checked straightaway when containers are created. The status will show as starting before the first check runs. This gives the container time to execute any startup tasks. A container with a passing health check will show as healthy; an unhealthy container displays unhealthy.
    In docker-compose you can write the healthchecks like the next snippet:
    ```yaml
    ---
    version: '3.4'
    services:
      jellyfin:
        image: linuxserver/jellyfin:latest
        container_name: jellyfin
        restart: unless-stopped
        healthcheck:
          test: curl http://localhost:8096/health || exit 1
          interval: 10s
          retries: 5
          start_period: 5s
          timeout: 10s
    ```
* New: [List the dockers of a registry.](docker.md#list-the-dockers-of-a-registry)
    List all repositories (effectively images):
    ```bash
    $: curl -X GET https://myregistry:5000/v2/_catalog
    > {"repositories":["redis","ubuntu"]}
    ```
    List all tags for a repository:
    ```bash
    $: curl -X GET https://myregistry:5000/v2/ubuntu/tags/list
    > {"name":"ubuntu","tags":["14.04"]}
    ```
    If the registry needs authentication you have to specify username and password in the curl command
    ```bash
    curl -X GET -u : https://myregistry:5000/v2/_catalog
    curl -X GET -u : https://myregistry:5000/v2/ubuntu/tags/list
    ```
* New: [Searching by attribute and value.](beautifulsoup.md#searching-by-attribute-and-value)
    ```python
    soup = BeautifulSoup(html)
    results = soup.findAll("td", {"valign" : "top"})
    ```
* New: [Install a specific version of Docker.](docker.md#installation)
    Follow [these instructions](https://docs.docker.com/engine/install/debian/)
    If that doesn't install the version of `docker-compose` that you want use [the next snippet](https://stackoverflow.com/questions/49839028/how-to-upgrade-docker-compose-to-latest-version):
    ```bash
    VERSION=$(curl --silent https://api.github.com/repos/docker/compose/releases/latest | grep -Po '"tag_name": "\K.*\d')
    DESTINATION=/usr/local/bin/docker-compose
    sudo curl -L https://github.com/docker/compose/releases/download/${VERSION}/docker-compose-$(uname -s)-$(uname -m) -o $DESTINATION
    sudo chmod 755 $DESTINATION
    ```
    If you don't want the latest version set the `VERSION` variable.
* New: [Dockerize a PDM application.](python_docker.md#using-pdm)
    It is possible to use PDM in a multi-stage Dockerfile to first install the project and dependencies into `__pypackages__` and then copy this folder into the final stage, adding it to `PYTHONPATH`.
    ```dockerfile
    FROM python:3.11-slim-bookworm AS builder
    RUN pip install pdm
    COPY pyproject.toml pdm.lock README.md /project/
    COPY src/ /project/src
    WORKDIR /project
    RUN mkdir __pypackages__ && pdm sync --prod --no-editable
    FROM python:3.11-slim-bookworm
    ENV PYTHONPATH=/project/pkgs
    COPY --from=builder /project/__pypackages__/3.11/lib /project/pkgs
    COPY --from=builder /project/__pypackages__/3.11/bin/* /bin/
    CMD ["python", "-m", "project"]
    ```
### [Click](click.md)
* New: Split stdout from stderr in tests.
    By default the `runner` is configured to mix `stdout` and `stderr`, if you wish to tell apart both sources use:
    ```python
    def test(runner: CliRunner):
      ...
      runner.mix_stderr = False
    ```
* New: [File system isolation.](click.md#file-system-isolation)
    You may need to isolate the environment variables if your application read the configuration from them. To do that override the `runner` fixture:
    ```python
    @pytest.fixture(name="runner")
    def fixture_runner() -> CliRunner:
        """Configure the Click cli test runner."""
        return CliRunner(
            env={
                'PASSWORD_STORE_DIR': '',
                'GNUPGHOME': '',
                'PASSWORD_AUTH_DIR': '',
            },
            mix_stderr=False
        )
    ```
    If you define the fixture in `conftest.py` you may need to use another name than `runner` otherwise it may be skipped, for example `cli_runner`.
### [SQLite](sqlite.md)
* New: [Import a table from another database.](sqlite.md#import-a-table-from-another-database)
    If you have an SQLite databases named `database1` with a table `t1` and `database2` with a table `t2` and want to import table `t2` from `database2` into `database1`. You need to open `database1` with `litecli`:
    ```bash
    litecli database1
    ```
    Attach the other database with the command:
    ```sqlite
    ATTACH 'database2file' AS db2;
    ```
    Then create the table `t2`, and copy the data over with:
    ```sqlite
    INSERT INTO t2 SELECT * FROM db2.t2;
    ```
### [Promql](promql.md)
* New: [Add basic operations.](promql.md#usage)
    Selecting series:
    * Select latest sample for series with a given metric name:
      ```promql
      node_cpu_seconds_total
      ```
    * Select 5-minute range of samples for series with a given metric name:
      ```promql
      node_cpu_seconds_total[5m]
      ```
    * Only series with given label values:
      ```promql
      node_cpu_seconds_total{cpu="0",mode="idle"}
      ```
    * Complex label matchers (`=`: equality, `!=`: non-equality, `=~`: regex match, `!~`: negative regex match):
      ```promql
      node_cpu_seconds_total{cpu!="0",mode=~"user|system"}
      ```
    * Select data from one day ago and shift it to the current time:
      ```promql
      process_resident_memory_bytes offset 1d
      ```
    Rates of increase for counters:
    * Per-second rate of increase, averaged over last 5 minutes:
      ```promql
      rate(demo_api_request_duration_seconds_count[5m])
      ```
    * Per-second rate of increase, calculated over last two samples in a 1-minute time window:
      ```promql
      irate(demo_api_request_duration_seconds_count[1m])
      ```
    * Absolute increase over last hour:
      ```promql
      increase(demo_api_request_duration_seconds_count[1h])
      ```
    Aggregating over multiple series:
    * Sum over all series:
      ```promql
      sum(node_filesystem_size_bytes)
      ```
    * Preserve the instance and job label dimensions:
      ```promql
      sum by(job, instance) (node_filesystem_size_bytes)
      ```
    * Aggregate away the instance and job label dimensions:
      ```promql
      sum without(instance, job) (node_filesystem_size_bytes)
      ```
      Available aggregation operators: `sum()`, `min()`, `max()`, `avg()`, `stddev()`, `stdvar()`, `count()`, `count_values()`, `group()`, `bottomk()`, `topk()`, `quantile()`.
    Time:
    * Get the Unix time in seconds at each resolution step:
      ```promql
      time()
      ```
    * Get the age of the last successful batch job run:
      ```promql
      time() - demo_batch_last_success_timestamp_seconds
      ```
    * Find batch jobs which haven't succeeded in an hour:
      ```promql
      time() - demo_batch_last_success_timestamp_seconds > 3600
      ```
* New: [Run operation only on the elements that match a condition.](promql.md#run-operation-only-on-the-elements-that-match-a-condition)
    Imagine we want to run the `zfs_dataset_used_bytes - zfs_dataset_used_by_dataset_bytes` operation only on the elements that match `zfs_dataset_used_by_dataset_bytes > 200e3`. You can do this with `and`:
    ```
    zfs_dataset_used_bytes - zfs_dataset_used_by_dataset_bytes and zfs_dataset_used_by_dataset_bytes > 200e3
    ```
* New: [Substracting two metrics.](promql.md#substracting-two-metrics)
    To run binary operators between vectors you need them to match. Basically it means that it will only do the operation on the elements that have the same labels. Sometimes you want to do operations on metrics that don't have the same labels. In those cases you can use the `on` operator. Imagine that we want to substract the next vectors:
    ```
    zfs_dataset_used_bytes{type='filesystem'}
    ```
    And
    ```
    sum by (hostname,filesystem) (zfs_dataset_used_bytes{type='snapshot'})
    ```
    That only have in common the labels `hostname` and filesystem`.
    You can use the next expression then:
    ```
    zfs_dataset_used_bytes{type='filesystem'} - on (hostname, filesystem) sum by (hostname,filesystem) (zfs_dataset_used_bytes{type='snapshot'})
    ```
    To learn more on Vector matching read [this article](https://iximiuz.com/en/posts/prometheus-vector-matching/)
* New: [Ranges only allowed for vector selectors.](promql.md#ranges-only-allowed-for-vector-selectors)
    You may need to specify a subquery range such as `[1w:1d]`.
### [Logql](logql.md)
* New: Introduce LogQL.
    [LogQL](https://grafana.com/docs/loki/latest/logql/) is Grafana Loki’s PromQL-inspired query language. Queries act as if they are a distributed `grep` to aggregate log sources. LogQL uses labels and operators for filtering.
    There are two types of LogQL queries:
    - Log queries: Return the contents of log lines.
    - Metric queries: Extend log queries to calculate values based on query results.
* New: [Apply a pattern to the value of a label.](logql.md#apply-a-pattern-to-the-value-of-a-label)
    Some logs are sent in json and then one of their fields can contain other structured data. You may want to use that structured data to further filter the logs.
    ```logql
    {app="ingress-nginx"} | json | line_format `{{.log}}` | pattern ` - -  "  "    "" ` | method != `GET`
    ```
    - `{app="ingress-nginx"}`: Show only the logs of the `ingress-nginx`.
    - `| json`:  Interpret the line as a json.
    - ```| line_format `{{.log}}` | pattern ` - -  "  "    "" ````: interpret the `log` json field of the trace with the selected pattern
    - ```| method != `GET````: Filter the line using a key extracted by the pattern.
* New: [Count the unique values of a label.](logql.md#count-the-unique-values-of-a-label)
    Sometimes you want to alert on the values of a log. For example if you want to make sure that you're receiving the logs from more than 20 hosts (otherwise something is wrong). Assuming that your logs attach a `host` label you can run