the six on the bottom correspond to a sliding-window updating strategy. In
particular, at 5 and 50 repetitions, the error-rate curves are much lower in the
lower panels. An updating strategy seems to improve performance, especially if
operating with only a few training repetitions.
Overall, the lowest average equal-error rate was 7.1%, observed for the Man-
hattan (scaled) detector with 100 repetitions of training, hold and keydown-
keydown features, sliding-window updating, and unpracticed impostors. Among
Why Did My Detector Do That?!
263
Detectors
Nearest Neighbor (Mahalanobis)
Outlier Count (z−score)
Manhattan (scaled)
0
50
100
150
200
Updating None
Feature Set H+UD
 : 
 : 
Impostor Practice None
 : 
Updating None
Feature Set H+DD
 : 
 : 
Impostor Practice None
 : 
Updating None
Feature Set H+UD
 : 
 : 
 : 
Impostor Practice Very High
Updating None
Feature Set H+DD
 : 
 : 
 : 
Impostor Practice Very High
e
t
a
R
r
o
r
r
l
E
−
a
u
q
E
0.30
0.20
0.10
0.30
0.20
0.10
0.30
0.20
0.10
Updating None
 : 
Feature Set H+DD+UD+Ret
 : 
Impostor Practice None
 : 
Updating None
 : 
Feature Set H+DD+UD+Ret
Impostor Practice Very High
 : 
 : 
Updating Sliding Window
 : 
Feature Set H+UD
 : 
Impostor Practice None
 : 
Updating Sliding Window
 : 
Feature Set H+DD
 : 
Impostor Practice None
 : 
Updating Sliding Window
 : 
Feature Set H+UD
Impostor Practice Very High
 : 
 : 
Updating Sliding Window
 : 
Feature Set H+DD
Impostor Practice Very High
 : 
 : 
Updating Sliding Window
Feature Set H+DD+UD+Ret
 : 
 : 
Impostor Practice None
 : 
Updating Sliding Window
Feature Set H+DD+UD+Ret
Impostor Practice Very High
 : 
 : 
 : 
0.30
0.20
0.10
0.30
0.20
0.10
0.30
0.20
0.10
0
50
100
150
200
Number of Training Repetitions
Fig. 1. The average equal-error rates for each detector in the experiment as a function
of training amount, feature set, updating strategy, and impostor practice. Each curve
shows the eﬀect of training on one of the three detectors. Each panel displays the
results for one combination of updating (None/Sliding Window), feature set (H:hold
times, DD:keydown-keydown times, UD:keyup-keydown times, Ret:Return-key times),
and impostor practice (None/Very High). Comparisons can be made across panels
because the scales are the same. For instance, the error-rate curves in the upper six
panels (no updating) are higher than the curves in the lower six panels (sliding-window
updating). This comparison suggests updating reduces error rates.
264
K. Killourhy and R. Maxion
the very-practiced impostor results, the lowest average equal-error rate was 9.7%,
observed for the same combination of algorithm, training amount, feature set,
and updating. The empirical results would seem to recommend this combina-
tion of detector, training amount, and feature set, but we would withhold a
recommendation without further statistical analysis.
4 Statistical Analysis
The empirical results and visualization in Section 3 provide some insight into
what factors might be important, but to make predictions about an anomaly
detector’s future performance we need a statistical model. In this section, we
describe the statistical analysis that we performed, and we present the model
that it produced.
4.1 Procedure
The analysis is described in stages. First, we explain what a linear mixed-eﬀects
model is. Then, we describe how we estimate the model parameters. Finally, we
lay out the procedure for selecting a particular linear mixed-eﬀects model.
Linear mixed-eﬀects models. In statistical language, we intend to model the
eﬀect of six factors—algorithm, training amount, feature set, updating strategy,
impostor practice, and typist (or subject)—on a response variable: the detector’s
equal-error rate. Fixed and random are terms used by statisticians to describe
two diﬀerent kinds of eﬀect. When a model has both ﬁxed and random eﬀects,
it is called a mixed-eﬀects model.
The diﬀerence between ﬁxed and random eﬀects is sometimes subtle, but the
following rule of thumb is typically applied. If we care about the eﬀect of each
value of a factor, the factor is a ﬁxed eﬀect. If we only care about the variation
among the values of a factor, the factor is a random eﬀect.
For instance, we treat the algorithm as a ﬁxed eﬀect and the subject (or
typist) as a random eﬀect. Practitioners want to know which algorithm’s equal-
error rate is lower: Manhattan (scaled) or Nearest Neighbor (Mahalanobis). We
care about the eﬀect of each value of the factor, so algorithm is a ﬁxed eﬀect. In
contrast, practitioners do not want to know which subject’s equal-error rate is
lower: Subject 1 or Subject 2. Neither subject will be a typist on their system.
What we care about is how much variation there is between typists, and so the
subject is a random eﬀect.
The following example of a mixed-eﬀects model for keystroke-dynamics data
may further elucidate the diﬀerence:
Y = μ + Ah + Ti + Fj + Uk + Il + S + 
S ∼ N(0, σ2
s)
 ∼ N(0, σ2
 )
(1)
The notation in model equation (1) may seem daunting at ﬁrst. On the ﬁrst
line of the model, Y is the response (i.e., the equal-error rate); μ is a baseline
Why Did My Detector Do That?!
265
equal-error rate; Ah, Ti, Fj, Uk, and Il are the ﬁxed eﬀects of the algorithm,
training amount, feature set, updating strategy, and impostor practice, respec-
tively; S is the random eﬀect of the typist (or subject); and,  is the noise term.
On the second line, the distribution of the random eﬀect (S) is assumed to be
Normal with zero mean and an unknown variance, denoted σ2
s. On the third line,
the distribution of the noise () is Normally distributed with zero mean and a
diﬀerent unknown variance, denoted σ2
 .
The ﬁrst term in the model equation (μ) denotes the average equal-error rate
for one particular combination of ﬁxed-eﬀect factor values, called the baseline
values. For instance, the baseline values might be the Manhattan (scaled) de-
tector, 5 training repetitions, hold and keyup-keydown times, no updating, and
unpracticed impostors. The average equal-error rate for that combination is μ
(e.g., μ = 17.6%).
For each of the ﬁve ﬁxed eﬀects, there is a separate term in the model equation
(Ah, Ti, Fj, Uk, Il). These terms denote the eﬀect on the equal-error rate of
departing from the baseline values. For instance, Ah is a placeholder for either
of two departures from the Manhattan (scaled) baseline algorithm : A1 denotes
the change to Outlier-count (z-score) and A2 denotes the change to Nearest
Neighbor (Mahalanobis) detector. If the detector in the baseline combination
were replaced with the Outlier-count (z-score) detector, the equal-error rate
would be calculated as μ + A1 (e.g., 17.6% + 2.7 = 20.3%).
For the random eﬀect, there is both a term in the model equation (S) and a
distributional assumption (S ∼ N(0, σ2
s)). Like the ﬁxed-eﬀects terms, S repre-
sents the eﬀect of a departure. Speciﬁcally, it introduces a per-subject eﬀect that
is negative (S  0) for
hard-to-discriminate subjects. Unlike the ﬁxed-eﬀects term, S is a random vari-
able centered at zero. Its variance (σ2
s) expresses a measure of the typist-to-typist
variation in the model.
The ﬁnal term in the model equation () is the noise term representing the
unknown inﬂuences of additional factors on the equal-error rate. Like the random
eﬀect (S),  is a Normally distributed random variable. Its variance, σ2
 expresses
a measure of the residual uncertainty in our equal-error predictions.
Parameter estimation. When ﬁtting a linear mixed-eﬀects model, the un-
 ) are estimated from the
known parameters (e.g., μ, Ah, Ti, Fj, Uk, Il, σ2
data. There are a variety of accepted parameter-estimation methods; a popular
one is the method of maximum-likelihood. From any estimate of the parame-
ters, it is possible to derive a probability density function. Among all estimates,
the maximum-likelihood estimates are those which give the greatest probability
density to the observed data [18].
s, and σ2
However, the maximum-likelihood methods have been shown to produce bi-
s). The favored method is a
ased estimates of the variance parameters (e.g., σ2
slight elaboration called REML estimation (for restricted or residual maximum
likelihood) which corrects for the bias in maximum-likelihood estimation [16,18].
We adopt the REML estimates.
266
K. Killourhy and R. Maxion
Model selection. In the discussion so far, we have explained how to interpret
model equation (1) and how to do parameter estimation given such an equation.
We have not explained how to select that model equation in the ﬁrst place. For
instance, consider the following alternative:
Y = μ + Ah + Ti + AThi + S + 
S ∼ N(0, σ2
s)
 ∼ N(0, σ2
 )
(2)
In model equation (2), the terms corresponding to feature set (Fj), updating
(Uk), and impostor practice (Il) do not appear, so they are assumed to have no
eﬀect. An interaction term between algorithm and training (AThi) appears, so
the eﬀect of training is assumed to depend on the algorithm.
The interaction term denotes the eﬀect on the equal-error rate of a departure
from the baseline values in both algorithm and training amount. Without an
interaction term, the eﬀect would be additive (Ah + Ti). With an interaction
term, the additive eﬀect can be adjusted (Ah +Ti+AThi), increased or decreased
as ﬁts the data. Interaction eﬀects can be estimated with REML estimation just
like the other parameters.
Looking at the relationship between the algorithm and training in Figure 1,
we would expect to see a model with an AThi interaction. The Nearest Neighbor
(Mahalanobis) has a much steeper slope from 5–50 training repetitions than the
other two detectors. If the eﬀects of the algorithm and training were additive
(i.e., no interaction eﬀect), the slopes of the three curves would be parallel.
Consequently, model equation (2) might describe our data better.
Model equations (1) and (2) are but two members of a whole family of pos-
sible models that we might use to describe our data. We need a method to
search through this family of models and ﬁnd the one that is most appropriate.
Speciﬁcally, we need a way to compare two models and decide which one better
explains the data.
Of the various model-comparison strategies, one often employed is Schwartz’s
Bayesian Information Criterion (BIC). The details are beyond the scope of this
paper, but in brief a model’s BIC score is a summary combining both how well
the model ﬁts the data (i.e., the likelihood of the model) and also the number of
parameters used to obtain the ﬁt (i.e., the number of terms in the model). Having
more parameters leads to a better ﬁtting model, and having fewer parameters
leads to a simpler model. BIC captures the trade-oﬀ between ﬁt and simplicity.
When comparing two models, we calculate and compare their BIC scores. Of the
two, we adopt the one with the lower score [9].
Let us note one procedural issue when performing BIC-based model selection
using mixed-eﬀects models. REML estimation is incompatible with this heuris-
tic, and so when comparing two models using BIC, the maximum-likelihood
estimates are used. Once a model is selected, the parameters are re-estimated
using REML. Intuitively, we use the maximum-likelihood estimates because, de-
spite their bias, they allow us to do model selection. Then, once we have chosen
a model, we can switch to the better REML estimates.
Why Did My Detector Do That?!
267
For the analysis of our experimental results, we begin with a model contain-
ing all the ﬁxed eﬀects, all interactions between those eﬀects, and no per-subject
random eﬀect. We estimate the parameters of the model using maximum like-
lihood. Then, we add a per-subject random eﬀect, estimate the parameters of
the new model, and compare the BIC scores of the two. If the one with the
per-subject random eﬀect has a lower BIC (and it does), we adopt it. Then, in
a stepwise fashion, we omit each term in the model, re-estimate the parameters,
and recalculate the BIC. If we obtain a lower BIC by omitting any of the terms
of the model, we drop the term which lowers the BIC the most and repeat the
process. When no more terms can be dropped in this way, we adopt the current
model as ﬁnal and estimate the parameters using REML. This procedure is quite
typical for mixed-eﬀects model selection [7,16].
As another procedural note, we do not drop terms if they are involved in
higher-order interactions that are still part of the model. For instance, we would
not drop feature set (Fj) as a factor if the interaction between algorithm and
feature set (AFhj) is still in the model. This so-called principle of hierarchy
enables us to more easily interpret the resulting model [7]. For the statistical
analysis, we used the R statistical programming language (version 2.10.0) [17].
To ﬁt linear mixed-eﬀects models, we used the lme4 mixed-eﬀects modeling
package (version 0.999375-32) [3].
4.2 Results
We begin by describing the equation obtained through model selection since it
informs us of the broad relationships between the factors. Then, we present the
parameter estimates which quantify the eﬀects of each factor and enable us to
make predictions about a detector’s future error rates.
Selected model. We arrived at the following model equation to describe the
experimental data:
Y = μ + Ah + Ti + Uk + Il
+ AT hi + AU hk + T U ik + U I kl + AT U hik
+ S + 
S ∼ N(0, σ2
s)
 ∼ N(0, σ2
 )
(3)
The equation has been split over multiple lines to make it easier to describe.
The ﬁrst line shows the main eﬀects in the model. Note that the algorithm (Ah),
training amount (Ti), updating (Uk), and impostor practice (Il) are all terms
retained in the model, but the feature set (Fj) has been dropped. During model
selection, it did not substantially improve the ﬁt of the model. This result is not
surprising given how little change there was in any single quadrant of Figure 1.
The second line shows the two-way and three-way interaction eﬀects in the
model. The interactions between the algorithm, training, and updating (AThi,
AUhk, T U ik, and AT Uhik) suggest a complex relationship between these three
268
K. Killourhy and R. Maxion
factors. The two-way interaction between updating and impostor practice (U Ikl)
suggests that updating may mitigate the impostor-practice threat. We will ex-