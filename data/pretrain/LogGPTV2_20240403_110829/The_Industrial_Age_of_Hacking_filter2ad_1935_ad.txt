### Challenges in Semi-Autonomous, Team-Based Analysis Environments

During daily team discussions, several factors challenge the effectiveness of a semi-autonomous, team-based analysis environment. Investing significant resources into a single target can either reveal novel flaws or none at all. A hacker must invest considerable time and effort to determine which is the case.

#### Minimum Skill Threshold
Apprentice hackers are prone to falling into "rabbit holes." Votipka described this phenomenon: "Without prior experience guiding triage, practitioners relied on incidental discovery, driven by curiosity, personal creativity, and persistence, with ample time to navigate the complexity of a program. This approach is time-consuming and haphazard, leading to inconsistent results [40, §VI.A.1]."

The extensive prerequisite knowledge required for some targets makes recruitment more difficult. For the depth-first projects discussed in this paper, we sought expertise in:
1. Software reverse engineering and assembly architectures.
2. C software development.
3. Understanding and modifying software build toolchains.
4. Binary patching.
5. Source auditing.
6. Bug finding.
7. The use of static analysis tools.
8. Fuzzing.

We also aimed to find self-motivated problem solvers. Unsurprisingly, less-skilled subjects were overwhelmed by the demands of such tasks. They felt more surprised, frustrated, and doubtful compared to their experiences with breadth-first strategies (SB). Subjects also claimed that depth-first strategies (SD) were a less effective use of their team's skills. These sentiments likely arose from the quick exhaustion of novice work at the beginning of a bug-finding session, leaving tasks that require more skilled practitioners. Early on, novice subjects found valuable information through internet research, but their contributions diminished significantly over the week.

#### Feedback Loop
When teams were assigned a single target, they continued working even when automation might be on the path to a solution. This is inefficient because human time is expensive, while computer time is relatively inexpensive. SD left subjects with less time to interact with tools and harness them effectively. Hackers were unable to maximize the time spent producing new harnesses to test new code. There is a natural break where, once a harness is complete, it is inefficient for the hacker to continue until they know what automation will discover.

#### Knowledge Sharing and Tasking
A team simultaneously investigating the same target incurs high synchronization overhead. Some findings are of general interest and should be shared immediately, but other information may not be broadly relevant. Communication overhead is a concern, but under-communicating leads to duplicate work. Balancing this is not always clear. Feedback indicated that SB made subjects feel less part of a team, as it naturally leads to more independent work and reduced real-time communication in favor of asynchronous methods like notes and code submissions. Research in related disciplines supports this, showing that the most productive teams in cyber defense exercises have the fewest direct human interactions [4].

The discrete tasks in the fuzzing process seem conducive to parallelization. In practice, these tasks form a pipeline, with progress on one task being necessary to advance to the next. For some targets, such as ubus [29], emulating the target is a nontrivial prerequisite to fuzzing. The narrow target selection of SD does little to help with parallelizing the fuzzing pipeline.

#### Output
Ultimately, Team A found zero bugs in uhttpd and three bugs in dropbear; Team B found zero and four, respectively. With SD, hackers tended to delve deeply into complex components of a target, developing tunnel vision and ignoring other components. Deadlines led to overlooked bugs that might have been easy to find using automation techniques and minimal human effort.

### Breadth-First Strategy Discussion

#### Minimum Skill Threshold and Feedback
Apprentice hackers were both more prolific and more effective using SB. SB allows humans to hand off work to machines and only resume work on a target once the machine has discovered a solution. This model creates a feedback loop, minimizing human time and iterating until the desired outcome is achieved.

#### Knowledge Sharing and Tasking
SB allows team members to work independently with confidence, make progress, and communicate key information asynchronously. This reduces overhead and redundancy while creating a growing record of findings. Pairing novices with experts often resulted in the expert spending more time teaching than hacking. In an SB model, team members can record and convey their problem-solving, allowing experts to review and suggest paths forward based on their experience. SB’s large set of targets means hackers can create a collection of fuzzing pipelines as part of a parallel strategy.

### Subsequent and Future Work

We applied our breadth-first strategy to other large-scale projects after our experiment, and we recorded additional lessons and suggested areas for future work.

#### Targeting
We further automated the targeting stage to make leaders more efficient. For one project, a script enumerated binaries on each device and established issues on GitLab, easing decision-making and progress tracking. Future experiments could benefit from prioritizing targets, as analysts tend to work through the unsorted queue from top to bottom.

#### Information Gathering
Future work could investigate using web scrapers for common research tasks. For example, a script could collect search results for "objdump CVE" or "fuzzing objdump" and append this information to each target’s GitLab issue.

#### Program Understanding
Further research is needed in program understanding and its impact on decision-making. Automated tools should identify potential bug indicators, justifying additional time spent improving harnesses and understanding target programs. Without these indicators, scaling becomes difficult, as analysts may focus too much on challenging targets and overlook easier-to-find bugs. Tools could add information to GitLab issues, such as the lack of basic runtime protection mechanisms, the presence of the SUID bit, and program outputs, to help prioritize targets.

#### Attack Surface Analysis
In a pilot study, transferring fuzz harnesses to a separate network was a significant undertaking. We adopted Docker to reduce overhead, making hackers responsible for test builds. This switch drastically reduced the overhead incurred when transferring harnesses to different networks for fuzzing.

#### Automated Exploration
Automation in this stage involves running completed harnesses on computing resources. An architecture like Clusterfuzz [32] matches our intent. During the depth-first strategy, we used a single fuzz job on many nodes. Transitioning to many targets, we opted for a simpler structure with jobs running on a single node and employing all cores, which is more efficient [7].

#### Vulnerability Recognition
Our experiment focused on building teams around the process of harnessing target applications, but more work is needed to manage fuzzing campaign results—vulnerability recognition at scale. Techniques for balancing crash triage and harnessing new targets must be developed, especially with limited manpower.

#### Other
We found overheads in SB that were less impactful in SD. Enforcing GitLab policies and managing targets on and off computing resources became time-consuming with many projects. Fuzzing, archiving, and reviewing results were difficult to balance with other targets in the queue. Higher leadership added to the target queue, requiring us to assign priorities while balancing ongoing work. These practical matters are ripe for future work.

### Conclusion

Frustrated with the pitfalls of SD, we sought a better approach and found one. Evidence indicates that SB is more effective at finding bugs and has positive side effects. SB more efficiently employs hackers of varying skill levels, boosts documentation and learning resources, and better applies automated bug-finding tools. It also clearly defines work roles and unit tasks. Our experiment is repeatable, allowing researchers to test other hypotheses related to the hacking process in a similar environment. Finally, we learned, coached, and hacked for fun and profit.

### Acknowledgments

We are grateful for the aid provided by Leslie Bell, Temmie Shade, James Tittle, Andrew Ruef, Richard Bae, and ForAllSecure. The staff at Dreamport hosted our pilot and experiment, providing space, computing resources, and support. We thank our participants in both the actual study and the pilot. This work was performed in part during the NSA’s Computer Network Operator Development Program, with support from the Army, Navy, and Air Force.

### Self-Assessment and Schedule

[Self-assessment and schedule details remain unchanged as they are specific and structured.]

This revised text aims to be more coherent, professional, and easier to understand, with a clearer structure and improved flow.