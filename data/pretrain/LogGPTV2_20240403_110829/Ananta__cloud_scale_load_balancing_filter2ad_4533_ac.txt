and IP payload of the packet, which is essential for achieving Di-
rect Server Return (DSR). All Muxes in a Mux Pool use the exact
same hash function and seed value. Since all Muxes have the same
mapping table, it doesn’t matter which Mux a given new connec-
tion goes to, it will be directed to the same DIP.
3.3.3 Flow State Management
Mux supports two types of mapping entries – stateful and state-
less. Stateful entries are used for load balancing and stateless en-
tries are used for SNAT. For stateful mapping entries, once a Mux
has chosen a DIP for a connection, it remembers that decision in
a ﬂow table. Every non-SYN TCP packet, and every packet for
connection-less protocols, is matched against this ﬂow table ﬁrst,
and if a match is found it is forwarded to the DIP from the ﬂow
table. This ensures that once a connection is directed to a DIP, it
continues to go to that DIP despite changes in the list of DIPs in the
mapping entry. If there is no match in the ﬂow table, the packet is
treated as a ﬁrst packet of the connection.
Given that Muxes maintain per-ﬂow state, it makes them vulner-
able to state exhaustion attacks such as the SYN-ﬂood attack. To
counter this type of abuse, Mux classiﬁes ﬂows into trusted ﬂows
and untrusted ﬂows. A trusted ﬂow is one for which the Mux has
seen more than one packet. These ﬂows have a longer idle time-
out. Untrusted ﬂows are the ones for which the Mux has seen only
one packet. These ﬂows have a much shorter idle timeout. Trusted
and untrusted ﬂows are maintained in two separate queues and they
have different memory quotas as well. Once a Mux has exhausted
its memory quota, it stops creating new ﬂow states and falls back to
lookup in the mapping entry. This allows even an overloaded Mux
to maintain VIP availability with a slightly degraded service.
3.3.4 Handling Mux Pool Changes
When a Mux in a Mux Pool goes down, routers take it out of ro-
tation once BGP hold timer expires (we typically set hold timer to
30 seconds). When any change to the number of Muxes takes place,
ongoing connections will get redistributed among the currently live
Muxes based on the router’s ECMP implementation. When this
happens, connections that relied on the ﬂow state on another Mux
may now get misdirected to a wrong DIP if there has been a change
in the mapping entry since the connection started. We have de-
signed a mechanism to deal with this by replicating ﬂow state on
two Muxes using a DHT. The description of that design is outside
the scope of this paper as we have chosen to not implement this
mechanism yet in favor of reduced complexity and maintaining low
latency. In addition, we have found that clients easily deal with oc-
casional connectivity disruptions by retrying connections, which
happen for various other reasons as well.
3.4 Host Agent
A differentiating component of the Ananta architecture is an agent,
called Host Agent, which is present on the host partition of every
physical machine that is served by Ananta. The Host Agent is the
key to achieving DSR and SNAT across layer-2 domains. Further-
more, the Host Agent enables data plane scale by implementing
Fastpath and NAT; and control plane scale by implementing VM
health monitoring.
3.4.1 NAT for Inbound Connections
For load balanced connections, the Host Agent performs stateful
layer-4 NAT for all connections. As encapsulated packets arrive at
the Host Agent, it decapsulates them and then performs a NAT as
per the NAT rules conﬁgured by Ananta Manager. The NAT rules
describe the rewrite rules of type: (V IP, protocolv, portv) ⇒
(DIP, protocolv, portd). In this case, the Host Agent identiﬁes
packets that are destined to (V IP, protocolv, portv), rewrites the
destination address and port to (DIP, portd) and creates bi-directional
ﬂow state that is used by subsequent packets of this connection.
When a return packet for this connection is received, it does re-
verse NAT based on ﬂow state and sends the packet to the source
directly through the router, bypassing the Muxes.
Source NAT for Outbound Connections
3.4.2
For outbound connections, the Host Agent does the following.
It holds the ﬁrst packet of a ﬂow in a queue and sends a mes-
sage to Ananta Manager requesting a VIP and port for this con-
nection. Ananta Manager responds with a VIP and port and the
Host Agent NATs all pending connections to different destinations
using this VIP and port. Any new connections to different destina-
tions (remoteaddress, remoteport) can also reuse the same port
as the TCP ﬁve-tuple will still be unique. We call this technique
port reuse. AM may return multiple ports in response to a single
request. The HA uses these ports for any subsequent connections.
Any unused ports are returned back after a conﬁgurable idle time-
out. If the HA keeps getting new connections, these ports are never
returned back to AM, however, AM may force HA to release them
at any time. Based on our production workload, we have made a
number of optimizations to minimize the number of SNAT requests
a Host Agent needs to send, including preallocation of ports. These
and other optimizations are discussed later in this paper.
3.4.3 DIP Health Monitoring
Ananta is responsible for monitoring the health of DIPs that are
behind each VIP endpoint and take unhealthy DIPs out of rotation.
DIP health check rules are speciﬁed as part of VIP Conﬁguration.
On ﬁrst look, it would seem natural to run health monitoring on the
Mux nodes so that health monitoring trafﬁc would take the exact
same network path as the actual data trafﬁc. However, it would put
additional load on each Mux, could result in a different health state
on each mux and would incur additional monitoring load on the
DIPs as the number of Muxes can be large. Guided by our principle
of ofﬂoading to end systems, we chose to implement health mon-
itoring on the Host Agents. A Host Agent monitors the health of
local VMs and communicates any changes in health to AM, which
then relays these messages to all Muxes in the Mux Pool. Perhaps
surprising to some readers, running health monitoring on the host
makes it easy to protect monitoring endpoints against unwarranted
trafﬁc – an agent in the guest VM learns the host VM’s IP address
via DHCP and conﬁgures a ﬁrewall rule to allow monitoring trafﬁc
only from the host. Since a VM’s host address does not change
(we don’t do live VM migration), migration of a VIP from one in-
stance of Ananta to another or scaling the number of Muxes does
not require reconﬁguration inside guest VMs. We believe that these
beneﬁts justify this design choice. Furthermore, in a fully managed
cloud environment such as ours, out-of-band monitoring can detect
network partitions where HA considers a VM healthy but some
Muxes are unable to communicate with its DIPs, raise an alert and
even take corrective actions.
3.5 Ananta Manager
The Ananta Manager (AM) implements the control plane of Ananta.
It exposes an API to conﬁgure VIPs for load balancing and SNAT.
Based on the VIP Conﬁguration, it conﬁgures the Host Agents and
Mux Pools and monitors for any changes in DIP health. Ananta
212Manager is also responsible for keeping track of health of Muxes
and Hosts and taking appropriate actions. AM achieves high avail-
ability using the Paxos [14] distributed consensus protocol. Each
instance of Ananta runs ﬁve replicas that are placed to avoid cor-
related failures. Three replicas need to be available at any given
time to make forward progress. The AM uses Paxos to elect a
primary, which is responsible for performing all conﬁguration and
state management tasks. We now look at some key AM functions
in detail.
3.5.1 SNAT Port Management
AM also does port allocation for SNAT (§3.2.3). When an HA
makes a new port request on behalf of a DIP, AM allocates a free
port for the VIP, replicates the allocation to other AM replicas, cre-
ates a stateless VIP map entry mapping the port to the requesting
DIP, conﬁgures the entry on the Mux Pool and then sends the allo-
cation to the HA. There are two main challenges in serving SNAT
requests — latency and availability. Since SNAT request is done
on the ﬁrst packet of a connection, a delay in serving SNAT request
directly translates into latency seen by applications. Similarly, AM
downtime would result in failure of outbound connections for appli-
cations resulting in complete outage for some applications. Ananta
employs a number of techniques to reduce latency and increase
availability. First, it allocates a contiguous port range instead of
allocating one port at a time. By using ﬁxed sized port ranges, we
optimize storage and memory requirements on both the AM and the
Mux. On the Mux driver, only the start port of a range is conﬁgured
and by making the port range size a power of 2, we can efﬁciently
map a range of ports to a speciﬁc DIP. Second, it preallocates a set
of port ranges to DIPs when it ﬁrst receives a VIP conﬁguration.
Third, it tries to do demand prediction and allocates multiple port
ranges in a single request. We evaluate the effectiveness of these
techniques in §5.
3.6 Tenant Isolation
Ananta is a multi-tenant load balancer and hence tenant isola-
tion is an essential requirement. The goal of tenant isolation is to
ensure that the Quality of Service (QoS) received by one tenant is
independent of other tenants in the system. However, Ananta is
an oversubscribed resource and we do not guarantee a minimum
bandwidth or latency QoS. As such, we interpret this requirement
as follows – the total CPU, memory and bandwidth resources are
divided among all tenants based on their weights. The weights are
directly proportional to the number of VMs allocated to the tenant.
We make another simplifying assumption that trafﬁc for all VIPs
is distributed equally among all Muxes, therefore, each Mux can
independently implement tenant isolation. We ﬁnd this assumption
to hold true in our environment. If this assumption were to not hold
true in the future, a global monitor could dynamically inform each
Mux to assign different weights to different tenants. Memory fair-
ness is implemented at Muxes and AM by keeping track of ﬂow
state and enforcing limits.
3.6.1 SNAT Fairness
AM is a critical resource as it handles all SNAT port requests.
Excessive requests from one tenant should not slow down the SNAT
response time of another tenant. AM ensures this by processing re-
quests in a ﬁrst-come-ﬁrst-serve (FCFS) order and it ensures that at
any given time there is at most one outstanding request from a DIP.
If a new request is received while another request from the same
DIP is pending, the new request is dropped. This simple mecha-
nism ensures that VIPs get SNAT allocation time in proportion to
the number of VMs. Furthermore, there are limits on the number of
ports allocated and rate of allocations allowed for any given VM.
3.6.2 Packet Rate Fairness
Mux tries to ensure fairness among VIPs by allocating available
bandwidth among all active ﬂows. If a ﬂow attempts to steal more
than its fair share of bandwidth, Mux starts to drop its packets with
a probability directly proportional to the excess bandwidth it is us-
ing. While bandwidth fairness works for TCP ﬂows that are send-
ing large-sized packets, it does not work for short packets spread
across ﬂows, or ﬂows that are non-TCP (e.g., UDP) or ﬂows that
are malicious (e.g., a DDoS attack). A key characteristic of these
ﬂows is that they do not back off in response to packet drops, in
fact we sometimes see the exact opposite reaction. Since dropping
packets at the Mux is not going to help and increases the chances
of overload, our primary approach has been to build a robust detec-
tion mechanism for overload due to packet rate. Each Mux keeps
track of its top-talkers – VIPs with the highest rate of packets. Mux
continuously monitors its own network interfaces and once it de-
tects that there is packet drop due to overload, it informs AM about
the overload and the top talkers. AM then identiﬁes the topmost
top-talker as the victim of overload and withdraws that VIP from
all Muxes, thereby creating a black hole for the VIP. This ensures
that there is minimal collateral damage due to Mux overload. De-
pending on the policy for the VIP, we then route it through DoS
protection services (the details are outside the scope of this paper)
and enable it back on Ananta. We evaluate the effectiveness of
overload detection and route withdrawal in §5.
3.7 Design Alternatives
3.7.1 DNS-based Scale Out
Ananta uses BGP to achieve scale out among multiple active in-
stances of Mux. A traditional approach to scaling out middlebox
functionality is via DNS. Each instance of the middlebox device,
e.g., load balancer, is assigned a public IP address. The author-
itative DNS server is then used to distribute load among IP ad-
dresses of the instances using an algorithm like weighted round-
robin. When an instance goes down, the DNS server stops giving
out its IP address. This approach has several limitations. First, it
is harder to get good distribution of load because it is hard to pre-
dict how much load can be generated via a single DNS resolution
request. For example, load from large clients such as a megaproxy
is always sent to a single server. Second, it takes longer to take
unhealthy middlebox nodes out of rotation due to DNS caching
– many local DNS resolvers and clients violate DNS TTLs. And
third, it cannot be used for scale out of stateful middleboxes, such
as a NAT.
3.7.2 OpenFlow-based Load Balancing
An alternative to implementing Mux functionality in general-
purpose servers is to use OpenFlow-capable switches [16]. How-
ever, currently available OpenFlow devices have insufﬁcient sup-
port for general-purpose layer-4 load balancing. For example, ex-
isting OpenFlow switches only support 2000 to 4000 ﬂows, whereas
Mux needs to maintain state for millions of ﬂows. Another key
primitive lacking in existing OpenFlow hardware is tenant isola-
tion. Finally, in a pure OpenFlow-based network, we will also need
to replace BGP with centralized routing in Ananta Manager, which
has certain drawbacks as discussed in §6. Other researchers have
also attempted to build load balancers using OpenFlow switches [28],
however, these solutions do not yet meet all the requirements, e.g.,
tenant isolation and scalable source NAT across layer-2 domains.
213Figure 10: Staged event-driven (SEDA) Ananta Manager.
Ananta manager shares the same threadpool across multiple stages