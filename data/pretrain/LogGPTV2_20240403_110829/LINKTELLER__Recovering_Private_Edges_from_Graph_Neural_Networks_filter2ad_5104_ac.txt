Next, we will theoretically show that the inﬂuence value
(cid:107)iv u(cid:107) comes with a nice property for GCN models, that is,
two nodes that are at least k + 1 hops away have no inﬂuence
on each other in a k-layer graph convolutional network. We
start from the simple case of a 1-layer GCN in Proposition 1.
Proposition 1 (Inﬂuence value of a 1-layer GCN). For a 1-
layer trained GCN model with parameters W , when its input
adjacency matrix is A and feature matrix is X, when there
is no edge between node u and node v, the inﬂuence value
(cid:107)iv u(cid:107) = 0.
We omit the proof in Appendix A1 and next present a
natural extension of the above conclusion for a k-layer GCN.
Theorem 1 (Inﬂuence value for a k-layer GCN). For a k-
layer trained GCN model, when node u and node v are at
least k + 1 hops away, the inﬂuence value (cid:107)iv u(cid:107) = 0.
The complete proof is provided in Appendix A2. With the
guarantee provided by Proposition 1, we can identify the con-
nected pairs against a 1-layer GCN with high conﬁdence; the
criterion is that the pair is connected if and only if the inﬂu-
ence value is non-zero. For GCNs with more layers, though
this criterion does not directly apply, Theorem 1 can help to
rule out nodes that are k + 1 hops apart, thus eliminating
a signiﬁcant number of negative examples (i.e., unconnected
pairs). Moreover, the node pairs that are directly connected
would have higher inﬂuence values, observed by studies on
local neighborhood properties [17]. Although there is no strict
guarantee that the inﬂuence values of the connected pairs
are the largest since the values also depend on the features
and the learned weights, in practice, the learned weights will
generally display a preference for connected pairs for better
label propagation, and thus the corresponding inﬂuence values
of connected pairs are larger.
IV. COUNTERMEASURES OF LINKTELLER:
DIFFERENTIALLY PRIVATE GCN
In this section, we aim to evaluate to what extent the pro-
posed LINKTELLER in Section III-B reveals the private con-
nection information effectively through a trained GCN, as well
Algorithm 1: Link Re-identiﬁcation Attack (LINKTELLER)
Input: A set of nodes of interest V (C) ⊆ V (I); the associated node
features X; the inference API GBB(·, ·); density belief ˆk,
reweighting scale ∆
Output: a 0/1 value for each pair of nodes, indicating the
1 Function InfluenceMatrix(V (I), X, GBB(·, ·), v):
absence/presence of edge
2
3
4
5
6
1 , . . . , (1 + ∆)x(cid:62)
P = GBB(V (I), X)
X(cid:48) = [x(cid:62)
P (cid:48) = GBB(V (I), X(cid:48))
I = 1
return I
∆ (P (cid:48) − P )
v , . . . , ](cid:62)
7
8 for each node v ∈ V (C) do
9
10
11
iuv ← (cid:107)I[u, :](cid:107)
I ← InfluenceMatrix (V (I), X, GBB(·, ·), v)
for each node u ∈ V (C) do
(cid:46) The norm of the u-th row of I
12 Sort all iuv in a descending order
13 n ← |V |
14 m ← ˆk · n(n−1)
15 Assign 1 to the ﬁrst m pairs, and 0 to the remaining
2
as the sufﬁcient conditions of the attack, via considering differ-
ent countermeasure approaches. The most direct countermea-
sure or defense against such an attack would be a differentially
private GCN model. However, so far there is no existing work
directly training differentially private GCN models to our best
knowledge. As a result, we ﬁrst revisit the general framework
and principles of developing a differentially private (DP) GCN
against such edge re-identiﬁcation attacks (Section IV-A). We
then formally deﬁne the DP GCN, followed by two proposed
practical algorithms to train a DP GCN. We also discuss the
upper bound of the precision of general edge re-identiﬁcation
attacks on DP GCNs. Importantly, we point out
the
theoretical guarantee of differential privacy is insufﬁcient in
preserving both privacy and utility for a GCN. It is equally
important to empirically choose an appropriate privacy budget
to strike a better privacy-utility balance.
that
A. Overview of DP GCN Framework
In the following sections, we review the deﬁnition of edge
differential privacy for graph algorithms [19] and present two
practical DP GCN training algorithms via graph structure in-
put perturbation. Input perturbation for GCN is a non-trivial
problem since naively adding noise to the graph structure
would destroy the sparsity of the adjacency matrix. The loss
of sparsity greatly limits a GCN’s performance and increases
its memory and computation cost.
To preserve the sparsity of the adjacency matrix, we discuss
two approaches for GCN input perturbation: EDGERAND and
LAPGRAPH. EDGERAND adapts the idea in M¨ulle et al. [24]
and randomly ﬂips each entry in the adjacency matrix ac-
cording to a Bernoulli random variable. LAPGRAPH improves
upon EDGERAND by pre-calculating the original graph density
using a small privacy budget and using that density to clip the
perturbed adjacency matrix. Compared to EDGERAND, LAP-
GRAPH preserves the sparsity of the adjacency matrix under
a small privacy budget.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2009
Differentially Private GCN:
In edge differential pri-
vacy [19], two undirected graphs are said to be neighbors if
one graph can be obtained by the other by adding/removing
one edge. Deﬁnition 2 deﬁnes the neighboring relation using
the adjacency matrix representation.
Deﬁnition 2 (Neighboring relation). Let A be the set of ad-
jacency matrices of undirected graphs. Any pair of two sym-
metric matrices A, A(cid:48)
∈ A are said to be neighbors when the
graph represented by A(cid:48) could be obtained by adding/removing
one edge from graph A, denoted as A ∼ A(cid:48). Further, we
denote the differing edge as e = A ⊕ A(cid:48).
Deﬁnition 3 (ε-edge differential privacy). A mechanism M
is ε-edge differentially private if for all valid matrix A ∈ A,
and A(cid:48)
∼ A, and any subset of outputs S ⊆ Range(M), the
following holds:
(2)
Pr[M(A) ∈ S] ≤ exp(ε) · Pr[M(A
The probability Pr is taken over the randomness of M.
(cid:48)
) ∈ S].
Deﬁnition 3 formally presents the deﬁnition of ε-edge dif-
ferential privacy (ε-edge DP). It guarantees that the outputs of
a mechanism M should be indistinguishable on any pair of
neighboring input graphs differing in one edge.
Next, we apply ε-edge DP to a GCN. To protect against
link re-identiﬁcation attacks, we need to guarantee that a
GCN’s inference results do not reveal the edge information
of its input graph. Speciﬁcally, in the inductive training, the
edge information of both the training graph and the inference
graph should be protected. In addition, the privacy protection
should hold even if the attacker submits an inﬁnite number of
inference queries.
Based on the above criteria, we ﬁrst perturb the graphs
before training a GCN. Since the training and the inference
steps are both post-processing on the perturbed DP graphs,
the edge information is protected for both the training and
inference graphs. Moreover, submitting more queries would
not reveal sensitive edge information.
We present the detailed algorithm of perturbation, train-
ing, and inference in a differentially private GCN framework
in Algorithm 2 in Appendix D1. First, the adjacency matrix
A is perturbed to meet the DP guarantee. Second, a DP GCN
model is trained on a subset of training nodes in the perturbed
graph. Finally, during inference, the DP GCN model is used
to predict the labels for a subset of inference nodes in the
perturbed graph. Essentially, ε-edge DP is achieved in the step
of adjacency matrix perturbation (line 1-2), and the guaran-
tee is provided by the privacy guarantee of the perturbation
mechanism. Since the same perturbed graph is used for both
the training and inference steps, making multiple inferences
would not consume additional privacy budget.
The following theorem provides differential privacy guaran-
tee for the training procedure in Algorithm 2 for GCN models.
Theorem 2 (ε-edge differentially private GCN). The DP GCN
model trained by Algorithm 2 is ε-edge differentially private
if the perturbation mechanism Mε is ε-edge differentially pri-
vate.
We omit all proofs for the DP guarantees in Appendix B.
Next, we show that the Inference step in Algorithm 2 guar-
antees ε-edge DP for edges in both the training and testing
graph (AV (T ) and AV (I)). To prove this privacy guarantee, we
ﬁrst introduce the parallel composition property of ε-edge DP.
Lemma 1 (Parallel composition of ε-edge DP). If the per-
turbation mechanism Mε is ε-edge differentially private and
A1, A2, . . . , Am are adjacency matrices with non-overlapping
edges, the combination of Mε(A1), Mε(A2), . . . , Mε(Am) is
also ε-edge differentially private.
The following theorem guarantees differential privacy for
any inference using the DP GCN model.
Theorem 3 (ε-edge differentially private GCN inference).
The Inference step in Algorithm 2 is ε-edge differentially
private for any V (I) ⊆ V .
The above analysis of the general DP GCN framework
provides privacy guarantees for GCN models trained following
the principles in Algorithm 2. Next, we will introduce two
such concrete training mechanisms.
B. Practical DP GCN
In Algorithm 2, the perturbation step Mε takes the adjacency
matrix of the input graph and adds noise to the adjacency
matrix to guarantee ε-edge DP. In this section, we present two
practical DP mechanisms for this process.
The intuition behind perturbing the adjacency matrix is to
add enough noise in the adjacency matrix to guarantee the
indistinguishability between any pair of neighboring adjacency
matrices A and A(cid:48)—the probability of distinguishing the per-
turbed matrix A and A(cid:48) should be bounded by a small constant
ε. The smaller ε is, the stronger the privacy protection is.
In addition to the privacy requirements, the perturbed ad-
jacency matrix A(cid:48) also needs to satisfy the following two
requirements in order to be used as a training/inference graph
for DP GCN. First, for large graphs, A(cid:48) needs to preserve a
reasonable level of sparsity to avoid huge memory consump-
tion when training a GCN model. Second, each row in the
perturbed adjacency matrix A(cid:48) should represent the same node
as its corresponding row in the original adjacency matrix A.
This requirement ensures that the node features and labels can
be associated with the right graph structure information in the
perturbed adjacency matrix during training and inference.
However, the second requirement is often not satisﬁed by
prior work on DP synthetic graph generation [21], [25]–[28].
This line of work aims at generating graphs that share similar
statistics with the original graphs. Though the desired statistics
of the graphs are preserved, the nodes in the generated graph
and the original graph are intrinsically unrelated. Therefore,
the new DP graph structure cannot be connected with the node
features and labels to train a DP GCN model. More discussions
on prior works are provided in the related work section.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:20:38 UTC from IEEE Xplore.  Restrictions apply. 
2010
To satisfy the privacy and utility requirements for DP GCN,
we introduce two perturbation methods that directly add noise
to the adjacency matrix.
1) Edge Randomization (EDGERAND): We set out with a
discrete perturbation method proposed in M¨ulle et al. [24].
This algorithm was originally proposed as a pre-processing
step for DP node clustering. Since the algorithm naturally pre-
serves the sparse structure of the adjacency matrix, we adopt
it as the input perturbation algorithm for DP GCN and name
it EDGERAND. We present the algorithm for EDGERAND
in Algorithm 3 in Appendix D2. We ﬁrst randomly choose
the cells to perturb and then randomly choose the target value
from {0, 1} for each cell to be perturbed.
In EDGERAND, the level of the sparsity of the perturbed
adjacency matrix is purely determined by the sampling param-
eter s, which can be conveniently controlled to adapt to the
given privacy budget ε. The relationship between s and ε is
characterized in Theorem 4.
Theorem 4. EDGERAND guarantees ε-edge DP for ε ≥
s − 1(cid:1), s ∈ (0, 1].
ln(cid:0) 2
EDGERAND guarantees differential privacy for the per-
turbed adjacency matrix. However,
the privacy protection
comes at the cost of changing the density of the perturbed
graph. Let the density of the input graph to EDGERAND be
k, the expectation of the density of the output graph is k(cid:48) =
(1− s)k + s/2. Take ε = 1 as an example, in this case, s shall
be at least 0.5379 according to Theorem 4, and k(cid:48) is therefore
larger than 1/4. As such, when ε is small, the perturbed graph
generated by EDGERAND could have a much higher density
compared to the original one. This would increase the memory
consumption for training DP GCNs on large graphs and may
cause memory errors when the perturbed adjacency matrix
becomes too dense to ﬁt into the memory.
2) Laplace Mechanism for Graphs
(LAPGRAPH):
EDGERAND is not applicable to large graphs under small pri-
vacy budgets due to the huge memory consumption caused by
a dense adjacency matrix. Therefore, we propose LAPGRAPH
to address this problem.
The classical idea of adding Laplace noise to the private
value is also applicable to our scenario. The difference is that,
in traditional scenarios, Laplace noise is applied to entities
such as a database entry, while in our case, the private entity is
the adjacency matrix. Therefore, additional care shall be taken
to tailor the Laplace mechanism to the graph scenario.
By the deﬁnition of Laplace mechanism [18], adding a
certain amount of noise to each cell in the adjacency matrix
will lead to any two neighboring adjacency matrices being
indistinguishable. However, directly applying this mechanism
will add a huge amount of continuous noise to each cell of
the adjacency matrix, which inevitably undermines the sparse
property of the matrix. The loss of sparse property introduces
two problems: First, it drastically increases the computation
and memory cost of training a GCN. Second, adding the con-
tinuous noise in the adjacency matrix is equivalent to adding
new weighted edges between almost every pair of nodes in
the graph, which greatly impairs the utility of the adjacency
matrix and, consequently, the GCN trained on it.
To retain the sparsity, after adding noise, we only keep
the largest T cells as existing edges in the perturbed graph.
To preserve the original graph structure, we set T to be the
approximation of the number of edges in the original graph
using a small portion of the differential privacy budget. We
name the perturbation method LAPGRAPH and present the
details in Algorithm 4 in Appendix D2. The privacy guarantee
for this method is given in Theorem 5.