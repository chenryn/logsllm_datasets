15 minutes duration because they continuously invoke
NtWaitForSingleObject / NtWaitForMultiple-
Object on alertable objects (Mutexes, Events). This set con-
tains samples that use these syscalls invocation to implement
some form of stalling code and other cases where the malicious
program really needs to wait for some event (e.g., wait for tasks
performed by other threads).
The categories described above cover 93.57% of the sam-
ples that reached the 15 minutes threshold. For the remaining
6.43% we were unable to identify a common pattern. However,
we can hypothesize that this group includes some undetected
GUI samples or malware that
implementing complex and
long-lasting computations (for instance crypto loops or crypto
mining functions).
VI.
IMPACT ON MALWARE CLASSIFICATION
In the previous section we focused on the volume of
system calls and basic blocks that can be collected by running
malware samples for a given amount of time. However, the
volume alone tells only part of the story. For instance, it is
possible that while most of the data is collected over the ﬁrst
minutes of execution, these early events are mostly associated
to generic actions (e.g., loading shared libraries and testing
internet connectivity), and that the “quality” of data improves
instead over time. In this section we propose one possible
way to assess the quality of data, by measuring its impact
on the accuracy of a machine learning classiﬁer with different
execution time of the samples. The choice of the classiﬁer can
slightly change the contribution of individual features in the
classiﬁcation task. However, the overall accuracy of the applied
classiﬁer reﬂects the strength of the statistical association
between the involved features and the malware classiﬁcation
output. A higher classiﬁcation accuracy denotes that the input
features are statistically more informative with respect to the
classiﬁcation use. In our study, by analyzing the classiﬁcation
power of the features within different time windows, we can
therefore unveil the association between the length of the time
window and the amount of information useful for classiﬁcation
in each window.
A. Classiﬁer
The literature is full of ML-based approaches applied to
dynamic analysis traces. For instance, classic machine learning
models [16], [77], such as Markov chain and Support Vector
Machines, were applied on sequences of system calls derived
from dynamic analysis to capture sequential patterns of succes-
sively executed system calls. Pascanu et al. [73], inspired by
text classiﬁcation research, proposed the use of recurrent neural
networks, such as Long Short-Term Memory (LSTM) [42] and
Gated Recurrent Units (GRU) [27] for modeling system call
12
sequences. Similarly Jindal et al. [47] considered the dynamic
analysis reports containing system calls, network activities,
changes to the registry and ﬁle actions as documents to which
they applied recurrent neural networks to automatically extract
language features from the reports. The success of all these
approaches follows the same spirit: the sequential patterns of
the behavioral features, such as the executed system calls, play
an important role in characterizing malware.
Since our goal is not to design a new ML-based clas-
siﬁcation scheme, we decided to mimic the state-of-the art
sequential-mining-centric solutions in our study. At each time
step, we transformed all
the system calls (including their
parameters) available in the collected malware samples to
categorical features. Then a sequential inference model was
applied for malware classiﬁcation. Given a sequence of n
system calls x = {x0, x1, x2, ..., xn}, we used the categorical
index xi of each system call (the string with both system
call names and parameters) as the corresponding categorical
feature. The derived categorical feature vector x is used as
input to the classiﬁer. Let x(i) denote the system call feature
vector extracted from the i-th minute of the run-time of
a malware sample. Each x(i) is composed by a sequence
of system calls {xj
i} (j=1,2,3,...,t). The sequential inference
model is organized similarly as Recurrent Neural Network
[42]. It is deﬁned with a cascaded structure as follows:
hi = δ(w1 ∗ f (x(i)) + w0 ∗ hi−1) (i = 2, 3..., m)
h1 = f (x(1))
(1)
where hi denotes the latent embedding of the system call
sequences from the beginning to the i-th time step. In our
study, we simply set hi as a scalar variable and δ as the
sigmoid activation functions. f (xi) denotes the embedding
function mapping the categorical feature vector x(i) to a scalar
embedding variable. w1 and w0 are the combining coefﬁcients
concatenating the current feature embedding f (xi) and that
of the previous time window hi−1. f is chosen as Gradient
Boosted regression trees to generate the embedding of the
categorical features, as proposed by [111]. Combining the
tree-based classiﬁer and sequential
the
design of the classiﬁer is robust against the potential imbalance
of malware samples of different families. In the following
experiments, f is empirically ﬁxed to have 100 trees. Empirical
results verify the setting of the trees. The training process
was conducted recurrently to optimize a binomial classiﬁcation
objective as:
inference structure,
f∗, w∗
s(cid:88)
0, w∗
m, ys) = −yslog(hs
1 = min
f,w,h
i=1
m, ys)
(cid:96)(hs
m) − (1 − ys)log(hs
m)
(2)
(cid:96)(hs
where hs
m denotes the embedding of the system call sequence
sample xs derived at the ﬁnal time step of xs. It integrates
the information from all m time steps and uses it as the
embedding feature vector of xs. ys denotes the class label
of xs. In our study, ys = 1 indicates that xs is malicious
and vice versa. According to Eq.1. the designed temporal
model encodes the sequential pattern of the system call
sequences via recurrently updating the embedding variable
h. At each time step, the embedding variable hi integrates
the embedding feature of the system calls observed at both
the current and the last time steps. In our study, we used
Gradient Boosted trees as an economic embedding function
to avoid the highly intense computation of training LSTM
[42] and pre-training word2vec embeddings of the categorical
features as the input to LSTM [62]. Gradient Boosted trees are
designed intrinsically to handle categorical input without any
pre-trained embeddings. Moreover, cascaded Gradient Boosted
trees can be equally powerful in capturing non-linear relations
between input features and the predicted malware classes
[112]. Compared to decision trees, the gradient tree model
is known for its tolerance to the class imbalance issue. This
model is embedded with class re-weighting in its design of
loss function, which balances the impact of misclassiﬁcation
penalty between positive and negative classes. Furthermore,
we deploy in the experimental study a cascaded boosted tree
model, in which each layer corresponds to a time window of
1 min. We extend the cascaded model to cover continuously
longer run-time periods, as reported by Table IV and Table V.
Note that our goal is to understand which minute during
the dynamic analysis provides the best information. Therefore,
we do not claim either to outperform the existing techniques
proposed to date, or that our machine learning technique is the
most appropriate for this problem.
B. Experiments
As presented earlier, a large number of both benign and
malicious samples run for less than 5 minutes. To prevent the
designed classiﬁer from overﬁtting due to early stop of the
samples, we decided to work only with the samples that run
for at least 5 minutes, and used the ﬁrst 5 minutes to understand
whether with longer analysis time more informative behaviours
are observed.
By converting the sequence of system calls into the cate-
gorical features, we obtained a richer (i.e., a bigger number of
unique system calls with parameters) system call collections.
However, we would like to stress that richer collections of sys-
tem calls do not necessarily indicate more useful information
for the classiﬁcation. Instead, encoding irrelevant system calls
into the feature set can dilute the discriminating power of the
features.
We organized two experimental tests. At ﬁrst, we divided
the whole run-time period into successive and non-overlapped
chunks of one minute. In this way, we evaluated the malware
classiﬁcation accuracy using the system-calls observed within
the increasingly larger time windows of the ﬁrst 1, 2, 3, 4
and 5 minutes separately, as shown in Table IV. After that, in
Table V, we further applied the classiﬁer with 100 trees on
the system call collections observed with the ﬁrst, the second,
the third, the forth and the ﬁfth minute. The variation of the
classiﬁcation performances obtained in each of the windows
is used to justify which time period provides the most useful
information for classifying malware samples.
Our dataset is composed by 1500 benign samples that meet
our criteria, to which we added 6K malware samples chosen
randomly among those that run for more than ﬁve minutes. In
both of the experiments (in Table IV and Table V), for each
length setting of the time window, we conducted 10-fold cross
validation and computed averaged ROC-AUC scores (Area-
Under-Curve score of Receiver Operating Curve) and PR-AUC
scores (Area-Under-Curve scores of Precision-Recall Curve).
13
TABLE IV: Classiﬁcation accuracy from the ﬁrst K-minutes
Time window length
First minute
First two minutes
First three minutes
First four minutes
First ﬁve minutes
First ten minutes
ROC-AUC score
PR-AUC score
0.968
0.967
0.965
0.965
0.965
0.966
0.950
0.955
0.953
0.953
0.953
0.953
TABLE V: Classiﬁcation accuracy for each minute
Minute
First minute
Second minute
Third minute
Fourth minute
Fifth minute
ROC-AUC score
PR-AUC score
0.968
0.910
0.907
0.914
0.910
0.950
0.900
0.889
0.894
0.901
The ROC Curve summarises the trade-off between the true
positive rate and false positive rate for a model using different
thresholds over the probabilistic output of the classiﬁer. The
Precision-Recall curve is used as an alternative yet popular
accuracy measurement in information retrieval [79]. The PR
curve is usually used to complement the ROC, especially in the
case where testing samples are imbalanced. In each round of
the 10-fold cross validation, we randomly split the whole data
set. 80% of the data instances are used to train the classiﬁer.
The remaining 20% of the data instances are used to evaluate
the classiﬁcation accuracy of the built classiﬁer. The sampling
strategy was designed to ensure that the samples selected as
part of both the training and the testing set covered the same
set of malware families. The purpose is to guarantee a fair
evaluation of classiﬁcation performances. By computing the
average accuracy obtained by across the validation tests we
remove the impact of potential bias / artifacts introduced by
the training-testing data split.
The classiﬁcation performances obtained by using different
time windows are provided in Table IV. In contrast, Table V
provides the classiﬁcation performance measurement obtained
by using only the system calls observed within the ﬁrst, second,
third, fourth and ﬁfth minute separately. As shown in Table IV,
the best classiﬁcation accuracy is observed by using the data
collected during the ﬁrst one and two minutes. By further
extending the length of the time window to ﬁve minutes, the
classiﬁcation accuracy decreases slightly and then stabilizes
quickly.
In addition to these tests on the ﬁrst ﬁve minutes, we also
extended our experiments to compute the AUC scores of the
time window covering the ﬁrst ten minutes, by following the
given cross-validation test protocols. As observed in the last
row of Table IV, the ROC-AUC and PR-AUC scores of the
ﬁrst ten minutes are 0.966 and 0.953 respectively (thus slightly
worse than by using only the ﬁrst two minutes). This conﬁrms
our intuition and our previous observations: longer time peri-
ods do not result in better classiﬁcation performances. In fact,
a time window with longer temporal coverage enables us to
collect more system call observations in the dynamic analysis
report. Nevertheless, many of those system calls recorded over
the later windows already appear in the ﬁrst two minutes, as we
unveil by Figure 5 and Figure 6 in Section V. These recurrent
system calls don’t bring signiﬁcantly more useful information
for the classiﬁcation stage, and even cause a small accuracy
deterioration of the ML model. In fact, it is a well-known issue
of machine learning-based classiﬁers that irrelevant features
can dilute the descriptive power of the feature representation
and decrease the resulting classiﬁcation accuracy [33]. The
results given in Table V conﬁrm our conclusion. The system
calls collected during the ﬁrst minute provide the most useful
information about the behavioural proﬁles of the samples. In
contrast, The AUC scores derived using only the system calls
collected in the second, the third, the forth and the ﬁfth minute
are consistently lower than that of the ﬁrst minute. This denotes
that extending the execution time window does not consistently
introduce as useful features as those collected during the ﬁrst
minute. Table IV shows that combining the actions observed in
the second minute can still help to characterize the behaviours
of the samples with the ML models. Longer time windows
don’t introduce signiﬁcant improvements of the ML model’s
performance.
VII. DISCUSSION
In the previous sections we showed the results we obtained
from our measurement study. We now want to put these results
into context and discuss how they inﬂuence previous dynamic
malware analysis experiments conducted by other researchers.
We also provide recommendations for future dynamic malware
analysis experiments and discuss possible limitations of our
study.
A. Threat to Validity and Limitations
Just as every other malware analysis experiment, also our
work suffers from common limitations related to the nature of