under different server-side DNN models: FasterRCNN-ResNet101
(44%) and FasterRCNN-ResNet50 [20] (54%) has the same architec-
ture but different feature extractors, while Yolo [66] (51%) uses a
different architecture and feature extractor (¬ß3.2). This implies that
the benefit of DDS is agnostic to the server-side DNN architecture.
We leave a full examination of different DNN architectures (e.g.,
MaskRCNN [39]) to future work.
5.4 Sensitivity to network settings
Accuracy vs. available bandwidth: We then vary the available
bandwidth and compare DDS with AWStream, which is performance-
wise the closest baseline. Figure 14 shows that given different avail-
able bandwidth, DDS can adapt its configurations to cope with
(a) Bandwidth vs. streaming delay
(b) Latency vs. streaming delay
Figure 15: The response delay of AWStream, camera-only approach
and DDS with respect to different network bandwidth and network
latency. DDS is more sensitive to network latency.
them while achieving higher accuracy. We notice that the accuracy
of semantic segmentation is lower than that of object detection
(i.e., the segmentation model is more sensitive to quality degrada-
tion). We speculate that this is because the segmentation accuracy
is highly sensitive to the pixels around the object edges, which tend
to be modified with slight quality degradation (this effect seems
more pronounced when small objects are close to each other).
Impact of network bandwidth and latency: Figure 15a shows
the impact of varying network bandwidth (while keeping the net-
work latency at 10ms) on the streaming delay of DDS and AW-
Stream. We ensure that the accuracy of DDS is always higher than
the accuracy of AWStream. We use Linux netem to vary the net-
work bandwidth and latency. We see that DDS has lower streaming
delay (i.e., less time is spent on the network) than AWStream. This
is because DDS sends less data over the network than AWStream
and many objects/pixels need only one iteration to be detected and
classified. Similarly, Figure 15b shows the impact of network latency
(while keeping the bandwidth at 500kbps) on the streaming delay of
DDS and AWStream. We see that when latency is over a threshold
(‚àº90ms in this experiment), DDS has higher streaming delay than
AWStream. This is because for those objects detected in Stream B,
they experience two iterations, which makes the streaming delay
of DDS more sensitive to long network latency than AWStream.
Impact of bandwidth variance: Figure 16 compares DDS with
AWStream under an increasing bandwidth variance. We use syn-
thetic network bandwidth traces to evaluate the impact of band-
width variance in a controlled manner. The available bandwidth of
this trace is drawn from a normal distribution of 900¬∑ ùëÅ (1, ùúé2)Kbps
while we increase ùúé from 0.1 to 0.9. We observe that DDS maintains
a higher accuracy than AWStream. Although DDS and AWStream
use the same bandwidth estimator (average of the last two seg-
ments), DDS uses the available bandwidth more efficiently because
DDS‚Äôs feedback control system continually adapts the model con-
figuration parameters to bandwidth. Thus, DDS adaptively selects
the best possible configuration parameters at each time instant.
566
BetterBetterBetter375400450500Bandwidth (kbps)0123Streaming delay (s)DDSAWStream10456090120Network latency (ms)0123Streaming delay (s)DDSAWStreamServer-Driven Video Streaming for Deep Learning Inference
SIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
(a) Accuracy
(b) Network delay
Figure 16: DDS can handle bandwidth variance and maintain a
sizeable gain over the baseline of AWStream even under substantial
bandwidth fluctuation.
(a) Object detection
(b) Semantic segmentation
Figure 18: Sensitivity analysis of DDS‚Äôs parameters. By varying these
parameters, DDS can flexibly adapt itself to reach desirable accuracy
for a given bandwidth constraint.
(a) Server GPU
(b) Server CPU
(c) Client CPU
Figure 19: Compared to prior solutions, DDS has low additional
systems overhead on both client and server.
Figure 20: DDS can handle server disconnection (or server failure)
gracefully by falling back to client-side logic
CPU (the CPU usage may exceed 100% since it may leverage more
than one CPU cores), server-side CPU and server-side GPU. This
is because DDS invokes extra encoding, decoding and inference
costs in Stream B. However, the profiling cost of AWStream is
a substantial server-side CPU and GPU costs. We estimate it by
profiling 30 configurations (compared to 216 claimed in [78]) over
a 10-second video every 4 minutes (which is much less often than
profiling a 30-second video every 2 minutes as used in [78]), the
server-side CPU and GPU costs of AWStream have already become
higher than DDS. That said, we acknowledge that if AWStream
updates the profile less frequently (e.g., every tens of minutes),
its GPU usage could be lower than DDS, but that might cause its
profile to be out of date and less accurate. On the server side, both
Vigil and Glimpse incur minimal CPU overheads (since they do not
need to decode the video) and much less GPU overheads than DDS
and AWStream (since their camera-side logics reduce the need for
server-side inference).
Fault tolerance: We stress test DDS with temporary server-side
disconnection. By default, the camera runs DDS protocol, and it also
has a local object tracking algorithm as a backup. Figure 20 shows
the time-series of response delay and accuracy. First, DDS maintains
a desirable accuracy, but at ùë° = 5 second, the server is disconnected.
We see that DDS waits for a short time (until server times out at
ùë° = 5.5) and falls back to tracking the last detection results from
the server DNN. This allows for a graceful degradation in accuracy,
rather than crashing or delaying the inference indefinitely. Between
the server disconnection and the timeout, the segments will be
placed in a queue, and when the local inference begins, the queue
(a) Network trace 1
(b) Network trace 2
Figure 17: Impact of available bandwidth on performance under
re-scaled real network trace.
Even when the variance in available bandwidth is high (ùúé > 0.7),
DDS maintains a relatively low response delay while AWStream‚Äôs
delay increases.
Under real network traces: Next, Figure 17 evaluates DDS against
AWStream on two real network traces [1]. Since the available band-
width in the traces on average exceeds the bandwidth needed to
stream the original video, we stress-test DDS by scaling the available
bandwidth by a constant factor to mimic settings where multiple
cameras share the bottleneck bandwidth (TCP-induced variances
are ignored). In particular, we scale the average bandwidth of trace
to 1,100kbps and 600kbps, while retaining the relative bandwidth
variance in the trace. We can see DDS consistently achieves higher
accuracy under different mean available bandwidth.
Impact of parameter settings: Figure 18 shows the impact of key
parameters of DDS on its accuracy/bandwidth tradeoffs: the QP in
Stream A (‚Äúlow QP‚Äù), the QP in Stream B (‚Äúhigh QP‚Äù), the objectness
threshold, and the number of feedback regions (i.e., the ùëò introduced
in ¬ß3.2). We vary one parameter at a time and test them on the
same set of traffic videos. The figures show that by varying these
parameters, we can flexibly trade accuracy for bandwidth usage.
Overall, their effect roughly falls on the same Pareto boundary,
so there may not be significant difference between the choices of
parameters to vary when coping with bandwidth fluctuation.
5.5 System microbenchmarks
Camera-side and server-side overheads: Figure 19 compares
the systems overheads of DDS with the baselines. We benchmark
their performance on our server, with one RTX 2080 Super and
one 16-core Intel Xeon Silver 4100. We scale the CPU and GPU
usage by normalizing their runtime (e.g., 2x runtime on the same
number of fully used CPUs will be translated to 2x more CPU usage).
Figure 19 shows that compared to AWStream (when the profiling
cost is excluded), DDS has 2x more overheads at both client-side
567
0.1000.1250.1500.1750.2000.2250.2500.275Normalized bandwidth consumption0.650.700.750.800.85Accuracyobjectness thresh.low QPhigh QP0.400.450.500.550.600.650.700.75Normalized bandwidth consumption0.650.700.750.80Accuracy# of feedback regionslow QP050100150GPU usage (%)AWStream profileAWStream w/o profileDDSGlimpseVigil0200400600800CPU usage (%)DDSAWStream w/o profileAWStream profile02004006008001000CPU usage (%)AWStreamDDSGlimpseVigilSIGCOMM ‚Äô20, August 10‚Äì14, 2020, Virtual Event, USA
K. Du, A. Pervaiz, X. Yuan, A. Chowdhery, Q. Zhang, H. Hoffmann, J. Jiang
closest efforts to DDS are scalable video coding [62] and region-
of-interest (ROI) encoding [59]. However, these approaches opti-
mize human-perceived quality. Scalable video coding can utilize
the bandwidth more efficiently than traditional encoding methods,
but it still compresses video uniformly in its entirety. ROI encod-
ing requires the viewer to specify the region of interest, and a
recent proposal [54] uses the region-proposal network (RPN) to
generate ROI regions. Much work on adaptive bitrate streaming
(e.g., [35, 46, 50, 70]) has focused on adapting the bitrate of pre-
coded video chunks to bandwidth fluctuation, but less on adapting
encoding to the dynamic video content as DDS does.
7 LIMITATIONS AND DISCUSSION
Strict server-side resource budget: In some sense, DDS reduces
bandwidth usage at the expense of relying on server-side DNN to
run inference more than once per frame. Similar tradeoffs can be
found in AWStream, which triggers costly reprofiling periodically,
and CloudSeg, which enforces an upfront super-resolution model
customization process. All these techniques may not be directly
applicable where server resources have strict budgets or GPU cost
is proportional to its usage (e.g., cloud instances).
Implication to privacy: Privacy is an emerging concern in video
analytics [75]. While DDS does not explicitly preserve privacy, it
is amenable to privacy-preserving techniques. Since DDS does not
send out full resolution video, it could be repurposed to denature
videos before sending only a part of the video to the server.
Edge AI accelerators: Though DDS makes minimal assumption
on camera‚Äôs local computation capacity, it benefits from the trend
of more accelerators being added on edge devices, by using camera-
side heuristics as described in ¬ß4.2. We also envision DDS running
alongside the camera local analytics to share the workload and pro-
vide higher inference accuracy with minimal bandwidth overheads.
8 CONCLUSION
Video streaming has been a driving application of networking re-
search. This work argues that the emerging AI applications inspire a
paradigm shift away from the traditional source-driven approach to
video streaming. We have developed a concrete DNN-driven design,
called DDS, that exploits opportunities unique to deep learning ap-
plications: (1) unlike user QoE, video inference accuracy depends
less on pixels than on what is in the video, and (2) deep learning
models offers extra information that helps us decide how video
should be encoded/streamed. We believe that the development of
such video streaming protocols will significantly impact not only
video analytics, but also the future analytics stack of many dis-
tributed AI applications.
ACKNOWLEDGMENTS
We thank the anonymous reviewers and our shepherd Paolo Costa.
In this project, Junchen Jiang and Kuntai Du are supported by NSF
(CNS-1901466). Junchen Jiang is also supported by Google Faculty
Research Award. Moreover, Ahsan Pervaiz and Henry Hoffmann
are supported by NSF (CCF-1439156, CNS-1526304, CCF-1823032,
CNS-1764039), ARO (W911NF1920321), DOE (DESC0014195 0003),
and DARPA BRASS program.
(a) Reducing bandwidth usage by
smarter encoding
Figure 21: System refinements to (a) reduce Stream B bandwidth and
(b) reduce response delay (both of them are introduced in ¬ß4.2).
(b) Reducing response delay by early
reporting
will be gradually cleaned up. When the server is back online (at
ùë° = 13), the camera will be notified with at most a segment-worth
of delay (0.5 second), and begin to use the regular DDS protocol
to resume video analytics. Meanwhile the camera will continue to
send a liveness probe every video segment.
Performance optimization: Figure 21 examines two performance
refinements. First, figure 21(a) shows that (1) putting the proposed
regions on a black background frame yields about 2√ó bandwidth
savings over encoding each region separately; and (2) compressing
these frames in an mp4-format leads to another 10√ó bandwidth
savings. Second, figure 21(b) shows that returning the first-iteration
output (i.e., the high-confidence results in Stream A before Stream
B starts), we reduce the average response delay by about ‚àº 40%.
6 RELATED WORK
We discuss the most closely related work in three categories.
Video analytics systems: The need to scale video analytics has
sparked much systems research: DNN sharing (e.g., [42, 44]), re-
source allocation (e.g., [52, 79]), vision model cascades (e.g., [48, 71]),
efficient execution frameworks (e.g., [51, 58, 64]), as well as cam-
era/edge/cloud collaboration (e.g., [30, 32, 54, 63, 72, 76, 78, 80],
see ¬ß2.3 for a detailed discussion) or multi-camera collaboration
(e.g., [43, 45]). The most related work to DDS is AWStream [78]
which shares with DDS the ethos of using a server DNN-generated
profile. The key distinction is that such feedback is not real-time
video content, so it cannot zoom in on specific regions on the cur-
rent frames. Vigil [80] sends cropped regions, but it is bottlenecked
by the camera computing power (See ¬ß2.3). DDS shares the concept
of server-driven streaming with its own preliminary design [63]
and the partially server-driven solutions [30, 54]. But it is the first
solution that achieves high accuracy (by correcting objects mis-
labeled and missed by the low-quality video or the camera-side
model) in multiple vision tasks and DNN models, and fully utilizes
the video streaming codec to minimize bandwidth usage.
Vision applications: Computer vision and deep learning have a
substantial body of research (e.g., [34, 47, 56, 65, 67, 68, 77]). Recent
works on video object detection show it is inefficient to apply object
detection DNN frame by frame; instead it should be augmented
by tracking [41] (similar to ¬ß4.2) or by a temporal model such as
LSTM [53, 55]. This work complements DDS by designing new,
server-side deep learning models. DDS‚Äôs distinctive advantage is
that it explicitly optimizes the bandwidth/accuracy tradeoffs in a
way that is largely agnostic to the server-side DNN.
Internet video encoding/streaming: Recent innovations in video
encoding have provided better compression gains (e.g., [33]). The
568
02000400060008000Bandwidth  Consumption (Kbps)Seperate regionsBlack-bkgd framesBlack-bkgd video0.00.51.01.52.0Response delay (s)DDS w/o early reportingDDS w/ early reportingAWStreamServer-Driven Video Streaming for Deep Learning Inference