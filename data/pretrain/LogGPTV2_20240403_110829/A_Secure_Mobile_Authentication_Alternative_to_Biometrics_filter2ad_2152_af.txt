further show that ai.lock is also a δ -LSIM function when its hash
value is a single bit.
We set δ = τ , where τ is the error tolerance threshold obtained
from the ai.lock training process (see Table 3), for diﬀerent values
of λ. Table 7 shows the P1 and P2 values achieved by the basic
ai.lock over the holdout dataset. We perform Mann-Whitney one-
sided test with alternative hypothesis P1 > P2. Based on the ob-
served p − value = 0.00, (α = 0.05), for diﬀerent values of λ, we
conclude that the alternative hypothesis is true, hence, ai.lock is a
δ -LSIM function over the holdout dataset.
10
λ
150
250
350
500
DI2E module (Inception v.1)
DI2E module (Inception v.3)
PCA + LSH module
0.7
1.9
0.044
0.7
1.9
0.049
0.7
1.9
0.051
0.7
1.9
0.066
Table 8: Processing time (in seconds) of SLSS ai.lock mod-
ules, for diﬀerent values of λ. When using Inception.h5, the
overall ai.lock speed is below 0.8s.
7.5 On the Entropy of Imageprints
We have used the entropy evaluation dataset (see § 6.2) to empir-
ically calculate the entropy of the imageprints generated by the
ai.lock variants. Œe empirical entropy of an authentication solu-
tion is propositional to the size of the keyspace that the aŠacker
needs to search to ﬁnd a match for the authentication secret. For
biometric information, estimating this size is diﬃcult. In such cases,
1
the entropy can be estimated as −lo❕2(
F AR ) [46]. We performed
this study for diﬀerent values of λ and the best parameter choice
of ai.lock (see § 7.1), using the entropy evaluation dataset.
On the 2 billion image pairs in the entropy evaluation dataset,
the FAR of the SLSS ai.lock variant is 0.020% and 0.035% when λ
is 50 and 500 respectively, for an entropy of 12.28 bits and 11.48
bits. We have visually inspected several hundreds of image pairs
that resulted in false accepts and observed that a signiﬁcant propor-
tion were due to images that contained the same object type, e.g.
ribbons, helmets, etc. Œis result is not unexpected: the SLSS vari-
ant uses only the last hidden layer of Inception.v3 network. Since
Inception.v3 is trained for image classiﬁcation task, it is expected
to have similar activations on the last hidden layer for images of
the same object type. We expect to eliminate this situation by re-
quiring the match between activations of multiple inception layers
(multi layer variant).
Œe FAR of the MLMS ai.lock variant on the entropy evaluation
dataset, for λ values of 500 and 150, is 0.0007% and 0.0004% respec-
tively. Œerefore, the estimated entropy of ai.lock imageprints is
17.14 and 18.02 bits respectively.
7.6 ai.lock Speed
We have implemented ai.lock using Android 7.1.1 and Tensorﬂow
0.12.1 and have evaluated its speed using 1,000 images of the Nexus
dataset on a Nexus 6P smartphone (‹alcomm Snapdragon 810
CPU and 3GB RAM). Table 8 shows the average processing time
of the 3 main ai.lock modules for diﬀerent values of λ. Indepen-
dent of the value for λ, ai.lock’s DI2E module takes 1.9s to com-
pute the activations of all the layers of Inception.v3. When using
Inception.h5 [63] (a smaller network), DI2E module takes 0.7s. Œe
combined PCA and LSH speed increases with the value of λ, but
is below 70ms for λ = 500. Œe processing overhead of ai.lock is
below 2s and 1s using Inception.v3 and Inception.h5 respectively.
To minimize its impact on user experience on a Nexus 6P, ai.lock
needs to use Inception.h5. Œe most signiﬁcant processing over-
head of ai.lock is on computing the activation of the DNN, which
directly depends on the size of the network. Note that compress-
ing the network using the DNN distillation approach [25] can al-
leviate this overhead. Future device and Inception improvements
will likely improve the ai.lock performance and accuracy.
8 DISCUSSION AND LIMITATIONS
Default authentication, revocation and recovery. If the image
based authentication fails a number of times or the ai.lock secret
is not available, the authentication falls back to the default authen-
tication mechanism, e.g. text passwords.
Strong passwords. ai.lock beneﬁts from users choosing strong,
high-entropy and unique objects for authentication. ai.lock can
use datasets of images of frequently occurring, thus low entropy,
objects and learn to reject similar objects during their registration
by the user. Further, the image classiﬁcation task can be adapted to
detect images belonging to classes of weak, low-entropy authenti-
cation objects. In addition, similar to text passwords, users could
be encouraged to pick an ordered combination of personal objects
for authentication.
Usability. Although usability is not the focus of this paper, we ex-
pect ai.lock to share several limitations with face based authentica-
tion mechanisms due to their similarities in the form factor. Œese
include susceptibility to inappropriate lighting conditions [9]. While
the FAR of ai.lock remains small under illumination changes, its
FRR increases, aﬀecting its usability. However, DNNs are capable
of learning representations that are invariant to input changes, e.g.
lighting, translation, etc. Œus, the DI2E module of ai.lock can be
further ﬁne-tuned to be more resistant to illumination changes. We
leave the investigation of such improvement for future work.
In [6] we have evaluated the usability aspects of an image based
authentication approach, and have shown that (1) the user entry
time was signiﬁcantly shorter compared to text passwords on a
mobile device, (2) the participants were able to remember their au-
thentication objects 2 and 7 days a‰er registering them, and (3) the
participants perceived object based authentication to be easier to
use than text passwords, and were willing to adopt it. Further stud-
ies are required to understand (1) the user choice of the secret ob-
jects or scenes and whether it impacts the secret key space, (2) the
ability of ai.lock to ﬁlter out common or low-entropy images, (3)
the scenarios where users are willing to adopt ai.lock authentica-
tion and (4) other limitations associated to ai.lock authentication.
Shoulder surﬁng. Similar to face based authentication, ai.lock
is vulnerable to shoulder surﬁng aŠacks where the adversary cap-
tures images of the objects or scenes used by victims. However,
ai.lock eliminates remote aŠacks, e.g., [51], moves the target away
from sensitive body features, and enables users to trivially change
their image-passwords. Similar to biometrics, ai.lock can also ben-
eﬁt from liveness veriﬁcation techniques [55], that ensure that the
adversary has physical access to the authentication object or scene,
to prevent sophisticated image replay aŠacks. In addition, in Ap-
pendix B we show that the knowledge of the authentication ob-
ject type does not provide the adversary with signiﬁcant advantage
when launching a brute force aŠack.
Multi-factor authentication. ai.lock can also be used in conjunc-
tion with other authentication solutions. For instance, the image
password set and authentication steps described in § 4.1 can take
advantage of a secondary secret (e.g. password, PIN), increasing
the number of authentication factors to improve security. To this
end, let r be a random salt. We modify x in the fuzzy biometric pro-
tection solution outlined in § 4.1 to be the randomized hash of the
secondary secret computed using salt r . Randomized hashing en-
sures the required formaŠing and bit length for x can be achieved
using key derivation function (e.g. HKDF [36]), etc. Œe random
salt r needs to be stored along with the other authentication cre-
dentials, i.e. SS(R, x).
Compromised device. Our model assumes an adversary that
physically captures a victimﬁs device and thus has black-box ac-
cess to the authentication function. ai.lock is not resilient to an
adversary who installs malware on the victim device. Such mal-
ware may for instance leverage PlaceRaider [66] to construct three
dimensional models of the environment surrounding the victim,
including of the authentication object.
Trusted hardware can secure ai.lock and even obviate the need
for secure sketches. However, it would reduce the number of de-
vices where ai.lock can be applied. Techniques similar to AuDroid [50]
could be employed to ensure that unauthorized processes or exter-
nal parties cannot access and misuse the device camera, however,
they may still leave ai.lock vulnerable to cache aŠacks [38].
9 CONCLUSIONS
In this paper, we introduced ai.lock, the ﬁrst secure and eﬃcient
image based authentication system. We have presented a suite of
practical yet powerful image based aŠacks and built large scale
aŠack datasets. We have shown that even under our powerful at-
tacks, ai.lock achieves beŠer entropy than state-of-the-art biomet-
ric authentication solutions.
10 ACKNOWLEDGMENTS
We thank the shepherd and reviewers for their excellent feedback.
Œis research was supported in part by grants from the NSF (CNS-
1526494, CNS-1527153 and SES-1450619) and the Florida Center
for Cybersecurity.
REFERENCES
[1] 2015. Replacing RSA SecurID: Why Are Customers Switching to Duo Secu-
rity? (2015). hŠps://duo.com/blog/replacing-rsa-securid-why-are-customers-
switching-to-duo-security.
[2] 2017.
TAPS - Make touchscreen gloves using a Sticker w/ Touch ID.
hŠps://www.kickstarter.com/projects/nanotips/taps-touchscreen-
(2017).
sticker-w-touch-id-ships-before-x?token=5b586aa6.
[3] Mart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeﬀrey
Dean, MaŠhieu Devin, Sanjay Ghemawat, Geoﬀrey Irving, Michael Isard,
Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek G.
Murray, Benoit Steiner, Paul Tucker, Vijay Vasudevan, Pete Warden, Martin
Wicke, Yuan Yu, and Xiaoqiang Zheng. 2016. TensorFlow: A System for
Large-Scale Machine Learning. In 12th USENIX Symposium on Operating Sys-
tems Design and Implementation (OSDI 16). USENIX Association, GA, 265–283.
hŠps://www.usenix.org/conference/osdi16/technical-sessions/presentation/abadi
[4] Apple Support 2017. About Touch ID security on iPhone and iPad.
(2017).
hŠps://support.apple.com/en-us/HT204587.
[5] Sunpreet S Arora, Kai Cao, Anil K Jain, and Nicholas G Paulter. 2014. 3D Fin-
gerprint Phantoms. In 2014 22nd International Conference on Paˆern Recognition.
684–689. DOI:hŠps://doi.org/10.1109/ICPR.2014.128
[6] Mozhgan Azimpourkivi, Umut Topkara, and Bogdan Carbunar. 2017. Camera
Based Two Factor Authentication Œrough Mobile and Wearable Devices. In
ACM International Joint Conference on Pervasive and Ubiquitous Computing.
[7] Yu Bai, Li Zhuo, Bo Cheng, and Yuan Fan Peng. 2014. Surf feature extraction
in encrypted domain. In 2014 IEEE International Conference on Multimedia and
Expo (ICME). 1–6. DOI:hŠps://doi.org/10.1109/ICME.2014.6890170
[8] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. 2006. SURF: Speeded Up Ro-
bust Features. Springer Berlin Heidelberg, Berlin, Heidelberg, 404–417. DOI:
hŠps://doi.org/10.1007/11744023 32
[9] Chandrasekhar Bhagavatula, Blase Ur, Kevin Iacovino, Su Mon Kywe, Lor-
rie Faith Cranor, and Marios Savvides. 2015. Biometric authentication on iphone
11
and android: Usability, perceptions, and inﬂuences on adoption. Proceeding of
Usable Security (USEC) (2015), 1–2.
[10] Joseph Bonneau. 2012. Œe Science of Guessing: Analyzing an Anonymized
Corpus of 70 Million Passwords. In 2012 IEEE Symposium on Security and Privacy.
538–552. DOI:hŠps://doi.org/10.1109/SP.2012.49
[11] Raj Chandra Bose and Dwijendra K Ray-Chaudhuri. 1960. On a class of error
correcting binary group codes. Information and Control 3, 1 (1960), 68 – 79. DOI:
hŠps://doi.org/10.1016/S0019-9958(60)90287-4
[12] Moses S. Charikar. 2002.
Similarity Estimation Techniques from Rounding
Algorithms. In Proceedings of the Širy-fourth Annual ACM Symposium on
Šeory of Computing (STOC ’02). ACM, New York, NY, USA, 380–388. DOI:
hŠps://doi.org/10.1145/509907.509965
[13] S. Chopra, R. Hadsell, and Y. LeCun. 2005. Learning a similarity metric discrim-
inatively, with application to face veriﬁcation. In 2005 IEEE Computer Society
Conference on Computer Vision and Paˆern Recognition (CVPR’05), Vol. 1. 539–
546 vol. 1. DOI:hŠps://doi.org/10.1109/CVPR.2005.202
[14] Ed. D. Hardt. 2012. Še OAuth 2.0 Authorization Framework. RFC 6749. IETF.
hŠp://tools.ietf.org/html/rfc6749
[15] Darren Davis, Fabian Monrose, and Michael K. Reiter. 2004. On User Choice in
Graphical Password Schemes. In Proceedings of the 13th Conference on USENIX
Security Symposium - Volume 13 (SSYM’04). USENIX Association, Berkeley, CA,
USA, 11–11. hŠp://dl.acm.org/citation.cfm?id=1251375.1251386
[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
Imagenet: A large-scale hierarchical image database. In IEEE Con-
DOI:
2009.
ference on Computer Vision and Paˆern Recognition. IEEE, 248–255.
hŠps://doi.org/10.1109/CVPR.2009.5206848
[17] Yevgeniy Dodis, Leonid Reyzin, and Adam Smith. 2004.
Fuzzy Ex-
tractors: How to Generate Strong Keys from Biometrics and Other Noisy
Data.
DOI:
hŠps://doi.org/10.1007/978-3-540-24676-3 31
Springer Berlin Heidelberg, Berlin, Heidelberg, 523–540.
[18] Jeﬀ Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoﬀman, Ning Zhang, Eric
Tzeng, and Trevor Darrell. 2014. DeCAF: A Deep Convolutional Activation Fea-
ture for Generic Visual Recognition.. In Icml, Vol. 32. 647–655.
[19] Nathaniel Wesley Filardo and Giuseppe Ateniese. 2012. High-Entropy Visual
Identiﬁcation for Touch Screen Devices. Springer Berlin Heidelberg, Berlin, Hei-
delberg, 182–198. DOI:hŠps://doi.org/10.1007/978-3-642-29101-2 13
[20] Karl Pearson F.R.S. 1901.
to systems of points
11
559–572.
arXiv:hŠp://dx.doi.org/10.1080/14786440109462720
in space.
(1901),
LIII. On lines and planes of closest ﬁt
Philosophical Magazine Series 6 2,
DOI:hŠps://doi.org/10.1080/14786440109462720
[21] Javier Galbally, Arun Ross, Marta Gomez-Barrero, Julian Fierrez, and Javier
Ortega-Garcia. 2012. From the iriscode to the iris: A new vulnerability of iris
recognition systems. Black Hat Brieﬁngs USA (2012).
[22] Jan-Mark Geusebroek, Gertjan J. Burghouts, and Arnold W.M. Smeul-
Inter-
DOI:
Œe Amsterdam Library of Object
1 (2005),
ders. 2005.
national
hŠps://doi.org/10.1023/B:VISI.0000042993.50813.60
Journal of Computer Vision 61,
103–112.
Images.
[23] Ian Goodfellow,
Sherjil Ozair, Aaron Courville,
Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David
and Yoshua Bengio.
Warde-Farley,
2014.
Informa-
tion Processing Systems 27, Z. Ghahramani, M. Welling, C. Cortes, N. D.
Lawrence, and K. Q. Weinberger (Eds.). Curran Associates, Inc., 2672–2680.
hŠp://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf
Generative Adversarial Nets.
In Advances in Neural
[24] Eiji Hayashi, Bryan Pendleton, Fatih Ozenc, and Jason Hong. 2012. WebTicket:
Account Management Using Printable Tokens. In Proceedings of the SIGCHI Con-
ference on Human Factors in Computing Systems (CHI ’12). ACM, New York, NY,
USA, 997–1006. DOI:hŠps://doi.org/10.1145/2207676.2208545
[25] Geoﬀrey Hinton, Oriol Vinyals, and Jeﬀ Dean. 2015. Distilling the knowledge
in a neural network. arXiv preprint arXiv:1503.02531 (2015).
[26] Alexis Hocquenghem. 1959. Codes correcteurs d’erreurs. Chiﬀres 2, 147-156
(1959), 8–5.
[27] Chao-Yung Hsu, Chun-Shien Lu, and Soo-Chang Pei. 2012.
ture Extraction in Encrypted Domain With Privacy-Preserving SIFT.
Transactions on Image Processing 21, 11 (Nov 2012), 4593–4607.
hŠps://doi.org/10.1109/TIP.2012.2204272
Image Fea-
IEEE
DOI:
[28] Piotr Indyk and Rajeev Motwani. 1998. Approximate Nearest Neighbors: To-
wards Removing the Curse of Dimensionality. In Proceedings of the Širtieth
Annual ACM Symposium on Šeory of Computing (STOC ’98). ACM, New York,
NY, USA, 604–613. DOI:hŠps://doi.org/10.1145/276698.276876
[29] Anil K. Jain, Karthik Nandakumar, and Abhishek Nagar. 2008. Biometric Tem-
plate Security. EURASIP J. Adv. Signal Process 2008, Article 113 (Jan. 2008),
17 pages. DOI:hŠps://doi.org/10.1155/2008/579416
[30] Jeﬀ Kent. 2017. python-bchlib. (2017). hŠps://github.com/jkent/python-bchlib/.
[31] A.
In Proceed-
DOI:
ings IEEE International Symposium on Information Šeory,. 408–.
hŠps://doi.org/10.1109/ISIT.2002.1023680
Juels and M. Sudan. 2002.
A fuzzy vault scheme.
[32] Ari Juels and Martin WaŠenberg. 1999.
In Proceedings of
nications Security (CCS ’99). ACM, New York, NY, USA, 28–36.
hŠps://doi.org/10.1145/319709.319714
A Fuzzy Commitment Scheme.
the 6th ACM Conference on Computer and Commu-
DOI:
[33] Nikolaos Karapanos, Claudio Marforio, Claudio Soriente, and Srdjan ˇCapkun.
Sound-proof: Usable Two-factor Authentication Based on Am-
2015.
bient Sound.
the 24th USENIX Conference on Security
Symposium (SEC’15). USENIX Association, Berkeley, CA, USA, 483–498.
hŠp://dl.acm.org/citation.cfm?id=2831143.2831174
In Proceedings of
[34] Œomas P Keenan. 2015. Hidden Risks of Biometric Identiﬁers and How to Avoid
Œem. BlackHat USA (2015).
[35] Klaus Kollreider, Hartwig Fronthaler, and Josef Bigun. 2008. Verifying live-
ness by multiple experts in face biometrics. In 2008 IEEE Computer Society
Conference on Computer Vision and Paˆern Recognition Workshops. 1–6. DOI:
hŠps://doi.org/10.1109/CVPRW.2008.4563115