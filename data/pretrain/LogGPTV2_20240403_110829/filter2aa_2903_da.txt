（dev->agp->base + init->ring_start）映射到线性地址。最后的map字段
是用于描述映射信息的。以下是初始化map结构体和调用内存映射函数
的代码。
dev_priv->ring.map.offset = dev->agp->base + init->ring_start;
dev_priv->ring.map.size = init->ring_size;
dev_priv->ring.map.type = _DRM_AGP;
dev_priv->ring.map.flags = 0;
dev_priv->ring.map.mtrr = 0;
drm_core_ioremap(&dev_priv->ring.map, dev);
其中，init->ring_start的值等于Start字段。
中间的head和tail字段用于描述当前正在使用的命令流，head用于描
述开头，tail用于描述结尾，二者都是相对于Start的偏移量。
最后再介绍一下space字段，它代表环形缓冲区上的空闲空间大
小，是通过以下公式计算的。
ring->space = ring->head - (ring->tail + 8);
if (ring->space space += ring->Size;
在i915驱动中，也有一个与drm_i810_ring_buffer_t类似的结构体，
名叫intel_ringbuffer，位于intel_ringbuffer.h中，由于篇幅所限，不再详
述。
11.4.3 环形缓冲区寄存器
上面介绍的数据结构是CPU端代码使用的。在GPU端，Gen以寄存
器的形式来报告环形缓冲区接口。它使用了4个寄存器，分别为：尾偏
移、头偏移、起始地址和控制寄存器。下面是i915驱动中的对应宏定
义。
#define RING_TAIL(base)        _MMIO((base)+0x30)
#define RING_HEAD(base)        _MMIO((base)+0x34)
#define RING_START(base)       _MMIO((base)+0x38)
#define RING_CTL(base)         _MMIO((base)+0x3c)
其中，base是宏的参数，目的是复用这套宏描述Gen的多个环形缓
冲区。以下是定义base的几个宏。
#define RENDER_RING_BASE       0x02000
#define BSD_RING_BASE          0x04000
#define GEN6_BSD_RING_BASE     0x12000
#define GEN8_BSD2_RING_BASE    0x1c000
#define VEBOX_RING_BASE        0x1a000
#define BLT_RING_BASE          0x22000
其中，RENDER_RING_BASE是着色器引擎的，BLT_RING_BASE
是2D位块操作的，VEBOX_RING_BASE是视频增强引擎（Video
Enhancement Engine）的，其他几个都是视频引擎的，BSD是BitStream
Decoder的缩写。
从上述定义可以看出，目前的Gen GPU支持多个环形缓冲区，每个
执行引擎都有自己的命令接收器和环形缓冲区，这样主要是了为了避免
单一的环形缓冲区成为CPU和GPU之间的瓶颈，避免千军万马过独木
桥。
11.5 逻辑环上下文和执行列表
随着GPU应用的增多，让系统中的多个应用可以“同时”使用GPU成
为一个重要目标。这里的“同时”故意加上引号，表示多个应用以分时间
片的方式轮番使用GPU，与CPU上的多任务类似，表面上看好像多个任
务同时在运行。要做到这一点，就必须让GPU也支持较低粒度的抢先式
调度，也就是当有新的重要任务要运行时，可以“立刻”打断当前的任
务，迅速切换到新的任务。
为了实现这个目标，2014年推出的Gen8引入了一种新的方式来给
GPU下达任务，称为逻辑环上下文和执行列表（Logical Ring Context
and Execlist）。简单来说，CPU端要为每个GPU任务准备一个规定格式
的结构体，称为逻辑环上下文（Logical Ring Context，LRC）。与CPU
上的线程上下文结构体类似，LRC中记录GPU任务的详细运行状态。当
需要运行一个任务时，只要把它的LRC发送到相应执行引擎的执行列表
提交端口（ExecList Submit Port，ELSP）即可。提交时会附带一个全局
唯一的提交标志（ID），用于识别这个任务。提交ID的长度是20位。
11.5.1 LRC
与CPU的线程上下文类似，LRC是GPU引擎执行状态的一份副本。
Gen有多个执行引擎，每个引擎的LRC格式是不同的。在《Gen编程手
册》的卷7（3D-Media-GPGPU Engine）中，描述了LRC的格式，其位
置为Render Command Memory Interface → Render Engine Logical Context
Data → Register/State Context。
以Gen8渲染引擎（Render Engine）的LRC为例，它包含了20个内存
页，共有80KB，分为以下三个部分。
与进程相关的（Per-Process）硬件状态页，大小为4KB。
环形缓冲区上下文，包括环形缓冲区寄存器的状态、页目录指针
等。
引擎上下文，用于记录流水线状态、非流水线状态和统计信息等。
通过i915驱动的i915_dump_lrc虚拟文件，可以让i915驱动（函数名
为i915_dump_lrc_obj）帮我们把环形缓冲区上下文对应的内存页映射到
CPU端，并显示它的内容。清单11-1是其中的一部分。软件环境为Linux
4.13内核，GPU是KabyLake（Gen9.5）。
清单11-1 环形缓冲区上下文
CONTEXT: rcs0 0
   Bound in GGTT at 0xfffe7000
   [0x0000] 0x00000000 0x1100101b 0x00002244 0xffff000a // LRI头和CTX_CTRL
   [0x0010] 0x00002034 0x00000448 0x00002030 0x00000448 // 环的头和尾
   [0x0020] 0x00002038 0x00001000 0x0000203c 0x00003001 // 环的起始和控制
   [0x0030] 0x00002168 0x00000000 0x00002140 0xfffdddd8 // BB_ADDR
   [0x0040] 0x00002110 0x00000000 0x0000211c 0x00000000 // BB_STATE
   [0x0050] 0x00002114 0x00000000 0x00002118 0x00000000 // 第二个BB
   [0x0060] 0x000021c0 0xffffe081 0x000021c4 0xffffe002 // BB_PER_CTX_PTR
   [0x0070] 0x000021c8 0x00000980 0x00000000 0x00000000
   [0x0080] 0x00000000 0x11001011 0x000023a8 0x00000293 // CTX_TIMESTAMP
   [0x0090] 0x0000228c 0x00000000 0x00002288 0x00000000 // PDP3
   [0x00a0] 0x00002284 0x00000000 0x00002280 0x00000000 // PDP2
   [0x00b0] 0x0000227c 0x00000000 0x00002278 0x00000000 // PDP1
   [0x00c0] 0x00002274 0x00000002 0x00002270 0x22844000 // PDP0
   [0x00d0] 0x00000000 0x00000000 0x00000000 0x00000000
   [0x00e0] 0x00000000 0x00000000 0x00000000 0x00000000
   [0x00f0] 0x00000000 0x00000000 0x00000000 0x00000000
   [0x0100] 0x00000000 0x11000001 0x000020c8 0x80000088
   [0x0110] 0x61040001 0x00000000 0x00000000 0x00000000 // GPGPU CSR基地址
   [0x0120] 0x00000000 0x00000000 0x00000000 0x00000000
   [0x0130] 0x00000000 0x00000000 0x00000000 0x00000000
   [0x0140] 0x00000000 0x11001057 0x00002028 0xffff0000
   [0x0150] 0x0000209c 0xfeff0000 0x000020c0 0xffff0000
   [0x0160] 0x00002178 0x00000001 0x0000217c 0x00145855
   [0x0170] 0x00002358 0x138a36f8 0x00002170 0x00000000
结合i915驱动中的intel_lrc.c和PRM，可以了解清单11-1中常用字段
的含义。其中，第一行的第一个DWORD是NOOP命令，第二个
DWORD是Load_Register_Immediate（LRI）命令的命令头，其后的若干
行都是“寄存器地址+寄存器取值”的形式。可以认为GPU在加载这个上
下文时会先执行NOOP命令，然后执行LRI命令。在执行LRI命令时便会
把随后的寄存器内容加载到寄存器中。
第2行和第3行便是环形缓冲区寄存器的信息，分别是4个寄存器
（即环的头、尾、起始和控制寄存器）的地址和取值。
随后的三行描述两套批缓冲区（BB）的状态，每套有三个寄存
器，分别是BB_ADDR_UDW（高的DWORD）、BB_ADDR（低的
DWORD）和BB_STATE。从0x90开始的4行都是关于页目录信息的，
PDP代表页目录指针（Page Directory Pointer）。如果要了解上述寄存器
的详细定义，快捷方法是打开PRM卷2的c部分（比如skl-vol02c-
commandreference-registers-part1.pdf），然后搜索寄存器的地址。
11.5.2 执行链表提交端口
等待执行的LRC一般是以链表的形式放在队列里的，这个链表有时
称为执行链表（ExecList），也叫运行链表（RunList）。Gen公开了一
个名为执行链表提交端口（ExecList Submit Port，ELSP）的寄存器，用
于接收要执行的LRC。为了提高吞吐率，Gen的4个执行引擎都有自己的
ELSP寄存器，而且每个ELSP寄存器内部都有一对端口，可以接收两个
LRC。提交的方法是把LRC的描述符写到ELSP寄存器，先写LRC 1的描
述符，再写LRC 0的描述符。
11.5.3 理解LRC的提交和执行过程
观察i915驱动的虚拟文件是理解执行链表提交过程的一种好方法。
在Ubuntu的终端窗口中先切换到su身份，然后转移到i915驱动的debugfs
文件夹，便可以使用cat命令观察了。
# sudo su
# cd /sys/kernel/debug/dri/0 
# cat i915_gem_request && cat i915_engine_info
第三条命令是关键，前一个cat显示请求队列，gem是GPU Engine
Manager的缩写，代表i915驱动中管理执行引擎的部分，后一个cat显示
执行引擎的状态。把两个cat命令连在一起提交是为了缩短二者之间的时
间差，让两条命令显示的信息尽可能接近同一时间点。
我们先解读任务较少时的简单结果，再看复杂情况。清单11-2是在
Ubuntu系统启动后没有运行其他应用软件时的结果。前两行是
i915_gem_request的内容，第3行起是i915_engine_info的内容，后者会显
示4个执行引擎的状态。为了节约篇幅，这里只截取渲染引擎的部分。
清单11-2 渲染引擎的请求队列和执行状态（简单情况）
root@gedu-i7:/sys/kernel/debug/dri/0# cat i915_gem_request && cat i915_eng
ine_info
rcs0 requests: 1
    dc9 [4:807] prio=2147483647 @ 12ms: compiz[1955]/1
GT awake? yes
Global active requests: 1
rcs0
    current seqno dc9, last dc9, hangcheck d7b [11688 ms], inflight 1
    Requests:
        first  dc9 [4:807] prio=2147483647 @ 12ms: compiz[1955]/1
        last   dc9 [4:807] prio=2147483647 @ 12ms: compiz[1955]/1
    RING_START: 0x00031000 [0x00000000]
    RING_HEAD:  0x00000620 [0x00000000]
    RING_TAIL:  0x00000620 [0x00000000]
    RING_CTL:   0x00003000 []
    ACTHD:  0x00000000_02000620
    BBADDR: 0x00000000_0b31c59c
    Execlist status: 0x00000301 00000000
    Execlist CSB read 5, write 5
        ELSP[0] idle
        ELSP[1] idle
清单11-2中，第1行的rcs0是执行引擎的简称，即渲染引擎命令流化
器（Render Command Streamer）的缩写，0代表0号，因为系统中只有一
个，其意义不大。后面是活跃的请求数：1个。接下来是请求的概要信
息，是通过print_request函数显示的，其核心代码如下。
drm_printf(m, "%s%x%s [%x:%x] prio=%d @ %dms: %s\n", prefix,
   rq->global_seqno,
   i915_gem_request_completed(rq) ? "!" : "",
   rq->ctx->hw_id, 
   rq->fence.seqno,
   rq->priotree.priority,
   jiffies_to_msecs(jiffies - rq->emitted_jiffies),
   rq->timeline->common->name);
第1列是描述性的前缀，比如first和last等。第2列是全局的流水号，
每个引擎独立维护，单调递增，可以通过cat i915_gem_seqno来观察其
详情。第3列可能为空或者显示一个惊叹号，惊叹号代表已经完成，一
般完成了就会被移除队列，很少会看到。第4列是LRC的硬件ID。第5列
是用于与硬件同步的栅栏ID（fence ID）。第6列是优先级。第7列为排
队时间，即从发射到软件队列时起到观察时的时间差，单位为毫秒。最
后一列为提交任务的进程名。
也就是说，清单11-2中只有一个任务在队列中，是Ubuntu的窗口合
成器进程compiz的，它的进程ID为1955。
下面解说engine_info的概要行。
current seqno dc9, last dc9, hangcheck d7b [11688 ms], inflight 1
其含义为：当前正在执行的是dc9号请求，最后一次提交的也是这
个请求，最近一次做挂起检查时执行的请求是d7b号（engine-
>hangcheck.seqno），检查的时间（engine-
>hangcheck.action_timestamp）在11688ms前，飞行（活跃）状态的请求
一共有1个。
Requests下是第一个请求和最近一次提交的请求，二者都是dc9号。
随后的4行是关于环形缓冲区的，每行三列。第1列为名称。第2列
是通过下面这样的代码从硬件读到的值。
I915_READ(RING_START(engine->mmio_base))
第3列是活跃请求上下文结构体中的值。当时没有活跃请求，所以
显示为0或空。
最后两行是ELSP端口的状态，因为唯一的请求正在执行，目前两
个端口都空闲，所以没有任务在排队。
接下来再看一个GPU很忙碌时的输出结果。清单11-3是在执行