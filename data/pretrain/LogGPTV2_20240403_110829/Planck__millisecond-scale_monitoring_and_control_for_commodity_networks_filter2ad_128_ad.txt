Reacting to Congestion The controller subscribes to link utiliza-
tion events from the collector(s). The core design for trafﬁc engineer-
ing is to, for every congestion notiﬁcation, greedily route each ﬂow
in the notiﬁcation to a less congested path, if possible, by quickly
changing routing labels. The greedy routing of ﬂows, presented in
Algorithm 1, which uses Algorithm 1 from DevoFlow [6] to imple-
ment find_path_btlneck, considers each ﬂow independently
and ﬁnds the alternate path of a ﬂow with the largest expected bot-
tleneck capacity, which, for a constant number of alternate paths of
length P, is only expected to be an O(P) operation.
When selecting alternate paths, ﬂow rerouting must consider
ﬂows on other links that it has heard of in the past. In order to
avoid using stale information, ﬂow entries in the controller are ex-
punged after a speciﬁed timeout and thus trafﬁc engineering does not
consider them when calculating available bandwidth along routes.
Fast Rerouting In addition to implementing Algorithm 1 and to
leverage shadow-MAC-address-based alternate routes, a mechanism
for dynamically changing a ﬂow’s destination MAC address is re-
quired. We implemented two different mechanisms to switch a
ﬂow’s destination MAC address, both requiring a single message:
(i) using an OpenFlow rule to rewrite the destination MAC address
at the source host’s ingress switch and (ii) using spoofed ARP mes-
sages to update the ARP cache of the source host.
The OpenFlow-based rerouting mechanism is straightforward, but
the TCAM rule state requirements of this method at each switch can
be proportional to the number of hosts in the network. To address
this problem, we developed MAC address rewriting through ARP
spooﬁng, which requires no switch state, and thus no OpenFlow
rule installation to reroute a ﬂow. If the controller sends an ARP
message pretending to be from the destination IP but using the
alternate shadow MAC address, the source will update its ARP
cache to the new address and almost immediately send packets
using the new route. Although this may seem unsafe at ﬁrst, since
this is essentially ARP poisoning, in many SDN environments the
controller intercepts all ARP messages from hosts, so the controller
is the only entity capable of sending ARP messages.
Two caveats to the ARP-based rerouting mechanism are that
some operating systems, e.g., Linux, ignore spurious ARP replies
and in addition lock an ARP cache entry for a period of time after
changing. The ﬁrst problem is solved by sending unicast ARP
requests which, on Linux, still performs MAC learning for the
request and thus updates its ARP cache. The second method requires
setting a sysctl to enable faster ARP cache updates.
7. APPLICATION EVALUATION
We evaluate our Planck-based trafﬁc engineering application with
a series of synthetic and realistic workloads based on similar tests
done in previous related work [2, 6, 10].
7.1 Methodology
Testbed All experiments were conducted on a physical testbed
consisting of 16 workload generator servers, ﬁve collector servers
and ﬁve 64-port, 10 Gbps IBM RackSwitch G8264 top-of-rack
switches [16]. The workload generators are IBM x3620 M3s with
six-core Intel Westmere-EP 2.53 GHz CPUs and Mellanox Con-
nectX 10 Gbps NICs. The collector machines are IBM x3650 M4s
with two eight-core Intel Sandy Bridge-EP 2.4 GHz CPUs and seven
two-port Intel 82599 10 Gbps NICs. We used Linux 3.5. Note that
our experiments use only a fraction of the resources of these servers.
Topology To evaluate Planck and its applications, we wanted a
network topology that provided high path diversity. We chose to
build a three-tier fat-tree [1] with 16 hosts.
We built the 16-host fat-tree topology by subdividing four 64-port
switches into 20 ﬁve-port logical switches using OpenFlow. Due
to limited TCAM size we place only ﬁve logical switches on each
physical switch, leaving many ports unused. Four ports of the sub-
switch were wired up to build the fat-tree, the ﬁnal port was used
for sampling and connected directly to a collector interface.
Workloads We evaluate the trafﬁc engineering schemes on our
testbed with a set of synthetic and realistic workloads, similar to
previous related work [2, 6, 10]. A brief description of the workloads
follows. As in the prior work, host indices used in the descriptions
are contiguous within pods.
Stride(8): The node with index x sends a ﬂow to the node with
index (x + 8) mod (num_hosts). Stride(8) is a taxing workload
because all ﬂows traverse the core.
Shufﬂe: Each node sends a large amount of data to every other
node in the network in random order. This workload mimics
Hadoop/MadReduce workloads commonly encountered in real data
center networks. In our evaluation, each node sends to two other
nodes at a time, and the shufﬂe completes when all nodes ﬁnish.
SourceDestinationBase RouteAlt Route 1Alt Route 2Rewrite to Base MACSSSSSHHFigure 14: Average ﬂow throughput in each workload.
Figure 15: Two ﬂows initially use the same bottleneck link and
are then rerouted, demonstrating the latency to detect conges-
tion and reroute one of the ﬂows. Flow 1 does not reduce its
sending rate because the detection and rerouting occurs faster
than the switch’s buffer ﬁlls, so it sees no loss.
Random Bijection: Every node is exactly the source of one ﬂow
and the destination of another ﬂow. Each run in our experiment
represents a different random bijection mapping.
Random: Every node picks a destination not equal to itself from
a uniform distribution. Each run is a different mapping. These runs
allow for hotspots to form in the network.
For each of the above workloads, we perform experiments with
100 MiB, 1 GiB, and 10 GiB ﬂow sizes, unless otherwise noted. A
ﬂow size in the shufﬂe workload represents the amount of data each
host needs to send to another host, so a 1 GiB workload represents
a 240 GiB shufﬂe. We tried different conﬁgurations of shufﬂe and
stride, and also other workloads such as Staggered Prob (as in [2]),
but we found the trends to be consistent, so we omit those results
for brevity. All experiments are run using TCP and we run all
 combinations over 15 runs.
We run four different routing algorithms for each of the work-
loads. As an upper bound, all 16 hosts connect to one of our 64-port
10 Gbps Ethernet switches. This topology is used to represent an
optimal non-blocking network, referenced by the name Optimal. To
provide a baseline, we use PAST [39], a static multipath routing
algorithm with performance comparable to ECMP, which is refer-
enced by the name Static. To emulate previous trafﬁc engineering
projects that rely on polling [2, 6], we implement a global ﬁrst ﬁt
routing algorithm that greedily reroutes every ﬂow either once a sec-
ond, which is referenced by the name Poll-1s, or once every 100 ms,
which is called Poll-0.1s. Lastly, the trafﬁc engineering algorithm
described in Section 6.2 uses a 3 ms ﬂow timeout, approximately the
latency of rerouting a ﬂow, and is referenced by the name PlanckTE.
7.2 Planck Control Loop Latency
Figure 15 demonstrates the full control loop of our system when
ARP messages are used for fast control. In this experiment, we
show the throughput over time of two ﬂows, Flow 1 and Flow 2.
The ﬂows use initial routes that collide, and Flow 2 is started after
Figure 16: A CDF of the routing latency of both OpenFlow rule
control and ARP-based control.
Flow 1 has reached steady state. The labels Detection and Response
mark the times when congestion is detected and the ﬂow is rerouted,
respectively. The latency between detecting the ﬁrst packet that
causes congestion and sending a congestion notiﬁcation was be-
tween 25–240 µs across runs, and the difference between detection
and response is 2.6 ms in this ﬁgure. Because the throughput of
Flow 1 never decreases, we can see that the switch had sufﬁcient
buffer capacity to avoid dropping a packet during the time in which
both ﬂows were active but before a ﬂow was rerouted.
Figure 16 further characterizes the control loop of our system by
presenting a CDF of the response latency of both OpenFlow- and
ARP-based control, where response latency is deﬁned as the time
from when the congestion notiﬁcation was sent to the time at which
a collector sees a packet with the updated MAC address. We see
that ARP-based control takes around 2.5 ms to 3.5 ms, while the
latency for OpenFlow-based control varies from about 4 ms to 9 ms,
with the median control time taking over 7 ms. As in Section 5.3,
the majority of this latency can be attributed to switch buffering.
7.3 Trafﬁc Engineering
In this section, we evaluate the effectiveness of a trafﬁc engineer-
ing scheme designed to work within our monitoring framework. We
analyze the performance of this scheme by comparing it against
other trafﬁc engineering schemes under various workloads.
Varying Flow Sizes We ﬁrst investigate the performance of each
trafﬁc engineering scheme under a variety of ﬂow sizes. If PlanckTE
can operate on ﬁne-grained timescales, then its performance should
track the performance of Optimal for smaller ﬂow sizes. We vary the
ﬂow sizes in a stride(8) workload from 50 MiB up to 100 GiB and
plot the average throughput achieved by each ﬂow in the network
in Figure 17. We use average ﬂow throughput as a metric because
ﬁne-grained trafﬁc engineering can impact the initial stages of a
ﬂow’s throughput and capturing these effects are important.
We note the following trends in the ﬁgure. First, PlanckTE can
effectively route on small time scales, given its performance relative
to Optimal. PlanckTE and Optimal have similar performance for
ﬂows as small as 50 MiB, which theoretically can take as little as
 0 1 2 3 4 5 6 7 8 9 10100M1G10G100M1G10G100M1G10G100M1G10GAvg Flow Tput (Gbps)StaticPoll-1sPoll-0.1sPlanckTEOptimalRandom BijectionStrideRandomShuﬄe012345678910 0.42 0.43 0.44 0.45 0.46 0.47 0.48Throughput (Gbps)Time (s)DetectionResponseFlow 1Flow 2 0 0.2 0.4 0.6 0.8 1 2 3 4 5 6 7 8 9 10FractionResponse Latency (ms)ARPOpenFlowFigure 17: Average ﬂow throughput for varying ﬂow sizes in
stride(8), shown at log-scale, for ﬂow sizes ranging from 50 MiB
to 100 GiB.
(a) Shufﬂe
4.2 ms to transfer. Poll-1s can only engineer ﬂows larger than 1 GiB
because these are the ﬁrst ﬂow sizes whose stride(8) workload takes
longer than one second to complete, whereas Poll-0.1s can optimize
100 MiB ﬂows. The performance of Poll-1s and Poll-0.1s eventually
approach Optimal’s performance as the ﬂow size increases. With
100 GiB ﬂows, all schemes but static provide similar performance.
Varying Workloads Figure 14 presents the performance of differ-
ent trafﬁc engineering schemes for each of the workloads detailed
in Section 7.1. As in the previous section, the average throughput
of an individual ﬂow is used as the metric. For each workload,
the performance of three different ﬂow sizes (100 MiB, 1 GiB and
10 GiB) are presented for the trafﬁc engineering schemes.
We notice the following trends in the ﬁgure. First, PlanckTE
can closely track the performance of Optimal for all ﬂow sizes.
Even under the smallest ﬂow size of 100 MiB, PlanckTE typically
comes within 1–4% of Optimal’s performance (with the exception
of the shufﬂe workload, where PlanckTE comes within a worst
case of 12.3% to Optimal). Second, for both Poll schemes, the
performance increases as the ﬂow size increases with Poll-0.1s
performing better as expected. Finally, PlanckTE provides beneﬁts
over both Poll schemes. The improvement is small for the shufﬂe
workloads where trafﬁc is distributed across many links, and we
saw similar trends for Staggered Prob (not shown) where trafﬁc is
typically isolated near the network edges. However, in the other
workloads, the improvement of PlanckTE over Poll-1s ranges from
24.4% (random) to 53% (stride), and the improvement over Poll-0.1s
ranges from 11% (random) to 33% (random bijection).
Lastly, we examine the performance of two different workloads
with 100 MiB ﬂow sizes in more detail. Figure 18(a) presents a
CDF comparing shufﬂe completion times for each host in each
engineering scheme. The median host completion times are 3.31
seconds for Poll-1s, 3.01 seconds for Poll-0.1s, 2.86 seconds for
PlanckTE, and 2.52 seconds for Optimal.
Figure 18(b) contains a CDF of the individual ﬂow throughputs
for the stride(8) workload. PlanckTE and Poll-0.1s obtain median
ﬂow throughputs of 5.9 Gbps and 4.9 Gbps respectively, and we can
see that the performance of PlanckTE closely tracks that of Optimal.
8. RELATED WORK
The work most related to Planck consists of low-latency net-
work measurement techniques and dynamic routing mechanisms
for trafﬁc engineering. Trafﬁc engineering is a broad ﬁeld, but we
focus on the recent work predominantly aimed at ﬁne-grained trafﬁc
engineering for the data center supported by SDN.
A few recent efforts have focused on how to implement counters
and measurement in switches. OpenSketch [44] proposes adding
reconﬁgurable measurement logic to switches and exposing an in-
(b) Stride
Figure 18: CDFs for 100 MiB ﬂow workloads. The ﬁrst ﬁgure
shows host completion times for their shufﬂe and the second
ﬁgure shows the ﬂow throughputs for a stride(8) workload.
terface to program it in much the same way that OpenFlow allows
for programming forwarding behavior. Other work [25] looks at
software-deﬁned counters with a hybrid implementation across the
switching ASIC and control plane CPU that might provide better
trade-offs in terms of how quickly counters can be read. Addi-
tionally, future switching ASICs could provide the ability for fast
data-plane only sampling without involving the control plane CPU.
Planck provides similar beneﬁts, i.e., comprehensive low-latency
measurements, today and this paper uses it to demonstrate how such
measurements could be used if and when such new hardware arrives.
A separate related line of work looks at improving network mea-
surements in terms of accuracy and coverage [7, 8, 9]. Unlike our
work, which focuses on obtaining ﬂow-level throughput measure-
ments at very tight time-scales, they focus on accurately monitoring
transit networks at coarser time-scales.
Our trafﬁc engineering application draws inspiration from Hed-
era [2] and MicroTE [4] and takes a similar approach of measuring
the network and rerouting ﬂows based on measurements. However,
Hedera and MicroTE use ﬁve second and one second control loops,
respectively. Further, they both use 1 Gbps links and would need to
be noticeably faster to get similar results on 10 Gbps links.
Mahout [5] gets low-latency measurements by modifying end-
hosts to detect elephant ﬂows in 1.5–5.5 ms. We expect that using
these detections to infer global network state would take notably
longer. Planck detects more than just elephant ﬂows without end-
host modiﬁcations at the same or smaller time-scales.
LocalFlow [31] shows the beneﬁt of a faster trafﬁc engineering
control loop in simulation, but does not consider implementing it.
B4 [20] and SWAN [15] monitor and shape trafﬁc at the edge of
the network, but these projects operate on the order of seconds or
minutes. Similarly, Seawall [35] and EyeQ [21] monitor and shape
trafﬁc at the edge, but do not monitor switches.
Additional work has looked at integrating optical links in data cen-
ters [10, 40, 42]. This work typically leverages optical networks by
building hybrid networks that contain both optical circuit switches
and traditional electrical packet switches and needs fast measure-
ments to quickly predict a trafﬁc matrix and then schedule optical
 0 2 4 6 8 10 0.01 0.1 1 10 100Avg Flow Throughput (Gbps)Flow Size (GiB)StaticPoll-1sPoll-0.1sPlanckTEOptimal 0 0.2 0.4 0.6 0.8 1 1.5 2 2.5 3 3.5 4 4.5FractionTime (sec)StaticPoll-1sPoll-0.1sPlanckTEOptimal 0 0.2 0.4 0.6 0.8 1 1 2 3 4 5 6 7 8 9 10FractionFlow GbpsStaticPoll-1sPoll-0.1sPlanckTEOptimalcircuits to carry the largest components. Despite this, they can only
measure the network every 75–100 ms at minimum. Planck could
be used to inform such schemes at much smaller time-scales.
SideCar [36], like Planck, directly attaches servers, called Side-
Cars, to switches with the goal of observing and modifying network
behavior. While Planck’s collectors are similar, we attach multiple
switches to each and use them to monitor and change the behavior
of a network of unmodiﬁed end-hosts. By contrast, SideCar attaches