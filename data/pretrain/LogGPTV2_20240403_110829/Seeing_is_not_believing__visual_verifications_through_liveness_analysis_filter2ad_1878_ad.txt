RF
Classifier
J48
Figure 8: Movee (per-module) server side overhead:
video processing is the most expensive. The total
cost is however under 1.3s.
is equal to the probability that a classiﬁer will rank a ran-
domly chosen genuine sample higher than a randomly chosen
fraudulent one. An area of 1 represents a perfect test; an
area of 0.5 represents a worthless test.
6.4 Experimental Results
6.4.1 Movee server overhead
Figure 8 shows the overhead (divided into modules) of
the liveness analysis on the server, running on the above
described Dell laptop, for 6s videos. The values are an aver-
age over 10 experiment runs. It shows that the VMA is the
most time consuming module, slightly exceeding 1s. The
IMA and Classiﬁcation components (running the J48 classi-
ﬁer) impose the smallest overheads, together being 110ms.
MLP takes an average of 940 ms and Random Forest an av-
erage of 140 ms. The overhead of the SC module is around
150ms, with the smallest cost imposed by the stretching step
and the highest cost by the penalty based DTW.
6.4.2 Attack detection analysis
It is straightforward to see that Movee prevents the “Copy-
Paste” and “Replay” attacks of Section 2.1: no sensor stream
exists. Movee does not detect the “Projection”, as the video
and sensor streams are indeed captured in the same user
hand movement. However, a human observer can immedi-
ately detect that the movie is of a poster. In the following,
we study the ability of Movee to detect the last, but more
complex “Random” and “Direction Sync” attacks. For this,
we explore the accuracy of Movee in detecting fraudulent
samples, on both random and direction sync data sets, us-
ing the above mentioned classiﬁers. The results are shown
in Figure 9. For the random data set, the multilayer percep-
tron neural networks (MLP) provides the highest accuracy,
92% whereas the Random Forest (RF) and the C4.5/J48 De-
cision Tree exhibit accuracy of 91% and 90% consequently.
Figure 10(a) shows the ROC curve and the computed EER
value for MLP and the random dataset. The EER value of
MLP is as small as 0.08.
Finally, we evaluated the impact of each step of the SC
module on the accuracy of Movee, for both test datasets. For
each dataset, we measured the accuracy of the three classi-
Figure 9: Movee accuracy, on random and direction
sync data sets, when diﬀerent classiﬁers are used.
MLP outperforms the random forest and J48 classi-
ﬁers on the random dataset (92%) while J48 outper-
forms the random forest and MLP on the direction
sync dataset (84%).
ﬁers when (i) no alignment phase was applied, (ii) when
stretching and DTW were applied, (iii) for stretching, cali-
bration and DTW, and (iv) for stretching, calibration and
penalty based DTW. Figure 10(b) shows the results for the
random dataset, and Figure 10(c) shows the results for the
direction sync dataset. The stretching and DTW steps con-
tribute the most to the accuracy of Movee: almost 12% for
all three classiﬁers on the random attack and almost 8% for
the direction sync attack. For the direction sync attack, the
Penalization step brings the most accuracy improvement, al-
most 11% for all three classiﬁers. Note that MLP exhibits
the best performance for the random attack; C4.5 achieves
the best performance for the direction sync attack. When
all processing steps are applied, C4.5 should be chosen:
it
outperforms MLP and RF’s accuracy by 6% for the direction
sync attack, while it lags only 2% behind MLP’s accuracy
for the random attack.
6.5 Limitations
We have not experimented with very short videos (less
than 6s) or with videos shot in unusual circumstances: in-
volving very high accelerometer activity, e.g., running, or
when the user is in a moving vehicle. Due to the lack of
gyroscope sensors in the Samsung Admire device, we have
not integrated gyroscope readings to verify camera rotation
movements.
Furthermore, we have not experimented with doctored
video and accelerometer streams. For instance, given an
input video, the attacker can use the work of Davison et
al. [22] to recover the 3D trajectory of the camera. Then,
given root access (e.g., using [9, 7], create a corresponding
accelerometer sample and feed it to Movee, e.g., using a so-
lution similar to [12]. We defer the task of providing trust
for the integrity of the mobile app, as well as the trust for
the integrity of the device’s connection to its camera and
accelerometer sensors to the providers of Movee. Establish-
ing the integrity of a mobile platform and mobile apps is
currently an active area of research [38, 1, 39].
Finally, we have not experimented with “green screen” at-
tacks, where the attacker captures a video with a portion
246
MLP
RF
J48
y
c
a
r
u
c
c
A
r
e
i
f
i
s
s
a
C
l
92
88
84
80
76
72
68
MLP
RF
J48
y
c
a
r
u
c
c
A
r
e
i
f
i
s
s
a
C
l
88
84
80
76
72
68
64
60
56
NO Align
S+DTW
S+C+DTW
S+C+DTW+P
NO Align
S+DTW
S+C+DTW
S+C+DTW+P
Alignment Phases
Alignment Phases
(a)
(b)
(c)
Figure 10: (a) ROC curve on random dataset for Movee when the MLP classiﬁer is used. The EER of MLP
is as low as 0.08. (b) The impact of the SC steps on the Movee accuracy, for the three classiﬁers, for the
random attack, and (c) for the direction sync attack. Stretching+DTW provide the highest improvement
(10%) for the random attack. The penalization brings an 11% improvement for the direction sync attack.
of the scene being a green screen. Following the video cap-
ture, the attacker overlays additional video footage or static
images on the green section. We note however that Movee
raises the bar here: an attacker needs to invest in additional
equipment to thwart the defenses of Movee. The quality
of the equipment determines the (in)ability of a human ob-
server to detect the attack.
7. RELATED WORK
The combination of video and accelerometer data has been
studied by Hong et. al. [28] in order to improve the compute-
intense motion estimation in video encoding. They have
shown that the use of accelerometer data improves the speed
of the encoding process by a factor of 2-3. Moiz et. al. [33]
introduced and developed a wearable, multi-modality, mo-
tion capture platform, and used its inertial and ultrasonic
sensors to estimate position. The focus of our work is diﬀer-
ent, on verifying liveness of a video through the consistency
of its video and accelerometer data.
Indyk et al. [29] studied the problem of ﬁnding pirated
video on the Internet. They propose to extract a small num-
ber of pertinent features (temporal ﬁngerprints) based on
the shot boundaries of a video sequence, and match them
against a database of videos. We note our work is on an or-
thogonal problem, of verifying the liveness of a video claimed
to have been taken by a mobile device user. As such, these
two problems can complement each other.
A ﬂavor of liveness analysis similar to the one we proposed,
is used to verify biometric liveness. Kollreider et al. [31]
study the problem of verifying the actual presence of a live
face in contrast to a photograph (playback attack) for face
recognition based biometrics. They introduce a lightweight
optical ﬂow approach that estimates face motion estimation
on the structure tensor and a few input frames. Park et. al.
[36] introduce a liveness detection method for distinguishing
a two-dimensional object from a three-dimensional object.
The approach proposed uses video sequence images and does
not require additional hardware or user interaction. Their
work has direct application to face recognition biometrics: it
can identify the use of a ﬂat picture. Further work is needed
to understand the vulnerability of this approach to photo
movement and photo bending/3D printing attacks.
Multi-modal approaches relying on diﬀerent sensor sets [25,
19, 18] have been proposed, to exploit the static and dy-
namic relationship between voice and face information from
speaking faces for biometric authentication. Chetty [18] pro-
posed liveness checking techniques for multimodal biomet-
ric authentication systems. Their techniques fuse acoustic
and visual speech features and measure the degree of syn-
chronization between the lips and the voice extracted from
speaking face video sequences.
Accelerometers have been used to provide biometric in-
formation, in the form of gait or gesture recognition. Man-
tyjarvi et al. [32] proposed several solutions that achieve
low EER (equal error rates) for identifying users of mobile
devices from gait signal acquired with three-dimensional ac-
celerometers, when the device was worn on the belt, at the
back. Pylv¨an¨ainen [37] used 3D accelerometers and hidden
Markov models to identify gestures performed using a mo-
bile device.
Summary. Our work introduces novel techniques for com-
bining video and inertial sensor data to verify the liveness
of a video stream. Movee veriﬁes that the video has indeed
been shot as claimed by the user, using her mobile device.
We note that Movee does not require additional equipment,
but requires the user to install and shoot the video using
Movee’s client application.
8. CONCLUSIONS
In this paper we have introduced the concept of “liveness”
analysis, of verifying that a video has been shot on a claimed
mobile device. We have proposed Movee, a system that re-
lies on the accelerometer sensors ubiquitously deployed on
most recent mobile devices to verify the ownership of a si-
multaneously captured video stream. We have implemented
Movee, and, through extensive experiments, we have shown
that (i) it is eﬃcient in diﬀerentiating fraudulent and gen-
uine videos and (ii) imposes reasonable overheads on the
server. In future work we intend to integrate more sensors
(e.g., gyroscope), as well as the use of MonoSLAM [22] as
an alternative VMA implementation to improve accuracy.
247
9. ACKNOWLEDGMENTS
We thank the shepherd and the anonymous reviewers for
their excellent feedback.
10. REFERENCES
[1] Arxan: Protecting the App Economy.
http://www.arxan.com/.
[2] Chicago Works. http://www.chicagoworksapp.com/.
[3] Kickstarter. http://www.kickstarter.com/.
[4] NYC 311: Pothole or Other Street Surface Complaint.
http://www.nyc.gov/apps/311/allServices.htm?
requestType=topService&serviceName=Pothole+or+
Other+Street+Surface+Complaint.
[5] Open Source Computer Vision. http://opencv.org/.
[6] Optical mouse.
https://en.wikipedia.org/wiki/Optical_mouse.
[7] Root and Me. https://play.google.com/store/
apps/details?id=com.iamjake.root&hl=en.
[8] Sensor Delay. http://developer.android.com/
reference/android/hardware/SensorManager.html.
[9] Unlock Root. http://www.unlockroot.com/.
[10] Vine. http://vine.co/.
[11] Weka. http://www.cs.waikato.ac.nz/ml/weka/.
[12] XPrivacy 1.9.5: The ultimate privacy manager.
http://forum.xda-developers.com/showthread.
php?t=2320783.
[13] YouTube. http://www.youtube.com.
[14] A. Ali, F. Deravi, and S. Hoque. Liveness detection
using gaze collinearity. In Emerging Security
Technologies (EST), pages 62–65, 2012.
[15] A. Anjos and S. Marcel. Counter-measures to photo
attacks in face recognition: A public database and a
baseline. In Biometrics (IJCB), pages 1–7, 2011.
[16] L. Breiman. Random forests. Machine Learning,
45:5–32, 2001.
[17] S. Capkun, K. B. Rasmussen, M. Cagalj, and M. B.
Srivastava. Secure location veriﬁcation with hidden
and mobile base stations. IEEE Trans. Mob. Comput.,
7(4):470–483, 2008.
[18] G. Chetty. Biometric liveness detection based on cross
modal fusion. In Information Fusion, 2009. FUSION
’09. 12th International Conference on, pages
2255–2262, July.
[19] G. Chetty and M. Wagner. Multi-level liveness
veriﬁcation for face-voice biometric authentication. In
Biometric Symposium, 2006.
[20] D. Cliﬀord and G. Stone. Variable penalty dynamic
time warping code for aligning mass spectrometry
chromatograms in r. Journal of Statistical Software,
47(8):1–17, 4 2012.
[21] I. Coope. Circle ﬁtting by linear and nonlinear least
squares. Journal of Optimization Theory and
Applications, 76:381–388, 1993.
[22] A. J. Davison, I. D. Reid, N. D. Molton, and
O. Stasse. Monoslam: Real-time single camera slam.
IEEE Trans. Pattern Anal. Mach. Intell.,
29(6):1052–1067, June 2007.
[23] E. De Castro and C. Morandi. Registration of
translated and rotated images using ﬁnite fourier
transforms. IEEE Trans. Pattern Anal. Mach. Intell.,
9(5):700–703, May 1987.
[24] J. B. J. Fourier and A. Freeman. The Analytical
Theory of Heat. Cambridge University Press, 2009.
[25] R. Frischholz and U. Dieckmann. Bioid: A multimodal
biometric identiﬁcation system. IEEE Computer,
33(2):64–68, 2000.
[26] S. I. Gallant. Perceptron-based learning algorithms.
Trans. Neur. Netw., 1(2):179–191, June 1990.
[27] B. F. Hildebrand. Introduction to numerical analysis:
2nd edition. Dover Publications, Inc., 1987.
[28] G. Hong, A. Rahmati, Y. Wang, and L. Zhong.
Sensecoding: accelerometer-assisted motion estimation
for eﬃcient video encoding. MM ’08, pages 749–752.
ACM, 2008.
[29] P. Indyk, G. Iyengar, , and N. Shivakumar. Finding
pirated video sequences on the internet. Technical
report, Stanford University, 1999.
[30] R. Kohavi. A study of cross-validation and bootstrap
for accuracy estimation and model selection. pages
1137–1143, 1995.
[31] K. Kollreider, H. Fronthaler, and J. Big¨un.
Non-intrusive liveness detection by face images. Image
Vision Comput., 27(3):233–244, 2009.
[32] J. Mantyjarvi, M. Lindholm, E. Vildjiounaite, S.-M.
Makela, and H. Ailisto. Identifying users of portable
devices from gait pattern with accelerometers. In
ICASSP ’05, volume 2, pages ii/973–ii/976 Vol. 2,
March.
[33] F. Moiz, D. Leon-Salas, and Y. Lee. A wearable
motion tracker. BodyNets ’10, pages 214–219.
[34] M. M ˜Aijller. Dynamic time warping. In Information
Retrieval for Music and Motion, pages 69–84. Springer
Berlin Heidelberg, 2007.
[35] G. Pan, L. Sun, Z. Wu, and S. Lao. Eyeblink-based
anti-spooﬁng in face recognition from a generic
webcamera. In ICCV 2007., pages 1–8.
[36] G.-t. Park, H. Wang, and Y.-s. Moon. Liveness
detection method and apparatus of video image,
August 2007.
[37] T. Pylv¨an¨ainen. Accelerometer based gesture
recognition using continuous hmms. IbPRIA’05, pages
639–646. Springer-Verlag, 2005.
[38] A. Shabtai, Y. Fledel, and Y. Elovici. Securing
android-powered mobile devices using selinux. Security
Privacy, IEEE, 8(3):36–44, 2010.
[39] J. Six. Application Security for the Android Platform:
Processes, Permissions, and Other Safeguards.
O’Reilly, 2011.
[40] J. O. Smith. Spectral Audio Signal Processing. W3K
Publishing, 2011. online book.
[41] N. P. H. Thian and S. Bengio. Evidences of equal
error rate reduction in biometric authentication
fusion, 2004.
[42] L. von Ahn, M. Blum, N. J. Hopper, and J. Langford.
Captcha: Using hard ai problems for security. In In
Proceedings of EUROCRYPT, pages 294–311.
Springer-Verlag, 2003.
[43] Wikipedia. http://en.wikipedia.org/wiki/
Receiver_operating_characteristic.
248