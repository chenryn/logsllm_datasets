SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
S. Hu et al.
yet very easy to implement using the Active Queue Management
(AQM) feature at commodity switches. In §4.1, we introduce two
implementation options using Weighted Random Early Detection
(WRED) and RED/ECN, respectively. In our testbed experiments,
we adopt RED/ECN to realize the proposed selective dropping.
3.3 Loss Recovery
In Aeolus, scheduled packets are less likely to be dropped as we
well protect them. However, unscheduled packets can be dropped
under selective dropping. Hence, a fast loss recovery of unscheduled
packets is needed. Given we have safeguarded scheduled packets,
our idea is to retransmit lost unscheduled packets using subsequent
scheduled packets, whose deliveries are guaranteed by the property
of proactive transport. Therefore, loss recovery simply reduces to
loss detection in Aeolus.
Loss detection: Aeolus enables per packet ACK at the receiver to
quickly notify the sender of arrival unscheduled packets. We use
selective ACK rather than cumulative ACK for loss detection in
the middle, and leverage a simple probing to detect tail losses of
unscheduled packets. Specically, the Aeolus sender transmits a
probe packet right after the last unscheduled packet. This probe
packet carries the sequence number of the last unscheduled packet,
and is of minimum Ethernet packet size, e.g., 64 bytes. When the
receiver receives the probe packet, it returns an ACK carrying the
sequence number of the probe packet. Once the sender receives such
a probe ACK, it can immediately infer all the losses of unscheduled
packets, including the last one. Finally, it is worthwhile to note that
to guarantee the delivery of the probe packet and all ACKs, we treat
them as scheduled in the network.
Retransmission: As introduced above, Aeolus retransmits lost
unscheduled packets using subsequent scheduled packets. Upon
receiving credits, the sender can retransmit old packets or trans-
mit new packets. Specically, the sender has three types of pack-
ets to transmit: sent but unacknowledged unscheduled packets,
loss-detected unscheduled packets, and unsent scheduled pack-
ets. We prioritize them in the order of loss-detected unscheduled
packets, unsent scheduled packets, and sent but unacknowledged
unscheduled packets, respectively. We give the highest priority
to loss-detected unscheduled packets because we want to ll the
gap as soon as possible to minimize the memory footprint of re-
sequence buer. We prioritize unsent scheduled packets over sent
but unacknowledged unscheduled packets to avoid redundant re-
transmissions.
3.4 Why this Works
The key of Aeolus is its simple yet eective selective dropping
mechanism, which not only delivers good performance but also
signicantly simplies both rate control and loss recovery designs.
With selective dropping, new ows can start at line-rate to fully
utilize spare bandwidth without aecting scheduled packets. For
pre-credit unscheduled packets, the cooperation of line-rate start
and selective dropping maximizes their potential benets (e.g., uti-
lize the spare bandwidth) and minimizes their side eects (e.g.,
aect scheduled packets) simultaneously. Furthermore, by selec-
tive dropping, Aeolus only drops unscheduled packets. Therefore,
loss recovery becomes relatively simple because packet losses only
happen in the pre-credit stage (or rst batch) and the deliveries of
subsequent scheduled packets are guaranteed. We just need to lo-
cate the losses in the rst batch and then eciently retransmit them
only once using scheduled packets. We do not need sophisticated
schemes to handle many challenging corner cases, e.g., packet loss
of retransmission packets. Compared to TCP which has many com-
plex loss recovery mechanisms [15] for dierent scenarios, Aeolus’s
loss recovery is extremely simple but more ecient.
4 IMPLEMENTATION
4.1 Switch implementation
The Aeolus switch selectively drops unscheduled packets while
preserving scheduled packets in one switch queue. Here we propose
two implementation options to realize this.
WRED: Weighted random early detection (WRED) is an extension
to random early detection (RED) where a single queue has several
dierent sets of queue dropping/marking thresholds. WRED typi-
cally supports three packet colors6: red, yellow and green, and each
color has its own dropping thresholds in a switch queue. WRED is
widely supported by commodity switching chips [4, 5].
To implement selective dropping using WRED, we mark sched-
uled and unscheduled packets with dierent DSCP values at the
end host. At the switch, we congure access control list (ACL) table
to set the arriving packet’s color based on its DSCP eld. Therefore,
scheduled and unscheduled packets can be marked with dierent
colors in the switch pipeline. For unscheduled packets, we can set
both the high and low dropping thresholds to the desired selective
dropping threshold. For scheduled packets, we can set its high/low
dropping threshold to a very large value (e.g. total buer size) so
that scheduled packets will not be dropped by WRED.
RED/ECN: Though WRED is widely supported by switching chips,
it may not be exposed by all switch OSes to users. Some switch
OSes (e.g., Arista EOS [1]) just provide a simple RED/ECN cong-
uration interface where a switch queue only has a single set of
dropping/marking thresholds.
Now, we show how to realize selective dropping only using
RED/ECN feature exposed to users. ECN mechanism uses the 2-bit
ECN eld in the IP header to encode whether a packet is ECN capa-
ble or has experienced congestion. When both endpoints support
ECN, they will mark their data packets with 10 (ECN capable trans-
port, ECT(0)) or 01 (ECT(1)). Otherwise, packets will be marked
with 00 (Non-ECT). At the switch, when the buer occupancy is
larger than the ECN marking threshold, the arriving packet will
be marked (changed the code point to 11) if it is ECN capable,
otherwise get dropped. This mechanism has been well studied in
previous work [19, 24, 33].
Therefore, we can implement the selective dropping by reinter-
preting the RED/ECN as follows. At the sender side, we set the ECN
elds of unscheduled packets and scheduled packets to Non-ECT
and ECT(0), respectively. At the switch, we enable ECN marking
and congure both the high and low RED thresholds to the se-
lective dropping threshold. In this way, any unscheduled packets
exceeding this threshold will be selectively dropped by switch. At
the receiver side, we simply ignore the ECN marks of the arriving
packets.
6Color is a metadata attached to the packet in switch processing pipeline.
Aeolus: A Building Block for Proactive Transport in Datacenters
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Application
send()
Application
receive()
Flow Classification Module
pre-credit
Aeolus 
Control Logic
request, unscheduled, 
probe, ACK
credit-induced
Proactive 
Control Logic
credit, scheduled 
Packet Marking Module
DPDK Library
batch deliver
DPDK Poll Mode Driver
unscheduled
Aeolus 
Control Logic
new flow
loss info
request, unscheduled, 
probe, ACK
scheduled
Proactive 
Control Logic
credit, scheduled
Packet Dispatch Module
DPDK Library
DPDK Poll Mode Driver
NIC
TX Ring Buffer
NIC
RX Ring Buffer
(a) Packet sending pipeline
(b) Packet receiving pipeline
Figure 7: Aeolus software implementation architecture on
top of proactive solutions.
4.2 Host implementation
To evaluate the benets of Aeolus to augment proactive solutions,
we have implemented a prototype of Aeolus with two recent proac-
tive transports, ExpressPass [14] and Homa [29]. Our implementa-
tion is based on DPDK 18.05 [2], which allows the network stack
to bypass the kernel and communicate directly with the NIC.
As shown in Figure 7, the main modication of Aeolus on top
of existing proactive transports is to add an Aeolus control logic, a
ow classication module, a packet marking module and a packet
dispatch module. As our implementation does not touch the core
code of proactive transports, dierent proactive protocols can be
“swapped out” easily while still remaining compatibility with Aeo-
lus.
Packet sending pipeline: As shown in Figure 7(a), application
starts data transmission by calling a send() function.The ow clas-
sication module tracks the per-ow state using a table. Each
ow is identied using the 5-tuple (i.e., source/destination IPs,
source/destination ports and protocol ID), and initially classied
as pre-credit ow. The ow enters credit-induced state once it n-
ishes its rst-RTT packet transmission. Pre-credit ows and credit-
induced ows are processed by the Aeolus control logic and the
proactive control logic separately.
The Aeolus control logic checks sender buers of its belonging
ows iteratively in a round robin fashion, reading in the data, seg-
menting the data into unscheduled packets, constructing request,
probe and ACK, and forwarding these packets to the next-stage
processing module. As for the proactive control logic, it follows its
original processing logic without any modication.
We note that in the design of some proactive solutions like Homa,
the receiver needs to learn about the ow size information from the
header of successfully received unscheduled packet(s). We encode
the ow size information in the header of probe such that the
receiver can still learn about the ow demand even when all the
unscheduled packets get dropped.
The packet marking module marks outgoing packets. It sets
ECN elds of unscheduled packets and scheduled packets to 00
(Non-ECT), and 10 (ECT(0)), respectively. To increase throughput,
the marked packets are sent to the TX ring buer of NIC in batch
via DPDK. We choose a batch size of 15 packets in our current
implementation.
Web
Server [31]
81%
19%
0%
64KB
Cache
Follower [31]
53%
18%
29%
701KB
0 - 100KB
100KB - 1MB
> 1MB
Ave. ow size
Table 2: Flow size distributions of realistic workloads.
Web
Data
Search [9] Mining [17]
83%
8%
9%
7.41MB
52%
18%
20%
1.6MB
Packet receiving pipeline: We leverage DPDK poll model driver
to periodically poll the RX Ring buer of NIC. Once a batch of
packets are received, the packet dispatch module will distribute
them to the corresponding control logic.
The Aeolus control logic mainly performs three operations on
receipt of a packet: (1) notify the ow classication module to
change the state of a ow in case an ACK of a ow is received
for the rst time; (2) notify proactive control logic the arrival of a
new ow when a request is received; (3) do loss detection based on
received ACKs and notify the proactive control logic to perform
loss retransmission with scheduled packets.
5 EVALUATION
We evaluate Aeolus using a combination of large-scale simulations
and testbed experiments. The key ndings are:
• Aeolus improves the normal-case FCT of ExpressPass [14],
with the mean FCT reduced by up to 56%.
• Aeolus improves the tail FCT of Homa [29], with the 99th
percentile FCT reduced by up to 1400⇥.
• Aeolus preserves the superior performance of NDP [18]
without requiring switch modications.
5.1 Evaluation setup
Choices of baseline proactive transport: We choose three re-
cent proactive transport solutions: ExpressPass [14], Homa [29]
and NDP [18], to represent dierent design choices of proactive
transport. ExpressPass forbids data transmissions in the rst RTT,
and Homa and NDP blindly send unscheduled packets in the rst
RTT.
Testbed: We built a small testbed that consists of 8 servers con-
nected to a Mellanox SN2000 switch using 10Gbps links. Our switch
supports ECN and strict priority queueing with 8 queues. Each
server is equipped with an Intel 82599EB 10GbE NIC that supports
DPDK. We enable RED/ECN at the switch to implement selective
dropping. The base RTT is around 14us. For ExpressPass [14], the
conguration is simple as we only have a single queue to trans-
mit data packets, including scheduled packets and unscheduled
packets. In contrast, Homa [29] uses multiple priority queues to
serve unscheduled and scheduled packets separately. As a result,
conguring per queue selective dropping at switches can no longer
protect scheduled packets from the impact of unscheduled packets.
For Homa, we congure per-port ECN/RED [12].
Simulator: For all the three schemes, we use the simulators pro-
vided by the authors with their recommended conguration options.
For ExpressPass, we implemented Aeolus on top of ExpressPass’s
open source code [3] with ns-2 simulator. For Homa, we imple-
mented Aeolus on top of Homa’s open source code [6] with OM-
NeT++ simulator. Homa assumes innite switch buer in its sim-
ulations, and its simulator lacks loss recovery mechanism. Hence,
we extended Homa’s simulator to implement a timeout-based loss
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
S. Hu et al.
(a) MCT with 30KB message size
(b) Mean MCT with dierent messages
sizes
Figure 8: Message completion times (MCT) of 7-to-1 incast.
The message size varies from 30KB to 50KB.
(a) Web Server: 0-100KB
(b) Cache Follower: 0-100KB