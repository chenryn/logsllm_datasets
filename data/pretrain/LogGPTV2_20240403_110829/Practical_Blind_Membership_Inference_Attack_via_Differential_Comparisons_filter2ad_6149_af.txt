85.5 ± 0.47
91.0 ± 0.17
98.9 ± 0.49
94.8 ± 0.14
88.2 ± 0.93
98.8 ± 0.68
93.2 ± 0.82
90.8 ± 0.68
97.6 ± 0.39
94.0 ± 0.47
90.4 ± 1.02
96.2 ± 0.27
93.2 ± 0.46
90.4 ± 0.86
96.3 ± 0.15
93.2 ± 0.80
97.2 ± 1.70
86.2 ± 3.55
92.8 ± 1.72
90.7 ± 0.86
97.4 ± 0.37
93.9 ± 0.63
86.1 ± 0.78
98.9 ± 0.60
92.1 ± 1.15
83.5 ± 0.40
98.9 ± 0.69
90.6 ± 0.52
90.6 ± 0.68
97.3 ± 0.42
93.8 ±0.53
84.9 ± 8.05
90.8 ± 3.06
87.1 ± 3.39
75.3 ± 0.47
99.1 ± 0.47
85.5 ± 0.47
91.1 ± 0.17
98.9 ± 0.49
94.8 ± 0.14
88.2 ± 0.93
98.8 ± 0.68
93.2 ± 0.82
90.8 ± 0.68
97.6 ± 0.39
94.0 ± 0.47
Birds-200
95.1 ± 1.90
53.5 ± 27.5
58.3 ± 27.4
98.1 ± 0.08
60.3 ± 23.1
68.6 ± 21.3
99.9 ± 0.06
57.9 ± 0.81
71.4 ± 0.65
95.5 ± 0.08
98.2 ± 0.26
96.8 ± 0.09
94.5 ± 0.24
98.0 ± 0.80
96.2 ± 0.26
83.8 ± 11.9
99.1 ± 0.60
90.3 ± 6.77
94.9 ± 3.55
63.8 ± 35.9
69.5 ± 25.6
98.4 ± 0.15
57.6 ± 17.1
71.2 ± 13.7
76.1 ± 1.26
99.9 ± 0.01
86.4 ± 0.81
95.3 ± 0.49
99.2 ± 0.59
97.2 ± 0.03
93.9 ± 0.50
99.0 ± 0.76
96.4 ± 0.09
95.8 ± 0.53
97.1 ± 1.88
96.4 ± 0.33
95.7 ± 0.56
97.8 ± 0.05
96.8 ± 0.28
89.9 ± 0.08
96.8 ± 0.14
93.2 ± 0.03
99.9 ± 0.06
57.9 ± 0.81
71.4 ± 0.65
95.5 ± 0.08
98.2 ± 0.26
96.8 ± 0.09
94.5 ± 0.24
98.0 ± 0.80
96.2 ± 0.26
83.8 ± 11.9
99.1 ± 0.60
90.3 ± 6.77
95.7 ± 0.48
98.1 ±0.13
96.9 ± 0.18
75.4 ± 1.26
98.2 ± 0.23
85.3 ± 0.89
76.1 ±1.26
57.9 ± 0.81
86.4 ± 0.81
95.3 ± 0.49
98.2 ± 0.26
97.2 ± 0.03
93.9 ± 0.50
99.0 ± 0.76
96.4 ± 0.09
95.8 ± 0.53
97.4 ± 0.05
96.4 ± 0.33
target dataset, BLINDMI-DIFF-w/o is an alternative option as
opposed to BLINDMI-DIFF-w/.
[Observation RQ1-5] The variation of BLINDMI-1CLASS
is larger than the one of BLINDMI-DIFF.
Take Birds-200 for example. The best performance of
BLINDMI-1CLASS is higher than the one of BLINDMI-DIFF.
The reason is that the performance of ML classiﬁer depends on
the training data: If many data samples lie along the decision
boundary,
the one-class model can learn the membership
semantics and thus outperforms BLINDMI-DIFF.
C. RQ2: Defenses
In this
subsection, we evaluate the performance of
BLINDMI against state-of-the-art defenses of MI attacks. Note
that we evaluate all the defenses under the blackbox setting
because some attacks only work under the blackbox but not
the blind setting. Now, we describe three general defense
directions and representative works in each direction below.
• Output probability alteration based on adversarial exam-
ple.
Such a defense alters the output probabilities so
that it becomes hard for an adversary to infer membership
information. A representative approach in this category is
called MemGuard [26], which changes the output probability
distribution so that it looks like an adversarial example to
the inference model built by the adversary. We adopt the
original implementation of MemGuard.8
• Regularization-based fortiﬁcation of ML model.
Such a
defense fortiﬁes existing ML models, especially DNN, via
regularization. Two representative approaches in this cate-
gory are MMD+Mix-up [30] (which include two previous
defenses, namely dropout [49] and L2-Regularizer [41])
and the adversarial regularization [36]. We implement our
own version of MMD+Mix-up and adopt an open-source
version of the adversarial regularization.9 Note that
the
MMD+Mixup defense is adaptive with a regularizer based
on BlindMI, i.e., minimizing the cluster distance between
members and non-members during the training process.
As a comparison, the adversarial regularization is based
on the NN attack, because it requires that the MI attack
be differentiable with gradients while BlindMI is not. (If
we adopt a differentiable distance function in adversarial
regularization, adversarial regularization boils down to the
MMD+Mixup method.)
• Differential privacy-based protection. Such a defense adds
noise to the output to fool an adversary. A representative
approach in this category is DP-Adam [1] and we adopt an
open-source version of DP-Adam.10
Note that in our experiment, we adopt a dataset that is
at least included in the corresponding defense paper for our
evaluation. That is, we choose CH-MNIST for MemGuard and
DP-Adam, and CIFAR-100 for MMD+Mix-up and Adversar-
ial Regularization. Because these defenses adopted different
datasets in the paper and we follow what what adopted.
1) Attacks against MemGuard: We ﬁrst evaluate the per-
formance of MemGuard under existing MI attacks. The utility-
is set up as [0, 0.1, 0.3, 0.5, 0.7, 1.0] for
loss budget
8https://github.com/jjy1994/MemGuard
9https://github.com/NNToan-apcs/python-DP-DL
10https://github.com/tensorﬂow/privacy
MemGuard, which represents the percentage of altered outputs.
We show the evaluation results in Figure 3(a) and also make
the following observations.
[Observation RQ2-1] Attacks with ground-truth labels gen-
erally have a higher F1-score than those without when attack-
ing MemGuard.
Our ﬁrst observation is that MemGuard is generally vul-
nerable to attacks that utilize ground-truth labels. For example,
the worst performing attacks are Top1-Threshold and Top3-
NN, which will likely remove the output probability of the
ground-truth labels. By contrast, the best performed attacks
are Label-only, Top2+True, and BLINDMI, which are all able
to utilize the ground-truth label information. The reason is that
although MemGuard alters the output probabilities, it does not
change the prediction class because MemGuard does not want
to inﬂuence the legacy performance of the model.
There are two more things worth noting. First, the perfor-
mance of NN is actually better than the one of Top3-NN. The
reason is similarly: NN adopts all the probability scores of the
output, which contains the one corresponding to the ground-
truth label for certain; by contrast, Top3-NN only adopts the
top three probability scores, which may not contains the one
corresponding to the ground-truth label.
Second, Top2+True performs better than Label-only when
the privacy budget is small, but then degrades quickly when
the privacy budget increases. The reason is that when the
budget is small, top two probability scores will provide some
membership information. When the budget increases, the top
two of more samples are altered, which affects the performance
of Top2+True.
[Observation RQ2-2] BLINDMI still outperforms all exist-
ing attacks even if the output probabilities were adversarially
altered.
Our second observation is that BLINDMI still performs the
best among all attacks. The underlying reasons are two-fold.
First, although adversarial examples are close to the decision
boundary, the decision boundary itself is a hyper-dimensional
manifold and the projection of members and non-members
on the manifold are still far from each other,
thus being
distinguishable. Second, although MemGuard alters output
probability scores, sufﬁcient information still exist, because
MemGuard does not change the prediction results.
2) Attacks against DP-Adam: In this part, we evaluate all
existing attacks against DP-Adam under the noise multiplier
as [0, 0.002, 0.004, 0.006, 0.008, 0.01]. The evaluation results
are shown in Figure 3(b) and we make the following observa-
tions.
[Observation RQ2-3] Attacks relying on binary comparison
tend to have a low F1-score against DP-Adam.
The reason behind this observation is that differential
privacy (DP) perpetuates the probability outputs so that the
boundary between members and non-members is blurred.
Therefore,
the performances of Top1-Threshold and Loss-
Threshold are the worst. Consider Top1-Threshold for exam-
ple: It is hard to differentiate members and non-members based
on a single threshold of the highest output probability score
due to the perpetuation enforced by DP.
[Observation RQ2-4] BLINDMI has a higher performance
than Label-only regardless of when the privacy-utility budget
10
is small or large, while Label-only attack degrades slower with
a mid-size budget.
The reason is that Label-only attack depends on the per-
formance gap of the target model on the training and testing
datasets. Such a gap persists with a mid-size budget, but starts
to shrink quickly for a large budget—and that is why Label-
only’s performance degrades ﬁnally with a large budget.
3) Attacks against MMD+Mixup: In this part, we evaluate
all the MI attacks against MMD+Mixup with different privacy-
utility budgets, i.e., the loss weight in the MMD as [0, 0.1, 0.5,
1, 2.5, 5]. Note that this budget controls the tradeoff between
privacy and utility: A larger privacy-utility budget increases
privacy protection, but at the same time decreases the model’s
utility. The evaluation results are in Figure 3(c)—BLINDMI
clearly outperforms all existing attacks. We also make the
following observation.
[Observation RQ2-5] Attacks selecting more probability
scores generally have a higher F1-score than those selecting
less when attacking MMD+Mix-up.
Speciﬁcally, Label-Only and Top1-Threshold are the worst
when comparing with other attacks. The reason is that
MMD+Mix-up also changes the target model’s performance
on the training dataset and therefore Label-Only and Top1-
Threshold are heavily affected. As a comparison, other attacks
also rely on the probability score of other classes, thus outper-
forming these two attacks.
4) Attacks against Adversarial Regularization: In this part,
we evaluate all the MI attacks against the Adversarial Regular-
ization [36] with different privacy-utility budget as [0, 0.3, 0.7,
1, 1.5, 2]. BLINDMI clearly outperforms all existing attacks
as shown in Figure 3(d). Now we describe our observations.
[Observation RQ2-6] Ground-truth Label plays an im-
portant role in defeating Adversarial Regularization and the
results depend on how such labels are used in the attack.
Speciﬁcally, Label-Only, Top2+True and BLINDMI, which
all adopt ground-truth labels, are the best three MI attacks
among all, while Loss-Threshold, which also adopts ground-
truth labels, is the worst. The reason is that Loss-Threshold
relies on the training data of a shadow model, which is
drastically different from a model
trained with adversarial
regularization.
[Observation RQ2-7] Simple MI attacks, except
BLINDMI, tend to have a better performance.
for
Speciﬁcally, Label-Only and Top1-Threshold performs
well compared with other attacks. The reason is that although
the adversarial regularization fortiﬁes the model via regular-
ization, the output probability, especially the probability score
of the predicted class, still contains abundant information.
D. RQ3: Nonmember Set Quality and Size
In this subsection, we evaluate how different generation
methods and size of non-members affect
the performance
of BLINDMI, particularly BLINDMI-DIFF and BLINDMI-
1CLASS. Without loss of generality, we use the EyePACS
dataset and the blind setting: the target dataset consists of
20,000 samples and the size of non-member datasets changes
from 20 to 10,000. Here are the settings used in the different
non-member generation methods:
• Sample transformation. We adopt the Sobel operator.
• Random perpetuation. We adopt Gaussian noise with the
mean value as zero and the variance as 0.001.
• Random generation. We adopt a uniform distribution in
generating feature values.
• Cross-domain sample. We adopt samples from CH-
MNIST.
We show our experiment results in Table IX and also make
the following observations.
[Observation RQ3-1] The performance of BLINDMI-DIFF
stays mostly stable with a little increase as opposed to a
big increase of BLINDMI-1CLASS as the size of nonmember
datasets increases.
Our ﬁrst observation of RQ3 is on how the size of
nonmembers affects F1-score. The F1-score of BLINDMI-
DIFF is almost constant with around 1% boost as the size
increases from 20 to 10,000. As a comparison, the F1-score of
BLINDMI-1CLASS has between 5% and 12% increase except
for random generated non-members.
The reason is that BLINDMI-1CLASS adopts a learning
model, particularly one-class SVM, which needs some training
data to learn the underlying semantics. As a comparison,
BLINDMI-DIFF directly compares the distribution between the
target and the non-member, which is effective in extracting
membership semantics from just a few samples.
[Observation RQ3-2] The quality of sample transformation
is the best, while the random generation of non-members is the
worst among all four methods.
Our second observation is on how different nonmember
generation methods affect attack performance. Table IX shows
that sample transformation is the most effective method for
both BLINDMI-DIFF and BLINDMI-1CLASS. Since the dif-
ferences are relatively small, we perform two statistical tests,
i.e., (i) the Mann-Whitney U test and the P-value, and (ii) the
maximum mean discrepancy (MMD) tests, to demonstrate the
statistical signiﬁcance.
First, the U test value and P-value are shown in Table X.
A large U test value and P-value indicates that two sets are
similar, indicating statistical insigniﬁcance. Random genera-
tion is signiﬁcantly different from all three other methods;
Cross-domain sample selection is more similar to random
perpetuation than sample transformation.
Second, we show the MMD value with standard error
of the mean between generated samples and real-world non-
members. A smaller MMD value indicates that the gener-
ated samples are close to real-world nonmembers. Clearly,
nonmember generated by the sample transformation is the
closest to real-world nonmembers; those generated randomly
are the farthest—the MMD value is even larger than the one of
members. The reason is that random generated samples follow
a uniform distribution, which are far from the member and
non-member boundary.
E. RQ4: BLINDMI-DIFF with different classiﬁers and kernel