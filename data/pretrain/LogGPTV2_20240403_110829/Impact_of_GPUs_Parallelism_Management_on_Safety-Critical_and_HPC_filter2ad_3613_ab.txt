block  for  a  total  of  480  threads  while  for  the  C2050,  14 
blocks of 32 threads each can be executed in parallel, for a 
total  of  448  threads.  If  more  threads  or  blocks  are 
instantiated, their execution will be delayed until they can be 
dispatched, as described in the previous section. 
While the Devices Under Test (DUTs) are neither used in 
HPC nor in safety critical applications, their architecture and 
parallelism  management is  very  similar to the one of  more 
powerful  devices  (like  Kepler  GPUs  used 
in  HPC 
applications) or of low-power devices (like TEGRA System 
on Chip, used in automotive or aerospace applications). The 
choice of NVIDIA products is dictated by their diffusion in 
the  marked.  Nevertheless,  the  proposed  test  methodology 
and the achieved results are easily extendable to other GPUs 
families and vendors. 
The  C2050  is  equipped  with  an  ECC  mechanism 
designed for HPC applications. In the presented results, the 
ECC was disabled. The proposed experiments are aimed at 
evaluating the parallelism management variations effects on 
the  GPU,  which  includes  caches  and  internal  registers. 
Enabling the ECC would mask the contribution of errors in 
memory resources to the GPU error rate. Moreover, the ECC 
introduce 
the 
performances of the board of up to 30% and reducing the on-
board memory availability of 15% [23]. Such overhead may 
impact the scheduling and parallelism management in a non-
predictable  way,  as  no  detailed  information  about  the  ECC 
mechanism is available.  
a  non-negligible  overhead, 
reducing 
457
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
B.  Neutron Beams 
Experiments  were  performed  at  Los  Alamos  National 
Laboratory’s  (LANL)  Los  Alamos  Neutron  Science  Center 
(LANSCE) Irradiation of Chips and Electronics House II, in 
September  2012  and  in  the  VESUVIO  beam  line  in  ISIS, 
Rutherford Appleton Laboratories, Didcot, UK, in December 
and May 2013. As shown in Fig. 3, both of these facilities 
provide  a  white  neutron  source  that  emulates  the  energy 
spectrum of the atmospheric neutron flux. The ISIS spectrum 
has a lower component of high-energy neutrons with respect 
to  the  LANSCE  and  the  terrestrial  one.  The  relationship 
between neutron energy and modern devices cross section is 
still  an  open  question.  Nevertheless,  ISIS  beam  has  been 
empirically  demonstrated  to  be  suitable  to  mimic  the 
LANSCE one and the terrestrial radiation environment [24]. 
To  limit  experimental  error  in  the  following  discussion  a 
direct  comparison  will  be  made  only  among  experiments 
performed in the same facility and in the same time slot. 
The  neutron  flux  was  approximately  1x106 n/(cm2·s)  in 
LANSCE  and  4x104  n/(cm2·s)  in  ISIS  for  energies  above 
10MeV.  It  is  worth  noting  that  even  if  the  used  neutron 
fluxes are several orders of magnitude higher than the natural 
one  (which  is  of  about  13  n/(cm2·s)  at  sea  level  [25]),  the 
experiments  were  carefully  tuned  to  make  negligible  the 
probability  of  having  more  than  one  neutron  generating  a 
failure in a single code execution (observed error rates were 
actually  lower  than  10-2  errors/execution).  This  allows  the 
scaling  of  experimental  data  in  the  natural  radioactive 
environment without introducing artificial behaviors. 
The  beam  was  focused  on  a  spot  with  a  diameter  of  2 
inches  plus  1  inch  of  penumbra,  which  provided  uniform 
irradiation of the GPU chip without directly affecting nearby 
board  power  control  circuitry  and  DDR  memories.  Even  if 
the  beam  is  collimated,  scattering  neutron  may  be  found 
outside the beam spot, thus to ensure that the DDR content 
was consistent during our experience, we periodically check 
it and no error has been observed. As input and output data 
were stored in the DDR, the errors reported in the following 
sections were only caused by the corruption of the GPU core 
resources.  This  also  means  that  transmitted  data  between 
GPU and DDR are considered only inside the GPU chip. 
On a realistic application, a higher number of blocks may 
extend the exposure time of input or output data in the DDR, 
increasing  the  probability  of  having  them  corrupted  by 
neutrons.  However,  DDR  sensitivity  has  been  proved  to 
decrease  with  the  shrinking  of  technology  nodes  [26],  and 
modern  DDR  chips  are  provided  with  efficient  ECC  that 
increase  by  several  orders  of  magnitudes 
the  device 
reliability [27]. 
Irradiation  was  performed  at  room  temperature  with 
normal incidence and nominal voltages. 
C.  Radiation Test Protocol 
A desktop PC controlled the DUT through a 20 cm PCI-
Express  bus  extension  with  fuses  to  prevent  latchups  from 
compromising  the  DUT  and  PC  functionalities.  No  latchup 
was observed during the overall test experience. 
The only role of the CPU was to initialize the GPU and 
gather  test  results.  The  sequence  of  operations  required  to 
458
Figure 3: LANSCE, ISIS, and TRIUMF neutrons spectra compared to 
the terrestrial one at sea level multiplied by 107 and 108 [24]. 
GPU through the PCI-Express bus; 
execute a test on the GPU can be detailed as follows: 
x  Initialization:  the  PC  loads  instructions  and  data  on  the 
x  Test:  the  PC  triggers  the  GPU  with  the  cuda  thread 
synchronize  command.  The  GPU  actually  executes  the 
code while the PC is in idle state. When the test finishes, 
the GPU stores the results in the DDR. 
x  Readback:  the  PC,  using  cuda  memory  copy  operations, 
downloads through the PCI-Express bus the experimental 
data from the GPU DDR, and checks for mismatches. 
 Due to the high frequency of operation of both the PCI-
Express  bus  and  the  CPU,  test  initialization  and  result 
gathering  are  performed  on  the  order  of  milliseconds.  We 
believe these operations are quick enough that it is negligible 
the probability that radiation effects would corrupt the GPU 
during these calculations.  Observed error rates were, in fact, 
lower than 10-1 errors/s. 
IV.  SCHEDULERS IMPACT ON GPU CROSS SECTION 
A.  Benchmark Codes and Thread Distributions 
Two 
synthetic  benchmark  codes  were  designed 
specifically  to  evaluate  the  impact  of  blocks  and  warps 
scheduling  and  dispatching  on  the  execution  of  a  parallel 
code:  sums  and  mults.  In  sums  (mults)  every  instantiated 
thread performs 10,000 sums (multiplications) with random 
double-precision  floating-point  data.  Each  thread  executed 
its  operations  on  dedicated 
internal  registers,  without 
requiring  the  use  of  cache  or  shared  memory.  The  threads 
independency is maintained, as threads do not interact with 
each  other.  Having  simple  codes  avoid  the  introduction  of 
undesired  secondary  effects  or  latencies  that  may  mask  or 
affect  the  scheduling  strain  variations  that  are  being 
investigated. 
Two  sets  of  thread  distributions  were  designed  for  the 
GTX480:  one  in  which  the  block  size  was  set  to  32  (the 
maximum number of threads a SM can execute in parallel in 
CUDA 2.0 devices) and the number of blocks varied from 30 
to 300, and the other in which the number of blocks was set 
to  512  and  the  block  size  varied  from  64  to  1024  (the 
maximum number of threads instances in  a single block in 
CUDA 2.0 devices). Both of these conditions place different 
strains to the  GPU schedulers. The first set of distributions 
exacerbates  the  blocks  assignment  scheduling,  while  the 
second intensifies the SM internal scheduling.  
Tab. I and Tab. III list the two sets of tested distributions, 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:48:15 UTC from IEEE Xplore.  Restrictions apply. 
EXECUTION TIME FOR THE DIFFERENT THREAD DISTRIBUTIONS 
(BLOCK SIZE FIXED TO 32) 
EXECUTION TIME FOR THE DIFFERENT THREAD DISTRIBUTIONS 
(GRID SIZE FIXED TO 512) 
B32G30 
B32G150 
B32G300 
Block 
size 
32 
32 
32 
tmults 
[ms] 
0.8 
2.4 
4.4 
t/threads 
[10-4ms] 
7.8 
4.9 
4.4 
Block 
size 
64 
256 
1024 
B64G512 
B256G512 
B1024G512 
TABLE I 
tsums 
[ms] 
0.8 
2.4 
4.2 
Grid 
size 
30 
150 
300 
TABLE II 
EXPERIMENTAL CROSS SECTIONS 
B32G30 
B32G150 
B32G300 
σsums [10-6cm2] 
0.73±10% 
5.13±7% 
14.37±7% 
18
16
14
12
10
8
6
4
2
0
]
2
m
c
6
-
0
1
[
n
o
i
t
c
e
S
s
s
o
r
C
TABLE III 
tsums 
[ms] 
14.6 
50.9 
199.9 
Grid 
size 
512 
512 
512 
TABLE IV 
σsums [10-6cm2] 
13.30±6% 
14.79±5% 
20.81±5% 
EXPERIMENTAL CROSS SECTIONS 
tmults 
[ms] 
14.5 
51.2 
200.2 
t/threads 
[10-4ms] 
4.4 
3.9 
3.8 
σmults [10-6cm2] 
9.93±6% 
12.07±5% 
16.52±5% 
mults
sum
sums
mult
σmults [10-6cm2] 
0.66±10% 
3.65±7% 
9.40±7% 
mults
sum
sums
mult
B64G512 
B256G512 
B1024G512 
]
2
m
c
6
-
0
1
[
n
o
i
t
c
e
S
s
s
o
r
C
25
20
15
10
5
B32G30
B32G150
B32G300
Figure  4:  cross  sections  of  sums  and  mults  when  the  block  size 
(number of threads per block) is fixed to 32. 
labeled BxGy, where x is the block size (i.e., the number of 
threads per block) and y is the grid size (i.e., the number of 
blocks).  The  number  of  instantiated  threads  (obtained 
multiplying  the  grid  size  and  block  size)  is  higher  in  the 
distribution with a fixed number of blocks with respect to the 
fixed  number  of  threads  per  block  one.  Smaller  grid  sizes 
were  actually  tested  with  an  increased  block  size,  but  they 
did not provide a statistically significant amount of data. The 
fourth  and  fifth  columns  of  Tab.  I  and  Tab.  III  report  the 
execution time of sums and mults in each distribution. Please 
note that just the GPU kernel execution time was measured, 
not considering the initialization and readback time. 
The instructions each thread is in charge of executing are 
the  same  in  all  the  distributions.  Moreover,  no  shared 
resource is instantiated as each thread is completely isolated 
and  works  with  dedicated  registers.  This  ensures  a  linear 
increment  of  the  resources  used  by  threads  with  the  code 
throughput while scheduling strain will depend only on the 
number of blocks and warps to dispatch. 
fully  exploit 
All  distributions 
the  GPU  parallel 
capabilities. Grid sizes are multiple of 15 (i.e., the number of 
SMs available in the GTX480) and block sizes are multiple 
of 32 (i.e. the number of CUDA cores in each SM).  This is 
essential to guarantee that, at a given time, the GPU is fully 
used  and  avoid  the  measure  of  lower  cross  section  caused 
only by the exposure of a lower GPU area. 
The  execution  times  of  both  sums  and  mults  increase 
when  the  number  of  threads  increases.  Nevertheless,  the 
average  time  for  executing  a  threads  in  sums  and  mults, 
calculated dividing the code execution time by the number of 
0
B64G512
B256G512
Figure  5:  cross  sections  of  sums  and  mults  when  the  grid  size 