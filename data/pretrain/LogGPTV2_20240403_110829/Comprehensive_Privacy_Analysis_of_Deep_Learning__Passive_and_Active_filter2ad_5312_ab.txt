the training dataset, the gradient of the loss ∂L
∂W over the data
record is pushed towards zero, after each round of training.
This is exactly what we can exploit to extract information
about a model’s training data.
to all parameters
For a target data record (x, y), the adversary can compute
the loss of the model L(f (x; W), y), and can compute the
∂L
gradients of the loss with respect
∂W
using a simple back-propagation algorithm. Given the large
number of parameters used in deep neural networks (millions
of parameters),
the vector with such a signiﬁcantly large
dimension cannot properly generalize over the training data
(which in many cases is an order of magnitude smaller in
size). Therefore, the distribution of the model’s gradients on
members of its training data, versus non-members, is likely
to be distinguishable. This can help the adversary to run
an accurate membership inference attack, even though the
classiﬁcation model (with respect to its predictions) is well-
generalized.
Inference model. We illustrate the membership inference
attack in Figure 1. The signiﬁcance of gradient (as well as
activation) computations for a membership inference attack
varies over the layers of a deep neural network. The ﬁrst layers
tend to contain less information about the speciﬁc data points
in the training set, compared to non-member data record from
the same underlying distribution. We can provide the gradients
and activations of each layer as separate inputs to the attacker,
as the attacker might need to design a speciﬁc attack for each
layer. This enables the inference attack to split the inference
task across different layers of the model, and then combine
them to determine the membership. This engineering of the
attack model architecture empowers the inference attack, as it
x
W1
W2
··· Wi
target model
Fig. 1: The architecture of our white-box inference attack. Given
target data (x, y), the objective of the attack is to determine its
membership in the training set D of target model f. The attacker
runs the target model f on the target input x, and computes all the
hidden layers hi(x), the model’s output f (x), and the loss function
L(f (x), y; W), in a forward pass. The attacker also computes the
gradient of the loss with respect to the parameters of each layer
∂L
∂Wi , in a backward pass. These computations, in addition to the one-
hot encoding of the true label y, construct the input features of the
inference attack. The attack model consists of convolutional neural
network (CNN) components and fully connected network (FCN)
components. For attacking federated learning and ﬁne-tuning, the
attacker observes each attack feature T times, and stacks them before
they are passed to the corresponding attack component. For example,
the loss features are composed as L = {L{1}, L{2},··· , L{T}}).
The outputs of the CNN and FCN components are appended together,
and this vector is passed to a fully connected encoder. The output
of the encoder, which is a single value, is the attack output. This
represents an embedding of the membership information in a single
value. In the supervised attack setting, this embedding is trained to
be Pr{(x, y) ∈ D}. In the unsupervised setting, a decoder is trained
to reconstruct important features of the attack input (such as the
model’s output uncertainty H(f (x)), and the norm of its gradients
(cid:2)
(cid:2)
(cid:2) ∂L
(cid:2)) from the attack output. This is similar to deep auto-encoders.
∂W
All unspeciﬁed attack layers are fully connected. The details of the
architecture of the attack is presented in Table XIV in Appendix A.
(cid:24)(cid:21)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
reduces the capacity of the attack model and helps ﬁnding the
optimal attack algorithm with less background data.
,··· ,
inputs to the attack model are the set of
The distinct
, ∂L
∂L
gradients
the set of activation vectors for
different layers h1(x), h2(x),··· , the model output f (x), the
∂W2
∂W1
one-hot encoding of the label y, and the loss of the model on
the target data L(f (x; W), y). Each of these are separately
fed into the attack model, and are analyzed separately using
independent components.
Inference attack components. The attack model is composed
of feature extraction components and an encoder component.
To extract features from the output of each layer, plus the
one-hot encoding of the true label and the loss, we use fully
connected network (FCN) submodules with one hidden layer.
We use convolutional neural network (CNN) submodules for
the gradients. When the gradients are computed on fully
connected layers (in the target model), we set the size of the
convolutional kernel to the input size of the fully connected
layer,
to capture the correlation of the gradients in each
activation function. We reshape the output of each submodule
component into a ﬂat vector, and then concatenate the output
of all components. We combine the outputs of all attack
feature extraction components using a fully connected encoder
component with multiple hidden layers. The output of the
encoder is a single score, which is the output of the attack.
This score (in the supervised attack raining) predicts the
membership probability of the input data.
B. Inference Target: Stand-alone vs. Federated Learning
There are two major types of training algorithms for deep
learning, depending on whether the training data is available
all in one place (i.e., stand-alone centralized training), or it
is distributed among multiple parties who do not trust each
other (i.e., federated learning) [8]. In both cases, the attacker
could be the entity who obtains the ﬁnal trained model. In
addition to such attack setting, the attacker might observe an
updated version of the model after ﬁne-tuning, for instance,
which is very common in deep learning. Besides,
in the
case of federated learning, the attacker can be an entity who
participates in the training. The settings of ﬁne-tunning and
federated learning are depicted in Table I.
Stand-alone ﬁne-tunning. A model f is trained on dataset
D. At a later stage it is updated to fΔ after being ﬁne-tuned
using a new dataset DΔ. If the attacker observes the ﬁnal
outcome, we want to measure the information leakage of the
ﬁnal model fΔ about the whole training set D∪DΔ. However,
given that two versions of the model exist (before and after
ﬁne-tuning), we are also interested in measuring the extra
information that could be learned about the training data, from
the two model snapshots. The attacker might also be interested
only in recovering information about the new set DΔ. This is
very relevant in numerous cases where the original model is
trained using some unlabeled (and perhaps public) data, and
then it is ﬁne-tunned using sensitive private labeled data.
The model for inference attacks against ﬁne-tunned models
is a special case of our membership inference model for at-
tacking federated learning. In both cases, the attacker observes
multiple versions of the target model.
Federated learning. In this setting, N participants, who have
different training sets Di, agree on a single deep learning task
and model architecture to train a global model. A central server
keeps the latest version of the parameters W for the global
model. Each participant has a local model, hence a local set
of parameters Wi. In each epoch of training, each participant
downloads the global parameters, updates them locally using
SGD algorithm on their local training data, and uploads them
back to the server. The parameter server computes the average
value for each parameter using the uploaded parameters by
all participants. This collaborative training continues until the
global model converges.
{t}
i
{t}
i
There are two possibilities for the position of the attacker
in federated learning: The adversary can be the centralized pa-
rameter server, or one of the participants. A curious parameter
server can receive updates from each individual participant
over time W
, and use them to infer information about
the training set of each participant. A malicious parameter
server can also control the view of each participant on the
global model, and can act actively to extract more information
about the training set of a participant (as we discuss under
active attacks). Alternatively, the adversary can be one of
the participants. An adversarial participant can only observe
the global parameters over time W {t}
, and craft his own
adversarial parameter updates W
to gain more information
about the union of the training data of all other participants.
In either of these cases, the adversary observes multiple
versions of the target model over time. The adversary can try
to run an independent membership inference attack on each of
these models, and then combine their results. This, however,
might not capture the dependencies between parameter values
over time, which can leak information about the training data.
Instead, in our design we make use of a single inference model,
where each attack component (e.g., components for gradients
of layer i) processes all of its corresponding inputs over the
observed models at once. This is illustrated in Figure 1. For
example, for the attack component that analyzes the loss value
L, the input dimension can be 1× T , if the adversary observes
T versions of the target model over time. The output of the
attack component is also T times larger than the case of
attacking a stand-alone model. These correlated outputs, of all
attack components, are processed all at once by the inference
model.
C. Attack Mode: Passive vs. Active Inference Attack
The inference attacks are mostly passive, where the ad-
versary makes observations without modifying the learning
process. This is the case notably for attacking models after
the training is over, e.g., the stand-alone setting.
Active attacks. The adversary, who is participating in the
training process, can actively inﬂuence the target model in
order to extract more information about its training set. This
could be the case notably for running inference attacks against
(cid:24)(cid:21)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
federated learning. In this setting, the central parameter server
or a curious participant can craft adversarial parameter up-
dates for a follow-up inference attack. The inference model
architecture will be the same for passive and active attacks.
The active attacker can exploit the SGD algorithm to run the
active attack. The insight we use to design our attack is that
the SGD algorithm forcefully decreases the gradient of the loss
on the training data, with the hope that this generalizes to the
test data as well. The amount of the changes depends on the
contribution of a data point in the loss. So, if a training data
point leads to a large loss, the SGD algorithm will inﬂuence
some parameters to adapt themselves towards reducing the loss
on this point. If the data point is not seen by the model during
training, the changes in the gradient on this point is gradual
throughout the training. This is what we exploit in our active
membership inference attack.
Let x be a data record, which is targeted by the adversary
to determine its membership. Let us assume the adversary is
one of the participants. The attacker runs a gradient ascent on
x, and updates its local model parameters in the direction of
increasing the loss on x. This can simply be done by adding
the gradient to the parameters,
W ← W + γ
∂Lx
∂W
,
(2)
where γ is the adversarial update rate. The adversary then
uploads the adversarially computed parameters to the central
server, who will aggregate them with the parameter updates
from other participants. The adversary can run this attack on
a batch of target data points all at the same time.
If the target record x is in the training set of a participant,
its local SGD algorithm abruptly reduces the gradient of the
loss on x. This can be detected by the inference model, and
be used to distinguish members from non-members. Repeated
active attacks, which happens in federated learning, lead to
high conﬁdence inference attacks.
D. Prior Knowledge: Supervised vs. Unsupervised Inference
To construct his inference attack model, the adversary needs
to ﬁnd the meaningful mapping between the model’s behavior
on a data point and its membership in the training set. The
most straightforward way of learning such relationship is
through some known members of the training data, and some
data points from the same distribution which are not in the
training data set. This is illustrated in Table I. The adversary
has a dataset D(cid:3)
that overlaps with the target dataset D. Given
this dataset, he can train the attack model in a supervised way,
and use it to attack the rest of the training dataset.
Let h be the inference attack model. In the supervised
setting, we minimize the (mean square) loss of the attacker
for predicting the membership of the data points in its training
set D(cid:3)
:
(cid:5)
d∈D(cid:2)∩D
(h(d) − 1)2 +
(cid:5)
d∈D(cid:2)\D
(h(d))2
(3)
If the adversary does not have known samples from the
there are two possibilities for training
training set,
target
the inference attack models: supervised training on shadow
models [6], and unsupervised training on the target model.
Shadow models are models with the same architecture as the
target model. The training data records for the shadow models
are generated from the same distribution as the target training
data, but do not have a known overlap with the target training
set. The attacker trains the attack model on the shadow models.
As the behavior of the shadow models on their training data is
more or less similar to the behavior of the target model on its
training data, the attack models trained on the shadow models
are empirically shown to be effective.
The attack output for (shadow) supervised training setting
is the probability of membership.
h(d) = Pr(d ∈ D; f )
(4)
Unsupervised training of inference models. We introduce
an alternative approach to shadow training, which is unsuper-
vised training of the attack model on the target model. The
assumption for this attack is that the attacker has access to a
dataset D(cid:3)
which partially overlaps with the target training set
D, however, the adversary does not know which data points
are in D(cid:3) ∩ D.
Our objective is to ﬁnd a score for each data point that rep-
resents its embedding in a space, which helps us easily separat-
ing members from non-members (using clustering algorithms).
The attack’s output should compute such representations. We
make use of an encoder-decoder architecture to achieve this.
This is very similar to the auto-encoders for unsupervised deep
learning. As shown in Figure 1, the output of the attack is fed
into a decoder. The decoder is a fully connected network with
one hidden layer.
The objective of the decoder is to reconstruct some key
features of the attack input which are important for member-
ship inference. These include the loss value L, whether the
target model has predicted the correct label 1y=arg max f (x),
the conﬁdence of the model on the correct label f (x)y, the
(cid:2)(cid:2). As previous work [6] as well
prediction uncertainty (entropy) of the model H(f (x)), and the
norm of the gradients
as our empirical results show, these features are strong signals
for distinguishing members from non-members. The encoder-
decoder architecture maximizes the information that the attack
output contains about
it generates a
membership embedding for each data point. Note that after
training the attack model, the decoder plays no role in the
membership inference attack.
these features. Thus,
(cid:2)(cid:2) ∂L
∂W
The attack in the unsupervised setting is a batch attack,
where the adversary attacks a large set of data records (disjoint
from his background knowledge). We will use the encoder to
for each target data record, and we compute the embedding
value (output of the encoder model). Next, we use a clustering
algorithm (e.g., we use the spectral clustering method) to
cluster each input of the target model in two clusters. Note
that the outcome of the clustering algorithm is a threshold, as
the attack output is a single number. We predict the cluster
with the larger gradient norm as non-members.
(cid:24)(cid:21)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:51:43 UTC from IEEE Xplore.  Restrictions apply. 
III. EXPERIMENTAL SETUP
C. Pre-trained Models
We implemented our attacks using Pytorch.1 We trained all
of the models on a PC equipped with four Titan X GPU each
with 12 GB of memory.
A. Datasets
We used three datasets in our experiments: a standard image
recognition benchmark dataset, CIFAR100, and two datasets
Purchase100 and Texas100 [6].
CIFAR100. This is a popular benchmark dataset used to
evaluate image recognition algorithms [9]. It contains 60, 000
color (RGB) images, each 32 × 32 pixels. The images are
clustered into 100 classes based on objects in the images.
Purchase100. The Purchase100 dataset contains the shop-
ping records of several thousand online customers, extracted
during Kaggle’s “acquire valued shopper” challenge.2 The
challenge was designed to identify offers that would attract
new shoppers. We used the processed and simpliﬁed version
of this dataset (courtesy of the authors of [6]). Each record
in the dataset is the shopping history of a single user. The
dataset contains 600 different products, and each user has a
binary record which indicates whether she has bought each of
the products (a total of 197, 324 data records). The records
are clustered into 100 classes based on the similarity of the
purchases, and our objective is to identify the class of each
user’s purchases.
Texas100.
includes hospital discharge data
records released by the Texas Department of State Health
Services 3. The records contain generic information about the
patients (gender, age, and race), external causes of injury (e.g.,
drug misuse), the diagnosis, and patient procedures. Similar to
Purchase100, we obtained the processed dataset (Courtesy of
the authors [6]), which contains 67, 330 records and 6, 170
binary features.
This dataset
B. Target Models
We investigate our attack model on the previously
mentioned three datasets, Texas100, Purchase100 and CI-
FAR100. For the CIFAR100 dataset we used Alexnet [10],
ResNet [11], DenseNet [12] models. We used SGD opti-
mizer [13] to train the CIFAR100 models with learning rates
of 0.01, 0.001, 0.0001 for epochs 0 − 50, 50 − 100, 100 − 300
accordingly. We used l2 regularization with weight of 0.0005.
For the Texas100 and Purchase100 datasets, we used fully
connected models. For Purchase100, we used a model with
layer sizes of 600, 1024, 512, 256, 128, 100 (where 100 is the
output layer), and for Texas100, we used layers with size
1024, 512, 256, 128, 100 (where 100 is the output layer). We
used Adam [13] optimizer with the learning rate of 0.001
for learning of these models. We trained each model for 100
epochs across all of our experiments. We selected the model
with the best testing accuracy across all the 100 epochs.
1https://pytorch.org/
2https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data
3https://www.dshs.texas.gov/thcic/hospitals/Inpatientpudf.shtm
To demonstrate that our attacks are not
limited to our
training algorithm, we used publicly available pre-trained
CIFAR100 models4. All of these models are tuned to get the
best testing accuracy using different regularization techniques.
D. Federated Learning
We performed the training for all of the federated learning
experiments. Speciﬁcally, we used the averaging aggregation
method for the federated scenario [8]. Each training party
sends the parameter updates after every epoch of training to
the central model, and the central server averages the models’
updates from the parties and sends the updated model to all
parties. In our experiments, we use the same training dataset
size for all parties, and each party’s training data is selected
uniformly at random from our available datasets.
E. Attack Models
Table XIV in Appendix A, presents the details of our
attack model architecture. As can be seen, we used ReLU
activation functions, and we initialized the weights using a
normal distribution with mean 0 and standard deviation of
0.01. The bias values of all layers are initialized with 0. The
batch size of all experiments is 64. To train the attack model
we use the Adam optimizer with a learning rate of 0.0001. We
train attack models for 100 epochs and pick the model with