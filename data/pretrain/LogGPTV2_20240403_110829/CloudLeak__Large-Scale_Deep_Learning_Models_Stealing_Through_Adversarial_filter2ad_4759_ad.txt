models are at all points inaccessible to the public, or users
other than the creator (i.e. they are treated as black box models
during our attack). Additionally, we also consider two existing
black-box models inside Face++ Emotion Recognition API
and Clarifai Safe for Work (NSFW) API. Different from the
previous MLaaS platforms provided by Microsoft, IBM and
Google, the Face++ emotion recognition and Clarifai NSFW
models only allow users to directly query the pre-trained
models through MLaaS APIs to meet their needs. They do
not allow ﬁne tuning on thier models for individual needs.
Trafﬁc Sign Recognition. We upload a well-labeled training
set and train a victim model for trafﬁc sign recognition using
it
Microsoft Trafﬁc Recognition API. Trafﬁc sign recognition
based on deep learning aims to classify different types of
trafﬁc signs from images, which can be used by self-driving
cars to automatically recognize trafﬁc signs. The dataset used
to train the victim model online is the GTSRB dataset [56]
with a training set containing 39000 images of 43 different
trafﬁc signs and a corresponding testing dataset containing
8000 images.
Flower Recognition. The victim model pre-trained on Mi-
crosoft Cloud Vision Service is for ﬂower recognition. The
ﬂower recognition classiﬁes images of ﬂowers into different
categories (e.g., Daisy, Sunﬂower, Fire lily, etc). This task is
known to be difﬁcult as the ﬂower dataset is less uniform.
Flowers in the same species may have various color appear-
ances, and the target objects are inﬂuenced by several lighting
conditions. Consequently,
is a well-known classiﬁcation
problem which people would like to use deep learning to solve.
The victim classiﬁcation model is trained on the VGG Flowers
dataset [57], including 6146 images from 102 different ﬂower
types and the corresponding testing dataset which contains
1020 images with 10 images for each 102 classes.
Face Emotion Recognition. For the Face++ Emotion Recog-
nition API [16], users cannot access the exact training sets
or its distribution and only observe the outputs (i.e., labels
or conﬁdence score) to chosen inputs by querying the API.
The victim model pre-trained on Face++ Emotion Recognition
API only allows an end user to upload a photo and fetch
the response about the emotions of detected faces, i.e., black-
box. This API returns probability scores on the likelihood that
an image contains emotions such as happiness, fear, surprise,
anger, disgust, neutral and sadness. Different from previous
victim models trained by users on their datasets, the Face++
Emotion Recognition API only provides the interface which
can be queried by users, meaning that users cannot access the
exact training process. Since the ofﬁcial test set of Face++
Emotion Recognition API are not provided, we create a test
set which contains 1010 images in 7 categories roughly.
Offensive Content Moderation. The Clarifai Not Safe For
Work (NSFW) API recognizes whether images contains var-
ious offensive contents which can be utilized by users to
automatically ﬁlter these contents from their platforms. Typ-
ically users access the API by querying it with image inputs
and receive resulting conﬁdence scores for two output labels
(NSFW - Not Safe For Work and SFW - Safe For Work).
However, the details of the victim model inside the Clarifai
NSFW API, such as training set and network architecture, are
generally inaccessible to users. Here, we attempt to steal the
black-box victim model provided by Clarifai. We also collect
1k images in two categories from Github as the test set to
evaluate the victim/substitute model accuracy.
These models inside the MLaaS act as the black-box victim
models of our model stealing attack. We launch the attack
against these victim models without knowing the exact training
sets and the internal
information of these models. In our
attack scheme, we re-train the local substitute model with
the synthetic dataset which is generated from the examples
by querying the victim model. Here, we use ﬁve different
strategies (e.g., RS, PGD, CW, FA and FF)) to craft these
query examples for the purpose of comparison. In particular,
8
Fig. 5: Adversarial Examples generated by our FeatureFool algorithm.
the adversarial examples generated by our FF are shown in
Figure 5.
We elaborate on the ablation study in two aspects: (a)
We study the inﬂuence of different datasets and/or transfer
architecture selections on the model stealing attack effective-
ness; (b) We also show the comparison between our model
stealing attack and existing attacks such as F. Tram`er attack [6],
Correia-Silva attack [14] and Papernot attack [21] against
commercialized MLaaS platforms in real world. The details
of these comparison experiments will be demonstrated in the
remaining sections.
We leverage open-source implementations of four pop-
ular pre-trained models: AlexNet, VGG19, VGGFace and
ResNet50. All experiments were carried out on a server
equipped with Intel E5-2623 v4 2.60GHz processor, 16GB
of RAM, four NVIDIA GeForce GTX 1080Ti GPUs. The
training starts from a relatively large learning rate and then
the learning rate would decrease during training to allow for
more ﬁne-grained weight updates. The pre-trained weights are
used to initialize our model extraction attack framework. We
split the training vectors into two parts: a training dataset
and a validation dataset. Then we use the stochastic gradient
descent (SGD) method to minimize the cross-entropy loss
while training the designed framework. We also apply some
basic but powerful data augmentation techniques like ﬂips,
rotations, and scaling.
B. MLaaS Models Extraction Attacks
1) Case Study 1: Trafﬁc Recognition Model: We train a
model for the GTSRB dataset through Microsoft Custom Vi-
sion inference and set it up as the black-box victim model. The
experimental results of our stealing attack on this victim model
are shown in Table III. We use the designed VGG19 DeepID
as the transfer architecture of the substitute model and generate
ﬁve types of synthetic datasets for training this substitute
model. With 0.43k queries, our substitute model achieves only
10.21% (13.10×) accuracy with random examples, 10.49%
(13.16×) accuracy with PGD examples, 12.01% (15.53×)
accuracy with CW examples, 11.64% (14.94×) accuracy with
FA and 15.96% (20.48×) accuracy with FF, illustrating that too
few queries fail to extract enough information from the victim
model for model stealing attack. With 2.15k queries, our local
substitute model achieves 70.03% accuracy with RS samples,
72.20% accuracy with PGD examples, 74.94% accuracy with
CW examples, 71.30% accuracy with FA samples and 76.05%
accuracy with FF examples, which is similar to the 77.93%
accuracy achieved by the victim model trained on Microsoft
Trafﬁc Recognition API. Our method can achieve the same
level of accuracy with fewer queries.
The total cost for stealing victim model with 76.05% test
accuracy is around $2.15 US dollars. Moreover, a local sub-
stitute model trained by adversarial examples always achieves
higher accuracy and test agreement than the model trained by
random samples, especially when the number of queries to the
victim model is small.
9
AdversarialSourceGuideKDEFGTSRBVGG FlowersAdversarialSourceGuideSourceGuide AdversarialService
Model
Queries
Microsoft
Trafﬁc
Flower
Clarifai
NSFW
Face++
Emotion
0.43k
1.29k
2.15k
0.51k
1.53k
2.55k
0.50k
1.00k
1.50k
0.68k
1.36k
2.00k
Dataset
Non-Feature-based
Feature-based
Price ($)
PGD
10.49 (13.16×)
59.91 (76.87×)
72.20 (92.65×)
27.84 (30.70×)
68.14 (75.14×)
83.24 (91.79×)
66.20 (71.88×)
74.90 (81.32×)
78.50 (85.23×)
30.19 (41.87×)
50.19 (69.61×)
62.00 (85.99×)
CW
12.10 (15.53×)
61.25 (78.60×)
74.94 (96.16×)
29.41 (32.43×)
69.22 (76.33×)
89.20 (98.36×)
71.50 (79.80×)
85.10 (93.60×)
89.70 (97.39×)
42.08 (58.36×)
67.23 (93.25×)
71.19 (98.74×)
FA
11.64 (14.94×)
49.25 (63.20×)
71.30 (91.49×)
28.14 (31.03×)
68.63 (75.68×)
84.12 (92.76×)
66.20 (71.88×)
75.00 (81.43×)
80.20 (87.08×)
37.05 (51.39×)
60.29 (83.62×)
64.10 (88.90×)
FF
15.96 (20.48 ×)
66.91 (85.86×)
76.05 (97.63×)
31.86 (35.13×)
72.35 (79.78×)
88.14 (97.19×)
76.20 (82.74×)
87.10 (94.57×)
91.60 (99.46×)
44.00 (61.03×)
65.33 (90.61×)
70.76 (98.14×)
0.43
1.29
2.15
1.53
4.59
7.65
0.60
1.20
1.80
0.34
0.68
1.00
RS
10.21 (13.10×)
45.30 (58.13×)
70.03 (89.86×)
26.27 (28.97×)
64.02 (70.59×)
79.22 (87.35×)
65.10 (70.68×)
72.30 (78.50×)
76.10 (82.63×)
26.10 (36.20×)
43.14 (59.83×)
58.10 (80.58×)
TABLE III: Comparison of performance on victim models and their local substitute models. We report the accuracy on test sets
in two forms: absolute (x%) or relative to black-box victim model (×). Accuracy (%) of black-box victim models are: 77.93
(100×) for Microsoft trafﬁc recognition model, 90.69 (100×) for Microsoft ﬂower recognition model, 92.10 (100×) for Clarifai
Not Safe For Work (NSFW) model and 72.10 (100×) for Face++ emotion recognition model, respectively.
Upon quantitative analysis, we observe that: (i) Adversarial
perturbation increases the diversity of synthetic datasets, result-
ing in a more successful transfer set. As a result, adversarial
examples help extract more decision information from the
victim model than random samples and hence improve the
query effectiveness, but this advantage shrinks as the number
of queries increases. (ii) Compared to RS, PGD and FA,
the substitute models trained on the CW and FF synthetic
datasets achieve better performance on the same test dataset.
The main reason for this is that, by solving optimization
problems for generating “informative” examples, CW and FF
strategies control the misclassiﬁcation conﬁdence by adjusting
the parameter κ in Equation (5) and the parameter M in Equa-
tion (10), and thus effectively generate adversarial examples
which lie approximately on the decision boundary of victim
classiﬁers. Compared to CW method, the FF attempts to vary
the contribution of different feature components and generate
adversarial examples that can contribute with more boundary
information about the victim model. Thus, our FF method can
achieve the same level of accuracy with fewer queries than the
CW method.
2) Case Study 2: Flower Recognition Model: The exper-
imental results of this victim model are shown in Table III.
We use the popular 50 layer ResNet50 model trained on the
ImageNet dataset as the transfer architecture of our substitute
model. As shown in Table III, we can see the accuracy of our
self-trained victim model is 92.01% (100×). Our substitute
model achieves 31.86% (35.13×) accuracy with 0.51k queries
and 72.35% (79.78×) accuracy with 1.53k queries by using the
FF training set. With few queries (e.g., 0.51k and 1.53k), the
FF strategy leads to better performance compared to the other
strategies such as RS, PGD, CW and FA. With 2.55k queries,
substitute model trained on the FF synthetic dataset obtains
97.19× performance of the black-box victim model, which is
comparable to the performance of the substitute model trained
on the CW synthetic dataset (98.36×). In this case, compared
to CW strategy, feature-based adversarial attacks such as FA
and FF may add more perturbations to legitimate examples in
order to maximize the uncertainty of these examples away from
decision boundary of the victim classiﬁer. Although adversarial
examples with large perturbations pollute the synthetic training
set, the substitute model trained on our FF still achieves strong
performance on all test set, which is similar to the accuracy
achieved by the substitute model trained on the CW synthetic
set. These trends also appear while stealing other black-box
models inside MLaaS platforms (We illustrate the details in the
following sections). Moreover, with 2.55k adversarial queries
to Microsoft custom vision service, it costs $7.65 US dollars
to extract a substitute model that achieves at least 91.76×
performance of the victim model.
3) Case Study 3: Emotion Recognition Model: So far our
attack framework assumes that the victim models inside APIs
are trained by users themselves. Here we consider a more
representative attack scenario where an adversary who targets
the pay-as-you-go commercialized MLaaS platform has no
knowledge about the exact training data or its distribution (we
assume that the adversary has some knowledge of the training
set but not the details, avoiding to use the irrelevant images
in the test), model architecture and training strategy, but can
observe the classiﬁcation outputs. Speciﬁcally, we utilize the
proposed attack algorithm to steal the Face++ Emotion Recog-
nition API in the black-box setting. The transfer architecture
of our substitute model is the VGGFace trained on VGG-Face
dataset to recognize 2622 identities. The dataset utilized to
train the substitute model is the KDEF dataset [58], which
contains 4900 pictures of human facial expression. The set of
pictures contains 70 individuals displaying 7 different emo-
tional expressions, including happy, fear, sad, surprise, angry,