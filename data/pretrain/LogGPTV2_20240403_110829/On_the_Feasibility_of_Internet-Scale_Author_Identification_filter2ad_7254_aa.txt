title:On the Feasibility of Internet-Scale Author Identification
author:Arvind Narayanan and
Hristo S. Paskov and
Neil Zhenqiang Gong and
John Bethencourt and
Emil Stefanov and
Eui Chul Richard Shin and
Dawn Song
2012 IEEE Symposium on Security and Privacy
On the Feasibility of Internet-Scale Author Identiﬁcation
Arvind Narayanan
PI:EMAIL
Hristo Paskov
PI:EMAIL
Neil Zhenqiang Gong
PI:EMAIL
Emil Stefanov
PI:EMAIL
Eui Chul Richard Shin
PI:EMAIL
John Bethencourt
PI:EMAIL
Dawn Song
PI:EMAIL
Abstract—We study techniques for identifying an anonymous
author via linguistic stylometry, i.e., comparing the writing
style against a corpus of texts of known authorship. We exper-
imentally demonstrate the effectiveness of our techniques with
as many as 100,000 candidate authors. Given the increasing
availability of writing samples online, our result has serious
implications for anonymity and free speech — an anonymous
blogger or whistleblower may be unmasked unless they take
steps to obfuscate their writing style.
While there is a huge body of literature on authorship
recognition based on writing style, almost none of it has studied
corpora of more than a few hundred authors. The problem
becomes qualitatively different at a large scale, as we show,
and techniques from prior work fail to scale, both in terms of
accuracy and performance. We study a variety of classiﬁers,
both “lazy” and “eager,” and show how to handle the huge
number of classes. We also develop novel techniques for conﬁ-
dence estimation of classiﬁer outputs. Finally, we demonstrate
stylometric authorship recognition on texts written in different
contexts.
In over 20% of cases, our classiﬁers can correctly identify
an anonymous author given a corpus of texts from 100,000
authors; in about 35% of cases the correct author is one of
the top 20 guesses. If we allow the classiﬁer the option of
not making a guess, via conﬁdence estimation we are able to
increase the precision of the top guess from 20% to over 80%
with only a halving of recall.
I. INTRODUCTION
Anonymity and free speech have been intertwined
throughout history. For example, anonymous discourse was
essential to the debates that gave birth to the United States
Constitution—the Founding Fathers wrote highly inﬂuential
“federalist” and “anti-federalist” papers under pseudonyms
such as Publius and Cato [1]. Fittingly, the Constitution pro-
tects the right to anonymous free speech, as the US Supreme
Court has ruled repeatedly. A 1995 decision reads [2]:
Anonymity is a shield from the tyranny of the
majority . . . It thus exempliﬁes the purpose behind
the Bill of Rights, and of the First Amendment in
particular: to protect unpopular individuals from
retaliation . . . at the hand of an intolerant society.
Today, anonymous speech is more-or-less equivalent
to
anonymous online speech, and more relevant than ever all
over the world. WikiLeaks has permanently and fundamen-
tally changed international diplomacy [3], and anonymous
activism has helped catalyze the recent popular uprisings in
the Middle East and North Africa [4].
© 2012, Arvind Narayanan. Under license to IEEE.
DOI 10.1109/SP.2012.46
300
Yet a right to anonymity is meaningless if an anonymous
author’s identity can be unmasked by adversaries. There
have been many attempts to legally force service providers
and other intermediaries to reveal the identity of anonymous
users. While sometimes successful [5; 6], in most cases
courts have upheld a right to anonymous speech [7; 8; 9].
All of these efforts have relied on the author revealing their
name or IP address to a service provider, who may in turn
pass on that information. A careful author need not register
for a service with their real name, and tools such as Tor can
be used to hide their identity at the network level [10]. But
if authors can be identiﬁed based on nothing but a passive
comparison of the content they publish to other content
found on the web, no legal precedents or networking tools
can possibly protect them.
After all, any manually generated material will inevitably
reﬂect some characteristics of the person who authored
it, and these characteristics may be enough to determine
whether two pieces of content were produced by the same
person. For example, perhaps some anonymous blog author
is prone to several speciﬁc spelling errors or has other
recognizable idiosyncrasies. If an adversary were to ﬁnd
material with similar characteristics that
the author had
posted in an identiﬁed venue, the adversary might discover
the true identity of the blog’s author.
We investigate large-scale stylometric author identiﬁca-
tion, enough to constitute a widespread threat to anonymity.
Previous work has shown that the author of a sample text
can often be determined based on a manual or automated
analysis of writing style, but only when the author is already
known to be among a small set of possibilities (up to
300). Koppel et al. study authorship attribution with a larger
number of authors, but this is not necessarily based on
writing style (for a detailed discussion, see Section II).
Before this work, it was unknown whether this type of attack
could apply in any scenario resembling the Internet in scale.
Our work. To answer the above question, we have
assembled a dataset comprising over 2.4 million posts taken
from 100,000 blogs—almost a billion words. We chose
blogs as our data source rather than IRC or message boards
because blog posts are more readily available due to the RSS
standard. Moreover, blogs are a common choice for political
expression, raising privacy issues.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
This dataset forms the basis for a series of large-scale
experiments, in which we extract a set of features from
each post in the dataset and use them to train classiﬁers
to recognize the writing style of each of the 100,000
blog authors. We experimented with representative lazy and
eager classiﬁers: nearest neighbor (NN), naive Bayes (NB)
and support vector machines (SVM) and regularized least
squares classiﬁcation (RLSC).
Our technical contributions.
1) We describe features of the data that make it difﬁcult
to scale classiﬁcation beyond a few hundred authors
(Section IV; Analysis). In particular, the “masking”
problem rules out many naive approaches (Section
V). Otherwise-excellent techniques like linear discrim-
inant analysis (LDA) fail because they are unable to
take advantage of some normalization techniques due
to the sparsity of the data (Section V-A). Efﬁciency is
a crucial consideration for Internet-scale analyses. We
describe our techniques for complexity improvements,
both asymptotic (Section V-B; RLSC vs. SVM) and
(large) constant factors (Section VI).
2) As with Koppel et al. [11], we ﬁnd that straightforward
lazy classiﬁers like NN perform surprisingly well.
However, unlike the explanation of [11] (similarity-
based approaches work better than classiﬁers when
the number of classes is large), or that of [12] (lazy
methods are particularly well-suited to NLP applica-
tions), we ﬁnd that the difference is simply due to the
difﬁculty of conﬁguring more complex models with
more parameters given a small number of training
examples in each class, such as those based on an
analysis of the covariance matrix. In particular we
ﬁnd that normalization makes a huge difference; an
RLSC classiﬁer with appropriate normalization per-
forms equally well as NN.
3) We develop techniques for conﬁdence estimation. The
ﬁrst
is a variant of the “gap statistic” [13] that
measures the difference between the best and the
second-best matching classes. The second is to run
two different classiﬁers and only output a result if
they agree. The third is to combine the above two
by meta-learning. Toward a related goal, we explore
the strategy of combining two different classiﬁers
using the respective conﬁdence scores and other input
features. We show that a meta-learner can achieve a
boost in accuracy by picking the output of one or the
other classiﬁer by estimating which is more likely to
be correct.
4) Finally, we use pairs (or tuples) of blogs listed under a
user’s Google proﬁle as a way of generating unlabeled
text that has a different context from the correspond-
ing labeled text. We argue that validating stylometric
classiﬁcation on such a cross-context dataset, which
is largely unexplored in previous work with online
corpora, is an important measure of the applicability
to many real-world scenarios.
Results and impact of our work. In experiments where
we match a sample of just 3 blog posts against the rest of the
posts from that blog (mixed in with 100,000 other blogs), the
nearest-neighbor/RLSC combination is able to identify the
correct blog in about 20% of cases; in about 35% of cases,
the correct blog is one of the top 20 guesses. Via conﬁdence
estimation, we can increase precision from 20% to over 80%
with a recall of 50%, which means that we identify 50% of
the blogs overall compared to what we would have if we
always made a guess.
The efﬁcacy of the attack varies based on the number
of labeled and anonymous posts available. Even with just
a single post in the anonymous sample, we can identify
the correct author about 7.5% of the time (without any
conﬁdence estimation). When the number of available posts
in the sample increases to 10, we are able to achieve a 25%
accuracy. Authors with relatively large amounts of content
online (about 40 blog posts) fare worse: they are identiﬁed
in over 30% of cases (with only 3 posts in the anonymous
sample).
Our results are robust. Our numbers are roughly equiv-
alent when using two very different classiﬁers, nearest
neighbor and RLSC; we also veriﬁed that our results are
not dominated by any one class of features. Further, we
conﬁrmed that our techniques work in a cross-context set-
ting: in experiments where we match an anonymous blog
against a set of 100,000 blogs, one of which is a different
blog by the same author, the nearest neighbor classiﬁer can
correctly identify the blog by the same author in about 12%
of cases. Finally, we also manually veriﬁed that in cross-
context matching we ﬁnd pairs of blogs that are hard for
humans to match based on topic or writing style; we describe
three such pairs in an appendix to the full version of the
paper.
The strength of the deanonymization attack we have
presented is only likely to improve over time as better
techniques are developed. Our results thus call into ques-
tion the viability of anonymous online speech. Even if
the adversary is unable to identify the author using our
methods in a fully automated fashion, he might be able to
identify a few tens of candidates for manual inspection as
we detail in Section III. Outed anonymous bloggers have
faced consequences ranging from ﬁring to arrest and political
persecution [14; 15].
Our experiments model a scenario in which the victim
has made no effort to modify their writing style. We do not
claim that our work will work in the face of intentional
obfuscation;
there is evidence that humans are
good at consciously modifying their writing style to defeat
stylometry [16]. A semi-automated tool for writing-style
obfuscation was presented in [17]. We hope that our work
indeed,
301
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
will motivate the development of completely automated
tools for transforming one’s writing style while preserving
the meaning. At the same time, privacy advocates might
consider user education in order to inform privacy-conscious
users about the possibility of stylometric deanonymization
attacks. Regardless of future developments, our work has
implications for the authors of all the sensitive anonymous
speech that already exists on the Web and in various
databases around the world.
II. RELATED WORK
Stylometry. Attempts to identify the author of a text
based on the style of writing long predate computers. The
ﬁrst quantitative approach was in 1877 by Mendenhall, a
meteorologist, who proposed the word-length distribution
as an author-invariant feature [18]. In 1901 he applied this
technique to the Shakespeare–Bacon controversy [19]. The
statistical approach to stylometry in the computer era was
pioneered in 1964 by Mosteller and Wallace who used func-
tion words and Bayesian analysis1 to identify the authors of
the disputed Federalist Papers [20].
The latter work was seminal; dozens of papers appeared
in the following decades, mostly focusing on identifying
different features. For an overview, see Stamatatos’s survey
[21]. These studies considered a small number of authors
(under 20) due to limitations in data availability. Since the
late 1990s, the emergence of large digital corpora has trans-
formed the nature of the ﬁeld. Research in the last decade has
been dominated by the machine-learning paradigm, and has
moved away from the search for a single class of features
toward an inclusive approach to feature extraction. Some
important works are [22; 23; 24].
Like most stylometric techniques, these studies consider
“topic-free” models and are able to discriminate between
100–300 authors. They have studied different domains (e-
mail, blogs, etc.) Our own work is probably closest in spirit