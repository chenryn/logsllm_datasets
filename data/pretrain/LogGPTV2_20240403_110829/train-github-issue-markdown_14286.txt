## Issue description
in a simple test network I get an error when dropout layer is included (error
does not happen when the dropout layer is excluded):
Traceback (most recent call last):  
File "testnet.py", line 48, in  
r = net(V)  
RuntimeError:  
Function.apply returned the wrong number of outputs.:  
operation failed in interpreter:  
/home/tester/anaconda3/lib/python3.6/site-
packages/torch/nn/functional.py(554): dropout  
/home/tester/anaconda3/lib/python3.6/site-
packages/torch/nn/modules/dropout.py(53): forward  
/home/tester/anaconda3/lib/python3.6/site-
packages/torch/nn/modules/module.py(491): **call**  
testnet.py(25): forward  
testnet.py(48):
## Code example
    import torch
    from torch import jit
    import torch.nn as nn
    from torch.autograd import Variable
    import torch.optim as optim........
    @torch.jit.compile(nderivs=1)
    class TestNet(nn.Module):
        def __init__(self):
            super(TestNet, self).__init__()
            self.net1 = nn.Linear(100, 200)
            self.net2 = nn.Linear(200, 1)
            self.bn = nn.BatchNorm1d(200)
            self.sigmoid = nn.Sigmoid()
            self.ReLU = nn.ReLU(inplace=False)
            self.drop = nn.Dropout(0.5)
        def forward(self, V):
            return self.sigmoid(self.net2(self.drop(self.ReLU(self.bn(self.net1(V)))
    #        return self.sigmoid(self.net2(self.ReLU(self.bn(self.net1(V))))).squeez
    use_cuda = False
    net = TestNet()
    criterion = nn.BCELoss()
    if use_cuda:
        net.cuda()
        criterion.cuda()
        V = Variable(torch.randn(100, 100)).cuda()
        label = Variable(torch.randn(100)).cuda()
    else:
        V = Variable(torch.randn(100, 100))
        label = Variable(torch.randn(100))
    optim_betas = (0.9, 0.999)
    optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01, betas=optim_betas)
    criterion = nn.BCELoss()
    net.train()
    for i in range(0,1000000):
        net.zero_grad()
        r = net(V)
        err = criterion(r, label)
        err.backward()...
## System Info
PyTorch version: 0.5.0a0+59f5f9a  
Is debug build: No  
CUDA used to build PyTorch: None
OS: Ubuntu 16.04.4 LTS  
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.9) 5.4.0 20160609  
CMake version: version 3.9.4
Python version: 3.6  
Is CUDA available: No  
CUDA runtime version: No CUDA  
GPU models and configuration: No CUDA  
Nvidia driver version: No CUDA  
cuDNN version: No CUDA
Versions of relevant libraries:  
[pip] numpy (1.14.0)  
[pip] numpydoc (0.7.0)  
[pip] torch (0.5.0a0+59f5f9a)  
[conda] magma-cuda80 2.3.0 1 pytorch  
[conda] torch 0.5.0a0+59f5f9a