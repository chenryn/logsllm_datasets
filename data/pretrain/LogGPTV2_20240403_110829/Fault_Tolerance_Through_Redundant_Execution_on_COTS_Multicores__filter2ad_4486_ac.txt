Dhrystone
86 (0)
193
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
VIRTUALISED SPLASH-2 EXECUTION TIME (S) ON X86 FOR N RUNS.
TABLE IV
Name
BARNES
CHOLESKY
FFT
FFM
LU-C
LU-NC
OCEAN-C
OCEAN-NC
RADIOSITY
RADIX
RAYTRACE
VOLREND
WATER-NS
WATER-S
Geometric mean
N
30
300
100
20
30
20
1000
1000
25
20
1000
100
600
600
Base
61 (0)
66 (0)
64 (0)
76 (0)
64 (0)
62 (0)
64 (0)
65 (1)
66 (0)
66 (0)
60 (0)
86 (0)
66 (1)
67 (0)
CC-D
93 (19)
792 (150)
142 (13)
160 (37)
437 (17)
381 (27)
173 (1)
171 (1)
75 (0)
89 (4)
65 (1)
133 (1)
92 (2)
84 (0)
Fact
1.52
12.08
2.22
2.11
6.83
6.12
2.71
2.65
1.12
1.34
1.09
1.54
1.41
1.25
2.30
presently support 64-bit VMs) are built with optimisation level
-Os and dynamically linked (the Buildroot [44] settings to
create Linux user-mode applications optimised for size); they
also use different libraries.
The results show a high performance impact of CC-RCoE
in a virtualised environment, with execution time increasing
55% for Dhrystone and almost tripling on Whetstone.
The overheads are dominated by the cost of VM exits and
entries. Intel virtualisation support reduces the impact of this
cost by minimising the need for VM exits through system-
call redirection, extended page tables and other optimisations;
so normal execution has few VM exits. In contrast, our
breakpoints force VM exits. The take-away is that CC-RCoE’s
support for virtualisation comes at signiﬁcant cost.
4) SPLASH-2: SPLASH-2 [45] is a suite of parallel scien-
tiﬁc computing kernels, which we again run in a Linux VM on
x86, with NPROC=2 (i.e. two threads); results are summarised
in Table IV. Overheads range from 10% to a factor 12, with
mean execution time around 2.3 times the baseline, which
is comparable to the virtualised Whetstone overhead and
thus in the expected range. The different sensitivities of the
benchmarks is a reﬂection of the time spent in tight loops. The
results conﬁrm that the code taking a signﬁcant share of overall
execution time should be ported to native execution, rather
than virtualised. With NPROC=1 (single-threaded applications)
the mean overhead drops slightly to 2.02.
5) Memory Bandwidth: To quantify the effect of redundant
co-execution on the memory bandwidth available to appli-
cations, we stress the memory system with a simple copy
benchmark. It uses memcpy() between two page-aligned
memory buffers, each of which is four times the size of the
last-level cache. We pre-map the buffers to avoid page faults,
and each run repeats the copy 100 times. We report the average
of 100 runs (i.e. 10,000 memcpy() invocations) in Table V.
MEMORY COPY BANDWIDTH (BW, GIB/S), AND REMAINING FRACTION
TABLE V
(%) UNDER RCOE.
LC-D
CC-D
LC-T
Base
CC-T
BW BW % BW % BW % BW %
7.8 31
25.4
1.7
0.6 32
12.5 49
1.1 64
12.4 49
1.1 64
7.9 31
0.7 40
x86
Arm
We synchronise the replicas by executing a barrier at the start
and end of each run. Relative standard deviations are ≤ 1%.
As expected, the replicas competing for memory bandwidth
reduces observable throughput to roughly 50% for DMR and
33% for TMR on x86. On Arm, a single core cannot saturate
the memory system, and this bandwidth reserve lessens the
impact on throughput. Predictably, there is little difference
between the LC and CC approaches.
B. System Benchmarks
We run Redis [46], a key-value store, as a system bench-
mark. Redis has a number of desirable features, such as exer-
cising CPU, memory, and network. It is implemented in ANSI
C without external dependencies, and adopts a single-threaded,
event-driven design and thus saves us from analysing source
code for data races. To avoid hiding overheads behind I/O
latencies, we run Redis as a volatile store without persistence.
We run Redis as a native seL4 process, with a second
process running the lwIP network stack [47] and an Ethernet
driver. Due to the different handling of I/O interfaces, the
drivers for the two conﬁgurations are slightly different. We
investigate three conﬁguration options for each setup, which
differ in the effort put into detecting divergence:
No arguments (N): minimal effort, synchronise on I/O only
(device register or DMA buffer access and interrupt
handling);
Arguments (A): in addition, add all arguments to the sig-
nature on each syscall. This is the default version, as
described in Section III-C;
Synchronise (S): as above, but also vote on each syscall.
Obviously, from N to S cost will increase but detection latency
will decrease, so this represents a performance-safety trade-off.
We evaluate performance of the Redis server using the
Yahoo! cloud serving benchmarks (YCSB) [48], running on
dedicated load generator machines, connected to the evaluation
platforms by dedicated Gigabit Ethernet links. We ensure that
throughput is not limited by the load generators. For all runs
we set recordcount to 70,000; we set operationcount
to 10×recordcount, except for YCSB-E, where it
is
1×recordcount. These settings result in a database size
of around 160 MiB on Arm and 190 MiB on x86 (as reported
by the info memory Redis client command), signiﬁcantly
larger than the last-level cache sizes. For each platform, we run
the YCSB benchmark set 10 times. Error bars show standard
deviations.
194
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
REDIS THROUGHPUTS NORMALISED TO BASELINE.
TABLE VI
A
Base
B
LC-D-N
LC-D-A
LC-D-S
C
CC-D-N
CC-D-A
CC-D-S
D
LC-T-N
LC-T-A
LC-T-S
E
CC-T-N
CC-T-A
CC-T-S
Mode
D-N
D-A
D-S
T-N
T-A
T-S
LC-RCoE
Arm
x86
CC-RCoE
Arm
x86
78–80% 74–79% 53–56% 51–54%
77–79% 74–79% 52–56% 50–54%
72–75% 61–66% 48–53% 38–42%
71–72% 68–73% 45–48% 46–49%
69–71% 68–73% 45–48% 46–49%
63–66% 53–58% 39–43% 34–38%
 80
 70
 60
 50
 40
 30
 20
 12
 11
 10
 9
 8
 7
 6
 5
 4
 3
client reads back the values, it can detect data corruptions
by comparing the embeded checksums and the recalcuated
checksums of the return vaules. We run until the server fails,
the client detects a checksum mismatch, or the error-detection
mechanisms report an error, then restart the system and repeat.
The left-hand columns of Table VII show results on x86,
using the default LC-A conﬁguration of Section V-B. We
target the memory of all kernel replicas, including the kernel
shared region, as well as the user memory of the primary
replica. When injecting faults we ensure the same sequence
of pseudo-random numbers for all conﬁgurations by seeding
the generator with the same number.
For the base case, we ﬁnd that about 4% of injected
faults lead to observed errors, the majority of which lead to
corruption detected by the client or even the client faulting
(“YCSB errors”), the remainder leading to faults in the server.
DMR and TMR are about equally effective in detecting
errors, with both LC conﬁgurations failing in about 1% and
CC in about 1.4% of cases to detect the error before leading
to corrupt output or YCSB errors. The slightly higher failure
rate of CC results from the slightly reduced SoR: The primary
copies input data from its private DMA buffer to a shared one,
from which the non-primaries replicate the data.
Uncontrolled errors are due to the following reasons: (1)
The DMA buffers are outside the SoR. Data corruptions
in input buffers cannot be detected if they do not lead to
divergence, and data in output buffers can be corrupted after
the replicas have voted (with FT_Add_Trace) and released
the data into the output buffers. (2) If the primary becomes
unresponsive, it cannot trigger synchronisations for interrupts,
leading to a hang. (3) Errors in the shared kernel-memory
regions can affect multiple replicas.
The kernel exceptions merit further scrutiny. An analysis of
the logs of the LC-T conﬁguration reveals that two of them
were caused by corrupted kernel instructions, the third was
potentially caused by a change of kernel-object type, result-
ing de-referencing an invalid pointer. The seL4 veriﬁcation
proves that there are no kernel exceptions (assuming correctly-
functioning hardware) and the kernel halts if an exception is
raised, so these are controlled errors.
In the right-hand columns of Table VII we show results
obtained on Arm. Here we inject faults into all replicas’
A
B
C
D
E
Fig. 3. Average Redis throughput (A–D: 1000 transactions per second, E: 20
transactions per second) on x86 (top) and Arm.
Results are shown in Fig. 3, we summarise the performance
degradation in Table VI. For readability we omit results for
YCSB-F, which is very similar to A and always shows virtually
indistinguishable results. We also observe that the results are
remarkably similar for the two platforms, so we can examine
them together without making reference to the processor.
Loosely-coupled DMR loses 20–38% throughput, the addi-
tional degradation of TMR is smaller, about an extra 15% over
DMR. The additional cost of including syscall arguments in
the signature (“A” vs “N”) is negligible, which justiﬁes using
“A” as the default conﬁguration. Voting on each system call
(“S”) has a higher cost impact, but that impact is less than the
baseline cost of replication.
The overhead of the CC approach is signiﬁcantly higher
than LC. The cost is dominated by the need to move all device
accesses into the kernel, signiﬁcantly increasing the number
of system calls. The extra cost of voting on system calls is
comparatively small, indicated by the fairly moderate cost of
reducing the error-detection latency.
C. Error Detection
We use software fault injection to test the ability of RCoE
to detect errors, and also experiment with over-clocking.
1) Random memory faults: We run the Redis server from
Section V-B and use a spare CPU core to ﬂip random bits in
memory. We modify the client to embed CRC32 checksums
into the values sent to the store on the server. When the
195
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:13:08 UTC from IEEE Xplore.  Restrictions apply. 
ERRORS RESULTING FROM MEMORY FAULT INJECTIONS ON X86 AND ARM.
TABLE VII
x86 – no exception-handler barriers
Arm – with exception-handler barriers
Base LC-D LC-T CC-D CC-T Base LC-D LC-T LC-D-N LC-T-N CC-D CC-T
60k
185k
1000
2297
0
1001
6
137
820
0
0
339
0
0
516
N/A
N/A
478
6
2297
N/A
994
214k
1000
299
10
0
0
0
678
13
309
691
243k
1000
647
57
291
5
0
N/A
N/A
1000
N/A
224k
1000
381
13
0
0
0
602
4
394
606
202k