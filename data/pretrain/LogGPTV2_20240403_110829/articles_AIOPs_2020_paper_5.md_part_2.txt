### 4.1 Auxiliary Sample Generation

As described, student models are trained on auxiliary samples in conjunction with the outputs of the teacher model. These auxiliary samples are drawn from a restricted range but are otherwise independent of the training data used to train the teacher model.

In the use case study for log anomaly detection, auxiliary samples are generated randomly as follows. Let \( T \) be the set of unique templates. An auxiliary sample is defined as \( \tilde{x} = (t_i \sim T : i = 1, 2, \ldots, w + 1) \), where \( w \) is the input sequence length. The template \( t_{w+1} \) will be used as the prediction target. Note that templates are randomly sampled from the unique template set \( T \). Thus, auxiliary input samples for DeepLog are random template sequences.

### 4.2 Experiment 1: Training of an Untrained Model

In our first experiment, we investigate the ability of the proposed method to mitigate the cold-start problem. A DeepLog model is trained on the HDFS data. Out of the 570,204 labeled log sequences in the HDFS dataset, the first 4,855 normal sequences are used for training, and the remaining 565,349 sequences are used as the test set. The test set contains 16,838 anomalies and 553,366 normal sequences. The two hyper-parameters of DeepLog are set as follows: \( w = 10 \) and \( k = 9 \), where \( w \) is the window size determining the input sequence length and is required to generate auxiliary samples. We use cross-entropy loss to learn a distribution when predicting possible next log lines. The teacher model uses a batch size of 16 and is trained over 20 epochs on all 4,855 normal log sequences. The trained DeepLog model is utilized as the teacher, while a completely untrained model with the same architecture and parametrization adopts the role of the student.

The teacher performs the following transformation: \( \varphi : X^{w \times |T|} \rightarrow Y^{|T| \times [-1, 1]} \) to generate the knowledge representation. It takes a sequence with \( w \) one-hot encoded templates and outputs a tanh-transformed probability distribution over all \( w \) templates. Different amounts of knowledge representation tuples are tested: {10, 50, 100, 500, 1000, 5000}. The student model uses a batch size of 16 and is trained over 10 epochs for each knowledge representation size. Auxiliary input samples are generated as described in section 4.1. Due to the randomness in sample generation, the experiment is repeated five times.

Figure 2 shows the results as a boxplot, which illustrates the F1-scores for the teacher and student models. The bottom and top whiskers reflect the minimum and maximum non-outlier values. The line in the middle of the box represents the median, and the box boundaries are the first and third quartiles of the value distribution. The leftmost bar shows the F1-score for the teacher model, which is 0.965. This bar is a flat line because it represents a single value. The remaining bars visualize the F1-scores for each knowledge representation size, from 10 to 5,000. It can be observed that a knowledge representation size of at least 100 is required to achieve a decent F1-score for the student model. As expected, the score of the student increases with the number of used knowledge representation tuples. Due to the random sampling, the F1-scores of student models trained with 100 and 500 knowledge representation tuples show high uncertainty. The 0.95 confidence interval ranges from 0.153 to 0.558 for size 100 and from 0.313 to 0.805 for size 500. The student model’s F1-score becomes increasingly stable with higher knowledge representation sizes and reaches 0.961 within the 0.95 confidence interval of [0.943, 0.958] for size 5000. Compared to the teacher model’s F1-score of 0.965, we conclude that our method can be utilized to mitigate the cold-start problem by training an untrained model via knowledge representations of trained models.

### 4.3 Experiment 2: Federated Learning

In this experiment, we investigate how multiple DeepLog models behave as teachers and students. This allows us to train distributed models on locally available training data and subsequently synchronize the knowledge of the models. We simulate eight distributed HDFS systems by creating a set of unique sequences. Out of the 570,204 labeled log sequences in the HDFS dataset, we again use the first 4,855 normal sequences to generate unique sequences of size \( w \). This results in 4,092 unique training samples. These 4,092 training samples are evenly and randomly split into eight sets, with each set containing 511 training samples. Therefore, eight DeepLog models are respectively trained. The two hyper-parameters of each DeepLog model are set as follows: \( w = 10 \) and \( k = 9 \). To evaluate the model performance in predicting potentially unseen samples, the remaining 565,349 sequences are used as a joint test set.

We initially trained the eight DeepLog models over 10 epochs with a batch size of 16 and cross-entropy loss as the loss function. The trained DeepLog models are utilized as teacher models for each other. Hence, all eight nodes are also students and are trained with the knowledge representation from the teachers with a batch size of 1 over 5 epochs. We test different amounts of knowledge representation per model: {10, 50, 100, 500, 1000}. Note that a student model is trained on the representations of all teacher models that have a higher or equal score than itself.

Auxiliary input samples are generated as described in section 4.1. Due to the random generation of auxiliary samples, the experiment is repeated seven times. Figure 3 shows the results as a boxplot, which illustrates the F1-scores of all eight models (marked as A-H) for different amounts of knowledge representation tuples over seven experiment executions. The properties of the boxplot are the same as described in section 4.2. The leftmost bars in the "Before" category for each model show the F1-scores after initial training on the locally available unique sequence set, without applying federated learning. The F1-scores range from 0.520 to 0.938. The 0.95 confidence interval for all eight nodes in this section ranges from 0.875 to 0.901.

Figure 4 shows a zoomed-in version of the same boxplot, focusing on the F1-score range between 0.90 and 0.94. As expected, an increasing amount of knowledge representations leads to overall higher F1-scores. Furthermore, it is visible that even 10 knowledge representations significantly improve the F1-scores. After 10 knowledge representations, the lowest F1-score is 0.906, and the 0.95 confidence interval for this section ranges from 0.923 to 0.933. In the category of 50 knowledge representations, the number of existing outliers is the same, but the 0.95 confidence interval ranges from 0.930 to 0.933, which is an improvement compared to 10 knowledge representations. The highest F1-score of 0.939 is observed in this category. The lowest deviations of the F1-score for all nodes occur at 1000 knowledge representations, with a 0.95 confidence interval from 0.933 to 0.935. In this category, the highest F1-score is 0.938 and the lowest is 0.928. Compared to the 0.95 confidence interval of 0.875 to 0.901 before the teacher-student process, we observe an improvement in every category.

This experiment indicates that it is possible to train different models with the same configuration in a distributed system. The method preserves data privacy by using auxiliary samples to transfer knowledge between models. A central instance for synchronization of the training process is not required, and neither model parameters nor gradients need to be transferred.

### 5 Conclusion

In this work, we proposed a federated learning solution for synchronizing distributed models trained on locally available data. Our method does not require sharing original training data or model parameters. Training is done via the assignment of teacher and student roles to existing models. Students are trained on the output of teachers via auxiliary samples and respective outputs, referred to as knowledge representations. We evaluated our approach in a case study of log anomaly detection. DeepLog models were trained on distinct and unique log sequences from the HDFS dataset. After that, the teacher and student roles were applied to the models to test the ability to synchronize them.

In our first experiment, we showed that this approach can mitigate the cold start problem. For this experiment, we set up a trained teacher DeepLog model and an untrained DeepLog model as a student. We investigated how well the student adapts to the teacher with different amounts of knowledge representations. After applying the proposed method, the student model achieved a comparable F1-score of 0.96, while the teacher achieved 0.97.

In the second experiment, we demonstrated that our method allows the synchronization of several models trained on different distinct training datasets through the proposed decentralized federated learning process. We split the training set into eight equal and unique log sequence subsets and distributed these among eight DeepLog models. With this training data, all nodes could perform an initial training step before entering the role of teachers and students. Even with distributing a small amount of 10 knowledge representations, all nodes could improve to an F1-score 0.95 confidence interval between 0.923 and 0.933, compared to their initially trained models, which reached a 0.95 confidence interval of 0.875 to 0.901. After 1,000 knowledge representations, the models achieve a 0.95 confidence interval of between 0.933 and 0.935.

For future work, we plan to investigate more datasets to verify the general applicability of our approach. Additionally, generating random sequences from a relatively small set of discrete elements represents a comparably limited search space for auxiliary sample generation. We expect the knowledge transfer to be harder within larger discrete sets or even within continuous spaces. Another goal is to research methods and heuristics to stabilize and accelerate the process of knowledge transfer with increasingly complex auxiliary samples.