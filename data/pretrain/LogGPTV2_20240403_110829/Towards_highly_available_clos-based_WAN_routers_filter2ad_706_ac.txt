when they are permutations of one another. For example, the mid-
dle and right failure patterns in Figure 7 are isomorphic because one
can swap the rightmost L1 nodes (and L2 nodes) of either pattern
to arrive at the other pattern. A canonical form of a graph G is a
graph ψ (G) that is isomorphic to G, such that every graph that is
isomorphic to G has the same canonical form as G, and any two
non-isomorphic graphs have different canonical forms. Canonical-
izing failure patterns can result in fewer patterns to search and is
crucial to scaling the pre-computation of effective capacity.
A Polynomial Time Algorithm. While it is not known whether
polynomial time algorithms exist for general graph canonization,
we have developed a polynomial time algorithm for Clos topologies.
Our algorithm leverages the fact that Clos topologies are bipartite.
Given a failure graph, the algorithm reorders links and nodes so that
all links are on the left (to the extent possible). This transformation
results in a canonical failure pattern. For example, in Figure 7, if
there is a single failed link between the second L1 switch and the
second L2 switch, by reordering the first and second L1 (respectively
L2) switches, we can arrive at the canonical failure pattern where
the failed link is between the first L1 switch and the first L2 switch
(the pattern shown in Figure 7). From this example, it is tempting
to assume that all single link failures are isomorphic, by symmetry.
But this is not the case, because isomorphism must also take into
account the trunks to which external links belong. For example, the
failure pattern in Figure 7 is not isomorphic to one between the 3rd
L1 switch and the 2nd L2 switch. Our canonicalization algorithm
(§A.5), takes these dependencies into account.
With this algorithm, suppose we wish to determine the effective
capacity for a failure pattern G: we first compute ψ (G), then use
the pre-computed effective capacity for ψ (G). We now discuss how
to compute effective capacity.
3.4 Effective Capacity
Given a trunk set, T is the set of all possible traffic matrices for
that trunk set. By design, a WAN router (in the absence of failure)
is non-blocking for all T ∈ T . One way to define effective capacity
is to enumerate the set of traffic matrices that the WAN router can
support for each failure pattern. This is computationally challeng-
ing, and also complicates traffic engineering algorithms that must
constrain their path computations to match these traffic matrices.
For this reason, we use a simpler definition of effective capacity.
Consider the set of all traffic matrices θT : every traffic matrixT ∈ T
is scaled element-wise by a scaling factor θ ∈ [0, 1]. We say that
the effective capacity of the WAN router is the largest θ such that
the router is non-blocking for every matrix in θT . Defining effective
capacity this way allows us to keep the TE algorithm unchanged; we
can simply scale the capacity of each trunk incident on the WAN
router by θ and run the TE algorithm to generate the paths.
Computing Effective Capacity. We obtain θ by solving an over-
loaded multicommodity flow problem [3, 8]. However, in formulat-
ing this, we need to be consistent with current practice in WANs,
which splits traffic evenly across all links in a trunk (§2). Thus, the
formulation must constrain the problem to ensure uniform splitting
of traffic both on incoming traffic and on outgoing traffic. Equally
important, we must find the effective capacity across all possible
traffic matrices and all possible failure patterns. The input to the
formulation includes a trunk set {Mk}, a trunk configuration {wsk},
a traffic set T , and a set of link-failure patterns F .
The output is the effective capacity θ, defined using a min-max
optimization objective:
min
T ∈T
min
F ∈F
ij
ab
ij
ba


(i, j)∈K2 r
a∈L1 r
b∈L2 r
max
=
+ θtijwai/Mi =
θ ∈Θ(F,T) θ
a∈L1 r
ij
ba
ij
ab ≤ I[(a, b) (cid:60) F]
ba ∈ R+
ij
ij
ab , r
r
θ ∈ [0, 1]
θ :
where
Θ(F ,T) =
ij
ab
,∀b ∈ L2,(i, j) ∈ K2
+ θtijwaj/Mj
b∈L2 r
,∀a ∈ L1,(i, j) ∈ K2
,∀(a, b) ∈ L1 × L2 ∪ L2 × L1
,∀a ∈ L1, b ∈ L2,(i, j) ∈ K2
(6)
This formulation finds the smallest θ across every pair of traffic
matrix and failure pattern. For each pair, it computes the largest
θ satisfying the constraints in Θ(F ,T). The first two constraints
ensure flow conservation at L2 switches and L1 switches. The sec-
ond one also imposes uniform splitting of ingress and egress traffic
on each trunk. The third describes the link capacity under a given
failure scenario, where I[·] is an indicator function. The last two
constrain the range of decision variables.
As in §2, this formulation is also intractable because the traffic
matrix set T is infinite. Here too, we can leverage the fact T forms
a convex polytope, and use the extreme traffic matrices in this
polytope to compute effective capacity. We prove this in §A.6.
It follows then that we can find the effective capacity by con-
sidering every pair of (a) canonical failure pattern F ∈ F and (b)
extreme traffic matrix T ∈ E. For each such pair, we solve the linear
program maxθ ∈Θ(F,T) θ. Using the canonical failure patterns and
the extreme traffic matrices reduces the complexity of the optimiza-
tion significantly. Furthermore, we can parallelize the computation
of effective capacity for each canonical pattern and each extreme
traffic matrix, so this computation scales well. We defer the discus-
sion of L1 switch failures and arbitrary combinations of failures to
§A.7 and §A.8 respectively.
429
ABCABCCCA WAN router with a failed linkABCABCCCLink-failure patternACACBCBCIsomorphic group by canonical formTowards Highly Available Clos-Based WAN Routers
SIGCOMM ’19, August 19–23, 2019, Beijing, China
sent from an L1 switch to an L2 switch, or vice versa, for a given
pair of trunks, is independent of the traffic matrix. To understand
why, consider the following two quantities defined for traffic from
trunk i to trunk j:
(cid:2)wsi/Mi − ws j/Mj
(cid:2)ws j/Mj − wsi/Mi
(cid:3)
(cid:3)
= tij ˆu
ij
s
ˆd
ij
= tij
s .
+
+
ij
s (w,T) = tij
s (w,T) = tij
ij
u
d
Figure 8: WCMP routing uses a multipath table. In today’s
switches, these tables have limited sizes.
4 COMPACT FORWARDING TABLES
In this section, we describe how we derive compact forwarding
tables to minimize upflow in the presence of failures.
4.1 Background
As described in §1, WAN routers today use ECMP [20] to forward
traffic: each L1 switch splits incoming traffic evenly across all links
to L2 switches. However, our approach may require an uneven
traffic split because, at an L1 switch, some fraction of incoming
traffic may be subject to early forwarding, while the rest of the
traffic needs to traverse L2 switches.
Prior work has described a weighted version of ECMP, called
WCMP [40], which assigns weights in proportion to the desired
traffic split ratio. Today’s switches implement WCMP using a mul-
tipath table (Figure 8): they assign each split entries in this table
in proportion to its weight. For example, if tunnel A should split
traffic across tunnels B and C as 3 : 2, the multipath table will have
5 entries as shown. The switch hardware will evenly split the traffic
across these 5 entries, achieving the desired 3 : 2 traffic split.
Unfortunately, modern switches have limited multipath table
entries, and arbitrary weight ratios can exceed table capacity. For
example, a weight ratio of two relatively prime numbers 233 : 767
requires 1000 entries.
4.2 Goal and Challenges
Motivated by this, we design compact forwarding tables by minimiz-
ing the number of entries needed for multipath tables for a given
trunk configuration, a failure pattern, and effective capacity.
Our design must address two challenges. First, it must preserve
early forwarding opportunities in order to minimize upflow. (One
way to compact the forwarding table is to adjust weights at the cost
of increased upflow, but this would negate the benefits of computing
the minimal-upflow wiring in §2). Second, the resulting forwarding
table must ensure that the WAN router remains non-blocking across
all traffic matrices; computing and modifying WCMP weights in
response to traffic matrix changes is infeasible both computationally
and operationally since traffic matrices change quickly over time.
4.3 Compacting Forwarding Table
Input and Output. The input to our algorithm is a trunk wiring
configuration {w} (from §2), a failure pattern F and effective ca-
pacity (from §3). The output is a set of integer WCMP [40] weights
that ensures non-blocking behavior under any traffic matrix for
that failure pattern and uses the fewest multipath entries.
Decoupling Traffic Matrices from WCMP Weight Calcula-
tions. Conceptually, it seems difficult to compute WCMP weights
that would ensure non-blocking behavior across all traffic matrices.
However, given a trunk configuration {w}, the proportion of traffic
430
The first quantity is the upflow volume from switch s, for a given
traffic matrix T and a given wiring {w}: i.e., it measures the total
volume of traffic at switch s sent up to L2 switches. The second
quantity is the downflow volume at s: the total volume at s received
from L2 switches forwarded on a trunk j at switch s.
Notice that both of these quantities have two components: a
traffic matrix component tij and a (respectively) upflow fraction ˆu
ij
s
or a downflow fraction ˆd
s . Our key insight is that WCMP weight
ij
calculations can be designed independent of traffic matrix by basing
the weight calculations on upflow and downflow fractions. (As an
aside, a TE algorithm does not compute a traffic matrix but routes
tunnels (or tunnel groups [25]) on trunks. Thus, trunk i might
carry traffic for multiple tunnels. For some tunnels, traffic will exit
the WAN router using trunk j. We have abstracted this detail by
describing the total volume of such traffic using the term tij).
Minimizing Multipath Table Entries. To minimize multipath
table entries we observe that, at a given switch s, for traffic between
trunks i and j, if the upflow fraction is non-zero, there cannot be any
downflow. This is by design: if there is upflow, it means that all the
links of trunk j at s are used for early forwarding, so there is no
capacity left for traffic from other switches to exit on trunk j at s.
Flow Counts. Thus, at each switch we can define a quantity
called the flow count v
s /αij
ij
ˆd
s /αij
ij
0
ij
s =
v
 ˆu
s as follows:
ij
, ˆu
s > 0
ij
, ˆd
s > 0
ij
, otherwise
∀s ∈ L1,(i, j) ∈ K2
.
which is either the upflow fraction or the downflow fraction de-
pending on the trunk wiring. For a reason described below, we scale
these fractions by the fractional greatest common divisor (FGCD)
αij of each traffic pair (i, j) across L1 switches, so that all v
s values
ij
are integers, hence the name flow count.
An Example. Figure 9 illustrates this idea for a trunk pair (C, A)
and two different values of traffic between these trunks tCA. In the
example on the left, the incoming traffic on the rightmost L1 switch
is 1 unit, and the switch forwards 3/4ths to the L2 layers (early
forwarding the rest), which is evenly distributed across the other
three L1 switches. In the example on the right, the incoming traffic
is half that. Despite this, the upflow/downflow fractions and the
flow counts are the same in all cases.
Now, suppose at switch s the flow count for trunk pair (i, j)
corresponds to an upflow. Consider two other switches s1 and s2
have (downward) flow counts. To compute the WCMP weights
across the network, we solve an optimization that determines how
to route these (upflow) flow counts from L1 switches (e.g., s) to L2
switches, and subsequently from L2 switches to the corresponding
L1 switches (e.g., s1 and s2) which have “available” downward flow
counts.
ABC10Gbps6Gbps4GbpsIndexEgress01234Multipath TableSIGCOMM ’19, August 19–23, 2019, Beijing, China
Sucha Supittayapornpong, Barath Raghavan, and Ramesh Govindan
Figure 9: Fraction of different volumes is the same and is con-
verted to flow count by FGCD.
Figure 10: Compact routing does not assign unnecessary weights
to multiple ports.
The key intuition for why our approach compacts routing tables
rests on the observation that, when “matching” the upflow flow
counts to the downflow flow counts, we can avoid splitting the
traffic unless not doing so would reduce the effective capacity. Fig-
ure 10 illustrates this and shows which switch ports have associated
WCMP weights for forwarding traffic from B to A. Without our
approach (left), the second L1 switch splits its traffic (i.e., divides
its upflow) on both internal ports. This assignment uses 2 entries
in the multipath table. Instead, the compact routing (right) realizes
that this multiple-interface assignment is unnecessary and only
uses the port connected to the first L2 switch, resulting in fewer
WCMP entries.
ij
b
The Optimization Objective. Thus, at a high-level, our opti-
mization objective is to match the upflow and downflow counts,
subject to the effective capacity constraint. To formalize this, let
represent the total flow count from L2 switch b to L1 switch
ij
x
ba
a for trunk pair (i, j). Let’s define X
as the sum of these values
across all L1 switches a. We make two observations. First, that X