v101
y
c
n
e
u
q
e
r
F
12
10
8
6
4
2
0
0
500
1000
1500
2000
2500
0
200
400
600
800
1000
Bin Size
Bin Size
d500
v551
Figure 4: Real User vs. Virtual Center Points
is the padded rating of ri,j. Note that since we are using
the padded user rating vectors, there are no null ratings.
Our experimental results demonstrate the eﬀectiveness
of using virtual centers as opposed to real users as center
points. Figure 4(a) shows that using real user centers for
clustering the sample points results in bin sizes varying from
50 to 2000, whereas virtual centers yield bin sizes within the
range 200 to 900. Since balanced clusters facilitate easy
data handling and faster running time in later anonymiza-
tion steps, we use virtual centers in our implementation.
4.3 Step 3: Homogenization
To defend against both the structure-based attack and
label-based attack, our ﬁnal step is to homogenize the users
in each anonymization group so they have identical review
graphs. A straightforward way to do this is to apply the pop-
ularly used generalization and suppression techniques [30,
33]. However, as pointed out by [26], generalization and sup-
pression may completely destroy the utility of the data for
collaborative ﬁltering. We take a diﬀerent approach, homog-
enization, which consists of adding fake edges and labels so
that all users within an anonymization group are connected
to the same set of item nodes with the same ratings.
Formally, the homogenization of the anonymization group
corresponding to cluster C of users {u1, . . . , uk} has the fol-
lowing operations: union, complete, and average. First, we
construct the union of the review graphs of all users in C.
Let the union result be GC = (C ∪ N (C), EC, LC ). Sec-
ond, we add fake edges between users and items to create
C , a complete bipartite subgraph; i.e., E∗
G∗
C = C × N (C).
For instance, in Figure 1 (b), user 0001 and 0002 are in the
same anonymization group. A fake edge is added between
user 0001 and movie English Patient, so that both user 0001
and 0002 review the same set of movies in the review graph.
Third, we re-label all the edges in G∗
C , including the fake
ones, with the appropriate average ratings. When calcu-
lating the average ratings, we can use either the ratings in
the original dataset or the ones in the padded dataset. Fol-
lowing this, we design two homogenization schemes: padded
anonymization and pure anonymization.
Padded anonymization uses the ratings in the padded
dataset. That is, for each anonymization group, we calculate
the homogenized rating of each item as the average padded
rating over all users in the group. Formally,
(∀ui ∈ C, oj ∈ O) L∗
C (ui, oj ) = ˆrj = Xui∈C,ri,j 6=0
ri,j/m,
where m is the number of users in the database, and ri,j is
the padded rating of user i for item j. Note that an average
rating is computed for every item in the dataset, since in
padded anonymization there is an edge between every user
and every item in the anonymized review graph.
Pure anonymization uses the ratings in the original
dataset. That is, for each anonymization group, we refer
back to the unpadded data and calculate the homogenized
rating of each item as the average rating over only users who
rated that item in the original dataset. Formally,
(∀ui ∈ C, oj ∈ N (C)) L∗
C (ui, oj) = ˆrj = Xui∈C,ri,j 6=0
ri,j/k′,
where k′ is the number of users in C who rated item oj .
Note that in all variants of Predictive Anonymization, all
users in cluster C are assigned the same homogenized rat-
ing ˆrj for each item oj . The diﬀerence is that in padded
anonymization, the released dataset contains padded values,
which eﬀectively obscure real data and thus provide stronger
privacy protection against homogeneity attacks (more de-
tails are in Section 6). However, the padded-anonymized
data deviates signiﬁcantly from the original dataset, as it is
strongly inﬂuenced by padded values. If the predictions used
in the padding step were not accurate, this may adversely
aﬀect the utility of the released data. Furthermore, much of
the structure of the original data is lost – including sparsity,
one of the most deﬁning characteristics of recommendation
data. This casts doubt on the ability of padded anonymiza-
tion to preserve the integrity of the data. On the other hand,
a released dataset that has undergone pure anonymization
does not contain any padded values, and thus better pre-
serves the integrity and utility of the data, although it may
provide weaker privacy guarantees. A more detailed com-
parison of the two methods is described in Section 7.
5. ANALYSIS
In this section, we analyze the complexity and security of
our predictive anonymization algorithms.
5.1 Complexity Analysis
Let m be the number of users, and let n be the number of
items (m = 480, 189 and n = 17, 700 in the Netﬂix dataset).
Using recent techniques for optimization of SVD, Step 1
(Section 4.1) can be performed in O(mn) time. Step 2.1
(Section 4.2) takes O(s) time, where s is the size of the sam-
ple. The bounded t-means algorithm in Step 2.2 (Section
4.2) has complexity O(st1n). For Step 2.3 (Section 4.2), us-
ing the center points from the sample to partition the dataset
into bins runs in O(mt1n) time; clustering on each bin takes
time O(|Bi|tn) = O((|Bi| ∗|Bi|/k)n), where |Bi| is linear in
m/t1. Thus the complexity is O(m2n/(t2
1k)). There are t1
bins overall, so the total complexity is O(m2n/(t1k)). To re-
duce the quadratic complexity in m, we set t1 = √m, which
results in the complexity of this step being O(m3/2n/k).
For Step 3 (Section 4.3), homogenization of each cluster C
takes complexity O(|C|n) = O(kn). There are m/k clus-
ters, thus the total complexity is O(mn). Based on the
above, the complexity of the entire anonymization approach
is O(mn + s√mn + m3/2n + m3/2n/k + mn). Since s < m,
the complexity is O(m3/2n).
5.2 Privacy Analysis
In this section, we analyze the guarantee that our Pre-
dictive Anonymization algorithm provides both node re-
identiﬁcation privacy and link existence privacy deﬁned in
Section 3.1. The following theorem is analogous to the cor-
rectness of the k-anonymity model on relational databases.
Theorem 5.1. Node Re-identiﬁcation Privacy Let G
be the bipartite review graph for a recommender dataset,
and let G∗ be the corresponding released review graph. If
G∗ is k-anonymous, then a user cannot be re-identiﬁed in
G∗ with conﬁdence greater than 1
k .
As shown in Section 4.3, fake edges are added between
user vertices and item vertices during the homogenization
step, which prevents the adversary from explicitly determin-
ing which edges exist in the original dataset (or furthermore
their labels). Assume that all (user, item) ratings are inde-
pendent, both the existence and the values of the ratings.
Furthermore, assume the adversary has no prior knowledge
about the likelihoods that users have rated items. Then the
conﬁdence with which the adversary can learn the existence
of a link is at most 1/k. This claim is stated concisely as
follows.
Theorem 5.2. Link Existence Privacy Assume that
all ratings are independent. Then an adversary with no
prior knowledge employing a label-based attack cannot pre-
dict the existence of an edge (vu, vo) with conﬁdence greater
than 1
k .
Suppose an adversary has prior knowledge that the proba-
bility a user has rated item o is p. Obtaining this knowledge
is often feasible in practice by learning aggregate informa-
tion about the database. For example, in the Internet Movie
Database (IMDB), the most frequently rated movie is “The
Shawshank Redemption”, which has been rated by 2.4% of
registered users. Assume that p ≤ 1/k (a reasonable as-
sumption due to the sparsity of recommender datasets). We
claim that even with this additional prior knowledge, the
adversary cannot signiﬁcantly improve his conﬁdence that a
user has rated item o.
Theorem 5.3. Link Existence Privacy With Prior
Knowledge Assume that all ratings are independent. Then
an adversary with prior knowledge p ≤ 1
k for item o employ-
ing a label-based attack cannot predict the existence of an
edge (vu, vo) with conﬁdence greater than 1
k + p
2−kp .
Proofs can be found in the Appendix. Note that the great-
est conﬁdence gain occurs when p = 1/k, at which point the
adversary has a 2/k conﬁdence probability. Therefore, as-
suming that p ≤ 1/k, the maximum conﬁdence in predicting
the existence of a link is bounded by 2/k. Furthermore, rec-
ommendation data is typically very sparse, so it is of note
that the adversary conﬁdence tends to 1/k as p → 0.
While it may be diﬃcult to learn the existence of a link,
learning the non-existence of links is easy with a label-based
attack. However, we claim that due to the sparsity of the
data and the practical signiﬁcance of a link, it is reason-
able to assume that only positive link existence should be
considered sensitive.
6. ACHIEVING L-DIVERSITY
The l-diversity model provides complementary privacy
protection to k-anonymity. In relational data, l-diversity re-
quires that sensitive attributes should have diversity by hav-
ing at least l distinct values in each k-anonymous class [22].
However, the deﬁnition and security implications of
l-
diversity in recommender databases are unclear. Therefore,
we give the ﬁrst formal deﬁnition of l-diversity for labeled
bipartite review graphs, and an algorithm to realize both
k-anonymity and l-diversity in recommender systems.
To appreciate the need for l-diversity in recommendation
data, we need to ﬁrst understand a subtle attack against link
privacy. Once the anonymization group of a target victim
is identiﬁed via a structure-based or label-based attack, it
becomes easier to target that user for more sophisticated at-
tacks. Although the adversary cannot explicitly identify any
user, more can be deduced from the anonymized data than
what we want to allow. For example, suppose an adversary
only has the background knowledge to perform a structure-
based attack. After identifying the correct anonymization
group, since the k users in that group have identical re-
view proﬁles, the adversary can easily learn new information
about the ratings that the target user gave to those items.
This problem is further exacerbated by the fact that some
items are rarely reviewed. Exploring rare items to re-identify
users in Netﬂix data was recently studied [26], and facilitates
the easy identiﬁcation of a user’s anonymization group, leav-
ing the target susceptible to the above attacks. We refer to
these attacks as homogeneity attacks following [22]. The
threat of these homogeneity attacks motivates the need for
l-diversity.
Deﬁnition 5. Homogeneity Attack in Bipartite
Graphs Given a released bipartite review graph G∗ =
(VU ∪ VO, E∗, L∗), let GA
u ) be the
subgraph representing the adversary knowledge for a user
u. Let {vu′} denote the set of nodes, including vu, each of
which vu′ ∈ VU has GA
u′ . If all the nodes in {u′} are
identical, then we say the review proﬁle of user u is uniquely
identiﬁed by the homogeneity attack.
u = ({vu}∪ N A(vu), EA
u ⊆ G∗
u , LA
Unlike relational data, our k-anonymization algorithm
alone provides some degree of privacy even in the face of
a homogeneity attack (See Theorem 5.3). Because we add
fake edges during anonymization to average the k users, a
rating in the anonymized data does not necessarily mean
that the target user has rated that item, or if so, that the
anonymized rating accurately reﬂects the true rating of that
user. However, as explained above, homogeneity attacks can
be eﬀective in some scenarios.
First, we formally deﬁne (b, l)-diversity. We present an
extension to the Predictive Anonymization algorithm that
realizes both k-anonymity and (b, l)-diversity. The b indi-
cates that the adversary’s prior knowledge includes at most
b items that have been reviewed by the user.
Intuitively,
(b, l)-diversity requires that every subset of b items must be
included in at least l diﬀerent anonymization groups.
·
Deﬁnition 6. (b, l)-diversity Given a bipartite review
graph G = (VU∪VO, E, L), let G∗ = (VU∪VO, E∗, L∗) be the
corresponding k-anonymized review graph with anonymiza-
S Ci = U . We say G∗ satisﬁes (b, l)-diversity if
tion groups
for every set of b items B = {o1, . . . , ob} ⊂ O that have been
rated by a user, there are at least l distinct anonymization
groups Ci such that B ⊂ N (Ci).
To achieve (1, l)-diversity, we modify our algorithm as fol-
lows: After homogenization is performed, we check whether
each item o has been covered by l groups. If it has not, we
randomly select anonymization groups C such that o 6∈ C
and add fake edges between item o and all user nodes in C,
so that every item o is connected to at least l groups. The
labels on these fake edges are computed using the padded
values for the corresponding users. The above method can
be easily generalized to realize (b, l)-diversity, the details of
which are omitted here.
7. EXPERIMENTS
We have done a set of experiments to evaluate both the ef-
fectiveness and eﬃciency of our k-anonymization algorithm
(without l-diversity). Speciﬁcally, we want to evaluate the
impacts of padding and anonymization on the utility and
structure of the anonymized data, and the amount of in-
formation change introduced by the anonymization. In this
section, we describe our experiment design and results.
7.1 Setup
We ran our experiments parallelized on 6 diﬀerent ma-
chines. Four of the machines are equipped with eight In-
tel(R) Xeon(R) CPU 3.00GHz, 16GB memory and CentOS
5.2 Linux, and the other two machines are equipped with
two Intel(R) Core(TM)2 Duo CPU at 3.00GHz, 3GB mem-
ory and Fedora 8 Linux. We implemented our algorithm in
C++, Java, and Perl.
We use the entire Netﬂix dataset for our experiment. The
original data contains a total of 480,189 users’ ratings on
17,770 movies. The ratings range from 1 to 5, with 0 mean-
ing a rating does not exist. The Netﬂix challenge set (a.k.a.
probe set) is used to evaluate the performance of a pre-
diction algorithm. It contains 459,178 users and 1,425,333
user-movie pairs to be predicted. The users are a subset
of the original Netﬂix dataset. In our experiments, we use