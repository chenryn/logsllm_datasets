Our optimization utilizes the fact that all PathORAM leaf IDs
are known in advance and paths in a tree-based storage share the
buckets close to the root. The core idea is to read all paths first,
processes the requests and and then write all paths back. This way
the client makes a single read request, which is executed much
faster than many small requests. Requests are then processed in
main memory, including re-encryptions. Finally, the client executes
the write requests using remapped leaves as a single operation,
saving again compared to sequential execution.
This optimization provides up to 8 times performance boost in
our experiments. We note that the gains in speed and I/O overhead
are achieved at the expense of main memory, which is not an issue
given that the memory is released after a batch, and our exper-
iments confirm that. The security guarantees of PathORAM are
maintained with this optimization, since the security proof in [65,
Section 3.6] still holds. Randomized encryption, statistically inde-
pendent remapping of leaves, and stash processing do not change.
5.3.2 Lightweight ORAM servers. We have found in our experi-
ments that na√Øve increase of the number of CPU cores and gigabytes
of RAM does not translate into linear performance improvement
after some threshold. Investigating the observation we have found
that the Epsolute protocol, executing parallel ORAM protocols, is
highly intensive with respect to main memory access, cryptographic
operations and network usage. The bottleneck is the hardware ‚Äî
we have confirmed that on a single machine the RAM and network
are saturated quickly preventing the linear scaling.
To address the problem, we split the user party U into multi-
ple lightweight machines that are connected locally to each other
and reside in a single trust domain (e.g., same data center). Specifi-
cally, we maintain a client machine that receives user requests and
prepares ORAM read requests, and up to ùëö lightweight ORAM
machines, whose only job is to run the ORAM protocols in parallel.
See Fig. 2 for the schematic representation of the architecture. We
Figure 2: Lightweight ORAM machines diagram. A user sends a
query to U modeled as the client machine, which uses local data
index and DP structures to prepare a set of ORAM requests, which
are sent to respective ORAM machines. These machines execute the
ORAM protocol against the untrusted storage of S.
emphasize that U is still a single party, therefore, the security and
correctness guarantees remain valid.
The benefit of this approach is that each of the lightweight
machines has its own hardware stack. Communication overhead
among U machines is negligible compared to the one between U
and S. The approach is also flexible: it is possible to use up to ùëö
ORAM machines and the machines do not have to be identical. Our
experiments show that when the same number of CPU cores and
amount of RAM are consumed the efficiency gain is up to 5 times.
6 EXPERIMENTAL EVALUATION
We have implemented our solution as a modular client-server ap-
plication in C++. We open-sourced all components of the software
set: PathORAM [14] and B+ tree [11] implementations and the
main query executor [12]. We provide PathORAM and B+ tree com-
ponents as C++ libraries to be used in other projects; the code is
documented, benchmarked and tested (228 tests covering 100 % of
the code). We have also published our datasets and query sets [13].
For cryptographic primitives, we used OpenSSL library (version
1.1.1i). For symmetric encryption in ORAM we have used AES-CBC
algorithm [26, 27] with a 256-bits key (i.e., ùúÇ2 = 2‚àí256), for the hash
algorithm H used to partition records among ORAMs we have used
SHA-256 algorithm [52]. Aggregate tree fanout ùëò is 16, proven to
be optimal in [58].
We designed our experiments to answer the following questions:
Question-1 How practical is our system compared to the most
efficient and most private real-world solutions?
Question-2 How practical is the storage overhead?
Question-3 How different inputs and parameters of the system
affect its performance?
Question-4 How well does the system scale?
Question-5 What improvements do our optimizations provide?
Question-6 What is the impact of supporting multiple attributes?
1Query:ages18to21UntrustedserverpartySTrusteduserpartyUUser4ORAMrequests:ORAMIDsBlockIDs2TrueindicesLightweightORAMmachineLightweightORAMmachineKVSStoreKVSStoreKVSStoreKVSStore3Computingtheamountofnoise5ORAMGETrequests5ORAMGETrequests5ORAMGETrequests5ORAMGETrequestsDPhistogramB+treeApplicationDPtreeClient6prunning fakerecordsSession 7C: Database and Privacy CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2270To address Question-1 we have run the default setting using
conventional RDBMS (MySQL and PostgreSQL), Linear Scan ap-
proach and Shrinkwrap [5]. To target Question-2, we measured
the exact storage used by the client and the server for different data,
record and domain sizes. To answer Question-3, we ran a default
setting and then varied all parameters and inputs, one at a time. For
Question-4 we gradually added vCPUs, ORAM servers and KVS
instances and observed the rate of improvement in performance.
For Question-5 we have run the default setting with our optimiza-
tions toggled. Lastly, for Question-6 we have used two datasets to
construct two indices and then queried each of the attributes.
6.1 Data sets
We used two real and one synthetic datasets ‚Äî California public pay
pension database 2019 [67] (referred to as ‚ÄúCA employees‚Äù), Public
Use Microdata Sample from US Census 2018 [68] (referred to as
‚ÄúPUMS‚Äù) and synthetic uniform dataset. We have used salary / wages
columns of the real datasets, and the numbers in the uniform set
also represent salaries. The NULL and empty values were dropped.
We created three versions of each dataset ‚Äî 105, 106 and 107
records each. For uniform dataset, we simply generated the target
number of entries. For PUMS dataset, we picked the states whose
number of records most closely matches the target sizes (Louisiana
for 105, California for 106 and the entire US for 107). Uniform dataset
was also generated for different domain sizes ‚Äî number of distinct
values for the record. For CA employees dataset, the set contains
260 277 records, so we contracted it and expanded in the following
way. For contraction we uniformly randomly sampled 105 records.
For expansion, we computed the histogram of the original dataset
and sampled values uniformly within the bins.
Each of the datasets has a number of corresponding query sets.
Each query set has a selectivity or range size, and is sampled either
uniformly or following the dataset distribution (using its CDF).
6.2 Default setting
The default setting uses the Œ†ùõæ from Section 5 and lightweight
ORAM machines from Section 5.3.2 and Fig. 2. We choose the Œ†ùõæ
because it outperforms Œ†no‚àíùõæ in all experiments (see Question-4
in Section 6.5). In the setting, there are 64 Redis services (8 ser-
vices per one Redis server VM), 8 ORAM machines communicating
with 8 Redis services each, and the client, which communicates
with these 8 ORAM machines. We have empirically found this con-
figuration optimal for the compute nodes and network that we
used in the experiments. ORAM and Redis servers run on GCP
n1-standard-16 VMs (Ubuntu 18.04), in regions us-east4 and
us-east1 respectively. Client machine runs n1-highmem-16 VM
in the same region as ORAM machines. The ping time between the
regions (i.e. between trusted and untrusted zones) is 12 ms and the
effective bandwidth is 150 MB/s. Ping within a region is negligible.
Default DP parameters are ùúñ = ln(2) ‚âà 0.693 and ùõΩ = 2‚àí20,
which are consistent with the other DP applications proposed in
the literature [38]. Buckets number is set as the largest power of
ùëò = 16 that is no greater than the domain of the dataset ùëÅ .
Default dataset is a uniform dataset of 106 records with domain
size 104, and uniformly sampled queries with selectivity 0.5 %. De-
fault record size is 4 KiB.
6.3 Experiment stages
Each experiment includes running 100 queries such that the over-
head is measured from loading query endpoints into memory to
receiving the exact and whole query response from all ORAM ma-
chines. The output of an experiment is, among other things, the
overhead (in milliseconds), the number of real and noisy records
fetched and communication volume averaged per query.
6.4 RDBMS, Linear Scan and Shrinkwrap
On top of varying the parameters, we have run similar workloads
using alternative mechanisms ‚Äî extremes representing highest
performance or highest privacy. Unless stated otherwise, the client
and the server are in the trusted and untrusted regions respectively,
with the network configuration as in Section 6.2.
Relational databases. Conventional RDBMS represents the most
efficient and least private and secure solution in our set. While
MySQL and PostgreSQL offer some encryption options and no
differential privacy, for our experiments we turned off security
features for maximal performance. We have run queries against
MySQL and PostgreSQL varying data and record sizes. We used
n1-standard-32 GCP VMs in us-east1 region, running MySQL
version 14.14 and PostgreSQL version 10.14.
Linear Scan. Linear scan is a primitive mechanism that keeps all
records encrypted on the server then downloads, decrypts and
scans the entire database to answer every query. This method is
trivially correct, private and secure, albeit not very efficient. There
are RDBMS solutions, which, when configured for maximum pri-
vacy, exhibit linear scan behavior (e.g., MS-SQL Always Encrypted
with Randomized Encryption [48] and Oracle Column Transparent
Data Encryption [53]). For a fair comparison we make the linear
scan even more efficient by allowing it to download data via parallel
threads matching the number of threads and bytes per request to
that of our solution. Although linear scan is wasteful in the amount
of data it downloads and processes, compared to our solution it has
a benefit of not executing an ORAM protocol with its logarithmic
overhead and network communication in both directions.
Shrinkwrap. Shrinkwrap [5] is a construction that answers feder-
ated SQL queries hiding both access pattern and communication
volume. Using the EMP toolkit [71] and the code Shrinkwrap au-
thors shared with us, we implemented a prototype that only answers
range queries. This part of Shrinkwrap amounts to making a scan
over the input marking the records satisfying the range, sorting the
input, and then revealing the result set plus DP noise to the client.
For the latter part we have adapted Shrinkwrap‚Äôs Truncated Laplace
Mechanism [5, Definition 4] to hierarchical method [58] in order
to be able to answer an unbounded number of all possible range
queries. We have emulated the outsourced database setting by using
two n1-standard-32 servers in different regions (12 ms ping and
150 MB/s bandwidth) executing the algorithm in a circuit model
(the faster option per Shrinkwrap experiments) and then revealing
the result to the trusted client. We note that although the complex-
ity of a Shrinkwrap query is O(ùëõ log ùëõ) due to the sorting step, its
functionality is richer as it supports more relational operators, like
JOIN, GROUP BY and aggregation. We also note that since MySQL,
Session 7C: Database and Privacy CCS ‚Äô21, November 15‚Äì19, 2021, Virtual Event, Republic of Korea2271PostgreSQL and Shrinkwrap are not parallelized within the query,
experiments using more CPUs do not yield higher performance.
6.5 Results and Observations
After running the experiments, we have made the following obser-
vations. Note that we report results based on the default setting.
‚Ä¢ Epsolute is efficient compared to a strawman approach, RDBMS
and Shrinkwrap: it is three orders of magnitude faster than
Shrinkwrap, 18 times faster than the scan and only 4‚Äì8 times
slower than a conventional database. In fact, for different queries,
datasets, and record sizes, our system is much faster than the
linear scan, as we show next.
‚Ä¢ Epsolute‚Äôs client storage requirements are very practical: client
size is just below 30 MB while the size of the offloaded data is
over 400 times larger.
‚Ä¢ Epsolute scales predictably with the change in its parameters:
data size affects performance logarithmically, record size ‚Äî lin-
early, and privacy budget ùúñ ‚Äî exponentially.
‚Ä¢ Epsolute is scalable: using Œ†ùõæ with the lightweight ORAM ma-
chines, the increase in the number of threads translates into linear
performance boost.
‚Ä¢ The optimizations proposed in Section 5.3 provide up to an order
‚Ä¢ Epsolute efficiently supports multiple indexed attributes. The
overhead and the client storage increase slightly due to a lower
privacy budget and extra local indices.
For the purposes of reproducibility we have put the log traces of
all our experiments along with the instructions on how to run them
on a publicly available page epsolute.org. Unless stated otherwise,
the scale in the figures is linear and the ùë•-axis is categorical.
of magnitude performance gain.
Figure 3: Different range-query mechanisms (log scale). Default set-
ting: 106 4 KiB uniformly-sampled records with the range 104.
Question-1: against RDBMS, Linear Scan and Shrinkwrap.
The first experiment we have run using Epsolute is the default
setting in which we observed the query overhead of 840 ms. To put
this number in perspective, we compare Epsolute to conventional
relational databases, the linear scan and Shrinkwrap.
For the default setting, MySQL and PostgreSQL, configured for
no privacy and maximum performance, complete in 97 ms and
220 ms respectively, which is just 8 to 4 times faster than Epsolute,
see Fig. 3. Conventional RDBMS uses efficient indices (B+ trees) to
locate requested records and sends them over without noise and
encryption, and it does so using less hardware resources. In our
experiments RDBMS performance is linearly correlated with the
result and record sizes.
Figure 4: Linear scan performance, logarithmic scale. The experi-
ments are run for the default setting of 106 records of size 4 KiB and
64 threads, with one of the three parameters varying.
Linear scan experiments demonstrate the practicality of Epsolute
compared to a trivial ‚Äúdownload everything every time‚Äù approach,
see Fig. 4. Linear scan‚Äôs overhead is O(ùëõ) regardless of the queries,
while Epsolute‚Äôs overhead is O(log ùëõ) times the result size. Accord-
ing to our experiments, Epsolute eclipses the linear scan at 4 KiB, 64
threads and only ten thousand records (both mechanisms complete
in about 120 ms). For a default setting (at a million records), the
difference is 18 times, see Fig. 4.
Because Shrinkwrap sorts the input obliviously in a circuit model,
it incurs O(ùëõ log ùëõ) comparisons, each resulting in multiple circuit
gates, which is much more expensive than the linear scan. Unlike
linear scan, however, Shrinkwrap does not require much client
memory as the client merely coordinates the query. While Shrink-
wrap supports richer set of relational operators, for range queries
alone Epsolute is three orders of magnitude faster.
ùëõ
ùëõ
Record
1 KiB