title:Systematic Analysis of Randomization-based Protected Cache Architectures
author:Antoon Purnal and
Lukas Giner and
Daniel Gruss and
Ingrid Verbauwhede
Systematic Analysis of Randomization-based
Protected Cache Architectures
Antoon Purnal*, Lukas Giner†, Daniel Gruss†, and Ingrid Verbauwhede*
†Graz University of Technology
*imec-COSIC, KU Leuven
Abstract—Recent secure cache designs aim to mitigate side-
channel attacks by randomizing the mapping from memory
addresses to cache sets. As vendors investigate deployment of
these caches, it is crucial to understand their actual security.
In this paper, we consolidate existing randomization-based se-
cure caches into a generic cache model. We then comprehensively
analyze the security of existing designs,
including CEASER-S
and SCATTERCACHE, by mapping them to instances of this
model. We tailor cache attacks for randomized caches using a
novel PRIME+PRUNE+PROBE technique, and optimize it using
burst accesses, bootstrapping, and multi-step proﬁling. PRIME+
PRUNE+PROBE constructs probabilistic but reliable eviction
sets, enabling attacks previously assumed to be computationally
infeasible. We also simulate an end-to-end attack, leaking secrets
from a vulnerable AES implementation. Finally, a case study of
CEASER-S reveals that cryptographic weaknesses in the random-
ization algorithm can lead to a complete security subversion.
Our systematic analysis yields more realistic and comparable
security levels for randomized caches. As we quantify how
design parameters inﬂuence the security level, our work leads to
important conclusions for future work on secure cache designs.
I. INTRODUCTION
Caches reduce the latency for memory accesses with high
locality. This is crucial for performance but also an inherent
side channel that has been exploited in many microarchitectural
attacks, e.g., on cryptographic implementations [3], [34], [54],
[14], user input [40], [33], [12], [32], system secrets [13], [16],
[9], covert channels [27], [11], [30], and transient-execution
attacks like Spectre [19], [6], [4] and Meltdown [23], [47].
Due to the limited size of the cache, some addresses are
bound to be allocated to the same cache set, i.e., they are
congruent and contend for the same resources. While some
attacks are enabled by the attacker’s capability to ﬂush cache
lines, others work purely with this cache contention. The basic
building block for measuring cache contention is the eviction
set, a set of congruent addresses. Accessing the addresses in
this eviction set brings the cache into a known state. Measuring
how long this takes, tells the attacker whether some process
worked on congruent addresses since the last eviction.
To mitigate contention-based attacks, the cache hardware can
be augmented to so-called protected cache architectures. One
line of work reduces interference through better isolation [41],
[17], [50], [55], [56], [24], [18], [42], or partial isolation (e.g.,
locking cache lines) [51], [8]. Another promising line of work is
randomized cache architectures [51], [52], [21], [25], [26], [45],
[38], [39], [53], which randomize the otherwise predictable
mapping of memory addresses to cache sets. Several recently
Fig. 1: Security argument for randomized caches.
proposed randomized caches [45], [38], [39], [53] evaluate a
dedicated hardware mapping to perform the randomization on
the ﬂy. Consequently, these architectures only slightly change
the interface to the outside, and can maintain efﬁcient and
scalable sharing of caches. However, even if the randomized
mapping is (cryptographically) unpredictable, there are cache
collisions due to the limited size of the cache. Hence, existing
proposals incorporate some notion of rekeying, i.e., renewing
randomization at runtime. This limits the temporal window in
which eviction sets can be used for an attack.
While randomized cache architectures show promise to
thwart eviction-based cache attacks with reasonable overhead,
supporting them with quantiﬁed security claims (a default for
cryptographic algorithms) is challenging. Figure 1 depicts the
established security argument. The randomized mapping is used
as a trust anchor for security in ideal attack conditions, yielding
a (conservative) estimate for the rekeying condition. Currently,
the security transfer from the randomization mapping to ideal-
case security is not well-understood, which we highlight by
improving state-of-the-art attacks by orders of magnitude.
Assuming that system activity increases the attack complexity,
ideal-case security implies real-world security. However, it is
unclear to which extent the rekeying condition can be relaxed.
The high interest in these novel cache designs and their
seeming relevance to mitigate a growing list of attacks motivates
the following fundamental questions of this paper:
Can we accurately compare security levels for randomized
caches? How realistic are security levels reported for secure
randomized caches? Do secure randomized caches provide
substantially higher security levels than regular caches?
In this paper, we systematically cover the attack surface of
randomization-based protected caches. We consolidate existing
proposals into a generic randomized cache model, and identify
attacker objectives in such caches. We then analyze this model,
resulting in a comprehensive and parametrized analysis, serving
as a baseline for future secure caches and their analysis.
We present PRIME+PRUNE+PROBE (PPP), a technique to
ﬁnd probabilistic but reliable eviction sets in randomized caches.
Improving the approach by Werner et al. [53], PPP dramatically
outperforms traditional eviction, turning infeasible attacks (e.g.,
RandomizedmappingIdeal-casesecurityReal-worldsecurityRekeyingconditionSectionV-VISectionVIISectionVIII> 1030 accesses) into feasible ones (e.g., < 107 accesses).
We also analyze security under complicating system effects,
e.g., noise and multiple victim accesses, culminating with
successful key recovery from a vulnerable AES implementation.
Latency constraints associated with the cache hierarchy have
inspired designers to invent new [38], [45] or repurpose exist-
ing [45] low-latency structures for the randomization mapping.
Security arguments then rely on their alleged unpredictability.
We falsify this assumption for CEASER-S, and propose that
future designs use mappings that resist extensive cryptanalysis.
Contributions.
• We consolidate existing proposals into a generic randomized
In summary, our main contributions are:
cache architecture model.
• We derive a comprehensive and parametrized analysis of
all computation-based randomized cache architectures. We
improve noise-free attacks by several orders of magnitude.
• We analyze non-ideal effects in proﬁling on randomized
caches, and demonstrate the ﬁrst end-to-end attack.
• We study the security requirements of the core randomized
mapping and show that the security of CEASER-S can be
completely subverted, even with frequent rekeying.
Outline. Section II provides background. Section III presents
our generalized cache model. Section IV generalizes contention-
based attacks for randomized caches. Section V presents
ideal-case eviction set construction, Section VI describes
optimizations, and Section VII considers aggravating system
effects. Section VIII shows how exploiting internals can
completely subvert security guarantees. Section IX discusses
results and compares existing proposals. Section X concludes.
II. BACKGROUND
A. Caches and Cache Hierarchies
CPUs hide memory latency using caches to buffer data
expected to be used in the near future. Caches are organized in
cache lines. In a directly mapped cache, each memory address
can be cached by exactly one of the cache lines, determined
by a ﬁxed address-based mapping. If a memory address can be
cached in any cache line, the cache is called fully-associative.
If a memory address can only be cached in a (ﬁxed) subset
of cache lines, the cache is called set-associative. Addresses
mapping to the same set are called congruent. Upon a cache
line ﬁll request, a replacement policy determines which cache
line in the set is replaced. The so-called cache line tag uniquely
identiﬁes a cached address. CPU caches can be virtually or
physically indexed and tagged, i.e., cache (set) index and the
cache line tag are derived from the virtual or physical address.
CPUs have multiple cache levels, with the lower levels being
faster and smaller than the higher levels. If all cache lines from
a cache A are required to be also present in a cache B, cache
B is called inclusive with respect to cache A. If a cache line
can only reside in one of two cache levels at the same time, the
caches are called exclusive. If the cache is neither inclusive nor
exclusive, it is called non-inclusive. The last-level cache (LLC)
is often inclusive to lower-level caches and shared across cores
to enhance the performance upon transitioning threads between
cores and to simplify cache coherency and lookups.
The L1 cache is often considered the lowest level cache. It is
usually virtually indexed and physically tagged. All higher-level
caches are usually physically indexed and physically tagged.
Again for performance, the last-level cache today is typically
composed of multiple independent slices, e.g., one slice per
physical or logical core. Each (physical) address maps to one
of the slices. After selecting the slice, the cache (set) index
is selected as described before. The slices are interconnected,
e.g., by a ring bus, allowing all cores to access all last-level
cache lines. The mapping from physical addresses to slices has
been reverse-engineered for certain microarchitectures [28]. In
this work, we focus on the complete mapping function which
combines the mapping from addresses to slices, sets, and lines.
B. Cache Attacks
Caches reduce the latency of memory accesses with temporal
or spatial locality, e.g., recent memory accesses. An attacker can
observe the latency and make deductions, e.g., on other recent
memory accesses. The ﬁrst cache attacks deduced cryptographic
secrets by observing the execution time [20], [35], [46], [3]. The
best techniques today are FLUSH+RELOAD [54] and PRIME+
PROBE [34]. FLUSH+RELOAD ﬂushes an address, then waits,
and by reloading determines whether the victim accessed it
in the meantime. While FLUSH+RELOAD requires a ﬂush
instruction to remove a cache line from all cache levels, EVICT+
RELOAD [12] uses cache contention. Both FLUSH+RELOAD
and EVICT+RELOAD only work on (read-only) memory shared
between attacker and victim. PRIME+PROBE [34] overcomes
this limitation. PRIME+PROBE measures cache contention
instead of memory latency. The attacker ﬁlls (primes) a subset
of the cache (e.g., a slice, a set, a line) and measures (probes)
how long it takes. The time to ﬁll the subset is higher if a
victim replaces an attacker cache line with a congruent address.
Mounting PRIME+PROBE requires information about how
addresses map to cache lines, which can be gained implicitly
in certain scenarios. This is trivial for the L1 cache and, hence,
the ﬁrst PRIME+PROBE attacks targeted the L1 cache [36],
[34]. More recently, PRIME+PROBE attacks were mounted on
last-level caches [27], [33], [29], [22], [30].
Cache attacks based on cache contention generally consist of
two phases. In the proﬁling phase, the attacker ﬁnds a so-called
eviction set, a set of addresses with a high degree of contention
in a subset of the cache. In the exploitation phase, the attacker
accesses this eviction set to bring the cache into a known state.
For EVICT+RELOAD, the attacker uses it to evict an entire
cache set (including a target address) and to later on reload the
target address to determine whether it has been accessed in the
meantime. PRIME+PROBE works similarly, except that it does
not reload the target address but accesses the eviction set again
to measure contention caused by victim memory accesses.
Early approaches for ﬁnding eviction sets were based on
knowing addresses and their congruence, and simply collected a
set of such addresses. With address information unavailable, the
attacker instead starts with a set of addresses, large enough to
be a superset of an eviction set with high probability. Elements
are removed from this set until it has minimal size. Recently,
this eviction set reduction has been improved from quadratic
to linear complexity in the size of the initial set [49], [39].
C. Randomized Cache Architectures
State-of-the-art randomized cache architectures replace pre-
dictable address-to-index mappings with deterministic but
random-looking mappings. The original proposals consider
a software-managed look-up table, whereas newer designs
compute the randomized mapping on-the-ﬂy in hardware.
1) Table-based architectures: RPCache [51] uses a permu-
tation table to randomize the mapping from memory addresses
to cache lines. Occasionally updating the permutation aims to
mitigate statistical attacks. Random-ﬁll cache [25] issues cache
ﬁll requests to random addresses in spatial proximity instead
of the accessed ones. Table-based architectures face scalability
issues, which are especially prohibitive for last-level caches.
2) Computation-based architectures: Recent designs (TIME-
SECURE CACHE [45], CEASER [38],
[39], SCATTER-
CACHE [53]) cope with this scalability problem by computing
the mapping in hardware instead of storing it. This computation
should have very low latency. Given their ﬂexibility and
scalability, computation-based designs are proposed for last-
level caches, which have the largest latency budget and are
important to protect as they are usually shared across cores.
3) Cache partitions: Algorithmic advances in eviction set
construction [49], [39] have shown that only randomizing the
memory address is insufﬁcient to protect against contention-
based cache attacks. As a key insight, CEASER-S and SCAT-
TERCACHE partition the cache and use the randomized mapping
to derive a different cache-set index in each of these partitions.
Not only does this signiﬁcantly raise the bar for ﬁnding eviction
sets, but it also hinders using them in the exploitation phase.
4) Rekeying: Even if the mapping from address to cache set
in each partition is unpredictable, the attacker can, over time,
still identify sets of addresses contending in the cache. Thus,
randomized caches rely on rekeying, i.e., sampling a new key
to refresh the randomization. Selecting an appropriate rekeying
condition marks an important security-performance trade-off.
5) Security analysis: Computation-based randomized caches
show promise to mitigate cache-based side-channel attacks.
Although all proposals come with ﬁrst-party security analyses,
they currently lack a systematic and complete analysis (that
we rely on and know, e.g., for cryptographic schemes).
III. GENERIC RANDOMIZED CACHE MODEL
In this section, we present a generic randomized cache model
that covers all proposed computation-based randomized caches
to this date. We use it to cover the attack surface of randomized
caches systematically. In later sections, we will quantify the
inﬂuence of each parameter on the residual attack complexity.
A. Randomization-based Protected Cache Model
Although some protected cache designs ﬁx the cache
conﬁguration, we consider a generic nw-way set-associative
cache with 2b sets (i.e., b index bits). Then, let N = nw · 2b
denote the number of cache lines. As with traditional caches,
Fig. 2: Computation-based randomized cache model
the atomic unit of the mapping from addresses to cache sets is
the cache line, for which we assume a generic size of 2o bytes
(i.e., o line offset bits). The model makes abstraction of the
line offset bits, as they do not contribute to the randomization.
In accordance with traditional caches, processes cannot
monitor the data in the cache directly, nor can they infer
to which cache way a certain memory address is allocated.
The only interface available is the access latency when reading
speciﬁc addresses, i.e., it is low in case of a cache hit and high
in case of a miss. In some practical cases, an attacker might
also have access to ﬂush semantics. However, our attacks do not
rely on it and we thus assume it to be disabled architecturally.
1) Generic model: Figure 2 depicts our generic computation-
based randomized cache, featuring the following components:
1 The memory address a is the primary input
to the
randomization design. a is either a physical or virtual address,
impacting the degree of control an attacker has over a.
2 The key K captures the design’s entropy (unpredictability).
3 The security domain separator s optionally differentiates
the randomization for processes in different threat domains.
4 The randomized mapping RK(a, s) is the core of the
architecture. It is a pseudorandom mapping, i.e., deterministic
but random-looking, for which the algorithmic description is
publicly known, but the key K is not (Kerckhoff’s principle).
The LLC slicing function can be encapsulated in R (i.e., one
randomized cache), or not (i.e., per-slice randomized caches).
5 The randomized cache is divided into P partitions, where
1 ≤ P ≤ nw. An input address a has, in general, a different
index in each of these partitions. To accommodate this, R has
to supply P · b pseudorandom bits. We assume P divides nw.
6 When caching a, one of the partitions is truly randomly
selected, and the corresponding cache-set index in this partition
is determined based on the pseudorandom output of R. Then,
one of the cache lines in this set is replaced by a, adhering to the
replacement policy within the partition. We consider random
replacement (RAND) and least-recently used (LRU). Under
attack, several stateful policies can degenerate to LRU [10].
7 The rekeying period T denotes the condition for entropy
renewal. It should be strict enough to maintain high security,
and loose enough to maintain high performance.
2) Instantiating Caches: Table I shows how existing designs
instantiate this model. The key K can be a cryptographic key
(CEASER-S, SCATTERCACHE), a set of cryptographic keys, or
selection of a random permutation (TIME-SECURE CACHE).
TIME-SECURE CACHE (TSC) implements domain separation
TABLE I: Instantiating the generic model for existing cache designs.
Design
Unprotected
TSC [45]
CEASER [38]
CEASER-S [39]
SCATTERCACHE [53]
K
∅
keys / select
key
key
key
s
∅
RKs (a)
∅
?
RK (a, s)
P