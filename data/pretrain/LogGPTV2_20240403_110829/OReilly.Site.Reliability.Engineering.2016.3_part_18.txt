operation of services.
Modern research identifies two distinct ways of thinking that an individual may, con‐
sciously or subconsciously, choose when faced with challenges [Kah11]:
128 | Chapter 11: Being On-Call
• Intuitive, automatic, and rapid action
• Rational, focused, and deliberate cognitive functions
When one is dealing with the outages related to complex systems, the second of these
options is more likely to produce better results and lead to well-planned incident
handling.
To make sure that the engineers are in the appropriate frame of mind to leverage the
latter mindset, it’s important to reduce the stress related to being on-call. The impor‐
tance and the impact of the services and the consequences of potential outages can
create significant pressure on the on-call engineers, damaging the well-being of indi‐
vidual team members and possibly prompting SREs to make incorrect choices that
can endanger the availability of the service. Stress hormones like cortisol and
corticotropin-releasing hormone (CRH) are known to cause behavioral consequences
—including fear—that can impair cognitive functions and cause suboptimal decision
making [Chr09].
Under the influence of these stress hormones, the more deliberate cognitive approach
is typically subsumed by unreflective and unconsidered (but immediate) action, lead‐
ing to potential abuse of heuristics. Heuristics are very tempting behaviors when one
is on-call. For example, when the same alert pages for the fourth time in the week,
and the previous three pages were initiated by an external infrastructure system, it is
extremely tempting to exercise confirmation bias by automatically associating this
fourth occurrence of the problem with the previous cause.
While intuition and quick reactions can seem like desirable traits in the middle of
incident management, they have downsides. Intuition can be wrong and is often less
supportable by obvious data. Thus, following intuition can lead an engineer to waste
time pursuing a line of reasoning that is incorrect from the start. Quick reactions are
deep-rooted in habit, and habitual responses are unconsidered, which means they
can be disastrous. The ideal methodology in incident management strikes the perfect
balance of taking steps at the desired pace when enough data is available to make a
reasonable decision while simultaneously critically examining your assumptions.
It’s important that on-call SREs understand that they can rely on several resources
that make the experience of being on-call less daunting than it may seem. The most
important on-call resources are:
• Clear escalation paths
• Well-defined incident-management procedures
• A blameless postmortem culture ([Loo10], [All12])
The developer teams of SRE-supported systems usually participate in a 24/7 on-call
rotation, and it is always possible to escalate to these partner teams when necessary.
Feeling Safe | 129
The appropriate escalation of outages is generally a principled way to react to serious
outages with significant unknown dimensions.
When one is handling incidents, if the issue is complex enough to involve multiple
teams or if, after some investigation, it is not yet possible to estimate an upper bound
for the incident’s time span, it can be useful to adopt a formal incident-management
protocol. Google SRE uses the protocol described in Chapter 14, which offers an
easy-to-follow and well-defined set of steps that aid an on-call engineer to rationally
pursue a satisfactory incident resolution with all the required help. This protocol is
internally supported by a web-based tool that automates most of the incident man‐
agement actions, such as handing off roles and recording and communicating status
updates. This tool allows incident managers to focus on dealing with the incident,
rather than spending time and cognitive effort on mundane actions such as format‐
ting emails or updating several communication channels at once.
Finally, when an incident occurs, it’s important to evaluate what went wrong, recog‐
nize what went well, and take action to prevent the same errors from recurring in the
future. SRE teams must write postmortems after significant incidents and detail a full
timeline of the events that occurred. By focusing on events rather than the people,
these postmortems provide significant value. Rather than placing blame on individu‐
als, they derive value from the systematic analysis of production incidents. Mistakes
happen, and software should make sure that we make as few mistakes as possible.
Recognizing automation opportunities is one of the best ways to prevent human
errors [Loo10].
Avoiding Inappropriate Operational Load
As mentioned in “Balanced On-Call” on page 127, SREs spend at most 50% of their
time on operational work. What happens if operational activities exceed this limit?
Operational Overload
The SRE team and leadership are responsible for including concrete objectives in
quarterly work planning in order to make sure that the workload returns to sustaina‐
ble levels. Temporarily loaning an experienced SRE to an overloaded team, discussed
in Chapter 30, can provide enough breathing room so that the team can make head‐
way in addressing issues.
Ideally, symptoms of operational overload should be measurable, so that the goals can
be quantified (e.g., number of daily tickets < 5, paging events per shift < 2).
Misconfigured monitoring is a common cause of operational overload. Paging alerts
should be aligned with the symptoms that threaten a service’s SLOs. All paging alerts
should also be actionable. Low-priority alerts that bother the on-call engineer every
hour (or more frequently) disrupt productivity, and the fatigue such alerts induce can
130 | Chapter 11: Being On-Call
also cause serious alerts to be treated with less attention than necessary. See Chap‐
ter 29 for further discussion.
It is also important to control the number of alerts that the on-call engineers receive
for a single incident. Sometimes a single abnormal condition can generate several
alerts, so it’s important to regulate the alert fan-out by ensuring that related alerts are
grouped together by the monitoring or alerting system. If, for any reason, duplicate
or uninformative alerts are generated during an incident, silencing those alerts can
provide the necessary quiet for the on-call engineer to focus on the incident itself.
Noisy alerts that systematically generate more than one alert per incident should be
tweaked to approach a 1:1 alert/incident ratio. Doing so allows the on-call engineer to
focus on the incident instead of triaging duplicate alerts.
Sometimes the changes that cause operational overload are not under the control of
the SRE teams. For example, the application developers might introduce changes that
cause the system to be more noisy, less reliable, or both. In this case, it is appropriate
to work together with the application developers to set common goals to improve the
system.
In extreme cases, SRE teams may have the option to “give back the pager”—SRE can
ask the developer team to be exclusively on-call for the system until it meets the
standards of the SRE team in question. Giving back the pager doesn’t happen very
frequently, because it’s almost always possible to work with the developer team to
reduce the operational load and make a given system more reliable. In some cases,
though, complex or architectural changes spanning multiple quarters might be
required to make a system sustainable from an operational point of view. In such
cases, the SRE team should not be subject to an excessive operational load. Instead, it
is appropriate to negotiate the reorganization of on-call responsibilities with the
development team, possibly routing some or all paging alerts to the developer on-call.
Such a solution is typically a temporary measure, during which time the SRE and
developer teams work together to get the service in shape to be on-boarded by the
SRE team again.
The possibility of renegotiating on-call responsibilities between SRE and product
development teams attests to the balance of powers between the teams.2 This working
relationship also exemplifies how the healthy tension between these two teams and
the values that they represent—reliability versus feature velocity—is typically resolved
by greatly benefiting the service and, by extension, the company as a whole.
2 For more discussion on the natural tension between SRE and product development teams, see Chapter 1.
Avoiding Inappropriate Operational Load | 131
A Treacherous Enemy: Operational Underload
Being on-call for a quiet system is blissful, but what happens if the system is too quiet
or when SREs are not on-call often enough? An operational underload is undesirable
for an SRE team. Being out of touch with production for long periods of time can
lead to confidence issues, both in terms of overconfidence and underconfidence,
while knowledge gaps are discovered only when an incident occurs.
To counteract this eventuality, SRE teams should be sized to allow every engineer to
be on-call at least once or twice a quarter, thus ensuring that each team member is
sufficiently exposed to production. “Wheel of Misfortune” exercises (discussed in
Chapter 28) are also useful team activities that can help to hone and improve trouble‐
shooting skills and knowledge of the service. Google also has a company-wide annual
disaster recovery event called DiRT (Disaster Recovery Training) that combines theo‐
retical and practical drills to perform multiday testing of infrastructure systems and
individual services; see [Kri12].
Conclusions
The approach to on-call described in this chapter serves as a guideline for all SRE
teams in Google and is key to fostering a sustainable and manageable work environ‐
ment. Google’s approach to on-call has enabled us to use engineering work as the pri‐
mary means to scale production responsibilities and maintain high reliability and
availability despite the increasing complexity and number of systems and services for
which SREs are responsible.
While this approach might not be immediately applicable to all contexts in which
engineers need to be on-call for IT services, we believe it represents a solid model that
organizations can adopt in scaling to meet a growing volume of on-call work.
132 | Chapter 11: Being On-Call
CHAPTER 12
Effective Troubleshooting
Written by Chris Jones
Be warned that being an expert is more than understanding how a system is supposed to
work. Expertise is gained by investigating why a system doesn’t work.
—Brian Redman
Ways in which things go right are special cases of the ways in which things go wrong.
—John Allspaw
Troubleshooting is a critical skill for anyone who operates distributed computing sys‐
tems—especially SREs—but it’s often viewed as an innate skill that some people have
and others don’t. One reason for this assumption is that, for those who troubleshoot
often, it’s an ingrained process; explaining how to troubleshoot is difficult, much like
explaining how to ride a bike. However, we believe that troubleshooting is both learn‐
able and teachable.
Novices are often tripped up when troubleshooting because the exercise ideally
depends upon two factors: an understanding of how to troubleshoot generically (i.e.,
without any particular system knowledge) and a solid knowledge of the system.
While you can investigate a problem using only the generic process and derivation
from first principles,1 we usually find this approach to be less efficient and less effec‐
tive than understanding how things are supposed to work. Knowledge of the system
typically limits the effectiveness of an SRE new to a system; there’s little substitute to
learning how the system is designed and built.
1 Indeed, using only first principles and troubleshooting skills is often an effective way to learn how a system
works; see Chapter 28.
133
Let’s look at a general model of the troubleshooting process. Readers with expertise in
troubleshooting may quibble with our definitions and process; if your method is
effective for you, there’s no reason not to stick with it.
Theory
Formally, we can think of the troubleshooting process as an application of the
hypothetico-deductive method:2 given a set of observations about a system and a the‐
oretical basis for understanding system behavior, we iteratively hypothesize potential
causes for the failure and try to test those hypotheses.
In an idealized model such as that in Figure 12-1, we’d start with a problem report
telling us that something is wrong with the system. Then we can look at the system’s
telemetry3 and logs to understand its current state. This information, combined with
our knowledge of how the system is built, how it should operate, and its failure
modes, enables us to identify some possible causes.
Figure 12-1. A process for troubleshooting
2 See https://en.wikipedia.org/wiki/Hypothetico-deductive_model.
3 For instance, exported variables as described in Chapter 10.
134 | Chapter 12: Effective Troubleshooting
We can then test our hypotheses in one of two ways. We can compare the observed
state of the system against our theories to find confirming or disconfirming evidence.
Or, in some cases, we can actively “treat” the system—that is, change the system in a
controlled way—and observe the results. This second approach refines our under‐
standing of the system’s state and possible cause(s) of the reported problems. Using
either of these strategies, we repeatedly test until a root cause is identified, at which
point we can then take corrective action to prevent a recurrence and write a postmor‐
tem. Of course, fixing the proximate cause(s) needn’t always wait for root-causing or
postmortem writing.
Common Pitfalls
Ineffective troubleshooting sessions are plagued by problems at the Triage, Examine,
and Diagnose steps, often because of a lack of deep system understanding. The fol‐
lowing are common pitfalls to avoid:
• Looking at symptoms that aren’t relevant or misunderstanding the meaning of
system metrics. Wild goose chases often result.
• Misunderstanding how to change the system, its inputs, or its environment, so as
to safely and effectively test hypotheses.
• Coming up with wildly improbable theories about what’s wrong, or latching on
to causes of past problems, reasoning that since it happened once, it must be hap‐
pening again.
• Hunting down spurious correlations that are actually coincidences or are correla‐
ted with shared causes.
Fixing the first and second common pitfalls is a matter of learning the system in ques‐
tion and becoming experienced with the common patterns used in distributed sys‐
tems. The third trap is a set of logical fallacies that can be avoided by remembering
that not all failures are equally probable—as doctors are taught, “when you hear hoof‐
beats, think of horses not zebras.”4 Also remember that, all things being equal, we
should prefer simpler explanations.5
4 Attributed to Theodore Woodward, of the University of Maryland School of Medicine, in the 1940s. See
https://en.wikipedia.org/wiki/Zebra_(medicine). This works in some domains, but in some systems, entire
classes of failures may be eliminable: for instance, using a well-designed cluster filesystem means that a
latency problem is unlikely to be due to a single dead disk.
5 Occam’s Razor; see https://en.wikipedia.org/wiki/Occam%27s_razor. But remember that it may still be the case
that there are multiple problems; in particular, it may be more likely that a system has a number of common
low-grade problems that, taken together, explain all the symptoms rather than a single rare problem that
causes them all. Cf https://en.wikipedia.org/wiki/Hickam%27s_dictum.
Theory | 135
Finally, we should remember that correlation is not causation:6 some correlated
events, say packet loss within a cluster and failed hard drives in the cluster, share
common causes—in this case, a power outage, though network failure clearly doesn’t
cause the hard drive failures nor vice versa. Even worse, as systems grow in size and
complexity and as more metrics are monitored, it’s inevitable that there will be events
that happen to correlate well with other events, purely by coincidence.7
Understanding failures in our reasoning process is the first step to avoiding them and
becoming more effective in solving problems. A methodical approach to knowing
what we do know, what we don’t know, and what we need to know, makes it simpler
and more straightforward to figure out what’s gone wrong and how to fix it.
In Practice
In practice, of course, troubleshooting is never as clean as our idealized model sug‐
gests it should be. There are some steps that can make the process less painful and
more productive for both those experiencing system problems and those responding
to them.
Problem Report
Every problem starts with a problem report, which might be an automated alert or
one of your colleagues saying, “The system is slow.” An effective report should tell
you the expected behavior, the actual behavior, and, if possible, how to reproduce the
behavior.8 Ideally, the reports should have a consistent form and be stored in a search‐
able location, such as a bug tracking system. Here, our teams often have customized
forms or small web apps that ask for information that’s relevant to diagnosing the
particular systems they support, which then automatically generate and route a bug.
This may also be a good point at which to provide tools for problem reporters to try
self-diagnosing or self-repairing common issues on their own.
It’s common practice at Google to open a bug for every issue, even those received via
email or instant messaging. Doing so creates a log of investigation and remediation
activities that can be referenced in the future. Many teams discourage reporting prob‐
lems directly to a person for several reasons: this practice introduces an additional
step of transcribing the report into a bug, produces lower-quality reports that aren’t
6 Of course, see https://xkcd.com/552.
7 At least, we have no plausible theory to explain why the number of PhDs awarded in Computer Science in the
US should be extremely well correlated (r2 = 0.9416) with the per capita consumption of cheese, between 2000
and 2009: http://tylervigen.com/view_correlation?id=1099.
8 It may be useful to refer prospective bug reporters to [Tat99] to help them provide high-quality problem
reports.
136 | Chapter 12: Effective Troubleshooting
visible to other members of the team, and tends to concentrate the problem-solving
load on a handful of team members that the reporters happen to know, rather than
the person currently on duty (see also Chapter 29).
Shakespeare Has a Problem
You’re on-call for the Shakespeare search service and receive an alert, Shakespeare-
BlackboxProbe_SearchFailure: your black-box monitoring hasn’t been able to find
search results for “the forms of things unknown” for the past five minutes. The alert‐
ing system has filed a bug—with links to the black-box prober’s recent results and to
the playbook entry for this alert—and assigned it to you. Time to spring into action!
Triage
Once you receive a problem report, the next step is to figure out what to do about it.
Problems can vary in severity: an issue might affect only one user under very specific
circumstances (and might have a workaround), or it might entail a complete global
outage for a service. Your response should be appropriate for the problem’s impact:
it’s appropriate to declare an all-hands-on-deck emergency for the latter (see Chap‐