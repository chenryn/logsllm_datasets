Delay (s)
 9
 12
s
t
s
a
c
d
a
o
r
B
f
o
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
 1
 0.8
 0.6
 0.4
 0.2
s
t
s
a
c
d
a
o
r
B
f
o
F
D
C
 0
 0
 0.5
 1
2s
3s
4s
 1.5
 2
2s
3s
4s
 0.5
 1
 1.5
 2
 2.5
 3
Average Polling Delay per Broadcast (s)
Variance of Polling Delay per Broadcast (s)
Figure 11: HLS/RTMP end-to-end de-
lay breakdown.
Figure 12: CDF of average polling de-
lay with diﬀerent polling intervals.
Figure 13: CDF of polling delay vari-
ance with diﬀerent polling intervals.
cover new chunks. Furthermore, after a fresh chunk is avail-
able on Wowza, it is only transferred to Fastly when an HLS
viewer polls Fastly. These extra overheads lead to a consid-
erable amount of polling delay for HLS viewers.
To study the polling delay in detail, we use our passive
measurements to perform trace-driven simulations9. Since
our crawler polls the Fastly server at a very high frequency
of once every 0.1s, it can immediately identify the arrival of
new chunks on Fastly servers. Using the collected chunk ar-
rival timestamps, we simulate periodic polling for each sin-
gle HLS viewer of the broadcast10, and study both the aver-
age and standard deviation of the polling delay within each
broadcast. The results are shown in Figure 12 and 13.
We make two key observations. First, the average polling
delay is large (1-2s) and increases with the polling interval.
Using 2s and 4s polling intervals, the average delay is half of
the polling interval. Yet using 3s interval, the average delay
varies largely between 1s and 2s. This is because the chunk
inter-arrival time varies slightly around 3s, thus polling at the
same frequency creates large delay variations. Second, the
STD results in Figure 13 show that the polling delay varies
largely within each broadcast as viewers are unable to pre-
dict the chunk arrival time. Such high variance translates
into delay jitters at the viewer device which are then handled
by the client-buﬀering.
The choice of polling interval also reﬂects the tradeoﬀ be-
tween scalability and latency. Smaller polling interval re-
duces average polling delay and variance, but comes at the
cost of generating more polling requests to the CDN servers.
Our controlled experiments revealed that in Periscope, the
polling interval varies between 2s and 2.8s.
Scalability. We also compare the cost of supporting scal-
ability when running RTMP and HLS. We set up a Wowza
Stream Engine11 on a laptop, serving as a CDN node. The
laptop has 8GB memory, 2.4GHz Intel Core i7 CPU, and
9As discussed earlier, our crawl of real-world broadcasts
captures the delay from broadcasters to CDNs but not those
from CDNs to viewers.
10Here we assume that the per-viewer polling delay does not
depend on the number of viewers. That is, the CDN servers
have suﬃcient resources to handle all polling requests. We
also assume the communication delay between the viewer
and CDN is negligible compared to the polling delay.
11https://www.wowza.com/products/streaming-engine
1Gbps bandwidth. We start RTMP and HLS viewers (sepa-
rately) on other machines connecting to the laptop via Ether-
net, and measure the laptop’s resource usage including CPU
and memory as we vary the number of viewers. Our re-
sults show that RTMP and HLS result in similar and sta-
ble memory consumption but very diﬀerent CPU usage with
respect to diﬀerent numbers of viewers. Speciﬁcally, Fig-
ure 14 shows that supporting RTMP users requires much
higher CPU power than that for HLS. The gap between the
two elevates with the number of viewers, demonstrating the
higher cost of supporting RTMP scalability. The key rea-
son for RTMP’s higher CPU usage is that it operates on
small frames instead of large chunks, leading to signiﬁcantly
higher processing overhead.
5.3
Impact of CDN Geolocation
We now examine the impact of CDN geolocation on Periscope
performance. By studying our detailed broadcast measure-
ments, we seek to understand how Wowza and Fastly con-
nect Periscope users to their data centers and how such as-
signment aﬀects end-to-end delay. Furthermore, using our
knowledge on the Wowza and Fastly data center locations,
we examine the chunk transfer delay between their data cen-
ters (i.e. the Wowza2Fastly delay).
We discov-
Periscope’s Geolocation Optimization.
ered three types of geolocation optimization that Periscope
uses to improve scalability and latency. First, Periscope
connects each broadcaster to the nearest Wowza data cen-
ter12, which will eﬀectively reduce the upload delay. Sec-
ond, Fastly fetches video chunks from Wowza and copies
them to multiple Fastly data centers, which helps to provide
scalability. Finally, each HLS viewer connects to the near-
est Fastly data center using IP anycast, thus minimizing the
last mile delay. In contrast, RTMP viewers always connect
to the same Wowza data center that the broadcaster connects
to, avoiding data transfer among Wowza data centers.
The CDN geoloca-
Wowza to Fastly Transmission.
tion also directly aﬀects the Wowza-to-Fastly delay, i.e. the
latency for Wowza to distribute video chunks to diﬀerent
Fastly data centers. We estimate this latency from our crawled
12More than half of our crawled broadcasts contain the GPS
coordinates of the broadcaster. Thus we directly compare
each broadcaster’s location to her associated Wowza data
center location.
493)
%
(
e
g
a
s
U
U
P
C
 100
 80
 60
 40
 20
 0
RTMP
HLS
100
200
300
400
500
# of Viewers
s
t
s
a
c
d
a
o
r
B
f
o
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
Co-located (0km)
(0, 500km]
(500, 5,000km]
(5,000, 10,000km]
>10,000km
 0.5
 1
 1.5
 2
Wowza2Fastly Delay (s)
Figure 14: CPU usage of server using
RTMP and HLS with diﬀerent num-
ber of viewers.
Figure 15: Wowza-to-Fastly Delay. We
group datacenter pairs based on their
geographic distance. Co-located pairs
are located at the same city.
broadcast traces13. Figure 15 plots the CDF of the Wowza2Fastly
delay across broadcasts, where we group Wowza and Fastly
data center pairs based on their geographical distance (dis-
tance between the cities). Here co-located means that the
two data centers are in the same city. As expected, pairs
with longer distances experience larger delay.
Figure 15 also shows a clear distinction between co-located
pairs and all other pairs. In particular, there is a signiﬁcant
gap (>0.25s) between the latency of co-located pairs and that
of nearby city pairs (<500km), e.g., San Francisco to Los
Angeles. We suspect that this is because each Wowza server
ﬁrst transfers video chunks to the co-located Fastly data cen-
ter, who then behaves as a gateway and distributes the data to
other Fastly data centers. The extra delay is likely the result
of coordination between this gateway and other data centers.
6. OPTIMIZING CLIENT-SIDE BUFFER-
ING
Our delay analysis has shown that client-side buﬀering in-
troduces signiﬁcant delay to both RTMP and HLS viewers.
The resulting video playback delay depends on the broad-
cast channel quality, buﬀering strategies and conﬁgurations
at the mobile app. While buﬀering is necessary to ensure
smooth video replay, setting reasonable parameters for the
buﬀer can dramatically impact streaming delay. In this sec-
tion, we perform trace-driven experiments to understand if
(and by how much) Periscope can reduce this buﬀering de-
lay, and the corresponding impact on end-user video quality.
Buﬀering Strategy. We start by describing the buﬀering
strategy. For HLS, we were able to decompile Periscope’s
Android source code and analyze its media player settings.
We found that Periscope users a naive buﬀering strategy.
Speciﬁcally, when the live streaming starts, the client (viewer)
ﬁrst pre-buﬀers some video content (P seconds of video) in
13Our HLS crawler (as a viewer) polls each Fastly server at
a very high frequency of once per 0.1s, and thus can record
the earliest time that a chunklist is updated at each Fastly
server. Furthermore, operating at a polling frequency 20+
times faster than normal HLS viewers, our crawler will ini-
tiate the ﬁrst poll on the Fastly server that triggers the server
to poll Wowza and obtain a fresh chunklist. Thus we are
able to estimate the Wowza2Fastly delay 11 - 9 (Figure 10)
by measuring 11 - 7 and minimizing 9 - 7 .
its buﬀer. During live streaming playback, the client inserts
newly arrived video content into the buﬀer, which are orga-
nized and played by their sequence numbers to mitigate de-
lay jitter and out-of-order delivery. Arrivals that come later
than their scheduled play time are discarded. This smoothes
out video playback but also introduces extra latency. We
found from the source code that Periscope conﬁgures a suﬃ-
ciently large memory to buﬀer video content and avoid drop-
ping packets.
We were unable to identify the exact buﬀering setting for
RTMP, because Periscope implements a customized RTMP
protocol stack, and the source code is obfuscated. But results
from our controlled experiments suggest that Periscope uses
the same buﬀering strategy for RTMP and HLS.
Next, we implement the above buﬀering strategy in trace-
driven simulations for both RTMP and HLS clients. Our
goal is to understand how Periscope’s buﬀering parameters
impact playback delay, and whether it can be optimized for
better performance. For simplicity, we do not put any hard
constraint on the physical buﬀer size, but vary the pre-buﬀer
size (P) to study its impact.
We perform trace-driven simulations us-
Experiments.
ing our measurements on our 16,013 real-world broadcasts.
For each broadcast, we extract from our measurements a se-
quence of video frame/chunk arrival times (at the Wowza/Fastly
server). This covers the path from the broadcaster to the
CDN. We then simulate the path from the CDN to the view-
ers. For RTMP viewers, we implement client-side buﬀering,
and for HLS viewers, we implement polling (at an interval
of 2.8s) and buﬀering. While we cannot capture possible de-
lay variances from last mile links, we assume it is under one
second in our simulations.
We evaluate the viewer experience via two metrics. The
ﬁrst is the video play smoothness, represented by the stalling
ratio, i.e. the duration of stalling moment (no video to play)
divided by the duration of the broadcast. The second metric
is the buﬀering delay, the time gap between a video frame/chunk
arriving at the viewer till the time it got played. For each
broadcast, we compute the average buﬀering delay across
all the frames/chunks.
Figure 16(a)-(b) plot the stalling ratio and average buﬀer-
ing delay for RTMP users by varying P from 0s to 1s. As
expected, pre-buﬀering more video content (i.e.
larger P)
494s
t
s
a
c
d
a
o
r
B
f
o
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
1s
0.5s
0s
 0.02
 0.04
 0.06
 0.08
 0.1
RTMP Stalling Ratio
(a) Stalling Time Ratio
s
t
s
a
c
d
a
o
r
B
f
o
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
0s
0.5s
1s
 0
 1
 2
 3
 4
 5
 6
 7
 8
 9  10
RTMP Buffering Delay (s)
(b) Buﬀering Delay
Figure 16: RTMP: the impact of diﬀerent buﬀer size (for pre-download) to
buﬀering delay and stalling.
s
t
s
a
c
d
a
o
r
B
f
o
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
 0
9s
6s
3s
0s
 0.1
 0.2
HLS Stalling Ratio
(a) Stalling Time Ratio
 1
 0.8
 0.6
 0.4