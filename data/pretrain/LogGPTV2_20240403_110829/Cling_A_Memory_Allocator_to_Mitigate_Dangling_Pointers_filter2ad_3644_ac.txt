fully because stack unwinding is normally intended for
use in slow, error handing code paths.
To detect simple allocation wrappers, Cling initiates
a probing mechanism after observing a single allocation
site requesting multiple allocation sizes. This probing
ﬁrst uses a costly but reliable unwind of the caller’s stack
frame (using libunwind) to discover the stack loca-
tion of the suspected wrapper function’s return address.
Then, after saving the original value, Cling overwrites
the wrapper’s return address on the stack with the ad-
dress of a special assembler routine that will be inter-
posed when the suspected wrapper returns. After Cling
returns to the caller, and, in turn, the caller returns, the
overwritten return address transfers control to the inter-
posed routine. This routine compares the suspected al-
location wrapper’s return value with the address of the
memory allocated by Cling, also saved when the probe
was initiated. If the caller appears to return the address
just returned by Cling, it is assumed to be a simple wrap-
per around an allocation function.
To simplify the implementation, probing is aborted if
the potential wrapper function issues additional alloca-
tion requests before returning. This is not a problem in
practice, because simple malloc wrappers usually per-
form a single allocation. Moreover, a more thorough im-
plementation can easily address this.
The probing mechanism is only initiated when multi-
ple allocation sizes are requested from a single alloca-
tion site, potentially delaying wrapper identiﬁcation. It
is unlikely, however, that an attacker could exploit this
window of opportunity in large programs. Furthermore,
this rule helps prevent misidentifying typical functions
encapsulating the allocation and initialization of objects
of a single type, because these request objects of a sin-
gle size. Sometimes, such functions allocate arrays of
various sizes, and can be misidentiﬁed. Nevertheless,
these false positives are harmless for security; they only
introduce more pools that affect performance by over-
constraining allocation, and the performance impact in
our benchmarks was small.
Similarly, the current implementation identiﬁes func-
tions such as strdup as allocation wrappers. While we
could safely pool their allocations (they are of the same
type), the performance impact in our benchmarks was
again small, so we do not handle them in any special
way.
While this probing mechanism handles well the com-
mon case of malloc wrappers that return the allocated
memory through their function return value, it would not
detect a wrapper that uses some other mechanism to re-
turn the memory, such as modifying a pointer argument
passed to the wrapper by reference. Fortunately, such
malloc wrappers are unusual.
Allocation sites identiﬁed as potential wrappers
through this probing mechanism are marked as such in
the hashtable mapping allocation site addresses to their
pools, so Cling can unwind one more stack level to get
the real allocation site whenever allocation requests from
such an allocation site are encountered, and associate it
with a distinct pool.
Stack unwinding is platform speciﬁc and, in general,
expensive. In 32-bit x86 systems, the frame pointer reg-
ister ebp links stacks frames together, making unwind-
ing reasonably fast, but this register may be re-purposed
in optimized builds. Heuristics can still be used with
optimized code, e.g. looking for a value in the stack
that points into the text segment, but they are slower.
Data-driven stack unwinding on 64-bit AMD64 systems
is more reliable but, again, expensive. Cling uses the
libunwind library to encapsulate platform speciﬁc de-
tails of stack unwinding, but caches the stack offset of
wrappers’ return addresses to allow fast unwinding when
possible, as described next, and gives up unwinding if
not.
Care must be taken when using a cached stack offset to
retrieve the real allocation site, because the cached value
may become invalid for functions with a variable frame
size, e.g. those using alloca, resulting in the retrieval
of a bogus address. To guard against this, whenever a
new allocation site is encountered that was retrieved us-
ing a cached stack offset, a slow but reliable unwind (us-
ing libunwind) is performed to conﬁrm the allocation
site’s validity. If the check fails, the wrapper must have
a variable frame size, and Cling falls back to allocating
all memory requested through that wrapper from a single
pool. In practice, typical malloc wrappers are simple
functions with constant frame sizes.
3.6 Limitations
Cling prevents vtable hijacking, the standard exploita-
tion technique for use-after-free vulnerabilities, and its
constraints on function and data pointers are likely to
prevent their exploitation, but it may not be able to pre-
vent use-after-free attacks targeting data such as creden-
tials and access control lists stored in objects of a single
type. For example, a dangling pointer that used to point
to the credentials of one user may end up pointing to the
credentials of another user.
Another theoretical attack may involve data structure
inconsistencies, when accessed through dangling point-
ers. For example, if a buffer and a variable holding its
length are in separate objects, and one of them is read
through a dangling pointer accessing an unrelated object,
the length variable may be inconsistent with the actual
buffer length, allowing dangerous bound violations. In-
terestingly, this can be detected if Cling is used in con-
junction with a defense offering spatial protection.
Cling relies on mapping allocation sites to object
types. A program with contrived ﬂow of control, how-
ever, such as in the following example, would obscure
the type of allocation requests:
1 int size = condition ? sizeof( ←(cid:45)
struct A) : sizeof(struct B);
2 void *obj = malloc(size);
Fortunately, this situation is less likely when allocating
memory using the C++ operator new that requires a type
argument.
A similar problem occurs when the allocated object is
a union: objects allocated at the same program location
may still have different types of data at the same offset.
Tail-call optimizations can also obscure allocation
sites. Tail-call optimization is applicable when the call to
malloc is the last instruction before a function returns.
The compiler can then replace the call instruction with
a simple control-ﬂow transfer to the allocation routine,
avoiding pushing a return address to the stack. In this
case, Cling would retrieve the return address of the func-
tion calling malloc. Fortunately, in most cases where
this situation might appear, using the available return ad-
dress still identiﬁes the allocation site uniquely.
Cling cannot prevent unsafe reuse of stack allocated
objects, for example when a function erroneously returns
a pointer to a local variable. This could be addressed by
using Cling as part of a compiler-based solution, by mov-
ing dangerous (e.g. address taken) stack based variables
to the heap at compile time.
Custom memory allocators are a big concern. They al-
locate memory in huge chunks from the system allocator,
and chop them up to satisfy allocation requests for indi-
vidual objects, concealing the real allocation sites of the
program. Fortunately, many custom allocators are used
for performance when allocating many objects of a sin-
gle type. Thus, pooling such custom allocator’s requests
to the system allocator, as done for any other allocation
site, is sufﬁcient to maintain type-safe memory reuse. It
is also worth pointing that roll-your-own general purpose
memory allocators have become a serious security liabil-
ity due to a number of exploitable memory management
bugs beyond use-after-free (invalid frees, double frees,
and heap metadata corruption in general). Therefore, us-
ing a custom allocator in new projects is not a decision
to be taken lightly.
Usability in 32-bit platforms with scarce address space
is limited. This is less of a concern for high-end and fu-
ture machines. If necessary, however, Cling can be com-
bined with a simple conservative collector that scans all
words in used physical memory blocks for pointers to
used address space blocks. This solution avoids some
performance and compatibility problems of conservative
garbage collection by relying on information about ex-
plicit deallocations. Once address space is exhausted,
only memory that is in use needs to be scanned and
any 16K block of freed memory that is not pointed by
any word in the scanned memory can be reused. The
chief compatibility problem of conservative garbage col-
lection, namely hidden pointers (manufactured pointers
invisible to the collector), cannot cause premature deal-
locations, because only explicitly deallocated memory
would be garbage collected in this scheme. Neverthe-
less, relying on the abundant address space of modern
machines instead, is more attractive, because garbage
collection may introduce unpredictability or expose the
program to attacks using hidden dangling pointers.
Implementation
3.7
Cling comes as a shared library providing implementa-
tions for the malloc and the C++ operator new alloca-
tion interfaces. It can be preloaded with platform speciﬁc
mechanisms (e.g. the LD PRELOAD environment vari-
able on most Unix-based systems) to override the sys-
tem’s memory allocation routines at program load time.
4 Experimental Evaluation
4.1 Methodology
We measured Cling’s CPU, physical memory, and vir-
tual address space overheads relative to the default GNU
libc memory allocator on a 2.66GHz Intel Core 2 Q9400
Figure 7: Cumulative distribution function of memory
allocation sizes for gzip, vpr, gcc, parser, and
equake.
Figure 9: Cumulative distribution function of mem-
ory allocation sizes for hmmer, h264ref, omnetpp,
astar, and dealII.
Figure 8: Cumulative distribution function of mem-
ory allocation sizes for perlbmk, vortex, twolf,
espresso, and gobmk.
Figure 10: Cumulative distribution function of mem-
ory allocation sizes for sphinx3, soplex, povray,
xalancbmk, and Firefox.
CPU with 4GB of RAM, running x86 64 GNU/Linux
with a version 2.6 Linux kernel. We also measured two
variations of Cling: without wrapper unwinding and us-
ing a single pool.
We used benchmarks from the SPEC CPU 2000
and (when not already included in CPU 2000) 2006
benchmark suites [22].
Programs with few alloca-
tions and deallocations have practically no overhead
with Cling, thus we present results for SPEC bench-
marks with at least 100,000 allocation requests. We also
used espresso, an allocation intensive program that
is widely used in memory management studies, and is
useful when comparing against related work. Finally,
in addition to CPU bound benchmarks, we also evalu-
ated Cling with a current version of the Mozilla Firefox
web browser. Web browsers like Firefox are typical at-
tack targets for use-after-free exploits via malicious web
sites; moreover, unlike many benchmarks, Firefox is an
application of realistic size and running time.
Some programs use custom allocators, defeating
Cling’s protection and masking its overhead. For these
experiments, we disabled a custom allocator implemen-
tation in parser. The gcc benchmark also uses a
custom allocation scheme (obstack) with different
semantics from malloc that cannot be readily dis-
abled. We include it to contrast its allocation size
distribution with those of other benchmarks. Recent
versions of Firefox also use a custom allocator [10]
that was disabled by compiling from source with the
--disable-jemalloc conﬁguration option.
The SPEC programs come with prescribed input data.
For espresso, we generated a uniformly random input
ﬁle with 15 inputs and 15 outputs, totalling 32K lines.
For Firefox, we used a list of 200 websites retrieved from
our browsing history, and replayed it using the -remote
 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 14M1M256K64K16K4K1K256641641Fraction of Allocation RequestsAllocation Request Size (Bytes)gzipvprgccparserequake 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 14M1M256K64K16K4K1K256641641Fraction of Allocation RequestsAllocation Request Size (Bytes)perlbmkvortextwolfespressogobmk 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 14M1M256K64K16K4K1K256641641Fraction of Allocation RequestsAllocation Request Size (Bytes)hmmerh264refomnetppastardealII 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 14M1M256K64K16K4K1K256641641Fraction of Allocation RequestsAllocation Request Size (Bytes)sphinx3soplexpovrayxalancbmkfirefoxBenchmark
CPU2000
gzip
vpr
gcc
parser
equake
perlbmk
vortex
twolf
CPU2006
gobmk
hmmer
dealII
sphinx3
h264ref
omnetpp
soplex
povray
astar
xalancbmk
Other
espresso
ﬁrefox
Allocation Sites
Not Wrappers Wrappers
Unwound
Allocation Requests
Small
Large
Deallocation Requests
Large
Small
3
11
5
218
31
10
5
3
50
8
285
25
342
158
285
44
102
304
0
2
1
3
0
3
0
1
5
4
0
2
0
1
6
0
0
1
49
2101
7
51
0
59
66
3
0
90
0
129
15
107
0
6
0
17
25
0
0
1
14
595
419,724
107,184
194,871
787,695,542
1,335,048
31,399,586
4,594,278
574,552
621,144
2,405,928
151,324,612
14,160,472
168,634
267,167,577
190,986
2,414,082
4,797,794
135,037,352
16,483
547
4,922
46,532
19
33,088
28,094
15
20