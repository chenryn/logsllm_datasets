In the other case (if the signature was invalid), the server
sends a request message to all other servers asking them to
compute the signature shares again, but this time along with
the proofs. Upon receiving such a request, each server gen-
erates the proof for its signature share and sends the share
along with this proof to all servers. These shares with proofs
are then processed in the same way as in the unoptimized
algorithm. In parallel to the last two steps, the server also
waits for receiving a valid ﬁnal signature and terminates as
soon as a correct signature is received.
Note that the step of waiting in parallel for a valid sig-
nature is necessary since it is not guaranteed that there are
still enough honest replicas around to resend their shares
with proofs — they might already have terminated the pro-
tocol. But then they have sent out the correct signature and
it will be received by the waiting server.
Optimistic Signature Protocol with Trial and Error
(OPTTE). Protocol OPTPROOF above works well when all
servers are honest, but its performance is worse than the un-
optimized protocol in the presence of corrupted servers.
We therefore propose another optimization that per-
forms much better in the presence of corrupted servers
than both schemes above, but works only for
rela-
tively small . For practical values of , however, it turns
out to be the fastest variation.
This protocol exploits the fact that there is only a lim-
ited number of corrupted servers, and so if a server collects
enough shares from distinct servers in a set, then this set
will contain a subset consisting of enough correct shares.
This subset is found by trial and error.
The protocol starts out like our ﬁrst optimistic protocol,
where no server computes a proof. Every server sends only
its share to all others. A server then receives   (cid:1) shares and
tries to assemble them to a valid signature. If it fails, it con-
tinues to receive signature shares and tries to assemble ev-
ery subset of   (cid:1) shares to a ﬁnal signature, until at most
(cid:2)  (cid:1) shares have been received. This is guaranteed to suc-
ceed because there are at most  invalid shares. However, the
algorithm may take exponential time in  when  is a frac-
tion of .
4. Implementation
We have implemented the pragmatic approach from Sec-
tion 3.4, relying on the SINTRA prototype for atomic
broadcast and RSA threshold signatures. Our implementa-
tion further includes the basic threshold signature protocol
from Section 3.3 and the two optimized versions described
in Section 3.5.
4.1. Structure
The prototype uses BIND, the Berkeley Internet Name
Daemon, which is the most widely used DNS server imple-
mentation. The servers run a modiﬁed version of named
from BIND snapshot 9.3.0s2002111, and the clients in our
system use the standard dig program (for read requests) and
the nsupdate program (for write requests). We used this
snapshot of the development version of BIND instead of
the most recent release (BIND 9.2.2) because this was the
only version available that had support for RSA signatures
with SHA1 message digest.
The SINTRA prototype is implemented in Java,
whereas BIND is written in C. We link them using a mod-
ule called Wrapper implemented in Java, which is the
main component of our replicated name service implemen-
tation.
Wrapper acts as a proxy between the clients and the
original named from BIND. Wrapper runs on every au-
thoritative name server of the zone and intercepts all re-
quests from clients on UDP port 53. It then interacts with
SINTRA for atomic broadcast and threshold signatures,
and with named for DNS operations. Wrapper reads its
conﬁguration parameters from a ﬁle. These parameters in-
clude the values of  and , the identities of all servers for
the zone, and the threshold signature protocol to use (c.f.,
Section 3). Then it starts the SINTRA runtime system and
opens an atomic broadcast channel among all members of
the group.
Internally, Wrapper runs several threads. These threads
include two dispatcher threads that listen for client re-
quests and for requests from named for computing thresh-
old signatures, respectively. Upon receiving such a request,
these threads dispatch the request to one of several worker
threads. The Wrapper also has a LinkReader and a Chan-
nelReader thread, which interact with the communication
primitives provided by SINTRA. LinkReader listens for
asynchronous point-to-point messages from Wrapper mod-
ules running on other servers, and ChannelReader listens
for messages that are sent through atomic broadcast.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:49:50 UTC from IEEE Xplore.  Restrictions apply. 
4.2. Operation
Every server runs Wrapper and a modiﬁed copy of
named, which is conﬁgured to listen on a different port
than 53.
When Wrapper receives a client request to the name
server on port 53, it broadcasts the request on the atomic
broadcast channel to all servers in the group. Any such re-
quest delivered by atomic broadcast and received by Chan-
nelReader is forwarded to named on the respective port.
When named has ﬁnished processing the request, it sends
the answer back to Wrapper, which returns it directly to
the client. All communication between named and Wrap-
per is done through datagram sockets such that named be-
lieves it is communicating with a client. Note that Wrap-
per does not have to interpret client requests since its oper-
ation is independent of the particular request; every request
is simply forwarded on the atomic broadcast channel to all
servers.
When named performs a dynamic update in a signed
zone data, it must compute new SIG records on the modi-
ﬁed data. The signature routine of named has been mod-
iﬁed so that it forwards the request on another datagram
socket to a dispatcher thread in the local Wrapper. The re-
quest is then processed using one of the threshold signature
protocols from Section 3. The output of this protocol is re-
turned by Wrapper to named, and named completes the
request as usual.
4.3. Initialization
SINTRA requires manual key distribution before it can
be invoked. In particular, there is a key generation utility
that must be run by a trusted entity and that outputs the pri-
vate key shares of every server for several threshold public-
key encryption and signature schemes used by SINTRA’s
protocols. The ﬁle with these private keys must be trans-
ported over a secure channel to every server (typically us-
ing SSH).
Creating a DNSSEC signed zone requires creating a
zone key and signing the zone data using this key. BIND
provides a utility for this. For ease of implementation, we
have so far not generated the threshold key shares from
BIND’s zone key or modiﬁed this utility to generate thresh-
old key shares, but use a threshold signature key generated
by the initialization utility of SINTRA. The correspond-
ing public key is then included in the zone data and the key
shares are included in the private data of every server, which
is distributed analogously to the initialization data for SIN-
TRA.
When the replicated name service ﬁrst begins operating,
we assume that all servers store the same zone ﬁle. A spe-
cial command may then be invoked on a single server to
sign the zone data using the distributed key.
4.4. Details
For the purpose of testing the signature generation pro-
tocol, we can also conﬁgure a server to misbehave and to
mimic a corrupted server. A server that is corrupted in this
way inverts all the bits in its signature share before send-
ing it to the others
We note that although the design of our system does
not impose any synchrony assumptions on the distributed
system model, the implementation is not entirely asyn-
chronous. Speciﬁcally, the communication in the current
SINTRA prototype implementation is based on TCP, which
involves timeouts. Furthermore, the client applications dig
and nsupdate use a timeout to decide how long to wait for
a server response and will contact the next server in the list
if the timeout expires (c.f., Section 3.4).
5. Benchmarks
5.1. Setup
Our experimental setup has two parts, one on a local-
area network and the other one on the Internet over large
distances. The local setup consists of a cluster of four ma-
chines at the IBM Zurich Research Laboratory connected
by a 100 Mbits/s switched Ethernet. Experiments on the lo-
cal setup provide a base case to determine how much of the
latency in the Internet setup is caused by the network and
how much by the protocol itself.
The Internet setup includes the cluster of four machines
in Zurich and three more machines: one at the IBM T.J. Wat-
son Research Center in New York, one at the IBM Austin
Research Laboratory and one at the IBM Almaden Research
Center in San Jose, connected by the IBM intranet. Table 1
shows the operating system, CPU type, clock speed, and the
Java VM version of each machine. The machines are stan-
dard x86-architecture laptops and desktops running Linux.
They represent the best choices we could obtain with rea-
sonable effort and hence the set is rather diverse; for exam-
ple, the machines in Austin and Almaden are considerably
faster than those in Zurich and New York.
Our Internet setup mimics a typical DNS installation
used by large international organizations, where a zone cor-
responding to a geographic site like Zurich is served from
a small local cluster of name servers, close to where most
of the queries arise, together with a few name servers at re-
mote locations that serve as backups. Figure 1 shows the
setup along with the average round-trip latencies measured
on each link.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:49:50 UTC from IEEE Xplore.  Restrictions apply. 
Location
Zurich
New York
Austin
San Jose
# of machines
OS
4
1
1
1
Linux 2.2.x
Linux 2.2.x
Linux 2.4.x
Linux 2.4.x
CPU
P II
P II
dual P III
P III
MHz
266
300
1260
930
Java
IBM 1.4.1
IBM 1.4.1
Sun 1.4.2
Sun 1.4.2
Table 1. Details of machines used in the experiments.
San Jose
83
44
New York
185
Austin
53
93
167
Zurich
Figure 1. The experimental setup on the Inter-
net with average round-trip times in millisec-
onds.
In the experiments that involve corrupted servers, we
conﬁgure one of the servers in the Zurich cluster to be cor-
rupted when there is only a single corruption; with two cor-
rupted servers, we conﬁgure the Zurich server and the server
at Austin to simulate corrupted behavior.
The prototype consists of the Java modules mentioned in
Section 4, the modiﬁed named from BIND, and the SIN-
TRA prototype written in Java. The RSA public-key signa-
tures are provided by SINTRA, where they are implemented
using the standard Java BigInteger package. Zone signa-
tures use 1024-bit RSA moduli with SHA-1 and PKCS #1
encoding.
5.2. Experiments
We measure the time taken by the replicated name ser-
vice to carry out a read, add, or delete request issued from
dig or nsupdate, and to send back a response to the client.
The requests are always sent by a client on the Zurich LAN
to one of the servers in the Zurich cluster. We repeat these
experiments for several values of  and  and with the three
different threshold signature protocol variations presented
in Section 3.
Table 2 shows the results (with times in seconds). Each
number in the table is the average of 20 measurements run
in sequence. The ﬁrst column gives the setup using a pair of
values (cid:2) (cid:3), where  is the number of servers,  (cid:2)  (cid:0)
corrupted servers can be tolerated, and (cid:3) servers are actu-
ally simulating corrupted behavior.