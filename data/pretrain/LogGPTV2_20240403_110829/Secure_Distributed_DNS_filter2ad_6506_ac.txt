### In Case of Invalid Signature
If the signature is determined to be invalid, the server initiates a request message to all other servers, asking them to recompute their signature shares along with the corresponding proofs. Upon receiving this request, each server generates a proof for its signature share and sends both the share and the proof to all other servers. These shares, now accompanied by proofs, are then processed in the same manner as in the unoptimized algorithm. Simultaneously, the server awaits the receipt of a valid final signature, terminating the process as soon as a correct signature is received.

#### Note on Parallel Waiting
The step of waiting in parallel for a valid signature is essential because it cannot be guaranteed that there will still be enough honest replicas available to resend their shares with proofs; they may have already terminated the protocol. However, if they have terminated, it implies that they have already sent out the correct signature, which will be received by the waiting server.

### Optimistic Signature Protocol with Trial and Error (OPTTE)
The OPTPROOF protocol performs well when all servers are honest, but its performance degrades in the presence of corrupted servers, making it less efficient than the unoptimized protocol. To address this, we propose an optimization called OPTTE, which performs better in the presence of corrupted servers, particularly for relatively small values of \( \ell \). For practical values of \( \ell \), this protocol is the fastest variation.

This protocol leverages the fact that there is only a limited number of corrupted servers. If a server collects enough shares from distinct servers, it can find a subset consisting of sufficient correct shares through a trial-and-error process.

#### Protocol Operation
1. **Initial Phase**: The protocol begins similarly to the first optimistic protocol, where no server computes a proof. Each server sends only its share to all others.
2. **Share Collection**: A server receives \( t \) shares and attempts to assemble them into a valid signature. If it fails, it continues to collect more shares.
3. **Subset Assembly**: The server tries to assemble every subset of \( t \) shares into a final signature until at most \( 2t \) shares have been received. This is guaranteed to succeed because there are at most \( \ell \) invalid shares.
4. **Time Complexity**: The algorithm may take exponential time in \( \ell \) when \( \ell \) is a significant fraction of \( n \).

### Implementation
We have implemented the pragmatic approach from Section 3.4, utilizing the SINTRA prototype for atomic broadcast and RSA threshold signatures. Our implementation includes the basic threshold signature protocol from Section 3.5 and the two optimized versions described in Section 3.5.

#### Structure
- **BIND Integration**: The prototype uses BIND, the Berkeley Internet Name Daemon, which is the most widely used DNS server implementation. We use a modified version of `named` from BIND snapshot 9.3.0s2002111.
- **Wrapper Module**: The Wrapper module, implemented in Java, acts as a proxy between the clients and the original `named` from BIND. It intercepts all requests on UDP port 53 and interacts with SINTRA for atomic broadcast and threshold signatures, and with `named` for DNS operations.
- **Thread Management**: Wrapper runs several threads, including dispatcher threads for client requests and threshold signature computations, and reader threads for asynchronous point-to-point messages and atomic broadcast.

#### Operation
- **Client Requests**: When Wrapper receives a client request, it broadcasts the request on the atomic broadcast channel to all servers. The request is then forwarded to `named`, and the response is returned to the client.
- **Dynamic Updates**: For dynamic updates in signed zone data, `named` forwards the request to a local Wrapper thread, which processes it using one of the threshold signature protocols and returns the result to `named`.

#### Initialization
- **Key Distribution**: SINTRA requires manual key distribution. A trusted entity must run a key generation utility to output private key shares for each server, which are then securely distributed.
- **DNSSEC Zone Creation**: A DNSSEC signed zone is created using a zone key and signed zone data. We use a threshold signature key generated by SINTRA's initialization utility.

#### Testing and Misbehavior
- **Corrupted Servers**: For testing, a server can be configured to misbehave by inverting all bits in its signature share.
- **Synchrony Assumptions**: Although the system design does not impose synchrony assumptions, the current implementation uses TCP, which involves timeouts.

### Benchmarks
#### Experimental Setup
- **Local Setup**: A cluster of four machines at the IBM Zurich Research Laboratory connected by a 100 Mbits/s switched Ethernet.
- **Internet Setup**: Includes the Zurich cluster and three additional machines at IBM research centers in New York, Austin, and San Jose, connected by the IBM intranet.

#### Experiments
- **Measurements**: We measure the time taken by the replicated name service to handle read, add, or delete requests issued from `dig` or `nsupdate`.
- **Results**: Table 2 shows the average times for various values of \( n \) and \( \ell \) and the three different threshold signature protocol variations.

```plaintext
Table 2: Benchmark Results (times in seconds)
| n, f, c | Unoptimized | OPTPROOF | OPTTE |
|---------|-------------|----------|-------|
| ...     | ...         | ...      | ...   |
```

Each number in the table is the average of 20 sequential measurements. The first column specifies the setup with \( n \) being the number of servers, \( f \) the number of tolerated corrupted servers, and \( c \) the actual number of corrupted servers.