### BFT-SMaRt Ordering Service Architecture

**Fig. 4: BFT-SMaRt ordering service architecture.**

1. **Batching:**
   - The ordering nodes, built on top of the BFT-SMaRt service replica, receive a stream of totally ordered envelopes.
   - Each node maintains a `blockcutter` object, which stores the received envelopes before they are assembled into a block.
   - The `blockcutter` manages envelopes for each Fabric channel and creates batches of envelopes to be included in a block for the ledger associated with that channel.
   - This custom batching mechanism is implemented instead of using BFT-SMaRt's native batching due to:
     - The need to keep envelopes from different channels separate.
     - Support for configuration envelopes that must remain isolated from regular envelopes.
     - Differences in batching policies (Fabric limits by size in bytes, while BFT-SMaRt limits by the number of requests).

2. **Parallelization:**
   - After the `blockcutter` is drained, a sequence number is assigned to the future block.
   - The block header, containing the sequence number and cryptographic hashes from the previous header and the blockâ€™s envelopes, is submitted to the signing/sending thread pool.
   - This thread pool ensures deterministic behavior across nodes because:
     - Block headers and envelopes are generated sequentially within the node thread.
     - Each node only needs to maintain the block header from the previous iteration.
   - The Fabric Java SDK is used to handle and create data structures, generate cryptographic hashes, and ECDSA signatures.

3. **Durability and Node Membership:**
   - The BFT-SMaRt replica provides additional capabilities such as durability (in case all ordering nodes fail) and reconfiguration of the group of ordering nodes.
   - The state includes headers for the last block associated with each channel, information about the current configuration of channels, and envelopes stored in the `blockcutter`.
   - The state remains bounded and smaller than the ledger maintained by Fabric peers due to the constant size of headers and periodic draining of envelopes.

4. **Validation and Reconfiguration:**
   - Transactions can be validated by the signing/sending threads before generating block signatures.
   - Special transactions for channel reconfiguration need to be validated and executed before being submitted to the `blockcutter`.

### BFT-SMaRt Message Pattern

**Fig. 3: BFT-SMART message pattern.**

- Frontends collect 2f + 1 matching blocks from ordering nodes, ensuring at least f + 1 valid signatures to peers and clients.
- Frontends are part of the peer trust domain and are responsible for:
  - Relaying envelopes to the ordering cluster on behalf of the client.
  - Receiving blocks from the ordering cluster and relaying them to the peers maintaining the distributed ledger.

### Evaluation

#### A. Parameters Affecting the Ordering Performance

The throughput of the ordering service (TP_os) is bounded by:
- The rate at which envelopes are ordered by BFT-SMaRt (TP_bftsmart).
- The number of blocks signed per second (TP_sign).
- The size of the generated blocks.

Given an envelope size (es), block sizes (bs), and a number of receivers (r):
\[ \text{TP}_{\text{os}} \leq \min(\text{TP}_{\text{sign}} \times \text{bs}, \text{TP}_{\text{bftsmart}}) \]

#### B. Signature Generation

- A benchmark program was run on a Dell PowerEdge R410 server to estimate TP_sign.
- The server can generate up to 8.4k signatures/sec with 16 threads.
- The effect of block size is negligible as ECDSA signatures are computed over the hash of the block.
- With blocks containing 100 envelopes, the server can sign up to 840k envelopes/sec.

#### C. Ordering Cluster in a LAN

- Experiments were conducted with clusters of 4, 7, and 10 nodes, withstanding 1, 2, and 3 Byzantine faults, respectively.
- Blocks were configured to contain either 10 or 100 envelopes.
- Envelope sizes tested: SHA-256 hash (40 bytes), three ECDSA endorsement signatures (200 bytes), and transaction messages of 1 and 4 kbytes.
- Throughput measurements were gathered at the leader replica (node 0).
- Experiments were repeated 3 times, each lasting 5 minutes.

**Fig. 6: Local-area results.**
- Throughput drops with an increase in the number of receivers, but the impact is smaller for larger transactions (1k and 4 kbytes).
- For these envelope sizes, the overhead of the replication protocol is greater than the overhead of transmitting blocks of 10 and 40 kbytes.

This optimized text provides a clear, coherent, and professional description of the BFT-SMaRt ordering service architecture, its components, and the evaluation results.