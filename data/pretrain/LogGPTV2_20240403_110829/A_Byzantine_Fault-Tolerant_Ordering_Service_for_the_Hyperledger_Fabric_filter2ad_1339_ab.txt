
#$

  !
 


"
& 
Fig. 4: BFT-SMaRt ordering service architecture.
pre-determined number of envelopes for a channel (the block
size), it notiÔ¨Åes the node thread that it is time to drain its
envelopes and create the next block.
c) Parallelization: After the blockcutter is drained, a
sequence number is assigned to the future block and submitted
to the signing/sending thread pool alongside with the respec-
tive block header. This header contains the aforementioned
sequence number and the cryptographic hashes from the
previous header and the hash for the block‚Äôs envelopes. Notice
that this thread pool does not cause non-determinism across
the nodes because (1) the block header and envelopes to be
assigned to new blocks are generated sequentially within the
node thread, and (2) the only structures that each node needs
to maintain as part of the application state is the block header
from the previous iteration of the node thread. Similarly to
the frontend, the Fabric Java SDK is used to correctly handle
and create the data structures used by the system. In addition,
this SDK is also used to generate cryptographic hashes and
ECDSA (Elliptic Curve DSA) signatures [19] that can be
validated by other components of Fabric. Once the block is
created and signed, it is transmitted to all active frontends. This
is done through a custom replier (supported by the extensible
API of BFT-SMART) that, instead of sending the operation
result (i.e., the generated block) to the invoking client, sends it
to a set of registered BFT-SMART clients (i.e., the frontends).
d) Durability and Node Membership: Besides the trans-
action ordering and execution, the BFT-SMART replica also
provides additional capabilities that are fundamental for prac-
tical state machine replication, such as durability (of state,
in case all ordering nodes fail) and reconÔ¨Åguration of the
group of ordering nodes. The state is comprised by the headers
for the last block associated to each channel, information
about the current conÔ¨Åguration of channels, and the envelopes
currently stored at the blockcutter. Since the headers have a
constant size and the envelopes are periodically drained from
the blockcutter, the state maintained at the ordering nodes will
always be bounded and remain smaller than the size of the
ledger maintained by Fabric peers.
e) Validation and ReconÔ¨Åguration: One last aspect of
this service relates to channel reconÔ¨Åguration and transaction
validation. Fabric‚Äôs architecture is resilient to blocks contained
junk transactions, hence ordering services can avoid perform-
ing transaction validation. In the particular case of our ordering
service, transactions can be validated by the signing/sending
threads prior to generating block signatures. Transactions can








Fig. 3: BFT-SMART message pattern.
known frontends, which collect 2f + 1 matching blocks from
ordering nodes. The 2f + 1 blocks are necessary because
frontends do not verify signatures. However,
this number
guarantees a minimum of f + 1 valid signatures to peers
and clients.4 Frontends are part of the peer trust domain and
are responsible for (1) relaying the envelope to the ordering
cluster on behalf of the client, and (2) receiving the blocks
generated by the ordering cluster and relaying them to the
peers responsible for maintaining the distributed ledger.
a) Architecture: BFT-SMaRt‚Äôs ordering service architec-
ture is illustrated in Figure 4. The frontend is composed by
the Fabric codebase and a BFT shim. The Fabric codebase
(implemented in Go) provides an interface for Fabric clients
to submit envelopes. These envelopes are relayed to the BFT
shim using UNIX sockets. This shim is implemented in Java
and maintains (1) a client thread pool that receive envelopes
and relays them to the ordering cluster, and (2) a receiver
thread that collects blocks from the cluster. Envelopes (resp.
blocks) are sent to (resp. received from) the cluster through
the BFT-SMaRt proxy. The proxy does that by issuing an
asynchronous invocation request to the BFT-SMART client-
side library, ensuring it does not block waiting for replies.
To ensure that the shim performs computations on equivalent
data structures to the Fabric codebase, the ordering service
uses the Hyperledger Fabric Java SDK to parse and assemble
data structures used in Fabric.
b) Batching: The ordering nodes are implemented on top
of the BFT-SMaRt service replica, thus receiving a stream
of totally ordered envelopes. Each node maintains an object
named blockcutter, where the envelopes received from the
service replica are stored before being assembled into a block.
The blockcutter is responsible for managing the envelopes
associated to each Fabric channel and creating a batch of
envelopes to be included in a block for the ledger associated
to that channel. We implement this batching mechanism in-
stead of relying on BFT-SMART‚Äôs native batching because
(1) each BFT-SMART‚Äôs batch may contain envelopes that
are not associated to the same channel, which means the
envelopes cannot be all assembled into the same block; (2)
Fabric supports conÔ¨Åguration envelopes, which are supposed
to remain isolated from regular envelopes; and (3) Fabric‚Äôs
native batching policies are not equivalent to BFT-SMART‚Äôs
(for instance, Fabric imposes a batching limit based on its
size in terms of bytes, whereas BFT-SMART limit is based
on number of requests per batch). Once the blockcutter holds a
4If the frontends are programmed to perform signature veriÔ¨Åcation, only
f + 1 matching blocks sufÔ¨Åce.
54
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:14 UTC from IEEE Xplore.  Restrictions apply. 
then be removed from the block if the validation fails. The
exception to this is a special category of transactions that are
used to perform channel reconÔ¨Åguration. These transactions
need to be validated and executed prior to submitting them to
a blockcutter.
VI. EVALUATION
In this section we describe the experiments conducted to
evaluate BFT-SMART‚Äôs ordering service and discuss the ob-
served results. Our aim here is not to evaluate the whole Fabric
system, but only the ordering service, which may typically be
the bottleneck of the system.
A. Parameters affecting the Ordering Performance
The throughput of the ordering service (i.e., the rate at
which envelopes are added to the blockchain TP os)
is
bounded by one of three factors: a) the rate at which en-
velopes are ordered by BFT-SMART (TP bftsmart) for a given
envelope size, number of envelopes per block and number of
receivers; b) the number of blocks signed per second (TP sign);
or c) the size of the generated blocks. These parameters are
illustrated in Figure 5.
Given an envelope size es, block sizes bs, and a number
of receivers r (i.e., the peer frontends to which the ordering
nodes transmit the generated blocks), the peak throughput of
the ordering service is bounded as follows:
TP bs,es,r
os
‚â§ min(TP sign √ó bs, TP bs,es,r
bftsmart
)
(1)
An important remark is that this equation considers that a
block is signed only once by each ordering node, however,
in Fabric 1.0 a block need to be signed twice. The second
signature is needed to attach the block transaction to an
execution context (details are out of the scope of this paper).
If this is the case for the considered application, the T Psign
term used in the equation must be replaced by T Psign
.
2
B. Signature Generation
In order to estimate TP sign, we run a very simple signature
benchmark program written in Java in a Dell PowerEdge
R410 server, which possesses two quad-core 2.27 GHz Intel
Xeon E5520 processor with hyper-threading (thus having 16
hardware threads) and 32 GB of memory. The server runs
Ubuntu 14.04 with JVM 1.8.0. Our program spawns a number
of threads to create ECDSA signatures for blocks of Ô¨Åxed size
and calculates how many of such signatures are generated per
second.
Our results show that our server can generate up to 8.4k
signatures/sec, when running with 16 threads. Furthermore,
the effect of the block size is mostly negligible as the ECDSA
signature is computed over the hash of the block. These results,
together with the fact that a blocks are expected to contain
10+ envelopes in Fabric, lead us to conclude that signature
generation is not expected to be a bottleneck in our setup.5
5For example, by using blocks with bs = 100 envelopes, we can sign up
to TP sign √ó bs = 840k envelopes/sec.
55
 
 !

 !




 !
	



 !
Fig. 5: Ordering service performance model.
C. Ordering Cluster in a LAN
The experiments aims to evaluate the BFT-SMART or-
dering service by using clients that emulate the behavior of
multiple ordering service frontends. They were executed with
clusters of 4, 7, and 10 nodes, withstanding 1, 2, and 3
Byzantine faults, respectively. Furthermore, we also Ô¨Åddled
with the block size, by conÔ¨Åguring each cluster conÔ¨Åguration
to assemble blocks containing either 10 or 100 envelopes
(i.e., transactions). This is meant to observe the behaviour of
each cluster when throughput is bound by either the rate of
signature generation or by the rate of envelope reception. The
environment is comprised by Dell PowerEdge R410 servers,
like the one described before, connected through a Gigabit
ethernet.
For each micro-benchmark conÔ¨Ågured to have x nodes and
y envelopes/block, we gathered results for (1) envelopes with
different sizes, and (2) a variable number of receivers. More
precisely, each envelope size is representative of submitting
to the ordering cluster: (1) a SHA-256 hash (40 bytes); (2)
three ECDSA endorsement signatures (200 bytes); and (3)
transaction messages of 1 and 4 kbytes. In practice, and
considering the way Fabric 1.0 operates, the values related
with (3) are more representative of the size of a transaction.
In particular, our limited experience shows that transactions
compressed with gzip tend to be usually close to 1 kbyte.
Nonetheless, measurements for (1) and (2) are important to
show the potential of the ordering service if different design
choices were taken in future versions of Fabric.
Measurements for the throughput associated to block gen-
eration were gathered at ordering node 0 (the leader replica
of BFT-SMART‚Äôs replication protocol). To reach the system‚Äôs
peak throughput, each execution was performed using 16 to
32 clients distributed across 2 additional machines. We also
repeated the micro-benchmark with 4 nodes with blocks of
100 envelopes. All experiments used 16 signing threads (to
match the number of available cores) and were repeated 3
times taking 5 minutes each.
The obtained results for local-area are presented in Figure 6.
Even though throughput drops when increasing the number of
receivers, the impact of the number of receivers is considerably
smaller for larger transactions (1k and 4 kbytes). This is be-
cause for these envelope sizes, the overhead of the replication
protocol is greater than the overhead of transmitting blocks
of 10 and 40 kbytes. In particular, since the batch limit of
the BFT-SMART is set to 400 requests (default value), the
PROPOSE message of the underlying replication protocol can
have up to 0.4-1.6MBs with these envelope sizes.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:27:14 UTC from IEEE Xplore.  Restrictions apply. 
)
c
e
s
/
s
n
a
r
t
k
(
t
u
p
h
g
u
o
r
h
T
)
c
e
s
/
s
n
a
r
t
k
(
t
u
p
h
g
u
o
r
h
T
 60
 50
 40
 30
 20
 10
 0
40 bytes
200 bytes
1 kbytes
4 kbytes
)
c
e
s
/
s
n
a
r
t
k
(
t
u
p
h
g
u
o
r
h
T
 60
 50
 40
 30
 20
 10
 0
40 bytes
200 bytes
1 kbytes
4 kbytes
)
c
e
s
/
s
n
a
r
t
k
(
t
u
p
h
g
u
o
r
h
T
 60
 50
 40
 30