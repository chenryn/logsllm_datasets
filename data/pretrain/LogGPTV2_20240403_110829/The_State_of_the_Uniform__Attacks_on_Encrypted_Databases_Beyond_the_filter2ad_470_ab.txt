The formulation of KPT appears as a set of constraints in
this new algorithm. Due to the minimum distortion insight,
our proposed AGNOSTIC-RECONSTRUCTION-KNN always
outputs a reconstruction as opposed to the all-or-nothing
approach of KPT. Furthermore, since we don’t explicitly build
the set of all possible solutions, our approach scales to larger k
compared with KPT. An illustration of a reconstruction for a
real-world dataset of privacy-sensitive geolocation is shown in
Figure 1. This reconstruction is achieved with half the queries
compared to KPT, under a Gaussian query distribution, and
with one-dimensional relative error of 0.08%.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:16 UTC from IEEE Xplore.  Restrictions apply. 
1225
II. BACKGROUND
A database is a collection DB of n records (idi, val(idi)),
i = 0, . . . , n − 1 where idi is a unique identiﬁer and val(idi)
is a value from the universe [α, β]. We assume discrete values
so that α, β, and val(idi) are integers and denote with N =
β − α + 1 the size of the universe. For the sake of simplicity
of the analysis, we assume that the mapping from records to
values is injective, that is, there is a single record in the database
associated with a value. We note though that our attacks can
be extended to the case of non-injective mapping from records
to values in which case the distance is 0 when consecutive
records correspond to the same value. We call density of the
database the percentage of values from the universe that are
assigned to records. E.g., the density assumption studied by
Lacharit´e et al. [34] corresponds to density 100%. A range
query consists of two values a ≤ b from the universe and its
response is the set of identiﬁers of the database records with
values within interval [a, b]. A k-NN query consists of a value
from the universe and its response is a set of k unordered
identiﬁers that are closest to the query point, where k is ﬁxed
and decided at setup-time. We use the term query to refer to
the plaintext query parameter(s) and the term search token to
refer to the encrypted query parameter(s) that the client sends
to the server. We deﬁne access-pattern leakage as the set of
encrypted records that are retrieved as part of the response
to a token. We deﬁne search-pattern leakage as the ability
of the server to observe whether two tokens were generated
from the same plaintext query. Although there are response-
hiding STE schemes that minimize the access-pattern leakage
by imposing a storage overhead, the widely-used constructions
actually reveal the access-pattern for the sake of efﬁciency. To
the best of our knowledge, all structured encryption schemes
leak the search-pattern [23].
Assumptions. Our techniques have no knowledge about the
query distribution, data distribution, or access to any auxiliary
information about them. Our assumptions are as follows:
• Static Database. No updates, i.e., addition, deletions, take
place once the database is encrypted.
• Fixed Query Distribution. We assume that the adversary
issues independent and identically distributed (i.i.d.) queries
with respect to a ﬁxed query distribution. We emphasize that
our adversary does not know any information about the family
or the parameters of the query distribution.
• Correctness. We consider schemes where the response
to the issued query is correct. We do not consider schemes
that return missing responses or false positive responses, e.g.,
Logarithmic-SRC [17] and “over-covers” from [22].
• One-dimensional Data Values. We do not address en-
crypted databases for high-dimensional data [14].
• Known Setup. We assume that the adversary knows the
number of encrypted values n, the size of the universe of
values N and the endpoints of the universe α, β.
• Injective Mapping of Search Tokens. We assume that
distinct queries, can be either a pair of values like the range
queries or a single value like the k-NN, map to distinct search
tokens. The injective mapping is satisﬁed, to the best of our
knowledge, by all known STE encryption schemes.
Order Reconstruction. There is a plethora of
tech-
niques [27], [33], [34] in the literature that reconstructs the
order of the encrypted values using only the access-pattern
leakage. For simplicity of the exposition, we assume that the
adversary can successfully reconstruct the order by using the
appropriate algorithms from the above works and we instead
focus on the problem of reconstructing the plaintext values.
Thus, we treat the ordering as an input to our new value
reconstruction algorithms and our techniques are not affected
by how this ordering was constructed.
III. HOW TO EXPLOIT SEARCH-PATTERN LEAKAGE
In this section, we introduce our main tool to reconstruct
the plaintext values of an encrypted database without any
knowledge about the data or query distribution. Given a ﬁxed
query distribution, the repetition of search tokens, i.e., search-
pattern leakage, reveals information about the total number of
search tokens that return a speciﬁc encrypted response. This
key observation relates our attack to the extensively studied
problem of estimating the support size of a distribution.
We ﬁrst show how to partition token-response pairs and
interpret them as samples from the unknown query distribution.
Next, we benchmark two widely-used non-parametric estima-
tors under various query distributions. Finally, we propose
a new modular estimator for our attack. Since we obtain a
different estimator per encrypted response, the next section
shows how to glue the acquired estimations together to
reconstruct the encrypted database in its entirety.
A. Conditional Probability Distributions over the Leakage
In this subsection, we show how an adversary that
is
given a multiset of m token-response pairs D = {(t1, r1),
. . . , (tm, rm)}, can partition the tokens and analyze each group
as a sample from a conditional probability distribution. By
conditioning on the information observed from the access-
pattern leakage, we group the information observed by the
search-pattern leakage.
Remark 1. Let D = {(t1, r1), . . . , (tm, rm)} be the multiset of
tokens and their corresponding response under an arbitrary to-
ken distribution. The mutliset of tokens with the same associated
response, i.e., Di := {tj|(tj, ri) ∈ D, ri ⊆ {id0, . . . , idn−1}},
is a sample from the conditional probability distribution
pT|R (T = t|R = ri).
Fig. 3. To observe response r = {id1, id2, id3} the start of the query range
must be in-between v0 and v1 and the end must be in-between v3 and v4.
Thus, the total number of queries that return r is (v1 − v0) · (v4 − v3).
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:16 UTC from IEEE Xplore.  Restrictions apply. 
1226
Range Queries. We recall again our assumption that the
mapping from range queries to tokens is injective. However,
we note that our attack can be applied also to structured
encryption schemes that generate multiple tokens per query
with no false positives. In this scenario the attacker creates
a canonical ordering of the collection of tokens, e.g., by
lexicographical-ordering, and treats their concatenation as a
single token. Schemes with this property include the BRC
and URC token generation presented in [17], as well as the
cover selection approach presented in [22]. The partition of
the token-response pairs is performed with respect to a speciﬁc
response. Consider a database with values {v0,··· , vn−1}
from a universe [α, β]. Since we do not consider schemes
with false positives, the number of distinct tokens that return
a given response r = {idi,··· , idj} is equal the product
(vi − vi−1) · (vj+1 − vj), where v−1 and vn refer to α and β,
respectively. An example is depicted in Figure 3.
Remark 2. For the case of range queries on an encrypted
database the support size of
the conditional distribution
pT|R (T = t|R = {idi, . . . , idj}), where 0 ≤ i ≤ j ≤ n − 1,
is the product of (1) the distance between values vi−1 and
vi and (2) the distance between values vj and vj+1, i.e.,
(vi − vi−1) · (vj+1 − vj).
k-NN Queries. A Voronoi diagram gives a natural partition
of the query space for k-NN queries. Speciﬁcally each segment
of the partition has the property that all the queries that land
inside the segment have the same k nearest neighbors, i.e., the
same response. It is known [33] that given a Voronoi diagram,
the endpoints of each Voronoi segment correspond to bisectors
between the values.
arbitrary token distribution. Let R be a random variable whose
possible values are the k-NN responses with respect to DB.
Remark 3. For the case of k-NN queries on an encrypted
database, the support size of the conditional distribution
pT|R (T = t|R = {idi, . . . , idi+k−1}) is also the length of the
corresponding Voronoi segment, i.e., bi,i+k − bi−1,i+k−1.
B. Estimate Support Size of Each Distribution
In this subsection, we show how to utilize the frequency
of the observed search tokens so as to estimate the total
number of search tokens that return a speciﬁc response r,
i.e., estimate the support size of a conditional probability
distribution with respect to r. In our approach, each response
has a different non-parametric estimator that is “ﬁne-tuned” for
the speciﬁc conditional probability distribution. We focus on a
single response but in the next section, we describe how an
adversary can combine the estimations for different responses
to achieve approximate reconstruction of the entire encrypted
database. Furthermore, the estimation techniques described here
are applied to both range and k-NN queries. To comply with
the notation in the literature [44] on support size estimators,
in this subsection N denotes the support size of a single query
distribution, whereas in the rest of the paper N denotes the
size of the universe of values, i.e., N = β − α + 1.
Formulation. We assume a conditional probability distribu-
tion pT|R with respect to response r that contains N distinct
search tokens observed with probabilities πi = (π0, . . . , πN−1).
The adversary does not know the support size N or probabili-
ties πi. The main question we address is:
Given a sample D of m search tokens (with mul-
tiplicities) from pT|R, what is the total number of
search tokens in pT|R with non-zero probability?
Fig. 4. Voronoi diagram of a database with 6 values v0, . . . , v5 and 2-NN
queries. Short vertical black lines indicate distinct queries and tall vertical
green lines indicate bisectors bi,i+2 for values vi and vi+2.
Figure 4 shows the Voronoi diagram for 2-NN queries on
a database DB with values v0, . . . , v5 from range [α, β]. The
bisectors of the diagram, bi,i+2, partition the query points into
intervals where queries yield the same response. E.g., all query
points between bisectors b1,3 and b2,4 yield response {v2, v3}.
In our scenario of an encrypted database, the response is a pair
of identiﬁers. Accordingly, we deﬁne the following partition
of query tokens for k-NN queries: a search token t belongs to
group Di if its response is {idi, . . . , idi+k−1}, for i ∈ [0, n−k].
We recall here our assumption of an injective mapping from
queries to tokens, i.e., we never map two distinct queries to the
same token. Therefore, the probability distribution on k-NN
queries transfers to the probability distribution on tokens.
Let T be a random variable whose possible values are the
tokens for k-NN queries generated by the client under an
(cid:2)m
(cid:2)m
Let fi be the number of search tokens that are observed i
times in the sample. We brieﬂy recall the terminology from [44].
The ﬁngerprint of sample D is the vector F = (f1, f2, . . . , fm),
where |D| = m. Vector F is essentially the frequency of the
frequencies. Then we can express the total number of all
distinct search tokens as N = f0 +
fi and the number of
i=1
observed search tokens as d =
fi. Similarly to [44], we
call the histogram of the query distribution Q over the elements
of pT|R the mapping hQ : (0, 1] → [0, N ], where hQ(π) is
the number of pT|R elements that occur in probability mass
function Q with probability π. Notice that the ﬁngerprint is
deﬁned according to a sample while the histogram is deﬁned
according to the query distribution.
i=1
One Experiment Captures Multiple Distributions. We
call a distribution property symmetric, or label-invariant, if it
only depends on the histogram of the distribution. A symmetric
property does not depend on which outcome maps to which
probability. The next remark follows from Lemma 17 in [3].
Remark 4. The support size of pT|R is a symmetric property.
Jumping ahead, this important property comes into play
in our evaluation. When we ﬁx the query distribution in
our experiments, we implicitly ﬁx the conditional probability
distributions too. The symmetric property implies that from the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 22,2021 at 04:12:16 UTC from IEEE Xplore.  Restrictions apply. 
1227
Fig. 5. An illustration of three query distributions with the same histogram.
The result of a support size estimation is the same in all three cases.
point of view of the estimator, it makes no difference which
token maps to which ﬁxed probability value. Thus, the result
of an experiment would be the same for every assignment of
the chosen ﬁxed probability values to tokens. As an example,
the three probability mass functions presented in Figure 5
have the same set of probabilities but different labelings. Since
the ﬁngerprint is the same, the support size estimation on the
ordered “towers” on the left gives the same estimation as the
pmf in the middle or the bell-shaped pmf to the right.
Related Work. The problem of estimating the support size
of a distribution has appeared in several ﬁelds in different
forms. Examples include the estimations of the number of
English words Shakespeare knew [21], the number of species
in a population of plants or animals [7], and how many dies
were used on an ancient coin [42]. As reviewing this large body
of work is beyond the scope of this paper, we refer the reader
to the following surveys [5], [12], [24]. We note that naive
application of the estimators for the equiprobable case [29],
[36] to settings with varying probabilities has been shown to
give an estimation with negative bias [36].
In our work, instead of deploying parametric estimators that
assume an underlying family of distributions, we use a more
general non-parametric approach that is distribution agnostic.
The Jackknife Method. Resampling techniques are non-
parametric methods of statistical inference that draw repeated
subsamples from the original sample D. In this work we
are interested in the jackknife method originally proposed by
Quenouille in [40]. In certain scenarios it is not known how to
compute an efﬁcient unbiased estimator of a statistic of interest
generally denoted as θ. Therefore given a biased estimator ˆθ
for a statistic the jackknife approach estimates the bias via
sampling with replacement from D. An estimate of the bias
(cid:3)biasJack can be used to correct the estimator as follows:
ˆθJack = ˆθ −(cid:3)biasJack.
(cid:2)m
The resampling approach of the jackknife is the following:
to form a new sample we leave one observation out so as to
create the subsample D(i) = (d1, . . . , di−1, di+1, . . . , dm). We
denote as ˆθ(i) the estimation of θ that is computed based on
D(i). The term ˆθ(.) denotes the average of all possible leave-
ˆθ(i)/m. The jackknife
one-out estimations, i.e., ˆθ(.) =
bias is deﬁned as:
ˆθ(i) − ˆθ).
The multiplicative term (m − 1) in the above expression is
rather counter-intuitive at ﬁrst sight. One way to interpret this
term is to assume that for a ﬁxed m the expected value of
the estimator ˆθ is the estimand plus a bias term of the form
(cid:3)biasJack = (m − 1)(ˆθ(.) − ˆθ) = (m − 1)(
m(cid:4)
1
m
i=1
i=1
(cid:5)
(cid:6)
m(cid:4)
E[ˆθ(i)]
i=1
b1(θ)
m
= bias
bias = b1(θ)/m. In this case we get:
E[(cid:3)biasJack] = (m − 1)
(cid:7)
(cid:8)
E[ˆθ] − 1
m
= (m − 1)
θ +
b1(θ)
m
− θ − b1(θ)
m − 1
=
(cid:9)NJ(i) =
α(i)
k fk,
(cid:2)m
(cid:9)NJ(2) = d + 2m−3
k=1
Therefore the expectation of the bias estimate is the true formula
of the bias. The above exposition concerns the ﬁrst order
jackknife estimator since it corrects biases of the order O(1/m).
This approach can be generalized to formulate the k-th order
jackknife estimator that results in a bias of the order O(m−k−1).
There is an inherit trade-off between the bias and variance,
the higher the order of the jackknife estimator the smaller the
bias and the larger the variance. Our estimators come directly
from the work of Burnham and Overton [6], [7] and where
originally proposed for estimating animal populations. The
statistic that we are interested in is the total number of distinct
classes N. The initial biased estimator (cid:9)N is the number of
(cid:2)m
distinct classes observed in sample D, i.e., (cid:9)N = d =
of the originally biased estimator (cid:9)N. The order of the jackknife
fi.
The following expressions present the “bias-corrected” formula
i=1
describes the level of bias correction applied. For a ﬁxed sample
size m the jackknife estimator of order i is a simple linear
combination of the ﬁngerprint F = (f1, . . . , fm). That is the
i-th order jackknife estimator can be expressed as:
k
f2,
(m−1)m
where α(i)
(m−2)(m−1)m f3.
m f1 − (m−2)2
m(m−1)
(m−3)3
f2 +
m f1,
m f1 − (3m2−15m+19)
(1)
coefﬁcients are a function of the sample size m.
The jackknife estimators for (cid:9)NJ(1), (cid:9)NJ(2), and (cid:9)NJ(3) are:
(cid:9)NJ(1) = d + m−1
(cid:9)NJ(3) = d + 3m−6
The derivation of the jackknife estimators (cid:9)NJ(i) for i ∈ [4, 10]
the analytical expression of jackknife estimators (cid:9)NJ(i), for
appear in the Appendix, these analytical expressions may be
of independent interest since they have not appeared before.
Selection of the Jackknife Order. Since we have we have
i ∈ [0, 10] an interesting question is how can we choose the
appropriate order i given what we observed so far? To tailor
the order of the jackknife estimator given the data in hand
we deploy the order-selection technique originally proposed
in [7] based on hypothesis testing. At a high-level this method
H(cid:3)
tests the null hypothesis Hi : E[(cid:9)NJ(i+1) − (cid:9)NJ(i)] = 0 against
i : E[(cid:9)NJ(i+1) − (cid:9)NJ(i)] (cid:6)= 0 sequentially for i ≤ 10 and