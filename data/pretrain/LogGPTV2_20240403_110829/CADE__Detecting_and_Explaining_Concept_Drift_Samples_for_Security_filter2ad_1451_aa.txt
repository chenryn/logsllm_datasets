title:CADE: Detecting and Explaining Concept Drift Samples for Security
Applications
author:Limin Yang and
Wenbo Guo and
Qingying Hao and
Arridhana Ciptadi and
Ali Ahmadzadeh and
Xinyu Xing and
Gang Wang
CADE: Detecting and Explaining Concept Drift 
Samples for Security Applications
Limin Yang, University of Illinois at Urbana-Champaign; Wenbo Guo, 
The Pennsylvania State University; Qingying Hao, University of Illinois at 
Urbana-Champaign; Arridhana Ciptadi and Ali Ahmadzadeh, Blue Hexagon; 
Xinyu Xing, The Pennsylvania State University; Gang Wang, University of 
Illinois at Urbana-Champaign
https://www.usenix.org/conference/usenixsecurity21/presentation/yang-limin
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.CADE: Detecting and Explaining Concept Drift Samples
for Security Applications
Limin Yang*, Wenbo Guo†, Qingying Hao*, Arridhana Ciptadi‡
Ali Ahmadzadeh‡, Xinyu Xing†, Gang Wang*
*University of Illinois at Urbana-Champaign †The Pennsylvania State University ‡Blue Hexagon
PI:EMAIL, PI:EMAIL, PI:EMAIL, {arri, ali}@bluehexagon.ai, PI:EMAIL, PI:EMAIL
Abstract
Concept drift poses a critical challenge to deploy machine
learning models to solve practical security problems. Due
to the dynamic behavior changes of attackers (and/or the
benign counterparts), the testing data distribution is often
shifting from the original training data over time, causing
major failures to the deployed model.
To combat concept drift, we present a novel system CADE
aiming to 1) detect drifting samples that deviate from existing
classes, and 2) provide explanations to reason the detected
drift. Unlike traditional approaches (that require a large num-
ber of new labels to determine concept drift statistically), we
aim to identify individual drifting samples as they arrive. Rec-
ognizing the challenges introduced by the high-dimensional
outlier space, we propose to map the data samples into a
low-dimensional space and automatically learn a distance
function to measure the dissimilarity between samples. Using
contrastive learning, we can take full advantage of existing
labels in the training dataset to learn how to compare and
contrast pairs of samples. To reason the meaning of the de-
tected drift, we develop a distance-based explanation method.
We show that explaining “distance” is much more effective
than traditional methods that focus on explaining a “decision
boundary” in this problem context. We evaluate CADE with
two case studies: Android malware classiﬁcation and network
intrusion detection. We further work with a security com-
pany to test CADE on its malware database. Our results show
that CADE can effectively detect drifting samples and provide
semantically meaningful explanations.
1 Introduction
Deploying machine learning based security applications can
be very challenging due to concept drift. Whether it is mal-
ware classiﬁcation, intrusion detection, or online abuse detec-
tion [6, 12, 17, 42, 48], learning-based models work under a
“closed-world” assumption, expecting the testing data distribu-
tion to roughly match that of the training data. However, the
Figure 1: Drifting sample detection and explanation.
environments in which the models are deployed are usually
dynamically changing over time. Such changes may include
both organic behavior changes of benign players and mali-
cious mutations and adaptations of attackers. As a result, the
testing data distribution is shifting from the original training
data, which can cause serious failures to the models [23].
To address concept drift, most learning-based models re-
quire periodical re-training [36, 39, 52]. However, retraining
often needs labeling a large number of new samples (expen-
sive). More importantly, it is also difﬁcult to determine when
the model should be retrained. Delayed retraining can leave
the outdated model vulnerable to new attacks.
We envision that combating concept drift requires estab-
lishing a monitoring system to examine the relationship be-
tween the incoming data streams and the training data (and/or
the current classiﬁer). The high-level idea is illustrated in
Figure 1. While the original classiﬁer is working in the pro-
duction space, another system should periodically check how
qualiﬁed the classiﬁer is to make decisions on the incom-
ing data samples. A detection module () can ﬁlter drifting
samples that are moving away from the training space. More
importantly, to reason the causes of the drifting (e.g., attacker
mutation, organic behavior changes, previous unknown sys-
tem bugs), we need an explanation method () to link the
detection decision to semantically meaningful features. These
two capabilities are essential to preparing a learning-based
security application for the open-world environment.
USENIX Association
30th USENIX Security Symposium    2327
Training DataOriginal ClassiﬁerIncoming Samples...labelsAttack-2Attack-1Benign120Detect Drifting Explain Drifting ProductionSpaceMonitoringSpace“Interpretation”Facilitate Model UpdateCADEPrior works have explored the detection of drifting sam-
ples by directly checking the prediction conﬁdence of the
original classiﬁer ( 0 ) [32]. A low conﬁdence score could in-
dicate that the incoming sample is a drifting sample. However,
this conﬁdence score is a probability (sum up to 1.0) calcu-
lated based on the assumption that all the classes are known
(closed-world). A drifting sample that does not belong to any
existing classes might be assigned to a wrong class with high
conﬁdence (validated by existing works [25, 32, 37]). A more
recent work presents the idea to compute a non-conformity
measure between the incoming sample and each of the ex-
isting classes to determine ﬁtness [38]. This non-conformity
measure is calculated based on a distance function to quantify
the dissimilarity between samples. However, we ﬁnd that such
distance functions could easily lose effectiveness, especially
when the data is sparse with high dimensionality.
Our Method.
In this paper, we present a new method for
detecting drifting samples, coupled with a novel method to
explain the detection decisions. Collectively, we build a sys-
tem called CADE, which is short for “Contrastive Autoencoder
for Drifting detection and Explanation.” The key challenge is
to derive an effective distance function to measure the dissim-
ilarity of samples. Instead of arbitrarily picking the distance
function, we leverage the idea of contrastive learning [29] to
learn the distance function from existing training data, based
on existing labels. Given the training data (multiple classes)
of the original classiﬁer, we map the training samples into a
low-dimensional latent space. The map function is learned
by contrasting samples to enlarge the distances between sam-
ples of different classes, while reducing the distance between
samples in the same class. We show the resulting distance
function in the latent space can effectively detect and rank
drifting samples.
To explain a drifting sample, we identify a small set of im-
portant features that differentiate this sample from its nearest
class. A key observation is that traditional (supervised) expla-
nation methods do not work well [22, 28, 53, 62]. The insight
is that supervised explanation methods require both classes
(drifting samples and existing class) to have sufﬁcient sam-
ples to estimate their distributions. However, this requirement
is difﬁcult to meet, given the drifting sample is located in a
sparse space outside of training distribution. Instead, we ﬁnd
it is more effective to derive explanations based on distance
changes, i.e., features that cause the largest changes to the
distance between the drifting sample and its nearest class.
Evaluation. We evaluate our methods with two datasets,
including an Android malware dataset [7] and an intrusion
detection dataset released in 2018 [57]. Our evaluation shows
that our drifting detection method is highly accurate, with
an average F1 score of 0.96 or higher, which outperforms
various baselines and existing methods. Our analysis also
demonstrates the beneﬁt of using contrastive learning to re-
duce the ambiguity of detection decisions. For the explanation
model, we perform both quantitative and qualitative evalua-
tions. Case studies also show that the selected features match
the semantic behaviors of the drifting samples.
Furthermore, we worked with our collaborators in a secu-
rity company to test CADE on their internal malware database.
As an initial test, we obtained a sample of 20,613 Windows
PE malware that appeared from August 2019 to February
2020 from 395 families. This allows us to test the system
performance with more malware families and in a diverse set-
ting. The results are promising. For example, CADE achieves
an F1 score of 0.95 when trained on 10 families and tested on
160 previously unseen families. This leads to the interest to
further test and deploy CADE in a production system.
Contributions.
This paper has three main contributions.
• We propose CADE to complement existing supervised
learning based security applications to combat concept
drift. We introduce an effective method to detect drifting
samples based on contrastive representation learning.
• We illustrate the limitation of supervised explanation
methods in explaining outlier samples and introduce a
distance-based explanation method for this context.
• We extensively evaluate the proposed methods with two
applications. Our initial tests with a security company
show that CADE is effective. We have released the code of
CADE here1 to support future research.
2 Background and Problem Scope
In this section, we introduce the background for concept drift
under the contexts of security applications, and discuss the
limitations of some possible solutions.
Concept Drift.
Supervised machine learning has been
used in many security contexts to train detection models.
Concept drift is a major challenge to these models when
deployed in practice. Concept drift occurs as the testing data
distribution deviates from the original training data, causing
a shift in the true decision boundary [23]. This often leads to
major errors in the original model over time.
To detect concept drift, researchers propose various tech-
niques, which mostly involve the collection of new sets of data
to statistically assess model behaviors [9,10,20,31]. For some
of these works, they also require the effort of data labeling. In
security applications, knowing the existence of new attacks
and collecting data about them are challenging in the ﬁrst
place. Besides, labeling data is time-consuming and requires
substantial expertise. As such, it is impractical to assume that
most incoming data can be sufﬁciently labeled.
Besides supervised models, semi-supervised anomaly de-
tection systems are not necessarily immune to concept drift.
For example, most network intrusion detection systems are
1https://github.com/whyisyoung/CADE
2328    30th USENIX Security Symposium
USENIX Association
learned on “normal” trafﬁc, and then used to detect incom-
ing trafﬁc that deviates from the learned “norm” as at-
tacks [24, 34, 48]. For such systems, they might detect previ-
ously unknown attacks; however, concept drift, especially in
benign trafﬁc, could easily cause model failures. Essentially,
intrusion detection is still a classiﬁcation problem, i.e., to dis-
tinguish normal trafﬁc from abnormal trafﬁc. Its training is
performed only with one category of data. This, to some ex-
tent, weakens the learning outcome. The systems still rely on
the assumption that the normal data has covered all possible
cases – which is often violated in the testing phase [60].
Our Problem Scope.
Instead of detecting concept drift
with well-prepared and fully labeled data, we focus on a more
practical scenario. As shown in Figure 1, we investigate in-
dividual samples to detect those that are shifted away from
the original training data. This allows us to detect drifting
samples and labels (a subset of) them as they arrive. Once we
accumulate drifting samples sufﬁciently, we can assess the
need for model re-training.
In a multi-class classiﬁcation setting, there are two major
types of concept drift. Type A: the introduction of a new class:
drifting samples come from a new class that does not exist in
the training dataset. As such, the originally trained classiﬁer
is not qualiﬁed to classify the drifting samples; Type B: in-
class evolution: the drifting samples are still from the existing
classes, but their behavior patterns are signiﬁcantly different
from those in the training dataset. In this case, the original
classiﬁer can easily make mistakes on these drifting samples.
In this paper, we primarily focus on Type A concept drift,
i.e., the introduction of a new class in a multi-class setting.
Taking malware classiﬁcation for example (Figure 1), our goal
is to detect and interpret drifting samples from previously un-
seen malware families. Essentially, the drifting samples are
out-of-distribution samples with respect to all of the existing
classes in the training data. In Section 6, we explore adapt-
ing our solution to address Type B concept drift (in-class
evolution) and examine the generalizability of our methods.
Possible Solutions & Limitations. We brieﬂy discuss the
possible directions to address this problem and the limitations.
The ﬁrst direction is to use the prediction probability of the
original classiﬁer. More speciﬁcally, a supervised classiﬁer
typically outputs a prediction probability (or conﬁdence) as a
side product of the prediction label [32]. For example, in deep
neural networks, a softmax function is often used to produce
a prediction probability which indicates the likelihood that
a given sample belongs to each of the existing classes (with
a sum of 1). As such, a low prediction probability might
indicate the incoming sample is different from the existing
training data. However, we argue that prediction probability
is unlikely to be effective in our problem context. The reason
is this probability reﬂects the relative ﬁtness to the existing
classes (e.g., the sample ﬁts in class A better than class B). If
the sample comes from an entirely new class (neither class A
nor B), the prediction probability could be vastly misleading.
Many previous studies [25, 32, 37] have demonstrated that
a testing sample from a new class can lead to a misleading
probability assignment (e.g., associating a wrong class with a
high probability). Fundamentally, the prediction probability
still inherits the “closed-world assumption” of the classiﬁer,
and thus is not suitable to detect drifting samples.
Compared to prediction probability, a more promising di-
rection is to assess a sample’s ﬁtness to a given class directly.
The idea is, instead of assessing whether the sample ﬁts in
class A better than class B, we assess how well this sample
ﬁts in class A compared to other training samples in class
A. For example, autoencoder [33] can be used to assess a
sample’s ﬁtness to a given distribution based on a reconstruc-
tion error. However, as an unsupervised method, it is difﬁcult
for an autoencoder to learn an accurate representation of the
training distribution when ignoring the labels (see Section 4).
In a recent work, Jordaney et al. introduced a system called
Transcend [38]. It deﬁnes a “non-conformity measure” as the
ﬁtness assessment. Transcend uses a credibility p-value to
quantify how similar the testing sample xxx is to training sam-
ples that share the same class. p is the proportion of samples
in this class that are at least as dissimilar to other samples in
the same class as xxx. While this metric can pinpoint drifting
samples, such a system is highly dependent on a good def-
inition of “dissimilarity”. As we will show in Section 4, an
arbitrary dissimilarity measure (especially when data dimen-
sionality is high) can lead to bad performance.
3 Designing CADE
We propose a system called CADE for drift sample detection
and explanation. We start by describing the intuitions and
insights behind our designs, followed by the technical details
for each component.
3.1
Insights Behind Our Design
As shown in Figure 1, our system has two components to ()
detect drifting samples that are out of the training distribution;
and () explain the drifting samples to help analysts under-
stand the meaning of the drift. Through initial analysis, we
ﬁnd both tasks face a common challenge: the drifting samples
are located in a sparse outlier space, which makes it difﬁcult
to derive meaningful distance functions needed for both tasks.
First, detecting drifting samples requires learning a good
distance function to measure how “drifting samples” are dif-
ferent from existing distributions. However, the outlier space
is unboundedly large and sparse. For high-dimensional data,
the notion of distance starts to lose effectiveness due to the
“curse of dimensionality” [74]. Second, the goal of explana-
tion is to identify a small subset of important features that
most effectively differentiate the drifting sample from the
USENIX Association
30th USENIX Security Symposium    2329
training data. As such, we also need an effective distance