### Table 7: Transferability from Clean Models to Trapdoored Counterparts

In Table 7, we present the transferability of adversarial examples from clean models to their trapdoored counterparts. For all six attacks and across all models, the transferability never exceeds 3%. This definitive result indicates that, regardless of how successful an attacker is in removing or unlearning the trapdoor, or even if they reconstruct the original model, their efforts will be futile. The reason is that adversarial examples generated from these models do not work on the trapdoored model \( F_\theta \), which is the actual target of the attack.

### 7.2 Advanced Adaptive Attacks (Carlini)

In this section, we discuss two advanced adaptive attacks developed in collaboration with Dr. Nicholas Carlini. These attacks are specifically designed to detect and exploit weaknesses in the design of trapdoors. After the paper was accepted, Nicholas approached us and offered to test the robustness of trapdoors by developing more sophisticated adaptive attacks. Both attacks are significantly more effective in weakening trapdoor defenses. Below, we describe the key approaches and results of these attacks on different types of trapdoor defenses.

**Note:** A prior version of the paper included results on two other adaptive attacks: a low learning rate attack that carefully scans the loss landscape for adversarial examples, and a lower-bound perturbation attack that tries to avoid trapdoors by imposing a lower bound on the size of the perturbation. Our results show that both attacks are largely ineffective against trapdoors. Due to space constraints, we focus here on the two stronger Carlini attacks, and refer readers to [40] for detailed results on the low learning rate and lower-bound perturbation attacks.

#### General Principles

Nicholas' two attacks share two general principles:
1. **Trapdoor Detection and Avoidance:** They use different techniques to map out the boundaries of trapdoors in a protected model, i.e., their detection signatures, and then devise ways to compute adversarial perturbations that avoid these signatures.
2. **Computational Power:** They leverage significant computational power, well beyond normal experimental levels, such as running 10,000 optimization iterations instead of terminating on convergence. We consider this reasonable for an attacker and do not view computational overhead as a mitigating factor.

We evaluate these attacks against variants of trapdoors previously discussed: randomized neuron signatures (§6.5) and multiple trapdoors per label (§4.1).

- **Randomized Neuron Signatures:** These can make the signature dynamic at runtime. Since trapdoor avoidance is likely a greedy operation, the inclusion or exclusion of a small number of neurons can significantly alter the result. In practice, the defender can quickly switch between different neuron subsets in unpredictable sequences, making it difficult for attackers to optimize.
- **Multiple Trapdoors:** These cover more of the feature space, making trapdoor avoidance more challenging. Additionally, we hypothesize that additional trapdoors will remove some natural adversarial examples, similar to a randomized smoothing function.

When evaluating multiple trapdoors, we assume the attacker knows the exact number of trapdoors inserted into the model per label. To eliminate variance due to randomness in trapdoor placement, we generated five models for each trapdoor configuration and performed ten attacks on each model to generate each data point.

#### Oracle Signature Attack

Nicholas' first attack uses our oracle adversary assumptions, where the attacker knows the precise values of the trapdoor. With whitebox access to the model, the attacker can generate neuron values matching the input with the trapdoor for any layer in the model. The attacker then jointly optimizes for maximum distance in cosine distance from the known trapdoor signature while searching for adversarial examples.

**Figure 5** shows the results of the oracle signature attack on MNIST with both neuron randomization and multiple trapdoors per label for all label defense. The top (black) line represents the attack's ability to find successful adversarial examples. The red line marked as 100% sample rate shows the attack success after detection by the trapdoor defense without neuron randomization. Without randomized neuron sampling, the signature attack succeeds nearly 90% of the time, effectively breaking the trapdoor defense. The other two lines show the attack success rate using neuron randomization. We observe that neuron sampling helps mitigate the attack, reducing the success rate to 40% after detection, by using only 5% of neuron values to build the detection signature. The defender can update the random neuron subset in real time with near-zero computational cost. We note that additional trapdoors have only a slight effect on detection because the attacker always knows the number of trapdoors present.

#### Trapdoor Vault Attack

Nicholas' second attack, which we call the vault attack, does not require knowledge of the trapdoor signature. Instead, it estimates the trapdoor signature from the results of computing adversarial examples on multiple input images. We extrapolate this further to target multiple trapdoors by using a histogram/clustering approach to approximate neuron signatures for each of the N trapdoors. The attacker then computes joint optimization that maximizes distance to known trapdoor signatures while searching for adversarial examples. Again, we assume attackers know the exact number of trapdoors present in the model.

**Figure 6** shows the results of the vault attack on MNIST with both neuron randomization and multiple trapdoors. We see only small benefits from having multiple trapdoors in the model. However, in this setting, the trapdoor defense detects more attacks due to errors in the signature approximation (which can likely be improved with effort). We note that when combining randomized neuron sampling (at 5%) with multiple trapdoors, we can detect significantly more attacks, dropping the success rate below 40%.

### Discussion and Next Steps

Time constraints greatly limited the amount of exploration possible in both mitigation mechanisms and further adaptive attacks. Under base conditions (single trapdoor with 100% neuron signature sampling), both attacks effectively break the trapdoor defense. While our preliminary results show some promise of mitigation, clearly much more work is needed to explore additional defenses and more powerful adaptive attacks. These attacks are dramatically more effective than other countermeasures because they were custom-tailored to target trapdoors. We consider their efficacy as validation that defense papers should work harder to include more rigorous, targeted adaptive attacks.

### 8 Conclusion and Future Work

In this paper, we propose using honeypots to defend DNNs against adversarial examples. Unlike traditional defenses, our proposed method trains trapdoors into normal models to introduce controlled vulnerabilities (traps) into the model. Trapdoors can defend all labels or particular labels of interest. Across multiple application domains, our trapdoor-based defense has high detection success against adversarial examples generated by a suite of state-of-the-art adversarial attacks, including CW, ElasticNet, PGD, BPDA, FGSM, and SPSA, with negligible impact on normal input classification accuracy.

In addition to analytical proofs of the impact of trapdoors on adversarial attacks, we evaluate and confirm trapdoors' robustness against multiple strong adaptive attacks, including black-box attacks and unlearning attacks. Our results on Carlini’s oracle and vault attacks show that trapdoors do have significant vulnerabilities. While randomized neuron signatures can help mitigate the attacks, it is clear that further effort is necessary to study both stronger attacks and mitigation strategies on honeypot-based defenses like trapdoors.

### Acknowledgments

We are thankful for the significant time and effort contributed by Nicholas Carlini in helping us develop stronger adaptive attacks on trapdoors. We have learned much in the process. We also thank our shepherd Ting Wang and anonymous reviewers for their constructive feedback. This work is supported in part by NSF grants CNS-1949650, CNS-1923778, CNS-1705042, and by the DARPA GARD program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of any funding agencies.

### References

[1] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet. 2018. Turning your weakness into a strength: Watermarking deep neural networks by backdooring. In Proc. of USENIX Security.
...
[54] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. 2016. Improving the robustness of deep neural networks via stability training. In Proc. of CVPR.

### Appendix

#### 8.1 Proof of Theorem 1 & 2

**Proof of Theorem 1**

This theorem assumes that after injecting the trapdoor \(\Delta\) into the model, we have...

**Table 8: Model Architecture for MNIST**

FC stands for fully-connected layers.