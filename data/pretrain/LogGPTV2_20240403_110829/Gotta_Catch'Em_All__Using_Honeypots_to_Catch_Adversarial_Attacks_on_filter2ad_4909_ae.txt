Table 7, we show the transferability from clean models to their trap-
doored counterparts. For all 6 attacks and all models, transferability
is always never higher than 3%. This deﬁnitive result states that no
matter how successful an attacker is at removing or unlearning the
trapdoor, or if they otherwise rebuild the original model, their ef-
forts will fail because adversarial examples from these models do
not work on the trapdoored model Fθ that is the actual attack target.
7.2 Advanced Adaptive Attacks (Carlini)
In this section, we present results on two advanced attacks devel-
oped in collaboration with Dr. Nicholas Carlini, both crafted to de-
tect and leverage weaknesses in the design of trapdoors. Nicholas
approached us after the paper was accepted and offered to test the
robustness of trapdoors by developing more advanced adaptive at-
tacks. Both attacks are signiﬁcantly more successful in weakening
trapdoor defenses. Here, we describe both attacks, their key ap-
proaches and their results on different types of trapdoor defenses.
We note that a prior version of the paper included results on two
other adaptive attacks: a low learning rate attack that more care-
fully scans the loss landscape for adversarial examples, and a lower-
bound perturbation attack that tries to avoid trapdoors by imposing
a lower bound on the size of the perturbation. Our results show both
attacks are largely ineffective against trapdoors. Due to space con-
straints, we focus on two stronger Carlini attacks here, and refer
readers to [40] for detailed results on low learning rate and lower-
bound perturbation attacks.
Generalities. Nicholas’ two attacks share two general principles.
First, they use different techniques to map out the boundaries of
trapdoors that exist in a protected model, i.e. their detection signa-
tures, and then devise ways to compute adversarial perturbations
that avoid them. Second, they leverage signiﬁcant compute power,
well beyond normal experimental levels, e.g. running 10K optimiza-
tion iterations instead of terminating on convergence. We consider
these quite reasonable for an attacker and do not consider computa-
tional overhead a mitigating factor.
Instead, we evaluate these attacks against variants of trapdoors
previously discussed: randomized neuron signatures (§6.5) and mul-
tiple trapdoors per label (§4.1). First, randomized neuron signatures
can effectively make the signature dynamic at run time. Since trap-
door avoidance is likely a greedy operation, the inclusion or exclu-
sion of a small number of neurons can signiﬁcantly alter the result.
In practice, the defender can quickly switch between different neu-
ron subsets in unpredictable sequences, making attacker optimiza-
tions difﬁcult. Second, multiple trapdoors cover more of the fea-
ture space, making trapdoor avoidance more difﬁcult. In addition,
we hypothesize that additional trapdoors will remove some natural
adversarial examples, much like a randomized smoothing function.
When we evaluate using multiple trapdoors, we assume the attacker
knows exactly the number of trapdoors that have been inserted into
 1
 0.8
 0.6
 0.4
 0.2
e
t
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 0
 1
Attack Convergence Rate
Sample Rate=100%
Sample Rate=10%
Sample Rate=5%
 3
 5
 7
 9
 11
 13
# of Trapdoors per Label
Figure 5: Oracle Signature Attack success against random neu-
ron sampling and multiple trapdoors.
 1
 0.8
 0.6
 0.4
 0.2
e
t
a
R
s
s
e
c
c
u
S
k
c
a
t
t
A
 0
 1
Attack Convergence Rate
Sample Rate=100%
Sample Rate=10%
Sample Rate=5%
 3
 5
 7
 9
 11
 13
# of Trapdoors per Label
Figure 6: Trapdoor Vault Attack success against random neu-
ron sampling and multiple trapdoors.
the model per label. Note that we generated 5 models for each trap-
door conﬁguration to eliminate variance in results due to random-
ness in trapdoor placement and then performed 10 attacks on each
model to generate each single data point.
Oracle Signature Attack. Nicholas’ ﬁrst attack uses our oracle
adversary assumptions, where the attacker knows the precise values
of the trapdoor. Combined with whitebox access to the model, the
attacker is thus able to generate neuron values matching input with
the trapdoor for any layer in the model. The attacker then jointly
optimizes for maximum distance in cosine distance from the known
trapdoor signature while searching for adversarial examples.
We plot in Figure 5 the results of the oracle signature attack on
MNIST with both neuron randomization and multiple trapdoors per
label for all label defense. The top (black) line shows the attack’s
ability to ﬁnd successful adversarial examples. The red line marked
as 100% sample rate represents the attack success after detection by
trapdoor defense without neuron randomization. Without random-
ized neuron sampling, the signature attack succeeds nearly 90% of
the time, effectively breaking the trapdoor defense. The other two
lines show the attack success rate using neuron randomization. We
do see that neuron sampling helps to mitigate the attack, reducing
attack success to 40% after detection, by using only 5% of neuron
values to build the detection signature. Recall that the defender can
update the random neuron subset in real time with near zero com-
putational cost. We note that additional trapdoors have only a slight
effect on detection, because the attacker always knows the number
of trapdoors present.
Trapdoor Vault Attack. Nicholas’ second attack, which we call
the vault attack, does not require knowledge of the trapdoor sig-
nature. Instead, it estimates the trapdoor signature from the results
of computing adversarial examples on multiple input images. We
extrapolate this further to target multiple trapdoors by using a his-
togram/clustering approach to approximate neuron signatures for
each of the N trapdoors. The attacker then computes joint optimiza-
tion that maximizes distance to known trapdoor signatures while
searching for adversarial examples. Again we assume attackers know
the exact number of trapdoors present in the model.
We plot in Figure 6 the results of the vault attack on MNIST with
both neuron randomization and multiple trapdoors. Again, we see
only small beneﬁts from having multiple trapdoors in the model.
However, in this setting the trapdoor defense does detect more at-
tacks because of errors in the signature approximation (which can
likely be improved with effort). We do note that when combining
randomized neuron sampling (at 5%) with multiple trapdoors, we
can detect signiﬁcantly more attacks, dropping attack success to be-
low 40%.
Time constraints greatly limited
Discussion and Next Steps.
the amount of exploration possible in both mitigation mechanisms
and further adaptive attacks. Under base conditions (single trap-
door with 100% neuron signature sampling), both attacks effec-
tively break the trapdoor defense. While our preliminary results
show some promise of mitigation, clearly much more work is needed
to explore additional defenses (and more powerful adaptive attacks).
These attacks are dramatically more effective than other coun-
termeasures because they were custom-tailored to target trapdoors.
We consider their efﬁcacy as validation that defense papers should
work harder to include more rigorous, targeted adaptive attacks.
8 CONCLUSION AND FUTURE WORK
In this paper, we propose using honeypots to defend DNNs against
adversarial examples. Unlike traditional defenses, our proposed method
trains trapdoors into normal models to introduce controlled vulnera-
bilities (traps) into the model. Trapdoors can defend all labels or par-
ticular labels of interest. Across multiple application domains, our
trapdoor-based defense has high detection success against adversar-
ial examples generated by a suite of state-of-the-art adversarial at-
tacks, including CW, ElasticNet, PGD, BPDA, FGSM, and SPSA,
with negligible impact on normal input classiﬁcation accuracy.
In addition to analytical proofs of the impact of trapdoors on
adversarial attacks, we evaluate and conﬁrm trapdoors’ robustness
against multiple strong adaptive attacks, including black-box attacks
and unlearning attacks. Our results on Carlini’s oracle and vault at-
tacks show that trapdoors do have signiﬁcant vulnerabilities. While
randomized neuron signatures can help mitigate the attacks, it is
clear that further effort is necessary to study both stronger attacks
and mitigation strategies on honeypot-based defenses like trapdoors.
ACKNOWLEDGMENTS
We are thankful for signiﬁcant time and effort contributed by Nicholas
Carlini in helping us develop stronger adaptive attacks on trapdoors.
We have learned much in the process. We also thank our shepherd
Ting Wang and anonymous reviewers for their constructive feed-
back. This work is supported in part by NSF grants CNS-1949650,
CNS-1923778, CNS-1705042, and by the DARPA GARD program.
Any opinions, ﬁndings, and conclusions or recommendations ex-
pressed in this material are those of the authors and do not necessar-
ily reﬂect the views of any funding agencies.
REFERENCES
[1] Yossi Adi, Carsten Baum, Moustapha Cisse, Benny Pinkas, and Joseph Keshet.
2018. Turning your weakness into a strength: Watermarking deep neural net-
works by backdooring. In Proc. of USENIX Security.
[2] Anish Athalye, Nicholas Carlini, and David Wagner. 2018. Obfuscated gradients
give a false sense of security: Circumventing defenses to adversarial examples.
In Proc. of ICML.
[3] J. Buckman, A. Roy, C. Raffel, and I. Goodfellow. 2018. Thermometer encoding:
One hot way to resist adversarial examples. In Proc. of ICLR.
[4] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. 2018.
Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE
International Conference on Automatic Face & Gesture Recognition (FG 2018).
IEEE, 67–74.
[5] Yinzhi Cao, Alexander Fangxiao Yu, Andrew Aday, Eric Stahl, Jon Merwine, and
Junfeng Yang. 2018. Efﬁcient Repair of Polluted Machine Learning Systems via
Causal Unlearning. In Proc. of ASIACCS.
[6] Nicholas Carlini, Anish Athalye, Nicolas Papernot, Wieland Brendel, Jonas
Rauber, Dimitris Tsipras, Ian Goodfellow, Aleksander Madry, and Alexey
Kurakin. 2019.
arXiv preprint
arXiv:1902.06705 (2019).
On Evaluating Adversarial Robustness.
[7] Nicholas Carlini and David Wagner. 2016. Defensive distillation is not robust to
adversarial examples. arXiv preprint arXiv:1607.04311 (2016).
[8] Nicholas Carlini and David Wagner. 2017. Adversarial examples are not easily
detected: Bypassing ten detection methods. Proc. of AISec (2017).
[9] Nicholas Carlini and David Wagner. 2017. Magnet and efﬁcient defenses
against adversarial attacks are not robust to adversarial examples. arXiv preprint
arXiv:1711.08478 (2017).
[10] Nicholas Carlini and David Wagner. 2017. Towards evaluating the robustness of
neural networks. In Proc. of IEEE S&P.
[11] Antonin Chambolle. 2004. An algorithm for total variation minimization and
applications. Journal of Mathematical Imaging and Vision 20, 1 (2004), 89–97.
[12] Pin-Yu Chen, Yash Sharma, Huan Zhang, Jinfeng Yi, and Cho-Jui Hsieh. 2018.
EAD: elastic-net attacks to deep neural networks via adversarial examples. In
Proc. of AAAI.
[13] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. 2017. Tar-
geted Backdoor Attacks on Deep Learning Systems Using Data Poisoning. arXiv
preprint arXiv:1712.05526 (2017).
[14] Ambra Demontis, Marco Melis, Maura Pintor, Matthew Jagielski, Battista Big-
gio, Alina Oprea, Cristina Nita-Rotaru, and Fabio Roli. 2019. Why do adversar-
ial attacks transfer? explaining transferability of evasion and poisoning attacks.
In Proc. of USENIX Security. 321–338.
[15] G. S. Dhillon, K. Azizzadenesheli, J. D. Bernstein, J. Kossaiﬁ, A. Khanna, Z. C.
Lipton, and A. Anandkumar. 2018. Stochastic activation pruning for robust ad-
versarial defense. In Proc. of ICLR.
[16] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. 2014. Explaining and
harnessing adversarial examples. arXiv preprint arXiv:1412.6572 (2014).
[17] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. 2017. Badnets: Iden-
tifying vulnerabilities in the machine learning model supply chain. In Proc. of
Machine Learning and Computer Security Workshop.
[18] C. Guo, M. Rana, M. Cisse, and L. van der Maaten. 2018. Countering adversarial
images using input transformations. In Proc. of ICLR.
[19] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. 2016. Ms-
celeb-1m: A dataset and benchmark for large-scale face recognition. In European
Conference on Computer Vision. Springer, 87–102.
[20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual
learning for image recognition. In Proc. of CVPR.
[21] Warren He, James Wei, Xinyun Chen, Nicholas Carlini, and Dawn Song. 2017.
Adversarial example defenses: Ensembles of weak defenses are not strong. In
Proc. of WOOT.
[22] J. Zico Kolter and Eric Wong. 2017. Provable defenses against adversarial exam-
ples via the convex outer adversarial polytope. In Proc. of NeurIPS.
[23] Alex Krizhevsky and Geoffrey Hinton. 2009. Learning multiple layers of features
from tiny images. Technical Report.
[24] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2016. Adversarial examples
in the physical world. arXiv preprint arXiv:1607.02533 (2016).
[25] Alexey Kurakin, Ian Goodfellow, and Samy Bengio. 2017. Adversarial machine
learning at scale. In Proc. of ICLR.
[26] Yann LeCun, LD Jackel, Léon Bottou, Corinna Cortes, John S Denker, Harris
Drucker, Isabelle Guyon, UA Muller, Eduard Sackinger, Patrice Simard, et al.
1995. Learning algorithms for classiﬁcation: A comparison on handwritten digit
recognition. Neural Networks 261 (1995), 276.
[27] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. 2018. Fine-pruning: De-
fending against backdooring attacks on deep neural networks. In International
Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 273–294.
[28] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xi-
angyu Zhang. 2019. ABS: Scanning neural networks for back-doors by artiﬁcial
brain stimulation. In Proceedings of the 2019 ACM SIGSAC Conference on Com-
puter and Communications Security. ACM, 1265–1282.
[29] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang
Wang, and Xiangyu Zhang. 2018. Trojaning Attack on Neural Networks. In Proc.
of NDSS.
[30] Shiqing Ma, Yingqi Liu, Guanhong Tao, Wen-Chuan Lee, and Xiangyu Zhang.
2019. NIC: Detecting Adversarial Samples with Neural Network Invariant Check-
ing. In Proc. of NDSS.
[31] Xingjun Ma, Bo Li, Yisen Wang, Sarah M Erfani, Sudanthi Wijewickrema, Grant
Schoenebeck, Dawn Song, Michael E Houle, and James Bailey. 2018. Charac-
terizing adversarial subspaces using local intrinsic dimensionality. In Proc. of
ICLR.
[32] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and
Adrian Vladu. 2018. Towards deep learning models resistant to adversarial at-
tacks. In Proc. of ICLR.
[33] Dongyu Meng and Hao Chen. 2017. Magnet: a two-pronged defense against
adversarial examples. In Proc. of CCS.
[34] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z. Berkay Ce-
lik, and Ananthram Swami. 2017. Practical black-box attacks against machine
learning. In Proc. of AsiaCCS.
[35] Nicolas Papernot, Patrick McDaniel, Xi Wu, Somesh Jha, and Ananthram Swami.
2016. Distillation as a defense to adversarial perturbations against deep neural
networks. In Proc. of IEEE S&P.
[36] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. 2015. Deep face
recognition.. In bmvc, Vol. 1. 6.
[37] Ximing Qiao, Yukun Yang, and Hai Li. 2019. Defending Neural Backdoors via
Generative Distribution Modeling. arXiv preprint arXiv:1910.04749 (2019).
[38] P. Samangouei, M. Kabkab, and R. Chellappa. 2018. Defensegan: Protecting
classiﬁers against adversarial attacks using generative models. In Proc. of ICLR.
[39] Ali Shafahi, W Ronny Huang, Christoph Studer, Soheil Feizi, and Tom Goldstein.
2019. Are adversarial examples inevitable?. In Proc. of ICLR.
[40] Shawn Shan, Emily Wenger, Bolun Wang, Bo Li, Haitao Zheng, and Ben Y. Zhao.
2020. Gotta Catch ’Em All: Using Honeypots to Catch Adversarial Attacks on
Neural Networks. arXiv preprint: 1904.08554 (2020).
[41] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter. 2016.
Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recogni-
tion. In Proc. of CCS.
[42] Y. Song, T. Kim, S. Nowozin, S. Ermon, and N. Kushman. 2018. Pixeldefend:
Leveraging generative models to understand and defend against adversarial ex-
amples. In Proc. of ICLR.
[43] James C Spall et al. 1992. Multivariate stochastic approximation using a simulta-
neous perturbation gradient approximation. IEEE Trans. Automat. Control 37, 3
(1992), 332–341.
[44] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel. 2012. Man vs. computer:
Benchmarking machine learning algorithms for trafﬁc sign recognition. Neural
Networks (2012).
[45] Octavian Suciu, Radu M˘arginean, Yi˘gitcan Kaya, Hal Daumé III, and Tudor Du-
mitra¸s. 2018. When Does Machine Learning FAIL? Generalized Transferability
for Evasion and Poisoning Attacks. In Proc. of USENIX Security.
[46] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Er-
han, Ian Goodfellow, and Rob Fergus. 2014. Intriguing properties of neural net-
works. In Proc. of ICLR.
[47] Jonathan Uesato, Brendan O’Donoghue, Aaron van den Oord, and Pushmeet
Kohli. 2018. Adversarial risk and the dangers of evaluating against weak attacks.
arXiv preprint arXiv:1802.05666 (2018).
[48] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao
Zheng, and Ben Y. Zhao. 2019. Neural Cleanse: Identifying and Mitigating Back-
door Attacks in Neural Networks. In Proc. of IEEE S&P.
[49] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille. 2018. Mitigating adversarial
effects through randomization. In Proc. of ICLR.
[50] Weilin Xu, David Evans, and Yanjun Qi. 2018. Feature squeezing: Detecting
adversarial examples in deep neural networks. In Proc. of NDSS.
[51] YouTube. https://www.cs.tau.ac.il/~wolf/ytfaces/. (????). YouTube Faces DB.
[52] Valentina Zantedeschi, Maria-Irina Nicolae, and Ambrish Rawat. 2017. Efﬁcient
defenses against adversarial attacks. In Proc. of AISec.
[53] Jialong Zhang, Zhongshu Gu, Jiyong Jang, Hui Wu, Marc Ph Stoecklin, Heqing
Huang, and Ian Molloy. 2018. Protecting intellectual property of deep neural
networks with watermarking. In Proc. of AsiaCCS.
[54] Stephan Zheng, Yang Song, Thomas Leung, and Ian Goodfellow. 2016.
Im-
proving the robustness of deep neural networks via stability training. In Proc.
of CVPR.
APPENDIX
8.1 Proof of Theorem 1 & 2
Proof of Theorem 1
PROOF. This theorem assumes that after injecting the trapdoor
∆ into the model, we have
Table 8: Model Architecture for MNIST. FC stands for fully-