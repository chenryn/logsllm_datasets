ing solutions as well as the baseline CQCC-GMM solution
– many ensemble solutions used CQCC-GMM as the base-
line model. Void uses a single classiﬁcation model and just
97 features. Compared to STFT-LCNN, Void uses 153 times
less memory and is about 8 times faster in detection. Void is
1.8 times faster and uses 87 times less memory compared to
the baseline CQCC-GMM solution. Void’s feature size and
testing time performances (shown in Section 7.4) sit more
comfortably with the near-zero-second latency and model
complexity requirements. While being lightweight, Void still
achieves an EER of 11.6%, ahead of the second best solution
in the ASVspoof competition [10]. Although this is higher
than the 10% EER requirement, our ensemble solution that
uses MFCC-GMM (MFCC is moderately light, and is already
being used by speech recognition services) achieves 8.7%
EER, and satisfy the EER requirement. Further, we demon-
strated 0.3% EER against our own dataset.
Being mindful of how light Void is, another possible de-
ployment scenario would involve deploying the Void classiﬁer
at the device level: when a user submits a voice command, the
voice assistant running on the user’s device would ﬁrst make
a voice liveness decision, and drop attack-like commands im-
mediately. With this type of on-device deployment, we would
not introduce any new detection (computational) burden on
servers.
9.2 Low-incidence population
In practice, Void would be used by a low-incidence population
where the number of replay attacks being performed would
be much smaller than the number of legitimate uses. Even if
Void is conﬁgured with a threshold value to minimize false
rejection rates (e.g., lower than 5%), users might be annoyed
by infrequent voice command rejections. Hence, when an
attack is detected, users should be asked to perform explicit
authentication (e.g., uttering voice passwords) to still proceed
with a suspicious command if authentication is successful.
Further, Kwak et al. [33] shows that about 90% of existing
mobile voice assistant users use less than 20 commands per
month – for those light users, there will only be about ﬁve
falsely rejected commands every 5 months of use.
However, the incidence level would be quite different when
voice assistants are used in homes, e.g., through a smart
speaker. This is because there would be frequent loudspeaker
noises being generated from TV speakers (e.g., [13]) and
standalone speakers. Voice assistants would be stressed with
much larger volumes of loudspeaker noises (e.g., TV pro-
grams or music being played), and be expected to accurately
detect and disregard them. Accurate detection and ﬁltering
of loudspeaker noises would improve the reliability of us-
ing voice assistants at homes (lower false acceptances), and
signiﬁcantly improve efﬁciency of speech to text translation
engines as loudspeaker noises would not be analyzed.
9.3 Limitations
We tested Void against the ASVspoof dataset (see Section 6.3),
which consists of 26 different playback devices and 25 differ-
ent recording devices (microphones), including studio moni-
USENIX Association
29th USENIX Security Symposium    2697
tors and headsets, and studio-quality condenser microphones.
Our results in Section 7.6.3 show that Void’s performance
could be downgraded when high quality speakers, such as ex-
pensive studio monitors, are used to replay recorded samples.
Our audio EQ manipulation attack results (see Section 8.4)
showed that carefully crafted adversarial attacks that involve
altering frequency responses, or exploiting SVM gradients
may be performed to compromise Void. However, such at-
tacks would require strong signal processing expertise.
10 Related work
Recent studies have demonstrated that voice assistants are
prone to various forms of voice presentation attacks [6,11,12,
20, 23, 33]. Carlini et al. [24, 25] presented hidden voice com-
mand attacks to generate mangled voice commands that are
unintelligible to human listeners but can still be interpreted as
commands by devices. Zhang et al. [18] extended this attack
to make voice commands completely inaudible by modulat-
ing voice commands on ultrasonic carriers. To overcome the
limitations of short attack ranges of inaudible attacks (works
within about 5ft) [18, 19], Roy et al. [20] demonstrated the
feasibility of launching such attacks from longer distances
(i.e., within 25ft range) by using multiple ultrasonic speak-
ers. They striped segments of voice signals across multiple
speakers placed in separate locations.
Many approaches have been proposed to detect machine-
generated voice attacks. “VoiceLive” [27] measures the “time
difference of arrival” changes in sequences of phoneme
sounds using dual microphones available on smartphones.
The measured changes are used to determine the sound origin
of each phoneme (within the human vocal system) for live-
ness detection. VoiceLive was evaluated with 12 participants,
demonstrating 1% EER. Zhang et al. [3] also proposed artic-
ulatory gesture-based liveness detection (analyzing precise
articulatory movements like lip and tongue movements); their
approaches, however, are only applicable to scenarios where
a user is physically speaking near a smartphone’s microphone.
In contrast, Void would work well even when users are a
few meters (speaking distances) away from target devices.
Chen et al. [12] leveraged magnetic ﬁelds emitted from loud-
speakers to detect replay attacks. Their approach, however,
requires users to utter a passphrase while moving smartphones
through a predeﬁned trajectory around sound sources. Blue et
al. [14] found that the amount of energy in a sub-bass region
(between 20Hz and 250Hz) can be used to distinguish live
human voices from speaker generated voices. However, their
approach relies on being aware of ambient noise power as a
priori while performing noise ﬁltering – this is necessary to
measure the amount of energy with high accuracy. Therefore,
this technique could be compromised by intentionally control-
ling the ambient noise power that the noise ﬁltering is relying
on. Feng et al. [11] proposed a voice authentication system
that uses a wearable device, such as eyeglasses – collecting
a user’s body surface vibrations, and matching it with voice
signals received by a voice assistant through a microphone.
Although their approach is capable of achieving about 97%
accuracy, they rely on an additional hardware that users have
to carry.
An extensive study was conducted to analyze the perfor-
mances of machine learning-based replay attack detection
techniques proposed as part of the 2017 ASVspoof competi-
tion [7]. According to the study ﬁndings, the equal error rates
(EER) varied from 6.7% to 45.6% [30] – most solutions used
an ensemble approach, and used CQCC-GMM as a baseline
model, which alone is complex and uses about 14,000 fea-
tures. We implemented STFT-LCNN [30], which is one of the
two deep learning models used by the top performing solution
from the competition. Our evaluations showed that despite
its low EER, STFT-LCNN alone is unacceptably heavy and
slow. Likewise, existing solutions have been designed merely
to minimize EERs.
Tom et al. [17] achieved 0% EER on the ASVspoof evalua-
tion set using Residual Network as the deep learning model
and group delay grams as the classiﬁcation features. Group de-
lay (GD) grams are obtained by adding a group delay function
over consecutive frames as a time-frequency representation.
However, they used another external dataset to pre-train a
model, and applied transfer learning technique on that model
using the ASVspoof train set. Since their model training meth-
ods and assumptions are not consistent with how ASVspoof
models are suppose to be trained (i.e., they assume other
datasets are available), we do not directly compare Void with
their solution. We implemented their model as close as pos-
sible to the descriptions provided in the paper, and analyzed
its time and space complexity as described in Appendix I.
Their model uses 786,432 features compared to Void’s 97
features, and uses about 1,195MB of memory on average for
classifying a sample. Void uses just 2MB.
Consequently, multiple complex models and heavy features
have been used without considering any latency and model
complexity requirements described in Section 3.1. Void was
designed to use a small number of features, and guarantee fast
training and classiﬁcation times as well as model simplicity.
Further, all the literature discussed above present model struc-
ture and accuracy results without providing insights into the
spectral power features and their characteristics – replying
on deep learning techniques for feature extraction has this
limitation. In contrast, we explain the spectral power patterns
and non-linearity for loudspeaker detection as part of feature
engineering (see Section 4).
11 Conclusion
Void analyzes the spectral power patterns of voice signals to
accurately detect voice replay attacks. Compared with existing
methods using multiple, heavy classiﬁcation models, Void
runs on a single efﬁcient classiﬁcation model with 97 features
2698    29th USENIX Security Symposium
USENIX Association
only, and does not require any additional hardware.
Our experiments, conducted on two large datasets col-
lected under numerous varying conditions (demographics,
speaker/microphone types, and background noises), showed
that Void can achieve 0.3% EER on our own dataset, and
11.6% EER on the ASVspoof evaluation set. On average,
Void took 0.03 seconds to classify a given voice sample, and
used just 1.98 megabytes of memory. Void is about 8 times
faster, and 153 times lighter (with respect to feature size) than
the top performing LCNN-based solution. Also, our ensemble
solution (that uses moderately light, already available MFCC
features) achieves 8.7% EER – making it a much more practi-
cal, and attractive solution for businesses to consider. More-
over, Void is resilient to adversarial attacks including hidden
command [24, 25], inaudible voice command [18–20], voice
synthesis [6, 12], EQ manipulation, and combining replay at-
tacks with live-human voices achieving over 86% detection
rates for all of those attack types.
Acknowledgment
This work was supported by Samsung Research and NRFK
(2019R1C1C1007118). The authors would like to thank all the
anonymous reviewers and Carrie Gates for their valuable feedback.
Note that Hyoungshick Kim is the corresponding author.
References
[1] Y. Wang, R.J. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N.
Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. Le, Y. Agiomyr-
giannakis, R. Clark, R. A. Saurous, “Tacotron: Towards End-
to-End Speech Synthesis”, in Proceedings of the 18th INTER-
SPEECH, 2017.
[2] A. Gibiansky, S. Arik, G. Diamos, J. Miller, K. Peng, W. Ping, J.
Raiman, Y. Zhou, “Deep Voice 2: Multi-Speaker Neural Text-to-
Speech”, in Advances in Neural Information Processing Systems
30, pp. 2966-2974, 2017.
[3] L. Zhang, S. Tan, J. Yang, “Hearing Your Voice is Not Enough:
An Articulatory Gesture Based Liveness Detection for Voice
Authentication”, in Proceedings of the 24th ACM SIGSAC Con-
ference on Computer and Communications Security, 2017.
[4] S. McCandless, “An algorithm for automatic formant extraction
using linear prediction spectra”, in IEEE Transactions on Acous-
tics, Speech, and Signal Processing, vol. 22, no. 2, pp. 135-141,
1974.
[5] T. F. Li, S. Chang, “Speech recognition of mandarin syllables
using both linear predict oding cepstra and Mel frequency cep-
stra”, in Proceedings of the 19th Conference on Computational
Linguistics and Speech Processing, 2007.
[6] A. Janiki, F. Alegre, and N. Evans, “An assessment of automatic
speaker veriﬁcation vulnerabilities to replay spooﬁng attacks”,
in Security and Communication Networks, pp. 3030-3044, 2016.
[7] T. Kinnunen, M. Sahidullah, H. Delgado, M. Todisco, N. Evans,
J. Yamagishi, and K. A. Lee, “The ASVspoof 2017 Challenge:
Assessing the Limits of Replay Spooﬁng Attack Detection”, in
Proceedings of the 18th INTERSPEECH, 2017.
[8] H. Delgado, M. Todisco, M. Sahidullah, N. Evans, T. Kinnunen,
K. A. Lee, J. Yamagishi, “ASVspoof 2017 Version 2.0: meta-
data analysis and baseline enhancements”, in Proceedings of the
Speaker and Language Recognition Workshop, 2018.
[9] T. Kinnunen, N. Evans, J. Yamagishi, K. A. Lee, M.
Sahidullah, M. Todisco, and H. Delgado, “ASVspoof
and
2017: Automatic Speaker Veriﬁcation Spooﬁng
Countermeasures
Plan”,
[On-
line:]
https://www.asvspoof.org/data2017/
asvspoof-2017_evalplan_v1.2.pdf, 2017.
Evaluation
Challenge
[10] T. Kinnunen, M. Sahidullah, H. Delgado, M. Todisco, N.
Evans, J. Yamagishi, and K. A. Lee, “The ASVspoof 2017
Challenge: Assessing the Limits of Replay Spooﬁng Attack
Detection”, [Online:] https://www.asvspoof.org/
slides_ASVspoof2017_Interspeech.pdf, 2017.
[11] H. Feng, K. Fawaz, and K. G. Shin, “Continuous Authentica-
tion for Voice Assistants”, in Proceedings of the 23rd Annual
International Conference on Mobile Computing and Network-
ing, 2017.
[12] S. Chenyz, K. Reny, S. Piaoy, C. Wang, Q. Wangx, J. Weng, L.
Suy, and A. Mohaisen, “You Can Hear But You Cannot Steal: De-
fending against Voice Impersonation Attacks on Smartphones”,
in Proceedings of IEEE 37th International Conference on Dis-
tributed Computing Systems, 2017.
[13] A. Liptak, “Amazon’s Alexa started ordering people doll-
houses after hearing its name on TV”, [Online:] https:
//www.theverge.com/2017/1/7/14200210/
amazon-alexa-tech-news-anchor-order-dollhouse,
2017.
[14] L. Blue, L. Vargas, and P. Traynor, “Hello, Is It Me You’re
Looking For?: Differentiating Between Human and Electronic
Speakers for Voice Interface Security” in Proceedings of the
11th ACM Conference on Security & Privacy in Wireless and
Mobile Networks, 2018.
[15] D. Luo, H. Wu, and J. Huang, “Audio recapture detection
using deep learning”, in Proceedings of the 3rd IEEE China
Summit and International Conference on Signal and Information
Processing, 2015.
[16] Sound Engineering Academy, “Human Voice Frequency
Range”, [Online:] http://www.seaindia.in/blog/
human-voice-frequency-range/
[17] F. Tom, M. Jain and P. Dey, “End-To-End Audio Replay Attack
Detection Using Deep Convolutional Networks with Attention”,
in Proceedings of the 19th INTERSPEECH, 2018.
[18] G. Zhang, C. Yan, X. Ji, T. Zhang, T. Zhang, W. Xu, “Dolphi-
nAtack: Inaudible Voice Commands”, in Proceedings of the 24th
ACM SIGSAC Conference on Computer and Communications
Security, 2017.
[19] L. Song, P. Mittal, “POSTER: Inaudible Voice Commands”, in
Proceedings of the 24th ACM SIGSAC Conference on Computer
and Communications Security, 2017.
[20] N. Roy, S. Shen, H. Hassanieh, R. R. Choudhury, “Inaudible
Voice Commands: The Long-Range Attack and Defense”, in
USENIX Association
29th USENIX Security Symposium    2699
Proceedings of 15th USENIX Symposium on Networked Systems
Design and Implementation, 2018.
[37] V. Gunnarsson, “Assessment of nonlinearities in loudspeakers”,
in Chalmers University of Technology, 2010.
[21] I. R. Titze, “Principles of Voice Production”, in Prentice Hall,
1994.
[22] R. J. Baken, “Clinical Measurement of Speech and Voice”, in
Taylor & Francis, 2000.
[23] S. Panjwani and A. Prakash, “Crowdsourcing Attacks on Bio-
metric Systems”, in Proceedings of the 10th Symposium On
Usable Privacy and Security, 2014.
[24] T. Vaidya, Y. Zhang, M. Sherr, and C. Shields, “Cocaine Noo-
dles: Exploiting the Gap between Human and Machine Speech
Recognition”, in Proceedings of the 9th USENIX Workshop on
Offensive Technologies, 2015.
[25] N. Carlini, P. Mishra, T. Vaidya, Y. Zhang, M. Sherr, C. Shields,
D. Wagner, and W. Zhou, “Hidden Voice Command”, in Pro-
ceedings of the 25th USENIX Security Symposium, 2016.
[26] P. Castiglioni, “Levinson-durbin algorithm”, in Encyclopedia
of Biostatistics, 2005.
[27] L. Zhang, S. Tan, J. Yang, and Y. Chen, “VoiceLive: A
Phoneme Localization based Liveness Detection for Voice Au-
thentication on Smartphones”, in Proceedings of the 23rd ACM
SIGSAC Conference on Computer and Communications Secu-
rity, 2016.
[28] L. Breiman, “Random Forests”, in Machine Learning, vol. 45,
no. 28, pp. 5–32, 2001.
[29] ASVspoof, [Online:] https://datashare.is.ed.ac.
uk/handle/10283/2778
[30] G. Lavrentyeva, S. Novoselov, E. Malykh, A. Kozlov, O. Ku-
dashev, V. Shchemelinin, “Audio replay attack detection with
deep learning frameworks”, in Proceedings of the 18th INTER-
SPEECH, 2017.
[31] X. Zhao, Y. Wang, and D. Wang, “Robust speaker identiﬁcation
in noisy and reverberant conditions”, in IEEE International Con-
ference on Acoustics, Speech and Signal Processing (ICASSP),
2014.
[32] G. Valenti, A. Daniel, and N. Evans, “End-to-end automatic
speaker veriﬁcation with evolving recurrent neural networks”,
in Odyssey 2018 The Speaker and Language Recognition Work-
shop, 2018.
[33] I. Kwak, J. Huh, S. Han, I. Kim, and J. Yoon, “Voice presen-
tation attack detection through text-converted voice command
analysis”, to appear in ACM CHI Conference on Human Factors
in Computing Systems, 2019.
[34] W. Frank and R. Reger and U. Appel, “Loudspeaker
nonlinearities-analysis and compensation”, in Conference
Record of the Twenty-Sixth Asilomar Conference on Signals,
Systems Computers, 1992.
[35] The
Audibility
of
[Online:]
Frequencies,
audioholics.com/loudspeaker-design/
audibility-of-distortion-at-bass, 2015.
Distortion
At