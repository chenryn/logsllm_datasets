title:Understanding PCIe performance for end host networking
author:Rolf Neugebauer and
Gianni Antichi and
Jos&apos;e Fernando Zazo and
Yury Audzevich and
Sergio L&apos;opez-Buedo and
Andrew W. Moore
Understanding PCIe performance for end host
networking
Gianni Antichi
Queen Mary, University of London
Sergio López-Buedo
Rolf Neugebauer
Independent Researcher
Yury Audzevich
University of Cambridge
José Fernando Zazo
Naudit HPCN
Andrew W. Moore
University of Cambridge
ABSTRACT
In recent years, spurred on by the development and avail-
ability of programmable NICs, end hosts have increasingly
become the enforcement point for core network functions
such as load balancing, congestion control, and application
specific network offloads. However, implementing custom
designs on programmable NICs is not easy: many potential
bottlenecks can impact performance.
Universidad Autónoma de Madrid
KEYWORDS
PCIe, reconfigurable hardware, Operating System
ACM Reference Format:
Rolf Neugebauer, Gianni Antichi, José Fernando Zazo, Yury Audze-
vich, Sergio López-Buedo, and Andrew W. Moore. 2018. Understand-
ing PCIe performance for end host networking. In SIGCOMM ’18:
SIGCOMM 2018, August 20–25, 2018, Budapest, Hungary. ACM, New
York, NY, USA, 15 pages. https://doi.org/10.1145/3230543.3230560
This paper focuses on the performance implication of PCIe,
the de-facto I/O interconnect in contemporary servers, when
interacting with the host architecture and device drivers. We
present a theoretical model for PCIe and pcie-bench, an
open-source suite, that allows developers to gain an accu-
rate and deep understanding of the PCIe substrate. Using
pcie-bench, we characterize the PCIe subsystem in modern
servers. We highlight surprising differences in PCIe imple-
mentations, evaluate the undesirable impact of PCIe features
such as IOMMUs, and show the practical limits for common
network cards operating at 40Gb/s and beyond. Furthermore,
through pcie-bench we gained insights which guided soft-
ware and future hardware architectures for both commercial
and research oriented network cards and DMA engines.
CCS CONCEPTS
• Networks → Network adapters; Network servers; •
Hardware → Networking hardware; Buses and high-speed
links;
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies
are not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for components of this work owned by others than the author(s) must
be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Request permissions from permissions@acm.org.
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
© 2018 Copyright held by the owner/author(s). Publication rights licensed
to ACM.
ACM ISBN 978-1-4503-5567-4/18/08...$15.00
https://doi.org/10.1145/3230543.3230560
327
1 INTRODUCTION
The idea of end hosts participating in the implementation of
network functionality has been extensively explored in enter-
prise and datacenter networks [6, 7, 25, 49, 56, 58]. Moreover,
the disruptive introduction into the market of programmable
NICs, alongside the deployment in datacenters of hybrid
Xeon and FPGA platforms [15], has boosted the demand for
new refined solutions which combine software functions and
hardware NIC acceleration to improve end host network-
ing performance [53], flexibility [16], or a combination of
both [29]. Several efforts try to leverage end host hardware
programmability to improve datacenter scalability [12, 13]
or specific network functions such as load balancing, appli-
cation level quality of service and congestion control [1].
In this paper, we show that PCIe, alongside its interaction
with the host architecture and device drivers, can signifi-
cantly impact the performance of network applications. Past
research has mostly considered this impact in the context
of specific applications such as Remote DMA (RDMA) [24],
GPU-accelerated packet processing [17], and optimized Key-
Value-Store (KVS) applications [31, 32, 34]. In contrast, we
argue that a more generic approach to studying and char-
acterizing PCIe is needed as it has become essential to im-
plement not only specialized, high-performance network
functions, but also storage adaptors and custom accelera-
tor cards, such as for machine learning [23]. It is in this
context that we introduce a theoretical model for PCIe (§3),
design a methodology to characterize PCIe in real systems
(§4), describe its implementation (§5), and present the results
derived using our approach (§6). This permits us to draw
several specific conclusions about the way PCIe currently
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Neugebauer et al.
operates and the implications this has for operating system
and application design (§7). For example, we demonstrate
how both NUMA for PCIe devices and IOMMUs can have a
negative performance impact on application performance.
The contributions of this paper are:
• We introduce a model of PCIe. The model provides a
baseline for expected PCIe throughput for a wide range
of PCIe configurations. It also allows to quickly assess
the performance impact when iterating through design
alternatives for device and device driver interactions, e.g.,
when implementing custom NIC functionality.
• We outline a methodology, pcie-bench: a combination of
micro-benchmarks that systematically measures aspects
of performance for both a device’s DMA engines and,
more crucially, the PCIe root complex1.
• We describe two implementations of pcie-bench based
on commodity PCIe peripherals: commercially available
programmable NICs from Netronome and the latest NetF-
PGA boards. The implementations are available as open
source2 to allow for the reproducibility of our results and
to enable other practitioners to apply to other systems
and architectures.
• We present the first, publicly-disclosed, detailed measure-
ments of PCIe performance across several generations
of conventional host-systems; this permits us to trace
the evolution of PCIe root complex implementations and
highlight some surprising differences between implemen-
tations even from the same vendor.
• We discuss the lessons we learned alongside a number
of practical use-cases for pcie-bench. From the original
need to characterize, debug, and improve implementa-
tions, to aid and evaluate the growing area of research
of customized solutions enabled by programmable NICs.
Furthermore, the results are also directly applicable to
the growing number of other high-performance such as
specialized accelerator cards.
2 MOTIVATION
Traditionally, I/O interconnects in commodity systems, such
as PCIe, are not widely studied, in part, because they have
worked effectively for many years. It is only as we have I/O
devices that approach (and exceed) the capabilities offered by
PCIe, e.g., dual-port 40Gb/s network adapters, that we have
seen a significant number of hardware-software co-designs3
1The PCIe root complex connects the processors and memory subsystem
of a host system with the PCIe switch fabric to individual PCIe devices. Its
functionality is similar to the PCI host bridge in older systems.
2https://www.pcie-bench.org
3Such I/O-aware software design is hardly new. In the past, when faster
ATM and 1Gb/s PCI bus adapters were introduced, many driver and kernel
redesigns have been proposed [9, 50, 60].
with the constraints of PCIe in mind [16, 29, 30]. Along-
side them, many novel software frameworks have also been
proposed to avoid the overhead of conventional network
stacks [3, 8, 46, 54].
Unfortunately, with 40 and 100Gb/s NICs, PCIe, even in
combination with optimized software stacks, is becoming
the bottleneck. Moreover, modern network applications with
tight latency requirements can be affected by the delays in-
troduced by both the DMA engines in PCIe devices and the
PCIe end-host implementation. Finally, the host-side imple-
mentation of PCIe in modern x86 based servers has been
changing dramatically, and alternative server architectures
are also emerging. We now look at these aspects in more
detail.
Figure 1: Modeled bidirectional bandwidth of a PCIe
Gen 3 x8 link, the achievable throughout of a simplis-
tic NIC and a modern NIC with a typical kernel driver
and a DPDK driver.
PCIe impact on network application throughput. A PCIe
Gen 3 x8 interface, typically used by 40Gb/s NICs, has a
throughput of 62.96 Gb/s at the physical layer. However,
PCIe protocol overheads reduce the usable bandwidth to
around 50 Gb/s, or significantly less, depending on the PCIe
access patterns. Figure 1 shows the effective bi-directional
bandwidth achievable for such a device (Effective PCIe
BW). The saw-tooth pattern is caused by the packetized struc-
ture of the PCIe protocol where the data to be transferred,
e.g., a network packet, is broken up into smaller PCIe packets
with their own PCIe level headers.
Apart from transferring the packet data itself, a NIC also
has to read TX and freelist descriptors, write back RX (and
sometimes TX) descriptors and generate interrupts. Device
drivers also have to read and update queue pointers on the
device. All these interactions are PCIe transactions consum-
ing additional PCIe bandwidth. If we model a very simple
NIC, which DMAs individual descriptors for every packet
and where the driver reads/writes the queue pointers for
328
10152025303540455055025651276810241280Bandwidth(Gb/s)TransferSize(Bytes)EffectivePCIeBW40GEthernetSimpleNICModernNIC(kerneldriver)ModernNIC(DPDKdriver)Understanding PCIe performance for end host networking
SIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
every packet, we can see a dramatic decrease in achievable
bi-directional bandwidth (Simple NIC). Such a device would
only achieve 40 Gb/s line rate throughput for Ethernet frames
larger than 512B. Most modern NICs (and drivers) therefore
implement a number of optimizations, such as en-queuing
descriptors in batches, pre-fetching descriptors, interrupt
moderation, etc. These optimizations increase the complex-
ity of the NIC but are required to achieve acceptable through-
put. Modern NIC (kernel driver) in the figure shows the
throughput of such a moderately optimized NIC/driver com-
bination with a typical Linux kernel driver. Modern software
frameworks, such as DPDK [8], enable further optimizations
at the driver level (no NIC hardware changes). By disabling
interrupts and polling write-back descriptors in host memory
instead of device registers, the number of PCIe transactions
is further reduced and the achievable bandwidth is increased
(Modern NIC (DPDK driver)). We detail how these models
are derived in Section 3. However, Figure 1 shows that when
designing custom offloads to programmable NICs, developers
and researchers have to be acutely aware of the overheads
imposed by PCIe transactions, caused both by the device and
the device driver.
PCIe impact on network application latency. We used
an ExaNIC [11] to estimate the contribution of PCIe to the
overall end-host latency experienced by a network applica-
tion. We executed a loopback test to measure the total NIC
latency (from application to the wire and back) with various
sized packets. The test writes a packet to the driver’s buffer
and measures the latency between when the packet starts
to be written to PCIe and when the packet returns. The test
utilizes kernel bypass mode, so does not include any kernel
overheads. We also modified the ExaNIC firmware, using
Exablaze firmware development kit4, to measure the PCIe
latency contribution.
Figure 2: Measurement of NIC PCIe latency.
Figure 2 shows the results from the test. On the ExaNIC,
the PCIe subsystem contributes between 77% of the latency
for 1500B packets, increasing to more than 90% for small
4http://www.exablaze.com/docs/exanic/user-guide/fdk/fdk/
329
packet sizes. In particular, the overall latency is well within
the range which has been shown to have negative impact on
performance for several common datacenter applications [63].
Measurement of NIC PCIe latency Figure 2 illustrates
that the round trip latency for a 128B payload is around
1000 ns with PCIe contributing around 900 ns. With 40 Gb/s
Ethernet at line rate for 128B packets, a new packet needs
to be received and sent around every 30ns. Assuming that
the measured PCIe latency is symmetric, this implies that
the NIC has to handle at least 30 concurrent DMAs in each
direction to accommodate this latency in order to achieve
line rate for 128B packets. Given that a NIC also has to issue
DMAs for descriptors, the number of in-flight DMAs a NIC
has to handle is even higher. Furthermore, as we show in
§6.2, some systems introduce significant variance in latencies,
which the NIC’s DMA engines also have to accommodate.
This not only introduces significant complexity when writing
optimized software for programmable NICs but also imposes
constraints on the device driver having to supply host buffers
in large enough batches for the NIC to consume.
PCIe root complex is evolving. In modern, x86 based
servers, the PCIe root complex has seen a rapid evolution
in recent years. Specifically, together with the memory con-
troller(s), it has been closely integrated with the CPU cores,
enabling tighter integration of PCIe devices with a CPU’s
caches, e.g., Intel’s Data Direct I/O (DDIO) [20]. For multi-
socket systems, this integration also results in non-uniform
memory access (NUMA [28]) for PCIe devices: Some DMA re-
quests may target memory local to the socket the PCIe device
is attached to while others need to traverse the CPU inter-
connect. Finally, most modern systems have an IOMMU [22]
interposed in the data path between a PCIe device and the
host. The IOMMU performs address-translation for addresses
present in PCIe transactions and utilizes an internal Trans-
action Lookaside Buffer (TLB) as a cache for translated ad-
dresses. On a TLB miss, the IOMMU must perform a full
page table walk, which may delay the transaction and thus
may increase latency and impact throughput. These recent
technologies not only impact the overall latency (and band-
width) of PCIe transactions, but they also introduce variance
as transactions are now depending on the temporal state of
caches, IOMMU TLB and the characteristics of the CPU in-
terconnects. Furthermore, after years of a relative x86 mono-
culture in the server market, designs around ARM64 and
Power processors have all received mainstream deployment.
Each of these architectures have vastly different implementa-
tions of the I/O subsystem and most of the associated details
are not publicly available, making it hard for researchers
to characterize and understand the differences among these
server architectures.
600800100012001400160018002000220024000200400600800100012001400160090.6%84.4%77.2%MedianLatency(ns)TransferSize(Bytes)NICPCIecontributionSIGCOMM ’18, August 20–25, 2018, Budapest, Hungary
R. Neugebauer et al.
The impact of PCIe on the throughput experienced when
data needs to be transferred from hardware to software and
vice versa, the predominant contribution of PCIe to the over-
all NIC latency, the introduction of new technologies in the
PCIe root complex, and the mainstream deployment of new
server architectures have motivated us to build pcie-bench:
to get a better, more detailed and systematic understand-
ing of the PCIe implementation in commodity systems and
to help developers to fine tune applications to make more
efficient use of the underlying system.
3 MODELLING PCIE