(vi) proper labels and groupings. While there exist a few publicly
available malware collections, e.g., VXHeavens, VirusShare, and
theZoo, most of these are not proper datasets and thus lack adequate
labeling, balanced and well distributed types of malware, or recent
samples. For these reasons we decided to analyze an industrial
dataset that contained ground-truth labels.
3.1 Analysis: Industrial Dataset
Engineers at a security company provided us with a labeled dataset
of malware features. The dataset consisted of JSON files repre-
senting the behavior of a malicious sample during execution. To
generate these JSON files, a team collected malicious Windows
Portable Executables that attempted to propagate themselves over
a network. Each malware was then executed within a Cuckoo Sand-
box to extract sets of features. To group the malware into families,
labels were constructed using Suricata network intrusion detection
(NID) signatures. All malicious samples that triggered the same
signature were grouped into the same family. Upon receiving the
dataset, a few steps had to be taken to clean the data. First, several
malware JSON files contained no usable data within them and were
removed. Next, we discovered that many samples were classified in
two or more groups, so these samples needed to be removed also.
After filtering, we were left with 5,673 usable samples. Discarding
problematic samples caused group sizes to become highly unbal-
anced. The largest family contained 772 samples while the smallest
groups only contained 1, and the mode of all groups was merely
3 samples. Once the dataset was cleaned, we grouped the samples
using multiple clustering techniques to observe if we could repro-
duce the provided family groupings [10, 11]. In forming clusters, we
tested several different groupings of malware features, including
system and network interactions, to observe which combinations
produce the best results.
3.2 Issues with Malware Labeling
An important issue is the difficulties in producing proper ground-
truth labels for malware. Often it may be necessary to identify or
group malware based on families of similar samples. Unfortunately,
labeling malware into distinguishable groups can be a difficult
task. Most anti-virus vendors strongly disagree with how strands
of malware should be labeled and often produce groupings with
vastly different levels of specificity [4]. Due to this difficulty, most
malware datasets do not come with classification labels or a ground-
truth with which to compare results. To work around this, many
researchers have implemented anti-virus majority voting systems
to construct labels. This technique calls for only using malware
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2606samples that have the same label across a majority of anti-virus
vendors. However, this method can have multiple drawbacks. First,
it can significantly reduce the number of samples within a dataset as
most malware do not have universal labels. Out of 14,212 malicious
samples in the ANUBIS dataset, only 2,658 were deemed usable
through anti-virus voting [6]. Furthermore, only using malware
whose labels are universally agreed on, may bias towards positive
results as a model could potentially only be classifying malware
that are easiest to group or distinguish [13]. Efforts to produce
standardized malware classification labels are ongoing, e.g., MAEC
[12]. Until standardized methods are produced, manual labeling
may be necessary to accurately test the performance of a model.
4 CASE STUDY: INTRUSION DATASETS
For intrusion detection, we analyzed the CICIDS2017 dataset since
the KDD Cup/DARPA dataset is quite old and has been analyzed
already [8]. The CICIDS2017 dataset is a synthetic dataset con-
sisting of a complete capture of all send and receive traffic from
the main switch of the Victim Network [15]. It contains both raw
packet capture files and 3119345 network flows analyzed by their
CICFlowMeter and labeled by attack type. In our exploration of the
network flow information, we noticed several issues:
Missing Information. The dataset contains 288602 completely
empty records and 1358 instances that are missing the number of
bytes sent. We removed these empty/incomplete instances.
Duplicates. We found 202 duplicate instances in the dataset. Of
these duplicates, 201 are benign and 1 is a DoS Hulk attack. If we
ignore the timestamp, then the number of duplicates increases to
12981. Of these 12981 duplicates, 5393 instances are benign, 7561
instances are DoS Hulk, and 27 are DoS slowloris.
Attack Diversity. After removing instances with missing informa-
tion and duplicates not ignoring timestamps, the dataset contained
2829183 instances. Of these instances, 80.32% were labeled benign,
and the rest were spread out over the various attacks. Moreover,
even amongst the attacks, we see a skewed distribution. The three
most common attacks constitute 92.87% of the attack instances, and
the least common attacks have less than 50 instances each.
Dataset Difficulty. As an indirect measure of dataset difficulty, we
perform a small data experiment, where we used a Decision Tree
classifier to identify malicious traffic with only a randomly selected
training set of 0.1% (2828 instances) of the dataset. Our test set
consisted of 100000 randomly selected instances, which were not
used in the training set. Over 10 iterations, we managed to achieve
a mean accuracy of 92.88%.
5 RELATED WORK
The DBLP query, security data quality, produced 35 results over the
period 1994-2015. However, the relevant papers numbered fewer
than 10. We repeated the query on ACM Digital Library (DL),
Google Scholar (allintitle query), and IEEE Xplore. Many of the
retrieved results from DBLP, DL, and Xplore had security in the
title of the journal or conference (e.g., Journal of Computer Security,
etc.) A summary of the most relevant work is below.
A nice taxonomy of data quality terms (“dimensions”) is pre-
sented in [18]. In [14], the goal was to integrate security and accu-
racy into data quality evaluation. Data quality challenges in sharing
threat intelligence were discussed in [16]. A general survey is [5].
6 CONCLUSIONS
Through three different case studies we have illustrated the issues
with datasets for security challenges. We have shown how to find
them and how to address them. We defined dataset difficulty as a
measure of dataset quality. Much remains to be done, for example,
when to stop cleaning. One possibility is to see if dataset sources or
other key metadata can be identified accurately based on the data.
ACKNOWLEDGMENTS
Research supported in part by NSF grants CNS 1319212, DGE
1433817, DUE 1356705 and IIS 1659755.
REFERENCES
[1] Ayman El Aassal, Shahryar Baki, Avisha Das, and Rakesh M. Verma. 2019. An
In-Depth Benchmarking Evaluation of Phishing Detection Research for Security
Needs. (2019). To be submitted.
[2] Ayman El Aassal, Luis Moraes, Shahryar Baki, Avisha Das, and Rakesh Verma.
2018. Anti-Phishing Pilot at ACM IWSPA 2018: Evaluating Performance with
New Metrics for Unbalanced Datasets. In Proc. of IWSPA-AP Pilot. CEUR, 2–10.
[3] Idan Amit, John Matherly, William Hewlett, Zhi Xu, Yinnon Meshi, and Yigal
Weinberger. 2018. Machine Learning in Cyber-Security - Problems, Challenges
and Data Sets. (Dec. 2018). Online.
[4] Michael Bailey, Jon Oberheide, Jon Andersen, Z Morley Mao, Farnam Jahanian,
and Jose Nazario. 2007. Automated classification and analysis of internet malware.
In RAID. Springer, 178–197.
[5] Carlo Batini, Cinzia Cappiello, Chiara Francalanci, and Andrea Maurino. 2009.
Methodologies for Data Quality Assessment and Improvement. ACM Comput.
Surv. 41, 3 (July 2009), 16:1–16:52.
[6] Ulrich Bayer, Paolo Milani Comparetti, Clemens Hlauschek, Christopher Kruegel,
and Engin Kirda. 2009. Scalable, behavior-based malware clustering.. In NDSS,
Vol. 9. USENIX, 8–11.
[7] Avisha Das, Shahryar Baki, Ayman El Aassal, Rakesh Verma, and Arthur Dunbar.
[n. d.]. SoK: A Comprehensive Reexamination of Phishing Research from the
Security Perspective. ([n. d.]). Under review.
[8] Abhishek Divekar, Meet Parekh, Vaibhav Savla, Rudra Mishra, and Mahesh
Shirole. 2018. Benchmarking datasets for Anomaly-based Network Intrusion
Detection: KDD CUP 99 alternatives. (2018). Online.
[9] Y. Fang, C. Zhang, C. Huang, L. Liu, and Y. Yang. 2019. Phishing Email Detection
Using Improved RCNN Model With Multilevel Vectors and Attention Mechanism.
IEEE Access 7 (2019), 56329–56340. https://doi.org/10.1109/ACCESS.2019.2913705
[10] H. Faridi, S. Srinivasagopalan, and R. Verma. 2018. Performance Evaluation of
Features and Clustering Algorithms for Malware. IEEE ICDMW (ADMiS) (2018).
[11] H. Faridi, S. Srinivasagopalan, and R. Verma. 2019. Parameter Tuning and Confi-
dence Limits of Malware Clustering. In Proc. 9th CODASPY. ACM, 169–171.
[12] Ivan Kirillov, Desiree Beck, Penny Chase, and Robert Martin. 2011. Malware
attribute enumeration and characterization. (2011).
[13] Peng Li, Limin Liu, Debin Gao, and Michael K Reiter. 2010. On challenges in
evaluating malware clustering. In RAID. Springer, 238–255.
[14] Leon Reznik and Elisa Bertino. 2013. POSTER: Data Quality Evaluation: Integrat-
ing Security and Accuracy. In Proc. of CCS (CCS ’13). ACM, 1367–1370.
[15] Iman Sharafaldin, Arash Habibi Lashkari, and Ali A Ghorbani. 2018. Toward Gen-
erating a New Intrusion Detection Dataset and Intrusion Traffic Characterization.
In ICISSP. SciTePress, 108–116.
[16] Christian Sillaber, Clemens Sauerwein, Andrea Mussmann, and Ruth Breu. 2016.
Data Quality Challenges and Future Research Directions in Threat Intelligence
Sharing Practice. In Proc. Workshop on Information Sharing and Collaborative
Security (WISCS ’16). ACM, 65–70.
[17] Robin Sommer and Vern Paxson. 2010. Outside the Closed World: On Using
Machine Learning for Network Intrusion Detection. In 31st IEEE Symp. on Security
and Privacy, S&P 2010, 16-19 May 2010. IEEE, 305–316.
[18] Gurvirender Tejay, Gurpreet Dhillon, and Amita Goyal Chin. 2005. Data Quality
Dimensions for Information Systems Security: A Theoretical Exposition (Invited
Paper). In Security Management, Integrity, and Internal Control in Information
Systems. Springer US, Boston, MA, 21–39.
[19] Rakesh Verma and Nabil Hossain. 2014. Semantic Feature Selection for Text with
Application to Phishing Email Detection. In Information Security and Cryptology
– ICISC 2013, Hyang-Sook Lee and Dong-Guk Han (Eds.). Springer International
Publishing, Cham, 455–468.
[20] R.M. Verma, M. Kantarcioglu, D.J. Marchette, E.L. Leiss, and T. Solorio. 2015.
Security Analytics: Essential Data Analytics Knowledge for Cybersecurity Pro-
fessionals and Students. IEEE Security & Privacy 13, 6 (2015), 60–65.
PosterCCS ’19, November 11–15, 2019, London, United Kingdom2607