title:Hybrid Batch Attacks: Finding Black-box Adversarial Examples with
Limited Queries
author:Fnu Suya and
Jianfeng Chi and
David Evans and
Yuan Tian
Hybrid Batch Attacks: Finding Black-box Adversarial 
Examples with Limited Queries
Fnu Suya, Jianfeng Chi, David Evans, and Yuan Tian, University of Virginia
https://www.usenix.org/conference/usenixsecurity20/presentation/suya
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.In 29th USENIX Security Symposium, August 2020 (Accepted: June 2019; This version: December 2, 2019)
Hybrid Batch Attacks: Finding Black-box
Adversarial Examples with Limited Queries
Fnu Suya, Jianfeng Chi, David Evans, Yuan Tian
University of Virginia
Abstract
We study adversarial examples in a black-box setting where
the adversary only has API access to the target model and
each query is expensive. Prior work on black-box adversarial
examples follows one of two main strategies: (1) transfer at-
tacks use white-box attacks on local models to ﬁnd candidate
adversarial examples that transfer to the target model, and (2)
optimization-based attacks use queries to the target model and
apply optimization techniques to search for adversarial exam-
ples. We propose hybrid attacks that combine both strategies,
using candidate adversarial examples from local models as
starting points for optimization-based attacks and using labels
learned in optimization-based attacks to tune local models for
ﬁnding transfer candidates. We empirically demonstrate on
the MNIST, CIFAR10, and ImageNet datasets that our hybrid
attack strategy reduces cost and improves success rates. We
also introduce a seed prioritization strategy which enables
attackers to focus their resources on the most promising seeds.
Combining hybrid attacks with our seed prioritization strat-
egy enables batch attacks that can reliably ﬁnd adversarial
examples with only a handful of queries.
1 Introduction
Machine learning (ML) models are often prone to misclas-
sifying inputs, known as adversarial examples (AEs), that
are crafted by perturbing a normal input in a constrained,
but purposeful way. Eﬀective methods for ﬁnding adversarial
examples have been found in white-box settings, where an ad-
versary has full access to the target model [8,17,24,32,39], as
well as in black-box settings, where only API access is avail-
able [10, 21, 22, 36, 38, 43]. In this work, we aim to improve
our understanding of the expected cost of black-box attacks in
realistic settings. For most scenarios where the target model
is only available through an API, the cost of attacks can be
quantiﬁed by the number of model queries needed to ﬁnd a
desired number of adversarial examples. Black-box attacks
often require a large number of model queries, and each query
takes time to execute, in addition to incurring a service charge
and exposure risk to the attacker.
Previous black-box attacks can be grouped into two cat-
egories: transfer attacks [35, 36] and optimization attacks
[10, 21, 22, 38, 43]. Transfer attacks exploit the observation
that adversarial examples often transfer between diﬀerent
models [17, 27, 29, 36, 41]. The attacker generates adversar-
ial examples against local models using white-box attacks,
and hopes they transfer to the target model. Transfer attacks
use one query to the target model for each attempted candi-
date transfer, but suﬀer from transfer loss as local adversarial
examples may not successfully transfer to the target model.
Transfer loss can be very high, especially for targeted attacks
where the attacker’s goal requires ﬁnding examples where
the model outputs a particular target class rather than just
producing misclassiﬁcations.
Optimization attacks formulate the attack goal as a black-
box optimization problem and carry out the attack using a
series of queries to the target model [1, 4, 10, 18, 21, 22, 28, 33,
43]. These attacks require many queries, but do not suﬀer from
transfer loss as each seed is attacked interactively using the
target model. Optimization-based attacks can have high attack
success rates, even for targeted attacks, but often require many
queries for each adversarial example found.
Contributions. Although improving query eﬃciency and at-
tack success rates for black-box attacks is an active area of
research for both transfer-based and optimization-based at-
tacks, prior works treat the two types of attacks independently
and fail to explore possible connections between the two ap-
proaches. We investigate three straightforward possibilities
for combining transfer and optimization-based attacks (Sec-
tion 3), and ﬁnd that only one is generally useful (Section 4):
failed transfer candidates are useful starting points for opti-
mization attacks. This can be used to substantially improve
black-box attacks in terms of both success rates and, most
importantly, query cost. Compared to transfer attacks, hybrid
attacks can signiﬁcantly improve the attack success rate by
adopting optimization attacks for the non-transfers, which
increases per-sample query cost. Compared to optimization
attacks, hybrid attacks signiﬁcantly reduce query complexity
USENIX Association
29th USENIX Security Symposium    1327
ARTIFACTEVALUATEDPASSEDwhen useful local models are available. For example, for both
MNIST and CIFAR10, our hybrid attacks reduce the mean
query cost of attacking normally-trained models by over 75%
compared to state-of-the-art optimization attacks. For Image-
Net, the transfer attack only has 3.4% success rate while the
hybrid attack approaches 100% success rate.
To improve our understanding of resource-limited black-
box attacks, we simulate a batch attack scenario where the
attacker has access to a large pool of seeds and is motivated to
obtain many adversarial examples using limited resources. Al-
ternatively, we can view the batch attacker’s goal as obtaining
a ﬁxed number of adversarial examples with fewest queries.
We demonstrate that the hybrid attack can be combined with
a novel seed prioritization strategy to dramatically reduce the
number of queries required in batch attacks (Section 5). For
example, for ImageNet, when the attacker is interested in ob-
taining 10 adversarial examples from a pool of 100 candidate
seeds, our seed prioritization strategy can be used to save over
70% of the queries compared to random ordering of the seeds.
2 Background and Related Work
In this section, we overview the two main types of black-box
attacks which are combined in our hybrid attack strategies.
2.1 Transfer Attacks
Transfer attacks take advantage of the observation that ad-
versarial examples often transfer across models. The attacker
runs standard white-box attacks on local models to ﬁnd ad-
versarial examples that are expected to transfer to the tar-
get model. Most works assume the attacker has access to
similar training data to the data used for the target model,
or has access to pretrained models for similar data distribu-
tion. For attackers with access to pretrained local models, no
queries are needed to the target model to train the local mod-
els. Other works consider training a local model by querying
the target model, sometimes referred to as substitute train-
ing [27, 36]. With naïve substitute training, many queries are
needed to train a useful local model. Papernot et al. adopt a
reservoir sampling approach to reduce the number of queries
needed [36]. Li et al. use active learning to further reduce
the query cost [27]. However, even with these improvements,
many queries are still needed and substitute training has had
limited eﬀectiveness for complex target models.
Although adversarial examples sometimes transfer between
models, transfer attacks typically have much lower success
rates than optimization attacks, especially for targeted attacks.
In our experiments on ImageNet, the highest transfer rate of
targeted attacks observed from a single local model is 0.2%,
while gradient-based attacks achieve nearly 100% success.
Liu et al. improve transfer rates by using an ensemble of local
models [29], but still only achieve low transfer rates (3.4% in
our ImageNet experiments, see Table 3).
Another line of work aims to improve transferability by
modifying the white-box attacks on the local models. Dong et
al. adopt the momentum method to boost the attack process
and leads to improved transferability [15]. Xie et al. improve
the diversity of attack inputs by considering image transfor-
mations in the attack process to improve transferability of
existing white-box attacks [45]. Dong et al. recently proposed
a translation invariant optimization method that further im-
proves transferability [16]. We did not incorporate these meth-
ods in our experiments, but expect they would be compatible
with our hybrid attacks.
2.2 Optimization Attacks
Optimization-based attacks work by deﬁning an objective
function and iteratively perturbing the input to optimize that
objective function. We ﬁrst consider optimization attacks
where the query response includes full prediction scores, and
categorize those ones that involve estimating the gradient of
the objective function using queries to the target model, and
those that do not depend on estimating gradients. Finally, we
also brieﬂy review restricted black-box attacks, where attack-
ers obtain even less information from each model query, in the
extreme, learning just the label prediction for the test input.
Gradient Attacks. Gradient-based black-box attacks numer-
ically estimate the gradient of the target model, and execute
standard white-box attacks using those estimated gradients.
Table 1 compares several gradient black-box attacks.
The ﬁrst attack of this type was the ZOO (zeroth-order
optimization) attack, introduced by Chen et al. [10]. It adopts
the ﬁnite-diﬀerence method with dimension-wise estimation
to approximate gradient values, and uses them to execute a
Carlini-Wagner (CW) white-box attack [8]. The attack runs
for hundreds to thousands of iterations and takes 2D queries
per CW optimization iteration, where D is the dimensionality.
Hence, the query cost is extremely high for larger images (e.g.,
over 2M queries on average for ImageNet).
Following this work, several researchers have sought more
query-eﬃcient methods for estimating gradients for executing
black-box gradient attacks. Bhagoji et al. propose reducing
query cost of dimension-wise estimation by randomly group-
ing features or estimating gradients along with the principal
components given by principal component analysis (PCA) [4].
Tu et al.’s AutoZOOM attack uses two-point estimation based
on random vectors and reduces the query complexity per CW
iteration from 2D to 2 without losing much accuracy on es-
timated gradients [43]. Ilyas et al.’s NES attack [21] uses a
natural evolution strategy (which is in essence still random
vector-based gradient estimation) [44], to estimate the gradi-
ents for use in projected gradient descent (PGD) attacks [32].
Ilyas et al.’s BanditsTD attack incorporates time and data
dependent information into the NES attack [22]. Al-Dujaili et
al.’s SignHunter adopts a divide-and-conquer approach to es-
timate the sign of the gradient and is empirically shown to be
superior to the BanditsTD attack in terms of query eﬃciency
1328    29th USENIX Security Symposium
USENIX Association
Attack
ZOO [10]
Bhagoji et. al [4]
AutoZOOM [43]
NES [21]
BanditsTD [22]
SignHunter [1]
Cheng et al. [13]
Gradient Estimation
Queries per Iteration
White-box Attack
δ
ˆg = {ˆgi, ˆg2, ..., ˆgD}, ˆgi ≈ f (x+δei)− f (x−δei)
ZOO + random feature group or PCA
(cid:80)N
ui ∼ U, ˆg = 1
ui ∼ N(0,I), ˆg = 1
NES + time/data dependent info
f (x+δui)− f (x)
δ
f (x+δui)
(cid:80)N
ui
ui
N
N
δ
i
i
Gradient sign w/ divide-and-conquer method
(I−vvT )ui
ui ∼ U, ˆg = 1
||(I−vvT )ui||2
√
λ· v +
1− λ·
i (
√
N
)
(cid:80)N
2D
≤ 2D
N + 1
N
N
2(cid:100)log(D)+1(cid:101)
N
CW [8]
FGSM [17], PGD [32]
CW [8]
PGD
PGD
PGD
PGD
Table 1: Gradient attacks. These attacks use some method to estimate gradients and then leverage white-box attacks. D is data
dimension, ei denotes standard basis, N is the number of gradient averages. f (x) denotes prediction conﬁdence of image x: for
targeted attacks, it denotes the conﬁdence of target class; for untargeted attacks, it denotes the conﬁdence of original class. δ is a
small constant. v is the local model gradient. λ is a constant controlling the strength of local and target model gradients.
Attack
Applicable Norm
Objective Function
Solution Method
Sim-BA [18]
NAttack [28]
Moon et al. [43]
L2, L∞
L2, L∞
L∞
f (x(cid:48))
(cid:82)
maxS⊆V f (x + (cid:80)
min
x(cid:48)
l(x(cid:48))π(x(cid:48)|θ)dx(cid:48)
i∈S ei − (cid:80)
min
θ
Iterate: sample q from Q, ﬁrst try q, then −q
Compute θ∗, then sample from π(x(cid:48) | θ∗)
i(cid:60)S ei) Compute S∗, then x + (cid:80)
i∈S∗ ei − (cid:80)
i(cid:60)S∗ ei
Table 2: Gradient-free attacks. These attacks deﬁne an objective function and obtain the AE by solving the optimization problem.
Q denotes a set of orthonormal candidate vectors, l(x(cid:48)) denotes the cross-entropy loss of image x(cid:48)
with original label (untargeted
attack) or target label (targeted attack). π(x(cid:48)|θ) denotes the distribution of x(cid:48) parameterized by θ, V denotes ground set of all
pixel locations. Variables with ∗ are locally-optimal solutions obtained by solving the corresponding optimization problems.
and attack success rate [1]. Cheng et al. recently proposed im-
proving the BanditsTD attack by incorporating gradients from
surrogate models as priors when estimating the gradients [13].
For our experiments (Section 4.2), we use AutoZOOM and
NES as representative state-of-the-art black-box attacks.1
Gradient-free Attacks. Researchers have also explored
search-based black-box attacks using heuristic methods that
are not based on gradients, which we call gradient-free at-
tacks. One line of work directly applies known heuristic
black-box optimization techniques, and is not competitive
with the gradient-based black-box attacks in terms of query
eﬃciency. Alzantot et al. [2] develop a genetic programming
strategy, where the ﬁtness function is deﬁned similarly to CW
loss [8], using the prediction scores from queries to the black-
1We also tested BanditsTD on ImageNet, but found it less competitive to
the earlier attacks and therefore, do not include the results in this paper. We
have not evaluated SignHunter and the attack of Cheng et al. [13], but plan to
include more results in the future versions and have released an open-source
framework to enable other attacks to be tested using our methods.
box model. A similar genetic programming strategy was used
to perform targeted black-box attacks on audio systems [40].
Narodytska et al. [34] use a local neighbor search strategy,
where each iteration perturbs the most signiﬁcant pixel. Since
the reported query eﬃciency of these methods is not com-
petitive with results for gradient-based attacks, we did not
consider these attacks in our experiments.
Several recent gradient-free black-box attacks (summarized
in Table 2) have been proposed that can signiﬁcantly outper-
form the gradient-based attacks. Guo et al.’s Sim-BA [18]
iteratively adds or subtracts a random vector sampled from
a predeﬁned set of orthonormal candidate vectors to gener-
ate adversarial examples eﬃciently. Li et al.’s NAttack [28]
formulates the adversarial example search process as identi-