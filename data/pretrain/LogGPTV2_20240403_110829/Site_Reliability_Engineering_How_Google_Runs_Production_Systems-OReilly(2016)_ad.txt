The Chubby[Bur06] lock service provides a filesystem-like API for maintaining locks. Chubby handles these locks across datacenter locations. It uses the Paxos pro‐tocol for asynchronous Consensus (see Chapter 23).Chubby also plays an important role in master election. When a service has five repli‐cas of a job running for reliability purposes but only one replica may perform actual work, Chubby is used to select which replica may proceed.
Data that must be consistent is well suited to storage in Chubby. For this reason, BNS uses Chubby to store mapping between BNS paths and IP address:port pairs.Monitoring and Alerting
We want to make sure that all services are running as required. Therefore, we run many instances of our Borgmon monitoring program (see Chapter 10). Borgmon reg‐ularly “scrapes” metrics from monitored servers. These metrics can be used instanta‐neously for alerting and also stored for use in historic overviews (e.g., graphs). We can use monitoring in several ways:18  |  Chapter 2: The Production Environment at Google, from the Viewpoint of an SRE
• Set up alerting for acute problems.
• Compare behavior: did a software update make the server faster?
• Examine how resource consumption behavior evolves over time, which is essen‐	tial for capacity planning.
Our Software InfrastructureOur Software Infrastructure
Our software architecture is designed to make the most efficient use of our hardware infrastructure. Our code is heavily multithreaded, so one task can easily use many cores. To facilitate dashboards, monitoring, and debugging, every server has an HTTP server that provides diagnostics and statistics for a given task.All of Google’s services communicate using a Remote Procedure Call (RPC) infra‐structure named Stubby; an open source version, gRPC, is available.3 Often, an RPC call is made even when a call to a subroutine in the local program needs to be per‐formed. This makes it easier to refactor the call into a different server if more modu‐larity is needed, or when a server’s codebase grows. GSLB can load balance RPCs in the same way it load balances externally visible services.A server receives RPC requests from its frontend and sends RPCs to its backend. In traditional terms, the frontend is called the client and the backend is called the server.
Data is transferred to and from an RPC using protocol buffers,4 often abbreviated to“protobufs,” which are similar to Apache’s Thrift. Protocol buffers have many advan‐tages over XML for serializing structured data: they are simpler to use, 3 to 10 times smaller, 20 to 100 times faster, and less ambiguous.Our Development Environment
Development velocity is very important to Google, so we’ve built a complete develop‐ment environment to make use of our infrastructure [Mor12b].
Apart from a few groups that have their own open source repositories (e.g., Android and Chrome), Google Software Engineers work from a single shared repository [Pot16]. This has a few important practical implications for our workflows:3 See .
4 Protocol buffers are a language-neutral, platform-neutral extensible mechanism for serializing structured 	data. For more details, see /.
Our Software Infrastructure  |  19
• If engineers encounter a problem in a component outside of their project, they can fix the problem, send the proposed changes (“changelist,” or CL) to the owner for review, and submit the CL to the mainline.• Changes to source code in an engineer’s own project require a review. All soft‐	ware is reviewed before being submitted.When software is built, the build request is sent to build servers in a datacenter. Even large builds are executed quickly, as many build servers can compile in parallel. This infrastructure is also used for continuous testing. Each time a CL is submitted, tests run on all software that may depend on that CL, either directly or indirectly. If the framework determines that the change likely broke other parts in the system, it noti‐fies the owner of the submitted change. Some projects use a push-on-green system, where a new version is automatically pushed to production after passing tests.Shakespeare: A Sample Service
To provide a model of how a service would hypothetically be deployed in the Google production environment, let’s look at an example service that interacts with multiple Google technologies. Suppose we want to offer a service that lets you determine where a given word is used throughout all of Shakespeare’s works.
We can divide this system into two parts:• A batch component that reads all of Shakespeare’s texts, creates an index, and writes the index into a Bigtable. This job need only run once, or perhaps very infrequently (as you never know if a new text might be discovered!).
• An application frontend that handles end-user requests. This job is always up, as 	users in all time zones will want to search in Shakespeare’s books.The batch component is a MapReduce comprising three phases.
The mapping phase reads Shakespeare’s texts and splits them into individual words. This is faster if performed in parallel by multiple workers.
The shuffle phase sorts the tuples by word.
In the reduce phase, a tuple of (word, list of locations) is created.
Each tuple is written to a row in a Bigtable, using the word as the key.20  |  Chapter 2: The Production Environment at Google, from the Viewpoint of an SRE
Life of a Request
Figure 2-4 shows how a user’s request is serviced: first, the user points their browser to shakespeare.google.com. To obtain the corresponding IP address, the user’s device resolves the address with its DNS server (1). This request ultimately ends up at Goo‐gle’s DNS server, which talks to GSLB. As GSLB keeps track of traffic load among frontend servers across regions, it picks which server IP address to send to this user.Figure 2-4. The life of a request
The browser connects to the HTTP server on this IP. This server (named the Google Frontend, or GFE) is a reverse proxy that terminates the TCP connection (2). The GFE looks up which service is required (web search, maps, or—in this case—Shake‐speare). Again using GSLB, the server finds an available Shakespeare frontend server, and sends that server an RPC containing the HTML request (3).The Shakespeare server analyzes the HTML request and constructs a protobuf con‐taining the word to look up. The Shakespeare frontend server now needs to contact the Shakespeare backend server: the frontend server contacts GSLB to obtain the BNS address of a suitable and unloaded backend server (4). That Shakespeare backend server now contacts a Bigtable server to obtain the requested data (5).The answer is written to the reply protobuf and returned to the Shakespeare backend server. The backend hands a protobuf containing the results to the Shakespeare frontend server, which assembles the HTML and returns the answer to the user.This entire chain of events is executed in the blink of an eye—just a few hundred mil‐liseconds! Because many moving parts are involved, there are many potential points of failure; in particular, a failing GSLB would wreak havoc. However, Google’s policies of rigorous testing and careful rollout, in addition to our proactive error recovery
Shakespeare: A Sample Service  |  21Shakespeare: A Sample Service  |  21
methods such as graceful degradation, allow us to deliver the reliable service that our users have come to expect. After all, people regularly use www.google.com to check if their Internet connection is set up correctly.
Job and Data OrganizationJob and Data Organization
Load testing determined that our backend server can handle about 100 queries per second (QPS). Trials performed with a limited set of users lead us to expect a peak load of about 3,470 QPS, so we need at least 35 tasks. However, the following consid‐erations mean that we need at least 37 tasks in the job, or N + 2:
• During updates, one task at a time will be unavailable, leaving 36 tasks.• A machine failure might occur during a task update, leaving only 35 tasks, just 	enough to serve peak load.5A closer examination of user traffic shows our peak usage is distributed globally: 1,430 QPS from North America, 290 from South America, 1,400 from Europe and Africa, and 350 from Asia and Australia. Instead of locating all backends at one site, we distribute them across the USA, South America, Europe, and Asia. Allowing for N + 2 redundancy per region means that we end up with 17 tasks in the USA, 16 in Europe, and 6 in Asia. However, we decide to use 4 tasks (instead of 5) in South America, to lower the overhead of N + 2 to N + 1. In this case, we’re willing to toler‐ate a small risk of higher latency in exchange for lower hardware costs: if GSLB redi‐rects traffic from one continent to another when our South American datacenter is over capacity, we can save 20% of the resources we’d spend on hardware. In the larger regions, we’ll spread tasks across two or three clusters for extra resiliency.Because the backends need to contact the Bigtable holding the data, we need to also design this storage element strategically. A backend in Asia contacting a Bigtable in the USA adds a significant amount of latency, so we replicate the Bigtable in each region. Bigtable replication helps us in two ways: it provides resilience should a Bigtable server fail, and it lowers data-access latency. While Bigtable only offers even‐tual consistency, it isn’t a major problem because we don’t need to update the contents often.We’ve introduced a lot of terminology here; while you don’t need to remember it all, it’s useful for framing many of the other systems we’ll refer to later.
5 We assume the probability of two simultaneous task failures in our environment is low enough to be negligi‐ble. Single points of failure, such as top-of-rack switches or power distribution, may make this assumption invalid in other environments.22  |  Chapter 2: The Production Environment at Google, from the Viewpoint of an SRE
PART II
Principles
This section examines the principles underlying how SRE teams typically work—the patterns, behaviors, and areas of concern that influence the general domain of SRE operations.The first chapter in this section, and the most important piece to read if you want to attain the widest-angle picture of what exactly SRE does, and how we reason about it, is Chapter 3, Embracing Risk. It looks at SRE through the lens of risk—its assessment, management, and the use of error budgets to provide usefully neutral approaches to service management.Service level objectives are another foundational conceptual unit for SRE. The indus‐try commonly lumps disparate concepts under the general banner of service level agreements, a tendency that makes it harder to think about these concepts clearly. Chapter 4, Service Level Objectives, attempts to disentangle indicators from objectives from agreements, examines how SRE uses each of these terms, and provides some recommendations on how to find useful metrics for your own applications.Eliminating toil is one of SRE’s most important tasks, and is the subject of Chapter 5, Eliminating Toil. We define toil as mundane, repetitive operational work providing no enduring value, which scales linearly with service growth.Whether it is at Google or elsewhere, monitoring is an absolutely essential compo‐nent of doing the right thing in production. If you can’t monitor a service, you don’t know what’s happening, and if you’re blind to what’s happening, you can’t be reliable. Read Chapter 6, Monitoring Distributed Systems, for some recommendations for what and how to monitor, and some implementation-agnostic best practices.In Chapter 7, The Evolution of Automation at Google, we examine SRE’s approach to automation, and walk through some case studies of how SRE has implemented auto‐mation, both successfully and unsuccessfully.
Most companies treat release engineering as an afterthought. However, as you’ll learn in Chapter 8, Release Engineering, release engineering is not just critical to overall sys‐tem stability—as most outages result from pushing a change of some kind. It is also the best way to ensure that releases are consistent.A key principle of any effective software engineering, not only reliability-oriented engineering, simplicity is a quality that, once lost, can be extraordinarily difficult to recapture. Nevertheless, as the old adage goes, a complex system that works necessar‐ily evolved from a simple system that works. Chapter 9, Simplicity, goes into this topic in detail.
Further Reading from Google SREFurther Reading from Google SRE
Increasing product velocity safely is a core principle for any organization. In “Making Push On Green a Reality” [Kle14], published in October 2014, we show that taking humans out of the release process can paradoxically reduce SREs’ toil while increasing system reliability.
CHAPTER 3
Embracing Risk
Written by Marc Alvidrez 
Edited by Kavita GulianiYou might expect Google to try to build 100% reliable services—ones that never fail. It turns out that past a certain point, however, increasing reliability is worse for a ser‐vice (and its users) rather than better! Extreme reliability comes at a cost: maximizing stability limits how fast new features can be developed and how quickly products can be delivered to users, and dramatically increases their cost, which in turn reduces the numbers of features a team can afford to offer. Further, users typically don’t notice the difference between high reliability and extreme reliability in a service, because the user experience is dominated by less reliable components like the cellular network or the device they are working with. Put simply, a user on a 99% reliable smartphone cannot tell the difference between 99.99% and 99.999% service reliability! With this in mind, rather than simply maximizing uptime, Site Reliability Engineering seeks to balance the risk of unavailability with the goals of rapid innovation and efficient ser‐vice operations, so that users’ overall happiness—with features, service, and perfor‐mance—is optimized.Managing Risk
Unreliable systems can quickly erode users’ confidence, so we want to reduce the chance of system failure. However, experience shows that as we build systems, cost does not increase linearly as reliability increments—an incremental improvement in reliability may cost 100x more than the previous increment. The costliness has two dimensions:
25
The cost of redundant machine/compute resourcesThe cost associated with redundant equipment that, for example, allows us to take systems offline for routine or unforeseen maintenance, or provides space for us to store parity code blocks that provide a minimum data durability guarantee.
The opportunity cost 
The cost borne by an organization when it allocates engineering resources to build systems or features that diminish risk instead of features that are directly visible to or usable by end users. These engineers no longer work on new features and products for end users.In SRE, we manage service reliability largely by managing risk. We conceptualize risk as a continuum. We give equal importance to figuring out how to engineer greater reliability into Google systems and identifying the appropriate level of tolerance for the services we run. Doing so allows us to perform a cost/benefit analysis to deter‐mine, for example, where on the (nonlinear) risk continuum we should place Search, Ads, Gmail, or Photos. Our goal is to explicitly align the risk taken by a given service with the risk the business is willing to bear. We strive to make a service reliable enough, but no more reliable than it needs to be. That is, when we set an availability target of 99.99%,we want to exceed it, but not by much: that would waste opportuni‐ties to add features to the system, clean up technical debt, or reduce its operational costs. In a sense, we view the availability target as both a minimum and a maximum.The key advantage of this framing is that it unlocks explicit, thoughtful risktaking.
Measuring Service RiskAs standard practice at Google, we are often best served by identifying an objective metric to represent the property of a system we want to optimize. By setting a target, we can assess our current performance and track improvements or degradations over time. For service risk, it is not immediately clear how to reduce all of the potential factors into a single metric. Service failures can have many potential effects, including user dissatisfaction, harm, or loss of trust; direct or indirect revenue loss; brand or reputational impact; and undesirable press coverage. Clearly, some of these factors are very hard to measure. To make this problem tractable and consistent across many types of systems we run, we focus on unplanned downtime.For most services, the most straightforward way of representing risk tolerance is in terms of the acceptable level of unplanned downtime. Unplanned downtime is cap‐tured by the desired level of service availability, usually expressed in terms of the number of “nines” we would like to provide: 99.9%, 99.99%, or 99.999% availability. Each additional nine corresponds to an order of magnitude improvement toward 100% availability. For serving systems, this metric is traditionally calculated based on the proportion of system uptime (see Equation 3-1).26  |  Chapter 3: Embracing Risk
Equation 3-1. Time-based availability
availability = 	uptime + downtime 	uptime
Using this formula over the period of a year, we can calculate the acceptable number of minutes of downtime to reach a given number of nines of availability. For example, a system with an availability target of 99.99% can be down for up to 52.56 minutes in a year and stay within its availability target; see Appendix A for a table.At Google, however, a time-based metric for availability is usually not meaningful because we are looking across globally distributed services. Our approach to fault iso‐lation makes it very likely that we are serving at least a subset of traffic for a given service somewhere in the world at any given time (i.e., we are at least partially “up” at all times). Therefore, instead of using metrics around uptime, we define availability in terms of the request success rate. Equation 3-2 shows how this yield-based metric is calculated over a rolling window (i.e., proportion of successful requests over a one-day window).Equation 3-2. Aggregate availability
availability =successful requests total requests
For example, a system that serves 2.5M requests in a day with a daily availability tar‐get of 99.99% can serve up to 250 errors and still hit its target for that given day.In a typical application, not all requests are equal: failing a new user sign-up request is different from failing a request polling for new email in the background. In many cases, however, availability calculated as the request success rate over all requests is a reasonable approximation of unplanned downtime, as viewed from the end-user per‐spective.Quantifying unplanned downtime as a request success rate also makes this availabil‐ity metric more amenable for use in systems that do not typically serve end users directly. Most nonserving systems (e.g., batch, pipeline, storage, and transactional systems) have a well-defined notion of successful and unsuccessful units of work. Indeed, while the systems discussed in this chapter are primarily consumer and infra‐structure serving systems, many of the same principles also apply to nonserving sys‐tems with minimal modification.For example, a batch process that extracts, transforms, and inserts the contents of one of our customer databases into a data warehouse to enable further analysis may be set to run periodically. Using a request success rate defined in terms of records success‐fully and unsuccessfully processed, we can calculate a useful availability metric despite the fact that the batch system does not run constantly.Measuring Service Risk  |  27
Most often, we set quarterly availability targets for a service and track our perfor‐mance against those targets on a weekly, or even daily, basis. This strategy lets us manage the service to a high-level availability objective by looking for, tracking down, and fixing meaningful deviations as they inevitably arise. See Chapter 4 for more details.
Risk Tolerance of ServicesRisk Tolerance of Services
What does it mean to identify the risk tolerance of a service? In a formal environment or in the case of safety-critical systems, the risk tolerance of services is typically built directly into the basic product or service definition. At Google, services’ risk tolerance tends to be less clearly defined.To identify the risk tolerance of a service, SREs must work with the product owners to turn a set of business goals into explicit objectives to which we can engineer. In this case, the business goals we’re concerned about have a direct impact on the perfor‐mance and reliability of the service offered. In practice, this translation is easier said than done. While consumer services often have clear product owners, it is unusual for infrastructure services (e.g., storage systems or a general-purpose HTTP caching layer) to have a similar structure of product ownership. We’ll discuss the consumer and infrastructure cases in turn.Identifying the Risk Tolerance of Consumer Services
Our consumer services often have a product team that acts as the business owner for an application. For example, Search, Google Maps, and Google Docs each have their own product managers. These product managers are charged with understanding the users and the business, and for shaping the product for success in the marketplace. When a product team exists, that team is usually the best resource to discuss the reli‐ability requirements for a service. In the absence of a dedicated product team, the engineers building the system often play this role either knowingly or unknowingly.There are many factors to consider when assessing the risk tolerance of services, such as the following:
• What level of availability is required?
• Do different types of failures have different effects on the service?
• How can we use the service cost to help locate a service on the risk continuum?
• What other service metrics are important to take into account?
28  |  Chapter 3: Embracing Risk28  |  Chapter 3: Embracing Risk
Target level of availability
The target level of availability for a given Google service usually depends on the func‐tion it provides and how the service is positioned in the marketplace. The following list includes issues to consider:
• What level of service will the users expect?
• Does this service tie directly to revenue (either our revenue, or our customers’	revenue)?• Is this a paid service, or is it free?
• If there are competitors in the marketplace, what level of service do those com‐	petitors provide?
• Is this service targeted at consumers, or at enterprises?Consider the requirements of Google Apps for Work. The majority of its users are enterprise users, some large and some small. These enterprises depend on Google Apps for Work services (e.g., Gmail, Calendar, Drive, Docs) to provide tools that enable their employees to perform their daily work. Stated another way, an outage for a Google Apps for Work service is an outage not only for Google, but also for all the enterprises that critically depend on us. For a typical Google Apps for Work service, we might set an external quarterly availability target of 99.9%, and back this target with a stronger internal availability target and a contract that stipulates penalties if we fail to deliver to the external target.YouTube provides a contrasting set of considerations. When Google acquired You‐Tube, we had to decide on the appropriate availability target for the website. In 2006, YouTube was focused on consumers and was in a very different phase of its business lifecycle than Google was at the time. While YouTube already had a great product, it was still changing and growing rapidly. We set a lower availability target for YouTube than for our enterprise products because rapid feature development was correspond‐ingly more important.Types of failures
The expected shape of failures for a given service is another important consideration. How resilient is our business to service downtime? Which is worse for the service: a constant low rate of failures, or an occasional full-site outage? Both types of failure may result in the same absolute number of errors, but may have vastly different impacts on the business.An illustrative example of the difference between full and partial outages naturally arises in systems that serve private information. Consider a contact management application, and the difference between intermittent failures that cause profile pic‐tures to fail to render, versus a failure case that results in a user’s private contacts
Risk Tolerance of Services  |  29being shown to another user. The first case is clearly a poor user experience, and SREs would work to remediate the problem quickly. In the second case, however, the risk of exposing private data could easily undermine basic user trust in a significant way. As a result, taking down the service entirely would be appropriate during the debugging and potential clean-up phase for the second case.At the other end of services offered by Google, it is sometimes acceptable to have reg‐ular outages during maintenance windows. A number of years ago, the Ads Frontend used to be one such service. It is used by advertisers and website publishers to set up, configure, run, and monitor their advertising campaigns. Because most of this work takes place during normal business hours, we determined that occasional, regular, scheduled outages in the form of maintenance windows would be acceptable, and we counted these scheduled outages as planned downtime, not unplanned downtime.Cost
Cost is often the key factor in determining the appropriate availability target for a ser‐vice. Ads is in a particularly good position to make this trade-off because request suc‐cesses and failures can be directly translated into revenue gained or lost. In determining the availability target for each service, we ask questions such as:• If we were to build and operate these systems at one more nine of availability, 	what would our incremental increase in revenue be?
• Does this additional revenue offset the cost of reaching that level of reliability?
To make this trade-off equation more concrete, consider the following cost/benefit for an example service where each request has equal value:Proposed improvement in availability target: 99.9% → 99.99% Proposed increase in availability: 0.09% 
Service revenue: $1M 
Value of improved availability: $1M * 0.0009 = $900
In this case, if the cost of improving availability by one nine is less than $900, it is worth the investment. If the cost is greater than $900, the costs will exceed the projec‐ted increase in revenue.It may be harder to set these targets when we do not have a simple translation func‐tion between reliability and revenue. One useful strategy may be to consider the back‐ground error rate of ISPs on the Internet. If failures are being measured from the end-user perspective and it is possible to drive the error rate for the service below the background error rate, those errors will fall within the noise for a given user’s Internet connection. While there are significant differences between ISPs and protocols (e.g.,30  |  Chapter 3: Embracing Risk
TCP versus UDP, IPv4 versus IPv6), we’ve measured the typical background error rate for ISPs as falling between 0.01% and 1%.
Other service metrics
Examining the risk tolerance of services in relation to metrics besides availability is often fruitful. Understanding which metrics are important and which metrics aren’t important provides us with degrees of freedom when attempting to take thoughtful risks.Service latency for our Ads systems provides an illustrative example. When Google first launched Web Search, one of the service’s key distinguishing features was speed. When we introduced AdWords, which displays advertisements next to search results, a key requirement of the system was that the ads should not slow down the search experience. This requirement has driven the engineering goals in each generation of AdWords systems and is treated as an invariant.AdSense, Google’s ads system that serves contextual ads in response to requests from JavaScript code that publishers insert into their websites, has a very different latency goal. The latency goal for AdSense is to avoid slowing down the rendering of the third-party page when inserting contextual ads. The specific latency target, then, is dependent on the speed at which a given publisher’s page renders. This means that AdSense ads can generally be served hundreds of milliseconds slower than AdWords ads.This looser serving latency requirement has allowed us to make many smart trade-offs in provisioning (i.e., determining the quantity and locations of serving resources we use), which save us substantial cost over naive provisioning. In other words, given the relative insensitivity of the AdSense service to moderate changes in latency per‐formance, we are able to consolidate serving into fewer geographical locations, reduc‐ing our operational overhead.Identifying the Risk Tolerance of Infrastructure Services
The requirements for building and running infrastructure components differ from the requirements for consumer products in a number of ways. A fundamental differ‐ence is that, by definition, infrastructure components have multiple clients, often with varying needs.
Target level of availabilityTarget level of availability
Consider Bigtable [Cha06], a massive-scale distributed storage system for structured data. Some consumer services serve data directly from Bigtable in the path of a user request. Such services need low latency and high reliability. Other teams use Bigtable as a repository for data that they use to perform offline analysis (e.g., MapReduce) on