adversarial examples do not transfer between systems — an
assumption that has been already shown to be wrong in the
image domain [45].
Yang et al. [67], also utilize speciﬁc properties of the audio
domain and uses the temporal dependency of the input signal.
For this, they compare the transcription of the whole utterance
with a segment-wise transcription of the utterance. In the case
of a benign example, both transcriptions should be the same,
which will not be the case for an adversarial example. This
proved effective against static attacks, and the authors also
construct and discussed various adaptive attacks but these
were later shown to be insufﬁcient [55].
Besides approaches that aim to harden models against ad-
versarial examples, there is a line of research that focuses on
detecting adversarial examples: Liu and Ditzler [68] utilizing
quantization error of the activations of the neural network,
which appear to be different for adversarial and benign au-
dio examples. Däubener et al. [69] trained neural networks
capable of uncertainty quantiﬁcation to train a classiﬁer on
different uncertainty measures to detect adversarial examples
as outliers. Even if they trained their classiﬁer on benign
examples only, it will most likely not work for any kind of
attack, especially those aware of the detection mechanism.
In contrast, our approach does not rely on detection by aug-
menting the entire system to become more resilient against
adversarial examples. The basic principle of this has been
discussed as a defense mechanism in the image domain with
JPEG compression [70, 71] as well as in the audio domain
by Carlini and Wagner [59], Rajaratnam et al. [72], An-
dronic et al. [73], and Olivier et al. [74]. These approaches,
however, were only used as a pre-processing step to remove
semantically irrelevant parts from the input and thereby de-
stroy adversarial perturbations added by (static) attackers. In
contrast, we aim to train an ASR system that uses the same
information set as the human auditory systems. Consequently,
adversarial examples computed against this system are also
restricted to this set, and an attack cannot be hidden in inaudi-
ble ranges. Similar to the referenced approaches, we rely on
psychoacoustics and baseband ﬁltering. However, we do not
solely employ this as a pre-processing step but train a new
system with our augmentation data (i.e., removing impercep-
tible information from the training set). This allows us to not
simply destroy adversarial perturbations but rather conﬁne
the available attack surface.
6 Discussion
We have shown how we can augment an ASR system by
utilizing psychoacoustics in conjunction with a band-pass
ﬁlter to effectively remove semantically irrelevant information
from audio signals. This allows us to train a hardened system
that is more aligned with human perception.
Model Hardening Our results from Section 4.2 suggest
that the hardened models primarily utilize information avail-
able within audible ranges. Speciﬁcally, we observe that
models trained on the unmodiﬁed data set appear to use any
available signals and utilize information both from audible
and non-audible ranges. This is reﬂected in the accuracy
drop when presented with psychoacoustically ﬁltered input
(where only audible ranges are available). In contrast, the
augmented model performs comparably well on both types of
input. Hence, the model focuses on the perceivable audible
ranges and ignores the rest.
Robustness of the System We demonstrated how we can
create a more realistic attacker, which actively factors in the
augmentations during the calculation of adversarial examples.
In this case, however, the attack is forced into the audible
range. This makes the attack signiﬁcant more perceptible —
resulting in an average SNRseg drop of up to 24.33 dB for
speech samples. These results also transfer to other types
of audio content (i.e., music and birds tweeting) and are fur-
ther conﬁrmed by the listening test conducted in Section 4.4.
In summary, the results of these experiments show that an
attack is clearly perceivable. Further, we ﬁnd that the adver-
sarial examples, calculated with the adaptive attack, are easily
distinguishable from benign audio ﬁles by humans.
USENIX Association
30th USENIX Security Symposium    2321
Implementation Choices
In general, our augmentations
can be implemented in the form of low-cost pre-processing
steps with no noteworthy performance overhead. Only the
model needs to be retrained from scratch. However, the cost
of this could—in theory—be partially alleviated by transfer
learning. We leave this question as an interesting direction
for future research.
Robustness-Performance Tradeoff The results of the
adaptive attack (cf. Table 2) show that a larger margin Φ
leads to stronger robustness. Speciﬁcally, for Φ = 14, the
attacker was unable to ﬁnd any successful adversarial exam-
ple in our experiments. However, this incurs an expected
robustness-performance trade-off as previous research indi-
cates that adversarial robustness is generally correlated with
a loss in accuracy [53].
In the case of our strong white-box attacker, we recom-
mend a margin Φ ≥ 12, which result in a degraded system
performance by at least 1.82 percentage points in terms of the
benign WER. In this case, though, we already granted the
attacker many concessions: full access to the model with all
parameters, ideal playback (i.e., adversarial examples are fed
directly into the recognizer and are not played over-the-air),
and an easy target. We chose to study our attacker in this
setting as this poses the strongest class of attacks and allows
us to gain meaningful insights.
In contrast to white-box attacks, black-box attack don’t
have direct access to the gradient and for example rely on sur-
rogate models [75] or generative algorithms [76] to construct
adversarial examples. Therefore, adversarial examples from
these attacks are typically more conspicuous and can even
introduce semantic changes such that humans can perceive
the hidden transcription if they are made aware of it [75].
Considering our augmentations, we expect that current black-
box attacks are able to construct valid adversarial examples
against DOMPTEUR. However, we expect these to be signif-
icantly more noisy (in comparison to the adaptive attacker)
as DOMPTEUR forces modiﬁcations to the signal into audible
ranges regardless of the underlying attack strategy. Especially
in a realistic over-the-air setting, we suspect much higher dis-
tortions since the attacker is much more constrained. In such
a setting, a smaller Φ might also already sufﬁce. We leave
this as an interesting research direction for future work.
Improvement of the Attack The adaptive attack presented
in Section 4.3 can successfully compute adversarial examples,
except for very aggressive ﬁltering. While Figure 4 clearly
shows that the attack has converged, we were still unable
to ﬁnd working adversarial examples. However, other tar-
get/input utterance combinations may still exist, for which the
attack works and novel attack strategies should be studied.
Forcing Semantics into Adversarial Examples We have
shown how we can force adversarial audio attacks into the
audible range. This makes them clearly perceivable. Ulti-
mately, the goal is to push adversarial examples towards the
perceptual boundary between original and adversarial mes-
sage. Intuitively, adversarial examples should require such
extensive modiﬁcation that a human listener will perceive
the target transcription, i. e., that the adversarial perturbation
carries semantic meaning. We view our work as a ﬁrst suc-
cessful step into that direction and leave the exploration of
this strategy as an interesting question for future work.
7 Conclusion
In this work, we proposed a broadly applicable design princi-
ple for ASR systems that enables them to resemble the human
auditory system more closely. To demonstrate the principle,
we implemented a prototype of our approach in a tool called
DOMPTEUR. More speciﬁcally, we augment KALDI using
psychoacoustic ﬁltering in conjunction with a band-pass ﬁl-
ter. In several experiments, we demonstrate that our method
renders our system more robust against adversarial examples,
while retaining a high accuracy on benign audio input.
We have argued that an attacker can ﬁnd adversarial ex-
amples for any kind of countermeasure, particularly if we
assume the attack to have full white-box access to the sys-
tem. Speciﬁcally, we have calculated adversarial examples
for DOMPTEUR via an adaptive attack, which leverages the
full knowledge of the proposed countermeasures. Although
this attack is successful in computing adversarial examples,
we show that the attack becomes much less effective. More
importantly, we ﬁnd that adversarial examples are of poor
quality, as demonstrated by the SNRseg and our listening
test.
In summary, we have taken the ﬁrst steps towards bridg-
ing the gap between human expectations and the reality of
ASR systems—hence taming adversarial attacks to a certain
extent by robbing them of their stealth abilities.
Acknowledgments We would like to thank our shepherd
Xiaoyu Ji and the anonymous reviewers for their valuable
comments and suggestions. We also thank our colleagues
Nils Bars, Merlin Chlosta, Sina Däubener, Asja Fischer, Jan
Freiwald, Moritz Schlögel, Steffen Zeiler for their feedback
and fruitful discussions. This work was supported by the
Deutsche Forschungsgemeinschaft (DFG, German Research
Foundation) under Germany’s Excellence Strategy – EXC-
2092 CASA – 390781972.
2322    30th USENIX Security Symposium
USENIX Association
References
[1] Michael J Pazzani and Daniel Billsus. Content-Based
In The Adaptive Web.
Recommendation Systems.
Springer, 2007.
[2] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hin-
ton. ImageNet Classiﬁcation with Deep Convolutional
Neural Networks. In Advances in Neural Information
Processing Systems (NeurIPS), 2012.
[3] Volodymyr Mnih, Koray Kavukcuoglu, David Silver,
Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex
Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg
Ostrovski, et al. Human-level Control through Deep
Reinforcement Learning. nature, 2015.
[4] David Silver, Aja Huang, Chris J Maddison, Arthur
Guez, Laurent Sifre, George Van Den Driessche, Julian
Schrittwieser, Ioannis Antonoglou, Veda Panneershel-
vam, Marc Lanctot, et al. Mastering the Game of Go
with Deep Neural Networks and Tree Search. nature,
2016.
[5] Christopher Berner, Greg Brockman, Brooke Chan,
Vicki Cheung, Przemysław D˛ebiak, Christy Dennison,
David Farhi, Quirin Fischer, Shariq Hashme, Chris
Hesse, et al. Dota 2 with Large Scale Deep Rein-
forcement Learning. Computing Research Repository
(CoRR), abs/1912.06680, 2019.
[6] Oriol Vinyals, Igor Babuschkin, Wojciech M. Czarnecki,
Michaël Mathieu, Andrew Dudzik, Junyoung Chung,
David H. Choi, Richard Powell, Timo Ewalds, Petko
Georgiev, et al. Grandmaster Level in StarCraft II using
Multi-Agent Reinforcement Learning. nature, 2019.
[7] Andrew W. Senior, Richard Evans, John Jumper, James
Kirkpatrick, Laurent Sifre, Tim Green, Chongli Qin,
Augustin Žídek, Alexander WR Nelson, Alex Bridgland,
et al.
Improved Protein Structure Prediction using
Potentials from Deep Learning. nature, 2020.
[8] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and
Lior Wolf. DeepFace: Closing the Gap to Human-Level
Performance in Face Veriﬁcation. In IEEE Conference
on Computer Vision and Pattern Recognition (CVPR),
2014.
[9] Wayne Xiong, Jasha Droppo, Xuedong Huang, Frank
Seide, Mike Seltzer, Andreas Stolcke, Dong Yu, and
Geoffrey Zweig. Achieving Human Parity in Con-
versational Speech Recognition. Computing Research
Repository (CoRR), abs/1610.05256, 2016.
[10] Ashwin Ram, Rohit Prasad, Chandra Khatri, Anu
Venkatesh, Raefer Gabriel, Qing Liu, Jeff Nunn,
Behnam Hedayatnia, Ming Cheng, Ashish Nagar, et al.
Conversational AI: The Science Behind the Alexa Prize.
In Alexa Prize, 2017.
[11] Lauren Goode.
Amazon’s Alexa will now lock
your door for you (if you have a ’smart’ lock).
https://www.theverge.com/circuitbreaker/2016/7/28/123
05678/amazon-alexa-works-with-august-smart-lock-
door-WiFi-bridge. Accessed: 2021-06-02.
[12] Stephen Shankland. Meet Tesla’s self-driving car com-
puter and its two AI brains. https://www.cnet.com/news/
meet-tesla-self-driving-car-computer-and-its-two-ai-
brains/. Accessed: 2021-06-02.
[13] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever,
Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob
Fergus. Intriguing Properties of Neural Networks. In
International Conference on Learning Representations
(ICLR), 2014.
[14] Liwei Song and Prateek Mittal. POSTER: Inaudible
Voice Commands. In ACM Conference on Computer
and Communications Security (CCS), 2017.
[15] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang,
Taimin Zhang, and Wenyuan Xu. DolphinAttack: In-
In ACM Conference on
audible Voice Commands.
Computer and Communications Security (CCS), 2017.
[16] Xuejing Yuan, Yuxuan Chen, Yue Zhao, Yunhui Long,
Xiaokang Liu, Kai Chen, Shengzhi Zhang, Heqing
Huang, Xiaofeng Wang, and Carl A. Gunter. Comman-
derSong: A Systematic Approach for Practical Adversar-
ial Voice Recognition. In USENIX Security Symposium,
2018.
[17] Lea Schönherr, Katharina Kohls, Steffen Zeiler,
Thorsten Holz, and Dorothea Kolossa. Adversarial At-
tacks Against Automatic Speech Recognition Systems
via Psychoacoustic Hiding. In Symposium on Network
and Distributed System Security (NDSS), 2019.
[18] Lea Schönherr, Thorsten Eisenhofer, Steffen Zeiler,
Thorsten Holz, and Dorothea Kolossa.
Imperio: Ro-
bust Over-the-Air Adversarial Examples for Automatic
In Annual Computer
Speech Recognition Systems.
Security Applications Conference (ACSAC), 2020.
[19] Hadi Abdullah, Kevin Warren, Vincent Bindschaedler,
Nicolas Papernot, and Patrick Traynor. SoK: The Faults
in our ASRs: An Overview of Attacks against Auto-
matic Speech Recognition and Speaker Identiﬁcation
Systems. In IEEE Symposium on Security and Privacy
(S&P), 2020.
USENIX Association
30th USENIX Security Symposium    2323
[20] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt
Fredrikson, Z. Berkay Celik, and Ananthram Swami.
The Limitations of Deep Learning in Adversarial Set-
tings. In IEEE European Symposium on Security and
Privacy (EuroS&P), 2015.
[21] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi,
and Pascal Frossard. DeepFool: A Simple and Accurate
Method to Fool Deep Neural Networks. In IEEE Con-
ference on Computer Vision and Pattern Recognition
(CVPR), 2015.
[22] Nicholas Carlini and David Wagner. Towards Eval-
In IEEE
uating the Robustness of Neural Networks.
Symposium on Security and Privacy (S&P), 2017.
[23] Jan Hendrik Metzen, Tim Genewein, Volker Fischer,
and Bastian Bischoff. On Detecting Adversarial Per-
In International Conference on Learning
turbations.
Representations (ICLR), 2017.
[24] Reuben Feinman, Ryan R. Curtin, Saurabh Shintre,
and Andrew B. Gardner. Detecting Adversarial Sam-
ples from Artifacts. Computing Research Repository
(CoRR), abs/1703.00410, 2017.
[25] Nicholas Carlini and David Wagner. Adversarial Exam-
ples are Not Easily Detected: Bypassing Ten Detection
Methods. In ACM Workshop on Artiﬁcial Intelligence
and Security (AISec), 2017.
[26] Justin Gilmer, Ryan P. Adams, Ian Goodfellow, David
Andersen, and George E. Dahl. Motivating the Rules
of the Game for Adversarial Example Research. Com-
puting Research Repository (CoRR), abs/1807.06732,
2018.
[27] Adi Shamir, Itay Safran, Eyal Ronen, and Orr Dunkel-
man. A Simple Explanation for the Existence of Ad-
versarial Examples with Small Hamming. Computing
Research Repository (CoRR), abs/1901.10861, 2019.
[28] Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Lo-
gan Engstrom, Brandon Tran, and Aleksander Madry.
Adversarial Examples Are Not Bugs, They Are Features.
In Advances in Neural Information Processing Systems
(NeurIPS), 2019.
[29] Brian B. Monson, Eric J. Hunter, Andrew J. Lotto,
and Brad H. Story. The Perceptual Signiﬁcance of
High-frequency Energy in the Human Voice. Frontiers
in Psychology, 2014.
[31] Herve A. Bourlard and Nelson Morgan. Connectionist
Speech Recognition: A Hybrid Approach. Kluwer Press,
1994.
[32] Awni Hannun, Carl Case, Jared Casper, Bryan Catan-
zaro, Greg Diamos, Erich Elsen, Ryan Prenger, Sanjeev
Satheesh, Shubho Sengupta, Adam Coates, and An-
drew Y. Ng. Deep Speech: Scaling Up End-to-End
Speech Recognition. Computing Research Repository