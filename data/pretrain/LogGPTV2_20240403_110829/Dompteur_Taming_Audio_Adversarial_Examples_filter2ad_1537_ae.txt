### Adversarial Examples and Their Transferability

Adversarial examples, which are inputs intentionally designed to cause machine learning models to make mistakes, do not always transfer between different systems. This assumption, however, has been shown to be incorrect in the image domain [45]. 

Yang et al. [67] leverage specific properties of the audio domain, particularly the temporal dependency of the input signal. They compare the transcription of the entire utterance with a segment-wise transcription. For benign examples, both transcriptions should match, while for adversarial examples, they will differ. This method is effective against static attacks. The authors also constructed and discussed various adaptive attacks, but these were later found to be insufficient [55].

### Detection of Adversarial Examples

In addition to methods aimed at hardening models against adversarial examples, there is a body of research focused on detecting them. Liu and Ditzler [68] utilized the quantization error of neural network activations, which differs between adversarial and benign audio examples. Däubener et al. [69] trained neural networks capable of uncertainty quantification to detect adversarial examples as outliers based on different uncertainty measures. Even though their classifier was trained only on benign examples, it may not be effective against all types of attacks, especially those that are aware of the detection mechanism.

### Our Approach: Augmenting the System

Our approach does not rely on detection but instead aims to augment the entire system to become more resilient against adversarial examples. This principle has been explored in the image domain with JPEG compression [70, 71] and in the audio domain by Carlini and Wagner [59], Rajaratnam et al. [72], Andronic et al. [73], and Olivier et al. [74]. These methods typically remove semantically irrelevant parts from the input, thereby destroying adversarial perturbations added by static attackers. In contrast, we aim to train an Automatic Speech Recognition (ASR) system that uses the same information set as the human auditory system. Consequently, adversarial examples computed against this system are restricted to this set, making it impossible to hide an attack in inaudible ranges. We use psychoacoustics and baseband filtering, not just as a pre-processing step, but to train a new system with augmented data. This allows us to confine the available attack surface rather than simply destroying adversarial perturbations.

### Discussion

We have demonstrated how to augment an ASR system using psychoacoustics in conjunction with a band-pass filter to effectively remove semantically irrelevant information from audio signals. This results in a hardened system that aligns more closely with human perception.

#### Model Hardening

Our results from Section 4.2 suggest that the hardened models primarily utilize information within audible ranges. Models trained on unmodified data sets use any available signals, including both audible and non-audible ranges. This is reflected in the accuracy drop when presented with psychoacoustically filtered input, where only audible ranges are available. In contrast, the augmented model performs comparably well on both types of input, indicating that it focuses on perceivable audible ranges and ignores the rest.

#### Robustness of the System

We demonstrated how to create a more realistic attacker that factors in the augmentations during the calculation of adversarial examples. In this case, the attack is forced into the audible range, making it significantly more perceptible, resulting in an average SNRseg drop of up to 24.33 dB for speech samples. These results also apply to other types of audio content, such as music and bird sounds, and are further confirmed by the listening test conducted in Section 4.4. Overall, the results show that the attack is clearly perceivable, and the adversarial examples calculated with the adaptive attack are easily distinguishable from benign audio files by humans.

### Implementation Choices

Our augmentations can be implemented as low-cost pre-processing steps with no significant performance overhead. Only the model needs to be retrained from scratch, but this cost could potentially be reduced through transfer learning, which we leave as an interesting direction for future research.

### Robustness-Performance Tradeoff

The results of the adaptive attack (cf. Table 2) show that a larger margin Φ leads to stronger robustness. For Φ = 14, the attacker was unable to find any successful adversarial example in our experiments. However, this incurs a robustness-performance trade-off, as previous research indicates that adversarial robustness is generally correlated with a loss in accuracy [53]. For a strong white-box attacker, we recommend a margin Φ ≥ 12, which results in a degraded system performance by at least 1.82 percentage points in terms of the benign Word Error Rate (WER). In this setting, we granted the attacker full access to the model, ideal playback, and an easy target to study the strongest class of attacks and gain meaningful insights.

### Black-Box Attacks

In contrast to white-box attacks, black-box attacks do not have direct access to the gradient and often rely on surrogate models [75] or generative algorithms [76] to construct adversarial examples. These attacks are typically more conspicuous and can introduce semantic changes that humans can perceive if made aware of them [75]. Considering our augmentations, we expect current black-box attacks to be able to construct valid adversarial examples against DOMPTEUR, but they will likely be significantly noisier. In a realistic over-the-air setting, we suspect much higher distortions, and a smaller Φ might suffice. This is another area for future research.

### Conclusion

In this work, we proposed a broadly applicable design principle for ASR systems that enables them to resemble the human auditory system more closely. We implemented a prototype of our approach in a tool called DOMPTEUR, augmenting KALDI using psychoacoustic filtering in conjunction with a band-pass filter. Our experiments demonstrate that our method renders the system more robust against adversarial examples while retaining high accuracy on benign audio input. Although an attacker can find adversarial examples for any countermeasure, especially with full white-box access, we show that the attack becomes much less effective and of poor quality, as demonstrated by the SNRseg and our listening test. In summary, we have taken the first steps towards bridging the gap between human expectations and the reality of ASR systems, thus taming adversarial attacks to a certain extent by robbing them of their stealth abilities.

### Acknowledgments

We would like to thank our shepherd Xiaoyu Ji and the anonymous reviewers for their valuable comments and suggestions. We also thank our colleagues Nils Bars, Merlin Chlosta, Sina Däubener, Asja Fischer, Jan Freiwald, Moritz Schlögel, and Steffen Zeiler for their feedback and fruitful discussions. This work was supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany’s Excellence Strategy – EXC-2092 CASA – 390781972.