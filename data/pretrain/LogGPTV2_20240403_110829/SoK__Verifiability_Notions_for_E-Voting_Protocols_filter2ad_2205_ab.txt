provided that the processes are connectible: two processes
are connectible if common external channels, i.e., channels
with the same name, have opposite directions (input/output);
internal channels are renamed, if necessary. A process π where
all programs are given the security parameter 1(cid:5) is denoted
by π((cid:5)). In the processes we consider the length of a run is
always polynomially bounded in (cid:5). Clearly, a run is uniquely
determined by the random coins used by the programs in π.
Protocol. A protocol P is deﬁned by a set of agents Σ (also
called parties or protocol participants), and a program πa which
is supposed to be run by the agent. This program is the honest
program of a. Agents are pairwise connected by channels and
every agent has a channel to the adversary (see below).2
Typically, a protocol P contains a scheduler S as one of its
participants which acts as the master program of the protocol
process (see below). The task of the scheduler is to trigger
the protocol participants and the adversary in the appropriate
order. For example, in the context of e-voting, the scheduler
would trigger protocol participants according to the phases of
an election, e.g., i) register, ii) vote, iii) tally, iv) verify.
If πa1
, . . . , πan are the honest programs of the agents of P,
The process πP is always run with an adversary A. The
adversary may run an arbitrary probabilistic polynomial-time
program and has channels to all protocol participants in πP.
Hence, a run r of P with adversary (adversary program) πA is
a run of the process πP (cid:5) πA. We consider πP (cid:5) πA to be part of
the description of r, so that it is always clear to which process,
including the adversary, the run r belongs.
The honest programs of the agents of P are typically
then we denote the process πa1
(cid:5) . . . (cid:5) πan by πP.
speciﬁed in such a way that the adversary A can corrupt the
programs by sending the message corrupt. Upon receiving such
a message, the agent reveals all or some of its internal state to
the adversary and from then on is controlled by the adversary.
Some agents, such as the scheduler or a judge, will typically not
be corruptible, i.e., they would ignore corrupt messages. Also,
agents might only accept corrupt message upon initialization,
modeling static corruption. Altogether, this allows for great
ﬂexibility in deﬁning different kinds of corruption, including
various forms of static and dynamic corruption.
We say that an agent a is honest in a protocol run r if the
agent has not been corrupted in this run, i.e., has not accepted a
corrupt message throughout the run. We say that an agent a is
honest if for all adversarial programs πA the agent is honest in
all runs of πP (cid:5) πA, i.e., a always ignores all corrupt messages.
Property. A property γ of P is a subset of the set of all runs
of P.3 By ¬γ we denote the complement of γ.
Negligible, overwhelming, δ-bounded. As usual, a function
f from the natural numbers to the interval [0,1] is negligible
if, for every c > 0, there exists (cid:5)0 such that f ((cid:5)) ≤ 1
(cid:5)c for all
(cid:5) > (cid:5)0. The function f is overwhelming if the function 1− f is
negligible. A function f is δ-bounded if, for every c > 0 there
exists (cid:5)0 such that f ((cid:5)) ≤ δ + 1
B. Veriﬁability
(cid:5)c for all (cid:5) > (cid:5)0.
The KTV framework comes with a general deﬁnition of
veriﬁability. The deﬁnition assumes a judge J whose role is
to accept or reject a protocol run by writing accept or reject
on a dedicated channel decisionJ. To make a decision, the
judge runs a so-called judging procedure, which performs
certain checks (depending on the protocol speciﬁcation), such
as veriﬁcation of all zero-knowledge proofs (if any). Intuitively,
J accepts a run if the protocol run looks as expected. The
judging procedure should be part of the protocol speciﬁcation.
So, formally the judge should be one of the protocol participants
in the considered protocol P, and hence, precisely speciﬁed.
The input to the judge typically is solely public information,
including all information and complaints (e.g., by voters) posted
on the bulletin board. Therefore the judge can be thought of
as a “virtual” entity: the judging procedure can be carried out
by any party, including external observers and even voters
themselves.
The deﬁnition of veriﬁability is centered around the notion
of a goal of the protocol. Formally, a goal is simply a property
γ of the system, i.e. a set of runs (see Section III-A). Intuitively,
such a goal speciﬁes those runs which are “correct” in some
protocol-speciﬁc sense. For e-voting, intuitively, the goal would
contain those runs where the announced result of the election
corresponds to the actual choices of the voters.
Now, the idea behind the deﬁnition is very simple. The
judge J should accept a run only if the goal γ is met, and hence,
the published election result corresponds to the actual choices
of the voters. More precisely, the deﬁnition requires that the
probability (over the set of all runs of the protocol) that the
goal γ is not satisﬁed but the judge nevertheless accepts the run
is δ-bounded. Although δ = 0 is desirable, this would be too
strong for almost all e-voting protocols. For example, typically
not all voters check whether their ballot appears on the bulletin
2We note that in [37] agents were assigned sets of potential programs they
could run plus an honest program. Here, w.l.o.g., they are assigned only one
honest program (which, however, might be corrupted later on).
3Recall that the description of a run r of P contains the description of
the process πP (cid:5) πA (and hence, in particular the adversary) from which r
originates. Hence, γ can be formulated independently of a speciﬁc adversary.
781781
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:36 UTC from IEEE Xplore.  Restrictions apply. 
board, giving an adversary A the opportunity to manipulate
or drop some ballots without being detected. Therefore, δ = 0
cannot be achieved in general.
By Pr[π((cid:5)) (cid:7)→ (J: accept)] we denote the probability that π,
with security parameter 1(cid:5), produces a run which is accepted
by J. Analogously, by Pr[π((cid:5)) (cid:7)→ ¬γ, (J: accept)] we denote
the probability that π, with security parameter 1(cid:5), produces a
run which is not in γ but nevertheless accepted by J.
Deﬁnition 1 (Veriﬁability). Let P be a protocol with the set of
agents Σ. Let δ ∈ [0,1] be the tolerance, J ∈ Σ be the judge and
γ be a goal. Then, we say that the protocol P is (γ, δ)-veriﬁable
by the judge J if for all adversaries πA and π = (πP (cid:5) πA), the
probability
Pr[π((cid:5)) (cid:7)→ ¬γ, (J: accept)]
is δ-bounded as a function of (cid:5).
A protocol P could trivially satisfy veriﬁability with a judge
who never accepts a run. Therefore, one of course would also
require a soundness or fairness condition. That is, one would
except at the very least that if the protocol runs with a benign
adversary, which, in particular, would not corrupt parties, then
the judge accepts a run. Formally, for a benign adversary πA
we require that Pr[π((cid:5)) (cid:7)→ (J: accept)] is overwhelming. One
could even require that the judge accepts a run as soon as a
certain subset of protocol participants are honest, e.g., the voting
authorities (see, e.g., [37] for a more detailed discussion). These
kinds of fairness/soundness properties can be considered to be
sanity checks of the judging procedure and are typically easy
to check. Most deﬁnitions of veriﬁability in the literature do
not explicitly mention this property. For brevity of presentation,
we therefore mostly ignore this issue here as well. In the
subsequent sections, we, however, mention and brieﬂy discuss
fairness conditions unless addressed by a deﬁnition.
Deﬁnition 1 captures the essence of the notion of veriﬁability
in a very simple way, as explained above. In addition, it provides
great ﬂexibility and it is applicable to arbitrary classes of e-
voting protocols. This is in contrast to most other deﬁnitions of
veriﬁability, as we will see in the subsequent sections, which are
mostly tailored to speciﬁc classes of protocols. This ﬂexibility in
fact lets us express the other deﬁnitions in terms of Deﬁnition 1.
There are two reasons for the ﬂexibility. First, the notion of
a protocol P used in Deﬁnition 1 is very general: a protocol
is simply an arbitrary set of interacting Turing machines, with
one of them playing the role of the judge. Second, the goal
γ provides great ﬂexibility in expressing what an e-voting
protocol is supposed to achieve in terms of veriﬁability.
As mentioned in the introduction, in the following sections,
we present all relevant deﬁnitions of veriﬁability from the
literature, discuss them in detail, and then express their essence
in terms of Deﬁnition 1. The latter, in particular, allows for a
uniform treatment of the various deﬁnitions from the literature,
and by this a better understanding of the individual deﬁnitions
and their relationships to other deﬁnitions. Advantages and
disadvantages of the deﬁnitions can be clearly seen in terms
of the classes of protocols that are captured by the deﬁnitions
and the security guarantees that they give. It seems particularly
interesting to see which goals γ (in the sense deﬁned above)
these deﬁnitions consider. In Section X, among others, we use
these insights to distill precise guidelines for important aspects
of deﬁnitions of veriﬁability and propose goals γ applicable to
a broad class of e-voting protocols, and hence, we provide a
particularly useful instantiation of Deﬁnition 1 given what we
have learned from all deﬁnitions from the literature.
The following sections, in which we present and discuss
the various deﬁnitions of veriﬁability from the literature, are
ordered in such a way that deﬁnitions that are close in spirit are
discussed consecutively. All sections follow the same structure.
In every section, we ﬁrst brieﬂy sketch the underlying model,
then present the actual deﬁnition of veriﬁability, followed by
a discussion of the deﬁnition, and ﬁnally the casting of the
deﬁnition in Deﬁnition 1. We emphasize that the discussions
about the deﬁnitions provided in these sections reﬂect the
insights we obtained by casting the deﬁnitions in the KTV
framework. For simplicity and clarity of the presentation, we,
however, present the (informal) discussions before casting the
deﬁnitions.
IV. A SPECIFIC VERIFIABILITY GOAL BY K ¨USTERS ET AL.
In [37], K¨usters et al. also propose a speciﬁc family of
goals for e-voting protocols that they used in [37] as well
as subsequent works [40], [39], [38]. We present this family
of goals below as well as the way they have instantiated the
model when applied to concrete protocols. Since this is a
speciﬁc instantiation of the KTV framework, we can omit the
casting of their deﬁnition in this framework.
A. Model
When applying the KTV framework in order to model
speciﬁc e-voting protocols, K¨usters et al. model static corruption
of parties. That is, it is clear from the outset whether or not a
protocol participant (and in particular a voter) is corrupted. An
honest voter V runs her honest program πV with her choice
c ∈ C provided by the adversary. This choice is called the actual
choice of the voter, and says how the voter intends to vote.
B. Veriﬁability
In [37], K¨usters et al. propose a general deﬁnition of
accountability, with veriﬁability being a special case. Their
veriﬁability deﬁnition, as mentioned, corresponds to Deﬁni-
tion 1. Their deﬁnition, however, also captures the fairness
condition which we brieﬂy mentioned in Section III-B. To this
end, K¨usters et al. consider Boolean formulas with propositional
variables of the form hon(a) to express constraints on the
honesty of protocol participants. Roughly speaking, given a
Boolean formula ϕ, their fairness condition says that if in a
run parties are honest according to ϕ, then the judge should
accept the run.
While just as in Deﬁnition 1, the veriﬁability deﬁnition
proposed by K¨usters et al. does not require to ﬁx a speciﬁc
goal, for e-voting they propose a family {γk}k≥0 of goals,
which has been applied to analyze various e-voting protocols
and mix nets [37], [40], [39], [38].
Roughly speaking, for k ≥ 0, the goal γk contains exactly
those runs of the voting protocol in which all but up to k votes
of the honest voters are counted correctly and every dishonest
voter votes at most once.
Before recalling the formal deﬁnition of γk from [37], we
ﬁrst illustrate γk by a simple example. For this purpose, consider
an election with ﬁve eligible voters, two candidates, with the
result of the election simply being the number of votes for
each candidate. Let the result function ρ (see Section II) be
deﬁned accordingly. Now, let r be a run with three honest and
782782
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:17:36 UTC from IEEE Xplore.  Restrictions apply. 
two dishonest voters such that A, A, B are the actual choices
of the honest voters in r and the published election result in
r is the following: one vote for A and four votes for B. Then,
the goal γ1 is satisﬁed because the actual choice of one of
the honest voters choosing A can be changed to B and at the
same time the choice of each dishonest voter can be B. Hence,
the result is equal to ρ(A,B,B,B,B), which is the published
result. However, the goal γ0 is not satisﬁed in r because in this
case, all honest voters’ choices (A,A,B) have to be counted
correctly, which, in particular, means that the ﬁnal result has
to contain at least two votes for A and at least one vote for
B. In particular, a ﬁnal result with only two votes for A but
none for B would also not satisfy γ0, but it would satisfy γ1.
(Recall from Section II that abstention is a possible choice.)
Deﬁnition 2 (Goal γk). Let r be a run of an e-voting protocol.
Let nh be the number of honest voters in r and nd = n− nh
be the number of dishonest voters in r. Let c1, . . . , cnh be the
actual choices of the honest voters in this run, as deﬁned above.
Then γk is satisﬁed in r if there exist valid choices ˜c1, . . . , ˜cn
such that the following conditions hold true:
(i) The multiset {˜c1, . . . , ˜cn} contains at least nh−k elements
(ii) The result of the election as published in r (if any) is
of the multiset {c1, . . . , cnh
equal to ρ({˜c1, . . . , ˜cn}).
}.
If no election result is published in r, then γk is not satisﬁed
in r.
With this goal, Deﬁnition 1 requires that if more than
k votes of honest voters were dropped/manipulated or the
number of votes cast by dishonest voters (which are subsumed
by the adversary) is higher than the number dishonest voters
(ballot stufﬁng), then the judge should not accept the run. More
precisely, the probability that the judge nevertheless accepts
the run should be bounded by δ.
We note that the deﬁnition of γk does not require that
choices made by dishonest voters in r need to be extracted
from r in some way and that these extracted choices need to be
reﬂected in {˜c1, . . . ˜cn}: the multiset {˜c1, . . . , ˜cn} of choices
is simply quantiﬁed existentially. It has to contain nh − k
honest votes but no speciﬁc requirements are made for votes
of dishonest voters in this multiset. They can be chosen fairly
independently of the speciﬁc run r (except for reﬂecting the
published result and the requirement that there is at most one
vote for every dishonest voter). This is motivated by the fact
that, in general, one cannot provide any guarantees for dishonest
voters, since, for example, their ballots might be altered or
ignored by dishonest authorities without the dishonest voters
complaining (see also the discussion in [37]).
C. Discussion
The goal γk makes only very minimal assumptions about
the structure of a voting system. Namely, it requires only that,
given a run r, it is possible to determine the actual choice
(intention) of an honest voter and the actual election result.
Therefore, the goal γk can be used in the analysis of a wide
range of e-voting protocols.
One drawback of the goal γk is that it assumes static