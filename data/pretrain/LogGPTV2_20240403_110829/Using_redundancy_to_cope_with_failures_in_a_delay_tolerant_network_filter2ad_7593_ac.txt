Consider an investor who has the choice to invest in n assets.
The return on the ith assets is described by a random variable
Si. Given a utility function that represents the interests of the
investor, in what proportion should she invest her wealth on
various assets to maximize her utility function.
The vector
of allocation weights (amount invested in each asset) is called
a “portfolio,” and is represented by (x1, x2 . . . xk).
In these
terms, diﬀerent assets correspond to diﬀerent paths in a DTN,
and xi becomes the amount of traﬃc assigned to each path.
We seek to maximize a utility function, which for DTN will
mean trying to maximize the probability that a message can be
successfully decoded from its code blocks when transported over
lossy paths. For Gaussian failures this is same as maximizing a
portfolio’s return while minimizing its risk and is captured by
the Sharpe-Ratio.
To optimize the Sharpe-Ratio, we recall the notion of an eﬃ-
cient frontier, proposed by Markowitz [1]. The eﬃcient frontier
represents all the portfolios which have the “highest return for
a given risk” or, equivalently, the “lowest risk for a given re-
turn.”3 Risk is measured by σY . Figure 3 shows an example
eﬃcient frontier. It is easy to see that the portfolio that maxi-
mizes the Sharpe-Ratio also lies on the eﬃcient frontier. This is
because the Sharpe-Ratio has the mean in the numerator and
the variance in the denominator.4 Our problem is now reduced
to ﬁnding a point on the eﬃcient frontier that maximizes the
Sharpe-Ratio. Geometrically, this is the tangent from the point
(0, 1/r) to the eﬃcient frontier, as shown in Figure 3.
To ﬁnd a portfolio that maximizes the Sharpe-Ratio, we use
It can be
a simple numerical search on the eﬃcient frontier.
shown that under mild conditions the Sharpe-Ratio function is
concave. This allows us to do an eﬃcient one dimensional search
over the eﬃcient frontier.5 The complexity of this approach is
n
n
3A point (σ1, µ1) on the eﬃcient frontier is obtained by solving
the following quadratic optimization problem:
n
Minimize σ1
j=1 xixjσij,
2, where σ2
1 =
i=1 xipi = µ1, 
i=1
j=1 xj = 1, and 0 ≤ xi ≤ ui
Subject to: 
4For now, assume that the numerator is positive for the opti-
mal allocation. The case when the numerator is always negative
occurs when paths have very low probability and is less inter-
esting. This also guides us that the replication factor should be
large enough to make (µY − 1/r) positive.
5A one dimensional search is based upon ﬁnding a point for
n
O(In6), where I is the number of iterations in the numeric
search and n6 is the complexity of solving a convex quadratic
optimization problem. This numerical approach will be referred
to as the Markowitz Numeric algorithm.
4.3 Approximation for optimizing the Sharpe-Ratio
We now present an eﬃcient approximation to optimize the
Sharpe-Ratio. The key observation is that if we remove the
constraints that bound the value of xi (0 ≤ xi ≤ ui), a closed
form solution can be obtained. The following equation gives a
portfolio ˆx ≡ (x1, x2 . . . xn) that maximizes the Sharpe-Ratio.
−1(r ˆp − ˆ1)
ˆV
ˆ1 ˆV −1(r ˆp − ˆ1)
ˆx =
(5)
Here, ˆV is the covariance matrix, i.e vij = σij , ˆp is the n-
dimensional vector of the path probabilities and ˆ1 is the unit
vector of dimension n.
The derivation of the optimal point is discussed elsewhere [1].
The above solution is obtained by assuming that the numerator
of the Sharpe-Ratio is positive. This assumption does not hold
true if all paths have low probability of success. This is simi-
lar to our discussion in the Bernoulli case when p  ui then xi = ui
4: if 
∀i ∈ 1..n, xi = xi/
n
1 xi > 1 then
n
1 xj
n
1 xj
n
1 xi

l = 1 −
t = arg maxi∈1..n{pi |xi  0 do
9:
10:
11:
12:
13: end if
Algorithm 1: Normalization algorithm to enforce volume con-
n
i=1 xi = 1). Lines
7-12 use a simple heuristic to adjust for slack variables after the
constraints are enforced. Its complexity is O(nlog(n)).
straints (0 ≤ xi ≤ ui) and normalize xi (
end while
ˆx =
ˆV −1(r ˆp−ˆ1)
ˆ1 ˆV −1(r ˆp−ˆ1)
Markowitz algorithm
1: if max(µY ) > 1/r then
2:
3: else
4: Maximize µY
5: end if
6: Use normalization algorithm
(pi−1/r)
Algorithm 2: Markowitz algorithm. For independent Si, step
In step 4, µY is maximized by
2 is trivial as xi =
using greedy allocation. The computational complexity of this
approach is O(n3 +nlog(n)) if Si are dependent and O(nlog(n))
if they are independent.
σii
.
The above solution is quite elegant. It states that for the ith
path, xi should be proportional to the path’s excess probability
(pi − 1/r) and inversely proportional to the path’s variance.
The proportionality constant is determined by observing that

n
i=1 xi = 1.
The above formula may lead to assignments of the xi that do
not satisfy the volume constraints. Algorithm 1 is a simple nor-
malization algorithm devised to enforce the volume constraints.
If any xi is negative, it is set to zero and if xi > ui, it is set
to ui. This approach, i.e., using Equation 5 followed by the
normalization algorithm, will be referred to as the Markowitz
algorithm and is summarized as Algorithm 2.
4.4 Sharpe-Ratio for Other Distributions
Although the argument that optimizing the Sharpe-Ratio
−1) relies on the Gaussian assump-
maximizes the P rob(Y > r
tion, we now discuss why maximizing the Sharpe-Ratio is a
good approach for other distributions as well. The numerator
in the Sharpe-Ratio denotes the average amount of data that
is received in excess of what is required to decode the message
(1/r). If the standard deviation of Y (denominator) is small,
the probability that Y deviates from its mean is also small,
by Chebyshev’s inequality. Therefore, if the numerator is pos-
itive, i.e., µY > 1/r, it is reasonable to minimize the standard
deviation (thereby maximizing the Sharpe-Ratio). The ratio
captures both the excess return and the risk of not obtaining
which the slope of the function is close to zero.
the expected return. It gives us one way of systematically ap-
proaching the problem. The beauty of this approach is that it
accounts for complex aspects such as path correlations and is
computationally eﬃcient.
5. EVALUATION
In an eﬀort to better understand the problem we are attack-
ing, and the beneﬁts of the mathematical approach we have
discussed, we evaluate diﬀerent techniques in three simulation
scenarios. The ﬁrst scenario examines DTN routing over data
MULEs, where paths are independent and data loss is Bernoulli
due to message expiration when MULEs get delayed. We then
move on to DTN routing over a set of city buses, where paths
are multi-hop and dependent. Finally, we discuss a large sen-
sor network scenario, where there are many paths and losses
are partial in nature. Our main goal is to evaluate the per-
formance diﬀerences between using simple replication and the
erasure coding based Markowitz approach.
5.1 Performance Metric
The basic performance metric used is the failure probability
(F P ) for a message, deﬁned as 1− P rob(Y > r
−1). For simula-
tions in which multiple messages are transmitted, we consider
the distribution of F P , also referred to as the failure rate (F R).
5.2 Allocation Techniques Considered
Simple Replication (SRep)
In this technique, identical copies of a message are sent over the
r highest probability paths. No erasure coding is used. This ap-
proach eﬀectively does a greedy allocation by considering only
the best paths. Its complexity is O(n).
All the following techniques use erasure coding and diﬀer in
how they allocate code blocks to diﬀerent paths.
Simple Replication with Coding (SRep-Code)
Here, an equal proportion of erasure code blocks are sent over
the r highest probability paths. Normalization algorithm is
used to enforce volume constraints. This has the same perfor-
mance as SRep when the path failure model is Bernoulli and
the contact volume is suﬃcient for an entire message.
Proportional (Prop)
This is a simple heuristic in which allocation is proportional to
the path probability  xi = pi
 pi, along with the normalization
algorithm to enforce volume constraints. Its complexity is O(n).
Markowitz Numeric (MkwNu)
This is the numeric method for maximizing the Sharpe-Ratio, as
discussed in Section 4.2. It requires solving a series of quadratic
optimization problems with O(n) constraints.
Markowitz (Mkw)
This is the eﬃcient approximation for maximizing the Sharpe-
Ratio, as discussed in Section 4.2. The computational complex-
ity of this approach is O(nlog(n)) if Si are independent and
O(n3 + nlog(n)) otherwise. Although the Markowitz approach
is derived under a Gaussian path failure model, it is applicable
as an approximation for other failure models (see Section 4.4).
Mixed Integer Programming (MIP)
MIP is the optimal technique when the failure model is Bernoulli
(see Section 3.2) and acts as yardstick for Bernoulli failures;
unfortunately, it has exponential complexity. Our current im-
plementation uses CPLEX and and we were able to solve for
problem sizes as large as 16.
6. DATA MULE SCENARIO
In our ﬁrst evaluation scenario, we consider the case in which
mobile entities (called MULEs) are used to ”carry” data from
the source to the destination [9, 18, 25]. For example, in a
sparse sensor network, Data-MULEs roam in the environment,
retrieving data from the sensor(s) when they are physically close
to them (via limited-range radios), and then transporting the
data to an access-point, potentially located far away from the
source.
For this experiment, we assume that MULEs do not forward
messages to each other, and therefore, diﬀerent MULEs act as
independent forwarding nodes from the source to the destina-
tion. A message may expire if a MULE takes a long time to
reach the destination. If the MULE mobility is random, it is
hard to predict whether a MULE will deliver the data on time.
This unpredictability makes the problem of selecting the best
forwarding MULE(s) hard. By replicating data on multiple in-
dependent MULEs, the probability is increased that at least
one MULE will reach the destination in time. In this scenario,
message expiration due to delayed MULEs is the only cause
of data loss. In other simulation scenarios we investigate the
implications of multiple hops and other sources of data loss.
In this scenario, the path failure model is Bernoulli: either
the MULE arrives on time at the destination and delivers all the
p
Algorithm
.41
SRep
Prop
Mkw
MIP
SRep
.61 Mkw, Prop
MIP
SRep
.86 Mkw, Prop
MIP
# of MULEs (→) (n)
8
4
32
16
64
36% 35% 37% 36% 36%
48% 58% 70% 82% 88%
36% 35% 37% 36% 36%
36% 35% 37%
15% 15% 15% 15% 15%
19% 17% 11% 3%
1%
–
–
15% 15% 11%
2%
2%
2%
2%
0%
0%
0%
1%
1%
0%
–
–
2%
1%
1%
–
–
Table 1: Failure rates with diﬀerent MULE densities and
success probabilities. r = 2 in all cases. When p ≥ .5 both
Proportional and Markowitz divide the code blocks equally
among all MULEs and hence, are shown together.
data or the message expires and no data is transmitted. The
probability that the ith path is successful is pi = P rob(Di ≤ T ),
where Di is the delay distribution of using the ith MULE and
T is the message expiration time. The delay distribution Di is
highly dependent on the environmental parameters, such as the
topology, MULE velocity, radio range, mobility model, etc. In
our simulations, each node records a contact history for each
MULE along with the last time that MULE contacted the des-
tination. This history is used to estimate the delay distribution.
Simulation Setup and Parameters
For the purposes of this section we use the following mobility
model. We assume a planar environment with a dimension of
1km x 1km, with a single source and a single destination at
opposite corners of the environment. Messages are generated
once per hour, the message size is 10KB, the contact band-
width is 100Kbps, and the storage capacity for MULE is 1MB.
For these default settings, the message can be transmitted com-
pletely during a single contact with MULE. The eﬀect of split-
ting due to small contact volume (relative to the message size)
is considered in Section 6.2.
Each MULE follows a random waypoint mobility model, and
unless otherwise speciﬁed, the MULE velocity is 10m/s (36
km/h). The radio range is 25m, and contact occurs whenever a
MULE moves within range of a node. With these settings, the
average value of delay is approximately 120 minutes. With this
setup, we can create diﬀerent delivery probabilities p (given by
P rob(D ≤ T )) by varying the relationship between the data
deadline T (default is two hours) and/or changing the MULE
velocities.
In the next few sections we evaluate and compare diﬀerent
replication techniques for various aspects of this scenario.
6.1 MULE Density
We now demonstrate the eﬀect of the MULE density on the
failure probability (FP) by varying the number of MULEs in
the environment. All MULEs have the default velocity (10m/s),
so they have approximately the same probability of success for
a given deadline T . Table 1 shows the FP for three scenarios
with deadlines of 80, 120, and 200 minutes (corresponding to
p = 0.41, 0.61, 0.86, respectively). We keep r ﬁxed at two, so
these choices illustrate three “regimes” based on the product
p · r. Results for Markowitz-Numeric and SRep-Code are omit-
ted because they have the same performance as Markowitz and
SRep, respectively.
The ﬁrst regime occurs when p = .41, hence p · r is less than
2
# of Fragments per Message (→)
16
1
15% 28% 48% 72% 92%
3%
15% 16% 15% 11%
3%
3%
3%
3%
3%
4
8
Algorithm
SRep
SRep-Code
Mkw
0
Algo-