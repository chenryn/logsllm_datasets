Along with the output of the source data, targeted poisoning ef-
fects the overall accuracy of the global model. We measure this
effect on the accuracy by calculating the accuracy drop due to poi-
soning on dataset.
Deﬁnition 2.4. (Accuracy Drop) Accuracy drop (AD) in a model
MP due to poisoning is deﬁned as the difference in the overall
accuracy of a model trained using benign dataset and malicious
dataset, speciﬁcally as below:
AD(MB→MP ) = AMB − AMP
where AMB and AMP is the accuracy of benign model MB and
malicious model MP respectively.
Higher is the value of accuracy drop AD(MB→MP ), larger is
the inﬂuence of poisoning on the global model. Clearly, perform-
ing targeted attacks is more difﬁcult than random attacks, as the
attacker has to poison the training data with a speciﬁc goal. It is
even harder to perform targeted attacks in collaborative learning
where each user contributes a small portion of data towards the en-
tire training dataset. Since all the previous poisoning attacks are
performed in direct collaborative machine learning setting [42], it
is not well-understood if targeted attacks are equally effective when
the users submit only their masked features. As a ﬁrst step towards
this direction, we explore the effectiveness of targeted poisoning at-
tacks at various levels of poisoning in indirect collaborative learn-
ing and study the efﬁcacy of these attacks on deep learning systems.
Defense Solution. As a preventive measure, we design a defense
for poisoning attacks in the indirect collaborative learning setting.
To thwart these poisoning attacks, the server should distinguish ma-
licious users among all the participants and exclude the features
contributed by these users while training the global model. This
detection and elimination strategy ensures that the global model is
not inﬂuenced due to the poisoned data. Thus, any server that em-
ploys this defense before computing the global model can guaran-
tee a robust and accurate model even under attack by a fraction f of
malicious users. However, the challenge in designing this defense
lies in correctly detecting the malicious users. One method is to
observe the labels of samples of all users and detect discrepancies
between them, as suggested by previous work [30]. The difﬁculty
of identifying malicious users escalates when the server does not
have access to the entire original training data but can only observe
the masked features of the data. In this work, we investigate the
problem of designing AUROR— a defense that thwarts poisoning
attacks in indirect collaborative learning setting without access to
training data. A global model trained using AUROR MA is robust
510(cid:6)(cid:22)(cid:17)(cid:25)(cid:22)(cid:21)(cid:14)(cid:13)(cid:1)(cid:10)(cid:21)(cid:13)(cid:1)
(cid:11)(cid:14)(cid:21)(cid:17)(cid:16)(cid:21)(cid:1)
(cid:15)(cid:14)(cid:10)(cid:26)(cid:29)(cid:24)(cid:14)(cid:25)(cid:1)
(cid:2)(cid:21)(cid:10)(cid:19)(cid:31)(cid:32)(cid:14)(cid:1)
(cid:20)(cid:10)(cid:25)(cid:18)(cid:14)(cid:13)(cid:1)
(cid:15)(cid:14)(cid:10)(cid:26)(cid:29)(cid:24)(cid:14)(cid:25)(cid:1)
(cid:3)(cid:17)(cid:21)(cid:10)(cid:19)(cid:1)(cid:16)(cid:19)(cid:22)(cid:11)(cid:10)(cid:19)(cid:1)
(cid:20)(cid:22)(cid:13)(cid:14)(cid:19)(cid:1)
(cid:8)(cid:24)(cid:10)(cid:17)(cid:21)(cid:1)(cid:1)
(cid:16)(cid:19)(cid:22)(cid:11)(cid:10)(cid:19)(cid:1)
(cid:1)(cid:20)(cid:22)(cid:13)(cid:14)(cid:19)(cid:1)
(cid:4)(cid:13)(cid:14)(cid:21)(cid:27)(cid:15)(cid:31)(cid:1)
(cid:20)(cid:10)(cid:19)(cid:17)(cid:12)(cid:17)(cid:22)(cid:29)(cid:25)(cid:1)
(cid:29)(cid:25)(cid:14)(cid:24)(cid:25)(cid:1)
(cid:2)(cid:9)(cid:7)(cid:5)(cid:7)(cid:1)
(cid:4)(cid:13)(cid:14)(cid:21)(cid:27)(cid:15)(cid:31)(cid:1)
(cid:17)(cid:21)(cid:13)(cid:17)(cid:12)(cid:10)(cid:27)(cid:30)(cid:14)(cid:1)
(cid:15)(cid:14)(cid:10)(cid:26)(cid:29)(cid:24)(cid:14)(cid:25)(cid:1)
(cid:2)(cid:25)(cid:25)(cid:17)(cid:16)(cid:21)(cid:1)
(cid:25)(cid:29)(cid:25)(cid:23)(cid:17)(cid:12)(cid:17)(cid:22)(cid:29)(cid:25)(cid:1)
(cid:16)(cid:24)(cid:22)(cid:29)(cid:23)(cid:25)(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
Figure 2: The design details and steps involved in AUROR. AU-
ROR takes the masked features from users as input and pro-
duces a the ﬁnal accurate global model.
and effective if the attack success rate SRMA and the accuracy
drop AD(MB→MA) of MA are small enough to be acceptable for
practical purposes.
Research Questions. In summary, we aim to answer the following
research questions:
• RQ1: What is the impact of targeted poisoning attacks in
• RQ2: Can we effectively thwart poisoning attacks on deep
learning systems without having access to the entire training
dataset?
indirect collaborative deep learning?
2.4 Our Approach
(cid:3)
(cid:3)
(cid:3)
We propose AUROR— a defense against poisoning attack in in-
direct collaborative learning setting that can detect malicious users
with high accuracy, thereby building a robust global model. AU-
ROR does not need access to raw dataset and computes only on
submitted features. In designing AUROR, we highlight several key
observations that allow us to detect the malicious users.
Key Observations. Our ﬁrst observation is that whatever be the
underlying distribution δ of the training data, the distribution of the
preserves the distribution in the benign setting,
masked features δ
δ ∼ δ
. The second observation is that poisoning on the train-
ing data directly affects the distribution pattern of some speciﬁc
masked features referred to as indicative features. Let i be an in-
(i) within statis-
dicative feature then in an attack setting δ(i) (cid:2) δ
tical error bounds. Thus, in an attack scenario, indicative features
from majority of honest users will exhibit a similar distribution
while those from malicious users will exhibit an anomalous dis-
tribution. As long as the fraction of malicious users is within range
(  1) is represented as,
xi = fi(Wixi−1 + bi)
where Wi, bi and fi(·) are weight matrix, bias vector and activa-
tion function respectively for layer i. The bias vector bi enhances
the representation capability of the network.
In its absence, the
model will be restricted in its expressiveness. The weight matrix
and bias vector are denoted together as parameter P. This parame-
ter P represents the contribution from each input value towards the
learnt model. Note that, for the ﬁrst input layer, the output vector
is same as the raw input data.
To determine the parameter P, multilayer perceptron uses back-
propagation technique with the gradient descent method [37]. The
gradient descent technique calculates the gradient value over the
entire training data in single iteration. However, this method does
not scale to large training datasets. Alternatively, stochastic gradi-
ent descent [18] divides the whole training data into small subset of
training data called min-batch, and trains the model on each min-
batch. A single iteration of stochastic gradient descent operates
over the min-batches of the entire dataset. For example, if there
100 samples in the complete dataset and the min-batch size is 10
samples, then one iteration trains on 10 min-batches.
Convolutional Neural Network. The traditional MLP model has
very high dimension because of the fully connected network and
hence does not scale well in terms of performance. The convolu-
tional neural networks (CNN) model is a type of multilayer percep-
tron model which requires minimal amount of processing because
of its structure. In a CNN model, the layers in the network are not
necessarily fully connected. This structure reduces the number of
parameters used to train the model thereby making the training pro-
cess faster. For layers that are not fully connected, each node only
connects with a small region of the nodes in previous layer, like a
3 × 3 region. This region is called as local receptive ﬁeld for the
node.
In addition to decreasing the number of parameters, when sliding
the local receptive ﬁeld across all input values with certain stride,
we will apply the node with same weights and bias though all lo-
cal receptive ﬁeld, which is called as parameter sharing scheme.
This scheme is based on an assumption that if a feature is useful in
one spatial region, then it should also be valuable for other regions.
The shared parameters are deﬁned as a kernel or ﬁlter. To enhance
the representation capability of network, we increase the number of
nodes in the convolutional layer, which is also called as the depth
of the layer. Pooling layer is another periodically existed layer in
CNN after successive convolutional layers. Most commonly, we
use the MAX pooling layer with ﬁlters of size 2 × 2. Pooling layer
can progressively reduce the amount of parameters and computa-
tion for training the network.
In this paper, we use both the traditional MLP and CNN model,
which are widely used in deep learning, to perform poisoning at-
tacks and apply our defense on them.
3.2 Privacy Preserving Deep Learning
For performing attacks, we select the privacy-preserving model
for image recognition using deep learning proposed by Shokri et
al. [39], which is a state-of-the-art system that provides indirect
collaborative learning. The model uses stochastic gradient descent
method to compute gradient values. Each user trains its own model
based on its training dataset and generates the gradient value for
each iteration. They mask these gradient values using the noise
generated from Laplace distribution, making the local output dif-
ferential private before submitting to the server. We refer to these
masked gradient values as masked features.
The server collects the masked gradient values from all the users
and aggregates them to update the global model. After updating
the global model, the server generates an updated set of parame-
ters from the global model that capture the features from the entire
training dataset. The users download these updated parameters and
provide them as input to their local model, thus generating differ-
ent gradient values. This process is repeated several times until the
global model stabilizes.
4. TARGETED POISONING ATTACKS
We study the efﬁcacy of targeted poisoning attacks using two im-
age recognition tasks in indirect collaborative learning. The image
recognition system uses deep learning algorithm as its underlying
technique to train a global model. The ﬁnal global model classiﬁes
the test images into a given set of categories. In this system, poi-
soning attacks aim to classify a source test image as a target image.
We perform our attacks with the following goals:
a reasonable attack success rate
• To understand the amount of poisoning necessary to achieve
• To understand the difﬁculty levels for performing targeted
• To understand the impact of poisoning attacks on the accu-
attacks with different goals
racy of the global model
4.1 Handwritten Digit Images
Dataset. We use the MNIST dataset of handwritten digits [24],
a popular benchmark for training and testing deep learning mod-
els [38]. The dataset provides 60,000 training samples and 10,000
test samples where around 1000 test samples correspond to each
digit. Each image has 28 × 28 pixels, which is ﬂattened into a
vector with 784 features and given as input to the deep learning
model. To assign a value for each feature, integer numbers from
0 to 255 that represent different shades of grey are transformed to
ﬂoating point values. Hence, each feature has a ﬂoating point value
between 0 and 1.
Network Architecture. We use a simple multilayer perceptron
neural network to train the classiﬁer for handwritten digits. There
are only two hidden layers using Tanh function as its activation
512(cid:3)
(cid:4)
(cid:5)
(cid:6)
(cid:7)
(cid:8)
(cid:9)