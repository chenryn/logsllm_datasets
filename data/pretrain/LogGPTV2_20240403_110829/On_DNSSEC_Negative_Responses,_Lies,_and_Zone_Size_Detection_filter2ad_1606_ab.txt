Table 1. Distance weights for zone size detection using decile divisions (i.e., q = 10).
Decile (i)
Weight (wi) 0.41 0.15 0.10 0.08 0.07 .05 .05 0.04 0.03 0.02
10
1
2
3
4
5
6
7
8
9
238
J. Demke and C. Deccio
The result of this weighted approach is that the NSEC3 records with larger
distances—which are more likely to cover an arbitrary q-nxdomain query but
are less representative of the zone’s NSEC3 distances—contribute less to the aver-
age than NSEC3 with small distances—which are less likely to cover an arbitrary
q-nxdomain query and are more representative of the zone’s NSEC3 distances.
4.3 Validation
To test the validity of our zone size detection methodology, we issued 1,000 trials,
each consisting of 18 q-nxdomain queries, for each of the zones we created. The
zones were served locally on a BIND DNS server. For each trial, the 18 queries
yielded a total of 20 NSEC3 records, which comprised N; in addition to the 18
NSEC3 records covering the unique names queried, every response included the
NSEC3 record that covers the wildcard record and the NSEC3 record matching
the zone name [23].
First, we investigated the accuracy of our methodology using diﬀerent quan-
tile (q) values. Speciﬁcally, we evaluated the 1,000 trials against the zone of size
10,000 using quantile values of 20, 10, and 5. We measured accuracy in terms of
percentage of error based on the actual zone size, i.e.,
z(cid:4) − z
z
e =
(5)
Thus, values of e closer to 0 indicate higher accuracy of zone size prediction,
e  0 indicates a high guess (z(cid:4) > z).
The results are shown in Fig. 4 as a CDF.
Fig. 4. Error (e) for size prediction of a DNS zone of size 106 for various values of q.
On DNSSEC Negative Responses, Lies, and Zone Size Detection
239
Weighting the NSEC3 distances, by any quartile value (Eq. 4) signiﬁcantly
improved the accuracy from the zone size estimates based on unweighted dis-
tance averages (Eq. 3). Even the highest zone size estimates calculated using
unweighted averages were lower than the actual zone size, with the median error
being about 48% low. In contrast, for about 60% of the trials (between the 30
and 90 percentiles) for q = 10 and q = 20, z(cid:4) was within 15% of z. And for about
30% of the trials (between the 50 and 80 percentiles), the z(cid:4) was within 7% of z.
Because the error for q = 10 and q = 20 were comparable, and q = 10 requires
fewer queries to have at least one NSEC3 record in every quantile, we use q = 10
for the remainder of our experiments.
We next tested the accuracy of zone size prediction against zones of diﬀerent
sizes, the results of which are shown in Table 2 and Fig. 5. Consistent among
the zones of all sizes was that z(cid:4) was low more often than not, with the median
values of e ranging between −6% and −16%. For zones smaller than 100,000,
75% or more of the trials had error values that were within 20% of the size of
the zone.
Fig. 5. Error (e) for zone size prediction of DNS zones of various sizes using q = 10.
Table 2. Statistics for zone size prediction of DNS zones of various sizes using q = 10.
Zone size
Median error
Fraction of trials −0.20 ≤ e ≤ 0.20
104
103
102
−0.06 −0.05 −0.07 −0.16 −0.11
0.70
0.77
106
105
0.78
0.76
0.61
240
J. Demke and C. Deccio
5 DNS Zone Size Measurement Study
To perform our DNS zone size measurement on deployed DNS zones, we analyzed
2,182,987 DNSSEC-signed zones to determine the strategy they employ for nega-
tive responses. This would allow us to identify our candidate DNS zones. The list
of zones consisted of DNSSEC-signed second-level domains extracted from the
zone ﬁles for 821 top-level domains (TLDs). The TLD zone ﬁles themselves were
obtained from the following sources: Verisign’s Zone File Access [6]; the Cen-
tralized Zone Data Service (CZDS) [2]; the Public Interest Registry (PIR) [5];
the Internet Foundation in Sweden (IIS) [4]; and Domains Index [3], from which
we acquired domains under gov. DNSSEC-signed domains were identiﬁed as
those with at least one delegation signer (DS) record in the TLD zone ﬁle. The
breakdown of the domains and their TLD are shown in Table 3. Nearly 80% of
the zones analyzed were under the com and se TLDs. This was because of the
signiﬁcant presence of DS records in those domains.
Table 3. Breakdown of domains analyzed, both by TLD and by detected negative
response type.
TLD Zones analyzed Traditional Traditional White lies Black lies Unclassiﬁed
NSEC
com
se
net
nu
org
app
911,576 (42%) 112,168
802,198 (37%) 77,549
127,545 (6%)
14,390
118,158 (5%)
9,508
95,319 (4%)
9,252
33,254 (2%)
492
Other 94,937 (4%)
17,686
NSEC3
725,521
147,294
103,136
33,801
79,557
7,223
70,687
NSEC3
18,879
NSEC
36,501
539,178
408
2,762
5,089
66,690
74
2,214
2,964
25,232
219
2,136
Total 2,182,987
(100%)
241,045
(11%)
1,167,219
(53%)
657,091
(30%)
2,804
48,059
(2%)
17,823
37,072
1,920
7,623
1,076
33
1,099
66,646
(3%)
5.1 Zone Analysis
For each zone in our data set, we identiﬁed the authoritative servers using DNS
lookups for the NS (name server) records and the corresponding A and AAAA
(IPv4 and IPv6 address) records. Having the set of IP addresses for servers
authoritative for the domains, we issued three queries to every authoritative
server: a q-nxdomain query, a q-nodata query, and a q-nodata-type query. The
three queries were intended to elicit diﬀerent types of negative response behavior,
including any of the following:
– NXDOMAIN: a response indicating that the name queried name doesn’t exist.
– wildcard: a response synthesized from a wildcard, with NSEC or NSEC3 records
to indicate that the queried name didn’t exist (as speciﬁed by DNSSEC [23]).
On DNSSEC Negative Responses, Lies, and Zone Size Detection
241
– NODATA: a response indicating that the name exists, but with no records cor-
responding to the type queried [7].
The expected response for q-nxdomain was either an NXDOMAIN or wildcard
response, and the expected response for q-nodata and q-nodata-type was NODATA.
Under DNSSEC requirements, all such responses would include NSEC or NSEC3
records. Table 3 shows the breakdown of response strategies observed by author-
itative servers: traditional NSEC, traditional NSEC3, white lies with NSEC3, and
black lies with NSEC. If at least one of the query responses matched a given nega-
tive response strategy, then the zone was included in the count for that strategy.
We note that for a very small (less than 1%) percentage of the zones analyzed,
we observed several diﬀerent negative response behaviors, such that they are
represented in multiple categories. For example, for some zones, white lies was
used in response to our q-nxdomain, but NSEC records were returned in response
to the q-nodata-type. Also, for 3% of the DNS zones we analyzed, none of our
queries resulted in NSEC or NSEC3 records, so their negative response strategy
remained unclassiﬁed.
The responses for the unclassiﬁed zones fell into several categories. Some
of the q-nxdomain queries yielded non-wildcard positive responses (i.e., indi-
cating that the record existed), the result of server-side record synthesis with
online-signing. This method is employed by organizations in an eﬀort to not
even disclose the fact that the response is a wildcard—which would otherwise be
apparent. Some responses lacked NSEC or NSEC3 records due to misconﬁguration.
For example, a DS record existed, but the zone was actually not DNSSEC-signed,
or the response had response code SERVFAIL.
We observed nearly one-third of the zones employed white lies, while just over
half used traditional NSEC3. About 11% of zones were signed with traditional
NSEC, while about 2% of zones used black lies. The combined presence of white
lies and black lies implied that a minimum of 32% of the zones we analyzed
employed a online signing.
5.2 Detecting Zone Size in the Wild
We tested our zone detection methodology in the wild by issuing 20 queries to
each of the zones in our dataset that were signed with plain NSEC3, i.e., without
white lies. The results of this measurement are shown in Fig. 6. We found that
85% of the zones we probed were so small that even with only 20 queries, we
received fewer than 10 unique NSEC3 records, which is the minimum size of N
necessary to apply our methodology. The fact that NSEC3 records were being
returned multiple times with these zones was evidence that the zone was small,
and was—quite likely—being completely enumerated with our small number of
queries. Thus, for |N| < 10, we simply use z(cid:4) = |N| as our zone size estimate.
For the zones we measured, 99% were smaller than 40, but the top 1% reached
up to nearly four million.
242
J. Demke and C. Deccio
Fig. 6. Estimated zone sizes (z(cid:2)
) for the NSEC3-signed zones in our data set (Table 3).
6 Conclusion
In this paper, we have presented methodology for learning the size of a DNS
zone by issuing relatively few DNS queries. We demonstrated the accuracy of
our technique in a lab environment and showed that in approximately 75% of
cases, the methodology would yield a an estimate that is within 20% of the
actual zone size, with only 18 queries. We deployed this methodology on over
one million NSEC3 zones in our data set and learned that most of these zones are
small, with 85% having fewer than 10 domain names.
As part of our study, we measured some of the DNSSEC negative response
behaviors currently deployed. We learned that the most popular negative
response strategy deployed in our data set is traditional NSEC3, which is used
by 53% of zones, and makes them candidates for DNS zone size estimation,
using our methodology. Privacy-preserving strategies such as NSEC3 with white
lies and NSEC with black lies are also gaining some traction with 30% and 2%
deployment, respectively.
The techniques presented in this paper serve as a general purpose tool to
better understand the DNS ecosystem, in terms of the size of deployed DNS
zones, speciﬁcally those signed with NSEC3. It also provides a new insight into
information disclosure, regardless of how innocuous the revealing of the size of
DNS zone might be to an organization. This knowledge can only beneﬁt and
empower the designers, maintainers, and users of the Internet.
On DNSSEC Negative Responses, Lies, and Zone Size Detection
243
References
1. BIND open source DNS server. https://www.isc.org/downloads/bind/
2. Centralized Zone Data Service. https://czds.icann.org/
3. Domains index. https://domains-index.com/
4. The Internet Foundation in Sweden. https://www.iis.se/
5. Public Interest Registry. https://pir.org/
6. Verisign. https://www.verisign.com/
7. Andrews, M.: RFC 2308: negative caching of DNS queries (DNS NCACHE), March
1998
8. Arends, R., Austein, R., Larson, M., Massey, D., Rose, S.: RFC 4033: DNS security
introduction and requirements, March 2005
9. Arends, R., Austein, R., Larson, M., Massey, D., Rose, S.: RFC 4034: resource
records for the DNS security extensions, March 2005
10. Bird, S., Loper, E., Klein, E.: Natural Language Processing with Python. O’Reilly
Media Inc., Sebastopol (2009)
11. Deccio, C., Chen, C.C., Mohapatra, P., Sedayao, J., Kant, K.: Quality of name res-
olution in the domain name system. In: 2009 17th IEEE International Conference
on Network Protocols, October 2009
12. DNSCurve: DNSCurve: Usable security for DNS. http://dnscurve.org/nsec3walker.
html
13. Elz, R., Bush, R.: RFC 2181: clariﬁcations to the DNS speciﬁcation, July 1997
14. Gardiner, C.: Stochastic Methods: A Handbook for the Natural and Social Sciences.
Springer, Heidelberg (2009)
15. Goldberg, S., Naor, M., Papadopoulos, D., Reyzin, L., Vasant, S., Ziv, A.: NSEC5:
provably preventing DNSSEC zone enumeration. In: NDSS 2015, February 2015
16. Grant, D.: Economical with the truth: making DNSSEC answers cheap. https://
blog.cloudﬂare.com/black-lies/
17. Josefsson, S.: RFC 4648: the base16, base32, and base64 data encodings, October
2006
18. Kaminsky, D.: Phreebird. https://dankaminsky.com/phreebird/
19. Mockapetris, P.: RFC 1034: domain names - concepts and facilities, November 1987
20. Mockapetris, P.: RFC 1035: domain names - implementation and speciﬁcation,
November 1987
21. Osterweil, E., Ryan, M., Massey, D., Zhang, L.: Quantifying the operational status
of the DNSSEC deployment. In: Proceedings of the 6th ACM/USENIX Internet
Measurement Conference (IMC 2008), October 2008
22. Ramasubramanian, V., Sirer, E.G.: Perils of transitive trust in the domain name
system. In: IMC 2005 Proceedings of the 5th ACM SIGCOMM Conference on
Internet Measurement, October 2015
23. Sisson, G., Arends, R., Blacka, D.: RFC 5155: DNS security (DNSSEC) hashed
authenticated denial of existence, March 2008
24. Wander, M., Schwittmann, L., Boelmann, C., Weis, T.: GPU-based NSEC3 hash
breaking. In: 2014 IEEE 13th International Symposium on Network Computing
and Applications. IEEE, August 2014