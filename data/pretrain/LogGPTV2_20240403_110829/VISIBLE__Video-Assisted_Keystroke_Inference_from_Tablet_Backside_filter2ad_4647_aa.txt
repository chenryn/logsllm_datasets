title:VISIBLE: Video-Assisted Keystroke Inference from Tablet Backside
Motion
author:Jingchao Sun and
Xiaocong Jin and
Yimin Chen and
Jinxue Zhang and
Yanchao Zhang and
Rui Zhang
VISIBLE: Video-Assisted Keystroke Inference from
Tablet Backside Motion
Jingchao Sun∗, Xiaocong Jin∗, Yimin Chen∗, Jinxue Zhang∗, Rui Zhang†, and Yanchao Zhang∗
{jcsun, xcjin, ymchen, jxzhang, yczhang}@asu.edu
∗ Arizona State University
† University of Hawaii
PI:EMAIL
Abstract—The deep penetration of tablets in daily life has
made them attractive targets for keystroke inference attacks that
aim to infer a tablet user’s typed inputs. This paper presents
VISIBLE, a novel video-assisted keystroke inference framework
to infer a tablet user’s typed inputs from surreptitious video
recordings of tablet backside motion. VISIBLE is built upon
the observation that the keystrokes on different positions of
the tablet’s soft keyboard cause its backside to exhibit different
motion patterns. VISIBLE uses complex steerable pyramid de-
composition to detect and quantify the subtle motion patterns of
the tablet backside induced by a user’s keystrokes, differentiates
different motion patterns using a multi-class Support Vector
Machine, and reﬁnes the inference results using a dictionary
and linguistic relationship. Extensive experiments demonstrate
the high efﬁcacy of VISIBLE for inferring single keys, words,
and sentences. In contrast to previous keystroke inference attacks,
VISIBLE does not require the attacker to visually see the tablet
user’s input process or install any malware on the tablet.
I.
INTRODUCTION
The past few years have witnessed the proliferation of
tablets in everyday life. According to a Gartner report [1],
global tablet shipments will reach 321 million and surpass
PC shipments in 2015. Being lighter than laptops and having
larger touchscreens than smartphones, tablets perfectly ﬁll the
gap between laptops and smartphones and have become an
indispensable category of mobile computing devices. People
are increasingly using tablets in every aspect of life, including
voice/video communications, Internet browsing, web transac-
tions, online banking, reading, multimedia playing, etc.
The deep penetration of tablets in people’s daily life
has made them attractive targets for various keystroke in-
ference attacks that aim to infer a user’s typed inputs (such
as usernames, passwords, SSNs, and emails) on the tablet
touchscreen. Although existing authentication schemes [2]–[5]
can prevent unauthorized access to mobile devices, prior work
has shown that an attacker can successfully infer the PIN or
even the words entered on the soft (tablet) keyboard by sur-
reptitiously video-recording a target user’s input process and
Permission to freely reproduce all or part of this paper for noncommercial
purposes is granted provided that copies bear this notice and the full citation
on the ﬁrst page. Reproduction for commercial purposes is strictly prohibited
without the prior written consent of the Internet Society, the ﬁrst-named author
(for reproduction of an entire paper only), and the author’s employer if the
paper was prepared within the scope of employment.
NDSS ’16, 21-24 February 2016, San Diego, CA, USA
Copyright 2016 Internet Society, ISBN 1-891562-41-X
http://dx.doi.org/10.14722/ndss.2016.23060
then analyzing the reﬂection of the touchscreen, spatial hand
dynamics, or the relative ﬁnger positions on the touchscreen
[6]–[13]. These studies commonly assume that the attacker can
capture a user’s interaction with the touchscreen with little or
no visual obstruction, which greatly limits the applicability of
these attacks.
In this paper, we propose VISIBLE, a novel video-assisted
keystroke inference framework that allows an attacker to infer
a tablet user’s typed inputs on the touchscreen by recording
and analyzing the video of the tablet backside during the
user’s input process. VISIBLE is motivated by our observation
that the keystrokes on different positions of the tablet’s soft
keyboard cause its backside to exhibit different motion pat-
terns. In contrast to previous keystroke inference techniques
[6]–[13], VISIBLE does not require the attacker to visually
see the victim’s input process and thus enables much more
surreptitious keystroke inference from a distance.
The design of VISIBLE faces two major challenges. First,
the backside motion caused by user keystrokes is very subtle
and requires effective methods to detect and quantify. Second,
since the soft keyboard on the tablet is much smaller than
a normal keyboard, the motion patterns caused by tapping
adjacent keys are close, making accurate differentiation partic-
ularly challenging. To tackle these two challenges, VISIBLE
uses complex steerable pyramid decomposition to detect and
quantify the subtle keystroke-induced motion patterns of the
tablet backside, differentiates different motion patterns using a
multi-class Support Vector Machine, and reﬁnes the inference
results using a dictionary and linguistic models.
We thoroughly evaluate the performance of VISIBLE via
comprehensive experiments on an Apple iPad 2 tablet and
a Google Nexus 7 tablet. Our experiment results show that
VISIBLE can infer a single key entered on the alphabetical
soft keyboard with an average accuracy of 36.2% and that the
correct key is within the inferred key’s one-hop and two-hop
neighbors with probabilities 83.6% and 97.9%, respectively.
Similarly, VISIBLE achieves an accuracy of 38% for single-
key inference on the PIN soft keyboard, and the correct key is
within the inferred key’s one-hop neighbors with probability
68%. For word inference, VISIBLE can produce a list of
candidate words, and the correct word is in the top-5, top-
10,
top-25, and top-50 candidate words with probabilities
48.0%, 63.0%, 77.8%, and 92.6%, respectively. We also show
that the attacker can successfully infer typed sentences based
on the linguistic relationship between adjacent words. These
experiment results conﬁrm the high efﬁcacy of VISIBLE.
The rest of this paper is organized as follows. Section II
presents the related work. Section III introduces some back-
ground knowledge for video processing. Section IV describes
the adversary model. Section V details the design of VISIBLE.
Section VI evaluates VISIBLE through extensive experiments.
Section VII concludes this paper and discusses possible coun-
termeasures and future work.
II. RELATED WORK
In this section, we brieﬂy introduce the prior work most
related to VISIBLE. Prior keystroke inference attacks can be
broadly classiﬁed into two categories: video-based attacks and
sensor-based attacks.
a) Video-based Attacks: In this category, the adversary
uses video-based side channels in combination with computer
vision techniques to infer a user’s typed inputs. Early work
along this line focuses on physical keyboards. Backes et al.
[6], [7] exploited the reﬂections of screens on glasses, tea
pots, spoons, plastic bottles, eyes of the user, walls, and even
the user’s clothes to recover the content displayed on the
computer monitor. Balzarotti et al. [8] introduced an attack
that automatically recovers the typed text solely from a video
of the user typings by analyzing the light diffusion surrounding
the key change. This attack requires a camera to directly record
the ﬁnger typings on the physical keyboard.
There have also been some video-based attacks on the soft
keyboards of touchscreen mobile devices. In [9], Maggi et al.
presented an attack that automatically recognizes typed inputs
from the key magniﬁcations of touchscreen mobile devices.
Raguram et al. [10] showed how to automatically reconstruct
the text input on a soft keyboard from the reﬂection of the
device’s screen on the victim’s sunglasses. Xu et al. [11]
introduced an attack to accurately reconstruct the text input
on a mobile device by tracking the positions of the victim’s
ﬁngers as they move across the soft keyboard. In [12], Yue
et al. showed how to infer the user input even if neither text
nor popup can be seen from the video of user typings. This
attack exploits the homographic relationship between touching
images and a reference image showing a soft keyboard. All
these attacks require the attacker to acquire a video capturing
the victim’s typings on the touchscreen or the touchscreen
reﬂection.
Our work is most related to [13], in which Shukla et al.
introduced a video-based attack on the PIN-entry process of
a smartphone that decodes the typed PIN by exploiting the
spatiotemporal dynamics of the hands during typing. Both
VISIBLE and the attack proposed in [13] only require the
attacker to video-record the backside of a smartphone, which
was considered safe previously. VISIBLE, however, has much
wider applicability than [13]. In particular, the attack intro-
duced in [13] requires the attacker to record the victim’s hand
movements during the PIN-entry process, which is not always
possible. For example, the victim’s hand movements are very
likely to be obscured by the tablet itself. In contrast, VISIBLE
works even if the victim’s hand movements are not visible
from the video of the device backside.
b) Sensor-based Attacks: Tremendous efforts have been
made on inferring user inputs on mobile devices from the data
generated by various on-board sensors. It has been shown in
[14] and [15] that the user’s password can be inferred from
the smartphone’s accelerometer data. Moreover, some recent
work [16], [17] demonstrated a similar attack that exploits the
data from both accelerometer and gyroscope. Other on-board
sensors that have been exploited include microphones and front
cameras [18], [19]. All these work require the attacker to obtain
sensor data via either malicious applications (e.g., malicious
Apps or web scripts) or unprotected network transmissions,
which limit
their applicability. In contrast, VISIBLE only
requires the attacker to record the video of the tablet backside
during the victim’s typing process, which is both easier to
launch and more difﬁcult to detect.
Also related is the work on using on-board sensors to
infer the keystrokes of nearby physical keyboards. In [20],
Zhuang et al. showed how to recover typed keys from sound
recordings of a user’s typings on a physical keyboard. Berger
et al. [21] presented another attack that infers the user input
from the acoustic emanations of the physical keyboard with
the assistance of a dictionary. A similar attack was presented
in [22], which also uses acoustic emanations of the physical
keyboard but does not need a language model or dictionary. In
[23], the authors demonstrated an attack that infers the typed
keys of a physical keyboard from the vibration caused by each
keystroke detected by a nearby smartphone’s accelerometer.
Such attacks, although effective, can only be used when the
attacker is near the victim due to the short transmission range
of acoustic and vibration signals. In contrast, VISIBLE can be
launched from a much larger distance.
III. VIDEO PROCESSING BASICS
In this section, we introduce two computer vision tech-
niques, phase-based optical ﬂow estimation and complex steer-
able pyramid decomposition, underlying VISIBLE.
A. Phase-based Optical Flow Estimation
An optical ﬂow refers to apparent motion patterns of
image objects between two consecutive frames caused by the
object or camera movement. Optical ﬂow estimation is the
process of characterizing and quantifying the object motions
in a video stream, often for motion-based object detection
and tracking systems. Phase-based optical ﬂow is a popular
optical ﬂow estimation technique which estimates the motion
ﬁeld using phase information. For example, constant phase
contours are tracked by computing the phase gradient of
spatiotemporally bandpassed images, which provides a good
approximation to the motion in [24]. As another example,
Gautama et al. [25] proposed to estimate motion by computing
the temporal gradient of the phases of a partially bandpassed
video. In comparison with other ﬂow estimation techniques,
phased-based estimation methods are more robust to smooth
shading, lighting variations, and small deviations from image
translations.
B. Complex Steerable Pyramid Decomposition
Steerable pyramid decomposition [26] is a standard tech-
nique that decomposes an image according to spatial scale,
orientation, and position to capture the variance of a texture in
both intensity and orientation, which has been widely used in
image processing and motion detection. Since an image may
2
(a) An iPad and a holder.
(b) Attack scenario.
(c) The same attack scenario from a different angle.
Fig. 1. Examples of a tablet holder and an attack scenario.
sentences. This enables VISIBLE to infer any sensitive typed
input on the tablet, such as usernames, passwords, and emails.
The victim is alert to shoulder-surﬁng attacks in the sense
that the attacker cannot get too close to the victim during his
typing process. In addition, the attacker is unable to see the
tablet touchscreen or the victim’s hand movement during his
typing process from any direction. Moreover, we assume that
the attacker cannot obtain the sensor data by running malware
such as Trojans or malicious web scripts on the victim’s tablet.
These assumptions make previous video-based attacks [6]–[13]
and sensor-based attacks [14]–[23] inapplicable.
We assume that the attacker has the following capabilities.
First, the attacker can use camcorders with advanced lens to
record the backside of the victim’s tablet during his input
process, possibly from a long distance. Second, the attacker
can record the attack scenario and reconstruct it afterwards.
Speciﬁcally, the attacker can measure the angle between the
victim’s tablet and the desk, the angle between the tablet and
the camcorder, and the distance between the tablet and the
camcorder by analyzing multiple images taken from different
angles and positions using distance and angle estimation
algorithms [29]. Finally, the attacker has the same holder and
the tablet with the same soft keyboard layouts as the victim’s.
V. VISIBLE FRAMEWORK
In this section, we give an overview of VISIBLE and then
detail each step of the attack.
A. VISIBLE Overview
VISIBLE infers the victim’s typed inputs from the video
of tablet backside motion. The high-level design of VISIBLE
is shown in Fig. 3, which consists of eight steps as follows.
1)
2)
Video Recording and Preprocessing: In this step, we
record a video capturing the motion of the tablet
backside during the victim’s typing process. We as-
sume neither the touchscreen nor the victim’s hand
movement can be seen from the video. We crop the
video clip to keep the text-input part only.
Areas of Interests (AOIs) Detection and Selection:
In this step, we detect all
the areas with texture
information on the tablet backside and select a few
areas as AOIs for further processing. Exemplary AOIs
are the buttons, camera, loudspeaker, logo, and texts
on the tablet backside.
(a) Alphabetical keyboard.
(b) PIN keyboard.
Fig. 2. Alphabetical and PIN soft keyboard illustrations.
contain multiple objects of different sizes, and these objects
may contain features of different sizes and be at different
distances from the viewer, any analysis procedure that is only
applied at a single scale may lose information at other scales.
To simultaneously detect multiple objects’ motion patterns,
analysis need be carried out at different scales simultaneously
[27]. In addition, the same object may also exhibit totally
different features in different orientations. To comprehensively
analyze the features of an object and detect its motion pattern,
it is necessary to decompose it in different orientations.
Complex steerable pyramid decomposition [28] extends the
original steerable pyramid decomposition by representing an
image in a complex form comprising real and imaginary parts.
In comparison with steerable pyramid decomposition, complex
steerable pyramid decomposition additionally measures local
phase and energy in some texture descriptors. Such measures
have proved important
throughout computer vision. Using
complex steerable pyramid decomposition, we can obtain the
phase and amplitude of each pixel of an image at each spatial
scale and orientation over time.
IV. ADVERSARY MODEL
We consider a victim user with a tablet such as iPad 2 or
Nexus 7. We assume that the victim places the tablet on a tablet
holder (e.g., the one shown in Fig. 1(a)) on a desk and types on
a soft keyboard. Such scenarios are very common in daily life,
e.g., in conferences or seminars where researchers take notes
or write emails. We focus on two types of soft keyboards in
this paper, the alphabetical and PIN keyboards, as shown in
Fig. 2. The extension of VISIBLE to the alphanumeric soft
keyboard is left as future work.
We assume that an attacker intends to infer any typed input
on the victim’s tablet, which can be single keys, words, or
3
Fig. 3. The VISIBLE framework.
3)
AOI Decomposition: In this step, we decompose each
selected AOI in each frame using complex steerable
pyramid decomposition.