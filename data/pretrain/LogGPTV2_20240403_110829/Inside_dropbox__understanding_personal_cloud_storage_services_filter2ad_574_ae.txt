ation of the Dropbox protocol, notiﬁcation connections are
re-established immediately after that.
Most devices in Home 1 , Home 2 and Campus 2 stay con-
nected up to 4 hours in a single session. In Campus 1 , a sig-
niﬁcantly higher percentage of long-lasting sessions is seen.
This can be explained by the prevalence of workstations in
a typical 8-hours work routine. Inﬂections at the tail of the
distributions are seen in all curves, as a consequence of the
devices kept always on-line.
5.6 Implications
The results in this section help to understand the current
usage of the Dropbox client. This is needed, for instance,
for provisioning data-centers and networks to handle cloud
storage traﬃc. Our analysis of typical user behaviors reveals
that users have diﬀerent interests on the application. For in-
stance, although Dropbox is already installed in more than
6% of the households (see Sec. 3.3), less than 40% of these
households are fully using its functionalities, i.e., synchro-
nizing devices, sharing folders etc. Interestingly, the results
are very similar in both home networks, reinforcing our con-
clusions. The very high amount of traﬃc created by this
limited percentage of users motivates our expectations that
cloud storage systems will be among the top applications
producing Internet traﬃc soon. Geographically dispersed
sources as well as longitudinal data are, however, necessary
to check whether the conclusions of this section can be gen-
eralized, as more people adopt such solutions.
6. WEB STORAGE
In addition to the client application, Dropbox allows users
to access shared folders and ﬁles using both its main Web
interface and a direct link download mechanism. In the fol-
lowing, we analyze the usage of these interfaces. Fig. 17
presents the CDFs of the number of bytes in the storage
ﬂows of the Dropbox main Web interface. Separate CDFs
for uploads and downloads are presented.
491F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Upload
Download
 1
 0.8
 0.6
 0.4
 0.2
 0
Campus 1
Campus 2
Home 1
Home 2
100
1k
10k
100k
1k
10k
100k
1M 10M
Size (bytes)
Size (bytes)
Figure 17: Storage via the main Web interface.
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Campus 1
Home 1
Home 2
100
1k
10k
100k
1M
10M
100M
Download (bytes)
Figure 18: Size of direct link downloads.
Considering the number of uploaded bytes, it becomes
clear that the main Web interface is hardly used for up-
loading content. More than 95% of the ﬂows submitted less
than 10kB. When considering downloaded bytes, up to 80%
of ﬂows exchanged less 10kB. These distributions are, how-
ever, strongly biased toward the SSL handshake sizes for two
reasons: (i) the Dropbox interface retrieves thumbnails from
storage servers using SSL; (ii) Web browsers open several
parallel connections when retrieving those HTTP objects.
The remaining ﬂows have less than 10MB in more than 95%
of the cases, showing that only small ﬁles are normally re-
trieved from this Web interface.
Additionally, we analyze ﬂows related to direct link down-
loads. Note that these ﬂows correspond to 92% of the
Dropbox Web storage ﬂows in Home 1 , conﬁrming that
this mechanism is highly preferred over the main Dropbox
Web interface. Fig. 18 shows the CDF of the size of direct
link downloads.11 Since these downloads are not always en-
crypted, the CDF does not have the SSL lower-bound. Inter-
estingly, only a small percentage of direct link downloads is
bigger than 10MB, suggesting that their usage is not related
to the sharing of movies or archives.
7. CONCLUSIONS
To the best of our knowledge, we are the ﬁrst to analyze
the usage of Dropbox on the Internet. Our analysis assessed
the increasing interest on cloud-based storage systems. Ma-
jor players like Google, Apple and Microsoft are oﬀering the
service. We showed that, in this landscape, Dropbox is cur-
rently the most popular provider of such a system.
By analyzing ﬂows captured at 4 vantage points in Eu-
rope over a period of 42 consecutive days, we showed that
Dropbox is by now responsible for a considerable traﬃc vol-
ume. In one of our datasets, for instance, Dropbox is already
equivalent to one third of the YouTube traﬃc.
We presented an extensive characterization of Dropbox,
both in terms of the system workload as well as the typical
usage. Our main ﬁndings show that the Dropbox service
performance is highly impacted by the distance between the
clients and the data-centers, which are currently located in
the U.S. The usage of per-chunk acknowledgments in the
client protocol combined with the typically small chunk sizes
deeply limits the eﬀective throughput of the service. In this
paper, we identiﬁed two possible improvements to the pro-
tocol: (i) the usage of a chunk bundling scheme; (ii) the
introduction of delayed acknowledgments. We showed that
the recent deployment of a bundling mechanism improved
the system performance dramatically. In addition, we ex-
pect that the overall performance will be improved by the
deployment of other data-centers in diﬀerent locations.
Regarding the typical workload of the Dropbox system,
our analysis showed a variety of user behaviors. For instance,
a considerable number of users take full advantage of the
Dropbox functionalities, actively storing and retrieving ﬁles
and sharing several folders. However, we also noted around
one third of users completely abandoning their clients, sel-
dom exchanging any data during the 42 days of observations.
8. ACKNOWLEDGMENTS
This work has been carried out in the context of the TMA
COST Action IC0703, the EU-IP project mPlane and the
IOP GenCom project SeQual. mPlane is funded by the Eu-
ropean Commission under the grant n-318627. SeQual is
funded by the Dutch Ministry of Economic Aﬀairs, Agricul-
ture and Innovation via its agency Agentschap NL.
9. REFERENCES
[1] A. Bergen, Y. Coady, and R. McGeer. Client
Bandwidth: The Forgotten Metric of Online Storage
Providers. In Proceedings of the 2011 IEEE Paciﬁc
Rim Conference on Communications, Computers and
Signal Processing, PacRim’2011, pages 543–548, 2011.
[2] I. Bermudez, M. Mellia, M. M. Munaf`o,
R. Keralapura, and A. Nucci. DNS to the Rescue:
Discerning Content and Services in a Tangled Web. In
Proceedings of the 12th ACM SIGCOMM Conference
on Internet Measurement, IMC’12, 2012.
[3] M. Cha, H. Kwak, P. Rodriguez, Y.-Y. Ahn, and
S. Moon. I Tube, You Tube, Everybody Tubes:
Analyzing the World’s Largest User Generated
Content Video System. In Proceedings of the 7th ACM
SIGCOMM Conference on Internet Measurement,
IMC’07, pages 1–14, 2007.
[4] N. Dukkipati, T. Reﬁce, Y. Cheng, J. Chu,
T. Herbert, A. Agarwal, A. Jain, and N. Sutin. An
Argument for Increasing TCP’s Initial Congestion
Window. SIGCOMM Comput. Commun. Rev.,
40(3):26–33, 2010.
[5] A. Finamore, M. Mellia, M. Meo, M. M. Munaf`o, and
D. Rossi. Experiences of Internet Traﬃc Monitoring
with Tstat. IEEE Network, 25(3):8–14, 2011.
[6] A. Finamore, M. Mellia, M. M. Munaf`o, R. Torres,
11Uploads are not shown since a single HTTP request is sent.
Campus 2 is not depicted due to the lack of FQDN.
and S. G. Rao. YouTube Everywhere: Impact of
Device and Infrastructure Synergies on User
492Experience. In Proceedings of the 11th ACM
SIGCOMM Conference on Internet Measurement,
IMC’11, pages 345–360, 2011.
[7] M. Gjoka, M. Sirivianos, A. Markopoulou, and
X. Yang. Poking Facebook: Characterization of OSN
Applications. In Proceedings of the First Workshop on
Online Social Networks, WOSN’08, pages 31–36, 2008.
[8] S. Halevi, D. Harnik, B. Pinkas, and
A. Shulman-Peleg. Proofs of Ownership in Remote
Storage Systems. In Proceedings of the 18th ACM
Conference on Computer and Communications
Security, CCS’11, pages 491–500, 2011.
[9] D. Harnik, B. Pinkas, and A. Shulman-Peleg. Side
Channels in Cloud Services: Deduplication in Cloud
Storage. IEEE Security and Privacy, 8(6):40–47, 2010.
[10] S. H¨at¨onen, A. Nyrhinen, L. Eggert, S. Strowes,
P. Sarolahti, and M. Kojo. An Experimental Study of
Home Gateway Characteristics. In Proceedings of the
10th ACM SIGCOMM Conference on Internet
Measurement, IMC’10, pages 260–266, 2010.
[11] W. Hu, T. Yang, and J. N. Matthews. The Good, the
Bad and the Ugly of Consumer Cloud Storage. ACM
SIGOPS Operating Systems Review, 44(3):110–115,
2010.
[12] A. Lenk, M. Klems, J. Nimis, S. Tai, and
T. Sandholm. What’s Inside the Cloud? An
Architectural Map of the Cloud Landscape. In
Proceedings of the 2009 ICSE Workshop on Software
Engineering Challenges of Cloud Computing,
CLOUD’09, pages 23–31, 2009.
[13] A. Li, X. Yang, S. Kandula, and M. Zhang.
CloudCmp: Comparing Public Cloud Providers. In
Proceedings of the 10th ACM SIGCOMM Conference
on Internet Measurement, IMC’10, pages 1–14, 2010.
[14] M. Mellia, M. Meo, L. Muscariello, and D. Rossi.
Passive Analysis of TCP Anomalies. Computer
Networks, 52(14):2663–2676, 2008.
[15] A. Mislove, M. Marcon, K. P. Gummadi, P. Druschel,
and B. Bhattacharjee. Measurement and Analysis of
Online Social Networks. In Proceedings of the 7th
ACM SIGCOMM Conference on Internet
Measurement, IMC’07, pages 29–42, 2007.
[16] M. Mulazzani, S. Schrittwieser, M. Leithner,
M. Huber, and E. Weippl. Dark Clouds on the
Horizon: Using Cloud Storage as Attack Vector and
Online Slack Space. In Proceedings of the 20th
USENIX Conference on Security, SEC’11, 2011.
[17] G. Wang and T. E. Ng. The Impact of Virtualization
on Network Performance of Amazon EC2 Data
Center. In Proceedings of the 29th IEEE INFOCOM,
pages 1–9, 2010.
[18] Q. Zhang, L. Cheng, and R. Boutaba. Cloud
Computing: State-of-the-Art and Research
Challenges. Journal of Internet Services and
Applications, 1:7–18, 2010.
[19] M. Zhou, R. Zhang, W. Xie, W. Qian, and A. Zhou.
Security and Privacy in Cloud Computing: A Survey.
In Sixth International Conference on Semantics
Knowledge and Grid, SKG’10, pages 105–112, 2010.
Client
Tstat
Amazon
Client
Tstat
Amazon
SYN
SYN/ACK
ACK + SSL_client_hello
SYN
SYN/ACK
ACK +
SSL_client_hello (PSH)
ACK + SSL_server_hello
ACK + SSL_server_hello
ACKs
SSL_server_hello (PSH)
∆t
ACK + SSL_cipher_spec
∆t
ACKs
SSL_server_hello 
ACK + 
SSL_cipher_spec (PSH)
ACK +
SSL_cipher_spec (PSH)
ACK + SSL_cipher_spec 
ACK
Data
ACK
HTTP_retrieve (2 x PSH)
HTTP_OK (PSH)
Data
≈
Data
≈
≈
HTTP_retrieve (2 x PSH)
≈
HTTP_OK (PSH)
SSL_alert (PSH) +
FIN/ACK
6
0
s
Data
SSL_alert + FIN/ACK
6
0
s
ACKs
RST
ACKs
RST
(a) Store
(b) Retrieve
Figure 19: Typical ﬂows in storage operations.
APPENDIX
A. STORAGE TRAFFIC IN DETAILS
A.1 Typical Flows
Fig. 19 shows typical storage ﬂows observed in our testbed.
All packets exchanged during initial and ﬁnal handshakes are
depicted. The data transfer phases (in gray) are shortened
for the sake of space. Key elements for our methodology,
such as TCP segments with PSH ﬂag set and ﬂow durations,
are highlighted. To conﬁrm that these models are valid in
real clients, Tstat in Campus 1 was set to record statistics
about the ﬁrst 10 messages delimited by TCP segments with
PSH ﬂag set. In the following, more details of our methodol-
ogy and the results of this validation are presented.
A.2 Tagging Storage Flows
Storage ﬂows are ﬁrst identiﬁed using FQDNs and SSL
certiﬁcate names, as explained in Sec. 3.1. After that, they
are classiﬁed based on the number of bytes sent by each
endpoint of the TCP connection. The method was built
based on the assumption that a storage ﬂow is used either
for storing chunks or for retrieving chunks, but never for
both. This assumption is supported by two factors: (i) when
both operations happen in parallel, Dropbox uses separate
connections to speed up synchronization; (ii) idle storage
connections are kept open waiting for new commands only
for a short time interval (60s).
Our assumption could be possibly violated during this idle
interval. In practice, however, this seems to be hardly the
case. Fig. 20 illustrates that by plotting the number of bytes
in storage ﬂows in Campus 1 . Flows are concentrated near
the axes, as expected under our assumption.
493Store
Retrieve
f(u)
)
s
e
t
y
b
(
d
a
o
l
n
w
o
D
1G
100M
10M
1M
100k
10k
1k
100
100
1k
10k
100k
1M 10M 100M 1G
Upload (bytes)
Figure 20: Bytes exchanged in storage ﬂows in
Campus 1 . Note the logarithm scales.
Store
Retrieve
F
D
C
 1
 0.8
 0.6
 0.4
 0.2
 0
Campus 1
Campus 2
Home 1
Home 2
 1
 0.8
 0.6
 0.4
 0.2
 0
 0  100  200  300  400  500  600
 0  100  200  300  400  500  600
Proportion (bytes)
Proportion (bytes)
Figure 21: Payload in the reverse direction of stor-
age operations per estimated number of chunks.
Flows in Fig. 20 are already divided into groups. The limit
between store and retrieve regions is determined by the func-
tion f (u) = 0.67(u − 294) + 4103, where u is the number of
uploaded bytes. This function was empirically deﬁned us-
ing the extra information collected in Campus 1 , where the
following was observed:
• Both store and retrieve operations require at least 309
bytes of overhead from servers;
• Store and retrieve operations require at least 634 and
362 bytes of overhead from clients, respectively;
• Typically, SSL handshakes require 294 bytes from
clients and 4103 bytes from servers.
f (u) is centralized between the regions determined accord-
ing these constants. For improving visualization, SSL over-
heads are subtracted from each point in the ﬁgure.
Since client machines in Campus 1 are relatively homoge-
nous, the gap between the two groups is very clear in this
dataset. More variation in message sizes is observed at other
vantage points. SSL handshakes, in particular, are aﬀected
by diﬀerent software conﬁgurations. However, this does not
change considerably the regions of each storage operation
when compared to Campus 1 .
Finally, we quantify the possible error caused by violations
of our assumption.
In all vantage points, ﬂows tagged as
store download less 1% of the total storage volume. Since
this includes protocol overheads (e.g., HTTP OK messages
in Fig. 19), mixed ﬂows marked as store might have only a
negligible impact in our results. Similar reasoning is valid
for retrieve ﬂows.
A.3 Number of Chunks
The number of chunks transported in a storage ﬂow (c) is
estimated by counting TCP segments with PSH ﬂag set (s) in
the reverse direction of the transfer, as indicated in Fig. 19.
For retrieve ﬂows, c = s−2
2 . For store ﬂows c = s − 3 or
c = s − 2, depending on whether the connection is passively
closed by the server or not. This can be inferred by the
time diﬀerence between the last packet with payload from
the client and the last one from the server: when the server
closes an idle connection, the diﬀerence is expected to be
around 1 minute (otherwise, only a few seconds). Tstat
already records the timestamps of such packets by default.
We validate this relation by dividing the amount of pay-
load (without typical SSL handshakes) in the reverse direc-
tion of a transfer by c. This proportion has to be equal to
the overhead needed per storage operation. Fig. 21 shows
that for the vast majority of store ﬂows the proportion is
about 309 bytes per chunk, as expected.
In Home 2 , the
apparently misbehaving client described in Sec. 4.3 biases
the distribution: most ﬂows from this client lack acknowl-
edgment messages. Most retrieve ﬂows have a proportion
between 362 and 426 bytes per chunk, which are typical sizes
of the HTTP request in this command. The exceptions (3%
– 8%) might be caused by packet loss in our probes as well
as by the ﬂows that both stored and retrieved chunks. Our
method underestimates the number of chunks in those cases.
A.4 Duration
Fig. 19 shows the duration used when computing the
throughput of a storage ﬂow (∆t). Since initial TCP/SSL
handshakes aﬀect users’ perception of throughput, the ﬁrst
SYN packet is taken as the begin of the transfer. Termina-
tion handshakes, on the other hand, are ignored. In store
ﬂows, the last packet with payload sent by the client is con-
sidered the end of the transfer. In retrieve ﬂows, the last
packet with payload is normally a server alert about the
SSL termination. We compensate for that by subtracting
60s from the duration of retrieve ﬂows whenever the diﬀer-
ence between the last packet with payload from the server
and the one from the client is above 60s.
Because of our monitoring topology, ∆t does not include
the trip time between clients and our probes. ∆t is, there-
fore, slightly underestimated. Fig. 19 also shows that 4 or 5
RTTs are needed before the client starts to send or to receive
data. In some cases, this already accounts for about 500ms
in the ﬂow duration. Note that the initial TCP congestion
window in place at servers was forcing a pause of 1 RTT
during the SSL handshake. This parameter has been tuned
after the release of Dropbox 1.4.0, reducing the overhead.
494