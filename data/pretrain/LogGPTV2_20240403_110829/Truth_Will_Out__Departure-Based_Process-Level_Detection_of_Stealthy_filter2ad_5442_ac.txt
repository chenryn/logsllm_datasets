sentation we mean the vector p in Lr that is the closest possible to
x, in the sense that ||p − x|| is minimal. It is well known that this
best approximation vector is given by the orthogonal projection
p = Px, where P is the projection matrix onto Lr and ||p − x|| is
the projection error.
Since by construction, the signal subspace is the same as the
range of the matrix U, the projection matrix that maps vectors in
RL to Lr is given by P = U(UT U)
−1UT (see Eq. (2)). Evidently, for
the projection matrix P to be defined, the columns of U must be
linearly independent so that UT U is invertible. The column vectors
ui of U, resulting from the SVD of the trajectory matrix X (step 2),
are not only linearly independent, but also orthonormal. It follows
that UT U = I, where I is the identity matrix, and the projection
matrix is thus reduced to P = UUT .
As the best representation of the lagged vectors in the signal
subspace can be obtained by an orthogonal projection, we first
project the training vectors onto Lr and compute the centroid of
the resulting cluster (see Eq. (7)). Then, to detect malicious changes
in the process behavior, pasad checks, in real time, if the most recent
sensor observations are persistently departing from the cluster of
projected training vectors in the signal subspace (step 4). This is
done by keeping track of the distance between the most recent test
vector and the centroid of the cluster.
2.6 The Isometry Trick
In a nutshell, the isometry trick states that, for an arbitrary vector
x in RL, computing the norm of the vector UT x has the effect of
implicitly projecting x onto the subspace Lr and computing its
norm there.
To make progress, consider the linear map UT : RL → Rr and
note that for all x in RL, the following equality holds
||UT Px|| = ||Px||
(9)
(see Appendix A for a proof). Informally, Eq. (9) implies that when-
ever a vector x is projected onto Lr , the resultant vector is one
whose length does not change when acted upon by the transforma-
tion UT . Formally, first note the key property
(10)
(see Appendix B for a proof) and that K (UT )⊥ = R (U) (from
Eq. (1)), then UT is said to be a partial isometry [24].
R (P) = R (U)
Definition (Partial Isometry). A not necessarily square matrix
A is called a partial isometry if the vectors v and Av have the same
Euclidean norm whenever v is in the orthogonal complement of the
kernel of A.
A (linear) isometry between two normed vector spaces is a linear
map that preserves the length of vectors (and by linearity, the dis-
tance between two vectors) for all vectors in its domain. Note that
U : Rr → RL is an isometry, since for every y ∈ Rr , ||Uy|| = ||y||.1
On the other hand, ||UT x|| and ||x|| are not equal in general. How-
ever, when the domain of UT , namely RL, is restricted to the orthog-
onal complement of its kernel, UT becomes an isometry as asserted
by Eq. (9). Indeed, for every x in RL, let v = Px, then ||UT v|| = ||v||,
for every v ∈ R (P) = R (U) = K (UT )⊥.
But (UT )д = U; that is, U is a generalized inverse of UT , since
(11)
which follows directly from the fact that UT U = I. Hence, by virtue
of Eq. (11), one can rewrite Eq. (9) as
UT = UT UUT = UT P
||UT x|| = ||UUT x||,
(12)
suggesting that computing norms in Lr can be done without the
need for an explicit projection.
We refer to using the property that UT is a partial isometry to
compute the norms ||UT x|| in Rr instead of the norms ||UUT x|| in
Lr as the isometry trick.
Next, we show that this property allows for more efficient track-
ing of the distance between the test vector and the centroid during
the detection phase. In §2.8, we extend this result to show that Lr
1Since ||Uy||2 = (Uy) · (Uy) = yT UT Uy = yT y = ||y||2, ∀ y ∈ Rr .
Session 5A: CyberphysicalCCS’18, October 15-19, 2018, Toronto, ON, Canada821is isomorphic to the r-dimensional Euclidean space. A key benefit
of this isomorphism is the ability to visualize the normal behavior
as a cluster of lagged vectors in the signal subspace, as well as the
departure from the cluster when the process is under attack.
2.7 Efficiency: Implicit Projection
In the detection phase, a departure score for the most recent test
vector is obtained by iteratively computing the distance Dj from
the centroid determined in the training phase (see Eq. (8)) .
By writing Dj = ||P(c−xj )||2 = ||UUT (c−xj )||2, we observe that
computing the distance from the centroid of the cluster amounts to
computing the (squared) norm of the projection of the difference
vector (c − xj ) onto Lr . From the isometry trick, we learned that
in order to compute the norm of the projection of any vector x in
the trajectory space, it is sufficient to compute the norm of UT x
without the need for an explicit projection onto Lr . With this in
mind, computing UT x instead of UUT x at every iteration leads to a
significant gain in performance (see §2.10 for more performance
analysis), which increases the deployability of pasad on limited-
resource hardware, and its applicability to real industrial settings.
Thus, we can now efficiently compute the centroid as ˜c = UT c
and the departure score as Dj = ||˜c − UT xj||2, replacing Eq. (7) and
Eq. (8) respectively in the actual implementation. Consequently,
the departure score can be evaluated more efficiently in the low-
dimensional space Rr , while still gaining the luxury of computing
the distance from the cluster in the signal subspace.
2.8 Validation: Visualizing the Departure
We now extend the result in Eq. (12) to show that by restricting the
domain of UT to R (U) = R (P), we obtain an isomorphism between
the signal subspace and a low-dimensional Euclidean space, which
allows us to visualize the behavior of the underlying process. We
do not make the claim that the ability to visualize the departure of
the process in ICS would offer plant engineers a complete picture
of the causes and physical implications of the attack. However, we
consider that the chief advantage of being able to visualize the
time-series data and the departure is that it empirically validates
the theoretical claims about pasad that we have made in §2.4.
We showed in §2.6 that the linear map UT , being a partial isome-
try, preserves the vector norm when its domain is restricted to R (U).
,
then evidently ˜U is an isometry. With the following proposition,
we show that ˜U is further an isomorphism between the vector
spaces Lr and Rr . The signal subspace being isomorphic to the
r-dimensional Euclidean space effectively means that the two vec-
tor spaces are fundamentally the “same” for all practical purposes
(e.g., with respect to dimension, linear independence, vector norm,
distance between vectors, linear combinations, etc). The key benefit
for pasad then is that it can operate in a simpler space where com-
puting the distance from the centroid can be done more efficiently
and where the time-series data can be visualized.
Let ˜U : R (U) → R (UT ) be the restricted map, i.e., ˜U = UT(cid:12)(cid:12)(cid:12)R (U)
Proposition. The restricted linear map ˜U : R (U) → R (UT ) is
an isometric isomorphism and admits a Moore-Penrose pseudoinverse
˜U+ = U. R (U) and R (UT ) are therefore isometrically isomorphic
(R (U) (cid:27) R (UT )).
For a sketch of the proof (see Appendix C for a complete proof),
note that a linear isometry between two normed vector spaces is
a (linear) isomorphism if it is bijective (i.e., both injective and sur-
jective). ˜U is injective since it is left-invertible as ˜U+ ˜U = UUT = P,
where P is the identity map on R (U) (by the idempotence prop-
erty in (3)). Moreover, ˜U is surjective since it is right-invertible as
˜U ˜U+ = UT U = I, where I is the identity map on R (UT ). Hence, ˜U
is a bijective isometry, and thus an isomorphism.
Finally, since all r columns of U are linearly independent, U has
full rank, meaning that its kernel contains only the zero vector.
Therefore, by Eq. (1), R (UT ) = Rr , and as R (U) = Lr by construc-
tion, it follows, by the previous proposition, that ˜U : Lr → Rr is
an isomorphism and consequently
Lr (cid:27) Rr .
(13)
While vectors in the r-dimensional signal subspace Lr have L
components (since Lr ⊂ RL), vectors in the range of the partial
isometry UT have only r components (since R (UT ) = Rr ). Thus,
by Eq. (13), images of all vectors in the trajectory space can be
expressed with respect to the standard basis for Rr . By choosing
only the first 3 basis vectors of the signal subspace, empirically
deemed sufficient for capturing the main structure, we can plot the
data vectors as points in R3 and visualize the structure.
Figure 1 depicts a visualization of the departure of the Tennessee-
Eastman process from normal operating conditions when the pro-
cess is under a stealthy type-SA3 attack (described in detail in
§3.1.1). The upper-left subplot shows a time series of raw measure-
ments corresponding to the sensor being monitored by pasad. The
initial subseries was used for training to extract the basis vectors
of the signal subspace, map the training vectors to R3, and com-
pute the centroid of the cluster they form. As shown on the right,
when the process is running under normal operating conditions,
the projected test vectors lie close to the cluster, whereas when
the process is under attack, the vectors start departing from the
cluster. The lower-left subplot shows the values for the distance
Dj from the centroid ˜c which pasad iteratively computes for every
test vector xj, and how the departure was detected shortly after
the attack started.
2.9 Choice of Parameters
There are three parameters involved in pasad: the length N of the
initial part of the time series used for training, the lag parameter
L, and the statistical dimension r. For the length of the training
subseries, the best practice is to have it sufficiently large so that it
incorporates an essential part of the signal in the noisy time series.
To determine the lag parameter L and the statistical dimension r,
there are standard SSA recommendations and guidelines to fol-
low [19, 39]. For the lag parameter, it should be that L ≤ N /2, and
in practice, the choice ⌊L = N /2⌋ often proves adequate. As for the
statistical dimension, we choose r such that the r leading eigenvec-
tors provide a good description of the signal and the lower L − r
eigenvectors correspond to noise. The choice of r can be assisted
by a scree plot,2 in which the spectrum of the eigenvalues obtained
2A scree plot is a statistical test, frequently used in factor analysis, to estimate the
number of factors (eigenvectors in our case) that correspond to most of the variability
in the data.
Session 5A: CyberphysicalCCS’18, October 15-19, 2018, Toronto, ON, Canada822Figure 1: A visualization of the departure of the TE process from the normal state. The upper-left subplot shows the raw
sensor measurements. As shown on the right, when the process is running under normal conditions, the test vectors lie close
the cluster, whereas when the process is under attack, the vectors start departing from the cluster. The lower-left subplot
shows the values for the departure score and how the departure was detected shortly after the attack onset.
(cid:8)Dn,r ,τ : N < n < τ(cid:9) where
in the SVD step reveals the noise level as a flat tail, such that the
statistical dimension is the number of eigenvalues above this level.
Finally, to determine the threshold θ, we run pasad on a valida-
tion series, which is the subseries observed under normal conditions
and preceded by the training subseries, i.e., the subseries determined
by the interval (N + 1, τ − 1), where τ is the starting time of the
attack (when applicable). More formally, we define the threshold as
θ = Mn,r ,τ + ϵ such that Mn,r ,τ = max
n
Dn,r ,τ is the departure score corresponding to observations in the
specified range and ϵ is a relatively small constant. We motivate
this approach to determining the alarm threshold in §4.5.
2.10 Implementation & Performance
A pseudocode of pasad is shown in Algorithm 1. The most expen-
sive computational step during training is computing the singu-
lar value decomposition of the trajectory matrix X.3 In general,
the SVD of an m×n matrix is computable in time proportional to
O(min{mn2, m2n}). In our particular case, X is of dimension L×K,
where K = N − L + 1. Then, we have L ≤ N /2 as mentioned in §2.9,
which implies that L ≤ (K + L − 1)/2 ⇒ L ≤ K − 1 ⇒ L < K, so
that min{LK2, L2K} = L2K. Thus the time complexity of the SVD
step in pasad is O(L2K ).
Although the time complexity of the training procedure is qua-
dratic in the size of the lag parameter L, performance is not a critical
issue for the training phase since it is an offline procedure. On the
other hand, it is crucial that testing on incoming observations can
be done efficiently to allow for real-time protection. As mentioned
in §2.7, the departure score for the jth test vector is computed as
||˜c−UT xj||2. First, ˜c−UT xj is evaluated in O(rL), since it involves a
3Note that the hankel function on Line 19 is used because the trajectory matrix has a
Hankel structure.
product of matrices with dimensions r×L and L×1 respectively, then
the elements of the resultant vector are squared and added in O(r ).
However, r is a constant that does not depend on L and typically
r << L; therefore, the overall time complexity of the detection
phase is linear in L.
3 A FRAMEWORK FOR VALIDATION
In this section, we describe three different scenarios for validating
pasad: the Tennessee-Eastman process, a dataset from the SWaT
testbed, and a network traffic from a real water distribution plant.
3.1 Scenario I: The Tennessee-Eastman Process
We have developed a set of new attacks on the Tennessee-Eastman
process control model [12] that aim to cause tangible impact on the
underlying physical process.4 To provide an intelligible explanation
of the attacks we have designed, a high-level description of the
process is in order.
The TE simulation model simulates a real plant-wide chemical
process. The process was originally released with no embedded
control strategy as the aim of its release was to challenge the control
theory community to develop and benchmark different optimized
control strategies. Indeed, several strategies have been proposed in
response to the challenge [32, 45]. More recently however, acting as
both a realistic and safe environment for experimentation, the TE
process has transcended its original objective and has come to be a
popular choice amongst ICS security researchers [8, 28, 29, 31, 37].
We use the popular simulation model proposed by Downs and
Vogel [12], who modified some aspects of the chemical process,
4The TE attack data and pasad’s code are made publicly available at https://github.com/
mikeliturbe/pasad
Training vectors form a clusterTest vectors depart from the cluster when the process is under attackTest vectors fall close to the cluster during normal process operationDeparture score crossing the threshold shortly after the attack startedSession 5A: CyberphysicalCCS’18, October 15-19, 2018, Toronto, ON, Canada823Algorithm 1 Pasad: An algorithm for detecting stealthy attacks
on control systems.
Required: Training subseries Tt r ain and the lag parameter L.
Outcome: An alarm is raised whenever the departure score crosses
a prespecified threshold.
1: N ← length(Tt r ain)
2: UT ← pasad_train(Tt r ain, N , L)
3: K ← N − L + 1
4: for i ← 1, K do
xi ← ith traininд_vector
5:
s ← s + xi
6:
7: end for
8: c ← s/K