### Stopping Condition and Sensitivity of Degree Sequence Query

The stopping condition for the algorithm is defined as follows:
\[
\Delta q \leq \frac{4n + 2}{2(\Lambda + 1) + 1} \leq \frac{2n + 1}{4n + 2} = \frac{1}{2}
\]

When considering the sensitivity of the degree sequence query, it is important to note that the sensitivity of the count query is no longer 1. Therefore, a standard count query cannot be used. Instead, we allow the algorithm to continue until a maximum depth is reached. In our experiments, we use a depth of 15.

### Summarizing the Resulting Regions

At the end of the process, we calculate the smooth sensitivity of the count query for each region. The rationale behind this approach is that removing one node will likely shift some node degrees in a region by 1 and, at most, remove one node completely. Consequently, only the nodes with degrees at the region boundaries will affect the count. By calculating the smooth sensitivity based on these counts (and adding noise sampled from a fat-tailed distribution, such as the Cauchy distribution), we can release accurate counts for each region while satisfying differential privacy. Finally, we deterministically choose the lower boundary of the domain for each region to represent the degree of nodes in the region.

### Graph Statistics

| Dataset       | Nodes  | Edges   | α (Power Law Coefficient) | maxd (Maximum Degree) |
|---------------|--------|---------|--------------------------|-----------------------|
| Caltech       | 769    | 16656   | 1.14                     | 248                   |
| Georgetown    | 9414   | 425638  | 1.04                     | 1235                  |
| UNC           | 18163  | 766800  | 1.05                     | 3795                  |
| Enron (Undirected) | 36692 | 183831 | 1.56                     | 1383                  |

### Experimental Evaluation

The anonymization scheme was implemented and tested on four graph datasets with varying sizes and properties. The properties of the datasets are summarized in Table 1. The datasets Caltech, Georgetown, and UNC are Facebook college networks captured in 2005 [30]. These datasets show the full intra-school links as they were in September 2005. Each node represents an individual, and each edge represents a friendship link. The datasets vary in the number of nodes and edge density, demonstrating the efficacy of our approach on graphs ranging from a few hundred nodes to tens of thousands of nodes. The last dataset, the Enron email network [18], represents individuals as nodes and undirected edges indicate at least one email exchange between the two parties. This graph is sparser compared to the Facebook networks.

#### Utility Measures

To evaluate our results, we employ two statistical distance measures commonly used in the literature for evaluating the similarity of two distributions [16, 21]. The first is the Kolmogorov-Smirnov (KS) statistic, which is defined as the maximum distance between the empirical cumulative distribution functions of two samples: 
\[
KS(X, Y) = \max_x |F_X(x) - F_Y(x)|
\]
where \( F_X(x) = \frac{1}{n} \sum_{i=1}^n I_{X_i \leq x} \). Smaller values of KS-distance indicate closer distributions and better utility. The KS-distance is sensitive to differences in both location and shape of the empirical cumulative distribution functions but is less sensitive to the tail of the distributions.

The second measure we use is the Earth Mover’s Distance (EMD), which allocates equal sensitivity to all nodes in the graph and is thus as sensitive at the tail as it is at the median.

#### Results

Figures 4 and 5 (in Appendix A) show the results for the KS-distance and the EMD, respectively. Each anonymization was repeated 5 times for each value of \(\epsilon\) ranging from 0.01 to 10.0. The graphs show the results along with the standard error. We also repeat the experiments under different assumptions of the maximum degree. In Figures 4(a) and 5(a), we assume prior knowledge of the maximum degree and set \(\Lambda\) to the actual maximum degree shown in Table 1. In Figures 4(b) and 5(b), we set \(\Lambda\) to its maximum possible value given a graph of size \(n\) (\(\Lambda = n - 1\)). In this case, we summarize the partitions by choosing the lower bound for the partition domain.

We compare the results of our anonymization method to the work of Hay et al. [16], which presents the only other method of satisfying differential privacy for graph degree sequence queries. The experimental results are very promising. For the KS-distance (Figure 4), the RPS algorithm achieves high accuracy, especially for larger graphs. This is because larger values of \(\Lambda\) only affect higher-degree nodes, which are in the tail of the distribution to which the KS-distance is less sensitive. The results imply that accurate degrees are reported for most of the nodes in the graph, which is sufficient in many analysis scenarios. The RPS algorithm performed much better than the Laplacian and constrained inference alternatives, especially for smaller values of \(\epsilon\).

For the EMD (Figure 5), the results are also very good for both settings of \(\Lambda\). As expected, \(\Lambda = \text{maxd}\) results in better accuracy, but the deviation between both settings of \(\Lambda\) is not large. For \(\epsilon \geq 1\), the result is close to 10 and is at least 100 units smaller than the result for the constrained inference method. This is attributed to the flexibility RPS provides in choosing how to summarize the partitions. The results for all graphs consistently outperformed the constrained inference alternative.

### Related Work

Privacy research is motivated by several privacy breach incidents [29, 3, 25, 26]. Most work has focused on syntactic methods of achieving privacy, which are deemed insufficient for privacy protection [29, 22, 21]. In this paper, we utilize a stronger privacy guarantee: differential privacy. Differential privacy was introduced in a series of papers [6, 11, 4, 9, 7] and methods of satisfying it are presented in [7, 27, 24].

The Mondrian algorithm [20] performs a similar partitioning and summarization of the dataset but aims to satisfy the weaker privacy notion of k-anonymity. Blum et al. [5] proposed an approach using non-recursive partitioning, but their results are mostly theoretical and lack general practical applicability. Friedman and Schuster [14] provide an approach for data mining with differential privacy using the exponential mechanism, but their approach does not focus on publishing an entire dataset. Privacy Integrated Queries (PINQ) [23] is a platform that provides methods, including count and median, to implement the RPS algorithm. However, PINQ only provides mechanisms for differentially private database access rather than actual data sanitization methods.

Work has also been done on improving the accuracy of interactive data release [28], classifying queries as "easy" or "hard" based on whether the majority of databases consistent with previous answers to hard queries would give an accurate answer. Our approach, however, deals with the non-interactive case.

Anonymizing graph properties has been motivated by [2, 8]. Hay et al. [16] present a method to release the degree sequence while protecting edges. We show how our framework can be used to release the degree sequence while protecting nodes.

### Conclusion

In this paper, we address the problem of differentially private data release. We first consider the non-interactive mode of differentially private data release, examining current negative results and commenting on their inapplicability to general differentially private data publication. We then propose a general and practical anonymization framework called Recursive Partitioning and Summarization (RPS). RPS works by issuing a set of differentially private queries to recursively divide the dataset into several regions. We then generate a synthetic dataset by summarizing each region in a differentially private manner. We experimentally evaluate the utility of our framework in three domains: synthetic datasets, the Adult census dataset, and real-world graph datasets. All our results indicate the applicability of our framework to general data release.

### Acknowledgements

This paper is based upon work supported by the United States National Science Foundation under Grant No. 1116991, and by the United States AFOSR under grant titled “A Framework for Managing the Assured Information Sharing Lifecycle.”

### References

[1] A. Asuncion and D. Newman. UCI machine learning repository, 2010.
[2] L. Backstrom, C. Dwork, and J. Kleinberg. Wherefore art thou r3579x?: anonymized social networks, hidden patterns, and structural steganography. In WWW, pages 181–190, 2007.
[3] M. Barbaro and J. Tom Zeller. A face is exposed for AOL searcher no. 4417749. New York Times, Aug 2006.
[4] A. Blum, C. Dwork, F. McSherry, and K. Nissim. Practical privacy: the sulq framework. In PODS ’05: Proceedings of the twenty-fourth ACM SIGMOD-SIGACT-SIGART symposium on Principles of database systems, pages 128–138, New York, NY, USA, 2005. ACM.
[5] A. Blum, K. Ligett, and A. Roth. A learning theory approach to non-interactive database privacy. In STOC, pages 609–618, 2008.
[6] I. Dinur and K. Nissim. Revealing information while preserving privacy. In PODS, pages 202–210, 2003.
[7] C. Dwork. Differential privacy. In ICALP, pages 1–12, 2006.
[8] C. Dwork. Differential privacy: A survey of results. In TAMC, pages 1–19, 2008.
[9] C. Dwork, F. McSherry, K. Nissim, and A. Smith. Calibrating noise to sensitivity in private data analysis. In TCC, pages 265–284, 2006.
[10] C. Dwork, M. Naor, O. Reingold, G. Rothblum, and S. Vadhan. On the complexity of differentially private data release: efficient algorithms and hardness results. Proceedings of the 41st annual ACM symposium on Theory of computing, pages 381–390, 2009.
[11] C. Dwork and K. Nissim. Privacy-preserving datamining on vertically partitioned databases. In CRYPTO, pages 528–544, 2004.
[12] C. Dwork, G. Rothblum, and S. Vadhan. Boosting and differential privacy. Foundations of Computer Science (FOCS), 2010 51st Annual IEEE Symposium on, pages 51 – 60, 2010.
[13] C. Dwork and S. Yekhanin. New efficient attacks on statistical disclosure control mechanisms. Advances in Cryptology–CRYPTO 2008, pages 469–480, 2008.
[14] A. Friedman and A. Schuster. Data mining with differential privacy. In KDD ’10: Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 493–502, New York, NY, USA, 2010. ACM.
[15] M. Götz, A. Machanavajjhala, G. Wang, X. Xiao, and J. Gehrke. Privacy in search logs. CoRR, abs/0904.0682, 2009.
[16] M. Hay, C. Li, G. Miklau, and D. Jensen. Accurate Estimation of the Degree Distribution of Private Networks. In ICDM, pages 169–178, 2009.
[17] M. Hay, V. Rastogi, G. Miklau, and D. Suciu. Boosting the accuracy of differentially private histograms through consistency. Proc. VLDB Endow., 3:1021–1032, September 2010.
[18] B. Klimt and Y. Yang. Introducing the Enron corpus. In CEAS, 2004.
[19] A. Korolova, K. Kenthapadi, N. Mishra, and A. Ntoulas. Releasing search queries and clicks privately. In WWW, pages 171–180, 2009.
[20] K. LeFevre, D. DeWitt, and R. Ramakrishnan. Mondrian multidimensional k-anonymity. In ICDE, page 25, 2006.
[21] N. Li, T. Li, and S. Venkatasubramanian. t-closeness: Privacy beyond k-anonymity and l-diversity. In ICDE, pages 106–115, 2007.
[22] A. Machanavajjhala, J. Gehrke, D. Kifer, and M. Venkitasubramaniam. ℓ-diversity: Privacy beyond k-anonymity. In ICDE, page 24, 2006.
[23] F. McSherry. Privacy integrated queries: an extensible platform for privacy-preserving data analysis. In SIGMOD, pages 19–30, 2009.
[24] F. McSherry and K. Talwar. Mechanism design via differential privacy. In FOCS, pages 94–103, 2007.
[25] A. Narayanan and V. Shmatikov. Robust de-anonymization of large sparse datasets. In S&P, pages 111–125, 2008.
[26] A. Narayanan and V. Shmatikov. De-anonymizing social networks. In IEEE Symposium on Security and Privacy, pages 173–187. IEEE Computer Society, 2009.
[27] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth sensitivity and sampling in private data analysis. In STOC, pages 75–84, 2007.
[28] A. Roth and T. Roughgarden. Interactive privacy via the median mechanism. In Proceedings of the 42nd ACM symposium on Theory of computing, STOC ’10, pages 765–774, New York, NY, USA, 2010. ACM.
[29] L. Sweeney. k-anonymity: A model for protecting privacy. Int. J. Uncertain. Fuzziness Knowl.-Based Syst., 10(5):557–570, 2002.
[30] A. L. Traud, E. D. Kelsic, P. J. Mucha, and M. A. Porter. Community structure in online collegiate social networks. arXiv:0809.0960, 2008.
[31] X. Xiao, G. Wang, and J. Gehrke. Differential privacy via wavelet transforms. IEEE Transactions on Knowledge and Data Engineering, 23:1200–1214, 2011.

### Appendix

#### A. Utility of Degree Sequence Anonymization

To demonstrate the utility of anonymizing the degree sequence query, we provide the results for the experiments described in Section 5 in Figures 4 and 5. Figure 4 uses the KS-Distance of the anonymized sequences to the original.

\[
\begin{array}{c}
\text{KS-Distance} \\
\begin{array}{cccccc}
 & 0.9 & 0.8 & 0.7 & 0.6 & 0.5 & 0.4 & 0.3 & 0.2 & 0.1 & 0 \\
\end{array}
\end{array}
\]