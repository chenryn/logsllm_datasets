∆q ≤
=
4n + 2
2(Λ + 1) + 1
4n + 2
≤
2n + 1
4n + 2
=
1
2
Stopping condition We also need to consider the sensi-
tivity of the degree sequence query when taking the counts
in each region. Note that the sensitivity of the count query
here is no longer 1, therefore we cannot use the normal count
query. We therefore allow the algorithm to continue until a
maximum depth is reached. We use a depth of 15 in our
experiments.
Summarizing the resulting regions When summarizing
each region at the end, we calculate the smooth sensitivity
Caltech
Georgetown
UNC
Enron (Undirected)
nodes
769
9414
18163
36692
edges
16656
425638
766800
183831
α
1.14
1.04
1.05
1.56
maxd
248
1235
3795
1383
Table 1: Graph Statistics - α is the power law coeﬃcient
of the degree distribution, and maxd is the maximum degree
of nodes in the graph
of the count query. The reasoning behind this approach
is that removing one node will most likely only shift some
of the node degrees in a region by 1 and remove, at most,
one node completely. Hence, in addition to the removed
node, only the nodes with degrees at the region boundaries
will aﬀect the count. By calculating the smooth sensitivity
based on these counts (and adding noise sampled from a
fat tailed distribution, ex: Cauchy), we can release accurate
counts for each region while satisfying diﬀerential privacy.
Finally, we deterministically choose the lower boundary of
the domain for each region to represent the degree of nodes
in the region.
5.2 Experimental Evaluation
The previous anonymization scheme was implemented and
tested on 4 graph datasets with varying sizes and properties.
The properties for the datasets are summarized in Table 1.
The datasets Caltech, Georgetown and UNC are Facebook
college networks captured in 2005 [30]. These datasets show
the full intra-school links as they were in September 2005.
Each node represents an individual and each edge represents
a friendship link. Each of these datasets contains a diﬀer-
ent number of nodes and a diﬀerent density of edges; and
hence show the eﬃcacy of our approach on graph with a few
hundred nodes to graphs of tens of thousands of nodes. The
last dataset we use is the Enron email network [18]. Each
node in the dataset represents an individual, and each undi-
rected edge indicates that at least one email was exchange
between the two parties. This graph is sparser compared to
the Facebook networks.
Utility Measures To evaluate our results, we employ two
statistical distance measures commonly used in the litera-
ture for evaluating the similarity of two distributions [16,
n Pn
21]. The ﬁrst is the Kolmogorov-Smirnoﬀ statistic, also
known as the KS-distance. Given two empirical distribu-
tion functions FX and FY , where FX (x) = 1
i=1 IXi ≤x,
the KS-Distance between two such distributions is deﬁned
as the maximum distance between the two distributions at
any point: KS(X, Y ) = maxx |FX (x) − FY (x)|. It is sensitive
to diﬀerences in both location and shape of the empirical cu-
mulative distribution functions of the two samples. Smaller
values of KS-distance indicate closer distributions and better
utility. The KS-distance is widely used as as a general non-
parametric distance measure. It was also employed by Hay
et al. to evaluate the distance between a degree sequence and
its anonymized counterpart [16]. The KS-distance, however,
is less sensitive to the tail of the distributions. We thus use
another distance measure to account for such discrepancies.
The second measure we use is the Earth Mover’s Distance
(EMD), as described in the previous section. Hay et al.
employed the Mallow’s distance at p=2 (equivalent to the
EMD) for their evaluation of similarity of degree sequences
[16].
Results Figure 4 and Figure 5 (in Appendix A) show the
results for the KS-distance and the Earth Mover’s Distance
respectively. Each anonymization was repeated 5 times for
each value of ǫ ranging from 0.01 to 10.0. The graphs show
the results along with the standard error. We also repeat
the experiments under diﬀerent assumptions of maximum
degree. In Figure 4(a) and Figure 5(a), we assume a priori
knowledge of the maximum degree and hence set Λ to be the
actual maximum degree shown in Table 1. In Figure 4(b) and
Figure 5(b), we set Λ to its maximum possible value given
a graph of size n (Λ = n − 1). In this case, we summarize
the partitions by choosing the lower bound for the partition
domain.
We compare the results of our anonymization method to
the work of Hay et al.
[16], which presents the only other
method of satisfying diﬀerential privacy for graph degree se-
quence queries. The experimental results look very promis-
ing. While considering the KS-distance (Figure 4), the RPS
algorithm can achieve fairly high accuracy. Furthermore,
the KS-Distance for the both settings of Λ was small. This
is even more the case with larger graphs. We note, however,
that this is because larger values of Λ only aﬀect higher de-
gree nodes and hence only the tail of the distribution to
which the KS-distance is less sensitive. The results imply
that accurate degrees are reported for most of the nodes in
the graph and this is suﬃcient in several analysis scenarios.
In addition, the RPS algorithm performed much better than
the Laplacian and constrained inference alternative. This
remained to be the case for all graphs under all values of ep-
silon. The deviation is most accentuated for smaller values
of epsilon, which imply a large amount of noise added via
the Laplacian mechanism.
We also analyzed the accuracy of the anonymized degrees
under the Earth Mover’s distance (Figure 5). This measure
allocates equal sensitivity to all nodes in the graph and is
thus as sensitive at the tail as it is at the median. The re-
sults are also very good for both settings of Λ. As expected,
Λ = maxd results in better accuracy; however the deviation
between both settings of Λ is not large. Furthermore, the
EMD for both is considered good. For ǫ ≥ 1, the result is
close to 10 and is at least 100 units smaller than the result
for the constrained inference method. We attribute this to
the beneﬁts of the ﬂexibility RPS gives in choosing how to
summarize the partitions. The results for all graphs consis-
tently outperformed the constrained inference alternative.
6. RELATED WORK
Work on privacy is largely motivated by a number of pri-
vacy breach incidents [29, 3, 25, 26]. Most work done, how-
ever, has focused on syntactic methods of achieving privacy,
which are deemed insuﬃcient for privacy protection [29, 22,
21]. In this paper we utilize a stronger privacy guarantee:
diﬀerential privacy. Diﬀerential privacy was presented in a
series of papers [6, 11, 4, 9, 7] and methods of satisfying it
are presented in [7, 27, 24].
The Mondrian algorithm [20] performs a similar partition-
ing and summarization of the dataset, however it aims to
satisfy the weaker privacy notion of k-anonymity.
In ad-
dition, Blum et al. [5] proposed an approach that employs
non-recursive partitioning, but their results are mostly theo-
retical and lack general practical applicability. [14] gives an
approach for data mining for diﬀerential privacy that uses
the exponential mechanism; however, their approach is not
concerned with publishing an entire dataset. Privacy Inte-
grated Queries (PINQ) [23] is a platform which provides
methods,
including count and median, to implement the
RPS algorithm. PINQ, however, only provides mechanisms
for diﬀerentially private database access, rather than actual
data sanitization methods.
Work has also been done on improving the accuracy of
interactive data release [28]. This classiﬁes queries as “easy”
or “hard”, according to whether or not the majority of
databases consistent with previous answers to hard queries
would give an accurate answer to it. A “hard” query is an-
swered using the Laplacian mechanism. An “easy” query
simply returns the corresponding median value. Our ap-
proach, however, deals with the non-interactive case.
Anonymizing graph properties has been motivated by [2,
8].
In addition, [16] present a method to release the de-
gree sequence while protecting edges. We show how our
framework can be used to release the degree sequence while
protecting nodes.
7. CONCLUSION
In this paper we tackle the problem of diﬀerentially private
data release. We ﬁrst consider the non-interactive mode
of diﬀerentially private data release. We examine current
negative results and comment on how they are inapplicable
to general diﬀerentially private data publication.
We then propose a general and practical anonymization
framework called Recursive Partitioning and Summarization
(RPS). RPS works by issuing a set of diﬀerentially private
queries to recursively divide the dataset into several regions.
We then generate a synthetic dataset by summarizing each
region in a diﬀerentially private manner. We experimen-
tally evaluate the utility of our framework in three domains.
First, we benchmark our method using synthetically gener-
ated datasets and show that RPS can indeed preserve the
characteristics of such datasets. Next, we apply RPS to the
Adult census dataset. We run common data mining algo-
rithms and evaluate the eﬀect of anonymization. Finally, we
show how our framework can be applied to graph datasets by
anonymizing the degree sequences of four real-world graph
datasets with diﬀerent properties. All our results indicate
the applicability of our framework to general data release.
Acknowledgements
This paper is based upon work supported by the United
States National Science Foundation under Grant No.
1116991, and by the United States AFOSR under grant ti-
tled “A Framework for Managing the Assured Information
Sharing Lifecycle”.
8. REFERENCES
[1] A. Asuncion and D. Newman. UCI machine learning
repository, 2010.
[2] L. Backstrom, C. Dwork, and J. Kleinberg. Wherefore
art thou r3579x?: anonymized social networks, hidden
patterns, and structural steganography. In WWW,
pages 181–190, 2007.
[3] M. Barbaro and J. Tom Zeller. A face is exposed for
aol searcher no. 4417749. New York Times, Aug 2006.
[4] A. Blum, C. Dwork, F. McSherry, and K. Nissim.
Practical privacy: the sulq framework. In PODS ’05:
Proceedings of the twenty-fourth ACM
SIGMOD-SIGACT-SIGART symposium on Principles
of database systems, pages 128–138, New York, NY,
USA, 2005. ACM.
[5] A. Blum, K. Ligett, and A. Roth. A learning theory
approach to non-interactive database privacy. In
STOC, pages 609–618, 2008.
[6] I. Dinur and K. Nissim. Revealing information while
preserving privacy. In PODS, pages 202–210, 2003.
[7] C. Dwork. Diﬀerential privacy. In ICALP, pages 1–12,
2006.
[8] C. Dwork. Diﬀerential privacy: A survey of results. In
TAMC, pages 1–19, 2008.
[9] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data
analysis. In TCC, pages 265–284, 2006.
[10] C. Dwork, M. Naor, O. Reingold, G. Rothblum, and
S. Vadhan. On the complexity of diﬀerentially private
data release: eﬃcient algorithms and hardness results.
Proceedings of the 41st annual ACM symposium on
Theory of computing, pages 381–390, 2009.
[11] C. Dwork and K. Nissim. Privacy-preserving
datamining on vertically partitioned databases. In
CRYPTO, pages 528–544, 2004.
[12] C. Dwork, G. Rothblum, and S. Vadhan. Boosting and
diﬀerential privacy. Foundations of Computer Science
(FOCS), 2010 51st Annual IEEE Symposium on,
pages 51 – 60, 2010.
[13] C. Dwork and S. Yekhanin. New eﬃcient attacks on
statistical disclosure control mechanisms. Advances in
Cryptology–CRYPTO 2008, pages 469–480, 2008.
[14] A. Friedman and A. Schuster. Data mining with
diﬀerential privacy. In KDD ’10: Proceedings of the
16th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 493–502,
New York, NY, USA, 2010. ACM.
[15] M. G¨otz, A. Machanavajjhala, G. Wang, X. Xiao, and
J. Gehrke. Privacy in search logs. CoRR,
abs/0904.0682, 2009.
[16] M. Hay, C. Li, G. Miklau, and D. Jensen. Accurate
Estimation of the Degree Distribution of Private
Networks. In ICDM, pages 169–178, 2009.
[17] M. Hay, V. Rastogi, G. Miklau, and D. Suciu.
Boosting the accuracy of diﬀerentially private
histograms through consistency. Proc. VLDB Endow.,
3:1021–1032, September 2010.
[18] B. Klimt and Y. Yang. Introducing the enron corpus.
In CEAS, 2004.
[19] A. Korolova, K. Kenthapadi, N. Mishra, and
A. Ntoulas. Releasing search queries and clicks
privately. In WWW, pages 171–180, 2009.
[20] K. LeFevre, D. DeWitt, and R. Ramakrishnan.
Mondrian multidimensional k-anonymity. In ICDE,
page 25, 2006.
[21] N. Li, T. Li, and S. Venkatasubramanian. t-closeness:
Privacy beyond k-anonymity and l-diversity. In ICDE,
pages 106–115, 2007.
[22] A. Machanavajjhala, J. Gehrke, D. Kifer, and
M. Venkitasubramaniam. ℓ-diversity: Privacy beyond
k-anonymity. In ICDE, page 24, 2006.
[23] F. McSherry. Privacy integrated queries: an extensible
platform for privacy-preserving data analysis. In
SIGMOD, pages 19–30, 2009.
[24] F. McSherry and K. Talwar. Mechanism design via
diﬀerential privacy. In FOCS, pages 94–103, 2007.
[25] A. Narayanan and V. Shmatikov. Robust
de-anonymization of large sparse datasets. In S&P,
pages 111–125, 2008.
[26] A. Narayanan and V. Shmatikov. De-anonymizing
social networks. In IEEE Symposium on Security and
Privacy, pages 173–187. IEEE Computer Society,
2009.
[27] K. Nissim, S. Raskhodnikova, and A. Smith. Smooth
sensitivity and sampling in private data analysis. In
STOC, pages 75–84, 2007.
[28] A. Roth and T. Roughgarden. Interactive privacy via
the median mechanism. In Proceedings of the 42nd
ACM symposium on Theory of computing, STOC ’10,
pages 765–774, New York, NY, USA, 2010. ACM.
[29] L. Sweeney. k-anonymity: A model for protecting
privacy. Int. J. Uncertain. Fuzziness Knowl.-Based
Syst., 10(5):557–570, 2002.
[30] A. L. Traud, E. D. Kelsic, P. J. Mucha, and M. A.
Porter. Community structure in online collegiate
social networks. arXiv:0809.0960, 2008.
[31] X. Xiao, G. Wang, and J. Gehrke. Diﬀerential privacy
via wavelet transforms. IEEE Transactions on
Knowledge and Data Engineering, 23:1200–1214, 2011.
APPENDIX
A. UTILITY OF DEGREE SEQUENCE
ANONYMIZATION
To demonstrate the utility of anonymizing the degree se-
quence query, we provide the results for the experiments
described in Section 5 in Figures 4 and 5. Figure 4 uses the
KS-Distance of the anonymized sequences to the original.
e
c
n
a
t
s
i
D
-
S
K
e
c
n
a
t
s
i
D
-
S
K
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0