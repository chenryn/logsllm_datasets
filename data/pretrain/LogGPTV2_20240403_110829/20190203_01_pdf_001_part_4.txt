### 1. Establishing a Streaming Replication Connection
A streaming replication connection to the database server is established using `libpq` to create a `walsender` communication channel.

### 2. Monitoring Data Changes
A separate thread is used to monitor data changes streamed through the `walsender` interface.

### 3. Transforming and Caching Data Changes
As data changes become available, they are transformed and stored in an in-memory cache.

### 4. Applying Cached Data Changes
On the next scheduled interval, the in-memory cached data changes are applied to each target database using JDBC batches of SQL statements (referred to as transaction sets). This process is similar to the trigger-based method described in Section 2.2.9. If one or more target databases are not accessible, the data changes are saved in a local file on the host running the publication server. For more information on in-memory caching and data persistence, see Section 2.2.10.5.

### 5. Updating the WAL Segment's LSN
The log sequence number (LSN) of the WAL segment, which identifies the last set of applied changes based on the last replicated transaction, is updated. The update is confirmed to the database server.

### 6. Clearing the In-Memory Cache
The applied data changes are cleared from the in-memory cache.

### 7. Repeating the Process
Steps 3 through 6 are repeated.

**Note:** A single SQL statement executed against a source table may result in multiple rows being modified and returned in the changeset stream. For example, if a single `UPDATE` statement affects 10 rows in the source table, 10 rows will be returned in the changeset stream—one for each row in the source table that was updated. When the publication server applies these changes to the target table, 10 `UPDATE` statements will be executed.

### 2.2.10.4 Replication Origin
Starting with PostgreSQL version 9.5, the logical decoding framework introduced a feature called "replication origin." This feature allows applications to identify, label, and mark certain aspects of a logical decoding session.

For more information on replication origin, see the PostgreSQL Core Documentation: [Replication Origins](https://www.postgresql.org/docs/current/static/replication-origins.html).

For the log-based method of synchronization replication, this feature provides performance improvements when the master nodes are running PostgreSQL version 9.5 or later.

As previously described, the log-based method uses WAL files to obtain changes applied to the publication tables. After the changes are retrieved through the `walsender` interface, the publication server applies the set of changes to the other master nodes using transaction sets consisting of JDBC batches of SQL statements. These changes are also recorded in the WAL files of each database server hosting the target master nodes.

Redundant or "replayed" changes are included in the changeset stream received by the publication server. These replayed changes must be ignored and not applied since they are duplicates of changes already applied to the target tables through the JDBC batches.

Replayed changes result in performance overhead as all such changes are transmitted over the network from the database server to the publication server, and then the publication server must discard the redundant changes.

With the replication origin feature, the publication server can set up logical decoding sessions so that these replayed changes are not included in the changeset stream transmitted over the network, thus eliminating this performance overhead.

**Conditions for Using Replication Origin:**
- Replication origin applies only to multi-master replication systems, not to single-master replication systems.
- Replication origin eliminates streaming of replayed changes only from PostgreSQL versions 9.5 or later. Replayed changes are still included in the changeset stream from PostgreSQL version 9.4 but are discarded by the publication server. Thus, multi-master replication systems consisting of both PostgreSQL versions 9.4 and 9.5 utilize the replication origin advantage on the 9.5 database servers.
- The `max_replication_slots` configuration parameter must be set to a minimal level to ensure the publication server can create the additional replication slots for replication origin.

For each master node database, in addition to the replication slot used for the changeset stream, an additional number of replication slots is required—one additional slot corresponding to every other master node to support the replication origin usage. Therefore, for each master node, the total number of replication slots required is equal to the total number of master nodes in the entire MMR system.

For a given database server (i.e., a PostgreSQL database cluster containing master node databases), the total number of replication slots required is equal to the total number of master nodes in the entire MMR system multiplied by the number of master node databases residing within the given database cluster.

**Example:**
Assume a 6-node multi-master replication system using three database clusters:
- Database cluster #1 contains 3 master node databases.
- Database cluster #2 contains 2 master node databases.
- Database cluster #3 contains 1 master node database.

The total number of master nodes is six. Multiply the number of master node databases in each database cluster by six to determine the required minimum setting for `max_replication_slots` for that database cluster.

| Postgres Database Server | max_wal_senders | max_replication_slots |
|--------------------------|-----------------|-----------------------|
| Cluster #1 (3 master nodes) | 3               | 18                    |
| Cluster #2 (2 master nodes) | 2               | 12                    |
| Cluster #3 (1 master node)  | 1               | 6                     |

If the `max_replication_slots` parameter is not set to a high enough value, synchronization replication still succeeds, but without the replication origin performance advantage. The publication server log file will contain the following warning in such cases:

```
WARNING: Failed to setup replication origin xdb_mmrnode_c_emp_pub_6. Reason:
ERROR: could not find free replication state slot for replication origin with OID 4
Hint: Increase max_replication_slots and try again.
```

**Example of Replication Slot Information:**
- Maximum allowable number of replication slots:
  ```sql
  SHOW max_replication_slots;
  ```
  Output:
  ```
  max_replication_slots
  -----------------------
  9
  (1 row)
  ```

- Displaying the replication slots:
  ```sql
  SELECT slot_name, slot_type, database, active FROM pg_replication_slots ORDER BY 1;
  ```
  Output:
  ```
  slot_name | slot_type | database | active
  -------------+-----------+-----------+--------
  xdb_47877_5 | logical   | mmrnode_a | t
  xdb_47878_5 | logical   | mmrnode_b | t
  xdb_47879_5 | logical   | mmrnode_c | t
  (3 rows)
  ```

- Displaying the replication origins:
  ```sql
  SELECT * FROM pg_replication_origin ORDER BY 2;
  ```
  Output:
  ```
  roident | roname
  ---------+--------------------------
  5       | xdb_mmrnode_a_emp_pub_39
  2       | xdb_mmrnode_a_emp_pub_6
  1       | xdb_mmrnode_b_emp_pub_1
  6       | xdb_mmrnode_b_emp_pub_39
  3       | xdb_mmrnode_c_emp_pub_1
  4       | xdb_mmrnode_c_emp_pub_6
  (6 rows)
  ```

The replication origin name is assigned in the format `xdb_srcdbname_pubname_remotedbid`, where `srcdbname` is the source database name, `pubname` is the publication name, and `remotedbid` is the publication database ID of a remote database.

The replication slots are in the active state when the publication server is running and are deactivated when the publication server is shut down. The replication slots and replication origin sessions are deleted from the database cluster when their corresponding master nodes are removed from the multi-master replication system using the xDB Replication Console or the xDB Replication Server CLI.

If some situation occurs where the replication slots are not properly deleted, see Section 10.3.4.4 for instructions on manually deleting them.

### 2.2.10.5 In-Memory Caching and Persistence
Data changes are fetched and stored in memory buffers to optimize the data replication process. This avoids the overhead associated with repeatedly fetching the same set of changes from the database server when there are multiple target databases. This approach is sufficient as long as all target databases are accessible during a replication event and the data fits within the available cache.

However, if one or more target databases are unavailable due to network connectivity problems, server downtime, etc., the in-memory data changes must be persisted for later retrieval when the target databases become available for synchronization with the source database.

The xDB Replication Server architecture uses Java object serialization to persist the in-memory state of the data. Object serialization converts object data and other relevant information into a sequence of bytes that can be stored in a file.

**Examples of Evicting In-Memory Data to Persistent Storage:**
- Before the next replication event, the in-memory cache is filled with data changes and needs to be evicted to accommodate a new set of changes.
- During a synchronization event, all changes available in the cache are applied successfully to some target databases, but one or more other target databases cannot be accessed. All applied changes held in memory must be persisted and retained so that these changes can be reloaded and applied when the inaccessible databases become available.

The cache size corresponds to the heap size configured for the publication server by the `-Xmxnnnm` setting of the `JAVA_HEAP_SIZE` parameter in the xDB Startup Configuration file. See Section 2.3.1.4 for information on the xDB Startup Configuration file.

The persistence I/O overhead can be minimized by increasing the heap size value and defining a more frequent synchronization interval, such as every few seconds. See Section 7.2 for information on setting a replication schedule.

Data changes are persisted in a local file on the host running the publication server. The file is stored in the directory `XDB_HOME/xdata`. Each time persistence occurs, a new file is created. After the files have been processed, they are periodically removed from disk.

### 2.2.11 Multi-Master Parallel Replication
In a multi-master replication system, transactions can be replicated from one master node to another using either the trigger-based method (see Section 2.2.9) or the log-based method (see Section 2.2.10).

For a single replication event to be considered finished and complete, transactions that have occurred on all master nodes since the previous replication event must be successfully replicated to all other master nodes by the configured synchronization method.

This involves a series of multiple replication sets, each identified by a master node acting as the source, which contains the transactions that need to be replicated to all other master nodes acting as targets. For a multi-master replication system consisting of `n` master nodes, there will be `n` such replication sets—each with a different master node acting as the source.

Since the initial support of multi-master replication systems in xDB Replication Server version 5.0, such a series of multiple replication sets were always initiated in a strictly serial manner. That is, the transaction replication from a source master node to all target master nodes must be completed before the start of the transaction replication from the next master node to all other target master nodes, and so on.

**Example:**
Consider a 3-master node system consisting of master nodes A, B, and C. If applications have applied transactions to tables in all three master nodes and a synchronization replication event is initiated, the transactions are replicated in the following manner:
1. Transactions made on master node A are replicated to master nodes B and C.
2. When Step 1 is completed, transactions made on master node B are replicated to master nodes A and C.
3. When Step 2 is completed, transactions made on master node C are replicated to master nodes A and B.

The latency time, which is the time to complete the entire replication event, is the sum of the replication times where each master node acts as the source (i.e., the sum of the times for steps 1, 2, and 3).

For the log-based method, this latency time has been reduced by implementing parallel replication, where each replication set from a given master node acting as the source runs simultaneously with all other replication sets where the other master nodes act as the source. Thus, a replication set from a master node does not wait for others to complete before it can start; steps 1, 2, and 3 all run simultaneously instead of one after the other.

**Note:** Parallel replication applies only to the log-based method and not to the trigger-based method. There is no required configuration setting to enable the use of parallel replication for the log-based MMR system.

In addition to parallel replication, optimization of replicating from a given master node to all other master nodes (i.e., within the context of a single replication set) has been implemented with the use of multiple threads. This is referred to as parallel synchronization and applies to both the trigger-based and log-based methods. See Section 5.8.2.2 for information on parallel synchronization.

### 2.2.12 Table Filters
Table filters specify the selection criteria for rows in publication tables or views that are to be included during replications to subscriptions from the publication database in a single-master replication system or between master nodes in a multi-master replication system. Rows that do not satisfy the selection criteria are excluded from replications to subscriptions or master nodes on which these table filters have been enabled.

#### 2.2.12.1 Implementing Table Filters
Implementing table filters is a two-part process:
1. **Defining Table Filters:** Define a set of available table filters during the process of creating the publication by specifying specific, named rules applicable to selected publication tables or views expressed in the form of SQL `WHERE` clauses.
2. **Enabling Table Filters:** Enable the defined table filters only on those subscription tables of a single-master replication system or master node tables of a multi-master replication system where filtering is to occur during replication to those particular target tables. No filtering occurs during replication to a target subscription table or master node table if no filters have been specifically enabled on that table in the subscription or master node.

It is strongly recommended to perform a snapshot replication to the subscriptions or master nodes that contain tables on which the filtering criteria have changed (by adding, removing, or modifying filter rules). A snapshot ensures that the content of the subscription tables or master node tables is consistent with the updated filtering criteria.

**Note (For MMR Only):** When using table filters in a multi-master replication system, the master definition node, which provides the source of the table content for a snapshot, should contain a superset of all the data contained in the other master nodes of the multi-master replication system. This ensures that the target of a snapshot receives all the data that satisfies any filtering criteria enabled on the other master nodes. If the master definition node contains only a subset of all the data contained in the other master nodes, a snapshot to another master node may not result in the complete set of data required for that target master node.

#### 2.2.12.2 Effects of Table Filtering
A filter enabled on a table only affects the results from snapshot or synchronization replications targeted to that table by the xDB Replication Server. Filtering has no effect on changes made directly on the target table by external user applications such as an SQL command line utility.

**Effects on a Targeted, Filtered Table:**
- **Snapshot Replication:** A row from the source table of the snapshot is inserted into the target table if the row satisfies the filtering criteria. Otherwise, the row is excluded from insertion into the target table.
- **INSERT Statement:** When an `INSERT` statement is executed on a source table followed by a synchronization replication, the row is inserted into the target table of the synchronization if the row satisfies the filtering criteria. Otherwise, the row is excluded from insertion into the target table.
- **UPDATE Statement:**
  - If a row in the result set has no corresponding row in the target table with the same primary key value, and the updated row in the result set satisfies the filtering criteria, the row is inserted into the target table.
  - If a row in the result set has a corresponding row in the target table with the same primary key value, and the updated row in the result set satisfies the filtering criteria, the row in the target table is updated accordingly.
  - If a row in the result set has a corresponding row in the target table with the same primary key value, and the updated row in the result set no longer satisfies the filtering criteria, the corresponding row in the target table is deleted.
- **DELETE Statement:**
  - If a row in the result set has a corresponding row in the target table with the same primary key value, the row with that primary key value is deleted from the target table.
  - If a row in the result set has no corresponding row in the target table with the same primary key value, no action is taken on the target table for that row.

Thus, regardless of whether the transaction on the source table is an `INSERT`, `UPDATE`, or `DELETE` statement, the goal of a table filter is to ensure that all rows in the target table satisfy the filter rule.

#### 2.2.12.3 Table Settings and Restrictions for Table Filters
- **REPLICA IDENTITY Setting for Filtering in a Log-Based Replication System:**
  For replication systems using the log-based method of synchronization replication, a publication table on which a filter is to be defined must have the `REPLICA IDENTITY` option set to `FULL`.

  **Note:** This `REPLICA IDENTITY FULL` setting is not required for tables in single-master, snapshot-only publications. See Section 2.2.7 for information on snapshot-only publications.

---

**Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved.**

**EDB Postgres Replication Server User’s Guide**