1. A streaming replication connection to the database server is opened using libpq
to establish a walsender communication channel.
2. A separate thread is used to monitor data changes streamed through the
walsender interface.
Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 32
EDB Postgres Replication Server User’s Guide
3. As the data changes become available, they are transformed to populate an in-
memory cache.
4. On the next scheduled interval, the in-memory cached data changes are applied to
each of the target databases in JDBC batches of SQL statements (referred to as
transaction sets) in the same manner as described in Section 2.2.9 for the trigger-
based method. If one or more target database servers are not accessible, the data
changes are saved in a local file on the host running the publication server.
Section 2.2.10.5 for information on in-memory caching and data persistence.
5. The value of the WAL segment’s log sequence number (LSN) identifying the last
set of applied changes based on the last replicated transaction is updated. The
update is confirmed to the database server.
6. The applied data changes are cleared from the in-memory cache.
7. Steps 3 through 6 are repeated.
Note: A single SQL statement executed against a source table may result in many rows
modified and returned in the changeset stream, and therefore, many SQL statements
executed against the target table. For example, if a single UPDATE statement affects 10
rows in the source table, 10 rows will be returned in the changeset stream – one for each
row in the source table that was updated. When the publication server applies the changes
to the target table, 10 UPDATE statements will be executed.
2.2.10.4 Replication Origin
Starting with Postgres version 9.5, a feature called replication origin has been introduced
to the logical decoding framework. Replication origin allows an application to identify,
label, and mark certain aspects of a logical decoding session.
For information on replication origin see the PostgreSQL Core Documentation located at:
https://www.postgresql.org/docs/current/static/replication-origins.html
For the log-based method of synchronization replication, this provides performance
improvement provided that the master nodes are running under Postgres version 9.5 or
later.
As previously described, the log-based method uses the WAL files to obtain the changes
applied to the publication tables. After the changes are retrieved through the walsender
interface, the publication server applies the set of changes to the other master nodes using
transaction sets consisting of JDBC batches of SQL statements. When these changes are
applied to the tables in the other target master nodes, the same changes are also recorded
in the WAL files of each database server hosting the target master nodes.
Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 33
EDB Postgres Replication Server User’s Guide
These redundant or “replayed” changes are included in the changeset stream received by
the publication server. These replayed changes must be ignored and not applied since
they are duplicates of all changes that have already been applied to the target tables
through the JDBC batches.
The replayed changes result in performance overhead as all such changes are transmitted
over the network from the database server to the publication server, and then the
publication server must discard such redundant changes.
With the replication origin feature, the publication server is able to set up the logical
decoding sessions so that these replayed changes are not included in the changeset stream
transmitted over the network to the publication server thus eliminating this performance
overhead.
The following are the conditions under which replication origin is used:
 Replication origin applies to multi-master replication systems only, not to single-
master replication systems.
 Replication origin eliminates streaming of replayed changes only from Postgres
versions 9.5 or later. Replayed changes are still included in the changeset stream
from Postgres version 9.4, but are discarded by the publication server. Thus multi-
master replication systems consisting of both Postgres versions 9.4 and 9.5 utilize
the replication origin advantage on the 9.5 database servers.
 The max_replication_slots configuration parameter must be set at a certain
minimal level to ensure that the publication server can create the additional
replication slots for replication origin.
For each master node database, in addition to the replication slot used for the changeset
stream, an additional number of replication slots is required – one additional slot
corresponding to every other master node to support the replication origin usage. Thus,
for each master node, the total number of replication slots required is equal to the total
number of master nodes in the entire MMR system.
Therefore, for a given database server (that is, a Postgres database cluster containing
master node databases), the total number of replication slots required is equal to the total
number of master nodes in the entire MMR system multiplied by the number of master
node databases residing within the given database cluster.
For example, assume the usage of a 6-node multi-master replication system using three
database clusters as follows:
 Database cluster #1 contains 3 master node databases.
 Database cluster #2 contains 2 master node databases.
 Database cluster #3 contains 1 master node database.
Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 34
EDB Postgres Replication Server User’s Guide
The total number of master nodes is six. Multiply the number of master node databases in
each database cluster by six to give the required minimum setting for
max_replication_slots for that database cluster.
The following table shows the required, minimum settings for
max_replication_slots as well as max_wal_senders.
Table 2-1 – Replication Origin Configuration Parameter Settings
Postgres Database Server max_wal_senders max_replication_slots
Cluster #1 (3 master nodes) 3 18
Cluster #2 (2 master nodes) 2 12
Cluster #3 (1 master node) 1 6
If the max_replication_slots parameter is not set to a high enough value,
synchronization replication still succeeds, but without the replication origin performance
advantage.
The publication server log file contains the following warning in such cases:
WARNING: Failed to setup replication origin xdb_mmrnode_c_emp_pub_6. Reason:
ERROR: could not find free replication state slot for replication origin with
OID 4
Hint: Increase max_replication_slots and try again.
The following example shows some of the replication slot information for a 3-master
node system running on a single database cluster.
The following shows the maximum allowable number of replication slots:
SHOW max_replication_slots;
max_replication_slots
-----------------------
9
(1 row)
The number should be sufficiently greater than the number of replication slots and
replication origins currently allocated.
The following displays the replication slots:
SELECT slot_name, slot_type, database, active FROM pg_replication_slots ORDER
BY 1;
slot_name | slot_type | database | active
-------------+-----------+-----------+--------
xdb_47877_5 | logical | mmrnode_a | t
xdb_47878_5 | logical | mmrnode_b | t
xdb_47879_5 | logical | mmrnode_c | t
(3 rows)
Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 35
EDB Postgres Replication Server User’s Guide
The following shows the replication origins.
SELECT * FROM pg_replication_origin ORDER BY 2;
roident | roname
---------+--------------------------
5 | xdb_mmrnode_a_emp_pub_39
2 | xdb_mmrnode_a_emp_pub_6
1 | xdb_mmrnode_b_emp_pub_1
6 | xdb_mmrnode_b_emp_pub_39
3 | xdb_mmrnode_c_emp_pub_1
4 | xdb_mmrnode_c_emp_pub_6
(6 rows)
The replication origin name is assigned in the format
xdb_srcdbname_pubname_remotedbid where srcdbname is the source database
name, pubname is the publication name, and remotedbid is the publication database ID
of a remote database.
The replication slots are in the active state when the publication server is running. The
replication slots are deactivated when the publication server is shut down.
The replication slots and replication origin sessions are deleted from the database cluster
when their corresponding master nodes are removed from the multi-master replication
system using the xDB Replication Console or the xDB Replication Server CLI.
Should some situation occur where the replication slots are not properly deleted when
required, see Section 10.3.4.4 for instructions on manually deleting them.
2.2.10.5 In-Memory Caching and Persistence
The data changes are fetched and stored in memory buffers to optimize the data
replication process. This avoids the overhead associated with repeatedly fetching the
same set of changes from the database server when there are multiple target databases.
This approach is sufficient as long as all of the target databases are accessible during a
replication event and the data fits within the available cache.
However, if one or more of the target databases is unavailable due to network
connectivity problems, server down time, etc. the in-memory data changes must be
persisted for later retrieval when the target databases becomes available for
synchronization with the source database.
The xDB Replication Server architecture utilizes Java object serialization to persist the
in-memory state of the data. Object serialization is the conversion of object data and
other relevant information to a sequence of bytes that can then be stored in a file.
The following are examples that can result in the eviction of in-memory data to persistent
storage:
Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 36
EDB Postgres Replication Server User’s Guide
 Before the next replication event occurs, the in-memory cache is filled with the
data changes and needs to be evicted to accommodate a new set of changes.
 In the replication system, there are multiple target databases. During a
synchronization event, all of the changes available in the cache are applied
successfully to some of the target databases. However one or more of the other
target databases cannot be accessed. All of the applied changes held in memory
must be persisted and retained so that these changes can be reloaded and applied
when the inaccessible databases becomes available.
The cache size corresponds to the heap size configured for the publication server by the -
Xmxnnnm setting of the JAVA_HEAP_SIZE parameter in the xDB Startup Configuration
file. See Section 2.3.1.4 for information on the xDB Startup Configuration file.
The persistence I/O overhead can be minimized by increasing the heap size value and
defining a more frequent synchronization interval such as for every few seconds. See
Section 7.2 for information on setting a replication schedule.
The data changes are persisted in a local file on the host running the publication server.
The file is stored in the directory XDB_HOME/xdata.
Each time persistence occurs, a new file is created. After the files have been processed,
they are periodically removed from disk.
2.2.11 Multi-Master Parallel Replication
For a multi-master replication system, transactions can be replicated from one master
node to another by one of the synchronization methods described in the previous sections
– either the trigger-based method (see Section 2.2.9) or the log-based method (see
Section 2.2.10).
For a single replication event to be considered finished and complete, transactions that
have occurred on all master nodes since the previous replication event must be
successfully replicated to all other master nodes by the configured synchronization
method.
This consists of a series of multiple replication sets, each identified by a master node
acting as the source master node, which contains the transactions that needs to be
replicated to all other master nodes acting as the target master nodes. So for a multi-
master replication system consisting of n number of master nodes, there will be n such
replication sets – each with a different master node acting as the source.
Since the initial support of multi-master replication systems in xDB Replication Server
version 5.0, such a series of multiple replication sets were always initiated in a strictly
serial manner. That is, the transaction replication from a source master node to all target
master nodes must be completed before the start of the transaction replication from the
next master node to all other target master nodes, and so on.
Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 37
EDB Postgres Replication Server User’s Guide
For example, consider a 3-master node system consisting of master node A, master node
B, and master node C.
If applications have applied transactions to tables in all three master nodes and then a
synchronization replication event is initiated either on demand by the xDB Replication
Console, an xDB Replication Server CLI command, or by a scheduled replication, the
transactions are replicated in the following manner:
1. Transactions that were made on master node A are replicated to master node B
and master node C.
2. When Step 1 has been completed, transactions that were made on master node B
are replicated to master node A and master node C.
3. When Step 2 has been completed, transactions that were made on master node C
are replicated to master node A and master node B.
The time to complete the entire replication event, referred to as the latency time, is
basically the sum of the replication times where each master node acts as the source (that
is, the sum of the times for steps 1, 2, and 3).
For the log-based method, this latency time has been reduced by the implementation of
parallel replication whereby each replication set from a given master node acting as the
source, executes and runs simultaneously with all other replication sets where the other
master nodes act as the source.
Thus, a replication set from a master node is not waiting for others to complete before it
can start so steps 1, 2, and 3 all run simultaneously instead of one after the other.
Note that parallel replication applies only to the log-based method and not for the trigger-
based method.
There is no required configuration setting to enable the use of parallel replication for the
log-based MMR system.
Note: In addition to parallel replication, optimization of replicating from a given master
node to all other master nodes (that is, within the context of a single replication set) has
been implemented with the use of multiple threads. This is referred to as parallel
synchronization. Parallel synchronization applies to both the trigger-based and log-based
methods. See Section 5.8.2.2 for information on parallel synchronization.
2.2.12 Table Filters
Table filters specify the selection criteria for rows in publication tables or views that are
to be included during replications to subscriptions from the publication database in a
single-master replication system or between master nodes in a multi-master replication
Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 38
EDB Postgres Replication Server User’s Guide
system. Rows that do not satisfy the selection criteria are excluded from replications to
subscriptions or master nodes on which these table filters have been enabled.
2.2.12.1 Implementing Table Filters
Implementing table filters is a two-part process. First, a set of available table filters must
be defined. This can be done during the process of creating the publication by defining
specific, named rules applicable to selected publication tables or views expressed in the
form of SQL WHERE clauses.
Once a set of available table filters have been defined, they must be enabled only on
those subscription tables of a single-master replication system or master node tables of a
multi-master replication system where filtering is to occur during replication to those
particular target tables. No filtering occurs during replication to a target subscription table
or master node table if no filters have been specifically enabled on that table in the
subscription or master node.
It is strongly recommended that a snapshot replication be performed to the subscriptions
or master nodes that contain tables on which the filtering criteria has changed either by
the addition of filter rules, the removal of filter rules, or the modification of existing filter
rules.
A snapshot ensures that the content of the subscription tables or master node tables is
consistent with the updated filtering criteria.
Note (For MMR only): When using table filters in a multi-master replication system, the
master definition node, which provides the source of the table content for a snapshot,
should contain a superset of all the data contained in the other master nodes of the multi-
master replication system. This ensures that the target of a snapshot receives all of the
data that satisfies any filtering criteria enabled on the other master nodes.
On the contrary, if the master definition node contains only a subset of all the data
contained in the other master nodes, then a snapshot to another master node may not
result in the complete set of data that is required for that target master node.
2.2.12.2 Effects of Table Filtering
A filter enabled on a table only affects the results from snapshot or synchronization
replications targeted to that table by the xDB Replication Server. Filtering has no effect
on changes made directly on the target table by external user applications such as an SQL
command line utility.
Filtering has the following effects on a targeted, filtered table.
Note: In the following discussion, a result set refers to the set of rows in a table satisfying
the selection criteria of an UPDATE or DELETE statement executed on that table.
Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 39
EDB Postgres Replication Server User’s Guide
In a snapshot replication, a row from the source table of the snapshot is inserted into the
target table if the row satisfies the filtering criteria. Otherwise the row is excluded from
insertion into the target table.
When an INSERT statement is executed on a source table followed by a synchronization
replication, the row is inserted into the target table of the synchronization if the row
satisfies the filtering criteria. Otherwise the row is excluded from insertion into the target
table.
When an UPDATE statement is executed on a source table followed by a synchronization
replication, the UPDATE result set of the source table determines the action on the target
table of the synchronization as follows.
 If a row in the result set has no corresponding row in the target table with the
same primary key value, and the updated row in the result set satisfies the filtering
criteria, then the row is inserted into the target table. (That is, a row that was
previously non-existent in the target table is added because the updated row in the
source table now satisfies the filtering criteria.)
 If a row in the result set has a corresponding row in the target table with the same
primary key value, and the updated row in the result set satisfies the filtering
criteria, then the row in the target table is updated accordingly. (That is, the
update is applied to an existing, matching row in the target table that still satisfies
the filtering criteria after the update.)
 If a row in the result set has a corresponding row in the target table with the same
primary key value, and the updated row in the result set no longer satisfies the
filtering criteria, then the corresponding row in the target table is deleted. (That is,
an existing, matching row in the target table no longer satisfies the filtering
criteria after the update, so the row is removed from the target table.)
When a DELETE statement is executed on a source table followed by a synchronization
replication, the DELETE result set of the source table determines the action on the target
table of the synchronization as follows.
 If a row in the result set has a corresponding row in the target table with the same
primary key value, then the row with that primary key value is deleted from the
target table. (That is, an existing, matching row in the target table is removed.)
 If a row in the result set has no corresponding row in the target table with the
same primary key value, then no action is taken on the target table for that row.
(That is, there is no existing, matching row in the target table, so there is no row
to remove from the target table.)
Thus, regardless of whether the transaction on the source table is an INSERT, UPDATE, or
DELETE statement, the goal of a table filter is to ensure that all rows in the target table
satisfy the filter rule.
Copyright © 2010 - 2018 EnterpriseDB Corporation. All rights reserved. 40
EDB Postgres Replication Server User’s Guide
2.2.12.3 Table Settings and Restrictions for Table Filters
This section lists specific table settings and restrictions on the use of table filters.
REPLICA IDENTITY Setting for Filtering in a Log-Based Replication System
For replication systems using the log-based method of synchronization replication, a
publication table on which a filter is to be defined must have the REPLICA IDENTITY
option set to FULL.
Note: This REPLICA IDENTITY FULL setting is not required for tables in single-
master, snapshot-only publications, See Section 2.2.7 for information on snapshot-only