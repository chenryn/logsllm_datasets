were configured to use a combination of access methods and IP
protocols for our initial measurement framework validation. We
verified the clocks on each end of the connection. All had accurate
and consistent notions of time using NTP. Our RPs were topologi-
cally diverse and verified to consistently reach all PPs with latency
of at most 1 second. Using default RP software settings we verified
the expected synchronization refresh intervals shown in Table 1
with what we observed from the RPs to the PPs.
Having confirmed our visibility and the ability to accurately
detect interval frequency patterns, these directly observed vantage
points establish our ground truth and baseline. We can then fit RP
software default timers and observed interval patterns onto our
entire pool of PP access logs to fingerprint uncontrolled RP cache
servers and identify anomalous operating patterns.
3 RESULTS
In this section, we study the real-world access behavior of RPKI
RP deployments from our vertical and horizontal measurement
framework. We analyze insight from our own controlled PPs in the
delegation tree along with contributed data from two RIR-operated
PPs.1 This allows us to spot anomalies, identify trends, uncover
problems, and propose enhancements to improve the system.
3.1 Completeness of RPKI View
We first compare the synchronization patterns among RPs as seen
from different PPs to answer our initial question: Do RPs fetch data
from each and every PP?
Figure 2 depicts the global RP population over time by counting
distinct RP IPv4 or IPv6 addresses seen each day, distinguished by
1AFRINIC and APNIC PP access logs, not publicly available.
different PP and fetching protocols.2 The number of RPs is clearly
on the rise. rsync activities at our PP dropped once RRDP was
activated at the end of 2019, because modern RP software prefers
RRDP over rsync. RIR PPs, however, continue to see both protocols,
because the Trust Anchor Locator (TAL) specifies a rsync URI and
RPs are advised to use this URI to retrieve the object referenced at
every refresh interval [16]. This is why the AFRINIC PP did not see
a similar drop in rsync when it activated RRDP in March, 2020.
While the access patterns are congruent among PPs, each PP
sees different numbers of RPs, suggesting that not all RPs have a
complete set of RPKI repository data. Particularly noteworthy is
that our PP appears to see roughly 20% fewer RPs than the RIR
PP parents. Surprisingly, a consistent set of RPs appear unable or
unwilling to retrieve RPKI data from child PPs. We find over 80%
of the missing RPs are using rsync to the RIR PPs the same day.
We found that some RPs would eventually return to our child PP
on another day using RRDP, but access was often intermittent.
Furthermore, about 60% of these missing RPs have synchronization
interval patterns matching the RIPEv2 or RIPEv3 validator. A partial
explanation for this phenomenon is revealed in an access method
packet filter experiment we conducted and detailed in § 3.4.
3.2 Type of Networks Hosting RPs
There has been relatively little discussion or guidance provided
to operators on RP deployment strategies. The rpki-rtr specifi-
cation [8] suggests three deployment models, where RPKI cache
data is maintained: (i) by an upstream provider or third party, (ii) in
one or more local cache systems, or (iii) throughout a distributed
set of cache systems within each major network region. Our own
experience and initial anecdotal evidence suggested that most oper-
ators setup one or two distinct RPs for their entire network, while
a small handful of operators implemented their own unique, and
often times complex, design strategies encompassing aspects of the
three basic deployment scenarios.
2There have been moments of transient data loss, but we don’t believe these affect
overall conclusions.
MarAprMayJunJulAugSepOctNovDecJan2020FebMarAprDate [days]20040060080010001200RPs [# distinct IP addresses]AFRINICAPNICResearch PPaggregateMarAprMayJunJulAugSepOctNovDecJan2020FebMarAprDate [days]020040060080010001200RPs [# distinct IP addresses]AFRINICAPNICResearch PPaggregateIMC ’20, October 27–29, 2020, Virtual Event, USA
Kristoff, et al.
Figure 4: The number of RPs seen per ASN in the first four
months of 2020.
Figure 3: RPs deployed over time by classification of origi-
nating network.
Figure 3 shows the location of RPs based on the self-reported net-
work classification as published in PeeringDB [27]. Recent upticks
in RP growth are driven by three sectors: Cable/DSL/ISP, Content,
and NSP (network service provider). Content networks have been
responsible for a noticeable rise in RPs led by Facebook, which went
from zero RPs in 2019 to nearly 70 in early 2020. While hosting
networks are commonly used for RPs, such as Linode and DigitalO-
cean with approximately 50 and 25 RPs respectively in 2020, most
networks appear to be deploying RPs within their own network
boundaries near where their routers would reside.
To gain a sense of RP deployment by network, we graph the
cumulative distribution function of distinct RP addresses per ASN
seen in the first four months of 2020 in Figure 4. Over extended
periods of time we found that the majority of RPs can be classified
as server-class systems with stable source IP addresses. On average,
80% of RPs IP addresses will return each month and approximately
10% of all RPs have rpki in the DNS hostname. Even the few cases
where RPs are not static server systems, this data provides a unique
window into the natural roving population of RPs.
We were not surprised to find hosting providers with dozens
of RPs over the course of monitoring period. A number of opera-
tors, researchers, or limited test RP installations appear on these
networks for a variety of reasons. However, the network with the
largest number of RPs, over 1000 distinct IP addresses, was a DNS
infrastructure service provider with an elaborate RP design and
deployment model. When we reached out to them to understand
why so many RPs originate from their network, they explained
that they deploy dozens of RPs throughout their network using a
container-based model. These containers are transient nodes, with
unstable addresses that may change frequently over short durations
as network conditions change.
We also observed access via relay nodes of the Tor onion overlay,
which not only obfuscates the IP address and location of the actual
RP but also leads to distinct RP IP addresses per PP. One reason for
this could be to conceal planned RPKI deployment, which we will
study in future work.
3.3 Timeliness
Having considered the differences in access and view in the previ-
ous sections, we turn our attention on how current a RPKI view
RPs are seeing. We refer to this quality as timeliness, which is a
measure primarily derived from the configured refresh interval
(aka synchronization schedule) of an RP. An RP must balance a re-
fresh interval to ensure it obtains current data, but fetching too
aggressively may place unnecessary strain on the PP infrastructure
or waste RP cycles. The specifications are vague on RP refresh guid-
ance, but suggest local RPs be “synchronized with each other at
least every four to six hours.” [6] An active IETF Internet-Draft [9]
suggests rsync-based refresh intervals of once an hour and RRDP
polling intervals of no more than every ten minutes. In practice,
modern RP software defaults to one hour or less refresh intervals
(refer to Table 1).
We examine inter-arrival connection times from each RP address
to a PP in the wild. To avoid long term instability effects, we ran-
domly choose a representative sample day. Most stable RPs will
synchronize with a PP multiple times per day. Based on current
practices and common recommendations we set a lower bound of
20 syncs per day. We reason that this helps ensure we consider only
stable RP configurations and exclude short-lived, unstable RPs that
may be used for testing or lab environments. However, on average
up to 20% of RP IP addresses still fall below this threshold, which
is significant. This would suggest a sizable population of RPs are
lagging severely behind what is in the RPKI repository. Some RIRs
update their repository as often as every few minutes if there are
pending changes.
Our calculated interval from the minimum threshold connec-
tions are shown for all RIR PPs on March 4, 2020 and the year prior
on the same date, see Figures 5 and 6. We notice RRDP exhibits
reasonably predictable behavior compared to rsync, which varies
from PP to PP and can be quite noisy. We believe this is in part
due to the nature of rsync integration with RP software, which
is a system call with less predictable completion times when re-
fresh intervals tend to be longer using this access method. AFRINIC
rsync frequency intervals are roughly what we might expect, while
APNIC rsync seems to have peaks at unexpected intervals. When
we evaluate corresponding user-agent strings in RRDP from the
same IP addresses at APNIC, the majority of rsync clients at the
unmarked 5-minute interval appear to be from RIPEv3 installations.
We find no commonality of source IP address or origin AS among
MarAprMayJunJulAugSepOctNovDecJan2020FebMarAprDate [days]050100150200250300350400RPs [# distinct IP addresses]Cable/DSL/ISPContentEducation/ResearchEnterpriseNANon-ProfitNot DisclosedNSP100101102103Relying Parties [# IP addresses per origin ASN]0.50.60.70.80.91.0CDFOn Measuring RPKI Relying Parties
IMC ’20, October 27–29, 2020, Virtual Event, USA
(a) rsync
(b) RRDP
Figure 5: Average connection interval at AFRINIC.
(a) rsync
(b) RRDP
Figure 6: Average connection interval at APNIC.
these RPs. We suspect, but cannot verify that there exists or used
to exist a build or common configuration for this software with
that frequency interval. The small rsync peaks at 15 minutes con-
sists primarily of rsync-only clients. Again we were unable to find
any source material documenting this refresh rate from existing
software.
In 2019, rsync was still the dominant access method used by
RPs. We found rsync frequency intervals to only partially track
modern RP software implementations with the implication that
a number of RPs are running earlier generations of RP code and
custom synchronization schedules.
When using RRDP, all well-known RP software implementations
set an identifying user agent string in the HTTP connection. Since
most RPs use a combination of RRDP and rsync, we leverage the
RP IP address history from our vertical and horizontal views across
a set of PPs. We are able to see RRDP fingerprints for roughly 95%
for all RPs that connect using rsync this way.
The remaining small number of RPs we are unable to fingerprint
by correlating RRDP user-agent strings from a corresponding IP
address are rsync-only clients. There is one well known current
implementation, rpki-client, and a couple of older ones (rcynic,
rpstir) that support only rsync through cron with a recommended
1-hour synchronization refresh interval by default. [11, 18, 29] As
as a result of our child PP configuration we discovered that all
rsync-fetching RPs to our PP will include a trailing slash (/) of
the replicated directory in the initial synchronization command
with the exception of rpki-client. Since rpki-client is the only
active implementation of an rsync-only RP this allows us to identify
nearly all RP clients. As of this writing (Summer 2020), our PP sees
approximately 40 distinct rpki-client RPs per day.
2102060RP refresh interval [minutes]50050100150Binned RPs [# of distinct IP addresses]RIPEv3 (2min)Routinator (10min)OctoRPKI (20min)FORT/cron (60min)201920202102060RP refresh interval [minutes]50050100150Binned RPs [# of distinct IP addresses]RIPEv3 (2min)Routinator (10min)OctoRPKI (20min)FORT/cron (60min)20202102060RP refresh interval [minutes]50050100150Binned RPs [# of distinct IP addresses]RIPEv3 (2min)Routinator (10min)OctoRPKI (20min)FORT/cron (60min)201920202102060RP refresh interval [minutes]50050100150Binned RPs [# of distinct IP addresses]RIPEv3 (2min)Routinator (10min)OctoRPKI (20min)FORT/cron (60min)20192020IMC ’20, October 27–29, 2020, Virtual Event, USA
Kristoff, et al.
(a) Research Publication Point PP-a
(b) Research Publication Point PP-b
Figure 7: Impact of blocking synchronization protocols. On PP-a RRDP was first blocked on May 8 while rsync was still allowed,
then rsync was blocked on May 13 while RRDP was allowed, and on PP-b vice versa. rsync fall-back rarely happened.
3.4 Filtering Experiment
Operating our own PP server enables us to shed light on unexpected
RP behavior. When we evaluated the effect of disabling one of the
access methods, (i.e., the mandatory rsync vs. emerging RRDP) we
were surprised by what we uncovered.
Most RP software have configuration settings to handle transient
connection failures. PP operators can mitigate failures by deploying
multiple PP instances such as with the use of DNS round robin or
load balancing solutions. RPs will typically retry unreachable PPs
at every synchronization interval.
Most PPs provide service interfaces for RRDP (a HTTPS-based
service) as well as rsync (TCP port 873). On May 8, we blocked
access from any RP to the RRDP service on one child PP-a and
blocked rsync on a different child PP-b. On May 13, we swapped
the protocol blocking on each server. We expected RPs that were
using RRDP to fall back to rsync after a sufficient period of time.
This would comply with current specs [20]. Conversely, we did not
anticipate many RPs to migrate to RRDP from rsync; we assumed
those RPs probably do not yet support RRDP or have been manually
set to support only rsync. With our child CA we verified compli-
ance with technical specifications, i.e., how RPs would behave if
one of the two access protocols suddenly became unavailable. This
allows us to assess both robustness in general and potential transi-
tion scenarios in case rsync is deprecated. As shown in Figure 7,
many RPs do not use implementations that comply with current
specifications. The sudden decrease of RRDP sources at a on May
8 should show a corresponding rise in rsync when RPs discover
RRDP has failed. Likewise we would expect the same at b on May
13. However, most RP software accounting for nearly 90% of all RPs
do not fall back to rsync. For software popularity see Appendix C.
This experiment exposes a looming problem in RP behavior with
delegated PPs. For example, if provider A publishes the associated
ROA for an IPV4 /16, allocates a /19 from it to child B, and the child
CA has the ROA which is not fetched, B’s announcement for the
/19 will be judged invalid. Either through natural failure modes or
a targeted service interface attack, the loss of the RRDP service
interface on a child PP can lead to a loss of reachability for B’s /19
by any network operator using the RPKI for route validation. This
RP behavior anomaly led to a lively exchange on the IETF SIDROPS
working group mailing list [7].
4 CONCLUSION AND OUTLOOK
The RPKI is the rapidly growing foundation upon which improved
security of the Internet BGP routing system is being constructed.