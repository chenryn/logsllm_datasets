SMART: Secure and minimal architecture for (establishing dynamic)
root of trust. In NDSS. Internet Society, 2012.
[21] Karim Eldefrawy et al. SMART: Secure and minimal architecture for
(establishing a dynamic) root of trust. In NDSS, 2012.
[22] Aurélien Francillon et al. A minimalist approach to remote attestation.
In DATE, 2014.
[23] Olivier Girard. openMSP430, 2009.
[24] Chris Hawblitzel, Jon Howell, Jacob R Lorch, Arjun Narayan, Bryan
Parno, Danfeng Zhang, and Brian Zill. Ironclad apps: End-to-end security
via automated full-system veriﬁcation. In OSDI, volume 14, pages 165–
181, 2014.
[25] Intel. Intel Software Guard Extensions (Intel SGX).
[26] Ahmed Irfan, Alessandro Cimatti, Alberto Griggio, Marco Roveri, and
Roberto Sebastiani. Verilog2SMV: A tool for word-level veriﬁcation. In
Design, Automation & Test in Europe Conference & Exhibition (DATE),
2016, pages 1156–1159. IEEE, 2016.
[27] Rick Kennell and Leah H Jamieson. Establishing the genuinity of remote
computer systems. In USENIX, 2003.
[28] Gerwin Klein, Kevin Elphinstone, Gernot Heiser, June Andronick, David
Cock, Philip Derrin, Dhammika Elkaduwe, Kai Engelhardt, Rafal Kolan-
ski, Michael Norrish, Thomas Sewell, Harvey Tuch, and Simon Winwood.
seL4: Formal veriﬁcation of an OS kernel. In Proceedings of the ACM
SIGOPS 22Nd Symposium on Operating Systems Principles, SOSP ’09,
pages 207–220, New York, NY, USA, 2009. ACM.
[29] Patrick Koeberl, Steffen Schulz, Ahmad-Reza Sadeghi, and Vijay Varad-
harajan. TrustLite: A security architecture for tiny embedded devices.
In EuroSys. ACM, 2014.
[30] P. Koeberl et al. TrustLite: A security architecture for tiny embedded
devices. In EuroSys, 2014.
[31] X. Kovah et al. New results for timing-based attestation. In IEEE S&P
’12, 2012.
[32] Xavier Leroy. Formal veriﬁcation of a realistic compiler. Communica-
tions of the ACM, 52(7):107–115, 2009.
[33] Yanlin Li, Jonathan M. McCune, and Adrian Perrig. Viper: Verifying
the integrity of peripherals’ ﬁrmware. In CCS. ACM, 2011.
[34] Jonathan M McCune, Bryan J Parno, Adrian Perrig, Michael K Reiter,
and Hiroshi Isozaki. Flicker: An execution infrastructure for tcb mini-
mization. In Proceedings of the 3rd ACM SIGOPS/EuroSys European
Conference on Computer Systems 2008, pages 315–328, 2008.
[35] Job Noorman, Jo Van Bulck, Jan Tobias Mühlberg, Frank Piessens, Pieter
Maene, Bart Preneel, Ingrid Verbauwhede, Johannes Götzfried, Tilo
Müller, and Felix Freiling. Sancus 2.0: A low-cost security architecture
for iot devices. ACM Trans. Priv. Secur., 20(3):7:1–7:33, July 2017.
[36] Ivan De Oliveira Nunes, Ghada Dessouky, Ahmad Ibrahim, Norrathep
Rattanavipanon, Ahmad-Reza Sadeghi, and Gene Tsudik. Towards sys-
tematic design of collective remote attestation protocols. In ICDCS,
2019.
[37] Jr. Petroni et al. Copilot — A coprocessor-based kernel runtime integrity
monitor. In USENIX, 2004.
[38] Srivaths Ravi, Anand Raghunathan, and Srimat Chakradhar. Tamper
resistance mechanisms for secure embedded systems. In VLSI Design,
2004. Proceedings. 17th International Conference on, pages 605–611.
IEEE, 2004.
[39] Arvind Seshadri, Mark Luk, Adrian Perrig, Leendert van Doorn, and
Pradeep Khosla. Scuba: Secure code update by attestation in sensor
networks. In ACM workshop on Wireless security, 2006.
[40] Arvind Seshadri, Adrian Perrig, Leendert Van Doorn, and Pradeep
Khosla. SWATT: Software-based attestation for embedded devices.
In IEEE S&P ’04, 2004.
[41] A. Seshadri et al. Pioneer: Verifying code integrity and enforcing untam-
pered code execution on legacy systems. In ACM SOSP, 2005.
[42] Trusted Computing Group. Trusted platform module (tpm), 2017.
[43] A Virtualization. Secure virtual machine architecture reference manual.
AMD Publication, 33047, 2005.
[44] Shaza Zeitouni, Ghada Dessouky, Orlando Arias, Dean Sullivan, Ahmad
Ibrahim, Yier Jin, and Ahmad-Reza Sadeghi. Atrium: Runtime attestation
resilient under memory attacks. In Proceedings of the 36th International
Conference on Computer-Aided Design, pages 384–391. IEEE Press,
2017.
[45] Jean-Karim Zinzindohoué, Karthikeyan Bhargavan, Jonathan Protzenko,
and Benjamin Beurdouche. Hacl*: A veriﬁed modern cryptographic
library. In Proceedings of the 2017 ACM SIGSAC Conference on Com-
puter and Communications Security, pages 1789–1806. ACM, 2017.
USENIX Association
29th USENIX Security Symposium    785
APPENDIX
otherwise
otherwise
A Sub-Module Veriﬁcation
APEX is designed as a set of seven sub-modules. We now de-
scribe APEX’s veriﬁed implementation, by focusing on two
of these sub-modules and their corresponding properties. The
Verilog implementation of omitted sub-modules is available
in [1]. Each sub-module enforces a sub-set of the LTL speciﬁ-
cations in Deﬁnition 6. As discussed in Section 6, sub-modules
are designed as FSMs. In particular, we implement them as
Mealy FSMs, i.e, their output changes as a function of both the
current state and current input values. Each FSM takes as input
a subset of the signals shown in Figure 2 and produces only
one output – EXEC – indicating violations of PoX properties.
To simplify the presentation, we do not explicitly represent
the value of EXEC for each state transition. Instead, we deﬁne
the following implicit representation:
1. EXEC is 0 whenever an FSM transitions to NotExec
state.
is triggered.
2. EXEC remains 0 until a transition leaving NotExec state
3. EXEC is 1 in all other states.
4. Sub-modules composition: Since all PoX properties
must simultaneously hold, the value of EXEC produced
by APEX is the conjunction (logical AND) of all sub-
modules’ individual EXEC ﬂags.
(PC  ERmax)
PC = ERmin ∧ ¬ irq
notER
otherwise
(PC  ERmax)
∧¬ irq
PC = ERmin∧
¬[Wen ∧ (Daddr ∈ METADATA)]∧
¬[DMAen ∧ (DMAaddr ∈ METADATA)]
Run
[Wen ∧ (Daddr ∈ METADATA)]∨
[DMAen ∧ (DMAaddr ∈ METADATA)]
NotExec
Figure 7: Veriﬁed FSM for LTL 10, a.k.a., MP3- Challenge
Temporal Consistency.
The only possible path from notER to midER is through f stER.
Similarly, the only path from midER to notER is through lstER.
A transition to the NotExec state is triggered whenever: (1) any
sequence of values for PC do not follow the aforementioned
conditions, or (2) irq is logical 1 while PC is inside ER. Lastly,
the only way to transition out of the NotExec state is to restart
ER’s execution.
Figure 7 shows the FSM veriﬁed to comply with LTL 10
(MP3- Challenge Temporal Consistency). The FSM has two
states: Run and NotExec. The FSM transitions to the NotExec
state and outputs EXEC = 0 whenever a violation happens, i.e.,
whenever METADATA is modiﬁed in software. It transitions
back to Run when ER’s execution is restarted without such
violation.
B Proofs of Implementation Correctness & Se-
curity
In this section we discuss the computer proof for APEX’s imple-
mentation correctness (Theorem 1) and the reduction, showing
that APEX is a secure PoX architecture as long as VRASED is
a secure RA architecture (Theorem 2). A formal LTL computer
PC = ERmin
∧¬ irq
f stER
PC = ERmin ∧¬ irq
otherwise
NotExec
otherwise
lastER
PC = ERmax
∧¬ irq
Theorem 1. Deﬁnition 4∧ LTLs 3 –12 → Deﬁnition 5.
(PC > ERmin ∧ PC  ERmin ∧ PC < ERmax)
∧¬ irq
Figure 6: Veriﬁed FSM for LTLs 4-6, a.k.a., EP2- Ephemeral
Atomicity.
Figure 6 represents a veriﬁed model enforcing LTLs 4-6,
corresponding to the high-level property EP2- Ephemeral
Atomicity. The FSM consists of ﬁve states. notER and midER
represent states when PC is: (1) outside ER, and (2) within
ER respectively, excluding the ﬁrst (ERmin) and last (ERmax)
instructions. Meanwhile, f stER and lstER correspond to states
when PC points to the ﬁrst and last instructions, respectively.
proof for Theorem 1 is available at [1]. We here discuss the
intuition behind such proof. Theorem 1 states that LTLs 3 –
12, when considered in conjunction with the machine model in
Deﬁnition 4, imply APEX’s implementation correctness.
Recall that Deﬁnition 5 states that, in order to have EXEC =
1 during the computation of XProve, at least once before such
event (EXEC = 1) the following must have happened:
1. The system reached state S0 in which the software stored
in ER started executing from its ﬁrst instruction (PC =
ERmin).
2. The system eventually reached a state S1 when ER ﬁn-
ished executing (PC = ERmax). In the interval between S0
and S1 PC remained executing instructions within ER, and
there were no interrupts, no resets, and DMA remained
inactive.
3. The system eventually reached a state S2 when XProve
started executing (PC = CRmin). In the interval between
786    29th USENIX Security Symposium
USENIX Association
S0 and S2 the memory regions of METADATA and ER
were not modiﬁed.
4. In the interval between S0 and S2 the OR memory region
was only modiﬁed by ER’s software execution (PC ∈
ER∨¬ Modify_Mem(OR)).
The ﬁrst two properties to be noted are LTL 12 and LTL 11.
LTL 12 establishes the default state of EXEC is 0. LTL 11
enforces that the only possible way to change EXEC from 0 to
1 is by having PC = ERmin. In other words, EXEC is 1 during
the computation of XProve only if, at some point before that,
the code stored in ER started to execute (state S0).
To see why state S1 (when ER execution ﬁnishes, i.e., PC =
ERmax) is reached with ER executing atomically until then, we
look at LTLs 4, 5, 6, and 9. LTLs 4, 5 and 6 enforce that PC
will stay inside ER until S1 or otherwise EXEC will be set to
0. On the other hand, it is impossible to execute instructions
of XProve (PC ∈ CR) without leaving ER, because LTL 9
guarantees that ER and CR do not overlap, or EXEC = 0.
So far we have argued that to have a token H that reﬂects
EXEC = 1 the code contained in ER must have executed suc-
cessfully. What remains to be shown is: producing this token
implies the code in ER and METADATA are not modiﬁed in
the interval between S0 and S2 and only ER’s execution can
modify OR in the same time interval.
Clearly, the contents of ER can not be modiﬁed after S0
because Modify_Mem(ER) directly implies that LTL 3 will
set EXEC = 0. The same reasoning is applicable for modiﬁca-
tions to METADATA region with respect to LTL 10. The same
argument applies to modifying OR, with the only exception
that OR modiﬁcations are allowed only by the CPU and when
PC ∈ ER (LTL 7). This means that OR can only be modiﬁed by
the execution of ER. In addition, LTL 7 also ensures that DMA
is disabled during the execution of ER to prevent unauthorized
modiﬁcation of intermediate results in data memory. There-
fore, the timeline presented in Figure 3 is strictly implied by
APEX’s implementation. This concludes the reasoning behind
Theorem 1.
Proof. (Theorem 2) Assume that AdvPoX is an adversary capable
of winning the security game in Deﬁnition 2 against APEX with more
than negligible probability. We show that, if such AdvPoX exists, then
it can be used to construct (in a polynomial number of steps) AdvRA
that wins VRASED’s security game (Deﬁnition 7) with more than
negligible probability. Therefore, by contradiction, nonexistence of
AdvRA (i.e., VRASED’s security) implies nonexistence of AdvPoX
(APEX’s security).
First we recall that, to win APEX’s security game, AdvPoX must
provide (HAdv, OAdv), such that XVerify(HAdv,OAdv,S ,C hal,·) = 1.
To comply with conditions 3.a and 3.b in Deﬁnition 2, this must be
done in either of the following two ways:
Case1 AdvPoX does not execute S in the time window between treq
and tveri f (i.e., ¬XAtomicExecP rv(S ,treq → tveri f )).
Case2 AdvPoX calls XAtomicExecP rv(S ,treq → tveri f ) but modi-
ﬁes its output O in between the time when the execution of S
completes and the time when XProve is called.
Theorem 2. APEX is secure according to Deﬁnition 2 as long as
VRASED is a secure RA architecture according to Deﬁnition 7.
Deﬁnition 7. VRASED’s Security Game [15]
7.1 RA Security Game (RA-game):
Notation:
- l is the security parameter and |K | = |C hal| = |MR| = l
- AR(t) denotes the content of AR at time t
RA-game:
1. Setup: Adv is given oracle access to SW-Att calls.
2. Challenge: A random challenge C hal ← ${0,1}l is gener-
ated and given to Adv.
3. Response: Adv responds with a pair (M,σ), where σ is ei-
ther forged by Adv, or is the result of calling SW-Att at some
arbitrary time t.
4. Adv wins
if and only
HMAC(KDF(K ,C hal),M).
if M (cid:54)= AR(t)
and σ =
7.2 RA Security Deﬁnition:
An RA scheme is considered secure if for all PPT adversaries Adv,
there exists a negligible function negl such that:
Pr[Adv, RA-game] ≤ negl (l)
However, according to the speciﬁcation of APEX’s XVerify algorithm
(see Deﬁnition 3), a token HAdv will only be accepted if it reﬂects an
input value with EXEC = 1, as expected by V rf. In APEX’s imple-
mentation, O is stored in region OR and S in region ER. Moreover,
given Theorem 1, we know that having EXEC = 1 during XProve
implies three conditions have been fulﬁlled:
Cond1 The code in ER executed successfully.
Cond2 The code in ER and METADATA were not modiﬁed after
starting ER’s execution and before calling XProve.
Cond3 Outputs in OR were not modiﬁed after completing ER’s
execution and before calling XProve.
The third condition rules out the possibility of Case2 since that case
assumes Adv can modify O, resided in OR, after ER execution and
EXEC stays logical 1 during XProve. We further break down Case1
into three sub-cases:
Case1.1 AdvPoX does not follow Cond1-Cond3. The only way
for AdvPoX to produce (HAdv, OAdv) in this case is not to call
XProve and directly guess H .
Case1.2 AdvPoX follows Cond1-Cond3 but does not execute S
between treq and tveri f . Instead, it produces (HAdv, OAdv) by
calling:
OAdv ≡ XAtomicExecP rv(ERAdv,treq → tveri f )
(13)
where ERAdv is a memory region different from the one spec-
iﬁed by V rf on XRequest (AdvPoX can do this by modifying