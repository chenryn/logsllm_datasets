### Key Software Components and Experimental Setup

#### 4.1 Software Environment
The experiments utilize four primary components:
- A generic proxy network implementation.
- The Apache web server [16] as the application.
- The web testing tool "siege" [17] to simulate user access.
- The DDoS attack tool "Trinoo" [11].

##### 4.1.1 Proxy Network Implementation
The generic proxy network consists of multiple proxy nodes, which are software programs that forward application messages. As illustrated in Figure 4, each pair of neighboring proxies maintains a persistent TCP connection, established according to the given topology and bootstrap location information. These connections are shared among users, and messages can be routed using any specified routing algorithm. The proxy network can be configured to support various topologies and routing algorithms.

##### 4.1.2 Application Service
We use the Apache web server as a representative application front-end. Since our focus is on the network impact of DoS attacks, specific details of the application logic at the back-end are not critical. The Apache server serves files of different sizes to simulate typical scenarios.

##### 4.1.3 User Simulator
To generate user requests, we use "siege," a web testing toolkit. Siege generates web requests based on a list of URLs and measures the response time for each request, allowing us to simulate user access and traffic.

##### 4.1.4 DDoS Attack Toolkit
Trinoo [11] is a widely available DDoS attack toolkit. It includes a daemon and a master program. A typical Trinoo network comprises compromised Internet hosts running the Trinoo daemon. The master program controls the Trinoo network to execute DDoS attacks. Given a list of IP addresses, the Trinoo daemons send UDP packets to the targets at a specified start time. In its original form, the Trinoo daemon repeatedly sends UDP packets at full speed. For controlled experiments, we modified the Trinoo daemon to allow adjustment of the sending rate.

#### 4.2 MicroGrid Simulation Toolkit
MicroGrid [9, 10] is an integrated online packet-level simulator that models virtual network environments. It allows users to configure and deploy arbitrary virtual networks to a cluster and run unmodified applications within them. Three key capabilities of MicroGrid are essential to our study:
- **High-Fidelity Simulation**: MicroGrid can simulate large networks with high fidelity, even at high traffic levels, and has demonstrated good scalability in realistic large-scale simulations with up to 20,000 routers.
- **Realistic Topology and Routing**: Integrated with the topology generator maBrite [20], MicroGrid can create realistic Internet-like topologies and set up BGP routing policies automatically. It supports Internet routing protocols such as BGP [21] and OSPF [22], as well as networking protocols like IP, UDP, TCP [23], and ICMP [24].
- **Direct Execution of Unmodified Applications**: MicroGrid supports the direct execution of unmodified applications, allowing us to study the properties of the proxy network and detailed system behavior in a large-scale, realistic network environment with real applications and attacks.

These capabilities make MicroGrid superior to testbeds like PlanetLab [25] or small-scale simulators like NS2 [26], where the scale, intensity, and range of attack scenarios are limited.

#### 4.3 Simulation Setup

##### 4.3.1 Simulated Network
As shown in Figure 5, the proxy network, Apache server, siege programs, and Trinoo attackers are deployed in the MicroGrid simulated network environment. The maBrite topology generator creates Internet-like Power-Law network topologies [20, 27]. We use two virtual networks in our experiments: R1K (1000 routers, 20 ASes) and R10K (10,000 routers, 40 ASes). Both networks span a geographic area of 5000 miles by 5000 miles, similar to the size of the North American continent. OSPF routing is used inside ASes, and BGP4 is used for inter-AS routing.

##### 4.3.2 Physical Resources
Our experiments use two clusters:
- **MicroGrid Simulator Cluster**: A 16-node dual 2.4GHz Xeon Linux cluster with 1GB main memory per machine, connected by a 1Gbps Ethernet switch.
- **Other Software Components Cluster**: A 24-node dual 450MHz PII Linux cluster with 1GB main memory per machine, connected by a 100Mbps Ethernet switch.
- **Inter-Cluster Connection**: A 1Gbps link connects the two clusters.

### Experiments and Results

#### 5.1 Proxy Network Performance
To understand the performance implications of the proxy network approach, we compare the user-observed service performance for direct application access and proxy network mediation. Users choose edge proxies based on proximity, and no user authentication is used. The proxy network is deployed in a resource pool of 1000 randomly sampled hosts from the network, with edge proxies distributed uniformly and application proxies placed close to the application.

Figure 6 shows the results in the R1K simulated network for a tree-topology 192-node proxy network with 64 edge proxies. The X-axis represents the response time for downloading files of different sizes (1.5KB, 100KB, or 1MB), and the Y-axis shows the Cumulative Density Function (CDF) of user-observed response time over the user population. A curve closer to the Y-axis indicates better performance for more users.

Despite the expectation that proxies might degrade performance, the proxy network improves performance. For small requests (e.g., 1.5KB), the 50th percentile response time is reduced by half. For moderate-sized requests (e.g., 100KB) and large files (e.g., 1MB), the improvement is even more significant. The main reasons for these improvements are:
1. **Improved Connection Setup Time**: The proxy network reduces the connection setup cost by establishing shorter TCP connections with smaller round-trip times (RTTs).
2. **Persistent TCP Connections**: The persistent TCP connections among proxies have already opened their congestion windows, avoiding the slow start phase.
3. **Enhanced Throughput and Robustness**: Shorter TCP connections improve throughput and robustness, as studied in Logistic Networking [28].

These factors collectively contribute to the improved performance observed in the experiments.