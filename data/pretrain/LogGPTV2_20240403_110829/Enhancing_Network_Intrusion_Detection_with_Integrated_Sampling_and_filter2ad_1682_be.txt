### 1. Introduction

#### 1.1 Automated Malware Collection
Malware, or malicious software, is a significant threat to the security and integrity of computer systems. Self-replicating malware, in particular, can spread rapidly across networks, compromising multiple machines and forming botnets that can launch distributed denial-of-service (DDoS) attacks. To effectively counter these threats, it is crucial to have a deep understanding of the nature and behavior of such malware.

**Why Collect Malware?**
- **Defensive Measures:** Analyzing individual pieces of malware helps in refining intrusion detection and antivirus systems, improving their ability to detect and mitigate threats.
- **Statistical Insights:** Large-scale collection of malware provides valuable data on attack patterns, trends, and rates, enabling better-informed defensive strategies.

**Challenges in Malware Collection:**
- **Manual Analysis:** Traditional methods involve detailed forensic examinations of infected machines, which is time-consuming and impractical for large-scale analysis.
- **Speed of Spread:** Sophisticated malware can spread faster than human analysts can respond, necessitating automated solutions.

#### 1.2 Honeypot Technology
Honeypots are systems designed to attract and study unauthorized or illicit activities. They are categorized into two main types:

- **Low-Interaction Honeypots:**
  - **Description:** Emulate services or operating systems with limited interaction.
  - **Advantages:** Low risk, easy to deploy and maintain.
  - **Example:** Honeyd [14].
  - **Use Case:** Learning about attack patterns and attacker behavior.

- **High-Interaction Honeypots:**
  - **Description:** Provide a real system for attackers to interact with.
  - **Advantages:** Detailed insights into attacker behavior and techniques.
  - **Disadvantages:** Higher risk, more complex to set up and maintain.
  - **Example:** GenIII Honeynet [3].
  - **Use Case:** In-depth study of attack proceedings and behaviors.

**Trade-offs:**
- **Expressiveness vs. Scalability:** High-interaction honeypots offer more detailed insights but are less scalable compared to low-interaction honeypots.

### 2. The Nepenthes Platform

#### 2.1 Overview
Nepenthes is a platform designed to collect self-replicating malware in an efficient and effective manner. It combines the scalability of low-interaction honeypots with the expressiveness of high-interaction ones.

- **Key Features:**
  - **Emulation:** Emulates only the vulnerable parts of a service, reducing resource requirements.
  - **Flexibility:** Uses vulnerability modules to configure the honeypot for different vulnerabilities.
  - **Scalability:** Easy to deploy and maintain, making it suitable for large-scale use.

#### 2.2 Architecture
Nepenthes is not a traditional honeypot but a platform for deploying honeypot modules. These modules, known as vulnerability modules, allow for highly flexible configuration.

- **Vulnerability Modules:**
  - **Function:** Each module emulates a specific vulnerability.
  - **Advantages:** Allows for targeted and efficient emulation, reducing the risk of full system compromise.

#### 2.3 Advantages
- **Automation:** Knowledge of expected attacker behavior enables automation, making it easier to handle large volumes of malware.
- **Unique Features:** Can implement features not available in high-interaction honeypots, such as selective emulation and dynamic reconfiguration.

### 3. Experimental Results

#### 3.1 Real-Time Risk Assessment
- **Experiment Setup:**
  - **Data Set:** Lincoln Laboratory data set (3.5 hours).
  - **Processing Time:** 2 minutes 44 seconds.
  - **TU Vienna Data Set:** 3 days.
  - **Processing Time:** 20 minutes 54 seconds.

- **Results:**
  - **Real-Time Capability:** The model can process alerts in real-time for multiple Class C networks.
  - **Risk Assessment:** Provides a fine-grained and probabilistic model for network risk assessment using Hidden Markov Models (HMMs).

#### 3.2 Limitations and Future Work
- **Model Parameter Estimation:**
  - **Current Approach:** Parameters were estimated manually.
  - **Future Work:** Investigate the use of training algorithms for automatic parameter estimation.

- **Inter-Host Dependencies:**
  - **Current Approach:** Dependencies between hosts were not considered.
  - **Future Work:** Include inter-host dependencies to provide a more accurate overview of network risk.

- **Multiple Sensors:**
  - **Future Implementation:** Develop a general framework for handling multiple sensors, each represented by an HMM.

- **Online Testing:**
  - **Current Status:** Experiments were run in offline mode.
  - **Future Work:** Test the system with live traffic in an online environment.

### 4. Related Work

- **Traditional Risk Assessment:**
  - **References:** [14] and [15].
  - **Methodologies:** CORAS [2] and MORDA [5].

- **Alert Prioritization:**
  - **Approaches:** [12], [4], [7], [9].
  - **Methods:** Impact-based prioritization, alert verification, and online/offline systems.

- **Real-Time Risk Assessment:**
  - **Previous Work:** [6].
  - **Limitation:** Limited to individual hosts.

### 5. Conclusions and Future Work

- **Contributions:**
  - **Real-Time Network Risk Assessment:** Probabilistic model using HMMs.
  - **Integration with STAT Framework:** Prioritization of IDS alerts based on risk assessment.
  - **Evaluation:** Simulated and real-world data.

- **Future Work:**
  - **Parameter Estimation:** Use of training algorithms.
  - **Inter-Host Dependencies:** Incorporate for more accurate risk assessment.
  - **Multiple Sensors:** Implement a general framework.
  - **Online Testing:** Validate the system with live traffic.

### References

1. Andr´e ˚Arnes, Karin Sallhammar, Kjetil Haslum, Tønnes Brekne, Marie Elisa-
beth Gaup Moe, and Svein Johan Knapskog. Real-time risk assessment with
network sensors and intrusion detection systems. In International Conference on
Computational Intelligence and Security (CIS 2005), 2005.
2. CORAS IST-2000-25031 Web Site, 2003. http://www.nr.no/coras.
3. Herv´e Debar, David A. Curry, and Benjamin S. Feinstein.
Intrusion detection
message exchange format (IDMEF) – internet-draft, 2005.
4. Neil Desai.
IDS correlation of VA data and IDS alerts.
securityfocus.com/infocus/1708, June 2003.
http://www.
5. Shelby Evans, David Heinbuch, Elizabeth Kyule, John Piorkowski, and James Wall-
ner. Risk-based systems security engineering: Stopping attacks with intention.
IEEE Security and Privacy, 02(6):59 – 62, 2004.
6. Ashish Gehani and Gershon Kedem. Rheostat: Real-time risk management. In
Recent Advances in Intrusion Detection: 7th International Symposium, (RAID
2004), Sophia Antipolis, France, September 15-17, 2004. Proceedings, pages 296–
314. Springer, 2004.
7. Ron Gula. Correlating ids alerts with vulnerability information. Technical report,
Tenable Network Security, December 2002.
8. Cristopher Kruegel, Engin Kirda, Darren Mutz, William Robertson, and Giovanni
Vigna. Polymorphic worm detection using structural information of executables. In
Proceedings of the International Symposium on Recent Advances in Intrusion De-
tection (RAID 2005), volume 3858 of LNCS, pages 207–226, Seattle, WA, Septem-
ber 2005. Springer-Verlag.
9. Cristopher Kruegel and William Robertson. Alert veriﬁcation: Determining the
In Proceedings of the 1st Workshop on the De-
success of intrusion attempts.
tection of Intrusions and Malware and Vulnerability Assessment (DIMVA 2004),
Dortmund, Germany, July 2004.
10. Cristopher Kruegel, William Robertson, and Giovanni Vigna. Using alert veriﬁca-
tion to identify successful intrusion attempts. Practice in Information Processing
and Communication (PIK 2004), 27(4):219 – 227, October – December 2004.
11. Lincoln Laboratory.
Lincoln laboratory
scenario
(DDoS)
1.0,
2000.
http://www.ll.mit.edu/SST/ideval/data/2000/LLS DDOS 1.0.html.
12. Phillip A. Porras, Martin W. Fong, and Alfonso Valdes. A mission-impact-based
approach to infosec alarm correlation. In Proceedings of the International Sympo-
sium on the Recent Advances in Intrusion Detection (RAID 2002), pages 95–114,
Zurich, Switzerland, October 2002.
13. Lawrence R. Rabiner. A tutorial on hidden markov models and selected applica-
tions in speech recognition. Readings in speech recognition, pages 267–296, 1990.
14. Standards Australia and Standards New Zealand. AS/NZS 4360: 2004 risk man-
agement, 2004.
15. Gary Stonebumer, Alice Goguen, and Alexis Feringa.
Risk management
guide for information technology systems, special publication 800-30, 2002.
http://csrc.nist.gov/publications/nistpubs/800-30/sp800-30.pdf.
16. Sun Microsystems, Inc. Installing, Administering, and Using the Basic Security
Module. 2550 Garcia Ave., Mountain View, CA 94043, December 1991.
17. Giovanni Vigna, Richard A. Kemmerer, and Per Blix. Designing a web of highly-
conﬁgurable intrusion detection sensors. In W. Lee, L. M`e, and A. Wespi, editors,
Proceedings of the 4th International Symposium on Recent Advances in Intrusion
Detection (RAID 2001), volume 2212 of LNCS, pages 69–84, Davis, CA, October
2001. Springer-Verlag.
18. Giovanni Vigna, Fredrik Valeur, and Richard Kemmerer. Designing and implement-
ing a family of intrusion detection systems. In Proceedings of European Software
Engineering Conference and ACM SIGSOFT Symposium on the Foundations of
Software Engineering (ESEC/FSE 2003), Helsinki, Finland, September 2003.