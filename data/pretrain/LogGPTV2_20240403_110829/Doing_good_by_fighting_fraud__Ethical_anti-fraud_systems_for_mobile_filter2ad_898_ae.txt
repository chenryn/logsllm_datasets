Concretely, from Figure 10 which presents more detailed
results, we see that the average frame rate improves from 1.30
FPS on devices running Boxer Android to 4.07 FPS on devices
running Daredevil Android. Daredevil Android also increases
the average success rate from 46.72% to 88.46%. We also see
an improvement in success rates on iOS, going from 88.60%
on Boxer iOS to 89.13% on Daredevil iOS. Additionally, the
average scan duration decreases from 15.45s to 10.55s on
Android and from 10.02s to 9.37s on iOS. In our system we
start the scan duration timer when the user clicks on the “scan
card” button and ﬁnish it after the scan is complete, which
includes accepting camera permissions, pulling their card out
of their wallet, scanning the card, and the 1.5s voting phase
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:08 UTC from IEEE Xplore.  Restrictions apply. 
1632
Frame Rate (Frames Per Second)Success Rate(%)02550751000102030Daredevil AndroidBoxer iOSDaredevil iOSfor error correction in the main loop.
Daredevil also improves the usability of card scanning with
4.88% (Figure 11) of Android phones being able to process
fewer than 1 FPS, compared to 44.61% with Boxer Android.
Similar to Boxer, the success rate for Android devices with less
than 1 FPS (37.92%) is lower than the average success rate
for Android overall (88.46%), however the overall increase in
devices that can run the Daredevil ML at 1 FPS or higher
leads to a higher overall success rate (Figure 10 and Figure
11).
We see from Figure 11 that for both Boxer and Daredevil,
as the frame rate increases the overall success rate increases as
well. Beyond 1 FPS, the success rate for Daredevil witnesses
a precipitous rise compared to Boxer, this can be attributed
to Daredevil being trained with orders of magnitude more
data (Section VI-E), the use of an efﬁcient machine learning
pipeline (Section V-D) and marginal improvements seen from
the updated back-end network (Section VI-E). It is clear that
Boxer can also beneﬁt from these improvements, however,
given that 44.61% of the Android devices operate at below
1 FPS for Boxer (and Daredevil also struggles with devices
that operate at frame rates below 1 FPS), a signiﬁcant portion
of the devices will be excluded from these improvements.
Daredevil’s architecture reduces the number of devices that
operate at below 1 FPS to 4.88% which results in signiﬁcantly
higher overall success rates.
As with our measurement study Section III, we also evalu-
ated failed attempts with Daredevil. We present the results in
Appendix C.
B. Does Daredevil’s fraud check work in the wild, while
remaining ethical?
To evaluate Daredevil’s ability to stop fraud in real-time,
we report results from a large international app deploying our
SDK. For a test period of 3 months, the app ﬂagged 12,474
transactions as suspicious and challenged them with Daredevil
to verify their payment method.
Daredevil passed 7,612 transactions and blocked the re-
maining 4,862 transactions. Of the 7,612 transactions passed
by Daredevil, only 12 resulted in chargebacks, leading to a
false negative rate of 0.16%. We are unable to report the false
positive rate since the app did not share the false positive
data with us, please see Section VI-C for a evaluation of the
Daredevil’s false positive rate. Based on this initial test, the
app has decided to deploy Daredevil.
To determine if Daredevil’s fraud decisions are correlated
with the device frame rates we further analyze the performance
characteristics of the passed and blocked devices. We ﬁnd
that the average frame rate of devices that Daredevil passed
was 1.84 FPS and the average frame rate of the devices that
Daredevil blocked was 1.94 FPS, indicating that the frame
rates for the two groups is roughly the same. To visualize these
results, we plot the CDF of percentage of devices vs frame rate
(FPS) for the two groups and present the results in Figure 12.
We see that the plots look very similar indicating that frame
rate is not a discriminating factor between the blocked and
passed groups.
For companies, chargebacks are the ground truth because
they represent exactly what they are liable for ﬁnancially.
However, it is possible that there was fraud that happened
but the victim failed to report the fraudulent charge to their
issuing bank, thus the actual amount of fraud may be higher
than the chargeback count that we report in this experiment.
C. What is Daredevil’s false positive rate when scanning real
cards and running anti-fraud models?
To evaluate Daredevil’s false positive rate, we report results
from four authors scanning 105 cards in a lab setting using
the latest production anti-fraud models as of December 2020.
In this experiment, we invoke the fraud ﬂow and record
the number of scans that
the system incorrectly ﬂags as
being fraudulent. This section complements our real-world
evaluation of Daredevil’s fraud systems in VI-B that shows
our false negative rate.
We scan 105 different real cards multiple times on different
resource-constrained and well-provisioned Android and iOS
devices for a total of 310 scans. The devices we use are iPhone
SE (1st gen), Google Pixel 2, Nexus 6, iPhone 6s, and iPhone
11. Of these 310 scans, Daredevil incorrectly ﬂags seven scans
as fraudulent, giving a false positive rate of 2.2%. The false
positives are uniformly spread across all devices, indicating
that Daredevil does not unfairly permit well-provisioned or
resource-constrained devices, similar to our fraud decisions as
discussed in Figure 12.
Six out of the seven reported false positives were transient
in nature, i.e. further scans of the same card (which we would
expect from a good user) did not result in false positives. The
other card was consistently ﬂagged incorrectly by our fake
media detection model.
D. Does our use of redundancy improve overall accuracy?
In this section, we evaluate the effectiveness of our re-
dundancy based decomposition strategy (described in Sec-
tion V-D) in aiding fraud detection. Speciﬁcally, we evaluate
the gains in accuracy on executing our card tampering detec-
tion and fake media detection models in the completion loop.
We run a user study with and without the card detection
model in the main loop to show how it beneﬁts the card
tampering detection and fake media detection models running
in the completion loop. The user-facing feedback from the card
detection model ensures that users center their credit cards so
that both models necessarily make their predictions on valid
credit card images.
Users participating in our study randomly run one of two
versions of our app and scan 30 different predetermined credit
card images on a browser that we provide via a link. We use
their scans to evaluate the impact of the feedback from card
detection in terms of the number of mistakes made by card
tampering detection (i.e. objects present on the card that the
model fails to detect as well as objects not present on the card
that the model incorrectly detects) and the accuracy of fake
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:08 UTC from IEEE Xplore.  Restrictions apply. 
1633
Daredevil
Boxer
Android FPS
= 2 FPS
Count
Success rate
Count
Success rate
23,314 (4.88%)
48,271 (10.10%)
406,009 (85.01%)
37.92%
84.08%
91.88%
146,890 (44.61%)
97,798 (29.70%)
84,584 (25.68%)
31.87%
49.97%
68.72%
Fig. 11: Success rates for Android devices running Daredevil and Boxer by frame rate. We can see that Daredevil signiﬁcantly
reduces the percentage of devices that operate below 1 FPS.
Back
End
MBv1
MBv2
Size
1.8MB
1.65MB
No. of
Params.
869,754
861,242
Recall
Precision
54.06%
56.25%
100%
100%
FPS on
Pixel 3a
7.19
7.09
Fig. 14: Comparison of model parameters and accuracy met-
rics on our benchmark datasets using Daredevil with back-
ends MobileNet V1 (MBv1) and MobileNet V2 (MBv2).
We can see that using MobileNet V1 as back-end leads to
less than 1% increase in model parameters with no decrease
in precision and marginal decrease in recall. It should be
noted that the second model (with back-end MobileNet V2)
is currently in production, all the statistics from Daredevil
evaluation correspond to this model.
No. of images
495,134
939,165
1,374,707
2,006,452
2,500,612
Recall
20%
27.96%
42.08%
49.06%
56.25%
Precision
98.46%
98.89%
99.26%
100%
100%
Fig. 15: Impact of varying the amount of training data on
model accuracy. The model consists of Daredevil OCR with
MobileNet V2 back-end.
by users scanning their credit cards. Crucially, this is the same
benchmark we use to evaluate models that are shipped in
production. The test set consists of 640 image frames extracted
from 32 videos. We train Daredevil OCR with MobileNet
V1 and MobileNet V2 back-ends and report the results in
Figure 14. We deﬁne a correct prediction as one where the
model can correctly extract the card number from the image
frame, while an incorrect prediction is one where the model
extracts an incorrect card number (valid but incorrect), ﬁnally
all frames where the model is able to extract only a partial
number are considered missed predictions. Accordingly, recall
is the fraction of the frames where the model produced a
correct prediction and precision is the fraction of the all the
predictions that were correct.
Critically, from Figure 14 we see that using MobileNet
V2 instead of MobileNet V1 as the back-end network results
in less than 1% reduction in the number of parameters,
indicating that the reduction in the overall parameters is a
direct result of Daredevil’s architecture independent of the
back-end network. We also see Daredevil with MobileNet
Fig. 12: CDF of percentage of devices against the frame rates
for devices passed and blocked by Daredevil. We see that the
two plots look very similar, indicating that Daredevil’s fraud
decision is largely independent of the frame rate.
Card tampering
detect. # errors
Fake media
detect. acc.
No Card Detection
With Card Detection
1.94 errors per frame
1.26 errors per frame
86.24%
95.26%
Fig. 13: Results from our user study indicate fewer errors made
by the card tampering detection model and higher accuracy of
fake media detection model when aided by card detection.
media detection in detecting both, the presence and absence
of screens.
Our university’s IRB board reviewed our user study and
ruled it to be exempt from IRB.
Figure 13 summarizes our results. Our design of decomposi-
tion centered on the card detection model ensures that we pass
high-quality frames to the machine learning models, resulting
in fewer errors for the card tampering detection model, de-
creasing the errors per frame from 1.94 errors per frame down
to 1.26 errors per frame. This change also improves accuracy
for our fake media detection model increasing the accuracy
from 86.24% to 95.26%. Overall, these improvement lead to
more accurate fraud detection. For more details on our user
study, please see Appendix D.
E. What is the impact of back-end networks and data aug-
mentation on overall success rates?
To quantify the impact of back-end networks, we validate
our models on image frames extracted from videos recorded
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:14:08 UTC from IEEE Xplore.  Restrictions apply. 
1634
V1 closely matches Daredevil with MobileNet V2 in recall
and precision (Figure 14) further highlighting the back-end
agnostic nature of Daredevil.
To quantify the impact of data augmentation on the im-
provement of overall success rates. We train Daredevil OCR
(Mobile Net V2 back-end) by varying the amount of training
data. Our training data is generated using a custom Generative
Adversarial Network (GAN) [13] architecture and we also use
standard data augmentation techniques in addition to the GAN.
We evaluate the models using the same benchmark as before
and report the results in Figure 15.
In summary, we conclude that with Daredevil’s architecture
we are able to achieve the desired frame rate and with high-
ﬁdelity synthetic data we are able to achieve the desired
accuracy.
Apps such as Google Pay and Apple Pay are restrictive in
the users they allow to use their systems. Firstly, they are not
available in all regions around the world [15],
[4]. More
importantly, these services are restrictive in their support to
pre-paid cards [14]. Over 8 million households in the United
States rely on pre-paid cards, most of whom are blocked from
using these services [1].
Payment card fraud using card skimmers has been studied
recently by Scaife et al. [47]. In this work, researchers built
a card skimmer detector that can be used at physical payment
terminals such as ATMs and gas stations. In another work,
Scaife et al. [46] did a survey of gas pump card skimmer
detection techniques including Bluetooth skimmer detection
on iOS and Android apps,
to identify common skimmer
detection characteristics.
VII. RELATED WORK
VIII. CONCLUSIONS
Our work is related to papers in the areas of ﬁnancial
fraud, challenge based authentication, computer vision, ma-
chine learning systems and machine learning for mobile.
Recent work has focused on devising challenges that rely
on having users interact with their mobile phones to collect
signals that are then processed for veriﬁcation [32], [51]. Liu