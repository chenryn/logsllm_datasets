# Unexpected Means of Protocol Inference

**Authors:**
- Justin Ma
- Kirill Levchenko
- Christian Kreibich
- Stefan Savage
- Geoffrey M. Voelker

**Affiliations:**
- Department of Computer Science and Engineering, University of California, San Diego, USA
- Computer Laboratory, University of Cambridge, UK

**Contact:**
- {jtma, klevchen, savage, voelker}@cs.ucsd.edu

**Abstract:**
Network managers often need to associate network traffic with specific applications, a task critical for debugging, security, analytics, and policy enforcement. Traditionally, this has been done by relying on well-established global port mappings. However, factors such as firewall port blocking, tunneling, dynamic port allocation, and the proliferation of new distributed applications have diminished the effectiveness of this approach. We explore three alternative mechanisms that use statistical and structural content models to automatically identify traffic using the same application-layer protocol, independent of port numbers. These mechanisms allow for the identification of known applications regardless of port number and can distinguish between unknown applications. We evaluate the classification performance of each mechanism using real-world traffic traces from multiple sites.

**Categories and Subject Descriptors:**
C.2.2 [Computer-Communication Networks]: Network Protocols

**General Terms:**
Algorithms, Measurement, Experimentation

**Keywords:**
Application Signatures, Traffic Classification, Protocol Analysis, Sequence Analysis, Network Data Mining, Relative Entropy, Statistical Content Modeling

## 1. Introduction
The Internet architecture uses port numbers to associate services with end hosts. Historically, well-known ports have been used to identify application-layer protocols. However, recent developments, such as the widespread adoption of firewalls and the use of non-standard ports, have undermined the reliability of this method. For example, some applications use common open ports (e.g., TCP port 80 for HTTP) or dynamically allocate ports to avoid detection. This transformation has created significant challenges for network managers, who need accurate knowledge of the applications on their networks for accounting, analysis, and policy enforcement.

In this paper, we address the problem of automatically classifying network flows based on the application-layer protocols they employ, using only flow content. While external features like packet sizes and inter-arrival times can aid classification, we argue that flow content is the most reliable source of information about the involved protocols. Our contributions include:
- A generic architectural and mathematical framework for unsupervised protocol inference.
- Three classification techniques: product distributions, Markov processes, and common substring graphs.
- An evaluation of these classifiers using real-world traffic traces in semi-supervised post-hoc classification and new protocol discovery settings.

Our work aims to relieve network analysts from the need to manually classify unknown protocols or new variants. By automatically grouping protocols without prior knowledge, our approach simplifies the process of recognizing and classifying new traffic.

## 2. Relation to Existing Work
Traditional network-level application analysis has relied on well-known ports, but this approach has been undermined by new application patterns, especially peer-to-peer (P2P) use. Recent research has explored deeper structural analyses and payload-based classification. While these methods have shown promise, they often require manual involvement and are limited in their ability to distinguish between similar applications.

Our work builds on these approaches by removing the requirement for a priori knowledge of the protocols. Using raw network data, our unsupervised algorithms classify traffic into distinct protocols based on correlations in packet content. This approach is similar to the work of Bernaille et al., who use packet sizes as protocol signatures, but extends it to a more general and flexible framework.

## 3. Protocol Inference
### 3.1 Background
A session is a pair of flows, each consisting of application-layer data exchanged between an initiator and a responder. Sessions are identified by a 5-tuple (initiator address, initiator port, responder address, responder port, IP protocol number). Flows are identified by the same 5-tuple and the direction of the flow. A session includes only the data exchanged between two ports during its lifetime, excluding packet-level information.

All sessions occur within the context of an application protocol, which defines how the data is interpreted. To infer the protocol, sessions must be distinguishable based on their content. This requires a protocol model, which is a set of premises about how a protocol manifests in a session.

### 3.2 Protocol Models
Protocol inference relies on a protocol model, which describes the likelihood of each pair of flows in a session. To make the problem tractable, we assume that a protocol can be inferred from the first n bytes of a session. We treat distributions on sessions as pairs of distributions on flows, reducing the complexity significantly.

### 3.3 A Priori Information
We recognize three variants of the protocol inference problem based on the available a priori information:
- **Fully described:** Each protocol is given as a (possibly probabilistic) grammar.
- **Fully correlated:** Each protocol is defined by a class of grammars, but the exact grammar is unknown and must be learned from labeled sessions.
- **Partially correlated:** The exact grammar is unknown, and only limited information is available about which sessions share a common protocol.

Our focus is on partially correlated protocol inference, where the training data consist of unlabeled sessions with auxiliary information indicating that certain sessions use the same protocol.

### 3.4 Protocol Construction
Constructing a protocol description involves grouping sessions into equivalence groups based on the given correlations, constructing a cell (a tentative protocol description) for each group, clustering similar cells, and merging them to create a more stable protocol description. The resulting cells define distinct protocols and are used to classify new sessions.

### 3.5 Cells as Classifiers
Protocols are described by distributions, and unknown sessions can be classified by matching them with the maximum-likelihood distribution (cell). This is captured by the "Score" operation, which determines the likelihood that a session is using the protocol described by the cell.

## 4. Statistical Models
### 4.1 Statistical Setting
We view a protocol as a pair of distributions on byte strings (flows) of length n. We introduce the concept of relative entropy, a measure of dissimilarity between two distributions. Symmetric relative entropy, defined as \( D(P, Q) = D(P|Q) + D(Q|P) \), is used to compare and merge cells.

### 4.2 Cell Operations
- **Construct:** Given a set of sessions, create a cell representing the flow distributions from initiators to responders and from responders to initiators.
- **Compare:** Determine the similarity between two cells.
- **Merge:** Combine two cells believed to represent the same protocol.
- **Score:** Determine the likelihood that a session is using the protocol described by a cell.

These operations form the basis for our classification experiment, which evaluates the performance of our statistical models using real-world traffic traces.

---

This optimized version of the text is more structured, coherent, and professional, making it easier to read and understand.