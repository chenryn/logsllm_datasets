title:Unexpected means of protocol inference
author:Justin Ma and
Kirill Levchenko and
Christian Kreibich and
Stefan Savage and
Geoffrey M. Voelker
Unexpected Means of Protocol Inference
∗
Justin Ma
∗
Kirill Levchenko
∗
Stefan Savage
Geoffrey M. Voelker
Christian Kreibich
∗
†
∗
Dept. of Computer Science and Engineering
{jtma,klevchen,savage,voelker}@cs.ucsd.edu
University of California, San Diego, USA
†
University of Cambridge
Computer Laboratory, UK
PI:EMAIL
ABSTRACT
Network managers are inevitably called upon to associate network
trafﬁc with particular applications. Indeed, this operation is crit-
ical for a wide range of management functions ranging from de-
bugging and security to analytics and policy support. Tradition-
ally, managers have relied on application adherence to a well es-
tablished global port mapping: Web trafﬁc on port 80, mail trafﬁc
on port 25 and so on. However, a range of factors — including
ﬁrewall port blocking, tunneling, dynamic port allocation, and a
bloom of new distributed applications — has weakened the value
of this approach. We analyze three alternative mechanisms using
statistical and structural content models for automatically identi-
fying trafﬁc that uses the same application-layer protocol, relying
solely on ﬂow content. In this manner, known applications may
be identiﬁed regardless of port number, while trafﬁc from one un-
known application will be identiﬁed as distinct from another. We
evaluate each mechanism’s classiﬁcation performance using real-
world trafﬁc traces from multiple sites.
Categories and Subject Descriptors
C.2.2 [Computer-Communication Networks]: Network Proto-
cols
General Terms
Algorithms, Measurement, Experimentation
Keywords
Application Signatures, Trafﬁc Classiﬁcation, Protocol Analysis,
Sequence Analysis, Network Data Mining, Relative Entropy, Sta-
tistical Content Modeling
1.
INTRODUCTION
The Internet architecture uses the concept of port numbers to
associate services to end hosts. In the past, the Internet has relied
on the notion of well known ports as the means of identifying which
application-layer protocol a server is using [9].
In recent years,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’06, October 25–27, 2006, Rio de Janeiro, Brazil.
Copyright 2006 ACM 1-59593-561-4/06/0010 ...$5.00.
however, a number of factors have undermined the accuracy of this
association.
In particular, the widespread adoption of ﬁrewalling has made
some ports far easier to use than others (i.e., the commonly “open”
ports such as TCP port 80, used for HTTP trafﬁc, TCP port 25,
used for SMTP, and UDP port 53, used for DNS). Thus, to en-
sure connectivity, there is an increasing incentive to simply use
these ports for arbitrary applications, either directly or using the
native protocol as a tunneling transport layer. Other applications
allocate ports dynamically to eliminate the need for application
layer demultiplexing. For example, streaming media protocols,
such as H.323 and Windows Media, Voice-Over-IP services such
as SIP, and multi-player games like Quake routinely rendezvous on
ports dynamically selected from a large range. The popular Skype
service initializes its listening port randomly at installation, en-
tirely abandoning the notion of well known ports for normal clients
[2]. Finally, some applications use non-standard ports explicitly
to avoid classiﬁcation. Peer-to-peer (P2P) applications routinely
allow users to change the default port for this purpose, and some
use combinations of tunneling and dynamic port selection to avoid
detection [20]. We can expect this trend of unordered port use to
increase further in the future.
Unfortunately, this transformation has created signiﬁcant prob-
lems for network managers. Accurate knowledge of the spectrum
of applications found on a network is crucial for accounting and
analysis purposes, and classifying trafﬁc according to application
is also a key building block for validating service differentiation
and security policies. However, classiﬁcation based on well known
port numbers remains standard practice. While newer tools are be-
ing developed that exploit packet content in their analyses, all of
these require ongoing manual involvement – either to create signa-
tures or to label instances of new protocols.
In this paper we tackle the problem of automatically classifying
network ﬂows according to the application-layer protocols they em-
ploy. We do this relying solely on ﬂow content. While ﬂow-external
features such as packet sizes, header ﬁelds, inter-arrival times, or
connection contact patterns can be used to aid classiﬁcation, we
argue that only the ﬂow content itself can deliver unambiguous in-
formation about the application-layer protocols involved. We make
the following contributions:
• We propose a generic architectural and mathematical frame-
work for unsupervised protocol inference.
• We introduce three classiﬁcation techniques for capturing sta-
tistical and structural aspects of messages exchanged in a
protocol: product distributions, Markov processes, and com-
mon substring graphs.
• We compare the performance of these classiﬁers using real-
world trafﬁc traces in two use settings: semi-supervised post-
hoc classiﬁcation and new protocol discovery, highlighting
the individual strengths and weaknesses of the three tech-
niques.
We believe that the most signiﬁcant impact of our work will be
relieving network analysts from the need to classify unknown pro-
tocols or new protocol variants. We show that it is possible to au-
tomatically group protocols without a priori knowledge. Thus, la-
beling a single protocol instance is sufﬁcient to classify all such
trafﬁc. In effect, we have substituted the painful process of manual
ﬂow analysis and classiﬁer construction with the far easier task of
recognizing a protocol instance.
The remainder of this paper is structured as follows. We ﬁrst
explore the problem space and position our work in Section 2. We
introduce protocol inference in Section 3 and show how our three
classiﬁers ﬁt in this problem space in Sections 4 and 5. We have
implemented the classiﬁers in a single framework, and describe this
framework in Section 6. We present our evaluation in Section 7.
We discuss our approach and results in Section 8 and conclude in
Section 9.
2. RELATION TO EXISTING WORK
Traditionally, network-level application analysis has depended
heavily on identiﬁcation via well known ports [4, 7, 18]. New ap-
plication patterns, particularly P2P use, undermined this assump-
tion, leading measurement researchers to seek workarounds. One
class of solutions focuses on deeper structural analyses of com-
munication patterns, including the graph structure between IP ad-
dresses, protocols and port numbers over time, and the distribution
of packet sizes and inter-arrival times across connections [3, 11, 12,
15, 23]. These approaches depend on the uniqueness of speciﬁc
communication structures within a particular application. While
this approach has been shown to work well for separate application
classes (e.g., Mail vs. P2P), it is most likely unable to distinguish
between application instances (e.g., one P2P system vs. another).
Another line of research has focused on payload-based classiﬁ-
cation. Early efforts focused on using hand-crafted string classiﬁers
to overcome the limitations of port-based classiﬁcation for various
classes of applications [6, 10, 20]. Thus, the Jazz P2P protocol
could be recognized by scanning for “X-Kazaa-*” in transport-
layer ﬂows. Moore and Papagiannaki have shown how to further
augment such signatures with causal inference to improve classiﬁ-
cation [14].
However, the manual nature of this approach presents several
drawbacks. First, it presupposes that network managers know what
protocols they are looking for. In fact, new application protocols
come into existence at an alarming rate and many network man-
agers would like to be alerted that there is “a new popular applica-
tion on the block” even if they have no prior experience with it. Sec-
ond, even for well known protocols, constructing good signatures
is a delicate job, requiring expressions that have a high probabil-
ity of matching the application and few false matches to instances
of other protocols. The latter of these problems has recently been
addressed by Haffner et al. [8], who automate the construction of
protocol signatures by employing a supervised machine learning
approach on trafﬁc containing known instances of each protocol.
Their results are quite good, frequently approaching the perfor-
mance of good manual signatures.
Our work builds further upon this approach by removing the re-
quirement that the protocols be known in advance. By simply us-
ing raw network data, our unsupervised algorithms classify trafﬁc
into distinct protocols based on correlations between their packet
content. Thus, using no a priori information we are able to create
classiﬁers that can then distinguish between protocols. In this sense
(i.e., of being unsupervised), our approach is similar in spirit to that
of Bernaille et al. [3], who suggest using the sizes of the ﬁrst six
packets in a session as the protocol signature.
3. PROTOCOL INFERENCE
Below we deﬁne the problem background and basic terminol-
ogy and then describe the foundation of our approach to protocol
inference, namely statistical protocol modeling.
3.1 Background
The basic unit of communication between processes on Internet
hosts, be it a large TCP connection or a single UDP packet, is a ses-
sion. A session is a pair of ﬂows, each a byte sequence consisting
of the application-layer data sent by the initiator to the responder
and the data sent by the responder to the initiator. Each session is
identiﬁed by the 5-tuple consisting of initiator address, initiator port
number, responder address, responder port number, and IP proto-
col number. Flows are identiﬁed by the same 5-tuple and the ﬂow
direction, either from the initiator to the responder or from the re-
sponder to the initiator. We emphasize that a session consists only
of the data exchanged between two ports on a pair of hosts during
the session’s lifetime; it does not include packet-level information
such as inter-arrival time, frame size, or header ﬁelds.
All sessions occur with respect to some application protocol, or
simply protocol, which deﬁnes how communicating processes in-
terpret the session data. By observing the network we can identify
communication sessions, but we cannot directly learn the session
protocol. For this to be possible, sessions of different protocols
must “look different” from each other. To formalize what this prop-
erty means, we need the concept of a protocol model.
3.2 Protocol Models
Protocol inference relies, explicitly or implicitly, on a protocol
model: a set of premises about how a protocol manifests itself in
a session (i.e., a pair of ﬂows, one from the initiator to the respon-
der and one from the responder to the initiator). From the network
view, a protocol is simply a distribution on sessions; that is, a pro-
tocol is described by the likelihood of each pair of ﬂows.
To make the problem tractable, we restrict ourselves to ﬁnite dis-
tributions by bounding the length of a session. In other words:
Premise 1. A protocol is a distribution on sessions of length at
most n.
Another way to think of Premise 1 is that we are assuming that the
protocol to which a session belongs can be inferred from the ﬁrst n
bytes of a session; in our experiments, we ﬁx n to be 64 bytes as
in [8].
Unfortunately it is infeasible to work with n-byte session distri-
2·n possible pairs of ﬂows. To be
butions as these consist of 256
useful, a protocol model must be simultaneously (1) sufﬁciently
expressive to capture real-world protocols, and (2) compact so that
it can be learned from a small number of samples and described
efﬁciently.
Toward this end, we treat distributions on sessions as a pair of
distributions on ﬂows, rather than a distribution on pairs of ﬂows:
Premise 2. A protocol is a pair of distributions on ﬂows (one from
the initiator to the responder and one from the responder to the ini-
tiator).
2·n to 2 · 256
What we gain from Premise 2 is a drastic reduction on complex-
n values, to exactly describe a protocol.
ity, from 256
Unfortunately this is still not enough to satisfy requirement (2), and
therefore it is necessary to further restrict the class of distributions
used by our classiﬁcation models. For the statistical models (Sec-
tion 4), this class is given explicitly; for Common Substring Graphs
(Section 5), this class is implicit in the data structure.
3.3 A priori Information
The problem of protocol inference may be qualiﬁed by the type
of information about protocols available a priori. We recognize
three such variants of the problem:
Fully described. In fully described protocol inference, each proto-
col is given as a (possibly probabilistic) grammar. Identify-
ing the protocol used by a session is a matter of determining
which known description best matches the session.
Fully correlated. In fully correlated protocol inference, each pro-
tocol is assumed to be deﬁned by some (possibly probabilis-
tic) class of grammars, but the exact grammar is unknown.
The grammar of each protocol must be learned from a set of
session instances labeled with the protocol.
Partially correlated. In partially correlated protocol inference, a
protocol is also assumed to be deﬁned by some (possibly
probabilistic) class of grammars, but the exact grammar is
unknown. Unlike the fully trained case, however, only lim-
ited information is available about which sessions have a
common protocol.
The focus of this work is on partially correlated protocol infer-
ence, meaning that the training data consist of a set of unlabeled
sessions with additional information of the form “Session A and
Session B are using the same protocol.” This auxiliary information
is partial because not all sessions using the same protocol are iden-
tiﬁed as such, and only positive equivalences are given. In Section 6
we describe how such training data may be obtained using mild
real-world assumptions about protocol persistence on host ports.
Since all given correlations are positive (i.e., information that two
sessions share the same protocol) but partial, it is impossible to in-
fer any negative correlation between sessions through the absence
of positive correlation (unlike the fully correlated case). In the ab-
sence of negative correlation, there may be several hypotheses that
are consistent with the training data, ranging from all absent corre-
lations being negative (maximum number of distinct protocols) to
all absent correlations being positive (a single protocol for all ses-
sions). We describe how to distinguish between these two cases, as
well as the cases in between, next.
3.4 Protocol Construction
Constructing a protocol description requires some session in-
stances of each protocol extracted from the training data. The cor-
relation information in the training data allows us to group sessions
into protocol equivalence groups consisting of sessions known to
use the same protocol. We then construct a tentative protocol de-
scription, called a cell, in accordance with the protocol model. As
multiple cells may describe the same protocol, we cluster similar
cells and merge them to create a more stable protocol description.
The resulting cells deﬁne distinct protocols, and are used in the
second phase to classify new sessions. To implement the above
algorithm, a cell must support the following three operations.
Construct. Given a set of sessions of a protocol equivalence group,
construct a protocol description in accordance with the pro-
tocol model.
Compare. Given two cells, determine their similarity, namely the
degree to which we believe them to represent the same pro-
tocol.
Merge. Combine two cells believed to represent the same proto-
col. This operation should be the equivalent of constructing
a new cell from the original protocol equivalence groups of
the merged cells.
Relying on the above operations, we can describe construction
more rigorously.
1. Combine training data sessions into equivalence groups based
on the given correlations. Each group consists of sessions us-
ing the same protocol.
2. Construct a cell from each equivalence group.
3. Cluster similar cells together based on the result of the Com-
pare operation between pairs of cells.
4. Merge clustered cells into a single cell.
Steps 1, 2, and 4 are fairly straightforward in view of the four
cell operations described earlier. Step 3, however, requires further
elaboration. The objective of Step 3 is to correctly combine cells
representing the same protocol into one. This objective requires
that cells of the same protocol be “similar” and cells of different
protocols “dissimilar.” This premise is central to our work, so we
state it formally.
Premise 3. For some size threshold σ and similarity threshold τ,
cells constructed from protocol equivalence groups containing at
least σ sessions must have similarity greater than τ if the underly-
ing protocols are the same, and less than τ if the underlying proto-
cols are different.
We base our protocol inference algorithms on this premise with
parameters σ and τ determined empirically. Note that we do not
claim that Premise 3 is always true in the real world, only that it
is a useful assumption for designing protocol inference algorithms.
Each of our three protocol models (Sections 4 and 5) deﬁnes its
own similarity measure.
3.5 Cells as Classiﬁers
Because in our model protocols are described by distributions,
we can classify unknown sessions by matching them with the max-
imum-likelihood distribution (cell). This is captured by the follow-
ing cell operation:
Score. Given a cell and a session, determine the likelihood that the
session is using the protocol described by the cell.
We use this as the basis for our classiﬁcation experiment pre-
sented in Section 7.
4. STATISTICAL MODELS
In this section we describe our ﬁrst two protocol models. Premises
1 and 2 tell us that a protocol may be viewed as a pair of distribu-
tions on byte strings (ﬂows) of length n. With this in mind, it is
natural to view a protocol model entirely in a statistical setting. Be-
fore recasting cell operations in statistical terms, we introduce the
concept of relative entropy and likelihood with respect to a distri-
bution.
Deﬁnition. Let P and Q be two distributions1 on some ﬁnite set
U. The relative entropy between P and Q is
D(P|Q) =
X
x∈U
P (x) log2
P (x)
Q(x)
.
Relative entropy is a measure of “dissimilarity” between two dis-
tributions. However, it is not a metric in the strict mathematical
sense. For more information on relative entropy and some of its
interpretations, see for example the text by Cover and Thomas [5].
In this paper, we use symmetric relative entropy, deﬁned as
D(P, Q) = D(P|Q) + D(Q|P )
X
=
x∈U
(P (x) − Q(x)) log2
P (x)
Q(x)
.
There are other, semantically more natural ways of deﬁning the
distance between two distributions. However, symmetric relative
entropy is the easiest measure to compute for the special distribu-
tions deﬁned by our two statistical protocol models, and has pro-
vided excellent results in practice.
We can now describe cell operations deﬁned in Section 3 in sta-
P ), the
tistical terms. A cell consists of a pair of distributions (
ﬁrst representing the ﬂow distribution from initiators to responders
and the second the ﬂow distribution from responders to initiators
within the protocol.
(cid:2)
P,
(cid:3)
(cid:3)
(cid:2)
P,
Construct. Given a set of sessions of a protocol equivalence group,
P is the distribution of ﬂows from
create a cell (
P is the dis-
initiators to responders in the set of sessions and
tribution of ﬂows from responders to initiators in the set of