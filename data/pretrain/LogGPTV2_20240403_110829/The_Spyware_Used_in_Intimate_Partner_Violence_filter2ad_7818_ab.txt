Google or in an ofﬁcial app store.
We emulate a hypothetical abuser seeking apps for IPS. We
hypothesize that most abusers begin by performing searches
such as “track my wife” or “read SMS from another phone”
in a search engine. Under this hypothesis, we gather exam-
ples of both resources for abusers (such as how-to guides)
and apps readily found by abusers. While our measurement
methodology may not uncover all IPS apps, we believe it
surfaces a representative sample of apps as used by abusers
because: (1) our approach uncovers a huge number of IPS
tools that (2) cover in aggregate all the types of tools reported
by survivors [16, 29, 40].
Below we describe our methodology for searching in greater
detail. For concreteness and because of its large market share,
we focus on the Android ecosystem, speciﬁcally using search
interfaces provided by Google.com and Google Play. Our
techniques can be applied to other ecosystems that provide
a search engine; see Appendix B for our treatment of Apple.
Methodology. To perform searches, we need a wide-ranging
list of queries that an abuser might use. For this, we utilize
query recommendation APIs provided by search engines, such
as a query completion API (provided by Google Play) and a
related query API (obtained by parsing the search results page
of Google). A query completion API responds with a number
of recommended search phrases that contain the submitted
query as a substring, whereas a related query API responds
with a set of search phrases believed to be semantically related
to the submission. In both cases the intent of the APIs is to
suggest related search phrases as educated by prior searches
made by other users of the engine.
We use a query snowballing approach to ﬁnd a large set of
useful queries given a small set of queries as a seed set. The
procedure is straightforward: query the recommendation APIs
on all queries in the seed set, collect the resulting recommen-
dations, query the resulting recommended search terms, and
continue until some predetermined number of queries have
been discovered (e.g., (cid:2) = 10,000 search phrases), or until we
converge to a set where no new recommendations are found.
More on query snowballing is given in Appendix A.
A. Searching for IPS on Google
We begin by applying our query snowballing to the Google
search engine. We used the Python Requests library [17] to
make the queries and download the results, and the Lxml
library [5] to parse the pages. Search results vary based on,
among many other factors, the query browser and the search
history. We used a user-agent identifying the request as from
a Chrome browser on Linux and disallowed any client-side
cookies to minimize inﬂuence of historical searches.
Google’s related query suggestions provide semantically
close queries, so we use in our seed set relatively long and
complete queries, as opposed to the smaller seed terms we use
for Google Play (see below). We begin with queries such as,
“how to catch my cheating spouse” or “track my husband’s
Type
Description
Blogs
Videos
Forums
News
Downloads
App sites
App store
Other
How-to blogs for IPS
How-to videos for IPS
Q&A forums for IPS
News about using spying software
Pages hosting IPS apps
Websites of apps
Link to apps in the ofﬁcial app stores
Irrelevant pages
#
21
12
7
2
12
5
2
39
Example
best-mobile-spy.com
youtube.com
quora.com
theguardian.com
download.cnet.com
thetruthspy.com
GPS Phone Tracker
amazon.com
Fig. 2: Types of websites found in manual analysis of 100 randomly
sampled URLs from our 27,741 URLs found via Google search.
phone without them knowing.” The returned suggestions are
not always relevant to our study, e.g., suggestions for “cheat”
include suggestions related to cheat codes for video games. We
therefore ﬁlter the queries using regular expression blacklists
(built via manual inspection). The initial set of seed queries
and the blacklists used are given in Appendix A.
Our snowballing process did not converge even after con-
sidering query sets of sizes up to (cid:2) = 10,000, therefore we
consider all 10,000 query recommendations. We submitted
each of these queries to Google and recorded the top 10 results
for each query. From these searches, we collected 27,741
URLs on 7,167 unique domains. We manually investigated a
random subset of 100 URLs to group their associated websites
into six major categories; see Figure 2. Nearly two-thirds of
the sampled pages are directly related to IPS, with only 39
URLs linking to unrelated content. We now discuss the 61
IPS-related URLs, ﬁrst those that provide information about
how to engage in IPS, and then those that link to IPS apps.
Information about conducting IPS. The majority of the IPS-
related URLs (65%) link to blogs, videos, or question-and-
answer forums discussing how to engage in IPS. The blogs
describe how to use one or more tools to spy on someone.
Example blog post topics include “Read your wife’s messages
without touching her phone” on a blog linking to mSpy and
“These apps can help you catch a cheating spouse” appearing
on the NY Post news site. News articles that came up in our
searches also point to incidents of spyware being used for
IPS. All these serve to direct those wanting to engage in IPS
to app websites, even should that website try to distance itself
from IPS. We discuss more about disingenuous blogging in
Section V. The video tutorials (mostly hosted on Youtube)
similarly explain how to setup and use apps for spying.
The question-and-answer forums focus on spying or track-
ing with discussions about how to use various tools for spying
on intimate partners. For example, in one forum an (ab)user
posts “I’m looking for an app I can install on my wife’s phone that
is hidden so that I can see where she is or has been via cell towers
or gps.” In reply, another (ab)user posts,
[Install] Cerberus from the market. Once installed and
conﬁgured, can be set to be invisible in the app drawer.
You can also record audio and take pictures remotely with
it! Be sure to silence the camera ﬁrst though!
IPS apps. The remaining IPS-related URLs linked to home
pages for apps, links to Google Play application pages, or
444
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:39:30 UTC from IEEE Xplore.  Restrictions apply. 
websites that aggregate a number of download links for apps.
From the Google search results and in the resulting web pages,
we collected 2,249 unique URLs pointing to Google Play
(extracted using regular expression search), among these URLs
we found 1,629 active apps listed in Google Play. Manual
analysis of a random sample of 100 of these apps revealed
that 22 were usable for IPS. All of which were separately
discovered by our search in the Play store (see Section III-B).
The prevalence of Google Play links found via search on
Google suggests that on-store apps will be found by abusers.
The dual-use nature of most of these on-store apps suggests
that tools used for IPS are, and will continue to be, allowed
on app stores.
To gather actual examples of off-store apps (distributed
outside Google Play), we examined all references to apps
found in the 100 manually-analyzed URLs. We also found
479 domains (among the full set of 7,167 domains returned by
searches) containing the words “spy”, “track” or “keylog”. We
investigated a random subset of 50 such domains, and found
that either they are discussing or hosting spyware apps. Finally,
we came across a web service called AlternativeTo [11],
which gives suggestions for alternative apps for some queried
application. We queried this service with the apps we had
identiﬁed thus far to ﬁnd more spyware apps.
Ultimately we found 32 unique off-store apps. These all
constitute overt spyware, as they advertise their ability to
surreptitiously track and monitor a device. Nine of the apps
had been discontinued at the time of our study and are no
longer available. The remaining 23 serve, in later sections, as
our corpus of off-store apps. We believe this corpus compre-
hensively represents a current snapshot of off-store spyware:
in many subsequent manual searches about IPS related topics
in the course of this research, we did not ﬁnd any reference
to additional off-store spyware.
B. Searching for IPS-relevant apps in Google Play
Our results above revealed that apps on Google Play come
up when searching Google for IPS-related phrases. We there-
fore investigate Play Store directly, to see what types of IPS-
related tools it hosts.
We perform a similar query snowballing procedure as
discussed above using the query completion API provided
by Google Play with smaller seed queries (as opposed to
the longer ones used in the previous Google search, see
Appendix A). In Google Play search, the snowball querying
converged rapidly to a ﬁnal set of suggested phrases.
With each phrase in the ﬁnal set, we search Google Play
and collect the metadata of the ﬁrst 50 apps returned. This
metadata contains, among other information, the description
of the app, the minimum version of Android supported by
the app, the date of the last update to the app in the Play
Store, a range for the number of downloads, the average user
rating, and a unique identiﬁer (called the Android app ID). For
each application we also downloaded the most recent reviews
(up to 200, the API limit), and the requested permissions as
listed in the application’s manifest ﬁle. We did this search
using a modiﬁed version of an unofﬁcial scraper for Google
Play called google-play-scraper [48]. We limit our searches to
at most ﬁve queries per second to minimize any operational
overhead on the search engines. This type of scraping is
generally considered acceptable research behavior [45].
Every day3 for one month starting on Oct 23, 2017, we
repeat query snowballing, and then perform searches on the
cumulative set of queries found in earlier days.
Results. On average the size of the query snowball retrieved
each day was 530 (with a standard deviation of 6). The set of
queries retrieved every day changed over time even though the
seed queries were the same. Every week, we saw about 40 new
queries gathered using our snowballing approach. The total
size of our query pool was 675 after one month of crawling.
In Figure 3a we show the change in the number of terms we
saw via snowballing and the total size of the query pool each
day. Google updates their query completion API periodically
to incorporate recent searches by users, which might account
for the periodic increase in the cumulative set of terms even
after crawling for a month.
The number of apps found in this process also varies
over time — rather more rapidly than the queries. We found
an average of 4,205 unique apps each day (with standard
deviation 450). We saw on average 288 new apps each day,
with a similar number of apps going missing (ones found in
previous days not found via our search procedure on a given
day). See Figure 3b. In total we collected 9,224 apps.
Our measurement suggests that
the results of searches
change signiﬁcantly over time. Part of this is the change in
the set of queries searched, and the rest is due to the fact that
many apps are removed from Google Play, some are updated,
and new apps are posted. Looking at the update date, we found
that 32% of all 9,224 apps were updated at least once, and 15%
were updated three times during our one month of study. Every
week on average 15% of the apps’ binaries were updated, with
the highest number of apps being updated on Mondays and
Thursdays (see Figure 3c). We also found that 208 (2%) of
the observed apps were removed from the Google Play store
(the Google Play pages of these apps return HTTP error 404)
during our study period. Apps may have been removed by
Google or by the developer, though we do not know which is
the case for these apps.
Developers can classify their apps within a ﬁxed set of
genres, which improves discovery of the app. However, we
found inconsistencies in the reported genres of some apps. For
example, an IPS-relevant app titled Friends & Family Tracker
was listed as a casual gaming app.
Among the 9,224 searched apps, many were not relevant to
our study. For example, the search results include many apps
for tracking ﬁnances or pregnancy, which cannot be used as
spyware. This necessitated a mechanism for pruning apps that
are not IPS-relevant.
3Scans were not performed on Nov 07 and Nov 08 due to a power failure.
445
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:39:30 UTC from IEEE Xplore.  Restrictions apply. 
700
650
600
550
s
e
s
a
r
h
p
f
o
#
Cumulative
Daily
Cumulative
Daily
s
p
p
a
f
o
#
8,000
6,000
4,000
Apps updated
400
300
200
100
s
p
p
a
f
o
#
10-22 10-27 11-01 11-06 11-11 11-16 11-21
10-22 10-27 11-01 11-06 11-11 11-16 11-21
10-22 10-27 11-01 11-06 11-11 11-16 11-21
Date of crawl (mm-dd)
Date of crawl (mm-dd)
Date of update (mm-dd)
(a)
(b)
(c)
Fig. 3: (a) The size of Google Play recommendation snowballs each day, and the size of the cumulative set of distinct queries. (b) The
number of distinct apps found each day, and the cumulative number of apps over time. (c) Number of apps updated each day by the developer.
C. Pruning false positives
Many of the apps we discovered via Play Store search are
not relevant to IPS, as per our scoping discussed in Section II.
We therefore need to ﬁlter out such false positives. The large
number of apps suggests the need for a scalable approach.
Pruning via machine learning. We decide to use supervised
machine learning to help ﬁlter out apps that are not IPS-
relevant. We hand labeled 1,000 randomly sampled apps from
the 3,777 apps we found in the ﬁrst day of crawling (ignoring
the apps whose descriptions are not in English); we refer to
this dataset as TR. We labeled them as IPS tools or benign
based on the information available on their Google Play page.
Of these 1,000 apps 280 (28%) were marked as IPS tools.
In building the model, we consider the description, sum-
mary, genre, and list of required permissions of the apps. We
tried other information, such as installation count and reviews,
but did not ﬁnd any improvement in accuracy.
We used a bag-of-words (BoW) model for the descriptions
and summaries using the CountVectorizer function provided
by the Scikit-Learn [50] library. We considered n-grams of
words, for 1 ≤ n ≤ 4 to construct the BoW model, ignoring
those that appear in less than 1% of the apps or more than
95% of the apps. From the BoW model we picked the 1,000
most discriminatory features based on the χ2-statistic [61]. We
treated each permission and each genre as individual words,
and created a BoW model for them. We picked the 50 most
discriminatory features from this model based on χ2-statistic.
Finally, we took the union of these features to represent each
app in a 1,050-dimensional feature space.
To train our model, we tried different machine learning
approaches, which we compared using an area under the
curve (AUC) metric [33]. For each ML algorithm, we perform
10-fold cross validation with randomly selected folds using
the hand-labeled training data TR, and then considered the