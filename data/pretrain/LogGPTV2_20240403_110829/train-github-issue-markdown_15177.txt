#### Describe the bug
When doing model selection on very high-dimensional data using `GridSearchCV`,
I found that when my estimator design has a `ColumnTransformer` where one of
the transformers has a columns spec that is a numpy array of e.g. 50k column
names, that `GridSearchCV` and the embedded joblib `Parallel` calls consume a
great deal of shared memory (/dev/shm) that's proportional to the number of
`GridSearchCV` fits. I always set `ColumnTransfomer` `n_jobs=1` in this setup.
When troubleshooting, I found that for every fit `GridSearchCV` is doing, for
some reason joblib `Parallel` is creating a duplicate pickle file of the same
column name numpy array in /dev/shm. I realized this by setting
`JOBLIB_TEMP_FOLDER` and seeing all the duplicate pickle files and loading
some of them to check. The shared memory usage can add up to many GBs since
for e.g. each pickle file might be 4 MB * 5000 fits = 20 GB.
What I then found out which is stranger, is that if I instead make the
`ColumnTransformer` columns spec a Python list instead of a numpy array then
there isn't any joblib `Parallel` /dev/shm pickling issue but this causes
major `GridSearchCV` and joblib `Parallel` performance degradation. I see that
each parallel job isn't even using close 100% CPU as if it's spending a lot of
time with I/O or overhead. This is not typical at all, in my experience model
selection with most model designs in sklearn is very CPU bound with low
overhead. So changing the `ColumnTransformer` column spec dtype for a very
large list of column names from a numpy array to Python list with everything
else being the exactly same results in a substantial change in `GridSearchCV`
behavior.
#### Steps/Code to Reproduce
I need to still come up with a minimal example using `sklearn.datasets` or
other public one, does anyone know of any very high-dimensional example
dataset that can be imported for use with sklearn?
    TODO
#### Expected Results
#### Actual Results
#### Versions
    System:
        python: 3.8.1 | packaged by conda-forge | (default, Jan 29 2020, 14:55:04)  [GCC 7.3.0]
    executable: /home/hermidalc/soft/miniconda3/envs/sklearn-workflows/bin/python
       machine: Linux-5.5.8-200.fc31.x86_64-x86_64-with-glibc2.10
    Python dependencies:
           pip: 20.0.2
    setuptools: 45.2.0.post20200209
       sklearn: 0.22.1
         numpy: 1.18.1
         scipy: 1.4.1
        Cython: None
        pandas: 1.0.1
    matplotlib: 3.1.3
        joblib: 0.14.1
    Built with OpenMP: True