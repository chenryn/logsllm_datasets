group contains one labeled number as the test sample, sk
, and a
ts
, containing other n − 1 labeled numbers. Then we
training set, Sk
tr
conduct n rounds of experiments. During the kth round, the Clus-
tering Ranker (CR) is seeded with only Sk
. Using it, CR cuts the
tr
dendrogram at a height to generate the best set of clusters (please
see Section 5.6 for details). Next, we check whether the test sample
could be correctly classified by the selected optimum clustering
sk
ts
outcome and stores the result for this round. Basically, we pretend
that the true label of the sk
is unknown and try to label it using
ts
the set of output clusters.
Figure 10: Flowchart of Evaluation Experiment
A spam source number test sample could be correctly classified
as true positive (TP) if it is in a pure spam cluster. Similarly, a benign
test sample would be considered as false positive (FP) if it is either
in a pure spam cluster or a mixed cluster. If a test spam number
is in a cluster that does not contain any labeled numbers from the
training set, the system will reject the test case and does not provide
any classifications for it.
6.1.3 Ground Truth Selection and Analysis. Before reporting our
evaluation results, we would need to provide more details about
our ground truth. As it was discussed in Section 6.1.2, we need to
feed the system with a set of seed labeled spam and labeled benign
numbers. We have already discussed in Section 3.2 an overview of
how we collect and label known spam and benign numbers, so here
we report some statistics about our ground truth.
Table 3 reports the total number of source numbers whose call
volume is in the top 2% for each test day of De dataset in July.
The column “num w/ complaint” shows how many of the source
numbers have complaints according to Baidu. The min and max
number of complaints are also shown along side the 50th, 75th,
90th, and 98th percentiles of complaints count.
Generate N Trainging_Sets and Test_Cases of Labeled NumbersStartRound > N?CR selects the best outcomeCompute TP or FP Rate for this roundCompute test TP and FP RateEndNoYesLabeled Spam NumbersLabeled Benign NumbersTraining Set [Str(k)]One Labeled Spam NumberOne Labeled Benign NumberTest Case [Sts(k)]ORReject this test case?NoStored Rejection Size till NowRejected Number+1YesCompute Labeled Spam Number Rejection RateLeave One OutStored TP and FP Rate for each roundSession 7: Cellular, Phone, and EmailASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea281Table 3: Number of user complaints according to Baidu for
source numbers in De
date
07/15
07/16
07/17
# source
numbers
15,044
13,402
17,455
num w/
complaint
3,542
2,610
5,408
min
1
1
1
complaints
complaints
max
10,574
10,574
10,574
percentile
90th
75th
8
13
13
8
8
14
98th
42
42
37
50th
3
3
3
As it was discussed in Section 3.2, we compile the list of known
spam numbers based on two different rules. According to the first
rule, we label any number in Sb ∩ Sc as spam, where Sb denotes
numbers that have been complained about according to Baidu and
Sc denotes spam numbers according to the provider of CDRs. Ac-
cording to the second rule, we count as spam numbers in Sb(θ),
that is those numbers that have received more than θ complaints
in Baidu. To set the value of θ, we use the information reported in
Table 3 as follows. Conservatively, we can count as spam only those
numbers that were overwhelmingly complained about by setting
θ to a value that corresponds to the 90th percentile of complaints.
These numbers are highly likely spam as they have received a large
number of complaints. Similarly, we can label more numbers as
spam by lowering θ to values that correspond to 75th and 50th per-
centiles. Note that as we reduce θ, we risk adding some noise into
our spam sets. In Section 6.2, we show how these different labeled
spam sets affect the system’s accuracy. Table 4 shows the number
of phone numbers that could be labeled as spam among all unique
numbers in each day of the De dataset according to these rules. In
the table and for simplicity, we use θ = 50th notation to indicate
that the value of θ corresponds to 50th percentile of complaints,
for example. The total number of labeled benign numbers is also
shown (we discussed how to label benign numbers in Section 3.2).
Table 4: Number of spam numbers in De based on different
criteria
date
07/15
07/16
07/17
spam labeling rule
Sc ∩ Sb
Sb(θ = 50th)
Sb(θ = 75th)
Sb(θ = 90th)
Sb(θ = 50th)
Sb(θ = 75th)
Sb(θ = 90th)
Sb(θ = 50th)
Sb(θ = 75th)
Sb(θ = 90th)
Sc ∩ Sb
Sc ∩ Sb
θ
3
8
13
1
3
8
13
1
3
8
14
1
120
# spam # benign
1,838
952
374
230
1,482
747
291
212
2,920
1,454
551
279
143
111
was able to find a set of clusters where Ls = 1.0 and Lb = 1.0
in all rounds of leave-one-out cross-validation. Only on 07/15 and
using Sb(θ = 50th), we were not able to completely separate known
spam and benign numbers. A manual analysis of the results for
this day indicated that this happened because the labeled spam set
contained noise due to the low θ threshold value. This caused the
Clustering Ranker to compromise and generate two clusters that
were mixed ones each containing one known spam and one known
benign number in one round of cross-validation.
Similarly, Table 5 also shows the TP and FP values in our leave-
one-out experiment (see Section 6.1.2). Again, the TP rates are
maximized and FP rates are minimized, except in the same case
discussed above. The “rejected” column, reports the number of
test labeled numbers that our system rejected (i.e. did not label
during test) as they ended up in clusters without any known spam
or benign numbers. These numbers obviously were not counted
towards the computation of TP and FP values.
An important observation in Table 5 is the value of Lu, the
number of completely unknown numbers that are in pure spam
clusters. Our system labels these numbers as new spam numbers.
To signify the importance of this result, the column “BL expansion”
reports by how much percent the original labeled spam set could
be expanded when these previously unknown numbers are added
to it as new spam numbers. As it can be seen, in the best case, we
expanded the list of spam numbers by 249.13%, 158.49%, and 158.42%
in each of the three test days, respectively. This is quite significant
as it shows the effectiveness of our system in augmenting spam
blacklists.
Also, notice that when the system is fed with a list labeled spam
numbers generated by computing Sb ∩ Sc, the number of output
clusters is fewer compared to others, while by using Sb(θ = 50th)
dataset a lot of clusters are generated each containing only few
numbers in them. The root cause of this phenomenon is again re-
lated to the added noise. Because the Clustering Ranker module
makes its best effort to maximally separate spam and benign num-
bers, it keeps splitting the clusters to avoid having mixed clusters
in the output. The number of output clusters inevitably increase as
a result.
Overall, our system performed the best by using Sb ∩ Sc, judging
by values of Lb, Ls, Lu, rejection rate, and expansion rate. In essence,
in this case, far fewer clusters are generated while at the same time
maximum separation of spam and benign numbers are maintained
and TP and FP are maximized and minimized respectively. This
consequently leads to lower rejection rates and higher Lu and
expansion rates as more unknown numbers would be in pure spam
clusters.
6.2 Evaluation Results and Analysis
For each day in the De CDR dataset, we conducted our evaluation
experiments. To perform the experiments, we also generated dif-
ferent sets of labeled spam numbers following our discussion in
Section 6.1.3. Table 5 shows the result for each day and for each set
of labeled spam numbers.
Column “# clusters” reports the number of clusters generated by
cutting the dendrogram at the optimum height for each day and
each labeled spam set. The values for Ls and Lb are also shown. As
it can be seen, with one exception, our Clustering Ranker module
6.3 Early Detection of Spam Numbers
In this section, we show how many of the previously unknown
source numbers that were identified as spam by our system on July
2017 actually received some complaints later and in the following
months. Specifically, we want to show that our system is able to
detect many unknown numbers as spam in early stages and many
months before they were added to Baidu due to user complaints.
Using Sb ∩ Sc and Sb(θ = 90th) our system labeled many pre-
viously unknown numbers as spam (see Lu and expansion rate in
Session 7: Cellular, Phone, and EmailASIACCS’18, June 4–8, 2018, Incheon, Republic of Korea282Table 5: Evaluation results using different labeled spam sets for each day of De
date
07/15
07/16
07/17
spam labeling rule
Sc ∩ Sb
Sc ∩ Sb
Sb(θ = 90th)
Sb(θ = 75th)
Sb(θ = 50th)
Sb(θ = 90th)
Sb(θ = 75th)
Sb(θ = 50th)
Sb(θ = 90th)
Sb(θ = 75th)
Sb(θ = 50th)
Sc ∩ Sb
# source numbers
15,044
13,042
17,455
# clusters
5,371
10,081
13,448
13,448
6,517
8,492
11,721
11,721
8,257
15,403
16,499
16,499
Ls
1
1
1
0.98
1
1
1
1
1
1
1
1
Lb
1
1
1
0.98
1
1
1
1
1
1
1
1
TP
100%
100%
100%
98%
100%
100%
100%
100%
100%
100%
100%
100%