𝐸𝑜𝑝𝑡 = 𝑎𝑟𝑔𝑚𝑖𝑛
𝐸
𝛿2
𝑖
= 𝑎𝑟𝑔𝑚𝑖𝑛
𝐸
(2)
(cid:17)
(cid:16) 𝑁
𝑖=1
(cid:16)𝛼𝑖 − 𝜃𝑖(𝐸)(cid:17)2
𝑁
𝑖=1
With larger 𝑁 , i.e., more measurements from the user, the 𝐸𝑜𝑝𝑡
converges better.
.
Figure 10: Near-field localization illustration. (a) Illustration of
symbols. (b) Localizing phone using absolute diffraction path length
from two ears.
Figure 11: Near-field HRTF (linear) interpolation
The idea behind near-field HRTF interpolation is actually simple. If
available measurements are from polar angles 𝜙1, 𝜙2, ...𝜙𝑁 around
the head, the interpolation module basically takes adjacent near-
field HRTFs and linearly interpolates for all angles between 𝜙𝑖 and
𝜙𝑖+1. Of course, the HRTFs from 𝜙𝑖 and 𝜙𝑖+1 need to be aligned
142
020406080Sample-1-0.500.5Amplitude LeftRightPersonalizing Head Related Transfer Functions for Earables
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
carefully along their first taps before the interpolation; otherwise
spurious echoes will get injected into the HRTF. To this end, we
convert the HRTFs into the time domain impulse responses (i.e.,
HRIRs), align them, and interpolate. Finally, observe that for a given
interpolated location 𝐿 and HRTF 𝐻𝐿, we can partly assess the
quality of interpolation (i.e., by modeling the diffraction from the
known head parameters 𝐸 and the location 𝐿). If the interpolated
HRTF deviates from this model, we adjust the channel taps to
match the expected time-difference and the amplitudes. These tuned
channels for every angle [0, 180] is converted back to the frequency
domain, and declared as the final near-field HRTF.
By now, we have covered the system design for measuring the
personalized near-field HRTF for a given user. Building on this,
we will then show how we estimate the far-field HRTF from our
near-field estimations.
4.3 Near-far conversion
Recall from Figure 7 that for a given angle 𝜃, the near and far-field
HRTFs are not the same. The goal of this module is to synthesize the
far-field HRTF from near-field measurements. Our observation is
that the far-field sound arrives to the ears as parallel rays (Figure 12),
while the near field sound – behaving as a point source – emanates
rays in all directions (Figure 13). This means the far field sound
rays are actually contained in the near field measurements. The
challenge lies in decomposing the near-field signals and extracting
out the appropriate rays. An accurate solution to this problem is
complex and computationally heavy because decomposing entails
searching in a high dimensional space. We develop a heuristic
based on first-order diffraction models and the physics of signal
propagation. Our intuition is to understand directions from which
far-field rays would arrive, and identify near-field locations that lie
on those rays (see Figure 12). We elaborate with an example next.
define several “critical” rays: ray 𝐵 − 𝐿 that arrives at the left ear,
ray 𝐷 − 𝑅 that arrives at the right ear, and ray 𝐶 − 𝑄 (also arriving
from angle 𝜃) is perpendicular to the tangent on the head at point
𝑄. Our observation is that the physics of wave propagation dictates
which rays will arrive at which ear. In other words, the incident
signal will diffract along a direction that deviates least from its
original direction. Hence, rays arriving on the left of 𝑄 (i.e., ones
passing through the arc [𝐶, 𝐵]) will diffract towards the left ear
due to the curvature of the ellipse. Rays impinging the right of 𝑄
(i.e., passing through the arc [𝐶, 𝐷]) will propagate towards the
right ear. And signals on the outer side of 𝐵 and 𝐷 will not arrive
at either ear.
Building on this intuition, observe that near-field HRTF measured
from locations in arc [𝐶 − 𝐵] can help synthesize the far-field HRTF
at angle 𝜃 at the left ear. Similarly, near-field HRTF from arc [𝐶−𝐷]
would contribute to the far-field HRTF on the right ear. Thus, UNIQ
approximates the far-field HRTF for the left ear as an average of
near-field left-ear HRTFs from locations in [𝐶 − 𝐵]; for the right
ear, average is from [𝐶 − 𝐷]. The method repeats for each value of
𝜃 ∈ [0, 180], meaning that 𝐵, 𝐶, and 𝐷 would change accordingly.
Additional attempts on near-far conversion
While the above approach yields encouraging results, it is admit-
tedly a heuristic. We have been exploring relatively deeper ap-
proaches, and while we have not succeeded yet, we discuss two of
them here. We believe these are rich topics of future work.
Our approach is aimed at decomposing the components of near-
field measurements – both diffraction and multipath from each
arrival angle – and then aggregating a subset of these components
to synthesize the far-field effect. Figure 13 aims to explain this
systematically. When transmitting from the near-field, the sound
source should be considered as a point source, emitting rays in
different directions 𝜃1, 𝜃2, ..., 𝜃𝑁 . Let us focus on a single point 𝑋𝑘
on the near field trajectory. Our measured near-field HRTF for
point 𝑋𝑘 is essentially the sum of the effects from all the signal rays
emanating from 𝑋𝑘, hence can be modeled as:
𝐻𝑛𝑒𝑎𝑟 (𝑋𝑘) =
𝐻(𝑋𝑘, 𝜃𝑖)
(4)
Now if we want to synthesize far-field signals from direction 𝜃𝑙, we
need to select only the 𝜃𝑙-bound rays from each of the points on
the near field trajectory (as shown by the yellow arrows in Figure
13). We can write this synthesize process as:
𝑖=0
𝑁
𝑀
𝐻𝑓 𝑎𝑟 (𝜃𝑙) =
𝐻(𝑋𝑖, 𝜃𝑙)
(5)
.
Figure 12: Near-far conversion: near-field HRTF on different part
of trajectory A would contribute to far-field HRTF at different ears.
Figure 12 shows a roughly circular trajectory (A) on which we have
estimated near-field HRTFs. Suppose we want to synthesize the far
field HRTF arriving from angle 𝜃 as shown in the figure. The signal
paths from the far-field, or rays, arrive in parallel, intersecting with
the trajectory A at different locations (e.g., B, C, D) Now, let us
𝑖=0
Evidently, if we can decouple the RHS of equation 4, and obtain
𝐻(𝑋𝑘, 𝜃𝑖) for any given 𝑘, 𝑖, then we can recombine and find the
𝐻𝑓 𝑎𝑟 (𝜃𝑙) in equation 5 (of course we still need to tune the delay
of each ray based on geometry). Hence, the core research question
pertains to correctly performing this decomposition.
■ Attempt 1: speaker beamforming: Modern smartphones have
2 speakers (one for the left channel and one for the right). If we
can utilize these 2 speakers to create a time-varying beamforming
pattern, this could help estimate 𝐻(𝑋𝑘, 𝜃𝑖). Specifically, denote the
143
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Zhijian Yang and Romit Roy Choudhury
4.4 Interface to Applications
The near and far-field HRTFs estimated by UNIQ can now be ex-
ported to earphone applications as a lookup table. The table is
indexed by 𝜃, and for each 𝜃𝑖, there are 4 vector entries:
𝜃𝑖 : ⟨𝐻𝑙𝑒 𝑓 𝑡
𝑛𝑒𝑎𝑟 , 𝐻𝑟𝑖𝑔ℎ𝑡
𝑛𝑒𝑎𝑟 ⟩ , ⟨𝐻𝑙𝑒 𝑓 𝑡
𝑓 𝑎𝑟
, 𝐻𝑟𝑖𝑔ℎ𝑡
𝑓 𝑎𝑟
⟩
Each HRTF is obviously a channel filter, so when an application
intends to synthesize a binaural sound 𝑆 from a desired location 𝐿,
the application first determines if 𝐿 is nearby or far-away, and the
angle 𝜃𝑖 of the location 𝐿 relative to the head. If 𝐿 is far-away, then
the application filters the sound as
𝑌𝑙𝑒 𝑓 𝑡 = 𝐻𝑙𝑒 𝑓 𝑡
𝑓 𝑎𝑟
𝑆, 𝑌𝑟𝑖𝑔ℎ𝑡 = 𝐻𝑟𝑖𝑔ℎ𝑡
𝑓 𝑎𝑟
𝑆
The earphone now plays the two sounds, 𝑌𝑙𝑒 𝑓 𝑡 and 𝑌𝑟𝑖𝑔ℎ𝑡 on the
left and right ears, respectively. The user perceives the sound to be
coming from angle 𝜃𝑖 from a far-away location. We next present
one potential application that can benefit from the estimated HRTFs.
Figure 13: Near-far conversion attempts: if we can decouple near-
field HRTF into rays, then far-field HRTF essentially needs to ex-
tract out one ray from each near-field location and recombine with
appropriate weights.
𝑁
𝑁
𝑖=0
beamforming pattern at one time instance as 𝑤(𝜃), which is a
function of angle 𝜃. Then we can rewrite Equation 4 as
𝐻𝑛𝑒𝑎𝑟 (𝑋𝑘) =
𝑤(𝜃𝑖) · 𝐻(𝑋𝑘, 𝜃𝑖)
(6)
𝑖=0
By creating time varying beamforming patterns 𝑤𝑡 (𝜃) – by chang-
ing the relative phase and amplitude of the 2 speakers – we can
generate multiple equations, one for each time instance. This could
enable us to solve for 𝐻(𝑋𝑘, 𝜃𝑖). The difficulty, however, is that the
2 speakers are unable to create a spatially narrow beam pattern.
This eventually leads to the system of equations being ill-ranked
and causes large errors for the estimated 𝐻(𝑋𝑘, 𝜃𝑖).
■ Attempt 2: blind decoupling: The net effect of 𝐻(𝑋𝑘, 𝜃𝑖) on
each signal ray has 2 components. First, the diffraction around the
head creates a delay and attenuation. Second, the signal bounces
from the pinna, creating an effect we call the pinna multipath.
Hence, the net effect on each signal ray can be expressed as
𝐻(𝑋𝑘, 𝜃𝑖) = 𝐴𝑖𝛿(𝜏𝑖) ∗ ℎ𝑘
(7)
where 𝛿 is the Dirac delta function, 𝜏𝑖 is the ray’s diffraction delay,
𝐴𝑖 is signal attenuation, and ℎ𝑘 is the time domain pinna multi-
path channel (∗ denotes convolution here). We plug Equation 7 to
Equation 4, and we can have
𝐴𝑖𝛿(𝜏𝑖) ∗ ℎ𝑘
(8)
𝐻𝑛𝑒𝑎𝑟 (𝑋𝑘) =
Now, if we can estimate𝑁
𝑖=0 𝐴𝑖𝛿(𝜏𝑖) and ℎ𝑘 separately, the de-
coupling can be solved. 𝛿(𝜏𝑖) can be estimated from diffraction
geometry, but we do not know 𝐴𝑖 and ℎ𝑘. This becomes a blind
decomposition problem. While sparsity opportunities could help
solve this problem, we realize that our physics based signal model
may be inadequate to capture the sophisticated real-world signal
propagation patterns. We believe machine learning techniques are
relevant here; we leave that to future work.
4.5 Binaural Angle of Arrival (AoA)
Understanding the incoming direction of real ambient sounds (rel-
ative to the user’s head) can enable smart earphones to fuel new
applications. For instance, earphones could serve as hearing aids,
and beamform in the direction of a desired speech signal; thus, Alice
and Bob could listen to each other more clearly by wearing head-
phones in a noisy bar. In another example, earphones could analyze
the AoAs of music echoes in a shopping mall and enable navigation
by triangulating the music speakers. Now, to accurately estimate
the AoAs of these ambient sounds, the earphones need to apply
the HRTF (since conventional AoA techniques are not designed
to cope with the HRTF distortions). This motivates HRTF-aware
AoA estimation, with both unknown source signals (such as Alice
and Bob’s speech) and known signals (such as those from ambient
acoustic speakers).
■ Known source signals: If the source signal is known, we first
extract the acoustic channels from the left and right ears. To now es-
timate AoA, we look for the following 2 features from the channels:
(1) the first tap relative delay between left and right channels, and
(2) the shape of the time-domain channel. Observe that (1) is im-
pacted by head diffraction and (2) is related to the pinna multipath,
both embedding information about the signal’s AoA. As mentioned
in Section 2, both these features vary across humans. This is why
the personalized HRTF is helpful here. We match these 2 features
from our measured channel against our estimation 𝐻𝑅𝑇 𝐹(𝜃) — the
𝜃 that maximizes the match is our AoA estimate.
Mathematically, let 𝑡0 be the relative first tap delay from our bin-
aural recording, and 𝑡(𝜃) be the same relative delay but for the
personal HRIR templates estimated for each 𝜃. Also denote 𝑐𝐿(𝜃)
and 𝑐𝑅(𝜃) as the correlation values for left/right channels with
(left/right) HRIR templates for all 𝜃. We define a target matching
function 𝑇 that contains both relative delay and channel correlation
information:
𝑇 (𝜃) = 𝜆|𝑡0 − 𝑡(𝜃)| + [1 − 𝑐𝐿(𝜃)] + [1 − 𝑐𝑅(𝜃)]
(9)
After training for the appropriate 𝜆, we find the actual AoA by
minimizing the target function.
144
Personalizing Head Related Transfer Functions for Earables
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
■ Unknown source signal: For unknown source signals, we can
no longer extract the 2 acoustic channels for left and right ears,
making it difficult to find the relative first tap delay, or left/right
channel shape.
However, we still have the opportunity to infer the first tap delay
from the relative channels between the left and right ear-recordings
– this can help estimate the AoA.
Of course, this is not straightforward since signals arriving at both
ears contain a lot of pinna multipath, and thus have poor auto-
correlation. This will cause multiple peaks in the relative channel,
as shown in Figure 14. Let us assume each peak has a relative delay
Δ𝑡𝑖. Based on our diffraction model, each relative delay Δ𝑡𝑖 can
further translate into 2 AoAs: 𝐴𝑜𝐴𝑖,1 and 𝐴𝑜𝐴𝑖,2 (one for front and
one for back). Now our task is to find the true AoA from all the