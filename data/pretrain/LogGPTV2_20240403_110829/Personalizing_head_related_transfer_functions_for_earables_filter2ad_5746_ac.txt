ğ¸ğ‘œğ‘ğ‘¡ = ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›
ğ¸
ğ›¿2
ğ‘–
= ğ‘ğ‘Ÿğ‘”ğ‘šğ‘–ğ‘›
ğ¸
(2)
(cid:17)
(cid:16) ğ‘
ğ‘–=1
(cid:16)ğ›¼ğ‘– âˆ’ ğœƒğ‘–(ğ¸)(cid:17)2
ğ‘
ğ‘–=1
With larger ğ‘ , i.e., more measurements from the user, the ğ¸ğ‘œğ‘ğ‘¡
converges better.
.
Figure 10: Near-field localization illustration. (a) Illustration of
symbols. (b) Localizing phone using absolute diffraction path length
from two ears.
Figure 11: Near-field HRTF (linear) interpolation
The idea behind near-field HRTF interpolation is actually simple. If
available measurements are from polar angles ğœ™1, ğœ™2, ...ğœ™ğ‘ around
the head, the interpolation module basically takes adjacent near-
field HRTFs and linearly interpolates for all angles between ğœ™ğ‘– and
ğœ™ğ‘–+1. Of course, the HRTFs from ğœ™ğ‘– and ğœ™ğ‘–+1 need to be aligned
142
020406080Sample-1-0.500.5Amplitude LeftRightPersonalizing Head Related Transfer Functions for Earables
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
carefully along their first taps before the interpolation; otherwise
spurious echoes will get injected into the HRTF. To this end, we
convert the HRTFs into the time domain impulse responses (i.e.,
HRIRs), align them, and interpolate. Finally, observe that for a given
interpolated location ğ¿ and HRTF ğ»ğ¿, we can partly assess the
quality of interpolation (i.e., by modeling the diffraction from the
known head parameters ğ¸ and the location ğ¿). If the interpolated
HRTF deviates from this model, we adjust the channel taps to
match the expected time-difference and the amplitudes. These tuned
channels for every angle [0, 180] is converted back to the frequency
domain, and declared as the final near-field HRTF.
By now, we have covered the system design for measuring the
personalized near-field HRTF for a given user. Building on this,
we will then show how we estimate the far-field HRTF from our
near-field estimations.
4.3 Near-far conversion
Recall from Figure 7 that for a given angle ğœƒ, the near and far-field
HRTFs are not the same. The goal of this module is to synthesize the
far-field HRTF from near-field measurements. Our observation is
that the far-field sound arrives to the ears as parallel rays (Figure 12),
while the near field sound â€“ behaving as a point source â€“ emanates
rays in all directions (Figure 13). This means the far field sound
rays are actually contained in the near field measurements. The
challenge lies in decomposing the near-field signals and extracting
out the appropriate rays. An accurate solution to this problem is
complex and computationally heavy because decomposing entails
searching in a high dimensional space. We develop a heuristic
based on first-order diffraction models and the physics of signal
propagation. Our intuition is to understand directions from which
far-field rays would arrive, and identify near-field locations that lie
on those rays (see Figure 12). We elaborate with an example next.
define several â€œcriticalâ€ rays: ray ğµ âˆ’ ğ¿ that arrives at the left ear,
ray ğ· âˆ’ ğ‘… that arrives at the right ear, and ray ğ¶ âˆ’ ğ‘„ (also arriving
from angle ğœƒ) is perpendicular to the tangent on the head at point
ğ‘„. Our observation is that the physics of wave propagation dictates
which rays will arrive at which ear. In other words, the incident
signal will diffract along a direction that deviates least from its
original direction. Hence, rays arriving on the left of ğ‘„ (i.e., ones
passing through the arc [ğ¶, ğµ]) will diffract towards the left ear
due to the curvature of the ellipse. Rays impinging the right of ğ‘„
(i.e., passing through the arc [ğ¶, ğ·]) will propagate towards the
right ear. And signals on the outer side of ğµ and ğ· will not arrive
at either ear.
Building on this intuition, observe that near-field HRTF measured
from locations in arc [ğ¶ âˆ’ ğµ] can help synthesize the far-field HRTF
at angle ğœƒ at the left ear. Similarly, near-field HRTF from arc [ğ¶âˆ’ğ·]
would contribute to the far-field HRTF on the right ear. Thus, UNIQ
approximates the far-field HRTF for the left ear as an average of
near-field left-ear HRTFs from locations in [ğ¶ âˆ’ ğµ]; for the right
ear, average is from [ğ¶ âˆ’ ğ·]. The method repeats for each value of
ğœƒ âˆˆ [0, 180], meaning that ğµ, ğ¶, and ğ· would change accordingly.
Additional attempts on near-far conversion
While the above approach yields encouraging results, it is admit-
tedly a heuristic. We have been exploring relatively deeper ap-
proaches, and while we have not succeeded yet, we discuss two of
them here. We believe these are rich topics of future work.
Our approach is aimed at decomposing the components of near-
field measurements â€“ both diffraction and multipath from each
arrival angle â€“ and then aggregating a subset of these components
to synthesize the far-field effect. Figure 13 aims to explain this
systematically. When transmitting from the near-field, the sound
source should be considered as a point source, emitting rays in
different directions ğœƒ1, ğœƒ2, ..., ğœƒğ‘ . Let us focus on a single point ğ‘‹ğ‘˜
on the near field trajectory. Our measured near-field HRTF for
point ğ‘‹ğ‘˜ is essentially the sum of the effects from all the signal rays
emanating from ğ‘‹ğ‘˜, hence can be modeled as:
ğ»ğ‘›ğ‘’ğ‘ğ‘Ÿ (ğ‘‹ğ‘˜) =
ğ»(ğ‘‹ğ‘˜, ğœƒğ‘–)
(4)
Now if we want to synthesize far-field signals from direction ğœƒğ‘™, we
need to select only the ğœƒğ‘™-bound rays from each of the points on
the near field trajectory (as shown by the yellow arrows in Figure
13). We can write this synthesize process as:
ğ‘–=0
ğ‘
ğ‘€
ğ»ğ‘“ ğ‘ğ‘Ÿ (ğœƒğ‘™) =
ğ»(ğ‘‹ğ‘–, ğœƒğ‘™)
(5)
.
Figure 12: Near-far conversion: near-field HRTF on different part
of trajectory A would contribute to far-field HRTF at different ears.
Figure 12 shows a roughly circular trajectory (A) on which we have
estimated near-field HRTFs. Suppose we want to synthesize the far
field HRTF arriving from angle ğœƒ as shown in the figure. The signal
paths from the far-field, or rays, arrive in parallel, intersecting with
the trajectory A at different locations (e.g., B, C, D) Now, let us
ğ‘–=0
Evidently, if we can decouple the RHS of equation 4, and obtain
ğ»(ğ‘‹ğ‘˜, ğœƒğ‘–) for any given ğ‘˜, ğ‘–, then we can recombine and find the
ğ»ğ‘“ ğ‘ğ‘Ÿ (ğœƒğ‘™) in equation 5 (of course we still need to tune the delay
of each ray based on geometry). Hence, the core research question
pertains to correctly performing this decomposition.
â–  Attempt 1: speaker beamforming: Modern smartphones have
2 speakers (one for the left channel and one for the right). If we
can utilize these 2 speakers to create a time-varying beamforming
pattern, this could help estimate ğ»(ğ‘‹ğ‘˜, ğœƒğ‘–). Specifically, denote the
143
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
Zhijian Yang and Romit Roy Choudhury
4.4 Interface to Applications
The near and far-field HRTFs estimated by UNIQ can now be ex-
ported to earphone applications as a lookup table. The table is
indexed by ğœƒ, and for each ğœƒğ‘–, there are 4 vector entries:
ğœƒğ‘– : âŸ¨ğ»ğ‘™ğ‘’ ğ‘“ ğ‘¡
ğ‘›ğ‘’ğ‘ğ‘Ÿ , ğ»ğ‘Ÿğ‘–ğ‘”â„ğ‘¡
ğ‘›ğ‘’ğ‘ğ‘Ÿ âŸ© , âŸ¨ğ»ğ‘™ğ‘’ ğ‘“ ğ‘¡
ğ‘“ ğ‘ğ‘Ÿ
, ğ»ğ‘Ÿğ‘–ğ‘”â„ğ‘¡
ğ‘“ ğ‘ğ‘Ÿ
âŸ©
Each HRTF is obviously a channel filter, so when an application
intends to synthesize a binaural sound ğ‘† from a desired location ğ¿,
the application first determines if ğ¿ is nearby or far-away, and the
angle ğœƒğ‘– of the location ğ¿ relative to the head. If ğ¿ is far-away, then
the application filters the sound as
ğ‘Œğ‘™ğ‘’ ğ‘“ ğ‘¡ = ğ»ğ‘™ğ‘’ ğ‘“ ğ‘¡
ğ‘“ ğ‘ğ‘Ÿ
ğ‘†, ğ‘Œğ‘Ÿğ‘–ğ‘”â„ğ‘¡ = ğ»ğ‘Ÿğ‘–ğ‘”â„ğ‘¡
ğ‘“ ğ‘ğ‘Ÿ
ğ‘†
The earphone now plays the two sounds, ğ‘Œğ‘™ğ‘’ ğ‘“ ğ‘¡ and ğ‘Œğ‘Ÿğ‘–ğ‘”â„ğ‘¡ on the
left and right ears, respectively. The user perceives the sound to be
coming from angle ğœƒğ‘– from a far-away location. We next present
one potential application that can benefit from the estimated HRTFs.
Figure 13: Near-far conversion attempts: if we can decouple near-
field HRTF into rays, then far-field HRTF essentially needs to ex-
tract out one ray from each near-field location and recombine with
appropriate weights.
ğ‘
ğ‘
ğ‘–=0
beamforming pattern at one time instance as ğ‘¤(ğœƒ), which is a
function of angle ğœƒ. Then we can rewrite Equation 4 as
ğ»ğ‘›ğ‘’ğ‘ğ‘Ÿ (ğ‘‹ğ‘˜) =
ğ‘¤(ğœƒğ‘–) Â· ğ»(ğ‘‹ğ‘˜, ğœƒğ‘–)
(6)
ğ‘–=0
By creating time varying beamforming patterns ğ‘¤ğ‘¡ (ğœƒ) â€“ by chang-
ing the relative phase and amplitude of the 2 speakers â€“ we can
generate multiple equations, one for each time instance. This could
enable us to solve for ğ»(ğ‘‹ğ‘˜, ğœƒğ‘–). The difficulty, however, is that the
2 speakers are unable to create a spatially narrow beam pattern.
This eventually leads to the system of equations being ill-ranked
and causes large errors for the estimated ğ»(ğ‘‹ğ‘˜, ğœƒğ‘–).
â–  Attempt 2: blind decoupling: The net effect of ğ»(ğ‘‹ğ‘˜, ğœƒğ‘–) on
each signal ray has 2 components. First, the diffraction around the
head creates a delay and attenuation. Second, the signal bounces
from the pinna, creating an effect we call the pinna multipath.
Hence, the net effect on each signal ray can be expressed as
ğ»(ğ‘‹ğ‘˜, ğœƒğ‘–) = ğ´ğ‘–ğ›¿(ğœğ‘–) âˆ— â„ğ‘˜
(7)
where ğ›¿ is the Dirac delta function, ğœğ‘– is the rayâ€™s diffraction delay,
ğ´ğ‘– is signal attenuation, and â„ğ‘˜ is the time domain pinna multi-
path channel (âˆ— denotes convolution here). We plug Equation 7 to
Equation 4, and we can have
ğ´ğ‘–ğ›¿(ğœğ‘–) âˆ— â„ğ‘˜
(8)
ğ»ğ‘›ğ‘’ğ‘ğ‘Ÿ (ğ‘‹ğ‘˜) =
Now, if we can estimateğ‘
ğ‘–=0 ğ´ğ‘–ğ›¿(ğœğ‘–) and â„ğ‘˜ separately, the de-
coupling can be solved. ğ›¿(ğœğ‘–) can be estimated from diffraction
geometry, but we do not know ğ´ğ‘– and â„ğ‘˜. This becomes a blind
decomposition problem. While sparsity opportunities could help
solve this problem, we realize that our physics based signal model
may be inadequate to capture the sophisticated real-world signal
propagation patterns. We believe machine learning techniques are
relevant here; we leave that to future work.
4.5 Binaural Angle of Arrival (AoA)
Understanding the incoming direction of real ambient sounds (rel-
ative to the userâ€™s head) can enable smart earphones to fuel new
applications. For instance, earphones could serve as hearing aids,
and beamform in the direction of a desired speech signal; thus, Alice
and Bob could listen to each other more clearly by wearing head-
phones in a noisy bar. In another example, earphones could analyze
the AoAs of music echoes in a shopping mall and enable navigation
by triangulating the music speakers. Now, to accurately estimate
the AoAs of these ambient sounds, the earphones need to apply
the HRTF (since conventional AoA techniques are not designed
to cope with the HRTF distortions). This motivates HRTF-aware
AoA estimation, with both unknown source signals (such as Alice
and Bobâ€™s speech) and known signals (such as those from ambient
acoustic speakers).
â–  Known source signals: If the source signal is known, we first
extract the acoustic channels from the left and right ears. To now es-
timate AoA, we look for the following 2 features from the channels:
(1) the first tap relative delay between left and right channels, and
(2) the shape of the time-domain channel. Observe that (1) is im-
pacted by head diffraction and (2) is related to the pinna multipath,
both embedding information about the signalâ€™s AoA. As mentioned
in Section 2, both these features vary across humans. This is why
the personalized HRTF is helpful here. We match these 2 features
from our measured channel against our estimation ğ»ğ‘…ğ‘‡ ğ¹(ğœƒ) â€” the
ğœƒ that maximizes the match is our AoA estimate.
Mathematically, let ğ‘¡0 be the relative first tap delay from our bin-
aural recording, and ğ‘¡(ğœƒ) be the same relative delay but for the
personal HRIR templates estimated for each ğœƒ. Also denote ğ‘ğ¿(ğœƒ)
and ğ‘ğ‘…(ğœƒ) as the correlation values for left/right channels with
(left/right) HRIR templates for all ğœƒ. We define a target matching
function ğ‘‡ that contains both relative delay and channel correlation
information:
ğ‘‡ (ğœƒ) = ğœ†|ğ‘¡0 âˆ’ ğ‘¡(ğœƒ)| + [1 âˆ’ ğ‘ğ¿(ğœƒ)] + [1 âˆ’ ğ‘ğ‘…(ğœƒ)]
(9)
After training for the appropriate ğœ†, we find the actual AoA by
minimizing the target function.
144
Personalizing Head Related Transfer Functions for Earables
SIGCOMM â€™21, August 23â€“27, 2021, Virtual Event, USA
â–  Unknown source signal: For unknown source signals, we can
no longer extract the 2 acoustic channels for left and right ears,
making it difficult to find the relative first tap delay, or left/right
channel shape.
However, we still have the opportunity to infer the first tap delay
from the relative channels between the left and right ear-recordings
â€“ this can help estimate the AoA.
Of course, this is not straightforward since signals arriving at both
ears contain a lot of pinna multipath, and thus have poor auto-
correlation. This will cause multiple peaks in the relative channel,
as shown in Figure 14. Let us assume each peak has a relative delay
Î”ğ‘¡ğ‘–. Based on our diffraction model, each relative delay Î”ğ‘¡ğ‘– can
further translate into 2 AoAs: ğ´ğ‘œğ´ğ‘–,1 and ğ´ğ‘œğ´ğ‘–,2 (one for front and
one for back). Now our task is to find the true AoA from all the