NMI to all other CPUs (excluding C itself). Since the NMI
cannot be masked, this causes the target CPUs to receive an
NMI. The NMI handler is invoked, which is an idle spin-lock
loop that stalls the CPU on which it runs. Once the intercept
has been handled on C, XMHF signals the spin-lock which
causes the other CPUs to resume.
Note that DRT automatically disables NMI generation,
so the XMHF secure-loader and runtime initialization are
guaranteed to be single-threaded. The XMHF core then sets
up the NMI handler as described above. At runtime, a NMI
handler execution on a given CPU is also guaranteed to be
atomic. The CPU initiating the quiescing will wait for all
the other CPU NMI handlers to enter the idle spin-lock loop
before proceeding to execute an intercept handler.
3) Ensuring MPROT: XMHF uses HPTs for efﬁcient guest
memory access control. In particular, the hardware ensures
that all memory accesses by guest instructions go via a
two-level translation in the presence of the HPT. First, the
virtual address supplied by the guest is translated to a guest
physical addresses using guest paging structures. Next, the
guest physical addresses are translated into the actual system
physical addresses using the permissions speciﬁed within
the HPT. If the access requested by the guest violates the
permissions stored in the HPT, the hardware triggers an
intercept indicating a violation.
XMHF leverages hardware DMA protections to protect its
memory regions from direct access by devices. The DMA
protection is part of the hardware platform (e.g., chipset) and
is speciﬁed using a DMA table within XMHF. In particular,
the hardware ensures that all memory accesses by system
devices are translated using the permissions speciﬁed within
the DMA table. The hardware disallows DMA if the access
requested violates the permissions stored in the DMA table.
XMHF uses the Extended Page Tables (EPT) and Nested
Page Tables (NPT) on Intel and AMD x86 platforms respec-
tively for guest memory access control. On Intel platforms,
XMHF uses the VT-d (Virtualization Technology for Directed
I/O) support to provide DMA protection. The VT-d page ta-
bles contain mappings and protections for the DMA address
space as seen by system devices. On AMD platforms, XMHF
relies on the Device Exclusion Vector (DEV) for DMA
protection. DEV’s bitmap structure allows DMA protection
to be set for a given memory address range.
4) Ensuring INIT: During initialization XMHF sets up the
HPTs and DMA table permissions so that memory addresses
corresponding to the hypervisor memory regions are marked
read-only and therefore cannot be modiﬁed by either the
guest or system devices.
The XMHF core memprot and dmaprot components set
up the EPT/NPTs and the VT-d/DEV DMA protection
permissions on Intel and AMD x86 platforms respectively.
435
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
5) Ensuring MED: During initialization, XMHF activates
the platform hardware DMA protection mechanism that en-
forces DMA access control for hypervisor memory accesses
by system devices. More concretely, the XMHF core dmaprot
component activates the VT-d and DEV DMA protection
mechanisms on Intel and AMD x86 platforms respectively,
to prevent system devices from accessing memory regions
belonging to XMHF.
Access control protections for guest memory accesses
are described by the HPTs and enforced by the CPU
when operating within guest-mode. XMHF uses partitions2
to contain guest code and data. A partition is essentially a
bare-bones CPU hardware-backed execution container that
enforces system memory isolation for the guest or a portion
of it based on HPTs that are initialized by the hypervisor.
XMHF creates a primary partition in order to run the guest
operating environment. XMHF can also instantiate secondary
partitions on demand when requested by a hypapp. These
secondary partitions are capable of running speciﬁed code
within a low-complexity isolated environment, which is
explicitly designed without support for scheduling or device
interaction (Figure 1). This is useful when a hypapp wishes
to implement desired security properties at a ﬁne granularity,
e.g., portions of an untrusted application within the operating
environment (e.g., TrustVisor [7] and Alibi [1]).
core
component
uses
partition
The XMHF
the
VMLAUNCH/VMRESUME and VMRUN CPU instructions on
Intel and AMD x86 platforms respectively to instantiate
partitions. The following paragraphs describe how XMHF
supports multi-processor guests while ensuring hypervisor
memory protection.
On x86 platforms, only one CPU – called the boot-strap
processor (BSP) – is enabled when the system starts. The
other CPUs remain in halted state until activated by software
running on the BSP. During its initialization, XMHF activates
the remaining CPUs and switches all the CPUs (including
the BSP) to host-mode. Next, XMHF sets up the HPTs on
all the cores and switches the BSP to guest-mode to start
the guest; the remaining CPUs idle in host-mode within
XMHF. Finally, the XMHF core smpguest component uses
a combination of HPTs and intercept handling (described
below) to ensure that the remaining cores are switched to
guest-mode before they execute guest code. This ensures that
HPT access control is always enabled for all CPU cores.
A native multicore capable OS, on the x86 platform, uses
the CPU Local Advanced Programmable Interrupt Controller
(LAPIC) for multicore CPU initialization [23], [24]. More
speciﬁcally, the LAPIC Interrupt Control Register (ICR) is
used to deliver the startup sequence to a target core. On
x86 platforms, the LAPIC is accessed via memory-mapped
I/O encompassing a single physical memory page. XMHF
leverages Hardware Page Tables (HPTs) to trap and intercept
accesses to the LAPIC memory page by the guest OS.
Subsequently, any writes to the LAPIC ICR by the guest
causes the hardware to trigger a HPT violation intercept. The
XMHF core handles this intercept, disables guest interrupts
and sets the guest trap-ﬂag and resumes the guest. This
causes the hardware to immediately trigger a single-step
intercept, which is then handled by the XMHF core to process
the instruction that caused the write to the LAPIC ICR. If
a startup command is written to the ICR, XMHF voids the
instruction and instead runs the target guest code on that
core in guest-mode.
6) Ensuring SAFEUPD: XMHF requires both the XMHF
core and the hypapp to use a set of well-deﬁned interfaces
provided by the core to perform all changes to the HPTs.
These interfaces upon completion, ensure that permissions
for the hypervisor memory regions remain unchanged. In
the current XMHF implementation, a single XMHF core
API function setprot provided by the core memprot
component allows manipulating guest memory protections
via the HPTs.
V. XMHF VERIFICATION
In this section we present our veriﬁcation efforts on the
XMHF implementation. We discuss which DRIVE properties
are manually audited and why they are likely to remain valid
during XMHF development. We also show how most of the
veriﬁcations of DRIVE properties are reduced to inserting
assertions in XMHF’s source code, which is then checked
automatically using CBMC [22]. As our focus is on verifying
the memory integrity of the XMHF core, we use a simple
hypapp for veriﬁcation purposes. The hypapp implements
a single hypercall interface to manipulate guest memory
protections via Hardware Page Tables (HPT).
A. Overview
The XMHF veriﬁcation process is largely automated. We
manually audit 422 lines of C code and 388 lines of
assembly language code. The manual auditing applies to
functions in XMHF core that are unlikely to change as
development proceeds. The automatic verﬁcation of 5208
lines of C code uses CBMC.
In addition to the system assumptions presented in §II,
the soundness of our veriﬁcation results depends on two
additional assumptions: (i) CBMC is sound. i.e., if CBMC
reports that XMHF passes an assertion, then all executions
of XMHF satisfy that assertion; and (ii) the XMHF core
interface – determined by the available types of hardware
virtualization intercepts – is complete, i.e., XMHF handles
all possible intercepts from guests.
2Such CPU execution containers are often called hardware virtual ma-
chines in current parlance. However, this is a misnomer in our case since,
technically, a virtual machine presents to the guest a virtualized view of
the system devices in addition to enforcing memory isolation.
B. Verifying Modularity (MOD)
We verify MOD by engineering the source code of
XMHF to ensure that the implementations of init() and
436
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
ih 1(), . . . , ih k() are modular (recall §III-A). The XMHF
implements the init() function
core startup component
in XMHF. It ﬁrst performs required platform initialization,
initializes memory such that MPROT holds, then starts the
guest in guest-mode. The XMHF core eventhub component
implements ih 1(), . . . , ih k() in XMHF. More speciﬁcially,
the eventhub component consists of a single top-level inter-
cept handler function which is called whenever any guest
intercept is triggered. We refer to this function as ihub().
The arguments of ihub() indicate the actual intercept that
was triggered. Based on the value of these arguments, ihub()
executes an appropriate sub-handler.
C. Verifying Atomicity (ATOM)
We rely on the hardware semantics of Dynamic Root-
of-Trust (DRT) (§IV-B2) to discharge ATOMinit. There are
preliminary veriﬁcation results of the correctness of DRT
at
the design-level [33], which forms the basis of our
assumptions on DRT’s semantics.
We check ATOMih by manually auditing the functions
implementing CPU-quiescing (§IV-B2). More speciﬁcally
we manually audit three C functions which are responsible
for stalling and resuming the CPUs and for handling the
NMI used for CPU quiescing, to ensure proper intercept
serialization. While these checks are done manually, we
believe that it is acceptable for several reasons. First, the
functions total to only 60 lines of C code. They are largely
self-contained (no dependent functions and only four global
variables) and invoked as wrappers at the beginning and
end of the intercept handlers. Therefore, we only need to
perform manual re-auditing if any of the CPU-quiescing
functions themselves change. Given the simple design and
functionality of quiescing, and based on our development
experience so far, we anticipate the quiescing functions to
remain mostly unchanged as development proceeds.
D. Verifying MPROT
MPROT is always preserved by XMHF since the HPTs and
DMA protection data structures are statically allocated. We
verify MPROT automatically by employing a build script
that inspects relevant symbol locations in the object and
executable ﬁles produced during the build process to ensure
that the DMA protection and HPT data structures reside
within the correct data section in hypervisor memory M.
E. Verifying INIT
INIT is checked by a combination of manual audits and
automatic veriﬁcation on the XMHF source to ensure that
before XMHF’s init() function completes, the following are
true: DMA table and HPTs are correctly initialized so that
memory addresses corresponding to the hypervisor memory
regions cannot be changed by either the guest or system
devices; and the intercept entry point in XMHF points to
ihub().
//start a partition by switching to guest-mode
//cpu = CPU where the partition is started
void xmhf_partition_start(int cpu)
{
...
#ifdef VERIFY
assert( cpu_HPT_enabled );
assert( cpu_HPT_base == HPT_base );
assert( cpu_intercept_handler == ihub );
#endif
//switch to guest-mode
}
Outline of xmhf_partition_start,
Figure 2.
the function
used to execute a target cpu in guest-mode. cpu_HPT_enabled
and cpu_HPT_base enforce hardware page table (HPT) protections.
cpu_intercept_handler is where the CPU transfers control to when
an intercept is triggered in guest-mode. HPT_base and ihub are the
XMHF initialized HPTs and the intercept handler hub respectively. These
assertions allow automatic veriﬁcation of DRIVE properties INIT and MED
in XMHF using a model checker.
The manual audits involve 311 lines of C code and 338
lines of assembly language code which include platform
hardware initialization, loops including runtime paging and
DMA table and HPT setup, and concurrency in the form
of multicore initialization within XMHF. Given the stable
hardware platform and multicore initialization logic as well
as paging, DMA and HPT data structure initialization re-
quirements, we postulate that the manually audited code
will remain mostly unchanged as development proceeds,
ensuring minimal manual re-auditing effort.
1) DMA table and HPT initialization: The XMHF secure-
loader and the XMHF core startup component set up the
DMA table to prevent system devices from accessing XMHF
memory regions. We manually audit the C functions respon-
sible for setting up the DMA table to verify that the functions
assign entries in the DMA table such that all addresses in
M are designated read-only by devices.
Before the XMHF init() function transfers control to the
guest, it calls a C function to setup the HPTs for the guest.
We manually audit this C function to verify that the function
assigns each entry in the HPTs such that all addresses in M
are designated read-only by the guest.
the guest
2) Intercept entry point: The XMHF init() function
ﬁnally invokes the xmhf_partition_start function to
start
in guest-mode. We insert an assertion
in xmhf_partition_start and use CBMC to auto-
matically verify the assertion. The inserted assertion in
xmhf_partition_start checks that the CPU is setup to
transfer control to ihub() on an intercept (Figure 2).
F. Verifying MED
MED is veriﬁed by ensuring the following: (1) XMHF’s
init() function in the end sets the CPU to execute in guest-
mode with the appropriate hardware memory protections
in place. In particular, in a multicore system, every CPU
that is initialized is set to execute in guest-mode. (2) DMA
protection is always active during XMHF runtime. We insert
437
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:03 UTC from IEEE Xplore.  Restrictions apply. 
assertions in XMHF source code and use CBMC to automat-
ically verify these assertions.
execution:
XMHF
1) Guest-mode
initializes
memory protection for a CPU and ﬁnally uses
the
xmhf_partition_start function to execute a target CPU
in guest-mode. Therefore, we verify that CPU transitions
to the correct guest-mode by model-checking the validity
of assertions inserted in the xmhf_partition_start
function (Figure 2). The assertions check that appropriate
ﬁelds in the MMU data structures are set to point to the
correct HPTs and that HPT protections are in effect before
using the CPU instruction that performs the switch to
guest-mode.
On a multicore platform, XMHF uses a combination of
the nested page fault (ih npf) and the single-step (ih db)
intercepts to ensure that each CPU that is initialized is set to