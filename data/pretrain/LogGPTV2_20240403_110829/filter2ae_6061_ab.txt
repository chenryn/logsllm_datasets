> I know this question, but if the IP format is banned, the website using the
> IP format will not be collected. Although it is not used a lot, it will
> still be encountered. There is no better way to think about it now.
然后过了几天,作者意识到了这样并不算修复了SSRF漏洞,再次commit了一个补丁.
具体更新内容:
添加了过滤代码,关键代码如下:
    public static function remote($url, $_count = 0) {
        if(!iHttp::is_url($url,true)){
            $parsed = parse_url($url);//解析url
            $validate_ip = true;
            preg_match('/\d+/', $parsed['host']) && $parsed['host'] = long2ip($parsed['host']);//获取host部分,如果是十进制或其他进制的ip地址,转化成标准的ip地址
            if(preg_match('/\d+\.\d+\.\d+\.\d+/', $parsed['host'])){
                $validate_ip = filter_var($parsed['host'], FILTER_VALIDATE_IP, FILTER_FLAG_NO_PRIV_RANGE | FILTER_FLAG_NO_RES_RANGE);//匹配正确的ip格式,过滤非法ip地址字符与内外地址
            }
            if(!in_array($parsed['scheme'],array('http','https')) || !$validate_ip|| strtolower($parsed['host'])=='localhost'){
                if (spider::$dataTest || spider::$ruleTest) {
                    echo "{$url} 请求错误:非正常URL格式,因安全问题只允许抓取 http:// 或 https:// 开头的链接";
                    echo "{$url} 请求错误:非正常URL格式,因安全问题只允许抓取 http:// 或 https:// 开头的链接或私有IP地址";
                }
                return false;
            }
        }
            ...
    }
可以看到,这次添加了检查ip地址的格式,以及是否是内网ip.
以普通开发者的角度思考,很多情况都是哪里出了问题就修哪里,什么东西能绕过就过滤什么.也很难要求他们完全了解安全漏洞,因此也导致了修复再次被绕过.
### CVE-2018-15895补丁分析
与上个漏洞提交者是同一个人. 
然而,普通开发人员通常是哪里有问题就去解决哪里的问题,并不一定能对某个漏洞有深入的认识,更不用说了解全部攻击与绕过手段,但是只要漏了一种,修复补丁就等于完全没有.
我们都知道,SSRF的常用绕过手法还有302重定向与DNS重绑定.漏洞提交者也演示了这两种方式.具体POC可以看[issue内容](https://github.com/idreamsoft/iCMS/commit/59ad0f303bf900f552f737b63c6fa8d92c1403d7).
关键代码如下:
    public static function safe_url($url) {
            $parsed = parse_url($url);
            $validate_ip = true;
            if($parsed['port'] && is_array(self::$safe_port) && !in_array($parsed['port'],self::$safe_port)){
                if (spider::$dataTest || spider::$ruleTest) {
                    echo "请求错误:非正常端口,因安全问题只允许抓取80,443端口的链接,如有特殊需求请自行修改程序".PHP_EOL;
                }
                return false;
            }else{
                preg_match('/^\d+$/', $parsed['host']) && $parsed['host'] = long2ip($parsed['host']);
                $long = ip2long($parsed['host']);
                if($long===false){
                    $ip = null;
                    if(self::$safe_url){
                        @putenv('RES_OPTIONS=retrans:1 retry:1 timeout:1 attempts:1');
                        $ip   = gethostbyname($parsed['host']);
                        $long = ip2long($ip);
                        $long===false && $ip = null;
                        @putenv('RES_OPTIONS');
                    }
                }else{
                    $ip = $parsed['host'];
                }
                $ip && $validate_ip = filter_var($ip, FILTER_VALIDATE_IP, FILTER_FLAG_NO_PRIV_RANGE | FILTER_FLAG_NO_RES_RANGE);
            }
            if(!in_array($parsed['scheme'],array('http','https')) || !$validate_ip){
                if (spider::$dataTest || spider::$ruleTest) {
                    echo "{$url} 请求错误:非正常URL格式,因安全问题只允许抓取 http:// 或 https:// 开头的链接或公有IP地址".PHP_EOL;
                }
                return false;
            }else{
                return $url;
            }
        }
可以看到,使用了第17行使用了`gethostbyname` 确定parse_url解析后的host部分,来防护DNS rebinding 攻击.
并且在curl的options中,注释了`// CURLOPT_FOLLOWLOCATION => 1,// 使用自动跳转`,来防护302重定向绕过.
经常打ctf的小伙伴可能就会注意到了,攻击的思路可以针对parse_url的解析问题.历代parse_url存在不少方式缺陷,比如scheme,host,port,path等均有过绕过的记录.而这些细节,是开发者很难注意到的.如果想要再次绕过,这里就是个很好的突破点.
### 新的绕过
光黑名单和检查host真实ip来说,基本上是万无一失.但是作者万万没想到,来自php自身的背后一刀.在2017年blackhat上orange师傅演讲的[
A New Era of SSRF ](https://www.blackhat.com/docs/us-17/thursday/us-17-Tsai-A-New-Era-Of-SSRF-Exploiting-URL-Parser-In-Trending-Programming-Languages.pdf)中,有一个新的攻击方式,利用php中的parse_url函数和libcurl对url的解析差异,导致了对host的过滤失效,成功绕过.
从orange师傅的ppt中偷一张图来解释.
php-curl拓展解析url的host在第二个@之后,而parse_url则是最后一个@之后.
因此我们可以使用如下payload绕过:
`http://ip/admincp.php?app=spider_project&do=test&url=http://m09ic@127.0.0.1:PI:EMAIL/&rid=2&pid=1&title=`
可以看到,同时绕过了port和host的限制,访问到了只对本地开放的81端口的phpinfo内容.成功绕过过滤实现SSRF.
这里有一个小坑,在较新版本的php-curl中,已经修复了多个@的解析问题,使用多个@会报错,不知道为啥不是调整到与parse_url一致,这种修复显然影响了可用性.
该漏洞也不单是cms的问题,也有curl的问题.不管所使用的所有开源组件是不是安全的,在常见漏洞上cms中再加一层过滤是必要的.
大多数linux发行版并没有使用最新版本的curl.可以在 
这里查询linux发行版与curl版本的对应关系,应该少有公司会实时更新操作系统版本,只要不是最新版本的操作系统,基本都存在该漏洞.
我只测试了ubuntu,在ubuntu16.04及以下均可以使用该方式绕过.而在ubuntu18.04中,已经不再可以.`exec_curl`函数执行会直接返回false.
ubuntu16.04的curl版本是:
    # curl -V
    curl 7.47.0 (x86_64-pc-linux-gnu) libcurl/7.47.0 GnuTLS/3.4.10 zlib/1.2.8 libidn/1.32 librtmp/2.3
    Protocols: dict file ftp ftps gopher http https imap imaps ldap ldaps pop3 pop3s rtmp rtsp smb smbs smtp smtps telnet tftp 
    Features: AsynchDNS IDN IPv6 Largefile GSS-API Kerberos SPNEGO NTLM NTLM_WB SSL libz TLS-SRP UnixSockets
已经提交了[issue](https://github.com/idreamsoft/iCMS/issues/88),坐等作者的修复,期待是否还有被绕过的可能:D
## 最后
( **面向github代码审计** )
一个开发人员很难有精力去了解一个攻击方式的方方面面,也很难让开发者紧跟攻击手法的趋势.在刚才的例子看到,虽然开发者积极的解决漏洞,但是并不能有效缓解漏洞,总有普通开发者不知道的方式再次绕过.
总得来说,初尝代码审计时,可以多翻翻issue中多次出现漏洞的点,这种地方再次出现漏洞的几率相对来说较高.
另外,这个漏洞在利用要进入后台,又过滤了各种敏感协议,实际上危害并不大,仅仅用来学习代码审计的思路以及常见SSRF的绕过与防护方式.