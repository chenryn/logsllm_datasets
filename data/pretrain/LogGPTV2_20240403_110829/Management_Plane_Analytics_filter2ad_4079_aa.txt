title:Management Plane Analytics
author:Aaron Gember-Jacobson and
Wenfei Wu and
Xiujun Li and
Aditya Akella and
Ratul Mahajan
Management Plane Analytics
*Aaron Gember-Jacobson, *Wenfei Wu, *Xiujun Li, *Aditya Akella, †Ratul Mahajan
*University of Wisconsin-Madison, †Microsoft Research
ABSTRACT
While it is generally held that network management is tedious and
error-prone, it is not well understood which speciﬁc management
practices increase the risk of failures.
Indeed, our survey of 51
network operators reveals a signiﬁcant diversity of opinions, and
our characterization of the management practices in the 850+ net-
works of a large online service provider shows signiﬁcant diversity
in prevalent practices. Motivated by these observations, we develop
a management plane analytics (MPA) framework that an organiza-
tion can use to: (i) infer which management practices impact net-
work health, and (ii) develop a predictive model of health, based
on observed practices, to improve network management. We over-
come the challenges of sparse and skewed data by aggregating data
from many networks, reducing data dimensionality, and oversam-
pling minority cases. Our learned models predict network health
with an accuracy of 76-89%, and our causal analysis uncovers some
high impact practices that operators thought had a low impact on
network health. Our tool is publicly available, so organizations can
analyze their own management practices.
Categories and Subject Descriptors
C.2.3 [Computer-Communication Networks]: Network Opera-
tions—Network management
Keywords
Network management practices; network health; quasi-experimental
design; decision trees
1.
INTRODUCTION
Computer networks are logically composed of three planes: data,
control, and management (Figure 1). The data plane forwards pack-
ets. The control plane generates forwarding tables and ﬁlters for
the data plane using conﬁguration ﬁles and routing protocols (e.g.,
OSPF and BGP)—or control programs in the case of software de-
ﬁned networking (SDN). The management plane is a collection of
practices that deﬁne the network’s physical composition, control
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full cita-
tion on the ﬁrst page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or re-
publish, to post on servers or to redistribute to lists, requires prior speciﬁc permission
and/or a fee. Request permissions from Permissions@acm.org.
IMC’15, October 28–30, 2015, Tokyo, Japan.
c(cid:13) 2015 ACM. ISBN 978-1-4503-3848-6/15/10 ...$15.00.
DOI: http://dx.doi.org/10.1145/2815675.2815684.
Figure 1: The three network planes
plane conﬁguration, and monitoring schemes based on an organi-
zation’s policies and objectives.
The networking community has a strong track record of devel-
oping innovative tools and techniques to discover how control and
data planes function, even when the network does not directly re-
veal that information. Examples include techniques to infer the
paths taken by packets [35], link characteristics [10], available band-
width along a path [16], loss rate and re-ordering [24], network
topology [31], and so on.
However, little work has gone into characterizing the manage-
ment plane, despite its importance to well-functioning networks.
As a result, we lack a systematic understanding of what manage-
ment practices are common today—i.e., how do operators design
and (re)conﬁgure the physical and logical structure of their net-
works? Even simple practices such as how heterogeneous are a
network’s devices, and how often and why are networks changed,
are poorly understood in the research community. Furthermore,
while it is generally held that network management is tedious and
error-prone, researchers and operators alike don’t have a principled
understanding of which management practices pose a higher risk
of performance and availability problems. Indeed, our survey of
51 network operators (Figure 2) reveals a signiﬁcant diversity of
opinions regarding this issue.
This paper makes two contributions. First, we present a sys-
tematic characterization of the management practices employed in
over 850 networks managed by a large online service provider. Our
characterization offers the ﬁrst in-depth look into the management
practices used in modern networks. Second, we propose a manage-
ment plane analytics (MPA) framework that uncovers the relation-
ship between network health (e.g., the frequency of performance
and availability problems) and management practices. An orga-
nization can apply MPA to its networks to: (i) determine which
practices cause a decline, or improvement, in network health; and
Management plane  Defines the (cid:374)etwork’s physical co(cid:373)positio(cid:374), control  plane configuration,  and monitoring schemes Primary entities: CLI, scripts, wiring diagrams Data plane Forwards packets Primary entities: forwarding tables, filters Control plane Generates forwarding tables and filters for the data plane Primary entities: routing tables, configuration 395(ii) develop a predictive model of health, based on management
practices, to help shape future practices and aid what-if analysis.
In our work we face three main challenges. First, management
practices and their impact are rarely directly logged. We show how
management practices and network health can be inferred from
other data, including inventory records, snapshots of device con-
ﬁgurations, and trouble ticket logs. This data is indirect and noisy,
but useful information can be extracted from it.
The second challenge is dealing with a limited amount of data.
For any given network in an organization (e.g., a data center net-
work hosting a certain Web service) the number of available snap-
shots of its design and operation may be limited, and some snap-
shots may be missing due to incomplete or inconsistent logging.
Our key insight is to identify causal relationships and build predic-
tive models by aggregating data from many networks and many
months. This aggregation eliminates noise from individual net-
works and provides a broader picture of an organizations’ practices.
Finally, we show that skew in network operations data makes
it challenging to identify causal relationships and build accurate
predictive models. Common approaches for decomposing depen-
dencies (e.g., ANOVA [3] or ICA [9]) are unsuitable, because rela-
tionships between network health and management practices may
be non-monotonic. Similarly, quasi-experimental designs (QEDs)
that rely on exact matching to eliminate the effects of confounding
practices [21] do not work, because many management practices
are strongly related. Furthermore, predictive models constructed
using standard learning algorithms (e.g., decisions trees built using
C4.5 [27]) inaccurately predict situations that lead to poor network
health due to the presence of substantially more “healthy network”
cases in the data.
We use three main techniques to overcome this skew. First,
we use mutual information to uncover statistical dependencies be-
tween network health and management practices. Mutual informa-
tion quantiﬁes the extent to which knowing a management prac-
tice reduces uncertainty about network health. Second, we use
QEDs based on propensity score matching [33] to discover causal
relationships between management practices and network health.
Propensity scores systematically account for the bias in treatment
selection caused by confounding practices. Finally, when learning
predictive models, we use oversampling and boosting [12] to im-
prove accuracy for the minority (i.e., “unhealthy network”) cases.
Key ﬁndings from applying our methods to several hundred net-
works of a large online service provider (OSP) are:
• There is signiﬁcant variation in management practices across
the networks, even though they are all managed based on the
same set of recommended guidelines. For instance, we ﬁnd
networks differ substantially with respect to hardware and
ﬁrmware heterogeneity, the extent of automation used, and
the way changes are made to them.
• By applying MPA to the OSP’s data, we determine that num-
ber of devices, number of change events, and number of
change types have a strong statistical dependence and causal
relationship with network health (quantiﬁed using number of
tickets). While in some cases our causal analysis agrees with
operator feedback (e.g., number of change events has high
impact), in other cases it contradicts them (e.g., the fraction
of changes where an ACL is modiﬁed has moderately high
impact, despite a majority opinion that its impact is low).
• Decision trees for predicting two coarse-grained health classes
have 91% (cross-validation) accuracy, whereas those for ﬁve
ﬁne-grained classes have 81% accuracy. While our enhance-
ments improve multi-class accuracy signiﬁcantly, ﬁne-grained
predictions are still suboptimal due to a lack of sufﬁcient
data. When applied in an online fashion, our 2-class (5-class)
model can predict network health with 89% (76%) accuracy.
Our work is a step toward designing a better management plane
that reduces the burden on operators and reduces the frequency of
failures. Although the observations we make for the OSP’s net-
works may not apply to all networks—due to differences between
organizations and types of networks (e.g., data centers vs. wide
area networks)—our publicly available MPA framework [2] can be
applied to any set of networks to identify the extent to which par-
ticular management practices impact the health of those networks.
Now is a particularly relevant time for this undertaking because,
in the form of SDN, the community is engaged in re-architecting
networks. By providing operators a detailed understanding of the
strengths and weaknesses of their current management plane, our
work can help inform the design of the next generation manage-
ment plane.
2.
INFERRING MANAGEMENT
PRACTICES
Our goal is to design a framework that an organization that oper-
ates a collection of networks (e.g., a university campus, or an online
service provider) can use to understand and improve its manage-
ment plane. Organizations that manage a large number of devices
typically do not view all devices as belonging to one network, but
instead view the devices as partitioned across multiple networks.
A network in this context is a collection of devices that either con-
nects compute equipment that hosts speciﬁc workloads or connects
other networks to each other or the external world. A workload is
a service (e.g., a ﬁle system, or an application) or a group of users
(e.g., students using PCs in a department).
A challenge in designing our desired framework is that manage-
ment practices are not explicitly logged. While the control and data
planes can be queried to quantify their behavior [10, 16, 24, 31, 35],
no such capability exists for the management plane. This gap stems
from humans being the primary actors in the management plane.
Operators translate high-level intents into a suitable setup of de-
vices, protocols, and conﬁgurations to create a functional, healthy
network. Even when recommended procedures are documented,
there is no guarantee that operators adhere to these practices.
Fortunately, we are able to infer management practices from
other readily available data sources. In this section, we describe
these sources and the management practice metrics we can infer.
2.1 Data Sources
We can infer management practices and network health from
three data sources that are commonly available. Such data sources
have already been used in prior work, albeit to study a limited set of
management practices [6, 20, 26, 34]. We build upon these efforts
to provide a more thorough view of management practices and their
relationship to network health. The data sources are:
1) Inventory records. Most organizations directly track the set
of networks they manage, and the role the networks play. They
also record the vendor, model, location, and role (switch, router,
load balancer, etc.) of every device in their deployment, and the
network it belongs to. This data can be used to infer a network’s
basic composition and purpose.
2) Device conﬁguration snapshots. Network management sys-
tems (NMS) track changes in device conﬁgurations to aid network
operators in a variety tasks, such as debugging conﬁguration errors
or rolling back changes when problems emerge. NMSes such as
RANCID [28] and HPNA [36] subscribe to syslog feeds from net-
work devices and snapshot a device’s conﬁguration whenever the
396Design practices
D1. Number of services, users, or networks connected
D2. Number of devices, vendors, models, roles (e.g.,
switch, router, ﬁrewall), and ﬁrmware versions
D3. Hardware and ﬁrmware heterogeneity
D4. Number of data plane constructs used (e.g., VLAN
spanning tree, link aggregation), and instance counts
D5. Number and size of BGP & OSPF routing instances
D6. Intra- and inter-device conﬁg reference counts
Operational practices
O1. Number of conﬁg changes and devices changed
O2. Number of automated changes
O3. Number and modality of changes of speciﬁc types
(e.g., interface, ACL, router, VLAN)
O4. Number of devices changed together
Table 1: Management practice metrics
device generates a syslog alert that its conﬁguration has changed.
Each snapshot includes the conﬁguration text, as well as metadata
about the change, e.g., when it occurred and the login information
of the entity (i.e., user or script) that made the change. The snap-
shots are archived in a database or version control system.
3) Trouble ticket logs. When users report network problems, or
monitoring systems raise alarms, a trouble ticket is created in an in-
cident management system. The ticket is used to track the duration,
symptoms, and diagnosis of the problem. Each ticket has a mix of
structured and unstructured information. The former includes the
time the problem was discovered and resolved, the name(s) of de-
vice(s) causing or effected by the problem, and symptoms or res-
olutions selected from pre-deﬁned lists; the latter includes syslog
details and communication (e.g., emails and IMs) between opera-
tors that occurred to diagnose the issue.
2.2 Metrics
Using these data sources, we can infer management practices and
network health, and model them using metrics. We broadly classify
management practices into two classes (Table 1): design practices
are long-term decisions concerning the network’s structure and pro-
visioning (e.g., selecting how many switches and from which ven-
dors); operational practices are day-to-day activities that change
the network in response to emerging needs (e.g., adding subnets).
Design Practices. Design practices inﬂuence four sets of network
artifacts: the network’s purpose, its physical composition, and the
logical structure and composition of its data and control planes.
The metrics we use to quantitatively describe a network’s purpose
and its physical composition are rather straightforward to compute,
and are listed in lines D1 and D2 in Table 1. We synthesize these
metrics to measure a network’s hardware heterogeneity using a nor-
− Pi,j pij log2pij
log2N
, where pij is
malized entropy metric (line D3):
the fraction of devices of model i that play role j (e.g., switch,
router, ﬁrewall, load balancer) in the network, and N is the size
of the network. This metric captures the extent to which the same
hardware model is used in multiple roles, or multiple models are
used in the same role; a value close to 1 indicates signiﬁcant het-
erogeneity. We compute a similar ﬁrmware heterogeneity metric.
Computing metrics that capture the logical composition and struc-
ture of the data and control planes is more intricate as it involves
parsing conﬁguration ﬁles. To conduct our study, we extended Bat-
ﬁsh [11] to parse the conﬁguration languages of various device
vendors (e.g., Cisco IoS). Given parsed conﬁgurations, we deter-
mine the logical composition of the data plane by enumerating the
number of logical data plane constructs used (e.g., spanning tree,
VLAN, link aggregation), as well as the number of instances of
each (e.g., number of VLANs conﬁgured); Table 1, line D4.
To model control plane structure, we leverage prior work on con-
ﬁguration models [5]. In particular, we extract routing instances
from device conﬁgurations, where each instance is a collection of
routing processes of the same type (e.g., OSPF processes) on dif-
ferent devices that are in the transitive closure of the “adjacent-to”
relationship. A network’s routing instances collectively implement
its control plane. We enumerate the number of such instances, as
well as the average size of each instance (Table 1, line D5) using
the same methodology as Benson et al. [5].
Finally, we enumerate the average number of inter- and intra-
device conﬁguration references in a network [5]. These metrics
(Table 1, line D6) capture the conﬁguration complexity imposed in
aggregate by all aspects of a network’s design, as well as the impact
of speciﬁc conﬁguration practices followed by operators.
Operational Practices. We infer operational practices by com-
paring two successive conﬁguration snapshots from the same de-
vice. If at least one stanza differs, we count this as a conﬁguration
change.
We compute basic statistics about the conﬁguration changes ob-
served over a certain time window (Table 1, line O1). In addition,
we study the modality of changes (line O2). We infer modality
(automated vs. manual) using the login metadata stored with con-
ﬁguration snapshots: we mark a change as automated if the login
is classiﬁed as a special account in the organization’s user manage-
ment system. Otherwise we assume the change was manual. This
conservative approach will misclassify changes made by scripts ex-
ecuting under a regular user account, thereby under-estimating the
extent of automated changes.
To model change type, we leverage the fact that conﬁguration
information is arranged as stanzas, each containing a set of options
and values pertaining to a particular construct—e.g., a speciﬁc in-
terface, VLAN, routing instance, or ACL. A stanza is identiﬁed by
a type (e.g., interface) and a name (e.g., TenGigabit0/1). When part
(or all) of a stanza is added, removed, or updated, we say a change
of type T occurred, where T is the stanza type. We count the num-
ber of changes of each type over a certain time window (Table 1,
line O3).
There are a few challenges and limitations with this approach.
First, type names differ between vendors: e.g., an ACL is deﬁned in
Cisco IoS using an ip access-list stanza, while a firewall
filter stanza is used in Juniper JunOS. We address this by man-
ually identifying stanza types on different vendors that serve the
same purpose, and we convert these to a vendor-agnostic type iden-
tiﬁer. Second, even after generalizing types, a change with the same
effect may be typiﬁed differently on different vendors: e.g., an in-
terface is assigned to a VLAN in Cisco IoS using the switchport
access vlan option within an interface stanza, while in Ju-
niper JunOS the interface option is used within a vlan stanza;
even though the effect of the change is the same, it will be typiﬁed
as an interface change on a Cisco device and a VLAN change on
a Juniper device. Operators using MPA should be aware of this
limitation and interpret prediction results according to the mix of
vendors in their networks.
In addition to computing change metrics over changes on in-
dividual devices, we compute change metrics over change events
(Table 1, line O4). Change events account for the fact that mul-
tiple devices’ conﬁgurations may need to be changed to realize a
desired outcome. For example, establishing a new layer-2 network
segment (e.g., a VLAN) requires conﬁguration changes to all de-
vices participating in the segment.
397s
e
s
n
o
p
s
e
R
f
o
#
35
30
25
20
15
10
5
0
No impact
Low impact
Medium impact
High impact
Not sure
No. of
devices
No. of
models
No. of
No. of
firmware versions
protocols
Inter−device
complexity
No. of change
events
Avg. devices
changed/event
Frac. events w/
mbox change
Frac. events
automated
Frac. events
Frac. events w/
w/ router change
ACL change
Figure 2: Results of operator survey
O(10)
e
g
n
a
h
C
f
o
#
s
t
n
e
v
E
0
NA 1
2
5
10
15
30
Window Size (minutes)
Figure 3: Impact of change grouping threshold (δ) on the num-
ber of change events; each box shows the 25th, 50th, and 75th
percentile number of change events per-network per-month us-
ing various values of δ; whiskers indicate the most extreme dat-
apoints within twice the interquartile range
To identify change events, we group changes using a simple
if a conﬁguration change on a device occurs within δ
heuristic:
time units of a change on another device in the same network, then
we assume the changes on both devices are part of the same change
event. Figure 3 shows how different values of δ inﬂuence the num-
ber of change events. The rest of our analysis uses δ = 5 minutes,
because operators indicated they complete most related changes
within such a time window.
In the future, we plan to also con-
sider the change type and affected entities (e.g., VLAN or subnet)
to more ﬁnely group related changes.
Network Health. The health of a network can be analyzed from
many perspectives, including performance (e.g., latency or through-
put), quality of experience (e.g., application responsiveness), and
failure rate (e.g., packet loss or link/device downtime). Networks
are often equipped with monitoring systems that track these met-
rics and raise alarms when critical thresholds are crossed. In the
networks we study, trouble tickets are automatically created when
such alarms are raised. Tickets are also created when users report