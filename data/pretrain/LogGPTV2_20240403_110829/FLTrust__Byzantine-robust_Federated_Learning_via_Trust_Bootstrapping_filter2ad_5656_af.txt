LF attack
Krum attack
Trim attack
Scaling attack 0.03 / 0.00 0.03 / 0.01 0.04 / 0.00 0.04 / 0.00 0.06 / 0.01 0.42 / 0.01
Adaptive attack
0.04
0.05
0.08
0.12
0.90
0.90
(a) MNIST-0.1
0.2
0.04
0.04
0.04
0.05
0.4
0.04
0.04
0.07
0.08
(b) MNIST-0.5
0.2
0.05
0.05
0.05
0.06
0.4
0.06
0.08
0.08
0.08
Bias probability
No attack
LF attack
Krum attack
Trim attack
Scaling attack 0.05 / 0.00 0.05 / 0.01 0.06 / 0.00 0.07 / 0.01 0.12 / 0.00 0.86 / 0.01
Adaptive attack
0.06
0.07
0.08
0.13
0.90
0.90
(c) Fashion-MNIST
Bias probability
No attack
LF attack
Krum attack
Trim attack
Scaling attack 0.11 / 0.02 0.12 / 0.04 0.12 / 0.04 0.13 / 0.02 0.15 / 0.03 0.90 / 0.00
Adaptive attack
0.14
0.14
0.16
0.90
0.90
0.90
0.2
0.11
0.11
0.12
0.14
0.4
0.12
0.12
0.16
0.15
Bias probability
(d) CIFAR-10
0.2
0.18
0.19
0.18
0.20
0.4
0.18
0.20
0.19
0.24
No attack
LF attack
Krum attack
Trim attack
Scaling attack 0.18 / 0.02 0.18 / 0.00 0.18 / 0.03 0.22 / 0.04 0.90 / 0.00 0.90 / 0.00
Adaptive attack
0.20
0.20
0.27
0.68
0.90
0.90
Bias probability
No attack
LF attack
Krum attack
Trim attack
Scaling attack 0.05 / 0.01 0.05 / 0.01 0.06 / 0.02 0.06 / 0.03 0.07 / 0.05 0.48 / 0.34
Adaptive attack
0.05
0.05
0.06
0.09
0.48
0.48
(e) HAR
0.4
0.06
0.06
0.05
0.06
0.2
0.04
0.05
0.05
0.05
Bias probability
0.125
0.10
0.12
0.12
0.13
(f) CH-MNIST
0.2
0.10
0.12
0.12
0.13
0.4
0.11
0.12
0.14
0.14
No attack
LF attack
Krum attack
Trim attack
Scaling attack 0.14 / 0.03 0.14 / 0.02 0.15 / 0.02 0.16 / 0.06 0.14 / 0.01 0.89 / 0.01
Adaptive attack
0.13
0.14
0.14
0.89
0.89
0.89
0.6
0.05
0.05
0.89
0.12
0.6
0.08
0.10
0.12
0.12
0.6
0.15
0.12
0.90
0.21
0.6
0.21
0.24
0.33
0.63
0.6
0.06
0.05
0.05
0.09
0.6
0.13
0.17
0.17
0.20
0.8
0.05
0.78
0.89
0.46
0.8
0.11
0.25
0.86
0.16
0.8
0.16
0.14
0.90
0.90
0.8
0.90
0.90
0.90
0.90
0.8
0.07
0.07
0.09
0.14
0.8
0.13
0.21
0.19
0.20
1.0
0.34
0.84
0.89
0.89
1.0
0.80
0.89
0.89
0.89
1.0
0.90
0.90
0.90
0.90
1.0
0.90
0.90
0.90
0.90
1.0
0.48
0.48
0.48
0.48
1.0
0.89
0.89
0.89
0.89
Speciﬁcally, when the root dataset has 100 training examples,
the testing error rates of FLTrust under attacks are similar to
that of FedAvg without attacks, and the attack success rate
of the Scaling attack is close to 0. When the size of the root
dataset increases beyond 100, the testing error rates and attack
success rates of FLTrust further decrease slightly.
We also evaluate the impact of the bias probability in Case
II. Table V shows the testing error rates of FLTrust under
different attacks and the attack success rates of the Scaling
13
(a) LF attack
(b) Krum attack
(c) Trim attack
(d) Scaling attack
(e) Adaptive attack
Fig. 5: Impact of the total number of clients on the testing
error rates of different FL methods under different attacks ((a)-
(c)) and the attack success rates of the Scaling attacks, where
MNIST-0.5 is used. The testing error rates of all the compared
FL methods are similar and small under the Scaling attacks,
which we omit for simplicity.
attacks when the bias probability varies. The second column in
each table corresponds to the bias probability with which Case
II reduces to Case I. We increase the bias probability up to 1.0
to simulate larger difference between the root data distribution
and the overall training data distribution. We observe that
FLTrust is accurate and robust when the bias probability is
not too large. For instance, when the bias probability is no
more than 0.4 for MNIST-0.5, the testing error rates of FLTrust
under attacks are at most 0.08, compared to 0.05 when the bias
probability is 0.1. Our results show that FLTrust works well
when the root data distribution does not diverge too much from
the overall training data distribution.
Impact of the total number of clients: Figure 5 shows
the testing error rates of different FL methods under different
attacks, as well as the attack success rates of the Scaling
attacks, when the total number of clients n increases from 50 to
n = 20%.
400. We set the fraction of malicious clients to be m
We observe that FLTrust can defend against the attacks for all
considered total number of clients. Speciﬁcally, FLTrust under
attacks achieves testing error rates similar to FedAvg under no
attacks, while the attack success rates of the Scaling attacks
are close to 0 for FLTrust. Existing methods can defend against
the Scaling attacks on MNIST-0.5, i.e., the attack success
rates are close to 0. However, they cannot defend against the
Krum attack, Trim attack, and/or our adaptive attack, i.e., their
corresponding testing error rates are large.
50100200300400Number of clients0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust50100200300400Number of clients0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust50100200300400Number of clients0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust50100200300400Number of clients0.00.20.40.60.81.0Attack success rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust50100200300400Number of clients0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust(a) LF attack
(b) Krum attack
(a) MNIST-0.1
(b) MNIST-0.5
(c) Trim attack
(d) Scaling attack
(c) Fashion-MNIST
(d) CIFAR-10
(e) Adaptive attack
(e) HAR
(f) CH-MNIST
Fig. 6: Impact of the fraction of malicious clients on the testing
error rates of different FL methods under different attacks ((a)-
(c)) and the attack success rates of the Scaling attacks, where
MNIST-0.5 is used. The testing error rates of all the compared
FL methods are similar and small under the Scaling attacks,
which we omit for simplicity.
Impact of the number of malicious clients: Figure 6 shows
the testing error rates of different FL methods under different
attacks and the attack success rates of the Scaling attacks on
MNIST-0.5, when the fraction of malicious clients increases
from 0 to 95%. Trim-mean cannot be applied when the fraction
of malicious clients exceeds 50% because the number of local
model updates removed by Trim-mean is twice of the number
of malicious clients. Therefore, for Trim-mean, we only show
the results when the malicious clients are less than 50%.
We observe that, under existing attacks and our adaptive
attacks, FLTrust can tolerate up to 90% of malicious clients.
Speciﬁcally, FLTrust under these attacks still achieves testing
error rates similar to FedAvg without attacks when up to 90%
of the clients are malicious, while the attack success rates of
the Scaling attacks for FLTrust are still close to 0 when up to
95% of the clients are malicious. However, existing Byzantine-
robust FL methods can tolerate much less malicious clients.
For instance, under Krum attack, the testing error rate of the
global model learnt by Krum increases to 0.90 when only 10%
of the clients are malicious, while the testing error rates of the
global models learnt by Trim-mean and Median become larger
than 0.85 when the fraction of malicious clients reaches 40%.
Figure 7 further shows the testing error rates of the global
models learnt by FLTrust as a function of the fraction of
malicious clients under the adaptive attacks on all datasets.
Fig. 7: Impact of the fraction of malicious clients on the testing
error rates of FLTrust under the adaptive attacks.
Our results show that FLTrust
is robust against adaptive
attacks even if a large fraction of clients are malicious on
all datasets. Speciﬁcally, for MNIST-0.1 (MNIST-0.5, Fashion-
MNIST, CIFAR-10, HAR, or CH-MNIST), FLTrust under
adaptive attacks with over 60% (over 40%, up to 60%, up
to 60%, up to 40%, or over 40%) of malicious clients can still
achieve testing error rates similar to FedAvg under no attack.
VII. DISCUSSION AND LIMITATIONS
FLTrust vs. fault-tolerant computing: Fault-tolerant com-
puting [6] aims to remain functional when there are malicious
clients. However, conventional fault-tolerant computing and
federated learning have the following key difference:
the
clients communicate with each other to compute results in
fault-tolerant computing [6], while clients only communi-
cate with a cloud server in federated learning. Our FLTrust
leverages such unique characteristics of federated learning to
bootstrap trust, i.e., the server collects a root dataset, and uses
it to guide the aggregation of the local model updates.
Different ways of using the root dataset: Fang et al. [15]
also proposed to use a root dataset (they called it validation
dataset). However, we use the root dataset in a way that is
different from theirs. In particular, they use the root dataset
to remove potentially malicious local model updates in each
iteration, while we use it to assign trust scores to clients and
normalize local model updates. As shown by Fang et al. [15],
their way of using the root dataset is not effective in many
cases.
14
010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Attack success rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksKrumTrim-meanMedianFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksFLTrust010204060809095Fraction of malicious clients (%)0.00.20.40.60.81.0Testing error rateFedAvg w/o attacksFLTrustPoisoned root dataset: Our FLTrust requires a clean root
dataset. We acknowledge that FLTrust may not be robust
against poisoned root dataset. The root dataset may be poi-
soned when it is collected from the Internet or by an insider
attacker. However, since FLTrust only requires a small root