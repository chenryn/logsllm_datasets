







Fig. 3. OBDD for G, variable order: e0 < e1 < e2 < e3 < e4
terminal nodes are marked by an asterisk. The ﬁrst/second
there can be six
partition has one/two block(s). In fact,
∗
∗.
different partitions: [0 1], [0 1]
[1], [0][1]
, [0][1], [0]
3[0 1]∗/[0]∗[1] is the high/low child of OBDD-root node G.
, [0]
[1]
∗
∗
∗
529
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:01:15 UTC from IEEE Xplore.  Restrictions apply. 
∗
In contrast to the factoring algorithm, the reliability subgraphs
of G are now represented by partitions. This comes along with
great advantages with regard to space and time requirements.
The total number of partitions in the k-th level depends on the
cardinality of Fk. In particular, it grows exponentially with the
size of Fk (see [11]).
∗ is [2 3]
Any partition that has no marked blocks is failed which
means that at least one of the terminals is disconnected from
the frontier set: in the example, the low child of [0]
[1] is [1][2]
and hence false.
Recognizing reliability isomorphic subgraphs4 is further
simpliﬁed by representing reliability subgraphs as partitions:
two subgraphs G1 and G2 are reliability isomorphic if their
associated partitions are identical with regard to the same edge
ordering. In the third level of the depicted OBDD, partition
∗ is recognized three times. This recognition avoids
[1 2]
redundant computations and contributes to a higher efﬁciency
of the algorithm. If all terminals are connected in one block
of a partition, a working conﬁguration is found: the high child
∗ and hence we have to link to the true-
of [2]
leaf. In total, the result OBDD consist of 10 nodes and only
six partitions are held in memory at a time. By dropping the
two redundant nodes which have the same low and high child,
the size of this OBDD shrinks to 8. The maximal width of the
OBDD depends on |Fmax|. Here, |Fmax| = 2 and the maximal
width is three. Consequently, the memory requirements of
the algorithm depends on the size of Fmax. In particular,
the time complexity of this approach highly depends on
|Fmax|. According to Hardy et al. [11], the complexity for the
(cid:6)
two-terminal reliability equals O(cid:5)|E| · |Fmax|3 · B|Fmax|
(cid:6)
the complexity is O(cid:5)|E| · |Fmax| · B|Fmax|
5.
O(cid:5)|E| · |Fmax| · B|Fmax| · 2
and
for the all-terminal and
k-terminal case respectively. So this approach allows for
computing the reliability of networks with bounded |Fmax|
in time linear to the number of edges.
|Fmax|(cid:6)
Notably,
[3]
∗
∗
a) Some more facts about the approach: The decompo-
sition approach is given as pseudo code in Procedure 1. Unlike
Hardy, we have omitted the partition-number-transforming
functions [11] since the overhead for transforming partitions
into numbers and numbers to partitions becomes a serious
issue for large |Fk|. Instead, the partitions are directly hashed
according to the conventions proposed by Hermann [24]: for
∗ can be hashed by two vectors.
example, partition [1 2][3]
The ﬁrst vector part = [1, 1, 2, 3] states that nodes 1, 2 are
in the ﬁrst block, node 3 in the second and node 4 in the
third block. The second vector consists of Boolean entries
b = [0, 1, 1] recording the block marking.
So each partition node pk is attached with an integer vector
and a Boolean vector. In addition to that, the appropriate
BDD node which can be retrieved by the function getbdd(), is
stored. After having determined an ordering by bfs, the initial
step in Procedure 1 is to link the root node of the BDD to
[4]
4In general, two subgraphs are reliability isomorphic if they represent the
same Boolean function w.r.t. a stipulated reliability measure.
5B|F| is known as the Bell number which grows exponentially with the
size of F [11].
two new child nodes p0 and p1. They are put into the list
prevLevel in order to be further processed. The BDD emerges
level by level whereas at each level k boundary set Fk has to
be updated. The workloads or partition nodes are processed
until prevLevel is empty. The number of workloads held in
prevLevel corresponds to the width of level k in the BDD. By
processing the partition nodes from prevLevel, new partition
nodes are created and added to the nextLevel. At most, there
are two levels of partition nodes held in memory at once.
We have found that Hardy’s approach lacks the possibility
of creating a BDD false-leaf when merging an edge during
factoring. This fact was also revealed by [24] and is considered
in lines 12-14 of Procedure 1.
The derivation of new partitions part1 by merging and part0
by deleting the edge variable from pk, is not an obvious task.
Hence, we refer to Section III-A for details.
Procedure 1 BDDP artition
Require: Graph G = (V, E), set of terminal nodes K
1: Determine an edge ordering by bfs heuristic
2: Initialize F1 = {u, v} with e = (u, v) being the ﬁrst edge
in the variable order, initialize root node BDDroot
3: root.high = p1.getbdd(), root.low = p0.getbdd()
4: Add the ﬁrst two partitions p0, p1 to prevLevel
5: for k = 2 to |E| do
6:
7:
8:
9:
10:
compute Fk
while prevLevel (cid:2)= ∅ do
pk = prevLevel.pop()
Derive part1 from pk
if all K-vertices are merged into the same block in part1
then
pk.getbdd().high = true
else if one K-vertex is disconnected in part1 then
//This case is missing in Hardy’s approach
pk.getbdd().high = false
else
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34: end for
35: return root
else
if part1 is not in hash table then
nextLevel.add(part1)
insert part1 in hash table
end if
pk.getbdd().high = part1.getbdd()
end if
Derive part0 from pk
if one K-vertex is disconnected in part0 then
pk.getbdd().low = false
if part0 is not in hash table then
nextLevel.add(part0)
insert part0 in hash table
end if
pk.getbdd().low = part0.getbdd()
end if
end while
prevLevel = nextLevel, nextLevel.clear()
2) Sorting of blocks: Hardy’s approach lacks the descrip-
tion of distinctly arranging the blocks within the partitions.
This is vital for correctly assigning and identifying the block
numbers. Hence, we brieﬂy propose a distinct arrangement
scheme at this point: ﬁrst, the blocks are sorted with respect
530
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:01:15 UTC from IEEE Xplore.  Restrictions apply. 
to their cardinality. Subsequently, the blocks with the same
cardinality are sorted according to the lowest vertex number.
Since the vertex numbers are unique and appear only once
in some block, we obtain an unambiguous block arrange-
ment convention. Suppose, we have the following partition
p = [5][3 4][0 1 2][2 8][6],
then after sorting we obtain
p = [5][6][2 8][3 4][0 1 2].
III. IMPROVING THE DECOMPOSITION APPROACH
We have outlined in Section II-C that the complexity of the
decomposition approach notably depends on the size of the
maximal boundary set Fmax. Since |Fmax| results from the
process of variable ordering, we must focus on conceiving
variable ordering techniques which preferably lead to low
|Fmax|. Finding a graph decomposition which leads to the
smallest |Fmax| corresponds to the path- or treewidth problem,
also called partial k-tree problem [25]: given a graph G,
ﬁnd a tree decomposition for G of minimal treewidth. This
problem is motivated by the fact that once a tree decomposition
for a graph G with bounded treewidth k ∈ N has been
determined, many NP-hard problems for general graphs G can
be solved in time linear to the size of G but exponential in
k or |Fmax|. Unfortunately, ﬁnding a tree decomposition with
smallest treewidth is an NP-complete problem [25]. A lot of
research has been done on ﬁnding tree-decompositions with
minimal treewidth. A noteworthy result is due to Bodlaender
[26]: for ﬁxed k, he found a linear time algorithm which
decides whether a graph G has bounded treewidth of at most
k, and if so, it ﬁnds a tree-decomposition of G with treewidth
at most k. However, the constant factor k of the algorithm is
very large - much too large for practical purposes. In addition,
the algorithm in its present form is probably not practical.
Thus, Carlier, Hardy and Hermann [19], [11], [24] suggested
the bfs-ordering as a good alternative variable ordering for
the decomposition method. Apart from this, Lucet proposed
in her PhD thesis [27] heuristics such as simulated annealing
or randomization for obtaining low |Fmax| values. However,
these heuristics have not yet been shown to deliver better
results than the bfs heuristic. In this section, we propose
another heuristic which yields lower |Fmax| values for a large
worst case complexity of O(cid:5)|V | · |E| · |Fmax| · Nmax
variety of different graph structures. The new heuristic has a
, where
N (Fk) := {v /∈ Fk : (u, v) ∈ Ek, u ∈ Fk}6 is the set of
nodes adjacent to frontier set Fk and Nmax := maxk |N (Fk)|
is the maximal neighborhood among all frontier sets. Since
a ROBDD is canonical in terms of a Boolean term, we can
expect improvements for other BDD-based approaches, such
as the KLY method, if the new heuristic yields an OBDD with
signiﬁcantly smaller size.
Indeed, the experiments in Section IV show that the new
heuristic ﬁnds for many example networks - especially irreg-
ular networks - edge orderings with much lower |Fmax| than
the currently used bfs-heuristic. To get a better understanding
of the evolvement of the new heuristic, we ﬁrst explain
(cid:6)
the derivation of new partitions during the decomposition
procedure. This non-trivial derivation was omitted by [11].
At the end of this section the new heuristic is compared with
the current bfs-heuristic by means of three sample networks.
A. Derivation of new partitions
Based on an available partition,
two new partitions are
derived by edge-contraction or edge-deletion.
Let e1 < e2 < . . . < em be an ordering of edges for a graph
G. We say that this edge ordering has a connected property
if for each k ∈ {1, 2, . . . , m}, the edge set Ek is connected.
Since the bfs and the new heuristic have an edge ordering of
connected property, we obtain at most one new node which
is added to the previous boundary set Fk−1 at each factoring
step. So we are confronted with two cases when factoring on
edge e := (n1, n2): Case I. where a new node n2 ∈ Fk is
added to Fk−1 and Case II. where no new nodes are added
to Fk−1. For Case I., there are two sub-cases and for case II.
there are four sub-cases to distinguish. They are listed in Table
I. For example, Sub-case a) means that n1 leaves the previous
frontier set Fk−1 in the next iteration k.
The four sub-cases a) to d) lead to ten elementary cases for
each contraction and deletion operation (see Tables II, III, IV,
V). For Tables III, V we have omitted Sub-case d) which is
analog to a) when interchanging the roles of n1 and n2.
Starting with the edge-contraction accompanied by Case I.a,
we have to delete node n1 from the block of the current
partition. The block wherein n1 is contained is named b(n1)7.
Subsequently, n2 is added to b(n1) which may cause a change
in the block order. To ﬁnally obtain the new partition, the block
is, if required, rearranged inside the current partition according
to the sorting deﬁned in Section II-C2. The proceeding for
Case I.b is similar, except that the blocksize of b(n1) increases
by one. Consequently, the position of b(n1) either remains or
is shifted up within its current partition.
Facing Case II., we have to distinguish whether n1, n2 are in
the same block or in different blocks. For the latter case, the
different blocks are merged into one block which is then placed
into the current partition according to the deﬁned sorting. By
deleting nodes n1 or n2 from the blocks, we must account for
the case that the respective blocks may be empty - or decay
- after node removal. In this case, we do not need to merge
the blocks (”possibly” merge blocks for II.a and II.c). we note
that if a decaying block is a marked block, then the child of
the current BDD-partition-node must be a BDD false-leaf. For
the deletion operation with regard to Case I. (see Table IV),
we have to create a new block for node n2 and appropriately
place the new block into the current partition. Otherwise, all
other partition manipulating operations are similar to those for
contraction.
B. A novel heuristic for ﬁnding a good variable order
In order to obtain a substantial improvement of the de-
composition approach, a better heuristic than the currently
6Ek is the complementary edge set in the k-th level (Section II-C).
7A block containing both n1 and n2 is declared as b(n1,2).
531
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:01:15 UTC from IEEE Xplore.  Restrictions apply. 
CASE-BY-CASE ANALYSIS FOR DEDUCTION OF NEW PARTITIONS
TABLE I
a) n1 /∈ Fk, n2 ∈ Fk
b) n1, n2 ∈ Fk
c) n1, n2 /∈ Fk
d) n1 ∈ Fk, n2 /∈ Fk
I. New frontier node n2:
n1 ∈ Fk−1, n2 ∈ Fk
II. No new frontier node:
n1, n2 ∈ Fk−1
relevant
relevant
relevant
relevant
relevant
relevant
irrelevant
irrelevant
TABLE II
CONTRACT I.
a)
b)
delete n1, add n2 to b(n1) & rearrange b(n1)
add n2 to b(n1) & rearrange b(n1)
TABLE III
CONTRACT II.
n1, n2 are in the same block
delete n1 & check if b(n1) decays,
sort blocks
return current partition
n1, n2 are in different blocks
delete n1 & check if b(n1) decays,
possibly merge blocks & sort blocks
merge blocks & sort blocks
delete n1, n2 & check if b(n1,2) decays,
delete n1, n2 & check if b(n1) or b(n2) decay,
sort blocks
possibly merge blocks & sort blocks
TABLE IV
DELETE I.
a)
b)
delete n1 & check if b(n1) decays & create new block b(n2) & sort blocks
create new block b(n2) & sort blocks
TABLE V
DELETE II.
n1, n2 are in the same block
delete n1 & check if b(n1) decays,
sort blocks
return current partition
n1, n2 are in different blocks
delete n1 & check if b(n1) decays,
sort blocks
return current partition
delete n1, n2 & check if b(n1,2) decays,