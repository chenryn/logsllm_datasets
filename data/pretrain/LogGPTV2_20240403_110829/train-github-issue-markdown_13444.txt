### Describe the workflow you want to enable
Currently, there is no way to derive the _Sensitivity_ or the **Specificity**
from the confusions matrics  
The only way to be calculated is to do it yourself. this can be simple if you
are doing a simple binary classification like this one on the image.  
![Screen Shot 2021-11-07 at 3 08 21 PM](https://user-
images.githubusercontent.com/72753578/140646253-1db308b0-c4fc-44fc-9969-e1feb225d57d.png)
But if you are dealing with multiple categories more than 2 this can be
slightly complicated and it can take time!
Also, I think that calculating Sensitivity and Specificity will help analyze
our model much much better
![Screen Shot 2021-11-07 at 3 14 06 PM](https://user-
images.githubusercontent.com/72753578/140646452-8c0a628b-2f16-49fc-86e0-7150c4e7e1b5.png)
.
### Describe your proposed solution
Add 2 new functions to `sklearn.metrics` that accept the confusion matrix and
then return sensitive or specificity  
Should be something like this
    from sklearn.metrics import sensitive_score, specificity_score, confusion_matrix
    conf = confusion_matrix(y_test, y_pred)
    specificity = specificity_score(conf)
    sensitive = sensitive_score(conf)
We can do even better by make it more intuitively to use
    # it will caculate the confusion matrix internally !
    specificity = specificity_score(y_test, y_pred)
    sensitive = sensitive_score(y_test, y_pred)
### Describe alternatives you've considered, if relevant
I think this feature will be `good to have` one.  
I can implement it myself!! and open a new pull request