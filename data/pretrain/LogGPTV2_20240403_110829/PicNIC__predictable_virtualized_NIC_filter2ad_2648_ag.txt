[67] Ahmed Saeed, Nandita Dukkipati, Valas Valancius, Terry Lam, Carlo Contavalli,
and Amin Vahdat. 2017. Carousel: Scalable Traffic Shaping at End-Hosts. In
SIGCOMM. ACM, Los Angeles, CA, 404–417.
[68] SDNCentral. 2014.
Brocade Vyatta 5600 vRouter: Performance Valida-
tion. https://networkbuilders.intel.com/docs/Vyatta_5600_Performance_Test_
Full_Report.pdf Online, accessed: 2019-07.
[69] Alan Shieh, Srikanth Kandula, Albert G Greenberg, Changhoon Kim, and Bikas
Saha. 2011. Sharing the Data Center Network. In NSDI. USENIX Association,
Boston, MA, 309–322.
[70] Anirudh Sivaraman, Suvinay Subramanian, Mohammad Alizadeh, Sharad Chole,
Shang-Tse Chuang, Anurag Agrawal, Hari Balakrishnan, Tom Edsall, Sachin
Katti, and Nick McKeown. 2016. Programmable Packet Scheduling at Line Rate.
In SIGCOMM. ACM, Florianópolis, Brazil, 44–57.
[71] Brent Stephens, Aditya Akella, and Michael Swift. 2019. Loom: Flexible and
Efficient NIC Packet Scheduling. In NSDI. USENIX Association, Boston, MA,
33–46.
[72] Brent Stephens, Aditya Akella, and Michael M Swift. 2018. Your Programmable
NIC Should be a Programmable Switch. In HotNets. ACM, Redmond, WA, 36–42.
[73] Tolly. 2016. Mellanox Spectrum vs. Broadcom StrataXGS Tomahawk: 25GbE
and 100GbE Performance Evaluation – Evaluating Consistency & Predictabil-
ity. https://www.mellanox.com/related-docs/products/tolly-report-performance-
364
Online, accessed: 2019-07.
[76] Di Xie, Ning Ding, Y Charlie Hu, and Ramana Kompella. 2012. The Only Constant
is Change: Incorporating Time-varying Network Reservations in Data Centers.
In SIGCOMM. ACM, Helsinki, Finland, 199–210.
[77] Yiwen Zhang, Juncheng Gu, Youngmoon Lee, Mosharaf Chowdhury, and Kang G
Shin. 2017. Performance Isolation Anomalies in RDMA. In Workshop on Kernel-
Bypass Networks (KBNets). ACM, 43–48.
[78] Yibo Zhu, Haggai Eran, Daniel Firestone, Chuanxiong Guo, Marina Lipshteyn,
Yehonatan Liron, Jitendra Padhye, Shachar Raindel, Mohamad Haj Yahia, and
Ming Zhang. 2015. Congestion Control for Large-scale RDMA Deployments. In
SIGCOMM. ACM, London, United Kingdom, 523–536.
Appendices
Appendices are supporting material that has not been peer re-
viewed.
A PicNIC for Hardware-based Stacks
While we have described PicNIC’s design and implementation for
a software-based network virtualization stack, we find that (i) isola-
tion can break, resulting in unpredictable performance, even with
virtualization stacks implemented in hardware [22], and (ii) Pic-
NIC’s design principles (§3) can ensure predictable performance in
such stacks as well.
A.1 Causes of Unpredictable Performance
Isolation breakage in hardware also arise from contention for shared
resources. Consider a hardware NIC or FPGA-based virtualization
stack connected to the CPU over PCIe. Fig. 15 shows a simplified
view on a host with a modern system-on-chip (SoC) architecture
such as Intel Skylake [52]. The NIC hardware can be connected over
PCIe either directly to the I/O controller in the System Agent on
the SoC, or through a Platform Controller Hub (PCH). The primary
resource under contention is the bandwidth between the NIC and
the main memory (DRAM) where packets are delivered to the guest
OS networking stack. There are multiple potential bottlenecks along
the path: PCIe bandwidth ( 1(cid:2) or 2(cid:2)), DMI bandwidth ( 3(cid:2)) and the
DRAM bandwidth ( 4(cid:2)).
PCIe bandwidth. Consider the NIC at either dev0 or dev1 in Fig. 15
connected through PCIe Gen3 x8, commonly used for 40Gb/s NICs
today [31]. While the theoretical raw bit-rate offered by PCIe Gen3
x8 is 8 GT/s × 8 bits = 64 Gb/s, the entire bandwidth is not achievable
for data transfer because of, for example, encoding, packetization,
and protocol overheads of PCIe. In fact, the theoretical throughput
when accessing main memory in 64B granularity from NIC turns
out to be 44.8 Gb/s [43], and credit-based PCIe protocols for reli-
able read and write further reduce the effective bandwidth [40].
In addition to packet DMA, the NIC also shares this bandwidth
for operations such as reading/writing descriptors, updating queue
pointers and signalling interrupts. Performing this naively results
in effective bandwidth for packet DMA to be less than 40 Gb/s [53]
and thus PCIe bandwidth becomes a bottleneck, especially with
small packets.
DMI bandwidth. If the NIC is connected to the PCH on the chipset,
the PCH to System Agent connection can introduce another bottle-
neck. In the Intel Skylake architecture, this connection is through
PicNIC: Predictable Virtualized NIC
SIGCOMM ’19, August 19–23, 2019, Beijing, China
C
o
S
core 0
. . .
core N
Ring/Mesh interconnect
dev0
dev1
System Agent
I/O
PCIe 3
1(cid:2)
2(cid:2)
PCIe 3
controller
3(cid:2)
DMI 3.0
PCH
. . .
Memory
controller
devn
DRAM
DDR3
4(cid:2)
NIC DMA path
Other accesses
Figure 15: Potential bottlenecks within host hardware.
Direct Media Interface (DMI) 3.0, which offers a maximum band-
width equivalent to PCIe Gen3 x4 with raw bit-rate of 8 GT/s × 4
bits = 32 Gb/s.
DRAM bandwidth. Finally, packets need to be written to or read
from guest OS stack memory on the host DRAM. Each DRAM
channel has a bandwidth that depends on the DRAM frequency
and bus-width. For instance, a single-channel DDR3-1600 which
runs at 1600 MT/s with a 64-bit bus-width offers a peak bandwidth
of 102.4 Gb/s. The sustainable bandwidth for DRAM is much lower
(typically 70-80%) than the peak owing to factors such as access
patterns [65]. Even though DRAM bandwidth in modern systems is
much greater than networking bandwidth, the DRAM bandwidth
is also shared by others such as I/O and compute. This leads to
memory isolation issues in shared public clouds, and also affects
networking performance.
Isolation breakage. In practice, PCIe bandwidth can often be a pri-
mary bottleneck [43, 53, 61]. Sharing these resources based on
SLOs is key to ensuring predictable performance (design principle
P1). Failing to do so, e.g., if one VM monopolizes PCIe bandwidth,
can lead to other VMs being starved unfairly with unpredictable
network performance. Fair-sharing at ingress may lead to packets
being dropped at ingress for VMs receiving packets at rates exceed-
ing their fair-share. Drops at ingress lead to wasted resources at
egress, in the fabric, and at the ingress NIC. So, we need to imple-
ment admission control by creating fair backpressure all the way to
the sources and applying appropriate rate limits at the egress NIC
(design principle P2). Egress rate limiting, by itself, isn’t sufficient
to ensure isolation at the egress as we showed in §2.1. If VMs keep
sending excessive packets for throttled flows that are going to be
dropped by the traffic shaper in the NIC, it leads to unnecessarily
wastage of DRAM and PCIe bandwidth, hardware clock cycles, and
limited SRAM that buffers packets in the NIC for shaping. Thus,
we need to augment in-hardware rate limiting with appropriate
backpressure to the guest networking stack.
A.2 Hardware Design of PicNIC
Based on the design principles of PicNIC (§3), and the causes of
unpredictable performance in hardware-based virtualization stacks,
PicNIC’s constructs can be adopted in hardware as outlined next.
Ingress. At the ingress, PicNIC implements per-VM queues in hard-
ware so that packets can be moved from shared NIC Rx queues to
per-VM queues at NIC line rate and avoid unfair NIC drops. Unlike
the software-based implementation where we need to explicitly
provision enough CPU cycles for this function, in case of HW, this
module can hash packet headers and enqueue to the per-VM queue
independent of other functions. To ensure that the NIC to DRAM
bandwidth is shared as per-SLO, PicNIC keeps track of the PCIe
bandwidth (which can be different from the network bandwidth
because PCIe bandwidth overheads are per-TLP) used per VM and
schedules per-VM queues based on SLOs and PCIe bandwidth us-
age. Thus, packets belonging to offending flows (sending more than
SLOs and breaking isolation) will experience queueing delays and
may also be dropped fairly in the per-VM queues, while packets
belonging to well-behaved flows continue to experience low delays.
Packets may also be dropped in per-VM queues if the receiver VM
is slow (e.g. compute bottlenecked) and is not able to consume
packets destined to it.
To ensure VMs receive traffic at a predictable bandwidth, PicNIC
implements timing wheel (TW) [74] in NIC hardware at the ingress.
Based on per-VM ingress rate, the TW sets a timestamp for each
packet which decides when the packet is released from the stack
and delivered to the VM. Thus, VMs receive regularly paced packets
as per their SLO instead of bursts. Packets that exceed the timing
wheel quota are dropped. However, we expect PCCB to implement
proper admission control and avoid drops at ingress.
Congestion Control. PicNIC needs to avoid ingress drops as it leads
to wasted resources and decreases efficiency. Again, the delay ex-
perienced by packets in per-VM queues acts as a good signal for
how much the ingress rate exceeds the SLO-based fair share, and
it is also an early indicator for drops. Using this delay as conges-
tion signal, PicNIC implements PCCP to apply backpressure to
the sources under PCIe bandwidth contention. Both PCCB and
PCCP are implemented in hardware using a similar approach as
the software-based implementation. The feedback packets can be
generated and consumed in the NIC without involving the host
CPU. The state needed for congestion control at ingress (∼5kB)
consumes a very small fraction of on-NIC SRAM. At the egress,
the VM-VM rate-limit table can be maintained either completely in
the NIC or in the DRAM (with entries cached in NIC) depending
on the trade-off between on-NIC SRAM capacity and the PCIe and
memory bandwidth needed for read/write to in-memory table. The
choice also depends on how the virtualization stack implements
other lookup tables.
Egress Shaping and backpressure. At the egress, PicNIC implements
flexible shaping framework in HW that can enforce a hierarchy of
rates—e.g., total egress rate limit per-VM and VM-to-VM rate limits—
while ensuring isolation. PicNIC uses a combination of i) an on-NIC
scheduler and ii) per-flow ((cid:10)VM-src, VM-dst, vnet-id(cid:11)) FIFO queues
in main memory to achieve scalable shaping similar to SENIC [64]
and Loom [71]. When a packet is ready to be sent by a VM, the
guest vNIC driver enqueues the packet in the corresponding per-
flow queue but doesn’t mark the packet as transmission complete
immediately. The driver sends the packet descriptor along with any
metadata, e.g. VM-src, VM-dst and vnet-id, needed to make schedul-
ing decisions to the NIC as a doorbell write using memory-mapped
I/O over PCIe. The on-NIC scheduler uses the metadata to compute
the egress timestamp for each packet based on the applicable rate
limits and enqueues the descriptor to a calendar queue [12, 67, 70].
When the on-NIC scheduler dequeues a descriptor from this queue,
it issues a DMA read request to fetch the corresponding packet
365
SIGCOMM ’19, August 19–23, 2019, Beijing, China
P. Kumar et al.
To enable deferred completions without support for out-of-order
completions, a “reordering buffer” is required. Basically, before a
completion is returned to the VM, it is first put in a buffer. The order
of completions in this buffer is then checked, and if there is a batch
of reordered completions that can now be returned, all are returned.
If not, this completion is held until all missing completions become
available. We call this behavior deferred completion.
C Production Results
To demonstrate the practicality of PicNIC, we deployed it in pro-
duction of a large-scale public cloud provider. We present a subset
of results based on the deployment.
Egress. Of PicNIC’s egress features (§5.4), we show the results with
Packet Accounting + out-of-order (OOO) completions. Of the Guest
OS features, TSQ is enabled by default in Linux based kernels, while
NAPI-TX is a feature we are working to turn on in production.
Packet Accounting improves isolation for buffered traffic across
VMs as shown in §6.1. This feature is enabled throughout the pe-
riod of interest; in the interest of space, we do not tease out the
before/after impact of Packet Accounting.
§2 demonstrated how rate limiting flows at the egress can intro-
duce HoL blocking delay for non-rate-limited flows. Fig. 16 (top)
shows the extent of HoL blocking delay in production before and
after we deployed OOO completions. OOO completions and Packet
Accounting eliminate HoL blocking between rate-limited and non-
rate-limited flows at the egress. Consequently, OOO completions,
by itself, also improves the tail latency for customer traffic by ∼13%
as shown in Fig. 16 (bottom).
Ingress. The key construct at the ingress is CWFQ. Fig. 17 shows
that as a result of CWFQs, we see 96% decrease in packet drops at
NIC Rx queues. Excess packets are dropped at the per-VM queues.
We note that CWFQs by themselves cannot eliminate NIC packet
drops when the incoming PPS load is greater than the engine’s ca-
pacity to pull packets from NIC Rx queues, such as DoS attacks and
incast type workloads. When CWFQs are coupled with congestion
control, we observe a substantial reduction in packet drops and a
sub-ms response time in mitigating isolation breakages even under
DoS attacks.
Fabric Congestion. We expect PicNIC to complement prior work on
performance isolation in the network fabric in order to guarantee
predictable network performance to tenants. We built a prototype of
such a complete system that can handle congestion in the fabric. For
this, we extended PicNIC to incorporate ECN signals from the fabric
and use a DCTCP-like algorithm [2] to compute the appropriate
rates in PCC.
400
300
200
100
)
s
p
p
k
(
e
t
a
r
p
o
r
D
0
0
CWFQ rollout period
1
2
Number of days
Host NIC drops
Per-VM Fair drops
3
4
Figure 17: PicNIC’s CWFQs reduce unfair ingress drops at host NIC by 96%
and drop packets fairly on a per-VM basis.
)
s
μ
(
y
a
l
e
d
g
n
i
k
c
o
l
b
L
o
H
100000
10000
1000
100
10
1
0
1.2
1.0
0.8
0.6
0.0
)
s
m
(
T
T
R
.
c
r
e
p
h
t
9
9
99.9th perc.
99th perc.
Median
Enable OOO completions
O
a
able OO
O
aEna
letions
O comp
Before
No HoL blocking
After
Mean = 0.91 ms
0
1
2
3
4
Number of days
Mean = 0.79 ms
5
6
7
Figure 16: OOO completions with PicNIC eliminates HoL blocking delay
and decreases tail RTT by 13% in production.
from DRAM, performs any on-NIC virtualization functions (e.g.,
encapsulation) and sends it on the wire. This avoids the need for
buffering packets in the NIC. When reading a packet from DRAM,
the NIC also generates the corresponding completion event by writ-
ing to the in-DRAM completion queue. The driver forwards these
completion events as generated by the NIC. As completions are or-
dered by the scheduler based on rate limits, they can be out of order
w.r.t. descriptor ordering. This avoids HoL blocking as discussed
in §5.4.3. Since the on-NIC scheduler needs to store a reference to
each packet until it is sent out, and on-NIC SRAM is limited, we
need to ensure that the number of pending descriptors is bounded.
PicNIC achieves this using packet accounting (§5.4.2) by limiting
each per-VM on-DRAM queue capacity, and hence prevents rate
limited flows from exhausting on-NIC SRAM capacity for packet
descriptors. As further optimization, e.g. to minimize the number
of PCIe writes, doorbell writes can be batched together [43, 71].
B Deferred Completions
Deferred Completions in networking stacks return the completion
of a packet to VM only once processing has been completed and
the packet has left the NIC.
366