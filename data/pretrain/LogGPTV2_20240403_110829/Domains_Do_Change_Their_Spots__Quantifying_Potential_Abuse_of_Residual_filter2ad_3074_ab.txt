domains, re-register and deploy them, and identify cases of residual
trust. We adopt a domain selection strategy that relies on passive DNS,
which refers to the capturing of live DNS records, consequently build-
ing partial replicas of DNS zones, and storing them in a database [15].
Passive DNS data help to answer questions that are difficult or im-
possible with standard DNS, such as historical resolution information
(e.g., “What IP address did this domain name point to in the past?”).
III. EXPERIMENT DESIGN & METHODOLOGY
In this section, we present the rationale behind our experimental
design, beginning with our strategy for domain selection and
automated infrastructure for
re-registration, deployment, and
monitoring. We end with detailing the methodology of residual trust
detection in our traffic analysis.
A. Domain Selection
Our domain selection criterion is derived from the intuition
that domains with a large amount of traffic pre-expiration are
more likely to receive traffic post-expiration because of their sheer
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:51 UTC from IEEE Xplore.  Restrictions apply. 
2131
s
t
s
e
u
q
e
R
f
o
r
e
b
m
u
N
8000
6000
4000
2000
0
High DNS Traffic
Low DNS Traffic
Placebo Hosts
1 2 3 4 5 6 7 8 910
1 2 3 4 5 6 7 8 910
1 2 3 4 5 6 7 8 910
Virtual Machine ID
Fig. 1. Traffic received on recently expired and re-registered domains. Each virtual
machine was assigned a unique IP address. Placebo hosts are control hosts that do
not have an associated domain name.
Fig. 2. Pre-expiration vs. post re-registration DNS resolutions of a random sample
of re-registered dropped domains. After re-registration, the number of DNS resolutions
is generally lower by one order of magnitude. All sampled domains with over 1 million
pre-expiration resolutions receive at least over 10K resolutions after re-registration.
popularity, whether the traffic is from unaware users or to-be-updated
infrastructure tools. The number of DNS resolutions for a domain is
an appropriate proxy for the amount of traffic received by a domain,
because DNS is one of the fundamental services on the Internet and
the protocol is necessary to resolve domain names to IP addresses.
We use a commercial passive DNS database [16] as our proxy to
gauge DNS activity for a given domain and attempt to re-register
domains with a high number of pre-expiration resolutions.
Because dropcatchers are known to re-register domain names as
soon as they become available and may spend hundreds of thousands
of dollars for that privilege [3], [4], we focus on domain names
that are of no interest to them. Namely, the threat model that we
investigate is that of an attacker who can opportunistically identify
the “hidden gems” that dropcatchers missed (i.e., ones that were not
registered immediately after being dropped) and can register them for
nominal prices (i.e., the typical price of a domain registration). This
attacker will register domains based on how many times they used to
be resolved by DNS when they were active, as opposed to looking for
the same traits that dropcatchers look for (e.g., their historical Alexa
ranking, how short each domain is, if there’s a website available for
it on Internet Archive, etc.) [4].
We first conducted two small-scale monitoring experiments before
our domain registration process: the first to determine whether the
intuition that valuable domains expire and receive residual trust traffic
is reasonable and the second to determine an appropriate empirical
threshold for the number of historical DNS resolutions to use for our
domain selection. In the former, we use the passive DNS database and
re-register ten historically high-traffic domains several hours after they
were released for public registration. These domains were selected
by sorting a sample of 20K domains that were dropped on June 13,
2019 by the number of their historical DNS resolutions. Because these
domains remained on the market for several hours after they became
available, we know that dropcatchers were not interested in them;
according to Lauinger et al., dropcatchers register their target domains
mere seconds after they are released [3]. As part of our control groups,
we also re-registered ten historically low-traffic domains, and created
ten placebo machines that are not associated with any domain names.
These machines receive traffic only from network-scanning bots that
probe online IPs. All machines in the three groups logged the times-
tamp and client IP address of each HTTP(S) request and responded
with a blank page. In addition, to ensure that there is no difference in
network placement, we place all the groups in the same class-C subnet.
Figure 1 shows the number of requests received by each container
in a one-week period. We observe that the low-traffic group and the
placebo group received similar volume of HTTP(S) traffic; these
low-traffic domains did not receive more traffic than any other online
container, regardless of whether their IP resolved from a registered
domain. On the other hand, we observed that two historically
high-volume domains received more than twice as much traffic as
any historically low-volume or placebo container. This suggests that
the domain names which were not deemed valuable by dropcatchers
are still referenced by third-party, dependent infrastructure that is
unaware of the change in ownership.
In our second motivating experiment, we obtained a list of dropped
domains on June 20, 2019 and monitored the domains to select
those that were re-registered within two weeks. This sample includes
a mix of domains that were dropcaught and domains that were
re-registered later. For each selected domain, we compared its total
number of DNS resolutions (accounting for SOA, A, NS, and MX
records) at the time of its expiration with its number of additional
DNS resolutions two weeks after re-registration. Figure 2 compares
the number of lookups after re-registration with the number before
expiration, showing that the number of resolutions after re-registration
is generally one order of magnitude lower than the number of
resolutions pre-expiration. We empirically choose a threshold of 1
million resolutions pre-expiration which is a likely indicator that the
domain will receive over 10K resolutions within two weeks after
re-registration. This experiment also served as an estimate of the
amount of traffic that our re-registered domains would receive. We
restrict the TLDs considered to the most popular (.com, .net,
.org, .info and .biz) to narrow down the set of domain
candidates. Although the TLDs we consider are popular among
dropcatchers, our domain selection strategy is different from that of
typical dropcatchers as described by Miramirkhani et al. [4], which
reduced the contention for the domains that we selected.
Our domain registration process consisted of a pilot phase and
a main phase. In our pilot phase, we experimented with domain
selection strategies and re-registered 29 domains; in our main phase,
we re-registered 172 domains over the course of one month beginning
on August 8, 2019. For the first 20 days, we randomly sampled the
dropped domains to reduce the number of queries to the passive DNS
database, due to service quotas. In the last 10 days, we were able to
query all dropped domains and identify more candidates. On several
occasions, our daily candidate identification process resulted in sets
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:51 UTC from IEEE Xplore.  Restrictions apply. 
2132
Fig. 3. A diagram of our system modules. Our deployment module also installs honeypot services for each registered domain and all logs, including raw packet capture
files, are collected by our raw log storage. All the dashed lines on the right-hand side belong to an iterative process involving manual analysis.
of domains with similar lexical structures (e.g., vvol1kans.com,
vvol1ccan.com, wol1ck.com, wol1kkano.com,
and
vvol1can.com). These domains used squatting techniques that
were more complex than typical techniques [17], [18], [19], [20],
[21] and were difficult to automatically detect; for this reason,
we manually filtered out these similar domains. For each set of
similar domains, we opted to keep the one with the highest amount
of traffic. In rare cases, if we were able to find some interesting
characteristics of a lower-volume domain, we would select it instead
of the highest-volume domain.
Our main registration phase identified 550 domains of interest; of
these, 321 domains (58% of all identified) were manually identified as
highly similar to another candidate and excluded from consideration.
We were unable to register 57 domains (24% of the remaining
candidates) because they were re-registered by another entity. It
is unclear whether these domains were immediately caught or
re-registered hours after they were dropped because our module
executed at regular intervals. We were able to successfully re-register
172 (75%) of the remaining candidates.
B. Infrastructure
Figure 3 presents a high-level overview of our system, comprising a
domain selection module, container and honeypot deployment module,
log collector, and traffic analyzer. The domain selection module
performed daily queries to Domainmonster [22], DynaDot [23],
NameJet [24], Pool [25], and SnapNames [26] to obtain a list of
domains that were about to expire and followed the process outlined
in the previous section to select the domains considered valuable
enough to attempt to re-register.
Upon a successful re-registration of the domain name, the
deployment module creates a new container on the host machine,
allocates it a unique IP address (from our range of two class C blocks),
and creates the corresponding DNS records in our two name servers.
We use wildcard DNS (NS, A, and MX) records to capture requests
to all subdomains to quantify the effects of residual trust as accurately
as possible. In addition to the containers for re-registered domains, we
also randomly create containers that are not tied to any domain name,
but are otherwise identically configured and located in the same IP
address range. Following our terminology from Section III-A, we
call these extra containers placebos and use them to filter out traffic
to our domain servers that is not caused by residual trust. We do not
register domains for these placebo hosts to minimize our costs and
because we had already observed that they receive similar amounts of
traffic as historically low-traffic domains from our initial experiment.
Because these servers are not tied to any domain name, clients
can only discover them through network scanning. We reason that
these network-probing clients will also contact our domain servers,
enabling us to use traffic to our placebo servers as a type of negative
filter in our analysis of residual trust traffic (we further elaborate on
how we accomplish this in Section III-D).
We configure the containers with well understood and popular
protocols on the Internet, as the protocols that will actually receive
residual trust traffic will vary depending not only on the specific
domains registered, but also on the time of re-registration for
these domains. Moreover, traffic on non-standardized ports may
be associated with more than one service which would make it
challenging to interpret incoming traffic to these ports on the fly. As
a result, the deployment module configures containers to listen for
traffic on only the most common service ports, namely HTTP(S),
SSH, Telnet, and FTP. All HTTP(S) requests are served a custom 404
error page with bot traps (see Section III-D), along with a script that
attempts to collect a browser fingerprint. HTTPS is supported using
an X.509 certificate from Let’s Encrypt [27]. The SSH service is
a medium-interaction honeypot that logs a large selection of possible
events [28]. The Telnet and FTP services are low-interaction and
collect only the credentials of the login attempts [29].
In addition to the honeypot service logs, we collect packet captures
of the first 64 bytes of every packet entering the containers. The
logs are regularly collected from each container by a log-collector
module and stored on a large network-attached storage device. Our
traffic analyzer then ingests all raw logs from the log collector into
an Elasticsearch database [30].
C. Residual Trust Detection
Public systems on the Internet are bound to receive large amounts
of automated traffic, whether they are from spam, benign crawlers
for search engines and historical archives, or malicious fingerprinting
tools and vulnerability scanners. Identifying such automated traffic
is a prerequisite to our further analysis of residual trust traffic. As a
first step in our analysis, we therefore distinguish between two types
of accesses, bot traffic and trust traffic, using architectural
elements, request characteristics, and IP blocklists.
Our traffic analysis pipeline combines the intuition behind the
common block-listing and allow-listing approaches. The pipeline
comprises a series of bot filters followed by trust filters
that query our database. If a request matches a specific filter, the
corresponding client IP address is tagged with the filter type and name.
An IP address may be associated with multiple bot filters and
trust filters, but bot filters (the block-lists) are assigned more
weight than trust filters (the allow-lists) — any IP address with
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:51 UTC from IEEE Xplore.  Restrictions apply. 
2133
Fig. 4. Sankey diagram of the results of the bot and residual trust detection pipeline on HTTP(S) traffic. Each flow represents an exclusive set of IP addresses and each
node a stage in the filtering process. Each IP is bucketed in the first matching filter flow and the filters were applied sequentially to all client IP addresses in the top-down
order depicted, for each horizontal depth level from the initial node. For example, if an IP is tagged with bot filters of {dnsbl, bot-fingerprint} and a trust filter
of residual-path, the IP address would be in the flow [init, bot, dnsbl] because bot precedes trust and dnsbl precedes bot-fingerprint. IP
addresses are marked neutral if they have neither filter tag applied (i.e., they did not exhibit any indication that their requests resulted from bot or trust traffic).
Section III-D includes a detailed discussion of the analysis pipeline.
a mix of both types will be marked bot. Thus, an IP address will be
characterized as a) bot if it has any bot filter tags, b) trust if it
has no bot filter tags and any trust filter tags, or c) neutral
if it has no associated filter tags.
We categorize clients at IP address granularity. Although an IP
address can be shared by multiple clients, it is difficult to determine the
client or session boundaries for a given IP address from a server-side
perspective. Using heuristics (e.g., consider all requests from the same
IP address within 60 seconds of one another to be one session) is prone
to false positives in bot classification. We instead opt for a broader, IP
address granularity, and note that any number of IP addresses is a lower
bound of the actual number of clients behind those IPs. For classifying
residual trust traffic, this approach is conservative — if any of the ac-
cesses from an IP address are classified as bot, all requests from that
IP address are considered as bot. In most cases, we present the resid-
ual trust traffic results as a percentage of IP addresses tagged trust.
D. Bot and Trust Indicators
We develop a traffic analysis pipeline which comprises a series
of filters. The filters rely on architectural elements, suspicious request
characteristics, and external resources. Architectural elements are
intrinsically part of our system: the placebo servers, bot traps, and
a JavaScript fingerprinting script. Most of the other filters are derived
from suspicious request characteristics (e.g., resembles an exploit
or fingerprint attempt) that were manually identified for each type
of service. Our last type of filter relies on information gleaned from
external resources, such as web archives and IP blocklists used for
traditional networking rules. All filters are run sequentially and the
results of this filter for HTTP(S) traffic can be seen in Figure 4.
a) Architectural elements (placebos, bot traps, and fingerprints):
Placebo servers are servers that are not associated with any expired
domain names, but are otherwise identical in configuration. They
are allocated at random IP addresses from the same range and run
the same set of services as our domain servers. Because the placebo
servers are not associated with any of the domains we re-register,
all traffic they receive is not related to residual trust from DNS. We
therefore use them as a control group to learn the network signatures
of bot traffic, and we mark all IP addresses that access any service
on any placebo hosts as bots. The corresponding tag that represents
this filter in Figure 4 is placebo.
Our second and third architectural mechanisms affect only the
HTTP(S) service, with which we serve a web page that includes “bot
traps” and an innocuously-named fingerprinting script (utils.js).
We use the popular FingerprintJS [31] fingerprinting script, extended
with a function to also send a POST request to a public IP
geolocation service. Bot traps are designed to lure bots into requesting
certain paths that would never be requested by an end user web
browser [10], [32]. Every trap is associated with a unique URL in
its href attribute or in its content, and these trap elements include
a) inconspicuous 1x1 images, b) HTML comments, c) dynamically
removed content after the page is loaded using JavaScript, d) elements
that are not displayed (i.e., display=none), and e) elements that
inherit the invisible property from their parents.
Crawlers can fall for all of the traps or none of them, depending
on their crawling logic. However, prior work has shown that bot
behavior is generally too simple to avoid all of these traps [33]. Even
sophisticated crawlers, such as those used for research or by search
engines like Google, still fall for these traps. Moreover, we discovered
that 4,764 unique IP addresses that self-identified as bots in the HTTP
User-Agent header fall for these bot traps (approximately 43.42% of
all IPs that self-declare themselves as a bot). We tag any IP addresses
that request trap paths as bots and the corresponding tag in Figure 4
is bot-path.
Our last architectural mechanism is an augmented fingerprinting
script [31], which also collects the document referrer and sends an
extra POST request for the current IP address geolocation before
sending the final fingerprint in the payload of a POST request back
to the container. The lack of a fingerprint can be used to identify
bots, because most do not support JavaScript [33]. Although some
crawlers support JavaScript, the benign ones can be easily identified
by the User-Agent header and by their well-known IP address
ranges. If these values are spoofed (such that one is pretending to
be a crawler), in our results, it will constitute a false negative rather
than a false positive of an instance of residual trust (which is still
aligned with our goal of quantifying the lower bound on the amount
of residual trust traffic). We use the fingerprints for multiple filters:
• for each fingerprint, if the User-Agent contains “bot” or
a reverse DNS lookup on the IP address yields a domain that
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:51 UTC from IEEE Xplore.  Restrictions apply. 
2134
t =120) :
like pw ( r1 . domain , r1 .pw, d2)
def tag repeated logins ( ip , d1=1, d2=2,
is bot = False
is user = False
for r1 in get reqs from ip ( ip ) :
Listing 1. Filter to detect and tag IP addresses for repeated login attempts. d1 and
d2 are Levenshtein distances. bot_like_pw checks whether the password
contains a common variant of the domain name and whether the password is similar
to one from RockYou’s leak.
1
2
3
4
5
6