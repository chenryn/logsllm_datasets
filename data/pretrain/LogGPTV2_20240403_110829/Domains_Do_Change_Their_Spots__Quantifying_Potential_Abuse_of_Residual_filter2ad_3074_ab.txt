### Domain Re-Registration and Residual Trust Detection

#### I. Introduction
In this study, we explore the re-registration of recently expired domains, their deployment, and the detection of residual trust. We employ a domain selection strategy based on passive DNS (pDNS) data, which involves capturing live DNS records to build partial replicas of DNS zones and storing them in a database [15]. pDNS data allows us to answer questions that are difficult or impossible with standard DNS, such as historical resolution information (e.g., "What IP address did this domain name point to in the past?").

#### II. Experimental Design & Methodology
This section outlines the rationale behind our experimental design, starting with the domain selection strategy and automated infrastructure for re-registration, deployment, and monitoring. We conclude with the methodology for detecting residual trust in our traffic analysis.

**A. Domain Selection**
Our domain selection is guided by the intuition that domains with high pre-expiration traffic are more likely to receive post-expiration traffic due to their popularity. The number of DNS resolutions for a domain serves as a proxy for its traffic, as DNS is essential for resolving domain names to IP addresses. We use a commercial pDNS database [16] to gauge DNS activity and re-register domains with a high number of pre-expiration resolutions.

Given that dropcatchers often re-register valuable domain names immediately upon expiration, we focus on domains that are not of interest to them. Specifically, we target "hidden gems" that dropcatchers miss, allowing us to register them at nominal prices. Our approach differs from typical dropcatchers, who look for traits like historical Alexa rankings, domain length, and website availability on Internet Archive [4].

**B. Initial Experiments**
We conducted two small-scale experiments to validate our hypothesis and determine an appropriate threshold for domain selection. In the first experiment, we re-registered ten historically high-traffic and ten low-traffic domains several hours after they were released. Additionally, we set up ten placebo machines without associated domain names. These machines logged HTTP(S) requests and responded with a blank page. All groups were placed in the same class-C subnet to ensure no network placement differences.

Figure 1 shows the traffic received by each container over a one-week period. The low-traffic and placebo groups received similar volumes of traffic, while two high-traffic domains received significantly more traffic, suggesting that these domains are still referenced by unaware third-party infrastructure.

In the second experiment, we monitored dropped domains from June 20, 2019, and selected those re-registered within two weeks. Figure 2 compares pre- and post-re-registration DNS resolutions, showing that the number of resolutions generally decreases by one order of magnitude. We empirically chose a threshold of 1 million pre-expiration resolutions, indicating that the domain will likely receive over 10K resolutions within two weeks post-re-registration.

**C. Domain Registration Process**
Our registration process included a pilot phase and a main phase. In the pilot phase, we experimented with domain selection strategies and re-registered 29 domains. In the main phase, we re-registered 172 domains over one month, starting on August 8, 2019. We used random sampling and later queried all dropped domains to identify more candidates. We manually filtered out domains with similar lexical structures, keeping the one with the highest traffic.

**D. Infrastructure**
Figure 3 provides an overview of our system, including a domain selection module, container and honeypot deployment module, log collector, and traffic analyzer. The domain selection module performs daily queries to various services to obtain a list of expiring domains and selects valuable ones for re-registration.

Upon successful re-registration, the deployment module creates a new container, allocates a unique IP address, and sets up DNS records. We use wildcard DNS records to capture subdomain requests and create placebo containers to filter out non-residual trust traffic. The containers are configured to listen for common service ports (HTTP(S), SSH, Telnet, and FTP) and collect logs, including packet captures.

**E. Residual Trust Detection**
To detect residual trust, we distinguish between bot traffic and trust traffic using architectural elements, request characteristics, and IP blocklists. Our traffic analysis pipeline combines block-listing and allow-listing approaches. Each IP address is tagged with filter types, and bot filters are given more weight than trust filters. An IP address is categorized as bot, trust, or neutral based on the applied filters.

**F. Bot and Trust Indicators**
Our traffic analysis pipeline includes filters based on architectural elements, suspicious request characteristics, and external resources. Placebo servers, bot traps, and fingerprinting scripts help identify bot traffic. Bot traps lure bots into requesting specific paths, and fingerprinting scripts collect browser fingerprints. We tag IP addresses that fall for these traps as bots. The results of the HTTP(S) traffic analysis are shown in Figure 4.

#### III. Conclusion
This study demonstrates the feasibility of re-registering recently expired domains and detecting residual trust. By focusing on high-traffic domains and using a combination of architectural elements and traffic analysis, we can effectively identify and quantify residual trust traffic.