25:
26:
27:
28:
29: upon receive RECOVERY[c, Ballot] from pk ∧ Ballot > Ballotsj [c]
30:
31:
32:
33:
34:
send RECOVERYR[c, Ballotsj [c], Hj.GETINFO(c)] to pk
send RECOVERYR[c, Ballotsj [c], NOP] to pk
Ballotsj [c] ← Ballot
if Hj.CONTAINS(c) then
else
Fig. 5. RECOVERY phase executed by node pk. Node pj is a receiver of
the RECOVERY message.
For this reason, CAESAR also includes an explicit recovery
procedure (Figure 5) that ﬁnalizes the decision of commands
whose leader either crashed or has been suspected. Given the
aforementioned example, whenever the failure detector of pk
suspects pi, pk attempts to become c’s leader and ﬁnalizes the
decision of c. This is done by executing a Paxos-like prepare
phase, and collecting the most recent information about c from
56
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:00:16 UTC from IEEE Xplore.  Restrictions apply. 
a quorum of CQ nodes as follows: pk increments its current
ballot for c, i.e., Ballotsk[c], (line 2) and it broadcasts a
RECOVERY message for c with the new ballot (line 3). Then, it
waits for a quorum of CQ RECOVERYR replies, which contain
information about c, before ﬁnalizing the decision for c (line
4). RECOVERYR from pj contains either the tuple of c in Hj
or NOP if such a tuple does not exist (lines 31–34).
A node pj that receives a RECOVERY message from pk
replies only if its ballot for c is lesser than the one it has
received. In such a case, pj also updates its ballot for c (lines
29–30). Like in Paxos, this is done to guarantee that no two
leaders can compete to ﬁnalize the decision for the same
command concurrently. In fact, if two leaders pk1 and pk2 both
successfully execute lines 3 and 4 of the recovery procedure
with ballots B1 and B2, respectively, then, if B1 < B2, for
any quorum of nodes S, there always exists a node in S
that never replies to pk1 (see the reception of FASTPROPOSE,
SLOWPROPOSE, RETRY, and STABLE messages in Figure 4).
When node pk successfully becomes c’s leader, it ﬁlters
the information for c that it has received by only keeping in
RecoverySet the data associated with the maximum ballot,
named MaxBallot in the pseudocode (lines 5–6). Each
tuple of the set is a sequence of node identiﬁer, timestamp,
predecessors set, status, and forced boolean indicating: the
node that sent the information, the timestamp, the predecessors
set, the status of c on that node, and whether that information
has been forced by a WhiteList or not on that node. Then, pk
takes a decision for c according to the content of RecoverySet
as follows. i) If there exists a tuple with status stable, then pk
starts a stable phase for c by using the necessary info from
that tuple, e.g., timestamp and predecessors set (lines 7–8). ii)
If there exists a tuple with status accepted, then pk starts a
retry phase for c by using the necessary info from that tuple
(lines 9–10). iii) If there exists a tuple with status rejected
or RecoverySet is empty, c was never decided, and hence pk
starts a fast proposal phase for c (lines 11–13, and 26–28) by
using a new timestamp (as described in Section V-B). iv) If
there exists a tuple with status slow-pending, then pk starts
a slow proposal phase for c by using the necessary info from
that tuple (lines 14–15). v) If the previous conditions are false,
then RecoverySet contains tuples with the same timestamp
T ime and status f ast-pending (lines 16–25). In this last case,
pk starts a proposal phase for c with timestamp T ime because
c might have been decided with that timestamp in a previous
fast decision (line 25). If so, pk has to also choose the right
predecessors set that was adopted in that decision. Therefore,
it has to either choose a predecessors set in RecoverySet that
was forced by a previous recovery, if any (lines 19–20), or it
has to build its own WhiteList of commands that should be
forced as predecessors of c (lines 21–24).
This is done by noticing that: if c was decided in a fast de-
cision with ballot MaxBallot then the size of RecoverySet
+ 1, which is the minimum size
cannot be lesser than
of the intersection of any classic quorum and any fast quorum
(lines 21 and 24); if a command ¯c was previously decided in
a fast decision and it has to be a predecessor of c, then there
(cid:2)CQ
(cid:3)
2
(cid:2)CQ
(cid:3)
2
+ 1 tuples in RecoverySet,
cannot exist a subset of
whose predecessors sets do not contain ¯c (line 22). Note that,
the case in which ¯c was previously decided in a slow decision
and has to be a predecessor of c is handled by the computation
of predecessors set in the fast proposal phase (see line P13 of
Figure 4, and lines 1–3 of Figure 3).
F. Correctness
The complete formal proof on the correctness of CAESAR
is in the technical report [23], where we have also formalized
a description of the algorithm in TLA+ [24], which has been
model-checked with TLC model-checker. Here we provide the
main intuition on how we proceeded in proving that CAESAR
implements the speciﬁcation of Generalized Consensus.
Let us also deﬁne the predicate DECIDED[c,T ,Pred,B] as
a predicate that is equal to true whenever a node decides
a command c with timestamp T , predecessors set Pred,
and ballot B. Then we can prove that CAESAR guarantees
Consistency by proving the following two theorems:
- ∀c, ¯c, (DECIDED[c,T ,Pred,B] ∧ DECIDED[¯c, ¯T ,Pred,B] ∧
¯T < T ∧ c ∼ ¯c ⇒ ¯c ∈ Pred);
- ∀c (∃B, DECIDED[c,T ,Pred,B] ∧ ∀¯c ∈ Pred, DE-
CIDED[¯c, ¯T ,Pred,B] ⇒ ∀B(cid:18) ≥ B,(DECIDED[c,T (cid:18),Pred
(cid:18),B(cid:18)]
⇒ T (cid:18) = T ∧ Pred
(cid:18) = Pred)).
VI. IMPLEMENTATION AND EVALUATION
We implemented CAESAR in Java and contrasted it with
four state-of-the-art consensus protocols: M 2Paxos, EPaxos,
Multi-Paxos, and Mencius. We used the Go language imple-
mentations of EPaxos, Multi-Paxos, and Mencius from the
authors of EPaxos. For M 2Paxos, we used the open-source
implementation in Go. Note that Go compiles to native binary
while Java runs on top of the Java Virtual Machine. Thus,
we use a warmup phase before each experiment in order to
kickstart the Java JIT Compiler.
Competitors have been evaluated on Amazon EC2, using
m4.2xlarge instances (8 vCPU and 32GB RAM) running
Ubuntu Linux 16.04. Our benchmark issues client commands
to update a given key of a fully replicated Key-Value store.
Two commands are conﬂicting if they access the same key. The
command size is 15 bytes, which include key, value, request
ID, and operation type.
In our evaluations, we explored both conﬂicting and non-
conﬂicting workloads. When the clients issue conﬂicting com-
mands, the key is picked from a shared pool of 100 keys
with a certain probability depending on the experiment. As
a result, by categorizing a workload with 10% of conﬂicting
commands, we refer to the fact that 10% of the accessed keys
belong to the shared pool. To measure latency, we issued
requests in a closed loop by placing 10 clients co-located
with each node (50 in total), and for throughput the clients
injected requests to the system in an open loop. Performance
of competitors has been collected with and without network
batching (the caption indicates that).
We deployed the competitors on ﬁve nodes located in
Virginia (US), Ohio (US), Frankfurt (EU), Ireland (EU), and
57
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:00:16 UTC from IEEE Xplore.  Restrictions apply. 
(cid:7)(cid:16)(cid:22)(cid:14)(cid:16)(cid:20)(cid:16)(cid:8)
(cid:6)(cid:15)(cid:16)(cid:21)
(cid:1)(cid:5)(cid:6)(cid:9)(cid:5)(cid:8)
(cid:2)(cid:4)(cid:5)(cid:10)(cid:7)(cid:9) (cid:3)(cid:11)(cid:4)(cid:5)(cid:10)(cid:7)(cid:9)
(cid:2)(cid:22)(cid:8)(cid:20)(cid:17)(cid:13)(cid:25)(cid:22)(cid:24)
(cid:3)(cid:22)(cid:12)(cid:18)(cid:8)(cid:20)(cid:11)
(cid:5)(cid:25)(cid:19)(cid:9)(cid:8)(cid:16)
(cid:31)(cid:33)(cid:29)
(cid:31)(cid:32)(cid:29)
(cid:31)(cid:30)(cid:29)
(cid:30)(cid:35)(cid:29)
(cid:30)(cid:34)(cid:29)
(cid:30)(cid:33)(cid:29)
(cid:30)(cid:32)(cid:29)
(cid:30)(cid:30)(cid:29)
(cid:35)(cid:29)
(cid:34)(cid:29)
(cid:33)(cid:29)
(cid:28)
(cid:10)
(cid:12)
(cid:23)
(cid:19)
(cid:27)
(cid:1)
(cid:26)
(cid:10)
(cid:20)
(cid:12)
(cid:24)
(cid:8)
(cid:4)
(cid:31)(cid:33)(cid:29)
(cid:31)(cid:32)(cid:29)
(cid:31)(cid:30)(cid:29)
(cid:30)(cid:35)(cid:29)
(cid:30)(cid:34)(cid:29)
(cid:30)(cid:33)(cid:29)
(cid:30)(cid:32)(cid:29)
(cid:30)(cid:30)(cid:29)
(cid:35)(cid:29)
(cid:34)(cid:29)
(cid:33)(cid:29)
(cid:3)(cid:2)(cid:1) (cid:5)(cid:2)(cid:1) (cid:4)(cid:3)(cid:2)(cid:1)(cid:6)(cid:3)(cid:2)(cid:1)(cid:7)(cid:3)(cid:2)(cid:1)(cid:4)(cid:3)(cid:3)(cid:2)(cid:1)
(cid:31)(cid:33)(cid:29)
(cid:31)(cid:32)(cid:29)
(cid:31)(cid:30)(cid:29)
(cid:30)(cid:35)(cid:29)
(cid:30)(cid:34)(cid:29)
(cid:30)(cid:33)(cid:29)
(cid:30)(cid:32)(cid:29)
(cid:30)(cid:30)(cid:29)
(cid:35)(cid:29)
(cid:34)(cid:29)
(cid:33)(cid:29)
(cid:31)(cid:33)(cid:29)
(cid:31)(cid:32)(cid:29)
(cid:31)(cid:30)(cid:29)
(cid:30)(cid:35)(cid:29)
(cid:30)(cid:34)(cid:29)
(cid:30)(cid:33)(cid:29)
(cid:30)(cid:32)(cid:29)
(cid:30)(cid:30)(cid:29)
(cid:35)(cid:29)
(cid:34)(cid:29)
(cid:33)(cid:29)
(cid:31)(cid:33)(cid:29)
(cid:31)(cid:32)(cid:29)
(cid:31)(cid:30)(cid:29)
(cid:30)(cid:35)(cid:29)
(cid:30)(cid:34)(cid:29)
(cid:30)(cid:33)(cid:29)
(cid:30)(cid:32)(cid:29)
(cid:30)(cid:30)(cid:29)
(cid:35)(cid:29)
(cid:34)(cid:29)
(cid:33)(cid:29)
(cid:3)(cid:2)(cid:1) (cid:5)(cid:2)(cid:1) (cid:4)(cid:3)(cid:2)(cid:1)(cid:6)(cid:3)(cid:2)(cid:1)(cid:7)(cid:3)(cid:2)(cid:1)(cid:4)(cid:3)(cid:3)(cid:2)(cid:1)
(cid:3)(cid:2)(cid:1) (cid:5)(cid:2)(cid:1) (cid:4)(cid:3)(cid:2)(cid:1)(cid:6)(cid:3)(cid:2)(cid:1)(cid:7)(cid:3)(cid:2)(cid:1)(cid:4)(cid:3)(cid:3)(cid:2)(cid:1)
(cid:3)(cid:2)(cid:1) (cid:5)(cid:2)(cid:1) (cid:4)(cid:3)(cid:2)(cid:1)(cid:6)(cid:3)(cid:2)(cid:1)(cid:7)(cid:3)(cid:2)(cid:1)(cid:4)(cid:3)(cid:3)(cid:2)(cid:1)
(cid:3)(cid:2)(cid:1) (cid:5)(cid:2)(cid:1) (cid:4)(cid:3)(cid:2)(cid:1)(cid:6)(cid:3)(cid:2)(cid:1)(cid:7)(cid:3)(cid:2)(cid:1)(cid:4)(cid:3)(cid:3)(cid:2)(cid:1)
Fig. 6. Average latency for ordering and processing commands by changing the percentage of conﬂicting commands. Batching is disabled. Bars are overlapped:
e.g., in the case of 30% conﬂicts in Virginia, latency values are 90 msec, 108 msec, and 127 msec, for CAESAR, EPaxos, and M 2Paxos, respectively.
Mumbai (India). This conﬁguration spreads nodes such that
the latency to achieve a quorum is similar for all quorum-
based competitors. It is worth recalling that in a system with 5
nodes, CAESAR requires contacting one node more than other
quorum-based competitors to reach a fast decision. The round
trip time (RTT) that we measured in between nodes in EU and
US are all below 100ms. The node in India experiences the
following delays with respect to the other nodes: 186ms/VA,
301ms/OH, 112ms/DE, 122ms/IR. As in EPaxos, CAESAR
uses separate queues for handling different types of messages,
and each of these queues is handled by a separate pool of
threads. In CAESAR, conﬂicting commands are tracked using
a Red-Black tree data structure ordered by their timestamp.
Multi-Paxos is deployed in two settings: one where the
leader is located in Ireland, which is a node close to a quorum,
and one where the leader is in Mumbai, which needs to contact
nodes at long distance to have a quorum of responses.
A. Non-faulty Scenarios
In Figure 6, we report the average latency incurred by
CAESAR, EPaxos, and M 2Paxos to order and execute a
command. Given the latency of a command is affected by
the position of the leader that proposes the command itself,
we show the results collected in each site. Each cluster of data
shows the behavior of a system while increasing the percentage
of conﬂicts in the range of {0% – no conﬂict, 2%, 10%, 30%,
50%, 100% – total order}.
At 0% conﬂicts, EPaxos and M 2Paxos provide comparable
performance because both employ two communication steps
to order commands and the same size for quorums, with
EPaxos slightly faster because it does not need to acquire
the ownership on submitted commands before ordering. The
performance of CAESAR is slightly slower (on average 18%)
than EPaxos because of the need of contacting one more node
to reach consensus.
When the percentage of conﬂicting commands increases up
to 50%, CAESAR sustains its performance by providing an
almost constant latency; all other competitors degrade their
performance visibly. The reasons vary by protocol. EPaxos
degrades because its number of slow decisions increases ac-
cordingly, along with the complexity of analyzing the conﬂict
graph before delivering. For M 2Paxos,
the degradation is
related to the forwarding mechanism implemented when the
requested key is logically owned by another node. In that case,
M 2Paxos passes the command to that node, which becomes
responsible to order it. This mechanism introduces an addi-
tional communication delay, which contributes to degraded
performance especially in geo-scale where the node having
the ownership of the key may be faraway. At last, we included
also the case of 100% conﬂicts. Here all competitors behave
poorly given the need for ordering all commands, which does
not represent their ideal deployment.
(cid:34)
(cid:14)
(cid:16)
(cid:27)
(cid:23)
(cid:33)
(cid:1)
(cid:31)