D
n
o
i
t
o
M
5
2
0
2
5
1
0
1
5
0
e
c
n
a
i
t
s
D
n
o
i
t
o
M
Video
Sensor
5
2
0
2
5
1
0
1
5
0
Video
Sensor
i
e
c
n
a
t
s
D
n
o
i
t
o
M
0
2
5
1
0
1
5
0
0
20
40
60
80
0
20
40
60
80
0
20
40
60
80
Time Index
(a)
Time Index
(b)
Time Index
(c)
Figure 6: The Video Motion Analysis module has a limited ability to determine the average speed of the
device motion. For this reason, we use the average speed calculated from the inertial sensor motion vector
to calibrate that of the video motion vector. This ﬁgure shows the eﬀect of calibration in the similarity
computation. (a) No calibration. (b) Truncated Mean Calibration. (c) Curve Fitting Calibration.
For each pair of points in the sensor and video vectors, com-
pute their ratio and add it to a ratio vector. Compute the
truncated mean of the ratio vector, discarding 12.5% from
both the low and the high ends of the distribution.
Polynomial curve ﬁtting. Polynomial curve ﬁtting [21]
constructs the polynomial that has the best ﬁt to a series of
data points (see Figure 6(c)). To compute the coeﬃcients
that best ﬁt the curve to the given data, the least squares
method [21] is used which minimizes the error between the
data and the ﬁtted polynomial [21]. Let SPs denote the av-
erage value over the points on the ﬁtted curve for the sensor
stream and let SPv denote the average value of the points
on the curve of the video stream. Compute the calibration
factor as CF = SPs
SPv
.
Figures 6(b) and 6(c) show sample calibration outputs
for these two methods, when compared to the uncalibrated
version shown in Figure 6(a).
3.3.4 Example Alignment
To illustrate the need for the DTW, stretch and calibra-
tion steps previously described, we provide here experimen-
tal results of their use on a genuine sample of video and iner-
tial sensor streams, captured using Movee (see Section 4 for
implementation details). Figure 5(a) shows the alignment
between the video and inertial sensor streams when only
DTW is used. Figure 5(b) shows the resulting alignment
when DTW and stretching are applied. Finally, Figure 5(c)
shows the alignment achieved when DTW is applied along
with stretching and calibration. The experiment shows that
stretching is vital to achieve a good alignment, while cali-
bration further improves the quality of the alignment.
3.4 Classiﬁcation
The Similarity Computation module produces 14 features
that represent the nature of the similarity between the mo-
tion information inferred from the video stream and the one
observed from the inertial sensor data. The features are: (1)
the movement direction of the target from the center of the
screen (see Section 4), (2-5) the cumulative shift of the video
and accelerometer on the x and (y) axes (4 descriptors), (6)
the video motion direction, (7) the sensor motion direction,
(8) the DTW distance after stretching and calibration steps,
(9) the calibration factor, CF , (10) the normalized penalty
Figure 7: Movee in action: Target icon (bullseye)
at the bottom of the screen shows the direction in
which the user needs to move the camera.
cost, (11) the ratio of overlap points, (12) the ratio of diag-
onal moves, (13) the ratio of expansion moves and (14) the
ratio of contractions moves.
The Classiﬁcation module runs trained classiﬁers over these
features to determine whether there is suﬃcient evidence
that the video stream is genuine. Section 6.2 describes the
classiﬁers used in our experiments.
4. MOVEE IMPLEMENTATION
We have implemented a Movee client using Android and a
server component using C++ and PHP. We used the OpenCV
(Open Source Computer Vision) library [5] for the video mo-
tion analysis. The client allows users to capture movies and
simultaneously provide proofs of liveness. Figure 7 shows
a snapshot of Movee. When the user starts the client, she
is presented with an initial screen that instruct her to hold
the device ﬁrmly before pressing the start button. This is
done to prevent initial accelerometer reading errors. Fur-
thermore, once the user presses the start button, a target
appears (bullseye). The user is instructed to move the cam-
era in the direction of the target. Once the camera super-
imposes on the target, the target will change to a new place
on the screen, and the user needs to continue to follow it. A
progress bar indicates the elapsed time.
244
The target disappears after the ﬁrst 6 seconds. This de-
notes the veriﬁcation step. During the veriﬁcation step,
the Movee client captures the video stream and logs the
accelerometer data. Following the veriﬁcation period, the
user can continue capturing the intended scenes. The Movee
client only captures the data during the veriﬁcation interval,
which it sends to our server. We were inspired by Vine [10]
to choose the veriﬁcation interval to be 6s. Vine is an ap-
plication that allows users to create and post (on Twitter,
Facebook) video clips. This choice has the additional ad-
vantage that it keeps the size of the video ﬁle small (around
150 KB in the Samsung Admire Phone), thus reducing com-
munication overheads. While the veriﬁcation step can be
performed on the client, we chose to impose a communica-
tion overhead for the improved liveness analysis performance
of a more powerful server.
We have used a Samsung Admire smartphone running An-
droid OS Gingerbread 2.3 with an 800MHz CPU to test the
client side and a Dell laptop equipped with a 2.4GHz Intel
Core i5 processor and 4GB of RAM for the server.
5. APPLICATIONS
Citizen journalism. The recent emergence of video shar-
ing sites, e.g.,
[13, 10] has paved the way toward citizen
journalism: people that witness events of public importance
(e.g., public protests, natural and man-made disasters, me-
teorite landings) are now able to post their records of the
events and share them with the community at large. The
popularity of such sites, the rapid interest in certain videos
and the intrinsic fame they bring to their creators raise im-
portant authenticity questions. People may upload fraudu-
lent videos (e.g., created from old footage or from movies),
in order to deceive, bias public opinion or simply cheat their
way to fame. Movee can be used in conjunction with trusted
location and time veriﬁcation solutions (e.g., [17]) to verify
claims made by video uploaders.
Smarter cities, Mobile 311. Mobile 311 apps by munici-
palities and metropolitan governments [4, 2] tap into crowd-
sourced reporting of potholes and open manholes for city
maintenance, and to avoid possible hazards. To gauge the
correctness and severity of a case, the systems require multi-
ple users to report the same case before dispatching a crew.
The shortcoming is that, the system is set to wait for mul-
tiple complaints to come in before action, and may even be
accused of malpractice due to inaction even in the presence
of information. Movee can act as the required witness to
the genuine-ness of the reported case, and can eliminate the
need to wait for multiple reports.
Prototype veriﬁcation. Kickstarter [3] requires that real
prototypes are used in the promotion videos of campaigns.
Movee can be used by participants to verify the liveness of
footage provided as evidence.
6. EVALUATION
We ﬁrst describe our data collection process and the ex-
perimental setup. Second, we evaluate the overhead of the
liveness analysis on the server and then study the ability of
Movee to detect the attacks introduced in Section 2.1.
6.1 Data Collection
We have used the implemented Movee application to col-
lect video and accelerometer samples. We used the 3D ac-
celerometers, available in most recent smartphones and tablets,
to acquire motion acceleration data. The Samsung Admire
smartphones (Android OS Gingerbread 2.3 with 800MHz
CPU), on which we collected the data, sample accelerome-
ter readings at 16.67Hz [8] mode.
Motion direction inference. Given the video shift val-
ues computed by the VMA module, V Sx,i and V Sy,i the
motion direction of the video is determined. For instance,
if V Sx,i < 0 and |V Sx,i| (cid:6) |V Sy,i|, the motion direction is
to the right. A similar process is used to determine move-
ment in the other directions. Furthermore, the IMA module
maps the accelerometer values on its coordinate system. We
exemplify the sensor motion direction decision using the fol-
lowing example (that we extended for all other directions
and device orientation combinations).
If the device is in
landscape orientation and ASy,i < 0, ASy,i (cid:6) ASx,i and
ASy,i (cid:6) ASz,i.
Data collection. We have collected data from 10 users 1.
Each user was asked to use Movee, following the instructions
shown on the screen: move the device in one direction, for 6
seconds. We have collected 10 well deﬁned (6s long) samples
from each user; the total of 100 samples are stored in a “gen-
uine” dataset. We have created two test datasets. The ﬁrst
dataset, that we call the “random” dataset, contains 50 gen-
uine samples and 50 fraudulent samples created according
to the Random attack. Each fraudulent “random” sample is
created from one genuine sample, by coupling its video with
the inertial sensor data of another, randomly chosen sample.
The second dataset, called the “direction sync” set, contains
the other 50 genuine samples and 50 fraudulent samples cre-
ated according to the Direction Sync attack: Each fraudu-
lent sample couples the video of one genuine sample with
the inertial sensor data of another genuine sample, with the
same direction of movement. This is eﬀectively modeling
the scenario of one user taking the video and another user
(the attacker) emulating the movement in the video.
6.2 Experiment Setup
The Classiﬁcation module (see Section 3.4) runs trained
classiﬁers to determine whether there is suﬃcient evidence
that a video stream is genuine. We have used three classi-
ﬁers, Multilayer Perceptron (MLP) [26], Decision Tree (C4.5)
and Random Forest (RF) [16].
We have applied 10-fold cross-validation tests [30] to as-
sess how the results of the statistical analysis will generalize
to an independent data set. We have used the Weka version
3.7.9 data mining suite [11] to perform the experiments, with
default settings: For the backpropagation algorithm of the
MLP classiﬁer, we set the learning rate to 0.3 and the mo-
mentum rate to 0.2.
6.3 Metrics
We brieﬂy deﬁne the metrics we use to evaluate the accu-
racy of Movee. We borrow several metrics from biometrics.
The Receiver Operating Characteristic (ROC) curve [43] is
a visual characterization of the trade-oﬀ between the False
Accept Rate (FAR) and the False Reject Rate (FRR). The
Equal Error Rate (EER) [41] is the rate at which both accept
and reject errors are equal. A lower EER denotes a more
accurate solution. The area under the ROC curve (AUC)
1Of the 10 users, 7 are males and 3 females, aged 23-32,
occupation ranging from biology to fashion design, housewife
and software, civil and electrical engineering
245
)
s
m
(
e
m
i
t
n
o
i
t
u
c
e
x
e
e
g
a
r
e
v
A
0
0
0
1
0
0
8
0
0
6
0
0
4
0
0
2
0
VMA
IMA
Stretching
Calibration
DTW+Penalization
Classification
)
%
(
y
c
a
r
u
c
c
A
0
2
1
0
0
1
0
8
0
6
0
4
0
2
0
Mixed Dataset
Hard Dataset
VMA
IMA
SC
Classification (J48)
Movee Modules
MLP