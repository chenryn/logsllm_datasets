nicate via a shared object called the chain. We annotated any
modiﬁcation to this shared object with an output annotation.
We annotate the public and private keys of each player:
KeyPair pair = keyGenerator.generateKeyPair();
this.keyPair = new KeyPair(
@input ‘‘pubKey’’ pair.getPublic (),
@input ‘‘privKey’’ pair .getPrivate ());
the use of private key for signatures,
We also re-label
indicating that it is a permitted use of the private key:
dsa.initSign (@relabel ‘‘privKeyForSigning’’ privKey);
193
The inferred policy for the program indicates that the
only sensitive information being communicated to the other
player is the use of the private key for signature creation;
the private key does not otherwise get revealed.
output chain (cid:55)→Reveal(privKeyForSigning[0+])
We provided signatures for the cryptographic classes
that are used in the program (e.g., java. security .KeyPair,
java. security .Signature). Our signatures indicated that there
was no information ﬂow from, for example, a signing key to
the message digest. This lack of information ﬂow is correct
under symbolic models of cryptography (and corresponds to
only negligible information ﬂows in computational models).
However, in an implementation, the ciphertext is clearly
computed from the plaintext, and so our analysis conserva-
tively concludes that the ciphertext reveals (non-negligible)
information about the plaintext. Thus, our analysis would not
be able to infer the signatures of the cryptographic library
from the library implementation.
G. Comparison with Jif policies
Two of the case studies, Battleship and Mental Poker,
have equivalent Jif versions, with security annotations from
the decentralized label model (DLM) [21]. The security
guarantees offered by DLM policies differ signiﬁcantly from
the guarantees offered by our security policies. Our security
policies focus on what
information can be revealed by
program execution, whereas DLM policies focus on who
may learn and release information. For example, in the Jif
Battleship program, Player 1’s board is annotated as readable
only by Player 1, and so releasing information to Player 2
requires an explicit declassiﬁcation annotation in a code
context with Player 1’s authority. The Jif annotations do not
directly describe what information is declassiﬁed.
VI. RELATED WORK
In this work, we use static analysis to infer declassiﬁcation
policies that a program satisﬁes. Our declassiﬁcation policies
specify strong information-ﬂow security, yet are simple
and intuitive. Sabelfeld and Myers [7] survey language-
based techniques for static reasoning about information-ﬂow
security. We focus on two areas of related work: speciﬁcation
of declassiﬁcation policies and inference of security policies.
Declassiﬁcation policies. Declassiﬁcation, or informa-
tion release, occurs when sensitive information is made
more public. Declassiﬁcation violates the semantic security
condition of noninterference [10], yet commonly occurs in
systems that handle sensitive information. Sabelfeld and
Sands [9] survey semantic security conditions for declas-
siﬁcation, and categorize them based on: what information
may be revealed, when the information may be revealed, who
controls the release, and where in the system or program
the release occurs. Our policies specify what information
may be released, characterized as expressions whose eval-
uation an observer may learn. Through conditional poli-
cies (if d then p1 else p2) we can express certain conditions
under which release may or may not occur (to wit, con-
ditions determined by program inputs), a form of when-
declassiﬁcation. Our policies can also express what code
must be executed in order for the release to occur: a limited
form of where-declassiﬁcation.
The most closely related security condition is localized
delimited release [22, 23] which uses escape hatch expres-
sions to specify what secret information may be released,
and requires release to occur only at declassify commands.
The security condition requires that when an output occurs,
the observer can only learn the valuation of escape hatch
expressions for which an appropriate declassify command
has been executed. This is similar to satisfying the se-
curity policy if-executed release-e then Reveal(e), where
release-e is a mark command that occurs immediately
before any declassiﬁcation of e.
Rocha et al. [24] use graphs to describe how output values
are allowed to depend on inputs. Like us, they seek to
ease the annotation burden on the programmer; whereas we
focus on policy inference, they focus on allowing the user
to specify the graph policy separately from the code, and
then to verify whether unannotated code satisﬁes the policy.
Sabelfeld and Sands [9] present prudent principles for
declassiﬁcation security conditions. We satisfy semantic
consistency, conservativity, and non-occlusion; the principle
of monotonicity of release is not applicable, as our programs
have no declassiﬁcation annotations.
Inference of security policies. Backes et al. [25] present
an analysis that automatically discovers an equivalence
relation characterizing the secret
information a program
may reveal. They quantify a program’s information ﬂow by
computing sizes of the equivalence classes. While potentially
very precise, such quantitative policies may be difﬁcult to
interpret; in contrast our policies are qualitative and we have
emphasized readability and intuition in their design.
Banerjee et al. [26] suggest model checking to determine
if programs satisfy declassiﬁcation policies expressed as
abstraction functions; counter-examples produced by the
model checker allow the declassiﬁcation policy to be reﬁned,
and the process repeated, to determine of the least amount
of information that a program declassiﬁes. As with Backes
et al., we believe the key difference is that our declassiﬁca-
tion policies represent a better trade-off between precision,
intuitiveness, and ease of inference.
King et al. [16] statically infer information ﬂows to inves-
tigate the precision of security-type checking with respect to
implicit information ﬂows. Although they perform a context-
sensitive analysis, they infer ﬂow-insensitive types: within
a given context, a variable is assumed to always contain
information at
the same security level. By contrast, we
infer ﬁner-grained policies, similar to ﬂow-sensitive security
types [27].
Pottier and Conchon [28] describe how to extend existing
type systems with information security, enabling standard
type inference algorithms to infer security types. However,
such algorithms are unable to take full advantage of our
precise security policies. For example, conditional polices
incorporate path-sensitive information that is unavailable in
standard typing disciplines.
Liu and Milanova [29, 30] and Livshits et al. [31] present
static analyses to infer explicit information ﬂows. Although
the analyses are efﬁcient and practical, they do not track
implicit ﬂows, and so it is unclear what security guarantees
they provide. For example, an analysis of the password
checking program from the Introduction that ignores implicit
ﬂows may conclude that the attacker learns nothing about
the secret password.
Smith and Thober [32] perform type-inference for a highly
polymorphic object-oriented security-type system. Top-level
security policies are speciﬁed independently of code, and
inferred types are used to determine whether the program
is secure, given the policy. Like us, they seek to reduce the
programmer burden. Although they simplify policy enforce-
ment, the programmer must explicitly state security policies.
By contrast, we aim to infer policies, which further reduces
programmer burden.
King et al. [33] present a model for information-ﬂow
blame that aids identiﬁcation of code that violates security.
Information-ﬂow blame helps the programmer understand
information ﬂows in a program. However, it requires that
program types are annotated with security policies, and it
cannot infer policies or provide security guarantees if the
program fails to type check.
Tschantz and Wing [34] develop a tool that analyzes
C-language programs and infers incident-insensitive non-
interference policies, which allow the existence of high-
security data, but not the data itself, to be revealed. Their
tool discovers a set of program traces that violate incident-
insensitive noninterference, and works over a subset of C.
VII. CONCLUSION AND FUTURE WORK
This work demonstrates that is possible to infer precise
and expressive information-ﬂow policies for Java programs.
Key contributions include deﬁning an expressive policy
language that is suited for precise inference, deﬁning an
dataﬂow inference algorithm, and implementing the analysis
for Java.
Several avenues for further research remain open.
It can be difﬁcult to understand how security annotations
affect inferred policies and and how non-local control ﬂow
(e.g., exceptions) lead to implicit ﬂows. One possible way
to mitigate these difﬁculties would be to be build more
sophisticated user interfaces for the analysis tools.
194
Policy inference as described in this paper relies on
expensive program analyses. Scalability may be improved by
moving to a ﬂow insensitive analysis. Fortunately, artifacts
developed in this work provide a promising testbed for
evaluating scalable techniques.
ACKNOWLEDGMENTS
We thank Aslan Askarov and Andrei Sabelfeld for sharing
the mental poker source code. We thank our reviewers and
shepherd for useful feedback. This research is sponsored
by the Air Force Research Laboratory. This research is
supported by the National Science Foundation under Grant
No. 1054172.
REFERENCES
[1] A. Askarov and A. Sabelfeld, “Localized delimited release: com-
bining the what and where dimensions of information release,” in
Proc. 2007 Workshop on Programming Languages and Analysis for
Security. New York, NY, USA: ACM Press, 2007.
[2] K. R. O’Neill, M. R. Clarkson, and S. Chong, “Information-ﬂow
security for interactive programs,” in CSFW ’06.
IEEE, 2006.
[3] A. C. Myers, A. Sabelfeld, and S. Zdancewic, “Enforcing robust
declassiﬁcation,” in Proc. Computer Security Foundations Workshop,
2004.
[4] R. Giacobazzi and I. Mastroeni, “Abstract non-interference: param-
eterizing non-interference by abstract interpretation,” in POPL ’04.
New York, NY, USA: ACM, 2004.
[5] D. Clark, S. Hunt, and P. Malacaria, “Quantiﬁed interference for a
while language,” Electronic Notes in Theoretical Computer Science,
vol. 112, Jan. 2005.
[6] S. Chong and A. C. Myers, “End-to-end enforcement of erasure and
declassiﬁcation,” in CSF ’08.
IEEE, 2008.
[7] A. Sabelfeld and A. Myers, “Language-based information-ﬂow secu-
rity,” IEEE Journal on Selected Areas in Communications, vol. 21,
no. 1, Jan. 2003.
[8] A. Askarov and A. Sabelfeld, “Security-typed languages for imple-
mentation of cryptographic protocols: A case study,” in ESORICS
’05, 2005.
[9] A. Sabelfeld and D. Sands, “Dimensions and principles of declassi-
ﬁcation,” in CSFW ’05.
IEEE, 2005.
[10] J. A. Goguen and J. Meseguer, “Security policies and security
IEEE
models,” in Proc. IEEE Symposium on Security and Privacy.
Computer Society, Apr. 1982.
[11] M. R. Clarkson and F. B. Schneider, “Hyperproperties,” in Proc. 21st
IEEE Computer
IEEE Computer Security Foundations Symposium.
Society, Jul. 2008.
[12] D. E. Denning and P. J. Denning, “Certiﬁcation of programs for
secure information ﬂow,” Communications of the ACM, vol. 20, no. 7,
Jul. 1977.
[13] A. C. Myers, L. Zheng, S. Zdancewic, S. Chong, and N. Nystrom,
“Jif: Java information ﬂow,” 2001–2009, software release. Located
at http://www.cs.cornell.edu/jif.
[14] N. Nystrom, M. Clarkson, and A. C. Myers, “Polyglot: An exten-
sible compiler framework for Java,” in Compiler Construction, 12th
International Conference, CC 2003, ser. Lecture Notes in Computer
Science, G. Hedin, Ed., no. 2622. Warsaw, Poland: Springer-Verlag,
Apr. 2003.
[15] A. Milanova, A. Rountev, and B. G. Ryder, “Parameterized object
sensitivity for points-to analysis for Java,” ACM Transactions on
Software Engineering and Methodology, vol. 14, no. 1, 2005.
[16] D. King, B. Hicks, M. Hicks, and T. Jaeger, “Implicit ﬂows:
Can’t live with ’em, can’t live without ’em,” in Proc. International
Conference on Information Systems Security (ICISS), ser. LNCS, vol.
5352, Dec. 2008.
[17] K. D. Cooper, T. J. Harvey, and K. Kennedy, “A simple, fast
dominance algorithm,” Software Practice & Experience, vol. 4, 2001.
[18] B. De Sutter, L. Van Put, and K. De Bosschere, “A practical interpro-
cedural dominance algorithm,” ACM Transactions on Programming
Languages and Systems, vol. 29, no. 4, 2007.
[19] A. C. Myers, “JFlow: Practical mostly-static information ﬂow con-
trol,” in POPL ’99. New York, NY, USA: ACM Press, 1999.
[20] Y. Smaragdakis, M. Bravenboer, and O. Lhot´ak, “Pick your con-
texts well: understanding object-sensitivity,” in Proceedings of the
38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of
Programming Languages. New York, NY, USA: ACM, 2011, pp.
17–30.
[21] A. C. Myers and B. Liskov, “Complete, safe information ﬂow with
decentralized labels,” in Proc. IEEE Symposium on Security and
Privacy.
IEEE Computer Society, May 1998.
[22] A. Askarov and A. Sabelfeld, “Gradual release: Unifying declas-
siﬁcation, encryption and key release policies,” in Proc. IEEE
Symposium on Security and Privacy.
IEEE Computer Society, 2007.
[23] ——, “Tight enforcement of information-release policies for dy-
namic languages,” in Proc. 22nd IEEE Computer Security Founda-
tions Workshop, 2009.
[24] B. P. S. Rocha, S. Bandhakavi, J. d. Hartog, W. H. Winsborough, and
S. Etalle, “Towards static ﬂow-based declassiﬁcation for legacy and
untrusted programs,” in Proceedings of the 2010 IEEE Symposium
on Security and Privacy. Washington, DC, USA: IEEE Computer
Society, 2010.
[25] M. Backes, B. K¨opf, and A. Rybalchenko, “Automatic discovery
and quantiﬁcation of information leaks,” in Proc. 2009 30th IEEE
Symposium on Security and Privacy. Washington, DC, USA: IEEE
Computer Society, 2009.
[26] A. Banerjee, R. Giacobazzi, and I. Mastroeni, “What you lose is what
you leak: Information leakage in declassiﬁcation policies,” Electronic
Notes in Theoretical Computer Science, vol. 173, 2007.
[27] S. Hunt and D. Sands, “On ﬂow-sensitive security types,” in Con-
ference Record of
the Thirty-Third Annual ACM Symposium on
Principles of Programming Languages. New York, NY, USA: ACM
Press, Jan. 2006.
[28] F. Pottier and S. Conchon, “Information ﬂow inference for free,” in
Proc. 5th ACM SIGPLAN International Conference on Functional
Programming. New York, NY, USA: ACM Press, 2000.
[29] Y. Liu and A. Milanova, “Static analysis for inference of explicit
information ﬂow,” in Proc. 8th ACM SIGPLAN-SIGSOFT Workshop
on Program Analysis for Software Tools and Engineering. New
York, NY, USA: ACM, 2008.
[30] ——, “Practical static analysis for inference of security-related
program properties,” in Proc. IEEE 17th International Conference
on Program Comprehension, May 2009.
[31] B. Livshits, A. V. Nori, S. K. Rajamani, and A. Banerjee, “Merlin:
Speciﬁcation inference for explicit information ﬂow problems,” in
PLDI ’09, 2009.
[32] S. F. Smith and M. Thober, “Improving usability of information
ﬂow security in Java,” in Proc. 2007 Workshop on Programming
Languages and Analysis for Security. New York, NY, USA: ACM
Press, 2007.
[33] D. King, T. Jaeger, S. Jha, and S. A. Seshia, “Effective blame for
information-ﬂow violations,” in Proc. 16th ACM SIGSOFT Interna-
tional Symposium on Foundations of Software Engineering, 2008.
[34] M. C. Tschantz and J. M. Wing, “Extracting conditional conﬁden-
tiality policies,” in SEFM ’08: Proc. 2008 Sixth IEEE International
Conference on Software Engineering and Formal Methods. Wash-
ington, DC, USA: IEEE Computer Society, 2008.
195