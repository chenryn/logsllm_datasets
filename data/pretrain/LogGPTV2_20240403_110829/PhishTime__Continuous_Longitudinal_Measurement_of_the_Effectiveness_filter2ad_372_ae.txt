100.0%
60.5%
49.4%
44.0%
46.3%
59.3%
50.6%
59.7%
37.7%
51.9%
58.0%
58.0%
73.5%
66.7%
37.7%
45.7%
41.7%
6.5%
0.0%
0.0%
0.0%
42.6%
57.8%
0.0%
0.0%
0.0%
25.9%
0.0%
24.1%
0.0%
9.3%
0.0%
4.3%
0.0%
3.7%
0.0%
0.0%
51.9%
13.0%
0.0%
0.0%
0.0%
0.0%
02:48
03:30
02:39
03:02
12:45
03:09
09:35
08:51
10:01
06:51
11:56
06:59
11:48
10:07
12:19
02:29
02:43
-
03:06
-
10:47
00:55
00:56
00:38
01:40
02:46
01:57
02:55
01:31
02:33
00:41
02:21
27:24
01:50
00:57
01:53
02:41
00:49
-
-
-
00:40
17:30
-
-
-
43:51
-
11:46
-
11:19
-
00:01
-
46:28
-
-
04:53
00:35
-
-
-
-
Experiment G
(Reporting Methods) Chrome Suspicious Site Reporter (CSSR)
20.4%
90.7%
00:38
10:13
0.0%
0.0%
-
-
0.0%
0.0%
-
-
20.4%
90.7%
00:17
10:17
20.4%
90.7%
0.0%
0.0%
0.0% coverage
Table 4: Blacklist performance aggregated by each batch (average of all deployments).
of the six deployments. Opera remained consistent in terms of
speed across (cid:27)ve deployments. For GSB and SmartScreen, we
applied ANOVA to each respective set of raw baseline desktop
blacklist speed observations (as aggregated in Table 2), treat-
ing each deployment as a separate group. We found the di(cid:29)er-
ences to be statistically signi(cid:27)cant, with a p-value below 0.01
for both tests. We believe that these variations—even if rela-
tively small—stem from the complexity of the underlying anti-
phishing systems, the order in which reports are processed,
anddatasharingmethodsacrosstheecosystem[64]. However,
except for GSB in mobile Firefox,blacklists in mobile browsers
did not prove to be consistent with their desktop counterparts.
Also, notably, coverage dropped dramatically during
Deployment 3, both for non-evasive and evasive phishing,
as shown in Tables 2 and 3. In analyzing this anomaly, we
(cid:27)rst ruled out technical issues and con(cid:27)rmed that all of our
e-mail and API reports were successfully delivered. We also
redeployed Experiment A with prior domains and reproduced
the degraded coverage. After analyzing the results of
single-entity reporting in Experiment E (summarized in
Table 4), we found that the coverage from reports sent
directly to PayPal was similarly low: its coverage in GSB was
9.3% in Deployment 3, down from 44.4% in Deployment 1.
Upon comparing crawler tra(cid:28)c between these deployments,
we found that crawler activity as a result of PayPal reports
was absent from the majority of websites in Deployment 3.
Although we cannot rule out other ecosystem factors, we
believe that this absence was a key cause of the overall
coverage drop,and we disclosed it to PayPal (we later received
acknowledgment of the issue, which was ultimately resolved).
Baseline coverage recovered in subsequent deployments,
except for a single website in Deployment 4 that failed to be
blacklisted by GSB. Despite being blacklisted by SmartScreen
and crawled by numerous other entities, it was never crawled
by GSB: our original GSB report was likely never acted on,
and GSB did not appear to discover the URL through the other
entities to whichwe reported. Thoughan outlier,this suggests
thatthe ecosystem may bene(cid:27)tfrom more robustdata sharing.
Blacklist Persistence. Across all of our deployments,
once a URL was blacklisted in a particular blacklist, we did
not observe de-blacklisting during the deployment or within
one week immediately thereafter. After the conclusion of our
(cid:27)nal deployment, we served 404 errors from all of our URLs
and monitored them for an extended period. We found that
the earliest removal from blacklists occurred 29 days after
we had originally reported the respective URL.
We suspectthatde-blacklisting may dependon factors such
as the presence of benign content on a domain, the domain’s
reputation, or whether the web server is online. Although
our experiments were not designed to pinpoint criteria for de-
blacklisting, we believe that the premature removal of phish-
ing URLs from blacklists is currently not a signi(cid:27)cant issue.
8.3 Typical Evasion Techniques
In Table 4, we show blacklist performance for the speci(cid:27)c
batches within each experiment, aggregated across all
deployments. This allows us to compare speed and coverage
when blacklists are faced with di(cid:29)erent evasion techniques.
Websites with mobile user agent cloaking (in Experiment
B) had a negligible e(cid:29)ect on desktop blacklisting compared to
the unevasive Experiment A (if we disregard the skew from
Deployment 3): modern blacklists can, in fact, reliably detect
phishing websites with certain basic evasion techniques.
Interestingly, however, both GSB and Opera had 0% coverage
on mobile devices across all deployments of Experiment B,
which is very undesirable given that Experiment B websites
were con(cid:27)gured to be accessible exclusively on mobile de-
vices. In Figure 6, we visualize blacklisting of these websites
in each blacklist over the full duration of our deployments.
In Experiment C, we tested the addition of three types
of redirection on top of the basic evasion techniques in
Experiment B. For brevity, we focus our discussion on
blacklisting by GSB on desktop, and we use the average speed
and coverage across all deployments of Experiment B (00:59
and 94.2%, respectively), calculated per Table 4, as a point of
comparison. On average, redirection via bit.ly links slowed
388    29th USENIX Security Symposium
USENIX Association
Figure 6: Comparison of all blacklists’ aggregate performance
for uncloaked websites vs. websites with Mobile cloaking.
blacklisting speedofthe corresponding landing pages to 02:58,
and reduced coverage to 86.1%. Redirection via .com domain
names slowed the speed to 02:48 and reduced coverage to
88.9%. By adding .htaccess cloaking on top of redirection,
speed only slowed to 02:43, but coverage fell further to 84.3%.
As shown in Table 4, the blacklisting speed of the correspond-
ing lures was at least 1 hour faster in each case; however,
attackers’ ability to easily generate many lures increases the
importance of blacklisting the actual landing pages [61].
In Experiment D, we re-deployed phishing websites on the
same .com domains as in Experiment C, but with di(cid:29)erent
paths, to simulate how attackers might re-use compromised
domains in the wild. Although we observed increased speed
and coverage compared to Experiment C, the speed remained
slower than in experiments without redirection. Only 4.3%
of URLs in Experiment D were blacklisted immediately
upon deployment, which may represent a gap exploitable
by phishers. In Figure 7, we visualize the di(cid:29)erence in GSB
desktop blacklisting of the cloaking techniques considered in
this section. To maintain consistency,we exclude Deployment
3 from the (cid:27)gure. For clarity, we also omit the batches with
bit.ly links,as theyfollowedthe same trendas .com redirection
links, and were only blacklisted slightly more slowly.
In mobile Chrome and mobile Safari, Experiment C cover-
agerangedfromjust9.3%to25.9%andwas8to40hoursslower
than on desktop. Only landing pages, rather than lures, were
blacklisted. Interestingly, in Experiment D, coverage dropped
to a range of 3.7% to 4.3%, despite the ecosystem’s knowledge
of our domains from previously blacklisted URLs. We discuss
the implications of these inconsistencies in Section 9.
Figure 7: Comparison of aggregate speed and coverage of
GSB against the di(cid:29)erent evasion techniques we tested.
Experiment C, and 88.9% of the links within Experiment D,
with an average speed of 11:36 (far slower than the tested
blacklists). After we switched to a new (non-disabled) API
key for subsequent links, no other links were disabled during
our research, except for a single URL during Deployment 3.
We disclosed these (cid:27)ndings to bit.ly but received no response.
8.4 Emerging Evasion Techniques
None of the batches of sophisticated cloaking techniques
within Experiment F saw any blacklisting, except for one
batch with CAPTCHA cloaking, which had 42.6% coverage
in SmartScreen only (shown in Table 4). Upon further investi-
gation, we discovered that SmartScreen’s detection occurred
due to its classi(cid:27)cation of obfuscation within the CAPTCHA
JavaScript code as malware. Because such detection can triv-
ially be bypassed [66], we believe that behavior-based evasion
techniques represent a threat to the anti-phishing ecosystem.
A fundamental distinction between the advanced cloaking
techniques in Experiment F and the other experiments is
that they require interaction from a human user to trigger
the display of phishing content (i.e., clicking on a button,
solving a CAPTCHA challenge, or moving the mouse). Such
behaviors might be typical of a human user (and may not even
raise suspicion if the user is successfully fooled by an e-mail
lure, or if the landing page matches the look-and-feel of the
impersonated organization). However, web crawlers would
need to be specially developed to emulate such behaviors or
otherwise (cid:27)ngerprint such cloaking.
8.5 Single-entity Reporting
In Experiment E, we observed clear di(cid:29)erences in blacklist
response when comparing reporting to the APWG (only) with
reporting to PayPal (only), as shown in Table 4. Even if we ex-
clude the problematic performance of PayPal during Deploy-
ment 3 (as discussed in Section 8.2), reporting to the APWG
resulted in higher coverage across all blacklists and more
crawler tra(cid:28)c to each website. However, the speed of GSB
blacklisting after reporting to PayPal was 01:31 faster than
Overall, we observe that delays and gaps exist in the
blacklisting of typical phishing websites: these gaps provide
a prime opportunity for attackers to successfully target
their victims [46], help explain the prevalence of evasion
techniques, and should be addressed by the ecosystem.
Disabling of Bit.ly Links. To deter abuse, bit.ly disables
redirection links that point to known malicious content.
During Deployment 1, bit.ly disabled 98.1% of the links within
USENIX Association
29th USENIX Security Symposium    389
Figure 8: Comparison of traditional URL-only reporting with
evidence-based reporting in Google Chrome (Experiment G).
that of the APWG. This suggests that between entities, there
exist di(cid:29)erent implementations for propagating reported
URLs to blacklists. Due to each entity’s unique strengths, we
believe it is important to report phishing to multiple entities.
8.6 Evidence-based Reporting
In Figure 8 and Table 4, we compare the di(cid:29)erence in GSB
speed and coverage between traditional URL reporting
and evidence-based reporting through CSSR [11] from
Experiment G (note that we limit the x-axis in the (cid:27)gure as
coverage did not increase after 24 hours).
We observe that the two reporting approaches each re-
sulted in a distinct crawler response and subsequent blacklist
performance. Traditional URL reporting was followed by an
immediate burst of crawler tra(cid:28)c and a negligible amount of
crawler tra(cid:28)c in the hours thereafter. Even though 50% of the
phishing websites we reported were successfully retrieved by
a crawler, only 20.4% were ultimately blacklisted. The earliest
blacklisting occurred20 minutes afterreporting,andcoverage
stopped growing after approximately four hours. Reporting
through CSSR yielded a slower initial speed, but resulted not
onlyin90.7%coveragewithin24hours,butalsoasubstantially
higher amount of crawler tra(cid:28)c, spread over a long period of
time, with 47.5% fewer requests being denied by cloaking. The
earliest blacklisting occurred 50 minutes after reporting, and
coverage matched that of the traditional reporting approach
by the (cid:27)ve-hour mark. Although we did not repeat these
measurements over time, we found the di(cid:29)erences in the
observed distributions of blacklist speeds to be signi(cid:27)cant
in the Mann-Whitney U test [39], with a p-value below 0.01.
8.7 Crawler Tra(cid:28)c
Between May 2019 and January 2020, the 4,158 URLs in our
main deployments received a total of 2.14 million HTTP
requests from 41,750 distinct web crawler IPs. An additional
20.50 million requests came from our monitoring infrastruc-
ture to check our websites’ blacklist status. Our websites
Figure 9: Cumulative Distribution Function of crawler tra(cid:28)c
to our phishing websites, across all deployments.
remained online for the duration of our deployments (i.e.,
were nottaken down [1]) as we hadmade ourhosting provider
aware of the nature of our research. Over time, across all de-
ploymentsexceptthethird,wealsoobservedaconsistent,very