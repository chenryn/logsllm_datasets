packets gets sampled. We assume based on [16] that Net-
Flow can handle packets no smaller than 40 bytes at OC-3
speeds.
10Of course, technology and cost impose limitations on the
amount of available SRAM but the current limits for on and
oﬀ-chip SRAM are high enough for our algorithms.
331Measure
Sample and hold Multistage ﬁlters
Sampled NetFlow
Exact measurements
/ longlived%
Relative error
Memory bound
Memory accesses
1.41/O
2O/z
1
longlived%
/ 1/u
0
√
0.0088/
zt
2/z + 1/z log10(n) min(n,486000 t)
1 + log10(n)
1/x
Table 2: Comparison of traﬃc measurement devices
iii) Provable Lower Bounds: A possible disadvantage
of Sampled NetFlow is that the NetFlow estimate is not an
actual lower bound on the ﬂow size. Thus a customer may be
charged for more than the customer sends. While one can
make the average overcharged amount arbitrarily low (us-
ing large measurement intervals or other methods from [5]),
there may be philosophical objections to overcharging. Our
algorithms do not have this problem.
iv) Reduced Resource Consumption: Clearly, while
Sampled NetFlow can increase DRAM to improve accuracy,
the router has more entries at the end of the measurement
interval. These records have to be processed, potentially ag-
gregated, and transmitted over the network to the manage-
ment station. If the router extracts the heavy hitters from
the log, then router processing is large; if not, the band-
width consumed and processing at the management station
is large. By using fewer entries, our algorithms avoid these
resource (e.g., memory, transmission bandwidth, and router
CPU cycles) bottlenecks.
v) Faster detection of long-lived ﬂows: In a security
or DoS application, it may be useful to quickly detect a
large increase in traﬃc to a server. Our algorithms can
use small measurement intervals and detect large ﬂows soon
after they start. By contrast, Sampled NetFlow can be much
slower because with 1 in N sampling it takes longer to gain
statistical conﬁdence that a certain ﬂow is actually large.
6. DIMENSIONING TRAFFIC MEASURE-
MENT DEVICES
We describe how to dimension our algorithms. For appli-
cations that face adversarial behavior (e.g., detecting DoS
attacks), one should use the conservative bounds from Sec-
tions 4.1 and 4.2. Other applications such as accounting can
obtain greater accuracy from more aggressive dimensioning
as described below. Section 7 shows that the gains can be
substantial. For example the number of false positives for
a multistage ﬁlter can be four orders of magnitude below
what the conservative analysis predicts. To avoid a priori
knowledge of ﬂow distributions, we adapt algorithm param-
eters to actual traﬃc. The main idea is to keep decreasing
the threshold below the conservative estimate until the ﬂow
memory is nearly full (totally ﬁlling memory can result in
new large ﬂows not being tracked).
Figure 5 presents our threshold adaptation algorithm. There
are two important constants that adapt the threshold to
the traﬃc: the “target usage” (variable target in Figure 5)
that tells it how full the memory can be without risking ﬁll-
ing it up completely and the “adjustment ratio” (variables
adjustup and adjustdown in Figure 5) that the algorithm
uses to decide how much to adjust the threshold to achieve
a desired increase or decrease in ﬂow memory usage. To give
stability to the traﬃc measurement device, the entriesused
ADAPTTHRESHOLD
usage = entriesused/f lowmemsize
if (usage > target)
threshold = threshold ∗ (usage/target)adjustup
else
if (threshold did not increase for 3 intervals)
threshold = threshold ∗ (usage/target)adjustdown
endif
endif
Figure 5: Dynamic threshold adaptation to achieve
target memory usage
variable does not contain the number of entries used over
the last measurement interval, but an average of the last 3
intervals.
Based on the measurements presented in [6], we use a
value of 3 for adjustup, 1 for adjustdown in the case of
sample and hold and 0.5 for multistage ﬁlters and 90% for
target.
[6] has a more detailed discussion of the threshold
adaptation algorithm and the heuristics used to decide the
number and size of ﬁlter stages. Normally the number of
stages will be limited by the number of memory accesses
one can perform and thus the main problem is dividing the
available memory between the ﬂow memory and the ﬁlter
stages.
Our measurements conﬁrm that dynamically adapting the
threshold is an eﬀective way to control memory usage. Net-
Flow uses a ﬁxed sampling rate that is either so low that a
small percentage of the memory is used all or most of the
time, or so high that the memory is ﬁlled and NetFlow is
forced to expire entries which might lead to inaccurate re-
sults exactly when they are most important: when the traﬃc
is large.
7. MEASUREMENTS
In Section 4 and Section 5 we used theoretical analysis
to understand the eﬀectiveness of our algorithms.
In this
section, we turn to experimental analysis to show that our
algorithms behave much better on real traces than the (rea-
sonably good) bounds provided by the earlier theoretical
analysis and compare them with Sampled NetFlow.
We start by describing the traces we use and some of the
conﬁguration details common to all our experiments.
In
Section 7.1.1 we compare the measured performance of the
sample and hold algorithm with the predictions of the ana-
lytical evaluation, and also evaluate how much the various
improvements to the basic algorithm help. In Section 7.1.2
we evaluate the multistage ﬁlter and the improvements that
apply to it. We conclude with Section 7.2 where we com-
332Trace
Number of ﬂows (min/avg/max)
5-tuple
destination IP
AS pair
MAG+ 93,437/98,424/105,814
MAG 99,264/100,105/101,038
IND
COS
13,746/14,349/14,936
5,157/5,497/5,784
40,796/42,915/45,299
43,172/43,575/43,987
7,177/7,401/7,775
7,353/7,408/7,477
8,723/8,933/9,081
1,124/1,146/1,169
-
-
Mbytes/interval
(min/avg/max)
201.0/256.0/284.2
255.8/264.7/273.5
91.37/96.04/99.70
14.28/16.63/18.70
Table 3: The traces used for our measurements
pare complete traﬃc measurement devices using our two
algorithms with Cisco’s Sampled NetFlow.
We use 3 unidirectional traces of Internet traﬃc: a 4515
second “clear” one (MAG+) from CAIDA (captured in Au-
gust 2001 on an OC-48 backbone link between two ISPs) and
two 90 second anonymized traces from the MOAT project of
NLANR (captured in September 2001 at the access points
to the Internet of two large universities on an OC-12 (IND)
and an OC-3 (COS)). For some of the experiments use only
the ﬁrst 90 seconds of trace MAG+ as trace MAG.
In our experiments we use 3 diﬀerent deﬁnitions for ﬂows.
The ﬁrst deﬁnition is at the granularity of TCP connections:
ﬂows are deﬁned by the 5-tuple of source and destination IP
address and port and the protocol number. This deﬁnition
is close to that of Cisco NetFlow. The second deﬁnition us-
es the destination IP address as a ﬂow identiﬁer. This is a
deﬁnition one could use to identify at a router ongoing (dis-
tributed) denial of service attacks. The third deﬁnition uses
the source and destination autonomous system as the ﬂow
identiﬁer. This is close to what one would use to determine
traﬃc patterns in the network. We cannot use this deﬁni-
tion with the anonymized traces (IND and COS) because
we cannot perform route lookups on them.
Table 3 describes the traces we used. The number of ac-
tive ﬂows is given for all applicable ﬂow deﬁnitions. The
reported values are the smallest, largest and average value
over the measurement intervals of the respective traces. The
number of megabytes per interval is also given as the small-
est, average and largest value. Our traces use only between
13% and 27% of their respective link capacities.
The best value for the size of the measurement interval
depends both on the application and the traﬃc mix. We
chose to use a measurement interval of 5 seconds in all our
experiments. [6] gives the measurements we base this deci-
sion on. Here we only note that in all cases 99% or more of
the packets (weighted by packet size) arrive within 5 seconds
of the previous packet belonging to the same ﬂow.
Since our algorithms are based on the assumption that a
few heavy ﬂows dominate the traﬃc mix, we ﬁnd it useful
to see to what extent this is true for our traces. Figure 6
presents the cumulative distributions of ﬂow sizes for the
traces MAG, IND and COS for ﬂows deﬁned by 5-tuples.
For the trace MAG we also plot the distribution for the case
where ﬂows are deﬁned based on destination IP address, and
for the case where ﬂows are deﬁned based on the source and
destination ASes. As we can see, the top 10% of the ﬂows
represent between 85.1% and 93.5% of the total traﬃc vali-
dating our original assumption that a few ﬂows dominate.
7.1 Comparing Theory and Practice
We present detailed measurements on the performance on
sample and hold, multistage ﬁlters and their respective op-
// //
100
c
i
f
f
a
r
t
f
o
e
g
a
t
n
e
c
r
e
P
90
80
70
60
50
0
MAG 5-tuples
MAG destination IP
MAG AS pairs
IND
COS
5
10
15
20
25
30
Percentage of flows
Figure 6: Cumulative distribution of ﬂow sizes for
various traces and ﬂow deﬁnitions
timizations in [6]. Here we summarize our most important
results that compare the theoretical bounds with the results
on actual traces, and quantify the beneﬁts of various opti-
mizations.
7.1.1 Summary of ﬁndings about sample and hold
Table 4 summarizes our results for a single conﬁguration:
a threshold of 0.025% of the link with an oversampling of
4. We ran 50 experiments (with diﬀerent random hash func-
tions) on each of the reported traces with the respective ﬂow
deﬁnitions. The table gives the maximum memory usage
over the 900 measurement intervals and the ratio between
average error for large ﬂows and the threshold.
The ﬁrst row presents the theoretical bounds that hold
without making any assumption about the distribution of
ﬂow sizes and the number of ﬂows. These are not the bounds
on the expected number of entries used (which would be
16,000 in this case), but high probability bounds.
The second row presents theoretical bounds assuming that
we know the number of ﬂows and know that their sizes have
a Zipf distribution with a parameter of α = 1. Note that the
relative errors predicted by theory may appear large (25%)
but these are computed for a very low threshold of 0.025%
and only apply to ﬂows exactly at the threshold.11
The third row shows the actual values we measured for
11We deﬁned the relative error by dividing the average error
by the size of the threshold. We could have deﬁned it by
taking the average of the ratio of a ﬂow’s error to its size
but this makes it diﬃcult to compare results from diﬀerent
traces.
333Algorithm
Maximum memory usage (entries)/ Average error
General bound
Zipf bound
Sample and hold
+ preserve entries
+ early removal
16,385 / 25%
7,441 / 25%
COS 5-tuple
MAG 5-tuple MAG destination IP MAG AS pair
16,385 / 25%
16,385 / 25%
16,385 / 25%
5,489 / 25%
5,081 / 25%
8,148 / 25%
714 / 24.40% 1,313 / 23.83% 710 / 22.17%
1,038 / 1.32% 1,894 / 3.04% 1,017 / 6.61%
803 / 1.18%
859 / 5.46%
1,964 / 24.07%
3,213 / 3.28%
2,294 / 3.16%
2,303 / 24.33%
3,832 / 4.67%
2,659 / 3.89%
IND 5-tuple
16,385 / 25%
6,303 / 25%
1,525 / 2.92%
Table 4: Summary of sample and hold measurements for a threshold of 0.025% and an oversampling of 4
the basic sample and hold algorithm. The actual memory
usage is much below the bounds. The ﬁrst reason is that
the links are lightly loaded and the second reason (partially
captured by the analysis that assumes a Zipf distribution of
ﬂows sizes) is that large ﬂows have many of their packets
sampled. The average error is very close to its expected
value.
The fourth row presents the eﬀects of preserving entries.
While this increases memory usage (especially where large
ﬂows do not have a big share of the traﬃc) it signiﬁcantly
reduces the error for the estimates of the large ﬂows, because
there is no error for large ﬂows identiﬁed in previous inter-
vals. This improvement is most noticeable when we have
many long lived ﬂows.
The last row of the table reports the results when pre-
serving entries as well as using an early removal threshold
of 15% of the threshold (our measurements indicate that
this is a good value). We compensated for the increase in
the probability of false negatives early removal causes by
increasing the oversampling to 4.7. The average error de-
creases slightly. The memory usage decreases, especially in
the cases where preserving entries caused it to increase most.
We performed measurements on many more conﬁgura-
tions, but for brevity we report them only in [6]. The results
are in general similar to the ones from Table 4, so we on-
ly emphasize some noteworthy diﬀerences. First, when the
expected error approaches the size of a packet, we see signif-
icant decreases in the average error. Our analysis assumes
that we sample at the byte level. In practice, if a certain
packet gets sampled all its bytes are counted, including the
ones before the byte that was sampled.
Second, preserving entries reduces the average error by
70% - 95% and increases memory usage by 40% - 70%. These
ﬁgures do not vary much as we change the threshold or the
oversampling. Third, an early removal threshold of 15%
reduces the memory usage by 20% - 30%. The size of the
improvement depends on the trace and ﬂow deﬁnition and
it increases slightly with the oversampling.
7.1.2 Summary of ﬁndings about multistage ﬁlters
Figure 7 summarizes our ﬁndings about conﬁgurations with
a stage strength of k = 3 for our most challenging trace:
MAG with ﬂows deﬁned at the granularity of TCP connec-
tions. It represents the percentage of small ﬂows (log scale)
that passed the ﬁlter for depths from 1 to 4 stages. We
used a threshold of a 4096th of the maximum traﬃc. The
ﬁrst (i.e., topmost and solid) line represents the bound of