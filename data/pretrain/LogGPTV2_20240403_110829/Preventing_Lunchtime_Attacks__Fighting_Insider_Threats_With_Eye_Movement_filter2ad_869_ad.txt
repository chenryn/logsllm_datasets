makes it harder to mimic another person’s features without
having to circumvent liveness detection mechanisms. On the
other hand the feature becomes useless once another person
is able to copy it. Attacks on ﬁngerprint sensors, including
the iPhone’s TouchID feature, using mock ﬁngers created of
various materials have recently been shown to be feasible under
practical conditions [31], [32]. This is particularly dangerous
as copies of ﬁngerprints can be easily collected in an ofﬁce
environment, for example by lifting them off a coffee mug.
Another downside of hard biometrics lies in poor collectability
and high intrusiveness.
Facial Recognition may seem like a convenient method
to provide continuous authentication but is not feasible in
a high-security context due to imperfect liveness detection.
Attacks on facial recognition software are possible using simple
photographs [33] or more complex 3D video models [34].
Behavioral biometrics are typically less susceptible to these
kinds of replication attacks, but their performance with regard to
false accept rates (FAR) and false reject rates (FRR) often makes
them unsuitable for standalone authentication. This is a result of
the low time-stability of human behavior as well as noise effects
created by external distractions. One of the oldest behavioral
biometrics has been proposed in 1980 and exploits distinctive
keystroke patterns [35]. Since then extensive research based
on this biometric has been conducted using different classiﬁers
with static and dynamic texts in multiple environments. The
error rates are low for static texts, but increase rapidly for free-
form texts as many unpredictable pauses are introduced into
the typing process. Additionally templates are usually tied to
keyboard layouts and even physical devices. As the identifying
features are conceptually simple this type of identiﬁcation
can be imitated. The authors of [36] designed a software that
facilitates imitation attacks by providing positive and negative
Fig. 12: The ECDF plot shows that 92.2% of all attackers are
detected within the ﬁrst 40 seconds of using the system. The
system failed to detect 2.76% of attackers as their biometric
templates are very close to that of a legitimate user.
to only 45 seconds. Figure 12 shows that most attackers can be
detected even before this 45-second mark, as the number of the
attacker’s samples in the sliding window gradually increases.
While most attackers are detected quickly (92.2% within 40
seconds) the system fails to detect 2.76% of attackers within
the scope of our data (i.e., the system exhibits systematic false
negatives). These false negatives occur when the biometric
templates of two users are very close. This problem could be
dealt with by using a second biometric (see Section VII) that
is likely to be independent from eye movements. A framework
that allows combining several biometrics is described in [29].
These results are very encouraging and signiﬁcantly outper-
form related work both in terms of error rates and universality
(see Section VII for details). Our solution allows a ﬁne-grained
trade-off between classiﬁcation speed, accuracy, detection time
and resistance to imitation attacks. The time stability of our
features makes it possible to use old templates for an extended
period without having to frequently retrain the classiﬁer (which
would require extensive effort). As blindness is the only known
condition that prevents reliable eye tracking this makes our
biometric an excellent step towards universal and transparent
continuous authentication.
11
feedback depending on the difference between the attacker’s
and the user’s patterns. Two recent comprehensive surveys of
keystroke dynamics can be found in [37], [38].
Mouse movements have been extensively studied as a
potential behavioral biometric that can be combined particularly
well with keystroke patterns, as both traits are usually collected
at different times. A survey on the extensive body of work
can be found in [39]. The best accuracy has been reported
with a FAR of 0.36% and a FRR of 0% [40]. As the data
was collected on the test subjects’ own PCs it is questionable
whether the classiﬁer did not distinguish input devices instead
of subjects [41].
Given the increasing share of smartphones and tablets
keyboard and mouse are no longer used ubiquitously. A recent
study reported an equal error rate of 2-3% when identifying
subjects across sessions based on their stroke patterns on a
smartphone touchscreen [25]. A similar approach that also
tests the resistance to imitation attacks is described in [42].
However, the authors only account for observation, not for a
compromised user template.
There has been some work on the way the human body
modiﬁes electrical currents. The authors of [43] measure the
body’s response to an electric square pulse signal and report
100% accuracy over a static dataset and 88% over a dataset
that contains samples taken over several weeks. However, the
number of samples collected is extremely low. It is unclear
whether the accuracy stays at these levels when subjects are
monitored continuously. Similar work that uses bioimpedance
as a biometric reports a recognition rate of 90%, but requires
augmentation with hand geometry [44]. Furthermore, the scope
of the study was limited to a family-size study with up to 5
subjects.
Eye movements have previously been studied as an input
channel that is resistant to shouldersurﬁng attacks. These
systems still rely on a conventional PIN, a password or a
passphrase. The authors of [45] developed a system using a
Tobii 1750 gazetracker and report a password entry time of
9 to 12 seconds with error rates between 3 and 15%. Similar
work used eye gestures instead of passwords and reduced the
fraction of successful shouldersurﬁng attacks to 55% with an
average input time of 5.3 seconds [46].
Our work is perhaps most closely related to [47]. The
authors use a Tobii X120 gazetracker with a sampling rate
of 120Hz to capture a subject’s eye movements while he is
watching a movie and use short-term eye gaze direction to
construct feature vectors which are modeled using Gaussian
mixtures. Depending on the amount of training data an equal
error rate of 28.7 to 47.1% is reported. The authors do not state
whether the type of video affects the templates (e.g., whether
training and testing with different videos is possible). A different
approach by Cantoni et al. attempts to distinguish individuals
by the way they look at different images [22]. However, their
approach is not suitable for task-independent identiﬁcation
and they do not state to what degree these patterns change
over time, especially given the static nature of the pictures.
Using density and duration of ﬁxations as their main features
they report an EER of 27.06%. Liang et al. measure the eye’s
tracking behaviour when a moving stimulus is displayed [23].
They use the acceleration of eye movements while the subjects
are pursuing a moving shape as input to both Support Vector
Machines (SVM) and a Back-Propagation neural network. In
an experiment with ﬁve subjects they achieve an identiﬁcation
accuracy of 82%. However, their design requires the display
of speciﬁc stimuli and can not be adapted to general tasks or
continuous authentication. Furthermore they do not evaluate
the time stability of the user templates.
VIII. CONCLUSION
In this work we have contributed a set of 21 discriminative
features based on a person’s eye movement patterns. The
usefulness of these features is not limited to our design,
they can be used with a wide set of general
tasks like
web browsing or writing e-mails. We designed a controlled
experiment that accounts for different ways an inside attacker
can obtain information from a na¨ıve or colluding user, to aid
in impersonation attacks. Using gaze tracking data from our
experiments, we quantify the advantage an adversary has in
impersonating a user and test if the adversary has obtained
knowledge about the task the user normally performs. The
data collected during our experiments comes from 30 members
of the general public. The data shows that eye movements,
speciﬁcally the features we have presented, provide a rich
source of distinguishing information. Using data from a single
session we achieve an equal error rate of 3.98%. While the
effects of task familiarity are measurable, it does not allow
the attacker to circumvent our authentication system. In order
to test the time stability of our features we performed two
repetitions of the experiments, two weeks apart. Our results
indicate that users can be authenticated reliably over the entire
period. The universal nature of eye movements and the low
error rates make this biometric an excellent primitive, on which
to build other continuous authentication mechanisms.
ACKNOWLEDGEMENT
We would like to thank our shepherd Gianluca Stringhini
and the anonymous reviewers for their invaluable feedback.
This work was supported by the Engineering and Physical
Sciences Research Council [grant number EP/M50659X/1].
REFERENCES
[1]
I. G. Group, “Eye tracking and gaze interaction,” www.gazegroup.org.
[2] A. Duchowski, Eye tracking methodology: Theory and practice.
Springer, 2007, vol. 373.
[3] B. Cassin, M. L. Rubin, and S. Solomon, Dictionary of eye terminology.
Wiley Online Library, 1984.
[4] S. Martinez-Conde, S. L. Macknik, X. G. Troncoso, and T. A. Dyar,
“Microsaccades counteract visual fading during ﬁxation,” Neuron, vol. 49,
no. 2, pp. 297–305, 2006.
[5] R. Abadi and E. Gowen, “Characteristics of saccadic intrusions,” Vision
research, vol. 44, no. 23, pp. 2675–2690, 2004.
[6] A. Jones, R. Friedland, B. Koss, L. Stark, and B. Thompkins-Ober,
“Saccadic intrusions in alzheimer-type dementia,” Journal of neurology,
vol. 229, no. 3, pp. 189–194, 1983.
[7] B. A. Clementz, J. A. Sweeney, M. Hirt, and G. Haas, “Pursuit
gain and saccadic intrusions in ﬁrst-degree relatives of probands with
schizophrenia.” Journal of abnormal psychology, vol. 99, no. 4, p. 327,
1990.
[8] K. Rayner, C. M. Rotello, A. J. Stewart, J. Keir, and S. A. Duffy,
“Integrating text and pictorial information: eye movements when looking
at print advertisements.” Journal of Experimental Psychology: Applied,
vol. 7, no. 3, p. 219, 2001.
12
[9] M. Wedel and R. Pieters, “Eye ﬁxations on advertisements and memory
for brands: A model and ﬁndings,” Marketing science, vol. 19, no. 4,
pp. 297–312, 2000.
[31] C. Barral and A. Tria, “Fake ﬁngers in ﬁngerprint recognition: Glycerin
supersedes gelatin,” in Formal to Practical Security. Springer, 2009,
pp. 57–69.
[32] F. Rieger. (2013) Chaos computer club breaks apple touchid. [Online].
Available: http://www.ccc.de/en/updates/2013/ccc-breaks-apple-touchid/
[33] N. M. Duc and B. Q. Minh, “Your face is not your password face
authentication bypassing lenovo–asus–toshiba,” Black Hat Brieﬁngs,
2009.
[34] A. Boehm, D. Chen, M. Frank, L. Huang, C. Kuo, T. Lolic, I. Martinovic,
and D. Song, “Safe: Secure authentication with face and eyes,” in IEEE
PRISMS 2013, June 2013.
[35] R. S. Gaines, W. Lisowski, S. J. Press, and N. Shapiro, “Authentication
by keystroke timing: Some preliminary results,” DTIC Document, Tech.
Rep., 1980.
[36] C. M. Tey, P. Gupta, and D. GAO, “I can be you: Questioning the use
of keystroke dynamics as biometrics.” The 20th Annual Network &
Distributed System Security Symposium (NDSS 2013), 2013.
[37] D. Shanmugapriya and G. Padmavathi, “A survey of biometric
keystroke dynamics: Approaches, security and challenges,” arXiv preprint
arXiv:0910.0817, 2009.
[38] S. P. Banerjee and D. L. Woodard, “Biometric authentication and
identiﬁcation using keystroke dynamics: A survey,” Journal of Pattern
Recognition Research, vol. 7, no. 1, pp. 116–139, 2012.
[39] K. Revett, H. Jahankhani, S. T. de Magalh˜aes, and H. M. Santos, “A
survey of user authentication based on mouse dynamics,” in Global
E-Security. Springer, 2008, pp. 210–219.
[40] Y. Nakkabi, I. Traor´e, and A. A. E. Ahmed, “Improving mouse dynamics
biometric performance using variance reduction via extractors with
separate features,” Systems, Man and Cybernetics, Part A: Systems and
Humans, IEEE Transactions on, vol. 40, no. 6, pp. 1345–1353, 2010.
[41] Z. Jorgensen and T. Yu, “On mouse dynamics as a behavioral biometric
for authentication,” in Proceedings of the 6th ACM Symposium on
Information, Computer and Communications Security. ACM, 2011, pp.
476–482.
[42] N. Zheng, K. Bai, H. Huang, and H. Wang, “You are how you touch:
User veriﬁcation on smartphones via tapping behaviors,” Tech. Rep.
WM-CS-2012-06, Tech. Rep., 2012.
[43] K. B. Rasmussen, M. Roeschlin, I. Martinovic, and G. Tsudik, “Au-
thentication using pulse-response biometrics,” in Proceedings of the
21st Network and Distributed System Security Symposium (NDSS 2014),
2014.
[44] C. Cornelius, J. Sorber, R. Peterson, J. Skinner, R. Halter, and D. Kotz,
“Who wears me? bioimpedance as a passive biometric,” in Proc. 3rd
USENIX Workshop on Health Security and Privacy, 2012.
[45] M. Kumar, T. Garﬁnkel, D. Boneh, and T. Winograd, “Reducing shoulder-
surﬁng by using gaze-based password entry,” in Proceedings of the 3rd
symposium on Usable privacy and security. ACM, 2007, pp. 13–19.
[46] A. De Luca, M. Denzel, and H. Hussmann, “Look into my eyes!: Can
you guess my password?” in Proceedings of the 5th Symposium on
Usable Privacy and Security. ACM, 2009, p. 7.
[47] T. Kinnunen, F. Sedlak, and R. Bednarik, “Towards task-independent
person authentication using eye movement signals,” in Proceedings of
the 2010 Symposium on Eye-Tracking Research & Applications. ACM,
2010, pp. 187–190.
[10] R. J. Jacob, “Eye tracking in advanced interface design,” Virtual
environments and advanced interface design, pp. 258–288, 1995.
[11] W. L. Ottati, J. C. Hickox, and J. Richter, “Eye scan patterns of
experienced and novice pilots during visual ﬂight rules (vfr) navigation,”
in Proceedings of the Human Factors and Ergonomics Society Annual
Meeting, vol. 43, no. 1. SAGE Publications, 1999, pp. 66–70.
[12] D. Tock and I. Craw, “Tracking and measuring drivers’ eyes,” Image
and Vision Computing, vol. 14, no. 8, pp. 541–547, 1996.
[13] T. Ito, S. Mita, K. Kozuka, T. Nakano, and S. Yamamoto, “Driver
blink measurement by the motion picture processing and its application
to drowsiness detection,” in Intelligent Transportation Systems, 2002.
Proceedings. The IEEE 5th International Conference on.
IEEE, 2002,
pp. 168–173.
[14] M. S. Devi and P. R. Bajaj, “Driver fatigue detection based on eye
tracking,” in Emerging Trends in Engineering and Technology, 2008.
ICETET’08. First International Conference on.
IEEE, 2008, pp. 649–
652.
[15] C. MacLachlan and H. C. Howland, “Normal values and standard
deviations for pupil diameter and interpupillary distance in subjects aged
1 month to 19 years,” Ophthalmic and Physiological Optics, vol. 22,
no. 3, pp. 175–182, 2002.
[16] D. Kahneman and J. Beatty, “Pupil diameter and load on memory.”
Science, 1966.
[17] S. Taptagaporn and S. Saito, “How display polarity and lighting
conditions affect the pupil size of vdt operators,” Ergonomics, vol. 33,
no. 2, pp. 201–208, 1990.
[18] D. R. Jasinski, J. S. Pevnick, and J. D. Grifﬁth, “Human pharmacology
and abuse potential of the analgesic buprenorphine: a potential agent
for treating narcotic addiction,” Archives of General Psychiatry, vol. 35,
no. 4, p. 501, 1978.
(2011) Cybersecurity watch survey. [Online]. Available: http://resources.
sei.cmu.edu/asset ﬁles/Presentation/2011 017 001 54029.pdf
[19]
[20] Michelle and E. Kowalski, “Insider Threat Study: Computer System
Sabotage in Critical Infrastructure Sectors,” May 2005. [Online].
Available: http://www.cert.org/archive/pdf/insidercross051105.pdf
[21] M. Kandias, A. Mylonas, N. Virvilis, M. Theoharidou, and D. Gritzalis,
“An insider threat prediction model,” in Trust, Privacy and Security in
Digital Business. Springer, 2010, pp. 26–37.
[22] V. Cantoni, C. Galdi, M. Nappi, M. Porta, and D. Riccio, “Gant: Gaze
analysis technique for human identiﬁcation,” Pattern Recognition, 2014.
[23] Z. Liang, F. Tan, and Z. Chi, “Video-based biometric identiﬁcation
using eye tracking technique,” in Signal Processing, Communication
and Computing (ICSPCC), 2012 IEEE International Conference on.
IEEE, 2012, pp. 728–733.
[24] Z. M. Hafed and J. J. Clark, “Microsaccades as an overt measure of
covert attention shifts,” Vision research, vol. 42, no. 22, pp. 2533–2545,
2002.
[25] M. Frank, R. Biedert, E. Ma, I. Martinovic, and D. Song, “Touchalytics:
On the applicability of touchscreen input as a behavioral biometric for
continuous authentication,” IEEE Transactions on Information Forensics
and Security, 2012.
J. Dougherty, R. Kohavi, M. Sahami et al., “Supervised and unsupervised
discretization of continuous features,” in ICML, 1995, pp. 194–202.
[26]
[27] C. Ding and H. Peng, “Minimum redundancy feature selection from
microarray gene expression data,” Journal of bioinformatics and
computational biology, vol. 3, no. 02, pp. 185–205, 2005.
[28] K. Holmqvist, M. Nystr¨om, and F. Mulvey, “Eye tracker data quality:
what it is and how to measure it,” in Proceedings of the Symposium on
Eye Tracking Research and Applications. ACM, 2012, pp. 45–52.
[29] H. Crawford, K. Renaud, and T. Storer, “A framework for continuous,
transparent mobile device authentication,” Computers & Security, vol. 39,
pp. 127–136, 2013.
[30] A. K. Jain, A. Ross, and S. Pankanti, “Biometrics: a tool for information
security,” Information Forensics and Security, IEEE Transactions on,
vol. 1, no. 2, pp. 125–143, 2006.
13