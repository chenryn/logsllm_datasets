## I. 引言
本文旨在探讨人工智能（AI）系统中的安全问题，特别是那些常被忽视的问题。尽管深度学习在处理自然生成的数据如语音和图像时表现出很高的准确率，但在面对恶意构造的输入时仍然存在显著的安全漏洞。攻击者可以通过系统化的方法生成误判样本，从而实现几乎100%的成功率，进行逃逸攻击。

### 小结
本文将继续介绍大众所忽视的人工智能安全问题。虽然深度学习系统在正常输入下可以达到很低的误判率，但当攻击者使用系统化方法生成误判样本时，攻击成功率可以接近100%，从而实现稳定的逃逸攻击。随着人工智能应用的普及，对逃逸攻击的研究将越来越深入。

## II. 软件漏洞与AI系统的安全性
### 1. 数据流篡改
基于数据流篡改，攻击者可以利用任意写内存漏洞直接修改AI系统中的关键数据（如标签、索引等），导致系统输出错误的结果。

### 2. 控制流劫持
另一种常见的攻击方式是通过控制流劫持（如堆溢出、栈溢出等漏洞）来完成对抗攻击。由于控制流劫持漏洞允许攻击者执行任意代码，因此可以完全控制AI系统，使其输出攻击者预期的结果。

关于软件漏洞造成的问题已在第一章中详细讨论。这里仅作简要介绍，更多细节请参考ISC 2017大会人工智能与安全论坛发布的内容。

## III. 深度学习数据流处理中的安全风险
深度学习在许多领域受到广泛关注，特别是在图形图像领域，如人脸识别和自动驾驶等应用正在逐渐进入我们的生活。随着深度学习的普及，其安全性也日益受到重视。

目前对深度学习的安全讨论包括：
- 深度学习平台中的漏洞
- 深度学习模型中的隐藏错误
- 对深度学习系统的逃逸攻击

360安全团队发现，在深度学习的数据处理流程中同样存在安全风险。攻击者即使不利用平台软件漏洞或机器学习模型弱点，仅通过深度学习数据流中的处理问题，也可以实现逃逸或数据污染攻击。

### 攻击实例
以深度学习图片识别应用为例，我们通过几个例子来展示降维攻击的效果。

#### 示例1：羊群照片
原始图片是一张羊群的照片。我们将这张图片送入Caffe平台自带的经典图片识别应用程序中，该程序使用的神经网络模型是GoogleNet，训练数据来自ImageNet比赛。

**结果：**
- Caffe的深度学习应用认为这张图片是“白狼”。
- TensorFlow的识别结果也是“白狼”。

```plaintext
# sheep.png — MD5: fce6aa3c1b03ca4ae3289f5be22e7e65
$ ./classification.bin models/bvlc_googlenet/deploy.prototxt models/bvlc_googlenet/bvlc_googlenet.caffemodel data/ilsvrc12/imagenet_mean.binaryproto data/ilsvrc12/synset_words.txt /tmp/sample/sheep.png
---------- Prediction for /tmp/sample/sheep.png ----------
0.8890 - "n02114548 white wolf, Arctic wolf, Canis lupus tundrarum”
0.0855 - "n02120079 Arctic fox, white fox, Alopex lagopus"
```

#### 示例2：小羊照片
另一张图片是一只小羊。在Caffe的机器学习应用中，这张图片被识别为“猞猁猫”。其他平台如TensorFlow和Torch也有类似结果。

### 降维攻击原理
降维攻击是对深度学习应用的数据流处理进行攻击的一种新型方法。深度学习系统的核心是神经网络，通常假定输入数据具有固定的维度。实际输入数据可能与模型期望的维度不匹配，解决这一问题的方法有两种：
1. 丢弃不符合固定维度的输入。
2. 对输入进行维度调整。

在广泛图片识别应用中，通常采用第二种方法，即对大图进行降维，对小图进行升维。常用的降维算法包括最近点抽取和双线性插值等。这些算法旨在尽量保持图片原有特征，但未考虑恶意构造的输入。

上述两个攻击示例都是针对最常用的双线性插值算法构造的恶意样本。通过特殊处理，可以使降维后的图片产生异常结果，从而误导深度学习系统。

### 降维攻击影响范围及防范手段
降维攻击会影响使用维度调整的深度学习系统。著名的深度学习平台如TensorFlow、Caffe和Torch都提供了维度调整函数。根据初步分析，几乎所有流行的深度学习图片识别程序都有被降维攻击的风险。

**防范措施：**
- 过滤超出异常范围的图片
- 对降维前后的图片进行比对
- 采用更健壮的降维算法

### 小结
降维攻击是对深度学习数据流进行攻击的一种新型方法，主要影响对任意图片进行识别的深度学习应用程序。希望通过本文提醒公众，在拥抱人工智能热潮的同时，需要持续关注深度学习系统中的安全问题。

## 参考文献
[1] Freebuf 机器学习对抗性攻击报告  
[2] Ian Goodfellow and Jonathon Shlens and Christian Szegedy, Explaining and Harnessing Adversarial Examples. International Conference on Learning Representations, 2015.  
[3] Guyen, A., J. Yosinski, and J. Clune, Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. 2015: p. 427-436.  
[4] Moosavi Dezfooli, Seyed Mohsen and Fawzi, Alhussein and Frossard, Pascal, DeepFool: a simple and accurate method to fool deep neural networks, Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.  
[5] Weilin Xu, Yanjun Qi, and David Evans, Automatically Evading Classifiers A Case Study on PDF Malware Classifiers, NDSS, 2016.  
[6] Qixue Xiao, Kang Li, Deyue Zhang, and Weilin Xu, Security Risks in Deep Learning Implementations, arXiv:1711.11008, 2017.  
[7] Qixue Xiao, Kang Li, Deyue Zhang, and Yier Jin, The Downscaling Attack Against Deep Learning Applications, arXiv:1711.11008, 2017.  
[8] Richard Chirgwin, What do TensorFlow, Caffe, and Torch have in Common? Open CVEs, 2017.