  1. 基于数据流篡改可以利用任意写内存漏洞，直接将AI系统中的一些关键数据进行修改(如标签、索引等)， 使得AI系统输出错误的结果。
  2. 另一种则是通过常规的控制流劫持(如堆溢出、栈溢出等漏洞)来完成对抗攻击，由于控制流劫持漏洞可以通过漏洞实现任意代码的执行，因此必然可以控制AI系统输出攻击者预期的结果。
关于软件漏洞造成的问题我们在第一章里已有详细介绍。 这里只做了一个简单介绍, 更多细节请参考ISC 2017大会人工智能与安全论坛所发布的内容。
### 小结
本文的目的是继续介绍被大众所忽视的人工智能安全问题。虽然深度学习在处理自然生成的语音图像等以达到相当高的准确率，但是对恶意构造的输入仍然有巨大的提升空间。虽然深度学习系统经过训练可以对正常输入达到很低的误判率，但是当攻击者用系统化的方法能够生成误判样本的时候，攻击的效率就可以接近100%，
从而实现稳定的逃逸攻击。 随着人工智能应用的普及，相信对逃逸攻击的研究也会越来越深入。
## III. 深度学习数据流处理中的安全风险
深度学习在很多领域受到广泛关注。 尤其在图形图像领域里，人脸识别和自动驾驶等应用正在逐渐进入我们的生活。 深度学习的流行与普及也自然带来了安全方面的考虑。
目前对深度学习的安全讨论包括深度学习平台中发现的漏洞，深度学习模型中隐藏的错误，还有对深度学习系统的逃逸攻击。
360
安全团队发现在深度学习的数据处理流程中，同样存在安全风险。攻击者在不利用平台软件实现漏洞或机器学习模型弱点的情况下，只利用深度学习数据流中的处理问题，就可以实现逃逸或数据污染攻击。
  1. 攻击实例
以深度学习图片识别应用为攻击目标， 我们用几个例子来介绍降维攻击的效果。
先从一张图片开始。下面这个图应该是羊群的照片吧。 我们把这张图片送到深度学习图片识别应用程序里，看看深度学习系统会怎么说。
这里我们用的程序来自Caffe平台自带的经典图片识别应用例子，识别所用的神经元网络是由谷歌发布的GoogleNet，数据来自著名的ImageNet
比赛，模型是由伯克利用谷歌的模型加上ImageNet的数据培训的。这个平台的识别能力大家没有疑问吧。
Caffe的深度学习应用认为上面的图片是 狼 ！ （图片识别程序输出如下。 TensorFlow 的例子结果也是狼！）
    # sheep.png — MD5:  fce6aa3c1b03ca4ae3289f5be22e7e65
    $ ./classification.bin  models/bvlc_googlenet/deploy.prototxt  models/bvlc_googlenet/bvlc_googlenet.caffemodel  data/ilsvrc12/imagenet_mean.binaryproto  data/ilsvrc12/synset_words.txt  /tmp/sample/sheep.png
    ---------- Prediction for /tmp/sample/sheep.png ----------    0.8890 - "n02114548 white wolf, Arctic wolf, Canis lupus tundrarum”
    0.0855 - "n02120079 Arctic fox, white fox, Alopex lagopus"
再看一个例子。 下面这张图，还是羊，人来看应该是一只小羊。  
那么在机器学习系统里它会被认成什么呢？
Caffe的机器学习应用会把这一张认为是猫，具体讲属于ImageNet里的猞猁猫！其它平台TensorFlow，Torch等流行的图片识别应用也是这样。
为什么会是这样？ 深度学习的训练模型没有错。问题出在深度学习应用的数据流处理上。
### 降维攻击原理
我们在前一段时间讨论过关于深度学习的逃逸攻击，主要介绍了各种让机器学习系统做出错误判别的方法。
目前学术界对深度学习逃逸攻击的研究大多集中在对抗样本生成的方法，通过各种算法在图片上生成扰动，从而导致深度学习系统的误判。
这篇文章提到的降维攻击没有使用传统的对抗样本生成策略。 降维攻击是对深度学习应用的数据流处理进行了攻击。
深度学习系统的核心是神经元网络。 深度学习所使用的静态神经元网络往往假定它的输入是一个固定的维度，这样便于设计深度神经元网络。
固定的维度带来的问题是：实际的输入并不一定与神经元网络模型输入用相同的维度。
解决这种维度不匹配的方法有两个，一个是要求所有的输入都必须是模型使用的维度，其它输入一概扔掉。
另外一个选择是对输入进行维度调整。对于试图对广泛图片进行识别的应用里，大多采用了第二种方法。在具体图像识别应用里，就是把大的输入图片进行维度缩减，小的图片进行维度放大。
下图为一个典型的深度学习应用的数据流处理过程。
【 深度学习应用的数据流程图 】
维度变化的结果是，深度学习模型真正用到的数据是维度变化过的图片。
维度变化的算法有很多种，常用的包括最近点抽取，双线性插值等。这些算法的目的是在对图片降维的同时尽量保持图片原有的样子。
但是这些常用的降维算法没有考虑恶意构造的输入。上面的两个攻击图片例子都是针对最常用的双线性插值构造的恶意攻击样本。
我们用下面的图片展示人看到的图片和深度学习真正看到的图片 （左边一列是原始输入，右边是深度学习系统后端模型认为的输入）。
图片左边是对深度学习应用的输入图片，右边是降维后的图片。 羊群图片经过缩减，就会变成一只雪地里的白狼。 卡通小羊的图片也就变成了可爱小猫的图片。
当然这些输入图片是经过特殊处理构造的，专门让降维函数出现这种异常的结果。
基于这个攻击思路，我们也对其它深度学习应用进行了测试。 例如著名的深度学习教科书案例 MINST
手写数字识别，我们可以成功生成对人很清楚的数字，但会被深度学习系统误识别的图片。下面显示了四组图片。
每一组中，左边是对应用的输入，也就是人看到的图片；右边是人看不到，但是被机器学习模型最后处理的图片。
这样的图片变化，造成深度学习系统出现错误识别应该不难理解。
### 降维攻击影响范围及防范手段
降维攻击会影响到使用维度调整的深度学习系统。 著名的深度学习平台，包括TensorFlow，Caffe，Torch都提供维度调整函数供深度学习应用程序使用。
下面的表格里展示了常用深度学习框架中使用的维度调整算法。使用这些算法的程序都可能受到降维攻击的影响。
根据我们的初步分析，几乎所有网上流行的深度学习图片识别程序都有被降维攻击的风险。
对于降维攻击的防范，用户可以采用对超出异常的图片进行过滤，对降维前后的图片进行比对，以及采用更加健壮的降维算法等。
### 小结
本文的目的是继续介绍被大众所忽视的人工智能安全问题。降维攻击是对深度学习的数据流进行攻击的一种新型攻击方法，主要影响对任意图片进行识别的深度学习应用程序。
我们希望通过这些工作提醒公众，在拥抱人工智能热潮的同时，需要持续关注深度学习系统中的安全问题。
## 参考文献
[1] Freebuf 机器学习对抗性攻击报告
[2] Ian Goodfellow and Jonathon Shlens and Christian Szegedy, Explaining and
Harnessing Adversarial Examples. International Conference on Learning
Representations, 2015.
[3] Guyen, A., J. Yosinski, and J. Clune, Deep neural networks are easily
fooled: High confidence predictions for unrecognizable images. 2015: p.
427-436.
[4] Moosavi Dezfooli, Seyed Mohsen and Fawzi, Alhussein and Frossard, Pascal,
DeepFool: a simple and accurate method to fool deep neural networks,
Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2016.
[5] Weilin Xu, Yanjun Qi, and David Evans, Automatically Evading Classifiers A
Case Study on PDF Malware Classifiers, NDSS, 2016
[6] Qixue Xiao, Kang Li, Deyue Zhang, and Weilin Xu, Security Risks in Deep
Learning Implementations, arXiv:1711.11008.
, 2017
[7] Qixue Xiao, Kang Li, Deyue Zhang, and Yier Jin, The Downscaling Attack
Against Deep Learning Applications, arXiv:1711.11008.
, 2017
[8] Richard Chirgwin, What do TensorFlow, Caffe, and Torch have in Common?
Open CVEs.
2017