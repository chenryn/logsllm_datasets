it is unlikely that preﬁx hijacking is effective—most ISPs in prac-
tice are conﬁgured to ﬁlter competing advertisements for addresses
originating in their direct customers/providers, so the ARROW ISP
would effectively ﬁlter out most forged announcements.
It is worth noting that if all we need is alternating compliant ISPs,
the average number of ARROW hops we would need for an end to
end path in today’s Internet is very small, typically one or two.
An endpoint can also constrain that all communications sent to it
should be through ARROW tunnels by providing other endpoints
with an ARROW address as opposed to its actual IP address.
In any case, it is important to know how many BGP hops are
between the ARROW hops to gauge the vulnerability of the con-
nection to attacks and failures. To ﬁgure out the number of inter-
mediate hops, sources can initiate a traceroute to originate from the
last ARROW supporting hop of the source end of the path, directed
at the ﬁrst ARROW hop of the destination end of the path. This can
be done by sending a traceroute via ARROW from the source end-
point to the ﬁrst ARROW hop of the destination end through the
existing partial circuit.
Misbehaving ARROW users. We have thus far assumed that AR-
ROW clients are well-behaved. A malicious or faulty client could
repeatedly send false failover notices, or set up an ARROW cir-
cuit with a loop. An ARROW-supporting ISP would need to de-
tect these scenarios (this is feasible because clients require strong
identities to buy ARROW service) and ignore or rate-limit failover
trafﬁc from misbehaving customers.
3.6 Business issues
An ISP has an incentive to ensure correct forwarding of AR-
ROW trafﬁc across its network, because it is receiving revenue in
addition to the price it is receiving for carrying the packet from
Source ISPTarget ISPARROW ISPARROWARROWARROWARROWIngressIngressIngressSecret IPDoSAttackerits immediate neighbor. Also, since endpoints have the ability to
switch over to pre-conﬁgured backup paths, it is in the ISP’s in-
terest to perform local fault recovery quickly if it wants to retain
the trafﬁc from the ARROW customers. Further, since ARROW
would allow ISPs to attract trafﬁc that they normally wouldn’t re-
ceive, there is an incentive for ISPs to implement ARROW even
when other ISPs don’t.
An ISP might intentionally disrupt trafﬁc to an ARROW provider,
e.g., if it sees a packet destined for an ARROW address, it might
drop it. While a sufﬁcient number of such misbehaving ISPs will
cause a problem regardless of the technology used, ARROW would
allow customers to route around them, and provide further incentive
(in the form of more potential customers) for other ISPs to adopt
ARROW. As another option, ARROW destined packets could be
encrypted when traversing non-cooperative ISPs, so that they ap-
pear to be SSL trafﬁc.
Although ARROW will allow enterprises to contract for exactly
the amount of route control, resilience, and DoS protection that
they need, ISPs may also ﬁnd it useful to leverage ARROW ser-
vices on behalf of their customers. That is, a customer-facing ISP
would arrange tunnels to important data services, and this would be
(nearly) transparent to the ISP’s customers, except that they would
ﬁnd their Internet service through the ISP to be highly reliable. This
aggregation will be particularly valuable for thin devices that lack
the ability to monitor routes and perform route control on their own.
4.
IMPLEMENTATION
In this section, we describe our implementation of ARROW and
its deployment. The deployment is a step toward building and run-
ning an ISP supporting ARROW, such that any Internet user will
be able to try out ARROW for themselves. To this end we leverage
the BGP Transit Portal [33] and PlanetLab VICCI clusters [34]. We
have also built several services designed to enhance end-to-end per-
formance and security on this deployment. These services include
a DoS prevention system akin to the one described in §3.5, and a
simple CDN.
4.1 Serval Implementation
We have integrated ARROW support into the Serval [25] proto-
col stack. Serval is a great ﬁt for ARROW as it already supports
client-side failover for connection failures and we can extend this
support to failover among several ARROW paths. To extend Ser-
val to support ARROW, we have added a new packet header ex-
tension (header extensions are a Serval feature), which contains the
ARROW authenticator. This extension is included on every data
packet. If the source endpoint’s service access table contains a for-
ward rule with an ARROW authenticator annotation, this header
extension will be generated with the corresponding authenticator
and all data packets forwarded to the speciﬁed next-hop service
router. The service routers detect the ARROW extension and match
it in a special ARROW authenticator table to the next-hop IP ad-
dress, possibly with another ARROW annotation. Each packet’s
destination IP address and potentially ARROW authenticator is
rewritten according to this table.
4.2
Internet Atlas
To provide the Internet atlas service, each ARROW ISP contin-
uously performs measurements to generate and maintain a map of
Internet paths from each of its PoPs to each routable preﬁx, with
each path annotated with a rich set of attributes corresponding to
the latency, available bandwidth, and loss rate of the path.
These collected measurements are made available through a net-
work atlas service (with XMLRPC and SunRPC interfaces) so that
require ’atlas’
egress = ARGV[0]
prefixes = Array.new
(1..ARGV.length-1).each { |i|
prefixes.push(ARGV[i])
}
atlas = Atlas.new
prefixes.each{ |p|
atlas.addPath(egress, p)
}
responses = atlas.queryPendingPaths
responses.each{ |r|
if (r.latency < 300) # 300ms
puts(r.path.join(" "))
end
}
Figure 5: A Ruby program that returns all ARROW paths with
latency below 300 ms from a given egress PoP to a number of
given IP addresses. Hops on a path are separated by spaces,
paths are separated by newlines.
they can be queried dynamically for metrics, such as reachability,
latency and throughput performance, between an ARROW PoP and
an arbitrary IP address. It is kept up-to-date with live active probe
measurements, which are performed at regular intervals and also
selectively reprobed based on passive observation of BGP feeds.
As such, it can be used to determine the performance between any
two PoPs, as well as the performance to any Internet preﬁx from an
egress PoP. This is especially useful when choosing among multi-
ple ARROW-offering ISPs.
To determine whether keeping the network atlas up-to-date is
feasible, we have evaluated the average daily rate with which In-
ternet paths change at the PoP-level. To do so, we count daily
PoP-level changes in downstream paths from all tier-1 ASes to all
preﬁxes within the entire year 2012 of historical iPlane traceroute
measurements (cf. §5.4). We count in two ways: 1. also counting
paths with IP-level changes where we have no IP-to-PoP mapping,
and 2. where we ignore these changes. We arrive at an average rate
of 10% and 1%, respectively, of paths changing daily, which can be
processed efﬁciently at the atlas service.
The network atlas service expects a Ruby program on its input
and provides the output of that program as its result. The Ruby
program can make repeated queries to the database of collected
measurements (using the call queryPendingPaths), ﬁlter out unnec-
essary results, and report back to the endpoint a pruned list of paths
that match application-speciﬁc criteria. Figure 5 demonstrates a
program that returns all paths with a latency below 300ms from a
given egress PoP to a number of IP preﬁxes, given as a list of IP
addresses living within each preﬁx, respectively.
4.3 Wide Area Deployment
We have deployed ARROW nodes in the wide area using the
Transit Portal (TP) BGP testbed [33] and the VICCI set of geo-
graphically distributed compute clusters [34]. TP lets us announce
/24 preﬁxes using six US universities as our providers: Univer-
sity of Washington (UW), University of Wisconsin (WISC), Geor-
gia Tech (GATech), Princeton University, Clemson University and
University of Southern California (USC). VICCI clusters are co-
located with most of the TP sites, except for another VICCI cluster
site at Stanford University that is not co-located. We envision each
TP/VICCI site as a PoP for our ARROW ISP. An ARROW client
simply addresses packets to a speciﬁc IP address in the TP preﬁx.
The Portal software router then redirects these packets to an AR-
ROW software router via a VPN tunnel. We deploy the ARROW
routers on a co-located VICCI cluster.
For redundancy, ARROW routers have replicas that are kept con-
sistent via a distributed coordination service [12].
In a real de-
ployment, router-speciﬁc protocols would be used to keep standby
routers consistent with their primaries. Software routers run a Ser-
val routing component in user space (kernel modiﬁcations are not
allowed on VICCI); this is the data plane. Each router also runs an
ARROW circuit creation daemon, a replica manager, and a failover
management daemon; the control plane—implemented in Python.
4.4 ARROW Services
We have built several prototype services on our ARROW de-
ployment, to demonstrate the value of additional services deployed
with ARROW. We brieﬂy describe two of these services in this sec-
tion: a DoS protection service and a content distribution network.
DoS Protection. We have implemented the Phalanx [7] DoS pro-
tection scheme and deployed it at ARROW software router nodes at
each Transit Portal PoP. The service can be offered to anyone on the
Internet wishing to obtain a DoS protection layer for their own ser-
vices. Phalanx achieves DoS protection by requiring clients to in-
terleave packets to the server through a predetermined sequence of
bandwidth-limiting proxy nodes (called mailboxes), each of which
has a different IP address. Since the attacker does not know this se-
quence, they can affect only a fraction of the trafﬁc exchanged. The
client and server initially agree on the random sequence of mail-
boxes (through a secret key) before trafﬁc can ﬂow. The commu-
nication proceeds by the client sending a packet to the appropriate
mailbox and the server requesting the packet from that mailbox.
The router at the TP node forwards client packets and server re-
quests to the ARROW VICCI nodes where client packets are stored
until the server requests them.
Content Distribution Network. We have also implemented a sim-
ple CDN offering a trafﬁc cache at each Transit Portal PoP. The
CDN consists of a controller and a group of geo-replicated cache
nodes. The controller is responsible for collecting content requests
and directs them to the cache replica with the lowest latency to
the requesting client. Each ISP offering CDN service is in a good
position to know this information for its PoPs. In our prototype de-
ployment, each replica stores a copy of each ﬁle. After receiving a
client request, the controller instructs the cache nodes to measure
their RTT latencies to the client and picks the one with the lowest
RTT to serve the content to the client.
5. EVALUATION
We evaluate ARROW via simulation on AS and PoP-level In-
ternet topology, as well as by measurement of our implementa-
tion, both when deployed on a local cluster of machines, as well
as within our large-scale deployment. Speciﬁcally, we seek to an-
swer the following questions:
• How are throughput and latency of Internet trafﬁc affected when
an ARROW path (of various lengths) is used?
• How nimble are the individual failover mechanisms that are de-
ployed along an ARROW path?
• How quickly is ARROW able to re-establish end-to-end con-
nectivity in the event of a link failure on the Internet?
• How resilient are various ARROW deployments to (potentially
cascading) Internet link and AS failures and can we use AR-
ROW to avoid untrusted ISPs?
UDP/TCP
Serval
1 ARROW hop
2 ARROW hops
RTT [µs]
44/96/107
73/81.23/154
113/131.96/290
158/191.38/444
Throughput [Gbits/s]
9.05/9.36/9.68
9.35/9.52/9.74
9.37/9.55/9.85
8.19/8.49/8.72
Table 1: RTT and throughput (min/avg/max) of different AR-
ROW path lengths vs. UDP/TCP and Serval.
Figure 6: ARROW 6-node cluster deployment.
• How effectively do various ARROW deployments prevent IP
preﬁx-hijacking attacks?
• Is the end-to-end latency of ARROW paths comparable to that
of regular BGP paths?
We are especially concerned with evaluating whether a single
tunnel (one ARROW hop) is enough to provide all of the bene-
ﬁts. Hence, with the exception of performance evaluation, we only
evaluate one-hop tunnels within the experiments in this section (as
opposed to the number of ARROW-supporting ASes, which we do
vary).
5.1 Performance Overhead
To evaluate the latency and throughput overheads of our AR-
ROW prototype, we have deployed it on a 6-node cluster. All nodes
run Linux 3.2.0 on Intel Xeon E5-2430 processors at 2.2 GHz clock
frequency, with 15 Mbytes total cache, 4 Gbytes memory, and In-
tel X520 dual-port 10 Gigabit Ethernet adapters, connected to a 10
Gigabit Ethernet switch. Figure 6 shows this setup.
In the cluster, one node acts as the source endpoint of a route
and another one as the destination. The other nodes are used as
ARROW routers. The deployment is symmetrical: Both forward
and reverse ARROW paths are established between source and tar-
get, over distinct nodes in the cluster. We can construct up to 2
ARROW hops in this symmetrical fashion. We determine the la-
tency along a path by measuring the average round-trip time (RTT)
of 100 individual 64 byte UDP packets to the destination which
echoes them back unmodiﬁed. We measure the average throughput
over 5 TCP transfers of a data stream over 10 seconds each, using
the iperf1 bandwidth measurement tool.
Table 1 shows the measurement results of different lengths of
ARROW routes compared to UDP/TCP and unmodiﬁed Serval.
The TCP, UDP and Serval measurements measure direct bandwidth
and RTT between two endpoints, without going through any inter-
mediate hops. The ARROW measurements forward packets ac-