title:Machine Learning with Membership Privacy using Adversarial Regularization
author:Milad Nasr and
Reza Shokri and
Amir Houmansadr
Machine Learning with Membership Privacy
using Adversarial Regularization
Milad Nasr1, Reza Shokri2, Amir Houmansadr1
1 University of Massachusetts Amherst, 2 National University of Singapore
PI:EMAIL, PI:EMAIL, PI:EMAIL
8
1
0
2
l
u
J
6
1
]
L
M
.
t
a
t
s
[
1
v
2
5
8
5
0
.
7
0
8
1
:
v
i
X
r
a
ABSTRACT
Machine learning models leak information about the datasets on
which they are trained. An adversary can build an algorithm to
trace the individual members of a model’s training dataset. As a
fundamental inference attack, he aims to distinguish between data
points that were part of the model’s training set and any other data
points from the same distribution. This is known as the tracing
(and also membership inference) attack. In this paper, we focus on
such attacks against black-box models, where the adversary can
only observe the output of the model, but not its parameters. This is
the current setting of machine learning as a service in the Internet.
We introduce a privacy mechanism to train machine learning
models that provably achieve membership privacy: the model’s
predictions on its training data are indistinguishable from its pre-
dictions on other data points from the same distribution. We de-
sign a strategic mechanism where the privacy mechanism antici-
pates the membership inference attacks. The objective is to train
a model such that not only does it have the minimum prediction
error (high utility), but also it is the most robust model against its
corresponding strongest inference attack (high privacy). We for-
malize this as a min-max game optimization problem, and design
an adversarial training algorithm that minimizes the classiﬁcation
loss of the model as well as the maximum gain of the membership
inference attack against it. This strategy, which guarantees mem-
bership privacy (as prediction indistinguishability), acts also as a
strong regularizer and signiﬁcantly generalizes the model.
We evaluate our privacy mechanism on deep neural networks
using diﬀerent benchmark datasets. We show that our min-max
strategy can mitigate the risk of membership inference attacks
(close to the random guess) with a negligible cost in terms of the
classiﬁcation error.
KEYWORDS
Data privacy; Machine learning; Inference attacks; Membership
privacy; Indistinguishability; Min-max game; Adversarial process
1 INTRODUCTION
Large available datasets and powerful computing infrastructures,
as well as advances in training complex machine learning mod-
els, have dramatically increased the adoption of machine learning
in software systems. Many algorithms, applications, and services
that used to be built based on expert knowledge, now can be de-
signed using much less engineering eﬀort by relying on advanced
machine learning algorithms. Machine learning itself has also been
provided as a service, to facilitate the use of this technology by sys-
tem designers and application developers. Data holders can train
models using machine learning as a service (MLaaS) platforms (by
1
Google, Amazon, Microsoft, ...) and share them with others or use
them in their own applications. The models are accessible through
prediction APIs, which allow simple integration of machine learn-
ing algorithms into applications in the Internet.
A wide range of sensitive data, such as online and oﬄine proﬁles
of users, location traces, personal photos, speech samples, medical
and clinical records, and ﬁnancial portfolios, is used as input for
training machine learning models. The conﬁdentiality and privacy
of such data is of utmost importance to data holders. Even if the
training platform is trusted (e.g., using conﬁdential computing, or
by simply training the model on the data owner’s servers) the re-
maining concern is if a model’s computations (i.e., its predictions)
can be exploited to endanger privacy of its sensitive training data.
The data required for training accurate models presents seri-
ous privacy issues. The leakage through complex machine learn-
ing models maybe less obvious, compared to, for example, linear
statistics [18]. However, machine learning models, similar to other
types of computations, could signiﬁcantly leak information about
the datasets on which they are computed. In particular, an adver-
sary, with even black-box access to a model, can perform a mem-
bership inference [23, 43] (also known as the tracing [17]) attack
against the model to determine whether or not a target data record
is a member of its training set [45]. The adversary exploits the dis-
tinctive statistical features of the model’s predictions on its train-
ing data. This is a fundamental threat to data privacy, and is shown
to be eﬀective against various machine learning models and ser-
vices [45].
In this paper, we focus on protecting machine learning models
against this exact threat: black-box membership inference attacks.
There are two major groups of existing defense mechanisms. The
ﬁrst group includes simple mitigation techniques, such as limit-
ing the model’s predictions to top-k classes, therefore reducing the
precision of predictions, or regularizing the model (e.g., using L2-
norm regularizers) [19, 45]. These techniques may impose a negli-
gible utility loss to the model. However, they cannot guarantee any
rigorous notion of privacy. The second major group of defenses
use diﬀerential privacy mechanisms [1, 6, 10, 40, 41]. These mech-
anisms do guarantee (membership) privacy up to their privacy pa-
rameter ϵ. However, the existing mechanisms may impose a sig-
niﬁcant classiﬁcation accuracy loss for protecting large models on
high dimensional data for small values of ϵ. This comes from not
explicitly including utility into the design objective of the privacy
mechanism. Also, it is because the diﬀerential privacy mechanisms
are designed so as to guarantee input indistinguishability for all
possible input training datasets (that diﬀer in a constant number
of records), and for all possible parameters/outputs of the models.
Whereas, we explicitly include utility in the objective of the pri-
vacy mechanism which protects the very existing training dataset.
Contributions. In this paper, we design a rigorous privacy
mechanism for protecting a given training dataset, against a par-
ticular adversarial objective. We want to train machine learning
models that guarantee membership privacy: No adversary can
distinguish between the predictions of the model on its training
set from the model’s prediction on other data samples from the
same underlying distribution, up to the privacy parameter. This
is a more targeted privacy notion than diﬀerential privacy, as we
aim at a very speciﬁc (prediction) indistinguishability guarantee.
Our objective is that the privacy-preserving model should achieve
membership privacy with the minimum classiﬁcation loss.
We formalize membership inference attacks and deﬁne the de-
fender’s objective for achieving membership privacy for classiﬁca-
tion models. Based on these deﬁnitions, we design an optimiza-
tion problem to minimize the classiﬁcation error of the model
and the inference accuracy of the strongest attack who adaptively
maximizes his gain. Therefore, this problem optimizes a compo-
sition of two conﬂicting objectives. We model this optimization
as a min-max privacy game between the defense mechanism
and the inference attack, similar to privacy games in other set-
tings [2, 24, 27, 34, 46]. The solution is a model which not only
is accurate but also has the maximum membership privacy against
its corresponding strongest inference attack. The adversary cannot
design a better inference attack than what is already anticipated by
the defender; therefore, membership privacy is guaranteed. There
does not also exist any model that, for the same level of member-
ship privacy, can give a better accuracy. So, maximum utility (for
the same level of privacy) is also guaranteed.
To ﬁnd the solution to our optimization problem, we train the
model in an adversarial process. The classiﬁcation model maps
features of a data record to classes, and computes the probability
that it belongs to any class. The primary objective of this model
is to minimize prediction error. The inference model maps a tar-
get data record, and the output of the classiﬁer on it, to its mem-
bership probability. The objective of the inference model is to
maximize its membership inference accuracy. To protect data pri-
vacy, we add the gain of the inference attack as a regularizer for
the classiﬁer. Using a regularization parameter, we can control
the trade-oﬀ between membership privacy and classiﬁcation er-
ror. We train the models in a similar way as generative adversarial
networks [21] and other adversarial processes for machine learn-
ing [11, 14, 29, 35, 36, 38]. Our training algorithm can converge
to an equilibrium point where the best membership inference at-
tack against it is random guess, and this is achieved with minimum
classiﬁcation accuracy loss.
We present the experimental results on deep neural networks
using benchmark ML datasets as well as the datasets used in the
ML privacy literature. We compute various statistics of models’
predictions on their training and test sets, in order to illustrate
the worst case and the average case gaps between these statistics
(which cause the privacy risk). The gaps are reduced by several or-
ders of magnitude when the model is trained using our min-max
privacy mechanism, compared to non-privacy-preserving models.
Our results verify our theoretical analysis that we impose only
a negligible loss in classiﬁcation accuracy for a signiﬁcant
gain in membership privacy. For the CIFAR100 dataset trained
with Alexnet and Densenet architectures, the cost is respectively
1.1% and 3% drop in the prediction accuracy, relative to the regular
non-privacy-preserving models. For the Purchase100 and Texas100
datasets (used in [45]), the cost of membership privacy in terms of
classiﬁcation accuracy drop is 3.6% and 4.4%, respectively, for re-
ducing the inference accuracy from 67.6% to 51.6% and from 63% to
51%, respectively. Note that the membership privacy is maximum
when the membership inference accuracy is 50% (random guess).
We also show that our mechanism strongly regularizes the
models, by signiﬁcantly closing the gap between their training
and testing accuracy, and preventing overﬁtting. This directly
follows from the indistinguishability of our privacy-preserving
model’s prediction distributions on training versus test data. For
example, on the Purchase100 dataset, we can obtain 76.5% testing
accuracy for 51.8% membership inference accuracy. In contrast, a
standard L2-norm regularizer may provide a similar level of pri-
vacy (against the same attack) but with a 32.1% classiﬁcation accu-
racy.
2 MACHINE LEARNING
In this paper, we focus on training classiﬁcation models using su-
pervised learning. Table 1 summarizes the notations and formally
states the objective function of the classiﬁer. Let X be the set of
all possible data points in a d-dimensional space, where each di-
mension represents one attribute of a data point (and will be used
as the input features in the classiﬁcation model). We assume there
is a predeﬁned set of k classes for data points in X . The objective
is to ﬁnd the relation between each data point and the classes as a
classiﬁcation function f : X −→ Y . The output reﬂects how f clas-
siﬁes each input into diﬀerent classes. Each element of an output
y ∈ Y is a score vector that shows the relative association of any
input to diﬀerent classes. All elements of a vector y are in range
[0, 1], and are normalized such that they sum up to 1, so they are
interpreted as the probabilities that the input belongs to diﬀerent
classes.
,
Let Pr(X
Y) represent the underlying probability distribution of
all data points in the universe X × Y , where X and Y are random
variables for the features and the classes of data points, respec-
tively. The objective of a machine learning algorithm is to ﬁnd a
classiﬁcation model f that accurately represents this distribution
and maps each point in X to its correct class in Y . We assume we
have a lower-bounded real-valued loss function l(f (x), y) that, for
each data point (x , y), measures the diﬀerence between y and the
model’s prediction f (x). The machine learning objective is to ﬁnd
a function f that minimizes the expected loss:
L(f ) =
E
(x ,y)∼Pr(X
,
[l(f (x), y)]
Y)
(1)
We can estimate the probability function Pr(X
Y) using samples
drawn from it. These samples construct the training set D ⊂ X ×Y .
Instead of minimizing (1), machine learning algorithms minimize
the expected empirical loss of the model over its training set D.
,
LD (f ) =
1
|D | (x ,y)∈D
l(f (x), y)
(2)
2
Classiﬁcation model
f : X −→ Y
Loss
L(f ) =
E
l(f (x), y) Pr(X
,
Y) dx dy
[l(f (x), y)] = ∫X ×Y
l(f (x), y)
LD (f ) = 1
(x ,y)∼Pr(X
,
Y)
|D | (x ,y)∈D
Empirical loss
Optimization problem min
f
LD (f ) + λ R(f )
Table 1: Deﬁnition, loss, and optimization problem for the classiﬁcation model f , where x ∈ X is a data point, y ∈ Y is a
classiﬁcation vector, D is the model’s training set, l () is a loss function, R() is a regularizer, and λ is the regularization factor.
Inference model
h : X × Y 2 −→ [0, 1]
Gain
Gf (h) =
E
[lo❕(h(x , y , f (x)))] +
E
[log(1 − h(x , y , f (x)))]
Empirical gain
Gf , D A
, D ′A(h) = 1
,y′)∈D ′A log(1 − h(x ′
, y ′
, f (x ′)))
(x ,y)∼PrD (X
,
Y)
(x ,y)∼Pr\D (X
,
Y)
2 |D A |(x ,y)∈D A log(h(x , y , f (x))) + 1
2 |D ′A |(x ′
Optimization problem max
h
Gf , D A
, D ′A(h)
Y) and Pr\D (X
Table 2: Deﬁnition, gain, and optimization problem for the membership inference a(cid:31)ack h, where f is the target classiﬁer,
PrD (X
Y) are the conditional probability distributions of data points in the target training set D and outside
it, respectively. The adversary’s background knowledge is composed of datasets DA (a subset of the training set D) and D ′A
(samples drawn from Pr(X
Y) which are outside D). See Figure 1 for the illustration of the relation between h and f .
,
,
,
We can now state the optimization problem of learning a classi-
ﬁcation model as the following:
min
f
LD (f ) + λ R(f )
(3)
where R(f ) is a regularization function.
The function R(f ) is designed to prevent the model from overﬁt-
ting to its training dataset [7]. For example, the regularization loss
(penalty) increases as the parameters of the function f grow arbi-
trarily large or co-adapt themselves to ﬁt the particular dataset D
while minimizing LD (f ). If a model overﬁts, it obtains a small loss
on its training data, but fails to achieve a similar loss value on other
data points. By avoiding overﬁtting, models can generalize better
to all data samples drawn from Pr(X
Y). The regularization factor
λ controls the balance between the classiﬁcation loss function and
the regularizer.
,
For solving the optimization problem (3), especially for non-
convex loss functions for complex models such as deep neural net-
works, the commonly used method is the stochastic gradient de-
scent algorithm [4, 52]. This is an iterative algorithm where in each
epoch of training, it selects a small subset (mini-batch) of the train-
ing data and updates the model (parameters) towards reducing the
loss over the mini-batch. After many epochs of training, the algo-
rithm converges to a local minimum of the loss function.
3
3 MEMBERSHIP INFERENCE ATTACK
The objective of membership inference attacks, also referred to as
tracing attacks, is to determine whether or not a target data record
is in a dataset, assuming that the attacker can observe a function
over the dataset (e.g., aggregate statistics, model).
The membership inference attacks have mostly been studied for
analyzing data privacy with respect to simple statistical linear func-
tions [5, 17, 18, 23, 42, 43]. The attacker compares the released sta-
tistics from the dataset, and the same statistics computed on ran-
dom samples from the population, to see which one is closer to the
target data record. Alternatively, the adversary can compare the
target data record and samples from the population, to see which
one is closer to the released statistics. In either case, if the target
is closer to the released statistics, then there is a high chance that
it was a member of the dataset. The problem could be formulated
as a hypothesis test, and the adversary can make use of likelihood
ratio test to run the inference attack.
In the case of machine learning models, the membership infer-
ence attack is not as simple, especially in the black-box setting.
The adversary needs to distinguish training set members from
non-members from observing the model’s predictions, which are