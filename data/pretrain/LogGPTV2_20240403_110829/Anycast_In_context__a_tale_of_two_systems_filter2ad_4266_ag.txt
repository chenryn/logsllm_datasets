CDN User Counts (§4.3)
APNIC User Counts (§4.3)
DITL Packet Traces (§2.1)
DITL ∩ CDN (§3, §4.3, §7)
RIPE Atlas (§5.2, §7.1)
USC/ISI (§4.3)
Local DNS / Activity Measurements (§4.3)
Table 2: Summary of Datasets
# of
Measurements Duration
Year
11.0 ×109
50.0 ×107
—
—
51.9 ×109
18.6 ×109
10.0 ×103
10.0 ×107
68.0 ×104
1 week
1 week
1 month
updated daily
2 days
—
1 hour
1 year
1 month
2019
2019
2019
2019
2018
2018–2019
Various
2018
2020
# of
ASes
59 000
10 600
39 000
23 000
50 300
35 500
3 300
1
2
Technology/Format
Windows TCP/IP, HTTPService (TCP RTT)
Odin [17] (HTTP GET)
Custom URL DNS Requests
Google Ad Delivery Network
Packet Traces
Root DNS query and user counts
ping, traceroute
Packet Traces
Packet Traces, Chrome Webtime Tracker
Table 3: Strengths and Weaknesses of Datasets
Dataset
Sampled CDN Server-Side Logs (§6)
Sampled CDN Client-Side Measurements (§5.2)
CDN User Counts (§4.3)
APNIC User Counts (§4.3)
DITL Packet Traces (§2.1)
DITL ∩ CDN (§3, §4.3, §7)
RIPE Atlas (§5.2, §7.1)
USC/ISI (§4.3)
Local DNS / Activity Measurements (§4.3)
Strengths
Has client to front-end mappings, global coverage
Can hold user population fixed across rings, global coverage
Precise estimates of user counts, global coverage
Global coverage, publicly accessible
Global coverage
Global coverage, attributes queries to users
Historic data, reproducibility
Precise, below the recursive,
Precise, at the end user
Weaknesses
Cannot hold user population fixed across rings
Do not know which front-end the client reached, smaller scale
Under estimates user counts
Not validated, coarse granularity
Noisy, only above the recursive resolver
Excludes v6
Limited coverage
Limited coverage, no information about users
Limited coverage, small scale
and act as recursives for similar sets of users. We now justify this
decision and discuss the implications of this preprocessing step on
the results presented in Section 4.3.
In Table 4 we summarize the extent to which the recursives
seen by Microsoft are representative of the recursives seen in DITL,
and vice-versa, without aggregating by /24. We also display cor-
responding statistics when aggregating by /24 for comparison in
parentheses. Clearly joining by /24 makes a significant difference,
increasing various measures of overlap by tens of percents and in
certain cases by up to 64%.
As an analogy to Figure 3, in Figure 9 we show the number of
queries each Microsoft user executes to the roots per day without
aggregating query and user statistics by /24 ( CDN ). We also show
APNIC as in Figure 3 for comparison, even though APNIC is not
affected by /24 volume aggregation. Users of CDN only send 0.036
queries to the roots each day at the median – roughly one 30𝑡ℎ of
the estimate obtained when aggregating statistics by /24. This small
daily user latency makes sense, given that we only capture 8.4% of
DITL volume without joining the datasets by /24 (Table 4).
Table 4 and Figure 9, demonstrate that the decision to aggregate
statistics and join DITL captures with Microsoft user counts by /24
led to both much greater representativeness of the analysis and very
different conclusions about user interactions with the root DNS.
We would now like to justify this decision using measurements. If,
as we assume, IP addresses in the same /24 are colocated, they are
probably routed similarly. Prior work has shown that only a small
fraction of anycast paths are unstable [77], and so we expect that,
over the course of DITL, IP addresses in the same /24 reach the
same anycast sites.
As a way of quantifying routing similarity in a /24, in Figure 10
we show the percent of queries from each /24 in DITL that do not
reach the most “popular” anycast site for each /24 in each root
deployment. We label root letters alongside the total number of
Figure 9: A CDF of the number of queries Microsoft users experience
due to root DNS resolution, per day, without joining recursives by
/24 in DITL with recursives seen by Microsoft ( CDN ). This unrep-
resentative analysis yields an estimate of daily user queries far, far
lower than in Section 4.3.
Table 4: Statistics displaying the extent to which the recursives of
users in Microsoft’s CDN overlap recursives seen in the 2018 DITL
captures without users and volumes by /24. Also shown in paren-
theses are corresponding statistics when joining by /24. Joining the
datasets by /24 increases most measures of representation by tens of
percents, with some measures increased by up to 64%.
dataset
DITL ∩ CDN
Statistic
DITL Recursives
DITL Volume
CDN Recursives
CDN Volume
Percent Overlap (by /24)
2.45% (29.3%) of DITL Recursives
8.4% (72.2%) of DITL Query Volume
41.9% (78.8%) of CDN Recursives
47.05% (88.1%) of CDN Query Volume
To increase the representativeness of our analysis, we aggregate
Microsoft user counts and DITL query volumes by resolver /24,
and join the two datasets on /24 to create the DITL∩CDN dataset.
The intuition behind this preprocessing step is that IP addresses in
the same /24 are likely colocated, owned by the same organization,
10−310−210−1100101102103Queries per User per Day0.00.20.40.60.81.0CDF of UsersIdealCDNAPNICSIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
for both 2018 and 2020 DITLs, G root is not included and I root is
completely anonymized.)
Figure 10: Fractions of queries generated by /24s that do not hit the
most popular site for each /24 and for each root letter in question.
The legend indicates the number of global sites (G) and total (global
and local) sites (T). For all root letters, more than 80% of /24s have
all queries visit the most popular site, suggesting queries from the
same /24 are usually routed similarly.
(a)
𝑓 𝑘 = 1 −∑︁
𝑖
𝑞𝑘
𝑖 𝑗𝐹
𝑄𝑘
(3)
sites (local and global) that they had during the 2018 DITL. For each
root letter and for each /24 that queried that root letter in DITL, we
look at how queries from the /24 are distributed among sites.
𝑖 𝑗 be the number of daily queries from IP 𝑖 in /24 𝑘 toward
anycast site 𝑗. We then calculate the fraction of queries that do not
visit the most “popular” site as
Let 𝑞𝑘
where 𝑗𝐹 is the favorite site for /24 𝑘 (i.e., the site the /24 queries
the most), and 𝑄𝑘 is the total number of queries from /24 𝑘. We plot
these fractions for all /24s in DITL, and for each root deployment.
(We do not include /24s that had only one IP from the /24 visit the
root letter in question.)
For more than 80% of /24s, all queries visit only one site per root
letter, suggesting that queries from the same /24 are routed similarly.
This analysis is slightly biased by the size of the root deployment.
For example, two IP addresses selected at random querying B root
would hit the same site half the time, on average. However, even
for L root, with 138 sites, more than 90% of /24s direct all queries to
the most popular site. We believe Figure 10 provides evidence that
recursives within the same /24 prefix are located near each other,
and hence serve similar sets of users.
Even queries from a single IP address within a /24 may reach
multiple sites for a single root over the course of the DITL cap-
tures. Such instability can make routing look less coherent across
IP addresses in a /24, even if they are all routed the same way. Con-
trolling for cases of changing paths for the same IP makes intra-/24
routing even more coherent. If we let the distribution of queries
generated by an IP address to a root be a point mass, with all the
queries concentrated at that IP addresses’ favorite site, all queries
from more than 90% of all /24s to all roots are routed to the same
site (not shown).
B.3 Implications of Using the 2018 DITL
At the time of writing, the 2020 DITL was available to use in the
study, but we chose to use the 2018 study since the 2018 study
had better coverage of root letters. (Neither has perfect coverage –
(b)
Figure 11: Queries per user per day to the root DNS and inflation of
root letters calculated using the 2020 DITL. Our high level conclu-
sions about how much inflation is in the root DNS and the number
of queries users experience per day do not change depending on the
year.
For the 2020 DITL specifically, B root was not available at the
time of writing (but may be in the future), E root includes only one
site (out of 132), F root does not include any Cloudflare sites (more
than half the volume), and L root is completely anonymized (hence
unusable). The 2018 DITL has none of these limitations, and so our
results apply to more letters. Studying the root DNS system as a
whole is a key strength of our analysis compared to prior work, so
we feel coverage is more important than having the most up-to-date
results for only a subset of root letters.
For completeness, and to demonstrate that our larger takeaways
about root DNS latency and inflation do not change significantly
from year to year, we calculate queries per day (as in Figure 3) and
inflation (as in Figure 2) for the root letters for which we have data,
and the results are shown in Figure 11.
Our high level conclusions about root DNS latency do not change
when looking at the 2020 DITL – most users still experience about
one DNS query per day, and the number of root queries sent by
recursives is still far from the ‘ideal’ querying behavior of one
record per TTL. Inflation results are also similar – individual root
letters have less inflation (for example, D root improved). Average
geographic inflation is almost exactly the same as in 2018, with
approximately 10% of users experiencing more than 20 ms (2,000
km) of inflation.
0.00.20.40.60.8Fraction of Queries Generated by /24 That Did Not Go To Favorite Site0.00.20.40.60.81.0CDF of /24sB Root (2G 2T)A Root (5G 5T)M Root (5G 6T)C Root (10G 10T)E Root (15G 85T)D Root (20G 117T)K Root (52G 53T)J Root (68G 110T)F Root (94G 141T)L Root (138G 138T)10−310−210−1100101102103Queries per User per Day0.00.20.40.60.81.0CDF of UsersIdealCDNAPNIC020406080100120140Geographic Inflation per Root Query (ms)0.00.10.20.30.40.50.60.70.80.91.0CDF of UsersM - 8H - 8C - 10D - 23A - 51K - 75J - 127All RootsSIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
𝑁 =(cid:6)𝑙𝑜𝑔2
𝐷
𝑊
(cid:7)
C NUMBER OF RTTS IN A PAGE LOAD
To estimate the latency a user experiences when interacting with
Microsoft’s CDN (§5.2), we first estimate the number of RTTs re-
quired to load a typical web page hosted by Microsoft’s CDN. The
number of RTTs in a page load depends on a variety of factors, so
we aim to find a reasonable lower bound on the number of RTTs
users incur for typical pages. A lower bound on the number of
RTTs to load pages is a conservative measure of the impact of
CDN inflation, as latency inflation accumulates with each addi-
tional RTT, and larger pages (more RTTs) would be impacted more.
We provide an estimate of this lower bound based on modeling
and evaluation of a set of web pages hosted by Microsoft’s CDN
using Selenium (a headless web browser), finding that 10 RTTs is
a reasonable estimate. We scale latency by the number of RTTs in
Section 5.2 to demonstrate how improvements in latency help users
(and, conversely, how inflation hurts users).
Users incur latency to Microsoft’s CDN when they download
web objects via HTTP. We calculate the number of RTTs required
to download objects in each connection separately, and sum RTTs
over connections while accounting for parallel connections. For
a single TCP connection, the number of RTTs during a page load
depends on the size of files being downloaded. This relationship is
approximated by
(4)
where 𝑁 is the number of RTTs, 𝐷 is the total number of bytes