period of time before it is deleted. In addition a post-
ing can be censored by a particular news administra-
tor or by someone posting cancel or supercede requests
(http://www.faqs.org/faqs/usenet/cancel-faq/)
to
Usenet.
A much more ambitious implementation is currently
being designed (http://www.cypherspace.org/
eternity-design.html).
FreeNet [7] is an adaptive network approach to the
censorship problem. FreeNet is composed of a net-
work of computers (nodes) each of which is capable
of storing ﬁles locally. In addition, each node in the
network maintains a database that characterizes the
ﬁles stored on some of the other nodes in the network.
When a node receives a request for a non-local ﬁle it
uses the information found in its database to decide
which node to forward the request to. This forwarding
is continued until either the document is found or the
message is considered timed-out. If the document is
found it is passed back through the chain of forward-
ing nodes. Each node in this chain can cache the ﬁle
locally. It is this caching that plays the main role in
dealing with the censorship issue. The multiple copies
make it diﬃcult for someone to censor the material. A
ﬁle can be published anonymously by simply upload-
ing it to one of the nodes in the adaptive network.
The FreeNet implementation is still in its infancy and
many features still need to be implemented.
Intermemory [11] is a system for achieving an im-
mense self-replicating distributed persistent RAM us-
ing a set of networked computers. An individual wish-
ing to join the Intermemory donates some disk space,
for an extended period of time, in exchange for the
right to store a much smaller amount of data in the
Intermemory. Each donation of disk space is incorpo-
rated into the Intermemory. Data stored on the In-
termemory is automatically replicated and dispersed.
It is this replication and dispersion that gives the In-
termemory properties similar to Anderson’s Eternity
Service. The main focus of the Intermemory project is
not anonymous publishing but rather the preservation
of electronic media. A small Intermemory prototype
is described in [6]. The security and cryptographic
components were not fully speciﬁed in either paper so
we cannot comment on its anonymity properties.
Benes [4] describes in detail how one might imple-
ment a full-ﬂedged Eternity service. Benes and several
students at Charles University are attempting to cre-
ate a software implementation of the Eternity Service
based on this thesis.
3 Publius
In this section we describe how our system achieves
the stated goals. We call the content that is pub-
lished with the desired robustness properties Publius
content.
3.1 Overview
Our system consists of publishers who post Publius
content to the web, servers who host random-looking
content, and retrievers who browse Publius content on
the web. At present the system supports any static
content such as HTML pages, images, and other ﬁles
such as postscript, pdf, etc. Javascript also works.
However, there is no support for interactive scripting
such as CGI. Also, Java applets on Publius pages are
limited in what they can do.
We assume that there is a static, system-wide list
of available servers. Publius content is encrypted by
the publisher and spread over some of the web servers.
In our current system, the set of servers is static. The
publisher takes the key, K that is used to encrypt the
ﬁle to be published and splits it into n shares, such
that any k of them can reproduce the original K, but
k − 1 give no hints as to the key [22].
Each server receives the encrypted Publius content
and one of the shares. At this point, the server has no
idea what it is hosting – it simply stores some random
looking data.
To browse content, a retriever must get the en-
crypted Publius content from some server and k of
the shares. As described below, a mechanism is in
place to detect if the content has been tampered with.
The publishing process produces a special URL that
is used to recover the data and the shares. The pub-
lished content is cryptographically tied to the URL.
Any modiﬁcation to the stored Publius content or the
URL results in a failed tamper check. If all tamper
checks fail the Publius content cannot be read.
In addition to the publishing mechanism, we pro-
vide a way for publishers (and nobody else) to update
or delete their Publius content.
In the next several
sections, we describe the Publius functions in some
detail. We use a simple example of a publisher with
one HTML ﬁle. Publishing more complicated con-
tent, such as web pages that have links to each other,
is covered in Section 4.
3.2 Publish
The following text describes the publish pseu-
docode of Figure 1. This pseudocode is executed by
the Publius client proxy in response to a publish re-
quest. To publish Publius content, M , the publisher,
Alice, ﬁrst generates a random symmetric key, K. She
then encrypts M to produce {M }K, M encrypted un-
der K, using a strong symmetric cipher. Next, Alice
splits K into n shares using Shamir secret sharing,
such that any k of them can reproduce the secret.
that, with high probability, d unique servers are found.
This problem is equivalent to the well known Coupon
Collectors Problem [15].
In the Coupon Collectors
Problem there are y diﬀerent coupons that a collector
wishes to collect. The collector obtains coupons one
at a time, randomly with repetitions. The expected
number of coupons the collector needs to collect be-
fore obtaining all y diﬀerent coupons is y ∗ ln(y). By
analogy, a unique slot in the available server list is
equivalent to a coupon. Therefore for each key K we
create n = (cid:100)d ∗ ln(d)(cid:101) shares. Any unused shares are
thrown away.
Now, Alice uses each locationi as an index into
the list of servers. Alice publishes {M }k, sharei, and
some other information in a directory called namei
on the server at location locationi in the static list.
Thus, given M , K, and m, the locations of all of the
shares are uniquely determined. The URL that is pro-
duced contains at least d namei values concatenated
together. A detailed description of the URL structure
is given in Section 4.
Figure 2 illustrates the publication process.
For each of the n shares, Alice computes
3.3 Retrieve
namei = wrap(H(M · sharei))
That is, each share has a corresponding name. The
name is calculated by concatenating the share with
the message, taking a cryptographic hash, H, of the
two, and xoring the ﬁrst half of the hash output with
the second half. We call the xor of the two halves
wrap.
In our system, we use MD5 [20] as the hash
function, so each namei is 8 bytes long. Note that
the namei’s are dependent on every bit of the web
page contents and the share contents. The namei val-
ues are used in the Publius server addressing scheme
described below.
Recall that each publisher possesses a static list of
size m of the available servers in the system. For each
of the n shares, we compute
locationi = (namei MOD m) + 1
to obtain n values each between 1 and m. If at least
d unique values are not obtained, we start over and
pick another K. The value d represents the minimum
number of unique servers that will hold the Publius
content. Clearly this value needs to be greater than
or equal to k since at least k shares are needed to re-
construct the key K. d should be somewhat smaller
than m. It is clearly desirable to reduce the number
of times we need to generate a new key K. There-
fore we need to create a suﬃcient number of shares so
The following text describes the retrieve pseu-
docode of Figure 3. This pseudocode is executed by
the Publius client proxy in response to a retrieve re-
quest. The retriever, Bob, wishes to view the Publius
content addressed by Publius URL U . Bob parses out
the namei values from U and for each one computes
locationi = (namei MOD m) + 1
Thus, he discovers the index into the table of servers
for each of the shares. Next, Bob chooses k of these
arbitrarily. From this list of k servers, he chooses one
and issues an HTTP GET command to retrieve the
encrypted ﬁle and the share. Bob knows that the
encrypted ﬁle, {M }K is stored in a ﬁle called ﬁle on
each server, in the namei directory. The key share is
stored in a ﬁle called share in that same directory.
Next, Bob retrieves the other k − 1 shares in a
similar fashion (If all goes well, he does not need to
retrieve any other ﬁles or shares). Once Bob has all
of the shares, he combines them to form the key, K.
Then, he decrypts the ﬁle. Next, Bob veriﬁes that
all of the namei values corresponding to the selected
shares are correct by recomputing
namei = wrap(H(M · sharei))
using M that was just decrypted. If the k namei’s are
all correct (i.e.
if they match the ones in the URL),
Procedure P ublish (document M )
Generate symmetric key K
Encrypt M under key K producing {M }K
Split key K into n shares such that k shares are required to reconstruct K
Store the n shares in array share[1..n]
locations used = {}
for i = 1 to n:
name=MD5(M · share[i])
name=XOR(top 64 bits(name),bottom 64 bits(name))
location=(name MOD serverListSize)+1
if (location is not a member of locations used):
locations used = locations used ∪ {location}
serverIP Address = serverList[location]
Insert (serverIP Address, share[i]) into Publish Queue
publiusU RL = publiusU RL · name
endif
endfor
if (sizeof(locations used)< d) then
Empty (Publish Queue)
return P ublish(M )
else
for each (serverIP Address, share) in Publish Queue:
HTTP PUT({M }K and share on Publius Server with IP address serverIP Address)
return publiusU RL
endif
End P ublish
Figure 1: Publish Algorithm
Bob can be satisﬁed that either the document is in-
tact, or that someone has found a collision in the hash
function.
If something goes wrong, Bob can try a diﬀerent set
of k shares and an encrypted ﬁle stored on one of the
other n servers. In the worst case, Bob may have to
try all of the possible (cid:0)n
k(cid:1) combinations to get the web
page before giving up. An alternate retrieval strategy
would be to try all n∗ (cid:0)n
k(cid:1) combinations of shares and
documents. Each encrypted document can be tested
against each of the (cid:0)n
k(cid:1) share combinations.
If we are willing to initially download all the shares
from all the servers then yet another method for deter-
mining the key becomes available. In [10], Gemmell
and Sudan present the Berlekamp and Welch method
for ﬁnding the polynomial, and hence the key K, cor-
responding to n shares of which at most j are corrupt.
The value j must be less than (n − d)/2 where d is
one less than the number of shares needed to form
the key. However if the number of corrupt shares is
greater than (n − d)/2 we are not quite out of luck.
We can easily discover whether K is incorrect by per-
forming the veriﬁcation step described above. Once
we suspect that key K is incorrect we can just per-
form a brute force search by trying all n∗ (cid:0)n
k(cid:1) combi-
nations of shares and documents. The following ex-
ample illustrates this point. If we have n = 10 shares
and require 3 shares to form K then the Berlekamp
and Welch method will generate the correct polyno-
mial only if less than ((10 − 2)/2) = 4 shares are
corrupted. Suppose 6 shares are corrupt. Of course
we don’t know this ahead of time so we perform the
Berlekamp and Welch method which leads us to key
K. Key K is tested against a subset of, or perhaps
all, the encrypted documents. All of the tamper check
failures lead us to suspect that K is incorrect. There-
fore we perform a brute force search for the correct key
by trying all n∗ (cid:0)n
k(cid:1) combinations of shares and doc-
uments. Assuming we have a least one untampered
encrypted document this method will clearly succeed
as we have 4 uncorrupted shares, only three of which
are needed to form the correct key.
Once the web page is retrieved Bob can view it in
his browser. In our implementation, all of the work
is handled by the proxy. Publius URLs are tagged as
special, and they are parsed and handled in the proxy.
The proxy retrieves the page, does all of the veriﬁca-
tion, and returns the web content to the browser. So,
all of this is transparent to the user. The user just
points and clicks as usual. Section 4 describes Pub-
lius URL’s and the proxy software in detail.
Available
Server Table
135.207.8.15
121.113.8.5
105.3.14.1
201.18.24.5
...
210.183.28.4
...
209.185.143.19
...
206.35.113.9
1
2
3
4
7
12
m
name   = 1e0995d6698
name   = 620a8a3d63b
1
name   = de26fe4fc8c6
...
...
2
n
2
n
loction   = 4
location   = 12
1
location   = 7
Server 3
Server 4
201.18.24.5
Server 8
/publius/1e0995d6698/{M}K
Server 12
209.185.143.19
Server 7
210.183.28.4
/publius/620a8a3d63b/{M}K
/publius/de26fe4fc8c6/{M}K
Servers
Publisher
Figure 2: The Publius publication process The publisher computes the namei values by hashing the web
page and the symmetric key shares together. Then, those values are used to compute the locations. The publisher
then uses the location value as an index into the static location table and publishes the encrypted ﬁle, along with
the share in a directory named namei on the appropriate server.
3.4 Delete
It is desirable for Alice to be able to delete her
Publius content from all servers, while nobody else
should be able to delete this content. To achieve
this, just before Alice publishes a ﬁle she generates
a password P W . Alice then sends the encrypted doc-
ument, share and H(server domain name · P W ) to
the servers that will be hosting Alice’s published doc-
ument. H(server domain name · P W ) is the hash
of the domain name of the server concatenated with
a password P W . The server stores this hash value
in the same directory as the encrypted ﬁle and the