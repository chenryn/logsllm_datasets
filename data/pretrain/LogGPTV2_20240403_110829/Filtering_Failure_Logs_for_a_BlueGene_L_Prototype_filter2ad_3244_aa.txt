title:Filtering Failure Logs for a BlueGene/L Prototype
author:Yinglung Liang and
Yanyong Zhang and
Anand Sivasubramaniam and
Ramendra K. Sahoo and
Jos&apos;e E. Moreira and
Manish Gupta
Filtering Failure Logs for a BlueGene/L Prototype
Yinglung Liang† Yanyong Zhang† Anand Sivasubramaniam‡ Ramendra K. Sahoo§
Jose Moreira§ Manish Gupta§
†ECE Dept.
‡CSE Dept.
Rutgers University
{ylliang,yyzhang}@ece.rutgers.edu
Penn State University
PI:EMAIL
§Exploratory Systems Software Dept.
IBM T. J. Watson Research Center
{rshaoo,jmoreira,mgupta}@us.ibm.com
Abstract
The growing computational and storage needs of several
scientiﬁc applications mandate the deployment of extreme-
scale parallel machines, such as IBM’s BlueGene/L which
can accommodate as many as 128K processors. In this pa-
per, we present our experiences in collecting and ﬁltering
error event logs from a 8192 processor BlueGene/L pro-
totype at IBM Rochester, which is currently ranked #8 in
the Top-500 list. We analyze the logs collected from this
machine over a period of 84 days starting from August 26,
2004. We perform a three-step ﬁltering algorithm on these
logs: extracting and categorizing failure events; temporal
ﬁltering to remove duplicate reports from the same loca-
tion; and ﬁnally coalescing failure reports of the same er-
ror across different locations. Using this approach, we can
substantially compress these logs, removing over 99.96% of
the 828,387 original entries, and more accurately portray
the failure occurrences on this system.
1
Introduction
The growing computational and storage demands of
applications continue to fuel the research and develop-
ment of high-end computer systems, whose capabilities
and scale far exceed those available in the market today.
Many of these applications play critical roles in impacting
economies of enterprizes and even countries, health and hu-
man development, military/security, and in enhancing the
overall quality of life.
It is widely recognized that par-
allelism in processing and storage is essential to meet the
immense demands imposed by many of these applications
(e.g. protein folding, drug discovery, weather modeling, na-
tional infrastructure simulations, etc.), and there is a press-
ing need to accelerate the deployment of large scale paral-
lel systems with several thousand processors. IBM’s Blue-
Gene/L [3, 15, 6, 5] is a recent commercial offering to meet
these demands, with two deployments of this system (on a
smaller scale than the maximum 128K processors allowed
by this architecture) already making it to the top 10 of the
Top 500 Supercomputers list [2].
While performance, and the usability issues (such as pro-
gramming/tuning tools) to some extent, have been the pri-
mary targets of investigation traditionally, there is a growing
problem - the occurrence of failures - which is demanding
equal attention [17, 18, 20, 29, 22, 4, 13, 7, 21, 27, 11, 25,
23, 28]. Although fault-aware design has gained impor-
tance for uniprocessor and small-scale systems, the prob-
lem escalates to a much higher magnitude when we move
to the large scale parallel systems. In addition to the multi-
plicative factor in the error occurrences, the complex inter-
dependencies between the processors (e.g.
in the way the
applications communicate, in the way the processors are al-
located to jobs, etc.) exacerbate the error rates of these sys-
tems [11, 25, 23, 28] containing thousands of processors.
Rather than treat errors/failures as an exception, we need to
recognize that they are commonplace on these systems.
Understanding the failure behavior of large scale paral-
lel systems is crucial towards alleviating these problems.
This requires continual online monitoring and analysis of
events/failures on these systems over extended periods of
time. The data obtained from such analysis can be useful in
several ways. The failure data can be used by hardware and
system software designers during early stages of machine
deployment in order to get feedback about system fail-
ures and performance from the ﬁeld (for hardware/software
revisions).
It can help system administrators for main-
tenance, diagnosis, and enhancing the overall health (up-
time). They can isolate where problems occur, and take ap-
propriate remedial actions (replace the node-card, replace
the disks/switches, reboot a node, select points for software
rejuvenation, schedule down-times, etc.). Finally, it can be
useful in ﬁne-tuning the runtime system for checkpointing
(e.g. modulating the frequency of checkpointing based on
error rates), job scheduling (e.g. allocating nodes which are
less failure prone), network stack optimizations (e.g. em-
ploying different protocols and routes based on error condi-
tions), etc.
Even if there have been production environments collect-
ing these system events/failures, there are no published re-
sults to date on such data in the context of large scale par-
allel machines. More importantly, the monitoring and log-
ging of these events need to be continuously done over ex-
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
1
tended periods of time, spanning several months/years. The
consequence of such monitoring is the voluminous amount
of information that can accumulate because of the tempo-
ral (over long periods of time) and spatial (on each of the
thousands of nodes) dimensions that are involved. Further,
there are (i) several extraneous (and non-critical) events that
can get recorded, (ii) the same event could get recorded in
multiple ways at a node, and (iii) the same event could be
detected in different ways across the nodes. Consequently,
it becomes imperative to ﬁlter this evolving data and isolate
the speciﬁc events that are needed for subsequent analysis.
Without such ﬁltering, the analysis can lead to wrong con-
clusions. For instance, the same software error can materi-
alize on all nodes that an application is running on, and can
reduce the MTBF than what is really the case.
To our knowledge, this is the ﬁrst paper to present sys-
tem event/failure logs from an actual 8192-processor Blue-
Gene/L system over a period of 84 days, and describes
our experiences in ﬁltering these events towards identify-
ing the speciﬁc failures to be useful in subsequent analy-
sis. In the raw event logs from this system, we have over
828,387 events with 211,997 fatal failures. We ﬁrst per-
form a temporal ﬁltering step by eliminating event records
at a node which are within 2 minutes of each other. This
step brings down the fatal failures to 9,150. However, when
we closely examine the log after the temporal ﬁltering, we
still ﬁnd that not all events are independent/unique. We
break down the events based on the system components -
the memory hierarchy, the torus network, the node cards,
the service cards, and the midplane switches - impacted by
these failures. We then show how the event records for each
of these components can further be ﬁltered by a closer ex-
amination of the nature of the errors and how the errors get
reported/propagated. At the end of all such ﬁltering, we iso-
late the number of actual errors over this 84 day period to
3111, which represents a reduction of 99.96% of the fatal
failures reported in the original logs. Such ﬁltering is ex-
tremely important when accumulating and processing the
failures in a continuous fashion for online decision making.
The rest of this paper is organized as follows. The next
section brieﬂy summarizes related work. Subsequently, we
give an overview of the BG/L architecture, together with
speciﬁcs on the system where the logs were collected in
Section 3. The details on the ﬁltering steps are given in sec-
tion 4. Finally, section 5 provides the concluding remarks.
2 Related Work
Though there has been previous interest in monitoring
[16, 8, 25, 23]),
and ﬁltering system events/failures (e.g.
1Please note that these numbers do not reﬂect the MTBF of actual Blue-
Gene machine [1] because the prototype is not as mature as the actual de-
ployment in terms of both hardware and software.
there has been no prior published work on failures in large
scale parallel systems spanning thousands of nodes. Collec-
tion and ﬁltering of failure logs has been examined in the
context of much smaller scale systems. Lin and Siewiorek
[16] found that error logs usually consist of more than one
failure process, making it imperative to collect these logs
over extended periods of time. In [8], the authors make rec-
ommendations about how to monitor events and create logs.
It has been recognized [26, 9, 14, 12] that it is critical to co-
alesce related events since failures propagate in the time and
error detection domain. The tupling concept developed by
Tsao [26], groups closely related events, and is a method
of organizing the information in the log into a hierarchical
structure to possibly compress failure logs [9].
Tang [25, 23] studied the error/failure log collected from
a VAX cluster system consisting of seven machines and four
storage controllers. Using a semi-Markov failure model,
they further pointed out that failure distributions on differ-
ent machines are correlated rather than independent. In their
subsequent work [24], Tang and Iyer pointed out that many
failures in a multicomputer environment are correlated with
each other, and they studied the impact of correlated failures
on such systems. Xu [28] performed a study of error logs
collected from a heterogeneous distributed system consist-
ing of 503 PC servers. They showed that failures on a ma-
chine tend to occur in bursts, possibly because common so-
lutions such as reboots cannot completely remove the prob-
lem causing the failure. They also observed a strong indi-
cation of error propagation across the network, which leads
to the correlation between failures of different nodes. In our
previous study [19], we reported failure data for a large-
scale heterogenous AIX cluster involving 400 nodes over a
1.5 year period and studied their statistical properties.
3 BG/L Architecture and Error Logs
In this section, we give an overview of the BG/L pro-
totype on which the event logging has been performed, to-
gether with details on the logs themselves.
3.1 BG/L Architecture
In this study, we use event/failure logs collected from a
8192-processor BlueGene/L DD1 prototype, housed at IBM
Rochester, which currently stands at number 8 in the Top
500 list of supercomputers [2]. The machine has been up
since May, 2004, and has been primarily running parallel
scientiﬁc applications. This prototype has 4096 compute
chips, with each chip containing two processors.
Each compute chip consists of two PPC 440 cores (pro-
cessors), with a 32KB L1 cache and a 2 KB L2 (for
prefetching) for each core. The cores share a 4MB EDRAM
L3 cache. In addition, a shared fast SRAM array is used
for communication between the two cores. L1 caches in
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
2
the cores are parity protected, and so are most of the in-
ternal buses. The L2 caches and EDRAM are ECC pro-
tected. Each chip has the following network interfaces: (i)
a 3-dimensional torus interface that supports point-to-point
communication, (ii) a tree interface that supports global op-
erations (barriers, reductions, etc.), (iii) a gigabit ethernet
interface that supports ﬁle I/O and host interfaces, and (iv)
a control network that supports booting, monitoring and di-
agnostics through JTAG access.
A compute card contains two such chips, and also houses
a 256MB or 512MB DRAM for each chip on the card. In
this paper, we use the term compute card and compute node
interchangeably. A node card contains 16 such compute
cards, and a midplane holds 16 node cards (a total of 512
compute chips or 1K processors). The BG/L prototype has
2048 compute cards, 128 node cards, and 8 midplanes, that
are housed in 4 racks. I/O cards, each containing two chips
and some amount of memory (usually larger than that of
compute nodes), are also housed on each midplane. There
are 4 such I/O cards (i.e., 8 I/O chips) for each midplane on
the prototype, i.e. 1 I/O chip for every 64 compute chips.
All compute nodes go through the gigabit ethernet interface
to these I/O chips for their I/O needs.
In addition, a midplane also contains 24 midplane
switches (“link chips”) to connect with other midplanes.
When crossing a midplane boundary, BG/L’s torus, global
combining tree and global interrupt signals pass through
these link chips. The BG/L prototype, thus, has 192 link
chips totally. A midplane also has 1 service card that per-
forms system management services such as verifying the
heart beat of the nodes, and monitoring errors. This card
has much more powerful computing abilities, and runs a
full-ﬂedged IBM DB2 database engine for event logging.
In most cases, a midplane is the granularity of job allo-
cation, i.e., a job is assigned to an integral number of mid-
planes. Though rare, it is also possible to subdivide a mid-
plane, and allocate part of it (an integral number of node
cards) to a job. However, in this case, a job cannot run
on node cards across different midplanes. Compute cards
are normally shut down when they are not running any job.
When a job is assigned, the card is reset and the network is
conﬁgured before execution begins.
3.2 System Error Logs
Error events are logged through the Machine Monitoring
and Control System (MMCS). There is one MMCS process
per midplane, running on the service node. However, there
is a polling agent that runs on each compute chip. Errors
detected by a chip are recorded in its local SRAM via an in-
terrupt. The polling agent at some later point would pick up
the error records from this SRAM and ship them to the ser-
vice node using a JTAG-mailbox protocol. The frequency
of the polling needs to be tuned based on the SRAM ca-
pacity, and speeds of the network and the compute nodes.
The failure logs that we have obtained are collected at the
frequency of less than a millisecond.
After procuring the events from the individual chips of
that midplane, the service node records them through a DB2
database engine. These events include both hardware and
software errors at individual chips/compute-nodes, errors
occurring within one of the networks for inter-processor
communication, and even errors such as temperature emer-
gencies and fan speed problems that are reported through
an environmental monitoring process in the backplane. Job
related information is also recorded in job logs.
We have been collecting failure logs since August 26,
2004 until the present. The raw logs contain all the events
that occur within different components of the machine. In-
formation about scheduled maintenances, reboots, and re-
pairs is not included. Each record of the logs describes an
event using several attributes as described below:
• Record ID is the sequence number for an error entry,
which is incremented upon each new entry being ap-
pended to the logs.
• Event time is the time stamp associated with that event.
• Event type speciﬁes the mechanism through which the
event is recorded, with most of them being through
RAS [10].
• Event Severity can be one of the following levels -
INFO, WARNING, SEVERE, ERROR, FATAL, or
FAILURE - which also denotes the increasing order
of severity. INFO events are more informative in na-
ture on overall system reliability, such as “a torus prob-
lem has been detected and corrected”, “the card status
has changed”, “the kernel is generating the core”, etc.
WARNING events are usually associated with node-
card/link-card/service-card not being functional. SE-
VERE events give more details on why these cards
may not be functional (e.g.“link-card is not acces-
sible”, “problem while initializing link/node/service
card”, “error getting assembly information from the
node card”, etc.). ERROR events report problems that
are more persistent and further pin-point their causes
(“Fan module serial number appears to be invalid”,
“cable x is present but the corresponding reverse ca-
ble is missing”, “Bad cables going into the linkcard”,
etc.). All of these above events are either informative
in nature, or are related more to initial conﬁguration
errors, and are thus relatively transparent to the ap-
plications/runtime environment. However, FATAL or
FAILURE events (such as “uncorrectable torus error”,
“memory error”, etc.) are more severe, and usually
lead to application/software crashes. Our primary fo-
cus in this study is consequently on FATAL and FAIL-
URE events.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
3
• Facility attribute denotes the component where the
event is ﬂagged, which can be one of the following:
LINKCARD, APP, KERNEL, DISCOVERY, MMCS,
or MONITOR. LINKCARD events report problems
with midplane switches, which is related to commu-
nication between midplanes. APP events are those
ﬂagged in the application domain of the compute
chips. Many of these are due to the application be-
ing killed by certain signals from the console. In addi-
tion, APP events also include network problems cap-
tured in the application code. Events with KERNEL
facility are those reported by the OS kernel domain
of the compute chips, which are usually in the mem-
ory and network subsystems. These could include
memory parity/ECC errors in the hardware, bus errors
due to wrong addresses being generated by the soft-
ware, torus errors due to links failing, etc. Events
with DISCOVERY facility are usually related to re-
source discovery and initial conﬁgurations within the
machine (e.g. “service card is not fully functional”,
“fan module is missing”, etc), with most of these be-
ing at the INFO or WARNING severity levels. MMCS
facility errors are again mostly at the INFO level,
which report events in the operation of the MMCS. Fi-
nally, events with MONITOR facility are usually re-
lated to the power/temperature/wiring issues of link-
card/node-card/service-card. Nearly all MONITOR
events are in the FATAL or FAILURE severity levels.
event
(i.e., which chip/node-card/service-card/link-card ex-
periences the error), can be speciﬁed in two ways. It
can either be speciﬁed as (i) a combination of job ID,
processor, node, and block, or (ii) through a separate
location ﬁeld. We mainly use the latter approach (loca-
tion attribute) to determine where an error takes place.
• Location
of
an
Between August 26, 2004 and November 17, 2004 (84
days), there are totally 828,387 events in the raw log.
4 Filtering and Preprocessing Techniques
While event logs can help one understand the failure
properties of a machine to enhance the hardware and sys-
tems software for better failure resilience/tolerance, it has
been recognized [26, 9, 14, 12] that such logs must be care-
fully ﬁltered and preprocessed before being used in decision
making since they usually contain a large amount of redun-
dant information. As today’s machines keep boosting their
logging granularity, ﬁltering is becoming even more critical.
Further, the need to continuously gather these logs over ex-
tended periods of time (temporal) and across the thousands
of hardware resources/processors (spatial) on these paral-
lel machines, exacerbates the volume of data collected. For
example, the logging tool employed by the BG/L prototype
has generated 828,387 entries over a period of 84 days. The
large volume of the raw data sets, however, should not be
simply interpreted as a high system failure rate. Instead, it
calls for an effective ﬁltering tool to parse the logs and iso-
late unique failure records. In this section, we present our
ﬁltering algorithm, which involves three steps: ﬁrst extract-
ing and categorizing failure events from the raw logs, then
performing a temporal ﬁltering step to remove duplicate re-
ports from the same location, and ﬁnally coalescing failure
reports across multiple locations. Using these techniques,
we can substantially compress the generated error logs and
more accurately portray the failure occurrences on the BG/L
prototype for subsequent analysis.
4.1 Step I: Extracting and Categorizing Failure
Events
While informational warnings and other non-critical er-
rors may be useful for system diagnostics and other issues,
our main focus is on isolating and studying the more se-
rious hardware and software failures that actually lead to
application crashes since those are what are needed for soft-
ware/hardware revisions and designing more effective fail-
ure resilience/tolerance mechanisms. Consequently, we are
mainly interested in events at severity level of either FA-
TAL or FAILURE, which are referred to as failures in this
paper. As a result, we ﬁrst screen out events with lower
severity levels. This step removes 616,390 error records
from 828,387 total entries, leaving only 211,997 failures,
which constitute 14.7% of the total error records. Next,
we remove those application level events due to applica-
tions being killed by signals from the console (these en-
tries usually have APP facility and their event descriptions
start with “Applications killed by signal x”), since these are
not failures caused by the underlying system (hardware or
software). This step further brings down the log size from
211,997 to 190,775 entries.
Instead of lumping together failures from different com-
ponents of the machine, we categorize them into ﬁve
classes: (i) memory failures, denoting failures in any part
of the memory hierarchy on all the compute nodes; (ii) net-
work failures, denoting exceptions within the torus when
application processes running on the compute nodes ex-
change messages; (iii) node card failures, denoting prob-
lems with the operations of node cards;
(iv) service
card failures, denoting errors with service cards; and (v)
midplane switch failures denoting failures with midplane
switches or their links. These ﬁve classes cover all the major
components of the BG/L hardware, or at least those compo-
nents that are included in the error events. It is to be noted
that our original plan was to include compute nodes as one
of BG/L’s components instead of memory, but we use mem-
ory in this paper because nearly all the failures we have ob-
served so far on a compute node are within the memory hi-
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
4
71174 RAS KERNEL FATAL 2004-09-11 10:16:18.996939
71175 RAS KERNEL FATAL 2004-09-11 10:16:19.093152
71176 RAS KERNEL FATAL 2004-09-11 10:16:19.177100
71177 RAS KERNEL FATAL 2004-09-11 10:16:19.229378
71178 RAS KERNEL FATAL 2004-09-11 10:16:19.319986