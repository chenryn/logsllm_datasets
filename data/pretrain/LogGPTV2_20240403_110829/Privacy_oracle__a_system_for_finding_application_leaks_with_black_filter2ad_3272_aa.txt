title:Privacy oracle: a system for finding application leaks with black
box differential testing
author:Jaeyeon Jung and
Anmol Sheth and
Ben Greenstein and
David Wetherall and
Gabriel Maganis and
Tadayoshi Kohno
Privacy Oracle: a System for Finding Application Leaks
with Black Box Differential Testing
Jaeyeon Jung, Anmol Sheth, Ben Greenstein, David Wetherall
Intel Research, Seattle, WA
{jaeyeon.jung, anmol.n.sheth, benjamin.m.greenstein, david.wetherall}@intel.com
Gabriel Maganis, Tadayoshi Kohno
University of Washington, Seattle, WA
{gym,yoshi}@cs.washington.edu
ABSTRACT
We describe the design and implementation of Privacy Oracle, a
system that reports on application leaks of user information via the
network trafﬁc that they send. Privacy Oracle treats each appli-
cation as a black box, without access to either its internal struc-
ture or communication protocols. This means that it can be used
over a broad range of applications and information leaks (i.e., not
only Web trafﬁc content or credit card numbers). To accomplish
this, we develop a differential testing technique in which perturba-
tions in the application inputs are mapped to perturbations in the
application outputs to discover likely leaks; we leverage alignment
algorithms from computational biology to ﬁnd high quality map-
pings between different byte-sequences efﬁciently. Privacy Oracle
includes this technique and a virtual machine-based testing system.
To evaluate it, we tested 26 popular applications, including sys-
tem and ﬁle utilities, media players, and IM clients. We found that
Privacy Oracle discovered many small and previously undisclosed
information leaks. In several cases, these are leaks of directly iden-
tifying information that are regularly sent in the clear (without end-
to-end encryption) and which could make users vulnerable to track-
ing by third parties or providers.
Categories and Subject Descriptors
D.2.5 [SOFTWARE ENGINEERING]: Testing and Debugging
General Terms
Experimentation, Security
Keywords
Personal information leaks, Black-box testing, Sequence alignment
algorithm, Differential fuzz testing, Data loss prevention
1.
INTRODUCTION
Modern personal computational devices, from desktops to mo-
bile computers, smart phones and consumer electronics, run a wide
variety of applications that send user information over the network
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’08, October 27–31, 2008, Alexandria, Virginia, USA.
Copyright 2008 ACM 978-1-59593-810-7/08/10 ...$5.00.
to other parties. This information is used in many productive ways,
e.g., current location information is used to interact with maps.
However, leaks of personal information are also a potential cause
of concern because they may invade privacy in ways that were not
expected or desired by users, e.g., when third parties build detailed
proﬁles of user behavior. Thus, we believe that users would prefer
to be aware of the information exposed by their applications so that
they can assess whether it meets their needs. Similarly, people who
administer machines stand to beneﬁt from knowledge of what ap-
plications are disclosing which information to whom, so that they
can better assess and set privacy and security policies.
Today, however, it is very difﬁcult to know how personal infor-
mation is disclosed by networked applications. Users must rely
heavily on descriptions provided by application developers (or mar-
keters) since there is no real, independent way to automatically ver-
ify what is disclosed in practice. Tools to check network trafﬁc for
personal information are typically limited to a small set of well-
deﬁned information, such as credit card or social security num-
bers. Thus, many information leaks are discovered accidentally.
For example, recent events include concern over surreptitious user
tracking as part of Adobe Creative Suite 3 [21], and concern over
spyware bundled with Sears My SHC Community software [22].
These events suggest both public interest in and demand for a bet-
ter understanding of the privacy properties of their devices, e.g.,
Nike+iPod Sport Kit [20]. Further, data loss prevention is an im-
portant yet challenging goal for companies and other large organi-
zations.
The goal of our work is to develop general tools and techniques
that enable consumers and their agents to discover leaks of per-
sonal information in applications. To be actionable, we want to
characterize leaks in terms of what the information is, when is it
exposed, and who can receive it; leaks should be characterized in
a speciﬁc but sufﬁciently high-level manner to be meaningful to
users. To provide signiﬁcant value over existing point solutions
(such as regexp-based network scanners), we want these tools to be
applicable to a wide variety of applications and platforms, and to
be able to discover a broad set of information leaks. This precludes
us from requiring access to source code or proprietary information,
or from depending on platform-speciﬁc information tracking (taint)
systems such as Panorama [29].
Our approach is to employ a conceptually straightforward testing
and analysis methodology that we refer to as differential black-box
fuzz testing, which is embodied in the Privacy Oracle system that
we present and evaluate in this paper. We treat an application as
a black-box, to gain broad applicability by remaining agnostic to
its internal structure and communication protocols. We test the ap-
plication with different inputs, mapping input perturbations to out-
put perturbations to infer the likely leaks of personal information1.
To make the problem tractable, we focus on information that the
device exposes explicitly, whether over a radio or wired network
connection. That is, we ignore information that a device might ac-
cidentally expose via side-channels like timing variations or packet
sizes.
The key challenges, which we address in the body of this paper,
are in making this approach work well in practice. One issue is
that output changes can be caused by many factors besides input,
such as the environment or remote parties that interact with the ap-
plication. We ﬁnd that we can use virtual machines and automated
test harnesses to provide a sufﬁciently repeatable environment for
many applications. Another issue is how to match the changes in
output in ways that are most likely to reﬂect semantic changes in
input. We ﬁnd that standard longest-common subsequence match-
ing algorithms such as diff are not sufﬁcient for the task. Instead,
we adapt sequence alignment algorithms from computational biol-
ogy (speciﬁcally, Dialign [12, 13]) that better match our semantic
needs and are able to efﬁciently deal with large datasets.
To evaluate our Privacy Oracle system, we implemented a ver-
sion to check software that runs on Windows. We use virtual ma-
chines as repeatable testing enviroments, automate input with the
AutoIT tool [1], and observe network trafﬁc as output; we also ex-
plore the beneﬁts of using system hooks to observe output prior to
end-to-end encryption, e.g, inside SSL channels, when the applica-
tion does not implement its own encryption. We then chose to test
the top 20 applications from download.com as well as 6 of the most
popular IM, email and media clients. As a result, we were able to
ﬁnd many previously undisclosed leaks. These include personal in-
formation that was sent in the clear, to system information that was
harvested without user assistance, and information that was sent to
parties that users may not realize were involved. In several cases,
identifying information is regularly sent in the clear, which makes
it trivial for third parties to track users and/or violate their privacy.
We make two main contributions in this paper. The ﬁrst is our
Privacy Oracle system, which applies the black-box differential
fuzz testing technique to discovering leaks. This design is broadly
applicable because of the weak assumptions it places on applica-
tions and information leaks. If it can be made to work well, then
we expect it to be of general utility and particularly applicable when
information ﬂow and taint tracking are not an option. We consider
our results to show that the methodology has promise. The sec-
ond contribution is our study of leaks of personal information by
more than two dozen popular applications; we ﬁnd many instances
of three different types of information leaks. This, and larger scale
studies, are made possible by the Privacy Oracle system. We hope
that work such as ours will mean that applications are routinely
checked by many parties for privacy issues.
The rest of the paper is organized as follows: §2 deﬁnes our
goals and sketches a high-level idea of our approach. §3 describes
the design of the black box differential testing for discovering in-
formation exposure. §4 presents our system, Privacy Oracle, imple-
menting the black box differential testing. §5 discusses the ﬁndings
from 26 applications. §6 discusses the efﬁcacy of Privacy Oracle
along with its limitations. §7 discusses previous studies related to
our work. §8 summarizes the paper with future directions.
1“Differential black-box fuzz testing” should not be confused with
“differential testing” in which the same inputs are fed to multiple
versions of a program [3].
2. GOALS AND APPROACH
Our grand goal is to design and implement a fully automated
testing suite for ﬁnding personal information exposure from appli-
cation programs without any speciﬁc knowledge about the internal
state of the target application. The system that we present in this pa-
per is a ﬁrst step toward such a “Privacy Oracle”. In this section, we
deﬁne the goals of Privacy Oracle and present an overview of our
solution. It is within the scope of our paper to show which informa-
tion (e.g., email address, machine name) is exposed to which party
(e.g., application servers, advertisement servers) during which pro-
cess (e.g., install, sign in) in which form (e.g., clear text, encrypted,
encoded).
It is not the scope of the paper to show whether this
exposure conforms to the end user license agreement (EULA) or
whether the exposed information is actually stored and used by the
collecting end. We also note that Privacy Oracle is not designed to
detect malicious data leakage that is intentionally trying to avoid
detection. Rather, Privacy Oracle is designed to detect accidental
information exposure from standard development practices.
Figure 1: An example of testing a popular IM client for the exposure of a
user’s account name
Before we provide an example of Privacy Oracle, we ﬁrst deﬁne
a few terms used in the rest of the paper. A target application is the
application under test. Test parameters are the application speciﬁc
inputs like a username, password, or a search string, and system
speciﬁc information like IP addresses or machine names that the
target application might use. Test inputs are the speciﬁc instances
of the target application’s test parameters, like alice and bob for
usernames.
Figure 1 shows an example of an investigation procedure to de-
termine whether the test parameter, the username, is exposed to
third parties when a user signs in to a popular IM client. We hy-
pothesize that if the target application exposes a username (or any-
thing that is a variant of it), two different usernames should map to
two different byte sequences in the messages transmitted to remote
servers. That is, two different outputs generated from two different
test inputs (with everything else being equal) suggests that the dif-
fering segments in the output may correspond to the application test
parameter. This assumption breaks, however, when encryption or
compression is in use, or when output messages contain session IDs
or time-dependent cookies that change independent of the applica-
tion test parameters. The three primary steps of the investigation
procedure of Privacy Oracle in Figure 1 are as follows:
1. Given the test parameter (username), Privacy Oracle gener-
ates a set of test inputs (bob and alice). It then executes the
target application with the same test input (username = bob)
multiple times (T1 and T2), and collects network traces. By
comparing the set of network traces from the same test in-
put, Privacy Oracle identiﬁes byte segments that remain un-
bobalicebobtestparametersoutputnetwork tracessession ID, timeuser ID (hash of user name)targetapplicationT1T3T2123541235412354fuzz testingchanged for the given usage of the application (segments 1,
3, 4, and 5).
2. Privacy Oracle runs the target application with slightly mod-
iﬁed test inputs (username = alice in T3), and collects net-
work traces. It then compares network traces from the ﬁrst
experiment with those from the second experiment to deter-
mine byte segments that only change when the test parameter
changes (segment 4) and therefore highly suspected as car-
rying the test input back to a remote server.
3. Using the report generated by Privacy Oracle, an expert can
conclude that when a user signs in, the target application ex-
poses the username (bob), in the clear to application servers
4 times. Furthermore, a hash of the user account name (e.g.,
n=5a37g0qe6tjhh) is transmitted to two different advertise-
ment servers 37 times.
This seemingly straightforward black box testing approach, how-
ever, can fail if not carefully designed. The primary challenge is to
obtain consistent outputs when the same test inputs are provided.
Speciﬁcally, besides the test inputs, the following factors could also
affect the output network traces:
Application and operating system state. Application programs
track users’ inputs and change their behavior in an attempt to pro-
vide a more convenient service. For example, once an initial con-
tact with a server is made, the server can tell the application pro-
gram to store a piece of information about the user (e.g., cookies).
In such a case, the ﬁrst sign-in execution will include an additional
message to set up a cookie and the second sign-in execution may
transmit the cookie back to the server, resulting in two different
outputs.
Extraneous network activity. The host operating system of the
target application may independently generate packets during a test
(e.g., periodic NetBIOS broadcast messages, NTP updates, etc).
This extraneous trafﬁc can pollute output data sets, misleading
analysis results. However, it is important to include all the auxil-
iary trafﬁc triggered by the test program for ﬁnding comprehensive
leaks (e.g., mDNS [23] requests triggered by iTunes). For clarity,
we stress that extraneous trafﬁc differs from auxiliary trafﬁc in that
the former is generated by the OS and the latter by the application
itself.
Variable server responses and network delay. A server that com-
municates with a target application can unexpectedly change its be-
havior during test runs, which is beyond our control. Moreover,
variable network delay can cause message reordering between test
runs. If messages with a similar structure are reordered between
two tests, it may appear that the differences between the two test
outputs are due to content changes, causing false positives.
More importantly, even when we are given consistent output
traces, accurately isolating input-dependent byte regions from raw
packet data is difﬁcult. We defer explaining our solutions to later
sections because they are materialized throughout the design and
the implementation of Privacy Oracle. However, quick forward
pointers are §4 for the virtual machine based test harness and §3.2
for the ﬂow alignment tool.
3. BLACK BOX DIFFERENCE TESTING
FOR FINDING INFORMATION LEAKS
We describe our black-box testing approach for discovering user
information exposure. We enumerate the categories of test param-
eters that Privacy Oracle uses to generate the test inputs and then
present an overview of the output data analysis algorithm and how
we triage network packet data to fully leverage the algorithm.
3.1 Application Test Parameters
Test parameters can be drawn from any data that an application
program collects from users or from the system on which the pro-
gram is running. Choosing a “right” set of parameters for testing is
difﬁcult because it is not clear which parameters are more privacy
sensitive than the others and each application poses a different pri-
vacy risk even when the same piece of information is exposed2. In
what follows, we deﬁne the three broad categories of test parame-
ters used by Privacy Oracle to detect information exposure.
Personal data. During installation and initial use, an application
may prompt for user preferences (e.g., whether to create a desktop
shortcut) or for personal data such as name, email address (e.g.,
when prompted to subscribe to a newsletter), organization, gender,
and zip code.
Application usage data. Web sites use cookies to track users’ Web
navigation. Interestingly, an increasing number of (non Web brows-
ing) applications augment this tracking by reporting users’ activity
to Web sites. For example, we observe that upon a user’s search for
media ﬁles, a popular media player crafts an HTTP GET message
with the user’s query string and sends it to a well-known advertise-
ment placing company. As such, while an application itself does
not collect any personal information, it can indirectly disclose a
user’s activity and identifying information to third parties via cook-
ies.
System conﬁguration information. While system conﬁguration
information might not be thought of as being “personal” in nature,
it may link strongly to user identity. Concern about this threat
is reﬂected, for example, by the decision of the European Union
Data Protection Working Party to designate IP addresses as per-