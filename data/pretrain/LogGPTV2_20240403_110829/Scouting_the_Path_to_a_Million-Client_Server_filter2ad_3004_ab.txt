performance of the stack.
342
Y. Zhao et al.
Our baseline performance, denoted in the Fig. 2 by fq, is for the case when
ﬂows are not rate limited and scheduled following a fair queuing policy, requiring
packets to be queued for some ﬂows so that other ﬂows can achieve their fair
share. To quantify that delay, we compare the performance of the baseline to a
scenario in which each ﬂow is rate limited such that the aggregate rate that is
90% of NIC capacity, denoted in Fig. 2 by per flow rate limit. Under this
scenario, no queuing should happen in the Qdisc as demand is always smaller
than the network capacity. Latency drops by an order of magnitude in that
scenario at 300k ﬂows and by more at smaller numbers of ﬂows, leading to the
conclusion that hundreds of milliseconds of delay are added because of queuing
delays at the Qdisc. We further validate this conclusion by employing a Qdisc
that implements the CoDel AQM algorithm, conﬁgured with a target latency
of 100 µs. CoDel drops packets if their queueing delay exceeds the target delay.
At 300k ﬂows, the delay of codel is lower than the baseline by an order of
magnitude, validating our conclusion. Note that CoDel comes at a price of higher
CPU utilization due to packet drop and retransmission (Fig. 2d). For the rest
of the paper, we attempt to better understand the causes of the observed large
delays and high CPU utilization at large numbers of ﬂows.
4 Admission Control to the Stack
Network stacks are typically optimized to maximize the number of packets per
second they can handle, allowing applications unrestricted access to the stack in
many cases, especially in Linux. However, as the number of ﬂows increases, appli-
cations can overwhelm the stack by generating packets at a larger rate than the
network stack can process and transmit them. This congestion, left unchecked,
can lead to hundreds of milliseconds of added delay. Admission control of pack-
ets to the stack can avoid this problem by regulating the access of applications
to stack resources. Linux already has several such mechanisms, which work well
with a relatively small number of ﬂows (e.g., tens of thousands of ﬂows), but fail
at large numbers of ﬂows (e.g., hundreds of thousands). We examine admission
control mechanisms based on the knob they control. In particular, admission con-
trol mechanisms decide three values: 1) the size of each individual packet (the
larger the packets the smaller the packet rate for the same byte rate), 2) the
total number of admitted packets (i.e., limiting the number of packets through
backpressure), and 3) the size of a new batch of admitted packets.
4.1 Packet Sizing
The Linux stack implements packet autosizing, an operation that helps improve the
pacing function for low throughput ﬂows. Pacing is an integral function for several
modern congestion control algorithms including BBR [16,21]. In particular, pac-
ing spreads out packets over time to avoid sending them in bursts. The autosizing
algorithm is triggered if a ﬂow is sending at a rate lower than 512 Mbps (i.e., a thou-
sand Maximum Segment Sized (MSS) segments every second, assuming an MSS
of 64KB). When triggered, it reduces the size of the segments transmitted every
1ms, where inter-packet gap is enforced through a pacer (e.g., fq [21]) and packet
Scouting the Path to a Million-Client Server
343
(a) Packet Rate
(b) CPU Usage
Fig. 3. CUBIC v.s. BBR with 5% drop rate. The relationship between number of ﬂows
and packet rate is similar at 0% drop but there is no diﬀerence between BBR and
CUBIC at 0% drop rate (Appendix E).
segmentation to MTU size is done in hardware. Automatic packet sizing can also
be beneﬁcial for ensuring fairness between ﬂows [42].
Autosizing infers the rate of a ﬂow by dividing the number of bytes sent
during an RTT (i.e., the cwnd) over the measured RTT. This allows for main-
taining the same average sending rate while spreading packet transmission over
time. The technique provides a tradeoﬀ between CPU utilization and network
performance by increasing the number of packets per second handled by the
server while lowering the size of bursts the network deals with. The CPU cost of
autosizing is aﬀected by the number of ﬂows handled by the server. In particular,
the same aggregate rate of 512 Mbps can result in a packet rate of 1k packets
per second for one ﬂow or 1M packets per second for 1k ﬂows in the worst case.2
This overpacing can overwhelm the stack, leading to an increase in delay
(Fig. 2c). This leads the autosizing algorithm to misbehave. In particular, the
RTT increases when the stack is overloaded, leading to underestimation of the
rates of all ﬂows handled by the stack. This causes the autosizing mechanism
to reduce the size of bursts unnecessarily, creating more packets, increasing the
congestion at the server [46]. Another side eﬀect of autosizing is causing dif-
ferent congestion control algorithms to have diﬀerent CPU costs. In particular,
algorithms that react more severely to congestion (e.g., CUBIC which halves its
window on a packet drop) send at lower rates, forcing autosizing to create more
packets. However, algorithms that react mildly to congestion (e.g., BBR), main-
tain high rates and send lower number of packets. Figure 3 shows the diﬀerence
between CUBIC and BBR at 5% drop rate induced by a netem Qdisc at the
receiver. We set MTU size to 7000 to eliminate the CPU bottleneck.
Reducing delay introduced in the stack can help autosizing infer the rates
of ﬂows more accurately. However, as we will show later, scheduling ﬂows,
including delaying packets, is essential to scaling the end host. This means that
2 The number of packets is typically much smaller than the worst case scenario due
to imperfect pacing. Delays in dispatching packets, resulting from imperfect pacing,
require sending larger packets to maintain the correct average rate, leading to a
lower packet rate. However, the CPU cost of autosizing increases with the number
of ﬂows even with imperfect pacing.
344
Y. Zhao et al.
autosizing-like algorithms need to diﬀerentiate between network congestion and
end-host congestion. This will be useful in avoiding generating extra packets
which might congest the end host but not the network.
4.2 Backpressure
As
the number of ﬂows
When a ﬂow has a packet to send, its thread attempts to enqueue the packet to
the packet scheduler (i.e., the Qdisc in the kernel stack). In order to avoid Head-
of-Line (HoL) blocking, ﬂows are prevented from sending packets continuously
by TCP Small Queue (TSQ). In particular, TSQ limits the number of packets
enqueued to the Qdisc to only two packets per ﬂow [20]. TSQ oﬀers a rudimentary
form of admission control that is based on a per-ﬂow threshold to control the
total number of packets in the stack.
increases, TSQ
becomes ineﬀective because the number of pack-
ets admitted to the stack grows with the number
of ﬂows. Consequently, the length of the queue in
the Qdisc will grow as the number of ﬂows grows,
leading to long delays due to buﬀerbloat. If we
limit the queue length of the Qdisc, packets will be
dropped at the Qdisc after they are admitted by
TSQ. The current approach in Linux is to immedi-
ately retry to enqueue the dropped packets, lead-
ing to poor CPU utilization as threads keep retry-
ing to enqueue packets. Figure 4 shows the CPU usage for transmitting packets
from the TCP layer to the qdisc with diﬀerent values of maximum queue length
at the qdisc. The CPU usage includes only the operation before enqueuing the
packet onto the qdisc. The shorter the queue length, the higher the drop rate,
leading to higher CPU utilization.
Fig. 4. CPU usage as a func-
tion of Qdisc queue length
Another down side of the lack of backpres-
sure is that packet scheduling becomes reliant
on thread scheduling. In particular, when a
packet is dropped, it is the responsibility of
its thread to try to enqueue it again imme-
diately. The frequency at which a thread can
“requeue” packets depends on the frequency
at which it is scheduled. This is problematic
because the thread scheduler has no notion of
per-ﬂow fairness, leading to severe unfairness
between ﬂows. As explained in the previous
section, starvation at the Qdisc leads to hun-
dreds of milliseconds of delay on average. We
further investigate the eﬀects of this unfair-
ness on per-ﬂow throughput. Figure 5 com-
pares the CDF of rates achieved when fq is
used with a small number of 300 and 30k
ﬂows. The two scenarios are contrasted with
(a) 300 ﬂows
(b) 30k ﬂows
Fig. 5. CDF of ﬂow rate
Scouting the Path to a Million-Client Server
345
the per-ﬂow pacing scenario which achieves best possible fairness by rate lim-
iting all ﬂows to the same rate, with aggregate rate below NIC capacity, thus
avoiding creating a bottleneck at the scheduler. In the 30k ﬂows scenario, the
largest rate is two orders of magnitude greater than the smallest rate. This is
caused by the batching on the NIC queue. The net tx action function calls into
the Qdisc layer and starts to dequeue skb through the dequeue skb function.
Multiple packets can be returned by some queues, and a list of skb may be sent
to NIC, blocking packets from other queues. We observe that there are many
more requeue operations in Qdisc when pacing is not used than when pacing
is used, indicating that pacing prevents the NIC from being overwhelmed by a
subset of queues.
Some previous works address the problem partially by enforcing per-ﬂow
scheduling instead of per-packet scheduling and only allowing a ﬂow to enqueue
a packet when there is room for it in the scheduler, avoiding unnecessary drops
and retries [31,46], however, these works do not consider the interaction between
layers that may lead to unfairness when fairness is enforced separately on each
layer as we show in this section.
4.3 Batching Ingress Packets
The two previous sections discuss con-
trolling the packet rate on the egress
path. In this section, we consider con-
trolling the packet rate on the ingress
path. It should be noted that although
we focus on egress path on server side,
ingress path eﬃciency may also aﬀect
the egress path eﬃciency because
delayed ACK caused by CPU satura-
tion can lead to performance degrada-
tion in traﬃc transmission.
A receiver has little control on the
Fig. 6. Rates of RX Interrupts and ACKs
per second
number of incoming packets, aside from ﬂow control. By coalescing packets
belonging to the same ﬂow on the ingress path using techniques like LRO, the
receiver can improve the CPU eﬃciency of the receive path by generating less
interrupts. Batching algorithms deliver packets to the software stack once the
number of outstanding packets in the NIC reach a certain maximum batch size
or some timer expires. As the number of ﬂows increases, the chances of such coa-
lescing decrease as the likelihood of two incoming packets belong to the same ﬂow
decreases (Fig. 6). In the Linux setting, this is especially bad as increasing the
number of incoming packets results in an increase in the number of interrupts,
leading to severe degradation in CPU eﬃciency.
Better batching techniques that prioritize short ﬂows, and give LRO more
time with long ﬂows, can signiﬁcantly help improve the performance of the
ingress path. Some coarse grain adaptive batching techniques have been pro-
posed [30,43]. However, we believe that better performance can be achieved
with ﬁne-grain per-ﬂow adaptive batching, requiring coordination between
the hardware and software components of the stack.
346
Y. Zhao et al.
5 Per-Packet Overhead
To identify the operations whose overhead increases as the number of ﬂows
increases, we use perf [11] and observe the CPU utilization and latency of dif-
ferent kernel functions as we change the number of ﬂows. The CPU utilization
results show the aggregated CPU usage by all ﬂows. We keep the aggregate
data rate the same and only change the number of ﬂows. Our goal is to ﬁnd
the operations whose computational complexity is a function of the number of
ﬂows. Operations that are bottlenecked on a diﬀerent type of resource will have
higher latency as we increase the number of ﬂows. Figures 7a and 7b show the
top four functions in each category. There is an overlap between functions with
high latency and functions with high CPU utilization; this is typical because high
CPU utilization can lead to high latency (e.g., fq dequeue and inet lookup).
However, there are functions with high latency but low CPU utilization (e.g.,
tcp ack and dev queue xmit). Through further proﬁling of the code of these
functions, we ﬁnd that there are two types of bottlenecks that arise: cache pres-
sure and lock contention. Note that the overhead of the tg3 poll work function
is part of ineﬃciency of the Linux reception path [14] and is not the focus of our
work.
(a) CPU Usage
(b) Function Latency
(c) Cache Misses
Fig. 7. Function proﬁling
Data Structures: There are two operations whose complexity is a function of
the number of ﬂows: packet scheduling and packet demultiplexing. The over-
head of packet scheduling is captured by the CPU utilization of fq enqueue
and fq dequeue. The two functions handle adding and removing packets to the
fq Qdisc, which sorts ﬂows in a red-black tree based on the soonest transmis-
sion time of their packets. The overhead of enqueue and dequeue operations in
O(log(n)), where n is the number of ﬂows. The overhead of packet demultiplex-
ing is captured by the CPU utilization of inet lookup which matches incoming
packets to their ﬂows using a hashmap. In the case of collision, ﬁnding a match
requires processing information of ﬂows whose hash collide. This increases the
cache miss ratio of the function (Fig. 7c), further increasing the latency of the
function.
Scouting the Path to a Million-Client Server
347
Aggregate
cache
Fig. 8.
misses
Some approximation scheduling algorithms
have been proposed to reduce the data struc-
ture overhead [18,38,39], but their main focus is
to improve FQ. Data structure overhead requires
reexamining all complex data structures used in
the stack, taking into account that the stack can
process millions of packets per second coming
from millions of ﬂows.
Cache Pressure: One of the functions with the
highest cache miss ratio is tcp ack, which clears
the TCP window based on received acknowledge-
ments. The function does not use any complex data structures or wait on locks
so the high cache miss stems from the overhead of fetching ﬂow information
and modifying it. As shown in Fig. 8, the cache miss ratio in both L2 cache and
Last Level Cache (LLC) increases as the number of ﬂows increases. While cache
misses are not a huge bottleneck in our setting, we believe that as the number of
ﬂows increases, with tighter requirements on latency, cache miss ratio will have
to be minimized.
Lock Contention: Another source of increased
latency is
lock contention when accessing
shared resources. Our experiment conﬁrms that
the biggest critical section in the networking
stack is the one used to protect access to the
qdisc, done in dev queue xmit. The overhead
of acquiring the qdisc lock is well documented
[35,38], and increasing the number of ﬂows
exacerbates the problem, even with constant
packet rate. Figure 9 shows that as the time to
acquire lock increases by 4 times as the num-
ber of ﬂow increases from 1k to 300k. Another
factor contributing to the increase in lock acquisition time is the increase in
packet rate which we have shown to increase as the number of ﬂows increases
(Fig. 3a). Distributed and lazy coordination between independent queues can
help alleviate the problem by reducing the need for locking [24,38].
Fig. 9. Time to acquire qdisc
lock