tical problems. On-call engineers could actually accomplish work when they weren’t
being kept up by pages at all hours. Ultimately, temporarily backing off on our alerts
allowed us to make faster progress toward a better service.
Gmail: Predictable, Scriptable Responses from Humans
In the very early days of Gmail, the service was built on a retrofitted distributed pro‐
cess management system called Workqueue, which was originally created for batch
processing of pieces of the search index. Workqueue was “adapted” to long-lived pro‐
cesses and subsequently applied to Gmail, but certain bugs in the relatively opaque
codebase in the scheduler proved hard to beat.
At that time, the Gmail monitoring was structured such that alerts fired when indi‐
vidual tasks were “de-scheduled” by Workqueue. This setup was less than ideal
because even at that time, Gmail had many, many thousands of tasks, each task repre‐
senting a fraction of a percent of our users. We cared deeply about providing a good
user experience for Gmail users, but such an alerting setup was unmaintainable.
To address this problem, Gmail SRE built a tool that helped “poke” the scheduler in
just the right way to minimize impact to users. The team had several discussions
about whether or not we should simply automate the entire loop from detecting the
problem to nudging the rescheduler, until a better long-term solution was achieved,
but some worried this kind of workaround would delay a real fix.
Monitoring for the Long Term | 65
This kind of tension is common within a team, and often reflects an underlying mis‐
trust of the team’s self-discipline: while some team members want to implement a
“hack” to allow time for a proper fix, others worry that a hack will be forgotten or
that the proper fix will be deprioritized indefinitely. This concern is credible, as it’s
easy to build layers of unmaintainable technical debt by patching over problems
instead of making real fixes. Managers and technical leaders play a key role in imple‐
menting true, long-term fixes by supporting and prioritizing potentially time-
consuming long-term fixes even when the initial “pain” of paging subsides.
Pages with rote, algorithmic responses should be a red flag. Unwillingness on the part
of your team to automate such pages implies that the team lacks confidence that they
can clean up their technical debt. This is a major problem worth escalating.
The Long Run
A common theme connects the previous examples of Bigtable and Gmail: a tension
between short-term and long-term availability. Often, sheer force of effort can help a
rickety system achieve high availability, but this path is usually short-lived and
fraught with burnout and dependence on a small number of heroic team members.
Taking a controlled, short-term decrease in availability is often a painful, but strategic
trade for the long-run stability of the system. It’s important not to think of every page
as an event in isolation, but to consider whether the overall level of paging leads
toward a healthy, appropriately available system with a healthy, viable team and long-
term outlook. We review statistics about page frequency (usually expressed as inci‐
dents per shift, where an incident might be composed of a few related pages) in
quarterly reports with management, ensuring that decision makers are kept up to
date on the pager load and overall health of their teams.
Conclusion
A healthy monitoring and alerting pipeline is simple and easy to reason about. It
focuses primarily on symptoms for paging, reserving cause-oriented heuristics to
serve as aids to debugging problems. Monitoring symptoms is easier the further “up”
your stack you monitor, though monitoring saturation and performance of subsys‐
tems such as databases often must be performed directly on the subsystem itself.
Email alerts are of very limited value and tend to easily become overrun with noise;
instead, you should favor a dashboard that monitors all ongoing subcritical problems
for the sort of information that typically ends up in email alerts. A dashboard might
also be paired with a log, in order to analyze historical correlations.
Over the long haul, achieving a successful on-call rotation and product includes
choosing to alert on symptoms or imminent real problems, adapting your targets to
goals that are actually achievable, and making sure that your monitoring supports
rapid diagnosis.
66 | Chapter 6: Monitoring Distributed Systems
CHAPTER 7
The Evolution of Automation at Google
Written by Niall Murphy with John Looney and Michael Kacirek
Edited by Betsy Beyer
Besides black art, there is only automation and mechanization.
—Federico García Lorca (1898–1936), Spanish poet and playwright
For SRE, automation is a force multiplier, not a panacea. Of course, just multiplying
force does not naturally change the accuracy of where that force is applied: doing
automation thoughtlessly can create as many problems as it solves. Therefore, while
we believe that software-based automation is superior to manual operation in most
circumstances, better than either option is a higher-level system design requiring nei‐
ther of them—an autonomous system. Or to put it another way, the value of automa‐
tion comes from both what it does and its judicious application. We’ll discuss both
the value of automation and how our attitude has evolved over time.
The Value of Automation
What exactly is the value of automation?1
Consistency
Although scale is an obvious motivation for automation, there are many other rea‐
sons to use it. Take the example of university computing systems, where many sys‐
tems engineering folks started their careers. Systems administrators of that
background were generally charged with running a collection of machines or some
1 For readers who already feel they precisely understand the value of automation, skip ahead to “The Value for
Google SRE” on page 70. However, note that our description contains some nuances that might be useful to
keep in mind while reading the rest of the chapter.
67
software, and were accustomed to manually performing various actions in the dis‐
charge of that duty. One common example is creating user accounts; others include
purely operational duties like making sure backups happen, managing server failover,
and small data manipulations like changing the upstream DNS servers’ resolv.conf,
DNS server zone data, and similar activities. Ultimately, however, this prevalence of
manual tasks is unsatisfactory for both the organizations and indeed the people
maintaining systems in this way. For a start, any action performed by a human or
humans hundreds of times won’t be performed the same way each time: even with the
best will in the world, very few of us will ever be as consistent as a machine. This
inevitable lack of consistency leads to mistakes, oversights, issues with data quality,
and, yes, reliability problems. In this domain—the execution of well-scoped, known
procedures—the value of consistency is in many ways the primary value of automa‐
tion.
A Platform
Automation doesn’t just provide consistency. Designed and done properly, automatic
systems also provide a platform that can be extended, applied to more systems, or
perhaps even spun out for profit.2 (The alternative, no automation, is neither cost
effective nor extensible: it is instead a tax levied on the operation of a system.)
A platform also centralizes mistakes. In other words, a bug fixed in the code will be
fixed there once and forever, unlike a sufficiently large set of humans performing the
same procedure, as discussed previously. A platform can be extended to perform
additional tasks more easily than humans can be instructed to perform them (or
sometimes even realize that they have to be done). Depending on the nature of the
task, it can run either continuously or much more frequently than humans could
appropriately accomplish the task, or at times that are inconvenient for humans. Fur‐
thermore, a platform can export metrics about its performance, or otherwise allow
you to discover details about your process you didn’t know previously, because these
details are more easily measurable within the context of a platform.
Faster Repairs
There’s an additional benefit for systems where automation is used to resolve com‐
mon faults in a system (a frequent situation for SRE-created automation). If automa‐
tion runs regularly and successfully enough, the result is a reduced mean time to
repair (MTTR) for those common faults. You can then spend your time on other
tasks instead, thereby achieving increased developer velocity because you don’t have
to spend time either preventing a problem or (more commonly) cleaning up after it.
2 The expertise acquired in building such automation is also valuable in itself; engineers both deeply under‐
stand the existing processes they have automated and can later automate novel processes more quickly.
68 | Chapter 7: The Evolution of Automation at Google
As is well understood in the industry, the later in the product lifecycle a problem is
discovered, the more expensive it is to fix; see Chapter 17. Generally, problems that
occur in actual production are most expensive to fix, both in terms of time and
money, which means that an automated system looking for problems as soon as they
arise has a good chance of lowering the total cost of the system, given that the system
is sufficiently large.
Faster Action
In the infrastructural situations where SRE automation tends to be deployed, humans
don’t usually react as fast as machines. In most common cases, where, for example,
failover or traffic switching can be well defined for a particular application, it makes
no sense to effectively require a human to intermittently press a button called “Allow
system to continue to run.” (Yes, it is true that sometimes automatic procedures can
end up making a bad situation worse, but that is why such procedures should be
scoped over well-defined domains.) Google has a large amount of automation; in
many cases, the services we support could not long survive without this automation
because they crossed the threshold of manageable manual operation long ago.
Time Saving
Finally, time saving is an oft-quoted rationale for automation. Although people cite
this rationale for automation more than the others, in many ways the benefit is often
less immediately calculable. Engineers often waver over whether a particular piece of
automation or code is worth writing, in terms of effort saved in not requiring a task
to be performed manually versus the effort required to write it.3 It’s easy to overlook
the fact that once you have encapsulated some task in automation, anyone can exe‐
cute the task. Therefore, the time savings apply across anyone who would plausibly
use the automation. Decoupling operator from operation is very powerful.
Joseph Bironas, an SRE who led Google’s datacenter turnup efforts
for a time, forcefully argued:
“If we are engineering processes and solutions that are not auto‐
matable, we continue having to staff humans to maintain the sys‐
tem. If we have to staff humans to do the work, we are feeding the
machines with the blood, sweat, and tears of human beings. Think
The Matrix with less special effects and more pissed off System
Administrators.”
3 See the following XKCD cartoon: http://xkcd.com/1205/.
The Value of Automation | 69
The Value for Google SRE
All of these benefits and trade-offs apply to us just as much as anyone else, and Goo‐
gle does have a strong bias toward automation. Part of our preference for automation
springs from our particular business challenges: the products and services we look
after are planet-spanning in scale, and we don’t typically have time to engage in the
same kind of machine or service hand-holding common in other organizations.4 For
truly large services, the factors of consistency, quickness, and reliability dominate
most conversations about the trade-offs of performing automation.
Another argument in favor of automation, particularly in the case of Google, is our
complicated yet surprisingly uniform production environment, described in Chap‐
ter 2. While other organizations might have an important piece of equipment without
a readily accessible API, software for which no source code is available, or another
impediment to complete control over production operations, Google generally avoids
such scenarios. We have built APIs for systems when no API was available from the
vendor. Even though purchasing software for a particular task would have been much
cheaper in the short term, we chose to write our own solutions, because doing so pro‐
duced APIs with the potential for much greater long-term benefits. We spent a lot of
time overcoming obstacles to automatic system management, and then resolutely
developed that automatic system management itself. Given how Google manages its
source code [Pot16], the availability of that code for more or less any system that SRE
touches also means that our mission to “own the product in production” is much eas‐
ier because we control the entirety of the stack.
Of course, although Google is ideologically bent upon using machines to manage
machines where possible, reality requires some modification of our approach. It isn’t
appropriate to automate every component of every system, and not everyone has the
ability or inclination to develop automation at a particular time. Some essential sys‐
tems started out as quick prototypes, not designed to last or to interface with automa‐
tion. The previous paragraphs state a maximalist view of our position, but one that
we have been broadly successful at putting into action within the Google context. In
general, we have chosen to create platforms where we could, or to position ourselves
so that we could create platforms over time. We view this platform-based approach as
necessary for manageability and scalability.
The Use Cases for Automation
In the industry, automation is the term generally used for writing code to solve a wide
variety of problems, although the motivations for writing this code, and the solutions
4 See, for example, http://blog.engineyard.com/2014/pets-vs-cattle.
70 | Chapter 7: The Evolution of Automation at Google
themselves, are often quite different. More broadly, in this view, automation is “meta-
software”—software to act on software.
As we implied earlier, there are a number of use cases for automation. Here is a non-
exhaustive list of examples:
• User account creation
• Cluster turnup and turndown for services
• Software or hardware installation preparation and decommissioning
• Rollouts of new software versions
• Runtime configuration changes
• A special case of runtime config changes: changes to your dependencies
This list could continue essentially ad infinitum.
Google SRE’s Use Cases for Automation
In Google, we have all of the use cases just listed, and more.
However, within Google SRE, our primary affinity has typically been for running
infrastructure, as opposed to managing the quality of the data that passes over that
infrastructure. This line isn’t totally clear—for example, we care deeply if half of a
dataset vanishes after a push, and therefore we alert on coarse-grain differences like
this, but it’s rare for us to write the equivalent of changing the properties of some
arbitrary subset of accounts on a system. Therefore, the context for our automation is
often automation to manage the lifecycle of systems, not their data: for example,
deployments of a service in a new cluster.
To this extent, SRE’s automation efforts are not far off what many other people and
organizations do, except that we use different tools to manage it and have a different
focus (as we’ll discuss).
Widely available tools like Puppet, Chef, cfengine, and even Perl, which all provide
ways to automate particular tasks, differ mostly in terms of the level of abstraction of
the components provided to help the act of automating. A full language like Perl pro‐
vides POSIX-level affordances, which in theory provide an essentially unlimited
scope of automation across the APIs accessible to the system,5 whereas Chef and Pup‐
pet provide out-of-the-box abstractions with which services or other higher-level
entities can be manipulated. The trade-off here is classic: higher-level abstractions are
easier to manage and reason about, but when you encounter a “leaky abstraction,”
5 Of course, not every system that needs to be managed actually provides callable APIs for management—forc‐
ing some tooling to use, e.g., CLI invocations or automated website clicks.
The Use Cases for Automation | 71
you fail systemically, repeatedly, and potentially inconsistently. For example, we often
assume that pushing a new binary to a cluster is atomic; the cluster will either end up
with the old version, or the new version. However, real-world behavior is more com‐
plicated: that cluster’s network can fail halfway through; machines can fail; communi‐
cation to the cluster management layer can fail, leaving the system in an inconsistent
state; depending on the situation, new binaries could be staged but not pushed, or
pushed but not restarted, or restarted but not verifiable. Very few abstractions model
these kinds of outcomes successfully, and most generally end up halting themselves
and calling for intervention. Truly bad automation systems don’t even do that.
SRE has a number of philosophies and products in the domain of automation, some
of which look more like generic rollout tools without particularly detailed modeling
of higher-level entities, and some of which look more like languages for describing
service deployment (and so on) at a very abstract level. Work done in the latter tends
to be more reusable and be more of a common platform than the former, but the
complexity of our production environment sometimes means that the former
approach is the most immediately tractable option.
A Hierarchy of Automation Classes
Although all of these automation steps are valuable, and indeed an automation plat‐
form is valuable in and of itself, in an ideal world, we wouldn’t need externalized
automation. In fact, instead of having a system that has to have external glue logic, it
would be even better to have a system that needs no glue logic at all, not just because
internalization is more efficient (although such efficiency is useful), but because it has
been designed to not need glue logic in the first place. Accomplishing that involves
taking the use cases for glue logic—generally “first order” manipulations of a system,
such as adding accounts or performing system turnup—and finding a way to handle
those use cases directly within the application.
As a more detailed example, most turnup automation at Google is problematic
because it ends up being maintained separately from the core system and therefore
suffers from “bit rot,” i.e., not changing when the underlying systems change. Despite
the best of intentions, attempting to more tightly couple the two (turnup automation
and the core system) often fails due to unaligned priorities, as product developers
will, not unreasonably, resist a test deployment requirement for every change. Sec‐
ondly, automation that is crucial but only executed at infrequent intervals and there‐
fore difficult to test is often particularly fragile because of the extended feedback
cycle. Cluster failover is one classic example of infrequently executed automation:
failovers might only occur every few months, or infrequently enough that inconsis‐
tencies between instances are introduced. The evolution of automation follows a
path:
72 | Chapter 7: The Evolution of Automation at Google
1) No automation
Database master is failed over manually between locations.
2) Externally maintained system-specific automation
An SRE has a failover script in his or her home directory.
3) Externally maintained generic automation