trast to the FGP approach, scalability with high efﬁciency
is observable with up to 8 threads. Further speed-ups
when using the CPU located on the second socket are no-
ticeable in the MExp and MVMul experiments, achiev-
ing a throughput of more than 100M non-linear gates per
second.
In summary, for all presented applications the CGP ap-
proach signiﬁcantly outperforms the FGP approach re-
garding scalability and efﬁciency due to its coarser gran-
ularity, which implies a better thread utilization.
542  24th USENIX Security Symposium 
USENIX Association
12
ideal parallelization
MExp - CGP
MVMul - CGP
BioMatch - CGP
MVMul - FGP
BioMatch - FGP
MExp - FGP
 16
 14
 12
 10
 8
 6
 4
 2
p
u
-
d
e
e
p
S
 16
 14
 12
 10
 8
 6
 4
 2
 0
 0
 2
 4
 6
 8
 10
 12
 14
 0
 16
Threads
Figure 6: Circuit garbling. The speed-up of circuit garbling
for all three applications when using the FGP, CGP and differ-
ent numbers of computing threads. CGP signiﬁcantly outper-
forms FGP for all applications.
Circuit width analysis. The limited scalability of FGP
is explainable when investigating the different circuit
properties. In Figure 7 the distribution of level widths
for all circuits is illustrated.
10^6
10^5
10^4
10^3
10^2
10^1
10^0
BioMatch
MExp
MVMul
Figure 7: Number of non-linear gates per level and circuit.
For the MVMul application, the CBMC-GC compiler
produces a circuit with a median level width of 2352
non-linear gates per level, whereas the BioMatch and
MExp circuits only show a median width below 100
non-linear gates per level. The major reasons for small
circuit widths in comparison to the overall circuit size
is that high-level TPC compilers such as CBMC-GC or
the compiler by Kreuter et al. [23] have been developed
with a focus on minimizing the number of non-linear
gates. Minimizing the circuit depth or maximizing the
median circuit width barely inﬂuence the sequential run-
time of Yao’s protocol and is therefore not addressed in
the ﬁrst place. Looking at the building blocks that are
used in CBMC-GC, we observe that arithmetic blocks
(e.g. adder, multiplier) show a linear increase in the av-
erage circuit width when increasing the input size. How-
ever, multiplexers, as used for dynamic array accesses
and for ‘if’ statements, show a circuit width that is in-
dependent (constant) of the number of choices. Thus,
a 2-1 multiplexer and a n-1 multiplexer are compiled to
circuits with similar sized levels, yet with different cir-
cuit depths. Moreover, comparisons have a constant cir-
cuit width for any input bit size. Based on these insights
we deduce, that the MVMul circuit shows a signiﬁcantly
larger median circuit width, because of the absence of
any dynamic array access, conditionals or comparisons.
This is not the case with the BioMatch and MExp appli-
cations. Considering that every insufﬁcient saturation of
threads leads to an efﬁciency loss of parallelization, we
conclude that scalability of FGP is not guaranteed when
increasing input sizes.
6.4 Full protocol (online)
To motivate that the parallelization of circuit garbling can
be used in Yao’s full protocol, we evaluated the protocol
for all applications running on two separated cloud in-
stances in the same Amazon region (LAN setting). We
observed an average round trip time of 0.6±0.3ms and a
transfer bandwidth of 5.0±0.4 Gbps using iperf. Fol-
lowing the results of the ofﬂine experiments, we bench-
mark the more promising CGP approach in the online
setting.
To measure the beneﬁts of parallelization, we ﬁrst
benchmark the single core performance of Yao’s protocol
in the described network environment. Table 3 shows the
sequential runtime for all applications using two security
levels κ = 80 bit (short term) and κ = 128 bit (long term).
This runtime includes the time spent on the input as well
as the output phase. Furthermore, the observed through-
put, measured in non-linear gates per second, as well as
the required bandwidth are presented. We observe that
for security levels of κ = 80 and κ = 128 a similar gate
throughput is achieved. Consequently, we deduce that
in this setup the available bandwidth is not stalling the
computation. We also observe that that the time spent on
OTs in all applications is practically negligible (< 5%)
in comparison with the time spent on circuit garbling.
In Figure 8 the performance gain of CGP is presented.
The speed-up is measured in relation to the sequential
total runtime. The timing results show that CGP scales
almost linearly with up to 4 threads when using κ = 80
bit labels. Using κ = 128 bit labels, no further speed-
up beyond 3 threads is noticeable. Thus, the impact of
the network limits is immediately visible. Five (κ = 80
bit), respectively three (κ = 128 bit) threads are sufﬁcient
to saturate the available bandwidth in this experiment.
Achieving further speed-ups is impossible without in-
creasing the available bandwidth or developing new TPC
techniques. However, to the best of our knowledge with
6M non-linear gates per second on a single core, as well
as with approximately 32M non-linear gates per second
on a single socket, we report the fastest garbling speed in
USENIX Association  
24th USENIX Security Symposium  543
13
Circuits
ttotal
[s]
gps
[M]
bw
[Gbps]
tinput
[s]
κ = 128
κ = 80
κ = 128
κ = 80
κ = 128
κ = 80
κ = 128
κ = 80
BioMatch
2.71±0.02
2.56±0.03
6.23±0.04
6.56±0.07
1.48±0.01
0.97±0.01
<0.02s
<0.02s
MExp
1.43±0.01
1.42±0.01
6.17±0.05
6.21±0.04
1.47±0.01
0.92±0.01
<0.01s
<0.01s
MVMul
0.20±0.00
0.19±0.00
6.22±0.00
6.43±0.00
1.48±0.00
0.95±0.00
< 0.01s
< 0.01s
Table 3: Yao’s protocol, single-core performance. The run-
time (ttotal), non-linear gate throughput in million gates per
second (gps), required bandwidth (bw) and time spent in the
input phase (tinput), including the OTs when executing Yao’s
protocol for all applications in a LAN setting.
 8
 7
 6
 5
 4
 3
 2
 1
p
u
-
d
e
e
p
S
 0
 0
ideal parallelization
MExp k=80
BioMatch k=80
MVMul k=80
MVMul k=128
BioMatch k=128
MExp k=128
 1
 2
 3
 4
 5
 6
 7
Threads
 8
 7
 6
 5
 4
 3
 2
 1
 0
 8
Figure 8: Yao’s protocol - CGP The speed-up of all three ap-
plications in the LAN setting with κ = 128 bit and κ = 80 bit
security.
an online setting of Yao’s protocol. We abstain from an
evaluation in a WAN setting due to the high bandwidth
that is required to show the scalability of parallelization.
The best bandwidth that we could observe during our
experiments between two cloud regions was 350 Mbps,
which is insufﬁcient to benchmark parallel scalability.
Inter-party parallelization
6.5
A new application of parallelization in Yao’s protocol is
presented in § 5. We performed two experiments to show
the applicability of IPP in practical settings. The ﬁrst
experiment measures the computational efﬁciency gain
in the same setting as described in § 6.4. In the second
experiment the beneﬁts of IPP in a WAN setting with
limited bandwidth are presented.
Computational efﬁciency gain. In this experiment the
raw IPP performance for all example applications, as
well as the combination of CGP and IPP techniques is
explored. To realize IPP, our implementation uses multi-
ple threads per core to utilize the load balancing capabili-
ties of the underlying OS without implementing a sophis-
ticated load balancer. Due to the heterogeneous hard-
ware environment, e.g. unpredictable caching and net-
working behaviour, we evaluated three different work-
load distribution strategies. The ﬁrst strategy uses one
thread per core and thus only functions with at least two
cores. Then, each party has exactly one garbling and one
evaluating thread. The second and third strategy use two
or four independent threads per core to garble and eval-
uate at the same time. Moreover, to illustrate that IPP
is a modular concept, all circuits are evaluated using a
sequential code block that exposes all inner input and
output wires before and after every parallel region. This
guarantees the evaluation of mixed functionalities. Con-
sequently, all results include the time spent on transfer-
ring all required input bits to and from parallel regions.
Otherwise applications such as the MVMul application,
which is a pure parallel functionality, would proﬁt more
easily from IPP. Even though this weakens the results
for the example applications, we are convinced that this
procedure provides a better insight into the practical per-
formance of IPP.
The results of this experiment are reported in Table 4.
We ﬁrst observe that only the MExp application signif-
icantly proﬁts from IPP. This is due to the small shar-
ing state in comparison to the circuit complexity. For
both security levels IPP outperforms the raw CGP ap-
proach with an additional speed-up of 10-30% on all
cores. The performance of the MVMul applications ac-
tually decreases when using IPP. This is because of the
large state that needs to be transferred. The performance
gain through IPP cannot overcome the newly introduced
overhead of 31ms, which is more than 15% of the se-
quential run-time.
In summary, parallelizable applications that show a
small switching surface (measured in number of bits
compared to the overall circuit size) proﬁt from IPP.
Thus, IPP is a promising extension to Yao’s protocol that
utilizes circuit decomposition beyond naive paralleliza-
tion, independently of other optimization techniques.
Bi-directional bandwidth exploitation. The second
experiment aims towards increasing the available band-
width by exploiting bidirectional data transfers. Com-
monly, Ethernet connections have support for full duplex
(bi-directional) communication. When using standard
Yao’s garbled circuits, only one communication direc-
tion is fully utilized. However, with IPP the available
bandwidth can be doubled by symmetrically exploiting
both communication channels. This practical insight is
evaluated in a WAN setting between two cloud instances
of type m3.xlarge with 100± 10ms latency and a mea-
sured bandwidth of 92± 27Mbps. Each hosts runs two
threads (a garbling and a evaluating thread) using only
544  24th USENIX Security Symposium 
USENIX Association
14
Cores
Environment
IPP
none
1
2
4
2