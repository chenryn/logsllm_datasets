# PhishFarm: A Scalable Framework for Measuring the Effectiveness of Evasion Techniques against Browser Phishing Blacklists

**Authors:**
- Adam Oest
- Yeganeh Safaei
- Adam Doupé
- Gail-Joon Ahn
- Brad Wardman
- Kevin Tyers

**Affiliations:**
- Adam Oest, Yeganeh Safaei, Adam Doupé, Gail-Joon Ahn: Arizona State University
- Brad Wardman, Kevin Tyers: PayPal, Inc.

**Contact:**
- {aoest, ysafaeis, doupe, gahn}@asu.edu
- {bwardman, ktyers}@paypal.com

## Abstract

Phishing attacks have reached unprecedented volumes in recent years. Modern phishing websites are becoming increasingly sophisticated, employing various cloaking techniques to evade detection by security infrastructure. In this paper, we introduce PhishFarm, a scalable framework designed to systematically test the resilience of anti-phishing entities and browser blacklists against evasion efforts. We deployed 2,380 live phishing sites (on new, unique, and previously unseen .com domains) using six different HTTP request filters based on real-world phishing kits. Subsets of these sites were reported to ten distinct anti-phishing entities, and we measured the occurrence and timeliness of native blacklisting in major web browsers to assess the effectiveness of protection extended to users and organizations. Our experiments revealed significant shortcomings in current infrastructure, allowing some phishing sites to remain undetected while still accessible to victims. Simple cloaking techniques, such as those based on geolocation, device type, or JavaScript, reduced the likelihood of blacklisting by over 55% on average. Additionally, we found that blacklisting did not function as intended in popular mobile browsers (Chrome, Safari, and Firefox), leaving users particularly vulnerable. Following the disclosure of our findings, anti-phishing entities have improved their ability to detect and mitigate several cloaking techniques, and blacklisting has become more consistent between desktop and mobile platforms. However, further work is needed to ensure adequate user protection. The PhishFarm framework is designed for continuous monitoring and can be extended to test future state-of-the-art evasion techniques used by malicious websites.

## I. Introduction

Phishing has maintained record-breaking levels of volume in recent years [1] and continues to pose a significant threat to Internet users. In 2018, up to 113,000 unique monthly phishing attacks were reported to the Anti-Phishing Working Group (APWG) [2]. Beyond damaging well-known brands and compromising victims' identities, financials, and accounts, cybercriminals inflict millions of dollars in indirect damage annually due to the need for an expansive anti-abuse ecosystem [3]. With the increasing number of Internet users and services, especially on mobile devices [4], the feasibility of large-scale social engineering is also increasing. Given the potential for lucrative data, phishers are engaged in a relentless cat-and-mouse game with the ecosystem, seeking to stay ahead of mitigation efforts to maximize the effectiveness of their attacks. Although new phishing attack vectors are emerging (e.g., via social media as a distribution channel [5]), malicious actors still primarily deploy "classic" phishing websites [2].

Today's major web browsers, both on desktop and mobile platforms, natively incorporate anti-phishing blacklists and display prominent warnings when a user attempts to visit a known malicious site. Due to their ubiquity, blacklists are often a user's primary and sometimes only technical line of defense against phishing. Unfortunately, blacklists are inherently reactive [6], meaning a malicious website will generally not be blocked until its nature is verified by the blacklist operator. Phishing sites exploit this weakness by using cloaking techniques [7] to avoid or delay detection by blacklist crawlers. Cloaking has only recently been scrutinized in the context of phishing [8]; to date, there have been no formal studies of the impact of cloaking on blacklisting effectiveness. This shortcoming is important to address, as cybercriminals could potentially cause ongoing damage without the ecosystem's knowledge.

In this paper, we conduct a carefully controlled experiment to evaluate how ten different anti-phishing entities respond to reports of phishing sites that employ cloaking techniques representative of real-world attacks. We measure how this cloaking impacts the effectiveness (i.e., site coverage and timeliness) of native blacklisting across major desktop and mobile browsers. We performed preliminary tests in mid-2017, disclosed our findings to key entities (including Google Safe Browsing, Microsoft, browser vendors, and the APWG), and conducted a full-scale retest in mid-2018. Unlike prior work, we created our own (innocuous) PayPal-branded phishing websites (with permission) to minimize confounding effects and allow for an unprecedented degree of control.

Our work reveals several shortcomings within the anti-phishing ecosystem and underscores the importance of robust, ever-evolving anti-phishing defenses with good data sharing. Through our experiments, we found that cloaking can prevent browser blacklists from adequately protecting users by significantly decreasing the likelihood that a phishing site will be blacklisted or substantially delaying blacklisting, especially when geolocation- or device-based request filtering techniques are applied. Moreover, we identified a critical gap in the protection of top mobile web browsers: shockingly, mobile Chrome, Safari, and Firefox failed to show any blacklist warnings between mid-2017 and late 2018, despite the presence of security settings that implied blacklist protection. As a result of our disclosures, users of these mobile browsers now receive comparable protection to desktop users, and anti-phishing entities now better protect against some of the cloaking techniques we tested. We propose several additional improvements to further strengthen the ecosystem and will freely release the PhishFarm framework to interested researchers and security organizations for continued testing of anti-phishing systems and potential adaptation for measuring variables beyond just cloaking.

**Contributions:**
- A controlled empirical study of the effects of server-side request filtering on blacklisting coverage and timeliness in modern desktop and mobile browsers.
- A reusable, automated, scalable, and extensible framework for carrying out our experimental design.
- Identification of actionable real-world limitations in the current anti-phishing ecosystem.
- Enhancements to blacklisting infrastructure after disclosure to anti-phishing entities, including phishing protection in mobile versions of Chrome, Safari, and Firefox.

## II. Background

### A. Phishing Attacks

An online phishing attack consists of three stages: preparation, distribution, and data exfiltration. First, an attacker deploys a spoofed version of a legitimate website (by copying its look and feel) such that it is difficult for an average user to discern that it is fake. This deployment can be done using a phishing kit, as discussed in Section II-B. Second, the attacker sends messages (such as spam emails) to the user, leveraging social engineering to insist that action is needed [13]. If the victim is successfully fooled, they visit the site and submit sensitive information such as account credentials or credit card numbers. Finally, the phishing site transmits the victim’s information back to the phisher, who will attempt to fraudulently use it for monetary gain either directly or indirectly [14].

Phishing attacks are constantly evolving in response to ecosystem standards and may include innovative components that seek to circumvent existing mitigations. A current trend is the adoption of HTTPS by phishing sites, which helps them avoid negative security indicators in browsers and may give visitors a false sense of security [15], [16]. At the end of 2017, over 31% of all phishing sites reported to the APWG used HTTPS, up from less than 5% a year prior [2]. Another recent trend is the adoption of redirection links, which allows attackers to distribute a link that differs from the actual phishing landing page. Redirection chains commonly consist of multiple hops [17], [18], each potentially leveraging a different URL shortening service or open redirection vulnerability [19]. Notably, redirection allows the number of unique phishing links being distributed to grow well beyond the number of unique phishing sites, and such links might thus better slip past spam filters or volume-based phishing detection [20]. Furthermore, redirection through well-known services such as bit.ly may better fool victims [5], though it also allows intermediaries to enact mitigations. Ultimately, phishers cleverly attempt to circumvent existing controls to maximize the effectiveness of their attacks. The anti-phishing community should seek to predict such actions and develop new defenses while ensuring that existing controls remain resilient.

### B. Phishing Kits

A phishing kit is a unified collection of tools used to deploy a phishing site on a web server [21]. Kits are generally designed to be easy to deploy and configure; when combined with mass-distribution tools [9], they greatly lower the barrier to entry and enable phishing on a large scale [22]. They are part of the cybercrime-as-a-service economy [23] and are often customized to facilitate a specific attack [24]. In the wild, they are deployed on compromised infrastructure or infrastructure managed directly by malicious actors.

#### 1) Cloaking

A recent study found that phishing kits commonly use server-side directives to filter (i.e., block or turn away) unwanted (i.e., non-victim) traffic, such as search engine bots, anti-phishing crawlers, researchers, or users in locations that are incompatible with the phishing kit [8]. Attributes such as the visitor’s IP address, hostname, user agent, or referring URL are leveraged to implement these filtering techniques. Similar approaches, known as cloaking, have historically been used by malicious actors to influence search engine rankings by displaying different web content to bots than human visitors [7]. Users follow a misleading search result and are thus tricked into visiting a site that could contain malware or adware. Relatively little research has focused on cloaking outside of search engines; our experiment in Section III is the first controlled study, to the best of our knowledge, that measures the impact of cloaking within phishing attacks.

### C. Anti-Phishing Ecosystem

Even though phishing attacks only directly involve the attacker, the victim, and the impersonated organization, a large amount of collateral damage is caused due to the abuse of a plethora of independent systems that ultimately facilitate the attack [25]. Credential reuse, fueled by the sale of credentials in underground economies [9], [26], causes phishing threats aimed at one organization to potentially affect others. Over time, an anti-phishing ecosystem has matured, consisting of commercial security vendors, consumer antivirus organizations, web hosts, domain registrars, email and messaging platforms, and dedicated anti-phishing entities [27], [8]. These entities include enterprise firms that operate on behalf of victim organizations, clearinghouses that aggregate and share abuse data, and the blacklists that directly protect web browsers and other software.

### D. Detecting Phishing

The distribution phase of phishing attacks is inevitably noisy, as links to each phishing site are generally sent to a large set of potential victims. Thus, the thousands of phishing attacks reported each month easily translate into messages with millions of recipients [29]. Detection can also occur during the preparation and exfiltration stages. As artifact trails propagate through the security community, they can be used to detect and respond to attacks. Detection methods include classification of emails [30], [31], analyzing underground tools and drop zones [10], identifying malicious hostnames through passive DNS [32], URL and content classification [33], [20], [34], [35], malware scanning by web hosts [36], monitoring domain registrations [26] and certificate issuance [37], and receiving direct reports. All of these potential detection methods can result in reports that may be forwarded to anti-phishing entities that power blacklists [6].

### E. Browser Blacklists

The final piece of the puzzle in defending against phishing is the conversion of intelligence about attacks into the protection of users. Native browser blacklists are a key line of defense against phishing and malware sites, as they are enabled by default in modern web browsers and thus automatically protect even unaware users. In the absence of third-party security software, once a phishing message reaches a potential victim, browser blacklists are the only technical control that stands between the victim and the display of the phishing content. Studies have shown that blacklists are highly effective at stopping a phishing attack whenever a warning is shown [6]. Such warnings are prominent but typically only appear after the blacklist operator’s web crawling infrastructure verifies the attack; some are also based on proactive heuristics [6].

Today's major web browsers are protected by one of three different blacklist providers, as shown in Table I. While Google Safe Browsing (GSB) and SmartScreen are well-documented standalone blacklists, Opera does not publicly disclose the sources for its blacklist. Prior work by NSS Labs suggests that PhishTank and Netcraft are among Opera’s current third-party partners [38]; this is supported by our experimental results. We know that blacklists are effective when they function as intended [39], but is this always the case? Do blacklists offer adequate protection against phishers’ evasion efforts such as cloaking, or are phishers launching attacks with impunity? We focus on answering these questions in the rest of this paper.

## III. Experimental Methodology

Our primary research goal is to measure how cloaking affects the occurrence and timeliness of blacklisting of phishing sites within browsers. On a technical level, cloaking relies on filtering logic that restricts access to phishing content based on metadata from an HTTP request. Filtering is widely used in phishing kits [8]; attackers aim to maximize their return on investment by only showing the phishing content to victims rather than anti-abuse infrastructure. If a phishing kit suspects that the visitor is not a potential victim, a 404 “not found,” 403 “forbidden,” or 30x redirection response code [40] may be returned by the server in lieu of the phishing page. Attackers could also choose to display benign content instead.

Prior studies of browser blacklists have involved observation or honeypotting of live phishing attacks [25], [6], [39], but these tests did not consider cloaking. It is also difficult to definitively identify cloaking techniques simply by observing live sites. Our experimental design addresses this limitation.

### A. Overview

At a high level, we deploy our own (sterilized) phishing websites on a large scale, report their URLs to anti-phishing entities, and make direct observations of blacklisting times across major web browsers. We divide the phishing sites into multiple batches, each of which targets a different entity. We further sub-divide these batches into smaller groups with different types of cloaking. Once the sites are live, we report their URLs to the anti-phishing entity being tested, such that the entity sees a sufficient sample of each cloaking technique. We then monitor the entity’s response, with our primary metric being the time of blacklisting relative to the time we submitted each report. We collect secondary metrics from web traffic logs of each phishing site. Our approach is thus empirical in nature, but it is distinct from prior work because we fully control the phishing sites in question and, therefore, have ground truth on their deployment times and cloaking techniques.

All the phishing sites that we used for our experiments spoofed the PayPal.com login page (with permission from PayPal, Inc.) as it appeared in mid-2017 (we discuss the hosting approach in Section IV-C2). Our phishing sites each used new, unique, and previously unseen .com domain names and were hosted across a diverse set of IP addresses spanning three continents. As part of our effort to reliably measure the time between our report and browser blacklisting for each site, we never reported the same phishing site to more than one entity, nor did we reuse any domains. To summarize, each experiment proceeded as follows:

1. **Selecting a specific anti-phishing entity to test.**
2. **Deploying a large set of new, previously unseen PayPal phishing sites with desired cloaking techniques.**
3. **Reporting the sites to the entity.**
4. **Measuring if and when each site becomes blacklisted across major web browsers.**

We split our experiments into two halves: preliminary testing of ten anti-phishing entities (mid-2017) and full follow-up testing of five of the best-performing entities (mid-2018). The latter test involved a large sample size designed to support statistically significant inferences. In between the two tests, we disclosed our preliminary findings to key entities to afford them the opportunity to evaluate any shortcomings we identified. We discuss our approach in more detail in the following sub-sections and present the results in Section V.

#### 1) Targeted Web Browsers

We strove to maximize total market share while selecting the browsers to be tested. In addition to traditional desktop platforms, we wanted to measure the extent of blacklisting on mobile devices—a market that has grown and evolved tremendously in recent years [41]. We thus considered all the major desktop and mobile web browsers, as shown in Table I.