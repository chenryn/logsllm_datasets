title:PhishFarm: A Scalable Framework for Measuring the Effectiveness
of Evasion Techniques against Browser Phishing Blacklists
author:Adam Oest and
Yeganeh Safaei and
Adam Doup&apos;e and
Gail-Joon Ahn and
Brad Wardman and
Kevin Tyers
(cid:19)(cid:17)(cid:18)(cid:26)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:1)(cid:52)(cid:90)(cid:78)(cid:81)(cid:80)(cid:84)(cid:74)(cid:86)(cid:78)(cid:1)(cid:80)(cid:79)(cid:1)(cid:52)(cid:70)(cid:68)(cid:86)(cid:83)(cid:74)(cid:85)(cid:90)(cid:1)(cid:66)(cid:79)(cid:69)(cid:1)(cid:49)(cid:83)(cid:74)(cid:87)(cid:66)(cid:68)(cid:90)
PhishFarm: A Scalable Framework for Measuring
the Effectiveness of Evasion Techniques Against
Browser Phishing Blacklists
Adam Oest
˚
:
, Yeganeh Safaei
, Brad Wardman
:
PayPal, Inc.
{aoest, ysafaeis, doupe, gahn}@asu.edu, {bwardman, ktyers}@paypal.com
˚
, Adam Doup´e
§
Arizona State University,
˚§
, Gail-Joon Ahn
Samsung Research,
˚
˚
:
, Kevin Tyers
Abstract—Phishing attacks have reached record volumes in
recent years. Simultaneously, modern phishing websites are grow-
ing in sophistication by employing diverse cloaking techniques
to avoid detection by security infrastructure. In this paper, we
present PhishFarm: a scalable framework for methodically testing
the resilience of anti-phishing entities and browser blacklists to
attackers’ evasion efforts. We use PhishFarm to deploy 2,380
live phishing sites (on new, unique, and previously-unseen .com
domains) each using one of six different HTTP request ﬁlters
based on real phishing kits. We reported subsets of these sites
to 10 distinct anti-phishing entities and measured both the
occurrence and timeliness of native blacklisting in major web
browsers to gauge the effectiveness of protection ultimately
extended to victim users and organizations. Our experiments
revealed shortcomings in current infrastructure, which allows
some phishing sites to go unnoticed by the security community
while remaining accessible to victims. We found that simple
cloaking techniques representative of real-world attacks— in-
cluding those based on geolocation, device type, or JavaScript—
were effective in reducing the likelihood of blacklisting by over
55% on average. We also discovered that blacklisting did not
function as intended in popular mobile browsers (Chrome, Safari,
and Firefox), which left users of these browsers particularly
vulnerable to phishing attacks. Following disclosure of our
ﬁndings, anti-phishing entities are now better able to detect and
mitigate several cloaking techniques (including those that target
mobile users), and blacklisting has also become more consistent
between desktop and mobile platforms— but work remains to
be done by anti-phishing entities to ensure users are adequately
protected. Our PhishFarm framework is designed for continuous
monitoring of the ecosystem and can be extended to test future
state-of-the-art evasion techniques used by malicious websites.
I. INTRODUCTION
Phishing has maintained record-shattering levels of volume
in recent years [1] and continues to be a major threat to today’s
Internet users. In 2018, as many as 113,000 unique monthly
phishing attacks were reported to the APWG [2]. Beyond dam-
aging well-known brands and compromising victims’ identi-
ties, ﬁnancials, and accounts, cybercriminals annually inﬂict
millions of dollars of indirect damage due to the necessity
of an expansive anti-abuse ecosystem which serves to protect
the targeted companies and consumers [3]. With an ever-
increasing number of Internet users and services— in particu-
lar on mobile devices [4]— the feasibility of social engineering
on a large scale is also increasing. Given the potential for
lucrative data, phishers are engaged in a tireless cat-and-
mouse game with the ecosystem and seek to stay a step ahead
of mitigation efforts to maximize the effectiveness of their
attacks. Although new phishing attack vectors are emerging
(e.g. via social media as a distribution channel [5]), malicious
actors still primarily deploy “classic” phishing websites [2].
These malicious sites are ultimately accessed by victim users
who are tricked into revealing sensitive information.
Today’s major web browsers, both on desktop and mobile
platforms, natively incorporate anti-phishing blacklists and
display prominent warnings when a user attempts to visit a
known malicious site. Due to their ubiquity, blacklists are
a user’s main and at times only technical line of defense
against phishing. Unfortunately, blacklists suffer from a key
weakness:
they are inherently reactive [6]. Thus, a mali-
cious website will generally not be blocked until its nature
is veriﬁed by the blacklist operator. Phishing sites actively
exploit this weakness by leveraging cloaking techniques [7] to
avoid or delay detection by blacklist crawlers. Cloaking has
only recently been scrutinized in the context of phishing [8];
to date, there have been no formal studies of the impact
of cloaking on blacklisting effectiveness (despite numerous
empirical analyses of blacklists in general). This shortcoming
is important to address, as cybercriminals could potentially be
causing ongoing damage without the ecosystem’s knowledge.
In this paper, we carry out a carefully-controlled experiment
to evaluate how 10 different anti-phishing entities respond
to reports of phishing sites that employ cloaking techniques
representative of real-world attacks. We measure how this
cloaking impacts the effectiveness (i.e. site coverage and
timeliness) of native blacklisting across major desktop and
mobile browsers. We performed preliminary tests in mid-2017,
disclosed our ﬁndings to key entities (including Google Safe
Browsing, Microsoft, browser vendors, and the APWG), and
conducted a full-scale retest in mid-2018. Uniquely and unlike
prior work, we created our own (innocuous) PayPal-branded
phishing websites (with permission) to minimize confounding
effects and allow for an unprecedented degree of control.
Our work reveals several shortcomings within the anti-
phishing ecosystem and underscores the importance of robust,
ever-evolving anti-phishing defenses with good data sharing.
Through our experiments, we found that cloaking can prevent
browser blacklists from adequately protecting users by signif-
(cid:165)(cid:1)(cid:19)(cid:17)(cid:18)(cid:26)(cid:13)(cid:1)(cid:34)(cid:69)(cid:66)(cid:78)(cid:1)(cid:48)(cid:70)(cid:84)(cid:85)(cid:15)(cid:1)(cid:54)(cid:79)(cid:69)(cid:70)(cid:83)(cid:1)(cid:77)(cid:74)(cid:68)(cid:70)(cid:79)(cid:84)(cid:70)(cid:1)(cid:85)(cid:80)(cid:1)(cid:42)(cid:38)(cid:38)(cid:38)(cid:15)
(cid:37)(cid:48)(cid:42)(cid:1)(cid:18)(cid:17)(cid:15)(cid:18)(cid:18)(cid:17)(cid:26)(cid:16)(cid:52)(cid:49)(cid:15)(cid:19)(cid:17)(cid:18)(cid:26)(cid:15)(cid:17)(cid:17)(cid:17)(cid:21)(cid:26)
(cid:18)(cid:20)(cid:21)(cid:21)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:45 UTC from IEEE Xplore.  Restrictions apply. 
icantly decreasing the likelihood that a phishing site will be
blacklisted, or substantially delaying blacklisting, in particular
when geolocation- or device-based request ﬁltering techniques
are applied. Moreover, we identiﬁed a gaping hole in the
protection of top mobile web browsers: shockingly, mobile
Chrome, Safari, and Firefox failed to show any blacklist
warnings between mid-2017 and late 2018 despite the presence
of security settings that implied blacklist protection. As a
result of our disclosures, users of the aforementioned mobile
browsers now receive comparable protection to desktop users,
and anti-phishing entities now better protect against some of
the cloaking techniques we tested. We propose a number of
additional improvements which could further strengthen the
ecosystem, and we will freely release the PhishFarm frame-
work1 to interested researchers and security organizations
for continued testing of anti-phishing systems and potential
adaptation for measuring variables beyond just cloaking. Thus,
the contributions of this work are as follows:
‚ A controlled empirical study of the effects of server-side
request ﬁltering on blacklisting coverage and timeliness
in modern desktop and mobile browsers.
‚ A reusable, automated, scalable, and extensible frame-
work for carrying out our experimental design.
‚ Identiﬁcation of actionable real-world limitations in the
current anti-phishing ecosystem.
‚ Enhancements to blacklisting infrastructure after disclo-
sure to anti-phishing entities, including phishing protec-
tion in mobile versions of Chrome, Safari, and Firefox.
phisher, who will attempt to fraudulently use it for monetary
gain either directly or indirectly [14].
Phishing attacks are ever-evolving in response to ecosystem
standards and may include innovative components that seek
to circumvent existing mitigations. A current trend (mimick-
ing the wider web) is the adoption of HTTPS by phishing
sites, which helps them avoid negative security indicators in
browsers and may give visitors a false sense of security [15],
[16]. At the end of 2017, over 31% of all phishing sites
reported to the APWG used HTTPS, up from less than 5%
a year prior [2]. Another recent
trend is the adoption of
redirection links, which allows attackers to distribute a link
that differs from the actual phishing landing page. Redirection
chains commonly consist of multiple hops [17], [18], each
potentially leveraging a different URL shortening service or
open redirection vulnerability [19]. Notably, redirection allows
the number of unique phishing links being distributed to grow
well beyond the number of unique phishing sites, and such
links might thus better slip past spam ﬁlters or volume-based
phishing detection [20]. Furthermore, redirection through well-
known services such as bit.ly may better fool victims [5],
though it also allows the intermediaries to enact mitigations.
Ultimately, phishers cleverly attempt to circumvent existing
controls in an effort to maximize the effectiveness of their
attacks. The anti-phishing community should seek to predict
such actions and develop new defenses while ensuring that
existing controls remain resilient.
II. BACKGROUND
B. Phishing Kits
Phishing is a type of social engineering attack that seeks
to trick victims into disclosing sensitive information, often via
a fraudulent website which impersonates a real organization.
Attackers (phishers) then use the stolen data for their own
monetary gain [9], [10], [11]. Cybercriminals are vehement
in their attacks and the scale of credential theft cannot be
overstated. Between March 2016 and 2017, malware, phishing,
and data breaches led to 1.9 billion usernames and passwords
being offered for sale on black market communities [12].
A. Phishing Attacks
An online phishing attack consists of three stages: prepara-
tion, distribution, and data exﬁltration, respectively. First, prior
to involving any victims, an attacker deploys a spoofed version
of a legitimate website (by copying its look and feel) such that
it is difﬁcult for an average user to discern that it is fake. This
deployment can be done using a phishing kit, as discussed
in Section II-B. Second, the attacker sends messages (such
as spam e-mails) to the user (leveraging social engineering to
insist that action is needed [13]) and lures the user to click on
a link to the phishing site. If the victim is successfully fooled,
he or she then visits the site and submits sensitive information
such as account credentials or credit card numbers. Finally,
the phishing site transmits the victim’s information back to the
1Framework details are available at https://phishfarm-project.com
A phishing kit
is a uniﬁed collection of tools used to
deploy a phishing site on a web server [21]. Kits are generally
designed to be easy to deploy and conﬁgure; when combined
with mass-distribution tools [9], they greatly lower the barrier
to entry and enable phishing on a large scale [22]. They are
part of the cybercrime-as-a-service economy [23] and are often
customized to facilitate a speciﬁc attack [24]. In the wild, they
are deployed on compromised infrastructure or infrastructure
managed directly by malicious actors.
1) Cloaking: A recent study found that phishing kits com-
monly use server-side directives to ﬁlter (i.e. block or turn
away) unwanted (i.e. non-victim) trafﬁc, such as search engine
bots, anti-phishing crawlers, researchers, or users in locations
that are incompatible with the phishing kit [8]. Attributes such
as the visitor’s IP address, hostname, user agent, or referring
URL are leveraged to implement these ﬁltering techniques.
Similar approaches, known as cloaking, have historically
been used by malicious actors to inﬂuence search engine rank-
ings by displaying different web content to bots than human
visitors [7]. Users follow a misleading search result and are
thus tricked into visiting a site which could contain malware
or adware. Relatively little research has focused on cloaking
outside of search engines; our experiment in Section III is
the ﬁrst controlled study, to the best of our knowledge, that
measures the impact of cloaking within phishing attacks.
(cid:18)(cid:20)(cid:21)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:45 UTC from IEEE Xplore.  Restrictions apply. 
TABLE I: Overview of market share and blacklist providers
of major web browsers (* denotes browsers we tested).
Browser Name
Phishing
Blacklist
Provider
Desktop Web Browsers
Google Chrome*
Mozilla Firefox*
Safari*
Internet Explorer (IE)*
Microsoft Edge*
Opera*
Others
Google
Safe Browsing
(GSB)
Microsoft
SmartScreen
Opera
Varies
Mobile Web Browsers
Google Chrome*
Safari*
UC Browser
Samsung Internet
Opera* (non-mini)
Android Browser
Mozilla Firefox*
Others
GSB
GSB
None
None
Opera
None
GSB
Varies
Est. Market Share
(Worldwide) [28], [4]
7/2017
7/2018
63.48%
13.82%
5.04%
9.03%
3.95%
2.25%
2.43%
50.07%
17.19%
15.55%
6.54%
5.58%
3.41%
0.06%
1.60%
67.60%
11.23%
5.01%
6.97%
4.19%
2.48%
2.52%
55.98%
17.70%
12.49%
5.12%
4.26%
1.95%
0.31%
2.19%
C. Anti-phishing Ecosystem
Even though phishing attacks only directly involve the
attacker, the victim, and the impersonated organization, a large
amount of collateral damage is caused due to the abuse of
a plethora of independent systems which ultimately facilitate
the attack [25]. Moreover, credential re-use, fueled by the
sale of credentials in underground economies [9], [26], causes
phishing threats aimed at one organization to potentially affect
others. Over time, an anti-phishing ecosystem has matured; it
consists of commercial security vendors, consumer antivirus
organizations, web hosts, domain registrars, e-mail and mes-
saging platforms, and dedicated anti-phishing entities [27],
[8]. These entities consist of enterprise ﬁrms that operate on
behalf of victim organizations, clearinghouses that aggregate
and share abuse data, and the blacklists which directly protect
web browsers and other software.
D. Detecting Phishing
The distribution phase of phishing attacks is inevitably
noisy, as links to each phishing site are generally sent to a
large set of potential victims. Thus, the thousands of phishing
attacks reported each month easily translate into messages with
millions of recipients [29]. Detection can also occur during the
preparation and exﬁltration stages. As artifact trails propagate
through the security community, they can be used to detect and
respond to attacks. Detection methods include classiﬁcation
of e-mails [30], [31], analyzing underground tools and drop
zones [10], identifying malicious hostnames through passive
DNS [32], URL and content classiﬁcation [33], [20], [34],
[35], malware scanning by web hosts [36], monitoring domain
registrations [26] and certiﬁcate issuance [37], and receiving
direct reports. All of these potential detection methods can
result in reports which may be forwarded to anti-phishing
entities that power blacklists [6].
E. Browser Blacklists
The ﬁnal piece of the puzzle in defending against phishing is
the conversion of intelligence about attacks into the protection
of users. Native browser blacklists are a key line of defense
against phishing and malware sites, as they are enabled by
default in modern web browsers and thus automatically protect
even unaware users. In the absence of third-party security
software, once a phishing message reaches a potential victim,
browser blacklists are the only technical control that stands
between the victim and the display of the phishing content.
Studies have shown that blacklists are highly effective at
stopping a phishing attack whenever a warning is shown [6].
Such warnings are prominent, but typically only appear after
the blacklist operator’s web crawling infrastructure veriﬁes the
attack; some are also based on proactive heuristics [6].
Today’s major web browsers are protected by one of three
different blacklist providers, as shown in Table I. While
Google Safe Browsing (GSB) and SmartScreen are well-
documented standalone blacklists, Opera does not publicly
disclose the sources for its blacklist. Prior work by NSS
Labs suggests that PhishTank and Netcraft are among Opera’s
current
this is supported by our
experimental results. We know that blacklists are effective
when they function as intended [39], but is this always the
case? Do blacklists offer adequate protection against phishers’
evasion efforts such as cloaking, or are phishers launching
attacks with impunity? We focus on answering these questions
in the rest of this paper.
third-party partners [38];
III. EXPERIMENTAL METHODOLOGY
Our primary research goal is to measure how cloaking af-
fects the occurrence and timeliness of blacklisting of phishing
sites within browsers. On a technical level, cloaking relies on
ﬁltering logic that restricts access to phishing content based
on metadata from an HTTP request. Filtering is widely used
in phishing kits [8]; attackers aim to maximize their return on
investment by only showing the phishing content to victims
rather than anti-abuse infrastructure. If a phishing kit suspects
that the visitor is not a potential victim, a 404 “not found”,
403 “forbidden”, or 30x redirection response code [40] may be
returned by the server in lieu of the phishing page. Attackers
could also choose to display benign content instead.
Prior studies of browser blacklists have involved observation
or honeypotting of live phishing attacks [25], [6], [39], but
these tests did not consider cloaking. It is also difﬁcult to
deﬁnitively identify cloaking techniques simply by observing
live sites. Our experimental design addresses this limitation.
A. Overview
At a high level, we deploy our own (sterilized) phishing
websites on a large scale, report their URLs to anti-phishing
entities, and make direct observations of blacklisting times
across major web browsers. We divide the phishing sites into
multiple batches, each of which targets a different entity.
We further sub-divide these batches into smaller groups with
different types of cloaking. Once the sites are live, we report
their URLs to the anti-phishing entity being tested, such that
the entity sees a sufﬁcient sample of each cloaking technique.
We then monitor the entity’s response, with our primary metric
being the time of blacklisting relative to the time we submitted
(cid:18)(cid:20)(cid:21)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:42:45 UTC from IEEE Xplore.  Restrictions apply. 
each report. We collect secondary metrics from web trafﬁc logs
of each phishing site. Our approach is thus empirical in nature,
however it is distinct from prior work because we fully control
the phishing sites in question, and, therefore, have ground truth
on their deployment times and cloaking techniques.
All the phishing sites that we used for all of our experiments
spoofed the PayPal.com login page (with permission from
PayPal, Inc.) as it appeared in mid-2017 (we discuss the
hosting approach in Section IV-C2). Our phishing sites each
used new, unique, and previously-unseen .com domain names,
and were hosted across a diverse set of IP addresses spanning
three continents. As part of our effort to reliably measure the
time between our report and browser blacklisting for each site,
we never reported the same phishing site to more than one
entity, nor did we re-use any domains. To summarize, each
experiment proceeded as follows:
1) Selecting a speciﬁc anti-phishing entity to test.
2) Deploying a large set of new, previously-unseen PayPal
phishing sites with desired cloaking techniques.
3) Reporting the sites to the entity.
4) Measuring if and when each site becomes blacklisted
across major web browsers.
We split our experiments into two halves: preliminary
testing of 10 anti-phishing entities (mid-2017) and full follow-
up testing of ﬁve of the best-performing entities (mid-2018).
The latter test
involved a large sample size designed to
support statistically signiﬁcant inferences. In between the two
tests, we disclosed our preliminary ﬁndings to key entities
to afford them the opportunity to evaluate any shortcomings
we identiﬁed. We discuss our approach in more detail in the
following sub-sections and present the results in Section V.
1) Targeted Web Browsers: We strove to maximize to-
tal market share while selecting the browsers to be tested.
In addition to traditional desktop platforms, we wanted to
measure the extent of blacklisting on mobile devices— a
market which has grown and evolved tremendously in recent
years [41]. We thus considered all the major desktop and