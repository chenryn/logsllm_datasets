title:Anycast Latency: How Many Sites Are Enough?
author:Ricardo de Oliveira Schmidt and
John S. Heidemann and
Jan Harm Kuipers
Anycast Latency: How Many Sites Are Enough?
Ricardo de Oliveira Schmidt1(B), John Heidemann2, and Jan Harm Kuipers1
1 University of Twente, Enschede, The Netherlands
PI:EMAIL, PI:EMAIL
2 USC/Information Sciences Institute, Marina Del Rey, USA
PI:EMAIL
Abstract. Anycast is widely used today to provide important services
such as DNS and Content Delivery Networks (CDNs). An anycast service
uses multiple sites to provide high availability, capacity and redundancy.
BGP routing associates users to sites, deﬁning the catchment that each
site serves. Although prior work has studied how users associate with
anycast services informally, in this paper we examine the key question
how many anycast sites are needed to provide good latency, and the worst
case latencies that speciﬁc deployments see. To answer this question, we
ﬁrst deﬁne the optimal performance that is possible, then explore how
routing, speciﬁc anycast policies, and site location aﬀect performance.
We develop a new method capable of determining optimal performance
and use it to study four real-world anycast services operated by diﬀerent
organizations: C-, F-, K-, and L-Root, each part of the Root DNS service.
We measure their performance from more than 7,900 vantage points
(VPs) worldwide using RIPE Atlas. (Given the VPs uneven geographic
distribution, we evaluate and control for potential bias.) Our key results
show that a few sites can provide performance nearly as good as many,
and that geographic location and good connectivity have a far stronger
eﬀect on latency than having many sites. We show how often users see the
closest anycast site, and how strongly routing policy aﬀects site selection.
1 Introduction
Internet content providers want to provide their customers with good service,
guaranteeing high reliability and fast performance. These goals can be limited by
underlying resources at servers (load) and in the network (throughput, latency,
and reliability). Replicating instances of the service at diﬀerent sites around the
Internet can improve all of these factors by increasing the number of available
servers, moving them closer to the users, and diversifying the network in between.
Service replication is widely used for naming (DNS) and web and media
Content Delivery Networks (CDNs). Two diﬀerent mechanisms associate users
with particular service instances: DNS-based redirection [12] and IP anycast [1,
30], and they can be combined [13,28]). When the service is DNS, IP anycast is
the primary mechanism, used by many operators, including most root servers,
top-level domains, many large companies, and public resolvers [22,38]. IP anycast
is also used by several web CDNs (Bing, CloudFlare, Edgecast), while others use
c(cid:2) Springer International Publishing AG 2017
M.A. Kaafar et al. (Eds.): PAM 2017, LNCS 10176, pp. 188–200, 2017.
DOI: 10.1007/978-3-319-54328-4 14
Anycast Latency: How Many Sites Are Enough?
189
DNS-based redirection (Akamai, Google, and Microsoft), or their combination
(LinkedIn). This paper, however, focuses only on IP anycast.
In IP anycast, service is provided on a speciﬁc service IP address, and that
address is announced from many physical locations (anycast sites), each with
one or multiple servers1. BGP routing policies then associate each user with
one site, deﬁning that site’s catchment. Optimally users are associated with the
nearest site, minimizing latency. BGP provides considerable robustness, adapt-
ing to changes in service or network availability, and allowing for some policy
control. However, user-to-site mapping is determined by BGP routing, a distrib-
uted computation based on input of many network operators policies. Although
mapping generally follows geography [27], studies of routing have shown that
actual network topology can vary [36], and recent observations have shown that
the mapping can be unexpectedly chaotic [6,23].
Anycast has been widely studied, typically with measurement studies that
assess anycast coverage and latency [5,8,9,17,21,25,26,29,34], and also to enu-
merate anycast sites [19]. Latency studies using server-side traces show that any-
cast behaves roughly as expected—many geographically distributed sites reduce
latency. These studies also show surprising complexity in how users are assigned
to anycast sites. While prior studies cover what does happen, no prior work
deﬁnes what could and should happen—that is, what latency is possible, and
the reasons actual latency may diﬀer from this ideal.
The main contribution of this paper is to develop a new measurement
methodology that identiﬁes optimal
latency in IP anycast systems (Sect. 2),
enabling a ﬁrst evaluation of how close actual latencies are to their potential. Our
insight is that we can determine optimal anycast latency by measuring unicast
latency to all anycast sites of a system, providing a comparison to the assigned
site by BGP. Thus, while prior work reports only latency for the selected anycast
site, we can see when catchments diﬀer from optimal and then study why. Our
dataset from this study is publicly available at http://traces.simpleweb.org/.
Our second contribution is to carry out a measurement study of four IP
anycast deployments: the C-, F-, K- and L-Root DNS services, consisting of more
than 240 sites together. These services have diﬀerent architectures and deploy-
ment strategies, that we study from around 7,900 RIPE Atlas probes worldwide,
creating a rich dataset to inform our understanding of anycast latency.
The ﬁnal contribution of this work is what we learn from this ﬁrst compar-
ison of actual and optimal anycast latency. Our central question is: How many
anycast sites are “enough” to get “good” latency? To answer this question, we
must ﬁrst answer several related questions: Does anycast give good absolute
performance (Sect. 3.1)? Do users get the closest anycast site (Sect. 3.2)? How
much does the location of each anycast site aﬀect the latency it provides overall
(Sect. 3.3)? How much do local routing policies aﬀect performance (Sect. 3.5)?
With these questions resolved, we return to our key contribution and show that
a modest number of well-placed anycast sites—as few as twelve—can provide
1 The term anycast instance can refer to a site or to speciﬁc servers at a site. Because
of this ambiguity we avoid that term in this paper.
190
R. de Oliveira Schmidt et al.
nearly as good performance as many (Sect. 3.6). We also show that more sites
improve the tail of the performance distribution (Sect. 3.4).
This paper focuses on anycast latency. We consider latency because it moti-
vates huge investments, such as Google’s 2013 expansion to thousands of loca-
tions [12], gradual expansion of Root DNS anycast to more than 500 sites [18],
and CDN design in multiple companies. We recognize that anycast serves other
purposes as well, including distributing load, improving resilience to Denial-
of-Service attacks, and to support policy choices. These are, however, out of
the scope of this paper. Our population of vantage points is European-centric
(Sect. 3.3); while this skew aﬀects our speciﬁc results, it does not change our
qualitative conclusions. Broader exploration of CDNs, other metrics, and other
sets of vantage points are future work (some in-progress).
2 Measurement Methodology
Our approach to observe anycast latency is straightforward: from as many loca-
tions (vantage points, or VPs) as we can, we measure latency to all anycast sites
of each service that we study. These measurements approximate the catchment
of VPs that each site serves. We use RIPE Atlas probes as VPs, and we study
the C-, F-, K- and L-Root DNS services as our targets. We measure latency with
pings (ICMP echo requests), and identify sites with DNS CHAOS queries. Prior
studies [6,19] have used both of these mechanisms, but only to preferred site; to
our knowledge, we are the ﬁrst to measure latency to all anycast sites from all
VPs, the key that allows us to study optimal latency (not just actual), and to
explore policy questions (Sect. 3).
Measurement sources: We use more than 7,900 VPs (probes) in the RIPE
Atlas framework [32]. Figure 1 shows the locations of all VPs: these cover 174
countries and 2927 ASes. We maximize coverage by using all probes that are
available at each measurement time. The exact number, shown in Table 1, varies
slightly as VPs come and go over measurements taken in 2015 and 2016. While
RIPE VPs are global, their geographic distribution does not exactly match that
of the overall Internet population. We show in Sect. 3 that this skew strongly
aﬀects the speciﬁc quantitative latencies we observe, favoring sites and VPs in
Europe. But it does not aﬀect our qualitative results about the number of anycast
sites and the eﬀects of routing policies.
Measurement targets: We study four operational anycast services: the C-,
F-, K- and L-Root DNS services [18] (Fig. 2). Each service is run by a diﬀerent
operator and is optimized to meet their goals. They are diverse in both number
of sites (with C small, F and K mid-sized, and L numerous), and in routing
policy: all C and L sites are global (available to all), while many K and most
F sites are local (service limited to speciﬁc AS). To identify optimal possible
latency (Sect. 3), we chose these services because they all make public the uni-
cast IP address of each site. We measure K Root both in 2015 (K), and again
Anycast Latency: How Many Sites Are Enough?
191
L
L
L
L
K
K
L
L
L
L
F
F
L
F
L
C
L
L
FF
LL
L
L
F
L
C
F
C
L
C
L
F
L
L
K
L
K
F
L
LL
K
F
L
L
K
K
L
F
L
L
F
L
L
LL
L
LL L
KK
F FF
K
L
F
L
K
L
L
F
L
KKKKK
CC
F
L
L
L
F
L
L
L
K
C
K
L
L
L
L
L
K
F
L
K
L
F
F
F
K
L
L
L
F
C
KK
L L
F
K
L
L
L
F
L
F
L
L
K
L
K
L
K
L
L
L
F
L
L
L
F
L
L
K
L
F
L
F
L
L
F
L
F
F
L
F
L
F
L
L
L
F
L
F
L
L
F
C-Root
F-Root
K-Root
L-Root
L
L
L
L
L
L
L
L
L
L
L
L
F
L
L
L
L
L
L
L
F
F
L
F
F
F
L
L
L
L
L
L
L
F
F
L
F
L
L
L
K
F
F
L
F
L
F
F
L
F
F
L
LL
L
F
L
L
L
L
K
F
L
L
LL
L
L
F
L
F
L
L
Fig. 1. Locations of more than 7,900
vantage points we use from RIPE
Atlas.
Fig. 2. Locations of sites for each service
(each site is identiﬁed by its letter).
Table 1. Summary of each root service, its size in sites, and their routing policy;
measurement date and number of VPs then available; how many hits are optimal,
latency for each type of hit, and the cost of mishits (Sect. 3.2). We measure K-Root
both before (K) and after (NK) its change in routing policy (Sect. 3.5).
letter sites (local) date
VPs hit type
median RTT (ms)
mishit penalty(ms)
optimal mishit all optimal mishit
(pref.) 25%ile 50%ile 75%ile
C
F
K
NK
L
8 (0)
58 (53)
33 (14)
36 (1)
2015-09 5766 84%
2015-12 6280 44%
2015-11 6464 41 %
2016-04 5557 40 %
144 (0)
2015-12 5351 24%
16%
56%
59%
60%
76%
32 28
25 12
32 14
30 12
30 11
61
39
43
41
47
55
20
23
19
16
2
8
8
9
10
5
15
18
18
24
10
51
42
48
82
in 2016 (NK—New K ) after major changes on its anycast policies, discussing
implications in Sect. 3.5.
Measuring anycast catchments: We map the catchments of each anycast
service by observing DNS CHAOS queries [39] (with name hostname.bind and
type TXT) from each VP. The reply to each VP’s CHAOS query indicates its
anycast site, as determined by BGP routing. The exact contents of the reply
are service-speciﬁc, but several root operators (including C, F, K and L) reply
with the unicast hostname of the reached site. For example, a reply for C Root
is lax1b.c.root-servers.org, where lax gives the geographic location of the
replying site and 1b identiﬁes the replying server within the site. The resolution
of this name gives the unicast IP address of that server. Sites sometimes have
multiple servers, but we treat all servers at a site as equivalent.
Measuring latency: We use ICMP ECHO requests (pings) to measure latency
from VPs to both the public anycast service address (BGP-assigned site), and
the unicast address of all sites for each service. To suppress noise in individual
pings, we use multiple pings and report the 10th-percentile value as the measured
latency. On average VPs send 30 pings to each anycast site, but the exact number
varies due to dynamics on the RIPE Atlas framework, limitations on availability
of probes, and measurement scheduling.
192
R. de Oliveira Schmidt et al.
3 Observation and Findings
3.1 Does Anycast Give Good Absolute Performance?
We ﬁrst look at absolute latency seen from VPs for each anycast service. The
solid lines in Fig. 3 show the distribution of latency seen from each VP to the
service of the four measured letters. It reports the actual RTT to each VP’s
BGP-assigned site. We see that all letters provide low latency to most users:
median RTT for C and K Root is 32 ms, L’s median is 30 ms and F’s is 25 ms.
Is 30 ms latency “good”? For DNS during web browsing (DNS on www.
example.com), every millisecond matters. However, names at the root (like com)
are easily cachable: there are only around 1000 names and they allow caching
for two days, so shared caches at recursive resolvers are very eﬀective. But we
consider 30 ms great, and somewhat arbitrarily deﬁne 100 ms as high latency
(matching ideal network latencies from New York to California or Sydney).
More study is needed to understand the relationship between Root DNS
performance and user-perceived latency to provide deﬁnitive thresholds.
This data shows that median latency does not strictly follow anycast size—
while F and L have better latency than C and K, corresponding with their
larger number of anycast sites (58 and 144 vs. 8 and 33), the improvement is
somewhat modest. Actual latency is no more than 30 ms diﬀerent between any
letter in most of the distribution. (At the tail of the distribution however, this
diﬀerence increases up to 135 ms.) This result is quite surprising since there is a
huge diﬀerence on the sizes of the anycast deployments of the measured letters.
For services with many sites, careful route engineering can also make a large
diﬀerence in latency. F’s median latency is lower than L’s (25 ms vs. 30 ms),
even though it has about half the sites (58 vs. 144). This diﬀerence may be from
route engineering by F, explicitly using RIPE Atlas for debugging [6].
3.2 Do Users Get the Closest Anycast Site?
While we showed a few sites can provide good latency, do they provide optimal
latency? Anycast relies on BGP to map users to sites, but BGP only approxi-
mates shortest-path routing. The dotted lines in Fig. 3 show the optimal possible
performance based on unicast routing to each individual site of all measured
letters, ignoring anycast routing policies and catchments. We see that C-Root’s
actual service is very close to optimal (solid and dotted lines nearly overlap). We
believe that this is because C has only a few, geographically distributed sites,
and all sites are global—that is, C’s sites are all visible across the Internet.
By contrast, larger anycast deployments show a larger diﬀerence between
actual and optimal latency. These diﬀerences arise because more sub-optimal
choices are available, and because these services have some or many local nodes
that might place policy limitations on routing (Sect. 3.5). Looking at optimal
possible performance in Fig. 3 we see that routing freedom would improve median
latency for F-, K- and L-Root by 16 ms, 19 ms and 14 ms, which represents an
improvement of 36%, 40% and 53% respectively. (We recognize that constraints
Anycast Latency: How Many Sites Are Enough?
193
 1
 0.8
 0.6
 0.4
 0.2
F
D
C
median RTT = 32ms
C-Root actual