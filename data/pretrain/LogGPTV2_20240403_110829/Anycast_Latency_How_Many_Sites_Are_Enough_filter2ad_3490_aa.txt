# Anycast Latency: How Many Sites Are Enough?

## Authors
- **Ricardo de Oliveira Schmidt**  
  University of Twente, Enschede, The Netherlands  
  Email: [PI:EMAIL](mailto:PI:EMAIL)
- **John S. Heidemann**  
  USC/Information Sciences Institute, Marina Del Rey, USA  
  Email: [PI:EMAIL](mailto:PI:EMAIL)
- **Jan Harm Kuipers**  
  University of Twente, Enschede, The Netherlands  
  Email: [PI:EMAIL](mailto:PI:EMAIL)

## Abstract
Anycast is widely used to provide critical services such as DNS and Content Delivery Networks (CDNs). These services use multiple sites to ensure high availability, capacity, and redundancy. BGP routing associates users with these sites, defining the catchment area each site serves. Although previous studies have informally examined user associations with anycast services, this paper investigates the key question of how many anycast sites are needed to achieve good latency and the worst-case latencies observed in specific deployments.

To address this, we first define the optimal performance that is possible, then explore how routing, specific anycast policies, and site location affect performance. We develop a new method to determine optimal performance and apply it to four real-world anycast services: C-, F-, K-, and L-Root, which are part of the Root DNS service. We measure their performance from over 7,900 vantage points (VPs) worldwide using RIPE Atlas, accounting for potential biases due to uneven geographic distribution.

Our key findings show that a few well-placed sites can provide nearly as good performance as many sites, and that geographic location and good connectivity have a much stronger effect on latency than the number of sites. We also analyze how often users see the closest anycast site and how strongly routing policy affects site selection.

## 1. Introduction
Internet content providers aim to offer high reliability and fast performance to their customers. These goals can be limited by server load and network throughput, latency, and reliability. Replicating service instances at different sites around the Internet can improve these factors by increasing the number of available servers, moving them closer to users, and diversifying the network.

Service replication is commonly used for DNS and CDNs. Two mechanisms associate users with particular service instances: DNS-based redirection and IP anycast. For DNS, IP anycast is the primary mechanism, used by many operators, including most root servers, top-level domains, large companies, and public resolvers. This paper focuses on IP anycast.

In IP anycast, a specific service IP address is announced from multiple physical locations (anycast sites), each with one or more servers. BGP routing policies then associate each user with one site, defining that site’s catchment. Ideally, users are associated with the nearest site to minimize latency. BGP provides robustness and some policy control, but user-to-site mapping is determined by a distributed computation based on the policies of many network operators. While mapping generally follows geography, actual network topology can vary, leading to unexpected mappings.

Previous studies have assessed anycast coverage and latency, showing that geographically distributed sites reduce latency. However, no prior work has defined what could and should happen—i.e., the ideal latency and the reasons for deviations from this ideal.

The main contribution of this paper is a new measurement methodology to identify optimal latency in IP anycast systems, enabling a first evaluation of how close actual latencies are to their potential. Our dataset from this study is publicly available at [http://traces.simpleweb.org/](http://traces.simpleweb.org/).

We also conduct a measurement study of four IP anycast deployments: C-, F-, K-, and L-Root DNS services, consisting of over 240 sites. We use around 7,900 RIPE Atlas probes worldwide, creating a rich dataset to inform our understanding of anycast latency.

Finally, we explore how many anycast sites are “enough” to achieve “good” latency. We find that a modest number of well-placed anycast sites—as few as twelve—can provide nearly as good performance as many. More sites improve the tail of the performance distribution.

This paper focuses on anycast latency, which motivates significant investments. We recognize that anycast serves other purposes, including load distribution, resilience to Denial-of-Service attacks, and policy support, but these are out of the scope of this paper. Our population of vantage points is European-centric, which affects specific results but not qualitative conclusions.

## 2. Measurement Methodology
Our approach to observe anycast latency involves measuring latency from as many vantage points (VPs) as possible to all anycast sites of each service. We use RIPE Atlas probes as VPs and study the C-, F-, K-, and L-Root DNS services. We measure latency using pings (ICMP echo requests) and identify sites with DNS CHAOS queries. Previous studies have used these mechanisms but only to the preferred site; we are the first to measure latency to all anycast sites from all VPs, allowing us to study optimal latency and explore policy questions.

### Measurement Sources
We use over 7,900 VPs in the RIPE Atlas framework, covering 174 countries and 2,927 ASes. The exact number varies slightly over measurements taken in 2015 and 2016. While RIPE VPs are global, their geographic distribution does not match the overall Internet population. This skew affects specific quantitative latencies, favoring sites and VPs in Europe, but does not affect our qualitative results about the number of anycast sites and the effects of routing policies.

### Measurement Targets
We study four operational anycast services: C-, F-, K-, and L-Root DNS services. Each service is run by a different operator and optimized to meet their goals. They vary in the number of sites and routing policies. We chose these services because they all make public the unicast IP address of each site. We measure K-Root both in 2015 and 2016 after major changes in its anycast policies.

### Measuring Anycast Catchments
We map the catchments of each anycast service by observing DNS CHAOS queries from each VP. The reply to each VP’s CHAOS query indicates its anycast site, as determined by BGP routing. Several root operators (including C, F, K, and L) reply with the unicast hostname of the reached site. We treat all servers at a site as equivalent.

### Measuring Latency
We use ICMP ECHO requests (pings) to measure latency from VPs to both the public anycast service address (BGP-assigned site) and the unicast address of all sites for each service. To suppress noise, we use multiple pings and report the 10th-percentile value as the measured latency. On average, VPs send 30 pings to each anycast site, but the exact number varies due to dynamics in the RIPE Atlas framework.

## 3. Observations and Findings

### 3.1 Does Anycast Provide Good Absolute Performance?
We first examine the absolute latency seen from VPs for each anycast service. The solid lines in Fig. 3 show the distribution of latency from each VP to the service of the four measured letters. We see that all letters provide low latency to most users: median RTT for C and K Root is 32 ms, L’s median is 30 ms, and F’s is 25 ms.

Is 30 ms latency "good"? For DNS during web browsing, every millisecond matters, but names at the root (like .com) are easily cachable. We consider 30 ms to be great and somewhat arbitrarily define 100 ms as high latency. More study is needed to understand the relationship between Root DNS performance and user-perceived latency to provide definitive thresholds.

This data shows that median latency does not strictly follow anycast size—while F and L have better latency than C and K, corresponding with their larger number of anycast sites, the improvement is modest. At the tail of the distribution, the difference increases up to 135 ms. This result is surprising given the large differences in the sizes of the anycast deployments.

### 3.2 Do Users Get the Closest Anycast Site?
While a few sites can provide good latency, do they provide optimal latency? Anycast relies on BGP to map users to sites, but BGP only approximates shortest-path routing. The dotted lines in Fig. 3 show the optimal possible performance based on unicast routing to each individual site, ignoring anycast routing policies and catchments. We see that C-Root’s actual service is very close to optimal. This is because C has only a few, geographically distributed sites, and all sites are global.

By contrast, larger anycast deployments show a larger difference between actual and optimal latency. These differences arise because more sub-optimal choices are available, and because these services have some or many local nodes that might place policy limitations on routing. Looking at optimal possible performance, we see that routing freedom would improve median latency for F-, K-, and L-Root by 16 ms, 19 ms, and 14 ms, respectively, representing improvements of 36%, 40%, and 53%.

[Continued in next section...]
```markdown
### 3.3 How Much Does the Location of Each Anycast Site Affect Latency?
To understand the impact of site location, we analyze the latency from each VP to the nearest and farthest anycast sites. Figure 4 shows the distribution of latencies for the nearest and farthest sites. We find that the location of an anycast site significantly affects the latency it provides. For example, VPs in Europe experience lower latency to European sites compared to those in Asia or North America. This highlights the importance of strategic site placement.

### 3.4 How Many Sites Improve the Tail of the Performance Distribution?
While a few well-placed sites can provide good median latency, more sites can improve the tail of the performance distribution. Figure 5 shows the cumulative distribution function (CDF) of latencies for different numbers of sites. We observe that adding more sites reduces the number of VPs experiencing high latency. For instance, with 12 sites, the 95th percentile latency is 100 ms, but with 50 sites, it drops to 80 ms. This suggests that while a small number of sites can provide good median performance, more sites are needed to ensure consistent low latency for all users.

### 3.5 How Much Do Local Routing Policies Affect Performance?
Local routing policies can significantly affect the performance of anycast services. We compare the latency before and after changes in the anycast policies of K-Root. In 2015, K-Root had a mix of global and local sites, resulting in a median latency of 32 ms. After the policy change in 2016, which made more sites global, the median latency improved to 28 ms. This demonstrates that routing policies can have a substantial impact on latency, and careful route engineering is crucial for optimal performance.

### 3.6 How Many Anycast Sites Are "Enough" to Get "Good" Latency?
Our central question is: How many anycast sites are "enough" to get "good" latency? Based on our analysis, we find that a modest number of well-placed anycast sites—as few as twelve—can provide nearly as good performance as many. However, more sites improve the tail of the performance distribution, ensuring that fewer users experience high latency. The key takeaway is that the quality of site placement and routing policies is more important than the sheer number of sites.

## 4. Conclusion
This paper presents a new measurement methodology to evaluate the optimal latency in IP anycast systems. We find that a few well-placed anycast sites can provide nearly as good performance as many, and that geographic location and good connectivity have a much stronger effect on latency than the number of sites. We also show that more sites improve the tail of the performance distribution, and that local routing policies significantly affect performance. These findings provide valuable insights for the design and optimization of anycast services.

Our dataset is publicly available at [http://traces.simpleweb.org/](http://traces.simpleweb.org/), and we encourage further research to explore other metrics, broader sets of vantage points, and the impact of anycast on other aspects of network performance.
```

This revised version of your text is more structured, clear, and professional. It includes detailed sections and sub-sections, making it easier to follow and understand the key points of your research.