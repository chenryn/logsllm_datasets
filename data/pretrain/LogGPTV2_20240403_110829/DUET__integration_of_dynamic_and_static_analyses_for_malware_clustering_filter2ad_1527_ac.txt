IMPROVING ENSEMBLE WITH CLUS-
TER QUALITY MEASURES
Standard cluster ensemble approaches weight all clusters
equally, i.e., Mij is set to 1 if two data points, xi and xj, are
in the same cluster, regardless of cluster quality. However,
clustering is essentially an unsupervised learning process.
Without prior knowledge of the underlying data distribu-
tion, every clustering algorithm implicitly assumes certain
data models. When these assumptions are not supported
by the input data, the algorithms may produce erroneous
or meaningless clusters. In other words, because of the ex-
ploratory nature, most clustering algorithms will create clus-
ters even for data points that have little or no correlation,
eventually degrading the quality of ensemble clusters.
DUET overcomes this limitation and improves existing en-
semble algorithms by diﬀerentiating high-quality clusters that
have non-random structures from those that are created due
to the artifact of the clustering algorithms.
Ideally, when
grouping certain type of malware, we should give higher
weights to the clustering methods that are known to bet-
ter at handling that particular type. For instance, we want
to rely more on dynamic analysis when clustering heavily-
obfuscated malware programs. Unfortunately, such infor-
mation is not always readily available.
Instead, we use
cluster-quality as an indirect measure to enable the ensemble
method to bias towards high-quality clusters. Our objective
is to deﬁne quality measures that evaluate the “goodness” of
clustering by looking at the intra- and inter-cluster data cor-
relations, and incorporate them in the ensemble algorithms.
Intuitively, high-quality clusters should be compact and well
separated from other clusters, i.e., data points share a strong
bond. We measure cluster quality with following metrics:
• Cluster Cohesion (Co) determines how closely sam-
ples in a cluster are related. For a cluster C t
i , cohesion
is calculated as the average distance function between
two members: Co(C t
Note that, smaller Co indicates better cluster cohesion.
• Cluster Separation (Cs) measures how well a clus-
ter is separated from others. The separation between
two clusters, C t
j ) =
x∈Ct
d(x, y). Then, the separation of
|Ct
the cluster C t
all other clusters: Cs(C t
i ) = 1
k−1
where k is the number of clusters.
i is deﬁned as the average separation from
i , C t
j )
Pk
j=1;j(cid:4)=i Cs(C t
j , is deﬁned as Cs(C t
i and C t
;y∈Ct
i |∗|Ct
j|
x,y∈Ct
|Ct
i |∗(|Ct
i |−1)
P
i ) =
2
P
1
i , C t
i
j
i ;x 10 N -grams
4026 (71.29%)
4038 (71.51%)
4622 (81.85%)
4605 (81.55%)
5454 (96.58%)
< 10 N -grams
1621 (28.71%)
1609 (28.49%)
1025 (18.15%)
1042 (18.45%)
193 (3.41%)
1
1
sulting in a low recall value. In fact, this result is in line with
previous research6. Note also that the recall score is much
more stable than precision and coverage with little ﬂuctu-
ation across diﬀerent parameter values, which, to some de-
gree, supports our conjecture that the low recall is caused
by input data rather than clustering algorithms per se.
Finally, the coverage results in Fig. 2 show that both clus-
terings can create useful clusters for only 50–70% of samples.
In addition, there exists a natural trade-oﬀ between preci-
sion and coverage; an increase in precision is accompanied
by a decrease in coverage, and vice versa. The reason is
that, in order to achieve a higher precision, clustering algo-
rithms must avoid merging unrelated malware programs into
the same cluster. While this produces high-quality clusters
for some samples, many more remain uncovered in singular
clusters. For example, as M ind and Pmax both decrease
toward 0.1, precision improves because a small Pmax dic-
tates that each prototype includes only extremely close data
points within its range and a small M ind terminates the
merging process early to avoid combining unrelated classes.
Together, they ensure that only very similar samples are
grouped together, excluding a large portion of samples with
moderate resemblance. This leads to a sharp drop in cover-
age from 75 to 45% for DUET-D and below 30% for DUET-S .
7.3 Evaluation of Cluster Ensemble
we evaluate DUET’s cluster ensemble under two scenarios.
input to DUET are 8 base clusterings—
In each scenario,
two clusterings for each combination of 3-gram/4-gram and
static/dynamic with diﬀerent Pmax and M ind values.
In
the ﬁrst (best-case) scenario, Pmax and M ind are selected
to make each base clustering optimized for precision (thus
with low coverage) or coverage (thus with low precision).
Note that this is an ideal but not realistic scenario, since in
reality, we would not be able to determine which values of
Pmax and M ind would achieve optimal precision or cover-
age. As a result, the best-case scenario is used to evaluate
the optimal performance of cluster ensemble under an ideal
condition. Our second (random) scenario oﬀers a more re-
alistic evaluation by randomly choosing Pmax and M ind for
the 8 input clusters. Tables 7 lists the parameters of these
two scenarios. From the tables, we can see that the precision
for the best-case scenario’s clusterings ranges from 0.69 to
0.88, while the random scenario has a slightly lower range
of 0.57 to 0.83. Most recall values are surrounding 0.3 with
a couple of outliers (0.2 and 0.49). The coverage is usually
50–68% for dynamic clusterings and 54–74% for static clus-
terings. Next, we show that DUET can leverage the diverse
perspectives of input clustering to improve ﬁnal results.
Cluster ensemble based on the ball algorithm: Fig. 3
plots the precision and coverage7 of this ensemble approach
with diﬀerent threshold β (Section 5.2) between 0.2 to 0.9.
From the ﬁgure, one can see that the ball algorithm achieves
a precision value consistently higher than 0.8 and cover-
age close to 80%. Using a threshold of 0.5, the precision
for the best-case and random scenarios are 0.85 and 0.8,
respectively—both very close to the maximum of individual
clusterings. Furthermore, the coverages in these two cases
6
the clustering algorithms in [15] created 200 clusters for 3,935
samples, and the size of each cluster is comparable to DUET.
7
Due to space limitation and small variation of recall values across
diﬀerent parameters, we will plot only precision and coverage, and
present recall using its average and standard deviation.
n
o
s
i
i
c
e
r
P
0.95
0.9
0.85
0.8
0.75
0
n
o
s
i
i
c
e
r
P
0.9
0.8
0.7
0.6
0.5
0
Clustering Precision−− Static 3 gram
Clustering Recall − Static 3 gram
Clustering Coverage −− −− Static 3 gram
0.4
0.3
0.2
0.1
0
l
l
a
c
e
R
1
0.8
0.6
0.4
Mind
0.2
1
0.4
0.8
0.6
Pmax
0.7
0.6
0.5
0.4
e
g
a
r
e
v
o
C
0
0.2
0.5
Mind
1 0
0.5
Pmax
0.5
Mind
1 0
0.5
Pmax
Clustering Precision −− Behavioral 3 gram
Clustering Recall − Behaviroal 3 gram
Clustering Coverage −− Behavioral 3 gram
l
l
a
c
e
R
0.4
0.3
0.2
0.1
0
1
1 0
0.2
0.4
0.6
Pmax
0.8
1
e
g
a
r
e
v
o