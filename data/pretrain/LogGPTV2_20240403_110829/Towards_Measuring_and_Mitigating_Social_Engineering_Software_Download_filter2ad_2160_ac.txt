files distributed by a fake Flash Player upgrade at-
tack campaign (see Section 3) may all include the
word “Adobe” in the filename to convince the user
that the downloaded file is legitimate.
(2) File Size Similarity: Benign files that are identical
or variants (i.e., different versions) of the same soft-
ware are usually very close in size. Similarly, SE
campaigns typically infect victims with a variant of
the same malware family. While the malware file’s
size may vary due to polymorphism, the size dif-
ference is typically small, compared to the total file
size.
(3) URL Structure Similarity: A benign website that
serves software downloads will often host all of its
executable files at the same or very similar struc-
tured URLs.
In a similar way, SE campaigns of-
ten use malware distribution “kits” and go weeks or
even months before a noticeable change in the struc-
ture of their URL paths is observed. This is unlike
malicious URLs, which frequently change to avoid
blacklisting.
(4) Domain Name Similarity: While the domain names
used to distributed malware files belonging to the
same SE attack campaign may change, some cam-
paigns will reuse some keywords in their domains
that are meant to deceive the user. For instance,
the domains used in a Fake AV malware campaign
may contain the keyword “security” to convince the
user of its legitimacy. Also, download events related
to (different versions of) the same benign software
are often distributed via a handful of stable domain
names.
(5) Shared Domain Predecessor: SE attacks that share
a common node (or predecessor) in the download
path are often related. For instance, an SE mal-
ware campaign may exploit an ad network with weak
anti-abuse practices. Therefore, while the final do-
main in the download path from which the malware
is actually downloaded may change (e.g., to avoid
blacklisting), the malware download paths of differ-
ent users that fall victim to the same SE campaign
may share a node related to the abused ad network,
for example. On the other hand, in case of benign
downloads both the download and “attention grab-
bing” domain tend to be stable, as the main goal is
quality of service towards the end user.
(6) Shared Hosting: While domains involved in mal-
ware distribution tend to change frequently, SE mal-
ware campaigns often reuse (parts of) the same host-
ing infrastructure (e.g., some IPs). The intuition is
that hosting networks that tolerate abuse (knowingly
or otherwise) are a rare and costly resource. On the
other hand, domain names are significantly easier to
obtain and can be used as crash-and-burn resource
from the adversary. One the benign downloads side,
legitimate software distribution websites are usually
stable and do not change hosting very frequently, for
quality of service reasons.
(7) HTTP Response Header Similarity: The headers
in an HTTP response are the result of the installed
server-side software and configuration of the web
server. The set of response headers and their asso-
ciated values offer a lot of variation. However, most
of the web servers for a benign site tend to have
common configurations so they respond with simi-
lar headers. Also, some SE campaigns use the same
platform for their attacks and do not change their
server-side configurations even when they move to
new domains.
For each of the 13,762 downloads we compute a fea-
ture vector based on the features listed above, and cal-
culate the pairwise distance between these feature vec-
tors. We then apply an agglomerative hierarchical clus-
tering algorithm to the obtained distance matrix. Finally,
we cut the dendrogram output by the hierarchical clus-
tering algorithm to obtain the final clusters of download
events. To cut the dendrogram we chose a conserva-
tive cut height to error on the side of not grouping re-
lated downloads and significantly reduce the possibility
of grouping unrelated ones. This process produced 1,205
clusters, thus resulting in an order of magnitude reduc-
USENIX Association  
25th USENIX Security Symposium  779
7
tion in the number of items that require manual inspec-
tion. In the following section we explain how we ana-
lyzed and labeled these clusters.
4.4 Labeling SE Download Attacks
After clustering similar software download events, we
manually examine each cluster to distinguish between
those that are related SE download attack campaigns,
and clusters related to other types of software download
events, including benign downloads, malware downloads
triggered by drive-by downloads, and (benign or mali-
cious) software updates. This labeling process allows
us to focus our attention on studying SE download at-
tacks, and to exclude other types of benign and malicious
downloads (notice that because in this paper we are pri-
marily interested on SE attacks, we exclude non-SE mal-
ware attacks from our study).
To perform the cluster labeling, each cluster was man-
ually reviewed by randomly sampling 10% of the down-
load events grouped in the cluster, and then performing
a detailed manual analysis of the events in this sample
set. For small clusters (e.g. clusters with < 50 events)
we sampled a minimum of 5 download events. For clus-
ters containing less than 5 download traces, we reviewed
all of the events. As discussed earlier, our clustering pro-
cess uses a conservative cut height. The net effect is that
the clusters tend to be “pure,” thus greatly reducing the
possibility of errors during the cluster labeling process.
At the same time, some groups of download events that
are similar to each other may be split into smaller clus-
ters. However, this does not represent a significant prob-
lem for our labeling process. The only effect of having a
larger number of highly compact clusters is to create ad-
ditional manual work, since a random sample of events
from each cluster is manually analyzed.
In addition to manually reviewing the download paths
contained in the clusters, to assist our labeling we also
make use of antivirus (AV) labels for the downloaded ex-
ecutable files. To increase AV detections we “aged” the
downloads in our dataset for a period of two months, be-
fore scanning them with more than 40 AV engines using
virustotal.com. Notice that AV labels are mainly
used for confirmation purposes. The actual labeling of
SE attacks is performed via an extensive review of each
download path (i.e., sequence of pages/URLs visited to
arrive to the executable file download). If we suspect a
cluster is malicious (based on our manual analysis), hav-
ing one or more AV hits offers additional confirmation,
but is not required if we have strong evidence that the
download was triggered by an SE attack.
Updates: Even though the heuristics we described in
Section 4.2 filter out the vast majority of software up-
dates, our heuristics are quite conservative and therefore
some update events may still remain. To determine if
a download event is related to a (malware or benign) up-
date, we examine the length of the download path and the
time between requests. If the length of the total down-
load path is < 4 or the time between requests is < 1 sec-
ond, we consider the download event for detailed manual
review. In this case, we analyze the HTTP transactions
that precede the download by examining the content for
artifacts, such as text and clickable buttons, that are indi-
cators of human interaction. If none are found we label
the download as an update.
Drive-by: Next we look for drive-by download indica-
tors. To assist our labeling, we borrow some of the fea-
tures proposed in [30]. Notice that the labeling of drive-
by downloads is not a contribution of our paper. This
is only a means to an end. The novel contributions of
this paper are related to studying the characteristics of
SE download attacks.
To label drive-by attacks, we look for “exploitable
content,” such as pdf, flash, or java code on the path to
a malware download. Browser plugins and extensions
that process this type of content often expose vulnera-
bilities that are exploited by attackers. If we suspect the
download event under analysis is a drive-by, we reverse
engineer the content of the HTTP transactions that pre-
cede the suspected attack. This typically requires deob-
fuscating javascript and analyzing potentially malicious
javascript code. For instance, if we identify code that
checks for vulnerable versions of browser software and
plugins (an indication of “drive-by kits”), we label the
download as drive-by. We identify and label 26 drive-by
downloads.
Social Engineering:
If the cluster is not labeled as
update or drive-by, we further examine the download
events to determine if the they are due to SE attacks. For
this analysis, we inspect the content of all HTTP trans-
actions on the download path. This content was saved at
the time of the download and does not require revisiting
of the URLs on the download path. Because SE down-
loads are attacks on the user, they are initiated by a user-
browser interaction (e.g., clicking a link). Therefore, our
goal is to identify the page on the download path con-
taining the link likely clicked by the user that ultimately
initiated the executable file download. By manually re-
viewing the web content included in the download path,
we attempt to determine if deception or questionable per-
suasion tactics were used to trick the user into download-
ing the executable file (see Section 3). In case of positive
identification of such tactics, we label the cluster as so-
cial engineering; otherwise, we label it as “likely” be-
nign.
Notice that the analysis and labeling of SE download
attacks is mainly based on the identification of decep-
tion tactics to trick a user to download an executable file.
However, we also use AV scanning for confirmation pur-
780  25th USENIX Security Symposium 
USENIX Association
8
poses. By doing so, we found that the majority of the
clusters we label as social engineering contained one or
more of downloaded files that were labeled as malicious
by some AVs. This provides additional confirmation of
our labeling of SE download attacks.
Overall, among 1,205 clusters in our dataset, we la-
beled 136 clusters as social engineering. In aggregate,
these clusters included a total of of 2,004 SE download
attacks. In Section 3 we analyzed these SE download at-
tacks and developed a categorization system that allows
us to organize these attacks based on the deception and
persuasion tactics used to attack the user. In the next sec-
tion, we measure the popularity of these tactics based on
the data we collected.
5 Measuring SE Download Attacks
In this section we measure the popularity of the tac-
tics attackers employ to gain the user’s attention and
of the deception and persuasion techniques that con-
vince users to (unknowingly) download malicious and
unwanted software. In addition, we measure properties
of ad-based SE download attacks, which can be used to
inform the development of effective defenses against SE
attacks that leverage ad campaigns.
5.1 Popularity of SE Download Attacks
Table 1 shows the number and percentage of SE down-
load attacks for each tactic employed by the attackers to
gain the user’s attention. Over 80% of the SE attacks we
observed used ads displayed on websites visited by the
user. An additional 7% employed both search and ad,
whereby the user first queries a search engine and is then
presented with targeted ads based on the search terms.
The popularity of ads in SE download attacks is likely
due to the fact that they are a very efficient way for at-
tackers to reach a large audience, thus maximizing the
number of potential victims. Furthermore, well-crafted
targeted ads are naturally highly effective at attracting a
user’s attention.
Table 1: Popularity of SE techniques for gaining atten-
tion.
User’s Attention
Ad
Search+Ad
Search
Web Post
Total
1,616
146
127
115
Percentage
80.64%
7.29%
6.34%
5.74%
Gaining the user’s attention is not sufficient for an SE
download attack to succeed. A user must also be tricked
into actually “following the lead” and downloading the
malicious or unwanted software. Table 2 shows the pop-
ularity of the deception and persuasion techniques we ob-
served in our dataset of SE download attacks. The most
popular combination of deception and persuasion tech-
niques is repackage+entice, making up over 48% of the
observations. In most of these cases, the user is offered
some type of “free” software of interest (e.g., a game or
utility). Unfortunately, while the free software itself may
not be malicious, it is often bundled with malicious ap-
plications such as adware or PUPs.
Table 2: Popularity of SE techniques for tricking the user.
Trick
Repackage+Entice
Invent+Impersonate+Alarm
Invent+Impersonate+Comply
Repackage+Decoy
Impersonate+Decoy
Impersonate+Entice+Decoy
Invent+Comply
Impersonate+Alarm
Total
972
434
384
155
46
12
4
1
Percentage
48.50%
21.66%
19.16%
7.74%
2.30%
0.60%
0.20%
0.05%
The next two most popular combinations of deception
and persuasion tactics are invent+impersonate+alarm
and invent+impersonate+comply, comprising 22% and
19% of the SE downloads we observed. An example of
invent+impersonate+alarm is a Fake Java update attack,
whereby the user is shown an ad that states “WARN-
ING!!! Your Java Version is Outdated and has Security
Risks, Please Update Now!” and uses images (e.g., logos
or icons) related to Java (notice that this and all other ex-
amples we use throughout the paper represent real-world
cases of successful SE attacks from our dataset). Ads
like this are typically presented to users while they are
visiting legitimate websites. In this example, the attacker
is inventing the scenario that the user’s Java VM is out-
of-date, alarming them with “WARNING!!!” displayed
in a pop-up ad, and then impersonating a Java update
that must be installed to resolve the issue. Notice that
this is different from repackage+entice, in that the real
Java software update is never delivered (only the mal-
ware is). Furthermore, the attacker leverages alarming
messages about a well-known software to more aggres-
sively “push” the user to download and install malicious
software.
The difference between invent+impersonate+alarm
and invent+impersonate+comply is in the persuasion
component; i.e., alarm vs. comply. Alarm leverages
fear (e.g., the computer may be compromised) to compel
the user to download and install malicious software. On
the other hand, comply leverages a pretend requirement
necessary to complete a desired user task. For instance,
a user may be presented with an ad on a video stream-
ing website that says “Please Install Flash Player Pro To
Continue. Top Video Sites Require The Latest Adobe