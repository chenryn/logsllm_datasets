)
7
3
(
Y
B
R
H
)
6
(
A
B
)
2
1
1
(
S
E
)
7
1
(
)
0
1
(
L
A
S
I
)
5
(
T
M
)
3
1
(
Y
C
)
9
8
1
(
T
)
3
1
(
)
4
2
(
K
M
E
E
I
)
5
1
(
T
L
)
9
(
D
M
)
0
7
(
G
B
)
6
5
(
U
H
)
7
0
4
(
L
N
)
4
3
(
I
)
0
2
1
(
S
L
P
)
6
3
(
K
S
)
0
3
7
(
E
D
)
4
3
(
O
R
)
7
4
1
(
T
A
)
8
(
D
A
)
4
1
2
(
Z
C
)
4
8
(
I
F
)
8
8
(
)
7
3
(
E
I
U
L
)
0
2
(
V
L
)
4
6
4
(
B
G
)
1
7
3
(
)
1
7
1
(
U
R
E
B
)
2
6
(
R
G
)
2
7
1
(
)
2
7
5
(
)
1
9
1
(
A
U
R
F
H
C
)
8
9
(
K
D
)
7
0
1
(
O
N
)
s
m
(
T
T
R
 400
 300
 200
 100
 0
)
s
m
(
T
T
R
 400
 300
 200
 100
 0
country code (# of VPs)
(b) L-Root
Fig. 5. Median RTT (quartiles as error bars) for countries with at least 5 VPs (number
of VPs per country is given between parenthesis). Letters at top indicate continents.
mal (as deﬁned by C’s 8 sites). We then add U.S. west (LAX) and east (JFK)
coasts, and then Frankfurt (FRA), each pulling the distribution closer to opti-
mal, particularly in the tail. With the four-site combination, we virtually reach
C’s optimal possible performance. This data shows that geographically distributed
anycast sites can improve latency for the most distant users. Wide geographic
distribution helps because mature networks become well-connected, with latency
converging down to the speed-of-light (in ﬁber) limit.
Although both network topology and routing policies mean network and
geographic proximity may diverge [36], dispersion in geography correlates with
network dispersion.
Finally, comparing these ﬁgures shows that site location matters more than
number of sites. Four ideally positioned sites do well (the CDG, LAX, JFK,
and FRA line in Fig. 4b is leftmost), while four poorly chose sites are far from
optimal (compare the LAX, ORD, IAD, JFK line against optimal in Fig. 4a).
3.4 How Much Do “Many Sites” Help?
A key result of Fig. 3 is that the four letters provide roughly similar latency across
most VPs, in spite of an 18× more sites (C- and L-Root show similar median
latencies, 32 ms vs. 30 ms). While many sites does not aﬀect median latency,
more sites help the tail of the distribution, from 70th to 90th percentiles. To
evaluate this tail, we next examine each country with at least 5 VPs. (We omit
countries with fewer to avoid potential bias from bad “last miles” [3].)
196
R. de Oliveira Schmidt et al.
With countries grouped by continent, Fig. 5 reports the median latency for
C- (Fig. 5a) and L-Root (Fig. 5b). Latency is highest for countries in Africa and
Asia for both roots, and also in Oceania and South America for C-Root. We
expect high latency for C-Root in these areas because its anycast sites are only
in Europe and North America. With global anycast sites, high latency for L-Root
is surprising. Using our 100 ms threshold for high latency (Sect. 3.1), we observe
that C has about 38 countries above that threshold, while L has only about 21.
L’s many additional sites improve latency, but not everywhere. Somewhat more
troubling is that L shows high latency for several European countries (Portugal,
PT; Belarus, BY; Croatia, HR; Bosnia, BA; and Spain, ES). Even with European
sites, routing policies send traﬃc from these countries to long distances.
When we look at countries with highest latency in Fig. 5, L’s many sites do
improve some VPs in each country, as shown by the lower quartiles. However, the
high median shows that these improvements are not even across all VPs in these
countries. This wide variation suggests interconnection inside these countries can
be poor, resulting in good performance for those VPs in ISPs that have a local
anycast site, while VPs in other ISPs have to travel long distances. For example,
from all 20 VPs in the Philippines (PH), 7 VPs are able to reach their optimal
L sites located in the Philippines itself, with average RTT of 18 ms. The other
13 VPs, however, reach L sites in U.S. and Australia, seeing average RTT of
56 ms. None of the “unlucky” 13 VPs are within the same ASes than the other 7
“lucky” ones. We therefore conclude that routing policies can drastically reduce
the beneﬁts of many sites.
3.5 Do Local Anycast Policies Hurt Performance?
Anycast sites are often deployed with a local routing policy, where the site is
only available to the hosting AS, or perhaps also directly adjacent ASes. An
important question in anycast deployments is how much these policies impact
on performance. The anycast deployments we studied allow us to answer if policy
routing matters. The similar distributions of latency among the four letters we
study (Fig. 3) show that policy does not matter much. C- and L-Root place no
restriction on routing, while about half of F- and most of K-Root sites are local
in our initial study (Table 1). We also observe K after they changed almost all
sites to global (NK in Table 1).
We study mishits to get a more detailed look into this question. In Table 1,
mishits are VPs that do not hit the optimal site. We have examined mishits based
on those that go to local or global sites in detail in our technical report [35]. Due
to space, we summarize those ﬁndings here and refer that report for the detailed
analysis. We see that a fair number of VPs are prevented from accessing their
nearest site because they instead go to a global site: this case accounts for about
58% of F-root VPs that mishit, and 42% of K-Root mishits. Thus, restrictive
local routing does add latency; and relaxing this policy could improve median
latency from 37 ms to 19 ms in F-Root, and from 43 ms to 25 ms in K-root.
K-Root provided a natural experiment to evaluate if relaxing routing helps.
After our initial measurements of K-Root in 2015-11, K changed all but one
Anycast Latency: How Many Sites Are Enough?
197
site to global routing; our NK dataset re-examines K-Root in 2016-04, after this
policy change. Comparing K and NK in Table 1, we see only modest changes
in latency: 2 ms drop in median latency, and no real change in the fraction
of mishits. From discussion with the K-Root operators, we learned that local
routing policies were inconsistently applied (routing limits were often ignored by
peers), thus routing policies can be dominated by routing bugs.
Our main conclusion is that careful examination and debugging routing polices
of local sites can make a large diﬀerence in performance. Bellis’ tuning of F-Root
anycast routing showed that debugging can improve performance [6].
3.6 How Many Sites?
Given this analysis, how many sites are needed for reasonable latency? Section 3.1
shows minimal diﬀerence for median latency from 8 to 144 sites, suggesting 8
sites are reasonable based on C-Root measurements from RIPE Atlas. If we
consider two sites per six continents for some redundancy, and account for under-
representation of VPs in some areas, we suggest twelve sites can provide
reasonable latency. We caution that this number is only a rough suggestion—
by no means do we suggest that 12 is perfect but 11 or 13 is horrible. This
count considers only latency; we recognize more sites may be needed for many
other reasons (for example, DDoS-defense and many dimensions of diversity),
and it applies to an individual IP anycast service, not DNS or a CDN, which
often employ multiple, independent IP anycast services. It assumes geographic
distribution (Sect. 3.3) and that routing problems allow use of geographically
close sites (Sects. 3.4 and 3.5), and eﬀective DNS caching (Sect. 3.1).
4 Related Work
The DNS Root has been extensively studied in the past. CAIDA’s measurement
infrastructure skitter [11] has enabled several early studies on DNS perfor-
mance [8,9,21,25]. In 2004, Pang et al. [29] combined probing and log analy-
sis to show that only few DNS servers were being used by a large fraction of
users. Following works studied the performance of DNS, focusing on latency
between clients and servers [5,17,34]. DNS CHAOS has been used to study client-
server aﬃnity [7,34]. Liu et al. [27] used clients geolocation to estimate RTT,
and others evaluated the eﬀect of route changes on the anycast service [4,10].
Liang et al. [26] used open resolvers to measure RTT from the DNS Root and
major gTLDs. Bellis [6] carried out a comprehensive assessment of latency in
F Root’s anycast, ﬁxing faulty route announcements to improve performance.
Other work [14,24] used large and long-term datasets to show that the expan-
sion of the anycast infrastructure improved overall performance of the Root DNS.
Finally, Calder et al. [13] studied the choice of anycast or LDNS for redirection
to CDN services.
Our work diﬀers from these prior studies in methodology and analysis. We
build on prior studies, but deﬁne optimal possible performance and measure
198
R. de Oliveira Schmidt et al.
it with probes to unicast addresses of all sites. This new methodology allows
our analysis to go beyond measurements of what happens to statements about
what could happen, allowing the ﬁrst answers about eﬀects of routing policy.
In addition, this methodology allows us to estimate performance of alternate
anycast infrastructures that are subsets of current deployments, enabling strong
conclusions about the eﬀect of numbers of sites on latency.
Furthermore, complementing our work are studies that enumerate and char-
acterize content delivery services that use IP anycast. To exemplify some, Calder
et al. [12] used EDNS client subnet (ECS) and latency measurements to char-
acterize Google’s serving infrastructure. Streibelt et al. [37] also used ECS to
study Google’s, Edgecast’s and CacheFly’s ancyast user to server mapping. Fan
et al. [19] combined DNS queries and traceroutes to study the anycast at TLDs.
Cicalese et al. [16] used latency measurements to geolocate anycast services, and
later characterize IPv4 anycast adoption [15]. Fan et al. [20] combined ECS and
open resolvers to measure Google’s and Akamai’s front-ends. Finally, Akhtar
et al. [2] proposed a statistical approach for comparing CDNs performance.
5 Conclusions
We studied four real-world anycast deployments (the C-, F-, K- and L-Root DNS
nameservers) with 7,900 VPs (RIPE Atlas probes) to systematically explore
the relationship between IP anycast and latency. Unique to our collection is
the combination of latency to each VP’s current site, and to all sites, allowing
evaluation of optimal possible latency. We collected new data for each of the
measured services in 2015 and revisited K-Root in 2016 to evaluate changes
in its routing policies. Our methodology opens up future directions, including
assessment of anycast for resilience to Denial-of-Service and load balancing in
addition to latency reduction.
Our new ability to compare actual to optimal latency allows us untangle
several aspects of our central question: how many anycast sites are “enough”.
Our data shows similar median performance (about 30 ms) from 8 to 144 sites,
suggesting that as few as twelve sites can provide reasonable latency,
provided they are geographically distributed, have good local interconnectivity,
and DNS caching is eﬀective.
Acknowledgments. We thank Geoﬀ Huston (APNIC), George Michaelson (APNIC),
Ray Bellis (ISC),Cristian Hesselman (SIDN Labs), Benno Overeinder (NLnet Labs) and
Jaap Akkerhuis (NLnet Labs), Duane Wessels (Verisign), Paul Vixie (Farsight), Romeo
Zwart (RIPE NCC), Anand Buddhdev (RIPE NCC), and operators from C Root for
their technical feedback.
This research uses measurements from RIPE Atlas, operated by RIPE NCC.
Ricardo Schmidt’s work is in the context of SAND (Self-managing Anycast Net-
works for the DNS: http://www.sand-project.nl) and DAS (DNS Anycast Security:
http://www.das-project.nl) projects, sponsored by SIDN, NLnet Labs and SURFnet.
John Heidemann’s work is partially sponsored by the U.S. Dept. of Homeland Secu-
rity (DHS) Science and Technology Directorate, HSARPA, Cyber Security Division,
Anycast Latency: How Many Sites Are Enough?
199
via SPAWAR Systems Center Paciﬁc under Contract No. N66001-13-C-3001, and via
BAA 11-01-RIKA and Air Force Research Laboratory, Information Directorate under
agreement numbers FA8750-12-2-0344 and FA8750-15-2-0224. The U.S. Government is
authorized to make reprints for Governmental purposes notwithstanding any copyright.
The views contained herein are those of the authors and do not necessarily represent
those of DHS or the U.S. Government.
References
1. Abley, J., Lindqvist, K.E.: Operation of Anycast Services. RFC 4786 (2006)
2. Akhtar, Z., Hussain, A., Katz-Bassett, E., Govindan, R.: DBit: assessing statisti-
cally signiﬁcant diﬀerences in CDN performance. In: IFIP TMA (2016)
3. Bajpai, V., Eravuchira, S.J., Sch¨onw¨alder, J.: Lessons learned from using the RIPE
atlas platform for measurement research. ACM CCR 45(3), 35–42 (2015)
4. Ballani, H., Francis, P.: Towards a global IP anycast service. In: ACM SIGCOMM,
pp. 301–312 (2005)
5. Ballani, H., Francis, P., Ratnasamy, S.: A measurement-based deployment proposal
for IP anycast. In: ACM IMC, pp. 231–244 (2006)
6. Bellis, R.: Researching F-root Anycast Placement Using RIPE Atlas (2015).
https://labs.ripe.net/
7. Boothe, P., Bush, R.: Anycast Measurements Used to Highlight Routing Instabil-
ities. NANOG 34 (2005)
8. Brownlee, N., Claﬀy, K.C., Nemeth, E.: DNS Root/gTLD performance measure-
ment. In: USENIX LISA, pp. 241–255 (2001)
9. Brownlee, N., Ziedins, I.: Response time distributions for global name servers. In:
PAM (2002)
10. Bush, R.: DNS anycast stability: some initial results. In: CAIDA/WIDE Workshop
(2005)
11. CAIDA. Skitter. http://www.caida.org/tools/measurement/skitter/
12. Calder, M., Fan, X., Hu, Z., Katz-Bassett, E., Heidemann, J., Govindan, R.: Map-
ping the expansion of Google’s serving infrastructure. In: ACM IMC, pp. 313–326
(2013)
13. Calder, M., Flavel, A., Katz-Bassett, E., Mahajan, R., Padhye, J.: Analyzing the
performance of an anycast CDN. In: ACM IMC, pp. 531–537 (2015)
14. Castro, S., Wessels, D., Fomenkov, M., Claﬀy, K.: A day at the root of the internet.
ACM CCR 38(5), 41–46 (2008)
15. Cicalese, D., Aug´e, J., Joumblatt, D., Friedman, T., Rossi, D.: Characterizing IPv4
anycast adoption and deployment. In: ACM CoNEXT (2015)
16. Cicalese, D., Joumblatt, D., Rossi, D., Buob, M.-O., Aug´e, J., Friedman, T.: A
ﬁstful of pings: accurate and lightweight anycast enummeration and geolocation.
In: IEEE INFOCOM, pp. 2776–2784 (2015)
17. Colitti, L.: Eﬀect of anycast on K-root. In: 1st DNS-OARC Workshop (2005)
18. DNS Root Servers. http://www.root-servers.org/
19. Fan, X., Heidemann, J., Govindan, R.: Evaluating anycast in the domain name
system. In: IEEE INFOCOM, pp. 1681–1689 (2013)
20. Fan, X., Katz-Bassett, E., Heidemann, J.: Assessing aﬃnity between users
and CDN sites. In: Steiner, M., Barlet-Ros, P., Bonaventure, O. (eds.) TMA
2015. LNCS, vol. 9053, pp. 95–110. Springer, Heidelberg (2015). doi:10.1007/
978-3-319-17172-2 7
200
R. de Oliveira Schmidt et al.
21. Fomenkov, M., Claﬀy, K.C., Huﬀaker, B., Moore, D.: Macroscopic internet topol-
ogy and performance measurements from the DNS root name servers. In: USENIX
LISA, pp. 231–240 (2001)
22. Google Public DNS. https://developers.google.com/speed/public-dns/
23. Kuipers, J.H.: Analysing the K-root anycast infrastructure (2015). https://labs.
ripe.net/
24. Lee, B.-S., Tan, Y.S., Sekiya, Y., Narishige, A., Date, S.: Availability and eﬀective-
ness of root DNS servers: a long term study. In: IFIP/IEEE NOMS, pp. 862–865
(2010)
25. Lee, T., Huﬀaker, B., Fomenkov, M., Claﬀy, K.C.: On the problem of optimization
of DNS root servers’ placement. In: PAM (2003)
26. Liang, J., Jiang, J., Duan, H., Li, K., Wu, J.: Measuring query latency of top level
DNS servers. In: Roughan, M., Chang, R. (eds.) PAM 2013. LNCS, vol. 7799, pp.
145–154. Springer, Heidelberg (2013). doi:10.1007/978-3-642-36516-4 15
27. Liu, Z., Huﬀaker, B., Fomenkov, M., Brownlee, N., Claﬀy, K.C.: Two days in the
life of the DNS anycast root servers. In: Uhlig, S., Papagiannaki, K., Bonaventure,
O. (eds.) PAM 2007. LNCS, vol. 4427, pp. 125–134. Springer, Heidelberg (2007).
doi:10.1007/978-3-540-71617-4 13
28. Palsson, B., Kumar, P., Jaﬀeralli, S., Kahn, Z.A.: TCP over IP anycast - pipe
dream or reality? (2015). https://engineering.linkedin.com/
29. Pang, J., Hendricks, J., Akella, A., Prisco, R.D., Maggs, B., Seshan, S.: Availability,
usage, and deployment characteristics of the domain name server. In: ACM IMC,
pp. 1–14 (2004)
30. Partridge, C., Mendez, T., Milliken, W.: Host Anycasting Service. RFC 1546 (1993)
31. RIPE NCC. Dnsmon (2015). https://atlas.ripe.net/dnsmon/
32. RIPE NCC Staﬀ: RIPE Atlas: a global Internet measurement network. Internet
Protocol J. 18(3), 2–26 (2015)
33. Rootops. Events of 2015–11-30. Technical report, Root Server Operators (2015)
34. Sarat, S., Pappas, V., Terzis, A.: On the use of anycast in DNS. In: ICCCN, pp.
71–78 (2006)
35. Schmidt, R.d.O., Heidemann, J., Kuipers, J.H.: Anycast latency: how many sites
are enough? Technical report ISI-TR-2016-708, USC-ISI, May 2016
36. Spring, N., Mahajan, R., Anderson, T.: Quantifying the causes of path inﬂation.
In: ACM SIGCOMM, pp. 113–124 (2003)
37. Streibelt, F., B¨ottger, J., Chatzis, N., Smaragdakis, G., Feldman, A.: Exploring
EDNS-client-subnet adopters in your free time. In: ACM IMC, pp. 305–312 (2013)
38. Toonk, A.: How OpenDNS achieves high availability with anycast routing (2013).
https://labs.opendns.com/
39. Woolf, S., Conrad, D.: Requirements for a Mechanism Identifying a Name Server
Instance. RFC 4892 (2007)