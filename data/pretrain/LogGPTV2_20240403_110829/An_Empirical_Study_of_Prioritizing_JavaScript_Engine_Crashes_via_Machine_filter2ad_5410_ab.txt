was assigned with a CVE or rewarded from Google [19]. For Mozilla,
we leveraged a list of known security vulnerabilities and collected
CVEs of SpiderMonkey [49]. We then searched PoC code for the
collected CVEs from patch commits, bug reports, and websites that
have been archiving exploits [26, 55].
For non-security bugs, we collected test cases that did not have
security-related labels. For each Chakra bug, we read its correspond-
ing GitHub issue and selected crashing bugs triggering non-security
bugs. In the case of V8 and SpiderMonkey, we collected bug reports
with PoC code, but without assigned CVEs or rewards.
Building JS engine binaries. There is a remaining challenge. It
is to find the correct version of a target JS engine for each PoC
triggering a security or non-security bug. Our objective is to prepare
the engine binary which generates a crash-dump at the time the
bug is reported. Unfortunately, we observed that a large number of
bug reports missed information on their target JS engine version
where the bugs were found. For each bug report without an explicit
version, when we knew its patch commit or patch date, we retrieved
the JS engine repository for which the last commit was ahead of the
patch date. When we could not even obtain the patch information,
we restored the repository for which the latest commit date was
ahead of the date when the report was filed. For instance, if the
bug report was filed on January 15, 2019, we retrieved the latest
repository whose commit occurred prior to January 15, 2019.
Given a labeled PoC code snippet, we prepared JS engine binaries
to produce crash-dumps from which CRScope extracts features.
Note that each JS engine supports both ia32 and x64 architectures
except for Chakra; Chakra only supports x64 architecture. Each JS
engine also provides debug and release modes. Therefore, each test
for a target JS engine can have up to four corresponding binaries.
Table 1 summarizes our ground truth dataset. We collected a
total of 766 crash instances by running 339 PoCs on V8, Spider-
Monkey, and Chakra engines for each architecture and each mode.
165 security bugs (69 in Chakra, 50 in V8, and 46 in SpiderMonkey
Session 9: FuzzingAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand648Table 2: List of features used in CRScope
Feature Name
JS engine name
Architecture type
Compile mode
Signal type
Crash type
Crashing instruction
Crashing function
Backtrace
Preprocessed
Feature Extraction
✗
✗
✗
✗
✗
✗
✓
✓
LabelEncoder
LabelEncoder
LabelEncoder
LabelEncoder
LabelEncoder
TfidfVectorizer, CountVectorizer
TfidfVectorizer, CountVectorizer
TfidfVectorizer, CountVectorizer
PoCs) caused 368 JS engine binaries to crash. The others are caused
by 174 non-security bugs. Whereas the number of V8 PoCs is the
smallest, the number of Chakra crash instances is smaller than the
others. It is because the number of Chakra engine binaries is smaller
than others due to not supporting the ia32 architecture.
Difficulties in building the ground truth. We emphasize that
preparing the ground truth data for major JS engines is an arduous
task. There was no previous study or public project involving the
classification of JS engine crashes. There are also a limited number
of CVEs with publicly available PoC code, thus hindering the col-
lection of training instances. Furthermore, to collect crashes from
the binary when the report was filed, we carefully read every bug
report and validated whether the corresponding PoC code worked.
We compiled 727 binary instances, which accounted for 1200GB
(1.2TB). This task was also challenging because compiling old ver-
sions of JS engines often requires the usage of deprecated libraries
and outdated OS supports. In some instances, building the old ver-
sions of JS engines required investigation and non-trivial engineer-
ing costs because the method of building each JS engine has changed
over time. If a rolled back repository contained a compilation error,
we had to manually fix this error by looking through commit logs.
Compiling 727 binaries took a long time and required significant
computing resources. We did not intend to miss any bugs with PoC
code resulting in a target JS engine crash. Three graduate students
invested nine months in identifying and validating 339 bugs and
their bug reports from Chrome, Mozilla, and Chakra BTSs. To sup-
port open science and further research, we release the ground truth
data and source code at https://github.com/WSP-LAB/CRScope.
4 METHODOLOGY
This section describes how we design and implement CRScope.
Section 4.1 explains each feature that CRScope extracts from a
crash-dump and their preprocessing. Section 4.2 describes six clas-
sification models that we evaluate to select a proper model for
CRScope.
4.1 Extracting Features
Given a crash-dump file, CRScope extracts eight different feature
types. Table 2 summarizes the extracted features. The first and
second columns represent their types and whether CRScope pre-
processed them, respectively. The last column indicates the method
of vectorizing each feature. We applied three different vectorizing
methods: LabelEncoder, CountVectorizer, and TfidfVectorizer [4].
LabelEncoder encodes a feature value into a number which varies
from zero to the number of its unique instances minus one. For
the feature of JS engine name, Chakra, V8, and SpiderMonkey are
encoded into 0, 1, and 2, respectively.
CountVectorizer converts a feature consisting of various elements
into the vector of element frequencies over the dataset. Note that
crashing instruction feature has many vocabularies, e.g., opcode and
operand. After conducting the CountVectorizer extraction, the crash-
ing instruction feature becomes a sparse vector with the counts for
each vocabulary contained in the crashing instruction.
TfidfVectorizer operates similar to CountVectorizer except for
being able to filter out elements with frequencies that are too low
or too high, by computing the Term Frequency - Inverse Document
Frequency (TF-IDF) value for each element [30].
Before applying the CountVectorizer and TfidfVectorizer meth-
ods, we extracted n-gram tokens from each crashing instruction,
crashing function, and backtrace while varying n from one to five.
We encoded these tokens into feature vectors, thus capturing the
relationships between operations, operands, and call sequences.
We applied LabelEncoder to encode the top five features in Table 2
and vectorized the bottom three features using TfidfVectorizer and
CountVectorizer. All of the encoded vectors above are concatenated
into one large vector for each crash instance. The lengths of the
vectors generated using the Chakra, V8, and SpiderMonkey datasets
are approximately 18,000, 24,000, and 30,000, respectively. However,
significant features are only subsets of them, and an unnecessarily
large size of vector dimension hinders the correct classification of
crash instances [68].
We reduced the feature vector dimension by conducting two
preprocessing steps. Hall et al. stated that a good feature subset
should contain features uncorrelated with (not predictive of) each
other [25], which guided our correlation-based feature selection
process. First, we computed a Pearson correlation coefficient [3]
for every feature pair. For each feature, we removed highly corre-
lated features the coefficients of which were over 0.9 because these
features are redundant in performing classifications [25]. We then
selected 100 features after conducting SelectKBest from [4], which
internally performs a chi-square test. It computes chi-squared sta-
tistics between each feature and class and eliminates features that
are likely to be irrelevant for classification.
The followings describe the details of our extraction method for
each feature type.
JS engine name. Each browser vendor could have subtle differ-
ences in the criteria for determining a security bug. We thus provide
a target engine name to a model and let the model pick this feature
or not while training. Its feature value is among Chakra, V8, and
SpiderMonkey.
Architecture type and compile mode. A JS engine has up to
four different binaries for the supporting architectures and com-
pile modes (See Section 3). Since crash-dumps and other extracted
features differ among these four different binaries, we feed a target
model with ia32 and x64 for the architecture type as well as release
and debug for the compile mode.
Signal type. When a process is abnormally terminated, it receives
a terminal signal, then crashes. This terminal signal depends on the
root cause of the crash. For instance, SIGSEGV is a segmentation
violation signal which informs that a process attempted to access a
Session 9: FuzzingAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand649not-allowed memory region. SIGABRT is an abort signal indicat-
ing that the process is unable to run further because of asserting
statements or other causes. The feature value is either SIGSEGV,
SIGILL, SIGABRT, or SIGFPE.
Crash type. Exploitable is a gdb plugin that determines the ex-
ploitability of a crash-dump file. It also classifies a crash type. There
exist 22 different crash types such as ReturnAv, BranchAv, DestAvN-
earNull, and BadInstruction. Each crash type represents an instance
where access violations occurred or means that the current crashing
instruction is illegal. Although we use Exploitable, it is trivial to
compute this label by analyzing a crash-dump without it.
Crashing instruction. CRScope extracts the instruction at which
the crash occurred. This instruction provides useful information
about the crash cause. If a crashing instruction is a read or write
operation, the main cause of the crash could be due to the reading
or writing of an invalid memory, respectively.
An instruction consists of an opcode and its operands. CRScope
extracts n-grams (from one to five) from this instruction. For ex-
ample, from the mov eax, ebx instruction, CRScope extracts mov,
eax, ebx, mov eax, eax ebx, and mov eax ebx. Each n-gram token
becomes a feature element, the value of which is its frequency over
the entire dataset. That is, CRScope extracts a vector for which the
element corresponds to an n-gram token frequency for each of the
TfidfVectorizer and CountVectorizer algorithms.
Crashing function. The function at which the crash occurred is
the top frame of the stack trace. It is the actual function where the
crash occurred. Schroter et al. found that 40% of bugs were fixed
in these top functions [54], and Wu et al. located 50.6% of crashing
faults by examining the top function [64]. We thus assume that a
crashing function, the top of a stack trace, is closely correlated with
crashes. We feed this feature into the CRScope classifiers. CRScope
also preprocesses this feature by transforming a function name into
a sequence of its namespace, class, and member function name,
excluding arguments and templates. It then extracts n-gram (from
one to five) tokens from this sequence. Our intention behind this
scheme is to capture the semantic that appeared in word tokens of
function names.
Backtrace. A backtrace (also called stack trace) is an ordered list
of callees or active stack frames when the crash occurs. This stack
trace is one of the most useful pieces of information when de-
velopers want to identify the root cause of the crash by tracking
execution flows. In practice, many software vendors send these
backtraces from client crashes to triage the problems. Schroter et al.
demonstrated that bug reports with backtraces had got resolved sig-
nificantly sooner than other bug reports [54]. The intuition behind
collecting this feature is that when a crash occurs, any callee in the
stack trace is highly correlated with security bugs. We expect that
the function name itself contains a semantic of its internal logic,
which hints at a model in performing the classification.
CRScope preprocesses the extracted backtrace features. For a
given backtrace, CRScope only extracts the sequence of callees,
each of which only contains a function name excluding other parts
such as namespaces and classes. It then extracts n-gram (from one
to five) tokens, each of which becomes a callee sequence with length
n.
Table 3: Hyperparameters of each model explored via grid
search
Models
MNB
DTC
RFC
SVC
LR
MLP
Parameters
alpha
max_features
splitter
criterion
max_depth
max_features
n_estimators
warm_start
criteriton
max_depth
penalty
loss
dual
tol
C
max_iter
multi_class
penalty
dual
tol
C
solver
max_iter
warm_start
hidden_layer_sizes
activation
solver
alpha
learning_rate
tol
epsilon
Values
0.01, 0.1, 1, 10, 100
sqrt, auto, log2, None
best, random
gini, entropy
None, 2, 5, 10, 20, 50, 100, 500, 1000
auto, sqrt, log2, None
10, 50, 100
False, True
gini, entropy
2, 5, 10, 20, 50, 100
l1, l2
hinge, squared_hinge
True, False
1e-4, 1e-6, 1e-8, 1e-10, 1e-20
1, 10, 100, 1000, 10000
100, 1000, 10000, 100000
ovr, crammer_singer
l1, l2
True, False
1e-4, 1e-6, 1e-8, 1e-10
1, 10, 100, 1000
liblinear, newton-cg, lbfgs, sag
100, 1000, 10000, 100000
False, True
(100, ), (100, 25, )
identity, logistic, tanh, relu
lbfgs, sgd, adam
1e-4, 1e-2, 1, 100
constant, invscaling, adaptive
1e-4, 1e-6, 1e-8, 1e-10, 1e-20
1e-1, 1e-2, 1e-3, 1e-5, 1e-10
4.2 Classification models
Using features extracted as described in Section 4.1, we evaluated
Multinomial Naive Bayes (MNB), Decision Tree Classifier (DTC),
Random Forest Classifier (RFC), Linear Support Vector Classifica-
tion (SVC), Logistic Regression (LR), and Multi-layer Perceptron
Classifier (MLP) machine learning models in the scikit-learn pack-
age [4]. Our evaluation goal is to find a model with the best perfor-
mance for CRScope in classifying security bugs.
Each model requires different hyperparameters, thus making
hyperparameter tuning imperative because they affect evaluation
results. We thus conducted the grid search in [4] to find the best
hyperparameters for the models referred to above. The employed
grid search is an accuracy-guided exhaustive search technique,
thus selecting the set of hyperparameters that produces the highest
accuracy.
Table 3 shows hyperparameters that we explored by conducting
the grid search. For each model, hyperparameters in the second
column are determined by exhaustively searching over specified
values in the third column. We used scikit-learn default values
for other parameters. For the details of each hyperparameter, we
Session 9: FuzzingAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand650Table 4: Example of the grid search for DTC model
(1)
log2
log2
log2
log2
log2
auto
sqrt
log2
log2
log2
log2
log2
log2
log2
log2
None
log2