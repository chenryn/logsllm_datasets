# User Interface Defect Detection by Hesitation Analysis

**Authors:**
- Robert W. Reeder
- Roy A. Maxion

**Affiliation:**
Dependable Systems Laboratory  
Computer Science Department  
Carnegie Mellon University  
Pittsburgh, Pennsylvania 15213, USA

**Contact:**
- PI: [EMAIL]
- PI: [EMAIL]

## Abstract

Delays and errors are common consequences of user interface difficulties, which can lead to severe problems, especially in mission-critical applications where speed and accuracy are paramount. These difficulties often stem from design defects that confuse or mislead users. Current methods for identifying such defects are time-consuming and costly, as they require human analysts to pinpoint the moments when users encounter difficulties. This paper introduces an automated method for detecting user difficulties by identifying hesitations during system use. The method's accuracy was evaluated by comparing its judgments with ground truth data generated by human analysts. The results show that at various threshold parameter values, the method can identify up to 92% of periods of user difficulty (with a 35% false-alarm rate), 86% (with a 24% false-alarm rate), and 20% (with a 3% false-alarm rate). The potential applications of this method in addressing interface defects are discussed.

## 1. Introduction

Undependable user interfaces pose a significant obstacle to achieving overall system dependability. Avizienis et al. [1] emphasize the critical nature of this challenge in dependable computing, stating:

"The problems of complex human-machine interactions (including user interfaces) remain a challenge that is becoming very critical – the means to improve their dependability and security need to be identified and incorporated."

User interface dependability can be defined as the extent to which a user interface allows human users to complete intended goals within specified speed and accuracy targets. When an interface hinders users from meeting these targets, it is likely due to design defects. Following Maxion and deChambeau [16], a user interface defect is defined as any aspect of a user interface that impairs a user’s ability to achieve a goal. Examples include ambiguous labels on controls, incomprehensible instructions, confusing icons, and inadequate system-status feedback. Such defects can cause delays and errors, hindering goal completion.

To achieve interface dependability, defects must be detected, diagnosed, and resolved. However, detecting user interface defects is challenging. Various techniques, including inspection-based methods, user modeling, user opinion surveys, field observation, and laboratory testing, have been studied and used in practice [2, 5, 11]. Each method has significant weaknesses, such as failing to detect defects (misses) or incorrectly classifying non-problematic aspects as defects (false alarms). Observation-based techniques, particularly field observation and laboratory testing, are generally considered the most effective for obtaining valid results. Nielsen [19, p. 165] notes:

"Testing with real users is the most fundamental usability method and is in some sense irreplaceable, since it provides direct information about how people use computers and what their exact problems are with the concrete interface being tested."

The primary drawbacks of observation-based techniques are their high cost in terms of usability-analyst time and their unreliability due to analyst error. These techniques require expert analysts to observe and record instances of user difficulty, which can be expensive. Nielsen estimated the cost of usability-analyst time at US$100 per hour in 1993 [21]. Additionally, the expense limits the amount of user data that can be analyzed, potentially missing rarely-occurring defects. Analysts may also miss instances of user difficulty due to rapid events or loss of vigilance [19, 23].

This paper introduces a method called hesitation detection, which automatically identifies instances of user difficulty. An instance of user difficulty is defined as a situation where a user's ability to achieve a goal is impaired, indicating an interface defect. The method identifies anomalously long pauses in user interactions with the mouse and keyboard. While such hesitations can occur for various reasons, they often indicate user difficulty. The method's accuracy is measured by the hit rate (percentage of all user difficulties detected) and the false-alarm rate (percentage of benign events mistakenly classified as difficulties).

Assuming the method accurately detects user difficulties, hesitation detection offers several advantages over human observation:
1. **Cost-Effectiveness:** It is automated, saving human-analyst time.
2. **Better Coverage:** More data can be analyzed than by a human analyst alone.
3. **Error-Free:** It does not miss instances due to limited attention or lack of vigilance.
4. **Repeatability:** It is deterministic and produces consistent results.

To evaluate the accuracy of hesitation detection, a detector was implemented and compared with a usability analyst's ratings of user difficulty in a laboratory study. The study collected usability data from users performing tasks with two different interfaces for setting file permissions on a Windows XP system. Users were instructed to think aloud, explaining their actions. Mouse and keyboard data were analyzed for hesitations, while screen video and think-aloud audio recordings were analyzed by a human expert. Both methods produced judgments of periods of user difficulty.

Using expert judgment as ground truth, the hit and false-alarm rates of the hesitation detector were computed. The method's accuracy can be tuned using a sensitivity parameter, and a receiver operating characteristic (ROC) curve was computed to show the tradeoff between hit and false-alarm rates. Representative points on the ROC curve include a 100% hit rate with a 63% false-alarm rate, a 92% hit rate with a 35% false-alarm rate, an 86% hit rate with a 24% false-alarm rate, and a 20% hit rate with a 3% false-alarm rate.

Although these results may initially seem disappointing, they represent a substantial improvement over previous methods and offer significant savings in analyst time. For example, at one sensitivity value, 60% of an analyst's time can be saved while still detecting 81% of all defects. This assumes an ideal analyst; for real analysts, time savings could be as high as 96%. The saved time can be used to analyze more data, potentially revealing additional defects.

## 2. Objective and Approach

This paper addresses the following research question:

**With what accuracy can instances of user difficulty be detected through automated hesitation detection?**

This question is important because hesitations do not always indicate difficulty, and user difficulty is not always manifested as hesitations. To determine the potential application of hesitation detection to interface defect detection, its accuracy must be measured. The terms hesitation detection, user difficulty, and accuracy are defined and explained below.

### 2.1 Hesitation Detection Method

The hesitation detector used in this study is an algorithm that takes as input a time-stamped, chronologically ordered data stream of mouse and keyboard events, including mouse movements, clicks, and keystrokes. It outputs a list of hesitations—anomalously long pauses between events. These pauses are identified by computing the latency between every pair of consecutive events, calculating the average latency, and outputting latencies that exceed a certain threshold. The threshold is specified as a number of standard deviations from the average latency. A low threshold will classify more pauses as hesitations, increasing hits but also false alarms. A high threshold will classify fewer pauses, reducing false alarms but increasing misses. Individual differences in mouse and keyboard activity are accounted for by computing latency averages and standard deviations independently for each user.

### 2.2 User Difficulty

User difficulty is an internal cognitive state where the ability to achieve a goal is impaired. This state can be characterized as confusion, frustration, uncertainty, lack of knowledge, or indecision. Since "user difficulty" is not directly observable, it must be inferred from observable events in data such as video and think-aloud audio recordings. An operational definition of "user difficulty" is needed. For this study, the following criteria were used by human usability experts to determine the onsets and offsets of periods of user difficulty:

#### 2.2.1 Criteria for Onset of User Difficulty

Five criteria were used to signal the onset of a period of user difficulty:

1. **User Statements:**
   - Indicative phrases: "I’m confused about...", "I’m not sure...", "I don’t know...", "I can’t figure out...", "I’m having a problem...", "I assume...", "How do I...".

2. **Silence and Inactivity:**
   - The user is silent and inactive with both the mouse and keyboard for at least 3 seconds. This threshold was chosen based on the data, as it seemed to be the minimum amount of silence and inactivity that genuinely indicated confusion. Silence or inactivity alone does not necessarily count as difficulty unless accompanied by other criteria.

3. **Toggling:**
   - The user toggles an interface control, such as a checkbox, through one or more full cycles without intervening actions. Toggling generally indicates confusion about the control's function.