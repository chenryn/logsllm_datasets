title:User Interface Defect Detection by Hesitation Analysis
author:Robert W. Reeder and
Roy A. Maxion
User Interface Defect Detection by Hesitation Analysis
Robert W. Reeder and Roy A. Maxion
PI:EMAIL, PI:EMAIL
Dependable Systems Laboratory
Computer Science Department
Carnegie Mellon University
Pittsburgh, Pennsylvania 15213 / USA
Abstract
Delays and errors are the frequent consequences of
people having difﬁculty with a user interface. Such de-
lays and errors can result in severe problems, particularly
for mission-critical applications in which speed and ac-
curacy are of the essence. User difﬁculty is often caused
by interface-design defects that confuse or mislead users.
Current techniques for isolating such defects are time-
consuming and expensive, because they require human an-
alysts to identify the points at which users experience dif-
ﬁculty; only then can diagnosis and repair of the defects
take place. This paper presents an automated method for
detecting instances of user difﬁculty based on identifying
hesitations during system use. The method’s accuracy was
evaluated by comparing its judgments of user difﬁculty with
ground truth generated by human analysts. The method’s
accuracy at a range of threshold parameter values is given;
representative points include 92% of periods of user difﬁ-
culty identiﬁed (with a 35% false-alarm rate); 86% (24%
false-alarm rate); and 20% (3% false-alarm rate). Appli-
cations of the method to addressing interface defects are
discussed.
1
Introduction
Undependable user interfaces present a major obstacle
to achieving overall system dependability. Avizienis et al.
[1], in outlining current challenges to dependable comput-
ing, state this in no uncertain terms:
The problems of complex human-machine inter-
actions (including user interfaces) remain a chal-
lenge that is becoming very critical – the means
to improve their dependability and security need
to be identiﬁed and incorporated.
User interface dependability can be deﬁned as the extent
to which a user interface allows human users to complete
intended goals within speciﬁed speed and accuracy targets.
When an interface prevents users from meeting these speed
and accuracy targets, defects in the design of the interface
are likely to be at fault. Following Maxion and deChambeau
[16], a user interface defect is deﬁned here as any aspect of a
user interface that impairs a user’s ability to achieve a goal.
Examples of such defects include ambiguous labels on in-
terface controls, incomprehensible instructions, confusing
icons, and inadequate system-status feedback. Such de-
fects can confuse users, causing delays and errors in users’
progress toward goal completion.
Interface dependability can only be achieved if interface
defects are detected, diagnosed, and recovered from. How-
ever, merely detecting user-interface defects has proven to
be a very difﬁcult problem. A variety of techniques, in-
cluding inspection-based methods (usability experts inspect
interface designs for defects), user modeling (a software
model of a human user is built and used to simulate inter-
action with an interface), user opinion surveys, ﬁeld obser-
vation of users at work, and laboratory user testing have
been studied by researchers and used in practice [2, 5, 11].
However, all of these methods have signiﬁcant weaknesses,
including both failure to detect defects (misses) and classi-
ﬁcation of non-problematic aspects of interfaces as defects
(false alarms). Of all available methods, observation-based
techniques, i.e., ﬁeld observation and laboratory user test-
ing, are generally accepted as the best for acquiring valid
results. Nielsen [19, p. 165] says:
[T]esting with real users is the most fundamental
usability method and is in some sense irreplace-
able, since it provides direct information about
how people use computers and what their exact
problems are with the concrete interface being
tested.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:35 UTC from IEEE Xplore.  Restrictions apply. 
The primary drawbacks of observation-based techniques
are their expense in terms of usability-analyst time, and
their unreliability due to analyst error. Whether conducted
in the ﬁeld or in the laboratory, these techniques require
one or more expert usability analysts to observe user actions
with an interface, and to record instances of user difﬁculty,
which is symptomatic of an interface defect. Usability-
analyst time can be quite expensive; Nielsen valued it at
US$100 per hour in 1993 [21]. The expense of usability-
analyst time often limits the amount of user data that can
be covered. Thus, rarely-manifested defects may be missed
during testing. Besides their expense, usability analysts are
error prone; they’re human. They may fail to notice in-
stances of user difﬁculty, either because such instances oc-
cur too rapidly for the analyst to record them all, or be-
cause they occur so infrequently that analysts lose vigilance
[19, 23]. Analyst error is another cause of missed defects.
Although this kind of error can be mitigated by videotap-
ing and later reviewing user sessions, reviewing videotapes
introduces more expense; Nielsen estimates that reviewing
videotape can take three to ten times as long as the original
user test [19].
This paper introduces a method, called hesitation detec-
tion, or automatically detecting instances of user difﬁculty.
An instance of user difﬁculty is deﬁned as an instance in
which a user’s ability to achieve a goal is impaired; so, by
deﬁnition, user difﬁculty is symptomatic of an interface de-
fect. The method identiﬁes hesitations, deﬁned as anoma-
lously long pauses, in users’ interactions with the mouse
and keyboard. Although such hesitations can occur for
many reasons, they often indicate user difﬁculty. Like other
methods for interface defect detection, hesitation detection
accuracy can be characterized by two measures: ﬁrst, by the
percentage of all user difﬁculties that it detects (the hit rate),
and second, by the percentage of benign events that it mis-
takenly classiﬁes as difﬁculties (the false-alarm rate). This
paper addresses the question of how accurate, in terms of hit
rate and false alarm rate, hesitation detection is at detecting
instances of user difﬁculty and, hence, interface defects.
Assuming it does have the ability to detect instances of
user difﬁculty accurately, hesitation detection provides sev-
eral signiﬁcant enhancements to observation by a human us-
ability analyst alone. First, it is cheap; since hesitation de-
tection is automated, it can save human-analyst time. Sec-
ond, it provides better coverage; much more data can be
searched for instances of user difﬁculty than could feasibly
be searched by a human analyst alone. Third, it is immune
to human error; it does not miss instances of user difﬁculty
due to limited attention or to lack of vigilance. Finally, its
results are repeatable; it is a deterministic method.
To address the question of hesitation detection accuracy,
a hesitation detector was implemented and its output was
compared with a usability analyst’s ratings of user difﬁculty
for a laboratory user study. In the user study, usability data
were collected from users performing tasks with two dif-
ferent interfaces for setting ﬁle permissions on a Windows
XP system. The users were instructed to think aloud as
they worked, explaining what they were doing at each point
in the task [6]. Mouse and keyboard data were collected
and subjected to hesitation analysis, while screen video and
think-aloud audio recordings were collected and subjected
to analysis by a human usability expert. Both methods, hes-
itation detection and expert analysis, produced judgments
of periods during which users had difﬁculty with the inter-
faces.
Using expert judgment as ground truth, the hit and false-
alarm rates of the hesitation detector were computed. The
hesitation-detection method allows these accuracy rates to
be tuned according to a sensitivity parameter, so a re-
ceiver operating characteristic (ROC) curve, which shows
the tradeoff between hit and false-alarm rates as the sensi-
tivity parameter is adjusted, was computed. Representative
points on the ROC curve include a 100% hit rate with a 63%
false-alarm rate; a 92% hit rate with a 35% false-alarm rate;
an 86% hit rate with a 24% false-alarm rate; and a 20% hit
rate with a 3% false alarm rate.
Although these results may at ﬁrst appear disappointing,
they are in fact quite good, providing a substantial improve-
ment over previous results, and providing a foundation for
huge economies in analyst time savings. For example, at
one detector sensitivity value, under very conservative as-
sumptions about how an analyst works, 60% of an analyst’s
time can be recovered while still detecting 81% of all de-
fects. This computation assumes an ideal analyst who re-
quires only one viewing of the data; for any real analyst,
time savings would be as high as 96%. Furthermore, the
saved time could be used to scan more data, which would
reveal more defects, possibly including those not previously
presented to the detector. It is concluded that hesitation de-
tection can provide signiﬁcant aid to human analysts in de-
tecting interface defects.
2 Objective and approach
This paper addresses the following research question:
With what accuracy can instances of user difﬁ-
culty be detected through automated hesitation
detection?
This question is important because hesitations do not al-
ways indicate difﬁculty, and user difﬁculty is not always
manifested as hesitations. So, in order to determine whether
hesitation detection has any potential application to inter-
face defect detection, it is ﬁrst necessary to measure its ac-
curacy. To clarify this research question, the terms hesita-
tion detection, user difﬁculty, and accuracy are deﬁned and
explained forthwith.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:35 UTC from IEEE Xplore.  Restrictions apply. 
2.1 Hesitation detection method
The hesitation detector employed in this study is an al-
gorithm that takes as input a time-stamped, chronologically
ordered data stream of mouse and keyboard events, includ-
ing mouse movements, mouse clicks, and keystrokes.
It
outputs a list of hesitations – anomalously long pauses be-
tween events in the data stream. Anomalously long pauses
are identiﬁed by computing the latency between every pair
of consecutive events in the data stream, computing the
average-length latency between events, and outputting those
latencies that exceed a certain threshold. The threshold
is speciﬁed as a number of standard deviations from the
average-length latency. The threshold value serves as a sen-
sitivity parameter for the detector. A low threshold value
will cause the detector to classify more pauses as hesita-
tions, thus potentially outputting more hits at the risk of
also outputting more false alarms. A high threshold value
will cause the detector to classify fewer pauses as hesita-
tions, thus potentially reducing false alarm output at the risk
of more misses. Note that individual differences in mouse
and keyboard activity are taken into account by computing
a latency average and standard deviation independently for
each user. Thus, hesitations are deﬁned relative to each
user, and are still valid for users who use input devices un-
usually quickly or slowly.
2.2 User difﬁculty
User difﬁculty is, conceptually, an internal cognitive state
in which the ability to achieve a goal is impaired. This state
can be characterized colloquially as confusion, frustration,
uncertainty, lack of knowledge, indecision, etc. Since “user
difﬁculty,” deﬁned this way, is not directly observable from
usability test data, it must be inferred from events that are
directly observable in data, such as video and think-aloud
audio recordings. Thus, an operational deﬁnition of “user
difﬁculty” is needed. For the purposes of the present work,
the criteria listed below were used by human usability ex-
perts to determine the onsets and offsets of periods of user
difﬁculty. These criteria constitute an operational deﬁnition
of “user difﬁculty.” Note that even these relatively objec-
tive criteria have some room for subjectivity, e.g., deciding
which statements constitute confusion. This subjectivity in
determining user difﬁculty was measured by comparing the
extent of agreement among multiple raters; section 4 con-
tains details.
2.2.1 Criteria for onset of user difﬁculty
Five criteria were used to signal the onset of a period of user
difﬁculty. These ﬁve criteria are directly observable events
in video and think-aloud audio data. The ﬁve criteria are:
1. User statements: These occur when a user makes a
statement or asks a question indicating confusion, frus-
tration, uncertainty, lack of knowledge, or indecision.
Such statements may start with phrases such as:
(cid:127) “I’m confused about ...”
(cid:127) “I’m not sure ...”
(cid:127) “I don’t know ...”
(cid:127) “I can’t ﬁgure out ...”
(cid:127) “I’m having a problem ...”
(cid:127) “I assume ...”
(cid:127) “How do I ...”
2. Silence and inactivity: This occurs when the user
is silent and inactive with both the mouse and key-
board for at least 3 seconds. This 3-second threshold
was arbitrarily chosen, but seemed to be the minimum
amount of silence and inactivity that genuinely indi-
cated confusion in the data used in this study. Silence
or inactivity alone does not necessarily count as difﬁ-
culty (unless accompanied by one or more of the other
four criteria).
3. Toggling: This occurs when a user toggles an inter-
face control, such as a checkbox, through one or more
full cycles of its various states, without any interven-
ing actions. An example of toggling is when a user
checks, then unchecks, a checkbox. Toggling gener-
ally indicates that a user is confused about what the