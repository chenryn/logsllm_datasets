EXPERIMENT: Watching the cache manager in action
In this experiment, we use Process Monitor to view the underlying file system activity, including 
cache manager read-ahead and write-behind, when Windows Explorer copies a large file (in this 
example, a DVD image) from one local directory to another. 
First, configure Process Monitor’s filter to include the source and destination file paths, the 
Explorer.exe and System processes, and the ReadFile and WriteFile operations. In this example, 
the C:\Users\Andrea\Documents\Windows_10_RS3.iso file was copied to C:\ISOs\ Windows_10_
RS3.iso, so the filter is configured as follows:
You should see a Process Monitor trace like the one shown here after you copy the file:
The first few entries show the initial I/O processing performed by the copy engine and the first 
cache manager operations. Here are some of the things that you can see:
I 
The initial 1 MB cached read from Explorer at the first entry. The size of this read depends on an
internal matrix calculation based on the file size and can vary from 128 KB to 1 MB. Because this
file was large, the copy engine chose 1 MB.
EXPERIMENT: Watching the cache manager in action
In this experiment, we use Process Monitor to view the underlying file system activity, including 
cache manager read-ahead and write-behind, when Windows Explorer copies a large file (in this 
example, a DVD image) from one local directory to another. 
First, configure Process Monitor’s filter to include the source and destination file paths, the 
Explorer.exe and System processes, and the ReadFile and WriteFile operations. In this example, 
the C:\Users\Andrea\Documents\Windows_10_RS3.iso file was copied to C:\ISOs\ Windows_10_
RS3.iso, so the filter is configured as follows:
You should see a Process Monitor trace like the one shown here after you copy the file:
The first few entries show the initial I/O processing performed by the copy engine and the first 
cache manager operations. Here are some of the things that you can see:
I
The initial 1 MB cached read from Explorer at the first entry. The size of this read depends on an 
internal matrix calculation based on the file size and can vary from 128 KB to 1 MB. Because this 
file was large, the copy engine chose 1 MB.
592 
CHAPTER 11 Caching and file systems
I 
The 1-MB read is followed by another 1-MB noncached read. Noncached reads typically indi-
cate activity due to page faults or cache manager access. A closer look at the stack trace for
these events, which you can see by double-clicking an entry and choosing the Stack tab, reveals
that indeed the CcCopyRead cache manager routine, which is called by the NTFS driver’s read
routine, causes the memory manager to fault the source data into physical memory:
I 
After this 1-MB page fault I/O, the cache manager’s read-ahead mechanism starts reading the
file, which includes the System process’s subsequent noncached 1-MB read at the 1-MB offset.
Because of the file size and Explorer’s read I/O sizes, the cache manager chose 1 MB as the
optimal read-ahead size. The stack trace for one of the read-ahead operations, shown next,
confirms that one of the cache manager’s worker threads is performing the read-ahead.
I
The 1-MB read is followed by another 1-MB noncached read. Noncached reads typically indi-
cate activity due to page faults or cache manager access. A closer look at the stack trace for 
these events, which you can see by double-clicking an entry and choosing the Stack tab, reveals 
that indeed the CcCopyRead cache manager routine, which is called by the NTFS driver’s read 
CcCopyRead cache manager routine, which is called by the NTFS driver’s read 
CcCopyRead
routine, causes the memory manager to fault the source data into physical memory:
I
After this 1-MB page fault I/O, the cache manager’s read-ahead mechanism starts reading the 
file, which includes the System process’s subsequent noncached 1-MB read at the 1-MB offset. 
Because of the file size and Explorer’s read I/O sizes, the cache manager chose 1 MB as the 
optimal read-ahead size. The stack trace for one of the read-ahead operations, shown next, 
confirms that one of the cache manager’s worker threads is performing the read-ahead.
CHAPTER 11 Caching and file systems
593
After this point, Explorer’s 1-MB reads aren’t followed by page faults, because the read-ahead 
thread stays ahead of Explorer, prefetching the file data with its 1-MB noncached reads. However, 
every once in a while, the read-ahead thread is not able to pick up enough data in time, and 
clustered page faults do occur, which appear as Synchronous Paging I/O.
If you look at the stack for these entries, you’ll see that instead of MmPrefetchForCacheManager, 
the MmAccessFault/MiIssueHardFault routines are called.
As soon as it starts reading, Explorer also starts performing writes to the destination file. 
These are sequential, cached 1-MB writes. After about 124 MB of reads, the first WriteFile opera-
tion from the System process occurs, shown here:
The write operation’s stack trace, shown here, indicates that the memory manager’s mapped 
page writer thread was actually responsible for the write. This occurs because for the first couple 
of megabytes of data, the cache manager hadn’t started performing write-behind, so the 
memory manager’s mapped page writer began flushing the modified destination file data. (See 
Chapter 10 for more information on the mapped page writer.)
After this point, Explorer’s 1-MB reads aren’t followed by page faults, because the read-ahead 
thread stays ahead of Explorer, prefetching the file data with its 1-MB noncached reads. However, 
every once in a while, the read-ahead thread is not able to pick up enough data in time, and 
clustered page faults do occur, which appear as Synchronous Paging I/O.
If you look at the stack for these entries, you’ll see that instead of MmPrefetchForCacheManager, 
MmPrefetchForCacheManager, 
MmPrefetchForCacheManager
the MmAccessFault/
MmAccessFault/
MmAccessFault MiIssueHardFault
/MiIssueHardFault
/
 routines are called.
MiIssueHardFault routines are called.
MiIssueHardFault
As soon as it starts reading, Explorer also starts performing writes to the destination file. 
These are sequential, cached 1-MB writes. After about 124 MB of reads, the first WriteFile opera-
tion from the System process occurs, shown here:
The write operation’s stack trace, shown here, indicates that the memory manager’s mapped 
page writer thread was actually responsible for the write. This occurs because for the first couple 
page writer thread was actually responsible for the write. This occurs because for the first couple 
page writer
of megabytes of data, the cache manager hadn’t started performing write-behind, so the 
memory manager’s mapped page writer began flushing the modified destination file data. (See 
Chapter 10 for more information on the mapped page writer.)
594 
CHAPTER 11 Caching and file systems
To get a clearer view of the cache manager operations, remove Explorer from the Process 
Monitor’s filter so that only the System process operations are visible, as shown next.
With this view, it’s much easier to see the cache manager’s 1-MB write-behind operations 
(the maximum write sizes are 1 MB on client versions of Windows and 32 MB on server versions; 
this experiment was performed on a client system). The stack trace for one of the write-behind 
operations, shown here, verifies that a cache manager worker thread is performing write-behind:
As an added experiment, try repeating this process with a remote copy instead (from one 
Windows system to another) and by copying files of varying sizes. You’ll notice some different 
behaviors by the copy engine and the cache manager, both on the receiving and sending sides.
To get a clearer view of the cache manager operations, remove Explorer from the Process 
Monitor’s filter so that only the System process operations are visible, as shown next.
With this view, it’s much easier to see the cache manager’s 1-MB write-behind operations 
(the maximum write sizes are 1 MB on client versions of Windows and 32 MB on server versions; 
this experiment was performed on a client system). The stack trace for one of the write-behind 
operations, shown here, verifies that a cache manager worker thread is performing write-behind:
As an added experiment, try repeating this process with a remote copy instead (from one 
Windows system to another) and by copying files of varying sizes. You’ll notice some different 
behaviors by the copy engine and the cache manager, both on the receiving and sending sides.
CHAPTER 11 Caching and file systems
595
Disabling lazy writing for a file
If you create a temporary file by specifying the flag FILE_ATTRIBUTE_TEMPORARY in a call to the 
Windows CreateFile function, the lazy writer won’t write dirty pages to the disk unless there is a se-
vere shortage of physical memory or the file is explicitly flushed. This characteristic of the lazy writer 
improves system performance—the lazy writer doesn’t immediately write data to a disk that might 
ultimately be discarded. Applications usually delete temporary files soon after closing them.
Forcing the cache to write through to disk
Because some applications can’t tolerate even momentary delays between writing a file and seeing 
the updates on disk, the cache manager also supports write-through caching on a per-file object basis; 
changes are written to disk as soon as they’re made. To turn on write-through caching, set the FILE_
FLAG_WRITE_THROUGH flag in the call to the CreateFile function. Alternatively, a thread can explicitly 
flush an open file by using the Windows FlushFileBuffers function when it reaches a point at which the 
data needs to be written to disk.
Flushing mapped files
If the lazy writer must write data to disk from a view that’s also mapped into another process’s address 
space, the situation becomes a little more complicated because the cache manager will only know 
about the pages it has modified. (Pages modified by another process are known only to that process 
because the modified bit in the page table entries for modified pages is kept in the process private 
page tables.) To address this situation, the memory manager informs the cache manager when a user 
maps a file. When such a file is flushed in the cache (for example, as a result of a call to the Windows 
FlushFileBuffers function), the cache manager writes the dirty pages in the cache and then checks to 
see whether the file is also mapped by another process. When the cache manager sees that the file is 
also mapped by another process, the cache manager then flushes the entire view of the section to write 
out pages that the second process might have modified. If a user maps a view of a file that is also open 
in the cache, when the view is unmapped, the modified pages are marked as dirty so that when the lazy 
writer thread later flushes the view, those dirty pages will be written to disk. This procedure works as 
long as the sequence occurs in the following order:
1.
A user unmaps the view.
2.
A process flushes file buffers.
If this sequence isn’t followed, you can’t predict which pages will be written to disk.
596 
CHAPTER 11 Caching and file systems
EXPERIMENT: Watching cache flushes
You can see the cache manager map views into the system cache and flush pages to disk by 
running the Performance Monitor and adding the Data Maps/sec and Lazy Write Flushes/sec 
counters. (You can find these counters under the “Cache” group.) Then, copy a large file from one 
location to another. The generally higher line in the following screenshot shows Data Maps/sec, 
and the other shows Lazy Write Flushes/sec. During the file copy, Lazy Write Flushes/sec signifi-
cantly increased.
Write throttling
The file system and cache manager must determine whether a cached write request will affect sys-
tem performance and then schedule any delayed writes. First, the file system asks the cache manager 
whether a certain number of bytes can be written right now without hurting performance by using the 
CcCanIWrite function and blocking that write if necessary. For asynchronous I/O, the file system sets up 
a callback with the cache manager for automatically writing the bytes when writes are again permitted 
by calling CcDeferWrite. Otherwise, it just blocks and waits on CcCanIWrite to continue. Once it’s noti-
fied of an impending write operation, the cache manager determines how many dirty pages are in the 
cache and how much physical memory is available. If few physical pages are free, the cache manager 
momentarily blocks the file system thread that’s requesting to write data to the cache. The cache man-
ager’s lazy writer flushes some of the dirty pages to disk and then allows the blocked file system thread 
to continue. This write throttling prevents system performance from degrading because of a lack of 
memory when a file system or network server issues a large write operation. 
EXPERIMENT: Watching cache flushes
You can see the cache manager map views into the system cache and flush pages to disk by 
running the Performance Monitor and adding the Data Maps/sec and Lazy Write Flushes/sec 
counters. (You can find these counters under the “Cache” group.) Then, copy a large file from one 
location to another. The generally higher line in the following screenshot shows Data Maps/sec, 
and the other shows Lazy Write Flushes/sec. During the file copy, Lazy Write Flushes/sec signifi-
cantly increased.
CHAPTER 11 Caching and file systems
597
Note The effects of write throttling are volume-aware, such that if a user is copying a large 
file on, say, a RAID-0 SSD while also transferring a document to a portable USB thumb drive, 
writes to the USB disk will not cause write throttling to occur on the SSD transfer.
The dirty page threshold is the number of pages that the system cache will allow to be dirty before 
throttling cached writers. This value is computed when the cache manager partition is initialized (the 
system partition is created and initialized at phase 1 of the NT kernel startup) and depends on the 
product type (client or server). As seen in the previous paragraphs, two other values are also com-
puted—the top dirty page threshold and the bottom dirty page threshold. Depending on memory 
consumption and the rate at which dirty pages are being processed, the lazy writer scan calls the 
internal function CcAdjustThrottle, which, on server systems, performs dynamic adjustment of the cur-
rent threshold based on the calculated top and bottom values. This adjustment is made to preserve the 
read cache in cases of a heavy write load that will inevitably overrun the cache and become throttled. 
Table 11-1 lists the algorithms used to calculate the dirty page thresholds. 
TABLE 11-1 Algorithms for calculating the dirty page thresholds
Product Type
Dirty Page Threshold
Top Dirty Page Threshold
Bottom Dirty Page Threshold
Client
Physical pages / 8
Physical pages / 8
Physical pages / 8
Server
Physical pages / 2
Physical pages / 2
Physical pages / 8
Write throttling is also useful for network redirectors transmitting data over slow communica-
tion lines. For example, suppose a local process writes a large amount of data to a remote file system 
over a slow 640 Kbps line. The data isn’t written to the remote disk until the cache manager’s lazy 
writer flushes the cache. If the redirector has accumulated lots of dirty pages that are flushed to disk 
at once, the recipient could receive a network timeout before the data transfer completes. By using 
the CcSetDirtyPageThreshold function, the cache manager allows network redirectors to set a limit on 
the number of dirty cache pages they can tolerate (for each stream), thus preventing this scenario. By 
limiting the number of dirty pages, the redirector ensures that a cache flush operation won’t cause a 
network timeout.
System threads
As mentioned earlier, the cache manager performs lazy write and read-ahead I/O operations by 
submitting requests to the common critical system worker thread pool. However, it does limit the use 
of these threads to one less than the total number of critical system worker threads. In client systems, 
there are 5 total critical system worker threads, whereas in server systems there are 10.
Internally, the cache manager organizes its work requests into four lists (though these are serviced 
by the same set of executive worker threads):
I 
The express queue is used for read-ahead operations.
I 
The regular queue is used for lazy write scans (for dirty data to flush), write-behinds, and
lazy closes.
598 
CHAPTER 11 Caching and file systems
I 
The fast teardown queue is used when the memory manager is waiting for the data section
owned by the cache manager to be freed so that the file can be opened with an image section
instead, which causes CcWriteBehind to flush the entire file and tear down the shared cache map.
I 
The post tick queue is used for the cache manager to internally register for a notification after
each “tick” of the lazy writer thread—in other words, at the end of each pass.
To keep track of the work items the worker threads need to perform, the cache manager creates 
its own internal per-processor look-aside list—a fixed-length list (one for each processor) of worker 
queue item structures. (Look-aside lists are discussed in Chapter 5 of Part 1.) The number of worker 
queue items depends on system type: 128 for client systems, and 256 for server systems. For cross-
processor performance, the cache manager also allocates a global look-aside list at the same sizes as 
just described.
Aggressive write behind and low-priority lazy writes
With the goal of improving cache manager performance, and to achieve compatibility with low-speed 
disk devices (like eMMC disks), the cache manager lazy writer has gone through substantial improve-
ments in Windows 8.1 and later.
As seen in the previous paragraphs, the lazy writer scan adjusts the dirty page threshold and its 
top and bottom limits. Multiple adjustments are made on the limits, by analyzing the history of the 
total number of available pages. Other adjustments are performed to the dirty page threshold itself 
by checking whether the lazy writer has been able to write the expected total number of pages in the 
last execution cycle (one per second). If the total number of written pages in the last cycle is less than 
the expected number (calculated by the CcCalculatePagesToWrite routine), it means that the underly-
ing disk device was not able to support the generated I/O throughput, so the dirty page threshold is 
lowered (this means that more I/O throttling is performed, and some cache manager clients will wait 
when calling CcCanIWrite API). In the opposite case, in which there are no remaining pages from the 
last cycle, the lazy writer scan can easily raise the threshold. In both cases, the threshold needs to stay 
inside the range described by the bottom and top limits.
The biggest improvement has been made thanks to the Extra Write Behind worker threads. In server 
SKUs, the maximum number of these threads is nine (which corresponds to the total number of critical 
system worker threads minus one), while in client editions it is only one. When a system lazy write scan 
is requested by the cache manager, the system checks whether dirty pages are contributing to memory 
pressure (using a simple formula that verifies that the number of dirty pages are less than a quarter of 
the dirty page threshold, and less than half of the available pages). If so, the systemwide cache manager 
thread pool routine (CcWorkerThread) uses a complex algorithm that determines whether it can add 
another lazy writer thread that will write dirty pages to disk in parallel with the others.
To correctly understand whether it is possible to add another thread that will emit additional I/Os, 
without getting worse system performance, the cache manager calculates the disk throughput of 
the old lazy write cycles and keeps track of their performance. If the throughput of the current cycles 
is equal or better than the previous one, it means that the disk can support the overall I/O level, so 
it makes sense to add another lazy writer thread (which is called an Extra Write Behind thread in this 
CHAPTER 11 Caching and file systems
599
case). If, on the other hand, the current throughput is lower than the previous cycle, it means that the 
underlying disk is not able to sustain additional parallel writes, so the Extra Write Behind thread is 
removed. This feature is called Aggressive Write Behind. 
In Windows client editions, the cache manager enables an optimization designed to deal with low-
speed disks. When a lazy writer scan is requested, and when the file system drivers write to the cache, 
the cache manager employs an algorithm to decide if the lazy writers threads should execute at low 
priority. (For more information about thread priorities, refer to Chapter 4 of Part 1.) The cache manager 
applies by-default low priority to the lazy writers if the following conditions are met (otherwise, the 
cache manager still uses the normal priority):
I 
The caller is not waiting for the current lazy scan to be finished.
I 
The total size of the partition’s dirty pages is less than 32 MB.
If the two conditions are satisfied, the cache manager queues the work items for the lazy writers in 
the low-priority queue. The lazy writers are started by a system worker thread, which executes at prior-
ity 6 – Lowest. Furthermore, the lazy writer set its I/O priority to Lowest just before emitting the actual 
I/O to the correct file-system driver.
Dynamic memory
As seen in the previous paragraph, the dirty page threshold is calculated dynamically based on the 
available amount of physical memory. The cache manager uses the threshold to decide when to 
throttle incoming writes and whether to be more aggressive about writing behind.