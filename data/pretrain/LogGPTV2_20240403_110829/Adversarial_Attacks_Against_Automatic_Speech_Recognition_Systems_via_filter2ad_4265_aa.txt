title:Adversarial Attacks Against Automatic Speech Recognition Systems via
Psychoacoustic Hiding
author:Lea Sch&quot;onherr and
Katharina Kohls and
Steffen Zeiler and
Thorsten Holz and
Dorothea Kolossa
Adversarial Attacks Against Automatic Speech
Recognition Systems via Psychoacoustic Hiding
Lea Sch¨onherr, Katharina Kohls, Steffen Zeiler, Thorsten Holz, and Dorothea Kolossa
{lea.schoenherr, katharina.kohls, steffen.zeiler, thorsten.holz, dorothea.kolossa}@rub.de
Ruhr University Bochum, Germany
Abstract—Voice interfaces are becoming accepted widely as
input methods for a diverse set of devices. This development
is driven by rapid improvements in automatic speech recogni-
tion (ASR), which now performs on par with human listening in
many tasks. These improvements base on an ongoing evolution
of deep neural networks (DNNs) as the computational core of
ASR. However, recent research results show that DNNs are
vulnerable to adversarial perturbations, which allow attackers
to force the transcription into a malicious output. In this paper,
we introduce a new type of adversarial examples based on
psychoacoustic hiding. Our attack exploits the characteristics
of DNN-based ASR systems, where we extend the original
analysis procedure by an additional backpropagation step. We
use this backpropagation to learn the degrees of freedom for
the adversarial perturbation of the input signal, i.e., we apply a
psychoacoustic model and manipulate the acoustic signal below
the thresholds of human perception. To further minimize the
perceptibility of the perturbations, we use forced alignment to
ﬁnd the best ﬁtting temporal alignment between the original audio
sample and the malicious target transcription. These extensions
allow us to embed an arbitrary audio input with a malicious
voice command that is then transcribed by the ASR system,
with the audio signal remaining barely distinguishable from the
original signal. In an experimental evaluation, we attack the state-
of-the-art speech recognition system Kaldi and determine the best
performing parameter and analysis setup for different types of
input. Our results show that we are successful in up to 98 %
of cases with a computational effort of fewer than two minutes
for a ten-second audio ﬁle. Based on user studies, we found that
none of our target transcriptions were audible to human listeners,
who still understand the original speech content with unchanged
accuracy.
I.
INTRODUCTION
Hello darkness, my old friend. I’ve come to talk with
you again. Because a vision softly creeping left its seeds
while I was sleeping. And the vision that was planted in
my brain still remains, within the sound of silence.
Simon & Garfunkel, The Sound of Silence
Motivation. Deep neural networks (DNNs) have evolved
into the state-of-the-art approach for many machine learning
tasks,
including automatic speech recognition (ASR) sys-
tems [43], [57]. The recent success of DNN-based ASR
Network and Distributed Systems Security (NDSS) Symposium 2019
24-27 February 2019, San Diego, CA, USA
ISBN 1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23288
www.ndss-symposium.org
systems is due to a number of factors, most importantly their
power to model large vocabularies and their ability to perform
speaker-independent and also highly robust speech recognition.
As a result, they can cope with complex, real-world environ-
ments that are typical for many speech interaction scenarios
such as voice interfaces. In practice, the importance of DNN-
based ASR systems is steadily increasing, e. g., within smart-
phones or stand-alone devices such as Amazon’s Echo/Alexa.
On the downside, their success also comes at a price:
the number of necessary parameters is signiﬁcantly larger
than that of the previous state-of-the-art Gaussian-Mixture-
Model probability densities within Hidden Markov Models
(so-called GMM-HMM systems) [39]. As a consequence, this
high number of parameters gives an adversary much space
to explore (and potentially exploit) blind spots that enable
her to mislead an ASR system. Possible attack scenarios
include unseen requests to ASR assistant systems, which may
reveal private information. Diao et al. demonstrated that such
attacks are feasible with the help of a malicious app on
a smartphone [14]. Attacks over radio or TV, which could
affect a large number of victims, are another attack scenarios.
This could lead to unwanted online shopping orders, which
has already happened on normally uttered commands over
TV commercials, as Amazon’s devices have reacted to the
purchase command [30]. As ASR systems are also often
included into smart home setups, this may lead to a signiﬁcant
vulnerability and in a worst-case scenario, an attacker may
be able to take over the entire smart home system, including
security cameras or alarm systems.
Adversarial Examples. The general question if ML-based
systems can be secure has been investigated in the past [5], [6],
[26] and some works have helped to elucidate the phenomenon
of adversarial examples [16], [18], [25], [47]. Much recent
work on this topic focussed on image classiﬁcation: different
types of adversarial examples have been investigated [9], [15],
[32] and in response, several types of countermeasures have
been proposed [12], [19], [60]. These countermeasures are
focused on only classiﬁcation-based recognition and some
approaches remain resistant [9]. As the recognition of ASR
systems operates differently due to time dependencies, such
countermeasures will not work equally in the audio domain.
In the audio domain, Vaidya et al. were among the ﬁrst to
explore adversarial examples against ASR systems [52]. They
showed how an input signal (i. e., audio ﬁle) can be modiﬁed
to ﬁt the target transcription by considering the features instead
of the output of the DNN. On the downside, the results show
high distortions of the audio signal and a human can easily
perceive the attack. Carlini et al. introduced so-called hidden
voice commands and demonstrated that targeted attacks against
HMM-only ASR systems are feasible [8]. They use inverse
feature extraction to create adversarial audio samples. Still, the
resulting audio samples are not intelligible by humans (in most
of the cases) and may be considered as noise, but may make
thoughtful listeners suspicious. To overcome this limitation,
Zhang et al. proposed so-called DolphinAttacks: they showed
that it is possible to hide a transcription by utilizing non-
linearities of microphones to modulate the baseband audio
signal with ultrasound higher than 20 kHz [61]. This work
has considered ultrasound only, however, our psychoacoustics-
based approach instead focuses on the human-perceptible
frequency range. The drawback of this and similar ultrasound-
based attacks [42], [48] is that the attack is costly as the
information to manipulate the input features needs to be
retrieved from recordings of audio signals with the speciﬁc
microphone, which is used for the attack. Additionally, the
modulation is tailored to a speciﬁc microphone, such that the
result may differ if another microphone is used. Recently,
Carlini and Wagner published a technical report
in which
they introduce a general
targeted attack on ASR systems
using connectionist temporal classiﬁcation (CTC) loss [10].
Similarly to previous adversarial attacks on image classiﬁers,
it works with a gradient-descent-based minimization [9], but
it replaces the loss function by the CTC-loss, which is op-
timized for time sequences. On the downside, the constraint
for the minimization of the difference between original and
adversarial sample is also borrowed from adversarial attacks
on images and therefore does not consider the limits and
sensitivities of human auditory perception. Additionally, the
algorithm often does not converge. This is solved by multiple
initializations of the algorithm, which leads to high run-time
requirements—in the order of hours of computing time—to
calculate an adversarial example. Also recently, Yuan et al.
described CommanderSong, which is able to hide transcripts
within music [59]. However, this approach is only shown to
be successful in music and it does not contain a human-
perception-based noise reduction.
Contributions. In this paper, we introduce a novel type
of adversarial examples against ASR systems based on psy-
choacoustic hiding. We utilize psychoacoustic modeling, as
in MP3 encoding, in order to reduce the perceptible noise.
For this purpose, hearing thresholds are calculated based on
psychoacoustic experiments by Zwicker et al. [62]. This limits
the adversarial perturbations to those parts of the original
audio sample, where they are not (or hardly) perceptible by
a human. Furthermore, we use backpropagation as one part
of the algorithm to ﬁnd adversarial examples with minimal
perturbations. This algorithm has already been successfully
used for adversarial examples in other settings [9], [10]. To
show the general feasibility of psychoacoustic attacks, we feed
the audio signal directly into the recognizer.
A key feature of our approach is the integration of the
preprocessing step into the backpropagation. As a result, it
is possible to change the raw audio signal without further
steps. The preprocessing operates as a feature extraction and
is fundamental to the accuracy of an ASR system. Due to the
differentiability of each single preprocessing step, we are able
to include it in the backpropagation without the necessity to
invert the feature extraction. In addition, ASR highly depends
on temporal alignment as it
is a continuous process. We
enhance our attack by computing an optimal alignment with
the forced alignment algorithm, which calculates the best
starting point for the backpropagation. Hence, we make sure
to move the target transcription into parts of the original audio
sample which are the most promising to not be perceivable by
a human. We optimize the algorithm to provide a high success
rate and to minimize the perceptible noise.
We have implemented the proposed attack to demonstrate
the practical feasibility of our approach. We evaluated it against
the state-of-the-art DNN-HMM-based ASR system Kaldi [38],
which is one of the most popular toolchains for ASR among
researchers [17], [27], [28], [40], [41], [50], [51], [53], [59]
and is also used in commercial products such as Amazon’s
Echo/Alexa and by IBM and Microsoft [3], [58]. Note that
commercial ASR systems do not provide information about
their system setup and conﬁguration.
Such information could be extracted via model stealing and
similar attacks (e. g., [20], [34], [37], [49], [54]). However,
such an end-to-end attack would go beyond the contributions
of this work and hence we focus on the general feasibility of
adversarial attacks on state-of-the-art ASR systems in a white-
box setting. More speciﬁcally, we show that it is possible
to hide any target
transcription in any audio ﬁle with a
minimum of perceptible noise in up to 98 % of cases. We
analyze the optimal parameter settings,
including different
phone rates, allowed deviations from the hearing thresholds,
and the number of iterations for the backpropagation. We need
less than two minutes on an Intel Core i7 processor to generate
an adversarial example for a ten-second audio ﬁle. We also
demonstrate that it is possible to limit the perturbations to
parts of the original audio ﬁles, where they are not (or only
barely) perceptible by humans. The experiments show that in
comparison to other targeted attacks [59], the amount of noise
is signiﬁcantly reduced.
This observation is conﬁrmed during a two-part audibility
study, where test listeners transcribe adversarial examples and
rate the quality of different settings. The results of the ﬁrst user
study indicate that it is impossible to comprehend the target
transcription of adversarial perturbations and only the original
transcription is recognized by human listeners. The second part
of the listening test is a MUSHRA test [44] in order to rate
the quality of different algorithm setups. The results show that
the psychoacoustic model greatly increases the quality of the
adversarial examples.
In summary, we make the following contributions in this paper:
•
•
Psychoacoustic Hiding. We describe a novel type
of adversarial examples against DNN-HMM-based
ASR systems based on a psychoacoustically designed
attack for hiding transcriptions in arbitrary audio ﬁles.
Besides the psychoacoustic modeling, the algorithm
utilizes an optimal temporal alignment and backprop-
agation up to the raw audio ﬁle.
Experimental Evaluation. We evaluate the proposed
attack algorithm in different settings in order to ﬁnd
adversarial perturbations that lead to the best recogni-
tion result with the least human-perceptible noise.
2
Fig. 1: Overview of a state-of-the-art ASR system with the three main components of the ASR system: (1) preprocessing of the
raw audio data, (2) calculating pseudo-posteriors with a DNN, and (3) the decoding, which returns the transcription.
•
User Study. To measure the human perception of
adversarial audio samples, we performed a user study.
More speciﬁcally, human listeners were asked to tran-
scribe what they understood when presented with ad-
versarial examples and to compare their overall audio
quality compared to original unmodiﬁed audio ﬁles.
A demonstration of our attack is available online at https:
//adversarial-attacks.net where we present several adversarial
audio ﬁles generated for different kinds of attack scenarios.
II. TECHNICAL BACKGROUND
Neural networks have become prevalent in many machine
learning tasks,
including modern ASR systems. Formally
speaking, they are just functions y = F (x), mapping some
input x to its corresponding output y. Training these networks
requires the adaptation of hundreds of thousands of free
parameters. The option to train such models by just presenting
input-output pairs during the training process makes deep
neural networks (DNNs) so appealing for many researchers.
At the same time, this represents the Achilles’ heel of these
systems that we are going to exploit for our ASR attack. In
the following, we provide the technical background as far as
it is necessary to understand the details of our approach.
A. Speech Recognition Systems
There is a variety of commercial and non-commercial ASR
systems available. In the research community, Kaldi [38] is
very popular given that it is an open-source toolkit which
provides a wide range of state-of-the-art algorithms for ASR.
The tool was developed at Johns Hopkins University and is
written in C++. We performed a partial reverse engineering of
the ﬁrmware of an Amazon Echo and our results indicate that
this device also uses Kaldi internally to process audio inputs.
Given Kaldi’s popularity and its accessibility, this ASR system
hence represents an optimal ﬁt for our experiments. Figure 1
provides an overview of the main system components that we
are going to describe in more detail below.
1) Preprocessing Audio Input: Preprocessing of the audio
input is a synonym for feature extraction: this step transforms
the raw input data into features that should ideally preserve all
relevant information (e. g., phonetic class information, formant
structure, etc.), while discarding the unnecessary remainder
(e. g., properties of the room impulse response, residual noise,
or voice properties like pitch information). For the feature
extraction in this paper, we divide the input waveform into
overlapping frames of ﬁxed length. Each frame is transformed
individually using the discrete Fourier transform (DFT) to
obtain a frequency domain representation. We calculate the
logarithm of the magnitude spectrum, a very common feature
representation for ASR systems. A detailed description is given
in Section III-E, where we explain the necessary integration
of this particular preprocessing into our ASR system.