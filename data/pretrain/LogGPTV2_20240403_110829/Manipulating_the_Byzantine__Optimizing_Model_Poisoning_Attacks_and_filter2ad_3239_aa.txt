title:Manipulating the Byzantine: Optimizing Model Poisoning Attacks and
Defenses for Federated Learning
author:Virat Shejwalkar and
Amir Houmansadr
Manipulating the Byzantine:
Optimizing Model Poisoning Attacks and Defenses
for Federated Learning
Virat Shejwalkar
University of Massachusetts Amherst
PI:EMAIL
Amir Houmansadr
University of Massachusetts Amherst
PI:EMAIL
Abstract—Federated learning (FL) enables many data owners
(e.g., mobile devices) to train a joint ML model (e.g., a next-
word prediction classiﬁer) without the need of sharing their
private training data. However, FL is known to be susceptible
to poisoning attacks by malicious participants (e.g., adversary-
owned mobile devices) who aim at hampering the accuracy
of the jointly trained model through sending malicious inputs
during the federated training process. In this paper, we present a
generic framework for model poisoning attacks on FL. We show
that our framework leads to poisoning attacks that substantially
outperform state-of-the-art model poisoning attacks by large
margins. For instance, our attacks result in 1.5× to 60× higher
reductions in the accuracy of FL models compared to previously
discovered poisoning attacks.
Our work demonstrates that existing Byzantine-robust FL
algorithms are signiﬁcantly more susceptible to model poisoning
than previously thought. Motivated by this, we design a de-
fense against FL poisoning, called divide-and-conquer (DnC). We
demonstrate that DnC outperforms all existing Byzantine-robust
FL algorithms in defeating model poisoning attacks, speciﬁcally,
it is 2.5× to 12× more resilient in our experiments with different
datasets and models.
I.
INTRODUCTION
Federated learning (FL) is an emerging learning paradigm
in which many data owners (called clients) collaborate in
training a common machine learning model, without sharing
their private training data. In this setting, a central server (e.g.,
a service provider) repeatedly collects some updates that the
clients compute using their local private data, aggregates the
clients’ updates using an aggregation algorithm (AGR), and
ﬁnally uses the aggregated client updates to tune the jointly
trained model (called the global model), which is broadcasted
to a subset of the clients at the end of each FL training epoch.
While FL has emerged as a promising solution for many
in-the-wild learning settings that involve mutually untrusting
clients, FL mechanisms are prone to poisoning attacks [31]:
malicious clients can attempt
to degrade the utility (e.g.,
model accuracy) of the resulting global model by contributing
malicious model updates during the FL training process. A
Network and Distributed Systems Security (NDSS) Symposium 2021
21-25  February  2021, Virtual 
ISBN  1-891562-66-5
https://dx.doi.org/10.14722/ndss.2021.24498 
www.ndss-symposium.org
poisoning attack can be either untargeted [31], [4], [17], [37],
where the goal is to minimize the accuracy of the global
model on any test input, or targeted [6], [3], where the goal
is to minimize the accuracy on speciﬁc test inputs. To defeat
poisoning attacks, a recent line of work has investigated the
design of Byzantine-robust FL algorithms, where the central
server uses some robust aggregation algorithm (AGR)
[39],
[8], [37], [31], [2], [12] to reduce the impact of malicious
model updates while preserving the utility of the global model.
Note that while centralized (i.e., non-FL) models have long
been known to be vulnerable to poisoning attacks through ma-
nipulation of training data, called data poisoning attacks [20],
[7], recent works [17], [6], [3], [4], [31] have demonstrated
advanced poisoning attacks tailored to FL, where malicious
clients directly manipulate model updates (e.g., gradients) that
they share with the central server during the FL training
process. Such attacks are known as model poisoning attacks
and are shown [4], [17] to be substantially more impactful on
FL than traditional data poisoning attacks. This paper focuses
on untargeted model poisoning attacks on FL.
A generic framework for model poisoning attacks on FL.
As the ﬁrst main contribution of this work, we present a
generic framework for model poisoning of FL. Unlike previous
works [17], [4], [31], [37], we consider a comprehensive set
of possible threat models for model poisoning attacks along
two dimensions of the adversary’s knowledge: the knowledge
of the updates shared by benign clients, and the knowledge
of the AGR algorithm that the server uses. We demonstrate
that the model poisoning attacks launched using our frame-
work outperform state-of-the-art model poisoning attacks in
defeating all existing Byzantine-robust FL algorithms.
The high-level approach of our attack is as follows. The
adversary computes a benign reference aggregate using some
benign data samples she knows; then she computes a malicious
perturbation vector (as will be explained in detail), e.g., a
unit vector in the opposite direction of the benign aggregate.
Finally, the adversary computes her malicious model update
by maximally perturbing the benign reference aggregate in
the malicious direction with the goal of evading detection
by robust aggregation algorithms. Based on this intuition,
we provide a general optimization framework (Section IV-A)
to mount optimal model poisoning attacks in different FL
settings. To make this optimization problem computationally
tractable, we introduce a modiﬁcation to our general optimiza-
tion problem: We ﬁx the type of malicious perturbation, and
search for the optimal coefﬁcient of the perturbation vector.
We present an algorithm to ﬁnd the most effective coefﬁcient
for any model poisoning attack objective that is formulated
using our general optimization framework (Section IV-D). We
also show that selecting the appropriate malicious perturbation
can signiﬁcantly boost
the impact of model poisoning on
FL, and propose a simple yet effective approach to select
the most effective perturbation vector for a given FL setting
(Section VI-C).
We perform various optimizations to tailor our attacks to
speciﬁc threat models. Speciﬁcally, we present attacks that
are tailored to state-of-the-art AGR algorithms [8], [31], [39],
which we call AGR-tailored attacks. Our AGR-tailored attacks
(Section IV-B) aim to maximize the perturbation to a reference
benign update while evading detection by the target robust
AGRs. We note that this also solves the fundamental objective
of maximizing the distance between the aggregates with and
without the attacks.
We also present AGR-agnostic attacks for the adversaries
with no knowledge of the underlying AGR algorithm (Sec-
tion IV-C). These adversaries do not know the constraints
of the server’s AGR. We circumvent this challenge based on
the key intuition behind robust AGRs: a robust AGR ﬂags
an update as malicious if it wanders far away from (the
majority of) benign updates. Therefore, our AGR-agnostic
attacks constrain their search of the most malicious updates to
a ball of a small radius around the clique of benign updates.
Based on this intuition, we propose two novel AGR-agnostic
attacks, both of which outperform the state-of-the-art AGR-
agnostic attack, LIE [4].
Evaluations. We extensively evaluate our attacks using four
benchmark classiﬁcation datasets (Section VI). We show that
the impact of our poisoning attacks (measured by the reduction
in model accuracy) is signiﬁcantly higher than that of state-of-
the-art model poisoning attacks, i.e., Fang [17] and LIE [4],
for all of the datasets. For instance, for CIFAR10 with Alexnet,
without any knowledge of the updates of benign clients, the
accuracy reductions due to the Fang and LIE attacks are
11.8% and 30.0%, respectively, while the reductions due to our
AGR-tailored and AGR-agnostic attacks are 45.6% and 44.5%,
respectively, i.e., 1.5× and 4× higher accuracy reduction.
For Purchase, our AGR-tailored and AGR-agnostic attacks on
Krum AGR [8] incur 15× and 60× higher accuracy re-
duction. For FEMNIST with Trimmed-mean (Median) AGR,
the accuracy reductions due to our AGR-tailored and AGR-
agnostic attacks are 2× to 3× (4× to 18×) higher than the
accuracy reductions due to Fang and LIE attacks.
Defending against model poisoning on FL. As the second
main contribution of this work, we propose a novel robust
aggregation algorithm (Section VII) to defend against model
poisoning attacks on FL. Our robust AGR, called the divide-
and-conquer (DnC), is inspired by existing defenses against
data poisoning attacks on centralized machine learning. Specif-
ically, these defenses use spectral analysis [16], [15], [28], [26],
e.g., singular value decomposition, to detect and ﬁlter outliers
in poisoned data. The intuition behind our DnC is that, when
FL clients’ data are identically and independently distributed
(iid), a malicious model poisoning update is impactful if and
only if it deviates signiﬁcantly from benign updates along a
speciﬁc malicious direction in the updates’ space. Therefore,
DnC ﬁrst computes the principle component of the set of
its input updates, speciﬁcally,
the direction of the largest
variance. Then, it computes scalar product of each submitted
model update with this principal component, which we call
projections. Finally, DnC removes a constant fraction of the
submitted model updates that have the largest projections.
However, it is computationally impossible to perform spectral
analysis of extremely high dimensional model updates in FL.
Therefore, DnC performs dimensionality reduction to enable
spectral analysis of its input updates while ensuring the effec-
tive detection of malicious updates.
Our DnC AGR provides strong theoretical robustness guar-
antees on the removal of malicious updates when benign up-
dates are iid [16], [15], [28], [26], [36]. We also evaluate DnC
against an adaptive attack using our generic attack framework
(Section IV-A).
Evaluations. We evaluate DnC for four benchmark classiﬁ-
cation datasets. We show that for the three iid datasets, i.e.,
MNIST, CIFAR10, and Purchase, DnC signiﬁcantly reduces
the impact of model poisoning attacks on FL when compared
to previous defenses (Section VII-C1). For instance, compared
to existing robust AGRs, DnC reduces the maximum reduction
in the accuracy of the global model due to model poisoning
attacks from 36.8% to 6.1% for CIFAR10 with Alexnet, from
32.5% to 6.3% for CIFAR10 with VGG11, from 11.6% to
1.8% for Purchase, and from 4.4% to 1.9% for MNIST. Note
that, in all of these cases, the most impactful attack against
DnC is our own adaptive attack. We also show the superiority
of DnC in defeating model poisoning on FL is due to its
effective ﬁltering of the malicious updates with high poisoning
impacts. For the non-iid FEMNIST dataset, we show that
DnC outperforms existing robust AGRs in mitigating model
poisoning attacks by large margins when the adversary has no
knowledge of the benign update of benign clients.
II. BACKGROUND
A. Federated Learning
∂θt
We consider a standard federated learning (FL) setting [30],
[21], [22] with a server and n clients with possibly disjoint
private datasets drawn from a data distribution D; the datasets
may not be independently and identically distributed (iid). In
epoch t, the server selects a subset of clients and broadcasts
the current global model parameters θt to the chosen clients.
the chosen clients then compute stochastic gradients,
All
∇t,i = ∂L(b,θt)
, using a randomly sampled minibatch b of
their private data and synchronously send it to the server. Here,
L(b, θt) is the loss, e.g., cross-entropy loss, computed using
minibatch b and model parameters θt. Then, the server ag-
gregates the collected gradients using some aggregation algo-
rithm, fagr(∇{t,i∈[n]}), e.g., dimension-wise Average. Finally,
the server computes a new model θt+1 using the aggregate
and SGD, and broadcasts it to a new subset of randomly
selected clients. This process is repeated until the global model
converges, i.e., has low loss L(Dval, θ) on validation data Dval.
The FL output is the global model with the maximum accuracy
on Dval across all FL epochs.
2
FL can either be cross-device FL with a very large (mil-
lions) number of clients and only a subset of them is chosen in
an epoch, or cross-silo FL with a moderate number of clients
(tens to hundreds) and all of them participate in every epoch.
Unlike previous works which consider only cross-silo FL [4],
[17], [37], we consider both the FL settings.
B. Poisoning attacks on FL
Federated learning is known to be vulnerable to various
poisoning attacks [8], [4], [6], [3], [31], [17], [29], [38], [32],
[20]. We divide these attacks based on the adversary’s goal and
capabilities. Based on the goal of adversary, there can be two
types of attacks: untargeted and targeted attacks. In untargeted
poisoning attacks, the goal is to minimize the accuracy of the
global model on any test input [17], [4], [31], [29], [38]. In
targeted poisoning attacks, the goal is to minimize the accuracy
on speciﬁc test inputs, while maintaining high accuracies on
the rest of the test inputs [6], [3]. Backdoor attacks [3] are a
subset of targeted attacks, where the targeted test inputs have
a backdoor trigger. Untargeted attacks can completely cripple
the global model, and therefore we believe, pose a more severe
threat to FL.
Based on the adversary’s capabilities, there are two types
of attacks: model and data poisoning attacks. In model poi-
soning attacks [17], [4], [31], [38], [6], [3], the adversary
can directly manipulate the gradients on malicious devices
before sharing them with the server in each epoch. While
in data poisoning attacks [32], [20], the adversary can only
indirectly manipulate the gradients on malicious devices by
poisoning training datasets on the devices. Due to the direct
manipulation of gradients, model poisoning attacks can achieve
higher attack impacts on FL. Also, as data poisoning attacks
cannot compute arbitrary gradients, it is not trivial to imitate
the malicious gradients of model poisoning attacks via training
data poisoning. Therefore, in order to understand the severity
of the threat of poisoning to FL, we focus on the stronger
untargeted model poisoning attacks on FL.
C. Existing robust aggregation algorithms for FL
In non-adversarial FL settings, dimension-wise Aver-
age [14], [23], [30] is an effective aggregation algorithm
(AGR) to aggregate clients’ gradients. However, even a sin-
gle malicious client can manipulate the Average AGR based
FL [8], [3], [5]. Therefore, multiple Byzantine-robust AGRs
for FL [8], [31], [37], [39], [2], [19], [10] are proposed to
defend against poisoning attacks by malicious clients.
1) Krum: Blanchard et al. [8] propose Krum AGR based on
the intuition that the malicious gradients need to be far from the
benign gradients in order to poison the global model. Hence,
Krum selects the gradient from the set of its input gradients
that is closest to its n − m − 2 neighboring gradients in the
squared Euclidean norm space; here, m is an upper bound on
the number malicious clients in FL.
2) Multi-krum: Blanchard et al. [8] modify Krum AGR to
Multi-krum AGR in order to effectively utilize the knowledge
shared by the clients in each FL epoch [8]. Multi-Krum selects
a gradient using Krum from a remaining-set (initialized to
the set of all the received gradients), adds it to a selection-
set (initialized to an empty set), and removes it from the
remaining-set. This way, Multi-krum selects c gradients such
that n − c > 2m + 2. Finally, Multi-krum averages the gradi-
ents in the selection-set. Multi-krum signiﬁcantly outperforms
Krum in terms of the global model accuracy.
3) Bulyan: Mhamdi et al. [31] show that a malicious
gradient can remain close to benign gradients while having
√
a single gradient dimension with a very large value (on order
of Ω( p
d)) and prevent convergence of the global model. As a
remedy, they propose Bulyan AGR, which requires n ≥ 4m+3
for its robustness guarantees to hold. Bulyan ﬁrst selects
θ(θ ≤ n − 2m) gradients in the same fashion as Multi-krum,
and then computes Trimmed-mean of the selected gradients;
please refer to [31] for more details.
[37]
is
Trimmed-mean
[39],
4) Trimmed-mean:
a
coordinate-wise AGR which aggregates each dimension of
input gradients separately. Speciﬁcally, for a given dimension
j, it sorts the values of jth-dimension of all gradients, i.e.,
sorts ∇j{i∈[n]}. Then it removes β largest and smallest values
and computes average of the rest of the values as its aggregate
of dimension j. We use Trimmed-mean where β equals
m, the number of malicious clients. Yin et al. [39] show
that Trimmed-mean achieves order-optimal error rates when
m ≤ β ≤ n
2 for strongly convex objective function.
5) Median: Median [39], [37] is an another coordinate-
wise AGR which aggregates its input gradients by comput-
ing median of the values of each of the dimensions of the
gradients. Yin et al. [39] give theoretical guarantees on the
robustness of Median AGR, while Fang et al. [17] empirically
show that Median AGR has better robustness than the more
sophisticated Krum AGR.
6) Adaptive federated average (AFA): AFA [33] removes
malicious gradients based on their cosine-similarities with a
benign gradient. More speciﬁcally, in each FL round, AFA
computes a weighted average of collected gradients and
computes cosine similarities between the weighted average
and each of the collected gradients. Then, AFA discards the
gradients whose similarities are out of a range; this range is
a simple function of mean, median and standard deviation of
the similarities.
7) Fang defenses: Fang et al. [17] propose defenses that
are meta-AGRs and rely on existing robust AGRs to detect
malicious gradients. More speciﬁcally, consider a robust AGR
A. Given a set of gradients G, the corresponding Fang defense,
called Fang-A, computes a score for each gradient in ∇i ∈ G
as follows. Fang-A computes two aggregates using A, one with
∇i and one without ∇i in G, i.e., A(G) and A(G−∇i). Fang-
A then computes losses and/or errors of the models obtained
by updating the global model using the two aggregates. Then
Fang-A assigns a score to each ∇i such that
the lower
the negative impact of ∇i on the loss and/or error of the
corresponding model, the higher the score. Finally, Fang-A