Targeted attacks [7], [59] aim to misclassify specific sets/-
classes of input, hence they are “discriminate.” Such discrimi-
nate attacks can be either used for “integrity” or “availability”
violations, depending on how the poisoned data is used.
Semantic backdoor attacks [3], [61] have the same goal as the
targeted attacks, but the targeted inputs should have specific
properties, e.g., a pixel pattern or a word sequence. Hence,
these are “discriminate,” “availability” or “integrity” attacks.
Artificial backdoor attacks [67]
aim to misclassify any
input containing a backdoor trigger, hence these attacks are
“indiscriminate” attacks. Note that, such test inputs should be
modified to have the backdoor trigger and only the adversary
or a malicious client know the trigger. Hence, these attacks
aim to evade the detection, i.e., cause an integrity violation.
Hence, these are “integrity indiscriminate” attacks.
Untargeted attacks [5], [23], [55] aim to misclassify any test
input, i.e., they are “indiscriminate” attacks. But, test inputs
need not be modified in order to misclassify. Hence, these are
“availability” attacks.
Finally, the error specificity of each of these attacks can be
either “specific” or “generic.”
Focus of our work:
In this work, we focus on untargeted
attacks, i.e., indiscriminate availability attacks with generic
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1357
Table II: The key dimensions of the threat models of poisoning attacks on FL. Each combination of these dimensions constitutes
a threat model (Table IV). However, we argue in Section III-B2 that only two of these combinations are practical threat models.
Dimension
Attribute
Objective
of the
adversary
Security violation
Attack specificity
Error specificity
Description
Misclassify a (adversarially crafted) test input in order to evade detection.
Misclassify an unmodified test input to cause service disruption for benign users.
Values
Integrity
Availability
Discriminate Misclassify a small and/or specific set of inputs at the test time.
Indiscriminate Misclassify all or most of inputs at the test time.
Specific
Generic
Misclassify a given modified/pristine test input to a specific class.
Misclassify a given modified/pristine test input to any class.
Adversary can access the global model parameters as well as its predictions, e.g., in the model
poisoning case.
Whitebox
Knowledge of
the global model
Nobox
Adversary cannot access parameters or predictions of global model, e.g., in the data poisoning case.
Knowledge of
the data from
the distribution of
benign clients’
data
Full
Partial
Model poison Adversary breaks into the compromised clients (e.g., by circumventing security protocols of operating
systems such as Android) and directly manipulates their model updates.
Capabilities in
terms of access
to client devices
Data poison
Adversary can only manipulate local data of the compromised clients; the clients use this data to
compute their updates. Adversary does not break into the compromised clients.
Knowledge
of the
adversary
Capabilities
of the
adversary
Online
Offline
Adversary repeatedly and adaptively poisons the compromised clients during FL, e.g., model poisoning
attacks [7], [23], [55]. Impacts of these attacks can persist over the entire FL training.
Adversary poisons the compromised clients only once at the beginning of FL, e.g., baseline label
flipping attacks [23], [61]. Impact of such attacks may quickly fade away.
Capabilities in
terms of frequency
of the attack
(Attack mode)
error specificity, for the following reasons.
threat
Untargeted attacks pose a great
to production FL:
Untargeted attacks are designed to impact all clients and
all test inputs. For instance, FL on FEMNIST achieves an
85% [52] accuracy in a benign setting, and untargeted attacks
reduce the accuracy to, e.g.,
[78, 82]% depending on the
percentages of compromised clients. Such an accuracy drop is
significant for production FL, as a malicious service provider
can gain advantage over their competitors by causing such
small, yet noticeable, accuracy reductions in the competing
services and such small accuracy reductions can impact most
clients and data from all classes in arbitrary fashion.
Untargeted attacks can go undetected for long duration: As
discussed above, the untargeted attack aims at reducing the
overall accuracy of the global model, even by only a few
percentage points. Such a small reduction in accuracy is
hard to detect in practical settings due to the absence of
reliable benchmarks for the target application. For instance,
the affected service provider will never know that they could
have achieved an 85% accuracy and will believe that [78, 82]%
is the highest achievable accuracy.
Constructing untargeted attacks is more challenging: Un-
targeted attacks aim to solve a more challenging problem,
which is affecting arbitrary test inputs. However, while there
exist several defenses to protect FL against untargeted poison-
ing [10], [41], [55], [70], these attacks are not studied under
production FL environments (as discussed later on).
2) Adversary’s Knowledge: Below we elaborate on two
dimensions of adversary’s knowledge: knowledge of the global
model and knowledge of the data from the benign distribution.
Knowledge of the global model: This can be nobox or
whitebox. In the nobox case, the adversary does not know the
model architecture, parameters, or outputs, and is the most
practical setting in FL [32], e.g., the data poisoning adversary
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1358
.........Adversary can access local data only of the compromisedclients, but not of the benign clients.Adversary can access the local data of all of the collaboratingclients, i.e., benign and compromised clients, in FL.Full knowledgePartial knowledge......has nobox knowledge of the global model. In the whitebox
case, the adversary knows the global model parameters and
outputs, whenever the server selects at least one compromised
client. The model poisoning adversary always has whitebox
knowledge of the global model. As we will explain in Sec-
tion III-B3, this is a relatively less practical setting in FL, as
it assumes complete control of the compromised devices.
Knowledge of the data from benign distribution: This can
be full or partial. In full knowledge case, the adversary can
access the benign local data of compromised as well as benign
clients. In partial knowledge case, the adversary can access the
benign local data only of the compromised clients. We only
consider the partial knowledge case, because accessing the
data of all the clients is impractical in production FL.
3) Adversary’s Capability: Below, we elaborate on the the
adversary’s capability in terms of access to client devices and
frequency of attack, i.e., the attack mode.
Capability in terms of access to client devices: Based on
the FL stages (part of FL pipeline on client device) that the
adversary poisons, there can be a model poisoning adversary
or a data poisoning adversary. The model poisoning adversary
can break into a compromised device (e.g., by circumventing
the security protocols of operating systems such as Android)
and can directly manipulate the poisoned updates [5], [10],
[23], [41], [50], [55]. This adversary can craft highly effective
poisoned updates, but due to unreasonable amount of access
to client devices, it can compromise very small percentages of
FL clients [24], [32].
On the other hand, a data poisoning adversary cannot
break into a compromised device and can only poison its
local dataset. The compromised clients use their local poi-
soned datasets to compute their poisoned updates, hence this
adversary indirectly manipulates the poisoned updates. Due
to the indirect manipulation, these updates may have less
poisoning impact than the model poisoning updates. But, due
to the limited access required to the compromised clients, this
adversary can compromise relatively large percentages of FL
clients [24], [32].
Capability in terms of attack frequency (Attack mode): The
mode of poisoning attacks on FL can be either offline or online.
In the offline mode, the adversary poisons the compromised
clients only once before the start of FL training, e.g., the
baseline label flip attack [23] flips the labels of data of
compromised clients once before the FL training starts. In the
online mode, the adversary repeatedly and adaptively poisons
the compromised clients, e.g., existing model poisoning at-
tacks [5], [55] repeatedly poison the updates of compromised
clients selected by the server.
Finally, we assume that the compromised clients can collude
to exchange their local data and model updates in order to
increase impacts of their attacks.
B. Practical Considerations for Poisoning Threat Models
1) Salient Features of Production Federated Learning:
Production FL can be either cross-device or cross-silo [32].
In cross-device FL, the number of clients (N) is large (from
few thousands to billions) and only a small fraction of them
is chosen in each FL training round, i.e., n ≪ N. In cross-
device FL, clients’ devices are highly resource constrained,
and therefore, they can process only a limited amounts of data
in an FL round. Also, as the devices have highly unreliable
network connections, it is expected that a small fraction of
the selected devices may drop out in any given FL round.
Note that, this equally impacts both benign and compromised
clients and does not affect
this is similar
to how the choice of n has no impact on the robustness
(Section V-C3). In cross-silo FL, N is moderate (up to 100)
and all clients are selected in each round, i.e., n = N. Clients
are large corporations, e.g., banks, and have devices with
ample resources. Hence, they can process very large amounts
of data and client drop-outs do not happen.
the robustness;
In both FL types, the on-device model used for inference
and the on-device model being trained are different. Hence, an
adversary cannot gain any insight into the training-model by
querying the inference-model, i.e., nobox access (Table II), and
must break into the device, i.e., get whitebox access (Table II).
Finally, we assume that production systems are adequately
protected against standard attack vectors and vulnerabilities
such as Sybil attacks. For instance, if the adversary manages
to operate millions of fake accounts [22], we argue that the
service provider should prioritize improving their security
attestation protocols instead of deploying FL. Section III-B3
also explains that the cost of operating a large scale, persistent
botnet in modern operating systems, e.g., Android, is non-
trivial. Please refer to [32] for more details on production FL.
2) Understanding the practicality of threat models: For our
goal of untargeted poisoning with the partial knowledge of
the benign data, we can combine the rest of the dimensions in
Table II and obtain eight possible threat models (Table IV). We
argue that only T4 (nobox offline data poison) and T5 (white-
box online model poison) are of practical value, and below,
justify why other models are less relevant in practice: (1) With
model poisoning capability, the adversary has whitebox access
by default, hence, T1 and T2 in Table IV are not valid. (2)
In cross-device FL, only a few selected clients get the most
recent global model in each round. Hence, to gain whitebox
access to the model, the adversary needs to control (i.e., break
into) a large number of devices (so that in most FL rounds,
the FL server picks at least one of them), which is impractical
in practice as we explain in Section III-B3. With whitebox
access, the adversary can mount the stronger online model
poisoning attacks (MPAs) instead of data poisoning attacks
(DPAs). Therefore, T3, T7, and T8 are not reasonable threat
models, as they combine whitebox access with either offline
attacks or DPAs. (3) Under T6 (nobox online data poison), the
adversary mounts an online attack, i.e., they adaptively poison
the local data of compromised clients. But, as the adversary
has no knowledge of the (current) global model due to nobox
access, they cannot generate new poisoning data adaptively.
Hence, the combination of nobox and online is not practical.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1359
Table III: Practical ranges of FL parameters based on the literature and discussions on FL production systems [11], [24], [32]
and the ranges used in untargeted FL poisoning and robust AGRs literature [5], [10], [23], [41], [55]. MPA means model
poisoning attack and DPA means data poisoning attack. Red (green) cells denote impractical (practical) ranges.
Parameters/Settings
What we argue to be practical
FL type + Attack type
Cross-silo + DPAs
Cross-device + {MPAs, DPAs}
Total number of FL
Order of [103, 1010] for cross-device
clients, N
Number of clients
chosen per round, n
% of compromised
clients, M
Average size of benign
clients’ data, |D|avg
Maximum size of
local poisoning data
[2, 100] for cross-silo
Small fraction of N for cross-device
All for cross-silo
M ≤0.1% for DPAs
M ≤0.01% for MPAs
[50, 1000] for cross-device
Not applicable to cross-silo
Up to 100 × |D|avg for DPAs
Not applicable to MPAs
Used in previous
untargeted works
Cross-silo + MPAs
[50, 100]
All
[20, 50]%
Not studied for cross-device
[50, 1000] for cross-silo
∼ |D|avg
Table IV: The eight possible threat models for untargeted
poisoning attacks on FL. T3-T8 are valid, but only T4 and
T5 represent practical FL deployments (Section III-C).
Capability
∈ {MP, DP}
T1 Model poison
T2 Model poison
T3 Model poison
T4 Model poison