# Title: Are We There Yet? Timing and Floating-Point Attacks on Differential Privacy Systems

## Authors
Jiankai Jin, Eleanor McMurtry, Benjamin I. P. Rubinstein, Olga Ohrimenko

## Affiliations
- **Jiankai Jin, Benjamin I. P. Rubinstein, Olga Ohrimenko**: School of Computing and Information Systems, The University of Melbourne
- **Eleanor McMurtry**: Department of Computer Science, ETH Zurich (Work done in part while at The University of Melbourne)

## Conference
2022 IEEE Symposium on Security and Privacy (SP)

## Abstract
Differential privacy (DP) is a widely adopted privacy framework that has been implemented in several mature software platforms. Careful implementation is essential to ensure end-to-end security guarantees. This paper examines two implementation flaws in the noise generation commonly used in DP systems. First, we investigate the Gaussian mechanism's vulnerability to a floating-point representation attack, similar to the one Mironov conducted against the Laplace mechanism in 2011. Our experiments demonstrate the success of this attack against DP algorithms, including deep learning models trained using differentially-private stochastic gradient descent (DP-SGD).

In the second part, we study discrete counterparts of the Laplace and Gaussian mechanisms, which were proposed to mitigate the shortcomings of floating-point representation. However, these implementations are susceptible to a timing attack. An observer who can measure the time to draw (discrete) Laplace or Gaussian noise can predict the noise magnitude, thereby recovering sensitive attributes. This attack invalidates the differential privacy guarantees of systems implementing such mechanisms.

We show that several state-of-the-art DP implementations are vulnerable to these attacks, with success rates up to 92.56% for floating-point attacks on DP-SGD and up to 99.65% for end-to-end timing attacks on private sums protected with discrete Laplace. Finally, we evaluate and suggest partial mitigations.

## Introduction

Consider the equation:
\[ z - y = 0.1234567890004 \]
If \( y \) is known to be one of 0, 2000, or 20000, can \( y \) be equal to 0, 2000, or 20000? On a machine using double-precision floating-point format, while \( z = 2000.1234567890004 \) cannot be represented, \( 2000.1234567890003 \) and \( 2000.1234567890006 \) can. Similarly, for \( z = 20000.1234567890004 \). Without knowing \( z \), we can definitively say that \( y \) must equal 0 if it is known to be one of 0, 2000, or 20000.

Differential privacy (DP) is a widely accepted privacy framework, deployed by organizations such as the U.S. Census Bureau, Apple, Google, Microsoft, and others. Research on DP includes algorithmic performance trade-offs, new models in different settings, and practical implementations. Robust implementations are crucial to provide end-to-end privacy that matches theoretical DP guarantees.

Implementations of DP algorithms often raise concerns not considered in theoretical analysis, which focuses on idealized settings. Mironov [6] first discussed the implications of finite-precision computers, where certain floating-point values cannot be generated, leading to potential privacy breaches. Haeberlen et al. [7] and Andrysco et al. [8] showed that DP algorithms may suffer from timing side-channels, as computation times can vary based on sensitive data.

In this paper, we extend Mironov’s attack to other DP mechanisms and study its effects on real-world DP implementations. We also describe another timing side-channel due to the timing of noise samplers.

### a) Floating-Point Representation of the Gaussian Distribution

The Gaussian mechanism, based on additive Gaussian noise, achieves approximate differential privacy, meaning it may fail to provide pure DP with a small, controllable probability \( \delta \). It has advantages over the Laplace mechanism, including lighter tails and superior composition properties. Generalizations like R´enyi differential privacy [9] and truncated concentrated differential privacy [10] have emerged, offering tight composition analysis and efficient privacy amplification by subsampling.

A question arises: are the same attacks as in [6] possible against the Gaussian mechanism? While several works [12], [13] mention the feasibility, no one has demonstrated this possibility. In this paper, we develop an attack confirming that common Gaussian sampling implementations are subject to floating-point attacks. We show that the attack is feasible and has a significant success rate, even for samplers based on different methods.

The most prominent use of the Gaussian mechanism is in training machine learning (ML) algorithms using DP-SGD [11]. We demonstrate that our attack can determine if a batch contains a particular record, violating DP guarantees of DP-SGD. ML model training naturally reveals sequential Gaussian samples to an adversary, as it returns a noisy gradient for each parameter.

### b) Timing Attacks Against Discrete Distribution Samplers

In the second part, we study discrete versions of the Laplace and Gaussian mechanisms [12], [13], which avoid floating-point representations. These mechanisms, however, are susceptible to a timing side-channel. An adversary who observes the time to draw a sample can determine the noise magnitude, revealing the noiseless (private) value.

Our timing attack exploits the underlying technique of direct simulation of geometric sampling, where values are sampled until a coin toss results in a "head." The number of coin tosses is tied to the noise magnitude; timing the sampler reveals this number and leaks the noise magnitude. Though timing has been identified as a potential side-channel in DP [7], [8], we are the first to show that noise distribution samplers, not the mechanisms themselves, give rise to secret-dependent runtimes.

- **Empirical Results**: We demonstrate that Gaussian samplers in NumPy, PyTorch, and Go are vulnerable to our attack. Focusing on the Opacus DP library [4] by Facebook, we show that DP-SGD is vulnerable to information leakage.
- **Discrete Methods**: We observe that discrete methods developed to protect against floating-point attacks for both the Laplace and Gaussian mechanisms suffer from timing side-channels. We show that two libraries are vulnerable: a DP library by Google [3] and the implementation accompanying another work on discrete distributions [12], [14].
- **Mitigations**: We discuss and evaluate mitigations against each attack.

**Disclosure**: We have informed the maintainers of the mentioned DP libraries of our findings. They have acknowledged our report and the disclosure dates.

## Background

In this section, we provide background on floating-point representation, differential privacy, and the Laplace and Gaussian mechanisms for DP.

### A. Floating-Point Representation

Floating-point values represent real values using three numbers: a sign bit \( b \), an exponent \( e \), and a significand \( d_1d_2...d_d \). For example, 64-bit (double precision) floating-point numbers allocate 1 bit for \( b \), 11 bits for \( e \), and 52 bits for the significand. Such a floating-point number is defined as:
\[ (-1)^b \times (1.d_1d_2...d_d)_2 \times 2^{e-1023} \]

Crucially, the number of real values representable using floating-point numbers is finite, leading to potential vulnerabilities in DP implementations.