title:Are We There Yet? Timing and Floating-Point Attacks on Differential
Privacy Systems
author:Jiankai Jin and
Eleanor McMurtry and
Benjamin I. P. Rubinstein and
Olga Ohrimenko
2022 IEEE Symposium on Security and Privacy (SP)
Are We There Yet? Timing and Floating-Point
Attacks on Differential Privacy Systems
Jiankai Jin∗, Eleanor McMurtry†‡, Benjamin I. P. Rubinstein∗, Olga Ohrimenko∗
∗School of Computing and Information Systems, The University of Melbourne
†Department of Computer Science, ETH Zurich
2
7
6
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
Abstract—Differential privacy is a de facto privacy framework
that has seen adoption in practice via a number of mature
software platforms. Implementation of differentially private (DP)
mechanisms has to be done carefully to ensure end-to-end
security guarantees. In this paper we study two implementation
ﬂaws in the noise generation commonly used in DP systems. First
we examine the Gaussian mechanism’s susceptibility to a ﬂoating-
point representation attack. The premise of this ﬁrst vulnerability
is similar to the one carried out by Mironov in 2011 against the
Laplace mechanism. Our experiments show the attack’s success
against DP algorithms, including deep learning models trained
using differentially-private stochastic gradient descent.
In the second part of the paper we study discrete counterparts
of the Laplace and Gaussian mechanisms that were previously
proposed to alleviate the shortcomings of ﬂoating-point repre-
sentation of real numbers. We show that such implementations
unfortunately suffer from another side channel: a novel timing
attack. An observer that can measure the time to draw (discrete)
Laplace or Gaussian noise can predict the noise magnitude,
which can then be used to recover sensitive attributes. This
attack invalidates differential privacy guarantees of systems
implementing such mechanisms.
We demonstrate that several commonly used, state-of-the-art
implementations of differential privacy are susceptible to these
attacks. We report success rates up to 92.56% for ﬂoating point
attacks on DP-SGD, and up to 99.65% for end-to-end timing
attacks on private sum protected with discrete Laplace. Finally,
we evaluate and suggest partial mitigations.
I. INTRODUCTION
Given the equation
it
if one knows that
z − y = 0.1234567890004 ,
to 0, 2000 or 20000? Though one may
can y be equal
is possible to answer this question
ask “what
is z?”,
without knowing z,
the arithmetic was
computed on a machine using the double-precision ﬂoating-
point
format. While z = 2000.1234567890004 cannot be
represented, 2000.1234567890003 and 2000.1234567890006
can be. Similarly for z = 20000.1234567890004. In fact
without knowing z at all, we can say deﬁnitively that y must
equal 0 if it is known to be one of 0, 2000, or 20000.
Differential privacy (DP) is a de facto privacy framework
that has received signiﬁcant interest from the research com-
munity and has been deployed by the U.S. Census Bureau,
Apple, Google, Microsoft, and many others. Research on DP
ranges from algorithms with different performance trade-offs,
‡Work done in part while at The University of Melbourne.
to new models in different settings, and also to practical
implementations [1], [2], [3], [4], [5]. Robust implementations
are crucial to provide end-to-end privacy that matches on-paper
differential privacy guarantees.
Implementations of DP algorithms often raise concerns not
considered in theoretical analysis (which focuses on idealized
settings). Mironov [6] was ﬁrst to discuss the implications of
the fact that one cannot represent—and thus cannot sample
from—all real numbers on a ﬁnite-precision computer. Focus-
ing on the Laplace mechanism, Mironov’s attack proceeds by
observing that certain ﬂoating-point values cannot be gener-
ated by a DP computation and hence a release could reveal
the (private) noiseless value. On the other hand, Haeberlen et
al. [7] and Andrysco et al. [8] showed that DP algorithms may
suffer from timing side-channels since such algorithms can
take different time depending on sensitive values in a dataset.
In this paper we extend Mironov’s attack to other DP mech-
anisms and study its effects on real-world DP implementations.
We then describe another timing side-channel that can arise in
DP implementations due to the timing of the noise samplers.
a) Floating-Point Representation of the Gaussian Distri-
bution: The Gaussian mechanism (based on additive Gaussian
noise) is another well-studied DP mechanism. It achieves
what is often called approximate differential privacy, meaning
that the mechanism may fail completely to provide pure DP
with some small and controllable probability δ. However,
the mechanism has advantages over the Laplace mechanism,
including lighter tails than the Laplace distribution and su-
perior composition properties when answering many queries
with independent noise. Generalizations like R´enyi differential
privacy [9] can perform tight composition analysis. Recently
truncated concentrated differential privacy [10] has emerged
as a promising generalization that bounds the residual pri-
vacy loss from approximate DP, allows efﬁciently-computable
optimal composition, and captures privacy ampliﬁcation by
subsampling as present in [11]. Because of these advantages,
the Gaussian mechanism is often the tool of choice for
applications such as deep learning [11] that involve carefully
controlled privacy budgets over sequences of releases.
A question arises: are the same attacks as in [6] possible
against the Gaussian mechanism? Though several works [12],
[13] mention that it may be feasible, to the knowledge of the
authors no one has demonstrated this possibility nor shown
how to carry out this attack in practice. In this paper we study
these two questions and demonstrate an attack conﬁrming that
© 2022, Jiankai Jin. Under license to IEEE.
DOI 10.1109/SP46214.2022.00118
473
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:38:32 UTC from IEEE Xplore.  Restrictions apply. 
common implementations of Gaussian sampling are subject to
ﬂoating-point attacks.
Unfortunately directly using the attack from [6] is not
possible since Gaussian noise is drawn using very different
techniques to Laplace. The main challenge is due to some
Gaussian samplers being based on two random values and
not one. Such methods produce two independent Gaussian
samples, and most implementations cache one of them to be
used next time the mechanism is called. We develop an attack
that uses both of these values. Due to the two equations with
several unknowns that the attacker needs to solve, there can be
more than one pair of feasible values. As a result our attack
can yield false positives. Nevertheless, we show that the attack
is still feasible and has a signiﬁcant success rate. Additionally,
we show that a Gaussian sampler based on a different method
that generates only one sample is also susceptible to a ﬂoating-
point attack, and the attack succeeds at a higher rate than for
samplers based on two values.
The most prominent recent use of the Gaussian mecha-
nism is in training machine learning (ML) algorithms us-
ing differentially-private stochastic gradient descent
(DP-
SGD) [11]. We show that we can mount the attack in this
setting to determine if a batch contains a particular record
or not, violating DP guarantees of DP-SGD. Moreover, ML
model training naturally reveals sequential Gaussian samples
to an adversary, as it returns a noisy gradient for each
parameter of the model.
b) Timing Attacks Against Discrete Distribution Sam-
plers: In the second part of the paper we study the primary
method that has been proposed to defend against ﬂoating-
point attacks: discrete versions of the Laplace and Gaussian
mechanisms [12], [13]. These approaches employ sampling
algorithms that make no use of ﬂoating-point representations.
We observe that such mechanisms, though defending against
ﬂoating-point attacks, are susceptible to a timing side channel:
an adversary who observes the time it takes to draw a sample
can determine the generated noise’s magnitude. When used
within a DP mechanism, our attack reveals the noise contained
in the result, and thus reveals the noiseless (private) value.
Our timing attack is possible due to the underlying tech-
nique that these discrete samplers rely on: direct simulation
of geometric sampling, meaning values are sampled until
a coin toss results in a “head”. The number of such coin
tosses is tied to the magnitude of the noise returned; timing
the sampler reveals this number and thus leaks the noise
magnitude. Though timing has been identiﬁed as a potential
side-channel in DP [7], [8], to our knowledge we are the ﬁrst to
show that noise distribution samplers and not the mechanisms
themselves give rise to secret-dependent runtimes.
• We use the above results to demonstrate empirically that
Gaussian samplers as implemented in NumPy, PyTorch
and Go are vulnerable to our attack. Focusing on the
Opacus DP library implementation [4] by Facebook, we
also show that DP-SGD is vulnerable to information
leakage under our attack. *
• We then observe that discrete methods developed to
protect against ﬂoating-point attacks for both the Laplace
and Gaussian mechanisms suffer from timing side chan-
nels. We show that two libraries are vulnerable to these
attacks: a DP library by Google [3] and the implementa-
tion accompanying another work on discrete distributions
in [12], [14].
• We discuss and evaluate mitigations against each attack.
Disclosure: We have informed maintainers of the DP
libraries mentioned above of the results of this paper. They
have acknowledged our report and notiﬁcation of the disclo-
sure dates.
II. BACKGROUND
In this paper we develop attacks on differential privacy
based on ﬂoating-point representation and timing channels. In
this section, we give background on how ﬂoating-point values
are represented on modern computers, differential privacy, and
the Laplace and Gaussian mechanisms for DP.
A. Floating-Point Representation
Floating-point values represent real values using three num-
bers: a sign bit b, an exponent e, and a signiﬁcand d1d2 . . . dd.
For example, 64-bit (double precision) ﬂoating-point numbers
allocate 1 bit for b, 11 bits for e, and 52 bits for the
signiﬁcand. Such a ﬂoating-point number is deﬁned to be
(−1)b × (1.d1d2 . . . dd)2 × 2e−1023.
Crucially, the number of real values representable using