class evolution). We defer a more in-depth analysis to future
work.
Finally, our evaluation in Section 7 is limited to N = 15
training classes (and 155 previously unseen testing classes).
We limited to N = 15 to make sure each training class has
enough samples to train an accurate original classiﬁer. To
test a larger N, we tried to apply CADE to several other mal-
ware datasets but did not ﬁnd a suitable one that could meet
our need. For example, the Ember-2018 dataset [6] provides
malware samples from a large number of families. However,
the family labels are not well curated. For instance, a pop-
ular malware family name in Ember-2018 is called “high”
(8,417 samples) which turns out to be incorrectly parsed from
VirusTotal reports: the original entry name in the reports is
“Malicious (High Conﬁdence),” which is not a real malware
family name. We have observed other similar parsing errors
and inconsistencies in the labels. The Ember-2017 dataset [6]
and the UCSB packed malware dataset [3] do not provide mal-
ware family information. The dataset from Microsoft Malware
Classiﬁcation Challenge [55] only has 9 malware families,
which is smaller than our Blue Hexagon dataset. Given our
unsuccessful efforts, we defer the examination of a larger
number of training classes to future work.
9 Related work
Machine Learning used in Security. Machine learning
has been used to solve many security problems such as mal-
ware detection [6,7,17,42], malware family attribution [4,11],
and network intrusion detection [24, 34, 48, 60]. More re-
cently, researchers look into using deep learning methods to
perform binary analysis [27, 69], software vulnerability iden-
tiﬁcation [72], and severity prediction [30]. Most of these
machine learning models need to address the concept drift
problem when deployed in practice.
Out of Distribution (OOD) Detection.
Recently, the
machine learning community has made progress in out-of-
distribution detection [14, 32, 41, 46, 49]. These works are
relevant, but have different assumptions and goals compared
to ours. At the high-level, most of these methods try to cal-
ibrate the “probability” produced by the original classiﬁer
to detect OOD samples. The researchers indeed recognized
that the probability could be untrustworthy when it comes to
previously unseen distributions [14, 32]. To avoid assigning
a high probability to an OOD sample, the proposed methods
usually need to introduce an auxiliary OOD dataset to the
training data. These methods are difﬁcult to realize in security
applications for two reasons. First, auxiliary OOD dataset (i.e.,
previously unseen attacks) is extremely difﬁcult to obtain in
the ﬁrst place. Second, these solutions require re-designing
the original classiﬁer (e.g., a functional malware detector),
which is inconvenient to do in the production environment.
Instead, our method does not rely on auxiliary OOD dataset
and is decoupled from the original classiﬁer.
Classiﬁcation Trustworthiness.
A related line of work
aims to assess the trustworthiness of the classiﬁcation re-
sults [11, 37, 50]. A common goal is to identify untrusted
predictions, e.g., predictions on adversarial attacks. Most of
these methods are based on the idea of “nearest neighbors”.
The intuition is, an untrusted prediction is more likely to
have a different label from its nearest neighbors. For example,
DkNN [50] derives a trust score by comparing a testing sam-
ple with its neighboring training samples at each layer of a
Deep Neural Network (DNN). Another recent work [37] com-
pute the trust score based on the neighboring “high-density-
sets”. However, such neighbor-based methods still rely on
a good distance function. As acknowledged in [37], their
method may suffer in a high dimensional space. Overall,
these methods are focused on different problems from ours.
Their goal is to identify misclassiﬁcations within existing
classes (not drifting samples from new classes). Another sys-
tem EC2 [11] uses a threshold of prediction probability to
ﬁlter out untrustworthy predictions. Related to this direction,
active learning methods also use prediction probability to
select low-conﬁdence samples to be labeled for model retrain-
ing [47, 73]. As discussed before (see [32]), the prediction
probability itself can be misleading under concept drift.
2340    30th USENIX Security Symposium
USENIX Association
Machine Learning Explanation.
A collection of recent
works focus on post-hoc interpretation methods for machine
learning classiﬁers [8, 22, 35, 58, 59] and study the robustness
of explanations [15,71]. Given a testing sample, the goal is to
pinpoint important features to explain the classiﬁcation deci-
sions. Most methods are designed for deep neural networks.
For example, perturbation-based methods would subtly manip-
ulate the input and observe the variation of output to identify
important features [13, 18, 21, 22]. Gradient-based methods
(e.g., saliency maps) back-propagate gradients through the
deep neural network to measure the sensitivity of each fea-
ture [56,58,59,61]. Other explanation methods treat the target
classiﬁer as a blackbox [53, 54]. Systems such as LIME [53],
LEMNA [28], and SHAP [44] try to use a simpler model (e.g.,
linear regression) to approximate the decision boundary near
the input sample, and then use the simpler model to pinpoint
features to generate the explanations.
Our method falls into the category of perturbation-based
method. A key difference is existing methods are designed for
supervised classiﬁers and try to explain the decision bound-
aries. Our method is focusing on explaining distance changes,
which are more suitable for outlier detection. Only a few
works aim to explain unsupervised models [19, 43]. We used
COIN [43] as a baseline in our evaluation, and showed the
advantage of distance-based explanation.
10 Conclusion
In this paper, we build a novel system CADE to complement
supervised classiﬁers to combat concept drift in security con-
texts. Using a contrastive autoencoder and a distance-based
explanation method, CADE is designed to detect drifting sam-
ples that deviate from the original training distribution and
provide the corresponding explanations to reason the meaning
of the drift. Using various datasets, we show that CADE out-
performs existing methods. Working with an industry partner,
we demonstrate CADE’s ability to detect and explain drifting
samples from previously unseen families.
Acknowledgment
We thank our shepherd David Freeman and anonymous re-
viewers for their constructive comments and suggestions. This
work was supported in part by NSF grants CNS-2030521 and
CNS-1717028, and Amazon Research Award.
References
[1] Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis,
Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving,
Michael Isard, et al. Tensorﬂow: A system for large-scale machine
learning. In Proc. of USENIX OSDI, 2016.
[3] Hojjat Aghakhani, Fabio Gritti, Francesco Mecca, Martina Lindorfer,
Stefano Ortolani, Davide Balzarotti, Giovanni Vigna, and Christopher
Kruegel. When malware is packin’heat; limits of machine learning
classiﬁers based on static analysis features. In Proc. of NDSS, 2020.
[4] Mansour Ahmadi, Dmitry Ulyanov, Stanislav Semenov, Mikhail Troﬁ-
mov, and Giorgio Giacinto. Novel feature extraction, selection and
In Proc. of CO-
fusion for effective malware family classiﬁcation.
DASPY, 2016.
[5] Bruce An. More adware and plankton variants seen in app stores.
TrendMicro, 2012.
[6] Hyrum S Anderson and Phil Roth. Ember: an open dataset for
training static pe malware machine learning models. arXiv preprint
arXiv:1804.04637, 2018.
[7] Daniel Arp, Michael Spreitzenbarth, Malte Hubner, Hugo Gascon, Kon-
rad Rieck, and CERT Siemens. Drebin: Effective and explainable
detection of android malware in your pocket. In Proc. of NDSS, 2014.
[8] Sebastian Bach, Alexander Binder, Grégoire Montavon, Frederick
Klauschen, Klaus-Robert Müller, and Wojciech Samek. On pixel-wise
explanations for non-linear classiﬁer decisions by layer-wise relevance
propagation. PloS one, 2015.
[9] Manuel Baena-Garcıa, José del Campo-Ávila, Raúl Fidalgo, Albert
Bifet, R Gavalda, and R Morales-Bueno. Early drift detection method.
In Fourth international workshop on knowledge discovery from data
streams, 2006.
[10] Albert Bifet and Ricard Gavalda. Learning from time-changing data
with adaptive windowing. In Proc. of SDM, 2007.
[11] Tanmoy Chakraborty, Fabio Pierazzi, and VS Subrahmanian. Ec2:
Ensemble clustering and classiﬁcation for predicting android malware
families. TDSC, 2017.
[12] Eshwar Chandrasekharan, Mattia Samory, Anirudh Srinivasan, and Eric
Gilbert. The bag of communities: Identifying abusive behavior online
with preexisting internet data. In Proc. of CHI, 2017.
[13] Chun-Hao Chang, Elliot Creager, Anna Goldenberg, and David Duve-
naud. Explaining image classiﬁers by counterfactual generation. In
Proc. of ICLR, 2019.
[14] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Ro-
bust out-of-distribution detection in neural networks. arXiv preprint
arXiv:2003.09711, 2020.
[15] Jiefeng Chen, Xi Wu, Vaibhav Rastogi, Yingyu Liang, and Somesh Jha.
Robust attribution regularization. In Proc. of NeurIPS, 2019.
[16] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hin-
ton. A simple framework for contrastive learning of visual representa-
tions. arXiv:2002.05709, 2020.
[17] Yizheng Chen, Shiqi Wang, Dongdong She, and Suman Jana. On
training robust pdf malware classiﬁers. In Proc. of USENIX Security,
2020.
[18] Piotr Dabkowski and Yarin Gal. Real time image saliency for black
box classiﬁers. In Proc. of NeurIPS, 2017.
[19] Xuan Hong Dang, Ira Assent, Raymond T Ng, Arthur Zimek, and
Erich Schubert. Discriminative features for identifying and interpreting
outliers. In Proc. of ICDE, 2014.
[20] Denis Moreira dos Reis, Peter Flach, Stan Matwin, and Gustavo Batista.
Fast unsupervised online drift detection using incremental kolmogorov-
smirnov test. In Proc. of KDD, 2016.
[21] Ruth C Fong, Mandela Patrick, and Andrea Vedaldi. Understanding
deep networks via extremal perturbations and smooth masks. In Proc.
of ICCV, 2019.
[2] Hervé Abdi and Lynne J. Williams. Principal component analysis.
WIREs Computational Statistics, 2010.
[22] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black
boxes by meaningful perturbation. In Proc. of ICCV, 2017.
USENIX Association
30th USENIX Security Symposium    2341
[23] João Gama, Indr˙e Žliobait˙e, Albert Bifet, Mykola Pechenizkiy, and
Abdelhamid Bouchachia. A survey on concept drift adaptation. ACM
computing surveys (CSUR), 2014.
[24] Pedro Garcia-Teodoro, Jesus Diaz-Verdejo, Gabriel Maciá-Fernández,
and Enrique Vázquez. Anomaly-based network intrusion detection:
Techniques, systems and challenges. Computers & Security, 2009.
[25] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining
and harnessing adversarial examples. Proc. of ICLR, 2015.
[26] Antonio Gulli and Sujit Pal. Deep learning with Keras. 2017.
[27] Wenbo Guo, Dongliang Mu, Xinyu Xing, Min Du, and Dawn Song.
Deepvsa: Facilitating value-set analysis with deep learning for post-
mortem program analysis. In Proc. of USENIX Security, 2019.
[28] Wenbo Guo, Dongliang Mu, Jun Xu, Purui Su, Gang Wang, and Xinyu
Xing. Lemna: Explaining deep learning based security applications. In
Proc. of CCS, 2018.
[29] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduc-
tion by learning an invariant mapping. In Proc. of CVPR, 2006.
[30] Zhuobing Han, Xiaohong Li, Zhenchang Xing, Hongtao Liu, and Zhiy-
ong Feng. Learning to predict severity of software vulnerability using
only vulnerability description. In Proc. of ICSME, 2017.
[31] Maayan Harel, Shie Mannor, Ran El-Yaniv, and Koby Crammer. Con-
cept drift detection through resampling. In Proc. of ICML, 2014.
[32] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassi-
ﬁed and out-of-distribution examples in neural networks. In Proc. of
ICLR, 2017.
[33] Geoffrey E Hinton and Ruslan R Salakhutdinov. Reducing the dimen-
sionality of data with neural networks. Science, 2006.
[34] Elike Hodo, Xavier Bellekens, Andrew Hamilton, Christos Tachtatzis,
and Robert Atkinson. Shallow and deep networks intrusion detection
system: A taxonomy and survey. arXiv preprint arXiv:1701.02145,
2017.
[35] Sara Hooker, Dumitru Erhan, Pieter-Jan Kindermans, and Been Kim.
A benchmark for interpretability methods in deep neural networks. In
Proc. of NeurIPS, 2019.
[36] Steve TK Jan, Qingying Hao, Tianrui Hu, Jiameng Pu, Sonal Oswal,
Gang Wang, and Bimal Viswanath. Throwing darts in the dark? detect-
ing bots with limited data using neural data augmentation. In Proc. of
S&P, 2020.
[37] Heinrich Jiang, Been Kim, Melody Guan, and Maya Gupta. To trust or
not to trust a classiﬁer. In Proc. of NeurIPS, 2018.
[38] Roberto Jordaney, Kumar Sharad, Santanu K Dash, Zhi Wang, Davide
Papini, Ilia Nouretdinov, and Lorenzo Cavallaro. Transcend: Detecting
concept drift in malware classiﬁcation models. In Proc. of USENIX
Security, 2017.
[39] Alex Kantchelian, Sadia Afroz, Ling Huang, Aylin Caliskan Islam,
Brad Miller, Michael Carl Tschantz, Rachel Greenstadt, Anthony D.
Joseph, and J. D. Tygar. Approaches to adversarial drift. In Proc. of
AISec, 2013.
[40] Christophe Leys, Christophe Ley, Olivier Klein, Philippe Bernard, and
Laurent Licata. Detecting outliers: Do not use standard deviation
around the mean, use absolute deviation around the median. Journal of
Experimental Social Psychology, 2013.
[41] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the
reliability of out-of-distribution image detection in neural networks.
Proc. of ICLR, 2018.
[42] Martina Lindorfer, Matthias Neugschwandtner, and Christian Platzer.
Marvin: Efﬁcient and comprehensive mobile app classiﬁcation through
static and dynamic analysis. In Prof. of COMPSAC, 2015.
[43] Ninghao Liu, Donghwa Shin, and Xia Hu. Contextual outlier interpre-
tation. In Proc. of IJCAI, 2018.
[44] Scott M. Lundberg and Su-In Lee. A uniﬁed approach to interpreting
model predictions. In Proc. of NeurIPS, 2017.
[45] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete
distribution: A continuous relaxation of discrete random variables. In
Proc. of ICLR, 2017.
[46] Marc Masana, Idoia Ruiz, Joan Serrat, Joost van de Weijer, and Anto-
nio M Lopez. Metric learning for novelty and anomaly detection. In
Proc. of BMVC, 2018.
[47] Brad Miller, Alex Kantchelian, Sadia Afroz, Rekha Bachwani, Edwin
Dauber, Ling Huang, Michael Carl Tschantz, Anthony D. Joseph, and
J.D. Tygar. Adversarial active learning. In Proc. of AISec, 2014.
[48] Yisroel Mirsky, Tomer Doitshman, Yuval Elovici, and Asaf Shabtai.
Kitsune: an ensemble of autoencoders for online network intrusion
detection. In Proc. of NDSS, 2018.
[49] Aristotelis-Angelos Papadopoulos, Mohammad Reza Rajati, Nazim
Shaikh, and Jiamian Wang. Outlier exposure with conﬁdence control
for out-of-distribution detection. arXiv preprint arXiv:1906.03509,
2019.
[50] Nicolas Papernot and Patrick McDaniel. Deep k-nearest neighbors: To-
wards conﬁdent, interpretable and robust deep learning. arXiv preprint
arXiv:1803.04765, 2018.
[51] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion,
O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vander-
plas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duch-
esnay. Scikit-learn: Machine learning in Python. Journal of Machine
Learning Research, 2011.
[52] Feargus Pendlebury, Fabio Pierazzi, Roberto Jordaney, Johannes Kinder,
and Lorenzo Cavallaro. TESSERACT: Eliminating experimental bias
in malware classiﬁcation across space and time. In Proc. of USENIX
Security, 2019.
[53] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. "why should
i trust you?" explaining the predictions of any classiﬁer. In Proc. of
KDD, 2016.
[54] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Anchors:
High-precision model-agnostic explanations. In Proc. of AAAI, 2018.
[55] Royi Ronen, Marian Radu, Corina Feuerstein, Elad Yom-Tov, and Man-
sour Ahmadi. Microsoft malware classiﬁcation challenge. arXiv
preprint arXiv:1802.10135, 2018.
[56] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakr-
ishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual
explanations from deep networks via gradient-based localization. In
Proc. of ICCV, 2017.
[57] Iman Sharafaldin, Arash Habibi Lashkari, and Ali A Ghorbani. To-
ward generating a new intrusion detection dataset and intrusion trafﬁc
characterization. In Prof. of ICISSP, 2018.
[58] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning
important features through propagating activation differences. In Proc.
of ICML, 2017.
[59] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside
convolutional networks: Visualising image classiﬁcation models and
saliency maps. Workshop at ICLR, 2014.
[60] Robin Sommer and Vern Paxson. Outside the closed world: On using
machine learning for network intrusion detection. In Proc. of S&P,