watching γ. After the model is updated, we reset the false
positive rate to zero and record it again. Another situation
is that when there is a new pre-dominating node Bi (i > t)
containing the balanced labeled data, it is the time to update
the model with the new data (see Section 4.3). In this way, the
model could learn new features from the inputs which let the
program execute to new code that has never been touched. Us-
ing this approach, we could ensure the accuracy of the model
while keeping the model updating at a reasonable frequency.
To avoid missing a PoC (i.e., to avoid ﬁltering out any PoC),
we temporarily store unreachable inputs in the PUI. When the
model is updated, we use the new model to check the inputs in
the PUI again, and pick out the reachable ones for execution.
Based on our evaluation, the model is accurate enough that
no PoC is missed.
Implementation
5
In this section, we describe the implementation of FuzzGuard,
including model initialization, model prediction, model up-
dating and the details of the deployment of FuzzGuard.
Model Initialization. At the initial stage, FuzzGuard starts
to train the model only after enough data are collected; and
continues to update the model after another set of data (not a
single input) are collected. Such data should be balanced (i.e.,
the number of reachable inputs is similar to the number of
unreachable ones). Particularly, before the model is trained,
all the inputs should be fed into the target program for real
execution. FuzzGuard records the reachability of the inputs.
Once enough2 balanced data are gained, FuzzGuard starts to
train the model. Then it utilizes the trained model to predict
the reachability of a newly generated input, executes the tar-
get program if it is reachable, and records the reachability in
real execution. Such data are collected to update the model
for better performance. As mentioned before, DGF requires
a target (potential) buggy code whose location is known for
fuzzing. To set the pre-dominating nodes of the buggy node,
we generate Call Graph (CG) and Control Flow Graph (CFG)
of the target program and set the pre-dominating node ac-
cording to the deﬁnition as mentioned in Section 4. In our
implementation, we use NetworkX [6] to automatically ﬁnd
the pre-dominating nodes from the CG and CFG generated
by LLVM.
Model Prediction and Updating. To further collect data for
updating the model, we set the θs for SSD to 0.85 and the
default sampling rate is 5% in each round of mutation. When
SSD exceeds the threshold, the sampling rate will decrease
to (1− θs)/5 (i.e., less than 3%). Based on our evaluation,
setting the threshold using this value has the best performance.
Considering that models’ accuracy varies a lot for different
2In our implementation, FuzzGuard starts to train the model after 50
thousand balanced inputs are gained. The number is far less than the number
of the inputs generated by AFLGo in a testing task (usually 30 million).
USENIX Association
29th USENIX Security Symposium    2261
f ault ← 0
Algorithm 1 Function Checker()
Input: argv, timeout and input
1:
2: if check(input) is reachable then
3:
4:
5:
6: end if
7: return f ault
f ault ← run_target(argv,timeout)
label ← check_trace(input)
send(label)
programs, we dynamically change θ f according to the previ-
ous executions: θ f = 1− accavg, where accavg represents the
average accuracy of the models updated previously.
Model Implementation. For the training model, we imple-
ment a CNN model using PyTorch [7]. It contains three 1-
dimensional convolution layers (k = 3, stride = 1). Note that
the 1-dimensional convolution layer takes each input as a row
sequence; and each row has 1024 bytes. Each convolution
layer is followed by a pooling layer and a ReLU [4] as the ac-
tivation function. We also have Dropout layers (disabling rate
= 20%) to avoid over-ﬁtting of the neural networks. There is a
fully-connected layer at the end of the neural networks, which
is used to score the reachabilities of each node in the target
path to the buggy code. Also we use the Adam optimizer [21]
to help the learning function converge to the optimal solution
rapidly and stably. The training process ends when the loss
value of the learning function becomes stable.
Deployment of FuzzGuard. To achieve the data sharing, we
add a function Checker() to aﬂ-fuzz.c in AFLGo. Algorithm 1
shows the details of Checker(). The function Checker() han-
dles all parameters in run_target() (i.e., argv, timeout in Al-
gorithm 1) and receive an input which is saved in a piece of
memory. Before the input is fed into the target program, it is
sent to FuzzGuard (i.e., check(input) at line 2 in Algorithm 1).
Only when FuzzGuard returns with the result showing that
the execution path is reachable, the target program is exe-
cuted with the input (line 3 in Algorithm 1). After executing
the target program, Checker() reads the reachability of the
input from the function check_trace() (Line 4 in Algorithm 1)
and sends it to FuzzGuard for further learning (line 5 in Al-
gorithm 1). We plan to release our FuzzGuard for helping
researchers in the community.
6 Evaluation
In this section, we evaluate the effectiveness of FuzzGuard
with 45 vulnerabilities. The results are compared with a
vanilla AFLGo. According to the experiment results, Fuz-
zGuard boosts the performance of fuzzing up to 17.1 times
faster than that of AFLGo. Then we provide an understanding
of the performance boost and break down the performance
overhead of FuzzGuard. We also analyze the accuracy of
FuzzGuard and show our ﬁndings.
6.1 Settings
We ﬁrst selected 15 real-world programs handling 10 common
ﬁle formats, including network packages (e.g., PCAP), videos
(e.g., MP4, SWF), texts (e.g., PDF, XML), images (e.g., PNG,
WEBP, JP2, TIFF) and compressed ﬁles (e.g., ZIP). Unfor-
tunately, three programs (i.e., mupdf, rzip, zziplib) cannot
be compiled3, and two programs (i.e., apache, nginx) do not
give the details of vulnerabilities. So we chose the rest 10 as
the target programs and the corresponding bugs in the past 3
years4. Table 1 shows the details of each vulnerability, includ-
ing program names and line numbers of the vulnerable code
(the column Vuln. Code). For different input formats, we use
the test cases provided by AFLGo as the initial seed ﬁles to
start fuzzing (we believe that AFLGo will perform well using
the initial seed ﬁles chosen by itself). All the experiments and
measurements are performed on two 64-bit servers running
Ubuntu 16.04 with 16 cores (Intel(R) Xeon(R) CPU E5-2609
v4 @ 1.70GHz), 64GB memory and 3TB hard drive and 2
GPUs (12GB Nvidia GPU TiTan X) with CUDA 8.0.
6.2 Effectiveness
To show the effectiveness of FuzzGuard, we evaluate AFLGo
equipped with FuzzGuard and the original one using 45 vul-
nerabilities in 10 real programs (as demonstrated in Table 1).
The ideal comparison for the AFLGo equipped with Fuz-
zGuard and the vanilla AFLGo is to compare the time of
fuzzing using AFLGo (TAFLGo) and the corresponding time
when equipping AFLGo with FuzzGuard (T+FG). However,
we cannot directly use the same seed input to compare the
fuzzing process of AFLGo and that of AFLGo+FuzzGuard.
This is because the mutation is random, and the generated
sequence of inputs (even if from the same seed input) could
be quite different in the two fuzzing processes, which further
makes the time spent on execution quite different. So our idea
is to make the generated sequence of inputs be the same in
the two different fuzzing processes. Particularly, for a vul-
nerability of a target program, we use a vanilla AFLGo to
perform fuzzing and record the sequence of all the mutated
inputs IAFLGo in order (the number of the inputs NInputs is
shown in Table 1) until the target vulnerability is triggered
(e.g., a crash) or timeout (200 hours in our evaluation). In
this process, the fuzzing time TAFLGo (as shown in Table 1) is
also recorded. Then we utilize the same sequence of inputs
IAFLGo to test AFLGo equipped with FuzzGuard, recording
the ﬁltered inputs If iltered (the number of the ﬁltered inputs
is Nf iltered, and the ratio of ﬁltered inputs to all the generated
inputs f iltered = Nf iltered/NInputs are shown in Table 1). We
also record the time cost of FuzzGuard (TFG) including the
time of training and prediction. In this way, we are able to
know the time when FuzzGuard is equipped, and compare the
3We tried to ﬁx the compile errors (e.g., missing libraries). However, due
to too many errors, it is very hard to ﬁx all the errors.
4We excluded 5 vulnerabilities (out of 50) triggered in minitues by
AFLGo. Obviously, there is no need to utilize FuzzGuard to speedup.
2262    29th USENIX Security Symposium
USENIX Association
Table 1: Effectiveness of FuzzGuard.
Vuln. Code
Program
Ap4AvccAtom.cpp:83
Bento4 v1.5.1.0
ec_strings.c:182
Ettercap v0.8.2
tiff.c:2375
GraphicsMagick v1.3.31
png.c:6945
GraphicsMagick v1.3.31
png.c:7503
GraphicsMagick v1.3.31
png.c:5007
GraphicsMagick v1.3.31
GraphicsMagick v1.3.30
png.c:3810
GraphicsMagick v1.3.27 webp.c:716
png.c:7061
GraphicsMagick v1.3.26
GraphicsMagick v1.3.26
tiff.c:2433
rle.c:753
GraphicsMagick v1.3.26
GraphicsMagick v1.3.26
list.c:232
ImageMagick v7.0.8-13 msl.c:8353
ImageMagick v7.0.8-3
dib.c:1306
ImageMagick v7.0.8-3
bmp.c:2062
ImageMagick v7.0.7-16 webp.c:769
ImageMagick v7.0.7-16 webp.c:403
tiff.c:1934
ImageMagick v7.0.7-1
bmp.c:894
ImageMagick v7.0.5-5
Jasper v2.0.14
jp2_enc.c:309
jpc_dec.c:1700
Jasper v2.0.10
jpc_dec.c:1881
Jasper v2.0.10
jas_seq.c:254
Jasper v2.0.10
decompile.c:1930
Libming v0.4.8
Libming v0.4.7
parser.c:1645
parser.c:64
Libming v0.4.7
parser.c:3381
Libming v0.4.7
parser.c:3095
Libming v0.4.7
parser.c:2993
Libming v0.4.7
Libming v0.4.7
parser.c:3126
parser.c:3232
Libming v0.4.7
parser.c:3221
Libming v0.4.7
parser.c:3250
Libming v0.4.7
parser.c:3089
Libming v0.4.7
Libming v0.4.7
parser.c:3061
parser.c:3071
Libming v0.4.7
parser.c:3209
Libming v0.4.7
outputtxt.c:143
Libming v0.4.7
tif_dirwrite.c:1901
Libtiff v4.0.9
Libtiff v4.0.7
tif_swab.c:289
tiffcp.c:1386
Libtiff v4.0.7
tif_read.c:346
Libtiff v4.0.7
SAX2.c:2035
Libxml2 v2.9.4
PdfPainter.cpp:1945
Podofo v0.9.5
Tcpreplay v4.3.0-beta1
get.c:174
No.
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
Avg.
NInputs
UR.
NFunctions NConstraints
1.8 M 38.1%
1.8 K
676
49.2 M 99.0%
41.5 K
420
32.1 M 95.9%
170.3 K
3.3 K
30 M 96.6%
319.8 K
4.9 K
16.4 M 99.9%
21.9 K
1.5 K
16 M 99.9%
317.1 K
4.4 K
22.6 M 99.9%
168.4 K
3.1 K
67.5 M 99.5%
749.3 K
10.7 K
56.9 M 98.4%
320 K
4.9 K
78.4 M 75.3%