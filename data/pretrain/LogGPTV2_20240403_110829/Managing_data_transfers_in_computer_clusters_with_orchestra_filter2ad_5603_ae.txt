broadcast and shufﬂing algorithms. In these experiments, we com-
pare the applications before and after adopting Orchestra. In par-
ticular, “before” entails running with a HDFS-based broadcast im-
plementation (with the default 3× replication) and a shufﬂe with
5 threads per receiver (the default in Hadoop). Meanwhile, “after”
entails Cornet and shufﬂe with 30 threads per receiver.
Figure 16 illustrates the breakdown of time spent in different ac-
tivities in each iteration of Monarch in a 30 node EC2 cluster. We
7For n concurrent transfers, theoretical maximum speedup of the
average completion time using FIFO over fair sharing is 2n
n+1×.
0%20%40%60%80%100%051015202530354045% of active flows Time(s) Low Priority Job 0High Priority Job 1High Priority Job 2High Priority Job 30%20%40%60%80%100%051015202530354045% of active flows Time(s) No ITCFIFOFIFO+01020304050607080Time (s)No ITCFIFOFIFO+01020304050607080Time (s)Job 4Job 3Job 2Job 1020406080100120Time (s)AfterBeforeComputationShuffleBroadcastScheduling and Management in Data-intensive Applications.
A plethora of schemes exist to schedule and manage tasks of data-
intensive applications. Examples include fair schedulers for Hadoop
[43] and Dryad [29], and Mantri [11] for outlier detection. The core
tenet of existing work in this area is achieving data locality to avoid
network transfers as much as possible. Mesos [26] provides a thin
management layer to allow diverse cluster computing frameworks
to efﬁciently share computation and storage resources, but leaves
sharing of network resources to underlying transport mechanisms.
Orchestra complements these systems by enabling the implemen-
tation of network sharing policies across applications.
One-to-many Data Transfer Mechanisms. Broadcast, multicast,
and diverse group communication mechanisms in application and
lower layers of the network stack have been studied extensively in
the literature. Diot et al. provide a comprehensive survey and tax-
onomy of relevant protocols and mechanisms of distributed multi-
point communication in [16]. Cornet is designed for transferring
large amounts of data in high-speed datacenter networks.
SplitStream [13] improves network utilization and tackles the
bottleneck problem observed in d-ary trees by creating multiple
distribution trees with disjoint leave sets. However, it is designed
primarily for multimedia streaming over the Internet, where frames
can be dropped. Maintaining its structural constraints in presence
of failure is complicated as well.
BitTorrent [4] is wildly popular for ﬁle-sharing. BitTorrent and
similar peer-to-peer mechanisms are in use to distribute planet-
scale software updates [20]. However, Murder [8] is the only known
BitTorrent deployment inside a datacenter. Antfarm [36] uses a
central coordinator across multiple swarms to optimize content dis-
tribution over the Internet. Cornet is a BitTorrent-like system that
is optimized for datacenters and uses adaptive clustering algorithm
in the TC to infer and take advantage of network topologies.
Incast or Many-to-one Transfers. TCP incast collapse is typi-
cally observed in barrier-synchronized request workloads where a
receiver synchronously receives small amounts of data from a large
number of senders [41]. However, incast collapse has been re-
ported in MapReduce-like data-intensive workloads as well [14].
The latter case boils down to a special case of shufﬂe with only one
reducer. With Orchestra, the TC can effectively limit how many
senders are simultaneously sending and at what rate to alleviate
this problem for data-intensive workloads.
Inferring Topology from Node-to-Node Latencies. Inferring node
topology in CornetClustering (Section 5.3) is similar in spirit to in-
ferring network coordinates [17]. These methods could act as a
substitute for the non-metric multidimensional scaling step in the
CornetClustering procedure.
9 Conclusion
We have argued that multi-node transfer operations have a signif-
icant impact on the performance of data-intensive cluster applica-
tions and presented an architecture called Orchestra that enables
global control both across and within transfers to optimize per-
formance. We focused on two common transfer patterns: broad-
casts and shufﬂes. For broadcasts, we proposed a topology-aware
BitTorrent-like scheme called Cornet that outperforms the status
quo in Hadoop by 4.5×. For shufﬂes, we proposed an optimal al-
gorithm called Weighted Shufﬂe Scheduling (WSS). Overall, our
schemes can increase application performance by up to 1.9×. In
addition, we demonstrated that inter-transfer scheduling can im-
prove the performance of high-priority transfers by 1.7× and re-
duce average transfer times by 31%. Orchestra can be implemented
(a) Before
(b) After
Figure 17: Per-iteration completion times when scaling the col-
laborative ﬁltering application using Orchestra.
see that its communication overhead in each iteration decreased
from 42% of the run time to 28%, and iterations ﬁnished 22% faster
overall. There is a 2.3× speedup in broadcast and a 1.23× speedup
in shufﬂe. The improvements for both broadcast and shufﬂe are in
line with the ﬁndings in Sections 7.1 and 6.2.
Figure 17(b) presents the per-iteration completion times for the
collaborative ﬁltering job while scaling it up to 90 nodes using
Cornet. Unlike the HDFS-based solution (Figure 17(a)), broad-
cast time increased from 13.4 to only 15.3 seconds using Cornet.
As a result, the job could be scaled up to 90 nodes with 1.9× im-
provement in iteration times. The average time spent in broadcast
decreased by 3.6×, from 55.8s to 15.3s, for 90 nodes. These results
are in line with Section 7.1 given 385 MB broadcast per iteration.
8 Related Work
Full Bisection Bandwidth Datacenter Networks. A large num-
ber of new datacenter network architectures have been proposed
in recent years [9, 21, 23, 24, 35] to achieve full bisection band-
width and improved network performance. However, full bisection
bandwidth does not mean inﬁnite bisection bandwidth. Orchestra is
still valuable in full bisection bandwidth networks to enable inter-
transfer prioritization and scheduling, to balance shufﬂe transfer
rates using WSS, and to speed up broadcasts using Cornet. For ex-
ample, the experiments in Section 7 show that Orchestra improves
job performance even on EC2’s network, which appears to have
near-full bisection bandwidth.
Centralized Network Controllers. Centralized controllers for rout-
ing, access control, and load balancing in the network had been pro-
posed by the 4D architecture [22] and projects like Tesseract [42],
Ethane [12], PLayer [30], and Hedera [10]. While PLayer and
Ethane focus on access control, our primary objective is application-
level performance improvement. The scope of our work is lim-
ited to shared clusters and datacenters, whereas 4D, Tesseract, and
Ethane are designed for wide-area and enterprise networks. How-
ever, unlike Hedera or any of the existing proposals for centralized
control planes, we work at the granularity of transfers to optimize
overall application performance, and not at the packet or ﬂow level.
Performance Isolation in Datacenter Networks. Seawall [38]
performs weighted fair sharing among cloud tenants running ar-
bitrary numbers of TCP and UDP ﬂows through a shim layer at the
hypervisor using a cross-ﬂow AIMD scheme. It can be leveraged
by Orchestra to enforce inter-transfer scheduling policies. How-
ever, Seawall itself is not aware of transfer-level semantics.
10306090Number of machines050100150200250Time (s)Comp.Broadcast10306090Number of machines050100150200250Time (s)Comp.Broadcastat the application level and does not require hardware changes to
run in current datacenters and in the cloud.
Acknowledgments
We thank the AMPLab members, the anonymous reviewers, and
our shepherd, Yinglian Xie for useful comments on the paper, and
Michael Armbrust, Jon Kuroda, Keith Sklower, and the Spark team
for infrastructure support. This research was supported in part by
gifts from AMPLab founding sponsors Google and SAP, AMPLab
sponsors Amazon Web Services, Cloudera, Huawei, IBM, Intel,
Microsoft, NEC, NetApp, and VMWare, and by matching funds
from the State of California’s MICRO program (grants 06-152, 07-
010), the National Science Foundation (grants CNS-0509559 and
CNS-1038695), the University of California Industry/University
Cooperative Research Program (UC Discovery) grant COM07-10240,
and the Natural Sciences and Engineering Research Council of
Canada.
10 References
[1] Amazon EC2. http://aws.amazon.com/ec2.
[2] Apache Hadoop. http://hadoop.apache.org.
[3] BitTornado. http://www.bittornado.com.
[4] BitTorrent. http://www.bittorrent.com.
[5] DETERlab. http://www.isi.deterlab.net.
[6] Fragment replicate join – Pig wiki.
http://wiki.apache.org/pig/PigFRJoin.
[7] LANTorrent. http://www.nimbusproject.org.
[8] Murder. http://github.com/lg/murder.
[9] H. Abu-Libdeh, P. Costa, A. Rowstron, G. O’Shea, and A. Donnelly.
Symbiotic routing in future data centers. In SIGCOMM, pages
51–62, 2010.
[10] M. Al-Fares, S. Radhakrishnan, B. Raghavan, N. Huang, and
A. Vahdat. Hedera: Dynamic ﬂow scheduling for data center
networks. In NSDI, 2010.
[11] G. Ananthanarayanan, S. Kandula, A. Greenberg, I. Stoica, Y. Lu,
B. Saha, and E. Harris. Reining in the outliers in mapreduce clusters
using Mantri. In OSDI, 2010.
[12] M. Casado, M. J. Freedman, J. Pettit, J. Luo, N. McKeown, and
S. Shenker. Ethane: Taking control of the enterprise. In SIGCOMM,
pages 1–12, 2007.
[13] M. Castro, P. Druschel, A.-M. Kermarrec, A. Nandi, A. Rowstron,
and A. Singh. Splitstream: high-bandwidth multicast in cooperative
environments. In SOSP, 2003.
[14] Y. Chen, R. Grifﬁth, J. Liu, R. H. Katz, and A. D. Joseph.
Understanding TCP incast throughput collapse in datacenter
networks. In WREN, pages 73–82, 2009.
[15] J. Dean and S. Ghemawat. MapReduce: Simpliﬁed data processing
on large clusters. In OSDI, pages 137–150, 2004.
[16] C. Diot, W. Dabbous, and J. Crowcroft. Multipoint communication:
A survey of protocols, functions, and mechanisms. IEEE JSAC,
15(3):277 –290, 1997.
[17] B. Donnet, B. Gueye, and M. A. Kaafar. A Survey on Network
Coordinates Systems, Design, and Security. IEEE Communication
Surveys and Tutorials, 12(4), Oct. 2010.
[18] C. Fraley and A. Raftery. MCLUST Version 3 for R: Normal mixture
modeling and model-based clustering. Technical Report 504,
Department of Statistics, University of Washington, Sept. 2006.
[19] P. Ganesan and M. Seshadri. On cooperative content distribution and
the price of barter. In ICDCS, 2005.
[20] C. Gkantsidis, T. Karagiannis, and M. VojnoviC. Planet scale
software updates. In SIGCOMM, pages 423–434, 2006.
[21] A. Greenberg, J. R. Hamilton, N. Jain, S. Kandula, C. Kim, P. Lahiri,
D. A. Maltz, P. Patel, and S. Sengupta. VL2: A scalable and ﬂexible
data center network. In SIGCOMM, 2009.
[22] A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers, J. Rexford,
G. Xie, H. Yan, J. Zhan, and H. Zhang. A clean slate 4D approach to
network control and management. SIGCOMM CCR, 35:41–54, 2005.
[23] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang,
and S. Lu. BCube: A high performance, server-centric network
architecture for modular data centers. In SIGCOMM, pages 63–74,
2009.
[24] C. Guo, H. Wu, K. Tan, L. Shi, Y. Zhang, and S. Lu. DCell: A
scalable and fault-tolerant network structure for data centers. In
SIGCOMM, pages 75–86, 2008.
[25] T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical
Learning: Data Mining, Inference, and Prediction. Springer, New
York, NY, 2009.
[26] B. Hindman, A. Konwinski, M. Zaharia, A. Ghodsi, A. Joseph,
R. Katz, S. Shenker, and I. Stoica. Mesos: A Platform for
Fine-Grained Resource Sharing in the Data Center. In NSDI, 2011.
[27] U. Hoelzle and L. A. Barroso. The Datacenter as a Computer: An
Introduction to the Design of Warehouse-Scale Machines. Morgan
and Claypool Publishers, 1st edition, 2009.
[28] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad:
Distributed data-parallel programs from sequential building blocks.
In EuroSys, pages 59–72, 2007.
[29] M. Isard, V. Prabhakaran, J. Currey, U. Wieder, K. Talwar, and
A. Goldberg. Quincy: Fair scheduling for distributed computing
clusters. In SOSP, 2009.
[30] D. A. Joseph, A. Tavakoli, and I. Stoica. A policy-aware switching
layer for data centers. In SIGCOMM, 2008.
[31] J. B. Kruskal and M. Wish. Multidimensional Scaling. Sage
University Paper series on Quantitative Applications in the Social
Sciences, 07-001, 1978.
[32] G. Malewicz, M. H. Austern, A. J. Bik, J. C. Dehnert, I. Horn,
N. Leiser, and G. Czajkowski. Pregel: A system for large-scale graph
processing. In SIGMOD, 2010.
[33] Y. Mao and L. K. Saul. Modeling Distances in Large-Scale Networks
by Matrix Factorization. In IMC, 2004.
[34] D. G. Murray, M. Schwarzkopf, C. Smowton, S. Smith,
A. Madhavapeddy, and S. Hand. Ciel: A Universal Execution Engine
for Distributed Data-Flow Computing. In NSDI, 2011.
[35] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri,
S. Radhakrishnan, V. Subramanya, and A. Vahdat. PortLand: A
scalable fault-tolerant layer 2 data center network fabric. In
SIGCOMM, pages 39–50, 2009.
[36] R. Peterson and E. G. Sirer. Antfarm: Efﬁcient content distribution
with managed swarms. In NSDI, 2009.
[37] B. Pfaff, J. Pettit, K. Amidon, M. Casado, T. Koponen, and
S. Shenker. Extending networking into the virtualization layer. In
HotNets 2009.
[38] A. Shieh, S. Kandula, A. Greenberg, and C. Kim. Sharing the data
center network. In NSDI, 2011.
[39] D. B. Shmoys. Cut problems and their application to
divide-and-conquer, chapter 5, pages 192–235. PWS Publishing Co.,
Boston, MA, USA, 1997.
[40] K. Thomas, C. Grier, J. Ma, V. Paxson, and D. Song. Design and
evaluation of a real-time URL spam ﬁltering service. In IEEE
Symposium on Security and Privacy, 2011.
[41] V. Vasudevan, A. Phanishayee, H. Shah, E. Krevat, D. G. Andersen,
G. R. Ganger, G. A. Gibson, and B. Mueller. Safe and effective
ﬁne-grained TCP retransmissions for datacenter communication. In
SIGCOMM, pages 303–314, 2009.
[42] H. Yan, D. A. Maltz, T. S. E. Ng, H. Gogineni, H. Zhang, and Z. Cai.
Tesseract: A 4D network control plane. In NSDI ’07.
[43] M. Zaharia, D. Borthakur, J. Sen Sarma, K. Elmeleegy, S. Shenker,
and I. Stoica. Delay scheduling: A simple technique for achieving
locality and fairness in cluster scheduling. In EuroSys, 2010.
[44] M. Zaharia, M. Chowdhury, M. J. Franklin, S. Shenker, and I. Stoica.
Spark: Cluster Computing with Working Sets. In HotCloud, 2010.
[45] Y. Zhou, D. Wilkinson, R. Schreiber, and R. Pan. Large-scale parallel
collaborative ﬁltering for the Netﬂix prize. In AAIM, pages 337–348.
Springer-Verlag, 2008.