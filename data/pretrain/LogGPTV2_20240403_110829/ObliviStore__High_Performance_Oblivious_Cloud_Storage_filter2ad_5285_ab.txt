• If a block exists in the eviction cache assigned to partition p,
.
write it back to partition p of the server.
2:
• Else, write a dummy block to partition p of the server.
[40].
Figure
The
Write(blockid, block) operation is omitted, since it is similar
to Read(blockid), except that the block written to the eviction
cache is replaced with the new block.
partitioning
framework
The
B, the client ﬁrst looks up this position map to determine
the partition id p; then the client makes an ORAM call to
partition p and looks up block B. On fetching the block
from the server, the client logically assigns it to a freshly
chosen random partition – without writing the block to the
server immediately. Instead, this block is temporarily cached
in the client’s local eviction cache.
A background eviction process evicts blocks from the
eviction cache back to the server in an oblivious manner.
One possible eviction strategy is random eviction: with every
data access, randomly select 2 partitions for eviction. If
there exists a block in the eviction cache that is assigned
to the chosen partition, evict a real block; otherwise, evict a
dummy block to prevent information leakage.
√
The basic SSS ORAM algorithm is described in Figure 2.
Stefanov et. al. prove that the client’s eviction cache load
N ) with high probability. While the
is bounded by O(
position map takes asymptotically O(N ) space to store, in
real-world deployments, the position map is typically small
(e.g., less than 2.3 GB as shown in Table IV) and smaller
than or comparable to the size of the eviction cache. For
theoretic interest, it is possible to store the position map
recursively in a smaller ORAM on the server, to reduce the
client’s local storage to sub-linear – although this is rarely
necessary in practice.
B. Synchronous Amortized Shufﬂing Algorithm
The basic SSS construction as shown in Figure 2 employs
for each partition an ORAM scheme (referred to as the
partition ORAM) based on the original hierarchical construc-
tion by Goldreich and Ostrovsky [14], and geared towards
optimal practical performance.
Such a partition ORAM requires periodic shufﬂing oper-
255
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
ations: every 2i accesses to a partition ORAM, 2i blocks
√
need to be reshufﬂed for this partition ORAM. Reshufﬂing
N ) time in the worst case, and all subsequent
can take O(
data access requests are blocked waiting for the reshufﬂing
to complete.
√
Therefore, although the basic SSS construction has
O(log N ) amortized cost (non-recursive version), the worst-
N ) makes it undesirable in practice. To
case cost of O(
address this issue, Stefanov et. al. propose a technique that
spreads the shufﬂing work across multiple data accesses, to
avoid the poor worst-case performance.
On a high level, the idea is for the client to maintain
a shufﬂing job queue which keeps track of partitions that
need to be reshufﬂed, and the respective levels that need to
be reshufﬂed. A scheduler schedules O(log N ) amount of
shufﬂing work to be performed with every data access.
Stefanov et. al. devise a method for data accesses to
nonetheless proceed while a partition is being shufﬂed, or
pending to be reshufﬂed. Suppose that the client needs to
read a block from a partition that is currently being shufﬂed
or pending to be shufﬂed. There are two cases:
Case 1. The block has been fetched from the server
earlier, and exists in one of the local data structures: the
eviction cache, the shufﬂing buffer, or the storage cache. In
this case, the client looks up this block locally. To prevent
information leakage, the client still needs to read a fake
block from every non-empty level in the server’s partition.
Speciﬁcally,
• For levels currently marked for shufﬂing,
the client
prefetches a previously unread block which needs to be
read in for reshufﬂing (referred to as an early cache-in)
– unless all blocks in that level have been cached in.
• For levels currently not marked for shufﬂing, the client
requests a dummy block, referred to as a dummy cache-
in.
Case 2. The block has not been fetched earlier, and resides
in the server partition. In this case, the client reads the real
block from the level where the block resides in, and for
every other non-empty level, the client makes a fake read
(i.e., early cache-in or dummy cache-in), using the same fake
read algorithm described above.
IV. FORMAL DEFINITIONS
Traditional ORAMs assume synchronous I/O operations,
i.e., I/O operations are blocking, and a data request needs
to wait for a previous data request to end. To increase
the amount of I/O parallelism, we propose to make I/O
operations asynchrnous in ORAMs, namely, there can be
multiple outstanding I/O requests, and completion of I/O
requests are handled through callback functions.
Making ORAM operations asynchronous poses a security
challenge. Traditional synchronous ORAM requires that the
physical addresses accessed on the untrusted storage server
must be independent of the data access sequence.
256
In asynchronous ORAM, the security requirement is com-
plicated by the fact that the scheduling of operations is no
longer sequential or blocking. There can be many ways to
schedule these operations, resulting in variable sequences of
server-observable events (e.g., I/O requests). Not only must
the sequence of addresses accessed be independent of the
data access sequence, so must the timing of these events.
We now formally deﬁne asynchronous (distributed) Obliv-
ious RAM. For both the non-distributed and distributed
case, we ﬁrst deﬁne the set of all network or disk I/O
events (including the timing of the events) observable by
an adversary. The security deﬁnition or an asynchronous
(distributed) ORAM intuitively says that the set of events
observable by the adversary should not allow the adversary
to distinguish two different data request sequences of the
same length and timing.
Asynchronous ORAM. An asynchronous ORAM consists
of a client, a server, and a network intermediary. Let seq
denote a data access sequence:
seq := [(blockid1, t1), (blockid2, t2), . . . , (blockidm, tm)]
where each blockidi denotes a logical block identiﬁer, and
each ti denotes the time of arrival for this request. Given
any data access sequence seq, the ORAM client interacts
with the server to fetch these blocks. Let
events := [(addr1, τ1), (addr2, τ2), . . . , (addrc, τc)]
(1)
denote the event sequence resulting from a data access
sequence, where each addri denotes a requested physical
address on the server storage, and τi denotes the time at
which the request is sent from the client.
We assume that the network and the storage are both under
the control of the adversary, who can introduce arbitrary
delays of its choice in packet transmissions and responses
to requests.
Distributed asynchronous ORAM. A distributed asyn-
chronous ORAM consists of multiple distributed trusted
components which can communicate with each other, and
communicate with untrusted storage servers. The adversary
is in control of the storage servers, as well as all network
communication. Although in practice, the storage servers
are typically also distributed, for the security deﬁnitions
below, we consider all untrusted storage servers as a unity
– since they are all controlled by the adversary. In this
section, we consider the abstract model of distributed asyn-
chronous ORAM, while possible real-world instantiations
are described in Section VI.
For a distributed asynchronous ORAM, We can deﬁne
the sequence of all events to be composed of 1) all I/O
requests (and their timings) between a trusted component to
the untrusted storage; and 2) all (encrypted) messages (and
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:55:02 UTC from IEEE Xplore.  Restrictions apply. 
server
client
Storage cache
CacheIn(addr)
Fetch(addr)
Partition Reader
increment
Partition
states
ReadPartition(partition,
blockId)
Semaphores
decrement
CacheIn(addr)
CacheOut(addr,block)
Fetch(addr)
Store(addr, block)
Background shuffler
increment
decrement
Fetch(partition)
Eviction cache
Fetch(blockId)
Store(partition, block)
Read(blockId)
Write(blockId, block)
Figure 3: Overview of asynchronous ORAM algorithm. Solid
arrows: synchronous calls. Dotted arrows: asynchronous calls.
ORAM Main
Data structure
eviction cache
position map
storage cache
(cid:2)
their timings) between two trusted components:
events :=
(addr1, τ1, κ1), (addr2, τ2, κ2), . . . , (addrc, τc, κc),
2), . . . , (md, ˜τd, κd, κ(cid:2)
(m1, ˜τ1, κ1, κ(cid:2)
d)
(2)
1), (m2, ˜τ2, κ2, κ(cid:2)
partition states
(cid:3)
trusted component κi
where (addri, τi, κi) denotes that
requests physical address addri from untrusted storage at
time τi; and (mi, ˜τi, κi, κ(cid:2)
i) denotes that trusted component
κi sends an encrypted message m to trusted component κ(cid:2)
i
at time ˜τi.
(cid:4)
Similarly to the non-distributed case, we say that a dis-
tributed asynchronous ORAM is secure, if an adversary (in
control of the network and the storage) cannot distinguish
any two access sequences of the same length and timing
from the sequence of observable events.
Deﬁnition 1 (Oblivious accesses and scheduling). Let seq0
and seq1 denote two data access sequences of the same
length and with the same timing:
seq0 := [(blockid1, t1), (blockid2, t2), . . . , (blockidm, tm)] ,
(cid:2)
m, tm)
seq1 :=
Deﬁne the following game with an adversary who is in
control of the network and the storage server:
• The client ﬂips a random coin b.
• Now the client runs distributed asynchronous ORAM
algorithm and plays access sequence seqb with the
adversary.
outputs a guess b(cid:2) of b.
• The adversary observes the resulting event sequence and
(cid:2)
2, t2), . . . , (blockid
(cid:2)
1, t1), (blockid
(blockid
(cid:5)
We say that a (distributed) asynchronous ORAM is se-
cure, if
for any two
sequences seq0 and seq1 of the same length and timing,
(cid:6)(cid:6) ≤ negl(λ). where λ is a security parameter,
for any polynomial-time adversary,
= b] − 1
(cid:6)(cid:6)Pr[b(cid:2)
is a negligible function. Note that
and negl
the set of
events observed by the adversary in the non-distributed and
distributed case are given in Equations 1 and 2 respectively.
2
V. ASYNCHRONOUS ORAM CONSTRUCTION
We now describe how to make the SSS ORAM asyn-
chronous. This section focuses on the non-distributed case
ﬁrst. The distributed case is described in the next section.
257
Table II: Data structures used in ObliviStore
Purpose
Temporarily caches real reads before eviction.
Stores the address for each block, including
which partition and level each block resides
in.
Temporarily stores blocks read in from server
for shufﬂing, including early cache-ins and
shufﬂing cache-ins. Also temporarily stores
blocks after shufﬂing intended to be written
back to the server.
shufﬂing.
stores the state of each partition, including
which levels are ﬁlled, information related to
shufﬂing, and cryptographic keys.
shufﬂing buffer Used for locally permuting data blocks for
A. Overview of Components and Interfaces
As shown in Figure 3, our basic asynchronous ORAM
has three major functional components, the ORAM main
algorithm, the partition reader, and the background shufﬂer.
ORAM main. ORAM main is the entry point to the ORAM
algorithm, and takes in asynchronous calls of the form
Read(blockid) and Write(blockid, block). Response to
these calls are passed through callback functions.
The ORAM main handler looks up the position map to de-
termine which partition the requested block resides in, calls
the partition reader to obtain the block asynchronously, and
places the block in a freshly chosen random eviction cache.
If the request is a write request, the block is overwritten