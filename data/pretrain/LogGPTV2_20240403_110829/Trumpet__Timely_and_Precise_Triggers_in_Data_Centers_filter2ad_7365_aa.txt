title:Trumpet: Timely and Precise Triggers in Data Centers
author:Masoud Moshref and
Minlan Yu and
Ramesh Govindan and
Amin Vahdat
Trumpet: Timely and Precise Triggers
in Data Centers
Masoud Moshref (USC)
Minlan Yu (USC)
Ramesh Govindan (USC)
Amin Vahdat (Google, inc.)
network views to human operators, and require humans to
interpret their output and initiate network control.
Abstract
As data centers grow larger and strive to provide tight perfor-
mance and availability SLAs, their monitoring infrastructure
must move from passive systems that provide aggregated
inputs to human operators, to active systems that enable pro-
grammed control. In this paper, we propose Trumpet, an
event monitoring system that leverages CPU resources and
end-host programmability, to monitor every packet and report
events at millisecond timescales. Trumpet users can express
many network-wide events, and the system efﬁciently detects
these events using triggers at end-hosts. Using careful design,
Trumpet can evaluate triggers by inspecting every packet at
full line rate even on future generations of NICs, scale to
thousands of triggers per end-host while bounding packet
processing delay to a few microseconds, and report events
to a controller within 10 milliseconds, even in the presence
of attacks. We demonstrate these properties using an imple-
mentation of Trumpet, and also show that it allows operators
to describe new network events such as detecting correlated
bursts and loss, identifying the root cause of transient conges-
tion, and detecting short-term anomalies at the scale of a data
center tenant.
CCS Concepts
•Networks → End nodes; Network monitoring; Data
center networks;
Keywords
Network Event Monitoring; End-host Monitoring
1.
INTRODUCTION
Data center network management tasks range from fault di-
agnosis to trafﬁc engineering, network planning, performance
diagnosis, and attack prevention and require network moni-
toring. Commercial network monitoring tools (e.g., SNMP,
NetFlow) produce highly aggregated or sampled statistics
(e.g., per port count in SNMP, sampled NetFlow records)
at relatively coarse time-scales (seconds to minutes), often
provide input to dashboards designed to present aggregate
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’16, August 22-26, 2016, Florianopolis , Brazil
© 2016 ACM. ISBN 978-1-4503-4193-6/16/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2934872.2934879
As data centers evolve to larger scales, higher speed, and
higher link utilization, monitoring systems must detect events
(such as transient congestion or server load imbalance) more
precisely (inspecting every packet), at ﬁne time-scales (on
the order of milliseconds) and in a programmable fashion
(so that the set of events can be dynamically determined). In
this paper, we argue for a shift of monitoring infrastructure
from passive to active: Instead of presenting aggregated
network views to human operators, monitoring infrastructure
should both allow for interactive drill down based on current
conditions and for automated reaction to pre-deﬁned, ﬁne-
grained packet events at the scope of entire fabrics rather than
individual links or servers.
The challenge in achieving this goal is scale along many
dimensions: the number of endpoints, the high aggregate
trafﬁc in the network, and, as data center networks move to
support large numbers of tenants and services, the number
of events of interest. To address this challenge, we leverage
the relatively plentiful processing power at end-hosts in a
data center. These end-hosts are programmable, already need
to process every packet once, and can monitor all packets
without sampling using their powerful CPUs.
Contributions. In this paper, we present the architecture
and implementation of Trumpet: an event monitoring sys-
tem in which users deﬁne network-wide events, a centralized
controller installs triggers at end-hosts where triggers test
for local conditions, and the controller aggregates these sig-
nals and tests for the presence of speciﬁed network-wide
events. This architecture demonstrates the beneﬁts of push-
ing, to end-hosts, the substantial scalability challenges faced
by datacenter-scale per-packet monitoring. The key to Trum-
pet’s efﬁcacy is carefully optimized per-packet processing
inline on packets being demultiplexed by a software switch.
We demonstrate the beneﬁts of Trumpet with the following
contributions. First, Trumpet introduces a simple network-
wide event deﬁnition language which allows users to deﬁne
network-wide events by specifying a ﬁlter to identify the
set of packets over which a predicate is evaluated; an event
occurs when the predicate evaluates to true. Users can set a
predicate’s time and ﬂow granularity. Despite this simplicity,
the language permits users to capture many interesting events:
identifying if ﬂows trigger congestion on other ﬂows, when
aggregate trafﬁc to hosts of a service exceeds a threshold, or
when connections experience a burst of packet loss.
Our second contribution is scaling event processing at end-
hosts while still being able to detect events at timescales of
a few milliseconds. Upon receiving an event deﬁnition, the
central controller installs triggers corresponding to events at
end-hosts. A trigger determines whether the user-speciﬁed
event has occurred at an end-host; if so, the end-host sends
the trigger’s output to the controller. The controller collects
the trigger results from individual end-hosts to determine if
the event deﬁnition has been satisﬁed across the network. The
key challenge is to support thousands of triggers at full line
rate without requiring extra CPU cores on a software switch,
and without (a) dropping any packet, (b) missing any events,
and (c) delaying any packet by more than a few µs.
To achieve these properties, Trumpet processes packets in
two phases in the end-hosts: A match-and-scatter phase that
matches each incoming packet and keeps per 5-tuple ﬂow
statistics; and a gather-test-and-report phase which runs at
each trigger’s time granularity, gathers per-trigger statistics,
and reports when triggers are satisﬁed. We also design and
implement a suite of algorithms and systems optimizations in-
cluding caching to reduce computation, pre-fetching and data
structure layout to increase memory access efﬁciency, careful
use of virtual memory, and a queue-length adaptive schedul-
ing technique to steal cycles from packet processing. Our
design degrades gracefully under attack, and scales naturally
to higher-speed NICs likely to be deployed in datacenters in
the next few years.
Getting precise and accurate visibility into data centers is
a crucial problem, and Trumpet addresses an important part
of this space. In practice, a spectrum of solutions is likely
to be necessary, including approaches (Section 8) that use
NICs and switches. NIC ofﬂoading saves end-host CPU and
can monitor the ﬂows that bypass the hypervisor (e.g., using
SR-IOV and RDMA). Other schemes that mirror packets
Our third contribution is an evaluation of Trumpet across
a variety of workloads and event descriptions. We evaluated
Trumpet running on a single core with 4K triggers and (a)
64-byte packets on a 10G NIC at line rate and (b) 650-byte
packets on a 40G NIC at line rate. Trumpet sustains this
workload without packet loss, or missing a single event. More
generally, we characterize the feasible set of parameters, or
the feasibility region, in which Trumpet ﬁnishes every sweep,
and never loses a packet. Each of our optimizations provides
small beneﬁts, but because at full line-rate we have almost no
processing headroom, every optimization is crucial. Trumpet
can report network-wide events within 1ms after the ﬁrst
trigger is matched at one end-host. Its matching complexity
is independent of the number of triggers, a key reason we
are able to scale the system to large numbers of triggers.
Finally, Trumpet degrades gracefully under attack: with the
appropriate choice of parameters (which can be determined
by an accurate model), it can sustain a workload in which
96% of the packets are DoS packets, at the expense of not
being able to monitor ﬂows of size smaller than 128 bytes.
Fourth, we demonstrate the expressivity of Trumpet with
three use cases: a) pacing trafﬁc, using a tight control loop,
when that trafﬁc causes a burst of losses; b) automatically
identifying ﬂows responsible for transient congestion and
c) detecting, using a network-wide event, services whose
combined volume exceeds a threshold.
or packet headers to a controller, where an operator or a
script can examine headers or content, can help drill down
on events triggered by Trumpet. As an aside, these schemes
might need to process packets at high rates at the controller,
and Trumpet’s processing optimizations might be applicable
to these.
2. THE CASE FOR TRUMPET
Today’s monitoring infrastructure is designed for human
monitoring, so coarse time scales and high levels of aggre-
gation are sufﬁcient. Modern data centers need real-time,
ﬁne-grained, and precise monitoring to feed a variety of con-
trol systems. In this section, we elaborate on this observation,
which motivates the design of Trumpet.
Problems of today’s monitoring systems. Network man-
agement systems in data centers rely on detecting network
events. An event often indicates a network condition that
may require a corrective action, mostly through a computer-
assisted reaction. Events are often deﬁned in terms of packets
dropped, delayed, or delivered by the network, and can range
in topological scope from a speciﬁc ﬂow, to trafﬁc aggregates
(e.g., all network trafﬁc to a service), and in temporal scope
from being extremely short-lived to long-lived events.
Traditional monitoring systems only provide coarse-
grained views of the network at larger time scales, which
are sufﬁcient for humans to understand network state, but
may be insufﬁcient to capture events precisely or at ﬁne
time-scales. For example, SNMP provides per port counters
every few minutes, too coarse-grained for trafﬁc engineering
or performance diagnosis. OpenFlow provides counters
for aggregated ﬂows (due to limited TCAM sizes [40])
and reports the updated counters every few seconds [13,
46], which cannot capture sub-second trafﬁc events. sFlow
[54] uses packet sampling which cannot capture transient
events (e.g., transient packet losses), track connection states
(e.g., congestion window), and correctly estimate link load
[48]. These shortcomings in monitoring systems can lead
to signiﬁcant loss of network availability: a trafﬁc surge
in Google’s Compute Engine was detected 3 minutes after
causing 90% packet loss between two regions [22].
Moreover, today’s monitoring systems do not scale well
to larger networks with higher capacities and higher utiliza-
tion. Higher link speed and larger scales mean more packets
to monitor; higher network utilization requires more timely
event reporting, because delayed reports of an outage can
affect larger trafﬁc volumes. Unfortunately, higher utiliza-
tion also leaves fewer network resources for monitoring. For
example, the reporting frequency of OpenFlow counters is
inversely proportional to the number of connections managed
(increasing new connections from 150 to 250 per second re-
quires reducing reporting frequency from once per second
to once per 5 seconds [13]). As a result, the precision of a
network monitoring system can suffer at higher scales and
utilization and therefore adversely impact the goal of achiev-
ing service level objectives and high utilization, especially in
data center networks.
Data centers need precise, ﬁne time-scale event monitor-
ing. Data centers need novel kinds of event monitoring ca-
pabilities to capture a variety of network misbehaviors (e.g.,
misconﬁguration, transient looping, anomalies, drops) and as
input for network management decisions (e.g., trafﬁc engi-
neering, load balancing, VM migration). We describe a few
examples here and list more in Section 3.
Identify losses caused by trafﬁc bursts. Trafﬁc bursts are com-
mon in data centers and can improve application performance
[23, 58]. For example, NIC ofﬂoading [31] sends packets in
batches to reduce processing overhead, and distributed ﬁle
systems read and write in bulk to maximize disk throughput.
However, bursty trafﬁc may cause losses when they traverse
shallow buffered switches [51], which signiﬁcantly affects
application performance. To improve performance, it may
be necessary to detect lost packets (e.g., by retransmissions)
and packets in bursts (e.g., by tracking packet timestamps),
and identify their correlations (e.g., by correlating packet se-
quence numbers). All these detections are not possible using
packet sampling or ﬂow level counters.
Identify root cause of congestion. Transient network conges-
tion (e.g., incast [8]) is hard to diagnose. For example, a
MapReduce reducer may see signiﬁcant performance degra-
dation caused by network congestion, even though its aggre-
gate demand may be well below switch capacity. This is
because another application sending through the same switch
can trigger transient incast losses, increasing the reducer’s
processing time and therefore the job ﬁnish time. Such con-
gestion is often caused by short bursts of trafﬁc at short
timescales (10 ms). This may not be detectable on aggregate
counters in today’s switches (which have > 1s granularity).
To diagnose this behavior, it is important to identify TCP
ﬂows with high loss and correlate them with heavy hitters
going through the bottleneck.
Monitor server load balance and load burst. Maintaining the
service-level agreements (SLAs) for different applications
requires careful provisioning and load balancing in cloud ser-
vices [44] because imperfect load balance and short request
bursts can lead to long tail latency [28, 26]. A good way to
track service load is to monitor the network trafﬁc [5]. If we
can identify volume anomalies in short timescales, we can
identify inefﬁcient load balancing across servers, provision-
ing issues or DDoS attacks on some servers. For example,
operators can query whether the long tail latency is because
the service sees bursts of requests, or more than 50% of VMs
of a service see a trafﬁc surge as a result of a DDoS attack.
Trumpet. These kinds of event detection are beyond the
capabilities of today’s deployed systems. In this paper, we
consider a qualitatively different point in the design space