#### Code Sample, a copy-pastable example if possible
    s1=pd.Series([True,False,True,True])
    s2=pd.Series([True,True,False])
    s1.index=pd.MultiIndex.from_tuples( [(0, 2), (1, 1), (1, 2), (2, 1)],names=['st', 'at'])
    s2.index=pd.Index([0,1,2], name='st')
    ds1=pd.DataFrame(s1)
    ds2=pd.DataFrame(s2)
    s3=pd.Series([True,False,True])
    s4=pd.Series([True,True,False])
#### Expected Output
    st  at
    0   2      True
    1   1     False
    1   2      True
    2   1     False
    dtype: bool
For series, `*` and `&` give different result:
    In: s1 & s2
    Out: 
    st  at
    0   2     False
    1   1     False
    1   2     False
    2   1     False
    dtype: bool
    In: s1 * s2
    Out: 
    st  at
    0   2      True
    1   1     False
    1   2      True
    2   1     False
    dtype: bool
BUT for dataframes they give the same:
    In: ds1 * ds2
    Out: 
               0
    st at       
    0  2    True
    1  1   False
    1  2    True
    2  1   False
    In: ds1 & ds2
    Out: 
               0
    st at       
    0  2    True
    1  1   False
    1  2    True
    2  1   False
and last for series with single index, also both `s3 & s4` or `s3 * s4` give:
    0     True
    1    False
    2    False
    dtype: bool
#### output of `pd.show_versions()`
pandas: 0.18.1