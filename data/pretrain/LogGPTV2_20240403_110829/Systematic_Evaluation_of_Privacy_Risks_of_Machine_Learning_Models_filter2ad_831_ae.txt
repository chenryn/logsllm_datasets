### 4.2.1 Estimating the Probability of Being a Member

Following the same steps as described above, we compute the real probability of being a member and the average outputs of the attack classifier. Specifically, we follow the methodology outlined by Nasr et al. [31] to train the attack classifier using the target modelâ€™s predictions and one-hot encoded input labels as features.

**Figure 3** illustrates the distribution of training samples' privacy risk scores (top row) and the attack classifier's outputs on the training data (bottom row) for Purchase100 classifiers without defense, with AdvReg [31], and with early stopping. We also compare the privacy risk score and the attack classifier's output with the real probability of being a member, as shown in the last column of **Figure 3**, where the ideal case is used to check the effectiveness of the metrics.

Our proposed privacy risk score closely aligns with the actual probability of being a member: the privacy risk score curves for all three models are quite close to the line of the ideal case. In contrast, the attack classifiers' outputs fail to capture the membership probability. This is because neural network (NN) classifiers are trained to minimize the loss, meaning the output for a member should be close to 1, while the output for a non-member should be close to 0. With this training goal, the obtained attack classifiers failed to capture the privacy risks for individual samples.

We quantitatively measure the root-mean-square error (RMSE) between the estimated probability of being a member and the real probability of being a member. For the three Purchase100 classifiers, the RMSE values of our privacy risk score are 0.05, 0.09, and 0.06, respectively. In contrast, the RMSE values of the NN classifier's outputs are 0.26, 0.26, and 0.25, respectively. Similar results are observed for the undefended Texas100 classifier and the defended classifier by MemGuard [20], with details provided in Appendix C.

### 4.2.2 Validation Across Varied Model Architectures

We validate the effectiveness of the privacy risk score across different model architectures. For Purchase100 and Texas100 classifiers, we test two additional neural network depths by deleting the last hidden layer (depth=3) or adding one more hidden layer with 2048 neurons (depth=5). We also test two additional neural network widths by halving the number of hidden neurons (width=0.5) or doubling the number of hidden neurons (width=2.0). Additionally, we test ReLU, Tanh, and Sigmoid as activation functions. For CIFAR100 classifiers, besides DenseNet [18], we test other popular convolutional neural network architectures, including AlexNet [22], VGG [42], ResNet [14], and Wide ResNet [49].

As shown in **Figure 4**, our proposed privacy risk score metric indeed well represents the likelihood of a sample being in the training set under different model architectures. On the Texas100 dataset, the classifier fails to learn meaningful features using the Sigmoid activation function, achieving an accuracy of only 4%, and is thus omitted from the figure. We provide validation results with defended classifiers in Appendix D.

### 4.2.3 Heterogeneity of Members' Privacy Risk Scores

After validating the effectiveness of the privacy risk score metric, we show the heterogeneity of training samples' privacy risks by plotting the cumulative distribution of their privacy risk scores. We also investigate the performance of membership inference defense methods [20, 31] with a comparison between defended and undefended classifiers.

**Figure 5** presents the cumulative distributions of training points' privacy risk scores for Purchase100 classifiers. Compared to the undefended classifier, the defended classifier with adversarial regularization [31] has smaller privacy risk scores on average. However, the defended classifier has a small portion of training data with higher privacy risk scores than the undefended model. The undefended model has all members' privacy risk scores under 0.8, whereas the defended model has several training points with privacy risk scores higher than 0.8. Furthermore, the classifier with early stopping has a similar risk score distribution as the defended classifier.

### 4.2.4 Usage of Privacy Risk Score

From our definition and verification results in Section 4.2.1, we know that the privacy risk score of a data point indicates its probability of being a member. Instead of pursuing high average attack accuracy, the adversary can now identify which samples have high privacy risks and perform attacks with high confidence: a sample is inferred as a member if and only if its privacy risk score is above a certain probability threshold.

**Table 6** shows the attack results with precision and recall values for target classifiers with varying threshold values on privacy risk scores. From **Table 6**, we can see that with larger threshold values on privacy risk scores, the adversary indeed has higher precision values for membership inference attacks. For MemGuard [20], when setting the same threshold value on privacy risk scores, both undefended and defended Texas100 classifiers have similar attack precision, but the defended classifier has a smaller recall value. However, the defended Texas100 classifier still has severe privacy risks: 70.5% of training members can be inferred correctly with a precision of 71.3%, and 1.4% of training members can be inferred correctly with a precision of 88.2%. Similarly, while adversarial regularization [31] can lower the average privacy risks, it increases the privacy risks for certain members: on the defended Purchase100 classifier, 0.2% of training members can be inferred correctly with a precision of 83.3%. We urge designers of defense mechanisms to account for the full distribution of privacy risks in their analysis.

### 4.2.5 Impact of Model Properties on Privacy Risk Score

We perform an in-depth investigation of the privacy risk score by exploring its correlations with certain model properties, including sensitivity, generalization error, and feature embedding. We use the undefended Texas100 classifier from Jia et al. [20] for the following experiments.

**Figure 6** shows the cumulative distribution of training data's privacy risk scores for Texas100 classifiers. The defense method indeed decreases training samples' privacy risk scores. However, the defended classifier is still quite vulnerable: 70% of training samples have privacy risk scores higher than 0.6.

#### Privacy Risk Score and Sensitivity

We first study the relationship between privacy risk scores and model sensitivity with regard to training samples. The sensitivity is defined as the influence of one training sample on the target model by computing the difference after removing that sample. Since the privacy risk score is obtained with the measured distributions of modified prediction entropy (Equation (15)), we compute the model's sensitivity regarding a training point \( z = (x, y) \) as the logarithm of \(\frac{Mentr(\hat{F}_z(x), y)}{Mentr(F(x), y)}\), where \(\hat{F}_z\) means the retrained classifier after removing \( z \) from the training set.

**Figure 7** shows the relation between privacy risk scores and the model sensitivity. For each privacy risk score, we show the first quartile, the average, and the third quartile of model sensitivities with regard to training data. We can see that samples with higher privacy risk scores are likely to have a larger influence on the target model.

#### Privacy Risk Score and Generalization Error

We observe that training samples with high risk scores are typically concentrated in a few class labels. Therefore, we further compare privacy risk scores among different class labels. **Figure 8** shows the average privacy risk scores and generalization errors for all 100 classes, where we sort the class labels based on their generalization errors. Class labels with high generalization errors tend to have higher privacy risk scores, which is expected since the generalization error has a large influence on the success of membership inference attacks [41]. The Pearson correlation coefficient between average privacy risk scores and generalization errors is as high as 0.94.

#### Privacy Risk Score and Feature Embeddings

From the above experiment, we know that training samples from class labels with high generalization errors tend to have high privacy risk scores. Next, we investigate this further by looking into the feature representations of different class labels learned by the target classifier. We use the outputs of the last hidden layer of the target classifier as the feature embedding of the input sample. We pick the top 5 class labels (30, 93, 97, 18, 98) with the lowest average privacy risk scores (0.50, 0.52, 0.53, 0.54, 0.55) and at least 100 training samples, and the top 5 class labels (72, 49, 45, 51, 78) with the highest average privacy risk scores (0.82, 0.83, 0.83, 0.85, 0.90) and at least 100 training samples. We record feature embeddings for both training and test examples from these 10 class labels. Finally, we adopt t-Distributed Stochastic Neighbor Embedding (t-SNE) [27], a nonlinear dimensionality reduction technique, to visualize the feature embeddings.

**Figures 9a and 9b** show the t-SNE plots of training samples and test samples, respectively. The training samples are separated clearly based on class labels since the target classifier has a training accuracy close to 100%. Test samples from class labels with low risk scores (classes 30, 93, 97, 18, 98) have quite similar feature embeddings as training samples and are still well-separated. On the other hand, test samples from class labels with high risk scores (classes 72, 49, 45, 51, 78) exhibit differences in feature representations compared to corresponding training samples. From **Figure 9**, we also observe the heterogeneity of samples' privacy risks, in the granularities of both individual samples (e.g., different samples in class 78) and class labels (e.g., class 30 versus class 78). This further emphasizes the importance of fine-grained privacy risk analysis. It also validates our attack design of using class-dependent thresholds in Section 3.1. Our observations are also important for future defense work. A good defense approach should make training data and validation data have similar feature embeddings and consider the heterogeneity of samples' privacy risks.

### 5. Conclusions

In this paper, we argue that measuring membership inference privacy risks with neural network-based attacks is insufficient. We propose using a suite of metric-based attacks, including existing methods with our improved class-specific thresholds and a new proposed method based on modified prediction entropy, for benchmarking privacy risks of machine learning models. We also recommend comparing with early stopping when benchmarking a defense that introduces a tradeoff between model accuracy and privacy risks, and considering adaptive attackers with knowledge of the defense to rigorously evaluate the performance of defense approaches. With these benchmark attacks, we show that (1) the defense approach of adversarial regularization, proposed by Nasr et al. [31], only reduces privacy risks to a limited degree and is no better than early stopping; (2) the defense performance of MemGuard, proposed by Jia et al. [20], is greatly degraded with adaptive attacks.

Next, we introduce a new metric called the privacy risk score for a fine-grained analysis of individual samples' privacy risks. We demonstrate the effectiveness of the privacy risk score in estimating the true likelihood of an individual sample being in the training set and observe the heterogeneity of samples' privacy risk scores with experimental results. Finally, we perform an in-depth investigation into the correlation between privacy risks and model properties, including sensitivity, generalization error, and feature embeddings. We hope that our work convinces the research community about the importance of systematically and rigorously evaluating privacy risks of machine learning models.

### Acknowledgements

We are grateful to anonymous reviewers at USENIX Security for valuable feedback. We would also like to thank Googleâ€™s TensorFlow Privacy team for integrating our methods. This work was supported in part by the National Science Foundation under grants CNS-1553437 and CNS-1704105, the ARLâ€™s Army Artificial Intelligence Innovation Institute (A2I2), the Office of Naval Research Young Investigator Award, the Army Research Office Young Investigator Prize, Faculty research award from Facebook, Schmidt DataX award, and by Princeton E-filiates Award.

### References

[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang. Deep learning with differential privacy. In ACM Conference on Computer and Communications Security, 2016.

[2] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In International Conference on Machine Learning, 2018.