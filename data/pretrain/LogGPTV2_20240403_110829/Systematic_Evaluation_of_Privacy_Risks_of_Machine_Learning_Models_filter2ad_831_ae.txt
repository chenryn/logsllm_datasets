ing same steps as above, we compute the real probability of
being a member and the average outputs of the attack classi-
ﬁer. Speciﬁcally, we follow Nasr et al. [31] to train the attack
classiﬁer by using the target model’s predictions and one-hot
encoded input labels as features.
Figure 3 shows the distribution of training samples’ pri-
vacy risk scores (top row) and attack classiﬁer’s outputs on
training data (bottom row) for Purchase100 classiﬁers without
defense, with AdvReg [31], and with early stopping. We also
compare the privacy risk score and attack classiﬁer’s output
with the real probability of being a member, as shown in the
last column of Figure 3 where the ideal case is used to check
the effectiveness of metrics. We can see that our proposed
privacy risk score closely aligns with the actual probabil-
ity of being a member: the privacy risk score curves for all
three models are quite close to the line of the ideal case. On
the other hand, the attack classiﬁers’ outputs fail to capture the
membership probability. This is because the NN classiﬁers
are trained to minimize the loss, i.e., the output of a member
should be close to 1 while the output of a non-member should
be close to 0. With this training goal, the obtained attack classi-
ﬁers failed to capture the privacy risks for individual samples.
We also quantitatively measure the root-mean-square error
(RMSE) between estimated probability of member and real
probability of member. On the three Purchase100 classiﬁers,
the RMSE values of our privacy risk score are 0.05, 0.09, and
0.06; in contrast, the RMSE values of NN classiﬁer’s outputs
are 0.26, 0.26, 0.25, respectively. We observe similar results
on the undefended Texas100 classiﬁer and the defended clas-
siﬁer by MemGuard [20], with details in Appendix C.
We also validate the effectiveness of privacy risk score
across varied model architectures. For Purchase100 and
USENIX Association
30th USENIX Security Symposium    2625
(a) The ﬁrst three ﬁgures present the distributions of training samples’ privacy risk scores on Purchase100 classiﬁers without defense, with
AdvReg [31], and with early stoppping. The last ﬁgure shows that the privacy risk score can well represent the real probability of being a
member, with root mean square error (RMSE) of 0.05, 0.09, and 0.06.
(b) The ﬁrst three ﬁgures present the distributions of NN attack classiﬁer’s outputs over training samples on Purchase100 classiﬁers without
defense, with AdvReg [31], and with early stoppping. The last ﬁgure shows that the NN classiﬁer’s output fails to represent the real probability
of being a member, with RMSE values of 0.26, 0.26, and 0.25.
Figure 3: Estimate the probability of being a member with our proposed privacy risk score (Figure 3a), and with the NN attack
classiﬁer’s output (Figure 3b).
Figure 4: Validation of privacy risk score with different model architectures on (undefended) Purchase100 (left), Texas100
(middle), and CIFAR100 (right) classiﬁers. For Purchase100 and Texas100 classiﬁers, the legend is expressed as (activation
function, width, depth). The RSME values between privacy risk score (x-axis) and probability of being a member (y-axis) for all
lines are smaller than 0.09.
Texas100 classiﬁers, we test two additional neural network
depths by deleting the last hidden layer (depth=3) or adding
one more hidden layer with 2048 neurons (depth=5); we test
two additional neural network widths by halving the num-
bers of hidden neurons (width=0.5) or doubling the numbers
of hidden neurons (width=2.0); we also test ReLU, Tanh, or
Sigmoid as the activation functions. For CIFAR100 classi-
ﬁers, besides DenseNet [18], we test other popular convolu-
tional neural network architectures, including AlexNet [22],
VGG [42], ResNet [14], and Wide ResNet [49]. As show in
Figure 4, our proposed privacy risk score metric indeed well
represents the likelihood of a sample being in the training set
under different model architectures. On the Texas100 dataset,
the classiﬁer fails to learn meaningful features using the Sig-
moid activation function, achieving an accuracy of only 4%,
and is thus omitted from the ﬁgure. We provide validation
results with defended classiﬁers in Appendix D.
4.2.2 Heterogeneity of members’ privacy risk scores
After validating the effectiveness of the privacy risk score met-
ric, we show the heterogeneity of training samples’ privacy
risks by plotting the cumulative distribution of their privacy
risk scores. We also investigate the performance of mem-
2626    30th USENIX Security Symposium
USENIX Association
bership inference defense methods [20, 31] with comparison
between defended and undefended classiﬁers.
Figure 5: The cumulative distribution of privacy risk scores
for Purchase100 classiﬁers in Nasr et al. [31].
Figure 5 presents the cumulative distributions of training
points’ privacy risk scores for Purchase100 classiﬁers. We
can see that, compared with the undefended classiﬁer, the de-
fended classiﬁer with adversarial regularization [31] has
smaller privacy risk scores on average. However, we can
also see that the defended classiﬁer has a small portion
of training data with higher privacy risk scores than the
undefended model: the undefended model has all members’
privacy risk scores under 0.8, in contrast, the defended model
has several training points with privacy risk scores higher than
0.8. Furthermore, the classiﬁer with early stopping has a
similar risk score distribution as the defended classiﬁer.
4.2.3 Usage of privacy risk score
From our deﬁnition and veriﬁcation results in Section 4.2.1,
we know that privacy risk score of a data point indicates
its probability of being a member. Instead of pursuing high
average attack accuracy, now the adversary can identify which
samples have high privacy risks and perform attacks with high
conﬁdence: a sample is inferred as a member if and only if its
privacy risk score is above a certain probability threshold.
We show the attack results with precision and recall val-
ues in Table 6 for target classiﬁers with varying threshold
values on privacy risk scores. From Table 6, we can see that
with larger threshold values on privacy risk scores, the ad-
versary indeed has higher precision values for membership
inference attacks. For MemGuard [20], when setting the same
threshold value on privacy risk scores, both undefended and
defended Texas100 classiﬁers have similar attack precision,
but the defended classiﬁer has a smaller recall value. However,
the defended Texas100 classiﬁer still has severe privacy risks:
70.5% training members can be inferred correctly with the
precision of 71.3%, and 1.4% training members can be in-
ferred correctly with the precision of 88.2%. Similarly, while
adversarial regularization [31] can lower the average privacy
risks, it increases the privacy risks for certain members: on
the defended Purchase100 classiﬁer, 0.2% training members
can be inferred correctly with the precision of 83.3%. We
urge designers of defense mechanisms to thus account for the
full distribution of privacy risks in their analysis.
4.2.4
Impact of model properties on privacy risk score
We perform an in-depth investigation of privacy risk score
by exploring its correlations with certain model properties,
including sensitivity, generalization error, and feature embed-
ding. We use the undefended Texas100 classiﬁer from Jia et
al. [20] for the following experiments.
Figure 6: The cumulative distribution of privacy risk scores
for Texas100 classiﬁers in Jia et al. [20].
Figure 6 shows the cumulative distribution of training data’
privacy risk scores for Texas100 classiﬁers. We can see that
the defense method indeed decreases training samples’
privacy risk scores. However, the defended classiﬁer is
still quite vulnerable: 70% training samples have privacy
risk scores higher than 0.6.
Figure 7: The relation between privacy risk score and model
sensitivity.
Privacy risk score with sensitivity We ﬁrst study the rela-
tionship between privacy risk scores and model sensitivity
with regard to training samples. The sensitivity is deﬁned as
USENIX Association
30th USENIX Security Symposium    2627
Table 6: Membership inference attacks by setting different threshold values on privacy risk scores. For each threshold value, we
report the (precision, recall) pair of membership inference attacks.
dataset
Texas100
defense
method
1
0.9
threshold values on privacy risk scores
0.8
0.7
0.6
0.5
no defense
(85.4%, 21.2%)
(83.4%, 29.1%)
(81.2%, 45.3%)
(77.0%, 66.1%)
(72.8%, 85.4%)
(70.6%, 94.3%)
MemGuard [20]
(88.2%, 1.4%)
(84.5%, 7.6%)
(82.6%, 18.7%)
(77.0%, 43.7%)
(71.3%, 70.5%)
(66.0%, 99.9%)
no defense
Purchase100
early stopping
(N.A., 0%)
(N.A., 0%)
(N.A., 0%)
(N.A., 0%)
(N.A., 0%)
(63.4%, 7.8%)
(62.6%, 55.1%)
(61.6%, 90.9%)
(N.A., 0%)
(60.4%, 1.3%)
(57.1%, 20.2%)
(56.6%, 75.2%)
AdvReg [31]
(83.3%, 0.2%)
(83.3%, 0.2%)
(65.9%, 0.5%)
(63.9%, 1.7%)
(58.9%, 24.6%)
(56.5%, 76.3%)
the inﬂuence of one training sample on the target model by
computing the difference after removing that sample. Since
the privacy risk score is obtained with the measured distri-
butions of modiﬁed prediction entropy (Equation (15)), we
compute the model’s sensitivity regard to a training point
z = (x,y) as the logarithm of Mentr((cid:101)Fz(x),y)
Mentr(F(x),y) , where (cid:101)Fz means
the retrained classiﬁer after removing z from the training set.
Figure 7 shows the relation between privacy risk scores
and the model sensitivity. For each privacy risk score, we
show the ﬁrst quartile, the average, and the third quartile of
model sensitivities with regard to training data. We can see
that, samples with higher privacy risk scores are likely to
have a larger inﬂuence on the target model.
Privacy risk score with generalization error We observe
that training samples with high risk scores are typically con-
centrated in a few class labels. Therefore, we further compare
privacy risk scores among different class labels in this section.
Figure 8: Average privacy risk score vs generalization error
per class with a strong Pearson correlation coefﬁcient of 0.94.
Besides the privacy risk scores, we also record the general-
ization errors for different class labels. Figure 8 shows the av-
erage privacy risk scores and generalization errors for all 100
classes, where we sort the class labels based on their general-
ization errors. We can see that the class labels with high gen-
eralization errors tend to have higher privacy risk scores,
which is as expected since the generalization error has a large
inﬂuence on the success of membership inference attacks [41].
The Pearson correlation coefﬁcient between average privacy
risk scores and generalization errors is as high as 0.94.
Privacy risk score with feature embeddings From the
above experiment, we know that training samples from class
labels with high generalization errors tend to have high pri-
vacy risk scores. Next, we investigate this further by look-
ing into the feature representations of different class labels
learned by the target classiﬁer. We use the outputs of last
hidden layer of the target classiﬁer as the feature embedding
of the input sample. We pick the top 5 class labels (30, 93, 97,
18, 98) with lowest average privacy risk scores (0.50, 0.52,
0.53, 0.54, 0.55) and at least 100 training samples, and the
top 5 class labels (72, 49, 45, 51, 78) with highest average
privacy risk scores (0.82, 0.83, 0.83, 0.85, 0.90) and at least
100 training samples. We record feature embeddings for both
training and test examples from these 10 class labels. Finally,
we adopt the t-Distributed Stochastic Neighbor Embedding
(t-SNE) [27], a nonlinear dimensionality reduction technique,
to visualize the feature embeddings.
Figure 9a and Figure 9b show the t-SNE plots of training
samples and test samples, respectively. The training samples
are separated clearly based on class labels since the target clas-
siﬁer has the training accuracy close to 100%. Test samples
from class labels with low risk scores (classes 30, 93, 97, 18,
98) have quite similar feature embeddings as training samples
and are still well separated. On the other hand, test samples
from class labels with high risk scores (classes 72, 49, 45,
51, 78) exhibit differences in feature representations com-
pared to corresponding training samples. From Figure 9,
we also observe the heterogeneity of samples’ privacy risks,
in the granularities of both individual samples (e.g., different
samples in class 78) and class labels (e.g., class 30 versus class
78). This further emphasizes the importance of ﬁne-grained
privacy risk analysis. It also validates our attack design of
using class-dependent thresholds in Section 3.1. Our obser-
vations are also important for future defense work. A good
defense approach should make training data and validation
data have similar feature embeddings and consider the hetero-
geneity of samples’ privacy risks.
2628    30th USENIX Security Symposium
USENIX Association
(a) t-SNE plot for training samples in 10 class labels.
(b) t-SNE plot for test samples in 10 class labels
Figure 9: By using t-SNE [27], we visualize feature embeddings for both training samples and test samples. Training samples in
the ﬁrst 5 class labels have low privacy risk scores, and training samples in the last 5 class labels have high privacy risk scores.
5 Conclusions
In this paper, we ﬁrst argue that measuring membership in-
ference privacy risks with neural network based attacks is
insufﬁcient. We propose to use a suite of metric-based attacks,
including existing methods with our improved class-speciﬁc
thresholds and a new proposed method based on modiﬁed pre-
diction entropy, for benchmarking privacy risks of machine
learning models. We also make recommendations of compar-
ing with early stopping when benchmarking a defense that
introduces a tradeoff between model accuracy and privacy
risks, and considering adaptive attackers with knowledge of
the defense to rigorously evaluate the performance of defense
approaches. With these benchmark attacks, we show that (1)
the defense approach of adversarial regularization, proposed
by Nasr et al. [31], only reduces privacy risks to a limited
degree and is no better than early stopping; (2) the defense
performance of MemGuard, proposed by Jia et al. [20], is
greatly degraded with adaptive attacks.
Next, we introduce a new metric called the privacy risk
score for a ﬁne-grained analysis of individual samples’ pri-
vacy risks. We show the effectiveness of the privacy risk score
in estimating the true likelihood of an individual sample being
in the training set and observe the heterogeneity of samples’
privacy risk scores with experimental results. Finally, we per-
form an in-depth investigation about the correlation between
privacy risks and model properties, including sensitivity, gen-
eralization error, and feature embeddings. We hope that our
work convinces the research community about the importance
of systematically and rigorously evaluating privacy risks of
machine learning models.
Acknowledgements
We are grateful to anonymous reviewers at USENIX Se-
curity for valuable feedback. We would also like to thank
Google’s TensorFlow Privacy team for integrating our meth-
ods. This work was supported in part by the National Science
Foundation under grants CNS-1553437 and CNS-1704105,
the ARL’s Army Artiﬁcial Intelligence Innovation Institute
(A2I2), the Ofﬁce of Naval Research Young Investigator
Award, the Army Research Ofﬁce Young Investigator Prize,
Faculty research award from Facebook, Schmidt DataX award,
and by Princeton E-fﬁliates Award.
References
[1] Martin Abadi, Andy Chu, Ian Goodfellow, H Brendan
McMahan, Ilya Mironov, Kunal Talwar, and Li Zhang.
Deep learning with differential privacy. In ACM Confer-
ence on Computer and Communications Security, 2016.
[2] Anish Athalye, Nicholas Carlini, and David Wagner. Ob-
fuscated gradients give a false sense of security: Circum-
venting defenses to adversarial examples. In Interna-
tional Conference on Machine Learning, 2018.