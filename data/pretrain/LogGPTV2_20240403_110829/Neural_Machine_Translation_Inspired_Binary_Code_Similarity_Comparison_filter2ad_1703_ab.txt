vectors for similarity comparison. BinGo [8] introduces a
selective inlining technique to capture the function semantics
and extracts partial traces of various lengths to model functions.
However, all of these approaches compare similarity between
functions, and cannot handle code component similarity de-
tection when only a part of a function or code cross function
boundaries is reused in another program.
Summary. Currently, no solutions are able to meet all these
requirements: (a) working on binary code, (b) analyzing code
of different architectures, (c) resolving the code containment
problem. This work ﬁlls the gap and proposes techniques for
efﬁcient cross-architecture binary code similarity comparison
beyond function pairs. In addition, it is worth mentioning that
many prior systems are built on basic block representation or
comparison [21], [44], [37], [52], [19]; thus, they can beneﬁt
from our system which provides more precise basic block
representation and efﬁcient comparison.
III. OVERVIEW
Given a query binary code component Q, consisting of
basic blocks whose relation can be represented in a control
ﬂow graph (CFG), we are interested in ﬁnding programs, from
a large corpus of binary programs compiled for different archi-
tectures (e.g., x86 and ARM), that contain code components
semantically equivalent or similar to Q. A code component
here can be part of a function or contain multiple functions.
We examine code component semantics at three layers:
basic blocks, CFG paths, and code components. The system
architecture is shown in Figure 1. The inputs are the query
code component and a set of target programs. The front-end
disassembles each binary and constructs its CFG. (1) To model
the semantics of a basic block, we design the neural network-
based cross-lingual basic-block embedding model to represent
a basic block as an embedding. The embeddings of all blocks
are stored in a locality-sensitive hashing (LSH) database for
efﬁcient online search. (2) The path similarity comparison
component utilizes the LCS (Longest Common Subsequence)
algorithm to compare the semantic similarity of two paths, one
from the query code component and another from the target
program, constructed based on the LCS dynamic programming
algorithm with basic blocks as the sequence elements. The
3
Fig. 1: System architecture.
length of the common subsequence is then compared to the
length of the path from the query code component. The ratio
indicates the semantics of the query path as embedded in
the target program. (3) The component similarity comparison
component explores multiple path pairs to collectively calculate
a similarity score, indicating the likelihood of the query code
component being reused in the target program.
Basic Block Similarity Detection. The key is to measure the
similarity of two blocks, regardless of their target ISAs. As
shown in the right side of Figure 1, the neural network-based
cross-lingual basic-block embedding model takes a pair of
blocks as inputs, and computes a similarity score s ∈ [0, 1]
as the output. The objective is that the more the two blocks
are similar, the closer s is to 1, and the more the two blocks
are dissimilar, the closer s is to 0. To achieve this, the model
adopts a Siamese architecture [6] with each side employing
the LSTM [24]. The Siamese architecture is a popular network
architecture among tasks that involve ﬁnding similarity between
two comparable things [6], [10]. The LSTM is capable of
learning long range dependencies of a sequence. The two
LSTMs are trained jointly to tolerate the cross-architecture
syntactic variations. The model is trained using a large dataset,
which contains a large number of basic block pairs with a
similarity score as the label for each pair (how to build the
dataset is presented in Section V-D).
A vector representation of an instruction and a basic block
is called an instruction embedding and a block embedding,
respectively. The block embedding model converts each block
into an embedding to facilitate comparison. Speciﬁcally, three
main steps are involved in evaluating the similarity of two
blocks, as shown on the right side of Figure 1. (1) Instruction
embedding generation: given a block, each of its instructions
is converted into an instruction embedding using an instruction
embedding matrix, which is learned via a neural network
(Section IV). (2) Basic-block embedding generation:
the
embeddings of instructions of each basic block are then fed into
a neural network to generate the block embedding (Section V).
(3) Once the embeddings of two blocks have been obtained,
their similarity can be calculated efﬁciently by measuring the
distance between their block embeddings.
A prominent advantage of the model inherited from Neural
Machine Translation is that it does not need to select features
manually when training the models; instead, as we will show
later, the models automatically learn useful features during the
training process. Besides, prior state-of-the-art, Genius [19]
and Gemini [65], which use manually selected basic-block
4
Fig. 2: A sliding window used in skip-gram.
features, largely loses the information such as the semantics of
instructions and their dependencies. As a result, the precision
of our approach outperforms theirs by large margin. This is
shown in our evaluation (Section VII-E3).
IV.
INSTRUCTION EMBEDDING GENERATION
An instruction includes an opcode (specifying the operation
to be performed) and zero or more operands (specifying
registers, memory locations, or literal data). For example, mov
eax, ebx is an instruction where mov is an opcode and
both eax and ebx are operands.2 In NMT, words are usually
converted into word embeddings to facilitate further processing.
Since we regard instructions as words, similarly we represent
instructions as instruction embeddings.
Our notations use blackboard bold upper case to denote
functions (e.g., F), capital letters to denote basic blocks (e.g.,
B), bold upper case to represent matrices (e.g., U, W), bold
lower case to represent vectors (e.g., x, yi), and lower case to
represent individual instructions in a basic block (e.g., x1, y2).
A. Background: Word Embedding
A unique aspect of NMT is its frequent use of word
embeddings, which represent words in a high-dimensional
space, to facilitate the further processing in neural networks.
In particular, a word embedding is to capture the contextual
semantic meaning of the word; thus, words with similar contexts
have embeddings close to each other in the high-dimensional
space. Recently, a series of models [42], [43], [4] based on
neural networks have been proposed to learn high-quality word
embeddings. Among these models, Mikolov’s skip-gram model
is popular due to its efﬁciency and low memory usage [42].
The skip-gram model learns word embeddings by using a
neural network. During training, a sliding window is employed
on a text stream. In Figure 2, for example, a window of size
2 is used, covering two words behind the current word and
2Assembly code in this paper adopts the Intel syntax, i.e., op dst,
src(s).
Basic-block embedding generationBasic-block embedding generationSim ( b1, b2 )b1b2Instruction embedding generationB1B2Siamese architectureTarget programsFront-endPath similarity comparisonDetection resultQuery code segmentNeural network-based cross-lingual basic-block embedding modelComponent similarity comparison Neural machine translation made rapid progress recently.wtContext words (Ct); window size is 2. two words ahead. The model starts with a random vector for
each word, and then gets trained when going over each sliding
window. In each sliding window, the embedding of the current
word, wt, is used as the parameter vector of a softmax function
(Equation 1) that takes an arbitrary word wk as a training input
and is trained to predict a probability of 1, if wk appears in the
context Ct (i.e., the sliding window) of wt, and 0, otherwise.
P (wk ∈ Ct|wt) =
(1)
(cid:80)
exp(wT
t wk)
exp(wT
wi∈Ct
t wi)
where wk, wt, and wi are the embeddings of words wk, wt,
and wi, respectively.
Thus, given an arbitrary word wk, its vector representation
wk is used as a feature vector in the softmax function
parameterized by wt. When trained on a sequence of T words,
the model uses stochastic gradient descent to maximize the
log-likelihood objective J(w) as showed in Equation 2.
T(cid:88)
(cid:88)
because the denominator(cid:80)
J(w) =
1
T
t=1
wk∈Ct
wi∈Ct
exp(wT
However, it would be very expensive to maximize J(w),
t wi) sums over all
words wi in Ct. To minimize the computational cost, popular
solutions are negative sampling and hierarchical softmax. We
adopt the skip-gram with negative sampling model (SGNS) [43].
After the model is trained on many sliding windows, the
embeddings of each word become meaningful, yielding similar
vectors for similar words. Due to its simple architecture and
the use of the hierarchical softmax, the skip-gram model can
be trained on a desktop machine at billions of words per hour.
Plus, training the model is entirely unsupervised.
(log P (wk|wt))
(2)
B. Challenges
Some unique challenges arise when learning instruction
embeddings. First, in NMT, a word embedding model is usually
trained once using large corpora, such as Wiki, and then reused
by other researchers. However, we have to train an instruction
embedding model from scratch.
Second, if a trained model is used to convert a word that
has never appeared during training, the word is called an out-
of-vocabulary (OOV) word and the embedding generation for
such words will fail. This is a well-known problem in NLP, and
it exacerbates signiﬁcantly in our case, as constants, address
offsets, labels, and strings are frequently used in instructions.
How to deal with the OOV problem is a challenge.
C. Building Training Dataset
Because we regard blocks as sentences, we use instructions
of each block, called a Block-level Instruction Stream (BIS)
(Deﬁnition 1), to train the instruction embedding model.
Deﬁnition 1: (Block-level Instruction Stream) Given a basic
block B, consisting of a list of instructions. The block-level
instruction stream (BIS) of B, denoted as π(B), is deﬁned as
π(B) = (b1,··· , bn)
where bi is an instruction in B.
5
Fig. 3: Learning instruction embeddings for x86. π(B(j)
i )
represents the i-th block-level instruction stream (BIS) in the
function Fj. Each square in a BIS represents an instruction.
Preprocessing Training Data. To resolve the OOV problem,
we propose to preprocess the instructions in the training dataset
using the following rules: (1) The number constant values are
replaced with 0, and the minus signs are preserved. (2) The
string literals are replaced with . (3) The function names
are replaced with FOO. (4) Other symbol constants are replaced
with . Take the following code snippets as an example:
the left code snippet shows the original assembly code, and
the right one is the preprocessed result.
MOVL %ESI, $.L.STR.31
MOVL %EDX, $3
MOVQ %RDI, %RAX
CALLQ STRNCMP
TESTL %EAX, %EAX
JE .LBB0_5
MOVL ESI, 
MOVL EDX, 0
MOVQ RDI, RAX
CALLQ FOO
TESTL EAX, EAX
JE 
Note that the same preprocessing rules are applied to
instructions before generating their embeddings. This way, we
can signiﬁcantly reduce the OOV cases. Our evaluation result
(Section VII-C) shows that, after a large number of preprocessed
instructions are collected to train the model, we encounter very
few OOV cases in the later testing phase. This means the trained
model is readily reusable for other researchers. Moreover,
semantically similar instructions indeed have embeddings that
are close to each other, as predicted.
D. Training Instruction Embedding Model
We adopt the skip-gram negative sampling model as imple-
mented in word2vec [42] to build our instruction embedding
model. As an example, Figure 3 shows the process of training
the model for the x86 architecture. For each architecture, an
architecture-speciﬁc model is trained using the functions in our
dataset containing binaries of that architecture. Each function
is parsed to generate the corresponding Block-level Instruction
Streams (BISs), which are fed, BIS by BIS, into the model for
training. The training result is an embedding matrix containing
the numerical representation of each instruction.
The resultant instruction embedding matrix is denoted by
W ∈ Rde×V , where de is the dimensionality of the instruction
embedding selected by users (how to select de is discussed
in Section VII-F) and V is the number of distinct instructions
in the vocabulary. The i-th column of W corresponds to the
instruction embedding of the i-th instruction in the vocabulary
(all distinct instructions form a vocabulary).
word2vecAn Instruction embedding matrix Wx86 f1f2p (B1(2))p (B2(2))p (B1(1))p (B2(1))p (B3(1))p (f1 )p (f2 )Fig. 4: C source of a basic block from ec_mult.c in
OpenSSL and the assembly code for two architectures.
V. BLOCK EMBEDDING GENERATION
A straightforward attempt for generating the embedding
of a basic block is to simply compose (e.g., summing up) all
embeddings of the instruction in the basic block. However, this
processing cannot handle the cross-architecture differences, as
instructions that stem from the same source code but belong
to different architectures may have very different embeddings.
This is veriﬁed in our evaluation (Section VII-D).
Figure 4 shows a code snippet (containing one basic block)
that has been compiled targeting two different architectures,
x86-64 and ARM. While the two pieces of binary code are
semantically equivalent, they look very different due to different
instructions sets, CPU registers, and memory addressing modes.
The basic block embedding generation should be able to handle
such syntactic variation.
A. Background: LSTM in NLP
RNN is a type of deep neural network that has been
successfully applied to converting word embeddings of a
sentence to a sentence embedding [12], [30]. As a special
kind of RNN, LSTM is developed to address the difﬁculty of
capturing long term memory in the basic RNN. A limit of 500
words for the sentence length is often used in practice, and a
basic block usually contains less than 500 instructions.
In text analysis, LSTM treats a sentence as a sequence
of words with internal structures, i.e., word dependencies.
It encodes the semantic vector of a sentence incrementally,
left-to-right and word-by-word. At each time step, a new
word is encoded and the word dependencies embedded in
the vector are “updated”. When the process reaches the end of
the sentence, the semantic vector has embedded all the words
and their dependencies, and hence, can be viewed as a feature
representation of the whole sentence. That semantic vector is
the sentence embedding.
B. Cross-lingual Basic-block Embedding Model Architecture
Inspired by the NMT model that compares the similarity of
sentences of different languages, we design a neural network-
based cross-lingual basic-block embedding model to compare
the semantics similarity of basic blocks of different ISAs.
As showed in Figure 5, we design our model as a Siamese
architecture [6] with each side employing the identical LSTM.
6
Fig. 5: Neural network-based basic-block embedding model.
Each shaded box is an LSTM cell.
Our objective is to make the embeddings for blocks of similar
semantics as close as possible, and meanwhile, to make blocks
of different semantics as far apart as possible. A Siamese
architecture takes the embeddings of instructions in two blocks,
B1 and B2, as inputs, and produces the similarity score as
an output. This model is trained with only supervision on a
basic-block pair as input and the ground truth χ(B1, B2) as an
output without relying on any manually selected features.
For embedding generation, each LSTM cell sequentially
takes an input (for the ﬁrst layer the input is an instruction
embedding) at each time step, accumulating and passing
increasingly richer information. When the last
instruction
embedding is reached, the last LSTM cell at the last layer
provides a semantic representation of the basic block, i.e., a
block embedding. Finally, the similarity of the two basic blocks