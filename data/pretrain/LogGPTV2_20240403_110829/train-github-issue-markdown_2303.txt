 **Is this a request for help?** (If yes, you should use our troubleshooting
guide and community support channels, see
http://kubernetes.io/docs/troubleshooting/.):
**What keywords did you search in Kubernetes issues before filing this one?**
(If you have found any duplicates, you should instead reply there.):
* * *
**Is this a BUG REPORT or FEATURE REQUEST?** (choose one):
**Kubernetes version** (use `kubectl version`):
**Environment** :
  * AWS
  * Xenial
  * Terraform
**What happened** :  
I am setting up a K8s cluster using Terraform + kubeadm in AWS. Etcd, kube
master, and all kube nodes are in auto-scaling groups and behind load
balancers, with the idea being that if some or all nodes fail, the cluster
will heal itself. Most recently, I have been working to get the aws cloud-
provider to work. At one point, I corrected my kubelet systemd file and
deleted the kmaster and knode instances. When they came back up, the discovery
service failed to start on the master node.
     failed to create "kube-discovery" deployment [deployments.extensions "kube-discovery" already exists]
With the existing deployment, I was surprised that a new discovery pod wasn't
scheduled, but it stayed in pending.
    root@ip-10-253-128-238:~# kubectl describe pod/kube-discovery-1150918428-aca0v --namespace=kube-system
    Name:       kube-discovery-1150918428-aca0v
    Namespace:  kube-system
    Node:       /
    Labels:     component=kube-discovery
            k8s-app=kube-discovery
            kubernetes.io/cluster-service=true
            name=kube-discovery
            pod-template-hash=1150918428
            tier=node
    Status:     Pending
    IP:     
    Controllers:    ReplicaSet/kube-discovery-1150918428
    Containers:
      kube-discovery:
        Image:  gcr.io/google_containers/kube-discovery-amd64:1.0
        Port:   9898/TCP
        Command:
          /usr/local/bin/kube-discovery
        Volume Mounts:
          /tmp/secret from clusterinfo (ro)
          /var/run/secrets/kubernetes.io/serviceaccount from default-token-lhygh (ro)
        Environment Variables:  
    Conditions:
      Type      Status
      PodScheduled  False 
    Volumes:
      clusterinfo:
        Type:   Secret (a volume populated by a Secret)
        SecretName: clusterinfo
      default-token-lhygh:
        Type:   Secret (a volume populated by a Secret)
        SecretName: default-token-lhygh
    QoS Class:  BestEffort
    Tolerations:    dedicated=master:NoSchedule
    Events:
      FirstSeen LastSeen    Count   From            SubobjectPath   Type        Reason          Message
      --------- --------    -----   ----            -------------   --------    ------          -------
      59m       20s     147 {default-scheduler }            Warning     FailedScheduling    pod (kube-discovery-1150918428-aca0v) failed to fit in any node
    fit failure on node (ip-10-253-128-238.ec2.internal): MatchNodeSelector
**What you expected to happen** :  
Either for kubeadm to delete the existing deployment, or for that deployment
to sucessfully schedule new pods on the new master node.
**How to reproduce it** (as minimally and precisely as possible):  
Bring up a cluster using kubeadm and external etcd. After the cluster is up,
delete the master node and try to bring it back up (using the same command as
the first time).