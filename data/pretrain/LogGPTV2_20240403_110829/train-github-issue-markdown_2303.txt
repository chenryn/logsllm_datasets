### Request for Help

If you require assistance, please refer to our [troubleshooting guide](http://kubernetes.io/docs/troubleshooting/) and community support channels.

### Keywords Searched in Kubernetes Issues

Please list the keywords you searched in the Kubernetes issues before filing this one. If you found any duplicates, kindly reply there instead.

### Issue Type

- **Is this a BUG REPORT or FEATURE REQUEST?** (Choose one)

### Kubernetes Version

- **Kubernetes version** (Use `kubectl version`):

### Environment

- **Cloud Provider**: AWS
- **OS**: Xenial
- **Provisioning Tool**: Terraform

### Description of the Issue

I am setting up a Kubernetes (K8s) cluster using Terraform and kubeadm on AWS. The etcd, kube master, and all kube nodes are configured in auto-scaling groups and placed behind load balancers. This setup is intended to allow the cluster to self-heal if some or all nodes fail. Recently, I have been working on getting the AWS cloud-provider to function correctly.

During this process, I corrected my kubelet systemd file and deleted the kmaster and knode instances. When these instances came back up, the discovery service failed to start on the master node. The error message was:

```
failed to create "kube-discovery" deployment [deployments.extensions "kube-discovery" already exists]
```

Despite the existing deployment, a new discovery pod was not scheduled and remained in a pending state.

#### Detailed Pod Information

```sh
root@ip-10-253-128-238:~# kubectl describe pod/kube-discovery-1150918428-aca0v --namespace=kube-system
Name:               kube-discovery-1150918428-aca0v
Namespace:          kube-system
Node:               /
Labels:             component=kube-discovery
                    k8s-app=kube-discovery
                    kubernetes.io/cluster-service=true
                    name=kube-discovery
                    pod-template-hash=1150918428
                    tier=node
Status:             Pending
IP:                 
Controllers:        ReplicaSet/kube-discovery-1150918428
Containers:
  kube-discovery:
    Image:         gcr.io/google_containers/kube-discovery-amd64:1.0
    Port:          9898/TCP
    Command:       /usr/local/bin/kube-discovery
    Volume Mounts:
      /tmp/secret from clusterinfo (ro)
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-lhygh (ro)
    Environment Variables:  
Conditions:
  Type              Status
  PodScheduled      False 
Volumes:
  clusterinfo:
    Type:            Secret (a volume populated by a Secret)
    SecretName:      clusterinfo
  default-token-lhygh:
    Type:            Secret (a volume populated by a Secret)
    SecretName:      default-token-lhygh
QoS Class:          BestEffort
Tolerations:        dedicated=master:NoSchedule
Events:
  FirstSeen   LastSeen   Count   From                SubobjectPath   Type        Reason          Message
  ---------   --------   -----   ----                -------------   --------    ------          -------
  59m         20s        147     {default-scheduler}                Warning      FailedScheduling  pod (kube-discovery-1150918428-aca0v) failed to fit in any node
  fit failure on node (ip-10-253-128-238.ec2.internal): MatchNodeSelector
```

### Expected Behavior

- Either kubeadm should delete the existing deployment.
- Or, the existing deployment should successfully schedule new pods on the new master node.

### Steps to Reproduce

1. Set up a Kubernetes cluster using kubeadm with an external etcd.
2. After the cluster is up and running, delete the master node.
3. Attempt to bring the master node back up using the same command as initially used.

Thank you for your assistance.