## 可靠性
人们对于一个东西是否可靠，都有一个直观的想法。人们对可靠软体的典型期望包括：
* 应用程式表现出使用者所期望的功能。
* 允许使用者犯错，允许使用者以出乎意料的方式使用软体。
* 在预期的负载和资料量下，效能满足要求。
* 系统能防止未经授权的访问和滥用。
如果所有这些在一起意味著 “正确工作”，那么可以把可靠性粗略理解为 “即使出现问题，也能继续正确工作”。
造成错误的原因叫做 **故障（fault）**，能预料并应对故障的系统特性可称为 **容错（fault-tolerant）** 或 **回弹性（resilient）**。“**容错**” 一词可能会产生误导，因为它暗示著系统可以容忍所有可能的错误，但在实际中这是不可能的。比方说，如果整个地球（及其上的所有伺服器）都被黑洞吞噬了，想要容忍这种错误，需要把网路托管到太空中 —— 这种预算能不能批准就祝你好运了。所以在讨论容错时，只有谈论特定型别的错误才有意义。
注意 **故障（fault）** 不同于 **失效（failure）**【2】。**故障** 通常定义为系统的一部分状态偏离其标准，而 **失效** 则是系统作为一个整体停止向用户提供服务。故障的机率不可能降到零，因此最好设计容错机制以防因 **故障** 而导致 **失效**。本书中我们将介绍几种用不可靠的部件构建可靠系统的技术。
反直觉的是，在这类容错系统中，透过故意触发来 **提高** 故障率是有意义的，例如：在没有警告的情况下随机地杀死单个程序。许多高危漏洞实际上是由糟糕的错误处理导致的【3】，因此我们可以透过故意引发故障来确保容错机制不断执行并接受考验，从而提高故障自然发生时系统能正确处理的信心。Netflix 公司的 *Chaos Monkey*【4】就是这种方法的一个例子。
尽管比起 **阻止错误（prevent error）**，我们通常更倾向于 **容忍错误**。但也有 **预防胜于治疗** 的情况（比如不存在治疗方法时）。安全问题就属于这种情况。例如，如果攻击者破坏了系统，并获取了敏感资料，这种事是撤销不了的。但本书主要讨论的是可以恢复的故障种类，正如下面几节所述。
### 硬体故障
当想到系统失效的原因时，**硬体故障（hardware faults）** 总会第一个进入脑海。硬碟崩溃、记忆体出错、机房断电、有人拔错网线…… 任何与大型资料中心打过交道的人都会告诉你：一旦你拥有很多机器，这些事情 **总** 会发生！
据报道称，硬碟的 **平均无故障时间（MTTF, mean time to failure）** 约为 10 到 50 年【5】【6】。因此从数学期望上讲，在拥有 10000 个磁碟的储存丛集上，平均每天会有 1 个磁碟出故障。
为了减少系统的故障率，第一反应通常都是增加单个硬体的冗余度，例如：磁碟可以组建 RAID，伺服器可能有双路电源和热插拔 CPU，资料中心可能有电池和柴油发电机作为后备电源，某个元件挂掉时冗余元件可以立刻接管。这种方法虽然不能完全防止由硬体问题导致的系统失效，但它简单易懂，通常也足以让机器不间断执行很多年。
直到最近，硬体冗余对于大多数应用来说已经足够了，它使单台机器完全失效变得相当罕见。只要你能快速地把备份恢复到新机器上，故障停机时间对大多数应用而言都算不上灾难性的。只有少量高可用性至关重要的应用才会要求有多套硬体冗余。
但是随著资料量和应用计算需求的增加，越来越多的应用开始大量使用机器，这会相应地增加硬体故障率。此外，在类似亚马逊 AWS（Amazon Web Services）的一些云服务平台上，虚拟机器例项不可用却没有任何警告也是很常见的【7】，因为云平台的设计就是优先考虑 **灵活性（flexibility）** 和 **弹性（elasticity）**[^i]，而不是单机可靠性。
如果在硬体冗余的基础上进一步引入软体容错机制，那么系统在容忍整个（单台）机器故障的道路上就更进一步了。这样的系统也有运维上的便利，例如：如果需要重启机器（例如应用作业系统安全补丁），单伺服器系统就需要计划停机。而允许机器失效的系统则可以一次修复一个节点，无需整个系统停机。
[^i]: 在 [应对负载的方法](#应对负载的方法) 一节定义
### 软体错误
我们通常认为硬体故障是随机的、相互独立的：一台机器的磁碟失效并不意味著另一台机器的磁碟也会失效。虽然大量硬体元件之间可能存在微弱的相关性（例如伺服器机架的温度等共同的原因），但同时发生故障也是极为罕见的。
另一类错误是内部的 **系统性错误（systematic error）**【8】。这类错误难以预料，而且因为是跨节点相关的，所以比起不相关的硬体故障往往可能造成更多的 **系统失效**【5】。例子包括：
* 接受特定的错误输入，便导致所有应用伺服器例项崩溃的 BUG。例如 2012 年 6 月 30 日的闰秒，由于 Linux 核心中的一个错误【9】，许多应用同时挂掉了。
* 失控程序会用尽一些共享资源，包括 CPU 时间、记忆体、磁碟空间或网路频宽。
* 系统依赖的服务变慢，没有响应，或者开始返回错误的响应。
* 级联故障，一个元件中的小故障触发另一个元件中的故障，进而触发更多的故障【10】。
导致这类软体故障的 BUG 通常会潜伏很长时间，直到被异常情况触发为止。这种情况意味著软体对其环境做出了某种假设 —— 虽然这种假设通常来说是正确的，但由于某种原因最后不再成立了【11】。
虽然软体中的系统性故障没有速效药，但我们还是有很多小办法，例如：仔细考虑系统中的假设和互动；彻底的测试；程序隔离；允许程序崩溃并重启；测量、监控并分析生产环境中的系统行为。如果系统能够提供一些保证（例如在一个讯息伫列中，进入与发出的讯息数量相等），那么系统就可以在执行时不断自检，并在出现 **差异（discrepancy）** 时报警【12】。
### 人为错误
设计并构建了软体系统的工程师是人类，维持系统执行的运维也是人类。即使他们怀有最大的善意，人类也是不可靠的。举个例子，一项关于大型网际网路服务的研究发现，运维配置错误是导致服务中断的首要原因，而硬体故障（伺服器或网路）仅导致了 10-25% 的服务中断【13】。
尽管人类不可靠，但怎么做才能让系统变得可靠？最好的系统会组合使用以下几种办法：
* 以最小化犯错机会的方式设计系统。例如，精心设计的抽象、API 和管理后台使做对事情更容易，搞砸事情更困难。但如果介面限制太多，人们就会忽略它们的好处而想办法绕开。很难正确把握这种微妙的平衡。
* 将人们最容易犯错的地方与可能导致失效的地方 **解耦（decouple）**。特别是提供一个功能齐全的非生产环境 **沙箱（sandbox）**，使人们可以在不影响真实使用者的情况下，使用真实资料安全地探索和实验。
* 在各个层次进行彻底的测试【3】，从单元测试、全系统整合测试到手动测试。自动化测试易于理解，已经被广泛使用，特别适合用来覆盖正常情况中少见的 **边缘场景（corner case）**。
* 允许从人为错误中简单快速地恢复，以最大限度地减少失效情况带来的影响。例如，快速回滚配置变更，分批发布新程式码（以便任何意外错误只影响一小部分使用者），并提供资料重算工具（以备旧的计算出错）。
* 配置详细和明确的监控，比如效能指标和错误率。在其他工程学科中这指的是 **遥测（telemetry）**（一旦火箭离开了地面，遥测技术对于跟踪发生的事情和理解失败是至关重要的）。监控可以向我们发出预警讯号，并允许我们检查是否有任何地方违反了假设和约束。当出现问题时，指标资料对于问题诊断是非常宝贵的。
* 良好的管理实践与充分的培训 —— 一个复杂而重要的方面，但超出了本书的范围。
### 可靠性有多重要？
可靠性不仅仅是针对核电站和空中交通管制软体而言，我们也期望更多平凡的应用能可靠地执行。商务应用中的错误会导致生产力损失（也许资料报告不完整还会有法律风险），而电商网站的中断则可能会导致收入和声誉的巨大损失。
即使在 “非关键” 应用中，我们也对使用者负有责任。试想一位家长把所有的照片和孩子的影片储存在你的照片应用里【15】。如果资料库突然损坏，他们会感觉如何？他们可能会知道如何从备份恢复吗？