title:Toward dependability benchmarking of partitioning operating systems
author:Raul Barbosa and
Johan Karlsson and
Qiu Yu and
Xiaozhen Mao
Toward Dependability Benchmarking of Partitioning Operating Systems
Raul Barbosa
Department of Informatics Engineering
Johan Karlsson, Qiu Yu, Xiaozhen Mao
Department of Computer Science and Engineering
University of Coimbra
3030-290 Coimbra, Portugal
Email: PI:EMAIL
Chalmers University of Technology
SE-412 96 Gothenburg, Sweden
Email: PI:EMAIL
Abstract—This paper describes a dependability benchmark
intended to evaluate partitioning operating systems. The bench-
mark includes both hardware and software faultloads and
measures the spatial as well as the temporal isolation among
tasks, provided by a given real-time operating system. To
validate the benchmark, a prototype implementation is carried
out and three targets are benchmarked according to the
speciﬁed process. The results substantiate that the proposed
benchmark is able to compare and rank the targets in an
objective way, and that it provides the ability to identify aspects
of the target systems that need improvement.
Keywords-dependability benchmarking; fault injection; par-
titioning; operating systems; fault tolerance.
I. INTRODUCTION
Benchmarking the dependability of a computer system is
the means to evaluate or characterize its behavior in the pres-
ence of faults. Such an evaluation involves deﬁning relevant
measures (related to dependability as well as performance)
and specifying the procedure to obtain the necessary mea-
surements [1]. A good dependability benchmark provides the
ability to make a fair comparison between different systems,
and may be used to guide the development of dependability-
related functions.
To be useful, benchmarks usually represent the agree-
ment between industry and academia within a concrete
and well-deﬁned domain. This paper addresses the design
and validation of a dependability benchmark for real-time
operating systems, speciﬁcally focusing on the evaluation
of partitioning mechanisms. Real-time operating systems
are critical components in safety-related applications, e.g.,
in the avionics and in the automotive domains. To reduce
the number of hardware units deployed in each system, the
embedded systems industry is adopting methods to integrate
distinct functions and software components into each hard-
ware unit. Such integrated architectures have the potential
to reduce cost and increase reliability, but to achieve these
improvements it is necessary to develop partitioning mecha-
nisms to prevent faults in one function from affecting other
coexisting functions [2].
There are presently a large number of partitioning op-
erating systems available from diverse vendors [3]. How-
ever, there is no existing benchmark capable of evaluating
and comparing their ability to prevent error propagation
among processes or partitions. A great deal of research,
including concrete dependability benchmarks, has focused
on the robustness of the system call interface [4], on the
resilience against faulty device drivers [5], and on the ability
to cope with faults directly affecting the operating system
functions [6]. This paper complements these research efforts
with an approach to benchmarking the ability of an operating
system to prevent error propagation between applications.
The benchmarking approach consists in injecting faults
into one task and observing whether it has any impact on
the remaining tasks. We observe the logical value of the
output produced by each task as well as the time in which
the output is produced, to evaluate both spatial and temporal
partitioning. Several workloads can be used to exercise a
given benchmark target (i.e., a partitioning operating system)
and the benchmark speciﬁes faultloads based on software
faults as well as hardware faults.
A prototype implementation of the partitioning bench-
mark, using the GOOFI-2 tool [7] to conduct
the ex-
periments, was applied to evaluate three different targets:
µC/OS-II with Secern, µC/OS-II without partitioning, and
a basic scheduler. The ﬁrst target consists of the µC/OS-
II kernel running with the Secern partitioning extension [8];
the second target consists of the µC/OS-II kernel [9] without
any partitioning mechanisms; and the third target consists
of a simple scheduler that switches among different tasks
and provides no partitioning mechanisms. The goal was to
validate the benchmark in its ability to evaluate and compare
the different alternatives.
Given that Secern is a priori the most robust – it is
the only one equipped with partitioning mechanisms – a
partitioning benchmark should be able to rank it higher than
the other two. On the other hand, if this is not the case,
or if Secern has some vulnerabilities, a partitioning bench-
mark should provide information to guide the development
effort to improve the mechanisms. With this hypothesis as
background, this paper speciﬁes the benchmark procedure
and describes the experimental validation conducted on the
three benchmark targets.
The following section describes related work in de-
pendability benchmarking of operating systems. Section III
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:27:15 UTC from IEEE Xplore.  Restrictions apply. 
978-1-4244-9233-6/11/$26.00 ©2011 IEEE422speciﬁes the benchmark in detail, including the workloads,
faultloads, measures and benchmark procedures. Section IV
describes the experimental validation of the benchmark and
Section V discusses the experimental results. Lastly, the
main conclusions are summarized in Section VI.
II. RELATED WORK
Although dependability benchmarking is a recent ﬁeld,
there have already been numerous research efforts to deﬁne
practical benchmarks for speciﬁc domains [1]. These aim at
characterizing speciﬁc aspects related to the dependability of
a given system, such as error detection coverage, recovery
time, etc. The type of system under benchmarking can be
for instance a Web server, an OLTP system, or an operating
system.
Speciﬁcally addressing operating systems, and closely re-
lated to our work, the DeBERT benchmark aims to evaluate
the robustness of system calls [10]. Faults are injected by
corrupting one parameter during a function call and the
main focus of the dependability measures is response time.
Another approach that uses corruption of parameters to
the system call is the Ballista project [4]. Several other
authors have addressed the problem of benchmarking op-
erating systems with respect to application errors [11], [12].
These approaches are complementary to the one described
in this paper. Our goal is to evaluate if faults are able to
propagate from one application to another, rather than from
one application to the operating system.
Another area rich in contributions is the evaluation of re-
silience against faulty device drivers [5], [13]. Device drivers
are usually identiﬁed as a problematic area, given that driver
code usually has a higher defect density, and the potential
for fault propagation is also greater. Similarly, there have
been attempts to characterize the ability to cope with faults
affecting operating system functions [6]. These approaches
can be considered complementary to ours as well. Ideally,
a complete dependability benchmark for operating systems
should provide the means to evaluate the behaviour in the
presence of faults in any component – applications, drivers,
and the operating system itself.
III. BENCHMARK SPECIFICATION
The objective of this benchmark speciﬁcation is to allow
practitioners to evaluate the effectiveness of partitioning
mechanisms included in real-time operating systems. Poten-
tial benchmark targets are therefore any real-time operating
system, and the proposed benchmark evaluates the ability
to isolate faulty tasks in terms of spatial partitioning and
temporal partitioning. This section deﬁnes the faultloads, the
workloads, the benchmarking procedure and the dependabil-
ity measures.
A. Faultloads
The system should be evaluated against a faultload based
on hardware faults and a faultload based on software faults:
• Hardware faults. This faultload consists of single bit-
ﬂips affecting the context of one partition, including
memory locations and processor registers. This model
is used to emulate the effects of transient hardware
faults. Since, for a typical real-time kernel, most of the
time is spent executing tasks or in idle computation,
it is likely for a transient hardware fault to affect the
context of a single task. Our goal is to determine how
probable it is for such an error to propagate to outside
the partition and affect other partitions. Although single
bit-ﬂips in memory are not representative of faults oc-
curring in memory with error detection and correction,
these emulate transient faults occurring in the processor
or in the buses while the data is in transit, but before
the code is computed.
• Software faults. This faultload is intended to emulate
software defects in applications. This is a major concern
for partitioning operating systems, since these may be
used to execute a mixture of critical and non-critical
tasks, possibly with distinct levels of certiﬁcation. Con-
sequently, it is of extreme importance to guarantee that
software faults in one partition cannot propagate to
other partitions. To emulate software faults we use a
set of operators based on ﬁeld data [14].
B. Workloads
We created six workloads, speciﬁcally for the proposed
partitioning benchmark, consisting of programs used in
diverse domains where real-time operating systems are de-
ployed. The workloads are structured as a main loop that
repeats the operations indeﬁnitely. In order to increase the
conﬁdence in the results, one should run campaigns with
many different workload combinations, possibly with more
than two workloads at a time. The six workloads are:
• CRC32 Calculates the 32-bit cyclic redundancy check
for an input consisting of a ﬁxed number of bytes. This
is typically useful for network and communications.
• Altimeter Approximates the altitude given a pressure
value. This application is representative of the avionics
domain.
• Hamming Distance Computes the Hamming distance
of two numbers. It is a fundamental function in com-
munications and generally useful in information theory.
• Bit Count Counts the number of ones in a given
integer.
• LU Decomposition Decomposes an input matrix into
its corresponding lower and upper matrices.
• Merge Sort Sorts a sequence of integer numbers using
the merge sort algorithm.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:27:15 UTC from IEEE Xplore.  Restrictions apply. 
423C. Benchmarking Procedure
We begin by deﬁning the terms fault injection experiment
and fault injection campaign. A fault injection experiment
corresponds to injecting one fault and making measurements
on how the benchmark target subsequently behaves. A fault
injection campaign corresponds to a series of fault injec-
tion experiments that share the same conﬁguration. Each
campaign begins with a golden run, i.e., an experiment in
which no fault is injected, aiming at observing the fault-free
behavior of the system. In our platform, a single golden run
is sufﬁcient, as the system is nearly deterministic. When this
is not the case (e.g., if task scheduling is subject to jitter) one
should consider multiple golden runs to capture the normal
operation proﬁle.
The ﬁrst phase of the benchmarking procedure consists
in conﬁguring the workloads. A benchmark practitioner may
choose numerous distinct combinations of the six workloads
to exercise the benchmark target. At least two workloads
are necessary in any given conﬁguration – one workload in
which faults are injected, and one to be observed to check
for error propagation. For each conﬁguration of workloads
one should execute a campaign for the faultload based on
software faults, and one for the faultload based on hardware
faults.
In the second phase one executes the fault
injection
campaigns and collects the necessary data to calculate the
dependability measures. The output of each workload should
be collected and stored as a sequence of tuples (cid:104)v, t(cid:105), where
v represents the logical value of a workload’s output and t
represents the time in which that value was produced. This
will allow each experiment to be classiﬁed with respect to
partitioning violations. If t differs when compared to the
expected value, a temporal violation occurs; if v differs when
compared to the golden run, the experiment is classiﬁed as
a spatial violation.
The third phase corresponds to the classiﬁcation of each
experiment of a given campaign and the calculation of the
dependability measures. For a given campaign consisting of
n fault injection experiments we begin by determining the
proportion of activated faults, i.e., those that cause any of
the workloads to produce an incorrect output value or at an
incorrect time. We introduce two measures of partitioning
coverage:
• PCs This metric evaluates the spatial partitioning cov-
erage. In a campaign consisting of n experiments where
m is the number of activated faults, PCs is calculated by
taking the number of experiments in which the output
of the fault-free workload is logically equal to the one
produced in the golden run, and dividing this number
by m. Note that this includes experiments in which the
workload crashes and stops producing results or misses
one of its outputs.
• PCt This metric evaluates the temporal partitioning
coverage. In a campaign with m activated faults, this
metric is calculated by taking the number of experi-
ments in which the timing of the output of the fault-
free workload is not produced more than 5% later than
in the golden run, and dividing this number by m.
The reason for dividing the number of partitioning viola-
tions by the number of activated faults in a given campaign
(rather than the number of injected faults), is to ensure that
the metrics are as fair as possible. Otherwise, a benchmark
target in which few faults become active would implicitly
have a better score, although the partitioning mechanisms
would not be exercised. Note that according to the deﬁnition,
a fault is considered to be activated if, when injected into one
partition, any of the partitions exhibits an incorrect output.
To evaluate temporal partitioning, we adopted a sim-
ple threshold-based approach where tasks producing results
more than 5% later than in the golden run are classiﬁed
as timing failures. This approach is sufﬁcient to evaluate
the target systems considered in this paper, given that these
only aim to guarantee the correctness of the task with the
highest priority, and that task should never be delayed by
any other task. However, in the general case, one should
state task deadlines and classify an experiment as a timing
failure only when the deadline is missed by a fault-free task
(as a result of an error in another task).
IV. EXPERIMENTAL VALIDATION
This section describes an experimental validation of the
proposed benchmark. The goal is to validate the bench-
mark’s ability to rank different operating systems with
respect to the coverage provided in terms of partitioning.
We implemented the benchmark and executed several fault
injection campaigns using GOOFI-2 [7] to collect the nec-
essary measurements.
A. Benchmark Targets
We selected three benchmark targets to conduct exper-
iments according to the benchmark speciﬁcation. One of
the target operating systems provides robust partitioning,
whereas the other two do not. The intention is to validate
the benchmark, as it should be able to rank the partitioning
operating system higher, or provide strong reasons against
that otherwise. The three benchmark targets are:
• µC/OS-II with Secern. Secern [8] is an extension to
the µC/OS-II kernel, aiming to provide robust partition-
ing and fault tolerance mechanisms, which is built on
top of a version of µC/OS-II which has no partitioning
mechanisms. Using Secern, the private address space
of each process (or partition) is protected by memory
management hardware and there are mechanisms to
control the access to the system call interface.
• µC/OS-II without partitioning. This benchmark target
is the same as the previous one, but with all partitioning
features provided by Secern disabled.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:27:15 UTC from IEEE Xplore.  Restrictions apply. 
424• Basic scheduler. The basic scheduler cannot be re-
garded as a true operating system, as it acts as a mere
scheduler executing the workloads in turns, inside a
loop. Like the previous one,
this benchmark target
provides no partitioning mechanisms.
B. Experimental Setup
The system under benchmarking, depicted in Figure 1
for µC/OS-II with Secern, consists of a workstation running
GOOFI-2 and a development board containing an MPC5554
processor. GOOFI-2 controls the experiments and collects
the necessary data to calculate the measures of partitioning.
Figure 1. Evaluation platform for µC/OS-II and Secern.
The faultload based on hardware faults is emulated using
the single bit-ﬂip error model. Errors are uniquely deﬁned
using time-location pairs. The location corresponds to a bit
in a register or a memory element; the time corresponds to
the execution of an instruction, identiﬁed by its code address
along with an invocation count that speciﬁes the number of
iterations before injection.
We used instrumentation-based error injection (a runtime
SWIFI technique that injects errors into registers and mem-