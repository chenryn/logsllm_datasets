because a front-end can still be overloaded if each traﬃc portion (split) is in
very high volume. Second, some distributed (and network-based) attacks (e.g.,
port-scan) may be missed due to load-balancing because the evidence gathered
at each front-end module may be below the detection threshold. We then need a
correlator, which is essentially a back-end module in our architecture, to detect
these attacks. Third, there are complex analysis tasks that should be performed
in a back-end module rather than a front-end module because of their computa-
tional time and space requirements. These tasks include attack scenario analysis
(more in Section 3.3), and alarm correlation and reduction, which are considered
very important and desired IDS features [30]. In our approach, the back-end can
also carry out some analysis tasks shed from the front-end.
3.1 Performance Monitoring
According to Problem (5), the IDS can provide the expected value V(P) only
when the constraint T (P) ≤ Dmax is satisﬁed. The IDS thus needs to self-
monitor the run-time conditions, and reconﬁgure itself to operate under the
(new) constraints when necessary. As discussed in Section 2.3, Dmax should be
the mean audit event inter-arrival time.
There are two approaches in monitoring T (P) ≤ Dmax. In internal measur-
ing, since the front-end ID module knows the arrival time and detection time of
each audit event, it can compute both Dmax and T (P) (and including mi, tij, πi,
and pij) as a moving average. Alternatively, to avoid the overhead, the front-end
can periodically check (e.g., via libpcap) whether it is dropping audit events,
and if so, conclude that it needs to reconﬁgure. In external testing, the manager
periodically sends out a simulated attack that contains an event marked “attack-
simulation”. The front-end, upon “detecting” this simulated attack, is required
to reply to the manager the T (P) value along with the sequence number of
the simulated attack. The manager can detect the condition where the returned
T (P) is out of bound (according to historical data), and thus concluding that
the front-end is overloaded. If the manager receives no reply, it concludes that
the front-end is at a “fault” state (e.g., crashed due to crash attacks [23], or an
inﬁnite-loop due to implementation errors), and can take immediate action such
as activating another (replacement) front-end module.
3.2 Dynamic Reconﬁguration
As described in Section 2.1, an IDS conﬁguration is characterized by its collection
of run-time analysis tasks. Although an IDS may have a very comprehensive set
of tasks that it can use, its optimal conﬁguration, that is, the solution to Problem
(5), may include only a subset of these tasks because of run-time constraints.
When performance adaptation is enabled in the IDS, this subset (the active
tasks) is dynamic, that is, re-computed whenever necessary, rather than static.
An implementation of the dynamic task set is to equip the ID modules with a
Performance Adaptation in Real-Time Intrusion Detection Systems
265
common and complete set of analysis tasks, and have non-overlapping bit masks
specifying which tasks are activated at each module.
In Section 2.5, we discussed the practical considerations in measuring the
parameters needed to compute the optimal IDS conﬁguration. We can use some
heuristics to improve the parameter estimations. The back-end can perform at-
tack scenario analysis (will be discussed in Section 3.3) and supply p(cid:2)
ij, the
probability of an attack given the traﬃc and attack conditions seen thus far.
We can use p(cid:2)
ij as the updated (posterior) probability of intrusion in place of pij
when computing V(P) and T (P). However, we need to avoid being fooled by an
intelligent attacker who tries to divert IDS resource from his intended attacks,
e.g, by ﬁrst launching (nuisance) attacks that seem to lead to some other possible
attacks so that they will have artiﬁcially (and falsely) much higher probabilities
than his intended one(s). One solution is to always capture audit data and per-
form analysis tasks for the critical services. This is equivalent to always setting
the values of these tasks the highest and their required time the smallest.
3.3 Scenario Analysis
A scenario is a sequence of related attacks that together accomplish a mali-
cious end-goal. We can use scenario analysis to predict the likely forthcoming
intrusions to make better load-shedding decisions.
We can use site-speciﬁc threat models to form a base set of known scenarios. A
scenario graph is a directed graph where an edge from node ai to aj labeled with
p(cid:2)
ij and condition(s) condij speciﬁes that after ai occurs, aj will occur next with
probability p(cid:2)
ij if condij is true. A path speciﬁes an attack scenario. In run-time,
each reported attack is described by a set of attributes: name (type), timestamp,
target IP, target port, etc. The back-end “attaches” each attack to a node in the
network topology graph using the target attributes, and examines whether the
attack is part of an existing scenario(s) there. Based on the currently recognized
(partial) scenarios, the back-end reports to the front-end and the manager the
possible attacks, their probabilities, and their likely targets. The attack and
probability information are used to compute V(P) and T (P) for load-shedding
decisions (see Section 3.2). The target information is very useful to determine
(if necessary) what portion of the audit data the front-end can stop capturing.
We are actively studying how to automatically update scenario information and
discover new scenarios.
3.4 Prototype Systems
We next describe prototype systems, and present some experiment results. Our
main goals here are to see how performance monitoring and dynamic conﬁgura-
tion mechanisms can be built into an IDS, and how such adaptive IDS perform
in overload situation.
266
W. Lee et al.
Adaptive Bro. Our adaptive IDS comprises of a front-end IDS, a back-end
IDS and a manager module. We use Bro version 0.7a90 (on OpenBSD 2.9) with
our modiﬁcations as the front-end IDS. The back-end module runs on a diﬀerent
machine and is connected to the front-end on a private network. The manager
runs on a third machine and is on the same network that Bro is monitoring.
We modiﬁed Bro in two main areas. The ﬁrst is in adding bookkeeping func-
tions for the purpose of performance monitoring. Note that a Bro “event” is
diﬀerent from an “audit event” (an audit record arriving at IDS event queue)
described in Section 2.1. The latter is equivalent to a packet. In Bro, events are
generated from the processing of packets, and intrusion analysis (i.e., rule check-
ing and logging) is performed on events rather than on packets. We discussed
the constraint T (P) ≤ Dmax in Section 2.3. T (P) is equivalent to the expected
packet service time in Bro, and Dmax is equivalent to the mean inter-packet
arrival time. Clearly, this constraint must still hold for Bro, otherwise there will
be packet dropping (and the quality of event data will suﬀer) and detection
performance can suﬀer. To accommodate the notion of event-level (versus of
packet-level) analysis in Bro, we use T (cid:2)(P) to represent expected event service
time, which is the interval between the arrival of the ﬁrst packet of the event
to the completion of analysis of the last packet of the event (also the comple-
tion of the event). We use D(cid:2)
max to represent the mean inter-event arrival (or
generation) time. For Bro events, we should use the more meaningful constraint
max. It is easy to see that using T (cid:2)(P) ≤ Dmax is incorrect, and
T (cid:2)(P) ≤ D(cid:2)
max will likely lead to T (P) > Dmax.
T (cid:2)(P) > D(cid:2)
Our Adaptive Bro thus has the following measurements: number of packets
received per second, number of packets dropped per second, mean inter-event
arrival time, and a counter of each event. The packets received and dropped
is available from the libpcap pcap stats function and is already implemented
in the Bro HeartBeat function. The interval between two heart beats can be
conﬁgured. We used 1 second in our experiments. By recording the number of
events generated within a time interval (which is 0.1 seconds), mean inter-event
arrival time is computed as an average. Bro initiates reconﬁguration in two cases:
if it detects that there are dropped packets; or if it discovers that T (cid:2)(P) > D(cid:2)
max.
The second area of changes to Bro is adding dynamic reconﬁguration mech-
anisms. Recall that the process of reconﬁguration is to then compute a new
optimal solution to Problem (5) according to the new run-time constraints, and
then deactivate some analysis tasks and/or cease to capture certain audit data
types according to the newly computed conﬁguration. We implemented a dy-
namic programming Knapsack algorithm. The parameters associated with the
event-level analysis tasks are initially measured using benchmark experiments
and stored in a system conﬁguration ﬁle. For example, the service time for a
speciﬁc event is the average time taken by Bro to process packets, generate the
event, and analyze the event (e.g., match it against rules). The parameters are
then loaded in an array in Bro start-time, and can be dynamically updated.
For example, πi and mi are measured as moving averages in run-time, and p(cid:2)
ij
from the scenario analysis function in the back-end can replace pij. The com-
Performance Adaptation in Real-Time Intrusion Detection Systems
267
puted conﬁguration is represented as an array of ﬂags (Bro script variables).
These ﬂags are checked before the event analysis tasks (handlers) are invoked.
If all event analysis tasks for an audit type are disabled, then libpcap ﬁlter
is also reset to cease capturing such data. Since compiling and loading a new
ﬁlter at the libpcap layer incurs signiﬁcant delay, we modiﬁed Bro to keep a
set of pre-compiled ﬁlters and load them when necessary. Also, when changing
packet ﬁlters, the pcap setfilter function invokes the ioctl kernel function.
It turns out that ioctl, while changing the ﬁlters, clears out the packets that
have not been passed to the upper layer. We took out the code that clears the
buﬀer to avoid losing those packets that might match the new ﬁlter. Finally,
Bro has an option to store (remember) the “default” conﬁguration, the start-up
conﬁguration which is considered as the ‘optimal” or desirable one under nor-
mal situations, so that if it was reconﬁgured and has been stable (no need to
reconﬁgure again) for several heart beats (in our experiments, we used 10), it
can switch back to run the default conﬁguration.
We brieﬂy describe other modules in our system. The main functions of the
manager are to collect statistics and intrusion reports from the Bro and the
back-end, and create logs and alerts. It sends a test periodically to Bro to mea-
sure delay. Bro also sends the performance measurements (e.g., the numbers of
packet received and drop) every heart beat. If the manager does not receive Bro
performance measurements and or a reply from its test for a time threshold, it
raises an alarm (to security staﬀ) that Bro has probably crashed. The policies on
the ﬁrewall can be dynamically conﬁgured by the front-end. For example, it can
terminate an oﬀending connection. It can also delay packets when instructed.
The main functions of the back-end module include: sharing analysis load, for
example, probe (scan) detection shed from the front-end, and performing at-
tack scenario analysis. Presently, we only have very primitive scenario analysis
functions.
Adaptive Snort. We also implemented an adaptive IDS using Snort version
1.8.6 with the latest rule set, and with libpcap version 0.6.2 and OpenBSD
version 2.9. Unlike Bro, Snort applies intrusion detection rules on packet data
directly rather than on “events” extracted from packet data. Snort supports
“plug-ins”, which can be pre-processors (e.g., de-fragmentation) or detection
rules. Snort is thus more loosely coupled and easier to customize. We wanted
to study how diﬀerent IDS architectures inﬂuence the implementation of perfor-
mance adaptation mechanisms.
In Snort, packets go through ﬁrst the pre-processors then the rule trees. A
RuleTreeNode determines whether a packet is a “match” and hence needs to
be examined by its OptTreeNodes. In Bro, we can measure service time at the
Bro event level and use event service time to include preprocessing and event
analysis time because each packet contributes to a Bro event. For Snort, we need
to measure the service time at the packet level. There are two cases. First, for
packets that match an OptTreeNode (i.e., they match or “belong to” a partic-
ular Snort rule), the service time is preprocessing time plus rule checking time
268
W. Lee et al.
(which is the time spent traversing the rule tree up to and including the Opt-
TreeNode). In this case, we call the service time TR and keep a measurement for
each OptTreeNode (i.e., each Snort rule). Second, for packets that do not match
any of the RuleTreeNodes (i.e., they do not belong to any Snort rule), the service
time is the preprocessing time plus the time traversing the RuleTreeNodes. In
this case, we call the service time TP and keep a measurement for packets of
each protocol: http, telnet, ftp, ssh, finger, other-tcp, icmp, and udp. We
need to include TR and TP measurements when computing an optimal Snort
conﬁguration. Since preprocessing is the main factor in TP , we need to consider
the “value” of preprocessing in addition to the values of the rules. We assign
the highest value to preprocessing because it is always needed. If TP is too high
(e.g., when Snort is overloaded by packets that do not necessarily match rules),
the Knapsack algorithm can output a conﬁguration that does not include pre-
processing. Such a conﬁguration is not acceptable. In such a case, the following
iterative process is used: use Knapsack algorithm to ﬁrst determine what packet
ﬁlters should be used (what protocols are allowed) in order to keep TP low (e.g.,
half of the value in the previous iteration), using priorities among the proto-
cols; then use Knapsack to compute a Snort conﬁguration, considering both TP
and TR; if a conﬁguration including preprocessing is output, then terminate,
otherwise, continue to iterate.
In order to eﬃciently enable and disable Snort rules without having to tra-
verse the entire rule tree data structure, we implemented a direct access mech-
anism. It uses a two-dimensional linked list. The head nodes in one dimension
are the priorities of the rules (i.e., rank orders in terms of their values), and
the other dimension comprises of a list of pointers to all the rules having the
same priority. This data structure is populated when parsing the rules at Snort
start-up time.
Experiments. We conducted a set of experiments, similar to those described in
Section 2.4, to study the performance of our prototype Adaptive Bro and Adap-
tive Snort. We replayed the same traﬃc as explained earlier using tcpreplay.
Regarding the parameter measurements, we assigned damage costs (Cβ
ij) of in-
trusions in relative scales: 100 for root access, 50 for user access, 30 for DoS, and
2 for probing, according to analysis in [21,15]. Since we use automatic intrusion
responses (using the ﬁrewall), we assign all false alarm costs (Cα
ij) the same as
the DoS damage cost. Since we do not have statistics on attack distribution yet,
we assign the prior probabilities (pij) of all intrusions to be the same (eﬀectively,
1). As mentioned above, πi and mi are measured in run-time.
Figure 4 shows the behavior of the Adaptive Bro and Adaptive Snort when
overloaded with the same background and ﬂooding traﬃc used before. We de-
scribe some details as follows. For Adaptive Bro, the initial conﬁguration is to
detect all of its “known” attacks, which include more than 100 detection rules
on root access (e.g., imapd buﬀer-overﬂow), user access (e.g., PHF), DoS (e.g.,
smurf, syn-flood), and probes (e.g., portsweep). The initial libpcap ﬁlters
were set to be “(tcp[13]&0x7!=0) or (port ftp) or (port telnet) or (dst port
Performance Adaptation in Real-Time Intrusion Detection Systems
269
Traffic (Mbps)
Snort Drops (Pkts/sec)
Bro Drops (Pkts/sec)
50
100
150
200
250
300
350
400
450
Time (seconds)
90
80
70
60
50
40
30
20
10
0
0
s
p
b
M
n
i
l
e
m
u
o
V
c
i
f
f
a
r
T
Fig. 4. Behavior of adaptive IDSs under stress: when under stress, it changes to a new
conﬁguration to minimize delay and data loss.
80 or dst port 8080) or (port imap) or (udp port 53) or (icmp)” as before. For
Adaptive Snort, the initial conﬁguration consisted of a subset of the default rule
set ( 277 rules ) coming with the distribution and with the important prepro-
cessors (i.e. frag2, stream4, portscan, http-decode, unidecode, rpc-decode,
telnet-decode) activated. Initially when the traﬃc is low, the inter-event ar-
rival time is high and the systems can perform all the analysis tasks. When the
traﬃc rises high, the inter-event arrival time drops low and Bro discovers that
T (cid:2)(P) > D(cid:2)
max. It then invokes Knapsack to compute a new optimal conﬁgura-
tion for the current conditions. Also it can be noted that there were initial packet
drops due to the heavy load, but the quick reconﬁguration avoided further packet
drops. This happens at time t = 270. This is diﬀerent from the Original Bro, as
shown in Figure 3, where the situation of packet drops continues. Since the ﬂood