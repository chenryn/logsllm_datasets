communications between the two endpoints.
To demonstrate an integrity violation, the MITM sends
requests to a command server. It can tell the server to run
inputs on the atm and it can ask for the card ﬁle for any
account whose creation it initiated. Eventually the MITM
will declare the test complete. At this point, the same set of
atm commands is run using the oracle’s atm and bank without
the MITM. This means that any messages that the MITM
sends directly to the target submission’s atm or bank will not
be replayed/sent to the oracle. If the oracle and target both
complete the command list without error, but they diﬀer on
the outputs of one or more commands, or on the balances
of accounts at the bank whose card ﬁles were not revealed
to the MITM during the test, then there is evidence of an
integrity violation.
As an example (based on a real attack we observed), con-
sider a submission that uses deterministic encryption with-
out nonces in messages. The MITM could direct the com-
mand server to withdraw money from an account, and then
replay the message it observes. When run on the vulner-
able submission, this would debit the account twice. But
when run on the oracle without the MITM, no message is
replayed, leading to diﬀering ﬁnal account balances. A cor-
rect submission would reject the replayed message, which
would invalidate the break.
Privacy breaks. Privacy violations are also demonstrated
using a MITM. In this case, at the start of a test the com-
mand server will generate two random values, “amount” and
“account name.” If by the end of the test no errors have oc-
curred and the attacker can prove it knows the actual value
of either secret (by sending a command that speciﬁes it), the
break is considered successful. Before demonstrating knowl-
edge of the secret, the MITM can send commands to the
server with a symbolic “amount” and “account name”; the
server ﬁlls in the actual secrets before forwarding these mes-
sages. The command server does not automatically create a
secret account or an account with a secret balance; it is up to
the breaker to do that (referencing the secrets symbolically
when doing so).
As an example, suppose the target does not encrypt ex-
changed messages. Then a privacy attack might be for the
MITM to direct the command server to create an account
whose balance contains the secret amount. Then the MITM
can observe an unencrypted message sent from atm to bank;
this message will contain the actual amount, ﬁlled in by the
command server. The MITM can then send its guess to the
command server showing that it knows the amount.
As with the log problem, we cannot tell whether an in-
tegrity or privacy test is exploiting the same underlying
weakness in a submission, so we only accept one violation of
each category against each submission.
Timeouts and denial of service. One diﬃculty with our
use of a MITM is that we cannot reliably detect bugs in
submissions that would result in inﬁnite loops, missed mes-
sages, or corrupted messages. This is because such bugs can
be simulated by the MITM by dropping or corrupting mes-
sages it receives. Since the builders are free to implement
any protocol they like, our auto-testing infrastructure can-
not tell if a protocol error or timeout is due to a bug in the
target or due to misbehavior of the MITM. As such, we con-
servatively disallow reporting any such errors. Such ﬂaws in
builder implementations might exist but evidence of those
bugs might not be realizable in our testing system.
Contest
USA Brazil Russia
India Other
Spring 2015
Fall 2015
30
64
12
20
12
12
7
14
120
110
4. QUANTITATIVE ANALYSIS
This section analyzes data we have gathered from three
contests we ran during 2015.9 We consider participants’
performance in each phase of the contest, including which
factors contribute to high scores after the build-it round, re-
sistance to breaking by other teams, and strong performance
as breakers.
We ﬁnd that on average, teams that program in languages
other than C and C++, and those whose members know
more programming languages (perhaps a proxy for overall
programming skill), are less likely to have security bugs iden-
tiﬁed in their code. However, when memory management
bugs are not included, programming language is no longer
a signiﬁcant factor, suggesting that memory safety is the
main discriminator between C/C++ and other languages.
Success in breaking, and particularly in identifying security
bugs in other teams’ code, is correlated with having more
team members, as well as with participating successfully in
the build-it phase (and therefore having given thought to
how to secure an implementation). Somewhat surprisingly,
use of advanced techniques like fuzzing and static analysis
did not appear to aﬀect breaking success. Overall, integrity
bugs were far more common than privacy bugs or crashes.
The Fall 2015 contest, which used the ATM problem, was
associated with more security bugs than the Spring 2015
secure log contest.
4.1 Data collection
For each team, we collected a variety of observed and self-
reported data. When signing up for the contest, teams re-
ported standard demographics and features such as coding
experience and programming language familiarity. After the
contest, each team member optionally completed a survey
about their performance.
In addition, we extracted infor-
mation about lines of code written, number of commits,
etc. from teams’ Git repositories.
Participant data was anonymized and stored in a manner
approved by our institution’s human-subjects review board.
Participants consented to have data related to their activi-
ties collected, anonymized, stored, and analyzed. A few par-
ticipants did not consent to research involvement, so their
personal data was not used in the data analysis.
4.2 Analysis approach
To examine factors that correlated with success in build-
ing and breaking, we apply regression analysis. Each regres-
sion model attempts to explain some outcome variable using
one or more measured factors. For most outcomes, such as
participants’ scores, we can use ordinary linear regression,
which estimates how many points a given factor contributes
to (or takes away from) a team’s score. To analyze binary
outcomes, such as whether or not a security bug was found,
we apply logistic regression. This allows us to estimate how
each factor impacts the likelihood of an outcome.
We consider many variables that could potentially impact
teams’ results. To avoid over-ﬁtting, we initially select as
9We also ran a contest during Fall 2014 [31] but exclude it from
consideration due to diﬀerences in how it was administered.
Table 1: Contestants, by self-reported country.
potential factors those variables that we believe are of most
interest, within acceptable limits for power and eﬀect size.
(Our choices are detailed below.) In addition, we test mod-
els with all possible combinations of these initial factors and
select the model with the minimum Akaike Information Cri-
terion (AIC) [4]. Only the ﬁnal models are presented.
This was not a completely controlled experiment (e.g., we
do not use random assignment), so our models demonstrate
correlation rather than causation. Our observed eﬀects may
involve confounding variables, and many factors used as in-
dependent variables in our data are correlated with each
other. This analysis also assumes that the factors we exam-
ine have linear eﬀect on participants’ scores (or on likelihood
of binary outcomes); while this may not be the case in real-
ity, it is a common simpliﬁcation for considering the eﬀects
of many factors. We also note that some of the data we an-
alyze is self-reported, and thus may not be entirely precise
(e.g., some participants may have exaggerated about which
programming languages they know); however, minor devia-
tions, distributed across the population, act like noise and
have little impact on the regression outcomes.
4.3 Contestants
We consider three contests oﬀered at two times:
Spring 2015: We held one contest during May–June 2015
as the capstone to a Cybersecurity MOOC sequence.10 Be-
fore competing in the capstone, participants passed courses
on software security, cryptography, usable security, and hard-
ware security. The contest problem was the secure log prob-
lem (§3.1).
Fall 2015: During Oct.–Nov. 2015, we oﬀered two con-
tests simultaneously, one as a MOOC capstone, and the
other open to U.S.-based graduate and undergraduate stu-
dents. We merged the contests after the build-it phase, due
to low participation in the open contest; from here on we
refer to these two as a single contest. The contest problem
was the ATM problem (§3.2).
The U.S. had more contestants than any other country.
There was representation from developed countries with a
reputation both for high technology and hacking acumen.
Details of the most popular countries of origin can be found
in Table 1, and additional information about contestant de-
mographics is presented in Table 2.
4.4 Ship scores
We ﬁrst consider factors correlating with a team’s ship
score, which assesses their submission’s quality before it is
attacked by the other teams (§2.1). This data set contains all
101 teams from the Spring 2015 and Fall 2015 contests that
qualiﬁed after the build-it phase. Both contests have nearly
the same number of correctness and performance tests, but
diﬀerent numbers of participants. We set the constant mul-
10https://www.coursera.org/specializations/cyber-security
Contest
# Contestants
% Male
% Female
Age
% with CS degrees
Years programming
# Build-it teams
Build-it team size
# Break-it teams
(that also built)
Break-it team size
# PLs known per team
Spring ’15†
156
91%
5%
34.8/20/61
35%
9.6/0/30
61
2.2/1/5
65
(58)
2.4/1/5
6.8/1/22
Fall ’15†
122
89%
9%
33.5/19/69
38%
9.9/0/37
34
3.1/1/5
39
(32)
3.0/1/5
10.0/2/20
Fall ’15
23
100%
0%
25.1/17/31
23%
6.6/2/13
6
3.1/1/6
4
(3)
3.5/1/6
4.2/1/8
Table 2: Demographics of contestants from qualifying teams.
† indicates MOOC participants. Some participants declined
to specify gender. Slashed values represent mean/min/max.
tiplier M to be 50 for both contests, which eﬀectively nor-
malizes the scores.
Model setup. To ensure enough power to ﬁnd meaning-
ful relationships, we decided to aim for a prospective eﬀect
size roughly equivalent to Cohen’s medium eﬀect heuristic,
f 2 = 0.15 [7]. An eﬀect this size suggests the model can
explain up to 13% of the variance in the outcome variable.
With an assumed power of 0.75 and population N = 101,
we limited ourselves to nine degrees of freedom, which yields
a prospective f 2 = 0.154. (Observed eﬀect size for the ﬁnal
model is reported with the regression results below.) Within
this limit, we selected the following potential factors:
Contest: Whether the team’s submission was for the
Spring 2015 contest or the Fall 2015 contest.
# Team members: A team’s size.
Knowledge of C: The fraction of team members who
listed C or C++ as a programming language they know. We
included this variable as a proxy for comfort with low-level
implementation details, a skill often viewed as a prerequisite
for successful secure building or breaking.
# Languages known: How many unique programming
languages team members collectively claim to know (see the
last row of Table 2). For example, on a two-member team
where member A claims to know C++, Java, and Perl and
member B claims to know Java, Perl, Python, and Ruby,
the language count would be 5.
Coding experience: The average years of programming
experience reported by a team’s members.
Language category: We manually identiﬁed each team’s
submission as having one “primary” language. These lan-
guages were then assigned to one of three categories: C/C++,
statically-typed (e.g., Java, Go, but not C/C++) and dy-
namically-typed (e.g., Perl, Python). C/C++ is the baseline
category for the regression. Precise category allocations, and
total submissions for each language, segregated by contest,
are given in Figure 2.
Lines of code: The SLOC11 count of lines of code for
the team’s ﬁnal submission at qualiﬁcation time.
MOOC: Whether the team was participating in the MOOC
capstone project.
11http://www.dwheeler.com/sloccount
Figure 2: The number of build-it submissions in each con-
test, organized by primary programming language used.
The brackets group the languages into categories.
Results. Our regression results (Table 3) indicate that ship
score is strongly correlated with language choice. Teams
that programmed in C or C++ performed on average 121
and 92 points better than those who programmed in dy-
namically typed or statically typed languages, respectively.
Figure 3 illustrates that while teams in many language cate-
gories performed well in this phase, only teams that did not
use C or C++ scored poorly.
The high scores for C/C++ teams could be due to bet-
ter scores on performance tests and/or due to implementing
optional features. We conﬁrmed the main cause is the for-
mer. Every C/C++ team for Spring 2015 implemented all
optional features, while six teams in the other categories
implemented only 6 of 10, and one team implemented none;
the Fall 2015 contest oﬀered no optional features. We artiﬁ-
cially increased the scores of those seven teams as if they had
implemented all optional features and reran the regression
model. The resulting model had very similar coeﬃcients.
Our results also suggest that teams that were associated
with the MOOC capstone performed 119 points better than
non-MOOC teams. MOOC participants typically had more
programming experience and CS training.
Finally, we found that each additional line of code in a
team’s submission was associated with a drop of 0.03 points
in ship score. Based on our qualitative observations (see §5),
we hypothesize this may relate to more reuse of code from
libraries, which frequently are not counted in a team’s LOC
(most libraries were installed directly on the VM, not in the
submission itself). We also found that, as further noted be-
low, submissions that used libraries with more sophisticated,
lower-level interfaces tended to have more code and more
mistakes; their use required more code in the application,
lending themselves to missing steps or incorrect use, and
thus security and correctness bugs. As shown in Figure 3,
LOC is also (as expected) associated with the category of
language being used. While LOC varied widely within each
language type, dynamic submissions were generally short-
Figure 3: Each team’s ship score, compared to the lines
of code in its implementation and organized by language
category. Fewer LOC and using C/C++ correlate with a
higher ship score.
Factor
Coef.
SE p-value
Fall 2015
Lines of code
Dynamically typed
Statically typed
MOOC
-21.462
-0.031
-120.577
-91.782
119.359
28.359