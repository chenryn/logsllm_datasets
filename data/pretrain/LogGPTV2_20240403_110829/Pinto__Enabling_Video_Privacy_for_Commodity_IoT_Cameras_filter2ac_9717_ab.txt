5.1 fps
Processing time taken per frame
Detect time
431.5 ms
(89.7%)
146.2 ms
(75.5%)
Pixelate
0.05 ms
(0.01%)
0.05 ms
(0.02%)
I/O time
47.4 ms
(10.2%)
47.1 ms
(24.4%)
Table 1: Processing of realtime video stream on a low-cost
embedded device (1.2 GHz CPU).
Table 2: Time taken in each step when running real-time
blurring on a Raspberry Pi (1.2 GHz CPU).
fabricate video evidence. On the other hand, we assume to have a
trusted timestamping server [22] and thus, rolling back time is not
possible. More specifically, a hash of the video upon recording is
sent from an IoT camera (via WiFi or LTE) to the server that signs
the hash with the current time.
3.3 Desired Properties
Prior to sharing of videos, they
Visual privacy protection.
must be visually protected depending on situations, for example,
types of objects to be blurred upon the circumstances of requests,
request type, seriousness of incidents, etc. Such requests are not
known a priori. This calls for a solution framework that allows
post-processing for visual privacy protection.
Privacy-protected videos must be properly
Video authenticity.
authenticated. The conventional post-blurring of videos, which
invalidates their original, realtime signatures, is undesirable for
video authentication.
Privacy-protected videos must be of good
Fine video quality.
quality. First, they should have high frame rates. For example, videos
with low frame rates (below 12 fps) are perceived as jerky motion
[51]. Second, they should keep the blurring intensity and the size
of blurred areas as minimal as possible while protecting sensitive
objects. Overly-blurred videos are perceptually jarring, and signifi-
cantly degrade the human-perceived video quality [36, 39].
3.4 Limitations of Existing Approaches
Existing vision or image processing techniques fail to meet the
requirements above when running on low-end devices. We demon-
strate this by experimenting with their performance over realtime
video stream on a Raspberry Pi with 1.2 GHz CPU. Table 1 summa-
rizes the results.
Conventional blurring. As mentioned earlier, the conventional
post-blurring fails to provide video authenticity. On the other hand,
realtime blurring can produce a hash of a blurred video on the fly.
We have implemented the realtime blurring on a Raspberry Pi using
OpenCV [13] with dlib [10] and OpenALPR [1] libraries that come
with many pre-trained Haar classifiers for faces and license plates.
The resulting videos have an average frame rate of 2.3 fps. which is
even lower than the previous results reported in [46]. Note, their
system does not consider face privacy, which usually takes more
CPU time than plate blurring. Furthermore, the videos show some
frames that are not properly blurred. Note, failure in a single frame
entirely invalidates the purpose of blurring.
Fingerprinting [47] is an image process-
Video fingerprinting.
ing technique to recognize videos that have been modified slightly
(e.g., blurring, rotation, cropping, etc). It works by extracting char-
acteristic features of a video, called “fingerprint”, then matching it
against a “reference” database of copyrighted materials.
Such visual-similarity checking may be useful for video authenti-
cation because a post-blurred video could still produce a fingerprint
that is “probabilistically similar” to that of the original one. In this
case, fingerprints of original videos should be produced at the time
of recording for the realtime signatures on those fingerprints. How-
ever, this is a herculean task for simple, low-cost embedded devices.
Indeed, our experiment using a lightweight SIFT-based feature al-
gorithm [59] on a Raspberry Pi results in fingerprint generation
with a frame rate of 1.1 fps.
Digital watermarking. Another technique for detecting visual
similarity is digital watermark [63]. It works by embedding hidden
information, called “watermark” into a video, then using it later
to verify the integrity of the video. This is broadly used for digital
content control, e.g., to trace copyright infringements. Robust wa-
termarking [57, 65] has more advanced features to further detect
benign or malignant modifications in media files. This type of water-
mark may be also useful for video authentication if such watermark
could be generated and embedded over realtime video stream. Un-
fortunately, this is very challenging [29] especially for low-power
embedded devices. We run robust watermark embedding on re-
altime video stream using a fast DCT-based algorithm [23]. Our
experiment on a Raspberry Pi results in realtime watermarking
with a frame rate of 1.2 fps, hence poor quality videos.
The existing approaches all suffer from perfor-
Limitations.
mance difficulties or functional deficiencies when running on sim-
ple embedded devices. In summary, there are no existing adequate
solutions for low-end IoT cameras to achieve visual privacy protec-
tion, video authenticity, and fine video quality all together.
4 DESIGN OF PINTO
4.1 Key Features
Pinto exploits computational
Decoupled blurring procedure.
asymmetry of object blurring. This stems from our observation in
the experiment above. The conventional blurring procedure is as
follows: (i) take the realtime frame from camera module (I/O time);
(ii) detect faces/plates in the image (Detect time); (iii) blur those
areas (Pixelate time); and (iv) write the blurred frame to a video file
(I/O time). Table 2 shows the time taken in each step when running
the realtime blurring on Raspberry Pi. This result shows that the
face/plate detection phase is the main bottleneck for object blurring,
which takes orders of magnitude more CPU time (×104) than the
pixelation phase (only 0.05 ms per frame). Pinto decouples these two
Session 6A: IoTCCS’18, October 15-19, 2018, Toronto, ON, Canada1091Figure 3: H_Pixelation example.
tasks: the CPU-intensive object detection and the computationally-
lightweight pixelating operation. Pinto performs fast pixelation of
entire frames in real time while deferring the CPU-intensive object
detection until necessary for post processing.
To realize fine-grained visual privacy
Block-level operation.
on each frame, we take a grid-based approach. We divide each frame
into equal-sized subimage blocks, so that pixelation is indepen-
dently applied within each individual block. This sets up operational
boundaries for pixelation in each frame. Pinto performs block-level
pixelation on every block (in real time), blocks of sensitive objects
(post-processing for video sharing), and blocks of non-sensitive
objects (for verification). Pinto provides streamlined procedures
for (owner-side) realtime and post processing and (requester-side)
verification.
Pinto leverages pixelation for both visual pri-
Hash-Pixelation.
vacy protection and forgery prevention. We devise hash-pixelation
(or h_pixelation) for these purposes. Given an original subimage
block, the h_pixelation procedure (Fig. 3) is as follows: (1) hash the
subimage block; (2) pixelate it; (3) embed the hash into the pixelated
subimage block. To avoid visual jarring, we distribute a 256-bit hash
into the “lower” 16 bits of the first 16 pixels in the pixelated subim-
age block. We use the least-significant 16 bits (R:6, G:5, B:5) in each
of those 24-bit pixels (R:8, G:8, B:8). Modulating these “lower” bits
incurs no human-perceptible difference in pixelation.
The h_pixelation has the following properties: (i) given an
original subimage block, any entity can immediately generate its
h_pixelated version; (ii) it is however infeasible to invert, thus visu-
ally de-identifying the original, internal contents. (iii) any change
in the original subimage block (posterior fabrication) results in a
different h_pixelation than the original one (especially the embed-
ded hash part); (iv) it is also infeasible to find a different image that
produces the same h_pixelation result.
4.2 Framework
Figure 4 shows the overall framework of Pinto that consists of three
logical parts.
Realtime processing: Pinto-enabled camera performs block-level
h_pixelation of every block in realtime frames while recording
(Fig. 5a). The resulting fully h_pixelated video (1-min default) is
Figure 4: Pinto framework.
(a) Realtime h_pixelate: every block
(b) Post h_pixelate: critical blocks
Figure 5: Block-level operation examples.
hashed upon creation, called p_digest. This p_digest is immediately
sent (via WiFi or LTE) to a trusted timestamping server that signs
it along with the current time. The signed p_digest is later used
for certifying the time and integrity of the video. Thereafter, the
original video (not the h_pixelated version) and its timestamped
p_digest are stored in the device.
Post processing: When sharing of a certain video is needed, the
device applies any existing or customized object detection algorithm
to the corresponding stored, original video upon the circumstances
of requests. This does not require fast computation speed, and the
choice of a particular vision algorithm is independent of Pinto. In
each frame of the video, the blocks that overlap with the detected
regions of sensitive objects (e.g., faces, license plates) are called
critical blocks. During this process, indices of critical blocks in each
frame are logged in a compact form called p_profile. Block-level
h_pixelation of such critical blocks produces a privacy-protected
video, which we refer to as p_video (Fig. 5b).
Verification: Upon the release of p_video, the requester verifies
its authenticity. This is done by using its p_digest and p_profile that
are also made available along with the p_video. If no forgery has
occurred, block-level h_pixelation of non-critical blocks (by con-
sulting its p_profile) will successfully restore the fully h_pixelated
version that is authenticated by its signed p_digest.
Original subimage block 2. Pixelate H_Pixelation 1st pixel R G B 101011 01101 11101…111110 10011 00110 16th pixel … Hash (256 bits): … R G B 1. Hash 3. Embed the hash into the lower bits of the pixels A hash is embedded into the first 16 pixels  10 101011 011 01101 101 11101 10 11110 011 10011 101 00110 Video frame (Example: 9 blocks per frame) Real-Time Processing Post-Processing (if releasing a video) Stored     original video Pinto-Enabled Camera  Original video    stored Signed p_digest stored Realtime video frames Realtime video frames Real-time Video Frames Per-frame Encoding Per-frame Decoding Frame H_Pixelation p_digest Generation Critical-Block H_Pixelation  Generation of p_video & p_profile Object Detection  Trusted timestamping Server Signature Generation  p_digest  Certified p_digest  IoT communication channel p_video; p_profile; p_digest  Video Requesters Authenticity Verification Session 6A: IoTCCS’18, October 15-19, 2018, Toronto, ON, Canada1092(a) Realtime processing in camera device (at the time of recording).
(b) Post processing (prior to sharing of a video).
(c) Verification of video authenticity (at the requester-side).
Figure 6: Overall procedures of Pinto.
4.3 Procedural Description
4.3.1 Operating in Real Time
Fig. 6a illustrates Pinto’s realtime operation at the time of recording.
The main objective here is to continuously record a realtime video
stream at a fast rate and to produce its p_digest on the fly. Each
realtime frame taken from camera module is fed into two parallel
paths: one is to write it into a video file (Path 1), and the other is to
process it for the p_digest generation (Path 2). The total per-frame
processing time—hence the resulting frame rate—is determined by
the time taken in Path 2 that has more components than Path 1.
To minimize the per-frame processing time,
Frame operation.
we keep the components of Path 2 lightweight. In Path 2, the re-
altime frame is divided into the predefined number of equal-sized
blocks (We recommend the use of 196–256 blocks per frame in
the light of processing speed and video quality, discussed later in
Section 6). Then block-level h_pixelation is applied to every block.
The resulting h_pixelated frame is hashed (per-frame hash) then
discarded. The next frame is read from the camera module, and
processed on.
p_digest generation. Upon recording of the current 1-min video
u, its device A generates p_digestu (256 bits) by collectively hashing
all the per-frame hashes, and sends it to a trusted, online timestamp-
ing server. Then, A deletes those per-frame hashes, and proceeds
with frame operations for the next recording video. In the meantime,
the server S returns the time-stamped p_digestu:
cur , {H (p_diдestu|T u
S −→ A : T u
cur )}K−
.
S
S
cur and K−
are the current time and S’ private key, respec-
where T u
tively. This signed p_digestu is stored with video u.
4.3.2 Post Processing for Visual Privacy
Generation of p_video and p_profile. When a certain video u
needs to be shared, critical blocks in each frame of u are h_pixelated
to produce p_videou as illustrated in Fig. 6b. We develop a library
function that returns critical blocks when running any chosen
JPEG-encoded frame H_Pixelate Hash (per-frame)  Hashes of  h_pixelated frames in current 1-min video  Realtime  view Camera  module Original video file (1-min MJPEG) Camerap_digest (to be signed with the CURRENT time) Hash Store Block-wise h_pixelation  of every block Division of each frame into blocks (Example: 16 blocks per frame) Capture/Encode (per-frame)  Decode (per-frame) Decode Detect  (face/plate) Original  video file (1-min MJPEG)  H_Pixelate Block-wise h_pixelation  of critical blocks p_profile  (indices of  critical blocks) p_video   (critical-block   h_pixelated video) Result: each frame with all blocks h_pixelated  Hash      (per-frame) Hash (with  the TIME of video) A privacy-protected frame with critical blocks h_pixelated H_Pixelation of non-critical blocks by consulting p_profile  Video authenticity test digest p_profile  (indices of  critical blocks) p_video   (critical-block     h_pixelated video) p_digest (signed) Decode Hashes of h_pixelated frames in this video H_Pixelate Session 6A: IoTCCS’18, October 15-19, 2018, Toronto, ON, Canada1093Figure 7: MJPEG-compliant compression (post processing prior to video sharing).
object detection algorithm on video u. In this process, their indices
are logged in a compact data structure, p_profileu. It is a bit array
where each bit is associated with the block index indicating whether
it is a critical block or not. Given a 50-Mbyte, 1-min HD video with
256 per-frame blocks, the size of p_profile is at most 46 Kbytes,
which is less than 0.1% overhead. Once generated, p_videou and
p_profileu are released along with the signed p_digestu.
4.3.3 Verifying Video Authenticity
For each frame of p_videou, the requester performs block-level
h_pixelation of non-critical blocks (by using its p_profileu) and pro-
duces a per-frame hash of the resulting all-block-h_pixelated frame
as illustrated in Fig. 6c. These per-frame hashes are collectively
hashed, and the resulting one is further hashed along with the time
cur . The authenticity—the time and integrity of the
of recording, T u
video—is verified if it matches with the time-stamped p_digestu