# Title: Improving Reliability with Dynamic Syndrome Allocation in Intelligent Software-Defined Data Centers

## Authors:
- Ulya Bayram
- Dwight Divine
- Pin Zhou
- Eric W.D. Rozier

## Conference:
2015 45th Annual IEEE/IFIP International Conference on Dependable Systems and Networks

## Abstract:
We propose new algorithms for implementing a software-defined data center (SDDC) to enhance the dependability of storage systems without requiring additional hardware. Our system can predict future resource requirements and allocate overprovisioned resources to improve reliability. We introduce algorithms for a smart SDDC (S2DDC) that characterizes user I/O transactions (writes and deletes) and uses these models to predict overprovisioning, thereby improving reliability while minimizing the impact on quality of service (QoS). We experimentally compare several implementations of our methods, discuss ways to enhance the fault tolerance of our S2DDC, and present results showing a significant reduction in expected annual block loss due to disk failures and latent sector errors. We also highlight the benefits of using dependency-based usage models for estimating overprovisioning.

## 1. Introduction
The rapid growth of Big Data has posed significant challenges for the storage industry, with data production outpacing the ability of designers and researchers to build appropriate platforms [1], [2]. For example, the NASA Center for Climate Simulation reported a 300-fold increase in computing needs and a 2,000-fold increase in storage over the past decade, making storage infrastructure one of the largest challenges facing climate scientists [3]. Additionally, the need for long-term and reliable data storage has increased due to new laws such as Sarbanes-Oxley [4] and HIPAA [5], as well as regulations from organizations like NIH, NSF, and the U.K. Joint Information Systems Committee [6]. The associated costs, including power, curation, and personnel, have also risen [7], [6].

To address these challenges, the concept of Software-Defined Data Centers (SDDCs) [8], [9] has been proposed. SDDCs extend the capabilities of data centers by incorporating virtualization concepts such as abstraction, automation, and pooling, similar to those used in Software-Defined Networks (SDNs) [10], [11].

In this paper, we build upon the idea of intelligent, user-aware SDDCs first proposed in [9]. We introduce smart SDDCs (S2DDCs) that dynamically improve the reliability of existing systems without additional hardware. This is particularly important for cost-constrained domains such as primary scientific institutions and state or federally sponsored research facilities, where funding constraints necessitate inexpensive solutions that do not require specialized hardware.

Our novel system framework monitors both system resources and user I/O patterns to identify patterns and exploit overprovisioned storage. We propose a middleware layer to allocate overprovisioned space to improve reliability while maintaining QoS, extending the work proposed in [9].

The primary contributions of our paper are:
- Expansion of the prediction algorithm in [9] to analyze the effect of the number of clusters on QoS and reliability.
- Experimental study of overprovisioning patterns in real data, demonstrating the tunability of our method with different levels of risk aversion.
- Introduction of algorithms for utilizing overprovisioned space to allocate dynamic independent syndromes, improving system reliability and reducing expected annual block loss.

## 2. Related Work
While many studies focus on understanding the reliability of storage systems [12], [13], [14], system workloads [15], [16], [17], application I/O patterns [18], and performance modeling [19], there is a lack of research on modeling usage patterns within a system [9]. Previous work primarily focuses on statistical properties of production backup systems [15] and generalized workload patterns [16]. The authors of [18] provide a formal specification for characterizing applications, workloads, and hardware, and propose a storage configuration compiler to automate the exploration of the large storage configuration space.

A significant deficiency in the literature is the lack of studies on individual user behavior and methods to characterize these users. Most studies focus on system and application workloads, missing the finer-grained analysis of user-level workloads. The only study that considers user-level system workloads [9] highlights the importance of user-level modeling. In this paper, we adopt the SDDC model from [9] and propose our S2DDC system, which utilizes extra resources through automated syndrome allocation and includes failure modeling to improve system reliability.

## 3. Methods
### 3.1. Data Normalization
We preprocess the data by transforming it according to a pseudo-log2 scale [9] to make writes and deletes comparable and merge them into a single space. This normalization step ensures that write operations of the same order (GB, MB, etc.) are closer than those of a smaller order.

\[
f(o) = 
\begin{cases} 
\log_2(o) & \text{if } o \text{ is a write operation} \\
-\log_2(o) & \text{if } o \text{ is a delete operation} \\
0 & \text{if } o \text{ is a nop operation}
\end{cases}
\]

### 3.2. Behavior Characterization
We compare two clustering algorithms, k-means and mean-shift, to characterize user behavior patterns.

#### 3.2.1. K-Means
K-means is a well-studied and popular clustering method with three primary parameters: the number of clusters \(k\), initial centroids, and a distance metric. The selection of these parameters significantly impacts the performance and goodness-of-fit of the final clusters. We select and tune \(k\) based on experimental results, choose initial centroids randomly for writes, and uniformly for deletes, with one cluster allocated for no-operation cases.

#### 3.2.2. Mean-Shift
Mean-shift is a mode-seeking, deterministic clustering algorithm. We simplify the mean-shift algorithm by omitting the kernel function and gradient density estimation steps, allowing our clustering method to converge into a type of sequential clustering. We perform clustering by centering a window with a pseudo-log-scale radius at each data point and checking for data within the window range. Nearby clusters are merged to reduce complexity.

### 3.3. Predicting Future Behaviors
Once the cluster centers are identified, we create and train Markov models based on the dependent pattern of behaviors. We estimate transitions in the model by noting the temporal locality of user behaviors. Given a user's I/O transaction, we train the transition matrix and generate a state occupancy probability vector for the current time. This vector assigns a probability of 1.0 to the state corresponding to the last observed I/O transaction and 0.0 to the rest.

## 4. Experimental Evaluation
We evaluate our proposed framework using real system data, demonstrating the effectiveness of our S2DDC in improving system reliability and reducing expected annual block loss. We discuss the consequences of these results and outline plans for future work, including ways to extend and generalize our framework.

## 5. Conclusion
In this paper, we propose new algorithms for implementing a smart SDDC (S2DDC) that improves the reliability of storage systems without additional hardware. Our system predicts future resource requirements and allocates overprovisioned resources to enhance reliability while maintaining QoS. We experimentally validate our approach and demonstrate its effectiveness in reducing expected annual block loss. Future work will focus on further refining and generalizing our framework.

---

This revised version of the text is more structured, coherent, and professional, with clear section headings and improved readability.