the model. In non-security related applications, a larger L•
robustness distance means that a model is more robust. For
example, if the MNIST classiﬁer requires an average of 0.3
L•-norm distance changes in adversarial examples, it is more
robust than a model with 0.06 L•-norm robustness distance,
because the adversarial examples look more differently from
the original image.
Accuracy cutoff. In order to reproduce existing results in
related work [11], we use 0.5 prediction conﬁdence as the
cutoff to compute the accuracy scores for all trained models.
4.2.1 Benchmark Datasets
We evaluate the robustness improvements in 4 benchmark
datasets: breast cancer, cod-rna, ijcnn1, and binary MNIST (2
vs. 6). Table 2 shows the size of the training and testing data,
the percentage of majority class in the training and testing
set, respectively, and the number of features for these datasets.
We describe the details of each benchmark dataset below.
breast cancer. The breast cancer dataset [1] contains 2 classes
of samples, each representing benign and malignant cells.
The attributes represent different measurements of the cell’s
physical properties (e.g., the uniformity of cell size/shape).
cod-rna. The cod-rna dataset [2] contains 2 classes of samples
representing sequenced genomes, categorized by the existence
of non-coding RNAs. The attributes contain information on
the genomes, including total free-energy change, sequence
length, and nucleotide frequencies.
ijcnn1. The ijcnn1 dataset [3] is from the IJCNN 2001 Neural
Network Competition. Each sample represents the state of a
physical system at a speciﬁc point in a time series, and has
a label indicating “normal ﬁring" or “misﬁring". We use the
22-attribute version of ijcnn1, which won the competition.
The dataset has highly unbalanced class labels. The majority
class in both train and test sets are 90% negatives.
MNIST 2 vs. 6. The binary mnist dataset [5] contains hand-
100
75
50
25
0
100
75
50
25
0
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
cod−rna
0.0
0.1
0.2
MNIST 2 vs. 6
L∞
0.3
0.0
0.1
0.3
0.4
0.2
L∞
0.05
0.10
L∞
0.15
natural
Chen's
ours
Figure 6: Accuracy under attack.
written digits of “2" and “6". The attributes represent the gray
levels on each pixel location.
Hyperparameters validation set. In the hyperparameter tun-
ing experiments, we randomly separate the training set from
the original data sources into a 90% train set and a 10% valida-
tion set. We use the validation set to evaluate the performance
of the hyperparamters, and then train the model again using
selected hyperparameters using the entire training set.
Robustness evaluation set. In order to reproduce existing
results, we follow the same experiment settings used in [11].
We randomly shufﬂe the test set, and generate advesarial ex-
amples for 100 test data points for breast cancer, ijcnn1, and
binary MNIST, and 5,000 test points for cod-rna.
4.2.2 GBDT Results
We ﬁrst evaluate the robustness of our training algorithm
on the gradient boosted decision trees (GBDT) using the four
benchmark datasets. We measure the model robustness using
the L• evasion distance of the adversarial examples found by
Kantchelian et al.’s MILP attack [29], the strongest whitebox
attack that minimizes Lp-norm evasion distance for tree en-
semble models. We compare the robustness achieved by our
algorithm against regular training as well as the state-of-the-
art robust training algorithm proposed by Chen et al. [11].
Hyperparameters. To reproduce the results from existing
work [11] and conduct a fair comparison, we report results
from the same number of trees, maximum depth, and e for
L•-norm bound for regular training and Chen’s method in
Table 3. For our own training method, we reused the same
number of trees and maximum depth as Chen’s method. Then,
we conducted grid search of different e values by 0.01 step
size to ﬁnd the model with the best accuracy. For our cod-rna
model, we also experimented with e values by 0.001 step size.
To further evaluate their choices of hyperparameters, we have
conducted grid search for the number of trees and maximum
depth to measure the difference in model accuracy, by training
120 models in total. Our results show that the hyperparameters
2300    30th USENIX Security Symposium
USENIX Association
Dataset
# of
trees
4
80
60
Trained e
Tree Depth
Test ACC (%)
Test FPR (%)
Avg. l•
Improv.
breast-cancer
cod-rna
ijcnn1
Chen’s ours natural Chen’s ours natural Chen’s ours natural Chen’s ours natural Chen’s ours natural Chen’s
0.98 0.98 .2194 .3287 .4405 2.01x 1.34x
0.30
0.30
4.44 7.38 .0343 .0560 .0664 1.94x 1.19x
0.20 0.035
2.15 1.62 .0269 .0327 .0463 1.72x 1.42x
0.02
0.20
0.68 1.65 .0609 .3132 .3317 5.45x 1.06x
MNIST 2 vs. 6 1,000 0.30
0.30
Table 3: Test accuracy and robustness of GBDT models trained by our algorithm (ours), compared to regularly trained models
(natural) and the models trained by Chen and Zhang et al.’s method [11] (Chen’s), in XGBoost. The improvement (Improv.)
here denotes the average l• robustness distance on our models over regularly trained ones and Chen and Zhang’s, by measuring
adversarial examples found by Kantchelian’s MILP attack [29], the strongest whitebox attack.
97.81 96.35 99.27 0.98
96.48 88.08 89.64 2.57
97.91 96.03 93.65 1.64
99.30 99.30 98.59 0.58
6
4
8
4
8
5
8
6
8
5
8
6
and Chen’s method reach 0% accuracy at l• distance 0.5,
whereas the largest evasion distance for the regularly trained
model is 0.61. For the breast-cancer dataset, robustly trained
models have an larger evasion distance than the regularly
trained model for over 94% data points. Compared to Chen’s
method, our models maintain higher accuracy under attack in
all cases, except a small distance range for the ijcnn1 dataset
(l• from 0.07 to 0.10, Figure 6).
Model quality evaluation. Figure 7 shows the ROC curves
and the Area Under the Curve (AUC) for GBDT models
trained with natural, Chen’s, and our training methods. For all
the four testing datasets, the AUC of the model trained by our
method is on par with the other two algorithms. On average,
AUC of the model trained by our method is only 0.03 and
0.01 lower, while our method can increase the MILP attack
cost by 2.78⇥ and 1.25⇥ than natural and Chen’s training
methods, respectively. Table 3 shows that overall we maintain
relatively high accuracy and low false positive rate. However,
we have a high false positive rate for the cod-rna model and
low accuray for the ijcnn1 model, as the tradeoff to obtain
stronger robustness.
4.2.3 Random Forest Results
We evaluate the robustness of random forest models trained
with our algorithm on the four benchmark datasets. We com-
pare against Chen’s algorithm [11] and regular training in
scikit-learn [4]. Since Chen’s algorithm is not available in
scikit-learn, we have implemented their algorithm to train ran-
dom forest models ourselves. We compare the effectiveness
of our robust training algorithm against Chen’s method and
regular training, when using Gini impurity to train random
forest scikit-learn.
Hyperparameters. We conduct a grid search for the number
of trees and maximum depth hyperparameters. Speciﬁcally,
we use the following number of trees: 20, 40, 60, 80, 100,
and the maximum depth: 4, 6, 8, 10, 12, 14. For each dataset,
we train 30 models, and select the hyperparameters with the
highest validation accuracy. The resulting hyperparameters
are shown in Table 4. For the breast-cancer and binary mnist
datasets, we re-used the same e = 0.3 from Chen’s GBDT
models. We tried different L•  e values of robust training for
cod-rna and ijcnn1 datasets, and found out that e = 0.03 gives
a reasonable tradeoff between robustness and accuracy. For
Figure 7: ROC curves of GBDT models trained with natural,
Chen’s, and our training methods. AUC is given in the legend.
used in existing work [11] produced similar accuracy as the
best one 2. Note that the size of the breast-cancer dataset is
very small (only 546 training data points), so using only four
trees does not overﬁt the dataset.
Minimal evasion distance. As shown in Table 3, our train-
ing algorithm can obtain stronger robustness than regular
training and the state-of-the-art robust training method. On
average, the MILP attack needs 2.78⇥ larger L• perturba-
tion distance to evade our models than regularly trained ones.
Compared to the state-of-the-art Chen and Zhang et al.’s ro-
bust training method [11], our models require on average
1.25⇥ larger L• perturbation distances. Note that the robust-
ness improvement of our trained models are limited on binary
MNIST dataset. This is because the trained and tested robust-
ness ranges L•  0.3 are fairly large for MNIST dataset. The
adversarial examples beyond that range are not imperceptible
any more, and thus the robustness becomes extremely hard to
achieve without heavily sacriﬁcing regular accuracy.
Accuracy under attack. Using the minimal l• evasion dis-
tances of adversarial examples, we plot how the accuracy
of the models decrease as the attack distance increases in
Figure 6. Compared to regular training, our models maintain
higher accuracy under attack for all datasets except the breast-
cancer one. Both breast-cancer models trained by our method
2https://tinyurl.com/2b5egv49
USENIX Association
30th USENIX Security Symposium    2301
Dataset
ours
80 / 8
Test ACC (%)
Test FPR (%)
breast-cancer
Tree Num / Depth
Trained e
natural Chen’s ours natural Chen’s ours natural Chen’s ours natural Chen’s
Chen’s ours natural Chen’s
0.98 1.96 .2379 .3490 .3872 1.63x 1.11x
0.30 0.30 20 / 4
99.27 99.27 98.54 0.98
20 / 4
3.65 5.69 .0325 .0512 .0675 2.08x 1.32x
0.03 0.03 40 / 14 20 / 14 40 / 14 96.54 92.63 89.44 2.97
0.78 0.08 .0282 .0536 .1110 3.94x 2.07x
0.03 0.03 100 / 14 100 / 12 60 / 8
97.92 93.86 92.26 1.50
0.68 0.48 .0413 .1897 .2661 6.44x 1.40x
MNIST 2 vs. 6 0.30 0.30 20 / 14 100 / 12 100 / 14 99.35 99.25 99.35 0.68
Table 4: Test accuracy and robustness of random forest models trained by our algorithm (ours) compared to regularly trained
models (natural), in scikit-learn. The improvement (Improv.) here denotes the average l• robustness distance increase.
cod-rna
ijcnn1
Avg. l•
Improv.
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
100
75
50
25
0
100
75
50
25
0
breast−cancer
0.0
0.2
0.6
0.4
L∞
ijcnn1
0.0
0.1
0.2
L∞
0.3
100
75
50
25
0
100
75
50
25
0
)
%
(
y
c
a
r
u
c
c
A
)
%
(
y
c
a
r
u
c
c
A
cod−rna
0.0
0.1
0.2
MNIST 2 vs. 6
L∞
0.0
0.1
0.2
L∞
0.3
natural
Chen's
ours
Figure 8: Accuracy under attack.
Figure 9: ROC curves of random forest models trained with
natural, Chen’s, and our training methods in scikit-learn. AUC
is given in the legend.
example, when e = 0.2, we trained 30 random forest models
using Chen’s method for the cod-rna dataset, and the best
validation accuracy is only 79.5%. Whereas, using e = 0.03
increases the validation accuracy to 91%.
Minimal Evasion Distance. As shown in Table 4, the robust-
ness of our random forest models outperforms the ones from
regular training and Chen’s algorithm. Speciﬁcally, the av-
erage l• distance of adversarial examples against our robust