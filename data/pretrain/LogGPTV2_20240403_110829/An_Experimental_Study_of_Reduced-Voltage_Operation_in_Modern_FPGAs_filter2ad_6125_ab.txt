### 1. Introduction to Power-Efficiency Techniques

We integrate typical quantization [34] and pruning [33] techniques with our proposed hardware-level undervolting technique to further enhance the power efficiency of FPGA-based CNN accelerators.

### 2. Undervolting: Supply Voltage Underscaling Below the Nominal Voltage Level

The total power consumption of any hardware substrate is directly related to its supply voltage, with dynamic power varying quadratically and static power varying linearly. Thus, underscaling the supply voltage toward the threshold voltage significantly reduces power consumption. As manufacturing technology node sizes decrease, voltage underscaling has become a common power-saving approach. For example, the nominal voltage (Vnom) for Xilinx FPGAs is 1V, 0.9V, and 0.85V for 28nm, 20nm, and 16nm technology nodes, respectively. Our undervolting technique aims to reduce the supply voltage below the default Vnom. However, reducing the supply voltage below the guardband level can substantially increase circuit latency and introduce timing faults. These faults can manifest as bit-flips in memories or logic timing violations in data paths, potentially leading to reduced accuracy in CNNs or, in the worst case, system crashes.

#### 2.1. Approaches to Mitigate Undervolting Faults

Several approaches can be used to address undervolting faults:

- **Frequency Reduction**: Simultaneously decreasing the frequency [111], which incurs performance degradation.
- **Fault Mitigation Techniques**: Using Error Correction Codes (ECCs) for memory [9, 99] and Razor shadow latches for data paths [27], though these come at the cost of additional hardware.
- **Architectural Improvements**: Implementing additional iterations in CNN training [133], which may require hardware and/or software adaptations.

### 3. Undervolting Studies

There are two primary approaches to studying undervolting:

- **Simulation-Based Studies**: These studies, such as [89, 108, 127, 132], require less engineering effort but face challenges in validating results on real hardware.
- **Real Hardware Evaluation**: This approach, often performed on CPUs, GPUs, ASICs, and DRAMs [9, 18, 50, 78, 81, 138], requires substantial engineering effort and is device- and vendor-dependent. It provides exact experimental results and allows for the study of device-specific parameters like voltage guardbands and real power and reliability behavior.

In this paper, we adopt the real hardware evaluation approach by testing undervolting on modern off-the-shelf FPGA devices for state-of-the-art CNN workloads and benchmarks.

### 4. Experimental Methodology

Figure 1 illustrates the overall methodological flow of our experiments. In this section, we detail our implementation methodology, benchmarks, and undervolting methodology for the FPGA platform.

#### 4.1. CNN Model Development Platform

For our implementation, we use the Deep Neural Network Development Kit (DNNDK) [122], a CNN framework from Xilinx. DNNDK is an integrated framework that facilitates CNN development and deployment on Deep Learning Processing Units (DPUs). We chose DNNDK for its freely available nature, ensuring reproducibility and general applicability for state-of-the-art CNN implementations. Although we do not expect significant differences compared to other DNN platforms, we plan to verify this by repeating experiments on other platforms, such as DNNWeaver [101].

DNNDK provides a comprehensive set of toolchains for compression, compilation, deployment, and profiling, enabling the mapping of CNN classification phases onto FPGAs with hard CPU cores via a C/C++ programming interface. The DEep ComprEsN Tool (DECENT) within DNNDK handles quantization and pruning tasks. Quantization converts floating-point CNN models to INT8 precision [34], while pruning minimizes model size by removing unnecessary connections [33]. Our baseline evaluation uses an INT8 precision model without pruning. In Section 6, we evaluate different configurations for a more comprehensive analysis.

Among the soft DPUs provided by DNNDK, B4096 is the largest, utilizing 24.3% BRAMs and 25.6% DSPs, achieving a peak performance of 4096 operations/cycle at a default DPU frequency of 333MHz and DSP frequency of 666MHz. Up to three B4096 DPUs can be used in our hardware platform. Our experiments are based on the B4096 configuration to achieve peak performance.

#### 4.2. CNN Benchmarks

We evaluate undervolting in FPGA-based CNN accelerators using five commonly-used image classification benchmarks: VGGNet [106], GoogleNet [110], AlexNet [51], ResNet [35], and Inception [110]. To conduct a comprehensive analysis, we choose models with varying parameter sizes, from a few MBs (e.g., GoogleNet) to hundreds of MBs (e.g., AlexNet). The benchmarks have different numbers and types of layers, as shown in Table 1. The default activation function used in all benchmarks is ReLU.

| **CNN Model** | **Dataset** | **Inputs** | **Outputs** | **#Layers** | **Parameters Size** | **Inference Accuracy (%)** |
|---------------|-------------|------------|-------------|-------------|--------------------|---------------------------|
| VGGNet        | Cifar-10    | 32*32      | 10          | 6           | 8.7MB              | 86% (Our design @Vnom)     |
| GoogleNet     | Cifar-10    | 32*32      | 10          | 21          | 6.6MB              | 91% (Our design @Vnom)     |
| AlexNet       | Kaggle Dogs vs. Cats | 227*227 | 2            | 8           | 233.2MB            | 92.5% (Our design @Vnom)   |
| ResNet50      | ILSVRC2012  | 224*224    | 1000        | 50          | 102.5MB            | 68.8% (Our design @Vnom)   |
| Inception     | ILSVRC2012  | 224*224    | 1000        | 22          | 107.3MB            | 65.1% (Our design @Vnom)   |

#### 4.3. Undervolting

##### 4.3.1. Prototype FPGA Platform

Our prototype is based on the Xilinx ZCU102 FPGA platform, fabricated at a 16nm technology node. We chose this platform because it supports voltage underscaling and is compatible with DNNDK. Experiments were repeated on three identical samples of ZCU102 to study hardware platform variability. The ZCU102 is equipped with the Zynq UltraScale+ XCZU9EG-2FFVB1156E MPSoC, which combines a Processing System (PS) and user-Programmable Logic (PL) in a single device.

### 5. Conclusion

By integrating quantization, pruning, and undervolting, we aim to significantly improve the power efficiency of FPGA-based CNN accelerators. Our real hardware evaluation approach provides detailed insights into the practical implications of undervolting, including the trade-offs between power savings and potential faults. Future work will include verifying these results on other DNN platforms to ensure broad applicability.