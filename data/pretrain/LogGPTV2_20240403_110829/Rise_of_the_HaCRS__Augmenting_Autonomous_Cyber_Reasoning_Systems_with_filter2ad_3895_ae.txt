hiding the game board. A null pointer dereference bug can be trig-
gered and the program is crashed by playing at least one round and
then type “START OVER” to start a new game at the menu. Other
options like “PLAY AGAIN” do not trigger any crash immediately.
During the game play, this binary provides a clear indication
of what the format of the input should be. We observed that our
assistants strictly followed the expected input format and won the
first round quite fast, and then typed “PLAY AGAIN” to start a new
game. HaCRS then mutated up one of the round-winning input,
replaced “PLAY AGAIN” with “START OVER”, and triggered the
crash immediately. In comparison, neither AFL nor Driller could
win a single round of the game: They could not generate input that
satisfies the format requirement for sufficient number of times, and
consequently they never crashed this binary.
Vulnerability-detection case study: KPRCA_00028. This binary
implements a command line interpreter for expressions from an
imaginary programming language called SLUR. Built-in functions
include quote, cons, equal, lambda, etc., can be used inside SLUR
expressions. A typical SLUR expression looks like the following:
(quote e), or ((lambda (v1 v2) e) e1 e2). A null pointer dereference
vulnerability exists in the lambda function.
Although this program does not provide any hint of what build-
in functions there are and what form user input should have, we
noticed that our assistants were able capture function names from
the symbolic tokens in the guidance that HaCRS provided. Addition-
ally, our assistants were able to correctly determine the legitimate
format of SLUR expressions, and manually constructed expressions
using different built-in functions that are accepted by the binary.
With the help of input from our assistants, HaCRS was able to build
several long SLUR expressions including one or more lambda sub-
expressions that triggered the crash. We scrutinized the input that
AFL (without human assistance) used, and realized that it was un-
able to generate an expression in the correct form, which explains
why it could not crash this binary.
7 DISCUSSION
In this section, we discuss implications of the Rise of the HaCRS.
Specifically, we talk about the importance of our step of integrating
human effort into Cyber Reasoning Systems (and specifically, non-
expert human effort), take-aways from our evaluation, and future
steps.
7.1 Human Obsolescence
As with most examples of human-dependent technique, we expect
that the “intuition” that human assistants provide for HaCRS will
eventually be replaced by automated techniques. However, it is
not currently clear what shape such an analysis would take. While
carrying out the tasklets that HaCRS requested help with, humans
do not necessarily reason about code coverage (even though it
is used as a goal metric), but rather about the exploration of an
abstract state space of program, in a way that current automated
techniques do not consider.
When automated techniques are developed that can reproduce
this slice of human intuition, we expect that humans will be made
redundant, similar to the relentless advance of automation in as-
sembly lines. For now, however, it seems that we are still quite
relevant, even in the simplified Cyber Grand Challenge dataset.
7.2 Assistant Skill Levels
Interestingly, judging from feedback emails sent to us when our
human assistants experienced technical issues, more technically-
minded assistants tended to get frustrated and quit faster. This may
have been due to us attempting to simplify our assistant instruc-
tions. Combined with the relatively unimpressive performance of
the semi-experts, this implies that more research is needed into
presenting a correct abstraction for different skill levels, and that
the non-expert interface does not necessarily scale up to expert
users.
Of course, software testing specialists are quite well trained for
this sort of thing – they excel at identifying corner cases in software
despite being “semi-experts” in program analysis. Our future direc-
tion is the principled reintroduction of expertise into the process,
with the appropriate interface support, to better understand how
much of the system is impacted just by expertise-independent hu-
man “intuition” and how much is impacted by human experience,
but perhaps hampered by the current “simplified” interface.
Given a pool of abundant non-experts, adequate amounts of
such semi-experts, and a constrained number experts, HaCRS could
strategically distribute different tasks, with interfaces of varying
difficulty level, to its host of assistants. There are many open ques-
tions as to the best way to facilitate this interaction – should experts
inject inputs like non-experts do, or should they function at a lower
level of program paths and symbolic constraints? Once such vari-
able levels of assistance are supported, the HaCRS will have to be
taught to reason about a budget, in terms of the available human
talent, available time, and available money to pay its assistants.
This requires the potential integration of complex game theory
and approach planning algorithms, which are currently relatively
unexplored in the realm of Cyber Reasoning Systems.
7.3 Incentive Structures
Because Amazon Mechanical Turk is designed for quick tasks with
instant payoff, we settled on the incentive model of paying assis-
tants for triggering a pre-set amount of transitions in the program.
However, this ignores, to some extent, the humans’ effect on down-
stream automated analysis. Basically, not all transitions are created
equal, and some lead to more interesting mutations than others.
Thus, it would be interesting to explore an incentive structure in
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA359which assistants are rewarded based on how much code coverage
is achieved by any test cases derived from their tasklet solutions,
not just the coverage of the solutions themselves. This could allow
assistants to more carefully budget their time across different pro-
grams, as well: an assistant could put a small amount of time into
program Pa, move on to Pb, and check at a later time if his inputs
to Pa resulted in increased code coverage from the automation, and
provide more assistance as needed.
These sorts of improved incentive structures, that allow the
human to use the automation as an assistant at the same time as
the automation uses the human as an assistant, may bring the two
sides closer toward creating a hybrid “centaur” system.
7.4 Other Tasklets
Thus far, we have integrated human assistance into the test case
creation pipeline of the Mechanical Phish. However, the HaCRS
concept can be applied to other aspects of a CRS:
Test case selection. The stalling-out of the fuzzer, which HaCRS
addresses by providing human-assisted test cases, represents
only one side of the limitations of fuzzing-based vulnerability
discovery techniques. On the other end of the spectrum is the
“input explosion” that can occur when the fuzzer identifies too
many test cases, overwhelming the evolutionary algorithm.
Of course, automated techniques, such as AFLFast [3], have
been developed in an attempt to help with the selection of
test cases in this situation. However, the fact that human
assistants augment a CRS even in the presence of techniques
such as Driller suggests that exploring the use of human
assistance for test case selection, in addition to generation,
could be a promising direction of research.
Exploitation. Even though the Mechanical Phish exploited more
challenges during the Cyber Grand Challenge than any of its
opponents, it still had almost an 80% failure rate in convert-
ing a crash to an exploit. In many cases, this was because
the specific way in which it triggered a crash did not pro-
vide it with enough control over the program’s memory.
Crashing test cases that the CRS fails to exploit could be
dispatched to expert human assistants for “post-processing”,
and these assistants could modify the test-cases to achieve
more control of the state, allowing the CRS to weaponize
otherwise-unexploitable crashes.
Patching. One of the limitations of the Mechanical Phish is its in-
ability to create precise patches for software, due to a lack of
root cause analysis of vulnerabilities. This limitation forces
the Mechanical Phish to exclusively adopt costly general
patches that patch large swaths of code that are not vul-
nerable. Integrating human effort into the patch evaluation
process, specifically by having experts (or maybe, with a
carefully-designed interface, semi-experts) participate in the
root-cause analysis of identified crashes and the evaluation
of potential fixes, could significantly improve the effective-
ness of this component of the Mechanical Phish.
High-level planning. Likewise, human assistance can be lever-
aged in the planning process – for example, during the Cyber
Grand Challenge, it was not always a good idea to patch a
vulnerability (in fact, the Mechanical Phish lost its chance
at victory because it patched too many vulnerabilities [27]).
An ability to integrate human advice into the system would
go a long way to alleviating current limitations in the ability
of the Mechanical Phish to properly respond to changing
strategic situations.
We plan to explore some of these applications in our future work
in this field.
8 CONCLUSION
The use of principled human-assistance in Cyber Reasoning Sys-
tems constitutes a paradigm shift in our view of how binary anal-
ysis is done. Instead of the dichotomy between human-led, semi-
automated systems (HCH, as discussed in Section 2) and fully auto-
mated systems (CCC), we propose a C(H|C)C system, where com-
puters, which scale beyond human ability, make organizational calls
and humans, whose intuition has not yet been replicated, assist
when able. This system can utilize the insight of non-expert humans,
who are more abundant than expert humans and thus scale better.
In the absence of these humans, these systems are able to operate
fully autonomously, just at a lower effectiveness.
In this paper, we have taken a first look at how non-experts
impact the automated vulnerability discovery pipeline. The results
are significant: humans, with no security training, were able to
seriously improve the bug detection rate of a state-of-the-art vul-
nerability analysis engine. Further exploration is warranted. For
example, humans can confirm or repudiate results of static analysis,
combine behavior observed in different test cases into one, and help
verify automatically-generated patches. All of this is challenging or
simply infeasible with modern techniques, but the use of human as-
sistance can greatly augment Cyber Reasoning Systems with these
capabilities regardless.
ACKNOWLEDGMENTS
This work would not have existed without the amazing attitude
of the Shellphish CTF and CGC team, and owe our team-mates
enormous debts of gratitude for their support and brilliance over
the years. We would also like to thank our many, many workers on
Amazon Mechanical Turk, along with our amazing semi-expert vol-
unteers: Xingcheng Chen, Tao Du, Tao Zhan, and two anonymous
graduate students.
This material is based on research sponsored by the National
Science Foundation under award numbers CNS-1704253 and DGE-
1623246, by DARPA under agreement number FA8750-15-2-0084,
and by the Office of Naval Research under grant number N00014-
15-1-2948. The U.S. Government is authorized to reproduce and
distribute reprints for Governmental purposes notwithstanding any
copyright notation thereon. The views and conclusions contained
herein are those of the authors and should not be interpreted as
necessarily representing the official policies or endorsements, either
expressed or implied, of DARPA or the U.S. Government.
REFERENCES
[1] Thanassis Avgerinos, Sang Kil Cha, Alexandre Rebert, Edward J Schwartz, Maver-
ick Woo, and David Brumley. 2014. Automatic Exploit Generation. In Proceedings
of the ACM Conference on Computer and Communications Security (CCS).
[2] Jeff Barr and Luis Felipe Cabrera. 2006. AI gets a brain. Queue 4, 4 (2006).
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA360[3] Marcel Böhme, Van-Thuan Pham, and Abhik Roychoudhury. 2016. Coverage-
based greybox fuzzing as markov chain. In Proceedings of the 2016 ACM SIGSAC
Conference on Computer and Communications Security. ACM, 1032–1043.
[4] Juan Caballero, Pongsin Poosankam, Christian Kreibich, and Dawn Song. 2009.
Dispatcher: Enabling Active Botnet Infiltration Using Automatic Protocol Reverse-
engineering. In Proceedings of the ACM Conference on Computer and Communica-
tions Security (CCS).
[5] Juan Caballero, Heng Yin, Zhenkai Liang, and Dawn Song. 2007. Polyglot: Auto-
matic Extraction of Protocol Message Format Using Dynamic Binary Analysis.
In Proceedings of the ACM Conference on Computer and Communications Security
(CCS).
[6] Rondo E Cameron. 1993. A Concise Economic History of the World: From Paleolithic
Times to the Present. Oxford University Press, USA.
[7] Sang Kil Cha, Thanassis Avgerinos, Alexandre Rebert, and David Brumley. 2012.
Unleashing Mayhem on Binary Code. In Proceedings of the IEEE Symposium on
Security and Privacy.
[8] DARPA. 2016. CFE File Archive. (2016). http://repo.cybergrandchallenge.com/
cfe/.
[9] Werner Dietl, Stephanie Dietzel, Michael D. Ernst, Nathaniel Mote, Brian Walker,
Seth Cooper, Timothy Pavlik, and Zoran Popović. 2012. Verification Games:
Making Verification Fun. In Proceedings of the 14th Workshop on Formal Techniques
for Java-like Programs. 42–49. https://doi.org/10.1145/2318202.2318210
[10] Joshua Drake. 2015.
Stagefright
- Blackhat 2015 Slides.
(2015).
https://www.blackhat.com/docs/us-15/materials/us-15-Drake-Stagefright-
Scary-Code-In-The-Heart-Of-Android.pdf.
[11] Christopher B Eiben, Justin B Siegel, Jacob B Bale, Seth Cooper, Firas Khatib,
Betty W Shen, Barry L Stoddard, Zoran Popovic, and David Baker. 2012. Increased
Diels-Alderase Activity through Backbone Remodeling Guided by Foldit Players.
Nature biotechnology 30, 2 (2012).
[12] Patrice Godefroid, Michael Y Levin, and David A Molnar. 2008. Automated White-
box Fuzz Testing. In Proceedings of the Symposium on Network and Distributed
System Security (NDSS).
[13] Sean Heelan. 2009. Automatic Generation of Control Flow Hijacking Exploits for
Software Vulnerabilities. Ph.D. Dissertation. University of Oxford.
[14] Shih-Kun Huang, Min-Hsiang Huang, Po-Yen Huang, Chung-Wei Lai, Han-Lin Lu,
and Wai-Meng Leong. 2012. Crax: Software Crash Analysis for Automatic Exploit
Generation by Modeling Attacks as Symbolic Continuations. In Proceedings of
the IEEE International Conference on Software Security and Reliability (SERE).
[15] IARPA. 2010. STONESOUP Program. (2010). https://www.iarpa.gov/index.php/
[16] Peach Inc. 2013. Peach Fuzzer: Discover unknown vulnerabilities. (2013). http:
research-programs/stonesoup.
//peachfuzzer.com.
[17] Sam Kean. 2010. The Disappearing Spoon: And Other True Tales of Madness, Love,
and the History of the World from the Periodic Table of the Elements. Little, Brown
and Company.
[18] Alexander Kosoruko. 2000. Social Classification Structures: Optimal Decision
Making in an Organization. Late breaking papers of GECCO (2000), 175–178.
[19] Alex Kosorukoff. 2001. Human based genetic algorithm. In Systems, Man, and
Cybernetics, 2001 IEEE International Conference on, Vol. 5.
[20] Wenchao Li, Sanjit a Seshia, and Somesh Jha. 2012. CrowdMine: Towards Crowd-
sourced Human-Assisted Verification. In Proceedings of the 49th Annual Design Au-
tomation Conference. ACM, 1254–1255. https://doi.org/10.1145/2228360.2228590
[21] Zhiqiang Lin, Xuxian Jiang, Dongyan Xu, and Xiangyu Zhang. 2008. Automatic
Protocol Format Reverse Engineering through Context-Aware Monitored Execu-
tion. In Proceedings of the Symposium on Network and Distributed System Security
(NDSS).
[22] Heather Logas, Jim Whitehead, Michael Mateas, Richard Vallejos, Lauren Scott,
Dan Shapiro, John Murray, Kate Compton, Joseph Osborn, Orlando Salvatore,
Zhongpeng Lin, Michael Shavlovsky, Daniel Cetina, Shayne Clementi, and Chris
Lewis. 2014. Software Verification Games: Designing Xylem, The Code of Plants.
In Proceedings of the 9th International Conference on the Foundations of Digital
Games (FDG). Society for the Advancement of the Science of Digital Games,
Liberty of the Seas, Caribbean.
[23] Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An In-
put Generation System for Android Apps. In Proceedings of the ACM SIGSOFT
Symposium on Foundations of Software Engineering (FSE).
[24] Jeff H Perkins, Sunghun Kim, Sam Larsen, Saman Amarasinghe, Jonathan
Bachrach, Michael Carbin, Carlos Pacheco, Frank Sherwood, Stelios Sidiroglou,
Greg Sullivan, et al. 2009. Automatically Patching Errors in Deployed Software.
In Proceedings of the ACM SIGOPS Symposium on Operating systems principles.
[25] Dafna Shahaf and Eyal Amir. 2007. Towards a Theory of AI Completeness. In
AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning.
[26] Shellphish. 2016. Shellphish - The Cyber Grand Challenge.
(2016).
http:
//shellphish.net/cgc.
2017.
[27] Shellphish.
Cyber
Grand
Shellphish.
(2017).
http://phrack.org/papers/cyber_grand_shellphish.html.
[28] Yan Shoshitaishvili, Ruoyu Wang, Christopher Salls, Nick Stephens, Mario Polino,
Andrew Dutcher, John Grosen, Siji Feng, Christophe Hauser, Christopher Kruegel,
et al. 2016. SOK:(State of) The Art of War: Offensive Techniques in Binary
Analysis. In Proceedings of the IEEE Symposium on Security and Privacy.
[29] Nick Stephens, John Grosen, Christopher Salls, Andrew Dutcher, Ruoyu Wang,
Jacopo Corbetta, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2016. Driller: Augmenting Fuzzing Through Selective Symbolic Execution. In
Proceedings of the Symposium on Network and Distributed System Security (NDSS).
[30] The Verge. 2016. Google rebuilt a core part of Android to kill the Stagefright
vulnerability for good. (2016). http://www.theverge.com/2016/9/6/12816386/
android-nougat-stagefright-security-update-mediaserver.
[31] Luis Von Ahn and Laura Dabbish. 2004. Labeling Images with a Computer Game.
In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems.
ACM, 319–326.
[32] Luis Von Ahn, Ruoran Liu, and Manuel Blum. 2006. Peekaboom: A Game for
Locating Objects in Images. In Proceedings of the SIGCHI Conference on Human
Factors in Computing Systems. ACM, 55–64.
[33] Luis Von Ahn, Benjamin Maurer, Colin McMillen, David Abraham, and Manuel
Blum. 2008. reCAPTCHA: Human-Based Character Recognition via Web Security
Measures. Science 321, 5895 (2008), 1465–1468.
[34] Mike Walker. 2016.
The DARPA Cyber Grand Challenge.
https://
www.cybergrandchallenge.com/. (2016).
[35] Ruoyu Wang, Yan Shoshitaishvili, Christopher Kruegel, and Giovanni Vigna.
2017. Ramblr: Making Reassembly Great Again. In Proceedings of the Symposium
on Network and Distributed System Security (NDSS).
[36] Shuai Wang, Pei Wang, and Dinghao Wu. 2015. Reassembleable Disassembling.
In Proceedings of the USENIX Security Symposium.
[37] Kyle W Willett, Chris J Lintott, Steven P Bamford, Karen L Masters, Brooke D
Simmons, Kevin RV Casteels, Edward M Edmondson, Lucy F Fortson, Sugata
Kaviraj, William C Keel, et al. 2013. Galaxy Zoo 2: detailed morphological
classifications for 304,122 galaxies from the Sloan Digital Sky Survey. Monthly
Notices of the Royal Astronomical Society (2013).
[38] Michal Zalewski. 2014. American Fuzzy Lop. (2014). http://lcamtuf .coredump.cx/
afl/.
Session B3:  Investigating AttacksCCS’17, October 30-November 3, 2017, Dallas, TX, USA361Binary
CADET_00001
CADET_00003
CROMU_00001
CROMU_00003
CROMU_00005
CROMU_00017
CROMU_00019
CROMU_00029
CROMU_00031
CROMU_00037
CROMU_00040
CROMU_00041
CROMU_00044