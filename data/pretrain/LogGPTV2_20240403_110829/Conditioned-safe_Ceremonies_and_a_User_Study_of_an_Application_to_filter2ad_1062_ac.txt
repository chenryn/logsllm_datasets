• Limiting the effect of demand characteristics, where
users try to guess the study’s purpose and change their
behavior, perhaps unintentionally.
• Minimizing the impact of perceived authority ﬁgures
during the study [25, 37].
• Determining an appropriate physical location for the
experiment which minimizes any unrealistic inﬂuences
on users.
Our study addressed these issues in two ways: 1) we did
not use a laboratory, and 2) we employed deception to hide
the study’s true purpose. We recruited users remotely, and
during the consent process, we told users that our experi-
ment aimed to determine how well individuals can predict
high grossing movies. We told each user she will log in to
our Web site over a seven day period and make a prediction
of what she thinks will be the top three highest grossing
movies each day. Each user logged in from her “natural
habitat”: from her own computer, from anywhere, and at
any time she wished. We show a screenshot of our interface
in Figure 1.
Each user received $20 as base compensation, and we
rewarded her up to an additional $3 per prediction depend-
ing on the accuracy of her predictions. We told each user
that she must make seven predictions to complete the ex-
periment, so the total maximum a user could receive is $41.
We simulated the experience of risk by giving users
password-protected accounts at our Web site and creating
an illusion that money they “win” during the study was
“banked” in these accounts. We paid users at the end of
the study via PayPal and solicited each user’s PayPal email
Figure 1. User interface for making predictions at our study Web site.
address at the beginning of the study.4 To help suggest that
there was a risk that the user’s compensation could be stolen
if her account was hijacked, we provided an “account man-
agement” page which allowed the user to change the PayPal
email address associated with her account.
Although we told users they must make seven predic-
tions to complete the study, after each user made her ﬁfth
prediction, we simulated a MITM social engineering at-
tack against her the next time she logged in. After she en-
tered her username and password, we redirected her to an
“attack” server. We discuss the simulated attacks in Sec-
tion 6.4. After the simulated attack, we debriefed each user
about the true purpose of the study and requested her recon-
sent for the use of her data.
6.2 Recruitment
We recruited users through the Experimental Social Sci-
ence Laboratory (Xlab) at UC Berkeley. The Xlab is an in-
terdisciplinary facility that supports UC Berkeley investiga-
tors in running behavioral and social science experiments.
Members of the UC Berkeley community (i.e., students,
staff, etc.) register with the Xlab over the Web and receive
solicitations to participate in experiments via email. One
limitation of this recruitment method is that our user pool
was primarily composed of university students and staff and
may not be representative of the general population. Our ex-
periment used only native English speakers, and the subject
pool included approximately 1950 eligible users.
4Although we did not verify each user’s PayPal account was valid at
the start of the study, each user explicitly acknowledged she either had an
account or was willing to get one.
We contacted 225 randomly selected users in April 2008
through the Xlab.
Interested users signed up through the
Xlab’s system and received instructions to visit our study
Web site. We did meet any of the users in person. 208 users
signed up for our study, and we assigned them round-robin
to 5 study groups. One group used challenge questions for
registration and the other four groups used different variants
of email registration links. We discuss the email registration
groups further in Section 6.4.2. We excluded the results of
8 users and give details in Section 7.1. We show a summary
of the user groups and their sizes in Table 2.
6.3 Registration procedures
Each user created an account at our site, with a username
and password. We also asked for the user’s email address
and PayPal email address, if different. After a user entered
her username and password on her ﬁrst login, we redirected
her to a page that informed her that she must register her
computer before she could use it to access her account. If
the user chose to register her computer, we redirected her to
the registration page. If she was in the challenge question
group, we prompted her to set up her challenge questions.
She selected two questions and provided answers. After
conﬁrming the answers, she entered her account and pro-
ceeded with her ﬁrst prediction.
If she was part of an email registration group, then she
saw a page informing her that she had been sent a regis-
tration email and must click on the link labeled “Click on
this secure link to register your computer”. After clicking
on the link, she entered her account and made a predic-
tion. We sent registration emails in HTML format, but also
Group
Size Registration method
Attack description
Warnings in email?
1
2
3
4
5
41
40
39
40
40
Challenge questions
Solicit answers
Email
Email
Email
Email
Forward email to attacker
Forward email to attacker
Copy and paste link into text box
Copy and paste link into text box
N.A.
X
X
Table 2. Summary of study groups.
included a plain text alternative (using the multipart/
alternative content type) for users who had HTML
viewing disabled in their email clients. We embedded the
same registration link in both parts, but included a distin-
guishing parameter in the link to record whether the user
was presented with the HTML or plain text version of the
email. We discuss how we used this information in Sec-
tion 6.4.2. Screenshots of registration emails are shown in
Figures 3(a) and 3(b).
Both registration procedures set an HTTP cookie and a
Flash Local Shared Object on the user’s computer to indi-
cate the computer is registered. For subsequent login at-
tempts, we ﬁrst prompted the user for her username and
password. If the username and password were valid, our
server checked if the user’s computer was registered for that
username. If she was logging in from a registered computer,
then we redirected her to her account. If she was logging in
from a computer we didn’t recognize, then we prompted her
to answer her challenge questions (Figure 2(a)) or sent her
a new registration link to click on, depending on the user’s
group.
6.4 Simulated attacks
6.4.1 Challenge questions: Group 1
For the challenge question group, the attack attempted to
convince users to answer their challenge questions by pre-
senting the page shown in Figure 2(b). This is essentially
the same page users saw when they answered their chal-
lenge questions under “normal” conditions, but with the
warning and informative text removed.5 This attack: 1)
is straightforward, 2) closely mimics the legitimate regis-
tration process, and 3) was previously disclosed in the se-
curity community as a major weakness of challenge ques-
tions [46, 61].
6.4.2 Email: Groups 2-5
For the email groups, we simulated the two attacks we iden-
tiﬁed in Section 5: the copy and paste attack and the for-
5Even if users select their challenge questions from a pool of possible
questions, an attacker can easily determine a particular user’s questions by
relaying communications between the legitimate site and the user [46, 61].
warding attack. The copy and paste attack asked the user to
copy the registration link into a text box, and the forwarding
attack asked the user to forward the registration email to an
address with a similar domain name as our study site. We
simulated the forwarding attack against groups 2 and 3, and
simulated the cut and paste attack against groups 4 and 5.
We chose these attacks because we believed they are the
most compelling and straightforward attacks that we could
ethically implement. Another potentially effective attack
would be to try to hijack each user’s email account, but we
did not believe this attack was ethical. We leave other at-
tacks as a subject for future work.
For both attacks, the attack page ﬁrst told the user that
“because of problems with our email registration system”
she should not click on the link in the email she received.
For the copy and paste attack, the attack page presented a
text box with a “submit” button and instructed the user to
copy and paste the registration link into the box. For the
forwarding attack, it instructed the user to forward the email
to the attacker’s email address. We show screenshots of the
attack pages in Figures 4(a) and 4(b).
These attacks also presented pictorial versions of the in-
structions, with a screenshot of how the registration link
appears in the email. To maximize the effectiveness of
this picture, we gave the attacker the advantage of knowing
the distribution of HTML and plain text registration emails
previously viewed by the user during the study (see Sec-
tion 6.3). The attack displayed the pictorial instructions cor-
responding to the majority; in case of a tie we displayed a
screenshot of the HTML version.
6.4.3 Warnings
Some Web sites warn users about safe security practices,
e.g., how to resist phishing attacks against challenge ques-
tions. Although these warnings are sometimes useful, they
will likely be absent during an attack, when a user needs
them the most. Email registration has the advantage of
being able to include advisory information and contextual
warnings in each registration email. To measure the ef-
fectiveness of these kinds of warnings, we subdivided the
email groups into two groups: those who received warnings
in registration emails (groups 2 and 4) and those who did
not (groups 3 and 5). Everyone saw these warnings on le-
(a) User interface for answering challenge questions.
(b) Screenshot of the attack against challenge questions.
Figure 2. Normal challenge questions interface vs. simulated attack instructions.
(a) HTML registration email with warnings.
(b) HTML registration email without warnings.
Figure 3. Registration emails.
gitimate registration pages. A screenshot of these warnings
is shown in Figure 3(a).6 Group 1 users also received warn-
ings about safe practices when answering their challenge
questions, but we only showed group 1 users these warn-
ings during legitimate registrations. Group 1 users never
received warnings in email.
6.4.4 Attack success metrics
If a group 1 user answered her challenge questions correctly
on the attack page, we considered the attack a success and
ended the experiment. We assumed an attacker could dis-
6These warnings speciﬁcally warned against the attacks we simulated.
Although in the real world it may not be feasible to concisely warn users
against all the possible attacks, a site can certainly warn users against the
most successful or common attacks they have observed in the past.
tinguish between correct and incorrect answers (e.g., by re-
laying the user’s responses in real time to the legitimate
site), so if a user entered an incorrect answer, the attacker
prompted her again.
If a group 2-5 user clicked on the registration link ﬁrst,
then we considered the attack a failure.7
If the user for-
warded the email or submitted the link ﬁrst, then we con-
sidered the attack a success. Either way, we ended the ex-
periment for the user.
7These attacks actually simulated network level MITM attacks. Such
attackers might be able to intercept registration links and steal any regis-
tration tokens stored on the user’s computer. There are various proposals
that can help protect registration links and cookies against stronger adver-
saries [10, 28, 34], but we do not discuss the details here. Regardless, the
results of this study are applicable to a wide variety of social engineering
attacks, including phishing.
(a) Screenshot of the forwarding attack against email registration.
(b) Screenshot of the cut and paste attack against email registration.
Figure 4. Our simulated attacks against email registration.
For all users, attempts to navigate to other parts of the
site redirected the user back to the attack page. If the user
resisted the attack for 30 minutes, then on her next login,
the experiment ended and we considered the attack a fail-
ure. The attack pages for groups 1, 4, and 5 contained a
Javascript key logger, in case a user began to answer her
challenge questions or entered the link, but then changed
her mind and did not submit.
If our key logger detected
this, we considered the attack a success.
6.5 Debrieﬁng and exit survey
After a user completed the study, we redirected her to a
page that debriefed her about the true purpose of the exper-
iment and explained the reasons for deception. The debrief-
ing page explained the concept of machine authentication
and the different ways of registering computers. We then
obtained reconsent from each user. If a user reconsented,
we redirected her to an exit survey.
Our exit survey started with general demographic ques-
tions such as gender, age range, and occupational area. The
second section of the survey collected information on the
user’s general computing background, attitudes, and habits.
The ﬁnal section asked more speciﬁc questions about the
user’s experiences during the study. We discuss these ques-
tions in Section 7.
6.6 Ethics
Our simulated attacks were ethical. The risk to users
during the attacks was minimal. We only used data from
users who explicitly reconsented after a debrieﬁng on the
true nature of the study. The study protocol described here
was approved by the UC Berkeley’s Institutional Review
Board on human experimentation.
To protect users’ privacy, all connections to our web site
used SSL. We purchased a certiﬁcate for our domain which
is accepted by major Web browsers. In a real world attack,
an attacker would most likely not be able to obtain a valid
certiﬁcate for the target site. To avoid certiﬁcate warnings,
an attacker would probably use HTTP rather than HTTPS to
host the attack page. However, to protect users’ privacy, our
simulated attack page used SSL. Since our hypothesis is that
email registration is more secure than challenge questions,
we had to ensure that our imperfect simulation did not bias
the results against challenge questions. Our solution was
to maximize the beneﬁts of SSL for the challenge question
users and minimize the beneﬁts of SSL for the email regis-
tration users. In particular, we conservatively assumed that
our simulated adversary attacking email registration had ob-
tained a valid certiﬁcate for the target domain while our sim-
ulated adversary attacking challenge question based regis-
tration had not obtained a valid certiﬁcate. Group 2-5 users
did not see certiﬁcate warnings during the attack, but group
1 users did. We implemented this by redirecting group 1
users to a different Apache instance (at port 8090) with a
self-signed certiﬁcate, while group 2-5 users continued to
use the original Apache instance in “attack mode”. This im-
plies the “attack” domain shown in the URL bar for group
1 users included a port number, but the “attack” domain for
group 2-5 users did not.
7 Study results
7.1 User demographics
One email registration user did not complete the study,
and one email registration user did not reconsent. Due to a
misconﬁguration, our server did not send registration emails
to 6 users during the simulated attack. We excluded these
users’ data from our results, leaving 200 users total.
56% of users self-reported themselves as female, 41%
reported themselves as male, and 3% did not respond. Our
users were mostly young: 91% reported themselves as 18-
25 years old and 89% reported themselves as undergraduate
students. Among students, the mix of major areas was di-
verse. The largest group was physical sciences (i.e., chem-
istry, physics, biology, etc.), accounting for 25% of users,
and the second largest was economics and business, ac-
counting for 20% of users. Computer science and engineer-
ing accounted for 1.5% and 7.5% of users, respectively.
Most users reported using Windows (69%) and Mac OS
(25%) as their primary operating systems, and most users
reported using Firefox (70%), Internet Explorer (11%), and
Safari (10%) as their primary Web browsers. This differs
signiﬁcantly from recent statistics, which report Windows
as having 91% market share and Internet Explorer as having
72% market share [8].
78% of users reported using a Web browser at least 10
hours a week, and 70% of users reported they have con-
ducted ﬁnancial transactions online for at least a year. Types
of online transactions reported include PayPal (55%), bank-
ing (80%), investing (12%), auctions (42%), and shopping
(80%).
7.2 Success of simulated attacks
We summarize our results by group number in Table 1.
Our attack succeeded against 92.7% of challenge question
users and 41.5% of email users. This difference is statisti-
cally signiﬁcant (p < 0.001, Fisher’s exact test). The cut
and paste attack was slightly more effective than the for-
warding attack (47% vs. 40% with warnings, and 47% vs.