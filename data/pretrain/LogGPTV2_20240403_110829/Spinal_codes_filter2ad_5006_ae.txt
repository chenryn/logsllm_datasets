HotNets-X, Oct. 2011.
[28] G. Pottie and D. Taylor. A comparison of reduced complexity decoding
algorithms for trellis codes. JSAC, 7(9):1369–1380, 1989.
[29] C. Schurgers and M. B. Srivastava. A Systematic Approach to Peak-to-
Average Power Ratio in OFDM. In SPIE’s 47th Meeting, 2001.
[30] S. Sen, N. Santhapuri, R. Choudhury, and S. Nelakuditi. AccuRate:
Constellation-based rate estimation in wireless networks. NSDI, 2010.
[31] S. Sesia, G. Caire, and G. Vivier. Incremental redundancy hybrid ARQ
schemes based on low-density parity-check codes. IEEE Trans. on
Comm., 52(8):1311–1321, 2004.
[32] C. Shannon. Communication in the presence of noise. Proc. of the IRE,
37(1):10–21, 1949.
sity, 2004.
[33] A. Shokrollahi. Raptor codes. IEEE Trans. Info. Theory, 52(6), 2006.
[34] N. Shulman. Universal channel coding. PhD thesis, Tel-Aviv Univer-
[35] E. Soljanin, N. Varnica, and P. Whiting. Incremental redundancy hybrid
ARQ with LDPC and Raptor codes. IEEE Trans. Info. Theory, 2005.
[36] E. Telatar. Capacity of Multi-Antenna Gaussian Channels. European
Trans. on Telecom., 10(6):585–595, 1999.
[37] G. Ungerboeck. Channel coding with multilevel/phase signals. IEEE
Trans. Info. Theory, IT-28(1):55–67, Jan. 1982.
[38] G. Ungerboeck and I. Csajka. On improving data-link performance by
increasing the channel alphabet and introducing sequence coding. In
ISIT, 1976.
[39] A. Vila Casado, M. Griot, and R. Wesel. Informed dynamic scheduling
for Belief-Propagation decoding of LDPC codes. In IEEE ICC, 2007.
[40] A. Viterbi. Error bounds for convolutional codes and an asymptotically
optimum decoding algorithm. IEEE Trans. Info. Theory, 13(2):260–
269, 1967.
[41] M. Vutukuru, H. Balakrishnan, and K. Jamieson. Cross-Layer Wireless
Bit Rate Adaptation. In SIGCOMM, 2009.
[42] S. Wicker and V. Bhargava. Reed-Solomon codes and their applications.
Wiley-IEEE Press, 1999.
[43] S. Wong, H. Yang, S. Lu, and V. Bharghavan. Robust Rate Adaptation
for 802.11 Wireless Networks. In MobiCom, 2006.
Cawgn(P∗) = 1(cid:14)
APPENDIX: PROOF SKETCH OF THEOREM 1
We establish the theorem for the real-valued AWGN channel; the
complex channel simply doubles the capacity. We only cover the
uniform constellation mapping here. This argument applies to a
bubble decoder with depth d = 1 and width B polynomial in n (ex-
ponent to be determined). The proof for d0 6= 1, B0 ≥ max(1,2−kdB)
follows similarly and is omitted for space.
to the range [−p3P/2,p3P/2], giving an average symbol power
Inputs to the constellation mapping function are uniformly dis-
tributed, thanks to the mixing properties of the hash function and
RNG. Under the uniform mapping, these inputs are mapped directly
(variance) slightly less than P∗ , P/2; the difference vanishes as
c → ∞. The Shannon capacity [32] of the AWGN channel with
average power constraint P∗ is
2 log2 (1 + SNR) bits/symbol,
(4)
where SNR = P∗
σ 2 denotes the signal-to-noise ratio.
The rate of the spinal code after L passes is k/L. Reliable de-
coding is not possible until L passes have been received such that
k/L is less than Cawgn(P∗). We shall show that for essentially the
smallest L that satisﬁes this inequality, our polynomial-time decoder
will produce the correct message with high probability. In the re-
mainder of this section, we assume L is the smallest value such that
k/L  0, with
probability 1− O(exp(−Θ(ε2iL))),
(cid:0)αy j,‘ − x j,‘(M)(cid:1)2 ≤ (1 + ε)
i
∑
j=1
L
∑
‘=1
iLP∗
1 + SNR
,
(5)
for all 1 ≤ i ≤ L. To see why, consider the following: for each j, ‘,
under the AWGN channel,
(6)
where z j,‘ is independent Gaussian noise (mean 0, variance σ 2).
y j,‘ = x j,‘(M) + z j,‘,
Therefore,(cid:16)
(cid:17)2
αy j,‘ − x j,‘(M)
j,‘ + (1− α)2x j,‘(M)2
= α2z2
− 2α(1− α)z j,‘x j,‘(M).
(7)
By the independence of x j,‘(M) and z j,‘, and because E[x j,‘(M)2] ≈
P∗, the mean of the RHS of (7) is
α2σ 2 + (1− α)2P∗ =
P∗σ 4
(P∗)2σ 2
(P∗ + σ 2)2 +
P∗σ 2
(P∗ + σ 2)
=
=
(P∗ + σ 2)2
P∗
1 + SNR
.
(8)
Because all the summands on the LHS of (5) are independent and
identically distributed (i.i.d.), and there are iL summands in total,
the mean of the LHS is precisely iLP∗/(1 + SNR). Now the LHS of
(5) can be written as three summations, each having iL terms, one
each corresponding to the terms on the RHS of (7). Because each of
these is a summation of i.i.d. random variables with exponentially
decaying tails (and x2
j,‘ is bounded by 3P/2), applying a Chernoff-
style bound implies a concentration of these terms around their
means within error εiLP∗/(1 + SNR)), with probability decaying
as O(cid:0)exp(cid:0)− Θ(ε2iL)(cid:1)(cid:1) for small enough ε > 0. This argument
completes the justiﬁcation of (5).
For iL = Ω(ε−2 logn), the bound holds with probability at least
1 − 1/n4. Hence, by the union bound (“the probability that at
least one of the events happens is no greater than the sum of the
probabilities of the individual events”), it holds simultaneously for
all i = Ω(ε−2 logn) with probability ≥ 1− O(1/n3). We need this
bound for any contiguous set of indices (q,q + 1, . . . ,q + i) with
i = Ω(ε−2 logn). Since there are O(n) such intervals, by another
application of the union bound, this claim holds true with probability
at least 1− O(1/n2).
1, . . . ,m0
n) with m0
Invariant 2. Consider M0 = (m0
1 6= m1, i.e., M0
and M = (m1, . . . ,mn) differ at least in the ﬁrst bit. All the coded
symbols of M and M0 are mapped independently and at random.
That is, ¯x(M0) is independent of ¯y. We’ll focus, for the time being,
cube [−p3P/2,p3P/2]iL, and then map the co-ordinates in each
on the ﬁrst iL symbols. Now for the uniform constellation, one way
to obtain ¯x(M0) is to sample a point uniformly at random in the
2−c√
of the iL dimensions to the nearest quantized value (at granularity
6P). Therefore, the probability of ¯x(M0) falling within squared
probability that a uniformly sampled point in [−p3P/2,p3P/2]iL
distance (1 + ε)iLP∗/(1 + SNR) of α ¯y is bounded above by the
falls within r2 ≡ (1 + ε + δ1)iLP∗/(1 + SNR) of α ¯y, with δ1 =
[−p3P/2,p3P/2]iL: using Stirling’s approximation that lnK! ∼
6(1 + SNR)2−c. For the uniform distribution, this is merely the
ratio of the volume of a ball of radius r and the volume of the cube
K lnK − K,
1
(cid:16) πr2
(cid:17)iL/2
(cid:17)iL/2
1
(iL/2)!
6P
(cid:16) π(1 + ε + δ1)iL
(cid:17)iL/2
12(1 + SNR)
=
(iL/2)!
≈(cid:16) πe(1 + ε + δ1)
6(1 + SNR)
≈ 2−iL(Cawgn(P∗)−∆),
(9)
(cid:0)ε + δ1 + log(πe/6)(cid:1).
(cid:16)
where ∆ ≈ 1
2
Completing the proof using Invariants 1 and 2. Consider a bubble
decoder at stage i trying to estimate Mi using ¯yi. From Invariants 1
and 2, conditional on event (5) happening (which happens with high
probability for i = Ω(ε−2 logn)), the chances of an M0, differing
from M in the ﬁrst bit, having ¯xi(M0) closer to α ¯yi compared to
¯xi(M), is given by (9). There are at most 2ik−1 such messages. By
the union bound, the chance for such an event is at most
(cid:1)
P(i) = 2−iL(cid:0)Cawgn(P∗)−∆− k
(cid:17)
(cid:0)δ1 + log(πe/6)(cid:1)− k
L
(cid:0)δ1 + log(πe/6)(cid:1), then choosing
(10)
.
L
Cawgn(P∗)− 1
2
Thus, if k/L < Cawgn(P∗) − 1
2
ε = 1
makes the exponent
2
negative, i.e., P(i) will decay exponentially in i. Given M0 6= M,
let q(M,M0) = min{p : mp 6= m0
p} be the ﬁrst bit index where
they differ. The above argument has q(M,M0) = 1.
In general,
while decoding at stage i, at most 2 jk distinct M0 have q(M,M0) ∈
{(i − j)k + 1, . . . ,(i − j + 1)k − 1}.
In this scenario, the chance
that any such M0 has ¯xi(M0) closer to α ¯yi compared to ¯xi(M) is
the same as (10), with i replaced by j. This is because, given
that M0 and M have the same ﬁrst (i − j)k bits, their codewords
¯xi− j(M) = ¯xi− j(M0). Thus, the probability of the decoder ﬁnd-
ing ¯xi(M0) closer to α ¯yi is at most P( j). Since Invariant 1 holds
for i = Ω(ε−2 logn), at any decoding stage i, the chance that any
M0 6= M with i− q(M,M0) = Ω(ε−2 logn) has ¯xi(M0) closer to α ¯yi
compared to ¯xi(M) is bounded by
i−Ω(ε−2 logn)
∑
j=1
P( j) = O
(cid:16) 1
(cid:17)
,
n2
(11)
(cid:16)
for an appropriately large constant in the Ω(·) term above. Thus,
at any stage of decoding, only messages M0 that differ from M in
only the most recent O(ε−2 logn) bits can be closer to α ¯yi. But the
number of such messages is polynomial in n, with degree depending
on ε−2, and ε = 1
(half the
2
gap). Therefore, choosing B for the decoder to be this polynomial
value, we can ensure that at each stage, the correct message M is one
of the candidates. When decoding ends, the remaining candidates
are only those that differ from M in the last O(ε−2 logn) bits.
(cid:0)δ1 + log(πe/6)(cid:1)− k
Cawgn(P∗)− 1
2
(cid:17)
L