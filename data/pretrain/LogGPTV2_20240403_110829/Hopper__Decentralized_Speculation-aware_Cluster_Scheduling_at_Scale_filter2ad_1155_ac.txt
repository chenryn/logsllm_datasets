Incorporating DAGs of Tasks
The discussion to this point has focused on single-
phased jobs.
In practice, many jobs are deﬁned by
multiple-phased DAGs, where the phases are typically
pipelined. That is, downstream tasks do not wait for
all the upstream tasks to ﬁnish but read the upstream
outputs as the tasks ﬁnish, e.g., [6]. Pipelining is ben-
eﬁcial because the upstream tasks are typically bottle-
necked on other non-overlapping resources (CPU, mem-
ory), while the reading downstream takes network re-
sources. The additional complexity DAGs create for
our guidelines is the need to balance the gains due to
5Machines in the cluster are equally likely to cause a strag-
gler [12]; known problematic machines are already black-
listed (see §2).
overlapping network utilization with the improvements
that come from favoring upstream phases with fewer
remaining tasks.
We integrate this tradeoﬀ into Hopper using a weight-
ing factor, α per job, set to be the ratio of remaining
work in the downstream phase’s network transfer to the
remaining work in the upstream phase. Speciﬁcally, α
favors jobs with higher remaining communication and
lower remaining tasks in the current phase. The exact
details of estimating α are deferred to §6.3.
Given the weighting factor α, there are two key ad-
justments that we make to the guidelines discussed so
far. First, in Guideline 2, the prioritization of jobs based
on the virtual size Vi(t) is replaced by a prioritization
i (t)}, where Vi(t) is the virtual re-
based on max{Vi(t), V (cid:48)
maining number of tasks in the current phase and V (cid:48)
i (t)
is the virtual remaining work in communication in the
√
downstream phase.6 Second, we redeﬁne the virtual size
itself as Vi(t) = 2
β Ti(t)
αi. This form follows from the
analysis in [8] and is similar in spirit to the optimality
of square-root proportionality in load balancing across
heterogeneous servers [21].
For DAGs that are not strict chains, but are wide
and “bushy”, we calculate α by summing over all the
running and their respective downstream phases.
4.3 Incorporating Fairness
While fairness is an important constraint in clusters,
conversations with data center operators reveal that it
is not an absolute requirement. Thus, we relax the
notion of fairness currently employed by cluster sched-
ulers, e.g., [47], which enforce that if there are N (t)
active jobs and S available slots at time t, then each
job is assigned S/N (t) slots.
Speciﬁcally, to allow some ﬂexibility while still tightly
controlling unfairness, we deﬁne a notion of approximate
fairness as follows. We say that a scheduler is -fair if it
guarantees that every job receives at least (1−)S/N (t)
slots at all times t. The fairness knob  → 0 indicates
absolute fairness while  → 1 focuses on performance.
Hopper can be adjusted to guarantee -fairness in a
very straightforward manner.
In particular, if a job
receives slots less than its fair share, i.e., fewer than
(1 − )S/N (t) slots, the job’s capacity assignment is in-
creased to (1 − )S/N (t). Next, the remaining slots
are allocated to the remaining jobs according to Guide-
lines 2 or 3, as appropriate. Note that this is a form of
projection from the original (unfair) allocation into the
feasible set of allocations deﬁned by the fairness con-
straints.
Our experimental results (§7.3) highlight that even
at moderate values of , nearly all jobs ﬁnish faster
than they would have under fair scheduling. This fact,
though initially surprising, is similar to the conclusions
i (t)} is 2-
6Results in [31] show that picking the max{Ti(t), T (cid:48)
speed optimal for completion times when stragglers are not
considered.
384Figure 4: Decentralized scheduling architecture.
about SRPT-like policies. Despite being intuitively un-
fair to large job sizes, it in fact improves the average re-
sponse time of every job size (when job sizes are heavy-
tailed) compared to fair schedulers [28, 43, 44].
4.4 Incorporating Data Locality
As such, the guidelines presented does not consider
data locality [11, 48] in the scheduling of tasks. Tasks
reading their data from remote machines over the net-
work run slower.
In addition, such remote reads also
increase contention with other intermediate tasks (like
reduce tasks) that are bound to read over the network.
We devise a simple relaxation approach for balancing
adherence to our guidelines and locality. Speciﬁcally,
we adjust the ordering of jobs in Guideline 2 to include
information about locality. Instead of allotting slots to
the jobs with the smallest virtual sizes, we allow for
picking any of the smallest k% of jobs whose tasks can
run with data locality on the available slots. In practice,
a small value of k (≤ 5%) suﬃces due to high churn in
task completions and slot availabilities (§7.4).
5. DECENTRALIZED Hopper
In this section, we adapt the guidelines described in
§4 to design a decentralized (online) scheduler. Decen-
tralized schedulers are increasingly prominent as cluster
sizes grow. As we explain in this section, a key beneﬁt
of our guidelines in §4 is that they can be decentralized
with little performance loss.
Decentralized schedulers, like the recently proposed
Sparrow [36], broadly adopt the following design (see
Figure 4). There are multiple independent schedulers
each of which is responsible for scheduling one or a sub-
set of jobs; for simplicity, a single job never spans across
schedulers. Every scheduler assigns the tasks of its jobs
to machines in the cluster (referred to as workers) that
executes the tasks. The architecture allows for an in-
coming job to be assigned to any of the available sched-
ulers, while also seamlessly allowing new schedulers to
be dynamically spawned.
A scheduler ﬁrst pushes reservation requests for its
tasks to workers; each request contains the identiﬁer
of the scheduler placing the request along with the re-
maining number of unscheduled tasks in the job. When
a worker is vacant, it pulls a task from the correspond-
ing scheduler based on the reservation requests in its
waiting queue. In this framework, workers decide which
job’s task to run and the scheduler for the correspond-
ing job decides which task to run within the chosen job.
(a) Number of probes, d
(b) Number of refusals
Figure 5: The impact of number of probes and num-
ber of refusals on Hopper’s performance.
This decoupling naturally facilitates the design of Hop-
per.
Though we adopt an overall design structure simi-
lar to Sparrow for the decentralization of Hopper, it is
important to note that Hopper’s design is fundamentally
diﬀerent because it integrates straggler mitigation based
on the guidelines behind Hopper introduced in §4.
Decentralizing Hopper involves the following steps: ap-
proximating worker-wide information at each scheduler
(§5.1), deciding if the number of slots are constrained
(§5.2), and calculating virtual sizes (§5.3).
5.1 Power of Many Choices
Decentralized schedulers have to approximate the glo-
bal state of the cluster – the states of all the workers
– since they are unaware of other jobs in the system.
A common way to accomplish this is via the “power of
two choices” [38]. This celebrated and widely used result
highlights that, in many cases, one nearly matches the
performance of a centralized implementation by query-
ing two workers for their queue lengths, and choosing
the shorter of the queues. In fact, this intuition under-
lies the design of Sparrow as well, which combines the
idea with a form of “late binding”; schedulers send reser-
vation requests for every task to two workers and then
let workers pull a task from the corresponding scheduler
when they have a free slot. We adopt “late binding”, as
used in Sparrow, but replace the “power of two choices”
with the “power of many choices”.
The reason for this change is that the eﬀectiveness
of the “power of two choices” relies on having light-
tailed task size distributions. The existence of stragglers
means that, in practice, task durations are heavy-tailed,
e.g., [12, 14, 25]. Recent theoretical results have proven
Scheduler2 Job  Req Req Req …  ……  …  ReqqqqqResponse d probes Worker Worker Worker Worker Scheduler1 Probe countRatio in job durationover Centralized Scheduler  HopperSparrow24681011.21.41.61.822.2Util.=90%Util.=80%Util.=70%Util.=60%Refuse countRatio in job durationover Centralized Scheduler  024681011.522.533.544.555.5Util.=90%Util.=80%Util.=70%Util.=60%385that, when task sizes are heavy-tailed, probing d > 2
choices can provide orders-of-magnitude improvements
[18]. The value in using d > 2 comes from the fact
that large tasks, which are more likely under heavy-
tailed distributions, can cause considerable backing up
of worker queues. Two choices may not be enough to
avoid such backed-up queues, given the high frequency
of straggling tasks. More speciﬁcally, d > 2 allows the
schedulers to have a view of the jobs that is closer to
the global view.
We use simulations in Figure 5a to highlight the ben-
eﬁt of using d > 2 probing choices in Hopper and to
contrast this beneﬁt with Sparrow, which relies on the
power of two choices. Our simulation considers a clus-
ter of 50 schedulers and 10,000 workers and jobs with
Pareto distributed (β = 1.5) task sizes. Job perfor-
mance with decentralized Hopper is within just 15% of
the centralized scheduler; the diﬀerence plateaus be-
yond d = 4. Note that Sparrow (which does not co-
ordinate scheduling and speculation) is > 100% oﬀ for
medium utilizations and even further oﬀ for high uti-
lizations (not shown on the ﬁgure in order to keep the
scale visible). Further, workers in Sparrow pick tasks
in their waiting queues in a FCFS fashion. The lack
of coordination between scheduling and speculation re-
sults in a long waiting time for speculative copies in the
queues which diminishes the beneﬁts of multiple probes.
Thus parrow cannot extract the same beneﬁt Hopper has
from using more than two probes. Of course, these are
rough estimates since the simulations do not capture
overheads due to increased message processing, which
are included in the evaluations in §7.
5.2
Is the system capacity constrained?
In the decentralized setting workers implement our
scheduling guidelines. Recall that Guideline 2 or Guide-
line 3 is applied depending on whether the system is
constrained for slots or not. Thus, determining which
to follow necessitates comparing the sum of virtual sizes
of all the jobs and the number of slots in the cluster,
which is trivial in a centralized scheduler but requires
communication in an decentralized setting.
To keep overheads low, we avoid costly gossiping pro-
tocols among schedulers regarding their states. Instead,
we use the following adaptive approach. Workers start
with the conservative assumption that the system is ca-
pacity constrained (this avoids overloading the system
with speculative copies), and thus each worker imple-
ments Guideline 2, i.e., enforces an SRPT priority on
its queue. Speciﬁcally, when a worker is idle, it sends
a refusable response to the scheduler corresponding to
the reservation request of the job it chooses from its
queue. However, since the scheduler queues many more
reservation requests than tasks, it is possible that its
tasks may have all been scheduled (with respect to vir-
tual sizes). A refusable response allows the scheduler to
refuse sending any new task for the job if the job’s tasks
are all already scheduled to the desired speculation level
procedure ResponseProcessing(Response response )
Job j ← response.job
if response.type = non-refusable then
Accept()
else
if (j.current occupied < j.virtual size) Accept ()
else Refuse()
Pseudocode 2: Scheduler Methods.
procedure Response((cid:104)Job(cid:105) J, int refused count)
(cid:46) J: list of jobs in queue of the worker excluding
already refused jobs
if refused count ≥ refusal threshold then
j ← J.PickAtRandom()
SendResponse(j, non-refusable)
j ← J. min(virtual size)
SendResponse(j, refusable)
else
Pseudocode 3: Worker: choosing the next task to
schedule.
(ResponseProcessing in Pseudocode 2). In its refusal, it
sends information about the job with the smallest vir-
tual size in its list which still has unscheduled tasks (if
such an “unsatisﬁed” job exists).
Subsequently, the worker sends a refusable response
to the scheduler corresponding to second smallest job
in its queue, and so forth till it gets a threshold number
of refusals. Note that the worker avoids probing the
same scheduler more than once. Several consecutive
refusals from schedulers without information about any
unsatisﬁed jobs suggests that the system is not capacity
constrained. At that point, it switches to implement-
ing Guideline 3. Once it is following Guideline 3, the
worker randomly picks a job from the waiting queue
based on the distribution of job virtual sizes. If there
are still unsatisﬁed jobs at the end of the refusals, the
worker sends a non-refusable response (which cannot
be refused) to the scheduler whose unsatisﬁed job is the
smallest. Pseudocode 3 explains the Response method.
The higher the threshold for refusals, the better the
view of the schedulers for the worker. Our simulations
(with 50 schedulers and 10,000 workers) in Figure 5b
show that performance with two or three refusals is
within 10% − 15% of the centralized scheduler.
5.3 Updating Virtual Job Sizes
Computing the remaining virtual job size at a sched-
uler is straightforward. However, since the remaining
virtual size of a job changes as tasks complete, vir-
tual sizes need to be updated dynamically. Updat-
ing virtual sizes accurately at the workers that have
queued reservations for tasks of this job would require
frequent message exchanges between workers and sched-
ulers, which would create signiﬁcant overhead in com-
munication and processing of messages. So, our ap-
proach is to piggyback updates for virtual sizes on other
communication messages that are anyway necessary be-
tween a scheduler and a worker (e.g., schedulers send-
386ing reservation requests for new jobs, workers sending
responses to probe system state and ask for new tasks).
While this introduces a slight error in the virtual re-
maining sizes, our evaluation shows that the approx-
imation provided by this approach is enough for the
gains associated with Hopper.
Crucially, the calculation of virtual sizes is heavily
impacted by the job speciﬁcs. Job speciﬁc properties of
the job DAG and the likelihood of stragglers are cap-
tured through α and β, respectively, which are learned
online. Note that jobs from diﬀerent applications may