### Optimized Text

#### Feature Descriptions
- **Sex**
- **Occupation**
- **Age (per week)**
- **Education**
- **Education-Num**
- **Capital Loss**
- **Capital Gain**
- **Hours per Week**
- **Country**
- **Workclass**
- **Race**
- **Relationship**
- **Region**
- **Year of Birth**
- **History (e.g., Drug History, Smoking History)**
- **Census Data**

#### QII Measures for the Adult and Arrests Datasets
- **QII on Group Disparity by Sex in the Adult Dataset** (Figure 2c)
- **Influence on Group Disparity by Race in the Arrests Dataset** (Figure 2d)

#### Runtimes in Seconds for Transparency Report Computation
| Model          | Logistic Regression | Kernel SVM | Decision Tree | Decision Forest |
|----------------|---------------------|-------------|---------------|-----------------|
| Average QII    | 0.73                | 1.12        | 9.30          | 10.34           |
| Runtime (s)    | 234.93              | 322.82      | 2522.3        | 2413.3          |

#### Discussion
**A. Probabilistic Interpretation of Power Indices**

To quantitatively measure the influence of data inputs on classification outcomes, we propose causal interventions on sets of features. The aggregate marginal influence of a feature \( i \) for different subsets of features is a natural quantity representing its influence. To aggregate the various influences \( i \) has on the outcome, it is natural to define a probability distribution over subsets of \( N \setminus \{i\} \), where \( \Pr[S] \) represents the probability of measuring the marginal contribution of \( i \) to \( S \).

For the Banzhaf index, \( \Pr[S] = \frac{1}{2^{n-1}} \). For the Shapley value, \( \Pr[S] = \frac{(n-1)!}{(k!(n-k-1)!)} \) (where \( |S| = k \)). The Deegan-Packel Index selects minimal winning coalitions uniformly at random. These choices of values for \( \Pr[S] \) are based on some natural assumptions about how features interact, but they are not exhaustive. Other sampling methods can be defined that are more appropriate for specific models. For example, if the only possible interventions are of size \( \leq k + 1 \), it is reasonable to aggregate the marginal influence of \( i \) over sets of size \( \leq k \):

\[ \Pr[S] = \begin{cases} 
\frac{1}{\binom{n-1}{|S|}} & \text{if } |S| \leq k \\
0 & \text{otherwise}
\end{cases} \]

The key point is that one must define an aggregation method, and this choice reflects a normative approach to which marginal contributions are considered. While the Shapley and Banzhaf indices have desirable properties, they are a-priori measures of influence and do not factor in any assumptions about possible or desirable interventions.

One natural candidate for a probability distribution over \( S \) is an extension of the prior distribution over the dataset. If all features are binary, each set \( S \subseteq N \) can be identified with its indicator vector, and \( \Pr[S] = \pi(S) \) for all \( S \subseteq N \). If features are not binary, there is no canonical way to transition from the data prior to a prior over subsets of features.

**B. Fairness**

Due to the widespread and often opaque use of machine learning in decision-making, there is a legitimate concern about algorithms introducing and perpetuating social harms such as racial discrimination. As a result, the algorithmic foundations of fairness in personal information processing systems have received significant attention. While many algorithmic approaches focus on group parity as a metric for achieving fairness in classification, Dwork et al. argue that group parity is insufficient and propose a similarity-based approach, which prescribes that similar individuals should receive similar classification outcomes. However, this approach requires a similarity metric for individuals, which is often subjective and difficult to construct.

QII does not suggest a normative definition of fairness. Instead, we view QII as a diagnostic tool to aid fine-grained fairness determinations. In fact, QII can be used in the spirit of the similarity-based definition proposed by Dwork et al. By comparing the personalized privacy reports of individuals who are perceived to be similar, QII can help identify and mitigate potential biases in classification outcomes.

#### Case Studies

**Mr. X's Profile**
- **Age**: 23
- **Workclass**: Private
- **Education**: 11th
- **Education-Num**: 7
- **Marital Status**: Never-married
- **Occupation**: Craft-repair
- **Relationship**: Own-child
- **Race**: Asian-Pac-Islander
- **Gender**: Male
- **Capital Gain**: 14344
- **Capital Loss**: 0
- **Hours per Week**: 40
- **Country**: Vietnam

**Transparency Report for Mr. X’s Negative Classification**
- **QII on Individual Outcomes (Shapley)**: 0.5, 0.4, 0.3, 0.2, 0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5
- **Features**: Capital Gain, Workclass, Gender, Race, Hours per Week, Capital Loss, Occupation, Country, Age, Relationship, Education, Education-Num, Marital Status

**Mr. Y's Profile**
- **Age**: 27
- **Workclass**: Private
- **Education**: Preschool
- **Education-Num**: 1
- **Marital Status**: Married-civ-spouse
- **Occupation**: Farming-fishing
- **Relationship**: Other-relative
- **Race**: White
- **Gender**: Male
- **Capital Gain**: 41310
- **Capital Loss**: 0
- **Hours per Week**: 24
- **Country**: Mexico

**Transparency Report for Mr. Y’s Negative Classification**
- **QII on Individual Outcomes (Shapley)**: 0.4, 0.3, 0.2, 0.1, 0.0, 0.1, 0.2, 0.3, 0.4, 0.5
- **Features**: Capital Gain, Sex, Education, Relationship, Workclass, Race, Country, Capital Loss, Marital Status, Age, Hours per Week, Education-Num, Occupation

This optimized text provides a clearer and more structured presentation of the original content, making it easier to understand and follow.