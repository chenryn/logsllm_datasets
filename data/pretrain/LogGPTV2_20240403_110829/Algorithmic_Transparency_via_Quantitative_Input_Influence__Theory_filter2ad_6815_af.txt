S e x
O cc u p atio n
A g e
p er
w e e k
E d u c atio n
E d u c atio n-N u m
C a pital L o ss
C a pital G ain
H o urs
Feature
C o u ntry
W ork cla ss
R a c e
R elatio n s hip
R a c e
S e x
R e gio n
Y e ar
Birth
History
History
D ru g
S m o kin g
C e n s u s
Feature
(c) QII of Inputs on Group Disparity by Sex in the adult dataset
(d) Inﬂuence on Group Disparity by Race in the arrests dataset
Fig. 2: QII measures for the adult and arrests datasets
QII on Group Disparity
Average QII
QII on Individual Outcomes (Shapley)
QII on Individual Outcomes (Banzhaf)
0.73
1.12
9.30
10.34
TABLE III: Runtimes in seconds for transparency report computation
234.93
322.82
2522.3
2413.3
0.57
0.77
7.78
7.64
logistic
0.56
0.85
6.85
6.77
kernel-svm decision-tree
decision-forest
VIII. DISCUSSION
A. Probabilistic Interpretation of Power Indices
In order to quantitatively measure the inﬂuence of data
inputs on classiﬁcation outcomes, we propose causal inter-
ventions on sets of features; as we argue in Section III,
the aggregate marginal inﬂuence of i for different subsets
of features is a natural quantity representing its inﬂuence. In
order to aggregate the various inﬂuences i has on the outcome,
it is natural to deﬁne some probability distribution over (or
equivalently, a weighted sum of) subsets of N \ {i}, where
Pr[S] represents the probability of measuring the marginal
S⊆N\{i} mi(S).
contribution of i to S; Pr[S] yields a value
(cid:10)
610610
n!
k!(n−k−1)!
For the Banzhaf index, we have Pr[S] = 1
2n−1 , the Shapley
(here, |S| = k), and the Deegan-
value has Pr[S] =
Packel Index selects minimal winning coalitions uniformly at
random. These choices of values for Pr[S] are based on some
natural assumptions on the way that players (features) interact,
but they are by no means exhaustive. One can deﬁne other
sampling methods that are more appropriate for the model
at hand; for example, it is entirely possible that the only
interventions that are possible in a certain setting are of size
≤ k + 1, it is reasonable to aggregate the marginal inﬂuence
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:24 UTC from IEEE Xplore.  Restrictions apply. 
0.40
0.35
0.30
0.25
0.20
0.15
0.10
0.05
s
e
m
o
c
t
u
O
n
o
I
I
Q
0.00
0.0
Sex
Marital Status
0.2
0.4
0.6
0.8
Fraction of Discriminatory Zip Codes
1.0
(a) Change in QII of inputs as discrimination by Zip Code increases
in the adult dataset
0.35
0.30
0.25
0.20
0.15
0.10
s
e
m
o
c
t
u
O
n
o
I
I
Q
0.05
0.0
Race
Drug History
0.2
0.4
0.6
0.8
Fraction of Discriminatory Zip Codes
Age
Workclass
Education
Education-Num
Marital Status
Occupation
Relationship
Race
Gender
Capital Gain
Capital Loss
Hours per week
Country
23
Private
11th
7
Never-married
Craft-repair
Own-child
Asian-Pac-Islander
Male
14344
0
40
Vietnam
(a) Mr. X’s proﬁle
0.5
0.4
0.3
0.2
0.1
0.0
0.1
0.2
l
)
y
e
p
a
h
S
(
s
e
m
o
c
t
u
O
n
o
I
I
Q
0.3
C a pital G ain
W ork cla ss
G e n d er
R a c e
p er
H o urs
C a pital L o ss
w e e k
O cc u p atio n
C o u ntry
A g e
R elatio n s hip
E d u c atio n
E d u c atio n-N u m
M arital Statu s
(b) Transparency report for Mr. X’s negative classiﬁcation
Fig. 4: Mr. X
1.0
(b) Change in QII of inputs as discrimination by Zip Code increases
in the arrests dataset
Fig. 3: The effect of discrimination on QII.
(cid:11)
of i over sets of size ≤ k, i.e.
Pr[S] =
1
(n−1|S| )
0
if |S| ≤ k
otherwise.
The key point here is that one must deﬁne some aggregation
method, and that choice reﬂects some normative approach on
how (and which) marginal contributions are considered. The
Shapley and Banzhaf indices do have some highly desirable
properties, but they are, ﬁrst and foremost, a-priori measures
of inﬂuence. That is, they do not factor in any assumptions on
what interventions are possible or desirable.
One natural candidate for a probability distribution over S
is some natural extension of the prior distribution over the
dataset; for example, if all features are binary, one can identify
a set with a feature vector (namely by identifying each S ⊆ N
with its indicator vector), and set Pr[S] = π(S) for all S ⊆ N.
611611
If features are not binary, then there is no canonical way to
transition from the data prior to a prior over subsets of features.
B. Fairness
Due to the widespread and black box use of machine
learning in aiding decision making,
there is a legitimate
concern of algorithms introducing and perpetuating social
harms such as racial discrimination [28], [6]. As a result,
the algorithmic foundations of fairness in personal informa-
tion processing systems have received signiﬁcant attention
recently [29], [30], [31], [12], [32]. While many of of the
algorithmic approaches [29], [31], [32] have focused on group
parity as a metric for achieving fairness in classiﬁcation,
Dwork et al. [12] argue that group parity is insufﬁcient as
a basis for fairness, and propose a similarity-based approach
which prescribes that similar individuals should receive similar
classiﬁcation outcomes. However, this approach requires a
similarity metric for individuals which is often subjective and
difﬁcult to construct.
QII does not suggest any normative deﬁnition of fairness.
Instead, we view QII as a diagnostic tool to aid ﬁne-grained
fairness determinations. In fact, QII can be used in the spirit
of the similarity based deﬁnition of [12]. By comparing the
personalized privacy reports of individuals who are perceived
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:24 UTC from IEEE Xplore.  Restrictions apply. 
Age
Workclass
Education
Education-Num
Marital Status
Occupation
Relationship
Race
Gender
Capital Gain
Capital Loss
Hours per week
Country
27
Private
Preschool
1
Married-civ-spouse
Farming-fishing
Other-relative
White
Male
41310
0
24
Mexico
(a) Mr. Y’s proﬁle
0.4
0.3
0.2
0.1
0.0
0.1
0.2
0.3
0.4
l
)
y
e
p
a
h
S
(
e
m
o
c
t
u
O
n
o
I
I
Q
0.5
C a pital G ain
S e x
E d u c atio n
R elatio n s hip
W ork cla ss
R a c e
C o u ntry
C a pital L o ss
M arital Statu s
A g e
p er
w e e k
E d u c atio n-N u m
O cc u p atio n
H o urs