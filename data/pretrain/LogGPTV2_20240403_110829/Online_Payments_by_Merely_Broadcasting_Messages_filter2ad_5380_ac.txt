Bob’s representative replica collects and aggregates CREDIT
messages for the same incoming payment into a dependency
certiﬁcate for Bob’s xlog. If Bob’s representative fails in any
way, this certiﬁcate is not lost; the certiﬁcate is permanently
stored as CREDIT messages, distributed across the replicas
that approved the payment, so it can be reconstructed directly
from these replicas.
Note that replicas must keep track of used certiﬁcates,
ensuring that each payment takes effect not more than once.
This way, it is impossible for replicas to mistakenly apply a
dependency twice (e.g., double-deposit, as in a replay attack).
Listings 3 and 4 have to be adjusted to take dependencies into
account, see pseudocode in [27, Appendix B].
Certiﬁcates also play an important role in a sharded envi-
ronment, as they are transferable across shards: They enable
Bob to spend the money mentioned in the dependency not
only within his representative’s shard, but also across shards
(§V). Whenever Bob submits an outgoing payment, his
representative replica attaches the accumulated dependencies
alongside the outgoing payment.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:24:09 UTC from IEEE Xplore.  Restrictions apply. 
30
Comparison. Astro II is well-suited for environments where
bandwidth is scarce (e.g., WAN), whereas Astro I has lower
computation requirements and is therefore suited for systems
where computing resources are more scarce. Given a batching
scheme, however, we can amortize the cost of digital signa-
tures in Astro II, as we describe later (§VI-A). Moreover, we
expect the typical deployment of our system to be a wide-area
network where bandwidth is the scarce resource. Because of
these reasons, Astro II has an edge over Astro I in terms of
performance—a hypothesis we quantify in our experimental
evaluation (§VI-C).
The two systems handle transitive transactions differently.
Astro I does not reject
insufﬁciently funded transactions
(line 18, Listing 3), instead it queues them until enough funds
arrive. Queuing is necessary even with totality, since different
replicas may receive crediting transactions at different times.
Instead, the dependencies mechanism in Astro II allow the
spender’s representative to prove that the spender has sufﬁ-
cient funds to issue a payment.
There is an additional
important distinction between
Astro I and Astro II: the latter is amenable to sharding. To
understand why this is the case, we observe that sharding
requires the approval of payments across different shards.
In other words, some shard s1 has to convince some other
shard s2 that s1 approved a certain payment and s2 can settle
it. Digital signatures simplify this transfer of trust between
shards, because the payment of a spender from s1 appears as
a dependency in the xlog of the beneﬁciary in s2. Replicas
in s2 accept this dependency when they verify it is signed
by f + 1 replicas of s1. In the next section, we provide the
full details of the sharding mechanism which we implement
in Astro II.
V. ASYNCHRONOUS SHARDING
So far we described our payment protocol (§III) assuming
full replication. In this model, all replicas maintain a full copy
of the system state (i.e., xlogs) and approve and settle every
payment. The full replication architecture is simple to under-
stand and implement, and excels at small scale. This design
poses two scalability problems. First, throughput degrades
with increasing replica count (as we observe experimentally
in §VI-C). Second, each replica has to keep more state as
the number of xlogs (i.e., clients) increases.
We now reﬁne the architecture of our payment system
with sharding, which Astro II implements. Sharding is a
well-known technique [3], [4], [13], [48], [53], [76], [80],
allowing our system to scale-out in terms of both number
of replicas and number of clients. We deﬁne a shard as a
subset of system replicas, and to be associated with a subset
of all xlogs. We use the notation s(·) to denote the shard
to which some replica or client “·” belongs. Importantly,
sharding requires strengthening our assumption from §III, so
that the threshold N/3 on Byzantine replicas applies to every
shard.
Intuitively, each shard in Astro II executes an instance of
the basic payment protocol (§III) for its associated clients.
It also incorporates an additional mechanism that not only
prevents partial payment attacks, but also supports sharding
seamlessly. The broadcast step of Astro II is executed in the
shard of the spender, while the CREDIT messages may be
sent to a representative in another shard.
Let us consider a payment of amount x from spender A to
beneﬁciary B and illustrate how Astro II processes it. Let r
be the representative replica of A.
After broadcasting and approving the payment of client A,
all honest replicas in shard s(A) unicast a CREDIT message
to the beneﬁciary’s representative in s(B),
indicating the
crediting of amount x to the balance of client B. This message
comprises all details of this payment (including the sequence
number n assigned by client A), as well as a signature sig
indicating the approval of the payment from the perspective
of that replica. The representative of B interprets f+1 distinct
CREDIT messages as a dependency certiﬁcate, i.e., a proof
that the payment has been accepted by shard s(A). This
dependency certiﬁcate is stored at the representative of B and
gets added toB’s balance when the next outgoing transaction
issued by B is settled by the replicas in shard s(B).
Traditional sharded designs employ a 2PC protocol for
coordinating transactions that span multiple shards [10], [36].
The 2PC protocol relies on synchrony and has a delay of 3
communication steps; each such step usually has complexity
O(m) and in the Byzantine case it can reach up to O(m2),
where m is the size of a shard [36], [39]. In contrast, our
protocol based on the CREDIT message entails exactly 1
communication step and has overall complexity O(m). In
our experiments with Astro II implementing the Smallbank
Application [31] we observe that this sharding mechanism
has negligible overhead (§VI-C2).
The insight enabling such a simple sharding mechanism in
Astro is that we decouple payment processing at the spender
from the beneﬁciary. In fact, this mechanism is orthogonal
to how a payment is executed inside a shard (e.g., using a
consensus or a broadcast based protocol).
VI. EXPERIMENTAL EVALUATION
We now report on the experimental evaluation of our
consensus-free approach to payment systems. We ﬁrst de-
scribe the systems we evaluate, namely Astro I and II and a
baseline based on consensus (§VI-A). We also detail our eval-
uation methodology (§VI-B) and present our comprehensive
evaluation, covering both the common-case and performance
robustness (§§ VI-C and VI-D).
A. Systems under Evaluation
We build our baseline on top BFT-SMaRt, a mature state-
of-the-art BFT SMR (i.e., consensus) implementation [12],
used, for example, as the ordering service of Hyperledger
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:24:09 UTC from IEEE Xplore.  Restrictions apply. 
31
Fabric [70]1. For both Astro systems and BFT-SMaRt under
evaluation we assume the optimal threshold of N = 3f + 1
replicas, where f bounds the number of faulty replicas.
Batching in Astro I and II. We employ a 1- or 2-level batch-
ing scheme, depending on the variant of our system. First,
we perform batching at the level of the broadcast protocol.
Note that the ﬁrst step of the broadcast protocol (PREPARE
in §IV-A) is identical across our two systems. Brieﬂy, some
replica i sending a PREPARE is the one assembling a batch of
payments—potentially from different clients—with the goal
of amortizing both the cost of message authentication and
network processing overheads.
Second, to reduce the overhead of digital signatures neces-
sary for the BRB and the CREDIT messages, Astro II groups
together payments for which the beneﬁciary clients have the
same representative replica. Thus, when a replica i builds a
batch of payments to be broadcast, it includes sub-batches of
payments segregated according to the beneﬁciary replica. As
a result, there are as many signatures for CREDIT messages
as there are sub-batches. All payments in the batch are
processed together during broadcast, while the payments in
the sub-batches are processed together when settling and
unicasting.
Even though batching alleviates the computational burden
of cryptographic signatures, it relies on the fact that clients
have to trust their replicas for not issuing transactions without
the former’s consent. However, our approach can protect
clients from malicious representative behavior if the same
protocol adopts end-to-end client signatures.
Cryptography in Astro II. We used ECDSA on the NIST
P-256 curve provided from the Golang standard library,
which offers adequate performance. To avoid cryptographic
operations acting as a CPU bottleneck, we use one signature
per batch of 256 payments in the broadcast layer. With this
batch size, Astro II’s performance is only limited by available
bandwidth.
B. Evaluation Methodology
We use Amazon EC2 as our experimental platform.
Throughout all experiments, we use commodity-level virtual
machines (VMs) of type t2.medium [6], equipped with 4GiB
of RAM and 2 vCores. Unless we explicitly state otherwise,
we deploy each system so that every replica executes on a
separate VM. This avoids creating noise in our results, which
could arise due to performance interference.
Our deployment setup comprises four Amazon EC2 re-
gions in Europe, namely Frankfurt, Ireland, London, and
Paris. On average,
the bandwidth and round-trip latency
1In general,
there is a notable difference in complexity between
consensus—in particular, the Byzantine-fault tolerant versions—and broad-
cast algorithms. Both Astro implementations require less than 3.5K LOC in
Golang. Contrast this with libpaxos [67], a simple consensus implementa-
tion for the crash-only model, stretching over more than 6K LOC in C. At
the time of its original publication, the BFT-SMaRt implementation counted
around 13.5K LOC in Java [12, §III].
across machines of these four regions is around 30 MiB/sec
and 20ms, respectively. We deploy the replicas of each
system randomly across the corresponding regions. This de-
ployment reﬂects a scenario where participants are localized
in one geographic region of the globe (Europe). Later in
our experiments, we also introduce network delays at each
replica. As a result, we lessen the effect of sub-millisecond
latency between replicas in the same region and obtain more
realistic conditions with larger latencies (§VI-C2).
We use up to 15 VMs to deploy clients. Each request from
a client represents one payment. A request contains three
ﬁelds (the spender and beneﬁciary identities, along with the
amount) and the client authentication data. The beneﬁciary
and amount ﬁelds are random, and each payment operation
covers roughly 100 bytes.
For simplicity, we place all client VMs in Ireland. Spread-
ing clients around Europe does not inﬂuence our results. Each
such VM hosts a varying number of client processes. The
number of processes varies greatly, depending on each system
and the system size. For instance, to saturate BFT-SMaRt at
system size N = 4, we use around 800 total client threads;
for N = 100, 30 threads are sufﬁcient for saturation. For
Astro, we require more client threads to reach saturation,
since they are capable of higher performance. We report the
maximum achievable performance: our experiments assume
that all transactions can be settled immediately, i.e. clients
have enough balance, so transactions can not be blocked due
to insufﬁcient funds.
For throughput, we report on how many payments each
system settles per second, labeled pps. All experiments have
a runtime of 60 seconds, and we present the average result
across 3 runs. We also plot the standard deviation, but often
this is negligible and not clearly visible in the plots.
In BFT-SMaRt, each client keeps connections to all repli-
cas (a design decision of this protocol) [12]. For this reason,
all BFT-SMaRt clients experience similar latencies. In our
results we report on the latency as observed by a random
client. In our Astro systems, each client connects to a
single, random replica. To make all replicas execute payments
(which is the most realistic scenario), clients pick and submit
their workload to a random replica.
C. Performance Evaluation Results
We seek to answer the broad question of how our asyn-
chronous approach compares in performance, at varying
system sizes, with the consensus baseline. We discuss mi-
crobenchmarks for latency and throughput in a single shard
(§VI-C1), as well as results with the Smallbank [5] bench-
mark in a sharded scenario (§VI-C2).
1) Microbenchmarks:
Throughput. In Figure 3 we depict how throughput evolves
as a function of system size. For each system size, we plot
the peak throughput, i.e., before latency saturates. Note the
logscale axis, to better capture performance differences. We
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:24:09 UTC from IEEE Xplore.  Restrictions apply. 
32
 100000
 100000
 10000
 10000
 1000
 1000
)
)
s
s
p
p
p
p
(
(
t
t
u
u
p
p
h
h
g
g
u
u
o
o
r
r
h
h
t
t
k
k
a
a
e
e
P
P
Consensus-based system (BFT-SMaRt)
Broadcast echo-based system (Astro I)
Broadcast signature-based system (Astro II)
4 1
4 1
1
1
2
2
2
2
3
3
4
4
4
4