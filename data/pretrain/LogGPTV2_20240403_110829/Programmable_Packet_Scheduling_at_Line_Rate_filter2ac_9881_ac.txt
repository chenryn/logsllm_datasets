As an example, PIFOs cannot express the following pol-
icy: WFQ on a set of ﬂows A, B, and C, with the additional
constraint that the aggregate throughput of A+B doesn’t ex-
ceed 10 Mbit/s. One work around is to implement this as
HPFQ across two classes C1 and C2, with C1 containing A
and B, and C2 containing C alone. Then, we enforce the rate
limit of 10 Mbit/s on C1 as in Figure 4. However, this isn’t
equivalent to our desired policy. More generally, our pro-
gramming model for programmable scheduling establishes
a one-to-one relationship between the scheduling and shap-
ing transactions, which is constraining for some algorithms.
Output rate limiting.
The PIFO abstraction enforces
rate limits using a shaping transaction, which determines a
packet or PIFO reference’s scheduling time before it is en-
queued into a PIFO. The shaping transaction permits rate
limiting on the input side, i.e., before elements are enqueued.
An alternate form of rate limiting is on the output, i.e., by
limiting the rate at which elements are scheduled.
To illustrate the difference, consider a scheduling algo-
rithm with two priority queues, LO and HI, where LO is to be
rate limited to 10 Mbit/s. To program this using input side
rate limiting, we would use a shaping transaction to impose
a 10 Mbit/s rate limit on LO and a scheduling transaction
to implement strict priority scheduling between LO and HI.
Now, assume packets from HI starve LO for a long period of
time. During this time, packets from LO, after leaving the
shaping PIFO, accumulate in the PIFO shared with HI. Now,
if there are suddenly no more HI packets, all packets from LO
are transmitted at line rate, and no longer rate limited to 10
Mbit/s for a transient period of time, i.e., until all instances
of LO are drained out of the PIFO shared with HI. Input rate
limiting still provides long-term rate guarantees, while out-
put rate limiting provides short-term guarantees as well.
4. DESIGN
We now present a hardware design for a programmable
scheduler based on PIFOs. We target shared-memory
switches such as Broadcom’s Trident II [3] (Figure 8). In
these switches, a parser feeds packets from all ports into
a shared ingress pipeline, after which they enter a shared
scheduler and a similarly shared egress pipeline. To reduce
chip area, combinational logic and memory for packet pro-
cessing are shared across ports, both in the pipelines and in
the scheduler. As a result, digital circuits on the switch must
handle the aggregate processing requirements of all output
ports at minimum packet size, e.g., 64 10 Gbit/s ports each
transmitting 64 byte packets. This translates into ~1 bil-
lion packets per second, after accounting for minimum inter-
packet gaps, or a 1 GHz clock frequency.
We ﬁrst describe how scheduling and shaping transactions
can be implemented (§4.1). Then, we show how a tree of PI-
Figure 8: A 64-port shared memory switch. Combinational logic and memory are shared across ports, both in the pipelines
and in the scheduler. The switch runs at a clock frequency of 1 GHz.
FOs can be realized using a full mesh of PIFO blocks by
appropriately interconnecting these blocks (§4.2). We also
describe how a compiler (§4.3) could automatically conﬁg-
ure this mesh from a scheduling tree.
4.1 Scheduling and shaping transactions
To program and implement scheduling and shaping trans-
actions, we use Domino [37], a recent system to program
stateful data-plane algorithms at line rate. Domino intro-
duces hardware primitives (atoms), and software abstrac-
tions (packet transactions) to program stateful algorithms on
programmable switches [1, 5, 11, 17]
Atoms are processing units representing a programmable
switch’s instruction set, while a packet transaction is a
block of code that is guaranteed to execute atomically. The
Domino compiler compiles a scheduling or shaping packet
transaction into an atom pipeline that executes the transac-
tion atomically, rejecting the transaction if it can’t run at line
rate. Transactions may be rejected for two reasons; either
because there are not enough atoms to execute the transac-
tion, or because the transaction requires computation beyond
the atoms’ capabilities.
Domino proposes atoms that are expressive enough to
support many data-plane algorithms and small enough to
implement at 1 GHz . For instance, even the most expres-
sive of these atoms, called Pairs, occupies only 6000 µm2
in a 32 nm standard-cell library [37]; a 200 mm2 switch-
ing chip [23] can support 300 Pairs atoms with < 2% area
overhead. These 300 atoms are sufﬁcient for many data-
plane algorithms [37]. The Domino paper also shows how
the STFQ transaction (Figure 1) can be run at 1 GHz on a
switch pipeline with the Pairs atom.
Similarly, we could use the Domino compiler to com-
pile other scheduling and shaping transactions to an atom
pipeline. For example, the transactions for Token Bucket
Filtering (Figure 4c), minimum rate guarantees (§3.3), Stop-
and-Go queueing (§3.2), and LSTF (§3.1), can all be ex-
pressed as Domino programs. An important restriction in
Domino is the absence of loops, which precludes rank com-
putations containing a loop with an unbounded iteration
count. We have not, however, encountered a scheduling or
shaping transaction requiring this capability.
4.2 The PIFO mesh
We lay out PIFOs physically as a full mesh (Figure 9) of
Figure 9: Three PIFO blocks in a PIFO mesh
Figure 10: A single PIFO block. Enqueue operations exe-
cute transactions in the atom pipeline before enqueuing ele-
ments into a logical PIFO. Dequeue operations dequeue ele-
ments from logical PIFOs before looking up their next hop.
PIFO blocks (Figure 10). Each PIFO block supports multi-
ple logical PIFOs. These logical PIFOs correspond to PIFOs
for different output ports or different classes in a hierarchical
scheduling algorithm, which share the combinational logic
required for a PIFO. We expect a small number of PIFO
blocks in a typical switch (e.g., fewer than ﬁve) because each
PIFO block corresponds to a different level of a hierarchical
scheduling tree and most practical hierarchical scheduling
algorithms we know of do not require more than a few levels
of hierarchy. As a result, a full mesh between these blocks
is feasible (§5.3 has more details).
PIFO blocks run at 1 GHz and contain an atom pipeline
to execute scheduling and shaping transactions before en-
queuing into a logical PIFO. In every clock cycle, each PIFO
block supports one enqueue and dequeue operation on a log-
ical PIFO residing within that block (shaping transactions
require more than one operation per clock cycle and are dis-
cussed in §4.4).
The interface to a PIFO block is:
1. Enqueue an element (packet or reference to another
PIFO) given a logical PIFO ID, the element’s rank,
and some metadata that will be carried with the ele-
ment such as the packet length required for STFQ’s
rank computation. The enqueue returns nothing.
SchedulerParserDeparserIngress pipelineEgress pipelineTCPIPv4IPv6VLANEthmatch/actionStage 1match/actionStage 2match/actionStage 32match/actionStage 1match/actionStage 3264Outputports64InputportsLogicalPIFOsNext-hoplookupAtompipelineEnqDeqLogicalPIFOsNext-hoplookupAtompipelineEnqDeqLogicalPIFOsNext-hoplookupAtompipelineEnqDeqLogical PIFOsAtom pipelineNext-hop lookupEnq(logical PIFO ID,rank, metadata)Deq(logical PIFO ID)(a) Logical PIFO tree for HPFQ
(b) Physical PIFO mesh for HPFQ
Figure 11: Compiling HPFQ (Figure 2) to a PIFO mesh. On the left, the logical PIFO tree captures relationships between
PIFOs: which PIFOs dequeue or enqueue into which PIFOs. Red arrows indicate dequeues, blue indicates enqueues. On the
right, we show the physical PIFO mesh for the logical PIFO tree on the left, following the same notation.
(a) Logical PIFO tree for Hierarchies with Shaping
(b) Physical PIFO mesh for Hierarchies with Shaping
Figure 12: Compiling Hierarchies with Shaping (Figure 2) to a PIFO mesh. Same comments as Figure 11 apply.
2. Dequeue from a speciﬁc logical PIFO ID within the
block. The dequeue returns either a packet or a refer-
ence to another PIFO.
After a dequeue, besides transmitting a packet, a PIFO
block may communicate with another for two reasons:
1. To dequeue a logical PIFO in another block, e.g., when
dequeuing a sequence of PIFOs from the root to a leaf
of a scheduling tree to transmit packets.
2. To enqueue into a logical PIFO in another block, e.g.,
when enqueuing a packet that has just been dequeued
from a shaping PIFO.
We conﬁgure these post-dequeue operations using a small
lookup table, which looks up the “next hop” following a de-
queue. This lookup table speciﬁes an operation (enqueue,
dequeue, transmit), the PIFO block for the next operation,
and any arguments the operation needs.
4.3 Compiling from a scheduling tree to a
PIFO mesh
A programmer should not have to manually conﬁgure a
PIFO mesh. Instead, a compiler translates from a scheduling
tree to a PIFO mesh conﬁguration implementing that tree.
While we haven’t prototyped this compiler, we illustrate how
it would work using HPFQ (Figure 2) and Hierarchies with
Shaping (Figure 4).
The compiler ﬁrst converts the scheduling tree to a log-
ical PIFO tree that speciﬁes the enqueue and dequeue op-
erations on each PIFO. Figures 11a and 12a show this tree
for Figures 2 and 4 respectively. It then overlays this tree
over a PIFO mesh by assigning every level of the tree to
a PIFO block and conﬁguring the lookup tables to connect
PIFO blocks as required by the tree. Figure 11b shows the
PIFO mesh for Figure 2, while Figure 12b shows the PIFO
mesh for Figure 4.
If a particular level of the tree has more than one enqueue
or dequeue from another level, which arises in the presence
of shaping transactions (§4.4), we allocate new PIFO blocks
to respect the constraint that any PIFO block provides one
enqueue and dequeue operation per clock cycle, e.g., Fig-
ure 12b has an additional PIFO block containing TBF_Right
alone. Finally, we use the Domino compiler to compile
scheduling and shaping transactions.
WFQ_RootWFQ_LeftWFQ_RightPkts. fromflows A/BPkts. fromflows C/DTransmitpkt.Transmitpkt.LinkavailableLinkavailableTransmitpkt.Pkt. from allflows: A/B/C/DWFQ_RootWFQ_LeftWFQ_RightDEQDEQENQENQNext-hoplookupNext-hoplookupWFQ_RootWFQ_LeftWFQ_RightPkts. fromflows A/BPkts. fromflows C/DTBF_RightTransmitpkt.Transmitpkt.LinkavailableTransmitpkt.Pkt. fromflows C/DPkt. fromflows A/BWFQ_RootWFQ_LeftWFQ_RightTBF_RightPkt. fromflows C/DDEQDEQDEQENQENQENQNext-hoplookupNext-hoplookupNext-hoplookupLinkavailable4.4 Challenges with shaping transactions
Each PIFO block supports one enqueue and dequeue op-
eration per clock cycle. This sufﬁces for any algorithm
that only uses scheduling transactions (work-conserving al-
gorithms) because, for such algorithms, each packet needs
at most one enqueue and one dequeue at each level of its
scheduling tree, and we map the PIFOs at each level to a
different PIFO block.
However, shaping transactions pose challenges. Consider
Hierarchies with Shaping (Figure 12a). When the shaping
transaction enqueues elements into TBF_Right, these ele-
ments will be released into WFQ_Root at a future time T .
The external enqueue into WFQ_Root may also happen ex-
actly at T , if a packet arrives at T . This creates a conﬂict
because there are two enqueue operations in the same cy-
cle. Conﬂicts may also occur on dequeues. For instance, if
TBF_Right shared its PIFO block with another logical PIFO,
dequeue operations to the two logical PIFOs could occur at
the same time because TBF_Right can be dequeued at any
arbitrary wall-clock time.
In a conﬂict, only one of the two operations can proceed.
We resolve this conﬂict in favor of scheduling PIFOs. Shap-
ing PIFOs are used for rate limiting to a rate lower than the
line rate. Therefore, they can afford to be delayed by a few
clocks until there are no conﬂicts. By contrast, delaying
scheduling decisions of a scheduling PIFO would mean that
the switch would idle and not satisfy its line-rate guarantee.
As a result, shaping PIFOs only get best-effort service.
There are workarounds to this. One is overclocking the
pipeline at (say) 1.25 GHz instead of 1 GHz, providing spare
clock cycles for such best-effort processing. Another is to
provide multiple ports to a PIFO block to support multi-
ple operations every clock. These techniques are commonly
used in switches for background tasks such as reclaiming
buffer space, and can be applied to the PIFO mesh as well.
5. HARDWARE IMPLEMENTATION
This section describes the hardware implementation of
our programmable scheduler. We discuss performance re-
quirements (§5.1),
the implementation of a PIFO block
(§5.2), and the full-mesh interconnect between them (§5.3).
Finally, we estimate the area overhead of our design (§5.4).
5.1 Performance requirements
Our goal is a programmable scheduler competitive with
common shared-memory switches, such as the Broadcom
Trident II [3], used in many datacenters today. Based on the
Trident II, we target 1000 ﬂows that can be ﬂexibly allocated
across logical PIFOs and a 12 MByte packet buffer size [6]
with a cell size6 of 200 bytes. In the worst case, every packet
is a single cell. Hence, up to 60K packets/elements per PIFO
block can be spread out over multiple logical PIFOs.
Based on these requirements, our baseline design targets
a PIFO block that supports 64K packets and 1024 ﬂows that
can be shared across 256 logical PIFOs. Further, we target a
6Packets in a shared-memory switch are allocated in small
units called cells.
Figure 13: Block diagram of PIFO block with a ﬂow sched-
uler and a rank store. Logical PIFOs and metadata are not
shown for simplicity.
16-bit rank ﬁeld and a 32-bit metadata ﬁeld (e.g., p.length
in Figure 1) for our PIFO block. We put 5 such blocks to-
gether into a 5-block PIFO mesh that can support up to 5
levels of hierarchy in a scheduling algorithm—sufﬁcient for
most practical hierarchical schedulers we know of.
5.2 A single PIFO block
A PIFO block supports two operations: an enqueue that
inserts an element into a logical PIFO and a dequeue to re-
move the head of a logical PIFO. We ﬁrst describe an imple-
mentation of a PIFO block with a single logical PIFO and
then extend it to multiple logical PIFOs in the same block.
One naive implementation is a single sorted array. An
incoming element is compared against all array elements in
parallel to determine a location for the new element, and then
inserted there by shifting the array. However, each compar-
ison needs one comparator circuit, and supporting 64K of
these is infeasible.
At the same time, nearly all practical scheduling algo-
rithms group packets into ﬂows or classes,7 e.g., based on
trafﬁc type, ports, or addresses. They then schedule a ﬂow’s
packets in FIFO order because packet ranks increase across
a ﬂow’s consecutive packets. This motivates a design with
two parts (Figure 13):
1. A ﬂow scheduler that picks the element to dequeue
based on the rank of the head (earliest) elements of
each ﬂow. The ﬂow scheduler is effectively a PIFO
consisting of the head elements of all ﬂows.
2. A rank store, a FIFO bank that stores the ranks of ele-
ments beyond the head for each ﬂow in FIFO order.
This decomposition reduces the number of elements re-
quiring sorting from the number of packets (64K) to the
number of ﬂows (1024). During an enqueue, an element
(both rank and metadata) is appended to the end of the ap-
propriate FIFO in the rank store. For a ﬂow’s ﬁrst element,
we bypass the rank store and directly push it into the ﬂow
scheduler. To permit enqueues into this PIFO block, we also
supply a ﬂow ID argument to the enqueue operation.
The FIFO bank needed for the rank store is a well-
understood hardware design. Such FIFO banks are used to
buffer packet payloads in switches and much engineering ef-
fort has gone into optimizing them. As a result, we focus our
7Last-In First-Out (LIFO) is a counterexample. We can han-
dle LIFO by creating a new ﬂow for every packet, if there
are fewer than 1024 packets (ﬂows) in the buffer at any time.
2Rank Store(SRAM)ABDequeueEnqueueC68345524Increasing ranksFlow Scheduler(flip-flops)A0B1C3Increasing ranksimplementation effort on the ﬂow scheduler alone.
The Flow Scheduler. The ﬂow scheduler sorts an array of