title:A Reinforcement Learning Approach to Automatic Error Recovery
author:Qijun Zhu and
Chun Yuan
 A Reinforcement Learning Approach to Automatic Error Recovery 
Qijun Zhu 
Artificial Intelligence Lab 
       Tianjin University 
         Tianjin, China 
     PI:EMAIL 
Abstract 
The increasing complexity of modern computer sys-
tems  makes  fault  detection  and  localization  prohibi-
tively expensive, and therefore fast recovery from fail-
ures is becoming more and more important. A signifi-
cant fraction of failures can be cured by executing spe-
cific repair actions, e.g. rebooting, even when the exact 
root causes are unknown. However, designing reason-
able recovery policies to effectively schedule potential 
repair  actions  could  be  difficult  and  error  prone.  In 
this  paper,  we  present  a  novel  approach  to  automate 
recovery policy generation with Reinforcement Learn-
ing  techniques.  Based  on  the  recovery  history  of  the 
original  user-defined  policy,  our  method  can  learn  a 
new, locally optimal policy that outperforms the origi-
nal one. In our experimental work on data from a real 
cluster  environment,  we  found  that  the  automatically 
generated policy can save 10% of machine downtime. 
1. Introduction 
Maintaining  high  dependability  has  always  been  a 
critical topic for computer systems. Doing so usually is 
implemented  in  two  ways:  increasing  reliability  or 
availability.  Reliability  characterizes  the  ability  of  a 
system  to  perform  services  correctly,  which  can  be 
measured  by  the  meantime  between  failures  (MTBF). 
Availability  means  that  the  system  is  available  to  per-
form services, which can be characterized by the mean-
time to repair (MTTR). Despite great improvements in 
research  and  practice  in  software  engineering,  latent 
bugs in complex software systems persist, and often it 
is  just  too  difficult  to  improve  system  reliability  by 
recognizing  faults  or  fixing  bugs.  Actually,  as  the 
complexity of the software systems increases dramati-
cally,  analyzing  system  problems  and  finding  root 
causes  has  become  costly  and  time-consuming  work 
even  for  skilled  operators  and  diagnosticians  [13][18]. 
Making  computer  systems  more  consistently  available 
      Chun Yuan 
          Microsoft Research Asia 
No.49, Zhichun Road 
      Beijing, China 
PI:EMAIL 
is  indeed  practical  and  can  increase  effectiveness  and 
productivity.  
Traditional  fault  tolerant  techniques  rely  on  some 
form of redundancy to achieve high availability, which 
can  come  in  the  form  of  function  or  data  redundancy. 
However,  such  methods  usually  sacrifice  system  per-
formance  and  can  cause  high  hardware  costs  and  in-
crease complexity. For example, process pairs [3] util-
ize  good  processors  taking  over  the  functionality  of 
failed processors in which non-stop processing is at the 
cost  of  hardware  redundancy  and  performance.  Aura-
gen [4] also applies a similar scheme to the UNIX en-
vironment.  
Another  important  way  to  achieve  high  availability 
is  through  recovery  schemes  that  restore  systems  to  a 
valid  state  after  a  failure.  One  of  these  recovery 
schemes is based on check-pointing, which periodical-
ly  creates  a  valid  snapshot  of  a  system’s  state  and,  in 
the case of a failure, returns the system to a valid state. 
Often  this  method  is  system-specific  and  may  create 
great burdens on system designers and operators. Bak-
er et al. [1] utilized Recovery Box to realize quick re-
covery  in  which  operating  systems  and  application 
programs need to use the interface provided by Recov-
ery  Box  to  implement  data  insertion  and  retrieval. 
Moreover,  it  is  difficult  to  determine  the  right  time  to 
create a checkpoint and ensure its validity.  
A more popular recovery scheme is simple rebooting 
technique, which can be applied at various levels and is 
employed by many nontrivial systems today. Actually, 
a  significant  fraction  [5][10][14][22]  of  failures  are 
cured by simple recovery  mechanisms  such as reboot-
ing even when exact causes are unknown. Candea et al. 
[6]  built  crash-only  programs  to  crash  safely  and  re-
cover  quickly,  and  then  improved  this  approach  by 
introducing  a  fine-grained  mechanism  called  microre-
boot  [7]  which  can  provide  better  recovery  perfor-
mance and cause less disruption or downtime. 
However, to achieve efficient error recovery, poten-
tial  repair  actions  need  to  be  scheduled  reasonably 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007based on policies like state-action rules. An example of 
such  policies  includes  recursively  attempting  the  re-
maining  cheapest  action  [7].  This  simple  policy  may 
not  be  sufficient  in  real  environments  because  of  im-
precise  fault  localization,  recurring  failures,  or  failed 
repair  actions  [8].  The  overall  cost  of  cheap  actions, 
including  the  time  for  observing  recovery  effects,  is 
actually not that negligible either. Due to similar diffi-
culties in root cause analysis, as mentioned above, ge-
nerating  recovery  policies  automatically  could  be  im-
portant in effective error recovery. Joshi et al. [16][17] 
attempt  to  tackle  the  problem  with  a  model-based  ap-
proach  that  enables  automatic  recovery  in  distributed 
systems. Though their method works well in simulated 
experiments, there are still problems. First, the method 
needs detailed information on the system model, which 
is often too complex to obtain for large-scale systems. 
Second,  it  can  locate  faults  well  along  the  recovery 
process,  but  may  have  difficulties  in  determining  how 
to deal  with the faults since  some faults  may  need the 
combination of  several actions to complete a recovery 
in real systems.  
In this paper, we also utilize application-independent 
techniques  to  achieve  automatic  recovery.  However, 
we  are  focusing  on  recovery  policy  generation  when 
system  models  are  not  available.  To  the  best  of  our 
knowledge, this  has  not been  fully  studied before. We 
have  investigated  how  to  make  proper  decisions  on 
which  repair  action  to  choose  when  the  actual  root 
cause  is  only  localized  at  a  coarse  level.  Particularly, 
we  propose  a  novel  approach  based  on  reinforcement 
learning (RL) to automatically find the locally optimal 
policy,  and  show  that  it  can  achieve  better  recovery 
performance.  Another  benefit  of  our  learning-based 
approach is that it can adapt to the change of the envi-
ronment without human involvement. 
Our contributions are as follows:  
1.  An  offline  reinforcement  learning  method  to  au-
tomatically  generate  optimal  recovery  rules.  We 
should point out that the generated rules are local-
ly  optimal  since  the  learning  is  restricted  by  the 
original, user-defined rules to be optimized.  
2.  A  hybrid  approach  to  handle  noisy  states  that 
cannot  be  cured  by  generated  rules.  The  results 
show  that  our  approach  cannot  only  maintain 
nearly  the  same  performance  as  using  the  gener-
ated rules in isolation, but also can cover all poss-
ible states.  
3.  A  new  type-oriented  model  of  automatic  error 
recovery. Each rule corresponds to a potential er-
ror type induced from the recovery log.  
4.  Some  experience  in  reducing  rule-training  time. 
By  using  a  selection  tree,  we  can  guarantee  dis-
covery  of  optimal  rules  within  much  less  time 
than the standard RL process. 
The  rest  of  the  paper  is  organized  as  follows.  Sec-
tion 2 defines the automatic recovery problem and pro-
vides  an  overview  of  our  approach.  Section  3  gives 
additional details on the training  method, and presents 
some  assumptions  based  on  how  a  reasonable  evalua-
tion cab be conducted. Section 4 describes our experi-
mental  data  and  evaluation  framework.  Section  5 
presents  experimental  results.  Section  6  discusses  re-
lated work and Section 7 serves as our conclusion. 
2. Overview 
An automatic recovery framework typically consists 
of  three  functions:  event  monitoring,  fault  detection, 
and error recovery, as shown in the upper part of  Fig-
ure 1.  A recovery process  may run  like the  following: 
Event  monitoring  collects  various  information  and 
events  for  further  analysis,  such  as  symptoms  of  error 
states corresponding to different faults that occur in the 
target system. Then, fault detection recognizes failures 
and informs error recovery so that it can decide which 
repair action should be used based on the given recov-
ery policy and the  failure information. The chosen ac-
tion is applied to the corresponding component and the 
result  of  the  recovery  will  be  monitored,  which  may 
lead to another round of recovery. 
Figure 1: Automatic recovery framework. 
Usually,  recovery  policies  are  user-defined  by  sys-
tem  developers  or  operators.  The  issues  with  this  ap-
proach  are  manifold.  First,  policies  are  often  difficult 
to build and evaluate for large-scale, complex systems 
in  which  detailed  system  models  may  not  always  be 
available or up to date. Second, an ideal policy should 
be able to target each fault. However, due to the limita-
tion  of  fault  localization,  people  often  have  to  build 
coarse-grained  policies  to  cover  all  possible  error 
states.  This  sometimes  may  be  too  inaccurate  to  guar-
antee the desired result. Third, unanticipated errors and 
varying symptoms  may appear throughout the running 
of systems, which requires policies evolve over time. 
In  our  recovery  framework  we  have  two  additional 
offline components for automatically generating recov-
ery  policies,  as  shown  in  the  lower  part  of  Figure  1. 
Recovery  log  keep  a  history  of  error  recovery  via  the 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007event  monitoring  component.  Policy  generation  com-
ponents  learn recovery policies from the recovery  his-
tory  with  statistical  induction  techniques  to  instruct 
error  recovery.  Specifically,  we  use  reinforcement 
learning  to  generate  error  type-oriented  policies.  Our 
simulated  experiments  show  that  the  policies  learned 
by our method outperform manual ones.  
2.1. Problem formalization 
If  we  consider  the  recovery  process  as  selecting  a 
repair action according to current state and then getting 
a  reward  (e.g.  recovery  time)  after  taking  the  action, 
we  can  naturally  formalize  it  as  a  sequential  decision-
making  process,  or  particularly  a  Markov  decision 
process (MDP) [15]. 
A  Markov  decision  process  can  be  represented  as  a 
states, each of which consists of some related features; 
tuple ሺܵ,ܣ,ߜ,ܿ,ܵ଴ሻ, where ܵ is the set of possible error 
In  particular, ܵ଴ is  all  possible  starting  states; ܣ is  the 
available  repair  actions; ߜ is  the  state  transition  func-
tion,  which  decides  the  next  state ݏ௧ାଵ  based  on  the 
current state ݏ௧ and the selected action ܽ௧; ܿ is the cost 
function,  which  determines  the  cost  for  executing  an 
action under an error state. In our experiments, we use 
Meantime to Repair (MTTR) as the metric for evalua-
tion, so ܿ is based on recovery time (downtime). There-
fore our goal is to minimize the expected cost ܸ,  
ܸ ൌ ܧሾ෍ ܿሺݏ௧,ܽ௧ሻሿ                        ሺ1ሻ
that  is,  to  achieve  the  shortest  recovery  time.  We  will 
give more detailed explanation in Section 3.2. 
2.2. Reinforcement learning and Q-learning 
As  further  background  of  our  method,  we  give  a 
brief  introduction  to  reinforcement  learning,  an  unsu-
pervised learning  method for sequential decision  mak-
ing. In this learning paradigm, the learning agent rece-
ives  reinforcement  (reward)  after  each  action  execu-
tion. The objective of learning is to construct a control 
policy  so  as  to  minimize  the  discounted  cumulative 
reinforcement in the future or, for short, utility:  
ܸ௧ ൌ ෍ ߛ௞ܿ௧ା௞                       ሺ2ሻ
which  is  a  generalized  form  of  equation  (1). ߛ is  the 
discount factor. In this paper, we simply set it to 1.0 to 
make sure the expected cost is equal to MTTR. 
Q-learning  is  a  widely  used  reinforcement  learning 
algorithm.  The  idea  of  Q-learning  is  to  construct  an 
evaluation function called Q-function,  
ஶ
௞ୀ଴
ஶ
௧ୀ଴
ܳሺݏݐܽݐ݁,ܽܿݐ݅݋݊ሻ ՜ ݑݐ݈݅݅ݐݕ 
to predict the utility when the agent is executing some 
action  in  certain  state.  Given  an  optimal  Q-function 
and  a  state ݏ,  the  optimal  control  policy  is  simply  to 
choose  the  action ܽ such  that ܳሺݏ,ܽሻ is  minimal  over 
all actions. Often the Q-function can be represented in 
a generalized way like multi-layer neural networks and 
incrementally  learned  through  temporal  difference 
(TD)  methods  [23].  Given  a  sequence  of  state  transi-
tions,  the  Q-function  can  be  computed  by  iteratively 
applying the learning procedure to each two successive 
states  along  the  sequence.  Note  that  this  procedure  is 
actually  the  simplest  form  of  TD  methods, ܶܦሺ0ሻ . 
More  details  and  discussions  on  Q-learning  algorithm 
can be found in standard machine learning textbooks or 
related papers [20][21].  
2.3. Automated policy generation 
In this section, we will present the motivation for of-
fline training and a brief description of the policy gen-
eration process.  
2.3.1.  Offline  training.  There  are  a  few  issues  in  ap-
plying  reinforcement  learning  to  learn  recovery  poli-
cies online.  
1.  Before  finding  out  the  optimal  policy,  the  RL 
training  process  may  explore  many  bad  policies, 
which,  once  applied,  might  seriously  degrade 
normal system performance.  
2.  The  training  process  may  start  with  an  arbitrarily 
bad policy.  
3.  The training process requires tens of thousands of 
observations.  For  error  recovery,  several  years 
may be required to converge for infrequent errors.  
To  address  these  limitations,  we  devised  an  offline 
training  method  that  enables  RL  to  take  advantage  of 
user-defined policies. Although it is at the cost of miss-
ing the globally optimal policy and only producing the 
locally  optimal  one,  the  obvious  improvement  it  can 
bring  to  original  policies  and  the  avoidance  of  online 
training overhead still makes it a reasonable choice. 
2.3.2.  RL  approach.  We  use  the  error  types  induced 
from  failure  symptoms  to  approximate  the  real  faults. 
An  induced  error  type  represents  the  errors  that  share 
the  same  symptoms,  which  ideally  corresponds  to  a 
unique fault, though different faults may be inferred as 
the  same  error  type.  Specifically,  we  simply  use  the 
error types and the previously tried actions to form the 
states.  The  learning  algorithm  analyzes  a  real-world 
recovery  log  generated  by  a  user-defined  policy  and 
satisfies the following equation  
computes  the  value  of  the  Q-function ܳሺݏ,ܽሻ,  which 
ܳሺݏ,ܽሻ ൌ ܧሾܿሺݏ,ܽሻሿ ൅ ෍ܲሺݏᇱ|ݏ,ܽሻ݉݅݊௔ᇲܳሺݏᇱ,ܽᇱሻ
௦ᇲ
, where ݏᇱ ൌ ߜሺݏ,ܽሻ                            ሺ3ሻ  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:32:46 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Here,  the  Q-function ܳሺݏ,ܽሻ  stands  for  the  minimal 
time cost for state s beginning with action ܽ. The gen-
erated  recovery  policy  may  be  restricted  by  two  fac-
tors, the error types and the original recovery policy, so 
it can only achieve local optimum.  
3. Approach 
This section gives details on the RL approach to au-
tomatic recovery policy generation, and discusses some 
difficulties and our solutions. 
function Q-learning 
input 
return 
begin 
Pr   recovery processes (in the recovery log) 
Q    initial Q-function values 
updated Q-function values 
// select one recovery process from Pr 
p = SelectProcess(Pr) 
// induce error type based on recovery process 
t = InduceErrorType(p) 
// build initial state 
s = InitialState(t) 
// explore different recovery actions 
while(!Healthy(s)){ 
 a = SelectRecoveryAction(Q, s) 
 c = UpdateState(Pr, s, a, s’) 
 Record(s, a, c, s’) 
 s = s’ 
} 
// update Q-function values 
for every two successive states s, s’ in record 
  UpdateQfunction(Q, s, s’) 
return Q 
end 