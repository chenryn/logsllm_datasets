title:Seeing is not believing: visual verifications through liveness analysis
using mobile devices
author:Mahmudur Rahman and
Umut Topkara and
Bogdan Carbunar
Seeing is Not Believing: Visual Veriﬁcations Through
Liveness Analysis using Mobile Devices
Mahmudur Rahman
Florida International University
Miami, FL
mrahm004@cs.ﬁu.edu
Umut Topkara
IBM Research
Yorktown Heights, NY
PI:EMAIL
Bogdan Carbunar
Florida International University
Miami, FL
carbunar@cs.ﬁu.edu
ABSTRACT
The visual information captured with camera-equipped mo-
bile devices has greatly appreciated in value and importance
as a result of their ubiquitous and connected nature. Today,
banking customers expect to be able to deposit checks using
mobile devices, and broadcasting videos from camera phones
uploaded by unknown users is admissible on news networks.
We present Movee, a system that addresses the fundamental
question of whether the visual stream coming into a mobile
app from the camera of the device can be trusted to be un-
tampered with, live data, before it can be used for a variety
of purposes.
Movee is a novel approach to video liveness analysis for
mobile devices.
It is based on measuring the consistency
between the data from the accelerometer sensor and the in-
ferred motion from the captured video. Contrary to existing
algorithms, Movee has the unique strength of not depending
on the audio track. Our experiments on real user data have
shown that Movee achieves 8% Equal Error Rate.
1.
INTRODUCTION
In response to the ubiquitous and connected nature of mo-
bile devices, industries such as utilities, insurance, banking,
retail, and broadcast news have started to trust visual infor-
mation gleaned from or created using mobile devices. Mo-
bile apps utilize mobile device cameras for purposes varying
from authentication to location veriﬁcation, to tracking and
witnessing. Today, one can deposit a check using a mobile
phone, and videos from mobile phones uploaded by unknown
users are shown on broadcast news to a national audience.
We address the fundamental question of whether the vi-
sual stream that a mobile app receives from the camera of
the device can be trusted and has not been tampered with
by a malicious user attempting to game the system. We refer
to this problem as video “liveness” veriﬁcation. The practi-
cal attacks we consider are i) feeding a previously recorded
video through man in the middle software, ii) pointing the
camera to a replay of a video, and iii) pointing and moving
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
ACSAC ’13 Dec. 9-13, 2013, New Orleans, Louisiana USA
Copyright 2013 ACM 978-1-4503-2015-3/13/12 ...$15.00
http://dx.doi.org/10.1145/2523649.2523666.
the camera over a static photo. This problem is a corner-
stone in a variety of practical applications which use the
mobile device camera as a trusted witness, including citizen
journalism, smart cities, mobile authentication, and product
condition veriﬁcation for online sales (see Section 5).
In this paper, we propose to consult the motion sensors of
mobile devices in order to verify the “liveness” of the video
streams. We introduce Movee, a system that exploits the in-
herent movement of the user’s hand when shooting a video.
The veriﬁcation relies on the consistency between the in-
ferred motion from captured video and inertial sensor sig-
nals. Movee uses the intuition that being simultaneously
captured, these signals will necessarily bear certain relations,
that are diﬃcult to fabricate and emulate. In this case, the
movement of the scene in the video stream should have sim-
ilarities with the movement of the device that registers at
the motion sensors.
In essence, Movee provides CAPTCHA [42] like veriﬁca-
tions, by including the user, through her mobile device, into
the veriﬁcation process. However, instead of using the cog-
nitive strength of humans to interpret visual information,
we rely on their innately ﬂawed ability to hold a camera
still. Movee can also be viewed as a visual public notary
that stamps an untrusted video stream, with data simulta-
neously captured from a trusted sensor. This data can later
be used to verify the liveness of the video.
Previously, [25, 19, 18] have proposed to use audio streams
in captured videos as a means to protect against spooﬁng
in biometric authentication, by using static and dynamic
relations between voice and face information from speak-
ing faces. Others proposed video-based anti-spooﬁng meth-
ods [15], [14],
Instead,
we propose to use the previously unexplored combination of
video and accelerometer data to verify the liveness of the
video capture process.
[35] for biometric authentication.
Movee has four main modules. The Video Motion Anal-
ysis (VMA) module processes the video stream as it is cap-
tured by the camera. It uses video processing techniques to
infer the motion of the camera, producing a time-dependent
motion vector. VMA is inspired by the process used in im-
age stabilization capable cameras. Meanwhile, the Inertial
Sensor Motion Analysis (IMA) module converts the data
signal from the inertial sensors into another time-dependent
motion vector. When the video capture is completed, the
motion vectors from the VMA and IMA modules are com-
pared in the Similarity Computation (SC) module. SC relies
on a ﬂavor of the Dynamic Time Warping (DTW) algorithm
from speech and pattern recognition to compute the “simi-
239
larity” of the two motion vectors. The SC module also pro-
duces a set of features which summarize the nature of the
similarity. The features are used by the Classiﬁcation mod-
ule, which runs trained classiﬁers to decide whether the two
motion sequences corroborate each other. If they do, Movee
concludes the video is genuine.
The contributions of this work are the following.
• Introduce the “liveness” analysis problem to videos cap-
tured from mobile devices. Propose a suite of attacks
that enable the perpetrator to tamper with and claim
ownership of plagiarized media.
• Devise Movee, a lightweight liveness analysis solution
that veriﬁes the similarity of movement as inferred
from simultaneously captured video and inertial sensor
streams.
• Collect datasets of genuine and fraudulent video/inertial
sensor samples. Provide a full-ﬂedged implementation
of Movee, consisting of a mobile client and a server
component.
We have implemented Movee using a combination of An-
droid, for the mobile app, and C++/PHP for the processing
server. We have collected 100 genuine video/inertial sensor
samples from 10 diﬀerent users. We have used these samples
to create two test datasets, each containing video/inertial
sensor samples fabricated according to attacks against Movee.
Our cross-validation tests conducted on these test datasets
show that the accuracy of Movee in diﬀerentiating fraudu-
lent and genuine videos is 92% for one attack and 84% for
the other. Moreover, our implementation shows that the
liveness analysis of Movee is eﬃcient. The server, running
on a slightly outdated Dell laptop, takes an average of 1.3s
to analyze a 6s video.
2. SYSTEM MODEL
The system consists of a service provider (e.g., Vine [10]),
that oﬀers an interface for subscribers to upload videos they
shot on their mobile devices. We assume subscribers own
mobile devices equipped with a camera and inertial sensors
(i.e., accelerometers). Devices also have Internet connectiv-
ity, which, for the purpose of this work may be intermittent.
Each user is required to install an application on her mobile
device, which we henceforth denote as the “client”.
The client is used to capture videos that are later posted
to the provider hosted user account. The client simultane-
ously captures and uploads the video and the inertial sensor
streams from the device. The provider veriﬁes the authen-
ticity of the video by checking the consistency of the two
streams. The veriﬁcation is performed using limited infor-
mation: the two streams are from independent sources, but
have been captured at the same time on the same device.
In the remainder of the paper we use accelerometer and
inertial sensor interchangeably.
2.1 Attacker Model
We assume that the service provider is honest. Users
however can be malicious. As shown in the following, the
user can tamper with/copy video streams and inertial sen-
sor data. We assume that the inertial sensor and the data
extracted from the inertial sensor are genuine and have not
been tampered with. The goal of attackers is to fraudulently
claim ownership/creation of videos they upload. A malicious
user can launch several types of attacks on the video stream:
(cid:3)(cid:10)(cid:17)(cid:14)(cid:21)(cid:10)
(cid:2)(cid:12)(cid:12)(cid:14)(cid:16)(cid:14)(cid:21)(cid:19)(cid:17)(cid:14)(cid:23)(cid:14)(cid:21)
(cid:7)(cid:19)(cid:11)(cid:15)(cid:16)(cid:14)(cid:1)(cid:4)(cid:14)(cid:25)(cid:15)(cid:12)(cid:14)
(cid:9)(cid:15)(cid:13)(cid:14)(cid:19)(cid:1)(cid:5)(cid:21)(cid:10)(cid:17)(cid:14)(cid:22)
(cid:2)(cid:12)(cid:12)(cid:14)(cid:16)(cid:14)(cid:21)(cid:19)(cid:17)(cid:14)(cid:23)(cid:14)(cid:21)(cid:1)(cid:4)(cid:10)(cid:23)(cid:10)
(cid:9)(cid:15)(cid:13)(cid:14)(cid:19)(cid:1)(cid:7)(cid:19)(cid:23)(cid:15)(cid:19)(cid:18)(cid:1)
(cid:2)(cid:18)(cid:10)(cid:16)(cid:26)(cid:22)(cid:15)(cid:22)
(cid:6)(cid:18)(cid:14)(cid:21)(cid:23)(cid:15)(cid:10)(cid:16)(cid:1)(cid:8)(cid:14)(cid:18)(cid:22)(cid:19)(cid:21)(cid:1)
(cid:2)(cid:18)(cid:10)(cid:16)(cid:26)(cid:22)(cid:15)(cid:22)
(cid:8)(cid:15)(cid:17)(cid:15)(cid:16)(cid:10)(cid:21)(cid:15)(cid:23)(cid:26)(cid:1)
(cid:3)(cid:19)(cid:17)(cid:20)(cid:24)(cid:23)(cid:10)(cid:23)(cid:15)(cid:19)(cid:18)
(cid:3)(cid:16)(cid:10)(cid:22)(cid:22)(cid:15)(cid:27)(cid:12)(cid:10)(cid:23)(cid:15)(cid:19)(cid:18)
Figure 1: Movee uses four modules to verify a video
stream: Both i) Video Motion Analysis (VMA), and
ii) Inertial Sensor Motion Analysis (IMA), produce
movement estimations during capture, iii) Similarity
Computation extracts features, which iv) Classiﬁca-
tion uses to make the ﬁnal decision.
Copy-Paste attack. The attacker copies a video taken by
another user and uploads it as her own.
Replay attack. The attacker points the camera of the
device to a replay of a video.
Projection attack. The attacker points and moves the
camera of the device over a static photo or a projected image.
Random movement attack. Copy an existing video, then
move the device in random directions, allowing the capture
of inertial sensor data. Associate the video with the cap-
tured sensor stream and upload to the provider.
Direction sync attack. This is a sophisticated attack that
improves on the random movement attack. Speciﬁcally, the
attacker uses the device to emulate the movement observed
in the video, e.g., if the image moves to the right, the user
moves the device to the right.
3. MOVEE: SYSTEM OVERVIEW
We introduce Movee, a system that veriﬁes the authentic-
ity of a video taken with a mobile device. Movee performs
a liveness analysis based on the consistency of the inferred
motion from the simultaneously and independently captured
streams from the camera and the inertial sensors. If the data
from the inertial sensor corroborates the data from the cam-
era, the system concludes that the video was genuine: it has
been taken by the user pointing the camera to a real scene.
The Movee client is intended to be installed in mobile
devices as part of special purpose video capture apps. When
240
is a frame-by-frame displacement vector. However, it would
have been prohibitively expensive to compute the diﬀerences
between two frames for all possible pixel shifts, especially
considering how large each frame is. Phase Correlation [23]
allows us to ﬁnd the shift that minimizes the diﬀerence by
carrying the computation into the frequency domain.
Phase correlation is an image processing technique that
computes the spatial shift between two similar images (or
sub-images).
It is based on the Fourier shift property: a
shift in the spatial domain of two images results in a lin-
ear phase diﬀerence in the frequency domain of the Fourier
Transform (FT) [24]. It performs an element-wise multipli-
cation of the transform images, then computes the inverse
Fourier transform (IFT) of the result, and then ﬁnds the
shift that corresponds to the maximum amplitude, to yield
the resultant displacement. The maximum amplitude can
be deﬁned in the two-dimensional surface with delta func-
tions (colloquially referred to as peaks) at the positions cor-
responding to spatial shifts between the two images. Phase
correlation enables us to avoid the exhaustive search among
all possible pixel shifts of one of the video frames over the
next frame in order to ﬁnd the one shift amount that results
in minimal diﬀerence between the two frames. Instead, we
ﬁnd the location of the peak point in the Phase Correlation.
VMA ﬁrst retrieves the frame per second (fps) rate of the
stream and each available frame. In a pre-processing step,
it applies a Hamming window [40] ﬁlter to eliminate noise
from each frame. For each pair of consecutive frames, VMA
applies the phase correlation method to obtain linear shifts
between images in both X and Y directions. It then com-
putes the cumulative shift along the X and Y axes by adding
up the linear shifts for all consecutive frames retrieved from
that video. Let V Sx,i and V Sy,i denote the cumulative video
shifts of the i-th frame on the X and Y axes. We use (Sec-
tion 3.4) V Sx,i and V Sy,i as feature descriptors.
3.2 Inertial Sensor Motion Analysis (IMA)
The Inertial Sensor Motion Analysis (IMA) module (see
Figure 1) relies on the accelerometer sensor widely available
in mobile devices. The IMA processes the data from the
accelerometer in order to produce a motion direction and
magnitude which is then compared in the Similarity Com-
putation module with the output from the VMA module.
The inertial sensor coordinate system is deﬁned relative to
the screen of the phone in its default orientation. The X axis
is horizontal and points to the right, the Y axis is vertical
and points up and the Z axis points towards the outside of
the front face of the screen (coordinates behind the screen
have negative Z values). Let {(Ax,i, Ay,i, Az,i)|i = 1..m]}
denote the accelerometer trace, recorded every T seconds,
where (Ax,i, Ay,i, Az,i) is the i-th sample, containing ac-
celerometer readings on the three axes.
Filtering. In a pre-processing step, IMA uses a combina-
tion of low-pass and high-pass ﬁlters to remove the eﬀects of
gravity from the recorded raw acceleration values. Speciﬁ-
cally, for the low-pass ﬁlter, let Ga,i be the ﬁltered gravity
value on the a axis (a ∈ {X, Y, Z}) in the i-th sample and let
Ga,i+1 be the gravity value to be ﬁltered in the current, i+1-
th sample. Aa,i+1 is the acceleration reading on the a axis
for the i+1-th sample. Then, Ga,i+1 = αGa,i+(1−α)Aa,i+1,
∀a ∈ {x, y, z}. We have experimented with values of α rang-
ing between 0.6 and 0.95. In our experiments we have used
the value we found to perform best, α = 0.8.
Figure 2: The Video Motion Analysis module pro-
cesses each consecutive video frame and ﬁnds the
motion vector by computing the amount of displace-
ment that common image components have shifted
between two frames.
the user wants to capture a visual, be it a video or a photo,
two things happen simultaneously:
i) the camera turns on
and starts capturing video frames, ii) the inertial sensors are
turned on and start collecting a stream of data concurrent
with the camera.
Figure 1 shows a diagram of Movee. Movee infers the di-
rection of motion and the magnitude of motion from two
diﬀerent types of sensor data. The data from the camera
sensor is stored in periodically captured image frames. The
data from the inertial sensor in most mobile devices comes
in the form of periodically captured acceleration magnitudes
on 3 main axes as measured by the accelerometer. The Video
Motion Analysis (VMA) module uses an eﬃcient image pro-
cessing method to infer a motion vector over the timeline of
the video from frame-by-frame progress. The Inertial Sen-
sor Motion Analysis (IMA) module, converts the raw inertial
sensor readings into a motion vector over the same timeline.
Subsequently, the Similarity Computation (SC) module ex-
tracts features which represent agreements and diﬀerences
between the two motion data (from the VMA and IMA mod-
ules) for the same time period. The ﬁnal decision of whether
the captured video is genuine is made in the Classiﬁcation
module. The Classiﬁcation module uses trained classiﬁers
on the data produced by the SC module to ﬁnd out whether
the inertial sensor data corroborates the video sensor data.
In the rest of this section, we describe the details of the
four main modules of Movee.
3.1 Video Motion Analysis (VMA)
The Video Motion Analysis (VMA) module takes as input
the captured video stream and outputs an estimate for the
direction and magnitude of movement of the camera. The
output of VMA is then used by the Similarity Computation
module of Movee (see Figure 1).
It is possible to manually ﬁnd the movement of the camera
when given two consecutive photos taken with it: print the
photos on transparency ﬁlms and then shift one sheet on
the other and keep comparing the two prints until they line
up with minimal diﬀerence. The amount that the edges of
one sheet overhang the other represents the oﬀset between
the photos (see Figure 2). The common optical mice [6] use
this simple principle to determine pointer movement from a
stream of images taken with a low resolution optical sensor
mounted to their bottom side. The movement inferred from
this analysis will be limited to only two axes, i) horizontal
along the X axis, and ii) vertical along the Y axis.
VMA uses the shift and compare principle as well, by ap-
plying it on all consecutive frames of the video. The result
241
s
e
x
a
l
3
g
n
o
a
s
e
u
a
V
n
o
l
i
t
l
e
r
e
e
c