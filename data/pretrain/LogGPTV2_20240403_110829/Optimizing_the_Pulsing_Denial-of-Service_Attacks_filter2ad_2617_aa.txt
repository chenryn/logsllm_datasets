title:Optimizing the Pulsing Denial-of-Service Attacks
author:Xiapu Luo and
Rocky K. C. Chang
IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 1, FEBRUARY 2005
29
Defending Against Distributed Denial-of-Service
Attacks With Max-Min Fair Server-Centric
Router Throttles
David K. Y. Yau, Member, IEEE, John C. S. Lui, Feng Liang, and Yeung Yam
Abstract—Our work targets a network architecture and accom-
panying algorithms for countering distributed denial-of-service
(DDoS) attacks directed at an Internet server. The basic mech-
anism is for a server under stress to install a router throttle at
selected upstream routers. The throttle can be the leaky-bucket
rate at which a router can forward packets destined for the server.
Hence, before aggressive packets can converge to overwhelm
the server, participating routers proactively regulate the con-
tributing packet rates to more moderate levels, thus forestalling
an impending attack. In allocating the server capacity among
the routers, we propose a notion of level- max-min fairness.
We ﬁrst present a control-theoretic model to evaluate algorithm
convergence under a varitey of system parameters. In addition,
we present packet network simulation results using a realistic
global network topology, and various models of good user and at-
tacker distributions and behavior. Using a generator model of web
requests parameterized by empirical data, we also evaluate the
impact of throttling in protecting user access to a web server. First,
for aggressive attackers, the throttle mechanism is highly effective
in preferentially dropping attacker trafﬁc over good user trafﬁc.
In particular, level- max-min fairness gives better good-user
protection than recursive pushback of max-min fair rate limits
proposed in the literature. Second, throttling can regulate the
experienced server load to below its design limit – in the presence
of user dynamics – so that the server can remain operational
during a DDoS attack. Lastly, we present implementation results
of our prototype on a Pentium III/866 MHz machine. The results
show that router throttling has low deployment overhead in time
and memory.
Index Terms—Congestion control, distributed denial of service,
network security, router throttling.
Manuscript received December 27, 2002; revised March 22, 2003; approved
by IEEE/ACM TRANSACTIONS ON NETWORKING Editor V. Paxson. The work of
D. K. Y. Yau was supported in part by the National Science Foundation under
Grants CCR-9875742 (CAREER) and CNS-0305496, and in part by CERIAS.
The work of J. C. S. Lui was supported in part by an RGC Earmarked Grant.
The work of F. Liang was supported in part by CERIAS.
D. K. Y. Yau is with the Department of Computer Science, Purdue University,
West Lafayette, IN 47907 USA (e-mail: PI:EMAIL).
J. C. S. Lui
is with the Department of Computer Science and Engi-
neering, Chinese University of Hong Kong, Shatin, NT, Hong Kong (e-mail:
PI:EMAIL).
F. Liang is with the Department of Telecommunications, Zhejiang University
of Technology, Zhejiang, P. R. China.
Y. Yam is with the Department of Automation and Computer-aided Engi-
neering, Chinese University of Hong Kong, Shatin, NT, Hong Kong (e-mail:
PI:EMAIL).
Digital Object Identiﬁer 10.1109/TNET.2004.842221
I. INTRODUCTION
I N A DISTRIBUTED denial-of-service (DDoS) attack (e.g.,
[2], [3]), a cohort of malicious or compromised hosts (the
“zombies”) coordinate to send a large volume of aggregate
trafﬁc to a victim server. In such an episode, server resources
are usually under much more stress than resources of the
connecting network. There are two reasons. First, commercial
servers are typically hosted by ISP’s at web centers quite close
to the backbone network with high capacity pipes. Second,
the server will generally see the full force of the attack trafﬁc,
which has gone through aggregation inside the network. Hence,
the server system can become totally incapacitated under
extreme overload conditions.
We view DDoS attacks as a resource management problem.
Our goal in this paper is to protect a server system from having
to deal with excessive service request arrivals over a global
network. (It is possible to generalize the approach to protecting
an intermediate routing point under overload. However, im-
plementation issues of having routers initiate control requests
will then have to be addressed, which are not considered in the
current paper.) To do so, we adopt a proactive approach: Before
aggressive packets can converge to overwhelm a server, we
ask routers along forwarding paths to regulate the contributing
packet rates to more moderate levels, thus forestalling an im-
pending attack. The basic mechanism is for a server under
stress, say
, to install a router throttle at an upstream router
several hops away. The throttle limits the rate at which packets
will be forwarded by the router. Trafﬁc that
destined for
exceeds the rate limit can either be dropped or rerouted to
an alternate server, although we will focus exclusively on the
dropping solution in this paper.
exports its full service capacity
A key element in the proposed defense system is to install
appropriate throttling rates at the distributed routing points,
to
such that, globally,
the network, but no more. The “appropriate” throttles should
depend on the current demand distributions, and so must
be negotiated dynamically between server and network. Our
negotiation approach is server-initiated. A server operating
below the designed load limit needs no protection, and need
not install any router throttles. As server load increases and
, however, the server may
crosses the designed load limit
start to protect itself by installing and activating a rate throttle
at a subset of its upstream routers. After that, if the current
, then the
throttle fails to bring down the load at
to below
1063-6692/$20.00 © 2005 IEEE
30
IEEE/ACM TRANSACTIONS ON NETWORKING, VOL. 13, NO. 1, FEBRUARY 2005
(where
throttle rate is reduced.1 On the other hand, if the server load
), then the
falls below a low-water mark
throttle rate is increased (i.e., relaxed). If an increase does not
cause the load to signiﬁcantly increase over some observation
period, then the throttle is removed. The goal of the control
whenever
algorithm is to keep the server load within
a throttle is in effect.
Router throttling has been implemented on the CROSS/Linux
software router running on a Pentium III/864 MHz machine.
Our implementation results indicate that (i) since throttling re-
quires only looking up the IP destination address of a packet, it
has essentially the same processing complexity as standard IP
forwarding, and adds little computational overhead at a deploy-
ment router, and (ii) the amount of state information a router has
to keep per throttle is a few bytes, for storing the destination IP
address and the throttle value. Although throttling is space-efﬁ-
cient, the total amount of state information needed at a router is
nevertheless linear in the number of installed throttles. Hence,
it may not be possible for the routers to maintain state about
every Internet server. However, the approach can be feasible as
an on-demand and selective protection mechanism. The premise
is that DDoS attacks are the exception rather than the norm. At
any given time, we expect at most only a minor portion of the
network to be under attack, while the majority remaining por-
tion to be operating in “good health”. Moreover, rogue attackers
usually target “premium sites” with heavy customer utilization,
presumably to cause maximal user disruptions and to generate
the most publicity. These selected sites may then elect to protect
themselves in the proposed architecture, possibly by paying for
the offered services.
A. Our Contributions
Our contributions in this paper are:
• We contribute to the fundamental understanding of router
throttling as a mechanism against DDoS attacks. In par-
ticular, we advance a control-theoretic model useful for
understanding system behavior under a variety of param-
eters and operating conditions.
• We present an adaptive throttle algorithm that can ef-
fectively protect a server from resource overload, and
increase the ability of good user trafﬁc to arrive at the
intended server.
• We show how max-min fairness can be achieved across a
potentially large number of ﬂows, and the implication of
a notion of level- max-min fairness on DDoS attacks.
• We study how throttling may impact real application per-
formance. Speciﬁcally, we demonstrate via simulations
the performance impact on an HTTP web server.
• We present system implementation results to quantify the
deployment overhead of router throttling.
B. Paper Organization
The rest of this paper is organized as follows. In Sec-
tion II, we discuss the practical challenges of deploying router
throttling in the Internet. Our system model is introduced in
1Notice that reducing the throttle rate means increasing the extent of
throttling, because a router will restrict more trafﬁc destined for S.
Section III. In Section IV, we formally specify a baseline and
a fair algorithm for computing throttle rates. In Section V,
we present a control-theoretic mathematical model for under-
standing system performance under a variety of parameters and
operating conditions. To further examine system performance
under detailed packet network models, Section VI presents di-
verse ns2 simulation results using a realistic network topology.
Implementation of router throttling on the CROSS/Linux soft-
ware-programmable router, as well as its experimental evalu-
ation, is presented in Section VII. Section VIII compares our
solution approach with related work in the literature. Section IX
concludes.
II. DEPLOYMENT ISSUES
The objective of our work is to explore some fundamental is-
sues in mitigating DDoS attacks based on controlling aggressive
network attackers. We focus on the dynamic resource control
problem of giving good users productive access to a server’s re-
sources in spite of excessive demands from the attackers. We do
not claim to present a complete DDoS solution in the present
work. In particular, while our results are promising, several de-
ployment issues will have to be resolved to bring the solution
approach to bear in practice. These issues, discussed below, are
challenging and beyond the scope of this work.
First, our trust model is that routers in the defense network
trust each other, but they do not necessarily trust the network
users. In particular, these users may spoof packets, disobey
congestion signals, initiate bogus network requests, etc. As we
push the “defense perimeter” further away from the server to
be protected, requests to install router throttles are more likely
to cross multiple administrative domains. Establishing trust
relationships between the different domains, such that requests
originating from one domain will also be honored in the other
domains, is challenging and not addressed in the present work.
Second, our approach is most useful under the assumption that
attackers are signiﬁcantly more aggressive than regular users.
If the assumption is not true, good user trafﬁc can be penalized
to a comparable extent as attacker trafﬁc. Our solution is then
mainly useful in ensuring that a server under attack can remain
functional within the engineered load limits. However, it does
require more effort on the part of a malicious entity to assemble
a large number of attack machines each behaving as a regular
machine.
Third, since attackers can be highly unpredictable, it is in-
herently difﬁcult to exhaustively model attacker behavior using
only simulation experiments. In view of the problem, we have
developed an analytical model that allows us to more basically
and systematically study the behavior of our control strategy.
Our model brings forth several control parameters that will af-
fect system performance of stability and convergence speed.
Currently, these parameters must be chosen based on estimates
of the operating conditions and user policies to balance system
stability versus responsiveness. Adaptively and automatically
learning the best control parameters in a general setting is in-
teresting and requires further research.
Fourth, we assume that a protected server will send throttle re-
quests to deployment routers by multicast because it is the most
YAU et al.: DEFENDING AGAINST DISTRIBUTED DENIAL-OF-SERVICE ATTACKS WITH MAX-MIN FAIR SERVER-CENTRIC ROUTER THROTTLES
31
natural communication paradigm for our purpose. In practice,
we do not need full IP multicast support between routers. For
example, using topology information known to routers in an ISP,
routers can simply forward a throttle request to upstream routers
after incrementing a request hop count by one. Then routers in-
stall the throttle when the hop count parameter indicates that
they are in the deployment set. In this paper, we do not address
the full implementation details of such multicast support.
Fifth, our study assumes that router throttling is supported in a
speciﬁed set of deployment routers. This simpliﬁes the analysis
and experiments. If the assumption is not true, then we must be
able to identify at least one alternative supporting router on each
network path that sees substantial network trafﬁc. This will then
add the overhead of control message exchanges between routers
to identify supporting routers. Lastly, priority transmission tech-
niques should be investigated to ensure the reliable and timely
delivery of throttle messages from source to destination.
III. SYSTEM MODEL
We begin by stating Convention 1 that simpliﬁes our presenta-
tion throughout the rest of the paper. Then, we go on to describe
our system model.
We model a network as a connected graph
is the set of nodes and
Convention 1: All trafﬁc rate and server load quantities
stated in this paper are in units of kb/s, unless otherwise stated.
, where
is the set of edges. All leaf nodes
are hosts and thus can be a trafﬁc source. Hosts are not trusted.
In particular, they may spoof trafﬁc, disobey congestion signals,
initiate bogus network requests, etc. An internal node is a router;
a router cannot generate trafﬁc, but can forward trafﬁc received
the set
from its connected hosts or peer routers. We denote by
of internal routing nodes. All routers are assumed to be trusted.
The set of hosts,
, is partitioned into the set of ordi-
. models the
nary “good” users,
network links, which are assumed to be bi-directional. Since our
goal is to investigate control against server resource overload,
each link is assumed to have inﬁnite bandwidth. The assump-
tion can be relaxed if the control algorithm is also deployed to
protect routers from overload.
, and the set of attackers
In our control architecture, routers do not exchange control
information between each other beyond passing on throttle
requests (unlike, for example, traditional routing). This greatly
simpliﬁes the runtime overhead of our solution. Rather, the
target server makes all control decisions and then instructs the
deployment routers to implement the decisions accordingly.
(and we assume
. An attacker sends packets to
. In principle, while
In our study, we designate a leaf node in
. A good user sends packets to
as the target server
at some rate chosen from the
range
at some rate chosen
from the range
can usually be set
to a reasonable level according to how users normally access the
service at
), it is hard to prescribe
constraints on the choice of
. In this work, we target in partic-
ular the kind of attack in which
(although we will also examine system performance when such
a condition is not true). This is because if every attacker sends
at a rate comparable to a good user, then an attacker must recruit
or compromise a large number of hosts to launch an attack with
sufﬁcient trafﬁc volume.
is signiﬁcantly higher than
Fig. 1. Network topology illustrating R(3) deployment points of router
throttle, and offered trafﬁc rates.
When
. Speciﬁcally,
is under attack, it initiates the throttle defense mech-
anism outlined in Section I. The throttle does not have to be de-
ployed at every router in the network. Instead, the deployment
and are given
points are parameterized by a positive integer
by
contains all the routers that
are either
but are directly connected to a host.
hops away from or less than
hops away from
Fig. 1 shows an example network topology. In the ﬁgure, a
square node represents a host, while a round node represents a
router. The host on the far left is the target server
. The routers
in
are shaded in the ﬁgure. Notice that the bottom-most
router in
is only two hops away from , but is included
because it is directly connected to a host.
Given our system model, an important research problem is
how to achieve fair rate allocation of the server capacity among
the routers in
. To that end, we deﬁne the following notion
of level- max-min fairness:
Deﬁnition 1 (Level- Max-Min Fairness): A resource con-
trol algorithm achieves level- max-min fairness among the
routers
at
each router is the router’s max-min fair share of some rate
satisfying
, if the allowed forwarding rate of trafﬁc for
.