even in the quite active RISC-V community, no open and
properly working LLC designs are available that can be used
as foundation. Furthermore, for merely simulating such a de-
sign with a reasonably large cache, commercial EDA tools,
access to state-of-the-art technology libraries, and large mem-
ory macros with power models are required. As the result,
secure cache designs typically fail to deliver hardware imple-
mentation results (see Table 6 in [18]).
Because of these problems, similar to related work, we can
also not provide concrete numbers for the area and power
overhead. However, due to the way we designed SCATTER-
CACHE and the use of lightweight cryptographic primitives,
we can assert that the hardware overhead is reasonable. For ex-
ample, the 8-way SCv1 SCATTERCACHE with 512 kB that is
simulated in the following section, uses two parallel instances
of QARMA-64 with 5 rounds as IDF. One fully unrolled
instance has a size of 22.6 kGE [8] resulting in an IDF size
of less then 50 kGE even in case additional pipeline registers
are added. The added latency of such an IDF is the same as
the latency of the used primitive which has been reported as
2.2 ns. However, this latency can (partially or fully) be hidden
by computing the IDF in parallel to the lower level cache
lookup. Interestingly, with similar size, also a sponge-based
SCv1 IDF (e.g., 12 rounds of Keccak[200] [11]) can be instan-
tiated. Finally, there is always the option to develop custom
IDF primitives [55] that demand even less resources.
For comparison, in the BROOM chip [16], the SRAM
macros in the 1 MB L2 cache already consume roughly 50 %
of the 4.86 mm2 chip area. Assuming an utilization of 75 %
and a raw gate density of merely 3 MGate/mm2 [21] for the
used 28 nm TSMC process, these 2.43 mm2 already corre-
spond to 5.5 MGE. Subsequently, even strong IDFs are orders
of magnitude smaller than the size of a modern LLC.
In terms of overhead for the individual addressing of the
cache ways, information is more sparse. Spjuth et al. [64]
observed a 17 % energy consumption overhead for a 2-way
skewed cache. They also report that skewed caches can be
built with lower associativity and still reach similar perfor-
mance as traditional ﬁxed set-associative caches. Furthermore,
modern Intel architectures already feature multiple addressing
circuits in their LLC as they partition it into multiple smaller
caches (i.e., cache slices).
gem5 Results and Discussion
5.3
Figure 9 visualizes the cache hit rate of our L2 cache when
executing programs from the GAP benchmark suite. To ease
visualization, the results are plotted in percentage points (pp),
i.e., the differences between percentage numbers, using the
ﬁxed set-associative cache with random replacement policy
as baseline. All six algorithms (i.e., bc, bfs, cc, pr, sssp, tc)
have been evaluated. Moreover, as trace sets, both syntheti-
cally generated kron (-g16 -k16) and urand (-u16 -k16)
sets have been used. As can be seen in the graph, the BIP and
LRU replacement policies outperform random replacement
on average by 4.6 pp and 4 pp respectively. Interestingly, how-
ever, all random replacement based schemes, including the
skewed variants, perform basically identical.
688    28th USENIX Security Symposium
USENIX Association
BIP
LRU
SCv1
SCv2
Skewed
BIP
LRU Rand
SCv1
SCv2
Skewed
]
%
[
∆
e
t
a
R
t
i
H
)
r
e
t
t
e
b
s
i
r
e
h
g
i
h
(
5
0
bckron
bcurand
bfskron
bfsurand
cckron
ccurand
prkron
prurand
ssspkron
ssspurand
tckron
tcurand
mean
Figure 9: Cache hit rate, simulated with gem5, for the syn-
thetic workloads in the GAP benchmark suite with random
replacement policy as baseline.
BIP
LRU Rand
SCv1
SCv2
Skewed
]
%
[
e
t
a
R
t
i
H
)
r
e
t
t
e
b
s
i
r
e
h
g
i
h
(
2
8
.
8
9
4
6
.
8
9
7
7
.
7
9
7
7
.
7
9
8
9
.
7
9
9
.
8
9
6
.
1
9
2
.
2
9
2
6
.
9
8
8
8
.
8
8
4
8
.
8
8
2
2
.
7
8
4
1
.
3
7
9
0
.
2
7
9
0
.
8
6
2
9
.
8
6
4
8
.
8
6
1
.
9
6
4
1
.
0
3
3
1
.
0
3
2
.
0
3
7
8
.
9
1
2
8
.
9
1
5
3
.
0
2
6
5
.
5
1
5
5
.
5
1
9
5
.
5
1
1
8
.
2
5
7
.
2
3
4
.
3
Total
iTB walker dTB walker
Inst
Data
100
50
0
Figure 10: Cache hit rate, simulated with gem5, for scimark2.
The next benchmark, we visualized in Figure 10, is sci-
mark2 (-large 0.5). This benchmark shows an interesting
advantage of the skewed cache architectures over the ﬁxed-
set architectures, independent of the replacement policy, of
approximately 10 pp for the total hit rate. This difference is
mainly caused by the 5x difference in hit rate for data accesses.
Comparing the achieved benchmark scores in Figure 11 fur-
ther reveals that the fft test within scimark2 is the reason for
the observed discrepancy in cache performance.
To investigate this effect in more detail, we measured the
memory read latency using using lat_mem_rd 8M 32 from
lmbench in all cache conﬁgurations. The respective results
in Figure 12 feature two general steps in the read latency
at 32 kB (L1-cache size) and at 512 kB (L2-cache size). No-
tably, conﬁgurations with random replacement policy feature
a smoother transition at the second step, i.e., when accesses
start to hit main memory instead of the L2 cache.
Even more intersting results, as shown in Figure 13, have
been acquired by increasing the stride size to four times the
cache line size. Skewed caches like SCATTERCACHE break
the strong alignment of addresses and cache set indices. As
a consequence, a sparse, but strongly aligned memory ac-
cess pattern such as in lat_mem_rd, which in a standard
e
r
o
c
S
)
r
e
t
t
e
b
s
i
r
e
h
g
i
h
(
150
100
50
0
9
2
.
5
3
1
4
3
.
5
3
1
6
4
.
0
3
1
5
7
.
5
3
1
9