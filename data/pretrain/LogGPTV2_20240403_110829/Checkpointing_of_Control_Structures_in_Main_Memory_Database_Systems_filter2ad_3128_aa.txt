title:Checkpointing of Control Structures in Main Memory Database Systems
author:Long Wang and
Zbigniew Kalbarczyk and
Ravishankar K. Iyer and
H. Vora and
T. Chahande
Checkpointing of Control Structures in Main Memory Database Systems
L. Wang, Z. Kalbarczyk, R. K. Iyer
Center for Reliable and High-Performance Computing
Coordinated Science Laboratory
University of Illinois at Urbana-Champaign
1308 W. Main Street, Urbana, IL 61801
Email: {longwang, kalbar, iyer}@crhc.uiuc.edu
H. Vora, T. Chahande
Mascon IT Ltd.
1699 E. Woodfield Road,
Schaumburg, IL 60173
Email: {hvora, takshak}@masconit.com
Introduction
Abstract. This paper proposes an application-transparent,
low-overhead checkpointing strategy for maintaining consis-
tency of control structures in a commercial main memory
database (MMDB) system, based on the ARMOR (Adaptive
Reconfigurable Mobile Object of Reliability) infrastructure.
Performance measurements and availability estimates show
that
the proposed checkpointing scheme significantly en-
hances database availability (an extra nine in improvement
compared with major-recovery-based solutions) while incur-
ring only a small performance overhead (less than 2% in a
typical workload of real applications).
1
Main memory database (MMDB) systems store data perma-
nently in main memory, and applications can access the data
directly [5]. This offers high-speed access to shared data for
applications such as real-time billing, high-performance web
servers, etc. However, it also makes database systems highly
vulnerable to application errors/failures, as the database is
directly mapped into the application’s address space.
In addition to user data, the database maintains the control
structures (e.g., lock/mutex tables and file tables) necessary
for data operation. A database management system (DBMS)
maintains data integrity, including recovery in the case of an
error/failure of either applications or database services. How-
ever, the integrity of control structures is often not well main-
tained due to the less uniform interfaces to control structures
(compared with those to user data). As a result, errors in con-
trol structures can become a major cause of system downtime
and, hence, an availability bottleneck.
In this paper, we propose and evaluate an application-
transparent, low-overhead checkpointing strategy for main-
taining the consistency of control structures in a commercial
MMDB. The proposed solution is based on the ARMOR ar-
chitecture and an ARMOR runtime infrastructure [8], [18]. It
eliminates (or significantly reduces) cases requiring database
major recovery, a lengthy process that can take tens of sec-
onds and adversely impact availability. Importantly, the ap-
proach can be adapted relatively easily to other applications,
and the ARMOR runtime support creates a foundation for
providing system-wide error detection and recovery. This
work makes the following contributions:
• 
Introduction of a framework to provide support for
checkpointing of MMDB control structures.
•  Design and implementation of two checkpointing algo-
rithms. (i) Incremental checkpointing: (a) At runtime, a
post-transaction (upon transaction completion) state of
the control structure(s) accessed by each write transac-
tion (an update of the control structures) is collected and
(a) At
merged with the current checkpoint. (b) At recovery time,
the checkpoint is used directly to restore the correct state.
(ii) Delta checkpointing:
runtime, a pre-
transaction (before any updates occur) state of the con-
trol structure(s) accessed by a given transaction (both
write or read-only) is preserved as a current checkpoint
(delta). (b) At recovery time, the current state of control
structures in the shared memory is merged with the delta
checkpoint to restore the state.
•  Performance evaluation of the proposed checkpointing
algorithms. The data show that for a rather harsh work-
load of 60% write transactions, the performance over-
head varies in the range of 1% to 10%, depending on the
frequency of transactions.
•  Database availability estimation under different frequen-
cies of crashes that require major recovery. The data
show that under the error rate of one crash per week, a
checkpointing-based solution provides about five nines
of availability, one nine more than the baseline system.
Target System Overview
2
Target system. The target system in this study is a commer-
cial relational MMDB intended to support development of
high-performance, fault-resilient applications requiring con-
current access to shared data [2], [3]. The process accessing
the shared data can be either a client or a database service. A
service is a process that performs functions to assist the
proper processing of transactions. For example, the cleanup
service detects failures of connected clients/services and per-
forms recovery (including launching major recoveries).
In addition to supporting user data, the database supports
control structures (SysDB) necessary for correct operation.
Figure 1 depicts the example architecture of SysDB contain-
ing three tables: (i) the process table, which maintains proc-
ess ids and mutex lists for each process as well as information
on database mapping into the process address space, (ii) the
transaction table, which maintains logs and locks for active
transactions, and (iii) the file table, which keeps user database
files. Each client/service process maps SysDB into its own
address space before accessing the database.
Reliability problem. The error model we address in this paper
is the inconsistency of control structures due to the abnormal
termination (crash) of one of the clients or services1. Upon
such a crash, the target system denies services to all other
user processes and restarts the entire database system; this is
a major recovery. It may take tens of seconds, depending on
1 In the current implementation, we do not detect silent corruption of data,
i.e., incorrect data being written to the database.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:36:17 UTC from IEEE Xplore.  Restrictions apply. 
the size of the data files, and can significantly degrade system
availability (not acceptable for services provided to critical
applications). A major reason for these problems is the way
the database handles access to control structures in SysDB.
The system employs multiple mutexes to guarantee the mu-
tual exclusion semantic in accessing control structures by
user processes. When a client or service crashes while still
holding a mutex, the database may remain in an inconsistent
state. Since there is no way for the system to identify which
updates the crashed client has made, the cleanup process re-
starts the database to bring the system back into a consistent
state. Because major recovery imposes significant system
downtime, an approach is needed to eliminate or reduce cases
in which it is needed2.
Process 1
Mapinfo
Mapinfo
SysDB
SysDB
DB1
DB1
…
…
Process m
Mapinfo
Mapinfo
SysDB
SysDB
DB1
DB1
…
…
Process Table Transaction Table
File Table
Proc 1
Proc 1
…
…
Trans 1
Trans 1
…
…
File 1
File 1
…
…
Dirty Page
Tables
…
Mapinfo Mutex
…
lists
Logs Lock
table
…
SysDB
Figure 1: Example Control Structures (SysDB)
ARMOR High-Availability Infrastructure
3
The ARMOR infrastructure is designed to manage redundant
resources across interconnected nodes, detect errors in both
the user applications and the infrastructure components, and
recover quickly from failures when they occur. ARMORs
(Adaptive Reconfigurable Mobile Objects of Reliability) are
multithreaded processes internally structured around objects
called elements that contain their own private data and pro-
vide elementary functions or services. All ARMORs contain
a basic set of elements that provide a core functionality, in-
cluding the abilities to (i) implement reliable point-to-point
message communication between ARMORs, (ii) respond to
“Are-you-alive?” messages from the local daemon, and (iii)
capture ARMOR state.
ARMORs communicate solely through message-passing. The
ARMOR microkernel (present in each ARMOR process) is in
charge of distributing messages between elements within the
ARMOR and between the ARMORs present in the system. A
message consists of sequential operations that trigger element
actions. This modular, event-driven architecture permits the
ARMOR’s functionality and fault tolerance services to be
customized by choosing the particular set of elements that
make up the ARMOR. Several ARMOR processes constitute
the runtime environment, and each ARMOR plays a specific
role in the detection and recovery hierarchy offered to the
system and the application. The Fault Tolerance Manager
(FTM), Heart Beat ARMOR (HB), and Daemons are funda-
2 In this discussion, we consider only preserving the consistent state of con-
trol structures. Any inconsistency brought
to the user data is han-
dled/recovered by the default database services, such as two-phase commit,
checkpointing, and logging. As long as SysDB consistency is preserved, the
system can operate correctly and recover user data.
ARMOR-based Checkpointing
mental components of an ARMOR-based infrastructure. For
more details on ARMOR architecture the reader is referred to
[8], [17], [18].
4
Embedded ARMORs. In most of cases, an ARMOR launches
the application and monitors its behavior. The application is
treated as a black box, and only limited services can be pro-
vided by the ARMOR infrastructure, e.g., restart of the appli-
cation process. In the embedded ARMOR solution, an appli-
cation links the core structure of the ARMOR architecture
(the ARMOR microkernel) and uses the ARMOR API to
invoke/interface with the underlying element structure of an
embedded ARMOR. The embedded ARMOR process ap-
pears (i) as a full-fledged ARMOR to other ARMORs and (ii)
as a native application process to non-ARMOR processes
(e.g., database clients). As a result, the application can take
advantage of all services provided by ARMORs (e.g., adding
or removing elements to customize ARMOR functionality)
without having to change the original application’s organiza-
tion. In this way, the application does not need to be rewrit-
ten, and only lightweight instrumentation with a few AR-
MOR APIs is needed to embed the ARMOR-stub into the
application.
ARMOR-based checkpointing. In order to expose ARMOR
services, the database is instrumented in two ways: (i) AR-
MOR stubs are embedded in the database processes, facilitat-
ing communication channel(s) between the database server
and the ARMOR infrastructure. (ii) Functionalities are em-
bedded for checkpointing SysDB data structures; this modi-
fies selected library functions of the database but preserves
function interfaces and, hence, is transparent to clients.
Figure 2 illustrates the basic architecture of the ARMOR in-
frastructure integrated with the target database. The FTM,
FTM daemon, HB, and Daemon constitute the skeleton of the
ARMOR infrastructure. The solid lines are the ARMOR
communication channels. An ARMOR element called the
image keeper is embedded into the Daemon ARMOR to
maintain the image (checkpoint) of the data structures in
SysDB. When a client or service process opens the database,
the database kernel library creates an Embedded ARMOR
(EA) stub within the process and establishes the communica-
tion channel between the EA and the Daemon. From then on,
the checkpoint data can be transmitted directly from the
source process (with its EA stub) to the destination ARMOR,
which maintains the image in memory. The image is then
stored on disk by ARMOR’s checkpoint mechanism.
Client 1
EA
.
.
.
Client m
EA
FTM
Daemon
FTM
Daemon
Image
Keeper
HB
Service 1
(cleanup)
EA
Service 2
EA
.
.
.
Service n
EA
Figure 2: Basic Checkpointing Architecture
Arrows in Figure 2 depict the data flow during system opera-
tion. Each client or service, when it acquires a mutex (or re-
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:36:17 UTC from IEEE Xplore.  Restrictions apply. 
leases a mutex, depending on the checkpointing strategy ap-
plied), sends the related checkpoint through the ARMOR
communication channel
to the image keeper. The image
keeper processes the message according to the checkpointing
strategy. If there is no error, the checkpoint reflects the latest
consistent state of the SysDB. When a client/service crashes
while holding a mutex, the cleanup service requests from the
image keeper the saved correct copy of the relevant data
structure(s). On the successful restoration of the data, the
cleanup service allows the system to continue normal execu-
tion without invoking a major recovery.
5
This section discusses two algorithms for checkpointing con-
trol structures of the target database system:
incremental
checkpointing and delta checkpointing. We begin with brief
description, summarized in see Table 1, of similarities and
differences between the two proposed alternatives.
Checkpointing Algorithms
Incremental Checkpointing
5.1
In the incremental checkpointing scheme, only updates (in-
cremental changes) to data are sent to the image keeper. The
basic algorithm is as follows: