 query mode: prepared
 number of clients: 16
 number of threads: 4
 duration: 60 s
 number of transactions actually processed: 909280
 tps = 15151.787574 (including connections establishing)
 tps = 15155.568739 (excluding connections establishing)
 statement latencies in milliseconds:
 1.053459 select f_test1();
数据插入性能比对 MySQL , Oracle VS
PostgreSQL
数据库 硬件 TPS
Oracle DELL R910 稳定值2000,峰值2600
CPU : Intel(R) Xeon(R) CPU E7530 @ 1.87GHz 四路48线程
MEM : 32* 4G 128G
存储 : FusionIO 640G MLC
MySQL DELL R910 峰值1200*4，
Percona CPU : Intel(R) Xeon(R) CPU E7530 @ 1.87GHz 四路48线程 谷值0，
5.1.60-13.1 MEM : 32* 4G 128G 均值950*4
修改版 存储 : FusionIO 640G MLC
PostgreSQL IBM x3850 X5 稳定值24487
9.3.1 CPU : Intel(R) Xeon(R) CPU X7560 @ 2.27GHz 四路32线程
内存 : 8 * 8GB 64G
存储：OCZ RevoDrive3X2 480GB
PostgreSQL DELL R610 稳定值15151
9.3.1 CPU : Intel(R) Xeon(R) CPU E5504 @ 2.00GHz 2路8线程 (电源功率不够降频到1.6GHZ)
内存 : 12 * 8GB 96G
存储：OCZ RevoDrive3 240GB
数据插入性能比对 MySQL , Oracle VS
PostgreSQL
 PostgreSQL测试脚本
 CREATE OR REPLACE FUNCTION f_test1() RETURNS void
 LANGUAGE plpgsql
 STRICT
 AS $function$
 declare
 begin
 insert into t1(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t2(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t3(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t4(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t5(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t6(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t7(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t8(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t9(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t10(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t11(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 insert into t12(c1,c2,c3,c4,c5,c6,c7) values ('test1','test2','test3','test4','test5','test6','test7');
 return;
 exception when others then
 return;
 end;
 $function$;
数据插入性能比对 MySQL , Oracle VS
PostgreSQL
 PostgreSQL测试脚本
 create table t1 (id serial4 primary key, c1 text, c2 text, c3 text, c4 text, c5 text, c6 text, c7 text, c8 timestamp default now());
create table t2 (like t1 including all);
create table t3 (like t1 including all);
create table t4 (like t1 including all);
create table t5 (like t1 including all);
create table t6 (like t1 including all);
create table t7 (like t1 including all);
create table t8 (like t1 including all);
create table t9 (like t1 including all);
create table t10 (like t1 including all);
create table t11 (like t1 including all);
create table t12 (like t1 including all);
 vi test.sql
 select f_test1();
 pgbench -M prepared -n -r -f ./test.sql -c 16 -j 4 -T 600 -h $PGDATA -p 1921 -U postgres digoal
PostgreSQL VS MySQL 范围匹配查询
 MySQL使用场景
 CREATE TABLE ip_address_pool (
 id int(10) NOT NULL AUTO_INCREMENT COMMENT '自增主键',
 start_ip varchar(20) NOT NULL COMMENT '起始ip',
 end_ip varchar(20) NOT NULL COMMENT '截止ip',
 province varchar(128) NOT NULL COMMENT '省名',
 city varchar(128) NOT NULL COMMENT '城市',
 region_name varchar(128) NOT NULL COMMENT '地区名',
 company_name varchar(128) NOT NULL COMMENT '公司名',
 start_ip_decimal bigint(10) DEFAULT NULL,
 end_ip_decimal bigint(10) DEFAULT NULL,
 PRIMARY KEY (id),
 KEY idx_start_ip_Decimal (start_ip_decimal),
 KEY idx_end_ip_Decimal (end_ip_decimal)
 ) ENGINE=InnoDB AUTO_INCREMENT=436820 DEFAULT CHARSET=utf8 COMMENT='ip地址对应表';
PostgreSQL VS MySQL 范围匹配查询
 MySQL 中的查询, 范围匹配.
 select
 province,
 start_ip_Decimal as startIpDecimal,
 end_ip_Decimal as endIpDecimal
 from ip_address_pool
 where
 #{ip}>=start_ip_Decimal and
 #{ip} :ip::int8;
 pgbench -M prepared -c 8 -j 8 -f ./ip_test.sql -n -T 60 -h 127.0.0.1 -U postgres postgres
 QPS : 80171
大数据导入的优化案例
 http://blog.163.com/digoal@126/blog/static/163877040201392641033482/
 某运营商数据采集场景, 入库文件约300MB每个, 入库速度约100MB/s, 而直接拷贝文件的速度远远不止这个速度.
 测试场景 :
 8核 Intel(R) Xeon(R) CPU E5504 @ 2.00GHz (降频到1.6GHz)
 CentOS 6.4 x64, 硬盘 10K转SAS盘.
 PostgreSQL 9.3.1
 '--prefix=/home/pg93/pgsql9.3.1' '--with-pgport=1921' '--with-perl' '--with-tcl' '--with-python' '--with-openssl' '--with-pam' '--
without-ldap' '--with-libxml' '--with-libxslt' '--enable-thread-safety' '--with-wal-blocksize=16' '--enable-dtrace' '--enable-debug'
 测试表
 create table t (id int, c1 text, c2 text, c3 text, c4 text, c5 text, c6 text, c7 text, c8 text, c9 text, c10 text, c11 text, c12 text, c13
timestamp);
 测试数据
 insert into t select generate_series(1,610000), md5(random()::text), md5(random()::text), md5(random()::text),
md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text), md5(random()::text),
md5(random()::text), md5(random()::text), md5(random()::text), clock_timestamp();
 平均每个BLOCK存储18条记录. 平均每条记录455字节.
大数据导入的优化案例
 把数据拷贝到文件中, 用以模拟数据导入测试
 digoal=# copy t to '/home/pg93/t.dmp' with (header off);
 COPY 0 610000
 digoal=# \! ls -lh /home/pg93/t.dmp
 -rw-r--r-- 1 pg93 pg93 250M Oct 26 15:07 /home/pg93/t.dmp
 pgbench测试脚本
 pg93@db-172-16-3-150-> vi test.sql
 copy t from '/home/pg93/t.dmp' with (header off);
 测试, 8个连接, 每个连接执行4次.
 pgbench -M prepared -n -r -f ./test.sql -c 8 -j 4 -t 4
 每秒约导入91MB 或 22.3万条记录.
大数据导入的优化案例
 优化1
 更改为unlogged table.
 digoal=# update pg_class set relpersistence='u' where relname='t';
 digoal=# update pg_class set relpersistence='u' where relname='idx_t_id';
 每秒约导入106MB 或 25.8万条记录.
 提升不明显, 原因 :
 扩展block的锁为排他锁. (extend . ExclusiveLock )
 每次扩展1个块
 锁介绍可参考 : http://blog.163.com/digoal@126/blog/static/163877040201391674922879/
大数据导入的优化案例
 优化2 (仅针对锁的一种测试优化, 实际场景中使用不合适)
 保留测试表的最后一个数据块的最后一条记录(heap表的物理结构后面会讲到)
 因为回收垃圾只回收尾部的全空白BLOCK, 所以这样删除可以保留表的高水位.
 delete from t where ctid<>(select max(ctid) from t);
 更新fsm, 导入数据时不需要extend block.
 vacuum (freeze,verbose,analyze) t;
 checkpoint;
 每秒约导入188MB 或 45.77万条记录.
 提升较为明显.
大数据导入的优化案例
 优化3
 在优化2的基础上, 将硬盘更换为OCZ RevoDrive3 240G pci-e
 每秒约导入236MB 或 54.3万条记录.
 优化4
 删除索引, 纯粹的文本导入
 drop index idx_t_id;
 每秒约导入285.5MB 或 65.7万条记录.
 优化5, (优化2不切合实际,) 所以这里使用加大block size来减少extend block的次数.
 使用32KB的blocksize
 每秒约导入330MB 或 76万条记录.
 优化6, 采用32核的机器, 提高导入并行度.
 每秒约导入1097.3MB 或 267.6 万条记录(平均每条记录455字节).
从柱状图中快速读取需要大运算量的TOP数据
 http://blog.163.com/digoal@126/blog/static/1638770402013710105353862/
 创建测试表
 digoal=# create table test_1 (id serial4 primary key, info text, appid int, crt_time timestamp);
 digoal=# insert into test_1 (info,appid,crt_time) select md5(random()::text),round(10000*random())::int,clock_timestamp() from
generate_series(1,2000000);
 1. 从explain输出的信息中评估结果条数(存在一定偏差).
 可以省去实际执行SQL的开销.
 digoal=# select count(*) from test_1 where appid=1;
 189
 (1 row)
 digoal=# explain select * from test_1 where appid=1;
 QUERY PLAN
 -------------------------------------------------------------
 Seq Scan on test_1 (cost=0.00..45619.00 rows=197 width=49)
 Filter: (appid = 1)
 (2 rows)
从柱状图中快速读取需要大运算量的TOP数据
 digoal=# explain select * from test_1 where appid>1000;
 QUERY PLAN
 -----------------------------------------------------------------