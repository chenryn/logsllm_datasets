∀i.
(2)
Another option is to train one universal LSTM network with parameters θ(0)
using all data collected from all scenarios, and hope the trained universal LSTM
model can be used to predict bandwidth in all scenarios, i.e.,
universal:
ˆy(i) = LSTM
(cid:3)
x(i), θ(0)
(cid:4)
,
∀i.
(3)
The third option is to train a LSTM network using data from scenario i, then
use it to predict bandwidth in scenario j.
cross-scenario:
ˆy(j) = LSTM
(cid:3)
(cid:4)
,
x(j), θ(i)
i (cid:4)= j.
(4)
To generate training samples, we use a sliding-window based approach. For
example, to predict the bandwidth in the next second (m = 1) based on the
bandwidth measurements in the previous ﬁve seconds (n = 5), in the train-
ing, we use every consecutive six bandwidth measurements as one training data
point. The ﬁrst ﬁve seconds bandwidth form the input vector, and the sixth
second bandwidth is the output label. Likewise, for the general multiple sec-
onds prediction, i.e., predicting the future bandwidth for the next m seconds
based on the previous n seconds bandwidth, we use every consecutive n + m
bandwidth measurements as one data point. The ﬁrst n measurements form the
input vector, and the last m measurements form an output label vector.
4 Data Collection and Performance Evaluation
4.1 Datasets
It is critical to train and test LSTM models using large representative bandwidth
datasets. We ﬁrst used the HSDPA [7] dataset from the University of Oslo. It con-
sists of cellular bandwidth traces collected on diﬀerent transportation methods,
including Train, Tram, Ferry, Car, Bus and Metro. For each trace, it recorded the
bandwidth and location every 1,000 ms, and the duration for each trace ranges
from 500 to 1,000 s. However, we later found that the bandwidth traces are too
short for MSE analysis. We also collected long bandwidth traces in New York
City MTA bus and subway by ourselves. Figure 2 shows some sample routes for
our bandwidth collection, including Subway 7 Train, Subway Q Train, Bus B57
and B62. On each route, we conducted multiple experiments at diﬀerent time of
day. For each experiment, we connect a LTE mobile phone with unlimited data
plan to a remote server in our lab. We run iP erf and record TCP throughput
every 1,000 ms. All the bandwidth samples are logged on the server side. The
duration of each trace ranges from 10,000 to 20,000 s. It took us four months to
complete the ﬁrst batch of data. We are continuing this measurement campaign
and keep adding new traces to our NYU-METS Dataset for future research.
40
L. Mei et al.
(a) MTA Subway 7 Train
(b) MTA Subway Q Train
(c) Bus 57 Raw Trace
(d) MTA Bus 57
(e) MTA Bus 62
(f) Q Train Raw Trace
Fig. 2. New York City self-measured bandwidth
Table 1. Evaluation results on NYU-METS traces
Testset Average
RLS RMSE
RLS MAE
Harmonic RMSE
Harmonic MAE
LSTM RMSE
LSTM MAE
RLS RMSE Error Ratio
RLS MAE Error Ratio
HAR RMSE Error Ratio
HAR MAE Error Ratio
LSTM RMSE Error Ratio
LSTM MAE Error Ratio
7A Train 7B Train Bus 57 Bus 62 N Train
6.39
2.57
1.69
2.98
1.86
2.26
1.49
40.3%
26.5%
46.6%
29.1%
35.3%
23.3%
4.76
2.19
1.49
2.60
1.68
2.05
1.41
46.0%
31.3%
54.6%
35.4%
43.1%
29.6%
10.04
2.55
2.59
1.72
2.79
1.78
2.32
1.54
0.87
0.66
0.94
0.70
0.72
0.55
8.98
3.04
2.11
3.36
2.26
2.81
1.90
25.8% 34.2% 33.8%
17.1% 26.1% 23.5%
27.8% 37.0% 37.4%
17.7% 27.4% 25.2%
23.1% 28.2% 31.3%
15.3% 21.4% 21.2%
11.8% 21.2% 8.2%
Relative RMSE Impro over RLS
11.9% 21.6% 11.0%
Relative MAE Impro over RLS
Relative RMSE Impro over Harmonic 31.8% 26.7% 20.4% 31.1% 19.5%
Relative MAE Impro over Harmonic 24.9% 19.7% 15.8% 27.7% 18.9%
14.0% 6.7%
13.6% 5.9%
Realtime Mobile Bandwidth Prediction Using LSTM Neural Network
41
(a) Harmonic Mean
(b) RLS
(c) LSTM
Fig. 3. Harmonic Mean, RLS and LSTM predictions on Subway 7 Train
4.2 Next-Second Prediction
For the next-second prediction, the dimension of LSTM output is m = 1, and we
pick LSTM input dimension of n = 5 for evaluation. Figure 3 visually compares
the predicted values from Harmonic Mean, RLS and LSTM with the ground
truth for a trace collected on NYC Subway 7 Train. For LSTM training, we use
Adam optimizer [21] with default parameters (including learning rate, beta, etc)
in training. 80% of the trace is used for training, the rest 20% is used for testing.
We manually adjust dropout and epoch based on the performance of model.
We use the Root Mean Square Error (RMSE) and Mean Absolute Error
(MAE) between the predicted bandwidth and the ground truth as the main
accuracy measures. The complete prediction result of the three algorithms on
our NYU-METS Dataset is reported in Table 1. (LSTM runs in the per-scenario
mode). The unit is M bps. LSTM has the lowest RMSE and MAE cross all
mobility scenarios. The average accuracy improvement of LSTM over RLS and
Harmonic Mean in RMSE are 12.4% and 25.9% respectively, for MAE, these are
12.8% and 21.4% respectively. Since Harmonic Mean performs much worse than
the other two, in the following, we only compare LSTM with RLS.
Table 2 compares the accuracy of per-scenario LSTM with RLS on the
HSDPA dataset. The unit for the numbers is kbps. LSTM still outperforms
RLS in all mobility scenarios. The Relative Improvement of LSTM over RLS are
around 14.1% and 13.9% for RMSE and MAE respectively. For HSDPA dataset,
we also trained a universal LSTM model by using all traces from diﬀerent trans-
portation scenarios, including Bus, Tram, Train, Metro and Car, then test its
accuracy on individual transportation scenarios. However, it is performance is
inferior to the corresponding per-scenario models. For some scenarios, its per-
formance is even worse than RLS. Due to the space limit, we don’t report the
detailed statistics here. We defer the discussion on cross-scenario prediction to
the next section, and defer universal prediction to future investigation.
42
L. Mei et al.
Table 2. HSDPA traces evaluation result of LSTM and RLS
Ferry
FerryB Tram TramB Metro MetroB
Testset Average
248.4
217.6
118.8
133.4
RLS RMSE
RLS MAE
LSTM RMSE
LSTM MAE
71.3
53.1
60.8
45.6
88.9
58.5
80.4
50.1
35.3
25.5
31.5
23.3
35.6
26.6
30.2
22.3
96.0
34.2
25.8
29.2
23.2
119.7
35.5
26.9
32.5
24.3
28.7% 40.9% 29.8% 26.7% 35.7% 29.7%
RLS RMSE Error Ratio
21.4% 19.7% 21.5% 19.9% 26.7% 22.5%
RLS MAE Error Ratio
LSTM RMSE Error Ratio 24.5% 37.0% 26.6% 22.6% 30.4% 27.1%
18.4% 16.8% 19.6% 16.7% 24.3% 20.3%
LSTM MAE Error Ratio
Relative RMSE Impro
Relative MAE Impro
17.3% 10.6% 12.2% 17.8% 17.4% 9.3%
16.5% 16.9% 9.5% 19.3% 11.0% 10.6%
4.3 Multi-second Prediction
We now study the prediction accuracy for longer time intervals. For LSTM
model, we ﬁx the input vector dimension to be n = 10, and vary the output
vector dimension m from 2 to 5. In other words, LSTM network takes as input