Sensor Privacy. There are several parts to sen-
sor privacy: access control on sensors, sensor data
usage control once an application obtains access to
sensor data, and access visualization; we discuss re-
lated work for each.
Access control can take the form of user permis-
sions.
iOS’s permission system is to prompt a user
at the ﬁrst time of the sensor access (such as a
map application ﬁrst accessing GPS). Android and
latest Windows OSes use manifests at application
installation time to inform the user of sensor us-
age among other things; the installation proceeds
only if the user permits the application to perma-
nently access all the requested permissions. These
existing permission systems are either disruptive or
ask users’ permissions out-of-context. They are not
least-privilege; permanent access is often granted un-
necessarily. Felt et al [11] has shown that most peo-
ple ignore manifests, and the few who do read man-
ifests do not understand them. To address these is-
sues, access control gadgets (ACGs) [27] were intro-
duced to be trusted UI elements for sensors, which
are embeddable by applications; users’ authentic ac-
tions on an ACG (e.g., a camera trusted UI) grants
the embedding application permission to access the
represented sensor. In this paper, we argue that even
the ACG style of permission granting is too coarse-
grained for augmented reality systems because most
AR applications only require speciﬁc objects rather
than the entire RGB streams (Section 5.1).
Another form of access control
is to reduce
the sensitivity of private data (e.g., GPS coordi-
nates) available to applications. MockDroid [3]
and AppFence [14] allow using fake sensor data.
Krumm [19] surveys methods of reducing sensitive
information conveyed by location readings. Diﬀeren-
tial privacy [9] uses well-known methods for comput-
ing the amount of noise to add to give strong guar-
antees against an adversary’s ability to learn about
any speciﬁc individual. Similarly, we proposed mod-
ifying sensor inputs to recognizers in speciﬁc ways
to reduce false positives that could result in privacy
leaks. Darkly [18] transforms output from computer
vision algorithms (such as contours, moments, or
recognized objects) to blur the identity of the out-
put. Darkly can be applied to the output of our
recognizers.
Once an application obtains access to sensors, in-
formation ﬂow control approaches can be used to
control or monitor an application’s usage of the sen-
sitive data as in TaintDroid [10] and AppFence [14].
In access visualization, sensor-access widgets [15]
were proposed to reside within an application’s dis-
play with an animation to show sensor data being
collected by the application. Darkly [18] also gives
a visualization on its transforms (see above). Our
privacy goggles apply similar ideas to the AR envi-
ronment, allowing a user to visualize an application’s
eye view of the user’s world.
Abstractions for Privacy. Our notion of taking
raw sensor data and providing the higher-level ab-
straction of recognizers is similar to CondOS [4]’s
notion of Contextual Data Units. However, they nei-
ther choose a set of concrete Contextual Data Units
that are suitable for a wide variety of real-world ap-
plications nor address privacy concerns that arise
from applications having access to Contextual Data
Unit values. Koi [13] provides a location matching
abstraction to replace raw GPS coordinates in appli-
cations. The approach in Koi is limited to location
data and may require signiﬁcant work to integrate
into real applications, while our recognizers cover
many types of sensor data and were speciﬁcally cho-
sen to match application needs.
7 Future Work
Further Recognizer Visualization. The recog-
nizers we evaluated had straightforward visualiza-
428  22nd USENIX Security Symposium 
USENIX Association
14
tions, such as the Kinect skeleton. As we noted,
some recognizers, such as voice commands, do not
have obvious visualizations. Other recognizers might
extract features from raw video or audio for use by
a variety of object recognition algorithms, but not
in themselves have an easily understood semantics,
such as a fast Fourier transform of audio. One key
challenge here is to design visualizations for privacy
goggles that clearly communicate to users the impact
of allowing application access to the recognizer. For
example, with voice commands we might try show-
ing a video with sound where detected words are
highlighted with subtitles. A second key challenge
is characterizing the privacy impact of algorithmic
transforms on raw data, especially in the case of
computer vision features that have not been con-
sidered from a privacy perspective.
Third-Party Recognizers. All the recognizers in
this paper are assumed trusted. To enable new ex-
periences, we would like to support extension of the
platform with third-party recognizers. Supporting
third-party recognizers raises challenges, including
permissions for recognizers as well as sandboxing un-
trusted GPU code without sacriﬁcing performance.
We have developed recognizers in a domain-speciﬁc
language that enables precise analysis [8]. Dealing
with such challenges is intriguing future work, sim-
ilar in spirit to research on third-party driver isola-
tion in an OS. For example, we might require such
recognizers to go through a vetting program and
then have their code signed, similar to drivers in
Windows or applications on mobile phone platforms.
Sensing Applications. Besides traditional AR ap-
plications, other applications employ rich sensing
but do not necessarily render on human senses. For
example, robots today use the Kinect sensor for nav-
igating environment, and video conferencing can use
the “person texture” recognizer we describe. One of
our colleagues has also suggested that video confer-
encing can beneﬁt from a depth-limited camera [28].
These applications may also beneﬁt from recogniz-
ers.
Bystander Privacy. Our focus is on protecting
a user’s privacy against untrusted applications. Mo-
bile AR systems such as Google Glass, however, have
already raised signiﬁcant discussion of bystander pri-
vacy — the ability of people around the user to opt
out of recording and object recognition. Our archi-
tecture allows explicitly identifying all applications
that might have access to bystander information, but
it does not tell us when and how to stop sending rec-
ognizer events to applications. Making the system
aware of these issues is important future work.
8 Conclusions
We introduced a new abstraction, the recognizer, for
operating systems to support augmented reality ap-
plications. Recognizers allow applications to raise
the level of abstraction from raw sensor data, such
as audio and video streams, to ask for access to spe-
ciﬁc recognized objects. This enables applications
to act with the least privilege needed. Our analy-
sis of existing applications shows that all of them
would beneﬁt from least privilege enabled by an OS
with support for recognizers. We then introduced
a “privacy goggles” visualization for recognizers to
communicate the impact of allowing access to users.
Our surveys establish a clear privacy ordering on
core recognizers, show that users expect AR apps to
have limited capabilities, and demonstrate privacy
goggles are eﬀective at communicating capabilities
of apps that access recognizers. We built a prototype
on top of the Kinect for Windows SDK. Our imple-
mentation has negligible overhead for single appli-
cations, enables secure OS-level oﬄoading of heavy-
weight recognizer computation, and improves per-
formance for concurrent applications. In short, the
recognizer abstraction improves privacy and perfor-
mance for AR applications, laying the groundwork
for future OS support of rich sensing and AR appli-
cation rendering.
9 Acknowledgements
We thank Janice Tsai, our Privacy Manager, for re-
viewing our survey. We thank Doug Burger, Loris
D’Antoni, Yoshi Kohno, Franziska Roesner, Stuart
Schechter, Margus Veanes, and John Vilk for help-
ful discussions and review of drafts. Stuart Schechter
suggested the idea of a depth-limited camera for tele-
conferencing scenarios. This work was carried out
while the ﬁrst and fourth author were interning at
Microsoft Research.
References
[1] R. T. Azuma. A survey of augmented reality. Presence:
Teleoperators and Virtual Environments, 6(4):355–385,
August 1997.
[2] R. T. Azuma, Y. Baillot, R. Behringer, S. Feiner,
S. Julier, and B. MacIntyre. Recent advances in aug-
mented reality. Computer Graphics and Applications,
21(6):34–47, 2001.
[3] A. R. Beresford, A. Rice, N. Skehin, and R. Sohan.
MockDroid: Trading privacy for application functional-
ity on smartphones. In Workshop on Mobile Computing
Systems and Applications (HotMobile), 2011.
[4] D. Chu, A. Kansal, J. Liu, and F. Zhao. Mo-
It’s time to move up to condOS. May
bile apps:
USENIX Association  
22nd USENIX Security Symposium  429
15
ments using a webcam. IEEE Trans Biomed Engineer-
ing, 58(1):7–11, 2011.
[25] Project
Glass.
https://plus.google.com/
+projectglass/posts.
[26] Qualcomm.
SDK,
http://www.qualcomm.com/products_services/
augmented_reality.html.
Augmented Reality
2011.
[27] F. Roesner, T. Kohno, A. Moshchuk, B. Parno, H. J.
Wang, and C. Cowan. User-driven access control: Re-
thinking permission granting in modern operating sys-
tems.
In IEEE Symposium on Security and Privacy,
2011.
[28] S. Schecter. Depth-limited camera for skype - personal
communication, 2012.
[29] J. Shotton, A. Fitzgibbon, M. Cook, T. Sharp, M. Finoc-
chio, R. Moore, A. Kipman, and A. Blake. Real-time
human pose recognition in parts from a single depth im-
age. In Computer Vision and Pattern Recognition, June
2011.
[30] uSample.
Instant.ly survey creator, 2013.
http://
instant.ly.
[31] P. Viola and M. Jones. Robust Real-time Object De-
tection. In International Journal of Computer Vision,
2001.
2011.
default.aspx?id=147238.
http://research.microsoft.com/apps/pubs/
[5] M. Corporation. Kinect for xbox 360 privacy consid-
http://www.microsoft.com/privacy/
erations, 2012.
technologies/kinect.aspx.
[6] M. Corporation. Kinect for Windows SDK, 2013. http:
//www.microsoft.com/en-us/kinectforwindows/.
[7] L. D’Antoni, A. Dunn, S. Jana, T. Kohno, B. Livshits,
D. Molnar, A. Moshchuk, E. Ofek, F. Roesner,
S. Saponas, M. Veanes, and H. J. Wang. Operating sys-
tem support for augmented reality applications. In Hot
Topics in Operating Systems (HotOS), 2013.
FAST: A transducer-based language
[8] L. D’Antoni, M. Veanes, B. Livshits, and D. Mol-
nar.
for
tree manipulation, 2012. MSR Technical Report
2012-123 http://research.microsoft.com/apps/pubs/
default.aspx?id=179252.
[9] C. Dwork. The diﬀerential privacy frontier. In 6th The-
ory of Cryptography Conference (TCC), 2009.
[10] W. Enck, P. Gilbert, B.-G. Chun, L. P. Cox, J. Jung,
P. McDaniel, and A. N. Sheth.
TaintDroid: An
information-ﬂow tracking system for realtime privacy
monitoring on smartphones. In Conference on Operating
System Design and Implementation, 2010.
[11] A. P. Felt, E. Ha, S. Egelman, A. Haney, E. Chin, and
D. Wagner. Android permissions: User attention, com-
prehension, and behavior. In Symposium on Usable Pri-
vacy and Security (SOUPS), 2012.
[12] W. Garage. OpenCV, 2013. http://opencv.org/.
[13] S. Guha, M. Jain, and V. N. Padmanabhan. Koi: A
location-privacy platform for smartphone apps. In NSDI,
2012.
[14] P. Hornyack, S. Han, J. Jung, S. Schechter, and
D. Wetherall. These aren’t the droids you’re looking
for: retroﬁtting android to protect data from imperious
applications. In Conference on Computer and Commu-
nications Security, 2011.
[15] J. Howell and S. Schechter. What You See is What
They Get: Protecting users from unwanted use of micro-
phones, cameras, and other sensors. In Web 2.0 Security
and Privacy, IEEE, 2010.
[16] J. Howell and S. Schecter. What you see is what they
get: Protecting users from unwanted use of microphones,
cameras, and other sensors.
In Web 2.0 Security and
Privacy Workshop, 2010.
[17] E. Hutchings.
see
how new furniture would
pers
home,
augmented-reality-furniture-app.html.
2012.
Augmented reality
shop-
at
http://www.psfk.com/2012/05/
look
lets
[18] S. Jana, A. Narayanan, and V. Shmatikov. DARKLY:
Privacy for perceptual applications. In IEEE Symposium
on Security and Privacy, 2013.
[19] J. Krumm. A survey of computational location pri-
vacy. Personal Ubiquitous Computing, 13(6):391–399,
Aug 2009.
[20] Layar. Layar catalogue, 2013. http://www.layar.com/
layers.
[21] Microsoft Research Face SDK Beta. http://research.
microsoft.com/en-us/projects/facesdk/.
[22] Microsoft Speech Platform.
http://msdn.microsoft.
com/en-us/library/hh361572(v=office.14).aspx.
[23] R. A. Newcombe, S. Izadi, O. Hilliges, D. Molyneaux,
D. Kim, A. J. Davison, P. Kohli, J. Shotton, S. Hodges,
and A. Fitzgibbon. KinectFusion: Real-time dense sur-
face mapping and tracking. In 10th IEEE International
Symposium on Mixed and Augmented Reality, 2011.
[24] M. Poh, D. MacDuﬀ, and R. Picard. Advancements
in non-contact, multiparameter physiological measure-
430  22nd USENIX Security Symposium 
USENIX Association
16