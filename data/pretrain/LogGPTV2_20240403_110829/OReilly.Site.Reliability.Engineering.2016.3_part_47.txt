Example action items
• Make sure that your client backs off exponentially on failure.
• Make sure that you jitter automatic requests.
Processes and Automation
Google encourages engineers to use standard tools to automate common processes.
However, automation is never perfect, and every service has processes that need to be
executed by a human: creating a new release, moving the service to a different data
center, restoring data from backups, and so on. For reliability reasons, we strive to
minimize single points of failure, which include humans.
These remaining processes should be documented before launch to ensure that the
information is translated from an engineer’s mind onto paper while it is still fresh,
and that it is available in an emergency. Processes should be documented in such a
way that any team member can execute a given process in an emergency.
Example checklist question
• Are there any manual processes required to keep the service running?
Example action items
• Document all manual processes.
• Document the process for moving your service to a new datacenter.
• Automate the process for building and releasing a new version.
Development Process
Google is an extensive user of version control, and almost all development processes
are deeply integrated with the version control system. Many of our best practices
378 | Chapter 27: Reliable Product Launches at Scale
revolve around how to use the version control system effectively. For example, we
perform most development on the mainline branch, but releases are built on separate
branches per release. This setup makes it easy to fix bugs in a release without pulling
in unrelated changes from the mainline.
Google also uses version control for other purposes, such as storing configuration
files. Many of the advantages of version control—history tracking, attributing
changes to individuals, and code reviews—apply to configuration files as well. In
some cases, we also propagate changes from the version control system to the live
servers automatically, so that an engineer only needs to submit a change to make it go
live.
Example action items
• Check all code and configuration files into the version control system.
• Cut each release on a new release branch.
External Dependencies
Sometimes a launch depends on factors beyond company control. Identifying these
factors allows you to mitigate the unpredictability they entail. For instance, the
dependency may be a code library maintained by third parties, or a service or data
provided by another company. When a vendor outage, bug, systematic error, security
issue, or unexpected scalability limit actually occurs, prior planning will enable you to
avert or mitigate damage to your users. In Google’s history of launches, we’ve used
filtering and/or rewriting proxies, data transcoding pipelines, and caches to mitigate
some of these risks.
Example checklist questions
• What third-party code, data, services, or events does the service or the launch
depend upon?
• Do any partners depend on your service? If so, do they need to be notified of
your launch?
• What happens if you or the vendor can’t meet a hard launch deadline?
Rollout Planning
In large distributed systems, few events happen instantaneously. For reasons of relia‐
bility, such immediacy isn’t usually ideal anyway. A complicated launch might require
enabling individual features on a number of different subsystems, and each of those
configuration changes might take hours to complete. Having a working configuration
Developing a Launch Checklist | 379
in a test instance doesn’t guarantee that the same configuration can be rolled out to
the live instance. Sometimes a complicated dance or special functionality is required
to make all components launch cleanly and in the correct order.
External requirements from teams like marketing and PR might add further compli‐
cations. For example, a team might need a feature to be available in time for the key‐
note at a conference, but need to keep the feature invisible before the keynote.
Contingency measures are another part of rollout planning. What if you don’t man‐
age to enable the feature in time for the keynote? Sometimes these contingency meas‐
ures are as simple as preparing a backup slide deck that says, “We will be launching
this feature over the next days” rather than “We have launched this feature.”
Example action items
• Set up a launch plan that identifies actions to take to launch the service. Identify
who is responsible for each item.
• Identify risk in the individual launch steps and implement contingency measures.
Selected Techniques for Reliable Launches
As described in other parts of this book, Google has developed a number of techni‐
ques for running reliable systems over the years. Some of these techniques are partic‐
ularly well suited to launching products safely. They also provide advantages during
regular operation of the service, but it’s particularly important to get them right dur‐
ing the launch phase.
Gradual and Staged Rollouts
One adage of system administration is “never change a running system.” Any change
represents risk, and risk should be minimized in order to assure reliability of a sys‐
tem. What’s true for any small system is doubly true for highly replicated, globally
distributed systems like those run by Google.
Very few launches at Google are of the “push-button” variety, in which we launch a
new product at a specific time for the entire world to use. Over time, Google has
developed a number of patterns that allow us to launch products and features gradu‐
ally and thereby minimize risk; see Appendix B.
Almost all updates to Google’s services proceed gradually, according to a defined pro‐
cess, with appropriate verification steps interspersed. A new server might be installed
on a few machines in one datacenter and observed for a defined period of time. If all
looks well, the server is installed on all machines in one datacenter, observed again,
and then installed on all machines globally. The first stages of a rollout are usually
380 | Chapter 27: Reliable Product Launches at Scale
called “canaries”—an allusion to canaries carried by miners into a coal mine to detect
dangerous gases. Our canary servers detect dangerous effects from the behavior of
the new software under real user traffic.
Canary testing is a concept embedded into many of Google’s internal tools used to
make automated changes, as well as for systems that change configuration files. Tools
that manage the installation of new software typically observe the newly started
server for a while, making sure that the server doesn’t crash or otherwise misbehave.
If the change doesn’t pass the validation period, it’s automatically rolled back.
The concept of gradual rollouts even applies to software that does not run on Google’s
servers. New versions of an Android app can be rolled out in a gradual manner, in
which the updated version is offered to a subset of the installs for upgrade. The per‐
centage of upgraded instances gradually increases over time until it reaches 100%.
This type of rollout is particularly helpful if the new version results in additional traf‐
fic to the backend servers in Google’s datacenters. This way, we can observe the effect
on our servers as we gradually roll out the new version and detect problems early.
The invite system is another type of gradual rollout. Frequently, rather than allowing
free signups to a new service, only a limited number of users are allowed to sign up
per day. Rate-limited signups are often coupled with an invite system, in which a user
can send a limited number of invites to friends.
Feature Flag Frameworks
Google often augments prelaunch testing with strategies that mitigate the risk of an
outage. A mechanism to roll out changes slowly, allowing for observation of total sys‐
tem behavior under real workloads, can pay for its engineering investment in reliabil‐
ity, engineering velocity, and time to market. These mechanisms have proven
particularly useful in cases where realistic test environments are impractical, or for
particularly complex launches for which the effects can be hard to predict.
Furthermore, not all changes are equal. Sometimes you simply want to check whether
a small tweak to the user interface improves the experience of your users. Such small
changes shouldn’t involve thousands of lines of code or a heavyweight launch process.
You may want to test hundreds of such changes in parallel.
Finally, sometimes you want to find out whether a small sample of users like using an
early prototype of a new, hard-to-implement feature. You don’t want to spend months
of engineering effort to harden a new feature to serve millions of users, only to find
that the feature is a flop.
To accommodate the preceding scenarios, several Google products devised feature
flag frameworks. Some of those frameworks were designed to roll out new features
gradually from 0% to 100% of users. Whenever a product introduced any such frame‐
work, the framework itself was hardened as much as possible so that most of its appli‐
Selected Techniques for Reliable Launches | 381
cations would not need any LCE involvement. Such frameworks usually meet the
following requirements:
• Roll out many changes in parallel, each to a few servers, users, entities, or
datacenters
• Gradually increase to a larger but limited group of users, usually between 1 and
10 percent
• Direct traffic through different servers depending on users, sessions, objects,
and/or locations
• Automatically handle failure of the new code paths by design, without affecting
users
• Independently revert each such change immediately in the event of serious bugs
or side effects
• Measure the extent to which each change improves the user experience
Google’s feature flag frameworks fall into two general classes:
• Those that primarily facilitate user interface improvements
• Those that support arbitrary server-side and business logic changes
The simplest feature flag framework for user interface changes in a stateless service is
an HTTP payload rewriter at frontend application servers, limited to a subset of
cookies or another similar HTTP request/response attribute. A configuration mecha‐
nism may specify an identifier associated with the new code paths and the scope of
the change (e.g., cookie hash mod range), whitelists, and blacklists.
Stateful services tend to limit feature flags to a subset of unique logged-in user identi‐
fiers or to the actual product entities accessed, such as the ID of documents, spread‐
sheets, or storage objects. Rather than rewrite HTTP payloads, stateful services are
more likely to proxy or reroute requests to different servers depending on the change,
conferring the ability to test improved business logic and more complex new features.
Dealing with Abusive Client Behavior
The simplest example of abusive client behavior is a misjudgment of update rates. A
new client that syncs every 60 seconds, as opposed to every 600 seconds, causes 10
times the load on the service. Retry behavior has a number of pitfalls that affect user-
initiated requests, as well as client-initiated requests. Take the example of a service
that is overloaded and is therefore failing some requests: if the clients retry the failed
requests, they add load to an already overloaded service, resulting in more retries and
even more requests. Instead, clients need to reduce the frequency of retries, usually by
adding exponentially increasing delay between retries, in addition to carefully consid‐
382 | Chapter 27: Reliable Product Launches at Scale
ering the types of errors that warrant a retry. For example, a network error usually
warrants a retry, but a 4xx HTTP error (which indicates an error on the client’s side)
usually does not.
Intentional or inadvertent synchronization of automated requests in a thundering
herd (much like those described in Chapters 24 and 25) is another common example
of abusive client behavior. A phone app developer might decide that 2 a.m. is a good
time to download updates, because the user is most likely asleep and won’t be incon‐
venienced by the download. However, such a design results in a barrage of requests to
the download server at 2 a.m. every night, and almost no requests at any other time.
Instead, every client should choose the time for this type of request randomly.
Randomness also needs to be injected into other periodic processes. To return to the
previously mentioned retries: let’s take the example of a client that sends a request,
and when it encounters a failure, retries after 1 second, then 2 seconds, then 4 sec‐
onds, and so on. Without randomness, a brief request spike that leads to an increased
error rate could repeat itself due to retries after 1 second, then 2 seconds, then 4 sec‐
onds. In order to even out these synchronized events, each delay needs to be jittered
(that is, adjusted by a random amount).
The ability to control the behavior of a client from the server side has proven an
important tool in the past. For an app on a device, such control might mean instruct‐
ing the client to check in periodically with the server and download a configuration
file. The file might enable or disable certain features or set parameters, such as how
often the client syncs or how often it retries.
The client configuration might even enable completely new user-facing functionality.
By hosting the code that supports new functionality in the client application before
we activate that feature, we greatly reduce the risk associated with a launch. Releasing
a new version becomes much easier if we don’t need to maintain parallel release
tracks for a version with the new functionality versus without the functionality. This
holds particularly true if we’re not dealing with a single piece of new functionality, but
a set of independent features that might be released on different schedules, which
would necessitate maintaining a combinatorial explosion of different versions.
Having this sort of dormant functionality also makes aborting launches easier when
adverse effects are discovered during a rollout. In such cases, we can simply switch
the feature off, iterate, and release an updated version of the app. Without this type of
client configuration, we would have to provide a new version of the app without the
feature, and update the app on all users’ phones.
Overload Behavior and Load Tests
Overload situations are a particularly complex failure mode, and therefore deserve
additional attention. Runaway success is usually the most welcome cause of overload
Selected Techniques for Reliable Launches | 383
when a new service launches, but there are myriad other causes, including load bal‐
ancing failures, machine outages, synchronized client behavior, and external attacks.
A naive model assumes that CPU usage on a machine providing a particular service
scales linearly with the load (for example, number of requests or amount of data pro‐
cessed), and once available CPU is exhausted, processing simply becomes slower.
Unfortunately, services rarely behave in this ideal fashion in the real world. Many
services are much slower when they are not loaded, usually due to the effect of vari‐
ous kinds of caches such as CPU caches, JIT caches, and service-specific data caches.
As load increases, there is usually a window in which CPU usage and load on the ser‐
vice correspond linearly, and response times stay mostly constant.
At some point, many services reach a point of nonlinearity as they approach overload.
In the most benign cases, response times simply begin to increase, resulting in a
degraded user experience but not necessarily causing an outage (although a slow
dependency might cause user-visible errors up the stack, due to exceeded RPC dead‐
lines). In the most drastic cases, a service locks up completely in response to
overload.
To cite a specific example of overload behavior: a service logged debugging informa‐
tion in response to backend errors. It turned out that logging debugging information
was more expensive than handling the backend response in a normal case. Therefore,
as the service became overloaded and timed out backend responses inside its own
RPC stack, the service spent even more CPU time logging these responses, timing out
more requests in the meantime until the service ground to a complete halt. In serv‐
ices running on the Java Virtual Machine (JVM), a similar effect of grinding to a halt
is sometimes called “GC (garbage collection) thrashing.” In this scenario, the virtual
machine’s internal memory management runs in increasingly closer cycles, trying to
free up memory until most of the CPU time is consumed by memory management.
Unfortunately, it is very hard to predict from first principles how a service will react
to overload. Therefore, load tests are an invaluable tool, both for reliability reasons
and capacity planning, and load testing is required for most launches.
Development of LCE
In Google’s formative years, the size of the engineering team doubled every year for
several years in a row, fragmenting the engineering department into many small
teams working on many experimental new products and features. In such a climate,
novice engineers run the risk of repeating the mistakes of their predecessors, espe‐
cially when it comes to launching new features and products successfully.
To mitigate the repetition of such mistakes by capturing the lessons learned from past
launches, a small band of experienced engineers, called the “Launch Engineers,” vol‐
384 | Chapter 27: Reliable Product Launches at Scale
unteered to act as a consulting team. The Launch Engineers developed checklists for
new product launches, covering topics such as:
• When to consult with the legal department
• How to select domain names
• How to register new domains without misconfiguring DNS
• Common engineering design and production deployment pitfalls
“Launch Reviews,” as the Launch Engineers’ consulting sessions came to be called,
became a common practice days to weeks before the launch of many new products.
Within two years, the product deployment requirements in the launch checklist grew
long and complex. Combined with the increasing complexity of Google’s deployment
environment, it became more and more challenging for product engineers to stay up-
to-date on how to make changes safely. At the same time, the SRE organization was
growing quickly, and inexperienced SREs were sometimes overly cautious and averse
to change. Google ran a risk that the resulting negotiations between these two parties
would reduce the velocity of product/feature launches.
To mitigate this scenario from the engineering perspective, SRE staffed a small, full-
time team of LCEs in 2004. They were responsible for accelerating the launches of
new products and features, while at the same time applying SRE expertise to ensure
that Google shipped reliable products with high availability and low latency.
LCEs were responsible for making sure launches were executing quickly without the
services falling over, and that if a launch did fail, it didn’t take down other products.
LCEs were also responsible for keeping stakeholders informed of the nature and like‐
lihood of such failures whenever corners were cut in order to accelerate time to mar‐
ket. Their consulting sessions were formalized as Production Reviews.
Evolution of the LCE Checklist
As Google’s environment grew more complex, so did both the Launch Coordination
Engineering checklist (see Appendix E) and the volume of launches. In 3.5 years, one
LCE ran 350 launches through the LCE Checklist. As the team averaged five engi‐
neers during this time period, this translates into a Google launch throughput of over
1,500 launches in 3.5 years!