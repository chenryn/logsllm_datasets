We argue that our inference algorithm is sound. The
intuition behind the algorithm is that at each program point,
the context conservatively describes the information that
may be learned by knowing that control ﬂow has reached
that program point, and the information that may be learned
by examining the contents of each variable at that program
point. The transfer functions and merge operation generalize
information-ﬂow typing rules for security type systems such
as Jif [13], and we thus believe them to be sound. Standard
results for the soundness of dataﬂow analyses then entail
soundness of our analysis.
We also argue that our dataﬂow analysis always termi-
nates. There are only ﬁnitely many contexts that may occur
during dataﬂow analysis. This is due to the following facts.
1) Only ﬁnitely many precise input expressions are used
during the dataﬂow analysis, since a precise input
expression is used in the dataﬂow only if it was part of
the output of a previous analysis (see Section III-A).
2) Only ﬁnitely many input expressions are used during
the dataﬂow analysis, since indices of inputs ν[i] are
bounded, and only expressions occurring within the
program are used in policies.
3) Only ﬁnitely many marks are used during the dataﬂow
analysis, since there are only ﬁnitely many mark
commands in a program.
4) Only ﬁnitely many policies are used during the
dataﬂow analysis, since policies are normalized (Sec-
tion II) before they are stored in the context, and
given ﬁnitely many precise input expressions, input
expressions and marks, there are only ﬁnitely many
normalized policies.
We deﬁne a partial order ≤ over contexts, where
(cid:104)Γ1, pc1(cid:105) ≤ (cid:104)Γ2, pc2(cid:105) if and only if both Γ1 (cid:118) Γ2 and
pc1 (cid:118) pc2, where we extend the security policy partial
order (cid:118) pointwise to variable environments and pc-maps.1
The transfer functions used in the dataﬂow analysis are
to the partial order ≤, and the
monotonic with respect
context merge operation is an upper-bound operation for
this partial order. Thus, a standard work-queue algorithm
for the dataﬂow analysis will always ﬁnd a ﬁxed-point and
terminate.
IV. IMPLEMENTATION
We have implemented the dataﬂow analysis for precise
inference of security policies, described in Section III, as
an interprocedural object-sensitive dataﬂow analysis for the
Java programming language.
The implementation is a 35,100 non-comment non-blank
line extension for the Polyglot compiler framework [14],
a source-to-source compiler for the Java 1.4 programming
language. This extension is factored into 3 parts: 10,400
lines implement an object-sensitive pointer analysis [15] and
an interprocedural dataﬂow framework using this pointer
analysis; 11,400 lines implement a generic information-ﬂow
dataﬂow analysis that can be specialized for different secu-
rity policies; and 13,300 lines specialize this information-
ﬂow dataﬂow analysis for our security policies.
In this section, we discuss the extensions to the analysis
of Section III required to build a practical security policy
inference analysis for Java.
A. Programmer annotations
We aim to infer expressive security policies with few
annotations from the programmer. However, our security
policies rely on identifying sources of sensitive informa-
tion (inputs) and observable sinks of information (outputs).
While our tool could attempt
to infer these inputs and
outputs (for example, by determining which InputStreams
and OutputStreams may be connected to the network), we
have not done so. Instead, we rely on the programmer to
provide annotations indicating these sources and sinks. We
1The co-domain of pc-maps is both security policies and precise input
expressions; we extend the partial order over security policies by treating
precise input expression d as if it were the security policy Reveal(d).
189
emphasize that the programmer does not need to provide
annotations on ﬁelds, classes, or method headers before
being able to use our tool.
The programmer indicates inputs and outputs using the
annotations @input ν e and @output ν e respectively, where
ν is a literal string indicating the name of the input or
output, and e is an arbitrary Java expression. Input and
output annotations may occur in a program anywhere a Java
expression can occur. Input and output names are intended to
be unique within a program. A programmer can optionally
indicate the precision with which to track a particular input.
By default, we use precision 1, meaning that for input
channel ν, we track only input expressions ν[0] and ν[1+].
As shown in the Introduction’s example code, program-
mers can use the @track annotation in a method header.
This indicates that a method is security relevant and that
its name should be used to build if-executed policies.
Intuitively, this is similar to starting a method body with
a mark method-name command.
Improving the inference results.
If the results of
the inference are in accordance with the programmer’s
expectations about the security requirements of the program,
the programmer has received sufﬁcient security assurance.
However, if the results of the analysis do not meet the
programmer’s expectations, it could be due either to an
actual security issue in the program, or to imprecision in the
inference result. To address the later possibility, we provide
additional mechanisms for the programmer to improve the
precision of the analysis.
Implicit information ﬂows can be a signiﬁcant source of
imprecision, and, as King et al. [16] demonstrate, partic-
ularly implicit information ﬂows arising from conservative
handling of unchecked exceptions. To mitigate this source
of imprecision, we allow programmers to indicate that an
expression or statement e cannot throw an exception of
class E (or subclass thereof) by using the @suppress E e
annotation. This improves the precision of the control ﬂow
graph, and also improves the precision of the inference
analysis, since it reduces the spurious information ﬂows
that the inference conservatively assumes may occur due
to exceptional control ﬂow. If the programmer is incorrect
in her assertion that an expression cannot throw a certain
exception, then the analysis results may be unsound.
B. Precise input expressions
The analysis can track implicit ﬂows precisely when a
branching decision is determined by a precise input expres-
sion. Our analysis of Section III assumes that this infor-
mation is available through some other analysis, and does
not itself compute this information. We therefore implement
an additional interprocedural analysis to determine which
program expressions are equal to precise input expressions.
This is another information ﬂow analysis, albeit one that
tracks only explicit information ﬂows, and ignores implicit
ﬂows; it is similar to a constant propagation analysis.
C. Non-structured control ﬂow
Unlike the simple imperative language of Section II-B,
Java has unstructured control ﬂow, due to exceptions and
break, continue, and return commands. This complicates
the dataﬂow analysis in a few ways.
First, more work is required to determine the control ﬂow
graph for the program. Since our security policy inference
analysis tracks implicit information ﬂows, improving the
precision of the control ﬂow graph improves the precision of
the inferred security policies. The key sources of imprecision
in constructing the control ﬂow graph are dynamic dispatch
(where the method body to execute on method invocation
depends on the runtime class of the receiver object), and
exceptions (where control ﬂow depends on the runtime class
of the thrown exception). The pointer analysis provides in-
formation about the possible runtime classes of both receiver
objects and thrown exceptions. In addition, we perform
an interprocedural dataﬂow analysis to remove spurious
exceptions. For example, if we can prove that the receiver
of a ﬁeld access or method invocation can never be null,
then that operation can never throw a NullPointerException.
Similarly, given an unsafe cast operation (C )e, if the points-
to set of expression e indicates that e always points to
a subclass of class C, then the cast can never throw a
ClassCastException.
Second, the immediate post dominators of branch points
can no longer be determined by examining just the syntax
of the program. We instead implement an intraprocedural
post-dominance algorithm [17] to determine the immediate
post dominators of branch points. We use the output of
this algorithm to restore the program counter map at the
post dominator of a branch point to its value at the branch
point, as described in Section III-A. An interprocedural
post-dominance algorithm [18] would provide more precise
results, allowing us to potentially restore more program
counter maps. However,
inter-procedural post-dominator
analysis would improve precision only in speciﬁc instances,
with fairly complicated control ﬂow. In our experience, such
patterns of control ﬂow occur rarely in actual code, and
we have found that the intraprocedural analysis provides
sufﬁciently precise results.
Finally, we treat returns and throws like assignments, in
that we taint the value returned with the program counter
map in use at
the program point where the return or
throw occurs. This is because the program point at which a
return or throw occurs may reveal sensitive information, as
demonstrated in the following code snippet, where the value
of variable x depends on the sensitive input H[0].
int foo(boolean sensitiveInfo) {
if ( sensitiveInfo ) return 0;
return 1;
}
...
int x = foo(@input ‘‘H’’
...);
D. Interprocedural object-sensitive analysis
We have implemented the object-sensitive pointer analysis
of Milanova et al. [15]. We perform an object-sensitive
interprocedural dataﬂow analysis, using the results of this
pointer analysis to determine the different contexts in which
to analyze code. The sensitivity of the pointer analysis can
be adjusted, affecting the number of different contexts for
code analysis: increased sensitivity in the pointer analysis
leads to more contexts for code analysis (and thus more
precision).
Object-sensitive pointer analysis allows our interproce-
dural dataﬂow analysis to maintain greater precision (over
context-sensitive pointer analysis) for many common object-
oriented coding patterns, such as collections, and ﬁeld ac-
cess.
However, using just object-sensitivity can lead to im-
precision in our analysis for some common patterns, such
as getter-methods and methods that compute a function
of
its arguments (such as math library methods like
Math.max(int, int)). For example, consider the following
code.
1
2
3
4
5
6
7
8
9
10
class C {
int f ;
int getF() { return this. f ; }
}
...
C obj = new C();
if (@input ‘‘H’’ ...) {
int x = obj.getF();
}
int y = obj.getF();
Using object-sensitive interprocedural analysis, both calls to
the method C.getF() on the object obj use the same analysis
result, since they have the same receiver object. Thus, the
method is analyzed using a pc-map that merges the pc-maps
of both call sites. Since the pc-map of the ﬁrst call site
(line 8) contains sensitive information, and returns taint the
returned value with the pc-map, we incorrectly assume that
the value returned at the second call site (line 10) may reveal
sensitive information.
To maintain precision with in these cases, we analyze
methods at the leaves of the call graph based on both object-
context and call site. That is, any method that does not
invoke other code is analyzed separately for each call site
and object context. This avoids the problem with imprecise
pc-maps for many common cases, such as getter-methods
and math library functions, without unreasonable additional
computation.
190
)
c
e
s
(
e
m
i
t
e
c
n
e
r
e
f
n
I
<1
<1
<1
9
14
470
e
d
o
C
f
o
s
e
n
i
L
79
184
270
326
385
1369
s
n
o
i
t
a
t
o
n
n
A
2
3
8
6
18
6
Table I
CASE STUDIES