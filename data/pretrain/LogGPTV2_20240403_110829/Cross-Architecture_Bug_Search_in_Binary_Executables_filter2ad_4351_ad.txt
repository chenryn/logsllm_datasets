and w(cid:3)
i the number of formulas with that number of variables
in the Ô¨Årstand respectively the second, basic block. Multi-
MinHash thus solves the issue that formulas with only a
few samples (e.g., very few or no inputs) would be under-
represented in the hash value.
Second, we do not only store the smallest hash value per
the k smallest hash values‚Äîwhich we
hash function, but
denote as k-MinHash. This modiÔ¨Åcation allows us to estimate
the frequency of elements in the multi-set of I/O pairs to
some extent, i. e., we can recognize if a basic block has mul-
tiple equivalent formulas. Since deterministic hash functions
map equal values to equal outputs, one cannot substitute k-
MinHash against a MinHash with a larger number of hash
functions. However, k-MinHash can be trivially combined with
Multi-MinHash to beneÔ¨Åt from both concepts. When referring
to k-MinHash in the evaluation, we implicitly mean Multi-k-
MinHash for k = 3.
D. Bug Signature Matching
Given a bug signature and a target program, we have to
Ô¨Ånd code in the target program that is similar to the bug
signature. To this end, we Ô¨Årst iterate over the basic blocks of
the bug signature and compare them individually against every
basic block in the target program according to their MinHash
similarity. For each basic block in the bug signature, we sort
the resulting similarities, which results in a list of promising
initial candidates for a full bug signature match. Then, we try
to broaden the best b candidates with our Best-Hit-Broadening
(BHB) algorithm, which computes the similarity of two graphs
of basic blocks.
BHB works as follows: Given a pair of starting points (a
basic block from the signature and its corresponding matching
candidate in the target program), it Ô¨Årst explores the immediate
neighborhood of these basic blocks along their respective
CFGs (Figure 5a). When doing so, it strictly separates for-
ward and backward directions. After Ô¨Ånding a locally-optimal
715715
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:33 UTC from IEEE Xplore.  Restrictions apply. 
Signature
Target
S1
S2
1/3
1
1/3
1/4
S3
S4
T3
1/2
1
S5
T1
T2
T5
Signature
1/3
Target
Signature
1/3
Target
S1
S2
T4
S3
S4
T3
1/2
S5
1
T1
T2
T5
S1
S2
T1
T2
T4
S3
S4
T3
T4
1/2
S5
T5
(a) First BHB round with the initial
starting point and its candidate match
(annotated with the bold arrow).
(b) After the Ô¨Årst BHB round, another pair of
BBs is matched and the lower two nodes are
now adjacent.
the third step,
(c) After
three pairs are
matched. There are no further neighbors and
the other two matches are trivial.
Fig. 5: BHB example in three steps: bug signature on the left, target program on the right side. The difference between two
basic blocks is visualized by the different numbers of edges (n) of the shapes. The similarity is then calculated as
1
1+n.
matching among the newly discovered neighborhood nodes
with a matching algorithm, it picks the basic block pair with
the maximum similarity. That is, BHB broadens the already-
matched basic block pairs (Figure 5b). The broadening is
greedy, avoiding expensive backtracking steps. This process
is repeated (Figure 5c) until all basic blocks from the bug
signature have been matched. In the end, BHB computes the
overall matching similarity as the average similarity of all
matched pairs, which is ‚âà 0.77 in Figure 5. BHB is then
invoked for the other b‚àí1 matching candidates. The end result
of our bug signature search is a sorted list of BHB similarities,
revealing those matched code parts in the target program that
are most similar to the bug.
Listing 1: Best-Hit-Broadening Algorithm
( spi , spt ) )
MS = [ ] , MT = [ ]
PQ = ( sim ( spi , spt ) ,
while (PQ (cid:4)= ‚àÖ )
1 BHB( spi ‚àà SP , spt ‚àà Ci , Sig )
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
PS , PT := PQ . pop ( )
MS += PS , MT += PT
Dp := sim mat ( PS .pred \ MS , PT .pred \ MT )
(cid:2)
Mp := Hungarian ( Dp )
PQ . add ( sim ( Pi, Pj ) , (Pi, Pj ) ) , ‚àÄ(Pi, Pj ) ‚àà (cid:2)
Mp
Ds := sim mat ( PS .succ \ MS , PT .succ \ MT )
(cid:3)
Ms := Hungarian ( Ds )
PQ . add ( sim ( Pi , Pj ) , (Pi, Pj ) ) , ‚àÄ(Pi, Pj ) ‚àà (cid:3)
sim(MS [i], MT [i])
(cid:4)|MS|‚àí1
:=
d i s t
return d i s t
i=0
/
|Sig|
Ms
A formal description of the algorithm is provided in List-
ing 1. As input, BHB accepts a basic block from the signature
(spi ‚àà SP ) and a similar basic block from the target program
(spt ‚àà Ci). BHB keeps track of blocks that were already
matched to each other (MS from the bug signature and MT
716716
from the target program) in a parallel list. Lastly, P Q is a
priority queue of potential basic block matches, sorted by their
similarity in decreasing order. That is, pop retrieves the basic
block pair that is the current best match of the yet unmatched
pairs. Note that P Q only includes pairs for which both basic
blocks are adjacent to the basic blocks that have been matched
so far. Initially (line 3), it only contains the starting point from
the signature and its similar counterpart in the target program.
In lines 6 and 7, the algorithm takes the most similar pair
and adds them to the set of matched basic blocks MS and
MT . In lines 9 and 13, BHB computes the similarity matrix
between blocks in the bug signature and all blocks in the
target program that are adjacent to PS and PT , respectively.
At this point, it Ô¨Ånds a locally-optimal matching between
those adjacent nodes to decide where the broadening should
continue (lines 10 and 14). A so-called matching algorithm
Ô¨Ånds such a mapping between the left and right side of the
graph, where the sum of all mapped similarities is maximal.
We chose the Hungarian method [12], an algorithm with
runtime O(n3) in the number of nodes. Because we are only
matching neighbors of the current signature basic block with
the neighbors of the matching candidate in the target program,
and do not backtrack, n is quite small. The overall runtime is
thus dominated by the basic block-wise similarity metric.
The block pairs of that ideal mapping are added to the
queue P Q, including their similarity, which is computed with
the MinHash similarity (see Section III-C). BHB completes if
the queue is empty, i. e., all basic blocks in the bug signature
have been matched to a basic block in the target program (or
no matching is possible anymore). Finally, in line 17, BHB
computes the similarity between the bug signature and the tar-
get program by summing up the similarities for each matched
block pair, averaging over the size of the bug signature.
To determine how many candidates b to examine, we
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:05:33 UTC from IEEE Xplore.  Restrictions apply. 
 0.035
 0.03
 0.025
 0.02
 0.015
 0.01
 0.005
a
o
t
d
a
e
l
o
t
y
t
i
l
i
b
a
b
o
r
P
k
n
a
r
n
o
i
t
c
n
u
f
-
l
l
u
f
5
2
p
o
t
 0
 0
 200
 400
Candidate Rank
 600
 800
 1000
Fig. 6: Probability of the x-th candidate per basic block in the
signature leading to one of the top 25 function matches.
analyze the probability of the x-th candidate per basic block
in the signature leading to one of the top 25 function matches.
Fig. 6 shows that the best function matches usually stem
from the best individual candidate matches. For this paper, we
choose to investigate only the Ô¨Årst b = 200 candidates with
a full-blown invocation of the BHB algorithm, which offers a
reasonable trade-off between accuracy and performance.
IV. EVALUATION
This section gives an empirical evaluation of our approach:
First, we systematically evaluate our code comparison metric
in a binary, cross-architecture setting. Second, we apply it to
our use case: Ô¨Ånding bugs using a bug signature.
The experiments were conducted on an Intel Core i7-2640M
@ 2.8GHz with 8GB DDR3-RAM. To ease comparability,
they were performed with a single-threaded process. Our
experiments include 60 binaries from three architectures and
three compiler versions (x86, ARM, MIPS; all 32 bit) (gcc
v4.6.2/v4.8.1 and clang v3.0). While our focus was on
Linux, we encountered different core libraries, especially for
router software (DD-WRT/NetGear/SerComm/MikroTik).
Note that we also compared binaries across Windows and Mac
OS X, i.e., we covered all three major OSes.
Our approach allows us to Ô¨Ånd (sub-)function code snippets
at any granularity. Still, the following experiments mostly
show function-level comparisons, as function-level signatures
can be chosen automatically. If we manually excluded some
blocks, we would need to justify that choice. Instead, we
decided to use a fair and reproducible test case: function-level
signatures. However, in practice, analysts can choose entire
functions or only parts of a function as a signature.
A. False/True Positives Across Architectures
In this experiment, we aim to match all functions of one
binary A to all functions of another binary B, which measures
how generic our solution is in matching similar code. The
two binaries stem from the same source code base, and thus
principally contain the same code, but were compiled for
different architectures. For each function in A, we compute
	












	
	 !""#$ "%"&'
()(*		 !"* %"&'
()(*	
*()(*		* %"&'








Fig. 7: Ranks of true positives in function matching for
OpenSSL and BusyBox. Other architecture-combinations are
similar and were omitted.
the similarities to all functions in B, resulting in a list of
functions from B sorted by their similarity to the searched-for
function. We then plot the rank of the correct counterpart of
the function in A, i.e., the position in the sorted list, where the
most similar match is at rank 1. We derive the ground truth of
the function mappings based on the (identical) function names
in both binaries. Note that we use the symbolic information
for this mapping only and not in the similarity metric.
First, we evaluate our approach on a single architecture. We
compared two x86 binaries of very similar, but not identical
versions of BusyBox (v1.20.0 and v1.21.1). Figure 7 shows
a cumulative distribution function (CDF) on the ranks of all
functions of this experiment. A perfect method would result
in a straight line, i.e., all correct functions are at rank 1. In
this single-architecture comparison, we could perfectly match
90.4% of the functions (rank 1) and had close matches for
97% (ranked within the top 10). Some of the mismatches may
also be caused by slight changes between the two BusyBox