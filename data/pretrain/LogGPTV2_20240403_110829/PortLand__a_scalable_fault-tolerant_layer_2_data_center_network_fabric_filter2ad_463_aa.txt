# PortLand: A Scalable Fault-Tolerant Layer 2 Data Center Network Fabric

## Authors
Radhika Niranjan Mysore, Andreas Pamboris, Nathan Farrington, Nelson Huang, Pardis Miri, Sivasankar Radhakrishnan, Vikram Subramanya, and Amin Vahdat  
{radhika, apambori, farrington, nhuang, smiri, sivasankar, vikram.s3, vahdat}@cs.ucsd.edu  
Department of Computer Science and Engineering, University of California San Diego

## Abstract
This paper addresses the requirements for a scalable, easily manageable, fault-tolerant, and efficient data center network fabric. With trends in multi-core processors, end-host virtualization, and economies of scale, future single-site data centers are expected to have millions of virtual endpoints. Existing Layer 2 and Layer 3 network protocols face limitations in such settings, including scalability issues, difficult management, inflexible communication, and limited support for virtual machine (VM) migration. These limitations may be inherent in Ethernet/IP-style protocols when supporting arbitrary topologies. We observe that data center networks are often managed as a single logical network fabric with a known baseline topology and growth model. Leveraging this observation, we designed and implemented PortLand, a scalable and fault-tolerant Layer 2 routing and forwarding protocol for data center environments. Our implementation and evaluation show that PortLand holds promise for supporting a "plug-and-play" large-scale data center network.

### Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: Network communications; C.2.2 [Network Protocols]: Routing protocols

### General Terms
Algorithms, Design, Performance, Management, Reliability

### Keywords
Data center network fabric, Layer 2 routing in data centers

## 1. Introduction
There is an increasing trend toward migrating applications, computation, and storage into data centers spread across the Internet. The benefits of economies of scale are leading to the emergence of "mega data centers" hosting applications running on tens of thousands of servers [3]. For example, a web search request may access an inverted index spread across 1,000+ servers, and data storage and analysis applications may interactively process petabytes of information stored on thousands of machines. These scenarios present significant networking requirements.

In the future, a substantial portion of Internet communication will occur within data center networks. These networks are highly engineered, with common design elements. However, the routing, forwarding, and management protocols used in data centers were designed for general LAN settings and are proving inadequate in several dimensions. Consider a data center with 100,000 servers, each hosting 32 VMs. This translates to a total of three million IP and MAC addresses. Assuming one switch is required for every 25 physical hosts and accounting for interior nodes, the topology would consist of 8,000 switches. Current network protocols impose significant management overhead at this scale. For instance, an end host's IP address may be determined by its directly-connected physical switch and synchronized with replicated DHCP servers. VLANs provide some naming flexibility but introduce configuration and resource allocation overheads.

Ideally, data center network architects and administrators would have "plug-and-play" deployment for switches. Here are some key requirements for such a future scenario:

- **R1:** Any VM may migrate to any physical machine without changing its IP address, to avoid breaking pre-existing TCP connections and application-level state.
- **R2:** An administrator should not need to configure any switch before deployment.
- **R3:** Any end host should be able to efficiently communicate with any other end host in the data center along any available physical communication path.
- **R4:** There should be no forwarding loops.
- **R5:** Failures will be common at scale, so failure detection should be rapid and efficient. Existing unicast and multicast sessions should proceed unaffected to the extent allowed by underlying physical connectivity.

Mapping these requirements to the underlying network protocols, R1 and R2 essentially require supporting a single Layer 2 fabric for the entire data center. A Layer 3 fabric would require configuring each switch with subnet information and synchronizing DHCP servers to distribute IP addresses based on the host's subnet. Transparent VM migration is not possible at Layer 3 without IP mobility techniques because VMs must switch their IP addresses if they migrate to a different subnet. Layer 2 fabrics face scalability and efficiency challenges due to the need to support broadcast. R3 at Layer 2 requires MAC forwarding tables with potentially hundreds of thousands or even millions of entries, which is impractical with today's switch hardware. R4 is difficult for both Layer 2 and Layer 3 because forwarding loops are possible during routing convergence. A Layer 2 protocol may avoid such loops by employing a single spanning tree (inefficient) or tolerate them by introducing an additional header with a TTL (incompatible). R5 requires efficient routing protocols that can disseminate topology changes quickly to all points of interest. Existing Layer 2 and Layer 3 routing protocols, such as ISIS and OSPF, are broadcast-based, with every switch update sent to all switches, leading to high overhead.

Hence, the current assumption is that the vision of a unified plug-and-play large-scale network fabric is unachievable, leaving data center network architects to adopt ad hoc partitioning and configuration to support large-scale deployments. Recent work in SEATTLE [10] makes significant advances toward a plug-and-play Ethernet-compatible protocol. However, in SEATTLE, switch state grows with the number of hosts, forwarding loops remain possible, and routing requires all-to-all broadcast, violating R3, R4, and R5. Section 3.7 provides a detailed discussion of both SEATTLE and TRILL [28].

In this paper, we present PortLand, a set of Ethernet-compatible routing, forwarding, and address resolution protocols designed to meet the above requirements. The primary observation behind our work is that data center networks are often physically interconnected as a multi-rooted tree [1]. Using this observation, PortLand employs a lightweight protocol to enable switches to discover their position in the topology. PortLand further assigns internal Pseudo MAC (PMAC) addresses to all end hosts to encode their position in the topology. PMAC addresses enable efficient, provably loop-free forwarding with small switch state.

We have a complete implementation of PortLand, providing native fault-tolerant support for ARP, network-layer multicast, and broadcast. PortLand imposes minimal requirements on the underlying switch software and hardware. We hope that PortLand enables more flexible, efficient, and fault-tolerant data centers where applications can be flexibly mapped to different hosts, treating the data center network as a unified fabric.

## 2. Background

### 2.1 Data Center Networks

#### Topology
Current data centers consist of thousands to tens of thousands of computers, with emerging mega data centers hosting 100,000+ compute nodes. As an example, consider our interpretation of current best practices [1] for the layout of a 11,520-port data center network. Machines are organized into racks and rows, with a logical hierarchical network tree overlaid on top of the machines. In this example, the data center consists of 24 rows, each with 12 racks. Each rack contains 40 machines interconnected by a top-of-rack (ToR) switch that delivers non-blocking bandwidth among directly connected hosts. Today, a standard ToR switch contains 48 GigE ports and up to 4 available 10 GigE uplinks.

ToR switches connect to end-of-row (EoR) switches via 1-4 of the available 10 GigE uplinks. To tolerate individual switch failures, ToR switches may be connected to EoR switches in different rows. An EoR switch is typically a modular 10 GigE switch with a number of ports corresponding to the desired aggregate bandwidth. For maximum bandwidth, each of the 12 ToR switches would connect all 4 available 10 GigE uplinks to a modular 10 GigE switch with up to 96 ports. 48 of these ports would face downward towards the ToR switches, and the remainder would face upward to a core switch layer. Achieving maximum bandwidth for inter-row communication in this example requires connecting 48 upward-facing ports from each of 24 EoR switches to a core switching layer consisting of 12 96-port 10 GigE switches.

#### Forwarding
There are several data forwarding techniques in data center networks. The high-level dichotomy is between creating a Layer 2 network or a Layer 3 network, each with associated trade-offs. A Layer 3 approach assigns IP addresses to hosts hierarchically based on their directly connected switch. In the example topology above, hosts connected to the same ToR could be assigned the same /26 prefix, and hosts in the same row may have a /22 prefix. Such careful assignment enables relatively small forwarding tables across all data center switches.

Standard intra-domain routing protocols such as OSPF [21] can be employed among switches to find shortest paths among hosts. Failures in large-scale network topologies are commonplace. OSPF can detect such failures and broadcast the information to all switches to avoid failed links or switches. Transient loops with Layer 3 forwarding are less of an issue because the IP-layer TTL limits per-packet resource consumption while forwarding tables are being asynchronously updated.

Unfortunately, Layer 3 forwarding imposes administrative burdens. Adding a new switch generally requires manual configuration and oversight, which is error-prone. Improperly synchronized state between system components, such as a DHCP server and a configured switch subnet identifier, can lead to unreachable hosts and difficult-to-diagnose errors. The growing importance of end-host virtualization also makes Layer 3 solutions less desirable.

For these reasons, certain data centers deploy a Layer 2 network where forwarding is performed based on flat MAC addresses. A Layer 2 fabric imposes less administrative overhead but has its own challenges. Standard Ethernet bridging [23] does not scale to networks with tens of thousands of hosts due to the need to support broadcast across the entire fabric. A single forwarding spanning tree, even if optimally designed, would severely limit performance in topologies with multiple available equal-cost paths.

A middle ground between a Layer 2 and Layer 3 fabric involves using VLANs to allow a single logical Layer 2 fabric to cross multiple switch boundaries. While feasible for smaller-scale topologies, VLANs suffer from drawbacks. They require bandwidth resources to be explicitly assigned to each VLAN at each participating switch, limiting flexibility for dynamically changing communication patterns. Each switch must maintain state for all hosts in each VLAN, limiting scalability. VLANs also use a single forwarding spanning tree, limiting performance.

#### End Host Virtualization
The increasing popularity of end-host virtualization in the data center imposes several requirements on the underlying network. Commercially available virtual machine monitors allow tens of VMs to run on each physical machine, each with their own fixed IP and MAC addresses. In data centers with hundreds of thousands of hosts, this translates to the need for scalable addressing and forwarding for millions of unique endpoints. While individual applications may not yet run at this scale, application designers and data center administrators benefit from the ability to arbitrarily map individual applications to an arbitrary subset of available physical resources.

Virtualization also allows the entire VM state to be transmitted across the network to migrate a VM from one physical machine to another [11]. Such migration might occur for various reasons, such as statistical multiplexing, dynamic communication patterns, and variable heat distribution and power availability. This environment presents challenges for both Layer 2 and Layer 3 data center networks. In a Layer 3 setting, the IP address of a virtual machine is set by its directly-connected switch subnet number. Migrating the VM to a different switch would require assigning a new IP address based on the subnet number of the new first-hop switch, breaking all open TCP connections and invalidating session state. A Layer 2 fabric is agnostic to the IP address of a VM but faces challenges in scaling ARP and performing routing/forwarding on millions of flat MAC addresses.

### 2.2 Fat Tree Networks
Recently proposed work [6, 14, 15] suggests alternate topologies for scalable data center networks. In this paper, we consider designing a scalable, fault-tolerant Layer 2 domain over one such topology, a fat tree. The fat tree is an instance of the traditional data center multi-rooted tree topology (Section 2.1), and the techniques described in this paper generalize to existing data center topologies. We present the fat tree because our available hardware/software evaluation platform (Section 4) is built as a fat tree.

Figure 1 depicts a 16-port switch built as a multi-stage topology from constituent 4-port switches. In general, a three-stage fat tree built from k-port switches can support non-blocking communication among k^3/4 end hosts using 5k^2/4 individual k-port switches. We split the fat tree into three layers: edge, aggregation, and core. The fat tree as a whole is split into k individual pods, with each pod supporting non-blocking operation among k^2/4 hosts. Non-blocking operation requires careful scheduling of packets among all available paths, a challenging problem. While a number of heuristics are possible, for the purposes of this work, we assume ECMP-style hashing of flows [16] among the k^2/4 available paths between a given source and destination. While current techniques are less than ideal, we consider the flow scheduling problem to be beyond the scope of this paper.

### 2.3 Related Work
Recently, there have been several proposals for network architectures specifically targeting the data center. Two recent proposals [14, 6] suggest topologies based on fat trees [18]. As discussed earlier, fat trees are a form of multi-rooted trees that already form the basis for many existing data center topologies. They are fully compatible with our work, and our implementation runs on top of a small-scale fat tree. DCell [15] also recently proposed a specialized topology for the data center environment. While not strictly a multi-rooted tree, there is implicit hierarchy in the DCell topology, making it compatible with our approach.