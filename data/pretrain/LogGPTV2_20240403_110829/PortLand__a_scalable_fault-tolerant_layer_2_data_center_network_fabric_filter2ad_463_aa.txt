title:PortLand: a scalable fault-tolerant layer 2 data center network fabric
author:Radhika Niranjan Mysore and
Andreas Pamboris and
Nathan Farrington and
Nelson Huang and
Pardis Miri and
Sivasankar Radhakrishnan and
Vikram Subramanya and
Amin Vahdat
PortLand: A Scalable Fault-Tolerant Layer 2
Data Center Network Fabric
Radhika Niranjan Mysore, Andreas Pamboris, Nathan Farrington, Nelson Huang, Pardis Miri,
Sivasankar Radhakrishnan, Vikram Subramanya, and Amin Vahdat
{radhika, apambori, farrington, nhuang, smiri, sivasankar, vikram.s3, vahdat}@cs.ucsd.edu
Department of Computer Science and Engineering
University of California San Diego
ABSTRACT
This paper considers the requirements for a scalable, eas-
ily manageable, fault-tolerant, and eﬃcient data center net-
work fabric. Trends in multi-core processors, end-host vir-
tualization, and commodities of scale are pointing to future
single-site data centers with millions of virtual end points.
Existing layer 2 and layer 3 network protocols face some
combination of limitations in such a setting:
lack of scal-
ability, diﬃcult management, inﬂexible communication, or
limited support for virtual machine migration. To some ex-
tent, these limitations may be inherent for Ethernet/IP style
protocols when trying to support arbitrary topologies. We
observe that data center networks are often managed as a
single logical network fabric with a known baseline topol-
ogy and growth model. We leverage this observation in the
design and implementation of PortLand, a scalable, fault
tolerant layer 2 routing and forwarding protocol for data
center environments. Through our implementation and eval-
uation, we show that PortLand holds promise for supporting
a “plug-and-play” large-scale, data center network.
Categories and Subject Descriptors
C.2.1 [Network Architecture and Design]: Network
communications; C.2.2 [Network Protocols]: Routing
protocols
General Terms
Algorithms, Design, Performance, Management, Reliability
Keywords
Data center network fabric, Layer 2 routing in data centers
1.
INTRODUCTION
There is an increasing trend toward migrating applica-
tions, computation and storage into data centers spread
across the Internet. Beneﬁts from commodities of scale are
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’09, August 17–21, 2009, Barcelona, Spain.
Copyright 2009 ACM 978-1-60558-594-9/09/08 ...$10.00.
leading to the emergence of “mega data centers” hosting ap-
plications running on tens of thousands of servers [3]. For
instance, a web search request may access an inverted index
spread across 1,000+ servers, and data storage and analysis
applications may interactively process petabytes of informa-
tion stored on thousands of machines. There are signiﬁcant
application networking requirements across all these cases.
In the future, a substantial portion of Internet communi-
cation will take place within data center networks. These
networks tend to be highly engineered, with a number of
common design elements. And yet, the routing, forwarding,
and management protocols that we run in data centers were
designed for the general LAN setting and are proving inad-
equate along a number of dimensions. Consider a data cen-
ter with 100,000 servers, each hosting 32 virtual machines
(VMs). This translates to a total of three million IP and
MAC addresses in the data center. Assuming one switch
is required for every 25 physical hosts and accounting for
interior nodes, the topology would consist of 8,000 switches.
Current network protocols impose signiﬁcant management
overhead at this scale. For example, an end host’s IP ad-
dress may be determined by its directly-connected physi-
cal switch and appropriately synchronized with replicated
DHCP servers. VLANs may provide some naming ﬂexibility
across switch boundaries but introduce their own conﬁgura-
tion and resource allocation overheads.
Ideally, data cen-
ter network architects and administrators would have “plug-
and-play” deployment for switches. Consider some of the
requirements for such a future scenario:
• R1. Any VM may migrate to any physical machine.
Migrating VMs should not have to change their IP
addresses as doing so will break pre-existing TCP con-
nections and application-level state.
• R2. An administrator should not need to conﬁgure
any switch before deployment.
• R3. Any end host should be able to eﬃciently commu-
nicate with any other end host in the data center along
any of the available physical communication paths.
• R4. There should be no forwarding loops.
• R5. Failures will be common at scale, so failure de-
tection should be rapid and eﬃcient. Existing unicast
and multicast sessions should proceed unaﬀected to the
extent allowed by underlying physical connectivity.
Let us now map these requirements to implications for the
underlying network protocols. R1 and R2 essentially require
39supporting a single layer 2 fabric for the entire data center.
A layer 3 fabric would require conﬁguring each switch with
its subnet information and synchronizing DHCP servers to
distribute IP addresses based on the host’s subnet. Worse,
transparent VM migration is not possible at layer 3 (save
through techniques designed for IP mobility) because VMs
must switch their IP addresses if they migrate to a host
on a diﬀerent subnet. Unfortunately, layer 2 fabrics face
scalability and eﬃciency challenges because of the need to
support broadcast. Further, R3 at layer 2 requires MAC
forwarding tables with potentially hundreds of thousands
or even millions of entries, impractical with today’s switch
hardware. R4 is diﬃcult for either layer 2 or layer 3 because
forwarding loops are possible during routing convergence. A
layer 2 protocol may avoid such loops by employing a single
spanning tree (ineﬃcient) or tolerate them by introducing
an additional header with a TTL (incompatible). R5 re-
quires eﬃcient routing protocols that can disseminate topol-
ogy changes quickly to all points of interest. Unfortunately,
existing layer 2 and layer 3 routing protocols, e.g., ISIS and
OSPF, are broadcast based, with every switch update sent
to all switches. On the eﬃciency side, the broadcast over-
head of such protocols would likely require conﬁguring the
equivalent of routing areas [5], contrary to R2.
Hence, the current assumption is that the vision of a uni-
ﬁed plug-and-play large-scale network fabric is unachievable,
leaving data center network architects to adopt ad hoc par-
titioning and conﬁguration to support large-scale deploy-
ments. Recent work in SEATTLE [10] makes dramatic ad-
vances toward a plug-and-play Ethernet-compatible proto-
col. However, in SEATTLE, switch state grows with the
number of hosts in the data center, forwarding loops remain
possible, and routing requires all-to-all broadcast, violating
R3, R4, and R5. Section 3.7 presents a detailed discussion
of both SEATTLE and TRILL [28].
In this paper, we present PortLand, a set of Ethernet-
compatible routing, forwarding, and address resolution pro-
tocols with the goal of meeting R1-R5 above. The principal
observation behind our work is that data center networks are
often physically inter-connected as a multi-rooted tree [1].
Using this observation, PortLand employs a lightweight pro-
tocol to enable switches to discover their position in the
topology. PortLand further assigns internal Pseudo MAC
(PMAC) addresses to all end hosts to encode their position
in the topology. PMAC addresses enable eﬃcient, provably
loop-free forwarding with small switch state.
We have a complete implementation of PortLand. We
provide native fault-tolerant support for ARP, network-layer
multicast, and broadcast. PortLand imposes little require-
ments on the underlying switch software and hardware. We
hope that PortLand enables a move towards more ﬂexible,
eﬃcient and fault-tolerant data centers where applications
may ﬂexibly be mapped to diﬀerent hosts, i.e. where the
data center network may be treated as one uniﬁed fabric.
2. BACKGROUND
2.1 Data Center Networks
Topology.
Current data centers consist of thousands to tens of thou-
sands of computers with emerging mega data centers hosting
100,000+ compute nodes. As one example, consider our in-
terpretation of current best practices
[1] for the layout of
a 11,520-port data center network. Machines are organized
into racks and rows, with a logical hierarchical network tree
overlaid on top of the machines. In this example, the data
center consists of 24 rows, each with 12 racks. Each rack
contains 40 machines interconnected by a top of rack (ToR)
switch that delivers non-blocking bandwidth among directly
connected hosts. Today, a standard ToR switch contains 48
GigE ports and up to 4 available 10 GigE uplinks.
ToR switches connect to end of row (EoR) switches via
1-4 of the available 10 GigE uplinks. To tolerate individ-
ual switch failures, ToR switches may be connected to EoR
switches in diﬀerent rows. An EoR switch is typically a mod-
ular 10 GigE switch with a number of ports corresponding to
the desired aggregate bandwidth. For maximum bandwidth,
each of the 12 ToR switches would connect all 4 available
10 GigE uplinks to a modular 10 GigE switch with up to 96
ports. 48 of these ports would face downward towards the
ToR switches and the remainder of the ports would face up-
ward to a core switch layer. Achieving maximum bandwidth
for inter-row communication in this example requires con-
necting 48 upward facing ports from each of 24 EoR switches
to a core switching layer consisting of 12 96-port 10 GigE
switches.
Forwarding.
There are a number of available data forwarding tech-
niques in data center networks. The high-level dichotomy
is between creating a Layer 2 network or a Layer 3 net-
work, each with associated tradeoﬀs. A Layer 3 approach
assigns IP addresses to hosts hierarchically based on their
directly connected switch. In the example topology above,
hosts connected to the same ToR could be assigned the same
/26 preﬁx and hosts in the same row may have a /22 preﬁx.
Such careful assignment will enable relatively small forward-
ing tables across all data center switches.
Standard intra-domain routing protocols such as OSPF [21]
may be employed among switches to ﬁnd shortest paths
among hosts. Failures in large-scale network topologies will
be commonplace. OSPF can detect such failures and then
broadcast the information to all switches to avoid failed
links or switches. Transient loops with layer 3 forwarding is
less of an issue because the IP-layer TTL limits per-packet
resource consumption while forwarding tables are being
asynchronously updated.
Unfortunately, Layer 3 forwarding does impose adminis-
trative burden as discussed above. In general, the process
of adding a new switch requires manual administrator con-
ﬁguration and oversight, an error prone process. Worse,
improperly synchronized state between system components,
such as a DHCP server and a conﬁgured switch subnet iden-
tiﬁer can lead to unreachable hosts and diﬃcult to diagnose
errors. Finally, the growing importance of end host virtual-
ization makes Layer 3 solutions less desirable as described
below.
For these reasons, certain data centers deploy a layer 2
network where forwarding is performed based on ﬂat MAC
addresses. A layer 2 fabric imposes less administrative over-
head. Layer 2 fabrics have their own challenges of course.
Standard Ethernet bridging [23] does not scale to networks
with tens of thousands of hosts because of the need to sup-
port broadcast across the entire fabric. Worse, the presence
of a single forwarding spanning tree (even if optimally de-
40signed) would severely limit performance in topologies that
consist of multiple available equal cost paths.
A middle ground between a Layer 2 and Layer 3 fab-
ric consists of employing VLANs to allow a single logical
Layer 2 fabric to cross multiple switch boundaries. While
feasible for smaller-scale topologies, VLANs also suﬀer from
a number of drawbacks. For instance, they require band-
width resources to be explicitly assigned to each VLAN at
each participating switch, limiting ﬂexibility for dynamically
changing communication patterns. Next, each switch must
maintain state for all hosts in each VLAN that they par-
ticipate in, limiting scalability. Finally, VLANs also use a
single forwarding spanning tree, limiting performance.
End Host Virtualization.
The increasing popularity of end host virtualization in the
data center imposes a number of requirements on the un-
derlying network. Commercially available virtual machine
monitors allow tens of VMs to run on each physical machine
in the data center1, each with their own ﬁxed IP and MAC
addresses.
In data centers with hundreds of thousands of
hosts, this translates to the need for scalable addressing and
forwarding for millions of unique end points. While individ-
ual applications may not (yet) run at this scale, application
designers and data center administrators alike would still
beneﬁt from the ability to arbitrarily map individual appli-
cations to an arbitrary subset of available physical resources.
Virtualization also allows the entire VM state to be trans-
mitted across the network to migrate a VM from one phys-
ical machine to another [11]. Such migration might take
place for a variety of reasons. A cloud computing hosting
service may migrate VMs for statistical multiplexing, pack-
ing VMs on the smallest physical footprint possible while
still maintaining performance guarantees. Further, variable
bandwidth to remote nodes in the data center could war-
rant migration based on dynamically changing communica-
tion patterns to achieve high bandwidth for tightly-coupled
hosts. Finally, variable heat distribution and power avail-
ability in the data center (in steady state or as a result of
component cooling or power failure) may necessitate VM
migration to avoid hardware failures.
Such an environment currently presents challenges both
for Layer 2 and Layer 3 data center networks. In a Layer 3
setting, the IP address of a virtual machine is set by its
directly-connected switch subnet number. Migrating the
VM to a diﬀerent switch would require assigning a new IP
address based on the subnet number of the new ﬁrst-hop
switch, an operation that would break all open TCP con-
nections to the host and invalidate any session state main-
tained across the data center, etc. A Layer 2 fabric is ag-
nostic to the IP address of a VM. However, scaling ARP
and performing routing/forwarding on millions of ﬂat MAC
addresses introduces a separate set of challenges.
2.2 Fat Tree Networks
Recently proposed work [6, 14, 15] suggest alternate
topologies for scalable data center networks. In this paper,
we consider designing a scalable fault tolerant layer 2 do-
1One rule of thumb for the degree of VM-multiplexing allo-
cates one VM per thread in the underlying processor hard-
ware. x86 machines today have 2 sockets, 4 cores/processor,
and 2 threads/core. Quad socket, eight core machines will
be available shortly.
main over one such topology, a fat tree. As will become
evident, the fat tree is simply an instance of the traditional
data center multi-rooted tree topology (Section 2.1). Hence,
the techniques described in this paper generalize to existing
data center topologies. We present the fat tree because our
available hardware/software evaluation platform (Section 4)
is built as a fat tree.
Figure 1 depicts a 16-port switch built as a multi-stage
topology from constituent 4-port switches.
In general, a
three-stage fat tree built from k-port switches can support
non-blocking communication among k3/4 end hosts using
5k2/4 individual k-port switches. We split the fat tree into
three layers, labeled edge, aggregation and core as in Fig-
ure 1. The fat tree as a whole is split into k individual pods,
with each pod supporting non-blocking operation among
k2/4 hosts. Non-blocking operation requires careful schedul-
ing of packets among all available paths, a challenging prob-
lem. While a number of heuristics are possible, for the
purposes of this work we assume ECMP-style hashing of
ﬂows [16] among the k2/4 available paths between a given
source and destination. While current techniques are less
than ideal, we consider the ﬂow scheduling problem to be
beyond the scope of this paper.
2.3 Related Work
Recently, there have been a number of proposals for net-
work architectures speciﬁcally targeting the data center.
Two recent proposals [14, 6] suggest topologies based on fat
trees [18]. As discussed earlier, fat trees are a form of multi-
rooted trees that already form the basis for many existing
data center topologies. As such, they are fully compatible
with our work and in fact our implementation runs on top
of a small-scale fat tree. DCell [15] also recently proposed a
specialized topology for the data center environment. While
not strictly a multi-rooted tree, there is implicit hierarchy in
the DCell topology, which should make it compatible with