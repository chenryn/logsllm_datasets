# Failure Resilience for Device Drivers

**Authors:** Jorrit N. Herder, Herbert Bos, Ben Gras, Philip Homburg, and Andrew S. Tanenbaum  
**Affiliation:** Computer Science Department, Vrije Universiteit, Amsterdam, The Netherlands  
**Emails:** {jnherder, herbertb, beng, philip, ast}@cs.vu.nl

## Abstract
Studies have shown that device drivers and extensions contain 3-7 times more bugs than other operating system code, making them more likely to fail. This paper presents a failure-resilient operating system design that can recover from dead drivers and other critical components by monitoring and replacing malfunctioning components on the fly, transparently to applications and without user intervention. We focus on the post-mortem recovery procedure, explaining our defect detection mechanism, policy-driven recovery, and post-restart reintegration of components. We also discuss specific recovery steps for network, block device, and character device driver failures. Finally, we evaluate our design using performance measurements, software fault-injection experiments, and an analysis of the reengineering effort.

**Keywords:** Operating System Dependability, Failure Resilience, Device Driver Recovery

## 1. Introduction
While the ideal of bug-free software remains elusive, current systems must cope with the reality of software containing bugs. A key question is: "Can we build dependable systems out of unreliable, buggy components?" This paper addresses this question, particularly in the context of device drivers and other operating system extensions, which are known to be prone to failures and can disrupt normal operation.

Failure-resilient designs are common in many areas. For example, RAIDs continue functioning even if a drive fails, ECC memories correct bit errors, and TCP provides reliable data transport despite packet loss or corruption. In the application layer, init automatically respawns crashed daemons in some UNIX variants. These examples show how software can mask underlying failures and allow the system to continue as though no errors had occurred.

We extend these ideas to the operating system internals, focusing on tolerating and masking failures of device drivers and other extensions. Recovering from such failures is crucial because these components are often written by third parties and tend to be buggy. Our system consists of multiple isolated user-mode components structured to automatically detect and repair a broad range of defects without affecting running processes or bothering the user.

## 2. Contribution
We have built a failure-resilient operating system based on MINIX 3, which runs all servers and drivers as isolated user-mode processes. This architecture allows us to add mechanisms to detect and transparently repair failures. While several aspects of MINIX 3 have been published, this is the first detailed discussion of the recovery of malfunctioning device drivers.

The remainder of this paper is organized as follows:
- Section 2 surveys related work in operating system dependability.
- Section 3 presents our failure model.
- Section 4 discusses our isolation architecture.
- Section 5 introduces the defect detection mechanisms and policy-driven recovery procedure.
- Section 6 illustrates our ideas with concrete recovery schemes for network, block device, and character device drivers.
- Section 7 evaluates our system using performance measurements, software fault-injection, and an analysis of reengineering effort.
- Section 8 concludes the paper.

## 3. Dependability Context
Our work is part of the broader context of operating system dependability. Our failure-resilient design represents a special case of microreboots, which aim to reduce the mean time to recover (MTTR) and increase system availability. We apply this idea to drivers and other operating system extensions.

Several failure-resilient designs exist in the context of operating systems. Solaris 10, Nooks, SafeDrive, QNX, and paravirtualization are examples of approaches that support recovery through various mechanisms. However, our approach takes the UNIX model to its logical conclusion by putting all servers and drivers in unprivileged user-mode processes and supporting restarts through a flexible, policy-driven recovery procedure.

## 4. Failure Model
In our work, we define a failure as a deviation from the specified service, such as a driver crash. We are less interested in erroneous system states or the exact underlying faults. Once a failure is detected, we perform a microreboot of the failing or failed component to repair the system. This approach is based on the observation that many software failures are cured by rebooting, even when the exact causes are unknown.

Our system is designed to handle intermittent and transient failures in device drivers, which are a major source of downtime in software systems. Failures that can be handled include failstop and crash failures, panics due to internal inconsistencies, race conditions, and aging bugs. While our design cannot deal with Byzantine failures, algorithmic and deterministic failures, or performance failures, it can significantly improve system dependability by addressing transient driver failures.

## 5. Isolation Architecture
Strict isolation of components is crucial for enabling recovery. Each server and driver is encapsulated in a private, hardware-protected address space to prevent memory corruption. The kernel provides a virtual copy call to enable processes to copy data between address spaces in a capability-protected manner. Direct memory access (DMA) is managed through a trusted driver and an I/O MMU, ensuring safe I/O operations.

In addition, we have reduced the privileges of each component to minimize the impact of a failure. This isolation ensures that a problem in one component does not spread to others, allowing the system to continue running with minimal disruption.

By combining these mechanisms, our system can effectively detect and recover from a wide range of device driver failures, improving overall operating system dependability.