title:Failure Resilience for Device Drivers
author:Jorrit N. Herder and
Herbert Bos and
Ben Gras and
Philip Homburg and
Andrew S. Tanenbaum
Failure Resilience for Device Drivers
Jorrit N. Herder, Herbert Bos, Ben Gras, Philip Homburg, and Andrew S. Tanenbaum
Computer Science Dept., Vrije Universiteit, Amsterdam, The Netherlands
{jnherder, herbertb, beng, philip, ast}@cs.vu.nl
Abstract
Studies have shown that device drivers and exten-
sions contain 3–7 times more bugs than other operating
system code and thus are more likely to fail. Therefore,
we present a failure-resilient operating system design
that can recover from dead drivers and other critical
components—primarily through monitoring and replac-
ing malfunctioning components on the ﬂy—transparent
to applications and without user intervention. This pa-
per focuses on the post-mortem recovery procedure. We
explain the working of our defect detection mechanism,
the policy-driven recovery procedure, and post-restart
reintegration of the components.
Furthermore, we
discuss the concrete steps taken to recover from net-
work, block device, and character device driver failures.
Finally, we evaluate our design using performance
measurements,
software fault-injection experiments,
and an analysis of the reengineering effort.
Keywords: Operating System Dependability, Fail-
ure Resilience, Device Driver Recovery.
1 INTRODUCTION
Perhaps someday software will be bugfree, but for the
moment all software contains bugs and we had better
learn to coexist with them. Nevertheless, a question we
have posed is: “Can we build dependable systems out
of unreliable, buggy components?” In particular, we ad-
dress the problem of failures in device drivers and other
operating system extensions. In most operating systems,
such failures can disrupt normal operation.
In many other areas, failure-resilient designs are
common. For example, RAIDs are disk arrays that con-
tinue functioning even in the face of drive failures. ECC
memories can detect and correct bit errors transparently
without affecting program execution. Disks, CD-ROMs,
and DVDs also contain error-correcting codes so that
read errors can be corrected on the ﬂy. The TCP pro-
tocol provides reliable data transport, even in the face of
lost, misordered, or garbled packets. DNS can transpar-
ently deal with crashed root servers. Finally, init auto-
matically respawns crashed daemons in the application
layer of some UNIX variants. In all these cases, soft-
ware masks the underlying failures and allows the sys-
tem to continue as though no errors had occurred.
In this paper, we extend these ideas to the operating
system internals. In particular, we want to tolerate and
mask failures of device drivers and other extensions. Re-
covery from such failures is particularly important, since
extensions are generally written by third parties and tend
to be buggy [9, 39]. Unfortunately, recovering from
driver failures is also hard, primarily because drivers are
closely tied to the rest of the operating system. In addi-
tion, it is sometimes impossible to tell whether a driver
crash has led to data loss. Nevertheless, we have de-
signed an operating system consisting of multiple iso-
lated user-mode components that are structured in such
a way that the system can automatically detect and repair
a broad range of defects [10, 15, 30], without affecting
running processes or bothering the user. The architec-
ture of this system is shown in Fig. 1.
App
App
App
Process
Manager
Fork Drivers
Notify on Exit
e
c
a
p
S
r
e
s
U
Virtual File System
File Servers
Network Server
(V)FS
Servers
Untrusted Drivers
Expected to Fail
Device
Drivers
Microkernel
Reinc.
Server
Monitor System
Repair Defects
Data
Store
Publish Config
Backup State
Figure 1: Architecture of our failure-resilient operating sys-
tem that can recover from malfunctioning device drivers.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007In this paper, we focus on the post-mortem recov-
ery procedure that allows the system to continue nor-
mal operation in the event of otherwise catastrophic fail-
ures. We rely on a stable set of servers to deal with un-
trusted components. The reincarnation server manages
all system processes and constantly monitors the sys-
tem’s health. When a problem is detected, it executes
a policy script associated with the malfunctioning com-
ponent to guide the recovery procedure. The data store
provides naming services and can be used to recover lost
state after a crash. Changes in the system are broad-
cast to dependent components through the data store’s
publish-subscribe mechanisms in order to initiate further
recovery and mask the problem to higher levels. This is
how failure resilience works: a failure is detected, the
defect is repaired, and the system continues running all
the time with minimum disturbance to other processes.
While our design can recover from failures in all
kinds of components, including the TCP stack and sim-
ple servers, the focus of our research is to deal with
buggy device drivers. Studies on software dependability
report fault densities of 2–75 bugs per 1000 lines of exe-
cutable code [4, 41], but drivers and other extensions—
which typically comprise 70% of the operating system
code—have a reported error rate that is 3 to 7 times
higher [9], and thus are relatively failure-prone. For ex-
ample, 85% of Windows XP crashes can be traced back
to driver failures [39]. Recovery of drivers thus is an ef-
fective way to improve operating system dependability.
1.1 Contribution
We have built a failure-resilient operating system
to improve operating system dependability. Our work
is based on MINIX 3 [28], which runs all servers and
drivers as isolated user-mode processes, but is also ap-
plicable to other operating systems. This architecture al-
lowed us to add mechanisms to detect and transparently
repair failures, resulting in the design shown in Fig. 1.
While several aspects of MINIX 3 have been published
before [19, 20, 18], this is the ﬁrst time we discuss the
recovery of malfunctioning device drivers in detail.
The remainder of this paper is organized as follows.
We ﬁrst survey related work in operating system de-
pendability (Sec. 2). Then we present our failure model
(Sec. 3), discuss our isolation architecture (Sec. 4), in-
troduce the defect detection mechanisms and policy-
driven recovery procedure (Sec. 5). We illustrate our
ideas with concrete recovery schemes for network, block
device, and character device drivers (Sec. 6). We also
evaluate our system using performance measurements,
software fault-injection, and an analysis of reengineer-
ing effort (Sec. 7). Finally, we conclude (Sec. 8).
2 DEPENDABILITY CONTEXT
This work needs to be placed in the context of operat-
ing system dependability. Our failure-resilient design
represents a special case of microreboots [7, 8], which
promote reducing the mean time to recover (MTTR) in
order to increase system availability. We apply this idea
to drivers and other operating system extensions.
Several failure-resilient designs exist in the context
of operating systems. Solaris 10 [38] provides tools
to manage UNIX services running in the application
layer and can automatically restart crashed daemons.
Nooks [39, 40] implements in-kernel wrapping of com-
ponents to isolate driver failures and supports recov-
ery through shadow drivers that monitor communica-
tion between the kernel and driver. SafeDrive [44] com-
bines wrapping of the system API with type safety for
extensions written in C and provides recovery similar
to Nooks. QNX [21] provides software watchdogs to
catch and recover from intermittent failures in memory-
protected subsystems. Paravirtualization [13, 26] sup-
ports driver isolation by running each driver in a dedi-
cated, paravirtualized operating system and can recover
by rebooting the failed virtual machine. In contrast to
these approaches, we take the UNIX model to its logical
conclusion by putting all servers and drivers in unprivi-
leged user-mode processes and support restarts through
a ﬂexible, policy-driven recovery procedure.
Numerous other approaches attempt
to increase
operating system dependability by isolating compo-
nents. Virtual machine approaches like VM/370 [35],
VMware [36], and Xen [3] are powerful tools for run-
ning multiple services in isolation, but cannot prevent a
bug in a device driver from crashing the hosted operating
system. User-mode drivers have been used before, but,
for example, Mach [12] leaves lots of code in the ker-
nel, whereas L4Linux [16] runs the operating system in
a single server on top of L4 [24]. Other designs compart-
mentalize the entire operating system, including GNU
Hurd [6], SawMill Linux [14], and NIZZA [17]. Re-
cently, user-mode drivers also made their way into com-
modity systems such as Linux [25] and Windows [27].
These systems differ from our work in that we combine
proper isolation in user space with driver recovery.
Finally, language-based protection and formal veri-
ﬁcation can also be used to build dependable systems.
OKE [5] uses instrumented object code to load safely
kernel extensions. VFiasco [22] is an attempt at for-
mal veriﬁcation of the L4 microkernel. Singularity [23]
uses type safety to provide software isolated processes.
These techniques are complementary to our approach,
since our user-mode servers and drivers can be imple-
mented in a programming language of choice.
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20073 FAILURE MODEL
4 ISOLATION ARCHITECTURE
In our work, we are interested in the notion of a failure,
that is, a deviation from the speciﬁed service [31], such
as a driver crash. We are less interested in erroneous sys-
tem states or the exact underlying faults. Once a failure
has been detected, as described in Sec. 5.1, we perform a
microreboot [7] of the failing or failed component in an
attempt to repair the system. The underlying idea is that
a large fraction of software failures are cured by reboot-
ing, even when the exact failure causes are unknown.
Our system is designed to deal with intermittent
and transient failures in device drivers. We believe
that this focus has great potential to improve operat-
ing system dependability, because (1) transient failures
represent a main source of downtime in software sys-
tems [10, 15, 30] and (2) device drivers are relatively
failure-prone [9, 39]. Failures that can be handled by
our design include failstop and crash failures, such as
exceptions triggered by unexpected input; panics due to
internal inconsistencies; race conditions caused by un-
expected hardware timing issues; and aging bugs that
cause a component to fail over time, for example, due
to memory leaks. While hard to track down, these kinds
of problems tend to go away after a restart and can be
cured by replacing the malfunctioning component with
a fresh copy. In fact, our design can deal with a some-
what broader range of failures, since we can also start a
newer or patched version of the driver, if available.
Of course, there are limits to the failures our de-
sign can deal with. To start with, we cannot deal with
Byzantine failures, including random or malicious be-
havior. For example, consider a disk driver that ac-
cepts a write request and responds normally, but, in fact,
writes garbage to the disk or nothing at all. Such bugs
are virtually impossible to catch in any system. In gen-
eral, end-to-end checksums are required to prevent silent
data corruption [33]. Furthermore, algorithmic and de-
terministic failures that repeat after a restart cannot be
cured, but these can be found more easily through test-
ing. Our design also cannot deal with performance fail-
ures where timing speciﬁcations are not met, although
the use of heartbeat messages helps to detect unrespon-
sive components. Finally, our system cannot recover
when the hardware is broken or cannot be reinitialized
by a restarted driver.
It may be possible, however, to
perform a hardware test and switch to a redundant hard-
ware interface, if available. More research in this area is
needed, though.
In the remainder of this paper we focus on the re-
covery from transient driver failures, which, as argued
above, represent an important area where our design
helps to improve system dependability.
Strict isolation of components is a crucial prerequisite
to enable recovery, since it prevents problems in one
server or driver from spreading to a different one, in
the same way that a bug in a compiler process cannot
normally affect a browser process. Taking this notion
to our compartmentalized operating system design, for
example, a user-mode sound driver that dereferences an
invalid pointer is killed by the process manager, causing
the sound to stop, but leaving the rest of the system un-
affected. Although the precise mechanisms are outside
the scope of this paper, a brief summary of MINIX 3’s
isolation architecture is in place.
To start with, each server and driver is encapsulated
in a private, hardware-protected address space to pre-
vent memory corruption through bad pointers and unau-
thorized access attempts, just like for normal applica-
tions. Because the memory management unit (MMU)
denies access to other address spaces, the kernel pro-
vides a virtual copy call that enables processes to copy
data between address spaces in a capability-protected
manner. A process that wants to grant selective access
to its memory needs to create a capability describing the
precise memory area and access rights and pass an index
to it to the other party.
Direct memory access (DMA) is a powerful I/O
mechanisms that potentially can be used to bypass the
memory protection offered by our system. On com-
modity hardware, we can deny access to the DMA con-
troller’s I/O ports, and have a trusted driver mediate all
access attempts. However, this requires manual check-
ing of each device to see if it uses DMA. Modern hard-
ware provides effective protection in the form of an I/O
MMU [1]. To perform DMA safely a driver should ﬁrst
request the kernel to set up the I/O MMU by passing
an index to a memory capability similar to the one de-
scribed above. The overhead of this protection is a few
microseconds to perform the kernel call, which is gen-
erally amortized over the costs of the I/O operation.
In addition, we have reduced the privileges of each