create a labeled training dataset to build our attack model.
D. Adversary Model
We assume a similar adversary model as Naghibijouybari
et al. [41], in which a spy CUDA application (or spy for
short) and a victim CUDA application (or victim for short) are
3
 Victim VM  Spy VM  GPU Cloud Instance DNN  Model Malicious  CUDA Code Direct Access Execution  Side Channel (CUPTI) Victim Spy launched on the same GPU (illustrated in Figure 1). This is
feasible in the cloud settings, where multiple applications can
share the same physical GPU through I/O pass-through [60],
and prior works have demonstrated concrete techniques for
machine co-location [4], [53], [67]. We assume those tech-
niques are executed by the adversary.
We assume the adversary is able to run the spy while train-
ing is performed by the victim. The spy uses CUPTI reading
associated with each kernel execution to infer operations run
by the victim. The adversary has proﬁled combinations of
layers and their hyper-parameters on the same GPU to be
used by the victim. The goal of the adversary is to learn
which layers and hyper-parameters are chosen by the victim
model. We target the training stage because it typically takes
hours or days and the same layer sequence is executed many
times [12], [21], leaving abundant opportunities for the spy to
extract side-channel information about the victim. Comparing
with the work by Naghibijouybari et al. [41], one prominent
difference is that our attack does not rely on the activation
of Nvidia’s Multi-Process Service (MPS) feature. This makes
our attack more practical as MPS is disabled by default 2.
While Nvidia has released a patch [47] recently to restrict
CUPTI access, we found this mitigation can be easily bypassed
on the cloud. Based on our test on Amazon EC2, when we
rent a VM as the root user, initially CUPTI access is blocked.
But after downgrading the driver version using root’s
privilege from 418.40.04 (patched) to 384.130 (unpatched),
CUPTI can be accessed. To notice, when spy and victim
are on two different VMs sharing the same GPU, the spy can
freely downgrade the driver in its own VM, and such action is
completely invisible to the victim. Therefore, the GPU side-
channel based on CUPTI is still valid. We pick Nvidia as the
testing platform due to its high popularity in deep learning.
Our tested GPU is from Pascal architecture [46], which was
released in 2016 and widely used now.
A number of recent works studied attacks against DNN
conﬁdentiality [5], [13], [23]–[25], [41], [63], [65] and they
also consider layers and their hyper-parameters as secret. The
works are done by Hua et al. [25] and Batina et al. [5] were
able to reveal neuron weights but they rely on physical access
to the DNN accelerator. We will investigate how our adversary
(remote) can infer model weights as future work.
III. UNDERSTANDING MODEL EXECUTION ON GPU
In this section, we report our analysis of how DNN is
executed on GPU and motivate the design of MoSConS. We
ﬁrst describe how GPU kernels are scheduled in Section III-A.
Then, we demonstrate our observation regarding GPU con-
tention in Section III-B. Finally, we show the design of the spy
program and the difference between DNN layers’ execution
traces in Section III-C.
Experiment platform. Our study is carried out on a work-
station equipped with Nvidia GeForce GTX 1080 TI. The
2MPS
is
after
nvidia-cuda-mps-control -d
enabled
a
user
manually
runs
4
graphics driver version is 384.11. The CUDA version is
V9.0.176 and the cuDNN version is 7.4.2. The workstation
has installed Ubuntu 16.04 and we use Tensorﬂow 1.12.0.
A. Scheduling of GPU Kernels
When a DNN model is to be executed, the system stack
translates the model structure into the execution plan and the
hardware decides how to schedule the computation workload.
Unfortunately, not all the details were documented by the
stakeholders. Below we summarize the insights we learned
through proﬁling the execution of DNN models.
TensorFlow scheduling. When DNN model is executed on
Nvidia GPU,
the execution will be carried out by GPU
streams and each stream consists of CUDA kernels that invoke
Nvidia’s cuDNN APIs [11] like Conv2D and BiasAdd.
We found TensorFlow groups kernels under I/O streams
(one stream for CPU-to-GPU data transfer and one for GPU-
to-CPU transfer) and compute streams (one or more streams
for feed-forward and back-propagation computation). Those
streams execute in parallel while the overlap between I/O
and compute streams is fairly small (less than 1% of time
period when we train VGG [57] and ResNet [22]). Inside each
stream, the same kernel sequence is executed under different
training iterations. As such, proﬁling the layer computation
under the compute streams and using such information to
infer the victim’s DNN execution later is feasible. In addition
to TensorFlow, we found PyTorch and Caffe schedule GPU
similarly (e.g., serializing kernels during training) [26], [51].
As such, our attack is expected to succeed on other stacks too.
GPU Scheduling. When two GPU kernels are executed to-
gether (e.g., spy and victim), contention about GPU resources,
like cache and atomic memory units, will be introduced. To
handle contention, two approaches were developed by Nvidia.
The ﬁrst approach is to interleave kernel execution and
switch context based on time-sliced scheduler [9]. A number
of time-slices are given to each kernel, and they are scheduled
in a mostly round-robin manner. For example, assuming T SAi
and T SBi are the i-th time-slices for spy and victim kernels,
the execution order will be T SA1, T SB1, ..., T SAi, T SBi.
The duration of each time-slice depends on the priority of the
computing task. When a time slice expires, preemption will
force context switching between kernels.
The second approach is to let two kernels run concurrently
under the same GPU context, enabled by a CUDA enhance-
ment named Multi-Process Service (MPS) [43]. The MPS
service runs like a delegate and let other CUDA applications
connect to it and share context, through its scheduler.
B. GPU Contention
The two GPU scheduling approaches introduce a different
penalty to host applications and we compare them below.
For scheduling with MPS, Naghibijouybari et al. reverse-
engineered the co-location strategy of MPS and showed Left-
over policy is adopted such that a kernel takes the idle SM
not occupied by the prior kernel [40]. Nonetheless, when both
kernels attempt to occupy all SMs (i.e., creating N blocks if
Fig. 2. Spy and victim applications with MPS enabled.
Fig. 3. Spy and victim applications with MPS disabled.
the number of SMs is N), they can achieve co-location within
an SM. As a result, there is direct contention of the same
resources at the same time.
For the time-sliced scheduling without MPS, a kernel
does not share context with another kernel. However, context
switching between two kernels causes a performance penalty
to the following kernel. For instance, when the previous kernel
holds a large number of L2 cache entries, higher data-access
overhead will be observed for the next kernel.
The work done by Naghibijouybari et al. [41] infers the
neuron number of the input layer of a DNN model under the
MPS setting, but we found separating the layers and learning
their secret is infeasible. Figure 2 and Figure 3 show the
CUPTI readings under the TensorFlow timeline with MPS
on and off. It turns out MPS increases the execution time
of the spy kernel signiﬁcantly. In most cases, the spy kernel
has to wait for a victim’s training iteration. As such, when
the spy program turns on CUPTI, only one sample can be
obtained for one entire iteration. We speculate the slow-down
of spy kernel is caused by the uneven resource allocation of
MPS service (TensorFlow kernels dominate the resource). On
the other hand, time-sliced scheduling is able to make fairer
sharing between kernels when they are from different hosts.
While Naghibijouybari et al. [41] mentioned that they use
“several hundred consecutive kernels in spy” to cover “one
whole victim kernel execution”, their goal is to ensure co-
location of every SM with the victim, rather than obtaining
CUPTI readings about different DNN layers. We replicate their
setting and found those all kernels ﬁnish at the same time.
C. Analysis of DNN operations
Our attack depends on one assumption: different DNN
operations result in different context-switching penalty, which
is measurable by the spy. Below we test this assumption and
report our ﬁndings.
Design of spy. We experiment with the spy kernel running 4
blocks and 32 threads to contend with the victim kernels. As
such, 4 SMs will be taken by the spy. Given that TensorFlow
typically takes all SMs, 4 SMs for spy are enough to observe
the penalty caused by context overlapping.
Inside the spy kernel, dummy operations like matrix mul-
tiplications are executed to measure the context-switching
penalty caused by the prior victim kernel. Table I shows
the CUPTI readings about 5 different spy kernels when the
victim kernel runs MatMul (Matrix Multiplication). It turns
out when the spy kernel runs Conv200 (200x200 convolu-
tional operation), the best result is achieved, with the largest
reading on average and the smallest ratio between the standard
deviation and the average. In fact, this operation has the largest
overlap with DNN operations in terms of the requested units
and memory-access patterns, which explains its effectiveness.
Another advantage of Conv200 kernel is that its execution
time is shorter (2.5ms) comparing to other kernels, so a higher
sampling rate can be achieved.
Impact of DNN operations on spy. MoSConS would not
work if different DNN operations have same impact on spy’s
CUPTI readings. To assess the impact, we ﬁrst test different
DNN operations and collect the CUPTI readings with the spy
kernel Conv200. Table II shows the result, which clearly
indicates operation type has different impact on spy.
TABLE I
CUPTI READING OF DIFFERENT SPY KERNELS WHEN MA TMU L IS RUN BY
VICTIM. EVENT 1 IS FB SUBP1 WRITE SECTORS. EVENT 2 IS
FB SUBP0 READ SECTORS. WE RUN EACH COMBINATION OF SPY AND
VICTIM FOR 10,000 TIMES. THE RESULT IS REPRESENTED AS “AVERAGE
(STANDARD DEVIATION)”.
Spy Kernel
VectorAdd
VectorMul
MatMul
Conv100
Conv200
Event 1
2.64(18.57)
2.44(2.56)
15.94(7.51)
629.51(115.23)
2525.85(16.59)
Event 2
164.50(68.28)
163.39(22.94)
1472.27(82.92)
1548.48(256.18)
2489.24(175.62)
TABLE II
CUPTI READINGS OF CO N V200 SPY KERNEL WHEN DIFFERENT VICTIM
OPERATIONS ARE EXECUTED. NOP MEANS THE VICTIM KERNEL IS IDLE.
THE RESULT IS REPRESENTED SIMILARLY TO TABLE I.
Victim Op
MatMul
Conv2D
ReLU
BiasAdd
Sigmoid
NOP
Event 1
2525.85(16.59)
2542.45(28.73)
0(0)
0(0)
0(0)
34943.46(232.18)
Event 2
2489.24(175.62)
5695.03(976.65)
1164.95(1076.20)
948.89(777.74)
1287.55(1017.06)
18454.04(5370.17)
IV. MODEL EXTRACTION ATTACK
Challenges. Given that DNN structure can be seen as a
sequence of operations (or ops for short) and different ops
result in different samples read by a spy, a natural solution
for spy would be separating the sequence of samples ﬁrst and
then identifying the individual op from each segmentation.
This approach is widely used for side-channel attacks when
the secret is a sequence, e.g., PIN inputted on smartwatch [33],
[62]. However, this approach does not ﬁt our problem for
three reasons. 1) spy’s sampling rate is relatively low (less
than 1K samples per second) comparing to other powerful
side-channels (e.g., 2 Giga samples per second for power
5
analysis [38]), making op transitions and short ops un-
detectable. 2) computation-intensive ops like Conv2D take
much longer time to compute, resulting in uneven samples
among ops. 3) the CUPTI readings ﬂuctuate from time to time
even for the same op (e.g., the standard deviation of ReLU is
1076.20 shown in Table II). Those observations indicate the
complex relations between DNN structure and the context-
switching side-channel, which cannot be addressed by simple
statistical or machine-learning models. We tackle those issues
by launching a novel “slow-down” attack ﬁrst and then using
LSTM (Long short-term memory) models to detect different
DNN ops and hyper-parameters.
Slow-down attack on victim kernels. Though by shortening
the execution time of a spy kernel we can collect more
samples, we found the victim ops are less distinguishable.
Therefore, we choose to elongate the victim ’s execution time
to obtain more samples. Our approach is to let the attacker
launch more kernels inside the spy program, through which the
time-sliced scheduler will reduce the duration of each time-
slice for victim.
In particular, we tested hundreds of kernel parameter com-
binations by changing , with the
value of each ranging from 1 to 32 (multiplied by 2). We
found there is an upper-bound of the slow-down ratio, such
that higher numbers of kernels/blocks/threads are not always
more effective. After the above analysis, we decide to use 8
kernels for spy and put 2 kernels into a group with the same
settings. Assume each group is named as Gi while 0 ≤ i ≤ 3.
The number of blocks and threads for each kernel are set to
4× 2i and 4× 2i × 32 . Our empirical result shows the victim
can be slowed down for 17 times while the spy is slowed
down for less than 3 times. To notice, we can run the slow-
down attack sparingly so the overall victim training time will
be increased only slightly. In Section V-F, we elaborate on the
performance overhead of slow-down attacks on the victim.
Overview of the inference attack. Before the actual attack,
the adversary proﬁles models under different families to train
the inference models. During the attack, the adversary waits
for the bootstrapping of a TensorFlow process (e.g., through
monitoring GPU usage) before launching the inference attack.
Once training is started, the spy kernels are launched. When
enough CUPTI readings are collected, all spy’s kernels will be
terminated. Next, the spy’s CUPTI readings will be processed
by the inference models to predict the op sequence of the
victim, which we call OpSeq for short. The hyper-parameters
will be inferred as well and attached to OpSeq.
We found the unique characteristics of DNN training can
be exploited to develop sub-models for inferring layers and
hyper-parameters separately with high accuracy. 1) A spy
can obtain multiple samples for convolutional ops (conv
for short) and matrix multiplication ops (MatMul) due to
their long execution duration. 2) DNN training usually takes
many iterations resulting in the repeated OpSeq of the victim.
Between iterations, there is a gap where no kernel is executed.
3) Similar to natural-language sentences, DNN model has
6
STRUCTURES OF THE 5 LSTM MODELS USED IN MOSCO NS.
TABLE III
Vlong Mop
LSTM 256
LSTM 256
Vop
Mhp
LSTM 128
Mlong