```bash
chown -R user:user /home/user
chmod -R go-rwx /home/user
```
Useful resources:
- [What steps to add a user to a system without using useradd/adduser?](https://unix.stackexchange.com/questions/153225/what-steps-to-add-a-user-to-a-system-without-using-useradd-adduser)
Why do we need mktemp command? Present an example of use.
mktemp randomizes the name. It is very important from the security point of view.
Just imagine that you do something like:
```bash
echo "random_string" > /tmp/temp-file
```
in your root-running script. And someone (who has read your script) does
```bash
ln -s /etc/passwd /tmp/temp-file
```
The mktemp command could help you in this situation:
```bash
TEMP=$(mktemp /tmp/temp-file.XXXXXXXX)
echo "random_string" > ${TEMP}
```
Now this ln /etc/passwd attack will not work.
Is it safe to attach the strace to a running process on the production? What are the consequences?
`strace` is the system call tracer for Linux. It currently uses the arcane `ptrace()` (process trace) debugging interface, which operates in a violent manner: **pausing the target process** for each syscall so that the debugger can read state. And doing this twice: when the syscall begins, and when it ends.
This means `strace` pauses your application twice for each syscall, and context-switches each time between the application and `strace`. It's like putting traffic metering lights on your application.
Cons:
- can cause significant and sometimes massive performance overhead, in the worst case, slowing the target application by over 100x. This may not only make it unsuitable for production use, but any timing information may also be so distorted as to be misleading
- can't trace multiple processes simultaneously (with the exception of followed children)
- visibility is limited to the system call interface
Useful resources:
- [strace Wow Much Syscall (original)](http://www.brendangregg.com/blog/2014-05-11/strace-wow-much-syscall.html)
What is the easiest, safest and most portable way to remove -rf directory entry?
They're effective but not optimally portable:
- rm -- -fr
- perl -le 'unlink("-fr");'
People who go on about shell command line quoting and character escaping are almost as dangerous as those who simply don't even recognize why a file name like that poses any problem at all.
The most portable solution:
```bash
rm ./-fr
```
Write a simple bash script (or pair of scripts) to backup and restore your system. ***
To be completed.
What are salted hashes? Generate the password with salt for the /etc/shadow file.
**Salt** at its most fundamental level is random data. When a properly protected password system receives a new password, it will create a hashed value for that password, create a new random salt value, and then store that combined value in its database. This helps defend against dictionary attacks and known hash attacks.
For example, if a user uses the same password on two different systems, if they used the same hashing algorithm, they could end up with the same hash value. However, if even one of the systems uses salt with its hashes, the values will be different.
The encrypted passwords in `/etc/shadow` file are stored in the following format:
```bash
$ID$SALT$ENCRYPTED
```
The `$ID` indicates the type of encryption, the `$SALT` is a random (up to 16 characters) string and `$ENCRYPTED` is a password’s hash.
    Hash Type
    ID
    Hash Length
    MD5
    $1
    22 characters
    SHA-256
    $5
    43 characters
    SHA-512
    $6
    86 characters
Use the below commands from the Linux shell to generate hashed password for `/etc/shadow` with the random salt:
- Generate **MD5** password hash
```bash
python -c "import random,string,crypt; randomsalt = ''.join(random.sample(string.ascii_letters,8)); print crypt.crypt('MySecretPassword', '\$1\$%s\$' % randomsalt)"
```
- Generate **SHA-256** password hash
```bash
python -c "import random,string,crypt; randomsalt = ''.join(random.sample(string.ascii_letters,8)); print crypt.crypt('MySecretPassword', '\$5\$%s\$' % randomsalt)"
```
- Generate **SHA-512** password hash
```bash
python -c "import random,string,crypt; randomsalt = ''.join(random.sample(string.ascii_letters,8)); print crypt.crypt('MySecretPassword', '\$6\$%s\$' % randomsalt)"
```
###### Network Questions (27)
Create SPF records for your site to help control spam. ***
To be completed.
What is the difference between an authoritative and a nonauthoritative answer to a DNS query? ***
An authoritative DNS query answer comes from the server that contains the zone files for the domain queried. This is the name server that the domain administrator set up the DNS records on. A nonauthoriative answer comes from a name server that does not host the domain zone files (for example, a commonly used name server has the answer cached such as Google's 8.8.8.8 or OpenDNS 208.67.222.222).
If you try resolve hostname you get NXDOMAIN from host command. Your resolv.conf stores two nameservers but only second of this store this domain name. Why did not the local resolver check the second nameserver?
**NXDOMAIN** is nothing but non-existent Internet or Intranet domain name. If domain name is unable to resolved using the DNS, a condition called the **NXDOMAIN** occurred.
The default behavior for `resolv.conf` and the `resolver` is to try the servers in the order listed. The resolver will only try the next nameserver if the first nameserver times out.
The algorithm used is to try a name server, and if the query times out, try the next, until out of name servers, then repeat trying all the name servers until a maximum number of retries are made.
If a nameserver responds with **SERVFAIL** or a referral (**nofail**) or terminate query (**fail**) also only the first dns server will be used.
Example:
```
nameserver 192.168.250.20   # it's not a dns
nameserver 8.8.8.8          # not store gate.test.int
nameserver 127.0.0.1        # store gate.test.int
```
so if you check:
```
host -v -t a gate.test.int
Trying "gate.test.int"                        # trying first dns (192.168.250.20) but response is time out, so try the next nameserver
Host gate.test.int not found: 3(NXDOMAIN)     # ok but response is NXDOMAIN (not found this domain name)
Received 88 bytes from 8.8.8.8#53 in 43 ms
Received 88 bytes from 8.8.8.8#53 in 43 ms
                                              # so the last server in the list was not asked
```
To avoid this you can use e.g. `nslookup` command which will use the second nameserver if it receives a **SERVFAIL** from the first nameserver.
Useful resources:
- [Second nameserver in /etc/resolv.conf not picked up by wget](https://serverfault.com/questions/398837/second-nameserver-in-etc-resolv-conf-not-picked-up-by-wget)
Explore the current MTA configuration at your site. What are some of the special features of the MTA that are in use? ***
To be completed.
How to find a domain based on the IP address? What techniques/tools can you use? ***
To be completed.
Is it possible to have SSL certificate for IP address, not domain name?
It is possible (but rarely used) as long as it is a public IP address.
An SSL certificate is typically issued to a Fully Qualified Domain Name (FQDN) such as `https://www.domain.com`. However, some organizations need an SSL certificate issued to a public IP address. This option allows you to specify a public IP address as the Common Name in your Certificate Signing Request (CSR). The issued certificate can then be used to secure connections directly with the public IP address (e.g. `https://1.1.1.1`.).
According to the CA Browser forum, there may be compatibility issues with certificates for IP addresses unless the IP address is in both the commonName and subjectAltName fields. This is due to legacy SSL implementations which are not aligned with RFC 5280, notably, Windows OS prior to Windows 10.
Useful resources:
- [Are SSL certificates bound to the servers ip address?](https://stackoverflow.com/questions/1095780/are-ssl-certificates-bound-to-the-servers-ip-address)
- [SSL certificate for a public IP address?](https://serverfault.com/questions/193775/ssl-certificate-for-a-public-ip-address)
How do you do load testing and capacity planning for websites? ***
To be completed.
Useful resources:
- [How do you do load testing and capacity planning for web sites? (original)](https://serverfault.com/questions/350454/how-do-you-do-load-testing-and-capacity-planning-for-web-sites)
- [Can you help me with my capacity planning?](https://serverfault.com/questions/384686/can-you-help-me-with-my-capacity-planning)
- [How do you do load testing and capacity planning for databases?](https://serverfault.com/questions/350458/how-do-you-do-load-testing-and-capacity-planning-for-databases)
Developer reports a problem with connectivity to the remote service. Use /dev for troubleshooting.
```bash
#  - set remote host
#  - set destination port
# 1
timeout 1 bash -c "/" >/dev/null 2>&1 ; echo $?
# 2
timeout 1 bash -c 'cat  /' ; echo $?
# 2
&> echo > "/"
```
Useful resources:
- [Advanced Bash-Scripting Guide - /dev](http://www.tldp.org/LDP/abs/html/devref1.html#DEVTCP)
- [/dev/tcp as a weapon](https://securityreliks.wordpress.com/2010/08/20/devtcp-as-a-weapon/)
- [Test from shell script if remote TCP port is open](https://stackoverflow.com/questions/4922943/test-from-shell-script-if-remote-tcp-port-is-open)
How do I measure request and response times at once using curl?
`curl` supports formatted output for the details of the request (see the `curl` manpage for details, under `-w| -write-out 'format'`). For our purposes we’ll focus just on the timing details that are provided.
1. Create a new file, `curl-format.txt`, and paste in:
```bash
    time_namelookup:  %{time_namelookup}\n
       time_connect:  %{time_connect}\n
    time_appconnect:  %{time_appconnect}\n
   time_pretransfer:  %{time_pretransfer}\n
      time_redirect:  %{time_redirect}\n
 time_starttransfer:  %{time_starttransfer}\n
                    ----------\n
         time_total:  %{time_total}\n
```
2. Make a request:
```bash
curl -w "@curl-format.txt" -o /dev/null -s "http://example.com/"
```
What this does:
- `-w "@curl-format.txt"` - tells cURL to use our format file
- `-o /dev/null` - redirects the output of the request to /dev/null
- `-s` - tells cURL not to show a progress meter
`http://example.com/` is the URL we are requesting. Use quotes particularly if your URL has "&" query string parameters
You need to move ext4 journal on another disk/partition. What are the reasons for this? ***
To be completed.
Useful resources:
- [ext4: using external journal to optimize performance](https://raid6.com.au/posts/fs_ext4_external_journal/)
- [How to move an ext4 journal](https://unix.stackexchange.com/questions/278998/how-to-move-an-ext4-journal)
Does having Varnish in front of your website/app mean you don't need to care about load balancing or redundancy?
It depends. Varnish is a cache server, so its purpose is to cache contents and to act as a reverse proxy, to speed up retrieval of data and to lessen the load on the webserver.
Varnish can be also configured as a load-balancer for multiple web servers, but if we use just one Varnish server, this will become our single point of failure on our infrastructure.
A better solution to ensure load-balancing or redundancy will be a cluster of at least two Varnish instances, in active-active mode or active-passive mode.
What are hits, misses, and hit-for-pass in Varnish Cache?
A **hit** is a request which is successfully served from the cache, a **miss** is a request that goes through the cache but finds an empty cache and therefore has to be fetched from the origin, the **hit-for-pass** comes in when Varnish Cache realizes that one of the objects it has requested is uncacheable and will result in a pass.
Useful resources:
- [VCL rules for hits](https://book.varnish-software.com/4.0/chapters/VCL_Subroutines.html#vcl-vcl-hit)
- [VCL rules for hit-for-pass](https://book.varnish-software.com/4.0/chapters/VCL_Subroutines.html#hit-for-pass)
- [Example of the use](https://book.varnish-software.com/4.0/chapters/VCL_Basics.html#vcl-backend-response)
What is a reasonable TTL for cached content given the following parameters? ***
To be completed.
Developer says: htaccess is full of magic and it should be used. What is your opinion about using htaccess files? How has this effect on the web app
`.htaccess` files were born out of an era when shared hosting was common­place:
- sysadmins needed a way to allow multiple clients to access their server under different accounts, with different configurations for their web­sites.
The `.htaccess` file allowed them to modify how Apache works without having access to the entire server. These files can reside in any and every directory in the directory tree of the website and provide features to the directory and the files and folders inside it.
**It’s horrible for performance**
For `.htaccess` to work Apache needs to check EVERY directory in the requested path for the existence of a `.htaccess` file and if it exists it reads EVERY one of them and parses it. This happens for EVERY request. Remember that the second you change that file, it’s effective. This is because Apache reads it every time.
Every single request the web­server handles - even for the lowliest `.png` or `.css` file - causes Apache to:
- look for a `.htaccess` file in the directory of the current request
- then look for a `.htaccess` file in every directory from there up to the server root