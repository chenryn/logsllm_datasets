title:A Preemptive Deterministic Scheduling Algorithm for Multithreaded
Replicas
author:Claudio Basile and
Zbigniew Kalbarczyk and
Ravishankar K. Iyer
A Preemptive Deterministic Scheduling Algorithm for Multithreaded Replicas
Claudio Basile, Zbigniew Kalbarczyk, Ravi Iyer
Center for Reliable and High-Performance Computing
University of Illinois at Urbana-Champaign, IL 61081
(cid:0)basilecl,kalbar,iyer(cid:1)@crhc.uiuc.edu
Abstract
Software-based active replication is expensive in terms of
performance overhead. Multithreading can help improve per-
formance; however, thread scheduling is a source of nonde-
terminism in replica behavior. This paper presents a Preemp-
tive Deterministic Scheduling (PDS) algorithm for ensuring
deterministic replica behavior while preserving concurrency.
Threads are synchronized only on updates to the shared state.
A replica execution is broken into a sequence of rounds, and in
a round each thread can acquire up to two mutexes. When a
new round ﬁres, all threads’ mutex requests are known; thus, it
is possible to form a deterministic scheduling of mutex acquisi-
tions in the round. No inter-replica communication is required.
The algorithm is formally speciﬁed, and the proposed formalism
is used to prove its correctness.
Failure behavior and performance of a PDS algorithm’s im-
plementation are evaluated in a triplicated system and com-
pared with two existing solutions: nonpreemptive determinis-
tic schedulers and the Loose Synchronization Algorithm (LSA)
proposed by the authors in an earlier paper. The results show
that PDS outperforms nonpreemptive deterministic schedulers.
Compared with LSA, PDS has lower throughput; however, it
provides additional beneﬁts in terms of system dependability
and, hence, can be considered as a trade-off between perfor-
mance and dependability. These characteristics are investigated
with fault injection.
1 Introduction
Software-based active replication is a well-known technique
for providing fault tolerance using space redundancy and fault-
masking. Typically, replication is expensive in terms of perfor-
mance overhead [1–3]. Multithreading can help improve perfor-
mance by exploiting concurrency in thread execution;1 however,
since the thread/process scheduling performed by operating sys-
tems such as UNIX is asynchronous with replica execution, mul-
tithreaded replicas can exhibit nondeterministic behavior.
Solutions to replicate multithreaded applications/objects are
based on a nonpreemptive deterministic scheduler, which en-
forces the same thread interleaving on all replicas to achieve
determinism in replica state updates. In Eternal, determinism
is achieved by processing application threads (each of which
serves a client request) sequentially, which is effectively a
single-threaded solution [4].
In Transactional Drago, the ex-
ecutions of multiple logical threads are interleaved: if the run-
ning thread starts an I/O operation, the thread is suspended wait-
ing for I/O to complete while another thread may be scheduled
1Note that the performance overhead due to group communication is typi-
cally non-negligible and cannot be reduced by multithreading.
[5].2 Nonpreemptive deterministic schedules, although provid-
ing consistent replica behavior, cannot exploit concurrency in
thread execution, since only one physical thread is scheduled at
a given time. This results in poor scalability and performance of
the replicated system.
In contrast with nonpreemptive deterministic schedulers, [6]
describes a Loose Synchronization Algorithm (LSA) to main-
tain multithreaded replica consistency. The algorithm enforces
a compatible sequence of state updates in all replicas without
requiring the same thread interleaving. This is achieved by in-
tercepting mutex lock/unlock operations performed by applica-
tion threads on accessing the shared data. Intercepting mutex
lock/unlock operations was ﬁrst suggested in [7] for message-
logging-based recovery.
In the LSA algorithm, one replica (leader) decides the mutex
acquisition order and propagates it to other replicas (followers),
which enforce the leader-dictated order on the execution of their
threads. While the method preserves a large degree of concur-
rency, the presence of inter-replica communication can affect
overall system dependability (as shown in (cid:0) 5.2).
This paper proposes a Preemptive Deterministic Scheduling
(PDS) algorithm, with no leader/follower structure and no inter-
replica communication. In PDS, the key mechanism to ensure
determinism is the concept of a round (similar to the notion of
a barrier in parallel computing). A replica’s execution is broken
into a sequence of rounds, and in a round each thread can acquire
up to two mutexes. Any thread can trigger a new round. On re-
questing a mutex, a thread t checks if it can acquire the mutex
m it requests. If t cannot acquire m (e.g., m is held by another
thread or t has already acquired a maximum number of mutexes
for that round), it then checks whether all other threads are sus-
pended. If so, t starts a new round; otherwise, t is suspended.
When a new round is started, all threads’ mutex requests are
known, and therefore, a deterministic scheduling of mutex ac-
quisitions naturally occurs: threads simultaneously requesting
the same mutex acquire it according to increasing thread ids.
While typically most algorithms for providing fault tolerance
services are formally proved for correctness, this paper addi-
tionally advocates the use of error injection for sound assess-
ment of a proposed algorithm’s dependability. Employing error-
injection allows us to study the failure behavior of the over-
all system (including the algorithm) under realistic scenarios,
which cannot be achieved by using formal methods alone. The
major contributions of this paper are:
(cid:1) Speciﬁcation, proof of correctness, and implementation of
2To guarantee determinism, there are situations in which the algorithm waits
for the I/O to complete (keeping the CPU idle) although another thread can be
scheduled.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
a Preemptive Deterministic Scheduling (PDS) algorithm
for maintaining multithreaded replica consistency;
(cid:0) Study of performance-dependability trade-offs in select-
ing deterministic scheduling algorithms when replicating
multithreaded applications. The target strategies include
PDS, LSA, and a nonpreemptive deterministic scheduler
(NPDS).
A performance evaluation shows that PDS and LSA outper-
form NPDS by providing, respectively, two and ﬁve times more
throughput. This is because PDS and LSA can schedule multi-
ple threads to execute at the same time.
An error-injection-based evaluation of dependability shows
that because LSA relies on an inter-replica communication
channel for efﬁcient scheduling of mutex acquisitions, the algo-
rithm is more sensitive to the underlying communication layer’s
fail silence violations. This leads to a larger number of catas-
trophic failures (i.e., cases in which the entire replicated system
fails) for LSA than for PDS. (NPDS dependability characteris-
tics are similar to those of PDS because neither of the two al-
gorithms uses inter-replica communication.) Therefore, if mini-
mizing downtime is crucial (as it is for highly available systems),
PDS is a more appropriate choice than LSA; if performance con-
cerns have priority over minimizing downtime, then LSA can be
preferred to PDS.
Finally, error-injection experiments make it evident that er-
rors originating from the underlying communication layer (En-
semble [8] in our experiments) do propagate and can lead to
catastrophic failures of the entire replicated system. Conse-
quently, we argue that a middleware that provides fault toler-
ance services (e.g., reliable communications, process recovery)
to applications should itself be fault-tolerant.
2 Related Work
Early research on software-based replication focused on syn-
chronizing replicas at the interrupt level. For example, in the
TARGON/32 system, asynchronous events (e.g., UNIX signals)
are transformed into synchronous messages delivered to the des-
tination process and its backup [9]. In the Hypervisor system
(based on a primary/backup model) a virtual machine layer, be-
neath the operating system, uses a hardware register to count
the instructions executed by a primary machine between two
hardware interrupts [10]. This information is sent over the net-
work to a backup machine. The backup uses instruction counts
to reproduce the effects of the primary’s hardware interrupts
with respect to the backup’s instruction stream. Delta-4 pro-
vides semi-active replication with a leader/follower model and a
preemption-synchronization mechanism. When an interrupt ar-
rives, the leader determines the next preemption point at which
the interrupt will be served and sends this information to follow-
ers. The scheme is called semi-active because only the leader
interacts with the clients [11]. Synchronizing at the interrupt
level in software suffers from large performance overhead due
to the necessity of transferring ﬁne granularity synchronization
information over a network. In [12], nondeterminism is solved
for real-time systems off-line, via schedulability analysis.
More recent software approaches to replication attempt to
take advantage of the object-oriented paradigm and advocate
object replication rather then process replication (as discussed
above). AQuA provides transparent, single-threaded replication
to CORBA objects by means of proxies [1].
Solutions to replicate multithreaded applications/objects are
based on a nonpreemptive deterministic scheduler. In Eternal,
application threads are processed sequentially [4]. In Transac-
tional Drago, the executions of multiple logical threads are in-
terleaved on performing I/O operations [5].
The Loose Synchronization Algorithm [6], proposed by the
authors in an earlier paper, captures the natural concurrency in a
leader replica and projects it on follower replicas through inter-
replica communication. To the best of our knowledge, this is the
ﬁrst multithreaded solution for maintaining replica consistency.
3. System Model: Deﬁnitions and Assumptions
The system consists of a set of identical multithreaded pro-
cesses (replicas) running on different nodes. Each replica con-
sists of a set of threads (cid:1) and a set of mutexes  used to
protect partitions of shared data ((cid:1) and  can be inﬁnite). It
is assumed that the same thread/mutex ids are associated with
corresponding threads/mutexes of different replicas; this can be
enforced by using a hierarchical thread/mutex naming scheme
(e.g., as described in (cid:3) A). We deﬁne three atomic actions: (1)
requestm(cid:0) t, which corresponds to thread t requesting a mutex
m; (2) acquirem(cid:0) t, which corresponds to thread t acquiring
mutex m; and (3) releasem(cid:0) t, which corresponds to thread t
releasing a mutex m.
Deﬁnition 1 (Mutex Acquisition) A triple m(cid:0) t(cid:0) k (cid:4) (cid:1) 
 denotes a mutex acquisition made by thread t on mutex m; this
is the kth mutex acquisition made by t.
Expressing mutex acquisitions as triples emphasizes that they
are unique within each replica. To simplify the notation, how-
ever, a mutex acquisition m(cid:0) t(cid:0) k will be referred to as a pair
m(cid:0) t; k is retrieved by applying a function index to the pair
(e.g., k (cid:2) indexm(cid:0) t).
Two mutex acquisitions are deﬁned to be conﬂicting if they
are made by different threads on the same mutex. In general,
the order in which conﬂicting mutex acquisitions are made may
affect the result of a computation.
Deﬁnition 2 (History) The history Hr of a replica r is the se-
quence of mutex acquisitions by its threads at a given time. The
Hr
notation mi(cid:0) ti
(cid:1) mj(cid:0) tj indicates that mi(cid:0) ti temporally pre-
cedes mj(cid:0) tj in Hr.
Since threads within a replica r execute on the same node,
the order of the mutex acquisitions in Hr is determined by the
time (using the node’s local clock) at which the threads make
the acquisitions. Enforcing the same history on all replicas (un-
der the assumption of determinism as deﬁned later) makes all
replicas behave in the same way. This, however, is a stronger
requirement than necessary, since only the causal dependencies
between mutex acquisitions need to be preserved.
Deﬁnition 3 (Causal Precedence) The causal precedence be-
tween two mutex acquisitions mi(cid:0) ti and mj(cid:0) tj in a history
H
H, i.e., mi(cid:0) ti
(cid:0) mj(cid:0) tj, is deﬁned as the transitive closure of
the following relation:
1. ti (cid:2) tj (cid:6) mi(cid:0) ti
H
(cid:1) mj(cid:0) tj, for mutexes acquired by the
same thread, or
2. mi (cid:2) mj (cid:6) mi(cid:0) ti
tions.
H
(cid:1) mj(cid:0) tj, for conﬂicting mutex acquisi-
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Causal precedence implies temporal precedence, while the
reverse is not necessarily true. The notion of causal prece-
dence between two mutex acquisitions in a multithreaded pro-
cess is analogous to the notion of causal precedence between
two events in a distributed system [13]. Because concurrent
events in distributed systems are not causally related, concurrent
mutex acquisitions in a multithreaded process are those acquisi-
tions whose actual order of execution does not affect the result of
the computation. To preserve concurrency, we allow replicas to
schedule concurrent mutex acquisitions independently. Based
on the notion of causal precedence, the next deﬁnition intro-
duces the causal set of a mutex acquisition, which represents
all mutex acquisitions upon which a given mutex acquisition is
causally dependent.
Deﬁnition 4 (Causal Set) Given a mutex acquisition m(cid:0) t in a
history H, the causal set of m(cid:0) t is the set (cid:1)Hm(cid:0) t (cid:2) (cid:0)m(cid:0)(cid:0) t(cid:0) (cid:1)
H (cid:2) m(cid:0)(cid:0) t(cid:0)
H
(cid:0) m(cid:0) t(cid:3) (cid:4) (cid:0)m(cid:0) t(cid:3).
A deterministic scheduling algorithm must assume that
threads behave deterministically between two consecutive mu-
tex acquisitions. This assumption is somewhat similar to the
piecewise deterministic assumption made by proponents of
message-logging checkpointing [14]. While determinism is tra-
ditionally expressed in terms of state, the causal set is used as an
abstraction to represent a thread’s view of the replica’s state at
the moment of a given mutex acquisition.3 In this context, we re-
ﬁne the piecewise deterministic assumption made for message-
logging checkpointing as follows:
Deﬁnition 5 (Piecewise Thread Determinism) A thread t in a
replica r is piecewise deterministic iff given its last mutex ac-
quisition m(cid:0) t, the behavior of t is uniquely determined by
(cid:1)Hr m(cid:0) t and the replica’s initial state Sr
(cid:0). In particular, from
the initial state (i.e., before its ﬁrst mutex acquisition), t’s be-
havior is uniquely determined by Sr
(cid:0).
Because of this deﬁnition, outputs emitted by t between a mu-
tex acquisition m(cid:0) t and its next mutex acquisition are a func-
tion only of (cid:1)Hr m(cid:0) t and Sr
(cid:0). Moreover, race conditions are
precluded. Observe that a thread’s behavior in general depends
on the inputs the thread receives. Although the above deﬁnition
does not explicitly mention replica inputs, these can be incor-
porated in the model by requiring that corresponding threads of
different replicas are supplied the same sequence of inputs at the
same logical time.4
We now deﬁne the correctness properties of a determin-
istic scheduling algorithm. First however we introduce two
predicates: (1) t requests m, which holds if thread t has re-
quested a mutex m but has not acquired it yet (according to
the action deﬁnitions given earlier); and (2) t owns m, which
holds if thread t has acquired mutex m and has not released
these predicates are deﬁned as follows:5
it yet. Formally,
t requests m (cid:5) (cid:6) acquirem(cid:0) t (cid:7) requestm(cid:0) t; and t owns m (cid:5)
3For example, suppose that a thread t’s behavior after each of its mutex acqui-
sition m(cid:0) t(cid:0) k can be described as a function (cid:0)m(cid:0) t(cid:0) k(cid:0) LSt(cid:0) SSm, where LSt
and SSm are, respectively, t’s local state and the replica shared state associated
with m, both at the moment of m(cid:0) t(cid:0) k. Then, it can be shown that t’s behavior
can be expressed as a function (cid:1)(cid:1)H m(cid:0) t(cid:0) k(cid:0) Sr
(cid:0) is the replica initial
state.
(cid:0), were Sr
4This is relatively simple for blocking I/O operations.
5The linear temporal logic symbol (cid:0) denotes since. p (cid:0) q indicates that q
was true in the past and p has been true from that moment until now.
(cid:6) releasem(cid:0) t (cid:7) acquirem(cid:0) t. For a given application, we also
deﬁne a mutex dependency graph as a directed graph (cid:0) (cid:2) (cid:1)(cid:0) (cid:2)
where:
1. (cid:1) (cid:2) (cid:8) is the set of vertices, (cid:9) (cid:10) (cid:8)  (cid:8) is the set of edges,
and
2. t(cid:0) t(cid:0) (cid:1) (cid:2) iff thread t requests a mutex owned by thread t(cid:0),
i.e., (cid:12) m (cid:1)  (cid:2) t requests m (cid:14) t(cid:0) owns m.
The presence of a cycle in the mutex dependency graph is indi-
cated by the following predicate cycle:
cycle (cid:5) (cid:12) t(cid:0) (cid:2) (cid:2) (cid:2) tn (cid:1) (cid:1) (cid:8) (cid:12) m(cid:0) (cid:2) (cid:2) (cid:2) mn (cid:1) (cid:1) (cid:2)
n (cid:1)
(cid:0)
i(cid:2)(cid:0)
 ti requests mi (cid:14) ti(cid:1) mod n owns mi (cid:2)
Using the introduced predicates, the properties expected from a
correct multithreaded application are formalized:
Deﬁnition 6 (Correct Application) A multithreaded applica-
tion with piecewise deterministic threads is deﬁned to be correct
iff:
1. Each application thread releases only mutexes it owns;
2. If an application thread executes inﬁnitely often,6 then the
thread (a) eventually releases each mutex it acquires and
(b) requests mutexes inﬁnitely often;
3. The mutex dependency graph is acyclic.
The correctness of a deterministic scheduling algorithm is de-
ﬁned with respect a correct application as the conjunction of an
Internal Correctness property, which deﬁnes the behavior of the
algorithm only with respect to one replica, and an External Cor-
rectness property, which deﬁnes the behavior of the algorithm
with respect to other replicas. These two properties are formally
deﬁned below.
Property 1 (Internal Correctness) Given a replica r execut-
ing a correct application, the following conditions must always
hold:
1. (Mutual Exclusion) At most one thread holds a given mu-
tex: acquirem(cid:0) t (cid:3) acquirem(cid:0) t(cid:0) (cid:15) acquirem(cid:0) t (cid:3)
releasem(cid:0) t (cid:3) acquirem(cid:0) t(cid:0);
2. (No Lockout) If a thread requests a mutex,
then the
thread will eventually7 acquire the mutex: requestm(cid:0) t (cid:15)
 acquirem(cid:0) t.
Property 2 (External Correctness) Given two replicas r(cid:1) and
r(cid:6) executing a correct application and started from the same
initial state, two conditions must always hold:
1. (Safety) The causal sets of the mutexes acquired by both
replicas are the same: m(cid:0) t (cid:1) Hr(cid:0) (cid:16) Hr(cid:1) (cid:15) (cid:1)Hr(cid:0) m(cid:0) t (cid:2)
(cid:1)Hr(cid:1) m(cid:0) t;
2. (Liveness) Any mutex acquisition made by r(cid:1) is eventually
made by r(cid:6): m(cid:0) t (cid:1) Hr(cid:0) (cid:15)  m(cid:0) t (cid:1) Hr(cid:1).
In a replicated system, a deterministic scheduling algorithm
is required to satisfy the above correctness properties for any
pair of replicas.
6Given a predicate p, “p holds inﬁnitely often” indicates that from now on
p holds an inﬁnite number of times (this is denoted with   p using linear
temporal logic operators). In practice, parts (a) and (b) of the deﬁnition state
how threads must behave if they are always making progress.
7The linear temporal logic symbol  denotes eventually.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
m1
t1
  

  

m2