I is the set of all input links. After forwarding a packet with barrier
timestamp ğµ from input link ğ‘– to output link ğ‘œ, the switch performs
two updates. First, it updates register ğ‘…ğ‘– := ğµ. Second, it modifies
the barrier timestamp of the packet to ğµğ‘›ğ‘’ğ‘¤ as follows:
ğµğ‘›ğ‘’ğ‘¤ := min{ğ‘…ğ‘–|ğ‘– âˆˆ I}
(4.1)
Using (4.1), each switch independently derives the barrier times-
tamp based on all input links. As shown in Figure 5, barrier times-
tamps are aggregated hierarchically through layers of switches
hop-by-hop, and finally the receiver gets the barrier of all reach-
able hosts and links in the network. This algorithm maintains the
property of barrier timestamps, given the FIFO property of each
network link.
When the receiver receives a packet with barrier timestamp ğµ, it
first buffers the packet in a priority queue that sorts packets based
on the message timestamp. The receiver knows that the message
timestamp of all future arrival packets will be larger than ğµ. Hence,
it delivers all buffered packets with the message timestamp below
ğµ to the application for processing. If a process receives a message
with timestamp above ğµ, it is dropped, and a NAK message is
returned to the sender. So, if a link or switch is not strictly FIFO, out-
of-order messages will not violate correctness. Notice that barrier
Figure 5: Hierarchical barrier timestamp aggregation.
from queues according to priority, applies header modifications
and sends them to output links. One key property of the switch is
that the queuing model ensures FIFO property for packets with the
same priority, ingress port and egress port.
The switch can provide good programmability. First, the CPU
can be used to process (a small amount of) packets [71]. Second,
the switching chip is increasingly programmable in recent years.
For example, Tofino chip [45] supports flexible packet parsers and
stateful registers. Users can program Tofino using P4 [20] to achieve
customized stateful per-packet processing.
Despite good programmability, the switch typically has limited
buffer resource to hold packets. The average per-port packet buffer
is typically hundreds of kilobytes [14, 15] in size. As a result, it is
challenging to buffer many packets at the switches in a data center.
4 BEST EFFORT 1PIPE
Best effort 1Pipe provides total order, causal, and FIFO message
delivery, but does not retransmit lost packets and does not provide
atomicity. A naive approach to realize this is ordering all messages
with a centralized sequencer, which would be a bottleneck. Instead,
we will introduce how to achieve scalable ordering in the regular
DCN topology.
4.1 Message and Barrier Timestamp
Message timestamp. 1Pipe sender assigns a non-decreasing times-
tamp for each message. Messages in a scattering have the same
timestamp. Given recent efforts on ğœ‡s accurate clock synchroniza-
tion in data centers [28, 40, 66, 69], we synchronize monotonic
time of hosts, and use the local clock time as message timestamps.
Clock skew slows down delivery but does not violate correctness.
Message timestamp determines the delivery order at a receiver.
The receivers deliver arrival messages in ascending order of their
timestamps (ties are broken through sender ID).
Barrier timestamp. When a receiver delivers a message with
timestamp ğ‘‡ , it must be sure that it has received and delivered all
the messages whose timestamps are smaller than ğ‘‡ . A straightfor-
ward approach is to only deliver messages with non-decreasing
timestamps, and drop all out-of-order messages. However, since
different network paths have different propagation and queuing de-
lays, this approach will drop too many messages, e.g., 57% received
messages are out-of-order in our experiment where 8 hosts send
to one receiver. To this end, we introduce the concept of barrier
timestamp. A barrier timestamp is associated with either a link or a
Barrier: 1Barrier: 4Barrier: 2Barrier: 3<Barrier: 1Barrier: 1Barrier: 2Barrier: 2Barrier: 1<SIGCOMM â€™21, August 23â€“28, 2021, Virtual Event, USA
Bojie Li, Gefei Zuo, Wei Bai, and Lintao Zhang
aggregation only relies on hop-by-hop FIFO links instead of a FIFO
end-to-end transport. Therefore, 1Pipe can work with a variety of
multi-path routing schemes [9, 21, 33, 47, 58, 95, 100].
Causality. To preserve causal order in the Lamport logical clock
sense [63], the local clock time should be higher than delivered
timestamps. This is implied by timestamp aggregation because
each process has both sender ğ‘† and receiver ğ‘… roles, and a barrier
ğ‘‡ received by ğ‘… is aggregated from all senders including ğ‘†. Because
the local clock is monotonic, ğ‘†â€™s timestamp is higher than ğ‘‡ when
ğ‘… receives barrier ğ‘‡ .
4.2 Beacons on Idle Links
As shown before, at each hop, the per-packet barrier timestamp is
updated to the minimum barrier timestamp value of all possible
input links. As a result, an idle link stalls the barrier timestamp, thus
throttling the whole system. To avoid idle links, we send beacons
periodically on every link.
What is a beacon? Unlike the message packet, the beacon packet
only carries the barrier timestamp field and has no payload data.
How to send beacons? We send beacon packets on a per-idle-link
basis. Beacon packets can be sent by both hosts and switches, but
the destination must be its one-hop neighbors. This hop-by-hop
property ensures that the beacon overhead is unrelated to network
scale. For a beacon generated by the host, the barrier timestamp
is the host clock time. For a beacon generated by the switch, the
barrier timestamp is initialized according to (4.1).
When to send beacons? We introduce a beacon interval ğ‘‡ğ‘ğ‘’ğ‘ğ‘ğ‘œğ‘›.
When a host or a switch output link does not observe any message
packet for ğ‘‡ğ‘ğ‘’ğ‘ğ‘ğ‘œğ‘› time, it generates a beacon packet. We should
choose a suitable ğ‘‡ğ‘ğ‘’ğ‘ğ‘ğ‘œğ‘› to balance bandwidth overhead and sys-
tem delay. The beacons are sent at synchronized times on different
hosts, so that when network delays are identical, a switch receives
beacons almost simultaneously. If beacons were sent on random
times, the switch must wait for the last beacon to come, which in-
creases expected delay of a message by nearly one beacon interval.
With synchronized beacons, the expected delay overhead is only
half of the beacon interval.
Handling failures. When a host, link, or switch fails, the barrier
timestamp of its neighbors stop increasing. In order to detect fail-
ures, each switch has a timeout timer per input link. If no beacon or
data packet is received for a timeout, e.g., 10 beacon intervals, the
input link is considered to be dead and removed from the input link
list. After removal of the failed link, the barrier timestamp resumes
increasing. This failure handling mechanism is decentralized.
Addition of new hosts and links. For a new host, it synchronizes
clock with the time master. When a link is added between a host and
switch or between two switches, because the switch must maintain
monotonicity of its ğµğ‘›ğ‘’ğ‘¤, it suspends updates to ğµğ‘›ğ‘’ğ‘¤ until the
barrier received from the new link is greater than ğµğ‘›ğ‘’ğ‘¤.
5 RELIABLE 1PIPE
Now, we present the design of reliable 1Pipe that can handle packet
loss and failure.
Figure 6: Two Phase Commit in reliable 1Pipe.
Figure 7: Failure recovery in reliable 1Pipe.
5.1 Handling Packet Loss
When a receiver delivers a message with timestamp ğ‘‡ , it must make
sure that all messages below ğ‘‡ are delivered. So, if a receiver is un-
aware of packet loss, it cannot reliably deliver messages according
to the barrier timestamp. Even if the switch is capable to detect lost
packets, there is still a problem. For example, host ğ´ sends to ğµ,
then sends to ğ¶ via a different path. Three events happen according
to the following order: ğ´ â†’ ğµ is lost; ğ´ â†’ ğ¶ arrives; ğ´ crashes. In
this case, ğ´ â†’ ğ¶ is delivered, while ğ´ â†’ ğµ cannot be recovered.
The failure to deliver ğ´ â†’ ğµ and the delivery of ğ´ â†’ ğ¶ violate
reliable ordering property.
Our key idea to handle packet losses is a Two Phase Commit (2PC)
approach:
â€¢ Prepare phase: The sender puts messages into a send buffer,
and transmits them with timestamps. Switches along the path do
NOT aggregate timestamp barriers for data packets. The receiver
stores messages in a receive buffer, and replies with ACKs. The
sender uses ACKs to detect and recover packet losses.
â€¢ Commit phase: When a sender collects all the ACKs for data
packets with timestamps below or equal to ğ‘‡ , it sends a commit
message that carries commit barrier ğ‘‡ . The commit message is
sent to the neighbor switch rather than the receivers, as the red
arrow in Figure 6 shows. Each switch aggregates the minimum
commit barriers on input links, and produce commit barriers that
propagate to output links. This timestamp aggregation procedure
is exactly the same as Sec.4.1. A receiver delivers messages below
or equal to ğ‘‡ in the receive buffer when it receives a commit
barrier ğ‘‡ . Similar to best effort 1Pipe, commit messages also
need periodic beacons on idle links.
5.2 Handling Failure
Like in Sec. 4.2, crash failure of a component is detected by its
neighbors using timeout. However, a failed component cannot be
simply removed, because otherwise, the in-flight messages sent by
the failed component cannot be consistently delivered or discarded.
To achieve restricted failure atomicity in Sec.2.1, we use the net-
work controller in data centers to detect failures via beacon timeout.
The controller itself is replicated using Paxos [65] or Raft [80], so,
it is highly available, and only one controller is active at any time.
SenderReceiver 1Receiver 2PrepareACKSwitchCommitHostData Plane SwitchHostHostControllerâ‘ Detectâ‘¡Determineâ‘¢Broadcastâ‘£Discardâ‘¤Recallâ‘¥Callbackâ‘¦Resumeâ‘ Detect(beacontimeout)â‘£Discardâ‘¥Callback1Pipe: Scalable Total Order Communication in Data Center Networks
SIGCOMM â€™21, August 23â€“28, 2021, Virtual Event, USA
The controller needs to determine which processes fail and when
they fail. The former question is easier to answer. A process that
disconnects from the controller in a routing graph is regarded as failed.
For example, if a host fails, all processes on it are considered as
failed. If a Top-of-Rack (ToR) switch fails, and hosts in the rack are
only connected to one switch, then all processes in the rack fail.
The latter question, when processes fail, is harder to answer.
The challenge is that we cannot reliably determine the last commit-
ted and last delivered timestamp of a process. Because there is a
propagation delay from committing a timestamp to delivering the
timestamp to receivers, it is possible to find a timestamp ğ‘‡ commit-
ted by a failed process ğ‘ƒ but not propagated to any receiver, so that
all receivers have received messages from ğ‘ƒ before ğ‘‡ in the receive
buffer (and hence can deliver them), but no messages after ğ‘‡ have
been delivered, so they can be discarded. Failure timestamp of ğ‘ƒ is
defined as such, which is computed as the maximum last commit
timestamp reported by all neighbors of ğ‘ƒ. If multiple failures occur
simultaneously, we try to find a set of correct nodes in a routing
graph that separates failed nodes and all correct receivers. If such a
set cannot be found due to network partition, then we use a greedy
algorithm to find a set to separate as many receivers as possible. The
non-separable receivers sacrifice atomicity because some messages
after ğ‘‡ may have been delivered.
along with its last commit timestamp ğ‘‡ .
failure timestamp ğ‘‡ to all correct processes.
failure timestamps according to the routing graph.
The procedure to handle failure is as follows and shown in Fig-
ure 7: (see Appendix for correctness analysis)
â€¢ Detect: The neighbors of failed components notifies controller
â€¢ Determine: Controller determines failed processes and their
â€¢ Broadcast: Controller broadcasts the failed processes ğ‘ƒ and its
â€¢ Discard: Each correct process discards messages sent from ğ‘ƒ
with timestamp higher than ğ‘‡ in the receive buffer.
â€¢ Recall: Each correct process discards messages sent to ğ‘ƒ in
send buffer, which are waiting for ACK from ğ‘ƒ. If a discarded
message is in a scattering, according to failure atomicity, the
scattering needs to be aborted, i.e., messages to other receivers in
the same scattering need to be recalled. The sender sends a recall
message to such receivers, then each of the receivers discards the
messages in the receive buffer and responds ACK to the sender.
The sender completes Recall after collecting the ACKs.
â€¢ Callback: Each correct process executes the process failure
callback registered in Table 1, which enables applications to
customize failure handling. Then, it responds controller with a
completion message.
â€¢ Resume: Controller collects completions from all correct pro-
cesses, and then notifies network components to remove input
link from the failed component, thereby resuming barrier prop-
agation.
Controller Forwarding. If a network failure affects connectivity
between ğ‘† and ğ‘…, the Commit phase in 2PC and the Recall step in
failure handling may stall because ğ‘† repeatedly retransmits a mes-
sage but cannot receive ACK from ğ‘…. In this case, ğ‘† asks controller
to forward the message to ğ‘…, and waits for ACK from the controller.
If controller also cannot deliver the message, ğ‘… will be announced
as failed, and the undeliverable recall message is recorded. If con-
troller receives ACK of a recall message but cannot forward it to
ğ‘†, ğ‘† will be announced as failed. In summary, if a process does not
respond controller within timeout, it is considered as failed.
Receiver Recovery. If a process recovers from failure, e.g., the
network link or switch recovers, the process needs to consistently
deliver or discard messages in the receive buffer. The controller noti-
fies process of its own failure. Then, the process contacts controller
to get host failure notifications since its failure and undeliverable
recall messages. After delivering buffered messages, the recovered
process needs to join 1Pipe as a new process. This is because if a pro-
cess can fail and recover multiple times, the controller would need