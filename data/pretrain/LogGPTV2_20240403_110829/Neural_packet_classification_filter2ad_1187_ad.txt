action.
• Time(s): Upper-bound on classification time to query the subtree s . In
non-partitioned trees this is simply the depth of the tree.
• Space(s): Memory consumption of the subtree s .
Incorporating existing heuristics. NeuroCuts can easily incor-
porate additional heuristics to improve the decision trees it learns.
One example is adding rule partition actions. In addition to the cut
action, in our NeuroCuts implementation we also allow two types
of partition actions:
(1) Simple: the current node is partitioned along a single di-
mension using a learned threshold.
(2) EffiCuts: the current node is partitioned using the EffiCuts
partition heuristic [55].
Scaling out to handle large packet classifiers. The pseudocode
in Algorithm 1 is for a single-threaded implementation of Neuro-
Cuts. This is sufficient for small classifiers. But for large classifiers
with tens or hundreds of thousands of rules, parallelism can sig-
nificantly improve the speed of training. In Figure 7 we show how
Algorithm 1 can be adapted to build multiple decision trees in
parallel.
Handling classifier updates. Packet classifiers are often updated
by network operators based on application requirements, e.g., adding
access control rules for new devices. For small updates of only a few
rules, NeuroCuts modifies the existing decision tree to reflect the
changes. New rules are added to the decision tree according to the
262
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica
Action Space
Observation Space
(cid:3)
Observation Components
Tuple(Discrete(NumDims), Discrete(NumCutActions + NumPartitionActions))
BinaryString(Ranдedim
Box(low=0, high=1, shape=(278,))
min ) + BinaryString(Ranдedim
max ) + OneHot(Partitiondim
∀dim ∈ {SrcIP, DstIP, SrcPort, DstPort, Protocol }
+ OneHot(EffiCutsPartitionID) + ActionMask
min ) + OneHot(Partitiondim
max )
(cid:4)
Table 1: NeuroCuts action and observation spaces described in OpenAI Gym format [2]. Actions are sampled from two cate-
gorical distributions that select the dimension and action to perform on the dimension respectively. Observations are encoded
in a one-hot bit vector (278 bits in total) that describes the node ranges, partitioning info, and action mask (i.e., for prohibit-
ing partitioning actions at lower levels). When not using the EffiCuts partitioner, the Partitiondim
rule dimension coverage
thresholds are set to one of the following discrete levels: 0%, 2%, 4%, 8%, 16%, 32%, 64%, and 100%.
(cid:51)(cid:82)(cid:79)(cid:76)(cid:70)(cid:92)(cid:3)(cid:40)(cid:89)(cid:68)(cid:79)(cid:88)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:51)(cid:82)(cid:79)(cid:76)(cid:70)(cid:92)(cid:3)(cid:40)(cid:89)(cid:68)(cid:79)(cid:88)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:51)(cid:82)(cid:79)(cid:76)(cid:70)(cid:92)(cid:3)(cid:40)(cid:89)(cid:68)(cid:79)(cid:88)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:38)(cid:82)(cid:81)(cid:70)(cid:68)(cid:87)(cid:72)(cid:81)(cid:68)(cid:87)(cid:72)(cid:3)
(cid:87)(cid:85)(cid:72)(cid:72)(cid:3)(cid:85)(cid:82)(cid:79)(cid:79)(cid:82)(cid:88)(cid:87)(cid:86)
(cid:44)(cid:80)(cid:83)(cid:85)(cid:82)(cid:89)(cid:72)(cid:3)(cid:7578)(cid:15)(cid:3)(cid:7578)(cid:89)(cid:3)(cid:89)(cid:76)(cid:68)(cid:3)
(cid:86)(cid:87)(cid:82)(cid:70)(cid:75)(cid:68)(cid:86)(cid:87)(cid:76)(cid:70)(cid:3)(cid:74)(cid:85)(cid:68)(cid:71)(cid:76)(cid:72)(cid:81)(cid:87)(cid:3)
(cid:71)(cid:72)(cid:86)(cid:70)(cid:72)(cid:81)(cid:87)
(cid:37)(cid:85)(cid:82)(cid:68)(cid:71)(cid:70)(cid:68)(cid:86)(cid:87)(cid:3)(cid:81)(cid:72)(cid:90)(cid:3)(cid:89)(cid:68)(cid:79)(cid:88)(cid:72)(cid:86)(cid:3)(cid:82)(cid:73)(cid:3)(cid:7578)
Figure 7: NeuroCuts can be parallelized by generating deci-
sion trees in parallel from the current policy.
existing structure; deleted rules are removed from the terminal leaf
nodes. When enough small updates accumulate or a large update
is made to the classifier, NeuroCuts re-runs training.
5 IMPLEMENTATION
Deep RL algorithms are notoriously difficult to reproduce [15]. For a
practical implementation, we prioritize the ability to (i) leverage off-
the-shelf RL algorithms, and (ii) easily scale NeuroCuts to enable
parallel training of policies.
Decision tree implementation. We implement the decision tree
data structure for NeuroCuts in Python for ease of development. To
ensure minor implementation differences do not bias our results, we
use this same data structure to implement each baseline algorithm
(e.g., HiCuts, EffiCuts, etc.), as well as to implement NeuroCuts.
Branching decision process environment. As discussed in Sec-
tion 4, the branching structure of the NeuroCuts environment poses
a challenge due to its mismatch with the MDP formulation as-
sumed by many RL algorithms. A typical RL environment defines
a transition function Pa (st +1 |st ) and a reward function Ra (s, s (cid:6)).
The first difference is that the state transition function in Neuro-
Cuts returns multiple child states, instead of a single state., i.e.,
(st , at ) → {s0
}. Second, the final reward for NeuroCuts
is computed by aggregating across the rewards of child states. More
precisely, for the cut action we use max aggregation for classification
time and sum aggregation for memory footprint. For the partition
action, we use sum aggregation for both metrics.
, ..., sk
t +1
t +1
The recursive dependence of the NeuroCuts reward calculation
on all descendent state actions means that it is difficult to flatten
263
the tree structure of the environment into a single MDP, which
is required by existing off-the-shelf RL algorithms. Rather than
attempting to flatten the NeuroCuts environment, our solution is
to instead treat the NeuroCuts environment as a series of indepen-
dent 1-step decision problems, each of which yields an “immediate”
reward. The actual reward for these 1-step decisions is calculated
once the relevant sub-tree rollout is complete.
For example, consider a NeuroCuts tree rollout from a root node
s1. Based on πθ the agent decides to take action a1 to split s1 into s2,
s3, and s4. Of these child nodes, only s4 needs to be further split (via
a2), into s5 and s6, which finishes the tree. The experiences collected
from this rollout consist of two independent 1-step rollouts: (s1, a1)
and (s4, a2). Taking the time-space coefficient c = 1 and discount
factor γ = 1 for simplicity, the total reward R for each rollout would
be R = 2 and R = 1 respectively.
Multi-agent implementation. Since these 1-step decisions are
logically independent of each other, NeuroCuts execution can be re-
alized as a multi-agent environment, where each node’s 1-decision
problem is taken by an independent “agent” in the environment.
Since we want to learn a single policy, πθ , for all states, the agents
must be configured to share the same underlying stochastic neural
network policy. This ensures all experiences go towards optimizing
the single shared policy πθ . When using an actor-critic algorithm
to optimize the policies of such agents, the relevant loss calcula-
tions induced by this multi-agent realization are identical to those
presented in Algorithm 1.
There are several ways to implement the 1-step formulation of
NeuroCuts while leveraging off-the-shelf RL libraries. In Algorithm
1 we show standalone single-threaded pseudocode assuming a sim-
ple actor-critic algorithm is used. In our experiments, we use the
multi-agent API provided by Ray RLlib [30], which implements
parallel simulation and optimization of such RL environments.
Performance. We found that NeuroCuts often converges to its
optimal solution within just a few hundred rollouts. The size of the
rule set does not significantly affect the number of rollouts needed
for convergence, but affects the running time of each rollout. For
smaller problems (e.g., 1000 rules), this may be within a few minutes
of CPU time. The computational overhead for larger problem scales
with the size of the classifier, i.e., linearly with the number of rules
that must be scanned per action taken to grow the tree. The bulk
of time in NeuroCuts is spent executing tree cut actions. This is
largely an artifact of our Python implementation, which iterates
Neural Packet Classification
SIGCOMM ’19, August 19–23, 2019, Beijing, China
over each rule present in a node on each cut action. An optimized
C++ implementation of the decision tree would further reduce the
training time.
5.1 Optimizations
Rollout truncation. During the initial phase of learning, the unop-
timized policy will create excessively large trees. Since NeuroCuts
does not start learning until a tree is complete, it is necessary to
truncate rollouts to speed up the initial phase of training. For larger
classifiers, we found it necessary to allow rollouts of up to 15000
actions in length.
Depth truncation. Since valid solutions never involve trees of
depth greater than a few hundred, we also truncate trees once they
reach a certain depth. In our experience, depth truncation is only
a factor early on in learning; NeuroCuts quickly learns to avoid
creating very deep trees.
Proximal Policy Optimization. For better stability and more
sample-efficient learning, in our experiments we choose to use
Proximal Policy Optimization (PPO) [43]. PPO implements an actor-
critic style loss with entropy regularization and a clipped surrogate
objective, which enables improved exploration and sample effi-
ciency. We report the PPO hyperparameters we used in Table 2.
It is important to note however that this particular choice of RL
algorithm is not fundamental to NeuroCuts.
6 EVALUATION
In the evaluation, we seek to answer the following questions:
(1) How does NeuroCuts compare to the state-of-the-art ap-
proaches in terms of classification time and memory foot-
print? (Section 6.1 and 6.2)
(2) Beyond tabula rasa learning, can NeuroCuts effectively incor-
porate and improve upon pre-engineered heuristics? (Section
6.3)
(3) How sensitive is NeuroCuts to the hyperparameters of the
neural network architecture (Section 6.4), and the time-space
coefficient c (Section 6.5)?
For the results presented in the next sections, we evaluated
NeuroCuts within the space of hyperparameters shown in Table 2.
We did not otherwise perform extensive hyperparameter tuning;
in fact we use close to the default hyperparameter configuration of
the PPO algorithm. The notable hyperparameters we swept over
include:
• Allowed top-node partitioning (none, simple, and the Effi-
Cuts heuristic), which strongly biases NeuroCuts towards
learning trees optimized for time (none) vs space (EffiCuts),
or somewhere in the middle (simple).
• The max number of timesteps allowed per rollout before
truncation. It must be large enough to enable solving the
problem, but not so large that it slows down the initial phase
of training.
• We also experimented with values for the time-space tradeoff
coefficient c ∈ {0, 0.1, 0.5, 1}. When c 100×
median space improvement along with the better classification
times reported in Section 6.1. However, these time-optimized trees
are not competitive in space with the space-optimized NeuroCuts,
EffiCuts and CutSplit trees.
264
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Eric Liang, Hang Zhu, Xin Jin, and Ion Stoica
(cid:43)(cid:76)(cid:38)(cid:88)(cid:87)(cid:86)
(cid:43)(cid:92)(cid:83)(cid:72)(cid:85)(cid:38)(cid:88)(cid:87)(cid:86)
(cid:40)(cid:73)(cid:73)(cid:76)(cid:38)(cid:88)(cid:87)(cid:86)
(cid:38)(cid:88)(cid:87)(cid:54)(cid:83)(cid:79)(cid:76)(cid:87)
(cid:49)(cid:72)(cid:88)(cid:85)(cid:82)(cid:38)(cid:88)(cid:87)(cid:86)
(cid:25)(cid:19)
(cid:23)(cid:19)
(cid:21)(cid:19)
(cid:76)
(cid:72)
(cid:80)
(cid:55)
(cid:81)
(cid:82)
(cid:3)
(cid:76)
(cid:87)
(cid:68)
(cid:70)
(cid:76)
(cid:73)
(cid:76)
(cid:86)
(cid:86)
(cid:68)
(cid:38)
(cid:79)
(cid:19)
(cid:68)(cid:70)(cid:79)(cid:20)(cid:3)(cid:20)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:21)(cid:3)(cid:20)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:22)(cid:3)(cid:20)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:23)(cid:3)(cid:20)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:24)(cid:3)(cid:20)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:20)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:21)(cid:3)(cid:20)(cid:19)(cid:78)
(cid:68)(cid:70)(cid:79)(cid:22)(cid:3)(cid:20)(cid:19)(cid:78)