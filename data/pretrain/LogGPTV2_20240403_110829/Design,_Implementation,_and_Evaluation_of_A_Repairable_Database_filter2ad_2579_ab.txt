these two embedding applications have data dependencies
through inter-process communication.
The inter-transaction dependency that
the current
Phoenix prototype supports is based strictly on read-write
relationships between transactions: There is a dependency
from Transaction A to Transaction B if Transaction B reads
a data item last updated by Transaction A. The assumption
underlying this deﬁnition is that reading a particular item
changes the state of a transaction and affects its execution
results.
To compute inter-transaction dependencies, one can
maintain for each transaction a read/write log, which
records the set of records that are read and written by that
transaction at run time, and perform dependency analysis at
repair time. While conceptually simple, it has a major ﬂaw:
most DBMSs do not support read logs because keeping
read logs incurs serious performance penalty. Recognizing
this deﬁciency, Phoenix chose to build up inter-transaction
dependency directly at run time, thus eliminating the need
of read logging completely.
For each data item, Phoenix maintains an additional
ﬁeld that records the last transaction that updates the data
item. Let’s call this ﬁeld of data item X as Prev(x). When X
is read by Transaction T, then Phoenix establishes a depen-
dency relationship between T and the transaction indicated
in Prev(x). This algorithm is illustrated in Figure 1. For
example, after the ﬁrst transaction, Prev(1) = 0; once the
second transaction completes, transaction 1 is dependent
on transaction 0, as shown in the dependency graph that
Phoenix builds on the ﬂy.
Because inter-transaction dependencies need to survive
intrusion, they should be kept persistent and preferably in a
separate disk than other database disks. Speciﬁcally the de-
pendencies associated with a transaction should be written
to disk when the transaction commits.
Concurrent execution of transactions complicates the
maintenance of inter-transaction dependency. First, if T
just updates x, changing Prev(x) to T should happen be-
fore another transaction reads x. Prev(x) update should be
atomic with respect to the actual update of x. Secondly, if
a transaction T writes x and later aborts, Prev(x) should be
reset to its previous value before T’s update. This can be
accomplished by considering Prev(x) information to be a
part of the before image of an item x and then carrying out
necessary changes at transaction rollback time. Thirdly, if
transactions against a database system are allowed to ex-
ecute at read uncommitted isolation level, cascaded aborts
are possible due to dirty reads. In this case Phoenix must
carefully remove any dependency information associated
with the aborted transactions and also restore Prev(x) to it’s
earlier values. Handling aborts becomes considerably eas-
ier if all transactions are guaranteed to run at at least read
uncommitted isolation level. We discuss these issues fur-
ther in Section 4.2.4.
The choice of the granularity of a data item for which
the Prev() ﬁeld is maintained affects the run-time perfor-
mance overhead and the accuracy of dependency tracking.
Coarser granularity results in lower performance overhead
but a higher probability of false sharing, i.e., two transac-
tions that read and write different portions of the same data
item. From an implementation’s standpoint, the deﬁnition
of a data item must satisfy the following two properties:
• The read/write of a data item must be atomic so that
• There should be a unique way to refer to an item so
the Prev() ﬁeld update is atomic, and
that undo can be done at the same granularity.
Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE
OPERATION : r(x) / w(x) = read/write of item x
T3 :                                                       r(3) commit
T2 :                            w(3) r(1) w(4) commit
T1 :           r(1)  r(2) commit 
T0 : w(1) commit
DEPENDENCIES : 
T0
T1
T2
T3
OnRead (Item x, Transaction t) {
          LastToUpdate =  prev(x);
          if ( t does not depend on LastToUpdate)
                  AddDependency (t, LastToUpdate);
}
OnWrite(Item x, Transaction x) {
          Set prev(x) = x;
}
Figure 1: Creating dependency graphs from transaction history. Transaction 1 becomes dependent on Transaction 0 after reading item
1 that is last updated by Transaction 0. The algorithm on the right describes how the graph on the left is created and maintained.
A table row satisﬁes both properties and thus is chosen
as the basic unit for tracking inter-transaction dependen-
cies. Accordingly a data item read operation is deﬁned as
either of the following: (1) read access to a row satisfy-
ing WHERE or HAVING clause in a select operation, or
(2) read access to a row satisfying WHERE clause in up-
date/delete operation. To a given transaction, a row is read
only when the select statement returns a row or when a row
is chosen to be updated/deleted by an update/delete state-
ment. Note that this deﬁnition excludes those rows that
are read during the processing of a select operation or a
delete/update operation. That is, if the DBMS reads a set
of rows in order to service a select query, only the rows
that satisfy the query are considered read by the transac-
tion that issues the query. Similarly a write to a table row
is deﬁned as an update (insert) statement, which modiﬁes
(inserts) that row.
4 Prototype Implementation
The implementation of Phoenix adheres to the follow-
ing principles. Firstly, the addition of intrusion resilience
should be safe in the sense that it does not affect normal
transaction execution semantics even upon system failures.
Secondly, the performance and space overhead of Phoenix
should be kept to the minimum, preferably to a level that is
completely transparent to database users. Finally, database
administrators should be able to enable/disable Phoenix
easily.
Logically, Phoenix consists of two components: a run-
time component that builds up inter-transaction dependen-
cies during normal operation, and a repair-time component
that computes the set of corrupting transactions and per-
forms undo of their effects after an intrusion is detected.
These two components will be described in more detail in
the following subsections.
The current Phoenix prototype is built on PostgreSQL,
an open-source database management system derived from
Berkeley’s Postgres project. We chose PostgreSQL as
the underlying platform because it is open-source, it pro-
vides industry-strength transaction support, and most im-
portantly the fact that its multi-version concurrency control
and no-overwrite storage management are a perfect ﬁt with
Phoenix. In the next subsection, we will brieﬂy describe
these two PostgreSQL features to set the stage for subse-
quent discussion.
4.1 Multi-Versioning Structure in PostgreSQL
PostgreSQL uses a no-overwrite storage management pol-
icy to speed up the transaction processing. In this scheme,
every update to an existing table row creates a new version
of the row. Under this policy, both the before image and
after image of each database update are implicitly stored in
the database, and consequently there is no need for storing
undo information in a separate log. This essentially means
that a part of the standard write-ahead log (before images)
is implicitly stored as part of the database records. This
results in faster transaction abort handling and recovery as
there is no undo log to process.
With this database record structure, PostgreSQL
chooses to use a multi-version concurrency control
(MVCC) scheme to improve the transaction concurrency.
In MVCC, the concurrency control granularity is a table
row; a read access and a write access to the same table row
do not block each other; only a write access can block an-
other write access to the same table row.
In PostgreSQL, a table row contains one or more tuples,
each representing a distinct version of the table row. At run
time, when a transaction accesses a table row, the tuple it
actually sees depends on the visibility rule. Each tuple of
a table row comes with a metadata that consists of three
ﬁelds: Xmin contains the ID of the transaction that creates
this tuple, Xmax contains the ID of the transaction that up-
dates or deletes this tuple, Link points to the next tuple of
the same table row. When a transaction with an ID of N
accesses a table row, PostgreSQL returns the tuple of this
table row that has a valid Xmin and invalid Xmax with re-
spect to N.
In read committed isolation level, the ID in Xmin or
Xmax is valid with respect to N if and only if it is N or it
denotes a transaction that is already committed. In serial-
Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE
izable isolation level, the ID in Xmin or Xmax is valid with
respect to N if and only if it is N or it denotes a transaction
that is already committed before Transaction N starts.
In visibility rule processing, PostgreSQL needs to know
whether a transaction is already committed and when. For
this purpose, it keeps a separate transaction status log,
each entry of which corresponds to a transaction and in-
dicates whether the transaction is in progress, committed,
or aborted. To ensure correctness, the tuples updated by a
transaction must be written to disk before its entry in the
transaction status log can be changed to “committed.”
Multiple tuples per table row combined with visibility
rule processing result in both a powerful framework for
both concurrency control and error recovery. For example,
during recovery, to abort a previously in-progress transac-
tion, all that is needed is to convert the transaction’s corre-
sponding entry in the transaction status log to “aborted”.
Once this is done, none of the tuples it creates will be
visible to any transactions according to the visibility rule.
There is no need for any redo, because if a transaction is
considered “committed” in the transaction status log, all
the tuples it creates must be written to disk already.
From the standpoint of Phoenix implementation, Post-
greSQL provides two key beneﬁts. First, the Xmin ﬁeld of
every tuple provides the same information as Prev(). Sec-
ond, the extremely simple way of aborting a transaction
allows Phoenix to undo a committed transaction without
making any modiﬁcation to PostgreSQL’s source code.
4.2 Run-Time Logic
4.2.1 Recording Last Update Transaction
To keep track of inter-transaction dependencies at run time,
PostgreSQL needs to record the ID of the transaction that
last updates each database record. There are two ways to
implement this functionality. The ﬁrst approach is tied with
PostgreSQL’s multi-versioning record structure. When a
transaction accesses a table row, PostgreSQL returns one
of the row’s tuples according to the visibility rule; the Xmin
ﬁeld of this tuple gives the ID of the transaction that creates
the instance of the table row that the current transaction is
accessing. The additional space usage is only 4 bytes (size
of a unique transaction ID) per row and the time cost is
that of updating this 4-byte ﬁeld each time a row is written.
By exploiting the MVCC data structure inherent in Post-
greSQL, no additional code is required for Phoenix to keep
track of the last transaction that updates a given table row.
If there is no per-record Xmin ﬁeld, then an alternative
approach for maintaining Prev() information is based on
triggers. More speciﬁcally, we create a system table called
CreateTran that contains two columns : row OID and trans-
action ID. A tuple (o,x) in CreateTran indicates that the row
with OID o is last updated by a transaction with ID x. To
take control when a table row is updated, one can set up a
create/delete/update trigger for every database table. The
handler associated with a table’s trigger will be invoked
whenever a row in the table is created, deleted, or modi-
ﬁed. Moreover, the handler can access the row in question.
Using the trigger mechanism, we can modify the transac-
tion ID ﬁeld of the CreateTran entry corresponding to the
row in question to point to the current transaction. The
CreateTran table is implemented as an in-memory hash ta-
ble with row OID as the key. On both read and write ac-
cess to a table row, the system consults with the hash ta-
ble with the row’s OID, and updates the entry or builds
up inter-transaction dependency accordingly. Because the
hash table is memory-resident, the cost to access it is much
smaller.
4.2.2 Capturing Inter-Transaction Dependency
In Phoenix, an inter-transaction dependency is constructed
whenever a transaction reads a table row that is last up-
dated by another transaction. Conceptually one can set up
a read trigger for each database table and implement this
dependency tracking logic in the associated trigger handler.
Unfortunately, unlike write triggers, most DMBSes do not
support read triggers. Even when they do, the read trigger
semantics may not be an exact ﬁt of what Phoenix needs
and the associated performance overhead may be exces-
sive. As a result, Phoenix has no choice but to modify Post-
greSQL’s internals to construct inter-transaction dependen-
cies at run time.
Ideally, Phoenix needs to intercept the query execution
at the point when a table row satisfying the WHERE or
HAVING clause in the query is returned for further pro-
cessing. This requirement relates to our deﬁnition of read
given in section 3.2. The execution plan tree for a query
is a binary tree comprising nodes that perform certain tu-
ple producing operations. For example, an index scan node
can use a key and a BTree index to return matching rows,
a join node can merge-join the tuples produced by it’s left
and right subtrees, etc. Query execution proceeds in an
iterative manner by a parent node asking it’s child nodes
to produce a tuple for it to operate on. Each returned tu-
ple is operated upon in a node-speciﬁc manner. This pro-
cess continues until child nodes indicate that there are no
more tuples. The execution of a query plan is initiated by
the root node requesting a tuple from it’s children and ends
when the children return a null tuple. For SELECT state-
ments the root node will return the tuples it receives to the
PostgreSQL front end. For UPDATE statements, the root
node will perform the required update on the received tu-
ples. For DELETE statements, the root node will expire
the received tuples. The tuples that are operated upon by
an execution plan tree are in fact copies of the actual on-
disk tuples, instantiated at the lower levels in the plan tree.
We refer to them as cooked tuples. The data portion in
cooked tuples may change as they pass through projection
or join nodes (cooking) while the meta-data portion usually
Proceedings of the 21st International Conference on Data Engineering (ICDE 2005) 
1084-4627/05 $20.00 © 2005 IEEE
remains the same. Not every cooked tuple present at lower
levels reaches the root node as some of them are discarded
during join operations, intersect operations etc.
In order to add a dependency, Phoenix intercepts the
query execution plan when it identiﬁes the occurrence of
a read. At this point, it calls a hook function to add a de-
pendency between the current transaction and the last up-
date transaction for the tuple in question, before allowing
query execution to proceed as normal. The point of inter-
ception in query execution is the code associated with pro-
cessing the root node. Intercepting at the root node sufﬁces
as all rows returned to the root node for SELECT/UPDATE
statements will match the WHERE (or HAVING) clauses.
Thus, we can call our dependency-adding hook function
each time the root node’s children return a tuple. How-
ever, prior to adding a dependency during interception, we
also need to obtain the row ID(s) of the tuple as they are
needed for determining last update transaction. The row
ID information needed at the root level is maintained as
part of the cooked tuples. It is possible for a cooked tuple
to be based upon more than one original disk tuples. For
instance, if the SQL query involves a join of 5 tables, the
resulting tuple is a linear composition of some rows from
these 5 tables. Due to this we need to maintain a list of
row IDs for each cooked tuple. This list starts off as a sin-
gle element when the cooked tuple is instantiated (row ID
of disk based tuple) and will grow by one element at each
join node. As no deletions or traversal of this list (except
at the root node) are required we can maintain it with little
overhead. It should be noted that size of this row ID list is
usually quite small but such lists exist as meta-data for each
of the cooked tuples in the ﬁnal result of a query. Once the
row ID information is available at the root node, Phoenix
uses the Prev() information to ﬁnd out what transaction(s)
last updated the row in question and adds one or multiple
dependencies.
4.2.3 Persistence of Dependency Graph
Maintaining the inter-transaction dependency graph is a po-
tential performance bottleneck for Phoenix as one needs to
update and query the graph on each read during query ex-
ecution. Further, this graph is maintained as a shared data
structure operated upon by multiple transactions and thus
efﬁciency is even more critical. We exploit two properties
of the operations against the inter-transaction dependency
graph to improve its run-time efﬁciency. Firstly, traversal
of the graph is not required while Phoenix is operating. It
is only during the post-intrusion phase that we need to an-
alyze the dependency graph in detail. Secondly, deletion
of edges from the graph can be carried out in an asyn-
chronous manner and even deferred to the post intrusion
phase if space is not a concern (see Section 5). This is
because deletions need to be performed only when a trans-
action aborts. We can process a transaction abort event by
simply setting a ﬂag in the corresponding node of the graph
and taking appropriate action at analysis time.
We implement the inter-transaction dependency graph
as a hash table of hash tables. This is stored in the DBMSs
shared memory area and thus is accessible to all back-
end server processes, each of which services a SQL-based
transaction. The top-level hash table is indexed by a trans-
action ID. Each entry in this table is associated with one
transaction and is a hash table itself. To record the depen-
dency from Transaction X to Transaction Y, the system ﬁrst
retrieves the hash table associated with X by accessing the
top-level hash table. Next, Y is added to X’s hash table
through a hash access. To check if Transaction X depends
on Transaction Y, the system checks if Y is in the hash table
associated with X.
The part of the inter-transaction graph that describes
what a transaction depends on is made persistent when the
transaction is committed, at which time the hash table asso-
ciated with this committed transaction is written to a disk-
based log ﬁle. This log ﬁle contains one record per trans-
action. Each record starts with three ﬁelds: TransactionID,
numberDepends, and needsUndo, and is followed by num-
berDepends transaction IDs. The needsUndo ﬂag is set if