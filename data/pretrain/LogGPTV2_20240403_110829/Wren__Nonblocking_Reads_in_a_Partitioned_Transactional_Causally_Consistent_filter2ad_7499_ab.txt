snapshots they have installed (§ III-C). In CANToR, when a
transaction starts, it chooses a transaction coordinator, and it
uses as its snapshot timestamp the LST value known to the
coordinator.
Figure 1b depicts the nonblocking behavior of Wren. pz
proposes 5 as snapshot timestamp (because of px). Then c1
can read without blocking on both px and py, despite the
concurrent commit of T2. The trade-off is that c1 reads older
versions of x and y, namely X1 and Y1, compared to the
scenarion in Figure 1a, where it reads X2 and Y2.
Only assigning a snapshot slightly in the past, however, does
not solve completely the issue of blocking reads. The local
stable snapshot includes all the items that have been written
by all clients up until the boundary deﬁned by the snapshot and
on which c (potentially) depends. The local stable snapshot,
however, might not include the most recent writes performed
by c in earlier transactions.
Consider, for example,
the case in which c commits a
transaction T , that includes a write on item x, and obtains
a value ct as its commit timestamp. Subsequently, c starts
another transaction T´, and obtains a snapshot
timestamp
st´smaller than ct, because ct has not yet been installed at
all partitions. If we were to let c read from this snapshot, and
it were to read x, it would not see the value it had written
previously in T .
A simple solution would be to block the commit of T until
ct ≥ LST . This would guarantee that c can issue its next
transaction T (cid:2) only after the modiﬁcations of T have been
applied at every partition in the DC. This approach, however,
introduces high commit latencies.
2) Client-side cache. Wren takes a different approach that
leverages the fact that the only causal dependencies of c that
may not be in the local stable snapshot are items that c has
written itself in earlier transactions (e.g., x). Wren therefore
provides clients with a private cache for such items: all items
written by c are stored in its private cache, from which it reads
when appropriate, as detailed below.
When starting a transaction, the client removes from the
cache all the items that are included in the causal snapshot, in
other words all items with commit timestamp lower than its
causal snapshot time st. When reading x, a client ﬁrst looks up
x in its cache. If there is a version of x in the cache, it means
that the client has written a version of x that is not included
in the transaction snapshot. Hence, it must be read from the
cache. Otherwise, the client reads x from px. In either case,
the read is performed without blocking 2.
C. Dependency tracking and stabilization protocols
BDT. Wren implements BDT, a novel protocol to track the
causal dependencies of items. The key feature of BDT is that
every data item tracks dependencies by means of only two
scalar timestamps, regardless of the scale of the system. One
entry tracks the dependencies on local items and the other
entry summarizes the dependencies on remote items.
The use of only two timestamps enables higher efﬁciency
and scalability than other designs. State-of-the-art solutions
employ dependency meta-data whose size grows with the
number of DCs [8], [16], partitions [24] or causal dependen-
cies [7], [15], [21], [25]. Meta-data efﬁciency is paramount
for many applications dominated by very small items, e.g.,
Facebook [3], [26], in which meta-data can easily grow bigger
than the item itself. Large meta-data increases processing,
communication and storage overhead.
BiST. Wren relies on BDT to implement BiST, an efﬁcient sta-
bilization protocol to determine when updates can be included
in the snapshots proposed to clients within a DC (i.e., when
they are visible within a DC). BiST allows updates originating
in a DC to become visible in that DC without waiting for the
receipt of remote items. A remote update d, instead, is visible
in a DC when it is stable, i.e., when all the causal dependencies
of d have been received in the DC.
2The client can avoid contacting px, because Wren uses the last-writer-wins
rule to resolve conﬂicting updates (see § II-A). With other conﬂict resolution
methods, the client would always have to read the version of x from px, and
apply the updates(s) in the cache to that version.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
4
BiST computes two cut-off values that indicate, respectively,
which local and remote items can become visible to transac-
tions within a DC. The local component computed by BiST is
the LST, which we described earlier. The remote component
is the Remote Stable Time (RST), that, similarly to the LST,
indicates a lower bound on remote snapshots that have been
installed by every node within the local DC.
By decoupling local and remote items, BiST allows trans-
actions to determine the visibility of local
items without
synchronizing with remote DCs [8], in contrast to systems that
use a single scalar timestamp for dependency tracking [20],
[24]. This decoupling enables availability and nonblocking
reads also in the geo-replicated case, because a snapshot
visible to a transaction includes only remote items that have
already been received in the local DC.
With BiST, periodically partitions within a DC exchange the
commit timestamp of the latest local and remote transactions
they have applied. Then, each partition computes the LST,
resp., RST, as minimum of the received timestamps corre-
sponding to local, resp., remote transactions. Therefore, LST
and RST reﬂect local and remote snapshots that have been
already installed by all partitions in the DC, and from which
transactions can read without blocking, as we explain in the
following.
Snapshots and nonblocking reads. When a transaction T
starts, the local, resp., remote, entry of the corresponding
snapshot S is set to be the maximum between the LST, resp.,
RST on the coordinator and the highest LST, resp. RST, value
seen by the client, ensuring that clients see monotonically
increasing snapshots.
T uses the timestamps in S to determine the version of an
item that it can read, namely the freshest version of an item
that falls within the visible snapshot (let alone the items that
are read from the client-side cache, as described in § III-B). S
includes local, resp., remote, items whose timestamp is lower
than the LST, resp., the RST. Because both LST and RST
reﬂect snapshots installed by every partition in the DC, T can
read from S in a nonblocking fashion.
Trade-off. BiST enables high scalability and performance at
the expense of a slight increase in the time that it takes for
an update to become visible in a DC (the so called visibility
latency). By using BiST, Wren only tracks the lower bound
on the commit time of local transactions (LST) and replicated
transactions coming from all
the remote DCs (RST). We
describe this trade-off by sketching in Figure 2 how BiST
and other existing stabilization protocols work at a high level.
In the example, the local DC (DC2) has committed transac-
tions with timestamp up to 15. It has received commits from
DC0 with timestamp up to 4, and from DC1 with timestamp
up to 6. Wren exposes to transactions remote items with
timestamp up to 4, the minimum of 4 and 6. Cure [8] uses one
timestamp per DC, so transactions can see items from DC0
with timestamp up to 4 and from DC1 with timestamp up
to 6. GentleRain [20] uses a single timestamp (the local one)
to encode both local and remote snapshots, so transactions
Fig. 2: Resource efﬁciency vs freshness in BiST (one partition
per DC). DC2 is the local DC.
can see all items up to timestamp 15. However, they have to
block until the local DC has received all remote updates with
timestamps lower than or equal to 15.
Timestamps. So far, we have assumed that Wren uses logical,
Lamport clocks to generate timestamps. Wren, instead, uses
Hybrid Logical Physical Clocks (HLC) [27]. In brief, an HLC
is a logical clock whose value on a partition is the maximum
between the local physical clock and the highest timestamp
seen by the partition plus one. HLCs combine the advantages
of logical and physical clocks. Like logical clocks, HLCs can
be moved forward to match the timestamp of an incoming
event. Like physical clocks, they advance in the absence of
events and at approximately the same pace.
Wren’s use of HLCs improves the freshness of the snapshot
determined by BiST, which, as a by-product, also reduces the
amount of data stored in the client-side caches. HLCs have
previously been employed by CC systems to avoid waiting
for physical clocks to catch up when generating timestamps
for updates [28], [29]. HLCs alone, however, do not solve the
problem of blocking reads with TCC. A snapshot timestamp
can be “in the future” with respect to the installed snapshot of
a partition, regardless of whether the clock is logical, physical
or hybrid.
D. Fault tolerance and Availability
Fault tolerance (within a DC). Similarly to previous trans-
actional systems based on 2PC, Wren can integrate fault-
tolerance capabilities by means of standard replication tech-
niques such as Paxos [30].
Wren preserves nonblocking reads, even if such fault tol-
erance mechanisms are enabled. In blocking systems, instead,
fault tolerance increases the latency incurred by transactions
upon blocking, because it increases the duration of a commit.
The failure of a server blocks the progress of BiST, but
only during the short amount of time during which a backup
partition has not yet replaced the failed one. The failure of a
client does not affect the behavior of the system. The clients
only keep local meta-data, and cache data that have already
been committed to the data-store.
Availability (between DCs). BiST is always available. Trans-
actions are never blocked or delayed as a result of the
disconnection or failure of a DC. The disconnection of DCi
causes the RST to freeze in all DCs that get disconnected from
DCi. However, because BiST decouples local from remote
dependencies, any RST assigned to a transaction refers to a
snapshot that is already available in the DC, and the trans-
action can thus proceed. The LST, instead, always advances,
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:26:55 UTC from IEEE Xplore.  Restrictions apply. 
5
Algorithm 1 Wren client c (open session towards pm
n ).
1: function START
2:
3:
4:
5:
6:
7: end function
send (cid:3)StartTxReq lstc, rstc(cid:4) to p
receive (cid:3)StartTxResp id, lst, rst(cid:4) from p
rstc ← rst; lstc ← lst; idc ← id
RSc ← ∅; W Sc ← ∅
Remove from W Cc all items with commit timestamp up to lstc
m
n
m
n
D ← ∅; χ
(cid:2) ← ∅
for each k ∈ χ do
8: function READ(χ)
9:
10:
d ← check W Sc, RSc, W Cc (in this order)
11:
if (d (cid:8)= N U LL) then D ← d
12:
13:
end for
(cid:2) ← χ \ D.keySet()
14:
χ
send (cid:3)TxReadReq idc, χ(cid:2)(cid:4) to p
15:
receive (cid:3)TxReadResp D(cid:2)(cid:4) from p
16:
D ← D ∪ D
17:
RSc ← RSc ∪ D
18:
19:
return D
20: end function
m
n
m
n
(cid:2)
21: function WRITE(χ)
22:
23:
24:
25: end function
for each (cid:3)k, v(cid:4) ∈ χ do
end for
(cid:3) Update W Sc or write new entry
if (∃d ∈ W S : d == k)then d.v ← v else W Sc ← W Sc ∪ (cid:3)k, v(cid:4)
26: function COMMIT
27:
28:
29:
30:
31:
32: end function
send (cid:3)CommitReq idc, hwtc, W Sc(cid:4) to p
receive (cid:3)CommitResp ct(cid:4) from p
hwtc ← ct
Tag W Sc entries with hwtc
Move W Sc entries to W Cc
m
n
(cid:3) Only invoked if W S (cid:8)= ∅
m
n
(cid:3) Update client’s highest write time
(cid:3) Overwrite (older) duplicate entries
ensuring that clients can prune their local caches even if a DC
disconnects.
IV. PROTOCOLS OF WREN
We now describe in more detail the meta-data stored and
the protocols implemented by clients and servers in Wren.
A. Meta-data
Items. An item d is a tuple (cid:4)k, v, ut, rdt, idT , sr(cid:5). k and v
are the key and value of d, respectively. ut is the timestamp
of d which is assigned upon commit of d and summarizes the
dependencies on local items. rdt is the remote dependency
time of d, i.e., it summarizes the dependencies towards remote
items. idT is the id of the transaction that created the item
version. sr is the source replica of d.
Client. In a client session, a client c maintains idc which iden-
tiﬁes the current transaction, and lstc and rstc, that correspond
to the local and remote timestamp of the transaction snapshot,
respectively. c also stores the commit time of its last update
transaction, represented with hwtc. Finally, c stores W Sc, RSc
and W Cc corresponding to the client’s write set, read set and
client-side cache, respectively.
Servers. A server pm
n is identiﬁed by the partition id (n)
and the DC id (m). In our description, thus, m is the local
DC of the server. Each server has access to a monotonically
increasing physical clock, Clockm
n . The local clock value on
pm
n is represented by the hybrid clock HLC m
n .
pm
n also maintains V V m
n , a vector of HLCs with M entries.
[i], i (cid:6)= m indicates the timestamp of the latest update
V V m
n
6
n - transaction coordinator.
(cid:3) Update remote stable time
(cid:3) Update local stable time
m
(cid:3) Save TX context
(cid:3) Assign transaction snapshot
m
m
m
m
m
n − 1}(cid:4)
send (cid:3)SliceReq χi, lt, rt(cid:4) to p
receive (cid:3)SliceResp Di(cid:4) from p
D ← D ∪ Di
(cid:3)lt, rt(cid:4) ← T X[idT ]
D ← ∅
χi ← {k ∈ χ : partition(k) == i}
for (i : χi (cid:8)= ∅) do
n ← max{rst
n , rstc}
rst
n , lstc}
n ← max{lst
lst
idT ← generateU niqueId()
T X[idT ] ← (cid:3)lst
n , min{rst
m
n , lst
send (cid:3)StartTxResp idT , T X[idT ](cid:4)
Algorithm 2 Wren server pm
1: upon receive (cid:3)StartTxReq lstc, rstc(cid:4) from c do
2:
3:
4:
5:
6:
7: upon receive (cid:3)TxReadReq idT , χ(cid:4) from c do
8:
9:
10:
11:
12:
13:
14:
15:
16:
17: upon receive (cid:3)CommitReq idT , hwt, W S(cid:4) from c do
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
(cid:3)lt, rt(cid:4) ← T X[idT ]
ht ← max{lt, rt, hwt}
Di ← {(cid:3)k, v(cid:4) ∈ W S : partition(k) == i}
for (i : Di (cid:8)= ∅) do
send (cid:3)PrepareReq idT , lt, rt, ht, Di(cid:4) to p
receive (cid:3)PrepareResp idT , pti(cid:4) from p
end for
ct ← maxi:Di(cid:3)=∅{pti}
for (i : Di (cid:8)= ∅) do send (cid:3)Commit idT , ct(cid:4) to p
delete TX[idT ]
send (cid:3)CommitResp ct(cid:4) to c
end for
send (cid:3)TxReadResp D(cid:4) to c
m
i
m
i
m
i
m
i
(cid:3) Partitions with ≥ 1 key to read
(cid:3) Max timestamp seen by the client
(cid:3) Done in parallel
(cid:3) Max proposed timestamp
m
i
end for
(cid:3) Clear transactional context of c
received by pm
n that comes from the n-th partition at the i-th
[m] is the version clock of the server and represents
DC. V V m
n
n . The server also stores lstm