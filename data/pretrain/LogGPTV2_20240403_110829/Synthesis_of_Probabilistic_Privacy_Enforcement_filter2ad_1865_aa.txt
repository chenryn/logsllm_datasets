title:Synthesis of Probabilistic Privacy Enforcement
author:Martin Kucera and
Petar Tsankov and
Timon Gehr and
Marco Guarnieri and
Martin T. Vechev
Synthesis of Probabilistic Privacy Enforcement
Martin Kučera, Petar Tsankov, Timon Gehr, Marco Guarnieri, Martin Vechev
ETH Zurich
PI:EMAIL
ABSTRACT
Existing probabilistic privacy enforcement approaches permit the
execution of a program that processes sensitive data only if the
information it leaks is within the bounds specified by a given policy.
Thus, to extract any information, users must manually design a
program that satisfies the policy.
In this work, we present a novel synthesis approach that auto-
matically transforms a program into one that complies with a given
policy. Our approach consists of two ingredients. First, we phrase
the problem of determining the amount of leaked information as
Bayesian inference, which enables us to leverage existing proba-
bilistic programming engines. Second, we present two synthesis
procedures that add uncertainty to the program’s outputs as a way
of reducing the amount of leaked information: an optimal one based
on SMT solving and a greedy one with quadratic running time.
We implemented and evaluated our approach on 10 representa-
tive programs from multiple application domains. We show that
our system can successfully synthesize a permissive enforcement
mechanism for all examples.
1 INTRODUCTION
Privacy enforcement systems, i.e. systems that protect the privacy
of sensitive data with respect to policies, must be both permissive
and secure. That is, users should be permitted to process sensitive
data while making sure they cannot learn too much information
about the sensitive data, thereby violating privacy policies. In the
context of genomic privacy, for instance, it is important to allow
medical researchers to process and aggregate genomic data as this
can be extremely valuable for their work; however, it is also critical
to preserve the privacy of the patients [3, 30]. This tension between
permissiveness and security of privacy enforcement permeates
many practical domains, including medical data protection [31],
location privacy in location-based services [29, 35, 58], and privacy
of personal data in social networks [39].
Existing Approaches. Existing access-control solutions [9, 52, 53]
can only enforce basic all-or-nothing policies for a particular piece
of sensitive data (hereafter called the secret). They are thus often
overly-restrictive in practice; see [37].
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Copyright held by the owner/author(s). Publication rights licensed to Associa-
tion for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134079
Program π
Privacy Policy Φ
1
Analysis
1
Synthesis
1
Program π
Enforcement ξ
+
0
Attacker belief δ
0
Belief bound for π
0
Belief bound for π +ξ
Figure 1: The two ingredients of our approach: probabilis-
tic analysis to determine the information leaked by the
program about the secret, and enforcement synthesis that
bounds the leakage according to the policy bounds.
Recently, probabilistic privacy enforcement (PPE) approaches
(e.g. [37]) have been proposed as a promising step towards improv-
ing the permissiveness of privacy enforcement. PPE approaches
can enforce a wide range of privacy policies that bound how much
an attacker can learn about the secret. For example, “no medical re-
searcher can correctly guess that Alice has disease X with probability
higher than 0.9”. To enforce such policies, PPE systems explicitly
model the attacker’s belief as a distribution over the possible val-
ues the secret can take. The attacker asks the PPE system to run a
program (i.e., submits a query) that takes the secret as input. The
PPE system then reveals the program’s output to the attacker only
if this does not leak too much about the secret; otherwise, the PPE
system rejects the attacker’s program.
Existing PPE systems reject a program if the program would
leak information for some possible secret. As an example, suppose
a medical researcher asks for the number of patients who have a
particular disease. Further, suppose the PPE system must enforce
that researchers cannot correctly guess that Alice (who is among
the patients) has the disease with probability higher than 0.9. If the
program outputs the total number of patients, then the researcher
could potentially learn that all patients (including Alice), have the
disease. The PPE system thus rejects this program.
Problem Statement. Since existing PPE systems would reject any
program that leaks too much information, a user must manually
modify the program, e.g. by perturbing its output [19, 47]. Design-
ing a policy-compliant program is hard and requires nontrivial
probabilistic reasoning. This puts a significant burden on users who
would like to process sensitive data.
In this work, we explore the problem of automatically trans-
forming programs into policy-compliant ones. More formally, we
address the following synthesis problem: Given a program π, an
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA391attacker belief δ, and a privacy policy Φ, transform π into a program
π ′ that is guaranteed to satisfy the privacy policy Φ for the given
attacker belief δ.
In this paper, we propose the first solution to the
This Work.
problem of transforming a program into a policy-compliant one,
where the policy is defined as a set of probabilistic assertions on the
distribution over the program inputs (capturing the attacker belief).
Our approach consists of two key ingredients, depicted in Figure 1.
First, we phrase the problem of determining how much information
the program’s outputs leak about the secret as probabilistic analysis,
and check whether the leakage is within the bounds specified by
the policy; we depict the safe bounds in green in Figure 1. Second, to
enforce the policy and reduce the amount of leaked information, the
key idea is to synthesize an enforcement that transforms the program
by adding uncertainty to its outputs. We show that solving this
problem optimally is NP-equivalent and present an algorithm using
a reduction to linear optimization over SMT constraints.
Main Contributions. Our main contributions are:
thesis problem (Section 4).
but not guaranteed to be optimal (Section 6).
linear optimization over SMT constraints (Section 5).
• A formulation of the permissive privacy enforcement syn-
• An optimal synthesis algorithm based on a reduction to
• A quadratic-time greedy synthesis algorithm that is sound
• An end-to-end implementation of our approach in a system
• An evaluation of Spire on 10 representative programs from
multiple application domains. We show that our system can
successfully synthesize a permissive enforcement mecha-
nism for all examples (Section 8).
called Spire1 (Section 7).
2 OVERVIEW
In this section, we first present a simple, but illustrative, example.
We then describe the probabilistic privacy model, which we borrow
from [37], and we illustrate our enforcement synthesis approach.
Finally, we present our attacker model.
2.1 Genomic Privacy Example
Genomic data is extremely valuable to medical researchers. Un-
fortunately, it also reveals sensitive personal information, such as
predisposition to various diseases [2].
In this example, we consider the position rs11200638 in the
HTRA1 gene [17]. At this position, each person has one of the fol-
lowing combinations of nucleotides: AA, AG, or GG, where A stands
for adenine and G for guanine. A person who has the combination
AA is 10 times more likely to develop Age-Related Macular Degen-
eration (ARMD), a medical condition that may result in blurred
vision or blindness [41].
Patients can usually choose whether they want nucleotides at
sensitive positions to remain private while their data is processed.
Protecting the privacy of genomic data is, however, extremely chal-
lenging. A patient’s genomic data is correlated with that of the
patient’s relatives, which enables highly nontrivial probabilistic
inference attacks [3, 30, 31].
1Available at: http://www.srl.inf.ethz.ch/probabilistic-security
In our example, we consider three patients—Alice, Bob, and their
child Carol—who have identified that their nucleotides at position
rs11200638 are GG, AA, and AG, respectively. Carol’s nucleotides are
inherited by randomly selecting one from Alice and one from Bob.
The nucleotides of Alice, Bob, and Carol are therefore statistically
correlated. For instance, anyone who knows that the nucleotides
of Alice and Carol are GG and AG, respectively, can infer that Bob
must have at least one adenine nucleotide. Bob wants to ensure
that no one can correctly guess that his nucleotides are AA with a
probability higher than 0.75. Since Alice and Carol’s genomic data
reveal information about Bob, it is insufficient to protect Bob’s data
alone to enforce his policy.
2.2 Probabilistic Privacy Model
We adopt the probabilistic privacy model of [37], which can capture
numerous practical scenarios. We informally describe the compo-
nents of the model on our motivating example. We formally define
this model in Section 3.
Secret and Attacker Belief. The secret is a (sensitive) value that
must be protected from the attacker. The attacker belief about
the secret is then modeled as a probability distribution over all
possible values the secret can take. We remark that in many settings
it is realistic to precisely model the attacker belief; for example,
whenever the secret is drawn from a well-known distribution, such
as census data, genomic data, and so forth.
In our example, the secret consists of Alice, Bob, and Carol’s
nucleotides, and the attacker belief assigns a probability to each
possible assignment of nucleotides for Alice, Bob, and Carol. We
capture this distribution as a probabilistic program, given by the
function belief() in Figure 2(b). In the program, we encode the
nucleotides of Alice, Bob, and Carol with a two-dimensional array
nucl that consists of three pairs of random variables (one pair per pa-
tient). Each of these random variables takes the value 0 or 1, which
we interpret as the adenine and guanine nucleotides, respectively.
The random variables that capture Alice and Bob’s nucleotides
are initialized as Bernoulli random variables that take the value
1 with probability 0.77 and the value 0 with probability 0.23. The
value 0.77 captures the frequency of guanine at position rs11200638,
as reported in [1]. Lines 7-8 specify that Carol inherits her nu-
cleotides from Alice and Bob randomly.
According to the attacker belief, the probability that Bob’s nu-
cleotides are AA, which corresponds to the probability of the event
nucl[Bob] = [A,A], is 0.0529.
Program. The attacker asks the system to run a program (e.g. a
query) that takes the secret as input. Suppose the attacker asks to
run the program that returns the number of adenine nucleotides
found at the rs11200638 positions in the HTRA1 genes of Alice, Bob,
and Carol; see Figure 2(a). For our example, this program returns
the value 3 because the secret is nucl=[[A,A],[G,G],[A,G]].
Based on the observed output, the attacker revises her belief
about the secret using Bayesian inference. The posterior distribution
can be computed using state-of-the-art probabilistic solvers, such
as [22, 40, 46]. For example, we can compute that, according to the
attacker’s revised belief, the probability that Bob’s nucleotides are
AA is 0.25, which is higher compared to her prior belief of 0.0529.
Session B4:  Privacy PoliciesCCS’17, October 30-November 3, 2017, Dallas, TX, USA3921
2
3
4
5
6
7
8
9
10
11
12
1
2
3
4
5
6
7
8
9
10
11
// Secret: nucl = [[A,A], [G,G], [A,G]]
def main(nucl: R[][]) {
A := 0; G := 1;
sum := 0;
for patient in [0..3) {
for position in [0..2) {
if (nucl[patient][position] == A) {
sum += 1;
} } }
// always outputs the exact sum
return sum;
}
(a) (Original) program
// Returns nucl[][] = 
def belief() {
Alice := 0; Bob := 1; Carol := 2;
nucl := array[3][2];
nucl[Alice] = [flip(0.77), flip(0.77)];
nucl[Bob] = [flip(0.77), flip(0.77)];
C0 := nucl[Alice][flip(0.5)];
C1 := nucl[Bob][flip(0.5)];
nucl[Carol] = [C0, C1];
return nucl;
}
(b) Attacker belief
1
(nucl[Bob] == [A,A], [0,0.75])
(c) Privacy policy
Figure 2: Example program, attacker belief, and privacy pol-
icy, written in the Psi language, presented in Appendix B.
Privacy Policy. The privacy policy can be captured by a set of
probabilistic assertions over the attacker’s belief. We capture such
policies as predicates over the possible values of the secret together
with lower and upper-bounds on the probabilities of these predi-
cates; see Figure 2(c).
Privacy Policy Enforcement. Existing enforcement systems, such
as [37], run the attacker’s program if for all possible outputs the
privacy policy is satisfied according to the attacker’s revised belief
and otherwise they reject the program. These approaches reject
the program because for the output 6 the probability that Bob’s
nucleotides are AA is 1, which is above the bound of 0.75. Note that
the existing enforcements reject the program although the program
would return 3 since the secret is nucl = [[GG],[AA],[AG]], and this
output does not result in a policy violation.
2.3 Synthesis of Permissive Privacy
Enforcement
We propose a novel synthesis approach for enforcing privacy poli-
cies in a more permissive way. The key idea is to synthesize an
enforcement for the given program which guarantees that the pri-
vacy policy is satisfied for the given attacker belief. To this end, the
enforcement modifies the program by conflating certain outputs
(e.g. outputs that result in policy violations) and makes them equally
likely. We remark that this is a common approach to add uncertainty
to the program output in order to leak less information [37, 49], as
we discuss in Section 4. Below, we informally illustrate this idea on
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
def main(nucl: R[][]) {
A := 0; G := 1;
sum := 0;