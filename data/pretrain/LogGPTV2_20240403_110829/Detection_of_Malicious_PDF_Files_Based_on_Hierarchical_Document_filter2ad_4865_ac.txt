a nonlinear transformation to input data which maps it into
a feature space with special properties, the so-called Repro-
ducing Kernel Hilbert Space (RKHS). The elegance of SVM
consists in the fact that such transformation can be done im-
plicitly, by choosing an appropriate nonlinear kernel func-
tion k(x1, x2) which compares two examples x1 and x2.
The solution α to the dual SVM learning problem, equiv-
alent to the primal solution w, can be used for a nonlinear
decision function expressed as a comparison of an unknown
example x with selected examples xi in the training data,
the so-called “support vectors” (marked with the black cir-
cles in Figure 4):
y(x) = X
αiyik(x, xi) − ρ
∈∈SV
w
M
Figure 4. Linear and nonlinear SVM
Efﬁcient implementations of SVM learning are available
in various machine learning packages, including WEKA
8http://www.rulequest.com/see5-info.html, v.2.07.
and SHOGUN. In our experiments, we used a well-known
stand-alone SVM implementation LibSVM9.
5 Experimental Evaluation
The goal of the experiments presented in this section is to
measure the effectiveness and throughput of our proposed
method and its operational applicability under real-world
conditions. In addition, we compare the classiﬁcation per-
formance of our method to other existing methods for de-
tecting malicious PDF ﬁles: PJSCAN and the established
antivirus tools. Our evaluation is based on an extensive
dataset comprising around 660,000 real-world PDF docu-
ments.
5.1 Experimental Dataset
Dataset quality is essential for the inference of mean-
ingful models as well as for a compelling evaluation of
any data-driven approach. For our evaluation, we have ob-
tained a total of 658,763 benign and malicious PDF ﬁles
(around 595 GB). Our dataset was collected from Google
and VIRUSTOTAL10, an online service that checks ﬁles up-
loaded by ordinary users for viruses using the majority of
available antivirus programs. The collected data comprises
the following 6 datasets:
D1: VIRUSTOTAL malicious, containing 38,207 (1.4 GB)
malicious PDF ﬁles obtained from VIRUSTOTAL dur-
ing the course of 18 days, between the 5th and 22nd
of March 2012, which were labeled by at least 5 an-
tiviruses as malicious,
D2: VIRUSTOTAL malicious new, containing 11,409
(527 MB) malicious PDF ﬁles obtained from VIRUS-
TOTAL 2 months later, during the course of 33 days,
between the 23rd of May and 24th of June 2012, which
were labeled by at least 5 antiviruses as malicious,
D3: VIRUSTOTAL benign, containing 79,200 (75 GB) be-
nign PDF ﬁles obtained from VIRUSTOTAL during
the course of 18 days, between the 5th and 22nd of
March 2012, which were labeled by all antiviruses as
benign,
D4: Google benign, containing 90,384 (73 GB) benign
PDF ﬁles obtained from 1,000 Google searches for
PDF ﬁles, without search keywords, during the course
of 2,000 days, between the 5th of February 2007 and
the 25th of July 2012
9http://www.csie.ntu.edu.tw/˜cjlin/libsvm/, v.3.12.
10https://www.virustotal.com/.
D5: Operational malicious, containing 32,526 (2.7 GB)
malicious PDF ﬁles obtained from VIRUSTOTAL dur-
ing the course of 14 weeks, between the 16th of July
and 21st of October 2012, which were labeled by at
least 5 antiviruses as malicious,
D6: Operational benign, containing 407,037 (443 GB)
benign PDF ﬁles obtained from VIRUSTOTAL dur-
ing the course of 14 weeks, between the 16th of July
and 21st of October 2012, which were labeled by all
antiviruses as benign.
The VIRUSTOTAL data comprises PDF ﬁles used by
people from all over the world, which brings us as close
to real-world private PDF data as possible. In fact, the be-
nign VIRUSTOTAL data is even biased towards being ma-
licious, as users usually upload ﬁles they ﬁnd suspicious.
The dataset obtained by Google searches removes the bias
towards maliciousness in benign data and attempts to cap-
ture the features of an average benign PDF ﬁle found on the
Internet.
Note that we consider a VIRUSTOTAL ﬁle to be mali-
cious only if it was labeled as such by at least 5 antiviruses.
Files labeled malicious by 1 to 4 AVs are discarded al-
together from the experiments because there is little con-
ﬁdence in their correct labeling, as veriﬁed empirically.
Given the lack of reliable ground truth for all PDF ﬁles, we
assumed the zero false positive rate for the antivirus prod-
ucts and could not perform their fair comparison with the
proposed method with respect to the false positive rate.
5.2 Experimental Protocol
Two types of experiments were devised to evaluate the
detection performance of the presented method: laboratory
and operational experiments.
The three laboratory experiments operate on static
data, captured in a speciﬁc point in time, where training
and classiﬁcation data are intermixed using 5-fold cross-
validation11:
• The Standard experiment is designed to evaluate the
overall effectiveness of our method on known mali-
cious and average benign data. To this end, we use the
VIRUSTOTAL malicious dataset (D1) and the Google
benign dataset (D4).
115-fold cross-validation works as follows: we randomly split our data
into 5 disjoint subsets, each containing one ﬁfth of malicious and one ﬁfth
of benign ﬁles. Learning and classiﬁcation are repeated ﬁve times, each
time selecting a different combination of four subsets for learning and the
remaining one for classiﬁcation. This experimental protocol enables us
to classify every ﬁle exactly once while ensuring that no ﬁle processed in
the classiﬁcation phase was used in the learning phase for the respective
model.
• The Suspicious experiment is designed to evaluate the
effectiveness of our method on PDF ﬁles that ordinary
users do not trust. For this experiment, we use VIRUS-
TOTAL malicious data (D1) and VIRUSTOTAL benign
data (D3). The classiﬁcation task in this experiment is
harder than in the Standard experiment since its be-
nign data is biased towards malicious.
• The WithJS experiment is designed to enable the com-
parison of our method to PJSCAN. For this experi-
ment, a subset of the datasets used for the Standard
experiment (D1 and D4) was used comprising only
those ﬁles that contain directly embedded JavaScript
which PJSCAN can extract; i.e., 30,157 malicious and
906 benign ﬁles.
In contrast, in the two operational experiments, classiﬁ-
cation is performed on ﬁles which did not exist at all at the
time of training, i.e., ﬁles obtained at a later time:
• The Novel experiment evaluates our method on novel
malicious threats when trained on an outdated training
set. For this experiment, we apply the models learned
in the Standard experiment to new VIRUSTOTAL ma-
licious data (D2), which is two months newer. Novel
benign data was not evaluated as its observed change
in this timespan was not signiﬁcant.
• The 10Weeks experiment is designed to evaluate the
classiﬁcation performance of our method in a real-
world, day-to-day practical operational setup and com-
pare it to the results of the best VIRUSTOTAL antivirus
in the same time period. This experiment is performed
on the data from the Operational benign (D6) and ma-
licious (D5) datasets, containing ﬁles gathered during
the course of 14 weeks. The experiment is run once ev-
ery week, for 10 weeks starting from week 5. In every
run, feature selection is performed on ﬁles gathered in
the past 4 weeks. A new model is learned from scratch
based on these features and data; this model is used
to classify the ﬁles obtained during the current week.
Thus the data obtained during weeks 1 to 4 is used to
learn a model which classiﬁes data gathered in week 5,
weeks 2 to 5 are used for week 6, etc.
Note that, in practice, there are no fundamental difﬁcul-
ties for periodic re-training of classiﬁcation models as new
labeled data becomes available. The models deployed at
end-user systems can be updated in a similar way to signa-
ture updates in conventional antivirus systems. As will be
shown in Section 5.4, SVMs are efﬁcient enough to allow
periodic re-training of models from scratch. Our decision
tree learning algorithm implementation, however, lacked
the required computational performance and was not evalu-
ated in this experiment.
5.3 Experimental Results
Both the decision tree learning algorithm and the SVM
were evaluated in our laboratory experiments. For the SVM,
we selected the radial basis function (RBF) kernel with γ =
0.0025 and a cost parameter C = 12, based on an empirical
pre-evaluation.
5.3.1 The Standard experiment
Table 1 shows detection results for both classiﬁcation al-
gorithms in the Standard experiment. The top part shows
the confusion matrices (the number of positive and negative
ﬁles with true and false classiﬁcations) obtained by aggre-
gating the results of all ﬁve cross-validation runs. The bot-
tom part shows other performance indicators: the true and
false positive rates and the overall detection accuracy.
True Positives
False Positives
True Negatives
False Negatives
True Positive Rate
False Positive Rate
Detection Accuracy
Decision tree
SVM
38,102
51
90,783
105
.99725
.00056
.99879
38,163
10
90,824
44
.99885
.00011
.99958
Table 1. Aggregated results of the Standard
experiment
The Standard experiment evaluates the overall perfor-
mance of our method under laboratory conditions. As Ta-
ble 1 shows, although the SVM slightly outperforms the de-
cision tree learning algorithm, both algorithms show excel-
lent classiﬁcation performance. Very high detection accu-
racy (over 99.8%) was achieved, while false positives rate
remained in the low promille range (less than 0.06%).
This experiment raises the question of how our method
compares to modern, fully up-to-date commercial antivirus
products. We were able to acquire detection results for 43
AV engines available at VIRUSTOTAL at the time of data
collection. It is important to note that, because VIRUSTO-
TAL runs the AVs using their command-line interface, the
detection capabilities of the AVs were limited to static ﬁle
processing.
Figure 5 shows the comparison of true positive rates of
the VIRUSTOTAL AVs and of our method using both de-
cision trees and SVM. With the mentioned limitations in
place, both of our algorithms outperform the commercial
antivirus engines in this respect, and as many as 30 antivirus
engines miss at least 15% of the threats.
Total
Our Method (SVM)
Our Mehtod (DT)
GData
Avast
DrWeb
BitDefender
F-Secure
Kaspersky
Microsoft
NOD32
Sophos
AntiVir
Ikarus
eTrust-Vet
Emsisoft
VIPRE
ClamAV
F-Prot
McAfee-GW-Edition
Fortinet
K7AntiVirus
McAfee
nProtect
Commtouch
Comodo
AVG
TrendMicro
TrendMicro-HouseCall
VirusBuster
Norman
Symantec
AhnLab-V3
CAT-QuickHeal
PCTools
Antiy-AVL
Jiangmin
Rising
eSafe
Panda
VBA32
ViRobot
TheHacker
ByteHero
SUPERAntiSpyware
Prevx
0
5000
10000
15000
25000
Number of detected files
20000
30000
35000
40000
Figure 5. Comparison of our method to com-
mercial antiviruses
5.3.2 The Suspicious experiment
Results for the Suspicious experiment are shown in Table 2.
The classiﬁcation performance of both algorithms indeed
decreases when applied to this harder, suspicious data in
comparison to the Standard experiment, however, the per-
formance degradation is very small.
True Positives
False Positives
True Negatives
False Negatives
True Positive Rate
False Positive Rate
Detection Accuracy
Decision tree
SVM
38,118
68
79,132
89
.99767
.00086
.99866
38,163
27
79,173
44
.99885
.00034
.99939
Table 2. Aggregated results of the Suspicious
experiment
Decision tree
SVM PJScan
Decision tree
SVM
True Positives
False Positives
True Negatives
False Negatives
True Positive Rate
False Positive Rate