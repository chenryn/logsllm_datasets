any other file, a B-Tree may or may not be physically contiguous on disk.
Figure 122. The structure of an HFS+ B-Tree node
[View full size image]
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 10 of 81
The node structure shown in Figure 122 is shared by all node types. There is a node descriptor (struct
BTNodeDescriptor [bsd/hfs/hfs_format.h]) at the beginning of each node. The bLink and fLink fields of
this structure chain together nodes of a particular type (indicated by the kind field) in the tree. Immediately
following the node descriptor is the records segment that contains the node's records. Since node records can
be of varying lengths, the latter part of the node contains a list of 16-bit offsets, each being a record's offset
from the beginning of the node. The last entry in the offset list is the offset to the unused space in the nodethat
is, the space immediately following the records segment and before the offset list. Note that if there is no free
space left, there still is a free space offset entryit points to its own offset in that case.
Figure 123 shows the structure of a B-Tree header node. The header node has exactly three records, namely,
the following:
The header record contains general information about the B-Tree, such as the tree's node size, its depth,
[11] the number of the root node (if any), the number of leaf records in the tree, and the total number of
nodes.
[11] A tree's depth is the same as its height. We say depth because we visualize the B-Tree's
structure as growing downward.
The user data record provides 128 bytes of space for storing arbitrary information associated with the
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 11 of 81
tree. Of all the HFS+ B-Trees, only the Hot File Clustering B-Tree uses this area.
The map record contains a bitmap, each of whose bits indicates whether a node in the tree is in use or
not.
Figure 123. The structure of an HFS+ B-Tree header node
[View full size image]
As we noted earlier, a tree may have more nodes than can be represented by the header node's map record,
whose size depends on the node size. The sum of the sizes of the node descriptor (14 bytes), the header record
(106 bytes), the user data record (128 bytes), and the offset entries (4 x 2 bytes) is 256 bytes, leaving the
remaining space for the map record. If additional space is required, the tree uses map nodes to house the
extension of the bitmap. If a tree has one or more map nodes, the header node's fLink field will contain the
number of the next map node. The first map node's fLink field will contain the next map node's number, if
any, and so on, with the last map node's fLink field being set to zero. The bLink fields of all map nodes, and
that of the header node, are always set to zero.
12.2.6.3. Records
Whereas the header and map nodes of a B-Tree contain administrative information for the tree itself, the index
and leaf nodes contain file system information, where the type of information depends on the specific B-Tree
in question. Nevertheless, the records in index and leaf nodes have the same general structure, which is shown
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 12 of 81
in Figure 124.
Figure 124. The structure of an HFS+ B-Tree record
At the beginning of the record is a key length (keyLength), which is stored using either one or two bytes,
depending on whether the attributes field in the B-Tree's header node has the kBTBigKeysMask bit clear or
set, respectively. Immediately following the key length is the actual key. The key length may or may not
represent the actual key length, which is determined as follows.
 In the case of a leaf node, keyLength represents the actual key length.
 In the case of an index node, keyLength represents the actual key length if the
kBTVariableIndexKeysMask bit is set in the header node's attributes field.
 If the kBTVariableIndexKeysMask bit is not set in the header node's attributes field, the actual key
length is the constant value contained in the header node's maxKeyLength field.
As shown in Figure 124, a record's data may be preceded and succeeded by single pad bytes. Record data is
required to be aligned on a two-byte boundary and to have a size that is an even number of bytes. If the
combined size of the key length and the actual key is such that the data would start on an odd-numbered byte, a
pad byte is inserted before the data. Similarly, if the data's size is an odd number of bytes, a pad byte is
inserted after the data.
Index and leaf nodes contain only index and leaf records, respectively. Since these are B+ Trees, the actual
data is stored only in leaf nodes. An index record's data is merely a node numbera pointer to another index
node or a leaf node. In other words, index nodes together constitute an index for arbitrarily searching the data
stored in leaf nodes.
12.2.6.4. Searching
A fundamental operation involved in B-Tree access and manipulation is key comparison. An important
property of a B-Tree node is that all records within the node are stored such that their keys are in increasing
order. For simple keys, say, integers, comparison could be as trivial as numerical comparison. Complex
keyssuch as those used in HFS+ B-Treeshave several components and therefore require more complicated
comparison operations. Typically, the various components of a complex key are assigned precedence values.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 13 of 81
When two keys of a given type are compared, the individual components are compared in decreasing order of
precedence. If an individual comparison results in equality, the overall comparison operation moves to the next
component. This process continues until there is an inequality or until all components are exhausted, in which
case the keys are deemed equal.
Figure 125 shows a hypothetical B-Tree that uses fixed-size integral keys. The tree's height is 3. In general,
leaf nodes, all of which are at the same level and therefore have the same height, are assigned 1 as their height.
An index node immediately above the leaf node has 2 as its height, and so on. The index node with the highest
height is the root node, a reference to which is maintained in the header node. Each node's height is contained
in the height field of the node descriptor. The header node's height field is always set to zero, but its
treeDepth field contains the tree's depth, which is the same as the root node's height.
Figure 125. The contents of a hypothetical HFS+ B-Tree
[View full size image]
In an empty tree, there is no root node. Moreover, a root node does not have to be an index node. If all records
are contained within a single node, that node is both the root node and the solitary leaf node.
The following are noteworthy observations about the tree shown in Figure 125.
 All nodes with a given height are chained together in a noncircular, doubly linked list through the fLink
and bLink fields of their respective node descriptors. In a given chain, the bLink field of the first node
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 14 of 81
and the fLink field of the last node are both set to zero.
The header record in the header node contains node numbers of the root node, the first leaf node, and the
last leaf node.
Within a node, records are stored in increasing order of their keys.
At any given height, all keys in a node are less than all keys in the node after it in the same-level chain.
As a corollary, the first node in the chain contains the smallest keys and the last node contains the largest
keys.
Let us see how data corresponding to a given keythe search keycan be searched for in this tree. The search
always begins at the root node, which can always be found by examining the header record. The latter is at a
fixed location within the tree, and the tree itself is assumed to be at a known location. Thereafter, the search
proceeds downward, eventually ending at a leaf node that contains the search key, unless the key does not exist
in the tree. In particular, the search algorithm used by HFS+ does not back upit accesses a node at most once
during a given search operation.
Suppose the search key is 38. We begin by examining the root node's records, with the goal of finding the
greatest key that is at most equal to but not greater than the search key. In this case, we would choose 32,
which is the greatest key in the root node but is still less than 38. The record's data is a pointer that leads us to
an index node that is one level down in the B-Tree. This node has three records. Again, we search for the
record with the largest key that does not exceed the search key: we choose 35. The corresponding pointer leads
us to a leaf node. The search key matches the key of the first record in the leaf node. Therefore, the search is
successful.
A B-Tree search is analogous to a binary search, except that at each decision point, we decide between
multiple paths instead of two paths. Searches within a node can be performed using any algorithm, with binary
search and linear search (for small nodes) being common alternatives. The HFS+ implementation uses binary
search.
For all leaf nodes to be at the same level, a B-Tree must be balanced. There exist several techniques to balance
a B-Tree. HFS+ uses the left-rotate and left-split operations to maintain a balanced tree. Intuitively speaking,
existing records are moved to the left and new records are inserted at the rightmost points in the tree.
12.3. The Structure of an HFS+ Volume
Figure 126 shows the structure of a representative HFS+ volume. Besides regular files and directories, an
HFS+ volume contains (or may contain, since some are optional) the following entities.
Reserved areas appear at the beginning and end of the volume.
The volume header contains a variety of information about the volume, including the locations of the
volume's other key data structures.
The alternate volume header is a copy of the volume header. It is located near the end of the volume.
The Catalog B-Tree stores the basic metadata for files and directories, including the first extent record
(i.e., up to the first eight extents) for each file. The file system's hierarchical structure is also captured in
the Catalog B-Tree through records that store parent-child relationships between file system objects.
The Extents Overflow B-Tree stores overflow (additional) extent records of files that have more than
eight extents.
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 15 of 81
The Attributes B-Tree stores extended attributes for files and directories.
The Allocation file is a bitmap containing a bit for each allocation block, indicating whether the block is
in use or not.
The private metadata folder is used for implementing hard links and for storing files that are deleted
while they are open (/\xC0\x80\xC0\x80\ xC0\x80\xC0\x80HFS+ Private Data).
The Hot Files B-Tree is used by the built-in Hot File Clustering optimization mechanism for recording
information about frequently accessed files (/.hotfiles.btree).
The Startup file is meant to contain arbitrary information that an operating system might use to boot
from an HFS+ volume.
Journal files are used to hold information about the file system journal (/.journal_info_block) and
the contents of the journal itself (/.journal).
Quota files are used to hold information pertaining to volume-level user quotas (/.quota.user) and
group quotas (/.quota.group).
Figure 126. The structure of an HFS+ volume
[View full size image]
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 16 of 81
In the rest of this chapter, we will explore the implementation and working of HFS+ by using hfsdebug to
examine various aspects of the file system.
12.4. Reserved Areas
The first two logical sectors (1024 bytes) and the last logical sector (512 bytes) of a volume are reserved.
Although Mac OS X does not use these areas, they were used by earlier Mac OS versions.
The 1024-byte reserved area at the beginning of a volume was used as boot blocks. These blocks contained
information necessary to boot the system, including the entry point to the boot code and names of several
critical files, for example:
System file (typically System)
Shell file (typically Finder)
First debugger installed during startup (typically Macsbug)
Second debugger installed during startup (typically Disassembler)
File containing the startup screen (StartUpScreen)
System scrap file (Clipboard)
The boot blocks also contained configurable system parameters such as the maximum number of open files
allowed, the fraction of physical memory to be used for the system heap, and the number of event queue
entries to allocate.
The 512-byte reserved area at the end of a volume was used by Apple during system manufacturing.
12.5. The Volume Header
The most critical structure of an HFS+ volume is the 512-byte volume header, which is stored at a 1024-byte
offset from the start of the volumeimmediately after the first reserved area. The information contained in the
volume header includes locations of various other important data structures. Unlike the volume header, these
other structures do not have a predefined, fixed locationthe volume header serves as a starting point for the
operating system (or other entities, such as a disk utility) while accessing the volume.
A copy of the volume headerthe alternate volume headeris stored at a 1024-byte offset from the end of the
volume, immediately before the last reserved area. Disk and file system repair utilities typically make use of
this copy.
12.5.1. Viewing the Volume Header
Let us use hfsdebug to display the volume header of an HFS+ volume, which we will first create using
hdiutil. The following hdiutil command line creates a disk image containing a 32MB journaled HFS+
volume, with the volume name being HFSJ. We can then mount the volume using either the hdiutil or open
command-line programs.
$ hdiutil create -size 32m -fs HFSJ -volname HFSJ /tmp/hfsj.dmg
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 17 of 81
...
created: /tmp/hfsj.dmg
$ hdiutil mount /tmp/hfsj.dmg
/dev/disk10 Apple_partition_scheme
/dev/disk10s1 Apple_partition_map
/dev/disk10s2 Apple_HFS /Volumes/HFSJ
As Figure 127 shows, the volume header contains the extents of the HFS+ B-Trees and of other special files.
Figure 127. The contents of an HFS+ volume header
$ hfsdebug -d /dev/rdisk10s2 -v
# HFS Plus Volume
Volume size = 32728 KB/31.96 MB/0.03 GB
# HFS Plus Volume Header
signature = 0x482b (H+)
version = 0x4
lastMountedVersion = 0x4846534a (HFSJ)
attributes = 00000000000000000010000000000000
. kHFSVolumeJournaled (volume has a journal)
journalInfoBlock = 0x2
createDate = Sun Oct 9 19:24:50 2005
modifyDate = Sun Oct 9 19:28:36 2005
backupDate = Fri Jan 1 00:00:00 1904
checkedDate = Sun Oct 9 19:24:50 2005
fileCount = 3
folderCount = 3 /* not including the root folder */
blockSize = 4096
totalBlocks = 8182
freeBlocks = 6002
nextAllocation = 2807
rsrcClumpSize = 65536
dataClumpSize = 65536
nextCatalogID = 22
writeCount = 3
encodingsBitmap = 00000000000000000000000000000000
00000000000000000000000000000001
. MacRoman
# Finder Info
...
# Allocation Bitmap File
...
# Extents Overflow File
logicalSize = 258048 bytes
totalBlocks = 63
clumpSize = 258048 bytes
extents = startBlock blockCount % of file
0x803 0x3f 100.00 %
63 allocation blocks in 1 extents total.
63.00 allocation blocks per extent on an average.
# Catalog File
...
# Attributes File
logicalSize = 0 bytes
# Startup File
logicalSize = 0 bytes
Note in Figure 127 that the volume header's signature field contains the two characters H+. If this were a
case-sensitive volume, this field would contain HX. Similarly, the version field would contain the value 5
(instead of 4) for a case-sensitive volume.
While mounting an HFS+ volume, an HFS+ implementation is required to identify itself by setting the
lastMountedVersion field in the volume header. This way, an implementation can detect whether there might
be a problem because of an earlier mount (e.g., if a journaled volume was mounted without journaling).
file://C:\Dokumente und Einstellungen\Silvia\Lokale Einstellungen\Temp\~hhC53.htm 20.08.2007
Chapter 12. The HFS Plus File System Page 18 of 81
Examples of values contained in the lastMountedVersion field include 8.10 (mounted on Mac OS 8.1
through 9.2.2), 10.0 (mounted nonjournaled), HFSJ (mounted journaled), fsck (mounted by fsck), and
registered creator codes (mounted by a third party represented by the creator code).
The hfsdebug output in Figure 127 includes several dates. HFS+ dates are stored as 32-bit unsigned integers
containing the number of seconds since midnight, January 1, 1904, GMT.[12] The volume creation date,
however, is stored as local time instead of GMT. Since Unix-style dates are represented as the number of
seconds since midnight, January 1, 1970, UTC, one must convert HFS+ dates to Unix-style dates before
calling functions such as gmtime() and localtime().
[12] The date integer would overflow after 6:28:15 GMT on February 6, 2040.
Note that the backup date shown in Figure 127 is January 1, 1904. This is because the corresponding date
integer contains a zerothat is, it has not been set, say, by a backup utility.
The HFS+ volume header's information in Figure 127 indicates that there are three files and three folders[13]
(in addition to the root folder) on the newly created volume. Let us account for these files and folders, while
noting that some are created not along with the file system but when the volume is mounted in the Desktop
environment. (See the sidebar "Disk Arbitration.")
[13] We will treat the terms folder and directory as synonymous in the context of HFS+.
On a volume without user home directories (typically a nonboot volume), the per-user trash folders, which are
used for storing files that have been dragged to the trash, are named .TRashes/, with  being the
numerical user ID of a user.
% id
uid=501(amit) gid=501(amit) groups=501(amit) ...
% sudo ls -l /Volumes/HFSJ/.Trashes
total 0
drwx------ 2 amit
amit 68 19 Apr 00:58 501
On a boot volume, the per-user trash folders are in the respective home directories (~/.trash).
Thus, two folders are accounted for: .trashes and .trashes/501. These two folders will not exist if you
mount our newly created volume manuallysay, using the mount_hfs program from the command line.
Disk Arbitration
As we saw in Chapter 11, the Disk Arbitration daemon (diskarbitrationd) is in charge of
mounting volumes as disks, disk images, and removable media devices appear.
Volumes are mounted by diskarbitrationd under the /Volumes directory. Each such volume