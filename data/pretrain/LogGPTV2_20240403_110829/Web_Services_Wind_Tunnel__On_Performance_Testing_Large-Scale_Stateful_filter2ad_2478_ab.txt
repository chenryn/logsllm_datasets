j
iX  
jp for 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20071
01
X
p
p
12
p
22
p
n
1
2
X
p
02
p
13
p
23
...
...
...
...
n
n
X
p
0
p
n
1
p
2
n
p
n
2
...
p
nn
0
X
p
00
p
11
p
0
1
2
0
21
p
n
X
X
X
(cid:35)
X
n
where: 
• 
•  And we have: 
=
i
≤ ijp
ijp
∀
i
j
|
0
• 
n
•  ∑
For 
,
=
j
0
ijp represents the transition probability  
jn
;
=
,...,1,0
n
,...,1,0
1
≤
=
1 
During the  knowledge  exerciser  step  our  objective 
is to produce load patterns that are as close as possible 
to  those  in  production,  using  the  Markov  Transition 
Matrix  we  created  during  the  Knowledge  Retriever 
step. 
An important aspect of this methodology is that the 
transition  matrix  extracted  only  reflects  the  average 
behavior  over  a  certain  period  of  time.  Therefore,  in 
order to reproduce the exact same pattern observed in 
production  (in  terms  of  different  variables,  such  as 
CPU  utilization,  memory  utilization,  disk  utilization, 
etc.), the matrix has to be periodically updated. 
The  following  diagram  (figure  3)  describes  the 
entire workflow: 
Figure 3. Markov Chain workflow 
In  the  above  workflow,  we  first  aggregate  the 
production activity logs by session, and order them by 
timestamps. Through data mining over the aggregated 
activity logs, we can get: (1) the Markov Transaction 
Matrix  over  the  APIs,  and  (2)  the  interval  between 
each of the APIs transacting (some authors refer to it as 
“Thinking Time” [10]). Through data mining over each 
API’s parameters with aggregated statistics, we can get 
the  parameter  calling  patterns  of  each  API.  At  this 
point, we are done with the knowledge retriever step.  
Our next step is to exercise the knowledge collected 
in previous steps in the stress testing environment. In 
practice,  we  found  that  it  is  more  challenging  to 
recover and mimic the parameter patterns of each API 
call than to produce the Markov transition matrix. This 
occurs  due  to  the  fact  that  the  majority  of  input 
parameters  are  user-specific  information  with  a  high 
degree  of  randomness  (i.e.,  user’s  first  name).    For 
user-specific information, we created tools or methods 
that would generate valid parameter values by creating 
them or retrieving them from our data store, and then 
deposit them in parameter pools. 
We  then  use  a  thread  to  represent  a  client.  This 
thread will call APIs one by one based on the Markov 
Transaction Matrix. We also introduce a sleep interval 
between  two  contingent  API  calls  to  make  the 
simulation more realistic. The model will fetch needed 
parameters from the parameter pool for each API call. 
Our model is scalable such that generation of stress 
load is possible. The parameter generator and threads 
can  be  distributed  across  machines  via  configurable 
parameters.  
Figure 4 is an example of our stress load simulation 
during a 3 hour run. In this example, we generated load 
on the primary database servers in a test environment 
using knowledge learned from SQL profiler traces on 
database  servers  with  the  same  role  in  production 
environments. The load profile in our test clusters was 
very similar to that observed in production. 
n
o
i
t
a
z
i
l
i
t
U
U
P
C
%
CPU utilization in Production and Test Environment simulation
100
90
80
70
60
50
40
30
20
10
0
1 25 49 73 97 121 145 169 193 217 241 265 289 313 337 361 385 409 433 457 481 505 529 553 577 601 625 649 673 697 721
Time Series
Production
TestEnv
Figure 4. Load profiles in production and test 
The  Markov  chain  model  not  only  simulates 
production-like  stress  load  on  test  environments,  but 
also  provides 
insights  about  system 
behavior,  especially  the  correlation  among  APIs  and 
among  the  parameters  of  each  API.  For  example, 
highly correlated APIs might get a performance boost 
by  improving  locality,  while  unexpected  correlations 
important 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007might  be  an indication  of  a  potential area  in need  of 
further inspection. 
4. Cache-based Load Simulation Tools 
Methods  described  in  previous  sections  provide  a 
large  dataset  resembling  production  as  well  as  a 
practical  way 
to  analyze  and  simulate  real-user 
behaviors (i.e. scenarios). The performance tuning of a 
stateful  system,  however,  often  requires  a  particular 
API  to  be  executed  numerous  times  in  isolation  in 
order to determine the bottlenecks of the system. Web 
services  are  generally  modeled  after  finite  state 
machines  in  that  the  majority  of  APIs  expect  the 
invoking  entity  to  be  in  a  certain  state  prior  to  an 
operation  [11].  The  conventional  test  approach  is  to 
generate an entity and induce it into the required state 
before the actual API invocation (sometimes the state-
inducement makes use of other APIs). This on-the-fly 
data  generation  and  entity  state  inducement  can  hide 
actual  system  bottlenecks,  since  the  preparation  work 
may  be 
the 
conventional  approach  does  not  allow  for  taking 
advantage of sanitized production data.  
itself.  In  addition, 
the  bottleneck 
Our  solution  is  cache-based  load  simulation.  This 
process  requires  pre-determination  of  the  individual 
API calls to the state(s) in which the entity it operates 
against must be. Each defined state can be represented 
as  a  bucket.  The  bucket  definitions are  collections  of 
Boolean conditions. When a particular entity matches 
the pre-defined set of conditions, the entity is said to 
belong to the corresponding bucket. The states are not 
mutually  exclusive,  so  a  given  entity  can  potentially 
satisfy more than one state condition and therefore be 
present in more than one bucket in the cache. 
Once  the  API  state  mapping  and  the  evaluation 
criteria  for  each  state  have  been  established,  the 
process  becomes  trivial.  Given  any  API,  we  execute 
the following: 
1.  Determine the matching bucket for the API 
2.  Extract an entity from the bucket 
3. 
Invoke the API with the selected entity 
4.  Re-evaluate state post API invocation 
5.  Re-insert  the  entity  in  the  new  bucket(s).  If 
entity is no longer usable, it is discarded 
Our implementation of the entity cache uses a SQL 
database to prevent data loss in the event of a crash, as 
well  as  allowing  easy  data  sharing  across  multiple 
instances of the tool. 
To  simplify  access  to  the  database,  a  layer  of 
abstraction was introduced to wrap the database calls. 
This  layer  exposes  methods  to  modify  the  entities  as 
well as the buckets. This layer exposes other methods 
in addition to the normal “add”, “get”, and “remove” 
calls (table 3): 
Method Type 
Initialization 
Bucket Access 
Entity Access 
Method 
InitializeCache 
LoadBuckets 
Clear 
AddBucket 
GetBucketList 
GetCountPerBucket 
AddEntity 
GetEntityFromBucket 
RemoveEntity 
GetPreExistingEntity 
Table 3. DB access methods for cached-based simulation 
Since the operations performed on the entities will 
most  likely  change  the  entity  state,  each  entity  is 
similar to a critical section – only one thread may act 
on an entity at a time (however, multiple threads can 
still operate on different entities). To avoid corruption 
of entity state, we remove the entity from its matching 
buckets just prior to the API invocation, and after the 
API execution, we re-evaluate the entity state and place 
it back into the appropriate bucket(s). 
The  aforementioned  process  depends  on  a  well-
populated cache to work. During the cache-population 
the  database  access  method 
stage,  we  utilize 
“GetPreExistingEntity”  mentioned  above 
to  pick 
random  entities  from  the  existing dataset and use  the 
state  evaluation  step  to  place  the  entities  in  the 
appropriate buckets in the cache. Population of specific 
buckets may only be accomplished by executing a set 
of  pre-defined  steps,  which  may  or  may  not leverage 
existing  data.  Therefore,  the  resulting  cached  data 
would  then  be  a  combination  of  sanitized  data  and 
synthetic data.  
In  practice,  we  observed  that  the  entity  state  can 
usually  be  determined  by  parsing  the  results  from 
“state-retrieving”  API  calls  (e.g.  Get  calls).  In  cases 
where  the  state-retrieving  API  calls  do  not  provide 
sufficient information, we construct custom data access 
methods to query the data store.  
This cache-based approach can be implemented in 
any  of  the  existing  commercial  applications  for  load 
generation  since  the  model  is  focused  on  interactions 
with the underlying system and not the load volume.  
5. Results and Future Work 
We  have  described  in  this  paper  techniques  for 
building  proper  environments  and  performance  test 
tools  which  can  be  used  to  accurately  simulate  the 
(live) 
same  conditions  observed 
in  production 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007[6] O.C. McDonald, X. Wang, M. De Barros, R.K. Bonilla, 
Q.  Ke,  “Strategies  for  Sanitizing  Data  Items”,  US  Patent 
Application (patent pending), 2004 
[7] S. Abiteboul, P. Buneman, D. Suciu, "Data on the Web: 
From  Relational 
to  Semistructured  Data  and  XML", 
SIGMOD Record, Vol. 32, No. 4, December 2003 
[8] Stuart J. Russell, Peter Norvig, Artificial Intelligence: A 
Modern Approach (2nd Edition), Prentice Hall, Upper Saddle 
River, NJ, Dec, 2003 
[9]  W.R.  Gilks,  S.  Richardson,  D.J.  Spiegelhalter,  Markov 
Chain Monte Carlo in Practice, Chapman & Hall/CRC, Dec, 
1995. 
[10] J.D. Meier, Srinath Vasireddy, Ashish Babbar, and Alex 
Mackman,  Improving  .NET  Application  Performance  and 
Scalability, Microsoft Corp., Redmond, WA, April 2004 
[11] B. Benatallah, F. Casati and F. Toumani, “Web service 
conversation  modeling:  A  corner-stone  for  e-business 
automation,” IEEE Internet Computing, 2004. 
targeting 
tests  of 
large-scale  stateful  web 
environments, 
services. The use of Data Sanitization, Markov Chain 
Stress Model, and Cache-based Load Simulation Tools 
have  been  successfully  used  for  benchmark,  capacity 
planning,  and  scalability 
three  major 
distributed web services: Subscription and Commerce 
Web  Services,  Identity  Services  Web  Services,  and 
Customer  Assistance  Web  Services,  all  part  of  the 
Microsoft  Member  Platform  Group.  Accuracy  of 
performance  numbers  collected  in  test  laboratories 
have  increased  to  a  deviation  of  less  than  5%  from 
performance  numbers  observed 
in  production 
environments  (compared  to  ~9%  with  synthesized 
data). The number of real performance and functional 
issues found during the quality assurance process has 
increased  by  15%  with 
the 
techniques described in this paper as part of the testing 
methodology. 
the  introduction  of 
involves 
Future  work 
the  enhancement  and 
generalization  of  the  techniques  described  in  this 
paper, including: 
•  Extending the application of the data sanitization 
process  to  other  data  sources  in  addition  to 
relational databases 
•  Real-time data sanitization 
•  Generalization of the application of Markov Chain 
Stress Model to different log sources 
•  Generalization of the Cache-based load simulation 
tools to automatically identify potential matching 
buckets based on the Finite State Machine for the 
system being tested. 
6. References 
[1]  “Privacy  in  e-commerce: examining  user  scenarios  and 
privacy preferences”, Proceedings of the 1st ACM conference 
on Electronic commerce, ACM, 1999. 
[2]  W.E.  Howden,  “Reliability  of  the  path  analysis  testing 
strategy”, IEEE Trans. Software Engineering, vol SE-2, 1976 
Sep 
[3]  Y.  Saito,  B.N.  Bershad,  H.M.  Levy,  "Manageability, 
availability, and performance in porcupine: a highly scalable, 
cluster-based mail service", ACM Transactions on Computer 
Systems, 2000 
[4]  J.  Edvardsson,  "A  survey  on  automatic  test  data 
generation", Proceedings of the Second Conference on 
Computer Science and Engineering, ECSEL, October 1999. 
[5]  SRM  Oliveira,  OR  Zaıane,  “Protecting  Sensitive 
Knowledge By Data Sanitization”, Third IEEE International 
Conference on Data Mining, ICDM 2003 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:30:40 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007