1
2 BB_0xef13a586:
3
/∗ LIFT’s FastPath(FP) optimization ∗/
if (unlikely( REG(EBX) || MEM_E(rbuf(0) + 4) || REG(EDI) || . . . ))
{
/∗ propagation body ∗/
REG(EBX) = MEM_E(rbuf(0) + 4);
. . .
REG(EDI) |= MEM_E(rbuf(2) − rbuf(1) −200);
REG(ECX) = MEM_E(rbuf(2));
}
4
5
6
7
8
9
10
11
12
13
/∗ update the global address array ∗/
14
15 garg(0) = rbuf(1);
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
/∗ increase index ∗/
INC_IDX(3) ;
/∗ control transfer ∗/
if (likely(IS_VALID_EA(rbuf(0) )) {
/∗ direct jump ∗/
goto BB_0xef13a598;
} else {
if (likely(rbuf(0) == 0xef13a5ba))
goto BB_0xef13a5ba;
} else {
/∗ hash lookup for computed goto ∗/
goto locateBbIdLabel();
}
}
Listing 1: Example of secondary code for a BBL.
We exploit the fact that we can now run conditional statement
fast, since we are running the analysis outside the DBI, to imple-
ment a simpliﬁed version of the FastPath (FP) optimization pro-
posed by LIFT [28]. This is done in line 8, where we ﬁrst check
whether any of the input and output variables involved in this block
are tagged, before propagating empty tags.
Last, in line 15 we save the EA that the inter-block optimization
determined it is also used by successor BBLs in the global argu-
ments array garg, and progress the increasing index of the ring
buffer in line 18.
4. RUNTIME
The off-line application analysis produces the enqueueing stubs
for the primary and the analysis body for the secondary. These are
compiled into a single dynamic shared object (DSO) and loaded
by ShadowReplica at start up. In this section, we describe various
aspects of the runtime environment.
4.1 DFT
4.1.1 Shadow Memory Management
The shadow memory structure plays a crucial role in the perfor-
mance of the secondary. Avoiding conditional statements in in-line
DFT systems, frequently restricts them to ﬂat memory structures
like bitmaps, where reading and updating can be done directly. An
approach that does not scale on 64-bit architectures, because the
bitmap becomes excessively large. Due to our design, conditional
statements do not have such negative performance effects.
The approach we adopted is borrowed from libdft [19] that im-
plements byte-per-byte allocation, with a low-cost shadow address
translation, and dynamic allocation to achieve a small memory foot-
print. Shadow memory is maintained in sync with the memory used
by the application by intercepting system calls that manage mem-
ory (e.g., mmap(), munmap(), brk()) and enqueueing special
control commands that allocate and deallocate shadow memory ar-
eas. The information sent includes the command code and an ad-
dress range tuple (offset, length).
To access shadow memory, we ﬁrst consult a page table like data
structure, which points to a per-page ﬂat structure. The mecha-
nism does not require a check for page validity because intercepting
the memory management calls ensures it. Accesses to unallocated
space are trapped to detect invalid memory addresses.
4.1.2 DFT Sources and Sinks for Taint Analysis
Tagging memory areas, when data are read from a ﬁle or the
network, is performed by generating code that enqueues another
control command in the ring buffer. The data sent to the secondary
include the command code, an address range tuple (offset, length),
and the tag value (or label). The value with which to tag the address
range in shadow memory depends on the particular DFT-logic. For
instance, DTA uses binary tags, data can tainted or clean. In this
case, we can simply omit the tag value.
Checking for tagged data can be either done by enqueueing a
control command, or it can be done entirely by the secondary, as
it is in the case of DTA. For instance, whenever a BBL ends with
a ret instruction, the secondary checks the location of the return
address in the stack to ensure it is clean. If we wanted to check
for sensitive information leaks, we would have to issue a command
from the primary whenever a write() or send() system call
is made. Note that all checks are applied on the secondary, which
can take action depending on their outcome. For DTA, if a check
fails, we immediately terminate the application, as it signiﬁes that
the application’s control ﬂow has been compromised. Other actions
could involve logging memory contents to analyze the attack [32],
or allowing the attack to continue for forensics purposes [27].
Moreover, the primary synchronizes with the secondary before
potentially sensitive system calls (e.g., write() and send())
that may leak sensitive information from the system. This ensures
that the secondary does not fall behind, allowing an attack to go un-
detected for a long time, or at least not before leaking information.
This granularity of synchronization does leave a small but, never-
theless, existing time window that the attacker can leverage to sup-
press the secondary and our checks. We have complemented our
implementation to also require synchronization whenever a new
BBL (i.e., a new execution path) is identiﬁed at run time to guard
against such an event.
4.1.3 Generic Block Handler
BBLs that were not identiﬁed during proﬁling are processed by
the Generic Block Handler (GBH), our slow-path implementation
of DFT. In this case, the primary on-the-ﬂy transforms instructions
and their operands into their DFT-speciﬁc representation of TFA.
Then, it implements a stack-based interpreter that enqueues the
DFT operations (i.e., AND, OR, ASSIGN), which need to be per-
formed by the secondary, in the ring buffer.
4.2 Ring Buffer
We implemented the ring buffer, as a lock-free, ring buffer struc-
ture for a single producer (primary), single consumer (secondary)
model. Lamport et al. [20] initially proposed a lock-free design for
the model, however, the design suffers from severe performance
overheads on multi-core CPUs due to poor cache performance [14].
The most serious problem we observed was forced L1 eviction that
occurs as the secondary attempts to read the page that was just mod-
iﬁed by the primary, while it is still in the primary’s L1 cache and
before it is committed to RAM. In certain cases, the overhead due
to ring-buffer communication increased to the point that our frame-
work became unusable. To mitigate the problem, we switched to
241a N -way buffering scheme [33], where the ring-buffer is divided
into N separate sub-buffers, and primary and secondary threads
work on different sub-buffers at any point of time, maintaining a
distance of least as much as the size of a single sub-buffer.
Synchronizing between primary and secondary is also performed
through the shared data structure. We extended the typical N -way
buffering implementation, by adding a control entry for each sub-
buffer. When the primary ﬁnishes updating a sub-buffer, it records
the number of entries it enqueued in the control entry and moves
the next sub-buffer. If the next sub-buffer’s control entry is zero,
the secondary already ﬁnished its work, so the primary can proceed
to ﬁll that up as well. Otherwise, the primary waits, periodically
polling the control entry. Likewise, the secondary polls the con-
trol entry to check whether there are data for it to process. When
primary and secondary need to be synchronized (e.g., before the
execution of sensitive system calls), the primary retires early from
its current sub-buffer and waits until the secondary clears up all
entries and updates the control entry with zero, thus synchronizing.
Data Encoding. Data enqueued in the ring buffer belong to one
of the following three groups: (a) effective addresses (EAs), (b)
BBIDs, and (c) control commands. We exploit the fact that valid
EAs point to user-space ranges (0G ∼ 3G range on x86 Linux)
to quickly differentiate them from other entries without using ad-
ditional bits. We map BBIDs and control commands to address
ranges that correspond to kernel space (3G ∼ 4G range on x86
Linux), which immediately differentiates them from EAs that are
always in lower address ranges. This design is easily applicable on
64-bit architectures and other operating systems.
4.3 Support for Multi-threaded Applications
Multi-threaded applications are composed by critical regions,
where they access shared data in a safe manner by acquiring a
lock. To ensure that secondary threads operate on shadow mem-
ory in the same order that the primary threads operated on real
data, we need to identify when a thread enters such a critical re-
gion. We accomplish this by intercepting mutex related calls in
the POSIX threads library, namely pthread_mutex_lock()
and pthread_mutex_unlock(). Our approach can be eas-
ily extended to also support other locking schemes, but currently
does not guarantee shadow memory consistency for shared-data ac-
cesses where no locking is performed.
Lock-protected critical regions are handled as follows. When
one of the primary threads acquires a lock, it waits until its sec-
ondary clears all entries from the ring buffer, essentially synchro-
nizing with the secondary. Every lock in the primary is associ-
ated with a separate ring buffer structure and another secondary
thread attached to it, when it is created. The primary proceeds to
switch its ring-buffer pointer to the one that belongs to the particu-
lar lock and resumes execution. Similarly, when releasing the lock,
the primary synchronizes with the current secondary and resumes
using its original ring buffer. This process ensures that the order of
shadow memory operations is consistent with the order of opera-
tions in the application. In the future, we plan to explore additional
optimizations targeting multi-threaded applications.
5.
IMPLEMENTATION
We implemented the dynamic proﬁler as tool for the Pin DBI
framework in ∼ 3,800 lines of C++. The runtime environment,
responsible for injecting the enqueueing stubs in the application
and hosting the secondary threads, was also build as a tool for Pin
in ∼4,000 lines of C++ code. The code analysis and generation
represents the majority of our effort. It consists of ∼12,000 lines
of Python code.
 3.5
 3
)
d
e
z
i
l
a
m
r
o
n
(
n
w
o
d
w
o
l
S
 2.5
 2
128k
256k
512k
1024k
2048k
4096k
8192k
16834k
Figure 5: Choosing right ring-buffer size.
Currently, all of the secondary’s functionality is implemented as
a single function with labels for each BBL segment. This design
choice was made to exploit the compiler’s intra-procedure opti-
mizations that can better organize the code and improve locality.
An alternate design adopting a function abstraction per BBL cannot
enjoy these optimizations, and would suffer higher function call/re-
turn overheads. We discovered that a large number of labels caused
the GCC compiler to fail due to resource exhaustion when run-
ning CFG related optimizations.3 This currently prevents us from
building a single-function program for applications with more than
∼ 50K BBLs. As a workaround, we divide the analysis program
into multiple sub-functions, where each represents a clustered sub-
graph from the CFG.
Fig. 5 shows how ShadowReplica’s communication cost varies,
as we set the size of the ring-buffer with different values. In this ex-
periment, we ran bzip2 compression against Linux kernel source
(v3.7.10; ∼ 476MB) using a ring buffer comprising of 8 sub-buffers,
and having the secondary only consume data. When the size of
the whole structure is too small (128k), performance suffers due to
L1 cache eviction, as each sub-buffer of 16KB is smaller than the
size of L1 cache (32KB). When it becomes larger performance im-
proves, until it becomes larger than 4096KB, where performance
starts to suffer due to increased L3 cache misses. In our prototype
implementation, we set the ring buffer size to 512KB and conﬁg-
ured it with 8 sub-buffers. The size of each sub-buffer is 64KB,
twice as large as our test system’s L1 cache size.
6. EVALUATION
We evaluated ShadowReplica using common command-line util-
ities, the SPEC CPU2006 benchmark suite, and a popular web and
database (DB) server. Our aim is to quantify the performance gains
of our system, and analyze the effectiveness of the various opti-
mizations and design decisions we took. We close this section with
a security evaluation of the DTA and CFI tools we built over Shad-
owReplica.
The results presented throughout this section are mean values,
calculated after running 5 repetitions of each experiment. Our testbed
consisted of an 8-core host, equipped with two 2.66 GHz quad-
core Intel Xeon X5500 CPUs and 48GB of RAM, running Linux
(Ubuntu 12.04LTS with kernel v3.2). Note that ShadowReplica
can run on commodity multi-core CPUs, such as the ones found on
portable PCs. We used a relatively strong server for the evaluation
because it was available at the time. The only crucial resource is
the L3 cache, 32MB in our case, as the communication overhead
3Note that we have ﬁled a bug for this issue [17], but it has not been
completely resolved yet.
242Category
Application
# BBLs
# Ins. Unopt
Optimization
Intra DFT Inter Exec
Utilities
bzip2
tar
SPEC2006
473.astar
403.gcc
401.bzip2
445.gobmk
464.h264
456.hmmer
462.libquantum
429.mcf
471.omnetpp
400.perl
458.sjeng
483.xalanc
Server
Apache
application MySQL
Average
6175
8615
5069
78009
4588
25939
11303
7212
3522
3752
15209
27391
5535
34881
23079
40393
18792
8.59
5.20
7.38
4.37
7.85
5.97
5.76
19.14
9.16
5.96
5.36
5.71
5.29
4.79
8.39
6.87
7.24
4.97
2.31
4.09
2.54
4.12
3.04
4.88
12.62
4.66
3.21
2.88
2.86
2.49
2.02
3.17
4.21
4.17
2.93
1.82
2.48
1.93
2.75
2.11
3.12
6.99
2.61
2.12
2.11
1.89