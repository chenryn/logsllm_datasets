dataset, as in the mock attacker model, yields better EER. On
the other hand, it is also likely to lower the AR of the system,
due to increased variance in the negative training dataset. Thus,
the use of this model does not inﬂate our results.
Remark 4.3: We have used balanced datasets in our ex-
periments, i.e., the number of positive and negative samples
being the same. It is true that a balanced dataset is not ideal
for minimizing AR; more negative samples may reduce the
acceptance region. However, an unbalanced dataset, e.g., more
negative samples than positive samples, may be biased towards
the negative class, resulting in misleadingly high accuracy [44],
7
[45]. A balanced dataset yields the best EER without being
biased towards the positive or negative class.
C. Machine Learning Classiﬁers
Our initial hypothesis (Section III) stipulates that AR is
related to the training data distribution, and not necessarily
to any weakness of the classiﬁers learning from the data. To
demonstrate this distinction, we elected four different machine
learning algorithms: Support Vector Machines (SVM) with a
linear kernel (LinSVM), SVM with a radial basis function
kernel (RBFSVM), Random Forests (RNDF) and Deep Neural
Networks (DNN). Brieﬂy, SVM uses the training data to
construct a decision boundary that maximizes the distance
between the closest points of different classes (known as
support vectors). The shape of this boundary is dictated by the
kernel used; we test both a linear and a radial kernel. Random
Forests is an aggregation of multiple decision tree learners
formally known as an ensemble method. Multiple learners
in the aggregation are created through bagging, whereby the
training dataset is split into multiple subsets, each subset train-
ing a distinct decision tree. The decisions from the multiple
models are then aggregated to produce the random forest’s
ﬁnal decision. DNNs are a class of machine learning models
that contain hidden layers between an input and an output
layer; each layer containing neurons that activate as a function
of previous layers. Speciﬁcally we implement a convolutional
neural network with hidden layers leading to a ﬁnal layer of
our two classes, accept and reject. All four of these machine
learning models are trained as supervised learners. As such, we
provide the ground truth labels to the model during training.
The linear SVM was trained with C = 104, and default
values included within Scikit-learn’s Python library for the
remaining parameters [46]. For radial SVM we also used C =
104 while keeping the remaining parameters as default. The
Random Forests classiﬁer was conﬁgured with 100 estimators.
DNNs were trained with TensorFlow Estimators [47] with a
varying number of internal layers depending on the dataset.
The exact conﬁgurations are noted in Appendix B.
Remark 4.4: We reiterate that our
trained models are
reconstructions of past works. However, we endeavor that
our models recreate error rates similar to the originally re-
ported values on the same dataset. On Mahbub et al.’s touch
dataset [6], the authors achieved 0.22 EER with a RNDF
classiﬁer, by averaging 16 swipes for a single authentication
session. We are able to achieve a comparable EER of 0.21
on RNDF without averaging. For face authentication, we
evaluate a subset of CASIA-Webface, consequently there is no
direct comparison. The original Facenet accuracy in verifying
pairs of LFW [40] faces is 98.87% [3], but our adoption of
model-based authentication is closer to [48], unfortunately the
authors have ﬁxed a threshold for 0 FPR without reporting
their TPR. Nagrani, Chung and Zisserman’s voice authenti-
cator [4] reports an EER of 0.078 on a neural network. Our
classiﬁers achieve EERs of 0.03, 0.02, 0.04 and 0.12, which
are within range of this benchmark. Our gait authenticator
is the exception, it has not been evaluated for authentication
with it’s mixture of activity types. However, a review of gait
authentication schemes can be found at [49].
two classiﬁers, AR is lower than EER. However, by looking
at the AR curve for RNDF, we see that the AR curve is
well above the FPR curve when FRR ≤ 0.3. This can be
specially problematic if the threshold is set so as to minimize
false rejection at the expense of false positives. We also note
that the AR curve for DNN closely follows the FPR curve,
which may suggest that the AR is not as problematic for this
classiﬁer. However, by looking at Figure 5a, we see that this is
misleading since for some users the AR is signiﬁcantly higher
than FPR, making them vulnerable to random input attacks.
Also, note that the AR generally decreases as the threshold is
changed at the expense of FRR. However, except for RNDF,
the AR for the other three classiﬁers is signiﬁcantly higher
than zero even for FRR values close to 1.
2) Touch (Swipe) Authentication: The touch authenticator
has the highest EER of all four biometric modalities. Very few
users attained an EER lower than 0.2 as seen in Figure 5b.
This is mainly because we consider the setting where the
classiﬁcation decision is being made after each input sample.
Previous work has shown EER to improve if the decision is
made on an average vector of a few samples some work [28],
[22], [8]. Nevertheless, since our focus is on AR, we stick
to the per-sample decision setting. Figure 5b shows that more
than half of the users have ARs larger than FPR, and in some
cases where the FPR is fairly low (say 0.2), the AR is higher
than 0.5. Unlike gait authentication where RNDF classiﬁer
had ARs less than FPR for the majority of the users, all four
algorithms for touch authentication display high vulnerability
to the AR based random input attack. When viewing average
results in Figure 6b, we observe the average AR curve to be
very ‘ﬂat’ for both SVM classiﬁers and DNN. This indicates
that AR for these classiﬁer remains mostly unchanged even
if the threshold is moved closer to the extremes. RNDF once
again is the exception, with the AR curve approaching 0 as
the threshold is increased.
3) Face Authentication: Figure 5c shows that AR is either
lower or comparable to FPR for RBFSVM and DNN. Thus,
the FPR serves as a good measure of AR in these systems.
However, AR for most users is signiﬁcantly higher than FPR
for LinSVM and RNDF. This is true even though the EER
of these systems is comparable to the other two as seen in
Figure 6c. For LINSVM, we have an average AR of 0.15
compared to an EER of 0.05. For RNDF, the situation is worse
with the AR reaching 0.78 against an EER of 0.03. We also
note that while the AR is equal to FPR for DNN, its value of
0.10 is still worrisome to be resistant to random input attack.
The relatively high FPR for DNN as compared to RBFSVM is
likely due to a limited set of training data available in training
the neural network.
4) Voice Authentication: Figure 5d shows that once again
LinSVM and RNDF have a signiﬁcant proportion of users with
AR higher than FPR, whereas for both RBFSVM and DNN
the AR of users is comparable to FPR. Looking at the average
ARs in Figure 6d, we see that interestingly RNDF exhibits an
average AR of 0.01 well below the ERR of 0.04. The average
suppresses the fact that there is one user in the system with an
AR close to 1.0 even with an EER of approximately 0.1, and
two other users with an AR of 0.5 and 0.3 for which the EER
is signiﬁcantly below 0.1. Thus these speciﬁc users are more
susceptible to the random input attack. Only LinSVM has an
(a) Gait
(b) Touch
(c) Face
(d) Voice
Fig. 5.
Individual user scatter of AR and FPR. In a majority of conﬁgurations,
there is no clear relationship between AR and FPR, with the exception of the
RBFSVM and DNN classiﬁers for face and voice authentication.
D. Acceptance Region: Feature Vector API
In this section, we evaluate the acceptance region (AR)
by comparing it against FPR for all 16 authentication conﬁg-
urations (four datasets and four classiﬁers). In particular, we
display ROC curves showing the trade-off between FPR and
FRR against the acceptance region (AR) curve as the model
thresholds are varied. These results are averaged over all users.
While this gives an average picture of the disparity between
AR and FPR, it does not highlight that for some users AR
may be substantially higher than FPR, and vice versa. In such
a case, the average AR might be misleading. Thus, we also
show scattered plots showing per-user AR and FPR, where
the FPR is evaluated at EER. The per-user results have been
averaged over 50 repetitions to remove any bias resulting from
the sampled/generated vectors. The individual user AR versus
FPR scatter plots are shown in Figure 5, and the (average) AR
curves against the ROC curves are shown in Figure 6.
Remark 4.5: EER is computed in a best effort manner,
with only 100 discretized threshold values, to mitigate the stor-
age demands of the 1M uniformly random vectors measuring
AR. Unfortunately, there are some instances whereby the FRR
and FPR do not match exactly, as the threshold step induces
a large change in both FRR and FPR. Only 1/16 classiﬁers
exhibit an FPR-FRR discrepancy greater than 1%.
1) Gait Authentication: Figure 5a shows AR against FPR
of every user in the activity type (gait) dataset. Recall that in
this ﬁgure FPR is evaluated at EER. The dotted straight line is
the line where AR equals FPR (or ERR). We note that there is
a signiﬁcant proportion of users for which AR is greater than
FPR, even when the latter is reasonably low. For instance, in
some cases AR is close to 1.0 when the FPR is around 0.2.
Thus, a random input attack on systems trained for these target
users will be successful at a rate signiﬁcantly higher than what
is suggested by FPR. We also note that the two SVM classiﬁers
have higher instances of users for whom AR surpasses FPR.
Figure 6a shows the AR curve averaged across all users against
the FPR and FRR curves for all four classiﬁers. We can see
that AR is higher than the ERR (represented by the dotted
vertical line) for the two SVM classiﬁers. For the remaining
8
0.00.20.4EER0.000.250.500.751.00AR0.00.20.4EER0.000.250.500.751.00AR0.00.20.4EER0.000.250.500.751.00AR0.00.20.4EER0.000.250.500.751.00ARLINSVMRBFSVMRNDFDNN(a) Gait Average ROC
(b) Touch Average ROC
(c) Face Average ROC
Fig. 6. ROC curve versus the AR and RAR curves for all conﬁgurations. The EER is shown as a dotted vertical blue line. The FRR, FPR, AR and RAR
values shown in the legend are evaluated at EER (FPR = FRR). The RAR is only evaluated on the Touch and Face datasets.
(d) Voice Average ROC
9
0.00.20.40.60.81.0LINSVM threshold0.00.20.40.60.81.0Error0.160.24FRR - 0.16FPR - 0.16  AR - 0.240.00.20.40.60.81.0RBFSVM threshold0.140.18FRR - 0.14FPR - 0.14  AR - 0.180.00.20.40.60.81.0RNDF threshold0.090.03FRR - 0.09FPR - 0.09  AR - 0.030.00.20.40.60.81.0TFDNN threshold0.240.20FRR - 0.24FPR - 0.19  AR - 0.200.00.20.40.60.81.0LINSVM threshold0.00.20.40.60.81.0Error0.330.490.45FRR - 0.33FPR - 0.32  AR - 0.49RAR - 0.450.00.20.40.60.81.0RBFSVM threshold0.260.410.40FRR - 0.26FPR - 0.27  AR - 0.41RAR - 0.400.00.20.40.60.81.0RNDF threshold0.210.230.18FRR - 0.21FPR - 0.21  AR - 0.23RAR - 0.180.00.20.40.60.81.0TFDNN threshold0.330.300.32FRR - 0.33FPR - 0.32  AR - 0.30RAR - 0.320.00.20.40.60.81.0LINSVM threshold0.00.20.40.60.81.0Error0.050.150.12FRR - 0.05FPR - 0.05  AR - 0.15RAR - 0.120.00.20.40.60.81.0RBFSVM threshold0.040.010.09FRR - 0.04FPR - 0.04  AR - 0.01RAR - 0.090.00.20.40.60.81.0RNDF threshold0.030.780.02FRR - 0.03FPR - 0.03  AR - 0.78RAR - 0.020.00.20.40.60.81.0TFDNN threshold0.090.100.10FRR - 0.09FPR - 0.10  AR - 0.10RAR - 0.100.00.20.40.60.81.0LINSVM threshold0.00.20.40.60.81.0Error0.030.08FRR - 0.03FPR - 0.03  AR - 0.080.00.20.40.60.81.0RBFSVM threshold0.020.00FRR - 0.02FPR - 0.02  AR - 0.000.00.20.40.60.81.0RNDF threshold0.040.01FRR - 0.04FPR - 0.04  AR - 0.010.00.20.40.60.81.0TFDNN threshold0.120.08FRR - 0.12FPR - 0.11  AR - 0.08average AR (0.08) higher than EER (0.03). The average AR
of DNN is lower than EER (0.11), but it is still signiﬁcantly
high (0.08). For RBFSVM we have an average AR close to 0.
Observations
In almost every conﬁguration, we can observe that the
average AR is either higher than the FPR or at best comparable
to it. Furthermore, for some users the AR is higher than
FPR even though the average over all users may not reﬂect
this trend. This demonstrates that an attacker with no prior
knowledge of the system can launch an attack against it via
the feature vector API. Moreover, for both the linear and radial
SVM kernels, and some instances of the DNN classiﬁer, we
observe a relatively ﬂat AR curve as the threshold is varied,
unlike the gradual convergence to 1 experienced by the FPR
and FRR curves. These classiﬁers thus have a substantial
acceptance region that accept samples as positives irrespective
of the threshold. Random Forests is the only classiﬁer where
the AR curve shows signiﬁcant drop as the threshold is varied.
Random forests sub-divide the training dataset in a process
called bagging, where each sub-division is used to train one
tree within the forest. With different subsets of data, different
training data points will be closer to different empty regions
in feature space, thus producing varied predictions. Because
the prediction conﬁdence of RNDF is computed from the
proportion of trees agreeing with a prediction, the lack of
consensus within the ensemble of trees for the empty space
may be the reason for the non-ﬂat AR curve.
E. Acceptance Rate: Raw Input API
The results from the feature vector API are not necessarily
reﬂective of the success rate of a random input attack via the
raw input API. One reason for this is that the feature vectors
extracted from raw inputs may or may not span the entire
feature space, and as a consequence the entire acceptance
region. For this reason, we use the term raw acceptance rate
(RAR) to evaluate the probability of successfully ﬁnding an
accepting sample via raw random inputs. To evaluate RAR,
we select the touch and face biometric datasets. The raw input
of the touch authenticator is a time-series, whereas for the face
authentication system it is an image.
1) Raw Touch Inputs: We used a continuous auto-
regressive process (CAR) [50] to generate random timeseries.
We opted for CAR due to the extremely high likelihood of
time-series values having a dependence on previous values.
This time-series was then min-max scaled to approximate
sensor bounds. For example the x-position has a maximum
and minimum value of 1980 and 0 respectively, as dictated
by the number of pixels on a smartphone screen. Both the
duration and length of the time-series were randomly sampled
from reasonable bounds: 0.5 to 2.0 seconds and 30 to 200
data-points, respectively. The time-series was subsequently
parsed by the same feature extraction process as a legitimate
time-series, and the outputs scaled on a feature min-max
scale previously ﬁt on real user data. In total we generate
100,000 time-series, which are used to measure RAR over 50
repetitions of the experiment.
The results of our experiments are shown in Figure 6b,
with the curve labeled RAR showing the raw acceptance rate
as the threshold of each of the classiﬁers is changed. As we
can see, the RAR is large and comparable to AR. This seems
to indicate that the region spanned by random inputs covers
the acceptance region. However, on closer examination, this
happens to be false. The average volume covered by the true
positive region for the touch dataset (cf. Section III) is less
than 1.289 × 10−4 ± 5.462 × 10−4, yet the volume occupied
by the feature vectors extracted from raw inputs is less than