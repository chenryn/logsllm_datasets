rently used for block and network devices.
For block devices, system calls are issued by domU ap-
plications, which are translated into block-level operation
requests by the kernel. Traditionally, the backend driver in
dom0’s kernel receives the request from the frontend, and
sends them directly to the disk. Xen 3.0.3 introduced a
new architecture for block device I/O, illustrated in Fig-
ure 1, with some interesting new properties. A block tap
was introduced, allowing disk drivers to be implemented as
userspace applications. These userspace drivers tap into the
backend driver (blktap) and can directly manage disk activ-
ity with relatively small performance costs [32]. This archi-
tectural change added substantial ﬂexibility to disk driver
development, greatly simplifying it and at the same time
allowing more powerful functionalities to be implemented.
The fact that tapdisk drivers are regular userspace applica-
tions allows virtual disk I/O to be implemented with simple
ﬁle-manipulation system calls and/or library calls.
4 XenAccess Monitoring Library
4.1 Architecture
The primary goal for the XenAccess architecture is to
satisfy the six requirements stated in the Introduction. We
chose Xen as a virtualization solution because it is a Type
I VMM; it runs directly on the hardware, allowing for a
solid foundation to the TCB. It also already includes an in-
frastructure suitable to satisfy our monitoring needs, so that
changes to the VMM are unnecessary (property (1)). Like-
wise, by building on top of Xen’s infrastructure, we were
able to design the monitoring architecture to work without
changes to the target OS, allowing us to satisfy property (2).
To prevent the target OS from tampering with the monitors
and satisfy property (6), we place the monitors in a differ-
ent VM than the target OS. Xen provides sufﬁcient isola-
tion between VMs for this to be a viable solution. And, for
Figure 1: Blktap disk I/O architecture.
ular guest OSes are kept in unprivileged domains (domU),
whereas a single administrative domain exists as Domain
0 (dom0). Dom0 can be seen as a domain-level extension
of Xen in which all of the management functionalities are
located.
It has complete access rights to all virtual ma-
chines being run and also works as a device driver proxy
for domU’s virtual devices.
The VMM itself is a simple and thin software layer
whose main job is to guarantee proper isolation between
virtual machines, performing minimal resource manage-
ment. This isolation is quite robust, since Xen relies di-
rectly on hardware-level protection mechanisms and has a
much narrower interface than a standard operation system
(e.g., Linux).
3.2 Memory Management
One of the key tasks for a VMM is to partition the sys-
tem memory between each VM. Xen achieves this using
three levels of memory: machine, physical, and virtual ad-
dresses. The machine addresses are the actual addresses
used by the hardware and managed by the VMM. The phys-
ical addresses are what each paravirtualized OS uses. This
ﬁrst abstraction allows the VMM to assign non-contiguous
memory regions to a paravirtualized VM. As far as the VM
is concerned, the physical addresses are the addresses used
by the hardware. In reality, these addresses are translated
by a lookup table in the VMM into machine addresses. The
third type of address is a virtual address, which is used the
same way in the paravirtualized OS as traditionally used in
OSes.
Both machine and physical addresses are often referred
to in terms of a machine frame number (MFN) and a phys-
ical frame number (PFN). These numbers refer to a single
page of memory, which are 4k bytes each2. A complete ad-
dress is given as both an MFN or PFN and an offset into
2Here we refer to the x86 architecture where memory pages are usually
4k, but can also be 4M.
388388
needed to convert a kernel symbol or virtual address into a
memory mapped page are discussed in Section 4.2.1.
Whereas virtual memory introspection monitors the cur-
rent state of memory pages, the virtual disk monitoring cap-
tures data traveling to and from the disk. This data is cap-
tured by placing code that directly intercepts the data path
between the target OS and the hard drive it uses for data
storage. We chose the Blktap Architecture for this capabil-
ity due to its good performance [32] and its ability to de-
liver low-level information to user-space software such as
the XenAccess Library.
The XenAccess architecture utilizes functionality in-
cluded with Xen in order to reduce the implementation over-
head and adhere to property (1). At this point it is important
to emphasize that while we acknowledge that XenAccess’
functionality and its adherence to the principles established
in the Introduction are signiﬁcantly based on the infrastruc-
ture already provided by Xen, the XenAccess architecture
and the principles supporting it could be implemented on
other VMMs as well. The core functionality needed in
the VMM includes mapping memory between virtual ma-
chines, viewing VM-speciﬁc metadata (e.g., running kernel
version), and tapping into the data between a device (e.g.,
the hard disk drive) and the associated device driver. This
functionality could be added to any modern virtualization
environment, if it is not already there, allowing for sup-
port of the XenAccess monitoring architecture. The only
caution that must be taken is to implement at the VMM
level only that which is strictly necessary, and do it very
carefully, so as to minimize the probability of introducing
bugs in the TCB. All the remaining functionality should be
implemented in a special security or management domain
(such as Xen’s dom0) taking the appropriate performance
considerations.
4.2
Implementation
XenAccess is implemented in C as a shared library with
1935 source lines of code (SLOC). XenAccess makes use
of libxc, libxenstore, and the Blktap Architecture.
The current version is built to monitor a paravirtualized ver-
sion of Linux 2.6.16 running on Xen 3.0.4 1, however the
techniques here can be extended to work with other OSes.
The two primary monitoring functionalities in XenAccess
are virtual memory introspection and virtual disk monitor-
ing. Implementation details for each of these techniques are
discussed in the sections below.
4.2.1 Virtual Memory Introspection
XenAccess uses the xc map foreign range() func-
tion, provided through the XenControl Library (libxc),
to view the memory of another VM. Using this function
Figure 2: The XenAccess architecture leverages existing ca-
pabilities in Xen to reduce complexity and improve overall
performance.
environments that require more explicit isolation, manda-
tory access control is built into the Xen VMM [27]. Fi-
nally, we desire an architecture that can monitor any data
on the target OS in order to satisfy property (5). XenAccess
currently provides monitoring capabilities for both memory
and disk I/O. However, the architecture can easily be ex-
tended to monitor additional information such as network
trafﬁc, CPU context, and static disk contents. We examine
XenAccess’ adherence to properties (3) and (4) in Section
5.
Figure 2 shows the overall software architecture from
two perspectives. On the right, we show the location of the
critical components in relation to the VMM and VMs. Cur-
rently, XenAccess runs in Domain 0 as this simpliﬁes ac-
cess to the Blktap Architecture and the XenControl Library.
XenAccess could also run in a user domain after sufﬁcient
privileges are given for that domain to perform the monitor-
ing. The left side of Figure 2 shows how XenAccess ﬁts into
the Xen software stack. Here we emphasize that XenAccess
is a library intended for use by monitor applications. Some
simple example applications are discussed in Section 5.2.
Virtual memory introspection requires accessing the
memory of one VM from another. Xen provides a func-
tion in the XenControl Library that is used for this pur-
pose and this functionality could be added to other VMMs
using a small amount of additional code.
In Xen, the
function xc map foreign range(), maps the memory
from one VM into another. After the memory is mapped, it
can be treated as local memory, providing for fast monitor-
ing capabilities. In order to convert a XenAccess API call
into a call to xc map foreign range(), XenAccess
must perform several memory address translations. This
requires additional information about the target OS which
can be obtained from the XenStore, a database of informa-
tion about each VM, and interpreted using some knowledge
of the target operating system’s implementation. The steps
389389
eliminates the need to modify the VMM or the target OS,
satisfying properties (1) and (2). This function can be used
to map a memory page from the target OS using its MFN.
XenAccess uses this function for raw memory access and
then builds up from there using address translation tables in
the VMM and the target OS. For example, to convert a PFN
to a MFN, XenAccess uses lookup tables that are provided
by Xen. Similarly, to convert a virtual address to a MFN,
XenAccess uses the PTs.
Memory Introspection API
• xa init():
Initializes access to a speciﬁc domU
given a domain ID. This function takes a domain ID
and returns a structure that holds cached informa-
tion related to accessing that domain. All calls to
xa init() must eventually call xa destroy().
• xa destroy(): Destroys an instance by freeing
memory and closing any open handles.
• xa access kernel symbol(): Memory maps
one page from domU to a local address range. The
memory to be mapped is speciﬁed with a kernel sym-
bol (e.g., from System.map). This memory must be
unmapped manually with munmap().
• xa access virtual address():
Memory
maps one page from domU to a local address range.
The memory to be mapped is speciﬁed with a kernel
virtual address. This memory must be unmapped
manually with munmap().
• xa access user virtual address(): Mem-
ory maps one page from domU to a local address
range. The memory to be mapped is speciﬁed with
a virtual address inside a process address range. This
function also requires a process ID. This memory must
be unmapped manually with munmap().
We provide an overview of the implementation of each of
these functions.
to xa init().
All users of the introspection library must begin with
This function initializes the
a call
xa instance struct which holds information that is
used throughout the introspection process. Any work that
can be done “up front” and cached is held in this structure.
This includes locating the address of the kernel page direc-
tory, initializing a handle to libxc, initializing a pointer
to a PFN to MFN lookup table, determining if the domain
is paravirtualized or fully virtualized, and more. Once a
user is done with the library, a call should be made to
xa destroy() to free any memory associated with the
xa instance struct.
After
initializing
one
can use
any of
the
the
xa instance struct,
three
functions
access
Figure 3: The steps needed to map a kernel memory page
based on a kernel symbol using virtual memory introspec-
tion.
a
the
above.
simplest,
Starting with
the
listed
xa access virtual address() takes
kernel
virtual address and returns a pointer to the memory page
holding that address along with the offset to the speciﬁed
address within the memory page. This address translation
requires a PT lookup, which requires XenAccess to load
three memory pages. First, the page directory is loaded
to ﬁnd the location of the PT. Next, the PT is loaded to
ﬁnd the location of the address. Finally, the memory page
holding the address is loaded and this page, along with
an offset to the address, is returned to the user. Returning
a shared memory page contributes to the good inter-VM
memory copy performance shown in Section 5.1, which is
a requirement for property (3).
If
The
from domU.
xa access kernel symbol()
function,
shown in Figure 3, requires one extra step beyond the
virtual address translation described above. This step is
to convert a kernel symbol to a virtual address. XenAc-
cess performs this conversion using the System.map
ﬁle associated with the kernel
this
ﬁle is not available,
then this function will fail. The
System.map ﬁle is essentially a large table of symbols
and addresses. XenAccess scans this ﬁle until it ﬁnds
the symbol provided.
It then proceeds with a virtual
address access using the address associated with the kernel
symbol. Since this operation requires performing a lookup
from a ﬁle on disk,
it is considerably slower than the
xa access virtual address() function, but
the
results are cached so the average case is fast as discussed in
Section 5.1. Further performance improvements could be
achieved by memory mapping the ﬁle, moving the costly
ﬁle read operations into the xa init function. However,
most monitoring applications will repeatedly view the same
memory location, using the cached information.
390390
The ﬁnal function in the virtual memory introspection
API is xa access user virtual address(). This
function provides access to user space memory. Page table
lookups for a virtual address in user space are essentially
the same as kernel space. The main difference is that we
must lookup the location of the page directory associated
with the process. Recall that for kernel space, the location
of the page directory is cached during library initialization,
but the page directory locations for each process can change
as processes come and go. To lookup the page directory for
a process, XenAccess scans the kernel task list looking for
a process with the given process ID. Upon ﬁnding a match,
the page directory can be obtained from the task struct
in kernel memory. Using this page directory, the remainder
of the virtual address translation is the same as previously
described for the kernel.
HVM Support XenAccess has preliminary support to
perform memory introspection on fully virtualized (HVM)
VMs. In HVM VMs, physical addresses and machine ad-
dresses are the same. Therefore, XenAccess will automati-
cally detect HVM domains and not attempt to perform this
translation in those cases. In practice, the P2M translation
is a simple table lookup, so omitting this step does not mea-
surably improve performance. Since memory introspection
support for HVM VMs is in its early stages, there is some
reduced functionality. This reduced functionality is the rea-
son why there is no HVM performance data available for
the user address function in Figure 5.
Improving Performance Since XenAccess must use
memory from the target OS and the VMM to perform ad-
dress translations, these operations can be costly. Therefore,
XenAccess uses a least recently used (LRU) cache to store
the results of the address translations. This is similar to a
translation lookaside buffer (TLB). However, in the case of
XenAccess, we also cache kernel symbol names since disk
access is always a slow operation. This caching is critical to
achieving acceptable performance and satisfying property
(3), as discussed in Section 5.1.