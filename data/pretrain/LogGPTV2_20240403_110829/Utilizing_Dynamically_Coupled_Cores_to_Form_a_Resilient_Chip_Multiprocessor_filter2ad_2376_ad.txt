### Table 4: Simulated Parallel Applications and Input Sizes

| Application | Description | Problem Size |
|-------------|-------------|--------------|
| FMM | Evolution of galaxies | 16k particles |
| RAYTRACE | 3D ray tracing | 16k particles |
| Spec OpenMP | N-body problem | Car (unspecified) |
| SWIM-OMP | Shallow water model | 16k particles |
| EQUAKE-OMP | Earthquake model | 16k particles |
| Data Mining | MinneSpec-Large | 2,048 records, 100 epochs |
| BSOM | Self-organizing map | 12.3k sequences |
| BLAST | Protein matching | 18k points, 18 attributes |
| KMEANS | K-means clustering | 18k points, 18 attributes |
| SCALPARC | Decision Tree | 125k points, 32 attributes |

### Evaluation of Sequential and Parallel Applications

To evaluate sequential applications, we simulated 19 out of the 26 SPEC2000 benchmarks [8] using the SESC simulator [18]. We used the largest datasets from the MinneSPEC [10] reduced input set and ran them to completion. For parallel applications, we utilized a set of scalable scientific and data mining applications, as detailed in Table 4. These parallel benchmarks were simulated for 1, 2, 4, and 8 threads on 2, 4, 8, and 16 processors, respectively.

### 5.1 Results

#### 5.1.1 DCC Overhead

In this section, we assess the performance overhead of DCC over a baseline CMP with no fault tolerance. We focus on the additional overheads involved in orchestrating the detection and recovery schemes, rather than the inherent overhead of redundant execution, which is obvious.

**Sequential Applications**

We evaluated the performance overhead during fault-free execution by simulating the SPEC2000 benchmarks on a single core in our baseline CMP. We compared this to running the benchmarks redundantly on two cores with checkpoint intervals of 1,000, 5,000, and 10,000 cycles. The slowdown relative to single-core execution without fault tolerance is shown in Figure 9. The average overheads for intervals of 1,000, 5,000, and 10,000 cycles are 20%, 5%, and 3%, respectively. In DCC, a checkpoint takes approximately 100-200 cycles to complete, primarily due to synchronizing both cores, compressing the register file state, and communicating results over the system bus. A checkpoint interval of 1,000 cycles is insufficient to amortize the cost associated with taking a checkpoint, but an interval of 10,000 cycles reduces this overhead to 3%. Long checkpoints must be considered in I/O-intensive workloads, where prior work [24] suggests that checkpoints should be taken at least every 50,000 instructions to achieve high performance. Our applications achieve an IPC of about 1 on the baseline system, so approximately 10,000 instructions execute in a given checkpoint interval, which is well below the 50,000 instruction limit.

**Parallel Applications**

Parallel applications incur additional performance overheads due to the consistent management of shared variables across nodes, as discussed in Section 4. To assess these overheads, we compared speedups under DCC to our baseline CMP for 1, 2, 4, and 8 threads (2, 4, 8, and 16 processors). Table 5 reports the speedup across our nine parallel benchmarks, normalized to the performance of a single thread of execution on the baseline CMP. Additionally, Figure 10 shows speedups for benchmarks with the largest (barnes) and smallest (kmeans) overheads. On average, when using a 64-entry age table, the performance overhead for up to 8 threads is between 4% and 5%. A sensitivity study on the number of age table entries showed less than a 4% reduction in execution time overhead when a 1024-entry age table was used (Section 4). Overall, these results suggest that modest hardware additions are adequate to minimize the performance overhead of DCC on parallel applications.

### 5.1.2 Comparison Against Relaxed Input Replication

DCC uses long checkpoint intervals to amortize the cost of dynamic coupling. To maintain input coherence between redundant threads, we introduce an age table to track open read windows. Reunion [23] proposes a conceptually simpler scheme of relaxed input replication, where input incoherence may occur but would be detected as a fault. To guarantee forward progress, Reunion single-steps the cores to the first load instruction and still relies on dedicated communication channels for output comparison, thus lacking the flexibility of dynamic coupling. 

To assess the performance of relaxed input replication under DCC's larger checkpoint intervals, we compared it to DCC’s age table scheme across our parallel benchmarks (Figure 11). For applications with little read-write sharing, such as BLAST and SWIM, relaxed input replication incurs relatively modest overhead. However, for applications with more read-write sharing, such as RAYTRACE and SCALPARC, the execution times are significantly higher (more than two-fold for 10,000-cycle intervals).

Relaxed input replication performs poorly in this context for two main reasons. First, as the checkpoint interval increases, the redundant pair of cores becomes progressively out of sync. Second, the lack of an age table mechanism leads to frequent resynchronization, increasing the overall overhead.

![Figure 9: Execution Time Overhead](figure9.png)

![Figure 10: Speedup on Baseline and Fault Tolerant CMPs](figure10.png)

![Figure 11: Slowdown of Reunion’s Relaxed Input Scheme Compared to DCC’s Age Table Scheme](figure11.png)