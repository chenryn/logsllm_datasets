FMM
RAYTRACE
Spec OpenMP
SWIM-OMP
EQUAKE-OMP
Data Mining
BSOM
BLAST
KMEANS
SCALPARC
Description
Evolution of galaxies
N-body problem
3D ray tracing
Problem size
16k part.
16k part.
car
Shallow water model
Earthquake model
MinneSpec-Large
MinneSpec-Large
Self-organizing map
Protein matching
K-means clustering
Decision Tree
2,048 rec., 100 epochs
12.3k sequ.
18k pts., 18 attr.
125k pts., 32 attr.
Table 4. Simulated parallel applications and input sizes.
To evaluate sequential applications, we simulate 19 of the
26 SPEC2000 suite of benchmarks [8] using the SESC simula-
tor [18].4 We use the largest datasets from the MinneSPEC [10]
reduced input set and run them to completion. To evaluate parallel
applications, we use a set of scalable scientiﬁc and data mining
applications, shown in Table 4. These parallel benchmarks are
simulated for 1, 2, 4, and 8 threads, on 2, 4, 8, and 16 processors
respectively.
5.1 Results
5.1.1 DCC Overhead
In this section, we assess the performance overhead of DCC over
a baseline CMP with no fault-tolerance. We are not concerned
with the inherent overhead of executing a thread redundantly on
two processors. It is obvious that redundant execution occupies
twice the number of cores, and therefore cuts the effective number
of processing elements in half. We are interested in the additional
overheads involved with orchestrating the detection and recovery
schemes presented in this work.
Sequential Applications
We evaluate the performance overhead during fault-free execution
by simulating the SPEC2000 benchmarks on a single core in our
baseline CMP. We compare this to running the benchmarks re-
dundantly on two cores with checkpoint intervals of 1,000, 5,000,
and 10,000 cycles. The slowdown with respect to a single-core
execution with no fault tolerance is shown in Figure 9. The aver-
age overheads for intervals of 1,000, 5,000, and 10,000 cycles are
20%, 5%, and 3%, respectively. In DCC, a checkpoint takes on
the order of 100-200 cycles to complete. Most of this time is spent
synchronizing both cores, compressing the register ﬁle state and
communicating results over the system bus. A checkpoint interval
of 1,000 cycles is insufﬁcient to amortize the cost associated with
taking a checkpoint. However, an interval of 10,000 cycles re-
duces this overhead to 3%. One issue that needs to be considered
with long checkpoints is their interaction with I/O requests. Prior
work [24] has found that checkpoints should be taken at least ev-
ery 50,000 instructions in I/O intensive workloads to achieve high
performance. The applications we have studied obtain an IPC of
about 1 on the baseline system, so approximately 10,000 instruc-
tions execute in given checkpoint interval, which is well below this
50,000 instruction limit.
Parallel Applications
Parallel applications incur additional performance overheads due
to the management of shared variables consistently across nodes
(as discussed in Section 4). To see how these overheads scale,
we compare speedups under DCC to our baseline CMP for 1, 2,
4, and 8 threads (2, 4, 8, and 16 processors). Table 5 reports the
4our simulation infrastructure currently does not support the other
SPEC benchmarks
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:49:13 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 20071,000c
5,000c
10,000c
)
%
(
d
a
e
h
r
e
v
O
e
m
T
n
o
i
t
u
c
e
x
E
i
 25
 20
 15
 10
 5
 0
p
m
m
a
l
u
p
p
a
t
r
a
i
s
p
a
2
p
z
b
i
y
t
f
a
r
c
e
k
a
u
q
e
p
a
g
c
c
g
i
p
z
g
f
c
m
a
s
e
m
d
i
r
g
m
r
e
s
r
a
p
i
m
w
s
f
l
o
w
t
x
e
t
r
o
v
r
p
v
G
V
A
i
e
s
w
p
u
w
Figure 9. Execution time overhead.
speedup across our nine parallel benchmarks. Speedups are nor-
malized to the performance of a single thread of execution on the
baseline CMP. In addition, Figure 10 reports speedups for bench-
marks with the largest overheads (barnes, and smallest overhead,
kmeans). On average, when using a 64-entry age table, the per-
formance overhead for up to 8 threads is between 4% and 5%. A
sensitivity study on the number of age table entries shows less than
a 4% reduction in execution time overhead when a 1024-entry age
table is used (Section 4). Overall, these results suggest that mod-
est hardware additions are adequate to minimize the performance
overhead of DCC on parallel applications.
p
u
d
e
e
p
S
 8
 7
 6
 5
 4
 3
 2
 1
 0
kmeans
ft-kmeans
barnes
ft-barnes
1
2
4
Threads
8
Figure 10. Speedup on baseline and fault tolerant CMPs
(marked ft) for parallel benchmark with largest and smallest
overheads. All curves are normalized to a sequential run on
the baseline CMP.
5.1.2 Comparison Against Relaxed Input Replication
DCC utilizes long checkpoint intervals to amortize the cost of dy-
namic coupling. To maintain input coherence between redundant
threads, we introduce an age table to track open read windows. Re-
union [23] arguably proposes a conceptually simpler scheme of re-
laxed input replication: input incoherence may occur, but it would
be detected as a fault. To guarantee forward progress, Reunion
single-steps the cores to the ﬁrst load instruction. Reunion still
relies on dedicated communication channels for output compari-
son, and thus cannot provide the ﬂexibility of dynamic coupling.
Nevertheless, we would like to asses the performance of relaxed
input replication under DCC’s larger checkpoint intervals, and to
quantitatively establish whether we need our age table mechanism.
Figure 11 shows the slowdown of Reunion’s relaxed input
scheme compared to DCC’s age table scheme across our parallel
benchmarks. For applications that have little read-write sharing,
such as blast and swim, relaxed input replication incurs relatively
modest overhead. However, applications that have more read-
write sharing, raytrace and scalparc, incur signiﬁcantly higher ex-
ecution times (more than two-fold for 10,000-cycle intervals).
Relaxed input replication performs poorly in this context for
two main reasons. First, as the checkpoint interval increases the
redundant pair of cores becomes progressively out of synch. We
1000c
5000c
10000c
)
%
(
d
a
e
h
r
e
v
O
e
m
T
n
o
i
t
u
c
e
x
E
i
 300
 250
 200
 150
 100
 50
 0
s
e
n
r
a
b
t
s
a
b
l
m
o
s
b
m
m
f
e
k
a
u
q
e
s
n
a
e
m
k
e
c
a
r
t
y
a
r
c
r
a
p
a
c
s