title:Welcome to the Entropics: Boot-Time Entropy in Embedded Devices
author:Keaton Mowery and
Michael Yung Chung Wei and
David Kohlbrenner and
Hovav Shacham and
Steven Swanson
2013 IEEE Symposium on Security and Privacy
Welcome to the Entropics: Boot-Time Entropy in Embedded Devices
Keaton Mowery, Michael Wei, David Kohlbrenner, Hovav Shacham, and Steven Swanson
Department of Computer Science and Engineering
University of California, San Diego
La Jolla, California, USA
Abstract—We present three techniques for extracting en-
tropy during boot on embedded devices.
Our ﬁrst technique times the execution of code blocks early
in the Linux kernel boot process. It is simple to implement and
has a negligible runtime overhead, but, on many of the devices
we test, gathers hundreds of bits of entropy.
Our second and third techniques, which run in the boot-
loader, use hardware features — DRAM decay behavior and
PLL locking latency, respectively — and are therefore less
portable and less generally applicable, but their behavior is
easier to explain based on physically unpredictable processes.
We implement and measure the effectiveness of our tech-
niques on ARM-, MIPS-, and AVR32-based systems-on-a-chip
from a variety of vendors.
I. INTRODUCTION
Random numbers unpredictable by an adversary are cru-
cial to many computing tasks. But computers are designed to
be deterministic, which makes it difﬁcult to generate random
numbers. Substantial effort has gone into developing and
deploying subsystems that gather and condition entropy, and
that use it to generate random numbers on demand.
In this paper, we take an extreme position: Randomness
is a fundamental system service; a system cannot be said to
have successfully booted unless it is ready to provide high-
entropy randomness to applications.
Our main contributions are three techniques for gathering
entropy early in the boot process — before interrupts are
enabled, before a second kernel thread is spawned. Our
techniques are suitable for use even on embedded sys-
tems, where entropy-gathering is more challenging than on
desktop PCs. We implement our proposed techniques and
assess their effectiveness on systems-on-a-chip (SoCs) that
integrate ARM, MIPS, and even AVR32 CPU cores.
Motivation: Our work is inspired by the recent paper of
Heninger, Durumeric, Wustrow, and Halderman [16], which
uncovered serious ﬂaws in the design and implementation of
the Linux kernel’s randomness subsystem. This subsystem
exposes a blocking interface (/dev/random) and a non-
blocking interface (/dev/urandom); in practice, nearly
all software uses the nonblocking interface. Heninger et al.
observe (1) that entropy gathered by the system is not made
available to the nonblocking interface until Linux estimates
that 192 bits of entropy have been gathered, and (2) that
Linux is unnecessarily conservative in estimating the entropy
in events, and in particular that on embedded systems no
observed events are credited with entropy. These two facts
combine to create a “boot-time entropy hole,” during which
the output of /dev/urandom is predictable.
The Linux maintainers overhauled the randomness sub-
system in response to Heninger et al.’s paper. The timing
of every IRQ is now an entropy source, not just IRQs for
hard disks, keyboards, and mice. Entropy is ﬁrst applied to
the nonblocking pool, in the hope of supplying randomness
to clients soon after boot. (Clients waiting on the blocking
interface can block a bit longer.)
The new design leaves in place the race condition between
entropy accumulation and the reading of supposedly random
bytes from the nonblocking pool. It would be better, we
argue, to gather entropy so early in the boot process that all
requests for randomness can be satisﬁed.
In this paper, we present entropy-gathering techniques
that realize this vision. We show how to gather entropy
in the bootloader or early in the kernel boot process on
embedded systems running a variety of popular processors.
Our techniques require neither the multicore x86 processor
of desktop PCs nor the sophisticated sensors available to
smartphones. They do not require network connectivity.
They can be used in place of, or side by side with, Linux’s
current entropy-gathering infrastructure.
Our three techniques provide different
tradeoffs along
three metrics: (1) How many random bits can be obtained,
and how quickly? (2) How much system-speciﬁc knowledge
is required to implement the technique? (3) To what extent
can the entropy obtained be explained by well-studied phys-
ical processes that are believed to be unpredictable? None
of our proposed techniques is ideal along all three metrics.
Our ﬁrst technique: Instruction timing early in kernel
In our ﬁrst technique, we instrument the kernel’s
boot:
startup code to record how long each block of code takes
to execute. This approach has previously been used to
gather entropy in userland code; we show that it is also
applicable when a single kernel thread of execution runs,
with interrupts disabled, on an embedded system. On many
of the devices we tested (see Section II), this technique
gathers a surprisingly large amount of entropy — over 200
bits on the Raspberry Pi, for example — at negligible runtime
overhead; on other devices, less entropy is available.
We have not been able to account conclusively for the
large amount of entropy this technique gathers on some
© 2012, Keaton Mowery. Under license to IEEE.
DOI 10.1109/SP.2013.46
589
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
it gathers on other
devices or for the smaller amount
devices. In Section III, we pinpoint architectural features
that are partly responsible.
timing user input; or, in sensor-rich devices such as smart-
phones, from sensor noise in microphones [8, Section 5.3.1],
cameras [3], and accelerometers [38].
Our second and third techniques: DRAM decay and
PLL locking: In our second class of techniques, we take
advantage of architectural features that vary between SoCs,
rendering them less portable and less widely applicable, but
promising more entropy. In addition, we are able to pinpoint
more precisely the sources of the entropy we measure.
In Section IV, we show that it is possible for bootloader
code, running from on-chip SRAM,
to turn off DRAM
refresh. With refresh disabled, the contents of DRAM decay
unpredictably; we exploit
to obtain an entropy
source. In Section V, we show that our ability to repeatedly
reconﬁgure a peripheral clock on the BeagleBoard xM
translates into another high-rate entropy source.
this fact
A. Related Work
As noted above, the motivation for our paper is Heninger
et al.’s recent study of the Linux randomness subsystem [16].
Random number generation is hard, and ﬂaws in ran-
domness subsystems have been identiﬁed with dismaying
regularity. In 1996, Goldberg and Wagner analyzed the
random number generator in the Netscape browser [10]. A
decade later, Luciano Bello found that the OpenSSL package
shipped with Debian and Ubuntu had a broken random
number generator [37]. The bug’s effects were quantiﬁed by
Yilek et al. [41]. Cryptographers have designed “hedged”
cryptosystems whose security degrades as little as possible
in the absence of good randomness [2]. Otherwise secure
random number generators can break in novel settings: Ris-
tenpart and Yilek observed that virtual machine resets could
lead to randomness reuse and proposed solutions [31, 40].
Researchers have expended considerable effort consider-
ing how best to design randomness subsystems. Gutmann
described design principles for random number genera-
tors [11]; Kelsey, Schneier, Wagner, and Hall proposed
a formal security model for random number generators
and described attacks on deployed systems [23]. Kelsey,
Schneier, and Ferguson then proposed Yarrow, a concrete
design for a family of random number generators [24]. More
recently, NIST has made recommendations for producing
random numbers from an entropy pool [1]. Researchers have
also studied the effectiveness of the randomness subsystems
deployed with Linux [12, 26] and Windows [7]. Gutterman,
Pinkas, and Reinman, in their study of Linux randomness
system [12] speciﬁcally pointed out
the vulnerability of
Linux-based routers like those running OpenWRT software.
Entropy can be obtained from many sources: from ded-
icated hardware, using analog feedback circuits such as
phase-locked loops (PLLs) [9] or digital feedback circuits
(as included in Intel’s latest processors [4, 14]); from timing
other hardware devices, such as hard disks [6, 20]; from
Instruction timings have long been used as a source
In Section II-A we describe Bernstein’s
of entropy.
dnscache-conf program from 2000. The method was
explored in detail in the HAVENGE system of Seznec and
Sendrier [33]. In both cases, the entropy is assumed to
derive from the unpredictable arrival times of interrupts and
the behavior of the system scheduler. By contrast, our ﬁrst
technique (described in Section II) obtains entropy even
with interrupts disabled and a single thread of execution.
Pyo, Pae, and Lee, in a short note, observe that DRAM
refresh timings are unpredictable, which means that DRAM
access timings can be used as an entropy source [30].
Theoretical grounding for the unpredictability of instruc-
tion timing was given by McGuire, Okech and Zhou [27] and
Mytkowicz, Diwan, and Bradley [28]. These papers consider
x86 chips; the processors we study are considerably simpler.
Decay patterns in RAM, used in our second technique
(described in Section IV), have also been considered before.
Holcomb, Burleson, and Fu use SRAM decay as an entropy
source on RFID devices [18]. Halderman et al. studied
DRAM decay patterns in detail [13].
II. EARLY KERNEL ENTROPY
Our ﬁrst method for gathering entropy is an application
of a simple idea: After each unit of work in a code module,
record the current time using a high-resolution clock. Specif-
ically, we instrument start_kernel, the ﬁrst C function
run in the Linux kernel on boot, and use the cycle counter
as our clock.
Our approach is attractive. It runs as early as possible in
the kernel boot process: All but one use of randomness in the
Linux kernel occurs after start_kernel has completed.
It imposes almost no performance penalty, requiring, in our
prototype implementation, 3 KiB of kernel memory and exe-
cuting a few hundred assembly instructions. It is simple, self-
contained, and easily ported to new architectures and SoCs.
The question is, Does it work? Previous applications of
the same idea ran in user mode on general-purpose x86
machines. They could take advantage of the complexity
of the x86, the unpredictable arrival timing of interrupts,
interleaved execution of other tasks, and the overhead of
system call servicing when accessing a high-resolution
clock. By contrast, our code runs on an embedded device
with interrupts disabled and a single thread of execution.
Nevertheless, we are able to extract a surprising amount of
entropy — in some cases, hundreds of bits.
In this section, we discuss our
implementation and
evaluate its effectiveness on ARM SoCs from six vendors, a
MIPS SoC, and an AVR32 SoC. In Section III, we discuss
architectural mechanisms that are partly responsible for the
entropy we observe.
590
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
A. Genesis
In 2000, Daniel J. Bernstein released dnscache 1.00,
a caching DNS recursive resolver that is now part of the
djbdns package. DNS resolvers generally operate over
UDP, which means that an interested attacker can spoof the
answer to a query by simply forging a packet. To combat
this, each DNS query carries along with it a pre–selected
port number and query ID, which the response must have to
be considered valid. Therefore, dnscache, when acting as
a client of other DNS servers, must be able to choose these
port numbers and query IDs well [19, 22].
One of dnscache-conf’s duties is to provide en-
later be used by dnscache. To gather
tropy that will
this entropy, the dnscache-conf utility simply instru-
ments its own startup procedure with multiple calls to
gettimeofday(), and mixes each result into the entropy
pool. Due to the cost of each syscall, unpredictable hardware
interrupts, OS process scheduling, clock skew, and a host of
other factors, this method provides dnscache-conf with
high-quality entropy for the cost of a few extra syscalls. An
excerpt from dnscache-conf.c:
makedir("log");
seed_addtime();
perm(02755);
seed_addtime();
makedir("log/main");
seed_addtime();
owner(pw->pw_uid,pw->pw_gid);
seed_addtime();
perm(02755);
seed_addtime();
A method which works in userland on an x86 machine
might not apply to kernel-level code on much simpler
embedded devices. Indeed, we were initially skeptical: In
the absence of interrupts, multiple threads, syscall overhead,
and on simpler processors than the x86, would there still be
enough variation to make such a scheme viable?
B. Methodology
1) Kernel Instrumentation: To collect information about
the kernel boot process, we modiﬁed a Linux kernel for each
system we examined. Our kernel instrumentation consists of
a basic macro that can be inserted anywhere in kernel boot to
record the current cycle count with low overhead. The macro
recorded the current cycle count to an incrementing index
in a statically allocated array. We incremented the index at
compile time, and thus the only operations performed by the
measurement at run time are reading the cycle counter and
a single memory store.
We inserted the macro between every function call in
start_kernel, the ﬁrst C function called during kernel
boot. The majority of the code executed during this sequence
is straight-line, with a varying number of instructions ex-
ecuted during each function call. We chose this sampling
method because it offered the simplest patch to the kernel
at the earliest point in the boot process. Our instrumentation
then printed the measured times to the kernel log. An init
script copied out the relevant data from the log, truncated the
log, and immediately restarted the system using reboot.
Temperature data was not collected. In this manner, we
gathered data on thousands of restarts per day per machine
with minimal interaction. Machines were switched off and
the data pulled after 24–48 hours of continuous rebooting
and data collection.
To estimate the performance overhead, we implemented
a “production-ready” version, which skips printing to the
kernel log in lieu of mixing the results directly into the
kernel’s randomness pools. We then used the cycle counter to
measure the execution time of start_kernel, both with
and without our instrumentation. On the Raspberry Pi (de-
tailed in Section II-C3), our technique adds approximately
0.00019 seconds to the kernel boot process.
2) Devices: As described in the previous section, we
instrumented a variety of Linux kernels and ran them on
a broad variety of embedded platforms, ranging from high-
powered ARM computers to low-end special-purpose MIPS
and AVR devices.
ARM: ARM, Inc. licenses its processor architecture to
many companies that integrate ARM cores into their designs.
Two systems-on-a-chip that integrate the same ARM core
might nevertheless have very different performance charac-
teristics. To check the general applicability of our approach
to ARM-based embedded systems, we instrumented and col-
lected data from systems-on-a-chip from many of the most
prominent ARM licensees: Broadcom, Marvell, NVIDIA,
Texas Instruments, Qualcomm, and Samsung. These vendors
represent six of the top seven suppliers of smartphone
processors by revenue.
Speciﬁcally, the ﬁrst system we tested was the Raspberry
Pi, which contains a Broadcom BCM2835 SoC featuring a
1176JZF-S core, which is an ARM11 core implementing the
ARMv6 architecture. We also instrumented the BeagleBoard
xM, which uses a Texas Instruments DM3730 containing a
ARMv7 Cortex-A8; the Trim-Slice featuring an NVIDIA
Tegra 2, a ARMv7 Cortex-A9; the Intrinsyc DragonBoard,
with a Qualcomm SnapDragon SoC containing a Qual-
comm Krait ARMv7; the FriendlyARM Mini6410 with a
Samsung S3C6410, another version of the ARM1176JZF-S
ARM11 ARMv6 core; and the Cubox, which uses a Marvell
ARMADA 510 SoC containing a Sheeva ARMv7 core.
MIPS: Previous work on embedded device entropy
identiﬁed routers as important
targets, as they are con-
veniently located to inspect and modify network trafﬁc
and, as reported by Heninger et al. [16], routinely ship
with extremely poor entropy, as evidenced by their SSL
certiﬁcates.
591
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
With this in mind, we instrumented the early Linux boot
process on the Linksys WRT54GL router, containing a
Broadcom 5352EKPBG 200MHz MIPS “router-on-a-chip.”
Revered for their extensibility, the WRT54GL represents a
basic wireless router as found in the homes of millions.
AVR32: Finally, we instrumented a kernel for the Atmel
NGW100 mkII, which contains a AT32AP7000-U AVR32
core. The AVR32, designed by Atmel, represents one of the
smallest and lowest-power CPUs capable of running Linux.
Even on the AVR32, our techniques uncover substantial
randomness. The existence of instruction entropy on this
platform indicates that execution randomness is not solely
due to processor optimizations and complexity.
C. Results and Analysis
In this section, we will discuss the results of each device’s
instrumentation, and the expected quality of the
kernel
entropy extracted.
As the existence of true randomness is an open philosoph-
ical question (and therefore beyond the scope of this paper),
we will treat entropy as “unpredictability”: given the knowl-
edge that a remote attacker can possibly have, how difﬁcult
would it be to guess the device–generated random bits?
1) Statistical Tests: We are unable to conclusively pin-
point and characterize every source of entropy in these
systems. Therefore, this analysis will deal only with empir-
ical measurements, as sampled from each board over many
boots. We will rely mainly on two estimations: distribution
entropy and min-entropy.
Distribution entropy represents, for a given empirical
sample, the Shannon entropy of the underlying distribution.
For example, a set of samples consisting of 50 A’s and 50
B’s would have a single bit of distribution entropy, while
a set of samples consisting of 1024 unique values has a
distribution entropy of 10 bits. Distribution entropy can be
calculated, for a set S of n distinct observed values Vi, each
being seen Ci times, with C = |S| = ∑n
i=0(Ci), as:
D(S) = −
n
∑
i=1
Ci
C
· lg(cid:16)Ci
C (cid:17)
(1)
Note that distribution entropy will almost always underes-
timate the entropy of the underlying distribution. That is, the
distribution entropy calculated from a empirical sampling S
will always be less than or equal to lg (|S|), regardless of
the actual entropy of the underlying distribution.
Our other empirical estimator, min-entropy, measures the
prevalence of the most common element in a distribution. In
other words, if an adversary is allowed a single guess at the
value, min-entropy measures how often she will be correct.
For a set S of n distinct observed values Vi with counts Ci,
the min-entropy is:
With these two metrics, we can characterize the distribu-
tions sampled from each device and predict their real-world
entropy content.
2) Entropy Extraction: Furthermore, each boot sequence
generates a vector of test times, one per test. In our analysis,
we will examine both the sampled distributions of individual
test times, as well as the sampled distribution of test vectors.
The test vector, once generated, can be fed into an entropy
extractor to produce an evenly–distributed random seed,
which can then used to seed kernel pseudo-random number
generators.
The values in the test vector are partly correlated: if noth-
ing else, later tests have cycle counts larger than earlier tests.
Extracting the entropy from such a source is a challenging
theoretical problem [29], but under the random oracle heuris-
tic simply applying a cryptographic hash to the test vector
is sufﬁcient. NIST has published explicit recommendations
for implementing what they call “reseeding” in randomness
generators [1].
3) Raspberry Pi: The Raspberry Pi
is a popular
single-board ARM computer, built around the Broadcom
BCM2835 System–on–a–chip (SoC), which contains an
ARM 1176JZF-S ARM11 ARMv6 core clocked at 700
MHz. We modiﬁed the Linux 3.2.27 kernel provided for the
Raspberry Pi1 to perform our data collection. This involved
enabling and conﬁguring the hardware cycle counter and
the two hardware performance counters present in the ARM
1176JZF-S, as well as surrounding each function call in
start_kernel with instrumentation to record the current
counter values, and a ﬁnal function to dump our results to
the kernel log. We were able to surround every function in
start_kernel, for a total of 78 individual tests.
Next, we booted the instrumented kernel on four identical
Raspberry Pis, and recorded the counters for each boot.
In short, almost every test shows a surprising amount of
variation in the number of cycles it takes to execute. Figures
1, 2, 3, and 4 show a histogram of test times, in cycles,
for tests 4, 5, 36, and 41 as seen across 301,647 boots
across all four Raspberry Pi devices. The lighter regions are
the contribution of device #0, which, by itself, contributes
130,961 boots. These four graphs are representative of the
four classes of histogram that we see on the Raspberry
Pi: a “two-normal” distribution like Test 4, a “quantized”
distribution like Test 5, a “bimodal plus noise” distribution
like Test 36, and a “normal” distribution like Test 41.
For comparison, Test 4 corresponds to the initialization
function cgroup_init_early(), which is responsible
for setting up process groups for resource management,
and mostly involves setting memory locations to initial
values. Test 5 is local_irq_disable(), which disables
interrupts. It consists solely of the ARM instruction "cpsid
i", and the variation in this test is likely due to hardware
M(S) = − lg(cid:16) maxi(Ci)
C
(cid:17)
(2)
1Online: https://github.com/raspberrypi/linux
592
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:54:24 UTC from IEEE Xplore.  Restrictions apply. 
l
s
e
p
m
a
S
#
50,000
40,000
30,000
20,000
10,000
0
20000
21000
Cycles
22000
Figure 1: Histogram of cycle counts for Test 4 on 4 Rasp-
berry Pis. Lighter region is data from device #0 only.
l
s
e
p
m
a
S
#
100,000
50,000
0
150
200
Cycles
250
Figure 2: Histogram of cycle counts for Test 5 on 4 Rasp-
berry Pis. Lighter region is data from device #0 only.
l
s
e
p
m
a
S
#
40,000
30,000
20,000
10,000
0
600
700
800
900
1000
1100
Cycles
Figure 3: Histogram of cycle counts for Test 36 on 4
Raspberry Pis. Lighter region is data from device #0 only.
l
s
e
p
m
a
S
#
30,000
20,000
10,000
0
70000
72500
Cycles
75000
Figure 4: Histogram of cycle counts for Test 41 on 4
Raspberry Pis. Lighter region is data from device #0 only.
593
initialization state. Test 36 is prio_tree_init(), and is
simply a small loop which initializes an array. The relatively
quantized period of this function is likely due to stalls in
memory fetches and stores. Also, note that IRQs remain
disabled until Test 45, and so interrupts cannot be blamed
for any variation in these test times.
Overall, these distributions are far wider than we initially
expected. Test 41, in particular, has a minimum value of
69,098 cycles and a maximum of 76,625, almost 10.9%
more. In this region of 7,527 cycles, the data set contains
5,667 distinct test values.
Taken individually, the results of each test give an em-
pirical distribution over the cycles elapsed during execution.
If we treat a test as a random variable, we can extract that
entropy and use it to seed a random number generator.
To estimate the entropy contribution of each test, we apply
the distribution entropy calculation to our observed data. The
results of this calculation are in Table I.
However, further investigation is needed before we pro-
claim success. While each test has between 0.45 and 12.99
bits of distribution entropy, we cannot naively sum these
numbers and proclaim that to be our total entropy produced.
In order for that approach to be valid, each test must be
statistically independent — the time taken for test T must not
depend on the results for tests (0, . . . , T − 1). If, in the worst
case, T was a known function of (0, . . . , T − 1), then it would
not contribute any entropy whatsoever to the overall total,
even if it had plenty of distribution entropy: Its distribution
entropy would already be counted by the preceding tests.
(Note that we can always safely mix the results of T into
the entropy pool. In the worst case, doing so adds no beneﬁt.)
We applied a straightforward correlation test to the data
we gathered from the Raspberry Pi and our other sys-
tems. More sophisticated tests are possible, for example
using NIST’s test suite [32]. Speciﬁcally, we computed
the correlation coefﬁcients between each pair of tests. We
can then select a threshold of acceptable risk, and exclude