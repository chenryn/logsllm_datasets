𝑒𝜖0 + 1 +
1
=
+ 1,
where the last equality uses the identity 𝑥3 + 1 = (𝑥 + 1)(𝑥2 − 𝑥 + 1).
This completes the proof of Lemma 6.2.
■
𝑒𝜖0 (𝑒𝜖0 + 1) =
𝑒𝜖0
(𝑒𝜖0)3 + 1
𝑒𝜖0(𝑒𝜖0 + 1) =
(𝑒𝜖0 − 1)2
D OMITTED DETAILS FROM SECTION 6
D.1 Omitted Details from Section 7.1
Before proving (40), first we show an important property of 𝐸𝑚
that we will use in the proof.
Lemma D.1. 𝐸𝑚 is a non-increasing function of 𝑚, i.e.,
M(D′(𝑛)
𝑚+1)(𝒉)
𝑚+1)(𝒉)
≤ E
(cid:32)M(D(𝑛)
𝑛(cid:1) with |D𝑘| = |D′
(cid:33)𝜆
𝑘 =(cid:0)𝑑′
𝒉∼M(D′(𝑛)
𝑚 )
(cid:33)𝜆 ,
(cid:32)M(D(𝑛)
𝑛, 𝑑𝑛(cid:1) and D′(𝑛)
𝑚 )(𝒉)
𝑚 )(𝒉)
M(D(𝑛)
(66)
=
(cid:0)𝑑′
𝑛, 𝑑′
𝑛, . . . , 𝑑′
where, for any 𝑘 ∈ {𝑚, 𝑚 + 1}, D(𝑛)
𝑛, . . . , 𝑑′
Proof. Lemma D.1 follows from Lemma 5.3 in a straightforward
manner, as, unlike Lemma D.1, in Lemma 5.3 we consider arbitrary
pairs of neighboring datasets.
■
𝑘| = 𝑘.
𝑘
Here, steps (a) and (d) follow from the fact that 𝐸𝑚 is a non-increasing
function of 𝑚 (see Lemma D.1). Step (b) follows from the Chernoff
bound. In step (c), we used that M(𝑑𝑛) = R(𝑑𝑛) and M(𝑑′
R(𝑑′
𝑛), which together imply that
𝑛) =
(cid:19)𝜆(cid:35)
(cid:34)(cid:18)M(𝑑𝑛)
M(𝑑′
𝑛)
(cid:19)𝜆(cid:35)
(cid:34)(cid:18)R(𝑑𝑛)
R(𝑑′
𝑛)
𝐸0 = E
= E
≤ 𝑒𝜖0𝜆,
where the inequality follows because R is an 𝜖0-LDP mechanism.
■
D.2 Proof of Theorem 7.1
any 𝜆 ≥ 2 (including the non-integral 𝜆), we have
Theorem (Restating Theorem 7.1). Let 𝑚 ∈ N be arbitrary. For
sup
𝑚)∈D𝑚same
(D𝑚,D′
E𝒉∼M(D𝑚)
(cid:19)𝜆(cid:35)
(cid:34)(cid:18)M(cid:0)D′
𝑚(cid:1) (𝒉)
(cid:19)
M (D𝑚) (𝒉)
(cid:18)
≤ exp
𝜆2 (𝑒𝜖0 − 1)2
.
𝑚
(67)
Proof. Fix an arbitrary 𝑚 ∈ N. Let (D𝑚, D′
𝑚) ∈ D𝑚same and
𝐵) be the same as defined in the
1, . . . , 𝑝′
𝒑 = (𝑝1, . . . , 𝑝𝐵), 𝒑′ = (𝑝′
proof of Theorem 3.7 in Section 6.
(cid:170)(cid:174)(cid:172)𝜆
𝑝′
𝑗
𝑝 𝑗
ℎ 𝑗
𝑚
M (D𝑚) (𝒉)
(cid:19)𝜆(cid:35)
(cid:34)(cid:18)M(cid:0)D′
𝑚(cid:1) (𝒉)
(cid:169)(cid:173)(cid:171)1 + 𝐵∑︁
exp(cid:169)(cid:173)(cid:171)𝜆(cid:169)(cid:173)(cid:171) 𝐵∑︁
≤ E𝒉∼M(D𝑚)
= E𝒉∼M(D𝑚)
𝑝′
𝑗
𝑝 𝑗
𝑗=1
𝑗=1
ℎ 𝑗
𝑚
𝑝′
𝑗
𝑝 𝑗
𝑗=1
(cid:169)(cid:173)(cid:171) 𝐵∑︁
− 1(cid:170)(cid:174)(cid:172)𝜆
 ,
− 1(cid:170)(cid:174)(cid:172)(cid:170)(cid:174)(cid:172)
ℎ 𝑗
𝑚
(68)
In (68), 𝒉 is distributed according toM(D𝑚) = H𝑚(R(𝑑), . . . , R(𝑑)),
where the first equality uses (32) and the last inequality follows
from 1 + 𝑥 ≤ 𝑒𝑥.
where H𝑚 denotes the shuffling operation on 𝑚 elements and range
of R is equal to [𝐵]. Since all the 𝑚 data points are identical, and
all clients use independent randomness for computing R(𝑑), we
can assume, w.l.o.g., that M(D𝑚) is a collection of 𝑚 i.i.d. random
variables 𝑋1, . . . , 𝑋𝑚, where Pr [𝑋𝑖 = 𝑗] = 𝑝 𝑗 for 𝑗 ∈ [𝐵]. Thus, we
have (in the following, note that 𝒉 = (ℎ1, . . . , ℎ𝐵) is a r.v.)
where 1{·} denotes the indicator r.v. Substituting from (69) into (68),
we get
ℎ 𝑗 =
1{𝑋𝑖 =𝑗 }
=
𝑗=1
1
𝑚
1
𝑚
𝑝′
𝑗
𝑝 𝑗
𝐵∑︁
𝑚∑︁
𝐵∑︁
exp(cid:169)(cid:173)(cid:171)𝜆(cid:169)(cid:173)(cid:171) 𝐵∑︁
𝑗=1
𝑗=1
𝑖=1
𝑚∑︁
𝑖=1
𝑝′
𝑗
𝑝 𝑗
𝑝′
𝑗
𝑝 𝑗
(cid:34)
= E𝑋1,...,𝑋𝑚
exp
𝑚∑︁
𝑖=1
1
𝑚
𝑝′
𝑋𝑖
𝑝𝑋𝑖
,
(69)
1{𝑋𝑖 =𝑗 } =
− 1(cid:170)(cid:174)(cid:172)(cid:170)(cid:174)(cid:172)
(cid:32) 𝜆
(cid:32) 𝑝′
𝑚∑︁
ℎ 𝑗
𝑚
𝑚
𝑖=1
𝑋𝑖
𝑝𝑋𝑖
(cid:33)(cid:33)(cid:35)
− 1
E
𝒉∼M(D′(𝑛)
𝑚+1)
E𝒉∼M(D𝑚)
= E𝒉∼M(D𝑚)
E𝒉∼M(D′)
Now we can prove (40).
Proof of (40).
(cid:34)(cid:18) M(D)(𝒉)
∑︁
∑︁
M(D′)(𝒉)
=
𝑚<⌊(1−𝛾)𝑞(𝑛−1)⌋
(a)≤ 𝐸0
(cid:19)𝜆(cid:35)
≤ 𝑛−1∑︁
𝑚=0
𝑞𝑚𝐸𝑚 +
𝑞𝑚𝐸𝑚
∑︁
∑︁
𝑚≥⌊(1−𝛾)𝑞(𝑛−1)⌋
𝑚<⌊(1−𝛾)𝑞(𝑛−1)⌋
𝑚≥⌊(1−𝛾)𝑞(𝑛−1)⌋
𝑞𝑚𝐸𝑚
𝑞𝑚 +
∑︁
∑︁
𝑚≥⌊(1−𝛾)𝑞(𝑛−1)⌋
+
𝑚≥⌊(1−𝛾)𝑞(𝑛−1)⌋
+ 𝐸(1−𝛾)𝑞(𝑛−1) .
(b)≤ 𝐸0𝑒− 𝑞(𝑛−1)𝛾2
2
+
(c)≤ 𝑒𝜖0𝜆𝑒− 𝑞(𝑛−1)𝛾2
2
(d)≤ 𝑒𝜖0𝜆𝑒− 𝑞(𝑛−1)𝛾2
2
𝑞𝑚𝐸𝑚
𝑞𝑚𝐸𝑚
1
𝑚
𝐵∑︁
𝑗=1
𝑝′
𝑗
𝑝 𝑗
𝑞𝑚𝐸𝑚
E𝒉∼M(D𝑚)
Session 7D: Privacy for Distributed Data and Federated Learning CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea 2340(cid:18)𝑛
(cid:19)
distributed as a Binomial random variable Bin(𝑛, 𝑝). Thus, we have
M(D)(𝑘) =
M(D′)(𝑘) = (1 − 𝑝)
𝑘
𝑝𝑘(1 − 𝑝)𝑛−𝑘
(cid:19)
(cid:18)𝑛 − 1
𝑘 − 1
𝑝𝑘−1(1 − 𝑝)𝑛−𝑘
(cid:18)𝑛 − 1
(cid:19)
𝑘
𝑝
𝑝
𝑛
=
=
=
=
−
𝑒𝜖0
𝑛𝑒𝜖0
(72)
𝑒𝜖0
𝑛
𝑒𝜖0 + 1
𝑘
𝑛
𝑘
𝑛
𝑘
𝑛
𝑘