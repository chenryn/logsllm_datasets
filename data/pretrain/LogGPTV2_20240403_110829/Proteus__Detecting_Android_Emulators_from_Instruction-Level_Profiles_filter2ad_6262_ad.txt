### 5.4 Improving the Fidelity of QEMU

Leveraging Proteus's capabilities for identifying and classifying divergences in instruction-level behavior, this section demonstrates the feasibility of eliminating the sources of discrepancies to enhance QEMU’s fidelity.

We have modified the QEMU source code of the SDK emulator to address the top three detection methods listed in Table 2, which are based on incomplete sanitization of opcodes for Undefined encodings. Specifically, we aligned the decoding logic with the ARM ISA specification [4] to ensure that all opcode fields for these three cases are verified. For Undefined encodings, an illegal instruction exception is triggered.

These modifications eliminated 1,190 divergent cases as shown in Table 2. We evaluated the performance overhead using various CPU benchmarks from the MiBench suite [16]. As depicted in Figure 5, the minimal additional code required for the extra opcode checks did not introduce any measurable performance overhead. However, it should be noted that addressing alignment checks and endianness support in QEMU will require more extensive changes compared to the missing opcode checks for Undefined encodings.

**Figure 5: Overhead evaluation of fidelity enhancements.**

### 6 Discussion and Limitations

#### Countermeasures

One possible defense against CPU semantic attacks demonstrated in this work is to fix the root causes of instruction-level discrepancies in QEMU, as evaluated in Section 5.4. Enhancing QEMU's fidelity is crucial given the critical role of emulators in Android malware analysis and the increasing number of malicious apps employing evasion tactics. As a first step towards this goal, we have disclosed our root cause findings and shared a patch with the QEMU maintainers.

Proteus can enumerate a set of divergent instructions, similar to prior work that inspects x86 binaries to detect evasion [13]. This allows us to scan Android apps for the presence of divergent instructions. Such analysis can be adopted by malware analyzers (e.g., Google’s Bouncer [23]) to identify and prevent evasive malware from infecting Android users.

Another potential countermeasure is to use real hardware for dynamic analysis instead of emulators [22]. This approach can eliminate CPU-level discrepancies. However, practical limitations such as cost, scalability, and maintenance hinder widespread adoption. Additionally, the instrumentation required for analyzing applications on physical devices introduces artifacts, enabling fingerprinting [27]. Therefore, malware analysis systems for Android will continue to rely on emulators [23, 28, 30].

#### Limitations

Proteus uncovers several classes of observable artifacts in ARM CPU implementations between emulators and real devices. However, there may be other instruction-level discrepancies in current Android emulators that our system could not identify due to the limited scope of this work. This section discusses these limitations and describes open problems.

We demonstrated Proteus's capabilities on the ARMv7 architecture and for instructions in ARM mode. Recent Android devices also use the latest 64-bit ARMv8 variant of the ISA. Since ARMv8 provides compatibility with ARMv7, as evaluated in Section 5.3, the discrepancies discovered in this work also apply to ARMv8 CPUs. Discovering ARMv8-specific discrepancies using Proteus simply requires acquiring a Fast Model for an ARMv8 CPU (e.g., Cortex-A53) and repeating the experiments. Our present work did not explore instructions executing in Thumb mode, which provides improved code density via 16-bit instructions with limited functionality. Finally, this work focuses on ARM registers and did not explore potential discrepancies in the extension registers used by VFP/SIMD instructions. Expanding our system to include Thumb instructions and extension registers is part of our immediate future work.

Our current study does not fully address data-dependent divergences (e.g., depending on input values from registers or memory). This limitation is common to fuzzing approaches, as exhaustively exploring all possible inputs is computationally infeasible. One approach to improve Proteus would be to repeat the same test cases with several randomized inputs and corner cases (e.g., min/max values), as in prior work [21, 27].

As discussed in Sections 5.2 and 5.3, some divergences found by Proteus are due to Unpredictable instructions and do not correspond to implementation flaws. This is particularly the case since the ARMv7 specification written in SML [15], which we used to check Unpredictable instructions, does not cover all Unpredictable instruction encodings. A significant contribution of our analysis is that we discovered deterministic CPU-level discrepancies even in the presence of some Unpredictable instructions in our test cases. Recently, ARM has released an official machine-readable ISA specification written in a domain-specific language named ASL [5]. Unfortunately, the lack of official documentation and tools to work with ASL prevents us from relying on this resource. However, we find ASL specifications promising for enumerating Unpredictable encodings and improving our overall testing methodology.

### 7 Related Work

This section overviews prior work on discovering emulation detection methods and explains how Proteus distinguishes from or complements them. We also discuss existing defense approaches against evasive malware.

#### Finding Discrepancies in Emulation Environments

Jing et al. [17] identified a large number of detection heuristics based on differences in file system entries and return values of Android API calls. For instance, the presence of the "/proc/sys/net/ipv4/tcp_syncookies" file or a False return value from the "isTetheringSupported()" API indicates emulation. These discrepancies can be easily concealed by editing Android’s system images and API implementations to mimic real device views [12, 19]. Petsas et al. detect QEMU-based emulation by observing the side effects of caching and scheduling on QEMU [25]. Other work leverages performance side channels due to low graphics performance on emulators to fingerprint emulation [29]. These techniques, however, have practical limitations as they require many repeated trials and observations, increasing the detection risk of malware. Our work systematically uncovers observable differences in instruction semantics, achieving deterministic emulation detection through the execution of a single CPU instruction.

Similar to our approach, other works aim at discovering discrepancies in emulators at the instruction level. Various techniques [21, 24] execute randomized instructions on emulators and real hardware to identify discrepancies in x86 emulators. To ensure coverage of a wide set of instructions, other work [27] carefully constructs test cases with unique instructions based on manual analysis of the x86 ISA manual, while our technique is fully automated. Additionally, the analysis and findings of these studies are limited to the x86 instruction set, whereas the vast majority of mobile devices are powered by ARM CPUs. These studies classify divergences based on instructions (e.g., using mnemonics, opcodes), overlooking the fact that different instructions (e.g., LDM and STM) can diverge due to the same root cause (e.g., missing alignment check). Our study points to the unique root causes in the implementation of CPU emulators. As shown in Section 5.4, our findings are readily useful for improving the fidelity of QEMU. Finally, reliance on physical CPUs practically limits the number of test cases (e.g., instructions, register/memory operands, system register settings). We propose a novel scalable system that uses accurate functional models of ARM CPUs (i.e., Fast Models).

Martignoni et al. [20] used symbolic execution traces from a high-fidelity emulator to construct test cases that achieve high coverage while testing a low-fidelity emulator. The unavailability of such a high-fidelity emulator for Android, however, limits the applicability of this technique for our use.

#### Defense Against Evasive Malware

Several works propose detecting divergent behavior in malware as a defense mechanism. Balzarotti et al. [11] detect divergent behavior due to instruction semantics by replaying applications on emulators with the system call sequences gathered from real devices and comparing the runtime behavior. Lindorfer et al. [18] propose a more generic methodology for detecting evasive malware based on the similarity of execution behaviors collected from a set of virtual machines. These approaches do not systematically expose potential causes of divergences that future malware can use. Our work addresses the problem of proactively finding these instruction-level discrepancies and opens the possibility of preemptively fixing them.

Specifically for Android, other works [12, 19] systematically remove observable differences from API calls, file systems, and properties of emulator devices, demonstrating resistance against evasion. Such approaches, however, require enumeration of root causes of discrepancies. Our Proteus system aids these approaches by enumerating the divergent cases between emulators and real CPUs.

### 8 Conclusion

Scalable dynamic analysis of Android malware relies on emulators. However, due to observable discrepancies between emulated and real systems, malware can detect emulation-based analysis and alter its behavior to evade detection. Restoring the effectiveness of Android malware analysis requires systematic approaches to proactively identify potential detection tactics that can be used by malicious authors. This work presents the first systematic study of differences in instruction-level behavior of emulated and real ARM CPUs, which power the vast majority of Android devices. We presented the Proteus system for large-scale exploration of CPU semantic attacks against Android emulators. Proteus automatically analyzed detailed instruction-level traces collected from QEMU and accurate software models of ARM CPUs, revealing several major root causes for instruction-level discrepancies in QEMU. We demonstrated the feasibility of enhancing QEMU's fidelity by fixing the root causes of divergences without any performance impact. We are disclosing our findings and have submitted patches to QEMU as a step towards improving QEMU's resiliency against evasive malware.

**Acknowledgment:** This work was supported by the Office of Naval Research under grants N00014-15-1-2948 and N00014-17-1-2011. We would also like to thank Arm for providing us access to the Fast Models used in this work. Any opinions, findings, conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.

### References

1. Analyzing Xavier: An Information-Stealing Ad Library on Android. https://blog.trendmicro.com/trendlabs-security-intelligence/analyzing-xavier-information-stealing-ad-library-android/
2. Android Native Development Kit (NDK). https://developer.android.com/ndk/
3. ARM Fast Models. https://developer.arm.com/products/system-design/fast-guides/index.html
4. ARMv7-A/R Architecture Reference Manual. http://infocenter.arm.com/help/index.jsp?topic=/com.arm.doc.ddi0406c/index.html
5. Exploration Tools, A-Profile Architectures. https://developer.arm.com/products/architecture/a-profile/exploration-tools
6. Google has 2 billion users on Android. https://techcrunch.com/2017/05/17/google-has-2-billion-users-on-android-500m-on-google-photos/
7. Grabos Malware. https://securingtomorrow.mcafee.com/consumer/consumer-threat-notices/grabos-malware/
8. NetWinder Floating Point Notes. http://netwinder.osuosl.org/users/s/scottb/public_html/notes/FP-Notes-all.html
9. Number of Android applications. https://www.appbrain.com/stats/number-of-android-apps
10. QEMU emulation detection. https://wiki.koeln.ccc.de/images/d/d5/Openchaos_qemudetect.pdf
11. Balzarotti, D., Cova, M., Karlberger, C., Kruegel, C., Kirda, E., Vigna, G.: Efficient detection of split personalities in malware. In: NDSS (2010). http://www.eurecom.fr/publication/3022
12. Bordoni, L., Conti, M., Spolaor, R.: Mirage: toward a stealthier and modular malware analysis sandbox for android. In: Foley, S.N., Gollmann, D., Snekkenes, E. (eds.) ESORICS 2017. LNCS, vol. 10492, pp. 278–296. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-66402-6_17
13. Branco, R.R., Barbosa, G.N., Neto, P.D.: Scientific but not academical overview of malware anti-debugging, anti-disassembly and Anti-VM technologies. In: BlackHat (2012)
14. Egele, M., Scholte, T., Kirda, E., Kruegel, C.: A survey on automated dynamic malware-analysis techniques and tools. ACM Comput. Surv. (CSUR) 44(2), 6:1–6:42 (2008). https://doi.org/10.1145/2089125.2089126
15. Fox, A.: Directions in ISA specification. In: Beringer, L., Felty, A. (eds.) ITP 2012. LNCS, vol. 7406, pp. 338–344. Springer, Heidelberg (2012). https://doi.org/10.1007/978-3-642-32347-8_23
16. Guthaus, M.R., Ringenberg, J.S., Ernst, D., Austin, T.M., Mudge, T., Brown, R.B.: MiBench: a free, commercially representative embedded benchmark suite. In: IEEE International Workshop on Workload Characterization (IISWC) (2001)
17. Jing, Y., Zhao, Z., Ahn, G.J., Hu, H.: Morpheus: automatically generating heuristics to detect android mulators. In: Proceedings of the 30th Annual Computer Security Applications Conference (ACSAC), pp. 216–225. ACM (2014). https://doi.org/10.1145/2664243.2664250
18. Lindorfer, M., Kolbitsch, C., Milani Comparetti, P.: Detecting environment-sensitive malware. In: Sommer, R., Balzarotti, D., Maier, G. (eds.) RAID 2011. LNCS, vol. 6961, pp. 338–357. Springer, Heidelberg (2011). https://doi.org/10.1007/978-3-642-23644-0_18
19. Liu, L., Gu, Y., Li, Q., Su, P.: RealDroid: large-scale evasive malware detection on “real devices”. In: 26th International Conference on Computer Communication and Networks (ICCCN), pp. 1–8, July 2017. https://doi.org/10.1109/ICCCN.2017.8038419
20. Martignoni, L., McCamant, S., Poosankam, P., Song, D., Maniatis, P.: Path-exploration lifting: hi-fi tests for lo-fi emulators. In: International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), pp. 337–348. ACM, New York (2012). https://doi.org/10.1145/2150976.2151012
21. Martignoni, L., Paleari, R., Roglia, G.F., Bruschi, D.: Testing CPU emulators. In: International Symposium on Software Testing and Analysis (ISSTA) (2009). https://doi.org/10.1145/1572272.1572303
22. Mutti, S., et al.: Baredroid: large-scale analysis of android apps on real devices. In: Annual Computer Security Applications Conference, ACSAC (2015). https://doi.org/10.1145/2818000.2818036
23. Oberheide, J., Miller, C.: Dissecting the android bouncer. In: SummerCon (2012)
24. Paleari, R., Martignoni, L., Roglia, G.F., Bruschi, D.: A fistful of red-pills: how to automatically generate procedures to detect CPU emulators. In: Proceedings of the 3rd USENIX Conference on Offensive Technologies (WOOT) (2009)
25. Petsas, T., Voyatzis, G., Athanasopoulos, E., Polychronakis, M., Ioannidis, S.: Rage against the virtual machine: hindering dynamic analysis of android malware. In: European Workshop on System Security (EuroSec), pp. 5:1–5:6 (2014). https://doi.org/10.1145/2592791.2592796
26. Poeplau, S., Fratantonio, Y., Bianchi, A., Kruegel, C., Vigna, G.: Execute this! Analyzing Unsafe and Malicious Dynamic Code Loading in Android Applications. In: Network and Distributed System Security Symposium (NDSS) (2014)
27. Shi, H., Alwabel, A., Mirkovic, J.: Cardinal pill testing of system virtual machines. In: 23rd USENIX Conference on Security Symposium (2014)
28. Tam, K., Khan, S.J., Fattori, A., Cavallaro, L.: Copperdroid: Automatic reconstruction of android malware behaviors. In: NDSS (2015)
29. Vidas, T., Christin, N.: Evading android runtime analysis via sandbox detection. In: Proceedings of the 9th ACM Symposium on Information, Computer and Communications Security (ASIA CCS), pp. 447–458. ACM (2014). https://doi.org/10.1145/2590296.2590325
30. Yan, L.K., Yin, H.: DroidScope: seamlessly reconstructing the OS and Dalvik semantic views for dynamic android malware analysis. In: 21st USENIX Security Symposium, Bellevue, WA, pp. 569–584. USENIX (2012). https://www.usenix.org/conference/usenixsecurity12/technical-sessions/presentation/yan