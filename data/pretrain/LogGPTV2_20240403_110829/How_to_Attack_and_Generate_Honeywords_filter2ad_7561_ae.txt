### Honeyword Methods and Their Evaluation

#### Introduction
We will explore the effectiveness of seven honeyword methods derived from three password models. These methods are evaluated against three different password models, and the results are presented in Figure 6 of Appendix E.

#### List-Based Method
The List-based method consistently performs well in terms of both success-number and flatness. Regardless of the password model used to instantiate \( \text{PrPW}(\cdot) \) in Equation 4, an attacker (A) can only distinguish about 526 out of 104/19 passwords (see Figures 6(a) to 6(c)). With one guess against 20 sweetwords, A only achieves a 5% success rate (see Figures 6(d) to 6(f)). This indicates that the List-based method is the most effective among the three password models.

**Key Observations:**
1. The List-based method should be preferred for instantiating \( \text{PrHW}(\cdot) \) when facing a type-A1 attacker.
2. Wang et al.'s proposal [53] of using the hybrid method \( \frac{1}{3} \text{PCFG} \) to resist a type-A1 attacker is not optimal.

When A does not use the List-based attacks (Figures 6(e) and 6(f)), Markov or \( \frac{1}{3} \text{List} + \frac{1}{3} \text{PCFG} \) based methods sometimes outperform the perfect method and the List method. However, this does not contradict our preference, as it emphasizes that A is ineffective without List-based attacks.

#### Smoothing Techniques
When A uses the List-based password model to instantiate \( \text{PrPW}(\cdot) \), there are sweetwords with a large \( \frac{\text{PrPW}(\cdot)}{\text{PrHW}(\cdot)} \) ratio, even though these sweetwords are not real passwords. This issue is caused by the "+1" smoothing technique proposed by [53]: if \( \text{sw}_{i,j} \notin D \), set \( \text{Pr}(\text{sw}_{i,j}) = \frac{1}{|D|+1} \). Wang et al. [53] experimented with Laplace, Good-Turing, and +1 smoothing methods, finding the +1 method to be the most effective. However, this smoothing technique is suitable for popular passwords but not for unpopular ones. For extremely unpopular passwords, \( \frac{1}{|D|+1} \) is still too large, leading to false positives. We propose a new smoothing technique: if \( \text{sw}_{i,j} \notin D \) and \( \frac{\text{PrPW}(\text{sw}_{i,j})}{\text{PrHW}(\text{sw}_{i,j})} > 1 \), set \( \text{PrPW}(\text{sw}_{i,j}) = \text{PrHW}(\text{sw}_{i,j}) \). This eliminates false positives.

#### Type-A2 Attackers
Type-A2 attackers exploit user PII, making Equation 7 applicable. To resist such attackers, we designed the TarList method, which captures PII semantics in passwords. The TarList method inherits the advantages of the List method and can handle user PII. As shown in Figure 3(a), the optimal attacker only achieves a 5% success rate with one guess when \( k=20 \). A can only distinguish 531 real passwords when \( T_2=104 \), close to the perfect method (526).

#### Type-A3 Attackers
Type-A3 attackers exploit the user registration order, making Equation 8 applicable. With knowledge of the registration order, A can identify popular and unpopular sweetwords. The List-based method is vulnerable to unpopular passwords, so we designed the hybrid method \( \frac{1}{3} \text{PCFG} + \frac{1}{3} \text{Markov} + \frac{1}{3} \text{List} \) to resist type-A3 attackers. The hybrid method uses the smoothed List model to instantiate the password model and the same honeyword generation model as the server. 

**Practical Issues:**
1. Using external password datasets for training when the user base is small.
2. Handling sweetwords with \( \frac{\text{PrPW}(\text{sw}_{i,j})}{\text{PrHW}(\text{sw}_{i,j})} \gg 1 \).

External datasets are static, while the password distribution of the service is dynamic. Using an external dataset like Tianya results in poor performance (Figure 3(b)). We prefer using the internal training set, which improves performance (Figure 3(c)). For sweetwords with a large \( \frac{\text{PrPW}(\text{sw}_{i,j})}{\text{PrHW}(\text{sw}_{i,j})} \), we set a threshold \( \text{thd}=20 \) (Figure 3(d)). After addressing these issues, our hybrid method achieves 0.178-flatness against A3 (Figure 3(e)).

#### Type-A4 Attackers
Type-A4 attackers further exploit user PII, making Equation 9 applicable. We designed the \( \frac{1}{3} \text{TarPCFG} + \frac{1}{3} \text{Markov} + \frac{1}{3} \text{List} \) method to resist type-A4 attackers. The most severe attacker only achieves a 18.2% success rate with one guess against 20 sweetwords (Figure 3(f)) and recovers only 981 real passwords when \( T_2=104 \).

#### DoS Attacks
Our honeyword methods produce nearly indistinguishable honeywords, making them susceptible to DoS attacks. To mitigate this, we use blocklists, PSMs, rate-limiting, and customized alarm policies. A blocklist of 105 popular passwords significantly reduces DoS risks (Figure 4).

#### Model Extraction Attacks
An attacker who compromises the adaptive training model can extract high-entropy passwords. However, this risk is limited because:
1. Real user passwords are deleted after generating honeyword models.
2. A must generate password guesses and perform offline guessing.
3. Our system remains robust even if all sweetwords are recovered.

#### Summary
We retooled probabilistic password cracking models to build flat honeywords. This approach allows for easy integration of future improvements in password models. We overcame several practical challenges, resolving the question left in Juels-Rivest's work [35].

#### Evaluation Results
**Scalability with Varying k:**
The security of a honeyword method depends on \( k \), the number of sweetwords per account. Juels-Rivest recommended \( k=20 \) for a 5% success rate. Our hybrid method \( \frac{1}{3} \text{List} + \frac{1}{3} \text{Markov} + \frac{1}{3} \text{PCFG} \) reaches 0.1-flatness at \( k=200 \) (Figure 8(b)). For some password distributions, achieving 0.05-flatness may be impractical. We recommend \( k=40 \) for security-critical services.

**Conclusion:**
Our methods show promising results in resisting various types of attackers and address practical challenges in deploying honeyword systems.