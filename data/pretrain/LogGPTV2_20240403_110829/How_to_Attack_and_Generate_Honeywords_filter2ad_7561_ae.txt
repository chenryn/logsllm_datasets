) honeyword methods
will arise from combining these 3 password models. We attack
(
)
(
)
(
)
+
3
1
+
3
2
3
3
3List+ 1
3Markov+ 1
3Markov+ 1
these 7 honeyword methods by using three different password
models. Fig. 6 in Appendix E shows that
the List-based
method always parallels with the perfect method in terms of
both success-number and ﬂatness: Whichever password model
is used to instantiate PrPW(·) in Eq. 4,2 A can only distinguish
about 526=104/19 passwords (see Fig. 6(a)∼6(c)); A only
gains a 5% success rate with one guess against 20 sweetwords
(see Fig. 6(d)∼6(f)). Moreover, the List-based attacks are the
most effective among three password models.
All this indicates that: (1) We shall prefer the List-model
based honeyword method to instantiate PrHW(·) when subject
to a type-A1 attacker; and (2) Wang et al.’s proposal [53] of
using the hybrid method 1
3PCFG to resist
a A1 is not optimal. We note that, when A does not use
List-model based attacks (see Figs. 6(e) and 6(f)), Markov or
3List+ 1
3PCFG based methods sometimes perform
1
better than the perfect method and the List method. This does
not contradict with our preference but only emphasizes that A
is ineffective when does not use List-model based attacks.
We note that when A uses the List-based password model
to instantiate PrPW(·), there will be a number of sweetwords
that are with a large PrPW(·)
PrHW(·), yet these sweetwords are not real
passwords. We further investigate the issue and ﬁnd that this is
caused by the “+1” smoothing technique (proposed by [53]):
If swi;j ̸∈ D, set Pr(swi;j)= 1|D|+1 . Wang et al. [53] have
experimented with three smoothing methods (i.e., Laplace,
Good-Turing and +1), and found the +1 method most effective.
This kind of smoothing is suitable for attacking popular
passwords, which is the case for Juels-Rivest’s methods [35].
However, special attention shall be given to our methods where
unpopular passwords are vulnerable (see Appendix D). For
these extremely unpopular passwords,
1|D|+1 is still too large
and will result in a large PrPW(·)
PrHW(·), causing a false positive. We
devise a smoothing technique for A in such cases: If swi;j
̸∈ D and PrPW(swi;j )
PrHW(swi;j )=1. As a result,
these false positives can be eliminated.
For attackers of type-A2. A now further exploits user PII
as compared to a type-A1 attacker, and the Eq. 7 applies.
Thus, the corresponding method shall be able to capture the
PII semantics in passwords. This leads to our design of the
TarList method to best resist against a type-A2 attacker. The
rationale is that the TarList method inherits the merits of the
2To be most effective, A shall always use the system’s honeyword method
PrHW((cid:1)), which is public info, to instantiate her PrHW((cid:1)).
PrHW(swi;j ) > 1, then set PrPW(swi;j )
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:19:56 UTC from IEEE Xplore.  Restrictions apply. 
974
(a) 7 methods under a type A2 attacker. The results
are desirable and the experiment succeeds.
(b) 7 methods under A3; PrHW((cid:1)) uses an
external train set, without PrPW(·)
PrHW(·) >thd smooth.
(c) 7 methods under A3; PrHW((cid:1)) uses an internal
training set, with no PrPW(·)
PrHW(·) check.
the method List&PCFG
3 PCFG) that tackle
3 Markov+ 1
(f) Three targeted hybrid methods under A4;
(d) Four versions of
PrHW((cid:1)) uses an internal training set (i.e., passw-
&Markov (= 1
3 List+ 1
ords of the target site) and PrPW(·)
the PrPW(·)
PrHW(·) >20 smooth.
Fig. 3. Exploratory experiments examining practical issues under attackers A2 (cid:24) A4, trained on Dodonew-tr and tested on Dodonew-ts.
TarList under A2, 1
(e) Three hybrid methods under A3; PrHW((cid:1))
uses an internal training set (i.e., the passwords of
the target site) and PrPW(·)
3 TarPCFG under A4 are the best ones.
3 PCFG under A3, and 1
PrHW(·) >thd smooth, UB=user base.
PrHW(·) >20 smooth.
3 List+ 1
3 Markov+ 1
3 TarList+ 1
3 TarMarkov+ 1
List method as discussed above and can further deal with user
PII. As expected, Fig. 3(a) shows that the optimal attacker only
achieves a 5% success rate with one guess when k=20. A can
only distinguish 531 real passwords when T2=104, quite close
to that of the perfect method (i.e., 526=104/19). Hereafter we
give A’s success-number (at 104 failed attempts) instead of
the whole graph due to space constraints.
For attackers of type-A3. A now further exploits the user
registration order as compared to a type-A1 attacker, and Eq.
8 applies. With the knowledge of user registration order, A
now can ﬁgure out which sweetwords are popular or not (at a
given time point). As revealed in Appendix D, the List-based
password model alone is vulnerable to unpopular passwords.
For example, if A ﬁnds a sweetword that has never appeared
in the earlier users’ sweetlists, then she can be certain that
this sweetword is the current user’s real password. Thus, the
List honeyword method is unsuitable for a type-A3 attacker.
Fortunately, at the same time we ﬁnd that the PCFG-based
and Markov-based password models are good at capturing
unpopular passwords, even though they each has their own
defects (see Appendix D). This leads to our design of the
hybrid method 1
3PCFG to best resist a type-
A3 attacker. For hybrid models, the way to instantiate the best
strategy for the type-A3 attacker is similar to attacker A1;
that is, the type-A3 attacker uses the smoothed List model to
instantiate her password model, and uses the same honeyword
generation model as the server to instantiate her honeyword
model. In particular, A adaptively updates her honeyword
model using the sweetword ﬁle to increase her advantage.
3Markov+ 1
3List+ 1
However,
there are a number of practical
issues to be
addressed when applying the hybrid-model based honeyword
design approach, and the two most challenging ones are: (1)
Can we use external password datasets to be the training set
when the user base is not large?; and (2) What can we do when
≫1?
encountering a sweetword swi;j such that PrPW(swi;j )
PrHW(swi;j )
training
We now investigate the inﬂuence of external
datasets. For example, suppose a start-up web service wants
to adopt a honeyword system when it only has 104 users.
Generally, such a small user-base is considered insufﬁcient
to be used as training sets for password models like PCFG
[40] and TarPCFG [56]. Thus, it is natural to employ an
external training set. However, Fig. 3(b) shows that such an
approach is insecure: Under A3, 1
3PCFG
only achieves 0.2525-ﬂatness when the external Tianya
training set is used. The reason is that: The external dataset
is static, while the password distribution of the service under
study is dynamic as new users register, and as time goes on,
these two password distributions will be evidently different.
A3 can exploit this fact. We prefer only using the internal
training set, that is, the password dataset of its own users. As
shown in Fig. 3(c), things go better (ϵ-ﬂatness goes down by
5%∼10% in general), but the situation is still undesirable.
3Markov+ 1
3List+ 1
Fortunately, as discussed in Sec. IV-A, these sweetwords x
resulting in a large PrPW(x)
PrHW(x) are mainly unpopular ones (i.e.
with frequency f thd. Now,
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:19:56 UTC from IEEE Xplore.  Restrictions apply. 
975
3TarList+ 1
3 Markov+ 1
how to set thd? After a number of experiments, we ﬁnd that
thd=20 performs the best. Fig. 3(d) presents an illustration.
3List+ 1
3Markov+ 1
3TarMarkov+ 1
When user Ui registers, the site ﬁrst produces k-1 hon-
eywords using our hybrid method 1
3PCFG
trained on the internal training set, and then Ui’s password is
inserted into the training set for update. Note that all password
models we consider in this work can be trained in a streaming
fashion, and thus there is no need to temporarily keep plaintext
passwords on the server for training. Fig. 3(e) shows that,
after addressing these practical
issues, our hybrid method
can be 0.178-ﬂat (resp. 0.2525-ﬂat in Fig. 3(b)) against A3
when k=20. A3 can only distinguish 736 real passwords when
T2=104, suggesting our method is promising against A3.
For attackers of type-A4. A now further exploits user PII as
compared to a type-A3 attacker, and the Eq. 9 applies. This
leads to our design of the 1
3TarPCFG
method to best resist A. The rationale is the same with the case
for a type-A2 attacker. As expected, the most harsh attacker
only achieves a success rate of 18.2% with one guess against
a list of 20 sweetwords (see Fig. 3(f)), and recovers mere 981
real passwords when the global threshold T2=104.
DoS attack. As our honey-
word methods can produce
honeywords that are nearly
indistinguishable from real
passwords,
the fraction of
popular honeywords would
be close to the fraction of
popular passwords. There-
fore, when server S fails to
adopt proper mitigations, a
DoS attacker could trigger
false alarms by deliberate-
ly submitting popular pass-
words. As discussed in Sec.
II-B, S can effectively mitigate DoS threats by using block-
lists, PSMs, rate-limiting and customized alarm policies, etc.
We now conduct two preliminary experiments to show the
effect of blocklists against DoS attacks. We ﬁrst construct
a blocklist with 105 popular passwords for Chinese users
according to [56]; a blocklist of size 105 is widely recom-
mended [19], [31]. To test the DoS mitigation effectiveness
of our blocklist on the 1
3PCFG method, we
ﬁlter passwords that appear in the blocklist to simulate the
deployment of blocklist on registration, and similarly block
weak honeywords in the honeyword generation phase. Fig. 4
shows that introducing a blocklist signiﬁcantly alleviates the
DoS risks: When T1=1, with 100 online guesses the DoS
attacker can achieve a success rate of 6.08% when k=20 and
a success rate of 12.13% when k=40, while this ﬁgure will
be 0.003% for k=20 and 0.025% for k=40 when T1=3 (see
Fig. 4 in Appendix E). This indicates that a proper blocklist
can effectively mitigate DoS attacks in a large part. Further
coupled with PSMs, stricter rate-limiting and customized
alarm policies, DoS threats can be further mitigated.
Fig. 4. DoS success rate against
1
3 List+ 1
3 PCFG. Trained on
Dodonew-tr,
tested on Dodonew-ts.
“Blocklist” means a 100k blocklist is
used to ﬁlter weak passwords (and
honeywords), while “Normal” means
no blocklist
is used. “DoS success
rate” means the probability of hitting
T1=1 honeyword for each account.
3List+ 1
3Markov+ 1
Model extraction attack. For the attacker that can some-
how obtain the adaptive training model (e.g., compromise
S), it is possible to extract high entropy passwords (e.g.,
fullname+birth year) directly from the model without online
and ofﬂine guessing. Nevertheless, this risk is very limited.
First, real user passwords are only used in training, and
they can be deleted from the memory/disk once honeyword
models are generated/updated. Second, A still has to generate
a set of password guesses from (smoothed) password models,
and invest considerable efforts to perform ofﬂine guessing;
otherwise, it is impossible to know which password belongs
to which user account. Finally, our honeyword system remains
robust even when all sweetwords are recovered.
Summary. We retool probabilistic password cracking models
to build ﬂat honeywords. This approach has signiﬁcant beneﬁts
in that: Future improvements to password models (e.g., deep
learning) can be included easily into our honeyword meth-
ods. We manage to overcome several previously unexplored
challenges that arise in the practical adoption of password
models. This resolves the question of “can the password
models underlying cracking algorithms (e.g., PCFG [58]) be
easily adapted for use” as left in Juels-Rivest’s work [35].
V. EVALUATION RESULTS
We now examine the scalability of our methods, and eval-
uate their security by both experiments and user-studies.
A. Scalability with varying k
Clearly, the security of a honeyword method depends on
the parameter k which indicates how many sweetwords are
associated with each account. In Juels-Rivest’s work [35], k
is recommended to be 20, as they believed that it is acceptable
for the attacker A to gain “a chance of at most 5% of picking
the correct password” when given 20 sweetwords (i.e., being
ϵ=0.05 ﬂat). Existing literature only evaluates the situation of
k=20. Now a natural question arises: How can we set k to
ensure that the method achieves an expected security level
(e.g., 0.05-ﬂat)? In other words, how will a method perform
with varying k? We call this property as a method’s scalability.
As shown in Fig. 8(a) in Appendix E, a security goal
of 0.05-ﬂat seems prohibitively far away: Storing too many
sweetwords for each user will not only increase storage
cost but also delay login time. Though our hybrid method
3PCFG (under A3) shows much better scala-
3List+ 1
1
bility (see Fig. 8(b)), it only reaches 0.1-ﬂat when k=200. Note
that ϵ decreases rather slowly as k increases. Interestingly, we
kc , where a=0.084,
ﬁnd that ϵ and k well follow ϵ = a + b
b=1.4468, c=0.8329. As k → +∞, ϵ decreases monotonically
and ϵ → a=0.084>0.05. This suggests that, for some password
distributions, 0.05-ﬂat is likely out of practical reach.
3List+ 1
As shown in Fig. 8(b), our method 1
3Markov
3PCFG reaches ϵ=0.20 when k=20, ϵ=0.17 when k=30,
+ 1
ϵ=0.15 when k=40, and ϵ=0.14 when k=50. We have
experimented with other combinations, and got
similar
diminish returns. Since services that adopt honeywords
generally would be security-critical, we recommend k=40 to
3Markov+ 1
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:19:56 UTC from IEEE Xplore.  Restrictions apply. 
976
(a) Success-number graph: Tweaking-tail.