title:Web Performance Pitfalls
author:Theresa Enghardt and
Thomas Zinner and
Anja Feldmann
Web Performance Pitfalls
Theresa Enghardt1(B), Thomas Zinner1, and Anja Feldmann2
1 TU Berlin, Berlin, Germany
{theresa,zinner}@inet.tu-berlin.de
2 Max-Planck Institute for Informatics, Saarbr¨ucken, Germany
PI:EMAIL
Abstract. Web performance is widely studied in terms of load times,
numbers of objects, object sizes, and total page sizes. However, for all
these metrics, there are various deﬁnitions, data sources, and measure-
ment tools. These often lead to diﬀerent results and almost all studies do
not provide suﬃcient details about the deﬁnition of metrics and the data
sources they use. This hinders reproducibility as well as comparability
of the results. This paper revisits the various deﬁnitions and quantiﬁes
their impact on performance results. To do so we assess Web metrics
across a large variety of Web pages.
Amazingly, even for such “obvious” metrics as load times, diﬀerences
can be huge. For example, for more than 50% of the pages, the load times
vary by more than 19.1% and for 10% by more than 47% depending on
the exact deﬁnition of load time. Among the main culprits for such dif-
ference are the in-/exclusion of initial redirects and the choice of data
source, e.g., Resource Timings API or HTTP Archive (HAR) ﬁles. Even
“simpler” metrics such as the number of objects per page have a huge
variance. For the Alexa 1000, we observed a diﬀerence of more than 67
objects for 10% of the pages with a median of 7 objects. This highlights
the importance of precisely specifying all metrics including how and from
which data source they are computed.
Keywords: Web performance · Measurement
1 Introduction
Web browsing is one of the most prevalent applications in today’s Internet.
Thus, understanding its performance is critical. Hereby, both metrics as well
as experiments have to realistically reﬂect possible performance improvements
for actual users. Moreover, they need to be reproducible. However, quantifying
Web performance is challenging due to Web page diversity, heterogeneous devices
types and browsers, choice of metrics, including network-centric, browser-centric,
and user-centric metrics, and the lack of well-established standards. Given this
diversity, it is critical that studies provide suﬃcient details regarding their choice
of metrics, data sources, and tools, to (a) understand and interpret the results,
(b) to compare results across studies, and (c) to reproduce them independently.
c(cid:2) Springer Nature Switzerland AG 2019
D. Choﬀnes and M. Barcellos (Eds.): PAM 2019, LNCS 11419, pp. 286–303, 2019.
https://doi.org/10.1007/978-3-030-15986-3_19
Web Performance Pitfalls
287
PLT with redirects
PLT without redirects
8
0
.
F
D
C
E
4
0
.
0
0
.
10
100
1000
10000
100000
Time [ms]
Fig. 1. Page Load Time (PLT) with and without initial redirects.
For instance, Page Load Time (PLT) is a common metric used to estimate
user-perceived quality (QoE) and to evaluate mechanisms for improving Web
browsing. Thus, inaccuracies can lead to skewed results which may even lead
to wrong conclusions. PLT is often deﬁned as “time until onLoad1 event”. A
less considered aspect is the start point of the measurement. PLT may include
initial redirects, e.g., when a browser starts loading http://example.com and is
redirected to https://www.example.com—the actual landing page. Such redi-
rects increase PLT. To highlight that the discrepancies are non-negligible Fig. 1
depicts PLTs with and without initial redirects2. According to the most recent
W3C Navigation Timings speciﬁcation [3] initial redirects should be included
in all browser timings. But, whether redirects actually occur in a page load
depends on the web workload, i.e., whether one starts with http://example.com
or https://www.example.com. Moreover, even the end point of the measurement
is not always well speciﬁed (see the Survey section), nor is it obvious how to pre-
cisely measure it. We are not aware of any prior work that quantiﬁes the impact
of the exact choice of metric on the measurement results.
The main contributions of this paper are as follows. (1) We survey Web per-
formance studies and summarize which measurement tools, methods, and metrics
are used. Amazingly, we ﬁnd that a third of these studies do not provide precise
deﬁnitions of their metrics and/or data sources. However, it allows us to identify
tools which are typically used for evaluating Web performance. (2) We realize
a test environment that allows us to compare diﬀerent tools against a baseline
to assess their accuracy. Among our results are that in-/exclusion of initial redi-
rects skews the page load times by up to 47% for 10% of the pages. Moreover,
object sizes diﬀer from the packet trace for more than 60% of objects. This is
critical as metrics derived from object sizes, e.g., Byte Index of loaded objects
over time, diﬀer by more than 50%. (3) We discuss lessons learned regarding
Web performance measurements and provide guidance on how to increase the
1 See Fig. 2 for an overview and Appendix A for more explanation.
2 For details regarding the methodology and the corresponding dataset see Sect. 4.
288
T. Enghardt et al.
Fig. 2. Browser events and timings. See Appendix A for more details.
accuracy of measured load times and object sizes3. Most importantly: (1) HAR
ﬁles are the most reliable data source for object counts and sizes. Resource tim-
ings underestimate these metrics, as they do not include objects in embedded
frames, and they often do not provide object sizes for cross-origin objects. (2)
As redirects may highly inﬂuence load times, make a conscious choice whether
to include them.
2 Web Metrics and Tools
Typical Web metrics include load times, object sizes, number of objects, and page
sizes. Each of these metrics has various deﬁnitions and data sources. Moreover,
there are diﬀerent tools to measure them which we outline in the following.
2.1 Load Times
The time for loading Web pages strongly correlates with user experience [28]. To
load a Web page, a browser usually loads the base document, parses it, constructs
a Document Object Model (DOM), loads the referenced objects, processes them,
and displays the results. Figure 2 shows a detailed view of this process including
the browser events which are the basis of several commonly used load times
metrics.
Deﬁnitions for Load Times: Typically, Page Load Time (PLT) is deﬁned as
the time until the onLoad event. However, in the eye of the user, the actual Web
page display is often ﬁnished earlier, e.g., when the content is ﬁrst displayed
on the screen. Thus, other timings include domContentLoaded, when all objects
referenced in the base document have been loaded, Time To First Paint (TTFP),
when the ﬁrst content is rendered, or Above The Fold Time (AFT), when the
part of the page visible on the user’s screen has been fully rendered. Start times
can be the navigationStart, the fetchStart, or when the ﬁrst DNS request or TCP
connection is opened.
Data Sources for Load Times: Load times based on browser navigation
events are available through the standardized Navigation Timings API [2,3].
3 Our tools are publicly available at https://github.com/theri/web-measurement-
tools.
Web Performance Pitfalls
289
Moreover, Time To First Paint (TTFP) is currently being standardized [6]. Being
standardized implies that these metrics are available for diﬀerent browsers based
on a “similar” deﬁnition. HTTP Archive (HAR) ﬁles [7] also include onLoad and
domContentLoaded times. However, AFT is not standardized, and estimating
it requires not only load time data but also object positions within the Web
page [11]. Load times are available through the Resource Timings API from
version 1 [4] onward or from HAR ﬁles. Object positions are available by querying
the DOM, e.g., using JavaScript.
Tools: Most popular browsers4 implement Navigation Timings and Resource
Timings. The standardized version of TTFP is not yet supported by all browsers5
as of September 2018. AFT is realized via a browser plugin available for
Chrome [11]. HAR ﬁles can be exported using built-in developer tools.
To automate page loads, both Chrome and Firefox provide remote debugging
interfaces, i.e., the Chrome DevTools Protocol6, and Firefox Marionette7. For
both interfaces, there is a variety of clients to navigate to a page and interact
with it, e.g., to inject JavaScript code to export a timing.
Browser automation frameworks such as Selenium [8] allow more complex
Web page interactions using a standardized webdriver interface, which controls
Firefox using the Marionette protocol. The authors of Selenium advise against
using it for Web performance testing, as its complex setup may incur signiﬁcant
performance overhead [9]. Furthermore, WebPagetest [10] integrates diﬀerent
browser automation frameworks into a single platform. It provides a Web-based
User Interface for Navigation Timings, HAR ﬁles, load times, and Speed Index.
2.2 Number and Size of Objects
Number and sizes of objects are used to estimate the complexity of Web pages
and are needed to compute metrics such as Object Index or Byte Index.
Possible Deﬁnition of Object Count, Object Size, and Derived Met-
rics: Nowadays, Web pages often fetch objects continuously even after the ini-
tial page load has completed. Therefore, object counts should only include those
objects loaded until the onLoad event. This can be done by either observing
HTTP request-response pairs or by using the objects in the DOM. With regards
to object size, networking-related studies usually use the encoded size, i.e., the
number of bytes transferred over the network. One alternative is the decoded
size, namely the number of bytes after decompression. However, as objects are
transferred over HTTP there is overhead, namely the HTTP headers. Unfortu-
nately, it is often unclear if the object size includes the header or not. The total
page size is the sum of all object sizes. Byte Index is the integral of sizes of
objects loaded over time, see [1].
4 See http://gs.statcounter.com/browser-market-share/desktop/worldwide.
5 Chrome and Opera support it, Firefox is still validating their implementation.
6 https://chromedevtools.github.io/devtools-protocol/.
7 https://ﬁrefox-source-docs.mozilla.org/testing/marionette/marionette.
290
T. Enghardt et al.
Table 1. Survey of Web performance studies: metrics and data sources.
Metrics
PLT
Deﬁnition
Time of onLoad
Data source
Used in papers
Navigation timings 6
Time to load all objects
Unknown
HAR ﬁle
Unknown
HAR ﬁle
Unknown
1
2
1
3
DOM time
Time of domContentLoaded
Navigation timings 1
AFT
Visible content rendered
Resource timings
Object load
times
Time until object responseEnd Resource timings
Object size
Number of bytes transferred
HAR ﬁle
HAR ﬁle
Unknown
Number of
objects
HTTP request-responses before
onLoad
Resource timings
Number of DOM resources
HAR ﬁle
2
1
1
2
2
1
4
Data Sources for Object Sizes: One way to derive the number of objects is
to count the number of HTTP request-response pairs using the list of entries in a
HAR ﬁle. The number of objects involved in constructing a page is available via
the Resource Timings API. HAR ﬁles [7] as well as Resource Timings version 2 [5]
provide encoded and decoded body size of each object. In addition, HAR ﬁles
include HTTP headers, possibly including a Content-Length header, and header
size8, while Resource Timings also includes the transfer sizes of header and body.
An alternative is to extract the number of objects from a packet capture trace
if it is possible to successfully decrypt all elements. However, exact object sizes
can be oﬀ due to TLS padding.
3 Survey of Web Studies
Given the variety of metrics deﬁnitions, data sources, and tools, we survey Web
performance studies published at SIGCOMM, IMC, PAM, NSDI, and CoNEXT
during the last 8 years. In total, we include 15 papers [11–25], two of which
include links to their code repositories in their papers.
Table 1 summarizes the metrics and data sources of the surveyed papers.
Many of them use PLT, as it is well-known and widely used across academia and
industry, standardized by W3C, and readily available from various data sources.
However, the surveyed papers use diverse deﬁnitions and data sources which
surprisingly are often not even speciﬁed in the paper. We note that only one of
the surveyed papers even mentions initial redirects. Several papers compare PLT
8 Note that for HTTP/2, logged header sizes do not correspond to bytes on the wire
anymore due to HTTP/2 header compression.
Web Performance Pitfalls
291
Table 2. Survey of Web performance studies: browsers and automation tools.
Browser
Automation tool Used in papers
Chrome (desktop) DevTools
Selenium
Unknown
Chrome (mobile)
adb shell
Firefox (desktop) Selenium
Unknown
phantomJS
-
6
1
1
1
1
2
1
with other metrics such as AFT, which is more user-centric, but not standardized
and, thus, harder to measure. Finally, several papers in the survey (also) measure
the number, size, and load times of individual objects to compute integral metrics
to quantify the page load process. Such metrics are readily available from the
data sources. But many papers fail to precisely specify how they measure or
compute these metrics.
Tools Used to Fetch Pages: Table 2 summarizes which browsers and automa-
tion tools are used in the surveyed papers. Chrome is most popular, with Firefox
in second place. Most studies use the DevTools interface but some use Selenium.
To highlight the need for more information we point out that one paper uses a
dataset and testbed without stating either the browser or the tools used. Overall,
we conclude that a variety of diﬀerent tools are used, with yet unclear eﬀects on
the results.
4 Methodology
So far we have pointed out that many diﬀerent Web performance studies used
diﬀerent metrics. In this section, we explain our setup to understand the impact
of diﬀerent metrics. To compare the impact of diﬀerent frameworks9 and diﬀerent
Web pages we use the following tools10: (1) Firefox 61.0.2 with Selenium 3.14.0
and geckodriver 0.21.0, (2) Firefox 61.0.2 with Marionette, and (3) Chrome 69
with Chrome DevTools.
We load pages from a Thinkpad L450 with Debian Stretch. To avoid band-
width issues, our vantage point is directly connected to a university network.
To minimize the eﬀects of DNS caching and delay to the resolver, we use a
recursive resolver close11 to our vantage point instead of popular open resolvers.
9 Our scripts instrument browser automation frameworks directly to give us more
control and avoid the overhead of an integrated framework such as WebPagetest.
10 For realistic browser behavior, which includes the rendering engine, we open Web
browsers including the graphical user interface rather than using them in headless
mode.
11 Close in terms of network distance.
292
T. Enghardt et al.
8
.
0
F
D
C
E
4
.
0
0
.
0
0
t
e
s
a
t
a
d
n
i
y
c
n
e
u
q
e
r
F
400
300