# Web Performance Pitfalls

## Authors
- Theresa Enghardt<sup>1</sup>
- Thomas Zinner<sup>1</sup>
- Anja Feldmann<sup>2</sup>

### Affiliations
1. TU Berlin, Berlin, Germany
   - Email: {theresa, zinner}@inet.tu-berlin.de
2. Max-Planck Institute for Informatics, Saarbrücken, Germany

### Abstract
Web performance is extensively studied through various metrics, including load times, object counts, object sizes, and total page sizes. However, these metrics often have multiple definitions, data sources, and measurement tools, leading to inconsistent results. Most studies lack sufficient detail about metric definitions and data sources, hindering reproducibility and comparability. This paper revisits the definitions of web metrics and quantifies their impact on performance results by assessing a wide range of web pages.

Surprisingly, even for "obvious" metrics like load times, differences can be substantial. For over 50% of the pages, load times vary by more than 19.1%, and for 10% by more than 47%, depending on the exact definition. Key factors contributing to these differences include the inclusion or exclusion of initial redirects and the choice of data source, such as the Resource Timing API or HTTP Archive (HAR) files. Even simpler metrics like the number of objects per page show significant variance. For the Alexa top 1000, we observed a difference of more than 67 objects for 10% of the pages, with a median difference of 7 objects. This underscores the importance of precisely specifying all metrics, including how they are computed and from which data sources.

**Keywords:** Web performance, Measurement, Metrics, Data sources, Reproducibility

## 1. Introduction
Web browsing is one of the most prevalent applications in today's Internet, making it crucial to understand its performance. Metrics and experiments must realistically reflect potential performance improvements for actual users and be reproducible. Quantifying web performance is challenging due to the diversity of web pages, heterogeneous devices and browsers, and the lack of well-established standards. It is essential that studies provide detailed information about their choice of metrics, data sources, and tools to ensure interpretability, comparability, and reproducibility.

For example, Page Load Time (PLT) is a common metric used to estimate user-perceived quality (QoE) and evaluate mechanisms for improving web browsing. Inaccuracies in PLT can lead to skewed results and incorrect conclusions. PLT is often defined as the time until the `onLoad` event, but the start point of the measurement can vary. Initial redirects, such as from `http://example.com` to `https://www.example.com`, can increase PLT. Figure 1 illustrates the discrepancies in PLT with and without initial redirects. According to the W3C Navigation Timings specification, initial redirects should be included in browser timings. However, the occurrence of redirects depends on the starting URL, and the end point of the measurement is not always well-defined. We are not aware of any prior work that quantifies the impact of the exact choice of metric on the measurement results.

The main contributions of this paper are:
1. A survey of web performance studies summarizing the tools, methods, and metrics used. We find that a third of these studies do not provide precise definitions of their metrics and/or data sources.
2. A test environment comparing different tools against a baseline to assess their accuracy. Our results show that the inclusion or exclusion of initial redirects can skew page load times by up to 47% for 10% of the pages. Additionally, object sizes differ from packet traces for more than 60% of objects, affecting derived metrics like the Byte Index.
3. Lessons learned and guidance on increasing the accuracy of measured load times and object sizes. HAR files are the most reliable data source for object counts and sizes, while resource timings often underestimate these metrics. Redirects significantly influence load times, so their inclusion should be a conscious choice.

## 2. Web Metrics and Tools
Typical web metrics include load times, object sizes, number of objects, and page sizes. Each metric has various definitions and data sources, and there are different tools to measure them.

### 2.1 Load Times
Page Load Time (PLT) is a critical metric for user experience. To load a web page, a browser typically loads the base document, parses it, constructs a Document Object Model (DOM), loads referenced objects, processes them, and displays the results. Commonly used load times include:
- **onLoad**: Time until the `onLoad` event.
- **domContentLoaded**: Time until all objects referenced in the base document are loaded.
- **Time to First Paint (TTFP)**: Time until the first content is rendered.
- **Above The Fold Time (AFT)**: Time until the part of the page visible on the user’s screen is fully rendered.

Start times can be `navigationStart`, `fetchStart`, or when the first DNS request or TCP connection is opened.

**Data Sources for Load Times:**
- **Navigation Timings API**: Provides standardized load times.
- **Resource Timings API**: Available from version 1 onward.
- **HTTP Archive (HAR) files**: Include `onLoad` and `domContentLoaded` times.
- **TTFP**: Currently being standardized.
- **AFT**: Not standardized, requires load time data and object positions within the web page.

**Tools:**
- **Browsers**: Most popular browsers implement Navigation Timings and Resource Timings.
- **Browser Plugins**: AFT is realized via a browser plugin available for Chrome.
- **Automation Frameworks**: Selenium and WebPagetest integrate different browser automation frameworks into a single platform.

### 2.2 Number and Size of Objects
Number and sizes of objects are used to estimate the complexity of web pages and compute metrics like Object Index or Byte Index.

**Possible Definitions:**
- **Object Count**: Only include objects loaded until the `onLoad` event.
- **Object Size**: Networking-related studies use the encoded size (bytes transferred over the network), while the decoded size (bytes after decompression) is an alternative. The total page size is the sum of all object sizes.

**Data Sources for Object Sizes:**
- **HAR files**: Provide encoded and decoded body size of each object, including HTTP headers.
- **Resource Timings API**: Includes transfer sizes of header and body.
- **Packet Capture Traces**: Can extract object sizes if elements can be successfully decrypted.

## 3. Survey of Web Studies
We surveyed web performance studies published at SIGCOMM, IMC, PAM, NSDI, and CoNEXT over the last 8 years, including 15 papers. Table 1 summarizes the metrics and data sources used. Many studies use PLT, but the definitions and data sources vary, and often, details are not specified. Only one paper mentions initial redirects, and several compare PLT with other metrics like AFT.

**Table 1: Survey of Web Performance Studies: Metrics and Data Sources**

| Metric            | Definition                          | Data Source          | Papers Using |
|-------------------|-------------------------------------|----------------------|--------------|
| PLT               | Time of `onLoad`                    | Navigation Timings   | 6            |
|                   | Time to load all objects            | Unknown              | 1            |
|                   | Time of `onLoad`                    | HAR file             | 2            |
|                   | Time of `onLoad`                    | HAR file             | 1            |
|                   | Time of `onLoad`                    | DOM time             | 3            |
| AFT               | Visible content rendered            | Resource Timings     | 1            |
| Object Load Times | Time until `responseEnd`            | Resource Timings     | 2            |
| Object Size       | Number of bytes transferred         | HAR file             | 2            |
|                   | Number of bytes transferred         | HAR file             | 1            |
|                   | Number of bytes transferred         | Unknown              | 1            |
| Number of Objects | HTTP request-responses before `onLoad` | Resource Timings | 2            |
|                   | Number of DOM resources             | HAR file             | 2            |

**Table 2: Survey of Web Performance Studies: Browsers and Automation Tools**

| Browser           | Automation Tool | Papers Using |
|-------------------|-----------------|--------------|
| Chrome (desktop)  | DevTools        | 6            |
|                   | Selenium        | 1            |
| Chrome (mobile)   | adb shell       | 1            |
| Firefox (desktop) | Selenium        | 1            |
|                   | Unknown         | 1            |
| phantomJS         | -               | 1            |

## 4. Methodology
To understand the impact of different metrics, we set up a test environment using the following tools:
- **Firefox 61.0.2** with Selenium 3.14.0 and geckodriver 0.21.0
- **Firefox 61.0.2** with Marionette
- **Chrome 69** with Chrome DevTools

We loaded pages from a Thinkpad L450 with Debian Stretch, connected directly to a university network to avoid bandwidth issues. We used a recursive resolver close to our vantage point to minimize DNS caching and delay.

**Figure 1: Page Load Time (PLT) with and without initial redirects.**

**Figure 2: Browser events and timings. See Appendix A for more details.**

This setup allows us to compare the impact of different frameworks and web pages, highlighting the need for more detailed information in web performance studies.