a property and a party whose data exhibits it. This was
also noted by Melis et al. [39] in the federated learning
setting with a small number of parties.
8 Related work
Membership attacks on machine learning models aim
to determine whether a certain record was part of a train-
ing dataset or not [47, 50]. These attacks train shadow
models that are similar to the target model and use their
output (e.g., posterior probabilities over all classes) to
build a meta-classiﬁer that classiﬁes records as members
of the training data or not based on inference results of
the target model on the record in question. A recent link
stealing attack on graphs can be seen as a type of a mem-
bership attack that tries to infer whether two nodes have
a link between them in the training graph [22].
Attribute inference attacks [17, 53], on the other hand,
aim to determine the value of a sensitive attribute for
a single record. For example, the authors of [53] study
leakage of a sensitive value from a latent representation
of a record in the model (i.e., a feature extractor layer);
an attacker can obtain such intermediate record represen-
tations from having access to model parameters. They
show that an attribute of a record, even if censored using
adversarial learning, can be leaked. Hitaj et al [24] show
that a malicious party can construct class representatives
from a model trained in federated learning setting.
The work by Ganju et al. [18] and Ateniese et al. [7]
are closest to ours as they also consider leakage of dataset
properties. Different from this work, their attack is set
in a single-party setting and requires a white-box access
to the model, i.e., its parameters, that may not always
be possible (e.g., when the model access is via cloud-
hosted interface). Since the number of model parameters
in neural networks can be very large (several million),
approaches that are based on sophisticated methods for
reducing network representation are required [18]. We
show that attacks based on a combination of inferences
and logistic regression as a meta-classiﬁer are sufﬁcient
to learn attribute distribution.
Property leakage in a multi-party learning has been
demonstrated only in federated setting [39]. In this set-
ting an attacker obtains a gradient computed on a small
batch of records (e.g., 32) and tries to learn how a sen-
sitive feature is distributed in the batch. This setting is
arguably easier from the attacker point of view: an at-
tacker gains access to a much more granular computation
on the data compared to the access to a query interface
of the ﬁnal model trained on the whole dataset, as consid-
ered in this paper. Moreover, previous work on dataset
property leakage [7, 18, 39] did not consider the case
when the sensitive attribute is removed from the data and
the effect it has on the success of their attacks.
Recently, Zanella-Béguelin et al. [55] have demon-
strated leakage of text and general trends in the data
used to update next word prediction model. Salem et
al. [46], on the other hand, consider granular leakage
about records used to update the model: record labels
and their features. Similar to our work, Salem et al. use a
probing dataset to query the models to obtain the poste-
rior difference. This output is then given to an encoder-
decoder framework to reconstruct the meaning of the
difference between posteriors of the initial and updated
models. Our model update attack, in comparison, is about
identifying the distribution of a sensitive feature in the
dataset used to update the model and requires a simple
machine learning architecture.
9 Conclusion
We demonstrate an attack, set in the centralized multi-
party machine learning, that lets one of the parties learn
sensitive properties about other parties’ data. The attack
requires only black-box access to the model and can ex-
USENIX Association
30th USENIX Security Symposium    2699
tract the distribution of a sensitive attribute with small
number of inference queries. We show that trivial de-
fenses such as excluding a sensitive attribute from train-
ing are insufﬁcient to prevent leakage. Our attack works
on models for tabular, text, and graph data and datasets
that exhibit various correlation relationships among at-
tributes and class labels. Finally, we note that existing
techniques for secure computation and differential pri-
vacy are either not directly applicable to protect leakage
of population-level properties or do so at a high cost.
Acknowledgements
We thank Marcella Hastings and the anonymous re-
viewers for their valuable comments on the paper.
References
[1] Amazon product co-purchasing network metadata.
http://snap.stanford.edu/data/amazon-meta.html.
[2] Azure conﬁdential computing, Microsoft Azure.
https://azure.microsoft.com/en-au/
solutions/confidential-compute.
[3] Kaggle
health
https://www.kaggle.com/c/hhp.
dataset.
[4] Yelp open dataset. https://www.yelp.com/dataset.
[9] G. Cormode. Personal privacy vs population pri-
vacy: Learning to attack anonymization. In Pro-
ceedings of the 17th ACM SIGKDD International
Conference on Knowledge Discovery and Data Min-
ing. Association for Computing Machinery, 2011.
[10] H. Cramér. Mathematical methods of statistics,
volume 43. Princeton university press, 1999.
[11] E. Creager, D. Madras, J.-H. Jacobsen, M. Weis,
K. Swersky, T. Pitassi, and R. Zemel. Flexibly
fair representation learning by disentanglement. In
K. Chaudhuri and R. Salakhutdinov, editors, Inter-
national Conference on Machine Learning (ICML),
volume 97, pages 1436–1445, 2019.
[12] I. Damgård, V. Pastro, N. Smart, and S. Zakarias.
Multiparty computation from somewhat homomor-
In Advances in Cryptology—
phic encryption.
CRYPTO, pages 643–662, Berlin, Heidelberg, 2012.
Springer Berlin Heidelberg.
[13] S. de Hoogh, B. Schoenmakers, P. Chen, and
Practical secure decision
H. op den Akker.
tree learning in a teletreatment application.
In
N. Christin and R. Safavi-Naini, editors, Financial
Cryptography and Data Security, 2014.
[14] S. Dov Gordon, F.-H. Liu, and E. Shi. Constant-
round mpc with fairness and guarantee of output
delivery. In Advances in Cryptology—CRYPTO,
pages 63–82, Berlin, Heidelberg, 2015. Springer
Berlin Heidelberg.
[15] C. Dwork and A. Roth. The algorithmic foun-
dations of differential privacy. Foundations and
Trends in Theoretical Computer Science, 2014.
[16] H. Edwards and A. Storkey. Censoring represen-
tations with an adversary. In International Confer-
ence on Learning Representations (ICLR), 2 2016.
[17] M. Fredrikson, S. Jha, and T. Ristenpart. Model in-
version attacks that exploit conﬁdence information
and basic countermeasures. In ACM Conference
on Computer and Communications Security (CCS),
pages 1322–1333, 2015.
[18] K. Ganju, Q. Wang, W. Yang, C. A. Gunter, and
N. Borisov. Property inference attacks on fully con-
nected neural networks using permutation invariant
representations. In ACM Conference on Computer
and Communications Security (CCS), 2018.
[5] Global
AML
and
Financial
Crime
https://www.fca.
TechSprint.
org.uk/events/techsprints/
2019-global-aml-and-financial-crime-techsprint,
2019. [Online; accessed 25-Jan-2021].
[6] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan,
I. Mironov, K. Talwar, and L. Zhang. Deep learn-
ing with differential privacy. In ACM Conference
on Computer and Communications Security (CCS).
ACM, 2016.
[7] G. Ateniese, L. V. Mancini, A. Spognardi, A. Vil-
lani, D. Vitali, and G. Felici. Hacking smart ma-
chines with smarter ones: How to extract meaning-
ful data from machine learning classiﬁers. Int. J.
Secur. Netw., 2015.
[8] N. Carlini, C. Liu, U. Erlingsson, J. Kos, and
D. Song. The Secret Sharer: Evaluating and Testing
Unintended Memorization in Neural Networks. In
USENIX Security Symposium, 2019.
2700    30th USENIX Security Symposium
USENIX Association
[19] R. Gilad-Bachrach, K. Laine, K. Lauter, P. Rindal,
and M. Rosulek. Secure data exchange: A mar-
ketplace in the cloud. In Proceedings of the 2019
ACM SIGSAC Conference on Cloud Computing Se-
curity Workshop, CCSW’19, page 117–128, New
York, NY, USA, 2019. Association for Computing
Machinery.
[20] T. Graepel, K. Lauter, and M. Naehrig. Ml con-
ﬁdential: Machine learning on encrypted data. In
T. Kwon, M.-K. Lee, and D. Kwon, editors, Informa-
tion Security and Cryptology – ICISC 2012, 2013.
[21] J. Hamm. Preserving privacy of continuous high-
dimensional data with minimax ﬁlters. In Artiﬁcial
Intelligence and Statistics Conference (AISTATS),
2015.
[22] X. He, J.-Y. Jia, M. Backes, N. Gong, and Y. Zhang.
In
Stealing links from graph neural networks.
USENIX Security Symposium, 2020.
[23] I. Higgins, L. Matthey, A. Pal, C. Burgess, X. Glo-
rot, M. M. Botvinick, S. Mohamed, and A. Lerch-
ner. beta-vae: Learning basic visual concepts with a
constrained variational framework. In International
Conference on Learning Representations (ICLR),
2017.
[24] B. Hitaj, G. Ateniese, and F. Perez-Cruz. Deep
Models Under the GAN: Information Leakage from
Collaborative Deep Learning. In ACM Conference
on Computer and Communications Security (CCS),
2017.
[25] Y. Huang, D. Evans, J. Katz, and L. Malka. Faster
secure two-party computation using garbled cir-
cuits. In USENIX Security Symposium, SEC’11,
page 35, USA, 2011.
[26] T. Hunt, C. Song, R. Shokri, V. Shmatikov, and
E. Witchel. Chiron: Privacy-preserving machine
learning as a service. CoRR, abs/1803.05961, 2018.
[27] N. Hynes, R. Cheng, and D. Song. Efﬁcient deep
learning on multi-source private data. CoRR,
abs/1807.06689, 2018.
[28] C. Juvekar, V. Vaikuntanathan, and A. Chan-
drakasan. GAZELLE: A low latency framework
for secure neural network inference. In USENIX
Security Symposium, 2018.
[29] P. Karnati.
Data-in-use
protection
Intel SGX, 2018.
on
IBM cloud using
https://www.ibm.com/cloud/blog/
data-use-protection-ibm-cloud-using-intel-sgx.
[30] D. P. Kingma and J. Ba. Adam: A method
arXiv preprint
for stochastic optimization.
arXiv:1412.6980, 2014.
[31] T. N. Kipf and M. Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. arXiv
preprint arXiv:1609.02907, 2016.
[32] B. Knott, S. Venkataraman, A. Hannun, S. Sengupta,
M. Ibrahim, and L. van der Maaten. Crypten: Se-
cure multi-party computation meets machine learn-
ing. In Proceedings of the NeurIPS Workshop on
Privacy-Preserving Machine Learning, 2020.
[33] R. Kohavi. Scaling up the accuracy of Naive-Bayes
classiﬁers: A decision-tree hybrid. In Proceedings
of the Second International Conference on Knowl-
edge Discovery and Data Mining, KDD’96, page
202–207. AAAI Press, 1996.
[34] S. Laur, H. Lipmaa, and T. Mielikäinen. Crypto-
graphically private support vector machines.
In
Proceedings of the 12th ACM SIGKDD Interna-
tional Conference on Knowledge Discovery and
Data Mining, 2006.
[35] J. Leskovec, L. A. Adamic, and B. A. Huberman.
The dynamics of viral marketing. ACM Transac-
tions on the Web (TWEB), 1(1):5–es, 2007.
[36] J. Leskovec, K. J. Lang, A. Dasgupta, and M. W.
Mahoney. Community structure in large networks:
Natural cluster sizes and the absence of large well-
deﬁned clusters. Internet Mathematics, 2009.
[37] M. Lichman et al. UCI machine learning repository,
2013.
[38] F. Locatello, G. Abbati, T. Rainforth, S. Bauer,
B. Schölkopf, and O. Bachem. On the fairness of
disentangled representations. In Advances in Neu-
ral Information Processing Systems, pages 14584–
14597, 2019.
[39] L. Melis, C. Song, E. D. Cristofaro, and
Exploiting unintended fea-
V. Shmatikov.
In IEEE
ture leakage in collaborative learning.
Symposium on Security and Privacy (S&P), 2019.
[40] P. Mohassel and Y. Zhang. SecureML: a system for
scalable privacy-preserving machine learning. In
IEEE Symposium on Security and Privacy (S&P),
pages 19–38, 2017.
USENIX Association
30th USENIX Security Symposium    2701
[41] V. Nikolaenko, S. Ioannidis, U. Weinsberg, M. Joye,
N. Taft, and D. Boneh. Privacy-preserving matrix
factorization. In ACM Conference on Computer and
Communications Security (CCS), page 801–812,
New York, NY, USA, 2013. Association for Com-
puting Machinery.
[42] O. Ohrimenko, F. Schuster, C. Fournet, A. Mehta,
S. Nowozin, K. Vaswani, and M. Costa. Oblivious
multi-party machine learning on trusted processors.
In USENIX Security Symposium, 2016.
[43] R. Pass, E. Shi, and F. Tramèr. Formal abstractions
In Ad-
for attested execution secure processors.
vances in Cryptology - EUROCRYPT 2017 - 36th
Annual International Conference on the Theory and
Applications of Cryptographic Techniques, 2017.
[44] K. Pearson. Vii. note on regression and inheritance
in the case of two parents. proceedings of the royal
society of London, 58(347-352):240–242, 1895.
[45] T. Rabin and M. Ben-Or. Veriﬁable secret sharing
and multiparty protocols with honest majority. In
ACM Symposium on Theory of Computing (STOC),
STOC ’89, pages 73–85, New York, NY, USA, 1989.
Association for Computing Machinery.
[46] A. Salem, A. Bhattacharya, M. Backes, M. Fritz,
and Y. Zhang. Updates-leak: Data set inference
and reconstruction attacks in online learning. In
S. Capkun and F. Roesner, editors, USENIX Secu-
rity Symposium, 2020.
[47] A. Salem, Y. Zhang, M. Humbert, P. Berrang,
M. Fritz, and M. Backes. ML-leaks: Model and
data independent membership inference attacks and
defenses on machine learning models. In Sympo-
sium on Network and Distributed System Security
(NDSS), 2019.
[48] Microsoft SEAL (release 3.6). https://github.
com/Microsoft/SEAL, Nov. 2020. Microsoft Re-
search, Redmond, WA.
[49] D. J. Sheskin. Handbook of parametric and non-
parametric statistical procedures. crc Press, 2020.
[50] R. Shokri, M. Stronati, C. Song, and V. Shmatikov.
Membership inference attacks against machine
learning models. In IEEE Symposium on Security
and Privacy (S&P), 2017.
[51] C. Song, T. Ristenpart, and V. Shmatikov. Machine
learning models that remember too much. In ACM
Conference on Computer and Communications Se-
curity (CCS), New York, NY, USA, 2017. Associa-
tion for Computing Machinery.
[52] C. Song and V. Shmatikov. Auditing data prove-
nance in text-generation models. In International
Conference on Knowledge Discovery & Data Min-
ing (KDD), pages 196–206. ACM, 2019.
[53] C. Song and V. Shmatikov. Overlearning reveals