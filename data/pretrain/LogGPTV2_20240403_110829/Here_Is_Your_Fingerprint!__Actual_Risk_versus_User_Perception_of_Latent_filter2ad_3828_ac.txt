for image post-processing. Figure 3-(vii) shows the result of com-
bination. Note that the combination method can be generalized to
utilize more than one exactly matched Ti images.
s in Ts can also be utilized if more quali(cid:128)ed.
in T (cid:48)
Algorithm 1: Partial image cropping
Input: descriptorH , descriptorTs
, Ts
Output: T (cid:48)
s
1 L home = (0, 0) − descriptorH ;
2 R home = (250, 250) − descriptorH ;
+ L home;
3 L screen = descriptorTs
4 R screen = descriptorTs
+ R home;
5 Crop(L screen, R screen);
Algorithm 2: (cid:139)ality assessment & decision
Input: nH , nT (cid:48)
Output: replace method, combination method
, nH inner , bH inner
s
s ≥ nH
return replace method;
1 if nT (cid:48)
2
3 else if nH > nT (cid:48)
4
5
6
7
s
if nH inner/bH inner ≥ 0.6
return combination method;
else if nH inner/bH inner < 0.6
return replace method;
Algorithm 3: Combination method
Input: descriptorH , descriptorTs
Output: H(cid:48) (overlapping Hd with corresponding blocks of T (cid:48)
s )
1 blocksize = 25;
2 for each block Bi, j ∈ Hd do
, (cid:104)Hd , Hu(cid:105), T (cid:48)
s
L homei, j = L(Bi, j) − descriptorH ;
R homei, j = R(Bi, j) + blocksize − descriptorH ;
L screeni, j = descriptorTs
R screeni, j = descriptorTs
Overlap(L(Bi, j), R(Bi, j), L screeni, j , R screeni, j);
+ L homei, j;
+ R homei, j;
3
4
5
6
7
8 end
3.6 Veri(cid:128)cation
We verify the (cid:128)ngerprint images reconstructed by the SCRAP at-
tack, in so(cid:137)ware. Indeed, we employ the NBIS MINDTCT and BO-
ZORTH3 packages for measuring (cid:128)ngerprint image quality, minu-
tiae quality, and match scores. We see this as a reasonable method
of validation for the purpose of our paper. Note that actual forgery
a(cid:138)acks are covered in our separate follow-up work [26].
To measure match scores and also for comparison purposes, we
prepared template images, which mean a set of (cid:128)rmly impressed
(cid:128)ngerprint images in good quality. (cid:140)e (cid:128)rm (cid:128)ngerprints were taken
using a touch screen that was cleaned with alcohol, and cut into the
small size (See below). (cid:140)ree templates in small size show match
scores between 21 and 32 (Mdn = 24) in cross validation, as shown
in Figure 5. Note that such template scores may vary depending
the nature of the (cid:128)ngerprint each user has, while a match score
of greater than or equal to 40 usually indicates a true match for
the large full (cid:128)ngerprint [40]. (cid:140)us, we use the median value of
each user’s template scores as a reference point of success in our
experiment. We see this as a reasonable method of measurement for
a partial limited portion of the (cid:128)ngerprint. In Figure 5, the hatched
area, that is above the reference point, indicates a success, i.e., our
(cid:128)ngerprint images were reconstructed in good quality.
To consider the actual area required for small touch sensors, we
dealt with the case of the second generation Touch ID, larger than
the other related. (cid:140)erefore, we cut the reconstructed (cid:128)ngerprint
images and the template images into a circle shape according to
the size of the Touch ID (200 × 200 pixel), and used these to verify
image quality. Each image was handled as a 250 × 250 pixel image
(cid:128)lled with white color in the background, as shown in Figure 6.
517Figure 5: Veri(cid:128)cation of reconstructed (cid:128)ngerprint images – Minutiae quality spectrum; Match scores; NFIQ results. Note that
the match scores of the template images (Mdn = 24, max = 32) replace the common reference score 40 because the template
images can be used to pass the small touch sensors. (cid:135)e hatched area, which was determined by the quality of the template
images, indicates that the corresponding image has su(cid:129)cient quality for the small touch sensors in that sense.
Figure 6: Fingerprint images in our example experiment. (a)
Latent (cid:128)ngerprint image taken from home button. (b) Out-
put image of combination method. (c) Output image of re-
placement method. (d) Flipped and reversed image of (c). (e)
Ridge emphasized image of (d) for adversarial submission.
4 ACTUAL RISK: ATTACK EXPERIMENTS
To understand actual risk of our a(cid:138)ack in realistic scenarios, we
conduct an empirical study participated by users.
4.1 Methodology
We used iPhone 6 for Touch ID users in our study. We set the privacy
policy to remove and never reuse user data a(cid:137)er this study.
Study Design. We set six conditions for the simulation of realistic
scenarios. We asked participants to enroll at least one (cid:128)ngerprint
for Touch ID, and perform each scenario in sequence. Before they
start each task, we asked them to lightly rub iPhone with a cloth.
(1) Tapping — An ideal condition that a user taps on a touch
screen. We asked participants to tap apps and keys for one minute.
(2) Passcode-typing — A casual use case that a user types a pass-
code using a virtual keypad in the middle of touch screen. We asked
participants to type a random passcode, not a real passcode.
(3) Text-typing — Another use case that a user types text messages
using a virtual keyboard in the bo(cid:138)om of touch screen. We asked
participants to type any text they want for two minutes.
(4) Facebook — An active use case that a user taps, swipes, and
types on a touch screen. We asked to use Facebook for two minutes.
(5) In-pocket — A harsh condition that a user puts a smartphone
in a pocket. We asked participants to put iPhone in a back pocket
and walk around eight meters a(cid:137)er using it for one minute.
(6) Wiping — Another harsh condition that a user intentionally
wipes a touch screen. We asked to wipe iPhone a(cid:137)er using it for
one minute, as they have done in daily use.
When a participant completed a single task, we collected latent
(cid:128)ngerprints in another room, i.e., performed the latent (cid:128)ngerprint
collection step in our photographic setup, and returned iPhone
for the following task. We did not show the photographic setup
to participants to avoid preconception. (cid:140)e collected images were
“temporarily” stored in our local computer for further data analysis.
When the participant completed all tasks, we removed the en-
rolled (cid:128)ngerprint from Touch ID, and polished iPhone with alcohol
to ask them to impress (cid:128)rm (cid:128)ngerprints of the enrolled (cid:128)nger on a
clean touch screen, repeatedly for three times, for later-on veri(cid:128)ca-
tion of constructed (cid:128)ngerprints. We then collected the (cid:128)ngerprints
in our photographic setup, and set these as a template image.
We proceeded with the remaining steps of the SCRAP a(cid:138)ack,
and used the template images for veri(cid:128)cation as in Section 3.6.
4.2 Results
We recruited seven participants (three female; right-handed; av-
erage age 26; SD = 2.71) in a local university a(cid:137)er an approval of
the IRB, and explained our study and privacy policy to which all
participants consented. Each participant spent about 30 minutes
for experiments and we compensated them with a small gratuity.
Latent Fingerprint Collection. Overall, we collected 403 latent
(cid:128)ngerprint images (361 on a touch screen and 42 on a home bu(cid:138)on)
from 42 user tasks (seven users in six conditions). Figure 15 in Ap-
pendix E shows examples of conditions. Table 2 shows the number
of retrieved latent (cid:128)ngerprint images taken from touch screen per
condition. We observed that the Tapping condition produced the
most (73, Mean = 10.43, SD = 2.44) while the In-pocket condition
showed the least (42, Mean = 6, SD = 3.06).
We used a one-way ANOVA test to compare the average number
of latent (cid:128)ngerprint images for conditions. (cid:140)ere were signi(cid:128)cant
di(cid:130)erences across the six conditions (F(5, 41) = 3.549, p < .05). By
Fischer LSD post-hoc test, we could observe signi(cid:128)cant di(cid:130)erences
518Figure 7: Scoring results. (cid:88) means M(R) < M(T) ≤ M(C), i.e., the actual attack is very likely to succeed. ∗ means M(T) < M(R, C).
Table 2: Number of latent (cid:128)ngerprints per condition.
Conditions
Tapping
Passcode-typing
Text-typing
Facebook
In-pocket
Wiping
Total Mean SD Max Min
8
7
7
7
1
1
10.43
9.71
9.57
9.57
6
6.29
73
68
67
67
42
44
2.44
2.14
2.99
2.3
3.06
3.15
15
13
16
14
9
10
between the group of Tapping, Passcode-typing, Text-typing, Face-
book and the group of In-pocket, Wiping (p < .05, respectively).
(cid:140)e In-pocket and Wiping conditions are more likely to damage
latent (cid:128)ngerprints by pants, palm, or (cid:128)nger than other condition
group. We consider this can explain the result of ANOVA tests.
Constructed Fingerprint Veri(cid:128)cation. We computed 42 sets of
match scores (six conditions × seven participants) for each (cid:128)nger-
print image. Figure 7 depicts the results of 42 cases. M(x) denotes the
match score of x for Raw-captured, Template, and reConstructed
(cid:128)ngerprint images based on the regulation we used in Section 3.6. A
success is clearly marked with (cid:88) for the case that M(R) < M(T) ≤
M(C). It was interesting to see that M(R) was fairly higher than
M(T) for some participants, e.g., a tapping case of P6. (cid:140)is may be
related to pressing the home bu(cid:138)on with force. If M(T) < M(C),
then we clearly marked with (cid:88)∗ for such a case and considered it
a success too. It was also interesting to see that M(T) as a median
value of some participants were signi(cid:128)cantly lower (10.5, 14, 13,
13.5 respectively in the order of P4, P5, P6, P7). In this case, if the
threshold value is used as it is, there is a concern that the a(cid:138)ack
performance could be overestimated. (cid:140)erefore, we replaced M(T)
with the maximum value (22, 22, 19, 23) instead of the median value
to end such a concern and increase τ. In some cases, M(C) was
lower than M(R), e.g., P2, P5 with in-pocket. (cid:140)is implies that latent
(cid:128)ngerprints taken from touch screen slightly worsen the result, and
therefore it suggests an improvement in the future study.
(cid:140)e highest success rate was observed in Tapping and Passcode-
typing condition (85.7%) while the lowest was in Text-typing (42.9%)
in our experiments. Although not all cases were successful, the
results indicate our a(cid:138)ack is actual risk in realistic scenarios.
4.3 Limitations
Our sample was small (n = 7) and so was the latent (cid:128)ngerprints
(m = 403). (cid:140)is limitation happened because we aimed at pho-
tographing smudges (including latent (cid:128)ngerprints) with partici-
pants’ consent. Due to the small sample size, we were not able to
consider EER (equal error rate) and instead we directly compared
match scores. (cid:140)us, we had to experience a template dependency.
5 USER PERCEPTION: IN-PERSON SURVEYS
Regarding the empirical studies of the SCRAP a(cid:138)ack, we need to
gain the insight about user’s touch behavior, e.g., whether users
use the same (cid:128)ngers on both Touch ID and touch screen, and how
users perceive Touch ID and the latent (cid:128)ngerprints, in daily use.
5.1 Methodology
To understand user’s behavioral practice and perception gap, we
decided to conduct an in-person survey. (cid:140)is choice allowed us to
validate the participants’ answers immediately with follow-up com-
munications and also expect unforeseen answers with additional
questions and more explanations. A(cid:137)er an approval of the IRB, we
made e(cid:130)orts to recruit diverse participants in a local university,
co(cid:130)ee shops, and shopping malls through message boards and (cid:131)y-
ers distribution from July 1 to November 30, 2016. All participants
were given a small gratuity ($10) for their participation.
Study Design. We conducted paper surveys in person (followed
by further communications and selective interviews if necessary)
and collected participants’ answers anonymously. (cid:140)e collected
answers were later stored in our local computer for further data
analysis. Our survey questionnaires consist of the following parts:
Part A: Basic questions about iPhone experience and brief per-
ception regarding security and usability of Touch ID in comparison
with other unlocking methods (e.g., passcode) o(cid:130)ered by iPhone,
e.g., we asked participants how long they have used iPhone and
what unlocking method was easier to use and felt more secure.
Part B1: Main questions about the Touch ID experience, e.g., we
asked how they perceive Touch ID regarding security and usability.
Part B2: Main questions about the behavioral practice with (cid:128)n-
gers on Touch ID and touch screen. e.g., we asked participants
about (cid:128)nger enrollment for Touch ID and frequent (cid:128)nger usages on
519both Touch ID and touch screens (for Passcode typing, App selec-
tion, Text typing, and Swiping). We also asked about participants’
perception of latent (cid:128)ngerprints.
Part C: Demographic questions about age, gender, handedness,
education, and occupation.
For validation of participants’ answers, we asked participants