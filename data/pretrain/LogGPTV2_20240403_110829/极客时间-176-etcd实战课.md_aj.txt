# 查看lease的TTL、剩余时间    $ etcdctl lease timetolive 326975935f48f814    lease 326975935f48f814 granted with TTL(600s)， remaining(590s)当 Lease server 收到 client 的创建一个有效期 600 秒的 Lease请求后，会通过 Raft 模块完成日志同步，随后 Apply 模块通过 Lessor 模块的Grant 接口执行日志条目内容。首先 Lessor 的 Grant 接口会把 Lease 保存到内存的 ItemMap数据结构中，然后它需要持久化 Lease，将 Lease 数据保存到 boltdb 的 Leasebucket 中，返回一个唯一的 LeaseID 给client。 通过这样一个流程，就基本完成了 Lease的创建。那么节点的健康指标数据如何关联到此 Lease上呢？ 很简单，KV 模块的 API接口提供了一个\"\--lease\"参数，你可以通过如下命令，将 key node关联到对应的 LeaseID 上。然后你查询的时候增加 -w 参数输出格式为json，就可查看到 key 关联的LeaseID。     $ etcdctl put node healthy --lease 326975935f48f818    OK    $ etcdctl get node -w=json | python -m json.tool    {        "kvs":[            {                "create_revision":24，                "key":"bm9kZQ=="，                "Lease":3632563850270275608，                "mod_revision":24，                "value":"aGVhbHRoeQ=="，                "version":1            }            }以上流程原理如下图所示，它描述了用户的 key 是如何与指定 Lease关联的。当你通过 put 等命令新增一个指定了\"\--lease\"的 key 时，MVCC模块它会通过 Lessor 模块的 Attach 方法，将 key 关联到 Lease 的 key内存集合 ItemSet 中。![](Images/277cd6e078e5bf7e9fd4a8e72d79189d.png)savepage-src="https://static001.geekbang.org/resource/image/aa/ee/aaf8bf5c3841a641f8c51fcc34ac67ee.png"}一个 Lease 关联的 key 集合是保存在内存中的，那么 etcd重启时，是如何知道每个 Lease 上关联了哪些 key呢? 答案是 etcd 的 MVCC 模块在持久化存储 key-value 的时候，保存到 boltdb的 value 是个结构体（mvccpb.KeyValue）， 它不仅包含你的 key-value数据，还包含了关联的 LeaseID 等信息。因此当 etcd重启时，可根据此信息，重建关联各个 Lease 的 key集合列表。 如何优化 Lease 续期性能通过以上流程，我们完成了 Lease创建和数据关联操作。在正常情况下，你的节点存活时，需要定期发送 KeepAlive请求给 etcd 续期健康状态的 Lease，否则你的 Lease和关联的数据就会被删除。那么 Lease 是如何续期的? 作为一个高频率的请求 API，etcd 如何优化Lease 续期的性能呢？Lease 续期其实很简单，核心是将 Lease 的过期时间更新为当前系统时间加其TTL。关键问题在于续期的性能能否满足业务诉求。然而影响续期性能因素又是源自多方面的。首先是 TTL，TTL过长会导致节点异常后，无法及时从 etcd中删除，影响服务可用性，而过短，则要求 client 频繁发送续期请求。其次是Lease 数，如果 Lease 成千上万个，那么 etcd 可能无法支撑如此大规模的Lease 数，导致高负载。如何解决呢？首先我们回顾下早期 etcd v2 版本是如何实现 TTL 特性的。在早期 v2版本中，没有 Lease 概念，TTL 属性是在 key 上面，为了保证 key不删除，即便你的 TTL 相同，client 也需要为每个 TTL、key 创建一个HTTP/1.x 连接，定时发送续期请求给 etcdserver。 很显然，v2 老版本这种设计，因不支持连接多路复用、相同 TTL无法复用导致性能较差，无法支撑较大规模的 Lease场景。 etcd v3 版本为了解决以上问题，提出了 Lease 特性，TTL 属性转移到了Lease 上， 同时协议从 HTTP/1.x 优化成 gRPC协议。 一方面不同 key 若 TTL 相同，可复用同一个 Lease， 显著减少了 Lease数。另一方面，通过 gRPC HTTP/2实现了多路复用，流式传输，同一连接可支持为多个 Lease续期，大大减少了连接数。通过以上两个优化，实现 Lease性能大幅提升，满足了各个业务场景诉求。如何高效淘汰过期 Lease在了解完节点正常情况下的 Lease续期特性后，我们再看看节点异常时，未正常续期后，etcd 又是如何淘汰过期Lease、删除节点健康指标 key的。 淘汰过期 Lease 的工作由 Lessor 模块的一个异步 goroutine负责。如下面架构图虚线框所示，它会定时从最小堆中取出已过期的Lease，执行删除 Lease 和其关联的 key 列表数据的 RevokeExpiredLease任务。 ![](Images/77d50e30ccc96f08444bf6484d724910.png)savepage-src="https://static001.geekbang.org/resource/image/b0/6b/b09e9d30157876b031ed206391698c6b.png"}从图中你可以看到，目前 etcd 是基于最小堆来管理Lease，实现快速淘汰过期的Lease。 etcd 早期的时候，淘汰 Lease 非常暴力。etcd 会直接遍历所有Lease，逐个检查 Lease 是否过期，过期则从 Lease 关联的 key 集合中，取出key 列表，删除它们，时间复杂度是O(N)。 然而这种方案随着 Lease数增大，毫无疑问它的性能会变得越来越差。我们能否按过期时间排序呢？这样每次只需轮询、检查排在前面的Lease 过期时间，一旦轮询到未过期的 Lease，则可结束本轮检查。刚刚说的就是 etcd Lease 高效淘汰方案最小堆的实现方法。每次新增Lease、续期的时候，它会插入、更新一个对象到最小堆中，对象含有 LeaseID和其到期时间unixnano，对象之间按到期时间升序排序。etcd Lessor 主循环每隔 500ms 执行一次撤销 Lease检查（RevokeExpiredLease），每次轮询堆顶的元素，若已过期则加入到待淘汰列表，直到堆顶的Lease过期时间大于当前，则结束本轮轮询。相比早期 O(N)的遍历时间复杂度，使用堆后，插入、更新、删除，它的时间复杂度是 O(LogN)，查询堆顶对象是否过期时间复杂度仅为O(1)，性能大大提升，可支撑大规模场景下 Lease的高效淘汰。获取到待过期的 LeaseID 后，Leader 是如何通知其他 Follower节点淘汰它们呢？Lessor 模块会将已确认过期的 LeaseID，保存在一个名为 expiredC 的channel 中，而 etcd server 的主循环会定期从 channel 中获取 LeaseID，发起revoke 请求，通过 Raft Log 传递给 Follower节点。 各个节点收到 revoke Lease 请求后，获取关联到此 Lease 上的 key列表，从 boltdb 中删除 key，从 Lessor 的 Lease map 内存中删除此 Lease对象，最后还需要从 boltdb 的 Lease bucket 中删除这个Lease。 以上就是 Lease 的过期自动淘汰逻辑。Leader节点按过期时间维护了一个最小堆，若你的节点异常未正常续期，那么随着时间消逝，对应的Lease 则会过期，Lessor 主循环定时轮询过期的 Lease。获取到 ID 后，Leader发起 revoke 操作，通知整个集群删除 Lease和关联的数据。为什么需要 checkpoint 机制了解完 Lease 的创建、续期、自动淘汰机制后，你可能已经发现，检查 Lease是否过期、维护最小堆、针对过期的 Lease 发起 revoke 操作，都是 Leader节点负责的，它类似于 Lease 的仲裁者，通过以上清晰的权责划分，降低了Lease 特性的实现复杂度。那么当 Leader 因重启、crash、磁盘 IO 等异常不可用时，Follower节点就会发起 Leader 选举，新 Leader 要完成以上职责，必须重建 Lease过期最小堆等管理数据结构，那么以上重建可能会触发什么问题呢？当你的集群发生 Leader 切换后，新的 Leader 基于 Lease map 信息，按Lease 过期时间构建一个最小堆时，etcd早期版本为了优化性能，并未持久化存储 Lease 剩余 TTL信息，因此重建的时候就会自动给所有 Lease自动续期了。然而若较频繁出现 Leader 切换，切换时间小于 Lease 的 TTL，这会导致Lease 永远无法删除，大量 key 堆积，db大小超过配额等异常。为了解决这个问题，etcd引入了检查点机制，也就是下面架构图中黑色虚线框所示的CheckPointScheduledLeases的任务。 ![](Images/f685f8ce8c5d1c81d201e419e736a58c.png)savepage-src="https://static001.geekbang.org/resource/image/70/59/70ece2fa3bc400edd8d3b09f752ea759.png"}一方面，etcd 启动的时候，Leader节点后台会运行此异步任务，定期批量地将 Lease 剩余的 TTL 基于 Raft Log同步给 Follower 节点，Follower 节点收到 CheckPoint请求后，更新内存数据结构 LeaseMap 的剩余 TTL信息。 另一方面，当 Leader 节点收到 KeepAlive 请求的时候，它也会通过checkpoint 机制把此 Lease 的剩余 TTL 重置，并同步给 Follower节点，尽量确保续期后集群各个节点的 Lease 剩余 TTL一致性。 最后你要注意的是，此特性对性能有一定影响，目前仍然是试验特性。你可以通过experimental-enable-lent参数开启。 小结最后我们来小结下今天的内容，我通过一个实际案例为你解读了 Lease创建、关联 key、续期、淘汰、checkpoint机制。 Lease 的核心是 TTL，当 Lease 的 TTL 过期时，它会自动删除其关联的key-value 数据。首先是 Lease 创建及续期。当你创建 Lease 时，etcd 会保存 Lease 信息到boltdb 的 Lease bucket 中。为了防止 Lease 被淘汰，你需要定期发送LeaseKeepAlive 请求给 etcd server 续期 Lease，本质是更新 Lease的到期时间。续期的核心挑战是性能，etcd 经历了从 TTL 属性在 key 上，到独立抽象出Lease，支持多 key 复用相同 TTL，同时协议从 HTTP/1.x 优化成 gRPC协议，支持多路连接复用，显著降低了 server连接数等资源开销。其次是 Lease 的淘汰机制，etcd 的 Lease 淘汰算法经历了从时间复杂度O(N) 到 O(Log N) 的演进，核心是轮询最小堆的 Lease 是否过期，若过期生成revoke 请求，它会清理 Lease和其关联的数据。最后我给你介绍了 Lease 的 checkpoint 机制，它是为了解决 Leader异常情况下 TTL 自动被续期，可能导致 Lease永不淘汰的问题而诞生。思考题好了，这节课到这里也就结束了，我最后给你留了一个思考题。你知道 etcdlease 最小的 TTL时间是多少吗？它跟什么因素有关呢？感谢你的阅读，如果你认为这节课的内容有收获，也欢迎把它分享给你的朋友，谢谢。