Tracker
Fetcher
MTTFetcher
Writer
MTTWriter
Distributor
Gnt. Distributor
Fig. 2: A simpliﬁed diagram of DFITAGGER on a Rocket Chip.
and parameters, e.g., different optimization techniques and
conﬁguration parameters.
1) ISA Extensions: Following the design pattern of RISC-V,
we assign a new opcode to our new instructions that is similar
to the RV64I load/store instructions [76].
sdset1: We extend the memory request unit’s data ﬁeld by
one-bit to include the tag. To determine whether the tag should
be 0 or 1, we introduce a new conﬁguration to the set of control
signals for memory command type that is unique to sdset1.
ldchkx: We add a new, one-bit ﬁeld to the memory response
unit for the tag bit loaded with the machine word. To determine
whether the tag bit should be loaded, we assigned a new
memory command to these two instructions. Upon a valid
response from cache, HDFI compares the tag to the expected
value. This expected value is extracted from bit 12 of the
ldchkx instruction. A tag mismatch generates a new memory
exception; otherwise, the pipeline continues normally.
mvwtag: At the execution stage, HDFI ﬁrst calculates the
source address from the second register’s value and the
immediate offset using the ALU, and sends out a memory
read request to load the data and tag. The result is stored in a
new internal register that is capable of storing both data and
tag. Simultaneously, HDFI calculates the destination address
from the destination register’s value and the same offset using
a separate adder. Finally, we issue a memory store request
to store the internal register’s data and tag to the destination
address.
2) DFITAGGER: To avoid adding the tag bits physically
to the main memory, which is usually a set of DRAMs, we
implemented DFITAGGER to translate memory accesses with
tags from inner caches into data accesses and tag accesses.
Figure 2 shows the DFITAGGER we implemented for the Rocket
Chip. The DFITAGGER is designed to handle the memory
accesses that comply with the TileLink protocol which the
rocket chip uses to implement the cache coherence interconnect.
Among the ﬁve channels that the protocol deﬁnes, DFITAGGER
handles two of them because they are used to connect the L2
caches and the outer memory system.
To initiate a memory access, the inner cache generates one or
more beats of transaction through the Acquire channel, and the
DFITAGGER selectively intercepts the beats using the Acquire
Distributor. When the option tagger is enabled, the Acquire
Distributor bypasses the device accesses, drops the access
to the tag table or meta tag table (for protection purpose),
and forwards all the transactions heading to the memory
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:03 UTC from IEEE Xplore.  Restrictions apply. 
to the Acquire Queue, which simply forwards the incoming
transactions to memory. The Acquire Arbiter1 drops all the tag
bits in the transactions; the resulting memory accesses only
contain the data part of the incoming accesses.
In the mean time, the Tracker duplicates the required ﬁeld
of incoming transactions, including the tag bits, the transaction
id, the type of the transaction, and the address. When the
incoming transaction writes to memory, the Tracker updates
the corresponding tag bits in the tag table with the tag bits
in the transaction. To do this, the Tracker ﬁrst check the Tag
Cache, and uses the Fetcher and the Writer modules to fetch
and evict tag table entries.
Handling memory read accesses is similar, but the Tracker
need to intervene in the Grant channel as well. In the Rocket
Chip, the memory interface (which is a protocol converter)
uses the Grant channel to provide the caches with the read data.
To attach the tag bits to the Grant transactions, the Acquire
Queue changes the transaction id of read accesses so that the
corresponding Grant transactions are forwarded to the Grant
Queue. In the mean time, the Tracker accesses the Tag Cache
and uses other modules to prepare the corresponding tag bits.
Once the tag bits become available, the Grant Queue forwards
the transaction from the memory interface, after changing the
transaction id back to the original one and attaching the correct
tag bits for each machine word.
3) Tag Valid Bits: To reduce the number of tag table accesses,
HDFI adds a TVB for each machine word in the caches. Using
TVB, the cache can avoid fetching the tag bits when it reﬁlls
a cache line. To take advantage of this, the cache uses the
union ﬁeld of an Acquire transaction to mark if the response to
the transaction should have valid tag bits or not. The Acquire
Distributor then uses this ﬁeld to decide whether a transaction
could be directly forwarded to the Acquire Arbiter2 and bypass
the Acquire Queue.
The location of TVBs is also important. A simple solution is
to put the TVBs in the metadata array, where the cache holds
the cache tags and the coherence information. However, this
approach would increase the latency of write hits because the
cache has to update the metadata for every write operation. To
address this issue, we choose to put a tag fetched bit in the
metadata array for each cache line and extend the size of the
data array to store the TVBs for each word. The tag fetched
bit is set/cleared by the miss handler, which is called MSHR in
the Rocket Chip. When the handler fetches the cache line with
tags, it sets the bit; otherwise the bit is cleared. Since every
write operation should update the tag, the cache also sets the
TVB whenever a machine word is written.
Adding TVBs also requires the DFITAGGER to consider a
memory write access whose tag bits are partially valid. To
handle this, the cache attaches the TVBs for each machine
word to the Acquire transactions for memory writes. With the
TVBs, the DFITAGGER can selectively update the tag bits in
the corresponding tag table entry.
An important drawback of this implementation is that the
cache reﬁlls a cache line to handle an incoming load with tag
access even when the TVB of the requested machine word is
set, but if the Tag Fetched Bit is not set. We believe that we
can avoid these cache reﬁlls by augmenting the miss handler,
by letting it to consider the TVBs before evicting and reﬁlling
the cache, but the current implementation does not include
such feature.
4) Meta Tag Table: Enabling the Meta Tag Table adds the
shaded components and resource in Figure 2 to the DFITAGGER.
When handling an incoming tag table read access, the Tracker
checks whether the MTT cache and the tag cache has a
matching entry. If the Tracker fails to ﬁnd a matching tag table
entry, it checks the MTD and the matching MTT entry (loaded
into MTT cache if does not exist) to see if the corresponding tag
table entry is all zero. If so, the Tracker handle the incoming
tag table access without really fetching the entry from the
memory. To minimize the miss penalty, the MTTFetcher and
the MTTWriter handles the access to the MTT in the memory
in parallel with the existing Writer and Fetcher.
After updating the tag table entry and the MTT entry, the
Tracker checks if it can clear the corresponding MTT entry bit
and MTD bit. In particular, the Tracker clears the corresponding
bit in MTT entry if the updated tag table entry is ﬁlled with
zeros, and clears the MTD bit if the MTT entry is ﬁlled with
zeros.
B. Software Support
To utilize HDFI, we made the following changes to the
software.
1) Assembler: We modiﬁed the GNU assembler (gas) so that
it recognizes the new instruction extension and can generate
the correct binary.
2) Kernel Support: Our modiﬁcations to the OS kernel
include three parts. First, we modiﬁed its exception handler
to recognize the new tag mismatch exception. To handle this
exception, we reused the same logic as normal load/store
faults, i.e., generate a segment fault (SIGSEGV) for user mode
applications, and panic if the exception happens in kernel
space. Second, as mentioned in §IV, we implemented a special
memory copy routine with the new mvwtag instruction and
modiﬁed the CoW handler to invoke this routine to copy page
content, so that the tag information are preserved. Last, we
added routines to allocate the tag table and meta tag table,
and initialize the DFITAGGER with the base addresses of the
tables.
C. Security Applications
Most security applications mentioned in §V were imple-
mented based on the llvm-riscv toolchain [61] (RISCV branch).
Table III summarizes the effort of implementation/porting.
1) LLVM Shadow Stack: LLVM-based shadow stack is
implemented as part of the frame lowering process. Speciﬁcally,
we modiﬁed the getLoadStoreOpcodes function to return the
opcode of sdset1 for the storeRegToStackSlot function; and
return the opcode of ldchk1 for the loadRegFromStackSlot
function.
99
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:03 UTC from IEEE Xplore.  Restrictions apply. 
Solutions
Shadow Stack
VTable Protection
CPS
Kernel Protection
Library Protection
Heartbleed Prevention C (OpenSSL 1.0.1a)
Language
C++ (LLVM 3.3)
C++ (LLVM 3.3)
C++ (LLVM 3.3)
C (Linux 3.14.41)
C (glibc 2.22)
LoC
4
40
41
70
10
2
TABLE III: Required efforts in implementing or porting security
schemes in terms of lines of code. Given a software-based solution,
HDFI is easy to adopt or extend in practice.
2) VTable Pointer Protection: VTable pointer protection
is implemented in two steps. First, during compilation, we
enable the TBAA (type-based alias analysis) option so Clang
will annotate VTable load/store operations with corresponding
TBAA metadata (“vtable pointer”). This metadata will be
propagated to machine instruction, so in the second step, we
leveraged the DAG to DAG transformation pass to replace
sd instructions with sdset1 instructions, and to replace ld
instructions with ldchk1 instructions, if the machine instruction
has the corresponding TBAA of VTables.
3) Code Pointer Separation: To port CPS [44] to our
architecture, we performed the following modiﬁcations. (1)
Because code pointers are now protected by HDFI, we removed
the runtime library required by its original implementation. (2)
We modiﬁed the instrumentation, so when a code pointer is
stored to or loaded from memory, we annotate the correspond-
ing operations with a special TBAA metadata and removes
the original invocation to the runtime library. (3) Using the
same DAG to DAG transforming function, we replace the
sd and ld instructions with sdset1 and ldchk1, respectively.
Unfortunately, lacking link time optimization support in the
llvm-riscv toolchain, we cannot port the original CPS and CPI
implementations.
4) Kernel Protection: Due to the limitation of llvm-riscv
toolchain, even though we were able to generate LLVM
bitcode for the target kernel and apply the static analysis
of Kenali [66], we cannot use Clang to compile the kernel
into executable binary. As a result, we cannot perform
automated instrumentation to protect all the discover data. For
proof-of-concept, we utilize the analysis results to manually
instrumented the kernel to protect the uid ﬁelds in the cred
structure, which are the most popular target for kernel exploits.
Since we have implemented the shadow stack in GCC, we
were able to replace Kenali’s randomization-based stack
protection with our stack shadow.
The rest of the protection mechanisms are implemented
through manual modiﬁcation.
5) Standard libraries: To protect the integrity of saved
context of setjmp/longjmp, we modiﬁed setjmp.S and
__longjmp.S so general registers are saved with sdset1, and
restored with ldchk1 to enforce its integrity. To protect
the integrity of heap metadata, we manually modiﬁed the
linking and unlinking routine to use sdset1 for assigning
pointers and ldchk1 for loading pointers. To set the tag of
static code pointers to 1, we modiﬁed the dynamic loader
(elf_machine_rela) so that during the relation process, it
stores the patched code pointer with tag 1. And to protect
code pointers in GOT table and the exit handler, we modiﬁed
the dynamic loader to use sdset1 to set these pointers, and
ldchk1 to load these pointers.
6) Heartbleed: To protect sensitive data from Heartbleed
attacks, we modiﬁed OpenSSL so that (1) the private key
is stored with sdset1; and (2) when building the response
buffer, ldchk0 is used to make sure that all content copied to
this buffer has tag 0. To implement this protection, we used
background knowledge about Heartbleed to decide where to
put the checking routine (i.e., when constructing the response
buffer). For a prototype implementation, we believe this is
a reasonable limitation. To thoroughly protect the sensitive
data, one could use data ﬂow analysis or taint analysis [82]
to determine where to tag sensitive data, and where to put the
check.
D. Synthesized Attacks
To evaluate the effectiveness of the security applications we
implemented/ported, we developed/ported several synthesized
attacks against different targets.
1) RIPE Benchmark: RIPE [78] is an open sourced intrusion
prevention benchmark. It provides ﬁve testbed dimensions:
location of the buffer overﬂow, target code pointers, overﬂow
technique, attack payload and abused function. Since RIPE
was developed for the x86 platform, we need to modify it
to make it work on the RISC-V architecture. However, due
to time limitations, we could not port all the features of
RIPE. Speciﬁcally, our ported RIPE benchmarks support all
locations of buffer overﬂow, all target code pointers except the
frame pointer, both overﬂow techniques (direct and indirect),
one attack payload (return-to-libc), and one abused function
(memcpy).
2) Heap Exploit: To evaluate heap metadata protection, we
ported the example exploit from [39] to overwrite the return
address of a function.
3) VTable Hijacking: Due to the limitations of the FPGA,
we could not use real-world cases like browser attacks to
evaluate our VTable pointer protection mechanism. Instead, we
developed a simple attack that overwrites the VTable pointer
with a fake one, so the next invocation of the virtual function
will invoke the attacker controlled function.
4) Format String Exploit: Because the RIPE benchmark
does not cover attack targets used in recent attacks, we
implemented a simple program with format string vulnerability
to evaluate the ported CPS mechanism. We chose a format
string vulnerability because it is one of the most powerful
vulnerabilities that can be used as local stack read (%x), arbitrary
memory read (%s), and arbitrary memory write (%n). For attack
targets, we implemented two new attacks: GOT overwriting
and atexit handler overwriting.
5) Kernel Exploit: In the kernel, overwriting non-control
data is sufﬁcient to obtain root permissions without hijack-
ing control ﬂow. To test the feasibility of using HDFI to
defend against data-only attacks in the kernel, we back
1010
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:03 UTC from IEEE Xplore.  Restrictions apply. 
ported CVE-2013-6282 [1], an arbitrary memory read and write
vulnerability to our target kernel. Leveraging this vulnerability,
an attackers can modify the uid of a process and escalate their
privilege.
6) Heartbleed: Heartbleed (CVE-2014-0160) [15] is a heap
out-of-bounds read vulnerability in OpenSSL caused by missing
input validation when parsing malicious TLS heartbeat request.
This bug was marked as extremely critical, because researchers
have proved that it can be exploited to reveal private keys [34].
To reliably2 simulate such attacks, we modiﬁed vulnerable
OpenSSL (1.0.1a) to insert special characters as a decoy
private key. Since the decoy data is inserted in the affected
range of Heartbleed, it can always be leaked in default settings
through a Heartbleed attack.
VII. EVALUATION
In this section, we evaluate our prototype of HDFI by
answering the following questions:
• Correctness. Does our prototype comply with the RISC-V
standard (i.e., no backward compatibility issue)? (§VII-A)
• Efﬁciency. How much performance overhead does HDFI
introduce compared to the unmodiﬁed hardware? (§VII-B)
• Effectiveness. Can HDFI-powered security mechanisms
accurately prevent attacks? (§VII-C)
• Beneﬁts. Compared to their original implementation, does
HDFI-powered implementation perform better and/or is it
more secure? (§VII-D)
Experimental setup. All evaluations were done on the Xilinx
Zynq ZC706 evaluation board [80]. The OS kernel is Linux
3.14.41 with support for the RISC-V architecture [58]. Unless
otherwise stated, all programs (including the kernel) were
compiled with GCC 5.2.0 (-O2) and binutils 2.25, with a set
of patches to support RISC-V (commit 572033b) and default
kernel conﬁguration of RISC-V. While the board is equipped
with 1GB of memory, the Rocket Chip can only use 512MB
because the co-equipped ARM system requires 256MB. At
boot time, the kernel reserves 8MB for tag tables and 128KB
for the meta tag table, respectively. Following the environment
that the RISC-V community built, we use the Frontend Server
that runs on the ARM system and the Berkeley Boot Loader
that runs on the Rocket Chip to boot vmlinux. The Rocket Chip
accesses an ext2 ﬁle system in an SD card via the Front-end
Server.