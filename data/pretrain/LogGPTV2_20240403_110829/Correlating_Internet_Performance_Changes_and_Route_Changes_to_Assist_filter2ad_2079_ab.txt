To facilitate further investigation of changes, there are highlighted links in this table
that allow one to: view all the traceroutes for a selected remote host (as a color coded
web table accessible by clicking on the nodename); access text suitable for attaching
to trouble reports (Sum); review the log files (LOG*); review the route numbers (“R”)
seen for a given host together with when last seen; view the available bandwidth time-
); and to select times and remote hosts for which one
series for the last 48 hours (
wishes to view topology maps.
In  Fig.  2  for  hours  “07”  and  “08”,  it  can  be  seen  that  there  were  multiple  route
changes to European nodes in the same time frame. Each entry (there can be multiple
for  each  box  representing  an  hour)  provides  a  dot  to  denote  that  the  route  has  not
changed  from  the  previous  measurement.  If  the  route  has  changed,  the  new  route
number is displayed. The first measurement for each day  is  displayed  with  its  route
number.  This  very  compact  format  enables  one  to  visually  identify  if  several  routes
changed  at  similar  times,  (i.e.  route  numbers  appear  in  one  or  two  columns  for
multiple hosts (rows)), and whether the changes occur at multiple times and/or revert
back to the original routes.
 Note that the table has check boxes before the nodes and above the hour columns at
the  top  of  the  table.  By  checking  boxes  for  nodes  and  hours,  and  then  clicking  on
“SUBMIT  Topology  request”  the  viewer  can  generate  a  topographical  map  of  the
routes.
Correlating Internet Performance Changes and Route Changes         293
Fig. 3.  Topology  visualization  of  routes  from  SLAC  to  4  European  sites  between  07:00  and
09:00 Jan 15 '04
Fig.  3  is  an  example  of  such  a  topology  map  showing  routes  from  SLAC  between
07:00 and 09:00 on Jan. 15 2004 to European sites in France, the Czech Republic, and
2  sites  in  the  UK.  This  corresponds  to  the  route  changes  seen  in  Fig.  2,  and  the
multiple routes used can be clearly seen. The topology maps display the hop routers
colored by ISP, provide the router and end host names by “mouse over”, and provide
the ability to zoom in to help disentangle  more complex sets of routes.
4   Example of the Visualization of Achievable and Available
Bandwidth Changes, and Route Changes
The  measurements  of  achievable  and  available  bandwidth  use  very  different
techniques.  We  measure  achievable  bandwidth  via  an  IPERF  TCP  memory  to
memory transfer between two hosts over many seconds (usually 10 seconds).  IPERF
is network intensive as it sends as much TCP data as possible for the duration of the
measurement.  Thus  we  make  IPERF  measurements  every  90  minutes.  We  measure
available  bandwidth  using  ABwE  in  less  than  a  second  by  sending  20  UDP  packet
pairs, and measuring the time dispersal of the inter-packet delays upon arrival at the
remote host.  We repeat the ABwE measurements every minute. Fig. 4 is the graph of
294         C. Logg, J. Navratil, and L. Cottrell
the  IPERF  bandwidth  data  points  and  the  ABwE  measurements  from  SLAC  to
Caltech  for  a  24  hour  period  on  October  9,  2003.  ABwE  provides  estimates  of  the
current bottleneck capacity (the top line in Fig. 4), and the cross-traffic (the bottom
line in Fig.4) and the available bandwidth (middle line). The available bandwidth is
calculated  by  subtracting  the  cross-traffic  from  the  bottleneck  capacity.  The  IPERF
measurements are displayed as black dots on the graph.  The two measurements are
performed independently from two different hosts at SLAC. Note the corresponding
drop observed  by  the  two  bandwidth  measurements  at  about  14:00.    To  understand
what has happened here, it is only necessary to look at the traceroute history for this
day.
Fig. 5 is a snapshot of the traceroute table for Caltech corresponding to Fig. 4.  Note
the  change  from  route  #105  to  route  #110  at  14:00,  and  back  again  to  route  #105
about  17:00.  By  selecting  those  hours  and  the  Caltech  node  and  submitting  the
topology request, we can get a graph (Fig. 6) of the topology change responsible for
the bandwidth changes.
Fig.  4.  Plot  of  ABwE  (available  bandwidth)  measurements  and  corresponding  IPERF
(achievable bandwidth) measurements
Fig. 5.  Snapshot of traceroute summary entry for Caltech at 14:00 and 17:00
Correlating Internet Performance Changes and Route Changes         295
Fig. 6.  Graphical traceroute display. Note hop to ABILENE on the way to CENIC
5   Challenges in Traceroute Analysis
Analyzing traceroute data to identify unique routes and to detect “significant” changes
in routes can be tricky.  At many hops there can be non-responsive router responses
(see hop “12” in Fig. 7) one or more times, where no information is returned.
traceroute to NODE1-GIG.NSLABS.UFL.EDU (xx.yy.160.3)
 1  SLAC-RTR1  0.164 ms
 2  SLAC-RTR2  0.368 ms
 3  i2-gateway.stanford.edu (192.68.191.83)  0.288 ms
 4  STAN.POS.calren2.NET (171.64.1.213)  0.369 ms
 5  SUNV--STAN.POS.calren2.net (198.32.249.73)  0.626 ms
 6  Abilene--QSV.POS.calren2.net (198.32.249.162) 0.959 ms
 7  kscyng-snvang.abilene.ucaid.edu (198.32.8.103) 36.145 ms
 8  iplsng-kscyng.abilene.ucaid.edu (198.32.8.80) 53.343 ms
 9  atla-iplsng.abilene.ucaid.edu (198.32.8.78)  60.448 ms
 10  a713.c12008.atla.abilene.ucaid.edu (192.80.53.46) 71.304 ms
 11  ssrb-ewan-gsr-g20.ns.ufl.edu (128.227.254.122)  71.323 ms
 12  *
 13  nslab-bpop-rsm-v222.nslabs.ufl.edu (128.227.74.130) 76.356
Fig. 7. Traceroute output
296         C. Logg, J. Navratil, and L. Cottrell
This can happen for a variety of reasons due to the configuration of the router at that
hop and/or the type of software that it is running. There can be multiple non-responses
for the same hop. There can be hop changes within a network provider’s domain. In
many  cases  these  are  transparent  and  no  effect  on  the  throughput  is  seen,  while  at
other times these can be significant.  Looking at the trace route output unfortunately
does  not  solve  the  problem.  In  our  processing  of  traceroutes  to  detect  significant
routing  changes,  we  basically  ignore  non-responders.  However  route  changes,  even
within  a  network  provider’s  domain  are  considered  significant.    In  the  topology
graphs responsive and non-responsive hops are displayed.
6   Challenges in Identifying Throughput Changes
Ideally we would like to automate the detection of throughput changes and compare
them automatically to the traceroute changes.  We have mentioned the problem with
identifying  significant  traceroute  changes.  Identifying  throughput  changes  is  also
tricky. One has to set “thresholds” of change to  use in order to pick out throughput
changes. These thresholds must vary according to the throughput level.  On a gigabit
link  which  handles  high  volume  data  transfers,  a  drop  of  100-200  megabits  (10%-
40%)  may  simply  be  the  result  of  a  long/large  data  transfer,  and  may  not  be
significant.  On a 100 megabit link, a drop of 50 megabits (50%) may very well be
significant, or again it may mean that there is a sustained high volume data transfer
occurring.
The frequency of the data points also needs to be taken into consideration, to avoid
false  “alerts”  which  are  worse  than  no  alerts,  ideally  one  wants  to  identify  a
“sustained”  drop  before  alerting.    If  the  data  points  are  one  hour  apart,  it  may  take
several hours to have enough data to be sure it is a sustained drop.  If the data points
are  once  a  minute,  10  minutes  or  slightly  more  may  be  adequate.    Even  with  one
minute samples, the identification may be difficult.  In some cases we have seen the
drop  happen  gradually  over  a  day  or  two,  and  thus  the  percent  change  threshold  is
never exceeded.
7   Utilization
This set of tools has been in production use at SLAC for several months. It has already
been  successfully  used  in  several  problem  incidents  and  is  being  enhanced  as  a
consequence of its use. We will report on specific examples illustrating how the tools
have been used to identify and pin-point performance problems in various networks.
In some of these cases the problem went unidentified for several days or even weeks
in one case, but once identified and reported, the problem was fixed in hours. With the
large number of important collaborating sites, it is impractical to manually review all
the performance graphs for all the paths and detect problems quickly. This identifies
the need to automate the reliable detection of significant changes.
Correlating Internet Performance Changes and Route Changes         297
8   Future Plans
Work  is  in  progress  to  automate  the  identification  of  significant  changes,  and  to
automatically assist in gathering the associated relevant information (e.g. current, and
trace  routes  before  and  after  a  performance  change,  time  and  magnitude  of  the
change, topology map, and time series plots of the performance changes). This will be
gathered into email and a web page and sent to the local network administrator. We
expect to report on progress with this automation by the time of the conference. We
are  also  analyzing  the  ratio  of  significant  performance  problems  caused  by  route
changes and vice-versa, and the duration of significant performance degradations, and
will also report on this.
References
1.
2.
Experiences  and  Results  from  a  New  High  Performance  Network  and  Application
Monitoring Toolkit, Les Cottrell, Connie Logg, I-Heng Mei, SLAC-PUB-9641, published
at PAM 2003, April 2003.
ABwE:  A  Practical  Approach  to  Available  Bandwidth  Estimation,  Jiri  Navratil,  Les
Cottrell, SLAC-PUB-9622, published at PAM 2003.
SSH:  http://www.ssh.com/solutions/government/secureshell.html
IPERF:  http://dast.nlanr.net/Projects/Iperf/
3.
4.
5. BBFTP:  http://doc.in2p3.fr/bbftp/
6. BBCP:  http://www.slac.stanford.edu/~abh/bbcp/
7. QIPERF:  http://www-iepm.slac.stanford.edu/bw/iperf_res.html
8. GridFTP:  http://www.globus.org/datagrid/gridftp.html
9. Web Services:  http://www.w3.org/2002/ws/
10. MonALISA:  http://monalisa.cacr.caltech.edu/