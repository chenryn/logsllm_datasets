A sandboxing architecture should be ﬂexible and exten-
sible enough to implement a wide range of security poli-
cies and helpful features. Delegating architectures offer ex-
panded potential for easily supporting novel policies. More
speciﬁcally, because Ostia handles granting access to re-
sources at user level, it is inherently easier to alter the im-
plementation of system calls. This added ﬂexibility can be
a boon in a variety of scenarios.
For example, applications can be given their own per-
sonalized view of the ﬁle system namespace, e.g. mapping
/etc/shadow to an application-speciﬁc copy of the pass-
word ﬁle, at the same time preserving compatibility and not
exposing sensitive state [2]. It can also be useful for miti-
gating side effects caused by denying system calls [31, 16].
The capability to selectively modify some calls makes it
easier to apply other security mechanisms. For example,
the sandbox might respond to a request for a socket with a
socket running over a SOCKS connection to a ﬁrewall, or
with a socket that has had a socket ﬁlter applied to it. (A
socket ﬁlter is a Linux primitive for applying ﬁne grain re-
strictions on what can pass over a socket.)
Filtering sandboxes have provided some support for
changing call implementation. For example, some support
the ability to rewrite arguments, to change a call’s return
value, or to change the privilege level of a process while it
executes a system call [2, 31]. Unfortunately, for each new
change to how a call is executed, new support must be added
piecemeal to the kernel. Delegating sandboxes are easily
able to accommodate all of these features entirely at user
level because the agent controls the execution environment
of the call (e.g. call arguments, privilege level, descriptor
space) and the choice of calls executed for a given request.
5.3 Compatibility
For greatest utility, an application sandbox must be com-
patible with a wide range of software. As a ﬁrst step, it
must not require applications to be recompiled or otherwise
modiﬁed to run them in the sandbox. Ostia, as well as many
ﬁltering sandboxes, meets this criterion.
Ostia also supports multithreaded applications. No cur-
rent ﬁltering sandboxing system supports multithreaded ap-
plications due to the problem of race conditions. While it
is certainly possible to add this functionality to a ﬁltering
sandbox, the signiﬁcant additional effort and questions of
assurance raised have prevented us and others from includ-
ing this functionality.
Currently our system has been successfully used to sand-
box a wide variety of real world applications, including the
following:
Network servers: Apache, BIND, CUPS.
Network clients: konqueror, lynx, links, ssh, wget.
X applications: gimp, gphoto, konsole.
Viewers: gs, gv, xpdf, xli.
Editors: Emacs, nvi, vim.
Shells: bash, tcsh.
The broad applicability of interposition-based sandbox-
ing in a practical setting has also been demonstrated by
other systems [31, 2].
5.4 Deployability
The fewer prerequisites and dependencies a program has,
the more easily that program can be deployed in real sys-
tems.
Ostia relies on a kernel module, which causes some de-
ployment difﬁculty in itself, since it requires that any ma-
chine where it is installed has a C compiler and appropriate
headers, or a suitable precompiled module. Ostia’s mod-
ule is extremely simple, with few dependencies on kernel
internals, which makes porting and maintenance easy. On
the other hand, kernel modules required by many ﬁltering
sandboxes, including J2, are larger and more complex, with
important dependencies on kernel internals that may need
careful changes as the kernel evolves, which makes porting
and maintenance signiﬁcantly more difﬁcult.
Ostia does not require a kernel patch, greatly easing the
burden on the installer. Applying a kernel patch requires a
new system kernel to be compiled and installed followed by
a system reboot. This additional human overhead as well as
system downtime makes this approach a real impediment
to practical adoption. Some systems are loath to be taken
down for a kernel patch, normal users often do not have the
maturity to comfortably patch and recompile their own ker-
nel, and often even experienced users simply do not want to
expend this effort to try a new tool. (Some ﬁltering sand-
boxes, including J2, also do not require a kernel patch.)
Ostia’s loader program must intimately understand the
system executable format. A change in the executable for-
mat would require modiﬁcations to the loader program. Ex-
ecutable formats rarely change, so this is unlikely to be a
real barrier to deployment.
In conclusion, we believe that a delegating sandbox such
as Ostia can be more easily ported and deployed on a wide
range of platforms due to its minimal requirements for ker-
nel support and ease of installation.
5.5 Performance
Architectural features impact performance in important
ways. This section undertakes a quantitative comparison
of these features, examining the performance impacts of
different interposition mechanisms, concurrency strategies,
and the overhead of sandboxing on different workloads. We
primarily compare Ostia against J2, although other applica-
tion sandboxes are brieﬂy considered.
Test platform: All of our performance testing was con-
ducted on an IBM T30 laptop with a 1.8 GHz Pentium 4
processor and 1 GB of RAM, running Debian GNU/Linux
“sid” “unstable” with a Linux 2.4.20 kernel. Testing was
performed in single-user mode with all system services
turned off and the network interface disabled. Network ser-
vice tests were conducted locally over the loopback inter-
face.
Interposition overhead: Table 1 shows per-call interpo-
sition cost, the primary overhead imposed by sandboxing.
The table’s ﬁrst row shows the basic speed of interposition
in each system, using geteuid, a trivial system call. On
our test system, the minimum penalty of interposition for a
system call is therefore about 11.4 µs under Ostia. The sec-
ond row shows the speed of interposition for open, a more
substantial system call. Neither J2 nor Ostia has been heav-
ily optimized. In spite of this, its performance for open is
substantially better than previous published results, which
put its slowdown at 25× (25 times slower) [31], compared
with our numbers which only reﬂect 5× to 8× slowdown.
In principle, there are some basic limits on how much
this overhead can be reduced. Ostia and J2 both require con-
text switches to and from the policy-checking process for
every call they check. This imposes a basic penalty of two
additional context switches (essentially one system call) for
each checked call in addition to the overhead for making a
policy decision.
In a delegating sandbox, some additional calls may be
required to obtain a requested resource. Ostia’s callback
mechanism also requires two additional context switches
for the ﬁrst instance of each type of call it intercepts. This
call
geteuid
open
none
1.00µs
3.92µs
sandbox
J2
Ostia
9.70µs 9.7× 12.42µs 12.4×
20.42µs 5.2× 31.16µs 7.9×
Table 1: Microbenchmark results. Entries for J2
and Ostia show absolute times and number of
times slower than unsandboxed times.
Times
are “wall clock” times averaged over 10 sets of
100,000 iterations. Over the 10 sets, σ2 < .15 µs.
cause
open
basic interposition
policy decision
extra kernel overhead
total
J2
3.92 µs
8.70 µs
3.27 µs
4.53 µs
20.42 µs
Ostia
3.92 µs
11.42 µs
9.26 µs
6.56 µs
31.16 µs
Table 2: Time to execute open under J2 and Os-
tia, broken down into individual components: the
open itself, basic system call interposition over-
head, time to make a policy decision, and addi-
tional overhead in the kernel.
upfront overhead is quickly amortized away over the life-
time of the program. Other overheads, such as the cost of
copying arguments, can be kept to a minimum in a care-
ful implementation. This said, we believe that signiﬁcant
further speedups are achievable over our current naive im-
plementation. However, as we will see later, Amdahl’s law
will likely make further optimizations irrelevant for most
workloads.
Where the time goes: Table 2 breaks down the costs of re-
stricting a single open call. We attribute the same cost to the
actual ﬁle open, 3.92 µs, as in the unsandboxed timings for
open. We also assume that the basic cost of interposing on
a call is unchanged from that for geteuid. We calculate
the cost of making a policy decision by repeating the mea-
surements with the policy engine turned off and computing
the difference. Finally, we assume that the remainder of the
time is taken up in additional kernel overhead for check-
ing buffers for ﬁle names, copying data, transferring ﬁle de-
scriptors between processes, etc., all costs necessitated by
open but not by geteuid.
The table shows that Ostia’s policy engine is slower than
J2’s. This is understandable because the policy engine in
Ostia often makes several system calls, whereas the J2 pol-
icy engine for ﬁle system operations is largely a string-
matching operation. The table also shows that Ostia has
higher “extra” kernel overhead, which may be due to inter-
process ﬁle descriptor passing.
benchmark
web serving
decompress
encode
build
none
10.85s
3.13s
14.91s
8.12s
Ostia
sandbox
J2
.5%
10.88s
.0%
3.13s
14.94s
.0%
8.78s 8.1% 10.11s 24.5%
.2% 10.90s
.0%
3.13s
.2% 14.92s
Table 3: Macrobenchmark results. Entries for J2
and Ostia show absolute times and percent slower
than unsandboxed times. Times are averaged
“wall clock” times. Most entries are averaged over
10 runs with σ2 < .1 s; web service entries aver-
aged over 100 runs with σ2 < .5 s.
No. Procs.
1
10
25
50
100
none
3.90 s
3.94 s
3.92 s
3.91 s
3.91 s
J2
20.89 s
22.79 s
29.12 s
55.48 s
375.96 s
Ostia
31.07 s
31.62 s
32.71 s
32.77 s
31.87 s
Table 4: Scaling results. Times are “wall clock”
in seconds, required to open and close
time,
1,000,000 ﬁles.
“No. Procs” is the number of
processes that the ﬁle operations were divided
among on each row. Entries are the average of
3 runs after an initial, discarded run. σ2 < .1 s
except for J2 column.
Concurrency scaling: Concurrency strategy can signiﬁ-
cantly impact scalability, as a lack of parallelism in the
monitor or agent can cause a backlog of calls waiting to be
checked. Our sandboxes are at opposite ends of the concur-
rency spectrum: Ostia uses a purely multithreading model,
with one agent process per sandboxed process, whereas J2
multiplexes requests through a single monitor process.
To clearly show how a single process can act as a bot-
tleneck, we ran a simple microbenchmark that repeatedly
opened and closed ﬁles, dividing this work evenly among
a variable number of processes. Table 4 shows the results.
With no sandbox, running time varied only 1% between 1
and 100 processes; with Ostia, our multithreading sandbox,
only 5%. Under J2, our multiplexing sandbox, running time
for 100 processes was about 18× that for a single process,
and even at 10 processes a 10% increase was observed.
We draw two conclusions. First, the lack of parallelism
in a multiplexing sandbox can create a signiﬁcant perfor-
mance bottleneck. Even under relatively modest loads this
greatly impacts performance. Second,
the overhead of
naively scaling the number of sandbox processes with the
number of application processes is nominal. A third op-
tion that we did not explore is a thread pool approach where
parallelism could gradually scale with demand. However
the added complexity of such an approach seems unwar-
ranted given the success of a naive agent-per-process strat-
egy. Previous work has overlooked the beneﬁts of paral-
lelism, merely citing the overhead of additional processes
as the reason for multiplexing [36, 30]. Empirically, multi-
plexing does not seem to offer any performance beneﬁt; on
the contrary it signiﬁcantly limits scalability.
Typical application overhead: The primary use of sand-
boxing systems is to protect applications that are routinely
exposed to hostile inputs, such as helper applications and
network services. We benchmarked three such programs:
Web serving uses Apache to serve 5,000 static pages, to-
taling 6.4 MB, to a client running outside the sandbox.
Pages are requested and serviced serially for this test,
so J2’s policy engine serialization does not penalize it.
Decompress uses GNU gzip to decompress a 31 MB ﬁle,
discarding the output.
Encode converts a 48 MB WAV ﬁle to Ogg Vorbis format.
The ﬁrst three rows of Table 3 show the results. In each