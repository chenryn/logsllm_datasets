each representing a diﬀerent type of patient.
5.2 Classiﬁcation Models
For the CIFAR100 dataset, we used two diﬀerent neural network
architectures. (1) Alexnet architecture [31], trained with Adam
optimizer[28] with learning rate 0.0001, and 100 epochs of training.
(2) DenseNet architecture [25], trained with stochastic gradient de-
scent (SGD) for 300 epochs, with learning rate 0.001 from epoch 0
to 100, 0.0001 from 100 to 200, and 0.00001 from 200 to 300. Follow-
ing their architectures, both these models are regularized. Alexnet
uses Dropout (0.2), and Densenet uses L2-norm regularization (5e-
4).
For the Purchase100 dataset, we used a 4-layer fully connected
neural network with layer sizes [1024, 512, 256, 100]. We used Tanh
activation functions, similar to [45]. We initialized all of parame-
ters with a random normal distribution with mean 0 and standard
deviation 0.01. We trained the model for 50 epochs.
For the Texas dataset, we used a 5-layer fully connected neural
network with layer sizes [2048, 1024, 512, 256, 100], with Tanh ac-
tivation functions. We initialized all of parameters with a random
normal distribution with mean 0 and standard deviation 0.01. We
trained the model for 50 epochs.
Table 3 shows the number of training data as well as refer-
ence data samples which we used in our experiments for diﬀer-
ent datasets. It also reports the adversarial regularization factor λ
which was used in our experiments.
1http://pytorch.org/
2https://www.kaggle.com/c/acquire-valued-shoppers-challenge/data
6
h(x , y , f (x))
64 × 1
256 × 64
128 × 256
512 × 64
1024 × 512
100 × 1024
f (x)
512 × 64
100 × 512
y
Figure 3: The neural network architecture for the inference
attack model. Each layer is fully connected to its subsequent
layer. The size of each fully connected layer is provided.
5.3 Inference Attack Model
For the inference model, we also make use of neural networks. Fig-
ure 3 illustrates the architecture of our inference neural network.
The objective of the attack model is to compute the membership
probability of a target record (x , y) in the training set of the clas-
siﬁcation model f . The attack model inputs (x , y) as well as the
prediction vector of the classiﬁcation model on it, i.e., f (x). We
design the inference attack model with three separate fully con-
nected sub-networks. One network of layer sizes [100,1024,512,64]
operates on the prediction vector f (x). One network of layer sizes
[100,512,64] operates on the label which is one-hot coded (all el-
ements are 0 except the one that corresponds to the label index).
The third (common) network operates on the concatenation of the
output of the ﬁrst two networks and has layer sizes of [256,64,1]. In
contrast to [45] which trains k membership inference attack mod-
els (one per class), we design only a single model for the inference
attack. The architecture of our attack model, notably its last (com-
mon) layers, enables capturing the relation between the class and
the predictions of the model for training set members versus non-
members.
We use ReLu as the activation function in the network. All
weights are initialized with normal distribution with mean 0 and
standard deviation 0.01, and all biases are initialized to 0. We use
Adam optimizer with learning rate 0.001. We make sure every
training batch for the attack model has the same number of mem-
ber and non-member instances to prevent the attack model to be
biased toward one side.
Table 3 shows the number of known members of the training
set, DA, and known non-member data points, D ′A, that we assume
for the adversary, which is used for training his attack model. The
larger these sets are (especially the DA set), the more knowledge is
assumed for the attacker. As we are evaluating our defense mecha-
nism, we assume a strong adversary who knows a substantial frac-
tion of the training set and tries to infer the membership of the rest
of it.
Dataset
Purchase100
Texas100
CIFAR100
|D |
20,000
10,000
50,000
|D ′|
20,000
5,000
5,000
λ
3
2
6
|DA |
5,000
5,000
25,000
|D ′A |
20,000
10,000
5,000
Table 3: Experimental setup, including the size of the train-
ing set D and reference set D ′ in Algorithm 1, the adversar-
ial regularization factor λ, as well as the size of the adver-
sary’s known members DA of the classiﬁer’s training set and
known non-members D ′A.
2.0
1.5
1.0
0.5
s
s
o
l
r
e
i
f
i
s
s
a
l
C
0.0
0
0.35
n
i
a
g
r
e
k
c
a
t
t
A
0.30
0.25
0.20
0.15
0
Purchase100
With De fe ns e
Without De fe ns e
10
20
30
40
50
Epoc h
Purchase100
10
20
30
40
50
Epoc h
Figure 4: The trajectory of the classiﬁcation loss during
training with/without defense mechanism, as well as the in-
ference attack gain, using the Purchase100 dataset.
5.4 Empirical Results
Loss and Gain of the Adversarial Training
Figure 4 shows the empirical loss of the classiﬁcation model (2)
as well as the empirical gain of the inference model (5) through-
out the training using Algorithm (1). By observing both the clas-
siﬁer’s loss and the attack’s gain over the training epochs, we can
see that they converge to an equilibrium point. Following the opti-
mization problem (7), the attacker’s gain is the maximum that can
be achieved against the best defense mechanism. The classiﬁer’s
7
Without defense
With defense
Dataset
Training
accuracy
100%
Purchase100
81.6%
Texas100
CIFAR100- Alexnet
99%
CIFAR100- DenseNET 100%
Testing
accuracy
80.1%
51.9%
44.7%
70.6%
Attack
accuracy
67.6%
63%
53.2%
54.5%
Training
accuracy
92.2%
55%
66.3%
80.3%
Testing
accuracy
76.5%
47.5%
43.6%
67.6%
Attack
accuracy
51.6%
51.0%
50.7%
51.0%
Table 4: Comparison of membership privacy and training/test accuracy of a classiﬁcation model (without defense), and
a privacy-preserving model (with defense) on four diﬀerent models/datasets. Compare the two cases with respect to the
trade-oﬀ between testing accuracy and attack accuracy. See Table 3 for the experimental setup.
λ
Training
accuracy
Testing
accuracy
Attack
accuracy
0 (no defense)
1
2
3
10
100%
98.7%
96.7%
92.2%
76.3%
80.1%
78.3%
77.4%
76.5%
70.1%
67.6%
57.0%
55.0%
51.8%
50.6%
Table 5: The eﬀect of the adversarial regularization factor
λ, used in the min-max optimization (7), which also acts as
our privacy parameter, on the defense mechanism trained
on the Purchase100 dataset.
L2-regularization
factor
Training
accuracy
Testing
accuracy
Attack
accuracy
0 (no regularization)
0.001
0.005
0.01
100%
86%
74%
34%
80.1%
81.3%
70.2%
32.1%
67.6%
60%
56%
50.6%
Table 6: The results of using a L2−regularization as a mitiga-
tion technique for membership inference attack. The model
is trained on the Purchase100 dataset. Compare these results
with those in Table 4 which shows what we can achieve us-
ing the strategic min-max optimization.
Reference set size Testing accuracy Attack accuracy
1,000
5,000
10,000
20,000
30,000
80.0%
77.4%
76.8%
76.5%
76.4%
59.2%
52.8%
52.4%
51.6%
50.6%
Table 7: The eﬀect of the size of the reference set D ′ on the
defense mechanism for the Purchase100 dataset. Note that
(as also shown in Table 3) the size of the training set is 20,000.
loss is also the minimum that can be achieved while preserving
privacy against the best attack mechanism. As shown in Section 4,
by minimizing the classiﬁer’s loss, we train a model that not only
8
prevents the attack’s gain to grow large, but also forces the adver-
sary to play the random guess strategy.
In Figure 4 (top), we compare the evolution of the classiﬁca-
tion loss of the privacy-preserving model with the loss of the
same model when trained regularly (without defense). As we can
see, a regular model (without defense) arbitrarily reduces its loss,
thus might overﬁt to its training data. Later in this section, in Fig-
ure 6, we visualize the impact of this small loss on the output
of the model, and we show how this can leak information about
the model’s training set. The adversarial training for member-
ship privacy strongly regularizes the model. Thus, our privacy-
preserving mechanism not only protects membership privacy but
also signiﬁcantly prevents overﬁtting.
Privacy and Generalization
To further study the tradeoﬀ between privacy and predictive
power of privacy-preserving models, in Figure 5 we show the cu-
mulative distribution of the model’s generalization error over dif-
ferent classes. The plot shows the fraction of classes (y-axis) for
which the model has a generalization error under a certain value (x-
axis). For each class, we compute the model’s generalization error
as the diﬀerence between the testing and training accuracy of the
model for samples from that class [22]. We compare the generaliza-
tion error of a regular model and our privacy-preserving model. As
the plots show, the generalization error of our privacy mechanism
is signiﬁcantly lower over all the classes.
Table 4 presents all the results of training privacy-preserving
machine learning models using our min-max game, for all our
datasets. It also compares them with the same models when trained
regularly (without defense). Note the gap between training and
testing accuracy with and without the defensive training. Our
mechanism reduces the total generalization error by a factor of
up to 4. For example, the error is reduced from 29.7% down to 7.5%
for the Texas100 model, it is reduced from 54.3% down to 22.7% for
the CIFAR100-Alexnet model, and it is reduced from 29.4% down to
12.7% for the CIFAR100-Densenet model, while it remains almost
the same for the Purchase100 model. Our min-max mechanism
achieves membership privacy with the minimum generaliza-
tion error. Table 5 shows how we can control the trade-oﬀ between
prediction accuracy and privacy, by adjusting the adversarial reg-
ularization factor λ.
The regularization eﬀect of our mechanism can be compared to
what can be achieved using common regularizers such as the L2-
i (see our formalization of a
norm regularizer, where R(fθ ) =i θ 2
1.2
1.0
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t