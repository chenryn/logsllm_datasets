### 5.2 Classification Models

For the CIFAR100 dataset, we employed two different neural network architectures:

1. **AlexNet Architecture** [31]:
   - Optimizer: Adam [28]
   - Learning Rate: 0.0001
   - Training Epochs: 100
   - Regularization: Dropout (0.2)

2. **DenseNet Architecture** [25]:
   - Optimizer: Stochastic Gradient Descent (SGD)
   - Learning Rate Schedule:
     - 0.001 for epochs 0-100
     - 0.0001 for epochs 100-200
     - 0.00001 for epochs 200-300
   - Training Epochs: 300
   - Regularization: L2-norm regularization (5e-4)

For the Purchase100 dataset, we used a 4-layer fully connected neural network with the following specifications:
- Layer Sizes: [1024, 512, 256, 100]
- Activation Function: Tanh
- Parameter Initialization: Random normal distribution with mean 0 and standard deviation 0.01
- Training Epochs: 50

For the Texas dataset, we used a 5-layer fully connected neural network with the following specifications:
- Layer Sizes: [2048, 1024, 512, 256, 100]
- Activation Function: Tanh
- Parameter Initialization: Random normal distribution with mean 0 and standard deviation 0.01
- Training Epochs: 50

Table 3 summarizes the number of training data and reference data samples used in our experiments for each dataset, along with the adversarial regularization factor \( \lambda \) used.

### 5.3 Inference Attack Model

Our inference attack model is also based on neural networks. Figure 3 illustrates the architecture of the inference neural network. The goal of the attack model is to compute the membership probability of a target record \((x, y)\) in the training set of the classification model \( f \). The attack model takes as input the target record \((x, y)\) and the prediction vector \( f(x) \).

The inference attack model consists of three separate fully connected sub-networks:
1. Network with layer sizes [100, 1024, 512, 64] operating on the prediction vector \( f(x) \).
2. Network with layer sizes [100, 512, 64] operating on the one-hot coded label.
3. A common network with layer sizes [256, 64, 1] operating on the concatenation of the outputs from the first two networks.

We use ReLU as the activation function. All weights are initialized with a normal distribution with mean 0 and standard deviation 0.01, and all biases are initialized to 0. The optimizer used is Adam with a learning rate of 0.001. To ensure balanced training, every batch contains an equal number of member and non-member instances.

Table 3 also shows the number of known members of the training set (\( D_A \)) and known non-member data points (\( D'_A \)) assumed for the adversary. A larger \( D_A \) indicates more knowledge about the training set, which we assume for a strong adversary.

### 5.4 Empirical Results

#### Loss and Gain of the Adversarial Training

Figure 4 illustrates the trajectory of the classification loss during training with and without the defense mechanism, as well as the inference attack gain, using the Purchase100 dataset. Both the classifier's loss and the attacker's gain converge to an equilibrium point. This aligns with the optimization problem (7), where the attacker's gain is maximized against the best defense mechanism, and the classifier's loss is minimized while preserving privacy.

#### Privacy and Generalization

To further analyze the trade-off between privacy and predictive power, Figure 5 shows the cumulative distribution of the model's generalization error over different classes. The plot compares the generalization error of a regular model and our privacy-preserving model. Our privacy mechanism significantly reduces the generalization error across all classes.

Table 4 presents the results of training privacy-preserving machine learning models using our min-max game for all datasets, compared to the same models trained without defense. The gap between training and testing accuracy with and without defensive training is notable. Our mechanism reduces the total generalization error by up to a factor of 4. For example, the error is reduced from 29.7% to 7.5% for the Texas100 model, from 54.3% to 22.7% for the CIFAR100-Alexnet model, and from 29.4% to 12.7% for the CIFAR100-Densenet model, while it remains almost the same for the Purchase100 model.

Table 5 shows how adjusting the adversarial regularization factor \( \lambda \) can control the trade-off between prediction accuracy and privacy. Table 6 compares the results of using L2-regularization as a mitigation technique for membership inference attacks. Table 7 demonstrates the effect of the size of the reference set \( D' \) on the defense mechanism for the Purchase100 dataset.

In summary, our privacy-preserving mechanism not only protects membership privacy but also significantly prevents overfitting, achieving a better balance between privacy and predictive performance.