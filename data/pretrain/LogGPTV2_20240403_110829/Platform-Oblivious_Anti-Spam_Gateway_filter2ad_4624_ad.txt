outlier scores from our system. Red cross and blue points signify
spam and ham, respectively. These figures exhibit our method can
expose the outlier property of spam messages in all four datasets,
where most of the spam messages have higher scores than ham
messages, resided in the leftmost parts.
Although the outlier property is visible, how to set a threshold
for effectively separating spams from hams is still a challenging
Outlier ValuekjyxOutlier Ranking1071Platform-Oblivious Anti-Spam Gateway
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Table 3: Spam counts and ham counts in the Spam Zone, Un-
certain Zone, and Ham Zone from each dataset
Dataset
Label
Kaggle SMS
Metsis Email
Twitter Trending
Twitter Normal
Spam Zone
Spam
311
1966
33424
110154
Ham
21
394
7640
20573
Uncertain Zone
Ham
Spam
320
339
1376
893
42591
40625
86992
137683
Ham Zone
Spam
116
1492
21576
107380
Ham
4465
15258
532082
5360448
Figure 9: The performance of our system in Kaggle SMS
dataset with various spam threshold values.
− a−F
problem. Here, we propose a novel solution for intelligently identi-
fying a suitable spam threshold value to single out spam messages.
As the curves of all ranked values in Figure 7 follow exponential
ones, we regress ranked values according to an exponential func-
tion, i.e., L = e
b , where a and b are the parameters to fit the
ranked outlier values. F is the ranking value for a given outlier
score. This regression curve is also called Lorenz curve [14]. The
Lorenz curve of ranked outlier scores depicted in Figure 7(a) is
illustrated in Figure 8. With this curve, we take the leftmost point
that has the highest score and draw a tangent line at this point,
which intersects with the x-axis at a point k. Any point left to k
will be considered as an outlier. We define the region left to point k
as the Spam Zone, colored in orange in Figure 8. We draw another
tangent line of the Lorenz curve with the slop of −1, and assume it
intersects with the x-axis at point j. The region between the points
of k and j is defined as the Uncertain Zone, while the region at the
right of j is denoted as the Ham Zone. Uncertain Zone and Ham
Zone are colored in yellow and blue, respectively. Table 3 shows
the number of spams and hams in each region of every dataset. For
the messages in Spam Zone and Ham Zone, we are confident to tell
them as spams and hams, respectively. However, the messages in
Uncertain Zone are hard to be classified surely. If we assume 50%
of messages in the Uncertain Zone are spams, the spam ratio in a
target dataset, denoted as R, can be estimated by: R = n1+0.5∗n2
,
where n1 and n2 refer to the message counts in Spam Zone and
Uncertain Zone, respectively. N is the total number of messages
in the target dataset. Based on our estimation, the spam ratios of
four target datasets are 11.9%, 16.9%, 12.2%, and 4.1%, respectively.
When compared to the true spam ratios of target datasets, i.e., 13.4%,
20.0%, 14.6%, and 6.1%, respectively, from Table 2, our estimated
spam ratios are very close to the true values.
N
Figure 9 shows the values of precision, recall, and F1 score under
different threshold values for the outlier ranking curve, for the
Kaggle SMS dataset. It also exhibits the best performance to result
from the threshold of around 11% to 13%, very close to the true
spam ratio (i.e., 13.4%) and the estimated spam ratio (i.e., 11.9%).
It implies the estimated spam ratios from our proposed solution
can be safely employed by our system to filter out outlier spams
effectively. In the following experiment, the threshold values for
distinguishing spams among outlier ranking values are set to be
11.9%, 16.9%, 12.2%, and 4.1%, respectively, for datasets of Kaggle
SMS, Metsis Email, Twitter Trending, and Twitter Normal.
4.3 Performance Comparison
We conduct extensive experiments on the four aforementioned
datasets to compare our method with the existing unsupervised
and supervised methods listed in Section 4.1.
Comparing to Unsupervised Methods. We run the existing un-
supervised methods and our outlier method for 10 times on each
dataset and calculate the averaged values of precision, recall, and
F1 score. Table 4 lists the complete results of our method and of
existing unsupervised solutions on four datasets. We observe our
method has the precision and recall of 89.6%, 79.4%, of 85.2%, 71.2%,
of 88.7%, 82.6%, and of 85.1%, 78.4%, respectively, for the Kaggle
SMS, Metsis Email, Twitter Trending, and Twitter Normal datasets.
When comparing to other unsupervised solutions, our solution
is observed to clearly outperform in terms of three performance
metrics, except for the recall measure of Gibbs under Metsis Email
dataset. This demonstrates the advantage of our method in terms of
performance improvement. The reason is that our method explores
the generating process of data, by iteratively refining data for use.
Since a generative model renders a better learning capacity over a
discriminative model [31], our method is thus able to capture spam
patterns more precisely.
Comparing to Supervised Methods. We run the supervised meth-
ods listed in Section 4.1 on all four datasets via varying the ratio
of training set over test set (denoted as r) from 1 : 10 to 1 : 1
for comparison with our method. Due to the page limit, we only
include the detailed results from the Twitter Normal dataset un-
der various ratios of training set over test set, as listed in Table 5.
In the experiment, we keep the test set size as 1 million, and in-
crease the training set from 0.1 million, 0.2 million, 0.33 million,
0.5 million to 1 million. The corresponding ratios of training set
size over to test set size (i.e., r) are 1 : 10, 1 : 5, 1 : 3, 1 : 2 and
1 : 1. From this table, we observe the performance of all supervised
methods improves with the increasing of r, i.e., the increasing of
training set size. However, the results of our solution do not change
since it is an unsupervised method, without relying on the training
dataset. It is also observed that when r = 1
5 and r = 1
3,
our method can beat all supervised methods in terms of all three
performance metrics. When r = 1
2, our solution still outperforms all
supervised solutions in terms of recall. For precision, all the super-
vised methods except SVM outperform our method. When r = 1, all
supervised methods outperform ours in at least two metrics. These
results demonstrate that all supervised methods highly rely on the
training set size, with their performance level rising for a bigger
training set size. However, in a large dataset, it is impractical to
10, r = 1
0.00.10.20.3Spamthresholdvalues0.000.250.500.751.00ValueprecisionrecallF1-score1072ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Yihe Zhang, Xu Yuan, and Nian-Feng Tzeng
Table 4: Comparisons of precision and recall under our solution and under unsupervised methods for all four datasets
Dataset
Metric (%)
Alien-l [30]
Alien-s [30]
OUSLD [34]
JSF [58]
Hashing [12]
Gibbs [15]
Our Method
Kaggle SMS
Rec
51.1
52.9
42.5
44.6
41.4
55.0
79.4
Prs
79.4
78.9
54.4
50.7
39.7
64.5
89.6
Metsis Email
Rec
59.4
51.5
57.9
34.9
37.1
72.0
71.2
Prs
82.2
81.8
58.2
70.7
39.5
63.9
85.2
Twitter Trending Twitter Normal
Prs
79.4
78.5
61.1
60.6
41.5
45.2
88.7
Rec
60.6
58.5
52.5
38.7
47.2
66.2
78.4
Rec
53.3
56.3
51.4
44.4
44.1
67.1
82.6
Prs
74.1
77.9
60.0
57.7
46.3
51.5
85.1
Table 5: Comparisons of supervised methods and our solu-
tion with various ratios of training set over test set, with the
size of test set fixed to 1 million
r
1
10
1
5
1
3
1
2
1
1
Metric (%)
Prs
Rec
Prs
Rec
Prs
Rec
Prs
Rec
Prs
Rec
Bayes
80.2
32.0
81.8
38.5
82.5
48.1
84.6
63.6
88.0
74.5
C4.5 Ada
73.2
76.5
48.6
37.4
81.0
79.6
52.5
44.9
83.1
80.8
50.2
58.2
87.4
87.0
60.1
61.5