### Outlier Scores and Spam Detection

Our system generates outlier scores, with red crosses and blue points representing spam and legitimate (ham) messages, respectively. The figures demonstrate that our method effectively highlights the outlier characteristics of spam messages across all four datasets. In these datasets, most spam messages have higher outlier scores than ham messages, typically appearing in the leftmost sections.

Although the outlier property is evident, setting an appropriate threshold to distinguish between spam and ham remains a challenge. To address this, we propose a novel approach for intelligently determining a suitable spam threshold value.

### Determining the Spam Threshold

The ranked values of outlier scores in Figure 7 follow an exponential distribution. We fit these values using an exponential function, \( L = e^{a + bF} \), where \( a \) and \( b \) are parameters, and \( F \) is the ranking value for a given outlier score. This regression curve, known as the Lorenz curve, is illustrated in Figure 8.

To define the spam threshold, we take the leftmost point on the Lorenz curve, which has the highest score, and draw a tangent line at this point. The tangent intersects the x-axis at point \( k \). Any point to the left of \( k \) is considered an outlier, defining the Spam Zone, which is colored orange in Figure 8.

Next, we draw another tangent line with a slope of -1, intersecting the x-axis at point \( j \). The region between points \( k \) and \( j \) is defined as the Uncertain Zone, while the region to the right of \( j \) is the Ham Zone. The Uncertain Zone and Ham Zone are colored yellow and blue, respectively.

Table 3 summarizes the number of spam and ham messages in each zone for each dataset. For messages in the Spam Zone and Ham Zone, we can confidently classify them as spam and ham, respectively. However, messages in the Uncertain Zone are difficult to classify definitively. Assuming 50% of messages in the Uncertain Zone are spam, the estimated spam ratio \( R \) in a target dataset is given by:
\[ R = \frac{n_1 + 0.5 \times n_2}{N} \]
where \( n_1 \) and \( n_2 \) are the message counts in the Spam Zone and Uncertain Zone, respectively, and \( N \) is the total number of messages in the dataset. Based on this estimation, the spam ratios for the four datasets are 11.9%, 16.9%, 12.2%, and 4.1%, respectively. These estimates are very close to the true spam ratios of 13.4%, 20.0%, 14.6%, and 6.1%.

### Performance Evaluation

Figure 9 shows the precision, recall, and F1 score for different threshold values in the Kaggle SMS dataset. The best performance is achieved with a threshold between 11% and 13%, closely matching the true and estimated spam ratios. This suggests that our estimated spam ratios can be reliably used to filter out spam messages.

In subsequent experiments, the threshold values for distinguishing spam among outlier rankings are set to 11.9%, 16.9%, 12.2%, and 4.1% for the Kaggle SMS, Metsis Email, Twitter Trending, and Twitter Normal datasets, respectively.

### Performance Comparison

#### Unsupervised Methods

We compare our method with existing unsupervised methods on the four datasets. Table 4 lists the results, showing that our method outperforms other unsupervised solutions in terms of precision and recall, except for the recall measure of Gibbs under the Metsis Email dataset. Our method's advantage lies in its ability to iteratively refine data, leveraging the better learning capacity of generative models over discriminative models.

#### Supervised Methods

We also compare our method with supervised methods listed in Section 4.1. Due to space constraints, Table 5 provides detailed results for the Twitter Normal dataset with varying ratios of training set to test set (denoted as \( r \)) from 1:10 to 1:1. The test set size is fixed at 1 million, and the training set size increases from 0.1 million to 1 million. The performance of supervised methods improves with increasing training set size, but our unsupervised method's performance remains consistent. When \( r = 1/5 \) and \( r = 1/3 \), our method outperforms all supervised methods. At \( r = 1/2 \), our solution still excels in recall, but some supervised methods surpass it in precision. When \( r = 1 \), all supervised methods outperform ours in at least two metrics. These results highlight the dependency of supervised methods on the size of the training set.

### Conclusion

Our proposed method effectively identifies spam messages by leveraging outlier scores and a novel threshold determination technique. It outperforms both unsupervised and supervised methods in various scenarios, demonstrating its robustness and reliability.