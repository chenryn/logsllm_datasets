I'd love to be able to use scrapy's `Selector` outside of scrapy, as an xpath
(and css?) selector library for xml, html, sgml documents.  
I suppose I should go looking at BeautifulSoup for that, but I'm familiar with
`Selector` and like the way it works. I don't like plain lxml's way of doing
things and usually don't need any write-abilities on the tree anyway.
Would it be feasible and sensible to make the selector parts of scrapy not
depend on `SomeResponse` and expect response.url objects (but `SomeDocument`
instead) and put it together with things such as `LxmlDocument`, then have
some glue-code to bind it to object_ref and response objects and such in
scrapy?
At least in my mind this makes sense with the "librarization" goal of scrapy
in mind, but maybe I'm wrong or that'd be too much of a design change when
there is BeautifulSoup.
p.s. before anyone comments that it is possible: I know I _can_ use it outside
scrapy, with something like this (but thats still having all of scrapy inside
another project or rewriting a lot to factor out the selector stuff):
    namespaces = [
        ('x',    'http://www.w3.org/1999/xhtml'),
    ]
    with open('file') as f:
        data = f.read()
    xs = Selector(text=data, type='xml')
    for namespace, scheme in namespaces:
        xs.register_namespace(namespace, scheme)
    print(xs.xpath('/x:html').extract())