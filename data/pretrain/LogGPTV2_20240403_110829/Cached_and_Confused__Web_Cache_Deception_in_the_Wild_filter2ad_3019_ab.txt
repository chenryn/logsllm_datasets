caching rules in effect.
Next, the attacker uses social engineering channels to lure
a victim into visiting this URL, which would result in the
incorrect caching of the victim’s private information. The
attacker would then repeat the request and gain access to the
cached contents. Figure 1 illustrates these interactions.
In Step 1 , the attacker tricks the victim into visiting a URL
that requests /account.php/nonexistent.jpg. At a ﬁrst
glance this appears to reference an image ﬁle, but in fact does
not point to a valid resource on the server.
In Step 2 , the request reaches the web server and is pro-
cessed. The server in this example applies rewrite rules to
discard the non-existent part of the requested object, a com-
mon default behavior for popular web servers and application
frameworks. As a result, the server sends back a success re-
sponse, but actually includes the contents of account.php
in the body, which contains private details of the victim’s
account. Unaware of the URL mapping that happened at the
server, the web cache stores the response, interpreting it as a
static image.
Finally, in Step 3 , the attacker visits the same URL which
results in a cache hit and grants him unauthorized access to
the victim’s cached account information.
Using references to non-existent cacheable ﬁle names that
are interpreted as path parameters is an easy and effective
path confusion technique to mount a WCD attack, and is
the original attack vector proposed by Gil. However, we dis-
cuss novel and more advanced path confusion strategies in
Section 5. Also note that the presence of a Cache-Control:
no-store header value has no impact in our example, as it
is common practice to enable caching rules on proxy ser-
vices that simply ignore header instructions and implement
aggressive rules based on path and ﬁle extension patterns (see
Section 2.1).
WCD garnered signiﬁcant media attention due to its se-
curity implications and high damage potential. Major web
cache technology and CDN providers also responded, and
some published conﬁguration hardening guidelines for their
customers [8, 9, 43]. More recently, Cloudﬂare announced
options for new checks on HTTP response content types to
mitigate the attack [12].
Researchers have also published tools to scan for and detect
WCD, for instance, as an extension to the Burp Suite scanner
or as stand-alone tools [31, 54]. We note that these tools
are oriented towards penetration testing, and are designed to
perform targeted scans on web properties directly under the
control of the tester. That is, by design, they operate under
certain pre-conditions, perform information disclosure tests
via simple similarity and edit distance checks, and otherwise
require manual supervision and interpretation of the results.
This is orthogonal to the methodology and ﬁndings we present
in this paper. Our experiment is, instead, designed to discover
WCD vulnerabilities at scale in the wild, and does not rely on
page similarity metrics that would result in an overwhelming
number of false positives in an uncontrolled test environment.
2.4 Other Related Work
Caching mechanisms in many Internet technologies (e.g.,
ARP, DNS) have been targeted by cache poisoning attacks,
which involve an attacker storing a malicious payload in a
cache later to be served to victims. For example, James Kettle
recently presented practical cache poisoning attacks against
caching proxies [37, 38]. Likewise, Nguyen et al. demon-
strated that negative caching (i.e., caching of 4xx or 5xx error
responses) can be combined with cache poisoning to launch
denial-of-service attacks [47]. Although the primary goal of a
cache poisoning attack is malicious payload injection and not
private data disclosure, these attacks nevertheless manipulate
web caches using mechanisms similar to web cache deception.
Hence, these two classes of attacks are closely related.
More generally, the complex ecosystem of CDNs and their
critical position as massively-distributed networks of caching
reverse proxies have been studied in various security con-
texts [26, 56]. For example, researchers have explored ways
to use CDNs to bypass Internet censorship [22, 29, 67], ex-
ploit or weaponize CDN resources to mount denial-of-service
attacks [11, 60], and exploit vectors to reveal origin server
addresses behind proxies [34, 65]. On the defense front, re-
searchers have proposed techniques to ensure the integrity
of data delivered over untrusted CDNs and other proxy ser-
vices [40, 42, 44]. This research is orthogonal to WCD, and is
not directly relevant to our results.
668    29th USENIX Security Symposium
USENIX Association
Figure 1: An illustrated example of web cache deception. Path confusion between a web cache and a web server leads to
unexpected caching of the victim’s private account details. The attacker can then issue a request resulting in a cache hit, gaining
unauthorized access to cached private information.
3 Methodology
We present our measurement methodology in three stages:
(1) measurement setup, (2) attack surface detection, and
(3) WCD detection. We illustrate this process in Figure 2.
We implemented the tools that perform the described tasks
using a combination of Google Chrome and Python’s Re-
quests library [52] for web interactions, and Selenium [53]
and Google Remote Debugging Protocol [25] for automation.
3.1 Stage 1: Measurement Setup
WCD attacks are only meaningful when a vulnerable site
manages private end-user information and allows performing
sensitive operations on this data. Consequently, sites that pro-
vide authentication mechanisms are prime targets for attacks,
and thus also for our measurements. The ﬁrst stage of our
methodology identiﬁes such sites and creates test accounts on
them.1
Domain Discovery. This stage begins by visiting the sites
in an initial measurement seed pool (e.g., the Alexa Top n
1In the ﬁrst measurement study we present in Section 4, we scoped our
investigation to sites that support Google OAuth [51] for authentication due to
its widespread use. This was a design choice made to automate a signiﬁcant
chunk of the initial account setup workload, a necessity for a large-scale
experiment. In our follow-up experiment later described in Section 5 we
supplemented this data set with an additional 45 sites that do not use Google
OAuth. We discuss these considerations in their corresponding sections.
domains). We then increase site coverage by performing sub-
domain discovery using open-source intelligence tools [1, 27,
50]. We add these newly-discovered sub-domains of the pri-
mary sites (ﬁltered for those that respond to HTTP(s) requests)
to the seed pool.
Account Creation. Next, we create two test accounts on
each site: one for a victim, and the other for an attacker. We
populate each account with unique dummy values. Next, we
manually explore each victim account to discover data ﬁelds
that should be considered private information (e.g., name,
email, address, payment account details, security questions
and responses) or user-created content (e.g., comments, posts,
internal messages). We populate these ﬁelds with predeﬁned
markers that can later be searched for in cached responses to
detect a successful WCD attack. On the other hand, no data
entry is necessary for attacker accounts.
Cookie Collection. Once successfully logged into the sites
in our seed pool, crawlers collect two sets of cookies for all
victim and attacker accounts. These are saved in a cookie jar to
be reused in subsequent steps of the measurement. Note that
we have numerous measures to ensure our crawlers remain
authenticated during our experiments. Our crawlers period-
ically re-authenticate, taking into account cookie expiration
timestamps. In addition, the crawlers use regular expressions
and blacklists to avoid common logout links on visited pages.
USENIX Association
29th USENIX Security Symposium    669
VictimAttackerWeb CacheWeb ServerGET /account.php/nonexistent.jpg 200 OKCache-Control: no-store (!) GET /account.php/nonexistent.jpg 200 OK (!) 123Figure 2: A high-level overview of our WCD measurement methodology.
Table 1: Sample URL grouping for attack surface discovery.
Group By
URL
Query Parameter
Path Parameter
http://example.com/?lang=en
http://example.com/?lang=fr
http://example.com/028
http://example.com/142
3.2 Stage 2: Attack Surface Detection
Domain Crawls.
In the second stage, our goal is to map
from domains in the seed pool to a set of pages (i.e., complete
URLs) that will later be tested for WCD vulnerabilities. To
this end, we run a recursive crawler on each domain in the
seed pool to record links to pages on that site.
URL Grouping. Many modern web applications customize
pages based on query string or URL path parameters. These
pages have similar structures and are likely to expose similar
attack surfaces. Ideally, we would group them together and
select only one random instance as a representative URL to
test for WCD in subsequent steps.
Since performing a detailed content analysis is a costly
process that could generate an unreasonable amount of load on
the crawled site, our URL grouping strategy instead focuses
on the structure of URLs, and approximates page similarity
without downloading each page for analysis. Speciﬁcally, we
convert the discovered URLs into an abstract representation
by grouping those URLs by query string parameter names or
by numerical path parameters. We select one random instance
and ﬁlter out the rest. Table 1 illustrates this process.
This ﬁltering of URLs signiﬁcantly accelerates the mea-
surements, and also avoids overconsumption of the target
site’s resources with redundant scans in Stage 3. We stop
attack surface detection crawls after collecting 500 unique
pages per domain for similar reasons.
WCD Attack. The attack we mount directly follows the
scenario previously described in Section 2.3 and illustrated in
Figure 1. For each URL:
1. We craft an attack URL that references a non-existent
static resource. In particular, we append to the original
page “/.css”2. We use a random string as the
ﬁle name in order to prevent ordinary end-users of the
site from coincidentally requesting the same resource.
2. We initiate a request to this attack URL from the victim
account and record the response.
3. We issue the same request from the attacker account,
and save the response for comparison.
4. Finally, we repeat the attack as an unauthenticated user
by omitting any session identiﬁers saved in the attacker
cookie jar. We later analyze the response to this step
to ascertain whether attackers without authentication
credentials (e.g., when the site does not offer open or
free sign ups) can also exploit WCD vulnerabilities.
Marker Extraction. Once the attack scenario described
above is executed, we ﬁrst check for private information dis-
closure by searching the attacker response for the markers that
were entered into victim accounts in Stage 1. If victim mark-
ers are present in URLs requested by an attacker account, the
attacker must have received the victim’s incorrectly cached
content and, therefore, the target URL contains an exploitable
WCD vulnerability. Because these markers carry relatively
high entropy, it is probabilistically highly unlikely that this
methodology will produce false positives.
Secret Extraction. We scan the attacker response for the
disclosure of secret tokens frequently used as part of web
application security mechanisms. These checks include com-
mon secrets (e.g., CSRF tokens, session identiﬁers) as well
3.3 Stage 3: WCD Detection
In this ﬁnal stage, we launch a WCD attack against every URL
discovered in Stage 2, and analyze the response to determine
whether a WCD vulnerability was successfully exploited.
2Our choice to use a style sheet in our payload is motivated by the fact
that style sheets are essential components of most modern sites, and also
prime choices for caching. They are also a robust choice for our tests. For
instance, many CDN providers offer solutions to dynamically resize image
ﬁles on the CDN edge depending on the viewport of a requesting client
device. Style sheets are unlikely to be manipulated in such ways.
670    29th USENIX Security Symposium
USENIX Association
AlexaTop 5KMeasurement SetupDomain DiscoveryAccount CreationCookie CollectionDomain CrawlsURL GroupingWCD AttackMarker ExtractionSecret ExtractionAttack Surface DetectionWCD Detectionas any other application-speciﬁc authentication and autho-
rization tokens (e.g., API credentials). We also check for
session-dependent resources such as dynamically-generated
JavaScript, which may have private information and secrets
embedded in them (e.g., as explored by Lekies et al. [39]).
In order to extract candidates for leaked secrets, we scan at-
tacker responses for name & value pairs, where either (1) the
name contains one of our keywords (e.g., csrf, xsrf, token,
state, client_id), or (2) the value has a random compo-
nent. We check for these name & value pairs in hidden HTML
form elements, query strings extracted from HTML anchor
elements, and inline JavaScript variables and constants. Sim-
ilarly, we extract random ﬁle names referenced in HTML
script elements. We perform all tests for randomness by ﬁrst
removing dictionary words from the target string (i.e., us-
ing a list of 10,000 common English words [35]), and then
computing Shannon entropy over the remaining part.
Note that unlike our checks for private information leaks,
this process can result in false positives. Therefore, we per-
form this secret extraction process only when the victim and
attacker responses are identical (a strong indicator of caching),
or otherwise when we can readily conﬁrm a WCD vulner-
ability by searching for the private information markers. In
addition, we later manually verify all candidate secrets ex-
tracted in this step.
3.4 Veriﬁcation and Limitations
Researchers have repeatedly reported that large-scale Internet
measurements, especially those that use automated crawlers,
are prone to being blocked or served fake content by secu-
rity solutions designed to block malicious bots and content
scrapers [49, 66]. In order to minimize this risk during our
measurement, we used a real browser (i.e., Google Chrome)
for most steps in our methodology. For other interactions,
we set a valid Chrome user-agent string. We avoided gen-
erating excessive amounts of trafﬁc and limited our crawls
as described above in order to avoid triggering rate-limiting
alerts, in addition to ethical motivations. After performing our
measurements, we manually veriﬁed all positive ﬁndings and
conﬁrmed the discovered vulnerabilities.
Note that this paper has several important limitations, and
the ﬁndings should be considered a potentially loose lower
bound on the incidence of WCD vulnerabilities in the wild.
For example, as described in Section 4, our seed pool is biased
toward sites that support Google OAuth, which was a neces-
sary compromise to automate our methodology and render a
large-scale measurement feasible. Even under this constraint,
creating accounts on some sites required entering and veri-
fying sensitive information such as credit card or US social
security numbers which led to their exclusion from our study.
Furthermore, decisions such as grouping URLs based on
their structure without analyzing page content, and limiting
site crawls to 500 pages may have caused us to miss addi-
tional instances of vulnerabilities. Similarly, even though we
manually ﬁltered out false positives during our secret token
extraction process and veriﬁed all ﬁndings, we do not have
a scalable way of detecting false negatives. We believe that
these trade-offs were worthwhile given the overall security
beneﬁts of and lessons learned from our work. We emphasize
that the results in this paper represent a lower bound.
3.5 Ethical Considerations
Here, we explain in detail important ethical considerations
pertaining to this work and the results we present.
Performance Considerations. We designed our methodol-
ogy to minimize the performance impact on scanned sites and
inconvenience imposed on their operators. We did not perform
repeated or excessive automated scans of the targeted sites,
and ensured that our measurements did not generate unrea-
sonable amounts of trafﬁc. We used only passive techniques
for sub-domain enumeration and avoided abusing external
resources or the target site’s DNS infrastructure.
Similarly, our stored modiﬁcations to crawled web applica-
tions only involved creating two test accounts and ﬁlling out
editable ﬁelds with markers that we later used for data leakage
detection. We believe this will have no material impact on site
operators, especially in the presence of common threats such
as malicious bots and credential stufﬁng tools that generate
far more excessive junk trafﬁc and data.
Security Considerations. Our methodology entirely
avoids jeopardizing the security of crawled sites or their
end-users. In this work, we never injected or stored any
malicious payload to target sites, to web caches on the
communication path, or otherwise maliciously tampered
with any technology involved in the process. Likewise, the
experiments we performed all incorporated randomized
strings as the non-existent parts of URLs, thereby preventing
unsuspecting end-users from accidentally accessing our
cached data and receiving unexpected responses.
Note that this path randomization measure was used to
prevent inconveniencing or confusing end-users; since we
never exploited WCD to leak real personal data from a web
application or stored a malicious payload, our work never
posed a security risk to end-users.
Our experiments did not take into account robots.txt ﬁles.
This was a risk-based decision we consciously made, and
we believe that ignoring exclusion directives had no negative
impact on the privacy of these sites’ visitors. Robots.txt is not
a security or privacy mechanism, but is intended to signal to
data aggregators and search engines what content to index –
including a directive to exclude privacy sensitive pages would
actually be a misuse of this technology. This is not relevant to
our experiments, as we only collect content for our analysis,
and we do not index or otherwise publicly present site content.
USENIX Association
29th USENIX Security Symposium    671
Responsible Disclosure.
In this paper, we present a de-
tailed breakdown of our measurement ﬁndings and results
of our analysis, but we refrain from explicitly naming the
impacted sites. Even though our methodology only utilized
harmless techniques for WCD detection, the ﬁndings point at
real-world vulnerabilities that could be severely damaging if
publicly disclosed before remediation.
We sent notiﬁcation emails to publicly listed security con-
tacts of all impacted parties promptly after our discovery. In
the notiﬁcation letters we provided an explanation of the
vulnerability with links to online resources and listed the vul-