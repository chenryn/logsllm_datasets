permission set granted to the subjects when accessing the
objects, Ar = {a = (s, o, cr, p) | s ∈ Sr, o ∈ Or, p ∈ Pr}
is the set of all access patterns deﬁned by this rule, i.e.,
Ar = Sr × Or × {cr} × Pr.
Here, we extend the policy rule r with the set of concrete
access patterns Ar that this rule deﬁnes to allow. Note that,
the access patterns collected from runtime access events
(e.g., required by certain functionality) could be inconsistent
with the access patterns deﬁned by the policy rules, due to
the knowledge gap, which SPOKE is designed to address.
2.3 Android Functional Testing
A functional test examines whether a speciﬁc functional
component meets the design requirement, by feeding an in-
put and checking the expected output. In Android testing,
functional tests are developed using Android testing frame-
work, which is an integral part in the oﬃcial Android devel-
opment environment. With standard libraries such as An-
droidJUnitRunner, UI Automator [1], Android functional
tests are well organized and closely associated with design
requirements and end user operations. This makes such tests
self-explanatory and inherently carry rich semantics of the
functionality under test. Examples include checking speciﬁc
API functions (Unit test), clicking or typing on UI widgets
(UI test), and setting up an email account (Integration test).
We hypothesize that Android functional tests can enable
a systematic way to synchronize domain knowledge between
developers and policy engineers, providing a knowledge foun-
dation for the attack surface analysis of SEAndroid policy.
However, it is non-trivial to extract domain knowledge
from functional tests. Android functional tests are orig-
inally designed to test high-level functional operations in
Android application or framework layer, while low-level ac-
cess patterns in native layer are implicitly involved and thus
still obscure behind the scene. How to extract low-level
functionality-related access patterns while excluding test-
only/non-functional noise is one task that SPOKE is de-
signed to address.
Impact of Functional Test Coverage: By design, SPOKE
relies on functional tests as inputs. Test coverage can aﬀect
SPOKE’s performance. However, test coverage is orthogonal
to SPOKE, because one of our contributions is to leverage
the outcomes of both industrial practice and research eﬀorts
in the ﬁeld of software testing, to enhance the security anal-
ysis of SEAndroid policy.
Speciﬁcally, in the software industry, multiple coverage-
measuring tools [3] are developed to ensure the high test
coverage. As test-driven development (TDD) [13] is a pop-
ular software engineering practice, many Android testing
tools and frameworks are actively used in the industry (e.g.,
Testdroid [11], AWS device farm [2]).
Increasing test coverage is also an active research topic
in software testing [14, 17, 32, 41]. Various techniques have
been developed for automated testing and test input gener-
ation of mobile applications. For example, Dynodroid [29]
is an automated test input generation system for Android
apps. Swifthand [16] is a guided GUI testing system for An-
droid apps based on machine learning. Symbolic and con-
colic executions are also used to generate event sequence for
automated testing of Android apps [27].
2.4 Deﬁnitions introduced by SPOKE
We further use the following deﬁnitions to introduce sev-
eral new concepts used in this work.
Definition 4
(Functionality Trace). A functional-
ity trace is a set of descriptive items that can describe the
execution semantics of the functionality. In functional tests,
the following concrete and semantic code-level items can be
collected as descriptive items:
• Metadata of a functional test, such as test_class,
test_case, @annotation in a JUnit test
• Key function calls/control ﬂow within the execution of
a functionality, such as API calls
Intuitively, a descriptive item shows one aspect of a func-
tionality. By monitoring the runtime execution of the func-
tional test, we obtain concrete and speciﬁc items that de-
scribe how the functionality works from multiple aspects and
granularities. An example is (test_addFirewallRule(),
{Firewall.addRule(), Firewall.setIptablesOption()}),
where the ﬁrst item shows a high-level functional operation
under test, and the rest two items provide code-level details
of the functionality. In SPOKE, such descriptive items are
3
correlated with low-level access patterns, providing a full
picture of a functionality.
A knowledge base stores correlated functionality trace and
access patterns in a uniﬁed form. Formally, we deﬁne this
as the following.
Definition 5
(Knowledge Base). A knowledge base
is a set of pairs K = {(a, f )}, where ‘a’ denotes an access
pattern extracted from runtime access events in kernel layer
(Section 4.1.2) and ‘f ’ denotes a functionality trace collected
from Dalvik layer (Section 4.1.3).
In practice, K is stored in a database. We can query the
database to ﬁnd all access patterns correlated with a given
functionality trace f , i.e., Af = {a | isCorrelatedK(a, f ) =
T rue}, where isCorrelatedK(a, f ) is used to denote whether
they are correlated in K (used in Section 4.2.2).
A potentially unnecessary access pattern is one that is
deﬁned by a policy rule but not found in the knowledge base
and thus cannot be justiﬁed by the knowledge base. To be
more precise and formal, we deﬁne “potentially unnecessary”
as “unjustiﬁed w.r.t. the knowledge base”.
Definition 6
(Policy Rule Justification w.r.t. K).
A policy rule r is said to be justiﬁed with respect to a knowl-
edge base K, if for each access pattern ar deﬁned in Ar, there
is an equivalent access pattern a in K, correlated with at least
one functionality trace (ar = a and isCorrelatedK(a, f ) =
T rue). A rule is said to be partially justiﬁed (or unjusti-
ﬁed) if only a subset (or none) of Ar have equivalent access
patterns in the knowledge base.
As an example, by running a functional test test_addFi-
rewallRule(), policy engineers are able to learn the domain
knowledge based on a functionality trace:
android.app.enterprise.Firewall.addRule()
correlated with the access pattern:
(‘system_server’, ‘/system/bin/iptables’,
‘file’, ‘execute’)
and thus justiﬁes a policy rule allow system_server ipt-
ables_exec:file {execute}, which allows this access pat-
tern.
Among the access patterns that cannot be justiﬁed w.r.t.
K, we further focus on the over-permissive access pattern
which is deﬁned as following.
Definition 7
(Over-permissive Access Pattern).
An over-permissive access pattern is an unjustiﬁed access
pattern (s, o, c, p) deﬁned by a policy rule that can poten-
tially allow attackers to misuse or exploit the subject ‘s’ to
maliciously access the object ‘o’ with permission ‘p’, in order
to compromise the conﬁdentiality or integrity of ‘o’ 2.
3. PROBLEM AND ASSUMPTIONS
Problem Statement: In this paper, we seek to reduce the
attack surface of an SEAndroid policy by identifying po-
tentially unnecessary, and particularly over-permissive ac-
cess patterns allowed by the policy. To this end, we need
to bridge the knowledge gap between policy engineers and
functionality developers, which causes the necessary access
patterns to be unclear. Although functional tests can be
helpful with valuable domain knowledge, it is non-trivial to
2
In practice, over-permissive rules lead to over-privileged subjects
extract functionality-required low-level access patterns, as
they are implicitly involved in such high-level tests with ir-
relevant noises. Once the domain knowledge is extracted,
a system is also needed to match the knowledge with pol-
icy rules and provide attack surface analysis. SPOKE is
designed to address the aforementioned problems.
Assumptions: We assume that functionality developers
use functional tests to verify the design and execution logic
of the functionality, which is consistent with the industrial
practice. We therefore assume that functionality-required
access patterns can be extracted by running such tests. As
noted in Section 2.3, test coverage is orthogonal to SPOKE.
We also assume that tests need to be executed on real de-
vices because some functionalities require hardware features
such as ARM TrustZone. Before running each test, devices
are in the same clean state as being ready for normal user
operation. We also assume that target functionalities are
correctly implemented and already passed the tests success-
fully. Functionalities should involve multi-layer operations
in Android, which can incur native access patterns and thus
are visible by low-level SEAndroid access control. This is
the typical case for system functionalities in Android frame-
work. No malicious operations exist since tests are executed
in a clean state. Hence, all access patterns related to func-
tionalities (exclude test-only operations) should be allowed.
4. SPOKE
SPOKE is a novel test-driven SEAndroid POlicy Knowl-
edge Engine that achieves the three capabilities:
C1: Building an up-to-date knowledge base of various func-
tionalities and their corresponding access patterns to
bridge the knowledge gap between developers and pol-
icy engineers.
C2: Matching policy rules with corresponding functional-
ity traces and access patterns. The output of this pro-
cess is used as the basis to justify proper policy rules
and reveal unjustiﬁed policy rules, with respect to the
knowledge base.
C3: Analyzing the attack surface of unjustiﬁed policy rules
to pinpoint risky rules that allow potentially unneces-
sary and over-permissive access patterns that could be
misused by attackers.
The design of SPOKE is based on a key insight that An-
droid functional tests are rich semantic resources provided
by functionality developers that can be used to enable sys-
tematic and scalable domain knowledge collection. The col-
lected domain knowledge can then help policy engineers an-
alyze SEAndroid policy rules, including revealing whether
policy rules can be justiﬁed or not by corresponding func-
tionality, and identifying over-permissive policy rules that
are potentially exploitable by attackers.
Domain knowledge collection is achieved by a two-
step process. First, given a functional test, SPOKE exe-
cutes it in a scalable test running and multi-layer knowl-
edge extraction platform. The platform collects both high-
level functionality trace in Android framework layer and low-
level access patterns in Android native layer. By doing so,
SPOKE captures a full and detailed picture of how the func-
tionality works, which represents the domain knowledge of
the functionality.
4
Figure 1: SPOKE consists of three components from collecting test-driven tracing logs to generating ﬁnal report by the analysis engine.
Second, SPOKE parses the collected knowledge from dif-
ferent layers in diﬀerent forms into a knowledge base and
performs a cross-layer correlation to organize them in a uni-
ﬁed form. The correlation is based on multiple global vari-
ables (e.g., timestamp, process/user id) shared across layers
that align and match high-level functionality trace with low-
level access patterns. In addition, SPOKE also parses SE-
Android policy rules into a structural form, and associates
the rules with the access patterns that these rules are deﬁned
to allow.
Policy rule justiﬁcation w.r.t. K is the ﬁrst analysis
capability designed to use the collected knowledge. It per-
forms a matching between the access patterns deﬁned in the
policy rules and the access patterns correlated with func-
tionality trace in the knowledge base. The output of this
process checks whether the policy rules, speciﬁcally their
deﬁned access patterns, can be matched and therefore justi-
ﬁed by the corresponding functionality. On the other hand,
it also reveals the risky policy rules whose deﬁned access
patterns cannot be justiﬁed by current knowledge base.
Attack surface analysis is the second analysis capabil-
ity that further focuses on the risky policy rules and their de-
ﬁned but unjustiﬁed access patterns revealed from the above
step. Such unjustiﬁed access patterns not only lack critical
tests, but could also be unnecessary and over-permissive,
because they might allow potentially vulnerable subjects to
access valuable objects. To identify such over-permissive
access patterns, policy engineers use SPOKE to ﬁnd valu-
able/critical objects in the knowledge base, based on corre-
lated and rich-semantic functionality trace. Then SPOKE
searches the unjustiﬁed access patterns and highlights the
ones that can access these critical objects. This helps pol-
icy engineers pinpoint risky rules and corresponding over-
permissive access patterns with concrete evidence.
Figure 1 shows the major components in SPOKE that im-
plement the above three capabilities. The device farm with
multi-layer logging realizes the ﬁrst step of domain knowl-
edge collection. The knowledge base with cross-layer corre-
lation is the second step. The analysis engine provides both
policy rule justiﬁcation w.r.t. K and attack surface analysis.
In practice, it also provides visualization to illustrate the
attack surface results. The following sections explain more
details of each component.
4.1 Domain Knowledge Collection
To build a knowledge base, collecting runtime logs of func-
tional tests is the ﬁrst step for knowledge extraction. As we
focus on functionalities involving multi-layer operations, we
need to capture suﬃcient logs from each layer to get a full
picture of the functionality. Thus, SPOKE collects logs from
three layers: Linux kernel layer, Dalvik VM layer and An-
droid native layer with a distributed collecting mechanism,
as explained in the following sections.
4.1.1 Distributed Multi-layer Collection
Collecting runtime logs is not a straightforward task, espe-
cially in the case of low-level access events, which are based
on system calls. Given the high calling rate and large vol-
ume of system calls (i.e., GB-sized), it is necessary to sup-
port both high-rate and large-volume logging to capture all
access events, with the capability of keeping track of spe-
ciﬁc logging targets to identify diﬀerent process subjects
and ﬁle objects. Unfortunately, the logging buﬀer in one
device has upper-bound limitations of both speed and vol-
ume. Even with the maximum setting of the device, critical
access events are found to be missed in practice.
To address this challenge, we design a distributed logging
mechanism.
Inspired by distributed computing, we group
a set of identical Android devices (same model with same
setting) as a device farm. A centralized manager conﬁgures
each device to focus on speciﬁc logging targets. The overall
work load of logging a functional test is then divided and dis-
tributed to each device with a reduced logging work load, so
that diﬀerent logging targets are collected in parallel with-
out reaching each device’s logging limitation. For example,