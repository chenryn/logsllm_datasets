C. Intermediate Poison Attack
The DP-SGD privacy analysis assumes the existence of
an adversary who has complete access to the model training
pipeline,
this includes intermediate gradients computed to
update the model parameter values throughout training. Instead
of the ﬁnal model only, we now assume the Distinguisher is
given access to these intermediate model parameters values, as
shown in Protocol 2. (In the case of convex models, it is known
that releasing all intermediate models gives the adversary no
more power than just the ﬁnal model, however there is no
comparable theory for the case of deep neural networks.)
L ← maxT
(x, y), threshold τ, attack method (max or mean)
Distinguisher 2 White-box Membership attack
Require: all intermediate steps to step T f1,··· , fT , query input
1: if attack method is max then
2:
i=1 (cid:96)(fθi , x, y)
3: end if
4: if attack method is mean then
5:
6: end if
7: if L < τ then
return D(cid:48)
8:
9: end if
10: return D
i=1 (cid:96)(fθi , x, y)
(cid:80)T
L ← 1
T
Intermediate Model Adversary Game
Model Trainer
Adversary
(D, D(cid:48))
$← A
B = (D, D)
b
$← {0, 1}
i=1 ← T (Bb)
{fθi}N
{fθ1, fθ2 , . . . , fθN}
s ← B({fθi}, D, D(cid:48))
s ∈ {0, 1}
Check if s = b
Protocol 2: Adversary game for intermediate model adver-
sary. Round 1. The adversary chooses two datasets. Round
2. The model trainer randomly trains a model on one of these,
and returns the full sequence of model updates. Round 3. The
adversary predicts which dataset was used for training.
a) Crafter: (Crafter 2) Identical to prior.
b) Model
training: The model
trainer gets the two
datasets D and D(cid:48) from the Crafter. As before we train on one
of these two datasets. However, this time, the trainer reveals
all intermediate models from the stochastic gradient descent
process {fθi}N
i=1 to the Distinguisher.
c) Distinguisher: (Distinguisher 2) Given the sequence
of trained model parameters, we now construct our second
adversary to guess which dataset was selected by the trainer.
We modify Crafter 1 to leverage access to the intermediate
outputs of training. Instead of only looking at
the ﬁnal
model’s loss on the poisoned instance, our adversary also
analyzes intermediate losses. We compute either the average
or maximum loss for the poisoned examples over all of the
intermediate steps and guess the model was trained on D
if the loss is smaller than a threshold. We took the best
attacker between the maximum and average variants. In our
experiments, for smaller epsilons (ε ≤ 2) the maximum
worked better. For epsilons larger than 2, it was instead the
average. Distinguisher 2 outlines the resulting attack.
d) Results: Our results in Figure 5 show that this ad-
versary only slightly outperforms our previous adversary with
access to the ﬁnal model parameters only. This suggests that
access to the ﬁnal model output by the training algorithm leaks
ε
l
a
c
i
t
c
a
r
P
10
5
0
MNIST
Purchase
CIFAR
Theoretical
1
2
4
Theoretical ε
10
Fig. 5: Malicious input attack: the adversary has white-box
access to the training dataset. The results are slightly better
compared to the malicious input with blackbox access.
almost as much information as the gradients applied during
training. This matches what the theory for convex models
suggests, even though deep neural networks are non-convex.
D. Adaptive Poisoning Attack
Recall that the DP-SGD analysis treats each iteration of
SGD independently, and uses (advanced) composition methods
to compute the ﬁnal privacy bounds. As a result, there is no
requirement that the dataset processed at iteration i need be
the same as the dataset used at another iteration j.
We design new adversaries to take advantage of this ad-
ditional capability. As in the previous attack, our goal is to
design two datasets with the property that training on them
yields different models. Again, we will use a query input to
distinguish between the models, but for the ﬁrst time we will
not directly place the query input in the training data. Instead,
we will insert a series of (different) poison inputs, each of
which is designed to make the models behave differently on
the query input. We describe this process in detail below.
a) Crafter: (Crafter 3) We ﬁrst generate a query input
(xq, yq) by calling Crafter 2 from the prior section: this is
the input that the Distinguisher will use to make its guess.
Then, on each iteration of gradient descent, we generate a
fresh example (x, y) such that
if the model f trains on
D\{(x, y)}, we expect that the loss of the query input xq
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
874
Adaptive Input Poisoning Adversary Game
Model Trainer
Adversary
$← {0, 1}
$← Aquery
(x, y)
b
(D, D(cid:48))
fθ0
$← A(fθi, x, y)
Bi = {D, D(cid:48)}
b, x, y), θi)
θi+1 ← S(B(Bi
fθi+1
...
s ← B({fθi}, D, D(cid:48))
s ∈ {0, 1}
s
n
o
i
t
a
r
e
t
I
N
t
a
e
p
e
R
Check if s = b
Protocol 3: Adversary game for adaptive input poisoning
attack. Round 0. The adversary generates a query input.
Round 2i. The adversary chooses datasets D, D(cid:48) given the
current weights θi. Round 2i+1. The model trainer trains
for one minibatch on one of these datasets. Round N. The
adversary predicts which dataset was used for training.
will be signiﬁcantly larger than training on the full D, i.e.
(cid:96)(f D\{(x,y)}, xq, yq) (cid:28) (cid:96)(f D, xq, yq).
To generate the malicious input x, we perform double
backpropagation. We compute the gradient of the query input’s
loss, given a model trained on x. Then, we temporarily apply
this gradient update, and then again take a gradient this time
updating x so that it will minimize the loss of the query input.
This process is described in Crafter 3.
a\Da
Crafter 3 Dynamic Malicious Generation
Require: current model fθT , train steps T , input learning rate s,
model learning rate lr
a) ← Crafter 2
1: (Da, D(cid:48)
2: xquery, yquery ← D(cid:48)
3: x, y ← (cid:126)0
4: for T times do
5:
6:
7:
8:
9: end for
10: D(cid:48) = D ∪ {(x, y)}
11: return D, D(cid:48)
lmodel ← (cid:96)(f, x, , y)
f(cid:48)(x) ← f (x) − lr × ∇fθT
lmodel
lmalicious ← (cid:96)(f(cid:48), xquery, yquery)
x ← x + s∇xlmalicious
b) Model training: As usual, the model trainer randomly
chooses to train on the benign dataset D or the malicious
dataset D(cid:48). If D is selected,
training proceeds normally.
Otherwise, before each iteration of training, the trainer runs
Crafter 3 ﬁrst to get an updated malicious D(cid:48) and selects a
mini-batch from the given dataset. Recall that here, we are
interested in the attack for the purpose of establishing a lower
bound so while this attack assumes strong control from the
ε
l
a
c
i
t
c
a
r
P
10
5
0
MNIST
Purchase
CIFAR
Theoretical
1
2
4
Theoretical ε
10
Fig. 6: Adaptive malicious input attack:
the adversary
changes the training dataset in each iteration. The adversary
can achieve better results compared to prior attacks.
adversary on the dataset, it allows us to more accurately bound
any potential privacy leakage.
c) Distinguisher:
(Distinguisher 2) This adversary is
unchanged from the prior section, as described in Distin-
guisher 2. However, as indicated above, we use the query
input (xquery, yquery) instead of the poison examples that are
inserted into the training data.
d) Results: Figure 6 summarizes results for this setting.
Compared to the previous attack, the adversary now obtains a
tighter lower bound on privacy. For example, on the Purchase
dataset with ε = 2, this adaptive poisoning attack can achieve
a lower-bound εlower = 0.37 compared to 0.25 in the prior
section. Having the ability to modify the training dataset at
each iteration was the missing key to effectively exploit access
to the intermediate model updates and obtain tighter bounds.
E. Gradient attack
By taking a closer look at the DP-SGD formulation, we
see that the analysis assumes the adversary is allowed to
control not only the input examples x, but
the gradient
updates ∇θ(cid:96)(fθ, x, y) themselves. The reason this is allowed
is because the clipping and noising is applied at the level
of gradient updates, regardless of how they were obtained.
Thus, even though we intuitively know that the update vector
was generated by taking the gradient of some function, the
analysis does not make this assumption anywhere. While this
assumption might not be true in every setting, in federated
learning [37, 30, 48] the participants can directly modify the
gradient vectors, which can be a possible setting to deploy
such an attack. Nevertheless, we are interested in evaluating
this adversary primarily to understand what additional power
this gives the adversary.
All prior attacks relied on measuring the model’s loss on
some input example to distinguish between the two datasets.
Unlike the previous attacks, the Distinguisher this time will
directly inspect the weights of the neural network.
a) The Crafter: (Crafter 4) The Crafter inserts a water-
mark into the model parameters. To minimize the effect of
modifying these parameters, the adversary selects a set of
Authorized licensed use limited to: Tsinghua University. Downloaded on February 25,2022 at 12:26:44 UTC from IEEE Xplore.  Restrictions apply. 
875
Gradient Poisoning Adversary Game
Adversary
Model Trainer
$← {0, 1}
b
fθ0
G,G(cid:48) $← A(fθi)
Bi = {G(cid:48),G}
B ← B(Bi
b)
Bdp ← {clipp(x) + N : x ∈ B}
x
x∈Bdp
θi+1 ← θi − η(cid:80)
s ← B({fθi},G)
fθi+1
...
s ∈ {0, 1}
s
n
o
i
t
a
r
e
t
I
R
t
a
e
p
e
N
Check if s = b
Protocol 4: Adversary game for gradient poisoning attack.
Round 2i. The adversary chooses a collection of gradients
G,G(cid:48). Round 2i+1. The model trainer chooses a subset of one
of these gradient updates, and updates the parameters using
the clipped and noised gradients. Round 2N. The adversary
predicts which dataset was used for training.
Crafter 4 Gradient attack
Require: intermediate models up to step T f 1,··· , f t, clipping
norm C, number of poison parameters 2n, number of measure-
ments To, training dataset D
1: M ←(cid:80)min(To,T )−1
|f t+1 − f t|
t=1
s = −s
2: points ← select smallest 2n arguments fromM
3: ∇malicious ← (cid:126)0
4: s ← 1
5: for p in points do
6: ∇malicious[p] = s C√
7:
8: end for
9:
10: G ← {}
11: for (x, y) ∈ D do
12:
13: end for
14: return G,G ∪ ∇malicious
G.insert(∇l(f, x, y))
2n
to obtain a malicious gradient. It also selects a batch from
the training dataset and computes the corresponding private
gradient update, then with probability q it adds the malicious
gradient from Crafter 4. The model trainer reveals the model
parameters for all of the intermediate steps to the attacker.
c) The Distinguisher: (Distinguisher 3) Given the col-
lection of model weights {θi}N
i=1, the Distinguisher adversary
will directly inspect the model weights to make its guess.
Similar to the Crafter, the adversary observes the gradient
of the ﬁrst to iterations to ﬁnd which 2n model parameters
has the smallest overall absolute value. Since the Crafter tries
to increase the distance between these model parameters, to
detect if such watermark exist in the model parameters, the
Distinguisher computes the distance between the parameters,
and then performs hypothesis testing to detect the presence
of a watermark. The distance from the set of parameters
either come from the gradient distribution (Z) plus the added
Gaussian noise (i.e, null hypothesis), or are watermarked
which means they come from the gradient distribution plus
the added Gaussian noise plus the watermark:
Hnull : Z + N (0, σ2) Hwatermark : Z + N (