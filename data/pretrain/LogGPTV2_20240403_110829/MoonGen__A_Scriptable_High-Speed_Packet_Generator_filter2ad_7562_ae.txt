NICs in our testbed drop such packets early in the receive
ﬂow: they are dropped even before they are assigned to a
receive queue, the NIC only increments an error counter [11,
12, 13]. Subsequently, the packet processing logic is not af-
fected by this software rate control mechanism.
Figure 9 illustrates this concept. Shaded packets pi
j are
sent with an incorrect CRC checksum, all other packets pk
with a correct one. Note that the wire and all transmission
queues are completely ﬁlled, i.e., the generated rate has to
be the line rate.
284]
%
[
n
o
i
t
a
i
v
e
D
e
v
i
t
a
l
e
R
8
6
4
2
0
−2
0
1st quartile Median
3rd quartile
0.5
1
1.5
Oﬀered Load [Mpps]
]
s
µ
[
y
c
n
e
t
a
L
180
160
140
120
100
80
60
40
20
0
CBR (median)
CBR (25th/75th percentile)
Poisson (median)
Poisson (25th/75th percentile)
0
0.5
1
1.5
2
Oﬀered Load [Mpps]
Figure 10: Diﬀerences in forwarding latencies of
Open vSwitch with CBR traﬃc generated by hard-
ware and our software approach
In theory, arbitrary inter-packet gaps should be possible.
The NICs we tested refused to send out frames with a wire-
length (including Ethernet preamble, start-of-frame delim-
iter, and inter-frame gap) of less than 33 bytes, so gaps
between 1 and 32 bytes (0.8 ns to 25.6 ns) cannot be gen-
erated. Generating small frames also puts the NIC under
an unusually high load for which it was not designed. We
found that the maximum achievable packet rate with short
frames is 15.6 Mpps on Intel X540 and 82599 chips, only
5% above the line rate for packets with the regular minimal
size. MoonGen therefore enforces a minimum wire-length of
76 bytes (8 bytes less than the regular minimum) by default
for invalid packets. As a result, gaps between 0.8 ns and
60.8 ns cannot be represented.
8.2 Evaluation
We generate CBR traﬃc with our approach and compare
it to CBR traﬃc generated by the hardware facilities of our
NIC by comparing the response of a DuT.
We use Intel’s hardware implementation as reference gen-
erator. The same measurement with other software-based
packet generators is not possible as they don’t support ac-
curate timestamping. However, the results from Section 7.4
indicate that the latency would be aﬀected at low rates due
to the measurably diﬀerent interrupt rate (cf. Figure 7).
Figure 10 shows the diﬀerence of the 25th, 50th, and 75th
percentiles of the forwarding latency on a server running
Open vSwitch. The test is restricted to the range 0.1 Mpps
to 1.9 Mpps as the DuT becomes overloaded at higher rates
and the latency is a function of the buﬀer size of the DuT
after this point. We repeated the whole test 10 times, the
error bars in the ﬁgure show the resulting standard devia-
tions. The relative deviation is within 1.2σ of 0% for almost
all measurement points, only the 1st quartile at 0.23 Mpps
deviates by 1.5%±0.5%. Minor activity on the DuT, e.g., an
active SSH session, shows a signiﬁcantly larger (≥ 10%) ef-
fect on the latency with both rate control methods. This
shows that loading the DuT with a large number of invalid
packets does not cause system activity; the DuT does not
notice the invalid packets.
8.3 Example: Poisson Trafﬁc
CBR traﬃc is often an unrealistic test scenario for mea-
surements of latency. Bursts or a Poisson process allow for
Figure 11: Forwarding latency of Open vSwitch with
CBR and Poisson traﬃc patterns
more sophisticated tests that also stress buﬀers as the DuT
becomes temporarily overloaded.
Figure 11 shows measured latencies of Open vSwitch con-
ﬁgured to forward packets between two ports. We gener-
ate packets with CBR (hardware rate control) and Poisson
(CRC-based software rate control) traﬃc patterns and com-
pare their latencies. The outlier at 0.4 Mpps for CBR traﬃc
was reproducible across multiple re-measurements on diﬀer-
ent servers. The sudden drop in latency before the system
becomes overloaded was also reproducible. Both are likely
artifacts of the interaction between the interrupt throttle
algorithm found in the Intel driver [10] and the dynamic in-
terrupt adaption of Linux [25] on the DuT. The artifacts are
present regardless of how the CBR traﬃc is generated (cf.
Figure 10), so this is not caused by MoonGen but an eﬀect
of the traﬃc pattern on DuT.
The system becomes overloaded at about 1.9 Mpps, result-
ing in packet drops and a very large latency (about 2 ms in
this test setup) as all buﬀers are ﬁlled. The overall achieved
throughput is the same regardless of the traﬃc pattern and
method to generate it. This result shows that the traﬃc
pattern can aﬀect the DuT in an experiment measurably,
underlining the importance of a reliable precise packet gen-
erator.
8.4 Limitations of our Approach
A shorter per-byte transmission time improves both the
granularity and the minimum length that can be generated.
This means our solution works best on 10 GbE where the
granularity is 0.8 ns.
Due to the artiﬁcially enforced minimum size of 76 bytes,
gaps between 1 and 75 bytes (0.8 ns to 60 ns) cannot be pre-
cisely represented (cf. Section 8.1). It is possible to reduce
this range for tests with larger packets or lower rates. We
approximate gaps that cannot be generated by occasionally
skipping an invalid packet and increasing the length of other
gaps. The overall rate still reaches the expected average with
this technique, i.e., the accuracy is high but the precision is
relatively low3 for these delays.
A possible work-around for gaps with a length between
1 and 75 bytes is using multiple NICs to generate traﬃc
that is sent to a switch. The switch then drops the invalid
frames and multiplexes the diﬀerent streams before forward-
3Note that ±30 ns is still better than hardware rate control
and other software solutions (cf. Section 7.3).
285ing them to the DuT. This only works if the generated pat-
tern can be split into multiple streams, e.g., by overlaying
several Poisson processes.
However, short delays are often not meaningful in mod-
ern networks. For example, the 10GBASE-T transmission
standard used by most experiments for this paper operates
on frames with a payload size of 3200 bits on the physical
layer as deﬁned in IEEE 802.3 Section 4 55.1.3.1 [9]. This
means that any layers above the physical layer will receive
multiple packets encoded in the same frame as a burst. So
two back-to-back packets cannot be distinguished from two
packets with a gap of 232 bytes (185.6 ns) in the worst case
and failure to represent gaps between 1 and 75 bytes should
not be noticeable. Note that this limit on the physical layer
only applies to relatively short inter-arrival times, bad rate
control generating bursts is still inferior to our approach
(cf. Figure 7 in Section 7.3).
Another limitation is that our approach is optimized for
experiments in which the DuT (or the ﬁrst hop in a system
under test) is a software-based packet processing system and
not a hardware appliance. Hardware might be aﬀected by an
invalid packet. In such a scenario, we suggest to route the
test traﬃc through a store-and-forward switch that drops
packets with invalid CRC checksums. This eﬀectively re-
places the invalid packets with real gaps on the wire. Note
that the eﬀects of the switch on the inter-arrival times need
to be carefully evaluated ﬁrst.
9. REPRODUCIBLE RESEARCH
We encourage you to install MoonGen and reproduce the
results from this paper to verify our work. All experiments
presented here can be reproduced with the included example
scripts and NICs based on Intel 82599, X540, 82580, and
XL710 chips.
The performance evaluation in Section 5 is based on the
scripts found in examples/benchmarks, an Intel Xeon E5-
2620 v3 CPU, and Intel X540 NICs. The timestamping
accuracy in Section 6 was measured with the script time-
stamps.lua, the clock drift measurements with drift.lua.
Inter-arrival times in Section 7 were measured with inter-
arrival-times.lua. The script l2-load-latency.lua with
the timestamping task disabled was used to generate the
analyzed traﬃc. The suggested work-around for the hard-
ware rate control limitations at high rates is also imple-
mented in l2-load-latency.lua. Sending bursty traﬃc is
implemented in l2-bursts.lua. The example measurement
of the interrupt rate in Section 7.4 was conducted with l2-
load-latency.lua and zsend 6.0.2.
compare-rate-control-mechanisms.lua was used for the
evaluation in Section 8.2. The latency measurements with
Poisson and CBR traﬃc in Section 8.3 are based on l2-
and l2-poisson-load-latency.lua.
load-latency.lua
The DuT for these tests was Open vSwitch 2.0.0 on De-
bian Linux 3.7 with ixgbe 3.14.5 running on a server with
a 3.3 GHz Intel Xeon E3-1230 v2 CPU. Only a single CPU
core was used by conﬁguring the NIC with only one queue.
Each test was run for at least 30 seconds with at least 30 000
timestamped packets.
All measurements were conducted on Intel X540 NICs
except for the inter-arrival times (Intel 82580), ﬁber loop-
back measurements (Intel 82599), and 40 GbE tests (Intel
XL710). We used diﬀerent development versions of Moon-
Gen for the experiments described throughout this paper.
The performance evaluation with 10 GbE in Section 5 was
done with commit 492c0e4, and on 40 GbE with commit
a70ca21. We conﬁrmed that all described experiments still
work with the example scripts from commit a70ca21 in our
git repository [5].
10. CONCLUSIONS AND FUTURE WORK
We have presented a general-purpose packet generator
that uses hardware features of commodity NICs to imple-
ment functionality that was previously only available on ex-
pensive special-purpose hardware. MoonGen represents a
hybrid between a pure software-based solution and one based
on hardware. It combines the advantages of both approaches
while mitigating shortcomings by using both hardware-
speciﬁc features and novel software approaches.
MoonGen measures latencies with sub-microsecond accu-
racy and precision. The desired packet rate can be controlled
precisely through both hardware-support and our rate con-
trol algorithm based on ﬁlling gaps with invalid packets.
We have shown that it is feasible to use modern imple-
mentations of scripting languages to craft packets without
sacriﬁcing speed. This makes MoonGen ﬂexible and extensi-
ble. The ﬂexibility goes beyond the capabilities provided by
hardware load generators as each packet can be crafted in
real-time by a script. Tests that respond to incoming traﬃc
in real-time are possible as MoonGen also features packet
reception and analysis.
In the future, we will add additional example scripts and
support for hardware features of more NICs. MoonGen cur-
rently comes with example scripts to handle IPv4, IPv6,
UDP, TCP, ICMP, IPsec, and ARP traﬃc.
MoonGen’s ﬂexible architecture allows for further applica-
tions like analyzing traﬃc in line rate on 10 GbE networks or
doing Internet-wide scans from 10 GbE uplinks. MoonGen
is under active development, the latest version is available
in our public git repository [5].
Acknowledgments
This research was supported by the DFG MEMPHIS project
(CA 595/5-2), the KIC EIT ICT Labs on SDN, and the
EUREKA-Project SASER (01BP12300A).
We would like to thank the anonymous reviewers and our
colleagues Dominik Scholz, Johannes Reiﬀerscheid, Rainer
Sch¨onberger, Patrick Werneck, Lukas M¨ardian, Lukas Er-
lacher, and Stephan M. G¨unther for valuable contributions
to MoonGen and this paper.
11. REFERENCES
[1] Nicola Bonelli, Andrea Di Pietro, Stefano Giordano,
and Gregorio Procissi. Flexible High Performance
Traﬃc Generation on Commodity Multi–Core
Platforms. In Proceedings of the 4th International
Conference on Traﬃc Monitoring and Analysis, pages
157–170. Springer, 2012.
[2] Alessio Botta, Alberto Dainotti, and Antonio Pescap´e.
Do You Trust Your Software-Based Traﬃc Generator?
IEEE Communications Magazine, 48(9):158–165,
2010.
[3] Scott Bradner and Jim McQuaid. Benchmarking
Methodology for Network Interconnect Devices. RFC
2544 (Informational), March 1999.
286[4] G. Adam Covington, Glen Gibb, John W. Lockwood,
[18] Ntop. PF RING ZC (Zero Copy).
and Nick Mckeown. A Packet Generator on the
NetFPGA Platform. In 17th IEEE Symposium on
Field Programmable Custom Computing Machines,
pages 235–238, 2009.
[5] Paul Emmerich. MoonGen.
https://github.com/emmericp/MoonGen.
http://www.ntop.org/products/pf_ring/pf_ring-
zc-zero-copy/. Last visited 2015-04-28.
[19] Srivats P. ostinato. http://ostinato.org/. Last
visited 2015-08-24.
[20] Mike Pall. LuaJIT. http://luajit.org/. Last visited
2015-08-24.
[6] Sebastian Gallenm¨uller, Paul Emmerich, Florian
[21] Mike Pall. LuaJIT in realtime applications.
Wohlfart, Daniel Raumer, and Georg Carle.
Comparison of Frameworks for High-Performance
Packet IO. In ACM/IEEE Symposium on
Architectures for Networking and Communications
Systems (ANCS 2015), May 2015.
[7] Luke Gorrie. Snabb Switch.
https://github.com/SnabbCo/snabbswitch/.
[8] IEEE Standard for a Precision Clock Synchronization
Protocol for Networked Measurement and Control
Systems. IEEE 1588-2008, July 2008.
[9] IEEE. IEEE 802.3-2012 IEEE Standard for Ethernet
Section Four, 2012.
[10] Intel. Intel Server Adapters - Linux ixgbe Base Driver.
http://www.intel.com/support/network/adapter/
pro100/sb/CS-032530.htm. Last visited 2015-08-24.
[11] Intel 82580EB Gigabit Ethernet Controller Datasheet
Rev. 2.6. Intel, 2014.
[12] Intel 82599 10 GbE Controller Datasheet Rev. 2.76.
Intel, 2012.
[13] Intel Ethernet Controller X540 Datasheet Rev. 2.7.
Intel, 2014.
[14] Data Plane Development Kit. http://dpdk.org/.
Last visited 2015-08-24.
[15] Intel Ethernet Controller XL710 Datasheet Rev. 2.1.
Intel, December 2014.
[16] Product Brief - Intel Ethernet Controller XL710 10/40
GbE. Intel, 2014.
[17] NetFPGA. http://netfpga.org/. Last visited
2015-08-24.
http://www.freelists.org/post/luajit/LuaJIT-
in-realtime-applications,3, July 2012. Mailing list
post.
[22] Luigi Rizzo. The netmap project.
http://info.iet.unipi.it/~luigi/netmap/. Last
visited 2015-08-24.
[23] Luigi Rizzo. netmap: A Novel Framework for Fast
Packet I/O. In USENIX Annual Technical Conference,
pages 101–112, 2012.
[24] Charalampos Rotsos, Nadi Sarrar, Steve Uhlig, Rob
Sherwood, and Andrew W Moore. Oﬂops: An Open
Framework for OpenFlow Switch Evaluation. In
Passive and Active Measurement, pages 85–95.
Springer, 2012.
[25] Jamal Hadi Salim, Robert Olsson, and Alexey
Kuznetsov. Beyond Softnet. In Proceedings of the 5th
Annual Linux Showcase & Conference, volume 5,
pages 18–18, 2001.
[26] Joel Sommers and Paul Barford. Self-Conﬁguring
Network Traﬃc Generation. In Proceedings of the 4th
ACM SIGCOMM Conference on Internet
Measurement, IMC ’04, pages 68–81, New York, NY,
USA, 2004. ACM.
[27] Keith Wiles. Pktgen-DPDK.
http://github.com/Pktgen/Pktgen-DPDK/.
[28] Yinglin Yang, Sudeep Goswami, and Carl G. Hansen.
10GBASE-T Ecosystem is Ready for Broad Adoption,
2012. White paper.
287