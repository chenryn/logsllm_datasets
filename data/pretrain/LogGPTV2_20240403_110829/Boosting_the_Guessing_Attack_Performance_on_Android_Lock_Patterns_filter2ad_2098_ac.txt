To improve the performance of the smug attack, it is essential
to ﬁnd an optimal n-gram Markov model. Therefore, we experi-
mented with a number of reasonable n-gram Markov models with
varying n and smoothing techniques. As described in Section 3.4,
probability of zero can be assigned to some n-grams since some
patterns may not exist in the training set. This can be avoided by ap-
plying a well-known preprocessing technique called “smoothing,”
which assigns pseudo-counts to unseen events.
It is a necessary
process to avoid probability estimates that are zero for events that
are not present in training samples. There are various smoothing
techniques that can be used [14].
“Laplace smoothing” is a commonly used technique that works
by adding k pseudo-counts to the count of each event. In our ex-
periments, we simply used a special case of Laplace smoothing
with k= 1, which is popularly used in many applications such as
based on the total probability of events that appear exactly i+ 1
language modeling. Notation Add-n is used to refer to an n-gram
Markov model used with one additional pseudo-count for an event.
“Good-Turing smoothing” [11] is a more sophisticated technique,
which estimates the total probability of events that appear i times
times. In particular, the total probability for all unseen events in a
given dataset can be estimated by the total probability of items that
appear only once. We use GT-n as the notation to refer to n-gram
Markov model used with the Good-Turing smoothing technique.
To ﬁnd the best performing Markov model conﬁguration, we an-
alyzed the performance of smug attack under various n-Markov
models. First, we computed the average number of guessing at-
tempts without limiting the number of guesses on failed attacks.
Smug attack is successful if it unlocks a device within 20 guessing
attempts; if not, it is considered as an unsuccessful attack. Second,
we computed a more conservative average value, only considering
the successfully attacked cases. Last, we counted the total num-
ber of successfully cracked patterns within 20 guessing attempts.
Those results are presented in Table 1.
GT-2 showed the highest percentage of cracked patterns at 69.89%
although the average number of guessing attempts for successfully
cracked patterns (≤ 20) was 4.92 which is greater than the other
models. Therefore, we performed all subsequent smug attacks us-
ing GT-2.
To measure the efﬁciency of smug attack, we analyzed the aver-
age time to complete each step in a smug attack. As described in
Section 3, our smug attack implementation can roughly be divided
into two phases: (1) image processing to infer possible patterns
from smudges, and (2) sorting patterns based on the occurrence
probabilities computed using an n-gram Markov-model. With our
prototype implementation, the ﬁrst step took about 8.31 seconds on
average (with 0.56 standard deviation), and the second step took
about 9.71 seconds on average (with 24.18 standard deviation).
Hence, in total, it took only about 18.02 seconds on average to
complete smug attack.
In the second step, the standard deviation is quite large because
the processing time is highly dependent on the number of possible
pattern candidates identiﬁed, and this number can vary quite a lot
based on what the actual pattern is and the volume of smudges left
behind.
6. SECOND RESULTS: SMUG ATTACK PER-
FORMANCE
This section presents the results collected from performing the
fully optimized smug attack, GT-2 (see above), on all four different
mobile usage scenarios, and shows how the smug attack perfor-
mance can be affected by the increasing level of smudge obscurity.
6.1 Methodology
For the second experiment, we recruited seven male and ﬁve fe-
male (twelve in total) Android users to play the role of a victim.
All participants were university students with varying hand sizes in
their early and mid 20s.
In our study, we asked each participant to ﬁrst unlock Samsung
Galaxy S4 with 5 inch touchscreen (same as the ﬁrst experiment)
using a pattern that was randomly chosen from the testing pattern
set of 93 patterns which are different from the 219 patterns used for
constructing the n-gram Markov model, and perform three real-
world mobile device usage scenarios (see Table 2): Calling some-
one, texting someone, and using the Facebook app. Those addi-
tional tasks mimic some common real-world mobile device usage
scenarios. In the “using the Facebook app” task, for example, each
participant was asked to freely use the Facebook app for a few min-
utes – this task was designed to closely resemble the participants’
real-world use of their mobile devices. Each participant repeated
this process 30 times with a different pattern. After each round, we
took a picture of the smudges left behind, cleaned the touchscreen,
and reconﬁgured the device unlock setting with the next pattern. In
consequence, 360 randomly selected patterns were tested among
all 12 participants. Each participant was rewarded with USD 60
after completing all of those tasks, taking about 3 hours on average
to complete everything. We never explained the research purposes
to the participants.
Table 2: Procedures for user tasks.
Task
A. Unlocking
screen only
B. Calling
C. Texting
D. Using the
Facebook app
Procedures
1. Draw the given pattern to unlock the
device.
1. Draw the given pattern to unlock the
device.
2. Start the phone call app.
3. Enter a phone number randomly chosen
from the address book by explicitly press-
ing all the numbers, and make a call.
1. Draw the given pattern to unlock the
device.
2. Start the instant messenger app.
3. Type given text messages randomly se-
lected from real messages sent previously.
1. Draw the given pattern to unlock the
device.
2. Start the Facebook app.
3. Perform a normal, daily action (e.g.,
writing a new post, replying to a com-
ment, liking a post) on the Facebook app.
6.2 Smug attack performance
To show how effective the optimized smug attack is, we compare
the attack success rate of the smug attack against a pure Markov
model-based guessing attack. First, we evaluated the performance
of several Markov models, without any smudge support, on the
ﬁrst naive scenario where the participants merely unlocked phones.
The 3-gram Markov model with Laplace smoothing (see the top 20
most likely used patterns in Appendix F) produced the best results,
cracking 48 out of 360 patterns (13.33%) with 7.31 guesses on av-
erage (≤ 20). This result alone is signiﬁcant, but the smug attack
patterns (74.17%) from the same pattern set (p 5 segments).
6.3 Effects of smudge obscurity
This section analyzes the effects of different mobile device us-
age scenarios on the performance of the smug attack. Progress-
ing from the naive device unlocking task to the Facebook task (see
Table 2), the number of actions the participant had to perform in-
creased, and as a result, the volume of obscuring smudges left on
the touchscreen increased.
Intuitively, the performance of smug
attack should downgrade as the volume of obscuring smudges in-
creases, and this is exactly what we observed from those different
real-world usage scenarios. The same smug attack, with GT-2 con-
ﬁguration, was performed on the smudges collected from all of the
four tasks; however, we did not use the longest pattern ﬁrst trial
strategy on the calling, texting and Facebook tasks because addi-
tional smudge objects that are not relevant to the user’s original
pattern could have been created while performing those tasks (see
Table 2).
The smug attack success rates for unlocking device, calling, tex-
ting, and Facebook tasks were 74.17% (the average percentage
among 12 participants were σ= 10.90%), 52.50% (σ= 11.72%),
37.22% (σ = 9.89%), and 31.94% (σ = 9.95%), respectively,
showing a gradual decrease in the smug attack performance with
the increase in the volume of obscuring smudges (see Table 3). To
measure the statistical signiﬁcance between those attack success
rates, we performed the Fisher’s Exact Test (FET) with Bonferroni
correction. The attack success rate for the naive device unlock-
ing task was signiﬁcantly higher than the rates for all other three
tasks (all p< 0.001, pairwise corrected FET). Similarly, the suc-
ferences against both the texting task and the Facebook task (p<
cess rate for the calling task showed statistically signiﬁcant dif-
0.005, pairwise corrected FET). The average numbers of guessing
attempts were 3.79, 4.43, 5.36, and 4.82 for the four tasks, respec-
tively. Considering that there is about 42.23% difference in the
attack success rate between the naive device unlocking task and the
Facebook task, it is clear that obscuring smudges have a signiﬁcant
impact on the smug attack performance or on the performance of
smudge-supported attacks in general.
6.4 False positive and false negative analysis
For more detailed investigation on the effects of smudge obscu-
rity, we analyzed the characteristics of frequently misclassiﬁed seg-
ments in each of the three (calling, texting, and Facebook) tasks.
Across all the three additional tasks, we computed the false posi-
tive and false negative rates for each pattern segment, and compared
their rates with the rates computed for the naive device unlocking
task. We performed the Fisher’s Exact Test to identify rate dif-
ferences between pattern segments that are statistically signiﬁcant
(see Appendix E). “FP segment” denotes a non-existing segment
that is identiﬁed more frequently as an existing segment in one of
the three additional tasks compared to the device unlocking task.
“FN segment” denotes an existing segment that is identiﬁed more
frequently as a non-existing segment in one of the three additional
tasks compared to the device unlocking task.
For the calling task, we found just 6 FP segments, which were
mainly located in the upper left hand side of the pattern grid (see
Figure 9). It is hard to explain why such non-existing segments
were more frequently identiﬁed as existing segments when the smudge
obscurity increased. It might be due to the distribution of digits in
the phone numbers that the participant used to make calls (we did
ask participants to call a different person each time).
For the texting task, we found 4 FN segments and 9 FP segments,
which were mainly located around the lower part of the pattern grid
(see Figure 10). This is because for texting the participant mainly
interacts with the on-screen keyboard, which is located near the
lower part of the screen, affecting the lower part of the smudges
that were left from drawing a pattern.
For the Facebook task, we found 3 FN segments and 8 FP seg-
ments, which were mostly located on the right hand side of the
pattern grid (see Figure 11). We believe this is due to the partic-
ipant mainly scrolling up and down to view posts on his timeline,
which involves (in most cases) using the right thumb and ﬂicking
up or down on the right hand side of the touchscreen.
Table 3: Comparison of the smug attack performance across the four device usage scenarios.
Avg. # of guessing attempts
Avg. # of guessing attempts (≤ 20)
Total # of cracked patterns (≤ 20)
Unlocking only
4,634.66
3.79
Calling
6,811.83
4.43
Texting
9,783.01
5.36
Facebook
13,130.74
4.82
267 (74.17%)
189 (52.50%)
134 (37.22%)
115 (31.94%)
be used for FN segments. Further work is needed to try this kind of
optimization technique on those parameters.
7. MITIGATION STRATEGIES
In this section, we discuss three possible mitigation techniques
for smug attack. From the three, we implemented the ﬁrst tech-
nique that deliberately adds obscuring smudges by mandating users
to draw an additional random pattern upon log in, and evaluated its
effectiveness against smug attack. We explain this technique ﬁrst.
7.1 Adding obscurity
Our second experiment results (see Table 3) showed that adding
smudge obscurity by asking the participant to perform different
tasks on a device can signiﬁcantly degrade smug attack perfor-
mance. For instance, the performance decreased from 74.17% of
cracked patterns in the naive usage scenario to 31.94% in the Face-
book app usage scenario where participants left more irrelevant
smudges on the touchscreen. Such obscuring techniques can be
used to our advantage in mitigating smug attack: we could, for ex-
ample, ask users to draw additional random segments, which would
leave more redundant smudges on the touchscreen to obscure the
visibility. This technique is visualized in Figure 12. A user is ﬁrst
asked to draw the actual unlock pattern and this is displayed in
green. After unlocking his or her device, the user is then asked to
draw a given random pattern, which is shown in white. This sec-
ond pattern is a random pattern that is not stored anywhere—its
purpose is to simply get the user to draw more segments and leave
more smudges on the screen.
(a) Misclassiﬁed
(b) Task area
(c) Overlapped
Figure 9: Frequently misclassiﬁed segments found for the “call-
ing” task.
(a) Misclassiﬁed
(b) Task area
(c) Overlapped
Figure 10: Frequently misclassiﬁed segments found for the
“messaging” task.
(a) Misclassiﬁed
(b) Task area
(c) Overlapped
Figure 11: Frequently misclassiﬁed segments found for the
“Using the Facebook app” task.
Figure 12: An example pattern with an additional random pat-
tern drawn on top of it.
Those results suggest a clear limitation of smudge-based infer-
ence attacks, which will not perform well if a touchscreen has too
many obscuring smudges left behind (e.g., a device that has been
used by the victim for an hour or more without being cleaned).
Moreover, those results could be used to optimize the tool by
adjusting the smug attack parameters based on the tasks that a vic-
tim has performed on the stolen mobile device. For example, in
the segment decision procedure (see Section 3.3), a high threshold
value can be used for FP segments, and a low threshold value can
We implemented this obscurity based mitigation technique, and
asked the same 12 participants from the second experiment (see
Section 6) to draw each of the 30 patterns as well as a given ran-
dom (obscuring) pattern. For this experiment, the participants were
merely asked to perform the ﬁrst naive screen unlock task. Hence,
we compared the new smug attack performance against the base-
line performance, GT-2 (13.33% of cracked patterns), shown in Ta-
ble 3. The smug attack was performed without the heuristic that
tries longer patterns ﬁrst (see Section 3), expecting that such a
heuristic could be less effective when there are obscuring segments.
smug attack signiﬁcantly by increasing the possible pattern space.
We can see a pattern in Figure 14(b) that appears to contain the
same points (1, 2, 3 and 6) inferred from visible smudge residu-
als, but the actual pattern is “632123”, which is not only longer but
much more difﬁcult to infer from analyzing smudges. With this
amended policy, smug attack will now have to consider all possible
patterns that reuse points and segments, e.g., “1236321”, “123632”,
“21236”, and so on. With the current policy, there are 389,112 pos-
sible patterns, but with the amended policy, we can signiﬁcantly
increase the pattern space to 1,826,902,512. This is about 4,695
times larger than the original pattern space. Considering that An-
droid limits the number of failed attempts allowed to 20, this huge
increase in the pattern space will make it difﬁcult for smug attack
to succeed.
(a) 1-2-3-6
(b) 6-3-2-1-2-3
Figure 14: Non-repeated vs repeated points
7.3 Changing the pattern grid location
cation of the 3× 3 grid being static for a given Android device. It
Image processing techniques used by smug attack rely on the lo-
looks for smudges that match the location of the points and uses
them to identify possible patterns. Our smug attack tool uses the
pre-stored template images (see Section 3) to ﬁnd the exact grid
location from the input picture of the touchscreen, and to identify
smudges that match the location of the points and possible line seg-
ments. Our third mitigation strategy exploits this operation and