are several challenges involved when processing malicious
JavaScript for similarities. Attackers actively try to trigger
parsing issues in analyzers. The code is usually heavily
obfuscated, which means that statically examining the code
is not enough. The malicious code itself is designed to evade
signature detection from antivirus products. This renders
string-based and token-based code similarity approaches
ineffective against malicious JavaScript. We will show later
how regular code similarity tools, such as Moss [37], fail
when analyzing obfuscated scripts. In Revolver, we extend
tree-based code similarity approaches and focus on mak-
ing our system robust against malicious JavaScript. We
elaborate on our novel code similarity techniques in §3.4.
At a high-level overview, we use Revolver to detect and
understand the similarity between two code scripts. Intu-
itively, Revolver is provided with the code of both scripts
and their classiﬁcation by one or more honeyclient tools. In
our running example, we assume that the code in Figure 1
is ﬂagged as malicious and the one in Figure 2 as benign.
Revolver starts by extracting the Abstract Syntax Tree (AST)
corresponding to each script. Revolver inspects the ASTs
rather than the original code samples to abstract away possi-
ble superﬁcial differences in the scripts (e.g., the renaming
of variables). When analyzing the AST of Figure 2, it detects
that it is similar to the AST of the code in Figure 1. The
change is deemed to be interesting, since it introduces a
difference (the try-catch statement) that may cause a change
in the control ﬂow of the original program. Our system also
determines that the added code (the statement that tries to
USENIX Association  
22nd USENIX Security Symposium  639
load the ActiveX control) is indeed executed by tools visit-
ing the page, thus increasing the relevance of the detected
change (execution bits are described in more detail in §3.1).
Finally, Revolver classiﬁes the modiﬁcation as a possible
evasion attempt, since it causes the honeyclient to change its
detection result (from malicious to benign).
Assumptions and limitations. Our approach is based on a
few assumptions. Revolver relies on external detection tools
to collect (and make available) a repository of JavaScript
code, and to provide a classiﬁcation of such code as either
malicious or benign (i.e., Revolver is not a detection tool by
itself). To obtain code samples and classiﬁcation scores, we
can rely on several publicly-available detectors [6, 13, 25].
Attackers might write a brand new attack with all com-
ponents (evasion, obfuscation, exploit code) written from
scratch. In such cases, Revolver will not be able to ﬁnd any
similarities the ﬁrst time it analyzes these attacks. The lack
of similarities though can be used to our advantage, since
we can isolate brand-new attacks (provided that they can be
identiﬁed by other means) based on the fact that we have
never observed such code before.
In the same spirit, to detect evasions, Revolver needs to
inspect two versions of a malicious script: the “regular”
version, which does not contain evasive code, and the “eva-
sive” version, which attempts to circumvent detection tools.
Furthermore, if an evasion is occurring, we assume that a de-
tection tool would classify these two versions differently. In
particular, if only the evasive version of a JavaScript program
is available, Revolver will not be able to detect this evasion.
We consider this condition to be unlikely. In fact, trend
results from a recent Google study on circumvention [31]
suggest that malicious code evolves over time to incorporate
more sophisticated techniques (including evasion). Thus,
having a sufﬁciently large code repository should allow us
to have access to both regular and evasive versions of a
script. Furthermore, we have anecdotal evidence of mal-
ware authors creating different versions of their malicious
scripts and submitting them to public analyzers, until they
determine that their programs are no longer detected (this
situation is reminiscent of the use of anti-antivirus services
in the binary malware world [18]).
Revolver is not effective when server-side evasion (for
example, IP cloaking) is used: in such cases, the malicious
web site does not serve at all the malicious content to a de-
tector coming from a blocked IP address, and, therefore, no
analysis of its content is possible. This is a general limitation
of all analysis tools and can be solved by means of a better
analysis infrastructure (for example, by visiting malicious
sites from IP addresses and networks that are not known
to be associated with analysts and security researchers and
cannot be easily ﬁngerprinted by attackers).
3 Approach
In this section, we describe Revolver in detail, focusing
on the techniques that it uses to ﬁnd similarities between
JavaScript ﬁles.
A high-level overview of Revolver is presented in Figure 3.
First, we leverage an existing drive-by-download detection
tool (an “Oracle”) to collect datasets of both benign and
malicious web pages (§3.1). Second, Revolver extracts the
ASTs (§3.2) of the JavaScript code contained in these pages
and, leveraging the Oracle’s classiﬁcation for the code that
contains them, marks them as either benign or malicious.
Third, Revolver computes a similarity score for each pair of
ASTs, where one AST is malicious and the other one can
be either benign or malicious (§3.3–§3.4). Finally, pairs
that are found to have a similarity score higher than a given
threshold are further analyzed to identify and classify their
similarities (§3.5).
If Revolver ﬁnds similarities between two malicious
scripts, then we classify this case as an instance of evo-
lution (typically, an improvement of the original malicious
code). On the other hand, if Revolver detects similarities
between a malicious and a benign script, it performs an
additional classiﬁcation step. In particular, similarities can
be classiﬁed by Revolver into one of four possible categories:
evasions, injections, data dependencies, and general evolu-
tions. We are especially interested in identifying evasions,
which indicate changes that cause a script that had been
found to be malicious before to be ﬂagged as benign now.
It is important to note that, due to JavaScript’s ability to
produce additional JavaScript code on the ﬂy (which enables
extremely complex JavaScript packers and obfuscators),
performing this analysis statically would not be possible.
Revolver works dynamically, by analyzing all JavaScript
code that is compiled in the course of a web page’s execution.
By including all these scripts, and the relationships between
them (such as what code created what other code), Revolver
is able to calculate JavaScript similarities among malicious
web pages to an extent that is not, to our knowledge, possible
with existing state-of-the-art code comparison tools.
3.1 Oracle
Revolver relies on existing drive-by-download detection
tools for a single task: the classiﬁcation of scripts in web
pages as either malicious or benign. Notice that our approach
is not tied to a speciﬁc detection technique or tool; therefore,
we use the term “Oracle” to generically refer to any such
detection system. In particular, several popular low- and
high-interaction honeyclients (e.g., [6, 13, 25, 38]) or any
antivirus scanner can readily be used for Revolver.
Revolver analyzes the Abstract Syntax Trees (ASTs) of in-
dividual scripts rather than examining web pages as a whole.
Therefore, Revolver performs a reﬁnement step, in which
640  22nd USENIX Security Symposium 
USENIX Association
324#
-./012#
K/L2C#
%5MC#
N/>G6G/82#
;/6.C#
!"#
,#
$%&# '(# )*+#
!"#
,#
$%&# '(# )*+#
56761/.689#
0:7;#
,#
?46@#7AB#
,#
+/1606:#
E/8/FG2;2>G2>09#
H/D/50.6;8#6>I20=:>C#
JD/C6:>C#
Figure 3: Architecture of Revolver.
i) individual ASTs are extracted from the web pages obtained
from the Oracle, ii) their detection status is determined (that
is, each AST is classiﬁed as either benign or malicious),
based on the page classiﬁcation provided by the Oracle,
and iii) for each node in an AST, it is recorded whether the
corresponding statement was executed. Of course, if an
Oracle natively provides this ﬁne-grained information, this
step can be skipped.
More precisely, Revolver executes each web page using
a browser emulator based on HtmlUnit [1]. The emula-
tor parses the page and extracts all of its JavaScript con-
tent (e.g., the content of script tags and the body of
event handlers). In particular, the ASTs of the JavaScript
code are saved for later analysis. In addition, to obtain
the AST of dynamically-generated code, Revolver executes
the JavaScript code. At the end of the execution, for each
node in the AST, Revolver keeps an execution bit to record
whether the code corresponding to that node was executed.
Whenever it encounters a function that generates new code
(e.g., a call to the eval() or setTimeout() functions),
Revolver analyzes the code that is generated by these func-
tions. It also saves the parent-child relationship between
scripts, i.e., which script is responsible for the execution
of a dynamically-generated script. For example, the script
containing the eval() call is considered the parent of the
script that is evaluated. Similarly, Revolver keeps track of
which script causes network resources to be fetched, for
example, by creating an iframe tag.
Second, for each AST, Revolver determines if it is mali-
cious or benign, based on the Oracle’s input. More precisely,
an AST is considered malicious if it is the parent of a ma-
licious AST, or if it issued a web request that led to the
execution of malicious code. This makes Revolver ﬂexible
enough to work with any Oracle.
irrelevant for our analysis (and, in fact, often undesirable),
while retaining enough precision to achieve good results.
For example, consider a script obtained from the code in
Figure 1 via simple obfuscation techniques: renaming of
variables and function names, introduction of comments,
and randomization of whitespace. Clearly, we want Revolver
to consider these scripts as similar. Making this decision can
be non-trivial when inspecting the source code of the scripts.
In fact, as a simple validation, we ran Moss, a system for
determining the similarity of programs, which is often used
as a plagiarism detection tool [37], on the original script and
the one obtained via obfuscation. Moss failed to ﬂag the two
scripts as similar, as shown in the tool’s output here [23].
However, the two scripts are identical when their AST repre-
sentations are considered, since, in the trees, variables are
represented by generic VAR nodes, independently of their
names, and comments and whitespaces are ignored. This
makes tree-based code similarity approaches more suitable
for malicious JavaScript comparisons (and this is the reason
why our analysis leverages ASTs as well). However, as
shown in §3.4, we need to treat malicious code in a way
that is different from previous techniques targeting benign
codebases. Below, we describe our approach and necessary
extensions in more detail.
Revolver transforms the AST produced by the JavaScript
compiler into a normalized node sequence, which is the
sequence of node types obtained by performing a pre-order
visit of the tree. In total, there are 88 distinct node types,
corresponding to different constructs of the JavaScript lan-
guage. Examples of the node types include IF, WHILE, and
ASSIGN nodes.
Figure 4 summarizes the data structures used by Revolver
during its processing. We discuss sequence summaries in
the next Section.
3.2 Abstract Syntax Trees
Revolver’s core analysis is based on the examination of
ASTs rather than the source code of a script. The rationale
for using ASTs is that they abstract away details that are
3.3 Similarity Detection
After extracting an AST and transforming it in its normal-
ized node sequence, Revolver ﬁnds similar normalized node
sequences. The result is a list of normalized node sequence
USENIX Association  
22nd USENIX Security Symposium  641
&34$
*56789:;5=BE$
!"#$%&'#$()#$*+,#$-$
• $F$
• $*5=BB<:IJK56$
$$$?<86BJ$
Figure 4: Data structures used by Revolver.
pairs. In particular, pairs of malicious sequences are com-
pared to identify cases of evolution; pairs where one of the
sequences is benign and the other malicious are analyzed to
identify possible evasion attempts.
The similarity computation is based on computing the
directed edit distance between two node sequences, which,
intuitively, corresponds to the number of operations that are
required to transform one benign sequence into the malicious
one. Before discussing the actual similarity measurement,
we discuss a number of minimization techniques that we use
to make the computation of the similarity score feasible in
datasets comprising millions of node sequences.
Deduplication. As a ﬁrst step to reduce the number of
similarity computations, we discard duplicates in our dataset
of normalized node sequences. Since we use a canonical
representation for the ASTs, we can easily and efﬁciently
compute hashes of each sequence, which enables us to
quickly identify groups of identical node sequences. In the
remaining processing phases, we only need to consider one
member of a group of identical node sequences (rather than
all of its elements). Notice that identical normalized node
sequences may correspond to different scripts, and may also
have a different detection classiﬁcation (we describe such
cases in §3.5). Therefore, throughout this processing, we
always maintain the association between node sequences
and the scripts they correspond to, and whether they have
been classiﬁed as malicious or benign.
Approximate nearest neighbors. Given a repository of
n benign ASTs and m malicious ones, Revolver needs to
compute n× m comparisons over (potentially long) node
sequences. Even after the deduplication step, this may
require a signiﬁcantly large number of operations.
To address this problem, we introduce the idea of se-
quence summaries. A sequence summary is a compact sum-
marization of a regular normalized node sequence, which
stores the number of times each node type appears in the cor-
responding AST. Since there are 88 distinct node types, each
node sequence is mapped into a point in an 88-dimensional
Euclidean space. An advantage of sequence summaries
is that they bound the length of the objects that will be
compared (from potentially very large node sequences, cor-
responding to large ASTs, down to more manageable vectors
of ﬁxed length).
Then, for each sequence summary s, we identify its ma-
licious neighbors, that is, up to k malicious sequence sum-
maries t, such that the distance between s and t is less than
a chosen threshold τn. Intuitively, the malicious neighbors
correspond to the set of ASTs that we expect to be most
similar to a given AST. Determining the malicious neighbors
of a sequence summary is an instance of the k-nearest neigh-
bor search problem, for which various efﬁcient algorithms