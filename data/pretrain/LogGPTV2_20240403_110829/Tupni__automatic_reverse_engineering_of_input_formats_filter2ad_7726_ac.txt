The set Qi is a superset of the set of instructions that we consider
relevant. As outlined above, if records in r contain nested record
sequences then records of the same record type may have widely
different sets Qi. Therefore the next step of our algorithm is to
transform each set Qi into a set Q(cid:3)
i as follows.
Tupni collapses every segment of the instruction trace that con-
tains an execution of a child loop into a single virtual instruction.
This instruction is assigned a virtual identiﬁer (virtual EIP) such
that two virtual instructions have the same identiﬁer if and only if
they correspond to executions of the same loop. Q(cid:3)
i is set to Qi,
except that all instructions in the child loop execution are removed
from the Qi and replaced by the identiﬁer of the virtual instruction
that represents the child loop. The net effect of this transformation
is that iteration dependent loops that process child record sequences
contribute a single virtual instruction to the Q(cid:3)
i of the parent record.
This way, it only matters whether the child loop was executed, but
not what happened during its execution.
In summary, Tupni considers each record sequence r that was
identiﬁed in the previous step and computes Q(cid:3)
i for every record i
in r. Tupni classiﬁes two records i, j as having the same type if
Q(cid:3)
i = Q(cid:3)
j.
Running Example.
Recall that, in the previous step, Tupni had identiﬁed one record
sequence with two records. We have Q1 = {9, 10, 11} ∪ h1 and
Q2 = {9, 10, 11}∪ h5, where h1 and h5 denote the instructions in
record_hdlr[1] and record_hdlr[5], respectively, that
access the record sequence. Typically, h1 and h5 will be differ-
ent.
If neither of the two records contains a nested record sequence
then Q(cid:3)
In order to examine the other
case, let record_hdlr[1] contain a loop that iterates byte-by-
byte over the payload of record 1 (offsets 8 to 16). In this case, h1
contains up to 9 different instructions (for the nine loop iterations
1 = {9, 10, 11, v1},
from offset 8 to offset 16). Q(cid:3)
where v1 is a unique identiﬁer for the loop in record_hdlr[1].
1 would now be Q(cid:3)
1 = Q1 and Q(cid:3)
2 = Q2.
3.6 Constraint Identiﬁcation
This section describes how Tupni identiﬁes conditions on the
values of ﬁelds that an input must satisfy to be valid. This includes
relations between the values of multiple ﬁelds. We call such condi-
tions constraints. Tupni considers three types of constraints.
Symbolic Predicates.
The dynamic data ﬂow analysis engine [10] used by Tupni out-
puts a sequence of conditions on the input (symbolic predicates)
that were checked by the application during its execution. These
conditions encode the application’s execution path.
In general,
some of these conditions will represent properties that any valid
input must have while other conditions simply represent properties
of the particular input the application was run on. For the purpose
of gathering constraints on the format, we are interested in the for-
mer, but not in the latter.
Tupni generates constraints on the value of single ﬁelds (single-
value constraints) as follows. For each input ﬁeld i, Tupni outputs
the conjunction of all symbolic predicates of the form f (input[i]){,(cid:3)=}y, where f is a function that depends on nothing but in-
put values and constants that are hard coded into the application
(e.g., immediate operands of x86 instructions). In addition, we ig-
nore all symbolic predicates associated with checks for termination
values of record sequences, as the information contained in them is
already accounted for by the analysis of length determination (Sec-
tion 3.4).
Tupni also outputs symbolic predicates of the form input[x] =
f (input[y], input[z], . . .), where f is a function that depends on
nothing but input values and constants that are hard coded into
the application. This type of constraint can capture many types
of checksum calculations. We call such constraints functional con-
straints.
The constraints generated by this simple procedure may be par-
ticular to the input the application was run on and may not represent
the input format in general. This is a general limitation of dynamic
analysis. Recent systems such as Bouncer [9] and ShieldGen [14]
were designed to overcome this limitation in a different application
domain. We believe that the techniques employed by these systems
could be used to eliminate most of the remaining input speciﬁc con-
ditions.
Inter-Message Dependencies.
An inter-message constraint for a multi-message network proto-
col determines the value of a ﬁeld in a later message based on the
values of one or more ﬁelds in an earlier message. Typical exam-
ples include session IDs and sequence numbers.
Tupni inspects all attempts by the application to send data to the
network. If it ﬁnds tainted data in an outgoing message, it outputs
the associated data ﬂow graph that describes how the tainted output
ﬁeld depends on the previous input.
Length Fields.
Tupni uses three techniques for identifying length ﬁelds. The
ﬁrst technique makes use of the semantics of certain platform func-
tions as described in Section 3.7. The other two techniques are
similar to techniques used in Polyglot: identifying ﬁelds used to
compute pointers that are used to access other input ﬁelds and iden-
tifying ﬁelds that are accessed to compute the termination condition
of loops that process record sequences.
3.7 Platform-Speciﬁc Functions
Knowledge of the semantics of platform speciﬁc functions such
as system calls, application programming interface (API) functions
and functions in runtime libraries can signiﬁcantly enhance the ac-
curacy and functionality of Tupni. The idea of using the semantics
of library calls has been used in Bouncer [9]. However, the way in
which Tupni makes use of function calls is quite different from the
method in Bouncer. As a preliminary step, we have added speciﬁ-
cations of six string processing functions and 20 memory allocation
functions to Tupni. These speciﬁcations are used in the following
areas:
• Field identiﬁcation: Use of an input chunk as a function pa-
rameter can provide strong evidence that the chunk is a ﬁeld.
This evidence is incorporated into the ﬁeld recognition step
by increasing the weight w(c) of the chunk if Tupni sees a
call to a known function in the execution trace. More exten-
sive use of function semantics has the potential of providing
very detailed information about the types and semantics of
input ﬁelds. We leave this for future work.
• Identiﬁcation of record sequences: Functions may have record
sequences (or pointers to them) as parameters. For example,
a call to a standard string processing function with a char-
pointer that points to tainted data provides strong evidence
that the pointer marks the start of a string.
• Constraint identiﬁcation: function calls may provide infor-
mation about different kinds of constraints. We have used
knowledge of memory allocation functions to identify length
ﬁelds. For example, a call to malloc with a tainted size
parameter provides evidence that this parameter is a length
ﬁeld.
In general, we do not consider the parts of the execution trace
that are spent inside any of the known functions in our analysis.
Effectively, this collapses calls to these functions into a single vir-
tual instruction with special semantics.
3.8 Output Format
Input ::= F1
:D
S1[F1]
S1
R1
R2
::= R1 |
R2 |
...
::= F2
F3
...
::= F4
F5
...
:W
:W { F3 = SIZE(R1) }
:W
:W { F5 = SIZE(R2) }
Figure 5: The output format for the running example.
At this point, Tupni’s analysis of a single input is complete.
Tupni outputs the ﬁelds and record sequences it identiﬁes in an
enhanced BNF format. An example of this output format is shown
in Figure 3.8 for the running example. Tupni generates a rule that
lists the top-level ﬁelds and record sequences in the order of their
positions in the input. We refer to this rule as the root rule. For
each record sequence, Tupni generates an alternation rule to cover
all its record types. For each record type Tupni generates a rule to
list its ﬁelds and child record sequences. This process continues
recursively for all record sequences and record types. In addition,
Tupni outputs constraints in the enhanced BNF format.
3.9 Aggregation over Multiple Inputs
So far, we have discussed how Tupni analyzes a single execution
trace – corresponding to a single input. If multiple inputs of the
unknown format are available, the application can be run on each
of them, and Tupni can perform its analysis on each of the resulting
execution traces. The last step is to combine the individual results
into a single format speciﬁcation.
To do so, Tupni matches ﬁelds, record sequences, and record
types across different execution traces. Consider two execution
traces from the same application, but different inputs. Tupni con-
siders two base ﬁelds from two execution traces to match if they
are accessed by the same set of instructions. Tupni considers two
record sequences from two execution traces to match if they are
processed by the same loops (identiﬁed by the unique loop entry
point). Finally, Tupni considers two records from matching record
sequences to have the same type if their respective sets Q(cid:3)
i are
equal.
After identifying all matching ﬁelds, record sequences and record
types in the two execution traces, Tupni merges the BNF rules for
each of the execution traces into a single set of BNF rules. Tupni
ﬁrst identiﬁes the pairs of BNF rules that can be merged. These
include the root rules and the BNF rules of matched record se-
quences and record types. To merge BNF rules of matched record
sequences, Tupni simply creates a new alternation rule that includes
all record types in the old rules. Tupni merges the root rules and
rules of matched record types in two steps. It ﬁrst aligns the list of
ﬁelds and record sequences using the type-based sequence align-
ment technique proposed in [12]. If all ﬁelds and record sequences
are perfectly matched, it means that the two rules are identical;
Tupni simply keeps one. Otherwise, it creates a new alternation
rule for each pair of unmatched ﬁelds or record sequences. Then
it creates a new rule that lists matched ﬁelds and record sequences
as well as the alternation rules for unmatched ﬁelds or record se-
quences.
4.
IMPLEMENTATION AND EVALUATION
We have developed a prototype of Tupni. Our dynamic data
ﬂow engine was built on a re-implementation of the Vigilante sys-
tem [10]. Our prototype system uses iDNA [4], a binary program
translator, to capture and replay program execution traces. Our sys-
tem works on x86 instructions directly. Excluding the code of the
original Vigilante and iDNA, our Tupni prototype has 14,000 lines
of C++ and 4,100 lines of Perl.
In this section, we ﬁrst describe our experimental setup and eval-
uation methodology. After that, we present our experimental re-
sults.
4.1 Experimental Setup
Our evaluation is divided into two parts. We ﬁrst tested how
accurately Tupni can reverse engineer the format of a single input
message or ﬁle. Then we evaluated its capability of generalizing
the input format over multiple inputs.
In the ﬁrst set of experiments, we evaluated Tupni on both ﬁles
and network messages. We tested ﬁve binary ﬁles (WMF, BMP,
JPG, PNG and TIF), three binary network messages (DNS response,
TFTP data and RPC bind request), and two text network messages
(HTTP GET request and FTP port command). We selected these
input formats because they are representative protocols and were
studied in previous work. We intentionally avoided types of input
formats for which we knew a priori that our prototype would per-
form poorly (see Section 7 for a discussion of these formats). For
instance, our prototype does not work well if the ﬁeld boundaries
of a format do not coincide with byte boundaries due to its byte-
based taint tracking. For the compressed image ﬁles among our
test cases, we used Tupni to reverse engineer the container ﬁle for-
mat and ignored the format of the compressed data blobs. We used
WMF, the DNS response message, and the HTTP request message
to guide our design. We ran the remaining seven test cases without
changing the prototype.
For each input, we recorded an execution trace of a binary pro-
gram parsing and processing it on Windows XP Professional SP2.
Then we fed the execution trace to Tupni to reverse engineer the
format. The inputs and binary programs are listed in Table 1. In
all our test cases, Tupni can reverse engineer the formats in at most
5 minutes on a 3GHz machine running Windows XP Professional
SP2.
4.2 Evaluation Methodology
In our ﬁrst set of experiments on single test inputs, we focused
our evaluation on accuracy, that is, how accurately Tupni can iden-
tify the ﬁelds, record sequences, record types, and constraints in the
test inputs. Tupni automatically outputs the reverse engineered for-
mats in an enhanced BNF format (referred to as the Tupni format).
In order to evaluate the accuracy of Tupni’s output, we compare
the format speciﬁcation Tupni produced for each test case with the
published speciﬁcation [1, 2, 16, 20, 28, 34, 37, 39, 40] (referred to
as the published format) for the same format and identify discrep-
ancies. In our second set of experiments, we focus our evaluation
on completeness, that is, how well Tupni can infer a more complete
input format over multiple inputs.
It is hard to compare our results with previous work [6, 12, 26,
Format Name
WMF
BMP
JPG
PNG
TIF
DNS response
RPC bind
TFTP data
HTTP request
FTP port
Application
gdi32.dll
mspaint.exe
gdiplus.dll
gdi32.dll
gdiplus.dll
nslookup.exe
rpcss.dll
tftp.exe
inetinfo.exe
ftpsvc2.dll
Execution Trace (instructions)
3.2M
12.0M
15.7M
12.2M
12.0M
4.7M
128K
5.5M
156M
121M
Input Size (B) Run Time (min)
5
1
3
3
3
<1
<1
<1
3
3
4594
3126
3224
3543
4337
46
164
28
107
28
Table 1: Summary of test inputs in the evaluation.