with no argument vectors.
6
D. Mutz et al.
in min(cntasij , L) argument sets in the worst case partitioning. Thus, the worst
case partitioning value W P is deﬁned as:
W P (si) =
K(cid:2)
j=1
min(cntasij , L)
(3)
Now that we have the actual partitioning value, the optimal partitioning value,
and the worst case partitioning value, we can deﬁne a measure of the partition
quality Q(si) for a system call si. Q(si) is deﬁned as the ratio of the diﬀerence
between the actual and optimal partitioning to the diﬀerence between the worst
case and optimal partitioning:
Q(si) = AP (si) − OP (si)
W P (si) − OP (si)
Since the actual partitioning AP (si) must fall between W P (si) and OP (si),
Q(si) takes on values in the interval [0, 1] with 0 being the highest quality par-
titioning (i.e., no diﬀerence from the optimal case) and 1 being the worst (i.e.,
no diﬀerence from the worst case partitioning). In the special case where there
is no diﬀerence between W P (si) and OP (si), we deﬁne Q(si) to be 1.
Table 1. Observed argument sets for a ﬁctional system call foo(char *pathname) in
three diﬀerent calling contexts, C1, C2, and C3
Context Observed argument set
AS(C1, sf oo) = {"/tmp/a", "/tmp/b", "/tmp/c"}
C1
AS(C2, sf oo) = {"/tmp/a"}
C2
AS(C3, sf oo) = {"/tmp/a"}
C3
Consider the example shown in Table 1, which gives observed argument values
for a ﬁctional system call foo(char *pathname) for L = 3 diﬀerent calling con-
texts, C1, C2, and C3. Further, suppose that each argument appears 3 times dur-
ing the period of monitoring, that is, cntas(f oo)j = 3 for each of the three s(f oo)j.
Since the concrete argument vector (cid:2)"/tmp/a"(cid:3) appears in all three contexts and
the argument vectors (cid:2)"/tmp/b"(cid:3) and (cid:2)"/tmp/c"(cid:3) appear in one context each,
the actual partitioning AP (sf oo) is:
AP (sf oo) = (1 + 1 + 1) + (1 + 0 + 0) + (1 + 0 + 0) = 5
Because L = K = 3, the optimal partitioning for sf oo is
and the worst case partitioning for sf oo is
OP (sf oo) = max(3, 3) = 3
W P (sf oo) = min(3, 3) + min(3, 3) + min(3, 3) = 9
(4)
(5)
(6)
Combining the actual, optimal, and worst case partitioning, we have the follow-
ing measure of the overall quality of the partitioning for sf oo:
Q(sf oo) =
= 1/3
(7)
5 − 3
9 − 3
Exploiting Execution Context for the Detection of Anomalous System Calls
7
To evaluate our quality metric, we selected 9 root-owned services and pe-
riodic (cron) applications running in a production setting on 10 servers in an
undergraduate computer science lab. The 9 audited programs were chosen from
a larger pool of processes that run with root privileges in the following way.
First, no interactive command line executables were evaluated since they appear
sporadically and generate a relatively small number of audit records. For similar
reasons, 8 periodic and daemon processes were removed from the study because
they did not appear frequently enough in the audit set to produce a meaning-
ful evaluation. Second, script language interpreters (e.g., Perl and Python) were
removed since programs implemented in those languages execute with a virtu-
alized call stack. Next, 6 processes associated with the X11 windowing system
were eliminated because their role in the system is primarily to facilitate graph-
ical interaction with the user. Finally, 5 programs associated with the package
management and compilation subsystem were eliminated because they have a
peripheral role with respect to the security of the system.
Table 2 shows the mean and standard deviation of Q values across 36 security-
critical system calls issued by each of the 9 programs over a 10-day period.
Section 3.1 speciﬁes the monitored system calls and provides further justiﬁcation
for their inclusion in the study. Table 2 tabulates the average (μ) and standard
deviation (σ) of Q across each of the 36 system calls (denoted Q(s∗)). The
data shows that the values of Q recorded for a collection of real applications
in a production setting are optimal in 3 of 9 cases, and are never greater than
0.169. This suggests that including call stack information in system call argument
analysis is likely to produce models that outperform those that do not consider
execution context.
Table 2. Mean and standard deviation of Q over all system calls for the nine applica-
tions in the study
Application Q(s∗) μ Q(s∗) σ
cfenvd
0.066
0.191
cfexecd
0.000
crond
0.159
cupsd
idmapd
0.000
0.194
sendmail
0.379
slocate
0.218
sshd
0.000
ypbind
0.209
Overall
0.038
0.107
0.000
0.085
0.000
0.093
0.169
0.168
0.000
0.063
3 System Design
The empirical evaluation of context-sensitivity in the previous section showed
that system call arguments are often uniquely associated with speciﬁc calling
8
D. Mutz et al.
context in real-world applications. Therefore, we developed an intrusion detec-
tion system that takes advantage of this property. Our approach uses a collec-
tion of context-speciﬁc learning models that operate in three distinct phases.
The ﬁrst two phases consist of a training phase and a threshold learning phase,
during which learning is performed on attack-free audit data. In the training
phase, models gather examples of normal system call arguments. At the end of
this phase, detection models are generated for use in the two subsequent phases.
Following the training phase is the threshold learning phase, where thresholds
are computed for the ﬁnalized models by measuring their response to attack-free
data. In the ﬁnal detection phase, the trained models and thresholds are used
together to classify events as normal or anomalous.
In the following, we describe feature selection and the context-speciﬁc model-
ing approach in Section 3.1. Then, in Sections 3.2 through 3.4, we describe the
three phases of system operation. Finally, Section 3.5 provides details about the
audit collection infrastructure.
3.1 Feature Selection and the Context-Speciﬁc Modeling Approach
Experience shows that evidence of attacks often appears in the argument values
of system calls. Sometimes this may be due to “collateral damage” to local (stack)
variables when overwriting a return address. In these cases, damaged variables
are then used in system call invocations before the procedure returns. In other
cases, the attack is leveraging the privileges of the application to perform actions
that are not normally performed by the victim program. In many instances, these
diﬀerences can be identiﬁed by argument models.
To determine the set of system calls to use for our analysis, we studied the 243
system calls implemented in the version 2.6.10 of the Linux kernel to determine
which additional calls represent avenues to leveraging or increasing the privilege
of applications. This study identiﬁed 36 system calls, shown in Table 3, that
we found should be monitored to detect attempts to compromise the security
of a host. Note that in our system only arguments that have intrinsic semantic
meaning are modeled. Integer arguments corresponding to ﬁle descriptors and
memory addresses, for example, are ignored, since their values are not meaning-
ful across runs of an application. Additionally, these values rarely contain any
semantic information about the operation being performed.
In order to leverage the context information provided by the application’s call
stack at the time a system call is invoked, we instantiate detection models for
each calling context encountered during the training phase. We rely on audit
records that are composed of two parts: (a) the system call si that was invoked,
along with its arguments asij = (cid:2)asij
n (cid:3), and (b) the sequence of return
addresses gathered from the application’s call stack when the system call was
invoked. These addresses form the system call’s context C = (cid:2)r1, . . . , rl(cid:3). In all
three phases (training, thresholding, and detection), the pair (cid:2)C, si(cid:3) is used as a
lookup key in a data structure that maintains the collection of context-speciﬁc
models and thresholds.
1 , . . . , asij
Exploiting Execution Context for the Detection of Anomalous System Calls
9
Table 3. The 36 system calls monitored by the system
creat
link
chmod
rename mkdir
open
execve mknod
umount
umount2 symlink truncate
ftruncate fchmod ioperm
ipc
capset
fchown
setuid
unlink
mount
rmdir
uselib
iopl
mprotect create module prctl
lchown
setreuid
setresuid setresgid
setfsuid
setgid
setregid
chown
setfsgid
3.2 Training Phase
The ﬁrst phase of system operation is training, during which the audit records
received by the audit daemon are used as examples of normal behavior to
train context-speciﬁc argument models. This approach improves upon prior work
([12]), which did not consider execution context, but instead applied the same ar-
gument model instantiations to all invocations of a particular system call issued
by an application.
We now describe the individual argument models used to characterize normal
values for system call arguments. The models are described in substantial detail
in our previous work; the reader is referred to [12] and [14] for information
beyond the brief descriptions provided here.
The following three models are applied to string arguments:
– String Length: The goal of the string length model is to approximate the
actual (but unknown) distribution of the lengths of string arguments and to
detect instances that signiﬁcantly deviate from the observed normal behav-
ior. Usually, system call string arguments represent canonical ﬁle names that
point to an entry in the ﬁle system. These arguments are commonly used
when ﬁles are accessed (open, stat) or executed (execve), and their lengths
rarely exceed a hundred characters. However, when malicious input is passed
to programs, this input often occurs in an argument of a system call with
a length of several hundred bytes. The detection of signiﬁcant deviations is
based on the Chebyshev inequality.
– String Character Distribution: The string character distribution model cap-
tures the concept of a normal string argument by looking at its character
distribution. The approach is based on the observation that strings have
a regular structure, are often human-readable, and almost always contain
only printable characters. In the case of attacks that send binary data, a
completely diﬀerent character distribution can be observed. This is also true
for attacks that send many repetitions of a single character (e.g., the nop-
sledge of a buﬀer overﬂow attack). The detection of deviating arguments
is performed using a statistical test (Pearson χ2-test) that determines the
probability that the character distribution of a system call argument ﬁts the
normal distribution established during the training phase.
– String Structural Inference: Often, the manifestation of an exploit is immedi-
ately visible in system call arguments as unusually long strings or strings that
10
D. Mutz et al.
contain repetitions of non-printable characters. There are situations, how-
ever, when an attacker is able to craft her attack in a manner that makes
its manifestation appear more regular. For example, non-printable charac-
ters can be replaced by groups of printable characters. In such situations,
we need a more detailed model of the system call argument. Such a model
can be acquired by analyzing the argument’s structure. For the purposes
of this model, the structure of an argument is the regular grammar that
describes all of its normal, legitimate values. The process of inferring the
grammar from training data is based on a Markov model and a probabilistic
state-merging procedure. The details are presented in [21] and [22].
The fourth model can be used for all types of system call arguments:
– Token Finder: The purpose of the token ﬁnder model is to determine whether
the values of a certain system call argument are drawn from a limited set
of possible alternatives (i.e., they are elements or tokens of an enumera-
tion). An application often passes identical values such as ﬂags or handles to
certain system call arguments. When an attack changes the normal ﬂow of
execution and branches into maliciously injected code, these constraints are
often violated. The decision whether to identify the set as an enumeration or
a collection of random identiﬁers can be made utilizing a simple statistical
test, such as the non-parametric Kolmogorov-Smirnov variant, as suggested
in [13].
In prior work ([12]), models were instantiated for each system call (e.g., open,
execve). As we noted, in this paper models have been replicated for each calling
context C. In this way, when the audit daemon is operating in the training phase,
aggregate model instances are trained on the observed argument set AS(C, si).
3.3 Threshold Learning Phase
In our design, an aggregate model is used to associate a set of models with each
system call. The task of an aggregate model is to combine the outputs of all
models that are associated with a system call into a single anomaly score that
is used to assess whether the entire system call is normal or not. As in [12],
we sum the negative logarithm of the individual model outputs to produce one
score, which is then compared to a threshold (described below) to determine
whether or not an alert should be generated for the system call.
At the start of the threshold-learning phase, training ceases and all models
instantiated by the system are switched to detection mode. Each event in the
(attack-free) threshold learning set is then assigned an anomaly score by the
aggregate model speciﬁc to its system call si and context C. The threshold for
the aggregate model associated with the pair (C, si) is computed by adding
20% to the maximum anomaly score generated by the aggregate model over the
threshold training set.
Using a context-speciﬁc characterization allows thresholds to be independent
of one another, permitting some thresholds to be “loose” and others to be “tight”.
For example, in one context where there are a large number of training examples,
Exploiting Execution Context for the Detection of Anomalous System Calls
11
models might characterize the context’s features virtually ﬂawlessly implying a