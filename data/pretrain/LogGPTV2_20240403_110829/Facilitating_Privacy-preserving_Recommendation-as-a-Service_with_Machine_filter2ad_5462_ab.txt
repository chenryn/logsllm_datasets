# Performance Evaluation

## 4.1 The Sparse-Quantization-Reuse Method

Although CryptoRec can be efficiently executed on encrypted data, we introduce a sparse-quantization-reuse method to further optimize the process by reducing redundant calculations. This method reduces computation time by a factor of 9× without compromising accuracy. It achieves this by:

1. **Pruning Parameters**: Retaining only important parameters and discarding those that fall outside a predefined range.
2. **Quantizing Model Parameters**: Enforcing value sharing among model parameters.
3. **Reusing Calculated Results**: Reusing results from shared values whenever possible.

The extent of computation reduction depends on two factors:
1. **Pruning Ratio**: The number of pruned parameters divided by the total number of parameters.
2. **Reuse Ratio**: The proportion of unique parameters out of the total parameters, defined as \(1 - \frac{\text{number of unique parameters}}{\text{total number of parameters}}\).

Table 2 illustrates the pruning and reuse ratios for different datasets, showing that more than 90% of computations can be reduced using the sparse-quantization-reuse pipeline method.

| Dataset | Pruning Ratio | Reuse Ratio |
|---------|---------------|-------------|
| Netflix | 9.2%          | 90.7%       |
| ML1M   | 7.1%          | 90.5%       |
| Yahoo  | 29.4%         | 91.5%       |

For more details, please refer to the full version [11].

## 4.2 Evaluation and Comparison

### 4.2.1 Accuracy
CryptoRec supports two modes: fast mode and accurate mode. In the fast mode, recommendations are calculated using a pre-learned model (plaintext) and client data (ciphertext). The model parameter \(\Theta\) is independent of the client's data. In the accurate mode, the pre-learned \(\Theta\) is re-trained with the client's data to capture user features more precisely.

Among all the models, I-AutoRec leads in accuracy (RMSE), serving as the benchmark. For the baselines, we train the models from scratch as if there were a trusted environment. Table 3 shows that CryptoRec is competitive with state-of-the-art models in terms of accuracy.

| Model             | RMSE (Netflix) | Loss % | Iteration # | RMSE (ML1M) | Loss % | Iteration # | RMSE (Yahoo) | Loss % | Iteration # |
|-------------------|----------------|--------|-------------|-------------|--------|-------------|--------------|--------|-------------|
| I-NBM             | 0.9061 ± 0.005 | 8.7    | 1           | 0.8815 ± 0.007 | 5.4    | 1           | 0.9853 ± 0.014 | -0.3   | 1           |
| U-AutoRec         | 0.8849 ± 0.007 | 6.2    | 35          | 0.8739 ± 0.009 | 4.4    | 30          | 1.0583 ± 0.016 | 7.1    | 26          |
| BiasedMF          | 0.8587 ± 0.007 | 3.0    | 85          | 0.8628 ± 0.009 | 3.1    | 80          | 0.9980 ± 0.022 | 1.0    | 72          |
| CryptoRec-f       | 0.8586 ± 0.005 | 3.0    | 0           | 0.8781 ± 0.007 | 4.9    | 0           | 0.9880 ± 0.015 | 0.1    | 0           |
| CryptoRec-a(1)    | 0.8485 ± 0.004 | 1.8    | 1           | 0.8680 ± 0.006 | 3.7    | 1           | 0.9874 ± 0.011 | -0.1   | 1           |
| CryptoRec-a(n)    | 0.8391 ± 0.006 | 0.7    | 12          | 0.8543 ± 0.007 | 2.1    | 15          | 0.9214 ± 0.013 | -0.6   | 22          |
| I-AutoRec         | 0.8434 ± 0.006 | 0      | 140         | 0.8367 ± 0.004 | 0      | 110         | 0.9880 ± 0.015 | 0      | 125         |

### 4.2.2 Efficiency
- **Fast Mode**: Implemented with the Paillier cryptosystem [9] using the library python-paillier [2], which supports homomorphic addition operations. The secret key size is set to 2048.
- **Accurate Mode**: Implemented with the Fan-Vercauteren scheme [4] using the library SEAL [1], which supports both homomorphic addition and multiplication operations. The polynomial degree is set to 4096, and the plaintext modulus is 65537. Real numbers are encoded with 1024 coefficients for the integral part and 16 digits of precision for the fractional part.

Table 4 summarizes the communication and time costs for both modes. The fast mode can estimate a client's preferences on thousands of items in several seconds, while the accurate mode requires hours of computation and larger message sizes due to the high computational and spatial complexity of SWHE. However, the client's workload remains minimal.

| Mode   | Message Size (MB) | Server Time Cost (s) | Client Time Cost (s) | Message Size (GB) | Server Time Cost (h) | Client Time Cost (s) |
|--------|--------------------|----------------------|----------------------|-------------------|----------------------|----------------------|
| Fast   | 3.72               | 7.5                  | 1.04                 | 7.5               | 11.4                 | 4.8                  |
| Accurate | 7.3                | 14.2                 | 1.31                 | 9.4               | 14.3                 | 3.86                 |
| Fast   | 5.6                | 7.8                  | 1.08                 | 7.8               | 11.8                 | 10.9                 |

**Comparison**: Existing recommendation models often require non-linear operations or training from scratch, making them less efficient than CryptoRec. For example, GraphSC [7] took approximately 13 hours to run a single iteration of training BiasedMF on the ML1M dataset using 7 machines with 128 processors. In contrast, CryptoRec, leveraging the server's knowledge of most training data, still requires around 20 hours for each subsequent iteration with 8 processors, but it converges with fewer iterations [6].

## 5 Conclusion
We explored the potential of privacy-preserving recommender systems from a machine learning perspective. Our proposed CryptoRec, which transforms recommendation computations into simple matrix-multiplication operations, allows for secure and accurate estimation of user preferences on thousands of items in nearly real-time (seconds) using only a single PC.

## References
[1] Hao Chen, Kim Laine, and Rachel Player. 2017. Simple encrypted arithmetic library-SEAL v2. 1. In International Conference on Financial Cryptography and Data Security. Springer, 3–18.
[2] csiro. 2017. python-paillier. https://python-paillier.readthedocs.io/
[3] Christian Desrosiers and George Karypis. 2011. A comprehensive survey of neighborhood-based recommendation methods. Recommender systems handbook (2011), 107–144.
[4] Junfeng Fan and Frederik Vercauteren. 2012. Somewhat Practical Fully Homomorphic Encryption. IACR Cryptology ePrint Archive 2012 (2012), 144.
[5] grouplens. 2016. MovieLens. https://grouplens.org/datasets/movielens/1m/
[6] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. Computer 42, 8 (2009).
[7] Kartik Nayak, Xiao Shaun Wang, Stratis Ioannidis, Udi Weinsberg, Nina Taft, and Elaine Shi. 2015. GraphSC: Parallel secure computation made easy. In Security and Privacy (SP), 2015 IEEE Symposium on. IEEE.
[8] Netflix. 2010. Netflix. https://www.kaggle.com/netflix-inc/netflix-prize-data
[9] Pascal Paillier. 1999. Public-key cryptosystems based on composite degree residuosity classes. In Advances in cryptology—EUROCRYPT’99. Springer, 223–238.
[10] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015. Autorec: Autoencoders meet collaborative filtering. In Proceedings of the 24th International Conference on World Wide Web. ACM, 111–112.
[11] Jun Wang, Afonso Arriaga, Qiang Tang, and Peter YA Ryan. 2018. CryptoRec: Privacy-preserving Recommendation as a Service. arXiv preprint arXiv:1802.02432 (2018).
[12] Yahoo! 2016. R4-Yahoo! Movies. https://webscope.sandbox.yahoo.com/

**Poster Presentation: CCS'18, October 15-19, 2018, Toronto, ON, Canada**