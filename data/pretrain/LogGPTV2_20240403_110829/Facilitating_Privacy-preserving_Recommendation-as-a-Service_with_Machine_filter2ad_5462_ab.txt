7,637
item #
4,768
3,952
3,791
4 PERFORMANCE EVALUATION
4.1 The Sparse-Quantization-Reuse Method
Although CryptoRec can be efficiently executed on encrypted data,
we introduce a sparse-quantization-reuse method to overcome re-
dundant calculations, reducing the computation time by a factor
of 9× without affecting the accuracy. Our method retains only im-
portant parameters by pruning the parameters that fall outside a
pre-defined range, quantifies the model parameters to enforce value
sharing and reuses results calculated on shared values if possible.
The computation reduction depends on (1) the number of pruned
parameters, defined as pruning ratio: # of pruned par ameter
; (2)
# of all the par ameter
how many results can be reused at each row, defined as reuse
ratio: 1− # of unique par ameter
. Table 2 shows that more than 90%
# of all par ameter
pruning ratio
reuse ratio
netflix ml1m yahoo
9.2%
7.1%
29.4%
90.7%
90.5% 91.5%
Table 2: CryptoRec: pruning ratio and reuse ratio
of computations can be reduced by the sparse-quantization-reuse
pipeline method. For more details, we refer to the full version [11].
Poster PresentationCCS’18, October 15-19, 2018, Toronto, ON, Canada2307I-NBM
U-AutoRec
BiasedMF
CryptoRec-f
CryptoRec-a(1)
CryptoRec-a(n)
I-AutoRec
rmse
0.9061±0.005
0.8849±0.007
0.8587±0.007
0.8586±0.005
0.8485±0.004
0.8391±0.006
0.8434±0.006
netflix
loss% iteration#
8.7
6.2
3.0
3.0
1.8
0.7
0
1
35
85
0
1
12
140
rmse
0.8815±0.007
0.8739±0.009
0.8628±0.009
0.8781±0.007
0.8680±0.006
0.8543±0.007
0.8367±0.004
ml1m
loss% iteration#
5.4
4.4
3.1
4.9
3.7
2.1
0
1
30
80
0
1
15
110
rmse
0.9853±0.014
1.0583±0.016
0.9980±0.022
0.9880±0.015
0.9874±0.011
0.9214±0.013
0.9880±0.015
yahoo
loss% iteration#
-0.3
7.1
1.0
0.1
-0.1
-0.6
0
1
26
72
0
1
22
125
Table 3: Accuracy comparison. CryptoRec-f: fast mode; CryptoRec-a(iteration#): accurate mode.
4.2 Evaluation and Comparison
CryptoRec supports two modes, a fast mode and an accurate
mode. In the fast mode, we calculate recommendations with only a
pre-learned model (plaintext) and client data (ciphertext). Referring
to the prediction protocol from Section 2.1, the model parameter
Θ does not relies on the client’s data. In the accurate mode, before
calculating recommendations, we re-train the pre-learned Θ with
the client data, capturing user features for this client more precisely.
4.2.1 Accuracy. Among all the models, I-AutoRec leads the ac-
curacy (rmse) performance, serving as the benchmark. For the
baselines, we train the modes from scratch as if there is a trusted
environment. As shown in Table 3, CryptoRec is competitive with
the state-of-the-art in terms of accuracy performance.
4.2.2 Efficiency. fast mode: We implement this mode with an
additive HE scheme, Paillier cryptosystem [9] supported by the li-
brary python-paillier [2], as it requires only homomorphic addition
operations. We let the secret key size be 2048. Figure 1 precisely
describes the prediction process under this mode. accurate mode:
We implement this mode with a ring based somewhat (fully) HE
scheme (SWHE), the Fan-Vercauteren scheme [4] implemented in
the library SEAL [1], as it requires both homomorphic addition
and multiplication operations. We let the polynomial degree be
4096, the plaintext module be 65537. To encode real numbers, we
reserve 1024 coefficients of the polynomial for the integral part and
expand the fractional part to 16 digits of precision. Compared to the
fast mode, the accurate mode requires the server to re-train the
pre-learned model with the client’s data. Note that the re-training
process also relies on only addition and multiplication operations.
The communication and time cost of the client and server are
summarized in Table 4. It shows that the fast mode can estimate
a client’s preferences on thousands of items in several seconds;
For the accurate mode, we implemented the CryptoRec-a(1), as
we can still use the sparse-quantization-reuse method. The server
needs hours of computations and a larger message size due to the
high computational and spatial complexity of SWHE. Fortunately,
it does not significantly increase the workload of the client.
Comparison. The existing recommendation models require ei-
ther or both of non-linear operations (e.g., I-NBM, U-AutoRec, I-
AutoRec, etc.) and training models from scratch (e.g., I-AutoRec,
BiasedMF, etc.). Apparently, they can not be as efficient as Cryp-
toRec. A recent advance comes from GraphSC [7]. It shows that a
single iteration of training biasedMF (the feature dimension is 10),
on the dataset ml1m, took roughly 13 hours to run on 7 machines
mode
fast
accurate
Message size (MB)
Server time cost (s)
Client time cost (s)
Message size (GB)
Server time cost (h)
Client time cost (s)
netflix ml1m yahoo
3.72
7.3
5.6
1.04
7.5
11.4
4.8
14.2
7.1
1.31
9.4
14.3
3.86
10.9
5.8
1.08
7.8
11.8
Table 4: CryptoRec: efficiency evaluation
with 128 processors. In our setting, by making use of the fact that
the server knows most of the training data, it still needs around 20
hours for any i-th iteration with 8 processors, where i > 1. On top
of this, dozens of iterations are necessary for the convergence [6].
5 CONCLUSION
We explored the possibility of facilitating privacy-preserving rec-
ommender systems from the machine learning side. As the results
demonstrated, our proposed CryptoRec, which turns recommendation-
computations into a simple matrix-multiplication operation, allows
to securely and accurately estimate a user’s preferences on thou-
sands of items in a nearly real-time (seconds) level, with only a
single PC.
REFERENCES
[1] Hao Chen, Kim Laine, and Rachel Player. 2017. Simple encrypted arithmetic
library-SEAL v2. 1. In International Conference on Financial Cryptography and
Data Security. Springer, 3–18.
[2] csiro. 2017. python-paillier. https://python-paillier.readthedocs.io/
[3] Christian Desrosiers and George Karypis. 2011. A comprehensive survey of
neighborhood-based recommendation methods. Recommender systems handbook
(2011), 107–144.
[4] Junfeng Fan and Frederik Vercauteren. 2012. Somewhat Practical Fully Homo-
morphic Encryption. IACR Cryptology ePrint Archive 2012 (2012), 144.
[5] grouplens. 2016. MovieLens. https://grouplens.org/datasets/movielens/1m/
[6] Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization tech-
niques for recommender systems. Computer 42, 8 (2009).
[7] Kartik Nayak, Xiao Shaun Wang, Stratis Ioannidis, Udi Weinsberg, Nina Taft, and
Elaine Shi. 2015. GraphSC: Parallel secure computation made easy. In Security
and Privacy (SP), 2015 IEEE Symposium on. IEEE.
[8] Netflix. 2010. Netflix. https://www.kaggle.com/netflix-inc/netflix-prize-data
[9] Pascal Paillier. 1999. Public-key cryptosystems based on composite degree resid-
uosity classes. In Advances in cryptology—EUROCRYPT’99. Springer, 223–238.
[10] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015.
Autorec: Autoencoders meet collaborative filtering. In Proceedings of the 24th
International Conference on World Wide Web. ACM, 111–112.
[11] Jun Wang, Afonso Arriaga, Qiang Tang, and Peter YA Ryan. 2018. CryptoRec:
Privacy-preserving Recommendation as a Service. arXiv preprint arXiv:1802.02432
(2018).
[12] Yahoo! 2016. R4-Yahoo! Movies. https://webscope.sandbox.yahoo.com/
Poster PresentationCCS’18, October 15-19, 2018, Toronto, ON, Canada2308