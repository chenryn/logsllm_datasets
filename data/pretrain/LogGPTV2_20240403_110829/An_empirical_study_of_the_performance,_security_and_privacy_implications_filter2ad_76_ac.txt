scribed earlier (see Section III). The cache simula-
tor has two components: a local caching resolver
and a “remote server”. The caching resolver sends
queries to the remote server and performs veriﬁca-
tion of the responses. The remote server emulates
the DNS zone hierarchy and the nameservers for
each zone. The zone hierarchy is created by ﬁrst
processing our network traces to identify the Top-
Level Domains and Second-Level Domains. Each
zone is then assigned a unique 1024-bit RSA key-
pair, of which the private key is used to sign the
RRsets and the DS record of its child zone.
The caching resolver drives the simulation by re-
Figure 6. An example DNSSEC simulation. RTT1 and RTT2
are derived from the actual network trace
playing the queries from the trace. The queries are
forwarded to the remote server where the correct
(signed) response based on the trace is returned.
For each response we also simulate the network
latency for contacting the remote resolver using a
normalized average of the observed RTT from the
network trace. Once the caching resolver receives
a response, it veriﬁes the response and caches the
result for the speciﬁed TTL. As in Bindv9, we use
the OpenSSL cryptographic library to perform all
cryptographic operations. We used SHA1 as our
digest function. Subsequent queries within the TTL
period are served directly from cache.
Figure 6 depicts an example interaction between
the caching resolver and remote server compo-
nents of the simulator for resolving a query for
foo.com. The simulator is initialized with only the
public key of the root zone (DNSKEYroot). Step : the
query is “forwarded” to the remote server. Step :
the reply from the server (on behalf of the root)
contains the NS record and also a signed digest
(i.e., a DS record) of the public key (DNSKEY f oo.com)
for the foo.com zone. Step : the DS record is
veriﬁed using DNSKEYroot. Step : the resolver then
contacts the nameserver for the foo.com zone
and receives from ns1.foo.com, the A record for
foo.com, a signed digest of the A record (i.e., its
RRSIG), and also DNSKEY f oo.com. Step : the resolver
veriﬁes the authenticity of DNSKEY f oo.com by compar-
ing the cryptographic hash of DNSKEY f oo.com with the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:14 UTC from IEEE Xplore.  Restrictions apply. 
Caching Resolver@rootA foo.com ? DNSKEYrootNS ns1.foo.comSHA1DS foo.RSA/SHA1RRSIG DSfoofoo.com ?PI:EMAIL foo.comDNSKEY fooRRSIG ARRRemote Serververifyverify DNSKEYfooRTT1RTT2RRSIG DSfooDS fooSHA1DNSKEY fooRSA/SHA1RRSIG ARRSHA1A foo.comDS foo.0101010101010101010100101010101010101010101010101011010110101010Trace 0101010101010101010100101010101010101010101010101011010110101010Trace 123456767Figure 7. Resolver throughput
DS record obtained earlier from the root. Finally in
Steps  and : the resolver veriﬁes the A record by
checking the authenticity of the associated RRSIG
using DNSKEY f oo.com, and compares the embedded
cryptographic hashes in the signed digest to the
hash of the returned A record.
A. Results
In what follows, we use a 4 hour period during
peak load in the Fall dataset as input to our trace-
driven simulations. The trace contains 23.6 million
records. The overheads are computed relative to a
baseline where DNSSEC capability was disabled.
Obviously, as veriﬁcation of responses requires
additional CPU cycles, it has a direct eﬀect on the
throughput of the resolver. Figure 7 shows the
throughput achieved by the cache simulator for
fully resolving DNS queries. The average through-
put of 62,345 queries per min (qpm) decreases by
24.7% (to 46, 964 qpm) when veriﬁcation is enabled.
However, when prefetched records are excluded,
the drop in throughput relative to the baseline is
about 16.1% (to 52, 307 qpm).
The drop in throughput causes a signiﬁcant in-
crease in response times as observed by the clients.
Here, response times reﬂect both the simulated
RTT (to fetch the response) as well as the veriﬁca-
tion time at the resolver itself. Figure 8 shows the
response time for the same experiment. Notice that
over 80% of clients get a response within 100ms
in the baseline. With veriﬁcation on, but with
pre-resolutions excluded, over 70% of the clients
still get answers within 100ms. However, when
responses for extraneous requests must also be
veriﬁed, only 48% of the clients receive an answer
Figure 8. Observed response times for validating DNS answers
within 100ms. This result is particularly worrying
since prefetching was introduced as optimization
to reduce client response times, but when DNSSEC
is turned on, even valid requests see a considerable
increase in response time.
Finally, notice that for clients that witnessed
response times over 300 ms, all three scenarios
show similar performance. This is because the high
RTT (from the trace) observed by these clients
masks the cost of verifying responses. We now turn
our attention to the privacy implications of this
practice of pre-resolving domain names.
VI. Privacy Implications
As mentioned earlier, we believe DNS prefetch-
ing oﬀers Internet providers yet another avenue
to infringe on a user’s privacy, to the extent that
the context of their searching activity may no
longer be private. Indeed, very detailed informa-
tion can be harvested from these requests without
ones knowledge. The threat model we consider
here assumes access to DNS logs (e.g., as a DNS
provider would have), which we argue is a realistic
threat since many DNS providers have already
acknowledged their interests to “aggregate non-
personally-identifying information about the be-
havior of visitors to its websites and customers of
its DNS services” [21].
We examine the potential for abuse under this
threat model by describing an inference attack
that allows an attacker to reconstruct users search
queries by simply observing their DNS queries.
The key observation here is that domain names
often contain identiﬁable words as to the nature of
a website, and so if an adversary can observe the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:14 UTC from IEEE Xplore.  Restrictions apply. 
 35000 40000 45000 50000 55000 60000 65000 70000 75000 0 50 100 150 200 250 300Throughput (Queries/Min)Time (min)BaselineDNSSEC w/o Prefetching (1024)DNSSEC w Prefetching (1024) 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1 1 10 100 1000 10000CDFResponse Time (ms)BaselineDNSSEC w/o Prefetching (1024)DNSSEC w Prefetching (1024)68(a) Example browser events
(b) Groups with topical labels
Figure 9. Clients browsing activity and reconstructed search terms due to prefetching
domain names that were contained in, for example,
a search results page, then she would be able to
reliably reconstruct the client’s search term. Since
prefetching creates identiﬁable clusters of DNS
queries, we propose a technique for reconstructing
the search terms by ﬁrst identifying prefetching
events, and then applying algorithms similar to
the auto-completion techniques used by search
engines to provide suggestions to users. In doing
so, we are able to make intelligent guesses as to the
client’s likely search terms. The overall approach
we take is as follows:
Step  Grouping related entries: For pedagog-
ical purposes, we remind the reader how users
typically ﬁnd information on the Web. Loosely
speaking, they input a set of keywords into a
search engine, and then explore the ranked set
of returned links. Figure 9(a) shows an example
browsing session with three distinct groupings of
search based prefetching events from real data.
These groupings were identiﬁed using the heuris-
tic described in Figure 2. Obviously, while one
could manually sift through the data and ﬁnd rela-
tionships across groups, with thousands of clients
and hundreds of queries per second, this would be
impractical. We provide an automated technique
for achieving our goals with high accuracy.
Step  Keyword extraction: Once events have
been grouped, the next task is to extract mean-
ingful keywords from the domains names in each
group. To do so, each domain name is tokenized
into keywords using a sliding window algorithm.
We generate template words for each window
by using a n-recommend algorithm [22], which
provides word suggestions given a preﬁx. The
process is initialized by using ﬁrst m-characters
(m = 4) of a domain name as the preﬁx. The
longest word template match on the domain name
is selected as an identiﬁed keyword, the window
slides over to the end of the match and the entire
process is repeated until the entire domain name
is broken down into a set of recognizable words.
The result is a list of keywords, ordered from left
to right, for each domain name. If a domain name
cannot be broken down into constituent words, we
categorize it using a content classiﬁcation engine
(e.g. Google Insights) that labels the domain with
a broad topical classiﬁcation. In this case, only the
domain names (i.e., no client speciﬁc information)
is sent to the topical classiﬁcation engine. At this
stage, a sample output for Event1 in Figure 9(a)
would be (cid:104)neighborhood, scout, city, town, henderson,
county, chamber, owner(cid:105).
Step  Query Reconstruction: After the key-
words are tokenized and ranked, we locate groups
containing a search engine domain name (e.g.,
google, bing) and then attempt to recreate the ac-
tual search query within each group. In order
to reconstruct the search query, we again take
advantage of an n-recommender systems, and pro-
vide as input all the ﬁrst order words associated
with each grouping. This yields a list of suggested
queries. Each suggestion is compared with our list
of ordered words, and we output (as our inferred
query) the suggestion with the maximum number
of matches. For the running example, the output
for Event1is now: (cid:104)Henderson Neighborhood(cid:105).
As an optimization, we take advantage of the
observation that a user’s browsing history, or her
interactions on social networks, can be used to ﬁnd
identifying patterns in her access behavior [23].
Speciﬁcally, we augment our query reconstruction
scheme to utilize related tokens across multiple
browsing events. Revisiting the earlier example,
the events in Figure 9(a) shows that the search
for (cid:104)Henderson Neighborhood(cid:105) (Event1) was followed
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:43:14 UTC from IEEE Xplore.  Restrictions apply. 
Event2timeoatesrealty.comcoldwellbankerkasey.compreferredchoicemortgage.comreal-estate-longandfoster.commortgages.longandfoster.compreferredchoicemortgage.comEvent3homes.comrealestate.yahoo.comnc-hendersonville.comhendersonvillehomepro.combuythemountain.compreferredrealestatecenter.comEvent1owners.comneighborhoodlink.comcitytowninfo.comneighborhoods.comneighborhoodscout.comhendersoncountychamber.orgﬁnancecoldwellmortgagereal-estateEvent3timeﬁnanceplaceshendersonreal-estateEvent2placesneighbor-hoodhendersonEvent169(a) Snapshot of popular searches (July)
(b) Aggregate of individual Queries (July)
Figure 10. A tag cloud depicting several automatically reconstructed searches
by a search for (cid:104)real-estate henderson(cid:105) (Event2) and
(cid:104)mortgage real-estate(cid:105) (Event3). This additional con-
textual information can be used to link the recon-
structed queries of each event, thereby allowing
for a more speciﬁc picture of the user’s interests:
buying a house in henderson.
To automate this process, we apply an instance-
based learning method to relate individual brows-
ing events and recreate a user’s browsing proﬁle.
In order to ﬁnd similar browsing events, we ﬁrst
use a topical classiﬁer to label each event un-
der a broad-category. For example, real-estate and
mortgage can be broadly classiﬁed as related to
Finance. The resulting data has each group labeled
with a set of possible topics as shown in Fig-
ure 9(b). Once all the elements have been labeled
with a topic, we then apply a pattern matching
algorithm to ﬁnd related terms by checking if a
event contains topics in its groupings that match
to an already scanned event. If a match is found,
we consider the two groupings as being related.
For example, we would consider all three events
as related because of the linkage made by the
elements finance and places→henderson; here,
the symbol “→” denotes a subcategory. A sample
output is now: (cid:104)Real Estate Henderson Neighbor-
hood(cid:105),(cid:104)Mortgage Henderson Neighborhood(cid:105).
A. Evaluation
For illustrative purposes, Figure 10(a) presents
a tag cloud of the most popular reconstructed
browsing activity during the ﬁrst week of July.
Part (b) presents a similar illustration, but for a
4 hour period for 10 randomly picked client IPs.
For privacy reasons, we display the results as a
group, as doing otherwise would leak very speciﬁc
information about the browsing behavior of some
clients. While these results aptly demonstrate the
scope of the privacy breaches, evaluating the accu-
racy of our approach calls for ground truth.
To allow us to perform such an analysis, we
make use of our data generation framework and
use it to generate queries from 10 machines sim-
ulating clients that run a combination of browsers
(Chrome, Safari, Firefox, Internet Explorer, and
Opera). The searches made by these clients were
randomly chosen from a list of 1,000 topics derived
from Google Trends (from January to June 2010),
controversial topics from Wikipedia, and Alexa
Hot Topics (from February to May 2010). The list
is labeled by topic, with each topic having a set of
associated domain names.
During a week long period, we performed
two non-overlapping experiments: one where each
client selects a term with uniform probability, and
the other where the selection is biased by assigning
higher probabilities to domains and queries in
related topical areas (e.g., if the topic picked is
health-care, there is a high probability that sub-
sequent queries by that client will be related to
health-care). Once a client picks a topic, it ran-
domly performs either a search query, click on a
link on the returned search results, or use the loca-
tion bar to navigate to a domain name associated
with the topic. The client does so for no more than
20 minutes, with uniformly random chosen “think
times” of 1-5 minutes. At the end of the browsing
session, the client chooses a new search topic. No
two clients are allowed to choose the same topic.
All the activity is timestamped and logged.
Next, using all the recorded data spanning that
time period (recall queries from real clients are also
occurring simultaneously) we attempted to recon-
struct the searches. Our results are then compared
with the data logged at the generation framework
in order to compute our true positive and false