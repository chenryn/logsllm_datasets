repeatedly getting scheduled on the same nodes. Second,
applying 1-hour ﬁltering shows that the spatial distribution
evens out compared to Fig. 11. However,
interestingly it
continues to show uneven distribution; skewed toward the left
part of the supercomputer. This indicates that communication-
intensive applications are not scheduled evenly across the
cabinet. Applications scheduled on the left part are likely to
see more performance impact due to network congestion.
We also calculated the correlation coefﬁcient between
the spatial distribution of congested nodes and lane de-
grades/failures. The Spearman correlation coefﬁcient was close
to zero (0.01); the nodes with high ejection bandwidth are
not strongly correlated with the interconnect errors. This
result was expected since we found the correlation between
throttle events and interconnect errors were not high. However,
surprisingly, there is a low correlation between the heatmap
of congested nodes and throttled blades (Spearman correlation
0.01). This is because congested node information is collected
after the throttle command has been issued, so it may not
capture the nodes which actually caused the congestion and in-
duced throttling. This also indicates that the aggregate network
trafﬁc at the blade can be potentially different than individual
node-level trafﬁc. Future network performance tools should
focus on building more accurate and ﬁne-grained tools that
can detect the root cause in real time.
Next, we analyze the characteristics of applications running
on these congested nodes. First, we plot the frequency of
Fig. 14: Fraction of top 5 congestion-causing and other appli-
cations without ﬁlter (left), and with 1-hour ﬁlter (right).
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : A
4000
512 Other
App : F
128
Other
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : B
2
16 Other
App : G
400
320 Other
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : C
45
1200 Other
App : H
5120
1
Other
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : D
1024
Other
App : I
64
Other
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : E
216
4096 Other
App : J
15
Other
Fig. 15: Job size distribution of top 10 congestion-causing
applications.
unique applications that were running on congested nodes
when the throttling events occurred. Fig. 13 shows that only
a few applications tend to dominate. We refer to these appli-
cations as congestion-causing applications in our discussion;
however, we note that these applications may not be nec-
essarily responsible for increasing the congestion that even-
tually resulted in the network throttling event. For example,
5 applications alone appear in more than 70% of congested
node reporting events (Fig 14), while more than 250 unique
applications are logged in total across all congested node
reporting events. Interestingly, when 1-hour ﬁltering is applied,
the number of unique applications decreases signiﬁcantly. The
top 5 most frequently occurring applications appear only in
approx. 50% of congested node reporting events (Fig 14).
Total number of unique applications go down from 250 to
90. Reduction in the number of unique applications clearly
indicates that when multiple throttle events occur in the small
time period, they are not because of the same application. In
fact, it turns out that within a 1-hour time window, multiple
unique applications can cause nodes to be highly congested.
We also see the same results on a per-user basis with and
without ﬁltering. These results conﬁrm that applications and
users can work as proxies for each other.
Next, we analyze the job size of these applications. We limit
our discussion to the top 10 most frequent applications. As
application names can be business-sensitive, we identify them
with English letters. Fig. 15 shows the job size distribution of
top 10 applications that appear most frequently on congested
6
100%
80%
F
D
C
60%
40%
100%
80%
F
D
C
60%
40%
20%
0
Number of unique applications
100
200
0
Number of unique applications
20
40
60
Fig. 16: Distribution of unique applications over
top
bandwidth-heavy application events without ﬁlter (left), and
with 1-hr ﬁlter (right).
nodes. We make several interesting observations. First, most
of the applications tend to run on the same number of nodes
every time they appear in the congested node reporting events.
For example, applications A, B, and C run on 400, 2, and 45
nodes, respectively, for more than 90% of the time that they
appear in the congested node reporting events.
Moreover, counter-intuitively, the job sizes of these appli-
cations are relatively small. For instance, 7 out of the 10
applications most frequently have a job size of less than
512 nodes. In fact, 5 applications have the most frequent
job size of less than 128 nodes. In such cases, the many-to-
few communication pattern can be responsible for congesting
the nodes (high ejection bandwidth). Therefore, only focusing
on large scale jobs for identifying culprit applications is an
ineffective strategy. Our results show that node congestion is
caused by small-scale applications in real-world scenarios.
to
our
extend
understanding
Next, we want
of
job size
communication-intensive applications and their
distributions. On every throttle event,
the nlrd daemon
collects the bandwidth data of all applications running on the
system and lists the top 10 of these application sorted by
their network bandwidth consumption (total ﬂits/s aggregated
over all nodes). Note that these bandwidth-heavy applications
are different than the ones running on the top 10 heavily
congested nodes.
Fig. 16 shows that a few applications tend to be heavy-
hitters. For example, 5 applications alone appear in approxi-
mately 57% of the top bandwidth application reporting events
(Fig. 17), while more than 200 unique applications show up
in total across all top bandwidth application reporting events.
Interestingly, when 1-hour ﬁltering is applied, the number
of unique applications decreases signiﬁcantly. However, the
top 5 most frequently occurring applications constitute 50%
of top bandwidth application reporting events (Fig. 17). The
total number of unique applications reduces dramatically to
60. These results indicate that focusing on the top 5-10
applications can cover 50% of the communication-intensive
applications space. We also observe the same results on a
per-user basis with and without ﬁltering. These results again
conﬁrm that application and user can work as a proxy for each
other, even for top bandwidth application reporting events.
Next, we want to answer two questions: (1) are these top
bandwidth applications the same as the top congestion-causing
applications running on congested nodes?, and (2) is the job
size distribution of the top bandwidth applications different
7
C 8.25%
B 8.99%
B 7.37%
A 10.57%
D 7.2%
E 4.77%
G 6.88%
A 28.01%
H 6.63%
F 16.71%
Others 42.79%
Others 51.84%
Fig. 17: Fraction of top 5 bandwidth-heavy and other applica-
tions without ﬁlter (left), and with 1-hr ﬁlter (right).
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : A
4000
512 Other
App : N
128
256 Other
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : K
32
128 Other
App : D
128
512 Other
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : L
4245 1200 Other
App : O
5120 2568 Other
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : B
1024
128 Other
App : P
64
Other
100%
75%
50%
25%
0%
100%
75%
50%
25%
0%
App : M
216
125 Other
App : Q
15
Other
Fig. 18: Job size distribution of top 10 bandwidth-heavy
applications.
than that of the top congestion-causing applications?
Fig. 18 shows the job size distribution and anonymized
application names of the top 10 bandwidth-heavy applications
that appear most frequently in the top bandwidth application
reporting events. We observe that most of the applications tend
to run on the same number of nodes every time they appear
in the top bandwidth application reporting events. However,
interestingly, these applications are not the same as the top
congestion-causing applications running on congested nodes.
Only three applications are common between these two sets
(Fig. 18 vs. Fig. 15). They are situated at positions 1, 4, and 7,
in the ﬁgures. This indicates that bandwidth-heavy applications
are not necessarily the ones that cause congestion or run
on congested nodes. These bandwidth-heavy applications are
producing a signiﬁcant amount of trafﬁc, and are likely to be
spread over a large number of nodes or have a many-to-many
communication pattern. We notice that only 3 applications
have a job size larger than 4000 nodes, indicating that even
bandwidth-heavy applications are not necessarily large in size.
The communication pattern seems to be playing a critical role.
As an example, App K which runs mostly on 32 and 128 nodes
appears second in the bandwidth-heavy applications list, but
does not appear in the congestion-heavy applications list. This
could be because this particular application does not inten-
sively exhibit a many-to-one communication pattern. Many-
to-few and many-to-one communication patterns can result in
high congestion due to the concentration of messages over
a few nodes. In summary, bandwidth-heavy applications’ job
sizes are similar to that of congestion-causing applications’,
but there is no signiﬁcant overlap between these two sets and
they may differ in their communication patterns.
V. RELATED WORK
Various HPC interconnect networks are proposed for im-
proving HPC systems performance - QsNET [3], SeaStar [4],
Tofu [5], Blue Gene/Q [6], Aries [7], TH Express-2 [8] and
others - which use different types of topology like k-Ary n-
Cube, fat-tree/Clos, and dragonﬂy. Interconnect networks have
been a vital part of computer systems. Network resources
have been a major performance bottleneck in HPC systems
performance. Several studies are performed to understand [9],
[10], [11], [12], [13], [14], [15], [16], [17] and improve [18],
[19], [20], [21], [22], [23], [24] interconnect failures in HPC
systems.
Titan is the successor of Cray X-series which use the
XK7 system and 3D Gemini interconnect. The Gemini system
interconnect architecture is explained in [1] and evaluated in
[25], [2] using micro-benchmarks. Cray’s latest XC series
is implemented using the Aries interconnect which supports
better bandwidth, latency, message rate and scalability [26].
Our work differs from all these studies and evaluations as none
of these works evaluate how different interconnect errors and
congestion events occur on a large-scale HPC system. Our
ﬁeld data and analysis is unique and provides useful insights
that can be used by users, system architects, and operators to
improve the overall efﬁciency of HPC systems.
VI. CONCLUSION
Overall, we discussed many interesting insights derived
from our analysis. Interconnect faults like lane degrades
are continuous and related to heterogeneous load imbalance
among lanes. Link inactive errors do not have a temporal or
a spatial correlation with lane degrades, while interconnect
errors have a high correlation with link inactive/failed errors.
We showed that these characteristics can be exploited for
different purposes. We also demonstrated that multiple
applications can cause multiple congestion events within a
short period of time. Moreover, these applications can be,
surprisingly, small in size, not scheduled evenly across the
cabinet and have a many-to-few communication pattern. Our
analysis can be used in identifying such applications and users
to minimize the performance impact on other applications.
Acknowledgment We thank reviewers and Elmootazbellah Elnozahy
for their constructive feedback. The work was supported by in part
through NSF Grants (#1563728, #1561216 and #1563750), North-
eastern University and by the U.S. Department of Energy, Ofﬁce of
Science, Ofﬁce of Advanced Scientiﬁc Computing Research, program
manager Lucy Nowell. This work also used in part the resources of
the Oak Ridge Leadership Computing Facility, located in the National
Center for Computational Sciences at ORNL, which is managed by
UT Battelle, LLC for the U.S. DOE under contract number DE-
AC05-00OR22725.
REFERENCES
[1] R. Alverson, D. Roweth, and L. Kaplan, “The gemini system inter-
connect,” in High Performance Interconnects (HOTI), 2010 IEEE 18th
Annual Symposium on.
IEEE, 2010, pp. 83–87.
[2] M. Ezell, “Understanding the impact of interconnect failures on system
operation,” in Proceedings of Cray User Group Conference (CUG 2013),
2013.
[3] F. Petrini, E. Frachtenberg, A. Hoisie, and S. Coll, “Performance
evaluation of the quadrics interconnection network,” Cluster Computing,
vol. 6, no. 2, pp. 125–142, 2003.
[4] R. Brightwell, K. T. Pedretti, K. D. Underwood, and T. Hudson,
“Seastar interconnect: Balanced bandwidth for scalable performance,”
IEEE Micro, vol. 26, no. 3, pp. 41–57, 2006.
[5] Y. Ajima, S. Sumimoto, and T. Shimizu, “Tofu: A 6d mesh/torus
interconnect for exascale computers,” Computer, vol. 42, no. 11, pp.
0036–41, 2009.
[6] D. Chen, N. A. Eisley, P. Heidelberger, R. M. Senger, Y. Sugawara,
S. Kumar, V. Salapura, D. L. Satterﬁeld, B. Steinmacher-Burow, and
J. J. Parker, “The ibm blue gene/q interconnection network and mes-
sage unit,” in High Performance Computing, Networking, Storage and
Analysis (SC), 2011 International Conference for.
IEEE, 2011, pp.
1–10.
[7] G. Faanes, A. Bataineh, D. Roweth, E. Froese, B. Alverson, T. Johnson,
J. Kopnick, M. Higgins, J. Reinhard et al., “Cray cascade: a scalable
hpc system based on a dragonﬂy network,” in Proceedings of
the
International Conference on High Performance Computing, Networking,
Storage and Analysis.
IEEE Computer Society Press, 2012, p. 103.
[8] Z. Pang, M. Xie, J. Zhang, Y. Zheng, G. Wang, D. Dong, and G. Suo,
“The th express high performance interconnect networks,” Frontiers of
Computer Science, vol. 8, no. 3, pp. 357–366, 2014.
[9] S. L. Scott et al., “The cray t3e network: adaptive routing in a high
performance 3d torus,” 1996.
[10] M. Blumrich, D. Chen, P. Coteus, A. Gara, M. Giampapa, P. Hei-
delberger, S. Singh, B. Steinmacher-Burow, T. Takken, and P. Vranas,
“Design and analysis of the bluegene/l torus interconnection network,”
IBM Research Report RC23025 (W0312-022), Tech. Rep., 2003.
[11] W. J. Dally and B. P. Towles, Principles and practices of interconnection
networks. Elsevier, 2004.
[12] N. R. Adiga, M. A. Blumrich, D. Chen, P. Coteus, A. Gara, M. E. Gi-
ampapa, P. Heidelberger, S. Singh, B. D. Steinmacher-Burow, T. Takken
et al., “Blue gene/l torus interconnection network,” IBM Journal of
Research and Development, vol. 49, no. 2.3, pp. 265–276, 2005.
[13] J. Duato, S. Yalamanchili, and L. M. Ni, Interconnection networks: an
engineering approach. Morgan Kaufmann, 2003.
[14] P. Gill, N. Jain, and N. Nagappan, “Understanding network failures
in data centers: measurement, analysis, and implications,” in ACM
SIGCOMM Computer Communication Review, vol. 41, no. 4. ACM,
2011, pp. 350–361.
[15] D. Abts and B. Felderman, “A guided tour of data-center networking,”
Communications of the ACM, vol. 55, no. 6, pp. 44–51, 2012.
[16] C. Di Martino, W. Kramer, Z. Kalbarczyk, and R. Iyer, “Measuring
and understanding extreme-scale application resilience: A ﬁeld study of
5,000,000 hpc application runs,” in Dependable Systems and Networks
(DSN), 2015 45th Annual IEEE/IFIP International Conference on.
IEEE, 2015, pp. 25–36.
[17] S. Jha, V. Formicola, Z. Kalbarczyk, C. Di Martino, W. T. Kramer,
and R. K. Iyer, “Analysis of gemini interconnect recovery mechanisms:
Methods and observations,” Cray User Group, pp. 8–12, 2016.
[18] C. E. Leiserson, “Fat-trees: universal networks for hardware-efﬁcient
supercomputing,” IEEE transactions on Computers, vol. 100, no. 10,
pp. 892–901, 1985.
[19] W. J. Dally, “Performance analysis of k-ary n-cube interconnection
networks,” IEEE transactions on Computers, vol. 39, no. 6, pp. 775–785,
1990.
[20] ——, “Express cubes: Improving the performance of k-ary n-cube
interconnection networks,” IEEE Transactions on Computers, vol. 40,
no. 9, pp. 1016–1023, 1991.
[21] D. W. Mackenthun, “Method and apparatus for automatically routing
around faults within an interconnect system,” Sep. 12 1995, uS Patent
5,450,578.
[22] Y. Inoguchi and S. Horiguchi, “Shifted recursive torus interconnection
for high performance computing,” in High Performance Computing on
the Information Superhighway, 1997. HPC Asia’97.
IEEE, 1997, pp.
61–66.
[23] V. Puente, R. Beivide, J. A. Gregorio, J. Prellezo, J. Duato, and
C. Izu, “Adaptive bubble router: a design to improve performance
in torus networks,” in Parallel Processing, 1999. Proceedings. 1999
International Conference on.
IEEE, 1999, pp. 58–67.
[24] J. Domke, T. Hoeﬂer, and S. Matsuoka, “Fail-in-place network design:
interaction between topology, routing algorithm and failures,” in High
Performance Computing, Networking, Storage and Analysis, SC14:
International Conference for.
IEEE, 2014, pp. 597–608.
[25] A. Vishnu, M. ten Bruggencate, and R. Olson, “Evaluating the potential
of cray gemini interconnect for pgas communication runtime systems,”
in High Performance Interconnects (HOTI), 2011 IEEE 19th Annual
Symposium on.
IEEE, 2011, pp. 70–77.
[26] B. Alverson, E. Froese, L. Kaplan, and D. Roweth, “Cray xc series
network,” Cray Inc., White Paper WP-Aries01-1112, 2012.
8