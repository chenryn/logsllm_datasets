(ym by 
(method, code) (rate(rpc_invoke_cnt_c{code!="0"}
---
## Page 58
Consul、etcd和K8S。
试使用了Prometheus的服务发现功能。
分痛苦的过程，配置文件庞大，列表过长，修改的过程极其容易眼花导致误操作。所以我们尝
即可。例如：
从配置文档中不难发现Prometheus对服务发现进行了大量的支持，例如大家喜闻乐见的
但随着服务器量级增长，业务整合到同一组Prometheus时，每次上下线实例都是一个十
static_configs:
scrape_interval:5s
job_name:'serv
static_configs:
scrape_interval:5s
job_name:'serv
-targets:
1
10.
10.
'10.
10.
'10.
'10.
'10.
10.
'10
"10.
'10.
10.
'10.
:9100
：9100
:9100
:9100
微服务－Prometheus落地实践
：9100
:9100
:9100
:9100
:9100
:9100
:9100
:9100
:9100
53
---
## Page 59
下面是一个测试服务生成的json文件样例。
file_sd_config的要求生成对应的json格式。
er编写了一个小工具，定时到gokeeper中采集服务分类及分类中的服务器列表，并按照
Prometheus 会自动刷新 target列表，不需要手动触发reload操作。所以针对我们的gokeep-
过file_sd_config进行服务发现的配置。
Prometheus原生支持，但是通过简单适配也是同样能满足服务发现的需求。我们最终选择通
54Prometheus 落地实践－微服务
file_sd_config 接受json格式的配置文件进行服务发现。每次json文件的内容发生变更,
由于最近参与的几个项目深度使用公司内部的配置管理服务gokeeper，虽然不是
---
## Page 60
ing/configuration/#
支持。
线情况是否符合期望即可。社区比较推崇Consul作为服务发现组件，也有非常直接的内部配置
figs可以极大地减少配置维护的复杂度。只需要关注一下Prometheus后台采集任务的分组和在
 感兴趣的话可以直接参考官方文档进行配置和测试-https://prometheus.io/docs/operat-
如果用etcd作为服务发现组件也可以使用此种方式，结合confd配合模版和file_sd_con-
Prometheus配置文件中将file_sd_configs的路径指向json文件即可。
-job_name:
file_sd_configs:
scrape_interval:5s
"labels":{
"targets":[ 
"targets":[
labels":{
'qtest'
"job":"Gateway",
"10.10.10.4:65110"
"10.10.10.3:65110",
"job":"Center",
"service":"gtest
"service":"qtest"
"10.10.10.2:65160"
"10.10.10.1:65160",
微服务－Prometheus落地实践
55
---
## Page 61
在Global节点Prometheus.yml也需要进行修改。
在这里job:rpc_invoke_cnt:rate:1m 将作为metric名，用来存放查询语句的结果。
编辑规则文件：
 Prometheus.yml: 
Global节点采集归档数据用于绘图。首先需要在Shard节点进行一些配置。
要使用双节点重复采集进行保活。
ation来实现高可用方案，但是边缘节点和Global节点依然是单点，需要自行决定是否每一层都
高可用
56Prometheus 落地实践－微服务
使用方法比较简单，例如我们一个机房有三个Prometheus节点用于Shard，我们希望
高可用目前暂时没有太好的方案。官方给出的方案可以对数据做Shard，然后通过feder-
node_rules/zep.test.rules:
scrape_configs:
rule_files:
global:
- job_name: myjob
- node_rules/zep.test.rules 
slave：〇#给每一个节点指定一个编号三台分别标记为0，1，2
external_labels:
- source_labels: [_tmp_hash]
- source_labels:[_address_
relabel_configs:
-files:['/usr/local/prometheus/qtestgroups/*.json
file_sd_configs:
action:
regex:
action:
target_label:
modulus:
keep
^0$#表示第一个节点
hashmod
_tmp_hash 
3#3节点
#指定rulefile的路径
---
## Page 62
单对接即可使用。
rules_file中编辑的查询触发条件，Prometheus会主动通知Alertmanager然后发出报警。由
经实现了报警组分类，按标签内容发送不同报警组、报警合并、报警静音等基础功能。配合
报警
较严格，
查询的metric数据量比较大时，网络和磁盘lO开销巨大。因此在绘图时我们对查询语句限制比
据，横向扩展十分方便。
掉 Global 节点和大量存储资源，并且 Proxy 节点由于不需要存储数据，仅接受请求和计算数
Prometheus内建的函数和聚合操作，最后将计算数据抛给查询客户端。这样便可以直接节约
求，然后将查询语句拆解，到各shard节点抓取基础数据，然后再在Proxy这里进行
请求异常这类报警我们都放在shard节点进行报警。
链路的稳定性会影响数据到达的效率，进而导致报警实效降低。例如服务updown状态，API
达到单Prometheus节点的承载能力上限。
HTTP读取原始数据会造成大量lO和网络开销），同时所有数据写入Global节点也会使其很快
来存储再进行查询和报警的操作。这样不但会使Shard节点计算和查询的压力增大（通过
 Prometheus的报警功能目前来看相对比较简单。主要是利用Alertmanager这个组件。已
此外我们还编写了一个实验性质的 Prometheus Proxy工具，代替Global节点接收查询请
另外部分敏感报警尽量不要通过 Global 节点触发，毕竟从 Shard 节点到 Global 节点传输
在这里我们只采集了执行规则后的数据用于绘图，不建议将Shard节点的所有数据采集过
，基本不允许进行无labe限制的模糊查询。
-job_name: slaves
static_configs:
params:
metrics_path: /federate
scrape_interval:5s
honor_labels: true
- targets:
match[]:
-10.10.10.152:9090
-10.10.10.151:9090
-10.10.10.150:9090
微服务－Prometheus 落地实践
5
---
## Page 63
定制查询提供了极大的便利。
可以做非常多的定制化，同时Grafana的template也可以作为参数传到查询语句中，对多维度
绘图展示
监控项，随后再启用osddown 报警项才使报警恢复。
警项，直接重启 ceph_exporter，再调用 Prometheus APl 删掉对应 osd 编号的 osdupdown
ager都会发来一组osddown的报警短信。
有关闭报警，触发了几个osddown的报警，报警刷新周期2小时，那么每过两小时Alertman-
要么从后台silence掉，要么想办法使报警恢复。例如前段时间我们缩容Ceph集群，操作前没
恢复的规则一直不能触发，那么已触发的报警会按照Alertmanager配置的周期一直重复发送，
群报警使用的就是Slack，响应速度还是很不错的。
58Prometheus 落地实践－微服务
 对于页面展示，我们使用的是Grafana，如下面两张图，是两个不同服务的Dashboard,
如下图的报警详情页面，红色的是已触发的报警，绿色的是未触发报警：
对应编号的 osd 由于已经删掉已经不能再写入 up 对应的监控值，索性停掉 osddown 报
需要注意的是，如果报警已经触发，但是由于一些原因，比如删除业务监控节点，使报警
 Alertmanager也内建了一部分报警方式，如Email和第三方的Slack，初期我们的存储集
CRE
1111
三
---
## Page 64
本文链接： https:/lopsdev.cn/post/prometheus-with-wonder-1.html 
A：直接通过Prometheus接口处理即可
Q：删除配置文件job，但是通过查询还有job记录，怎么删除job记录？
简单修改即可成为Alertmanager的报警规则。
目中注册打点，并通过Http接口暴露出来。报警没有结合Grafana，不过大多数Grafana中使用的查询语句，
A：exporter 的编写可以单独拿出来写一篇文章了。我们这边主要使用的 Prometheus Golang SDK，在项
Q：exporter的编写及使用方法,以及Prometheus如何结合Grafana使用和 Prometheus 是如何进行报警的。
A：直接演示一条报警规则吧。
Q:Prometheus Alertmanager，能结合案例来一个么?
面对面：
 curl -X"DELETE""http://prometheus:9090/api/v1/series?match[={job="your job")" 
ALERT SlowRequest
ANNOTATIONS{
LABELs { qalarm = "true"
FOR1m
summary = "Ceph Slow Requests",
>
微服务－Prometheus落地实践
10
■
扫查看文章详
E
口
59
H
情
口
---
## Page 65
Neutron 网络数据面的基本架构。
Linux Director 和 Real Server 虚机所在的物理主机之间的网络可达。如下图是 Openstack
stack Neutron网络的整个架构。为了重点讲述Openstack Neutron网络及安全组，我们假设
确定网络结构
）－回归测试－总结回顾”的思路来 就基本搞定了。
确定网络结构－抓包定位－分析定位问题－纠正网络部署－单元测试－完整测试－（推入生产
正常的通讯，Real Server 是由 Hulk 平台创建的 Openstack 虚拟机。网络问题排查，按照“
问题简述
Openstack Neutron安全组，之后再开篇细讲背后的网络内核netfiter。
入探讨一下 iptables 和 netfilter 相关用法、原理等。本节先看看网络问题的解决思路以及
■带着问题了解OPENSTACKNEUTRON安全组
 60带着问题了解 Openstack Neutron 安全组－虚拟化
公司系统部同事为业务部门创建 LVS 服务时，Linux Director 与 Real Server 间无法进行
I Oct. 20th 2017 BY 李文新
虚拟化
遇到网络问题，首先考虑数据包在全流程中drop的位置，所以，首先我们要熟知Open-
 本文是由最近一个 Openstack Neutron 安全组的问题所触发而写。希望之后借此机会深
TAP device
Configured by L2Agent
VLAN101
vhetO