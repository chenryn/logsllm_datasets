title:SiP-ML: high-bandwidth optical network interconnects for machine learning
training
author:Mehrdad Khani Shirkoohi and
Manya Ghobadi and
Mohammad Alizadeh and
Ziyi Zhu and
Madeleine Glick and
Keren Bergman and
Amin Vahdat and
Benjamin Klenk and
Eiman Ebrahimi
SiP-ML: High-Bandwidth Optical Network Interconnects for Machine
Learning Training
Mehrdad Khani1, Manya Ghobadi1, Mohammad Alizadeh1, Ziyi Zhu2, Madeleine Glick2, Keren Bergman2,
Amin Vahdat3, Benjamin Klenk4, Eiman Ebrahimi4
1Massachusetts Institute of Technology 2Columbia University
3Google
4NVIDIA
tasks can still take days and even weeks [2–4]. Solutions such as
NVIDIA DGX [5] enable distributed training on a small number
of GPUs (e.g., 8–16) connected with a high-speed electrical switch
with Tbps bandwidth, but large-scale ML clusters must resort to
connecting GPU servers over much slower infiniband fabrics [6,
7]. We argue that future distributed ML training workloads are
likely to require several Tbps of bandwidth per device at large
scales, creating a pressing need for entirely new ways to build
interconnects for distributed ML systems.
With Silicon Photonic (SiP) technology [8–18], it is now possi-
ble to build I/O interfaces integrated with an electronic chip with
Tbps bandwidth [8, 19]. These optical I/O chiplets can be directly
integrated into a CPU/GPU/FPGA/ASIC package [20], providing
significantly higher bandwidth density than today’s technologies.
This paper proposes an end-to-end optical solution, called SiP-
ML, for strong scaling of ML workloads by leveraging SiP chiplets.
SiP-ML exploits the predictability of ML training traffic patterns to
find a parallelization strategy that meets the limitations of the opti-
cal topology at hand. Specifically, we explore two all-optical archi-
tectures: (i) SiP-OCS, an Optical Circuit Switch (OCS) design based
on commercially available switches; and (ii) SiP-Ring, a switch-
less ring design enabled by reconfigurable Micro-ring resonators
(MRRs) [21] embedded in SiP interfaces [22, 23]. Each of these archi-
tectures inherits one of the constraints of optical circuit-switched
interconnects to an extreme. Optical Circuit Switches are too slow
to reconfigure (e.g., 10 ms [24–26]) for ML models with a few mil-
liseconds of iteration time, while the ring topology can only support
communication between nearby GPUs. We show that SiP-ML’s par-
allelization algorithm can produce traffic patterns suited to both
these constraints by taking the degree limitation of all-optical circuit-
switched interconnects as an input parameter.
To evaluate SiP-ML, we develop a detailed simulator for dis-
tributed neural network training. Our simulation results show the
following: (1) for representative Natural Language Processing and
Computer Vision DNN models, SiP-ML speeds up the total training
time by a factor of 1.3–9.1× compared to today’s electrical network
fabrics; (2) although SiP-Ring’s switchless design constrains the
physical topology to a ring, it performs similarly to SiP-OCS be-
cause of the fast reconfigurability offered by the MRRs; (3) a SiP-ML
interconnect with per-GPU bandwidth B performs as well as or
better than an ideal, full-bisection electrical switch with per-GPU
bandwidth B/2; (4) when per-GPU bandwidth is high (e.g., order of
Terabits-per-second), hybrid parallelism strategies outperform data
parallelism by up to 2× in terms of time-to-accuracy.
This work does not raise any ethical issues.
ABSTRACT
This paper proposes optical network interconnects as a key enabler
for building high-bandwidth ML training clusters with strong scal-
ing properties. Our design, called SiP-ML, accelerates the training
time of popular DNN models using silicon photonics links capable
of providing multiple terabits-per-second of bandwidth per GPU.
SiP-ML partitions the training job across GPUs with hybrid data
and model parallelism while ensuring the communication pattern
can be supported efficiently on the network interconnect. We de-
velop task partitioning and device placement methods that take the
degree and reconfiguration latency of optical interconnects into
account. Simulations using real DNN models show that, compared
to the state-of-the-art electrical networks, our approach improves
training time by 1.3–9.1×.
CCS CONCEPTS
• Networks → Network architectures; Network design and
planning algorithms;
KEYWORDS
Optical networks, Distributed Machine Learning, Silicon photonics,
Reconfigurable networks
ACM Reference Format:
Mehrdad Khani, Manya Ghobadi, Mohammad Alizadeh, Ziyi Zhu, Madeleine 
Glick, Keren Bergman, Amin Vahdat, Benjamin Klenk, Eiman Ebrahimi. 
2021. SiP-ML: High-Bandwidth Optical Network Interconnects for Machine 
Learning Training. In ACM SIGCOMM 2021 Conference (SIGCOMM ’21), 
August 23–27, 2021, Virtual Event, USA. ACM, New York, NY, USA, 19 pages. 
https://doi.org/10.1145/3452296.3472900
1 
The ever-growing demand for more accurate machine learning (ML)
models has resulted in a steady increase in the dataset and model
sizes of deep neural networks (DNNs). Since 2012, the amount of
compute used in the largest AI training jobs has been increasing
exponentially with a 3.4-month doubling time [1], 50× faster than
the pace of Moore’s Law.
The computation requirements of large ML models has been
partly met by the rapid development of ML hardware accelerators
and specialized software stacks. Although hardware accelerators
have provided a significant amount of speed-up, today’s training
INTRODUCTION
This work is licensed under a Creative Commons Attribution International 4.0 License.
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8383-7/21/08.
https://doi.org/10.1145/3452296.3472900
657
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
M. Khani et al.
)
d
e
z
i
l
a
m
r
o
n
(
t
u
p
r
h
T
100
10
1
Transformer
ResNet50
Ideal
0.1
0.01
0.001
)
d
e
z
i
l
a
m
r
o
n
(
.
c
c
A
-
o
t
-
e
m
T
i
10
100
1,000
Number of GPUs
(b) Time-to-Accuracy
10
100
1,000
Number of GPUs
(a) Throughput
Figure 1: Weak scaling in today’s training systems.
2 BACKGROUND AND MOTIVATION
This section describes the key concepts of designing scalable ML
training interconnects. First, we discuss various parallelization
strategies for distributed training (§2.1). Then, we describe weak
and strong scaling and identify their network bandwidth require-
ments (§2.2). Finally, we introduce Silicon Photonics as a promising
technology to build high-bandwidth ML training interconnects
(§2.3).
2.1 Parallelization Strategies
Data Parallelism (DP). A popular parallelization strategy is data
parallelism where a batch of training data is distributed across
multiple workers. Each worker has an identical copy of the DNN
model but trains on a subset of the training batch, called a local
batch, in parallel. In DP training, workers need to communicate
their model weight updates after each iteration. This step can be
performed using various techniques such as broadcasting [27], pa-
rameter servers [28], ring-allreduce [29–31], and tree-reduce [32].
Model Parallelism (MP). In this approach, the DNN model is par-
titioned across different workers [33, 34]. The batch is copied to all
MP workers, and different parts of the DNN model are computed on
different workers, resulting in faster iteration times. Model paral-
lelism is an active area of research, with various proposals for model
partitioning [35–38]. Recent work has shown significant gains can
be obtained with model parallelism; however, the degree of model
parallelism has been limited to a few tens of workers [39–42].
Hybrid Parallelism. We consider a hybrid of the above paralleliza-
tion strategies. Our proposed interconnects and task partitioning
algorithms are designed specifically to support a hybrid of DP and
MP.Further, we do not make any assumptions about a specific com-
munication pattern, such as ring-allreduce or all-to-all. Our goal is
to support a variety of communication patterns using smart task
partitioning and GPU placement algorithms (details in §3).
2.2 Weak and Strong Scaling of ML Jobs
To identify the bandwidth requirements of ML systems, we first
describe two fundamental scaling paradigms.
Approach 1: Weak Scaling. The first approach is to scale the
throughput of data processing (number of processed data sam-
ples/sec) as the number of workers increases. The principal tech-
nique for throughput scaling is to keep the local batch size per
worker fixed and grow the global batch size as more workers are
added to the training job [43]. As a result, the entire system is able
to process a larger global batch while keeping the iteration time of
each worker the same. It is widely thought that training with large
batches reduces the time-to-accuracy because large batches can
produce better model updates, allowing the training to converge
with fewer total iterations [44, 45]. However, increasing the global
batch size in DNN training does not always translate to improving
the number of iterations for all models [46, 47]. As an example,
Fig. 1 compares the throughput and time-to-accuracy of two DNN
models: Transformer [48] and ResNet-50 [49]. The numbers are
obtained from Nvidia’s benchmark results [50]. As shown in Fig. 1a,
increasing the number of GPUs increases the batch size and thus
improves the throughput (images/sec) of both models. However,
the time-to-accuracy does not scale at the same rate and starts to
plateau at large scales, as shown in Fig. 1b. As we show in our eval-
uations, reducing the time-to-accuracy at 1000-GPU scale requires
significantly higher bandwidth than today’s clusters (§4).
Approach 2: Strong Scaling. Instead of reducing the number of it-
erations, a more effective scaling approach is to reduce the iteration
time as the number of workers increases. This approach is called
strong scaling [43]. In contrast to weak scaling where the system
operates on a larger global batch size as the system scales, strong
scaling parallelizes the computation for a fixed batch size either by
reducing the local batch size per worker or by partitioning the com-
putation task across workers. However, achieving strong scaling
is challenging, because reducing the iteration time leads to more
frequent model updates and, hence, requires the I/O bandwidth to
scale with the number of workers [47]. Furthermore, since each
worker must perform small granular computations, strong scaling
can be sensitive to network latency and small inefficiencies in the
compute/network software stack.
Bandwidth Requirements of Weak and Strong Scaling. Today,
the technique most commonly used to scale a distributed training
job is weak scaling using the DP strategy. This approach is popular
because as more workers are added to the job: (i) the computation
time of each worker remains constant (since the local batch is
constant); and (ii) the size of data transfers at each iteration remains
constant (because it depends on the DNN model).1 In contrast, in
strong scaling approaches, the bandwidth requirement increases
(often super linearly) as the system is scaled, since (i) strong scaling
leads to reduced computation time per worker and shorter training
iterations, and (ii) the amount of data exchanged at each iteration
stays the same or even grows with scale.2 In today’s systems, the
degree of MP is limited to 8 or 16 workers within one DGX box [51]
with Tbps communication bandwidth per GPU [42, 52–54].
2.3 Silicon Photonics for ML Training
A straightforward approach to meet the high-bandwidth require-
ment of large-scale training workloads is to augment the bandwidth
of existing electrical switches. However, recent trends in SERDES/-
packet switching technology suggest that we will hit a wall in
1 The amount of data transferred in DP in each iteration depends on the all-reduce
algorithm. With a ring-reduce implementation, each worker exchanges 2×M, where M
is the DNN model size. Note that as the number of workers increase, the bandwidth
per worker remains constant but the total required bandwidth grows.
2The amount of data transferred in MP in each iteration depends on the model parti-
tioning strategy but often increases significantly with scale, particularly when a kernel
is split on anything other than the batch dimension.
658
SiP-ML: Optical Network Interconnects for Machine Learning
SIGCOMM ’21, August 23–27, 2021, Virtual Event, USA
Figure 3: Two topologies we consider for SiP-ML.
domains: (i) a Gbps bandwidth domain that interconnects thou-
sands of servers using conventional network fabrics and off-the-
shelf NICs; (ii) an all-to-all Tbps bandwidth domain that tightly
connects a handful of GPUs inside a server or a DGX. In contrast, a
SiP-ML cluster consists of disaggregated GPUs, each equipped with
Tbps SiP interfaces, interconnected by an all-optical network. An
example of a SiP interface is the TeraPHY optical I/O technology de-
veloped by Ayar Labs [64], capable of carrying 2 Tbps bandwidth (80
wavelengths each carrying 25 Gbps [65]). A GPU can be equipped