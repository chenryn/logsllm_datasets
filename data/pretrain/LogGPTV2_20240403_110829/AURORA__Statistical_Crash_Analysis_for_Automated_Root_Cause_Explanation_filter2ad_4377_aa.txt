title:AURORA: Statistical Crash Analysis for Automated Root Cause Explanation
author:Tim Blazytko and
Moritz Schl&quot;ogel and
Cornelius Aschermann and
Ali Abbasi and
Joel Frank and
Simon W&quot;orner and
Thorsten Holz
Aurora: Statistical Crash Analysis for 
Automated Root Cause Explanation
Tim Blazytko, Moritz Schlögel, Cornelius Aschermann, Ali Abbasi, 
Joel Frank, Simon Wörner, and Thorsten Holz, Ruhr-Universität Bochum
https://www.usenix.org/conference/usenixsecurity20/presentation/blazytko
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.AURORA: Statistical Crash Analysis for Automated Root Cause Explanation
Tim Blazytko, Moritz Schlögel, Cornelius Aschermann, Ali Abbasi,
Joel Frank, Simon Wörner and Thorsten Holz
Ruhr-Universität Bochum, Germany
Abstract
Given the huge success of automated software testing tech-
niques, a large amount of crashes is found in practice. Identi-
fying the root cause of a crash is a time-intensive endeavor,
causing a disproportion between ﬁnding a crash and ﬁxing
the underlying software fault. To address this problem, vari-
ous approaches have been proposed that rely on techniques
such as reverse execution and backward taint analysis. Still,
these techniques are either limited to certain fault types or
provide an analyst with assembly instructions, but no context
information or explanation of the underlying fault.
In this paper, we propose an automated analysis approach
that does not only identify the root cause of a given crash-
ing input for a binary executable, but also provides the ana-
lyst with context information on the erroneous behavior that
characterizes crashing inputs. Starting with a single crashing
input, we generate a diverse set of similar inputs that either
also crash the program or induce benign behavior. We then
trace the program’s states while executing each found input
and generate predicates, i. e., simple Boolean expressions that
capture behavioral differences between crashing and non-
crashing inputs. A statistical analysis of all predicates allows
us to identify the predicate pinpointing the root cause, thereby
not only revealing the location of the root cause, but also pro-
viding an analyst with an explanation of the misbehavior a
crash exhibits at this location. We implement our approach in
a tool called AURORA and evaluate it on 25 diverse software
faults. Our evaluation shows that AURORA is able to uncover
root causes even for complex bugs. For example, it succeeded
in cases where many millions of instructions were executed
between developer ﬁx and crashing location. In contrast to
existing approaches, AURORA is also able to handle bugs with
no data dependency between root cause and crash, such as
type confusion bugs.
1 Introduction
Fuzz testing (short: fuzzing) is a powerful software testing
technique that, especially in recent years, gained a lot of trac-
tion both in industry and academia [28, 29, 31, 47, 49, 53, 59].
In essence, fuzzing capitalizes on a high throughput of in-
puts that are successively modiﬁed to uncover different paths
within a target program. The recent focus on new fuzzing
methods has produced a myriad of crashes for software sys-
tems, sometimes overwhelming the developers who are tasked
with ﬁxing them [37, 50]. In many cases, ﬁnding a new crash-
ing input has become the easy and fully automated part, while
triaging crashes remains a manual, labor-intensive effort. This
effort is mostly spent on identifying the actual origin of a
crash [58]. The situation is worsened as fuzzing campaigns
often result in a large number of crashing inputs, even if only
one actual bug is found: a fuzzer can identify multiple paths
to a crash, while the fault is always the same. Thus, an ana-
lyst has to investigate an inﬂated number of potential bugs.
Consequently, developers lose time on known bugs that could
be spent on ﬁxing others.
To reduce the inﬂux of crashes mapping to the same bug,
analysts attempt to bucket such inputs. Informally speaking,
bucketing groups crashing inputs according to some metric—
often coverage or hashes of the call stack—into equivalence
classes. Typically, it is assumed that analyzing one input from
each class is sufﬁcient. However, recent experiments have
shown that common bucketing schemes produce far too many
buckets and, even worse, cluster distinct bugs into the same
bucket [42]. Even if there are only a few inputs to investigate,
an analyst still faces another challenge: Understanding the
reasons why a given input leads to a crash. Often, the real
cause of a crash—referred to as root cause—is not located at
the point the program crashes; instead, it might be far earlier
in the program’s execution ﬂow. Therefore, an analyst needs
to analyze the path from the crashing location backward to
ﬁnd the root cause, which requires signiﬁcant effort.
Consider, for example, a type confusion bug: a pointer to
an object of type A is used in a place where a pointer to B is
expected. If a ﬁeld of B is accessed, an invalid access on a
subsection of A can result. If the structures are not compatible
(e. g., A contains a string where a pointer is expected by B),
this can cause memory corruption. In this case, the crashing
USENIX Association
29th USENIX Security Symposium    235
location is most likely not the root cause of the fault, as the
invariant “points to an instance of B” is violated in a different
spot. The code that creates the object of type A is also most
likely correct. Instead, the particular control ﬂow that makes
a value from type A end up in B’s place is at fault.
In a naive approach, an analyst could inspect stack and
register values with a debugger. Starting from the crash, they
can manually backtrace the execution to the root cause. Using
state-of-the-art sanitizers such as the ASAN family [51] may
detect illegal memory accesses closer to the root cause. In
our example, the manual analysis would start at the crashing
location, while ASAN would detect the location where the
memory corruption occurred. Still, the analyst has to manually
recognize the type confusion as the root cause—a complicated
task since most code involved is behaving correctly.
More involved approaches such as POMP [57], RE-
TRACER [33], REPT [32] and DEEPVSA [38] use auto-
mated reverse execution and backward taint analysis. These
are particularly useful if the crash is not reproducible. For
example, REPT and RETRACER can analyze crashes that
occurred on end-devices by combining core dumps and In-
tel PT traces. However, these approaches generally do not
allow to automatically identify the root cause unless there is
a direct data dependency connecting root cause and crashing
instruction. Furthermore, REPT and RETRACER focus on
providing an interactive debugging session for an analyst to
inspect manually what happened before the crash.
In cases such as the type confusion above, or when debug-
ging JIT-based software such as JavaScript engines, a single
crashing input may not allow identifying the root cause with-
out extensive manual reasoning. Therefore, one can use a
fuzzer to perform crash exploration. In this mode, the fuzzer
is seeded with crashing inputs which it mutates as long as
they keep crashing the target application. This process gener-
ates new inputs that are related to the original crashing input,
yet slightly different (e. g., they could trigger the crash via a
different path). A diverse set of crashing inputs that mostly
trigger the same bug can aid analysis. Observing multiple
ranges of values and different control-ﬂow edges taken can
help narrow down potential root causes. However, none of
the aforementioned methods takes advantage of this infor-
mation. Consequently, identifying the root cause remains a
challenging task, especially if there is no direct data depen-
dency between root cause and crashing instruction. Although
techniques such as ASAN, POMP, REPT and RETRACER
provide more context, they often fail to identify the root cause
and provide no explanation of the fault.
In this paper, we address this problem by developing an
automated approach capable of ﬁnding the root cause given
a crashing input. This signiﬁcantly reduces human effort:
unlike the approaches discussed previously, we do not only
identify a code location, but also an explanation of the prob-
lem. This also reduces the number of locations an analyst
has to inspect, as AURORA only considers instructions with a
plausible explanation.
To enable precise identiﬁcation of the root cause, we ﬁrst
pick one crashing input and produce a diverse set of similar
inputs, some of which cause a crash while others do not. We
then execute these newly-generated inputs while tracking the
binary program’s internal state. This includes control-ﬂow
information and relevant register values for each instruction.
Given such detailed traces for many different inputs, we create
a set of simple Boolean expressions (around 1,000 per instruc-
tion) to predict whether the input causes a crash. Intuitively,
these predicates capture interesting runtime behavior such
as whether a speciﬁc branch is taken or whether a register
contains a suspiciously small value.
Consider our previous type confusion example and assume
that a pointer to the constructor is called at some location in
the program. Using the tracked information obtained from the
diversiﬁed set of inputs, we can observe that (nearly) all calls
in crashing inputs invoke the constructor of type A, while calls
to the constructor of B imply that the input is not going to
cause a crash. Thus, we can pinpoint the problem at an earlier
point of the execution, even when no data taint connection
exists between crashing location and root cause. This exam-
ple also demonstrates that our approach needs to evaluate a
large set of predicates, since many factors have to be captured,
including different program contexts and vulnerability types.
Using the predicates as a metric for each instruction, we can
automatically pinpoint the possible root cause of crashes. Ad-
ditionally, the predicates provide a concrete explanation of
why the software fault occurs.
We built a prototype implementation of our approach in
a tool called AURORA. To evaluate AURORA, we analyze
25 targets that cover a diverse set of vulnerability classes,
including ﬁve use-after-free vulnerabilities, ten heap buffer
overﬂows and two type confusion vulnerabilities that previous
work fails to account for. We show that AURORA reliably
allows identifying the root cause even for complex binaries.
For example, we analyzed a type confusion bug in mruby
where an exception handler fails to raise a proper exception
type. It took an expert multiple days to identify the actual
fault. Using our technique, the root cause was pinpointed
automatically.
In summary, our key contributions are threefold:
• We present the design of AURORA, a generic approach
to automatically pinpoint the location of the root cause
and provide a semantic explanation of the crash.
• We propose a method to synthesize domain-speciﬁc pred-
icates for binary programs, tailored to the observed be-
havior of the program. These predicates allow accurate
predictions on whether a given input will crash or not.
236    29th USENIX Security Symposium
USENIX Association
• We implement a prototype of AURORA and demonstrate
that it can automatically and precisely identify the root
cause for a diverse set of 25 software faults.
To foster research on this topic, we release the implemen-
tation of AURORA at https://github.com/RUB-SysSec/
aurora.
not dereference the pointer but interpret the length ﬁeld as an
address, resulting in an attempt to dereference 0x20. Since
this leads to an illegal memory access, the program crashes.
To sum up, redeﬁning an exception type with a string leads
to a type confusion vulnerability, resulting in a crash when
this exception is raised. The developer ﬁx introduces a type
check, thus preventing this bug from provoking a crash.
2 Challenges in Root Cause Analysis
Despite various proposed techniques, root cause identiﬁcation
and explanation are still complex problems. Thus, we now
explore different techniques and discuss their limitations.
2.1 Running Example
The following code snippet shows a minimized example of
Ruby code that leads to a type confusion bug in the mruby
interpreter [16] found by a fuzzer:
1 N o t I m p l e m e n t e d E r r o r = S t r i n g
2 Module . c o n s t a n t s
In the ﬁrst line, the exception type NotImplementedError
is modiﬁed to be an alias of type String. As a consequence,
each instance of NotImplementedError created in the fu-
ture will be a String rather than the expected exception. In
the second line, we call the constants function of Module.
This function does not exist, provoking mruby to raise a
NotImplementedError. Raising the exception causes a crash
in the mruby interpreter.
To understand why the crash occurs, we need to dive
into the C code base of the mruby interpreter. Note
that mruby types are implemented as structs on the in-
terpreter level. When we re-assign the exception type
NotImplementedError to String, this is realized on C
level by modifying the pointer such that it points to a
struct representing the mruby String type. The method
Module.constants is only a stub that creates and raises
an exception. When the exception is raised in the second
line, a new instance of NotImplementedError is constructed
(which now actually results in a String object) and passed to
mruby’s custom exception handling function. This function
assumes that the passed object has an exception type without
checking this further. It proceeds to successfully attach some
error message—here “Module.constants not implemented”
(length 0x20)—to the presumed exception object. Then, the
function continues to ﬁll the presumable exception with de-
bug information available. During this process, it attempts to
dereference a pointer to a table that is contained within all
exception objects. However, as we have replaced the excep-
tion type by the string type, the layout of the underlying struct
is different: At the accessed offset, the String struct stores
the length of the contained string instead of a pointer as it
would be the case for the exception struct. As a result, we do
2.2 Crash Triaging
Assume our goal is to triage the previously explained bug,
given only the crashing input (obtained from a fuzzing run)
as a starting point. In the following, we discuss different
approaches to solve this task and explain their challenges.
Debugger. Starting at the crashing location, we can man-
ually inspect the last few instructions executed, the registers
at crashing point and the call stack leading to this situation.
Therefore, we can see that 0x20 is ﬁrst loaded to some register
and then dereferenced, resulting in the crash. Our goal then
is to identify why the code attempts to dereference this value
and how this value ended up there. We might turn towards
the call stack, which indicates that the problem arises during
some helper function that is called while raising an exception.
From this point on, we can start investigating by manually
following the ﬂow of execution backward from the crashing
cause up to the root cause. Given that the code of the mruby
interpreter is non-trivial and the bug is somewhat complex,
this takes a lot of time. Thus, we may take another angle
and use some tool dedicated to detecting memory errors, for
example, sanitizers.
Sanitizer. Sanitizers are a class of tools that often use
compile-time instrumentation to detect a wide range of soft-
ware faults. There are various kinds of sanitizers, such as
MSAN [52] to detect usage of uninitialized memory or
ASAN [51] to detect heap- and stack-based buffer overﬂows,
use-after-free (UAF) errors and other faults. Sanitizers usu-
ally rely on the usage of shadow memory to track whether
speciﬁc memory can be accessed or not. ASAN guards allo-
cated memory (e. g., stack and heap) by marking neighboring
memory as non-accessible. As a consequence, it detects out-
of-bounds accesses. By further marking freed memory as
non-accessible (as long as other free memory is available
for allocation), temporal bugs can be detected. MSAN uses
shadow memory to track for each bit, whether it is initialized
or not, thereby preventing unintended use of uninitialized
memory.
Using such tools, we can identify invalid memory accesses
even if they are not causing the program to crash immediately.
This situation may occur when other operations do not access
the overwritten memory. Additionally, sanitizers provide more
detailed information on crashing cause and location. As a
consequence, sanitizers are more precise and pinpoint issues
closer to the root cause of a bug.
USENIX Association
29th USENIX Security Symposium    237
Unfortunately, this is not the case for our example: re-
compiling the binary with ASAN provides no new insights
because the type confusion does not provoke any memory
errors that can be detected by sanitizers. Consequently, we
are stuck at the same crashing location as before.
Backward Taint Analysis. To deepen our understand-
ing of the bug, we could use automated root cause analysis
tools [32,33,57] that are based on reverse execution and back-
ward taint tracking to increase the precision further. However,
in our example, there is no direct data ﬂow between the crash
site and the actual root cause. The data ﬂow ends in the con-
structor of a new String that is unrelated to the actual root
cause. As taint tracking does not provide interesting informa-
tion, we try to obtain related inputs that trigger the same bug
in different crashing locations. Finding such inputs would
give us a different perspective on the bug’s behavior.
Crash Exploration. To achieve this goal, we can use the
so-called crash exploration mode [58] that fuzzers such as
AFL [59] provide. This mode takes a crashing input as a
seed and mutates it to generate new inputs. From the newly
generated inputs, the fuzzer only keeps those in the fuzzing
queue that still result in a crash. Consequently, the fuzzer
creates a diverse set of inputs that mostly lead to the same
crash but exhibited new code coverage by exercising new
paths. These inputs are likely to trigger the same bug via
different code paths.
To gain new insights into the root cause of our bug, we need
the crash exploration mode to trigger new behavior related to
the type confusion. In theory, to achieve this, the fuzzer could
assign another type than String to NotImplementedError.
However, fuzzers such as AFL are more likely to modify the
input to something like “Stringgg” or “Strr” than assigning
different, valid types. This is due to the way its mutations
work [30]. Still, AFL manages to ﬁnd various crashing inputs
by adding new mruby code unrelated to the bug.
To further strengthen the analysis, a fuzzer with access to
domain knowledge, such as grammar-based fuzzers [28, 35,
48], can be used. Such a fuzzer recognizes that String is a
grammatically valid element for Ruby which can be replaced
by other grammar elements. For example, String can be
replaced by Hash, Array or Float. Assume that the fuzzer
chooses Hash; the newly derived input crashes the binary at
a later point of execution than our original input. This result
beneﬁts the analyst as comparing the two inputs indicates that
the crash could be related to NotImplementedError’s type.
As a consequence, the analyst might start focusing on code
parts related to the object type, reducing the scope of analysis.
Still, this leaves the human analyst with an additional input to
analyze, which means more time spent on debugging.
Overall, this process of investigating the root cause of a
given bug is not easy and—depending on the bug type and
its complexity—may take a signiﬁcant amount of time and
domain knowledge. Even though various methods and tools
exist, the demanding tasks still have to be accomplished by a
human. In the following, we present our approach to automate
the process of identifying and explaining the root cause for a
given crashing input.
3 Design Overview
Given a crashing input and a binary program, our goal is to
ﬁnd an explanation of the underlying software fault’s root
cause. We do so by locating behavioral differences between
crashing and non-crashing inputs. In its core, our method
conducts a statistical analysis of differences between a set
of crashing and non-crashing inputs. Thus, we ﬁrst create a
dataset of diverse program behaviors related to the crash, then
monitor relevant input behavior and, ﬁnally, comparatively
analyze them. This is motivated by the insight that crashing
inputs must—at some point—semantically deviate from non-
crashing inputs. Intuitively, the ﬁrst relevant behavior during
program execution that causes the deviation is the root cause.
In a ﬁrst step, we create two sets of related but diverse
inputs, one with crashing and one with non-crashing inputs.
Ideally, we only include crashing inputs caused by the same
root cause. The set of non-crashing inputs has no such restric-
tions, as they are effectively used as counterexamples in our
method. To obtain these sets, we perform crash exploration
fuzzing on one initial crashing input (a so-called seed).
Given the two sets of inputs, we observe and monitor (i. e.,
trace) the program behavior for each input. These traces al-
low us to correlate differences in the observations with the
outcome of the execution. Using this statistical reasoning,
we can identify differences that predict whether a program
execution will crash or not. To formalize these differences, we
synthesize predicates that state whether a bug was triggered.
Intuitively, the ﬁrst predicate that can successfully predict the
outcome of all (or most) executions also explains the root
cause. As the ﬁnal result, we provide the analyst with a list
of relevant explanations and addresses, ordered by the quality
of their prediction and time of execution. That is, we prefer
explanations that predict the outcome well. Amongst good
explanations, we prefer these that are able to predict the crash
as early as possible.
On a high-level view, our design consist of three individual
components: (1) input diversiﬁcation to derive two diverse
sets of inputs (crashing and non-crashing), (2) monitoring
input behavior to track how inputs behave and (3) explana-
tion synthesis to synthesize descriptive predicates that distin-
guish crashing from non-crashing inputs. In the following, we
present each of these components.
Input Diversiﬁcation
3.1
As stated before, we need to create a diverse but similar set
of inputs for the single crashing seed given as input to our
approach. On the one hand, the inputs should be diverse such
that statistical analysis reveals measurable differences. On the
238    29th USENIX Security Symposium
USENIX Association
other hand, the inputs should share a similar basic structure