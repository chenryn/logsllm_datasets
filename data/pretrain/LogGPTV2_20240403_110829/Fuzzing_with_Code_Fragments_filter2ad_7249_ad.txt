### 4. TraceMonkey and Pre-Release Versions
Firefox 4 and its corresponding TraceMonkey version were pre-release (beta) versions. Changes to the TraceMonkey trunk branch were regularly merged back into the main repository.

Additionally, we ran LangFuzz on Mozilla’s type inference branch of TraceMonkey. At that time, this branch had alpha status and was not part of Firefox 4 (but was eventually included in Firefox 5). Since this branch was not a product branch, no security assessment was conducted for issues reported against it.

### 5.1 Real Defects Found
**Figure 10:** Real defects found in Mozilla, Google V8, and PHP. These defects were reported as customer defects, and their numbers are not comparable to the defect numbers in earlier figures. *The security lock might hide the issue report from public view because it might be exploitable. †Defects reported for PHP were not classified as security-relevant.*

### 5.2 Testing on Different JavaScript Engines

#### Google V8
Similar to the Mozilla field test, we tested LangFuzz on the Google V8 JavaScript engine contained within the development trunk branch. At the time of testing, Chrome 10, including the new V8 optimization technique "Crankshaft," was in beta stage, and fixes for this branch were regularly merged back into the Chrome 10 beta branch.

#### PHP
To verify LangFuzz’s language independence, we performed a proof-of-concept adaptation to PHP; see Section 6 for details. The experiment was conducted on the PHP trunk branch (SVN revision 309115) and lasted 14 days.

### 5.3 Can LangFuzz Detect Real Undetected Defects?
For all three JavaScript engines, LangFuzz found between 51 and 59 defects (see Figure 10). For the Mozilla TraceMonkey (FF4 Beta) branch, 39% of the found security issues were classified as security-related by the Mozilla development team. Only nine defects were classified as duplicates of bug reports not related to our experiments. The relatively low number of duplicates within all defect sets shows that LangFuzz detects defects that slipped through the quality gate of the individual projects, demonstrating its usefulness.

Although the fraction of security-related defects for the Google V8 branch is lower (19%), it still represents a significant number of new security-related defects. The number of security issues within the Mozilla TraceMonkey (Type Inference) branch is reported as zero, simply because this branch was not part of any product at the time of the experiment.

### 5.4 Economic Value
The number of defects detected by LangFuzz must be interpreted with regard to the actual value of these defects. Many of the defects were rewarded by bug bounty awards. Within nine months of experimenting with LangFuzz, defects found by the tool obtained 18 Chromium Security Rewards and 12 Mozilla Security Bug Bounty Awards. We can only speculate on the potential damage these findings prevented; in real money, however, the above awards translated into $50,000 in bug bounties. Indeed, during this period, LangFuzz became one of the top bounty collectors for Mozilla TraceMonkey and Google V8.

### 6. Adaptation to PHP
Although adapting LangFuzz to a new language is kept as simple as possible, some adaptations are required. Changes related to reading/running the respective project test suite, integrating the generated parser/lexer classes, and supplying additional language-dependent information (optional) are necessary. In most cases, the required effort for these changes is considerably lower than the effort required to write a new language fuzzer from scratch. The following is a short description of the changes required for PHP and in general:

- **Integration of Parser/Lexer Classes**: Given a grammar for the language, we first generate the Parser/Lexer Java classes using ANTLR (automatic step). For PHP, we chose the grammar supplied by the PHPParser project [5].

- **LangFuzz uses high-level parser/lexer classes** that override all methods called when parsing non-terminals. These classes extract the non-terminals during parsing and can be automatically generated from the classes provided by ANTLR. All these classes are part of LangFuzz and get integrated into the internal language abstraction layer.

- **Integration of Tests**: LangFuzz provides a test suite class that must be derived and adjusted depending on the target test suite. In the case of PHP, the original test suite is quite complex because each test is made up of different sections (not a single source code file). For our proof-of-concept experiment, we only extracted the code portions from these tests, ignoring setup/teardown procedures and other surrounding instructions. The resulting code files are compatible with the standard test runner, so our runner class does not need any new implementation.

- **Adding Language-Dependent Information (Optional)**: In this step, information about identifiers in the grammar and global built-in objects can be provided (e.g., taken from a public specification). For PHP, the grammar in use provides a single non-terminal in the lexer for all identifiers used in the source code, which we can add to our language class. Furthermore, the PHP online documentation provides a list of all built-in functions, which we can add to LangFuzz through an external file.

Adapting LangFuzz to test different languages is easy: provide the language grammar and integrate tests. Adding language-dependent information is not required but highly recommended.

### 7. Threats to Validity
Our field experiments covered different JavaScript engines and a proof-of-concept adaptation to a second weakly typed language (PHP). Nevertheless, we cannot generalize that LangFuzz will be able to detect defects in other interpreters for different languages. It might also be the case that there exist specific requirements or properties that must be met to make LangFuzz effective.

Our direct comparison with jsfunfuzz is limited to a single implementation and certain versions of this implementation. We cannot generalize the results from these experiments. Running LangFuzz and jsfunfuzz on different targets or testing windows might change comparison results.

The size and quality of test suites used by LangFuzz during learning and mutating have a major impact on its performance. Setups with fewer test cases or biased test suites might decrease LangFuzz’s performance.

Both jsfunfuzz and LangFuzz make extensive use of randomness. While some defects show up very quickly and frequently in all runs, others are harder to detect. Their discovery heavily depends on the time spent and the randomness involved. In our experiments, we tried to find a time limit that is large enough to minimize such effects but remains practical. Choosing different time limits might impact the experimental results.

For most experiments, we report the number of defects found. Some of the reported bugs might be duplicates. Duplicates should be eliminated to prevent bias. Although we invested a lot of effort to identify such duplicates, we cannot ensure that we detected all of them. This might impact the number of distinct defects discovered through the experiments.

### 8. Conclusion
Fuzz testing is easy to apply but needs language- and project-specific knowledge to be most effective. LangFuzz is an approach to fuzz testing that can easily be adapted to new languages (by feeding it with an appropriate grammar) and to new projects (by feeding it with an appropriate set of test cases to mutate and extend). In our evaluation, this made LangFuzz an effective tool in finding security violations, complementing project-specific tools that had been tuned towards their test subjects for several years. The economic value of the bugs uncovered by LangFuzz is best illustrated by the worth of its bugs, as shown by the awards and bug bounties it raised.

We recommend our approach for simple and effective automated testing of processors of complex input, including compilers and interpreters—especially those dealing with user-defined input.

### Acknowledgments
We thank the Mozilla, Google, and PHP development teams for their support. Guillaume Destuynder, Florian Gross, Clemens Hammacher, Christian Hammer, Matteo Maffei, and Eva May provided helpful feedback on earlier revisions of this paper.

### References
[1] https://bugzilla.mozilla.org/show_bug.cgi?id=610223.
[2] https://bugzilla.mozilla.org/show_bug.cgi?id=626345.
[3] https://bugzilla.mozilla.org/show_bug.cgi?id=626436.
[4] https://code.google.com/p/v8/issues/detail?id=1167.
[23] ZALEWSKI, M. Blog Entry. http://lcamtuf.blogspot.com/2011/01/announcing-crossfuzz-potential-0-day-in.html, 2011.
[24] ZELLER, A., AND HILDEBRANDT, R. Simplifying and isolating failure-inducing input. IEEE Transactions on Software Engineering (2002), 183–200.
[5] The phpparser project. http://code.google.com/p/phpparser/. Project website.
[6] AITEL, D. The advantages of block-based protocol analysis for security testing. Tech. rep., 2002.
[7] GODEFROID, P., KIEZUN, A., AND LEVIN, M. Y. Grammar-based whitebox fuzzing. SIGPLAN Not. 43, 6 (2008), 206–215.
[8] LINDIG, C. Random testing of C calling conventions. Proc. AADEBUG. (2005), 3–12.
[9] MCPEAK, S., AND WILKERSON, D. S. The delta tool. Project website. http://delta.tigris.org/.
[10] MILLER, B. P., FREDRIKSEN, L., AND SO, B. An empirical study of the reliability of Unix utilities. Commun. ACM 33 (December 1990), 32–44.
[11] MILLER, C., AND PETERSON, Z. N. J. Analysis of Mutation and Generation-Based Fuzzing. Tech. rep., Independent Security Evaluators, Mar. 2007.
[12] MOLNAR, D., LI, X. C., AND WAGNER, D. A. Dynamic test generation to find integer bugs in x86 binary Linux programs. In Proceedings of the 18th conference on USENIX security symposium (Berkeley, CA, USA, 2009), SSYM’09, USENIX Association, pp. 67–82.
[13] NEUHAUS, S., ZIMMERMANN, T., HOLLER, C., AND ZELLER, A. Predicting vulnerable software components. In Proceedings of the 14th ACM Conference on Computer and Communications Security (October 2007).
[14] OEHLERT, P. Violating assumptions with fuzzing. IEEE Security and Privacy 3 (March 2005), 58–62.
[15] PARR, T., AND QUONG, R. ANTLR: A predicated-LL(k) parser generator. Software: Practice and Experience 25, 7 (1995), 789–810.
[16] PURDOM, P. A sentence generator for testing parsers. BIT Numerical Mathematics 12 (1972), 366–375. 10.1007/BF01932308.
[17] RUDERMAN, J. Blog Entry. Introducing jsfunfuzz. http://www.squarefree.com/2007/08/02/introducing-jsfunfuzz/, 2007.
[18] SHU, G., HSU, Y., AND LEE, D. Detecting communication protocol security flaws by formal fuzz testing and machine learning. In Proceedings of the 28th IFIP WG 6.1 international conference on Formal Techniques for Networked and Distributed Systems (Berlin, Heidelberg, 2008), FORTE ’08, Springer-Verlag, pp. 299–304.
[19] SUTTON, M., AND GREENE, A. The art of file format fuzzing. In Blackhat USA Conference (2005).
[20] SUTTON, M., GREENE, A., AND AMINI, P. Fuzzing: Brute Force Vulnerability Discovery. Addison-Wesley Professional, 2007.
[21] TURNER, B. Random C program generator. Project website. http://sites.google.com/site/brturn2/randomcprogramgenerator, 2007.
[22] YANG, X., CHEN, Y., EIDE, E., AND REGEHR, J. Finding and Understanding Bugs in C Compilers. In Proceedings of the 2011 ACM SIGPLAN Conference on Programming Language Design and Implementation (June 2011), ACM SIGPLAN, ACM.