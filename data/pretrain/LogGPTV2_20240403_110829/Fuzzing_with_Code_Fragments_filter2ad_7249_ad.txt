4 and its corresponding TraceMonkey version were
pre-release (beta version). Changes to the Trace-
Monkey trunk branch were regularly merged back
into the main repository.
Additionally, we ran LangFuzz on Mozilla’s type
inference branch of TraceMonkey. At that time,
this branch had alpha status and has not been part
of Firefox 4 (but was eventually included in Fire-
fox 5). Since this branch was no product branch,
no security assessment was done for issues reported
against it.
11
Figure 10: Real defects found on Mozilla, Google V8,
and PHP. These defects were reported as customer de-
fects and their numbers are not comparable to the de-
fect numbers in earlier ﬁgures.∗The security lock might
hide the issue report from public because it might be ex-
ploitable. †Defects reported for PHP were not classiﬁed
security relevant.
Google V8 Similar to the Mozilla ﬁeld test, we tested
LangFuzz on the Google V8 JavaScript engine con-
tained within the development trunk branch. At
the time of testing, Chrome 10—including the new
V8 optimization technique “Crankshaft”—was in
beta stage and ﬁxes for this branch were regularly
merged back into the Chrome 10 beta branch.
PHP To verify LangFuzz’s language independence, we
performed a proof-of-concept adaptation to PHP;
see Section 6 for details. The experiment was con-
ducted on the PHP trunk branch (SVN revision
309115). The experiment lasted 14 days.
5.3.1 Can LangFuzz detect real undetected defects?
For all three JavaScript engines, LangFuzz found be-
tween 51 and 59 defects (see Figure 10). For the Mozilla
TraceMonkey (FF4 Beta) branch, most left group of bars
in Figure 10, 39% of the found security issues where
classiﬁed as security related by the Mozilla development
team. Only nine defects were classiﬁed as duplicates of
bug reports not being related to our experiments. The
relatively low number of duplicates within all defect sets
shows that LangFuzz detects defects that slipped through
the quality gate of the individual projects, showing the
usefulness of LangFuzz. Although the fraction of secu-
rity related defects for the Google V8 branch is lower
(19%), it is still a signiﬁcant number of new security re-
lated defects being found. The number of security is-
sues within the Mozilla TraceMonkey (Type Inference)
branch is reported as zero, simply because this branch
was not part of any product at the time of the experi-
total number of defectduplicates to non-LangFuzz defectssecurity related*604530150Mozilla TM(FF4 Beta)Mozilla TM(Type Inference)Chrome V8(Chrome 10 beta)PHP†512095400unknown45911182exactly those issue reports ﬁled during our ﬁeld experi-
ments. Due to the fact that some of the bug reports could
be used to exploit the corresponding browser version,
some issue reports are security locked requiring special
permission to open the bug report. At the time of writing
this paper, this affects all Google V8 issue reports.
LangFuzz detected 164 real world defects in popular
JavaScript engines within four months, including
31 security related defects. On PHP, LangFuzz
detected 20 defects within 14 days.
5.3.2 Economic value
The number of defects detected by LangFuzz must be
interpreted with regard to the actual value of these de-
fects. Many of the defects were rewarded by bug
bounty awards. Within nine month of experiment-
ing with LangFuzz, defects found by the tool obtained
18 Chromium Security Rewards and 12 Mozilla Security
Bug Bounty Awards. We can only speculate on the po-
tential damage these ﬁndings prevented; in real money,
however, the above awards translated into 50,000 US$ of
bug bounties. Indeed, during this period, LangFuzz be-
came one of the top bounty collectors for Mozilla Trace-
Monkey and Google V8.
6 Adaptation to PHP
Although adapting LangFuzz to a new language is kept
as simple as possible, some adaptations are required.
Changes related to reading/running the respective project
test suite, integrating the generated parser/lexer classes,
and supplying additional language-dependent informa-
tion (optional) are necessary. In most cases, the required
effort for these changes adaptation changes is consider-
ably lower than the effort required to write a new lan-
guage fuzzer from scratch. The following is a short de-
scription of the changes required for PHP and in general:
Integration of Parser/Lexer Classes. Given a gram-
mar for the language, we ﬁrst have to generate the
Parser/Lexer Java classes using ANTLR (automatic
step). For PHP, we choose the grammar supplied by
the PHPParser project [5].
LangFuzz uses so called high-level parser/lexer
classes that override all methods called when pars-
ing non-terminals. These classes extract the non-
terminals during parsing and can be automatically
generated from the classes provided by ANTLR.
All these classes are part of LangFuzz and get in-
tegrated into the internal language abstraction layer.
Integration of Tests. LangFuzz provides a test suite
class that must be derived and adjusted depending
Figure 11: Cumulative sum of the total number of de-
fects found depending on the length of the experiment.
Approximate 30 days for Mozilla TM (FF4 Beta) and
Google Chrome V8. We did no such analysis for PHP.
ment. This is why the Mozilla development team made
no security assessment for these issue reports.
For PHP, the number of detected defects is much lower
(see Figure 10). We considered the PHP experiment as
a proof-of-concept adaptation and invested considerably
less time into this experiment. Still, the total number of
18 defects was detected just within 14 days (1.3 defects
per day). The majority of these defects concerned mem-
ory safety violations in the PHP engine. We consider
these to be potential security vulnerabilities if an engine
is supposed to run on untrusted input. However, the PHP
development team did not classify reported issues as se-
curity relevant.
Figure 11 shows the cumulative sum of the total num-
ber of defects found depending on the length of the ex-
periment. The length of the different plotted lines corre-
sponds to the number of days each experiment was con-
ducted. The result for the Mozilla TraceMonkey (FF4
Beta) branch differs in length and shape. This stems from
the fact that during this experiment (which was our ﬁrst
experiment) we ﬁxed multiple issues within LangFuzz
which did not affect the two later applied experiments.
For both Mozilla projects, LangFuzz detected constantly
new defects. The curves of the two shorter experiments
show a very steep gradient right after starting LangFuzz.
The longer the experiment, the lower the number of de-
fects found per time period. Although this is no surpris-
ing ﬁnding, it manifests that LangFuzz quickly ﬁnd de-
fects produced by test cases that differ greatly from other
test cases used before.
The issue reports corresponding to the defects reported
during ﬁeld experiments can be found online using the
links shown in Table 2. Each link references a search
query within the corresponding issue tracker showing
12
020406080100number of days0102030405060total number of defects foundMozilla TM (FF4 Beta)MozillaTM (Type Inference)Chrome V8Experiment branch
Mozilla TM (FF4 Beta)
Mozilla TM (Type Inference)
Google V8
Link
http://tinyurl.com/lfgraph-search4
http://tinyurl.com/lfgraph-search2
http://tinyurl.com/lfgraph-search3
Table 2: Links to bug reports ﬁles during ﬁeld tests. Due to security locks it might be that certain issue reports require
extended permission rights and may not be listed or cannot be opened.
on the target test suite. In the case of PHP, the orig-
inal test suite is quite complex because each test is
made up of different sections (not a single source
code ﬁle). For our proof-of-concept experiment, we
only extracted the code portions from these tests,
ignoring setup/teardown procedures and other sur-
rounding instructions. The resulting code ﬁles are
compatible with the standard test runner, so our run-
ner class does not need any new implementation.
Adding Language-dependent Information (optional)
In this step, information about identiﬁers in the
grammar and global built-in objects can be pro-
vided (e.g. taken from a public speciﬁcation). In the
case of PHP, the grammar in use provides a single
non-terminal in the lexer for all identiﬁers used in
the source code which we can add to our language
class. Furthermore, the PHP online documentation
provides a list of all built-in functions which we
can add to LangFuzz through an external ﬁle.
Adapting LangFuzz to test different languages is easy:
provide language grammar and integrate tests. Adding
language dependent information is not required but
highly recommended.
7 Threats to Validity
Our ﬁeld experiments covered different JavaScript en-
gines and a proof-of-concept adaptation to a second weak
typed language (PHP). Nevertheless, we cannot general-
ize that LangFuzz will be able to detect defects in other
interpreters for different languages. It might also be the
case that there exist speciﬁc requirements or properties
that must be met in order to make LangFuzz be effective.
Our direct comparison with jsfunfuzz is limited to a
single implementation and limited to certain versions of
this implementation. We cannot generalize the results
from these experiments. Running LangFuzz and jsfun-
fuzz on different targets or testing windows might change
comparison results.
The size and quality of test suites used by LangFuzz
during learning and mutating have a major impact on it’s
performance. Setups with less test cases or biased test
suites might decrease LangFuzz’s performance.
Both jsfunfuzz and LangFuzz make extensive use of
randomness. While some defects show up very quickly
and frequently in all runs, others are harder to detect.
Their discovery heavily depend on the time spent and
the randomness involved. In our experiments, we tried to
ﬁnd a time limit that is large enough to minimize such ef-
fects but remains practical. Choosing different time lim-
its might impact the experimental results.
For most experiments, we report the number of de-
fects found. Some of the reported bugs might be dupli-
cates. Duplicates should be eliminated to prevent bias.
Although we invested a lot of efforts to identify such du-
plicates, we cannot ensure that we detected all of these
duplicates. This might impact the number of distinct de-
fects discovered through the experiments.
8 Conclusion
Fuzz testing is easy to apply, but needs language-
and project-speciﬁc knowledge to be most effective.
LangFuzz is an approach to fuzz testing that can easily be
adapted to new languages (by feeding it with an appropri-
ate grammar) and to new projects (by feeding it with an
appropriate set of test cases to mutate and extend). In our
evaluation, this made LangFuzz an effective tool in ﬁnd-
ing security violations, complementing project-speciﬁc
tools which had been tuned towards their test subject for
several years. The economic value of the bugs uncovered
by LangFuzz is best illustrated by the worth of its bugs,
as illustrated by the awards and bug bounties it raised.
We recommend our approach for simple and effective au-
tomated testing of processors of complex input, includ-
ing compilers and interpreters—especially those dealing
with user-deﬁned input.
Acknowledgments. We thank the Mozilla, Google and
PHP development teams for their support. Guillaume
Destuynder, Florian Gross, Clemens Hammacher, Chris-
tian Hammer, Matteo Maffei, and Eva May provided
helpful feedback on earlier revisions of this paper.
References
[1] https://bugzilla.mozilla.org/show_bug.cgi?id=
610223.
13
[2] https://bugzilla.mozilla.org/show_bug.cgi?id=
626345.
[3] https://bugzilla.mozilla.org/show_bug.cgi?id=
626436.
[4] https://code.google.com/p/v8/issues/detail?id=
1167.
[23] ZALEWSKI, M.
Blog En-
try. http://lcamtuf.blogspot.com/2011/01/announcing-crossfuzz-
potential-0-day-in.html, 2011.
Announcing cross fuzz.
[24] ZELLER, A., AND HILDEBRANDT, R. Simplifying and isolating
failure-inducing input. IEEE Transactions on Software Engineer-
ing (2002), 183–200.
[5] The
phpparser
project.
http://code.google.com/p/phpparser/.
Project
website.
Appendix
Parameter
synth.prob – Probability to generate
a required fragment
instead of using a
known one.
synth.maxsteps – The maximal number
of steps to make during the stepwise ex-
pansion. The actual amount is 3 + a ran-
domly chosen number between 1 and this
value.
fragment.max.replace – The maximal
number of fragments that are replaced dur-
ing test mutation. The actual amount is a
randomly chosen number between 1 and
this value.
identifier.whitelist.active.prob
– The probability to actively introduce a
built-in identiﬁer during fragment rewrit-
ing (i.e. a normal identiﬁer in the fragment
is replaced by a built-in identiﬁer).
Default
Value
0.5
5
2
0.1
Table 3: Common parameters in LangFuzz and their de-
fault values. See Section 4.4 on how these default values
were chosen.
[6] AITEL, D. The advantages of block-based protocol analysis for
security testing. Tech. rep., 2002.
[7] GODEFROID, P., KIEZUN, A., AND LEVIN, M. Y. Grammar-
based whitebox fuzzing. SIGPLAN Not. 43, 6 (2008), 206–215.
[8] LINDIG, C. Random testing of c calling conventions. Proc.
AADEBUG. (2005), 3–12.
[9] MCPEAK, S., AND WILKERSON, D. S. The delta tool. Project
website. http://delta.tigris.org/.
[10] MILLER, B. P., FREDRIKSEN, L., AND SO, B. An empirical
study of the reliability of unix utilities. Commun. ACM 33 (De-
cember 1990), 32–44.
[11] MILLER, C., AND PETERSON, Z. N. J. Analysis of Mutation
and Generation-Based Fuzzing. Tech. rep., Independent Security
Evaluators, Mar. 2007.
[12] MOLNAR, D., LI, X. C., AND WAGNER, D. A. Dynamic test
generation to ﬁnd integer bugs in x86 binary linux programs. In
Proceedings of the 18th conference on USENIX security sympo-
sium (Berkeley, CA, USA, 2009), SSYM’09, USENIX Associa-
tion, pp. 67–82.
[13] NEUHAUS, S., ZIMMERMANN, T., HOLLER, C., AND ZELLER,
A. Predicting vulnerable software components. In Proceedings
of the 14th ACM Conference on Computer and Communications
Security (October 2007).
[14] OEHLERT, P. Violating assumptions with fuzzing. IEEE Security
and Privacy 3 (March 2005), 58–62.
[15] PARR, T., AND QUONG, R. Antlr: A predicated-ll (k) parser
generator. Software: Practice and Experience 25, 7 (1995), 789–
810.
[16] PURDOM, P. A sentence generator for testing parsers. BIT Nu-
merical Mathematics 12 (1972), 366–375. 10.1007/BF01932308.
Blog Entry.
Introducing jsfunfuzz.
[17] RUDERMAN,
J.
http://www.squarefree.com/2007/08/02/introducing-jsfunfuzz/,
2007.
[18] SHU, G., HSU, Y., AND LEE, D. Detecting communication pro-
tocol security ﬂaws by formal fuzz testing and machine learning.
In Proceedings of the 28th IFIP WG 6.1 international confer-
ence on Formal Techniques for Networked and Distributed Sys-
tems (Berlin, Heidelberg, 2008), FORTE ’08, Springer-Verlag,
pp. 299–304.
[19] SUTTON, M., AND GREENE, A. The art of ﬁle format fuzzing.
In Blackhat USA Conference (2005).
[20] SUTTON, M., GREENE, A., AND AMINI, P. Fuzzing: Brute
Force Vulnerability Discovery. Addison-Wesley Professional,
2007.
[21] TURNER, B. Random c program generator. Project website.
http://sites.google.com/site/brturn2/randomcprogramgenerator,
2007.
[22] YANG, X., CHEN, Y., EIDE, E., AND REGEHR, J. Finding and
Understanding Bugs in C Compilers. In Proceedings of the 2011
ACM SIGPLAN Conference on Programming Language Design
and Implementation (June 2011), ACM SIGPLAN, ACM.
14