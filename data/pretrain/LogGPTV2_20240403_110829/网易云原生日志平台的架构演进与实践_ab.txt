|---|---|---|---|---|
| 资源占⽤较 |基于JRuby， |较消耗资源， |性能⼀般 |占⽤⼩，性能 |
| 资源占⽤较 |⽐较消耗资 |较消耗资源， |性能⼀般 |占⽤⼩，性能 |
| ⼩，性能优 |⽐较消耗资 |性能⼀般 |性能⼀般 |好，开发效率 |
| ⼩，性能优 |源，性能较差 |性能⼀般 |性能⼀般 |好，开发效率 || ⼩，性能优 |源，性能较差 |性能⼀般 |性能⼀般 |好，开发效率 |
| 异，语⾔栈匹 |源，性能较差 |性能⼀般 |性能⼀般 |低 |
配
● 技术语⾔栈：部分性能差，部分开发效率低
● 容器化场景⽀持：部分不⽀持，部分只⽀持容器标准输出● 完整⽇志解决⽅案：均未提供
3. 新的征程 
开源Loggie的现在和未来
⾃研新版Agent
基于Golang的轻量级、⾼性能、云原⽣⽇志采集、聚合Agent，⽀持多Pipeline和组件热插拔
核⼼概念
•	Source：输⼊源，⼀个Pipeline可以有多个不同的输⼊源
•	Sink：输出源，⼀个Pipeline仅能配置⼀种类型的输⼊源，但是可以有多个并⾏实例
•	Interceptor：拦截器，数据流经过多个Interceptor被链式处理
•	Queue：队列，⽬前有内存队列•	Queue：队列，⽬前有内存队列
•	Pipeline：管道，sources/interceptors/queue/sink共同组成了⼀个Pipeline，不同的Pipeline数据隔离
•	Discovery：配置下发，⽬前⽀持Kubernetes
•	Monitor EventBus：各组件监控数据的暴露或发送
•	Reloader：配置的动态更新
特性
⼀栈式⽇志解决⽅案
可插拔组件，可扩展性⼤⼤增强，可同
时⽀持⽇志中转、过滤、解析、切分、
⽇志报警等
吸收了我们⻓期的⼤规模运维经验，形成
了全⽅位的可观测性、快速排障、异常预
警、⾃动化运维能⼒
快速便捷的容器⽇志采集⽅式，原⽣
的Kubernetes动态配置下发
基于Golang，较⼩的资源占⽤，优异
的吞吐性能，较⾼的开发效率
	➤ 中转、聚合、解析、处理 01的吞吐性能，较⾼的开发效率
	➤ 中转、聚合、解析、处理 01
➤ ⽇志报警
⼀栈式⽇志解决⽅案
➤ 快速研发
不仅仅是⽇志采集 
优异的可扩展性
	➤ 更多扩展
	中转、聚合、解析、处理
➤ 基于各类Interceptor
流处理/SQL/Function
解析、切分
分流、聚合、转存 • 统⼀技术栈，⽆需引⼊Flume或者
	Logstash等第三⽅中转，便于维护
•	CRD⾥写解析规则等，实时⽣效，
	⾃动reload，易于使⽤
⽇志报警
➤ 之前 ➤ 现在 使⽤logAlert Interceptor即可
使⽤ElastAlert轮询Elasticsearch
（存在配置下发、告警接⼊、⾃
| 身⾼可⽤等问题） | ● | 基于采集链路 |
|---|---|---|
|  | | |
在Flink⾥解析关键字或者正则|  | | |
在Flink⾥解析关键字或者正则
| 匹配（存在重量级、依赖配套的 | ● | 单独部署，使⽤Elasticsearch Source |
|---|---|---|
| 运维、监控报警、排障等问题） |● |单独部署，使⽤Elasticsearch Source |
	快速研发
➤ 微内核，插件化，组件化
•	所有插件均抽象为component，通过实现⽣命
周期接⼝，可快速开发⼀个sink/source/
interceptor/discovery
Everything is component
	更多扩展
➤ 采集K8s Event
	● 组件复⽤（缓存、性能、重拾、	配置等等）
➤ 定时冷备	● 只需研发特定场景的功能组件	● 效率提升/可扩展性强
02 ➤ 基于CRD的使⽤⽅式
➤ Kubernetes下多架构⽀持02 ➤ 基于CRD的使⽤⽅式
➤ Kubernetes下多架构⽀持
云原⽣的⽇志形态
基于CRD的使⽤⽅式
只需create/update CRD实例即可：
•	采集指定Pods/Node节点/集群⽇志
•	⽇志流如何中转、聚合
•	转发到哪些下游
•	解析处理⽇志
•	限流/⽇志报警检测
 …
部署⼀键化
配置⾃动化
动态reload
Kubernetes下多架构⽀持
•	多形态：
•	Agent
•	Aggregator
•	多架构：
•	直接发送
•	中转、处理
•	多输出端
03 ➤ 完善的监控指标
	➤ 解决⼤规模场景下的痛点⽣产级特性
完善的监控指标
根据⻓期运维、排障需求归纳提炼
| ● | 完善的指标 | 完善的指标 |
|---|---|---|
| ● |● |采集进度 |
| ● |● |⻓期采集未完成 |
| ● |● |采集发送延迟 || ● |● |⻓期采集未完成 |
| ● |● |采集发送延迟 |
| ● |● |Fd情况 |
| ● |● |输出Qps |
| ● |● |服务级别指标 |
 …
●	多种暴露⽅式
●	原⽣Prometheus格式
●	Rest API
●	可发送Kafka
●	⽇志输出
解决⼤规模场景下的痛点
吸收⼤规模⽣产实践总结的经验教训，完善功能
| ➤ 稳定性 | ➤ 稳定性 | ➤ 运维排障 | ➤ 运维排障 |
|---|---|---|---|
| ● |独⽴Pipeline增强服务隔离性 |● |检测⻓时间未采集完成的⽇志⽂件 |
| ● |采集Qps限制 |● |各类参数均⽀持全局默认配置 |
| ● |可配置定时清理⽇志 |● |⽆采集情况的⽇志查询、下载 |
| ● |检测突发增⻓的⽇志⽂件 |● |提供常⻅排障运维接⼝ |
| ● |合理的Fd保留机制 | … | … || ● |合理的Fd保留机制 | … | … |
 …
04	➤ 基准测试
➤ 性能对⽐
资源与性能
基准测试
➤ Filebeat 865% ➤ Loggie 217%
73.3MB/s 	120MB/s
●	同等情况下，发送⾄Kafka（单⾏、单⽂件、相同发送并发度、⽆解析场景）
性能对⽐
●	Loggie和Filebeat消耗的CPU相⽐，⼤概仅为后者 
	的1/4，同时发送吞吐量为后者的1.6～2.6倍
●	Filebeat的极限吞吐量存在瓶颈，80MB/s后很难 
	提升，⽽Loggie的极限值更⾼，多⽂件采集下甚
	⾄达到了200MB/s+
开源计划
12⽉份正式开源
敬请期待
RoadMap&Doing
| ● | 更多组件与功能扩展 | 更多组件与功能扩展 | 参与贡献 |
|---|---|---|---|
| ● |● |Source: http |参与贡献 || ● |● |Source: http |参与贡献 |
| ● |● |Sink: file, clickhouse, pulsar, influxdb等 |参与贡献 |
| ● |● |持久化Queue |参与贡献 |
| ● |● |轻量级的流处理能⼒ |参与贡献 |
| ● |● |WASM形态⽀持⾃定义⽇志解析处理 |共同建设 |
| ● |● |⽀持接⼊Knative/KEDA等Serverless形态扩缩容指标 |共同建设 |
| ● |服务发现与配置中⼼ |服务发现与配置中⼼ |共同建设 |
| ● |服务发现与配置中⼼ |服务发现与配置中⼼ |共同建设 |
| ● |● |主机模式的配置下发：⽀持Consul, Apollo等 |共同建设 |
| ● |云原⽣与Kubernetes |云原⽣与Kubernetes |共同建设 |
| ● |● |⾃动注⼊Loggie sidecar形态⽀持 |共同建设 || ● |● |⾃动注⼊Loggie sidecar形态⽀持 |共同建设 |
| ● |● |Opentelemetry兼容与⽀持 |共同建设 |