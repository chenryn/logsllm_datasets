utilization for the SwitchBlade platform; (2) forwarding perfor-
mance and isolation for parallel data planes; and (3) data-plane
update rates.
6.1 Resource Utilization
To provide insight about the resource usage when different data
planes are implemented on SwitchBlade, we used Xilinx ISE [23]
9.2 to synthesize SwitchBlade. We found that a single physical
IPv4 router implementation developed by the NetFPGA group at
Stanford University uses a total of 23K four-input LUTs, which
consume about 49% of the total available four-input LUTs, on
the NetFPGA. The implementation also requires 123 BRAM units,
which is 53% of the total available BRAM.
We refer to our existing implementation with one OpenFlow, one
IPv6, one variable bit extractor, and one Path Splicing preproces-
sor with an IPv4 router and capable of supporting four VDPs as the
SwitchBlade(cid:147)base con(cid:2)guration(cid:148). This implementation uses 37K
four-input LUTs, which account for approximately 79% of four-
input LUTs. Approximately 4:5% of LUTs are used for shift reg-
isters. Table 4 shows the resource utilization for the base Switch-
Blade implementation; SwitchBlade uses more resources than the
base IPv4 router, as shown in table 5, but the increase in resource
utilization is less than linear in the number of VDPs that Switch-
Blade can support.
Sharing modules enables resource savings for different protocol
implementations. Table 5 shows the resource usage for implemen-
tations of an IPv4 router, an OpenFlow switch, and path splicing.
These implementations achieve 4 Gbps; OpenFlow and Path Splic-
ing implementations provide more resources than SwitchBlade.
But there is not much difference in resource usage for these imple-
mentations when compared with the possible con(cid:2)gurations which
SwitchBlade can support.
Virtual Data Planes can support multiple forwarding planes in
parallel. Placing four Path Splicing implementations in parallel
on a larger FPGA to run four Path Splicing data planes will re-
quire four times the resources of existing Path Splicing implemen-
tation. Because no modules are shared between the four forwarding
planes, the number of resources will not increase linearly and will
remain constant in the best case.
From a gate count perspective, Path Splicing with larger forward-
ing tables and more memory will require approximately four times
the resources as in Table 5; SwitchBlade with smaller forwarding
tables and less memory will require almost same amount of re-
sources. This resource usage gap begins to increase as we increase
the number of Virtual Data Planes on the FPGA. Recent trends
in FPGA development such as Virtex 6 suggest higher speeds and
larger area; these trends will allow more VDPs to be placed on a
single FPGA, which will facilitate more resource sharing.
)
s
p
p
0
0
0
’
(
i
t
e
a
R
g
n
d
r
a
w
r
o
F
Table 5: Resource usage for different data planes.
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0
Packet Forwarding Rate, Comparison
NetFPGA, Base Router
Linux Raw Packet Forwarding
 100
 1000
Packet Size (bytes)
Figure 8: Comparison of forwarding rates.
6.2 Forwarding Performance and Isolation
We used the NetFPGA-based packet generator [10] for traf(cid:2)c
generation to generate high speed traf(cid:2)c to evaluate the forwarding
performance of SwitchBlade and the isolation provided between
the VDPs. Some of the results we present in this section are derived
from experiments in previous work [4].
Raw forwarding rates. Previous work has measured the max-
imum sustainable packet forwarding rate for different con(cid:2)gura-
tions of software-based virtual routers [5]. We also measure packet
forwarding rates and show that hardware-accelerated forwarding
can increase packet forwarding rates. We compare forwarding rates
of Linux and NetFPGA-based router implementation from NetF-
PGA group [2], as shown in Figure 8. The maximum forwarding
rate shown, about 1:4 million packets per second, is the maximum
traf(cid:2)c rate which we were able to generate through the NetFPGA-
based packet generator.
The Linux kernel drops packets at high loads, but our con(cid:2)gura-
tion could not send packets at a high enough rate to see packet drops
in hardware.
If we impose the condition that no packets should
be dropped at the router, then the packet forwarding rates for the
Linux router drops signi(cid:2)cantly, but the forwarding rates for the
hardware-based router remain constant. Figure 8 shows packet for-
warding rates when this (cid:147)no packet drop(cid:148) condition is not imposed
(i.e., we measure the maximum sustainable forwarding rates). For
large packet sizes, SwitchBlade could achieve the same forward-
ing rate using in-kernel forwarding as we were using a single port
of NetFPGA router. Once the packet size drops below 200 bytes;
the software-based router cannot keep pace with the forwarding re-
quirements.
Forwarding performance for Virtual Data Planes. Figure 9
shows the data-plane forwarding performance of SwitchBlade
running four data planes in parallel versus the NetFPGA refer-
ence router [2], for various packet sizes. We have disabled the
rate limiters in SwitchBlade for these experiments. The (cid:2)gure
shows that running SwitchBlade incurs no additional performance
penalty when compared to the performance of running the refer-
191NetFPGA Hardware Router and 4 SwitchBlade Virtualized Data Planes
)
s
p
p
0
0
0
’
(
e
t
a
R
g
n
d
r
a
w
r
o
F
i
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0
6
4
1
0
4
SwitchBlade
NetFPGA Hardware Router
SwitchBlade with Traffic Filtering
2
0
4
5
0
4
7
0
4
1
0
0
4
1
5
1
8
Packet Size
Figure 9: Data plane performance: NetFPGA reference router
vs. SwitchBlade.
Packet Size(bytes)
64
104
204
504
704
1004
1518
Physical Router (’000s of packets)
Pkts Fwd @ Core
40 K
40 K
40 K
40 K
40 K
39.8 K
4 K
Pkts Sent
40 K
40 K
40 K
40 K
40 K
39.8 K
4 K
Pkts recv @Sink
20 K
20 K
20 K
20 K
20 K
19.9 K
1.9 K
Table 6: Physical Router, Packet Drop Behavior.
ence router [2]. By default, traf(cid:2)c belonging to any VDP can arrive
on any of the physical Ethernet interfaces since all of the ports are
in promiscuous mode. To measure SwitchBlade’s to (cid:2)lter traf(cid:2)c
that is not destined for any VDP, we (cid:3)ooded SwitchBlade with a
mix of traf(cid:2)c where half of the packets had destination MAC ad-
dresses of SwitchBlade virtual interfaces and half of the packets
had destination MAC addresses that didn’t belong to any vdp. As
a result, half of the packets were dropped and rest were forwarded,
which resulted in a forwarding rate that was half of the incoming
traf(cid:2)c rate.
Isolation for Virtual Data Planes. To measure CPU isolation, we
used four parallel data planes to forward traf(cid:2)c when a user-space
process used 100% of the CPU. We then sent traf(cid:2)c where each
user had an assigned traf(cid:2)c quota in packets per second. When
no user surpassed the assigned quotas, the router forwarded traf(cid:2)c
according to the assigned rates, with no packet loss. To measure
traf(cid:2)c isolation, we set up a topology where two 1 Gbps ports of
routers were (cid:3)ooded at 1 Gbps and a sink node were connected to
a third 1 Gbps port. We used four VDPs to forward traf(cid:2)c to the
same output port. Tables 6 and 7 show that, at this packet forward-
ing rate, only half of the packets make it through, on (cid:2)rst come
(cid:2)rst serve basis, as shown in fourth column. These tables show that
both the reference router implementation and SwitchBlade have the
same performance in the worst-case scenario when an output port is
(cid:3)ooded. The second and third columns show the number of pack-
ets sent to the router and the number of packets forwarded by the
router. Our design does not prevent against contention that may
arise when many users send traf(cid:2)c to one output port.
Forwarding performance for non-IP packets. We also tested
whether SwitchBlade incurred any forwarding penalty for forward-
ing custom, non-IP packets; SwitchBlade was also able to forward
these packets at the same rate as regular IP packets. Figure 10
Four Data Planes (’000s of packets)
Packet Size(bytes)
64
104
204
504
704
1004
1518
Pkts Sent
40 K
40 K
40 K
40 K
40 K
39.8 K
9.6 K
Pkts Fwd @ Core
40 K
40 K
40 K
40 K
40 K
39.8 K
9.6 K
Pkts recv @Sink
20 K
20 K
20 K
20 K
20 K
19.9 K
4.8 K
Table 7: Four Parallel Data Planes, Packet Drop Behavior.
R1
IP-ID (Path bits)
01 0011
... 010101
...
R0
src
01
00
R2
dst
Figure 10: Test topology for testing SwitchBlade implementa-
tion of Path Splicing.
shows the testbed we used to test the Path Splicing implementation.
We again used the NetFPGA-based hardware packet generator [10]
to send and receive traf(cid:2)c. Figure 11 shows the packet forward-
ing rates of this NetFPGA-based implementation, as observed at
the sink node. No packet loss occurred on any of the nodes shown
in Figure 10. We sent two (cid:3)ows with same destination IP address
but using different splicing bits to direct them to different routers.
Packets from one (cid:3)ow were sent to R2 via R1, while others went
directly to R2. In another iteration, we introduced four different
(cid:3)ows in the network, such that all four forwarding tables at router
R0 and R2 were looked up with equal probability; in this case,
SwitchBlade also forwarded the packets at full rate. Both these ex-
periments show that SwitchBlade can implement schemes like Path
Splicing and forward traf(cid:2)c at hardware speeds for non-IP packets.
In another experiment, we used the Variable Bit Extraction mod-
ule to extract (cid:2)rst 64 bits from the header for hashing. We used a
simple source and sink topology with SwitchBlade between them
and measured the number of packets forwarded. Figure 12 shows
the comparison of forwarding rates when forwarding was being
done using SwitchBlade based on the (cid:2)rst 64-bits of an Ethernet
frame and when it was done using NetFPGA base router.
6.3 Data-Plane Update Rates
Each VDP in a router on SwitchBlade needs to have its own
forwarding table. Because the VDPs share a single physical de-
vice, simultaneous table updates from different VDPs might cre-
ate a bottleneck. To evaluate the performance of SwitchBlade for
forwarding table update speeds, we assumed the worst-case sce-
nario, where all VDPs (cid:3)ush their tables and rewrite them again at
the same time. We assumed that the table size for each VDP is
400,000 entries. We updated all four tables simultaneously, but
there was no performance decrease while updating the forwarding
table from software. Four processes were writing the table entries