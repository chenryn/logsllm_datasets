PPS_LM
PPS_LC
 0
 200  400  600  800  1000
Thousands of objects
)
s
(
y
a
e
D
l
)
d
n
o
c
e
s
/
K
(
s
t
c
e
b
O
j
l
)
s
(
y
a
e
D
y
r
e
u
Q
 6
 5
 4
 3
 2
 1
 0
Query Delay Variation with p
CPU Load Variation with p
PPS_LM 2 q/s
PPS_LM 6 q/s
PPS_LM 10 q/s
PPS_LC 2 q/s
PPS_LC 6 q/s
PPS_LC 10 q/s
Target Delay
PPS_LM 2 q/s
PPS_LM 6 q/s
PPS_LM 10 q/s
PPS_LC 2 q/s
PPS_LC 6 q/s
PPS_LC 10 q/s
 120
 100
 80
 60
 40
 20
)
%
(
d
a
o
L
 5  10  15  20  25  30  35  40  45  50
p
 0
 0
 10
 20
 30
 40
 50
p
Figure 7: Single server performance
Figure 8: Effect of p on query delay
Figure 9: Effect of p on CPU Load
the advantages of minimizing memory usage and preventing the
garbage collector running during a query, which would increase
query delay, but the disadvantage of adding to the ﬁxed costs of a
query. PPS_LC (low CPU) does not force a garbage collection run
after a query; it has lower ﬁxed costs, but uses more memory and
may exhibit more variable query delays.
We do not claim that PPS is an “optimal” application in any way,
but merely note that real-world search applications also vary con-
siderably in their ratio of ﬁxed to variable costs. For example,
Google’s web search runs from memory, and has relatively low
ﬁxed costs because all users search the same web index. In con-
trast, with Google’s Gmail, queries from different users obviously
have to search different indexes. It doesn’t make sense to store all
such indexes in memory for all users. Loading a ﬁle from disk has a
large seek/rotate latency followed by a fast consecutive read phase,
so has a comparatively high ﬁxed cost.
As a test application PPS shares the main properties with web
search. The mechanisms are different, but the average cost of match-
ing in both cases has a large component that grows linearly with
the number of documents searched, although PPS search costs are
less dependent on the contents of the query. Both applications are
bottlenecked on CPU cycles or memory bandwidth. The different
versions of PPS have quite different ﬁxed costs, as we would also
expect when comparing regular web search with webmail search.
5.1.1 Characterizing PPS
Before applying ROAR to PPS, we ﬁrst examine how PPS per-
forms on a single machine. The main issue is query delay as shown
in Fig. 7. As expected, once the ﬁxed costs are satisﬁed, query de-
lay increases with the number of metadata objects to be searched.
When the number of objects is smaller, the ﬁxed costs associated
with running a query cease to be negligible, which shows up as a
performance drop off in the bottom graph in Fig. 7. The drop is
steeper for the low memory version.
In absolute terms, when searching one million metadata items a
single server takes 4.2 seconds to perform a query, which is unac-
ceptable, especially if many users are using PPS simultaneously.
Ideally we would like a PPS system that is able to respond to mul-
tiple, simultaneous requests in at most one second each. We will
now use ROAR to distribute PPS across multiple networked servers
in order to achieve this aim while increasing request throughput.
5.2 Basic Tradeoff
To examine how p impacts query delay and throughput we cre-
ated a dataset of one million ﬁles. From these we created an en-
crypted metadata index consisting of 30 keywords per ﬁle, plus
some other metadata. We distributed this index to our 47-server
ROAR deployment, and searched it with queries consisting of two
randomly chosen keywords that must both match for the ﬁle to
match. While this is a slightly artiﬁcial workload, the precise con-
tents being searched are not terribly relevant as distributed ren-
dezvous is content-agnostic.
To allow a single server to search its part of the index in one sec-
ond, we started with a value of p = 5, the smallest value that has
any hope of meeting our target search latency. From here, we pro-
gressively increased p all the way up to the largest possible value
of 47, at which point every server is processing 1/47 of every re-
quest. For each value of p, we tried workloads from two queries per
second up to ten queries per second; these corresponded to light,
moderate, and heavy workloads.
5.2.1 Query Latencies Decrease with p
The query latencies are shown in Figure 8. At low and mod-
erate load, query latency scales inversely proportional to p, as we
would hope, and is similar for both versions of PPS. It is clear that
to achieve a target latency we need to have p greater than a partic-
ular threshold. However, this threshold is not ﬁxed, but depends
on the offered load. This should not be a surprise: a query can-
not complete until all its sub-queries complete. There is inevitably
some short-term variation in the loads on the different machines, so
some sub-queries are delayed.
The heavy workload is sustainable at any p by the LC version,
and shows a similar slope to the other workloads. However, av-
erage delay for LM decreases initially, then increases as p = 20.
This is because nodes are close to saturation at this point, and any
small variation in query arrivals induces longer delays. If we in-
crease p further, LM saturates some nodes and cannot cope with
the load. This example serves to show that ﬁxed overheads de-
crease the maximum throughput when p increases.
5.2.2 Query Overheads Increase with p
Figure 9 plots mean CPU load (as measured by the “top” util-
ity) for varying values of p and for each of the workloads. The
error-bars show the standard deviation. The trend is clear: CPU uti-
lization increases with p. For the low memory version, the curves
show relative increases of 80% (from 22% to 40%), 54%(from 53%
to 85%), for the workloads of two and six queries per second re-
spectively. For the LC version, the relative increases are of ap-
proximately 10% in both cases. The differences between the two
versions show the overhead of more frequent garbage collection.
297)
%
(
d
a
o
L
 100
 80
 60
 40
 20
 0
p=10
p=47
 0  10  20  30  40  50  60  70  80
 0  10  20  30  40  50  60  70  80
Time (s)
Time (s)
Figure 10: Average system load for each node
Model
PPS_LM
PPS_LC
PE 2950
PE 1950
PE 1850
Sun X4100
51W
18.9W
50W
17W
10W
3W
7W
2W
Table 1: Energy Savings running at p = 5 instead of p = 47
At the highest load, the increase is more modest for LM, because
the nodes are saturated. For LC, the relative increase is 22% (from
67% to 82%).
To see this in more detail, Figure 10 shows a 20-second average
of CPU load for all our PPS_LM servers when p = 10 and p = 47
with 6 queries per second. When p = 10, individual load ﬂuctuates
much more as queries come and go. When p = 47 there are few
idle times and load is heavily and constant.
Our cluster can handle two of these workloads with any value
of p, but using large p values uses enough extra CPU power to
waste considerable energy (Table 1). Comparing p = 5 with p =
47, our newer servers3 were measured to consume 18W more with
PPS_LC and 50W more with PPS_LM. Our older servers4 have
less good CPU power management, so less savings. We expect that
the latest Intel Nehalem CPUs will show even greater savings than
those shown.
Each query requires a disk seek then reading 250MB of contigu-
ous data. On our systems the kernel disk buffer cache reduces I/O
and PPS is largely CPU bound, but in machines with less mem-
ory disk performance might matter. The ratio of seeks to reads
increases with p, wasting I/O bandwidth. The Maxtor 10K V disks
in our servers take 7.5ms on average to seek and transfer data at
73MB/s. When p = 5 it takes each server 680ms to sequentially
read its part of the data; when p = 47 it takes 80ms. At this point
seeks accounts for 10% of the transfer times, so if the system were
disk bound, using a higher p would reduce maximum throughput
by 10%.
Finally, the bandwidth required to run a single query increases
proportionally5 with p. This does not create a sizeable impact on
energy consumption, but will increase usage of the scarce cross-
section bandwidth. We will go in more detail on cross-section
bandwidth usage in Section 5.6.
In summary, increasing p above the minimum needed to satisfy
the required delay bounds increases system load. Depending on the
workload, very large values of p may reduce the peak throughput
that can be handled, or at the very least waste resources and energy.
5.2.3 Update Overhead increases with r
To see how server throughput (matches/second) is affected by
background updates of the dataset we created medium (5K up-
dates/sec) and high (20K updates/sec) update rates. Figure 11 shows
3Dell PowerEdge 2950, with two quad-core Xeon CPUss and PowerEdge 1950, with
two dual core Xeon CPUs
4Sun X4100 with one AMD Opteron CPU and Dell 1850 with one older Xeon CPU
5In our PPS deployment the increase is modest: from 2.5KB to 24KB per query
 350000
 300000
 250000
 200000
 150000
 100000
 50000
)
s
/
s
e
l
i
F
(
t
u
p
h
g
u
o
r
h
T
 0
 0
Zero Updates
Low Update Rate
High Update Rate
 2
 4
 6
 8
 10
 12
Time (s)
Figure 11: Effect of updates on server throughput
a single server’s throughput in these conditions in comparison with
no update load. Unsurprisingly, the higher the load the bigger the
reduction in throughput. For the moderate load, the average drop in
throughput is 20%; for the higher load, the drop is even sharper. In
applications like PPS, where the data are stored to disk, this effect
needs to be considered when determining and changing r.
5.2.4 Does the trade-off matter?
We have seen that larger values of p give lower delays but higher
system load, so there is a natural push of p to the minimum value
that achieves the desired query latency. We’ve also seen that higher
update rates, which can result from larger values of r, reduce server
processing speed; thus there is a push to minimize r. Taking these
two together, it follows that a distributed rendezvous system should
be run close to the minimum combination of p and r, that is p· r =
n, where n is the server count.
To summarize, minimizing p subject to latency constraints seems
a sensible goal. However, small p implies large r, which, in turn,
increases the bandwidth used to replicate the changing dataset and
the update processing load of the servers. Thus the ability to dy-
namically change the tradeoff between r and p is very useful to
ensure that the system runs at a good near-optimal operating point.
5.3 Changing p Dynamically
One of the beneﬁts of ROAR is its ability to repartition on-the-ﬂy
while still serving queries. To investigate how this works in practice
we implemented a simple adaptive strategy to change p based on
the average query latency seen by the front-end servers. Given an
average target delay of one second, the front-end servers instructed
the ROAR servers to adapt p to the minimum value that still yielded
the target latency (allowing for an error of 10%). Increasing p had
no cost, of course, but to decrease it servers needed to copy data;
this increased their load, so is more interesting.
We ran an experiment with this adaptive strategy starting with no
replication and p = 40, as if the system had just booted. We loaded
the system with a moderate search rate of six queries per second,
and plotted the behavior of the system as time goes by in Figure 12.
To start with, CPU load is very high and the query delay is less
than it needs to be. We see that ROAR can quickly change p with
minimal disruption to queries: within minutes average CPU load
decreases while query delay stays within acceptable bounds.
This same experiment can serve as an example of adaptation for
ﬂash crowds: when load becomes too high (above some predeﬁned
threshold) the system sacriﬁces query latency for lower CPU load.
The strategy of minimizing p while maintaining the desired query
delay seems sensible, yet in reality many other factors need to be
taken into account. The cost of pushing dataset changes out to
nodes gets higher as p decreases, so using larger values of p might
be desirable. In addition, p might need to be increased to reduce
the memory strain on each server (this seems to be a constraint in
298Values of P
 50
 100
 150
 200
 250
Time(s)