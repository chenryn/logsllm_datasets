analysis [59]; we include examples of both in App. D.
The tabular tool allows users to record their responses
to each subtask of the CoG framework; each section
supports data in follow-on sections. The graph-based
method provides users with an alternative, complemen-
tary method for eliciting the same data. Previous re-
search indicates that various learning styles beneﬁt from
multiple forms of data elicitation [31].
During the ﬁrst classroom example,
the instructor
guided participants through a scenario drawn from the
Star Wars movie franchise to determine the CoG for
the Galactic Empire. The instructor provided step-by-
624    27th USENIX Security Symposium
USENIX Association
In the second example,
step instructions for using the tabular and graphical tools
throughout.
the participants
worked together without instructor guidance to apply
CoG and framework tools to a ﬁctional e-commerce sce-
nario. We describe both ﬁctitious scenarios in App. A.
Prior to providing the intervention, the instructor ob-
served NYC3 employees at work for four days to bet-
ter understand their operating environment. The instruc-
tor developed the ﬁctitious scenarios so that they did not
reﬂect any speciﬁc conditions within NYC3. We chose
these scenarios in lieu of NYC3-speciﬁc scenarios to re-
duce bias during training that would inadvertently coach
participants towards providing “approved solutions.”
To control for variations in instruction, each group had
the same instructor. The instructor is a member of the
research team with extensive subject-matter knowledge
and experience, including six months of formal univer-
sity training on threat modeling. The instructor commu-
nicated this experience prior to each class to establish a
baseline of credibility with the group. During each class,
participants could ask questions at any time, and the in-
structor maintained a running log of these questions. To
maintain consistency across class sessions, the instruc-
tor incorporated answers to these questions at relevant
points in future sessions, and emailed the answers to par-
ticipants who had attended previous sessions.
Performance evaluation session. After all participants
ﬁnished the educational intervention training, they each
completed a 60-minute individual session where they ap-
plied CoG to their daily duties. For example, P17 used
the framework in his role as a security analyst to develop
plans for better defending NYC endpoint workstations
(See App. A.3). This phase of the study provided hands-
on reinforcement learning, as recommended by ELT and
SLT [6, 31].
We audio recorded each session, provided participants
with clean worksheets and whiteboards for brainstorm-
ing (App. D), and allowed participants to bring in any
notes from the previous educational intervention train-
ing. Without notifying the participants, we logged task
completion times for each step, in an effort to measure
the efﬁciency of the framework without putting undue
pressure on participants.
The interviewer used the constructive interaction
method for communicating with the participants, ask-
ing them to openly communicate throughout each sub-
task in Section 2.2 [40]. During each step, the instructor
re-stated participants’ previous verbal comments or doc-
umented responses to assist with data elicitation but did
not introduce any new concepts to prevent data bias. For
consistency, the same interviewer completed all perfor-
mance evaluation sessions.
At the completion of each session, we retained a copy
of the completed worksheets, photographed the white-
boards, and returned the original worksheets to the par-
ticipant to help guide their responses for the second on-
line survey. The aggregated worksheets and time logs
support measurements for the actual efﬁcacy of the CoG
framework (See Section 4.3.2).
The performance evaluation interviewer transcribed
responses to the open-ended questions after each session
using the audio recordings. Two researchers jointly an-
alyzed all open-ended survey questions and each tran-
scription using iterative open-coding [61]. In alignment
with this process, we coded each research artifact and
built upon the codebook incrementally. We resolved all
disagreements by establishing a mutually agreed upon
deﬁnition for coded terms. From here, we re-coded pre-
viously coded items using the updated codebook and re-
peated this process until we coded all responses, resolved
all disagreements, and the codebook was stable.
Post-training survey. In this 27-question online survey
(App. B), conducted immediately after the performance
evaluation session, we collected responses measuring the
framework’s actual and perceived efﬁcacy. We asked
participants to re-apply CoG to their daily duties, which
allowed them to account for any new details they might
have considered since the previous session. Addition-
ally, we asked them to re-evaluate their perception of the
NYC3 baseline security posture and their ability to com-
plete digital security tasks. Using this information, we
can measure changes in how participants view the orga-
nization and their own abilities [19]. Further, we asked
participants to evaluate their ability to complete digital
security tasks solely using the CoG framework and and
to answer comprehension questions measuring their cur-
rent understanding of the framework.
Follow-up survey. The 13-question follow-up survey
(App. B) allowed us to measure framework adoption,
knowledge retention, and perceived efﬁcacy 30 days
after researchers departed. To measure the extent to
which participants adopted CoG analysis without in-
structor stimulus, we asked them to describe whether
and how they used the information derived from CoG
analysis or the framework itself within their daily duties.
These questions allow us to understand participants’ abil-
ity to apply output from the framework, measure their
adoption rates at work, and measure their internalization
of CoG concepts. We also continued to use self-efﬁcacy
questions supplemented with survey questions from the
technology acceptance model (TAM) [12].
Long-term evaluation. After 120 days, we evalu-
ated the efﬁcacy of adopted defense plans for protecting
NYC3 systems. We used a combination of NYC3 inci-
dent reports and system logs extracted solely from de-
fensive measures that participants recommended and im-
plemented because of their use of CoG threat modeling.
USENIX Association
27th USENIX Security Symposium    625
NYC3 deployed these new defensive measures in “blind
spots,” so each veriﬁed intrusion attempt or vulnerability
clearly links an improved security posture to these new
defensive measures.
3.3 Limitations
All ﬁeld studies and qualitative research should be in-
terpreted in the context of their limitations.
We opted to measure only one threat-modeling frame-
work: although our sample represents 37% of the NYC3
workforce, 25 participants (in many cases with no over-
lap in work roles) would not have been sufﬁcient to thor-
oughly compare multiple approaches. Testing multiple
models within participants was impractical due to the
strong potential for learning effects and the need to limit
participants’ time away from their job duties. As such,
it is possible that other threat-modeling or training ap-
proaches would be equally or more effective. We believe,
however, that our results still provide insight as to how
threat modeling in general can beneﬁt a large enterprise.
As we will describe in Section 4.3.2 below, we used
two NYC3 leaders to jointly evaluate the defense plans
produced by our participants. More, and more inde-
pendent, evaluators would be ideal, but was infeasible
given conﬁdentiality requirements and time constraints
on NYC3 leadership.
Our results may be affected by demand characteristics,
in which participants are more likely to respond posi-
tively due to close interaction with researchers [27, 51,
63]. We mitigated this through (1) anonymous online
surveys that facilitated open-ended, candid feedback, (2)
removing researchers from the environment for 30 days
before the follow-up survey, and (3) collecting actual
adoption metrics. Further, because we explained the pur-
pose of the study during recruitment, there may be selec-
tion bias in which those NYC3 personnel most interested
in the topic or framework were more likely to participate;
we mitigated this by asking NYC3 leaders to reinforce
that (non-)participation in the study would have no im-
pact on performance evaluations and by recruiting a large
portion of the NYC3 workforce.
NYC3’s mission, its use of pervasive defensive tech-
nologies, and its adherence to common compliance stan-
dards indicate that NYC3 is similar to other large orga-
nizations [29, 44, 45]; however, there may be speciﬁc or-
ganizational characteristics of NYC3 that are especially
well (or poorly) suited to threat modeling. Nonetheless,
our results suggest many directions for future work and
provide novel insights into the use of threat modeling in
an enterprise setting.
TAM has been criticized (e.g., by Legris et al. [33])
for insufﬁcient use coverage. Additionally, the positive
framing of TAM questions may lead to social desirabil-
ity biases [16]. To address coverage, we use TAM in
conjunction with the Bandura self-efﬁcacy scales for a
more complete picture. Moreover, reusing validated sur-
vey items and scales in this study is a best-practice in
survey design that has been shown to reduce bias and
improve construct validity [18, 22]. Lastly, we elicited
participant feedback with a negative framing explicitly
after each performance evaluation session, and implic-
itly when assessing threat modeling adoption at the 30-
day evaluation. Eliciting feedback through negatively-
framed mechanisms allowed participants to provide their
perceptions from both perspectives.
For each qualitative ﬁnding, we provide a participant
count, to indicate prevalence. However, participants who
did not mention a speciﬁc concept during an open-ended
question may simply have failed to state it, rather than
explicitly disagreeing. We therefore do not use statistical
hypothesis tests for these questions.
4 Results
Below we present the results of our case study evaluating
threat modeling in an enterprise environment, drawing
from transcripts and artifacts from performance evalua-
tion sessions, survey answers, and logged security met-
rics. We report participant demographics, baseline met-
rics, immediate post-training observations, 30-day obser-
vations, and observations after 120 days.
We organize our ﬁndings within the established frame-
work of perceived efﬁcacy, actual efﬁcacy, and actual
adoption [32,41,50]. Participants’ perceived efﬁcacy and
belief that they will achieve their desired outcomes di-
rectly shape their motivation for adopting threat model-
ing in the future [3]. Actual efﬁcacy conﬁrms the validity
of perceptions and further shapes the likelihood of adop-
tion. Lastly, regardless of perceived or actual efﬁcacy,
a framework must be adopted in order to demonstrate
true efﬁcacy within an environment. Through these three
measurements, we provide security practitioners with the
ﬁrst structured evaluation of threat modeling within a
large-scale enterprise environment.
4.1 Participants
Qualitative research best practices recommend inter-
viewing 12-20 participants for achieving data saturation
in thematic analysis [23]. To account for employees who
might need to withdraw from the study due to pressing
work duties, we recruited 28 participants for our study.
Of these, 25 participants completed the study (Table 1),
above qualitative recommendations, and we also reached
saturation in our performance evaluation sessions. For
the rest of this paper, all results refer to the 25 partici-
pants who completed the study. This sample represents
37% of the NYC3 employees as of August 8, 2017.
Technicians such as network administrators and secu-
rity engineers account for 18 of the participants; the re-
mainder fulﬁll supporting roles within NYC3 (e.g., lead-
626    27th USENIX Security Symposium
USENIX Association
ID
P01
P02
P03
P04
P05
P06
P07
P08
P09
P10
P11
P12
P13
P14
P15
P16
P17
P18
P19
P20
P21
P22
P23
P24
P25
Duty
Position
Leadership
Data Engr.
Sec Analyst
Sec Engineer
Governance
Sec Engineer
Sec Engineer
Net Admin
Sec Engineer
Sec Engineer
Net Admin
Sec Engineer
Sec Analyst
Sec Engineer
Sec Engineer
Support Staff
Sec Analyst
Sec Engineer
Sec Analyst
Leadership
Sec Analyst
Leadership
Sec Analyst
Leadership
Leadership
IT Exp
(yrs)
16-20
16-20
11-15
11-15
16-20
6-10
0-5
21-25
11-15
11-15
16-20
25+
0-5
11-15
16-20
6-10
16-20
21-25
21-25
11-15
0-5
11-15
16-20
0-5
0-5
Trng.
(yrs)
6-10
6-10
0-5
0-5
6-10
11-15
6-10
6-10
0-5
6-10
6-10
6-10
0-5
0-5
25+
0-5
16-20
16-20
6-10
6-10
6-10
6-10
6-10
0-5
0-5
Educ.1
SC
G
SC
BS
SC
P
G
G
SC
BS
BS
G
BS
BS
SC
BS
G
G
SC
G
G
G
BS
BS
G
1 SC: Some College, BS: Bachelor’s, G: Graduate degree,
P: Prefer not to answer