### Analysis [59]
We provide examples of both the tabular and graph-based methods in Appendix D. The tabular tool allows users to record their responses for each subtask within the CoG framework, with each section supporting data in subsequent sections. The graph-based method offers an alternative, complementary approach for eliciting the same data. Previous research indicates that various learning styles benefit from multiple forms of data elicitation [31].

### Classroom Examples
#### First Example
In the first classroom example, the instructor guided participants through a scenario based on the Star Wars movie franchise to determine the Center of Gravity (CoG) for the Galactic Empire. The instructor provided step-by-step instructions for using both the tabular and graphical tools.

#### Second Example
In the second example, participants worked together without instructor guidance to apply the CoG and framework tools to a fictional e-commerce scenario. Both scenarios are described in detail in Appendix A.

### Pre-Intervention Observations
Prior to the intervention, the instructor observed NYC3 employees at work for four days to better understand their operating environment. The fictitious scenarios were developed to avoid reflecting any specific conditions within NYC3, thereby reducing bias during training and preventing participants from being coached towards "approved solutions."

### Instructional Consistency
To control for variations in instruction, the same instructor was used for all groups. The instructor, a member of the research team with extensive subject-matter knowledge and six months of formal university training on threat modeling, communicated this experience to establish credibility. During each class, participants could ask questions, and the instructor maintained a log of these questions. Answers were incorporated into future sessions and emailed to participants who had attended previous sessions.

### Performance Evaluation Session
After completing the educational intervention training, each participant underwent a 60-minute individual session where they applied the CoG framework to their daily duties. For instance, P17, a security analyst, used the framework to develop plans for better defending NYC endpoint workstations (see Appendix A.3). This phase provided hands-on reinforcement learning, as recommended by Experiential Learning Theory (ELT) and Social Learning Theory (SLT) [6, 31].

Each session was audio recorded, and participants were provided with clean worksheets and whiteboards for brainstorming (Appendix D). Participants could bring notes from the previous training. Task completion times were logged to measure the efficiency of the framework without putting undue pressure on participants.

The interviewer used the constructive interaction method, asking participants to openly communicate throughout each subtask in Section 2.2 [40]. The instructor restated participants' verbal comments or documented responses to assist with data elicitation but did not introduce new concepts to prevent bias. The same interviewer conducted all performance evaluation sessions for consistency.

At the end of each session, completed worksheets were retained, whiteboards were photographed, and original worksheets were returned to participants. The aggregated worksheets and time logs support measurements of the CoG framework's efficacy (Section 4.3.2).

### Data Analysis
The performance evaluation interviewer transcribed responses to open-ended questions using audio recordings. Two researchers jointly analyzed all open-ended survey questions and transcriptions using iterative open-coding [61]. Each research artifact was coded, and the codebook was built incrementally. Disagreements were resolved by establishing mutually agreed-upon definitions for coded terms. Previously coded items were re-coded using the updated codebook until all responses were coded, disagreements were resolved, and the codebook was stable.

### Post-Training Survey
This 27-question online survey (Appendix B) was conducted immediately after the performance evaluation session to measure the framework’s actual and perceived efficacy. Participants were asked to re-apply the CoG framework to their daily duties, allowing them to account for any new details. They also re-evaluated their perception of NYC3's baseline security posture and their ability to complete digital security tasks. This information helps measure changes in how participants view the organization and their own abilities [19]. Additionally, participants evaluated their ability to complete digital security tasks using the CoG framework and answered comprehension questions to measure their current understanding.

### Follow-Up Survey
The 13-question follow-up survey (Appendix B) measured framework adoption, knowledge retention, and perceived efficacy 30 days after the researchers departed. To assess the extent of CoG analysis adoption without instructor stimulus, participants were asked to describe whether and how they used the information derived from CoG analysis or the framework itself in their daily duties. This allowed us to understand participants’ ability to apply the framework, measure adoption rates, and assess internalization of CoG concepts. Self-efficacy questions were supplemented with survey questions from the Technology Acceptance Model (TAM) [12].

### Long-Term Evaluation
After 120 days, the efficacy of adopted defense plans for protecting NYC3 systems was evaluated using incident reports and system logs. These new defensive measures were deployed in “blind spots,” ensuring that verified intrusion attempts or vulnerabilities clearly linked to an improved security posture due to the use of CoG threat modeling.

### Limitations
All field studies and qualitative research have limitations. We only measured one threat-modeling framework, and while our sample represents 37% of the NYC3 workforce, 25 participants (with no overlap in work roles) were insufficient to thoroughly compare multiple approaches. Testing multiple models was impractical due to potential learning effects and the need to limit participants' time away from their job duties. Other threat-modeling or training approaches might be equally or more effective, but our results still provide insight into the benefits of threat modeling in large enterprises.

Two NYC3 leaders jointly evaluated the defense plans produced by participants. More independent evaluators would be ideal but were infeasible due to confidentiality requirements and time constraints.

Our results may be affected by demand characteristics, where participants respond positively due to close interaction with researchers [27, 51, 63]. We mitigated this through anonymous online surveys, removing researchers from the environment for 30 days before the follow-up survey, and collecting actual adoption metrics. Selection bias was mitigated by reinforcing that participation in the study would have no impact on performance evaluations and by recruiting a large portion of the NYC3 workforce.

NYC3's mission, use of pervasive defensive technologies, and adherence to common compliance standards indicate it is similar to other large organizations [29, 44, 45], though specific organizational characteristics may affect threat modeling. Our results suggest directions for future work and provide insights into the use of threat modeling in an enterprise setting.

TAM has been criticized for insufficient use coverage, and positive framing of TAM questions may lead to social desirability biases [16]. We addressed coverage by using TAM in conjunction with Bandura self-efficacy scales and reusing validated survey items and scales, which reduce bias and improve construct validity [18, 22]. Feedback was elicited with negative framing to allow participants to provide balanced perceptions.

### Results
We present the results of our case study evaluating threat modeling in an enterprise environment, drawing from transcripts, artifacts, survey answers, and logged security metrics. We report participant demographics, baseline metrics, immediate post-training observations, 30-day observations, and observations after 120 days.

### Participant Demographics
Qualitative research best practices recommend interviewing 12-20 participants for data saturation in thematic analysis [23]. We recruited 28 participants, of whom 25 completed the study (Table 1), representing 37% of NYC3 employees as of August 8, 2017. Technicians such as network administrators and security engineers accounted for 18 participants, with the remainder in supporting roles.

| ID | Duty Position | IT Exp (yrs) | Training (yrs) | Education |
|----|---------------|--------------|----------------|-----------|
| P01 | Leadership    | 16-20        | 6-10           | SC        |
| P02 | Data Engr.    | 16-20        | 6-10           | G         |
| P03 | Sec Analyst   | 11-15        | 0-5            | SC        |
| P04 | Sec Engineer  | 11-15        | 0-5            | BS        |
| P05 | Governance    | 16-20        | 6-10           | SC        |
| P06 | Sec Engineer  | 6-10         | 11-15          | P         |
| P07 | Sec Engineer  | 0-5          | 6-10           | G         |
| P08 | Net Admin     | 21-25        | 6-10           | G         |
| P09 | Sec Engineer  | 11-15        | 0-5            | SC        |
| P10 | Sec Engineer  | 11-15        | 6-10           | BS        |
| P11 | Net Admin     | 16-20        | 6-10           | BS        |
| P12 | Sec Engineer  | 25+          | 6-10           | G         |
| P13 | Sec Analyst   | 0-5          | 0-5            | BS        |
| P14 | Sec Engineer  | 11-15        | 0-5            | BS        |
| P15 | Sec Engineer  | 16-20        | 25+            | SC        |
| P16 | Support Staff | 6-10         | 0-5            | BS        |
| P17 | Sec Analyst   | 16-20        | 16-20          | G         |
| P18 | Sec Engineer  | 21-25        | 16-20          | SC        |
| P19 | Sec Analyst   | 21-25        | 6-10           | G         |
| P20 | Leadership    | 11-15        | 6-10           | G         |
| P21 | Sec Analyst   | 0-5          | 6-10           | G         |
| P22 | Leadership    | 11-15        | 6-10           | BS        |
| P23 | Sec Analyst   | 16-20        | 0-5            | BS        |
| P24 | Leadership    | 0-5          | 0-5            | G         |
| P25 | Leadership    | 0-5          | 0-5            | G         |

**Note:**
- SC: Some College
- BS: Bachelor’s
- G: Graduate degree
- P: Prefer not to answer

This structured evaluation provides security practitioners with the first comprehensive assessment of threat modeling in a large-scale enterprise environment.