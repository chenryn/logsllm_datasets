Latency Communications, which remains to be met in the future
SA architecture). (ii) The delay reduction mainly comes from the
second hop (i.e., from the gNB to cellular core network). The reduc-
tion attributes to the flatten architecture of 5G (i.e., part of cellular
core network functions sinks to gNB so as to minimize processing
latency [66]) and the specialized 25 Gbps fiber fronthual/backhual
(according to our communication with ISP technicians).
Delay vs. path length. To put the above delay reduction in a
big picture, we re-arrange the RTTs according to the geographical
distance of each path, as shown in Fig. 15. We see that: (i) The RTTs
of both 4G and 5G increase with path length. In particular, the RTT
increase by 5× as distance increases from 100 km to 2500 km, and
gap between the 4G and 5G networks is 22±3.57ms on average, but
RTT reaches up to 82.35ms on average for 5G paths. (ii) The RTT
the ratio between the gap (shown by the shade) and the absolute
RTT value becomes smaller as path distance increases. The findings
convey a message that the untamed latency in the wireline paths,
which is beyond mobile carriers’ control, may neutralize 5G’s latency
advantage. To unleash the full potential of 5G applications, the legacy
wireline networks also need to be retrofitted, so as to effectively reduce
the end-to-end latency. Emerging architectures that shorten the path
length, e.g., edge caching and computing, may also confine the latency.
5 APPLICATION PERFORMANCE
We take the mobile web browsing and emerging UHD panoramic
video telephony as two representative examples to examine the
application QoE under 5G.
5.1 Web Browsing
We build an HTML5 website and deploy it in an Apache 2.0 cloud
server. The website consists of multiple web pages, i.e., web search,
image, on-line shopping, map navigation and HTTP video stream-
ing. For instance, there are some pages with different size/resolution
images, which can be used to test the page loading time (PLT) of im-
age browsing. PLT comprises two parts: The content downloading,
and page rendering time. Both are measured using Google Chrome’s
developer tools [36] on a DELL G3 3779 laptop (Windows10 x64 |
Intel Core i7 8th generation | 64 GB RAM | 512 GB SSD). We use
HTTP/2.0 + BBR, and clear the web cache and cookie before each
experiment to avoid their artifacts. Fig. 16 plots the mean and std.
of PLT calculated across 10 websites in each category. Despite the
5× DL throughput gain (validated in Sec. 4), the 5G PLT shows
minimum reduction (5% on average) compared with that in 4G.
A breakdown of the PLT latency in Fig. 17 reveals two causes:
(i) The rendering time takes a dominant fraction in PLT (especially
for large-size pages), which only depends on the computational
capacity of end-devices rather than network throughput. (ii) Even
when considering the downloading time alone, 5G only provides a
marginal 20.68% reduction on average across the 5 categories. The
reason lies in the transitioning behavior of TCP, i.e., the slow-start
phase. Our measurement shows that, even for the most 5G-friendly
TCP (i.e., BBR), the slow-start phase lasts about 6s before it con-
verges to the high network bandwidth (Fig. 8). Unfortunately, most
web pages are only a few MB and have already finished download-
ing well before TCP converges, which heavily underutilizes the 5G
bandwidth.
To sum up, the web browsing performance is still handicapped by
the computational efficiency of mobile devices, which cannot be re-
solved by 5G. Also, the transient behaviors of TCP severely hamper the
5G network efficiency, especially for short bursty-flows. It is unlikely
486
 0 40 80 120 160 200 0 40 80 120 160 2005G RTT (ms)4G RTT (ms) 0 10 20 30 40 50012345678RTT (ms)Hop Identification4G5GFigure 16: PLT of different websites.
Figure 17: PLT of different images.
Figure 18: Video throughput.
Figure 19: Received 5.7K video throughput fluctuation
under 5G networks.
Figure 20: Frame delay of 4K video telephony.
that 5G will tailor itself for TCP, as it violates the end-to-end design
principle of the Internet [27]. However, a minor upgrade of the TCP
at the end hosts is still justifiable and may eventually unleash the
5G potential. For example, it has been shown that replacing TCP’s
slow-start probing with a deterministic bandwidth estimation [90]
may substantially improve TCP efficiency over cellular networks.
5.2 UHD Panoramic Video Telephony
Mobile UHD panoramic video telephony poses a high demand
on network capacity and stability, especially for the uplink (UL).
The previous study [18] has shown that 4K telephony produces
heavy traffic load with unpredictable fluctuations (35-68 Mbps [75]),
making it unaffordable for 4G networks. It is much anticipated that
5G will be a niche technology to resolve this issue. We now validate
the feasibility using the 360TEL system that we developed (Sec. 2).
Tolerance on video throughput fluctuation. 360TEL con-
sists of a UL pushing flow (sender→server) and DL pulling flow
(server→receiver). We first measure the UL video throughput at the
server using Wireshark and present the results in Fig. 18, where
dynamic represents the case of constantly changing the camera’s
view. We observe that the average received throughput of all HD
resolution videos (720P, 1080P, 4K and 5.7K) does not exceed the
5G UL capacity (100 Mbps at daytime from Sec. 4.1). However, 4G
networks cannot support a 5.7K video. The average throughput of
5.7K video under 4G is much smaller than that under 5G, because
of the network congestion and the consequent significant video
frame losses.
Despite enough bandwidth of 5G UL, the fluctuation of video
streams [18] may still cause low QoE. In our experiments, we ob-
serve that 5G can well tolerate 4K’s fluctuation, but is sometimes
ineffective for 5.7K video. Fig. 19 plots the received video through-
put over a 30s 5.7K video session, captured in static and dynamic
scenarios, respectively. The large fluctuation is remarkable in the
dynamic scenes, which escalates the video traffic (sometimes be-
yond the 5G UL bandwidth) and thus causes frame freezing. We
find 6 frame freezing events within our real-world measurement.
Frame delay. The end-to-end video frame delay is critical to
QoE of the real-time video telephony. We measure the frame delay
using a “stopwatch timing” method. Specially, we use the Insta360
camera to shoot a stopwatch (t1) and then record the time from
the same stopwatch displayed on at the video receiver (t2). ∆t=
t2− t1 is the end-to-end video frame delay. Fig. 20 plots ∆t for a
30s 4K video session described above. It is evident that 4G suffers
from severe congestion and hence occasionally long latency due
to its insufficient bandwidth. Surprisingly, even for 5G, the frame
latency remains on the level of 950ms, which falls short of the
460ms requirements of real-time video telephony [88].
We breakdown the frame delay, by separately examining frame
processing operations (camera capture, frame patch splice, codec
and video rendering), RTMP streaming/receiving and network
transmission. Following the same approach as in [18], we find
that the encoding time of the H.264 hardware codec is about 160ms
and the decoding time is 50ms. We then estimate the frame capture,
patch splice and rending time to be about 440ms by calculating
the latency gap between the stopwatch and the time of the pre-
view video shown on local devices. Note that the preview video
can roughly cover the whole sender’s frame processing, because it
is not sent to the transmission link by RTMP streaming. Overall,
the frame processing latency is about 650ms in our measurement,
which is 10× than the network transmission delay (66ms) for each
frame! In other words, the frame processing latency conspicuously
parallelizes the end-to-end delay in Sec. 4.4, even under 5G NSA
pattern.
To sum up, the high bandwidth of 5G provides more redundancy to
tolerate video traffic fluctuations, but the delay spent on smartphone’s
local processing remains as a prohibitive latency bottleneck, ruining
the user experiences in real-time interactive. Thus, it is imperative to
improve the smartphones’ processing capacities in order to support
5G’s niche applications, such as immersing interactive video telephony
which demands both high bandwidth and low end-to-end latency.
6 5G SMARTPHONE ENERGY CONSUMPTION
In this section, we first profile a 5G smartphone’s energy consump-
tion when running mainstream applications. We then run a micro-
487
01.02.03.04.05.06.0SearchImageShoppingMapVideoPLT (s)    Web Page TypeDownloadingRendering 4G 5G00.40.81.21.62.0124816PLT (s)    Image Size (MB)DownloadingRendering 4G 5G 0 30 60 90 120 150Throughput (Mbps)720P1080P 4K 5.7K  Video Resolution4G Static4G Dynamic5G Static5G Dynamic060100120 0 6 12 18 24 30Throughput    (Mbps)Time (s)StaticDynamic0.81.21.6 0 6 12 18 24 30Frame Delay(s)Time (s)4G5GFigure 22: Energy efficiency under fully-
Figure 21: Energy consumption break
down under daily app. usage.
saturated traffic.
scopic analysis of the power management scheme under the 5G
NSA. Guided by the measurement and analysis, we propose and
validate a power saving scheme. Our experimental results are de-
rived from two ZTE Axon10 Pro 5G phones, but the power budget
breakdown should be generalizable to other 5G phone models.
6.1 Profiling the Energy Consumption of 5G
Applications
Energy consumption on 5G radios vs. other smartphone
components. We use pwrStrip to measure the smartphone’s energy
consumption when running 4 typical applications: Google Chrome,
Tencent video player, Arrow.io Cloud Game and File downloader.
We breakdown the overall energy cost into 4 parts as follows: (i)
To get the Android system consumption, we turn off the screen and
turn on the "airplane" mode to kill all background applications. (ii)
We then measure the screen element at the maximum brightness
with other settings unchanged. (iii) To obtain the power consump-
tion of the application alone, we load the application’s contents
in advance and run the applications off-line. (iv) We finally record
the energy trace of the 4G/5G radio interface at normal operations.
Notably, the wireless channel quality may affect energy consump-
tion [29]. For instance, poor wireless channel degrades the bit-rate,
HARQ efficiency and other MAC operations, which increases the
energy per bit. To isolate the bias caused by wireless signal quality
variation, we carefully carry out all the measurements in regions
with a consistent RSRP level of [-80, -60] dBm.
The results in Fig. 21 show that: (i) The 5G module dominates
the energy cost (accounting for 55.18% on average of the total bud-
get across 4 applications), far exceeding the screen (30.73%) which
is known to be the most power-hungry component before 5G [42].
In comparison, 4G only accounts for 24.22% - 50.20%. The heavy
energy consumption of 5G radio attributes to its more powerful
baseband and RF hardware [86], e.g., wide-band data converters
(100 MHz vs. 20 MHz in 4G) and 4×4 MIMO [65]. In addition, we
note that the mainstream 5G smartphones have not launched a
SoC (System on Chip) solution [30, 56] with integrated CPU, GPU,
DSP and 5G radio modems. Instead, they adopt separate but less
energy efficient 5G modems as a plug-in to the legacy 4G SoC. An
example is the Qualcomm Snapdragon TM855 [70] plus SDX 50M
5G modem combo used in our phone model. Immature packaging
of 4G SoC and 5G modem, plus the interaction overhead between
the processor and the modem, lead to much more power loss than
an integrated solution. (ii) The total power consumption increases
with application traffic intensity and the fraction of power spent in
Data Transmission increases accordingly. We quantitatively run
Figure 23: A showcase of 5G energy man-
agement.
iperf3 UDP to download data for different lengths time with a
saturated sending rate. We calculate and plot the energy-per-bit in
Fig. 22. We find that the energy-per-bit of 5G is only 1
4 of 4G. This
implies that 5G can be much more energy efficient than 4G, but only
when if upper layer protocols can fully utilize its available bit-rate,
and a proper power management scheme is in position to activate the
radio only when necessary.
6.2 A Showcase of 5G Energy Management
The energy management of 5G radio follows a state machine (details
in Appendix B). In general, 5G radio switches between RRC_IDLE
(no Tx/Rx) and RRC_CON N ECT ED (on-going Tx/Rx) status, and
adopts the discontinuous reception (DRX) mechanism for power
saving. Fig. 23 plots the fine-grained energy traces for an example
application session, where we trigger a web loading per 3s for 10
times, starting from time 10s (t1) and ending at 40s (t3). Comparing
against the same experiment for 4G, we have the following observa-
tions: (i) Since 5G bandwidth is poorly utilized in short burst flows,
it consumes 1.67× more energy (J) than 4G when running the same
web loading sessions. (ii) The power consumption pattern of 5G
shows very obvious jagged fluctuations, which is caused by the dis-
continuous page downloading operations. In particular, when trig-
gering a webpage download, the energy consumption increases for
entering the RRC_CON N ECT ED. After each downloading trans-
mission (less than 3s interval), the UE uses the DRX mechanism that
consumes less power. Similar fluctuation within a smaller range is
also observed for 4G. (iii) 5G has an obvious long-tail stage when
rolling back from RRC_CON N ECT ED to RRC_IDLE, which leads
to an additional waste of energy. More specifically, after the transfer
ending at t3, we continue to monitor the Android kernel until the
power value recovers to the RRC_IDLE level. From our analysis,
4G returns to RRC_IDLE after about 10s (at t4), while 5G takes
about 20s to finish the tail stage (at t5). We can observe that the
long tail stage, while existing in 4G [42], is exacerbated by the
5G NSA architecture. In particular, to finish the switch from NR
RRC_CON N ECT ED to RRC_IDLE, the 5G module must first go
through the 4G state machine via LTE RRC Reconfiguration (see
Fig. 25 in Appendix B). The process is equivalent to activating an
LTE tail period again (marked by the black box), which compounds
the tail energy overhead.
6.3 Optimizing the 5G Power Management
The above experiments indicate a simple way to improve the power
efficiency of the 5G state machine: We can adopt a dynamic mode
488
 0 2000 4000 6000 8000BrowserPlayerGameDownloadPower (mW)    Application TypeSystemScreenApp4G5G00.51.01.52.02.501020304050Efficiency (uJ/bit)Transmission Time (s)4G5Gt1: Promotion Startt2: Transfer Startt3: Transfer Endt4: LTE Tail Endt5: NR Tail EndTable 4: Energy consumption (J) of different models.
Web
85.44±1.08
113.94±1.31
95.69±1.18
85.41±1.07
Video
227.13±5.26
140.19±0.69
123.03±0.57
133.66±0.71
Model
LTE
NR NSA
NR Oracle
Dyn. switch
File
357.67±8.22
157.29±1.03
139.72±1.03
150.80±1.03
selection scheme, which turns on the energy-hungry 5G module
only when necessary. Specifically, if the instantaneous traffic inten-
sity measured at the UE is approaching 4G’s capacity, i.e., 100 Mbps,
we switch the radio into the 5G NR module; Otherwise, it should
stay in 4G mode. To verify the effectiveness of this scheme, we
use a trace-driven simulator because the smartphone UE does not