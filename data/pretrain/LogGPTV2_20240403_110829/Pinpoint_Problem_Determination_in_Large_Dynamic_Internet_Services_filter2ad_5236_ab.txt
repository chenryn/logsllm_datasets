failure data point and mark it with all the requests that we
believe have failed.
The clustering algorithm then groups these components
and the failure data point together. The interesting result,
for our purposes, is the set of components clustered with our
failure data point. These are the components whose occur-
rences are most correlated with failures, and hence where
the root cause is likely to lie.
3. Pinpoint Implementation
We have implemented a prototype of Pinpoint on top of
the J2EE middleware platform, a network sniffer, and an
analyzer based on standard data clustering techniques. Our
prototype does not require any modiﬁcations to be made to
J2EE applications. Only our external fault detection mod-
ule requires application-speciﬁc checks—and these do not
require modiﬁcation of application components. For this
reason, Pinpoint can be used as a problem determination
aid for almost any J2EE application.
3.1. J2EE Platform
Using Sun’s J2EE 1.2 single-node reference implemen-
tation as a base, we have made modiﬁcations to support
client request tracing and simple fault detection. We have
also added a fault injection layer, used for evaluating our
system. We discuss fault injection as part of our experimen-
tal setup in Section 4.1.1.
J2EE supports three kinds of components: Enterprise
JavaBeans, often used to implement business and applica-
tion logic; Java Scripting Pages (JSP) used to dynamically
build HTML page; and JSP tags, components that provide
extensions to JSP. We have instrumented each of these com-
ponent layers.
We assign every client HTTP requests a unique ID as
it enters our system. We store this unique ID in a thread-
speciﬁc local variable and also return it in an HTTP header
for use by our external fault detector. With the assump-
tion that components do not spawn any new threads and the
fact that the reference implementation of J2EE we are using
does not support clustering, storing the request ID in thread-
speciﬁc local state was sufﬁcient for our purposes. If a com-
ponent had spawned threads, we would likely have had to
modify the thread creation classes or the application com-
ponent to ensure the request ID was correctly preserved.
Similarly, if our J2EE implementation used clustering, we
would have to modify the remote method invocation pro-
tocol and/or generated wrapper-code to automatically pass
the request ID between machines.
Our modiﬁed J2EE platform’s internal fault detection
mechanism simply logs exceptions that pass across com-
ponent boundaries. Though this is a simple error detection
mechanism, it does catch many real faults that are masked
and difﬁcult to detect externally. For example, when run-
ning an e-commerce demonstration application, a faulty in-
ventory database will generate an exception, which will be
masked with the message “Item out of stock” before being
shown to the user. Our internal fault detection system is
able to detect this fault and report it before it is masked.
3.2. Layer 7 Packet Sniffer
To implement our external failure detector, we have built
a Java-based Layer 7 network sniffer engine, called Snif-
ﬂet. It is built on a network packet capture library, Jpcap
[1], which provides wrappers around libpcap [17] to capture
TCP packets from the network interface. We have imple-
mented TCP and HTTP protocol checkers to monitor TCP
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:30 UTC from IEEE Xplore.  Restrictions apply. 
and HTTP failures. Snifﬂet uses a ﬂexible logging package,
log4j [12] from the Apache group, to log detected failures.
Snifﬂet detects TCP errors such as resets and timeouts,
including server freezes, and detects HTTP errors such as
404 (Not found) and 500 (Internal server error). It also pro-
vides an API that enables programmers to analyze HTTP
requests and responses, including content, for customized
failure detection. We have implemented custom content de-
tectors for the J2EE server that looked for simple failed re-
sponses, such as “Included servlet error”.
Snifﬂet listens for client request IDs in the HTTP re-
sponse headers of the service. Some failures, such as when
a client cannot connect to a service, occur before Snifﬂet
can ﬁnd an ID for a client request. In these cases, Snifﬂet
generates its own unique request ID for logging purposes.
3.3. Data Clustering Analysis
In our implementation of Pinpoint, we use a hierarchical
clustering method, an unweighted pair-group method using
arithmetic averages (UPGMA), and calculate distances be-
tween components using the Jaccard similarity coefﬁcient.
For our purposes, UPGMA’s main advantage is that it cal-
culates distances between clusters by averaging the distance
among all pairs of points within the clusters. This provides
a much less extreme calculation of this distance than other
methods, which use a nearest-neighbor or farthest-neighbor
calculation. The Jaccard similarity coefﬁcient calculates
distance between two points based on the ratio of the num-
ber of requests they appear in together out of all the requests
the two points appear in total. More details on these algo-
rithms can be found in standard data clustering textbooks,
such as [23, 18].
4. Evaluation
To validate our approach, we ran an e-commerce service,
the J2EE PetStore demonstration application, and system-
atically injected faults into the system over a series of runs.
We used Pinpoint to monitor the system and diagnose the
faults that we injected, and compare its results to other prob-
lem determination techniques. In this section, we detail our
experimental setup, describe the metrics we used to evaluate
Pinpoint’s efﬁcacy, and present the results of our trials.
4.1. Experimental Setup
We ran 133 tests that included single-component faults
and faults triggered by interactions between two, three and
four components. For each test, we ran the PetStore ap-
plication, monitored by Pinpoint, for ﬁve minutes. During
this period, we ran a client emulator that generated a work-
load on the application, while injecting deterministic faults
into the system. We restarted the application server between
each test to avoid contaminating a run with residual faulty
behavior from previous runs. The setup was a closed sys-
tem with a single transaction active at any time. Different
transactions used different sets of components.
Our physical machine setup has a server running on one
machines and clients on another. The J2EE server runs
on a quad-PIII 500MHz with 1GB of RAM running Linux
2.2.12 and Blackdown JDK 1.3. For convenience, Snifﬂet
also runs on the same machine. The clients run on a PIII
600MHz with 256MB of RAM running Linux 2.2.17 and
Blackdown JDK 1.3.
4.1.1 Fault Injection
In our experiments, we model faults that are triggered by the
use of individual components, or interactions among multi-
ple components. A fault is deﬁned by 1) the un-ordered trig-
ger set of “faulty” components which are together respon-
sible for the fault, and 2) the type of fault to be injected. In
these experiments, we inject four different types of faults:
(cid:15) Declared exceptions, such as Java RemoteExceptions
or IOExceptions.
(cid:15) Undeclared exceptions, such as runtime exceptions.
(cid:15) Inﬁnite loops, where the called component never re-
turns control to the callee.
(cid:15) Null calls, where the called component is never actu-
ally executed.
We chose to inject these particular faults because they
cause failures that span the axes from predictable to unpre-
dictable behaviors, simulating the range (if not the compo-
sition) of problems that can occur in a real system. In real
systems, declared exceptions are often handled and masked
directly by the application code. Undeclared exceptions are
less often handled by the application code, and more of-
ten are caught by the underlying middleware as a “last re-
sort.” Inﬁnite loops simply stop the client request from com-
pleting, while null calls prematurely prevent (perhaps vital)
functionality from working.
It is important to note that our fault injection system is
kept separate from our fault detection system. Though our
internal detection system does detect thrown exceptions rel-
atively trivially, inﬁnite loops are only detectable through
TCP timeouts seen by our external fault detector, the Snif-
ﬂet. The null call fault is usually not directly detectable at
all. To detect null call faults, our fault detection mecha-
nisms must rely on catching secondary effects of a null call,
such as subsequent exceptions or faults.
To inject faults into our system, we modiﬁed the J2EE
middleware to check a fault speciﬁcation table upon every
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:30 UTC from IEEE Xplore.  Restrictions apply. 
component invocation. If the set of components used in the
request matches a fault’s trigger set, we cause the speciﬁed
fault to occur at the last component in the set that is used.
For example, for a trigger set of size 3, a fault is injected at
the third component in the trigger set used in a request.
4.1.2 Client Browser Emulator
To generate load on our system, we built a client browser
emulator that captures traces of a person browsing a web
site, and then replays this log multiple times during test
runs. The client dynamically replaces cookies, hostname in-
formation, and username/password information in the logs
being replayed to match the current context. For example,
unique user ID’s need to be generated when creating new
accounts, and cookies provided by servers need to be main-
tained within sessions.
The requests include: searching, browsing for item de-
tails, creating new accounts, updating user proﬁles, placing
orders, and checkout.
4.2. Metrics
To evaluate the effectiveness of Pinpoint, we use two
metrics: accuracy and precision. A result is accurate when
all components causing a fault are correctly identiﬁed. For
example, if two components, A and B, are interacting to
cause a failure, identifying both would be accurate. Iden-
tifying only one or neither, would not be accurate. When
we measure the accuracy of a problem determination tech-
nique, we are measuring how often its results are accurate.
Precision, the second metric we use in our evaluation,
is the ratio between correctly identiﬁed faults and predicted
faults. For example, predicting the set fA, B, C, D, Eg when
only A and B are faulty gives a precision of 40%.
Other ﬁelds—including Data Mining, Information Re-
trieval, and Intrusion Detection—use precision and recall,
instead of accuracy. Recall is deﬁned as the ratio between
correctly identiﬁed faults and actual faults. For example, if
2 components, A and B, are faulty, identifying both compo-
nents would give a perfect recall of 100%. Identifying only
one of the two components gives a recall of 50%. How-
ever, for fault management systems, we believe accuracy
is a better metric because identifying a subset of the real
causes may misdirect the diagnosis and thus has little value.
A system with low accuracy is not useful because it fails
to identify the real faults. A system that has high accu-
racy with low precision is not useful either because it ﬂoods
users with too many false positives. An ideal system would
predict a minimal and correct set of of faults. In practice,
however, there is a tension between having high accuracy
and high precision. Maximizing precision often means that
potential faults are being thrown out, which decreases ac-
curacy. Maximizing accuracy often means that non-faulty
components are also included, which decreases precision.
To visualize the trade-offs an analysis technique makes
between accuracy and precision, we plot its accuracy ver-
sus false positives (1 - precision) as we vary the technique’s
sensitivity. This plot is called a Receiver Operating Charac-
teristic (ROC) curve. Generally, at a very high sensitivity,
an analysis technique will be very accurate, but also return
very many false positives. As we decrease the sensitivity,
we reduce the accuracy, but also reduce the number of false
positives returned. The ROC curve is especially useful be-
cause it allows us to examine analysis’ behavior without ar-
bitrarily choosing a sensitivity value. In our experiments
we evaluate analysis techniques by comparing their ROC
curves.
4.3. Evaluation Results
We compare Pinpoint’s clustering analysis to two tra-
ditional failure analysis techniques. The ﬁrst is detection,
which returns the set of components recorded by our inter-
nal fault detection framework. This is similar to the result
a monitoring system would generate, returning the compo-
nent where a failure is manifesting. At its lowest sensitivity,
this technique returns the single component where the fault
was detected. At higher sensitivities, detection inspects the
call stack, and returns the components in the call chain.
The second analysis technique we compare ourselves to
is dependency checking, which returns the components that
the failed requests use. In this technique, a component is
nominated as a potential fault if it occurs in more than some
percentage of the failed requests. This percentage is the in-
verse of the sensitivity setting. For example, by setting the
sensitivity to 0%, this technique returns only components
that occurred in 100% of the failed requests. Setting the
sensitivity close to 100% returns all the components used
in any of the failed requests.
It is worth noting that our
implementation of dependency checking takes advantage of
Pinpoint’s dynamic request tracing for dependency discov-
ery. Hence, the quality of its results are an over-estimation
of how well dependency checking would perform using a
static model. The key difference between this technique and
Pinpoint’s cluster analysis is that dependency checking does
not take the traces of successful requests into account.
We show in Figure 2 the summary results for all our ex-
periments, comparing Pinpoint with the other techniques.
Note that Pinpoint consistently has both a higher accu-
racy and a higher precision than detection and dependency
checking. This improvement is most striking in the results
for single-component failures, shown in Figure 3. Here, we
see that Pinpoint achieves an accuracy of 80-90% with a
relatively high precision of 50-60%. In comparison, depen-
dency checking never has a precision higher than 20% for
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:20:30 UTC from IEEE Xplore.  Restrictions apply. 
y
c
a
r
u
c
c
A
1
0.8
0.6
0.4
0.2
0
0
Simple Dependency Analysis
Detection Analysis
Pinpoint Cluster Analysis
0.2
0.4
0.6
0.8
1
False Positive Rate (1 - Precision)
y
c
a
r
u
c
c
A
1
0.8
0.6
0.4
0.2
0
0
0.2
0.4
0.6
0.8
1
False Positive Rate (1 - Precision)
1 Component Faults
2 Component Faults
3 Component Faults
4 Component Faults
Figure 2. Summary accuracy vs. false posi-
tive rate over all tests
Figure 4. Pinpoint’s accuracy vs. false posi-
tive rate for interacting component faults.
y
c
a
r
u
c
c
A
1
0.8
0.6
0.4
0.2
0
0