"value":
"name"
"type"
Constant Post-Aggregator会返回一个常数，比如 100。可以将Aggregator返回的结果转换
"aggregations":[
"type"
"name":,
:"hyperUniqueCardinality",
"type":“"hyperUnique”
"name":“unique_users"
"fieldName":"uniques"
"type":"count"
"name":"rows"
,
"constant"
:
Druid实时大数据分析原理与实践
---
## Page 151
第6章
"aggregations”:
Post-Aggregator的计算可以嵌套，
"postAggregations":[
"postAggregations":[
数据查询
"type":“doubleSum"
"fieldName":“total",
"type":"count"
"name":"rows"
"name":"average_users_per_row",
"fields":
"name":"tot"，
"type":
"fn":"/",
"arithmetic"
"type":"fieldAccess"
"name":"rows",
"fieldName":"rows"
"type":"hyperUniqueCardinality"
"fieldName":"unique_users",
以此得到更加丰富的计算方式。一个简单的示例如下：
127
---
## Page 152
128
"fields":[
"type":
"name": "average",
"fn"：
"arithmetic"
"value":100
"type": "constant",
"type":"arithmetic"
"fields"：
"name": "const",
"name":"div",
"fn":"7”,
"type":
"name":"rows",
"fieldName":“rows"
"type":"fieldAccess"
"name":"tot",
"fieldName":"tot",
"fieldAccess"
Druid实时大数据分析原理与实践
---
## Page 153
为东8区，因此需要在时间中加入“+08:00”。查询时间格式为：
6.2.5
3.
tive_contains 的 JSON示例如下：
2.
是否区分大小写。JOSN示例如下：
字符串匹配方式。
6.2.4
第6章
如果指定的维度的值包含给定的字符串，则匹配。不区分大小写，没有case_sensitive
insensitive_contains
fragment
如果指定的维度的值包含给定的字符串，则匹配。contains 可以通过case_sensitive 指定
contains
在查询中指定时间区间。Interval中的时间是ISO-8601格式。
"values"：["fragment1",“fragment2"]
如果指定的维度的值的任意部分包含给定的字符串，则匹配。JSON示例如下：
"value"
"case_sensitive":true,
"type":"contains",
Search Query在Filter的 search和 search查询中都会用到。Search Query定义了如下几种
case_sensitive":false,
"type":“fragment",
"type"
"value”:"some_value"
Interval
Search Query
数据查询
:"some_value"
"insensitive_contains",
，对于中国用户，所在时区
129
---
## Page 154
定Context时，
6.2.6
如果查询时需要将最后1秒包含在内，那么endtime最好往后推1秒。
"intervals"
130
Threaded
groupByIsSingle-
maxIntermediateRows
maxResults
minTopNThreshold
chunkPeriod
finalize
bySegment
populateCache
useCache
priority
timeout
字段名
Context 可以在查询中指定一
需要注意的是，这里intervals的时间区间为前闭后开：starttime<=datetime<endtime。
Context
:["2016-08-28T00:00:00+08:00/2016-08-29T00:00:00+08:00"]
则会使用Context中的默认参数。
false
50000
500000
0（off）
true
false
true
true
自动生成
0（未超时）
1000
默认值
一些参数。
是否使用单线程执行GroupBy，默认值在历史节点的配置
指定一些查询参数，如结果是否进缓存
定，查询时该字段的值只能小于配置项的值
配置GroupBy最多能处理的结果集条数，默认值在历史节
到最终的TopN
配置每个Segment返回的TopN的数量用于合并，从而得
指定是否将长时间跨度的查询切分为多个短时间跨度进
为false时，将返回序列化的结果，
是否返回Aggregator的最终结果，例如 HyperUnique，指定
指定为true时，将在返回结果中显示关联的 Segment
节点或历史节点配置的值
此次查询的结果是否缓存，
询节点或历史节点配置的值
此次查询是否利用查询缓存，如果手动指定，则会覆盖查
唯一标识一次查询的id，可以用该id取消查询
查询优先级
查询超时时间，单位是毫秒
描述
。Context并不是查询的必选项，因此在查询中不指
Context支持的字段如下：
Druid实时大数据分析原理与实践
如果手动指定，则会覆盖查询
而不是估算的基数数值
---
## Page 155
会在相应的地方填上。产生的字段如下：
入该广告主的客户库。每次访问都会产生一条记录，如果在访问中点击了“接待组件”，则
转化率的按钮或时间），每次访问和点击该按钮的操作都会被上报。如果发起聊天，就会进
发起聊天的按钮（命名为“接待组件”。在有些需求中可能是“购买”按钮或者是其他标志
6.3
第6章数据查询
new_user_id
click_user_id
user_id
ad_campaign
ad_media
ad_source
is_new
device_type
host
corpuin
timestamp
tid
字段名
"spec":{
案例中所需的DataSchema定义如下：
接下来，我们结合本案例来介绍Druid的查询语法。
系统会为每个访问该URL的用户生成一个user_id。当打开广告页面后，上面会有一个
http://www.mejia.wang/?ad_source=google&ad_campaign=test&ad_media=vedio
首先介绍本案例的相关背景。案例中的广告为一个拼有标识广告标识的URL，如：
本章使用真实案例来介绍如何进行查询，供读者参考。
案例介绍
"dataSchema":{
"dataSource":"visitor_statistics"
如果用户是新用户，则将user_id这里也填上
如果用户点击了接待组件，则将user_id填上
用户id
广告系列
广告媒介
广告来源
该用户是否是新访客
用户访问设备类型：1.PC2.Mobile3.other
域名
广告主id
访问时间
id
描述
13
---
## Page 156
132
"parser":
"metricsSpec":
"granularitySpec":
"parseSpec":
"segmentGranularity":"day"
"queryGranularity”:"day",
"intervals":[
'type":"uniform"
"dimensionsSpec":{
"type":“hyperUnique"
"name":"click_visit_count"
"fieldName";"click_userid"
"type": "hyperUnique"
"name":"new_visit_count""
"fieldName":"new_user_id"
"type":"hyperUnique"
"name":"visit_count""
"fieldName":"user_id"
"type":"count"
"name":
"2016-08-28T00:00:00+08:00/2016-08-29T00:00:00+08:00"
"dimensionExclusions":[],
"dimensions":[
"device_type",
"host",
"corpuin",
"tid"，
"count"
Druid实时大数据分析原理与实践
---
## Page 157
HyperUnique摄入聚合，这样就可以方便、快捷地获取各种行为对应的访客数。
第6章
这里使用 HyperUnique 进行访客数统计，对 user_id、new_user_id 和 click_userid 进行
"type": "index_hadoop"
数据查询
"tuningConfig":{
"ioConfig":[
"type": "hadoop"
"cleanuponFailure":false,
"type": "hadoop"
"inputSpec":
"partitionsSpec":{
"maxRowsInMemory":100000,
"type":"hashed"
"targetPartitionSize":5000000,
"type":"static"
"paths":
"type":"hadoopyString"
"timestampSpec":{
"format": "json",
："hdfs://$[集群地址}/olap/visitor_stat/",
"format":"auto"
"column":"timestamp",
"ad_campaign"
"ad_media",
"ad_source"
"is_new",
133
---
## Page 158
访客数、点击按钮数、新访客比率与点击按钮比率，我们可以用如下查询语句。
Timeseries来完成。例如，对指定客户id和host，统计一段时间内的访问次数、访客数、
6.4
134
"aggregations":[
"filter":{
"granularity":
"dataSource":
"queryType":
对于需要统计一段时间内的汇总数据，或者是指定时间粒度的汇总数据，Druid通过
Timeseries
"fields":[
"type":"and"
"type":“hyperUnique"
"fieldName":"visit_count"
'name":"visitor_count",
"type":"hyperUnique",
"fieldName”:"count"
'name":"pv",
"type":"longSum"
"value":"2852199351"
"dimension":"corpuin"
"type": "selector",
"value": "www.mejia.wang"
"dimension":"host",
"type":"selector"
"visitor_statistics"
"timeseries",
"all"
Druid实时大数据分析原理与实践
新
---
## Page 159
第6章
"postAggregations":
数据查询
"fields":
"fn": "/",
"name": "click_rate",
"type"：
"fields":
"fn": "7",
"name":
"type":
"fieldName":"click_visit_count"
"name": "click_visitor_count",
"type";
"fieldName":"new_visit_count"
"name": "new_visitor_count",
"arithmetic"
："hyperUnique"
"fieldName":"visitor_count"
"type": "hyperUniqueCardinality",
"fieldName":"click_visitor_count"
"type": "hyperUniqueCardinality",
"fieldName":"visitor_count"
"type":“hyperUniqueCardinality",