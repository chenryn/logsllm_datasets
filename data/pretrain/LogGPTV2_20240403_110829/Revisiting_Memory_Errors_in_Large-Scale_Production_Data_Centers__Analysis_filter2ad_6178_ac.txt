with socket failures had the highest number of errors in the
distribution. This large source of errors, if not controlled for, can
confound memory reliability conclusions by artiﬁcially inﬂating
the error rates for memory and creating the appearance of more
DRAM chip-level failures than in reality. Besides the work that
only measured socket and channel failures, but not DRAM chip-
level failures ([45]), we did not ﬁnd mention of controlling for
socket and channel errors in prior work that examined errors
in the ﬁeld (e.g., [44, 16, 47, 48, 10, 27, 28]).
Thus, memory errors are not always isolated events, and
correcting errors in hardware and handling MCEs in the sys-
tem software (as current architectures do) can easily cause
a machine to become unresponsive. We suspect that simple
hardware changes such as caching error events and having
system software poll the contents of the error cache once in a
while, instead of always invoking the system software on each
error detection, could greatly reduce the potential availability
impact of socket and channel failures. In addition, the DDR4
standard [19] will allow memory accesses to be retried by the
memory controller (by using a cyclic redundancy check on the
command/address bits and asserting an “alert” signal when an
error is detected) without interrupting the operating system,
which can help reduce the system-level unavailability resulting
from socket and channel failures.
Bank failures occur relatively frequently, on 14.08% of
servers with errors each month. We observe a larger failure
rate for banks than prior work that examined DRAM chip-
We observe that DRAM chip-level (banks, rows, columns,
cells, and spurious) failures contribute a relatively small number
of errors compared to sockets and channels: 6.06%, 0.02%,
0.20%, 0.93%, and 7.80%, respectively. This is because when
these components fail, they affect only a relatively small amount
of memory. Based on these ﬁndings, to help with the diagnosis
of memory failures, we recommend that memory error classiﬁ-
cation should always include components such as sockets and
channels.
So far, we have examined how component failures are
related to the number of errors reported. We next turn to how
component failures themselves (the underlying source of errors)
are distributed among servers. Figure 4 shows what fraction of
servers with correctable errors each month have each type of
failure that we examine. We plot error bars for the standard
deviation in fraction of servers that report each type of error
between months, though we ﬁnd that the trends are remarkably
stable, and the standard deviation is correspondingly very low
(barely visible in Figure 4).
Notice that though socket and channel failures account for
a large fraction of errors (Figure 3), they occur on only a small
fraction of servers with errors each month: 1.34% and 1.10%,
respectively (Figure 4). This helps explain why servers that
have socket failures often appear unresponsive in the repair
logs that we examined. Socket failures bombard a server with
a large ﬂood of MCEs that must be handled by the operating
system, essentially creating a kind of denial of service attack on
the server. Systems that have these type of failures have been
observed to appear unresponsive for minutes at a time while
correcting errors and handling MCEs. We believe that context
switching to the operating system kernel to handle the MCE
contributes largely to the unresponsiveness.
level failures on Google servers, which found 2.02% of banks
failed over the course of their study (Table 2 in [16]4). One
reason for this difference could be the different composition
of the servers evaluated. For example, while the prior work
examined older DDR and DDR2 DIMMs from over ﬁve years
ago, we examine newer DIMMs that use the DDR3 protocol.
The relatively large occurrence of bank failures suggests that
devices that support single chip failures (e.g., Chipkill [11]) can
provide additional protection to help ensure that such failures
do not lead to uncorrectable errors.
We ﬁnd that row and column failures are relatively infre-
quent, occurring in 0.92% and 0.99% of servers each month.
Prior work on Google servers found much larger rate of row
(7.4%) and column (14.5%) failures [16]. We believe that the
much larger estimate in prior work could potentially be due
to the confounding effects of socket and channel errors. Such
errors, if present and unaccounted for, can artiﬁcially increase
the number of row and column errors (e.g., the socket and
channel errors in Figure 3 may end up being misidentiﬁed as
other types of errors).
We observe that a relatively large fraction of servers experi-
ence cell failures, 25.54%. Similar to row and column failures,
prior work found a much larger amount of cell failures, 46.1%.
As with rows an columns, this could also potentially be due
to unaccounted-for socket and channel failures increasing the
perceived number of cell failures. The prevalence of this type of
failure prompted the prior work to examine the effectiveness of
page ofﬂining, where the operating system (OS) removes pages
that contain failed cells from the physical address space. While
the prior study evaluated page ofﬂining in simulation using the
same memory traces from their evaluation, we evaluate page
ofﬂining on a fraction (12,276) of the servers we examine in
Section VI and ﬁnd it to be less effective than reported in prior
work ([16]).
While prior work, which may not have controlled for socket
and channel failures, found repeat cell errors to be the dominant
type of failure (e.g., [16, 48, 10]); when controlling for socket
and channel failures (by identifying and separately accounting
for the errors associated with them), we ﬁnd spurious failures
occur the most frequently, across 56.03% of servers with errors.
Such errors can be caused by random DRAM-external events
such as alpha particle strikes from the atmosphere or chip
packaging [34] and DRAM-internal effects such as cells with
weak or variable charge retention times [30, 21, 20]. This is
signiﬁcant because, as we mentioned before and as we will
show in Section VI, spurious failures can limit the effectiveness
of the page-ofﬂining technique. To deal with these type of
failures, more effective techniques for detecting and reducing
the reliability impact of weak cells are required (some potential
options are discussed in [21, 20]).
IV. THE ROLE OF SYSTEM FACTORS
We next examine how various system factors are correlated
with the occurrence of failures in the systems we examine.
For this detailed analysis, we examine systems that failed
over a span of three months from 7/13 to 9/13. We focus
on understanding DRAM failures and excluded systems with
socket and channel failures from our study. We examine the
effects of DRAM density and DIMM capacity, DIMM vendor,
DIMM architecture, age, and workload characteristics on failure
rate.
A. DIMM Capacity and DRAM Density
DRAM density is measured in the number of bits per
chip and is closely related to the DRAM cell technology and
manufacturing process technology [20]. As DRAM cell and
4Other studies (e.g., [47, 48]) have similar ﬁndings. For brevity, we
compare against [16].
fabrication technology improves, devices with higher densities
can be manufactured. The most widely-available chip density
currently is 4 Gb as of 2014, with 8 Gb chips gaining adoption.
DRAM density is different from DIMM capacity. A DIMM
of a certain capacity could be composed in multiple ways
depending on the density and transfer width of its chips. For
example, a 4 GB capacity DIMM could have 16 (cid:2) 2 Gb chips
or 8 (cid:2) 4 Gb chips. Prior work examined DIMM capacity when
drawing conclusions [44, 40], and observed trends that were,
in the authors’ own words, either “not consistent” [44] or a
“weak correlation” [40] with error rate. This led the prominent
Schroeder et al. work to conclude that “unlike commonly
feared, we don’t observe any indication that newer generations
of DIMMs have worse error behavior.” Our results with DRAM
density stand to refute this claim as we explain below.
Similar to these works, we also ﬁnd that the error trends
with respect to DIMM capacity are not consistent. Figure 5
shows how the different capacities of DIMMs we examine are
related to device failure rate.5 The large error bars for 16 GB
and 24 GB DIMMs are due to the relatively small number of
DIMMs of those types. Notice that there is no consistent trend
across DIMM capacities.
e
t
a
r
e
r
u
l
i
a
f
r
e
v
r
e
s
e
v
i
t
a
e
R
l
0
0
.
1
0
5
.
0
●
●
●
●
0
0
.
0
●
e
t
a
r
e
r
u
l
i
a
f
r
e
v
r
e
s
e
v
i
t
a
e
R
l
0
0
.
1
0
5
.
0
0
0
.
0
2
8
16
24
●
2
●
1
●
4
DIMM capacity (GB)
Chip density (Gb)
Fig. 5: The relative failure rate
for servers with different DIMM
capacities. Similar to prior work,
we ﬁnd no consistent reliability
trend.
Fig. 6: The relative failure rate
for servers with different chip den-
sities. Newer densities (related to
newer technology nodes) show a
trend of higher failure rates.
In contrast to prior works [44, 40], we do observe indication
that newer generations of DRAM chips have worse error
behavior by examining failure rate as a function of DRAM
chip density. The servers we analyzed contained three different
types of DRAM chip densities: 1 Gb, 2 Gb, and 4 Gb. Figure 6
shows how different DRAM chip densities are related to device
failure rate. We can see that there is a clear trend of increasing
failure rate with increasing chip density, with 2 Gb devices
having 2:4(cid:2) higher failure rates than 1 Gb devices and 4 Gb
devices having 1:8(cid:2) higher failure rates than 2 Gb devices. This
is troubling because it indicates that business-as-usual practices
in DRAM design will likely lead to increased memory failure
rates in the future, as predicted by both industry [20] and
academia [21, 23, 38, 39] in recent works. To understand the
source of this trend, we next examine the failure rate for DRAM
cells.
Figure 7 shows the cell failure rate computed by normal-
izing the failure rates in Figure 6 by the number of cells in
each chip. Interestingly, cell failure rate had a brief increase
going from 1 Gb chips to 2 Gb chips but a recent decrease going
from 2 Gb chips to 4 Gb chips. This shows that the reliability
of individual DRAM cells may be improving recently. This is
5Recall from Section II-E that we examine relative server failure rates
compared to the sampled control group. Though relative failure rates happen
to be in the range Œ0; 1, they should not be confused with absolute failure
rates across the ﬂeet.
420420
likely due to the large amounts of effort that DRAM manufac-
turers put into designing faster and more reliable DRAM cell
architectures. Our insight is that small improvements in DRAM
cell reliability are easily outpaced by the quadratic increase in
number of cells per chip, leading to the trend of net decrease
in DRAM reliability as shown by the server failure rate data in
Figure 6. Unless more-than–quadratic improvements in DRAM
cell reliability are achieved in future devices, maintaining or
decreasing DRAM server failure rates in the future (while still
increasing DRAM chip capacity) will be untenable without
stronger hardware and/or software error correction.
●
0
0
.
1
0
5
.
0
0
0
.
0
e
t
a
r
e
r
u
l
i
a
f
r
e
v
r
e
s
e
v
i
t
a
e
R
l
1 Gb
2 Gb
4 Gb
●
1 Gb
2 Gb
4 Gb
e
t
a
r
e
r
u
l
i
a
f
r
e
v
r
e
s
e
v
i
t
a
e
R
l
0
0
.
1
0
5
.
0
0
0
.
0
x8 x4
●
8
16
●
32
48
●
4
●
8
e
t
a
r
e
r
u
l
i
a
f
l
l
e
c
e
v
i
t
a
e
R
l
3
1
−
e
0
.
3
3
1
−
e
5
.
1
0
0