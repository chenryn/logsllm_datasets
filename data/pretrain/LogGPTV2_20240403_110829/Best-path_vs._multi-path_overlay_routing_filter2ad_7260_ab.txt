probes:
(cid:15) Direct: A single packet using the direct Internet path.
(cid:15) Loss: Probe-based reactive routing that attempts to minimize
loss. Requires only probing overhead.
1Data is available at http://nms.lcs.mit.edu/ron/
Category
US Universities
US Large ISP
US small/med ISP
US Private Company
US Cable/DSL
Canada Private Company
Int’l Universities
Int’l ISP
#
7
4
5
5
3
1
3
2
Table 2: Distribution of the 30 testbed nodes.
Description
ISP
ISP
1Mbps DSL
.com
.edu
ISP
.edu
ISP
ISP
ISP
ISP
ISP
ISP
ISP
.com
.edu
.edu
AT&T
.com
.edu in lab
.edu data center
RoadRunner
ISP
.edu
.com
Small ISP
.edu
.edu
ISP
Dataset
Raw
Rwide
R2003
Samples
4,763,082
2,875,431
32,602,776
Dates
8 Jul 2002 – 11 Jul 2002
3 Jul 2002 – 8 Jul 2002
30 Apr 2003 – 14 May 2003
Table 3: The three datasets used in our experiments. The
Raw dataset contains one-way samples for three rout-
ing methods. The Rwide dataset has round-trip samples for
eleven methods. The R2003 dataset uses a larger number of
probing hosts to measure six routing methods.
loss
lat
direct
rand
loss optimized path (via probing)
latency optimized path (via probing)
direct Internet path
indirectly through a random node
Table 4: The types of routes between measurement nodes.
Probes consisted of one or more packets of these types, such as
direct rand (one packet directly, one via a random intermediate
node).
(cid:15) Lat: Probe-based reactive routing that minimizes latency and
avoids completely failed links.
(cid:15) Direct rand: 2-redundant mesh routing, with no probing
overhead. One copy of each packet is transmitted on the di-
rect Internet path; the second over a random indirect overlay
path. There is no delay between the packet transmissions. We
use the ﬁrst packet to predict the behavior of direct packets.
(cid:15) Lat loss: Probe-based 2-redundant multi-path routing. In the-
ory, this combination should be able to achieve the best of
both worlds. It sends the ﬁrst copy of each packet over a path
selected to minimize loss, and the second over a path selected
to minimize latency. We also use this to infer the lat packet.
(cid:15) Direct direct: 2-redundant routing with back-to-back packets
on the same path.
(cid:15) DD 10 ms: 2-redundant routing with a 10ms gap between
packets on the same path.
(cid:15) DD 20 ms: As above, with a 20ms gap.
We present four major ﬁndings. First, losses on alternate paths are
often not independent. If a packet sent directly from a source to a
destination is lost, the chances are over 60% that a packet sent from
that source to that destination via a random intermediate will also be
lost. Second, the average Internet loss rate is quite low (0.42%) but
individual paths at speciﬁc times have quite high loss rates. Third,
mesh routing improves latency in two ways. Mesh routing is able
to reduce the loss rate from 0.42% to 0.26%, which reduces retrans-
mission and timeout latency. The overall packet latency is reduced
by an average of 3 ms, but on 2% of the paths, mesh routing pro-
vides an average latency reduction of over 20 ms. Finally, path
selection improves mesh routing. Paths with a greater degree of
independence than the random paths used by mesh routing exist.
Using probe based routing reduces the conditional loss probability
to 55% for the second packet, suggesting that better path selection
methods can improve the performance of mesh routing.
4.1 Method
Each node periodically initiates probes to other nodes. A probe
consists of one or two request packets from the initiator to the tar-
get. The nodes cycle through the different probe types, and for
each probe, they pick a random destination node. After sending the
probe, the host waits for a random amount of time between 0.6 and
1.2 seconds, and then repeats the process.
Each probe has a random 64-bit identiﬁer, which the hosts log along
with the time at which packets were both sent and received. This
allows us to compute the one-way reachability between the hosts.
Most, but not all, hosts have GPS-synchronized clocks. We aver-
age one-way latency summaries and differences with those on the
reverse path to average out timekeeping errors. Each probing host
periodically pushes its logs to a central monitoring machine, where
this data is aggregated. Our post-processing ﬁnds all probes that
were received within 1 hour of when they were sent. We consider a
host to have failed if it stops sending probes for more than 90 sec-
onds, and we disregard probes lost due to host failure; our numbers
only reﬂect failures that affected the network, while leaving hosts
running. It is possible that we slightly under count outages caused
by power failures or other events that affect both host and network.
4.2 Base network statistics
In contrast to earlier studies, the paths we measured had relatively
low loss rates. Our paths’ average loss rates ranged from 0% on
many Internet2 or otherwise very fast connections, to about 6% be-
tween Korea and a DSL line in the United States. Figure 2 shows
the distribution of average loss rates (over several days) on a per-
path basis. The overall loss rate we observed on directly-sent sin-
gle packets in 2003 was 0.42%, reduced from an earlier 0.74%.
These changes may reﬂect topological changes to our testbed (it
grew from 17 to 30 nodes, and some original nodes left or moved)
as well as changes in the underlying loss rates, but the reduction
in loss is still noteworthy. All of these loss rates are substantially
lower than those observed in 1997 by Paxson.
Most of the time, the 20-minute average loss rates were close to
zero; the direct line in Figure 3 shows the distribution of 20-
minute loss samples. Over 95% of the samples had a 0% loss rate.
The sampling granularity for the CDF is relatively coarse, so it
groups low loss-rate conditions into the zero percent bin. During
many hours of the day, the Internet is mostly quiescent and loss
rates are low. During the worst one-hour period we monitored, the
average loss rate on our testbed was over 13%.
4.3 Effects on loss rate
Table 5 examines the overall loss and latency results. Comparing
the overall loss percentage (totlp) columns, we see that using probe
data to pick better paths can reduce loss from 0.42% to 0.33%, with
an almost insigniﬁcant effect on latency. Using random mesh rout-
ing can reduce the loss rate by almost 40%, and reduce the latency
by a few milliseconds.
Sending two packets back to back on the same path results in
loss improvements nearly as good as random mesh routing, es-
pecially if the two packets are delayed by 10 milliseconds (the
totlp column in Table 5). Using path selection in conjunction with
mesh routing results in a further improvement. These results sug-
gest that the two methods are taking advantage of different situ-
ations: Mesh routing’s packet redundancy is effective at masking
s
h
t
a
p
f
o
n
o
i
t
c
a
r
f
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 0
2003 dataset
2002 dataset
 1
 2
 3
 4
 5
 6
 7
average path-wide loss rate (%)
Figure 2: Cumulative distribution of long-term loss rates, on a
per-path basis. 80% of the paths we measured have an average
loss rate less than 1%.
l
s
e
p
m
a
s
f
o
n
o
i
t
c
a
r
f
 1
 0.995
 0.99
 0.985
 0.98
 0.975
direct
loss
direct direct
direct rand
lat loss
dd 10
dd 20
 0
 0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9
 1
loss rate
Figure 3: Cumulative distribution of 20-minute loss rates, on a
per-path basis.
transient congestion-triggered loss, possibly by de-correlating the
losses through temporal shifting.
In contrast, probe-based reac-
tive routing avoids paths with longer-term pathologies. As would
be expected, these two approaches can be combined—with high
overhead—to get the best of both worlds, reducing losses by 45%
and reducing latency by 13% (the lat loss row in Table 5).
Figure 3 shows the distribution of the 20-minute loss rates, and con-
ﬁrms the above conclusions. The loss avoidance routing is less ef-
fective at eliminating periods of small loss rates, but successfully
avoids as many or more of the periods where the loss rate is high
and sustained.
Our data suggests that most of the improvement in loss rates oc-
curs during periods of elevated loss. Loss rates on the Internet are
usually low (as is bandwidth utilization). 30% of the time during
our monitoring, the overall loss rate between our nodes is less than
0.1%, and 68% of the time it is less than 0.2%. During the remain-
ing periods, the average loss rate ranges up to 13%.
Simple Redundancy
Reactive
Loss % direct
8817
> 0
1999
> 10
962
> 20
> 30
630
486
> 40
379
> 50
255
> 60
130
> 70
> 80
74
31
> 90
direct direct
5183
1361
799
585
480
377
251
130
73
31
dd 10ms
4024
1291
796
591
481
367
245
130
65
37
dd 20 ms
3832
1275
783
575
465
359
249
128
64
30
lat
10695
1716
849
604
484
363
231
118
57
16
loss
7066
1362
791
573
468
359
219
106
59
31
Mesh
direct rand
3846
1236
793
579
468
369
235
125
60
28
Both