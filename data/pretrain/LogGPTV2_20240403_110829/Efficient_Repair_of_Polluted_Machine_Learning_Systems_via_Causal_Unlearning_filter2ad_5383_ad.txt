are partially due to the fact that the original dataset contain some
noisy data. Such noisy data can be classified into two categories.
First, if one unlearns some unpolluted, noisy data directly from the
training set, the detection accuracy also increases. Second, some
noisy data might not have impacts on the detection accuracy of the
learning model due to its small size. However, when these noisy
data is combined with a large, polluted data cluster, the combination
may have impacts of the detection accuracy. As evident by our
final detection accuracy, to add or remove such noisy data has very
Figure 2: The Detection Accuracy of Learning Models Polluted
by Mislabelling Attacks when Applying KARMA (The horizon-
tal line at the end of each curve indicates the vanilla accuracy
for that dataset).
little impacts on the learning model. In the future, as discussed in
Section 7, such noisy data, just like active learning, may guide the
learning algorithm to query the administrator or other oracles to
label new similar data samples to further improve the model.
6.4 Effectiveness in Accuracy Repair
In this section, we show the effectiveness of KARMA in the metrics
of detection accuracy.
Mislabelling Attacks. We first evaluate KARMA against misla-
belling attacks. Figure 2 shows the detection accuracy when clusters
are unlearned from learning models polluted by three mislabelling
attacks. The x-axis is the number of clusters unlearned, and the y-
axis is the detection accuracy. Due to the space limit in the figure,
we select three representative attacks: one with blind mislabelling at-
tacker, and two with targeted mislabelling attacker (ham as spam and
spam as ham respectively). The ratio of malicious workers is 15%
for all three attacks. Note that the results for the rest mislabelling
attacks are similar to the ones shown in the figure: The number of
clusters unlearned may vary a little, but the trend and final accuracy
differences are very close.
In Figure 2, we use a horizontal line at the end of each curve to
indicate the vanilla accuracy for that dataset. These vanilla accuracy
values are 92.02% (blind mislabelling), 93.03% (ham as spam), and
92.93% (spam as ham) from the left to the right for the horizontal
lines in the figure. The final accuracies after KARMA are very close
to the vanilla ones, within ±0.9%. Note that some of the final accura-
cies are higher than the vanilla, because the vanilla dataset without
pollution also contains some errors. If we perform KARMA upon the
vanilla dataset, the accuracy can also increase by 0.1%–0.2%.
Dictionary Attacks. In this subsection, we evaluate KARMA against
the dictionary attacks. Figure 3 shows the detection accuracy over
the iteration when clusters are unlearned. The x-axis is the number
of clusters unlearned, and the y-axis is the detection accuracy. The
vanilla detection accuracy is 94.825% in our experiment. Similar
to mislabelling attacks, the final accuracies after KARMA are close
to the vanilla one, within ±0.3%. We only include the results of
five datasets out of fifty because of the space limit of the graph.
The parameters for the five dictionary attacks are as follows: NC is
 20 30 40 50 60 70 80 90 100 0 5 10 15 20 25 30 35Detection Accuracy (%)Number of Clusters UnlearnedBlind MislabellingTargeted Mislabelling (spam->ham)Targeted Mislabelling (ham->spam)Causal Unlearning
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
as the oracle set and repair the learning model. If users still report
misclassifications for the repaired learning model and the misclas-
sifications are verified, the administrator can form a new oracle set
and further repair the model. The feedback loop can be repeated for
many times.
To prove this, we first apply KARMA with Sor acle,1/10, and repair
the learning model. Then, we obtain another Sor acle,1/10 based on
the current misclassifications, and repair the learning model again.
When we repeat the process for three times, the true positive arises
to 98.2%, the same as the original with Sor acle .
Second, we adopt a new oracle set that only contains all the mis-
classified emails but no correctly classified emails. All the dictionary
and mislabeled attacks are used to test the effectiveness of KARMA
under this circumstance. The true positives are between 97.4% and
99.2%, and the true negatives are between 79.3% and 85.7%. It is
worth noting that when we only include misclassified emails, the
true positives stay the same, but the true negatives drop a little. The
reason is as follows: KARMA does not know anything about correctly
classified emails, and therefore may make some mistakes. Therefore,
in order to maintain both true positive and true negative, we recom-
mend that the administrator forms Sor acle using both misclassified
and some correctly classified emails.
6.6 Performance Overhead
In this part of the section, we evaluate the performance of KARMA.
Let us first take a look at the theoretical value. In each iteration, the
time spent on the seed finder is a linear function with regards to
the size of misclassified data. The time spent on the peak finder –
which may be invoked many times – is with regards to the size of
training data, because each cluster, once decided, is deducted from
the working set. The time spent on the unlearning module is with
regards to the number of clusters. The number of clusters is relatively
small compared to the training data size, and the size of misclassified
data is smaller than the size of oracle set, and thus much smaller than
the size of training data. Therefore, the time complexity of KARMA
in each iteration is O(N), where N is the size of training dataset,
and the overall time complexity is O(kN), where k is the number of
iterations.
Our empirical evaluation show that the median overhead of per-
formance is 6.21 times of the training time with the maximum as
34.1 and the minimum as 3.81. It is worth noting that KARMA can
be incrementally deployed, i.e., any partial outputs can be used by
the administrator to repair the polluted learning model. For example,
if the administrator sets a satisfactory detection accuracy as 90%, the
maximum performance overhead is only 15.1 times of the training
time.
Note that although the performance of KARMA, an offline analysis
tool, is not ideal, KARMA is faster than the most naïve method that
searches the training set by brute force. The performance overhead
is exponential, because the method needs to include all the possible
subsets of the training set. As a comparison, our KARMA brings
down the performance overhead from exponential to linear in the
number of training points.
Figure 3: The Detection Accuracy of Learning Models Polluted
by Dictionary Attacks when Applying KARMA (NC means the
number of clusters injected in dictionary attacks, maximum de-
viation D% equals 30% for all five datasets, and both cluster
size (CS) and number of words (NW ) vary for different clusters
in each dataset).
marked in the figure, D% is 30% for all five, and both CS and NW
are selected randomly for each cluster in the datasets. The results for
the rest attacks are very similar to the ones depicted in Figure 3, and
our observation below also applies to the rest.
First, the number of clusters (NC) in each dictionary attack
roughly matches with but does not equal to the number of clus-
ters unlearned. In some cases when two separate injected clusters
are close enough, KARMA may consider them as one; at contrast,
due to the existence of parameter D, one cluster may be considered
as two separate clusters in KARMA.
Second, one major difference of KARMA between mislabelling
and dictionary attacks is the number of cluster unlearned (the x-axis
in Figure 2 and 3). The number of clusters unlearned for mislabelling
attacks is larger than the one for dictionary on average. The reason
is that dictionary attacks are launched with clusters (and some varia-
tions within each cluster) as opposed to mislabelling attacks, which
flip over the labels of emails randomly assigned by an administrator.
That is, the KARMA algorithm needs to find and form clusters when
finding polluted samples for mislabelling attacks.
6.5 Effects of Oracle Sets
In this section, we are going to evaluate how oracle sets affect
KARMA. We evaluate KARMA using two types of oracle sets: one
with less misclassified emails and one with only misclassified emails.
First, we randomly pick a mislabelling attack, and then reduce the
number of misclassified emails in the oracle set to 1/2 (Sor acle,1/2),
1/3 (Sor acle,1/3) and 1/10 (Sor acle,1/10). Both true positive and neg-
ative are measured to evaluate the effect of these oracle sets. The
original true positive is 98.2%, and true negative 90%. The true
positive drops to 98% for Sor acle,1/2, 96.9% for Sor acle,1/3, and
then 83.2% for Sor acle,1/10. The true negative stays the same when
the number of misclassified reduces.
This experiment tells us that KARMA requires that the oracle set
contains some amount of misclassified samples to make KARMA
effective. Now let us answer how to determine the size of Sor acle .
The administrator can first use a small number of misclassifications
 65 70 75 80 85 90 95 100 0 2 4 6 8 10 12Vanilla Accuracy: 94.825%Detection Accuracy (%)Number of Clusters UnlearnedNC: 4NC: 5NC: 10NC: 14NC: 15ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
Y. Cao et al.
6.7 Effects of Divergence Score Definition
In this part of the section, we study the effect of the divergence
score definition upon KARMA. As mentioned in Section 4.2.1, any
non-negative and symmetric definition of divergence score – which
represents similar contributions of samples made to a learning model
– can be used in KARMA. Thus, we compare three definitions: the
one defined in Section 4.2.1 (D1), Euclidean distance, and another
definition (D2) taking account into the feature frequency defined in
Appendix A.
Now let take a look at how D1, D2, and Euclidean distance affect
KARMA. We first use Euclidean distance, which performs the worst
among the three. In many cases, KARMA with Euclidean distance
does not converge, i.e., the final detection accuracy cannot reach
the vanilla level. The reason is that when computing the Euclidean
distance between two samples, we not only consider the common fea-
tures between these two, but also the different features. For example,
if two samples share many common features, but also have plenty of
different features, the Euclidean distance between these two samples
is large, contradicting with the fact that these samples make similar
contributions to learning model in their common features.
Next, we use D2 in KARMA. Unlike Euclidean distance, KARMA
with D2 converges, and the final detection accuracy is within ±1% of
the vanilla accuracy. The major difference between KARMA with D1
and D2 lies in the true positives and negatives. Evaluated against the
dictionary and mislabelling attacks, KARMA with D2 has a median
true negative as 92.3% ranging from 90.2% to 96.5%, and a median
true positive also as 92.3% ranging from 88.0% to 95.2%. That is, the
true negative of D2 is higher than the one of D1, but the true positive
is lower. Like all other researches, there is a trade-off between the
true positives and negatives in KARMA: When we try to boost one,
the other is lowered correspondingly.
To sum up, for all three definitions, D1 performs the best and Eu-
clidean distance performs the worst. This order aligns with how these
definitions deal with common and different features among samples,
especially different ones. D1 only considers common features but ig-
nore different ones all the time. D2 only considers common features
in the calculation of divergence scores, but when the cluster grows,
different features are included in the feature list. Euclidean distance
considers both common and different features among samples in the
calculation and the cluster growing stage.
Integrating Other Learning Systems
6.8
In this section, we evaluate two other learning systems to show
the generality of KARMA. The first system is a SVM-based spam
detector, showing that KARMA works with other learning algorithms,
and the second is a Bayes-based JavaScript malware detector (in
Appendix B), showing that KARMA works with other application
scenarios.
We show how to integrate KARMA with another support vector
machine (SVM) based spam filter. Because we cannot find an open-
source filter written in Python, we implement a version of a spam
filter with approximately 3000 lines of Python code by following
what has described in an online machine learning course [1, 4] taught
in Stanford University. Here are some details of the spam filter that
we implemented. The SVM library that we adopt is LIBLINEAR [3,
21], a popular open source tool that supports linear support vector
machines. The training phase of our spam filter can be divided as
three steps: preprocessing, feature extraction, and training. In the
preprocessing, we normalize each email by nine tactics, such as
removing non-words and replacing $ with ‘dollar’ (Details can be
found in the online course). Then, we extract features based on
1,900 spam words found commonly in SpamAssassin dataset [5],
and train the LIBLINEAR classifier. Our spam filter is consisted of
˜3,000 lines of Python code, and achieves 95.88% vanilla detection
accuracy.
Next, we integrate the KARMA with this SVM-based spam filter
(LIBLINEAR supports unlearning, called decremental learning), and
evaluate KARMA with polluted datasets. The results are very similar
to the one with SpamBayes and show that KARMA can successfully
restore the detection accuracy to the vanilla value with less than 1%
difference.
7 DISCUSSION
We are now discussing several important problems of KARMA.
Robustness of KARMA to Attacks. We are discussing the robust-
ness of KARMA to attacks in adversarial environment. There are
two cases to discuss: (i) attacks during the repairing process, and (ii)
attacks after the repairing process.
First, KARMA is robust to attacks during the repairing, because
there exists an oracle-in-the-loop feedback that can enhance the
security and mitigate potential attacks. Then, KARMA performs a
complete though prioritized search over the training set, i.e., KARMA
goes over every training sample in the search.
Second, KARMA is robust to attacks after the repairing, because
the administrator will block the data pollution attacks from the
same source. For example, the administrator can block the turk who
intentionally mislabels data in the training set and only adopt results
from trusted turks. We understand that attackers may compromise
new turks, but this at least reduces the attack surface.
False Positives and Negatives. In addition to the false positives and
negatives of KARMA, we believe that the oracle-in-the-loop feedback
can also help to improve false positives and negatives. Specifically,
the administrator will mitigate false positives by manually inspecting
the samples to unlearn and the users will report false negatives if the
model is not fully repaired.
Effects of KARMA on Unpolluted Model. If the learning model
is unpolluted, KARMA might still help to improve the model. Say,
for example, if the misclassification is caused by other reasons,
such as lack of data, the cause found by KARMA may help the
administrator or the developer to introduce new samples that can
clearly distinguish the misclassified and its cause. This, however, is
considered as beyond scope of the paper, and one may refer to the
literature [25, 32, 38] on introducing new samples.
Overfitting and Underfitting. The general problems of overfitting
and underfitting are orthogonal to the paper, and one may refer to
the literature [16, 40] for the problem. To the best of our knowledge,
KARMA does not cause additional overfitting or underfitting issues,
which we will discuss from two aspects: explanation and empirical
evaluation.
First, as shown in our deployment model, before an administra-
tor repairs the learning model with the misclassification cause, the
administrator will verify that samples found by KARMA are indeed
Causal Unlearning
ASIA CCS ’18, June 4–8, 2018, Incheon, Republic of Korea
polluted. That is, the administrator will preserve correctly-labeled
samples to avoid overfitting and underfitting.
Second, our restored detection accuracy—without any verification
from the administrator—are very high against an independent dataset
showing no underfitting or overfitting. If there is either underfitting
or overfitting, the detection accuracy against the independent dataset
will be much lower. That is, in empirical evaluation, we do not
observe any overfitting or underfitting.
8 RELATED WORK
In this section, we will discuss related work. We start from a general
problem, adversarial machine learning, in Section 8.1, and then
discuss existing defenses to data pollution attacks in Section 8.2.
After that, we discuss other similar techniques in Section 8.3.
8.1 Adversarial Machine Learning
The problem that KARMA solves, i.e., the data pollution, belongs to
a broad research topic, called adversarial machine learning [8, 26].
Adversarial machine learning defines the behavior of machine learn-
ing models under the existence of adversaries. In particular, prior
works classify such attacks into two major categories: causative at-
tacks where an attacker has access to the training set, and exploratory
attacks where an attacker can only craft samples to probe or explore
the learning model.
8.1.1 Causative Attacks. There are many causative attacks,
or called data pollution defined in the literature. Perdisci et al. [35]
attacks PolyGraph [34] by injecting well-crafted invariants and mis-
leading the signature generation. Particularly, the attacker sends the
crafted traffic to a honeypot collecting worm traffic and such traffic
will be picked up by automatic worm signature generation tools,
such as PolyGraph. Similarly, Nelson et al. [33] pollute SpamBayes
by injecting emails with dictionary words. According to them, only