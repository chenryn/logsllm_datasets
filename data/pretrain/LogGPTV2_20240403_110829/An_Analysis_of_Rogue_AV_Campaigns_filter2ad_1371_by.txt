plore the nature of these interactions in greater detail when we look at the
parameter estimates.
The third line of model equation (3) includes a per-subject random-eﬀect (S)
along with the residual error term (). From the presence of this per-subject term
in the model, we conclude that there is substantial typist-to-typist variation.
Some typists are easier to distinguish from impostors than others.
Parameter estimates. Table 1 compiles the REML estimates of the parame-
ters. Part (a) provides estimates of all the ﬁxed eﬀects, while Part (b) provides
estimates of the random eﬀects. The table is admittedly complex, but we include
it for completeness. It contains all the information necessary to make predictions
and to approximate the uncertainty of those predictions.
In Part (a), row 1 is the estimate of μ, the equal-error rate for the baseline
combination of factor values. The estimate, 17.6%, is the equal-error rate we
would predict when operating the Manhattan (scaled) detector with 5 repetitions
of training, no updating, and unpracticed impostors.
Rows 2–26 describe how our prediction should change when we depart from
the baseline factor values. Each row lists one or more departures along with
the estimated change in the predicted equal-error rate. To predict the eﬀect
of a change from the baseline values to new values, one would ﬁrst identify
every row in which the new values are listed. Then, one would add the change
estimates from each of those rows to the baseline to get a new predicted equal-
error rate.
For example, we can predict the equal-error rate for the Nearest Neighbor (Ma-
halanobis) detector with 200 training repetitions, no updating, and unpracticed
impostors. The detector and training amount depart from the baseline conﬁgu-
ration. Row 3 lists the eﬀect of switching to Nearest Neighbor (Mahalanobis) as
+19.2. Row 6 lists the eﬀect of increasing to 200 training repetitions as −7.2.
Row 15 lists the eﬀect of doing both as −18.8 (i.e., an interaction eﬀect). Since
the baseline equal-error rate is 17.6%, we would predict the equal-error rate of
this new setup to be 10.8% (17.6 + 19.2 − 7.2 − 18.8).
As another example, we can quantify the eﬀect of impostor practice. If no
updating strategy is used, according to row 8, impostor practice is predicted to
add +1.1 percentage points to a detector’s equal-error rate. If a sliding-window
updating strategy is used, according to rows 7, 8, and 20, impostor practice
is predicted to add +0.7 percentage points (−2.2 + 1.1 + 1.8). Consequently,
impostor practice only increases an impostor’s chance of success by a percentage
point, and the risk is somewhat mitigated by updating.
While the aim of the statistical analysis is to predict what eﬀect each factor
has, it would be natural for a practitioner to use these predictions to choose
the combination of factor values that give the lowest error. A systematic search
of Table 1(a) reveals the lowest predicted equal-error rate to be 7.2%, using
the Manhattan (scaled) detector with 100 training repetitions, sliding-window
updating, and unpracticed impostors. For very-practiced impostors, the lowest
Why Did My Detector Do That?!
269
Table 1. Estimated values for all of the parameters in the model. The estimates
are all in percentage points. Part (a) presents the ﬁxed-eﬀects estimates. The ﬁrst
row lists the combination of factor values which comprise the baseline along with
the predicted equal-error rate. The remaining 25 rows list changes to the algorithm,
amount of training, updating, and impostor practice, along with the predicted change
to the equal-error rate. Part (b) presents the estimated typist-to-typist variation and
the residual error. Both estimates are expressed as standard deviations rather than
variances (i.e., by taking the square-root of the variance estimates) to make them
easier to interpret. This table enables us to predict the error rates of a detector under
diﬀerent operating conditions, and also to estimate the uncertainty of those predictions.
Algorithm
Manhattan (scaled)
Outlier-count (z-score)
Nearest-neighbor (Mahalanobis)
None
Outlier-count (z-score)
50 reps
Nearest-neighbor (Mahalanobis) 50 reps
Outlier-count (z-score)
100 reps
Nearest-neighbor (Mahalanobis) 100 reps
Outlier-count (z-score)
200 reps
Nearest-neighbor (Mahalanobis) 200 reps
Nearest-neighbor (Mahalanobis)
1 μ
2 Ah
3
4 Ti
5
6
7 Uk
8 Il
9 AThi
10
11
12
13
14
15 AUhk Outlier-count (z-score)
16
17 T Uik
18
19
20 U Ikl
21 AT Uhik Outlier-count (z-score)
22
23
24
25
26
Sliding
Very High
50 reps
100 reps
200 reps
Training Updating Impostor Estimates
17.6
5 reps None
+2.7
+19.2
−4.9
−6.9
−7.2
−2.2
+1.1
−1.7
−17.0
−1.3
−17.9
−1.2
−18.8
−3.7
−7.7
−0.8
−1.3
+0.3
+1.8
+2.3
+8.1
+4.0
+7.9
+3.0
+8.4
Sliding
Sliding
50 reps Sliding
100 reps Sliding
200 reps Sliding
Sliding Very High
50 reps Sliding
Nearest-neighbor (Mahalanobis) 50 reps Sliding
Outlier-count (z-score)
100 reps Sliding
Nearest-neighbor (Mahalanobis) 100 reps Sliding
Outlier-count (z-score)
200 reps Sliding
Nearest-neighbor (Mahalanobis) 200 reps Sliding
(a): Estimates of the ﬁxed-eﬀects parameters of the model (in percentage points).
σs (Typist-to-typist)
σ (Residual)
Estimates of
Standard Deviation
6.0
6.6
(b): Estimates of the random-eﬀects parameters (as standard deviations).
270
K. Killourhy and R. Maxion
predicted equal-error rate is 10.1% for the same detector, training amount, and
updating strategy.
In Part (b) of Table 1, the typist-to-typist standard deviation (σs) is estimated
to be 6.0. Since the model assumes the typist-to-typist eﬀect to be Normally
distributed, we would predict that about 95% of typists’ average equal-error
rates will lie withing 2 standard deviations of the values predicted from Part
(a). For instance, suppose a new typist were added to a system operating with
the baseline factor values. The overall average equal-error rate is predicted to be
17.6%, and with 95% conﬁdence, we would predict that the average equal-error
rate for the new typist will be between 5.6% and 29.6% (i.e., 17.6 ± 2 × 6.0).
Likewise, there will be some day-to-day variation in a detector’s performance
due to unidentiﬁed sources of variation. Table 1(b) provides an estimate of 6.6
percentage points for this residual standard deviation (σ). Calculations similar
to those with the typist-to-typist standard deviation can be used to bound the
day-to-day change in detector performance. Note that these conﬁdence intervals
are quite large compared to the ﬁxed eﬀects in Part (a). Future research might
try to identify the source of the uncertainty and what makes a typist easy or
hard to distinguish.
A reader might ask what has really been learned from this statistical analy-
sis. Based on the empirical results in Section 3, it seemed that the Manhattan
(scaled) detector with 100 training repetitions and updating had the lowest er-
ror. It seemed that the feature set did not matter, and that impostor practice
had only a minor eﬀect. To answer, we would say that the statistical analysis has
not only supported these observations but explained and enriched them. We now
know which factors and interactions are responsible for the low error rate, and
we can predict how much that low error rate will change as the typists change
and the days progress.
5 Validation
The predictions in the previous section depend on the validity of the model.
While the model assumptions can be checked using additional statistical analysis
(e.g., residual plots to check Normality), the true test of a model’s predictive
validity is whether its predictions are accurate. In this section, we describe such
a test of validity and the outcome.
5.1 Procedure
We began by replicating the data-collection eﬀort that was used to collect the
data described in Section 3. The same apparatus was used to prompt subjects
to type the password (.tie5Roanl) and to monitor their keystrokes for typ-
ing errors. The same high-precision timing device was used to ensure that the
keystroke timestamps were collected accurately.
From among the faculty, staﬀ, and students of our university, 15 new subjects
were recruited. As before, these subjects typed the password 400 times over 8
Why Did My Detector Do That?!
271
sessions of 50 repetitions each. Each session occurred on separate days to capture
the natural day-to-day variation in typing rhythms. Their keystroke timestamps
were recorded and converted to password-timing vectors, comprised of 11 hold
times, 10 keydown-keydown times, and 10 keyup-keydown times.
The evaluation procedure described in Section 3 was replicated using this
new data set, for each algorithm, each amount of training data, each feature
set, and so on. Each subject was designated as the genuine user in separate
evaluations, with the other 14 subjects acting as impostors. We obtained 2,160
(3 × 4 × 3 × 2 × 2 × 15) new empirical equal-error rates from these evaluations.
To claim that our model is a useful predictor of detector error rates, two
properties should hold. First, the diﬀerence between the predicted equal-error
rate and each subject’s average equal-error rate should be Normally distributed
s). A zero mean indicates
with zero mean and variance predicted by the model (σ2
that the predicted equal-error rates are correct on average. A Normal distribution
of the points around that mean conﬁrms that the typist-to-typist variability has
been accurately captured by the model. Second, the residual errors, after the per-
subject eﬀects have been taken into account, should also be Normally distributed
 ). This property conﬁrms
with zero mean and variance predicted by the model (σ2
that the residual variability has also been captured by the model.
To test these properties, we calculate the diﬀerence between each empirical
equal-error rate and the predicted equal-error rate from the model. This calcu-
lation produces 2,160 prediction errors, 144 for each user. The per-typist eﬀect
for each user is calculated as the average of these 144 errors. The residual errors
are calculated by subtracting the per-typist eﬀect from each prediction error.
5.2 Results
Figure 2 contains two panels, the left one showing the distribution of the per-
typist eﬀects, and the right one showing the distribution of the residual errors.
y
t
i
s
n
e
D
6
5
4
3
2
1
0
y
t
i
s
n
e
D
8
6
4
2
0
−0.10 −0.05
0.00
0.05
0.10
0.15
−0.2 −0.1
0.0
0.1
0.2
0.3
0.4
Per−Typist Effects ( n = 15 )
Residuals ( n = 2160 )
Fig. 2. The distribution of the per-typist eﬀects and the residual errors compared to
their predicted Normal distributions. The histogram on the left shows the per-typist
eﬀect for the 15 subjects. The histogram on the right depicts the residual errors. Both
histograms closely match the Normal distributions predicted by the model. The match
between the predicted and the observed distributions validates the model.
272
K. Killourhy and R. Maxion
Overlaid on each histogram is the Normal distribution predicted by the model.
Both the per-typist-eﬀects histogram and the residual-error histogram closely
match the predicted Normal distributions.
It is diﬃcult to ascertain Normality from only 15 observations (one per sub-
ject), but the per-typist eﬀects appear to be clustered around a mean of zero
with the predicted variation about the mean. The residuals appear to be dis-
tributed as a bell-shaped curve with a mean of zero and the predicted variance.
The tails of the distribution are slightly thicker than Normal, but the overall
ﬁt is still very close. Based on these graphs, we conclude that the model can
accurately predict the detectors’ error rates on a new data set.
6 Related Work
Having demonstrated that we can explain detector’s error rates using inﬂuential
factors from the evaluation, we should put our ﬁndings in the context of other
keystroke-dynamics research. We cannot review the entire 30 year history of
keystroke dynamics, but Peacock et al. [14] provide a concise summary of many
of the developments during that time. In this section, we compare our ﬁndings
to prior research into the inﬂuences of the same six factors.
1. Algorithm: Several researchers have compared diﬀerent classiﬁcation algo-
rithms on a single data set, but few have compared anomaly detectors in
this way. Cho et al. [5] compared the Nearest Neighbor (Mahalanobis) de-
tector to an auto-associative neural network. Haider et al. [8] evaluated an
Outlier-count (z-score) detector, a diﬀerent neural network, and a fuzzy-logic
detector. In earlier work, we tried to reimplement 14 detectors and replicate
the results of earlier evaluations [12]. Our results diﬀered wildly from the
earlier ones (e.g., 85.9% vs. 1% error). In fact, the validation we report in
Section 5 is one of the few successful replications of error rates from an earlier
study using new data.
2. Amount of training: Joyce and Gupta [10] used only 8 password repeti-
tions to train their detector, but they found that the same accuracy was ob-
served with as few as 6 repetitions. Ara´ujo et al. [1] considered training sets
ranging from 6 to 10 repetitions. To our knowledge, while other researchers
have published evaluations using as many as 200 training repetitions [5], no
prior work has examined the error rates of detectors trained from as few as
5 to as many as 200 repetitions. In addition, earlier researchers have consid-
ered the eﬀect of training on a single algorithm. We found that the eﬀect of
training cannot be separated from the eﬀect of the algorithm.
3. Feature set: Ara´ujo et al. [1] ran evaluations of a Manhattan (scaled) de-
tector with seven diﬀerent feature sets, including the three we used. They
found that using all three types of feature (e.g., hold times, keydown-keydown
times, and keyup-keydown times) produced the best results. In contrast, we
found that, so long as hold times and either keydown-keydown times or keyup-
keydown times are included, the particular combination has negligible eﬀect.
Why Did My Detector Do That?!
273
Our ﬁndings beneﬁt from the statistical analysis we used to check whether
small eﬀects are substantial enough to be included in the model.
4. Updating strategy: Ara´ujo et al. [1] also compared a Manhattan (scaled)
detector using updating to one with no updating. Kang et al. [11] compared
a k-means detector trained with no updating to ones trained with grow-
ing and sliding windows. Both sets of researchers found that an updating
strategy lowered error rates. Their results for individual detectors, coupled
with our results for three detectors (backed by a validated statistical model),
strongly support the claim that updating reduces detector error rates. Our
results further demonstrate that window size (i.e., training amount) has an
important eﬀect on the error rate of a sliding-window detector.
5. Impostor practice: Lee and Cho [13] gave their impostors the opportunity
to practice, but they did not describe how many repetitions of practice were
taken by each impostor. Ara´ujo et al. [1] split their impostor subjects into
two groups. One group observed the genuine user typing the password, and
one group did not. The observers seemed to be more successful at mimicking
the typing style of the genuine user, but no statistical test was performed.
In contrast, our work operationalized practice in terms of the number of
repetitions and then quantiﬁed the eﬀect of practice on error rates.
6. Typist-to-typist variation: To our knowledge, no prior work has substan-
tially investigated whether some typists are easier to distinguish than others