PROPANE logging format [12]. The large number of test
cases considered in the fault injection process meant that no
additional data was generated for the evaluation of derived
predicates. Instead, 10-fold cross validation was used in
order to estimate the effectiveness of the predicates derived.
The process by which predicates were evaluated is discussed
further in Section VII-C.
B. Step 2: Algorithm Selection and Preprocessing
During preprocessing a purpose-built software tool was used
to automatically convert from the PROPANE logging format
to the format used by the Weka Data Mining Suite. To
demonstrate the effectiveness of the proposed methodology,
even in its most basic application, no technique was em-
ployed to enhance the learning algorithm to be used during
preprocessing. However, it should be noted that the issue
of class imbalance was addressed, through undersampling,
oversampling and varying the number of nearest neighbours,
in order to to identify an algorithm conﬁguration that would
yield the most effective predicate for each dataset. The
details of the undersampling and oversampling used in
ﬁnding the most effective predicates is detailed in Section
VII-D.
C. Step 3: Data Mining / Model Generation
In order to demonstrate the application of the methodology,
a speciﬁc symbolic pattern learning algorithm must be
used for the generation of predicates. In this paper we
use Decision Tree Induction for this purpose. Decision
Tree Induction is a symbolic pattern learning algorithm
that learns a disjunction of conjunctive rules describing a
concept. A decision tree consists of two types of nodes;
decision nodes and leaf nodes. A decision node contains
an input attribute value. Each edge emanating from a
decision node is labelled with one of the unique values in
the domain of the attribute labelling the decision node. A
leaf node is labelled using one of the classiﬁcation labels.
Each path of the tree from the root node to a leaf node
is interpreted as a set of conjunctive expressions that lead
to the classiﬁcation label at the associated leaf node. The
learning algorithm performs a greedy search of the space
of all possible trees choosing decision node attributes that
maximise the reduction in entropy of the class label. The
C4.5 decision tree induction algorithm was used to learn the
decision tree [34]. An example of the type of tree generated
by the algorithm can be seen in Figure 2, where non-leaf
nodes are labelled with variables, edges are labelled with
potential variable states and leaf nodes are labelled with a
failure classiﬁcation. A predicate is derived by interpreting
this structure as a conjunction of disjunctions.
Evaluation Method: To evaluate the effectiveness of the
baseline predicate generated, 10-fold cross validation was
Figure 2. Decision Tree Predicate Example
used to generate the confusion matrices for the adopted
data mining algorithm. The data was partitioned into 10
stratiﬁed samples, then for each cross validation run, one of
the partitions was used as the test sample, whilst the other
nine were used as the training set.
Table III shows the evaluation of the predicates that were
generated for all locations. The statistics shown in Table III
relate to predicates generated using a baseline conﬁguration
of the Decision Tree Induction algorithm, i.e., no attempt
was made to search for algorithm parameters which would
yield the most effective predicates. In Table III the FPR
and TPR columns give the mean false positive and true
positive rates taken across all 10 cross validations. A false
positive here corresponds to the situation where a predicate
incorrectly detects a state as being failure-inducing, whilst a
true positive corresponds to a predicate correctly identifying
a failure-inducing state. The AUC column shows the area
under the ROC curve, as described in Section IV. The Comp
column gives the complexity of the derived predicates, where
the stated value corresponds to the mean number of nodes in
the decision tree for all 10 cross validations. The Var column
gives the AUC variance across all 10 cross validations.
We observe from Table III that the mean AUC for all
baseline models is greater than 0.896422. As this measure
reﬂects both FPR and TPR, this is an indication that the
predicates generated are effective classiﬁers for failure in-
ducing states. Observe also that, aside from datasets FG-
B1 and FG-B3, the mean TPR for all models is greater
than 0.943459, with the maximum observed being 0.998690.
Further, the mean FPR is extremely low in all cases, with
the maximum observed value being 0.0025. This indicates
the highly-discriminatory nature of the predicates generated.
Finally, it is interesting to note that the variance of all the
models generated is consistently low, thus demonstrating the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:33:04 UTC from IEEE Xplore.  Restrictions apply. 
-3.040.49RWhlVel>3.7FALSE (390.0)TRUE (27.0)FALSE (16.0)TRUE (156.0)0.1TRUE (34.0)0.2SWhlVelcbdenom<=3.7CosWheel 33DECISION TREE INDUCTION RESULTS (NO SAMPLING)
Table III
DECISION TREE INDUCTION RESULTS (REFINED)
Table IV
Dataset
7Z-A1
7Z-A2
7Z-A3
7Z-B1
7Z-B2
7Z-B3
FG-A1
FG-A2
FG-A3
FG-B1
FG-B2
FG-B3
MG-A1
MG-A2
MG-A3
MG-B1
MG-B2
MG-B3
FPR
2E-05
0
0
1E-04
0
0
2E-04
3E-03
6E-04
1E-04
1E-05
1E-04
1E-09
3E-04
0
0
0
0
TPR
.9979
.9979
.9987
.9435
.9691
.9654
.9906
.9807
.9878
.7929
.9584
.8223
.9938
.9938
.9989
.9740
.9740
.9728
AUC
.9989
.9989
.9993
.9717
.9845
.9827
.9951
.9891
.9936
.8964
.9791
.9111
.9969
.9967
.9995
.9870
.9870
.9864
Comp
19.0
11.0
11.0
58.1
5.0
9.0
100.3
136.4
75.9
61.1
172.3
62.8
7.0
7.2
9.2
7.0
7.0
3.2
Var
3E-08
1E-08
1E-08
3E-04
1E-09
9E-10
7E-08
3E-06
3E-06
1E-32
1E-06
6E-08
1E-09
7E-08
1E-32
1E-32
1E-32
1E-30
consistency with which effective predicates are generated.
D. Step 4: Model Reﬁnement and Optimisation
Having generated and evaluated baseline predicates for error
detection mechanisms, these models can now be reﬁned by
varying the parameters associated with the Decision Tree
Induction algorithm. The results of this process are shown
in Table IV. The columns of Table IV are the same as
those given in Table III except for the S and N columns,
which show the sampling level and the number of nearest
neighbours used to generate the associated model respec-
tively. Each entry in the S column also shows the type of
sampling performed, where an O indicates oversampling and
a U indicates undersampling. A total of 10 undersampling
and 15 oversampling percentage levels were used in model
reﬁnement. These levels were distributed over the range
[5,100] and [100,1500] for undersampling and oversampling
respectively. The number of nearest neighbours considered
were distributed over the range [1,15].
The entries in Table IV show that each of the models
generated in the previous step were improved on, with
respect to the mean AUC measure, during the predicate
reﬁnement process. In some cases this improvement
is
relatively small, occasionally less than a 0.000001 increase,
but in the context of an error detection mechanism this
increase can be signiﬁcant. In almost all cases the variance
of all models is increased, though it should be noted that
these values remain extremely low.
In order to further validate the correctness of the results
presented, a cross validation for each model had its predicate
implemented as a runtime assertion in its corresponding
code location, i.e., the location at which logging took place
in order to generate the corresponding dataset. All fault
injection experiments were then repeated to ensure that the
Dataset
7Z-A1
7Z-A2
7Z-A3
7Z-B1
7Z-B2
7Z-B3
FG-A1
FG-A2
FG-A3
FG-B1
FG-B2
FG-B3
MG-A1
MG-A2
MG-A3
MG-B1
MG-B2
MG-B3
S
85(U)
300(O)
500(O)
300(O)
900(O)
700(O)
500(O)
900(O)
500(O)
35(U)
500(O)
500(O)
100(O)
40(U)
5(U)
75(U)
5(U)
5(U)
N
-
4
14
12
6
7
12
1
11
-
-
-
2
-
-
-
-
-
FPR
2E-05
5E-05
0
1E-03
3E-04
7E-05
1E-03
4E-03
1E-03
1E-02
2E-04
2E-04
0
0
0
0