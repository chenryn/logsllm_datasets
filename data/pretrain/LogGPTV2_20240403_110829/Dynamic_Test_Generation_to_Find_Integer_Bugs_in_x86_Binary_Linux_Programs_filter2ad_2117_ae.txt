4%
5509
12%
908
22%
102
310%
592
677
34.3%
8.8%
97
0.24%
2187
0.073%
2392
17.2%
36.3%
34.1%
36.3%
16%
12.7%
15%
7.5%
8.5%
4.0%
zzuf
326
1395
2472
20036
2210
538
20
6
1846
3560
3345
3561
334
275
316
209
237
114
NA
2934
2783
2816
252
259
266
123
125
115
12%
11.9%
12%
4.4%
4.5%
4.1%
Figure 8: Coverage metrics: the initial number of basic blocks, before testing; the number of blocks added during
testing; and the percentage of blocks added.
gzip-1
gzip-2
gzip-3
bzip2-1
bzip2-2
bzip2-3
mplayer-1
mplayer-2
mplayer-3
ffmpeg-1
ffmpeg-2
ffmpeg-3
convert-1
convert-2
exiv2-1
exiv2-2
exiv2-3
Total
206522s
208999s
209128s
208977s
208849s
162825s
131465s
131524s
49974s
73981s
131600s
24255s
14917s
97519s
49541s
69415s
154334s
Other
Record
Triage
SymExec Coverage
81.6%
17.6%
0.70%
0.06%
0.1%
80.89%
0.70% 17.59%
0.81%
0.005%
17.8%
0.68%
80.4%
1.09% 0.0024%
14.0% 83.915%
1.47%
0.335%
0.28%
84.32%
14.0%
0.283%
0.185%
1.25%
39.5%
3.09%
0.78% 31.09%
25.55%
14.2%
5.6% 22.95%
4.7%
52.57%
30.66%
5.53% 22.95% 25.20%
15.65%
9.7%
0.558% 1.467% 10.96%
77.31%
21.89%
4.67% 70.29%
0.579%
2.565%
36.138%
1.729%
9.75% 11.56%
40.8%
96.31% 0.1278% 0.833% 0.878% 1.8429%
70.17%
0.91%
0.89%
66.91%
5.28%
3.62%
5.78%
3.85%
1.15%
85.81%
2.43%
2.36% 24.13%
2.18%
1.89% 28.14%
10.62% 71.29%
9.18%
12.25% 65.64% 12.48%
1.41%
8.12%
3.50%
Figure 9: The percentage of time spent in each of the
phases of SmartFuzz. The second column reports the to-
tal wall-clock time, in seconds, for the run; the remaining
columns are a percentage of this total. The “other” col-
umn includes the time spent solving STP queries.
target program. For example, the ﬁrst run of mplayer,
which used a mp3 seedﬁle, spent 98.57% of total time
in symbolic execution, and only 0.33% in coverage, and
0.72% in triage. In contrast, the second run of mplayer,
which used a mp4 seedﬁle, spent only 14.77% of time in
symbolic execution, but 10.23% of time in coverage, and
40.82% in triage. We see that the speed of symbolic ex-
ecution and of triage is the major bottleneck for several
of our test runs, however. This shows that future work
should focus on improving these two areas.
13
7.5 Solver Statistics
Related Constraint Optimization Varies By Program.
We measured the size of all queries to the constraint
solver, both before and after applying the related con-
straint optimization described in Section 4. Figure 10
shows the average size for queries from each test pro-
gram, taken over all queries in all test runs with that
program. We see that while the optimization is effective
in all cases, its average effectiveness varies greatly from
one test program to another. This shows that different
programs vary greatly in how many input bytes inﬂuence
each query.
The Majority of Queries Are Fast. Figure 10 shows the
empirical cumulative distribution function of STP solver
times over all our test runs. For about 70% of the test
cases, the solver takes at most one second. The maxi-
mum solver time was about 10.89 seconds. These results
reﬂect our choice of memory model and the effectiveness
of the related constraint optimization. Because of these,
the queries to the solver consist only of operations over
bitvectors (with no array constraints), and most of the
sets of constraints sent to the solver are small, yielding
fast solver performance.
8 Conclusion
We described new methods for ﬁnding integer bugs
in dynamic test generation, and we implemented these
methods in SmartFuzz, a new dynamic test generation
tool. We then reported on our experiences building the
web site metafuzz.com and using it to manage test case
generation at scale. In particular, we found that Smart-
Fuzz ﬁnds bugs not found by zzuf and vice versa, show-
ing that a comprehensive testing strategy should use both
average before
average after
ratio (before/after)
mplayer
ffmpeg
exiv2
gzip
bzip2
convert
292524
350846
81807
348027
278980
2119
33092
83086
10696
199336
162159
1057
8.84
4.22
7.65
1.75
1.72
2.00
Figure 10: On the left, average query size before and af-
ter related constraint optimization for each test program.
On the right, an empirical CDF of solver times.
white-box and black-box test generation tools.
Furthermore, we showed that our methods can ﬁnd in-
teger bugs without the false positives inherent to static
analysis or runtime checking approaches, and we showed
that our methods scale to commodity Linux media play-
ing software. The Metafuzz web site is live, and we have
released our code to allow others to use our work.
9 Acknowledgments
We thank Cristian Cadar, Daniel Dunbar, Dawson En-
gler, Patrice Godefoid, Michael Levin, and Paul Twohey
for discussions about their respective systems and dy-
namic test generation. We thank Paul Twohey for helpful
tips on the engineering of test machines. We thank Chris
Karlof for reading a draft of our paper on short notice.
We thank the SUPERB TRUST 2008 team for their work
with Metafuzz and SmartFuzz during Summer 2008. We
thank Li-Wen Hsu and Alex Fabrikant for their help
with the metafuzz.com web site, and Sushant Shankar,
Shiuan-Tzuo Shen, and Mark Winterrowd for their com-
ments. We thank Erinn Clark, Charlie Miller, Prateek
Saxena, Dawn Song, the Berkeley BitBlaze group, and
the anonymous Oakland referees for feedback on ear-
lier drafts of this work. This work was generously sup-
ported by funding from DARPA and by NSF grants CCF-
0430585 and CCF-0424422.
References
[1] ASLANI, M., CHUNG, N., DOHERTY,
J.,
STOCKMAN, N., AND QUACH, W. Comparison of
blackbox and whitebox fuzzers in ﬁnding software
bugs, November 2008. TRUST Retreat Presenta-
tion.
14
[2] BLEXIM. Basic integer overﬂows. Phrack 0x0b
(2002).
[3] BLEXIM.
Basic integer overﬂows.
Phrack
0x0b, 0x3c (2002). http://www.phrack.org/
archives/60/p60-0x0a.txt.
[4] BRUMLEY, D., CHIEH, T., JOHNSON, R., LIN,
H., AND SONG, D. RICH : Automatically protect-
ing against integer-based vulnerabilities. In NDSS
(Symp. on Network and Distributed System Secu-
rity) (2007).
[5] BRUMLEY, D., NEWSOME, J., SONG, D., WANG,
H., AND JHA, S. Towards automatic generation of
vulnerability-based signatures.
In Proceedings of
the 2006 IEEE Symposium on Security and Privacy
(2006).
[6] CADAR, C., DUNBAR, D., AND ENGLER, D.
Klee: Unassisted and automatic generation of high-
coverage tests for complex systems programs.
In
Proceedings of OSDI 2008 (2008).
[7] CADAR, C., AND ENGLER, D. EGT: Execution
generated testing. In SPIN (2005).
[8] CADAR, C., GANESH, V., PAWLOWSKI, P. M.,
DILL, D. L., AND ENGLER, D. R. EXE: Auto-
matically Generating Inputs of Death. In ACM CCS
(2006).
[9] CHEN, K., AND WAGNER, D. Large-scale analy-
sis of format string vulnerabilities in debian linux.
In PLAS - Programming Languages and Analysis
for Security (2007). http://www.cs.berkeley.
edu/~daw/papers/fmtstr-plas07.pdf.
[10] CORPORATION, M. Vulnerability Type Distribu-
tions in CVE, May 2007. http://cve.mitre.
org/docs/vuln-trends/index.html.
[11] DEMOTT, J. The evolving art of fuzzing. In DEF
CON 14 (2006). http://www.appliedsec.com/
files/The_Evolving_Art_of_Fuzzing.odp.
[12] GANESH, V., AND DILL, D.
STP: A deci-
sion procedure for bitvectors and arrays. CAV
2007, 2007.
http://theory.stanford.edu/
~vganesh/stp.html.
[13] GODEFROID, P., KLARLUND, N., AND SEN, K.
DART: Directed Automated Random Testing.
In
Proceedings of PLDI’2005 (ACM SIGPLAN 2005
Conference on Programming Language Design and
Implementation) (Chicago, June 2005), pp. 213–
223.
[27] VUAGNOUX, M. Autodafe: An act of soft-
ware torture.
In 22nd Chaos Communications
Congress, Berlin, Germany (2005). autodafe.
sourceforge.net.
[28] WANG, T., WEI, T., LIN, Z., AND ZOU, W.
Intscope: Automatically detecting integer overﬂow
vulnerability in x86 binary using symbolic execu-
tion. In Network Distributed Security Symposium
(NDSS) (2009).
Automated Whitebox Fuzz Testing.
[14] GODEFROID, P., LEVIN, M., AND MOLNAR,
D.
In
Proceedings of NDSS’2008 (Network and Dis-
tributed Systems Security) (San Diego, Febru-
ary 2008). http://research.microsoft.com/
users/pg/public_psfiles/ndss2008.pdf.
[15] GODEFROID, P., LEVIN, M. Y., AND MOLNAR,
D. Active Property Checking. Tech. rep., Mi-
crosoft, 2007. MSR-TR-2007-91.
[16] HOCEVAR, S. zzuf, 2007. http://caca.zoy.
org/wiki/zzuf.
[17] LANZI, A., MARTIGNONI, L., MONGA, M.,
A smart fuzzer for x86
AND PALEARI, R.
executables.
In Software Engineering for Se-
cure Systems, 2007. SESS ’07: ICSE Workshops
2007 (2007). http://idea.sec.dico.unimi.
it/~roberto/pubs/sess07.pdf.
[18] LARSON, E., AND AUSTIN, T. High Coverage
Detection of Input-Related Security Faults.
In
Proceedings of 12th USENIX Security Symposium
(Washington D.C., August 2003).
[19] LEBLANC, D. Safeint 3.0.11, 2008. http://www.
codeplex.com/SafeInt.
[20] LMH. Month of kernel bugs, November 2006.
http://projects.info-pull.com/mokb/.
[21] MICROSOFT CORPORATION. Prefast, 2008.
[22] MILLER, B. P., FREDRIKSEN, L., AND SO, B. An
empirical study of the reliability of UNIX utilities.
Communications of the Association for Computing
Machinery 33, 12 (1990), 32–44.
[23] MOORE, H. Month of browser bugs, July 2006.
http://browserfun.blogspot.com/.
[24] NETHERCOTE, N., AND SEWARD, J. Valgrind: A
framework for heavyweight dynamic binary instru-
mentation. In PLDI - Programming Language De-
sign and Implementation (2007).
[25] O’CALLAHAN,
2008.
chronicle-recorder/.
R.
Chronicle-recorder,
http://code.google.com/p/
[26] SEWARD, J., AND NETHERCOTE, N. Using val-
grind to detect undeﬁned memory errors with bit
precision.
In Proceedings of the USENIX An-
nual Technical Conference (2005). http://www.
valgrind.org/docs/memcheck2005.pdf.
15