,
(cid:18) 1
1
(cid:19)(cid:27)
(cid:19)
(cid:18) 1−1
The best eigenvector selected to compute the different partition is
. It gives the following scores (2.6) and possible split (2.7):
There are two interesting results with this example :
CHAPTER 2. BACKGROUND
15
Cat
Dog
Salamander
Frog
Score
1
1/3
-1/3
-1
Table 2.6: Scores given by the covariance algorithm
Attribute Left
Attribute Right
Index left
Index right
Gain
Best split
Split 1
Cat
Sal, Fro, Dog
(4,5)
(1,2,3,6,7,8,9,10)
0
No
Split 2
Cat, Dog
Sal, Fro
(1,2,3,4,5)
(6,7,8,9,10)
1
Yes
Split 3
Cat, Dog, Sal
Fro
(1,2,3,4,5,6,7,8)
(9,10)
0
No
Table 2.7: Possible split in Covariance algorithm on the toy example
(Fro stands for Frog and Sal for Salamander)
• Finding the best split over all possible splits and making inter-
esting groups. In fact, regarding the data it seems that the dis-
ease only affect the mammals and this rule is learned by the al-
gorithm.
• It ﬁnds exactly the same split as the previous two algorithms.
(cid:19)
(cid:18) 1−1
The second point could be generalized to every binary classiﬁcation.
The equivalence between majority class and covariance methods is
clear because in a binary case the algorithm will always produce a 2x2
matrix with a best eigenvector co-linear to
which gives scores
like 2pi − 1, where pi is the probability of belonging to class A. The
equivalence with the method push left by purity will be discussed in
one of the following sections because it is not straight-forward.
As said in the introduction of this part, the algorithm have the same
spirit as the PCA. The criterion on which the partitions are selected is
the coordinate over the biggest axis of variation, which leads to the
classes that are well discriminated by the categorical attribute. That
16
CHAPTER 2. BACKGROUND
could be an explaination for why this heuristic provides such good re-
sults even if only p partitions over the 2p possible partitions are consid-
ered. In term of complexity, the computation of the best eigenvector is
really quick compared to a dataset scan, so this step in the algorithm is
negligible. There is currently no proof on the optimality of this method
but this is the one that seems to produce the best results.
To conclude this section, table 2.8 summarizes all the properties of the
four presented methods.
One hot
Push left
by purity
Majority class Covariance
Complexity
(number of split)
Grouping values
Handling well
multiclass
Handling regression
p
X
(cid:88)
(cid:88)
p2
(cid:88)
(cid:88)
(cid:88)
p
(cid:88)
X
X
p
(cid:88)
(cid:88)
X
Table 2.8: Algorithm properties
2.4 Performance improvement
Currently, decision trees are cornerstones of several state-of-the-art
machine learning algorithm such as Random Forest [1] and xgboost
[2]. All these methods perform well but are not globally explainable.
Some scientists have already made some research on how to improve
the performance of a single tree classiﬁer or regressor using the advan-
tages of strong black box models [19].
2.4.1 Re-sampling
In the past year, in many ﬁelds, data augmentation has been a leading
trend. For example, in image classiﬁcation it is common use to per-
form rotations, shifts and pixel swapping to produce new images in
the training set [20]. These sampling methods often give performance
boost to a model and make it more generalizable and less sensitive to
overﬁtting.
CHAPTER 2. BACKGROUND
17
Pedro Domingo in 1996 [21] initiated a methodology to merge in-
formation given by several models into a unique one. This ﬁrst step
is simple but have quite impressive results. It is based on the bagging
method developed by Breiman [1] the same year. Decision trees are
constructed on random subsets of the training dataset, then random
samples are generated using the Gaussian generation method. For
each feature, the mean and the standard deviation are computed and
new samples are generated shooting random Gaussian points. Then,
previous models, learned on the dataset without augmentation, are
used to compute the class of new generated samples. The class is given
by averaging the decisions given by all the learners. A single decision
tree is then learned on the whole dataset. In this paper [22], a bench-
mark is done over more than 20 datasets and, on average, this method
allows to have a signiﬁcant increase of performances due to the use of
the bagging method.
The main drawback of this method is that a lot of samples are gen-
erated in parts of space that have no samples. For example, think
about a dataset which has two features F1 and F2 which are strongly
correlated. When F1 is positive, F2 must also be positive but the al-
gorithm creates points that does not respect this rule, tweeking too
much the actual distribution. One idea to overcome this issue is not
to use the assumption that all features are independent and generate
new samples using a Gaussian Mixture model.
2.4.2 Online learning
One of the important drawbacks about decision trees is that all the
data must ﬁt in the computer memory. This creates two problems:
• Impossibility to learn model on large dataset.
• Impossibility to update the model over the time without re-building
a model from scratch.
The main problem resides in the way decision trees are built. In an
online process a feature could be important during a speciﬁc period
and then become irrelevant over time. The way this algorithm oper-
ates will create a ﬁrst split based on the ﬁrst relevant feature with no
possibility of update. Then, when the feature becomes useless, this
split will stay at the root of the tree and will not be updated. Even if
the presence of the split does not degrade the global performance of
18
CHAPTER 2. BACKGROUND
the decision tree, the global compactness of the decision tree will be
decreased.
Interestingly, the ﬁrst online algorithm which was proposed in the
literature rebuilds a model each n samples, where n is a number "well-
suited" for the problem. The ﬁrst steps forward were limited to ID3
algorithm with categorical features [23]. They chose ﬁrst to address
only this subproblem because it is possible to store at each node the
necessary information to compute the information gain. Actually, only
the value distribution per class is needed to recompute the Shanon en-
tropy or the Gini information. So, the ﬁrst idea was to recompute at
the entropy at each new batch of samples and see if the best attribute
to split on is still the same. If not, the previous splitting attribute is re-
placed by the new best one. This new method did not allow to retrieve
the same tree as the one constructed by the Quinlan method. In fact,
when a subtree is discarded, all information on its child nodes is lost
and the subtree is built from scratch. The next improvement to this
method is to be sure that changing the splitting attribute on a node
does not discard any information. The method proposed by Quinlan
in 1985 [7] is to pull a node up when the attribute becomes more im-
portant. After that there were no more signiﬁcant improvements on
the ID3 algorithm because the method used to pull up a node at the
top of the tree is too complicated and takes too much time.
The idea of incremental decision trees was re-explored in the early
2000s but this time based on the idea that we have to change a split
only if it has a statistical signiﬁcance. This was the birth of the Ho-
effding trees. With Hoeffding trees another hyperparameter appeared.
Considering two splitting attributes X1 and X2, a split is done, if and
only if:
(cid:115)
|G(X1) − G(X2)| >  =
log(nclass)2log( 1
δ )
2nsample
Where G(X) is the gain, δ is the desired probability to ﬁnd the best
split, nsamples is the number of samples used to compute the gain and
nclass the number of classes. This bound called "the Hoeffding bound"
ensures that if a split is done, then there is a probability of δ that it is the
best split. To speed up the algorithm, gains are not computed at each
step because they should not change signiﬁcantly in only a few steps.
This algorithm ensures that the produced tree has a high probability
to look like the optimal one.
CHAPTER 2. BACKGROUND
19
The last step done in this ﬁeld was to add Naïve Bayesian estimator
because of their truly online nature.
We are going to see a last family of algorithms which can be use-
ful for performance improvement of single decision trees: Ensemble
Method.
2.4.3 Ensemble methods
In the late 90s, after discovering decision trees, some researchers tried
to apply boostrap aggregating methods to decision trees in order to
improve their performances.
In 2001, the random forest algorithm
was born [1], and made a huge step forward in machine learning tech-
niques and opened a massive research ﬁeld on decision trees improve-
ment. There are still a lot of research on this algorithm because we do
not understand mathematically why this algorithm outperforms de-
cision trees that much. One step forward was taken in 2015 with the
article from Erwan Scornet [24] which proves under some assumptions
that random forest algorithm is consistent.
This algorithm is based on a very interesting idea called bootstrap-
ping aggregating, also known as bagging. To do so, several decision
trees are learnt on random parts of the dataset. The random subsam-
pling is performed in two ways. Firstly, only a small amount of all the
samples are selected, and secondly, on the features used to train the
model. After this step, a simple decision tree is built on this dataset
excerpt. In this algorithm, learnt decision trees are most of the time
stump, that is to say they are one-level depth decision trees. This
method provides a lot of weak classiﬁers which are only slightly better
than randomly guessing the output. Then comes the aggregating step,
all the decisions taken by weak decision trees are gathered to provide
a unique strong decision. The method of aggregating differs according
to the learning task. For regression, the best use is to take the mean of
the decisions and, for classiﬁcation, to take the majority class.
One other step that has been done to improve decision trees per-
formance is boosting. In tree learning all the samples have the same
importance, but looking at the splitting criterion, it is relatively easy to
add weights to samples that are taken into account in the splitting cri-
terion. Using the same notations if each sample has now a weight wxi
the entropy that we have described in equation 2.2 simply becomes:
20
CHAPTER 2. BACKGROUND
pi =
∑
x∈Di
∑
x∈D
H(D) = − m
∑
i=0
wx
wx
pilog(pi)
An interesting feeling that Freund and Shapire had is that, whatever
model it is, we have to learn more on samples that are wrongly classi-
ﬁed, and focus less on the samples that are well-classiﬁed. Using this
technique, the next generated model will be theoretically more accu-
rate on the previous misclassiﬁed samples. After this step, all gener-
ated models are aggregated using the same technique as in bagging.
One can directly see one of the drawbacks of this method: it can give
too much importance to the noise and outliers, and eventually build a
model that will be totally different from reality.
A last step has been recently done in these approaches of boosting and
bagging: the gradient boosting. The famous XGBoost [2] model con-
ceived by Tianqi Chen, Carlos Guestrin two researchers at the Wash-
ington University of Computer Science has won many of the recent
Machine Leaning competitions.
But all these improvements have the same drawbacks compared to
simple decision trees, we totally lose the explainability power and
the simplicity of the model. Some attempts have been made on these
methods to give the user some insights about the importance of some
features and how decisions are taken but none have been proven to be
reliable.
To conclude this part we have seen how decision tree algorithm
works. We have also reviewed different possibilities regarding the cat-
egorical split problem. Decision trees could be a possible way to follow
regarding explainability in machine learning, but their poor perfor-
mances compared to state-of-the-art-models such as xgboost or neural
networks raise the real needs to ﬁnd a way to improve them. But even
if global explainability could be a feature of interest there is a lot of
research about local explainability which led to the birth of methods
such as LIME or SHAP.
CHAPTER 2. BACKGROUND
21
Algorithm 1 Push left by purity
1: L ← ∅
2: R ← {ai}i∈[1,p]
3: BestPart ← (L, R)
4: BestGain ← 0
5: if Card(R) = 2 then
6:
7: else
8:
9:
10:
11:
12:
13:
BestPart ← ({a1},{a2})
while Card(R) ≥ 3 do
CurrBestPart ← (L, R)
CurrBestGain ← 0
BestValue ← R[0]
for a ∈ R do
g ← CG(L ∪ {a}, R \ {a})
partition
(cid:46) Compute the gain for the
if g > CurrBestGain then
CurrBestGain ← g
CurrBestPart ← (L ∪ {a}, R \ {a})
BestValue ← a
end for
if BestValue < g then
BestValue ← g
BestPart ← CurrBestPart
end if
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27: end if
28: return BestPart
end while
end if
L ← L ∪ {BestValue}
R ← R \ {BestValue}
22
CHAPTER 2. BACKGROUND
ci)
(cid:46) The ai are ordered using the π[i]
π[i] ← (Card((Xj,yj),j∈[1,N]|yj=cbest))
N
Algorithm 2 Majority class
1: Compute cbest = argmax(ci, i ∈ [1, m]Card((Xj, yj), j ∈ [1, N]|yj =
2: for j ∈ [1, m] do
3:
4: end for
5: L ← ∅
6: R ← {ai}i∈[1,p]
7: BestPart ← (L, R)
8: BestGain ← 0
9: for j ∈ [1, m − 1] do
10:
11:
12:
13:
14:
15:
16:
17: end for
18: return BestPart
L ← L ∪ {aj}
R ← R \ {aj}
g ← CG(L, R)
if BestValue < g then
BestValue ← g
BestPart ← (L, R)
(cid:46) Compute the gain for the partition (L, R)
end if
Algorithm 3 Covariance split
Compute πglobal, πi, C, ebest, si
L ← ∅
R ← {ai}i∈[1,p]
BestPart ← (L, R)
BestGain ← 0
for j ∈ [1, m − 1] do
L ← L ∪ {aj}
R ← R \ {aj}
g ← CG(L, R)
if BestValue < g then
BestValue ← g
BestPart ← (L, R)
end if
end for
return BestPart
(cid:46) The ai are ordered using the si
(cid:46) Compute the gain for the partition (L, R)
Chapter 3
Methods and evaluation
To evaluate the performance and the relevance of our proposed method,
there is a strong necessity to deﬁne several performance indicators.
The aim of this chapter is to ﬁrst show how the produced models are
evaluated, then to present a new method for the split on categorical
attributes, and ﬁnally to present the idea around raw performance en-
hancement using new sampling generation.
3.1 Key performance indicators
The main goal of this thesis is to increase the compactness of the
learned models while conserving their performances in terms of accu-
racy. Compactness could be seen as one of the indicator of the power of
explainability of a model. Only a little research has been done on what
really is explainability. It is a subjective notion which is really hard to
evaluate. With the rise of deep learning and other black box models,
this ﬁeld is currently exploding. An interesting article from MIT [25]
tries to decide how to evaluate explainability. It distinguishes explain-
ability, interpretability and completeness. For them, the interpretabil-
ity and the completeness are two ways of evaluating the explainability.
Their deﬁnitions of these three terms are:
• Explainability: the state of being explainable that is to say an-
swering to «why and why-should » questions.
• Interpretability: « describe the internals of a system in a way that
is understandable to humans ».
23
24
CHAPTER 3. METHODS AND EVALUATION
• Completeness: « describe the operation of a system in an accu-
rate way ».
When dealing with performance, the accuracy is not the only indicator