reordering on TCP. This is because back-to-back packets
within a given ﬂow may now traverse a completely different
route to the destination. Depending on the trafﬁc conditions
along these individual paths, the packets may arrive out of
order at the receiver. While it is well-known that TCP interacts
poorly with packet reordering, we argue that these ﬁne-grained
trafﬁc splitting schemes improve performance on regular data
center network architectures such as the fat-tree. Our intuition
is rooted in three basic observations.
1) Low queue sizes. Per-packet trafﬁc splitting avoids un-
even queue build-up along any path to the destination.
This leads to fewer packet drops and uniformly low
round-trip times for all ﬂows.
2) Better load balancing. All ﬂows between a source (or
sources within the same rack) and a destination (or
destinations with the same rack) traverse the same set of
paths and experience similar network conditions, leading
to better and fairer sharing of resources.
3) Independence from ﬂow-size distribution. Algorithms
that split trafﬁc at a packet granularity are not affected
by changes in ﬂow size distribution.
Of course, PLTS will also suffer from reordering due to the
potentially different latencies of packets along different paths.
This reordering will typically cause the TCP receiver to send
duplicate ACKs that may trigger congestion avoidance activity
at the TCP sender, causing it to reduce its congestion window
signiﬁcantly thus leading to a reduction in the achieved
throughput. We argue, however, that when all ﬂows are split
equally between all possible paths, queue lengths along all
the paths are typically equally loaded which causes a small
number of reorderings, but not enough to cause congestion
window reductions and fast retransmits. We discuss these
issues in more detail in the next section.
III. PACKET-LEVEL TRAFFIC SPLITTING
In this section, we discuss different techniques that employ
packet-level
trafﬁc splitting (PLTS) at switches. We also
discuss the adverse effects of PLTS on the TCP throughput in
the context of fat-tree network topologies. Finally, we discuss
different techniques that can be utilized to mitigate the negative
effects of PLTS using simple solutions that can be deployed
at the receiver TCP stack or within the end-host hypervisor.
A. Mechanisms
We use three simple techniques—random, counter-based
and round-robin—to demonstrate the power and limitations
of PLTS. All these techniques essentially split trafﬁc within a
ﬂow across all available paths, but differ based on the amount
of state maintained at the switches. Random picks an arbitrary
port at random (among the multiple next hops) to forward
a packet, and hence requires no state to implement. At the
other end of the spectrum, we discuss a per-ﬂow round robin
technique where packets within a ﬂow are transmitted in a
round robin fashion. However, it requires remembering the
last port used on a per-ﬂow basis making it more complicated
than the random. A variant of this algorithm, where packets for
the same destination are transmitted in a round-robin fashion,
is already implemented in many commercial switches (e.g.,
Cisco). Intuitively speaking, per-ﬂow round robin achieves
better load-balancing than destination-based; hence, we chose
this in our list of techniques.
A:1000B:400C:10000D:100A:500B:200C:5000D:50A:250B:100C:2500D:25C1C2C3C4A1A2T1T2PLTSECMPA:1000B:400C:10000D:100A:1000C:10000A:1000C:10000C1C2C3C4A1A2T1T2B:400D:100B:400D:100Load ImbalanceIn addition to the above, we also propose a new algorithm
that splits trafﬁc based on local port counters. This reduces
the amount of state required signiﬁcantly, but also results in
slightly worse load balancing in the network (as we shall
see) and consequently, a slight loss in throughput. While this
list of algorithms is not meant to be exhaustive, we chose
these three schemes as potential candidates since they can be
implemented in a rather straight-forward fashion. We discuss
these algorithms in more detail in the next few paragraphs.
1) Random path selection: One of the simplest techniques
is to randomly forward the packets of a given ﬂow along one
of the available paths to the destination. In this scheme called
PLTS-Random, for any incoming packet, the switch uses a
random number generator to decide the port on which the
packet should be forwarded. The forwarding decision about
the next incoming packet for the same ﬂow is independent of
the previous packets and switch does not need to maintain any
ﬂow level state about the port on which the last packet was
forwarded.
A uniform random number generator guarantees that all
paths will be equally loaded over a long interval of time. How-
ever, this scheme may suffer from some short-term imbalance
due to the completely random choice. This imbalance may re-
sult in momentary increases in queue lengths along one of the
paths while leaving some other path underutilized ultimately
resulting in less efﬁcient use of the network resources. But this
approach is simple to implement; thus, we focus on analyzing
its performance in this paper.
2) Counter-based load balancing: We can address the
short-term load imbalance due to the random choices by
explicitly storing a counter for each port and choose the
next-hop that is least loaded. This approach tries to keep the
local queue sizes comparable across all the ports at a switch.
Speciﬁcally, the switch maintains a counter corresponding to
each output port. While forwarding a packet, it selects the
output port with the smallest counter value and increments the
counter by the corresponding packet size (in bytes). Clearly,
this technique results in a keeping all the ports as balanced
as possible within the switch. For implementing this scheme
(referred to as PLTS-Counter), the only extra state required
are the counters corresponding to ports of the switch.
PLTS-Counter however can potentially lead to slightly sub-
optimal trafﬁc splitting. For example, in Figure 2, we show an
example scenario where the load may be slightly imbalanced
compared to the ideal. In the example, there are three ﬂows
marked A, B and C of equal size. Among the three, B’s
destination is within the pod. Let us suppose that B’s packets
and A’s packets are perfectly interleaved at the switch T 1, in
which case, A’s packets go via T 1− A1 link and B’s packets
go via T 1 − A2 link. (In general, the skew may depend on
the level of interleaving.) Because of this interleaving, all of
A’s packets are routed to A1 which means only two paths are
available for these packets. Meanwhile, since B’s destination
is within the pod, B’s packets do not compete for resources
at the links between A2 and the core switches C3 and C4. In
this scenario, we can see that ﬁrst two core links A1− C1 and
Fig. 2.
imbalance.
One scenario where counter-based approach may lead to load
A1−C2 carry half of A’s packets and a quarter of C’s packets
each, while the other two carry a quarter of C’s packets each.
In the ideal case where we do perfect trafﬁc splitting of every
ﬂow, as the next algorithm achieves, we would observe that
each of the core links would carry a quarter of A’s packets
and a quarter of C’s packets, similar to the scenario discussed
in Figure 1 example.
3) Round-robin across paths: The last mechanism we dis-
cuss, that will address the sub-optimality associated with the
previous scheme at the cost of extra state in the router, is
the per-ﬂow round robin scheme (referred to as PLTS-RR). In
PLTS-RR, each switch uses a round-robin policy to select the
port on which the packet corresponding to a particular ﬂow
has to be forwarded. To do this, it maintains the next hop taken
by the previous packet of that ﬂow. This information needs to
be consulted and updated for every packet corresponding to
the ﬂow.
One small issue with round-robin based trafﬁc splitting is
that packets can suffer from small delays due to synchro-
nization problems. For example, the two incoming ﬂows at
a switch can get synchronized in the selection of port using
round robin technique. They choose, say, port 1 for their ﬁrst
packet and then in a round robin fashion port 2 for their second
packet and so on. But for this to be a major problem, the packet
arrival rates of the two ﬂows has to also synchronize. We can
solve this problem to some extent by introducing a little bit
of randomness in the way round-robin operates. It, however,
requires a more detailed analysis that is outside the scope of
this paper.
While we have presented this algorithm assuming switches
can store one piece of extra state per-ﬂow to remember the
last port that was used to forward a packet for that ﬂow,
this may not be feasible in practice as the number of ﬂows
may be huge. We note however that weaker variants of this
algorithm are easily possible. For example, since all servers
within a rack are part of the same destination subnet, instead
of maintaining the state on a per-ﬂow basis, we could maintain
A:1000C:1000A:1000A:500C:250C1C2C3C4A1A2T1T2C:500B:1000B:1000C:250B’s destinationSlight load imbalancethe port information on a per-preﬁx basis. Since forwarding
may be preﬁx-based anyways, this requires just one extra
counter per entry in the forwarding table. While we do not
compare this scheme in this paper, we note that we believe
the performance of this variant will lie somewhere between
the pure per-ﬂow round robin and the counter-based approach.
(In our evaluation, the gap between the two is about 10%.)
B. Adverse effects of packet-level trafﬁc splitting
The packet forwarding techniques described above opti-
mizes the use of network resource available. We discussed
the various advantages of packet-level
trafﬁc splitting in
Section II, but the obvious down-side of ﬁne grained trafﬁc
splitting is packet reordering in a ﬂow which can adversely
affect TCP performance. Speciﬁcally, there are two types of
reordering that can happen—forward-path and reverse-path—
reordering that we explain next.
1) Forward-Path Reordering: Forward-path reordering is
the reordering of data packets, which results in the genera-
tion of duplicate acknowledgements (DUPACKs). If the TCP
receiver receives packets out of order, it would generate a
DUPACK that correspond to the last packet received in order.
While eventually the receiver would obtain all the packets
and would advance the acknowledgement number in ACKs,
the sender upon seeing three DUPACKs assumes that there
was a packet loss in the network because the window is
too high. Correspondingly, it would perform fast-retransmit
and reduce the window by half (other variants may reduce
it by a different amount). Unfortunately, both these actions
would not have actually been necessary and in fact cause two
major problems. First, the unnecessary retransmissions would
waste precious bandwidth resources in the network. Second,
and perhaps more importantly, the reduction in the congestion
window causes TCP sender to be not as aggressive and thus
would not fully utilize the available bandwidth. Both these ill-
effects are the main reason why ECMP (and other mechanisms
such as Hedera [3]) did not disturb the ordering of packets
within a given ﬂow.
In addition to these direct effects, there are also two in-
direct effects. First, RTT estimators for ﬂows not using TCP
timestamp option can, at least in theory, suffer from packet
reordering. But as discussed in [6], it turns out that this is not
a big problem in practice. Second, the TCP receiver is forced
to buffer out-of-order data packets and data is released to the
application in bursts. Of course, buffering itself is not such a
big deal as end hosts typically are reasonably well-provisioned
in terms of memory. Bursty release of data to the application,
however, can be a bit of a concern for multimedia applications
(which use UDP anyway precisely for this reason), but not so
much for bulk transfer applications.
2) Reverse-Path Reordering: If ACKs are reordered and
one ACK arrives early and acknowledges a large amount of
data, TCP sender is likely to transmit a series of new segments
at once. So we may see bursts of TCP segments from the
sender. One potential ﬁx to this problem is that we can choose
to do ECMP-based forwarding for ACK packets. On the other
the switches (see section IV),
hand, there may not be a need for such a ﬁx since back-to-
back packets may now take completely different routes; so,
the bursts are likely to be spread out throughout the network.
In the context of data center networks, PLTS scheme ensures
even-sized queues at
thus
keeping the network free of any hot-spots. The packets of
a given ﬂow travelling on parallel paths to the receiver would
encounter similar queue sizes along their paths. Thus, even if
some of the packets get reordered, not all of them trigger the
congestion control mechanism of TCP. The ill-effects of packet
reordering, which can be pretty severe in case of the Internet,
are relatively quite mild in data center networks. The latencies
observed across multiple paths are similar, the receiver does
not have to buffer a large number of out-of-order packets and
not all the reordered packets lead to fast retransmission. In
order to further reduce the adverse effects of PLTS, we discuss
a few potential techniques that can be implemented at the end
host.
C. Reducing spurious fast retransmissions
We can potentially mitigate the ill-effects of spurious fast
retransmits and improve TCP’s performance if we can some-
how detect and suppress spurious DUPACKs that are generated
due to reordering. While some research has been done to
distinguish packet reordering from a packet loss event [8], the
problem remains considerably difﬁcult in the general setting.
We can use some simple mechanisms to prevent the TCP
sender from reacting to DUPACKs too aggressively, since if
we implement PLTS, most DUPACKs will be due to transient
reordering. We discuss two techniques—the ﬁrst
involves
changes in the receiver TCP stack, while the second focusses
on a hypervisor-based solution at the end host.
1) Adjusting DUPACK threshold for Fast Retransmission:
One straightforward technique,
that was studied in prior
work RR-TCP[24] and TCP-PR [9], we can employ is to
dynamically adjust the TCP DUPACK threshold. In RR-TCP,
the receiver uses TCP-DSACK [13] to report the receipt of
duplicate segments to the TCP sender. Even without TCP-
DSACK, a TCP sender can assume that a fast retransmission is
spurious if it receives an acknowledgement for a retransmitted
segment in less than a fraction RTTmin [5]. In either case,
when a TCP sender detects a spurious retransmission, it can
undo the congestion window reduction that occurred in the
fast recovery phase. [8] lists many modiﬁcations to TCP to
make it robust to packet reordering that can also be used