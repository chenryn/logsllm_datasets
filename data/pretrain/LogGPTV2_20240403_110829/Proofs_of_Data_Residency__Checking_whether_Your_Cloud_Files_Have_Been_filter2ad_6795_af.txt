### A. Proof of Retrievability (PoR) Schemes

If a significant portion of the data blocks is lost, the verifier can detect this incident with overwhelming probability. A PoR scheme operates in a challenge-response fashion. The verifier \( V \) may issue a random challenge (which may contain one or multiple queries) at any time, and the prover \( P \) must respond correctly to assert its possession and the retrievability of the file.

The first construction by Juels and Kaliski [26] has been followed by various variants [20, 38] and extended to dynamic settings [37]. A similar concept known as Provable Data Possession (PDP) was proposed by Ateniese et al. [13]. It is generally believed that PDP provides weaker security guarantees than PoR because even if the prover passes the PDP audit, there is still a non-negligible probability that the verifier cannot fully recover the original outsourced file [37].

### B. Host Geolocation

While the concept of data residency ensures that data are kept intact on local drives of a storage server, it implicitly assumes that the geographic location of the storage server is known to the verifier. Therefore, techniques for geographically locating an online party are of interest. Since machines/systems on the Internet can be uniquely identified by IP addresses, the problem requires mapping an IP address to a geographic location.

If the IP address system had been designed to incorporate geographic information, this would have been straightforward. Unfortunately, this is not the case. Several proposals have been presented to address this problem [29, 24, 18]. Common among them are observations that major backbone Internet providers usually associate their host names with geographical clues, and that data traveling across the Internet are often routed via these backbone providers' nodes. Additionally, the route a data packet travels can be identified using trace engines such as Traceroute [11]. By matching the intermediary computer nodes in the routing information of a packet against those of the backbone Internet providers, a target host (the destination of the packet) can be roughly located [18]. However, this technique does not offer fine granularity. When the packet is nearing its destination, it will be transferred using smaller networks without associated geographical clues. At that level, WHOIS servers [23] are queried to infer a more precise location of the host. Other approaches rely on the premise that the latency in transmitting a packet between a pair of hosts is a function of the geographical distance between them, or a combination of partial IP-to-location and BGP prefix information to derive the target host’s location [29].

### C. Intel SGX

Intel SGX [6] is a set of extensions that provide protected execution environments (also known as trusted environments or enclaves). The Trusted Computing Base (TCB) of such enclaves consists solely of the processors and the code placed inside them by the enclave's owner, which is arguably minimal. Each enclave is associated with a region of physical memory, referred to as enclave memory. All accesses to enclave memory are protected by the processor. In other words, code and data loaded into the enclave cannot be disclosed or modified by the untrusted operating system (OS) or any other processes/software; any attempt to read or write the enclave's memory by non-enclave code will be blocked. On the other hand, enclave code can access both enclave memory and memory outside the enclave region (if the OS permits). Originally, memory pages could only be added to the enclave during its creation. However, since revision 2 of the SGX specification, enclave pages can be added via cooperation between the enclave and the (untrusted) OS [6] at any time during its lifetime. It is important to note that the enclave code must be loaded into the enclave during its creation.

Enclaves cannot directly execute OS-provided services such as I/O. To access these services, enclaves use OCalls (calls executed by the enclave code to transfer control to non-enclave code) and ECalls (APIs for untrusted applications to call into the enclave). These ECalls and OCalls form the enclave boundary interface, enabling communication between the enclave code and the untrusted application to service OS-provided functions. Care must be taken with each ECall exposed to the untrusted application, as it may open up an attack surface to the protected execution environment.

SGX enables CPU-based attestation, allowing a remote verifier to check if specific software has been loaded within the enclave using cryptographic means. Through this mechanism, the verifier can establish shared secrets with the enclave, thus bootstrapping an end-to-end encrypted channel for communicating sensitive data.

### D. Vulnerable Constructions and Potential Attacks

#### D.1 SW-PoR Based Data Residency Checking

**Protocol:**
We first consider a data residency checking protocol constructed on top of the PoR scheme by Shacham and Waters (SW-PoR) [36]. In this PoR scheme, the audit requests \( v \) data blocks and their associated homomorphic authentication tags. The response is aggregated from the requested data blocks, resulting in a much smaller size. In a SW-PoR based residency checking protocol, the verifier \( V \) measures the response latency and accepts the prover if the response is valid (with respect to the SW-PoR scheme) and the response latency is within an expected threshold.

**Dishonest Prover:**
We consider two adversaries who relocate the data to three remote storage servers and attempt to reduce response latency by speeding up the computation time required to generate the response. The first adversary, denoted by AOC, over-clocks its processor to evade detection. AOC performs the following steps upon receiving the challenge from the verifier:
1. The local server redirects the challenge to the three remote servers.
2. The three remote servers concurrently load the data and send them to the local server.
3. The local server over-clocks its processor to aggregate the data.

The second adversary, denoted by ADQ, parallelizes the aggregation in the following steps:
1. The local server redirects the challenge to the three remote servers.
2. The three remote servers concurrently load the data, aggregate them, and send the intermediate results to the local server.
3. The local server aggregates the received intermediate results.

**Empirical Results:**
We conduct experimental studies to compare the response latencies of the honest prover with those of the two adversaries. In these experiments, provers compute the responses using a vCPU Intel Xeon Family running at a base clock speed of 2.5GHz, except for AOC, who over-clocks its processor to run at a Turbo Boost speed of 3.3GHz.

We vary the block size (number of group elements in each data block) and the number of data blocks requested (i.e., audit size) in each challenge. We observe that the response latencies of the three provers generally follow normal distributions, each with different mean and standard deviation. We depict these distributions in Figure 10 by showing their means and standard deviations. To provide a better intuition about the adversaries' ability to evade latency measurements, we show in Figure 11 the cumulative distribution functions (CDFs) of their response latencies in experiments where the audit size is 700 blocks, with block sizes of 160 and 320 group elements. As can be seen from the figure, the CDFs of ADQ's latency measurements stochastically dominate those of the honest prover. Hence, ADQ can evade detection by intentionally introducing delays to the response times. Although the CDFs of AOC's latency measurements do not stochastically dominate the honest prover's, they are similar, requiring challenges of significant size to detect AOC's violation of the Service Level Agreement (SLA).

#### D.2 JK-PoR Based Residency Checking

**Protocol:**
One possible mitigation for the previous attack is to adopt a PoR scheme in which the prover performs virtually no computation during the residency checking, such as the authenticator-based PoR [26, 28]. In this scheme, the data owner pre-processes the file \( F \) using an error-erasure code to create \( \tilde{F} \), partitions \( \tilde{F} \) into \( m \) blocks, and appends a Message Authentication Code (MAC) under a secret key \( sk \) to each block before outsourcing them to the storage server. During the residency checking, the verifier issues a single request asking for \( v \ll m \) randomly chosen data blocks (the value of \( v \) is determined by the security setting of the scheme) and measures the latency incurred by the storage provider in delivering all the requested blocks.

**Dishonest Prover:**
Although it is no longer possible to speed up the response latency by over-clocking the processor or employing parallelism, a dishonest storage provider can still reduce the latency by distributing the fetching of the requested blocks. With a sufficient number of remote storage servers, the reduction in fetching time can offset the additional latency incurred by accessing the remote storage.

**Empirical Results:**
We empirically study the effectiveness of the dishonest prover. In our experiments, the honest prover \( P \) follows the protocol and keeps the user’s data in its own local drives, while the dishonest prover \( A \) distributes the data blocks to five different remote servers and pulls data blocks from these servers in parallel to the local server upon request. Each data block is appended with a 160-bit MAC. The storage servers are equipped with commodity storage hardware whose read latency ranges from 12 to 15ms on average.

We vary the number of blocks requested in each audit from 80 to 160, as well as the block size (512 and 1024 bytes), and observe that the response latencies of \( P \) and \( A \) generally follow normal distributions, each with different mean and standard variation. We show the means and standard deviations of these distributions in Figure 12. We also depict in Figure 13 their CDFs for audits of size 160 blocks. When the block size is 1024 bytes, although we do not have stochastic dominance, the two CDFs are similar. With a block size of 512 bytes, the CDF of \( A \)'s latency measurements stochastically dominates that of \( P \), implying that \( A \) can always evade detection.