User: I am writing a script in Python that establishes more than 100 parallel SSH connections, starts a script on each machine (the output is 10-50 lines), then waits for the results of the bash script and processes it. However it is run on a web server and I don't know, whether it would be better to first store the output in a file on the remote machine (that way, I suppose, I can start more remote scripts at once) and later establish another SSH connection (1 command / 1 connection) and read from those files? Now I am just reading the output but the CPU usage is really high, and I suppose the problem is, that a lot of data comes to the server at once.
Assistant: For creating lots of parallel SSH connections there is already a tool called pssh. You should use that instead.
But if we're really talking about 100 machines or more, you should really use a dedicated cluster management and automation tool such as Salt, Ansible, Puppet or Chef.