## Issue with 4-Node Task Submission

I submitted a 4-node task, with each node allocated 1 GPU. However, the task exited with an exception. Below are some of the log details and additional information:

### Log Information
- **NCCL Warning:**
  ```
  NCCL WARN Connect to 10.38.10.112 failed : Connection refused
  ```

- **Connection Retry Attempts:**
  ```
  tj1-asr-train-v100-13:941227:943077 [0] NCCL INFO Call to connect returned Connection refused, retrying
  tj1-asr-train-v100-13:941227:943077 [0] include/socket.h:390 NCCL WARN Connect to 10.38.10.112 failed : Connection refused
  ```

- **Additional NCCL Logs:**
  ```
  tj1-asr-train-v100-13:941227:943077 [0] NCCL INFO bootstrap.cc:100 -> 2
  tj1-asr-train-v100-13:941227:943077 [0] NCCL INFO bootstrap.cc:326 -> 2
  tj1-asr-train-v100-13:941227:943077 [0] NCCL INFO init.cc:695 -> 2
  tj1-asr-train-v100-13:941227:943077 [0] NCCL INFO init.cc:951 -> 2
  tj1-asr-train-v100-13:941227:943077 [0] NCCL INFO misc/group.cc:69 -> 2 [Async thread]
  ```

- **Numba Deprecation Warning:**
  ```
  /home/storage15/huangying/tools/anaconda3/envs/py36/lib/python3.6/site-packages/librosa/util/decorators.py:9: NumbaDeprecationWarning: An import was requested from a module that has moved location.
  Import of 'jit' requested from: 'numba.decorators', please update to use 'numba.core.decorators' or pin to Numba version 0.48.0. This alias will not be present in Numba version 0.50.0.
  from numba.decorators import jit as optional_jit
  ```

- **Traceback:**
  ```
  Traceback (most recent call last):
  File "/home/storage15/huangying/tools/anaconda3/envs/py36/lib/python3.6/runpy.py", line 193, in _run_module_as_main
    "main", mod_spec)
  File "/home/storage15/huangying/tools/anaconda3/envs/py36/lib/python3.6/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/storage15/huangying/tools/espnet/espnet2/bin/asr_train.py", line 23, in <module>
    main()
  File "/home/storage15/huangying/tools/espnet/espnet2/bin/asr_train.py", line 19, in main
    ASRTask.main(cmd=cmd)
  File "/home/storage15/huangying/tools/espnet/espnet2/tasks/abs_task.py", line 842, in main
    cls.main_worker(args)
  File "/home/storage15/huangying/tools/espnet/espnet2/tasks/abs_task.py", line 1174, in main_worker
    distributed_option=distributed_option,
  File "/home/storage15/huangying/tools/espnet/espnet2/train/trainer.py", line 163, in run
    else None
  File "/home/storage15/huangying/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 303, in __init__
    self.broadcast_bucket_size)
  File "/home/storage15/huangying/tools/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 485, in _distributed_broadcast_coalesced
    dist._broadcast_coalesced(self.process_group, tensors, buffer_size)
  RuntimeError: NCCL error in: /pytorch/torch/lib/c10d/ProcessGroupNCCL.cpp:410, unhandled system error, NCCL version 2.4.8
  ```

### Additional Configuration
- **Network Interface Settings:**
  - `NCCL_SOCKET_IFNAME` is set to `^lo,docker`.
  - Network interfaces on all nodes are `eth0, eth1, and lo`.

- **Node Configuration:**
  - `self.dist_backend: nccl`
  - `self.dist_init_method: file:///home/storage15/huangying/tools/espnet/egs2/voxforge/asr1/vox.init`
  - `self.dist_world_size: 4`
  - `self.dist_rank: 1`
  - `auto allocate gpu device: 0`
  - `devices ids is 0`

### Command Used
```bash
/home/storage15/huangying/tools/anaconda3/envs/py36/bin/python3 /home/storage15/huangying/tools/espnet/espnet2/bin/asr_train.py --use_preprocessor true --bpemodel none --token_type char --token_list data/token_list/char/tokens.txt --non_linguistic_symbols none --train_data_path_and_name_and_type dump/fbank_pitch/tr_en/feats.scp,speech,kaldi_ark --train_data_path_and_name_and_type dump/fbank_pitch/tr_en/text,text,text --valid_data_path_and_name_and_type dump/fbank_pitch/dt_en/feats.scp,speech,kaldi_ark --valid_data_path_and_name_and_type dump/fbank_pitch/dt_en/text,text,text --train_shape_file exp/asr_stats/train/speech_shape --train_shape_file exp/asr_stats/train/text_shape.char --valid_shape_file exp/asr_stats/valid/speech_shape --valid_shape_file exp/asr_stats/valid/text_shape.char --resume true --fold_length 800 --fold_length 150 --output_dir exp/asr_train_asr_transformer_fbank_pitch_char_normalize_confnorm_varsFalse --ngpu 1 --dist_init_method file:///home/storage15/huangying/tools/espnet/egs2/voxforge/asr1/vox.init --multiprocessing_distributed false --dist_launcher queue.pl --dist_world_size 4 --config conf/train_asr_transformer.yaml --input_size=83 --normalize=global_mvn --normalize_conf stats_file=exp/asr_stats/train/feats_stats.npz --normalize_conf norm_vars=False
```

### Summary
- The task is failing due to an NCCL connection issue, specifically a connection refusal to IP `10.38.10.112`, which is not one of the 4 nodes.
- The network interfaces and NCCL settings appear to be correctly configured.
- A Numba deprecation warning is also present, but it does not seem to be the cause of the failure.

Please let me know if you need further assistance or additional information.