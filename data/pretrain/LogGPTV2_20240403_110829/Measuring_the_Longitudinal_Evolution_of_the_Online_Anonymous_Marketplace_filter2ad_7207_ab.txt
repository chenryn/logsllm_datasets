one of the circuits being used becomes unreliable and
speeds up the time it takes to observe the entire site.
Completeness,
soundness, and instantaneousness
The goal of the data collection is to make an observa-
tion of the entire marketplace at an instantaneous point
in time, which yields information such as item listings,
pricing information, feedback, and user pages. Instan-
taneous observations are of course impossible, and can
only be approximated by scraping the marketplace as
quickly as possible. Scraping a site aggressively however
limits the stealth of the scraper; We manually identiﬁed
sites that prohibit aggressive scraping (e.g., Agora) and
imposed appropriate rate limits.
Scrape completeness is also crucial. A partial scrape
of a site may lead to underestimating the activities taking
place. Fortunately, since marketplaces leverage feedback
to build vendor reputation, old feedback is rarely deleted.
This means that it is sufﬁcient for an item listing and its
feedback to be eventually observed in order to know that
the transaction took place. Over time, the price of an
item may ﬂuctuate however, and information about when
the transaction occurred often becomes less precise, so it
is much more desirable to observe feedback as soon as
possible after it is left. We generally attempted a scrape
for each marketplace once every two to three days unless
the marketplace was either unavailable or the previous
scrape had not yet completed; having collected most of
the data we were interested in by that time, we scraped
considerably less often toward the end of our data collec-
tion interval (February through May 2015).
Many marketplaces that we observed have quite poor
reliability, with 70% uptime or lower. It is very difﬁcult
5However some marketplaces, e.g., Agora, use session cookies to
bind requests coming from different circuits, and require additional at-
tention.
36  24th USENIX Security Symposium 
USENIX Association
4
to extract entire scrapes from marketplaces suffering fre-
quent outages. This is particularly true for large sites,
where a complete scrape can take several days. As a
workaround, we designed the scraping infrastructure to
keep state and retry pages using an increasing back-off
interval for up to 24 hours. Using such a system allowed
the scraper to function despite brief outages in market-
place availability. Retrying the site after 24 hours would
be futile as in most cases, the session cookie would have
expired and the scrape would require a manual login, and
thus a manual restart.
Most marketplaces require the user to log in before
they are able to view item listings and other sensitive
information. Fortunately, creating an account on these
marketplaces is free. However, one typically needs to
solve a CAPTCHA when logging in; this was done man-
ually. The process of performing a scrape begins with
manually logging into the marketplace, extracting the
session cookie, and using it as input to the scrape to
continue scraping under that session. In many cases the
site will fail to respond to requests properly unless mul-
tiple cookies are managed or unless the user agent of
the scraper matches the user agent of the browser that
generated the cookie. We managed to emulate typical
browser behavior in all but one case (BlueSky). We were
unable to collect meaningful data on BlueSky, as an anti-
scraping measure on the server side was to annihilate any
session after approximately 100 page requests, and get
the user to log in again.
3.2 Parsing marketplaces
The raw page data collected by the scraper needs to be
parsed to extract information useful for analysis. The
parser ﬁrst identiﬁes which marketplace a particular page
was scraped from; it then determines which type of page
is being analyzed (item listing, user page, feedback page,
or any combination of those).
Each page is then parsed using a set of heuristics we
manually devised for each marketplace. We treat the in-
formation extracted as a single observation and record it
into a database. Information that does not exist or cannot
be parsed is assigned default values.
The heuristics for parsing can often become quite
complicated as many marketplaces observed over long
periods of time went through several iterations of page
formats. This justiﬁed our conscious decision to decou-
ple scraping from parsing so that we could minimize
data loss. Because of the high manual effort associ-
ated with creating and debugging new parsers for market-
places, we only generated parsers for marketplaces that
we perceived to be of signiﬁcance. While observing the
scrapes of several marketplaces, it became apparent that
their volume was either extremely small (<$1,000) or
was not measurable by observing the website (e.g., be-
cause feedback is not mandatory). These marketplaces
were omitted without greatly affecting the overall pic-
ture; their analysis is left for future work.
Internally validating data analysis
3.3
To ensure that the analysis we performed was not biased,
and as a safety against egregious errors, both authors
of this paper concurrently and independently developed
multiple implementations of the analysis we present in
the next section. During that stage of the work, the two
authors relied on the same data sources, but used different
analysis code and tools and did not communicate with
each other until all results were produced.
We then internally conﬁrmed that the independent esti-
mations of total market volumes varied by less than 10%
at any single point in time, and less than 5% on aver-
age, well within expected margin of errors for data in-
directly estimated from potentially noisy sources (user
feedback).6 The independent reproducibility of the anal-
ysis is important since, as we will show, estimating mar-
ket volumes presents many pitfalls, such as the risk of
double-counting observations or using a holding price as
the true value of an item.
3.4 Validating data completeness
The poor availability of certain marketplaces (e.g.,
Agora), combined with the large amount of time needed
to fully scrape very large marketplaces raises concerns
about data completeness. We attempt to estimate the
amount of data that might be missing through a process
known as marking and recapturing.
The basic idea is as follows. Consider that a given site
scrape at time t contains a number M of feedback. Since
we do not know whether the scrape is complete, we can
only assert that M is a lower bound on the total num-
ber of feedback F actually present on the site at time t.
Now, consider a second scrape (presumably taken after
time t), which contains n pieces of feedback left at or be-
fore time t. The number n is another lower bound of F.
We then estimate F as ˆF = nM/m, where m is the num-
ber of feedback captured in the ﬁrst scrape that we also
observe in the second scrape (m ≤ M).
The Schnabel estimator [36] extends the above tech-
nique to estimate the size of a population to multiple
samples, and is thus well-suited to our measurements.
For n samples, if we denote by Ct the number of feed-
back in sample t, by Mt the total number of unique previ-
ously observed feedback in sample (t − 1), and by Rt the
6These minor discrepancies can be attributed to slightly different
ﬁltering heuristics, which we discuss later.
USENIX Association  
24th USENIX Security Symposium  37
5
e
g
a
r
e
v
o
C
1.0
0.8
0.6
0.4
Marketplace
Agora
Evolution
Silk Road
Silk Road 2
20
0
5
10
Number of scrapes
15
k
c
a
b
d
e
e
F
f
o
s
e
c
e
P
i
f
o
r
e
b
m
u
N
l
a
t
o
T
300,000
200,000
100,000
Observed
Estimate
0
Jan 2014 Apr 2014 Jul 2014 Oct 2014 Jan 2015 Apr 2015
Date
Figure 2: Coverage of Agora, Silk Road 1, Silk Road
2, and Evolution. This plot estimates the fraction of all feed-
back we obtain for a given time, as a function of the number of
scrapes we collect.
Figure 3: Observed and estimated number of feed-
back present on Agora over time. The lower and upper
bounds for the estimate are nearly indistinguishable from the
estimate itself.
number of previously observed feedback during sample
t, we estimate the total number of feedback at time t as:
towards the very end of our dataset, which will require
us to censor some of this data when estimating volumes.
ˆF =
∑n
t=1CtMt
∑n
t=1 Rt
.
4 Analysis
The Schnabel estimator implicitly assumes that the
distribution is time-invariant and that samples are drawn
uniformly. To help ensure time invariance, the estima-
tor begins with a sample at time t. Pieces of feedback
with timestamps greater than t are omitted from all sam-
ples taken in the future (t + τ). It is also important not to
consider samples from too far into the future since items
are occasionally de-listed and the corresponding feed-
back destroyed. To help minimize the impact of feed-
back deleted in the future, we only use samples within
60 days of t in our estimate.
We illustrate this estimate in Figure 2 for Agora, Silk
Road 1, Silk Road 2, and Evolution after multiple obser-
vations have been made. Agora has relatively poor relia-
bility and, on average, a single scrape will not manage to
capture even half of the feedback present at that time on
the site. On other marketplaces it is typical on the ﬁrst
visit to see as much as 60% of the entire population, or
higher. After ten or more independent scrapes, we can
expect to obtain a dataset that approaches 90% coverage
or higher.
Figure 3 further illustrates our point, by comparing the
number of pieces of feedback observed on Agora to its
estimate. For most of the observed lifetime of Agora,
the data that we have is very close to what we estimate
the total to be. This is because information about a mar-
ketplace at a particular (past) point in time beneﬁts from
subsequent observations. Most recent observations do
not have this beneﬁt and therefore suffer from poor cov-
erage, leading to signiﬁcant divergence from their esti-
mate. This results in potentially large underestimations
We next turn to data analysis. We ﬁrst estimate the over-
all evolution of the sales volumes in the entire ecosystem
over the past couple of years. We then move to an assess-
ment of the types of products being sold over time. Last,
we discuss ﬁndings about vendor activity and techniques.
4.1 Sales volumes
The ﬁrst important question that our analysis answers is
how much product in terms of money is being bought and
sold on online anonymous marketplaces. While we can-
not directly measure the money being transacted from
buyers to sellers, or packages being shipped from ven-
dors to customers, we do make frequent observations of
product feedback left for particular item listings on the
marketplaces. Similar to prior work [13], we use these
observations of feedback as a proxy to estimate a lower
bound for sales.
Caveats
In many marketplaces (e.g., Silk Road, Silk
Road 2.0, Agora, Evolution among others) customers are
required to leave feedback for a vendor whenever they re-
ceive their order of one of the vendor’s items. An order
for an item may be of varying quantity, so a customer
that purchases a single quantity of a product, and a cus-
tomer that purchases multiple quantities of a product will
both leave a single feedback. In an effort to be conser-
vative, we make the assumption that for every feedback
observed, only a single quantity was purchased.
Our prudent strategy of estimating sales volume from
conﬁrmed observations of feedback diverges from other,
38  24th USENIX Security Symposium 
USENIX Association
6
simpler approaches, such as counting the number of item
listings offered (see, e.g., [15]). For instance, over the
observed lifetime of Evolution, a few of the most suc-
cessful item listings had feedback entries that indicated
over 1 million dollars had been spent on each of them.
The presence of these highly inﬂuential item listings sug-
gests that simply counting the total number of listings on
a site is a very poor indicator of sales volume. This claim
is compounded by the observation that the average sales
per item listing per day on Evolution in early July of 2014
was $85.14; but by September 2014, after new vendors
and item listings had entered, the sales per item listing
had declined to $19.42. Such volatile behavior is par-
ticularly common in marketplaces that are small or are
going through periods of rapid growth.
Estimation We derived the estimates for the total
amount of money transacted in three steps. We ﬁrst
took the set of all feedback observations that had been
collected and removed any duplicates. For example, on
two consecutive scrapes of a particular marketplace, the
same item listing and its entire feedback history were
observed and recorded twice.
It would be incorrect to
count two different observations of the same feedback
twice. We thus developed a criterion for uniqueness
for each marketplace—typically enforcing uniqueness of
ﬁelds such as feedback message body, the vendor for
which the feedback was left, the title of the item list-
ing and the approximate date the feedback was left. Two
pieces of feedback are considered different if and only if
they differ in at least one of these categories.
The second step was to identify the the point in time at
which the feedback was left. This time is an upper bound
on when the transaction occurred. We obtained this esti-
mate by noting the time of the observation and utilizing
any information available about the age of the feedback.
Different marketplaces have varying precision informa-
tion about feedback timestamps. In the most precise in-
stances, the time that the feedback was left is speciﬁed
within the hour; in the most ambiguous cases, we can
only infer the month in which feedback was deposited.
Fortunately, due to our rather high sampling rate of the
marketplaces, in most instances we have roughly a 24-
hour accuracy on feedback time.
The third and ﬁnal step is to identify the value of the
transaction that each feedback represents. This involves
pairing each feedback observation with a single obser-
vation of an item listing and its advertised price. Care-
ful attention must be paid here as a few caveats exist,
namely that the advertised price of an item listing varies
with time, and that, in some rare cases, the correspond-
ing item is never observed, leaving us unable to identify
the value of the transaction.
Item prices change for two different reasons. The ﬁrst
and most common reason is that the vendors responsi-
n
o
1.00
i
t
c
n
u
f
n
o
0.75
i
t
u
b
i
r
t
s
d
i
0.50
e
v
i
t
l
a
u
m
u
C
0.25
0.00
Treatment
Heuristic A
Heuristic B
No filtering
0.0
0.2
0.4
Coefficient of variation
0.6
Figure 4: C.d.f. of Coefﬁcient of Variation for sets of
observations of item listings Both heuristics perform very
similarly.
ble for selling items are subject to standard free market
pressures and may raise or lower their prices in response
to competition, supply, demand, or other factors. The