### Data Collection and Analysis for Online Marketplaces

#### 1. Data Collection Goals and Challenges

The primary goal of data collection is to observe the entire marketplace at a single point in time, capturing information such as item listings, pricing, feedback, and user pages. Achieving instantaneous observations is impractical, so we approximate this by scraping the marketplace as quickly as possible. However, aggressive scraping can compromise the scraper's stealth. We manually identified marketplaces that prohibit aggressive scraping (e.g., Agora) and imposed appropriate rate limits.

**Completeness, Soundness, and Timeliness:**
- **Completeness:** A partial scrape may lead to underestimating activities. Fortunately, old feedback is rarely deleted, making it sufficient to eventually observe an item listing and its feedback to confirm a transaction.
- **Timeliness:** Prices fluctuate over time, and the timing of transactions becomes less precise. Therefore, it is crucial to observe feedback as soon as possible after it is left.

**Scraping Frequency:**
- We generally scraped each marketplace every two to three days, unless the marketplace was unavailable or the previous scrape was incomplete. As the data collection period progressed (February through May 2015), the frequency decreased as most of the desired data had been collected.

**Marketplace Reliability:**
- Many marketplaces have poor reliability, with uptime as low as 70%. Frequent outages make complete scrapes challenging, especially for large sites. To mitigate this, our scraping infrastructure keeps state and retries pages with an increasing back-off interval for up to 24 hours. This allows the scraper to function despite brief outages.

**Login and CAPTCHA:**
- Most marketplaces require logging in to view sensitive information. Account creation is free, but manual CAPTCHA solving is necessary. The process involves manually logging in, extracting the session cookie, and using it for the scrape. Managing multiple cookies and matching the user agent of the browser is often required.

**Anti-Scraping Measures:**
- Some marketplaces, like BlueSky, implement anti-scraping measures. For instance, sessions are terminated after approximately 100 page requests, requiring a manual login and restart.

#### 2. Parsing Marketplaces

**Data Parsing:**
- Raw page data collected by the scraper needs parsing to extract useful information. The parser first identifies the marketplace and the type of page (item listing, user page, feedback page, or a combination).
- Heuristics are manually devised for each marketplace to parse the data. The extracted information is recorded into a database, with default values assigned to missing or unparsable data.

**Challenges in Parsing:**
- Marketplaces frequently change their page formats, making heuristics complex. To minimize data loss, we decoupled scraping from parsing. Due to the high manual effort, parsers were only created for significant marketplaces.

**Validation:**
- Both authors independently developed and tested the analysis code to ensure unbiased results. The independent estimations of total market volumes varied by less than 10% at any single point in time and less than 5% on average, within expected margins of error.

#### 3. Data Completeness and Validation

**Estimating Missing Data:**
- Poor availability and long scraping times raise concerns about data completeness. We use a marking and recapturing method to estimate missing data. This involves comparing feedback counts across multiple scrapes to estimate the total number of feedback present at a given time.

**Schnabel Estimator:**
- The Schnabel estimator extends the marking and recapturing technique to multiple samples, providing a robust method for estimating the size of the population. We apply this to estimate the total number of feedback, considering time-invariant distributions and uniform sampling.

**Coverage Analysis:**
- Figures 2 and 3 illustrate the coverage of feedback on various marketplaces. For example, Agora has relatively poor reliability, and a single scrape captures less than half of the feedback. Multiple scrapes improve coverage, approaching 90% or higher.

#### 4. Data Analysis

**Sales Volumes:**
- We estimate sales volumes by observing product feedback. While we cannot directly measure transactions, feedback provides a proxy for a lower bound of sales.
- Caveats include the requirement for customers to leave feedback, and the assumption that each feedback represents a single quantity purchase. This conservative approach differs from simpler methods like counting item listings.

**Estimation Steps:**
1. **Remove Duplicates:** Ensure feedback is unique by enforcing criteria such as message body, vendor, item title, and date.
2. **Identify Feedback Time:** Determine the upper bound of the transaction time using available timestamp information.
3. **Determine Transaction Value:** Pair feedback with item listings and advertised prices, accounting for price changes over time.

**Caveats in Price Changes:**
- Item prices vary due to market pressures, competition, supply, and demand. Careful attention is needed to match feedback with the correct price.

By following these steps, we aim to provide a comprehensive and accurate analysis of online marketplace activities.