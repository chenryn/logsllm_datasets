# Benchmarking Personal Cloud Storage

## Authors
- Idilio Drago
- Enrico Bocchi
- Marco Mellia
- Herman Slatman
- Aiko Pras

### Affiliations
- Politecnico di Torino, Italy
- University of Twente, the Netherlands

### Publication Details
- **Conference**: ACM IMCâ€™13 - Internet Measurement Conference, Barcelona, Spain
- **Date**: October 2013
- **Pages**: 205-212
- **Publisher**: ACM
- **DOI**: 10.1145/2504730.2504762
- **Availability**: [Porto Institutional Repository](http://porto.polito.it/2519105/) (Available since November 2013)

### Abstract
Personal cloud storage services are data-intensive applications that already contribute significantly to Internet traffic. Various companies offer these services, attracting an increasing number of users. However, little is known about the capabilities, architecture, and performance implications of these services. This paper presents a methodology to study cloud storage services. We apply our methodology to compare five popular services, revealing different system architectures and capabilities. The performance implications of these designs are assessed through a series of benchmarks. Our results show that no single service outperforms all others, with each suffering from certain limitations or poor implementations. In some scenarios, the upload time for the same file set can be seven times longer and consume twice as much capacity. Our methodology and results serve as both a benchmark and a guideline for system design.

### Categories and Subject Descriptors
- C.2 [Computer-Communication Networks]: Miscellaneous
- C.4 [Performance of Systems]: Measurement Techniques

### General Terms
- Measurement
- Performance

### Keywords
- Cloud Storage
- Internet Measurement

## 1. Introduction
Personal cloud storage services allow users to synchronize local folders with servers in the cloud. These services have gained popularity due to the significant amounts of remote storage offered for free or at reduced prices. More people are being attracted to these offers, using them to save personal files, synchronize devices, and share content with great simplicity. This high public interest has led various providers to enter the cloud storage market, making services like Dropbox, SkyDrive, and Google Drive increasingly pervasive in daily routines. Such applications are data-intensive and their increasing usage already contributes significantly to Internet traffic [3].

Previous studies on Dropbox [3] indicate that design and architectural choices strongly influence service performance and network usage. However, there is limited knowledge about how other providers implement their services and the implications of different designs. Understanding these aspects is valuable for building better-performing services that efficiently use network resources.

This paper has two main goals:
1. Investigate how different providers handle the problem of synchronizing users' files. To achieve this, we develop a methodology to understand both system architecture and client capabilities. We apply this methodology to compare five services, revealing differences in client software, synchronization protocols, and data center placement.
2. Examine the performance consequences of these designs. We define a series of benchmarks to assess the performance of each selected service from the perspective of users connected from Europe, highlighting differences in various usage scenarios.

Our results extend those in [3], where Dropbox usage is analyzed from passive measurements. Unlike previous work and [12, 16], which focus on a specific service, this paper compares several solutions using active measurements. The results in [3] guide our benchmarking definition. The authors of [11] benchmark cloud providers but focus only on server infrastructure. Similarly to our goal, [9] evaluates Dropbox, Mozy, Carbonite, and CrashPlan. Motivated by the extensive list of providers, we first propose a methodology to automate the benchmarking. Then, we analyze several synchronization scenarios and providers, shedding light on the impact of design choices on performance.

Our results reveal interesting surprises, from poor design choices to unexpected drops in performance in common scenarios. Overall, the lessons learned are useful for improving design choices and performance. All results and our benchmarking tool will be made available to the public to compare results from different locations and to extend the number of tested services.

## 2. Methodology and Services
This section describes the methodology we follow to design benchmarks to check the capabilities and performance of personal storage services. We use active measurements relying on a testbed composed of two parts: 
1. A test computer that runs the application-under-test in the desired operating system.
2. Our testing application, which can run either on the same test computer or on a separate one, provided it can intercept traffic from the test computer. Virtual machines can also be used, e.g., by running the test computer as a virtual host in a controller host. We used this setup in this paper.

Our testing application receives benchmarking parameters describing the sequence of operations to be performed. It acts remotely on the test computer, generating specific workloads in the form of file batches, which are manipulated using an FTP client. Files of different types are created or modified at runtime, such as text files composed of random words from a dictionary, images with random pixels, or random binary files. Generated files are synchronized to the cloud by the application-under-test, and the exchanged traffic is monitored to compute performance metrics, including the amount of traffic, the time before actual synchronization starts, and the time to complete synchronization.

### 2.1 Architecture and Data Centers
The architecture, data center locations, and data center ownership are important aspects of personal cloud storage, with both legal and performance implications. To identify how the analyzed services operate, we observe the DNS names of contacted servers during three states: 
1. Starting the application.
2. Immediately after files are manipulated.
3. When the application is idle.

For each service, a list of contacted DNS names is compiled. To reveal all IP addresses of the front-end nodes used by a service, DNS names are resolved to IP addresses by contacting more than 2,000 open DNS resolvers spread around the world. Cloud services rely on DNS to distribute workload, returning different IP addresses based on the originating DNS resolver [2]. The owners of the IP addresses are identified using the whois service. For each IP address, we determine the geographic location of the server. Since popular geolocation databases have limitations regarding cloud providers [14], we use a hybrid methodology that includes:
1. Informative strings (e.g., International Airport Codes) revealed by reverse DNS lookup.
2. The shortest Round Trip Time (RTT) to PlanetLab nodes [15].
3. Active traceroute to spot the closest well-known location of a router.

Previous works [2, 5] indicate that these methodologies provide an estimation with about a hundred kilometers of precision, which is sufficient for our goals.

### 2.2 Checking Capabilities
Our previous work [3] shows that personal storage applications can implement several capabilities to optimize storage usage and speed up transfers. These capabilities include:
- Chunking (splitting content into maximum size data units)
- Bundling (transmitting multiple small files as a single object)
- Deduplication (avoiding re-transmitting content already available on servers)
- Delta encoding (transmitting only modified portions of a file)
- Compression

For each capability, a specific test is designed to observe if it is implemented. Our testing application produces specific batches of files that would benefit from a given capability, and the exchanged traffic is analyzed to determine how the service operates.

### 2.3 Benchmarking Performance
After understanding the services' design in terms of data center locations and system capabilities, we check how these choices influence synchronization performance and the amount of overhead traffic. We perform eight experiments, each repeated 24 times per service, with at least five minutes between experiments to avoid creating abnormal workloads on servers. The benchmark of a single storage service lasts about one day.

### 2.4 Tested Storage Services
We focus on five services in this paper, although our methodology is generic and can be applied to any other service. We restrict our analysis to native clients, as our previous results show that this is the preferred means of using these services. The selected services are:
- Dropbox [4]
- Google Drive [6]
- SkyDrive [13]
- Wuala [10]
- Cloud Drive [1]

Dropbox, Google Drive, and SkyDrive are among the most popular offerings, according to Google Trends [8]. Wuala is included because it offers client-side encryption, allowing us to verify the impact of this privacy layer on synchronization performance. Finally, we include Cloud Drive to compare its performance with Dropbox, as both services use Amazon Web Services (AWS) data centers.

A Linux box is used to control the experiment and host the virtual machine running the test computer (Windows 7 Enterprise). Our testbed is connected to a 1GB/s Ethernet network at the University of Twente, ensuring that the network is not a bottleneck.

## 3. System Architecture

### 3.1 Protocols
All clients exchange traffic using HTTPS, except for Dropbox's notification protocol, which relies on plain HTTP. Interestingly, some Wuala storage operations also use HTTP, as users' privacy is secured by local encryption. All services except Wuala use separate servers for control and storage. Their identification is straightforward by monitoring the traffic exchanged when the client starts, is idle, and synchronizes files. Both server names and IP addresses can be used to identify different operations during our tests. For Wuala, we use flow sizes and connection sequences to identify storage flows.

We notice some relevant differences among applications during login and idle phases. Figure 1 reports the cumulative number of bytes exchanged with control servers over the initial 16 minutes. Two key observations are:
1. Applications authenticate the user and check for updates. SkyDrive requires about 150 kB in total, four times more than others, as it contacts many Microsoft Live servers during login (13 in this example).
2. Once login is completed, applications continue exchanging data with the cloud. Wuala is the most silent, polling servers every 5 minutes on average, equivalent to a background traffic of about 60 b/s. Google Drive follows closely, with a lightweight 40-second polling interval (42 b/s). Dropbox and SkyDrive use intervals close to 1 minute (82 b/s and 32 b/s, respectively). Amazon Cloud Drive behaves differently, polling every 15 seconds, each time opening a new HTTPS connection. This consumes 6 kb/s, i.e., about 65 MB per day. This information is relevant for users with bandwidth constraints (e.g., in 3G/4G networks) and for the system: 1 million users would generate approximately 6 Gb/s of signaling traffic alone. We believe this is a poor implementation that will likely be fixed in future releases.

### 3.2 Data Centers
Next, we analyze data center locations:
- **Dropbox** uses its own servers in the San Jose area for client management, while storage servers are hosted by Amazon in Northern Virginia.
- **Cloud Drive** uses three AWS data centers: two in Ireland and Northern Virginia for both storage and control, and a third in Oregon for storage only.
- **SkyDrive** relies on Microsoftâ€™s data centers in the Seattle area (for storage) and Southern Virginia (for storage and control), with a destination in Singapore (for control only).
- **Wuala** data centers are located in Europe: two in the Nuremberg area, one in Zurich, and a fourth in Northern France. None are owned by Wuala.
- **Google Drive** is different: TCP traffic is terminated at the closest Google edge node, from where it is routed to the actual storage/control data center using Googleâ€™s private network. Figure 2 shows the locations identified in our experiments, with over 100 different entry points. This architecture reduces client-server RTT and offloads storage traffic from the public Internet.

## 4. Cloud Service Capabilities

### 4.1 Chunking
Our first test aims to understand how services process large files. By monitoring throughput during the upload of files of different sizes, we determine whether files are exchanged as single objects or split into chunks, each delimited by a pause. Our experiments show that only Cloud Drive does not perform chunking. Google Drive uses 8 MB chunks, while Dropbox uses 4 MB chunks. SkyDrive and Wuala apparently change chunk sizes. Chunking simplifies upload recovery in case of failures and benefits users on slow networks.

### 4.2 Bundling
When a batch of files needs to be transferred, they could be bundled and pipelined to reduce transmission latency and control overhead. The benchmark consists of four upload sets, each containing the same amount of data split into 1, 10, 100, or 1000 files, respectively.

These experiments reveal a variety of synchronization strategies. Surprisingly, Google Drive and Cloud Drive open a separate TCP (and SSL) connection for each file. Cloud Drive opens three TCP/SSL control connections per file operation. When several files need to be exchanged, such a design strongly limits system performance due to TCP and SSL negotiations. Figure 3 shows the number of TCP SYN packets observed when Google Drive and Cloud Drive have to store 100 files of 10 kB each: 100 and 400 connections are opened, respectively, requiring 30 seconds and 55 seconds to complete the upload.

Other services reuse TCP connections more efficiently. However, SkyDrive and Wuala submit files sequentially, waiting for application layer acknowledgments.