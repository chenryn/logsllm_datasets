title:Benchmarking personal cloud storage
author:Idilio Drago and
Enrico Bocchi and
Marco Mellia and
Herman Slatman and
Aiko Pras
Politecnico di Torino
Porto Institutional Repository
[Proceeding] Benchmarking personal cloud storage
Benchmarking
In: ACM IMC’13 - Internet Measurement Conference, Barcelona, SP,
Original Citation:
Idilio Drago;Enrico Bocchi;Marco Mellia;Herman Slatman;Aiko Pras (2013).
personal cloud storage.
October 2013. pp. 205-212
Availability:
This version is available at : http://porto.polito.it/2519105/ since: November 2013
Publisher:
ACM
Published version:
DOI:10.1145/2504730.2504762
Terms of use:
This article is made available under terms and conditions applicable to Open Access Policy Article
("Public - All rights reserved") , as described at http://porto.polito.it/terms_and_conditions.
html
Porto, the institutional repository of the Politecnico di Torino, is provided by the University Library
and the IT-Services. The aim is to enable open access to all the world. Please share with us how
this access beneﬁts you. Your story matters.
(Article begins on next page)
Benchmarking Personal Cloud Storage
Enrico Bocchi, Marco Mellia
Idilio Drago, Aiko Pras, Herman Slatman
Politecnico di Torino, Italy
PI:EMAIL
PI:EMAIL
University of Twente, the Netherlands
{i.drago,a.pras}@utwente.nl
PI:EMAIL
ABSTRACT
Personal cloud storage services are data-intensive applica-
tions already producing a signiﬁcant share of Internet traf-
ﬁc. Several solutions offered by different companies attract
more and more people. However, little is known about each
service capabilities, architecture and – most of all – perfor-
mance implications of design choices. This paper presents
a methodology to study cloud storage services. We apply
our methodology to compare 5 popular services, revealing
different system architectures and capabilities. The implica-
tions on performance of different designs are assessed ex-
ecuting a series of benchmarks. Our results show no clear
winner, with all services suffering from some limitations or
poor implementations. In some scenarios, the upload time
of the same ﬁle set can take seven times more, wasting twice
as much capacity. Our methodology and results are useful
thus as both benchmark and guideline for system design.
Categories and Subject Descriptors
C.2 [Computer-Communication Networks]: Mis-
cellaneous; C.4 [Performance of Systems]: Measure-
ment Techniques
General Terms
Measurement, Performance
Keywords
Cloud Storage, Internet Measurement.
1.
INTRODUCTION
Personal cloud storage services allow to synchronize
local folders with servers in the cloud. They have gained
popularity, with companies oﬀering signiﬁcant amounts
of remote storage for free or reduced prices. More and
more people are being attracted by these oﬀers, saving
personal ﬁles, synchronizing devices and sharing con-
tent with great simplicity. This high public interest
pushed various providers to enter the cloud storage mar-
ket. Services like Dropbox, SkyDrive and Google Drive
are becoming pervasive in people’s routine. Such appli-
1
cations are data-intensive and their increasing usage al-
ready produces a signiﬁcant share of Internet traﬃc [3].
Previous results about Dropbox [3] indicate that de-
sign and architectural choices strongly inﬂuence service
performance and network usage. However, very little is
known about how other providers implement their ser-
vices and the implications of diﬀerent designs. This un-
derstanding is valuable as a guideline for building better
performing services that wisely use network resources.
The goal of this paper is twofold. Firstly, we investi-
gate how diﬀerent providers tackle the problem of syn-
chronizing people’s ﬁles. For answering this question,
we develop a methodology that helps to understand
both system architecture and client capabilities. We
apply our methodology to compare 5 services, revealing
diﬀerences on client software, synchronization protocols
and data center placement. Secondly, we investigate the
consequences of such designs on performance. We an-
swer this second question by deﬁning a series of bench-
marks. Taking the perspective of users connected from
Europe, we benchmark each selected service, highlight-
ing diﬀerences manifested in various usage scenarios.
Our results extend [3], where Dropbox usage is an-
alyzed from passive measurements. In contrast to our
previous work and [12, 16] that focus on a speciﬁc ser-
vice, this paper compares several solutions using active
measurements. The results in [3] are used to guide our
benchmarking deﬁnition. The authors of [11] bench-
mark cloud providers, but focusing only on server infras-
tructure. Similarly to our goal, [9] evaluates Dropbox,
Mozy, Carbonite and CrashPlan. Motivated by the ex-
tensive list of providers, we ﬁrst propose a methodology
to automate the benchmarking. Then, we analyze sev-
eral synchronization scenarios and providers, shedding
light on the impact of design choices on performance.
Our results reveal interesting surprises, from poor
design choices, to unexpected drop in performance in
common scenarios. Overall, the lesson learned is useful
to improve design choices and performance. All results
and our benchmarking tool will be available to the pub-
lic to compare results from diﬀerent locations and to
extend the number of tested services.
2. METHODOLOGY AND SERVICES
This section describes the methodology we follow to
design benchmarks to check capabilities and perfor-
mance of personal storage services. We use active mea-
surements relying on a testbed composed of two parts:
i) a test computer that runs the application-under-test
in the desired operating system; and ii) our testing ap-
plication. The testing application can run either in the
same test computer or in a separate one provided that
it can intercept traﬃc from the test computer. Virtual
machines can be used too, e.g., by running the test com-
puter as a virtual host in a controller host. We used this
latter setup in this paper.
Our testing application receives as input benchmark-
ing parameters describing the sequence of operations to
be performed. The testing application acts remotely
on the test computer, generating speciﬁc workloads in
the form of ﬁle batches, which are manipulated using a
FTP client. Files of diﬀerent types are created or mod-
iﬁed at run-time, e.g., text ﬁles composed of random
words from a dictionary, images with random pixels, or
random binary ﬁles. Generated ﬁles are synchronized
to the cloud by the application-under-test and the ex-
changed traﬃc is monitored to compute performance
metrics. These include the amount of traﬃc seen during
the experiments, the time before actual synchronization
starts, the time to complete synchronization, etc.
2.1 Architecture and Data Centers
The used architecture, data center locations and data
center owner are important aspects of personal cloud
storage. They have both legal and performance impli-
cations. To identify how the analyzed services operate,
we observe the DNS name of contacted servers when i)
starting the application; ii) immediately after ﬁles are
manipulated; and iii) when the application is in idle
state. For each service, a list of contacted DNS names
is compiled.
To reveal all IP addresses of the front-end nodes used
by a service, DNS names are resolved to IP addresses by
contacting more than 2,000 open DNS resolvers spread
around the world.1 In fact, cloud services rely on the
DNS to distribute workload, returning diﬀerent IP ad-
dresses according to the originating DNS resolver [2].
The owners of the IP addresses are identiﬁed using
the whois service. For each IP address, we look for
the geographic location of the server. Since popular
geolocation databases are known to have serious limita-
tions regarding cloud providers [14], we rely on a hybrid
methodology that makes use of: i) informative strings
(i.e., International Airport Codes) revealed by reverse
DNS lookup; ii) the shortest Round Trip Time (RTT)
to PlanetLab nodes [15]; and iii) active traceroute to
spot the closest well-known location of a router. Previ-
ous works [2, 5] indicate that these methodologies pro-
vide an estimation with about a hundred of kilometers
of precision, which is suﬃcient for our goals.
2.2 Checking Capabilities
Our previous work [3] shows that personal storage
applications can implement several capabilities to op-
timize storage usage and to speed up transfers. These
capabilities include the adoption of chunking (i.e., split-
ting content into a maximum size data unit), bundling
(i.e., the transmission of multiple small ﬁles as a sin-
gle object), deduplication (i.e., avoiding re-transmitting
content already available on servers), delta encoding
(i.e., transmission of only modiﬁed portions of a ﬁle)
and compression.
For each case, a speciﬁc test has been designed to
observe if the given capability is implemented. We de-
scribe each test directly in Sect. 4.
In summary, our
testing application produces speciﬁc batches of ﬁles that
would beneﬁt from a capability. The exchanged traﬃc
is analyzed to determine how the service operates.
2.3 Benchmarking Performance
After knowing how the services are designed in terms
of both data center locations and system capabilities, we
check how such choices inﬂuence synchronization per-
formance and the amount of overhead traﬃc.
In total, we perform 8 experiments in which ﬁles of
diﬀerent sizes and formats are synchronized. Each ex-
periment is repeated 24 times per service, allowing at
least 5 minutes between experiments to avoid creating
abnormal workloads to servers. The benchmark of a
single storage service lasts for about 1 day.
2.4 Tested Storage Services
We focus on 5 services in this paper for the sake of
space, although our methodology is generic and can be
applied to any other service. We restrict our analysis to
native clients, since our previous results show that this
is the largely preferred means to use these services.
Dropbox [4],2 Google Drive [6] and SkyDrive [13]
are selected because they are among the most popu-
lar oﬀers, according to the volume of search queries
containing names of cloud storage services on Google
Trends [8]. Wuala [10] is considered because it is a sys-
tem that oﬀers encryption at the client-side. We want to
verify the impact of such privacy layer on synchroniza-
tion performance. Finally, we include Cloud Drive [1]
to compare its performance to Dropbox, since both ser-
vices rely on Amazon Web Services (AWS) data centers.
A Linux box is used both to control the experiment
and to host the virtual machine running the test com-
1The list has been manually compiled from various sources
and covers more than 100 countries and 500 ISPs.
2Exact tested versions and URLs for downloading can be
found in the References.
2
Dropbox
SkyDrive
Wuala
Cloud Drive
Google Drive
 900
)
 600
B
k
(
c
i
f
f
a
r
T
 300
 0
 0
 2
 4
 6
 8
 10
 12
 14
 16
Time (min)
Figure 1: Background traﬃc while idle.
puter (Windows 7 Enterprise)3. Our testbed is con-
nected to a 1GB/s Ethernet network at the University
of Twente, in which the network is not a bottleneck.
3. SYSTEM ARCHITECTURE
3.1 Protocols
All clients exchange traﬃc using HTTPS, with the
exception of Dropbox notiﬁcation protocol, which relies
on plain HTTP. Interestingly, some Wuala storage op-
erations also use HTTP, since users’ privacy has already
been secured by local encryption.
All services but Wuala use separate servers for control
and storage. Their identiﬁcation is trivial by monitor-
ing the traﬃc exchanged when the client i) starts; ii)
is idle; and iii) synchronizes ﬁles. Both server names
and IP addresses can be used to identify diﬀerent op-
erations during our tests. For Wuala, we use ﬂow sizes
and connection sequences to identify storage ﬂows.
We notice some relevant diﬀerences among applica-
tions during login and idle phases. Fig. 1 reports the
cumulative number of bytes exchanged with control
servers considering the initial 16 min. Two consider-
ations hold: Firstly, the applications authenticate the
user and check if any content has to be updated. No-
tice how SkyDrive requires about 150 kB in total, 4
times more than others. This happens because the ap-
plication contacts many diﬀerent Microsoft Live servers
during login (13 in this example). Secondly, once login
is completed, applications keep exchanging data with
the cloud. Wuala is the most silent, polling servers ev-
ery 5 min on average, i.e., equivalent background traﬃc
of about 60 b/s. Google Drive follows close, with a
lightweight 40 s polling interval (42 b/s). Dropbox and
SkyDrive use intervals close to 1 min (82 b/s and 32 b/s,
respectively). The behavior of Amazon Cloud Drive is
completely diﬀerent: polling is done every 15 s, each
time opening a new HTTPS connection. This consumes
6 kb/s, i.e., about 65 MB per day! This information
is relevant, for instance, to users with bandwidth con-
straints (e.g., in 3G/4G networks), and to the system:
3Clients on OS X and Linux were also checked whether avail-
able and showed no signiﬁcant diﬀerences.
Figure 2: Google Drive’s edge nodes.
1 million users would generate approximately 6 Gb/s of
signaling traﬃc alone! We believe this is a bad imple-
mentation that will be ﬁxed in next releases.
3.2 Data Centers
Next, we analyze data center locations. Dropbox uses
own servers (in the San Jose area) for client manage-
ment, while storage servers are committed to Amazon
in Northern Virginia. Cloud Drive uses three AWS data
centers: two are used for both storage and control (in
Ireland and Northern Virginia); a third one is used for
storage only (in Oregon). SkyDrive relies on Microsoft’s
data centers in the Seattle area (for storage) and South-
ern Virginia (for storage and control). We also identi-
ﬁed a destination in Singapore (for control only). Not
surprisingly, most data centers are located in the U.S.
Wuala data centers instead are located in Europe: two
in the Nuremberg area, one in Zurich and a fourth in
Northern France. None is owned by Wuala. All these
services follow a centralized design where clients contact
the servers using the public Internet as expected.
Google Drive is diﬀerent: the application TCP traﬃc
is terminated at the closest Google’s edge node, from
where it is routed to the actual storage/control data
center using the private Google’s network. Fig. 2 shows
the locations identiﬁed in our experiments4. Overall,
more than 100 diﬀerent entry points have been located.
Such architecture allows to reduce client-server RTT
and to oﬄoad storage traﬃc from the public Internet.
Performance implications are discussed in Sect. 5.
4. CLOUD SERVICE CAPABILITIES
4.1 Chunking
Our ﬁrst test aims at understanding how the ser-
vices process large ﬁles. By monitoring throughput dur-
ing the upload of ﬁles diﬀering in size, we determine
4Our results match with Google’s points of presence [7]. Un-
derstanding how Google manages traﬃc inside its network
is outside the scope of this paper.
3
s
N
Y
S
P
C
T
e
v
i
t
a
l
u
m
u
C
 400
 300
 200
 100
 0
Cloud Drive
Google Drive
 3
)
 2
B
M
(
d
a
o
l
p
U
 1
 0
Append
Random
Dropbox
SkyDrive
Wuala
Cloud Drive
Google Drive
 15
 10
 5
 0
 0
 10
 20
 30
 40
 50
 60
 0.1
 0.5
 1
 1.5
 2
 1  2
Time (s)
File size (MB)
 4
File size (MB)
 6
 8
 10
Figure 3: Uploading 100 ﬁles of 10 kB.
Figure 4: Delta encoding tests. Note the x-axes.
whether ﬁles are exchanged as single objects (no pause
during the upload), or split into chunks, each delimited
by a pause. Our experiments show that only Cloud
Drive does not perform chunking. In fact, Google Drive
uses 8 MB chunks while Dropbox uses 4 MB chunks.
SkyDrive and Wuala apparently change chunk sizes.
Chunking is advantageous because it simpliﬁes up-
load recovery in case of failures: partial submission be-
comes easier to be implemented. Partial submission can
beneﬁt users connected to slow networks, for example.
4.2 Bundling
When a batch of ﬁles needs to be transferred, ﬁles
could be bundled and pipelined so that both transmis-
sion latency and control overhead impact are reduced.
The benchmark consists of 4 upload sets, each contain-
ing exactly the same amount of data, which is split into
1, 10, 100 or 1000 ﬁles, respectively.
These experiments reveal a variety of synchroniza-
tion strategies. Surprisingly, Google Drive and Cloud
Drive open one separate TCP (and SSL) connection
for each ﬁle. Considering management, Cloud Drive
opens 3 TCP/SSL control connections per ﬁle opera-
tion. When several ﬁles have to be exchanged, such
design strongly limits the system performance due to
TCP and SSL negotiations (see Sect. 5). Fig. 3 shows
the number of TCP SYN packets observed when Google
Drive and Cloud Drive have to store 100 ﬁles of 10 kB
each: 100 and 400 connections are opened respectively,
requiring 30 s and 55 s to complete the upload.
Other services reuse TCP connections in a smarter
way. However, SkyDrive and Wuala submit ﬁles sequen-
tially, waiting for application layer acknowledgments