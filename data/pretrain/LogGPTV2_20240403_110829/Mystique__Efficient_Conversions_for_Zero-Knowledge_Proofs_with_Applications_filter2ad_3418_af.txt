table, we can see that our protocol is highly scalable and all
basic operations are highly efﬁcient. The arithmetic-Boolean
conversion (i.e., A2B and B2A) consists of two phases. In the
preprocessing phase, two parties generate random zk-edaBits,
and the execution time per zk-edaBit decreases from 95 µs to
19 µs when the bandwidth increases from 50 Mbps to 1 Gbps.
In the online phase, two parties can convert authenticated wire
values between arithmetic and Boolean circuits cheaply. The
efﬁciency of the conversion from a public commitment to
privately authenticated values (i.e., C2A) is mainly dominated
by the computation of PRF in a Boolean circuit. It only takes
around 56 µs to apply the PRF to a 64-bit data block, when the
network bandwidth is at least 50 Mbps, due to the high com-
munication efﬁciency of our protocol. The terms Fix2Float
and Float2Fix represent the conversions between ﬁxed-point
and ﬂoating-point numbers, where the execution time for both
conversions is around 46 µs per conversion when the network
bandwidth is larger than 50 Mbps.
For the ZK proof of matrix multiplication (i.e., MatMul),
our protocol can obtain around 185 ms of execution time
for dimension 512× 512, when the network bandwidth is at
least 200 Mbps. The execution time is increased to about
1.5 s and 11 s for dimensions 1024× 1024 and 2048× 2048,
respectively. The main efﬁciency bottleneck is the local com-
putation of matrix multiplication by the prover. Compared to
the state-of-the-art ZK proof for matrix multiplication [54],
which takes 10 seconds to prove a 1024× 1024 matrix mul-
tiplication over a network bandwidth of 500 Mbps, our ZK
protocol achieves a 7× improvement.
USENIX Association
30th USENIX Security Symposium    513
Figure 12: Execution-time decomposition for ResNet-101 Inference. The top bar is for public-model private-feature inference;
the bottom bar is for private-model private-feature inference. The network bandwidth is throttled to 200 Mbps.
Model
Image
Private
Private
Public
Private
Public
Private
LeNet-5 ResNet-50 ResNet-101
Communication
16.5 MB
16.5 MB
16.4 MB
1.27 GB
1.27 GB
0.53 GB
1.98 GB
1.98 GB
0.99 GB
Private
Public
Private
Execution time (seconds) in a 50 Mbps network
736
735
369
Execution time (seconds) in a 200 Mbps network
535
541
262
Private
Public
Private
7.3
7.5
6.5
5.9
5.5
4.9
465
463
210
333
336
158
Private
Private
Public
Private
Private
Public
Table 3: Performance of zero-knowledge neural-network
inference. All models are trained using the CIFAR-10 dataset.
7.2 Benchmarking Private Inference
With these building blocks, we connect them together to build
a ZK system to prove the inference of large neural networks
as we described in Section 6.3. We consider three canonical
settings, where the model parameters and model feature input
can either be private to the prover or public to both parties.
We focus on three neural networks: LeNet-5, ResNet-50, and
ResNet-101. While the ﬁrst example is relatively simple, the
last two examples represent the state-of-the-art neural net-
works in terms of accuracy and complexity.
In Table 3, we summarize the performance for all neural
networks, where the commitment on a model or data is not
involved. After all optimizations, the slowest component in
our protocol is Batch Normalization, which only exists in
ResNet-50 and ResNet-101. For all models, we observe that
when the model is private, the overall execution time is higher
than the case in which the model parameters are public. This
is because more operations have to be done in ZK proofs for
private models. Regardless of this setting, LeNet-5 inference
takes several seconds to ﬁnish. For all settings, ResNet-50
2We use ReLU as activation function instead of tanh for better accuracy.
Figure 13: (cid:96)2-norm distance between the plaintext-
inference probability vector and the ZK-inference proba-
bility vector. The mean difference is 0.0011 for ResNet-50
and 0.0019 for ResNet-101.
(resp., ResNet-101) takes about 2.6–5.6 (resp., 4.4–9) minutes
to accomplish under a 200 Mbps network.
Microbenchmark. Figure 12 reports the microbenchmark of
our ResNet-101 inference. We collect the time usage of dif-
ferent components including the protocol setup, private input
(i.e., computing corresponding authenticated values), differ-
ent operators and framework overhead. Signiﬁcant amount of
costs are used in Batch Normalization, ReLU, convolution2D
and framework overhead. When the model is private, an addi-
tional proportion of time will also be used for private input.
Note that the Batch Normalization takes around 70% of time
in both cases because it involves complicated arithmetic oper-
ations and conversions between ﬂoating-point and ﬁxed-point
numbers, which are costly to maintain accuracy. It will be an
interesting future work to further improve the efﬁciency of
Batch Normalization and ReLU without losing accuracy.
Benchmarking the accuracy. Our approach is highly accu-
rate, but could still cause some accuracy loss. This could par-
ticularly be a concern for deep neural networks with hundred
of layers where the error could propagate and get ampliﬁed.
To benchmark the accuracy of our protocol, we ran the whole
CIFAR-10 testing dataset [42] containing 10000 imagines.
CIFAR is one of the standard ML dataset to benchmark the
514    30th USENIX Security Symposium
USENIX Association
ML applications
LeNet-5 ResNet-50 ResNet-101
9.8 s
ZK for evasion attacks
7.2 s
ZK for genuine inference
ZK for private benchmark 8.2 m
316 s
16.4 m
4.4 h
524 s
28 m
7.3 h
Table 4: Efﬁciency of our ZK system in different applica-
tions. All execution time is reported based on a 200 Mbps
network and two m5.2xlarge machines.
performance of algorithms. Imagines in CIFAR-10 are all
labeled within 10 different classes, each imagine is a 32× 32
color picture. The accuracy difference between the plaintext
model and our ZK model is only 0.02% for both ResNet-50
and ResNet-101. To further understand the accuracy differ-
ence, we also compare the underlying probability vector pre-
dicted for each testing imagine. The dataset CIFAR-10 has 10
classes, and thus each inference produces a probability vector
of length 10, denoted as pppi for all i ∈ [1,10000]. The ﬁnal
prediction of the i-th testing imagine is ArgMaxi(pppi). We are
interested in the distribution of (cid:107)pppi − ppp(cid:48)
i(cid:107)2, where pppi is from
plaintext inference and ppp(cid:48)
i is from ZK inference. In Figure 13,
we show the (cid:96)2-norm differences of all 10000 inferences, and
we can see that even for ResNet-101, the (cid:96)2-norm difference
is smaller than 0.006 for 95% of the case. For LeNet-5, 99.9%
of the (cid:96)2-norm difference are below 0.006. Therefore, for top-
k accuracy such as k = 5 (commonly used for ImageNet), our
ZK inference will be highly accurate.
7.3 End-to-End Applications
By connecting the private models/features to publicly com-
mitted models/features, Mystique can be used to build the
three end-to-end applications mentioned in the Introduction.
Since we use CIFAR-10 dataset, each image is of size 32×32
pixels and each pixel uses 3 bytes to represent the color. This
means that one image is of size 3072 bytes and takes about
2.6 milliseconds to convert from publicly committed values
to privately authenticated values. The sizes of three models
considered in this paper are 0.25 MB, 94 MB, and 170 MB.
They take 1.7 seconds, 646 seconds, and 1169 seconds to
convert from a public commitment to authenticated values
that can be used in our protocols directly. The cost to “pull” a
publicly committed model to be used in ZK proofs is high, but
it could always be amortized over multiple private inferences.
• ZK proofs for evasion attacks. In this case, we need to
prove knowledge of two almost identical inputs that get
classiﬁed to different results under a public model. There-
fore, the main cost is to prove the classiﬁcation result in
zero-knowledge under a public model twice.
• ZK proofs for genuine inference. In this application, the
model parameters are private but publicly committed, while
the input data is public. The main overhead is from: 1)
proving the consistency between committed values and au-
thenticated values for all model parameters; and 2) proving
correct classiﬁcation with private model and public input.
• ZK proofs for private benchmark. In this application,
the testing data set is publicly committed and the model
is public. Therefore, the main overhead comes from: 1)
proving the consistency between committed testing data
and authenticated data; and 2) proving correct classiﬁcation
with private input data and public model. In our example,
we assume a testing data set of 100 images, and thus the
second step is executed for 100 times, once for each image.
The execution time for every end-to-end application is re-
ported in Table 4. Note that in the “ZK for private benchmark”
application, 100 testing images were publicly committed, and
then are converted to privately authenticated values using our
conversion protocol shown in Section 5. Thus, the execution
time for this application is signiﬁcantly higher.
8 Conclusion
This paper presents various conversion protocols and builds
zero-knowledge machine-learning inference on top of it. Al-
though we have made a huge progress in proving ML algo-
rithms in zero-knowledge, there are still limitations to our ZK
system that deserves further exploration in future works. In
particular, our ZK protocol can only prove to one veriﬁer at
a time, and the communication cost is fairly high compared
to succinct ZK proofs like zk-SNARKs. We also observed
a very high overhead for Batch Normalization, which may
potentially be further optimized.
Acknowledgements
We thank Yuanfeng Chen, Gaofeng Huang, Junjie Shi, and
Yilin Yan from MatrixElements Technologies for helping to
integrate our protocol with Rosetta. Work of Kang Yang is
supported by the National Key Research and Development
Program of China (Grant No. 2020YFA0309703), and the
National Natural Science Foundation of China (Grant Nos.
62022018, 61932019). Work of Jonathan Katz is supported
by DARPA under Contract No. HR00112020025 and by NSF
award #1563722. Work of Xiao Wang is supported in part
by DARPA under Contract No. HR001120C0087. The views,
opinions, and/or ﬁndings expressed are those of the author(s)
and should not be interpreted as reﬂecting the position or pol-
icy of the the Department of Defense or the U.S. Government,
and no ofﬁcial endorsement should be inferred.
References
[1] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene
Brevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,
USENIX Association
30th USENIX Security Symposium    515
Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-
mawat, Ian Goodfellow, Andrew Harp, Geoffrey Irv-
ing, Michael Isard, Yangqing Jia, Rafal Jozefowicz,
Lukasz Kaiser, Manjunath Kudlur, Josh Levenberg, Dan
Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris
Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner,
Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent
Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol
Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke,
Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-
scale machine learning on heterogeneous systems, 2015.
Software available from http://tensorflow.org.
[2] Martin R. Albrecht, Christian Rechberger, Thomas
Schneider, Tyge Tiessen, and Michael Zohner. Ci-
phers for MPC and FHE. In Advances in Cryptology—
Eurocrypt 2015, Part I, volume 9056 of LNCS, pages
430–454. Springer, 2015.
[3] Abdelrahaman Aly, Emmanuela Orsini, Dragos Rotaru,
Nigel P. Smart, and Tim Wood. Zaphod: Efﬁciently com-
bining lsss and garbled circuits in scale. In Proceedings
of the 7th ACM Workshop on Encrypted Computing &
Applied Homomorphic Cryptography, WAHC’19, page
33–44, 2019.
[4] Scott Ames, Carmit Hazay, Yuval Ishai, and Muthura-
makrishnan Venkitasubramaniam. Ligero: Lightweight
sublinear arguments without a trusted setup. In ACM
Conf. on Computer and Communications Security
(CCS) 2017, pages 2087–2104. ACM Press, 2017.
[5] Toshinori Araki, Assi Barak, Jun Furukawa, Tamar
Lichter, Yehuda Lindell, Ariel Nof, Kazuma Ohara, Adi
Watzman, and Or Weinstein. Optimized honest-majority
MPC for malicious adversaries - breaking the 1 billion-
gate per second barrier. In IEEE Symp. Security and