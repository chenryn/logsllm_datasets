[15] James C. Corbett, Jeffrey Dean, Michael Epstein, Andrew
Fikes, Christopher Frost, J. J. Furman, Sanjay Ghemawat,
Andrey Gubarev, Christopher Heiser, Peter Hochschild,
Wilson Hsieh, Sebastian Kanthak, Eugene Kogan, Hongyi
Li, Alexander Lloyd, Sergey Melnik, David Mwaura, David
Nagle, Sean Quinlan, Rajesh Rao, Lindsay Rolig, Yasushi
Saito, Michal Szymaniak, Christopher Taylor, Ruth Wang,
and Dale Woodford. Spanner: Google’s globally distributed
database. ACM Trans. Comput. Syst., aug 2013.
[16] Lisa Glendenning, Ivan Beschastnikh, Arvind
Krishnamurthy, and Thomas Anderson. Scalable consistency
in scatter. In Proceedings of the Twenty-Third ACM
Symposium on Operating Systems Principles, SOSP ’11,
pages 15–28, New York, NY, USA, 2011. ACM.
[17] Jason Baker, Chris Bond, James C. Corbett, JJ Furman,
Andrey Khorlin, James Larson, Jean-Michel Leon, Yawei Li,
Alexander Lloyd, and Vadim Yushprakh. Megastore:
Providing scalable, highly available storage for interactive
services. In Proceedings of the Conference on Innovative
Data system Research (CIDR), pages 223–234, 2011.
[18] Intel. Intel distributed ledger. http://intelledger.github.io/,
2016.
[19] Chain Inc. Chain open standard: A secure blockchain
protocol for high-scale ﬁnancial networks.
http://chain.com/os/, 2016.
[20] Rhett Creighton. Domus tower blockchain.
http://domustower.com/domus-tower-blockchain-latest.pdf,
2016.
[21] David Mazieres. The stellar consensus protocol: A federated
model for internet-level consensus. April 2015.
[22] Arthur Britto David Schwartz, Noah Youngs. The ripple
protocol consensus algorithm. Ripple Labs Inc., 2014.
[23] Bitcoin client. https://github.com/bitcoin/bitcoin.
[24] Rachid Guerraoui, Florian Huc, and Anne-Marie Kermarrec.
Highly dynamic distributed computing with byzantine
failures. In Proceedings of the 2013 ACM Symposium on
Principles of Distributed Computing, PODC ’13, 2013.
[25] Gopal Pandurangan, Peter Robinson, and Amitabh Trehan.
Self-healing deterministic expanders. CoRR, abs/1206.1522,
2012.
[26] John R. Douceur. The sybil attack. In Proceedings of 1st
International Workshop on Peer-to-Peer Systems (IPTPS),
2002.
[27] James Newsome, Elaine Shi, Dawn Song, and Adrian Perrig.
The sybil attack in sensor networks: Analysis & defenses. In
Proceedings of the 3rd International Symposium on
Information Processing in Sensor Networks, IPSN ’04, pages
259–268, New York, NY, USA, 2004. ACM.
[28] Baruch Awerbuch and Christian Scheideler. Robust random
number generation for peer-to-peer systems. Theor. Comput.
Sci., 410(6-7):453–466, feb 2009.
[29] Bitcoin wiki. Proof of stake.
https://en.bitcoin.it/wiki/Proof_of_Stake, 2015.
[30] Stefan Dziembowski, Sebastian Faust, Vladimir
Kolmogorov, and Krzysztof Pietrzak. Proofs of space.
Cryptology ePrint Archive, Report 2013/796, 2013.
http://eprint.iacr.org/.
[31] Giuseppe Ateniese, Ilario Bonacina, Antonio Faonio, and
Nicola Galesi. Proofs of space: When space is of the
essence. Cryptology ePrint Archive, Report 2013/805, 2013.
http://eprint.iacr.org/.
[32] Nancy Lynch. Distributed algorithms. Morgan Kaufmann,
1996.
[33] Seth Gilbert, Calvin Newport, and Chaodong Zheng. Who
are you? secure identities in ad hoc networks. In Distributed
Computing, pages 227–242. Springer, 2014.
[34] Marcin Andrychowicz and Stefan Dziembowski. Distributed
cryptography based on the proofs of work. Cryptology ePrint
Archive, Report 2014/796, 2014.
http://eprint.iacr.org/2014/796.
[35] Juan Garay, Aggelos Kiayias, and Nikos Leonardos. The
bitcoin backbone protocol: Analysis and applications.
Cryptology ePrint Archive, Report 2014/765, 2014.
http://eprint.iacr.org/.
[36] Jae Kwon. Tendermint: Consensus without mining.
[37] IBM. Ibm blockchain. http://www.ibm.com/blockchain/,
2016.
[38] Digital Asset Holdings. Digital asset.
https://digitalasset.com/, 2016.
[39] Ethereum Foundation. Ethereum’s white paper.
https://github.com/ethereum/wiki/wiki/White-Paper, 2014.
[40] George Danezis and Sarah Meiklejohn. Centrally banked
cryptocurrencies. Cryptology ePrint Archive, Report
2015/502, 2015. http://eprint.iacr.org/.
[41] Yonatan Sompolinsky and Aviv Zohar. Accelerating bitcoin’s
transaction processing. fast money grows on trees, not
chains. Cryptology ePrint Archive, Report 2013/881, 2013.
http://eprint.iacr.org/.
[42] Loi Luu, Jason Teutsch, Raghav Kulkarni, and Prateek
Saxena. Demystifying incentives in the consensus computer.
Cryptology ePrint Archive, Report 2015/702, 2015.
http://eprint.iacr.org/.
[43] Thaddeus Dryja Joseph Poon. The bitcoin lightning network:
Scalable off-chain instant payments.
http://lightning.network/lightning-network-paper.pdf, 2015.
[44] Adam Back, Matt Corallo, Luke Dashjr, Mark Friedenbach,
Gregory Maxwell, Andrew Miller, Andrew Poelstra, Jorge
Timon, , and Pieter Wuille. Enabling blockchain innovations
with pegged sidechains.
https://blockstream.com/sidechains.pdf, 2014.
[45] Pieter Wuille. Would sidechains help bitcoin scale? 2015.
[46] Buteri Vitalik, Wood Gavin, Zamﬁr Vlad, Coleman Jeff,
Wampler-Doty Matthew, and Cohn John. Notes on scalable
blockchain protocols (version 0.3.2). https://github.com/
vbuterin/scalability_paper/raw/master/scalability.pdf, 2015.
[47] Gabriel Bracha. An O(log n) expected rounds randomized
byzantine generals protocol. J. ACM, 34:910–920, October
1987.
[48] Seth Gilbert and Dariusz R. Kowalski. Distributed agreement
with optimal communication complexity. In Proceedings of
the Twenty-ﬁrst Annual ACM-SIAM Symposium on Discrete
Algorithms, pages 965–977. Society for Industrial and
Applied Mathematics, 2010.
[49] Leslie Lamport. Fast paxos. Distributed Computing,
19(2):79–103, October 2006.
[50] Allen Clement, Edmund Wong, Lorenzo Alvisi, Mike
Dahlin, and Mirco Marchetti. Making byzantine fault
tolerant systems tolerate byzantine faults. In Proceedings of
the 6th USENIX Symposium on Networked Systems Design
and Implementation, pages 153–168. USENIX Association,
2009.
[51] Ramakrishna Kotla, Lorenzo Alvisi, Mike Dahlin, Allen
Clement, and Edmund Wong. Zyzzyva: Speculative
byzantine fault tolerance. ACM Trans. Comput. Syst.,
27(4):7:1–7:39, January 2010.
[52] Pierre-Louis Aublin, Rachid Guerraoui, Nikola Kneževi´c,
Vivien Quéma, and Marko Vukoli´c. The next 700 bft
protocols. ACM Trans. Comput. Syst., 32(4):12:1–12:45,
January 2015.
[53] V. King, J. Saia, V. Sanwalani, and E. Vee. Towards secure
and scalable computation in peer-to-peer networks. In
Foundations of Computer Science, 2006. FOCS ’06. 47th
Annual IEEE Symposium on, pages 87–98, 2006.
[54] Valerie King and Jared Saia. From almost everywhere to
everywhere: Byzantine agreement with ˜O(n3/2) bits. In
Proceedings of the 23rd International Conference on
Distributed Computing, pages 464–478. Springer-Verlag,
2009.
[55] Valerie King, Steven Lonargan, Jared Saia, and Amitabh
Trehan. Load balanced scalable byzantine agreement through
quorum building, with full information. In Distributed
Computing and Networking, volume 6522 of Lecture Notes
in Computer Science, pages 203–214. Springer Berlin
Heidelberg, 2011.
[56] Nicolas Braud-Santoni, Rachid Guerraoui, and Florian Huc.
Fast byzantine agreement. In Proceedings of the 2013 ACM
Symposium on Principles of Distributed Computing, pages
57–64. ACM, 2013.
[57] U. Feige. Noncryptographic selection protocols. In
Foundations of Computer Science, 1999. 40th Annual
Symposium on, pages 142–152, 1999.
[58] Valerie King and Jared Saia. Breaking the O(n2) bit barrier:
Scalable byzantine agreement with an adaptive adversary. J.
ACM, 58:18:1–18:24, July 2011.
[59] Jonathan Katz, Andrew Miller, and Elaine Shi.
Pseudonymous broadcast and secure computation from
cryptographic puzzles. Cryptology ePrint Archive, Report
2014/857, 2014. http://eprint.iacr.org/2014/857.
[60] Christian Decker, Jochen Seidel, and Roger Wattenhofer.
Bitcoin meets strong consistency. In Proceedings of the 17th
International Conference on Distributed Computing and
Networking, ICDCN ’16, pages 13:1–13:10, New York, NY,
USA, 2016. ACM.
[61] James Aspnes, Collin Jackson, and Arvind Krishnamurthy.
Exposing computationally-challenged byzantine impostors.
Department of Computer Science, Yale University, New
Haven, CT, Tech. Rep, 2005.
[62] Wikipedia. Coupon collector’s problem. https:
//en.wikipedia.org/wiki/Coupon_collector%27s_problem.
[63] Donald J. Newman. The double dixie cup problem. The
American Mathematical Monthly, 67(1):58–61, 1960.
[64] Christina Garman, Matthew Green, and Ian Miers.
Decentralized anonymous credentials. Cryptology ePrint
Archive, Report 2013/622, 2013. http://eprint.iacr.org/.
[65] Seth Gilbert and Nancy Lynch. Brewer’s conjecture and the
feasibility of consistent, available, partition-tolerant web
services. SIGACT News, 33(2):51–59, June 2002.
[66] Seth Gilbert and Nancy Lynch. Perspectives on the cap
theorem. Computer, 45(2):30–36, February 2012.
10. APPENDIX
10.1 The Scalability of ELASTICO
We now explain why ELASTICO achieves the desired asymptotic
O(n/ log log n) scalability by calculating the expected number of
PoW solutions that n processors have to generate such that each of
2s committees has c members. Our problems is equivalent to the
extended coupon collector problem [62] where there are 2s types
of coupon in some urn and we want to calculate the number of
draws to collect c copies of each coupon. It is shown in [63] that
the expected number of PoW solutions E is
E = 2s log 2s + (c − 1)2s log log 2s + O(2s)
= s · n
c
+ n log s − n
c
log s + O(n/c)
If c  s, we have
s · n
c
 24 · ln(2s) gives us such
scalability.
Lemma 8 (Balls to Bins). We throw balls uniformly at random into
a collection of X bins. When c > 24 · ln X, the expected number
of balls to ensure each bin to have at least c balls is O(cX)
Proof. Let us consider the scenario of throwing cX balls uniformly
at random into those X bins. The expected number of balls in
each bin is c. Let’s ﬁx a particular bin of interest and we consider
xi be a random variable that equals 1 if ball i lands in that bin,
and 0 otherwise. Let C = (cid:80) xi. Notice that the expected value
E(C) = c, and all xi are independent. Using Chernoff bound, we
have:
Pr [C  24 · ln X, we get:
Pr [C < c/2] ≤ 1/X 3.
Recall we are looking at all one particular bin. Now we consider
all X bins. Take a union bound, and we get the probability that any
bin has less than c/2 balls is at most 1/X 2. This also means that
with probability 1/X 2, we throw cX balls more into X bin. Doing
the same calculation for the new cX balls, we need to throw cX
more balls with probability 1/X 2.
Thus the expected number of balls we have to throw to ensure
each bin to have at least c/2 balls is:
cX + cX/X 2 + cX/X 4 + cX/X 6 + ... ≤ 2cX.
Therefore, the number of balls to throw guarantee each bin to
have at least c balls is O(cX).
10.2 Veriﬁcation Checks in ELASTICO
Different blockchain applications may require different veriﬁ-
cation checks on the transactions (e.g., [64]). In the presence of
sharding, supporting arbitrary checks within local committees is
not possible, as is well-known in sharded databases [65, 66].
In
a cryptocurrency, the most important checks are ensuring that the
payer signs the coin transfer and that the transaction is not double-
spending. We next discuss how one can efﬁciently implement such
veriﬁcation check for transactions in a hypothetical cryptocurrency
built on top of ELASTICO.
In Bitcoin and popular cryptocurrencies, a regular transaction
sends some amount of coins from a sender to a recipient. Each
transaction has two parts, namely an Input and an Output. The
Input speciﬁes the source of the coin and a proof (by digital
signatures) which shows the sender is a valid coin owner. The
Output names the recipient; later on the recipient can use this
Output as the source of the coin in his/her new transaction). Check-
ing if a transaction is double-spending entails checking if it uses
some coin which has been spent in a previous transaction. A naive
implementation of such check would require one to scan through
the entire history of the blockchain. In Bitcoin, to avoid such costs,
each node maintains a local database containing all “unspent trans-
action outputs" (or UTXO for short) to check if an Output has
been spent or not. Bitcoin nodes frequently update the UTXO
database after each new block based on the set of transactions in-
cluded in the block. Thus, if a transaction is ﬁnalized in the Bitcoin
blockchain, it is guaranteed to be valid.
In ELASTICO, our transaction veriﬁcation is similar, i.e., the
node also maintain a local database of UTXOs and update such
database after each epoch when they download data blocks from
committees. To avoid the scenario that two committees process the
same transaction, each committee in ELASTICO processes a sepa-
rate list of transactions which have some speciﬁc range of Input.
For example, if there are 4 committees, they will handle transac-
tions having Inputs’ IDs (i.e., the hashes of the Inputs) with
different preﬁxes of 00, 01, 10, 11 respectively. As a result, within
an epoch, all shards will include disjoint transaction inputs (thus
disjoint transaction sets).
The veriﬁcation in ELASTICO is superior to other solutions like
GHOST [40], Bitcoin- NG [9] in many aspects. First, ELASTICO
nodes do not need to verify data blocks from other committees.
Instead, they only need to check if a block is committed in the
consensus block (from the ﬁnal committee) to decide if the block
is valid. It is because the committee corresponding to that shard
already veriﬁes its block, and the ﬁnal committee has veriﬁed if
a block is agreed by a committee before committing the block to
the ﬁnal (consensus) block. On the other hand, in other solutions,
nodes have to verify all transactions included in the blockchain,
thus higher throughput will require more local computation from
nodes. Further, increasing the amount of veriﬁcation check will
slow down the block broadcast, and further pose a security threat
in the consensus protocol as pointed out by previous work [41].
Thus, ELASTICO scales up the throughput as the network al-
lows, without requiring more local computation for veriﬁcation at
each node or affecting the consensus security. Note that ELAS-
TICO still permits bandwidth efﬁciency in its consensus step for a
cryptocurrency application, but has to broadcast all the data blocks
to all nodes (as done in all solutions today). However, in appli-
cations where the consistency checks can themselves be sharded
and checked locally, there would be no need to broadcast the data
blocks to the network.