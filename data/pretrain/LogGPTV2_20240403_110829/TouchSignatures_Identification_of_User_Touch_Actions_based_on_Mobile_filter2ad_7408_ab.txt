yes
yes
yes
yes
yes
Background
other
same
—
—
—
—
—
—
—
—
—
—
yes
yes
yes
yes
yes
—
yes
yes
yes
yes
yes
yes
yes
—
—
—
—
—
—
—
yes
—
yes
yes
—
—
—
—
—
—
Chrome
Opera †
Firefox
Dolphin
UC Browser †
Baidu
CM Browser
Photon
Maxthon
Boat
Next
Yandex
Safari
Chrome
Dolphin
UC Browser
Baidu Browser
Maxthon
Yandex
Mercury
other
—
—
—
—
—
yes
yes
yes
yes
yes
yes
—
—
—
—
—
yes
—
—
—
Table 4: Mobile browser access to the orientation and motion sensor data on Android
and iOS under diﬀerent conditions. A † indicates a family of browsers (e.g., Opera and
Opera Mini are considered to be in the same Opera family). A yes (in italics) indicates
a possible security leakage vector. A yes (in italics and underlined) indicates a possible
security leakage vector only in the case when the browser was active before the screen is
locked.
other
—
—
—
—
yes
yes
yes
yes
yes
yes
yes
—
—
yes
yes
—
yes
yes
yes
yes
same
—
—
—
—
—
yes
yes
yes
yes
yes
yes
yes
yes
—
—
yes
yes
—
—
—
and carried out tests on diﬀerent combinations of mobile OSs and browsers.
We considered both Android and iOS, and on each mobile OS we tested the
major browsers plus those that are highly popular (see Appendix A). The
details of our tests and our ﬁndings are summarised in Table 4.
Table 4 shows the results of our tests as to whether each browser provides
access to device motion and orientation sensor data in diﬀerent conditions.
The culumn(s) list the device, mobile OS (mOS), and browser combination
under which the test has been carried out. In case of multiple versions of the
same browser, as for Opera and Opera Mini, we list all of them as a family
7
in one bundle since we found that they behave similarly in terms of granting
access to the sensor data with which we are concerned. The “yes” indications
under “active/same” show that all browsers provide access to the mentioned
sensor data if the browser is active and the user is working on the same
tab as the tab in which the code listening to the sensor data resides. This
represents the situation in which there is perhaps a common understanding
that the code should have access to the sensor data. In all other cases, as we
discuss bellow, access to the sensor data provides a possible security leakage
vector through which attacks can be mounted against user security. In the
following we give more details on these results.
Browser-active iframe access. HTML frames are commonly used to divide a
browser window into multiple segments, each of which can independently load
a separate web document possibly from a diﬀerent web origin. We embedded
our JavaScript listener into an HTML frame, namely iframe, which resided
within a web page at a diﬀerent web address. The test was to ﬁnd out whether
or not the listener in a separate segment of the browser window was able to
access the sensor data streams if the user was interacting (via touch actions)
with the content within the same tab but on a diﬀerent segment of the
browser window. Figure 1 (left) gives an example on how an iframe works
inside a page. The iframe content is loaded from a diﬀerent source and is
able to collect sensor data using JavaScript. Through experiments, we found
that all the browsers under test provided access to the sensor data streams
in this case. The ﬁndings are listed in the column under “active/iframe” in
Table 4 indicating such an access.
Browser-active diﬀerent-tab access. In this test, we had the browser active
and our JavaScript listener opened in a tab while the user was interacting
with the content on a separate tab. Figure 1 (right) gives an example of
this condition. Interestingly, we found that in addition to most of the other
browsers on Android and iOS, some major browsers such as Google Chrome
on iOS provided diﬀerent-tab access to the sensor data streams in this case.
The ﬁndings are listed in the column under “active/other” in Table 4 indi-
cating browser-active diﬀerent-tab access.
Browser-in-background access. In this test, we ﬁrst opened a web page con-
taining our JavaScript listener and then minimised the browser. While the
browser was still running in the background, the user would interact with
another app (via touch actions), or try to unlock the screen by providing
8
Figure 1: Left: An example of a page that includes an iframe (at the bottom of the page).
Right: An example of a pre-opened attack page while the user is working on a diﬀerent
tab. These two examples demonstrate why iframe and other tab accesses can be threats
to user security.
a PIN or pattern input. We ran the test in two cases: 1) the browser had
only the tab containing our JavaScript listener open, or 2) the browser had
multiple tabs open including one containing our JavaScript listener. Surpris-
ingly, we found that a few browsers on both the Android and iOS provided
access to the sensor data streams when the user was interacting with another
app. The ﬁndings are listed in the column under “background” in Table 4
indicating browser-in-background access.
Screen-locked access. In this test, we ﬁrst opened a web page containing
our JavaScript listener and then locked the screen. We found that a few
browsers on both Android and iOS, including Safari, provided access to the
sensor data streams even when the screen was locked. The ﬁndings are listed
in the column under “locked” in Table 4 indicating screen-locked access.
We emphasise that none of the tested browsers (on Android or iOS) asked
for any user permissions to access the sensor data when we installed them or
while performing the experiments.
The above ﬁndings suggest possible attack vectors through which mali-
cious web content may gather information about user activities and hence
breach user security. In particular, browser-active iframe access enables ac-
9
Figure 2: TouchSignatures overview
tive web content embedded in HTML frames, e.g. posing as an advertisement
banner, to discretely record the sensor data and determine how the user is in-
teracting with other segments of the host page. Browser-active diﬀerent-tab
access enables active web content that was opened previously and remains in
an inactive tab, to eavesdrop the sensor data on how the user is interacting
with the web content on other tabs. Browser-in-background and screen-locked
access enable active web content that remains open in a minimised browser
to eavesdrop the sensor data on how the user is interacting with other apps
and on user’s actions while carrying the device.
To show the feasibility of our security attack, in the following sections,
we will demonstrate that, with advanced machine learning techniques, we
are able to distinguish the user’s touch actions and PINs with high accuracy
when the user is working with a mobile phone.
3. TouchSignatures
3.1. Overview
Each user touch action, such as clicking, scrolling, and holding, and even
tapping characters on the mobile soft keyboard, induces device orientation
and motion traces that are potentially distinguishable from those of other
touch actions. Identiﬁcation of such touch actions may reveal a range of ac-
tivities about user’s interaction with other webpages or apps, and ultimately
their PINs. A user’s touch actions may reveal what type of web service the
user is using as the patterns of user interaction are diﬀerent for diﬀerent web
services: e.g., users tend to mostly scroll on a news web site, while they
tend to mostly type on an email client. On known web pages, a user’s touch
actions might reveal which part of the page the user is more interested in.
10
Combined with identifying the position of the click on a page, which is pos-
sible through diﬀerent signatures produced by clicking diﬀerent parts of the
screen, the user’s input characters could become identiﬁable. This in turn
reveals what the user is typing on a page by leveraging the redundancy in
human languages, or it may dramatically decrease the size of the search space
to identify user passwords.
We introduce TouchSignatures in order to distinguish user touch actions
and PINs in two phases. Figure 2 shows a top level view of the two phases
of TouchSignatures. The input of TouchSignatures system is a feature vector
which we will explain later and the output is the type of the touch action
(click, hold, scroll, and zoom) in phase one and the PIN digits (0 to 9) in
phase two. This is the ﬁrst attack in the literature that compromises user
security through JavaScript access to sensor data.
As this paper is only the ﬁrst investigation on JavaScript access to sensor
data on mobile devices, we limit the scope of the paper as follows. First, we
only identify digital PINs rather than alphanumeric passwords. We expect
it to be possible to extend our work to recognize the full alphanumeric soft
keyboard, but the classiﬁcation techniques will be quite diﬀerent. Second, in
the proof-of-concept implementation of the attack, we focus on working with
active web pages, which allows us to easily identify the start and end of a
touch action through the JavaScript access to the onkeydown, and onkeyup
events. A similar approach is adopted in other works (e.g., TouchLogger [8]
and TapLogger [25]). In a general attack scenario, a more complex segmen-
tation process is needed to identify the start and end of a touch action. This
may be achieved by measuring the peak amplitudes of a signal, as done in
[18]. However, the segmentation process will be more complex, and we leave
that to future work.
3.2. In-browser sensor data detail
The attack model we consider is malicious web content spying on a user
via JavaScript. The web content is opened as a web page or embedded as
an HTML frame in a segment of a web page. The user may be interacting
with the browser or any other app given that the browser is still running
in the background. We assume that the user has access to the Internet as
reasonably implied by the user launching the browser app. TouchSignatures’s
client-side malicious web content collects and reports sensor data to a server
which stores and processes the data to identify the user’s touch actions.
11
The sensor data streams available as per the W3C speciﬁcations [3], i.e.,
device motion and orientation, as follows:
• device orientation which provides the physical orientation of the device,
expressed as three rotation angles: alpha, beta, and gamma, in the
device’s local coordinate frame,
• device acceleration which provides the physical acceleration of the de-
vice, expressed in Cartesian coordinates: x, y, and z, in the device’s
local coordinate frame,
• device acceleration-including-gravity which is similar to acceleration ex-
cept that it includes gravity as well,
• device rotation rate which provides the rotation rate of the device about
the local coordinate frame, expressed as three rotation angles: alpha,
beta, and gamma, and
• interval which provides the constant rate with which motion-related
sensor readings are provided, expressed in milliseconds.
The device’s local coordinate frame is deﬁned with reference to the screen
in its portrait orientation: x is horizontal in the plane of the screen from left
of the screen towards right; y is vertical in the plane of the screen from the
bottom of the screen towards up; and z is perpendicular to the plane of
the screen from inside the screen towards outside. Alpha indicates device
rotation around the z axis, beta around the x axis, and gamma around the
y axis, all in degrees.
To design TouchSignatures, we employ the supervised learning approach,
i.e., train a machine learning system based on labelled data collected from
the ﬁeld. Consistent with the attack model discussed above, we developed
a suite of applications including a client-side JavaScript program in a web
page that records the sensor data and a server-side database management
system (DBMS) that captures and stores user sensor data in real-time. Sub-
sequently, we recruited diﬀerent groups of users5 and collected sensor data
samples for diﬀerent touch actions and PINs, using our client-side web page
5All experiments in this paper were ethically approved by the Ethical Review Commit-
tee at Newcastle University, UK.
12
Figure 3: The client side GUIs presented to the user during data collections (left for Touch
actions, and centre for PINs), and the data received at the server side (right).
that we developed for data collection purposes, while in real-time the cap-
tured sensor data was reported to and stored at our server-side database.
Eventually, we extracted a set of descriptive features from the sensor data
and trained a machine learning system for TouchSignatures which includes
multiple classiﬁers. In the following, we give the details of our application
implementation, experiments, feature extraction, and training algorithms.
3.3. Application Implementation
Client side. On the client side, we developed a listener, which records sensor
data streams, and a web page interface, which is used to collect labelled data
from the subjects in our experiment. The implementation is in JavaScript.
The listener mainly includes the following components: an event listener
which is ﬁred on page load and establishes a socket connection between
the client and server using Socket.IO6, an open source JavaScript library
supporting real-time bidirectional communication which runs in browser on
the client side; and two event listeners on the window object, ﬁred on de-
vice motion and device orientation Document Object Model (DOM) events
(called devicemotion and deviceorientation), which send the raw sensor
data streams to the server through the established socket. The sensor data
streams are sent continuously until the socket is disconnected, e.g. when the
6www.socket.io
13
tab that loads the listener is closed. The code is presented in Appendix B.
The (user) interface sits on top of the listener and is used for data collec-
tion. We developed an HTML 5 compliant page including JavaScript using
bootstrap7 (a popular framework for web app creation). Data collection
occurs in two rounds (for touch actions and PINs) and multiple steps. In
each step the user is instructed to perform a single touch action or enter a
4-digit PIN. Sensor data from the touch actions and PINs are collected from
the user successively. The label describing the type of the task or the digits
in the PIN and timing information for the tasks is reported to the server.
The GUI includes a concise instruction to the user as to what the user needs
to do at each step. Snapshots of the GUIs presented to the users in the two
diﬀerent phases are illustrated in Figure 3 (left, and centre). More details
can be found in Sections 4.2 and 5.2.
Server side. On the server side, we developed a server to host the data and
handle communications, and a database to handle the storage of the cap-
tured sensor data continuously. The server is implemented using Node.js8,
which is capable of supporting data intensive applications in real-time. The
Socket.IO JavaScript library sits on Node.js and handles the communica-
tions with the client; see Figure 3 (right). For the DBMS we have opted
to implement a NoSQL database on MongoLab9. NoSQL databases are
document-oriented, rather than relational. They are known for being ca-
pable to handle high-speed streams of data in real-time. MongoLab is a
cloud-based database-as-a-service NoSQL DBMS.
3.4. Feature extraction
In this section, we discuss the features we extract to construct the feature
vector which subsequently will be used as the input to the classiﬁer. We con-
sider both time domain and frequency domain features. The captured data
include 12 sequences: acceleration, acceleration-including-gravity, orienta-
tion, and rotation rate, with three sequences for each sensor measurement.
Before extracting features, to cancel out the eﬀect of the initial position and
orientation of the device, we subtract the initial value in each sequence from
subsequent values in the sequence.
7www.getbootstrap.com
8www.nodejs.org
9www.mongolab.com
14
Time domain features. In the time domain, we consider both the raw cap-
tured sequences and the (ﬁrst order) derivative of each sequence. The ratio-
nale is that each sequence and its derivative include complementary infor-
mation on the touch action. To calculate the derivative, since we have low
frequency sequences, we employ the basic method of subtracting each value
from the value appearing immediately afterwards in the sequence. That is, if
the sequence values are represented by vi, the derivative sequence is deﬁned
as di = vi − vi−1.
For the device acceleration sequences, we furthermore consider the Eu-
clidean distance between consecutive readings as a representation of the
change in device acceleration. This is simply calculated as the following
sequence:
q