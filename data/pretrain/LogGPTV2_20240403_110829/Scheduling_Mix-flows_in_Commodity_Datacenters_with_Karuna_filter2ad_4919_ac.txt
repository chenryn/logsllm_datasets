s∈S (l)xs(t),∀l
(Ql(t)+µ)xs(t)}
(7)
At a high-level, we transform the the long term (t→∞)
stochastic delay minimization problem (4) into a drift-plus-
penalty minimization problem (7) at every update instant t.
To solve the transformed problem, we develop an adaptive
source rate control algorithm.
4.1.2 Optimal congestion window update func-
tion
By considering the properties of the optimal solution and
the KKT conditions [8] of the above problem, we obtain a
primal algorithm to achieve optimality for (7). Eq.(8) sta-
bilizes the queueing system and minimizes the overall per-
packet delay of the network:
d
dt xs(t)=f(cid:48)
where fs(xs)=−Zs(t) γs(t)
In-
terested reader may refer to MCP technical report [10] for
derivation.
l∈L(s)λl(t),
xs(t)−Qs(t)xs(t), λl(t)=d(cid:48)
l(yl(t)).
(8)
Each ﬂow should adjust its transmission rate according to
s(xs(t))−(cid:80)
(8), which can be re-written as:
dt xs(t)=Θ(γs(t),xs(t))−(cid:80)
d
l∈L(s)(Ql(t)+λl(t)),
(9)
τs(t) )− (cid:80)
l∈L(s)
where Θ(γs(t),xs(t))= Zs(t)Ms(t)
τs(t)x2
s(t) = Zs(t)γs(t)
s(t)
x2
.
We then can derive the equivalent optimal congestion win-
dow update function:
Ws(t+τs(t))←Ws(t)+τs(t)(Θ(γs(t), Ws(t)
(Ql(t)+λl(t)))
(10)
Consider the two terms that constitute the difference be-
tween window sizes:
• The ﬁrst (source term), Θ(γs(t),xs(t)) where xs(t)= Ws(t)
τs(t) ,
is an increasing function of γs, and a decreasing function
of xs. A large γ for a ﬂow means that this ﬂow is more
urgent, i.e. it has large remaining data to send and/or an
imminent deadline. This term ensures that the ﬂow will
be more aggressive as its urgency grows.
• The second (network term),(cid:80)
l∈L(s)(Ql(t)+λl(t)), sum-
marizes the congestion in the links along the path. If any
link is congested, sources that use that link will reduce
their transmission rates. This term makes MCP ﬂows re-
act to congestion.
Combining these two terms, the update function allows
deadline ﬂows meet their deadlines, while impacting the other
ﬂows as little as possible.
Figure 5: Queue length approximation.
4.2 MCP: From theory to practice
We now turn Eq.(10) into a practical algorithm.
4.2.1 ECN-based network term approximation
The source term can be obtained using information from
upper layer applications (§7). However, obtaining the net-
work term is not easy, as the sum of all link prices, λl, and
queue lengths, Ql, are needed along the path, and this aggre-
gated path-level information is not directly available at the
source. This sum can be stored in an additional ﬁeld in the
packet header, and each switch adds and stores its own price
and queue length to this ﬁeld for every packet. However,
current commodity switches are not capable of such opera-
tions. For implementation, we use the readily available ECN
functionality in commodity switches to estimate the network
term.
Estimating queue lengths: The focus of our approxima-
tion is the aggregated queue lengths for each ﬂow, Q. We de-
note F (0≤F≤1) as the fraction of packets that were marked
in the last window of packets, and F is updated for every
window of packets. Both DCTCP and D2TCP compute F to
estimate the extent of congestion, and MCP further exploits
F to estimate queue lengths.
For our estimation, we abstract the DCN fabric as one
switch. Current data center topologies enable high bisec-
tion bandwidth in the fabric, which pushes the bandwidth
contention to the edge switches (assuming load-balancing is
done properly) [4, 24]. In particular, the bottleneck link usu-
ally occurs at the egress switch of the fabric. Our estimation
scheme therefore models the queueing behavior in the bot-
tleneck switch.
Figure 5 illustrates how a source s estimates the queue
length based on F . Assume the ECN threshold is K, the
current queue length is Q, and the last window size of s is
W . The fraction of packets in W of s that are marked by
ECN should be Q−K. Therefore, we have F≈ Q−K
W , and
thus ˆQ≈K+F×W , which is the estimate we use for the
aggregated queue length for each source.
Estimating link prices: The link price represents the level
of congestion at the bottleneck link, and, for mathematical
tractability, we make the simplifying assumption that the
link is an M/M/1 queue [27], d(y)=1/(C−y). Therefore,
the price of the link is proportional to the derivative of the
delay function, d(cid:48)(y)=(C−y)−2.The arrival rate can be di-
rectly obtained by two consecutive queue estimations at the
source: ˆy(t)=
ˆQ(t)− ˆQ(t−τs(t))
τs(t)
.
4.2.2 Practical MCP algorithm
Using the above estimation and Eq.(10), the congestion
window update function of a practical MCP therefore is:
τs(t)
)−2.
τs(t) )−(K+Fs(t)Ws(t)+λ(t))) (11)
We evaluate this algorithm in experiments (§8.1) and sim-
Ws(t+τs(t))+=τs(t)(Θ(γs(t), Ws(t)
where λ(t)=(C− Fs(t)Ws(t)−Fs(t−τs(t))Ws(t−τs(t))
ulations (§8.2).
4.2.3 Early ﬂow termination
Some ﬂows may need to be terminated before their dead-
lines in order to ensure that other ﬂows can meet theirs.
Optimally selecting such ﬂows has been shown to be NP-
hard [22]. We propose an intuitive heuristic for MCP to ter-
minate a ﬂow when there is no chance for it to complete be-
fore its deadline: when the residual rate of the ﬂow is larger
than the link capacity, the ﬂow will be aborted: Zs(t)>
minl∈L(s)Cl, where Zs(t) is the virtual queue of the ﬂow,
which stores the accumulative differences between the ac-
tual rates and the expected rates. Zs(t) is therefore a past
performance indicator for this ﬂow. This criterion implies
that the capacity of the path is no longer sufﬁcient for ﬁn-
ishing before the deadline. Early termination of ﬂows gives
more opportunities for other ﬂows to meet deadlines [39].
We evaluate this criterion in §8.1.3.
5. HANDLING NON-DEADLINE FLOWS
To consumer the bandwidth left over by type 1 ﬂows,
Karuna employs aggressive rate control such as DCTCP [3]
for type 2&3 ﬂows. Further, it leverages multiple lower pri-
ority queues in the network to minimize FCT of these ﬂows.
5.1 Splitting type 2 ﬂows
Since the sizes for type 2 ﬂows are known, implementing
SJF over them is conceptually simple. Karuna splits these
ﬂows to different priority queues according to their sizes:
Smaller ﬂows are sent to higher priority queues than larger
ﬂows. In our implementation, using limited number of pri-
ority queues, Karuna approximates SJF by assigning each
priority to type 2 ﬂows within a range of sizes. We denote
{βi} as the splitting thresholds, so that a ﬂow with size x
is given priority i if βi−1<x≤βi (β0=0 and βK=∞). With
this, we formulate and solve an optimization problem to ob-
tain the optimal splitting thresholds for different priorities
(see Appendix A for details). Karuna effectively performs
quantized SJF on type 2 ﬂows using these thresholds.
5.2 Sieving type 3 ﬂows
Type 3 ﬂows differ from type 2 ﬂows in that their sizes are
unknown. As a result, there is no ground-truth for Karuna to
directly split type 3 ﬂows into different priority queues for
approximating SJF. Inspired by PIAS [6] and Least Attained
Service scheduling [15, 16], Karuna addresses this issue by
sieving type 3 ﬂows through multiple priority queues, which
emulates SJF without knowing ﬂow sizes.
Speciﬁcally, in the lifetime of a type 3 ﬂow, Karuna sieves
it from higher priority queues to lower priority queues based
on the number of bytes it has sent. In this process, smaller
ﬂows are likely to complete in the ﬁrst few priority queues,
whereas long ﬂows eventually sink to the lowest priority
queues. In this way, Karuna ensures that short type 3 ﬂows
are generally prioritized over long ﬂows. All type 3 ﬂows
are at ﬁrst given the highest priority, and they are moved to
lower priorities as they send more bytes. The sieving thresh-
olds are denoted as {αi}. A ﬂow, which has transmitted x
bytes, is given priority i if αi−1<x≤αi.
The idea of sieving type 3 ﬂows to minimize FCT has
been well studied in [6]. However, in Karuna, we need to
address both type 2 and type 3 ﬂows together, which is a
different problem. We reformulate the threshold optimiza-
tion problem in [6] to jointly solve for the splitting thresh-
olds for type 2 ﬂows, and the sieving thresholds for type 3
ﬂows. We pose this as a sum-of-quadratic-ratios problem,
for which the solution in [6] is not applicable. Therefore,
we relax the problem to a quadratic programming problem
with linear constraints, and solve the relaxed problem (see
Appendix A). We further investigate the effectiveness and
robustness of the optimized thresholds in §8.2.1 and §8.2.3.
6. PRACTICAL ISSUES
We further examine several practical issues with Karuna,
and discuss how to solve them.
Starvation: Using strict priority queueing in switches can
potentially starve certain ﬂows. A key beneﬁt of Karuna is
that it throttles deadline ﬂows in the ﬁrst priority queue us-
ing conservative rates, leaving the rest of bandwidth to non-
deadline ﬂows. In the extreme case, if deadline ﬂows have
to take up all the bandwidth for their deadlines, non-deadline
ﬂows will starve. There is not much a transport mechanism
can do in such case, and the operators should consider in-
creasing the network capacity.
In another scenario, deadline ﬂows and small non-deadline
ﬂows in the higher priority queues can starve large non-
deadline ﬂows in the lowest priority queue. To counter this,
we employ ﬂow aging to elevate the priority of the ﬂows that
are being starved. Karuna identiﬁes starved ﬂows at end-
hosts by observing time-out events. For example, when a
ﬂow experiences κ TCP timeouts, Karuna elevates this ﬂow
to a higher priority. In our implementation, if it is a type 2
ﬂow, we re-split it to the queue based on the remaining size;
if it is a type 3 ﬂow, we move it to the highest priority queue
for non-deadline ﬂows (Queue 2) and let it re-sieve. We pick
κ from [2,10] uniformly at random for each ﬂow, so that two
long ﬂows with similar size can avoid synchronization and
congestion collapse [12]. Lifting the priorities of different
ﬂows with a random κ allows some of them to have higher
priority earlier (and thus ﬁnish earlier). We note that such
priority elevation may potentially lead to packet re-ordering,
but it is not a serious issue, since TCP can handle it well for
long ﬂows. In our experiments, we found that ﬂow aging
is effective in solving starvation, and priority elevation does
not have negative side-effects (see §8.1.3 and §8.2.2).
Trafﬁc variation across time and space: Trafﬁc in DCNs
can vary across both time and space. Fortunately, such trafﬁc
variation does not affect type 1 ﬂows, because type 1 ﬂows
are protected in highest priority queue. However, it does
potentially affect types 2&3 ﬂows, because they need to be
split or sieved into multiple queues according to respective
thresholds derived from a global trafﬁc distribution. As a
result, Karuna needs to dynamically update the thresholds
as trafﬁc varies.
It is challenging to accurately match the thresholds to the
trafﬁc. First, the distribution is always changing, and it takes
time to collect sizes and distribute thresholds. Second, traf-
ﬁc also varies in space, and thresholds derived from a global
distribution may not be perfect for each switch. When there
is a mismatch between the trafﬁc and thresholds, either pack-
ets of long ﬂows are mis-split (type 2) or stay too long (type
3) in the higher priority queue, or packets of short ﬂows are
mis-split (type 2) or get prematurely sieved (type 3) to the
lower priority queue. In both cases, the outcome is that short
ﬂows may queue behind long ﬂows, increasing their latency.
We ﬁnd that ECN used in network term estimation can
also be used to to address this problem. With ECN, we can
effectively keep low buffer occupation and minimize the im-
pact of long ﬂows on short ﬂows. In our evaluation, we ﬁnd
this effectively addresses such thresholds-trafﬁc mismatch
and makes Karuna resilient to trafﬁc variation (see §8.1.3
and §8.2.3).
One beneﬁt of Karuna’s resilience to trafﬁc variation is
that Karuna can afford to update thresholds infrequently. Thus,
we periodically update the thresholds at a ﬁxed time period,
determined by the time it takes to collect/distribute infor-
mation from/to the network (which depends on its scale).
However, our threshold computation (see Appendix) is fast,