problem, i.e., they learn ML models to directly predict input
patterns that can achieve higher code coverage. By contrast, we
ﬁrst use NNs to smoothly approximate the program branching
behavior and then leverage gradient-guided input generation
technique to achieve higher coverage. Therefore, our approach
is more tolerant to learning errors by ML models than the end-
to-end approaches. In this paper, we empirically demonstrate
that our strategy outperforms end-to-end modeling both in
terms of ﬁnding bugs and achieving higher edge coverage [72].
Taint-based fuzzing. Several evolutionary fuzzers have tried
to use taint
information to identify promising mutating
locations [85], [42], [63], [73], [55], [22]. For example,
TaintScope [85] is designed to identify input bytes that
affects system/library calls and focus on mutating these bytes.
Similarly, Dowser [42] and BORG [63] speciﬁcally use taint
information to target detection of buffer boundary violations
and buffer over-read vulnerabilities respectively. By contrast,
Vuzzer [73] captures magic constants through static analysis
and mutates existing values to these constants. Steelix [55]
instruments binaries to collect additional taint information about
comparing instructions. Finally, Angora [22] uses dynamic taint
tracking to identify promising mutation locations and perform
coordinate descent to guide mutations on these locations.
However, all
these taint-tracking-based approaches are
fundamentally limited by the fact that dynamic taint analysis
incurs very high overhead while static taint analysis suffers
from a high rate of
false positives. Our experimental
results demonstrate that NEUZZ easily outperforms existing
state-of-the-art taint-based fuzzers by using neural networks
to identify promising locations for mutation.
Several
[83],
[22] have tried to use different forms of gradient-guided
input generators [43],
fuzzers and test
optimization algorithms directly on the target programs.
However, without program smoothing, such techniques tend
to struggle and get stuck at the discontinuities.
and
execution. Symbolic
Symbolic/concolic
concolic
execution [50], [14], [77], [61], [36] use Satisﬁability Modulo
Theory (SMT) solvers to solve path constraints and ﬁnd
interesting test inputs. Several projects have also tried to
combining fuzzing with such approaches [17], [32], [82].
Unfortunately, these approaches struggle to scale in practice
due to several fundamental limitations of symbolic analysis
including path explosion, incomplete environment modeling,
large overheads of symbolic memory modeling, etc. [16].
Concurrent
to our work, NEUEX [79] made symbolic
execution more efﬁcient by learning the dependencies between
intermediate variables of a program using NNs and used
gradient-guided neural constraint
solving together with
traditional SMT solvers. By contrast, in this paper, we focus
on using NNs to make fuzzing more efﬁcient as it is by far
the most popular technique for ﬁnding security-critical bugs
in large, real-world programs.
Neural programs. A neural program is essentially a neural
learns a latent representation of the target
network that
program’s logic. Several
recent works have synthesized
such neural programs from input-output samples of a
program to accurately predict the program’s outputs for new
inputs [41], [74], [62]. By contrast, we use NNs to learn
smooth approximations of a program’s branching behaviors.
IX. CONCLUSION
We present NEUZZ, an efﬁcient learning-enabled fuzzer that
uses a surrogate neural network to smoothly approximate a
target program’s branch behavior. We further demonstrate how
gradient-guided techniques can be used to generate new test
inputs that can uncover different bugs in the target program. Our
extensive evaluations show that NEUZZ signiﬁcantly outper-
forms other 10 state-of-the-art fuzzers both in the numbers of
detected bugs and achieved edge coverage. Our results demon-
strate the vast potential of leveraging different gradient-guided
input generation techniques together with neural smoothing to
signiﬁcantly improve the effectiveness of the fuzzing process.
ACKNOWLEDGEMENT
We thank our shepherd Matthew Hicks and the anonymous
reviewers for their constructive and valuable feedback. This
work is sponsored in part by NSF grants CNS- 18-42456, CNS-
18-01426, CNS-16-17670, CNS-16-18771, CCF-16-19123,
CNS-15-63843, and CNS-15-64055; ONR grants N00014-17-
1-2010, N00014-16-1- 2263, and N00014-17-1-2788; an ARL
Young Investigator (YIP) award; a Google Faculty Fellowship;
and a Amazon Web Services grant. Any opinions, ﬁndings,
conclusions, or recommendations expressed herein are those
of the authors, and do not necessarily reﬂect those of the US
Government, ONR, ARL, NSF, Google, or Amazon.
(cid:25)(cid:18)(cid:22)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply. 
REFERENCES
[1] Guided in-process
fuzzing of Chrome
components.
https:
//security.googleblog.com/2016/08/guided-in-process-fuzzing-of-
chrome.html, 2016.
[2] DARPA
challenge
for
Linux, Windows,
and macOS.
https://github.com/trailofbits/cb-multios, 2017.
[3] Google fuzzing service for OS ﬁnds 1k bugs in ﬁve months.
http://www.eweek.com/cloud/google-fuzzing-service-for-os-ﬁnds-1k-
bugs-in-ﬁve-months, 2017.
[4] Address
sanitizer,
https://github.com/google/sanitizers, 2018.
sanitizer,
thread
and memory
sanitizer.
[5] Keras: The python deep learning library. https://keras.io/, 2018.
[6] An open source machine
learning framework for
everyone.
https://www.tensorﬂow.org/, 2018.
sanitizer.
behavior
[7] Undeﬁned
UndeﬁnedBehaviorSanitizer.html, 2018.
https://clang.llvm.org/docs/
[8] W. C. Abraham and A. Robins. Memory retention–the synaptic stability
versus plasticity dilemma. Trends in neurosciences, 28(2):73–78, 2005.
[9] O. Bastani, R. Sharma, A. Aiken, and P. Liang. Synthesizing program
input grammars. In Proceedings of the 38th ACM SIGPLAN Conference
on Programming Language Design and Implementation (PLDI), 2017.
[10] D. P. Bertsekas and A. Scientiﬁc. Convex optimization algorithms.
Athena Scientiﬁc Belmont, 2015.
[11] M. Böhme, V.-T. Pham, and A. Roychoudhury. Coverage-based
greybox fuzzing as markov chain. In Proceedings of the ACM SIGSAC
Conference on Computer and Communications Security (CCS), pages
1032–1043. ACM, 2016.
[12] K. Böttinger, P. Godefroid, and R. Singh. Deep Reinforcement Fuzzing.
arXiv preprint arXiv:1801.04589, 2018.
[13] R. H. Byrd, P. Lu, J. Nocedal, and C. Zhu. A limited memory algorithm
SIAM Journal on Scientiﬁc
for bound constrained optimization.
Computing, 16(5):1190–1208, 1995.
[14] C. Cadar, D. Dunbar, D. R. Engler, et al. KLEE: Unassisted and automatic
generation of high-coverage tests for complex systems programs. In
Proceedings of the USENIX Symposium on Operating Systems Design
and Implementation (OSDI), volume 8, pages 209–224, 2008.
[15] C. Cadar, P. Godefroid, S. Khurshid, C. S. Pasareanu, K. Sen, N. Tillmann,
and W. Visser. Symbolic execution for software testing in practice: pre-
liminary assessment. In Proceedings of the 33rd International Conference
on Software Engineering (ICSE), pages 1066–1071. IEEE, 2011.
[16] C. Cadar and K. Sen. Symbolic execution for software testing: three
decades later. Communications of the ACM, 56(2):82–90, 2013.
[17] S. K. Cha, M. Woo, and D. Brumley. Program-adaptive mutational
fuzzing. In Proceedings of the IEEE Symposium on Security & Privacy,
2015.
[18] S. Chaudhuri, S. Gulwani, and R. Lublinerman. Continuity analysis of
programs. In ACM Sigplan Notices, volume 45, pages 57–70. ACM, 2010.
[19] S. Chaudhuri, S. Gulwani, and R. Lublinerman. Continuity and robustness
of programs. Communications of the ACM, 55(8):107–115, 2012.
[20] S. Chaudhuri and A. Solar-Lezama. Smooth interpretation. ACM Sigplan
Notices, 45(6):279–291, 2010.
[21] S. Chaudhuri and A. Solar-Lezama. Smoothing a program soundly and
robustly. In International Conference on Computer Aided Veriﬁcation,
pages 277–292. Springer, 2011.
[22] P. Chen and H. Chen. Angora: Efﬁcient fuzzing by principled search.
2018.
[23] A. Conn, K. Scheinberg, and P. Toint. On the convergence of
derivative-free methods for unconstrained optimization. Approximation
theory and optimization: tributes to MJD Powell, pages 83–108, 1997.
[24] A. R. Conn, K. Scheinberg, and P. L. Toint. Recent progress in
unconstrained nonlinear optimization without derivatives. Mathematical
programming, 79(1-3):397, 1997.
[25] DARPA.
Cyber Grand Challenge.
http://archive.darpa.mil/
cybergrandchallenge/, 2016.
[26] DARPA.
Cyber Grand Challenge Repository.
https:
//github.com/cybergrandchallenge, 2017.
[27] B. Dolan-Gavitt. Of Bugs and Baselines. http://moyix.blogspot.com/
2018/03/of-bugs-and-baselines.html, 2018.
[28] B. Dolan-Gavitt, P. Hulin, E. Kirda, T. Leek, A. Mambretti, W. Robertson,
F. Ulrich, and R. Whelan. LAVA: Large-scale automated vulnerability
addition. In Proceedings of the IEEE Symposium on Security & Privacy,
pages 110–121, 2016.
[29] T. J. Draelos, N. E. Miner, C. C. Lamb, J. A. Cox, C. M. Vineyard, K. D.
Carlson, W. M. Severa, C. D. James, and J. B. Aimone. Neurogenesis
deep learning: Extending deep networks to accommodate new classes. In
Proceedings of the International Joint Conference on Neural Networks
(IJCNN), pages 526–533. IEEE, 2017.
[30] F. Duchene, S. Rawat, J.-L. Richier, and R. Groz. KameleonFuzz:
evolutionary fuzzing for black-box XSS detection. In Proceedings of
the 4th ACM conference on Data and application security and privacy,
pages 37–48. ACM, 2014.
[31] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu,
A. Pritzel, and D. Wierstra. Pathnet: Evolution channels gradient descent
in super neural networks. arXiv preprint arXiv:1701.08734, 2017.
[32] J. Fietkau, B. Shastry, and J.-P. Seifert. KleeFL - seeding fuzzers with
symbolic execution. https://github.com/julieeen/kleeﬂ, 2017.
[33] K.-I. Funahashi. On the approximate realization of continuous mappings
by neural networks. Neural networks, 2(3):183–192, 1989.
[34] A. Garg and K. Tai. Comparison of statistical and machine learning
methods in modelling of data with multicollinearity.
International
Journal of Modelling, Identiﬁcation and Control, 18(4):295–312, 2013.
[35] P. Godefroid, N. Klarlund, and K. Sen. DART: directed automated random
testing. In ACM Sigplan Notices, volume 40, pages 213–223. ACM, 2005.
[36] P. Godefroid, M. Y. Levin, D. A. Molnar, et al. Automated whitebox
In Proceedings of the Network and Distributed System
fuzz testing.
Security Symposium (NDSS), volume 8, pages 151–166, 2008.
[37] P. Godefroid, H. Peleg, and R. Singh. Learn&Fuzz: Machine learning
for input fuzzing. In Proceedings of the 32nd IEEE/ACM International
Conference on Automated Software Engineering, 2017.
[38] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio. Deep learning,
volume 1. MIT press Cambridge, 2016.
[39] I. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing
adversarial examples. In Proceedings of the International Conference
on Learning Representations (ICLR), 2015.
[40] B. Goodrich and I. Arel. Unsupervised neuron selection for mitigating
catastrophic forgetting in neural networks.
In Proceedings of the
International Symposium on Circuits and Systems, pages 997–1000.
Citeseer, 2014.
[41] A. Graves, G. Wayne, and I. Danihelka. Neural turing machines. arXiv
preprint arXiv:1410.5401, 2014.
[42] I. Haller, A. Slowinska, M. Neugschwandtner, and H. Bos. Dowsing
for overﬂows: A guided fuzzer to ﬁnd buffer boundary violations. In
Proceedings of the 22nd USENIX Security Symposium, 2013.
[43] M. Harman and P. McMinn. A theoretical and empirical study of
search-based testing: Local, global, and hybrid search. IEEE Transactions
on Software Engineering, 36(2):226–247, 2010.
[44] G. E. Hinton and D. C. Plaut. Using fast weights to deblur old memories.
In Proceedings of the ninth annual conference of the Cognitive Science
Society, pages 177–186, 1987.
[45] S. Hocevar. zzuf—multi-purpose fuzzer. http://caca.zoy.org/wiki/zzuf,
2011.
[46] R. Horst and P. M. Pardalos. Handbook of global optimization, volume 2.
Springer Science & Business Media, 2013.
[47] L. Intel. Circumventing fuzzing roadblocks with compiler transformations.
https://laﬁntel.wordpress.com/2016/08/15/circumventing-fuzzing-
roadblocks-with-compiler-transformations/, 2016.
[48] R. Kemker, A. Abitino, M. McClure, and C. Kanan. Measuring
arXiv preprint
forgetting in neural networks.
catastrophic
arXiv:1708.02072, 2017.
[49] S. Khurshid, C. S. P˘as˘areanu, and W. Visser. Generalized symbolic
execution for model checking and testing.
In Proceedings of the
International Conference on Tools and Algorithms for the Construction
and Analysis of Systems, pages 553–568. Springer, 2003.
[50] J. King. Symbolic execution and program testing. Communications of
the ACM, 19(7):385–394, 1976.
[51] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins,
A. A. Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, et al.
Overcoming catastrophic forgetting in neural networks. Proceedings
of the national academy of sciences, 2017.
[52] T. G. Kolda, R. M. Lewis, and V. Torczon. Optimization by direct
search: New perspectives on some classical and modern methods. SIAM
review, 45(3):385–482, 2003.
[53] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet classiﬁcation
with deep convolutional neural networks. In Proceedings of the Advances
in neural information processing systems (NIPS), pages 1097–1105, 2012.
(cid:25)(cid:18)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply. 
[81] S. Sivakorn, G. Argyros, K. Pei, A. D. Keromytis, and S. Jana. HVLearn:
Automated black-box analysis of hostname veriﬁcation in SSL/TLS
implementations. In Proceedings of the IEEE Symposium on Security
& Privacy, pages 521–538, May 2017.
[82] N. Stephens, J. Grosen, C. Salls, A. Dutcher, R. Wang, J. Corbetta,
Y. Shoshitaishvili, C. Kruegel, and G. Vigna. Driller: Augmenting
fuzzing through selective symbolic execution. In Network and Distributed
System Security Symposium (NDSS), volume 16, pages 1–16, 2016.
[83] L. Szekeres. Memory corruption mitigation via hardening and testing.
PhD thesis, Stony Brook University, 2017.
[84] J. Wang, B. Chen, L. Wei, and Y. Liu. Skyﬁre: Data-driven seed
In Proceedings of the IEEE Symposium on
generation for fuzzing.
Security & Privacy, 2017.
[85] T. Wang, T. Wei, G. Gu, and W. Zou. TaintScope: A checksum-aware
directed fuzzing tool for automatic software vulnerability detection. In
Proceedings of the IEEE Symposium on Security & Privacy, 2010.
[86] S. Wright and J. Nocedal. Numerical optimization. Springer Science,
35(67-68):7, 1999.
[87] J. Yosinski, J. Clune, T. Fuchs, and H. Lipson. Understanding neural
networks through deep visualization. In 2015 ICML Workshop on Deep
Learning, 2015.
[88] M. Zalewski.
(AFL) README.
Fuzzy Lop
American
http://lcamtuf.coredump.cx/aﬂ/README.txt, 2018.
[89] D. W. Zingg, M. Nemec, and T. H. Pulliam. A comparative evaluation
of genetic and gradient-based algorithms applied to aerodynamic
optimization. European Journal of Computational Mechanics/Revue
Européenne de Mécanique Numérique, 17(1-2):103–126, 2008.
[54] lcamtuf.
Fuzzing random programs without execve().
https:
//lcamtuf.blogspot.com/2014/10/fuzzing-binaries-without-execve.html,
2014.
[55] Y. Li, B. Chen, M. Chandramohan, S.-W. Lin, Y. Liu, and A. Tiu.
Steelix: Program-state based binary fuzzing. In Proceedings of the 11th
Joint Meeting on Foundations of Software Engineering (FSE), 2017.
[56] A. Mahendran and A. Vedaldi. Understanding deep image representations
In Proceedings of the IEEE Conf. on Computer
by inverting them.
Vision and Pattern Recognition (CVPR), 2015.
[57] M. McCloskey and N. J. Cohen.
Catastrophic interference in
connectionist networks: The sequential learning problem. In Psychology
of learning and motivation, volume 24, pages 109–165. Elsevier, 1989.
Catastrophic interference in
connectionist networks: The sequential learning problem. In Psychology
of learning and motivation, volume 24, pages 109–165. Elsevier, 1989.
[59] B. Miller, L. Fredriksen, and B. So. An empirical study of the reliability
[58] M. McCloskey and N. J. Cohen.
of unix utilities. Communications of the ACM, 33(12):32–44, 1990.
[60] B. P. Miller. Fuzz testing of application reliability. UW-Madison
Computer Sciences, 2007.
[61] D. Molnar, X. C. Li, and D. A. Wagner. Dynamic test generation to
ﬁnd integer bugs in x86 binary Linux programs. In Proceedings of the
18th Conference on USENIX Security Symposium, 2009.
[62] A. Neelakantan, Q. V. Le, M. Abadi, A. McCallum, and D. Amodei.
Learning a natural language interface with neural programmer. arXiv
preprint arXiv:1611.08945, 2016.
[63] M. Neugschwandtner, P. Milani Comparetti, I. Haller, and H. Bos. The
BORG: Nanoprobing binaries for buffer overreads. In Proceedings of
the 5th ACM Conference on Data and Application Security and Privacy,
2015.
[64] N. Nichols, M. Raugas, R.
Jasper, and N. Hilliard.
fuzzing: Reinitialization with deep neural models.
arXiv:1711.02807, 2017.
Faster
arXiv preprint
[65] J. Nocedal. Updating quasi-newton matrices with limited storage.
Mathematics of computation, 35(151):773–782, 1980.
[66] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and
A. Swami. The limitations of deep learning in adversarial settings. In Pro-
ceedings of the IEEE European Symposium on Security & Privacy, 2015.
[67] D. Parnas. Software aspects of strategic defense systems. Communications
of the ACM, 28(12):1326–1335, 1985.
[68] H. Peng, Y. Shoshitaishvili, and M. Payer. T-Fuzz: fuzzing by program
transformation. Proceedings of the IEEE Symposium on Security &
privacy, 2018.
[69] H. Peng, Y. Shoshitaishvili, and M. Payer. T-Fuzz: fuzzing by program
transformation. In Proceedings of the IEEE Symposium on Security &
Privacy, 2018.
[70] T. Petsios, J. Zhao, A. D. Keromytis, and S. Jana. SlowFuzz: automated
domain-independent detection of algorithmic complexity vulnerabilities.
In Proceedings of the ACM SIGSAC Conference on Computer and
Communications Security (CCS), pages 2155–2168, 2017.
[71] M. J. Powell. Uobyqa: unconstrained optimization by quadratic
approximation. Mathematical Programming, 92(3):555–582, 2002.
[72] M. Rajpal, W. Blum, and R. Singh. Not All Bytes Are Equal: Neural
Byte Sieve for Fuzzing. arXiv preprint arXiv:1711.04596, 2017.
[73] S. Rawat, V. Jain, A. Kumar, L. Cojocar, C. Giuffrida, and H. Bos.
VUzzer: Application-aware evolutionary fuzzing. In Proceedings of the
Network and Distributed Systems Security Conference (NDSS), 2017.
[74] S. Reed and N. d. Freitas. Neural Programmer-Interpreters. arXiv
preprint arXiv:1711.04596, 2015.
[75] B. Ren, H. Wang, J. Li, and H. Gao. Life-long learning based on dynamic
combination model. Applied Soft Computing, 56:398–404, 2017.
[76] A. Robins. Catastrophic forgetting, rehearsal and pseudorehearsal.
Connection Science, 7(2):123–146, 1995.
[77] K. Sen, D. Marinov, and G. Agha. CUTE: a concolic unit testing engine
for C. In ACM SIGSOFT Software Engineering Notes, volume 30, pages
263–272. ACM, 2005.
[78] K. Serebryany. libFuzzer – a library for coverage-guided fuzz testing.
http://llvm.org/docs/LibFuzzer.html, 2018.
[79] S. Shen, S. Ramesh, S. Shinde, A. Roychoudhury, and P. Saxena.
Neuro-symbolic execution: The feasibility of an inductive approach to
symbolic execution. arXiv preprint arXiv:1807.00575, 2018.
[80] K. Simonyan, A. Vedaldi, and A. Zisserman. Deep inside convolutional
networks: Visualising image classiﬁcation models and saliency maps.
In Proceedings of the IEEE Conf. on Computer Vision and Pattern
Recognition (CVPR), 2013.
(cid:25)(cid:18)(cid:24)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:52:15 UTC from IEEE Xplore.  Restrictions apply.