Subject: Issue with Batch Normalization in Prediction Mode After Restoring a TensorFlow Model

Hello,

I am currently using TensorFlow, specifically the last successful build #85. I have successfully trained a classifier that includes batch normalization and saved it using `classifier.save(logdir)`. When I attempt to restore this model using `classifier = tf.contrib.skflow.TensorFlowEstimator.restore(logdir)`, the restoration process appears to be successful. However, upon making predictions with `classifier.predict(X_test)`, it seems that the batch normalization layer is still operating in training mode rather than switching to inference (or prediction) mode.

Upon inspecting the source code for the batch normalization operations (`batch_norm_ops.py`), I noticed the following relevant snippet:

```python
is_training = array_ops_.squeeze(ops.get_collection("IS_TRAINING"))
mean, variance = control_flow_ops.cond(is_training,
                                       update_mean_var,
                                       lambda: (ema_mean, ema_var))
```

From this, I infer that the "IS_TRAINING" flag, which determines whether the model should use the current batch's statistics or the moving averages, is set to `True` at the time of saving the model. As a result, when the model is restored and used for prediction, the condition within the `control_flow_ops.cond` function continues to evaluate as `True`, leading to the use of `update_mean_var` instead of `(ema_mean, ema_var)` during the prediction phase, even though the actual intention is to switch to the latter for inference.

This issue becomes particularly noticeable in a binary classification scenario where a batch normalization layer precedes the softmax activation. In such cases, one can observe that the mean of the softmax outputs for any test data tends towards 0, indicating an incorrect application of batch normalization parameters during the prediction stage.

A potential solution to this problem might involve replacing the "IS_TRAINING" variable with a placeholder, allowing us to explicitly control its value (i.e., setting it to `False` for prediction). This way, we could ensure that the correct set of mean and variance values (the moving averages) are utilized during the inference phase.

Am I overlooking any other aspects or solutions that could address this behavior more effectively?

Thank you for your insights and assistance.

Best regards,  
[Your Name]