ject. The second opcode, write(file: &File, data:
Vec) takes a reference to a ﬁle object and again some
data that will be written and returns no value. Any number of
such write opcodes can reuse the same File object. The last
opcode close(file: File) consumes the File object,
and no further operations are possible on the ﬁle.
The graph encoding the test case shown in Listing 1 can
be seen in Figure 2. The input graphs generated from this
bytecode speciﬁcation are stored in a very compact serialized
format. During fuzzing, they are stored, generated, and mu-
tated directly in the memory shared between the fuzzer and the
agent. Consequently, we avoid unnecessary copy operations
and perform no allocations to generate the graphs.
The target component parses the graph stored in the shared
memory. To ease this task, we automatically compile the byte-
code speciﬁcation to a single C header ﬁle that implements a
bytecode interpreter. To compile the bytecode, the user has to
provide a C implementation of the behavior of each node. As
the tree-shaped data needs to be mutated, the fuzzer needs to
be aware of the structure and thus, they need to be described
in the speciﬁcation. Consequently, the C structs representing
these values can be generated automatically. On the other
hand, the fuzzer does not need to modify or use the values
that are created in the edges. Hence, the user can use arbitrary
C types as edge types.
4
Implementation Details
To be able to evaluate the impact of our design choices, we
implemented a prototype of our design. In this section, we
start by describing the steps we took to implement a high
performance, coverage-guided fuzzer backend which allows
us to run stable and deterministic fuzzing sessions. This in-
cludes getting coverage information, providing fast snapshot
reloads, and facilitating communication between the agent
and the fuzzer. We then describe the implementation details
of the fuzzing frontend that generates and mutates our afﬁne
typed bytecode programs. The prototype implementation is
available at https://github.com/RUB-SysSec/nyx.
4.1 Backend Implementation
The backend basically has to provide three features to the
frontend: (i) It has to measure the coverage produced by a
given test input, (ii) it has to provide a stable environment
that can handle misbehaving targets, and (iii) it has to pro-
vide communication channels. We build upon QEMU-PT and
KVM-PT as released in REDQUEEN and extended the imple-
mentation with the capabilities discussed in Section 3. We
now discuss how we implemented these three components.
4.1.1 Fast Coverage
To obtain coverage information from the target hypervisor,
we use the Intel-PT decoder released by Aschermann et al. [2]
as a basis for our coverage measurement. However, we added
some improvements on top of the original code that aim to
increase the decoding performance. The decoder consists of
two components: the Intel-PT parser, and the disassembler
that follows the trace through a disassembled control ﬂow
graph taken from a memory snapshot. We rewrote the decoder
to utilize an optimization technique known as “computed-
gotos”. As tracing the control ﬂow through the disassembled
control ﬂow graph is expensive, we also introduced a caching
layer. This layer can turn Intel-PT data directly into coverage
information (AFL-style bitmap entries [64]) if the same trace
fragments have been observed previously.
3.5 Applications beyond Hypervisor Fuzzing
4.1.2 Fast Snapshot Reloads
While this paper focuses on hypervisor fuzzing, all of the
techniques described here are working with any other kind of
software as well. Our prototype is capable of fuzzing hypervi-
sors, operating systems, and ring-3 applications in a uniﬁed
framework. This kind of structural speciﬁcation can be used to
express many different kind of fuzzing scenarios. For example,
in an ofﬂine experiment, we ported some of the SYZKALLER
speciﬁcations to our fuzzer. We also built a harness that al-
lows to explore the impact of fuzzing environment variables,
commandline arguments, as well as, STDIN and multiple ﬁles
as inputs to a ring-3 application at the same time.
Starting each test case from a clean snapshot is important to
obtain deterministic coverage results. If previous test cases
can affect the coverage produced by later test cases, coverage-
guided fuzzing performs signiﬁcantly worse. One of the major
features of NYX is the ability to restore VM snapshots many
thousands of times per second. To implement rapid snapshot
reloads, we need to reload three components of the VM. First
of all, the register state of the emulated CPU itself has to be
reset. Secondly, we also need to reset all modiﬁed pages of the
memory used by the virtual machine. Lastly, the state of all
devices emulated in QEMU (including hard disks) needs to
USENIX Association
30th USENIX Security Symposium    2603
pathopenwritedup2close"/tmp/A""foo"PPFFFFFbe reset. We now describe the details of the mechanisms used
to reset these components except for resetting the register
(which is trivial).
Fast Memory Resets To create a snapshot of the VM mem-
ory, we create a snapshot ﬁle that contains a dump of the
whole memory of the VM. We also implement a delta mecha-
nism that allows to create incremental update of this snapshot
ﬁle. Typically, we create one full snapshot per OS type, and
then use the delta snapshots at the start of the ﬁrst input. To
create this snapshot, we implemented a hypercall that the
agent uses to inform the fuzzer that it should create the incre-
mental snapshot from which each test case will be started.
To quickly reset the memory of the VM, we use our own
dirty page logger in KVM-PT. By default, KVM already pro-
vides the capabilities to log which pages have been dirtied
since the last time the CPU entered the VM (VM-Entry).
However, since KVM’s technique requires us to walk a large
bitmap to ﬁnd all dirty pages, we extended KVM-PT with the
capability to store the addresses of dirty pages in an additional
stack-like buffer. This can signiﬁcantly accelerate the mem-
ory restoration process, especially in cases where only a few
pages have been dirtied. Additionally, we need to ensure that
memory that is changed by the devices emulated by QEMU-
PT is also reset. To this end, we track a second map where
VM pages modiﬁed by QEMU-PT are also noted. Before we
start the next execution, each page that was changed either
inside the VM (as tracked by KVM-PT) or by QEMU-PT is
reset to the original content from the snapshot.
Fast Device Resets Resetting the device state is a much
more involved procedure compared to resetting the memory
of the VM. As noted before, QEMU manages a multitude of
devices. QEMU also provides a serialization/deserialization
mechanism for each device, which is used to store snapshots
of running VMs on the hard disk. Each device emulator pro-
vides a speciﬁcation for its state in form of a speciﬁc data
structure. QEMU iterates this data structure to identify ﬁelds,
integers, arrays, and so on. During serialization, these ﬁelds
get converted into a JSON string that can later be loaded dur-
ing deserialization. The whole process is painfully slow, but
ensures that VM snapshots can be loaded even on different
machines (where the compiler may change the in-memory
layout). To increase the performance, we mostly ignore these
device structure speciﬁcations. Instead, we log all writes once
during this process and obtain a list of all memory used by
the devices. Using this list, we can now reset the device’s
memory from our snapshot with a series of calls to memcpy. It
should be noted that a small subset of devices cannot be reset
like this, as they require to run some custom code after each
reset. We manually identiﬁed these devices in QEMU-PT
and call the original deserialization routine for these devices
speciﬁcally. Note that physical hardware which is used by the
guest via pass-through cannot be reset, as it is not possible to
access that state stored in real hardware.
Fast Disk Reset QEMU handles hard disks differently from
other devices. As their state is very large—potentially larger
than the available memory—the guest’s hard disk content
is stored on the host’s hard disk in a so-called qcow ﬁle. To
ensure we can handle targets that write ﬁles to hard disk,
we create our own overlay layer on top of QEMU’s qcow
handling. During the execution, we create a hashmap that
stores the content of modiﬁed sectors. This hashmap is stored
in memory and uses a ﬁxed set of buffer of pages. Every read
access to the disk image is ﬁrst checked against this hashmap,
and then against the original qcow ﬁle. We place an upper limit
on the number of sectors to be written during one test case to
ensure that misbehaving processes do not destroy the overall
fuzzing performance, similar to how AFL places limits on the
time and memory used per test case. Resetting the disk image
is then as easy as zeroing out the small hashmap. Critically,
we do not need to overwrite the actual disk data, as removing
the indices in the map sufﬁces. Overall, this makes the reset
process highly efﬁcient and effective.
4.1.3 Nested Hypervisor Communication
To intercept and distinguish our fuzzing hypercalls from nor-
mal hypercalls directed to the target hypervisor, we imple-
mented an additional, simple check in the host’s vmcall han-
dler routine. If a special value is placed in the RAX register by
the guest, the hypercall request is handled by KVM-PT. Oth-
erwise, this request is passed to the target hypervisor. To set
up a shared memory mapping between the host and the agent
OS, we need to allocate this memory region in L2 ﬁrst. Using
our hypercall interface, we pass all physical addresses of our
allocated memory region to the host by executing a special
hypercall. The host translates all guest physical addresses to
host virtual QEMU-PT addresses and creates a shared mem-
ory mapping. A visualization of this procedure is given in
Figure 3 1(cid:13). This shared memory region is later used by the
fuzzing logic to receive messages from the agent OS or to
pass new generated inputs to the agent. Prior to entering the
fuzzing loop, the agent OS (L2) executes a special hypercall
to create the snapshot for the fuzzing loop. The hypercall is
handled by KVM-PT, and instead of relaying it to the target
hypervisor (L1), another VM exit reason is passed. On the
next VM entry transition from the target hypervisor to the
agent OS, the snapshot will be created by QEMU-PT. This
procedure is visualized in Figure 3 2(cid:13). Once the fuzzing en-
gine has generated a new input, the snapshot is restored, and
the execution is continued in the agent OS running in L2. On
each transition from L2 to L1, Intel PT tracing is enabled,
and disabled vice versa. This communication is shown in
Figure 3 3(cid:13).
2604    30th USENIX Security Symposium
USENIX Association
Figure 3: Overview of NYX’s hypercall interaction between the various components: fuzzing logic, QEMU-PT, KVM-PT, L1 guest, and agent OS.
4.2 Fuzzing Frontend for Afﬁne Typed Byte-
code Programs
The main task of the fuzzing frontend is to generate candidate
inputs and to pass the inputs to the agent OS. We implemented
our own fuzzing frontend in Rust. This frontend is speciﬁcally
designed to generate and mutate the bytecode inputs and we
now describe the relevant details of our implementation.
4.2.1 Representation of the Bytecode
As noted earlier, we take great care in NYX to enable fast
and effective input generation. Each input is stored in two
arrays. The graph layout is stored in one array of u16 integers.
The additional tree-shaped data arguments are stored in a
byte array. This ﬂat, pointerless format allows fast generation
and sharing via shared memory. Each node/opcode has a
ﬁxed number of arguments and outputs. We allow up to 216
different node types, each with a unique ID. To encode a
given node, we ﬁrst push the type ID, and then one edge ID
for each argument and return value. All edge IDs introduced
as a return value can then be used as argument IDs for later
nodes.
Example 2. Consider the input in Listing 1. Assume
the ID for the variable path is p and the ID for the
variable ﬁle is f. The graph would be encoded into the
following array: [n_new_path_id, p, n_open_id,
p, f,n_write_id, f, n_dup2_id, f, f,
n_close_id, f].
ﬁrst opcode
path=new_path("/tmp/A"), we ﬁrst push the ID
of new_path (n_new_path ), then we push the ID of the
only return value (p). Note that we ignore the additional data
argument for now. We encode the remaining nodes in the
same fashion by pushing the node ID and then the edge IDs
for each argument or return value.
encode
To
the
The additional tree-shaped/binary data attached to each
node is stored in a second buffer. As we know what kind of
data is attached to each node, the values are simply concate-
nated. For binary data that is dynamically sized (e.g., strings
or byte vectors), the size is preﬁxed.
Example 3. When considering the graph representing the
input in Listing 1, we would encode the binary data used
as additional arguments to new_path and write as:
[7,"/tmp/A\0",4,"foo\0"]. Here, 7 and 4 are the
lengths of the following strings. The strings are stored as raw
bytes.
4.2.2 Generating Bytecode Interpreters
To interpret the results, we automatically transpile the speci-
ﬁcations into a single C-header interpreter for the bytecode.
The user simply has to ﬁll in the functions for each opcode.
This interpreter uses the information provided in the speciﬁ-
cation to iterate both memory buffers, keep track of the values
that are passed along the edges in the graph, and call the user-
provided functions for each node. In our example, we used
HYPER-CUBE and linked this interpreter into HYPER-CUBE
to produce a fuzzing agent for NYX.
5 Evaluation
We use our prototype implementation of NYX to evaluate the
results of our design choices. In particular, we aim to answer
the following ﬁve research questions:
• RQ 1. How does NYX compare to state-of-the-art ap-
proaches such as HYPER-CUBE and VDF?
• RQ 2. Does coverage guidance improve generative
fuzzing?
USENIX Association
30th USENIX Security Symposium    2605
ioctl(KVM_RUN_VCPU)Exit: Create SnapshotL2 Hypercall: Start Fuzzing Inject VM-Exit (NMI)Trap on next VM-Entry to L2Create SnapshotExit: Fuzzing DoneExecute Payload(cid:609) SnapshotSetup (cid:610) Fuzzing LoopFuzzing Input RequestGenerate InputInput ReadyContinue Execution in L2L2 Hypercall: Fuzzing Done (cid:608) Payload BuﬀerSetupL2 Hypercall: Prepare BuﬀerAllocate Payload BufferExit: Prepare Payload BuﬀerTranslate L2 PF to L1 PFRemap NYX SHM to L2 Payload BufferContinue Execution in L2Nyx FuzzerQEMU-PTKVM-PT(Host VMM)Hypervisor(Level 1 Guest)Agent OS(Level 2 Guest)Decode PT Data and Restore SnapshotVM-Exit Request (e.g. PIO)Perform VM-Exit L2 to L1Enable PTVM-Entry RequestHandle PIO RequestDisable PTPerform VM-Entry L1 to L2Perform VM-Exit L2 to L1• RQ 3. What are the performance gains provided by the
structured mutation engine?
• RQ 4. What is the performance impact of fast reloads?
• RQ 5. Can NYX ﬁnd previously unknown vulnerabilities
in well-tested parts of hypervisors?
As we will see, NYX drastically outperforms VDF on
almost all devices and performs comparable or better than
HYPER-CUBE on all but one device. In four cases, NYX dras-
tically outperforms HYPER-CUBE, using speciﬁcations that
are chosen to mirror the behavior of HYPER-CUBE. If we
use properly customized speciﬁcations, the results are im-
proved further. We were able to uncover 44 new bugs, many
of which represent serious security issues. Using the fast
snapshot restoration allows us to reset the whole VM with a
performance characteristics comparable to AFL’s fork server.
5.1 Evaluation Setup
All experiments were performed on Intel Xeon Gold 6230
CPUs. Each machine had 40 physical cores and 192GB of
memory as well as an SSD. We pinned each fuzzer to one
physical core and did not use hyper-threading. Each experi-
ment was repeated ten times to obtain statistically signiﬁcant
results [32]. In all plots, the lines mark the median of the ten
runs, and the shaded area display the best and worst run respec-
tively. We targeted QEMU 5.0.0 and bhyve 12.1-RELEASE.
VDF was evaluated on older versions of QEMU and we can
only compare with the numbers reported in the paper. While
this slightly reduces the strength of the comparison to VDF,
we believe it is much more meaningful to fuzz modern, well-
tested software. Additionally, VDF was already shown to
be signiﬁcantly slower than HYPER-CUBE. We also repeat
the HYPER-CUBE experiments using the newer version of
QEMU and observe very similar results.
5.2 Fuzzing Device Emulators
In the ﬁrst experiment, we compare NYX against HYPER-