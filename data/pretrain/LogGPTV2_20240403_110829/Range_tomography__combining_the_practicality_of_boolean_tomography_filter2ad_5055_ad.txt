 0.7
 0.65
 0.6
 0.55
 0.5
Sum-Tomo
Tomo
Norm
y
c
a
r
u
c
c
a
e
g
a
r
e
v
A
 1
 0.95
 0.9
 0.85
 0.8
 0.75
 0.7
 0.65
 0.6
 0.55
 0.5
Sum-Tomo
Norm
 10
 20
 30
 40
 50
 60
 10
 20
 30
 40
 50
 60
 10
 20
 30
 40
 50
 60
Number of lossy links
(a) Precision
Number of lossy links
(b) Recall
Number of lossy links
(c) Accuracy
Figure 4: Results of diﬀerent algorithms obtained in PlanetLab when actual link loss rates follow Gilbert process.
even though their loss rates are very diﬀerent. Hence, Tomo
often identiﬁes fewer lossy links. On the contrary, Norm usu-
ally selects more lossy links to justify the loss rate of every
lossy path (the number of links reported by Norm as lossy is
30% more than Sum-Tomo). This is the reason that Norm
gives low precision and high recall. Moreover, the accuracy
results in Figure 2c show that Sum-Tomo can estimate a
loss rate range for lossy links quite well. Speciﬁcally, if a
link is correctly detected by Sum-Tomo as lossy, then with
a high probability (above 95%) its loss rate range includes
the actual loss rate of that link.
The results of the previous algorithms in the Internet2
topology are very similar with the results in the ESNet topol-
ogy (when considering the same fraction of lossy links), and
so they are omitted.
Figure 3 shows the results of the three tomography al-
gorithms in ESNet, when lossy links are selected randomly
and their loss process follows the Gilbert model. Comparing
the results of Figures 2 and 3, note that the precision of all
algorithms does not change much. In addition, the recall of
Tomo remains the same because the binary state of a path
remains the same under these two loss processes. However,
we observe a small decrease (by about 2%) in the recall of
Sum-Tomo and Norm, compared to Figure 2b. With the
Gilbert process, the measured loss rate of a path can be
signiﬁcantly diﬀerent from the actual loss rate of its con-
stituent links, and so the value of α should be higher. This
means that each link that is detected by Sum-Tomo as lossy
can justify more lossy paths, and so Sum-Tomo may fail to
identify some lossy links. Nonetheless, the recall of Sum-
392e
c
n
e
r
e
f
f
i
d
e
g
a
r
e
v
A
 0.1
 0.05
 0
-0.05
-0.1
Recall
Precision
Accuracy
t
l
u
s
e
r
e
g
a
r
e
v
A
 1
 0.96
 0.92
 0.88
 0.84
 0.8
Recall
Precision
Accuracy
 2
 4
 6
 8  10  12  14  16  18  20
Number of lossy links
 0.2
 0.25
 0.3
 0.35
 0.4
 0.45
 0.5
α
Figure 5: The diﬀerence in the results of Sum-Tomo with
Method-1 versus Method-2 for the selection of α.
Figure 6: The sensitivity of various parameters to the value
of α.
Tomo is still considerably higher than that of its Boolean
counterpart (by up to 10%), while it remains close to the re-
call of Norm. Because the Gilbert process introduces more
error in the path measurements, the accuracy of both Sum-
Tomo and Norm algorithms declines (Figure 2c). However,
because the Analog approach is more sensitive to measure-
ment errors, we see a larger accuracy decrease in Norm (by
about 25%).
Similarly, Figure 4 shows results obtained with the Plan-
etLab topology. Here, the loss process follows the Gilbert
model while the selection of lossy links has the previously
mentioned edge-network bias (α is the same as before). Be-
cause the PlanetLab topology is much larger, we consider up
to 60 lossy links (about 5% of the total number of links in
that network). Figure 4a shows that the precision of Tomo
and Sum-Tomo are close. However, as shown in Figure 4b,
the recall of Sum-Tomo is higher than that of Tomo. This
is again because Tomo selects fewer links to justify lossy
paths (e.g. about 10% fewer links than Sum-Tomo). On the
other hand, the precision of Norm is quite low, as it does
not capture that paths sharing the same lossy link may mea-
sure diﬀerent loss rates. So, it reports about 40% more lossy
links than Sum-Tomo. The accuracy of the loss rate ranges
assigned to lossy links by Sum-Tomo is always higher than
94%.
The width of the loss rate ranges assigned to bad links
depends on the parameter α. When we use the Bernoulli
loss model (i.e. α ≈ 0.3), the width of the resulting loss
rate range is about 50% of the center value. For example,
when the center of the loss rate range is 0.04, the reported
range is from 0.03 to 0.05. With the Gilbert loss model (i.e.
α ≈ 0.5), the width of the loss rate range is about 80% of
the center value.
In comparison to a random selection of lossy links, the
edge-network bias results in a small decrease (between 2-
3%) in precision and recall for Tomo and Sum-Tomo. This is
because fewer paths traverse edge links than core links, and
so in some cases these two greedy tomography algorithms fail
to select the right lossy link because they prefer to select the
most shared link (which is often in the network core).
We now focus on the selection of α, comparing Method-
1 with Method-2. Here, we use the ESNet topology, while
lossy links are selected randomly and the link loss rates fol-
low the Gilbert process. Figure 5 shows the diﬀerences in
precision, recall and accuracy between Method-1 and Method-
2. Method-1 provides better estimates for α, as it relies on
prior measurements in which the ground truth is known.
The diﬀerence between the two methods is negligible when
the number of lossy links is not high (e.g., less than 10%
of all network links). With more lossy links, the recall ob-
tained with Method-2 is worse (by up to 6%), while the
accuracy improves (by at most 5%). The precision remains
roughly the same. The reason is that, when there are many
lossy links, Method-2 over-estimates the value of α since
each bad path is more likely to traverse more than one lossy
link. Then, Sum-Tomo becomes more similar to the Boolean
Tomo algorithm, which yields lower recall, as noted earlier.
On the other hand, Method-2 gives higher accuracy because
the performance ranges become wider as a result of a larger
α.
Finally, we investigate the sensitivity of the precision, re-
call and accuracy metrics to the value of α. Here, we use
the ESNet topology, while losses follow the Bernoulli pro-
cess. We ﬁx the number of lossy links to 10 (i.e. about 10%
of the network’s links are lossy). The value of α resulting
from Method-1 and Method-2 is around 0.3 and 0.4, respec-
tively. We vary α around these two values, from 0.2 to 0.5.
As Figure 6 shows, the accuracy is always improved with
increasing α because the estimated loss rate range becomes
wider. The precision has an increasing trend with α because
each link that is detected as lossy can potentially justify
more bad paths, and so Sum-Tomo detects fewer links as
lossy. For the same reason, the recall drops signiﬁcantly as
we increase α above the value recommended by Method-1 or
Method-2. In fact, when α is suﬃciently high the Sum-Tomo
algorithm behaves similar with the Boolean tomography al-
gorithm Tomo.
In summary, the errors with Sum-Tomo increase as the
number of lossy links increases, their locations are moved
closer to the edge of the network, and their loss rate varies
rapidly over time.
6. EXPERIMENTAL RESULTS
We have conducted extensive experiments on real net-
works to understand the practical issues involved in network
tomography and to examine the behavior of Range Tomog-
raphy algorithms in practice. In fact, the main purpose of
393this section is to show the characteristics of actual congestion
events in Internet paths and to illustrate how the Sum-Tomo
algorithm localizes such events. We ﬁrst present our method
to detect congestion events, and analyze the results of that
detection phase. We then use the Sum-Tomo algorithm to
localize those congestion events.
6.1 Detection of congestion events
We performed our experiments over three real networks:
Internet2, ESNet, and PlanetLab (the same topologies that
were described in Section 5, except that the PlanetLab topol-
ogy here consists of 60 sensors). The measurement data for
Internet2 and ESNet have been provided to us by the cor-
responding network operators, while the measurement data
for PlanetLab were obtained by us running the same mea-
surement tools that are used in Internet2 and ESNet.
The forwarding paths between sensors were measured us-
ing Paris-traceroute [22] every 30 minutes. Because multi-
path forwarding is common in practice (as reported by [26]),
it is necessary to use tools such as Paris-traceroute to avoid
false inference of paths that do not actually exist in the
network. We measured the one-way delay variations and
the loss rate in each path with active measurements (using
OWAMP), sending ten UDP 60-byte packets per second. In
the following results, we analyze a 3-day PlanetLab data set,
a 35-day Internet2 data set, and a 14-day ESNet data set.
Generally speaking, we deﬁne a congestion event as a sig-
niﬁcant increase in the path one-way delays and/or as the
appearance of packet losses. Speciﬁcally, if we observe a
signiﬁcant increase in the path one-way delay for at least k
consecutive packets, relative to the path’s base delay, we de-
tect a congestion event in that path. This approach falsely
detects congestion events in two cases: I) when the forward-
ing path between the two sensors changes (e.g. due to a
routing change or because of an NTP clock adjustment),
or II) when there is a CPU context-switch event in either
sensor, causing a short-term disruption in the measurement
process.
In the former, we observe a level-shift (rising or
falling) in the one-way packet delays. In the latter, we ob-
serve a sudden substantial increase in the one-way packet
delays, followed by a linear decrease towards the base de-
lay. Speciﬁcally, if we observe an increase of at least 80ms
in the one-way delay of two consecutive packets, we detect
the start of a context-switch event; the event lasts until the
delay returns back to its base level9. We preprocess the data
set to remove all detected context-switches and level-shifts
before applying the tomography algorithm.
6.2 Detection results
Table 1 shows the congestion event detection results for
the three data sets. The frequency of level-shifts is quite low
in all three networks (especially, in ESNet), suggesting that
the paths in these networks are stable over several hours.
On the other hand, the frequency of context-switches is rel-
atively high in ESNet and PlanetLab paths, indicating that
sensors are often highly-loaded (Internet2 sensors are much
better in this aspect). The frequency of congestion events
is much higher in Internet paths between PlanetLab hosts,
while this frequency is quite small in ESNet.
Table 1: The results of the detection process on three net-
works. The value in each cell represents the average occur-
rence frequency per day and per path.
Level-shift Context-switch Congestion
Internet2
ESNet
PlanetLab
0.01
0.0002
0.007
0.011
0.48
0.61
0.002
0.0002
0.005
Figure 7a shows the distribution of congestion event dura-
tion in the PlanetLab network. About 95% of the congestion
events last less than 30 seconds. These durations become
even shorter in Internet2 and ESNet (the maximum dura-
tion is 27sec and only 5% of congestion events last more than
20 seconds in those two networks). This result clearly con-
tradicts the assumption made by several tomography meth-
ods that the network conditions (e.g., congestion level at
diﬀerent links) remain constant for long periods of time. It
becomes clear that tomography methods should be able to
produce results based on short-term measurements.
Table 2 summarizes statistics about lossy congestion events
(congestion events with at least one lost packet)10. The
frequency of lossy congestion events is negligible in Inter-
net2 and ESNet, but it is large in PlanetLab paths. More
than 50% of the lossy congestion events include just one lost
packet, while there are at most 4 packet losses in about 80%
of such events (the loss rate is below 1% in about 80% of
lossy congestion events).
It is important to note that several tomography methods
rely on loss rate to infer and localize congestion events. The
previous results imply that most congestion events do not
introduce any packet losses, and if there are losses the loss
rate is often very low (making it diﬃcult to estimate the loss
rate in practice with a limited frequency of probing packets).
For these reasons, we deﬁne congestion events based on one-
way delay variations in the rest of this section.
Table 2: The number and frequency of lossy congestion
events (over all congestion events) in the three data sets.
lossy
#
events
6
0
535
Frequency of
lossy events
0.2%
0%
16.5%
lost
#
packets
1
0
[1-320]
Internet2
ESNet
PlanetLab
Assume we have detected a congestion event in a path
between the kth and lth probing packets. We use the average
one-way delay increase during that period to quantify the
congestion magnitude M ,
M = Pl
i=k (di − dbase)
where dbase is the base one-way delay (estimated as the me-