not track.
A general solution to this problem would be to ensure
that RSTs are indeed delivered and accepted, i.e., we
want “reliable RSTs.” We can do so, as follows. When-
to
, after normalizing it and sending it on, it synthesizes
, too. This additional
packet takes the form of a TCP “keep-alive,” which is a
dataless3 ACK packet with a sequence number just be-
. The
ever the normalizer sees a RST packet sent from 
a second packet and sends that to
low the point cumulatively acknowledged by 
TCP speciﬁcation requires that  must in turn reply to
the correct sequence number to be accepted by
the keep-alive with an ACK packet of its own, one with
, to en-
sure that the two TCP peers are synchronized. However,
only does this if the connection is still open; if it is
closed, it sends a RST in response to the keep-alive.
Thus, using this approach, there are four possible out-
comes whenever the normalizer forwards a RST packet
(and the accompanying keep-alive):
keep-alive;
, and so  will gen-
(i) The RST was accepted by 
erate another RST back to 
it, in which case  will generate an ACK in re-
(ii) the RST either did not make it to
upon receipt of the
, or 
sponse to the keep-alive;
ignored
it (though this latter shouldn’t happen);
(iii) the keep-alive did not make it to 
(iv) or, the response 
, or 
was lost before the normalizer could see it.
sent in reply to the keep-alive
ignored
The normalizer then uses the following rule for manag-
ing its state upon seeing a RST: upon seeing a RST from
, retain the connection state; but subsequently,
, tear down the state.4
Thus, the normalizer only discards the connection state
has indeed terminated the con-
to 
upon seeing a RST from 
to 
upon seeing proof that
nection. Note that if either 
or  misbehaves, the
scheme still works, because one of the RSTs will still
3In practice, one sends the last acknowledged byte if possible, for
interoperability with older TCP implementations.
4Of course we do not send a keep-alive to make the second RST
reliable or we’d initiate a RST war.


have been legitimate; only if 
collude will the
scheme fail, and, as noted earlier, in that case there is
little a normalizer or a NIDS can do to thwart evasion.
and 
The rule above addresses case (i). For case (ii), the nor-
malizer needn’t do anything special (it still retains the
connection state, in accordance with the rule). For cases
(iii) and (iv), it will likewise retain the state, perhaps
needlessly; but these cases should be rare, and are not
. They could be created by
is malicious; but not to much effect, as in that
is
subject to manipulation by
if 
case the connection is already terminated as far as 
concerned.
6.2 Cold start for TCP
Recall that the “cold start” problem concerns how a nor-
malizer should behave when it sees trafﬁc for a connec-
tion that apparently existed before the normalizer be-
gan its current execution ( 4.1). One particular goal
is that the normalizer (and NIDS) refrain from instanti-
ating state for apparently-active connections unless they
can determine that the connection is indeed active; other-
wise, a ﬂood of bogus trafﬁc for a variety of non-existent
connections would result in the normalizer creating a
great deal of state, resulting in a state-holding attack.
Accordingly, we need some way for the normalizer to
distinguish between genuine, pre-existing connections,
and bogus, non-existent connections, and to do so in a
stateless fashion!
As with the need in the previous section to make RSTs
trustworthy, we can again use the trick of encapsulating
the uncertainty in a probe packet and using the state held
(or not held) at the receiver to inform the normalizer’s
decision process.
to 
and 
Our approach is based on the assumption that the nor-
malizer lies between a trusted network and an untrusted
network, and works as follows. Upon seeing a packet
for which the normalizer does not have
is from the
trusted network, then the normalizer instantiates state
for a corresponding connection and continues as usual.
is from the untrusted network, the nor-
malizer transforms the packet into a “keep-alive” by
stripping off the payload and decrementing the sequence
It then forwards the modiﬁed
number in the header.
If there is indeed a
from 
knowledge of an associated connection, if 
However, if 
packet to 
connection between 
instantiate state for the connection, since 
 will either respond with a RST, or not at all (if
the keep-alive with an ACK, which will sufﬁce to then
is from the
trusted network. If no connection does in fact exist, then
itself
, then  will respond to
and forgets about it.
does not exist, for example). In both of these cases, the
normalizer does not wind up instantiating any state.
The scheme works in part because TCP is reliable: re-
moving the data from a packet does not break the con-
nection, because will work diligently to eventually de-
liver the data and continue the connection.
(Note that a similar scheme can also be applied when
the normalizer sees an initial SYN for a new connection:
by only instantiating state for the connection upon see-
ing a SYN-ACK from the trusted network, the load on a
normalizer in the face of a SYN ﬂooding attack is dimin-
ished to reﬂect the rate at which the target can absorb the
ﬂood, rather than the full incoming ﬂooding rate.)
Even with this approach, though, cold start for TCP still
includes some subtle, thorny issues. One in particular
concerns the window scaling option that can be negoti-
ated in TCP SYN packets when establishing a connec-
tion. It speciﬁes a left-shift to be applied to the 16 bit
window ﬁeld in the TCP header, in order to permit re-
ceiver windows of greater than 64 KB. In general, a
normalizer must be able to tell whether a packet will
be accepted at the receiver. Because receivers can dis-
card packets with data that lies beyond the bounds of
the receiver window, the normalizer needs to know the
window scaling factor in order to mirror this determina-
tion. However, upon cold start, the normalizer cannot
determine the window scaling value, because the TCP
endpoints no longer exchange it, they just use the value
they agreed upon at connection establishment.
We know of no fully reliable way by which the normal-
izer might infer the window scaling factor in this case.
Consequently, if the normalizer wishes to avoid this am-
biguity, it must either ensure that window scaling is sim-
ply not used, i.e., it must remove the window scale option
from all TCP SYN packets to prevent its negotiation (or
it must have access to persistent state so it can recover
the context for each active connection unambiguously).
Doing so is not without a potentially signiﬁcant cost:
window scaling is required for good performance for
connections operating over long-haul, high-speed paths
[1], and sites with such trafﬁc might in particular want
to disable this normalization.
More generally, this aspect of the cold start problem il-
lustrates how normalizations can sometimes come quite
expensively. The next section illustrates how they are
sometimes simply not possible.

6.3 Incompleteness of Normalization
7.1 Evaluation methodology
In the absence of detailed knowledge about the various
applications, normalizations will tend to be restricted to
the internetwork and transport layers. However, even at
the transport level a normalizer cannot remove all possi-
ble ambiguities. For example, the semantics of the TCP
urgent pointer cannot be understood without knowing
the semantics of the application using TCP:
0
1
2
3
4
r o b o t
URGENT
pointer
If the sender sends the text “robot” with the TCP ur-
gent pointer set to point to the letter “b”, then the ap-
plication may receive either “robot” or “root,” de-
pending on the socket options enabled by the receiving
application. Without knowledge of the socket options
enabled, the normalizer cannot correctly normalize such
a packet because either interpretation of it could be valid.
In this case, the problem is likely not signiﬁcant in prac-
tice, because all protocols of which we are aware ei-
ther enable or disable the relevant option for the entire
connection—so the NIDS can use a bifurcating analysis
without the attacker being able to create an exponential
increase in analysis state. However, the example high-
lights that normalizers, while arguably very useful for
reducing the evasion opportunities provided by ambigu-
ities, are not an all-encompassing solution.
7 Implementation
We have implemented norm, a fairly complete, user-
level normalizer for IP, TCP, UDP and ICMP. The code
comprises about 4,800 lines of C and uses libpcap
[10] to capture packets and a raw socket to forward
them. We have currently tested norm under FreeBSD
and Linux, and will release it (and NetDuDE, see below)
publicly in Summer 2001 via www.sourceforge.net.
Naturally, for high performance a production normalizer
would need to run in the kernel rather than at user level,
but our current implementation makes testing, debug-
ging and evaluation much simpler.
Appendix A summarizes the complete list of normaliza-
tions norm performs, and these are discussed in detail in
[4]. Here we describe our process for testing and evalu-
ating norm, and ﬁnd that the performance on commod-
ity PC hardware is adequate for deployment at a site like
ours with a bidirectional 100Mb/s access link to the In-
ternet.
In evaluating a normalizer, we care about completeness,
correctness, and performance. The evaluation presents
a challenging problem because by deﬁnition most of the
functionality of a normalizer applies only to unusual or
“impossible” trafﬁc, and the results of a normalizer in
general are invisible to connection endpoints (depend-
ing on the degree to which the normalizations preserve
end-to-end semantics). We primarily use a trace-driven
approach, in which we present the normalizer with an in-
put trace of packets to process as though it had received
them from a network interface, and inspect an output
trace of the transformed packets it in turn would have
forwarded to the other interface.
Each individual normalization needs to be tested in iso-
lation to ensure that it behaves as we intend. The ﬁrst
problem here is to obtain test trafﬁc that exhibits the be-
havior we wish to normalize; once this is done, we need
to ensure that norm correctly normalizes it.
With some anomalous behavior, we can capture packet
traces of trafﬁc that our NIDS identiﬁes as being am-
biguous. Primarily this is “crud” and not real attack traf-
ﬁc [12]. We can also use tools such as nmap [3] and
fragrouter [2] to generate trafﬁc similar to that an at-
tacker might generate. However, for most of the normal-
izations we identiﬁed, no real trace trafﬁc is available,
and so we must generate our own.
Figure 5: Using NetDuDE to create test trafﬁc
To this end, we developed NetDuDE (Figure 5), the
Network Dump Displayer and Editor. NetDuDE takes
libpcap packet traceﬁle, displays the packets graphi-
cally, and allows us to examine IP, TCP, UDP, and ICMP
header ﬁelds.5 In addition, it allows us to edit the trace-
ﬁle, setting the values of ﬁelds, adding and removing
options, recalculating checksums, changing the packet
ordering, and duplicating, fragmenting, reassembling or
deleting packets.
To test a particular normalization, we edit an existing
trace to create the appropriate anomalies. We then feed
the traceﬁle through norm to create a new normalized
trace. We then both reexamine this trace in NetDuDE
to manually check that the normalization we intended
actually occurred, and feed the trace back into norm, to
ensure that on a second pass it does not modify the trace
further. Finally, we store the input and output traceﬁles
in our library of anomalous traces so that we can perform
automated validation tests whenever we make a change
to norm, to ensure that changing one normalization does
not adversely affect any others.
7.2 Performance
As mentioned above, our current implementation of
norm runs at user level, but we are primarily interested
in assessing how well it might run as a streamlined ker-
nel implementation, since it is reasonable to expect that
a production normalizer will merit a highly optimized
implementation.
To address this, norm incorporates a test mode whereby
it reads an entire libpcap trace ﬁle into memory and
in addition allocates sufﬁcient memory to store all the
resulting normalized packets. It then times how long it
takes to run, reading packets from one pool of memory,
normalizing them, and storing the results in the second
memory pool. After measuring the performance, norm
writes the second memory pool out to a libpcap trace
ﬁle, so we can ensure that the test did in fact measure the
normalizations we intended.
These measurements thus factor out the cost of getting
packets to the normalizer and sending them out once the
normalizer is done with them. For a user-level imple-
mentation, this cost is high, as it involves copying the
entire packet stream up from kernel space to user space
and then back down again; for a kernel implementation,
it should be low (and we give evidence below that it is).
For baseline testing, we use three traceﬁles:
Trace T1: a 100,000 packet trace captured from the
Internet access link at the Lawrence Berkeley Na-
tional Laboratory, containing mostly TCP traf-
ﬁc (88%) with some UDP (10%), ICMP (1.5%),
5At the time of writing, ICMP support is still incomplete.
and miscellaneous (IGMP, ESP, tunneled IP, PIM;
0.2%). The mean packet size is 491 bytes.
Trace U1: a trace derived from T1, where each TCP
header has been replaced with a UDP header. The
IP parts of the packets are unchanged from T1.
Trace U2: a 100,000 packet trace consisting entirely of
92 byte UDP packets, generated using netcat.
T1 gives us results for a realistic mix of trafﬁc; there’s
nothing particularly unusual about this trace compared
to the other captured network traces we’ve tested. U1
is totally unrealistic, but as UDP normalization is com-
pletely stateless with very few checks, it gives us a base-
line number for how expensive the more streamlined
IP normalization is, as opposed to TCP normalization,
which includes many more checks and involves main-
taining a control block for each ﬂow. Trace U2 is for
comparison with U1, allowing us to test what fraction of
the processing cost is per-packet as opposed to per-byte.
We performed all of our measurements on an x86 PC
running FreeBSD 4.2, with a 1.1GHz AMD Athlon
Thunderbird processor and 133MHz SDRAM. In a bare-
bones conﬁguration suitable for a normalizer box, such
a machine costs under US$1,000.
For an initial baseline comparison, we examine how fast
norm can take packets from one memory pool and copy
them to the other, without examining the packets at all:
Memory-to-memory copy only
Trace
T1,U1
U2
pkts/sec
727,270
1,015,600
bit rate
2856 Mb/s
747 Mb/s
Enabling all the checks that norm can perform for both
inbound and outbound trafﬁc6 examines the cost of per-
forming the tests for the checks, even though most of
them entail no actual packet transformation, since (as in
normal operation) most ﬁelds do not require normaliza-
tion:
All checks enabled
Trace
T1
U1
U2
pkts/sec
101,000
378,000
626,400
bit rate
397 Mb/s
1484 Mb/s
461 Mb/s
Number of Normalizations
Trace
T1
IP TCP UDP
0
757
111,551
ICMP
0
Total
112,308
6Normally fewer checks would be enabled for outbound trafﬁc.
Comparing against the baseline tests, we see that IP
normalization is about half the speed of simply copy-
ing the packets. The large number of IP normalizations
consist mostly of simple actions such as TTL restora-
tion, and clearing the DF and Diffserv ﬁelds. We also
see that TCP normalization, despite holding state, is not
vastly more expensive, such that TCP/IP normalization