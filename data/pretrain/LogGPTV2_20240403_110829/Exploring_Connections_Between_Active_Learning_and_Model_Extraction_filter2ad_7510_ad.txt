such that their labels on xn are +1. Note that r > ⌊ o
2⌋ because
the majority label was +1. Deﬁne j = r−⌊ o
2⌋+ 1. Note that if
j trees in RF +1
(xn) will “ﬂip” their decision to −1 on xn, then
the decision on xn will be ﬂipped to −1. This is the intuition
we use to compute RF′n. There are(cid:0)r
j(cid:1) choices of trees and
we pick the one with minimum error on Sn−1, and that gives
us RF′n. Recall that (cid:0)r
j(cid:1) is approximately r j, but we can be
approximate by randomly picking j trees out of RF +1
(xn),
and choosing the random draw with the minimum error to
approximate RF′n.
n
n
n
5 Defense Strategies
Our main observation is that model extraction in the context
of MLaaS systems described at the beginning of § 3 (i.e.,
oracle access) is equivalent to QS active learning. Therefore,
any advancement in the area of QS active learning directly
translates to a new threat for MLaaS systems. In this section,
we discuss strategies that could be used to make the process
of extraction more difﬁcult.We investigate the link between
ML in the noisy setting and model extraction. The design of
a good defense strategy is an open problem; we believe this
is an interesting direction for future work where the ML and
security communities can fruitfully collaborate.
In this section, we assume that the MLaaS server S with the
knowledge of f ∗, S( f ∗), has the freedom to modify the pre-
diction before forwarding it to the client. More precisely, we
assume that there exists a (possibly) randomized procedure D
that the server uses to compute the answer ˜y to a query x, and
returns that instead of f ∗(x). We use the notation SD( f ∗) to
indicate that the server S implements D to protect f ∗. Clearly,
the learner that interacts with SD( f ∗) can still try to learn a
function f from the noisy answers from the server. However,
the added noise requires the learner to make more queries, or
could produce a less accurate model than f .
5.1 Classiﬁcation case
We focus on the binary classiﬁcation problem where F is an
hypothesis class of functions of the form f : X → Y and Y
is binary, but our argument can be easily generalized to the
multi-class setting.
First, in the following two remarks we recall two known
results from the literature [27] that establish information the-
oretic bounds for the number of queries required to extract
the model when any defense is implemented. Let ν be the
generalization error of the model f ∗ known by the server SD
and µ be the generalization error of the model f learned by an
adversary interacting with SD( f ∗). Assume that the hypoth-
esis class F has VC dimension equal to d. Recall that the
VC dimension of a hypothesis class F is the largest number
d such that there exists a subset X ⊂ X of size d which can
be shattered by F . A set X = {x1, . . . , xd} ⊂ X is said to be
shattered by F if |{( f (x1), f (x2), . . . , f (xd)) : f ∈ F }| = 2d.
Remark 3 (Passive learning). Assume that the adversary uses
a passive learning algorithm to compute f , such as the Em-
pirical Risk Minimization (ERM) algorithm, where given
a labeled training set {(X1,Y1), . . . (Xn,Yn)}, the ERM algo-
rithm outputs ˆf = arg min f∈F
1[ f (Xi) 6= Yi]. Then, the
adversary can learn ˆf with excess error ε (i.e., µ ≤ ν + ε) with
˜O( ν+ε
ε2 d) examples. For any algorithm, there is a distribution
such that the algorithm needs at least ˜Ω( ν+ε
ε2 d) samples to
achieve an excess error of ε.
1
n ∑n
i=1
Remark 4 (Active learning). Assume that the adversary uses
an active learning algorithm to compute f , such as the
disagreement-based active learning algorithm [27]. Then,
the adversary achieves excess error ε with ˜O( ν2
ε2 dθ) queries
(where θ is the disagreement coefﬁcient [27]). For any active
learning algorithm, there is a distribution such that it takes at
least ˜Ω( ν2
ε2 d) queries to achieve an excess error of ε.
Observe that any defense strategy D used by a server S
to prevent the extraction of a model f ∗ can be seen as a
randomized procedure that outputs ˜y instead of f ∗(x) with a
given probability over the random coins of D. In the discrete
case, we represent this with the notation
ρD( f ∗, x) = Pr[Yx 6= f ∗(x)],
(3)
where Yx is the random variable that represents the answer
of the server SD( f ∗) to the query x (e.g., ˜y ← Yx). When the
function f ∗ is ﬁxed, we can consider the supremum of the
function ρD( f ∗, x), which represents the upper bound for the
probability that an answer from SD( f ∗) is wrong:
ρD( f ∗) = sup
x∈X
ρD( f ∗, x).
Before discussing potential defense approaches, we ﬁrst
present a general negative result. The following proposition
states that that any candidate defense D that correctly responds
to a query with probability greater than or equal to 1
2 + c for
some constant c > 0 for all instances can be easily broken. In-
deed, an adversary that repetitively queries the same instance
x can ﬁgure out the correct label f ∗(x) by simply looking
at the most frequent label that is returned from SD( f ∗). We
prove that with this extraction strategy, the number of queries
required increases by only a logarithmic multiplicative factor.
Proposition 1. Let F be an hypothesis class used for clas-
siﬁcation and (L, O) be an active learning system for F
USENIX Association
29th USENIX Security Symposium    1317
in the QS scenario with query complexity q(ε, δ). For any
D, randomized procedure for returning labels, such that
there exists f ∗ ∈ F with ρD( f ∗)  0
2 − c for all x) and the effectiveness of
such that ρD( f ∗, x) ≤ 1
the adversary ˜A is not guaranteed anymore6.
Example 6 (Halfspace extraction under noise). For the case
of binary classiﬁcation via halfspaces, Alabdulmohsin et
al. [2] design a system that follows this strategy. They con-
sider the class Fd,HS and design a learning rule that uses
training data to infer a distribution of models, as opposed to
learning a single model. To elaborate, the algorithm learns
the mean µ and the covariance Σ for a multivariate Gaussian
distribution N (µ, Σ) on Fd,HS such that any model drawn
from N (µ, Σ) provides an accurate prediction. The problem
of learning such a distribution of classiﬁers is formulated as
a convex-optimization problem, which can be solved quite
efﬁciently using existing solvers. During prediction, when the
label for a instance x is queried, a new w is drawn at random
from the learned distribution N (µ, Σ) and the label is com-
puted as y = sign(hw, xi). The authors show that this random-
ization method can mitigate the risk of reverse engineering
without incurring any notable loss in predictive accuracy. In
particular, they use PAC active learning algorithms [9, 17]
(assuming that the underlying distribution D is Gaussian) to
learn an approximation ˆw from queries answered in three dif-
ferent ways: (a) with their strategy, i.e. using a new model for
each query, (b) using a ﬁxed model to compute all labels, and
(c) using a ﬁxed model and adding independent noise to each
label, i.e. y = sign(hw, xi + η) and η ← [−1, +1]. They show
that the geometric error of ˆw with respect to the true model is
higher in the former setting (i.e. in (a)) than in the others. On
15 different datasets, their strategy gives typically an order of
magnitude larger error. We empirically evaluate this defense
in the context of model extraction using QS active learning
algorithms in § 6.
Continuous case: Generalizing Proposition 1 to the continu-
ous case does not seem straightforward, i.e. when the target
model held by the MLaaS server is a real-valued function
f ∗ : X → R; a detailed discussion about the continuous case
appears in the appendix in [1].
6 Implementation and Evaluation
For all experiments described below, we use an Ubuntu 16.04
server with 32 GB RAM, and an Intel i5-6600 CPU clocking
3.30GHz. We use a combination of datasets obtained from
the scikit-learn library and the UCI machine learning
repository7, as used by Tramèr et al..
6Intuitively, in the binary case if ρD( f ∗, xi) ≥ 1
2 then the deﬁnition of yi
performed by ˜A in step 2 (majority vote) is likely to be wrong. However,
notice that this is not always the case in the multiclass setting: For example,
consider the case when the answer to query xi is deﬁned to be wrong with
probability ≥ 1
2 and, when wrong, is sampled uniformly at random among
the k− 1 classes that are different to the true class f ∗(x), then if k is large
enough, yi deﬁned via the majority vote is likely to be still correct.
7https://archive.ics.uci.edu/ml/datasets.html
1318    29th USENIX Security Symposium
USENIX Association
USENIX Association
29th USENIX Security Symposium    1319
1320    29th USENIX Security Symposium
USENIX Association
USENIX Association
29th USENIX Security Symposium    1321
Dataset
Adaptive Retraining
Accuracy
Queries
EAT
Queries Accuracy
Mushroom
Breast Cancer
Adult
Diabetes
11301
1101
10901
901
98.5
99.3
96.98
98.5
1001
119