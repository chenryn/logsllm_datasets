isenoughtosetapredicatelockonthereadheaptuplesandonthescannedleaf
pagesoftheindex. Itwill“lock”thewholerangethathasbeenread,notonlythe
exactvalues.
=> BEGIN ISOLATION LEVEL SERIALIZABLE;
=> EXPLAIN (analyze, costs off, timing off, summary off)
SELECT * FROM pred WHERE n BETWEEN 1000 AND 1001;
QUERY PLAN
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
Index Scan using pred_n_idx on pred (actual rows=2 loops=1)
Index Cond: ((n >= '1000'::numeric) AND (n  SELECT relation::regclass, locktype, page, tuple
FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34903
ORDER BY 1, 2, 3, 4;
relation | locktype | page | tuple
−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−
pred | tuple | 4 | 96
pred | tuple | 4 | 97
pred_n_idx | page | 28 |
(3 rows)
270
14.5 PredicateLocks
Thenumberofleafpagescorrespondingtothealreadyscannedtuplescanchange:
forexample,anindexpagecanbesplitwhennewrowsgetinsertedintothetable.
However,Postgretakesitintoaccountandlocksnewlyappearedpagestoo:
=> INSERT INTO pred
SELECT 1000+(n/1000.0) FROM generate_series(1,999) n;
=> SELECT relation::regclass, locktype, page, tuple
FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34903
ORDER BY 1, 2, 3, 4;
relation | locktype | page | tuple
−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−
pred | tuple | 4 | 96
pred | tuple | 4 | 97
pred_n_idx | page | 28 |
pred_n_idx | page | 266 |
pred_n_idx | page | 267 |
pred_n_idx | page | 268 |
pred_n_idx | page | 269 |
(7 rows)
Eachreadtupleislockedseparately,andtheremaybequiteafewofsuchtuples.
Predicatelocksusetheirownpoolallocatedattheserverstart. Thetotalnumber
ofpredicatelocksislimitedbythemax_pred_locks_per_transactionvaluemultiplied 64
by max_connections (despite the parameter names, predicate locks are not being 100
countedperseparatetransactions).
Herewegetthesameproblemaswithrow-levellocks,butitissolvedinadifferent
way: lockescalationisapplied.1
Assoonasthenumberoftuplelocksrelatedtoonepageexceedsthevalueofthe v.
max_pred_locks_per_pageparameter,theyarereplacedbyasinglepage-levellock. 2
=> EXPLAIN (analyze, costs off, timing off, summary off)
SELECT * FROM pred WHERE n BETWEEN 1000 AND 1002;
QUERY PLAN
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−
Index Scan using pred_n_idx on pred (actual rows=3 loops=1)
Index Cond: ((n >= '1000'::numeric) AND (n  SELECT relation::regclass, locktype, page, tuple
FROM pg_locks WHERE mode = 'SIReadLock' AND pid = 34903
ORDER BY 1, 2, 3, 4;
relation | locktype | page | tuple
−−−−−−−−−−−−+−−−−−−−−−−+−−−−−−+−−−−−−−
pred | page | 4 |
pred_n_idx | page | 28 |
pred_n_idx | page | 266 |
pred_n_idx | page | 267 |
pred_n_idx | page | 268 |
pred_n_idx | page | 269 |
(6 rows)
=> ROLLBACK;
v. Escalation of page-level locks follows the same principle. If the number of such
−2 locksforaparticularrelationexceedsthemax_pred_locks_per_relationvalue,they
getreplacedbyasinglerelation-levellock. (Ifthisparameterissettoanegative
64 value,thethresholdiscalculatedasmax_pred_locks_per_transactiondividedbythe
absolutevalueofmax_pred_locks_per_relation;thus,thedefaultthresholdis).
Lockescalationissuretoleadtomultiplefalse-positiveserializationerrors,which
negativelyaffectssystemthroughput. Soyouhavetofindanappropriatebalance
betweenperformanceandspendingtheavailableonlocks.
Predicatelockssupportthefollowingindextypes:
• -trees
v. • hashindexes,i,and
Ifanindexscanisperformed,buttheindexdoesnotsupportpredicatelocks,the
wholeindexwillbelocked.Itisonlytobeexpectedthatthenumberoftransactions
abortedfornogoodreasonwillalsoincreaseinthiscase.
For more efficient operation at the Serializable level,it makes sense to explicitly
declareread-onlytransactionsassuchusingtheclause.Ifthelockman-
agerseesthataread-onlytransactionwillnotconflictwithothertransactions,1it
1 backend/storage/lmgr/predicate.c,SxactIsROSafemacro
272
14.5 PredicateLocks
can release the already set predicate locks and refrain from acquiring new ones.
And if such a transaction is also declared , the read-only transaction p.
anomalywillbeavoidedtoo.
273
15
Locks on Memory Structures
15.1 Spinlocks
To protect data structures in shared memory, Postgre uses several types of
lighterandlessexpensivelocksratherthanregularheavyweightones.
Thesimplestlocksare spinlocks.Theyareusuallyacquiredforaveryshorttime
interval(nolongerthanseveralcycles)toprotectparticularmemorycellsfrom
concurrentupdates.
Spinlocksarebasedonatomicinstructions,suchascompare-and-swap.1 They
onlysupporttheexclusivelockingmode.Iftherequiredresourceisalreadylocked,
theprocessbusy-waits,repeatingthecommand(it“spins”intheloop,hencethe
name).Ifthelockcannotbeacquiredwithinthespecifiedtimeinterval,theprocess
pausesforawhileandthenstartsanotherloop.
Thisstrategymakessenseiftheprobabilityofaconflictisestimatedasverylow,
so after an unsuccessful attempt the lock is likely to be acquired within several
instructions.
Spinlockshaveneitherdeadlockdetectionnorinstrumentation.Fromthepractical
standpoint,weshouldsimplyknowabouttheirexistence;thewholeresponsibility
fortheircorrectimplementationlieswithPostgredevelopers.
1 backend/storage/lmgr/s_lock.c
274
15.2 LightweightLocks
15.2 Lightweight Locks
Next, there are so-called lightweight locks, or lwlocks.1 Acquired for the time
neededtoprocessadatastructure(forexample,ahashtableoralistofpointers),
lightweightlocksaretypicallyshort;however,theycantakelongerwhenusedto
protect/operations.
Lightweightlockssupporttwomodes:exclusive(fordatamodification)andshared
(forread-onlyoperations).Thereisnoqueueassuch: ifseveralprocessesarewait-
ingonalock,oneofthemwillgetaccesstotheresourceinamoreorlessrandom
fashion. In high-load systems with multiple concurrent processes,it can lead to
someunpleasanteffects.
Deadlock checks are not provided; we have to trust Postgre developers that
lightweightlocksareimplementedcorrectly.However,theselocksdohaveinstru-
mentation,so,unlikespinlocks,theycanbeobserved.
15.3 Examples
To get some idea of how and where spinlocks and lightweight locks can be used,
let’s take a look at two shared memory structures: buffer cache and  buffers.
Iwillnameonlysomeofthelocks;thefullpictureistoocomplexandislikelyto
interestonlyPostgrecoredevelopers.
Buffer Cache
Toaccessahashtableusedtolocateaparticularbufferinthecache,theprocess p.
must acquire a BufferMapping lightweight lock either in the shared mode for
readingorintheexclusivemodeifanymodificationsareexpected.
1 backend/storage/lmgr/lwlock.c
275
Chapter15 LocksonMemoryStructures
bufferstrategy
BufferMapping×128
freebuffers
clockhand
buffer
pin
hashtable
bufferheader
BufferIO
BufferContent
Thehashtableisaccessedveryfrequently,sothislockoftenbecomesabottleneck.
Tomaximizegranularity,itisstructuredasatrancheofindividuallightweight
locks,eachprotectingaseparatepartofthehashtable.1
Ahashtablelockwasconvertedintoatrancheoflocksasearlyas,inPostgre
.;tenyearslater,whenversion.wasreleased,thesizeofthetranchewasincreased
to,butitmaystillbenotenoughformodernmulti-coresystems.
Togetaccesstothebufferheader,theprocessacquiresa bufferheaderspinlock2
(thenameisarbitrary,asspinlockshavenouser-visiblenames).Someoperations,
suchasincrementingtheusagecounter,donotrequireexplicitlocksandcanbe
performedusingatomicinstructions.
Toreadapageinabuffer,theprocessacquiresa BufferContentlockintheheader
ofthisbuffer.3 Itisusuallyheldonlywhiletuplepointersarebeingread;lateron,
p. theprotectionprovidedby bufferpinningwillbeenough. Ifthebuffercontent
hastobemodified,theBufferContentlockmustbeacquiredintheexclusivemode.
1 backend/storage/buffer/bufmgr.c
include/storage/buf_internals.h,BufMappingPartitionLockfunction
2 backend/storage/buffer/bufmgr.c,LockBufHdrfunction
3 include/storage/buf_internals.h
276
15.3 Examples
When a buffer is read from disk (or written to disk), Postgre also acquires a
BufferIOlockinthebufferheader;itisvirtuallyanattributeusedasalockrather
thananactuallock.1 Itsignalsotherprocessesrequestingaccesstothispagethat
theyhavetowaituntilthe/operationiscomplete.
Thepointertofreebuffersandtheclockhandoftheevictionmechanismarepro-
tectedbyasinglecommon bufferstrategyspinlock.2
WALBuffers
WALBufMapping
WALWrite
hashtable
WALInsert×8
insertposition
PrevBytePos
CurBytePos
Wcachealsousesahashtabletomappagestobuffers. Unlikethebuffercache
hashtable,itisprotectedbyasingle WALBufMappinglightweightlockbecause
cacheissmaller(itusuallytakes 1 ofthebuffercachesize)andbufferaccess
32
ismoreordered.3
Writingofpagestodiskisprotectedbya WALWritelightweightlock,which
ensuresthatthisoperationisperformedbyoneprocessatatime.
Tocreateaentry,theprocessfirstreservessomespacewithinthepage
andthenfillsitwithdata. Spacereservationisstrictlyordered;theprocessmust
acquire an insert position spinlock that protects the insertion pointer.4 But
1 backend/storage/buffer/bufmgr.c,StartBufferIOfunction
2 backend/storage/buffer/freelist.c
3 backend/access/transam/xlog.c,AdvanceXLInsertBufferfunction
4 backend/access/transam/xlog.c,ReserveXLogInsertLocationfunction
277
Chapter15 LocksonMemoryStructures
oncethespaceisreserved,itcanbefilledbyseveralconcurrentprocesses.Forthis
purpose,eachprocessmustacquireanyoftheeightlightweightlocksconstituting
the WALInserttranche.1
15.4 Monitoring Waits
Withoutdoubt,locksareindispensableforcorrectPostgreoperation,butthey
canleadtoundesirablewaits. Itisusefultotracksuchwaitstounderstandtheir
origin.
off Theeasiestwaytogetanoverviewoflong-termlocksistoturnthelog_lock_waits
parameteron;itenablesextensiveloggingofallthelocksthatcauseatransaction
1s to wait for more than deadlock_timeout. This data is displayed when a deadlock
p. checkcompletes,hencetheparametername.
v.. However, the pg_stat_activity view provides much more useful and complete in-
formation. Whenever a process—either a system process or a backend—cannot
proceedwithitstaskbecauseitiswaitingforsomething,thiswaitisreflectedin
the wait_event_type and wait_event fields, which show the type and name of the
wait,respectively.
Allwaitscanbeclassifiedasfollows.2
Waitsonvariouslocksconstitutequitealargegroup:
Lock —heavyweightlocks
LWLock —lightweightlocks
BufferPin —pinnedbuffers
Butprocessescanbewaitingforothereventstoo:
IO —input/output,whenitisrequiredtoreadorwritesomedata
1 backend/access/transam/xlog.c,WALInsertLockAcquirefunction
2 postgresql.org/docs/14/monitoring-stats#WAIT-EVENT-TABLE.html
278
15.4 MonitoringWaits
Client —datasentbytheclient(psqlspendsinthisstatemostofthetime)
IPC —datasentbyanotherprocess
Extension —aspecificeventregisteredbyanextension
Sometimesaprocesssimplydoesnotperformanyusefulwork.Suchwaitsareusu-
ally“normal,”meaningthattheydonotindicateanyissues.Thisgroupcomprises
thefollowingwaits:
Activity —backgroundprocessesintheirmaincycle
Timeout —timer
Locksofeachwaittypearefurtherclassifiedbywaitnames.Forexample,waitson
lightweightlocksgetthenameofthelockorthecorrespondingtranche.1
You should bear in mind that the pg_stat_activity view displays only those waits
thatarehandledinthesourcecodeinanappropriateway.2 Unlessthenameofthe
waitappearsinthisview,theprocessisnotinthestateofwaitofanyknowntype.
Suchtimeshouldbeconsideredunaccountedfor;itdoesnotnecessarilymeanthat
theprocessisnotwaitingonanything—wesimplydonotknowwhatishappening
atthemoment.
=> SELECT backend_type, wait_event_type AS event_type, wait_event
FROM pg_stat_activity;
backend_type | event_type | wait_event
−−−−−−−−−−−−−−−−−−−−−−−−−−−−−−+−−−−−−−−−−−−+−−−−−−−−−−−−−−−−−−−−−
logical replication launcher | Activity | LogicalLauncherMain
autovacuum launcher | Activity | AutoVacuumMain
client backend | |
background writer | Activity | BgWriterMain
checkpointer | Activity | CheckpointerMain
walwriter | Activity | WalWriterMain
(6 rows)
Hereallthebackgroundprocesseswereidlewhentheviewwassampled,whilethe
clientbackendwasbusyexecutingthequeryandwasnotwaitingonanything.
1 postgresql.org/docs/14/monitoring-stats#WAIT-EVENT-LWLOCK-TABLE.html
2 include/utils/wait_event.h
279
Chapter15 LocksonMemoryStructures
15.5 Sampling
Unfortunately, the pg_stat_activity view shows only the current information on
waits;statisticsarenotaccumulated. Theonlywaytocollectwaitdataovertime
istosampletheviewatregularintervals.
Wehaveto takeintoaccountthe stochasticnature of sampling. The shorter the
waitascomparedtothesamplinginterval,thelowerthechancetodetectthiswait.
Thus,longersamplingintervalsrequiremoresamplestoreflecttheactualstateof
things (but as you increase the sampling rate, the overhead also rises). For the
samereason,samplingisvirtuallyuselessforanalyzingshort-livedsessions.
Postgreprovidesnobuilt-intoolsforsampling;however,wecanstilltryitout
usingthepg_wait_sampling1 extension. Todoso,wehavetospecifyitslibraryin
theshared_preload_librariesparameterandrestarttheserver:
=> ALTER SYSTEM SET shared_preload_libraries = 'pg_wait_sampling';
postgres$ pg_ctl restart -l /home/postgres/logfile
Nowlet’sinstalltheextensionintothedatabase:
=> CREATE EXTENSION pg_wait_sampling;
This extension can display the history of waits, which is saved in its ring buffer.
However,itismuchmoreinterestingtogetthewaitingprofile—theaccumulated
statisticsforthewholedurationofthesession.
Forexample,let’stakealookatthewaitsduringbenchmarking. Wehavetostart
thepgbenchutilityanddetermineitsprocesswhileitisrunning:
postgres$ /usr/local/pgsql/bin/pgbench -T 60 internals
=> SELECT pid FROM pg_stat_activity
WHERE application_name = 'pgbench';
pid
−−−−−−−
36520
(1 row)
Oncethetestiscomplete,thewaitsprofilewilllookasfollows:
1 github.com/postgrespro/pg_wait_sampling
280
15.5 Sampling
=> SELECT pid, event_type, event, count
FROM pg_wait_sampling_profile WHERE pid = 36520