           0.00    0.00   12.39    5.75    0.00   81.86  
dfa               0.00     0.00    0.00 7401.00     0.00 177624.00    24.00     0.24    0.03   0.03  24.10  
```  
使用zpool iostat看到fsync调用使用了zil设备.  
```  
# zpool iostat -v 1  
                                             capacity     operations    bandwidth  
pool                                      alloc   free   read  write   read  write  
----------------------------------------  -----  -----  -----  -----  -----  -----  
zp1                                        160G  43.3T      0  7.23K      0  86.7M  
  scsi-36c81f660eb17fb001b2c5fec6553ff5e  13.4G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c5ff465cff3ed  13.4G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c5ffa662f3df2  13.3G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c5fff66848a6c  13.4G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c600466cb5810  13.3G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c60096714bcf2  13.3G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c600e6761a9bd  13.3G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c601267a63fcc  13.3G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c601867f2c341  13.3G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c601e685414b5  13.4G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c602368a21621  13.3G  3.61T      0      0      0      0  
  scsi-36c81f660eb17fb001b2c602a690a4ed8  13.3G  3.61T      0      0      0      0  
logs                                          -      -      -      -      -      -  
  pcie-shannon-6819246149b014-part2        976M  1.03G      0  7.23K      0  86.7M  
cache                                         -      -      -      -      -      -  
  pcie-shannon-6819246149b014-part1       2.03M   800G      0      0      0      0  
----------------------------------------  -----  -----  -----  -----  -----  -----  
```  
接下来把这个zfs的logbias改成throughput, 也就是不使用zil设备, fsync的性能马上下降了.  
这里实际上VDEV块设备的iops利用率还不到20%, FreeBSD下面没有问题, 这是ZFSonLinux的一个问题, 已提交brian, 得到的回复如下.  
```  
Thanks,  
I've opened a new issue so we can track this.  
https://github.com/zfsonlinux/zfs/issues/2431  
The next step is somebody is going to have to profile the Linux case to   
see what's going on.  It seems like we're blocking somewhere in the   
stack unnecessarily.  Unfortunately, all the developers are swamped so   
I'm not sure when someone will get a chance to look at this.  If your   
interested in getting some additional profiling data I'd suggest   
starting with getting a call graph of fsync() using ftrace.  That should   
show us where the time is going.  
http://lwn.net/Articles/370423/  
Thanks,  
Brian  
```  
```  
# zfs set logbias=throughput zp1/data01  
> pg_test_fsync -f /data01/pgdata/1  
5 seconds per test  
O_DIRECT supported on this platform for open_datasync and open_sync.  
Compare file sync methods using one 8kB write:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                                n/a*  
        fdatasync                         330.846 ops/sec    3023 usecs/op  
        fsync                             329.942 ops/sec    3031 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                                    n/a*  
* This file system and its mount options do not support direct  
I/O, e.g. ext4 in journaled mode.  
Compare file sync methods using two 8kB writes:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                                n/a*  
        fdatasync                         329.407 ops/sec    3036 usecs/op  
        fsync                             329.606 ops/sec    3034 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                                    n/a*  
* This file system and its mount options do not support direct  
I/O, e.g. ext4 in journaled mode.  
Compare open_sync with different write sizes:  
(This is designed to compare the cost of writing 16kB  
in different write open_sync sizes.)  
         1 * 16kB open_sync write                    n/a*  
         2 *  8kB open_sync writes                   n/a*  
         4 *  4kB open_sync writes                   n/a*  
         8 *  2kB open_sync writes                   n/a*  
        16 *  1kB open_sync writes                   n/a*  
Test if fsync on non-write file descriptor is honored:  
(If the times are similar, fsync() can sync data written  
on a different descriptor.)  
        write, fsync, close               324.344 ops/sec    3083 usecs/op  
        write, close, fsync               329.272 ops/sec    3037 usecs/op  
Non-Sync'ed 8kB writes:  
        write                           84914.324 ops/sec      12 usecs/op  
```  
如果直接用SSD建立ZPOOL, 它的fsync性能如何呢? 和前面一个VDEVS使用机械硬盘+ZIL SSD性能基本一致.  
```  
# zpool destroy zp1  
# zpool create -o ashift=12 zp1 pcie-shannon-6819246149b014-part1  
# zfs create -o mountpoint=/data01 zp1/data01  
# mkdir /data01/pgdata  
# chown postgres:postgres /data01/pgdata  
> pg_test_fsync -f /data01/pgdata/1  
5 seconds per test  
O_DIRECT supported on this platform for open_datasync and open_sync.  
Compare file sync methods using one 8kB write:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                                n/a*  
        fdatasync                        6604.779 ops/sec     151 usecs/op  
        fsync                            7086.614 ops/sec     141 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                                    n/a*  
* This file system and its mount options do not support direct  
I/O, e.g. ext4 in journaled mode.  
Compare file sync methods using two 8kB writes:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                                n/a*  
        fdatasync                        5760.927 ops/sec     174 usecs/op  
        fsync                            5677.560 ops/sec     176 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                                    n/a*  
* This file system and its mount options do not support direct  
I/O, e.g. ext4 in journaled mode.  
Compare open_sync with different write sizes:  
(This is designed to compare the cost of writing 16kB  
in different write open_sync sizes.)  
         1 * 16kB open_sync write                    n/a*  
         2 *  8kB open_sync writes                   n/a*  
         4 *  4kB open_sync writes                   n/a*  
         8 *  2kB open_sync writes                   n/a*  
        16 *  1kB open_sync writes                   n/a*  
Test if fsync on non-write file descriptor is honored:  
(If the times are similar, fsync() can sync data written  
on a different descriptor.)  
        write, fsync, close              6561.159 ops/sec     152 usecs/op  
        write, close, fsync              6530.990 ops/sec     153 usecs/op  
Non-Sync'ed 8kB writes:  
        write                           81261.194 ops/sec      12 usecs/op  
```  
如果不使用ZFS, 直接使用EXT4的话, 性能如何呢?  
此时底层块设备的利用率明显提升.  
```  
# mkfs.ext4 /dev/disk/by-id/pcie-shannon-6819246149b014-part2  
# mount /dev/disk/by-id/pcie-shannon-6819246149b014-part2 /mnt  
# chmod 777 /mnt  
ing one 8kB write:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                   38533.583 ops/sec      26 usecs/op  
        fdatasync                       29027.342 ops/sec      34 usecs/op  
        fsync                           26695.490 ops/sec      37 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                       43047.350 ops/sec      23 usecs/op  
Compare file sync methods using two 8kB writes:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                   23826.738 ops/sec      42 usecs/op  
        fdatasync                       31193.925 ops/sec      32 usecs/op  
        fsync                           29445.494 ops/sec      34 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                       22241.529 ops/sec      45 usecs/op  
Compare open_sync with different write sizes:  
(This is designed to compare the cost of writing 16kB  
in different write open_sync sizes.)  
         1 * 16kB open_sync write       34597.675 ops/sec      29 usecs/op  
         2 *  8kB open_sync writes      22051.151 ops/sec      45 usecs/op  
         4 *  4kB open_sync writes      11751.948 ops/sec      85 usecs/op  
         8 *  2kB open_sync writes        804.951 ops/sec    1242 usecs/op  
        16 *  1kB open_sync writes        403.788 ops/sec    2477 usecs/op  
Test if fsync on non-write file descriptor is honored:  
(If the times are similar, fsync() can sync data written  
on a different descriptor.)  
        write, fsync, close             18227.669 ops/sec      55 usecs/op  
        write, close, fsync             18158.735 ops/sec      55 usecs/op  
Non-Sync'ed 8kB writes:  
        write                           288696.375 ops/sec       3 usecs/op  
```  
iostat看到此时的SSD设备利用率提高.  
```  
dfa               0.00     0.00    0.00 55244.00     0.00 441952.00     8.00     1.30    0.02   0.01  78.10  
```  
ZVOL+EXT4的性能   
```  
# zfs create -V 10G zp1/data02  
# mkfs.ext4 /dev/zd0  
# mount /dev/zd0 /tmp  
# chmod 777 /tmp  
```  
结果也不理想  
```  
> pg_test_fsync -f /tmp/1  
5 seconds per test  
O_DIRECT supported on this platform for open_datasync and open_sync.  
Compare file sync methods using one 8kB write:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                    5221.004 ops/sec     192 usecs/op  
        fdatasync                        4770.779 ops/sec     210 usecs/op  
        fsync                            2523.113 ops/sec     396 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                        5527.120 ops/sec     181 usecs/op  
Compare file sync methods using two 8kB writes:  
(in wal_sync_method preference order, except fdatasync  
is Linux's default)  
        open_datasync                    2740.871 ops/sec     365 usecs/op  
        fdatasync                        3774.486 ops/sec     265 usecs/op  
        fsync                            1927.523 ops/sec     519 usecs/op  
        fsync_writethrough                            n/a  
        open_sync                        2747.225 ops/sec     364 usecs/op  
Compare open_sync with different write sizes:  
(This is designed to compare the cost of writing 16kB  
in different write open_sync sizes.)  
         1 * 16kB open_sync write        4751.333 ops/sec     210 usecs/op  
         2 *  8kB open_sync writes       2729.912 ops/sec     366 usecs/op  
         4 *  4kB open_sync writes       1387.512 ops/sec     721 usecs/op  
         8 *  2kB open_sync writes        734.417 ops/sec    1362 usecs/op  
        16 *  1kB open_sync writes        364.665 ops/sec    2742 usecs/op  
Test if fsync on non-write file descriptor is honored:  
(If the times are similar, fsync() can sync data written  
on a different descriptor.)  
        write, fsync, close              3134.067 ops/sec     319 usecs/op  
        write, close, fsync              3486.530 ops/sec     287 usecs/op  
Non-Sync'ed 8kB writes:  
        write                           293944.412 ops/sec       3 usecs/op  
```  
对比以上几种情况, ZFS没有发挥出底层设备的FSYNC能力, 而直接使用块设备+ext4有明显改善, 不知道是不是zfs在Linux下的效率问题, 还是需要调整某些ZFS内核参数? 后面我使用FreeBSD进行一下测试看看是不是有同样的情况.  
FreeBSD的性能很好, 基本达到块设备的瓶颈. 如下 :   