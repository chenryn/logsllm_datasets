Cupid Coverage Speed-up
95% Coverage
+10.00%
-11.73%
+59.89%
+10.50%
+543.47%
+16.96%
+268.99%
+6.38%
+29.17%
+27.68%
+57.57%
+489.64%
+432.28%
97% Coverage
+10.00%
-2.58%
+45.03%
+3.06%
+1135.38%
-20.29%
+169.16%
-3.39%
+29.17%
-33.14%
+45.21%
+489.64%
+351.31%
99% Coverage
+10.00%
-3.62%
+1827.58%
+2.39%
+54.97%
-44.23%
+74.59%
+8.22%
+84.92%
-14.83%
+28.68%
+489.64%
+48.58%
𝑝-value
-
0.26
0.01
< 0.01
< 0.01
0.48
< 0.01
0.08
0.07
0.48
0.11
0.08
< 0.01
+59%
+90%
+74%
+64%
Table 3: Median branch coverage on the trainings binaries
from fuzzer-test-suite (10 runs with a run time of 10h). Bold
values highlight the best result. The last column represents
the 𝑝-value according to the Mann-Whitney U test between
EnFuzz and Cupid. Bold values highlight statistical signifi-
cance (𝑝 < 0.05).
Binary
boringssl
freetype2
harfbuzz
lcms
libjpeg-turbo
llvm-libcxxabi
pcre2
re2
vorbis
wpantund
Total
Improvement (sum)
Improvement (geomean)
EnFuzz
1145
5235
4124
970
1227
3305
4377
2190
932
4186
27691
-
-
Cupid
1145
6055
4272
1385
1386
3432
4189
2205
946
4302
29317
+5.87%
+7.25%
𝑝-value
0.47
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
< 0.01
0.03
0.12
0.02
might not exist one ultimate combination of fuzzers that outper-
forms all other combinations on all future binaries – as there are
fuzzers that work exceptionally well on some specific binaries but
not on others – we are able to find a combination that outperforms
all other combinations on average. In this regard, our top predicted
combination was indeed most often and more consistently ranked
in the first place than all other combinations (i.e., if one were to
use the real-world rankings to extract what should have been the
best-predicted combination, one would end up with our prediction).
9
In conclusion, our predicted ranking is effective in finding well-
performing combinations of fuzzers that realistically represents the
real-world performance seen by the combinations in question.
6.2.2 Evaluating on fuzzer-test-suite. To replicate the evaluation
against EnFuzz for the fuzzer-test-suite binaries, we first predict
the best 𝑛 = 4 combination of fuzzers. As the empirical performance
data was already collected for the previous experiment and no new
fuzzers were introduced, only the prediction had to be updated.
Predicting the full 𝑛 = 4 combination of fuzzers takes less than a
second on a off-the-shelf notebook (single process on an Intel(R)
Core(TM) i7-8550U @ 1.80GHz with 32 GB RAM)—even for a rather
unrealistic extreme case of 𝑛 = 10, 000 it takes less than five minutes
to calculate the top 1,000 predictions. An excerpt of the resulting
prediction ranking of 𝑛 = 4 is displayed in Table 7 in the appendix.
Given the training data, Cupid uses the complementarity metric
and diversity heuristic, as outlined above, to automatically select
the highest ranking, most diverse set of fuzzers: FairFuzz, libFuzzer,
QSYM, AFL. The fuzzers included in the EnFuzz combination are
AFL, libFuzzer, AFLFast, radamsa.
It is important to note that, according to our predicted ranking,
due to the low diversity in the pool of fuzzers, our selected com-
bination and EnFuzz will be very similar performance-wise—the
difference in their predicted performances is less than 1.4%—so in
total, only a small improvement is expected. Nevertheless, when
an expert-guided, hand-picked selection (EnFuzz) is outperformed
by a data-driven, linearly-scaling automatic process (Cupid), that is
already in itself a significant improvement in terms of man-power,
human bias, and processing time.
Experiment As the complementarity metric of Cupid is de-
signed to prefer fuzzers that achieve the same code coverage in a
shorter amount of time, we also calculate the speed-up of Cupid, in
comparison to EnFuzz, to reach 90%, 95%, 97% and 99% of the max-
imum code coverage (i.e., the improvements in the median time to
Table 4: Median number of unique bugs found in the Lava-
M set (10 runs for 10h each). The improvement is measured
in comparison to EnFuzz-Q (which outperformed EnFuzz).
(*) md5sum was excluded from the improvement calculation
as EnFuzz-Q did not run properly for this binary.
Binary
base64
md5sum*
uniq
who
Total bugs
Improvement (sum)
Improvement (geomean)
EnFuzz
EnFuzz-Q
Cupid
42
24
7
95
144
-
-
48
-
22
340
410
-
-
48
25
29
360
437
+6.59%
+11.75%
reach that coverage). Calculating the speed up to compare fuzzers
is also suggested in a recent work, that discusses the scalability of
fuzzing [2].
Results. Cupid outperforms EnFuzz in terms of median code
coverage in 16 out of 23 runs. The geomean improvement in branch
coverage is +2.31% for the test binaries (see Table 2) and +7.25%
for the training binaries (see Table 3). Furthermore, we achieve a
+90% speed-up to reach 95% of the maximum coverage, and +64%
to reach 99% of the maximum coverage. Our results show that the
differences in branch coverage are often less than 100, i. e., the
binaries flatline too fast and do not make much difference in the
total branch coverage after a 10h run. In fact, the median branch
difference (in the cases where EnFuzz has a higher score) is only
eight branches.
When we repeated the experiment with more diversity in the
pool of fuzzers (i. e., by adding Honggfuzz and lafIntel), it re-
sulted in even better and more statistically significant results with
+6.4% for the median branch coverage on the test binaries (see
Table 6 in the appendix).
In conclusion, our automatic, data-driven process was able to
select a better combination than a process consisting of expert guid-
ance and extensive evaluation. Furthermore, no additional human
action or runs are necessary if a different combination size (e. g.
𝑛 = 5) is required—the new prediction would take less than a sec-
ond, as opposed to multiple CPU years in an exhaustive evaluation.
Additionally, the results suggest our combination did not overfit
on the training data and generalizes well to the test binaries.
6.2.3 Evaluating on Lava-M. Although we do not encourage eval-
uating against Lava-M as recent research suggests that it is rather
unlike real-world vulnerabilities, mostly due do to its simplicity[9]
(i.e., bugs are triggered by finding a 4-byte magic value), we include
this experiment to replicate the Lava-M experiment of EnFuzz. To
this end, we compare the median number of bugs found by EnFuzz
to Cupid. As the authors of EnFuzz introduced a new combina-
tion called EnFuzz-Q (AFL, AFLFast, FairFuzz, QSYM) specifically
picked for this evaluation, we additionally compare against this
combination of fuzzers.
Also, as libFuzzer is included in the evaluation, we have to add
libFuzzer support to Lava-M, which requires a LLVMFuzzerTe-
stOneInput function that uses the given data to call the function
10
to be fuzzed in persistent mode. As no other fuzzer needs this
functionality, we only allowed libFuzzer to run in persistent mode.
We had to exclude the binary md5sum from the improvement cal-
culation as one of the fuzzer combinations (EnFuzz-Q) did not run
properly on this binary. The combination of md5sum and EnFuzz-Q
are the only ones affected by this.
Experiment We run each combination 10 times for 10h and
collect all corpus files to later extract all triggered bug IDs. For the
evaluation, we compare the median number of unique bugs each
fuzzer combination found.
Results. As shown in Table 4, the median run of Cupid finds
6.39% more bugs than EnFuzz in the same time (with the geometric
mean of the improvements being +11.75%). Note that the authors
of EnFuzz manually chose this selection of fuzzers (EnFuzz-Q) to
specifically target the Lava-M binaries, while we did not.
Summary. Not only does our evaluation show a strong pos-
6.2.4
itive correlation of Cupid’s approximate ranking of fuzzers with
real-world performances, but the same automatically selected target-
independent combination of fuzzers by Cupid outperforms two
expert-guided, hand-picked target-specific selections (for Google’s
fuzzer-test-suite and Lava-M).
7 RELATED WORK
Previous work on supporting fuzzing at a large scale has focused
on implementing parallel fuzzer synchronisation [18, 29], where
multiple parallel instances of the same fuzzer (e.g., AFL) share a
single corpus and synchronize their efforts.
EnFuzz [5] introduces the idea of ensemble fuzzing, i.e., a set of
fuzzers that synchronize, showing how selecting an ensemble of
diverse fuzzers can increase code coverage. The authors of EnFuzz
hand-pick a number of fuzzer configurations that perform best in
different scenarios (e.g., EnFuzz-Q performs better on LAVA-M,
while their EnFuzz selection performs better on Google’s fuzzer-
test-suite). In this paper, we generalize the intuition provided by
EnFuzz. In contrast to EnFuzz, Cupid presents an automated, data-
driven way of selecting a set of fuzzers to be used, without requiring
a knowledgeable expert to manually select the set of fuzzers best
suited for the task.
Previous work on hybrid fuzzing [25, 28, 30] can be considered
close in nature to ensemble fuzzing, in the sense that these solution
often contain a fast, lightweight base fuzzer (typically AFL) which
delegates hard-to-solve test cases to the heavyweight symbolic
execution engine. In fact, hybrid fuzzing can be expressed as an
instance of ensemble fuzzing.
Xu et al. [27] have identified fork (mainly due to kernel-side
locking mechanisms) and file system operations as bottlenecks in
libFuzzer and AFL-based fuzzers in collaborative fuzzing runs. To
avoid the negative impact of this on the scalability of fuzzers, they
propose new operating primitives, i.e., they replace the fork system
call, add a file system service that is specifically designed for small
file operations, and introduce a new test-case syncing mechanism.
Recent work by Böhme and Falk [2] shows that finding a linear
number of new bugs using parallel instances of the same fuzzer
requires an exponential increase in the number of fuzzing instances.
Whether this assumption holds when scaling up using a diverse set
of fuzzers has not been investigated yet and would be an interesting
area for future research.
8 DISCUSSION
In this paper, we have focused on having fuzzers collaborate to