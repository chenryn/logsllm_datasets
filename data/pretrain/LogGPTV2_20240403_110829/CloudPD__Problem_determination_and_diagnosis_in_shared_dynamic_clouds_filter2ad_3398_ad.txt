o
C
c
i
r
t
e
M
e
s
w
i
Mx1
Mx2
Mx3
Mx4
Mx5
r
i
a
P
0.6
0.5
0.4
0.3
0.2
0.1
0.0
CPU  ctxt
cache miss  page fault
Mx1
Mx2
Mx3
Mx4
Mx5
l
n
o
i
t
a
e
r
r
o
C
M
V
e
s
w
r
i
i
a
P
n
o
i
t
a
l
e
r
r
o
C
c
i
r
t
e
M
e
s
w
i
r
i
a
P
Workload Mix
(a) Hadoop mix
Workload Mix
(b) Olio mix
Figure 10: Stability of metric correlation across workload mixes.
CP U ⇐⇒ ctxt refers to correlation between CPU and context
switches on the same VM; cache miss ⇐⇒ page f ault implies
correlation between cache misses and page faults.
3) Stability of Correlation with Change in Workload Mix:
Changes in the workload mix result in the resource usage
behavior of the application to alter, but correlations across
all the VMs remain stable. The results of this experiment for
Hadoop and Olio are shown in Figure 9, where we observe that
the change in pairwise VM correlations is very low across
different workload transaction mixes. The transaction mixes
used are shown in Table IV. For Olio, bm1− bm5 corresponds
to different operation mixes in terms of browsing transactions.
Furthermore, the correlations across metric pairs on the same
VM, although weaker, also remain stable across changes in
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:49 UTC from IEEE Xplore.  Restrictions apply. 
the workload mixes (see Figure 10). A similar trend follows
for cross-metric correlations of the same VM across workload
intensity changes (plots not shown due to space constraints).
Note that, if all correlations are low, it is highly likely that
many faults may be missed. However, from our analysis,
CloudPD only requires critical mass of correlations to identify
faults. For example, from the ﬁgures (Figure 7 – Figure 9), we
observe correlations are greater than 0.17, and less than 0.52.
Although, the metric-pairs in Figure 10 have low correlation
values, we observed other resource pairs (not shown due to
space brevity) with higher correlations. We found many stable
moderate correlations in the value range of (0.2 − 0.5), and
failures cause their value to be dropped to either zero or
negative. This leads to high precision/accuracy as shown in
Table VI and Table IX. However, few errors like disk-hog,
which impact metrics like shared disk bandwidth are missed
because of low correlation values.
TABLE IV: Workload transaction mixes for Hadoop and Olio;
bm: browsing mix, rw-bd: read-write bidding mix.
Mix-type
Hadoop
Olio
Mx1
Mx2
Mx3
Mx4
Mx5
(streamSort,javaSort)-(s)
(streamSort,combiner)-(l)
(webdataScan,combiner)-(m)
(combiner,monsterQuery)-(l)
(webdataScan,webdataSort)-(s)
bm1+ 80% rw-bd
bm2 + 85% rw-bd
bm3 + 90% rw-bd
bm4 + 95% rw-bd
bm5 + 99% rw-bd
TABLE V: Number of injected faults (synthetic and trace-driven).
Fault Type
Synthetic
Trace-driven
invalid VM migration
invalid VM sizing
application misconﬁg.
CPU-hog
memory-hog
disk-hog
cache-hog
network-hog
3
3
4
4
4
2
2
2
Total number of faults
24
4 (cloud induced)
7 (cloud induced)
0
3
3
2
2
2
23
E. Synthetic Fault Injection Results
We compare the performance of CloudPD and other baselines
in terms of their effectiveness in detecting faults, diagnosis
time, and scalability. In these experiments, we ran each appli-
cation (Hadoop, Olio and RUBiS) independently for 24 hours.
For Hadoop, we continuously ran the Hadoop Sort benchmark
on 5 GB data. We ran Olio with 100 concurrent users and the
default browsing transaction mix (M x1 in Table IV). We used
the default workload mix for RUBiS (read-only browsing mix
and bidding mix with 15% read-write interactions). We divided
each experiment into 96 intervals of 15 minutes each. CloudPD
collects data from the Monitoring Engine and performs a di-
agnosis for each interval using all the competing methods. We
injected faults randomly in 24 of these 96 intervals, the details
of which are presented in Table V. The resource-hog faults
are custom-written pieces of code that continuously access a
particular resource, mimicking a software anomaly. Invalid VM
sizing implies faults, when the resource allocations do not meet
application demands. Invalid VM migration captures failed
migrations due to resource congestion at source/destination
host. Further details on these faults can be found in [27].
An interval is categorized as anomalous (for ground truth),
if the application latency or throughput is deviant by a cer-
tain threshold (obtained through empirical analysis) from the
latency/throughput observed for normal behavior. For Hadoop,
the latency is the end-to-end job completion time; for RUBiS,
latency is the end-to-end transaction response time. The latency
threshold chosen was 11%. For Olio, we used throughput,
deﬁned as the number of transactions per second, as the
application performance metric, and its threshold was set to
9%. Due to space constraints, we only present results for
Hadoop and Olio, but similar results were observed for RUBiS.
1) End-to-end Diagnosis Comparison: Table VI shows the
end-to-end diagnosis results for CloudPD and the other com-
peting methods. For Hadoop, CloudPD was able to correctly
detect and diagnose 21 out of the 24 faults and 69 out of the 72
normal intervals. It compares very favorably with Oracle B2,
which is able to identify only one more additional anomaly
compared to CloudPD with exhaustive analysis. B1 has low
recall and precision as it monitors only a VM’s CPU and
memory, and ignores other system metrics (both at VM and
server level). Further,
it also has a high false alarm rate,
where a normal change in operating context is classiﬁed as
an anomaly. CloudPD is able to avoid these false alarms as it
correlates data across multiple metrics (eg., CPU with context
switches and memory with page faults) and does not report
an anomaly if the correlations are consistent with the learning
models. B3 also recorded a low recall and precision and a high
false alarm rate, as it does not correlate across peers (VMs run-
ning the same application). However, its performance is better
than B1. B4, that uses static thresholds, again does not have
satisfactory performance. Similar results are observed for Olio
and RUBiS, although the performance of CloudPD is slightly
worse compared to Hadoop. We conjecture that this is because
Hadoop is a symmetric batch application (map/reduce tasks
are similar in type and intensity across all VMs with time),
whereas Olio is a more generic distributed application with
greater burstiness. There exists intra- and inter-tier correlations
among VMs for Olio. However, the magnitude of intra-tier VM
correlations is higher than inter-tier VMs correlations, which
results in some error in detecting anomalies. We emphasize
the fact that CloudPD is effective in accurately distinguishing
cloud anomalies from workload changes and application faults
even for shared resources such as cache.
We further analyze the speciﬁc faults that were undetected
by CloudPD and the other four baselines. These are listed
in Tables VII and VIII, for Hadoop and Olio, respectively.
CloudPD failed to identify 1 disk hog and 2 application
misconﬁguration faults. The disk hog eluded detection since
the VMs share their local disks across a Storage Area Network
(SAN). The deviation of disk utilization from normal values
was not sufﬁcient for the Event Generation Engine to raise an
alarm, preventing CloudPD from detecting it. However, B2
was able to detect this as correlations across metrics and VMs
calculated by the Problem Determination Engine showed a
signiﬁcant deviation from normal behavior. CloudPD could not
identify 2 application misconﬁguration faults as well, as the
difference in cross-resource and cross-VM correlations from
normal was not high enough to mark them as anomalies.
B2 missed the same 2 application misconﬁguration faults
as CloudPD. As B1 only monitors CPU and memory,
it
missed a total of 13 faults (it was effective in identifying
only CPU and memory hog faults). B3 failed to detect most
application related faults as it only performs correlations across
resource metrics within a VM and does not perform cross-
VM correlations. A faulty VM considered by itself appears
as though it is servicing a very large workload, and hence,
was not tagged as anomalous. B4 is sensitive to the speciﬁc
thresholds used, and is ineffective in identifying the different
manifestations of the same type of fault.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:43:49 UTC from IEEE Xplore.  Restrictions apply. 
TABLE VI: Comparing end-to-end diagnosis effectiveness for Hadoop, Olio and RUBiS benchmarks.
Method
CloudPD
B1
B2
B3
B4
CloudPD
B1
B2
B3
B4
CloudPD
B1
B2
B3
B4
# of Correct
Normal
Detections
# of Correct
Anomalous
Detections
# of Correct
Event
Generations
# of Total
Predicted
Anomalies
Recall
Precision
Accuracy
FAR
69
62
69
64
66
68
62
68
63
63
68
61
68
62
63
21
11
22
12
15
20
11
22
11
13
20
12
22
12
14
23
15
24
23
15
22
15
24
22
14
22
15
24
22
13
24
21
25
20
21
Hadoop
24
21
26
20
22
Olio
24
23
26
22
23
RUBiS
0.88
0.46
0.92
0.50
0.63
0.83
0.46
0.92
0.46
0.54
0.83
0.50
0.92
0.50
0.59
0.88
0.52
0.88
0.60
0.71
0.83
0.52
0.85
0.55
0.59
0.83
0.52
0.85
0.54
0.61
0.88
0.49
0.90
0.54
0.67
0.83
0.49
0.88
0.50
0.56
0.83
0.51
0.88
0.52
0.60
0.12
0.48
0.12
0.40
0.29
0.17
0.48
0.15
0.45
0.41
0.17
0.48
0.15
0.46
0.39
TABLE VII: Undetected anomalies for Hadoop.
TABLE VIII: Undetected anomalies for Olio.
Method
Undetected anomalies
Method
Undetected anomalies
CloudPD
B1
B2
B3
B4
1 disk hog + 2 application misconﬁg. (total 3)
2 network hog + 2 disk hog + 2 cache hog + 2 application misconﬁg.
+ 2 invalid VM sizing + 3 invalid VM migration (total 13)
2 application misconﬁg. (total 2)
2 disk hog + 2 cache hog + 2 application misconﬁg. + 2 memory hog
+ 1 CPU hog + 3 invalid VM migration (total 12)
2 disk hog + 1 cache hog + 1 memory hog + 2 application misconﬁg.
+ 3 invalid VM migration (total 9)
CloudPD
B1
B2
B3
B4
2 disk hog + 2 application misconﬁg. (total 4)
2 network hog + 2 disk hog + 2 cache hog + 2 application misconﬁg.
+ 2 invalid VM sizing + 3 invalid VM migration (total 13)
2 application misconﬁg. (total 2)
2 disk hog + 2 cache hog + 3 application misconﬁg. + 2 memory hog
+ 1 CPU hog + 3 invalid VM migration (total 13)
2 disk hog + 1 cache hog + 2 memory hog + 3 application misconﬁg.
+ 3 invalid VM migration (total 11)
2) Diagnosis Time and Scalability: Effective diagnosis is
just one of the goals for problem determination in cloud. A
dynamic and autonomic system like cloud requires diagnosis
to be performed quickly for efﬁcient dispatch of remediation
actions. Figure 11 shows the analysis time of each stage
of CloudPD and other base schemes with Hadoop and Olio
benchmarks. The numbers are averaged across the 96 intervals
in the 24-hour experiment. Note that the system state can
change every 15 minutes, and hence, remediation is relevant
only if performed in time much less than 15 minutes. The
Event Generation Engine of CloudPD takes on an average 17.8
seconds for Hadoop and is executed for every interval, and for
every monitored VM and metric. The Problem Determination
Engine is triggered only if an alarm is raised by the Event
Generation Engine. For other intervals, the time taken by
Problem Determination Engine is zero. Hence, although Prob-
lem Determination Engine takes longer than Event Generation
Engine (about 40 seconds for Hadoop), since the latter is
invoked only selectively, the analysis time is lower. The same
is true for Problem Diagnosis Engine as it is invoked only if
an anomaly is detected, allowing CloudPD to quickly detect
faults (time taken only marginally higher than B1 and B3,
which detect very few anomalies). In comparison, Oracle B2
has no Event Generation, and hence, the time spent in Problem
Determination Engine is 10X larger than CloudPD. The time
spent analyzing the Olio cluster is lesser compared to Hadoop
as the cluster size is smaller. Note that the Event Generation
Engine can be parallelized across multiple VMs,
thereby,
reducing the total time spent by CloudPD in this phase.
Figure 12 shows the effect of the increase in the number
of VMs on the analysis time of CloudPD. The analysis time
is shown as histograms with breakup of time taken by each
stage across different number of VMs in the Hadoop and Olio
)
s
d