title:HandLock: Enabling 2-FA for Smart Home Voice Assistants using Inaudible
Acoustic Signal
author:Shaohu Zhang and
Anupam Das
HandLock: Enabling 2-FA for Smart Home Voice Assistants using
Inaudible Acoustic Signal
Shaohu Zhang
PI:EMAIL
North Carolina State University
Raleigh, NC, USA
Anupam Das
PI:EMAIL
North Carolina State University
Raleigh, NC, USA
ABSTRACT
The use of voice-control technology has become mainstream and
is growing worldwide. While voice assistants provide convenience
through automation and control of home appliances, the open na-
ture of the voice channel makes voice assistants difficult to secure.
As a result voice assistants have been shown to be vulnerable to re-
play attacks, impersonation attacks and inaudible voice commands.
Existing defenses do not provide a practical solution as they either
rely on external hardware (e.g., motion sensors) or work under
very constrained settings (e.g., holding the device close to a user’s
mouth). We introduce the concept of using a gesture-based authen-
tication system for smart home voice assistants called HandLock,
which uses built-in microphones and speakers to generate and
sense inaudible acoustic signals to detect the presence of a known
(i.e., authorized) hand gesture. Our proposed approach can act as a
second-factor authentication (2-FA) for performing specific sensitive
operations like confirming online purchases through voice assis-
tants. Through extensive experiments involving 45 participants, we
show that HandLock can achieve on average 96.51% true-positive-
rate (TPR) at the expense of 0.82% false-acceptance-rate (FAR). We
perform a comprehensive analysis of HandLock under various set-
tings to showcase its accuracy, stability, resilience to attacks, and
usability. Our analysis shows that HandLock can not only success-
fully thwart impersonation attacks, but can do so while incurring
very low overheads and is compatible with modern voice assistants.
CCS CONCEPTS
• Security and privacy → Usability in security and privacy.
KEYWORDS
acoustic sensing, hand gesture, two-factor authentication, voice
assistants
ACM Reference Format:
Shaohu Zhang and Anupam Das. 2021. HandLock: Enabling 2-FA for Smart
Home Voice Assistants using Inaudible Acoustic Signal. In 24th International
Symposium on Research in Attacks, Intrusionsand Defenses (RAID ’21), October
6–8, 2021, San Sebastian, Spain. ACM, New York, NY, USA, 15 pages. https:
//doi.org/10.1145/3471621.3471866
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
RAID ’21, October 6–8,2021, San Sebastian, Spain
© 2021 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-9058-3/21/10...$15.00
https://doi.org/10.1145/3471621.3471866
1
1 INTRODUCTION
Voice-based computer interaction thrives on the ability to enable
users to interact with devices and services through voice instead of
keystrokes, mouse-movements or swipes. While speech recognition
has been an active field of research for many years, it has seen wide-
spread adoption in recent years, especially with the deployment of
smart home voice assistants (VAs) like Amazon Echo and Google
Home. These VAs enable consumers to not only listen to music
and flash briefings, but also control other smart home appliances.
However, the widespread use of VAs also gives rise to both security
and privacy concerns due to their always-listening capability [35]
and susceptibility to audio-based attacks [32, 43, 56]. According to
Edison Research, 63% of VA owners in the USA are concerned that
hackers might gain access to their home or personal information
through VAs [3].
One of the major security concerns with current VAs is the lim-
ited support for authentication. Other than simple customizable
wake words like “Alexa” or “Hi, Google,” there is not much sup-
port for authentication in VAs. VAs do provide the capability to
recognize different users based on their voice profiles, however,
such approach has been shown to be vulnerable to simple replay
attacks [31, 53]. Other features include using voice-based PIN codes
to restrict sensitive operations like voice-based online order. Again,
the PIN code has to be spoken out loud and is susceptible to passive
eavesdropping.
In recent years, several studies have proposed authenticating
users through microphones and speakers embedded in smart de-
vices [16, 34, 60]. BreathPrint [16] proposes utilizing the breathing
sound made by a user to uniquely identify the user. BiLock [60]
extracts biometric signatures from the sounds generated by a user’s
dental occlusion, captured through the built-in microphone of a
smartphone. Lippass [34] leverages unique Doppler profiles of
acoustic signals generated by a user’s moving lips to authenticate
the user. However, all of these schemes require the sensing device
(i.e., microphones or speakers) either to be placed very close to
the user’s mouth or held by the user, which does not amount to a
practical solution for smart home VAs.
In this paper, we introduce a hand-gesture based biometric au-
thentication scheme called HandLock that can recognize an autho-
rized user based on his/her hand movement. To this end, HandLock
emits inaudible acoustic signals and records the reflected signals to
identify a user. The underlying hypothesis for HandLock is that it
is possible to distinguish different users even if they perform the same
hand gesture due to their differing physical biometrics. Specifically,
as shown in the Figure 1, since the length of ulna and humerus of a
given user is fixed, the starting and ending positions of the hand
251RAID ’21, October 6–8,2021, San Sebastian, Spain
Shaohu Zhang and Anupam Das
purchases through VAs. To the best of our knowledge, we are the
first to propose such a 2-FA system for VAs without requiring
additional hardware.
• We design and implement HandLock using a commercial off-
the-shelf (COTS) speaker (ReSpeaker Core v2.0 [8]). We also
develop signal-processing techniques that are capable of extract-
ing acoustical phase change caused by hand movements from
raw acoustic signals. We, furthermore, develop machine learn-
ing models that can effectively identify users based on temporal
and spectral features derived from the extracted time-series
data representing gesture speed and acceleration.
• We evaluate our approach by recruiting 45 participants and
by collecting over 15,000 samples that cover five different ges-
tures. Our results show that HandLock can achieve on average
96.51% TPR. With three attempts, HandLock can achieve a TPR
of 99.91%. We also evaluate our system under both benign and
adversarial settings. Lastly, we thoroughly perform various sen-
sitivity analysis to showcase the effectiveness of our proposed
authentication system.
The remainder of this paper proceeds as follows. Section 2 pro-
vides background and describes related work. In Section 3, we
present the detailed design of HandLock. Section 4 presents the
comprehensive evaluation of our proposed authentication system.
We analyze the usability of our approach in Section 5. We list the
limitations of our approach in Section 6. Finally, we conclude in
Section 7.
2 RELATED WORK
Biometric authentication has been an active field of research for a
long time. Voice and facial recognition have been at the forefront of
such authentication systems. In this work, we look at recognizing
hand gestures through acoustic signals for authenticating voice-
assistant users. In this section, we will highlight some of the relevant
works in this field.
Voice-based Authentication. Voice-based authentication sys-
tems leverage unique human voice characteristics to recognize
a user [36]. These voice biometrics include voice based features
such as pronunciation, accent, speech speed, as well as physical
characteristics of vocal tract, mouth and nasal passages. However,
studies show that voice authentications are vulnerable to imper-
sonation [28, 29] and replay attacks [31, 53]. Kinnunen et al. [31]
reported that the EER of voice authentication systems can increase
anywhere from 1.76% to 31.46% under replay attacks. Researchers
have shown that it is easy to launch both black box (i.e., inverse
MFCC) [14, 20, 49] and white box (i.e., gradient descent) [45] at-
tacks against speech recognition systems. Recent works such as
DolphinAttack [56], BackDoor [42], CommanderSong [54], Sire-
nAttack [24] and LipRead [43] have shown that voice assistants are
vulnerable to inaudible voice commands which are incomprehen-
sible to human ear, but can be understood by speech recognition
systems. Even voice processing systems such as Google, Bing, IBM
and Azure speech APIs have been shown to be susceptible to hidden
voice commands [11].
Many schemes have been proposed to defend against replay
attacks that perform liveliness tests [19, 51, 57, 58], but such ap-
proaches are often not feasible for VAs. For instance, VocieLive [58]
Figure 1: Anatomy of the human arm and hand.
stay the same no matter how fast the user moves the hand. There-
fore, the speed profile of a given gesture from the same user should
remain similar as the speeds of different parts of the hand and limb
change, proportionally. We study the validity of this hypothesis
through a comprehensive measurement study. Theoretically, the
phase change that appears in the received acoustic signal is directly
proportional to the speed at which a human hand was moved while
performing a gesture. By combining our hypothesis with this theo-
retical result, we make the following observation: the phase shift
recorded on the received acoustic signal is significantly different
for different users, even if they all perform the same hand gesture.
HandLock, therefore, first emits inaudible acoustic signals during
the authentication phase and simultaneously records audio signal
through a microphone. After the user performs a hand gesture,
HandLock extracts the phase of the received signal as a time-series
data using signal processing techniques. Next, this time-series phase
data is converted into a time series of speed and acceleration. The
last step involves computing statistical features in the temporal and
spectral domain to differentiate users via a machine learning model.
We evaluate our approach by recruiting a total of 45 participants. We
evaluate our approach under various settings, including multi-user
enrollment, long-term stability and adversarial attempts. Through
our evaluations we see that our approach can be used not only to
determine the physical presence of a user, but also as an effective
second-factor authentication method for VAs.
There are several technical challenges in building an acoustic
sensing system to differentiate an individual’s hand gesture. First,
the time series of acoustical phase introduced by the hand is noisy
mixing with ambient noise, DC offset (caused by microphone imper-
fection), and multi-path propagation (impacted by human body and
surrounding environment). To tackle these challenges, we propose
a new Quadrature-based (Q-based) phase extraction approach to
remove the background noise, DC offset and multi-path propaga-
tion. Next, we need to select meaningful features to recognize a
predefined gesture performed by different users. Lastly, as gestures
can be performed at different speeds, we need to account for the
change in speed to make all hand speeds consistent for each user.
In summary, we make the following contributions:
• We introduce the idea of using gesture-based authentication
mechanism for VAs, using built-in microphones and speakers.
Our proposed approach can act as a second-factor authenti-
cation (2-FA) for sensitive operations such as making online
2
252HandLock: Enabling 2-FA for Smart Home Voice Assistants using Inaudible Acoustic Signal
RAID ’21, October 6–8,2021, San Sebastian, Spain
captures the time-difference-of-arrival (TDoA) dynamics of an ut-
tered passphrase to determine liveliness of an audio source. Voice-
pop [51] leverages the breathing noise to detect both replay and
impersonation attacks. However, both of these approaches require
the user to speak very closely to the microphone (in the range of
2–6 cm), which is feasible for authenticating one on a smartphone,
but not realistic for VAs.
Gesture-based Authentication. Body gesture is another biomet-
ric that has been utilized to uniquely identify users. A body of stud-
ies have been conducted to authenticate users on mobile phones
and wearable IoT devices, leveraging embedded or wearable sen-
sors [25, 33] and WiFi signals [32, 37, 40, 46]. VAuth [25] collects
the body-surface vibrations of the user via a wearable motion sen-
sor and correlates the data with the speech signal recorded by the
voice assistant’s microphone to achieve continuous authentication.
VSButton [32] utilizes the motion time-series data extracted from
wristband to secure IoT devices. However, these schemes depend
on external sensing hardware, and thus are not readily applicable
to VAs. Recent studies [37, 40, 46] have shown that existing WiFi
signals can be utilized for authentication in smart homes. WiID [46]
extracts speed time-series features from WiFi Channel State Infor-
mation (CSI) to infer 7 gestures such as circular arm motion, waving
arm motion and kicking to identify users. REVOLT [40] leverages
the WiFi and voice features to detect human presence and speaking
to counter replay attacks. While sensing gestures via wireless sig-
nals has the advantages of being device-free and unobtrusive, there
are two main drawbacks: 1) CSI logging requires special hardware
such as an USRP or an special WiFi card (e.g., Inter 5300 NIC), and
2) accuracy can significantly degrade in environments with moving
objects (e.g., pets moving) or when the position of the transceiver
changes.
Acoustic Sensing. Many acoustic-based gesture recognition sys-
tems have been proposed to recognize in-air gestures [18, 27, 39, 44,
48]. SoundWave [27], AudioGest [44], and MultiWave [39] all char-
acterize the Doppler effect to sense motion gestures. EchoTrack [18]
uses two speakers and one microphone in smartphones to track
hand motion. Strata [55] estimates the channel impulse response
(CIR) induced by acoustical multi-path to track fine-grained finger
gestures. FingerIO [38] uses OFDM modulated sound frames and
enables 2-D finger tracking based on the change of the echo profiles
of two consecutive frames. LLAP [52] uses Continuous Wave (CW)
signal to track a moving target based on the phase information of
the reflected signal. VSkin [48] characterizes the propagation of
structure-borne and air-borne acoustic signals to recognize gestures
performed on the back of mobile devices.
Recently, several acoustic signal based authentication systems
have been proposed for smartphones [16, 34, 60]. BreathPrint [16]
captures the breathing sound made by a user through an embedded
microphone in close proximity to the user’s nose to perform biomet-
ric authentication. BiLock [60] extracts signatures from the sounds
generated by a user’s occlusion activities which are recorded by the
built-in microphone of a smartphone or a smartwatch placed close
to the user’s lips to achieve biometric authentication. Lippass [34]
proposes a lip reading-based user authentication on smartphones
utilizing unique Doppler profiles of acoustic signals introduced
by lip movement while speaking. SpeakPrint [21] extracts MFCC
Table 1: Comparison with existing works.