title:Q-score: proactive service quality assessment in a large IPTV system
author:Han Hee Song and
Zihui Ge and
Ajay Mahimkar and
Jia Wang and
Jennifer Yates and
Yin Zhang and
Andrea Basso and
Min Chen
Q-score: Proactive Service Quality Assessment
in a Large IPTV System
Han Hee Song§, Zihui Ge‡, Ajay Mahimkar‡, Jia Wang‡, Jennifer Yates‡, Yin Zhang§,
The University of Texas at Austin§
Austin, TX, USA
Andrea Basso‡, Min Chen‡
AT&T Labs – Research ‡
Florham Park, NJ, USA
{hhsong,yzhang}@cs.utexas.edu {gezihui,mahimkar,jiawang,jyates,basso,mc4381}@research.att.com
Abstract — In large-scale IPTV systems, it is essential to main-
tain high service quality while providing a wider variety of service
features than typical traditional TV. Thus service quality assess-
ment systems are of paramount importance as they monitor the
user-perceived service quality and alert when issues occurs. For
IPTV systems, however, there is no simple metric to represent user-
perceived service quality and Quality of Experience (QoE). More-
over, there is only limited user feedback, often in the form of noisy
and delayed customer calls. Therefore, we aim to approximate the
QoE through a selected set of performance indicators in a proac-
tive (i.e., detect issues before customers reports to call centers) and
scalable fashion.
In this paper, we present a service quality assessment framework,
Q-score, which accurately learns a small set of performance indica-
tors most relevant to user-perceived service quality, and proactively
infers service quality in a single score. We evaluate Q-score using
network data collected from a commercial IPTV service provider
and show that Q-score is able to predict 60% of the service prob-
lems that are reported by customers with 0.1% false positives.
Through Q-score, we have (i) gained insight into various types of
service problems causing user dissatisfaction, including why users
tend to react promptly to sound issues while late to video issues;
(ii) identiﬁed and quantiﬁed the opportunity to proactively detect
the service quality degradation of individual customers before se-
vere performance impact occurs; and (iii) observed possibility to
allocate customer care workforce to potentially troubling service
areas before issues break out.
Categories and Subject Descriptors
C.4 [Computer-Performance of Systems]: Reliability, available-
ity, and serviceability
General Terms
Management, Reliability
Keywords
IPTV, Service, Quality, QoE, Assessment, Analysis, Video
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’11, November 2–4, 2011, Berlin, Germany.
Copyright 2011 ACM 978-1-4503-1013-0/11/11 ...$10.00.
1.
INTRODUCTION
IPTV technologies are transforming the global television indus-
try, enabling new operators to provide TV services whilst also en-
abling a wealth of innovative new IP-based services integrated with
more traditional TV. However, there is a pressing need to ensure
that the IPTV services being deployed deliver a Quality of Experi-
ence (QoE) that is equal to or better than traditional TV services.
The traditional approach to assessing quality of experience (QoE)
has been through subjective evaluation in controlled laboratory en-
vironments. Unfortunately, subjective evaluation is expensive, error-
prone and unreliable. A complementary approach is through user
feedback. It is, in general, collected by the call centers and it pro-
vides a direct measure of the service performance problems experi-
enced by the users. However, user feedback is not always complete
(not all users report service quality issues) and delayed (users have
already been negatively affected by the time they call customer care
center to report their service quality issues).
What operators lack today is a proactive approach to obtaining
comprehensive views of user’s quality of experience. Such views
are critical to proactively detecting service issues that matter to cus-
tomers, so that operators can rapidly respond to them to ensure a
high-quality customer experience. Without such visibility, opera-
tors risk being unaware of service degradations - instead they learn
about issues only when customers reach frustration points and re-
port to customer care call centers. Thus, proactive assessment of
quality of experience is crucial to providing operators with insights
into the ultimate metric - what customers are experiencing - so that
they can effectively manage their service offerings and detect and
ideally respond to issues before customers report issues. Proactive
assessment of quality of experience can also help operators to ef-
fectively dimension the customer care workforce in anticipation of
the large volume of user calls and customer-impacting conditions
can be avoided.
Although there is extensive monitoring of network elements in
place today and operators rapidly react to issues which are reported
by the network elements - there is no technology which can di-
rectly measure customer perception of TV service quality. Video
monitoring technologies exist, but it is still non-trivial to relate such
measurements to customer perception. Deploying video monitor-
ing to millions of customers is also prohibitively expensive, and
service providers are typically constrained by the technology avail-
able within the Set-Top Boxes.
In the absence of direct measurements, we instead focus on us-
ing network measurements to infer customer service experience.
However, such an approach is still challenging, due to (i) incom-
plete knowledge about the relationship between user-perceived is-
sues and network performance metrics, and (ii) user feedback about
195quality of experience is biased towards the negative (i.e., customer
calls on reporting issues) and is often delayed, noisy and limited.
In this paper, we propose a new framework, which we refer to
as Q-score, for proactive assessment of user perceived quality of
experience. Q-score constructs a single quality of experience score
using performance metrics collected from the network. Q-score
consists of two key components: (i) ofﬂine learning of the associa-
tion between the service quality of experience and the network per-
formance metrics collected from the servers, routers and in-home
equipment, and (ii) online computation of the score for individual
users or groups of users. Q-score captures the quality of experi-
ence by users in a timely fashion and can provide operators with
rapid notiﬁcation of service issues, often giving them a lead time of
several hours before the user reports to the call center.
Q-score Design Challenges. Due to the interwoven server and net-
work system, as well as the sophisticated hardware and software
composition of home network devices, assessing service quality of
experience is a sophisticated task. The proposed Q-score approach
uses customer reports (e.g., tickets) to provide feedback regarding
issues that customers are concerned about. Designing Q-score re-
quired us to address the following key challenges:
1. Associating QoE with network performance. Because of
the inherent difference between network-level performance
indicators and user-perceived quality of service, associating
the two does not occur naturally. Even for domain experts,
since there is no objective video quality metric, it is not trivial
to identify key performance indicators that are closely related
to quality of experience. Even if the metric were available,
it would require more ﬁnely grained monitoring of network
indicators, which in turn might introduce scalability issues.
2. Lack of timely, high-quality user feedback. User feedback
is inherently noisy, incomplete and delayed. Depending on
situations such as the individual viewer’s living schedule, the
severity of the issue, there are large variances between the be-
ginning of service quality issues and reporting times. Some
users issue a report immediately after they observe a ser-
vice quality degradation; others may wait hours before call-
ing customer service centers. Similarly, different users have
different tolerance levels to service quality issues - one user
may report incessantly regarding issues that another user may
barely notice. This all makes it inherently challenging to use
such feedback to associate service quality of experience with
network performance.
3. Large volume of diverse performance measurements. From
a network perspective, service providers typically collect ﬁne-
grained measurements from the routers and servers (e.g., real-
time syslogs, and regular polls of SNMP performance coun-
ters such as CPU, memory, packet counts, and losses). Some
performance measurements inside the home may be ﬁne-
grained (e.g., residential gateway events), whereas others may
be coarse-grained (e.g., hourly or daily summaries of Set-
Top Box events). Set-Top Box (STB) data collection is in-
tentionally not ﬁne-grained to minimize the potential of ser-
vice disruption due to measurements and due to the massive
scale of the measurement infrastructure that would be re-
quired. The diversity in the granularity of performance mea-
surements poses interesting technical challenges in inferring
the quality of experience.
Our Contributions.
1. We design and implement a prototype Q-score system for
proactively assessing quality of experience for IPTV users.
Q-score uses a multi-scale spatio-temporal statistical mining
technique for computing a single score capturing the quality
of experience. By performing spatio-temporal aggregation
and multi-scale association of the user feedback with net-
work performance metrics, it identiﬁes the right set of met-
rics useful for accurately quantifying the quality of experi-
ence.
2. We evaluate Q-score using data collected from a large com-
mercial IPTV service provider and show that Q-score is able
to predict 60% of customer service calls with 0.1% of false
positives.
3. We create three applications above Q-score. (i) Identifying
important Key Performance Indicators (KPIs) that are statis-
tically associated with the quality of experience , (ii) Predict-
ing bad quality of experience to users and generating alerts to
the Operations team, and (iii) Effective dimensioning of the
customer care workforce to dynamically allocate repair per-
sonnel to service regions as they experience issues for con-
ducting root-cause diagnosis and rapid repair.
Organization. The remainder of the paper is organized as follows.
In Section 2, we provide background information regarding the
IPTV network architecture and its data. We describe the design of
Q-score in Section 3, with details on its ofﬂine learning component
and online monitoring component. We present performance evalu-
ation results in Section 4. In Section 5, we explore three important
applications of Q-score. We review related work in Section 6 and
offer conclusions in Section 7.
2. BACKGROUND
In this section, we give an overview of the IPTV service archi-
tecture followed by a detailed description of the data sets used in
the paper.
2.1 IPTV Service Architecture
Figure 1 provides a schematic overview of an IPTV system. The
service network exhibits a hierarchical structure where video con-
tent is delivered via IP multicast from servers in the provider’s core
network to millions of Set-Top Boxes (STBs) within home net-
works. Speciﬁcally, either the Super Head-end Ofﬁce (SHO) which
serves as the primary source of national content or Video Head-end
Ofﬁces (VHOs) which governs local content at each metropolitan
area encodes, packetizes and sends the content towards end users.
Depending on the service provider, the video content goes through
several routers and switches in Intermediate Ofﬁces (IOs), Cen-
tral Ofﬁces (COs), a Digital Subscriber Line Access Multiplexer
(DSLAM), and a Residential Gateway (RG) before reaching STB
where the packetized content gets decoded and displayed on the
TV. All of the network entities comprising the IPTV service logs
Key Performance Indicators (KPIs) such as delivery status of data
and health diagnostics.
2.2 Data Sets
In the paper, we use data collected from a large commercial
IPTV service provider in the United States, which has customers
spread throughout four different time-zones. Our data set consists
of (i) network performance indicators, (ii) user behaviors and activ-
ities, and (ii) user feedback in the form of customer trouble tickets
196VOD Servers
Video Encoders
Local Video Encoders
Local VOD Servers
Video 
Contents
Data Set Type
Network
Performance
Indicators
STB
Spatial Level
Description
STB audio quality indicators
STB video quality indicators
STB syslog
STB resets
STB crashes
RG Reboots
RG
IO & CO
SNMP MIBs of routers & switches
SHO & VHO SNMP MIBs of routers & switches
User
Activity
Indicators
User Feedback User
User
STB power on/off log
STB Channel change log
STB Stream control log
Customer trouble tickets
Network
Table 1: Summary of data sets
Super
Head-end
Office
Video
Head-end
Office
Intermediate
Office
Centeral
Office
Home
SHO
VHO
IO
CO
DSLAM
Residential 
Gateway
Set-Top-Box
Phone
Computer
TV
User 
Action
Figure 1: IPTV service architecture
as summarized in Table 1. We normalize timestamps in all data sets
to GMT to accurately and effectively associate the user feedback
with performance metrics and user behaviors. The data has been
collected for 45 days from August 1st to September 15th, 2010.
Note that all the information related to the users are anonymized to
preserve their previcy.
Network Performance Indicators. Network performance indica-
tors are categorized into two types: (i) provider network perfor-
mance indicators, which are collected from routers and switches
in SHO, VHO, IO, CO of the IPTV service provider as shown in
Figure 1 and (ii) home network performance indicators, which are
collected from components inside user’s homes (i.e., RG and STB).
For the provider network performance data, we collected SNMP
MIBs from every router and switch in SHO, VHO, IO, and CO.
The SNMP MIBs report ﬁve minute average performance statistics
of CPU utilization and ﬁfteen minute average summaries for packet
count, packet delivery errors and discards.
From the home network side, we ﬁrst collected data relevant to
each STB and RG. Each STB records audio and video streaming-
related information including video throughput, receiver transport
stream errors, codec errors, DRM errors, and viewing duration of
TV channels. The video streaming-related information is reset when
the TV tuner clears its buffer by switching channels. While each
STB logs all the TV viewing information at all times, polling servers
only take a subset of the STBs’ statistics at each polling interval
(due to the high volume of audio and video log and trafﬁc over-
head during data delivery). As a result, only a sampled set of STBs
(i.e., 2% of all STBs) are used in our study. Secondly, we collected
STB syslog information that contains a wide variety of hardware
and software information, such as hard disk usage and memory us-
age, data delivery status such as packet error rate and buffer usage.
The diagnostic information are collected in the same way as the
STB audio and video log, i.e., sampled data were polled by col-
lection server in round robin fashion. Thirdly, we collected crash
and reset events log from each STB. The crash events log refers
to unexpected rebooting of STBs due to software malfunctions and
the reset refers to intentional and scheduled reboots commanded
by network operators due to, for instance, software updates. The
crash and reset log are periodically collected from all STBs with
millisecond scale time stamps. Last performance indicator taken
from home network is the reboot log of RGs that are commanded
by operators remotely. RG reboot logs are collected in the same
way as STB reboot logs. The crash and reboot logs are collected
from the entire seven million STBs.
User Activity Indicators. Because IPTV networks are highly user-
interactive systems, certain user activity patterns or habits can cre-
ate overload conditions on the STB and cause a service issue (e.g.,
a user changing channels too frequently may cause its upstream
device such as a DSLAM to be overwhelmed, leading to an in-
ability to handle all of the remaining STBs that it serves). Hence,
user activities are another important factor to be considered. Sim-
ilar to conventional TV users, IPTV users use a vendor/provider-
customized remote controller to control the STB. For this paper,
we collected logs from every STB that captures four types of user
activities performed: (i) power on/off: this is the result of the user
pressing the power button to turn on or off the STB; (ii) chan-
nel switch: this can be the result of one of the three actions: tar-
get switching by directly inputting the channel number, sequential
scanning by pressing the Up/Down button, or pre-conﬁgured fa-
vorite channel list; (iii) video stream control: this includes actions
such as fast forward, rewind, pause and play that are performed on
either live TV streams, VoD, or DVR; and (iv) on-screen menu in-
vocation: this log saves the user action of pulling up the STB menu
displayed on TV screen that lets the users to access the features
provided by the IPTV system.
User Feedback. For user feedback, we used calls made to the cus-
tomer care center of an IPTV service. Customer care cases are
records of user interactions at call centers. A customer call can
be related to service provisioning, billing and accounting, or ser-
vice disruption. Since the focus of our paper is on quality of ex-
perience (QoE), we speciﬁcally examined users’ reports on service
disruptions that later involved technical support as our user feed-
back. Each customer call related to service disruption includes the
anonymized user ID, report date and time, brief description of the
problem, and resolution of the issue.
197Network features
User activities
User feedback
Feature extraction
- Fixed interval time binning
- Derived feature generation
- Normalization
Derived features
Feature aggregation
- Temporal aggregation
- Spatial aggregation
Aggregated features
Online monitoring
Q-scores
Online continuous 
monitoring component
Feedback aggregation
- Temporal aggregation
- Spatial aggregation
Aggregated Feedbacks
Regression
Coefficient
Offline learning 
component
Figure 2: Overview of Q-score design