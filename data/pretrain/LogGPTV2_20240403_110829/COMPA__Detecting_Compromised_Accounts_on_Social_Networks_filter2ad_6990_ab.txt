the day during which an account is typically active. Many
users have certain periods during the course of a day where
they are more likely to post (e.g., lunch breaks) and others
that are typically quiet (e.g., regular sleeping hours). If a
user’s stream indicates regularities in social network usage,
messages that appear during hours that are associated with
quiet periods are considered anomalous.
Message Source. The source of a message is the name of
the application that was used to submit it. Most social net-
working sites offer traditional web and mobile web access
to their users, along with applications for mobile platforms
such as iOS and Android. Many social network ecosystems
provide access to a multitude of applications created by in-
dependent, third-party developers.
Of course, by default, a third-party application cannot
post messages to a user’s account. However, if a user
chooses to, she can grant this privilege to an application.
The state-of-the-art method of governing the access of third-
party applications is OAUTH [14]. OAUTH is implemented
by Facebook and Twitter, as well as numerous other, high-
proﬁle web sites, and enables a user to grant access to her
proﬁle without revealing her credentials.
By requiring all third-party applications to implement
OAUTH, the social network operators can easily shut down
individual applications, should that become necessary. In
fact, our evaluation shows that third-party applications are
frequently used to send malicious messages.
This model determines whether a user has previously
posted with a particular application or whether this is the
ﬁrst time. Whenever a user posts a message from a new
application, this is a change that could indicate that an at-
tacker has succeeded to lure a victim into granting access to
a malicious application.
Message Text (Language). A user is free to author her
messages in any language. However, we would expect that
each user only writes messages in a few languages (typi-
cally, one or two). Thus, especially for proﬁles where this
feature is relatively stable, a change in the language is an
indication of a suspicious change in user activity.
To determine the language that a message was written
in, we leverage the libtextcat library. This library
performs n-gram-based text categorization, as proposed by
Cavnar and Trenkle [15]. Of course, for very short mes-
sages, it is often difﬁcult to determine the language. This
is particularly problematic for Twitter messages, which are
limited to at most 140 characters and frequently contain ab-
breviated words or uncommon spelling.
Message Topic. Users post many messages that contain
chatter or mundane information. But we would also expect
that many users have a set of topics that they frequently
talk about, such as favorite sports teams, music bands, or
TV shows. When users typically focus on a few topics in
their messages and then suddenly post about some different
and unrelated subject, this new message should be rated as
anomalous.
In general, inferring message topics from short snip-
pets of text without context is difﬁcult. However, some
social networking platforms allow users to label messages
to explicitly specify the topics their messages are about.
When such labels or tags are available, they provide a
valuable source of information. A well-known example
of a message-tagging mechanism are Twitter’s hashtags.
By preﬁxing the topic keyword with a hash character a
user would use #Olympics to associate her tweet with the
Olympic Games.
More sophisticated (natural language processing) tech-
niques to extract message topics are possible. However,
such techniques are outside the scope of this paper.
Links in Messages. Often, messages posted on social net-
working sites contain links to additional resources, such as
blogs, pictures, videos, or news articles. Links in messages
of social networks are so common that some previous work
has strongly focused on the analysis of URLs, often as the
sole factor, to determine whether a message is malicious or
not. We also make use of links as part of the behavioral pro-
ﬁle of a user, but only as a single feature. Moreover, recall
that our features are primarily concerned with capturing the
normal activity of users. That is, we do not attempt to de-
tect whether a URL is malicious in itself but rather whether
a link is different than what we would expect for a certain
user.
To model the use of links in messages, we only make use
of the domain name in the URL of links. The reason is that
a user might regularly refer to content on the same domain.
For example, many users tend to read speciﬁc news sites
and blogs, and frequently link to interesting articles there.
Similarly, users have preferences for a certain URL short-
ening service. Of course, the full link differs among these
messages (as the URL path and URL parameters address
different, individual pages). The domain part, however, re-
mains constant. Malicious links, on the other hand, point to
sites that have no legitimate use. Thus, messages that link
to domains that have not been observed in the past indicate
a change. The model also considers the general frequency
of messages with links, and the consistency with which a
user links to particular sites.
Direct User Interaction. Social networks offer mecha-
nisms to directly interact with an individual user. The most
common way of doing this is by sending a direct message
that is addressed to the recipient. Different social networks
have different mechanisms for doing that. For example, on
Facebook, one posts on another user’s wall; on Twitter, it
is possible to directly “mention” other users by putting the
@ character before the recipient’s user name. Over time,
a user builds a personal interaction history with other users
on the social network. This feature aims to capture the in-
teraction history for a user.
In fact, it keeps track of the
users an account ever interacted with. Direct messages are
sent to catch the attention of their recipients, and thus are
frequently used by spammers.
Proximity. In many cases, social network users befriend
other users that are close to them. For example, a typi-
cal Facebook user will have many friends that live in the
same city, went to the same school, or work for the same
company. If this user suddenly started interacting with peo-
ple who live on another continent, this could be suspicious.
Some social networking sites (such as Facebook) express
this proximity notion by grouping their users into networks.
The proximity model looks at the messages sent by a user.
If a user sends a message to somebody in the same network,
this message is considered as local. Otherwise, it is consid-
ered as not local. This feature captures the fraction of local
vs. non-local messages.
If COMPA is implemented directly by a social network
provider, the geo-locations of the users’ IP addresses can be
used to signiﬁcantly improve the proximity feature. Unfor-
tunately, this information is not available to us.
3 Detecting Anomalous Messages
3.1 Training and Evaluation of the Models
In this section, we ﬁrst discuss how we train models for
each of the previously-introduced features. We then de-
scribe how we apply a model to a new message to compute
an anomaly score. Finally, we discuss how the scores of
the different models are combined to reach a ﬁnal anomaly
score that reﬂects how the new message is different from
the historic messages used when building the model.
Training. The input for the training step of a model is the
series of messages (the message stream) that were extracted
from a user account. For each message, we extract the rele-
vant features such as the source application and the domains
of all links.
Each feature model is represented as a set M. Each
element of M is a tuple .
f v is the value
of a feature (e.g., English for the language model, or
example.com for the link model). c denotes the num-
ber of messages in which the speciﬁc feature value f v was
present. In addition, each model stores the total number N
of messages that were used for training.
Our models fall into two categories:
- Mandatory models are those where there is one fea-
ture value for each message, and this feature value is
always present. Mandatory models are time of the day,
source, proximity, and language.
- Optional models are those for which not every mes-
sage has to have a value. Also, unlike for mandatory
models, it is possible that there are multiple feature
values for a single message. Optional models are links,
direct interaction, and topic. For example, it is possi-
ble that a message contains zero, one, or multiple links.
For each optional model, we reserve a speciﬁc element
with f v = null, and associate with this feature value
the number of messages for which no feature value is
present (e.g., the number of messages that contain no
links).
The training phase for the time of the day model works
slightly differently. Based on the previous description, our
system would ﬁrst extract the hour of the day for each mes-
sage. Then, it would store, for each hour f v, the number of
messages that were posted during this hour. This approach
has the problem that hour slots, unlike the progression of
time, are discrete. Therefore, messages that are sent close
to a user’s “normal” hours could be incorrectly considered
as anomalous.
To avoid this problem, we perform an adjustment step
after the time of the day model was trained (as described
above). In particular, for each hour i, we consider the val-
ues for the two adjacent hours as well. That is, for each
element  of M, a new count c(cid:48)i is calculated as the
average between the number of messages observed during
the ith hour (ci), the number of messages sent during the
previous hour (ci−1), and the ones observed during the fol-
lowing hour (ci+1). After we computed all c(cid:48)i, we replace
the corresponding, original values in M.
As we mentioned previously, we cannot reliably build
a behavioral proﬁle if the message stream of a user is too
short. Therefore, the training phase is aborted for streams
shorter than S = 10, and any message sent by those users
is not evaluated.
Evaluating a new message. When calculating the anomaly
score for a new message, we want to evaluate whether this
message violates the behavioral proﬁle of a user for a given
model. In general, a message is considered more anomalous
if the value for a particular feature did not appear at all in
the stream of a user, or it appeared only a small number
of times. For mandatory features, the anomaly score of a
message is calculated as follows:
1. The feature f v for the analyzed model is ﬁrst extracted
from the message. If M contains a tuple with f v as a
ﬁrst element, then the tuple  is extracted
from M. If there is no tuple in M with f v as a ﬁrst
value, the message is considered anomalous. The pro-
cedure terminates here and an anomaly score of 1 is
returned.
2. As a second step, the approach checks if f v is anoma-
lous at all for the behavioral proﬁle being analyzed. c
(cid:80)(cid:107)M(cid:107)
ci
is compared to ¯M, which is deﬁned as ¯M =
,
where ci is, for each tuple in M, the second element of
the tuple. If c is greater or equal than ¯M, the message
is considered to comply with the learned behavioral
i=1
N
proﬁle for that model, and an anomaly score of 0 is re-
turned. The rationale behind this is that, in the past,
the user has shown a signiﬁcant number of messages
with that particular f v.
3. If c is less than ¯M, the message is considered some-
what anomalous with respect to that model. Our ap-
proach calculates the relative frequency f of f v as
f = cf v
N . The system returns an anomaly score of 1
- f.
The anomaly score for optional features is calculated as:
1. The feature f v for the analyzed model is ﬁrst extracted
from the message. If M contains a tuple with f v as a
ﬁrst element, the message is considered to match the
behavioral proﬁle, and an anomaly score of 0 is re-
turned.
2. If there is no tuple in M with f v as a ﬁrst element, the
message is considered anomalous. The anomaly score
in this case is deﬁned as the probability p for the ac-
count to have a null value for this model. Intuitively,
if a user rarely uses a feature on a social network, a
message containing an f v that has never been seen be-
fore for this feature is highly anomalous. The proba-
N . If M does not have a
bility p is calculated as p = cnull
tuple with null as a ﬁrst element, cnull is considered
to be 0. p is then returned as the anomaly score.
As an example, consider the following check against the
language model: The stream of a particular user is com-
posed of 21 messages. Twelve of them are in English, while
nine are in German. The M of the user for that particular
model looks like this:
(,).
The next message sent by that user will match one of three
cases:
- The new message is in English. Our approach extracts
the tuple  from M, and compares c = 12
to ¯M = 10.5. Since c is greater than ¯M, the message
is considered normal, and an anomaly score of 0 is re-
turned.
- The new message is in Russian. Since the user never
sent a message in that language before, the message is
considered very suspicious, and an anomaly score of 1
is returned.
- The new message is in German. Our approach extracts
the tuple  from M, and compares c = 9
to ¯M = 10.5. Since c < ¯M, the message is con-
sidered slightly suspicious. The relative frequency of
German tweets for the user is f = c
N = 0.42. Thus,
an anomaly score of 1 − f = 0.58 is returned. This
means that the message shows a slight anomaly in the
user average behavior. However, as explained in Sec-
tion 5, on its own this score will not be enough to ﬂag
the message as malicious.
Computing the ﬁnal anomaly score. Once our system has
evaluated a message against each individual model, we need
to combine the results into an overall anomaly score for this
message. This anomaly score is a weighted sum of the val-
ues for all models. We use Sequential Minimal Optimiza-
tion [16] to learn the optimal weights for each model, based
on a training set of instances (messages and correspond-
ing user histories) that are labeled as malicious and benign.
Of course, different social networks will require different
weights for the various features. A message is said to vi-
olate an account’s behavioral proﬁle if its overall anomaly
score exceeds a threshold. In Section 5, we present a more
detailed discussion on how the threshold values are deter-
mined. Moreover, we discuss the weights (and importance)
of the features for the different social networks that we an-
alyzed (i.e., Twitter and Facebook).
3.2 Robustness of the Models