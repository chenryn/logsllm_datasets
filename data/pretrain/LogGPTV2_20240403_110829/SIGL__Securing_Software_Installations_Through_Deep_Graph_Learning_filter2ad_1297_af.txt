.
0
4
2
9
.
0
8
6
7
.
0
8
8
9
.
0
5
0
9
.
0
C
U
A
1
0.5
0
1 1 1 1 1 1 1 1 1
1 1
5
4
9
.
0
5
3
9
.
0
6
9
.
0
3 1 1
3
8
.
0
FireFox
FileZilla
P W Safe
M P3Gain
ShotCut
Team Viewer
Foobar
7Zip
TurboV NC
WinMerge
Launchy
Skype
WinRAR
DropBox
Slack
OneDrive
Flash
NotePad++
AV Remover
ICBC
Application-Speciﬁc Model Meta-Model
Figure 8: AUC result comparison for each installer using application-speciﬁc
vs. meta model. The Skype dataset is not used in training the meta model.
for one attack graph. When applying the adversarial model
trained on the Skype dataset to other installers, its perfor-
mance varies depending on the installer. In fact, its efﬁcacy
ﬂuctuates even within the Skype dataset itself where the target
model is known to the attacker. We investigate the changes
in anomaly scores of Skype’s attack graphs under the ad-
versarial inﬂuence. Fig. 7 shows that even the best possible
manipulation (predicted by the trained RL model) does not
necessarily reduce an attack graph’s anomaly score. Our re-
sults differ signiﬁcantly from prior work demonstrating the
efﬁcacy of adversarial attacks on graphs (e.g., [17]). This
prior work demonstrated efﬁcacy on graphs from citation and
social networks. We hypothesize that adversarial attacks are
less effective in our setting, because 1) provenance graphs are
structurally different from these network graphs, and 2) our
setting allows a more constrained set of changes to the graph.
5.10 Building SIGL Meta-Model
SIGL is designed to build one model per application, but it
can easily build a “meta-model” that learns generic software
installation behavior. Intuitively, such a generalized model can
classify unseen installers, thus saving considerable manual
labor from training new application-speciﬁc models. On the
other hand, it must perform comparably to those models to
warrant its usability for the installers in the training dataset.
Experimental Setup. We trained a meta-model using the
training sets from all but the Skype installer (selected ran-
domly). We then evaluated the meta-model using both the
benign and malicious datasets from each application, includ-
ing Skype. This experimental setup is identical to the one de-
scribed in § 5.2 to fairly compare against application-speciﬁc
models. We repeated this experiment by randomly excluding
different installers; the results are similar.
We further investigated meta-model performance when
trained with various numbers of applications. We excluded
5%, 10%, 20%, and 40% of the original applications from the
training set and rebuilt the meta-model for each scenario. We
evaluated each meta-model with two sets of test data, 1) the
benign and malicious test sets from the applications used in
training (INC in Fig. 9), and 2) the benign and malicious test
sets from the excluded applications (EXC).
Experimental Results. Fig. 8 shows the AUC results for all
the installers. For half of the installers, the AUC is unchanged;
even for the other half, it decreases marginally. Most installers
achieve over 0.9 AUC under the meta-model. Although the
1
0.75
0.5
C
U
A
INC
EXC
0
0 .0 5
0 .1
0 .2
0 .4
0 .5
Percentage of Excluded Applications
Figure 9: AUC results when meta-models are trained with various numbers
of applications. The meta-models are tested on applications included (INC)
in and excluded (EXC) from the training data.
model is never trained on the Skype dataset, it is able to ac-
curately separate its benign and malicious instances. This
result implies that commonalities exist in legitimate software
installations, and SIGL learns these shared characteristics. Sur-
prisingly, we also see AUC improvement for TeamViewer and
WinMerge, which is likely the result of model generalizability.
Fig. 9 shows the AUC results for meta-models trained with
different percentages of applications. When the meta-model
learns from a smaller set of applications, it inevitably faces
more challenges generalizing to unseen software, but works
better on the trained ones. Since the performance gracefully
degrades with an increasing number of new applications, SIGL
provides abundant opportunities for system administrators to
retrain the meta-model ( § 7).
5.11 Runtime Performance
SIGL takes, on average, fewer than 90 minutes (on a sin-
gle GPU on our local test machine) to train a model for a
particular software. Training for different installations can be
performed in parallel and/or distributed to the cloud. Table 2
shows the number of installation graphs we used for training.
We train only on the graphs available in our current database;
SIGL can be effective even across versions (§ 5.6) and on un-
seen software (§ 5.10). SIGL supports incremental learning to
efﬁciently train on new graph samples. With SIGL’s guidance
(§ 5.5), system administrators can easily decide to further
improve a model if top-ranked processes are not malicious.
Once trained, SIGL takes less than 1 second to evaluate a SIG.
5.12 SIGL in Linux
We see in § 5.10 that SIGL can build generic, application-
agnostic models that detect abnormal installation behavior
on Windows. In this section, we further demonstrate that
SIGL is generalizable to an even larger variety of software
packages and on different platforms. Since our enterprise
monitoring system collects only Windows audit data, we set
up our own Linux testbed and generated a dataset of 2,885
Python package installation graphs.
Experimental Setup. We trained SIGL on 1,708 benign in-
stallation graphs, each of which was collected using Linux
Audit from installing different Python packages including
popular tools [75] such as urlib3, and six. After training
such a meta-model on all 1,708 packages, we design our ex-
periments to focus on two research questions:
USENIX Association
30th USENIX Security Symposium    2357
Q1. Given that SIGL is trained on a large number of distinct
software packages, is it able to generalize to new benign pack-
ages and maintain a low false positive rate (FPR)? We are
particularly concerned with FPs, because anomaly-based sys-
tems are generally more likely to produce excessive FPs that
overwhelm cyberanalysts, especially when they are trained
on diverse datasets. We tested the model on 1,176 installation
graphs of benign packages unknown to the model.
Q2. Can SIGL accurately detect malicious software packages
and provide targeted guidance? We used a real-world mali-
cious Python package python3-dateutil that was uploaded
to PyPI in 2019. The benign version of the same package is a
popular utility tool that extends Python’s standard datetime
module. We note that the attack does not create any malicious
binary ﬁles on the victim system. Instead, it executes obfus-
cated malicious code in the package that transmits sensitive
user information to a remote host.
Experimental Results (Q1). Among 1,176 benign test
graphs, SIGL reports 29 FPs, resulting in only 2.47% FPR.
This further corroborates our experimental results in § 5.10
that SIGL is capable of learning from a diverse set of training
data to model generic installation behavior.
Experimental Results (Q2). SIGL correctly detects the ma-
licious Python package. It indicates the process making a
network connection to a Bitly URL as the most abnormal,
thus providing accurate attack attribution.
Overall, SIGL is effective in modeling diverse installation
behaviors from a large variety of software packages on differ-
ent OS platforms and installation frameworks.
6 Case Studies
We describe two case studies illustrating SIGL using differ-
ent real-world malicious installers in Table 3.
Malware Bundled with ESET AV Remover Installer.
In § 2, we described a real-world attack scenario where the
user is phished to install a legitimate ESET AV Remover in-
staller [53] bundled with malware. Fig. 1 shows a simpli-
ﬁed software installation graph from this scenario. When the
malware (taskhost.exe in the shaded area in Fig. 1) runs
during benign software installation (AVRemover.exe), it es-
tablishes a communication channel (x.y.z.s:t) with the
attacker, which allows the attacker to perform further damage
(e.g., exﬁltrate sensitive information). Note that the user is
unaware of this activity since she is distracted interacting with
the benign ESET AV Remover installer.
We discuss in § 2 how existing tools might fail to detect
malicious activities from such an installation. SIGL, on the
other hand, constructs a SIG from the audit data, and tests the
graph against the existing ESET AV Remover model. SIGL
generates a threat alert for this graph because its anomaly
score is much larger than the set threshold and orders of mag-
nitude greater than those of the training graphs. SIGL also
ranks the AVRemover.exe process node in the shaded area
in Fig. 1 among the most anomalous processes (i.e., targeted
Figure 10: The software installation graph from the malicious Flash installer.
The colored process nodes are top-ranked by SIGL.
guidance). We observe that AVRemover.exe is considered
more anomalous than the malware process taskhost.exe,
probably because it is uncommon for the installer process to
spawn two child processes at the beginning of the installa-
tion. SIGL ranks the malware process taskhost.exe lower
because structurally, it resembles benign process behavior
that also communicates with outside IP addresses. However,
system administrators can easily identify the malicious pro-
cess through quick one-hop backtracking starting from the
top-ranked AVRemover.exe process. Compared to the entire
SIG, SIGL reduces the number of events that the administrator
needs to inspect by two orders of magnitude.
Malware Embedded within Flash Installer. Different
from the malicious ESET AV Remover installer, the malicious
Flash installer embeds a dropper and a potentially unwanted
application (PUA). The dropper (downloader.exe) commu-
nicates with outside channels and downloads additional mal-
ware (e.g., yandexsetup.exe). The installer also installs anti-
virus software (AvastAntiVirusSetup.exe) without user
consent. Fig. 10 shows a simpliﬁed software installation graph.
SIGL identiﬁes FlashPlayer.tmp (red) as the most
anomalous process (i.e., targeted guidance) and down-
loader.exe (yellow) in the top 10. The additional processes
started by the installation process (FlashPlayer.tmp) and
their progeny subgraphs possibly lead to its high anomaly
score. The PUA, the dropper, and the malware it drops all be-
have differently from the benign Flash installer. SIGL ranks
the dropper process and all the malware processes (not shown
in Fig. 10 for clarity) above the PUA process, because the
PUA process behaves in a manner closer to that of the real
installation process (FlashPlayerInstaller.exe) than do
the other malicious processes. We can see from Fig. 10 that
their substructures resemble each other. Regardless, given
the dropper process, administrators already have sufﬁcient
information to conﬁrm the malicious nature of the installation.
2358    30th USENIX Security Symposium
USENIX Association
FlashPlayer.exeFlashPlayer.tmpFile WriteFlashPlayer.tmpProcess StartFlashPlayer.exeFile ReadFile ReadFlashPlayerDebug.exeFile WriteFlashPlayerDebug.exeProcess Startdownloader.exeFile Writedownloader.exeProcess StartAvastAntiVirusSetupOnline.exeFile WriteAvastAntiVirusSetupOnline.exeProcess Startcounters.datFile Readis-s2ge4.tmpFlashPlayerInstaller.exeFile WriteFlashPlayerInstaller.exeProcess StartPlayer.exeFile Writeutil.dllFile WriteFile WriteFile Writeyandexpacksetup.exeFile Writea.b.c.d:eIP WriteIP ReadFile Readx.y.z.s:tIP WriteAvastAntiVirusSetup.exeFile WriteAvastAntiVirusSetup.exeProcess StartIP ReadFile WriteFile WriteFile Write7 Discussion & Limitations
SIGL’s ML model shares characteristics common to other
statistical models [73]; model performance improves with
more training data. As we see in § 5.3 and § 5.4, SIGL achieves
good detection performance with only a small number of be-
nign installation graphs for training because of the speciﬁcity
of the domain, which enables SIGL to quickly learn represen-
tative behavior patterns. Other deep-learning-based detection
systems, e.g., DeepLog [19] and Tiresias [68], also enjoy the
same advantage as they target speciﬁc areas in the security
domain. For example, DeepLog mines log data in regulated
environments such as Hadoop and thus can learn normal ap-
plication behavior from a small fraction of normal log entries.
Regardless of training data size, one important key to
SIGL’s success, and of any modeling-based system, is data
quality. We see in § 5.3 that when data quality deteriorates,
it adversely affects system performance. However, SIGL can
signiﬁcantly outperform its peer systems, even with fairly
limited training data. We attribute its efﬁcacy to the fact that
SIGL learns on the entire graph, not a summary of it. This
makes SIGL desirable in an enterprise environment where the
only training data available have been generated internally or
in which the third party tools that collect the data might lose
data, e.g., due to small buffers or slow ingestion rates [54].
Software Evolution. We see that SIGL delivers consistent
performance across software versions (§ 5.6) and builds
application-agnostic models with a diverse training dataset
(§ 5.10, § 5.12). It can also learn deltas of software versions,
by modeling past versions of software, which we leave for
future work. However, as software continues to evolve and
additional software packages are installed, SIGL may even-
tually require retraining on the SIGs of new installers. We
lessen such burdens in several ways: 1) SIGL maintains a good
margin between anomaly scores of benign and malicious in-
stallers (§ 5.7). System administrators can easily position an
installer’s anomaly score among those used in training and
determine whether retraining is necessary. For example, the
benign NotePad++ installer with the highest anomaly score
(1.233× 10−4) is, in fact, the older version, while the training
instances used to model the newer version have much lower
scores (between 5× 10−6 and 5× 10−5). Admins might want
to consider retraining if they want all benign instances to
have anomaly scores < 1× 10−4. 2) SIGL provides effective
guidance (§ 5.5) to help analysts identify alert causes and dis-
miss false positives. 3) SIGL’s performance degrades slowly
(§ 5.10). 4) SIGL’s retraining is fast (§ 5.11).
Evasion. Stealthy malware might leverage process injection
techniques (e.g., DLL injection [14]) to inject malicious code
into a legitimate live process. If SIG did not capture the
causality relationship between the malware and the legiti-
mate process as a result of the injection, the attacker could
evade detection. This may be the case given that our current
prototype monitors only a subset of system events, but state-
of-the-art provenance-capture systems [60] are capable of
tracking memory-related events between processes, which
would allow SIGL to include affected legitimate processes
into analysis. We leave as future work to show that such
evasion is a mere artifact of our prototype, not the approach.
Attackers might use software installation to deposit mali-
cious software on a system but delay exploiting that software.
As SIGL is optimized for detecting malicious installations,
such a deployment might go unnoticed: SIGL might notice
that an extra piece of software appeared, but if that software is
not executed during the installation process, SIGL might not
ﬂag its existence as an anomaly. One possible solution is to
leverage forward tracking [44] to obtain a broader view of sys-
tem behavior to detect such time-dispersed anomalies. Prior
work [56] has shown that data provenance facilitates such
analysis by closely connecting causal events, even if they are
temporally distant. This makes it manageable to incorporate
forward tracking into SIGL. Interesting future work would
quantify the amount of tracking necessary for detection.
Benign Dataset. Many enterprises tightly control software
installation via centralized IT departments. Best practices for
deploying new software are to test initially on a limited set
of canary machines to detect stability or compatibility issues;
those machines are a natural source of labeled installation
data. Our IT department at NEC Labs America also places
remote telemetry facilities on end-user machines, collecting
data using enterprise-wide security monitoring solutions. Al-
though we cannot guarantee the collected data is perfectly
clean; in practice, our evaluation in § 5.8 demonstrates that
SIGL is robust against potential data contamination.
Adversarial Robustness. We evaluated two realistic adver-
sarial scenarios in § 5.9, considering systems constraints that
are absent in existing ML literature. We show that SIGL is ro-
bust against practical adversarial attacks, which is consistent
with recent studies [17, 84] showing that effectively attacking
graph structured data is hard. Granted, our evaluation is by
no means complete given increasing interests in ML to ad-
vance graph-based adversarial attacks. For example, Chang
et al. [12] recently proposed a graph signal-processing-based
approach to attack the graph ﬁlter of given models, nullifying
the need for any model information. Dai et al. [17] proposed
a genetic-algorithm-based attack in PBA (although it requires
additional information, e.g., a normality threshold). However,
these approaches are evaluated on the same citation network
datasets, which are structurally different from provenance
graphs (§ 5.9). Further technical discussion and evaluation of
adversarial ML is beyond the scope of this paper.
8 Related Work
Traditional approaches to securing software installations
emphasize authentication [6] (e.g., code signing [67] and
secure content distribution [57]), policy-guided sandbox-
ing [81], and information ﬂow control (IFC) [71]. Recent
incidents [24, 74] show that attackers can compromise legiti-
mate software distribution channels, bypassing cryptographic
USENIX Association
30th USENIX Security Symposium    2359
authentication protection. Meanwhile, in an enterprise envi-
ronment, sandboxing becomes impractical and is routinely
bypassed through social engineering and advanced exploit
techniques [33]; sophisticated policy-driven IFC is still too
complex to be widely adopted [79]. SIGL leverages audit data
easily collectable from enterprise workstations. Its core de-
sign lies at the intersection of graph-based malware detection
and provenance-based intrusion detection. We place SIGL in
the context of prior work in these areas.
Graph-Based Malware Detection. Panorama [82] uses taint
graphs to detect privacy-breaching malware. It analyzes infor-