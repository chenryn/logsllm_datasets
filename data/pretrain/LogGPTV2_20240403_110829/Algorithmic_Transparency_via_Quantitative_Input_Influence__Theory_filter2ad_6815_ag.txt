(b) Transparency report for Mr. Y’s negative classiﬁcation
Fig. 5: Mr. Y.
to be similar but received different classiﬁcation outcomes,
and identifying the inputs which were used by the classiﬁer to
provide different outcomes. Additionally, when group parity
is used as a criteria for fairness, QII can identify the features
that lead to group disparity, thereby identifying features being
used by a classiﬁer as a proxy for sensitive attributes.
The determination of whether using certain proxies for
sensitive attributes is discriminatory is often a task-speciﬁc
normative judgment. For example, using standardized test
scores (e.g., SAT scores) for admissions decisions is by and
large accepted, although SAT scores may be a proxy for
several protected attributes. In fact, several universities have
recently announced that they will not use SAT scores for
admissions citing this reason [33], [34]. Our goal
is not
to provide such normative judgments. Rather we seek to
provide ﬁne-grained transparency into input usage (e.g., what’s
the extent to which SAT scores inﬂuence decisions), which
is useful to make determinations of discrimination from a
speciﬁc normative position.
Finally, we note that an interesting question is whether
providing a sensitive attribute as an input to a classiﬁer is
fundamentally discriminatory behavior, even if QII can show
that
impact on the
the sensitive input has no signiﬁcant
612612
Birth Year
Drug History
Smoking History
Census Region
Race
Gender
(a) Mr. Z’s proﬁle
1984
None
None
West
Black
Male
l
)
y
e
p
a
h
S
(
e
m
o
c
t
u
O
n
o
I
I
Q
0.4
0.3
0.2
0.1
0.0
0.1
0.2
R a c e
Y e ar
Birth
G e n d er
R e gio n
History
D ru g
C e n s u s
History
S m o kin g
(b) Transparency report for Mr. Z’s positive classiﬁcation
Fig. 6: Mr. Z.
outcome. Our view is that this is a policy question and different
legal frameworks might take different viewpoints on it. At a
technical level, from the standpoint of information use, the
two situations are identical: the sensitive input is not really
used although it is supplied. However, the very fact that it
was supplied might be indicative of an intent to discriminate
even if that intended goal was not achieved. No matter what
the policy decision is on this question, QII remains a useful
diagnostic tool for discrimination because of the presence of
proxy variables as described earlier.
IX. RELATED WORK
A. Quantitative Causal Measures
Causal models and probabilistic interventions have been
used in a few other settings. While the form of the inter-
ventions in some of these settings may be very similar, our
generalization to account for different quantities of interests
enables us to reason about a large class of transparency
queries for data analytics systems ranging from classiﬁcation
outcomes of individuals to disparity among groups. Further,
the notion of marginal contribution which we use to compute
responsibility does not appear in this line of prior work.
Janzing et al. [35] use interventions to assess the causal
importance of relations between variables in causal graphs;
in order to assess the causal effect of a relation between two
variables, X → Y (assuming that both take on speciﬁc values
X = x and Y = y), a new causal model is constructed, where
the value of X is replaced with a prior over the possible values
of X. The inﬂuence of the causal relation is deﬁned as the KL-
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:24 UTC from IEEE Xplore.  Restrictions apply. 
Divergence of the joint distribution of all the variables in the
two causal models with and without the value of X replaced.
The approach of the intervening with a random value from the
prior is similar to our approach of constructing X−S.
Independently,
there has been considerable work in the
machine learning community to deﬁne importance metrics for
variables, mainly for the purpose of feature selection (see [36]
for a comprehensive overview). One important metric is called
Permutation Importance [37], which measures the importance
of a feature towards classiﬁcation by randomly permuting
the values of the feature and then computing the difference
of classiﬁcation accuracies before and after the permutation.
Replacing a feature with a random permutation can be viewed
as a sampling the feature independently from the prior.
There exists extensive literature on establishing causal re-
lations, as opposed to quantifying them. Prominently, Pearl’s
work [38] provides a mathematical foundation for causal rea-
soning and inference. In [39], Tian and Pearl discuss measures
of causal strength for individual binary inputs and outputs in a
probabilistic setting. Another thread of work by Halpern and
Pearl discusses actual causation [40], which is extended in [41]
to derive a measure of responsibility as degree of causality.
In [41], Chockler and Halpern deﬁne the responsibility of a
variable X to an outcome as the amount of change required
in order to make X the counterfactual cause. As we discuss
in Appendix A-B, the Deegan-Packel index is strongly related
to causal responsibility.
B. Quantitative Information Flow
One can think of our results as a causal alternative to
quantitative information ﬂow. Quantitative information ﬂow is
a broad class of metrics that quantify the information leaked
by a process by comparing the information contained before
and after observing the outcome of the process. Quantitative
Information Flow traces its information-theoretic roots to the
work of Shannon [42] and R´enyi [43]. Recent works have
proposed measures for quantifying the security of information
by measuring the amount of information leaked from inputs to
outputs by certain variables; we point the reader to [44] for an
overview, and to [45] for an exposition on information theory.
Quantitative Information Flow is concerned with information
leaks and therefore needs to account for correlations between
inputs that may lead to leakage. The dual problem of trans-
parency, on the other hand, requires us to destroy correlations
while analyzing the outcomes of a system to identify the causal
paths for information leakage.
C. Interpretable Machine Learning
An orthogonal approach to adding interpretability to ma-
chine learning is to constrain the choice of models to those that
are interpretable by design. This can either proceed through
regularization techniques such as Lasso [46] that attempt
to pick a small subset of the most important features, or
by using models that structurally match human reasoning
such as Bayesian Rule Lists [47], Supersparse Linear Integer
Models [48], or Probabilistic Scaling [49]. Since the choice
of models in this approach is restricted, a loss in predictive
accuracy is a concern, and therefore, the central focus in
this line of work is the minimization of the loss in accuracy
while maintaining interpretability. On the other hand, our
approach to interpretability is forensic. We add interpretability
to machine learning models after they have been learnt. As a
result, our approach does not constrain the choice of models
that can be used.
D. Experimentation on Web Services
There is an emerging body of work on systematic experi-
mentation to enhance transparency into Web services such as
targeted advertising [50], [51], [52], [53], [54]. The setting in
this line of work is different since they have restricted access
to the analytics systems through publicly available interfaces.
As a result they only have partial control of inputs, partial
observability of outputs, and little or no knowledge of input
distributions. The intended use of these experiments is to
enable external oversight into Web services without any coop-
eration. Our framework is more appropriate for a transparency
mechanism where an entity proactively publishes transparency
reports for individuals and groups. Our framework is also
appropriate for use as an internal or external oversight tool
with access to mechanisms with control and knowledge of
input distributions, thereby forming a basis for testing.
E. Game-Theoretic Inﬂuence Measures
Recent years have seen game-theoretic inﬂuence measures
used in various settings. Datta et al. [55] also deﬁne a measure
for quantifying feature inﬂuence in classiﬁcation tasks. Their
measure does not account for the prior on the data, nor does
it use interventions that break correlations between sets of
features. In the terminology of this paper, the quantity of
interest used by [55] is the ability of changing the outcome by
changing the state of a feature. This work greatly extends and
generalizes the concepts presented in [55], by both accounting
for interventions on sets, and by generalizing the notion of
inﬂuence to include a wide range of system behaviors, such
as group disparity, group outcomes and individual outcomes.
Game theoretic measures have been used by various re-
search disciplines to measure inﬂuence. Indeed, such measures
are relevant whenever one is interested in measuring the
marginal contribution of variables, and when sets of variables
are able to cause some measurable effect. Lindelauf et al. [56]
and Michalak et al. [57] use game theoretic inﬂuence measures
on graph-based games in order to identify key members of
terrorist networks. Del Pozo et al. [58] and Michalak et al. [59]
use similar ideas for identifying important members of large
social networks, providing scalable algorithms for inﬂuence
computation. Bork et al. [60] use the Shapley value to assign
importance to protein interactions in large, complex biological
interaction networks; Keinan et al. [61] employ the Shapley
value in order to measure causal effects in neurophysical
models. The novelty in our use of the game theoretic power
indices lies in the conception of a cooperative game via a
valuation function ι(S), deﬁned by an randomized intervention
613613
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:16:24 UTC from IEEE Xplore.  Restrictions apply. 
on inputs S. Such an intervention breaks correlations and
allows us to compute marginal causal inﬂuences on a wide
range of system behaviors.
X. CONCLUSION & FUTURE WORK
In this paper, we present QII, a general family of metrics
for quantifying the inﬂuence of inputs in systems that process
personal information. In particular, QII lends insights into the
behavior of opaque machine learning algorithms by allowing
us to answer a wide class of transparency queries ranging
from inﬂuence on individual causal outcomes to inﬂuence
on disparate impact. To achieve this, QII breaks correlations
between inputs to allow causal reasoning, and computes the
marginal inﬂuence of inputs in situations where inputs cannot
affect outcomes alone. Also, we demonstrate that QII can
be efﬁciently approximated, and can be made differentially
private with negligible noise addition in many cases.
An immediate next step in this line of work is to explore
adoption strategies in the many areas that use personal infor-
mation to aid decision making. Areas such as healthcare [3],
predictive policing [1], education [4], and defense [5] all have
a particularly acute need for transparency in their decision
making. It is likely that speciﬁc applications will guide us in
our choice of a QII metric that is appropriate for that scenario,
which includes a choice for our game-theoretic power index.
We have not considered situations where inputs do not
have well understood semantics. Such situations arise often in
settings such as image or speech recognition, and automated
video surveillance. With the proliferation of immense process-
ing power, complex machine learning models such as deep
neural networks have become ubiquitous in these domains.
Deﬁning transparency and developing analysis techniques in
such settings is important future work.
REFERENCES
[1] W. L. Perry, B. McInnis, C. C. Price, S. C. Smith, and J. S. Hollywood,
Predictive Policing: The Role of Crime Forecasting in Law Enforcement
Operations. RAND Corporation, 2013.
[2] T. Alloway, “Big data: Credit where credits due,” http://www.ft.com/
cms/s/0/7933792e-a2e6-11e4-9c06-00144feab7de.html.
[3] T. B. Murdoch and A. S. Detsky, “The inevitable application of big
data to health care,” http://jama.jamanetwork.com/article.aspx?articleid=
1674245.
[4] “Big
data
in
education,”
https://www.edx.org/course/
big-data-education-teacherscollegex-bde1x.
in
and
data
-
government,
security
[5] “Big
2015
http://www.prnewswire.com/news-releases/
big-data-in-government-defense-and-homeland-security-2015---2020.
html.
homeland
defense
2020,”
[6] J. Podesta, P. Pritzker, E. Moniz, J. Holdern, and J. Zients, “Big
data: Seizing opportunities, preserving values,” Executive Ofﬁce of the
President - the White House, Tech. Rep., May 2014.
[7] “E.G. Griggs v. Duke Power Co., 401 U.S. 424, 91 S. Ct. 849, 28 L.
Ed. 2d 158 (1977).”
[8] C. Dwork, F. McSherry, K. Nissim, and A. Smith, “Calibrating
noise to sensitivity in private data analysis,” in Proceedings of the
Third Conference on Theory of Cryptography, ser. TCC’06. Berlin,
Heidelberg: Springer-Verlag, 2006, pp. 265–284. [Online]. Available:
http://dx.doi.org/10.1007/11681878 14
[9] S. Kasiviswanathan, H. Lee, K. Nissim, S. Raskhodnikova, and
A. Smith, “What can we learn privately?” in Proceedings of the 49th
IEEE Symposion on Foundations of Computer Science (FOCS 2008),
Oct 2008, pp. 531–540.
[10] M. Lichman, “UCI machine learning repository,” 2013.
[Online].
Available: http://archive.ics.uci.edu/ml
[11] “National longitudinal surveys,” http://www.bls.gov/nls/.
[12] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and R. Zemel, “Fairness
through awareness,” in Proceedings of the 3rd Innovations in Theoretical
Computer Science Conference (ITCS 2012), 2012, pp. 214–226.
[13] M. Lichman, “UCI machine learning repository,” 2013.
[Online].
Available: http://archive.ics.uci.edu/ml
[14] L. Shapley, “A value for n-person games,” in Contributions to the Theory
of Games, vol. 2, ser. Annals of Mathematics Studies, no. 28. Princeton
University Press, 1953, pp. 307–317.
[15] M. Maschler, E. Solan, and S. Zamir, Game Theory.
Cambridge
University Press, 2013.
[16] L. S. Shapley and M. Shubik, “A method for evaluating the distribution
of power in a committee system,” The American Political Science
Review, vol. 48, no. 3, pp. 787–792, 1954.
[17] J. Banzhaf, “Weighted voting doesn’t work: a mathematical analysis,”
Rutgers Law Review, vol. 19, pp. 317–343, 1965.
[18] J. Deegan and E. Packel, “A new index of power for simple n-person
games,” International Journal of Game Theory, vol. 7, pp. 113–123,
1978.
[19] H. Young, “Monotonic solutions of cooperative games,” International
Journal of Game Theory, vol. 14, no. 2, pp. 65–72, 1985.
[20] S. Kullback and R. A. Leibler, “On information and sufﬁciency,” Annals
of Mathematical Statistics, vol. 22, no. 1, pp. 79–86, 1951.
[21] G. Chalkiadakis, E. Elkind, and M. Wooldridge, Computational Aspects
of Cooperative Game Theory. Morgan and Claypool, 2011.
[22] Y. Bachrach, E. Markakis, E. Resnick, A. Procaccia, J. Rosenschein,
and A. Saberi, “Approximating power indices: theoretical and empirical
analysis,” Autonomous Agents and Multi-Agent Systems, vol. 20, no. 2,
pp. 105–122, 2010.
[23] S. Maleki, L. Tran-Thanh, G. Hines, T. Rahwan, and A. Rogers, “Bound-
ing the estimation error of sampling-based shapley value approximation
with/without stratifying,” CoRR, vol. abs/1306.4265, 2013.
[24] W. Hoeffding, “Probability inequalities for sums of bounded random
variables,” Journal of the American Statistical Association, vol. 58, no.
301, pp. 13–30, March 1963. [Online]. Available: http://www.jstor.org/
stable/2282952?
[25] N. Li, W. H. Qardaji, and D. Su, “Provably private data anonymization:
Or, k-anonymity meets differential privacy,” CoRR, vol. abs/1101.2604,
2011. [Online]. Available: http://arxiv.org/abs/1101.2604
[26] Z. Jelveh and M. Luca, “Towards diagnosing accuracy loss in