1x
1x
1x
2x
–
NE
–
NE
NE,15169:13000
NE
>0
>0
100
100
100
>0
IPv6, we observed few queries over TCP between Jan. 1 and 9, so they are not
representative. After depolarizing, we see more queries over TCP.
Although overall latency improves, omitting the AMS site misses the oppor-
tunity to provide better latency to their data centers in the Netherlands and
Denmark. We therefore resumed peering over the BGP session, experimenting
with several policy routing choices shown in Table 6. We experimented with 1x
and 2x AS-PATH prepending, no-export, and a Google-speciﬁc “try-not-to-use
this path” community string [18]. We found that no-export and the commu-
nity string had no eﬀect, perhaps because of the BGP session, and neither did
single prepending. However, double AS-PATH prepending left AMS with about
10% of the total traﬃc load. Full details of our experiments are in our technical
report [39].
Depolarizing Microsoft to .nl Anycast A. Detection: We discovered
Microsoft anycast polarization through analysis of DNS/TCP across ASes (Fig.
6b and Fig. 6d) AS8075 (Microsoft) and AS15169 (Google) Microsoft’s preferred
site for .nl Anycast A is Miami (MIA), a diﬀerent preference than Google’s, but
the outcome was the same: large latency (median 80 ms) because global traﬃc
goes to one place.
Resolution: Again, we worked with the operators at .nl Anycast A MIA and
Microsoft to diagnose and resolve the problem. We conﬁrm that Anycast had
Fig. 12. Google depolarization results and RTT.
Old but Gold: Prospecting TCP to Engineer
281
a peering session with Microsoft in MIA, and not at any other sites. Again,
the result was a short AS-PATH and a preference for all Microsoft data centers
to use the Microsoft WAN to this site rather than other .nl Anycast A anycast
sites (having diﬀerent upstream providers per anycast site may cause such traﬃc
distributions [34]).
Options that could mitigate this polarization include de-peering with
Microsoft in MIA, peering with Microsoft at the remaining sites, or possibly
BGP-based traﬃc engineering. Because our ability to experiment with BGP was
more limited at this site, and we could not start new peerings at other sites, the
operator at MIA de-peered with Microsoft at our recommendation.
Figure 10 shows latency for this AS before and after our solution. Removing
the direct peering addressed the problem, and Microsoft traﬃc is now distributed
across all .nl Anycast A sites. As a result, the IQR falls from about 80 ms to
13 ms. The median latency also falls by 70 ms, from 90 ms to 20 ms. Our technique
identiﬁes problems with polarization and shows the dramatic improvement that
results.
4.4 Detecting BGP Misconﬁguration in Near Real-Time
Because it poses no additional cost on the network, passive measurement of
anycast latency with DNS/TCP is an ideal method for continuous, on-the-ﬂy
detection of BGP misconﬁguration. To this end, we have developed and deployed
Anteater within .nl, which is a live monitoring system that retrieves DNS/TCP
RTT continuously.
We show the Anteater architecture in Fig. 13. First, traﬃc is collected at
authoritative DNS servers of .nl, which is then exported to ENTRADA [61,70],
an open-source DNS traﬃc streaming warehouse that employs Hadoop [60] and
continuously ingests pcap ﬁles from these servers. (We also use ENTRADA for
other applications as well [36,67]). ENTRADA that extracts RTT for incoming TCP
handshakes, making it available for queries using Impala [26], an open-source
SQL engine for Hadoop. With Hadoop, ENTRADA supports scalable analysis of
large traﬃc.
Fig. 13. Anteater monitoring at .nl
Anteater then retrieves DNS/TCP RTT data for each anycast server, anycast
site, and various ASes, on an hourly basis (given that .nl authoritative servers
have good temporal coverage in 1h frames Sect. 2.1). It stores the information
282
G. C. M. Moura et al.
Fig. 14. Anycast B SYD site:
latency.
IPv4
Fig. 15. Anycast B SYD: queries rate
and resolvers.
on a relational database (PostgreSQL). We then use Grafana [19], a data visu-
alization dashboard and alert system. We then conﬁgure RTT thresholds that
triggers Grafana to send .nl operators email alerts. Anteater has been used in
.nl for the past two years and its proven helpful in detecting BGP misconﬁgu-
rations. We have released Anteater at [37]. Next we illustrate this use-case with
one example from that deployment.
EU Traﬃc Winding up in Australia: On 2020-04-08, .nl operators received an
alert from Anteater that detected a jump in median DNS RTT for Anycast B,
from 55 ms to more than 200 ms (see Fig. 14) but only for IPv4 traﬃc, and not
for IPv6.
To investigate this change, we evaluated the number of ASes (Fig. 14),
resolvers, and query rates (Fig. 15) using Anteater’s Grafana dashboard. We
see that the number of resolvers, queries, and latency grow, with many more
ASes, and about 3× more queries and resolvers. To rule out DDoS attacks or a
sudden burst in popularity for our domain, we conﬁrmed that these ASes and
resolvers have migrated from other sites (mostly Germany, site FRA) and went
to SYD. Since many of these clients are in Europe, this nearly antipodal detour
explains the latency increase.
We reached out to the operator of .nl Anycast B SYD. They conﬁrmed
and were already aware of the routing change. They informed us that a set of
their SYD preﬁxes had accidentally to propagated through a large, Tier-1 transit
provider. Since this provider peered with many other ASes in many places around
the globe, their propagation of the Anycast B anycast preﬁx provided a shorter
AS-Path and sent traﬃc to SYD. We also conﬁrmed these routing changes on
the RIPE RIS database of routing changes [54].
While catchment changes are not bad, route leaks that mis-route Europe to
Australia are not an improvement. The lightweight nature of DNS/TCP observa-
tions of latency support 24× 7 monitoring and allowed us to detect this problem,
which is why we developed Anteater to monitor the .nl operations.
5 Anycast Latency and Traﬃc
While DNS/TCP can be used to discover anycast latency, does latency matter?
DNS caching means users are largely insulated from latency. We next conﬁrm
Old but Gold: Prospecting TCP to Engineer
283
Fig. 16. Fraction of traﬃc going to each root anycast service, per day, from RSSAC-002
data. B- and H-Root are bold lines.
that latency does inﬂuence traﬃc to services when users have the choice of
several. A causal relationship between client selection and latency was previously
shown experimentally [43] and through code analysis [71], and our operational
measurements adds operational measurement conﬁrmation to these results.
Prior work has considered recursive resolver preference for lower latency [44].
Here we turn that analysis around and explore how changing anycast infrastruc-
ture shifts a client’s preferences towards authoritative name servers. We conﬁrm
that lower latency results in increased traﬃc from recursive resolvers that have a
choice between multiple anycast service addresses providing the same zone. (This
question diﬀers from studies that examine the optimality of a speciﬁc anycast
service with multiple sites [28,29].)
To examine this question, we use public RSSAC-002 statistics for the root
server system [56]. From this we use the “traﬃc-volume” statistic, which reports
queries per day for each root anycast service. (Recall that the Root DNS is
provided by 13 diﬀerent anycast service addresses per IP version, each using a
diﬀerent anycast infrastructure.) We show 6 months of data here (2019-11-01 to
2020-05-31), but we have noticed similar trends since 2016. This analysis omits
G- and I-Root, which did not provide data during this period.
Figure 16 shows the fraction of traﬃc that goes to each anycast service in the
root server system for one year. Two root letters deployed additional sites over
this period: B-Root originally had 2 sites but added 3 sites in 2020-02-01, then
optimized routing around 2020-04-01. H-Root originally had 2 sites but deployed
4 additional sites on 2020-02-11 and 3 additional sites on 2020-04-06. While other
letters also added sites, B and H’s changes were the largest improvements relative
to their prior size. We see that B and H’s share rises from about 4% in 2019-11
to about 6% in 2020-05.
This data conﬁrms that when new sites are created at a root letter, they oﬀer
some clients lower latency for that letter. Lower latency causes some clients to
shift more of their traﬃc to this letter (automatically, as described in [43]), so
its share of traﬃc relative to the others grows.
284
G. C. M. Moura et al.
6 Related Work
Passive TCP Evaluation: Janey Hoe was the ﬁrst to extract RTT from the
TCP handshake [20], and several groups have used it since then (e.g.,, Facebook
HTTP traﬃc [57]). We use this old idea, but we apply it to DNS RTT estimation
and to use to engineer and monitor Anycast DNS services in near real-time. In a
non-peer-reviewed work performed previously but independently from our own,
.cz operators [31,32] also employed DNS/TCP RTT to evaluate latency from
their services. While both use the same idea (derive latencies from the TCP
handshake), ours provides a comprehensive validation (Sect. 2). We also act on
the results, by carefully manipulating BGP to solve the identiﬁed problems,
and reduce latency in up to 90% (Sect. 4). Besides, our work includes freely
three tools: dnsanon, Anteater, and a modiﬁed version of KnotDNS. Linux ss
and ip utilities [30] can be also used to retrieve TCP information such as RTT.
However, they only provide averages. Although TCP congestion control may
interact with latency, since DNS/TCP is usually short (a single query and reply),
such interactions will be rare.
Anycast DNS Performance: Having a single upstream provider has been pre-
viously proposed as a solution to avoid routing unexpected behavior [4]. Later
research evaluated the impact of number of sites and anycast performance, show-
ing that, counterintuitively, sometimes more sites actually increase latency [59].
The behavior of anycast under DDoS has been examined [42], using data from
the 2015 attacks against the Root DNS servers [55]. Our discovery of polariza-
tion in Google has been showin in subsequent testbed experiments [64]. We had
already shared results of polarization for multiple hypergiants [39], and are the
ﬁrst to quantify the performance and show the beneﬁts of BGP-based ﬁxes.
There is one approach to measure anycast latency today: active measure-
ments. RIPE Atlas [52] measures latency from about 11k physical devices dis-
tributed worldwide. Commercial services are known to have fewer vantage points.
Our approach instead uses passive analysis of TCP traﬃc from real clients. It
provides far better coverage than RIPE Atlas (Sect. 2.1). We expect that Verf-
ploeter will soon support RTT measurements. Even when it does support RTT
measurements, our approach can provide coverage for all networks interacting
with the service. In addition, since our analysis is passive, it places no additional
strain on other networks and can run 24× 7. Last, a previous work proposed
using new BGP communities to improve the site catchment, which, in turn,
requires protocol level changes [28]. Contrary to their approach, ours relies only
on passive TCP traﬃc and does not involve protocol changes.
Anycast Optimization for Large CDNs with Multiple Providers: Going beyond
how many sites and where to place them, McQuistin et al. [34] have investi-
gated anycast networks with multiple upstream providers, as is common for large
CDNs. When diﬀerent sites have diﬀerent peers or transits catchment inconsis-
tencies can result, as we saw with Google and Anycast A (Sect. 4.3). They
propose taking active measurements of catchments each day and operator eval-
uation of catchment changes Our work also detects catchment changes, but only
Old but Gold: Prospecting TCP to Engineer
285
when it aﬀects latency (see Sect. 4.3 and Sect. 4.3), fortunately when changes
matte.r Schlinker et al. [57] describe how Facebook monitors their CDN for web
content, detecting anycast latency problems for their users. Our work instead
focuses shows how TCP results can summarize latency for a mostly UDP-based
workload, and studies authoritative DNS traﬃc, from recursive resolvers (not
end-users).
Performance-Aware Routing: Todd et al. [3] compare data from proposals for
performance-aware routing from three content/cloud providers (Google, Face-
book, and Microsoft) and show that BGP fares quite well for most cases. Others
proposed to perform traﬃc engineering based on packet loss, latency and jit-
ter [45,49].
DNS over TCP, TLS, and HTTP: There is recent interest in DNS over TCP
and TLS [23,72] and HTTP [21] to improve privacy. Most such work emphasizes
stub-to-recursives resolvers, while we focus on recursive-to-authoritative, where
only now IETF is considering alternatives to UDP. Increased use of DNS over
connection-oriented transport protocols will improve coverage we can provide.
7 Conclusions
We have shown that DNS TCP connections are a valuable source of latency infor-
mation about anycast services for DNS. Although TCP is not (today) the dom-
inant transport protocol for DNS, we showed that there is enough DNS/TCP to
provide good coverage for latency estimation. We also showed how we prioritize
the use of this information to identify problems in operational anycast networks.
We have used this approach to study three operational anycast services: two any-
cast servers of .nl, and one root DNS server (B-root). We documented one new
class of latency problems: anycast polarization, an interaction where hypergiants
get pessimal latency (100–200 ms) because of a poor interaction between their
corporate backbones and global anycast services. We showed how we addressed
this problem for .nl’s Anycast A with both Google and Microsoft. We also
documented several other problems for anycast latency discovered through our
analysis of DNS/TCP and showed that it enables continuous monitoring. Last,
we release freely two tools (dnsanon and Anteater) and a modiﬁed version of
KnotDNS. We believe this approach will be of use to other DNS operators.
Acknowledgments. We thank the operators of .nl Anycast A and B and B-root for
their time and collaboration. We also thank Casey Deccio for ﬁrst proposing using TCP
hanshake to measure DNS latency. Finally, we thank Klaus Darilion and our paper’s
anonymous reviewers for their paper suggestions.
John Heidemann’s research in this paper is supported in part by the DHS HSARPA
Cyber Security Division via contract number HSHQDC-17-R-B0004-TTA.02-0006-I
(PAADDOS) and by NWO. His and Wes Haradaker’s research are also supported
in part by NSF CNS-1925737 (DIINER), Giovane C. M. Moura, Joao Ceron, Jeroen
Bulten, and Cristian Hesselman research in this paper is supported by the Conconrdia
Project, an European Union’s Horizon 2020 Research and Innovation program under
Grant Agreement No 830927.
286
G. C. M. Moura et al.
A Extra Graphs on Temporal Coverage
Figure 17 show the temporal coverage of Anycast B for .nl. Figure 18 shows
temporal coverage for 2019-10-21. Together these graphs show that a core of
many ASes are covered by TCP at all times of day.
Fig. 17. .nl temporal coverage for Anycast B
Fig. 18. .nl temporal coverage for Anycast A and B on 2019-10-21
B Anycast Extra Data
Figure 19 shows the the latency for the top 10 ASes of Anycast B.
Figure 20 shows the latency for B-root top talkers by data size (log scale).
Old but Gold: Prospecting TCP to Engineer
287
Fig. 19. .nl Anycast B query RTT for the top 10 ASes ranked by most queries (bars
left axis). Data: 2019-10-15 to -22.
Fig. 20. B-root latency of top talkers by data size (log scale)
C Anycast A and B Top ASes
Table 7 shows ASes names and countries for top ASes observed for Anycast
A and Anycast B, as shown in Fig. 6 and discussed in priorization (Sect. 3)
(Fig. 21).
Fig. 21. Latency analysis to B-root by AS latency diversity.
288
G. C. M. Moura et al.
Table 7. ASes in the top 10 lists of Anycast A and B
AS number AS name
Country
42
1103