1.59e-06
3.46e-04
1.6e-04
p-value
8.1e-04
1.04e-06
2e-16
2.56e-07
Signif.
***
***
***
***
Table 8: Signiﬁcance of domain reputation features.
4.2.5 Combining Relevant Features
We combine in our ﬁnal logistic regression model all features
found signiﬁcant in the above analyses, listed in Table 9. We also
ran a χ2 goodness-of-ﬁt test to test the hypothesis that the ﬁnal
model ﬁts the training data set, and obtained a very high p-value
(close to 1), implying that the null hypothesis can not be rejected.
This ﬁnding gives us conﬁdence that the model is a good ﬁt to the
features modeling user demographics and behavior, and we present
our evaluation results on the predictive power of the model next.
Web
VPN
Category
Feature
Demographic Gender
Country
Level
Technical
VPN_conn
VPN_dur
VPN_sbytes
VPN_extip
Chat
File transfer
Freeware
Games
Social networks
Streaming
Non-Categorized
No_doms
Blocked
Challenged
Consented
New_domains
Description
Gender of user
Country of user’s ofﬁce
Level in management hierarchy
Technical level
Total no. VPN connections
Duration of VPN connections
Bytes sent in VPN connections
No. external IPs connecting to VPN
No. chat sites visited
No. ﬁle-transfers sites visited
No. freeware sites visited
No. gaming/gambling sites visited
No. social-networking sites visited
No. streaming sites visited
No. non-categorized sites visited
No. distinct domains visited
No. connections blocked by proxy
No. connections challenged by proxy
No. connections consented by proxy
No. new domains visited
Table 9: Features included in ﬁnal logistic regression model.
11254.3 Evaluation
To evaluate the ﬁnal model, we split the set of all hosts randomly
into equal-sized sets, training and testing. We estimate the parame-
ters of the logistic regression model with the training set, and com-
pute the risk scores of hosts in the testing set. Figure 10 shows the
cumulative distribution (CDF) of scores for hosts in the testing set.
It is clear that the CDF for malware-encountering hosts is distinct
from that for “clean” hosts, with the former having (on average)
higher scores than the latter.
Figure 10: Cumulative distribution of scores for hosts that encoun-
tered malware, and those that did not.
This suggests that the user demographic and behavior features
can be used to infer the likelihood of malware encounters. As an
example application of this result, hosts can be ranked according to
their risk score, under a model tuned to the particular organization.
Proactive measures can then be applied to hosts with the highest
scores so as to detect and remediate potential malware infections.
How well will such a prioritization approach work? To answer
this question, we ordered the hosts in the testing set based on the
risk score output by the model, and computed the malware en-
counter rate for the top n hosts. Figure 11 shows our results, aver-
aged over ten independent runs, where each run splits the hosts into
training and testing datasets, and builds the model on the training
set while computing risk scores for hosts in the testing set. Re-
sults for models built with all features and those with each feature
category are shown as separate lines in Figure 11.
Figure 11: Ordering hosts by their risk score. The malware en-
counter rate decreases with the hosts’ score ranking. Among the
top 1,000 hosts, the malware encounter rate is 51% (well more than
3× higher than the overall encounter rate of 15%).
With all 20 features combined, the malware encounter rate is
51% among the top 1,000 hosts—well more than 3× higher than
the overall malware encounter rate in the enterprise (15.31%), as
described in Section 3. Among the three feature categories, user
demographics is the most powerful at indicating risk, followed
by VPN behavior. Counter-intuitively, web activity contributes
marginally to the overall model. One explanation for this surpris-
ing result is that, in our study, only 3.11% of the hosts encountered
malware from the web (see Section 3.1), and among those, we have
visibility into only a small fraction that happened inside the corpo-
rate network.
5. PREVIOUS STUDIES
Several prior works have studied the relationship between the
likelihood of malware encounters and users’ online behavior or de-
mographic information. The datasets used in previous works vary,
as well as the methods by which they identify malware encounters.
In this section, we compare our results to those reported in prior
studies where possible, and highlight instances in which our ﬁnd-
ings corroborate or refute theirs. We also summarize some more
distantly related works in Appendix C.
Malware encounter rate: The 2013 Microsoft Security Intelli-
gence Report [12], involving over 600 million hosts installed with
Microsoft security products in the ﬁrst half of 2013, reported a
worldwide malware encounter rate of around 18%. Our enterprise
dataset, consisting of 62,884 hosts instrumented with the McAfee
client collected over four months, has a similar encounter rate of
15.31%. On average, our encounter rate during any week is 1.26%.
Other smaller datasets consisting of network packet captures ex-
hibit a similar encounter rate to ours. Maier et al. [10] found, over
a period of 14 days, that 1.23% of the users subscribed to an Eu-
ropean ISP exhibited scanning or spamming behavior or contacted
known malware sites. Carlinet et al. [4] observed 3.04% of the cus-
tomers of the Orange ISP in France generating trafﬁc that triggered
Snort alerts during the course of three hours.
User studies that surveyed or observed users showed a much
higher malware encounter rate. Among 295 university students
surveyed by Ngo and Paternoster [14], 46% reported encountering
malware at least once in the last year, while 38% of 50 participants
in a study by Lévesque et al. [9] were found to be infected over the
course of four months.
The lower encounter rate in our dataset compared to prior works
may be due to the network policies enforced in the enterprise.
A whitelist and blacklist are applied to outbound network con-
nections, and employees are not allowed by default to install
software on enterprise-managed hosts.
It is also possible that
Lévesque et al. and Ngo and Paternoster observe higher encounter
rates due to the populations they study (primarily students).
User Demographics: Ngo and Paternoster found that both age and
race are signiﬁcantly related to the likelihood of encountering mal-
ware, but not gender, marital status, or employment status. While
agreeing that those factors do not contribute to the risk of infection,
Lévesque et al. also found age to be irrelevant. The only signiﬁcant
factor they identiﬁed is technical expertise.
Our employee dataset does not include personal information
(age, gender, etc.), and obviously the users in our data were em-
ployed. That said, we did corroborate the observation that technical
expertise correlates with the likelihood of encountering malware.
User Behavior: Both Maier et al. and Ngo and Paternoster exam-
ined the relationship between the use of security products and mal-
ware encounters. The former found that neither the installation of
anti-virus scanners nor regular O/S and blacklist updates reduced
malicious activities. However, the latter found that having secu-
rity software led to a higher probability of encountering malware
(perhaps an artifact of the user study, as users would otherwise not
know that they were infected). The nature of our dataset prevents
us from observing hosts with varying software conﬁgurations, since
enterprise hosts are centrally conﬁgured and managed in our case.
One might expect that more exposure to the Internet would re-
sult in higher likelihood of malware encounters. This was the case
0.00.20.40.60.81.0FractionofHosts0.00.20.40.60.81.0Score(CDF)EncounteredMalwareNotEncounteredMalware10020004000600080001000012000IndexofHostsOrderedbyRiskScore0.10.20.30.40.5EncounterRateAllfeaturesDemographicVPNWeb1126in the studies of Carlinet et al. and Lévesque et al., as well as
of Canali et al. [3] in terms of trafﬁc volume and the number of
unique websites visited. It also holds true in our dataset. In ad-
dition, we ﬁnd some categories of websites to be correlated with
higher malware encounter rates than others, as did Carlinet et al.
and Lévesque et al., although the categories do not always agree.
Canali et al., on the other hand, use logistic regression to show
that browsing time, duration and number of distinct domains are
better indicators of a user being at risk than the domain category.
However, their deﬁnition of at risk is laxer and does not rely on
documented malware encounters.
6. LIMITATIONS
By the nature of the data available to us, our conclusions are
subject to a number of caveats. Perhaps most importantly, since
we relied on McAfee reports to indicate malware encounters, our
results do not reﬂect any potential encounters detected and elim-
inated prior to reaching McAfee analysis—e.g., by ﬁrewalls/IPS,
web browsers, or any of the other myriad security defenses com-
monly employed in IT infrastructure or leveraged by this enterprise
in particular—or that reached McAfee but evaded it.
It is well
known that no anti-virus solution detects all malware; at best, it
identiﬁes a large fraction of “mass-market” malware. As such, we
assume that our encounters are biased toward mass-market mal-
ware that entered the enterprise via poorly defended vectors.
Another caveat related to our data is that we do not have ground
truth for many aspects of our investigation, requiring us to lever-
age indirect indicators, instead. So, for example, we used McAfee
reporting delays to infer whether each malware encounter occurred
while the computer was on the corporate network (Section 3.2), and
we leveraged job titles as a surrogate for an objective measure of
technical proﬁciency (Section 3.3). It is important to bear in mind
that all such inferences come with a level of error to them, though
our belief is that the relatively large amount of data in our study
provides some statistical evidence for the correlations that we ob-
serve.
Unknowns will be a factor in virtually any study of this type,
introducing questions about the extent to which any single study—
including this one—will be representative more broadly. We be-
lieve that it is necessary to assemble a broad set of such studies to
reveal their common elements and differences. (We have attempted
to draw out such similarities and differences with other studies in
Section 5.) Only through repetition can lasting trends be identiﬁed.
7. RECOMMENDATIONS FOR THE EN-
TERPRISE
Our study suggests recommendations for reducing the rate of
malware encounters in enterprise settings:
Proactive scanning, alert prioritization. Enterprises often deploy
memory analysis tools on hosts. As use of these tools is labor-
intensive, they must be deployed selectively. Our logistic regres-
sion model identiﬁes hosts with a highly elevated risk of malware
encounters (51% among the top 1,000 such hosts). This allows
an enterprise to apply memory-scanning tools proactively, facili-
tating early detection of malware infection. Similarly, an enterprise
can prioritize investigation of alerts (e.g., generated by log-analysis
tools) based on modeled host risk.
User education. User education has the potential to reduce behav-
iors responsible for a signiﬁcant fraction of malware encounters.
A number of studies have afﬁrmed the successes of carefully tar-
geted educational campaigns, e.g., [7]. Our study highlights several
opportunities for such targeting, e.g., educating users in low GDP
countries to avoid the use of USB sticks with company laptops.
Caution with site categories. In the studied enterprise, most web-
based malware encounters permitted by the web proxy originate
from sites categorized as business-appropriate under enterprise pol-
icy. This suggests that currently deployed website content cate-
gories are an inadequate basis for access restrictions. Tightening
access policies by, e.g., reﬁning website categories and/or applying
per-user (or per-group) rules, may reduce web-based encounters.
Understanding the practical implications and effectiveness of
any recommendation would require in-ﬁeld studies, and likely dif-
fer across enterprises based on their policies and compliance re-
quirements. We hope our study will motivate future research in
these areas.
8. CONCLUSION
We have presented the ﬁrst large-scale epidemiological study of
malware encounters in the setting of a large enterprise. Our study
offers several key ﬁndings, including the preponderance of mal-
ware encounters outside the enterprise network, differences across
geographies in the most important vectors of malware propagation,
differences in encounter rates according to employee position in
the management hierarchy, and a signiﬁcant risk of web-based mal-
ware encounters that originated with sites categorized as safe and
business-appropriate by the enterprise web proxy. While our study
corroborates some ﬁndings in earlier research, it also sheds new
light on the special characteristics of enterprise malware penetra-
tion and shows how these characteristics can be combined in a lo-
gistic regression model to achieve accurate identiﬁcation of at-risk
hosts. Finally, our study suggests promising, concrete policy- and
education-based approaches to driving down malware encounter
rates in enterprise environments.
Acknowledgments
We are grateful to the anonymous enterprise who permitted us ac-
cess to their data for the purposes of this study. We are also grate-
ful to James Lugabihl, Robin Norris, Todd Leetham, Christopher
Harrington, Garrett Schubert, Justin Lamarre, Andrew Rutkiewicz,
Michael Blanchard, Ronald L. Rivest and members of RSA Labo-
ratories for their many useful comments and suggestions on design-
ing the experimental framework in the paper. We thank our shep-
herd Davide Balzarotti and anonymous reviewers for their feedback
on drafts of this paper. This work was supported in part by NSF
grant 0831245.
9. REFERENCES
[1] “epidemiology”. In Merriam-Webster.com, 15 May 2014.
[2] J. Caballero, C. Grier, C. Kreibich, and V. Paxson.
Measuring pay-per-install: The commoditization of malware
distribution. In 20th USENIX Security Symposium, Aug.
2011.
[3] D. Canali, L. Bilge, and D. Balzarotti. On the effectiveness
of risk prediction based on users browsing behavior. In 9th
ACM Symposium on Information, Computer and
Commmunications Security, June 2014.
[4] Y. Carlinet, L. Mé, H. Debar, and Y. Gourhant. Analysis of
computer infection risk factors based on customer network
usage. In 2nd International Conference on Emerging
Security Information, Systems and Technologies, pages
317–325, Aug. 2008.
1127[5] M. P. Collins, T. J. Shimeall, S. Faber, J. Janies, R. Weaver,
M. De Shon, and J. B. Kadane. Using uncleanliness to
predict future botnet addresses. In 7th ACM Internet
Measurement Conference, pages 93–104, Oct. 2007.
[6] A. Kleiner, P. Nicholas, and K. Sullivan. Linking
Cybersecurity Policy and Performance. Microsoft
Trustworthy Computing, 2013.
[7] M. W. Kreuter and R. J. Wray. Tailored and targeted health
communication: Strategies for enhancing information
relevance. American Journal of Health Behavior,