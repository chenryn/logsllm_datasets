size is W; (2) consequently, the window size is halved in the next 
RTT eliminating the congestion; and (3) in response, the window 
size is increased by one segment every RTT until the window size 
reaches W again at which point full-on congestion occurs again. 
The resultant sawtooth wave has a time period L. 
that a reasonable approximation suffices for achieving successful 
deadline-aware congestion avoidance. In Figure 4(a), the average 
While 𝑇𝑐 can be computed using a precise analysis, we found 
window size over the duration of 𝑇𝑐 is 34𝑊. Therefore, 𝑇𝑐 can be 
𝑻𝒄=𝑩/(𝟑𝟒𝑾) 
4(b), we still use this approximation for 𝑇𝑐 in all our evaluations. 
𝐵=𝑊2+𝑊2+1+𝑊2+2 +⋯𝑊2+𝐿−1∗𝑇𝑐𝐿   
Solving for 𝑇𝑐, we get: 𝑇𝑐= 𝐵3𝑊4−12              if 𝑇𝑐>𝐿 
𝐵=𝑊2+𝑊2+1+𝑊2+2+⋯+𝑊2+𝑇𝑐−1  
Solving for 𝑇𝑐, we get: 
   𝑇𝑐= −𝑊−12 +1/4(𝑊−1)2+2𝐵           if 𝑇𝑐<𝐿 
We  found  that  D2TCP  performs  similarly  with  precise  and 
approximate  expressions.  Therefore,  we  do  not  present  any 
evaluations using the precise expressions. 
3.2.4  Stability and convergence 
Though the average window size is different in the case of Figure 
A precise analysis of Figure 4(a) yields the following expression: 
Similarly, an analysis of Figure 4(b) yields: 
As  stated  above,  our  computation  of  d  makes  some 
approximations.  Fortunately,  D2TCP  has  some  in-built  self-
correction.  If  the  algorithm  under-estimates  a  flow’s  d,  the  flow 
may back off too much. However, in subsequent RTTs, the flow’s 
D will decrease more than a commensurate decrease in its B. As a 
result d will increase, causing the flow to increase its transmission 
rate.  The  same  is  true  for  over-estimations  of  d.  The  flow  will 
transmit  faster  than  needed,  and  its  B  will  decrease  faster  than 
needed. Consequently, subsequent RTTs will yield a lower d, thus 
correcting  the  over-estimation.  Furthermore,  if  the  aggressive 
transmission leads to severe congestion, then the natural response 
of the gamma function will dial up the value of p and throttle the 
flow as well. 
s
k
n
i
l
s
p
b
G
0
1
Rack Switch
machine
machine
machine
machine
machine
i
s
e
n
h
c
a
m
6
1
Figure 5: Real implementation testbed 
the  network 
inadvertently  driving 
and retransmission and timeout when there is a packet loss. 
We do not change other aspects of TCP, such as slow start, 
Note  that  when  flows  do  not  have  a  deadline  (e.g., 
in turn, determines the window size (hence sending rate). Because 
Another issue to consider with deadline-aware scheduling is 
the  possibility  of 
into 
congestive  collapse.  Imagine  if  all  flows  were  to  demand 
unreasonably  tight  deadlines,  then  their  resulting  congestion 
avoidance behavior would be to refuse downsizing their windows, 
effectively overloading the network. However, D2TCP’s gamma-
correction function guards against such overload at two levels: (1) 
Therefore, D2TCP’s worst case stability is similar to that of TCP.  
3.2.5  D2TCP summary 
near-deadline flows can send. Therefore, deadlines that are tighter 
beyond a certain limit get rounded down so applications will not 
drive  the  network  into  congestive  collapse.  (2)  At  extreme 
The effect of tighter deadlines is captured by the value of 𝑑 which, 
we cap the maximum value of 𝑑 at 2.0, we limit how aggressively 
congestion,  as 𝛼  and 𝑝  approach 1,  D2TCP  defaults  to  TCP. 
segment; otherwise, the sender computes 𝛼, 𝑇𝑐, and then d. The 
sender  then  uses  the  gamma-correction  function  to  obtain 𝑝  and 
resizes the congestion window based on 𝑝. 
background  flows)  we  use 𝑑=1  so  that  D2TCP  defaults  to 
The D2TCP algorithm is as follows: A sender that does not 
encounter  CE-marked  packets  increases  its  window  size  by  one 
DCTCP.  The  stability  of  DCTCP  for  long  flows  is  examined  in 
[1]. 
4.  EVALUATION 
We  evaluate  D2TCP  using  both  simulations  and  a  real 
implementation.  We  use  the  real  implementation  to  run  a  set  of 
microbenchmarks  on  a  small  testbed,  and  to  validate  our 
simulator.  We  rely  on  simulations  to  evaluate  production-like 
workloads  at  the  scale  of  thousands  of  servers.  Furthermore, 
because  D3 
limited  our 
comparisons against D3 to simulations. 
4.1  Small-scale Real Implementation 
real 
implementation  of  DCTCP  and  D2TCP.  We  use  a  set  of 
microbenchmarks that examine the basic functionality of D2TCP 
as a deadline-aware network protocol.  
4.1.1  Implementation methodology 
We  started  with  the  publicly  available  DCTCP  source  code 
from  [4].  We  then  built  our  D2TCP  implementation  on  top  of 
DCTCP, which amounted to around 100 additional lines of code. 
We  instrumented  D2TCP  to  operate  over  only  a  select  range  of 
TCP ports, thus allowing us to use the same kernel for different 
protocols.  We  deployed  this  implementation  on  the  testbed 
depicted in Figure 5. The testbed consists of a Top-of-Rack switch 
first  present  evaluations  on  a  small-scale 
requires  custom  hardware,  we 
We 
Flow-0 
Flow-1 
Flow-2 
Flow3 
)
s
p
b
G
(
h
t
i
d
w
d
n
a
B
2.0 
1.5 
1.0 
0.5 
0.0 
0
0
2
0
0
6
0
0
0
1
0
0
4
1
0
0
8
1
0
0
2
2
0
0
6
2
0
0
0
3
0
0
4
3
2.0 
1.5 
1.0 
0.5 
)
s
p
b
G
(
h
t
i
d
w
d
n
a
B
0.0 
0
0
2
0
0
6
0
0
0
1
0
0
4
1
0
0
8
1
0
0
2
2
0
0
6
2
0
0
0
3
0
0
4
3
0
0
8
3
Time (ms) 
Figure 6: Throughput for DCTCP (left) vs. D2TCP (right) 
Time (ms) 
connected  to  16  server  machines.  The  switch  is  based  on  a 
Broadcom  Scorpion  ASIC  with  24x  10Gbps  ports  and  a  4MB 
shared  packet  buffer,  and  the  servers  connect  to  the  switch  via 
10Gbps links. 
We set the key parameters of DCTCP and D2TCP to match 
K, the buffer occupancy threshold for marking CE-bits, is 20 for 
those in [1]: (1) 𝑔, the weighted averaging factor is 1/16; and (2) 
1Gbps links, and 65 for 10Gbps links. For D2TCP we cap 𝑑, the 
deadline imminence factor, to be between 0.5 and 2.0 (except in 
Section 4.2.4, where we explore the effects of varying this cap). 
We set RTOmin for all the protocols to be 20 ms. 
4.1.2  Deadline awareness 
We begin by examining the fundamental ability of D2TCP to 
schedule  the  network  in  a  deadline  aware  manner  via  deadline-
aware  congestion  avoidance.  In  this  experiment  we  have  four 
hosts transmit flows to a fifth “root” host. We choose flow sizes 
and deadlines to illustrate the impact of a deadline-aware protocol. 
We  set  the  flow  sizes  as  150,  220,  350  and  500  MB,  with 
respective deadlines of 1000, 1500, 2500 and 4000 ms. Note that 
the flow sizes and deadlines in this synthetic test are not intended 
to model a real workload. 
available 
bandwidth, 
far-deadline 
In Figure 6 we show the throughput achieved by the various 
flows  over  time,  for  both  DCTCP  and  D2TCP.  The  difference 
between DCTCP and D2TCP is most noteworthy in the 0-2200 ms 
timeframe.  As  expected,  DCTCP  grants  all 
flows  equal 
bandwidth,  and  consequently  the  near-deadline  flows  miss  their 
deadlines.  In  contrast,  D2TCP’s  deadline-aware  congestion 
avoidance allows the near deadline flows to take a larger share of 
the 
flows 
commensurately  relinquish  bandwidth.  Consequently,  the  flow 
times  for  flow  #0  and  #1  under  D2TCP  are 
completion 
significantly shorter than those under DCTCP. As the number of 
active  flows  decrease, 
the  opportunity  for  deadline-aware 
scheduling  among  them  also  decreases.  Consequently,  flow  #2 
and #3 have similar completion times under both schemes. These 
results establish the utility of a deadline aware network protocol. 
4.1.3  Mixing deadline and non-deadline traffic 
Recall  that  when  flows  have  no  deadlines  we  set 𝑑=1 
which  causes  D2TCP  to  behave  identical  to  DCTCP.  DCTCP’s 
stability and convergence for traffic patterns that consist entirely 
of such long non-deadline flows are examined in [1] in detail. We 
do not evaluate such traffic mixes in our paper, as the results for 
D2TCP would be identical to that for DCTCP. 
Instead, we examine traffic patterns that consist of a mix of 
deadline and non-deadline traffic. To that end, we set up a small-
scale  OLDI  application  with  one  root  and  the  number  of  leaves 
varying  between  20  and  40.  Because  the  testbed  has  only  16 
server  machines,  we  run  multiple  leaves  on  each  physical 
machine.  The  root  periodically  sends  a  query  to  all  the  leaves, 
which  in  turn,  idle  for  a  fixed  “computation  time”  and  then 
respond to the query. The replies from the leaves are sized 100-
and 
the 
DCTCP-Real 
D2-Real 
DCTCP-Sim  D2-Sim 
)
%
(
s
e
n
i
l
d
a
e
d
d
e
s
s
M
i
15 
10 
5 
0 
20 
)
s
p
b
M
(
/
w
b
w
o
l
f
g
n
o
L
600 
550 
500 
450 
400 
30 
40 
20 
30 
40 
Fan-in degree 
Fan-in degree 