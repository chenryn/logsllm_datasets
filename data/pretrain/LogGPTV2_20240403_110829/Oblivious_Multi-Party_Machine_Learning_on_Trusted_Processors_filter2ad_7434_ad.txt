in time O(cid:31)(M + ˜n)(log(M + ˜n))2 + T (M + ˜n)(log ˜n)2(cid:30)
where ˜n = max(n,m). It is data-oblivious, as there ex-
ists a simulator that depends only on T , M, n, m, and d
and produces the same trace.
Proof Outline. The Setup phase is the most expensive,
as it involves oblivious sorting on all the input ratings
at once, with a O((M + ˜n)(log(M + ˜n))2) run time. The
update phase runs in time O(M + n + m) since it requires
a single scan of U and V. The extraction phase similarly
runs in time O(M + n + m). The copying phase runs in
time O((M + m)(logn)2 + (M + n)(logm)2) due to (M +
m)/n sorts of U of size n and (M + n)/m sorts of V of
size m. Since all phases except Setup run T times, the
total run time is
O(cid:31)(M + ˜n)log2(M + ˜n) +T (M + ˜n)log2 ˜n(cid:30) .
A simulator can be built from the public parameters
mentioned in the beginning of the algorithm. It executes
every step of the algorithm:
it creates the interleaving
data structures that depend only on n, m, M and d and
updates them using the steps of the Setup once and runs
the Update, Extraction and Copy phases for T iterations.
As part of Setup, it invokes the simulator of the sequence
expansion algorithm described in the full version of the
paper.
4.7 Equally-Interleaved Expansion
We finally present our method for arranging tuples of U
and V in Matrix Factorization. We believe this method is
USENIX Association  
25th USENIX Security Symposium  627
applicable to other data processing scenarios. For exam-
ple, Arasu and Kaushik [4] use a similar, careful arrange-
ment of tuples to obliviously answer database queries.
Definition 1. A weighted list L is a sequence of pairs
(i,wi) with n elements i and integer weights wi ≥ 1.
An expansion I of L is a sequence of elements of length
∑n
1 wi such that every element i occurs exactly wi times.
Definition 2. Let α = ∑wi/n be the average weight of L
and the jth chunk of I be the sub-sequence of (cid:30)α(cid:29) ele-
ments I(cid:28)( j−1)α+1(cid:26), . . . ,I(cid:30) jα(cid:29).
I equally interleaves L when all its elements can be
collected by selecting one element from each chunk.
For example, for L = (a,4), (b,1), (c,1), every chunk
has α = 2 elements. The expansion I = a,b,a,c,a,a
equally interleaves L, as its elements a, b, and c can be
chosen from its third, first, and second chunks, respec-
tively. The expansion I(cid:25) = a,a,a,a,b,c does not.
We propose an efficient method for generating equal
interleavings. Since it is used as an oblivious building
block, we ensure that it accesses L, I and intermediate
data structures in a manner that depends only on n and
M = ∑wi, not on the individual weights. (In matrix fac-
torization, M is the total number of input ratings.) We
adopt the terminology of Arasu and Kaushik [4], even if
our definitions and algorithm are different. (In compari-
son, our expansions do not involve padding, as we do not
require that copies of the same element are adjacent).
Given a weighted list L, we say that element i is heavy
when wi ≥ α, and light otherwise. The main idea is to put
at most one light element in every chunk, filling the rest
with heavy elements. We proceed in two steps: (1) we re-
order L so that each heavy element is followed by light
elements that compensate for it; (2) we sequentially pro-
duce chunks containing copies of one or two elements.
Step 1: Assume L is sorted by decreasing weights
(wi ≥ wi+1 for i ∈ [1,n− 1]), and b is its last heavy el-
ement (wb ≥ α > wb+1). Let δi be the sum of differ-
ences defined as ∑ j∈[1,i](w j − α) for heavy elements and
∑ j∈[b+1,i](α − w j) for light elements. Let S be L (obliv-
iously) sorted by δ j, breaking ties in favor of higher el-
ement indices. This does not yet guarantee that light el-
ements appear after the heavy element they compensate
for. To this end, we scan S starting from its last element
(which is always the lightest), swapping any light ele-
ment followed by a heavy element (so that, eventually,
the first element is the heaviest).
Step 2: We produce I sequentially, using two loop
variables: k, the latest heavy element read so far; and w,
the remaining number of copies of k to place in I. We
repeatedly read an element from the re-ordered list and
produce a chunk of elements. For the first element k1,
we produce α copies of k1, and we set k = k1 and
w = wk1 − α. For each light element ki, we produce wki
copies of ki and α −wki copies of k, and we decrement w
by α − wki. For each heavy element ki, we produce w
copies of k and α − w copies of ki, and we set k = ki and
w = wki − (α − w).
Continuing with our example sequence L above, a is
heavy, b and c are light, and we have δa = 2, δb = 1, and
δc = 2. Sorting L by δ yields (b,1), (a,4), (c,1). Swap-
ping heavy and light elements yields (a,4), (b,1), (c,1)
and we produce the expansion I = a,a,b,a,c,a.
In the full version of the paper we prove that the al-
gorithm is oblivious, always succeeds and runs in time
O(n(logn)2 + ∑w).
5 Protocols
For completeness, we give an overview of the protocols
we use for running multi-party machine learning algo-
rithms in a cloud equipped with SGX processors. Our
protocols are standard, and similar to those used in prior
work for outsourcing computations [29,57]. For simplic-
ity, we describe protocols involving a single enclave.
We assume that each party agrees on the machine-
learning code, its public parameters, and the identities
of all other parties (based, for example, on their public
keys for signature). One of the parties sends this col-
lection of code and static data to the cloud data center,
where an (untrusted) code-loader allocates resources and
creates an enclave with that code and data.
Each party independently establishes a secure channel
with the enclave, authenticating themselves (e.g., using
signatures) and using remote attestation [2] to check the
integrity of the code and static data loaded into the en-
clave. They may independently interact with the cloud
provider to confirm that this SGX processor is part of that
data center. Each party securely uploads its private data
to the enclave, using for instance AES-GCM for con-
fidentiality and integrity protection. Each party uses a
separate, locally-generated secret key to encrypt its own
input data set, and uses its secure channel to share that
key with the enclave. The agreed-upon machine learning
code may also be optionally encrypted but we expect that
in the common case this code will be public.
After communicating with all parties, and getting the
keys for all the data sets, the enclave code runs the target
algorithm on the whole data set, and outputs a machine
learning model encrypted and integrity protected with a
fresh symmetric key. We note that denial-of-service at-
tacks are outside the threat model for this paper—the par-
ties or the data centre may cause the computation to fail
before completion. Conversely, any attempt to tamper
with the enclave memory (including its code and data)
would be caught as it is read by the SGX processor, and
hence the job completion guarantees the integrity of the
whole run. Finally, the system needs to guarantee that all
628  25th USENIX Security Symposium 
USENIX Association
10
parties get access to the output. To achieve this, the en-
clave sends the encrypted output to every party over their
secure authenticated channel, and waits for each of them
to acknowledge its receipt and integrity. It then publishes
the output key, sending it to all parties, as well as to any
reliable third-party (to ensure its fair availability).
6 Evaluation
This section describes our experiments to evaluate the
overhead of running our machine learning algorithms
with privacy guarantees. We ran oblivious and non-
oblivious versions of the algorithms that decrypt and pro-
cess the data inside SGX enclaves, using off-the-shelf
Intel Skylake processors. Our results show that, in all
cases, the overhead of encryption and SGX protection
was low. The oblivious version of algorithms with ir-
regular data structures, such as matrix factorization and
decision trees, adds substantial overhead, but we find that
it is still several orders of magnitude better than previous
work based on advanced cryptography.
6.1 Datasets
We use standard machine learning datasets from the UCI
Machine Learning Repository.4 We evaluate matrix fac-
torization on the MovieLens dataset [28]. Table 1 sum-
marizes our datasets and configuration parameters.
The Nursery dataset describes the outcomes of the
Slovenian nursery admission process for 12,960 appli-
cations in the 1980s. Given eight socio-economic at-
tributes about the child and parents, the task is to clas-
sify the record into one out of five possible classes. We
use the 0/1 encoding of the attributes as we evaluate the
records on binary decision trees. Hence, each record in
the dataset is represented using 27 features.
The MNIST dataset
is a set of 70,000 digitized
grayscale images of 28-by-28 pixels recording handwrit-
ten digits written by 500 different writers. The task is to
classify each image into one of ten possible classes.
The SUSY dataset comprises 5,000,000 instances pro-
duced by Monte Carlo simulations of particle physics
processes. The task is to classify, based on 18 observed
features, whether the particles originate from a process
producing supersymmetric (SUSY) particles or not.
The MovieLens datasets contain movie ratings (1–5):
100K ratings given by 943 users to 1682 movies.
The datasets were chosen either because they were
used in prior work on secure ML (e.g., Nursery in [13],
MovieLens in [47, 48]) or because they are one of the
largest in the UCI repository (e.g., SUSY), or because
they represent common benchmarks for particular algo-
rithms (e.g., MNIST for neural networks).
Our learning algorithms are iterative—the accuracy
(and execution time) of the model depends on the number
4https://archive.ics.uci.edu/ml/
of iterations. In our experiments, we fixed the number of
iterations a priori to a value that typically results in con-
vergence: 10 for k-means, 5 for neural network, 10 for
SVM, and 20 for matrix factorization.
6.2 Setup
The experiments were conducted on a single machine
with quad-core Intel Skylake processor, 8GB RAM, and
256GB solid state drive running Windows 10 enterprise.
This processor limits the amount of platform memory
that can be reserved for enclaves to 94MB (out of a to-
tal of 128MB of protected memory). Each benchmark
is compiled using the Microsoft C/C++ compiler ver-
sion 17.00 with the O2 flag (optimize for speed) and
linked against the Intel SGX SDK for Windows ver-
sion 1.1.30214.81. We encrypted and integrity protected
the input datasets with AES-GCM; we used a hardware-
accelerated implementation of AES-GCM based on the
Intel AES-NI instructions. We ran non-oblivious and
oblivious versions of our algorithms that decrypt and
process the binary data inside SGX enclaves. We com-
pare the run times with a baseline that processes the data
in plaintext and does not use SGX protection. Table 1
summarizes the relative run time for all the algorithms
(we report averages over five runs). Next we analyze the
results for each algorithm.
6.3 K-Means
We have implemented a streaming version of the k-
means clustering algorithm to overcome space con-
straints of enclaves. Our implementation partitions the
inputs into batches of a specified size, copies each batch
into enclave memory, decrypts it and processes each
point within the batch.
Table 1 shows the overheads for partitions of
size 1MB. The non-oblivious and oblivious versions
have overheads of 91% and 199% over baseline (6.8 sec-
onds). The overhead for the non-oblivious version is due
to the cost of copying encrypted data into the enclave and
decrypting it.
We observe similar overheads for longer executions.
The overheads decrease with the number of clusters
(34% with 30 clusters and 11% with 50 clusters for non-
oblivious version) and (154% for 30 clusters and 138%
for 50 clusters for oblivious version) as the cost of in-
put decryption is amortized over cluster computation.
By comparison, recent work [38] based on cryptographic
primitives reports 5 to 6 orders of magnitude slowdown
for k-means.
6.4 Neural Networks
We have implemented a streaming version of the algo-
rithm for training a convolution neural network (CNN)
on top of an existing library [20]. Table 1 shows the
overheads of training the network for the MNIST dataset.
USENIX Association  
25th USENIX Security Symposium  629
11
Algorithm SGX+enc.
K-Means
1.91
CNN
1.01
SVM
1.07
Matrix fact.
1.07
Decision trees
1.22
SGX+enc.+obl.
2.99
1.03
1.08
Dataset
MNIST
SUSY
115.00 MovieLens
Nursery
31.10
Parameters
Input size
# Instances
k=10, d=784
k=2, d=18
n=943, m=1,682
k=5, d=27
128MB
307MB
2MB
358KB
70K
2.25M
100K
6.4K
Table 1: Relative run times for all algorithms with SGX protection + encryption, and SGX protection + encryption + data oblivi-
ousness, compared with a baseline that processes the data in plaintext without SGX protection. Parameters of the datasets used for
each algorithm are provided on the right, where d is the number of features, k is the number of classes, n is the number of users
and m is the number of movies in the MovieLens dataset.
The low overheads (< 0.3%) reflect the observation that
the training algorithm is predominantly data oblivious,
hence running obliviously does not increase execution
time while achieving the same accuracy. We are aware
that state-of-the-art implementations use data-dependent
optimizations such as max pooling and adding noise;
finding oblivious algorithms that support these optimiza-
tions with good performance remains an open problem.
6.5 SVM
As described in Section 4.2, the correctness of supervised
learning methods requires that the input data instances be
independent and identically distributed. Our oblivious
SVM implementation achieves this by accessing a batch
of l data instances uniformly at random during each it-
eration. We implement random access by copying the
partition containing the instance into enclave memory,
decrypting the partition and then accessing the data in-
stance. In the experiments we set data partitions to be
of size 2KB and l = 20.
In addition, we use condi-
tional move instructions to make data accesses within the
training algorithm oblivious. These modifications allow
us to process datasets much larger than enclave mem-
ory. Our evaluation (Table 1) shows that random ac-
cess adds a 7% overhead to the non-oblivious SVM algo-
rithm, whereas the additional overhead of the oblivious
algorithm is marginal.
6.6 Decision Tree Evaluation
For the Nursery dataset, we use an offline-trained ensem-
ble of 32 sparse decision trees (182KB) with 295–367
nodes/leaves each and depths ranging from 14 to 16 lay-
ers. For this dataset, as shown in Table 1, our oblivi-
ous classifier running inside an enclave has an average
overhead of 31.1x over the baseline (255ms vs. 10ms).
The oblivious implementation of the algorithm em-
ploys the oget() primitive (see Section 3) for all data-
dependent array lookups. Without this optimization, us-
ing omoveEx() for the element-granular scanning of ar-
rays instead, the overhead is much higher (142.27x on
average). We observe that our oblivious implementation
scales well to even very large trees. For example, for an
ensemble of 32 decision trees (16,860KB) with 30,497–
32,663 nodes/leaves and 35–45 layers each,5 the average
overhead is 63.16x over the baseline.
In comparison, prior work based on homomorphic
encryption [13] uses much smaller decision trees (four
nodes on four layers for the Nursery dataset), has higher
overheads and communication costs, and scales poorly
with increasing depth. Our experiments show that