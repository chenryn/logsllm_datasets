### Three Versions of the Code

During encoding and reconstruction, Hitchhiker-XOR requires only XOR operations in addition to the operations of the underlying Reed-Solomon (RS) code. Hitchhiker-XOR+ offers more efficient reconstruction in terms of network and disk resources compared to Hitchhiker-XOR, while still using only XOR operations. However, Hitchhiker-XOR+ requires the underlying RS code to satisfy the all-XOR-parity property (§3.2). Hitchhiker-nonXOR provides the same efficiency in reconstruction as Hitchhiker-XOR+ without the all-XOR-parity requirement on the RS code, but it involves additional finite-field arithmetic during encoding and reconstruction.

### Connecting to More Machines During Reconstruction

Reconstruction in RS-coded systems typically requires connecting to exactly \( k \) machines, whereas Hitchhiker necessitates connecting to more than \( k \) machines. In some systems, this may increase read latency during reconstruction. However, our experiments on the production data-warehouse cluster at Facebook showed no such increase in read latency. Instead, we consistently observed a significant reduction in latency due to the significantly lower amounts of data required to be read and downloaded in Hitchhiker (see Fig. 11).

### Option of Operating as an RS-Based System

Hitchhiker's storage overheads and fault tolerance are identical to those of RS-based systems. Additionally, a reconstruction operation in Hitchhiker can be performed as in RS-based systems by downloading any \( k \) entire blocks. This feature allows Hitchhiker to operate as an RS-based system when necessary, such as when increased connectivity during reconstruction is not desired. It also ensures compatibility with other alternative solutions proposed outside the erasure-coding component (e.g., [3,5,18]).

### Choice of Hop-Length

As discussed in §4, a larger hop-length leads to more contiguous reads but requires coupling of bytes that are further apart. Reconstructing any byte also necessitates reconstructing its coupled byte. In scenarios where only part of a block needs to be reconstructed, all the bytes coupled with the bytes of this part must also be reconstructed, even if they are not required. A lower hop-length reduces the amount of such unnecessary reconstructions.

### Higher Encoding Time vs. Improvement in Other Metrics

Hitchhiker trades off a higher encoding time for improvements in other dimensions (Table 1). Encoding raw data into erasure-coded data is a one-time task, often executed as a background job. In contrast, reconstruction operations are performed repeatedly, and degraded read requests must be served in real time. Therefore, the gains in other metrics achieved by Hitchhiker outweigh the additional encoding cost in the systems we consider.

### Related Work

Erasure codes have many advantages over replication [25, 31]. The most attractive feature of erasure codes is that while replication entails a minimum of 2× storage redundancy, erasure codes can support significantly smaller storage overheads for the same levels of reliability. Many storage systems employ erasure codes for various application scenarios [2, 3, 9, 24].

Traditional erasure codes, however, face the problem of inefficient reconstruction. Several works (e.g., [3, 5, 18]) propose system-level solutions to reduce data transfer during reconstruction, such as caching the data read during reconstruction, batching multiple recovery operations in a stripe, or delaying recovery operations. While these solutions treat the erasure code as a black box, Hitchhiker modifies this black box by employing a new erasure code to address the reconstruction problem. Hitchhiker retains all the properties of the underlying RS-based system, allowing it to be used in conjunction with any solution proposed outside the erasure-code module.

The problem of reducing the amount of data accessed during reconstruction through the design of new erasure codes has received much attention recently [7, 11, 12, 17, 19, 21, 22, 27, 30, 32]. However, existing practical solutions either require additional parity units, increasing storage overheads [7, 12, 17, 21, 27], or are applicable in very limited settings [11, 15, 30, 32].

The idea of connecting to more machines and downloading smaller amounts of data from each node was proposed in [6] as part of the 'regenerating codes model'. However, all practical constructions of regenerating codes require high storage redundancy, e.g., codes in [21] require \( r \geq (k - 1) \). Rotated-RS [15] is another class of codes proposed for the same purpose, but it supports at most 3 parities and its fault tolerance is established via a computer search. Recently, optimized recovery algorithms [30, 32] have been proposed for EVEN-ODD and RDP codes, but they support only 2 parities. For the parameters where [15, 30, 32] exist, Hitchhiker performs at least as well while supporting an arbitrary number of parities. An erasure-coded storage system that optimizes data download during reconstruction is presented in [11]. While this system achieves minimal possible download during reconstruction, it supports only 2 parities and requires a decode operation for every read request since it cannot reconstruct an identical version of a failed unit but only a functionally equivalent version.

Systems proposed in [7, 12, 17] use local-repair codes to reduce the number of blocks accessed during reconstruction, thereby reducing the total amount of data read and downloaded. However, these codes necessitate adding at least 25% to 50% more parity units, increasing storage space requirements.

### Conclusion

We have introduced Hitchhiker, a systematically designed, new, and novel storage system that "rides" on top of existing Reed-Solomon based erasure-coded systems. Hitchhiker retains the key benefits of RS-coded systems over replication-based counterparts, namely optimal storage space needed for a targeted level of reliability and fine-grained flexibility in the design choice for the system. We show how Hitchhiker can reduce both network and disk traffic by 25% to 45% over RS-coded systems during reconstruction. Our implementation and evaluation of Hitchhiker on two HDFS clusters at Facebook reveal savings of 36% in computation time and 32% in the time taken to read data during reconstruction.

As we scale next-generation data centers and cloud storage systems, sustaining the massive growth in the volume of data needing to be stored and retrieved reliably and efficiently is a primary challenge. Replication, while ideal for flexible access and efficient reconstruction, is not sustainable for large volumes of data due to its 2× redundancy factor. RS-coded systems, which offer fine-grained redundancy factors between 1× and 2×, have gained traction despite their shortcomings in network and disk traffic during reconstruction. This underscores the importance of Hitchhiker, which aims to get the best of both worlds in a systematic and scalable manner.

### References

[1] HDFS-RAID. http://wiki.apache.org/hadoop/HDFS-RAID.
[2] Seamless Reliability. http://www.cleversafe.com/overview/reliable, Feb. 2014.
[3] R. Bhagwan, K. Tati, Y. C. Cheng, S. Savage, and G. Voelker. Total Recall: System Support for Automated Availability Management. In NSDI, 2004.
[4] D. Borthakur. HDFS and Erasure Codes (HDFS-RAID). http://hadoopblog.blogspot.com/2009/08/hdfs-and-erasure-codes-hdfs-raid.html, Aug. 2009.
[5] B.-G. Chun, F. Dabek, A. Haeberlen, E. Sit, H. Weatherspoon, M. F. Kaashoek, J. Kubiatowicz, and R. Morris. Efficient Replica Maintenance for Distributed Storage Systems. In NSDI, 2006.
[6] A. G. Dimakis, P. B. Godfrey, Y. Wu, M. Wainwright, and K. Ramchandran. Network Coding for Distributed Storage Systems. IEEE Trans. Inf. Th., Sept. 2010.
[7] K. Esmaili, L. Pamies-Juarez, and A. Datta. CORE: Cross-Object Redundancy for Efficient Data Repair in Storage Systems. In IEEE International Conf. on Big data, 2013.
[8] B. Fan, W. Tantisiriroj, L. Xiao, and G. Gibson. DiskReduce: RAID for Data-Intensive Scalable Computing. In Proceedings of the 4th Annual Workshop on Petascale Data Storage, pages 6–10. ACM, 2009.
[9] B. Fan, W. Tantisiriroj, L. Xiao, and G. Gibson. DiskReduce: RAID for Data-Intensive Scalable Computing. In ACM Workshop on Petascale Data Storage, 2009.
[10] S. Ghemawat, H. Gobioff, and S. Leung. The Google File System. In ACM SOSP, 2003.
[11] Y. Hu, H. C. Chen, P. P. Lee, and Y. Tang. NCCloud: Applying Network Coding for the Storage Repair in a Cloud-of-Clouds. In USENIX FAST, 2012.
[12] C. Huang, H. Simitci, Y. Xu, A. Ogus, B. Calder, P. Gopalan, J. Li, and S. Yekhanin. Erasure Coding in Windows Azure Storage. In USENIX ATC, 2012.
[13] S. Jiekak, A. Kermarrec, N. Scouarnec, G. Straub, and A. Van Kempen. Regenerating Codes: A System Perspective. arXiv:1204.5028, 2012.
[14] G. Kamath, N. Silberstein, N. Prakash, A. Rawat, V. Lalitha, O. Koyluoglu, P. Kumar, and S. Vishwanath. Explicit MBR All-Symbol Locality Codes. In ISIT, 2013.
[15] O. Khan, R. Burns, J. Plank, W. Pierce, and C. Huang. Rethinking Erasure Codes for Cloud File Systems: Minimizing I/O for Recovery and Degraded Reads. In FAST, 2012.
[16] S. Lin and D. Costello. Error Control Coding. Prentice-Hall Englewood Cliffs, 2004.
[17] S. Mahesh, M. Asteris, D. Papailiopoulos, A. G. Dimakis, R. Vadali, S. Chen, and D. Borthakur. Xoring Elephants: Novel Erasure Codes for Big Data. In VLDB, 2013.
[18] J. Mickens and B. Noble. Exploiting Availability Prediction in Distributed Systems. In NSDI, 2006.
[19] D. Papailiopoulos, A. Dimakis, and V. Cadambe. Repair Optimal Erasure Codes Through Hadamard Designs. IEEE Trans. Inf. Th., May 2013.
[20] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur, and K. Ramchandran. A Solution to the Network Challenges of Data Recovery in Erasure-Coded Distributed Storage Systems: A Study on the Facebook Warehouse Cluster. In Proc. USENIX HotStorage, June 2013.
[21] K. V. Rashmi, N. B. Shah, and P. V. Kumar. Optimal Exact-Regenerating Codes for the MSR and MBR Points via a Product-Matrix Construction. IEEE Trans. Inf. Th., 2011.
[22] K. V. Rashmi, N. B. Shah, and K. Ramchandran. A Piggybacking Design Framework for Read-and Download-Efficient Distributed Storage Codes. In IEEE International Symposium on Information Theory, 2013.
[23] I. Reed and G. Solomon. Polynomial Codes Over Certain Finite Fields. Journal of SIAM, 1960.
[24] S. Rhea, P. Eaton, D. Geels, H. Weatherspoon, B. Zhao, and J. Kubiatowicz. Pond: The OceanStore Prototype. In USENIX FAST, 2003.
[25] R. Rodrigues and B. Liskov. High Availability in DHTs: Erasure Coding vs. Replication. In IPTPS, 2005.
[26] N. Shah, K. Rashmi, P. Kumar, and K. Ramchandran. Distributed Storage Codes with Repair-by-Transfer and Non-Achievability of Interior Points on the Storage-Bandwidth Tradeoff. IEEE Trans. Inf. Theory, 2012.
[27] N. B. Shah. On Minimizing Data-Read and Download for Storage-Node Recovery. IEEE Communications Letters, 2013.
[28] K. Shvachko, H. Kuang, S. Radia, and R. Chansler. The Hadoop Distributed File System. In IEEE MSST, 2010.
[29] I. Tamo, Z. Wang, and J. Bruck. Zigzag Codes: MDS Array Codes with Optimal Rebuilding. IEEE Trans. Inf. Th., 2013.
[30] Z. Wang, A. Dimakis, and J. Bruck. Rebuilding for Array Codes in Distributed Storage Systems. In ACTEMT, 2010.
[31] H. Weatherspoon and J. D. Kubiatowicz. Erasure Coding vs. Replication: A Quantitative Comparison. In IPTPS, 2002.
[32] L. Xiang, Y. Xu, J. Lui, and Q. Chang. Optimal Recovery of Single Disk Failure in RDP Code Storage Systems. In ACM SIGMETRICS, 2010.

### Appendix

#### Hitchhiker-XOR
The encoding procedure of Hitchhiker-XOR first divides the \( k \) data units into \( (r - 1) \) disjoint sets of roughly equal sizes. For example, in the (k = 10, r = 4) code of Fig. 4, the three sets are units {1, 2, 3}, units {4, 5, 6}, and units {7, 8, 9, 10}. For each set \( j \in \{1, \ldots, r - 1\} \), the bytes of the first substripe of all units in set \( j \) are XORed, and the result is XORed with the second substripe of the \( (j + 1) \)-th parity unit.

Reconstruction of a data unit belonging to any set \( j \) requires the bytes of both substripes of the other data units in set \( j \), only the second byte of all other data units, and the second bytes of the first and \( (j + 1) \)-th parity units. The decoding procedure for reconstructing any data unit \( i \) is executed in three steps:
1. **Step 1:** The \( k \) bytes \(\{b_1, \ldots, b_k, f_1(b)\} \setminus \{b_i\}\) belonging to the second substripe of the units \(\{1, \ldots, k + 1\} \setminus \{i\}\) are identical to the \( k \) corresponding encoded bytes in the underlying RS code. Perform RS decoding of these \( k \) bytes to get \( b \) (which includes one of the desired bytes \( b_i \)).
2. **Step 2:** In the other bytes accessed, subtract out all components that involve \( b \).
3. **Step 3:** XOR the resulting bytes to get \( a_i \).

If the size of a set is \( s \), reconstruction of any data unit in this set requires \( (k + s) \) bytes (compared to \( 2k \) under RS).

#### Hitchhiker-XOR+
Assume without loss of generality that, in the underlying RS code, the all-XOR property is satisfied by the second parity. The encoding procedure first selects a number \( \ell \in \{0, \ldots, k\} \) and partitions the first \( (k - \ell) \) data units into \( (r - 1) \) sets of roughly equal sizes. On these \( (k - \ell) \) data units and \( r \) parity units, it performs an encoding identical to that in Hitchhiker-XOR. Next, in the second parity unit, the byte of the second substripe is XORed onto the byte of the first substripe.

Reconstruction of any of the first \( (k - \ell) \) data units is performed in a manner identical to that in Hitchhiker-XOR. Reconstruction of any of the last \( \ell \) data units requires the byte of the first substripe of the second parity and the bytes of the second substripes of all other units. The decoding procedure remains identical to the three-step procedure of Hitchhiker-XOR stated above.

For any of the first \( (k - \ell) \) data units, if the size of its set is \( s \), then reconstruction of that data unit requires \( (k + s) \) bytes (compared to \( 2k \) under RS). The reconstruction of any of the last \( \ell \) units requires \( (k + r + \ell - 2) \) bytes (compared to \( 2k \) under RS). The parameter \( \ell \) can be chosen to minimize the average or maximum data required for reconstruction as per the system requirements.

#### Hitchhiker-nonXOR
The encoding procedure is identical to that of Hitchhiker-XOR+, except that instead of XORing the bytes of the first substripe of the data units in each set, these bytes are encoded using the underlying RS encoding function, considering all other data units that do not belong to the set as zeros.

The collection of data bytes required for the reconstruction of any data unit is identical to that under Hitchhiker-XOR+. The decoding operation for reconstruction is a three-step procedure. The first two steps are identical to the first two steps of the decoding procedure of Hitchhiker-XOR described above. The third step requires an RS decoding operation (recall the (10, 4) case from §3.3).

In particular, the output of the second step when reconstructing a data unit \( i \) will be equal to \( k \) bytes that would have been obtained from the RS encoding of the data bytes in the units belonging to that set with all other data bytes set to zero. An RS decoding operation performed on these bytes now gives \( a_i \), thus recovering the \( i \)-th data unit (recall that \( b_i \) is reconstructed in Step 1 itself). The data access patterns during reconstruction and the amount of savings under Hitchhiker-nonXOR are identical to those under Hitchhiker-XOR+.