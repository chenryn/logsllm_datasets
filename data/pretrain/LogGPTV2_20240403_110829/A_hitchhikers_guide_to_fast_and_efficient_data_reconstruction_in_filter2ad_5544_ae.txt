a) Three versions of the code: During encoding and recon-
struction, Hitchhiker-XOR requires performing only XOR
operations in addition to the operations of the underlying RS
code. Hitchhiker-XOR+ allows for more eﬃcient reconstruc-
tion in terms of network and disk resources in comparison
to Hitchhiker-XOR, while still using only XOR operations
in addition to the RS code’s operations. Hitchhiker-XOR+,
however, requires the underlying RS code to satisfy the all-
XOR-parity property (§3.2). Hitchhiker-nonXOR provides
the same eﬃciency in reconstruction as Hitchhiker-XOR+
without imposing the all-XOR-parity requirement on the RS
code, but entails additional ﬁnite-ﬁeld arithmetic during en-
coding and reconstruction.
b) Connecting to more machines during reconstruction:
Reconstruction in RS-coded systems requires connecting to
exactly k machines, whereas Hitchhiker entails connecting
to more than k machines. In certain systems, depending on
the setting at hand, this may lead to an increase in the read
latency during reconstruction. However, in our experiments
on the production data-warehouse cluster at Facebook, we
saw no such increase in read latency. On the contrary, we
consistently observed a signiﬁcant reduction in the latency
due to the signiﬁcantly lower amounts of data required to
be read and downloaded in Hitchhiker (see Fig. 11).
c) Option of operating as an RS-based system: The stor-
age overheads and fault tolerance of Hitchhiker are identical
to RS-based systems. Moreover, a reconstruction operation
in Hitchhiker can alternatively be performed as in RS-based
systems by downloading any k entire blocks. Hitchhiker thus
provides an option to operate as an RS-based system when
necessary, e.g., when increased connectivity during recon-
struction is not desired. This feature also ensures Hitch-
hiker’s compatibility with other alternative solutions pro-
posed outside the erasure-coding component (e.g., [3,5,18]).
d) Choice of hop-length: As discussed in §4, a larger hop-
length leads to more contiguous reads, but requires coupling
of bytes that are further apart. Recall that reconstructing
any byte also necessitates reconstructing its coupled byte.
In a scenario where only a part of a block may need to
be reconstructed, all the bytes that are coupled with the
bytes of this part must also be reconstructed even if they
are not required. A lower hop-length reduces the amount
such frivolous reconstructions.
e) Higher encoding time vs.
improvement in other met-
rics: Hitchhiker trades oﬀ a higher encoding time for im-
provement along other dimensions (Table 1). Encoding of
raw data into erasure-coded data is a one time task, and
is often executed as a background job. On the other hand,
reconstruction operations are performed repeatedly, and de-
graded read requests must be served in real time. For these
reasons, the gains in terms of other metrics achieved by
Hitchhiker outweigh the additional encoding cost in the sys-
tems we consider.
8. RELATED WORK
Erasure
and cons
codes have many pros
over
replication [25, 31]. The most attractive feature of erasure
codes is that while replication entails a minimum of 2×
storage redundancy, erasure codes can support signiﬁcantly
smaller storage overheads for the same levels of reliability.
Many storage systems thus employ erasure codes for
Traditional
various application scenarios [2, 3, 9, 24].
erasure codes however face the problem of
ineﬃcient
reconstruction. To this end, several works (e.g., [3, 5, 18])
propose system level solutions that can be employed to
reduce data transfer during reconstruction operations, such
as caching the data read during reconstruction, batching
multiple recovery operations in a stripe, or delaying the
recovery operations. While these solutions consider the
erasure code as a black-box, Hitchhiker modiﬁes this black
box, employing the new erasure code of this paper to
address the reconstruction problem. Note that Hitchhiker
retains all the properties of the underlying RS-based
system.
Hence any solution proposed outside of the
erasure-code module can be employed in conjunction with
Hitchhiker to beneﬁt from both the solutions.
The problem of reducing the amount of data accessed dur-
ing reconstruction through the design of new erasure codes
has received much attention in the recent past [7, 11, 12, 17,
19, 21, 22, 27, 30, 32]. However, all existing practical solu-
tions either require the inclusion of additional parity units,
thereby increasing the storage overheads [7, 12, 17, 21, 27], or
are applicable in very limited settings [11, 15, 30, 32].
The idea of connecting to more machines and downloading
smaller amounts of data from each node was proposed in [6]
as a part of the ‘regenerating codes model’. However, all
existing practical constructions of regenerating codes neces-
sitate a high storage redundancy in the system, e.g., codes
in [21] require r ≥ (k − 1). Rotated-RS [15] is another class
of codes proposed for the same purpose. However, it sup-
ports at most 3 parities, and moreover, its fault tolerance
is established via a computer search. Recently, optimized
recovery algorithms [30, 32] have been proposed for EVEN-
ODD and RDP codes, but they support only 2 parities. For
the parameters where [15, 30, 32] exist, Hitchhiker performs
at least as good, while also supporting an arbitrary number
of parities. An erasure-coded storage system which also op-
timizes for data download during reconstruction is presented
in [11]. While this system achieves minimum possible down-
load during reconstruction, it supports only 2 parities. Fur-
thermore, [11] requires decode operation to be performed for
every read request since it cannot reconstruct an identical
version of a failed unit but only reconstruct a functionally
equivalent version.
The systems proposed in [7,12,17] employ another class of
codes called local-repair codes to reduce the number blocks
accessed during reconstruction. This, in turn, also reduces
the total amount of data read and downloaded during re-
construction. However, these codes necessitate addition of
at least 25% to 50% more parity units to the code, thereby
increasing the storage space requirements.
9. CONCLUSION
We have introduced a systematically-designed, new and
novel storage system called Hitchhiker that “rides” on top
of existing Reed-Solomon based erasure-coded systems.
Hitchhiker retains the key beneﬁts of RS-coded systems
over replication-based counterparts, namely that of (i)
optimal storage space needed for a targeted level of
reliability, as well as (ii) ﬁne-grained ﬂexibility in the
design choice for the system. We show how Hitchhiker can
additionally reduce both network traﬃc and disk traﬃc by
25% to 45% over
that of RS-coded systems during
reconstruction of missing or otherwise unavailable data.
Further, our implementation and evaluation of Hitchhiker
on two HDFS clusters at Facebook also reveals savings of
36% in the computation time and 32% in the time taken to
read data during reconstruction.
As we look to scale next-generation data centers and cloud
storage systems, a primary challenge is that of sustaining
this massive growth in the volume of data needing to be
stored and retrieved reliably and eﬃciently. Replication of
data, while ideal from the viewpoint of ﬂexible access and
eﬃcient reconstruction when faced with missing or unavail-
able nodes, is clearly not a sustainable option for all but a
small fraction of the massive volume of data needing to be
stored. Speciﬁcally, replication of data costs a redundancy
factor of at least 2×. Not surprisingly, therefore, RS-coded
systems, which can oﬀer near-arbitrary ﬁne-grained redun-
dancy factors between 1× and 2×, have received more trac-
tion in data centers, despite their shortcomings with regard
to large network and disk traﬃc requirements when faced
with reconstruction of missing or unavailable data. This un-
derscores the importance of Hitchhiker which aims at getting
the best of both worlds in a systematic and scalable manner.
10. REFERENCES
[1] HDFS-RAID.
http://wiki.apache.org/hadoop/HDFS-RAID.
[2] Seamless reliability.
http://www.cleversafe.com/overview/reliable, Feb. 2014.
[3] R. Bhagwan, K. Tati, Y. C. Cheng, S. Savage, and
G. Voelker. Total recall: System support for automated
availability management. In NSDI, 2004.
[4] D. Borthakur. HDFS and Erasure Codes (HDFS-RAID).
http://hadoopblog.blogspot.com/2009/08/
hdfs-and-erasure-codes-hdfs-raid.html, Aug. 2009.
[5] B.-G. Chun, F. Dabek, A. Haeberlen, E. Sit,
H. Weatherspoon, M. F. Kaashoek, J. Kubiatowicz, and
R. Morris. Eﬃcient replica maintenance for distributed
storage systems. In NSDI, 2006.
[6] A. G. Dimakis, P. B. Godfrey, Y. Wu, M. Wainwright, and
K. Ramchandran. Network coding for distributed storage
systems. IEEE Trans. Inf. Th., Sept. 2010.
[7] K. Esmaili, L. Pamies-Juarez, and A. Datta. CORE:
Cross-object redundancy for eﬃcient data repair in storage
systems. In IEEE International Conf. on Big data, 2013.
[8] B. Fan, W. Tantisiriroj, L. Xiao, and G. Gibson.
Diskreduce: RAID for data-intensive scalable computing.
In Proceedings of the 4th Annual Workshop on Petascale
Data Storage, pages 6–10. ACM, 2009.
[9] B. Fan, W. Tantisiriroj, L. Xiao, and G. Gibson.
Diskreduce: RAID for data-intensive scalable computing.
In ACM Workshop on Petascale Data Storage, 2009.
[10] S. Ghemawat, H. Gobioﬀ, and S. Leung. The Google ﬁle
system. In ACM SOSP, 2003.
[11] Y. Hu, H. C. Chen, P. P. Lee, and Y. Tang. Nccloud:
Applying network coding for the storage repair in a
cloud-of-clouds. In USENIX FAST, 2012.
[12] C. Huang, H. Simitci, Y. Xu, A. Ogus, B. Calder,
P. Gopalan, J. Li, and S. Yekhanin. Erasure coding in
Windows Azure storage. In USENIX ATC, 2012.
[13] S. Jiekak, A. Kermarrec, N. Scouarnec, G. Straub, and
A. Van Kempen. Regenerating codes: A system
perspective. arXiv:1204.5028, 2012.
[14] G. Kamath, N. Silberstein, N. Prakash, A. Rawat,
V. Lalitha, O. Koyluoglu, P. Kumar, and S. Vishwanath.
Explicit MBR all-symbol locality codes. In ISIT, 2013.
[15] O. Khan, R. Burns, J. Plank, W. Pierce, and C. Huang.
Rethinking erasure codes for cloud ﬁle systems: minimizing
I/O for recovery and degraded reads. In FAST, 2012.
[16] S. Lin and D. Costello. Error control coding. Prentice-hall
Englewood Cliﬀs, 2004.
[17] S. Mahesh, M. Asteris, D. Papailiopoulos, A. G. Dimakis,
R. Vadali, S. Chen, and D. Borthakur. Xoring elephants:
Novel erasure codes for big data. In VLDB, 2013.
[18] J. Mickens and B. Noble. Exploiting availability prediction
in distributed systems. In NSDI, 2006.
[19] D. Papailiopoulos, A. Dimakis, and V. Cadambe. Repair
optimal erasure codes through hadamard designs. IEEE
Trans. Inf. Th., May 2013.
[20] K. V. Rashmi, N. B. Shah, D. Gu, H. Kuang, D. Borthakur,
and K. Ramchandran. A solution to the network challenges
of data recovery in erasure-coded distributed storage
systems: A study on the Facebook warehouse cluster. In
Proc. USENIX HotStorage, June 2013.
[21] K. V. Rashmi, N. B. Shah, and P. V. Kumar. Optimal
exact-regenerating codes for the MSR and MBR points via
a product-matrix construction. IEEE Trans. Inf. Th., 2011.
[22] K. V. Rashmi, N. B. Shah, and K. Ramchandran. A
piggybacking design framework for read-and
download-eﬃcient distributed storage codes. In IEEE
International Symposium on Information Theory, 2013.
[23] I. Reed and G. Solomon. Polynomial codes over certain
ﬁnite ﬁelds. Journal of SIAM, 1960.
[24] S. Rhea, P. Eaton, D. Geels, H. Weatherspoon, B. Zhao,
and J. Kubiatowicz. Pond: The OceanStore prototype. In
USENIX FAST, 2003.
[25] R. Rodrigues and B. Liskov. High availability in DHTs:
Erasure coding vs. replication. In IPTPS, 2005.
[26] N. Shah, K. Rashmi, P. Kumar, and K. Ramchandran.
Distributed storage codes with repair-by-transfer and
non-achievability of interior points on the
storage-bandwidth tradeoﬀ. IEEE Trans. Inf. Theory, 2012.
[27] N. B. Shah. On minimizing data-read and download for
storage-node recovery. IEEE Communications Letters,
2013.
[28] K. Shvachko, H. Kuang, S. Radia, and R. Chansler. The
Hadoop distributed ﬁle system. In IEEE MSST, 2010.
[29] I. Tamo, Z. Wang, and J. Bruck. Zigzag codes: MDS array
codes with optimal rebuilding. IEEE Trans. Inf. Th., 2013.
[30] Z. Wang, A. Dimakis, and J. Bruck. Rebuilding for array
codes in distributed storage systems. In ACTEMT, 2010.
[31] H. Weatherspoon and J. D. Kubiatowicz. Erasure coding
vs. replication: A quantitative comparison. In IPTPS, 2002.
[32] L. Xiang, Y. Xu, J. Lui, and Q. Chang. Optimal recovery
of single disk failure in RDP code storage systems. In ACM
SIGMETRICS, 2010.
APPENDIX
Hitchhiker-XOR: The encoding procedure of Hitchhiker-XOR
ﬁrst divides the k data units into (r − 1) disjoint sets of roughly
equal sizes. For instance, in the (k = 10, r = 4) code of Fig. 4, the
three sets are units {1, 2, 3}, units {4, 5, 6} and units {7, 8, 9, 10}.
For each set j ∈ {1, . . . , r − 1}, the bytes of the ﬁrst substripe of
all units in set j are XORed, and the resultant is XORed with
the second substripe of the (j + 1)th parity unit.
Reconstruction of a data unit belonging to any set j requires
the bytes of both the substripes of the other data units in set j,
only the second byte of all other data units, and the second bytes
of the ﬁrst and (j + 1)th parity units. The decoding procedure
for reconstruction of any data unit i is executed in three steps:
Step 1 : The k bytes {b1, . . . , bk, f1(b)}\{bi} belonging to the
second substripe of the units {1, . . . , k + 1}\{i} are identical to
the k corresponding encoded bytes in the underlying RS code.
Perform RS decoding of these k bytes to get b (which includes
one of the desired bytes bi).
Step 2 : In the other bytes accessed, subtract out all components
that involve b.
Step 3 : XOR the resulting bytes to get ai.
If the size of a set is s, reconstruction of any data unit in this
set requires (k + s) bytes (as compared to 2k under RS).
Hitchhiker-XOR+: Assume without loss of generality that,
in the underlying RS code, the all-XOR property is satisﬁed by
the second parity. The encoding procedure ﬁrst selects a number
(cid:96) ∈ {0, . . . , k} and partitions the ﬁrst (k − (cid:96)) data units into
(r − 1) sets of roughly equal sizes. On these (k − (cid:96)) data units
and r parity units, it performs an encoding identical to that in
Hitchhiker-XOR. Next, in the second parity unit, the byte of the
second substripe is XORed onto the byte of the ﬁrst substripe.
Reconstruction of any of the ﬁrst (k−(cid:96)) data units is performed
in a manner identical to that in Hitchhiker-XOR. Reconstruction
of any of the last (cid:96) data units requires the byte of the ﬁrst sub-
stripe of the second parity and the bytes of the second substripes
of all other units. The decoding procedure remains identical to
the three-step procedure of Hitchhiker-XOR stated above.
For any of the ﬁrst (k − (cid:96)) data units, if the size of its set is
s then reconstruction of that data unit requires (k + s) bytes (as
compared to 2k under RS). The reconstruction of any of the last
(cid:96) units requires (k + r + (cid:96) − 2) bytes (as compared to 2k under
RS). The parameter (cid:96) can be chosen to minimize the average
or maximum data required for reconstruction as per the system
requirements.
Hitchhiker-nonXOR: The encoding procedure is identical to
that of Hitchhiker-XOR+, except that instead of XORing the
bytes of the ﬁrst substripe of the data units in each set, these
bytes are encoded using the underlying RS encoding function
considering all other data units that do not belong to the set
as zeros.
The collection of data bytes required for the reconstruction of
any data unit is identical to that under Hitchhiker-XOR+. The
decoding operation for reconstruction is a three-step procedure.
The ﬁrst two steps are identical to the ﬁrst two steps of the de-
coding procedure of Hitchhiker-XOR described above. The third
step requires an RS decoding operation (recall the (10, 4) case
from §3.3).
In particular, the output of the second step when
reconstructing a data unit i will be equal to k bytes that would
have been obtained from the RS encoding of the data bytes in the
units belonging to that set with all other data bytes set to zero.
An RS decoding operation performed on these bytes now gives ai,
thus recovering the ith data unit (recall that bi is reconstructed in
Step 1 itself.) The data access patterns during reconstruction and
the amount of savings under Hitchhiker-nonXOR are identical to
that under Hitchhiker-XOR+.