1.988ms
1.202ms
1.048ms
0.855ms
)
s
m
(
y
c
n
e
a
L
t
5
4
3
2
1
0
0
2
4
6
8
Index of target instance
10
4
x 10
s
e
c
n
a
t
s
n
I
i
g
n
b
o
r
P
1500
1000
500
0
Figure 17: End-to-end latency between an instance in VPC and all
other instances in other VPCs in EC2.
with the micro type in availability zone 1a to all live
VPC instances in EC2. As we can see, most end-to-end
latency values (over 99%) are above 1ms, and in very
rare cases (below 0.1%) the latency is below 0.850ms.
We perform such latency measurement from 18 sample
VPC instances with different types in different availabil-
ity zones, and similar distribution is repeatedly observed.
Based on such observations and the heuristics that in-
stances located on the same physical machine should
have lower latency than instances located in a different
physical location, we set a latency threshold for each type
of instance in each availability zone. The threshold is se-
lected so that for an instance in a VPC with certain type
and availability zone, the end-to-end latency between the
instance and 99.9% of all other VPC instances should
be above the threshold. For example, based on our mea-
surement introduced above, if we speculate that the target
VPC instance is located in availability zone 1a with mi-
cro type, the latency threshold is set to 0.850ms. Only
if the end-to-end latency between a probing instance and
a target instance is below the threshold, will the probing
instance be considered as a co-residence candidate.
If the probing instance passes the two rounds of ﬁlter-
ing, we will perform covert-channel construction to con-
ﬁrm co-residence.
6.4 VPC co-residence evaluation
To verify the feasibility of our VPC co-residence ap-
proach, we conducted a series of experiments in EC2. We
ﬁrst tested whether our approach can speculate the type
and availability zone of a target instance correctly. We
launched VPC instances in three availability zones with
six different types. For each combination, 20 instances
were launched. We applied our approach to speculate
0 1 2 3 4 5 6 7 8 9 1011121314151617181920
Target
Figure 18: The effort for co-residence with instances in VPC.
the type and availability zone of the target. If both the
type and availability zone are correctly inferred, we con-
sider that the target instance is correctly identiﬁed. Ta-
ble 3 lists our evaluation results. Each number in the
table indicates the number of the successfully identiﬁed
instances among the 20 launched instances for a zone-
type combination (e.g., 1a-t1.micro means t1.micro in-
stances launched in the us-east-1a zone). The results
show that our type/zone speculation can achieve an ac-
curacy of 77.8%.
We then evaluated the overall effectiveness of our ap-
proach for achieving co-residence. We launched 40 in-
stances in one VPC, with different types and availability
zones. We performed the full process of achieving co-
residence with VPC instances.
First, we measured the effectiveness of our two-stage
ﬁltering technique. Among all the probing instances we
launched, 63.2% of them did not pass the ﬁrst step ﬁl-
tering. For the second stage, our technique ﬁltered out
97.9% of the instances that passed the ﬁrst stage ﬁlter-
ing. For all the instances passed the two-stages ﬁlter-
ing, 17.6% of them passed the covert-channel veriﬁca-
tion, which are the instances actually co-resident with the
target.
Eventually, among 40 instances, we successfully
achieved co-residence with 18 of them. Figure 18 illus-
trates the effort we paid to achieve co-residence, showing
that to achieve co-residence in VPC is not an easy task.
An attacker may need to launch more than 1,000 probing
instances and such a process can take many hours.
Overall, we are the ﬁrst to demonstrate that an attacker
can achieve co-resident with a target inside a VPC with
high cost, and hence VPC only mitigates co-residence
threat rather than eliminating the threat all together.
USENIX Association  
24th USENIX Security Symposium  941
13
Table 3: The number of successfully identiﬁed targets.
Success
Success
Success
1a-t1.micro
16
1b-t1.micro
13
1d-t1.micro
12
1a-m1.small
13
1b-m1.small
13
1d-m1.small
18
1a-m1.medium
18
1b-m1.medium
19
1d-m1.medium
15
1a-m1.large
14
1b-m1.large
16
1d-m1.large
13
1a-m3.medium
16
1b-m3.medium
20
1d-m3.medium
14
1a-m3.large
17
1b-m3.large
17
1d-m3.large
18
7 A More Secure Cloud
ture.
Based on our measurement analysis, we have proposed
some guidelines towards more secure IaaS cloud man-
agement.
First, the cloud should manage the naming system
properly.
In general, a domain name is not sensitive
information. However, EC2’s automatic naming sys-
tem reveals its internal space.
In contrast, Azure and
Rackspace employ ﬂexible naming systems that can pre-
vent automatic location probing. However, automatic
domain name generation is more user-friendly since it
allows a user to launch instances in batch, while a cus-
tomer can only launch instances one by one in Azure and
Rackspace. Moreover, automatic domain name gener-
ation can help an IaaS vendor manage the cloud more
efﬁciently. To balance management efﬁciency and se-
curity, we suggest that IaaS clouds integrate automatic
domain name generation with a certain randomness. For
example, a random number that is derived from the cus-
tomer’s account information can be embedded into the
EC2 default domain name. This improved naming ap-
proach can prevent location probing while not degrading
management efﬁciency.
Second, it is controversial to publish all IP ranges of a
cloud. With the introduction of ZMap [10], it is not difﬁ-
cult to scan all public IPs in the cloud. We have demon-
strated that such scanning can cause serious security con-
cerns.
Third,
the routing information should be well-
protected. While trace-routing is a tool for a customer to
diagnose a networking anomaly, it can also be exploited
by an attacker to infer the internal networking informa-
tion of the cloud. However, the approach taken by Azure
and Rackspace is too strict. The prohibition of network-
ing probing deprives a customer from self-diagnosis and
self-management. A good trade-off is to show only part
of the paths, but always obscure the ﬁrst hop (ToR) and
the last second hop.
Fourth, VM placement should be more dynamic and
have more constraints. Locality reduction will make it
more difﬁcult for an attacker to locate a target.
IaaS
vendors can also leverage some historical information of
a user’s account to prevent the abuse of launching in-
stances. While EC2 has signiﬁcantly increased the dif-
ﬁculty of achieving machine-level co-residence, it is also
necessary to suppress rack-level co-residence in the fu-
8 Conclusion
We have presented a systematic measurement study on
the co-residence threat in Amazon EC2, from the per-
spectives of VM placement, network management, and
VPC. In terms of VM placement, we have demonstrated
that time locality in VM placement is signiﬁcantly re-
duced and VM placement in EC2 becomes more dy-
namic, indicating that EC2 has adjusted its VM place-
ment policy to mitigate co-residence. Regarding net-
work management, by conducting a large-scale trace-
routing measurement, we have shown that EC2 has re-
ﬁned networking conﬁgurations and introduced VPC to
reduce the threat of co-residence. We have also pro-
posed a novel method to identify a ToR-connected or
non-ToR-connected topology, which can help an attacker
to achieve rack-level co-residence. As the ﬁrst to in-
vestigate the co-residence threat in VPC, on one hand,
we have conﬁrmed the effectiveness of VPC in mitigat-
ing the co-residence threat. On the other hand, we have
shown that an attacker can still achieve co-residence by
exploiting a latency-based probing method, indicating
that VPC only mitigates co-residence threat rather than
eliminating the threat.
9 Acknowledgement
We would like to thank our shepherd Chris Grier and
the anonymous reviewers for their insightful and detailed
comments. This work was partially supported by ONR
grant N00014-13-1-0088.
References
[1] Amazon elastic compute cloud (ec2). http://aws.amazon.
com/ec2/.
[2] Google cloud platform. https://cloud.google.com/
compute/.
[3] Instance types in ec2. http://aws.amazon.com/ec2/
instance-types/.
[4] Microsoft azure services platfor. http://www.microsoft.
com/azure/default.mspx.
[5] AVIRAM, A., HU, S., FORD, B., AND GUMMADI, R. Deter-
minating timing channels in compute clouds. In Proceedings of
ACM CCSW’10, pp. 103–108.
942  24th USENIX Security Symposium 
USENIX Association
14
5
x 10
7 p.m.
6.95
6.9
6.85
6.8
6.75
6.7
s
t
s
o
h
e
v
i
l
f
o
r
e
b
m
u
N
4
x 10
2
s
t
s
o
h
f
o
r
e
b
m
u
N
1.5
1
0.5
250
200
150
100
50
6
5
2
d
o