there is only one register ﬁle shared between the two cores,
thus this scheme avoids race conditions between them.
The cores used in the architecture are open-source imple-
mentation of the MicroBlaze architecture [15]. After each
instruction is executed at each pipeline stage, their results
are compared (‘(cid:2)=’ in Fig. 4). If they match, the execution
is asserted as correct and the cores continue executing the
next instruction in the TBB. Otherwise, an error is detected
and the TBB is re-executed from the start.
The comparison that asserts whether an executed instruc-
tion is correct is ﬁne-grained at the signal level, and checks
all architectural signals. If there is a mismatch at any signal,
current execution is asserted as incorrect and the TBB is re-
executed, otherwise, execution proceeds normally. In order
to rollback correctly to the start of the TBB, the architecture
has three registers that store the start address of the block
(‘TBB Addr’ in Fig. 4). The ‘TBB Addr’ registers are
placed in a ‘delayed TMR’ arrangement, in which the central
541
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:23:31 UTC from IEEE Xplore.  Restrictions apply. 
ultimately terminates the registers deﬁned in the data-ﬂow
execution, implementing the register termination.
The commit step starts after the transaction steps ﬁnishes.
It is at the commit step that the destination addresses of
the store instructions executed in the transaction are written
in memory with their new values, setting the signal ‘Write
Enable’, and the branch to the next TBB is decided and
executed. Errors in the commit are handled in the same way
as in the transaction step, i.e., the register ﬁle mode is set
as ‘read-only’ and the TBB rollbacks.
The hardware support required to set the register ﬁle
in ‘read-only’ mode is a small logic that uses the ‘Write
Enable’ signal that comes out of the MEM stage of the
pipeline of each core and the signal ‘Force Reset’ which
is set in case of errors. If the ‘Write Enable’ signal is set,
the program is currently executing a store instruction, and
thus the load and arithmetic instructions re-executed in the
rollback cannot modify the register ﬁle. In this case, the
architecture forces the cores to execute NOPs in case the
‘Write Enable’ and ‘Force Reset’ signal is set instead of the
original instructions fetched from memory, which avoids the
modiﬁcation of the register ﬁle. This logic is implemented
in the ‘Mask Inst’ module in Fig. 4.
One important consideration is the execution of successive
TBBs in the pipeline. Consider that TBB ‘B’ is executed
after TBB ‘A’. Also consider that the last instruction of TBB
‘A’ is in the execute (EX) stage of the pipeline (i.e., TBB
‘A’ has not committed yet). The next TBB ‘B’ can enter
the instruction fetch (IF) stage of the pipeline. If an error
is detected during the EX stage in TBB ‘A’, the TBB ‘A’
can safely be re-executed as no instruction of the TBB ‘B’
modiﬁed the register ﬁle yet, because the commit step of the
TBB ‘A’ has set the register ﬁle as ‘read-only’ and this lock
was not released. In case the execution of ‘A’ were correct,
the lock on the register ﬁle would have been already released
when the ﬁrst instruction of ‘B’ reaches the EX stage, thus
the program execution would be correct.
About checkpointing, the only data that needs to be stored
to perform error recovery is the start address of the TBB
being executed, i.e., the ‘TBB Addr’ registers. Thus, because
the amount of stored data is very small, the transactional
core can perform ﬁne-grained checkpointing. This reduces
the error recovery latency to just a few instructions, as it
will be shown in the experimental evaluation.
Register File and Worst Case Rollback Latency: com-
pilers do a lot of work to maximize register usage among
basic blocks, which reduces the number of load instructions
and increases the re-use of produced data between the basic
blocks. On the other hand, maximizing the register re-use,
i.e., liveness, is not a good measure for reliability, because
the data that the register holds will be exposed to errors
during more cycles [16]. The TBB aggressively reduces
register liveness, and, as a result, it increases reliability.
A side-effect of the TBB being responsible for register
Returns to the
start of the TBB
(cid:2)
error detected in
data-flow execution
reset
LWI r1, r0, 5
LWI r2, r1, 0
ADDIK r4, r1, 1
ADDIK r5, r4, 0
LWI r6, r4, 0
ADDIK r7, r1, 0
SWI r0, r0, 7
SWI r2, r5, 0
SWI r6, r7, 0
Executes instructions
with write permission
register file
(cid:3)
BR   %nextTBB
commit
(a) error detected in data-ﬂow execution.
reset
LWI r1, r0, 5
LWI r2, r1, 0
ADDIK r4, r1, 1
ADDIK r5, r4, 0
LWI r6, r4, 0
ADDIK r7, r1, 0
SWI r0, r0, 7
SWI r2, r5, 0
SWI r6, r7, 0
Returns to the
start of the TBB
(cid:2)
error detected in
transaction
Executes instructions
with read-only
register file
(cid:3)
BR   %nextTBB
commit
(b) error detected in the transaction.
Figure 5. Transactional basic-block error scenarios.
deﬁnition and termination, and of not sharing registers with
other TBBs in the program, is that the size of the register
ﬁle can be considerably reduced. A typical basic block
has, on average, 5 instructions – 4.5 in SPEC and 5.9 in
MiBench [17]. Thus, we can expect that the number of
necessary registers to support the hardware implementation
of the TBB is around that number as well. Based on
the studies of implementing instruction caches in software,
which use a software cache of 16 positions [18], we have
adopted a register ﬁle with 16 general purpose registers in
this paper. Notice that 16 registers cuts by half the typical
size of the register ﬁle. In case a TBB needs more registers
than the 16 available, the compiler has to split this block
into smaller ones.
The length of the basic block has a very important
implication in the rollback machinery: it makes the worst
case latency of the error recovery mechanism deterministic
and completely known at compilation. In the worst case error
scenario, the last instruction of the TBB is the erroneous
one, which will force the re-execution of all instructions.
However, if the error is detected in the ﬁrst TBB instruction,
the recovery latency is 1 cycle.
On Multiple Errors & Timing and Hard-Faults: the
rollback mechanism implemented in the transactional core is
not limited to the detection and correction of single errors,
as it works with multiple errors as well. The comparators
depicted in Fig. 4 checks if all bits of the entire signal match,
and, thus, it does not matter by how many bits the mis-
match is. This is an important difference with current error
detection techniques based on signature checking in case
of control-ﬂow errors, such as [19]. In signature checking,
depending on how many bits are ﬂipped, the program can
542
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:23:31 UTC from IEEE Xplore.  Restrictions apply. 
Data bank #1
Data bank #2
A
B
Matrix-Matrix 
Multiplier
C
Output Checksum 
Generator
Input Checksum 
Generator
rTA
Br
Matrix-Vector 
Multiplier
Cr
(rTA)B
Row 
Comparator
rTC
Column 
Subtractor
ABFT
row error found
C(row,column)
C, Addr
residue
M
U
X
C
Address Generator
C(row, column)
column 
error 
found
A
Parameterizable # of multiplers
1
#
k
n
a
b
a
t
a
D
.
.
.
B
X
R
X
R
...
X
R
X
R
+
+
+
+
ACC
2
#
k
n
a
b
a
t
a
D
C
Figure 6. Block diagram of the RA3 core.
Figure 7. RA3’s parameterizable multiplier design.
branch to a memory area that is not covered by the additional
checking mechanism, which will ultimately lead to some sort
of segmentation fault at software level. In this case, the only
feasible alternative to recover execution is to re-launch the
entire application, incurring on heavy penalties to rollback
the system to a correct state.
Timing faults can occur due to the variability in the
manufacturing of transistors and due to natural aging and
also manifest as transient errors, thus, for future technology
nodes it is important to consider them in the fault model. In
the transactional core, timing faults will be indistinguishably
captured as an instance of a soft-error. If any the cores get
desynchronized at any time due to a timing fault, the error
detection mechanism will be triggered and the execution will
be rolled back to the beginning of the TBB.
With a DMR arrangement, a fault-tolerant system is not
able to identify the faulty core, hence it is not possible to
isolate it from the system [20]. However, with DMR it is
possible to know that the system has a permanent error.
In case of permanent errors, the proposed architecture is
able to detect them with small modiﬁcations in the rollback
machinery. The rollback can count how many times the same
TBB has re-executed, and if this count reaches a determined
threshold, a permanent error is assumed and ﬂagged. The
value of the threshold should be carefully deﬁned to avoid
that
long duration transients are classiﬁed as permanent
errors. In this paper we focus on transient errors, but the
handling of hard-faults will be tackled as future work.
C. Matrix Multiplication Accelerator: The RA3 Core
The Resilient Adaptive Algebraic Architecture (RA3) core
is the MoMa’s dedicated matrix multiplication unit. The
RA3 block diagram is presented in Fig. 6. Embedded
software often rely on matrix multiplication kernels in a
myriad of domains. For instance, in aviation and space, ﬂight
laws are implemented as matrix multiplications requiring
high reliability, because a single error might lead the aircraft
to crash. Therefore, it makes sense for MoMa to provide a
dedicated and reliable unit for matrix multiplication, reduc-
ing the workload of the MicroBlaze cores through software-
induced adaptation. The reader is referred to [21] for an
extensive evaluation of the RA3 core.
The matrix-matrix multiplier circuit performs the multi-
plication between two matrices (A and B in Fig. 6), and
generates the product matrix C at the output. This unit
contains 32 synthesized multipliers, which can be seen as
small cores analogous to GPU streaming processors. As
soon as the elements at the block’s outputs are ready, they
are written back into the main memory. The control of the
multiplier block is performed using power gating techniques,
because for a determined period of time it can be the
case where the 32 multipliers are all not necessary. This
capability of adapting at a very ﬁne-grained level enables
the RA3 architecture to exploit the available parallelism in
a power-efﬁcient fashion, because unused units are turned
off during runtime. These multipliers are the units protected
with Algorithm-Based Fault Tolerance (ABFT) [22].
For short, ABFT computes the checksum of the matrix
multiplication being computed. Let A and B be two n × n
matrices, and r = {1}n be an n length column vector. Let
AB = C be the operation being protected, and C ∗ be the
wrong result of AB caused by an error.
To detect the row of C ∗ where the error is, we calculate
both ABr (calculated with precedence A(Br) to reduce the
amount of required operations) and C ∗r vectors. To detect
the column where the error is, we calculate the vectors
rT AB (calculated as (rT A)B) and rT C ∗. To correct the
error we perform two checks: if ABr differs from C ∗r at
the i-th position, then the error is at row i, and if rT AB
differs from rT C ∗ at the j-th position, then the error is at
column j. The correct element C[i, j] is obtained as:
C[i, j] = C ∗[i, j] − (C ∗r[i] − ABr[i])
543
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:23:31 UTC from IEEE Xplore.  Restrictions apply. 
The RA3 implements ABFT concurrently with the ma-
trix multiplication computation, making this core the most
efﬁcient way to perform ABFT with minimum power con-
sumption and maximum performance. The way RA3 was
designed also ensures that an erroneous value in the matrix
multiplication is at least detected when ABFT is not capable
of correcting an error. For these cases where ABFT fails, the
TBB dispatching the matrix multiplication can just be re-
executed because the RA3 core does not modify the memory
positions that stores the input matrices A and B.
The computing node of the RA3 that is used to implement
the matrix-matrix multiplier block is called multiplier and
is represented in Fig. 7 by the blocks labeled as ‘X’. The
system designer must decide during the VHDL synthesis
step how many multipliers a given RA3 instance will have,
enabling the designer to pick a best-ﬁt architecture for its
design. After the system designer decides the number of
multipliers, the RA3 synthesis tooling automatically gener-
ates the adder tree of Fig. 7. At the adder tree generation
registers are inserted to reduce the length of the RA3 critical
path. These registers are inserted after the line of multipliers
(‘R’ registers) and after three serial adders (‘ACC’ register).
We chose three adders because their latency is similar to the
multiplier’s latency.
An important implementation issue is the need for two
separate data banks due to the cost of additional memory
ports. MoMa has a dual port memory, where one port is
read-only and the other is read/write. To read matrices A
and B and to write the resulting matrix C, it is necessary to
perform two reads and one write concurrently, thus matrices
A and B are stored in the same data bank, and C is written
in the other as shown in Fig. 6.
BENCHMARK USED TO EVALUATE MOMA
TABLE I
Acronym Application Description
bbsort
lsquares
crc32
kruskal
ﬂoyd
matmul
Bubble Sort of 100 elements
Least Squares of 24 pairs of points
CRC in 32 bits of the sentence ‘The quick
brown fox jumps over the lazy dog’
Kruskal minimum spanning tree of 7 nodes
and 11 edges (greedy algorithm)
Floyd-Warshall all-pairs shortest paths of 7
nodes and 11 edges (dynamic programming algorithm)
10 × 10 matrix multiplication in the transactional core
Varying dimensions in the RA3 core vs. ABFT’ed GPU
compiled with -O0 and 4,419 warm-up cycles of the binaries
compiled with -O1, -O2 and -O3 ﬂags.
The fault injection was performed using the VHDL code
of the architecture deployed in a Xilinx Virtex-5 FPGA
board similarly as [25]. The fault injection campaign was
exhaustive, i.e., at each program cycle we have injected one
fault in each signal of the entire netlist. The fault model we
assume is the single error, where only one bit is changed
when the fault is injected and the bit-ﬂip are forced to
last one clock cycle. This fault model correctly accounts
for Single Event Transients (SET) and Single Event Upsets
(SEU) errors [26, ch. 2, p. 14]. The simulation of SET errors
was done with a ‘saboteur’ module that, after one signal of
the circuit under test is randomly chosen, it ﬂips one of the
signal’s bit, which injects the error in the circuit’s logic.
The SEU simulation is similar, but the bit-ﬂip is inserted
in ﬂip-ﬂops, i.e., in memory elements. The fault injection
results in terms of error detection and correction coverage
are discussed in Section IV-D.
IV. MOMA CHARACTERIZATION AND EVALUATION
The algorithms and their data input used as benchmark in
A. Experimental Methodology
The area and power results obtained for the transactional
architecture were obtained with the Cadence RTL Compiler
using 65 nm transistor technology from the VHDL. The area
and power results for the register ﬁle were computed using
CACTI 6.5 [23]. There results are discussed in Section IV-B.
Performance results presented in Section IV-C were ob-
tained using RTL simulation, which allows extracting the
number of cycles required to ﬁnish the computation of
each application in the benchmark. We adopt as baseline
the MIPS architecture with performance results obtained
with the gem5 simulator [24]. We used the gem5’s atomic
memory model, which considers the memory hierarchy to
be ideally perfect, i.e., any memory access takes one cycle.
In that way,
is possible to compare the performance
extracted from the VHDL and gem5 executions, because the
performance overhead will appear in the use of instructions
that take more cycles to execute. We have removed from
the MIPS cycle count 4,426 warm-up cycles of the binaries
it
the experiments of this paper are presented in Table I.
B. Area and Peak Power Characterization
In the area and power results, we synthesized the trans-
actional core in four different operation frequencies: 12.5,
100, 250 and 300 MHz. As the baseline architecture for
calculating the area and peak power overhead, we adopted
a single MicroBlaze with its original register ﬁle of 32 reg-
isters synthesized in the same frequency as the transactional
core. We also synthesized ﬁve versions of the RA3 core with
1, 2, 4, 8, and 16 multipliers all running at 1 GHz. The RA3
core uses the transactional core as baseline.
Fig. 8a shows the relative area occupation of each ar-
chitectural unit of the transactional core, which shows that