disseminated.
4.2 Basic map dissemination
The map is disseminated by an RCP-like coordinator [9] using
reliable ﬂooding. The coordinator sends the map via TCP to a set
of nodes, which in turn sends the map to their neighbors along the
outgoing links in the map, and so on. To avoid receiving the map
multiple times form its neighbors, a node can ask a neighbor to can-
cel the map transmission once the node gets the map from another
neighbor. If a node is down during the map distribution, the node
will get the map from its neighbors once it comes up.
To ensure path isolation property (see lemma 2), we use public
key cryptography, where the coordinator signs each map with its
private key. The coordinator’s public key is distributed to all nodes
in the network either out-of-band (e.g., manually) or via a public-
key infrastructure (PKI), if available. Since map dissemination is
a relatively rare event, we believe that the overhead imposed by
the signature operations will be acceptable. Furthermore, since the
overhead on the coordinator is relatively independent on the net-
work size, we expect the coordinator to scale to large network sizes.
While we have described how maps are disseminated, the follow-
ing challenges remain, which we address next:
• How do nodes decide when to switch to a new version of the
map? How does routing happen during the window of switch-
ing maps when not all nodes route on the same version?
• How do we make the coordinator resilient to failures?
4.3 Transitioning to new map
We ﬁrst present a protocol that tries to ensure that all nodes route
using the same map. Then, we describe a mechanism that ensures
correct routing even when the protocol fails to transition all nodes
to the new map simultaneously.
When each node receives a new map, it sets a local timer that
expires after a period Tlp, where Tlp is a conservative estimate of
the diameter of the network. A node switches to a new map ei-
ther when: (a) the timer expires, or (b) it receives a packet that is
routed using a new map. Intuitively, when the timer at the ﬁrst node
that receives the map expires, all nodes would have received the
new network map completely. Hence, when it starts routing using
the new map, all nodes that route packets would switch to the new
map. This cascading process quickly resulting in the entire network
switching to the new map depending on the amount of trafﬁc ﬂow-
ing in the network. In the worst-case, all nodes would switch to the
new map within Tlp of each other.
In practice, for an intradomain topology spanning an entire conti-
nent, we expect that the longest shortest-path in the network would
be no larger than a few hundred milliseconds (for comparison, one-
way delay within the continental United States is roughly 50ms).
Hence, Tlp can be conservatively chosen to be a few seconds to
account for processing delays at each node as well.
4.3.1 Packet forwarding during map transitions
We now present a practical packet routing method using the
map update process that works even in the case when the map-
dissemination exceeds Tlp. Here, we assume that every node main-
tains not only the latest map, but the previous version of the map as
well. The basic idea is to downgrade a packet to using the previous
version of the map if the new map is not completely disseminated.
Each forwarded packet contains a sequence number that indicates
the sequence number of the map used to route that packet. When a
node starts routing a packet, its current active network map (which
is either the most recent map it has received or the previous version).
Let a node n receive a packet from n0. Let the sequence number
contained in the packet be s0, the sequence number of the active
map at the current node be s, and the largest received sequence
number received at the current node be smax (smax ≥ s).
Case (a) s0  0 is the maximum time for the map to be dissem-
inated across the network. The node suppresses the other maps it
receives to save bandwidth resources.
For routing, each node in the system would use the map from the
lowest numbered replica with the highest sequence number such
that the map is no older than time T +c, where c is deﬁned as
above. Even if there are network partitions, within all connected
components, eventual routing consistency would be reached. Parti-
tions heal at the network layer during subsequent updates.
5In practice, since the coordinator performs tasks only at coarse timescales,
having a small number of replicas for the coordinator might sufﬁce.
4.5 Periodicity of map updates
The map update period would depend on how frequently links
are decommissioned or added, planned network outages, and fre-
quency at which link costs change (say, for trafﬁc engineering). In
the extreme case, one could just disseminate the map as frequently
as the default OSPF LSA generation period (which is typically 30
seconds). Unlike OSPF, the coordinator can send only the differ-
ence from the previous network map to save bandwidth resources.
The only limitation is the overhead to sign the map.
5. EVALUATION OF FCP
We ﬁrst present our experimental methodology, the data sets we
used, and protocol conﬁgurations we used for comparison purposes.
We present results in two parts. In the ﬁrst part, we present results
that show that the overhead of FCP is very small. In the second
part, we compare FCP with OSPF as well as backup computation
techniques. Finally, we present the overhead involved in using SR-
FCP as a function of degree of inconsistency in the maps at the
routers. To summarize our results:
tion overhead and packet header overhead.
• The overhead of FCP is very low both in terms of computa-
• Unlike traditional link-state routing (such as OSPF), FCP can
provide both low loss-rate as well as low control overhead,
• Compared to prior work in backup path precomputations,
FCP provides better routing guarantees under failures despite
maintaining lesser state at the routers.
Table 1: Protocol conﬁguration parameters
FCP/Backup
Parameter
hello-interval
dead-interval
retransmit-interval
throttle-interval
OSPFD computations OSPFD default
400 ms
2 sec
2 sec
2 sec
1 sec
5 sec
5 sec
5 sec
50 ms
250 ms
n/a
n/a
5.1 Methodology
Protocols: We compared FCP with two alternate strategies: the
OSPF [23] link-state protocol, and an MPLS-like protocol that
precomputes backup-paths. To compare with OSPF, we leveraged
the OSPFD software router developed by John Moy [24], which
completely implements the OSPF protocol as speciﬁed by RFCs
2328 and 1765 [22, 23]. We conﬁgured OSPFD following the
millisecond-convergence recommendations given in [3], including
the incremental Dijkstra’s algorithm described in [25].
OSPFD also contains a network emulation toolkit for evaluat-
ing deployments, which we extended to support FCP and backup-
paths implementations. To compare with backup-path precompu-
tation, in our results, we use the sample selection algorithm used
by Juniper Networks [19]. Although algorithms exist to compute
optimal backup paths we found that such algorithms involved sub-
stantial computation time, which led to poor results when applied
to the dynamically changing networks we consider here.
Conﬁguration: To conﬁgure OSPFD’s timers, we conducted a
simulation study to determine settings that performed well on our
workloads. By default, we conﬁgured OSPFD to send one probe
every 400ms, and considered a link to be down if no probes are
received for 2 seconds. To reduce sensitivity to ﬂapping links, we
conﬁgured OSPFD to “treat bad news differently from good news”
by propagating link failures immediately but delaying propagation
of link arrivals for ﬁve seconds. Given that FCP and the backup-
path scheme do not propagate failure information globally, we con-
ﬁgured them with faster probing times (one HELLO every 50ms).
Each router sends pings to all other routers, with one ping every 15
seconds6. Finally, to fully stress FCP routing, the network maps are
never updated in the experiments we present.
Data: Link failures and arrivals were driven by ISIS traces col-
lected on the Abilene Internet2 backbone [1]. These traces contain
timestamped Link State Advertisements (LSAs). We modiﬁed the
network emulator to drive link failures by playing back LSAs based
on their timestamps. To investigate sensitivity to failure rate, we ar-
tiﬁcially vary the rate at which LSAs are played back against our
implementation. To evaluate larger networks and a wider range of
parameters, we also used Rocketfuel [29] topologies and used a
shifted Pareto distribution to drive the time-to-failure distribution
for each link. We vary the mean failure interarrival time, while hold-
ing the mean failure duration ﬁxed at 40 seconds (we found that
varying the failure duration gave similar results to varying the inter-
arrival time). Since Rocketfuel traces do not have link weights for
router-level topologies, we assign each link a weight of 1. Though
we had six AS topologies from the Rocketfuel data, we report re-
sults from AS 1239 (Sprint) Rocketfuel topology as a representative
sample. The Sprint AS topology has 283 nodes and 1882 links.
5.2 Overhead of FCP
FCP introduces extra overhead only for packets that encounter a
failed link. The overhead can be classiﬁed into: (a) network over-
head, i.e., stretch of routing the packet, (b) packet header overhead,
and (c) recomputation overhead.
Figure 4: Stretch for varying mean failure inter-arrival times (in sec-
onds). FCP incurs a stretch penalty, but this penalty is small even at
high link failure rates.
Stretch: After failure, FCP does not necessarily discover the next-
shortest path, incurring a stretch penalty. We deﬁne stretch to be
the ratio of the number of hops traversed by the packet divided by
the number of hops along the shortest working path between the
source and destination. Figure 4 shows the CDF of stretch over all
pairs of sources and destinations for increasing failure rates (where
failure rate is measured by the number of links that fail per second
in the network). The ﬁgure shows two pairs of CDFs, one pair for
paths only affected by failures (marked ‘-fp’) and the other pair for
all paths. With 1 failure per second, 2% of paths were affected by
failures, the average stretch was 1.07, and worst-case stretch was
6We found that sending pings at a faster rate could overload the OSPFD
implementation.
under 1.7. Even at very high failure rates (10 failures per second),
the average stretch was 1.1 and the worst-case stretch was under 2;
in this case, 11% of the paths were affected by failures.
Stretch increases with the rate of failures because the number of
failed links a packet encounters increases. Hence, FCP is forced
to reroute packets multiple times which reduces the chances that
the optimal end-to-end path is taken. However, as shown later, we
found this stretch was comparable to the backup-path selection
strategy that we compared against.
Figure 5: Packet overhead of FCP.
Packet header overhead: Figure 5 shows the CDF of minimum
packet overhead incurred by the failure header with FCP, using the
OC48 trace from CAIDA [2] to generate realistic trafﬁc workloads.
For the Abilene failure trace [1], FCP inﬂates the average packet
size by a negligible amount. The maximum header size during the
run is 8 bytes per packet, assuming each failure header takes 2
bytes. Although header sizes can potentially be larger in networks
with more simultaneous failures7, we can reduce the packet over-
head by using the label optimization described in Section 3.3. It is
important to note that headers are only added on link failure.
Recomputation overhead: Figure 6(a) shows a CDF of the num-
ber of recomputations per packet in a network with 1 link failure per
second. Roughly 2% of packets require recomputation to be per-
formed under the vanilla FCP implementation. Figure 6(b) shows
the time to recompute paths after link failure as measured on a
3GHz Intel Pentium processor with 2GB of RAM. Recomputation
time is below one millisecond on all topologies.
The number of on-demand recomputations performed depends
on the failure rates: about 2% and 13% of the failure-affected paths
require on-demand computation for failure rates of 1 and 10 link
failures per second respectively. Note that the backup precomputa-
tion that we perform is speciﬁc to FCP and has much lower over-
head than traditional backup precomputation strategies. This is be-
cause we only compute backup paths for adjacent link failures at