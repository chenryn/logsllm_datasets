CWE-295
Improper Certificate Validation
Incorrect Permission Assignment for Critical Resource
Improper Restriction of XML External Entity Reference
Improper Control of Generation of Code (’Code Injection’)
MDSE Score
75.56
45.69
43.61
32.12
26.53
24.54
17.94
17.35
15.54
14.1
11.47
11.08
10.78
9.74
6.33
5.5
5.48
5.36
5.12
5.04
5.04
4.4
4.3
4.23
4.06
6.4 The Two Most Dangerous CWEs: Injection
vs. Memory Errors
The two most distinctive groups of weaknesses both in the MDSE
Top 25 list and the two MSSW Top 20 lists are injection and memory
errors. However, the use of the MSSW equation and the split into
class and base lists considerably reorders these two groups, as well
as brings in new CWEs and drops some CWEs.
Injection Weaknesses. Injection is the most dangerous type
6.4.1
of weakness, represented by bases) CWE-89 (SQL Injection), CWE-
502 (Deserialization of Untrusted Data), CWE-78 (OS Command
Injection), CWE-94 (Code Injection), and CWE-611 (Improper Re-
striction of Extensible Markup Language (XML) External Entity
Reference), with ranks 1, 2, 4, 6, and 17 respectively in the base list
(see Table 3). The MDSE list also contains these five CWEs, however
the rankings of the first three are 6, 16, and 11 due to their lower
frequencies of 397, 85, and 217. The MSSW inclusion of their high
severity scores of 8.89, 9.01, and 8.58 moved them several positions
up in the base list. Note that CWE-502 covers Object Injection.
Also of importance is that the second ranked in the MDSE list
CWE-79 (Cross-site Scripting), is not in our MSSW base list. Al-
though it has the highest frequency of 1571, its severity score of
5.83 is relatively low.
The MSSW class list includes CWE-913 (Improper Control of
Dynamically-Managed Code Resources), CWE-116 (Improper En-
coding or Escaping of Output), and CWE-74 (Injection), ranked 1,
14, and 16 (see Table 2). The reason for that is CWE-913 is the parent
of CWE-502, CWE-116 is a typical cause of injection and CWE-74
is the parent of CWE-78, CWE-89, and CWE-94. Interestingly, the
class CWE-74 has rank 16 among classes, while its children bases
CWE-89, CWE-78, and CWE-94 are ranked 1, 4, 6 among bases. The
frequencies of 2455 for CWE-74, 384 for CWE-89, 194 for CWE-78,
and 100 for CWE-94, leave 1777 injection CVEs that are described
with CWEs that are either very infrequent or not severe. These are
bases CWE-79 (Cross-site Scripting) with the low severity of 5.83,
CWE-88 (Argument Injection) with the low frequency of 6, and
CWE-91 (XML Injection) with the low frequency of 16. Being not
too dangerous they bring the class CWE-74 down to rank 16. That
same base CWE-79, not included in the MSSW base list, is ranked
2nd in the MDSE list due to the frequency biasing.
6.4.2 Memory Weaknesses. The most dangerous memory weak-
nesses are CWE-787 (Out-of-bounds Write) and CWE-120 (Classic
Buffer Overflow) with ranks 3 and 5 – see Table 3. Both of them are
included in the base list but not the MDSE list, due to the correction
of the frequency bias towards proper inclusion of their severity
scores of 8.34 and 8.55.
The other memory weaknesses in the MSSW class and base lists
are as follows:
bounds read or write)
• bases CWE-125 and CWE-787 are buffer overflow (out of
• variant CWE-416 is use after free (use of deallocated memory
• variant CWE-415 is double free (deallocate of already deallo-
through a dangling pointer)
cated memory)
162ACSAC 2020, December 7–11, 2020, Austin, USA
C. Galhardo et al.
• class CWE-119 is a general memory corruption weakness,
which includes buffer overflow, use after free and double
free.
• class CWE-400 is memory overflow (stack/heap exhaustion)
[21]
Injection/Memory Weakness Comparison. Compared to MDSE,
6.4.3
the MSSW equation brings up several injection weaknesses with
much higher severity than that of any memory weaknesses. The
related CVE analysis confirms that the injection CVE are easier
to exploit and with higher impact. An injection directly leads to
arbitrary command, code, or script execution. Once a SQL injection
is in place, there is no need of additional sophisticated attack craft-
ing or use of glitches in the system. However, it takes considerable
extra effort for an attacker to turn a buffer overflow into an arbi-
trary code execution. He or she would need to have exceptional
skills, such as to apply spraying memory techniques. The possible
damage from an Object injection or from an SQL injection or from
is very high. Object injection could lead to remote code execution.
An SQL injection may expose huge amounts of structured data,
which is proven to be more valuable than raw data. Well formed
structured data is easy to read, sort, search, and make sense of it.
Via an SQL injection, an attacker could modify a database – insert,
update, delete data, execute admin operations, recover file content,
and even issue OS commands [25].
6.5 Next Most Dangerous CWEs
The next most dangerous groups of weaknesses in the MSSW class
and base lists relate to file input and upload, authentication, ran-
domization, cryptography, arithmetics and conversion, and input
validation:
• randomization – class CWE-330 (Use of Insufficiently Ran-
dom Values) with rank 5 is the class mostly directly assigned
to CVEs.
• authentication – base CWE-798 (Use of Hard-coded Creden-
tials) has rank 7; it is one of the contributors to the class
CWE-287 (Improper Authentication) with the same rank 7
in the class list.
• file upload – base CWE-434 (Unrestricted Upload of File with
Dangerous Type) has rank 8. It is the main contributors to
class CWE-669 with rank 3.
• cryptography – base CWE-352 (Cross-Site Request Forgery)
has rank 10, which relates to bugs in data verification. The
class list also has class CWE-326 (Inadequate Encryption
Strength) with rank 18, which is directly assigned to 35 CVEs
with severity 7.24.
• arithmetics and conversion – base CWE-190 (Integer Over-
flow or Wraparound) and base CWE-191 (Integer Underflow)
have ranks 13 and 20. They are the primary contributors to
pillar CWE-682 (Incorrect Calculation) with rank 9. Others
in this group on the top lists are bases CWE-131 (Incorrect
Calculation of Buffer Size), CWE-190 (Integer Overflow or
Wraparound), and CWE-191 (Integer Underflow – Wrap or
Wraparound).
• input validation - base CWE-129 (Improper Validation of
Array Index) has rank 16.
6.6 Mapping Dependencies
Both the MDSE and MSSW rankings heavily depend on how NVD
assigns CWEs to particular CVEs. The CWE selection is restricted to
view CWE-1003. Insufficient information about a CVE or an insuf-
ficiently specific CWE may lead to the use of the closest matching
CWE class or pillar to describe the CVE. For example, it makes
sense for class CWE-119 to be used for the memory corruption
CVE-2019-7098, as there is not much information (no code and
no details) – it could be any memory use error or a double free.
However, there does exist enough information about the use after
free CVE-2019-15554, but it is still mapped to class CWE-119 be-
cause there exists no appropriate base CWE. A close base CWE is
CWE-416 (Use After Free), but it does not really reflect memory
safe languages like Rust. It is also possible for a class CWE to be
assigned to a CVE even when a specific base CWE is available.
For example, the stack buffer overflow write CVE-2019-14363 is
assigned class CWE-119, although there is plenty of information to
map it more specifically to bases CWE-121 and CWE-120.
7 RELATED WORK
The constant need to improve information security has motivated
a widespread interest in metrics (both qualitative [8] and quanti-
tative [23]). As stated by Lord Kelvin, you cannot improve if you
cannot measure. However, many members of the software security
community doubt our ability to quantify security. Bellovin was
among the first [2] to argue about the infeasibility of software secu-
rity metrics. [4] discusses the limitations of the celebrated “Risk =
Threat × Vulnerability × Consequence” model that is widely used.
In [30] Verendel presents a critical survey of results and assump-
tions made in the community to quantify security. After reviewing
over 100 articles, he concludes that the validity of most methods is
still strikingly unclear. Many reasons explain this invalidity: lack of
validation, lack of comparison against empirical data, and the fact
that many assumptions in formal treatments are not empirically
well-supported in operational security.
Although we agree, we posit that acceptable but possibly im-
perfect metrics must be developed in order to facilitate security
decisions and to evaluate changes in security posture. To this end,
there have been substantial efforts to produce security metrics;
[30] surveys the literature of security metrics published between
1981 and 2008. More efforts can be found in [28], [26], and [20].
Security metrics that produce lists of the top security issues are also
very prevalent [29], [11]. Specific to software security, there is the
OWASP Top 10 [24] for web applications. Also, the CWE project
has the Common Weaknesses Scoring System (CWSS) [14] and
the Common Weakness Risk Analysis Framework (CWRAF) [16],
which are used together to provide the most important weaknesses
tailored to a particular organization.
There is also work to critique and improve the foundational
data structures used by the MDSE and MSSW metrics. CWEs have
been discussed in [31]. An entirely new approach to classifying
software bugs (weaknesses) is proposed by [3] and is currently
under development. The evolution of CWE is documented in [19]
(e.g., the addition of classification trees and content for mobile
applications and hardware). A critique of CVSS is available in [9].
In [12] a novel CWE data collection method is proposed along with
163Measurements of the Most Significant Software Security Weaknesses
ACSAC 2020, December 7–11, 2020, Austin, USA
simple atomic software security metrics. Our approach in contrast
is an aggregate metric designed to be a direct replacement for the
MDSE equation.
Along with much other work, our research should be considered
as an important step in the process to improve CWE. We believe
that our contribution is major as it points out a serious bias in the
CWE MDSE equation that is preventing accurate measurements of
the most significant software security weaknesses.
8 FUTURE WORK
This goal of this work is to identify and fix the unintended bias
in the MDSE equation towards frequency. Thus we design the
MSSW equation to, as evenly as possible, factor together frequency
and severity. And this is rational as it models typical security risk
matrices that equally combine probability and impact (e.g., [5]).
However, it is possible that intentionally biasing towards either
frequency or severity is more useful in this domain. Also, the CVSS
severity equation is itself an aggregate of exploitability and impact.
Future work should evaluate whether or not any intentional bias
should be added between these 3 factors.
Also, future work should evaluate additional metrics that might
be useful for determining the most significant CWEs. In particular, it
would be useful to identify CWEs whose associated vulnerabilities
are frequently used in actual and impactful breaches. We note that
the CVSS temporal equations provide some of this, but these results
are not commonly calculated and no public repository of this data
exists. That said, some data does exist to support such mappings
(e.g., [27]).
9 CONCLUSION
The field of security metrics is a difficult area of scientific research
because there is often no ground truth, unlike disciplines such as
physics and chemistry. This may lead one to focus on just taking
simple low level measurements that are inherently defensible; that
was the approach taken in [12]. However, creating aggregate met-
rics that compose multiple simple measurements is of practical
importance for the field of security. In this work we did just that,
aggregating frequency and severity (i.e., exploitability and impact)
into a single metric. Our objective is not for the correlations to nec-
essarily be equal, but that there exists a strong correlation for both
factors which more evenly balances the inclusion of the top fre-
quency and top severity CWEs. This seemingly simple task proved
challenging because of the differing distributions of both simpler
metrics. Indeed, the officially published CWE metric neglected this
property and did not achieve its stated objective (almost exclusively
choosing the most frequent CWEs). With our work, we claim to
have addressed the limitations and to have produced the most ac-
curate equation yet for measuring the most significant software
security weakness.
REFERENCES
[1] David W Baker, Steven M Christey, William H Hill, and David E Mann. 1999.
The Development of a Common Enumeration of Vulnerabilities and Exposures.
In Recent Advances in Intrusion Detection, Vol. 7. Online proceeding, Purdue, IN,
USA, 9.
[2] Steven M. Bellovin. 2006. On the Brittleness of Software and the Infeasibility of
Security Metrics. IEEE Security and Privacy 4, 4 (July 2006), 96. https://doi.org/
10.1109/MSP.2006.101
[3] Irena Bojanova, Paul E Black, Yaacov Yesha, and Yan Wu. 2016. The Bugs Frame-
work (BF): A Structured approach to express bugs. In 2016 IEEE International
Conference on Software Quality, Reliability and Security (QRS). IEEE, IEEE Press,
Vienna, Austria, 175–182.
[4] Louis Anthony (Tony) Cox, Jr. 2008. Some Limitations of “Risk = Threat ×
Vulnerability × Consequence” for Risk Analysis of Terrorist Attacks. Risk Analysis
28, 6 (2008), 1749–1761. https://doi.org/10.1111/j.1539-6924.2008.01142.x
[5] Pamela A Engert and Zachary F Lansdowne. 1999. Risk matrix user’s guide.
Bedford, MA: The MITRE Corporation (1999).
[6] FIRST. 2019. Common Vulnerability Scoring System Special Interest Group.
https://www.first.org/cvss Accessed: 2019-12-10.
[7] FIRST. 2019. Common Vulnerability Scoring System v3.1: Specification Document.
https://www.first.org/cvss/v3.1/specification-document Accessed: 2020-2-5.
[8] Debra S. Herrmann. 2007. Complete Guide to Security and Privacy Metrics: Mea-
suring Regulatory Compliance, Operational Resilience, and ROI (1st ed.). Auerbach
Publications, USA.
[9] Allen D. Householder Art Manion Deana Shick Jonathan Spring, Eric Hatleback.
2018. Towards Improving CVSS. https://resources.sei.cmu.edu/library/asset-
view.cfm?assetid=538368 Accessed: 2020-05-11.
[10] Robert A. Martin and Sean Barnum. 2008. Common Weakness Enumeration
(CWE) Status Update. Ada Lett. XXVIII, 1 (April 2008), 88–91. https://doi.org/10.
1145/1387830.1387835
[11] McAfee.
2020.
McAfee Labs
2019 Threats Predictions Report.
https://www.mcafee.com/blogs/other-blogs/mcafee-labs/mcafee-labs-2019-
threats-predictions/ Accessed: 2020-02-01.
[12] Peter Mell and Assane Gueye. 2020. A Suite of Metrics for Calculating the
Most Significant Security Relevant Software Flaw Types. In 2020 Conference on
Computers, Software and Applications (COMPSAC). IEEE, IEEE Computer Society
Press, Madrid, Spain.
[13] MITRE. 1999. Common Vulnerabilities and Exposures. https://cve.mitre.org
[14] MITRE. 2018. Common Weakness Scoring System (CWSS). https://cwe.mitre.
[15] MITRE. 2019. Common Weakness Enumeration. https://cwe.mitre.org Accessed:
[16] MITRE. 2019. Common Weakness Risk Analysis Framework (CWRAF). https:
//cwe.mitre.org/cwraf/ Accessed: 2020-04-10.
[17] MITRE. 2020. 2019 CWE Top 25 Most Dangerous Software Errors.
https:
//cwe.mitre.org/top25/archive/2019/2019_cwe_top25.html Accessed: 2020-02-01.
[18] MITRE. 2020. CWE Glossary. https://cwe.mitre.org/documents/glossary/ Ac-
cessed: 2020-05-11.
[19] MITRE. 2020. History of the Common Weakness Scoring System (CWSS). https:
//cwe.mitre.org/about/history.html Accessed: 2020-04-10.
[20] Patrick Morrison, David Moye, Rahul Pandita, and Laurie Williams. 2018. Map-
ping the field of software life cycle security metrics. Information and Software
Technology 102 (2018), 146 – 159. https://doi.org/10.1016/j.infsof.2018.05.011
[21] NIST. 2020. BF Memory Model. https://samate.nist.gov/BF/Classes/MEMModel.
Accessed: 2020-2-5.
org/cwss/ Accessed: 2020-04-10.
2019-12-10.
html Accessed: 2020-05-11.
2020-01-10.
[22] NVD. 2020. National Vulnerability Database. https://nvd.nist.gov Accessed:
[23] Xinming Ou and Anoop Singhal. 2011. Quantitative security risk assessment of
enterprise networks. Springer-Verlag, New York, NY, USA.
[24] OWASP. 2020. OWASP Top Ten.
https://owasp.org/www-project-top-ten/
Accessed: 2020-04-10.
[25] OWASP. 2020. SQL Injection. https://owasp.org/www-community/attacks/
SQL_Injection Accessed: 2020-05-11.
[26] Marcus Pendleton, Richard Garcia-Lebron, Jin-Hee Cho, and Shouhuai Xu. 2016.
A Survey on Systems Security Metrics. ACM Comput. Surv. 49, 4, Article 62 (dec
2016), 35 pages. https://doi.org/10.1145/3005714
[27] Guy Podjarny. 2017. Which of the OWASP Top 10 Caused the World’s Biggest
Data Breaches? https://snyk.io/blog/owasp-top-10-breaches/ Accessed: 2020-
09-22.
[28] T. W. Purboyo, B. Rahardjo, and Kuspriyanto. 2011. Security metrics: A brief
survey. In 2011 2nd International Conference on Instrumentation, Communications,
Information Technology, and Biomedical Engineering. IEEE, Bandung, Indonesia,
79–82.
[29] Symantec. 2020. 2019 Internet Security Threat Report. https://www.symantec.
com/content/dam/symantec/docs/reports/istr-24-2019-en.pdf Accessed: 2020-
02-01.
[30] Vilhelm Verendel. 2009. Quantified Security is a Weak Hypothesis: A Critical
Survey of Results and Assumptions. In Proceedings of the 2009 Workshop on New
Security Paradigms Workshop (Oxford, United Kingdom) (NSPW ’09). Association
for Computing Machinery, New York, NY, USA, 37–50. https://doi.org/10.1145/
1719030.1719036
[31] Y. Wu, Irena Bojanova, and Y. Yesha. 2015. They know your weaknesses - Do
you?: Reintroducing Common Weakness Enumeration. CrossTalk 28 (01 2015),
44–50.
164