We currently use a simple threshold based approach
for network SLA violation detection. If the packet drop
rate is greater than 10−3 or the 99th percentile latency
is larger than 5ms, we will categorize this as a network
problem and ﬁre alerts. 10−3 and 5ms are much larger
than the normal values. We keep Pingmesh historical
data for 2 months, and we run various data analysis
on top of the Pingmesh data to track the network SLAs
for diﬀerent data centers and customers. There are huge
opportunities in using data mining and machine learn-
ing to get more value out of the Pingmesh data.
In Section 5, we will study one speciﬁc packet drop
in detail: switch silent packet drops.
5. SILENT PACKET DROP DETECTION
In this section, we introduce how Pingmesh helps de-
tect switch silent packet drops. When silent packet
drops happen, the switches for various reasons do not
show information about these packet drops and the switches
seem innocent. But applications suﬀer from increased
latency and packet drops. How to quickly identify if
an ongoing live-site incident is caused by switch silent
packet drops therefore becomes critical.
In the past, we have identiﬁed two types of switch
silent packet drops: packet black-hole and silent random
packet drops. Next, we introduce how we use Pingmesh
to detect them.
Figure 6: The number of switches with packet black-
holes detected.
5.1 Packet black-hole
Packet black-hole is a special type of switch packet
drops. For a switch that is experiencing packet black-
holes, packets that meet certain ‘patterns’ are dropped
deterministically (i.e., 100%) by the switch. We have
identiﬁed two types of packet black-holes. In the ﬁrst
type of black-hole, packets with speciﬁc source destina-
tion IP address pairs get dropped. The symptom is as
following: server A cannot talk to server B, but it can
talk to servers C and D just ﬁne. All the servers A-D
are healthy.
In the second type of black-hole, packets with spe-
ciﬁc source destination addresses and transport port
numbers are dropped. Note that for this type of black-
hole, packets with the same source destination address
pair but diﬀerent source destination port numbers are
treated diﬀerently. For example, Server A can talk to
Server B’s destination port Y using source port X, but
not source port Z.
The ﬁrst type of black-holes is typically caused by
TCAM deﬁcits (e.g., parity error) in the switching ASIC.
Some TCAM entries in the TCAM table may get cor-
rupted, and the corruption causes only packets with
certain source and destination address patterns been
dropped.
(Since only destination address is used for
next-hop lookup for IP routing, on may wonder why
source IP address plays an role. Our guess is that a
TCAM entry includes not only destination address but
also source address and other meta data.)
We know less about the root causes of the second
type of black-hole. We suspect it may be because of
errors related to ECMP which uses source and destina-
tion IP addresses and port numbers to decide the next
forwarding hop.
Based on our experience, these two types of packet
black-holes can be ﬁxed by reloading the switch. Hence
the question becomes how to detect the switches with
black-holes.
We have devised a ToR switch black-hole detection
algorithm based on Pingmesh data. The idea of the
algorithm is that if many servers under a ToR switch
experience the black-hole symptom, then we mark the
147in that data center has increased signiﬁcantly and the
drops were not deterministic. Fig. 7 shows the packet
drop rate change of a service. Under normal condi-
tion, the percentage of latency should be at around
10−4 − 10−5. But it suddenly jumped up to around
2 × 10−3.
Using Pingmesh, we could soon ﬁgure out that only
one data center was aﬀected, and the other data centers
were ﬁne. Packet drops at ToR and Leaf layers cannot
cause the latency increase for all our customers due to
the much smaller number of servers under them. The
latency increase pattern shown in Figure 8(d) pointed
the problem to the Spine switch layer.
But we could not ﬁnd any packet drop hint (FCS
errors, input/output packet discards, syslog errors, etc.)
at those switches. We then suspected that this is likely a
case of silent packet drops. The next step was to locate
the switches that were dropping packets.
Again, by using Pingmesh, we could ﬁgure out several
source and destination pairs that experienced around
1%-2% random packet drops. We then launched TCP
traceroute against those pairs, and ﬁnally pinpointed
one Spine switch. The silent random packet drops were
gone after we isolated the switch from serving live traf-
ﬁc. The postmortem analysis with the switch provider
revealed that the packet drops were due to bit ﬂips of a
fabric module of that switch.
The above case is one of the ﬁrst silent random packet
drop cases we met and it took us long time to resolve.
After that we ran into more cases and we have improved
both Pingmesh data analysis and other tools for better
automatic random silent packet drop detection. Our
experiences told us that random silent packet drops may
be because of diﬀerent reasons, e.g., switching fabric
CRC checksum error, switching ASIC deﬁcit, linecard
not well seated, etc. These types of switch silent packet
drops cannot be ﬁxed by switch reload and we have
to RMA (return merchandise authorization) the faulty
switch or components.
Compared to packet drops due to network congestion
and link FCS errors, packet black-holes and silent ran-
dom drops are new and less understood to us. Due to
the whole coverage and always-on properties of Pingmesh,
we are able to conﬁrm that switch silent packet drops
do happen in real-world and categorize diﬀerent silent
packet drop types, and further locate where the silent
packet drops happen.
6. EXPERIENCES LEARNED
Pingmesh is designed to be scalable. We understand
that not all the networks are of our size. We believe
that the lessons we learned from Pingmesh are bene-
ﬁcial to networks of both large and small scales. One
of the lessons we learned is the value of trustworthy
latency data of full coverage. If the data is not trust-
worthy, then the results built on top of it cannot be
trusted. Our experience told us that not all SNMP
Figure 7: Silent random packet drops of a Spine switch
detected by Pingmesh during an incident.
ToR switch as a black-hole candidate and assign it a
score which is the ratio of servers with black-hole symp-
tom. We then select the switches with black-hole score
larger than a threshold as the candidates. Within a pod-
set, if only part of the ToRs experience the black-hole
symptom, then those ToRs are blacking hole packets.
We then invoke a network repairing service to safely
restart the ToRs. If all the ToRs in a podset experience
the black-hole symptom, then the problem may be in
the Leaf or Spine layer. Network engineers are notiﬁed
to do further investigation.
Figure 6 shows the number of ToR switches with
black-holes the algorithm detected. As we can see from
the ﬁgure, the number of the switches with packet black-
holes decreases once algorithm began to run.
In our
algorithm, we limit the algorithm to reload at most 20
switches per day. This is to limit the maximum num-
ber of switch reboots. As we can see, after a period of
time, the number of switches detected dropped to only
several per day.
We would like to note that the TCP source port of
the Pingmesh Agent varies for every probing. With
the large number of source/destination IP address pairs,
Pingmesh scans a big portion of the whole source/destination
address and port space. After Pingmesh black-hole de-
tection came online, our customers did not complain
about packet black-holes anymore.
5.2 Silent random packet drops
The higher the tier a switch is located in the net-
work, the more severe impact it will have when it begins
to drop packets. When a Spine switch drops packets
silently, tens of thousands of servers and many services
will be impacted and live-site incidents with high sever-
ity will be triggered.
Here we introduce how Pingmesh helped locate silent
In one inci-
random packet drops of a Spine switch.
dent, all the users in a data center began to experience
increased network latency at the 99th percentile. Us-
ing Pingmesh, we could conﬁrm that the packet drops
148data are trustworthy. A switch may drop packets even
though its SNMP tells us everything is ﬁne. We trust
Pingmesh data because we wrote the code, tested and
ran it. When there are bugs, we ﬁxed them. After sev-
eral iterations, we knew we can trust the data. Because
of the full coverage and trustworthy of its latency data,
Pingmesh could carry out accurate black-hole and silent
packet drop detection. As a comparison, simply using
switch SNMP and syslog data does not work since they
do not tell us about packet black-holes and silent drops.
In what follows, we introduce several additional lessons
we have learned from building and running Pingmesh,
which we believe can be applied to networks of diﬀerent
scales as well.
6.1 Pingmesh as an always-on service
From the beginning of the project, we believed that
Pingmesh needs to cover all the servers and be always-
on. But not everyone agreed. There were arguments
that latency data should only be collected on-demand;
that we should only let a few selected servers participate
in latency measurement, so as to reduce the overhead.
We disagree with both of them.
In its essence, the ﬁrst argument is always-on vs on-
demand. One may argue that it is a waste of resource if
the always-on latency data is not used, hence we should
only collect latency data when it is needed.This argu-
ment has two issues. First, we cannot predict when
the latency data will be needed since we do not know
when a live-site incident will happen. When a live-site
incident occurs, having network latency data readily at
hands instead of collecting them at that time is a much
better choice. Second, when something bad happens,
we typically do not know which network devices caused
the trouble, hence we do not even have the source des-
tination pairs to launch latency measurement.
Using only a small number of selected servers for
latency measurement limits the coverage of Pingmesh
data, and poses challenges on which servers should be
chosen. As we have demonstrated in the paper, letting
all the servers participate gives us the maximum pos-
sible coverage, and easily balance the probing activity
among all the servers. As we have demonstrated in the
paper, the CPU, memory and bandwidth overhead in-
troduced by Pingmesh is aﬀordable.
Having latency data that is always-on brings beneﬁts
that we did not recognize in the beginning. After experi-
encing a few live-site incidents due to packet black-hole
and switch silent packet drops, we found that we could
use the Pingmesh data to automatically detect these
types of switch failures, because of the whole coverage
and always-on nature of Pingmesh data (Section 5).
6.2 Loosely coupled components help evolve-
ment
Pingmesh beneﬁts from a loosely coupled system de-
sign. Pingmesh Controller and Pingmesh Agent interact
only through the pinglist ﬁles, which are standard XML
ﬁles, via standard Web API. Pingmesh Agent provides
latency data as both CSV ﬁles and standard perfor-
mance counters.
Due to its loosely coupled design, Pingmesh could be
built step by step in three phases. In the ﬁrst phase,
we focused on Pingmesh Agent. We built a simple
Pingmesh Controller which statically generates pinglist
ﬁles using a simpliﬁed pinglist generation algorithm.
The latency data was simply put into Cosmos without
automatic analysis. This phase demonstrated the feasi-
bility of Pingmesh. At the end of this phase, the latency
data was already used for network SLA calculation.
In the second phase, we built a full ﬂedged Pingmesh
Controller which automatically updates pinglists once
network topology is updated or conﬁguration is adjusted.
The new version of Pingmesh Controller is also of higher
capacity and more fault tolerant by setting up multiple
controllers in geo-distributed data centers.
In the third phase, we focused on data analysis and
visualization. We built a data processing pipeline which
automatically analyzes the collected latency data in ev-
ery 10 minutes, one hour, one day, respectively. The
processed results are then stored in database for visu-
alization, report and alert services.
The major tasks of these three phases were ﬁnished in
June 2012. After that, many new features were added
into Pingmesh:
Inter-DC Pingmesh. Pingmesh originally worked for
intra-DC. However, extending it to cover Inter-DC is
easy. We extended the Pingmesh Controller’s pinglist
generation algorithm so as to select a set of servers from
every data center and let them carry out Inter-DC ping
and the job was done. There is no single line of code or
conﬁguration change of the Pingmesh Agent. We did
add a new inter-DC data processing pipeline though.
QoS monitoring. After Pingmesh was deployed, net-
work QoS was introduced into our data center which dif-
ferentiates high priority and low priority packets based
on DSCP (diﬀerentiated service code point). Again, we
extended the Pingmesh Generator to generate pinglists
for both high and low priority classes. In this case, we
did need a simple conﬁguration change of the Pingmesh
Agent to let it listen to an additional TCP port which
is conﬁgured for low priority traﬃc.
VIP monitoring. Pingmesh was originally designed
to measure network latency of physical networks.
In
our data centers, load-balancing and IP address virtu-
alization is widely used. Address virtualization exposes
a logical Virtual IP address (VIP) to users, and the VIP
is mapped to a set of physical servers. The physical IP
addresses of these servers are called DIP (destination
IP). In our load-balancing system, there is a control
plan maintains the VIP to DIP mapping and a data
plan that delivers packets that target for a VIP to the
DIPs via packet encapsulation. When Pingmesh got
deployed, a natural extension is let Pingmesh to moni-
tor the availability of the VIPs. This again is done by
extending the Pingmesh Generation algorithm to cover
149(a) Normal
(b) Podset down
(c) Podset failure
(d) Spine failure
Figure 8: Network latency patterns through visualization.
the VIPs as the target, without touching the rest of the