Chr22 (6)
Whole Genome (6)
Hash Reference
(h:min:sec)
0:3:33 (1 core)
0:0:29 (1 core)
1:1:13 (1 core)
0:51:48 (8 cores)
0:10:11 (8 cores)
11:20:7 (8 cores)
Reference data perparation (One-time cost)
Sort hashed l-mers (h:min:sec)
Reference
(8 cores/node)
0:6:54 (5 nodes)
0:2:12 (5 nodes)
0:23:53 (20 nodes)
2:39:2 (20 nodes)
0:17:31 (20 nodes)
29:4:43 (30 nodes)
Generated (GB)
5.7
0.9
79.6
558.9
88.11
6997.8
Seeding (Every dataset)
(h:min:sec) (8 cores/node)
0:5:29 (5 nodes)
0:5:22 (5 nodes)
0:6:12 (20 nodes)
0:13:1 (20 nodes)
0:5:39 (20 nodes)
1:32:53 (30 nodes)
Table 4. Outsourced Computation
Workload of Our Private Cloud
Full Workload of CloudBurst
Reference
(# of errors)
Chr1 (3)
Chr22 (3)
Whole Genome (3)
Chr1 (6)
Chr22 (6)
Whole Genome (6)
Hash
seeds
(h:m:s)
(1 core)
0:1:3
0:1:4
0:1:8
0:6:33
0:5:26
0:6:1
Extension (1 core)
Bandwidth
Time
(h:m:s)
0:1:16
0:0:38
0:10:41
0:12:22
0:4:9
2:37:27
Mem
(GB)
1.85
1.47
6.92
3.71
3.13
8.97
Up
(GB)
0.72
0.72
0.72
5.07
5.07
5.07
Down
(MB)
11.18
11.11
17.83
74.93
74.96
120.71
Workload
(h:m:s)
(8 cores/node)
0:31:10 (1node)
0:3:52 (1node)
0:14:1 (20nodes)
0:59:23 (1node)
0:13:29 (1node)
0:26:43 (30nodes)
Bandwidth
Up
(GB)
0.81
0.81
0.81
0.81
0.81
0.81
Down
(MB)
11.32
11.52
15.16
998.06
402.46
1456.14
Outsource
ratio (%)
(*Estimate)
99.1
94.5
99.5 *
96.1
91.1
97.5 *
whole genome was merely a little more than 6 minutes, us-
ing 20 nodes. When the distance became 6, our approach
understandably ran slower (one hour and a half), given the
protection it offers and the seed-combination strategy that
moves the cost from the extension to the seeding.
In the
experiment, we set Java virtual machine’s maximum mem-
ory heap size to 1.6GB both for the cloud instances running
CloudBurst and those working on the seeding tasks in our
approach. We found that our approach consumed much less
memory than CloudBurst, which kept additional informa-
tion for extensions.
Extension performance (every dataset). The extension
tasks, based upon matched seeds or combinations, were so
small that they were all comfortably handled by the single
core: even in the case of the whole genome and the dis-
tance of 6, our desktop used about two hours and a half
to complete the extension. To understand the amount of
the workload our approach outsourced to the public cloud,
we tried to run CloudBurst on the desktop (using 8 cores).
This attempt succeeded when the references were Chr1 and
Chr22: in all these cases, at least 91% of the computation
were outsourced, even considering the time for preparing
the hash values of the seeds. However, running CloudBurst
on the whole genome within a single machine turned out to
be impossible, which forced us to look at its performance on
the public cloud. Speciﬁcally, for the distance of 6, Cloud-
Burst spent about 26 minutes on 30 8-core nodes to map all
the 10 million reads, whereas our cost on the private cloud
were merely 163 minutes using 1 node 1 core, including the
time for hashing the seeds. Therefore, we estimated that
over 97% workload was ofﬂoaded to the public cloud.
Communication overheads (every dataset). We found
that the communication overheads of our approach was
rather small. The maximum amount of data needed to be
uploaded to the public cloud was 5.07 GB (including the
hashes of the combined seeds for the 10 million reads),
which took only a couple of minutes to transfer on our 40
MBps link. The data downloaded from the public cloud
was much smaller, merely 120.71 MB. We can compare this
level of overheads with those incurred by having the whole
map job done by CloudBurst on the public cloud without
any privacy protection: about 0.81 GB data was uploaded in
this case and 1.42 GB data needed to be downloaded. Note
that our approach only needs to download a relatively small
amount of data for matched seed combinations5 rather than
the outcome of the whole computation.
Discussion. The overall computation time (the time spent
on the public cloud and the private cloud combined) of our
prototype was about 372 CPU hours for the mapping task
5Only 5% of the reads belong to human hosts.
on the whole genome with an edit distance of 6, which
is about 3.5 times as much as that for performing the
whole computation by CloudBurst, without privacy protec-
tion at all. Note that all existing privacy-preserving tech-
niques [17, 18, 20, 33, 35] are not even remotely close to
achieving this level of performance on the read-mapping
problem. On the EC2, this computation only costs $26
(estimated based on the price for reserved Cluster Compute
EC2 instance [1], whose computing power is comparable to
the cloud nodes we used) . At this expense, one does not
need to purchase and maintain a 240-core cluster: a single
desktop is sufﬁcient to do the whole work.
5 Related Work
Secure outsourcing of genomic computations. Most
of the proposed techniques for secure outsourcing of ge-
nomic computations focus on new cryptographic primi-
tives [17,21,35]. For example, a protocol for computing edit
distances [17] shares a matrix for dynamic programming to
multiple servers, which need to collaborate with each other
through homomorphic encryptions and oblivious transfers
to calculate each matrix element. This approach was found
to need 5 and a half minutes to compute an instance of the
size (25, 25) [35]. Another example is the work on opti-
mized SMC for DNA sequence alignment [35], which is
designed to leverage the special features of dynamic pro-
gramming to improve the performance of SMC. Compared
with [17], the approach is much more efﬁcient, taking about
14 seconds to complete the above task. Further improve-
ment on SMC [33] can align 2 100-element sequences in
4 seconds. Still, this overhead makes it hard to scale up
to the bar of comparing millions of reads with millions to
billions of l-mers. Recent developments on this line of re-
search include oblivious automata evaluation [20], which
only needs O(n) modular exponentiations to work on the
sequences with n elements. This performance, however,
still cannot sustain the scale of read mapping. Another re-
cent proposal [14] attempts to “disguise” DNA sequences
by scrambling some of its nucleotides to produce multiple
versions of the sequence and let multiple servers compute
on them. The outcomes of these computations are then an-
alyzed by the client to restore the edit distance between the
sequence and an l-mer. The problem with this approach is
that the server needs to communicate with the client for ev-
ery alignment attempt, making its scalability questionable.
Fundamentally, those approaches all fail to take advantage
of the special features of human genomes, which were uti-
lized in our research to build a simple and practical solution.
Secret-sharing based approaches may bring in new pol-
icy challenges: once the data has been shared to multiple
parties, the NIH completely loses the control of it, since
these parties can work together to restore the data; it is still
unclear whether the NIH needs to sign an agreement with
each of them, which these parties may want to avoid for
liability concerns [3], and if so, what the agreement will
look like. This concern is also applied to the approaches
such as distributed Smith-Waterman algorithm [53] that de-
composes a computation problem into small sub-problems
and allocates them to multiple problem solvers, under the
assumption that these parties will not collude. Another re-
lated approach [57] lets the provider of genomic data re-
place SNP values with symbols, which allows an untrusted
party to perform a program specialization on the sanitized
data. The approach assumes that the data provider knows
the locations of SNPs in its data, whereas reads do not carry
such information before they are mapped onto the reference
genome. Also remotely related to our work is the study on
the information leaks caused by aligning a query sequence
to those in a genomic database [27]. In our research, a sim-
ilar problem occurs when the public cloud analyzes the fre-
quencies of the hash values of l-mers. This threat, however,
was found to be very limited (Section 3.4).
Other secure outsourcing techniques. Early research on
secure outsourcing is mainly on delegating cryptographic
operations (e.g., modular exponentiations) to a set of un-
trusted helpers [31, 41]. More recent studies, in addition to
secure computing of edit distance, also include the compu-
tations such as linear algebra operations [16] and machine-
learning tasks [24]. For example, Peer-for-Privacy decom-
poses a category of data mining algorithms into vector addi-
tion steps and distributes them to multiple nodes on a cloud,
which can be securely evaluated through a special secret
sharing scheme [24]. All these approaches, however, may
not be suitable for computing edit distances and also incur a
large amount of communication during the computation.
6 Conclusion
In this paper, we propose a suite of new techniques that
achieve secure and scalable read mapping on hybrid clouds.
Our approach leverages the special features of the read map-
ping task, which only cares about small edit distances, and
of the Cloud, which is good at handling a large amount
of simple computation. These features enable us to split
the mapping computation according to the seed-and-extend
strategy:
the seeding stage performs simple computation
(exact matching) on a large amount of ciphertext, which is
undertaken by the public cloud, and the extension stage in-
volves a very small amount of relatively complicated com-