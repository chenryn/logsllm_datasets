## 开源PolarDB|PostgreSQL 应用开发者&DBA 公开课 - 5.1 PolarDB开源版本必学特性 - PolarDB 架构解读             
### 作者                        
digoal                        
### 日期                        
2024-01-24                        
### 标签                        
PostgreSQL , PolarDB , 应用开发者 , DBA , 公开课              
----                        
## 背景       
## PolarDB 架构解读        
PolarDB for PostgreSQL是基于PostgreSQL开源版本的开源云原生分布式数据库, 在讲解PolarDB架构之前, 首先对PostgreSQL的架构进行简单讲解, 这样我们更能理解为什么需要开发PolarDB for PostgreSQL云原生分布式数据库.   
### 1、PostgreSQL开源版本遇到的一些问题   
随着用户业务数据量越来越大，业务请求越来越频繁，业务越来越复杂，传统数据库系统面临巨大挑战，如：  
- 存储空间无法超过单机上限。  
- 通过只读实例进行读扩展，每个只读实例独享一份存储，增加了成本。  
- 由于创建只读实例需要从主库拷贝所有数据，随着数据量增加，导致创建只读实例的耗时增加。  
- 当业务压力大时产生的WAL日志较多，导致备库apply WAL的延迟较高，从而导致使用读写分离的业务可能出现问题。  
- 随着数据量的增加，当业务需要通过大量数据生成日报、月报、年报等分析数据，或需要进行较复杂的SQL请求时，或者需要快速给大表创建索引以解决性能问题时，无法高效的响应此类请求，需要增加数据仓库来实现该需求，增加了业务复杂度、故障率、成本等。  
### 2、PolarDB 的优势 
针对上述传统数据库的问题，阿里云研发了 PolarDB 云原生数据库。采用了自主研发的计算集群和存储集群分离的架构。具备如下优势：  
- 扩展性：存储计算分离，极致弹性。  
- 成本：共享一份数据，存储成本低。  
- 易用性：一写多读，透明读写分离。  
- 可靠性：三副本、秒级备份。  
![pic](20240124_02_pic_001.png)  
上图为PolarDB与PostgreSQL开源社区单机版的架构概览对比.    
### 3、PolarDB 存储计算分离架构概述   
单个计算节点无法发挥出存储侧大 IO 带宽的优势，也无法通过增加计算资源来加速大的查询。PolarDB开发了基于 Shared-Storage 的 MPP 分布式并行执行，来加速在 OLTP 场景下 OLAP 查询。 PolarDB 支持一套 OLTP 场景型的数据在如下两种计算引擎下使用：  
- 单机执行引擎：处理高并发的 OLTP 型负载。  
- 分布式执行引擎：处理大查询的 OLAP 型负载。  
![pic](20240124_02_pic_002.png)  
在使用相同的硬件资源时性能达到了传统 MPP 数据库的 90%，同时具备了 SQL 级别的弹性：在计算能力不足时，可随时增加参与 OLAP 分析查询的 CPU，而数据无需重分布。  
3\.1、架构原理  
![pic](20240124_02_pic_003.png)   
首先来看下基于 Shared-Storage 的 PolarDB 的架构原理。  
- 主节点为可读可写节点（RW），只读节点为只读（RO）。  
- Shared-Storage 层，只有主节点能写入，因此主节点和只读节点能看到一致的落盘的数据。  
- 只读节点的内存状态是通过回放 WAL 保持和主节点同步的。  
- 主节点的 WAL 日志写到 Shared-Storage，仅复制 WAL 的 meta 给只读节点。  
- 只读节点从 Shared-Storage 上读取 WAL 并回放。  
3\.2、低延迟复制  
传统流复制的问题。  
- 同步链路：日志同步路径 IO 多，网络传输量大。  
- 页面回放：读取和 Buffer 修改慢（IO 密集型 + CPU 密集型）。  
- DDL 回放：修改文件时需要对修改的文件加锁，而加锁的过程容易被阻塞，导致 DDL 慢。  
- 快照更新：RO 高并发引起事务快照更新慢。  
如下图所示：  
![pic](20240124_02_pic_004.png)   
流程如下:  
- 1、主节点写入 WAL 日志到本地文件系统中。  
- 2、WAL Sender 进程读取，并发送。  
- 3、只读节点的 WAL Receiver 进程接收写入到本地文件系统中。  
- 4、回放进程读取 WAL 日志，读取对应的 Page 到 BufferPool 中，并在内存中回放。  
- 5、主节点刷脏页到 Shared Storage。  
可以看到，整个链路很长，只读节点延迟高，影响用户业务读写分离负载均衡。  
3\.2\.1、PolarDB 优化 1：只复制 Meta  
因为底层是 Shared-Storage，只读节点可直接从 Shared-Storage 上读取所需要的 WAL 数据。因此主节点只把 WAL 日志的元数据（去掉 Payload）复制到只读节点，这样网络传输量小，减少关键路径上的 IO。如下图所示：  
![pic](20240124_02_pic_005.png)   
- 1、WAL Record 是由：Header，PageID，Payload 组成。  
- 2、由于只读节点可以直接读取 Shared-Storage 上的 WAL 文件，因此主节点只把 WAL 日志的元数据发送（复制）到只读节点，包括：Header，PageID。  
- 3、在只读节点上，通过 WAL 的元数据直接读取 Shared-Storage 上完整的 WAL 文件。  
通过上述优化，能显著减少主节点和只读节点间的网络传输量。从下图可以看到网络传输量减少了 98%。  
![pic](20240124_02_pic_006.png)   
3\.2\.2、PolarDB 优化 2：页面回放优化  
在传统 DB 中日志回放的过程中会读取大量的 Page 并逐个日志 Apply，然后落盘。该流程在用户读 IO 的关键路径上，PolarDB 借助存储计算分离可以做到：如果只读节点上 Page 不在 BufferPool 中，不产生任何 IO，仅仅记录 LogIndex 即可。  
PolarDB 可以将回放进程中的如下 IO 操作 offload 到 session 进程中：  
- 1、数据页 IO 开销。  
- 2、日志 apply 开销。  
- 3、基于 LogIndex 页面的多版本回放。  
如下图所示，在只读节点上的回放进程中，在 Apply 一条 WAL 的 meta 时：  
![pic](20240124_02_pic_007.png)   
- 1、如果对应 Page 不在内存中，仅仅记录 LogIndex。  
- 2、如果对应的 Page 在内存中，则标记为 Outdate，并记录 LogIndex，回放过程完成。  
- 3、用户 session 进程在读取 Page 时，读取正确的 Page 到 BufferPool 中，并通过 LogIndex 来回放相应的日志。  
- 4、可以看到，主要的 IO 操作有原来的单个回放进程 offload 到了多个用户进程。  
通过上述优化，能显著减少回放的延迟，比 AWS Aurora 快 30 倍。  
![pic](20240124_02_pic_008.png)   
3\.2\.3、PolarDB 优化 3：DDL 锁回放优化  
在主节点执行 DDL 时，比如：drop table，需要在所有节点上都对表上排他锁，这样能保证表文件不会在只读节点上读取时被主节点删除掉了（因为文件在 Shared-Storage 上只有一份）。在所有只读节点上对表上排他锁是通过 WAL 复制到所有的只读节点，只读节点回放 DDL 锁来完成。而回放进程在回放 DDL 锁时，对表上锁可能会阻塞很久，因此可以通过把 DDL 锁也 offload 到其他进程上来优化回放进程的关键路径。  
![pic](20240124_02_pic_009.png)   
通过上述优化，能够回放进程一直处于平滑的状态，不会因为去等 DDL 而阻塞了回放的关键路径。  
![pic](20240124_02_pic_010.png)   
上述 3 个优化之后，极大的降低了复制延迟，能够带来如下优势：  
- 读写分离：负载均衡，更接近 Oracle RAC 使用体验。  
- 高可用：加速 HA 流程。  
- 稳定性：最小化未来页的数量，可以写更少或者无需写页面快照。  
3\.2\.4、PolarDB 优化 4：Recovery 优化  
PG社区版本数据库 OOM、Crash 等场景恢复时间长，本质上是日志回放慢，在共享存储 Direct-IO 模型下问题更加突出。  
![pic](20240124_02_pic_011.png)   
PolarDB Lazy Recovery 优化:   
- 通过 [PolarDB LogIndex](https://apsaradb.github.io/PolarDB-for-PostgreSQL/zh/theory/logindex.html) 我们在只读节点上做到了 Lazy 的回放，那么在主节点重启后的 recovery 过程中，本质也是在回放日志，那么我们可以借助 Lazy 回放来加速 recovery 的过程：  
![pic](20240124_02_pic_012.png)   
- 1、从 checkpoint 点开始逐条去读 WAL 日志。  
- 2、回放完 LogIndex 日志后，即认为回放完成。  
- 3、recovery 完成，开始提供服务。  
- 4、真正的回放被 offload 到了重启之后进来的 session 进程中。  
优化之后（PG社区版本与PolarDB回放 500MB 日志量的对比）：  
![pic](20240124_02_pic_013.png)   
### 4、PolarDB HTAP 架构概述   
PolarDB 读写分离后，由于底层是存储池，理论上 IO 吞吐是无限大的。而大查询只能在单个计算节点上执行，单个计算节点的 CPU/MEM/IO 是有限的，因此单个计算节点无法发挥出存储侧的大 IO 带宽的优势，也无法通过增加计算资源来加速大的查询。PolarDB 开发了基于 Shared-Storage 的 MPP 分布式并行执行，来加速在 OLTP 场景下 OLAP 查询。  
4\.1、分布式 MPP 执行引擎   
PolarDB 底层存储在不同节点上是共享的，因此不能直接像传统 MPP 一样去扫描表。我们在原来单机执行引擎上支持了 MPP 分布式并行执行，同时对 Shared-Storage 进行了优化。 基于 Shared-Storage 的 MPP 是业界首创，它的原理是：  
- Shuffle 算子屏蔽数据分布。  
- ParallelScan 算子屏蔽共享存储。  
![pic](20240124_02_pic_014.png)   
如图所示：  
- 1、表 A 和表 B 做 join，并做聚合。  
- 2、共享存储中的表仍然是单个表，并没有做物理上的分区。  
- 3、重新设计 4 类扫描算子，使之在扫描共享存储上的表时能够分片扫描，形成 virtual partition。  
4\.2、Parallel Query 并行查询   
基于社区的 GPORCA 优化器扩展了能感知共享存储特性的 Transformation Rules。使得能够探索共享存储下特有的 Plan 空间，比如：对于一个表在 PolarDB 中既可以全量的扫描，也可以分区域扫描，这个是和传统 MPP 的本质区别。图中，上面灰色部分是 PolarDB 内核与 GPORCA 优化器的适配部分。下半部分是 ORCA 内核，灰色模块是我们在 ORCA 内核中对共享存储特性所做的扩展。  
![pic](20240124_02_pic_015.png)   
4\.2\.1、算子并行化  
PolarDB 中有 4 类算子需要并行化，下面介绍一个具有代表性的 Seqscan 的算子的并行化。为了最大限度的利用存储的大 IO 带宽，在顺序扫描时，按照 4MB 为单位做逻辑切分，将 IO 尽量打散到不同的盘上，达到所有的盘同时提供读服务的效果。这样做还有一个优势，就是每个只读节点只扫描部分表文件，那么最终能缓存的表大小是所有只读节点的 BufferPool 总和。  
![pic](20240124_02_pic_016.png)   
下面的图表中：  
- 1、增加只读节点，扫描性能线性提升 30 倍。  
- 2、打开 Buffer 时，扫描从 37 分钟降到 3.75 秒。  
![pic](20240124_02_pic_017.png)   
4\.2\.2、消除数据倾斜问题  
倾斜是传统 MPP 固有的问题：  
- 1、在 PolarDB 中，大对象的是通过 heap 表关联 `TOAST` 表，无论对哪个表切分都无法达到均衡。  
- 2、另外，不同只读节点的事务、buffer、网络、IO 负载抖动。  
以上两点会导致分布执行时存在长尾进程。  
![pic](20240124_02_pic_018.png)   
- 1、协调节点内部分成 DataThread 和 ControlThread。  
- 2、DataThread 负责收集汇总元组。  
- 3、ControlThread 负责控制每个扫描算子的扫描进度。  
- 4、扫描快的工作进程能多扫描逻辑的数据切片。  
- 5、过程中需要考虑 Buffer 的亲和性。  