the RTTs by two to obtain one-way latencies. The average
and maximum one-way latency is 91ms and 399ms, respec-
tively. When the number of simulated nodes is larger than
the number of measured DNS servers, we simulate multiple
nodes at a single DNS server site.
Unless otherwise noted, we report results on a 1,024-
node system and the simulation works as follows. Initially,
all nodes start at the same time and one random node is
designated as the root of the tree. For a target node degree
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
n
o
i
t
u
b
i
r
t
s
i
d
e
v
i
t
a
l
u
m
u
c
e
d
o
N
n
o
i
t
u
b
i
r
t
s
i
d
e
v
i
t
a
l
u
m
u
c
e
d
o
n
e
v
i
L
GoCast
proximity
overlay
gossip
no-wait
gossip
random
overlay
 0
 0.5
 1
 1.5
 2
 2.5
 3
Message delay (seconds)
(a) No node fails.
GoCast
proximity
overlay
gossip
no-wait
gossip
random
overlay
 0
 0.5
 1.5
 2.5
 1
Message delay (seconds)
 2
 3
 3.5
(b) 20% of nodes fail.
Figure 3. The propagation delay of multicast mes-
sages under different protocols (1,024 nodes).
2
Cdegree, each node initiates connections to Cdegree
random
nodes. After the initialization, the average node degree is
Cdegree and all neighbors are chosen at random. The over-
lay and the tree then adapt under GoCast’s maintenance pro-
tocols for 500 seconds in simulated time. After 500 sec-
onds, multicast messages are injected into the overlay from
random source nodes at a rate of 100 messages per second.
The parameters of GoCast are as follows. A node sends
out one gossip every t = 0.1 seconds (suggested by Bimodal
Multicast [2]). Every r = 0.1 seconds, a node wakes up to
maintain its neighbors. The default target node degrees are
Crand = 1 and Cnear = 5.
We ﬁrst present results that compare the message prop-
agation delay under different multicast protocols. The de-
lay is averaged over 1,000 multicast messages injected from
random source nodes. Figure 3(a) shows results for the ideal
case when no node fails. The “GoCast” curve represents
the complete GoCast protocol, in which messages propa-
gate both through the tree and through gossips exchanged
between overlay neighbors. The “proximity overlay” and
“random overlay” curves represent simpliﬁed versions of
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:09:24 UTC from IEEE Xplore.  Restrictions apply. 
the GoCast protocol that only propagate messages through
gossips exchanged between overlay neighbors; the system
neither maintains nor uses the tree. “Proximity overlay”
and “random overlay” differ in that each node in “proxim-
ity overlay” maintains 5 nearby neighbors and 1 random
neighbor, while each node in “random overlay” maintains 6
random neighbors only.
The “gossip” curve in Figure 3(a) represents a push-
based gossip protocol similar to that used in Bimodal Mul-
ticast. Every t = 0.1 seconds, each node sends a gossip to
a random node. The gossip fanout is 5, i.e., a node gos-
sips the ID of a received multicast message to 5 random
nodes (one node per gossip period). The receiver of a gos-
sip requests some multicast messages from the sender if the
contents of the gossip suggest that those multicast messages
are unknown to the receiver. The system represented by the
“no-wait gossip” curve is different.
In “no-wait gossip”,
upon receiving a multicast message, a node immediately
gossips the message to 5 other nodes without waiting for
the next gossip period (in other words, the gossip period t =
0). When message rate is high, “no-wait gossip” introduces
higher gossip trafﬁc than “gossip”. We use it here to reveal
the fundamental performance limits of gossip multicast.
The complete GoCast protocol (the “GoCast” curve) dis-
seminates messages signiﬁcantly faster than all other proto-
cols, because multicast messages mainly propagate without
stop through the efﬁcient tree that consists of low latency
links. Averaged over 1,000 runs, multicast messages reach
every node under 0.33 seconds. Gossip multicast (the “gos-
sip” curve) is the slowest in message propagation. More-
over, with fanout 5, some nodes never receive some of the
1,000 multicast messages due to the complete randomness
of gossips. A shorter gossip period can reduce message de-
lay at the expense of increased gossip trafﬁc. However, even
“no-wait gossip” is not as fast as “GoCast” for two reasons.
(1) Tree links in GoCast have low latencies. (2) To avoid
sending potentially large multicast messages redundantly,
“no-wait gossip” always sends gossips ﬁrst and then sends
the actual multicast messages upon requests, which incurs
extra delay. As in “gossip”, some nodes in “no-wait gossip”
never receive some multicast messages. The message delay
in “random overlay” is similar to that in “gossip” but every
node in “random overlay” receives every message, which is
guaranteed by the connectivity of the overlay. Owing to its
low latency links, “proximity overlay” propagates messages
noticeably faster than “random overlay” and “gossip”.
Figure 3(b) puts the different protocols under a stress test
when 20% of nodes fail concurrently at simulated time 500
seconds. The failed nodes are selected uniformly at random.
After node failures, multicast messages are injected into the
system but the system does not execute any of GoCast’s
maintenance protocols to repair the overlay or the tree. The
Y axis represents the cumulative distribution of live nodes.
With 20% nodes down, the overlay still remains connected.
n
o
i
t
u
b
i
r
t
s
i
d
e
v
i
t
a
l
u
m
u
c
e
d
o
N
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
1024 nodes
8192 nodes
 0.2
 0.3
 0.4
 0
 0.1
n
o
i
t
u
b
i
r
t
s
i
d
e
v
i
t
a
l
u
m
u
c
e
d
o
N
 1
 0.9
 0.8
 0.7
 0.6
 0.5
 0.4
 0.3
 0.2
 0.1
 0
1024 nodes
8192 nodes
 1
 1.5
 2
 0
 0.5
Message delay (seconds)
(a) No node fails.
Message delay (seconds)
(b) 20% of nodes fail.
Figure 4. Comparing message delays in GoCast
between a 1,024-node system and an 8,192-node
system.
“GoCast”, “proximity overlay”, and “random overlay” still
deliver every multicast message to every live node. Com-
paring the “gossip” curves in Figure 3(a) and 3(b), we see
that, for gossip multicast, a higher fraction of live nodes
do not receive some multicast messages in the face of node
failures. In terms of message delay, “GoCast” is more sen-
sitive to node failures than the other four protocols. This
is because, without any repair, the tree in GoCast is broken
into fragments and a message has to propagate among the
fragments through gossips. Once the message hits a node
in a tree fragment, it propagates without stop to other nodes
in the fragment through remaining tree links that connect
the fragment. This is the major reason why “GoCast” still
signiﬁcantly outperforms “proximity overlay”.
Figures 4(a) and 4(b) compare the delay of multicast
messages in the complete GoCast protocol between a 1,024-
node system and an 8,192-node system. When no node
fails, the difference between the two systems is small. For
the 8,192-node system, averaged over 1,000 multicast mes-
sages, a message reaches all nodes under 0.42 seconds.
When 20% of nodes fail, the difference between the two
systems becomes larger. The curve for the 8,192-node sys-
tem has a longer tail. The message delay for a small number
of nodes in the 8,192-system is about 60% longer than the
longest delay in the 1,024-node system. When a signiﬁcant
fraction of nodes fail concurrently, the tree in the larger sys-
tem is broken into more fragments than that in the smaller
system. Multicast messages propagate among the tree frag-
ments through slow gossips, which is the reason why the
difference in Figure 4(b) is more signiﬁcant than that in Fig-
ure 4(a). Overall, the increase in message delay is moderate
as the system size increases by eight fold, indicating that
GoCast is scalable.
We next evaluate GoCast’s ability to adapt the overlay
and the tree. This experiment simulates a 1,024-node sys-
tem. Initially, all nodes start at the same time and each node
initiates three random links. After initialization, the average
node degree is six. GoCast’s maintenance protocols then
adapt the overlay and the tree over time. The target random
degree is one and the target nearby degree is ﬁve.
Proceedings of the 2005 International Conference on Dependable Systems and Networks (DSN’05) 
0-7695-2282-3/05 $20.00 © 2005 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:09:24 UTC from IEEE Xplore.  Restrictions apply. 
n
o
i
t
u
b
i
r
t
s
i