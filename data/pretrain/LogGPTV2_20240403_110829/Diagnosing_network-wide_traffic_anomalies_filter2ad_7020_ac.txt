o
t
c
e
V
l
i
a
u
d
s
e
R
Mon
Tue
Wed
Thu
Fri
Sat
Sun
Mon
Tue
Wed
(a) Sprint-1
Thu
Fri
(b) Sprint-2
Sat
Sun
Figure 4: Timeseries plots of state vector squared magnitude ((cid:6)y(cid:6)2, upper) and residual vector squared magnitude ((cid:6)˜y(cid:6)2, lower) for
two weeks of Sprint data.
where
h0 = 1 − 2φ1φ3
3φ2
2
,
and φi =
mX
j=r+1
λi
j ;
for i = 1, 2, 3
and where λj is the variance captured by projecting the data on the
j-th principal component ((cid:6)Yvj(cid:6)2), and cα is the 1− α percentile
in a standard normal distribution. Jackson and Mudholkar’s result
holds regardless of how many principal components are retained in
the normal subspace.
Note that in this setting, the 1 − α conﬁdence limit corresponds
to a false alarm rate of α, if the assumptions under which this result
is derived are satisﬁed. The conﬁdence limit for the Q-statistic is
derived under the assumption that the sample vector y follows a
multivariate Gaussian distribution. However, Jensen and Solomon
point out that the Q-statistic changes little even when the underly-
ing distribution of the original data differs substantially from Gaus-
sian [12]. While we believe that normal trafﬁc in our datasets is
reasonably well described as multivariate Gaussian, we have not
closely examined the data for violations of this assumption. How-
ever, we ﬁnd that the Q-statistic gives excellent results in practice,
perhaps due to the robustness noted by Jensen and Solomon.
An important property of this approach is that it does not depend
on the mean amount of trafﬁc in the network. Thus, one can apply
the same test on networks of different sizes and utilization levels.
In Figure 4 we illustrate the effectiveness of subspace separa-
tion of y and ˜y on two of our datasets. The upper half of the ﬁg-
ures shows timeseries plots of (cid:6)y(cid:6)2 over week-long periods. On
these plots, we have marked with circles the locations where vol-
ume anomalies are known to occur (based on inspection of the un-
derlying ﬂows, as will be described in Section 6.2). It is clear that
the magnitude of the state vector y is dominated by effects other
than the anomalies, and that it is quite difﬁcult to see the effects of
anomalies on the trafﬁc volume as a whole.
In the lower half of each plot we show timeseries plots of the
SPE, (cid:6)˜y(cid:6)2, over the same one-week periods. For each dataset, the
α at the 1 − α = 99.5% and 99.9% conﬁ-
values of the Q statistic δ2
dence levels are also shown as dotted lines. The lower plots show
how the projection of the state vector onto the residual subspace ˜S
very effectively captures the anomalous trafﬁc while capturing little
normal trafﬁc, and so makes the statistical detection of anomalies
much easier.
This ﬁgure shows how sharply the subspace method is able to
separate anomalous trafﬁc patterns (lower plots) from the mass of
trafﬁc (upper plots). It also gives some insight into why (as we will
show in Section 6) the method yields such high detection rates com-
bined with low false alarm rates. As can be seen in the lower plots,
the distinct separation of anomalies from normal trafﬁc means that
almost all anomalies result in values of (cid:6)˜y(cid:6)2 greater than δ2
α, while
very few of the normal trafﬁc measurements yield (cid:6)˜y(cid:6)2 greater
than δ2
α.
5.2 Identiﬁcation
In the subspace framework, a volume anomaly represents a dis-
placement of the state vector y away from S. The particular direc-
tion of the displacement gives information about the nature of the
anomaly. Thus our general approach to anomaly identiﬁcation is to
ask which anomaly out of a set of potential anomalies is best able
to describe the deviation of y from the normal subspace S.
We denote the set of all possible anomalies as {Fi, i = 1, ..., I}.
This set should be chosen to be as complete as possible, because it
deﬁnes the set of anomalies that can be identiﬁed.
For simplicity of exposition, we will consider only one-dimensional
anomalies; that is, anomalies in which the additional per-link trafﬁc
can be described as a linear function of a single variable. However,
in Section 7 we show that it is straightforward to generalize the
approach to multi-dimensional anomalies.
Then each anomaly Fi has an associated vector θi which deﬁnes
the manner in which this anomaly adds trafﬁc to each link in the
network. We assume that θi has unit norm, so that in the presence
of anomaly Fi, the state vector y is represented by
y = y∗
+ θifi
where y∗
represents the sample vector for normal trafﬁc conditions
(and which is unknown when the anomaly occurs), and fi repre-
sents the magnitude of the anomaly.
Given some hypothesized anomaly Fi, we can form an estimate
of y∗
by eliminating the effect of the anomaly, which corresponds
to subtracting some trafﬁc contribution from the links associated
assuming anomaly Fi is
with anomaly Fi. The best estimate of y∗
found by minimizing the distance to the normal subspace S in the
direction of the anomaly:
ˆfi = arg min
fi
(cid:6)˜y − ˜θifi(cid:6)
where ˜y = ˜Cy and ˜θi = ˜Cθi. This gives ˆfi = (˜θT
assuming anomaly Fi is:
Thus the best estimate of y∗
i ˜θi)
i = y − θi ˆfi
y∗
= y − θi(˜θT
= (I − θi(˜θT
i ˜θi)
i ˜θi)
−1 ˜θT
i ˜y
−1 ˜θT
i ˜C) y
−1 ˜θT
i ˜y.
(1)
To identify the best hypothesis from our set of potential anoma-
lies, we choose the hypothesis that explains the largest amount of
residual trafﬁc. That is, we choose the Fi that minimizes the pro-
jection of y∗
i onto ˜S.
Thus, in summary, our identiﬁcation algorithm consists of:
1. for each hypothesized anomaly Fi, i = 1, ..., I, compute y∗
i
using Equation (1)
2. choose anomaly Fj as j = arg mini (cid:6) ˜Cy∗
i (cid:6).
As discussed in Section 2.2, in this paper we consider only the
set of anomalies that arise due to unusual trafﬁc in a single OD ﬂow.
Thus the possible anomalies are {Fi, i = 1, ..., n} where n is the
number of OD ﬂows in the network. In this case, each anomaly
adds (or subtracts) an equal amount of trafﬁc to each link it affects.
Then θi is deﬁned as column i of the routing matrix A, normalized
to unit norm: θi = Ai/(cid:6)Ai(cid:6).
5.3 Quantiﬁcation
Having formed an estimate of the particular volume anomaly,
Fi, we now proceed to estimate the number of bytes the constitute
this anomaly.
the chosen anomaly Fi is given by
The estimated amount of anomalous trafﬁc on each link due to
y(cid:4)
= y − y∗
i .
Then the estimated sum of the additional trafﬁc is proportional to
i y(cid:4)
θT
. Since the additional trafﬁc ﬂows over multiple links, one
must normalize by the number of links affected by the anomaly.
In the current case, where anomalies are deﬁned by the set of
OD ﬂows, our quantiﬁcation relies on A. We use ¯A to denote the
routing matrix normalized so that each column of A has unit sum,
. Then given identiﬁcation of anomaly Fi, our
that is: ¯Ai = AiP
Ai
quantiﬁcation estimate is:
i y(cid:4).
¯AT
5.4 Necessary and Sufﬁcient Conditions for
Detectability
Some anomalies may lie completely within the normal subspace
S and so cannot be detected by the subspace method. Formally,
this can occur if ˜Cθi = 0 for some anomaly Fi. In fact this is
very unlikely as it requires the anomaly and the normal subspace S
to be perfectly aligned. However, the relative relationship between
the anomaly θi and the normal subspace can make anomalies of a
given size in one direction harder to detect than in other directions.
A sufﬁcient condition for detectability in our context is given
in [5]. Specializing their results to the case of one-dimensional
anomalies, we can guarantee detectability of anomaly Fi if:
fi > 2δα(cid:6) ˜Cθi(cid:6)
If a single-ﬂow anomaly Fi consists of bi additional or missing
bytes, then this threshold becomes
bi >
2δα
(cid:6) ˜Cθi(cid:6)(cid:6)Ai(cid:6)
This shows that, the larger the projection of the normalized anomaly
vector in the residual subspace, the lower the threshold for de-
tectability at a given conﬁdence level.
In practice, the normal subspace S will tend to capture the direc-
tions of maximum variability in the data, meaning that it will tend
to be more closely aligned with those ﬂows that have the largest
variances. Thus, anomalies of a given size will tend to be harder
to detect in ﬂows with large variance, as compared to ﬂows with
small variance. We quantitatively explore this effect for our data in
Section 6.3.
6. VALIDATION
In this section, we evaluate the subspace anomaly diagnosis method
using the datasets introduced in Section 3.
6.1 Methodology
Our validation approach is centered on answering two questions:
(1) how well can the method diagnose actual anomalies observed
in real data? and (2) how does the time and location of the anomaly
affect performance of the method?
To answer the ﬁrst question, we proceed as follows: using time-
series analysis on OD ﬂow data, we ﬁrst isolate a set of “true”
anomalies. This allows us to then evaluate the subspace method
quantitatively. In particular, it allows us to measure both the detec-
tion probability and the false alarm probability.
To answer the second question, we injected anomalies of differ-
ent sizes in OD ﬂows and applied our procedure to diagnose these
known anomalies from link data. We perform this repeatedly for
each timestep and for each anomaly so as to form a complete pic-
ture of how diagnosis effectiveness varies with the time and loca-
tion of the anomaly.
In each case, we quantify the performance of each step in our
diagnosis procedure as follows. Detection success is measured by
two metrics: the detection rate and the false alarm rate. The de-
tection rate is the fraction of true anomalies detected. The false
alarm rate is the fraction of normal measurements that trigger an
erroneous detection. Identiﬁcation success is captured in the iden-
tiﬁcation rate, which is the fraction of detected anomalies that are
correctly identiﬁed. Finally, quantiﬁcation success is measured by
computing the mean absolute relative error between our estimate
and the true size of all the volume anomalies identiﬁed.
6.2 Actual Volume Anomalies
To identify the set of “true” anomalies in our data (as a precur-
sor to our validation step), we look for unusual deviations from
the mean in each OD ﬂow. There are two general classes of tech-
niques to detect such changes in a non-stationary timeseries. The
ﬁrst class of methods identiﬁes anomalies in an online manner,
based on gross deviations from forecasted behavior. Simple in-
stances of such a strategy are the exponential weighted moving av-
erage (EWMA) and Holt-Winters forecasting algorithms, both used
in [2, 14]; more sophisticated examples are ARIMA-based Box-
Jenkins forecasting models [14]. A second class of methods pro-
cess data in batches and are based on signal analysis techniques,
such as the wavelet analysis scheme used in [1]. Such schemes
model the timeseries mean by isolating low-frequency components;
anomalies are then ﬂagged at those points in time that deviate sig-
niﬁcantly from the modeled behavior of the mean.
It should be pointed out that no scheme is ideal for isolating and
quantifying spikes in a timeseries, and that some schemes are sen-
sitive to a suite of conﬁguration parameters and/or modeling as-
sumptions. Thus, to obtain sets of true anomalies, we employed a
candidate method from each class of techniques.
x 107
x 107
x 107
i
l
e
z
S
y
a
m
o
n
A
e
u
r
T
i
l
e
z
S
y
a
m
o
n
A
e
u
r
T
i
l
e
z
S
y
a
m
o
n
A
e
u
r
T
4
3.5
3
2.5
2
1.5
1
0.5
0
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
2.5
2
1.5
1
0.5
0
5
10
x 107
5
10
x 108
5
10
All
Detected
30
35
40
All
Detected
30
35
40
All
Detected
30
35
40
15
Anomaly (rank order)
20
25
15
Anomaly (rank order)
20
25
15
Anomaly (rank order)
20
25
i
l
e
z
S
y
a
m
o
n
A
e
u
r
T
i
l
e
z
S
y
a
m
o
n
A
e
u
r
T
i
l
e
z
S
y
a
m
o
n
A
e
u
r
T
4
3.5
3
2.5
2