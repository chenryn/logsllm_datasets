consistent for an external observer of the service. For example, when
Proxygen,an L7LB instance,isbeingupdatedwiththe Socket Takeover
mechanism, its state must continue to be perceived as healthy to the
observers in L4LB layer. Occasionally it is possible that the servers
going through deployment in peak hours suffer momentary CPU
and memory pressure, and consequently reply back as unhealthy
to external health monitors for the service. This seemingly momen-
tary flap can escalate to system wide instability due to mis-routing
of packets for existing connections if, for example, the L4LB layer
employs a consistent routing mechanism such as consistent-hash to
pick an L7LB destination server based on the source and destination
addresses in a packet header.
Remediation: To avoid instability in routing due to momentary
shuffle in the routing topology, such as changes in the list of healthy
servers going through a release process using the Socket Takeover
mechanism, we recommend adopting a connection table cache for
the most recent flows. In Facebook we employ a Least Recently Used
(LRU) cache in the Katran (L4LB layer) to absorb such momentary
shuffles and facilitate connections to be routed consistently to the
same end server. Adoption of such mechanism also usually yields
performance improvements.
5.2 Partial Post Replay
Here we discuss potential pitfalls of the store-and-replay solution
regarding HTTP semantics and app. server behavior; while some of
the solutions were part of the original design, some others are less
0.960.981.00[a] Fraction of baseline capacity0.00.20.40.60.81.0CDF across 10 edge clusters5% instances restartedZeroDowntimeHardRestart0.80.91.0[b] Fraction of baseline capacity0.00.20.40.60.81.0CDF across 10 edge clusters20% instances restartedZeroDowntimeHardRestartZero Downtime Release
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
(1) How does Zero Downtime Release fare in comparison with tra-
ditionalreleasetechniquesonperformanceandavailabilitygrounds?
(2) What are the operational benefits of using Zero Downtime Re-
lease at productions scale in terms of minimizing disruptions,
preserving capacity and release scheduling?
(3) What are the system overheads of Zero Downtime Release?
Evaluation Metrics Zero Downtime Release has been operational
at Facebook for multiple years and has assisted in rolling-out thou-
sands of code updates with minimal disruptions. A sophisticated
auditing infrastructure [37, 49] has been built over the years for
real-time monitoring of cluster and user performance, including re-
leases and their implications. Each restarting instance emits a signal
through which its status can be observed in real-time (e.g., health of
the parallel processes, duration of takeover etc.). The instances also
log system benchmarks (e.g., CPU utilization, throughput, Request
per Second (RPS) served etc.) as well as counters for the different
connections (e.g., Number of MQTT connections, HTTP status code
sent, TCP RSTs sent etc.). The monitoring systems also collect per-
formance metrics from the end-user applications and serve as the
source of measuring client-side disruptions (e.g., errors, HTTP codes
sent to user etc.). Our evaluation of Zero Downtime Release, we exam-
ine these data sources to analyze the performance and operational
implications of our Zero Downtime Release framework.
Evaluation Setup In our evaluation, we conduct experiments in
production clusters across the globe, serving live end-user traffic.
Experimenting with live deployments allows us to not only measure
the impact at scale, but also measure the impacts across the differ-
ent protocols. For each system component, we aim to highlight
improvement in target system’s availability, quality of service (QoS)
and their impact on client. We further explore their use in alleviat-
ing the complexity of managing hundreds of production clusters.
Finally, we address the additional costs related to persistent long
haul techniques and explore their impact on performance.
6.1 Comparison with Traditional Release
Tomeasuretheeffectivenessof Zero Downtime Release,weconducted
multiple HardRestart across 10 productions clusters. A HardRestart
mirrors the traditional roll-out process — updates are rolled out in
batches across a cluster and the restarting instances enter the drain-
ing mode (i.e., the server stops receiving new connection until the
end of draining period). Since the goal is to compare against Zero
Downtime Release, we set the same draining duration (20 minutes)
and test two batch sizes (5% and 20%) in ten randomly selected Edge
production clusters. During both restart strategies, we monitor sys-
tem metrics (e.g., idle CPU) and performance counters (e.g., HTTP
and MQTT stats). Furthermore, we analyzed the client-side disrup-
tions by examining performance metrics collected from end-users.
Improvedtimetocompletion. Figure16summarizesCom-
6.1.1
pletion Times of various restart mechanisms for Proxygen and App.
Server releases (i.e., time required to update our global deployments
for either Proxygen and App. Server). We observe that in the me-
dian update, Proxygen releases finish in 1.5 hours, whereas, App.
Server releases are even faster (25 minutes). The major factor behind
the differences in their completion time is the different draining
behavior. Proxygen are configured to drain for 20 minutes while
App. Server have a short draining interval (10-15 seconds) since their
workload is dominated by short-lived requests. As we are going to
Figure (9)
Impact of Downstream Connection Reuse
obvious and were only discovered after deploying the solution in
production.
• PreservingHTTP Semantics:PartialPostReplayisdesigned
to work with any HTTP version; some simple rules must be defined
for each protocol version to make sure that the necessary state is
transferred back to the proxy so that the original request can be
replayed to a different server. As an example HTTP/2 and HTTP/3
request pseudo-headers (beginning with ′ :′) are echoed in the re-
sponse message with a special prefix (e.g. ′pseudo− echo−path :′
for the ′ :path :′ pseudo-header). The most interesting corner cases
however were discovered with HTTP/1.1 and chunked transfer en-
coding where the body stream is split into chunks; each chunk is
preceded by a header that indicates the chunk length and the end of
the body is signaled by a chunk trailer. A proxy implementing PPR
must remember the exact state of forwarding the body to the original
server, whether it is in the middle or at the beginning of a chunk in
order to reconstitute the original chunk headers or recompute them
from the current state.
• Trust the app. server, but always double-check:Asolution
like PPR requires a Proxy and its immediate upstream hop to collab-
orate and implement the client and server-side of the mechanism. In
Facebook infrastructure since we control both sides there is implicit
trust on the app. server doing the right thing and not be malicious.
However, the upstream may also behave as a proxy itself forwarding
responses from another app. server which does not implement PPR
and may be using the HTTP response status code 379. We hit this
case in production, where the web-server acting as a proxy would re-
turn responses from a buggy upstream service returning randomized
HTTPresponsecodesduetoamemorycorruptionbug.Althoughthis
was due to a bug, we realized that there was the need for a more strict
check on the conditions to enable the feature on a specific request.
Remediation: The current implementation and RFC do not de-
fine a negotiation mechanism for the feature and assumes previous
knowledge at the intermediary and server that the peer is going
to support the feature. Also, HTTP response code 379 was specif-
ically picked within an unreserved range in the IANA status code
registry [6] and therefore no assumption can be made on the server
not using that status code for other purposes. To disambiguate then
we used the HTTP Status message, and defined that the proxy must
enable PPR only on seeing a 379 response code with PartialPOST
as the status message.
6 EVALUATION
Our evaluation of our framework, Zero Downtime Release, is moti-
vated by the following practical questions:
0204060Timeline [minutes]0.70.80.91.01.11.21.31.4# Publish msg.DCRwoutDCR020406080Timeline [minutes]0.70.80.91.01.11.21.31.4# New MQTT conn. ACKsDCRwoutDCRSIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Naseer et al.
Figure (10) Packet mis-routing
Figure (11) POST disruption
Figure (12) Proxy errors comparison
show next, Zero Downtime Release preserves capacity and minimizes
while taking order of tens of minutes to restart the tiers.
Improved L7 Cluster Capacity. Katran maintains an up-
6.1.2
dated view of available Proxygen through health-checks. Recall that
performingaHardRestartonaninstancecausesthisinstancetoblock
newconnectionsandthustofailhealth-checks,becausehealth-check
connections are rejected. Whereas Zero Downtime Release enables
the new Proxygen instance to take-over health-check responsibil-
ity. Looking at Katran logs, we observe the expected behavior: Zero
Downtime Restart stays transparent to Katran while, for HardRestart,
the restarted instances are removed from Katran table.
To explore the impact of the two restart approaches on clusters’
available capacity, we measure the idle CPU metrics under the drain-
ing phase of both restart approaches. Figure 8(b) plots the cluster’s to-
talidleCPUresources,normalizedbythebaselineidleCPUresources,
recorded right before the restart. In Socket Takeover (§ 6.3), we expect
an increase in CPU usage because of the parallel process on same
machine, leading to a slight (within 1%) decrease in cluster’s idle CPU.
However, this is radically different from the HardRestart case, where
the cluster’s CPU power degrades linearly with the proportion of
instances restarted because each instance is completely taken offline.
6.1.3 Minimizing User Faced Disruptions. Pub/Sub services
(Downstream Connection Reuse):
To measure MQTT related disruptions, we performed restarts
with and without Downstream Connection Reuse (DCR at the Origin.
Figure 9 highlights its impact on minimizing the client side disrup-
tions. The figure plots a timeline of Publish messages routed through
the tunnel to measure the impact of restarts on communication be-
tween end-users and their MQTT brokers (back-ends). The figure
also plots the median number of new MQTT connections created at
the back-ends, by measuring the number of ACKs sent in response to
MQTT connect messages from end-users. The number represent the
medianacrosstheclustermachinesandarenormalizedbytheirvalue
rightbefore restart. Incontrast toDCRcasewherethe numberofpub-
lished messages do not deteriorate during the restart, we observer a
sharp drop in Publish messages when Downstream Connection Reuse
is not used (woutDCR), indicating disruptions in communication be-
tween users and their MQTT brokers. On the other hand, we observe
a sharp spike in number of ACKs sent for new MQTT connections
for woutDCR case, indicating that the restarting instance terminated
the MQTT connection, leading to the clients retrying to reconnect
with the back-end brokers. With DCR, we do not observe any change
as connections between users and their back-end brokers are not
terminated and, instead, are routed through another Origin Proxygen
to same broker.
Web (Partial Post Replay): In absence of Partial Post Replay (PPR),
the restarting App. Server terminates the in-process POST requests
and returns error code (e.g., HTTP code 500) to the downstream
Proxygen and, eventually, the error code reaches the end-user. In case
of PPR, the server returns a code 379 and the partial POST data which
is then replayed to another App. Server alongside the original request.
To test Partial Post Replay’s effectiveness, we observe App. Server
restartsfromthedownstream Origin Proxygen’svantagepointandin-
spect the POST requests sent to a restarting server. A reception of 379
response, along with the partial request data, signals a request that
would have faced disruption in the absence of Partial Post Replay—
allowing us to measure the scale of disruptions due to App. Server
restarts. Figure 11 compares the Partial Post Replay’s impact by pre-
senting the percentage of connections disrupted across the web tier
for 7 days. Note that App. Server are restarted tens of times a day (Fig-
ure2a)andthe7daysworthofdatacoversaround70webtierrestarts.
We observe that Partial Post Replay is extremely effective at minimiz-
ing the POST requests disruptions. Although the percentage might
seem very small (e.g., 0.0008 at median), there are billions of POST
request per minute for the web-tier and even the small percentages
translate to huge number of requests (e.g, ∼6.8 million for median).
6.1.4 Minimizing Proxy Errors. A major benefit of using Zero
Downtime Release is to improve proxy performance during restart
w.r.t. errors. Errors result in connection terminations or 500 response
codes both of which are highly disruptive to end-user’s performance
and QoE. To measure these disruptions, we measured the errors
sent by the Edge proxy to end-users, under both kind of restarts.
Figure 12 presents the ratio of errors observed for the two restarts
(traditional and Zero Downtime Release). The 4 types of errors cor-
respond to different types of disruptions: (i) Connection Reset (conn.
rst.) refers to sending a TCP RST to terminate the connection, (ii)
Stream abort/unacknowledged refers to errors in HTTP, (iii) Time-
outs refer to TCP level timeouts, (iv) write timeout refers to case
when application times-out due to disruption in underlying connec-
tion. We observe a significant increase in all errors for “traditional”
as compared to Zero Downtime Release. Write timeouts increase by
as much as 16X and are significantly disruptive for user experience
as users can not retry right away.
Impact on consistent packet routing. Next, we measure
6.1.5
the efficacy of Socket Takeover for consistently routing packets to the
right proxy process, in cases where multiple proxies are available
(updated and draining instance). We disable the connection-ID based
1234567891011121314151617181920020# pkts [K]Traditional1234567891011121314151617181920Timeline [minutes]0.00.2# pkts [K]ZeroDowntime0.0000.0010.0020.003% conn. disrupted0.00.20.40.60.81.0CDFPPRwoutPPRConn.rst.Streamabort/unack.TimeoutsWritetimeoutsError type05101520Ratio of # errors(HardRestart / ZeroDowntime)Zero Downtime Release
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Figure (13) Timeline of Proxygen restart
QUIC packet routing and inspect the packets received at both in-
stances. Since the HardRestart case has only one instance running at
a time, no UDP mis-routing exists. In the context of this experiment,
traditional approach refers to the case where sockets are migrated to
the updated instance, but the system lacks connection-ID powered
user-space routing. Figure 10 present the number of UDP packets
mis-routed per instance. A packet is marked mis-routed if the wrong
proxy instance receives it i-e packets bound for the draining instance
are received at updated one. Although we observe some mis-routing
for Zero Downtime Release at start of the restart, their magnitude is
insignificant compared to traditional case, with 100X less packets
mis-routed for the worst case (tail at T=2).
6.2 Operational Benefits at Scale
To evaluate the effectiveness of Zero Downtime Release at scale, we
monitored 66 production cluster restarts across the globe.
6.2.1 Performanceandstabilityimprovements: Figure13shows
a timeline of the system and performance metrics (Requests Per Sec-
ond (RPS), number of active MQTT conn., throughput and CPU
utilization) during releases. The metrics are normalized by the value
just before the release was rolled-out. During each batch restart (20%
of the instances), we collected the target metrics from each cluster in-
stance and Figure 13 plots the distributions (averaged over a minute)
observed across two groups of machines: (i) the 20% restarted (GR),
(ii) the rest of 80% non-restarted (GN R). Observing the two groups
side by side demonstrates standing their behavior during restarts.
The timeline (x-axis in minutes) marks 4 phases: (i) T≤1 state before
restart, (ii) T=2 marks the start of batch restart, (iii) T=24 marks the
endofdrainingperiod,(iv)T≥24stateafterbatchrestartisconcluded.
All the observed metrics are normalized by their values at T=0. We
further present a cluster-wide view in form of the average metrics
calculated across all instances of the cluster. ).
Cluster-widebehavior:AcrossRPSandnumberofMQTTconn.,
we observe virtually no change in cluster-wide average over the
restart period. No significant change in these cluster-wide metrics
after T=2, even with 20% of the cluster restarting, this highlights the
benefits of Zero Downtime Release at scale in practice. We do observe
a small increase in CPU utilization after T=2, attributed to the system
overheadsof Socket Takeover,i.e.,two Proxygeninstancesrunparallel