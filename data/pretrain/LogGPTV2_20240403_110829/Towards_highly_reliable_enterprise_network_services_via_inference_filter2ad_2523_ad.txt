i
b
a
b
o
r
P
 0.2
 0.15
 0.1
 0.05
 0
 0
-wins
-dns
-prxy
-dc
not-dep
 1
 2
 3
 4
 5
 6
 7
 8
number of samples (x 1000)
Figure 11: Dependency probabilities for accessing the web por-
tal converge to stable values as the inference engine receives
more samples from clients.
y
t
i
l
i
b
a
b
o
r
P
 0.2
 0.15
 0.1
 0.05
 0
-wins
-dns
-prxy
-dc
not-dep
 0
 20
 40
 60
 80  100  120  140  160  180  200
number of clients
Figure 12: Dependency probabilities for accessing the web por-
tal converge as the inference engine aggregates samples from
more clients.
ence of such overlap in production environments bodes well for our
techniques, as it means Sherlock can construct succinct Inference
Graphs and Ferret can localize faults with fewer observations.
Figure 10 shows the dependency graph for visiting a major ﬁle
server. As before, clients depend on DNS, WINS, Domain Con-
troller (DC), and proxy servers to access the ﬁle server. Inter-
estingly, clients actually depend on four different ﬁle servers –
FileServerA-FileServerD to access the main ﬁle server. It turns out
that the name of the main ﬁle server is just the root name of a
distributed ﬁle system. The actual ﬁles are stored on several ﬁle
servers, each of which is responsible for a portion of the name
space. The client requests are sent to the ﬁle servers based on the
location of the clients and the requested ﬁles.
To summarize, our observations are three-fold. First, there is sig-
niﬁcant variety in service-level dependencies – some servers redi-
rect a majority of their requests while others exclusively serve the
requests locally. Second, even when two services appear to have
similar dependencies, there are differences in the strength of the
dependencies. For instance, clients may heavily depend on domain
controllers to access certain web servers which contain lots of sen-
sitive information, but this does not apply to accessing the web por-
tal. Finally, dependencies change over the time – we have seen con-
tent move across machines from one building to another. Hence, we
conclude that an automated algorithm for inferring dependencies is
necessary and useful.
Impact of number of samples: Section 4.1 describes how Sher-
lock computes service-level dependency graphs by aggregating the
results from multiple clients. In this section we examine how many
samples are required to produce stable probability estimates. Fig-
ure 11 shows how dependency probabilities for clients accessing
the web portal converge as the algorithm uses more samples. We
E
WS3
AD
DNS1
DNS2
WS1
D
SQL
WS2
B
C1−1
LAN1
...
C1−12
A
C2−1
LAN2
...
C2−11
C
R1
Figure 13: Physical topology of the testbed. Hosts are squares,
routers are circles. Example failure points indicated with cir-
cled letters. Hosts labeled C* are clients and WS* are web-
servers.
WS1
AD
DNS1
C1-1
LAN1
LAN2
R1
WS2
SQL
Server & 
Network 
Deps
C1-1 ↔ WS1
Service 
Deps
C1-1 ↔ gets 
certs from AD
C1-1 ↔ resolves
name with DNS1
C1-1 ↔ WS2 WS2 ↔ SQL
C1-1 fetches 
pages from WS1
C1-1 fetches 
pages from WS2
Figure 14:
Inference graph for client C1−1 accessing
WebServer1 (WS1) and WebServer2 (WS2). For clarity, we
elide the probability on edges, the specialized (failover) meta-
node for DNS1 and DNS2, and the activities of other clients.
show the probabilities for a set of true dependencies and the one
false dependency with the largest probabilities among false depen-
dencies. Note that the probabilities of the four true dependencies
(DC, DNS, WINS, and proxy) quickly exceed those of the false
dependency, even with only 200 samples. At about 4,000 samples,
the probabilities of all the true dependencies converge to their ﬁ-
nal values. The Inference Engine normally receives this number of
samples in a few hours during a regular day. Once converged, we
ﬁnd the service-level dependencies are stable over several days to
a couple of weeks.
Impact of number of clients: Figure 12 shows how dependency
probabilities for clients accessing the web portal converge as Sher-
lock aggregates samples from more clients. We show probabilities
for the same set of dependencies as before. Not surprisingly, when
we aggregate the results from very few clients, the false depen-
dency has a higher probability than some of the true dependen-
cies. Aggregating over even 20 clients reduces the false dependency
probability to a trivial value, showing the importance of aggrega-
tion in eliminating false positives.
6.2 Localizing Faults in Enterprise Network
We now turn our attention to Ferret, the fault localization algo-
rithm. We evaluate Ferret’s ability to localize faults in an enterprise
network and its sensitivity to errors in the inference graph. We also
compare it with prior work.
We begin with a simple but illustrative example where we in-
ject faults in our testbed (Figure 13). The testbed has three web
servers, one in each of the two LANs and one in the data center.
It also has an SQL backend server and supporting DNS and au-
thentication servers (AD). WebServer1 only serves local content
and WebServer2 serves content stored in the SQL database. Note
that the testbed shares routers and links with the production enter-
prise network, so there is substantial real background trafﬁc. We
use packet droppers and rate shapers along with CPU and disk load
generators to create scenarios where any desired subset of clients,
s
t
n
e
i
l
C
e
u
q
n
U
i
f
o
#
.
g
v
A
r
e
v
r
e
S
e
h
t
s
s
e
c
c
a
t
a
h
t
 10000
 1000
 100
 10
 1
 0
1s Window
10s Window
100s Window
1000s Window
10000s Window
 20
 40
 60
 80
 100
 120
Server ID
Figure 15: Average number of unique clients accessing the 128
most popular servers in a 10-second time window. The top 20
servers have more than 70 unique clients in every 10 s window.
servers, routers, and links in the testbed appear as failed or over-
loaded. Speciﬁcally, an overloaded link drops 5% of packets at ran-
dom and an overloaded server has high CPU and disk utilization.
Figure 14 shows the inference graph constructed by Sherlock,
with some details omitted for clarity. The arrows at the bottom-
level are the service-level dependencies inferred by our depen-
dency discovery algorithm. For example, to fetch a web page from
WebServer2, client C1−1 has to communicate with DNS1 for name
resolution and AD for certiﬁcates. WebServer2, in turn, retrieves
the content from the SQL database. Sherlock builds the complete
inference graph from the service-level dependencies as described
in Section 4.2.
Unlike traditional threshold-based fault detection algorithms,
Ferret localizes faults by correlating observations from multiple
vantage points. To give a concrete example, if WebServer1 is over-
loaded, traditional approaches would rely on instrumentation at the
server to raise an alert once the CPU or disk utilization passes a
certain threshold. In contrast, Ferret relies on the clients’ obser-
vations of WebServer1’s performance. Since clients do not expe-
rience problems accessing WebServer2, Ferret can exclude LAN1,
LAN2 and router R1 from the potentially faulty candidates, which
leaves WebServer1 as the only candidate to blame. Ferret formal-
izes this reasoning process into a probabilistic correlation algorithm
(described in Section 3.2) and produces a list of suspects ranked by
their likelihood of being the root cause. In the above case, the top
two root cause suspects are WebServer1 with a likelihood of 99.9%
and Router R1 with a likelihood of 9.0*10−9%. Ferret successfully
identiﬁes the right root cause while the likelihood of the second best
candidate is negligibly small.
Ferret can also deal with multiple simultaneous failures. To il-
lustrate this, we created a scenario where both WebServer1 and one
of the clients C1−3 were overloaded at the same time. In this case,
the top two candidates identiﬁed by Ferret are WebServer1 ∩ C1−3
with a likelihood of 97.8% and WebServer1 with a likelihood of
1.6%. WebServer1 appears by itself as the second best candidate
since failure of that one component explains most of the poor per-
formance seen by clients, and the problems C1−3 reports with other
services might be noise.
Ferret’s fault
localization capability is also affected by the
number of vantage points. For example, in the testbed where
WebServer2 only serves content in the SQL database, Ferret can-
not distinguish between congestion in WebServer2 and congestion
in the database. Observations from clients whose activities depend
on the database but not WebServer2, would resolve the ambiguity.
Ferret’s ability to correctly localize failures depends on having
observations from roughly the same time period that exercise all
paths in the Inference Graph. To estimate the number of observa-
tions available, we measured the average number of unique clients
that access a server during time windows of various sizes. We do
)
x
e
d
n
i
(
e
s
u
a
c
t
o
o
R
350
300
250
200
150
100
50
0
0
Server1
Server2
Link on R1
1
2
Time (days)
3
4
Figure 16: Root causes of performance problems identiﬁed by
Ferret over a 5-day period. Each Y-axis value represents a sep-
arate component in the inference graph and a dot indicates the
component is troubled or down at that time.
this for the 128 most popular servers in our organization using time
window lengths varying from 1 second to 105 seconds (roughly
3 hours). The data for Figure 15 were collected over a 24-hour pe-
riod during a normal business day. It shows that there are many
unique clients that access the same server in the same time window.
For instance, in a time window of 10 seconds, at least 70 unique
clients access every one of the top 20 servers. Given that there are
only 4 unique paths to the data center and 4-6 DNS/WINS servers,
we believe that accesses to the top 20 servers alone provide enough
observations to localize faults occurring at most locations in the
network. Accesses to less popular services leverage this informa-
tion, and need only provide enough observations to localize faults
in unshared components.
6.2.1 Evaluation of Field Deployment
We now report results from deploying Sherlock in our organiza-
tion’s production network. We construct the Inference Graph using
the algorithm described in Section 4.2. The resulting graph contains
2,565 nodes and 358 components that can fail independently.
Figure 16 shows the results of running the Sherlock system over
a 5-day period. Each Y-axis value represents one component, e.g. a
server, a client, a link, or a router, in the inference graph and the X-
axis is time. A dot indicates a component is in the troubled or down
state at a given time. During the 5 days, Ferret found 1,029 in-
stances of performance problems. In each instance, Ferret returned
a list of components ranked by their likelihood of being the root
cause. This ﬁgure illustrates how Sherlock helps network managers
by highlighting the components that cause user-perceived faults.