To execute concrete test cases, Test Engine performs SSOs
and drives the mobile app to the expected state. At the same
time, Test Engine feeds back observations to Test Oracle for
monitoring the state change and identifies potential vulnera-
bilities. Besides, once unexpected app behaviors are detected,
the tool will try to recover the app to the correct state.
We elaborate on the design of each portion in the next section.
3.2 Workflow of MoSSOT
Given the APK of the RP app, MoSSOT works as follows:
(1) The input APK is unpacked to extract necessary informa-
tion, e.g., package name. Then, the app is installed and its
information is passed to UI Explorer and Test Learner.
(2) UI Explorer executes the RP app and tries to find the set of
widgets that lead to a certain page, e.g., SSO login page. The
result is output to Test Engine for automatic UI driving.
(3) Test Learner requests Test Engine to perform normal SSOs
on the RP app. The network traces between the device and
the RP or IdP server are captured.
(4) Test Learner analyzes the network traces and learns the
app-specific customizations.
(5) Based on the specifications and documents (e.g., [21] and
[14]), we build an initial model beforehand, which is then
augmented with the learned app-specific customizations.
(6) With System Model, MoSSOT automatically generates test
cases in the form of abstract HTTP(S) requests. Then, Test
Engine performs SSOs and tampers the network traffic ac-
cording to the test cases.
(7) Test Oracle collects (UI & network) observations from Test
Engine. If the app is detected to perform unexpectedly, the
module will try to recover it from the error state.
(8) Test Oracle extracts the expected system behavior from Sys-
tem Model and compares it with the observations to deter-
mine whether the latter is normal or not.
(9) For any abnormal behavior, MoSSOT determines whether it
is exploitable or not. If so, the tool will output the test case.
(10) If the test case leads to the expected result, i.e., logging into
the RP with the identity in the IdP, System Model will re-
move associated test cases and then output the next one to
Test Engine (i.e., Step 6). The system continues the iterative
process until achieving the pre-defined requirements.
4 DETAILED DESIGN OF MOSSOT
In the section, we first introduce the framework of UI automation.
Then, we describe how to learn the customizations within practical
SSO implementations for constructing a comprehensive system
model. Finally, we talk about the actual model-based testing, give a
running example, and discuss some implementation challenges.
Session 4A: Mobile SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand271Figure 3: System architecture of MoSSOT
4.1 UI Automation Framework
Our testing system requires automated and repeated state manipu-
lation on a large number of mobile apps with only blackbox level
information as described in Challenge 1. To guarantee scalability,
efficiency, and robustness, we have to complete the following tasks:
(1) Targeted UI search: To support large-scale testing, our testing
system needs to automatically find the path to the target
login page of an RP app, where efficiency and accuracy are
the major requirements.
(2) Robust recording and replaying: Our testing needs to per-
form the login process repeatedly so a convenient UI path
recording method and a robust replay tool are essential.
(3) Handling random UI components: Many apps tend to dynam-
ically load remote resources and display them, e.g., adver-
tisements, which introduces randomness and makes the UI
automation complex and unstable.
We design three modules, namely UI Explorer, UI Navigator, and
Noise Reducer, to complete the three tasks. UI Explorer (Module A
in Fig. 3) is used to search for the set of UI elements that drive the
app to the destination page. The result is saved as the UI path and
later fed into UI Navigator, which is integrated into Test Engine
(Module B in Fig. 3) for replaying the UI paths to perform SSOs. On
the other hand, Noise Reducer provides a consistent environment
so that the other two can operate on the tested app robustly. We
discuss each of the three modules in detail here.
4.1.1 Modeling the UI System
Before the discussion, we need to first clarify a few terminolo-
gies. (1) element: Android UI layout, is made up of UI widgets with
several attributes. However, there is no single attribute that can act
as a unique identifier. In our framework, we define an element as
an object with the following properties: type, text, id, desc, click-
able, Xpath, screenshot. Some of these properties directly map to
XML attributes in the UI layout, while the others store additional
information, e.g., screenshot. The identifier of an element, either a
single property or a combination of some, is generated on the fly to
guarantee its uniqueness. (2) page: In most time, pages directly map
to Android activities. However, the content and layout may change
dramatically under the same activity, which appears to users as
multiple pages. Our abstracted page object hence is identified by
both activity name and page layout. (3) UI path: A UI path is de-
fined as an ordered list of elements that need triggering and a set of
matching conditions to identify the target page.
4.1.2 UI Explorer
UI Explorer (Module A in Fig. 3), as indicated by its name, auto-
matically searches for the UI path towards a destination. The input
of the module is simply a desired destination. In our SSO testing,
the destination is configured to be SSO login page and we developed
two exploration algorithms. Algorithm I is a novel heuristic-based
algorithm called level-based keyword scan (LKS), designed for effi-
ciency, while Algorithm II is a depth-first-search (DFS) algorithm
with custom prioritizing, built for higher accuracy. Details of the
algorithms are given in the following paragraphs. For scalability,
both algorithms purely rely on UI information instead of code anal-
ysis like done in [4, 7]. Our approach guarantees that obfuscated
or packed apps can also be tested. Besides, unlike previous work
[2, 12, 20, 32] focusing on test coverage, we aim at finding the target
UI element/page with higher accuracy and speed.
Algorithm I (LKS). The idea of the algorithm is based on the
observation that there are some general semantic patterns when
navigating to the SSO login page. For instance, one commonly seen
path is home → profile → login. To translate it into a heuristic
algorithm, we start with crafting a 2D-list containing keywords
commonly appear in each stage of pages with semantically similar
keywords in the same inner list (level). Based on that, the algorithm
first tries to find a matching keyword in the level representing the
stage of page closest to the destination, then tries consecutive levels
till the farthest one. Given the space restriction, we elaborate on
the algorithm and the generation of keyword list in Appendix B.1.
Algorithm II (DFS). The algorithm is an adapted DFS, where we
model the connection of elements and pages as a directed graph. In
the graph, pages and elements are vertices and transitions between
pages (via interacting with elements) is modeled as directed edges.
Notice that the graph could be cyclic, so our adapted DFS needs
to detect cycles and cut the loops. Meanwhile, within each page,
there are normally tens of elements. Instead of random searching,
a smart ordering improves efficiency greatly. We achieve this by
assigning weights to edges and do a prioritized DFS. In addition,
in our case, both the weight and search depth should be bounded
as most of the low-weight elements are not worth trying and login
page normally sits at a shallow path. Lastly, limited by Android UI
Output:Test Report2. UI pathsIdP / RP6. test   cases9. list of vulnerabilitiesModule C:Test LearnerModule A:UI Explorer(+Noise Reducer)Module B:Test EngineModule D:System ModelModule E:Test OracleInput1: APKInput2: Protocol specs      & IdP docs1(a). app   info3. request toperform normalSSOs4. referencenetwork traces0. initialmodel5. app-specifickey parametershttp(s) request response          7. observed system behavior8. expected system behavior10. rectification1(b). app infoTackling Challenge 3 (in Sec. 4.3)Tackling Challenge 2 (in Sec. 4.2)Tackling Challenge 1 (in Sec. 4.1)(fuzzed)Network traffic Test EngineUI Navigator(+Noise Reducer)Proxy(MitM Proxy)Mobile OS EmulatorInstructions to the appsNetwork traffic to/from the appsIdP & RP appIdP / RPStatus ofthe appsInput1:UI paths(from UIExplorer)Input2:Test cases(from SystemModel)Output:Observations(to Test Oracle)SnapshotsSession 4A: Mobile SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand272behavior, we cannot always simply jump back to the parent vertex,
so we achieve that by returning to the root and revisit the path.
More details of the algorithm are given in Appendix B.2.
4.1.3 UI Navigator
The module is for UI path replay and integrated into Test Engine
as shown in Figure 3. [27] shows that all state-of-the-art record
and replay tools have limitations, which make them less suitable
for production. Our tool, instead of trying to solve this general
problem, targets at providing a robust and efficient UI replay with
only simple user interactions. Instead of using coordinate-driven
navigation as done in [35], this module aims to find and trigger
widgets by their attributes, e.g., text, id, and even XPath, in a way
similar to [1]. This reason behind is that the former is sensitive to
noise, e.g., misaligned page, and scrolling. The exclusive feature of
our tool is the dynamic UI path handling. Given a UI path recorded
in JSON format, the module visits elements one by one till all of
them are consumed and the destination is matched. During the
process, it can happen that some target elements cannot be found
in the current page. Then, Noise Reducer will be triggered to detect
and clear noise until UI Navigator can resume the UI path replay.
4.1.4 Noise Reducer
The randomness of UI components motivates us to design Noise
Reducer, which can heuristically handle the contents considered as
noises, e.g., popups, advertisements, and loading pages. The module
first classifies the noises based on the features extracted from the
current UI layout, e.g., number of clickable elements, keywords,
and widget size. Then, for different types of noises, corresponding
handling strategies will be applied. For instance, the handler will try
to bypass a welcome page by swiping. Once Noise Reducer starts,
it runs in a loop until it cannot detect any noise. In our framework,
Noise Reducer is invoked passively when normal actions (e.g., taps)
fail in the other two modules.
4.2 Modeling the Mobile SSO Protocols
System Model (Module D in Fig. 3) is actually a finite state machine
(FSM). The FSM consists of system states and corresponding actions,
i.e., the necessary triggers for the next state.
4.2.1 Constructing the Initial Model
Firstly, we construct a state machine (i.e., the black path in Fig. 2)
based on the normal SSO process, where each action exactly maps
to the request/ response in Fig. 1. As shown in Fig. 2, each circle
represents a particular state during the SSO process and each edge
stands for a certain message (request/ response) leading to a state
transition. For example, S1 represents the state that the IdP app just
receives the response of last request (Req1), while Req2 corresponds
to the request from the IdP app to its server, seeking RP information.
Nevertheless, as discussed in Section 2.4, IdPs usually customize
their SSO services, which is not included in the normal workflow.
Thus, we also consider the IdP customizations in the initial model.
For example, when the user has authorized the RP before SSO,
each IdP performs differently. Among the three IdPs we study, Sina
Weibo adopts Automatic Authorization, where S1 jumps to S3 in
Fig. 2, while Facebook utilizes a different API to process the SSO
request. In contrast, WeChat always performs the same as the first
Figure 4: The learning process to augment initial model
• The italics highlighted in purple represent the learnt content.
• To avoid clutter, this example only shows the state transitions during
a normal SSO.
SSO attempt. Given the practical scenarios, we separate the involved
states into two versions: one for unauthorized login and the other
for pre-authorized login. System Model thereby can traverse both
situations via Req7, which revokes the authorization.
Besides, when analyzing the practical SSO network traffic, we
find that some RPs tend to deploy the server-to-server logics, e.g.,
Step 7 and Step 8 in Fig. 1, on the client side (RP app). Then, the
optional interaction between the RP app and IdP server becomes
visible to the mobile device. Therefore, we also reserve the corre-
sponding state in our initial model, i.e., S5(b) in Fig. 2, which will be
adaptively switched on/off according to the actual implementations.
4.2.2 Learning App-Specific SSO Implementations
However, the initial state machine is unaware of the app-specific
implementations. In other words, key parameters in each action
and the realization of Req4 and Resp4 (in Fig. 2) are unknown. Thus,
Test Learner (Module C in Fig. 3) needs to analyze practical SSO
network traffic to learn the implementation details.
Test Learner first requests Test Engine (Module B in Fig. 3) to
perform normal SSOs under particular settings, e.g., with different
IdP identities (Alice and Eve) and at a different time (logging as
Eve again). At the same time, the generated network traffic will
be intercepted and saved as reference network traces. As shown
in Fig. 4, Test Learner then performs differential analysis on the
traces to identify key parameters in each action. To confirm the
key parameters, Test Learner replays the request with the param-
eter removed. If the response remains the same, the parameter is
discarded, as the existence of a valuable parameter will affect the
consequent response. Ultimately, each action can be represented
by URL and learnt key parameters.
Another task of Test Learner is to identify the authentication
interaction between the RP app and its server, i.e., Req4 and Resp4 in
Fig. 4. Unlike the use of OAuth for websites, where the interaction is
realized by redirection, the OAuth standard does not define how the
RP app should deliver the received credential, e.g., access token, to
its backend server. In other words, it is implementation-specific and
subject to RP customizations. As such, Test Learner is responsible
for learning individual RP customization by examining the reference
network traces based on some heuristics. For instance, the edit
             Req4   {url: https://RP.com/sso,                                                learnt_parameter: [,        ]}ReferenceSSO trace(Alice)Req3Req2Req1S3S0(init)S1S2S4Req5Resp4S6(a)S5(a)https://api.weibo.cn/2/account/login?networktype=4g&state=40d72bad&session_id=2A4..&aid=01Anwm...https://api.weibo.cn/2/account/login?networktype=4g&state=40d7e2c3&session_id=2CE..&aid=01Anwm...ReferenceSSO trace(Eve)             {url: https://api.weibo.cn/2/account/login (known from IdP specs),                        learnt_parameter: [ , ]}ReferenceSSO TracesLearn                                                            {learnt_parameter:  (reflecting RP login status),                  extracted_value: [38006(from Alice’strace) , 38045(from Eve’s trace)]}LearnDifferential AnalysisSession 4A: Mobile SecurityAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand273distance between the domain name of the request URL and the
package name of the tested app (from Step 1(b) in Fig. 3) tends
to be small, e.g., douban.com and com.douban.movie. Besides, the
response (Resp4) should contain a user-identifier that are user-
dependent and session-independent. Then, Test Learner will record
the user identifier values, e.g., 38006 (Alice) and 38045 (Eve), within
the corresponding response (i.e., Resp4 in Fig. 4), so that Test Oracle
(Section. 4.3.2) can identify the RP login status in the test later.
4.2.3 Generating and Refining Test Cases
With the learnt parameters in each action, MoSSOT would gen-
erate test cases for the actual testing. Given the capability of the
attacker (defined in Section 2.5), he is able to tamper SSO-related
network traffic. For example, he may replace the access token by a
stolen one (from the same or a different RP app) in Req4 (i.e., Step 6
in Fig. 1). Once the verification within the RP server is incomplete/
incorrect, he may cheat the server into authenticating himself as
the victim. Thus, we design four types of test cases based on the
learnt key parameters:
(1) Remove or randomize a single parameter;
(2) Replace a single parameter with the one from a parallel
session (on the same app with a different identity);
(3) Replace a single parameter with the one from a different
session (on a different app with a different identity);
(4) Replace two parameters simultaneously with the ones from
a parallel/ different session.
The values for replacement are from the reference network traces,
so we will only log into the app with testing accounts without
affecting normal users. The property of protocol-defined parameters
will also be considered. For example, we prepare one extra unused