### Performance Analysis and Failure Recovery Experiments

#### 6.1 Performance Analysis
In our largest configuration, which includes 8 slave nodes using InnoDB, the system achieved throughputs of 14.6 for the ordering, shopping, and browsing workloads, respectively. The addition of an in-memory tier significantly improves performance even in the smallest configuration due to its superior speed. System throughput scales nearly linearly with increases in the in-memory tier size for browsing and shopping workloads, but less so for the ordering workload. This discrepancy is attributed to the saturation of the master database with update transactions, including lock waiting on the master, as a result of the costly index updates (due to rebalancing for inserts in the RB-tree index data structure). The read-only transactions aborted due to version inconsistency are below 2.5% of the total number of transactions in all experiments.

#### 6.2 Failure Reconfiguration Experiments
This section evaluates the system's fault tolerance and reconfiguration capabilities after node failures. We first present a fault tolerance experiment involving the reintegration of a failed node after recovery. Subsequently, we focus on failover to backup nodes, analyzing the impact of three key components: cleanup of partially propagated updates for aborted transactions, data migration, and buffer cache warmup. For this purpose, we inject faults into either the master or slave nodes in the in-memory tier and measure reconfiguration times in the following scenarios:

- **Stale Backup Case**: Master or active slave failure followed by reintegration of the failed node or integration of a stale backup.
- **Up-to-date Cold Backup Case**: Master or active slave failure followed by integration of an up-to-date cold backup node.
- **Up-to-date Warm Backup Case**: Master or active slave failure followed by integration of an up-to-date and warm spare backup node.

We also compare our failover times with those of a standalone on-disk InnoDB database.

##### 6.2.1 Fault Tolerance with Node Reintegration Experiment
We evaluate the performance of the node reintegration algorithm introduced in Section 4. This algorithm allows any failed node to be reallocated to the workload after recovery, implying a period of node downtime (e.g., due to node reboot).

In our test cluster configuration, we use a master database and 4 slave replicas running the shopping TPC-W workload. Figure 4 illustrates the effect of reintegration on both throughput and latency. We simulate the most complex recovery case, where the master fails at 720 seconds due to a machine reboot. The graph shows that the system adapts instantly, with throughput and latency degrading by approximately 20%. Since all slave databases are active and execute transactions, their buffer caches remain warm, preventing throughput from dropping further.

After a 6-minute reboot (depicted by the line in the upper part of the graph), the failed node resumes operation and subscribes with the scheduler. The scheduler then initiates the reintegration process. With a checkpoint period of 40 minutes, this experiment represents the worst-case scenario where all modifications since the start of the run need to be transferred to the joining node. It takes about 5 seconds for the node to catch up with the missed updates, and an additional 50 to 60 seconds to warm up the in-memory buffer cache, after which throughput returns to normal.

The next section provides a more detailed breakdown of the different recovery phases.

#### 6.3 Failover Experiments
We evaluate the performance of automatic reconfiguration using failover to spare backup nodes. In these experiments, we designate several databases as backup nodes and bring an active node down. The system immediately reconfigures by integrating a spare node into the computation.

We measure the time to restore peak performance and run the TPC-W shopping mix, averaging throughput and latency over 20-second intervals. Depending on the state of the spare backup, we differentiate failover scenarios into: stale backup, up-to-date cold backup, and up-to-date warm backup.

##### Stale Backup
As a baseline, we first show the results of failover in a dynamic content server using a replicated on-disk InnoDB back-end. This system is representative of state-of-the-art replicated solutions where asynchrony is used for scaling.

In this experiment, the InnoDB replicated tier contains two active nodes and one passive backup. The two active nodes are kept up-to-date using a conflict-aware scheduler, and both process read-only queries. The spare node is updated every 30 minutes. Figures 5(a) and 5(b) show the failover effect when one of the active nodes is killed after 30 minutes of execution. Service capacity is halved for nearly 3 minutes, with corresponding lower throughput and higher latency.

We conduct a similar experiment with our in-memory tier, using a master, two active slaves, and a 30-minute stale backup. We kill the master node to generate the worst-case failover scenario, including master reconfiguration. Figures 5(c) and 5(d) show the results, with a total failover time of about 70 seconds, less than a third of the InnoDB failover time.

Figure 6 breaks down the time spent in the three failover stages. Most of the failover time in our in-memory Dynamic Multiversioning (DMV) system is due to buffer-cache warm-up. The figure also compares the durations of the failover stages between InnoDB and DMV. The database update time, during which the log is replayed onto the backup, constitutes a significant 94-second fraction of the total failover time in the InnoDB case. In contrast, the catch-up stage is considerably reduced in our in-memory tier, where only in-memory pages are transferred to the backup node. The cache warm-up times are similar for both schemes. For the DMV case, there is an additional 6-second clean-up period during which partially committed update transactions are aborted due to the master failure and reconfiguration.

##### Up-to-date Cold Backup
In this suite of experiments, the spare node is always kept in sync with the rest of the system by sending it the log of modifications. To emphasize the buffer warmup phase, we use a larger database configuration with 400K customers and 100K items, yielding a database size of 800MB and a resident working set of approximately 460MB. We use a three-node cluster: one master, one active slave, and one backup.

In the first experiment, the buffer cache of the spare node is cold, so upon failover, the database needs to swap in a significant amount of data before achieving peak performance. We run the TPC-W shopping mix and kill the active slave database after approximately 17 minutes (1030 seconds) of running time. Figure 7 shows the perceived throughput for the duration of the cold backup experiment.

##### Up-to-date Warm Backup
In this section, we investigate the effect of our techniques for mitigating the warm-up effect. In the first case, the scheduler sends 1% of the read-only workload to the spare backup node. We conduct the same experiment with the same configuration and kill the active slave database at the same point during the run as in the previous experiment. The system reconfigures to include the spare backup. Figure 8 shows the throughput for this case. The effect of the failure is almost unnoticeable because the most frequently referenced pages are already in the cache.