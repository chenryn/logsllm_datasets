and 14.6 in our largest conﬁgurationwith 8 slaves with Inn-
oDB for the ordering, shopping and browsing mixes respec-
tively. Furthermore, we can see that a performance jump
is seen from adding the in-memory tier even in the small-
est conﬁguration due to its superior speed. Finally, sys-
tem throughput scales close to linearly with increases in in-
memory tier size for the browsing and shopping mixes and
less well for the ordering mix. This is caused by saturation
of our master database with update transactions including
lock waiting on the master as a side-effect of the costly in-
dex updates in our system (due to rebalancing for inserts in
the RB-tree index data structure). The read-only transac-
tions aborted due to version inconsistency are below 2.5%
out of the total number of transactions in all experiments.
6.2. Failure Recon(cid:12)guration Experiments
In this section, we ﬁrstshow a fault tolerance experiment
with reintegration of a failed node after recovery. Next, we
concentrate on fail-over onto backup nodes and we show
the impact of the three components of the fail-over path on
performance in our system: cleanup of partially propagated
0
500
1000
1500
Time (seconds)
Throughput
# of Slaves
Latency
Figure 4. Node reintegration (shopping mix).
updates for aborted transactions, data migration and buffer
cache warmup. For this purpose, we inject faults of either
master or slaves in the in-memory tier and show reconﬁgu-
ration times in the following scenarios:
i) Stale backup case: Master or active slave failure with
reintegration of the failed node or integration of a stale
ii) Up-to-date cold backup case: Master
backup node.
or active slave failure followed by integration of an up-to-
iii) Up-
date spare backup node with cold buffer cache.
to-date warm backup case: Master or active slave failure
followed by integration of an up-to-date and warm spare
backup node. We also compare our fail-over times with the
fail-over time of a stand-alone on-disk InnoDB database.
6.2.1 Fault Tolerance with Node Reintegration Exper-
iment
In this section, we evaluate the performance of the node
reintegration algorithm we introduced in section 4. The
algorithm permits any failed node to be reallocated to the
workload after recovery. This implies a period of node
down-time (e.g., due to node reboot).
We use the master database and 4 slave replicas in the
test cluster conﬁguration, running the shopping TPC-W
workload. Figure 4 shows the effect of reintegration on both
throughput and latency.
We consider the most complex recovery case, that of
master failure by killing the master database at 720 seconds
by initiating a machines reboot. We see from the graph that
the system adapts to this situation instantaneously with the
throughput and latency gracefully degrading by a fraction
of 20%. Since all slave databases are active and execute
transactions their buffer caches are implicitly warm. Hence
throughput drops no lower than the level supported by the
fewer remaining slave replicas.
After 6 minutes of reboot time (depicted by the line in
the upper part of the graph), the failed node is up and run-
ning again, and after it subscribes with the scheduler, the
scheduler initiates its reintegration into workload process-
ing as a slave replica. Since we used a checkpoint period of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:31:27 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 200740 minutes, this experiment shows the worst case scenario
where all modiﬁcationssince the start of the run need to be
transfered to the joining node. It takes about 5 seconds for
the joining node to catch up with the missed database up-
dates. After the node has been reintegrated, it takes another
50 to 60 seconds to warm-up the in-memory buffer cache of
the new node, after which the throughput is back to normal.
The next section provides a more precise breakdown of the
different recovery phases.
6.3. Failover Experiments
In this section we evaluate the performance of our au-
tomatic reconﬁguration using fail-over on spare back-up
nodes.
In all of the following experiments we designate
several databases as backup nodes and bring an active node
down. The system immediately reconﬁgures by integrat-
ing a spare node into the computation immediately after the
failure.
We measure the effect of the failure as the time to re-
store operation at peak performance. We run the TPC-W
shopping mix and measure the throughput and latency that
the client perceives, averaged over 20 second intervals.
Depending on the state of the spare backup, we differen-
tiate failover scenarios into: stale backup, up-to-date cold
backup and up-to-date warm backup. In the stale backup
experiments, the spare node may be behind the rest of the
nodes in the system, so both catch-up time and buffer warm-
up time are involved on fail-over. In the up-to-date experi-
ments, the spare node is in sync with the rest of the nodes
in the system, but a certain amount of buffer warm-up time
may be involved.
Stale Backup
As a baseline for comparison, we ﬁrst show the results of
fail-over in a dynamic content server using a replicated
on-disk InnoDB back-end. This system is representative
for state-of-the-art replicated solutions where asynchrony is
used for scaling a workload to a number of replicas.
In this experiment, the InnoDB replicated tier contains
two active nodes and one passive backup. The two ac-
tive nodes are kept up-to-date using a conﬂict-a ware sched-
uler [6] and both process read-only queries. The spare node
is updated every 30 minutes. Figures 5(a) and 5(b) show
the failover effect in an experiment where we kill one of the
active nodes after 30 minutes of execution time. We can see
from the ﬁgurethat service is at half its capacity for close
to 3 minutes in terms of lowered throughput, and higher la-
tency correspondingly.
We conduct a similar experiment interposing our in-
memory tier, having a master and two active slaves and one
30 minute stale backup. We used two active slaves, because
)
S
P
W
I
(
t
u
p
h
g
u
o
r
h
T
)
S
P
W
I
(
t
u
p
h
g
u
o
r
h
T
45
40
35
30
25
20
15
10
5
0
200
700
1200
1700
Time (seconds)
2200
2700
)
s
d
n
o
c
e
s
(
y
c
n
e
t
a
L
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
200
700
1200
1700
Time (seconds)
2200
2700
(a) InnoDB Throughput (2
nodes)
InnoDB Latency
(b)
nodes)
(2
140
120
100
80
60
40
20
0
200
700
1200
1700
Time (seconds)
2200
2700
(c) DMV Throughput (M +
2S)
)
s
d
n
o
c
e
s
(
y
c
n
e
t
a
L
5
4.5
4
3.5
3
2.5
2
1.5
1
0.5
0
200
700
1200
1700
Time (seconds)
2200
2700
(d) DMV Latency (M + 2S)
Figure 5. Failover onto stale backup: compar-
ison of InnoDB and DMV databases.
in the normal operation of our scheme the master is kept
lightly loaded and does not execute read-only transactions.
Subsequently, we kill the master node to generate the worst-
case fail-over scenario that includes master reconﬁguration.
The results are presented in ﬁgures 5(c) and 5(d). In this
case, the total failover time is about 70 seconds, less than
a third of the InnoDB fail-over time in the previous experi-
ment.
Furthermore, from the breakdown of the time spent in
the three fail-over stages presented in Figure 6, we can see
that most of the fail-over time in our in-memory Dynamic
Multiversioning based system is caused by the buffer-cache
warm-up effect. The ﬁgurealso compares the durations of
the failover stages between the InnoDB and in-memory Dy-
namic Multiversioning cases. We can see that the database
update time during which the database log is replayed onto
the backup (DB Update) constitutes a signiﬁcant94 second
fraction of the total fail-over time in the InnoDB case. This
time reﬂects the cost of reading and replaying on-disk logs.
In contrast, the catch up stage is considerably reduced in
our in-memory tier where only in-memory pages are trans-
fered to the backup node. The cache warm-up times are
similar for both schemes. For the DMV case, there is an ad-
ditional 6 second clean-up period (Recovery), during which
partially committed update transactions need to be aborted
due to the master failure and master reconﬁgurationoccurs.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:31:27 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007)
s
d
n
o
c
e
s
(
e
m
T
i
160.00
140.00
120.00
100.00
80.00
60.00
40.00
20.00
0.00
Recovery
DB Update
Cache Warm-up
InnoDB
DMV
Figure 6. Failover stage weights: cleanup
(Recovery), data migration (DB Update) and
Buffer cache warmup (Cache Warmup)
Up-to-date Cold Backup
In this suite of experiments, the spare node is always kept
in sync with the rest of the system by sending it the log of
modiﬁcations.
In order to emphasize the buffer warmup phase during
failover, we used a slightly larger database conﬁguration
comprised of 400K customers and 100K items. This yielded
a database size of 800MB and a resident working set of ap-
proximately 460MB. We use a three-node cluster: one mas-
ter, one active slave and one backup.
In this ﬁrstexperiment, the buffer cache of the spare node
is cold, so upon fail-over the database needs to swap-in a
signiﬁcant amount of data, before achieving peak perfor-
mance. We run the TPC-W shopping mix and after approx-
imately 17 minutes (1030 seconds) of running time, we kill
the active slave database forcing the system to start integrat-
ing the cold backup. Figure 7 shows the perceived through-
put for the duration of the cold backup experiment.
the peak throughput is restored.
Up-to-date Warm Backup
In this section, we investigate the effect on fail-over perfor-
mance of our techniques for mitigating the warm-up effect.
In the ﬁrstcase, the scheduler sends 1% of the read-only
workload to the spare backup node. We conduct the same
experiment with the same conﬁguration as above and we
kill the active slave database at the same point during the
run as in the previous experiment. As before, the system
reconﬁguresto include the spare backup. Figure 8 shows
the throughput for this case. The effect of the failure is al-
most unnoticeable due to the fact that the most frequently
referenced pages are in the cache.
)
S
P
W
I
(
t
u
p
h
g
u
o
r
h
T
70
60
50
40