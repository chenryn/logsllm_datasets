4465672
3631742
80000
15927
16110
2793
2394
1376980
1309872
160000
17158
17279
1836
1810
458622
488967
Table 1: Experimental evaluation of the Poisson model.
√
R). Hence requiring that the edge density not be close to 1 (so that no clique of size
density is O(N/
N or more exists by accident due to edge density) would imply R = Ω(N 2): once again, we see that
this statement appears critical to the success of our algorithm.
C.4 Experimental Validation of the Poisson Model
To evaluate the accuracy of the estimates given in the previous sections, and retroactively validate
the assumptions and approximations made in the course of computing these estimates, we have run
experiments computing the predictions of our model for the number of distinct volumes, as well as
vertices and edges in the graph, compared to the experimental value of these same numbers obtained
by running the actual algorithm on a uniform data distribution and averaging over 30 runs, for various
choices of N and R. The results are given in Table 1.
In each case, we have compared the estimates from our model to experimental values, for R =
N 2/2, N 2, 2N 2, 4N 2. It can be seen that the estimates provided by our model are fairly accurate,
especially regarding the number of distinct volumes. The estimated number of edges fares the worst,
being sometimes 40% lower than the experimental value—although that is still in the right order of
magnitude, which was our goal. Below N 2/2, the model breaks down. Nevertheless the model does
provide reasonably accurate estimates within the relevant parameter range where R is close to N 2.
36
D Query Complexity of Update Recovery Attack
In this section we present an analysis of the query complexity of the update attack from Section 4,
based on the same model as Appendix C.
We assume that the value k of the new record being added into the database is less than N/2. Due
to the reﬂection symmetry, this is without loss of generality. For simplicity we assume N is a multiple
of 4 (the analysis in other cases is very similar). For the sake of the analysis, a distribution needs to be
assumed on the set of queries: for that purpose we assume that range queries are uniformly distributed.
This is not a requirement of the algorithm.
As a preliminary step, let us assume that for every volume, one of the two conditions in lines 12
and 14 of Algorithm 4 is satisﬁed. That is, for every range query, the adversary does learn from its
volume whether the range matches the value k of the new record, and what the queried range is. The
question is then how fast that information enables the adversary to home in on k.
What needs to happen for the adversary to determine k exactly is that the singleton {k} should be
constructible from the set of queried ranges using basic set operations (intersections and set diﬀerences).
A suﬃcient condition is simply that a range [k, y] is queried with y ∈ [k, k + N/4], and another range
[k + 1, y(cid:48)] is queried with y(cid:48) ∈ [k + N/4, k + N/2]. Indeed in that case the ﬁrst query will trigger line 12
of Algorithm 4, and so the set of possible values will be intersected with [k, y]; while the second query
triggers line 14 and ensures that [k + 1, y(cid:48)] is subtracted from the set of possible values. It follows that
after these two queries the set of possible values is included in [k, y] \ [k + 1, y(cid:48)] = {k}.
Assuming uniform queries, the probability of a query of the ﬁrst type is:
N/4 + 1
N (N + 1)/2
≥ 1
2N
.
The same result holds for a query of the second type. It follows that the expected number of queries
before a query of each type has occurred is O(N ).2
The previous analysis, however, assumes that for every query, the adversary is able to determine the
queried range from its volume, and whether it includes the new record—that is, either one of the two
conditions in lines 12 and 14 of Algorithm 4 is satisﬁed. In reality, it is possible that neither condition
is fulﬁlled (as a result of collisions in range volumes). The question then is how much this impacts
query complexity. What we are going to show is that in the same parameter range where our main
√
algorithm succeeds, i.e. essentially R = Ω(N 2), the previous results still hold (albeit with a worse
constant, dependant on N/
If we focus on the ﬁrst condition on line 12, a query containing the target value k is discarded by
the algorithm whenever its volume v is such that v or v − 1 is in collision with the volume of another
range. According to the Poisson model introduced in Section 3.2 (and developed in Appendix C), the
probability that a range of length d is in collision with the volume of another range can be approximated
by:
R).
.
(2)
(cid:112)
N − d
πRd/N
2
Since we are in the parameter range where R = Ω(N 2), and assuming a uniform distribution of records
across values for the sake of the Poisson model, a range of length d contains on average dR/N = Ω(N )
records; so we are dealing with Poisson distributions with parameters Ω(N ), which entails that the
probability of two values drawn according to such a distribution being equal, or diﬀering by 1, is nearly
the same (more formally this amounts to approximating the corresponding Skellam distribution at 1
by its value at 0, which is a very good approximation in this setting). By the union bound, this means
2Conversely, it is straightforward to prove that the expected number of queries needed for exact recovery is Ω(N ).
37
that the probability that either v or v − 1 is in collision with another volume can be upper-bounded by
twice the value of Equation (2).
line 12 in the algorithm can be lower-bounded by 1 − (N − d)/
Hence the probability that a range of length d containing the target value k passes the condition of
It follows that the probability that a query on range [k, y] with y ∈ [k, k + N/4] is issued, and that
(cid:112)
πRd/N .
the query passes the condition of line 12 can be lower-bounded by:
N/4(cid:88)
(cid:16)
d=1
2
N 2
1 − N − d(cid:112)
πRd/N
(cid:16)
(cid:17) ≈ 1
2N
(cid:17)
.
1 − 4N√
πR
Hence the expected number of queries until both events occur is still O(N ). Thus, as long as
R = Ω(N 2), the behavior is the same as in the initial analysis where the eﬀects of volume collisions
were disregarded. In the end, the value k of the new record is recovered within an expected number of
queries O(N ).
Let us now turn our attention to the approximate recovery of k within an additive error N . From
now on we assume k ≥ N holds.3 The previous reasoning can be adapted to this case in a natural way:
instead of requiring a range [k, y] with y ∈ [k, k+N/4], and a range [k+1, y(cid:48)] with y(cid:48) ∈ [k+N/4, k+N/2],
which together allow exact recovery of k, it suﬃces to ask for a range [x, y] with x ∈ [k − N, k] and
y ∈ [k, k + N/4− 1], and a range [x(cid:48), y(cid:48)] with x(cid:48) ∈ [k + 1, k + N ] y(cid:48) ∈ [k + N/4, k + N/2]. Indeed the set
diﬀerence of these two ranges is included in [k − N, k + N ]. Observe that compared to exact recovery,
the number of possible ranges has been multiplied by N , and so the probability of a uniform query
being of either type becomes O() instead of O(1/N ) in the exact case. Hence the same reasoning as
the exact case yields that the required number of queries is O(1/).
In the end, if R = Ω(N 2), our model predicts that the required number of queries for exact recovery
is O(N ), and approximate recovery within N requires O(1/) queries. Of course the previous analysis
relies on the heuristics of our Poisson model, including assuming a uniform distribution of records across
values. In Section 4.2 we evaluate the accuracy of our model by running Algorithm 4 on real-world
datasets.
E Query Reconstruction When Exact Counts Are Known
In this section we describe our experiment which tested the ability of an adversary to reconstruct queries
with knowledge of the exact counts of every element in the database. It is reasonable to suspect that
knowing exact counts makes query reconstruction trivial; however in Appendix C we showed that in
certain parameter ranges, many volume collisions (i.e. two queries having the same volume) occur.
When two queries have the same volume they cannot be distinguished, even with exact counts. Thus,
the relevant quantity for the eﬀectiveness of query reconstruction with exact counts is the number of
queries with unique volumes or with volumes that very few other queries also have. Our experiment
measures this by computing the volume of every query for every HCUP hospital and counting the
number that have unique volumes. We also count the number of queries with volumes that at most ﬁve
other queries have. The results of this experiment are in Figure 8. The results for diﬀerent years of
HCUP data are not meaningfully diﬀerent, so we present only one experiment for each attribute.
3Such an assumption is necessary; e.g. if k = o(N ), then the probability that a uniform query contains k is o(), and
from there it can be shown that ω(1/) uniform queries will be required for any algorithm to recover k within N .
38
Attribute
Size # Queries % Unique % w/in 5
AGEDAY
LOS
AGE
NCHRONIC
NDX
NPR
AMONTH
ZIPINC
MRISK
SEV
365
365
91
26
16
16
12
4
4
4
66795
66795
4186
351
136
136
78
10
10
10
0
0
15
25
70
10
95
100
100
100
0
0
70
29
100
26
100
100
100
100
Figure 8: Per-attribute results for query reconstruction with exact counts.
F Correctness Proofs
This section contains proof of correctness for some algorithms used in the main body.
Lemma 1 (Correctness of Alg. 1). Let DB be a database with at most N diﬀerent values, let V be
the set of all possible range query volumes, and let Velem be the set of elementary range volumes that
contains the minimum complemented volume. Then, after running Algorithm 1 on (N, V ) to obtain the
sets Vnec and Vcand of necessary and candidate nodes, we have
Vnec ⊆ Velem ⊆ Vcand.
Proof. We show that Vnec ⊆ Velem ⊆ Vcand holds throughout Alg. 1. After line 11, we have Vnec ⊆ Velem
since R = vol([1, N ]) is in Velem, and vmin is in Velem by design. After line 12, we have Velem ⊆ Vcand since
all elementary volumes are complemented. To complete the proof, we show that if Vnec ⊆ Velem ⊆ Vcand,
then (i) Augment NEC (Vcand, Vnec) ⊆ Velem, and (ii) Velem ⊆ Reduce CAND(Vcand, Vnec).
First, consider the three ways in which Augment NEC can add elements to Vnec.
• (line 23) If |Vcand| = Nmin and Velem ⊆ Vcand, then clearly Velem = Vcand since |Velem| ≥ Nmin.
• (line 26) Let e be a non-complemented volume. Since Velem ⊆ Vcand, we know that every volume,
including e, arises as a node or an edge (or both) in the graph induced by Vcand. The volume e
has no complement, so it must arise as an edge, i.e., as the absolute diﬀerence of two volumes in
Vcand. If all such edges are incident to one node Vcand, then it must be in Velem.
• (line 30) Let v be a non-necessary complemented volume in Vcand. Every volume, including v,
arises as a node or an edge (or both) in the graph induced by Vcand. If the volume v arises only
as itself and maybe edges incident to itself, then it must be in Velem.
Next, consider Reduce CAND. Let v be a non-necessary complemented volume in Vcand. Since Vnec ⊆
Velem, and the volumes in Velem are all adjacent to each other, any node that is not adjacent to a subset
of volumes in Velem cannot be in Velem.
39
Lemma 2 (Correctness of Alg. 2). Let DB be a database of elements with N possible diﬀerent values,
let V be the set of all range query volumes, and let Velem be the set of elementary range volumes that
contains the minimum complemented volume. Suppose we are given two sets Vnec and Vcand such that
Vnec ⊆ Velem ⊆ Vcand. Then, after running Algorithm 2 on (N, Vcand, Vnec, V ) to obtain the set solutions,
we have
• Velem ∈ solutions if 0 /∈ V (the data is dense), or
• Velem ⊇ s for at least one s ∈ solutions (if the data is sparse).
(cid:108)−0.5 + 0.5 ·(cid:112)
1 + 8 · |V |(cid:109)
Proof. First, if the data is dense (0 /∈ V ), then clearly the number of elementary volumes is Nmin =
Nmax = N . If the data is sparse, there must be at least Nmin
elementary
volumes, otherwise there would be strictly fewer than |V | range query volumes. There can be at most
N − 1 because the occurrence of the volume 0 means at least one of the N values did not appear in
DB, and at most |Vcand| because it is known that Velem ⊆ Vcand.
def
=
Now consider the graph G = (Vcand, E) with edge set
def
= {(v1, v2) ∈ Vcand × Vcand : |v2 − v1| ∈ V \ {0}}
E
def
= G(Vcand \ Vnec). Since the subgraph induced by Velem is a clique in
and the induced subgraph Gnn
G, the subgraph induced by Velem \ Vnec will also be a clique in Gnn. Therefore, at least one of the
k , will have Velem \ Vnec as
maximal cliques in Gnn output by Find Maximal Cliques (line 14), say V ∗
k must be at least Nmin − |Vnec|, so the algorithm will proceed to line 19 in
a subclique. The size of V ∗
k } generates all volumes in V (and maybe others), solutions will
this iteration. Since Velem ⊆ {Vnec ∪ V ∗
be updated to include the output of Min Subcliques (line 20).
Since Velem \ Vnec is a subset of V ∗
k , it will arise as a subclique on line 39 of Min Subcliques. Velem
generates all volumes in V and no others, so the algorithm will proceed to line 41. If the data is dense,
then subcliques may contain only sets of size N = Nmin = Nmax, so Velem cannot be a superset of any
other element in subcliques, so it will be added to this set. If the data is not dense, however, then either
(i) there is already a strict subset of Velem in subcliques that generates exactly the volumes in V , or
(ii) there is no such set, and Velem is added to subcliques. In all cases, any element added to subcliques
in Min Subcliques will form part of the solutions output by Get Elem Volumes, completing the
proof.
40