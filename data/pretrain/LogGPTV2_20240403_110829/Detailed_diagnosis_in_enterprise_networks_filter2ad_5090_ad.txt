H
C
H
B
E
H
L
D
Figure ʈ. An example dependency graph. ˆe labels on edges de-
note whether the computed weight was high (H) or low (L).
causes may also have high weight edges leading to the e(cid:11)ects of inter-
est.ˆese include those that lie along paths from responsible causes
but may also include others if weights on those edges overestimate
the impact.
As an example, consider the dependency graph in Figure ʈ. For
simplicity, we show whether the edge weight is high (H) or low (L)
instead of numeric values. Assume that we set out to diagnose the
abnormal behavior of the component labeled E and that the real cul-
prit C is impacting it through B. Accordingly, C is connected to E
through a path of high weight edges, but so are B and D (via the path
D-B-E). Let us further assume that C is also hurting A and that the
high weight from D to B is erroneous.
Our goal is to rank causes such that more likely culprits have lower
ranks. A compact representation of our ranking function is shown
in Figure ʉ. ˆe rank of a component c with respect to an a(cid:11)ected
component of interest e is based on the product of two measures, and
components with larger products are ranked lower. ˆe (cid:12)rst measure
I(c→e) is the impact from c to e. ˆe second measure S(c) is a score
of the global impact of c.
Together, the two measures help achieve our goal. ˆe impact
I(c→e) from one component to another is the maximum weight
across all acyclic paths between them, where path weight is the geo-
metric mean of edge weights. Per this measure, in Figure ʈ, B, C, and
D have high impact on E but A has a low impact. ˆe score S(c) of a
component is the weighted sum of its impact on each other compo-
nent in the network, where the abnormality of the component is used
as the weight. Components that are highly impacting more abnormal
components will have a higher score. Per this measure, in Figure ʈ, C
will have a lower rank than B and D, despite the inaccurate weight on
the D− B edge because it has high impact to many abnormal nodes.
Of course, in any given situation whether the real culprit gets a low
rank depends on the exact values of edges weights and component
abnormalities. We (cid:12)nd in our evaluation that real culprits have low
ranks the vast majority of the time.
6.
IMPLEMENTATION
We have implemented NetMedic on the Windows platform. Our
implementation has two parts—data collection and analysis. ˆe (cid:12)rst
part captures and stores the state of various components. ˆe second
part uses the stored data to generate the dependency graph and con-
duct diagnosis.
ˆe main source of data is the Windows Performance Counter
framework [ʅʃ]. Using this framework, the operating system (OS)
and applications export named counters and update their values.
Each counter represents a di(cid:11)erent aspect of the exporter’s behavior.
“Performance” is a misnomer for this framework because it exposes
non-performance aspects as well. ˆe OS exports many machine-
wide counters such as processor and memory usage. It also exports
generic process-level aspects such as resource consumption levels. In
addition, many processes export application-speci(cid:12)c counters. See
Table ʆ for some counters exported by the Web server.
NetMedic reads the values of all exported counters periodically.
We do not interpret what a counter represents but simply make each
counter a state variable of the component to which it belongs. While
most counters represent values since the last time they were read,
some represent cumulative values such as the number of exceptions
since the process started. We identify such counters and recover their
current behavior by subtracting the values at successive readings.
ˆe Performance Counter interface does not tell us which pro-
cesses in the network are communicating with each other. We use
Rank(c→e) ∝ (I(c→e)· S(c))−1
I(c→e) = max(weight W(p) of acyclic paths p from c to e)
W(p) = (cid:16)∏n
n where e1 ··· en are edges of the path,
1 if c = e
j=1 E(ej)(cid:17)
1
S(c) = ∑e∈C I(c→e)· Ae where C is set of all components,
E(·) is edge weight
Ae is the abnormality of e
Figure ʉ. Our methodology for ranking causes.
a custom utility that snoops on all socket-level read and write calls.
ˆis snooping yields the identity of the calling processes along with
the IP addresses and ports being used on both ends. It lets us con-
nect communicating processes and measure how much tra(cid:14)c they
exchange. We also estimate response times from these socket-level
events as the time di(cid:11)erence between read and write calls. Including
these response times as a variable in the process state lets us diagnose
faults that delay responses even if the application does not expose this
information as a counter.
We measure path loss rate and delay by sending periodic probes
to machines with which a monitored machine communicates. For
paths that go outside the monitored network, we measure the part
up to the gateway.
NetMedic monitors machine, (cid:12)rewall, and application con(cid:12)gura-
tion stored in the Windows registry as well as (cid:12)les. We read all rel-
evant information once upon start and register callbacks for future
changes. Machine con(cid:12)guration includes information about running
services, device drivers, and mounted drives. Application con(cid:12)gura-
tion may be spread over multiple locations. Currently, the list of lo-
cations for an application is an input to NetMedic, but we plan to au-
tomatically infer where application con(cid:12)guration resides using so(cid:13)-
ware package managers and by tracking application read calls [ʆʃ].
Our data collectors are light-weight. In our deployment, the av-
erage processor usage due to data collection is under ʄʂ. ˆe exact
usage at a given time depends on the level of activity on the machine.
ˆe amount of data transmitted for analysis is under ʅʈʃ bytes per
second per machine. From these overheads and our experience with
data analysis, we believe that the current version of NetMedic can
scale to ʄʃʃ-machine networks, which su(cid:14)ces for small enterprises.
See §@ for a discussion on scaling NetMedic further.
While the data collection part of our system knows the meanings
of some variables (e.g., tra(cid:14)c exchanged), we do not use that infor-
mation in the analysis. Treating variables with known and unknown
meanings identically greatly simpli(cid:12)es analysis. It also makes analy-
sis platform-independent and applicable to a range of environments
with di(cid:11)erent sets of known variables. All that is required to port
NetMedic to a di(cid:11)erent environment is to implement data collection
on non-Windows machines. Much of the needed information is al-
ready there, e.g., in syslog or the proc (cid:12)le system [ʅʈ] in Linux. De-
veloping a Linux prototype is part of our future work.
7. EVALUATION
We now evaluate NetMedic to understand how well it does at link-
ing e(cid:11)ects to their likely causes. We (cid:12)nd that NetMedic is highly ef-
fective. Across a diverse set of faults it identi(cid:12)es the correct compo-
nent as the most likely culprit (§@.ʅ) in over @ʃʂ of the cases. ˆis
ability only slights degrades in the face of simultaneously occurring
faults (§@.ʈ). In contrast, a coarse diagnosis method performs rather
poorly—only for ʄʈʂ of the faults, is it able to identify the correct
component as the most likely culprit. We show that the e(cid:11)ectiveness
of NetMedic is due to its ability to cut down by a factor of three the
number the edges in the dependency graph for which the source is
deemed as likely impacting the destination (§@.ʆ). We also (cid:12)nd that
the extensions to the basic procedure for edge weight assignment sig-
ni(cid:12)cantly enhance the e(cid:11)ectiveness of diagnosis (§@.ʇ) and a modest
amount of history seems to be su(cid:14)cient (§@.ʉ).
250Evaluation Platforms: We have deployed our prototype in two en-
vironments. ˆe primary one is a live environment. ˆe deployment
spans ten client machines and a server machine inside an organiza-
tion. ˆe clients are actively used desktops that belong to volunteers
and have all the noise and churn of regularly used machines.
Because we are not allowed to instrument the real servers in this
environment, we deploy our own. As is common in small enterprises,
our server machine hosts multiple application servers, including Ex-
change (email), IIS (web) and MS-SQL (database). Co-hosted appli-
cation servers are challenging for diagnostic systems as application
interactions are more intertwined. ˆe server processes already ex-
port several application speci(cid:12)c counters.
We implemented custom client processes to communicate with
our application servers. ˆe existing client processes on the desktops
communicate with the real servers of our organization, and we could
not experiment with them without disrupting our volunteers. Our
clients export application speci(cid:12)c counters similar to those exported
by real clients, such as number of successful and failed requests, re-
quests of various types, etc.
Our second environment consists of three clients machines and
a server. Because this environment is completely dedicated to our
experiments, it is a lot more controlled. We do not consider it to be
a realistic setting and unless otherwise stated, the results below are
based on the (cid:12)rst environment. We present some results from the
controlled setting to compare how NetMedic behaves in two disparate
environments with di(cid:11)erent workloads, applications etc.
Methodology:
Ideally, we would like to diagnose real faults in our
deployment but are hindered by the inability to monitor real servers.
We are also hindered by ground truth, which is required to under-
stand the e(cid:11)ectiveness of diagnosis, being o(cid:13)en unavailable for real
faults. Hence, most of the results below are based on faults that we
inject. We do, however, present evidence that NetMedic can help with
faults that occur in situ (§@.@).
We inject the diverse set of ten faults shown in Table ʄ. We stay
as close to the reported fault as possible, including the kind of appli-
cation impacted. For instance, for Problem ʄ, we miscon(cid:12)gure the
IIS server such that it stops serving ASPX pages but continues serv-
ing HTML pages. Similarly, to mimic Problem ʇ, we made an email
client depend on information on a mounted drive.
Except for the experiments in §@.ʈ, where we inject multiple faults
simultaneously, each fault is injected by itself. We inject each fault at
least ʈ times, at di(cid:11)erent times of the day (e.g., day versus night),
to verify that we can diagnose it in di(cid:11)erent operating conditions.
Cumulatively, our experiments span a month, with data collection
and fault injection occurring almost non-stop.
For diagnosis, we specify as input to NetMedic a one minute win-
dow that contains a fault. We did not specify the exact e(cid:11)ect to diag-
nose; rather NetMedic diagnoses all the abnormal aspects in the net-
work. Unless otherwise speci(cid:12)ed, for each fault we use an hour-long
history. ˆe historical period is not necessarily fault-free. In fact, it
o(cid:13)en contains other injected faults as well as any naturally occurring
ones. We do this for realism. In a live environment, it is almost im-
possible to identify or obtain a fault-free log of behavior.
A coarse diagnosis method: We know of no detailed diagnosis tech-
niques to compare NetMedic against. To understand the value of de-
tailed history-based analysis of NetMedic, we compare it against a
Coarse diagnosis method that is based loosely on prior formulations
that use dependency graphs such as Sherlock and Score [ʅ, ʄ@]. ˆis
method uses the same dependency graph as NetMedic. But unlike
NetMedic, it captures the behavior of a component with one variable
that represents whether the component is behaving normally. ˆe de-
termination regarding normal behavior is made in the same way as in
NetMedic. Also unlike NetMedic, Coarse has simple component de-
pendencies. A component impacts a neighboring component with a
high probability (of ʃ.@) when both of them are abnormal. Otherwise,
the impact probability is low (ʃ.ʄ). ˆe exact values of these proba-
100
80
60
40
20
0
e
e
s
s
u
u
a
a
c
c
t
t
c
c
e
e
r
r
r
r
o
o
c
c
f
f
o
o
k
k
n
n
a
a
R
R
Coarse
NetMedic
0
20 40 60 80 100
50
40
30
20
10
0
e
e
s
s
u
u
a
a
c
c
t
t
c
c
e
e
r
r
r
r
o
o
c
c
f
f
o
o
k
k
n
n
a
a
R
R
Coarse
NetMedic
0
20 40 60 80 100
Cumulative % of faults
Cumulative % of faults
(a) Live environment
(b) Controlled environment
Figure @. E(cid:11)ectiveness of Coarse and NetMedic for each fault.
bilities are not signi(cid:12)cant, as long as one is high and the other is low.
Once these edge weights are assigned, the causes are ranked in a man-
ner that is similar to NetMedic. Keeping the ranking method the same
for Coarse lets us focus the evaluation in this paper on our method
for inferring impact among neighbors. We omit results that show that
our ranking method outperforms several other alternatives.
Metric: Our metric to evaluate diagnosis is the rank assigned to the
real cause for each anticipated e(cid:11)ect of a fault. For each fault, we re-
port the median and the maximum rank assigned across its multiple
e(cid:11)ects. For instance, for Problem ʄ, all Web clients that browse ASPX
pages are expected to be a(cid:11)ected. We study the rank assigned to the
con(cid:12)guration of Web server for each such client. ˆe median rank
represents average case behavior, i.e., what an operator who is diag-
nosing a randomly chosen e(cid:11)ect of the fault would experience. ˆe
maximum rank represents the worst case.
What should the rank be for the diagnosis to be useful to an op-
erator? Clearly, lower ranks are better, with a rank of one being per-
fect. However, even the ability to place the real cause within the top
few ranks helps administrators avoid many potential causes that they
would otherwise have to consider (close to ʄʃʃʃ in our deployment).
7.1 Dependency graph properties
We brie(cid:8)y describe the dependency graph constructed across the
eleven machines in our live environment. ˆe exact numbers vary
with time but the graph has close to a ʄʃʃʃ components and ʆʉʃʃ
edges. With roughly @ʃ processes per machine, most of the nodes
in the graph correspond to processes. Correspondingly, the vast ma-
jority of the edges are between components on the same machine,
such as edges between machines and processes. Edges that connect
components on di(cid:11)erent machines (e.g., due to communicating pro-
cesses) are a much smaller fraction. Hence, the dependency graph
is highly clustered, with clusters corresponding to machines and the
graph size grows roughly linearly with the number of machines. ˆis
linear growth in graph complexity makes it easier to scale NetMedic
to larger networks.
Each component provides a rich view of its state in our deploy-
ment. Processes have ʆʈ state variables on average, roughly half of
which are generic variables representing resource usage while the rest
are application speci(cid:12)c and vary with the application. IIS server, for
instance, exports ʄʅ@ application-speci(cid:12)c variables. Machines have
over a hundred variables in their state. ˆus, there are plenty of vari-
ables that are already exported by real applications and operating sys-