early over the frequency range. We subsequently
use digital ﬁlter banks to perform sub-band decom-
position on each word [27]. As discussed in Sec-
tion 3.1, sub-band decomposition gives better re-
sults than simple FFT because of better time res-
4
olution. The output of sub-band decomposition is
smoothed to make it more robust to measurement
variations and environmental noise. The extracted
features are stored in a database.
2. Computation of language models. To solve the
recognition task, we will complement acoustic in-
formation with information about the occurrence
likelihood of words in their linguistic context (e.g.,
the sequence “such as the” is much more likely than
“such of the”). More speciﬁcally, we estimate for
each word in our lexicon n-gram probabilities, i.e.,
the likelihood that the word occurs after a sequence
of n − 1 given words. These probabilities make
up a (statistical) language model. Probabilities are
computed based on frequency counts of n-place se-
quences (n-grams) from a corpus of text documents.
We need to extract these frequencies from a suf-
ﬁciently large corpus, which makes up the second
step of the training phase. In our experiments, we
used 3-gram frequencies extracted from a corpus of
10 million words of English text. For our domain-
speciﬁc experiments, we used a corpus of living-
will declarations consisting of 14,000 words of En-
glish text.
The second phase (Figure 4(b)), called the recognition
phase, uses the characteristic features of the trained
words to recognize new sound recordings of printed text,
complemented by suitable language-correction tech-
niques. The main steps are as follows:
1. Select candidate words. We start by extracting fea-
tures of the recording of the printed target text, as in
the ﬁrst step of the training phase. Let us call the ob-
tained sequence of features target features whereas
the features from the training phase stored in the
database are henceforth referred to as trained fea-
tures. Now, we subsequently compare, on a word-
by-word basis,
the obtained target features with
the trained features of the dictionary stored in the
database.
If the features extracted from different recordings of
the same word were always identical, one would ob-
tain a unique correspondence between trained fea-
tures and target features (under the assumption that
all text words are in the dictionary). However, mea-
surement variations, environmental noise, etc. show
that this is not the case. Multiple recordings of the
same word sometimes yield different features; for
example, printing the same word at different places
in the document results in differing acoustic em-
anations (Figure 10 illustrates how a single verti-
cal line already differs in the intensity); conversely,
recordings of words that differ signiﬁcantly in their
spelling might yield almost identical sound features.
We hence let the selected, trained word be a random
variable conditioned on the printed word, i.e., every
trained word will be a candidate with a certain prob-
ability. Using sufﬁciently good feature extraction
and distance computations between two features,
the probabilities of one or a few such trained words
will dominate for each printed word. The output
of the ﬁrst recognition step is a list of most likely
candidates, given the acoustic features of the target
word.
2. Language-based reordering to reduce word error
rate. We ﬁnally try to ﬁnd the most likely se-
quence of printed words given a ranked list of candi-
date words for each printed word. Although always
naively picking the most likely word based on the
acoustic signal might already yield a suitable recog-
nition quality, we employ Hidden Markov Model
(HMM) technology, in particular language models
and the Viterbi algorithm (see Section 3.3.3 for de-
tails), which is regularly used in speech recognition,
to determine the most likely sequence of printed
words. Intuitively, this technology works well for us
because most errors that we encounter in the recog-
nition phase are due to incorrectly recognized words
that do not ﬁt the context; by making use of linguis-
tic knowledge about likely and unlikely sequences
of words, we have a good chance of detecting and
correcting such errors. The use of HMM technology
yields accuracy rates of 70 % on average for words
for the general-purpose corpus, and up to 95 % for
the domain-speciﬁc corpus, see Section 3.3 for de-
tails.
We modiﬁed the Viterbi algorithm to meet our spe-
ciﬁc needs: ﬁrst, the standard algorithm accepts as
input a sequence of outputs, while we get for each
position an ordered list of likely candidates, and we
want to proﬁt from this extra knowledge; second,
we need to decrease memory usage, since a standard
implementation would consume more than 30 GB
of memory.
3 Technical Details
In this section we provide technical details about our at-
tack, including the background in audio processing and
Hidden-Markov Models.
3.1 Feature extraction
We are faced with an audio ﬁle sampled at 96 kHz with
16bit.
5
To split the recording into words, we use a threshold
on the intensity of the frequency band from 20 kHz to
48 kHz. For printers, our experiments have shown that
most interesting features occur above 20 kHz, making
this frequency range a reliable indicator despite its sim-
plicity; ignoring the lower frequencies moreover avoids
most noise added by the movement of the print-head etc.
From the split signal, we compute the raw spectrum
features by sub-band decomposition, a common tech-
nique in different areas of audio processing. The signal is
ﬁltered by a ﬁlter bank, a parallel arrangement of several
bandpass ﬁlters tuned in steps of 1 kHz over the range
from 1 kHz to 48 kHz.
For noise reduction the output of
the ﬁlters is
smoothed, normalized, the amount of data is reduced (the
maximal value out of 5 is kept), and smoothed again. The
result is appropriately discretized over time and forms a
set of vectors, one vector for each ﬁlter.
The feature design has a major inﬂuence on the run-
ning time and storage requirements of the subsequent
audio processing. We have experimented with several
alternative feature designs, but obtained the best results
with the design described above. The (Short-time) Fast
Fourier Transform (SFFT) [34] seems a natural alterna-
tive to sub-band decomposition. There is, however, a
trade-off between the frequency and the time resolution,
and we obtain worse results in our setting when we used
SFFTs, similar to earlier observations [42].
3.2 Select candidate words
Deciding which database entry is the best match for a
recording is based on the following distance function de-
ﬁned on features; the tool outputs the 30 most similar
entries along with the calculated distance. Given the fea-
tures extracted from the recording (~x1, . . . , ~xt) and the
features of a single database entry (~y1, . . . , ~yt) we com-
pute the angle between each pair of vectors ~xi, ~yi and
sum over all frequency bands:
∆((~x1, . . . , ~xt), (~y1, . . . , ~yt))
= Xi=1,...,t
arccos(cid:18) ~xi · ~yi
|~xi| · |~yi|(cid:19) .
To increase robustness and decrease computational com-
plexity in practical scenarios, some problems need to be
addressed: First, our implementation of cutting the au-
dio ﬁle sometimes errs a bit, which leads to slightly non-
matching samples. Thus we consider minor shiftings of
each sample by tiny amounts (two steps in each direction,
or a total of 5 measurements) and take the minimum an-
gle (i.e., the maximum similarity). Second, for a similar
reason, we tolerate some deviation in the length of the
6
features. We punish too large deviations by multiplying
with a factor of 1.2 if the length of the query and the
database entry differ by more than a deﬁned threshold.
The factor and the threshold are derived from our exper-
iments. Third, we discard entries whose length deviates
from the target feature by more than 15 % in order to
speed up the computation.
Using the angle to compare features is a common tech-
nique. Other approaches that are used in different sce-
narios include the following: M¨uller et al. present an
audio matching method for chroma based features that
handles tempo differences [28]. Logan and Salomon use
signatures based on clustered MFCCs as input for the
distance calculation in [24]. Furthermore, they use the
earth mover’s distance [32] for the signatures (minimum
amount of work to transform one signature into another)
and the Kullback Leibler (KL) distance for the clusters
inside the signature as distance measures.
3.3 Post-processing using HMM technol-
ogy
In this section we describe techniques based on language
models to further improve the quality of reconstruction.
These improve the word recognition rate from 63 %
to 70 % on average, and up to 72 % in some cases.
The domain-speciﬁc HMM-based post-processing even
achieves recognition rates of up to 95 %.
3.3.1 Introduction to HMMs
Hidden Markov models (HMMs) are graphical models
for recovering a sequence of random variables which
cannot be observed directly from a sequence of (ob-
served) output variables. The random variables are mod-
eled as hidden states, the output variables as observed
states. HMMs have been employed for many tasks that
deal with natural language processing such as speech
recognition [31, 18, 17], handwriting recognition [29] or
part-of-speech tagging [12, 14].
Formally, an HMM of order d is deﬁned by a ﬁve-tuple
hQ, O, A, B, Ii, where Q = (q1, q2, ..., qN ) is the set of
(hidden) states, O = (o1, o2, ..., oM ) is the set of obser-
vations, A = Qd+1 is the matrix of state transition prob-
abilities (i.e., the probability to reach state qd+1 when
being in state qd with history q1, . . . , qd−1), B = Q × O
are the emission probabilities (i.e., the probability of ob-
serving a speciﬁc output oi when being in state qj), and
I = Qd is the set of initial probabilities (i.e., the prob-
ability of starting in state qi). Figure 5 shows a graph-
ical representation of an HMM, where unshaded circles
represent hidden states and shaded circles represent ob-
served states.
q1
a12
q2
a23
q3
a34
.......
aN −1,N
qN
b11
b22
b33
o1
o2
o3
eN M
oM
Figure 5: Hidden Markov Model
In our setting the words that were printed are unknown
and correspond to the hidden states. The observed states
are the output of the ﬁrst stage of reconstruction from
the acoustic signals emitted by the printer. What makes
HMMs particularly attractive for our task is that they al-
low us to combine two sources of information: ﬁrst, the
acoustic information present in the observed signal, and
second, knowledge about likely and unlikely word com-
binations in a well-formed text. Both sources of infor-
mation are important for recovering the original text.
To utilize HMMs for our task, we need to solve two
problems: we need to estimate the model parameters of
the HMM (training phase), and we need to determine the
most likely sequence of hidden states for a sequence of
observations given the model (recognition phase). The
method described in Section 3.2 approximates the es-
timation of the emission probabilities by computing a
ranking of the candidate words given an observed acous-
tic signal. The initial probabilities, which model the
probability of starting in a given state, and the transi-
tion probabilities, which model the likelihood of differ-
ent words following each other in an English text, can
be obtained by building a language model from a large
text corpus. To address the second problem, determin-
ing the most likely sequence of hidden states (i.e., the
most likely sequence of printed words in the target text),
we can use the Viterbi algorithm [37]. In the following
two sections, we describe in more detail how we com-
pute the language models and how the candidate words
are reordered by applying the Viterbi algorithm.
3.3.2 Building the language models
A language model of size n assigns a probability to each
sequence of n words. The probability distribution can be
estimated by computing the frequencies of all n-grams
from a large text corpus. Note that language models are
to some extent domain and genre dependent, i.e., a lan-
guage model built from a corpus of ﬁnancial texts will
not be a very good model for predicting likely word se-
quences in biomedical texts. To cover a large range of
domains and thus make our model robust in the face of
arbitrary input texts, we train the language model on a
diverse selection of stable Wikipedia articles. The cor-
pus has a size of 63 MB and contains approximately 10
million words. For our domain-speciﬁc experiments, we
used a corpus of living-will declarations consisting of
14,000 words of English text. From the corpus, we ex-
tracted all 3-grams and computed their frequencies.1 We
took into consideration all 3-grams that appeared at least
3 times. As n-grams with probability 0 will never be
selected by the Viterbi algorithm, we smooth the proba-
bilities by assigning a small probability to each unseen
n-gram.
The length of an n-gram determines how many words
of context (i.e., how many previous hidden states in the
HMM) are taken into account by the language model.
Higher values for n can lead to better models but also
require exponentially larger corpora for an accurate esti-
mation of the n-gram probabilities. The higher the value
of n, the larger the likelihood that some n-grams never
appear in the corpus, even though they are valid word
sequences and thus may still appear in the printed text.
3.3.3 Reordering of candidate words based on lan-
guage models
Having built the language model, we can reorder the
candidate words using the model to select the most
the most likely sequence
likely word sequence (i.e.,
of hidden states).
This task is addressed by the
Viterbi algorithm [37], which takes as input an HMM
hQ, O, A, B, Ii of order d and a sequence of observa-
tions a1, . . . , aT ∈ OT . Its state consists of Ψ = T ×Qd.
First, the d-th step is initialized (the earlier are unused)
according to the initial distribution, weighted with the
1All 3-grams were converted to lower case and punctuation charac-
ters were stripped off.
7
observations:
Ψd,i1,...,id = Ii1,...,id Yk=1,...,d
Bik,ak
∀ 1 ≤ i, j ≤ N.
In the recursion, for increasing indices s, the maximum
of all previous values is taken:
Ψs,i1,...,id = Bid ,as max
i0 ∈Q(cid:0)Ai0,i1,...,id Ψs−1,i0,...,id−1(cid:1)
∀ s > d, 1 ≤ i, j ≤ N.
Finally, the sequence of hidden states can be obtained
by backtracking the indices that contributed to the maxi-
mum in the recursion step.
The memory required to store the state Ψ is O(T ·N d),
and the running time is O(T · N d+1), as we are opti-
mizing over all N hidden states for each cell, so mem-
ory requirements are a major challenge in implementing
the Viterbi algorithm. For example, using a dictionary
of 1, 000 words, the memory requirements of our imple-
mentation for 3-grams are slightly above 2 GB, and is
growing quadratically in N .
We use two techniques to overcome these problems:
1. First, instead of storing the complete transition ma-
trix A we compute the values on-the-ﬂy (keeping
only the list of 3-grams in memory).
2. Second, we do not optimize over all possible words,
but only over the M = 30 best rated words from
the previous stage. This brings down memory re-
quirements to O(T · M d) and execution time to
O(T · M d+1). The size of Ψ in this case is 130 MB
for 3-grams.
Further improvements are conceivable, e.g., by using
parallel scalability [40].
4 Experiments and Statistical Evaluation
In this section we describe our experiments for evaluat-
ing the attack. In addition to describing the set-up and the
experimental results on the recognition rate for sample
articles, we present our experiments for evaluating the
inﬂuence of using different microphones, printers, fonts,
etc. on the recognition rate; moreover, we identify and
evaluate countermeasures.
4.1 Setup
We use an Epson LQ-300+II (24 needles) without printer
cover and the in-built mono-spaced font for printing
texts. The sound is recorded from a short distance us-
ing a Sennheiser MKH-8040 microphone with nominal
frequency range from 30 Hz to 50 kHz. If nothing addi-
tional is mentioned the experiments were conducted in a
8
normal ofﬁce with the door closed and no people talking
inside the room. There was no special shielding against
noise from the outside (e.g., trafﬁc noise). In the training
phase we used a dictionary containing 1,400 words; the
dictionary consists of a list of the 1,000 most frequent
words from our corpus augmented with the words that
appeared in our example texts.2 Inﬂected forms, capital-
ization, as well as words with leading punctuation marks
need to be counted as different words, as their sound fea-
tures might signiﬁcantly differ (blurring propagates from
left to right within a word).
We work with the sound recordings of four different
articles from Wikipedia on different topics: two articles
on computer science (on source-code and printers), one
article on politics (on Barack Obama), and one article
on art (on architecture) with a total of 1,181 words to
evaluate the attack.
The training and matching phase have been imple-
mented in MATLAB using the Signal Processing Tool-
box – a MATLAB extension which allows to conve-
niently process audio signals. The HMM-based post-
processing is implemented in C. The tool is fully auto-
mated, with the only exceptions being threshold values
that need manual adaption for a given attack scenario. In
the scenario with the microphone placed 10cm in front