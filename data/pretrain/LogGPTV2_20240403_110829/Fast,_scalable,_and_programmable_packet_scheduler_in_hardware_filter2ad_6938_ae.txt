and hence PIFO scheduler can schedule at a higher rate than PIEO.
This is the price we pay in PIEO’s hardware design to achieve order
of magnitude more scalability than PIFO. Alternatively, one can
also implement PIEO primitive using PIFO’s hardware design7 and
achieve more expressibility than PIFO without compromising on
scheduling rate, albeit at a much smaller scale. We, however, argue
that the trade-off made in PIEO’s hardware design is a good trade-
off to make, as PIEO’s design is still extremely fast and can schedule
at tens of nanosecond timescale (even less at higher clock rates),
while also scale to tens of thousands of flows, which is critical in
today’s multi-tenant cloud networks [31, 32].
7 Porting PIEO primitive to PIFO’s hardware design is trivial despite PIEO support-
ing a more complex dequeue function, because the kind of predicates used in PIEO
implementation can be evaluated in parallel in flip-flops in one clock cycle.
377
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Vishal Shrivastav
as PIFO [37], and universal packet scheduling algorithms, such as
UPS [27], whose goals align very closely with our work. We discuss
the limitations of PIFO and UPS in §2.3. Somewhat complementary
to our work, Loom [38] proposes a flexible packet scheduling frame-
work in the NIC, using a new abstraction for expressing scheduling
policies, and an efficient OS/NIC interface for scheduling, while
leveraging the PIFO scheduler for enforcing the scheduling policies.
In principle, systems like Loom can use PIEO scheduler instead of
PIFO and achieve more expressibility and scalability.
Datastructures for hardware packet schedulers. Most packet
schedulers in hardware rely on FIFO-based datastructures as it en-
ables fast and scalable scheduling, at the cost of limited programma-
bility or accuracy (§2.3). A priority queue allows ordered scheduling
of elements, and hence can express a wide range of scheduling al-
gorithms. P-heap [7] is a scalable heap-based implementation of
priority queue in hardware. Unfortunately, a heap-based priority
queue cannot efficiently implement the "Extract-Out" primitive in
PIEO. PIFO [37] also provides a priority queue abstraction, but
implements it using an ordered list datastructure, also used to im-
plement PIEO. However, PIFO’s hardware implementation of the
ordered list is not scalable (Fig. 8). In this paper, we presented both
a fast and scalable implementation of an ordered list in hardware.
8 DISCUSSION
PIEO as an Abstract Dictionary Data Type. In general, PIEO
primitive can be viewed as an abstract dictionary data type [43],
which maintains a collection of (key, value) pairs, indexed by key,
and allows operations such as search, insert, delete and update.
PIEO presents an extremely efficient implementation of the dictio-
nary data type in hardware, which can do all the above mentioned
operations in O(1) time, while also being scalable. Further, it can
also very efficiently support certain other key dictionary operations
considered traditionally challenging, such as filtering a set of keys
within a range, as PIEO implementation described in §5 can be
naturally extended to support predicates of the form a ≤ key ≤ b.
Dictionary data types are one of the most widely used data types
in computer science, and PIEO presents a potential alternative to
the traditional hardware implementations of the dictionary data
type, such as hashtables and search trees.
9 CONCLUSION
We presented a new packet scheduling primitive, called Push-In-
Extract-Out (PIEO), which could express a wide range of packet
scheduling algorithms. PIEO assigns each element a rank and an
eligibility predicate, both of which could be programmed based
on the choice of the scheduling algorithm, and at any given time,
schedules the "smallest ranked eligible" element. We presented a fast
and scalable hardware design of PIEO scheduler, and prototyped it
on a FPGA. Our prototype could schedule at tens of nanosecond
timescale, while also scale to tens of thousands of flows.
Acknowledgments. I thank the anonymous SIGCOMM reviewers
and the shepherd, Anirudh Sivaraman, for their valuable comments
and suggestions. I also thank my mentors at Cornell, Hakim Weath-
erspoon and Rachit Agarwal, as well as the entire Microsoft Azure
Datapath team for valuable discussions. This work was partially
supported by NSF (award No. 1704742).
Figure 11: Rate-limit enforcement in PIEO prototype.
Figure 12: Fair queue enforcement in PIEO prototype.
6.3 Programmability
We show in §4 that one can express a wide range of scheduling
algorithms using the PIEO primitive. In this section, we program
two such algorithms, namely Token Bucket (§4.2) and WF2Q+ (§4.1),
atop our FPGA prototype using System Verilog as the programming
language. The two chosen algorithms implement two of the most
widely-used scheduling policies in practice, namely rate-limiting
and fair queuing.
We program a two-level hierarchical scheduler using our pro-
totype, with ten nodes at level-2 and ten flows within each node,
for a total of 100 flows. We implement packet generators, one per
flow, on the FPGA to simulate the flows. The link speed is 40 Gbps,
and we schedule at MTU granularity. For experiments, we assign
varying rate-limit values to each node at level-2 in the hierarchy,
and enforce it using the Token Bucket algorithm. The rate-limit
value of a particular node at level-2 is then shared fairly across
all it’s ten flows using WF2Q+ algorithm. In Fig. 11, we sample a
random level-2 node, and show that PIEO scheduler very accurately
enforces the rate-limit on that node. Further, in Fig. 12, we show
that for each rate-limit value assigned to the chosen level-2 node,
PIEO scheduler very accurately enforces fair queuing across all the
flows within that level-2 node.
7 RELATED WORK
Packet scheduling in hardware. Packet schedulers in hardware
have traditionally been fixed-function, that either implement spe-
cific scheduling primitives, such as rate-limit, as in [31, 24], and
fairness, as in [39, 33], or implement specific scheduling algo-
rithms, such as shortest-job-first, as in [4]. More recently, there
have been proposals for programmable packet schedulers, such
378
Fast, Scalable, and Programmable Packet Scheduler in Hardware
SIGCOMM ’19, August 19–23, 2019, Beijing, China
REFERENCES
[1] IEEE Standard 802.11Q-2005. 2005. Standard for local and metropolitan area
networks, Virtual Bridged Local Area Networks. (2005).
[2] Mohammad Alizadeh, Abdul Kabbani, Tom Edsall, Balaji Prabhakar, Amin Vahdat,
and Masato Yasuda. 2012. Less Is More: Trading a Little Bandwidth for Ultra-Low
Latency in the Data Center. In NSDI.
[3] Mina Tahmasbi Arashloo, Monia Ghobadi, Jennifer Rexford, and David Walker.
2017. HotCocoa: Hardware Congestion Control Abstractions. In HotNets.
[4] Wei Bai, Li Chen, Kai Chen, Dongsu Han, Chen Tian, and Hao Wang. 2015.
Information-Agnostic Flow Scheduling for Commodity Data Centers. In NSDI.
[5] Jon C. R. Bennett and Hui Zhang. 1996. Hierarchical Packet Fair Queueing
Algorithms. In SIGCOMM.
Queueing. In INFOCOMM.
[6] Jon C. R. Bennett and Hui Zhang. 1996. WF2Q: Worst-case Fair Weighted Fair
[7] R. Bhagwan and B. Lin. 2000. Fast and Scalable Priority Queue Architecture for
High-Speed Network Switches. In INFOCOMM.
[8] Bluespec. 2019. BSV High-Level HDL. (2019). https://bluespec.com/54621-2/
[9] Pat Bosshart, Glen Gibb, Hun-Seok Kim, George Varghese, Nick McKeown, Martin
Izzard, Fernando Mujica, and Mark Horowitz. 2013. Forwarding Metamorphosis:
Fast Programmable Match-Action Processing in Hardware for SDN. In SIGCOMM.
[10] Randy Brown. 1988. Calendar queues: A fast O(1) priority queue implementation
for the simulation event set problem. In Communications of the ACM.
[11] R. Colwell. 2013. The chip design game at the end of Moore’s law. In HotChips.
[12] IEEE DCB. 2011. 802.1Qbb - Priority-based Flow Control. (2011). http://www.
ieee802.org/1/pages/802.1bb.html
[13] Alan Demers, Srinivasan Keshav, and Scott Shenker. 1989. Analysis and Simula-
tion of a Fair Queueing Algorithm. In SIGCOMM.
[14] Hadi Esmaeilzadeh, Emily Blem, Renee St. Amant, Karthikeyan Sankaralingam,
and Doug Burger. 2011. Dark Silicon and the End of Multicore Scaling. In ISCA.
[15] Daniel Firestone, Andrew Putnam, Sambhrama Mundkur, Derek Chiou, Alireza
Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adrian Caulfield, Eric
Chung, Harish Kumar Chandrappa, Somesh Chaturmohta, Matt Humphrey, Jack
Lavier, Norman Lam, Fengfen Liu, Kalin Ovtcharov, Jitu Padhye, Gautham Popuri,
Shachar Raindel, Tejas Sapre, Mark Shaw, Gabriel Silva, Madhan Sivakumar,
Nisheeth Srivastava, Anshuman Verma, Qasim Zuhair, Deepak Bansal, Doug
Burger, Kushagra Vaid, David A. Maltz, and Albert Greenberg. 2018. Azure
Accelerated Networking: SmartNICs in the Public Cloud. In NSDI.
[16] Matthew P. Grosvenor, Malte Schwarzkopf, Ionel Gog, Robert N. M. Watson,
Andrew W. Moore, Steven Hand, and Jon Crowcroft. 2015. Queues Don’t Matter
When You Can JUMP Them!. In NSDI.
[17] Intel. 2015. Stratix V FPGA. (2015). https://www.intel.com/content/dam/www/
programmable/us/en/pdfs/literature/hb/stratix-v/stx5_51001.pdf
[18] Intel. 2018. Stratix 10 FPGA. (2018). https://www.intel.com/content/dam/www/
programmable/us/en/pdfs/literature/hb/stratix-10/s10-overview.pdf
[19] Keon Jang, Justine Sherry, Hitesh Ballani, and Toby Moncaster. 2015. Silo: Pre-
dictable Message Latency in the Cloud. In SIGCOMM.
[20] Ian Kuon and Jonathan Rose. 2007. Measuring the Gap Between FPGAs and
ASICs. IEEE Transactions on Computer-Aided Design of Integrated Circuits and
Systems 26, 2 (Feb. 2007).
[21] He Liu, Matthew K. Mukerjee, Conglong Li, Nicolas Feltman, George Papen, Ste-
fan Savage, Srinivasan Seshan, Geoffrey M. Voelker, David G. Andersen, Michael
Kaminsky, George Porter, and Alex C. Snoeren. 2015. Scheduling techniques for
hybrid circuit/packet networks. In CoNEXT.
[22] Rob McGuinness and George Porter. 2018. Evaluating the Performance of Soft-
ware NICs for 100-Gb/s Datacenter Traffic Control. In ANCS.
[23] P McKenney. 1990. Stochastic Fairness Queuing. In INFOCOMM.
[24] Mellanox. 2019. ConnectX-4. (2019). http://www.mellanox.com/page/products_
dyn?product_family=204&mtag=connectx_4_en_card
[25] William M. Mellette, Rob McGuinness, Arjun Roy, Alex Forencich, George Papen,
Alex C. Snoeren, and George Porter. 2017. RotorNet: A scalable, low-complexity,
optical datacenter network. In SIGCOMM.
PIFO implementation.
https://github.com/
[26] MIT. 2019.
(2019).
programmable-scheduling/pifo-hardware/blob/master/src/rtl/design/pifo.v
[27] Radhika Mittal, Rachit Agarwal, Sylvia Ratnasamy, and Scott Shenker. 2016.
Universal Packet Scheduling. In NSDI.
[28] Radhika Mittal, Terry Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel,
Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats.
2015. TIMELY: RTT-based Congestion Control for the Datacenter. In Sigcomm.
[29] Sung-Whan Moon, J. Rexford, and K.G. Shin. 2000. Scalable hardware priority
queue architectures for high-speed packet switches. IEEE Trans. Comput. 49, 11
(2000), 1215–1227.
[30] Jonathan Perry, Amy Ousterhout, Hari Balakrishnan, Devavrat Shah, and Hans
Fugal. 2014. Fastpass: A Centralized "Zero-queue" Datacenter Network. In SIG-
COMM.
[31] Sivasankar Radhakrishnan, Yilong Geng, Vimalkumar Jeyakumar, Abdul Kabbani,
George Porter, and Amin Vahdat. 2014. SENIC: Scalable NIC for End-Host Rate
Limiting. In NSDI.
[32] Ahmed Saeed, Nandita Dukkipati, Vytautas Valancius, Vinh The Lam, Carlo
Contavalli, and Amin Vahdat. 2017. Carousel: Scalable Traffic Shaping at End
Hosts. In SIGCOMM.
[33] Naveen Kr. Sharma, Ming Liu, Kishore Atreya, and Arvind Krishnamurthy. 2018.
Approximating Fair Queueing on Reconfigurable Switches. In NSDI.
[34] M Shreedhar and G Varghese. 1996. Efficient Fair Queuing using Deficit Round
Robin. IEEE/ACM Transactions on Networking (ToN) 4, 3 (1996), 375–385.
[35] Vishal Shrivastav, Asaf Valadarsky, Hitesh Ballani, Paolo Costa, Ki Suh Lee, Han
Wang, Rachit Agarwal, and Hakim Weatherspoon. 2019. Shoal: A Network
Architecture for Disaggregated Racks. In NSDI.
[36] A Sivaraman, A Cheung, M Budiu, C Kim, M Alizadeh, H Balakrishnan, G Vargh-
ese, N McKeown, and S Licking. 2016. Packet Transactions: High-Level Program-
ming for Line-Rate Switches. In SIGCOMM.
[37] Anirudh Sivaraman, Suvinay Subramanian, Mohammad Alizadeh, Sharad Chole,
Shang-Tse Chuang, Anurag Agrawal, Hari Balakrishnan, Tom Edsall, Sachin
Katti, and Nick McKeown. 2016. Programmable Packet Scheduling at Line Rate.
In SIGCOMM.
[38] Brent Stephens, Aditya Akella, and Michael Swift. 2019. Loom: Flexible and
Efficient NIC Packet Scheduling. In NSDI.
[39] Brent Stephens, Arjun Singhvi, Aditya Akella, and Michael Swift. 2017. Titan:
Fair Packet Scheduling for Commodity Multiqueue NICs. In ATC.
[40] George Varghese and Anthony Lauck. 1987. Hashed and Hierarchical Timing
Wheels: Efficient Data Structures for Implementing a Timer Facility. In SOSP.
[41] Bhanu Chandra Vattikonda, George Porter, Amin Vahdat, and Alex C. Snoeren.
2012. Practical TDMA for datacenter ethernet. In EuroSys.
[42] Shaileshh Bojja Venkatakrishnan, Mohammad Alizadeh, and Pramod Viswanath.
2016. Costly circuits, submodular schedules and approximate caratheodory
theorems. In SIGMETRICS.
[43] Wikipedia. 2019. Abstract Dictionary Data Type. (2019). https://en.wikipedia.
[44] Wikipedia. 2019. Earliest Deadline First. (2019). https://en.wikipedia.org/wiki/
[45] Wikipedia. 2019. Least Slack Time First. (2019). https://en.wikipedia.org/wiki/
org/wiki/Associative_array
Earliest_deadline_first_scheduling
Least_slack_time_scheduling
[46] Wikipedia. 2019. openCL. (2019). https://en.wikipedia.org/wiki/OpenCL
[47] Wikipedia. 2019. Shortest Job First.
(2019). https://en.wikipedia.org/wiki/
[48] Wikipedia. 2019. Shortest Remaining Time First. (2019). https://en.wikipedia.
org/wiki/Shortest_remaining_time
[49] Wikipedia. 2019.
System Verilog.
(2019).
https://en.wikipedia.org/wiki/
[50] Wikipedia. 2019. Token Bucket. (2019). https://en.wikipedia.org/wiki/Token_
Shortest_job_next
SystemVerilog
bucket
[51] Christo Wilson, Hitesh Ballani, Thomas Karagiannis, and Ant Rowtron. 2011. Bet-
ter Never Than Late: Meeting Deadlines in Datacenter Networks. In SIGCOMM.
[52] Jun Xu and Richard J. Lipton. 2002. On fundamental tradeoffs between delay
bounds and computational complexity in packet scheduling algorithms. In SIG-
COMM.
[53] H Zhang and D Ferrari. 1994. Rate-Controlled Service Disciplines. Journal of
High Speed Networks 3, 4 (1994), 389–412.
379