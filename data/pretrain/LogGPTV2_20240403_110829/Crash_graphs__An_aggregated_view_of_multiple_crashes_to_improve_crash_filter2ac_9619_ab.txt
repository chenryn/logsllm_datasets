bucket problem and thus in identifying duplicated auto-crash 
bug reports. 
Hypothesis 2: Crash graphs can predict if a given crash 
will  be  fixed.  Since  crash  graphs  capture  properties  of  all 
crashes in one bucket, graph features can be useful to predict 
fixable crashes. 
IV.  EXPERIMENTS  
This  section  presents  our  crash  graph  experience  of 
applying the graphs for crash triage tasks.   
A.  Duplicate Detection 
WER  uses  bucketing  algorithms  based  on  over  500 
heuristics  to  identify  causes  of  crashes  and  to  classify  the 
crashes into buckets based on their causes [9]. Each bucket is 
a basic unit for crash triage, i.e. prioritizing crashes based on 
hit counts in each bucket.  
In  most  cases,  WER  bucketing  algorithms  work 
reasonably  well.  However,  due 
to  non-deterministic 
properties  of  crashes,  those  caused  by  the  same  bugs  may 
also  produce  slightly  or  (sometimes)  significantly  different 
crash traces. As a result, the bucketing algorithms put such 
crashes  into  different  buckets.  This  second  bucket  problem 
leads  to  duplicated  auto-crash  bug  reports  as  discussed  in 
Section II.C. 
We  apply  our  crash  graphs  to  automatically  detect 
duplicate  auto-crash  bug  reports.  First,  we  propose  a  crash 
graph  similarity  measure  to  detect  duplicates  in  Section 
IV.A.1  and  show  the  experimental  results  using  Windows 
OS bug reports in Section IV.A.2. Section IV.A.3 discusses 
the  results  and  the  role  of  crash  graphs  in  detecting 
duplicates.   
1)  Measure 
Since  a  crash  graph  represents  multiple  crashes  in  one 
bucket,  our  approach  is  to  compare  two  crash  graphs  from 
two bug reports to determine if they are duplicates.  
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:42:01 UTC from IEEE Xplore.  Restrictions apply. 
488Graph  similarity  measures  have  been  widely  proposed 
and used in various domains including face recognition [19], 
text mining [14], and social network analysis [12]. 
In  this  paper,  we  use  the  following  graph  subset 
similarity measure to compare two crash graphs G1 and G2: 
Sim(G1,G2) =
E1 !E2
min( E1 , E2 )  
where E is the set of edges in G. 
Basically, this equation measures if a smaller graph is a 
subset  of  the  bigger  graph.  In  this  equation,  we  ignore  the 
node or edge weights. 
2)  Experiments 
We measure the similarity of two given bug reports using 
the  equation  shown  in  Section  IV.A.1.  If  the  similarity  is 
above  a  threshold,  we  assume  the  two  bug  reports  are 
duplicates.  
To  evaluate  the  similarity  measure  using  crash  graphs, 
we use auto crash-bug reports from the Windows OS project. 
These  bug  reports  include  duplicates  due  to  the  second 
bucket  problem.  These  duplicates  are  manually  marked  by 
Windows  OS  developers  which  allows  us  to  examine  the 
efficiency  of  our  method.  In  total,  we  use  ‘n’  (anonymized 
for confidentiality) bug reports from Windows OS projects. 
Among them, 13.3% of the reports are duplicates. We apply 
the  crash  graphs  construction  algorithm  and  the  similarity 
measure,  and  check  if  our  approach  can  detect  these 
manually marked duplicates.  
Since  our  approach  compares  similarity  of  two  given 
crash  graphs 
reports),  we  conduct  pair-wise 
comparisons for all bug reports. As shown in Table I, there 
are  n*(n-1)/2  pairs  for  n  bug  reports.  Among  these  pairs, 
only 0.32% are duplicated pairs.  
To  evaluate  the  performance  of  duplicate  detection,  we 
(bug 
use recall and precision measures [1].  
The recall for a given similarity threshold denotes: 
Recall (similarity) = 
MD !PDsimilarity
MD
where  |MD|  is  the  number  of  manually  marked  duplicates, 
PDsimilarity  is  predicted  duplicates  based  on  the  given 
threshold  value,  and  |MD∩PDsimilarity|  is  the  number  of 
correctly predicted duplicates.  
The precision for a given similarity threshold value is: 
Precision (similarity) =  MD !PDsimilarity
PDsimilarity
. 
In  general, 
identifying  only  0.32%  of 
the  entire 
population  is  a  challenging  problem.  The  precision  of 
existing approaches for detection of duplicate bug reports is 
around 40~60% [16, 17]; recall is typically very low or not 
measured.  
TABLE I.  
Name 
# of bug reports 
# of duplicated bugs 
# total bug pair  
# of duplicated bug pair 
# of non-duplicated bug  
WINDOWS OS BUG REPORTS FOR DUPLICATED BUG 
DETECTION 
Value 
n 
13.3% 
n*(n-1)/2 
0.32% 
99.68% 
Note that sophisticated bucketing algorithms are already 
applied  and  missed  the  duplicates  used  in  our  experiment 
data. 
Table  II  shows  the  precision  and  recall  of  detecting 
duplicates  using  the  crash  graph  similarity  measure.  The 
precision  and  recall  vary  based  on  the  similarity  threshold 
values. For example, when the similarity threshold is set to 
0.95, the recall and precision is around 60%. On setting the 
threshold  to  0.98,  the  precision  is  over  70%  while  recall 
remains around 60%.  
TABLE II.  
DUPLICATE DETECTION PRECISION AND RECALL FOR SELECTED 
SIMILARITY THRESHOLD VALUES. 
Similarity Threshold 
Precision 
1 
0.99 
0.98 
0.97 
0.96 
0.95 
70.3 
71.5 
71.0 
68.4 
65.0 
61.6 
Recall 
58.8 
62.4 
63.6 
64.2 
64.2 
64.2 
The  low  recall  is  due  to  non-deterministic  behaviors  of 
bugs  and  crashes.  It  is  possible  that  the  same  bugs  can 
manifest completely different crash traces.  
Even if one graph is completely a sub-graph of another 
(similarity is 1), the precision will not reach 100%. This is 
due to manual duplication marking, since the developer may 
neglect  or  forget  to  mark  duplicates  in  the  bug  reporting 
system.  In  this  case,  we  cannot  decide  if  our  prediction  is 
correct, so we conservatively assume it is a wrong detection. 
Thus,  our  precision  will  not  reach  100%,  even  if  we  only 
consider the exact sub-graphs (similarity is 1). 
Figure  4  shows  the  precision-recall  curve  for  various 
threshold  values.  Overall,  recall  and  precision  are  around 
60%. By sacrificing recall, precision can be increased to over 
70%.  Our  experimental  results  indicate  crash  graphs  can 
detect duplicated bug reports with reasonable accuracy. Note 
that  previous  approaches  [16,  17]  yield  around  40-60% 
precision. 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:42:01 UTC from IEEE Xplore.  Restrictions apply. 
489Similarity = 0 
Another  reason  of  the  second  bucket  problem  is  partial 
crash  traces.  It  might  be  possible  that  only  partial  crash 
traces  are  sent  to  the  WER  server.  In  this  case,  measuring 
partial trace similarity may cause the second bucket problem. 
For example, suppose we have two traces in Bucket 1 as 
shown in Figure 6. Suppose a client sent a new crash trace to 
the WER server. Unfortunately, it is a partial trace, or due to 
the  missing  symbol  information,  we  can  figure  out  only 
partial frame names. If we just compare similarities between 
two crash traces, the new trace will be put in a new bucket, 
since it is not similar to Trace 1 or Trace 2 in Bucket 1. 
Similarity = 1 
Figure 4.   Precision-recall curve for duplicate detection. 
Trace 1
A B C D
Trace 2
D E
F G
H
bucket 1
3)  Discussion 
In  this  section,  we  discuss  why  crash  graphs  can 
efficiently detect duplicated reports missed by sophisticated 
bucketing  algorithms.  One  simple  explanation  is  that  the 
crash  graph  is  an  aggregation  of  all  crashes  in  a  bucket. 
Comparing  all  crashes  using  crash  graphs  is  more  efficient 
than comparing crashes one by one.  
The  current  WER  bucketing  algorithms  compare  and 
classify  crashes  one  by  one.  When  a  client  sends  a  new 
crash, WER creates a new bucket and makes the first crash 
as the representative for the bucket. If there is another new 
crash, WER compares the crash with representative crashes 
in each bucket to decide if the new crash belongs to any of 
the existing buckets.  
This  comparison  is  computationally  efficient,  but  may 
cause  the  second  bucket  problem.  As  shown  in  Figure  5, 
suppose  Trace  1  is  the  representative  crash  for  Bucket  1. 
Later, Trace 2 and 3 are collected from clients. Suppose the 
trace similarity threshold is 90% – if the similarity of a new 
crash and Trace 1 is over 90%, the new crash will be put in 
Bucket 1. Suppose the trace similarity between Trace 1 and 
2, and Trace 1 and 3 are over 90%. Then, Trace 2 and 3 will 
be put in Bucket 1.  
However, since WER only measures similarity with the 
representative crash, it is possible that trace X is similar to 
one of the other crashes in the bucket, but not similar enough 
with the representative crash to be put in Bucket 1. This may 
result in a second bucket problem as shown in Figure 5.  
However, since crash graphs compare all traces, they are 
more effective for avoiding the second bucket problem. 
trace1
trace2
trace3
...
bucket1
80% similar
90% similar
trace x
Figure 5.   Bucketing algorithms which compare crash traces one by one. 
H
G
A
C
B
D
C
F
D
F
Trace 3
C D E
F
E
E
bucket 2
crash graph from bucket 2
Figure 6.   Second bucket problem due to partial crash trace. Crash graphs 
crash graph from bucket 1
do not suffer from this issue. 
However, when we construct a crash graph from Bucket 
1, and compare similarity using the crash graph, Trace 3 is a 
complete sub-graph of the crash graph from Bucket 1. Our 
crash graph approach identifies them as the same crash. 
B.  Predicting Fixable Crashes 
In this section, we investigate if crash graphs are useful 
to predict fixable crashes, since we believe crash graphs can 
capture crash properties such as fixability. Some crashes will 
not  be  fixed  for  various  reasons.  For  example,  a  crash  can 
occur due to third party software bugs or specific hardware 
issues.  Often,  identifying  the  fixability  of  a  given  crash 
requires  manual  effort.  If  we  can  predict  fixable  crashes  in 
advance with reasonable accuracy, it helps developers triage 
crashes.  
We  first  collect  auto-crash  bug  repots  from  Windows  7 
and  Exchange  14.  From  each  bug  report,  we  construct  a 
crash  graph  and  extract  (machine  learning)  features  from 
each graph. We use the features to train a model to predict if 
a given auto-crash bug report is fixable or not.  
1)  Subjects and Features 
For  our  experiments,  we  use  auto-crash  bugs  from 
Windows  7  and  Exchange  14  obtained  from  field  crashes. 
From  each  auto-crash  bug  report,  we  construct  weighted 
crash  graphs  as  explained  in  Section  III.A.  From  crash 
graphs, we extract the following machine learning features:  
Simple  graph  complexity: We  first  compute  the  graph 
complexity and density [18]. Although graph complexity is 
extracted for graphs in general, it is possible the complexity 
of a crash graph may capture the properties of the crash. We 
use  common  graph  complexity  measures  such  as  the 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 18,2021 at 14:42:01 UTC from IEEE Xplore.  Restrictions apply. 
490node/edge count and max in/out of nodes. Since we are using 
weighted  crash  graphs  for  these  experiments,  we  also  use 
weight related measures such as max/min weights of nodes 
and  edges,  and  min/max  out  weight  sums.  Table  III  shows 
and explains selected features. 
Distance-based  complexity: Besides 
the  simple  graph 
complexity, we extract distance-based complexity measures 
based  on  the  shortest  distance  between  all  pairs  of  crash 
graph  nodes  using  the  Floyd-Warshalls  algorithm  [7].  The 
initial  distance  between  two  connected  nodes  is  set  to  1. 
Then,  we  compute  distance-based  complexities  such  as 
eccentricity, density and radius. For example, the eccentricity 
of a node v is the greatest distance between v and any other 
node.  We  aggregate  all  eccentricities  with  minimum 
(=radius),  maximum  (=diameter)  and  average.  Table  III 
describes  selected  features,  while  detailed  measures  are 
described in [21]. 
Bug metadata: Bug metadata is widely used to classify bug 
reports [2, 10]. In our experiment, we extract features from 
auto-crash bug reports such as hit count, milestone, severity 
and  priority.  The  hit  count  is  very  important  to  prioritize 
crashes to fix. The milestone (version) is also a good feature 
candidate,  since  developers  care  more/less  about  some 
milestones or releases. In addition, severity and priority are 
used as features. 
We  compare  our  crash  graph  feature  based  prediction 
performance  to  a  baseline  approach  which  uses  the  bug 
metadata features. 
TABLE III.  
Group 
Bug  
meta data  
SELECTED MACHINE LEARNING FEATURES TO PREDICT 
FIXABLE BUGS 
Explanation 
Crash hit count 
Features 
Hit Count 
Milestone  Milestone of crashed program 
Severity 
Priority 
Node/edge 
count 
Severity of the crash 
Priority of the crash 
Count of nodes and edges 
Max in/out 
The number of incoming/outgoing 
edges of nodes 
Crash  graph 
features 
In/out ratio  Edge in/out ratio 
Eccentricity  Average distances between nodes 
Density 
Diameter 