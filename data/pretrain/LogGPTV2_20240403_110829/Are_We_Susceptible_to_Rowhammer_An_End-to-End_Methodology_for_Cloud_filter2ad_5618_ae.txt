0x11409 (75.7%) 0x11407 (40.0%) 0x11417 (36.7%)
0x11408
0x11409
0x11408 (76.6%) 0x1140A (76.5%)
0x1140A 0x11409 (75.3%) 0x1140B (74.2%)
0x1140B 0x1140C (80.3%) 0x1140A (79.5%)
0x1140C 0x1140B (77.0%) 0x1140D (76.5%)
0x1140D 0x1140C (76.6%) 0x1140E (75.9%)
0x1140D (77.5%) 0x1140F (76.6%)
0x1140E
0x1140F
0x1140E (77.5%) 0x11410 (39.9%) 0x11400 (37.7%)
0x11411 (77.7%) 0x1140F (40.5%) 0x1141F (37.7%)
0x11410
0x11412 (77.0%) 0x11410 (76.7%)
0x11411
0x11411 (78.1%) 0x11413 (77.2%)
0x11412
0x11414 (77.1%) 0x11412 (76.4%)
0x11413
0x11413 (74.7%) 0x11415 (74.0%)
0x11414
0x11415
0x11414 (77.8%) 0x11416 (77.4%)
0x11415 (79.1%) 0x11417 (78.3%)
0x11416
0x11417
0x11416 (75.8%) 0x11418 (39.4%) 0x11408 (36.8%)
Table III: Adjacency for 16 rows consecutive in the logical
address space.
Row
0x0000
0x0001
0x0002
0x0003
0x0004
0x0005
0x0006
0x0007
0x0008
Adjacent Rows
0x0001 (W)
0x0000 (W)
0x0001 (W)
0x0002 (W)
0x0003 (W)
0x0004 (W)
0x0005 (W)
0x0006 (W)
0x0009 (W)
0x000F (H)
0x0002 (W)
0x0003 (W)
0x0004 (W)
0x0005 (W)
0x0006 (W)
0x0007 (W)
0x0008 (H)
0x0007 (H)
spare row/bank edge (?)
0x07F8 (H)
0x0017 (H)
Table IV: Adjacency for the ﬁrst rows in the bank. (W)
represents half-row adjacency; (H) half-row.
0x1140F and 0x1141F. For brevity, we say a victim has whole-
row (or half-row) adjacency to refer to the adjacency of the
victim to the aggressor row.
Table III presents the row adjacency map inside the DRAM
device for 16 consecutive rows in the logical address space.
We conducted 16 experiments in which a different row acts as
the aggressor, and list the top two or three victim rows sorted
by the fraction of their bits that ﬂip (shown in parentheses);
all remaining victims have fewer than 1% of their bits ﬂip.
The data shows bimodal behavior: many rows map linearly
(whole-row adjacency), but some have each of their halves
mapped differently (half-row adjacency). Half-row adjacency
in shown in bold in Table III.
When examining different portions of the adjacency map,
we found that half-rows occur frequently but lack a speciﬁc
pattern. Table IV shows the adjacency map for the ﬁrst rows of
the bank. Row 0 is half-adjacent to row F, but the remaining
half is not adjacent to any other row in the bank. Half of
row 0 is located either next to a spare row [40] or on the
physical edge of the DRAM array. Also, some rows shown in
Table IV have an adjacency pattern different from all others.
For example, row 7 is half adjacent to rows 8 and 0x07f8.
Fewer bits ﬂip in half-rows than in whole-rows. We
characterize whether a bit’s position inﬂuences its likelihood
to be ﬂipped. We start by grouping bit ﬂips in a victim row by
their column addresses (a column address speciﬁes a word). A
row has 1024 columns, and a column contains a 64-bit word.
Figure 15a shows data from a whole-row victim. Although
some words have more bit ﬂips than others, this variation is
relatively low: more than 95% of all words have 40 to 60 bit
Figure 13: Number of bit ﬂips per row when suppressing REFs
for 15 sec in the absence of Rowhammer, for a representative
set of rows.
results in this section are based on using a single aggressor
row, similar to mounting a single-sided Rowhammer attack.
Our results show no differences between the DIMMs from
the same memory vendor. Most results are similar across
DIMMs supplied by different vendors; in these cases, we
present the results from a single vendor (referred to as vendor
#1). However, we note different vendors in the text when
results differ across vendors.
We veriﬁed that our fault injector suppresses refreshes on
all three Intel server architectures. We also reverse engineered
portions of the row adjacency maps of three DIMMs (one from
each vendor) on both Broadwell and Skylake and checked that
the results are identical on both platforms. On Cascade Lake,
we reverse engineered only one DIMM, with identical results
to Broadwell and Skylake. The data shown in the remainder
of this section was gathered on the Skylake platform.
Results. We started with a baseline experiment
in which
we suppressed REFs for 15 seconds without any additional
memory workload. Our goal was to determine the rate of bit
ﬂips due solely to suppressing REFs. These bit ﬂips are not
correlated with physical row adjacency. If we observed many
such bit ﬂips, combining suppressing REFs with Rowhammer
attacks would make it difﬁcult to attribute responsibility for
the bit ﬂips.
Fortunately, Figure 13 shows that the number of ﬂipped
bits is low. Even after suppressing REFs for 15 seconds, the
majority of rows show no failures. Ten rows have a single
ﬂipped bit (a row failure rate of 3.3%), and two have two
ﬂipped bits (a row failure rate of 0.7%). No row has more than
two ﬂipped bits. The low number of failures demonstrates that
our DRAM is resilient when not refreshed in the absence of
Rowhammer attacks.
Logical rows do not always map linearly. Figure 14a shows
the number of ﬂipped bits per row when the aggressor was row
0x11411 (logical address). The results suggest a linear map
because most bit ﬂips occur on rows 0x11410 and 0x11412
(the y-axis is logarithmic). Row 0x11410 has 50,274 ﬂipped
bits (out of 65536, or 76.7%), and row 0x11412 has 50,489
(77.0%). All other rows have signiﬁcantly fewer ﬂipped bits.
However, the map is not always linear. Figure 14b shows the
results when the aggressor row address was 0x11410. While
victim row 0x11411 is adjacent (with over 77% of its ﬂipped
bits), victim row 0x1140F, although adjacent in the logical
address space, has only 26,566 ﬂipped bits corresponding to
40.5% of its bits. Instead, a third victim row (0x1141F) has
24,680 of its ﬂipped bits (37.7%). These results indicate that
the aggressor row 0x11410 is adjacent to half of victim rows
11
0x1136c0x113d00x114340x11498LogicalRowNumber0123BitsFlippedperRowNoRowhammer(baseline)(a) Victim rows: 0x11410 and 0x11412.
(b) Victim rows: 0x11411, 0x1140F, and 0x1141F.
Figure 14: No. of bit ﬂips on rows with neighboring logical row numbers (y-axis is logarithmic). A row has 65,536 bits.
(a) Victim has whole-row adjacency.
(b) Two victim rows, each with half-row adjacency.
Figure 15: No. of bit ﬂips in each word of a victim row (the column address speciﬁes the word). A word has 64 bits.
(a) Victim has whole-row adjacency.
(b) Two victim rows, each with half-row adjacency.
Figure 16: No. of bit ﬂips in a representative victim row by their bit index positions within a memory word.
ﬂips, and a word has 47.6 bit ﬂips on average. This result
suggests that an aggressor row affects all words in a victim
whole-row more or less equally.
Half-row victims have half the number of bit ﬂips per word
(the remaining non-adjacent half is safe). Figure 15b shows the
number of bit ﬂips per word found in two half-row victims.
Each word has 25 and 23 bit ﬂips, respectively, on average
as opposed to 47.6 bit ﬂips for words located in a whole-row.
This result suggests that an aggressor row affects fewer bits
per word in victim half-rows than victim whole-rows.
All bits are equally susceptible for whole-row, but not
for half-row, adjacency. We further investigated whether the
position of a bit in a word affects its likelihood of being
ﬂipped? For this, we re-plotted the data from Figure 15 by
grouping bits by their bit position rather than their column
address. A row has 1024 bits in each bit position. Bit positions
are indexed 0 to 63 from the least to the most signiﬁcant bit.
Figure 16a shows the results of a whole-row victim. As
before, an aggressor row affects all bits in a victim whole-row
equally. Figure 16b shows the results for two half-row victims.
Surprisingly, the sets of bit positions are disjoint. One of the
half-row victims has bit ﬂips in positions 0 through 31 only,
whereas the other has bit ﬂips in positions 32 through 63.
These results indicate that the position of a bit determines its
likelihood of being ﬂipped in half-rows only. In a victim half-
row, either the most signiﬁcant (63-32) or the least signiﬁcant
(31-0) bits are ﬂipped, depending on memory geometry. For
little endian systems (such as ours), the region containing bit
ﬂips will inversely map to the most or least signiﬁcant bits,
respectively. For example, in big endian systems, the map
will be direct: words in row 0x1140F will have their least
signiﬁcant bits ﬂipped, while words in row 0x1141F, their
most signiﬁcant. All DIMMs from all three hardware vendors
exhibited this behavior.
Most, but not all, bits ﬂip from 1 to 0. Electromagnetic cou-
pling (considered to be a main reason for Rowhammer [69])
drains capacitors faster than normal. Memory encodings can
represent a charged capacitor as either a 1 or a 0, making the
data pattern another factor in a bit’s susceptibility to be ﬂipped.
Cells that encode data value 1 as a charge are called true-cells,
while anti-cells encode data value 0 as a charge [75].
To examine this effect, we seeded memory with four data
patterns: all 1s, two-thirds 1s (0xB6DB6D...), one-third 1s
(0x492492...), and all 0s. Figure 17 shows the number of bit
ﬂips in a victim row for different data patterns. This number
is directly proportional to the number of bits seeded with a
value of 1: 79.7% for all 1s, 57% for two-thirds 1s, 29.9% for
one-third 1s, and 3.8% for all 0s. While bits can ﬂip in both
directions, most bit ﬂips were seeded with a value of 1.
DIMMs from vendors #2 and #3 have fewer bit ﬂips. We
repeated the experiments with DIMMs sourced from the other
two hardware vendors. Vendor #2 has fewer bit ﬂips per row
12
0x1140A0x1140B0x1140C0x1140D0x1140E0x1140F0x114100x114110x114120x114130x114140x114150x114160x114170x11418LogicalRowNumber100101103105#ofBitsFlippedAggressorrow:0x114110x1140B0x1140C0x1140D0x1140E0x1140F0x114100x114110x114120x114130x114140x114150x114160x114170x114180x114190x1141A0x1141B0x1141C0x1141D0x1141E0x1141F0x11420LogicalRowNumber100101103105#ofBitsFlippedAggressorrow:0x1141001282563845126407688961023ColumnAddress03264#ofBitsFlippedAggressorRow:0x11410;VictimRow:0x1141101282563845126407688961023ColumnAddress032032#ofBitsFlippedAggressorRow:0x11410;VictimRow:0x1140fAggressorRow:0x11410;VictimRow:0x1141f0(LSB)816243240485663(MSB)BitPosition02565127681024#ofBitsFlippedAggressorRow:0x11410;VictimRow:0x114110(LSB)816243240485663(MSB)BitPosition02565127681024#ofBitsFlippedAggressorRow:0x11410;VictimRow:0x1140fAggressorRow:0x11410;VictimRow:0x1141fTesting a single bank of our server-class DIMMs using this
methodology takes 11 hours and 36 minutes. Because our
DIMMs have 16 banks, testing an entire DIMM would take
about a week. We adopted several optimizations to better scale
our methodology. First, we test several banks in parallel; we
concurrently test eight different banks on the same DIMM with
little interference. Second, rather than testing a row followed
by checking it for bit ﬂips, we batch multiple tests back-to-
back and follow them with a single check at the end. This
reduces the time needed for the checking step. With these
optimizations in place, our methodology can test an entire
DIMM in less than one day. In the future, we plan to further
scale up our methodology to simultaneously test multiple
DIMMs by ensuring the tested DIMMs share no channels.
In small-scale experiments with our
six server-class
DIMMs, we found it very difﬁcult
to ﬂip bits using our
testing methodology. At normal DRAM refresh rates (with
ECC disabled), we observed only two bit ﬂips on a single
DIMM. Despite many additional tests with the same aggressor
rows, we were unable to reproduce these bit ﬂips. However,
at lower refresh rates, the same DIMM showed hundreds of
bit ﬂips (when we increased the refresh interval by a factor of
3.5x). We were unable to produce bit ﬂips in our other DIMMs
even with a reduced refresh rate.
Our methodology has several limitations we plan to address
in future work.
Handling TRR. Some DDR4 DRAM claims it supports
Targeted Row Refresh (TRR) [61], a Rowhammer defense in
which the DIMM aggressively refreshes rows under attack.
However, researchers have mounted successful Rowhammer
attacks to such memory [77], [94], [2], [3], [18]; in these
cases, it is unclear whether TRR is ineffective or not yet
enabled. Because the details of TRR implementations remain
unknown, we designed our methodology to use the instruction
sequence with the highest rate of row activations. If TRR im-
plementations are reverse engineered, our methodology could
be adapted to use an instruction sequence that bypasses TRR
defenses while maximizing the row activation rate.
Scaling limitations. Unfortunately, our methodology for
reverse engineering row adjacency in a DIMM requires the
placement of a hardware fault injector between a DIMM and
its slot. This manual step creates too much overhead and
disruption to be performed at large scale. Instead, in practice,
we make a simplifying assumption: similar DIMMs sourced
from the same vendor have the same row adjacency map.
Another limitation stems from the choice of using the A14
bit in our current fault injector design (described in Section V).
For ACT commands, this bit encodes a row address. Currently,
our reverse engineering methodology cannot use a row whose
address has a high bit in A14. Our methodology therefore
tests only half the rows in a bank (those with a value of ’0’