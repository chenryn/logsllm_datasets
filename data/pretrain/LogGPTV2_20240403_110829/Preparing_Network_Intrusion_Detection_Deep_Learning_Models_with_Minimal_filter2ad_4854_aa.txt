title:Preparing Network Intrusion Detection Deep Learning Models with Minimal
Data Using Adversarial Domain Adaptation
author:Ankush Singla and
Elisa Bertino and
Dinesh C. Verma
Preparing Network Intrusion Detection Deep Learning Models
with Minimal Data Using Adversarial Domain Adaptation
Department of Computer Science
Department of Computer Science
Thomas J. Watson Research Center
Ankush Singla
Purdue University
PI:EMAIL
Elisa Bertino
Purdue University
PI:EMAIL
Dinesh Verma
IBM, Yorktown Heights
New York, USA
PI:EMAIL
ABSTRACT
Recent work has shown that deep learning (DL) techniques are
highly effective for assisting network intrusion detection systems
(NIDS) in identifying malicious attacks on networks. Training DL
classification models, however, requires vast amounts of labeled
data which is often expensive and time-consuming to collect. Also,
DL models trained using data from one type of network may not be
able to identify attacks on other types of network or identify new
families of attacks discovered over time. In this paper, we propose
and evaluate the use of adversarial domain adaptation to address
the problem of scarcity of labeled training data in a dataset by
transferring knowledge gained from an existing network intrusion
detection (NID) dataset. Our approach works for scenarios where
the source and target datasets have same or different feature spaces.
We demonstrate that our proposed approach can create highly
accurate DL classification models even when the number of labeled
samples in the target dataset is significantly small.
CCS CONCEPTS
• Security and privacy → Intrusion detection systems; • Com-
puting methodologies → Transfer learning; Neural networks.
KEYWORDS
Deep Learning, Intrusion Detection, Transfer Learning, Neural
Networks
ACM Reference Format:
Ankush Singla, Elisa Bertino, and Dinesh Verma. 2020. Preparing Network
Intrusion Detection Deep Learning Models with Minimal Data Using Ad-
versarial Domain Adaptation. In Proceedings of the 15th ACM Asia Con-
ference on Computer and Communications Security (ASIA CCS ’20), Octo-
ber 5–9, 2020, Taipei, Taiwan. ACM, New York, NY, USA, 14 pages. https:
//doi.org/10.1145/3320269.3384718
1 INTRODUCTION
Network intrusion detection systems (NIDS) detect malicious activ-
ity in a network (denial-of-service, probing, unauthorized access
etc.) by monitoring and analyzing the traffic flowing through the
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ASIA CCS ’20, October 5–9, 2020, Taipei, Taiwan
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-6750-9/20/10...$15.00
https://doi.org/10.1145/3320269.3384718
network. Traditional NIDS suffer from several drawbacks, includ-
ing: inability to detect new or unseen attacks, as in the case of
signature-based NIDS; time-consuming training phase and huge
numbers of false positives, as in the case of anomaly-based NIDS.
Deep learning (DL) based techniques have thus been proposed to
aid NIDS in addressing those drawbacks [6, 18, 36]. DL classifica-
tion models learn from labeled training data containing samples of
malicious and benign network traffic and apply this knowledge to
categorize new unseen data at runtime.
Training DL models, however, requires large amounts of labeled
data. Collecting and labeling data for different newer attack families
is often very time consuming and costly. In addition to this, labeled
data from one type of network may not be suited for training DL
models for another type of network. For example, a dataset used
to train a DL model for a traditional network may not work well
for mobile or IoT networks. Furthermore, very often the attack
distribution changes with time as the types of network devices,
protocols and attack vectors evolve, thus requiring new labeled
data to keep the DL models up to date.
In this paper, we propose the use of Domain Adaptation (DA) to
reduce the amount of labeled data required for training DL network
intrusion detection (NID) models. DA is a transfer learning tech-
nique that allows one to transfer knowledge from a source domain
with adequate training data, to a different but similar target domain
with minimal or no new training data. For example, a DL object
detection model prepared for identifying traffic signs by training
on a labeled dataset containing only images from USA might not
perform well in another country because of differences in back-
ground, language and traffic rules. However, the source DL model
can be re-purposed to operate in the target country by using DA
and only a small labeled set of traffic sign images from that country.
We already have a wide variety of public NID datasets [42] that
contain labeled data for identifying various different attack cate-
gories. Leveraging these existing NID datasets, DA can enable us
to prepare up-to date and highly accurate DL models for NID that
can identify the latest malicious attacks being discovered in the
wild as well as be able to work in specific types of network, like IoT
and mobile networks, with minimal new training data. In our work,
we use a specific form of DA, referred to as adversarial DA, that
leverages generative adversarial networks (GANs) [16] for creating
a domain-invariant mapping of the source and target datasets.
In this paper, we consider two broad scenarios to evaluate the
efficacy of adversarial DA in training accurate NID models based on
whether the source and the target datasets have a similar or different
feature space. ❶ We have a source NID dataset with a large amount
of labeled data samples and a target dataset with a few labeled data
Session 3: Network Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan127samples with the same same feature space. This is applicable to
cases in which the target dataset is created by collecting packet
data from networks using the same protocols as the source dataset,
maybe at a later period in time. The target dataset may contain
new attack types that were not present in the source dataset. In this
case, we want to be able to accurately detect both the new attack
types as well as the old attack types. ❷ In other cases, the source
NID dataset might have a different feature space than the target
NID dataset. This scenario applies to cases in which the network
protocols and the types of devices in the networks are different for
the source and target datasets, thus changing the feature space; for
example IoT networks might use different protocols than traditional
networks. In addition, the tools used to capture and process network
traffic might evolve allowing the capture of much more meaningful
features.
In the paper, we present an adversarial DA methodology for
training highly accurate DL models for both these scenarios when
the target NIDS has minimal or no training data. We first apply data
pre-processing techniques to prepare the source and target datasets
for adversarial DA and apply a dimensionality reduction technique
called Principal Component Analysis (PCA) to reduce the feature
dimensions of our datasets. In the case of source and target datasets
with different numbers of feature dimensions, PCA will serve the
additional purpose of generating intermediate data representations
that have an equal number of dimensions. We then use adversarial
DA to map the source and target datasets into a domain-invariant
representation using a custom GAN architecture, which results in
a DL classification model that can categorize the benign and attack
samples in both the source and the target datasets accurately.
Our paper has the following technical contributions:
(1) We introduce an adversarial DA methodology to train highly
accurate DL models with minimal new labeled data for a
NID in two specific scenarios:
• Source and target datasets with similar feature spaces.
• Source and target datasets with different feature spaces.
(2) We implement and test the described techniques on two
popular publicly available NID datasets.
(3) We evaluate the efficacy of the suggested DA techniques
with respect to the amount of data available in the target
dataset.
The rest of the paper is organized as follows. We introduce ter-
minology and basic concepts relevant to the paper in Section 2.
We then discuss related work in Section 3. We provide the detailed
description of our approach and techniques in Section 4. We then
describe the datasets selected for our evaluation, experimental setup
and other details in Section 5. We discuss the results of our experi-
ments in the Section 6. Finally, we outline conclusions and future
work in Section 7.
2 BACKGROUND
In this section we introduce the concepts and terminology relevant
to our methodology for training DL models for NID.
Figure 1: Architecture of a generative adversarial network
(GAN). The generator G learns to produce fake images from
a random noise vector z to fool the discriminator D, whose
goal is to differentiate between real and fake images.
2.1 Network intrusion detection systems
(NIDS)
Network intrusion detection systems monitor a network or a system
for malicious attacks or policy violations. NIDS are broadly clas-
sified into misuse-based and anomaly-based. Misuse-based NIDS
look for some specific patterns or signatures of previously identi-
fied attacks in the network traffic, system calls, etc. Misuse-based
NIDS however, require upto date databases containing rules and
signatures for all the attacks, making them ineffective against 0-
day attacks. Anomaly-based techniques learn the normal network
behavior during a training phase and then identify deviations from
the normal behavior in the network behavior at run-time. Unlike
misuse-based NIDS, they can detect newer attacks; however, they
can have high false positive rates (FPR) as they can flag any unseen
yet benign network behavior or system use as a potential malicious
attack. They have a time-consuming training phase and need to be
trained separately for every network deployment.
2.2 Deep Learning (DL)
Neural networks (NN) are systems inspired by the human brain and
how the network of neurons processes information and performs
computations [50]. Basic feed-forward NNs contain artificial neu-
rons containing a mathematical transformation function organised
in different layers. The layers between the input and the output
layers are known as hidden layers. Each hidden layer receives input
from the previous layers, uses its transformation function on them
and passes its output to the next layers. Each neuron learns and
optimizes the weights and biases for its transformation function
during the training phase to minimize some cost function measur-
ing the distance between the values generated by the NN and the
actual values. A deep neural network (DNN) is an NN with more
than one hidden layer between the input and output layers. Deep
learning (DL) is a sub-field of machine learning (ML), based on the
use of DNNs.
2.3 Generative adversarial networks (GAN)
An adversarial nets framework [16] consists of two NN models: a
generative model called generator (G) and a discriminative model
called discriminator (D). These NN models are pitted against each
other in an adversarial game over a certain real data distribution.
The generator’s goal is to generate a synthetic data sample that
GDFake	ImagesReal	Images[Real]or[Fake]ZSession 3: Network Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan128Figure 2: This figure illustrates how adversarial DA algo-
rithms map two separate datasets into a common domain-
invariant mapping. The two datasets are represented by
squares and circles, and each has two classes represented by
colors blue and red.
looks like it belongs to the real data distribution. The discrimina-
tor’s goal is to check any data record and distinguish whether it
is a real data sample or generated by G. The generator and the dis-
criminator learn their tasks simultaneously and get better at their
respective goals. The goal of this adversarial game is for G to pro-
duce synthetic data that D cannot distinguish from the original data.
Fig. 1 gives an example of a GAN that takes fake images that look
like they belong to the real data distribution. The generator G takes
a random vector z as an input and generates a set of fake images.
The discriminator D takes the real and fake generated images as the
input and tries to classify them into real and fake categories. The
training of GANs is modeled as a mini-max game where G and D
are trained simultaneously using back-propagation, and G tries to
minimize the value function V(G, D) provided below, while D tries
to maximize it:
max
D
V(G, D) = Ex[loд D(x)] + Ez[loд(1 − D(G(z))]
min
G
where: Ex is the expected value over all real data instances, Ez is
the expected value over all the fake generated data instances, D(x)
is the probability of D predicting a real instance as real, and D(G(z))
is the probability of D predicting a fake generated instance as real.
GANs have been shown to be effective at generating synthetic
images indistinguishable from real images by training on image
databases like MNIST [28], CelebA [23], and CIFAR-10 [26]. GANs
have also been used in language and speech processing [33, 38, 53],
domain adaptation [4, 19, 45, 49], and data augmentation [11, 12, 31].
2.4 Domain adaptation (DA)
The concept of transfer learning (TL) allows ML algorithms to use
the knowledge gained by learning one or more source tasks for
learning a target task. TL is especially useful when we do not have
enough training data in a particular domain, but there is a lot of
training data available in another different but similar domain. TL
has been shown to significantly improve ML algorithm accuracy
in applications like image classification [35, 41], text classification
[8, 10], and speech recognition [9, 20].
In any TL scenario, the source and target may either differ in do-
main (the feature space and the feature distribution of the datasets)
or task (the label space and the objective predictive function learnt
from the training data). Pan et al. classify TL into three broad
categories based on the difference between the source and target
Figure 3: Architecture of the generic framework for adver-
sarial DA using GANs. The generator(s) G learns (learn) to
produce a common domain-invariant mapping of the source
and target datasets. This mapping is used as an input for the
discriminator D which tries to predict whether the sample
belongs to the source or the target domain. The mapping is
also used as an input to a classifier which predicts which
class the sample belongs to. The final goal is to minimize
both the domain prediction and the class prediction loss.
domain, task and availability of labeled data [37]. ❶ Inductive: the
target domain can be similar or different from the source domain
but the target task is always different from the source task, and
some labeled data is available in the target domain; ❷ Transductive:
the target domain is related but different from the source domain
but the source and target tasks are the same; ❸ Unsupervised: the
target and source domains and tasks are different and no labeled
data is available in either source and target domains [37].
Domain adaptation (DA) is a special case of transductive TL i.e.
the source and target tasks are the same, and the domains are related
but different. DA can be further categorized into homogeneous and
heterogeneous, based on whether the source and target domains
have the same or different feature spaces, respectively [51]. Based
on the amount of labeled data present in the target domain, each
of these categories can be further classified into supervised DA
(small amount of labeled data available in the target domain), semi-
supervised DA (a small amount of labeled data and a lot of unlabeled
data are available in the target domain), unsupervised DA (no labeled
data but an adequate amount of unlabeled data is available in the
target domain).
Most DA methods try to align the source and target data dis-
tributions to learn a domain-invariant mapping of the datasets by
one of following three methods. ❶ by minimizing some distance
metric that calculates domain shift among the source and target
data distributions [17, 44]. ❷ By reconstructing the target and/or
source data into a domain-invariant mapping [5, 14, 15]. ❸ By us-
ing GANs to map the source and target data distributions into a
common subspace by using domain confusion [21, 29, 48].
Adversarial DA is a class of algorithms that leverages GANs
to learn a common domain-invariant mapping between the source
and target datasets (see Fig. 2), using an adversarial learning frame-
work. Most adversarial DA approaches employ some version of a
generic GAN framework (shown in Fig. 3) differing by: whether
the underlying models are generative or discriminative, which loss
functions are used, and whether weights are shared between G and
D [48]. The CoGAN framework uses two GANs, in which G and D
−10123−3−2−1012345Before Adversarial DA−101234−0.60−0.59−0.58−0.57−0.56−0.55−0.54−0.53After Adversarial DAGDDomain	PredictionSourceDataGTargetDataDomain-Invariant	MappingSharedWeightsClassifierClass	PredictionSession 3: Network Security ASIA CCS '20, October 5–9, 2020, Taipei, Taiwan129Before PCA
2
c
p
4
2
f1
2
2
4
4
6
6
2
f
4
2
0
f1
f2
0
0
0
4
2
0
−2
−4
−4 −2
pc1−4 −2
pc2−4 −2
After PCA
0
pc1
0
0
2
2
2
4
4
4