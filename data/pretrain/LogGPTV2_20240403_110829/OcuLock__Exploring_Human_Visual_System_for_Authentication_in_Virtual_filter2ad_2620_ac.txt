Physiological
Physiological
Physiological
Physiological
Physiological
Physiological
Physiological
Physiological
Behavioral
Behavioral
Behavioral
Behavioral
Behavioral
Behavioral
Component
V
V
V
H & V
H
H
V
V
H & V
H & V
H & V
H & V
H & V
H & V
H & V
reliability by considering low-level HVS biostructure and
behavior. Given the trace of HVS activities recognized in
Section VI-A, we go back to the EOG signals and extract these
features. For example, based on the time interval of a blink,
we can derive the eyelid stretch extent by inspecting the EOG
amplitude during that interval. We ﬁrst extracted a long list
of features. Then we tested the impact of removing each one
feature from the model. If the accuracy of the model remained
the same after we removed a feature, then this feature was
removed permanently. For example, the median eyelid close
speed was removed since it has duplicated effects as eyelid
close speed distribution.
The list of features is summarized in Table I. Most features,
e.g., eyelid close speed, have multiple samples because an HVS
activity, e.g., eye blink, can happen multiple times in an EOG
record. In this case, we store the feature as a distribution in
the form of probability density function (PDF). This unique
design enables OcuLock to capture a comprehensive view of
the feature compared to previous eye-based feature extraction
that generates a single scalar number for a feature. For features
with a single sample, e.g., metabolism intensity, we represent
them via a scalar number as well. We extract features from
both horizontal and vertical EOG signals except for the eyelid-
related features that are only extracted from EOGv. We
indicate this by “H” or “V” in Table I.
Physiological Features. Eyelid features decided by the
unique eyelid biostructure and extraocular muscles are ex-
tracted from the original EOG signal and the eyeblink trace.
Each blink is presented as a peak in the original EOG signal
(before transform), where upward-going signal indicates the
eyelid close phase and downward-going signal implies the
eyelid open phase. Hence, eyelid close speed can be calculated
by the slope of the upward-going EOG segment and eyelid
open speed can be computed by the slope of the downward-
going EOG segment. Eyelid stretch extent signiﬁes the largest
extent the eyelid can move and can be represented by the
maximum amplitude of EOG signal during eyelid close phase.
As discussed in Section II-B, the metabolism intensity of
RPE, uniquely determined by surrounding cell conditions, can
be revealed by the values of standing potential [4] and thus
can be measured by EOG. We derive metabolism intensity
by calculating the Arden Ratio. Arden Ratio is of positive
correlation with the RPE metabolic rate and has been used
Fig. 9: Saccade/ﬁxation (left) and blink (right) detection algo-
rithms can be optimized by seeking the best thresholds.
50 steps. As shown in Figure 9, the algorithm achieves the
highest F1 score when thsd reaches 0.036. We therefore select
it for saccade and ﬁxation recognition.
2) Blink Recognition.: As the biostructure features of the
eyelid are only presented when the eyelid is moving, i.e.,
during a blink, we need to recognize all eye blinks before the
feature extraction. A blink is the rapid closing of the eyelid
accompanied by a rapid eye globe rotation. In the wavelet-
transformed domain, the vertical signal component appears as
a signal peak followed by a signal valley [5] without a long
interval. Hence, blinks can be recognized by ﬁrst applying two
thresholds to the transformed EOGv to identify segments as
peaks or valleys, i.e., thbd and −thbd (two horizontal green
dashed lines). An interval threshold tht is then adopted to
drop those segments that are successive saccades. As shown
in Figure 8, the segment marked with “B” represents a blink.
To optimize the thresholds for OcuLock, we identiﬁed 359
blinks as ground truth from the same set of EOG records
mentioned above. We compared the recognized blinks with
ground truth across varying thbd from 0.01 to 0.08 (in 70
steps) and different tht (50, 100, 150, and 200 ms). As shown
in Figure 9, the algorithm achieves the optimal F1 score of
0.972 using thbd at 0.052 and tht at 200 ms, which are selected
for blink recognition.
B. Feature Extraction
In contrast to existing eye gaze authentication for smart-
phones and PCs with a high error rate and variability, OcuLock
explores the HVS as a whole and improves the performance
7
by doctors to examine the metabolism of RPE cells [43]. To
calculate the Arden Ratio, we ﬁrst search through the entire
EOG signals and derive the absolute values of the signal at
all peaks and valleys. The ratio between the maximum and
minimum absolute values is derived as the result.
The distinctive size and shape of the eye globe result in
different rotating and reachable distance of eyes for different
users. The EOG signal has a linear relationship to the rotating
angle of eyes and the coefﬁcient is determined by the standing
potential [4]. We approximate the standing potential by the
Arden Ratio [43] and then derive the rotating range in four
directions (up, down, left and right) by dividing the EOG
amplitude by the Arden Ratio. For example, the extent of right
rotating distance represents the angular distance between the
central reference point and the rightmost point the eye can
reach. We calculate this feature by the maximum amplitude of
all peaks in EOGh divided by the horizontal Arden Ratio.
Sympathetic signals show unique energy patterns which de-
pend on the nature and activeness of individual’s sympathetic
nerve systems. Such signals concentrate between 0.05 and 0.5
Hz frequency band of EOG signals. We derive its frequency
domain information by re-using the wavelet transform results
of EOG signals across the above frequency bands. Since this
is already computed in the signal processing step, sympathetic
signals can be extracted without additional overhead.
Behavioral Features. This category of features character-
izes the voluntary movement of the eye globe that signiﬁes
users’ unique viewing habits and preferences. They are ex-
tracted from the trace of ﬁxations and saccades detected from
EOG signals.
We extract the start time, duration, and centroid position
of all ﬁxation instances. Fixation centroid can be derived by
computing the average EOG signal amplitude during a ﬁxation,
which represents the horizontal and vertical offset with respect
to the resting position. Since these features are stored as a
distribution, ﬁxation start time and duration imply the temporal
characteristic of ﬁxations while ﬁxation centroid indicates the
spatial property of ﬁxations. Similarly, we extract the saccade
start time, saccade duration and saccade location. Saccade
location represents EOG values at ﬁve moments during a
saccade: the beginning, the ﬁrst quarter time of the saccade, the
medium moment, the third quarter time, and the ending. Both
temporal and spatial characteristics of saccades are extracted.
C. Authentication Decision
To make the authentication decision, previous biometric
authentication systems [29], [19], [7] use the extracted features
to directly trained a classiﬁer in order to differentiate a given
legitimate user from all other users. As a result, a classiﬁer
is built for each enrolled owner. Every time a new owner is
enrolled, a new classiﬁer has to be trained from scratch to
recognize the new owner’s feature patterns. Such a method-
ology requires extra overhead for classiﬁer training during
owner enrollment and thus could degrade users’ experience
in interacting with the authentication system.
To address this issue, we propose a new authentication
mechanism for OcuLock to utilize the distinctive features. Af-
ter features are extracted from the EOG signals, a comparison
Fig. 10: Experiment setup.
algorithm is adopted to compare each feature of the input EOG
with that of the owner’s EOG. A matching score indicating
the similarity and ranging from 0 to 1 is generated for each
feature. The resulting matching scores for all features are fed
to a comparator to determine whether the input EOG matches
the owner’s EOG, i.e., whether the current user is the owner.
If the matching scores of all features are high, the input record
is determined to be from the owner.
Similar to previous methods,
this procedure stores the
features, or a template, of the owner. However, it does not
require repeated classiﬁer training for each enrolled owner.
The comparison algorithm can accept
the features of any
input user and any owner and gauge the similarity. Therefore,
only one comparator that makes the authentication decision
based on a set of matching scores needs to be trained. This
mechanism signiﬁcantly reduces enrollment complexity and
improves system usability.
As we will show in Section VIII,
the choice of the
comparison algorithm and the machine learning model to build
the comparator affect the authentication performance. Hence,
it is important to select the optimal comparison algorithm and
comparator model.
VII. EXPERIMENT SETTING
Apparatus. As shown in Figure 10,
the prototype of
OcuLock consists of a VR HMD, a EOG acquisition device,
and a laptop. The VR HMD is a Lenovo Mirage Solo, the
ﬁrst standalone VR HMD powered by Google Daydream
[18]. The EOG acquisition device is a BIOPAC MP36R that
measures EOG at a sampling rate of 200 Hz via ﬁve Ag-AgCl
series lead electrodes. The Dell Inspiron 5577 laptop with a
2.8 GHz CPU is connected with the acquisition device for
processing the signal records. The authentication decision is
then sent back to VR HMD. Our proof-of-concept prototype
adopts a separate data acquisition and processing device for the
purpose of software compatablity. However, we point out that
integrated device including above three component is already
commercially available [32]. Therefore, our prototype design
does not decrease the potential of OcuLock.
Subjects. We recruited 70 subjects (27 females and 43
males, age from 19 to 32) through public advertisements and
email lists. All participants are university students. Among
these subjects, 24 of them wore glasses and they did not
remove their glasses during the experiments. 23 subjects have
8
(a) Fixed-Route;
Fig. 11: F1 scores for three stimuli using different comparison algorithms and comparator models.
(b) City-Street;
(c) Illusion.
used VR before while 47 never used it. Subjects were told
that their EOG will be recorded to extract unique features and
differentiate themselves from others. They signed a written
consent form in accordance with an existing IRB approval
we hold which allows for recording EOG and other responses
from human subjects for user authentication and VR system
evaluation. A subject sat in a chair in a relaxed posture and
wear the VR HMD with electrodes to view the three visual
stimuli. Five electrodes were ﬁxed on the HMD cover, the elec-
trode positions on different participants were the same. Each
stimulus was viewed for 10 seconds and the corresponding
EOG was collected. Each subject viewed the 3-stimuli session
for 10 times and a total of 700 EOG records were generated
for each stimulus.
Training and Testing Procedure. OcuLock uses a new
record-comparison based scheme for authentication decision.
To generate training and testing data for the decision-making
comparator, the subjects are randomly divided into two halves
for training and testing. For the 35 subjects for training, any
two records of the 350 records are compared to generate
61,075 samples as the training data. Each sample indicates
whether or not the two records are from the same person. A
total of 1,575 samples are from comparison between the same
subject, i.e., positive samples, while the others are negative
samples. Similarly, the testing set also has 61,075 samples and
is used for model evaluation. We repeat the above procedure
for 10 times and report the average results in the following.
Evaluation Metrics. While accuracy is a popular way to
evaluate a machine learning model, the unbalanced composi-
tion of our testing data could generate misleading accuracy. A
comparator could achieve 97% accuracy even if it predicts all
sample as negative. We instead use Equal Error Rate (EER)
and F1 score that have been widely used in authentication
systems. The EER is the rate when the false acceptance rate
(FAR) and the false rejection rate (FRR) are equal.
VIII. RELIABILITY EVALUATION RESULTS
In this section, we discuss the reliability of our system un-
der different impact factors such as the authentication duration,
the subset of selected features and the performance degradation
over time.
Fig. 12: The EERs using different authentication duration.
A. Choices of Algorithm and Model
To ensure that OcuLock achieves its best performance, we
test different comparator models including k-nearest neighbors
algorithm (kNN), a Support Vector Machine (SVM) using the
Gaussian radial basis function as the kernel, an SVM using a
linear kernel, and an SVM using a polynomial (poly) kernel.
Multiple comparison algorithms including Ansari-Bradley Test
(AB), Two-Sample Cramer-von Mises Test (CM), Two-Sample
Kolmogorov-Smirnov Test (KS), Mann-Whitney U-Test(MW),
and Two-Sample t-test (TS) [20] are also tested. Figure 11
shows the F1 scores for each combination of comparison
algorithm and comparator model. The F1 scores reach ∼ 98%
due to the unique and comprehensive features considered
in OcuLock. We also observe that AB Test achieves better
performance. This is because many proposed features are
distributions rather than scalar numbers. AB Test can capture
the shape information between two distributions and thus
characterize each user’s EOG more accurately. We herein
select the optimal combination for the remaining evaluations.
B. Time Efﬁciency
A practical authentication system should be able to ac-
curately identify the user within an acceptable amount of
time. To study the impacts of authentication duration on the
performance, we repeat the experiment procedure described
in Section VII for all 70 subjects with the viewing duration
for stimuli changed to 3, 5 and 7 seconds. The EER results
are demonstrated in Figure 12. Using 10-second records, the
three stimuli reaches EERs of 5.27%, 7.32% and 3.55% with
9
Fig. 13: The information gain of each included feature.
Fig. 14: ROC curves for three types of feature subsets.
standard deviations of 1.41%, 1.48% and 1.34%, respectively.
Decreasing duration slowly increases the EERs. For example,
the 3-second authentication achieves an EER of 5.75% for Il-
lusion stimulus. This result suggests a small trade-off between
convenience and security.
We also observe that among all
three stimuli Illusion
achieves the best performance because it elicits more micro-
saccades and blinks, as well as extraocular cell and muscle
activities. To evaluate other impact factors of OcuLock, we
use Illusion as an example in the following Section VIII-C
and VIII-D.
C. Feature Selection
Feature selection helps identify the important features
to reduce the computation complexity and overﬁting of the
comparator. To verify the impact of feature selection on
the comparator performance, we apply minimum redundancy
maximum relevance feature selection algorithm (mRMR) [9]
to select highly related features while minimizing the inter-
dependence between selected features. At ﬁrst, the most con-
tributing feature is selected. Then in each round, another
feature that enhances the model the most is added to the
feature subset. Each time, the authentication is executed on the
selected feature set and the corresponding information gain is
calculated.
Information Gain. Figure 13 reports the information gain
of each feature included for Illusion by mRMR feature selec-
tion algorithm. The X axis lists feature indexes as deﬁned in
Table I. These features are ranked from the most important on
the left to the least important on the right. We observe that
the top 5 features are Metabolism Intensity (V), Sympathetic
Energy (H), Extent of Up Rotation Distance, Extent of Left
Rotation Distance, Sympathetic Energy (V), and Extent of
Down Rotation Distance while the best behavioral feature is
ranked the 8th. This clearly shows the importance of low-level
HVS biostructure in identifying users compared to traditional
eye gaze.