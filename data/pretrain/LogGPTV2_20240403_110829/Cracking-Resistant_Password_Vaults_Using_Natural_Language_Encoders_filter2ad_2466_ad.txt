Encryption with HE takes as input a master password
mpw and plaintext M and outputs a ciphertext, which we
denote by C = HEnc(mpw, M ). Encryption is usually
randomized. Decryption takes as input a master password
and ciphertext C, and outputs a plaintext, denoted M =
HDec(mpw, C). In our setting, a plaintext
is a single
password P or vector of passwords (cid:2)P . We require that
HDec(mpw, HEnc(mpw, M )) = M with probability one
for all mpw, M (over any randomness used in encryption).
HE schemes are designed so that a ciphertext, when
(cid:4)=
decrypted under an incorrect master password mpw(cid:2)
mpw, emits a “plausible” plaintext M(cid:2). This requires a
good model, built into the HE scheme speciﬁcation, of the
distribution from which plaintexts are drawn.
As a concrete example Juels and Ristenpart gave an
HE scheme for a message M that is a uniformly sam-
pled prime number. (This scheme can be leveraged to
encrypted RSA private keys.) Their construction follows a
general approach to building HE schemes that composes
a distribution-transforming encoder (DTE) with a carefully
chosen, but still conventional, PBE scheme. The latter can
be, for example, CTR-mode encryption with key derived
from the master password using a PBKDF. A DTE scheme
speciﬁes a randomized encoding of a message as a string
of bits. Decoding does the reverse (deterministically). For
a secure HE scheme, it is a requirement that the output of
the encoding, for M drawn from some target distribution,
looks uniformly distributed and that decoding a uniform
bit string give rises to a distribution for M similar to the
target distribution. For prime numbers, Juels and Ristenpart
give a secure DTE that essentially converts a sampling
algorithm for uniformly distributed prime numbers into a
DTE encoding and decoding algorithm pair.
Given DTEs suitable for password vaults, an HE-based
approach has several beneﬁts over the hide-in-an-explicit-list
approach of Kamouﬂage. First, regardless of the quality of
the DTE, because there is only one ciphertext, the amount of
ofﬂine cracking effort required by an attacker is never less
than for a conventional PBE-only ciphertext. This means
that, unlike Kamouﬂage, an HE-based scheme will never
provide attackers with a speed-up in ofﬂine work. Addition-
ally, the size of ciphertexts in HE does not depend on the
number of decoys possible, rather decoys are generated “on-
the-ﬂy” during a brute-force attack for each guessed master
password. Therefore, given a good DTE, the online work
required by an adversary is essentially the strength (guessing
entropy) of mpw. A strong mpw will mean the attacker must
make many online queries.
We would like a cracking-resistant vault to support use of
both computer-generated and human-chosen passwords. The
former is relatively straightforward, as computer-generated
passwords come from an easy-to-characterize distribution.
Human-chosen passwords present a signiﬁcant challenge,
however, as they raise the question of whether one can
build DTEs that accurately model natural
language-type
distributions. We show in the next section that such modeling
is possible, and handle further challenges of building a full-
ﬂedged, encrypted vault management service in the sections
that follow.
it
As an example,
V. NATURAL LANGUAGE ENCODERS FOR PASSWORDS
Formally, a DTE is a pair of algorithms DTE =
(encode, decode), where encode is randomized and
decode is deterministic. In our context, encode takes as in-
put a vector of passwords (cid:2)P and outputs a bit string of some
length s. The deterministic decoder decode takes as input
an s-bit string and outputs a password vector. We require that
(cid:2)P )) =
the DTE be correct, meaning that decode(encode(
(cid:2)P with probability 1 over the coins used by encode. Our
DTEs will be designed so that the length s of outputs of
encode depends only on (cid:4), the number of passwords in (cid:2)P .
is simple to construct a DTE for
uniformly random, ﬁxed-length strings of symbols drawn
from an alphabet Σ that consists of the 96-character ASCII
printable characters. Encoding works in a symbol-by-symbol
manner on an input string σ1 (cid:5) σ2 (cid:5) . . . (cid:5) σk, where σi ∈ Σ.
Let σi denote the position of σi in Σ under some canonical
ordering of Σ . Then for each symbol σi in turn, encode
outputs a large (e.g., 128-bit) integer Xi selected randomly
subject to the constraint Xi mod 96 = σi. (See the full
version of the paper for details on security bounds and other
considerations.) Decoding operates in the natural way: Given
input X1 (cid:5) X2 (cid:5) . . . (cid:5) Xk, it yields output σ1 (cid:5) σ2 (cid:5) . . . (cid:5) σk
such that σi = Xi mod 96. Straightforward extensions that
we omit for brevity allow construction of a DTE over
passwords that conform to standard password-composition
488488
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:07:53 UTC from IEEE Xplore.  Restrictions apply. 
policies (such as needing at least one integer, one special
symbol, etc.). We refer to this DTE as UNIF.
We will use such a simple uniform DTE for computer-
generated passwords later. But it provides poor security for
human-selected passwords, which are clearly not distributed
uniformly. This observation brings us to one of our core
tasks: building DTEs that securely encode samples from
distributions of natural language-type text. Because we feel
that such DTEs will be of broad use, we give them a
special name: natural
language encoders, or NLEs. We
focus on DTEs for messages consisting of a single password
and, later, lists of passwords. We note, however, that our
constructions are quite general and may be applicable in
other natural language contexts.
NLEs from password models. A password probability
model (or simply password model) as deﬁned by [30] is
∗ → [0, 1] that assigns a probability to
a function p : Σ
P p(P ) = 1.
every password. A clear requirement is that
A complete password model is one that assigns a non-zero
(but possibly tiny) probability to every possible password
(up to some maximum length).
(cid:2)
Given such a password model, a ﬁrst, na¨ıve attempt at
building an NLE would be to use the inverse sampling DTE
from [23]. Consider the cumulative distribution function
(CDF) associated with p, which we denote by Fp, and an
associated ordering P1, P2, . . . over all passwords such that
Fp(Pi) > Fp(Pj) for i > j. Deﬁne Fp(P0) = 0. Then to
encode a password P = Pi, choose a random value in the
range [Fp(Pi−1), Fp(Pi)). To decode a value S ∈ [0, 1),
simply ﬁnd the smallest i such that Fp(Pi) > S. Of course,
implementation of such a scheme requires encodings to as-
sume the form of bit strings, not fractions. Such encoding is
possible using a suitably granular ﬁxed-point representation
of ﬂoats as detailed in the full version.
The resulting NLE, however, requires storing the CDF
and associated enumeration, as well as a look-up table for
decoding. This will be inefﬁcient for all but the smallest
password models.
NLEs from password samplers. We might instead turn to
password sampling algorithms. Early crackers, such as John
the Ripper, simply have stored dictionary lists of popular
passwords, and can produce samples in order of likelihood.
More modern crackers instead learn compact representations
of password distributions from password leaks [35], [37],
and permit efﬁcient sampling of passwords over the distri-
bution model.
In a bit more detail, we can view a sampling model for
passwords as a deterministic algorithm Samp that takes as
input a uniformly random bit string U of sufﬁcient length
(often called the “coins”) and produces a password P . We
can characterize Samp in terms of a distribution p, meaning
that a password P is output by Samp with probability p(P )
(over the selection of bit string U). The goal of a password
cracker is to learn an algorithm Samp from one or more
password leaks whose corresponding distribution p closely
approximates that of human-generated passwords seen in
practice.
We might hope, a priori, that one can build a secure
DTE from any sampling algorithm Samp. Unfortunately
this seems unlikely to work, in the sense that there exists
(admittedly artiﬁcial) Samp for which building a DTE
appears intractable. Brieﬂy, let Samp(U ) = H(U ) for some
cryptographic hash function H, where U is a random bit
string. Then the natural approach for DTE construction is to
set decode(U ) = H(U ). But for such decode, correctness
would mandate that encode(P ) somehow can sample from
the set H−1(U ) of preimages of U, which would contradict
the hash function’s security. Of course there may be other
ways to build encode, decode that use Samp only as a
black-box, and yet achieve correctness and security. We
conjecture that a full counter-example exists, but do not have
proof.
Such artiﬁcial counter-examples aside, for various classes
of Samp we can in fact use the straightforward approach
of having decode(U ) = Samp(U ). The only requirement
is that we can build encode(P ) that samples uniformly
from Samp−1
(P ). We show how to do so below for a
couple of useful classes of samplers: n-gram models and
probabilistic context-free grammar (PCFG) models. We start
with the single password case for the different models, and
then discuss extensions to the (trickier) case of a vault of
possibly related passwords.
NLEs from n-gram models. So-called n-gram models
are used widely in natural
[11],
[14], [30]. For our purposes, an n-gram is a sequence
of
For
the 4-grams of the word ‘password12’ are
example,
{pass, assw, sswo, swor, word, ord1, rd12}.
In building models,
to add two special
characters to every string: ˆ to the beginning and $ to the
end. Given this enhancement, the 4-gram set in the example
above would also include ˆpas and d12$.
An n-gram model is a Markov chain of order n − 1. The
Pr [ w1w2 ··· wk ] ≈ k(cid:3)
probability of a string is estimated by
wi|wi−(n−1) ··· wi−1
language domains
is convenient
characters
contained
longer
string.
in
a
it
(cid:4)
Pr
(cid:5)
i=1
where the individual probabilities in the product are calcu-
lated for a given model empirically via some text corpus.
For example, for the RockYou password leak, we let c(·)
denote the number of occurrences of a substring in the leak.
The empirical probability is then
Pr [ wi|w1w2 ··· wi−1 ] =
c(w1w2 ··· wi)
x c(w1 ··· wi−1x)
for any string w1 ··· wi of any length i. Let Fwi−(n−1)···wi−1
denote the CDF associated the probability distribution for
(cid:2)
489489
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:07:53 UTC from IEEE Xplore.  Restrictions apply. 
each history. Then the Markov chain associated to such
an n-gram model is a directed graph with nodes labeled
by n-grams. An edge from node wi−(n−1) ··· wi−1 to
wi−(n−2) ··· wi is labeled with wi and Fwi−(n−1)···wi−1 (wi).
To sample from the model one starts at node ˆ, samples from
[0, 1), ﬁnds the ﬁrst edge5 whose CDF value is larger than
the sample, follows it to move to the next node, and repeats.
The process ﬁnishes at a node having the stop symbol. The
sequence of wi values seen on the edges is the resulting
string.
Note that such a Markov chain may not have edges and
nodes sufﬁcient to cover all possible strings. For use in
encoding, then, we extend the Markov chain to ensure that
each node has an edge labeled with each character. We set
the probabilities for these edges to be negligibly small, and
re-normalize the other edge weights appropriately. If this
implies a new node we add it as well, and have its output
edges all have equal probability.
We can build a DTE by encoding strings as their path in
the Markov chain. To encode a string p = w1 ··· wk, process
each wi in turn by choosing randomly from the values in
[0, 1) that would end up picking the edge labeled with wi+1.
Decoding simply uses the input as the random choices in a
walk. Both encoding and decoding are fast, namely O(n)
for a password of length n.
In our experiments reported on later, we use a 4-gram
model trained from RY-tr. We denote the resulting DTE
by NG. We also explored 5-gram models, but in our ex-
periments these used up more space without a signiﬁcant
improvement in security.
NLEs from PCFG models. A PCFG is a ﬁve-tuple G =
(N, Σ, R, S, p) where N is a set of non-terminals, Σ is a
set of terminals, R is set of relations N → {N ∪ Σ},
S ∈ N is the start symbol, and p is a function of the form
R → [0, 1] denoting the probability of applying a given rule.
We require that for any non-terminal X ∈ N, it holds that
(cid:2)
β∈N∪Σ p(X → β) = 1. PCFGs are a compact way of
representing a distribution of strings in a language. Each
derivation for a member of the language deﬁned by the
underlying CFG has a probability associated to it.
Weir, Aggarwal, de Medeiros, and Glodek [37] were the
ﬁrst to apply PCFGs to the task of modeling password
distributions. They constructed a password cracker that could
enumerate passwords (in approximate order of descending
probability) in a way that ensured faster cracking com-
pared to previous approaches like John the Ripper. Weir
et al. parsed passwords into sets of contiguous sequences
of letters, digits or symbols. Further improvements are pos-
sible by employing their approach with alternative parsing
schemes. Jakobsson and Dhiman [21] and later Veras et
al. [35] used a (so-called) maximum coverage approach for
5We again assume an ordering on edges for which CDF values are strictly
increasing.
parsing passwords with the help of external language speciﬁc
dictionaries. Veras et al. also used the semantic meaning of
passwords to provide ﬁner granularity parsing, and improved
PCFG cracking performance.
We now show how to build a DTE for a single password
from any PCFG model. Intuitively, the encoding of a pass-
word will be a sequence of probabilities deﬁning a parse tree
that is uniformly selected from all such giving rise to the
same password. Decoding will just emit the string indicated
by the encoded parse tree. We ﬁrst ﬁx some deﬁnitions. A
rule l → r can be speciﬁed as a pair (l, r), where l is a non-
terminal and r is a terminal or non-terminal. Every edge in a
parse tree is a rule. A rule set is a lexicographically ordered
set of rules with the same left-hand-side. A rule list is an
ordered list of rules generated by depth-ﬁrst search of the
parse-tree of a string / password (with siblings taken in left
to right order).
A CFG is completely speciﬁed as a set of rule sets.
A PCFG is completely speciﬁed by what we call here an
admissible assignment of probabilities to CFG rules. Let S
be a rule set of size |S| and pS (l → r) be the probability
assigned to a rule l → r in S. An admissible assignment of
pS (l → r) = 1.
probabilities has the property that
We refer to the probability distribution over rules in a rule
set S for a given admissible assignment as its induced
probability distribution.
(l→r)∈S
(cid:2)
As a technical modiﬁcation to such a PCFG, we add a
special catch-all rule. Its left-hand side is the start symbol
and its right hand side represents any string. We assign this
catch-all rule a very low probability (and normalize other
probabilities accordingly). This rule ensures all passwords
can be parsed (and generated) by the PCFG model and that
the model will never fail to encode any real password.
For a given PCFG, a parse tree, and thus a string str, may
be speciﬁed as a sequence of probabilities p1, . . . , pk (for
sufﬁciently large k). To construct this parse tree, a rule is