title:Measurement Artifacts in NetFlow Data
author:Rick Hofstede and
Idilio Drago and
Anna Sperotto and
Ramin Sadre and
Aiko Pras
Measurement Artifacts in NetFlow Data
Rick Hofstede, Idilio Drago, Anna Sperotto, Ramin Sadre, and Aiko Pras
University of Twente
Centre for Telematics and Information Technology
Design and Analysis of Communications Systems (DACS)
{r.j.hofstede,i.drago,a.sperotto,r.sadre,a.pras}@utwente.nl
Enschede, The Netherlands
Abstract. Flows provide an aggregated view of network traﬃc by group-
ing streams of packets. The resulting scalability gain usually excuses the
coarser data granularity, as long as the ﬂow data reﬂects the actual net-
work traﬃc faithfully. However, it is known that the ﬂow export process
may introduce artifacts in the exported data. This paper extends the
set of known artifacts by explaining which implementation decisions are
causing them. In addition, we verify the artifacts’ presence in data from
a set of widely-used devices. Our results show that the revealed artifacts
are widely spread among diﬀerent devices from various vendors. We be-
lieve that these results provide researchers and operators with important
insights for developing robust analysis applications.1
Keywords: Network management, measurements, NetFlow, artifacts.
1
Introduction
Cisco’s NetFlow [2] and the recent standardization eﬀort IPFIX [11] have made
ﬂow export technologies widely popular for network monitoring. They owe this
success to their applicability to high-speed networks and widespread integration
into network devices. The pervasiveness of these technologies has resulted in a
variety of new application areas that go far beyond simple network monitoring,
such as ﬂow-based intrusion detection [13] and traﬃc engineering [4]. Regardless
of the application, ﬂow data is expected to reﬂect the network traﬃc faithfully.
Flow export is a complex process that includes both real-time aggregation
of packets into ﬂows and periodic export of ﬂow information to collectors. This
aggregation naturally results in a coarser view on the network traﬃc. Several
works have already compared the precision of ﬂow-based applications to their
packet-based counterparts [4, 12]. The scalability gain of using ﬂow data nor-
mally excuses the loss of precision. Any ﬂow-based application will, however, be
impaired by ﬂow data of poor quality, which can be caused by implementation
decisions. For example, the imprecision in ﬂow timestamps has already been
discussed in [9, 14]. Similarly, artifacts found in ﬂow data from Juniper devices
1 All measurement scripts used for our analysis are made public at
http://www.simpleweb.org/wiki/NetFlow_Data_Artifacts
M. Roughan and R. Chang (Eds.) PAM 2013, LNCS 7799, pp. 1–10, 2013.
c(cid:2) Springer-Verlag Berlin Heidelberg 2013
2
R. Hofstede et al.
are extensively analyzed in [3]. However, these works do not investigate how
widespread these artifacts are in ﬂow data from diﬀerent ﬂow export devices.
The goal of the paper is twofold. Firstly, we report on our experience acquired
while operating a Cisco Catalyst 6500, which is one of the most widely deployed
switching platforms [7]. We provide an analysis of artifacts identiﬁed in ﬂow
data exported by this device, along with a detailed description of their causes.
Secondly, inspired by [3], we analyze whether these artifacts are also present
in ﬂow data from other devices. Active experiments and ﬂow data analysis are
combined to evaluate the quality of six diﬀerent ﬂow exporters.
This paper is organized as follows. In Sect. 2 we analyze and explain the arti-
facts observed on a Cisco Catalyst 6500. After that, we investigate whether the
revealed artifacts are also present in ﬂow data from other devices. The experi-
ment setup is presented in Sect. 3. The results of our analyses are discussed in
Sect. 4. Finally, our conclusions are presented in Sect. 5.
2 Case Study: Cisco Catalyst 6500 (SUP720-3B)
The Cisco Catalyst 6500 is a widely deployed series of switches that can be found
in many service provider, enterprise and campus networks. In this section, we
discuss ﬁve artifacts that are present in ﬂow data from a speciﬁc device of this
series2, located in our production network. This knowledge is, therefore, gained
from our operational experience. It is important to note that this list is by
no means comprehensive, since artifacts are load- and conﬁguration-dependent.
Moreover, artifacts related to clock imprecisions discussed by previous works,
which we have observed as well, are not covered.
Imprecise Flow Record Expiration – Expiration is the process of removing
ﬂow records from the NetFlow table (i.e., ﬂow cache). This can be done for a
variety of reasons, such as timeouts and exporter overload. However, according
to the documentation, ﬂow records can be expired as much as 4 seconds ear-
lier or later than the conﬁgured timeout [1] when the device is not overloaded.
Moreover, the average expiration deviation should be within 2 seconds of the
conﬁgured value. This is because of the way in which the expiration process
is implemented: A software process scans the NetFlow table for expired ﬂow
records. Due to the time needed for scanning all ﬂow records, expiration is often
pre- or postponed.
TCP Flows Without Flag Information – TCP ﬂags are accounted for few
TCP ﬂows, since they are solely exported for software-switched ﬂows [1]. These
ﬂows are processed by a generic CPU, while hardware-switched ﬂows are pro-
cessed using Application Speciﬁc Integrated Circuits (ASICs). Whether a ﬂow
has been switched in hardware or software can be concluded from the engineID
ﬁeld in the ﬂow records. Since most packets are hardware-switched, only few
TCP ﬂows with ﬂags can be found in the exported data. Another observation
can be made regarding the handling of ﬂags of hardware-switched TCP ﬂows:
2 The exact conﬁguration can be found in Table 1 (Exporter 1).
Measurement Artifacts in NetFlow Data
3
In contrast to what is speciﬁed in [1], TCP FIN and RST ﬂags trigger the ex-
piration of ﬂow records. As such, TCP ﬂags are considered in the expiration
process, even though they are not exported.
Invalid Byte Counters – It has been observed before that byte counters in ﬂow
records are not always correct [9]. The counters represent the number of bytes
associated with an IP ﬂow [2], which is the sum of IP packet header and payload
sizes. IP packets are usually transported as Ethernet payload, which should have
a minimum size of 46 bytes according to IEEE 802.3-2008. If the payload of an
Ethernet frame is less than 46 bytes, padding bytes must be added to ﬁll up the
frame. However, stripping these padding bytes is not done for hardware-switched
ﬂows, resulting in too many reported bytes.
Non-TCP Flow Records With TCP ACK Flag Set – The ﬁrst packet of a
new ﬂow is subject to Access Control List (ACL) checks, while subsequent pack-
ets bypass them for the sake of speed. Bypassing ACL checks could also be done
by fragmenting packets, since packet fragments are not evaluated. To overcome
this security problem, Cisco has implemented a poorly documented solution that
has two implications on software-switched ﬂows. Firstly, ﬂag information in ﬂow
records is set to zero for all packet fragments, which are always software-switched.
Secondly, ﬂag information in ﬂow records of all other software-switched traﬃc is
set to a non-zero value, and TCP ACK was chosen for that purpose.
Gaps – Similarly to the devices analyzed in [3], this exporter often measures
no ﬂows during short time intervals. This is caused mostly by hardware limita-
tions, combined with a conﬁguration that is not well adjusted to the load of the
network. When a packet has to be matched to a ﬂow record, its key ﬁelds are
hashed and a lookup is done in a lookup table (NetFlow TCAM ). In our setup,
both the lookup table and the table storing the ﬂow records (NetFlow table)
consist of 128k entries with a hash eﬃciency of 90%, resulting in a net utiliza-
tion of roughly 115k entries. A table (named alias CAM or ICAM ) with only
128 entries is available to handle hash collisions, so that up to two ﬂows with
diﬀerent keys but identical hashes can be stored. The event in which a packet
belonging to a new ﬂow cannot be accommodated because of hash collisions is
called ﬂow learn failure. The evolution of ﬂow learn failures in this device can
be monitored using the CISCO-SWITCH-ENGINE-MIB (SNMP).
3 Experiment Setup
To understand whether the artifacts presented in the previous section can also
be identiﬁed in ﬂow data from other ﬂow exporters, several devices from three
vendors, installed in campus and backbone networks throughout Europe, have
been analyzed. Table 1 lists these devices, together with their hardware conﬁg-
uration and software versions. Given the variety of hardware conﬁgurations, we
cover a wide range of hardware revisions of widely used devices.
4
R. Hofstede et al.
Table 1. Assessed ﬂow exporters and their conﬁgurations
No.
1.
2.
3.
4.
5.
6.
Model
Software version
Cisco Catalyst 6500 WS-SUP720-3B (PFC3B, MSFC3) IOS 12.2(33)SXI5
Cisco Catalyst 6500 WS-SUP720-3B (PFC3B, MSFC3) IOS 12.2(33)SXI2a
Modules
Cisco Catalyst 6500
VS-SUP2T-10G-XL (PFC4XL,
MSFC5) + WS-X6904-40G
IOS 15.0(1)SY1
Cisco Catalyst 7600
RSP720-3C-GE (PFC3C, MSFC4)
IOS 15.2(1)S
Juniper T1600
MultiServices PIC 500
JUNOS 10.4R8.5
INVEA-TECH FlowMon
-
3.01.02
The ﬁrst two devices, both from the Cisco Catalyst 6500 series, have identical
hardware conﬁgurations and similar software versions, but are exposed to dif-
ferent traﬃc loads. We can therefore analyze whether the load of these devices
aﬀects the presence of artifacts. The third Cisco Catalyst 6500 has a signiﬁcantly
diﬀerent hardware conﬁguration and software version. The Cisco Catalyst 7600
series, represented by our fourth device, is generally similar to the Cisco Catalyst
6500 series, but uses diﬀerent hardware modules. Device 1, 2 and 4 use the same
hardware implementation of NetFlow (EARL7), while Device 3 is signiﬁcantly
newer (released in 2012) and uses Cisco’s EARL8 ASIC. The ﬁfth analyzed de-
vice is a Juniper T1600, which has also been analyzed in [3]. The inclusion of
this device allows us to extend the results in [3]. Finally, we have included a
dedicated ﬂow exporter (probe) from INVEA-TECH. In the remainder of this
paper, we denote each of the devices by its number in the table.
4 Artifact Analysis
Sect. 2 described a set of artifacts present in ﬂow data from a Cisco Catalyst 6500
(Exporter 1). This section evaluates whether these artifacts are also present in
ﬂow data from the other exporters listed in Sect. 3. For each artifact, we deﬁne
the experiment methodology, followed by a description of our observations in
both ﬂow and SNMP data. After that, we show some examples in which the
artifacts have impact on speciﬁc analysis applications. Also, we discuss whether
the artifacts are repairable or non-repairable.
Imprecise Flow Record Expiration – Flow exporters are expected to expire
ﬂow records at the conﬁgured active timeout Tactive and idle timeout Tidle, and
possibly after a packet with TCP FIN or RST ﬂag set has been observed. We
perform the following experiments to evaluate the behavior of the ﬂow exporters:
– Active Timeout: We send a series of packets with identical ﬂow key to
the ﬂow exporter for a period of Tactive + d. The inter-arrival time of the
packets is chosen to be less than Tidle. The experiment is performed for
d = −2,−1, . . . , 16 seconds. For each value of d, we repeat the experiment
100 times and count how often the ﬂow exporter generates two ﬂow records
Measurement Artifacts in NetFlow Data
5
from the received packets. Ideally, one should see only one ﬂow record per
experiment for d < 0 and always two ﬂow records per experiment for d ≥ 0.
– Inactive Timeout: We send two packets with identical ﬂow key to the ex-
porter, separated by a time diﬀerence of Tidle + d. The rest of the experiment
is performed as for the active timeout. Again, one ideally sees only one ﬂow
record per experiment for d < 0 and always two ﬂow records for d ≥ 0.
– TCP FIN/RST Flag: We send one packet with the FIN or RST ﬂag set,
followed by another packet after d time units. The rest of the experiment is
performed as for the active timeout (only for d = 0, 1, . . . , 16). Ideally, the
exporter always generates two ﬂow records.
For all experiments, the packets are generated such that they are processed
in hardware by the exporter, if applicable3. In addition, several initial packets
are generated where necessary, to avoid that special mechanisms for the early
expiration of records of small and short ﬂows (such as Cisco’s fast aging [1])
are applied. All exporters use an active timeout between 120 and 128 seconds,
and an idle timeout between 30 and 32 seconds. Note that we do not rely on
the timestamps in ﬂow records, which means that we are not susceptible to the
errors described in [14]. Instead, we use the time from the machines running the