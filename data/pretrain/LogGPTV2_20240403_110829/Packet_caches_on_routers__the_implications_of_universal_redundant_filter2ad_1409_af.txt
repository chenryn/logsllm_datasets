### Selection for Routing Inter-Domain Traffic

When using the Opt (Optimal) method, a network computes exit points for inter-domain traffic destined to its neighbor in a cooperative and globally optimal manner. In SP-Opt-RE (Shortest Path with Optimal Redundancy Elimination), the cooperative routes minimize the total latency across all network paths, both inter and intra-domain. In RA-Opt (Redundancy-Aware Optimal), the cooperative routes minimize the network footprint across both networks. RA-Opt is the algorithm described in Section 4.2.

In early-exit or hot potato routing (HP), each network optimizes its own local objective. In SP-HP-RE (Shortest Path with Hot Potato Redundancy Elimination), each network uses early exit routing into the other network. In RA-HP (Redundancy-Aware Hot Potato), each network selects peering locations that minimize its own network footprint. The baseline for comparison is SP-HP. Our metric of comparison is the network footprint computed over both ISP networks.

### Comparison of Routing Methods

Comparing RA-Opt with SP, we observe a significant reduction in network footprint, ranging from 28% to 50%. However, SP-Opt-RE does not show much improvement over SP-HP-RE. This is because early-exit paths originating from the Chicago PoP in AT&T already have near-optimal latencies to the PoPs in SprintLink. More importantly, SP-Opt-RE is inferior compared to RA-HP, underscoring the importance of redundancy-aware route computation in reducing network-wide utilization.

Figure 13(b) shows the distribution of network footprint reductions when different AT&T PoPs are chosen as the sources of inter-domain traffic. As with our prior analyses, redundancy elimination is generally beneficial, but redundancy awareness offers greater overall improvement.

### Evaluation Summary

Our extensive study has shown the substantial benefits of network-wide support for redundancy elimination, particularly by making network routing redundancy-aware. We found that the impact of traffic on ISP network resources can be significantly reduced, which is especially useful for controlling link loads during sudden overloads. Using routes computed based on stale profiles does not undermine the benefits of our approaches. While the initial cost of deploying redundancy elimination mechanisms on multiple network routers is high, the long-term benefits justify the investment.

We assumed throughout that each router carries fully decoded packets internally. However, our proposals can be extended to switch smaller encoded packets, possibly combining multiple related small packets into a larger one, with decoding/reconstruction occurring only where necessary. This can help overcome technology bottlenecks inside routers and save bandwidth on links.

### Implementation Challenges

In this section, we examine key challenges in deploying redundancy elimination mechanisms on fast routers and offer preliminary solutions. We evaluate the trade-offs introduced by our solutions via a software implementation based on the Click modular router [18]. Our implementation extends the base algorithm of Spring et al. [24].

#### Memory Accesses

A significant bottleneck in performing redundancy elimination at high speeds is the number of memory accesses required during various stages, such as on-the-fly lookup, insertion, deletion, and encoding the redundant region in a packet.

#### Memory Management

Another challenge is managing the amount of memory required to store key data structures, such as the fingerprint and packet stores. Our implementation focuses on developing memory-efficient ways to organize and access these data structures, an issue not thoroughly addressed in prior work.

#### Hash Function Computation

The computation of the hash function to obtain fingerprints for each packet is another key component. Rabin fingerprints used in [24] are well-suited for high-speed implementation because they rely on sliding hashes, allowing parallel computation with CRC checks as bytes arrive at the router.

### Packet Store

The layout of the packet store in our implementation is shown in Figure 14(a). We use a FIFO buffer, specifically a circular buffer with a maximum of T fixed-size entries. The oldest packet is evicted when there is no room for a new one. Preliminary studies showed that FIFO offers nearly the same performance as other policies (e.g., Least-Recently-Used) but is simpler to implement [14].

We use a global variable called "MaxPktID" (4B) to aid packet insertions and deletions. This is incremented before inserting a new packet, and the current value is assigned to a variable PktID, which becomes a unique identifier for the packet. The packet is stored at the location PktID % T in the store, indicating the starting memory address of the packet's location.

### Fingerprint Store

The fingerprint store holds metadata for representative fingerprints, including the fingerprint itself, the unique ID for the packet (PktID), and the byte offset in the packet where the region represented by the fingerprint starts.

When the packet store is full, we overwrite the new packet at the tail of the circular store and invalidate the associated fingerprints. To do this efficiently, we leverage the MaxPktID variable and the PktID stored in the fingerprint metadata.

### CuckooHash-Based Design

To improve hash table storage efficiency while ensuring O(1) lookups and inserts, we use CuckooHash [10] to design the fingerprint store. Each hash entry is divided into B buckets, and a set of k ≤ 2 independent hash functions are used during insertion. If any of the k × B locations are empty, the fingerprint is inserted; otherwise, the insertion fails.

Table 1 explores the trade-offs between hash table size, the number of buckets B, and the number of hash functions k. Using multiple hash buckets offers better performance, and with k = 1 and f = 2, we fail to insert just 2% of the fingerprints. When k = 2, the probability of failure is essentially zero, though it incurs twice as many memory accesses during lookups.

### Encoding

We use the same approach as Spring et al. [24] to encode duplicated chunks: for each duplicated byte string, we remove the matched region from the incoming packet and replace it with a "shim layer" containing the matching packet’s PktID, starting byte positions, and match length. Multiple matches result in multiple shims, ensuring non-overlapping regions.

### Benchmarking Results

We implemented packet-level redundancy elimination using the aforementioned data structures in the Click modular router [18]. Our current implementation runs on a 1.8GHz AMD Opteron processor with 8GB of RAM (64-bit Linux version 2.6.9), configured to use 400MB of memory for the packet store, resulting in a 200MB fingerprint store with 16 fingerprints per packet.

We evaluated the throughput performance using real packet traces, achieving an average throughput of 1.05Gbps. Profiling Click’s processing overhead, we achieved a throughput of 1.17Gbps. Precomputing fingerprints to avoid hash computation, we observed a throughput of 1.4Gbps with 16 fingerprints per packet. With faster DRAMs, we could reach OC-48 speeds in software.

### Conclusions

We explored the implications of deploying packet-level redundant content elimination as a primitive service on all routers. Using real packet traces and synthetic workloads, we showed that applying redundancy elimination on network links can reduce resource utilization by 10-50% in ISP networks. Modifying network protocols, particularly routing protocols, to leverage link-level redundancy elimination further enhances network-wide benefits. We developed a software prototype of a high-speed packet-level redundancy elimination mechanism that can run at OC48 speeds, with hardware implementations likely to achieve even higher speeds.

While the initial deployment cost is high, the long-term benefits of our approaches offer significant incentives for networks to adopt them. Our techniques can also be applied to limited-scale partial deployments across specific network links.

### Acknowledgments

We thank Fred Baker, Paul Barford, Mike Blodgett, Perry Brunelli, Paul Francis, Bruce Davie, Randy Katz, George Varghese, Jia Wang, and Ty Znati for their advice. We also thank the anonymous Sigcomm reviewers for their comments. This work was supported in part by NSF grants CNS-0746531, CNS-0626889, and CNS-0435382.

### References

[1] Netequalizer Bandwidth Shaper. http://www.netequalizer.com/.
[2] Packeteer WAN optimization solutions. http://www.packeteer.com/.
[3] Peribit WAN Optimization. http://www.juniper.net/.
[4] Riverbed Networks. http://www.riverbed.com.
[5] A. Anand, A. Gupta, A. Akella, S. Seshan, and S. Shenker. Packet Caches on Routers: The Implications of Universal Redundant Traffic Elimination (Extended Version). Technical Report 1636, UW-Madison, June 2008.
[6] B. Fortz and M. Thorup. Internet Traffic Engineering by Optimizing OSPF Weights. In Infocom, 2000.
[7] T. Ballardie, P. Francis, and J. Crowcroft. Core based trees (CBT). SIGCOMM Comput. Commun. Rev., 23(4):85–95, 1993.
[8] M. Caesar, D. Caldwell, N. Feamster, J. Rexford, A. Shaikh, and J. van der Merwe. Design and implementation of RCP. In NSDI, 2005.
[9] B. Davie and Y. Rekhter. MPLS: technology and applications. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 2000.
[10] U. Erlingsson, M. Manasse, and F. McSherry. A cool and practical alternative to traditional hash tables. In WDAS, 2006.
[11] L. Fan, P. Cao, J. Almeida, and A. Z. Broder. Summary cache: a scalable wide-area Web cache sharing protocol. In ACM SIGCOMM, 1998.
[12] B. Fortz, J. Rexford, and M. Thorup. Traffic engineering with traditional IP routing protocols. In Infocom, 2002.
[13] A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers, J. Rexford, G. Xie, H. Yan, J. Zhan, and H. Zhang. A clean slate 4D approach to network control and management. SIGCOMM Comput. Commun. Rev., 35(5):41–54, 2005.
[14] A. Gupta, A. Akella, S. Seshan, S. Shenker, and J. Wang. Understanding and Exploiting Network Traffic Redundancy. Technical Report 1592, UW-Madison, April 2007.
[15] S. Kandula, D. Katabi, B. Davie, and A. Charny. Walking the tightrope: responsive yet stable traffic engineering. In ACM SIGCOMM, 2005.
[16] U. Manber. Finding similar files in a large file system. In USENIX Winter Technical Conference, 1994.
[17] A. Medina, N. Taft, K. Salamatian, S. Bhattacharyya, and C. Diot. Traffic matrix estimation: existing techniques and new directions. In ACM SIGCOMM, 2002.
[18] R. Morris, E. Kohler, J. Jannotti, and M. F. Kaashoek. The Click modular router. SIGOPS Oper. Syst. Rev., 33(5):217–231, 1999.
[19] A. Muthitacharoen, B. Chen, and D. Mazières. A low-bandwidth network file system. SIGOPS Oper. Syst. Rev., 35(5), 2001.
[20] M. Rabin. Fingerprinting by Random Polynomials. Technical report, Harvard University, 1981. Technical Report, TR-15-81.
[21] M. Roughan, M. Thorup, and Y. Zhang. Performance of estimated traffic matrices in traffic engineering. In ACM SIGMETRICS, 2003.
[22] S. Singh, C. Estan, G. Varghese, and S. Savage. Automated worm fingerprinting. In OSDI, 2004.
[23] N. Spring, R. Mahajan, and D. Wetherall. Measuring ISP Topologies with Rocketfuel. In ACM SIGCOMM, 2002.
[24] N. Spring and D. Wetherall. A protocol-independent technique for eliminating redundant network traffic. In ACM SIGCOMM, 2000.
[25] A. Wolman et al. On the scale and performance of cooperative Web proxy caching. In ACM Symposium on Operating Systems Principles, 1999.