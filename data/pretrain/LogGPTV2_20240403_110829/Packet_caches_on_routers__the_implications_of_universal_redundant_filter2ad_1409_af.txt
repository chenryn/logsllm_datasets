selection for routing inter-domain trafﬁc into its neighbor. When
using Opt, a network computes exit points for the inter-domain traf-
ﬁc destined to its neighbor in a cooperative, globally optimal way.
In SP-Opt-RE, the cooperative routes minimize the sum total la-
(a) Packet and FP stores
(b) Fingerprint Hash table
Figure 14: Architecture of the ﬁngerprint and packet stores.
tency of all network paths (inter and intra-domain). In RA-Opt the
cooperative routes minimize the network footprint across both net-
works; RA-Opt is exactly the algorithm we descried in Section 4.2.
In early-exit or hot potato routing (HP), each network tries to opti-
mize its own local objective. In SP-HP-RE, each network uses early
exit routing into the other network. In RA-HP, each network selects
peering locations which minimize its own network footprint. The
baseline for comparison is SP-HP. Our metric of comparison is the
network footprint computed over both ISP networks.
Comparing RA-Opt with SP, we see that the reduction in network
footprint is very impressive, ranging between 28% and 50%. Also,
we note that SP-Opt-RE is not much better than SP-HP-RE. This is
because early-exit paths origination from the Chicago PoP in ATT
already have close-to-optimal latencies to the PoPs in SprintLink.
More importantly, we observe that SP-Opt-RE is inferior compared
to RA-HP. This further underscores the importance of redundancy-
aware route computation in reducing the network-wide utilization.
In Figure 13(b), we show a distribution of the reduction in net-
work footprints when different ATT PoPs are chosen as the sources
of the inter-domain trafﬁc. As with our prior analyses, we see that
redundancy elimination in general is vastly beneﬁcial, but redun-
dancy awareness offers greater overall improvement.
Evaluation Summary. Our extensive study has shown the vast
beneﬁts of network-wide support for redundancy elimination, and
in particular, of changing network routing to be redundancy-aware.
We see that the impact of trafﬁc on ISP network resources can be
reduced signiﬁcantly. This is especially useful to control link loads
in situations of sudden overload. Finally, using routes computed on
the basis of stale proﬁles does not seem to undermine the beneﬁts of
our approaches. Of course, the initial cost of deployment of redun-
dancy elimination mechanisms on multiple network routers will be
quite high. However, our analysis shows that the long-term beneﬁts
of a wide-spread deployment are high enough to offset the cost.
Note that we assumed throughout that each router carries fully
decoded packets internally. But our proposals can be extended so
that routers switch smaller encoded packets (perhaps combining
multiple related small packets into a larger packet), with decod-
ing/reconstruction occurring only where necessary. This can help
overcome technology bottlenecks inside routers, in addition to sav-
ing bandwidth on links.
7.
IMPLEMENTATION
In this section, we examine some of the key challenges that may
hinder the deployment of redundancy elimination mechanisms on
fast routers. We offer preliminary solutions to the challenges. We
evaluate the trade-offs introduced by our solutions via a software
implementation based on the Click modular router [18]. Our imple-
mentation extends the base algorithm of Spring et. al [24].
An important bottleneck in performing redundancy elimination at
high speeds is the number of memory accesses required during the
various stages of redundancy elimination, such as on-the-ﬂy lookup,
insertion, deletion, and encoding the redundant region in a packet.
A second challenge is controlling the amount of memory required
to store the key data structures at routers, namely the ﬁngerprint
and the packet stores. Our implementation is focused on developing
memory efﬁcient ways to organize and access the data structures.
These issues have not been considered carefully in prior work.
Another key component is the computation of the hash function to
obtain ﬁngerprints for each packet. Rabin ﬁnger-prints used in [24]
are well-suited for high-speed implementation.
In particular, be-
cause Rabin ﬁngerprint computation relies on use sliding hashes,
the ﬁngerprints can be computed in parallel with CRC checks, even
as the bytes in a packet arrive into a router.
7.1 Packet Store
The layout of the packet store in our implementation is showed
in Figure 14(a). We implement the packet store as a FIFO buffer. In
particular, we use a circular buffer with a maximum of T ﬁxed-size
entries. With FIFO buffering, the oldest packet in the packet store is
evicted when there is no room to insert a new packet. We considered
using other policies for eviction (such as Least-Recently-Used), but
a preliminary study of these policies showed that FIFO offers nearly
the same performance (in terms of the amount of redundant content
identiﬁed), but is simpler to implement (See [14] for details).
We use a global variable called “MaxPktID” (4B) to aid packet
insertions and deletions. This is incremented before inserting a new
packet. The current value of MaxPktID is assigned to a variable
PktID which becomes a unique identiﬁer for the packet. The packet
itself is stored at the location P ktID % T in the store. Thus PktID
also indicates the starting memory address of the packet’s location.
We take a short digression and describe the ﬁngerprint store to
provide context for the rest of the design of the packet store. The ﬁn-
gerprint store holds meta-data for representative ﬁngerprints, which
includes the ﬁngerprint itself, the unique ID for the packet (i.e., the
PktID) referred to by the ﬁngerprint, and the byte offset in the packet
where the region represented by the ﬁngerprint starts.
When the packet store is full, we simply overwrite the new packet
at the tail of the circular store. We must also ensure that the ﬁnger-
prints pointing to the evicted old packet are invalidated. Rather than
invalidate the associated ﬁngerprints one-by-one (which can require
a large number of memory accesses), we can leverage the MaxP-
ktID variable and the PktID stored in the ﬁngerprint meta-data: To
see why, note that if (P ktID  20GB in size. Even at this large size,
there is no real guarantee of collision-freeness and hash chaining.
To improve hash table storage efﬁciency while still ensuring O(1)
lookups and inserts, we use CuckooHash [10] to design the ﬁn-
gerprint store. The CuckooHash-based design is illustrated in Fig-
ure 14(b). Each hash entry is divided into B buckets. Each bucket
stores a key, which is a ﬁngerprint entry in our case. A set of k ≤ 2
independent hash functions are used during insertion of a represen-
tative ﬁngerprint into the hash table: If any of the k × B locations
are found empty, the ﬁngerprint is inserted at the ﬁrst empty loca-
NumHashes → 1
2
f ↓
1.2
1.5
2
15.1% 11.5%
12.4% 7.8%
9.5% 4.6%
NumHashes → 1
2
f ↓
1.2
1.5
2
5.0% 0.06%
3.4% 0.02%
2.0% 0.003%
(a) B = 1
(b) B = 2
Table 1: Fraction of ﬁngerprints that we fail to insert.
tion. If no bucket is empty, the ﬁngerprint is simply not inserted (in
this case, we consider the insertion to have “failed”).
In Table 1, we explore the trade-offs between hash table size, the
number of buckets B, and the number of hash functions k. In par-
ticular, we examine the fraction of representative ﬁngerprints that
we fail to insert for a real packet trace selected at random. The more
ﬁngerprints we fail to insert, the lesser the extent of redundancy we
can identify and remove. The hash table size is a factor f larger
than the target number of ﬁngerprints we want to store; Note that
the target number is ﬁxed (approximately) for a given packet store
size and a given number of representative ﬁngerprints per packet;
We ﬁx the number of representative ﬁngerprints at 16 per packet.
From Tables 1(a) and (b), we note that using multiple hash buck-
ets offers better performance irrespective of the number of hash
functions used. We see from Table 1(b) that for k = 1 and f = 2,
we fail to insert just 2% of the ﬁngerprints. When k = 2 hash func-
tions are used, the probability of failure is essentially zero for any
f. Note, however, that we incur twice as many memory accesses
(during lookups) when using two hash functions instead of one.
Our implementation uses a single hash function, two buckets, and
f = 2, as this offers a reasonable middle-ground in terms of the
redundancy identiﬁed and memory accesses per packet. This design
also brings the ﬁngerprint store size to ≤ 1.5GB at OC48 speeds.
7.3 Encoding
The approach we use to encode duplicated chunks is the same as
Spring et. al [24]: For each duplicated byte string found, we remove
the matched region from the incoming packet and replace it with
a “shim layer” containing the matching packet’s PktID (4B), 2B
each for the starting byte positions of the current and the matching
packet, and 2B for the length of the match. When a packet matches
multiple ﬁngerprints, we store one shim per match, ensuring that the
matching byte regions identiﬁed by each shim are non-overlapping.
In summary, memory accesses are incurred by routers during in-
sertion of a packet and its representative ﬁngerprints, and during
retrieval of matching packets to perform encoding. Since we use 16
representative ﬁngerprints per packet (by default) and not all pack-
ets see matches, the former set of accesses are likely to dominate
the per packet memory access overhead. Note that the number of
memory accesses grows with the number of ﬁnger-prints stored per
packet, but so does the amount of redundancy identiﬁed.
7.4 Benchmarking Results
We have implemented packet-level redundancy elimination using
the aforementioned data structures in the Click modular router [18].
Our current implementation runs on a 1.8GHz AMD Opteron pro-
cessor with 8GB of RAM (64-bit Linux version 2.6.9). We con-
ﬁgured the packet store to use 400MB of memory. This results in
a 200MB ﬁngerprint store when using 16 ﬁngerprints per packet.
Hash computation, packet and ﬁngerprint insertion, and encoding
are all done serially in our software implementation.
We evaluated the throughput performance of our implementa-
tion using real packet traces. To estimate the maximum possible
throughput, we read the packet trace from main memory instead
of receiving packets over the network (to avoid delays due to in-
terrupt processing). Our implementation achieved a throughput of
1.05Gbps on average across multiple packet traces.
Max FPs Overall No
per Pkt
No Click
Updated machine
32
16
10
speed Click or Hashing No Click or Hashing
0.67
1.05
1.15
1.0
1.34
1.62
1.39
1.93
2.31
0.71
1.17
1.3
Redundancy
percentage
17.3%
15.8%
14.67%
Table 2: Throughput of software implementation (in Gbps) for
a random packet trace.
We proﬁled Click’s processing overhead and, upon accounting
for it, found that we achieved a throughput of 1.17Gbps (Table 2).
Next, we examine how memory access latencies affect the per-
formance of our implementation. To do this, we precomputed the
ﬁnger prints for all packets to avoid hash computation. The through-
put due to the rest of the components of our implementation is
shown in Table 2. This includes ﬁngerprint insertions, packet in-
sertions, packet retrievals, match region computations, and encod-
ing the match regions. We ran microprocessor performance bench-
marks to conﬁrm that the software is memory-bound. We see that
when using 16 FPs per packet, our implementation runs at 1.4Gbps.
Memory benchmarks for our test machine showed read/write la-
tencies to be 120ns per access. In contrast, today’s high-end DRAMs
operate at 50ns or faster. To understand the likely improvement with
faster DRAMs, we ran our implementation on an updated machine
with a 2.4GHz processor running a 32-bit Linux (see Table 2). The
memory latency on this desktop was benchmarked at 90ns. We con-
sistently observed a speed-up of 1.4X: with ≤ 16 ﬁngerprints, we
were able to obtain close to 2Gbps. With fewer ﬁngerprints (10 per
packet, which resulted in an 18-22% drop in redundancy proportion
identiﬁed), we obtained 2.3Gbps. Thus with 50ns DRAM latencies,
it seems likely that we can easily reach OC-48 speeds in software.
8. OTHER RELATED WORK
We discussed past studies most relevant to our work in Section 2.
Below, we discuss a few other pieces of related work.
Several past studies have examined the beneﬁts of cooperative
caching of Web objects [25, 11]. These studies are similar in spirit to
our work, but we take the much broader view of making redundant
content elimination as a universal router primitive.
Our redundancy-aware routing algorithms are somewhat similar
to multicast routing algorithms [7]. The algorithms we develop es-
sentially build efﬁcient multicast distribution trees. The shape and
structure of our trees are inﬂuenced by destinations which observe
signiﬁcant overlap in bytes accessed. In contrast, multicast tree con-
struction simply tracks the location of multicast participants.
Recent trafﬁc engineering proposals have tried to improve the re-
sponsiveness to real time trafﬁc variations [15]. While we leave a
full comparison of our techniques against these approaches for fu-
ture work, we do believe that the beneﬁts of the recent approaches
can be further enhanced by making them redundancy-aware.
9. CONCLUSIONS
In this paper, we explored the implications of deploying packet-
level redundant content elimination as a primitive service on all rou-
ters. Using real packet traces as well as synthetic workloads, we
showed that applying redundancy elimination on network links can
reduce resource utilization by 10-50% in ISP networks. However,
the network-wide beneﬁts can be further enhanced by modifying
network protocols, in particular, the routing protocols, to leverage
link-level redundancy elimination. We presented and analyzed a
suite of redundancy-aware intra- and inter-domain routing proto-
cols. We showed that they offer ISPs much better control over link
loads, a great degree of ﬂexibility in meeting trafﬁc engineering ob-
jectives, and greater ability to offer consistent performance under
sudden trafﬁc variations. We have developed a software prototype
of a high-speed packet-level redundancy elimination mechanism.
Our implementation uses simple techniques to control the amount
of memory and the number of memory accesses required for redun-
dancy elimination. Our implementation can run at OC48 speeds.
Hardware implementation speeds are likely to be much higher.
Our focus was on studying the beneﬁts in the context of a univer-
sal deployment. However, our redundancy-aware techniques can be
applied to limited-scale partial deployments of redundancy elimina-
tion across speciﬁc network links (e.g. across cross-country intra-
domain links, or congested peering links).
Of course, deploying redundancy elimination mechanisms on mul-
tiple network routers is likely to be expensive to start with. How-
ever, we believe that the signiﬁcant long term beneﬁts of our ap-
proaches offer great incentives for networks to adopt them.
Acknowledgments. We wish to thank the following people for their
advice: Fred Baker, Paul Barford, Mike Blodgett, Perry Brunelli,
Paul Francis, Bruce Davie, Randy Katz, George Varghese, Jia Wang
and Ty Znati. We thank the anonymous Sigcomm reviewers whose
comments helped improve our paper. This work was supported in
part by NSF grants CNS-0746531, CNS-0626889 and CNS-0435382.
10. REFERENCES
[1] Netequalizer Bandwidth Shaper. http://www.netequalizer.com/.
[2] Packeteer WAN optimization solutions. http://www.packeteer.com/.
[3] Peribit WAN Optimization. http://www.juniper.net/.
[4] Riverbed Networks. http://www.riverbed.com.
[5] A. Anand, A. Gupta, A. Akella, S. Seshan, and S. Shenker. Packet Caches on
Routers: The Implications of Universal Redundant Trafﬁc Elimination
(Extended Version). Technical Report 1636, UW-Madison, June 2008.
[6] B. Fortz and M. Thorup. Internet Trafﬁc Engineering by Optimizing OSPF
Weights. In Infocom, 2000.
[7] T. Ballardie, P. Francis, and J. Crowcroft. Core based trees (CBT). SIGCOMM
Comput. Commun. Rev., 23(4):85–95, 1993.
[8] M. Caesar, D. Caldwell, N. Feamster, J. Rexford, A. Shaikh, and J. van der
Merwe. Design and implementation of RCP. In NSDI, 2005.
[9] B. Davie and Y. Rekhter. MPLS: technology and applications. Morgan
Kaufmann Publishers Inc., San Francisco, CA, USA, 2000.
[10] U. Erlingsson, M. Manasse, and F. McSherry. A cool and practical alternative to
traditional hash tables. In WDAS, 2006.
[11] L. Fan, P. Cao, J. Almeida, and A. Z. Broder. Summary cache: a scalable
wide-area Web cache sharing protocol. In ACM SIGCOMM, 1998.
[12] B. Fortz, J. Rexford, and M. Thorup. Trafﬁc engineering with traditional IP
routing protocols. In Infocom, 2002.
[13] A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers, J. Rexford, G. Xie,
H. Yan, J. Zhan, and H. Zhang. A clean slate 4D approach to network control
and management. SIGCOMM Comput. Commun. Rev., 35(5):41–54, 2005.
[14] A. Gupta, A. Akella, S. Seshan, S. Shenker, and J. Wang. Understanding and
Exploiting Network Trafﬁc Redundancy. Technical Report 1592, UW-Madison,
April 2007.
[15] S. Kandula, D. Katabi, B. Davie, and A. Charny. Walking the tightrope:
responsive yet stable trafﬁc engineering. In ACM SIGCOMM, 2005.
[16] U. Manber. Finding similar ﬁles in a large ﬁle system. In USENIX Winter
Technical Conference, 1994.
[17] A. Medina, N. Taft, K. Salamatian, S. Bhattacharyya, and C. Diot. Trafﬁc matrix
estimation: existing techniques and new directions. In ACM SIGCOMM, 2002.
[18] R. Morris, E. Kohler, J. Jannotti, and M. F. Kaashoek. The Click modular router.
SIGOPS Oper. Syst. Rev., 33(5):217–231, 1999.
[19] A. Muthitacharoen, B. Chen, and D. Mazières. A low-bandwidth network ﬁle
system. SIGOPS Oper. Syst. Rev., 35(5), 2001.
[20] M. Rabin. Fingerprinting by Random Polynomials. Technical report, Harvard
University, 1981. Technical Report, TR-15-81.
[21] M. Roughan, M. Thorup, and Y. Zhang. Performance of estimated trafﬁc
matrices in trafﬁc engineering. In ACM SIGMETRICS, 2003.
[22] S. Singh, C. Estan, G. Varghese, and S. Savage. Automated worm ﬁngerprinting.
In OSDI, 2004.
[23] N. Spring, R. Mahajan, and D. Wetherall. Measuring ISP Topologies with
Rocketfuel. In ACM SIGCOMM, 2002.
[24] N. Spring and D. Wetherall. A protocol-independent technique for eliminating
redundant network trafﬁc. In ACM SIGCOMM, 2000.
[25] A. Wolman et al. On the scale and performance of cooperative Web proxy
caching. In ACM Symposium on Operating Systems Principles, 1999.