# nf.io: A File System Abstraction for NFV Orchestration

**Authors:**
- Md. Faizul Bari
- Shihabur Rahman Chowdhury
- Reaz Ahmed
- Raouf Boutaba

**Affiliation:**
David R. Cheriton School of Computer Science, University of Waterloo

**Emails:**
- mfbari@uwaterloo.ca
- sr2chowdhury@uwaterloo.ca
- r5ahmed@uwaterloo.ca
- rboutaba@uwaterloo.ca

## CCS Concepts
- **Networks:** Network architectures; Network design principles

## Keywords
- Network Function Virtualization (NFV)
- Service Chain Orchestration
- File System Abstraction

## 1. Introduction
Middleboxes have become an integral part of modern enterprise and data center networks, serving various performance and security objectives. Traditionally, these middleboxes (e.g., firewalls, Intrusion Detection Systems (IDSs), Network Address Translators (NATs)) are dedicated hardware appliances. However, recent advancements in cloud and virtualization technologies have introduced the concept of Virtual Middleboxes or Virtual Network Functions (VNFs) and a new research field known as Network Function Virtualization (NFV). This area has gained significant traction from both industry and academia.

Despite the progress in NFV technology, a critical component for realizing the primary objective of NFV is still missing: a management and orchestration system that adheres to the principles of NFV, such as open source, open APIs, and standardized software solutions. Without this, network operators may face vendor lock-in, similar to proprietary hardware middleboxes.

Recent proposals like Stratos [8], OpenNF [9], and Split/Merge [10] aim to fulfill the requirements for VNF management and orchestration. However, they propose incompatible northbound APIs. What is needed is a standardized API that is flexible enough to express a wide range of NFV management and orchestration operations. Given that standardization efforts often take a long time and can be futile, we propose a different approach: using an existing, well-known, and standardized interface for NFV management and orchestration—the Linux file system interface.

We introduce nf.io, which utilizes the Linux file system as the northbound API for VNF orchestration. It adopts several operating system principles:
- Everything (resources, configurations) is represented as files.
- Common Linux utility programs (e.g., `mkdir`, `cp`, `mv`, `ln`) are used for state manipulation.
- Heterogeneous resource pools (e.g., different networking tool-chains like Linux bridge or Open vSwitch [5]) are controlled through a high-level abstraction.
- Resource-specific drivers are developed, similar to device drivers in an OS.

Existing NFV management and orchestration systems like Stratos or OpenNF can use the nf.io abstraction by developing resource drivers specific to their requirements.

## 2. System Description

### 2.1 Features
The key features of nf.io include:
- **Everything is a file:** States and configurations of a VNF deployment are represented as files organized in a hierarchical directory structure.
- **Centralized control:** A centralized point of control over a distributed VNF deployment.
- **Compatibility:** A rich set of existing file system utilities (e.g., `grep`, `mkdir`) and configuration management tools (e.g., Chef, Puppet) can be used with nf.io for VNF management.

### 2.2 File System Abstraction
nf.io uses a simple and intuitive directory hierarchy to store states regarding VNF deployment, configuration, and chaining. Figure 1 provides a high-level view of the nf.io directory hierarchy. The root of the file system with two users is shown in Figure 1(a). The `user-a` and `user-b` directories mark the home directories for the users. The VNFs and chains deployed by a user are organized under their respective home directories.

Figure 1(b) shows the structure of a directory representing a VNF. The `config` and `machine` directories contain configuration parameters. The `action` file is used by the user to perform different VNF operations (e.g., start, pause, resume, kill). The `status` file indicates the current status of the VNF (e.g., running, paused, error). The `stats` directory contains files for collecting data like packet drops, transmitted/received bytes, etc. The `rfs` directory mounts the file system of the VNF itself, allowing the user to directly change a configuration file and read different kinds of statistics from the VNF. A VNF chain is deployed by creating a directory under the `chns` directory. A chain directory contains symbolic links to the VNF instances that are part of the chain and markers to indicate the start and next VNFs in the chain.

### 2.3 Architecture
Figure 2 provides a high-level view of the nf.io architecture. The nf.io File System is a virtual file system that runs on top of the traditional OS file system. VNF operations are triggered when a user writes an operation string in the `action` files. nf.io performs these operations using three resource drivers:
- **Hypervisor Driver:** Abstracts the underlying diversity in virtualization technologies (e.g., processes on a physical machine, VMs on Xen or KVM, or lightweight containers provided by Docker or LXC) and provides a uniform interface to nf.io.
- **Network Driver:** Supports certain networking functionality from the underlying physical infrastructure, including setting up bridges, creating IP links between virtual Ethernet (veth) pairs, setting up tunnels (e.g., VXLAN or GRE), and installing forwarding rules. It hides the underlying heterogeneity and provides an abstract network interface to nf.io.
- **Chain Driver:** Interconnects different types of VNFs. It provides a function `chn-cnct(vnf1, vnf2)`, where `vnf1` and `vnf2` are two arbitrary VNFs. For a chain like `a → b → c`, this function must be called twice: first for `a → b`, and again for `b → c`. The task of interconnecting two VNFs depends on their types and whether their network interfaces are on the same or different IP subnets.

### 2.4 Implementation
The nf.io prototype is implemented using the Python API binding for FUSE [2]. We rewrote several file system calls like `mkdir`, `read`, `write`, `symlink`, etc., to implement the nf.io file system semantics. The Hypervisor Driver currently supports KVM, Xen, and Docker. We use libvirt [3] and Docker Remote API to control VMs and containers in KVM/Xen and Docker, respectively. The Network and Chain Drivers currently support two configurations: (i) Linux iptables and Linux bridge, and (ii) Open vSwitch. In both cases, we use GRE tunnels to connect VNFs deployed on different physical machines. Finally, we remotely mount the VNF’s file system under the `rfs` directory (Figure 1) using sshfs [6]. A demonstration of nf.io is available at http://faizulbari.github.io/nf.io/.

## 3. Demonstration
We demonstrate the capabilities of nf.io by showcasing use cases focused on three primary areas: (i) configuration, (ii) deployment, and (iii) monitoring of VNF instances and chains. First, we will show how to configure different parameters of a single VNF instance. Then, we will configure a service chain consisting of multiple VNFs and tweak different chain-level parameters. Next, we will deploy the service chain on Docker containers and run a client to generate some test data. Finally, we will demonstrate nf.io’s monitoring features by querying data both at the VNF and chain levels.

## 4. Acknowledgments
This work was supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) under the Smart Applications on Virtual Infrastructure (SAVI) Research Network.

## 5. References
[1] Docker. http://docker.com/.
[2] fusepy. https://github.com/terencehonles/fusepy.
[3] libvirt: The virtualization API. http://libvirt.org/.
[4] LXC: Linux Containers. https://linuxcontainers.org/.
[5] OVS: Open vSwitch. https://openvswitch.org/.
[6] sshfs. http://fuse.sourceforge.net/sshfs.html.
[7] Bari, M. F., Chowdhury, S. R., Ahmed, R., and Boutaba, R. On orchestrating virtual network functions in NFV. CoRR abs/1503.06377 (2015).
[8] Gember, A., Krishnamurthy, A., John, S. S., Grandl, R., Gao, X., Anand, A., Benson, T., Akella, A., and Sekar, V. Stratos: A network-aware orchestration layer for middleboxes in the cloud. Tech. rep., 2013.
[9] Gember-Jacobson, A., Viswanathan, R., Prakash, C., Grandl, R., Khalid, J., Das, S., and Akella, A. OpenNF: Enabling innovation in network function control. In Proc. of SIGCOMM (2014), ACM, pp. 163–174.
[10] Rajagopalan, S., Williams, D., Jamjoom, H., and Warfield, A. Split/merge: System support for elastic execution in virtual middleboxes. In Proc. of USENIX NSDI (2013), pp. 227–240.