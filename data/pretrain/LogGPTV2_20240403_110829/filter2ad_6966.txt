title:nf.io: A File System Abstraction for NFV Orchestration
author:Md. Faizul Bari and
Shihabur Rahman Chowdhury and
Reaz Ahmed and
Raouf Boutaba
nf.io: A File System Abstraction for NFV Orchestration
Md. Faizul Bari, Shihabur Rahman Chowdhury, Reaz Ahmed, Raouf Boutaba
David R. Cheriton School of Computer Science, University of Waterloo
{mfbari, sr2chowdhury, r5ahmed, rboutaba}@uwaterloo.ca
CCS Concepts
•Networks → Network architectures; Network de-
sign principles;
Keywords
Network Function Virtualization; Service Chain Orchestra-
tion; File System Abstraction
1.
INTRODUCTION
Middleboxes have become an integral part of modern en-
terprise and data center networks. They are used for realiz-
ing various performance and security objectives. Most mid-
dlboxes (e.g., ﬁrewalls, Intrusion Detection Systems (IDSs),
Network Address Translators (NATs), etc.) are dedicated
hardware appliances. However, recent advancements in cloud
and virtualization technologies have fueled the concept of
Virtual Middleboxes or Virtual Network Functions (VNFs)
along with a new research ﬁeld known as Network Function
Virtualization (NFV). This area of research has gained a
lot of traction from both industry and academia. Although
much progress has been made in NFV technology, a crucial
component for realizing the primary objective of NFV is still
missing – a management and orchestration [7] system that
conforms to the principles of NFV: open source, open API
and standardized software solutions. Without this feature,
network operators may end-up with the same situation of
vendor lock-in as with proprietary hardware middleboxes.
A number of recent proposals like Stratos [8], OpenNF [9],
and Split/Merge [10], strive to fulﬁll the requirements for
VNF management and orchestration. However, they pro-
pose incompatible northbound APIs. What is really needed
is a standardized API that is ﬂexible enough to express a
wide range of NFV management and orchestration opera-
tions. History shows that standardization eﬀorts usually
take a long time and often are futile. Hence, we take a dif-
ferent approach, and propose to use an existing, well known,
standardized interface for NFV management and orchestra-
tion: the Linux ﬁle system interface.
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for proﬁt or commercial advantage and that copies bear this notice and the full citation
on the ﬁrst page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
SIGCOMM ’15 August 17-21, 2015, London, United Kingdom
c(cid:13) 2015 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-3542-3/15/08.
DOI: http://dx.doi.org/10.1145/2785956.2790028
(a) nf.io root (b) VNF Instance
Figure 1: nf.io File System Abstraction
We call our proposed system nf.io. It utilizes the Linux
ﬁle system as the northbound API for VNF orchestration.
It adopts various operating system principles: (i) everything
(resource, conﬁguration) is represented as ﬁles, (ii) common
Linux utility programs (e.g., mkdir, cp, mv, ln, etc.) are used
for state manipulation, (iii) heterogeneous resource pools
(e.g., diﬀerent networking tool-chains like Linux bridge or
Open vSwitch [5]) are controlled through a high-level ab-
straction, and (iv) resource speciﬁc drivers are developed
similar to device drivers in an OS. Existing NFV manage-
ment and orchestration systems like Stratos or OpenNF can
use the nf.io abstraction by developing resource drivers spe-
ciﬁc to their requirements.
2. SYSTEM DESCRIPTION
2.1 Features
Key features of nf.io are as follows:
• Everything is a ﬁle: States and conﬁgurations of a
VNF deployment are represented as ﬁles organized in
a hierarchical directory structure.
• Centralized control: A centralized point of control
• Compatibility: A rich set of existing ﬁle system util-
ities (e.g., grep, mkdir, etc.) and conﬁguration man-
agement tools (e.g., Chef, Puppet, etc.) can be used
with nf.io for VNF management.
over a distributed VNF deployment.
2.2 File System Abstraction
nf.io uses a simple and intuitive directory hierarchy to
store states regarding VNF deployment, conﬁguration and
chaining. A high-level view of the nf.io directory hierarchy
is shown in Figure 1. The root of the ﬁle system with two
users is shown in Figure 1(a). The user-a and user-b di-
rectories mark the home directory for the users. The VNFs
361and chains deployed by a user are organized under his home
directory. The structure of a directory representing a VNF
is shown in Figure 1(b). The config and machine direc-
tories contain conﬁguration parameters. The action ﬁle is
used by the user to perform diﬀerent VNF operations (e.g.,
start, pause, resume, kill, etc.). The status ﬁle indicated
the current status of the VNF (e.g., running, paused, er-
ror, etc.). The ﬁles contained under the stats directory are
used to collect data like packet drops, transmitted/received
bytes, etc. The stats directory contains one ﬁle for each
measurement metric. The rfs directory mounts the ﬁle sys-
tem of the VNF itself, so that the user can directly change
a conﬁguration ﬁle and also read diﬀerent kinds of statis-
tics from the VNF. A VNF chain is deployed by creating a
directory under the chns directory. A chain directory con-
tains symbolic links to the VNF instances that are part of
the chain. It also contains markers to indicate the start and
next VNFs in the chain.
2.3 Architecture
Figure 2: nf.io Architecture
A high-level view of the nf.io architecture is shown in Fig-
ure 2. The nf.io File System is a virtual ﬁle system that
runs on top of the traditional OS ﬁle system. VNF opera-
tions are triggered when a user writes a operation string in
the action ﬁles. nf.io performs these operations by using
three resource drivers: (i) Hypervisor Driver, (ii) Network
Driver, and (iii) Chain Driver.
2.3.1 Hypervisor Driver
In nf.io, network functions can be deployed in a number
of ways. They can run as processes on a physical machine,
VMs on a hypervisor like Xen or KVM, or as light-weigh con-
tainers provided by Docker [1] or Linux Container (LXC) [4].
The hypervisor driver abstracts the underlying diversity in
these virtualization technologies and provides a uniform in-
terface to nf.io.
2.3.2 Network Driver
nf.io requires support for certain networking function-
ality from the underlying physical infrastructure.
In each
physical machine, nf.io must have the ability to (i) setup
bridges, (ii) create IP links between virtual ethernet (veth)
pairs, (iii) setup tunnels (e.g., VXLAN or GRE), and (iv)
install forwarding rules. Similar to the hypervisor driver,
the network driver hides the underlying heterogeneity and
provides an abstract network interface to nf.io.
2.3.3 Chain Driver
The chain driver interconnects diﬀerent types of VNFs.
It provides a function chn-cnct(vnf1, vnf2), where vnf1
and vnf2 are two arbitrary VNFs. For a chain like a → b
→ c, this function must be called twice: ﬁrst for a → b,
and again for b → c. The task of Interconnecting two VNFs
depends primarily on their types, and whether their network
interfaces are on the same or diﬀerent IP subnets.
2.4 Implementation
The nf.io prototype is implemented using the python
API binding for FUSE [2]. We rewrote a number of ﬁle
system calls like mkdir, read, write, symlink, etc. to im-
plement the nf.io ﬁle system semantics. The Hypervisor
Driver currently supports KVM, Xen and Docker. We use
libvirt [3] and Docker Remote API to control VMs and con-
tainers in KVM/Xen and Docker, respectively. The Network
and Chain Drivers currently support two conﬁgurations: (i)
Linux iptables and Linux bridge and (ii) Open vSwitch. In
both cases we use GRE tunnels to connect VNFs deployed
on diﬀerent physical machines. Finally, we remotely mount
the VNF’s ﬁle system under the rfs directory (Figure 1)
using sshfs [6]. A demonstration of nf.io is available at
http://faizulbari.github.io/nf.io/.
3. DEMONSTRATION
We demonstrate the capabilities of nf.io by showcasing
use cases focused on three primary areas: (i) conﬁguration,
(ii) deployment, and (iii) monitoring of VNF instances and
chains. First, we will show how to conﬁgure diﬀerent pa-
rameters of a single VNF instance. Then we will conﬁgure a
service chain consisting of multiple VNFs and tweak diﬀer-
ent chain level parameters. Next, we will deploy the service
chain on Docker containers and run a client to generate some
test data. Finally, we will demonstrate nf.io’s monitoring
features by querying data both at the VNF and chain levels.
4. ACKNOWLEDGMENTS
This work was supported by the Natural Science and Engi-
neering Council of Canada (NSERC) under the Smart Appli-
cations on Virtual Infrastructure (SAVI) Research Network.
5. REFERENCES
[1] Docker. http://docker.com/.
[2] fusepy. https://github.com/terencehonles/fusepy.
[3] libvirt: The virtualization API. http://libvirt.org/.
[4] LXC: Linux Containers. https://linuxcontainers.org/.
[5] OVS: Open vSwitch. https://linuxcontainers.org/.
[6] sshfs. http://fuse.sourceforge.net/sshfs.html.
[7] Bari, M. F., Chowdhury, S. R., Ahmed, R., and
Boutaba, R. On orchestrating virtual network
functions in NFV. CoRR abs/1503.06377 (2015).
[8] Gember, A., Krishnamurthy, A., John, S. S.,
Grandl, R., Gao, X., Anand, A., Benson, T.,
Akella, A., and Sekar, V. Stratos: A
network-aware orchestration layer for middleboxes in
the cloud. Tech. rep., 2013.
[9] Gember-Jacobson, A., Viswanathan, R.,
Prakash, C., Grandl, R., Khalid, J., Das, S.,
and Akella, A. OpenNF: Enabling innovation in
network function control. In Proc. of SIGCOMM
(2014), ACM, pp. 163–174.
[10] Rajagopalan, S., Williams, D., Jamjoom, H.,
and Warfield, A. Split/merge: System support for
elastic execution in virtual middleboxes. In Proc. of
USENIX NSDI (2013), pp. 227–240.
Hypervisor Driver Network Driver Chain Driver nf.io File System Command Line Utils. Custom Scripts Automation Tools Compute Resources Network Resources VNF  Chaining 362