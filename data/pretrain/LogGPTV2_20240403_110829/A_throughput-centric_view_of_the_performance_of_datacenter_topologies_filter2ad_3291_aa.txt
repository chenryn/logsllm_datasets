title:A throughput-centric view of the performance of datacenter topologies
author:Pooria Namyar and
Sucha Supittayapornpong and
Mingyang Zhang and
Minlan Yu and
Ramesh Govindan
A Throughput-Centric View of the Performance of Datacenter
Topologies
Pooria Namyar
University of Southern California
Sucha Supittayapornpong
Vidyasirimedhi Institute of Science
and Technology
Mingyang Zhang
University of Southern California
Ramesh Govindan
Minlan Yu
Harvard University
ABSTRACT
While prior work has explored many proposed datacenter designs,
only two designs, Clos-based and expander-based, are generally con-
sidered practical because they can scale using commodity switching
chips. Prior work has used two different metrics, bisection band-
width and throughput, for evaluating these topologies at scale. Little
is known, theoretically or practically, how these metrics relate to
each other. Exploiting characteristics of these topologies, we prove
an upper bound on their throughput, then show that this upper
bound better estimates worst-case throughput than all previously
proposed throughput estimators and scales better than most of
them. Using this upper bound, we show that for expander-based
topologies, unlike Clos, beyond a certain size of the network, no
topology can have full throughput, even if it has full bisection band-
width; in fact, even relatively small expander-based topologies fail
to achieve full throughput. We conclude by showing that using
throughput to evaluate datacenter performance instead of bisection
bandwidth can alter conclusions in prior work about datacenter
cost, manageability, and reliability.
CCS CONCEPTS
‚Ä¢ Networks ‚Üí Data center networks; Network performance
modeling; Network manageability; Topology analysis and
generation; ‚Ä¢ General and reference ‚Üí Metrics;
KEYWORDS
Data centers, Throughput, Clos topologies, Network management
ACM Reference Format:
Pooria Namyar, Sucha Supittayapornpong, Mingyang Zhang, Minlan Yu,
and Ramesh Govindan. 2021. A Throughput-Centric View of the Perfor-
mance of Datacenter Topologies. In ACM SIGCOMM 2021 Conference (SIG-
COMM ‚Äô21), August 23‚Äì28, 2021, Virtual Event, USA. ACM, New York, NY,
USA, 21 pages. https://doi.org/10.1145/3452296.3472913
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
¬© 2021 Association for Computing Machinery.
ACM ISBN 978-1-4503-8383-7/21/08.
https://doi.org/10.1145/3452296.3472913
University of Southern California
1 INTRODUCTION
A primary contributor to the success of cloud computing is the dat-
acenter, a warehouse-style agglomeration of compute and storage
on commodity servers. The performance of distributed applica-
tions running inside a datacenter, like search, reliable storage, and
social networks, is strongly determined by the design of the dat-
acenter network. This network consists of a topology in which
switches interconnect servers. Today, datacenters routinely have
tens of thousands of switches connecting hundreds of thousands
of servers. Our focus, in this paper, is on the design and evaluation
of topologies for such large-scale datacenters.
Datacenter topology designs. Two distinct classes of topology
designs have emerged in recent years. Clos [8] based designs include
Fat-tree [1], VL2 [15], Jupiter [42] and Facebook Fabric [3], and
failure-resilient variants, such as F10 [36]. These hierarchical de-
signs are bi-regular, in which a switch either connects to ùêª servers,
or none at all (Figure 1). More recent alternative designs target
lower installation costs and/or incur lower management costs than
Clos-based topologies. These designs employ an expander-graph to
interconnect switches, and include Jellyfish [44], Xpander [47], and
FatClique [52]. These topologies are uni-regular: every switch con-
nects to ùêª servers (Figure 1). In both classes, each server connects
to exactly one switch.1
Measures of topology capacity. The capacity of the data center
network limits the performance of applications it hosts. Intuitively,
a topology with enough capacity to permit every server to send
traffic at full line rate simplifies cloud application design: operators
can place application instances anywhere in the network without
impacting performance, and this placement flexibility enables ap-
plications to be more cost efficient and more robust to correlated
failures (e.g., of an entire rack or power domain) [15, 21, 35, 42].
Most prior work [1, 3, 15, 42, 52] has used the network‚Äôs bisection
bandwidth, the smallest aggregate capacity of the links crossing the
worst-case cut among all the cuts that divide the topology graph
into two halves, as a measure of its capacity. A topology has full
bisection bandwidth if its bisection bandwidth is at least equal to
half of the total servers; for Clos-based designs, such a topology
permits arbitrary application instance placement.
Other work [24, 26, 27, 50, 51] has explored an alternative mea-
sure of network capacity, throughput, defined as follows. The
throughput under traffic matrix ùëá is the highest scaling factor ùúÉ(ùëá)
such that the topology can support the traffic matrix, ùëá ¬∑ ùúÉ(ùëá),
1Other topology designs, such as DragonFly [30], and SlimFly [6], do not scale to the
sizes of modern data centers, so we do not consider them in this paper; see ¬ß7.
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
Namyar .et al.
In this paper, we take a first step in understanding the relation-
ship between these metrics by making the following contributions.
Contribution: The Difference Between Full Throughput and
Full Bisection Bandwidth for Uni-regular Topologies. We
prove (¬ß4) that for any uni-regular topology, there exists a size (in
terms of the number of servers) beyond which the topology cannot
have full throughput even if it has full bisection bandwidth. This is
true even of small instances of uni-regular topologies with as few
as 10-15K servers (¬ß4.2). By contrast, bi-regular Clos topologies are
not subject to this limit, and a full bisection bandwidth topology
always has full throughput (Figure 2). This means that a topology
designer cannot ensure application placement independence (more
precisely, the ability to support any arbitrary traffic demand) using
a full bisection bandwidth uni-regular topology (Table 1). Put
differently, for uni-regular topologies, full bisection bandwidth is
necessary but not sufficient to support arbitrary traffic demand; by
definition, full throughput is both necessary and sufficient.
Figure 2: Full throughput vs. Full bisection bandwidth.
Contribution: A Throughput-Centric View. Table 1 shows that
prior work has used bisection bandwidth to evaluate uni-regular
and bi-regular topologies; we show that using throughput can lead
to different conclusions, impacting cost and management complex-
ity (¬ß5.1). It is also the more appropriate metric: as the previous
contribution demonstrates, throughput better captures the capac-
ity of both uni-regular and bi-regular topologies, while bisection
bandwidth does not.
‚ñ∂ Prior work has argued that a full bisection bandwidth Jellyfish,
Xpander or FatClique uses 50% fewer switches than a full bisection
bandwidth Clos [8]. We show that a full throughput Jellyfish [44],
Xpander [47] or FatClique [52] uses only 25% fewer switches than a
full throughput Clos. This finding is important, because the smaller
cost differential may make uni-regular topologies less attractive
relative to Clos (whose packaging and routing simplicity may out-
weigh its higher cost).
‚ñ∂ Prior work has argued that a Jellyfish or FatClique can be ex-
panded: (a) with minor bandwidth loss while keeping the num-
ber of servers per switch constant; (b) using a random rewiring
strategy [52] simpler than that for Clos [53]. This assumes that
bandwidth loss is estimated using bisection bandwidth. We show
that, expanding a full throughput Jellyfish or FatClique by even a
small amount, while keeping fixed the number of servers per switch,
can result in a topology without full throughput. Thus, a designer
wishing to maintain full throughput for uni-regular topologies after
expansion may need to re-wire servers, requiring a much more
complex expansion strategy than Clos.
Figure 1: Uni-regular and bi-regular topologies.
without violating any link‚Äôs capacity constraint. The throughput
of a topology denoted by ùúÉ‚àó is the worst-case throughput among
all traffic matrices. A topology can support any traffic demand if
and only if ùúÉ‚àó is at least 1 (in this case, we say the topology has
full throughput). Because it can support any traffic demand, a full
throughput topology also permits arbitrary application instance
placement by definition.2
How prior work uses these metrics. These metrics can help
evaluate topology design, perform cost comparisons, or assess the
complexity of network expansion. As Table 1 shows a substantial
body of work has used bisection bandwidth to perform these
assessments on large-scale uni-regular and bi-regular topologies.
(Some prior work [27, 43, 44, 47] has used throughput to perform
some of these assessments, but for much smaller-scale topologies
with only a few thousand servers.)
Objective
Evaluate Design
Assess Cost
Estimate Expansion
complexity
Metric
BBW
BBW
BBW
Topology Class
bi-regular
uni-regular
bi-regular
uni-regular
bi-regular
uni-regular
Prior work
[1, 15, 42]
[44, 47, 52]
[15, 42, 44, 52]
[44, 52]
[10, 42, 52, 53]
[44, 52]
Table 1: Prior work has used bisection bandwidth for large-scale evalua-
tions.
Given this discussion, it is natural to ask: What is the difference
between these metrics for uni-regular and bi-regular topologies?
Should the papers listed in Table 1 have used throughput instead?
How would these assessments change if they did?
To our knowledge, the literature has not explored the precise
difference between these two metrics, but has explored related,
but slightly different questions. Bisection bandwidth is a graph-cut
based metric, and [27] has studied the relation between cut based
metrics and throughput at a scale much smaller than those we
consider in this paper. As well, [34] shows that the sparsest cut
of any topology for a given traffic matrix is within ùëÇ(ùëôùëúùëîùëÅ) of its
throughput for that traffic matrix. Finally, Yuan et al. [50] show
that bisection bandwidth cannot predict average throughput of a
topology.
2To actually achieve arbitrary instance placement, one also might need a scalable,
practical routing scheme that can exploit the topology‚Äôs available capacity. For Clos-
based networks, ECMP-based routing can do so. For large-scale uni-regular topologies,
we believe this question is open. We don‚Äôt address this in this paper since we focus on
topology properties.
A Throughput-Centric View of the Performance of Datacenter Topologies
SIGCOMM ‚Äô21, August 23‚Äì28, 2021, Virtual Event, USA
‚ñ∂ Datacenter designers have traded off topology capacity for lower
cost by designing over-subscribed topologies. The FatTree [1] paper
defines the over-subscription ratio of a topology as the ratio of
the worst-case achievable throughput between end-hosts to the
aggregate bisection bandwidth. Our results suggest that, for uni-
regular topologies, a more direct definition of over-subscription
ratio is the throughput itself (a throughput less than 1 indicates an
over-subscribed topology). We find that, for these topologies, the
bisection-bandwidth based over-subscription ratio overestimates
the throughput by up to 50%. Thus, a designer using that definition
would build a network whose actual capacity is lower than the
targeted capacity.
Contribution: An Efficiently-Computable, Tight, Through-
put Upper Bound. The previous contributions require a way to
compute the throughput of large uni-regular and bi-regular topolo-
gies. To this end, we make the following contributions.
‚ñ∂ We prove an upper bound on the throughput of uni-regular and
bi-regular topologies (¬ß2).
‚ñ∂ We empirically show (¬ß3) that this upper bound is tighter and
scales better than existing approaches of estimating network ca-
pacity or throughput: the throughput bound in [43], heuristics for
estimating throughput in [23, 24, 51], bisection bandwidth, and
sparsest cut [27].
‚ñ∂ This scalable throughput upper bound can be used to better assess