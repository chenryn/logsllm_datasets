title:Lawful Device Access without Mass Surveillance Risk: A Technical
Design Discussion
author:Stefan Savage
Lawful Device Access without Mass Surveillance Risk:
A Technical Design Discussion
Department of Computer Science and Engineering
Stefan Savage
PI:EMAIL
ABSTRACT
This paper proposes a systems-oriented design for supporting court-
ordered data access to “locked” devices with system-encrypted
storage, while explicitly resisting large-scale surveillance use. We
describe a design that focuses entirely on passcode self-escrow
(i.e., storing a copy of the user passcode into a write-only com-
ponent on the device) and thus does not require any changes to
underlying cryptographic algorithms. Further, by predicating any
lawful access on extended-duration physical seizure, we foreclose
mass-surveillance use cases while still supporting reasonable inves-
tigatory interests. Moreover, by couching per-device authorization
protocols with the device manufacturer, this design avoids creating
new trusted authorities or organizations while providing particu-
larity (i.e., no “master keys” exist). Finally, by providing a concrete
description of one such approach, we hope to encourage further
technical consideration of the possibilities and limitations of trade-
offs in this design space.
ACM Reference Format:
Stefan Savage. 2018. Lawful Device Access without Mass Surveillance Risk:,
A Technical Design Discussion. In 2018 ACM SIGSAC Conference on Com-
puter and Communications Security (CCS ’18), October 15–19, 2018, Toronto,
ON, Canada. ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/
3243734.3243758
1 INTRODUCTION
This paper focuses on a problem of considerable controversy—
balancing our collective security interests in allowing law enforce-
ment to access the contents of electronic devices under court order
(e.g., cell phones), and our individual privacy rights in securing our
data against illegal and/or unreasonable access. In particular, much
of this debate has centered around the special challenges posed by
encrypted data on such devices since, in principle, well-encrypted
data can resist all attempts to access it—whether by criminal ac-
tors or court-authorized law enforcement officers. This reality has
driven a divide between those in the law enforcement community
who have argued that new technical measures are necessary to
ensure that a lawful access capability can be maintained and civil
libertarians who argue that any potential technical measures will
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CCS ’18, October 15–19, 2018, Toronto, ON, Canada
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5693-0/18/10...$15.00
https://doi.org/10.1145/3243734.3243758
open the door to mass surveillance and inherently weaken security
against other threats.1
Absent better solutions, the current state of affairs is one that
should be unsatisfying to all sides—one in which lawful access is
achieved via unregulated private sector device access capabilities
that offer few intrinsic protections against abuse yet are inherently
undependable and expensive. To be clear, law enforcement’s access
to encrypted devices today is via the “lawful hacking” approach, in
which zero-day vulnerabilities are exploited to provide a mechanism
for bypassing existing device protections[10, 15, 27, 47]. However,
because vulnerabilities in well-engineered devices are challenging
to find, their value is directly tied to their secrecy; device manufac-
turers, rightly, have an interest in improving the security of their
platforms and will patch vulnerabilities they become aware of.2
The government’s obligation to share potentially exculpatory infor-
mation with defense counsel thus creates a natural disincentive to
use home-grown tools subject to such discovery.3 Instead, today’s
status quo is one in which public dollars fund private corporations
(e.g., Greyshift, Cellebrite, etc.) who develop zero-day exploits and
the tooling used to bypass data protection on personal devices;
these capabilities are then sold for profit in products and services
used to “unlock” encrypted devices. Unlike the designs we will ex-
plore in this paper, vulnerability-based access of this kind provides
no technical safeguards against surveillance use nor against misuse
by criminal parties. Similarly, we are unaware of any regulatory
oversight or statutory remedy that would prevent the sale of such
capabilities to a broad range of parties.
To date, the research community has made little progress on
improving this state of affairs. While there are many in the com-
munity who have strong feelings on the topic, much of this energy
has focused on the policy aspects of the debate and there has been
comparatively little constructive engagement with the underlying
technical questions and options. Absent broad explorations of the
technical design space or concrete engineering proposals to fo-
cus attention, the issue has become highly polarized—evidenced on
both sides by loaded terminology (e.g., “going dark” vs “backdoors”),
appeals to authority, and refutations of “straw man” arguments. As
a counterpoint, this paper describes a particular technical design—
one that makes explicit tradeoffs to try and simultaneously support
lawful government investigatory interests while explicitly seeking
to curtail risks of mass surveillance. It is our hope that in providing
1We use the term “lawful access” in preference to other terminology because it specifies
the goal of any such capability—that access is available via lawful process, but is not
available otherwise—without the negative connotations or design presuppositions that
most other terms impart.
2Bellovin et al. advocate for mandatory government vulnerability disclosures and
implicitly argue that vulnerability patching is a small issue because appropriate vul-
nerabilities are plentiful and updated slowly [27]. However, both claims seem at odds
with empirical reality for smartphone platforms with strong security engineering.
3This conflict is evident in several of the recent “Playpen” cases, notably US v Michaud.
a concrete design proposal we will stimulate a broader discussion
on the technical aspects of this issue.
In particular, this paper focuses on exploring a single pri-
mary question: Can mechanisms be designed to support court-
ordered “device unlock” capability, while explicitly curtail-
ing risks that this capability could be used for mass surveil-
lance? In pursuing this goal, we describe a prospective system
design in which devices self-escrow access credentials (e.g., informa-
tion that would allow the device to be “unlocked”) into a co-located
hardware component that can only be interrogated via physical
access (e.g., via a hardware JTAG test access port that accesses inter-
nal chip state and purposely has no software-based read interface).4
In this design, the same component also implements time-vaulting—
meaning that an extended period of continuous physical access
(e.g., 72 hours) must be demonstrated between before any release
of information can take place. Moreover, even after this period, the
hardware is only to provide valid data if the requestor provides
evidence of authorization via a device-specific secret. This secret is
to be maintained, per-device, by the manufacturer and would need
to be requested by lawful parties under court order. This authoriza-
tion mechanism is irrelevant to the primary goal of deterring mass
surveillance, but addresses a secondary goal of resisting criminal
access (e.g., stolen phones) or other uses outside an authorized legal
process. Finally, we envision multiple mechanisms by which the
use of this capability could be made transparent (i.e., that it would
be difficult to covertly unlock a device in this manner and then
return it to service).
We believe this design offers little benefit for mass surveillance
use because the extended physical access requirement does not
scale (i.e., it would require the regular physical seizure of millions
of devices). In addition, it has the benefit of requiring no change to
existing underlying cryptographic algorithms or protocols in use,
does not rely on the government to faithfully escrow secrets and
does not create any fundamentally new trust relationships. At the
same time, this approach provides a means to unlock (and hence
potentially decrypt) devices under court order.
In the remainder of this paper, we summarize some of the context,
describe our design assumptions and threat model, explain our basic
approach and implementation choices (particularly in the context of
modern smartphone devices) and discuss some of the most relevant
policy issues that have been brought up previously. Ultimately, this
paper is a kind of Gedankenexperiment, one that tries to identify a
reasonable set of design tradeoffs for how one might provide lawful
access to locked devices without creating undue risks of extra-legal
abuses. While this design is its primary contribution, we hope that
it also helps encourage the security community to actively engage
in exploring the technical aspects of controversial issues such as
this one.
2 BACKGROUND
The tension between the state’s law enforcement needs in obtaining
digital evidence and individual’s privacy interests in protecting their
information against unauthorized access has been a source of debate
4JTAG, an acronym for Joint Test Action Group, refers to a near-universally imple-
mented industry standard serial interface for low-level debug access to device and
chip hardware.
for well over twenty-five years. While it is well outside the scope
of this paper to provide a comprehensive review of the history,
research efforts, and policy debates that lead up to this point, it is
valuable to briefly touch on aspects of each to provide context for
readers new to the domain and to understand the concerns that
motivate this work.5
2.1 History
The modern conflict that motivates this paper is around court-
ordered device access—an issue that was highlighted in the widely
publicized San Bernardino Apple iPhone case. However, even
though device access is a distinct issue, there is significant historic
context—largely focused on real-time interception capabilities—that
inform the debate and help explain the distrust and polarization
that exists today.
CALEA & Clipper. In the early 1990s, the transition to digital com-
munication technology created technical challenges for the gov-
ernment’s existing ability to intercept voice calls. This transition
drove two policy efforts that set the stage for much of the modern
encryption debate: CALEA and Clipper.
The first challenge arose from the deregulation of the U.S. tele-
phone system and subsequent rapid innovation in digital com-
munications technology. The law enforcement community was
concerned that its court-authorized ability to intercept phone com-
munications might be imperiled as a result. In response, Congress
introduced the Communications Assistance for Law Enforcement
Act (CALEA) which mandated that telecommunications carriers
and their equipment manufacturers be responsible for providing
the technical capability to support voice wiretap requests. While
civil liberties and technology groups opposed the measure, there
was insufficient political will to stop it and CALEA passed the U.S.
Congress on voice votes and was signed into law by President
Clinton in 1994.
The second challenge arose, at almost the same time, over the
use of cryptography over such digital links. Ostensibly concerned
about criminal use of such technology, the U.S. proposed that tele-
phony equipment suppliers incorporate the NSA-designed Clipper
chip, which would implement the encryption, but would embed
an encrypted version of each session key in the data stream. Be-
cause the per-device keys used to encrypt these session keys were
escrowed with the government, law enforcement would be able to
later decode any encrypted session if necessary.6 Unlike CALEA, a
strong political backlash emerged against government mandates on
cryptographic algorithms as well as around the state’s escrowing of
secret keys in such systems. In the end, the Clipper approach was
abandoned and export laws liberalized to allow the international
distribution of strong cryptography in U.S. products and services.
The Clipper chip debacle became a touchstone narrative for
describing heavy-handed government actions around encryption
and CALEA would be an ongoing battleground going forward.7
5Note that this account is decidedly U.S.-centric, both in its accounting of history and
its discussion of law.
6At the time, the U.S. imposed tight export controls on cryptography, and thus the
government hoped that removing export licensing requirements for such devices
would make them attractive to the telecommunications sector.
7The original bill focused exclusively on the telephone network. However, subsequent
rulemaking expanded that remit to include IP-based broadband providers, as well as
Together, these conflicts helped define the form of such debates and
the language employed to this day.8
Going Dark. Over a decade later (2008), then-FBI Director Robert
Mueller and his staff started briefing legislators on what he called
the “Going Dark” problem [1]. At the time, he sought amendments
to CALEA to mandate real-time interception capability for a range
of Internet communications applications (e.g., Skype, Facetime,
Google Hangouts, etc.) These efforts were unsuccessful, but both
subsequent FBI directors, Comey and Wray, continued to advocate
for further data access capability via the “going dark” narrative.
Over time, the nature of this narrative increasingly focused on
encryption (as opposed to service access) and that is the context it
is most widely associated with today.
Surveillance and Snowden. In 2005 and 2006, a series of newspaper
articles and a class-action lawsuit filed by the EFF made it clear that
the NSA had been collecting a broad range of data from U.S. telecom-
munications carriers [2, 30, 45, 52]. Concern over these findings
were further inflamed in 2013 when Booz Allen Hamilton contractor
Edward Snowden leaked a large trove of NSA documents to several
media confidants, leading to a broad range of disclosures concern-
ing U.S. intelligence collection operations. These disclosures further
undermined the relationship between technology companies and
the U.S. government around questions of data access. As well, the
resulting perceptions that U.S. technology companies might be sub-
ject to de facto surveillance created a competitive disadvantage for
those companies when selling products and services into foreign
markets. Finally, the extent of the government’s collection capabili-
ties and efforts sharpened concern from the research community
about the risks of mass surveillance [53].9
While the Snowden disclosures concerned the activities of the
intelligence community and not those of law enforcement (and
indeed, the two communities have broad differences in interest,
capabilities, procedures and focus), these distinctions were not
always widely appreciated and distrust in government motivations
accrued across all agencies during this time.
Conflict with Apple. While most of these prior conflicts focused
around real-time data interception, the rise of smartphones created
a new (and far more common) domain for digital evidence issues—
data stored on personal devices.
As with most large tech companies, Apple has long operated a
group to respond to law enforcement requests under legal process.
Among the services they have provided (and provide to this day)