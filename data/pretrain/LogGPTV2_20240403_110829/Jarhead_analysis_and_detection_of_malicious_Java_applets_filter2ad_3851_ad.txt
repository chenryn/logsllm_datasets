visiting a web page to mine bitcoins [29]. While we had not
seen such behavior before, Jarhead correctly classiﬁes this
applet as malicious.
The remaining eight samples were properly classiﬁed by
Virustotal as benign, and hence, false positives for Jarhead.
Seven of our eight false positives had the potential to down-
load ﬁles to the user’s disk, and ﬁve of these seven would
even execute these ﬁles after downloading! Except for in-
tended use of these applets as software installers for poten-
tially benign programs, their behavior (and code patterns)
are essentially identical to malware. The last false positive
was a (likely benign) MIDlet that can send text messages to
arbitrary numbers.
We then inspected the 59 applets that Virustotal labeled
as malicious (while Jarhead labeled them as benign). Four
programs were partial exploits that did not implement ac-
tual malicious behavior. The remaining nine were false posi-
tives by Virustotal. We do not consider these partial exploits
(which are essentially incompletely packed archives) and the
nine benign programs to be properly labeled by Virustotal.
The remaining 46 samples were actual malware. They
were largely made up of two families of exploits for which
we had no samples in our (manual) training set, which we
used to build the classiﬁer. 28 samples belonged to a family
of MIDlet scam applets, and another 15 belonged to a new,
heavily obfuscated version of a new vulnerability (CVE-
2011-3544) that was not present in our training set. We
255
also missed one instance of CVE-2009-3869 [4] (discussed
previously) and one other malicious MIDlet. Moreover, one
exploit used a new method to introduce code that was not
in our training set.
Of the 46 false negatives, we missed 44 samples (or 96%)
because of limitations with the manual dataset used to train
our classiﬁer. While we were collecting features useful for
identifying these malicious applets, our classiﬁer did not
learn that they were important, because it was missing sam-
ples triggering these features in its training set. To show that
we can achieve better results with a better training set, and
to demonstrate that our features are indeed well-selected
and robust, we trained and tested a new classiﬁer on the
Wepawet dataset using ten-fold cross validation. For that
experiment, we found a total misclassiﬁcation count of 21
(1.6%), the total false positive rate was 0.9% (12 applets),
and the false negative rate 0.7% (9 applets).
The results for the Wepawet dataset are presented in Ta-
ble 3.
Original classiﬁer
10x cross validated
False positives
False negatives
2.1%
4.6%
0.9%
0.7%
Table 3: Jarhead’s performance on the Wepawet
dataset.
We also collected performance numbers during our ex-
periments. On average our analysis takes 2.8 seconds per
sample, with a median of 0.6 seconds. This shows that the
majority of samples is very fast to analyze, although there is
a narrow longtail for which the analysis takes longer (specif-
ically, 2% of the samples take longer than 10 seconds and
0.3% took longer than a minute). For the slower 50% of the
samples, more than 98% of the total running time was spent
in the disassembler. Thus, these numbers could be signiﬁ-
cantly improved simply by a more eﬃcient implementation
of the disassembler or by implementing our feature collection
directly on the bytecode so disassembling the code becomes
unnecessary.
6. POSSIBLE EVASION
We have seen that Jarhead performs well on real-world
data. In this section, we will discuss the limitations of Jar-
head, i.e., possible ways for malicious applets to avoid de-
tection by our system.
A lot of the usual limitations for static analysis [28, 25,
26] do not apply to Java bytecode. However, for a trusted
applet, it is possible to use the Java native interface (JNI)
to execute native code on the machine. This is not covered
by our analysis.
If the parts of the malware that are im-
plementing the malicious behavior are in the Java bytecode
parts of the applet, it is likely that we will detect them, oth-
erwise, there exist many analysis tools for native malware
that would be able to detect such malicious behavior.
Static analysis is also limited by the use of reﬂection in
a language. Interestingly, we found that reﬂection is not in
widespread use by benign Java applets. Malicious applets,
however, use it in an attempt to evade systems such as ours.
While we do not completely mitigate this problem, we have
features that aim to precisely detect this kind of evasion.
Moreover, other features that target the Jar ﬁle and its code
as a whole, such as code metrics and Jar content features,
are unaﬀected by reﬂection.
We examine each applet individually. Applets on the
same web page can communicate with each other, by call-
ing each others public methods. Applets can also be con-
trolled from the surrounding JavaScript in a similar fashion.
If malicious behavior is distributed among multiple applets
within a single page, or partly carried out by the surround-
ing JavaScript, our analysis scope is too limited, and we
might misclassify these applets. Fortunately, to the best
of our knowledge, malicious applets that use JNI and ma-
licious code splitting behavior between multiple applets (or
interacting with the surrounding JavaScript) do currently
not exist in the wild. Moreover, we can extend our analysis
to consider multiple applets that appear on the same page
together. We already combine all class ﬁles within a single
Jar archive, so such an extension would be rather straight-
forward.
While we have shown that today’s malicious applets are
very well covered by our features, a completely new class of
exploits or vulnerabilities could bypass our detection either
because we do not collect good features to capture the new
exploit or because the classiﬁer was unable to learn this ex-
ploit pattern from the training set. In these cases, it might
be necessary to add new features or extend the set of known
vulnerable functions. This would be straightforward to do.
In other cases, simply retraining the classiﬁer on a dataset
containing the new exploits might suﬃce.
Since we operate on the Java bytecode, identifying vul-
nerabilities in the underlying native implementation of the
JVM itself (such as CVE-2009-3869 [4]) is diﬃcult. The
reason is that corresponding exploits target a heap overﬂow
vulnerability by displaying a specially crafted image. The
set of possible functions within the Java API that can lead
to execution of the vulnerable function is very large, and
the API functions are widely used. Moreover, there is not
obvious malicious activity present in the Java class ﬁle when
this vulnerability is triggered.
7. RELATED WORK
A lot of research has been done to detect malware. In this
section, we present diﬀerent approaches and compare them
to Jarhead.
Signature-based approaches [33, 2] ﬁnd malware by match-
ing it against previously selected code or data snippets spe-
ciﬁc to a certain exploit. Signature-based detection sys-
tems can be evaded by obfuscation, and cannot catch ex-
ploits they do not have signatures for. Jarhead complements
signature-based techniques by identifying malicious samples
based speciﬁcally on their obfuscation and behavior features.
Jarhead is also able to detect previously-unknown families
of exploits (since it uses anomaly detection).
A broad range of low and high interactive honeyclients
were proposed to identify malware [30, 10, 11]. They can-
not detect malware that targets vulnerable components that
are not installed on the honeyclient. Speciﬁcally for Java
applets, this means that the honeyclients need to have the
correct version of the Java plugin installed, running in the
correct conﬁguration with the correct browser. Jarhead is
able to detect malicious applets independent of this envi-
ronment by using static analysis. Furthermore, the runtime
environment of honeyclients can be ﬁngerprinted by malware
as part of evasion attempts. Since Jarhead relies purely on
256
static analysis, ﬁngerprinting the analysis system is not pos-
sible.
Twelve years ago, Helmer suggested an intrusion detec-
tion system aimed at identifying hostile Java applets [18].
Their system is geared towards the detection of applets an-
noying the user, rather than real malicious ones as we see
today. Their approach is also based on machine learning
combined with anomaly detection, but the features are very
diﬀerent.
In particular, their system monitors the system
call patterns emitted from the Java runtime system during
applet execution. The system has not been tested on real
malicious applets and requires dynamic execution, exposing
it to similar problems as honeyclients. Jarhead has been
tested on a large real-world dataset of modern, malicious
applets, and it is not subject to the limitations that come
with dynamic malware execution.
8. CONCLUSIONS
We address the quickly growing problem of malicious Java
applets by building a detection system based on static anal-
ysis and machine learning. We implemented our approach
in a tool called Jarhead and tested it on real-world data.
We also deployed our system as a plugin for the Wepawet
system, which is publicly accessible. Our tool is robust to
evasion, and the evaluation has demonstrated that it oper-
ates with high accuracy.
In the future, we plan to improve our results by using
more sophisticated static analysis techniques to achieve even
higher accuracy. For example, we would like to use program
slicing [20] to statically determine whether a downloaded
ﬁle is indeed the one that is executed later in the program
or whether a suspicious image ﬁle is actually passed to a
vulnerable function.
9. ACKNOWLEDGMENTS
This work was supported by the Oﬃce of Naval Research
(ONR) under Grant N000140911042, by the National Sci-
ence Foundation (NSF) under grants CNS-0845559 and CNS-
0905537, and by Secure Business Austria.
10. REFERENCES
[1] Capture hpc. http://nz-honeynet.org.
[2] Clamav. http://www.clamav.net.
[3] CVE-2009-3867. National Vulnerability Database.
[4] CVE-2009-3869. National Vulnerability Database.
[5] CVE-2010-0094. National Vulnerability Database.
[6] CVE-2010-0842. National Vulnerability Database.
[7] CVE-2012-0507. National Vulnerability Database.
[8] Wepawet. http://wepawet.iseclab.org.
[9] Same origin policy.
http://www.w3.org/Security/wiki/Same Origin Policy,
2010.
[10] Yaser Alosefer and Omer Rana. Honeyware: A
web-based low interaction client honeypot. ICSTW
’10, 2010.
[11] Marco Cova, Christopher Kruegel, and Giovanni
Vigna. Detection and analysis of drive-by-download
attacks and malicious javascript code. In World-wide
web conference (WWW), 2010.
[12] Manuel Egele, Peter Wurzinger, Christopher Kruegel,
and Engin Kirda. Defending browsers against drive-by
downloads : mitigating heap-spraying code injection
attacks. In DIMVA’09, 2009.
[13] Sean Ford, Marco Cova, Christopher Kruegel, and
Giovanni Vigna. Analyzing and Detecting Malicious
Flash Advertisements. In Annual Computer Security
Applications Conference (ACSAC), 2009.
[14] Y. Fratantonio, C. Kruegel, and G. Vigna. Shellzer: a
tool for the dynamic analysis of malicious shellcode. In
Proceedings of the Symposium on Recent Advances in
Intrusion Detection (RAID).
[15] Mike Geide. 300% increase in malicious jars.
http://research.zscaler.com/2010/05/300-increase-in-
malicious-jars.html,
2010.
[16] Li Gong and Marianne Mueller e.a. Going beyond the
sandbox: An overview of the new security architecture
in the java development kit 1.2. USITS, 1997.
[17] Hall and Mark e.a. The weka data mining software: an
update. SIGKDD Explor. Newsl., 11(1), 2009.
[18] Guy G. Helmer and Johnny S. Wong e.a. Anomalous
intrusion detection system for hostile java applets.
Journal of Systems and Software, 55(3), 2001.
[19] Stefanie Hoﬀman. Microsoft warns of unprecedented
rise in java exploits.
http://www.crn.com/news/security/227900317/microsoft-
warns-of-unprecedented-rise-in-java-exploits.htm,
2010.
[20] Susan Horwitz and Thomas Reps e.a. Interprocedural
slicing using dependence graphs. ACM Transactions
on Programming Languages and Systems, 12, 1990.
[21] Wolfgang Kandek. The inconvenient truth about the
state of browser security.
http://laws.qualys.com/SPO1-204 Kandek.pdf, 2011.
[22] Brian Krebs. Java: A gift to exploit pack makers.
http://krebsonsecurity.com/2010/10/java-a-gift-to-
exploit-pack-makers,
2010.
[23] Brian Krebs. Exploit packs run on java juice.
http://krebsonsecurity.com/2011/01/exploit-packs-
run-on-java-juice/,
2011.
[24] Tim Lindholm and Frank Yellin. Java Virtual Machine
Speciﬁcation. Addison-Wesley Longman Publishing
Co., Inc., Boston, MA, USA, 2nd edition, 1999.
[25] Cullen Linn and Saumya Debray. Obfuscation of
executable code to improve resistance to static
disassembly. In Proceedings of the 10th ACM
conference on Computer and communications security,
CCS ’03, 2003.
[26] Douglas Low. Java control ﬂow obfuscation. Technical
report, 1998.
[27] Thomas J. McCabe. A complexity measure. IEEE
Trans. Software Eng., 2(4), 1976.
[28] Andreas Moser and Christopher Kruegel e.a. Limits of
static analysis for malware detection. In ACSAC, 2007.
[29] Satoshi Nakamoto. Bitcoin: A Peer-to-Peer Electronic
Cash System. 2008.
[30] Jose Nazario. Phoneyc: a virtual client honeypot.
LEET’09, 2009.
[31] Niels Provos and McNamee e.a. The ghost in the
browser analysis of web-based malware. In Proceedings
257
of the ﬁrst conference on First Workshop on Hot
Topics in Understanding Botnets, HotBots’07, 2007.
[32] Niels Provos and Panayiotis Mavrommatis e.a. All
your iframes point to us. Google Inc, 2008.
[33] Martin Roesch. Snort - lightweight intrusion detection
for networks. LISA ’99, 1999.
[34] Christian Seifert and Ian Welch e. a. Identiﬁcation of
malicious web pages through analysis of underlying
dns and web server relationships. In LCN, 2008.
258