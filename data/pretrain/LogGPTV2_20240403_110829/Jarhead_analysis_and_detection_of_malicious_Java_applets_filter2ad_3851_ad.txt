### Visiting a Web Page to Mine Bitcoins

While we had not previously observed such behavior, Jarhead correctly classified the applet as malicious. The remaining eight samples were classified by VirusTotal as benign, but Jarhead flagged them as false positives. Seven of these eight false positives had the potential to download files to the user's disk, and five of these seven would execute the downloaded files. Apart from their intended use as software installers for potentially benign programs, their behavior and code patterns closely resemble those of malware. The last false positive was a (likely benign) MIDlet capable of sending text messages to arbitrary numbers.

### Analysis of Malicious Applets

We then examined the 59 applets that VirusTotal labeled as malicious, while Jarhead classified them as benign. Four of these were partial exploits that did not implement actual malicious behavior. Nine others were false positives by VirusTotal. We do not consider these partial exploits (essentially incompletely packed archives) and the nine benign programs to be correctly labeled by VirusTotal. The remaining 46 samples were indeed malware. These primarily consisted of two families of exploits for which we had no samples in our training set. Twenty-eight samples belonged to a family of MIDlet scam applets, and another 15 were a new, heavily obfuscated version of a vulnerability (CVE-2011-3544) not present in our training set. Additionally, we missed one instance of CVE-2009-3869 and one other malicious MIDlet. One exploit used a novel method to introduce code that was not included in our training set.

Of the 46 false negatives, 44 (96%) were due to limitations in the manual dataset used to train our classifier. While we collected useful features for identifying these malicious applets, the classifier did not learn their importance because it lacked samples triggering these features in its training set. To demonstrate that a better training set could yield improved results and to show the robustness of our selected features, we trained and tested a new classifier on the Wepawet dataset using ten-fold cross-validation. In this experiment, the total misclassification count was 21 (1.6%), with a false positive rate of 0.9% (12 applets) and a false negative rate of 0.7% (9 applets).

### Performance Metrics

The performance of Jarhead on the Wepawet dataset is summarized in Table 3.

| Metric                | Original Classifier | 10x Cross-Validated |
|-----------------------|---------------------|---------------------|
| False Positives       | 2.1%                | 0.9%                |
| False Negatives       | 4.6%                | 0.7%                |

### Performance Analysis

During our experiments, we also collected performance metrics. On average, our analysis took 2.8 seconds per sample, with a median of 0.6 seconds. This indicates that the majority of samples are analyzed quickly, although there is a small long-tail where analysis takes longer (specifically, 2% of the samples take longer than 10 seconds and 0.3% take longer than a minute). For the slower 50% of the samples, more than 98% of the total running time was spent in the disassembler. Therefore, these numbers could be significantly improved by a more efficient implementation of the disassembler or by directly implementing feature collection on the bytecode, eliminating the need for disassembly.

### Possible Evasion Techniques

While Jarhead performs well on real-world data, there are limitations and potential evasion techniques. Many common static analysis limitations [28, 25, 26] do not apply to Java bytecode. However, for trusted applets, the Java Native Interface (JNI) can be used to execute native code on the machine, which is not covered by our analysis. If the malicious behavior is implemented in the Java bytecode, it is likely to be detected; otherwise, there are many tools available for analyzing native malware.

Static analysis is also limited by the use of reflection in a language. Interestingly, we found that reflection is not widely used by benign Java applets but is often employed by malicious ones to evade detection. While we do not completely mitigate this problem, we have features designed to detect such evasion. Additionally, other features targeting the JAR file and its code as a whole, such as code metrics and JAR content features, are unaffected by reflection.

We analyze each applet individually. Applets on the same web page can communicate with each other by calling each other's public methods and can be controlled by surrounding JavaScript. If malicious behavior is distributed among multiple applets within a single page or partly carried out by JavaScript, our analysis scope may be too limited, leading to misclassification. Fortunately, to the best of our knowledge, such malicious applets do not currently exist in the wild. Moreover, we can extend our analysis to consider multiple applets on the same page, which would be a straightforward extension since we already combine all class files within a single JAR archive.

While our features cover today's malicious applets well, a completely new class of exploits or vulnerabilities could bypass our detection if we do not collect good features to capture the new exploit or if the classifier cannot learn the new pattern from the training set. In such cases, adding new features or extending the set of known vulnerable functions might be necessary. Alternatively, retraining the classifier on a dataset containing the new exploits might suffice.

Since we operate on Java bytecode, identifying vulnerabilities in the underlying native implementation of the JVM (such as CVE-2009-3869) is challenging. Exploits targeting heap overflow vulnerabilities by displaying specially crafted images are difficult to detect because the set of possible Java API functions leading to the vulnerable function is large and widely used. Moreover, there is no obvious malicious activity in the Java class file when this vulnerability is triggered.

### Related Work

Much research has been conducted on detecting malware. Here, we compare different approaches to Jarhead.

**Signature-based approaches** [33, 2] detect malware by matching it against previously selected code or data snippets specific to certain exploits. These systems can be evaded through obfuscation and cannot catch unknown exploits. Jarhead complements signature-based techniques by identifying malicious samples based on their obfuscation and behavior features, and it can detect previously unknown families of exploits using anomaly detection.

**Honeyclients** [30, 10, 11] are low and high interactive systems designed to identify malware. They cannot detect malware targeting components not installed on the honeyclient. For Java applets, this means the honeyclient must have the correct Java plugin version, configuration, and browser. Jarhead, using static analysis, can detect malicious applets independent of the environment. Additionally, honeyclients can be fingerprinted by malware, but Jarhead, relying solely on static analysis, avoids this issue.

**Helmer's Intrusion Detection System** [18], proposed twelve years ago, aimed to identify annoying Java applets rather than truly malicious ones. Their approach, based on machine learning and anomaly detection, monitored system call patterns emitted during applet execution. It has not been tested on real malicious applets and requires dynamic execution, making it subject to similar problems as honeyclients. Jarhead, tested on a large real-world dataset of modern, malicious applets, does not have these limitations.

### Conclusions

We address the growing problem of malicious Java applets by building a detection system based on static analysis and machine learning. Our tool, Jarhead, has been tested on real-world data and deployed as a plugin for the Wepawet system, which is publicly accessible. Jarhead is robust to evasion and operates with high accuracy.

In the future, we plan to improve our results by using more sophisticated static analysis techniques, such as program slicing [20], to achieve even higher accuracy. For example, we aim to statically determine whether a downloaded file is executed later in the program or whether a suspicious image file is passed to a vulnerable function.

### Acknowledgments

This work was supported by the Office of Naval Research (ONR) under Grant N000140911042, the National Science Foundation (NSF) under grants CNS-0845559 and CNS-0905537, and Secure Business Austria.

### References

[1] Capture hpc. http://nz-honeynet.org.
[2] Clamav. http://www.clamav.net.
[3] CVE-2009-3867. National Vulnerability Database.
[4] CVE-2009-3869. National Vulnerability Database.
[5] CVE-2010-0094. National Vulnerability Database.
[6] CVE-2010-0845. National Vulnerability Database.
[7] CVE-2012-0507. National Vulnerability Database.
[8] Wepawet. http://wepawet.iseclab.org.
[9] Same origin policy. http://www.w3.org/Security/wiki/Same_Origin_Policy, 2010.
[10] Yaser Alosefer and Omer Rana. Honeyware: A web-based low interaction client honeypot. ICSTW '10, 2010.
[11] Marco Cova, Christopher Kruegel, and Giovanni Vigna. Detection and analysis of drive-by-download attacks and malicious JavaScript code. In World-wide web conference (WWW), 2010.
[12] Manuel Egele, Peter Wurzinger, Christopher Kruegel, and Engin Kirda. Defending browsers against drive-by downloads: mitigating heap-spraying code injection attacks. In DIMVA'09, 2009.
[13] Sean Ford, Marco Cova, Christopher Kruegel, and Giovanni Vigna. Analyzing and Detecting Malicious Flash Advertisements. In Annual Computer Security Applications Conference (ACSAC), 2009.
[14] Y. Fratantonio, C. Kruegel, and G. Vigna. Shellzer: a tool for the dynamic analysis of malicious shellcode. In Proceedings of the Symposium on Recent Advances in Intrusion Detection (RAID).
[15] Mike Geide. 300% increase in malicious jars. http://research.zscaler.com/2010/05/300-increase-in-malicious-jars.html, 2010.
[16] Li Gong and Marianne Mueller e.a. Going beyond the sandbox: An overview of the new security architecture in the Java Development Kit 1.2. USITS, 1997.
[17] Hall and Mark e.a. The WEKA data mining software: an update. SIGKDD Explor. Newsl., 11(1), 2009.
[18] Guy G. Helmer and Johnny S. Wong e.a. Anomalous intrusion detection system for hostile Java applets. Journal of Systems and Software, 55(3), 2001.
[19] Stefanie Hoffman. Microsoft warns of unprecedented rise in Java exploits. http://www.crn.com/news/security/227900317/microsoft-warns-of-unprecedented-rise-in-java-exploits.htm, 2010.
[20] Susan Horwitz and Thomas Reps e.a. Interprocedural slicing using dependence graphs. ACM Transactions on Programming Languages and Systems, 12, 1990.
[21] Wolfgang Kandek. The inconvenient truth about the state of browser security. http://laws.qualys.com/SPO1-204 Kandek.pdf, 2011.
[22] Brian Krebs. Java: A gift to exploit pack makers. http://krebsonsecurity.com/2010/10/java-a-gift-to-exploit-pack-makers, 2010.
[23] Brian Krebs. Exploit packs run on Java juice. http://krebsonsecurity.com/2011/01/exploit-packs-run-on-java-juice/, 2011.
[24] Tim Lindholm and Frank Yellin. Java Virtual Machine Specification. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 2nd edition, 1999.
[25] Cullen Linn and Saumya Debray. Obfuscation of executable code to improve resistance to static disassembly. In Proceedings of the 10th ACM conference on Computer and communications security, CCS '03, 2003.
[26] Douglas Low. Java control flow obfuscation. Technical report, 1998.
[27] Thomas J. McCabe. A complexity measure. IEEE Trans. Software Eng., 2(4), 1976.
[28] Andreas Moser and Christopher Kruegel e.a. Limits of static analysis for malware detection. In ACSAC, 2007.
[29] Satoshi Nakamoto. Bitcoin: A Peer-to-Peer Electronic Cash System. 2008.
[30] Jose Nazario. Phoneyc: a virtual client honeypot. LEET'09, 2009.
[31] Niels Provos and McNamee e.a. The ghost in the browser: analysis of web-based malware. In Proceedings of the first conference on First Workshop on Hot Topics in Understanding Botnets, HotBots'07, 2007.
[32] Niels Provos and Panayiotis Mavrommatis e.a. All your iframes point to us. Google Inc, 2008.
[33] Martin Roesch. Snort - lightweight intrusion detection for networks. LISA '99, 1999.
[34] Christian Seifert and Ian Welch e.a. Identification of malicious web pages through analysis of underlying DNS and web server relationships. In LCN, 2008.