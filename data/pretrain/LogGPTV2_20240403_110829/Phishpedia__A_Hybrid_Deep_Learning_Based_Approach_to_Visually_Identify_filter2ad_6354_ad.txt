### 5.5.1 Methodology

In this experiment, we employ the Triplet loss function [65] to train our model. For Op4, we replace the Siamese model with a standard perceptual hashing algorithm as implemented in [8].

### 5.5.2 Results

Table 4 summarizes the experimental results for the different technical options. Overall, Yolov3 demonstrates good identification accuracy, although it misses a significant number of phishing webpages. Additionally, two-stage training for the Siamese model improves recall compared to one-stage training. However, training the Siamese model in a conventional manner has an adverse effect on overall performance. Finally, the perceptual hashing algorithm is less effective than the Siamese model due to its inflexibility in handling minor changes in logos. Therefore, Phishpedia's approach to logo recognition and comparison is deemed sound.

| **Model** | **Technical Option** | **Identification Rate** | **Precision** | **Recall** | **Prediction Time (s)** |
|-----------|----------------------|------------------------|---------------|------------|-------------------------|
| ResNet Grayscale | Base | 99.68% | 96.59% | 99.63% | 0.19 |
| ResNet RGB | Op1 | 99.13% | 96.33% | 99.29% | 0.20 |
| ResNetV2 Grayscale | Op2 | 88.67% | 63.92% | 81.07% | 0.19 |
| ResNetV2 RGB | Op3 | 90.89% | 88.61% | 78.57% | 0.19 |
| Perceptual Hashing | Op4 | 24.58% | 59.03% | 79.37% | 0.10 |

### 5.6 Adversarial Defense (RQ5)

#### 5.6.1 Experiment on Gradient-based Techniques

We apply state-of-the-art adversarial attacks on both the object detection model and the Siamese model to (i) analyze Phishpedia's robustness against such attacks and (ii) evaluate the impact of adversarial defense techniques on Phishpedia's performance.

For the object detection model, we use the DAG adversarial attack [78] on a test set of approximately 1,600 screenshots. For the Siamese model, we select four adversarial attacks: DeepFool [48], i-FGSM [25], i-StepLL [34], and JSMA [24]. These attacks are applied to 1,000 labeled logos to assess the model's ability to match them against the target brand list. The attack iteration limit is set to 100, with learning rates of 0.5 for DAG and 0.05 for i-FGSM and i-StepLL (DeepFool and JSMA do not use a learning rate).

Table 5 shows the effect of adversarial attacks on the object detection model, comparing the original and transformed models' prediction accuracy. Table 6 reports the results of adversarial attacks on the Siamese model. The logo match accuracy is computed as \( \frac{k}{N} \), where \( k \) is the number of correctly matched logos out of \( N \) (1,000) logos.

| **Defense** | **Original Accuracy (mAP)** | **Transformed Accuracy (mAP)** |
|-------------|-----------------------------|---------------------------------|
| Without Attack | 59.6 | 58.9 |
| After Applying Adversarial Attack (DAG) | 12.9 (-46.7) | 58.7 (-0.02) |

| **Defense** | **Logo Match Accuracy without Attack** | **Logo Match Accuracy after Adversarial Attacks** |
|-------------|----------------------------------------|---------------------------------------------------|
| Original | 93.5% | 93.5% |
| Transformed | 93.5% | 93.5% |
| i-FGSM | 0.0% | 93.5% |
| i-StepLL | 0.1% | 93.5% |
| JSMA | 0.1% | 93.5% |
| DeepFool | 80.9% | 93.5% |

Our defense technique effectively mitigates existing state-of-the-art adversarial attacks, and Phishpedia's accuracy remains well-preserved.

#### 5.6.2 Experiment with Gradient-recovering Technique

While our gradient-masking approach is effective against popular gradient-based attacks, some adversarial attacks can recover gradients to facilitate the attack. We use BPDA (Backward Pass Differentiable Approximation) [12] to attack Phishpedia. BPDA assumes known gradient-masked layers and recovers the gradient through estimation.

Assuming the masked layers are known, we conduct attacks on Phishpedia's Siamese model with varying numbers of masked layers. Table 7 compares the model accuracy before and after the attacks. BPDA is effective for a small number of masked layers but less so for a large number, as increasing the estimated layers introduces more bias in the recovered gradients.

| **#Masked Layers** | **Accuracy before Attack** | **Accuracy after Attack** |
|--------------------|----------------------------|---------------------------|
| 3 | 93.5% | 93.5% |
| 7 | 93.6% | 90.5% |
| 13 | 93.6% | 92.3% |
| 17 (all) | 93.6% | 92.6% |

### 6. Phishing Discovery in the Wild (RQ6)

We design a phishing discovery experiment to compare Phishpedia with five other phishing detection/identification solutions in their effectiveness in detecting new phishing pages on the Internet.

#### 6.1 CertStream Service

We use the CertStream service [2], which provides new domains registered from the Certificate Transparency Log Network. This service helps monitor and audit the issuance of TLS/SSL certificates for new domains. In this study, we use CertStream to retrieve emerging new domains.

#### 6.2 Phishing Discovery Experiment

By integrating the reported emerging new domains with a phishing detector or identifier, we construct a phishing locator. We apply Phishpedia to scan and identify phishing webpages from the reported domains daily. We select five known approaches to evaluate their performance over 30 days (from September 10 to October 9, 2020). We record the landing URL and screenshot of each URL for post-mortem analysis. Each solution is configured to report a different number of phishing pages, and we manually investigate the top-reported phishing webpages. The number of samples picked for each solution is given in Table 9. Each reported phishing webpage is evaluated by two independent examiners, and disagreements are resolved through discussion. We also use VirusTotal [9] to check if it reports the same results. If a real phishing webpage is reported by a specific solution but not by any VirusTotal engine on the same day, it is considered a zero-day phishing webpage.

| **Tool** | **Category** | **#Reported Phishing** | **#Top Ranked Samples** |
|----------|--------------|------------------------|-------------------------|
| PhishCatcher | Detection | 1,421,323 | 5 |
| URLNet | Detection | 422,093 | 13 |
| StackModel | Detection | 327,894 | 9 |
| EMD | Identification | 299,082 | 3 |
| Phishzoo | Identification | 9,127 | 1,000 |
| Phishpedia | Identification | 1,820 | 1,000 |

#### 6.3 Baselines

We select baselines covering phishing detectors and identifiers, as shown in Table 8. URLNet [36] and StackModel [80] are recent techniques that outperform other state-of-the-art detection methods. URLNet uses only the URL string, while StackModel uses both URL and HTML content. We also include PhishCatcher [5], an open-source version of the commercial product PhishFinder [6].

| **Tool** | **Category** | **Input** | **Description** |
|----------|--------------|-----------|-----------------|
| PhishCatcher | Detection | URL | A rule-based phishing detector comparing new domains with legitimate ones. |
| URLNet | Detection | URL | A CNN-based approach predicting on a given URL. |
| StackModel | Detection | URL+HTML | A tree-model consisting of multiple layers of random forest. |
| EMD | Identification | URL+Screenshot | See Section I. |
| Phishzoo | Identification | URL+Screenshot | See Section I. |
| Phishpedia | Identification | URL+Screenshot | See Section III. |

This structured and detailed presentation should make the text more clear, coherent, and professional.