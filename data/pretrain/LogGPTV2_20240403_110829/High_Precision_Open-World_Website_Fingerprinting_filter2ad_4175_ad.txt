For both distance-based POs,
the best distance to use
was always the Bi-XCor distance or the Wa-kNN distance.
This implies that they contained signiﬁcant information not
sufﬁciently incorporated in future attacks; for the case of
Bi-XCor, this is perhaps because it had achieved a compar-
atively low TPR. Appropriately, the distance for Bi-XCor
could not save itself from relatively poor precision; no other
distance could, either. This would suggest that Bi-XCor’s
distance is useful, but its classiﬁer is weak.
For the too-close PO, in all cases Mclose = 1 was optimal
for precision. Mclose = 1 means that the PO rejected the
classiﬁcation of any element that was closer to a different class
than the assumed class. Increasing Mclose increased both TPR
and FPR in a ratio that was not favorable for precision.
For the too-far PO, the optimal value for Mf ar was slightly
less than 1 for each of the above classiﬁers. A larger Mf ar
weakened the precision optimizer, but a smaller Mf ar may
cause TPR to drop too signiﬁcantly. We show the effect
of Mf ar on the lower bound of π1000 for Ha-kFP and
the Wa-kNN distance in Figure 4. Peak precision occurred
at Mf ar = 0.84. It makes sense that Mf ar > 1 would
be imprecise:
this represents a PO that would not reject
classiﬁcation even if the distance to its assumed class was
Fig. 4: Lower bound of π1000 for Pa-CUMUL with too-far
PO, using the Bi-XCor distance on 100x100+10000 elements
while varying Mf ar. The PO rejected any assumed class for
which the testing element was at least Mf ar times as distant
as the expected distance to that class.
greater than expected. There was almost no change in precision
beyond Mf ar = 2 (when the PO almost never rejected a
classiﬁcation). It is interesting to see that a wide range of
values for Mf ar gives similar results, suggesting the power of
the PO is not a consequence of parameter overﬁtting.
D. Ensemble PO
In ensemble learning, multiple classiﬁers simultaneously
classify the same testing element, and we decide the ﬁnal
class based on an aggregate of each classiﬁer’s individual
classiﬁcation. In adapting ensemble learning techniques for
OWF, we hypothesized that disagreements between different
classiﬁers could show a lack of conﬁdence. Therefore, we
should reject classiﬁcation when different classiﬁers output
different classiﬁcations.
We evaluate a simple bagging scheme for OWF. We trained
all classiﬁers except Ca-OSAD (as it could not be trained on
the full data set) separately on the same training set using 10-
fold classiﬁcation. Then, we take a subset of the classiﬁers and,
for each testing element, we ask each of them to determine the
assumed class. We rejected classiﬁcation whenever there was
no unanimous decision among all classiﬁers in the chosen
subset of classiﬁers. Therefore, the more classiﬁers there are,
the more conservative our classiﬁcations become.
We show the results for all 31 possible subsets of 5 classi-
ﬁers in Table VI, focusing on the minimum π1000 (Wilson
method). The black marker indicates which algorithms are
used. Filled black rows represent that the algorithm forms part
of the subset for that result. From top to bottom, they represent
Bi-XCor, Pa-SVM, Wa-kNN, Ha-kFP, and Pa-CUMUL.
Row i contains results for the use of i classiﬁers in ensemble.
 0 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0 0.5 1 1.5 2π1000MfarTABLE VI: Lower bound for π1000 when a subset of the ﬁve WF attacks are used in ensemble PO. The marker besides each
result indicates which of the ﬁve WF attacks are used (black bar = used, white bar = not used). From top to bottom, the bars
represent Bi-XCor, Pa-SVM, Wa-kNN, Ha-kFP, and Pa-CUMUL in order.
.001
.18
.633
.795
.823
.021
.20
.630
.764
.031
.118
.657
.793
.006
.225
.696
.845
.005
.329
.588
.771
For example, the second result in the third row represents
the use of Bi-XCor, Pa-SVM and Ha-kFP. All results here
exceed a recall of 0.2.
From Table VI, we see that the optimal precision is achieved
when all except Pa-SVM are used for π1000 ≥ 0.845. The best
single classiﬁer was Wa-kNN; the best two-classiﬁer ensemble
was to add Ha-kFP; the best three-classiﬁer ensemble was
to add Pa-SVM; but the best four-classiﬁer ensemble does
not include Pa-SVM, as it was too conservative and rejected
too many correct classiﬁcations. Using more classiﬁers was
more conservative and generally gave better precision, except
in the ﬁve-classiﬁer case where Pa-SVM rejected many correct
positive classiﬁcations without helping to reject incorrect ones.
To expand on the above bagging scheme, we tested several
other schemes including giving different weights to different
classiﬁers and changing the number of votes required to accept
a classiﬁcation (instead of requiring all votes to accept a
classiﬁcation). Even with an exhaustive search of optimal
parameters, none of these schemes outperformed the simple
bagging scheme in the optimal case, so we omit these results.
IV. DIFFERENT SCENARIOS FOR A PRECISE WF ATTACK
To go beyond the standard open-world scenario, we test
the effectiveness of a WF attack in several other scenarios
in this section. We introduce each scenario in the following
and analyze how high precision helps the classiﬁer tackle the
scenario.
A. Identifying a sensitive client
We want to know if the attacker could determine, after some
period of observing the client, whether or not the client has
a habit of visiting sensitive pages. This scenario simulates an
attacker who wants to learn about the client’s online behavior.
For example, the attacker may want to ﬁgure out the client’s
political afﬁliation, romantic status, or other demographics by
deciding if the client visits certain pages frequently. High
recall and precision are both advantageous for this scenario,
so we want to know if our preference for high precision helps
classiﬁers.
Let us deﬁne a sensitive client as one who visits sensitive
pages at a rate of b, and a non-sensitive client as one who does
not visit sensitive pages. The attacker faces a binary classiﬁ-
cation problem to determine if a client is one of the above. To
do so, the attacker observes each client for N page accesses,
performs WF classiﬁcation on their accesses, and gets some
.337
.675
.347
.703
.399
.619
.288
.686
.387
.689
Fig. 5:
Success rate of identifying a sensitive client for
Ha-kFP with and without conﬁdence-based PO, while vary-
ing b, the rate of sensitive page visits.
number x of sensitive page accesses. The attacker decides that
the client is a sensitive client if x ≥ Midentif y. Midentif y
is a parameter that controls the trade-off between identifying
sensitive clients correctly and mistaking non-sensitive clients
as sensitive clients.
We compare Ha-kFP with and without conﬁdence-based
PO in Figure 5. We vary the base rate between 0.001 and
0.02 and model an attacker who has observed 1000 page
accesses. Therefore, in the toughest b = 0.001 case, the client
has only visited one sensitive page. To distinguish between
sensitive and non-sensitive clients, we set Midentif y to be
equal to 1000· (b· (RT P + RW P ) + (1− b)· RF P )/2, i.e. the
mean of the expected number of positive classiﬁcations for the
sensitive client and the non-sensitive client. We can see that
the optimized attack is much more successful at identifying
sensitive clients when b is low, with a 63% chance of correct
identiﬁcation at b = 0.002, up to 99% at b = 0.01. Not only
does the original attack achieve less success at identifying
sensitive clients, it also more frequently falsely identiﬁes non-
sensitive clients as sensitive ones: the rate increases with lower
b, from 25% at b = 0.01 to 43% at b = 0.001 (not shown in
the graph). This rate cannot be reduced without also reducing
the true identiﬁcation rate. The optimized attack did not make
false positives.
B. Attacking defenses
A number of defenses against WF have been proposed
for anonymity technologies like Tor. Much like WF attacks,
these defenses are almost always evaluated with recall: a good
defense would be judged by its ability to decrease the recall
of all classiﬁers. We wanted to know the precision of our
 0 0.2 0.4 0.6 0.8 1 0 0.005 0.01 0.015 0.02Success ratebHa-kFP with POHa-kFP without POTABLE VII: Best 20-precision (π20) with conﬁdence-based
PO on Ha-kFP against three defenses: Random Padding,
Tamaraw and WTF-PAD. Bandwidth (B/W) and Time over-
head are also given.
Name
Random Padding
Tamaraw [2]
WTF-PAD [12]
π20
Original
.17 ± .01
.0088 ± .0004
.24 ± 0.01
π20
With PO
.86 ± .08
.036 ± .005
.96 ± .02
Overhead
B/W Time
50%
50%
91% 247%
0%
32%
optimized attacks on those defenses and whether or not their
precision could be signiﬁcantly deterred by defenses.
We evaluated three WF defenses: Random Padding set to
produce 50% bandwidth and time overhead by adding random
packets, Tamaraw by Cai et al. [2], and WTF-PAD by Juarez
et al. [12]. Bandwidth overhead is equal to the percentage of
extra packets added by the defense, and time overhead is equal
to the percentage of extra time required to load each web page.
the results in Table VII. For these results
only, we lower the minimum recall requirement from 0.2
to 0.02, because no attack can achieve a higher recall than
0.06 for Tamaraw. For WTF-PAD, we could only test on
100x100+10000 because it was very slow.
We present
We see that WTF-PAD is slightly less capable of reducing
the precision of an attacker compared to Random Padding,
and much less capable compared to Tamaraw, though it is
the cheapest in overhead. Even without any PO, attacking
WTF-PAD was relatively easy compared to Tamaraw, though
Tamaraw is far more expensive, especially in time overhead.
Recent work by Sirinam et al. has shown high TPR against
WTF-PAD [25]. With precision optimization, Ha-kFP can be
precise (for r = 20) on both WTF-PAD and Random Padding;
a ﬁve-fold jump in precision is possible on Tamaraw, though
the best attack is still highly imprecise. Tamaraw cannot be
defeated, though its incredible cost in overhead has hampered
its adoption.
We did not evaluate any “targeted defenses”: a targeted
defense allows the client to choose which page to mimic while
accessing speciﬁc pages. While targeted defenses give clients
the advantage of creating a speciﬁc cover story, rather than a
series of randomly perturbed packet sequences, currently there
is no known mechanism to automatically choose correct targets
to mimic. Wrongly chosen targets could signiﬁcantly impede
the ability of the defense to lower precision, so we cannot
evaluate them fairly. These defenses include Glove [17],
Walkie-Talkie [32], and Decoy pages [20].
C. Attacking different data sets
Like most other works on WF, we perform the main
evaluation of our work on a data set that consists of 100
monitored web pages chosen from the top pages. Here, we
consider whether variations in the data, corresponding to
realistic scenarios a WF attacker would want to tackle, would
change the precision of our optimized classiﬁers.
HTTP/1.1 and HTTP/2. Our data set consists of a mix
of HTTP/1.1 (30%) and HTTP/2 (70%). HTTP/2, the newer
version, changes how resources are loaded to encourage same-
stream parallelism. We separate our data set into two parts
for those two versions and compare their recall and precision
for Ha-kFP. Before optimization, recall on the two sets was
respectively 0.91 and 0.88, while 1000-precision was 0.023
and 0.027. After optimization, recall was 0.44 and 0.37, while
1000-precision was respectively > 0.85 and > 0.84. There is
almost no difference between precision on the two data sets,
while the HTTP/1.1 data set showed a slightly higher recall.
It is possible that the slight difference is due to HTTP/2, but
it is also possible that the nature of the web pages was simply
different.
Same-domain pages. We want to test if the attacker can
precisely separate web pages of the same type and domain.
We collect a new Wikipedia data set for this purpose: our 100
monitored classes are all Wikipedia pages concerning sensitive
topics, for which we collect 100 instances each, and we also
collect 10,000 non-monitored Wikipedia traces corresponding
to random walks of Wikipedia’s pages following links starting
from the main page. Details of data collection can be found
in the Appendix. This is a more difﬁcult task since the pages
are highly similar in size and structure.
Our results show that original Ha-kFP can achieve π20 =
0.09 and π1000 = 0.002 on this data set with a low TPR of
0.52, while conﬁdence-based optimized Ha-kFP can achieve
π20 = 0.28 and π1000 = 0.008 lowering TPR to 0.2, with
K = 3 and Mmatch = 0.28. Although we are not able
to achieve high precision in this challenging problem, our
methods do bring a three- to four-fold increase in precision.
The difﬁculty of the above task can be seen in the following
statistics: the mean difference in the number of cells between
a randomly chosen monitored instance and a non-monitored
one was 810 (compared to a mean size of 1720 cells), while
in the original data set it was 4260 (compared to a mean size
of 4120 cells). This reﬂects the fact that monitored and non-
monitored pages were much more similar in the Wikipedia
data set. The web pages were also smaller, thus leaking less
information to classify.
Different monitored pages. We chose the top 100 Alexa
pages as our monitored sensitive set chieﬂy to ensure repro-
ducibility as lesser ranked pages often had a shorter lifespan. If
the site is taken down, the results can no longer be reproduced.
To address questions as to whether high-precision attacks on
the top 100 Alexa pages would be reproducible on other
pages, we collect a different data set here corresponding to 100
randomly chosen pages from the top 100,000 instead, similarly
with 200 instances each, making up a 100x200+80000 data set
as before. We use Ha-kFP with conﬁdence-based PO, the best
optimized attack available, and achieve π1000 > 0.81 on this
data set, a slightly diminished but still highly precise result.
This shows that our methods are similarly precise on other
web pages.
TABLE VIII: TPR of each of the four classes and what
portion of the false positives each class contributed to (“FPR
contribution”). TPR was calculated for a modiﬁed data set with
these 4 new classes and 96 original classes.
Name
AJAX1
AJAX2
LINKS1
LINKS2