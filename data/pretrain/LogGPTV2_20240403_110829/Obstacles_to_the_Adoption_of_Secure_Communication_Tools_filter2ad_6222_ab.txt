in addition to usability issues,
incomplete threat models, misaligned incentives, and lack of
understanding of the email architecture are key drivers of the
non-adoption of E2E-encrypted email. They concluded that
security researchers should focus on building “comprehensive
mental models of email security”.
Das et al. recently studied the role of social inﬂuence on
users’ decisions to adopt secure tools [58] and to use speciﬁc
security features of a speciﬁc application (Facebook) [59],
[60]. De Luca et al. also investigated how and why users use
mobile instant messengers that are advertised as being secure
(e.g., Threema) [61]. They concluded that peer inﬂuence,
not security and privacy, primarily drives users to adopt a
messenger. The objective of our study is to explore the user
experience of secure communications in more depth, identify
“other” factors that lead to the adoption and abandonment
of communication tools, and understand how users perceive
the “security” of communication tools, especially of those
advertised as being secure.
It is worth to mention that Dourish et al. studied how users
experience and practice security using a qualitative approach
(semi-structured interviews analyzed using Grounded The-
ory [20]) in 2004 [62]. Similarly, we use a qualitative approach
to understand how users manage their communications, secure
or not, as an “everyday, practical problem”. We “zoom out”
to understand users’ security needs and practices, and the
background against which they decide to use or stop using
a communication tool. We also explore what users look for in
a secure communication tool.
We know that the decisions users make may not deliver
on their actual security requirements. The gaps in mental
models identiﬁed by Renaud et al. suggest that users may think
they are more secure than they are [57]. Similarly, the folk
models of home network security described by Wash led his
participants to believe that their practices were secure when
they were not [63]. Thus, we study users’ knowledge of the
threats to their communications, and their mental models of
the tools and practices they use to protect against these threats.
III. METHODOLOGY
In this section, we discuss our research questions, recruit-
interview procedure, data analysis, research
ment process,
ethics, and the limitations of our work.
A. Research Questions
In this work, we explore (1) why, when and how users use
secure communications (Section III-C1), (2) what threats users
want to protect against when communicating (Section III-C2),
(3) which communication tools users perceive to be secure (or
Gaw et al. explored the social context behind users’ deci-
sions about whether and when to encrypt emails [56]. They
interviewed members of an activist organization under the
presumption that the organization’s employees would have
a strong incentive to encrypt emails. They found that the
perception of encryption behaviour by others (e.g., use of
encryption for protecting secrets is seen as “justiﬁed”, for gen-
139
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:09 UTC from IEEE Xplore.  Restrictions apply. 
insecure) and why (Section III-C3), and (4) how users think
secure communications can be achieved, and how they can be
breached (Section III-C4).
B. Participants
Our literature review (see Section II) shows that mainstream
users’ needs and practices of secure communications have not
been investigated. Instead of focusing on a speciﬁc at-risk
population, such as activists, whistleblowers, or journalists,
our main focus is understanding the needs and practices of
users of communication tools who do not consider themselves
to be at risk of targeted surveillance. This is because our focus
of enquiry is widespread adoption of secure communications.
We recruited our participants via posting ﬂyers around
University College London’s buildings and emailing university
staff members. We also distributed emails to staff members
in collaborating public- and private-sector organizations (e.g.,
banks, hospitals, universities). We asked interested participants
to complete an online pre-screening questionnaire, which
380 completed. The full questionnaire can be found in the
Appendix. We assessed participants’ technical knowledge and
cyber-security threat exposure via a set of simple questions.
We also provided them with a list of different communication
tools (those evaluated by the EFF Secure Messaging Score-
card), asking them to select all the tools they currently use
and the ones they stopped using. Additionally, we gave our
participants the option to specify other tools they have used,
but were not on the list.
We then divided the pool of eligible participants into sub-
groups, based on a number of variables: age, gender, education
level, study area, employment status, technical knowledge, and
previous cyber-security threat exposure. We conducted and
analyzed 10 unstructured interviews ﬁrst, followed by 50 semi-
structured interviews. Tables 1 and 2 summarize the demo-
graphics of our recruited participants for both the unstructured
and semi-structured interview sessions, respectively1.
With 60 participants, our study represents the largest qual-
itative study on this topic. We interviewed 23 male and 35
female participants. Two participants preferred not to indicate
their gender. Participants’ ages ranged from 18 to 70. Two
participants did not have a formal educational qualiﬁcation,
seven completed high-school education, 30 had a college
degree (e.g., BA, BSc), and 21 had a higher degree (e.g., MA,
MSc, PhD). 40 were high-school and university students, 17
were employed, and three were retired. Our participants used
a wide range of communication tools on different computing
platforms (e.g., Android, iOS, Mac OS X, Microsoft Win-
dows). None of the participants used a PGP-based tool, such
as Enigmail, GPGTools or Gpg4win. Only P23 and P57 used
an OTR-based tool; both have adopted Pidgin for some time
and then stopped using it.
We note that P2, P5 and P28 identiﬁed themselves as secu-
rity experts, so they did not necessarily represent mainstream
users of communication tools.
1 Tables 1 and 2 can be accessed from the ﬁrst author’s webpage.
C. Interview Procedure
The value of conducting qualitative research lies in pro-
viding a holistic understanding of the phenomenon under
enquiry using predominantly subjective qualitative data, which
can be supplemented by observational and other quantitative
data [64]. A single trained researcher conducted all 60 in-
terview sessions in the UK in English, by ﬁrst conducting
10 unstructured (open-ended) face-to-face interviews, lasting
for 35 minutes on average. The emerging themes shaped the
design of the script used for the 50 semi-structured face-to-face
interviews, lasting for 90 minutes on average. The interviewer
allowed participants to elaborate, share their thoughts, and ask
any clariﬁcation questions. The interviewer also asked follow-
up questions (or probed) where appropriate. This is a common
practice in semi-structured interviews, in which the interviewer
primarily uses a list of questions, but has discretion to ask
follow-ups or skip questions that have already been covered.
However, all interviews covered the following four areas in
the same order. Below, we describe the script we used for the
semi-structured interviews.
1) Adoption of communication tools: We asked participants
to specify the communication tools they have used by giving
them the same list of tools provided during the pre-screening
stage. This allowed us to compare their answers with those in
the pre-screening questionnaire. Also, we asked them to take
out their mobile phones and check all the communication tools
they have installed.
For each tool currently used or previously used by our
participants, we asked why they decided to adopt it and why
they stopped using it (if they had). The given answers helped
us understand why speciﬁc tools were widely adopted and
others were not. The key questions were:
• Why did you decide to adopt [this communication tool]?
• What computer platforms does the tool run on?
• Who do you communicate with?
• What is the context of use?
• Do you describe yourself as a regular user of the tool?
• Have you ever checked and/or changed the default set-
tings of the tool? Please elaborate.
• What kind of information do you regard as “sensitive”?
• Have you ever sent sensitive information via a commu-
nication tool? If yes, why and how did you do so?
• Why did you decide to stop using [this communication
tool], if applicable?
2) How users deﬁned secure communications: “Securing” a
communication tool is meaningless without deﬁning a security
policy and a threat model. Many communication tools are
advertised as “secure” or “encrypted”, but a recent academic
survey suggested that many are not as secure as they claim
to be [1]. The link between users’ perceptions of secure
communications and the actual security offered by different
communication tools has not been investigated so far.
To address this gap, we asked our participants about the kind
of protection (or security properties) a secure communication
tool should provide, what they want to protect, with whom
140
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:09 UTC from IEEE Xplore.  Restrictions apply. 
they communicate, who the attackers (or adversaries) might
be, and what their capabilities are.
We also elicited participants’ mental models of how they
think secure communications work. Mental models are cogni-
tive representations of external reality that underpin people’s
cognition, reasoning, decision-making and behavior [65]. We
invited our participants to draw how a communication tool
works, and whether there is a distinction between calling
someone and sending them a text (or multimedia) message.
A message could be an SMS, an email or an instant message.
We provided our participants with an iPad and a stylus pen. We
also recorded and transcribed participants’ verbal commentary
while drawing, along with the rest of the interviews.
3) Security ranking of communication tools: We asked our
participants to rank the communication tools they have used in
terms of the security level each tool offers. We provided them
with cards with the names and logos of the tools they have
used, and asked them to sort the tools from the most to the
least secure. We used this card sorting exercise to compare our
participants’ rankings with those on the EFF Secure Messaging
Scorecard [2] and to elicit the rationale behind their rankings.
We also wanted to assess the effectiveness of the EFF
Scorecard in communicating which communication tool is
secure and why. After our participants ranked the tools and
described their reasoning, we showed them the scorecard
(printed on a sheet of paper) and gave them 10 minutes to
explore it, compare their rankings, and ask any clariﬁcation
questions they had.
4) Security properties and mechanisms: In the last part of
the study, we wanted to probe our participants’ understanding
of how a security property can be achieved and how it can
be violated. We also asked participants about several spe-
ciﬁc security mechanisms: encryption, digital signatures and
cryptographic ﬁngerprints. We wanted to check their broader
understanding to see whether they can interpret the criteria on
the EFF Scorecard correctly or not.
Finally, we debriefed our participants and gave them the
time to ask any clariﬁcation questions about the study.
D. Pilot Study
We conducted a pilot study of ﬁve semi-structured inter-
views to check that the questions could be understood and
identify any potential problems in the script (e.g., cost, time,
adverse events) in advance, so that the methodology could be
ﬁne-tuned before launching into the main study. We used the
common practice of convenience sampling [66] by selecting
ﬁve colleagues for the pilot study. In addition to the ﬁve
sessions, we asked six researchers to review the study.
E. Data Analysis
To develop depth in our exploratory research, we conducted
multiple rounds of interviews, punctuated with periods of
analysis and tentative conclusions [19]. In total, we conducted,
transcribed (using an external transcription service) and ana-
lyzed all 10 unstructured and 50 semi-structured interviews.
We observed data saturation [67] between the 40th and 45th
interview; i.e., no new themes emerged in interviews 46–50,
and, hence, we stopped recruiting. Data saturation provides
a high degree of conﬁdence that we observed the range of
reasons for adoption (or non-adoption) of secure communi-
cations. The audio-recordings of the interview sessions were
transcribed, and then independently coded by three researchers
using Grounded Theory analysis [20], [21], an inductive/open-
ended method to discover explanations, grounded in empirical
data, about how things work. After coding all interviews and
creating the ﬁnal code-book, we tested for the inter-coder
agreement (or inter-rater reliability). The average Cohen’s
Kappa coefﬁcient (κ) for all themes in the paper is 0.83 [68]. A
κ value above 0.75 is considered an excellent agreement [69].
F. Ethics
The Research Ethics Board at University College London
reviewed and approved our research project (project ID no.:
6517/002). Before each interview, we asked our participants
to read an information sheet and sign a consent form that
explained the purpose of the study, and emphasized that
all data collected was treated as strictly conﬁdential and
handled in accordance with the provisions of the UK Data
Protection Act 1998 (registration no.: Z6364106/2015/08/61).
Participants had the option to withdraw at any point during
the study without providing any reason. We explained to them
that in such a case, none of their data would be used in the
analysis, and they would still receive the full reward of £10.
No participant withdrew.
G. Limitations
Our study has some limitations. Although our sample size is
large for a qualitative study, we did not cover a wide range of
cultural backgrounds. One can argue that this limits the gen-
eralizability of our results. However, we have documented the
study protocol step-by-step, meaning that it can be replicated
with participants in different cultural contexts.
Additionally, our study has limitations common to all qual-
itative studies. Research quality depends on the researcher’s
individual skills and might be inﬂuenced by their personal
biases. A single researcher, who was trained to conduct the
interviews consistently and ask questions in an open and
neutral way in order not to inﬂuence participants, conducted all
60 interviews. We note that the length of the interviews meant
that fatigue set in during the ﬁnal 20 minutes, so participants’
answers tended to be less detailed. However, the interviewer
prompted participants to give full answers to all questions.
Furthermore, some participants could have been concerned
about
therefore,
could have changed their answers in line with how they like
to be perceived.
the interviewer’s perception of them and,
IV. RESULTS
In this section, we present the key emerging and recur-
ring themes we observed across our interviews. We report
participants’ statements by labeling them from P1 to P60.
We additionally report how many participants mentioned each
141
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:20:09 UTC from IEEE Xplore.  Restrictions apply. 
theme to give an indication of the frequency and distribution
of themes. However, the main purpose of qualitative research
is to explore a phenomenon in depth, and not to generate
quantitative results. We identiﬁed several misconceptions of
secure communications among participants that underpinned
their reasoning and decision-making. We report those in their
respective sections: IV-A – IV-H.
A. Adoption Criteria of Communication Tools