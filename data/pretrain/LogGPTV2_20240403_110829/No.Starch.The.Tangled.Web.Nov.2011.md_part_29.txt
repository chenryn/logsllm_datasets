will engage in character set detection if the charset parameter is not found in
the Content-Type header or in the  tag. Some marked differences exist
between the implementations (for example, only Internet Explorer is keen to
detect UTF-7), but you should never assume that the outcome of character
set sniffing will be safe.
Character set autodetection will also be attempted if the character set is
not recognized or is mistyped; this problem is compounded by the fact that
charset naming can be ambiguous and that web browsers are inconsistent in
how much tolerance they have for common name variations. As a single data
point, consider the fact that Internet Explorer recognizes both ISO-8859-2
and ISO8859-2 (with no dash after the ISO part) as valid character set identi-
fiers in the Content-Type header but fails to recognize UTF8 as an alias for
UTF-8. The wrong choice can cause some serious pain.
NOTE Fun fact: The X-Content-Type-Options header has no effect on character-sniffing
logic.
Byte Order Marks
We are not done with character set detection just yet! Internet Explorer needs
to be singled out for yet another dramatically misguided content-handling
practice: the tendency to give precedence to the so-called byte order mark
(BOM), a sequence of bytes that can be placed at the beginning of a file to
identify its encoding, over the explicitly provided charset data. When such a
marker is detected in the input file, the declared character set is ignored.
Table 13-1 shows several common markers. Of these, the printable
UTF-7 BOM is particularly sneaky.
Table 13-1: Common Byte Order Markers (BOMs)
Encoding name Byte order mark sequence
UTF-7 “+/v” followed by “8”, “9”, “+”, or “/”
UTF-8 0xEF 0xBB 0xBF
UTF-16 little endian 0xFF 0xFE
UTF-16 big endian 0xFE 0xFF
UTF-32 little endian 0xFF 0xFE 0x00 0x00
UTF-32 big endian 0x00 0x00 0xFE 0xFF
GB-18030 0x84 0x31 0x95 0x33
NOTE Microsoft engineers acknowledge the problem with this design and, as of this writing,
say that the logic may be revised, depending on the outcome of compatibility tests. If
theproblem is resolved by the time this book hits the shelves, kudos to them. Until then,
allowing the attacker to control the first few bytes of an HTTP response that is not other-
wise protected by Content-Disposition may be a bad idea—and other than padding
the response, there is no way to work around this glitch.
208 Chapter 13
Character Set Inheritance and Override
Two additional, little-known mechanisms should be taken into account when
evaluating the potential impact on character set handling strategies in con-
temporary web browsers. Both of these features may permit an attacker to
force undesirable character encoding upon another page, without relying
oncharacter sniffing.
The first apparatus in question, supported by all but Internet Explorer, is
known as character set inheritance. Under this policy, any encoding defined for
the top-level frame may be automatically applied to any framed documents
that do not have their own, valid charset value set. Initially, such inheritance is
extended to all framing scenarios, even across completely unrelated websites.
However, when Stefan Esser, Abhishek Arya, and several other researchers
demonstrated a number of plausible attacks that leveraged this feature to
force UTF-7 parsing on unsuspecting targets, Firefox and WebKit developers
decided to limit the behavior to same-origin frames. (Opera still permits cross-
domain inheritance. Although it does not support UTF-7, other problematic
encodings, such as Shift JIS, are fair game.)
The other mechanism that deserves mention is the ability to manually
override the currently used character set. This feature is available through
the View > Encoding menu or similar in most browsers. Using this menu to
change the character set causes the page and all its subframes (including
cross-domain ones!) to be reparsed using the selected encoding, regardless
of any charset directives encountered earlier for that content.
Because users may be easily duped into selecting an alternative encoding
for an attacker-controlled page (simply in order to view it correctly), this
design should make you somewhat uncomfortable. Casual users can’t be
expected to realize that their election will also apply to hidden  tags
and that such a seemingly innocuous action may enable cross-site scripting
attacks against unrelated web properties. In fact, let’s be real: Most of them
will not know—and should not have to know—what an  is.
Markup-Controlled Charset on Subresources
We are nearing the end of the epic journey through the web of content-
handling quirks, but we are not quite done yet. Astute readers may recall that
in “Type-Specific Content Inclusion” on page82, I mentioned that on cer-
tain types of subresources (namely, stylesheets and scripts), the embedding
page can specify its own charset value in order to apply a specific transforma-
tion to the retrieved document, for example,
This parameter is honored by all browsers except for Opera. Where it is
supported, it typically does not take precedence over charset in Content-Type,
unless that second parameter is missing or unrecognized. But to every rule,
there is an exception, and all too often, the name of this exception is Inter-
net Explorer 6. In that still-popular browser, the encoding specified by the
markup overrides HTTP data.
Content Recognition Mechanisms 209
Does this behavior matter in practice? To fully grasp the consequences,
let’s also quickly return to Chapter 6, where we debated the topic of securing
server-generated, user-specific, JSON-like code against cross-domain inclu-
sion. One example of an application that needs such a defense is a search-
able address book in a webmail application: The search term is provided in
the URL, and a JavaScript serialization of the matching contacts is returned
to the browser but must be shielded from inclusion on unrelated sites.
Now, let’s assume that the developer came up with a simple trick to
prevent third-party web pages from loading this data through :
A single “//” prefix is used to turn the entire response into a comment.
Same-origin callers that use the XMLHttpRequest API can simply examine the
response, strip the prefix, and pass the data to eval(...)—but remote callers,
trying to abuse the  syntax, will be out of luck.
In this design, a request to /contact_search.php?q=smith may yield the fol-
lowing response:
// var result = { "q": "smith", "r": [ "PI:EMAIL" ] };
As long as the search term is properly escaped or filtered, this scheme
appears safe. But when we realize that the attacker may force the response
tobe interpreted as UTF-7, the picture changes dramatically. A seemingly
benign search term that, as far as the server is concerned, contains no illegal
characters could still unexpectedly decode to
// var result = { "q": "smith[CR][LF]
var gotcha = { "", "r": [ "PI:EMAIL" ] };
This response, when loaded via  inside
thevictim’s browser, gives the attacker access to a portion of the user’s
addressbook.
This is not just a thought exercise: The “//” approach is fairly common
on the Web, and Masato Kinugawa, a noted researcher, found several popu-
lar web applications affected by this bug. And a more contrived variant of the
same attack is also possible against other execution-preventing prefixes, such
as while (1);. In the end, the problems with cross-domain charset override on
 tags is one of the reasons why in Chapter 6, we strongly recommend
using a robust parser-stopping prefix to prevent the interpreter from ever
looking at any attacker-controlled bits. Oh—and if you factor in the support
for E4X, the picture becomes even more interesting,5 but let’s leave it at that.
Detection for Non-HTTP Files
To wrap up this chapter, let’s look at the last missing detail: character set
encoding detection for documents delivered over non-HTTP protocols. As
can be expected, documents saved to disk and subsequently opened over
thefile: protocol, or loaded by other means where the usual Content-Type
metadata is absent, will usually be subjected to character set detection logic.
210 Chapter 13
However, unlike with document determination heuristics, there is no sub-
stantial difference among all the possible delivery methods: In all cases, the
sniffing behavior is roughly the same.
There is no clean and portable way to address this problem for all text-
based documents, but for HTML specifically, the impact of character set
sniffing can be mitigated by embedding a  directive inside the docu-
ment body:
You should not ditch Content-Type in favor of this indicator. Unlike ,
the header works for non-HTML content, and it is easier to enforce and audit
on a site-wide level. That said, documents that are likely to be saved to disk
and that contain attacker-controlled tidbits will benefit from a redundant
 tag. (Just make sure that this value actually matches Content-Type.)
Content Recognition Mechanisms 211
Security Engineering Cheat Sheet
Good Security Practices for All Websites
 Instruct the web server to append the X-Content-Options: nosniff header to all HTTP
responses.
 Consult the cheat sheet in Chapter 9 to set up an appropriate /crossdomain.xml meta-policy.
 Configure the server to append default charset and Content-Type values on all responses
that would otherwise not have one.
 If you are not using path-based parameter passing (such as PATH_INFO), consider dis-
abling this feature.
When Generating Documents with Partly Attacker-Controlled Contents
 Always return an explicit, valid, well-known Content-Type value. Do not use text/plain or
application/octet-stream.
 For any text-based documents, return a explicit, valid, well-known charset value in the
Content-Type header; UTF-8 is preferable to any other variable-width encodings. Do not
assume that application/xml+svg, text/csv, and other non-HTML documents do not need a
specified character set. For HTML, consider a redundant  directive if it’s conceiv-
able that the file may be downloaded by the user. Beware of typos—UTF8 is not a valid
alias for UTF-8.
 Use Content-Disposition: attachment and an appropriate, explicit filename value for responses
that do not need to be viewed directly—including JSON data.
 Do not allow the user to control the first few bytes of the file. Constrain the response as
much as possible. Do not pass through NULs, control characters, or high-bit values unless
absolutely necessary.
 When performing server-side encoding conversions, be sure that your converters reject
all unexpected or invalid inputs (e.g., overlong UTF-8).
When Hosting User-Generated Files
Consider using a sandbox domain if possible. If you intend to host unconstrained or unknown
file formats, a sandbox domain is a necessity. Otherwise, at the very minimum, do the following:
 Use Content-Disposition: attachment and an appropriate, explicit filename value that matches
the Content-Type parameter.
 Carefully validate the input data and always use the appropriate, commonly recognized
MIME type. Serving JPEG as image/gif may lead to trouble. Refrain from hosting MIME
types that are unlikely to be supported by popular browsers.
 Refrain from using Content-Type: application/octet-stream and use application/binary instead,
especially for unknown document types. Refrain from returning Content-Type: text/plain.
Do not permit user-specified Content-Type headers.
212 Chapter 13
D E A L I N G W I T H
R O G U E S C R I P T S
In the previous five chapters, we examined a fairly broad
range of browser security mechanisms—and looking
back at them, it is fair to say that almost all share a com-
mon goal: to stop rogue content from improperly inter-
fering with any other, legitimate web pages displayed
in a browser. This is an important pursuit but also a
fairly narrow one; subverting the boundaries between unrelated websites
isalarge part of every attacker’s repertoire but certainly not the only trick
inthe book.
The other significant design-level security challenge that all browsers have
to face is that attackers may abuse well-intentioned scripting capabilities in
order to disrupt or impersonate third-party sites without actually interacting
with the targeted content. For example, if JavaScript code controlled by an
attacker is permitted to create arbitrary undecorated windows on a screen, the
attacker may find that, rather than look for a way to inject a malicious payload
into the content served at fuzzybunnies.com, it may be easier to just open a
window with a believable replica of the address bar, thus convincing the
userthat the content displayed is from a trusted site.
Unfortunately for victims, in the early days of the Web, no real attention
was given to the susceptibility of JavaScript APIs to attacks meant to disrupt or
confuse users, and, unlike cross-domain content isolation issues, this class of
problems is still not taken very seriously. The situation is unlikely to change
anytime soon: Vendor resources are stretched thin between addressing com-
paratively more serious implementation-level flaws in the notoriously buggy
browser codebases and rolling out new, shiny security features that appease
web application developers, users, and the mainstream press alike.
Denial-of-Service Attacks
The possibility of an attacker crashing a browser or otherwise rendering it
inoperable is one of the most common, obvious, and least appreciated issues
affecting the modern Web. In the era of gadgets and mashups, it can have
unexpectedly unpleasant consequences, too.
The most prominent reason why most browsers are susceptible to
denial-of-service (DoS) attacks is due simply to a lack of planning: Neither the
underlying document formats nor the capabilities exposed through scripting
languages were designed to have a sensible, constrained worst-case CPU or
memory footprint. In other words, any sufficiently complex HTML file or an
endless JavaScript loop could bring the underlying operating system to its
knees. Worse, the attempts to mandate resource limits or to give users a way
to resume control of a runaway browser following a visit to a rogue page meet
with resistance. For example, the authors of many of the recently proposed
HTML5 APIs provide no advice on preventing resource exhaustion attacks,
nor do they even acknowledge this need, because they think that any limits
imposed today will likely hinder the growth of the Web 5 or 10 years from
now. Browser developers, in turn, refuse to take any action absent any
standards-level guidance.
A common utilitarian argument against any proposed DoS defenses
isthat they are pointless—that the browser is hopelessly easy to crash in a
multitude of ways, so why take special measures to address a specific vector
today? It’s hard to argue with this view, but it’s also important to note that it
acts as a self-fulfilling prophecy: The steady increase in the number of DoS
vectors is making it more and more unlikely that the situation will be com-
prehensively addressed any time soon.
NOTE To be fair, the computational complexity of certain operations is not the only reason why
browsers are easy to crash. Vendors are also constrained by the need to maintain a sig-
nificant degree of synchronicity during page-rendering and script-execution steps (see
Chapter 6). This design eliminates the need for website developers to write reentrant
and thread-safe code and has substantial code complexity and security benefits. Unfor-
tunately, it also makes it much easier for one document to lock up the entire browser, or
at least a good portion thereof.
Regardless of all these considerations, and even if browser vendors refuse
to acknowledge DoS risks as a specific flaw, the impact of such attacks is dif-
ficult to ignore. For one, whenever a browser is brought down, there is a
214 Chapter 14
substantial risk of data loss (in the browser itself or in any applications indi-
rectly affected by the attack). Also, on some social-networking sites, an attacker
may be able to lock out the victim from the site simply by sharing a rogue gad-
get, or perhaps even a well-selected image, with the victim, preventing that
person from ever using that service again.
Some of the common tricks used to take a browser out of service include
loading complex XHTML or SVG documents, opening a very large number of
windows, running an endless JavaScript loop that allocates memory, queuing
a significant number of postMessage(...) calls, and so on. While these examples
are implementation-specific, every browser offers a fair number of ways to
achieve this goal. Even in Chrome, which uses separate renderer processes
toisolate unrelated pages, it’s not difficult to bring down the entire browser:
The top-level process mediates a variety of script-accessible and sometimes
memory- or CPU-intensive tasks.
Given the above, it’s no surprise that despite generally dismissive attitudes,
the major browsers nevertheless implement several DoS countermeasures.
They do not add up to a coherent strategy, and have they have been rolled
out only in response to the widespread abuse of specific APIs or to mitigate
nonmalicious but common programming errors. Nevertheless, let’s look at
them briefly.
Execution Time and Memory Use Restrictions
Because of the aforementioned need to enforce a degree of synchronicity
formany types of JavaScript operations, most browser vendors err on the side
of caution and execute scripts synchronously with most of the remaining
browser code. This design has an obvious downside: A good portion of the
browser may become completely unresponsive as the JavaScript engine is,
say, trying to evaluate a bogus while (1) loop. In Opera and Chrome, the top-
level user interface will still be largely responsive, if sluggish, but in most
other browsers, it won’t even be possible to close the browser window using
the normal UI.
Because endless loops are fairly easy to create by accident, in order to aid
developers, Internet Explorer, Firefox, Chrome, and Safari enforce a modest
time limit on any continuously or nearly continuously executing scripts. If
the script is making the browser unresponsive for longer than a couple of
seconds, the user will be shown a dialog and given the option to abort execu-
tion. Picking this option will have a result similar to encountering an unhan-
dled exception, that is, of abandoning the current execution flow.
Regrettably, such a limit is not a particularly robust defense against mali-
cious scripts. For example, regardless of the user’s choice, it is still possible to
resume execution through timers or event handlers, and it’s easy to avoid
triggering the prompt in the first place by periodically returning the CPU
briefly to an idle state in order to reset the counter. Too, as noted previously,
there are ways to hog CPU resources without resorting to busy loops: Render-
ing complex XHTML, SVG, or XSLT documents can be just as disruptive
and is not subject to any checks.
Dealing with Rogue Scripts 215
Execution time aside, there have been attempts to control the memory
footprint of executed scripts. The size of the call stack is limited to a browser-
specific value between 500 and 65535, and attempting a deeper recursion
will result in an unconditional stop. Script heap size, on the other hand, is
typically not restricted in a meaningful way; pages can allocate and use up
gigabytes of memory. In fact, most of the previously implemented restric-
tions (such as the 16MB cap in Internet Explorer 6) have been removed in
more recent releases.