point in both executions. Finding the dominant divergence
point comprises ﬁve different cases, which we describe next.
Once the dominant divergence point is found, it is added to
the worklist and the algorithm iterates.
Divergence types. The algorithm distinguishes between 5
types of divergences, shown in Table I, based on whether
the disaligned instructions belong to aligned or disaligned
regions. Note that these cases are named with respect to the
failing execution, because that is the execution that exhibits
the unexpected behavior.
In both Case 1 and Case 2, only one of the instructions
is in an aligned region. In Case 1, the passing instruction
has a corresponding instruction but the failing instruction
does not. We call this an Extra Execution, since the failing
trace executed extra instructions. In Case 2,
the failing
instruction has a corresponding instruction, but the passing
instruction does not. We call this an Execution Omission,
since the failing trace did not execute (but presumably
should have executed) some instructions in the passing trace.
The algorithm handles both of these cases similarly, adding
to the worklist the immediate divergence point from the
disaligned instruction.
In Case 3, both instructions belong to disaligned regions.
This implies that at the dominant divergence point, each run
started executing different instructions, rather than one run
skipping instructions. We call this an Execution Difference.
The algorithm handles this case by adding to the worklist
the divergence point that dominates both instructions.
In Case 4, both instructions belong to an aligned region,
however they do not align with each other but rather with
other instructions. So, the current instructions have a value
difference which is deﬁned in different instructions in both
traces, but
the deﬁnitions are neither extra nor omitted
executions. This means that the value difference was not
caused by a ﬂow difference, and thus must be caused by an
address difference. This can only happen when the value
difference manifests in a memory operand. We call this
case Invalid Pointer and differentiate between two sub-
cases: either a current instruction is reading from an invalid
memory location using a corrupted pointer (Case 4a: Wild
Read) or the deﬁning instruction is writing to an invalid
354
memory location using a corrupted pointer (Case 4b: Wild
Write). To differentiate both subcases, the pointer used to
dereference the memory operand is checked. If the operand
that holds the pointer (described in Section IV-E) has a value
difference, then it is a wild read. In this case, the memory
being read is not interesting; what is interesting is the fact
that it was read, i.e., that the pointer used to read memory
was corrupt. Thus, the memory operand is pruned and the
corrupted pointer is added to the worklist. If the pointer is
not a value difference, then it is a wild write. In this case,
what is interesting is the fact that the pointer used to write
memory was corrupt. Thus, the algorithm adds the pointers
in the deﬁning instructions to the worklist. Note that it is
important to differentiate between these subcases since they
need to be handled differently. Also, the fact that Slice-Align
can identify wild reads and writes is important for an analyst
that may want to exploit these invalid memory accesses.
C. Enhanced Graph Algorithm
To extend the Basic graph into an Enhanced graph that
includes the disaligned region(s) selected by the analyst, the
Slice-Align algorithm ﬁrst generates a Basic graph, and then
initializes a new worklist with the selected regions. For each
item in the worklist, the algorithm proceeds as follows. First,
the algorithm identiﬁes the immediate control dependency of
the current instruction as well as the data dependencies of
its operands. If there is a corresponding aligned instruction
in the other trace, the algorithm will prune the operands
according to the techniques described in Section IV-E. The
dependency edges are added to the graph, and if the target of
the dependency has not been processed by the Basic graph
algorithm, it is added to the worklist so that its dependencies
will in turn be explored by the Enhanced graph algorithm.
This process iterates until the worklist is empty.
D. Identifying Input Differences
As the failing execution progresses,
the difference-
inducing state propagates from the input difference(s) to the
target difference through one or more chains of ﬂow and
value differences. We say that a difference X affects another
difference Y if there exists at least one such chain from
X to Y . In this section, we prove that the Enhanced graph
algorithm will locate all, possibly multiple, input differences
that affect the target difference.
We use uppercase letters when referring to execution
differences (which are deﬁned over pairs of statements) and
lowercase letters when referring to speciﬁc statements. We
add subscripts to execution difference symbols when we
wish to differentiate between value and ﬂow differences. For
example, a value difference XVAL is deﬁned over statement
pair (px,fx), while a ﬂow difference XFLOW is deﬁned over
statement pair (px,⊥) or (⊥,fx).
We now deﬁne data and control dependence over exe-
cution differences. Informally, these deﬁnitions are based
on their analogs in dynamic program slicing (i.e., edges in
the program dependence graph [9], [12]), except they are
deﬁned with respect to pairs of execution statements rather
than statements from a single execution.
dd
−→ px ∧ fy
Let X and Y be two distinct execution differences. We
DD
−−→ X) iff
say Y is data dependent on X (denoted Y
dd
−→ fx. Similarly, Y is control dependent
py
cd
on X (denoted Y
−→ fx. If X
or Y are ﬂow differences, then only the predicate for the
ﬂow-differing statement needs to hold (equivalently, we say
CD
−−→ x) iff py
cd
−→ px ∧ fy
⊥
dd|cd
−−−→ x ∧ x
dd|cd
−−−→ ⊥ and ∀x ⇒ ⊥
We now show that even though we prune many irrelevant
execution differences, the input differences are guaranteed
to be present in the causal difference graph.
dd|cd
−−−→ ⊥).
Theorem 1: Any input difference that affects the target
difference will appear in the causal difference graph.
Proof: Our proof is by induction over the legal tran-
sitions between execution differences (e.g., edges in the
graph). In particular, we demonstrate that for any transition
DD|CD
from X to Y (i.e., Y
−−−−−→ X), if Y appears in the
worklist, then X will be identiﬁed by the algorithm and
added to the worklist.
In the base case, if the input difference appears in the
worklist then, by deﬁnition of input difference, there are
no more dependencies and the algorithm completes success-
fully.
For the inductive step, we enumerate all transitions from
DD|CD
X to Y (i.e., YVAL|FLOW
−−−−−→ XVAL|FLOW) that may occur
along the causal path, and explain how the Slice-Align
algorithm identiﬁes the source of the transition in each case.
First, consider a transition from a value difference XVAL
to another value difference YVAL. Note that YVAL cannot be
control dependent on XVAL since a value difference at branch
XVAL would necessarily imply a control ﬂow difference, but
by the deﬁnition of value difference, the statements in YVAL
are aligned. Thus, we need only consider the dependency
DD
YVAL
−−→ XVAL. If YVAL appears in the worklist, then XVAL
will be identiﬁed by the data slicing in the Basic graph
algorithm, which adds the data dependencies of every value
difference to the worklist.
Next, consider a transition from a value difference XVAL
to a ﬂow difference YFLOW. This transition can be caused
by either a data dependency or a control dependency. For
CD
a control dependency (YFLOW
−−→ XVAL), XVAL must be a
divergence point, by the above argument. Then XVAL will
be identiﬁed by the Basic graph algorithm, which adds the
divergence point of ﬂow differences to the worklist. For a
DD
data dependency (YFLOW
−−→ XVAL), the full slicing step of
the Enhanced graph algorithm will add XVAL to the worklist.
Note that the wild write processing (Case 4b in Table I)
does not handle this situation since it applies only to aligned
statements.
355
The third case is a ﬂow difference XFLOW to a value
difference YVAL. Note that an aligned statement (e.g. value
difference) cannot be control dependent on a ﬂow difference,
so here we need only consider the case of data dependency
DD
(YVAL
−−→ XFLOW). Intuitively, this type of transition means
that a disaligned statement wrote a value which was read by
the program after realignment (e.g., an execution omission).
Like the ﬁrst case, XFLOW will be identiﬁed by the Basic
graph algorithm since it represents a data dependency where
the source is a value difference.
Finally, consider the case of a ﬂow difference XFLOW to
a ﬂow difference YFLOW. This transition could represent a
data dependency or a control dependency. In either case,
the full slicing step of the Enhanced graph algorithm will
add XFLOW to the worklist.
Our induction hypothesis guarantees that for any node Y
that already appears in the worklist, the Slice-Align algo-
rithm will eventually reach the head of all causal difference
paths through Y . By deﬁnition of the affects relationship,
there must exist at least one causal difference path from each
input difference to the target difference. Thus, the proof is
complete by noting that the Slice-Align algorithm adds the
target difference to the initial worklist.
E. Extended Pruning with Address Normalization
An important feature for the scalability of Slice-Align is
the ability to prune edges in the graph when an operand of
an aligned instruction has the same value in both execution
traces. Without pruning, the graph may explode in size be-
cause the nodes that explain how those identical values were
generated need to be included, even if identical values cannot
be the cause of other execution differences. A basic approach
is to prune an operand when its corresponding operand in the
other execution has the same value. However, such pruning
is limited because many operands contain pointers that may
not have identical values between executions but are still
equivalent to each other (e.g., point to equivalent objects).
To address this problem, we use a memory normalization
technique that extends the basic pruning to include equiv-
alent pointers, even if they have different values. This ex-
tended pruning identiﬁes operands that hold pointers, applies
pruning based on the normalized addresses of those pointers,
and prunes other operands by direct value comparison.
Recall that the address of a memory operand in an x86
instruction is computed as: address = base + (index *
scale) + displacement, where the scale and the displacement
are constants and the base and index values are stored in
registers. The base register value and displacement can both
be pointers, as can the index register value if the scale equals
1. The ﬁrst step to prune a pointer is to identify where it is
stored. For this, we simply select the largest of the three as
the candidate pointer for the operand.
If the candidate pointer is the offset then we are done,
as it is a constant with no further dependencies. Otherwise,
our memory normalization tries to determine whether the
pointers, stored in the index or base register, are equivalent in
the two executions. This process, described next, comprises
two steps. First, each pointer is classiﬁed as a heap pointer,
stack pointer, or data section pointer. If both pointers have
the same classiﬁcation, a speciﬁc normalization rule for that
class is applied.
Heap pointer pruning. Direct comparison of heap pointers
often fails because equivalent allocations can return different
pointers. The ﬁrst step in heap pointer pruning is to check
whether the value of the candidate pointer in each trace
belongs to a live heap buffer. For this, during program
execution the execution monitor produces an allocation log
that captures the dynamic memory allocations/deallocations
performed by the program. We have implemented an API
that reads this allocation log and can answer for a given
memory address and a given point in the execution, whether
there is any live buffer that contains the address. If so, the
API provides the buffer information, including the buffer
start address, the buffer size, and the allocation site (i.e., the
counter of the allocation’s call instruction).
If both candidate pointers point to the heap, we try to
prune them. The key intuition to normalize heap addresses
is that an allocation invocation that is aligned returns an
equivalent pointer in each execution. More speciﬁcally, we
prune a candidate heap pointer if: 1) the allocation site for
the live buffers that contain the pointed-to addresses are
aligned, and 2) the offset of those pointed-to addresses, with
respect to the start address of the live buffer they belong
to, is the same. If both properties are satisﬁed, the register
holding the pointer is pruned.
Since the allocation log starts at process creation but
the trace starts when the ﬁrst input byte is read, a special
case may happen where the allocation site returned for a
live buffer is not present in the trace and no alignment
information is available for it. In this situation we apply
a more aggressive pruning that assumes the above condition
1) is true and prunes if condition 2) is satisﬁed.
Stack pointer pruning. Direct comparison of stack pointers
may fail because each thread of a process has a different
stack and because the base address of the stack (highest stack
address) can be randomized using Address Space Layout
Randomization (ASLR) [3]. To check whether the value
of the candidate pointers point to the stack, the range of
stack addresses accessed by each program thread in the
execution trace is computed and rounded to the nearest
page boundaries. The candidate pointer points to the stack
if its value is contained in its thread’s stack range. If
both candidates are stack pointers, they are normalized by
subtracting the thread’s stack base address. If the resulting
offsets are identical the register holding the pointer is pruned
Program
Name
Adobe Reader 9.2.0
reader-e1
Adobe Reader 9.2.0
reader-e2
Adobe Reader 9.2.0
reader-u1
reader-u2
Adobe Reader 9.2.0
reader-u10 Adobe Reader 9.2.0
reader-u11 Adobe Reader 9.2.0
reader-u14 Adobe Reader 9.2.0
ﬁrebird
Firebird SQL 1.0.3
gdi-2008
gdi-2007
tftpd
gdi32.dll v2180
gdi32.dll v2180
TFTPD32 2.21
Vuln. CVE
Unknown
Unknown
Unknown
Unknown
Unknown
Unknown
Unknown
2008-0387
2008-3465
2007-3034
2002-2226
conﬁcker
netsky
W32/Conﬁcker.A
W32/Netsky.C
N/A
N/A
OS
XP SP3
XP SP3
XP SP3
XP SP3
XP SP3
XP SP3
XP SP3
XP SP2
XP SP2
XP SP2
XP SP3
XP SP3
XP SP3
Table II: Programs and vulnerabilities in the evaluation.
from the graph.
Data section pointer pruning. Direct comparison of data
section pointers may fail if a dynamically loaded module
(e.g., a DLL) was loaded at different base addresses in
both executions. Since the execution trace contains all the
modules (base address and size) that were loaded in the
address space of the process, here for each candidate pointer
value we check if it belongs to the address range for each
module. If both candidate pointers belong to the same mod-
ule, we normalize the address by subtracting the module’s
base address and compare the resulting offset, pruning the
memory register if the offsets are identical.
V. EVALUATION
In this section we evaluate our differential slicing ap-
proach. The evaluation comprises three parts. First,
in
Section V-A we evaluate our tools for failure/vulnerability
analysis on 11 vulnerabilities. We show that our differential
slicing approach greatly reduces the number of instructions
and trace differences that an analyst needs to examine to
understand a vulnerability. Then, in Section V-B we perform
a user study with the help of two vulnerability analysts to
understand how useful our differential slicing approach is for
real users. Finally, in Section V-C we evaluate our tools for
analyzing 2 malware samples with environment-dependent
behaviors.
A. Evaluating the Causal Difference Graph
In this section we analyze our differential slicing approach
on the 11 vulnerabilities listed in the top portion of Table II.
For each vulnerability, we are given two inputs: a failing
input that manifests a crash and a passing input that does
not. Here, both inputs differ only in one byte. These give us
some ground truth for crashes in programs with no publicly
available source code (i.e., Adobe Reader and GDI, a built-in
graphics library in Windows) because we know that the byte
difference between inputs should be identiﬁed as the only
input difference in our graphs. Speciﬁcally, for the Adobe
356
Name
reader-e1
reader-e2
reader-u1
reader-u2
reader-u10
reader-u11
reader-u14
tftpd
ﬁrebird
gdi-2008
gdi-2007
Total instructions
Passing
2,800,163
1,616,642
2,430,400
1,921,514
408,618
1,868,942
1,194,053
626,622
6,698
42,124
36,792
Failing
1,819,714
1,173,531
1,436,993
1,053,840
272,994
1,112,828
155,906