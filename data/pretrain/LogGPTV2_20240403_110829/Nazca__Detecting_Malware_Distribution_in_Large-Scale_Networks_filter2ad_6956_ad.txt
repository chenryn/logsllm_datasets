This however, was expected; Nazca is designed to complement
traditional solutions, not to replace them. Note also that the
client in question was running ESET NOD32 antivirus (we can
infer that from the User-Agents of its connection), which
might have prevented the infection.
Finally, we attribute a higher count of candidates marked as
Unknown in the testing dataset to the fact that we received this
second dataset with a longer delay compared to the training
dataset. Many servers have gone ofﬂine by the time we were
fetching their payloads. Although this quick take down could
be an indication that some malicious activity was taking place,
we do not use this in our evaluation.
Fig. 5. Candidate selected by the Mutating Payloads technique (for clarity,
not all payloads are shown in the graph).
Technique
File Mutations
Distributed Hosting
Isolated Hosting
Exploit/Download Hosts
All techniques combined
TABLE II.
URLs
FQDNs
FQDNs
FQDNs
Type
Malware
Benign
Test
11
17
233
3
258
EFFICACY OF OUR TECHNIQUES.
Train
43
45
68
29
117
Train
0
4
12
9
9
Test
8
155
145
16
324
Unknown
Test
Train
107
16
238
42
140
47
28
152
637
85
.
1) Detection of File Mutations: With this technique, we
look for URLs that deliver different payloads over time. One
such URL is shown in the malicious neighborhood graph in
Figure 5. We have already shown a graph containing candidates
detected by this technique in Figure 3 where all the C&C
endpoints at the center of the ﬁgure exhibit this behavior.
Applying this technique to the testing set, we also found a
different set of URLs that was operating as a C&C.
In Figure 6, we show the number of variations across all
URLs that show this behavior in the training dataset. If we
consider only URLs that have shown more than 500 variations
during these two days, we obtain exclusively malicious URLs.
However, just four URLs fulﬁll this requisite. Instead, with
the minimum possible threshold, which is two versions per
day, this technique detects 49 malicious URIs in our training
dataset. However, it also has 6,702 false positives, 3,864 of
which are Antivirus URIs. The rest includes many failed/trun-
cated downloads. Clearly, this performance is not acceptable
and, by looking at
the graph, we should set a threshold
somewhere between 10 and 50. Notice that the majority of
the benign URLs that trigger this technique are endpoints
contacted by AV updaters. We can easily ﬁlter them with a
Fig. 6. Yield of different thresholds in the Mutating Payloads technique.
small whitelist: speciﬁcally, only six antivirus domains account
for all the false positives in this area.
To set a more sensible threshold, we trained a simple
classiﬁer using ﬁve-fold cross validation on the training set,
which has chosen a threshold of 12 variations a day. We
show the performance of varying this threshold on the training
dataset in Figure 7. The classiﬁer choice is intuitively correct,
as it yields a 91.1% true positive rate, with a 5.8% false
positive rate.
We also studied the timing of the changes in the payloads,
but found that there is no distinctive difference among the
trends of these timings in malicious and benign URLs, other
than the fact that malicious URLs tend to change more often.
2) Detection of Distributed Hosting and Domain Fluxing:
We look for groups of domains that host a similar set of
malicious executables. To evaluate this technique, we ﬁrst
discover all groups of domains that have an intersection of the
ﬁle they offer. From each of these tentative distributed hosting
infrastructures, we extract the features listed in Section IV-B.
10
Fig. 8. Distributed Hosting: decision tree classiﬁer.
Fig. 7. Mutating Payloads technique: performance with respect
threshold.
to the
Distributed hosting infrastructures Malicious
Predicted Malicious
12
Predicted Benign
0
Class recall
100 %
TABLE III.
Benign
5
128
96.24%
Class Precision
70.59%
100%
DISTRIBUTED HOSTING: CLASSIFIER PERFORMANCE.
.
We then train a few classiﬁers using the leave-one-out cross
validation. The classiﬁer that performs the best in the training
dataset is a Decision Tree classiﬁer, as shown in Figure 8.
This classiﬁer learns that a malicious distributed hosting in-
frastructure (i) hosts primarily executables, (ii) spans across
many ﬁrst-level domains, many of which are co-located on the
same server, or (iii) spans across to just two or three domains,
exclusively hosting the malware and a couple of supporting
ﬁles (usually JS and HTML).
The classiﬁer’s performance, evaluated on the training set
via the leave-one-out cross validation, is shown in Table III.
Note that
the classiﬁer operates on the entire distributed
hosting infrastructure as a single sample. 12 malicious infras-
tructures in the training set account for a total of 45 malicious
domains. Three of the ﬁve benign CDNs that are incorrectly
predicted as malicious belong to antivirus companies (Avira,
Lavasoft, and Spybot). One of these detected malicious dis-
tributed hosting infrastructures is shown in Figure 2.
3) Detection of Dedicated Malware Hosts: We look for
small domains that host a single malicious executable that is
unique in the network and a small set of supporting HTML
and JS ﬁles. By simply looking for domains that are involved
in the download of a single unique executable ﬁle and host
at most one other URL delivering HTML or JS, we obtain
241 suspicious candidates. Of these, 73 are malicious, 38 are
legitimate, and 130 are either unreachable or have an expired
domain record.
To boost the performance of this technique, we build a
simple classiﬁer that takes into account the registration date of
the domain under analysis. If the registration has been made in
Fig. 9. Dedicated Malware Hosts technique: performance of the classiﬁer
with respect to the registration date threshold.
the last three years (our threshold), we consider this domain as
a candidate. The performance of such a classiﬁer with respect
to the threshold chosen is shown in Figure 9.
Figure 10 shows an example of a domain detected by this
technique. 3322.org is a Chinese dynamic DNS provider
used by the Nitol botnet, which has been recently seized by
Microsoft [5].
4) Detection of Exploit/Download Hosts: We look for
domains that successfully perform the initial client infection.
To detect them, we search for trafﬁc that matches our model
of the infection process, where a client makes a ﬁrst request
to the suspicious domain, followed by another request, with
a different User-Agent, that downloads an executable. Just
by selecting domains that have been contacted multiple times
by the same host with different User-Agents, and at least
one of the subsequent requests has downloaded an executable
(with User-Agent u), we obtain 35 malicious domains and
697 legitimate ones in the training dataset.
Fig. 10. Candidate selected by the Dedicated Malware Hosts technique.
11
To improve the detection, we train a simple classiﬁer using
as the only feature the fraction of connections, scattered in the
whole dataset, performed with User-Agent u that resulted
in downloading executables. Choosing a threshold score for
the classiﬁer at 0.9 (that is, 90% of the HTTP requests made
with u in the dataset have delivered executables), we obtain a
classiﬁer that identiﬁes 29 malicious domains and nine false
positives in the training dataset.
E. Detection Step
The candidate selection step produced 1,637 unique URLs
as candidates in the training dataset, and 2,581 in the testing
dataset. The astute reader might notice that these numbers
are different from the values listed in Table II. The reason
is that the detection step operates on URIs. Table II shows the
number of candidates for unique FQDNs. Since a domain can
host multiple (sometimes many) distinct URIs, the numbers
for URIs are larger than for FQDNs.
Using all URIs as inputs, we generate the Malicious
Neighborhood graphs. Since multiple candidates might appear
together in the same graph, we generated a total of 209 graphs
for the training dataset and 156 graphs for the testing dataset.
For the training dataset, we have 169 singleton graphs, while
we have 98 for the testing dataset. The distribution of the graph
sizes is shown in Figure 11. Overall, we ﬁnd that the vast
majority of candidates are part of a graph of non-trivial size.
Note that the testing dataset graph sizes seem to be clustered.
That is because we have some vast malicious neighborhoods
that are only partially represented in a graph. Hence we need
multiple graphs, built around different candidates belonging to
these neighborhoods, to cover them completely.
We have already shown sections of these graphs through-
out
the paper. With them, we captured several
infection-
C&C Botnet infrastructures, users being tricked to download
software that quietly downloads and installs malware, and a
popular domain phpnuke.org being used to host malware.
Interestingly, we have seen that a major contributor to the
availability of malware comes from the ISP’s caching servers;
in four caching servers operated by the ISP, we found 54 mal-
ware samples. In many cases, malicious server that originally
delivered them has since gone ofﬂine, but the malware was
still being downloaded from the caching servers.
In addition to observing malware distribution campaigns,
we used our graphs to compute the malicious-likelihood scores
of all the selected candidates. With these, we have trained a
simple classiﬁer to tell apart malicious candidates from innocu-
ous ones, using ﬁve-fold cross validation. In the training set,
this classiﬁer yields 77.35% precision and 65.77% recall on
the malicious class, and 95.70% precision and 97.53% recall
on the benign class. In the testing set, we have 59.81% pre-
cision and 90.14% recall on the malicious class, and 99.69%
precision and 98.14% recall on the benign class. Note that
some misclassiﬁcations happen because our data has uncovered
an insufﬁcient section of the malicious infrastructure and we
could not see multiple candidates belonging to it. However, the
results show that benign URIs are rarely mistaken as malicious
(i.e., a very low false positive rate).
To better exemplify the nature of Nazca’s false negatives,
let’s consider one of them, http://downloads.shopperreports.
Fig. 11. Distribution of graph sizes in the Detection Step.
com/downloads/csupgrade/ComparisonShoppingUpgrade.exe.
This ﬁle was downloaded only once in our dataset by a
user that did not come into contact with any other malware
(probably because the infection did not happen). Because
of this, Nazca did not collect enough information on the
infrastructure delivering it to classify this ﬁle as malicious,
although it was ﬁrst deemed suspicious by our Dedicated
Malware Host detection technique in the second step. Nazca
was actually tracking the main infrastructure orchestrating this
infection campaign (the ClickPotato campaign) and the clients
infected by it. This misclassiﬁciation is then caused by the
scale of the monitored network, which is not large enough for
Nazca to passively observe all the various components of this
large infection. To mitigate this, Nazca could be coupled with
an active system, such as EvilSeed [8]or Li’s “Finding the
Linchpins” work [9], to better explore detected infrastructures
and improve coverage.
In our experiments, our system detected malware samples
from 19 domains that were not in our blacklists nor identiﬁed
by VirusTotal. We notiﬁed antivirus companies, and within
a few weeks these samples were ﬂagged as malicious. For
example, when Nazca detected the infrastructure in Figure 2,
three of the payloads from URL #4 were unlisted. Being
a content-agnostic system, Nazca cannot be evaded by mal-
ware that is particularly resilient resilient content analysis.
Instead, to evade Nazca malware writers have to devise new,
stealthier ways to distribute malware, which we envision to
be challenging without sacriﬁcing the scale of the malware
distribution. Content-agnostic systems like Nazca (or [10],
[11], [9]) can be evaded if malware writers deliver each piece
of malware without reusing any previously-used component
of their infrastructure (domain names, servers. . . ), which will
have an impact on their proﬁts.
VII. DISCUSSION
We discuss several design choices, usage scenarios, and
evasions against Nazca.
12
Fig. 12. Most popular protocols employed by 3,000 malware samples,
executed in the Anubis sandbox
Fig. 13. Most popular protocols employed by 621 malware samples, executed
in the Capture-HPC sandbox.
A. Why Only HTTP?
During our initial research, we asked ourselves what pro-
tocols we should focus on. To get an answer, we studied the
trafﬁc collected from malicious samples that were executed
in two different sandboxes. The ﬁrst sandbox is Anubis [7],
which runs executable ﬁles in an instrumented Microsoft
Windows environment. We obtained trafﬁc of 3,000 malicious
samples run by the Anubis network, and classiﬁed the network
connections types. The results are shown in Figure 12. We note
that HTTPS connections account for less than 1% of the total.
Because we do not observe the initial exploitation in Anubis
network trafﬁc (as the malware is already on the sandbox), in
this experiment, we are focusing on the control phase of the
malware’s lifecycle.
The second sandbox is Capture-HPC [12]. Different from
Anubis, Capture-HPC is a honeyclient; it drives a web browser
to visit a target domain, and gets exploited if the target
delivers malware to which the honeyclient is vulnerable. We
have collected the trafﬁc for 621 infections and classiﬁed
the connection types. From Figure 13, we see that even
during the exploitation and installation phases of the malware’s
lifecycle, the protocol that overwhelmingly dominates is HTTP.
Therefore, we chose to focus our efforts on HTTP trafﬁc.
B. What If Cyber-criminals Switch to HTTPS?
The use of an encrypted channel would severely limit the
data that a system like Nazca could mine, surely hampering its
detection performance. However, that is true for the majority of
systems that identify malware in network trafﬁc, for example
on a corporate ﬁrewall. So why do cyber-criminals still mas-
sively use HTTP? We believe that this is because switching to
HTTPS would ultimately harm the cyber-criminals’ revenues.
There are two ways in which cyber-criminals could handle
an HTTP malware server: by using a self-signed certiﬁcate
or adopting a certiﬁcate signed by some trusted certiﬁcation
authority. On the one hand, the ﬁrst solution would diminish
the rate of successful infections, as clients that visit the exploit
domain would be warned by their browsers that something
is suspicious (all major browsers do this), and many would
turn away. On the other hand, the second solution requires the
cyber-criminal to obtain a valid certiﬁcate. While this might
certainly happen sporadically, it is not in the interest of the
certiﬁcation authority to make a habit of signing certiﬁcates
for exploit domains, as most major browsers would quickly
remove their trust to that authority, thus hurting the certiﬁcation
authority business.
Another possibility is to piggyback on legitimate web
services to host
the cyber-criminal’s software and control
channels. There have been strands of malware that have used
Google Drive [13], Dropbox [14], and other web services
as C&C server and infection host. Nazca would not detect
such malware distribution frameworks. However, the compa-
nies running these web services usually monitor their servers