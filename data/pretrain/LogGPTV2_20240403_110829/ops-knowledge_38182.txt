User: Answer the question about Azure Data Explorer Ingesting historical data:
How can I ingest large amounts of historical data and ensure good performance?

n
Assistant: To efficiently ingest large quantities of historical data, use [LightIngest](../../lightingest.md). 
For more information, see [ingest historical data](../../ingest-data-historical.md). 
To improve performance for many small files, adjust the [batching policy](batching-policy.md), change batching conditions and address [latencies](batching-policy.md#batching-latencies). 
Use the batching policy [wizard](../../table-batching-policy-wizard.md) to quickly change policy settings. 
To improve ingestion performance when ingesting extremely large data files, use [Azure Data Factory](/azure/data-factory/) (ADF), a cloud-based data integration service.