1
1
1
2
2
2
3
3
3
Fig. 7.  Implementing OLA using minimum-cost blocking flow with convex cost function.  
 Differences from earlier solution highlighted in bold. 
5. DISTRIBUTED OLA 
We start by describing an approximate centralized version of OLA. 
We then show how this can be converted to a distributed sched-
uler,  using  an  extension  of  the  backlog-proportional  allocation 
method introduced earlier. 
Our  approximate  centralized  algorithm  uses  an  array  x(i,j) 
which is initialized to zero and which defines the number of cells 
to  be  transferred  from  input  i  to  output  j,  when  the  scheduling 
algorithm completes. It also uses a parameter ∆ ≤ ST, which de-
termines the accuracy of the approximation. During its execution, 
the algorithm maintains a list of the outputs, sorted in increasing 
order of x(+, j) + B( j). The algorithm repeats the following step so 
long as there are at least two outputs on the list. 
Let j1 and j2 be the indices of the first two outputs on the list. 
Increase  x(+,  j1),  by  repeatedly  increasing  x(i,j1)  for  selected 
values of i (input selection criteria are discussed below). Stop 
when x(+, j1) + B( j1) = x(+, j2) + B( j2) + ∆, or when x(+, j1) = 
ST or when x(+,  j1) = B(+,  j1), whichever occurs first. If either 
of the last two conditions occurs, remove j1 from the list. Oth-
erwise, move it down the list to maintain the list order. 
When the list has been reduced to a single output j, the algorithm 
increases x(+, j) until x(+, j) = min {ST, B(+,  j)} or until all inputs 
with cells for output j have scheduled all they can (ST ). 
 The  number  of  steps  performed  by  the  algorithm  is at most 
nST/∆.  It  can  be  implemented  to  run  in  O(m  +  (ST/∆)n2)  time, 
where m is the number of non-empty VOQs. This can be improved 
to O(m + (ST/∆)n log n), if the list is replaced with a heap. If ∆=1, 
the algorithm computes an OLA schedule (regardless of the input 
selection criterion). For larger values of ∆, it implements a ∆-OLA 
schedule, which is defined as any maximal schedule for which 
B( j1) + x(+, j1) < B( j2) + x(+, j2) − ∆ 
implies that x(+,  j1) = min{ST, B(+, j1)}. That is, a ∆-OLA sched-
uler  allows  the  output  queue  differences  at the end of a transfer 
phase to exceed ∆, only if there is no way to transfer more cells to 
the outputs with the smaller queues. ∆-OLA schedulers, like OLA 
schedulers are work-conserving when the speedup is at least 2 (a 
slight variant of the proof used for OLA can be used to show this). 
For  smaller  speedups,  we  can  trade-off  scheduling  performance 
against running time by adjusting ∆. 
The criterion used to select the next input to use to effect an 
increase  in  x(+,  j1)  does  not  affect  the  work-conservation  condi-
tion. However, different choices can affect performance when the 
speedup is less than two. In the performance results reported be-
low, we distribute the load approximately evenly among all inputs 
with traffic for output j1, using a round-robin technique. We main-
tain a list of inputs that can still send to j1 (they have both cells for 
j1 and uncommitted bandwidth) and use the first input on the list 
to increase the flow to j1. To obtain an even distribution, we take 
at most ∆ from an input at a time and then move that input to the 
end of the list. This method can be implemented without increas-
ing the time complexity of the algorithm. 
To convert a ∆-OLA scheduler to a practical distributed sched-
uler, we use the backlog proportional allocation technique intro-
duced earlier to allow inputs to divide the responsibility for sup-
plying  traffic  to  the  different  outputs.  This  allows  each  input  to 
operate independently of the others, once the initial exchange of 
information  takes  place. As with  DBL, this initial exchange sup-
plies input i with the values of B(j) and B(+,j) for every output j. 
Input i also has the values B(i,j) for all j and it uses these to com-
pute  values  σ(i,j)  =  B(i,j)/B(j).  Given  this  information,  input  i 
makes its scheduling decisions in a way that is similar to the cen-
tralized  algorithm.  In  particular,  input  i  maintains  a  list  of  the 
outputs for which it has cells, sorted in increasing order of B(j) + 
x(i,j)/σ(i,j).  It  then  repeats  the  following  step  so  long  as  the  list 
has at least two elements. 
Let j1 and j2 be the indices of the first two outputs on the list. 
Increase x(i, j1) until one of the following conditions holds. 
1.  x(i,+) = ST 
2.  x(i, j1) = σ(i,j1)ST 
3.  x(i, j1) = B(i, j1) 
4.  B(j1) + x(i,  j1)/σ(i,j1) = B(i, j2)+ ∆  + x(i, j2)/σ(i,j2) 
If  condition  1  occurs,  the  algorithm  terminates.  If  either  of 
conditions 2 or 3 occurs, remove j1 from the list. Otherwise, 
move j1 down the list so as to maintain the list order. 
When the list has been reduced to a single output j, the algorithm 
increases x(i, j) until x(i, j) = min {σ(i,j)ST, B(i,  j)} or until x(i,+) 
= ST, whichever occurs first. 
The  number  of  steps  performed  by  the  algorithm  is  at  most 
nST/∆. It can be implemented to run in O((ST/∆)n2) time, using a 
naive  list  implementation  or  O((ST/∆)n  log  n),  if  the  list  is  re-
placed with a heap. Using a hardware implementation of a sorted 
0.6
0.5
0.4
0.3
0.2
0.1
0
4000
miss  fraction
speedup =1.2, 3 inputs, 5 phases
average miss fraction
distributed OLA -  ∆=0.1
4250
4500
4750
5000
5250
Time
5500
5750
6000
6250
6500
Fig. 8. Example stress test for distributed OLA
list, this can be improved to O((ST/∆)n) at the cost of n registers 
and associated comparison logic. 
Fig. 8 shows how distributed OLA performs on a sample stress 
test. This example uses a value of ∆=0.1. Comparing this to Fig. 
3, we see that distributed OLA reduces the miss fraction during the 
critical period of the last phase by about 20% relative to DBL. For 
this situation, distributed OLA delivers nearly ideal performance, 
distributing the misses evenly among the different outputs experi-
encing misses. Fig. 9 shows how distributed OLA performs on a 
large number of different stress tests. Comparing these results to 
Fig. 4, we see that distributed OLA provides the largest improve-
ment for very small speedups. The speedups needed to reduce the 
misses to zero are the same for both DBL and distributed OLA.  
6. PRACTICAL CONSIDERATIONS 
While the main focus of this paper has been on establishing the 
theoretical  foundation  for  robust  distributed  scheduling,  we  be-
lieve that the results are of direct practical value. First, it is impor-
tant to discuss the significance of the idealized assumptions made 
to facilitate the analysis; specifically, the assumption that the sys-
tem operation is structured in discrete phases (arrival, transfer and 
departure). While systems could certainly be built that adhere to 
this  assumption,  this  would  imply  a  period  during  which  data 
forwarding  was  suspended,  while  scheduling  was  being  per-
formed. Pipelining can be used to eliminate this inefficiency. Dur-
ing  each  update  period,  a  pipelined  implementation  would  per-
form  the  scheduling  needed  to  handle  traffic  received  up  to  the 
start  of  the current update period. This traffic would then be al-
lowed  to  proceed  to  the  outputs  during  the  next  update  period. 
distributed OLA
∆=.02 (lines)
∆=.2 (mark)
5,11
4,9 
2 inputs , 5 phases
3,7 
1
0.8
0.6
0.4
0.2
n
o
i
t
c
a
r
f
s
s
i
m
.
g
v
a
0
1.05 1.1 1.15 1.2 1.25 1.3 1.35 1.4 1.45 1.5
speedup
Fig. 9. Miss fraction for distributed OLA on a variety of 
stress tests 
This  implies  that  all  cells  would  experience  a  delay  of  between 
one  and  two  update  periods.  While  our  analysis  can  be  applied 
directly to systems that operate in this way, we need to relax the 
definition of work-conservation to reflect this delay. We say that 
such a system with an update period of T is T-work-conserving, if 
an output link is never allowed to be idle, so long as there are no 
cells that arrived at least 2T time units earlier. (Note that by this 
definition, crossbar schedulers that pipeline scheduling with data 
transfer are 1-work-conserving.) 
In practice, it may be preferable not to adhere to a strict pipe-
lining discipline, but to allow scheduling to proceed on a more or 
less continuous basis, with ports periodically sending their status 
information and asynchronously updating the forwarding rates of 
their VOQs. This eliminates delays that are artificially imposed by 
the scheduler. Delays will still occur when the rate at which traffic 
arriving  at  an  input  for  a  given  output  increases  suddenly,  but 
during periods of relative rate stability there would be no unnec-
essary delays. Note however, that while our results provide strong 
evidence that such systems can be work-conserving, they do not 
specifically  apply  to  them.  It  would  be  interesting  to  see  if  one 
could formalize such asynchronously scheduled systems so as to 
enable rigorous statements about work-conservation. 
Another  important  issue  for  distributed  scheduling  is  the 
overhead of the required message exchanges required. The practi-
cal  variants  of  the  distributed  schedulers  described  here  require 
that  each  port  send  and  receive  2n  values,  each  update  period 
(where n is the number of ports). Using a compact floating point 
representation,  these  can  be  encoded  with  sufficient  accuracy  in 
4n bytes. If the update period is chosen so that the amount of data 
a port can send to or receive from the interconnection network per 
update  period  is  much  larger  than  4n,  the  overhead  required  to 
communicate  these  values  can  be  kept  acceptably  small.  For  a 
system with n=1,000 and 10 Gb/s links, an update period of 50 µs 
is enough to keep the overhead below 5%. 
A  related  issue  is  the  computational  overhead  of  distributed 
scheduling. Since the update period is necessarily a constant mul-
tiple of the number of ports, there is time to perform even moder-
ately complex algorithms. For a system with n=1000 and a clock 
frequency  of  200  MHz,  the  DBL  algorithm  can  be  executed  at 
each port in 5 µs, a small fraction of the required update period. 
While more complex algorithms such as distributed OLA are more 
challenging  to  implement  in  the  required  time,  even  these  are 
feasible to implement if ∆ is at least, say ST/10. 
In  this  paper,  we  have  not  addressed  the  interconnection  net-
work itself, and how it might interact with a distributed scheduler. 
The performance of multistage interconnection networks with buff-
ered  switch  elements  has  been  studied  in  great  detail,  using  both 
analysis and simulation (representative examples of analytical stud-
ies of such systems can be found in references [3,12]). The general 
conclusion of these studies is that these systems can provide excel-
lent performance when carrying traffic that does not cause sustained 
overloads on any output links. The use of distributed scheduling can 
ensure that this condition is met, allowing one to consider intercon-
nection network performance, as a largely independent issue. Most 
performance  studies  of  these  networks  have  been  done  assuming 
switch element chips that provide buffering for just a small number 
of  cells  per  port  (the  typical  range  is  2-16)  and  these  systems  are 
capable  of  throughputs  exceeding  90%  for  switch  element  buffer 
sizes of eight or more per port. Modern ICs allow the construction 
of switch elements with over four thousand cells, allowing system 
throughputs  to  approach  100%.  With  current  technology,  a  three 
stage,  multi-plane,  Clos-type  network  using  dynamic  routing  re-
quires roughly n switch element ICs to support n 10 Gb/s links (for 
values  of  n  ranging  from  about  100  to  several  thousand).  Such  a 
network can buffer several thousand cells per external link, allowing 
it to effectively smooth out any rate variations that may occur within 
an  update  period.  Since  rate-controlled  VOQs  feed  traffic  to  the 
network in a smooth, rather than a bursty fashion, the magnitude of 
such  variations  can  be  expected  to  be  quite  limited,  allowing  the 
network to deliver cells to the outputs with only very modest queue-
ing delays.  
7. CONCLUDING REMARKS 
We  believe  that  system  architectures  that  combine  distributed 
scheduling  and  buffered,  multistage  interconnection  networks  are 
among the most scalable and cost-effective architectures for imple-
menting high performance routers. These architectures make it fea-
sible today to build systems with aggregate capacities from 1 to 100 
Tb/s. Continued improvements in Moore’s Law will allow them to 
continue  to  scale  in  both  line  speed  and  total  capacity.  The  one 
drawback that such systems have suffered from is that their perform-
ance can degenerate when they are subjected to the extreme traffic 
situations  that  can  occur  in Internet routers. While various ad-hoc 
flow control techniques have been used to address this issue, it has 
not been possible up to this point, to make rigorous statements about 
their performance under extreme traffic. The theoretical results de-
veloped  here  show  that  the  performance  of  these  systems  can  be 
directly  comparable  to  the  performance  of  unbuffered  crossbars, 
controlled by centralized schedulers. While in both system contexts, 
the scheduling algorithms with the strongest theoretical guarantees 
are not practical to implement, these algorithms provide the insight 
needed to design practical variants capable of similar performance. 
There  are  some  interesting  ways  that  this  work  could  be  ex-
tended. First, it seems possible that algorithms like DBL and distrib-
uted OLA are work-conserving for small speedups. However, prov-
ing such results seems to require either extensions to the proof tech-
niques  used  here  (adapted  largely  from  earlier  work  on  crossbar 
scheduling), or entirely new techniques. Establishing such a result 
would  be  of  great  interest  from  both  a  theoretical  and  a  practical 
perspective.  
Reference [11] describes distributed scheduling algorithms that 
support weighted-fair queueing and algorithms that seek to guaran-
tee that packets that arrive at the same time for the same output link 
are forwarded at approximately the same time on that output link. 
The results developed here can likely be extended to allow rigorous 
statements  about  the  performance  of  these  or  similar  distributed 
schedulers.  
Finally, as noted in the introduction, whereas crossbar schedul-
ers must match inputs to outputs in a one-to-one fashion, distributed 
schedulers can divide the bandwidth at inputs and outputs arbitrar-
ily. It seems likely that this difference may allow the construction of 
distributed schedulers with speedups smaller than 2. Our failure to 
prove  such  a  result  may  be  just  a  consequence  of our reliance on 
proof  methods  adapted  from  crossbar  scheduling.  Our  simulation 
studies  suggest  that  speedups  close  to  1.5  may  be  sufficient  for 
work-conservation in distributed schedulers and we have some (so 
far 
that  suggests  work-
conservation could be achievable for speedups of slightly less than 
1.6.  The  establishment  of  such  a  result  would  be  of  considerable 
practical value and would also be interesting from a purely analyti-
cal standpoint, as it would likely require different proof techniques 
than those that have been employed so far. 
inconclusive)  analytical  evidence 
REFERENCES 
[1]  Ahuja, R., T. Magnanti and J. Orlin. Network Flows, Theory, 
Applications and Algorithms. Prentice-Hall, 1993. 
[2]  Anderson, T., S. Owicki., J. Saxe and C. Thacker. “High 
speed switch scheduling for local area networks,” ACM 
Trans. on Computer Systems, 11/93. 
[3]  Bianchi, G. and J. Turner. “Improved Queueing Analysis of 
Shared Buffer Switching Networks,” Proceedings of 
Infocom, 3/93. 
[4]  Chang, C. S., D.S. Lee and Y.S. Jou, “Load balanced 
Birkhoff-von Neumann switches, Part I: one-stage 
buffering”. Computer Communications Vol. 25. pp. 611-622, 
2002. 
[5]  Chuang, S.-T. A. Goel, N. McKeown, B. Prabhakar  
“Matching output queueing with a combined input output 
queued switch,” IEEE Journal on Selected Areas in 
Communications, 12/99. 
[6]  Keslassy, I., S.-T. Chuang, K. Yu, D. Miller, M. Horowitz, 
O. Solgaard, N. McKeown, “Scaling Internet routers using 
optics.” ACM SIGCOMM, 9/03. 
[7]  Krishna, P., N. Patel, A. Charny and R. Simcoe. “On the 
speedup required for work-conserving crossbar switches,” 
IEEE J. Selected Areas of Communications, 6/99. 
[8]  McKeown, N., V. Anantharam and J. Walrand. “Achieving 
100% throughput in an input-queued switch,” Proceedings 
of Infocom, 1996. 
[9]  McKeown, N., M. Izzard., A. Mekkittikul, W. Ellersick and 
M. Horowitz. “The Tiny Tera: a packet switch core,” Hot 
Interconnects, 1996. 
[10]  McKeown , Nick. “iSLIP: a scheduling algorithm for input-
queued switches,” IEEE Transactions on Networking, 4/99. 
[11]  Pappu, P., J. Turner and K. Wong. “Distributed Queueing in 
Scalable High Performance Routers,” Proceedings of 
Infocom, 4/03. 
[12]  Szymanski, T. and S. Shaikh. “Markov Chain Analysis of 
Packet-Switched Banyans with Arbitrary Switch Sizes, 
Queue Sizes, Link multiplicities and Speedups,” Proceedings 
of Infocom, 4/89. 
[13]  Tarjan., Robert. Data Structures and Network Algorithms. 
Society for Industrial and Applied Mathematics, 1983.