title:ML-Driven Malware that Targets AV Safety
author:Saurabh Jha and
Shengkun Cui and
Subho S. Banerjee and
James Cyriac and
Timothy Tsai and
Zbigniew Kalbarczyk and
Ravishankar K. Iyer
2020 50th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN)
ML-driven Malware that Targets AV Safety
Saurabh Jha∗, Shengkun Cui∗, Subho S. Banerjee∗, James Cyriac∗,
Timothy Tsai†, Zbigniew T. Kalbarczyk∗, and Ravishankar K. Iyer∗
∗University of Illinois at Urbana-Champaign, Urbana-Champaign, IL 61801, USA.
†NVIDIA Corporation, Santa Clara, CA 94086, USA.
Abstract—Ensuring the safety of autonomous vehicles (AVs) is
critical for their mass deployment and public adoption. However,
security attacks that violate safety constraints and cause accidents
are a signiﬁcant deterrent to achieving public trust in AVs, and
that hinders a vendor’s ability to deploy AVs. Creating a security
hazard that results in a severe safety compromise (for example,
an accident) is compelling from an attacker’s perspective. In
this paper, we introduce an attack model, a method to deploy
the attack in the form of smart malware, and an experimental
evaluation of its impact on production-grade autonomous driving
software. We ﬁnd that determining the time interval during which
to launch the attack is critically important for causing safety
hazards (such as collisions) with a high degree of success. For
example, the smart malware caused 33× more forced emergency
braking than random attacks did, and accidents in 52.6% of the
driving simulations.
Index Terms—Autonomous Vehicles, Security, Safety
I. INTRODUCTION
Autonomous vehicle (AV) technologies are advertised to be
transformative, with the potential to bring greater convenience,
improved productivity, and safer roads [1]. Ensuring the safety
of AVs is critical for their mass deployment and public
adoption [2]–[7]. However, security attacks that violate safety
constraints and cause accidents are a signiﬁcant deterrent to
achieving public trust in AVs, and also hinder vendors’ ability to
deploy AVs. Creating a security hazard that results in a serious
safety compromise (for example, an accident) is attractive from
an attacker’s perspective. For example, smart malware can
modify sensor data at an opportune time to interfere with the
inference logic of an AV’s perception module. The intention is
to miscalculate the trajectories of other vehicles and pedestrians,
leading to unsafe driving decisions and consequences. Such
malware can fool an AV into inferring that an in-path vehicle
is moving out of the lane while in reality the vehicle is slowing
down; that can lead to a serious accident.
This paper introduces i) the foregoing attack model, ii) a
method to deploy the attack in the form of smart malware
(RoboTack), and iii) an experimental evaluation of its impact
on production-grade autonomous driving software. Speciﬁcally,
the proposed attack model answers the questions of what, how,
and when to attack. The key research questions addressed by
RoboTack and the main contributions of this paper are:
Deciding what to attack? RoboTack modiﬁes sensor data
of the AV such that the trajectories of other vehicles and
pedestrians will be miscalculated. RoboTack leverages situation
awareness to select a target object for which the trajectory will
be altered.
Deciding how to attack? RoboTack minimally modiﬁes the
pixels of one of the AV’s camera sensors using an adversarial
machine-learning (ML) technique (described in §IV-C) to alter
the trajectories of pedestrians and other vehicles, and maintains
these altered trajectories for a short time interval. The change in
the sensor image and the perceived trajectory is small enough
to be considered as noise. Moreover, RoboTack overcomes
compensation from other sensors (e.g., LIDAR) and temporal
models (e.g., Kalman ﬁlters).
Deciding when to attack? RoboTack employs a shallow 3-
hidden-layered neural network (NN) decision model (described
in §IV-B) to identify the most opportune time with the intent of
causing a safety hazard (e.g., collisions) with a high probability
of success. In particular, the proposed NN models the non-
linear relationship between the AV kinematics (i.e., distance,
velocity, acceleration) and attack parameters (i.e., when and
how long to attack). We use a feed-forward NN because neural
networks can approximate complex continuous functions as
shown in the universal function approximation theorem [8].
Assessment on production software. We deployed Rob-
oTack on Apollo [9], a production-grade AV system from
Baidu, to quantify the effectiveness of the proposed safety-
hijacking attack by simulating ∼
2000 runs of experiments
for ﬁve representative driving scenarios using the LGSVL
simulator [10].
The key ﬁndings of this paper are as follows:
1) RoboTack is signiﬁcantly more successful
in creating
safety hazards than random attacks (our baseline) are.
Here, random attacks correspond to miscalculations of the
trajectories (i.e., trajectory hijacking) of randomly chosen
non-AV vehicles or pedestrians, at random times, and for
random durations. This random attack condition is the most
general condition for comparison, although we also show
results for a much more restrictive set of experiments.
RoboTack caused 33× more forced emergency braking
than random attacks did, i.e., RoboTack caused forced
emergency braking in 75.2% of the runs (640 out of 851).
In comparison, random attacks caused forced emergency
braking in 2.3% (3 out of 131 driving simulations).1
2) Random attacks caused 0 accidents, whereas RoboTack
caused accidents in 52.6% of the runs (299 out of 568).
3) RoboTack had higher a success rate in attacking pedestrians
(84.1% of the runs that involved pedestrians) than in at-
tacking vehicles (31.7% of the runs that involved vehicles).
4) Apollo’s perception system is less robust in detecting
pedestrians than in detecting other vehicles. RoboTack
automatically discerns that difference, and hence needs
only 14 consecutive camera frames involving pedestrians
1These numbers, while seemingly contradictory, are consistent, as we will
show in §VI.
978-1-7281-5809-9/20/$31.00 ©2020 IEEE
DOI 10.1109/DSN48063.2020.00030
113
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:50 UTC from IEEE Xplore.  Restrictions apply. 
1
Perception System
Ot
t
ˆSt−1
F1
ˆSt
F2
Corrected
D
M
Noisy
(1)
It
S
Sensors
0
EV
3
Before Attack
a) Move-out
After Attack
T
T
i
n
o
s
u
F
r
o
s
n
e
S
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)v1
1, (cid:2)x1, (cid:2)v1
Wt
(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)(cid:2)v2
2, (cid:2)x2, (cid:2)v2
TO
EV
b) Move-in
Sensors
GPS/IMU
RADAR
LIDAR
Camera(s)
5
Trajectory 
Hijacker
4
Safety Hi‐
jacker
It
Localization 
n
o
i
t
p
e
c
r
e
P
Wt
+
n
o
i
t
c
d
e
r
P
i
1
2
Scenario 
Matcher
3
AttackVectors
∗
(α
)
c) Disappear
r
e
l
l
(\1)
It
i
&
g
n
n
n
a
P
l
l
o
r
t
n
o
c
o
r
t
n
o
C
D
P
I
At
Figure 1: Overview of the ADS perception system and the proposed attack in RoboTack.
to cause accidents, while needing 48 consecutive camera
frames that only involve other vehicles to cause accidents.
Comparing RoboTack with adversarial
learning. Past
work has targeted deep neural networks (DNNs) used in the
perception systems of AVs to create adversarial machine-
learning-based attacks [11]–[14] that were shown to have
successful results (such as by causing misclassiﬁcation and/or
misdetection of a stop sign as a yield sign), and object
trackers [15]. The goal of this line of research is to create
adversarial objects on the road that fool the AV’s perception
system. However, such attacks 1) are limited because DNNs
represent only a small portion of the production autonomous
driving system (ADS) [9], and 2) have low safety impact due
to built-in compensation provided by temporal state-models
(which provide redundancy in time) and sensor fusion (which
provides redundancy in space) in ADS, which can mask the
consequences of such perturbations and preserve AV safety
(as shown in this paper, and by others [16]). To summarize,
adversarial learning tells one only what to attack. In contrast,
as we discuss in detail in §III-D, RoboTack tells one what,
when, and how to attack, making it highly efﬁcient in targeting
AV safety. Moreover, previous attacks have been shown only
on one camera sensor without considering i) the sensor fusion
module, and ii) the AV control loop (i.e., it considers only
statically captured video frames without running a real ADS).
In contrast, we show our attack on an end-to-end production-
grade AV system using a simulator that provides data from
multiple sensors.
II. BACKGROUND
A. Autonomous Driving Software
of the paper. Fig. 1 illustrates the basic control architecture of
an AV (henceforth also referred to as the Ego vehicle, EV). The
EV consists of mechanical components (e.g., throttle, brake, and
steering) and actuators (e.g., electric motors) that are controlled
by an ADS, which represent the computational (hardware and
software) components of the EV. At every instant in time, t, the
ADS system takes input from sensors It (e.g., cameras, LiDAR,
GPS, IMU) and infers Wt, a model of the world, which
consists of the positions and velocities of objects around the
EV. Using Wt and the destination as input, the ADS planning,
routing, and control module generates actuation commands
(e.g., throttle, brake, steering angle). Those commands are
smoothed out using a PID controller [17] to generate ﬁnal
actuation values At for the mechanical components of the EV.
The PID controller ensures that the AV does not make any
sudden changes in At.
B. Perception System
Deﬁnition 1. Object tracking is deﬁned as the process of
identifying an object (e.g., vehicle, pedestrian) and estimating
its state st at time t using a series of sensor measurements
(e.g., camera frames, LIDAR pointcloud) observed over time.
The state of the object is represented by the coordinates and
the size of a “bounding box” (bbox) that contains the object.
That estimated state at time t is used to estimate the trajectory
(i.e., the velocity, acceleration, and heading) for the object.
Deﬁnition 2. Multiple object tracking (MOT) is deﬁned as
the process of estimating the state of the world denoted by
ˆSt = (ˆs1
t ), where Nt represents the number of
objects in the world at time t, and ˆsi
t is the state of the ith
object.2
t , ..., ˆsNt
t , ˆs2
We ﬁrst discuss the terminologies associated with the au-
tonomous driving system (ADS) that are used in the remainder
2In this paper, the boldface math symbols represent tensors and nonbolded
symbols represent scalar values in tensors.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:29:50 UTC from IEEE Xplore.  Restrictions apply. 
114
t , o2