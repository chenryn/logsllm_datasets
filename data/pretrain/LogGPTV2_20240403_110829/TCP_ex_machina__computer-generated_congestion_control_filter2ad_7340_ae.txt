### Data and Throughput Analysis

- **Data Sizes:**
  - 100 KBytes
  - 1 MByte

- **Throughput Values (with Standard Deviation):**
  - Compound Throughput: 2.09 (.11) Mbps
  - Cubic Throughput: 1.31 (.16) Mbps, 1.28 (.11) Mbps
  - Other Throughputs: 1.79 (.18) Mbps, 2.75 (.27) Mbps, 3.9 (.13) Mbps

### Observations

RemyCC performs well at low duty cycles by quickly capturing spare bandwidth. At higher duty cycles with low mean off times, Cubic and Compound tend to capture a larger share of the bandwidth. However, the performance differences are small enough to suggest that a RemyCC designed for competing with more aggressive protocols could close this gap while maintaining high performance when competing with other RemyCCs.

### Graphical Representation

- **Throughput (Mbps) vs. Queueing Delay (ms):**
  - Vegas, Remy (δ=0.1, δ=1, δ=10), Cubic, Compound, NewReno, XCP, Cubic/sfqCoDel, CoDel
  - Normalized throughput share vs. RTT (ms)
  - Cubic-over-sfqCoDel, RemyCC (δ = 0.1, δ = 1, δ = 10)

### Cellular Link Performance

On variable cellular link traces, Remy's schemes outperformed existing congestion-control algorithms (end-to-end or otherwise) when the maximum degree of multiplexing was 4 or less. They also outperformed end-to-end schemes and sfqCoDel when the multiplexing was 8 or less. As network conditions deviated from the prior assumptions, Remy's performance declined but remained competitive with traditional TCP congestion control on the networks tested.

### Discussion

Much remains unknown about the capabilities and limits of computer-generated algorithms, especially decentralized ones that cooperate indirectly across a network to achieve a common goal. Although RemyCCs perform well on networks within or near their design parameters, even outperforming in-network schemes, the underlying reasons for their effectiveness are not yet fully understood.

We have attempted to create algorithms that surpass the generated RemyCCs without success, suggesting that Remy may have achieved something substantive. However, reverse-engineering the dozens of rules in a RemyCC to understand their purpose and function is challenging. Future RemyCCs for broader network classes will likely be even more complex, exacerbating this issue.

### Approach and Trade-offs

Our approach increases endpoint complexity to reduce overall network behavior complexity. Traditional TCP congestion control specifies simpler behavior for each endpoint, but the resulting multiuser network behavior is often suboptimal and variable. In contrast, our approach focuses on maximizing a well-specified overall objective, even if it means more complex endpoint algorithms. This trade-off is advisable because modern endpoints can handle complex algorithms almost as easily as simple ones, and users care more about the quality and consistency of overall behavior.

### Synthesis-by-Simulation

This approach makes it easier to compare competing congestion control proposals. Differences between two computer-generated algorithms can be attributed to different assumptions about expected networks, different goals, or varying levels of optimization. This allows implementers to make rational choices among competing options.

### Challenges and Future Work

- **Robustness:** We need better methods to predict the robustness of RemyCCs to unexpected inputs.
- **Performance Range:** How would a RemyCC designed for a 10,000-fold range of throughputs and RTTs perform?
- **Real-World Testing:** While we have some robustness against simulator peculiarities, real-world performance needs to be validated.

### Prior Knowledge Impact

We investigated the performance benefits of having specific prior information about the network and the consequences of incorrect information. Two RemyCCs were designed for a network with a known minimum RTT of 150 ms. One assumed a fixed link speed of 15 Mbps, while the other spanned a 10× range from 4.7 Mbps to 47 Mbps. The results show that more specific prior knowledge improves performance when correct but deteriorates when assumptions are violated.

### Summary of Results

Using a few CPU-weeks of computation, Remy produced several congestion-control algorithms, which outperformed human-generated algorithms on simulated networks. On networks adhering to design-time assumptions, Remy's end-to-end algorithms outperformed all human-generated congestion-control algorithms, including those with in-network support. RemyCC (δ = 0.1) achieved >1.7× gains in median throughput and >2.7× reductions in median queueing delay compared to Cubic and Compound. Against Cubic-over-sfqCoDel, RemyCC achieved a 40% increase in median throughput and a 7.8× decrease in median queueing delay.

### Conclusion

This paper explores whether distributed congestion-control algorithms for heterogeneous and dynamic networks can be designed by specifying assumptions and policies, letting computers work out the details. Our findings suggest significant potential for this approach, though much future work is needed. Remy, a program that designs end-to-end congestion-control algorithms, outperforms the best-known techniques in scenarios with varying network parameters. A computer-generated approach that maximizes explicit functions of throughput and delay may be the way forward for the networking community.

### Acknowledgments

We thank Anirudh Sivaraman, Leslie Kaelbling, Christopher Amato, Scott Shenker, Ranjita Bhagwan, Frans Kaashoek, and Nickolai Zeldovich for their contributions and support. We also acknowledge the members of the MIT Center for Wireless Networks and Mobile Computing (Wireless@MIT) and the NSF grant CNS-1040072.

### References

[References listed as provided, with proper formatting and citation.]

---

This revised text aims to be more structured, clear, and professional, with improved coherence and readability.