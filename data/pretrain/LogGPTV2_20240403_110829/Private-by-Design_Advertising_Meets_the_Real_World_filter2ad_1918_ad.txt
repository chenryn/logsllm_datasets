then use the user proﬁle knowledge to link together multiple reports. The ﬁrst
step may be made difﬁcult by making (Un/Uc) relatively coarse-grained, thus
making it harder to uniquely factor out its components. The second step is made
difﬁcult simply by the sheer number of clients that are likely to have similar user
scores. Thus, we conclude that exposure of (Un/Uc) does not constitute a serious
threat.
27
Broker analyzes ((Bn ⇥ Gn) ⇥ (Un/Uc)) (RaC)
This value is more difﬁcult to reverse engineer than (Un/Uc), and is therefore
also not a threat.
Client analyzes (B ⇥ G) (RaC)
An advertiser can use this value to determine the broker quality score G as-
signed by the broker to its own ads. This can be done by the advertiser simply
creating a client that receives its own ads, and using the known value of B to fac-
tor out G. Whether this is a problem needs to be decided by the broker, though
we point out that today Google reveals a coarse-grained quality score to its ad-
vertising customers.
Transmitting the product (B ⇥ G) to the client also reveals the overall rank-
ing of an ad to anyone running a client, including the advertiser’s competitors.
From this, they can also roughly estimate the advertiser’s bids. It is not clear
that this is a problem, for two reasons. First, in today’s advertising systems, an
advertiser can see how its competitors rank relative to itself simply by observing
how ads are displayed. RaC makes it easier and cheaper to obtain this ranking
information, but does not fundamentally change an advertiser’s ability to do so.
Second, historically in traditional advertising (print, TV, radio), advertisers can
monitor how much advertising their competitors do, and can generally know
the cost of that advertising. While certainly all things being equal advertisers
would like to keep this information secret, historically the inability to do so has
not, for the most part, prevented companies from advertising.
28
If exposing the product (B⇥G) to the client is an acceptable privacy loss, then
RaC should be the preferred auction method for its overall simplicity and lower
overhead. If it is not acceptable, then RaB and RaT, which both avoid exposing
this information, may be preferred.
4.2 Auction Properties
In this section, we discuss the various shortcomings of each of the approaches
with respect to the auction properties, especially ranking results and revenue.
4.2.1 System delays
There are several potential delays in the private-by-design advertising systems
that can change both the rankings and the computed CPC. With RaC, there is a
delay between when the ad was transmitted and adbox time when the ranking
takes place. With RaB and RaT, there is a delay between when the ranking
occurs and adbox time when the ranking is actually used. In either case, the bid
B or the broker quality score G used for ranking may no longer be correct, and
an out-of-date ranking takes place.
During the design of the auction approaches, these delays were a major con-
cern. As it turns out, at least for the auction data from Bing search advertis-
ing auctions (Chapter 5), the delays have only a minor impact on both broker
revenue and advertiser costs, even when the delay is several hours or a day.
Nevertheless, this may not be the case for other systems or future systems, and
so it remains important that these delays are engineered to be minimal. This
29
could be done, for instance, by having clients frequently request small numbers
of new ads.
4.2.2 Client selection
A problem encountered by the private-by-design advertising is that the broker
does not know which clients are best to send an ad to. For instance, suppose
that some number of clients M have requests ads for watches. The broker does
not know which clients may be interested in cheap watches, and which in ex-
pensive. The advertiser, however, might not have enough budget to pay for all
the clicks that would result if all watch ads are sent to all interested users.
Lets assume that the broker knows the clicks per delivered-ad rate. From
this, it can determine the number of clients N that should receive the ad with-
out exhausting the advertisers budget. If it randomly chooses N clients among
the interested clients, then it will not be sending all ads to the most interested
clients.
One way to solve this problem is for the broker to go ahead and send the
ad to all interested clients, but to also send a parameter giving the minimum
user score that a client must have in order to show the ad. This way, only the
best matching clients will show the ad. The broker may be able to establish
the expected click per delivered-ad rate for various user scores, and therefore
predict the setting of the user score based on the number of clients. If the broker
predicts too high, then it can lower the minimum user score and send this to
clients, thus causing more clients to show the ad.
30
4.2.3 Overhead and Latency
RaB and RaT add load to real-time advertising systems (systems that do not
prefetch ads at the client), to the point where neither is a very attractive op-
tion.The extra round trip to the broker (RaB) or third-party (RaT) has to happen
in real-time because the ads must be delivered in time to render the web page.
This is compounded by the fact that a number of ads must be shipped around
for each page load.
If all the ads are to be delivered to the client, the sheer volume of ads that
must be transmitted to the broker or third-party for ranking can also be a scal-
ing challenge. The Bing trace contained 15M unique ads for a single day (see
Section 5.3), with an average ad lifetime of roughly 9 days. This translates into
a little over 1.7M new ads per day per client. Each of these ads is given a user
score by each client, which is then transmitted to a broker or third party for rank-
ing. The Bing trace also counted 14M unique clients. This then translates into 24
tera-ads per day that must be ranked. Fortunately, this ranking function can be
split over many machines (with each taking some fraction of the total number
of ads to rank) without hurting the accuracy of the ranking signiﬁcantly (as-
suming that each machine has a representative sample of ads). Therefore, while
challenging, this sort is doable.
Though this is not related to the auction per se, note that each ad is roughly
250 bytes of text including the URL. Even ignoring client updates to B and G
values over the lifetime of an ad, this still requires 425MB of ads downloaded
per day per client (uncompressed), or about 43MB compressed (in bulk).
31
4.2.4 Auction Scope
An important aspect of the auction is the scope of the auction, by which we
mean the set of ads that compete in any given auction. As a general rule, the
more ads that compete, the higher the CPC. This is simply because the more
ads there are, the more probabilistically likely the next-ranked ad will have a
(B ⇥ G ⇥ U) closer to that of the clicked ad. On the other hand, the larger the
auction scope, the less fair it is in the sense that very different types of ads must
compete. A local pizza store may not wish to compete with Mercedes for ad
boxes.
The auction scope for search or contextual systems like Bing and Google is
the set of ads whose keywords match that of the search or web page. Today’s ad
exchanges, where advertisers bid in real time, typically for adboxes on premium
publishers, have a potentially much broader scope because any advertiser can
bid. The auction scope in a private-by-design advertising system is tunable. It
may be all ads in a client, or all ads within an ad type (i.e. an interest). What is
more, interests may be hierarchical (sports/tennis/clothing/shoes), and may be
more general or more speciﬁc, thus allowing for substantial ﬂexibility in auction
scope.
4.3 Attacks
In this section, we relax our assumption of honest-but-curious players, and con-
sider a number of malicious attacks and defenses.
32
Client click fraud
The client can commit a form of click fraud by lying about the value of
((Bn ⇥ Gn) ⇥ (Un/Uc)) (RaC) or (Un/Uc) (RaB or RaT). By inﬂating or deﬂating
these values, the client can cause advertisers to pay more or less, and cause
publishers to earn more or less. At a high level, this is very similar to normal
forms of click fraud that occur today, and in this sense our auctions do not al-
low fundamentally new forms of click fraud. Privad describes how to defend
against click-fraud even with anonymizing brokers [32]. The same method may
be used here. The basic idea is that the proxy tags reports with a per-report
unique identiﬁer. If the broker suspects click fraud, it informs the proxy of the
report ID of the suspicious report.
If a given client is suspected more times
than some threshold, its reports can be tagged by the proxy as coming from a
suspected client. In some cases the broker may suspect click fraud simply be-
cause the second price is impossibly high (i.e., higher than the ﬁrst price bid).
In most cases, however, the broker may use a variety of additional mechanism
to detect an ongoing click fraud attack. As described in Section 6.1, these mech-
anisms range from using statistical analysis of historical per-publisher and per-
advertiser click-trough rates to proactively setting up “bait ads”.
Proxy ﬁngerprints client user scores and resulting ranking (RaB and RaT)
It is difﬁcult but conceivable in RaB and RaT that the proxy could determine
user proﬁles through observation of the client user scores and rankings. For in-
stance, the proxy could establish a number of fake clients that pretend to have
various proﬁle attributes, and establish ﬁngerprints of the resulting user scores
and rankings. One way to do this might be to determine (B ⇥ G) given user
33
scores and corresponding ad ranks, and use these values as the ﬁngerprint.
The proxy could then compare these ﬁngerprints with the corresponding ﬁn-
gerprints of real clients. It could be that the signal-to-noise ratio is high enough
to successfully pull off this attack. One way to prevent this would be to encrypt
user scores and rankings. The user scores could be encrypted using the brokers
(RaB) or third-party’s (RaT) public key, and the rankings could be encrypted us-
ing symmetric keys created by the clients and conveyed securely to the broker
or third-party. These symmetric keys would be frequently modiﬁed to prevent
the broker or third-party from linking user scores with the same client, and pos-
sibly launching a similar ﬁngerprint attack.
Broker manipulates [Bn ⇥ Gn] (RaB, RaT)
A malicious broker could launch an attack on a private-by-design advertising
system to identify clients by inserting unique IDs into the encrypted ﬁelds [B, G]
or [B ⇥ G]. Once a client is identiﬁed in this way, unlinkability is lost, and the
broker can build up client proﬁles. The broker can then potentially identify
the client through external means. There is some cost to this approach, as the
broker must “waste” ads to do the tracking1. The homomorphic encryption
variant described in Section 3.2.4 defends against this attack. Because the client
multiplies the received encrypted ﬁelds with other ﬁelds, the values generated
by the broker are obscured.
1 One might argue that the same attack can be launched simply be creating unique Ad IDs
transmitted in the clear. However, this attack can at least be detected by third parties, for in-
stance running honey-farms of clients.
34
Broker sees client user scores (RaB)
The noise added to user scores, combined with the anonymization of the client
address and unlinkability of interest channels makes it extremely difﬁcult for
the broker to build up a user proﬁle. Having said that, we do not prove that
it is impossible, and it could well be that a clever broker could ﬁgure out how
to create a kind of user-proﬁle ﬁngerprint on otherwise anonymized channels.
From this, the broker could in theory link together channels with identical ﬁn-
gerprints, thus violating unlinkability claim. The RaT approach eliminates this
possibility altogether.
4.4 Discussion
Overall, we ﬁnd that RaB is a weak scheme because it opens up a ﬁngerprinting
attack at the broker. RaT solves this problem, though at the expense of requiring
yet another administratively distinct and non-colluding entity. Nevertheless,
we consider it to be a better alternative to RaB.
If exposing the (B ⇥ G) product to the client is not a problem for the broker
and advertisers, then RaC is better than RaT because it is simpler, incurs less
overhead and latency, and does not require the third party. In addition, it has
no issues here with respect to user score churn, because ranking takes place at
ad view time. If exposing (B ⇥ G) is a problem (this information, however, is
even today indirectly revealed through the ad positions in adboxes), then RaT
appears to be a reasonable approach.
35
CHAPTER 5
TRACE-BASED SIMULATION: EFFECT OF CHURN
Section 4.2 describes how various delays in all three auction systems may
distort rankings and CPC computation. How detrimental this delay is depends
on how much churn there is. Churn may affect rank, CPC value, and ultimately
revenue. RaC is affected by churn in B and G, while RaB and RaT are addition-
ally affected by churn in U. In this chapter, we use trace data from Microsoft’s
Bing advertising platform to study in depth the effect of auction delays on B and
G from both the advertiser and broker perspective. We ﬁnd that, while churn
exists, it has only a negligible impact on broker revenue and advertiser costs.
5.1 What Causes Churn?
B ⇥ G for an ad changes when either B or G changes. B can change in one of
three ways: ﬁrst, the advertiser can manually update the bid; second, the ad
network can automatically update the bid (as directed by the advertiser); third,
a third-party may update the bid on the advertiser’s behalf. Each of these has
different churn characteristics:
Advertiser: Manual updates, we believe, cause very little churn since they
are reactive over a long feedback cycle. Advertisers receive updated campaign
information (i.e., how many clicks, actual amount charged, budget left) at fairly
coarse intervals (few times a day). This limits the number of informed changes
to their campaigns.
Ad Network: The advertiser can invoke functionality provided by the ad net-
work to optimize his bidding strategy. For example, the ad network may al-
36
low the advertiser to set a preferred rank (e.g., position 4), and the ad network
automatically lowers or raises the bid to satisfy the request based on the mar-
ket. Other examples may include automatically modifying bids to meet a tar-
get number of impressions per day (while still being charged only for clicks), or
modifying bids based on time of day etc. Some of this functionality (e.g., modify
bids based on time-of-day) can be implemented in the client and would there-
fore not result in any added churn. Other functionality (e.g. preferred rank)
tends to be implemented today as a periodic update (once every few hours).
Third Party: Search Engine Optimization (SEO) companies optimize their
client’s bidding strategy in real-time [21] e.g., based on trending terms, real-
time click-through rates, etc. This could potentially result in high bid churn,
however, due to the premium nature of these SEO services, only a small number
of ads would be affected.
Aside from changes in B, G can also change. Recall G in our model is a
function of what the broker knows: G is computed based on the ad (past CTR,
landing page quality, etc.). G is largely a property of the ad itself, which we do
not expect to change quickly or dramatically. In any event, our Bing auction
trace unfortunately does not allow us to validate our assumption since it does
not isolate user-derived components of G from other components.
5.2 How Does Churn Affect Auctions?
Today auctions take place at the time when an ad is displayed to the user; rank-
ing and CPC calculations can immediately reﬂect any changes in B or G. Privacy
compatible auction designs described in Chapter 3 are limited in terms of how
37
fast new B and G information can be incorporated. Since G does not rapidly
change over time or can be engineered to remain relatively stable (e.g., using U
to reﬂect short-term changes in click probability), the main source of churn is
the changes in B. To understand the effects of churn in B values, we simulate
auctions that use stale B information for ranking and CPC computation, and
then compare the resulting ranking and CPC computation with auctions that
use up-to-date B information.
5.3 Dataset
For our trace driven simulations, we sampled around 2TB of log data from
Bing’s auction engine spanning a 48 hour period starting September 1, 2010.
The data covers over 150M auctions for over 18M unique ads shown to North
American Bing search users across all search topics. The trace record for an auc-
tion lists all the ads that participated in it (whether the ad was ultimately shown
or not), the bids corresponding to each ad, the corresponding quality scores, and
which if any of the ads were ultimately clicked by the user.
5.4 Methodology
We re-compute auction rankings and the CPC for each auction in our dataset
using stale bid information; we vary staleness from 1 minute to 2 days.
Auction rankings are re-computed using bid and quality data from the trace.
Since our trace does not show when the advertiser updated the bid, we infer the
38
time based on multiple auctions that a given ad participates in. If the bid for
the ad is the same for two consecutive auctions, we infer that the bid did not
change during that interval. If the bid is different, we infer that the bid changed
sometime between the two auctions; we use the mid-point as the time of change.
To simulate an auction at time T with stale information from d minutes ago, we
simply use the bids current as of time T   d in our trace. The quality score in
the trace is based on user features (e.g., search query), which correspond to U in
private auctions; since the client always has the current value of U we use the
same quality score for simulated auctions as in the trace.
CPCs are re-computed based on the re-computed auction rankings (using
the second-price formula of Equation 3.2). In other words, for an adbox at time
T in the trace, we compute the ranking based on bid values recorded at time
T   d and populate the adbox using resulting ranking. If the user clicked on an
ad in this adbox, the bid of the next lower ranked ad Bn that we use in the CPC
computation is the stale Bn taken at time T   d.
One limitation we face is that we cannot predict the change in user behavior
when auction rankings change. Consider, for example, two ads A1 and A2 where
in the trace they are ranked 1 and 2, while in the simulated stale auction they
are ranked 2 and 1 respectively. If the user clicked A2 in the trace, what might
we expect the user to click in our simulation? One option is to model the user as
clicking the same ad he clicked in the trace; thus in this case the user clicks A2 in
the simulation. Another option is to model the user as clicking the same position
he did in the trace; in this case the user clicks position 2 (A1 in the simulation).
In reality, the user model is neither of these two extremes — it is well-known
that both ad content and rank effect click-through rates (CTR) [27]. To account
39
for this, we simulate 5 user models: 1) same position, 2) 75% same position and
25% same ad, 3) 50%-50%, 4) 25% same position and 75% same ad, and 5) same
ad. Thus we establish an envelope of possible user behavior to get a sense of the
upper- and lower- bounds of our simulation results. Note that always clicking
on the same ad is a strictly conservative estimate. This is because an ad that
was clicked in the trace but is not shown to the user in our simulation (due to
being ranked too low) would not get clicked; at the same time, under the same-
ad model, an ad that was not shown in the trace (due to being ranked too low)
and was therefore not clicked would have no chance of getting clicked even if
it were to be shown to the user in the simulation. This asymmetry biases the
simulation towards fewer clicks (and therefore lower revenues). The only user
model immune to this limitation is the same-position model.
A second limitation we face is that we cannot predict how advertisers would
change their bidding strategy in response to auctions being based on stale infor-
mation. Enterprising advertisers or SEOs, may for instance, attempt to predict
what bid they might want to make one hour hence, and enter it into the system
well in advance. Advance bidding would reduce the effective staleness of infor-
mation. For our purposes, we assume the bidding strategy does not change.
5.5 Simulation Results
Overall our simulations show that there is no appreciable change in broker rev-