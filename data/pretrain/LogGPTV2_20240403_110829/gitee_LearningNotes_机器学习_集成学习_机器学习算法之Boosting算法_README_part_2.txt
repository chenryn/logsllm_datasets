- 步骤1：所有分布下的基础学习器对于每个观测值都应该有相同的权重
- 步骤2：如果第一个基础的学习算法预测错误，则该点在下一次的基础学习算法中有更高的权重
- 步骤3：迭代第2步，直到到达预定的学习器数量或预定的预测精度。
最后，将输出的多个弱学习器组合成一个强的学习器，提高模型的整体预测精度。Boosting总是更加关注被错误分类的弱规则。
Boosting算法的底层可以是任何算法，关于boosting算法，我们需要知道其中最有名的3个算法：
- AdaBoost(Adaptive Boosting)
- GBM(Gradient Boosting Machine)
- XGBoost
## AdaBoost
假设我们有一个分类问题，给定一个训练样本集，从这个分类问题中求出一个粗糙的分类规则（弱分类器）往往要比求一个精确的分类规则（强分类器）容易得多。提升方法便是从弱学习算法出发，反复学习得到一系列弱分类器（又称为基本分类器），然后通过组合这些弱分类器来构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（或说训练数据的权值分布），来针对不同的训练分布调用弱学习算法学习一系列弱分类器。这样一来，对于提升方法而言，就有了两个问题需要去解决：
- 在每一轮如何改变训练数据的权值或概率分布？
- 如何将弱分类器组合成一个强分类器？
AdaBoost针对第一个问题的做法是提高那些被前一轮弱分类器错误分类样本的权值，并降低那些被正确分类的样本的权值。经过一轮的权值加大后，后一轮的弱分类器就会更关注那些没有被正确分类的样本。持续下去，分类问题便会被一系列弱分类器“分而治之”。而对于第二个问题，即弱分类器的组合，AdaBoost采取加权多数表决法，具体的所，就是加大误差率小的弱分类器的权值，使其在表决中起更大的作用，另一方面，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。
下面这张图对Ada-boost做了恰当的解释：
- Box 1: 你可以看到我们假设所有的数据点有相同的权重（正号、负号的大小都一样），并用一个决策树桩D1将它们分为两部分。我们可以看到这个决策树桩将其中的三个正号标记的数据点分类错误，因此我们将这三个点赋予更大的权重交由下一个预测树桩进行分类。
- Box 2: 在这里你可以看到三个未被正确分类的（+）号的点的权重变大。在这种情况下，第二个决策树桩D2试图将这三个错误的点准确的分类，但是这又引起新的分类错误，将三个（-）号标记的点识别错误，因此在下一次分类中，这三个（-）号标记的点被赋予更大的权重。
- Box 3: 在这里三个被错误分类的（-）号标记的点被赋予更大的权重，利用决策树桩D3进行新的分类，这时候又产生了新的分类错误，图中用小圆圈圈起来的一个负号点和两个正号点
- Box 4: 在这里，我们将D1、D2和D3三个决策器组合起来形成一个复杂的规则，你可以看到这个组合后的决策器比它们任何一个弱分类器表现的都足够好。
![image-20200203223007585](images/image-20200203223007585.png)
该原理可同样用于回归算法。它在不同权重的训练数据集上生成一系列的弱学习器，最开始的时候所有的数据拥有相同的权重，对于第一个分类器没有正确分类的点则在下一个决策器中的权重将会加大，作为一个迭代的过程，直到分类器数量达到预定值或预测精度达到预定值。大多数情况下，我们在AdaBoost中使用decision stamps。但是如果它可以接受带有权重的训练集，我们也可以使用其他任何的机器学习算法作为基础学习器。我们可以使用AdaBoost算法解决分类和回归问题。
Python 代码：
``` python
from sklearn.ensemble import AdaBoostClassifier # For Classification
from sklearn.ensemble import AdaBoostRegressor  # For Regression
from skleran.tree import DecisionTreeClassifier
dt = DecisionTreeClassifier()
clf = AdaBoostClassifier(n_estimators=100, base_estimator=dt, learning_rate=1)
# Above I have used decision tree as a base estimator, you can use any ML learner as base estimator if it accepts sample weight
clf.fit(x_train, y_train)
```
可以调整参数以优化算法的性能：
- n_estimators：它控制了弱学习器的数量
- learning_rate：控制在最后的组合中每个弱分类器的权重，需要在learning_rate和n_estimators间有个权衡
- base_estimators：它用来指定不同的ML算法。
也可以调整基础学习器的参数以优化它自身的性能。
## Gradient Boosting
Gradient Boosting是一种Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。这句话有一点拗口，损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错（其实这里有一个方差、偏差均衡的问题，但是这里就假设损失函数越大，模型越容易出错）。如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度（Gradient)的方向上下降。
![image-20200203223042274](images/image-20200203223042274.png)
**偏差和方差**
广义的偏差（bias）描述的是预测值和真实值之间的差异，方差（variance）描述距的是预测值作为随机变量的离散程度。[《Understanding the Bias-Variance Tradeoff》](http://scott.fortmann-roe.com/docs/BiasVariance.html)当中有一副图形象地向我们展示了偏差和方差的关系：
![image-20200203224348159](images/image-20200203224348159.png)
模型的偏差是一个相对来说简单的概念：训练出来的模型在训练集上的准确度。要解释模型的方差，首先需要重新审视模型：模型是随机变量。设样本容量为n的训练集为随机变量的集合(X1, X2, …, Xn)，那么模型是以这些随机变量为输入的随机变量函数（其本身仍然是随机变量）：F(X1, X2, …, Xn)。抽样的随机性带来了模型的随机性。
定义随机变量的值的差异是计算方差的前提条件，通常来说，我们遇到的都是数值型的随机变量，数值之间的差异再明显不过（减法运算）。但是，模型的差异性呢？我们可以理解模型的差异性为模型的结构差异，例如：线性模型中权值向量的差异，树模型中树的结构差异等。在研究模型方差的问题上，我们并不需要对方差进行定量计算，只需要知道其概念即可。研究模型的方差有什么现实的意义呢？我们认为方差越大的模型越容易过拟合：假设有两个训练集A和B，经过A训练的模型Fa与经过B训练的模型Fb差异很大，这意味着Fa在类A的样本集合上有更好的性能，而Fb反之，这便是我们所说的过拟合现象。我们常说集成学习框架中的基模型是弱模型，通常来说弱模型是偏差高（在训练集上准确度低）方差小（防止过拟合能力强）的模型。
基于boosting框架的整体模型可以用线性组成式来描述，其中hi(x)为基模型与其权值的乘积：
$$
F(x)=\sum_{i}^{m} h_{i}(x)
$$
根据上式，整体模型的训练目标是使预测值F(x)逼近真实值y，也就是说要让每一个基模型的预测值逼近各自要预测的部分真实值。由于要同时考虑所有基模型，导致了整体模型的训练变成了一个非常复杂的问题。所以，研究者们想到了一个贪心的解决手段：每次只训练一个基模型。那么，现在改写整体模型为迭代式：
![image-20200203225353714](images/image-20200203225353714.png)
这样一来，每一轮迭代中，只要集中解决一个基模型的训练问题：使Fi(x)逼近真实值y。
使Fi(x)逼近真实值，其实就是使hi(x)逼近真实值和上一轮迭代的预测值Fi−1(x)之差，即残差y−Fi−1(x)。最直接的做法是构建基模型来拟合残差。研究者发现，残差其实是最小均方损失函数的关于预测值的反向梯度：
![image-20200203225417396](images/image-20200203225417396.png)
也就是说，若Fi−1(x)加上拟合了反向梯度的hi(x)得到Fi(x)，该值可能将导致平方差损失函数降低，预测的准确度提高。
引入任意损失函数后，我们可以定义整体模型的迭代式如下：