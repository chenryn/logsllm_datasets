finished with prefetching, e2, that is e2 < ea; (3) the coordinator
reads all e1’s and e2’s; (4) the coordinator designates the epoch
leader l1, which is the endpoint uploaded the largest e1; (5) the
coordinator designates the prefetch leader l2, which is the endpoint
uploaded the largest e2.
Subsequently, the following initial recovery procedure (IRP)
runs to recover epoch bitmaps and CBSS that is the most coherent
one prior to ea: (1) l1 uploads endpoint epoch bitmaps to the cloud
storage as a new cloud epoch bitmaps; (2) l2 uploads EBSS to the
cloud storage as a new CBSS; (3) the coordinator downloads epoch
bitmaps from the cloud storage and scans epoch bitmaps starting
at e2 and sequentially add each subsequent epoch (greater than the
previous epoch only by one) into the list L if there is an endpoint
in the recovery committee which wrote the epoch bitmap for; (4)
stop as soon as we found the epoch for which there is no owner
endpoint in the recovery committee.
Finally, we further forward recovery (FRP) to minimize data
loss by having endpoints to write more recent mutations which
may exist across endpoints’ EBSS: (1) the coordinator uploads L
and deletes epoch bitmaps for epochs newer than those included
in L; then notifies all other endpoints via the messaging service; (2)
endpoints download L; (3) endpoints upload dirty blocks for the
epoch in L for which they were the owner; (4) endpoints notify the
coordinator once they are done; (5) once the coordinator is notified
by all endpoints in the recovery committee, it notifies all others
about the completion of the recovery procedure.
𝑴2Endpoint ACloud Storage AEndpoint BEndpoint C𝑴1𝑴2𝑴3Cloud Storage BTemporary FailureTampering/FailureOwner𝑴1𝑴4𝑴1+𝑴2𝑴3From Endpoint AReplacement & Recovery𝑴New Mutation𝑴Replicated MutationCloud StorageTime Flow𝑴1𝑴2𝑴3From Endpoint B𝑴4𝑴5𝑴5𝑴1+𝑴2+𝑴3From Cloud Storage B𝑴4 is lost indefinitely𝑴5 must be discardedReco-very𝑴3Resume292Rocky: Replicating Block Devices for Tamper and Failure Resistant Edge-based Virtualized Desktop Infrastructure
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 9: Revisiting the example of the coherence problem,
which can be solved by utilizing mutation snapshots. Each
write is divided into a mutation snapshot, and a contiguous
sequence of mutation snapshots is needed to recover the co-
herent block device. Endpoint C and cloud storage B recover
the coherent block device by applying M1, as M2 gets lost and
M3 is not contiguous. M3 must be discarded on endpoint B.
Figure 8 gives an example that illustrates the high-level of Rocky’s
failure recovery procedure. Suppose that endpoint C and cloud stor-
age A create a hole in the sequence of mutation snapshots because
the mutation M4 is lost permanently (either due to a tampering
attack or a failure). In this case, M5 must be discarded on endpoint
B during the failure recovery to recover the coherent block de-
vice that has not been tampered. Consequently, the coherent block
device state we can recover is the one with a mutation snapshot
M1 + M2 from endpoint A and M3 from endpoint B applied sequen-
tially. Once cloud storage B gets the mutation snapshots uploaded,
endpoint C can replicate the merged snapshot M1 + M2 + M3 at
once.
Any component of Rocky can be tampered or failed at any point
in time and unexpectedly become unavailable. If a Rocky endpoint
temporarily fails due to network connection failures for a while or
rebooting the machine, it can simply resume the remaining work
it was doing before the failure. Because a Rocky endpoint keeps
prefetching blocks for each mutation snapshot sequentially, any
Rocky endpoints contain a coherence snapshot of the block device.
Figure 9 illustrates how Rocky’s periodic mutation snapshot up-
dates, periodic prefetch, and snapshot merging features can solve
the coherence problem described in Section 2. The key idea is that
each write sequence can be divided into multiple chunks of muta-
tion snapshots. Then, endpoints can periodically merge mutation
snapshots and prefetch to mutate endpoints’ block storage states
coherently. On the recovery, we start with finding the snapshot
that is the most up-to-date among available endpoints. We then
make the block storage state fast-forwarded by applying subsequent
mutation snapshots sequentially until we find any discontinuity.
5 EVALUATION
5.1 Prototype Implementation
Rocky is implemented in about 3K lines of Java code (https://github.
com/Kaelus/Rocky). It is based on a variant of NBD implementations
backed by FoundationDB (https://github.com/spullara/nbd.git). The
Figure 10: Read/Write Throughput Comparison between
Rocky and NBD.
NBD kernel module is equivalent to Rocky block device, which
passes block I/O to the user-level Rocky controller, which is the
main engine of the system implementing periodic mutation snap-
shot updates and periodic prefetch and snapshot merging. For end-
point epoch bitmaps, version map, endpoint block snapshot store,
Rocky uses LevelDB. For the cloud storage, Rocky uses the AWS Dy-
namoDB service, located in Seoul, Republic of Korea (ap-northeast-
2).
5.2 Throughput Measurement
Figure 10 demonstrates the benefit of prefetching. All workloads for
our experiments run on a machine equipped with Intel Core2 Quad
CPU where each core runs at 2.83 GHz, 8 GB RAM, and Samsung
SSD 860 EVO 500GB. We ran a workload, writing and reading
2 MB of data to the block device directly using the well-known
‘dd’ utility tool. To remove the buffer cache effect, we flushed all
buffer caches to the disk before running the workload. NBD is the
baseline that does not involve network communication with the
AWS DynamoDB service and Rocky’s implementation.
With Rocky, we varied the percentage of blocks locally present
by setting and resetting bits in the presence bitmap accordingly.
When every block is present locally, there were 8.4% and 11.9%
additional throughput overheads for writes and reads, respectively.
Write performance of Rocky is almost the same as NBD’s all the
time because processing writes in Rocky does not cause fetching
from the cloud storage. However, read performance is affected
dramatically depending on the percentage of blocks locally present.
We found that the major performance bottleneck is fetching from
the remote cloud storage. The performance drops exponentially
as the number of blocks fetched from the cloud storage increases.
Therefore prefetching as many blocks as possible is very important
to reduce perceivable performance degradation.
5.3 Reduction Ratio Measurement
We also measured the benefit of reducing repeated writes to the
same blocks by mutation snapshot and snapshot merging. We sim-
ulated a workflow involving photo editing and presentation slides
updating to see how many repeated blocks could be reduced. The
result of this study is presented in Table 1. Assuming Rocky uploads
a mutation snapshot after running the workflow, we analyzed the
Cloudlet A(Endpoint A)Cloudlet B (Endpoint B)Connector-Cloudlet A(Cloud Storage A)𝔀1𝔀2𝔀1𝔀2𝔀3𝔀3Cloudlet C (Endpoint C)𝔀1𝑴1Epoch 1Epoch 2𝑴2𝑴1𝑴2𝔀1𝔀2𝑴1𝑴3𝔀4CoherentIncoherentMutation SnapshotConnector-Cloudlet B(Cloud Storage B)𝑴1𝔀1𝔀2𝑴1𝔀1𝔀2DiscardTime FlowPresence Ratio (%)Throughput (KB/s)0250500750100012500255075100NBD write (KB/s)Rocky write (KB/s)NBD read (KB/s)Rocky read (KB/s)293ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Beom Heyn Kim and Hyoungshick Kim
Table 1: Mutation generated by a workflow of photo editing
and presentation slides creating. We indicate the number of
new ‘WRITES’ generated at each step of the workflow and
the new ‘BLOCKS’ written by those writes.
Metric
WRITES
BLOCKS
Copy-P Edit-P Create-S Edit-S
8600
8600
1040
120
2096
2056
48
0
ratio of the number of writes to the number of blocks newly added
to the mutation snapshot as our measurement metric, reduction
ratio, for the entire workflow and each task.
Initially, we started with an empty block device and created a
file system on it by running ‘mkfs.ext4’ on the device. Then, we
mounted the device on a host file system. Subsequently, our work-
flow started with copying a high-resolution photo of size 4.3 MB to
the mount point of the Rocky block device. This phase is labeled as
Copy-P. The photo was then edited using the ‘ImageMagick’ photo
editing application. Editing includes cropping, rotating, adding
frames, drawing a few lines and applying several miscellaneous
special effects, which is labeled as Edit-P.
After that, we created new presentation slides using ‘LibreOffice.’
We inserted a high-resolution photo of size 438.9 KB into the slide.
Then, we added several new slides with randomly typed texts along
with few new shapes added. This step is labeled as Create-S. Lastly,
we did some more editing to the slides by adding more slides with
more texts and shapes, which is represented by Edit-S.
Overall, 11784 writes were generated, while 10776 blocks were
newly included in the mutation snapshot uploaded after executing
the aforementioned workflow. Therefore, the reduction ratio is 8.6%
in total, but the reduction ratio is varied greatly depending on the
type of tasks. For Edit-P, there was the 100% reduction ratio because
even though there were 48 writes, no blocks were newly written
after Copy-P. Also, for Edit-S, we could gain the 88.5% reduction
ratio. Thus, we observe the tendency that tasks updating existing
files may gain significant advantages from mutation snapshot and
snapshot merging. However, we also like to note that tasks creating
a new file are not taking a significant benefit from our techniques,
as those tasks are likely to lead to a multitude of blocks that need to
be newly included in the mutation snapshot. For instance, Copy-P
shows 0% reduction ratio and Create-S shows only 1.9% reduction
ratio.
6 RELATED WORK
There have been many ransomware detection works [2, 4, 10, 15,
18, 22, 23, 30, 31, 34, 36]. However, those previous works do not
discuss a recovery mechanism. Many works also provide means
to prevent or recover for tamper-resistant storage systems against
malware [8, 19, 24, 26, 39, 44, 45]. Nonetheless, those previous
proposals do not work for replicated block devices but for a single
block device.
Several storage systems placing data on the edge for endpoints
have been studied recently [13, 17, 41]. Nevertheless, those sys-
tems usually mainly provide a key-value store interface and do not
solve the coherency problem. Although these systems can provide
low-latency over a wide-area network over the cloud, they cannot
provide a recoverable coherent block device abstraction in the pres-
ence of failures of for both endpoints and the cloud and cannot
provide tamper-resistance against malware.
A couple of research projects have explored how to build a system
image that can be replicated over a shared storage infrastructure
for endpoints [25, 32]. The Collective has developed their solution
for the enterprise environment where all endpoints are connected
within the enterprise’s private network, so it has not considered
the problems occurred by the shared storage server hosted on the
shared infrastructures like cloudlets. In addition, the Internet Sus-
pend/Resume project performed several empirical studies for real-
izing this idea. Also, ISR project maintainers proposed to use edge
computing for legacy applications via EdgeVDI [33] Nonetheless,
their proposals do not consider security and reliability aspects of
EdgeVDI.
Also, there have been many existing works for securing dis-
tributed file systems in the presence of the untrusted server compo-
nent, such as SUNDR, Sirius, Plutus [14, 21, 27]. However, they can-
not provide availability on the server failure. Hourglass, PDP, POR,
DepSky, and Hail [5–7, 20, 42] are exploring server-side solution
for untrusted cloud either by applying cryptography techniques in
a novel way or by using multiple cloud service providers. However,
they cannot provide block device abstraction and therefore do not
support coherency.
In terms of protecting consistency guarantee, Depot, Sporc, and
Venus [11, 28, 38] are related to protecting data consistency, similar
to Rocky’s coherency guarantee. However, their solutions are not
suitable for edge/fog computing environments and they are not
tamper-resistant solutions against malware. Salus and Windows
Azure [9, 43] ensures a strong consistency guarantee, but they are
not for edge computing but for the enterprise environment.
7 CONCLUSION
As 5G and edge computing technologies are emerging, we will
see the increasing number of applications taking benefits of short
network latency. EdgeVDI has been proposed recently as an appli-
cation that can provide a desktop environment to users needing
legacy applications and WAN-mobility. Among many challenges on
the road, two most significant problems are how to protect against
data tampering malware and failures, affecting data availability. We
propose a distributed replicated block device, Rocky, that enables
tamper and failure resistant EdgeVDI. Rocky stores a totally-ordered
contiguous write sequence as an append-only immutable mutation
history and replicates it across multiple cloudlets. Evaluating our
prototype, we found only about 10% performance overhead is re-
quired to provide a recoverable coherent block device abstraction
along with 88.5% to 100% reduction ratio for repeated writes.
ACKNOWLEDGMENTS
The authors would thank anonymous reviewers. Hyoungshick Kim
is the corresponding author. This work was supported Information
& communications Technology Promotion grant funded by the
Korea government (No.2018-0-00532).
REFERENCES
[1] Lawrence Abrams. 2021. Amazon AWS Outage Shows Data in the Cloud is Not Al-
ways Safe. https://www.bleepingcomputer.com/news/technology/amazon-aws-
294Rocky: Replicating Block Devices for Tamper and Failure Resistant Edge-based Virtualized Desktop Infrastructure
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
outage-shows-data-in-the-cloud-is-not-always-safe/ (last accessed: 03/18/2021).
[2] Muhammad Ejaz Ahmed, Hyoungshick Kim, Seyit Camtepe, and Surya Nepal.
2021. Peeler: Profiling Kernel-Level Events to Detect Ransomware. CoRR
abs/2101.12434 (2021).
[3] Jinwoo Ahn, Junghee Lee, Yungwoo Ko, Donghyun Min, Jiyun Park, Sungyong
Park, and Youngjae Kim. 2020. DISKSHIELD: A Data Tamper-Resistant Storage
for Intel SGX. In Proceedings of the 15th ACM Asia Conference on Computer and
Communications Security (ASIACCS).
[4] Bander Ali Saleh Al-rimy, Mohd Aizaini Maarof, and Syed Zainuddin Mohd Shaid.
2018. A 0-Day Aware Crypto-Ransomware Early Behavioral Detection Frame-
work. In Proceedings of the 2nd International Conference of Reliable Information
and Communication Technology (IRICT).