(feature groups) to be easily added and combined as weak learners.
Third, it can be used for both classification and regression. Fourth,
it is interpretable as its predictive power has strong mathematical
justifications and provides us with the ability to compute and ana-
lyze the (global) feature importance. Last but not the least, as will be
shown in ¬ß6.3, it outperforms other classical machine learning meth-
ods such as Random Forest (RF) and ùëò-Nearest Neighbors (KNN)
which have been proposed for 3G/4G signal strength/bandwidth
prediction problems in the literature [20, 34, 54, 60].
‚Ä¢ Seq2Seq ML Models. Initially devised for natural language
processing and machine translation, Seq2Seq learning has now be-
come ubiquitous for solving various high-dimensional time series
prediction problems [49, 50, 61]. Unlike the standard long short-
term memory (LSTM) models [35], Seq2Seq allows us to model an
arbitrary length of the predicted output sequence instead of an
immediate one-time prediction, thus capable of predicting over a
longer horizon into the future. Formally, let ùëãùë° = {ùë•1, ùë•2, . . . , ùë•ùë°}
be a sequence of inputs known a priori at time ùë° where each ùë•ùë° is a
feature vector. Let ùëåùë° = {ùë¶1, ùë¶2, . . . , ùë¶ùëò} be a sequence of ùëò outputs
to be predicted. In our case, ùëåùë° is a sequence of future throughput
values to be predicted over the future ùëò time slots. The time slots
are defined based on the prediction problem at hand (e.g., seconds
Figure 15: Seq2Seq w/ Encoder-Decoder Architecture.
for short-time prediction, or minutes/hours for long-term predic-
tion). In our design of the Seq2Seq ML models (see Fig. 15 for its
illustration), we incorporate an encoder-decoder architecture using
an LSTM-type network. Our models can work with different feature
groups represented as a sequence of high-dimensional inputs.
6 PERFORMANCE EVALUATION
Using the proposed Lumos5G framework for 5G throughput
prediction, we evaluate the performance of GDBT and Seq2Seq
models using different feature groups and their combinations.
We also compare our models with several other analytical
and ML models proposed in the literature for 3G/4G signal
strength/throughput prediction.
6.1 Evaluation Framework
We start by presenting the model setups and evaluation metrics
used in our evaluation framework.
Model Setups for GDBT & Seq2Seq. We perform grid search
for tuning the hyperparameters for both Seq2Seq and GDBT models
using throughput traces representing a new area, thus not part of
the training or testing data. Although the models were fairly robust
to multiple hyperparameter values, we select a set that provided
best performance. For GDBT models, we use a gradient boosting
regressor (and classifier) with 8000 estimators, bounded by depth
of size 8 and with 0.01 learning rate. For Seq2Seq models, we use
a two-layer LSTM Encoder-Decoder architecture with 128 hidden
units. We run Seq2Seq experiments for 2000 epochs, where the
batch size is set to 256. The input and output sequence length
is set to be 20. We keep the hyperparameters fixed throughout
all our experiments. To obtain classification results, during post-
processing, we additionally associate our predicted throughput with
throughput class. For both GDBT and Seq2Seq, we randomly split
our datasets using a 70/30 ratio for training and testing, respectively.
We consider mean-squared-error (MSE) as the loss function. All
experiments are run on a single machine with Intel Core i7-6850K
(12-core) CPU and 2√ó NVIDIA TITAN V GPUs. Time to train each
of the Seq2Seq and GDBT models varied depending on the area
or its dataset size. The number of data points representing each
area are governed by the trajectory length (see Table 2 for details).
Seq2Seq took 6 to 44 hours for training each model while GDBT
was comparatively much quicker taking 10-30 minutes.
Evaluation Metrics. For regression, we evaluate using standard
metrics ‚Äì Mean Average Error (MAE) and Root Mean Squared Error
(RMSE). For classification, we consider the weighted average F1
score as the main metric for evaluation. In addition, we also use
‚Ä¶LSTMy1LSTMLSTMLSTMLSTMLSTMInput Sequence, Length = 20EncoderDecodery20y2Class 0     [0, 300 Mbps)Class 1     [300 Mbps, 700 Mbps)Class 2     [700 Mbps, ‚àû)]Length = 20 (Predicted Throughput)Encoder Statex1‚Ä¶‚Ä¶Embedding(moving speed, geolocation,compass direction, 5G status, signal strength, etc.) Throughput Binning]]‚Ä¶x2x20Regression OutputClassification Output‚Ä¶Lumos5G: Mapping and Predicting Commercial mmWave 5G Throughput
IMC ‚Äô20, October 27‚Äì29, 2020, Virtual Event, USA
Table 7: Classification Results: Comparison of Models Using Weighted Average F1 Score ‚Üë and Recall ‚Üë Metrics.
Feature
Groups ‚áì
4-way Intersection (Outdoor)
Areas
Models
Global
L
L+M
T+M
L+M+C
T+M+C
GDBT
0.79 0.60
0.91 0.85
0.91 0.85
0.92 0.87
0.93 0.87
Seq2Seq
0.86 0.71
0.94 0.89
0.95 0.93
0.97 0.95
0.96 0.94
1300m Loop (Outdoor)
Seq2Seq
0.65 0.56
0.89 0.92
GDBT
0.58 0.74
0.79 0.88
‚Äì
‚Äì
0.89 0.93
0.96 0.98
‚Äì
‚Äì
Airport (Indoor)
GDBT
0.79 0.88
0.91 0.95
0.91 0.96
0.92 0.96
0.92 0.97
Seq2Seq
0.83 0.85
0.91 0.94
0.93 0.96
0.91 0.95
0.91 0.95
GDBT
0.78 0.73
0.90 0.89
0.91 0.89
0.92 0.92
0.92 0.93
Seq2Seq
0.73 0.46
0.93 0.92
0.94 0.93
0.96 0.95
0.95 0.95
Table 8: Regression Results: Comparison of Models Using Mean Average Error ‚Üì and Root Mean Square Error ‚Üì Metrics.
Metrics. ‚Üë Weighted Average F1-score
‚Üë Recall of low-throughput class [0, 300)
Feature
Groups ‚áì
Areas
Models
L
L+M
T+M
L+M+C
T+M+C
4-way Intersection (Outdoor)
GDBT
236 347
121 188
117 181
114 177
107 166
Seq2Seq
151 218
68 137
58 120
54 116
67 131
1300m Loop (Outdoor)
GDBT
313 395
220 293
Seq2Seq
234 327
81 147
‚Äì
130 192
‚Äì
‚Äì
28 65
‚Äì
Airport (Indoor)
GDBT
170 283
79 146
76 142
72 139
69 131
Seq2Seq
133 223
67 133
57 126
71 138
70 147
Global
GDBT
225 314
127 186
115 173
109 166
100 154
Seq2Seq
208 273
74 144
52 109
49 112
57 119
Metrics. ‚Üì Mean Absolute Error (MAE)
‚Üì Root Mean Square Error (RMSE)
recall to evaluate the low-throughput class (i.e., below 300 Mbps)
prediction. The recall is defined as True Positives/(True Positives +
False Negatives). The rationale of using recall for the low-throughput
class is that, misclassifying low-throughput as high-throughput
may often times incur more QoE degradation (e.g., a video stall)
compared to misclassifying high-throughput as low (e.g., only video
quality degradation without a stall). Therefore, in most cases, we
prefer that the low-throughput class gets a high recall value.
6.2 Results and Observations
Table 7 shows the classification results for both GDBT and
Seq2Seq models under different feature groupings, while Table 8
show the regression results. Datasets collected from three areas
under stationary+walking (4-way Intersection & Airport) and
stationary+walking+driving (1300m Loop) mobility scenarios are
used for training and testing. We additionally build a model by
combining data from all areas with known 5G panel locations into
a single dataset ‚Äì referred to as Global. In the case of GDBT, the
prediction is based only on the current feature values, whereas in
the case of Seq2Seq, recent feature history values (i.e., a sequence of
feature values) are used for prediction. The classification results of
each model in Table 7 contain two values in each cell: the weighted
average F1-score and recall of low-throughput class [0, 300) Mbps ‚Äì as
indicated at the bottom of the table. For 1300m Loop, no results are
reported for T+M and T+M+C, as we are unable to reliably obtain the
5G panel location information. In Table 8, we show the regression
results of GDBT and Seq2Seq models of all the areas. Additionally,
Fig. 16 shows sample regression prediction plots for L+M+C feature
group on Global dataset using GDBT and Seq2Seq, with¬± 200 Mbps
error bounds shaded.
Key Observations. The results in Tables 7 and 8 clearly
demonstrate that both Seq2Seq and GDBT are able to achieve
overall good prediction results especially under feature group
combinations that account for additional UE-side features beyond
geolocation. Location-based feature group alone is in general
inadequate to achieve high prediction accuracy, especially under
Figure 16: Regression plots for Seq2Seq and GDBT using
L+M+C feature groups on Global dataset.
high mobility. By combining additional features from mobility
and/or connection-related feature groups, the weighted average F1
scores for both GDBT and Seq2Seq throughput class predictions
are consistently above 0.89 except for one L+M result for GDBT
at the Loop area. The Seq2Seq model produces slightly better
prediction results over GDBT for possibly two reasons: (i) in the
case of throughput class prediction, Seq2Seq uses a sequence of past
feature values, which indicates the benefits of incorporating history
data for prediction; and (ii) as an LSTM-based general-purpose
encoder-decoder, Seq2Seq is known to have stronger representation
power [37, 59] compared to GDBT. This is best demonstrated in the
regression results shown in Table 8 where for most cases Seq2Seq
has far lower MAEs and RMSEs.
Transferability Analysis. Comparing feature groups ‚Äì L+M
v/s. T+M and L+M+C v/s. T+M+C, we see that the prediction
results obtained using tower-based (T*) features, which are
location-agnostic, match those using location-based (L*) features. A
key advantage in using the T-based feature groups is that ML models
trained on one area may potentially be transferable to another area
if both share similar environments. To demonstrate that, at the
TestingSamples(sortedbygroundtruththroughput)0500100015002000Throughput(Mbps)GDBTGroundtruththroughput(shadedregion:+/-200Mbpserror)PredictedthroughputTestingSamples(sortedbygroundtruththroughput)0500100015002000Throughput(Mbps)Seq2SeqGroundtruththroughput(shadedregion:+/-200Mbpserror)PredictedthroughputIMC ‚Äô20, October 27‚Äì29, 2020, Virtual Event, USA
Arvind Narayanan, Eman Ramadan, Rishabh Mehta, Xinyue Hu, et al.
Table 9: Performance Comparison With Baseline Models on
Global Dataset - Both Regression and Classification Setups.
Feature