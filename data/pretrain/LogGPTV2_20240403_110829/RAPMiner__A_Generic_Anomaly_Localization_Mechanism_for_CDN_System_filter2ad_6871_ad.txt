3-7
3-8
3-9
3-10
3-11
3-12
1-1
1-2
1-3
1-4
1-5
1-6
1-7
2-5
2-6
2-7
2-8
2-9
2-10
2-11
2-12
2-13
2-14
2-15
2-16
3-5
3-6
3-7
3-8
3-9
3-10
3-11
3-12
1-1
1-2
1-3
1-4
1-5
1-6
1-7
2-5
2-6
2-7
2-8
2-9
2-10
2-11
2-12
2-13
2-14
2-15
2-16
3-5
3-6
3-9
3-10
3-11
3-12
Fig. 7. Attribute Combinations Structure
top to bottom based on BFS. As shown in Fig. 7 middle,
when we search the ﬁrst vertex (a1,∗,∗,∗) (node “1-1”), we
can conclude that it is an anomalous attribute combination
according to Criteria 2, and then according to the deﬁnition
of RAP, it can be determined as a RAP. Meanwhile, all of
its descendants are certainly not RAPs according to Criteria
3, which can be pruned off to avoid unnecessary search,
thus improving efﬁciency. As shown in Fig. 7 bottom, after
traversing the remaining vertices of the ﬁrst layer based on
BFS and ﬁnding no anomalous vertices according to Criteria
2, we start traversing the vertices of the second layer in the
DAG. When traversing the vertex “2-6”, we can conclude that
it is a RAP according to Deﬁnition 1 and Criteria 2, so we
delete all of its descendant vertices according to Criteria 3.
Next, we use an early stop strategy to terminate the search
procedure of the DAG in advance, i.e., whenever we ﬁnd
a RAP, judge whether the current RAP candidate set has
covered the most ﬁne-grained anomalous attribute combination
in the dataset D. Finally, we sort the obtained candidate set
in descending order according to their RAPScore, (Eq. 3) and
return top-k RAPs. We deﬁne the RAPScore by considering
the layer that ac resides in, because the possibility that the
current ac is a root cause is negatively correlated with it.
Criteria 2: ∀ac,
support countD(ac)
In Criteria 2, tconf
Criteria 3: If ac is a RAP , ∀ac
tconf ,
Anomaly) = support countD(ac,Anomaly)
threshold with a large value.
if Conf idence(ac ⇒ Anomaly) >
then ac is anomalous. Where Conf idence(ac ⇒
, tconf is conﬁdence
(cid:2) ∈ Descendants(ac) is
certainly not a RAP , where ac denotes the current searching
attribute combination and D is the most ﬁne-grained dataset.
is a conﬁdence threshold with a
large value. Conf idence(ac ⇒ Anomaly) also means the
anomaly ratio of the current visiting attribute combination,
which is expressed by the percentage of anomalies in the
most ﬁne-grained descendant attribute combinations of the
current ac. support countD(ac) is the number of the descen-
dant attribute combinations of the current visiting ac in the
most ﬁne-grained attribute combination dataset D, whereas
support countD(ac, Anomaly) is the number of attribute
combinations which are the descendants of ac and meanwhile
anomalous in D. We can easily come to that
the higher
the conﬁdence, the more likely the current ac is anomalous.
Therefore, we should choose a relatively large tconf instead
of a very large tconf , due to the fact that a relatively large
tconf will achieve a good error-tolerant rate.
Subsequently, we present the detailed explanation below
combined with the directed acyclic graph (DAG) in Fig. 7.
In the DAG, each vertex represents an attribute combination,
and each edge indicates the parent-child relationship between
two vertices. Note that the start vertex of the edge is the
parent of the end vertex. Fig. 7 top shows the parent-child
relationship of all attribute combinations before search. We
assume that the root anomaly patterns are (a1,∗,∗,∗) and
(a2, b2,∗,∗). Next, we traverse each vertex layer-by-layer from
RAP Score = Conf idence(ac ⇒ Anomaly)
√
Layer
(3)
The detailed procedure of layer-by-layer top-down search is
shown in Algorithm 2. Besides, it also should be noted that, in
Fig. 7, red vertices denote the abnormal attribute combinations,
blue vertices are normal attribute combinations that have been
searched, and white vertices denote attribute combinations that
have not been searched. The mapping between vertices and
attribute combinations is reported in Table V.
V. EXPERIMENTS
A. Dataset
We use two datasets, the public Squeeze semi-synthetic
dataset [22] and the semi-synthetic dataset RAPMD. RAPMD
is created by injecting failures into the background data which
is collected from an ISP-operated CDN in China. Speciﬁcally,
the background data consists of KPIs of the most ﬁne-grained
attribute combination, e.g., “Out Flow” of (L1, Wireless, IOS,
Site1), which indicates the trafﬁc volume output from the
edge servers at location “L1” for the “IOS” users who surf
the “Site1” via the “wireless” network. And these KPIs are
collected every 60 seconds spanning about 35 days (from
Feburary 1st to March 7th). There are about 1440 time points
every day, of which 3 time points are randomly selected for
failures injection, thus we obtain 105 failures in total.
For the Squeeze dataset, it makes two assumptions.
1) Vertical Assumption: The anomaly degree of descendant
attribute combinations under the same RAP is the same.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:17:38 UTC from IEEE Xplore.  Restrictions apply. 
325
Algorithm 2 AC-Guided Layer-by-Layer Top-down Search
Input: The most ﬁne-grained attribute combinations dataset
(cid:2) returned by Alg.1, e.g.,
D, the same as Alg.1
The attributes set AttributeSet
{A,B}
The threshold tconf
The speciﬁed number of returned RAPs k
e.g., [(a1,∗,∗,∗), (a2, b2,∗,∗)]
(cid:2)
, Layer)
Output: The Root Anomaly Patterns Set, short for RAP Set,
1: for Layer ∈ 1, 2, ...,|AttributeSet
Calculate Conf idence(ac ⇒ Anomaly) on D
if Conf idence(ac ⇒ Anomaly) > tconf then
CandidateSet ← CandidateSet + ac
Prune off the brunch of ac
if CandidateSet Covers Danomalous then
for ac ∈ cuboid do
(cid:2)| do
Cuboids ← get cuboids(AttributeSet
for cuboid ∈ Cuboids do
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15: end for
16: for CandidateRAP ∈ CandidateSet do
Early stop and jump out of all loops.
end if
end for
end for
end if
17:
18: end for
19: CandidateSet
20: RAP Set ← CandidateSet
21: return RAP Set
reversely
(cid:2)(k)
Calculate RAP Score (Eq.3) of CandidateRAP
(cid:2) ←Sort CandidateSet by RAP Score
2) Horizontal Assumption: The anomaly degree of different
failures is different. Note that there may be several RAPs with
regard to one failure.
However, through a careful analysis of the failure data of an
ISP-operated CDN, we observe that even for the descendant
attribute combinations under the same RAP, their anomaly
magnitude may not be the same. Due to the fact that the KPI
for the most ﬁne-grained attribute combination in the real-
world CDN is usually sparse and fails to show the statistical
characteristic, resulting in the non-concentrated deviation be-
tween the predicted and actual values. On the contrary, we also
observe that two anomalies may show a similar degree, even
though they come from different failures in real-world CDN.
In addition, the Squeeze dataset can be classiﬁed into different
categories with regard to the dimension and the number of
RAPs at a certain time point. However, it is usually very hard
for us to know how many failures there are and which layer
of cuboids the failure in the real-world scenario.
To make up for the above three shortcomings of the squeeze
semi-synthetic dataset, we create RAPMD by referring to
the real-world root anomaly patterns in failures injection
procedure to avoid the ideal root anomaly patterns in RAPMD.
To this end, we randomly extract 105-timestep points from the
background data of a real-world ISP-operated CDN and inject
the failures with the following characteristics.
• Randomness 1: We randomly select the number of RAPs
that is in the range of [1, 3] at each time point. Any
dimension can be selected for each RAP, and the dimen-
sion between them is not necessary to be the same. For
example, for time point 001, we randomly select three
RAPs to inject faults, and these three RAPs are randomly
selected as (a1,∗,∗,∗),(∗, b1,∗,∗) and (∗,∗, c1, d1).
• Randomness 2: For each randomly selected root RAP’s
most ﬁne-grained descendant attribute combination, we
randomly select its Dev at [0.1, 0.9] (Eq. 4, where ε is
an extremely small value used to avoid dividing by 0). For
each of the remaining normal most ﬁne-grained attribute
combinations, we randomly select Dev at [−0.02, 0.09].
Finally, the predicted value (Eq. 5) for each of the most
ﬁne-grained attribute combinations is given based on the
selected Dev. This ensures that the relative deviation of
the most ﬁne-grained attribute combinations under the
same RAP may be different, and the relative deviation
of the most ﬁne-grained attribute combination under
different RAPs may be the same.
Dev = f − v
f + ε
f = v + Dev · ε
1 − Dev
(4)
(5)
B. Evaluation Metrics
Because the Squeeze semi-synthetic dataset is classiﬁed
according to the dimension and number of root anomaly
patterns, we know the number of RAPs in advance before
localizing. Therefore, we keep the number of returned results
of the algorithm the same as the actual number of RAPs,
and compare F 1-score as Eq. 6 of baseline methods and
RAPMiner in the experiments.
F 1-score = 2 × P recision × Recall
P recision + Recall
(6)
Since the number of failures injected into RAPMD at each
time point is random, and we pay more attention to the recall,
we adopt RC@k [28]–[30] to evaluate the performance of
baseline methods and RAPMiner, Eq. 7, where k is the number
of recommended results of the algorithm and T is the anomaly
set. P redi
t denotes the ith recommended result of the anomaly
t, and Realt denotes the real RAP set of the anomaly t.
(cid:2)
t∈T
RC@k =
(cid:2)k
(cid:2)
t ∈ Realt
i=1 P redi
t∈T |Realt|
(7)
To evaluate the efﬁciency of these algorithms, we directly
compare their average running time in identifying the RAPs.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:17:38 UTC from IEEE Xplore.  Restrictions apply. 
326
C. Baseline Methods
1) Adtributor [13]: This method is only applicable to the
scenario where the RAPs are all one-dimensional. Adtributor
proposes three metrics which are Explanation Power (EP),
Succinctness and Surprise respectively to search each attribute
value under each attribute and determine whether the attribute
value is the culprit of failure or anomaly.
2) iDice [14]: iDice is able to conduct root anomaly pattern
mining in multi-dimensional attribute combination space. It
mainly uses Impact for data pruning, Change Detection and
Isolation Power for localizing failures, i.e. mining RAPs based
on BFS.
3) FP-growth [15]: Association rule is employed to search
for root anomaly patterns. We adopt an efﬁcient implementa-
tion of the association rule, i.e., FP-growth [31], [32] to mine
the RAPs.
4) Squeeze [22]: A metric named GPS (General Potential
Score) is proposed to determine the root anomaly pattern and
tries to mine the root anomaly patterns by searching every
possible cuboid based on clustering. As mentioned above,
this method has two strict assumptions: (1) The anomaly
magnitude of all attribute combinations under the same root
anomaly pattern is the same; (2) The anomaly magnitude of
attribute combinations varies among different failures.
D. Hardware platform
Our experiments, including performance and running time
comparison, are conducted on a server with Intel(R) Xeon(R)
CPU E5-2620 v2 @ 2.10GHz.
E. Effectiveness Comparison
1) The Effectiveness on Squeeze Dataset: The Squeeze
dataset has several groups of data according to different
noise levels. We only select data of B0-level to evaluate the
effectiveness of the methods because the varying noise levels
only affect the anomaly detection of each most ﬁne-grained
attribute combination. However, RAPMiner does not focus
on predicting and detecting KPI for each most ﬁne-grained
attribute combination. The subsequent RAPScore no longer
uses the predicted and actual value of KPI but directly uses the
anomaly detection results of each most ﬁne-grained attribute
combination for anomaly localization. The more accurate the
anomaly detection results are, the more effective the anomaly
localization is. Therefore, the data with different noise levels
is almost the same for RAPMiner. We need to ensure that the
anomaly detection results are as accurate as possible, which
also belongs to anomaly detection research. The effectiveness
of RAPMiner and baseline methods on squeeze-B0 are shown
in Fig. 8(a). RAPMiner, Squeeze, and FP-growth achieve
comparable performance on squeeze-B0. Among them, RAP-
Miner shows the best F 1-score (i.e. (1, 1)-1.0, (1, 2)-0.995
and (1, 3)-0.985) on the groups in which the RAPs are one-
dimensional, and also achieves the best F 1-score on the
groups of three-dimensional RAPs (3, 1) and (3, 2), which
is 1.0 and 0.967 respectively. Squeeze achieves the best
F 1-score of 0.970 and 0.982, respectively, on the groups
of (2, 2) and (2, 3). The association rule mining based on
FP-growth achieves the best F 1-score on (2, 1) and (3, 3)
groups, which are 1.0 and 0.963 respectively. Adtributor only
performs well on groups of one-dimensional RAPs, which is
consistent with the assumption that its root anomaly patterns
are located in one-dimensional cuboids. iDice achieves the
inferior performance in all groups.
2) The Effectiveness on RAPMD Dataset: RAPMD does
not group data by dimension or number of failures to avoid the
desirable root anomaly patterns. We calculate RC@3, RC@4,
and RC@5 of RAPMiner and Baseline Methods. Since the
Squeeze algorithm can not return a speciﬁed number of results
on RAPMD, we keep the same results for these three metrics
of Squeeze. As shown in Fig. 8(b), RAPMiner achieves the
best performance (above 80%) with regard to all of RC@3,
RC@4 and RC@5 on RAPMD, which is at least 10% higher
than the sub-optimal method, i.e., the association rule mining
implemented with FP-growth. It can also be seen that the
Squeeze method is less effective on RAPMD, due to the
fact that it only works on datasets that follow the desirable