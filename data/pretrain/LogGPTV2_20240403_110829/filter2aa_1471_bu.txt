calculation of the total number of pages to be assigned to the root
compartment needs to be accurate.)
■    If the allocation has been requested on behalf of a child partition
(usually through a hypercall), the hypervisor will fail the request with
the status INSUFFICIENT_MEMORY. The root partition detects the
error and performs the allocation of some physical page (more details
are discussed later in the “Virtualization stack” section), which will be
deposited in the child compartment through the HvDepositMemory
hypercall. The operation can be finally reinitiated (and usually will
succeed).
The physical pages allocated from the compartment are usually mapped in
the hypervisor using a virtual address. When a compartment is created, a
virtual address range (sized 4 or 8 GB, depending on whether the
compartment is a root or a child) is allocated with the goal of mapping the
new compartment, its PDE bitmap, and its global zone.
A hypervisor’s zone encapsulates a private VA range, which is not shared
with the entire hypervisor address space (see the “Isolated address space”
section later in this chapter). The hypervisor executes with a single root page
table (differently from the NT kernel, which uses KVA shadowing). Two
entries in the root page table page are reserved with the goal of dynamically
switching between each zone and the virtual processors’ address spaces.
Partitions’ physical address space
As discussed in the previous section, when a partition is initially created, the
hypervisor allocates a physical address space for it. A physical address space
contains all the data structures needed by the hardware to translate the
partition’s guest physical addresses (GPAs) to system physical addresses
(SPAs). The hardware feature that enables the translation is generally referred
to as second level address translation (SLAT). The term SLAT is platform-
agnostic: hardware vendors use different names: Intel calls it EPT for
extended page tables; AMD uses the term NPT for nested page tables; and
ARM simply calls it Stage 2 Address Translation.
The SLAT is usually implemented in a way that’s similar to the
implementation of the x64 page tables, which uses four levels of translation
(the x64 virtual address translation has already been discussed in detail in
Chapter 5 of Part 1). The OS running inside the partition uses the same
virtual address translation as if it were running by bare-metal hardware.
However, in the former case, the physical processor actually executes two
levels of translation: one for virtual addresses and one for translating physical
addresses. Figure 9-8 shows the SLAT set up for a guest partition. In a guest
partition, a GPA is usually translated to a different SPA. This is not true for
the root partition.
Figure 9-8 Address translation for a guest partition.
When the hypervisor creates the root partition, it builds its initial physical
address space by using identity mapping. In this model, each GPA
corresponds to the same SPA (for example, guest frame 0x1000 in the root
partition is mapped to the bare-metal physical frame 0x1000). The hypervisor
preallocates the memory needed for mapping the entire physical address
space of the machine (which has been discovered by the Windows Loader
using UEFI services; see Chapter 12 for details) into all the allowed root
partition’s virtual trust levels (VTLs). (The root partition usually supports
two VTLs.) The SLAT page tables of each VTL belonging to the partition
include the same GPA and SPA entries but usually with a different protection
level set. The protection level applied to each partition’s physical frame
allows the creation of different security domains (VTL), which can be
isolated one from each other. VTLs are explained in detail in the section
“The Secure Kernel” later in this chapter. The hypervisor pages are marked
as hardware-reserved and are not mapped in the partition’s SLAT table
(actually they are mapped using an invalid entry pointing to a dummy PFN).
 Note
For performance reasons, the hypervisor, while building the physical
memory mapping, is able to detect large chunks of contiguous physical
memory, and, in a similar way as for virtual memory, is able to map those
chunks by using large pages. If for some reason the OS running in the
partition decides to apply a more granular protection to the physical page,
the hypervisor would use the reserved memory for breaking the large page
in the SLAT table.
Earlier versions of the hypervisor also supported another technique for
mapping a partition’s physical address space: shadow paging. Shadow
paging was used for those machines without the SLAT support. This
technique had a very high-performance overhead; as a result, it’s not
supported anymore. (The machine must support SLAT; otherwise, the
hypervisor would refuse to start.)
The SLAT table of the root is built at partition-creation time, but for a
guest partition, the situation is slightly different. When a child partition is
created, the hypervisor creates its initial physical address space but allocates
only the root page table (PML4) for each partition’s VTL. Before starting the
new VM, the VID driver (part of the virtualization stack) reserves the
physical pages needed for the VM (the exact number depends on the VM
memory size) by allocating them from the root partition. (Remember, we are
talking about physical memory; only a driver can allocate physical pages.)
The VID driver maintains a list of physical pages, which is analyzed and split
in large pages and then is sent to the hypervisor through the
HvMapGpaPages Rep hypercall.
Before sending the map request, the VID driver calls into the hypervisor
for creating the needed SLAT page tables and internal physical memory
space data structures. Each SLAT page table hierarchy is allocated for each
available VTL in the partition (this operation is called pre-commit). The
operation can fail, such as when the new partition’s compartment could not
contain enough physical pages. In this case, as discussed in the previous
section, the VID driver allocates more memory from the root partition and
deposits it in the child’s partition compartment. At this stage, the VID driver
can freely map all the child’s partition physical pages. The hypervisor builds
and compiles all the needed SLAT page tables, assigning different protection
based on the VTL level. (Large pages require one less indirection level.) This
step concludes the child partition’s physical address space creation.
Address space isolation
Speculative execution vulnerabilities discovered in modern CPUs (also
known as Meltdown, Spectre, and Foreshadow) allowed an attacker to read
secret data located in a more privileged execution context by speculatively
reading the stale data located in the CPU cache. This means that software
executed in a guest VM could potentially be able to speculatively read private
memory that belongs to the hypervisor or to the more privileged root
partition. The internal details of the Spectre, Meltdown, and all the side-
channel vulnerabilities and how they are mitigated by Windows have been
covered in detail in Chapter 8.
The hypervisor has been able to mitigate most of these kinds of attacks by
implementing the HyperClear mitigation. The HyperClear mitigation relies
on three key components to ensure strong Inter-VM isolation: core scheduler,
Virtual-Processor Address Space Isolation, and sensitive data scrubbing. In
modern multicore CPUs, often different SMT threads share the same CPU
cache. (Details about the core scheduler and symmetric multithreading are
provided in the “Hyper-V schedulers” section.) In the virtualization
environment, SMT threads on a core can independently enter and exit the
hypervisor context based on their activity. For example, events like interrupts
can cause an SMT thread to switch out of running the guest virtual processor
context and begin executing the hypervisor context. This can happen
independently for each SMT thread, so one SMT thread may be executing in
the hypervisor context while its sibling SMT thread is still running a VM’s
guest virtual processor context. An attacker running code in a less trusted
guest VM’s virtual processor context on one SMT thread can then use a side
channel vulnerability to potentially observe sensitive data from the
hypervisor context running on the sibling SMT thread.
The hypervisor provides strong data isolation to protect against a malicious
guest VM by maintaining separate virtual address ranges for each guest SMT
thread (which back a virtual processor). When the hypervisor context is
entered on a specific SMT thread, no secret data is addressable. The only
data that can be brought into the CPU cache is associated with that current
guest virtual processor or represent shared hypervisor data. As shown in
Figure 9-9, when a VP running on an SMT thread enters the hypervisor, it is
enforced (by the root scheduler) that the sibling LP is running another VP
that belongs to the same VM. Furthermore, no shared secrets are mapped in
the hypervisor. In case the hypervisor needs to access secret data, it assures
that no other VP is scheduled in the other sibling SMT thread.
Figure 9-9 The Hyperclear mitigation.
Unlike the NT kernel, the hypervisor always runs with a single page table
root, which creates a single global virtual address space. The hypervisor
defines the concept of private address space, which has a misleading name.
Indeed, the hypervisor reserves two global root page table entries (PML4
entries, which generate a 1-TB virtual address range) for mapping or
unmapping a private address space. When the hypervisor initially constructs
the VP, it allocates two private page table root entries. Those will be used to
map the VP’s secret data, like its stack and data structures that contain
private data. Switching the address space means writing the two entries in the
global page table root (which explains why the term private address space
has a misleading name—actually it is private address range). The hypervisor
switches private address spaces only in two cases: when a new virtual
processor is created and during thread switches. (Remember, threads are
backed by VPs. The core scheduler assures that no sibling SMT threads
execute VPs from different partitions.) During runtime, a hypervisor thread
has mapped only its own VP’s private data; no other secret data is accessible
by that thread.
Mapping secret data in the private address space is achieved by using the
memory zone, represented by an MM_ZONE data structure. A memory zone
encapsulates a private VA subrange of the private address space, where the
hypervisor usually stores per-VP’s secrets.
The memory zone works similarly to the private address space. Instead of
mapping root page table entries in the global page table root, a memory zone
maps private page directories in the two root entries used by the private
address space. A memory zone maintains an array of page directories, which
will be mapped and unmapped into the private address space, and a bitmap
that keeps track of the used page tables. Figure 9-10 shows the relationship
between a private address space and a memory zone. Memory zones can be
mapped and unmapped on demand (in the private address space) but are
usually switched only at VP creation time. Indeed, the hypervisor does not
need to switch them during thread switches; the private address space
encapsulates the VA range exposed by the memory zone.
Figure 9-10 The hypervisor’s private address spaces and private memory
zones.
In Figure 9-10, the page table’s structures related to the private address
space are filled with a pattern, the ones related to the memory zone are
shown in gray, and the shared ones belonging to the hypervisor are drawn
with a dashed line. Switching private address spaces is a relatively cheap
operation that requires the modification of two PML4 entries in the
hypervisor’s page table root. Attaching or detaching a memory zone from the
private address space requires only the modification of the zone’s PDPTE (a
zone VA size is variable; the PDTPE are always allocated contiguously).
Dynamic memory
Virtual machines can use a different percentage of their allocated physical
memory. For example, some virtual machines use only a small amount of
their assigned guest physical memory, keeping a lot of it freed or zeroed. The
performance of other virtual machines can instead suffer for high-memory
pressure scenarios, where the page file is used too often because the allocated
guest physical memory is not enough. With the goal to prevent the described
scenario, the hypervisor and the virtualization stack supports the concept of
dynamic memory. Dynamic memory is the ability to dynamically assign and
remove physical memory to a virtual machine. The feature is provided by
multiple components:
■    The NT kernel’s memory manager, which supports hot add and hot
removal of physical memory (on bare-metal system too)
■    The hypervisor, through the SLAT (managed by the address manager)
■    The VM Worker process, which uses the dynamic memory controller
module, Vmdynmem.dll, to establish a connection to the VMBus
Dynamic Memory VSC driver (Dmvsc.sys), which runs in the child
partition
To properly describe dynamic memory, we should quickly introduce how
the page frame number (PFN) database is created by the NT kernel. The PFN
database is used by Windows to keep track of physical memory. It was
discussed in detail in Chapter 5 of Part 1. For creating the PFN database, the
NT kernel first calculates the hypothetical size needed to map the highest
possible physical address (256 TB on standard 64-bit systems) and then
marks the VA space needed to map it entirely as reserved (storing the base
address to the MmPfnDatabase global variable). Note that the reserved VA
space still has no page tables allocated. The NT kernel cycles between each
physical memory descriptor discovered by the boot manager (using UEFI
services), coalesces them in the longest ranges possible and, for each range,
maps the underlying PFN database entries using large pages. This has an
important implication; as shown in Figure 9-11, the PFN database has space
for the highest possible amount of physical memory but only a small subset
of it is mapped to real physical pages (this technique is called sparse
memory).
Figure 9-11 An example of a PFN database where some physical memory
has been removed.
Hot add and removal of physical memory works thanks to this principle.
When new physical memory is added to the system, the Plug and Play
memory driver (Pnpmem.sys) detects it and calls the
MmAddPhysicalMemory routine, which is exported by the NT kernel. The
latter starts a complex procedure that calculates the exact number of pages in
the new range and the Numa node to which they belong, and then it maps the
new PFN entries in the database by creating the necessary page tables in the
reserved VA space. The new physical pages are added to the free list (see
Chapter 5 in Part 1 for more details).
When some physical memory is hot removed, the system performs an
inverse procedure. It checks that the pages belong to the correct physical
page list, updates the internal memory counters (like the total number of
physical pages), and finally frees the corresponding PFN entries, meaning
that they all will be marked as “bad.” The memory manager will never use
the physical pages described by them anymore. No actual virtual space is
unmapped from the PFN database. The physical memory that was described
by the freed PFNs can always be re-added in the future.
When an enlightened VM starts, the dynamic memory driver (Dmvsc.sys)
detects whether the child VM supports the hot add feature; if so, it creates a
worker thread that negotiates the protocol and connects to the VMBus
channel of the VSP. (See the “Virtualization stack” section later in this
chapter for details about VSC and VSP.) The VMBus connection channel
connects the dynamic memory driver running in the child partition to the
dynamic memory controller module (Vmdynmem.dll), which is mapped in
the VM Worker process in the root partition. A message exchange protocol is
started. Every one second, the child partition acquires a memory pressure
report by querying different performance counters exposed by the memory
manager (global page-file usage; number of available, committed, and dirty
pages; number of page faults per seconds; number of pages in the free and
zeroed page list). The report is then sent to the root partition.
The VM Worker process in the root partition uses the services exposed by
the VMMS balancer, a component of the VmCompute service, for
performing the calculation needed for determining the possibility to perform
a hot add operation. If the memory status of the root partition allowed a hot
add operation, the VMMS balancer calculates the proper number of pages to
deposit in the child partition and calls back (through COM) the VM Worker
process, which starts the hot add operation with the assistance of the VID
driver:
1. 
Reserves the proper amount of physical memory in the root partition
2. 
Calls the hypervisor with the goal to map the system physical pages
reserved by the root partition to some guest physical pages mapped in
the child VM, with the proper protection
3. 
Sends a message to the dynamic memory driver for starting a hot add
operation on some guest physical pages previously mapped by the
hypervisor
The dynamic memory driver in the child partition uses the
MmAddPhysicalMemory API exposed by the NT kernel to perform the hot
add operation. The latter maps the PFNs describing the new guest physical
memory in the PFN database, adding new backing pages to the database if
needed.
In a similar way, when the VMMS balancer detects that the child VM has
plenty of physical pages available, it may require the child partition (still
through the VM Worker process) to hot remove some physical pages. The
dynamic memory driver uses the MmRemovePhysicalMemory API to
perform the hot remove operation. The NT kernel verifies that each page in
the range specified by the balancer is either on the zeroed or free list, or it
belongs to a stack that can be safely paged out. If all the conditions apply, the
dynamic memory driver sends back the “hot removal” page range to the VM
Worker process, which will use services provided by the VID driver to
unmap the physical pages from the child partition and release them back to
the NT kernel.
 Note
Dynamic memory is not supported when nested virtualization is enabled.
Hyper-V schedulers
The hypervisor is a kind of micro operating system that runs below the root
partition’s OS (Windows). As such, it should be able to decide which thread
(backing a virtual processor) is being executed by which physical processor.
This is especially true when the system runs multiple virtual machines
composed in total by more virtual processors than the physical processors
installed in the workstation. The hypervisor scheduler role is to select the
next thread that a physical CPU is executing after the allocated time slice of
the current one ends. Hyper-V can use three different schedulers. To properly
manage all the different schedulers, the hypervisor exposes the scheduler
APIs, a set of routines that are the only entries into the hypervisor scheduler.
Their sole purpose is to redirect API calls to the particular scheduler
implementation.
EXPERIMENT: Controlling the hypervisor’s
scheduler type
Whereas client editions of Windows start by default with the root
scheduler, Windows Server 2019 runs by default with the core
scheduler. In this experiment, you figure out the hypervisor
scheduler enabled on your system and find out how to switch to
another kind of hypervisor scheduler on the next system reboot.
The Windows hypervisor logs a system event after it has
determined which scheduler to enable. You can search the logged
event by using the Event Viewer tool, which you can run by typing
eventvwr in the Cortana search box. After the applet is started,
expand the Windows Logs key and click the System log. You
should search for events with ID 2 and the Event sources set to
Hyper-V-Hypervisor. You can do that by clicking the Filter
Current Log button located on the right of the window or by
clicking the Event ID column, which will order the events in
ascending order by their ID (keep in mind that the operation can
take a while). If you double-click a found event, you should see a
window like the following:
The launch event ID 2 denotes indeed the hypervisor scheduler
type, where
1 = Classic scheduler, SMT disabled
2 = Classic scheduler
3 = Core scheduler
4 = Root scheduler
The sample figure was taken from a Windows Server system,
which runs by default with the Core Scheduler. To change the
scheduler type to the classic one (or root), you should open an
administrative command prompt window (by typing cmd in the
Cortana search box and selecting Run As Administrator) and type
the following command:
Click here to view code image
bcdedit /set hypervisorschedulertype 
where  is Classic for the classic scheduler, Core for the
core scheduler, or Root for the root scheduler. You should restart
the system and check again the newly generated Hyper-V-
Hypervisor event ID 2. You can also check the current enabled
hypervisor scheduler by using an administrative PowerShell
window with the following command:
Click here to view code image
Get-WinEvent -FilterHashTable @{ProviderName="Microsoft-
Windows-Hyper-V-Hypervisor"; ID=2}
-MaxEvents 1
The command extracts the last Event ID 2 from the System
event log.
The classic scheduler
The classic scheduler has been the default scheduler used on all versions of
Hyper-V since its initial release. The classic scheduler in its default
configuration implements a simple, round-robin policy in which any virtual
processor in the current execution state (the execution state depends on the
total number of VMs running in the system) is equally likely to be
dispatched. The classic scheduler supports also setting a virtual processor’s
affinity and performs scheduling decisions considering the physical
processor’s NUMA node. The classic scheduler doesn’t know what a guest
VP is currently executing. The only exception is defined by the spin-lock
enlightenment. When the Windows kernel, which is running in a partition, is
going to perform an active wait on a spin-lock, it emits a hypercall with the
goal to inform the hypervisor (high IRQL synchronization mechanisms are
described in Chapter 8, “System mechanisms”). The classic scheduler can
preempt the current executing virtual processor (which hasn’t expired its
allocated time slice yet) and can schedule another one. In this way it saves the
active CPU spin cycles.
The default configuration of the classic scheduler assigns an equal time
slice to each VP. This means that in high-workload oversubscribed systems,
where multiple virtual processors attempt to execute, and the physical
processors are sufficiently busy, performance can quickly degrade. To
overcome the problem, the classic scheduler supports different fine-tuning
options (see Figure 9-12), which can modify its internal scheduling decision:
■    VP reservations A user can reserve the CPU capacity in advance on
behalf of a guest machine. The reservation is specified as the
percentage of the capacity of a physical processor to be made