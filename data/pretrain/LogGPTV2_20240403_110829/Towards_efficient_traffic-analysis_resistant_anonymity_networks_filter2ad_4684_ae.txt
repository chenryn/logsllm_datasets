### Traceroute Dataset and Latency Estimation

Between March 1, 2010, and June 1, 2010, 200 million traceroutes were conducted among Ono users. These measurements provide end-to-end latency data between end-users, as well as hop-by-hop delays to intermediate routers.

To estimate the delay through an Aqua deployment, we need a set of mix locations and the latencies between them. We leverage the traceroute dataset by assuming that mixes will be placed in networks frequently traversed by paths between end users. Specifically, we count the number of distinct source/destination pairs that traverse each AS boundary (using IP-to-AS translation) and select the 100 most popular networks as mix locations. The latency between a client and a mix is obtained from the traceroute-based delay measured when the client probed the mix location. Since most pairs of mix locations are traversed by at least one end-to-end traceroute measurement, we obtain a nearly complete matrix of delays between our selected mix locations. Our simulation does not use paths for which our dataset does not contain a delay measurement.

To avoid bias from large tier-1 networks, we limit the selection to at most two mix locations per AS. The resulting set of locations spans 70 ASNs and 25 countries across North America, Europe, Asia, and South Africa.

For each set of latencies measured directly between endpoints (i.e., the last hop of a traceroute), we require at least three sample values and take the median latency. We further filter out latencies that are unreasonably large (>1000 ms), as they may indicate severe buffer bloat or other transient performance issues. Our goal is to capture the additional delay from mix hops located in the network core, rather than last-mile delays.

### Simulation Results

We obtained simulation results for latencies according to the following models:

- **Aqua**: There are two concatenated mix circuits, resulting in 6 mix hops in each direction (12 hops total). Each mix is located in a well-connected network, endpoints are in access networks, and we use a batch period of 10 ms.
- **Circuit-switched (Onion)**: There are two concatenated onion circuits, resulting in six distinct hops between endpoints, traversed once in each direction (12 hops total). Each onion hop is located in a Point of Presence (PoP), and endpoints are in access networks.
- **P2P (Tarzan)**: There are six onion hops, each located in an access network.

We sampled 100,000 pairs of hosts, simulated latency over 1000 randomly selected paths, and reported results for the median latencies. Figure 6 shows cumulative distribution functions (CDFs), where each point (x, y) represents the latency value (x) for a single source-destination pair (cumulative fraction, y). There is one curve for the direct path between a source and destination (labeled "direct"), and one curve each for the median latencies in Aqua, onion routing, and P2P routing (labeled as "[Aqua, Onion, P2P]").

### Analysis

While it is expected that there are higher delays in Aqua due to the six additional overlay hops in each direction compared to direct-path routing, the median latencies remain within a constant factor of the direct-path delays. For example, the median delay for Aqua latency is approximately five times the median delay for direct path latency when using a delay of 10 ms at each hop. Compared to onion routing, Aqua imposes only an additional 120 ms of delay due to buffering for chaffing at mixes. Thus, Aqua offers resilience to timing attacks with a modest additional delay (12%) over Tor.

P2P routing has nearly 20% larger median latency compared to Aqua with 10 ms delays at each hop. This is due to the latency of traversing last-mile links at each hop in P2P routing, compared to latencies between mixes in Aqua located in the network core. In the worst case, both Aqua and onion routing may suffer latencies of one or two seconds. Focusing on the top of Fig. 6, Aqua’s latency is a full second faster than P2P routing. While both approaches suffer large delays in the worst case, the impact on end-to-end performance differs. For onion routing, a poor circuit choice lasts until the circuit is torn down, potentially affecting many flows. Aqua, however, picks paths on a per-packet basis, limiting the impact of such poor choices.

### Impact of Latency on TCP Flow Rates

We now consider the impact of additional delays on the maximum rate for a TCP flow over a path and evaluate whether it affects end-to-end performance for applications like BitTorrent. Using the traditional formula:

\[ \text{Rate} = \frac{1.2 \times \text{MSS}}{\text{RTT} \times \sqrt{\text{loss}}} \]

we estimate the steady-state transfer rate for a flow experiencing 1% packet loss, with a maximum segment size (MSS) of 536 bytes and the RTT of the 90th percentile of the distribution in Fig. 6. We then compare this steady-state rate with the distribution of maximum transfer rates for peers in our dataset (considered the peer capacity). For each of the routing models considered, we estimate the end-to-end rate as the minimum of the per-hop TCP connection in the multi-hop overlay. We focus on upload rates because asymmetric bandwidth in access networks implies that senders’ transfer rates are the main bottleneck in BitTorrent. We assume there are 10 simultaneous flows for each user, based on the average number of parallel flows per peer in our dataset (9.6).

We find that the steady-state transfer rates for Aqua, Onion, and P2P routing are at least 10 KB/s, 12.3 KB/s, and 6.1 KB/s, respectively, for 90% of peers. By comparison, 90% of flows in our BitTorrent traces have a send rate less than 13.4 KB/s. When there are 10 parallel flows, we find that only 14.6% of paths experience throttling in the upload direction for Aqua (10% for Onion and 27% for P2P). Thus, we believe that the delays incurred by overlay routing should not significantly reduce available capacity in the system for the vast majority of peers and paths.

### Caveats

#### Simulation Limitations
Our simulation approach captures realistic session times, throughput capacities, and connection patterns for BitTorrent. However, our simulation does not account for the following factors:
- Additional latency from multiple hops in anonymous communication designs.
- Delays or bandwidth constraints at mixes, except for those required for batching.
- Throttling and kset formation, which alter empirical transfer rates and session durations recorded in the trace.

#### Dataset Limitations
There are several limitations of the Ono dataset:
- It is representative of paths between users in a P2P network but may not reflect paths between end-users and content providers such as CDNs and web servers.
- We do not consider the performance impact of load at each mix, though we expect the mix locations to be well provisioned.
- There is bias in the empirical dataset, representing locations where BitTorrent usage is high. We believe this is also where Aqua is most likely to be popular if deployed today.

### Preliminary Implementation Results

At the time of writing, we have implemented Aqua’s multipath routing component in Tor v0.2.2.37, adding approximately 3,000 lines of C code. The implementation of traffic obfuscation is still in progress. To quantify the CPU and memory usage of multipath routing, we performed a simple experiment involving a client downloading a 100MB file from an Apache server in a well-provisioned network with nine Aqua prototype mixes. The client’s download rate (averaged over 5 runs) is 9.12 Mb/s for a direct connection, 7.52 Mb/s with single-path Tor, and 7.68 Mb/s with the Aqua prototype. On average, Tor proxies use 2% of CPU and 45 MB of memory, while the Aqua prototype mixes use 2.1% of CPU and 46 MB of memory. As we can see, the Aqua prototype with multipath routing introduces negligible overhead relative to Tor.

### Conclusion

We have introduced Aqua, an efficient traffic-analysis-resistant anonymity network for BitTorrent applications. Aqua derives its efficiency from using different traffic obfuscation mechanisms in the core and at the edges of the network. In the core, Aqua employs uniform rate chaffing to take advantage of infrequent changes in aggregate traffic. Multipath routing disperses traffic hot spots in the core to minimize chaff overhead. At the edges, Aqua dynamically groups peers with correlated payload traffic patterns and couples their rate changes to efficiently provide k-anonymity. We showed that these mechanisms scale to much larger anonymity sets than existing work while achieving latency low enough to have minimal impact on TCP bulk performance in BitTorrent workloads. These properties allow Aqua to anonymize BitTorrent traffic with high bandwidth efficiency. Our future work aims to provide strong anonymity to a broader range of applications.

### Acknowledgements

We thank the anonymous reviewers and our shepherd, Katerina Argyraki, for their helpful feedback.

### References

[References listed as provided in the original text]