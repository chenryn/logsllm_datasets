### Elevating the Priority and Assembling a Response Team

The support case was escalated to Google's highest urgency level, with the issue reported to both engineering management and Site Reliability Engineering (SRE). A small, dedicated team of Google Music developers and SREs was assembled to address the problem. The problematic pipeline was temporarily disabled to prevent further user impact.

### Assessing the Damage

Manually checking the metadata for millions to billions of files distributed across multiple data centers was impractical. Therefore, the team quickly developed a MapReduce job to assess the damage and anxiously awaited its completion. On March 8th, the results were in: the refactored data deletion pipeline had erroneously removed approximately 600,000 audio references, affecting 21,000 users. Given the hasty nature of the diagnostic pipeline, the actual extent of the damage could be even greater.

### Timeline and Impact

It had been over a month since the faulty data deletion pipeline first ran, and that initial run alone had removed hundreds of thousands of audio tracks that should not have been deleted. The question now was whether the lost data could be recovered. If not, or if the recovery was too slow, Google would face significant backlash from its users. How could such a critical error go unnoticed?

### Resolving the Issue

#### Parallel Bug Identification and Recovery Efforts

The first step in resolving the issue was to identify the root cause of the bug and understand why it occurred. Without addressing the root cause, any recovery efforts would be futile. There was pressure to re-enable the pipeline to honor user requests for deleting audio tracks, but doing so would continue to harm innocent users who would lose their purchased or recorded audio files. The only way to break this impasse was to fix the issue at its core, and do so quickly.

#### Recovery Effort

While the root cause was being investigated, the recovery effort began. The audio tracks were backed up on tape, but unlike the Gmail case study, these encrypted backup tapes were stored offsite due to the large volume of data. To expedite the recovery, the team decided to troubleshoot the root cause while simultaneously retrieving the offsite backup tapes.

The engineers split into two groups: the most experienced SREs focused on the recovery, while the developers analyzed the data deletion code to fix the bug. Due to incomplete knowledge of the root problem, the recovery had to be staged in multiple passes. The first batch of nearly half a million audio tracks was identified, and the tape backup system team was notified of the emergency recovery at 4:34 p.m. Pacific Time on March 8th.

#### Tape Backup Recovery

The recovery team had one advantage: the recent annual disaster recovery testing exercise (DiRT) meant the tape backup team was familiar with the capabilities and limitations of their systems. They used a new tool tested during DiRT to map the audio files to backups and then to the actual tapes. This process determined that over 5,000 backup tapes needed to be recalled by truck. Datacenter technicians cleared space for the tapes, and a complex process of registering and extracting data from the tapes followed, with workarounds for bad tapes and drives.

Unfortunately, only 436,223 of the approximately 600,000 lost audio tracks were found on tape backups, meaning about 161,000 other audio tracks were lost before they could be backed up. The recovery team decided to focus on recovering the 161,000 missing tracks after initiating the recovery process for the tracks with tape backups.

#### First Wave of Recovery

On March 8th, the first wave of recovery began. Requesting 1.5 petabytes of data from thousands of tapes was challenging, and the custom-built tape backup software was not designed to handle such a large restore operation. The initial recovery was split into 5,475 restore jobs. By midnight on March 9th, all 5,475 restores were requested, and four hours later, a list of 5,337 backup tapes was generated for recall. The tapes arrived at the data center via truck deliveries.

Datacenter technicians manually loaded the tapes, a process that, based on past DiRT exercises, was hundreds of times faster than automated methods. Despite this, the 1.5 petabyte recovery took longer than the estimated two days. By March 10th, 74% of the 436,223 audio files had been successfully transferred. Additional truck deliveries were arranged to recall the omitted and redundancy tapes.

By March 11th, over 99.95% of the restore operation was complete, and the remaining files were being recalled. Although the data was safely on distributed filesystems, additional steps were necessary to make them accessible to users. However, an unrelated but critical production failure diverted the Google Music team for two days. The data recovery resumed on March 13th, and all 436,223 audio tracks were made accessible to users within seven days.

#### Second Wave of Recovery

With the first wave of recovery complete, the team focused on the 161,000 missing audio files. Most of these were store-bought and promotional tracks, which were quickly reinstated. For the user-uploaded files, the Google Music team prompted affected users to re-upload their files. This process lasted more than a week, concluding the full recovery effort.

### Addressing the Root Cause

The Google Music team eventually identified the flaw in the refactored data deletion pipeline. In large, complex services, even simple tasks like removing deleted data need to be performed in stages, each involving different datastores. To avoid slowing down the service, cloud computing engineers often make short-lived copies of data on secondary storage. However, this can introduce race conditions if the relative age of the secondary copies is not carefully coordinated.

Google Music’s data deletion pipeline was designed with coordination and margins for error, but as the service grew, performance optimizations increased the probability of race conditions. When the pipeline was refactored, this probability significantly increased, leading to more frequent issues.

In response, Google Music redesigned its data deletion pipeline to eliminate such race conditions and enhanced monitoring and alerting systems to detect similar large-scale deletion bugs early.

### General Principles of SRE Applied to Data Integrity

1. **Beginner’s Mind**: Never assume you fully understand a complex system. Trust but verify, and apply defense in depth.
2. **Trust but Verify**: Check the correctness of critical elements using out-of-band validators, even if API semantics suggest otherwise.
3. **Hope Is Not a Strategy**: Regularly exercise data recovery processes to ensure they work when needed.
4. **Defense in Depth**: Use multiple strategies to address a broad range of scenarios.
5. **Revisit and Reexamine**: Continuously validate assumptions and processes as systems and infrastructure evolve.

### Conclusion

Data availability is a critical concern for any data-centric system. Google SRE focuses on proving that systems can maintain data availability with a predicted maximum downtime. Recognizing that everything will go wrong is key to preparing for real emergencies. By continuously improving loss detection and recovery, the goal is to approach zero downtime and shift from planning recovery to prevention.

### Reliable Product Launches at Scale

Google's ability to launch new products and features rapidly is supported by Site Reliability Engineers (SREs) who ensure stability. A dedicated team of "Launch Coordination Engineers" (LCEs) consults with engineering teams on the technical aspects of launches and maintains a "launch checklist" to ensure reliable launches.

For example, the NORAD Tracks Santa project required handling a massive traffic surge. LCEs worked to prepare the infrastructure, ensuring that the site could handle the load and that children could watch Santa deliver presents without interruption. This project highlighted the importance of anticipating potential issues and coordinating between different engineering groups to ensure a successful launch.