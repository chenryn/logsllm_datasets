raising the priority of the support case to Google’s most urgent classification and
reporting the issue to engineering management and Site Reliability Engineering. A
small team of Google Music developers and SREs assembled to tackle the issue, and
the offending pipeline was temporarily disabled to stem the tide of external user
casualties.
Next, manually checking the metadata for millions to billions of files organized across
multiple datacenters would be unthinkable. So the team whipped up a hasty
362 | Chapter 26: Data Integrity: What You Read Is
MapReduce job to assess the damage and waited desperately for the job to complete.
They froze as its results came in on March 8th: the refactored data deletion pipeline
had removed approximately 600,000 audio references that shouldn’t have been
removed, affecting audio files for 21,000 users. Since the hasty diagnosis pipeline
made a few simplifications, the true extent of the damage could be worse.
It had been over a month since the buggy data deletion pipeline first ran, and that
maiden run itself removed hundreds of thousands of audio tracks that should not
have been removed. Was there any hope of getting the data back? If the tracks weren’t
recovered, or weren’t recovered fast enough, Google would have to face the music
from its users. How could we not have noticed this glitch?
Resolving the issue
Parallel bug identification and recovery efforts. The first step in resolving the issue was
to identify the actual bug, and determine how and why the bug happened. As long as
the root cause wasn’t identified and fixed, any recovery efforts would be in vain. We
would be under pressure to re-enable the pipeline to respect the requests of users who
deleted audio tracks, but doing so would hurt innocent users who would continue to
lose store-bought music, or worse, their own painstakingly recorded audio files. The
only way to escape the Catch-2210 was to fix the issue at its root, and fix it quickly.
Yet there was no time to waste before mounting the recovery effort. The audio tracks
themselves were backed up to tape, but unlike our Gmail case study, the encrypted
backup tapes for Google Music were trucked to offsite storage locations, because that
option offered more space for voluminous backups of users’ audio data. To restore
the experience of affected users quickly, the team decided to troubleshoot the root
cause while retrieving the offsite backup tapes (a rather time-intensive restore option)
in parallel.
The engineers split into two groups. The most experienced SREs worked on the
recovery effort, while the developers analyzed the data deletion code and attempted
to fix the data loss bug at its root. Due to incomplete knowledge of the root problem,
the recovery would have to be staged in multiple passes. The first batch of nearly half
a million audio tracks was identified, and the team that maintained the tape backup
system was notified of the emergency recovery effort at 4:34 p.m. Pacific Time on
March 8th.
The recovery team had one factor working in their favor: this recovery effort occur‐
red just weeks after the company’s annual disaster recovery testing exercise (see
[Kri12]). The tape backup team already knew the capabilities and limitations of their
subsystems that had been the subjects of DiRT tests and began dusting off a new tool
10 See http://en.wikipedia.org/wiki/Catch-22_(logic).
Case Studies | 363
they’d tested during a DiRT exercise. Using the new tool, the combined recovery team
began the painstaking effort of mapping hundreds of thousands of audio files to
backups registered in the tape backup system, and then mapping the files from back‐
ups to actual tapes.
In this way, the team determined that the initial recovery effort would involve the
recall of over 5,000 backup tapes by truck. Afterwards, datacenter technicians would
have to clear out space for the tapes at tape libraries. A long, complex process of regis‐
tering the tapes and extracting the data from the tapes would follow, involving work‐
arounds and mitigations in the event of bad tapes, bad drives, and unexpected system
interactions.
Unfortunately, only 436,223 of the approximately 600,000 lost audio tracks were
found on tape backups, which meant that about 161,000 other audio tracks were
eaten before they could be backed up. The recovery team decided to figure out how to
recover the 161,000 missing tracks after they initiated the recovery process for the
tracks with tape backups.
Meanwhile, the root cause team had pursued and abandoned a red herring: they ini‐
tially thought that a storage service on which Google Music depended had provided
buggy data that misled the data deletion pipelines to remove the wrong audio data.
Upon closer investigation, that theory was proven false. The root cause team
scratched their heads and continued their search for the elusive bug.
First wave of recovery. Once the recovery team had identified the backup tapes, the
first recovery wave kicked off on March 8th. Requesting 1.5 petabytes of data dis‐
tributed among thousands of tapes from offsite storage was one matter, but extracting
the data from the tapes was quite another. The custom-built tape backup software
stack wasn’t designed to handle a single restore operation of such a large size, so the
initial recovery was split into 5,475 restore jobs. It would take a human operator typ‐
ing in one restore command a minute more than three days to request that many
restores, and any human operator would no doubt make many mistakes. Just request‐
ing the restore from the tape backup system needed SRE to develop a programmatic
solution.11
By midnight on March 9th, Music SRE finished requesting all 5,475 restores. The tape
backup system began working its magic. Four hours later, it spat out a list of 5,337
backup tapes to be recalled from offsite locations. In another eight hours, the tapes
arrived at a datacenter in a series of truck deliveries.
11 In practice, coming up with a programmatic solution was not a hurdle because the majority of SREs are expe‐
rienced software engineers, as was the case here. The expectation of such experience makes SREs notoriously
hard to find and hire, and from this case study and other data points, you can begin to appreciate why SRE
hires practicing software engineers; see [Jon15].
364 | Chapter 26: Data Integrity: What You Read Is
While the trucks were en route, datacenter technicians took several tape libraries
down for maintenance and removed thousands of tapes to make way for the massive
data recovery operation. Then the technicians began painstakingly loading the tapes
by hand as thousands of tapes arrived in the wee hours of the morning. In past DiRT
exercises, this manual process proved hundreds of times faster for massive restores
than the robot-based methods provided by the tape library vendors. Within three
hours, the libraries were back up scanning the tapes and performing thousands of
restore jobs onto distributed compute storage.
Despite the team’s DiRT experience, the massive 1.5 petabyte recovery took longer
than the two days estimated. By the morning of March 10th, only 74% of the 436,223
audio files had been successfully transferred from 3,475 recalled backup tapes to dis‐
tributed filesystem storage at a nearby compute cluster. The other 1,862 backup tapes
had been omitted from the tape recall process by a vendor. In addition, the recovery
process had been held up by 17 bad tapes. In anticipation of a failure due to bad tapes,
a redundant encoding had been used to write the backup files. Additional truck deliv‐
eries were set off to recall the redundancy tapes, along with the other 1,862 tapes that
had been omitted by the first offsite recall.
By the morning of March 11th, over 99.95% of the restore operation had completed,
and the recall of additional redundancy tapes for the remaining files was in progress.
Although the data was safely on distributed filesystems, additional data recovery
steps were necessary in order to make them accessible to users. The Google Music
Team began exercising these final steps of the data recovery process in parallel on a
small sample of recovered audio files to make sure the process still worked as
expected.
At that moment, Google Music production pagers sounded due to an unrelated but
critical user-affecting production failure—a failure that fully engaged the Google
Music team for two days. The data recovery effort resumed on March 13th, when all
436,223 audio tracks were once again made accessible to their users. In just short of 7
days, 1.5 petabytes of audio data had been reinstated to users with the help of offsite
tape backups; 5 of the 7 days comprised the actual data recovery effort.
Second wave of recovery. With the first wave of the recovery process behind them, the
team shifted its focus to the other 161,000 missing audio files that had been deleted
by the bug before they were backed up. The majority of these files were store-bought
and promotional tracks, and the original store copies were unaffected by the bug.
Such tracks were quickly reinstated so that the affected users could enjoy their music
again.
However, a small portion of the 161,000 audio files had been uploaded by the users
themselves. The Google Music Team prompted their servers to request that the Goo‐
gle Music clients of affected users re-upload files dating from March 14th onward.
Case Studies | 365
This process lasted more than a week. Thus concluded the complete recovery effort
for the incident.
Addressing the root cause
Eventually, the Google Music Team identified the flaw in their refactored data dele‐
tion pipeline. To understand this flaw, you first need context about how offline data
processing systems evolve on a large scale.
For a large and complex service comprising several subsystems and storage services,
even a task as simple as removing deleted data needs to be performed in stages, each
involving different datastores.
For data processing to finish quickly, the processing is parallelized to run across tens
of thousands of machines that exert a large load on various subsystems. This distribu‐
tion can slow the service for users, or cause the service to crash under the heavy load.
To avoid these undesirable scenarios, cloud computing engineers often make a short-
lived copy of data on secondary storage, where the data processing is then performed.
Unless the relative age of the secondary copies of data is carefully coordinated, this
practice introduces race conditions.
For instance, two stages of a pipeline may be designed to run in strict succession,
three hours apart, so that the second stage can make a simplifying assumption about
the correctness of its inputs. Without this simplifying assumption, the logic of the
second stage may be hard to parallelize. But the stages may take longer to complete as
the volume of data grows. Eventually, the original design assumptions may no longer
hold for certain pieces of data needed by the second stage.
At first, this race condition may occur for a tiny fraction of data. But as the volume of
data increases, a larger and larger fraction of the data is at risk for triggering a race
condition. Such a scenario is probabilistic—the pipeline works correctly for the vast
majority of data and for most of the time. When such race conditions occur in a data
deletion pipeline, the wrong data can be deleted nondeterministically.
Google Music’s data deletion pipeline was designed with coordination and large mar‐
gins for error in place. But when upstream stages of the pipeline began to require
increased time as the service grew, performance optimizations were put in place so
Google Music could continue to meet privacy requirements. As a result, the probabil‐
ity of an inadvertent data-deleting race condition in this pipeline began to increase.
When the pipeline was refactored, this probability again significantly increased, up to
a point at which the race conditions occurred more regularly.
In the wake of the recovery effort, Google Music redesigned its data deletion pipeline
to eliminate this type of race condition. In addition, we enhanced production moni‐
366 | Chapter 26: Data Integrity: What You Read Is
toring and alerting systems to detect similar large-scale runaway deletion bugs with
the aim of detecting and fixing such issues before users notice any problems.12
General Principles of SRE as Applied to Data Integrity
General principles of SRE can be applied to the specifics of data integrity and cloud
computing as described in this section.
Beginner’s Mind
Large-scale, complex services have inherent bugs that can’t be fully grokked. Never
think you understand enough of a complex system to say it won’t fail in a certain way.
Trust but verify, and apply defense in depth. (Note: “Beginner’s mind” does not sug‐
gest putting a new hire in charge of that data deletion pipeline!)
Trust but Verify
Any API upon which you depend won’t work perfectly all of the time. It’s a given that
regardless of your engineering quality or rigor of testing, the API will have defects.
Check the correctness of the most critical elements of your data using out-of-band
data validators, even if API semantics suggest that you need not do so. Perfect algo‐
rithms may not have perfect implementations.
Hope Is Not a Strategy
System components that aren’t continually exercised fail when you need them most.
Prove that data recovery works with regular exercise, or data recovery won’t work.
Humans lack discipline to continually exercise system components, so automation is
your friend. However, when you staff such automation efforts with engineers who
have competing priorities, you may end up with temporary stopgaps.
Defense in Depth
Even the most bulletproof system is susceptible to bugs and operator error. In order
for data integrity issues to be fixable, services must detect such issues quickly. Every
strategy eventually fails in changing environments. The best data integrity strategies
12 In our experience, cloud computing engineers are often reluctant to set up production alerts on data deletion
rates due to natural variation of per-user data deletion rates with time. However, since the intent of such an
alert is to detect global rather than local deletion rate anomalies, it would be more useful to alert when the
global data deletion rate, aggregated across all users, crosses an extreme threshold (such as 10x the observed
95th percentile), as opposed to less useful per-user deletion rate alerts.
General Principles of SRE as Applied to Data Integrity | 367
are multitiered—multiple strategies that fall back to one another and address a broad
swath of scenarios together at reasonable cost.
Revisit and Reexamine
The fact that your data “was safe yesterday” isn’t going to help you tomorrow, or even
today. Systems and infrastructure change, and you’ve got to prove that your assump‐
tions and processes remain relevant in the face of progress. Consider the following.
The Shakespeare service has received quite a bit of positive press, and its user base is
steadily increasing. No real attention was paid to data integrity as the service was
being built. Of course, we don’t want to serve bad bits, but if the index Bigtable is lost,
we can easily re-create it from the original Shakespeare texts and a MapReduce.
Doing so would take very little time, so we never made backups of the index.
Now a new feature allows users to make text annotations. Suddenly, our dataset can
no longer be easily re-created, while the user data is increasingly valuable to our users.
Therefore, we need to revisit our replication options—we’re not just replicating for
latency and bandwidth, but for data integrity, as well. Therefore, we need to create
and test a backup and restore procedure. This procedure is also periodically tested by
a DiRT exercise to ensure that we can restore users’ annotations from backups within
the time set by the SLO.
Conclusion
Data availability must be a foremost concern of any data-centric system. Rather than
focusing on the means to the end, Google SRE finds it useful to borrow a page from
test-driven development by proving that our systems can maintain data availability
with a predicted maximum down time. The means and mechanisms that we use to
achieve this end goal are necessary evils. By keeping our eyes on the goal, we avoid
falling into the trap in which “The operation was a success, but the system died.”
Recognizing that not just anything can go wrong, but that everything will go wrong is
a significant step toward preparation for any real emergency. A matrix of all possible
combinations of disasters with plans to address each of these disasters permits you to
sleep soundly for at least one night; keeping your recovery plans current and exer‐
cised permits you to sleep the other 364 nights of the year.
As you get better at recovering from any breakage in reasonable time N, find ways to
whittle down that time through more rapid and finer-grained loss detection, with the
goal of approaching N =0. You can then switch from planning recovery to planning
prevention, with the aim of achieving the holy grail of all the data, all the time. Ach‐
ieve this goal, and you can sleep on the beach on that well-deserved vacation.
368 | Chapter 26: Data Integrity: What You Read Is
CHAPTER 27
Reliable Product Launches at Scale
Written by Rhandeev Singh and Sebastian Kirsch with Vivek Rau
Edited by Betsy Beyer
Internet companies like Google are able to launch new products and features in far
more rapid iterations than traditional companies. Site Reliability’s role in this process
is to enable a rapid pace of change without compromising stability of the site. We cre‐
ated a dedicated team of “Launch Coordination Engineers” to consult with engineer‐
ing teams on the technical aspects of a successful launch.
The team also curated a “launch checklist” of common questions to ask about a
launch, and recipes to solve common issues. The checklist proved to be a useful tool
for ensuring reproducibly reliable launches.
Consider an ordinary Google service—for example, Keyhole, which serves satellite
imagery for Google Maps and Google Earth. On a normal day, Keyhole serves up to
several thousand satellite images per second. But on Christmas Eve in 2011, it
received 25 times its normal peak traffic—upward of one million requests per second.
What caused this massive surge in traffic?
Santa was coming.
A few years ago, Google collaborated with NORAD (the North American Aerospace
Defense Command) to host a Christmas-themed website that tracked Santa’s progress
around the world, allowing users to watch him deliver presents in real time. Part of
the experience was a “virtual fly-over,” which used satellite imagery to track Santa’s
progress over a simulated world.
While a project like NORAD Tracks Santa may seem whimsical, it had all the charac‐
teristics that define a difficult and risky launch: a hard deadline (Google couldn’t ask
369
Santa to come a week later if the site wasn’t ready), a lot of publicity, an audience of
millions, and a very steep traffic ramp-up (everybody was going to be watching the
site on Christmas Eve). Never underestimate the power of millions of kids anxious
for presents—this project had a very real possibility of bringing Google’s servers to
their knees.
Google’s Site Reliability Engineering team worked hard to prepare our infrastructure
for this launch, making sure that Santa could deliver all his presents on time under
the watchful eyes of an expectant audience. The last thing we wanted was to make
children cry because they couldn’t watch Santa deliver presents. In fact, we dubbed
the various kill switches built into the experience to protect our services “Make-
children-cry switches.” Anticipating the many different ways this launch could go
wrong and coordinating between the different engineering groups involved in the
launch fell to a special team within Site Reliability Engineering: the Launch Coordi‐
nation Engineers (LCE).
Launching a new product or feature is the moment of truth for every company—the
point at which months or years of effort are presented to the world. Traditional com‐
panies launch new products at a fairly low rate. The launch cycle at Internet compa‐