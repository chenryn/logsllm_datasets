digits (0-9) encoded as 8-bits grayscale images of size 28 × 28
pixels (one color channel per pixel).
Target Models. We tested the aforementioned attacks on the
same pre-trained models as in Feature Squeezing [44]. Namely,
we use MobileNet [20] for ImageNet, a 7-layer CNN for MNIST4,
and a DenseNet model for CIFAR-105 [21]. These models achieve
a top-1 accuracy of 99.43%, 94.84%, and 68.36%, respectively.
The prediction performance of these models is at par with
best models6. To study the effects of introducing randomness,
we use the same data samples as used by Xu et al. [44]. We
use 100 adversarial samples for each of the 11 attacks for all
datasets. Each color channel of the pixel is normalized to be in
the range [0, 1]. We use 10 000 legitimate samples for CIFAR-10
and MNIST, and 200 samples for ImageNet (due to high com-
putation cost) to study the effect of adding randomness to the
defenses.
Experimental Setup. We evaluate the efficacy of the 3 de-
fenses proposed by Xu et al. [44]—bit depth reduction, median
smoothing and non-local smoothing, when combined with
randomness. We study each of the 11 attacks against the 3 de-
fenses with varying parameters for 3 datasets. The experiment
is repeated 200 times for each randomness level to compute
the statistics. In our evaluation, the accuracy over adversarial
samples is averaged over 200 runs. We note that for the deter-
ministic case when no randomness is added (δ = 0) the results
do not change.
Implementation. We implemented Randomized Squeezing in
Python and executed it using CPython. Namely, we adapted
the open source code released by Xu et al.7 to implement our
solution. We use a machine with 3.5 GHz processor and 32 GB
RAM for our experiments.
5.2 Graybox Adversaries
This section presents the results of our evaluation of Random-
ized Squeezing, compared to the deterministic Feature Squeez-
ing, against graybox attacks.
Choosing δ. Choosing the randomness magnitude δ appro-
priately is vital to designing an effective defense. The choice
depends upon the nature of dataset and defense used. As we
show in the paragraphs ahead the behavior of defenses can
vary for grayscale and color images. We study these variations
extensively by running experiments for changing δ. We present
our evaluation of ImageNet, CIFAR-10 and MNIST datasets
next.
ImageNet. Figure 3 shows the behavior of accuracy of the
classifier for both adversarial and legitimate samples as input
randomization (δ) is increased. The accuracy decreases as δ
4https://github.com/carlini/nn_robust_attacks/
5https://github.com/titu1994/DenseNet/
6http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_
results.html
7https://github.com/mzweilin/EvadeML-Zoo
is increased. Squeezing via reducing bit-depth shows a dras-
tic drop in accuracy beyond a certain randomness for most
attacks; as δ increases, a larger fraction of pixels breach the
quantization threshold which results in them being flipped
to 0 or 1 despite being very distant earlier, this results in ac-
curacy dropping sharply. The CW0 adversarial samples show
an improvement in accuracy for high δ, this is due to large L0
perturbations being undone due to noise. Median smoothing
methods are less affected by large randomness values due to
the randomness being averaged out. Hence we observe a grad-
ual decline in accuracy with increasing randomness. Note that
CIFAR-10 has color and each pixel has three color channels
each of which are normalized to [0, 1]. We introduce random-
ness individually to each channel for each pixel, hence the
effect of randomness becomes significant even at low δ values.
We conclude that an appropriately chosen δ provides desir-
able security properties, we found that δ = 0.1 provides the
best trade-off between accuracy on legitimate and adversarial
samples. We present the accuracy values in Table 2. We note
that the accuracy decreases slightly over legitimate samples.
For comparison, Table 1 shows the accuracy of defenses when
no randomness is added [44].
We further compare the robustness of Randomized Squeez-
ing with that of the other two randomized defenses, Cropping-
Rescaling and Region-Based Classification. As advocated in [16],
Cropping-Rescaling achieves an accuracy of 45-65% against
FGSM, while Region-Based Classification and Randomized
Squeezing achieve an accuracy of 34.7% and 53.44% (with me-
dian smoothing), respectively. Similarly, for CW2 Next (Next
and LL), Cropping-Rescaling achieves an accuracy of 40-65%
(adapted from [16]), while Region-Based Classification and
Randomized Squeezing achieve an accuracy of 79% and 70%
(with median smoothing), respectively. This shows that online
randomization defenses offer decent robustness to gray-box
attacks even when compared to offline defenses.
CIFAR-10. The results for running Randomized Squeezing
within the CIFAR-10 dataset are similar to ImageNet: adding
randomness helps make misclassified samples unpredictable.
Figure 4 shows that accuracy over both legitimate and adver-
sarial samples drops sharply on increasing δ. Identical to our
evaluation in the ImageNet dataset, we note that Randomized
Squeezing introduces small randomness for each color channel.
The accuracy over adversarial samples improves significantly
at δ = 0.05 for almost all defenses with just a small drop over
legitimate samples (cf. Tables 1 & 2). Large values of δ make
the classifier unusable as accuracy drops.
MNIST. As seen in Figure 5, the results of running Random-
ized Squeezing with the MNIST dataset are similar to Ima-
geNet and CIFAR-10. We present the accuracy values in Ta-
bles 1 & 2. The behavior of accuracy for the MNIST dataset is
similar to that of ImageNet and CIFAR-10 (cf. Figures 4).
Figure 6 shows the probabilities of the prediction errors
when used with 3 × 3 median smoothing with and without
randomness. Each row represents a specific attack strategy A;
the x-axis represents the adversarial sample set XA (in par-
ticular |XA| = 100); the color intensity of each cell indicates
Figure 3: ImageNet: Behavior of accuracy for magnitudes of randomness δ = [0, 0.05, 0.1, 0.2, 0.3]. We also plot the accuracy of
the model for legitimate samples as δ increases (shown once for each defense as the curve does not change).
Figure 4: CIFAR-10: Behavior of accuracy for magnitudes of randomness δ = [0, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]. We also plot the
accuracy of the model for legitimate samples as δ increases (shown once for each defense as the curve does not change).
the estimated probability (over the classifier’s randomness)
that the corresponding adversarial sample succeeds (note, this
probability is binary for a deterministic classifier). However,
when randomness is introduced the error probabilities spread
out for a large number of samples as they are no longer deter-
ministic. For each of the considered attacks A, the empirical
error over XA can be computed by summing up the prob-
abilities that each adversarial sample in XA succeeds, i.e.,
i=1 Pr [C$(xi) (cid:44) f (xi)]. For ease of presentation,
errXA (C$) = 100
we only present the results applied to one defense in order to
to demonstrate the effect of randomness, the results for other
defenses are similar.
Interpretation of Results. Increase in the magnitude of ran-
domness drives the accuracy of the classifier over legitimate
0.00.2δ0.00.51.0Acc./Adv.FGSMBIMCWInf(Next)CWInf(LL)DeepFoolCW2(Next)CW2(LL)CW0(Next)CW0(LL)BitDepth:5-bitBitDepth:4-bitMedianSmoothing:2X2MedianSmoothing:3X3Non-localMeans:11-3-4AccuracyAdversarialAccuracyLegitimate0.00.5δ0.00.51.0Acc./Adv.FGSMBIMCWInf(Next)CWInf(LL)DeepFoolCW2(Next)CW2(LL)CW0(Next)CW0(LL)JSMA(Next)JSMA(LL)BitDepth:5-bitBitDepth:4-bitMedianSmoothing:2X2Non-localMeans:11-3-4AccuracyAdversarialAccuracyLegitimateFigure 5: MNIST: Behavior of accuracy for δ = [0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]. We also plot the accuracy of the model for
legitimate samples as δ increases (shown once for each defense as the curve does not change).
Figure 6: MNIST: Unpredictability of errors for Median Smoothing (3 × 3) defense, without randomness (top figure, δ = 0)
and with randomness (bottom figure, δ = 0.5)
samples towards 10% as classification becomes akin to guess-
ing (for classification over 10 classes as in MNIST and CIFAR-
10). We made a deliberate choice to clip the pixel values when
they go outside the allowed bounds of [0, 1] rather than wrap-
ping around. A value of δ = 1 and wrapping around the pixel
values when they go out of bounds produces a truly random
pixel, and hence the image. We found that at this level of ran-
domness, accuracy over legitimate samples becomes close to
10%. Even lower values of δ produce a sharp drop in accuracy
over legitimate samples, hence we choose to clip the values
when they go out of bounds.
The primary motivation for our design of Randomized
Squeezing is to perturb the pixels in a manner which subsumes
the adversarial perturbation, and to which the adversary can-
not adapt while keeping the usefulness of the classifier intact.
The optimum magnitude of randomness δ to be used is contin-
gent on the defense used. High values of δ have strong effect
on the accuracy when used in conjunction with bit depth re-
duction, as it could change the value of a pixel drastically if the
bit depth is low. In contrast, methods like local and non-local
smoothing are much more resilient to high δ, as they average
out the noise from sections of images. The noise that we add,
being additive, is filtered out.
Note that we want to use the largest value of δ possible so as
to subsume the adversarial perturbations, while still maintain-
ing high accuracy. Not all defenses are equally potent for all
attacks and datasets, therefore the randomness magnitude δ
0.00.5δ0.00.51.0Acc./Adv.FGSMBIMCWInf(Next)CWInf(LL)CW2(Next)CW2(LL)CW0(Next)CW0(LL)JSMA(Next)JSMA(LL)BitDepth:1-bitMedianSmoothing:2X2MedianSmoothing:3X3AccuracyAdversarialAccuracyLegitimateFGSMBIMCWInf(Next)CWInf(LL)CW2(Next)CW2(LL)CW0(Next)CW0(LL)JSMA(Next)JSMA(LL)FGSMBIMCWInf(Next)CWInf(LL)CW2(Next)CW2(LL)CW0(Next)CW0(LL)JSMA(Next)JSMA(LL)0.00.20.40.60.81.0Table 1: Accuracy of original Feature Squeezing defenses without randomness (δ = 0) over adversarial samples (%). Xu et
al. [44] omit DeepFool on MNIST as the adversarial samples generated appear unrecognizable to humans; non-local smooth-
ing is not applied to MNIST as it is hard to find similar patches on such images for smoothing a center patch. JSMA is omitted
for ImageNet due to large memory requirement.
Squeezer
L∞ Attacks
L2 Attacks
L0 Attacks
Dataset
Name
Parameters
FGSM BIM
CW2
CW0
JSMA
None
MNIST
Bit Depth
Median Smoothing
None
CIFAR-10
Bit Depth
Median Smoothing
Non-local Means
None
ImageNet
Bit Depth
Median Smoothing
Non-local Means
1-bit
2 × 2
3 × 3
5-bit
4-bit
2 × 2
11-3-4
5-bit
4-bit
2 × 2
3 × 3
11-3-4
54
92
61
59
15
17
21
38
27
1
2
5
22
33
10
9
87
16
14
8
13
29
56
46
0
0
4
28
41
25
CW∞
Next
0
100
70
43
0
12
69
84
80
0
33
66
75
73
77
LL
0
100
55
46
0
19
74
86
84
0
60
79
81
76
82
DeepFool
-
-
-
-
2
40
72
83
76
11
21
44
72
66
57
Next LL Next LL Next LL
40
49
56
79
0
17
20
76
32
-
-
-
-
-
-
0
83
51
51
0
40
84
87
84
10
68
84
81
77
87
0
66
35
53
0
47
84
83
88
3
66
82
84
79
86
0
0
39
67
0
0
7
88
11
0
7
38
85
81
43
0
0
36
59
0
0
10