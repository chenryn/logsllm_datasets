2.  接下来，为了便于使用，我们希望在路径中添加 CLI 工具:
    ```
    cd istio-
    export PATH=$PWD/bin:$PATH
    ```
3.  Now, let's install Istio! Istio configurations are called *profiles* and, as mentioned previously, they can be completely customized using a YAML file.
    在这个演示中，我们将使用带 Istio 的内置`demo`配置文件，它提供了一些基本的设置。使用以下命令安装配置文件:
    ```
    istioctl install --set profile=demo
    ```
    这将导致以下输出:
    ![Figure 14.3 – Istioctl profile installation output](img/B14790_14_003.jpg)
    图 14.3–Istioctl 剖面安装输出
4.  Since the sidecar resource has not been released yet as of Kubernetes 1.19, Istio will itself inject Envoy proxies into any namespace that is labeled with `istio-injection=enabled`.
    要用此标记任何命名空间，请运行以下命令:
    ```
    kubectl label namespace my-namespace istio-injection=enabled
    ```
5.  To test easily, label the `default` namespace with the preceding `label` command. Once the Istio components come up, any Pods in that namespace will automatically be injected with the Envoy sidecar, just like we created manually in the previous section.
    要从集群中删除 Istio，请运行以下命令:
    ```
    istioctl x uninstall --purge
    ```
    这将导致一条确认消息，告诉您 Istio 已被删除。
6.  Now, let's deploy a little something to test our new mesh with! We will deploy three different application services, each with a deployment and a service resource:
    a.服务前端
    b.服务后端
    c.服务后端
    以下是*服务前端*的部署:
    Istio-service-deployment.yaml:
    ```
    apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: service-frontend
    spec:
      replicas: 1
      template:
        metadata:
          labels:
            app: service-frontend
            version: v2
        spec:
          containers:
          - name: service-frontend
            image: ravirdv/http-responder:latest
            ports:
            - containerPort: 5000
              name: svc-port
              protocol: TCP
    ```
    这里是*服务前端*的服务:
    机构服务：
    ```
    apiVersion: v1
    kind: Service
    metadata:
      name: service-frontend
    spec:
      selector:
        name: service-frontend
      ports:
        - protocol: TCP
          port: 80
          targetPort: 5000
    ```
    服务后端 A 和 B 的 YAML 将与*服务前端*相同，除了交换名称、映像名称和选择器标签。
7.  Now that we have a couple of services to route to (and between), let's start setting up some Istio resources!
    首先，我们需要一个`Gateway`资源。在这种情况下，我们不使用 NGINX 入口控制器，但这很好，因为 Istio 提供了一个`Gateway`资源，可以用于入口和出口。以下是“T2”的定义:
    Istio-gateway.yaml：
    ```
    apiVersion: networking.istio.io/v1alpha3
    kind: Gateway
    metadata:
      name: myapplication-gateway
    spec:
      selector:
        istio: ingressgateway
      servers:
      - port:
          number: 80
          name: http
          protocol: HTTP
        hosts:
        - "*"
    ```
    这些`Gateway`定义看起来非常类似于入口记录。我们有`name`和`selector`，Istio 使用它们来决定使用哪个 Istio 入口控制器。接下来，我们有一个或多个服务器，它们本质上是我们网关上的入口点。在这种情况下，我们不限制主机，我们接受端口`80`上的请求。
8.  Now that we have a gateway for getting requests into our cluster, we can start setting up some routes. We do this in Istio using `VirtualService`. `VirtualService` in Istio is a set of routes that should be followed when requests to a particular hostname are made. In addition, we can use a wildcard host to make global rules for requests from anywhere in the mesh. Let's take a look at an example `VirtualService` configuration:
    Istio-virtual-service-1.yaml:
    ```
    apiVersion: networking.istio.io/v1alpha3
    kind: VirtualService
    metadata:
      name: myapplication
    spec:
      hosts:
      - "*"
      gateways:
      - myapplication-gateway
      http:
      - match:
        - uri:
            prefix: /app
        - uri:
            prefix: /frontend
        route:
        - destination:
            host: service-frontend
            subset: v1
    ```
    在这个`VirtualService`中，如果请求与我们的`uri`前缀中的一个匹配，我们将请求路由到位于*服务前端*的入口点的任何主机。在这种情况下，我们在前缀上匹配，但是你也可以通过在 URI 匹配器中用`exact`替换`prefix`来使用精确匹配。
9.  So, now we have a setup fairly similar to what we would expect with an NGINX Ingress, with entry into the cluster dictated by a route match.
    然而，我们路线上的`v1`是什么？这实际上代表了我们的*前端服务*的一个版本。让我们继续并使用新的资源类型-Istio`DestinationRule`来指定这个版本。以下是`DestinationRule`配置的样子:
    istio-目的地-规则-1.yaml:
    ```
    apiVersion: networking.istio.io/v1alpha3
    kind: DestinationRule
    metadata:
      name: service-frontend
    spec:
      host: service-frontend
      subsets:
      - name: v1
        labels:
          version: v1
      - name: v2
        labels:
          version: v2
    ```
    如您所见，我们在 Istio 中指定了两个不同版本的前端服务，每个版本都查看一个标签选择器。从我们之前的部署和服务中，您可以看到我们当前的前端服务版本是`v2`，但是我们可以并行运行这两个版本！通过在入口虚拟服务中指定我们的`v2`版本，我们告诉 Istio 将所有请求路由到服务的`v2`。此外，我们还配置了我们的`v1`版本，这在之前的`VirtualService`中有参考。这个硬规则只是将请求路由到 Istio 中不同子集的一种可能方式。
    现在，我们已经设法通过网关将流量路由到我们的集群，并根据目的地规则路由到虚拟服务子集。在这一点上，我们有效地“进入”了我们的服务网格！
10.  Now, from our *Service Frontend*, we want to be able to route to *Service Backend A* and *Service Backend B*. How do we do this? More virtual services is the answer! Let's take a look at a virtual service for *Backend Service A*:
    Istio-virtual-service-2.yaml:
    ```
    apiVersion: networking.istio.io/v1alpha3
    kind: VirtualService
    metadata:
      name: myapplication-a
    spec:
      hosts:
      - service-a
      http:
        route:
        - destination:
            host: service-backend-a
            subset: v1
    ```
    如您所见，该`VirtualService`路由到我们服务的`v1`子集`service-backend-a`。我们还需要另一个`service-backend-b`的`VirtualService`，我们不会全部包括在内(但看起来几乎一样)。要查看完整的 YAML，请查看`istio-virtual-service-3.yaml`的代码库。
11.  一旦我们的虚拟服务准备好了，我们就需要一些目的地规则！*后端服务 A* 的`DestinationRule`如下:
istio-目的地-规则-2.yaml:
```
apiVersion: networking.istio.io/v1alpha3
kind: DestinationRule
metadata:
  name: service-backend-a
spec:
  host: service-backend-a
  trafficPolicy:
    tls:
      mode: ISTIO_MUTUAL
  subsets:
  - name: v1
    labels:
      version: v1
```
而*后端服务 B* 的`DestinationRule`也差不多，只是子集不同。我们不会包含代码，但是检查代码库中的`istio-destination-rule-3.yaml`以了解确切的规格。
这些目的地规则和虚拟服务加起来构成了以下路由图:
![Figure 14.4 – Istio routing diagram](img/B14790_14_004.jpg)
图 14.4–Istio 布线图
如您所见，来自*前端服务* Pods 的请求可以路由到*后端服务 A 版本 1* 或*后端服务 B 版本 3* ，并且每个后端服务也可以路由到另一个。这些对后端服务 A 或 B 的请求还涉及到 Istio 最有价值的特性之一——相互(双向)TLS。在此设置中，TLS 安全性在网格中的任意两点之间保持，这一切都是自动发生的！
接下来，让我们看一下使用 Kubernetes 的无服务器模式。
# 在 Kubernetes 上实现无服务器
云提供商的无服务器模式很快得到了的普及。无服务器体系结构由可以自动上下扩展的计算组成，甚至可以一直扩展到零(零计算容量被用来为功能或其他应用服务)。**功能即服务** ( **FaaS** )是无服务器模式的扩展，其中功能代码是唯一的输入，无服务器系统负责根据需要将请求路由到计算和扩展。AWS Lambda、Azure Functions 和 Google Cloud Run 是云提供商官方支持的一些更受欢迎的 FaaS/无服务器选项。Kubernetes 也有许多不同的无服务器框架和库，可以用来运行无服务器、零扩展的工作负载以及 Kubernetes 上的 FaaS。一些最受欢迎的如下:
*   *激活*
*   *无库*
*   *OpenFaaS*
*   裂变
关于 Kubernetes 上所有无服务器选项的完整讨论超出了本书的范围，因此我们将关注两个不同的选项，它们旨在为两个截然不同的用例提供服务: *OpenFaaS* 和 *Knative* 。
虽然 Knative 具有高度可扩展性和可定制性，但它使用了多个耦合组件，增加了复杂性。这意味着需要一些额外的配置来开始使用 FaaS 解决方案，因为函数只是 Knative 支持的许多其他模式之一。另一方面，OpenFaaS 使得在 Kubernetes 上启动和运行无服务器和 FaaS 变得非常容易。由于不同的原因，这两种技术都很有价值。
在这一章的教程中，我们将看看 Knative，它是最流行的无服务器框架之一，也通过其事件特性支持 FaaS。
## 在 Kubernetes 河上使用 FaaS 的克纳托
如前所述，Knative 是 Kubernetes 上无服务器模式的一组模块化构建块。由于这个原因，在我们能够进入实际功能之前，它需要一些配置。Knative 也可以安装 Istio，它将 Istio 用作路由和扩展无服务器应用的基底。其他非 Istio 路由选项也可用。
要在 FaaS 使用克纳蒂，我们需要安装*克纳蒂服务*和*克纳蒂事件*。虽然 knactivity service 将允许我们运行无服务器工作负载，但 knactivity Eventing 将提供向这些零扩展工作负载发出 FaaS 请求的途径。让我们通过以下步骤来实现这一点:
1.  首先，让我们安装主动服务组件。我们将从安装 CRDs 开始:
    ```
    kubectl apply --filename https://github.com/knative/serving/releases/download/v0.18.0/serving-crds.yaml
    ```
2.  接下来，我们可以自己安装服务组件:
    ```
    kubectl apply --filename https://github.com/knative/serving/releases/download/v0.18.0/serving-core.yaml
    ```
3.  在这一点上，我们需要安装一个网络/路由层，供 Knative 使用。让我们使用 Istio:
    ```
    kubectl apply --filename https://github.com/knative/net-istio/releases/download/v0.18.0/release.yaml
    ```
4.  我们需要 Istio 的网关 IP 地址。根据您运行该程序的位置(换句话说，AWS 或本地)，该值可能会有所不同。使用以下命令拉动它:
    ```
    Kubectl get service -n istio-system istio-ingressgateway
    ```
5.  Knative requires a specific DNS setup for enabling the serving component. The easiest way to do this in a cloud setting is to use `xip.io` "Magic DNS," though this will not work for Minikube-based clusters. If you're running one of these (or just want to see all the options available), check out the Knative docs at [https://knative.dev/docs/install/any-kubernetes-cluster/](https://knative.dev/docs/install/any-kubernetes-cluster/).
    要设置魔法域名系统，请使用以下命令:
    ```
    kubectl apply --filename https://github.com/knative/serving/releases/download/v0.18.0/serving-default-domain.yaml
    ```
6.  现在我们已经安装了 Knative Serving，让我们安装 Knative Eventing 来交付我们的 FaaS 请求。首先，我们需要更多的 CRD。使用以下命令安装它们:
    ```
    kubectl apply --filename https://github.com/knative/eventing/releases/download/v0.18.0/eventing-crds.yaml
    ```
7.  Now, install the eventing components just like we did with serving:
    ```
    kubectl apply --filename https://github.com/knative/eventing/releases/download/v0.18.0/eventing-core.yaml
    ```
    此时，我们需要为我们的事件系统添加一个队列/消息层来使用。我们提到 Knative 支持许多模块化组件了吗？
    重要说明