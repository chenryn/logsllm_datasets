or the access control list (ACL) in VFDs, configured by the tenant,
is inappropriate. Unfortunately, all these non-fault causes do lead
to problems and are hard for operations engineers to locate the
root cause as they are invisible to the cloud provider or are indeed
normal operations.
When a tenant suffers a packet dropping problem in his own
virtual network, a fault diagnosis request will be submitted to the
cloud provider, which describes the flows and components that are
experiencing problems. In addition, a series of failure detectors are
deployed and expect to find problems before tenants sense them.
These detectors generate alarms when pre-defined anomaly condi-
tions are breached. For instance, the throughput of a group of VMs
suddenly decreases (e.g., from 10GB/s to 3GB/s), or a surge increase
in the average or 99th percentile latency. Once a diagnosis request
or an alarm is received, operations engineers will determine the
set of suspicious VMs and flows with a filter including following
fields: srcVM_ID, dstVM_ID (IDs of sVM and dVM), NIC_ID (ID of
network inferface card) and 5-tuple (src_IP, dst_IP, src_Port,
dst_Port and Protocol). Then, VTrace is called to perform the
automatic diagnosis for virtual flows that are specified by the input
parameters. Note that the root cause returned by VTrace refers
to the concrete information that is useful in troubleshooting the
packet loss.
3 DESIGN AND IMPLEMENTATION
3.1 Design Goal and Requirements
We design VTrace to troubleshoot persistent packet loss issues in
the overlay network automatically while addressing the challenges
outlined in Section 1. Specifically, a two-tier goal should be satisfied:
(1) For normal scenarios, the complete journey of target packets will
be displayed; (2) When a packet loss happens, VTrace aims to locate
the problematic node(s) and present the underlying root cause. To
fix the problem, besides the root cause, tenants and operations
engineers need different types of location information, where the
former needs to know the culprit VFD while the physical server in
which the culprit VFD resides is vital for the latter. VTrace should
be able to provide both types of locations.
In the cloud network, there could be multiple ways to implement
such a system. However, considering that the cloud provider is the
supplier of services, there are several requirements for VTrace to
meet throughout its realization:
Requirement 1: To extract the on-site cause of a packet loss,
VTrace should be designed oriented to the packets in the virtual
production flow over the cloud-scale overlay network.
Requirement 2: The deployment of VTrace should be impercepti-
ble to tenants. It means that VTrace incurs an affordable overhead
on network functions, e.g., packet forwarding.
Requirement 3: VTrace should not request tenants to help im-
plement it, which implies that cloud tenants do nothing with the
packet when it leaves the tenants’ VMs.
Requirement 4: To quickly repair the fault, VTrace needs to be re-
sponsive, especially in the situation with a potentially large number
of VTrace tasks.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
C. Fang et al.
3.2 VTrace Overview
3.2.1 VTrace Architecture. Based on the two-tier goal of VTrace
and design requirements, we have designed VTrace architecture as
presented in Fig. 2. The operation of VTrace is as follows. When a
new VTrace task comes, VTrace Application creates a record for
the new task in the VTrace Task DataBase. In the meantime, the
controller configures cloud VFDs so that they will process target
packets with pre-defined rules. During this process, the relevant
logs are generated and then archived in the respective local Log
Agents. On the other hand, JStorm periodically queries the VTrace
Task Database for unprocessed tasks. When a new task is found,
JStorm starts reading the related log data from globally-distributed
Log Agents, and reconstructs virtual flow paths, followed by show-
ing the diagnostic results. In VTrace, all these components are
divided into two main parts as follows.
VTrace data generation (VDG). This part is at the core of
VTrace, which enables the automatic data generation about the
target packets’ trajectories and the root cause. In this part, the cloud
controller translates the received diagnosis request in configuration
policies and installs them into the network-wide VFDs to track and
analyze a subset of target packets (to be given in detail in Section
3.3).
Data collection and analysis (DCA). The log data of one
packet’s trajectory, generated in VDG, could be stored in Log Agents
that are distributed across different regions. DCA collects all rel-
evant log data for each task to reconstruct the target virtual flow
paths. Then, VTrace knows if a packet loss problem has occurred,
thereby telling the user the root cause contained in the log or dis-
playing an "OK" result.
3.2.2 VTrace API. Atop this architecture, VTrace exposes APIs
for users to issue tasks with addVTrace and query results via
getVTraceResult. Note that for each task, VTrace Application
checks and ensures that the same task will not be issued again
before it is completed to avoid conflicts.
addVTrace(filter, packet_count, trace_time): The possi-
ble virtual flows to be traced are specified by filter. The 5-tuple
in the filter specifies the granularity of the target virtual flow,
where Protocol includes TCP, UDP and ICMP, etc. Note that
VTrace users can only input parameters for some, not necessar-
ily all, fields of 5-tuple to specify a larger set of virtual flows, e.g.,
5-tuple=(IP_A,IP_B,*,*,TCP). The packet_count indicates the
number of packets to be tracked. The trace_time represents the
maximum time that VFDs use to match VTrace-interested packets.
getVTraceResult(filter): Users employ this API to query
task results with parameter filter.
3.3 VDG Design
To meet the design goal, we need to pinpoint the problematic node
first. This prerequisite knowledge can be obtained by tracing, col-
lection and analysis of target packets’ behavior at each hop. To
facilitate the automation of packet tracing and collection, we resort
to the VFD’s voluntary report and record of events. There are two
main ways to achieve it.
One feasible solution is that SDN-enabled switches proactively
send and then receive a message to and from the controller when
a target packet is observed [1]. Yet, the messages returned by the
34
Figure 2: VTrace architecture.
controller will be forwarded undesirably, inducing unexpected extra
network overhead. To avoid this, it requires to assign each switch
a tag such that no two directly connected switches have the same
tag. In this way, messages from the controller can be distinguished
from the traffic from adjacent switches. Usually, a graph coloring
algorithm is used to minimize the total number of tags, which is NP-
hard. Coupled with the high processing pressure of the controller
to handle a huge number of switches’ requests, this solution is not
suitable for our sheer-scale production cloud network.
The other possible method is to match and mirror the packets
of interest by VFDs. In [7, 26], the "match and mirror" action is
realized by leveraging the processing capacity of switching ASIC.
In these works, the mirrored packets are without any deep check
and thus have no insight into the on-site root cause of a packet
loss, which can not meet our diagnostic goal. But, the idea is worth
learning, as it only uses the basic functionality of switches and may
incur affordable overhead when applied in the cloud-scale overlay
network.
In addition, these packet-level tracing works such as [1, 7, 26]
focus on locating the culprit node. They still leave it to network
experts to diagnose the specific cause of network packet loss. In
our design, besides collecting the packet’s footprint data, VTrace
utilizes the deep inspection capability of the VFD’s slow path and
the inherent forwarding structure of VFDs to extract the on-the-
spot forwarding situation of virtual production traffic.
To achieve the two-tier goal of VTrace, we propose our core
idea as "coloring, matching and logging". Given that the VFD’s
slow path can process target packets in depth, our naive design
for this idea is to move all target packets to the slow path of the
VFD they pass through for further inspection. And all "coloring,
matching and logging" operations will be completed in the slow
path. Unfortunately, we find such a design unsatisfactory. The
corresponding reasons and our improvements are as follows.
First, the production traffic is forwarded in the fast path for high-
performance packet processing. It is concerning that due to the
discrepancy between the two paths, some packets are supposed
to be discarded in the fast path whereas they will be forwarded
normally if they are switched to the slow path. It means that the
packet drop in the production traffic might be imperceptible in
UsersVTraceApplicationVTraceTaskVTraceData GenerationControllerSrcDstVFD1VFD2VFD3……JStormProcess EngineVTraceTask DatabaseResults DatabaseShanghaiLog AgentUS‐EastLog AgentEU‐CentralLog AgentData Collection and AnalysisVTrace
SIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
Table 1: Debug probe examples.
Type of cause
Tenant security policy blocking
Tenant configuration error
Debug probe example
rx(tx)_acl_error: There are problems with
the ACL in the ingress (egress) of VFDs
vpc_route_error: The packet is discarded due
to the miss of VPC route configured by tenants
perform the "coloring" operation on packet_count packets. And all
VFDs are configured to match any packets with the colored tag. In
such a setting, it may still affect the forwarding performance if the
throughput of the traced flow is too large. This is because tracking
packet_count packets in a wink will lead to a tracing load burst. To
avoid this issue, a rate limit can be set for VTrace-interested flows
during the VTrace task. This operation is reasonable for problematic
flows, especially those that encounter persistent packet loss.
Then, each VFD will generate logs for the target packet in its
ingress and egress, and store the log data in the local Log Agent.
Existing systems such as NetSight [7], VND [24] and Everflow
[26] proposed to transmit the data to dedicated servers. In VTrace,
we turn to utilize the globally distributed Log Agents, which is a
mature product in our production cloud network, to record the log
data for the following reasons. First, using our Log Agent product
can quickly develop reliable log applications based on abundant
experience, which is convenient, easy to manage and incurs almost
few deployment costs. Second, such a solution is general since most
cloud systems have similar logging systems, such as Stackdriver
Logging [13] in Google cloud and AWS and the third-party logging
system Datadog [5].
3.4 VDG Implementation
VFDs such as vSwitches and vRouters are realized in a software
manner, e.g., Open vSwitch [23]. In our implementation, VFDs are
implemented in C language. In VTrace, we utilize VFD’s software
self-defining property to achieve automatic reporting of trace data
at a relatively low cost. The procedures of VDG implementation
in VFDs are illustrated in Fig. 4 and explained as follows. Our
evaluation in Section 4.1 shows that such a implementation brings
a negligible impact on the forwarding performance of VFDs.
Step 1: Coloring the target packet. First, we note that the source
VFD (e.g., the VFD 1 and VFD 𝑛 in Fig. 4) is responsible for coloring
target packets. Once a VTrace task is issued, the parameters filter,
packet_count and trace_time are specified by the VTrace user.
The cloud controller translates these parameters into configura-
tion policies and installs them into all source VFDs in the overlay
network. Source VFDs only "color" the target packets that have
not been "colored" when neither packet_count nor trace_time
is reached. Specifically, the "coloring" operation is done by setting
the field dscp in the packet header to a specific value, indicating
that the packet should be traced by the VFDs in the ensuing hops.
Given that dscp is also used for QoS management, in our produc-
tion network, the dscp value is carefully chosen and dedicated to
VTrace, in order to avoid conflicts with normal QoS management of
different levels of traffic. Besides, all operations-related dscp values
are prioritized according to their business scenarios, which ensures
that VTrace "coloring" will not influence those critical operations.
Figure 3: The forwarding model in a VFD.
the slow path, which contradicts with Requirement 1. To address
this concern, VTrace makes use of the inherent "fast path-slow
path" structure of VFDs [4]. Specifically, VTrace-enabled VFDs are
configured to guide the first target packet of a VTrace task into
the slow path for deep analysis (as Fig. 3 shows) such as forward-
ing lookup, NAT/DNAT (destination network address translation)
lookup and safety/security lookup. Besides, to extract on-site rea-
sons for packet drops, each VTrace-enabled VFD deploys debug
probes (i.e., if-else logics) at all software locations where packets are
possibly discarded. The most common debug probe examples are
listed in Table 1. For instance, an error code rx_acl_error (and a
detailed error description) will be raised when a packet is dropped
upon its arrival due to the configured ACL. These error-prone loca-
tions are selected based on operations engineers’ experience with
vSwitch and vRouter faults. In our real-life deployment, totally
more than 400 debug probes are deployed. After the deep check,
the slow path sends the flow entry update instructions to the fast
path and re-injects the first packet into the fast path for forwarding.
Then, all subsequent target packets will be processed directly in
the fast path. In this way, when a packet is discarded in the overlay
network, VTrace can capture the true packet forwarding status and
pop a unique error code (and a detailed exception).
Second, the fast and slow paths of a VFD share the physical CPU
of the server it resides. Performing deep analysis on all packets in
the target flow in either path brings excessive burden on the CPU,
which will ineluctably affect the forwarding performance of the fast
path and ultimately the tenants’ experience. This is conflicted with
Requirement 2. Since we consider persistent packet loss problems
in this work, it is reasonable to assume that diagnosing a subset of
VTrace-interested packets is the same as inspecting all of them (e.g.,
from our experiences, usually tracing tens to hundreds of packets is
enough to solve persistent issues). Based on this assumption, only
a small number of packets need to be processed deeply in VFDs.
The "coloring" operation on a given number (i.e., packet_count) of
target packets is useful for such a flexible tracing, which is similar
to the "debug bit" in [26]. But the "debug bit" is designed to be
marked by hosts in data centers, which violates Requirement 3.
Differently, VTrace puts the "coloring" operation on a subset of
VFDs, without asking tenants’ VMs for help. For VTrace to cover all
types of virtual flows, every VFD directly connected with tenants’
VMs or the Internet, which is defined as a source VFD, is required to
35
BUFFERIngress PipelineINPUTOUTPUTFast PathSlow PathVTraceFlowsForwarding RulesForwarding lookupNAT/DNAT lookupSafety/security lookupDebug probesDeep Analysis1stVTracepacket…Forwarding rules•Match + action•Egress selection•Log generation•Match + action•Log generationPARSERMatch+ColorMatch+ActionEgress PipelineMatch+Actionfor sourceVFD onlySIGCOMM ’20, August 10–14, 2020, Virtual Event, USA
C. Fang et al.
Table 2: Metrics in the log.
Description
unique flow ID for a VTrace task
the unique mark of each packet
the outer source IP of VXLAN packet
the outer destination IP of VXLAN packet
the IP of server where the VFD runs
packet’s location in a VFD: in, out, or error
the 5-tuple after NAT operation
packet’s location in the packet trajectory
the diagnosis result of problems
the time in the server where the VFD locates