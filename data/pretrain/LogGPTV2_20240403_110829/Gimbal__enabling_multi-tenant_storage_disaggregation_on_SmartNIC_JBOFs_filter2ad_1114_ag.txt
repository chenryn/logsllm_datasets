ceedings of the 6th International Systems and Storage Conference.
[27] Lawrence S. Brakmo and Larry L. Peterson. 1995. TCP Vegas: End to end conges-
tion avoidance on a global Internet. IEEE Journal on selected Areas in communica-
tions 13, 8 (1995), 1465–1480.
Internal parallelism of flash
memory-based solid-state drives. ACM Transactions on Storage (TOS) 12, 3 (2016),
1–39.
[29] Feng Chen, David A Koufaty, and Xiaodong Zhang. 2009. Understanding intrinsic
characteristics and system implications of flash memory based solid state drives.
ACM SIGMETRICS Performance Evaluation Review 37, 1 (2009), 181–192.
[30] Eric Chung, Andreas Nowatzyk, Tom Rodeheffer, Chuck Thacker, and Fang Yu.
2014. An3: A low-cost, circuit-switched datacenter network. Technical Report
MSR-TR-2014-35, Microsoft Research (2014).
[31] Tae-Sun Chung, Dong-Joo Park, Sangwon Park, Dong-Ho Lee, Sang-Won Lee,
and Ha-Joo Song. 2009. A survey of flash translation layer. Journal of Systems
Architecture 55, 5-6 (2009), 332–343.
[32] Brian F Cooper, Adam Silberstein, Erwin Tam, Raghu Ramakrishnan, and Russell
Sears. 2010. Benchmarking cloud serving systems with YCSB. In Proceedings of
the 1st ACM symposium on Cloud computing.
[33] A. Demers, S. Keshav, and S. Shenker. 1989. Analysis and Simulation of a Fair
Queueing Algorithm. In Proceedings of the 2015 ACM Conference on Special Interest
[28] Feng Chen, Binbing Hou, and Rubao Lee. 2016.
Group on Data Communication.
[34] Peter Desnoyers. 2012. Analytic modeling of SSD write performance. In Proceed-
ings of the 5th Annual International Systems and Storage Conference.
[35] Haggai Eran, Lior Zeno, Maroun Tork, Gabi Malka, and Mark Silberstein. 2019.
NICA: An Infrastructure for Inline Acceleration of Network Applications. In 2019
USENIX Annual Technical Conference (USENIX ATC 19).
[36] Daniel Firestone, Andrew Putnam, Sambhrama Mundkur, Derek Chiou, Alireza
Dabagh, Mike Andrewartha, Hari Angepat, Vivek Bhanu, Adrian Caulfield, Eric
Chung, Harish Kumar Chandrappa, Somesh Chaturmohta, Matt Humphrey, Jack
Lavier, Norman Lam, Fengfen Liu, Kalin Ovtcharov, Jitu Padhye, Gautham Pop-
uri, Shachar Raindel, Tejas Sapre, Mark Shaw, Gabriel Silva, Madhan Sivakumar,
Nisheeth Srivastava, Anshuman Verma, Qasim Zuhair, Deepak Bansal, Doug
Burger, Kushagra Vaid, David A. Maltz, and Albert Greenberg. 2018. Azure Accel-
erated Networking: SmartNICs in the Public Cloud. In 15th USENIX Symposium
on Networked Systems Design and Implementation (NSDI 18).
[37] Pawan Goyal, Harrick M Vin, and Haichen Chen. 1996. Start-time fair queueing:
A scheduling algorithm for integrated services packet switching networks. In
Conference proceedings on Applications, technologies, architectures, and protocols
for computer communications.
[38] Ajay Gulati, Irfan Ahmad, and Carl A. Waldspurger. 2009. PARDA: Proportional
Allocation of Resources for Distributed Storage Access. In 7th USENIX Conference
on File and Storage Technologies (FAST 09).
[39] Zvika Guz, Harry (Huan) Li, Anahita Shayesteh, and Vijay Balakrishnan. 2017.
NVMe-over-Fabrics Performance Characterization and the Path to Low-Overhead
Flash Disaggregation. In Proceedings of the 10th ACM International Systems and
Storage Conference.
[40] Sangtae Ha, Injong Rhee, and Lisong Xu. 2008. CUBIC: a new TCP-friendly
high-speed TCP variant. ACM SIGOPS operating systems review 42, 5 (2008),
64–74.
[41] Mingzhe Hao, Levent Toksoz, Nanqinqin Li, Edward Edberg Halim, Henry Hoff-
mann, and Haryadi S. Gunawi. 2020. LinnOS: Predictability on Unpredictable
Flash Storage with a Light Neural Network. In 14th USENIX Symposium on Oper-
ating Systems Design and Implementation (OSDI 20).
[42] Mohammad Hedayati, Kai Shen, Michael L Scott, and Mike Marty. 2019. Multi-
queue fair queuing. In 2019 {USENIX} Annual Technical Conference ({USENIX}
{ATC} 19). 301–314.
[43] Jaehyun Hwang, Qizhe Cai, Ao Tang, and Rachit Agarwal. 2020. TCP ≈ RDMA:
CPU-efficient Remote Storage Access with i10 . In 17th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 20).
[44] Sitaram Iyer and Peter Druschel. 2001. Anticipatory scheduling: A disk scheduling
framework to overcome deceptive idleness in synchronous I/O. In Proceedings of
the eighteenth ACM symposium on Operating systems principles.
[45] Myoungsoo Jung and Mahmut Kandemir. 2013. Revisiting widely held SSD expec-
tations and rethinking system-level implications. ACM SIGMETRICS Performance
Evaluation Review 41, 1 (2013), 203–216.
[46] Woon-Hak Kang, Sang-Won Lee, Bongki Moon, Yang-Suk Kee, and Moonwook
Oh. 2014. Durable write cache in flash memory SSD for relational and NoSQL
databases. In Proceedings of the 2014 ACM SIGMOD international conference on
Management of data. 529–540.
[47] Tae Yong Kim, Dong Hyun Kang, Dongwoo Lee, and Young Ik Eom. 2015. Improv-
ing performance by bridging the semantic gap between multi-queue SSD and
I/O virtualization framework. In 2015 31st Symposium on Mass Storage Systems
and Technologies (MSST).
[48] Ana Klimovic, Christos Kozyrakis, Eno Thereska, Binu John, and Sanjeev Kumar.
2016. Flash Storage Disaggregation. In Proceedings of the Eleventh European
Conference on Computer Systems.
[49] Ana Klimovic, Heiner Litz, and Christos Kozyrakis. 2017. ReFlex: Remote Flash
≈ Local Flash. In Proceedings of the Twenty-Second International Conference on
Architectural Support for Programming Languages and Operating Systems.
[50] Gautam Kumar, Nandita Dukkipati, Keon Jang, Hassan M. G. Wassel, Xian
Wu, Behnam Montazeri, Yaogong Wang, Kevin Springborn, Christopher Alfeld,
Michael Ryan, David Wetherall, and Amin Vahdat. 2020. Swift: Delay is Sim-
ple and Effective for Congestion Control in the Datacenter. In Proceedings of
the Annual Conference of the ACM Special Interest Group on Data Communica-
tion on the Applications, Technologies, Architectures, and Protocols for Computer
Communication.
[51] HT Kung and Alan Chapman. 1993. The FCVC (flow-controlled virtual channels)
proposal for ATM networks: A summary. In 1993 International Conference on
Network Protocols. IEEE, 116–127.
[52] H. T. Kung, Trevor Blackwell, and Alan Chapman. 1994. Credit-Based Flow
Control for ATM Networks:Credit Update Protocol, Adaptive Credit Allocation,
and Statistical Multiplexing. In Proceedings of the Conference on Communications
Architectures, Protocols and Applications.
[53] Sergey Legtchenko, Hugh Williams, Kaveh Razavi, Austin Donnelly, Richard
Black, Andrew Douglas, Nathanaël Cheriere, Daniel Fryer, Kai Mast, An-
gela Demke Brown, Ana Klimovic, Andy Slowey, and Antony Rowstron. 2017. Un-
derstanding Rack-Scale Disaggregated Storage. In Proceedings of the 9th USENIX
Conference on Hot Topics in Storage and File Systems.
118
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Min and Liu, et al.
[76] Eno Thereska, Hitesh Ballani, Greg O’Shea, Thomas Karagiannis, Antony Row-
stron, Tom Talpey, Richard Black, and Timothy Zhu. 2013. Ioflow: A software-
defined storage architecture. In Proceedings of the Twenty-Fourth ACM Symposium
on Operating Systems Principles.
[77] Matthew Wachs, Michael Abd-El-Malek, Eno Thereska, and Gregory R Ganger.
2007. Argon: Performance Insulation for Shared Storage Servers.. In FAST.
[78] Qiumin Xu, Huzefa Siyamwala, Mrinmoy Ghosh, Manu Awasthi, Tameesh Suri,
Zvika Guz, Anahita Shayesteh, and Vijay Balakrishnan. 2015. Performance
characterization of hyperscale applicationson on nvme ssds. In Proceedings of the
2015 ACM SIGMETRICS International Conference on Measurement and Modeling
of Computer Systems.
[79] Jinfeng Yang, Bingzhe Li, and David J Lilja. 2020. Exploring Performance Char-
acteristics of the Optane 3D Xpoint Storage Technology. ACM Transactions on
Modeling and Performance Evaluation of Computing Systems (TOMPECS) 5, 1
(2020), 1–28.
[80] Jinfeng Yang, Bingzhe Li, and David J Lilja. 2021. HeuristicDB: a hybrid storage
database system using a non-volatile memory block device. In Proceedings of the
14th ACM International Conference on Systems and Storage. 1–12.
[81] Yiying Zhang, Leo Prasath Arulraj, Andrea C Arpaci-Dusseau, and Remzi H
Arpaci-Dusseau. 2012. De-indirection for flash-based SSDs with nameless writes..
In FAST.
[54] Bojie Li, Zhenyuan Ruan, Wencong Xiao, Yuanwei Lu, Yongqiang Xiong, Andrew
Putnam, Enhong Chen, and Lintao Zhang. 2017. KV-Direct: High-Performance
In-Memory Key-Value Store with Programmable NIC. In Proceedings of the 26th
Symposium on Operating Systems Principles.
[55] Huaicheng Li, Mingzhe Hao, Stanko Novakovic, Vaibhav Gogte, Sriram Govindan,
Dan R. K. Ports, Irene Zhang, Ricardo Bianchini, Haryadi S. Gunawi, and Anirudh
Badam. 2020. LeapIO: Efficient and Portable Virtual NVMe Storage on ARM
SoCs. In Proceedings of the Twenty-Fifth International Conference on Architectural
Support for Programming Languages and Operating Systems.
[56] Jiaxin Lin, Kiran Patel, Brent E. Stephens, Anirudh Sivaraman, and Aditya Akella.
2020. PANIC: A High-Performance Programmable NIC for Multi-tenant Networks.
In 14th USENIX Symposium on Operating Systems Design and Implementation
(OSDI 20).
[57] Chun-Yi Liu, Yunju Lee, Myoungsoo Jung, Mahmut Taylan Kandemir, and Wonil
Choi. 2021. Prolonging 3D NAND SSD lifetime via read latency relaxation. In
Proceedings of the 26th ACM International Conference on Architectural Support for
Programming Languages and Operating Systems. 730–742.
[58] Ming Liu, Tianyi Cui, Henry Schuh, Arvind Krishnamurthy, Simon Peter, and
Karan Gupta. 2019. Offloading Distributed Applications onto SmartNICs Using
IPipe. In Proceedings of the ACM Special Interest Group on Data Communication.
[59] Ming Liu, Simon Peter, Arvind Krishnamurthy, and Phitchaya Mangpo
Phothilimthana. 2019.
E3: Energy-Efficient Microservices on SmartNIC-
Accelerated Servers. In 2019 USENIX Annual Technical Conference (USENIX ATC
19).
[60] Hui Lu, Brendan Saltaformaggio, Ramana Kompella, and Dongyan Xu. 2015.
vFair: latency-aware fair storage scheduling via per-IO cost-based differentiation.
In Proceedings of the Sixth ACM Symposium on Cloud Computing.
[61] Justin Meza, Qiang Wu, Sanjev Kumar, and Onur Mutlu. 2015. A Large-Scale
Study of Flash Memory Failures in the Field. In Proceedings of the 2015 ACM
SIGMETRICS International Conference on Measurement and Modeling of Computer
Systems.
[62] Radhika Mittal, Vinh The Lam, Nandita Dukkipati, Emily Blem, Hassan Wassel,
Monia Ghobadi, Amin Vahdat, Yaogong Wang, David Wetherall, and David Zats.
2015. TIMELY: RTT-Based Congestion Control for the Datacenter. In Proceedings
of the 2015 ACM Conference on Special Interest Group on Data Communication.
[63] Jeonghoon Mo, Richard J La, Venkat Anantharam, and Jean Walrand. 1999. Anal-
ysis and comparison of TCP Reno and Vegas. In IEEE INFOCOM’99. Conference
on Computer Communications. Proceedings. Eighteenth Annual Joint Conference
of the IEEE Computer and Communications Societies. The Future is Now (Cat. No.
99CH36320), Vol. 3. 1556–1563.
[64] Mihir Nanavati, Jake Wires, and Andrew Warfield. 2017. Decibel: Isolation and
Sharing in Disaggregated Rack-Scale Storage. In 14th USENIX Symposium on
Networked Systems Design and Implementation (NSDI 17).
[65] Stan Park and Kai Shen. 2012. FIOS: a fair, efficient flash I/O scheduler.. In FAST.
[66] Ahmed Saeed, Nandita Dukkipati, Vytautas Valancius, Vinh The Lam, Carlo
Contavalli, and Amin Vahdat. 2017. Carousel: Scalable traffic shaping at end
hosts. In Proceedings of the Conference of the ACM Special Interest Group on Data
Communication.
[67] Ahmed Saeed, Yimeng Zhao, Nandita Dukkipati, Ellen Zegura, Mostafa Ammar,
Khaled Harras, and Amin Vahdat. 2019. Eiffel: Efficient and Flexible Software
Packet Scheduling. In 16th USENIX Symposium on Networked Systems Design and
Implementation (NSDI 19).
[68] Mark R Schibilla and Randy J Reiter. 2012. Garbage collection for solid state disks.
US Patent 8,166,233.
[69] Bianca Schroeder, Raghav Lagisetty, and Arif Merchant. 2016. Flash Reliability
in Production: The Expected and the Unexpected. In 14th USENIX Conference on
File and Storage Technologies (FAST 16).
[70] Kai Shen and Stan Park. 2013. FlashFQ: A fair queueing I/O scheduler for flash-
based SSDs. In 2013 {USENIX} Annual Technical Conference ({USENIX} {ATC}
13). 67–78.
[71] Vishal Shrivastav. 2019. Fast, scalable, and programmable packet scheduler in
hardware. In Proceedings of the ACM Special Interest Group on Data Communica-
tion.
[72] Vishal Shrivastav, Asaf Valadarsky, Hitesh Ballani, Paolo Costa, Ki Suh Lee, Han
Wang, Rachit Agarwal, and Hakim Weatherspoon. 2019. Shoal: A Network
Architecture for Disaggregated Racks. In 16th USENIX Symposium on Networked
Systems Design and Implementation (NSDI 19).
[73] David Shue and Michael J Freedman. 2014. From application requests to Virtual
IOPs: Provisioned key-value storage with Libra. In Proceedings of the Ninth
European Conference on Computer Systems.
[74] Anirudh Sivaraman, Suvinay Subramanian, Mohammad Alizadeh, Sharad Chole,
Shang-Tse Chuang, Anurag Agrawal, Hari Balakrishnan, Tom Edsall, Sachin
Katti, and Nick McKeown. 2016. Programmable packet scheduling at line rate. In
Proceedings of the 2016 ACM SIGCOMM Conference.
[75] Arash Tavakkol, Juan Gómez-Luna, Mohammad Sadrosadati, Saugata Ghose,
and Onur Mutlu. 2018. MQSim: A Framework for Enabling Realistic Studies of
Modern Multi-Queue SSD Devices. In 16th USENIX Conference on File and Storage
Technologies (FAST 18).
119
Gimbal: Enabling Multi-tenant Storage Disaggregation on SmartNIC JBOFs
SIGCOMM ’21, August 23–28, 2021, Virtual Event, USA
Appendices are supporting material that has not been peer-
reviewed.
Appendix A Additional SSD characteristics
Figure 14 demonstrates the performance impact of write amplifica-
tion by comparing two conditions: SSDs pre-conditioned with large
sequential (clean) and random IO (fragmented) patterns. Adding
5% writes reduces the overall IOPS by 42.6% in the fragmented case.
Compared to the clean case, the fragmented one achieves 16.9%
and 17.8% of the throughput of write-only and 90/10 W/R scenarios,
respectively.
Figure 15 compares the unloaded latency in a clean SSD v.s. three
other scenarios for different IO sizes. On average, across these cases,
fragmented SSD, 70/30 read/write mix, and 8 concurrent IOs cause
52.0%, 83.6%, and 80.5% latency increase, respectively. Larger IOs
(e.g., 128/256KB) observe more degradation as they are more likely
to contend with other requests.
Appendix B Dynamics of Congestion Control
In this section, we provide experimental results that illustrate the
following aspects of SSD behavior: (a) delay as a function of SSD
load, (b) dynamic latency threshold determined by the congestion
control algorithm, and (c) bandwidth capability estimated by the
congestion control algorithm.
As shown in Figure 17, the observed latency increases dramat-
ically when the load exceeds the throughput capacity. With the
congestion control algorithm, the SSD maintains an average de-
lay in a stable range, providing throughputs close to the device
maximum. We plot the typical behavior of the congestion control
algorithm as it adapts the delay threshold based on observed laten-
cies. Figure 18 shows the latency threshold according to the EWMA
latency. As the number of outstanding IO increases, the EWMA
latency begins to exceed the threshold, and hit the threshold more
frequently as expected.
Appendix C Additional Details of the Gimbal Switch
virt_slot = tenant.curr_virt_slot
io.tenant = tenant
io.virt_slot = virt_slot
virt_slot.submits += 1
virt_slot.size += io.weighted_size
if virt_slot.size > 128KB then
Algorithm 2 Gimbal IO scheduler
1: procedure Sched_Submit
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12: procedure Sched_Complete
13:
14:
15:
16:
17:
18:
19:
move_to_active(tenant)
move_to_deferred(tenant)
virt_slot.is_full = true
close(virt_slot)
if open_virt_slot(tenant) == fail then
tenant = io.tenant
v_slot = io.virt_slot
v_slot.completions += 1
if v_slot.is_full AND v_slot.submits == v_slot.completions then
reset(v_slot)
if tenant.deferred AND open_slot(tenant) == success then
We now describe some more implementation details of the soft-
ware storage switch. We describe how to manage virtual slots, how
to handle heterogeneity in request bundle sizes caused by the write
tax and how the system maintains lists of tenants with requests.
We then provide the pseudocode of the scheduling algorithm.
Active and deferred list. Our scheduler calls a function on
every IO submission and is also triggered as a callback function
when there is an IO completion event. The whole logic is described
in Algorithm 2. Generally, the scheduler applies the DRR service
discipline for inter-tenant fair storage resource allocation. For each
slot in use, the scheduler tracks if its outstanding I/Os are completed
and free the slot when the operations are done. When the chosen
tenant is in the deferred list then the callback function tries to open
new virtual slot. If it succeeds, then the tenant is removed from the
deferred list, added to the active list. For each tenant in the active
list, the scheduler will accumulate the deficit value, and move to the
deferred list when there are no more available virtual slots. Here,