# 10+倍性能提升全过程--优酷账号绑定淘宝账号的TPS从500到5400的优化历程
## 背景说明
> 2016年的双11在淘宝上买买买的时候，天猫和优酷土豆一起做了联合促销，在天猫双11当天购物满XXX元就赠送优酷会员，这个过程需要用户在优酷侧绑定淘宝账号(登录优酷、提供淘宝账号，优酷调用淘宝API实现两个账号绑定）和赠送会员并让会员权益生效(看收费影片、免广告等等）
> 
> 这里涉及到优酷的两个部门：Passport(在上海，负责登录、绑定账号，下文中的优化过程主要是Passport部分）；会员(在北京，负责赠送会员，保证权益生效）
> 在双11活动之前，Passport的绑定账号功能一直在运行，只是没有碰到过大促销带来的挑战
---
整个过程分为两大块：
1.  整个系统级别，包括网络和依赖服务的性能等，多从整个系统视角分析问题；
1.  但服务器内部的优化过程，将CPU从si/sy围赶us，然后在us从代码级别一举全歼。
系统级别都是最容易被忽视但是成效最明显的，代码层面都是很细致的力气活。
整个过程都是在对业务和架构不是非常了解的情况下做出的。
## 会员部分的架构改造
-   接入中间件DRDS，让优酷的数据库支持拆分，分解MySQL压力
-   接入中间件vipserver来支持负载均衡
-   接入集团DRC来保障数据的高可用
-   对业务进行改造支持Amazon的全链路压测
## 主要的压测过程
![screenshot.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/10+倍性能提升全过程/6b24a854d91aba4d-6b24a854d91aba4dcdbd4f0155683d93.png)
**上图是压测过程中主要的阶段中问题和改进,主要的问题和优化过程如下：**
```
- docker bridge网络性能问题和网络中断si不均衡    (优化后：500->1000TPS)
- 短连接导致的local port不够                   (优化后：1000-3000TPS)
- 生产环境snat单核导致的网络延时增大             (优化后生产环境能达到测试环境的3000TPS)
- Spring MVC Path带来的过高的CPU消耗           (优化后：3000->4200TPS)
- 其他业务代码的优化(比如异常、agent等)          (优化后：4200->5400TPS)
```
**优化过程中碰到的比如淘宝api调用次数限流等一些业务原因就不列出来了**
---
## 概述
由于用户进来后先要登录并且绑定账号，实际压力先到Passport部分，在这个过程中最开始单机TPS只能到500，经过N轮优化后基本能达到5400 TPS，下面主要是阐述这个优化过程
## Passport部分的压力
### Passport 核心服务分两个：
-   Login              主要处理登录请求
-   userservice    处理登录后的业务逻辑，比如将优酷账号和淘宝账号绑定
为了更好地利用资源每台物理加上部署三个docker 容器，跑在不同的端口上(8081、8082、8083），通过bridge网络来互相通讯
### Passport机器大致结构
![screenshot.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/10+倍性能提升全过程/b509b30218dd22e0-b509b30218dd22e03149985cf5e15f8e.png)
### userservice服务网络相关的各种问题
---
#### 太多SocketConnect异常(如上图）
在userservice机器上通过netstat也能看到大量的SYN_SENT状态，如下图：
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/10+倍性能提升全过程/99bf952b880f1724-99bf952b880f17243953da790ff0e710.png)
#### 因为docker bridge通过nat来实现，尝试去掉docker，让tomcat直接跑在物理机上
这时SocketConnect异常不再出现
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/10+倍性能提升全过程/6ed62fd6b50ad278-6ed62fd6b50ad2785e5b57687d95ad6e.png)
#### 从新梳理一下网络流程
docker(bridge)----短连接--->访问淘宝API(淘宝open api只能短连接访问），性能差，cpu都花在si上；
如果 docker(bridge)----长连接到宿主机的某个代理上(比如haproxy）-----短连接--->访问淘宝API， 性能就能好一点。问题可能是短连接放大了Docker bridge网络的性能损耗
#### 当时看到的cpu si非常高，截图如下：
![image.png](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/10+倍性能提升全过程/4c1eff0f925f5997-4c1eff0f925f59977e2557acff5cf03b.png)
去掉Docker后，性能有所提升，继续通过perf top看到内核态寻找可用的Local Port消耗了比较多的CPU，gif动态截图如下(可以点击看高清大图）：
![perf-top-netLocalPort-issue.gif](https://cdn.jsdelivr.net/gh/shareImage/image@_md2zhihu_programmer_case_81efef37/10+倍性能提升全过程/fff502ca73e3112e-fff502ca73e3112e585560ffe4a4dbf1.gif)
**注意图中ipv6_rcv_saddr_equal和inet_csk_get_port 总共占了30%的CPU** (系统态的CPU使用率高意味着共享资源有竞争或者I/O设备之间有大量的交互。)
**一般来说一台机器默认配置的可用 Local Port 3万多个，如果是短连接的话，一个连接释放后默认需要60秒回收，30000/60 =500 这是大概的理论TPS值【这里只考虑连同一个server IP:port 的时候】**