in
batches. See Figure 12.
P2
𝐷2 =
[[𝜃]]P2
Figure 11: Setting for two-party logistic regression. Each
party holds a set of labeled documents, and the output is a
model that is secret-shared between the two parties.
6.1 Similarity Computation and 𝑘-Nearest
Neighbors
In secure 𝑘-NN [35], we have a database 𝐷 of labeled documents
distributed among one or multiple servers, where each 𝑑 ∈ 𝐷 is
represented as a feature vector using TF-IDF encoding. A client
wants to classify a document 𝑑 against 𝐷. The 𝑘-NN classification
algorithm, which is parameterized by a constant 𝑘, achieves that
goal by (a) computing sim(𝑑, 𝑝), for each 𝑝 ∈ 𝐷, and (b) assigning a
class 𝑐𝑑 to 𝑑 as the result of a majority vote among the classes of the
𝑘 most similar documents according to the similarities computed
in step (a). See Figure 10 for a schematic depiction of this setting.
In [35], the authors observe that for the commonly used cosine
similarity metric, secure 𝑘-NN reduces to one two-party sparse
matrix-vector multiplication between the client and each of the
data holders, followed by a generic MPC phase for the majority
vote. Their construction for similarity computation is equivalent
to the protocol in Figure 9 instantiated with Circuit-ROOM, which
we use as a baseline in our experimental evaluation (Section 8.1).
Note that this approach already exploits sparsity in the data.
However, our modular framework also allows us to use differ-
ent ROOM primitives to achieve better efficiency in the online
phase, while matching the speedups of [35] in the offline phase (cf.
Section 2). In Section 8.1, we show that our Poly-ROOM indeed
improves the online phase by up to 5x.
6.2 Logistic Regression Training
A drawback of non-parametric approaches like 𝑘-NN is that each
query depends on the entire training database. To circumvent
this, parametric approaches such as logistic regression first train a
smaller model 𝜃, which is then used to answer classification queries
faster.
Here, we assume a two-party setting where parties P1 and P2
hold a horizontally partitioned database, i.e., each party holds a
sparse dataset, where P𝑖 holds Xi ∈ R𝑛𝑖×𝑑, yi ∈ R𝑛𝑖 , with Xi being
the set of 𝑛𝑖 records with 𝑑 features and y𝑖 being the vector of
corresponding binary target labels. This corresponds to a training
dataset of size 𝑛 × 𝑑, 𝑛 = 𝑛1 + 𝑛2 distributed among P1 and P2,
and the goal is to build a shared model 𝜃 that is able to accurately
predict a target value for an unlabeled record, while keeping the
local training datasets private (cf. Figure 11).
A widely used algorithm for building this kind of model is mini-
batched stochastic gradient descent (SGD). Here, the empirical loss of
the model 𝜃 is minimized by iteratively refining it with an update
10
P1: 𝐷1 = (X1, y1), X1 ∈ R𝑛×𝑑, y1 ∈ {0, 1}𝑛,
P2: 𝐷2 = (X2, y2), X2 ∈ R𝑛×𝑑, y2 ∈ {0, 1}𝑛.
Output: Shared model [[𝜃]].
Protocol:
Parties: P1, P2.
Inputs:
for 𝑗 ∈ [2] do
1: 𝜃0 = (0)𝑖∈[𝑑]
2: for 𝑇 epochs do
for 𝑖 ∈ ⌊ 𝑛
𝑏 ⌋ do
3:
4:
Bj ← Xj[𝑖..𝑖+𝑏]
5:
[[uj]] ← MvMult(Bj, [[𝜃]])
6:
[[vj]] ← Sigmoid([[uj]])
7:
[[wj]] ← [[vj]] − yj[𝑖..𝑖+𝑏]
8:
[[gj]] ← MvMult(Bj⊤, [[wj]])
9:
10:
11:
12:
13:
14: end for
end for
[[g]] ← 1
[[𝜃]] ← [[𝜃]] − 𝜂[[g]]
2𝑏 ([[g1]] + [[g2]])
end for
Figure 12: Secure two-party gradient descent on sparse dis-
tributed training data.
rule of the form 𝜃𝑖+1 = 𝜃𝑖 − 𝜂g, for a step size 𝜂 and a gradient
quantity g. The training dataset is partitioned in so-called mini-
batches, each of which is used to compute a model update: In a
forward pass, the prediction loss of the current batch is computed,
and the gradient g of that loss is obtained as the result of a backward
pass.
Figure 12 shows a secure protocol for two-party SGD training.
It relies on a secure matrix multiplication protocol MvMult, and
an approximation of the logistic function Sigmoid(x) = 1/(1 +
𝑒x) introduced by Mohassel and Zhang [29], implemented with a
garbled circuit. The security of the protocol follows from the fact
that 𝜃 is always kept secret shared, and secure implementations of
the two sub-protocols above.
We now discuss how the calls to MvMult in lines 6 and 9 of the
protocol are instantiated with our protocols from Section 5.2. First
note that, as the Xj’s are sparse, so will be their mini-batches Bj
contributed by P1 or P2 in line 5. In fact, as common mini-batch
sizes are as small as 64 or at most 128, the mini-batches will be
sparse in their columns. We show that this is the case in the context
of concrete real-world datasets in Section 8. Hence, the call to
MvMult in line 6 involves a column-sparse matrix and a dense
vector, and thus we choose our protocol from Figure 8 instantiated
with Basic-ROOM. The choice of Basic-ROOM is justified by the fact
that the keys of [[𝜃]]P2 span the whole key domain K = [𝑑], as it is
a secret share, and hence Basic-ROOM does not incur unneccesary
overhead in this case. On the other hand, the computation of gj in
line 9 is a multiplication between a row sparse matrix and a dense
vector, for which we use our protocol from Section 5.2.2.
In Section 8.2, we compare the runtimes of our sparse implemen-
tation to those reported in [29]. With the exception of the smallest
dataset, we improve computation time by a factor of 2x–11x (LAN)
and 12x–94x (WAN), and communication by 26x–215x (LAN) and
4x–10x (WAN).
7 IMPLEMENTATION OF OUR FRAMEWORK
For our implementation, we follow the general architecture pre-
sented in Figure 1. For each layer of abstraction, we define generic
interfaces that are then matched by our concrete implementations.
This allows, for example, to use the same matrix multiplication
function for different ROOM instantiations, which in turn simpli-
fies development and makes sure our framework can be extended
seamlessly.
Most of our library is written as generic C++ templates that
abstract away from concrete integer, vector and matrix types. This
allows us to use Eigen’s expression templates [18], and thus avoid
unnecessary local matrix operations. For generic two-party com-
putation based on garbled circuits, we use Obliv-C [39]. As a PRF,
we use the AES-128 implementation in Obliv-C by Doerner [12].
The fast polynomial interpolation and evaluation that we need for
Poly-ROOM and ScatterInit is done using Yanai’s FastPolynomial
library [37]. The code for our framework is publicly available for
download.1
8 EXPERIMENTAL EVALUATION
Given the large number of parameters and tradeoffs that our frame-
work exhibits, a complete layer-by-layer evaluation of all compo-
nents from Figure 1 with all ranges of useful parameters is both
infeasible and not very useful. Instead, we chose to run experi-
ments on only two abstraction layers: ROOM micro-benchmarks,
which allow to compare our constructions with each other and with
future improvements, and entire applications, which allow us to
compare against previous work on application-specific protocols.
We present our micro-benchmarks in Appendix C.2, and focus on
our applications for the remainder of this section.
We implement each of the applications presented in Section 6 in
our framework. All of our applications are implemented end-to-end,
meaning they take the sparse feature vectors as inputs and return
the desired class or shared regression model as output. For Naive
Bayes, we refer to Appendix B.1.1.
We analyze three real-world datasets that represent common
classification tasks: sentiment analysis [27], topic identification [34],
and language detection [36]. Table 2 summarizes the properties of
each of the datasets, including the average number of features of
single documents. We also report, for reference, the classification
accuracies that can be achieved using the different methods out-
lined in Section 6: logistic regression, naive Bayes, and 𝑘-nearest
neighbors. These were obtained in the clear using out-of-the-box
Scikit-Learn [32] model fitting, without any sophisticated hyper-
parameter tuning.
For the Movie Reviews and 20Newsgroups datasets, features
correspond to words, using a TF-IDF representation. We assume a
public vocabulary of 150000 words for the first two datasets (Movie
reviews and 20newsgroups). This number has been used in previous
1https://github.com/schoppmp/room-framework
11
Figure 13: Running times of the matrix-vector multiplica-
tion needed for a single 𝑘-NN classification.
work [35] and corresponds to the size of a large dictionary of English
words. For the language classification task, 𝑛-grams of 𝑛 consecutive
characters are used instead. We assume the set of characters is
public.
All our experiments are performed on Azure DS14 v2 instances
with 110 GB of memory each, using a single core. We note that
the memory bottleneck in our experiments is the "dense" case that
we use as a baseline, not our ROOM-based implementations. For
LAN experiments, we use instances in the same region, while for
WAN experiments, we place one in the US and one in Europe. The
measured roundtrip time was 0.7ms in the LAN setting, and 85ms
in the WAN setting. The average data transfer rates were 2.73Gbit/s
and 245Mbit/s, respectively. As in previous work [29, 35], 64-bit
integers are used for K and V. Garbled circuits are run with 80-bit
security, due to Obliv-C. For Poly-ROOM, we use 𝑠 = 40 bits of
statistical security.
8.1 𝑘-Nearest Neighbors
For 𝑘-NN, the efficiency bottleneck is the computation of scores of
the query document with respect to each training sample, which
reduces to a secure matrix-vector multiplication where the matrix
is sparse in its columns and the vector is sparse. Thus, we can
implement this protocol using Figure 9, instantiated with any of
our ROOMs.
As observed in Section 6.1, Schoppmann et al. [35] solve this
problem using an approach similar to Circuit-ROOM, which is
why we use it as the baseline here. In most of our experiments
(Figure 13), their approach turns out to be faster in than a simple
dense multiplication.
Our new constructions using Basic-ROOM and Poly-ROOM
achieve a similar improvement over the dense case (up to 82x)
when it comes to total time, and at the same time a 2–5x improved
online time compared to [35]. Note that the online time includes
top-𝑘 selection (implemented using generic MPC) and multiplica-
tion of the reduced matrices, both of which are the same for [35]
and us. If we focus on the reduction time alone, our new approaches
are up to 40x faster.
MoviesNewsgroupsLanguages,ngrams=1Languages,ngrams=2Dataset1s2s5s10s30s1m2m5m10m30m1h2h5h10h1d2dRunningTimek-NN(LAN)DenseBasic-ROOMCircuit-ROOMPoly-ROOMO(cid:128)linetimeOnlinetimeDataset
Documents
Classes
Nonzero Features
Accuracy
34341
9051
783
783
Movies [27]
Newsgroups [34]
Languages [36], ngrams=1
Languages [36], ngrams=2
Table 2: Real-world datasets used in the experiments. These comprise a variety of classification tasks such as sentiment anal-
ysis of movie reviews (Movies), topic identification (Newsgroups), and language identification (Languages). For the latter, we
also investigate the effect of analyzing larger n-grams instead of single characters.
(*) 𝑘-NN was trained on a subsample of 10𝐾 examples due to memory limitations.
2
20
11
11
Log. Regression
0.88
0.73
0.96
0.99
Naive Bayes
0.85
0.76
0.87
0.99
𝑘-NN
(*) 0.74
0.57
0.96
0.99
Single (avg.)
136
98