the requests are sent to the server. On the other hand, the optimal
allocation in Figure 7(b) allocates two slots to object 1, minimizing
the load on the server.
Performance guarantee. Since servers have plenty of memory
to queue requests, servers are CPU-bounded, and the bottleneck is
on the number of requests that can be processed by a server per
second. Let the workload be W = {(ri , ci )}, and the solution to the
optimization problem be S = {(si )}. Let rs and re be the request
rates that can be supported by a switch and a server, respectively.
We assume that the switch is not the bottleneck, i.e., rs ≥ i ri ,
so the switch is always able to support the request rate i risi /ci .
This assumption is reasonable, because if rs < i ri , then the ToR
switch is congested. In such a case, not all lock requests can even
be received by the database rack in the first place, and the workload
would not be meaningful. Since the switch can process the request
rate i risi /ci , it requires ⌈(i ri −i risi /ci )/re ⌉ servers to serve
the remaining request rate. In other words, with one switch and
⌈(i ri − i risi /ci )/re ⌉ servers, NetLock guarantees to support
the workload W = {(ri , ci )}.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Zhuolong Yu, Yiwen Zhang, Vladimir Braverman, Mosharaf Chowdhury, and Xin Jin
Handling overflowed requests. It is possible that the queues in
the switch can be overflowed, because the switch cannot allocate
enough memory for the last object it handles or the estimation
of maximum contention for an object is inaccurate. For lock i,
we denote its switch queue as q1[i], and its server queue as q2[i].
When q1[i] is full, the switch forwards the overflowed requests to
the server. The overflowed requests are only buffered in q2[i] in
the server, not processed. Note that this is different from the locks
that are not allocated to the switch and only have queues in the
serversÐthe requests of those locks are both buffered and processed
by the servers. The switch puts a mark on the packets to distinguish
between these two cases.
As both q1[i] and q2[i] may contain requests, we need to ensure
that the requests are processed as they would in a single queue. To
achieve this, the requests are only granted and dequeued by q1[i]
in the switch, and new requests are only enqueued at q2[i] in the
server. When q1[i] becomes empty, the switch sends a notification
to the server, and the server pushes some requests from q2[i] to q1[i].
The number of requests that can be pushed is no bigger than the
number of available slots in q1[i] to ensure q1[i] is not overflowed.
When q2[i] becomes empty and q1[i] is not full, NetLock enters the
normal mode, i.e., new requests can directly be enqueued at q1[i]
in the switch. Because q2[i] is empty, enqueueing at q1[i] would
ensure the same order as a single queue.
Moving locks between the switch and lock servers. When the
popularity of a lock changes, the lock will be moved from the
switch to a lock server or from its lock server to the switch. When
moving a lock, NetLock pauses enqueuing new requests of this lock
and waits until the queue is empty to ensure consistency. Memory
fragmentation caused by moving locks between the switch and lock
servers would reduce the memory that can be actually used to store
lock requests. The memory layout on the switch is periodically
reorganized to alleviate memory fragmentation.
4.4 Policy Support
NetLock is a centralized lock manager that can support and enforce
policies. We consider the following three representative policies.
Starvation-freedom. Decentralized lock managers use partial in-
formation to grant locks, which can easily lead to lock starvation.
Lock starvation happens when the lock manager allows later lock
requests to acquire a lock before earlier lock requests, making some
requests wait indefinitely to get the lock. Lock starvation is typically
avoided by using a first-come-first-serve (FCFS) policy. The FCFS
policy stores lock requests in a first-in-first-out (FIFO) queue, and
always grants locks to the head of the queue. This policy is natively
supported by the circular queue we design for the switch data plane.
With this, NetLock supports request (lock) level starvation-freedom.
Note that, there can still be starvation if some transactions do not
complete because of deadlock, which is discussed in Section 4.5.
Service differentiation with priorities. It is challenging to sup-
port priority-based policies in the switch, as a register array can
only be accessed once when processing a packet and a priority
queue cannot be directly implemented with a register array. We
leverage the multi-stage structure of the switch data plane to sup-
port priorities. Specifically, we allocate one queue in each stage
for one priority. Since the packet is processed stage by stage, the
high-priority requests in earlier stages are granted first. The re-
quest processing with priorities in the switch data plane follows
Algorithm 2 with some tweaks. For a lock request with i-th prior-
ity, it is directly granted if all queues are empty, or if there is no
exclusive lock request holding the lock or queued in the same or
higher priority queues and the request itself is also for a shared
lock. After the lock is released, NetLock will first grant the lock to
the queue with the highest priority. Note that a priority can have a
large queue spanning multiple stages to expand its queue size. The
limitation of this solution is that the number of priorities is limited
to the number of stages, which is usually 10-20 in today’s switches.
This limitation can be alleviated by approximation, e.g., grouping
multiple fine-grained priorities into a single coarse-grained priority.
Moreover, only high-priority requests need to be processed in the
switch. Low-priority requests do not need fast processing, and can
always be offloaded to the lock servers.
Performance isolation with per-tenant quota. Cloud databases
often have multiple tenants and need to enforce fairness between
them. Without a centralized lock manager, a tenant can generate
requests and acquire locks at a faster rate than another tenant, and
thus occupies most of the resources. While an FCFS policy can avoid
starvation of the slower tenant, it cannot enforce the tenants to stay
within their shares. It requires the lock manager to use rate limiters
to enforce per-tenant quota. Rate limiters can be implemented in
the switch data plane with either meters that can automatically
throttle a tenant, or counters that count the tenants’ requests and
compare with their quotas.
4.5 Practical Issues
Switch memory size. We examine whether the switch memory
is sufficient for a lock manager from two aspects.
Think time. The think time affects the maximum turnover rate of
a memory slot. Let T be the duration of a request occupying a slot,
which includes the round trip time of sending the grant and release
messages and that of executing the transaction (i.e., think time). A
slot can be reused by 1/T times per second (i.e., the turnover rate),
providing a throughput of 1/T RPS. With S slots, the switch can
achieve S/T RPS. Given fast networks and low-latency transactions,
T can be a few tens of microseconds. As a switch has tens of MB
memory, 100K slots with 20B slot size only consume 2 MB memory,
which is a small portion of the total memory. Assuming T = 20 µs
and S = 100K, the switch can support S/T = 5 BRPS, which is
sufficient for the database servers the same rack. On the other hand,
if T = 1 ms, the switch needs 1M slots to achieve S/T = 1 BRPS.
Memory allocation. The memory allocation mechanism affects
the utilization of the switch memory. It determines whether the
switch can achieve the maximum rate S/T . If the switch memory is
allocated to unpopular locks, the switch would only process a small
portion of the total locks. Even when a memory slot is available,
it may not be used to process a new request for its lock as there
are no pending requests for this unpopular lock. If the memory
slots are empty for half of the time, then the switch needs to double
its memory slots in order to achieve the maximum rate. NetLock
uses an optimal knapsack algorithm to efficiently allocate switch
memory to popular locks to maximize the memory utilization. This
NetLock: Fast, Centralized Lock Management
Using Programmable Switches
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
handles skewed workload distributions. For uniform workload dis-
tributions, we combine multiple locks into one coarse-grained lock
to increase the memory utilization.
In summary, the think time determines the maximum turnover
rate of a memory slot and thus the maximum throughput the switch
can support with a given amount of memory, and the memory
allocation mechanism determines whether the system can achieve
the maximum turnover rate. Experimental results in Section 6.4
illustrate the relationship.
Scalability. We focus on rack-scale database systems in this paper.
Based on the above analysis on switch memory size and the experi-
mental results in Section 6.4, the memory of one switch is sufficient
for most rack-scale workloads, and the ToR switch can be naturally
used as the lock switch. In the cases where more memory is needed,
additional lock switches can be attached to the rack as specialized
accelerators for lock processing. For large-scale database systems
that span multiple racks, each rack runs an instance of NetLock to
handle the lock requests of its own rack.
Failure handling. We describe how to handle different types of
failures in NetLock.
• Transaction failure. Transaction failures can be caused by network
loss, application crashes, and client failures. When a transaction
fails without releasing its acquired locks, other transactions that
request for the same locks cannot proceed. NetLock uses a com-
mon mechanism, leasing [21], to handle transaction failures. It
stores a timestamp together with each lock, and a transaction
expires after its lease. The switch control plane periodically polls
the data plane to clear expired transactions.
• Deadlock. Deadlocks are caused by multiple transactions waiting
for locks held by others, and no transaction can make progress.
It is resolved in the same way as for transaction failures. Clients
retry when the leases expire until they succeed. In addition, dead-
locks can be avoided if priority-based policies are employed.
• NetLock failure. When a lock server fails, the locks allocated to
this server is assigned to another lock server. Clients resubmit
their requests to the new server, and the server waits for the
leases to expire before granting the locks. A switch failure is
handled in the same way by assigning the locks to a backup
switch. After the original switch restarts, the lock requests are
queued into the original switch. When releasing a lock, we only
grant locks from the backup switch until the queue in the backup
switch gets empty. After all the queues in the backup switch get
empty, the backup switch is no longer useful. When the switch
restarts, it also synchronizes its states with the lock servers and
waits for the overflowed requests that are buffered at the lock
servers to drain before the switch starts processing new requests
on the corresponding locks. The unpopular locks stored in lock
servers are not affected by switch failures.
5 IMPLEMENTATION
We have implemented a prototype of NetLock, including the lock
switch, the lock server, and the client.
The lock switch is implemented with 1704 lines of code in P4,
and is compiled to Barefoot Tofino ASIC [9]. The lock table has a
shared queue with a total of 100K slots. With 20B slot size, it only
consumes 2MB, which is a small portion of tens of MB on-chip
memory. The switch control plane is implemented with 750 lines
of code in Python, which allocates the memory in the shared queue
to different locks.
The lock server is implemented with 2807 lines of code in C. It
handles lock requests that cannot be directly processed by the lock
switch. To maximize the efficiency of multi-core processing and
improve the performance, it uses Intel DPDK [2], and leverages
Receive Side Scaling (RSS) to partition the lock requests between
cores and dispatch the lock requests to the appropriate RX queues
by the NIC for each core. With these optimizations, a lock server
can achieve up to 18 MRPS with a 40G NIC in our testbed.
The client is implemented with 3176 lines of code in C. It is used
to generate lock requests to measure the performance in the exper-
iments. It also uses Intel DPDK and RSS to optimize performance,
and one client server can generate up to 18 MRPS with a 40G NIC
in our testbed.
6 EVALUATION
6.1 Methodology
Testbed. The experiments of NetLock are conducted on our testbed
consisting of one 6.5 Tbps Barefoot Tofino switch and 12 servers.
Each server has an 8-core CPU (Intel Xeon E5-2620 @ 2.1GHz) and
one 40G NIC (Intel XL710).
Comparison. We compare NetLock with the state-of-the-art lock
manager DSLR [49] and DrTM [46]. Since DSLR and DrTM re-
quire RDMA, the experiments on DSLR and DrTM are conducted
in the Apt cluster of CloudLab [6]. The configuration is comparable
to our own testbed. Each server is equipped with an 8-core CPU
(Intel Xeon E5-2450 @ 2.1GHz) and a 56G RDMA NIC (Mellanox
ConnectX-3). We also compare NetLock with a recently proposed
switch-based solution NetChain [27]. NetChain is not a fully func-
tional lock manager, as it only supports exclusive locks. Therefore,
requests for shared locks are treated as exclusive locks. NetChain
handles concurrent requests with client-side retry. Since NetChain
only stores items in the switch, we adapt the lock granularity
based on the switch memory size and the number of locks, so that
NetChain can handle all the requests in the switch. We emphasize
that DSLR, DrTM and NetChain do not support flexible policies.
Workloads. We use two workloads. The first workload is a mi-
crobenchmark, which simply generates lock requests to a set of
locks. It is useful to measure the basic performance of lock pro-
cessing. The second workload is TPC-C [10]. It generates trans-
actions based on TPC-C, and each transaction contains a set of
lock requests. It is useful to measure the application-level perfor-
mance. We use two settings for TPC-C, which is the same as DSLR:
a low-contention setting with ten warehouses per node, and a
high-contention setting with one warehouse per node. We use
throughput, in terms of lock requests granted per second (RPS)
and transactions per second (TPS), and latency as the evaluation
metrics.
6.2 Microbenchmark
We use microbenchmark experiments to measure the basic through-
put and latency of the lock switch to process lock requests. We cover
both shared and exclusive locks.
SIGCOMM ’20, August 10–14, 2020, Virtual Event, NY, USA
Zhuolong Yu, Yiwen Zhang, Vladimir Braverman, Mosharaf Chowdhury, and Xin Jin
(a) Shared locks.
(b) Exclusive locks w/o contention.
(c) Exclusive locks w/ contention.
(d) Exclusive locks w/ contention.
Figure 8: Microbenchmark results of switch performance on handling lock requests.
Shared locks. We first evaluate the performance for shared locks.
We use all 12 servers in the testbed to generate requests to the
lock switch. Since the requests are for shared locks, there are no
contentions and the locks can be directly granted. Figure 8(a) shows
the relationship between latency and throughput. The median (av-
erage) latency is 8 µs (7.1 µs), and the 99% (99.9%) latency is 12 µs (14
µs). We emphasize that the latency is dominated by the processing
latency at the client software and NIC; the processing latency at the
switch is under 1 µs. The latency is not affected by the throughput,
because even we use all 12 servers to generate requests, they can
still not saturate the switch. The switch can handle the lock request
at line rate, and the Barefoot Tofino switch used in the experiment
is able to process more than 4 billion packets per second.
Exclusive locks. We then evaluate the performance for exclusive
locks. Similar to the previous experiment, we use 12 servers to
generate requests for exclusive locks. To measure the baseline per-
formance, the requests are sent to different locks and there are no
contentions. Figure 8(b) shows the results, which are similar to
those for shared locks. This is because in both cases, the requests
are directly granted by the switch and processed at line rate.
To show the impact of contention on exclusive locks, we let
the servers send lock requests to the same set of locks, and vary
the number of locks in the set. The level of contention decreases
as the number of locks increases. Figure 8(c) shows the impact of
contention on the throughput. Under high contention (i.e., when
the number of locks is small), the throughput is very limited. This is
because the requests for the same lock have to be processed one by
one, even though the switch still has spare capacity. The throughput
increases as the contention decreases. Under low contention, the
throughput is maximized by the speed of the 12 servers to generate
lock requests. Figure 8(d) shows the latency results. The latency is
more than 100 µs under high contention, and decreases to a few µs
under low contention.
Comparison with lock server. We also compare the performance
of a lock switch with a lock server. We use 10 servers to generate re-
quests, and the workloads are similar to the previous experiments:
shared locks, exclusive locks without contention, and exclusive
locks with contention (5000 locks). The lock server is implemented
with the same functionality and is configured with a different num-
ber of cores (1∼8) in this experiment. Figure 9 shows the throughput
of a lock switch and a lock server. The lock switch outperforms
the lock server by 7× as the lock server easily gets saturated by a
large number of requests. We emphasize that the lock switch is not
Figure 9: Comparison between a lock switch and a lock
server with various number of cores. Ten servers are used
to generate requests. The lock switch is not saturated. The
lock switch can support a few billion requests per second.
saturated by the ten clients in this experiment. The performance
gap would be even larger if there are more clients sending requests:
the switch can process a few billion requests per second and can
potentially replace hundreds of servers for the same functionality.
6.3 Benefits of NetLock
We show the benefits of NetLock on its performance improvement
and flexible policy support. The experiments use the TPC-C work-
load to show application-level performance.
Performance improvement over DSLR, DrTM and NetChain.
We show the performance improvement of NetLock over the state-
of-the-art solutions DSLR, DrTM and NetChain. We show two sce-
narios, and each is conducted under two TPC-C workload settings
(high-contention and low-contention). Figure 10 shows the through-
put and latency of the first scenario, where we use ten machines as
clients to generate requests, and two machines as lock servers that
run NetLock, DSLR or DrTM; NetChain only uses the switch, and
does not use any servers for lock processing. Because NetChain