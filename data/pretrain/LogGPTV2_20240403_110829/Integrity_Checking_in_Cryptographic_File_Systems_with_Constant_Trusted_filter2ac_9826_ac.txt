is updated ﬁrst with algorithm UpdateCtr. The counter
authentication information from trusted storage is also
updated with algorithm AuthCtr. Block B is then com-
pressed. If it has an adequate compression level, then the
compressed block is padded and encrypted, and a MAC
is concatenated at the end of the new block. Otherwise, a
new hash is appended to the Merkle tree RTreeF and an
encryption of B is stored in the (n + 1)-th block of F .
190
16th USENIX Security Symposium
USENIX Association
F.Update(i, B) :
 ← F.enc key
F.UpdateCtr(i)
F.AuthCtr()
Bc ← compress(B)
if |Bc| ≤ Lc
if i ∈ RArrF
RTreeF .DelOﬀsetTree(TSF , RArrF , i)
(F, i, pad(Bc))
C ← Elen
k1
F.WriteBlock(i, C||Hk2 (i||F.GetCtr(i)||B))
else
if i ∈ RArrF
j ← RArrF .SearchOﬀset(i)
RTreeF .UpdateTree(TSF , j, h(i||F.GetCtr(i)||B))
else
RTreeF .AppendTree(TSF , h(i||F.GetCtr(i)||B))
append i at end of RArrF
C ← Elen
(F, i, B)
k1
F.WriteBlock(i, C)
F.Append(B) :
 ← F.enc key
n ← F.blocks
F.UpdateCtr(n + 1)
F.AuthCtr()
Bc ← compress(B)
if |Bc| ≤ Lc
(F, n + 1, pad(Bc))
C ← Elen
k1
F.WriteBlock(i, C||Hk2 (n + 1||F.GetCtr(n + 1)||B))
else
F.Check(i, C):
 ← F.enc key
if F.CheckCtr() = false
return ⊥
if i ∈ RArrF
Bi ← Dlen
k1
j ← RArrF .SearchOﬀset(i)
if RTreeF .CheckTree(TSF , j,
(F, i, C)
h(i||F.GetCtr(i)||Bi)) = true
return Bi
else
return ⊥
else
i ← unpad(Dlen
k1
parse C as C 0||hval
Bc
Bi ← decompress(Bc
i )
if hval = Hk2 (i||F.GetCtr(i)||Bi)
(F, i, C 0))
return Bi
else
return ⊥
F.Delete():
n ← F.blocks
if n ∈ RArrF
RTreeF .DelOﬀsetTree(TSF , RArrF , n)
delete Bn from ﬁle F
RTreeF .AppendTree(TSF , h(n + 1||F.GetCtr(n + 1)||B))
append n + 1 at end of RArrF
C ← Elen
(F, n + 1, B)
k1
F.WriteBlock(n + 1, C)
Figure 5: The Update, Check, Append and Delete algorithms for the COMP-EINT construction.
- In the Check(i, C) algorithm for ﬁle F , the authen-
tication information from TSF for the block counters is
checked ﬁrst. There are two cases to consider. First, if
the i-th block of F is authenticated through the Merkle
tree RTreeF , as indicated by RArrF , then the block is de-
crypted and algorithm CheckTree is called. Otherwise,
the MAC of the block content is stored at the end of
the block and we can thus parse the i-th block of F as
C 0||hval. C 0 has to be decrypted, unpadded and decom-
pressed, in order to obtain the original block content Bi.
The value hval stored in the block is checked to match
the MAC of the block index i concatenated with the write
counter for block i and block content Bi.
- In the Delete algorithm for ﬁle F , the hash of the last
block has to be removed from the tree by calling the algo-
rithm DelOﬀsetTree (described in Figure 4), in the case
in which the last block is authenticated through RTreeF .
The construction COMP-EINT prevents against re-
play attacks by using write counters for either comput-
ing a MAC over the contents of blocks that can be com-
pressed enough, or a hash over the contents of blocks
that cannot be compressed enough, and authenticating
the write counters in trusted storage. It meets all the se-
curity properties of MT-EINT and RAND-EINT.
6
Implementation
Our integrity algorithms are very general and they can
be integrated into any cryptographic ﬁle system in either
the kernel or user space. For the purpose of evaluat-
ing and comparing their performance, we implemented
them in EncFS [14], an open-source user-level ﬁle sys-
tem that transparently encrypts ﬁle blocks. EncFS uses
the FUSE [12] library to provide the ﬁle system interface.
FUSE provides a simple library API for implementing
ﬁle systems and it has been integrated into recent ver-
sions of the Linux kernel.
In EncFS, ﬁles are divided into ﬁxed-size blocks
and each block is encrypted individually.
Several
ciphers such as AES and Blowﬁsh in CBC mode
are available for block encryption. We implemented
in EncFS the three constructions that provide in-
tegrity: MT-EINT, RAND-EINT and COMP-EINT.
While any length-preserving encryption scheme can be
used in the MT-EINT and COMP-EINT constructions,
RAND-EINT is constrained to use a tweakable cipher
for encrypting ﬁle blocks. We choose to encrypt ﬁle
blocks in MT-EINT and COMP-EINT with the length-
preserving stateful encryption derived from the AES ci-
pher in CBC mode (as shown in Section 4.2), and use the
USENIX Association
16th USENIX Security Symposium
191
80GB SATA 7200 RPM Maxtor.
The main challenge we faced in evaluating the pro-
posed constructions was to come up with representative
ﬁle system workloads. While the performance of the
Merkle tree construction is predictable independently of
the workload, the performance of the new integrity algo-
rithms is highly dependent on the ﬁle contents accessed,
in particular on the randomness of block contents. To
our knowledge, there are no public traces that contain ﬁle
access patterns, as well as the contents of the ﬁle blocks
read and written. Due to the privacy implications of re-
leasing actual users’ data, we expect it to be nearly im-
possible to get such traces from a widely used system.
However, we have access to three public NFS Harvard
traces [9] that contain NFS trafﬁc from several of Har-
vard’s campus servers. The traces were collected at the
level of individual NFS operations and for each read and
write operation they contain information about the ﬁle
identiﬁer, the accessed offset in the ﬁle and the size of
the request (but not the actual ﬁle contents).
To evaluate the integrity algorithms proposed in this
paper, we perform two sets of experiments. In the ﬁrst
one, we strive to demonstrate how the performance of
the new constructions varies for different ﬁle contents.
For that, we use representative ﬁles from a Linux distri-
bution installed on one of our desktop machines, together
with other ﬁles from the user’s home directory, divided
into several ﬁle types. We identify ﬁve ﬁle types of in-
terest: text, object, executables, images, and compressed
ﬁles, and build a set of ﬁles for each class of interest.
All ﬁles of a particular type are ﬁrst encrypted and the
integrity information for them is built; then they are de-
crypted and checked for integrity. We report the perfor-
mance results for the ﬁles with the majority of blocks
not random-looking (i.e., text, executable and object) and
for those with mostly random-looking blocks (i.e., image
and compressed). In this experiment, all ﬁles are written
and read sequentially, and as such the access pattern is
not a realistic one.
In the second set of experiments, we evaluate the ef-
fect of more realistic access patterns on the performance
of the integrity schemes, using the NFS Harvard traces.
As the Harvard traces do not contain information about
actual ﬁle block contents written to the disks, we gen-
erate synthetic block contents for each block write re-
quest. We deﬁne two types of block contents:
low-
entropy and high-entropy, and perform experiments as-
suming that either all blocks are low-entropy or all are
high-entropy. These extreme workloads represent the
“best” and “worst”-case for the new algorithms, respec-
tively. We also consider a “middle”-case, in which a
block is random-looking with a 50% probability, and plot
Figure 6: Prototype architecture.
CMC tweakable cipher [15] as the encryption method
in RAND-EINT.
In our integrity algorithms, we use
the SHA-1 hash function and the message-authentication
code HMAC instantiated also with the SHA-1 hash func-
tion. For compressing and decompressing blocks in
COMP-EINT we use the zlib library.
Our prototype architecture is depicted in Figure 6. We
modiﬁed the user space of EncFS to include the CMC
cipher for block encryption and the new integrity al-
gorithms. The server uses the underlying ﬁle system
(i.e., reiserfs) for the storage of the encrypted ﬁles. The
Merkle trees for integrity RTreeF and the index arrays of
the random-looking blocks RArrF are stored with the en-
crypted ﬁles in the untrusted storage space on the server.
For faster integrity checking (in particular to improve
the running time of the SearchOﬀset algorithm used in
the Update and Check algorithms of the RAND-EINT
and COMP-EINT constructions), we also keep the array
RArrF for each ﬁle, ordered by indices. The roots of the
trees RTreeF , and the arrays IntStartF and CtrValF or
their hashes (if they are too large) are stored in a trusted
storage space.
In our current implementation, we use
two extended attributes for each ﬁle F , one for the root
of RTreeF and the second for the arrays IntStartF and
CtrValF , or their hashes.
By default, EncFS caches the last block content writ-
ten to or read from the disk.
In our implementation,
we cached the last arrays RArrF , IntStartF and CtrValF
used in a block update or check operation. Since these ar-
rays are typically small (a few hundred bytes), they easily
ﬁt into memory. We also evaluate the effect of caching
of Merkle trees in our system in Section 7.1.
7 Performance Evaluation
In this section, we evaluate the performance of the new
randomness test and compression integrity constructions
for encrypted storage compared to that of Merkle trees.
We ran our experiments on a 2.8 GHz Intel D processor
machine with 1GB of RAM, running SuSE Linux 9.3
with kernel version 2.6.11. The hard disk used was an
192
16th USENIX Security Symposium
USENIX Association
the performance results of the new schemes relative to
the Merkle tree integrity algorithm for the best, middle
and worst cases.
7.1 The Impact of File Block Contents on
Integrity Performance
File sets. We consider a snapshot of the ﬁle system
from one of our desktop machines. We gathered ﬁles
that belong to ﬁve classes of interest: (1) text ﬁles are
ﬁles with extensions .txt, .tex, .c, .h, .cpp, .java, .ps, .pdf;
(2) object ﬁles are system library ﬁles from the direc-
tory /usr/local/lib; (3) executable ﬁles are system exe-
cutable ﬁles from directory /usr/local/bin; (4) image ﬁles
are JPEG ﬁles and (5) compressed ﬁles are gzipped tar
archives. Several characteristics of each set, including
the total size, the number of ﬁles in each set, the mini-
mum, average and maximum ﬁle sizes and the fraction
of ﬁle blocks that are considered random-looking by the
entropy test are given in Table 2.
Experiments. We consider three cryptographic ﬁle
systems: (1) MT-EINT with CBC-AES for encrypting
ﬁle blocks; (2) RAND-EINT with CMC encryption; (3)
COMP-EINT with CBC-AES encryption. For each cryp-
tographic ﬁle system, we ﬁrst write the ﬁles from each
set; this has the effect of automatically encrypting the
ﬁles, and running the Update algorithm of the integrity
method for each ﬁle block. Second, we read all ﬁles from
each set; this has the effect of automatically decrypting
the ﬁles, and running the Check algorithm of the integrity
method for each ﬁle block. We use ﬁle blocks of size
4KB in the experiments.
present
Micro-benchmarks. We ﬁrst
a micro-
benchmark evaluation for the text and compressed ﬁle
sets in Figure 7. We plot the total time to write and
read the set of text and compressed ﬁles, respectively.
The write time for a set of ﬁles includes the time to
encrypt all the ﬁles in the set, create new ﬁles, write the
encrypted contents in the new ﬁles and build the integrity
information for each ﬁle block with algorithms Update
and Append. The read time for a set of ﬁles includes the
time to retrieve the encrypted ﬁles from disk, decrypt
each ﬁle from the set and check the integrity of each
ﬁle block with algorithm Check. We separate the total
time incurred by the write and read experiments into
the following components: encryption/decryption time
(either AES or CMC); hashing time that includes the
computation of both SHA-1 and HMAC; randomness
check time (either the entropy test for RAND-EINT or
compression/decompression time for COMP-EINT);
Merkle tree operations (e.g., given a leaf index, ﬁnd its
index in inorder traversal or given an inorder index of
a node in the tree, ﬁnd the inorder index of its sibling
and parent); the time to update and check the root of the
tree (the root of the Merkle tree is stored as an extended
attribute for the ﬁle) and disk waiting time.
The results show that the cost of CMC encryption and
decryption is about 2.5 times higher than that of AES en-
cryption and decryption in CBC mode. Decompression
is between 4 and 6 times faster than compression and this
accounts for the good read performance of COMP-EINT.
A substantial amount of the MT-EINT overhead is due
to disk waiting time (for instance, 39% at read for text
ﬁles) and the time to update and check the root of the
Merkle tree (for instance, 30% at write for compressed
ﬁles).
In contrast, due to smaller sizes of the Merkle
trees in the RAND-EINT and COMP-EINT ﬁle systems,
the disk waiting time and the time to update and check
the root of the tree for text ﬁles are smaller. The results
suggests that caching of the hash values stored in Merkle
trees in the ﬁle system might reduce the disk waiting time
and the time to update the root of the tree and improve
the performance of all three integrity constructions, and
speciﬁcally that of the MT-EINT algorithm. We present
our results on caching next.
Caching Merkle trees. We implemented a global
cache that stores the latest hashes read from Merkle trees
used to either update or check the integrity of ﬁle blocks.
As an optimization, when we verify the integrity of a ﬁle
block, we compute all the hashes on the path from the
node up to the root of the tree until we reach a node that
is already in the cache and whose integrity has been val-
idated. We store in the cache only nodes that have been
veriﬁed and that are authentic. When a node in the cache
is written, all its ancestors on the path from the node to
the root, including the node itself, are evicted from the
cache.
We plot the total ﬁle write and read time in seconds
for the three cryptographic ﬁle systems as a function of
different cache sizes. We also plot the average integrity
bandwidth per block in a log-log scale. Finally, we plot
the cumulative size of the untrusted storage USF for all
ﬁles from each set. We show the combined graphs for
low-entropy ﬁles (text, object and executable ﬁles) in
Figure 8 and for high-entropy ﬁles (compressed and im-
age ﬁles) in Figure 9.
The results show that MT-EINT beneﬁts mostly on
reads by implementing a cache of size 1KB, while the
write time is not affected greatly by using a cache. The
improvements for MT-EINT using a cache of 1KB are
as much as 25.22% for low-entropy ﬁles and 20.34% for
high-entropy ﬁles in the read experiment. In the follow-
ing, we compare the performance of the three construc-
tions for the case in which a 1KB cache is used.
USENIX Association
16th USENIX Security Symposium
193
Total size
No. ﬁles Min. ﬁle size Max. ﬁle size
Avg. ﬁle size
Fraction of random-looking blocks
Text
Objects
Executables
Image
Compressed
245 MB
217 MB
341 MB
189 MB
249 MB
808
28
3029
641
2
27 bytes
15 bytes
24 bytes
17 bytes
80.44 MB
34.94 MB
92.66 MB
13.21 MB
2.24 MB
167.65 MB
307.11 KB
7.71 MB
112.84 KB
198.4 KB
124.05 MB
0.0351
0.0001
0.0009
0.502
0.7812
Table 2: File set characteristics.
Write micro-benchmarks for text files
Write micro-benchmarks for compressed files
70
60
50
40
30
20
10
0
60
50
40
30
20
10
0
)
s
n
i
(
e
m
T
i
)
s
n
i
(
e