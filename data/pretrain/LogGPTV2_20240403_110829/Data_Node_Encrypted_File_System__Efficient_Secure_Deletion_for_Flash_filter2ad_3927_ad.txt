read and write was identical to UBIFS, suggesting
that (for multiple reads) cryptographic operations
are easily pipelined into the relatively slower ﬂash
memory read/write operations.
Some key caching optimizations can be added to
UBIFSec to improve the throughput. Whenever a
page of ﬂash memory is read, the entire page can be
cached at no additional read cost, allowing eﬃcient
sequential access to keys, e.g., for a large ﬁle. Long-
term use of the ﬁle system may reduce its eﬃciency
as gaps between used and unused keys result in new
ﬁles not being assigned sequential keys.
Improved
KSA organization can help retain this eﬃciency.
Write throughput, alternatively,
is easily im-
proved with caching. The sequence of keys for data
written in the next purging epoch is known at purg-
ing time when all these keys are randomly generated
and written to the KSA. By using a heuristic on the
expected number of keys assigned during a purging
epoch, the keys for new data can be kept in mem-
ory as well as written to the KSA. Whenever a key
is needed, it is taken and removed from this cache
while there are still keys available.
YAFFS UBIFS UBIFSec
Read rate (MiB/s)
Power usage (mA)
GiB read per %
Write rate (MiB/s)
Power usage (mA)
GiB written per %
4.4
39
5.4
2.4
30
3.8
3.9
39
4.8
2.1
46
2.2
3.0
39
3.7
1.7
41
2.0
Table 3: I/O throughput and battery consumption for
YAFFS, UBIFS, and UBIFSec.
ing keys are overwritten when the key is no longer
needed during normal decryption and encryption op-
erations. Caches contain keys for a longer time
but are cleared during a purging operation to en-
sure deleted keys never outlive their deletion purging
epoch. Applications storing sensitive data in volatile
memory may remain after the data’s deletion and so
secure memory deallocation should be provided by
the operating system to ensure its unavailability [5].
Timing Analysis. We timed the following ﬁle
system functions: mounting/unmounting the ﬁle
system and writing/reading a page. Addition-
ally, we timed the following functions speciﬁc to
UBIFSec: allocation of the cryptographic context,
reading the encryption key, performing an encryp-
tion/decryption, and purging a KSA LEB. We col-
lected dozens of measurements for purging, mount-
ing and unmounting, and hundreds of measurements
for the other operations (i.e., reading and writing).
We controlled the delay caused by our instrumenta-
tion by repeating the experiments instead of execut-
ing nested measurements, i.e., we timed encryption
and writing to a block in separate experiments.
We mounted a partition of the Android’s ﬂash
memory ﬁrst as a standard UBIFS ﬁle system and
then as UBIFSec ﬁle system. We executed a se-
quence of ﬁle I/O operations on the ﬁle system. We
collected the resulting times and present the 80th
percentile measurements in Table 4. Because of
UBIFS’s implementation details, the timing results
for reading data nodes contain also the time required
to read relevant TNC pages (if they are not currently
cached) from the storage medium, which is reﬂected
in the increased delay. Because the data node size
for YAFFS is half that of UBIFS, we also doubled
the read/write measurements for YAFFS for com-
parison. Finally, the mounting time for YAFFS is
for mounting after a safe unmount—for an unsafe
unmount, YAFFS requires a full device scan, which
takes several orders of magnitude longer.
Caching keys in memory opens UBIFSec to at-
tacks. We ensure that all memory buﬀers contain-
The results show an increase in the time required
for each of the operations. Mounting and unmount-
11
File system
operation
mount
unmount
read data node
write data node
prepare cipher
read key
encrypt
decrypt
purge one block
80th percentile execution time (ms)
YAFFS UBIFS
UBIFSec
43
44
0.92
1.1
-
-
-
-
-
179
0.55
2.8
1.3
-
-
-
-
-
236
0.67
4.0
2.5
0.05
0.38
0.91
0.94
21.2
Table 4: Timing results for various ﬁle system function-
ality on an android mobile phone.
ing the storage medium continues to take a frac-
tion of a second. Reading and writing to a data
node increases by a little more than a millisecond,
an expected result that reﬂects the time it takes to
read the encryption key from the storage medium
and encrypt the data. We also tested for notice-
able delay by watching a movie in real time from a
UBIFSec-formatted Android phone running the An-
droid OS: the video was 512x288 Windows Media
Video 9 DMO; the audio was 96.0 kbit DivX au-
dio v2. The video and audio played as expected
on the phone; no observable latency, jitter, or stut-
ter was observed during playback while background
processes ran normally.
Each atomic update of an erase block takes about
22 milliseconds. This means that if every KSA LEB
is updated, the entire data partition of the Nexus
One phone can be purged in less than a ﬁfth of a
second. The cost to purge a device grows with its
storage medium’s size. The erasure cost for purging
can be reduced in a variety of ways: increasing the
data node size to use fewer keys, increasing the dura-
tion of a purging epoch, or improving the KSA’s or-
ganization and key assignment strategy to minimize
the number of KSA LEBs that contain deleted keys.
The last technique works alongside lazy on-demand
purging of KSA LEBs that contain no deleted keys,
i.e., only used and unused keys.
Granularity Tradeoﬀ Our
solution encrypts
each data node with a separate key allowing eﬃ-
cient secure deletion of data from long-lived ﬁles,
e.g., databases. Other related work instead encrypts
each ﬁle with a unique key, allowing secure deletion
only at the granularity of an entire ﬁle [19]. This is
well suited for media ﬁles, such as digital audio and
photographs, which are usually created, read, and
deleted in their entirety. However, if the encrypted
ﬁle should permit random access and modiﬁcation,
Data node size
(ﬂash pages)
1
8
64
512
4096
KSA size
Copy cost
(EBs per GiB)
(EBs)
64
8
1
0.125
0.016
0
0.11
0.98
63.98
511.98
Table 5: Data node granularity tradeoﬀs assuming 64
2-KiB pages per erase block.
then one of the following is true: (i) the cipher is used
in an ECB-like mode, resulting in a system that is
not semantically secure, (ii) the cipher is used in a
CBC-like mode where all ﬁle modiﬁcations require
re-encryption of the remainder of the ﬁle, (iii) the
cipher is used in a CBC-like mode with periodic IVs
to facilitate eﬃcient modiﬁcation, (iv) the cipher is
used in counter mode, resulting in all ﬁle modiﬁca-
tions requiring rewriting the entire ﬁle using a new
IV to avoid the two-time pad problem [20], or (v)
the cipher is used in counter mode with periodic IVs
to facilitate eﬃcient modiﬁcations.
We observe the that ﬁrst option is inadequate as a
lack of semantic security means that some informa-
tion about the securely deleted data is still available.
The second and fourth options are special cases of
the third and ﬁfth options respectively, where the IV
granularity is one per ﬁle and ﬁle modiﬁcations are
woefully ineﬃcient. Thus, a tradeoﬀ exists between
the storage costs of IVs and additional computation
for modiﬁcations. As the IV granularity decreases to
the data node size, the extra storage cost required
for IVs is equal to the KSA storage cost for DNEFS’s
one key per data node, and the modiﬁcation cost is
simply that of the single data node.
We emphasize that a scheme where IVs were not
stored but instead deterministically computed, e.g.,
using the ﬁle oﬀset, would inhibit secure deletion: so
long as the ﬁle’s encryption key and previous version
of the data node were available, the adversary could
compute the IV and decrypt the data. Therefore, all
IVs for such schemes must be randomly generated,
stored, and securely deleted.
Table 5 compares the encryption granularity trade
oﬀ for a ﬂash drive with 64 2-KiB pages per erase
block. To compare DNEFS with schemes that en-
crypt each ﬁle separately, simply consider the data
node size as equal to the IV granularity or the ex-
pected size ﬁle size. The KSA size, measured in
erase blocks per GiB of storage space, is the amount
of storage required for IVs and keys, and is the worst
case number of erase blocks that must be erased
during each purging operation. The copy cost, also
12
measured in erase blocks, is the amount of data that
must be re-written to the ﬂash storage medium due
to a data node modiﬁcation that aﬀects only one
page of ﬂash memory. For example, with a data
node size of 1024 KiB and a page size of 2 KiB, the
copy cost for a small change to the data node is 1022
KiB. This is measured in erase blocks because the
additional writes, once ﬁlling an entire erase block,
result in an additional erase block erasure, otherwise
unnecessary with a smaller data node size.
As we observed earlier, reducing the number of
keys required to be read from ﬂash per byte of data
improves read and write throughput. From these
deﬁnitions, along with basic geometry of the ﬂash
drive, it is easy to compute the values presented in
Table 5. When deploying DNEFS, the administra-
tor can choose a data node size by optimizing for
the costs given how frequently small erasures and
complete purges are executed.
5 Extensions and Optimizations
Compatibility with FTLs. The most widely-
deployed interface for ﬂash memory is the Flash
Translation Layer (FTL) [1], which maps logical
block device sectors (e.g., a hard drive) to physi-
cal ﬂash addresses. While FTLs vary in implemen-
tation, many of which are not publicly available, in
principle DNEFS can be integrated with FTLs in the
following way. All ﬁle-system data is encrypted be-
fore being written to ﬂash, and decrypted whenever
it is read. A key storage area is reserved on the ﬂash
memory to store keys, and key positions are assigned
to data. The FTL’s in-memory logical remapping
of sectors to ﬂash addresses must store alongside a
reference to a key location. The FTL mechanism
that rebuilds its logical sector to address mapping
must also rebuild the corresponding key location.
Key locations consist of a logical KSA erase block
number and the actual oﬀset inside the erase block.
Logically-referenced KSA erase blocks are managed
by storing metadata in the ﬁnal page of each KSA
erase block. This page is written immediately after
successfully writing the KSA block and stores the
following information: the logical KSA number so
that key references need not be updated after purg-
ing, and an epoch number so that the most recent
version of the KSA block is known. With this infor-
mation, the FTL is able to replicate the features of
UBI that DNEFS requires.
Generating a correct key state map when mount-
ing is tied to the internal logic of the FTL. Assuming
that the map of logical to physical addresses along
with the key positions is correctly created, then it
is trivial to iterate over the entries to mark the cor-
responding keys as used. The unmarked positions
are then purged to contain new data. The FTL
must also generate cryptographically-secure random
data (e.g., with an accelerometer [38]) or be able
to receive it from the host. Finally, the ﬁle sys-
tem mounted on the FTL must issue TRIM com-
mands [16] when a sector is deleted, as only the ﬁle
system has the semantic context to know when a
sector is deleted.
Purging Policies. Purging is currently performed
after a user-controlled period of time and before un-
mounting the device. More elaborate policies are
deﬁnable, where purging occurs once a threshold of
deleted keys is passed, ensuring that the amount of
exposable data is limited, so the deletion of many
ﬁles would thus act as a trigger for purging. A low-
level control allows user-level applications to trigger
a purge, such as an email application that purges
the ﬁle system after clearing the cache. We can al-
ternatively use a new extended attribute to act a
trigger: whenever any data node belonging to a sen-
sitive ﬁle is deleted, then DNEFS triggers an imme-
diate purge. This allows users to have conﬁdence
that most ﬁles are periodically deleted, while sensi-
tive ﬁles are promptly deleted.
Securely Deleting Swap. A concern for secure
deletion is to securely delete any copies of data made
by the operating system. Data that is quite large
may be written to a swap ﬁle—which may be on the
same ﬁle system or on a special cache partition. We
leave as future work to integrate our solution to a
secure deleting cache. (There exist encrypted swap
partitions [31], but not one that securely deletes the
memory when it is deallocated.) We expect it to
be simple to design, as cache data does not need to
persist if power is lost; an encryption-based approach
can keep all the keys in volatile memory and delete
them immediately when they are no longer needed.
Encrypted File System. Our design can be triv-
ially extended to oﬀer a passphrase-protected en-
crypted ﬁle system: we simply encrypt the KSA
whenever we write random data, and derive the
decryption key from a provided passphrase when
mounting.