value speciﬁes whether Mendosus successfully injected the
requested fault.
public static boolean cancel(int fault-
Type, int interval, SomeList parameters)
4The tuple (faultType, interval, parameters) serves as
unique identiﬁer of an injected fault in Mendosus.
This method asks Mendosus to cancel an on-going fault
of type faultType. The boolean return value speciﬁes
whether Mendosus was able to locate and cancel the fault.
Instrumented application code and Mendosus currently
communicate synchronously: on successful return of the
inject method, the fault has been injected, and any sub-
sequent use of the affected resource (within interval sec-
onds) will produce an error. Similarly, on a successful re-
turn of cancel, the previously injected fault already has
been canceled. For our tests, described in Section 4, we
used an interval large enough to ensure that faults re-
mained in effect until explicitly canceled. Our preliminary
experiment suggests that this synchronous approach is suf-
ﬁcient except in the presence of latent errors.
While our algorithms are not sufﬁcient to handle the gen-
eral problem of latent errors, we can use several simple vari-
ations on our approach to improve fault-catch coverage in
the presence of latent errors. First, we can use the approach
described above, which we label fault-cancel mode, though
this may cause faults to be canceled before latent errors are
observed. Second, we can use fault-not-cancel mode, in
which we never cancel a fault once injected. This could in-
crease coverage if a fault that had no impact when it was ini-
tially injected causes an exception during a later execution
of the same try block. Finally, we can use fault-reinject
mode, in which faults are canceled, but then injected again
at a later execution of the try (in a separate run of the pro-
gram). This could increase coverage if a fault triggers an
exception only in some particular program state (such as
when a disk buffer is nearly empty).
3.3 Multi-Threaded and Distributed Applications
Internet services are generally built as multi-threaded or
distributed applications. Several additional issues must be
addressed when testing a multi-threaded application.
For a multi-threaded application running on a single
node, as is the case for our initial benchmark, the question
arises: What happens when the thread that requested a fault
injection is context-switched out before the fault has been
canceled? Three scenarios are possible:
1. the fault does not affect other threads that run before
the original thread is allowed to run again,
2. some other thread is affected by the fault and crashes
the application, or
3. one or more other threads are affected by the fault, but
they recover sufﬁciently to not crash the application,
eventually allowing the original thread to run again
with the fault still activated. If more than one thread
executes the same try block and experiences the error
caused by the injected fault, we will count the catch
block as covered, regardless of which thread actually
executed it.
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:06:14 UTC from IEEE Xplore.  Restrictions apply. 
The ﬁrst case clearly does not raise any concern. The
second case is a successful test that requires bug ﬁxing in
the application before this particular fault-catch pair can be
exercised.
In the third case, our current instrumentation
will count the coverage of the fault-catch pair in the orig-
inal thread, as expected, but not any other catch block that
was exercised “incidentally”. This is not a problem, as a
failure to notice the incidental coverage will simply cause
our system to perform an unnecessary test.
Testing of distributed applications running on multiple
nodes raises at least two additional challenges. First, we
may wish to inject a fault that affects several nodes as soon
as one thread reaches a particular try. Our current system
can insert instrumentation to do this, but we have not yet
investigated how best to perform fault injection. For exam-
ple, we may or may not wish to allow the thread injecting
the fault to continue before the entire distributed fault injec-
tion is complete. If the thread is not blocked, we must make
sure that the fault is injected far enough in advance of the
vulnerable operation – we believe this goal will produce the
greatest challenge for our instrumentation algorithms.
Second, the question of how to inject faults at the time
when some system-wide condition has been achieved in a
distributed application raises even more interesting issues.
Simply blocking the thread that is communicating with
Mendosus is not sufﬁcient; other threads on other nodes
may be progressing past the vulnerable point of interest.
This has been partially studied before [10] although not in
the context of compiler-directed fault injection. We leave
this as an important issue for future work.
4 Feasibility Case Study
To offer proof-of-concept for our methodology we per-
formed a small case study using a http proxy server called
Mufﬁn [2].
In this experiment, we hand simulated our
compiler-directed analyses to determine where to inject
faults and inserted by hand instrumentation for communi-
cation with Mendosus and recording of coverage. We stud-
ied all faults using fault-cancel mode, and also used both
fault-not-cancel and fault-reinject mode for latent errors.
4.1 Mufﬁn and its Fault Vulnerabilities
Mufﬁn is a single-node, multi-threaded application
whose interactions with the operating system are mainly re-
ceiving and sending data over the network (i.e., relaying re-
quests and web pages). The disk access is essentially to
log fulﬁlled requests without ever trying to read them back
in; this data is easily stored in cache, so the footprint is too
small for meaningful experiments. Instead, we concentrated
on introducing faults related to network I/O.
Table 1 gives the faults used in our study (i.e., our uni-
verse of faults F , as mentioned in Section 2.3). These faults
are divided into two classes. The ﬁrst class is network hard-
ware failures such as NIC, link, and switch failures. For a
Server Thread
 New ServerSocket
 Accept client connection
 Fork Handler Thread
try block 8
try block 6
Handler Thread
 Read client request
 Connect to http server
 Send http request
 Receive http header
 Copy http content to client
 end
try block 0
try block 2
try block 3
try block 1
t
r
y
b
l
o
c
k
7
Figure 1. Structure of Mufﬁn
single-node application, these all have the same effect (i.e.,
the inability to read or write data), so we experiment with
only one entry from this class, namely NIC DOWN. In
addition, we do not consider transient packet loss because
Mufﬁn depends only on TCP, which completely hides such
faults from the application unless its seriousness approaches
that of a NIC DOWN. The second class is made up of
faults resulting from the operating system such as exhaus-
tion of resources or a corruption of essential data structures.
The complete set of network operations used in Mufﬁn is:
bind, connect, accept, read, write.
Seven of Mufﬁn’s nine catch blocks can catch IOEx-
ceptions due to the faults in Table 1. Figure 1 shows a
high-level view of the control ﬂow in Mufﬁn and the loca-
tion of the try blocks associated with these catches. Each
try block has only one associated catch; the numbers
shown in Figure 1 serve as identiﬁers of each try block or
the associated catch. We manually instrumented the try
and catch blocks as per the algorithms of Section 3.
try Operations Possible Faults
0
read
NIC DOWN, NET EBADF, NET EFAULT,
NET EPIPE, NET EAGAIN
1
read/write NIC DOWN, NET EBADF, NET EFAULT,
NET EPIPE, NET EAGAIN
NIC DOWN, NET EBADF, NET EFAULT,
NET EPIPE, NET EAGAIN
NIC DOWN, NET EBADF, NET EFAULT,
NET EPIPE, NET EAGAIN
NIC DOWN, NET ENOMEM
NET EAGAIN,
accept
connect NIC DOWN,
2 write
read
3
6
7
8
bind
NET ECONNREFUSED
NET EBADF, NET ENOMEM
Table 2. Vulnerable Operations in try Blocks
Table 2 lists the vulnerable operations in each try and
Proceedings of the 2003 International Conference on Dependable Systems and Networks (DSN’03) 
0-7695-1959-8/03 $17.00 (c) 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:06:14 UTC from IEEE Xplore.  Restrictions apply. 
Fault Class
Hardware
Fault Type
NIC DOWN
Operations
All except bind
OS error code NET EBADF
NET EFAULT
NET EPIPE
NET EAGAIN
NET ENOMEM
NET ECONNREFUSED connect
Description
Drop all the packets coming from or to a given
IP for a certain amount of time to simulate
some network hardware failure
Bad socket number
Buffer space unavailable
Socket with only one end open
bind,read,write
read,write
read,write
connect,read,write Necessary resource temporarily unavailable
bind,accept
Not enough system memory
Connection refused (e.g., remote node crashed
and not yet recovered)
Table 1. Faults Used in the Experiment
the corresponding faults which can cause exceptions reach-
ing the associated catch; in the terms deﬁned in Section
2.3, these comprise the relevant fault sets fi for each catch
i. Note that try blocks 0-3 are all vulnerable to the same
set of faults, as they all send or receive data, but different
sets of faults may affect try blocks 6-8, which involve the
establishment of network connections.
4.2 Experiment Speciﬁcs
Catch Faults
0
Exceptions
java.net.SocketException
NET EBADF
java.net.SocketException
NET EFAULT
java.net.SocketException
NET EPIPE
java.net.SocketException
NET EAGAIN
NIC DOWN
java.io.InterruptedIOException
java.net.SocketException
NET EBADF
java.net.SocketException
NET EFAULT
java.net.SocketException
NET EPIPE
java.net.SocketException
NET EAGAIN
java.io.IOException
NET EBAD
java.io.IOException
NET EFAULT
java.io.IOException
NET EPIPE
java.io.IOException
NET EAGAIN
java.net.SocketException
NET EBADF
java.net.SocketException
NET EFAULT
java.net.SocketException
NET EPIPE
NET EAGAIN
java.net.SocketException
NET ENOMEM java.net.SocketException
NIC DOWN
NET EAGAIN
NET CONNREFUSED java.net.ConnectException
NET EBADF
java.net.SocketException
NET ENOMEM java.net.SocketException
java.net.NoRouteToHostException
java.net.SocketException
1
2
3
6
7
8
Table 3. Faults and Exceptions Recorded
In our experiment,
the proxy server (Mufﬁn version
0.9.3a), the actual http server (Apache), and a synthetic
client that generates http requests are running separately,
each on one of three 800 MHz PIII PCs under Linux 2.2.14-
5.0. We used the IBM Java 2.13 Virtual Machine for Linux.
The client generates a stream of http requests according
to a Poisson process with a given arrival rate. Each request
is set to time out after 20 seconds if a connection cannot
be completed, and to time out after 600 seconds if, after
successful connection, the request cannot be completed.
For each test run of Mufﬁn, we injected a single fault into
one instrumented try block in fault-cancel mode. One run
was performed for each valid fault-try combination (see
Table 2) and data recorded for all these test runs. Table 3
shows the results of our experiment. The Faults column
gives the ei sets discussed in Section 2.3.
In all
the tests, all faults except NIC DOWN are