### Value Specification
The `value` parameter specifies whether Mendosus successfully injected the requested fault.

### Method: `cancel`
```java
public static boolean cancel(int faultType, int interval, SomeList parameters)
```
- **Description**: The tuple `(faultType, interval, parameters)` serves as a unique identifier for an injected fault in Mendosus.
- **Functionality**: This method requests Mendosus to cancel an ongoing fault of the specified `faultType`.
- **Return Value**: A boolean value indicating whether Mendosus was able to locate and cancel the fault.

### Synchronous Communication
Instrumented application code and Mendosus communicate synchronously:
- **Injection**: Upon successful return from the `inject` method, the fault has been injected. Any subsequent use of the affected resource within the specified `interval` (in seconds) will produce an error.
- **Cancellation**: On a successful return from the `cancel` method, the previously injected fault has been canceled.

For our tests, described in Section 4, we used an interval large enough to ensure that faults remained in effect until explicitly canceled. Our preliminary experiments suggest that this synchronous approach is sufficient, except in the presence of latent errors.

### Handling Latent Errors
Our algorithms are not sufficient to handle the general problem of latent errors. However, we can use several variations on our approach to improve fault-catch coverage:
1. **Fault-Cancel Mode**: This mode cancels the fault after injection, though it may cause faults to be canceled before latent errors are observed.
2. **Fault-Not-Cancel Mode**: In this mode, faults are never canceled once injected. This could increase coverage if a fault that had no initial impact causes an exception during a later execution of the same try block.
3. **Fault-Reinject Mode**: Faults are canceled and then re-injected at a later execution of the try block (in a separate run of the program). This could increase coverage if a fault triggers an exception only in specific program states, such as when a disk buffer is nearly empty.

### Multi-Threaded and Distributed Applications
Internet services are often built as multi-threaded or distributed applications, which introduce additional testing challenges.

#### Single-Node Multi-Threaded Applications
For a multi-threaded application running on a single node, consider the following scenarios when a thread that requested a fault injection is context-switched out before the fault is canceled:
1. **No Impact on Other Threads**: The fault does not affect other threads that run before the original thread is allowed to run again.
2. **Application Crash**: Another thread is affected by the fault and crashes the application.
3. **Partial Recovery**: One or more other threads are affected by the fault but recover sufficiently to not crash the application, eventually allowing the original thread to run again with the fault still activated. If multiple threads execute the same try block and experience the error caused by the injected fault, we count the catch block as covered, regardless of which thread executed it.

**Analysis**:
- **Scenario 1**: No concern.
- **Scenario 2**: Successful test requiring bug fixing in the application.
- **Scenario 3**: Current instrumentation counts the coverage of the fault-catch pair in the original thread, but not any other catch block exercised incidentally. This is not a problem, as missing incidental coverage will simply cause an unnecessary test.

#### Distributed Applications
Testing distributed applications running on multiple nodes raises additional challenges:
1. **Fault Injection Across Nodes**: We may need to inject a fault that affects several nodes as soon as one thread reaches a particular try. Our current system can insert instrumentation for this, but the best method for performing fault injection remains under investigation. For example, we must decide whether to allow the fault-injecting thread to continue before the entire distributed fault injection is complete.
2. **System-Wide Conditions**: Injecting faults when a system-wide condition is achieved in a distributed application is complex. Simply blocking the thread communicating with Mendosus is insufficient; other threads on other nodes may progress past the vulnerable point of interest. This issue has been partially studied [10], though not in the context of compiler-directed fault injection, and remains an important area for future work.

### Feasibility Case Study
To demonstrate the feasibility of our methodology, we performed a small case study using a HTTP proxy server called Mufﬁn [2]. We manually simulated our compiler-directed analyses to determine where to inject faults and inserted instrumentation for communication with Mendosus and recording of coverage. We tested all faults using fault-cancel mode and also used both fault-not-cancel and fault-reinject modes for latent errors.

#### Mufﬁn and Its Fault Vulnerabilities
Mufﬁn is a single-node, multi-threaded application primarily interacting with the operating system for network I/O (receiving and sending data). Disk access is minimal and mainly for logging fulfilled requests. We focused on introducing faults related to network I/O.

**Table 1: Faults Used in the Experiment**
| Fault Class | Fault Type | Operations | Description |
|-------------|------------|------------|-------------|
| Hardware    | NIC DOWN   | All except bind | Drop packets to simulate hardware failure |
| OS Error    | NET EBADF  | All except bind | Bad socket number |
|             | NET EFAULT | All except bind | Buffer space unavailable |
|             | NET EPIPE  | All except bind | Socket with only one end open |
|             | NET EAGAIN | All except bind | Necessary resource temporarily unavailable |
|             | NET ENOMEM | bind, accept | Not enough system memory |
|             | NET ECONNREFUSED | connect | Connection refused (e.g., remote node crashed) |

**Figure 1: Structure of Mufﬁn**
- **Server Thread**: New ServerSocket, Accept client connection, Fork Handler Thread
- **Handler Thread**: Read client request, Connect to HTTP server, Send HTTP request, Receive HTTP header, Copy HTTP content to client

**Table 2: Vulnerable Operations in Try Blocks**
| Try Block | Operations | Possible Faults |
|-----------|------------|-----------------|
| 0         | read       | NIC DOWN, NET EBADF, NET EFAULT, NET EPIPE, NET EAGAIN |
| 1         | read/write | NIC DOWN, NET EBADF, NET EFAULT, NET EPIPE, NET EAGAIN |
| 2         | write      | NIC DOWN, NET EBADF, NET EFAULT, NET EPIPE, NET EAGAIN |
| 3         | read       | NIC DOWN, NET EBADF, NET EFAULT, NET EPIPE, NET EAGAIN |
| 6         | accept     | NIC DOWN, NET EBADF, NET ENOMEM |
| 7         | connect    | NIC DOWN, NET ECONNREFUSED, NET EBADF, NET ENOMEM |
| 8         | bind       | NIC DOWN, NET EBADF, NET ENOMEM |

**Table 3: Faults and Exceptions Recorded**
| Catch | Faults | Exceptions |
|-------|--------|------------|
| 0     | NIC DOWN, NET EBADF, NET EFAULT, NET EPIPE, NET EAGAIN | java.net.SocketException, java.io.InterruptedIOException |
| 1     | NIC DOWN, NET EBADF, NET EFAULT, NET EPIPE, NET EAGAIN | java.net.SocketException, java.io.IOException |
| 2     | NIC DOWN, NET EBADF, NET EFAULT, NET EPIPE, NET EAGAIN | java.net.SocketException, java.io.IOException |
| 3     | NIC DOWN, NET EBADF, NET EFAULT, NET EPIPE, NET EAGAIN | java.net.SocketException, java.io.IOException |
| 6     | NIC DOWN, NET EBADF, NET ENOMEM | java.net.SocketException, java.io.IOException |
| 7     | NIC DOWN, NET ECONNREFUSED, NET EBADF, NET ENOMEM | java.net.ConnectException, java.net.SocketException, java.net.NoRouteToHostException |
| 8     | NIC DOWN, NET EBADF, NET ENOMEM | java.net.SocketException |

### Experiment Specifics
- **Setup**: Mufﬁn (version 0.9.3a), Apache HTTP server, and a synthetic client generating HTTP requests were run on three 800 MHz PIII PCs under Linux 2.2.14-5.0, using the IBM Java 2.13 Virtual Machine for Linux.
- **Client Behavior**: The client generates HTTP requests according to a Poisson process with a given arrival rate. Each request times out after 20 seconds if a connection cannot be completed, and after 600 seconds if the request cannot be completed after a successful connection.
- **Test Runs**: For each test run, a single fault was injected into one instrumented try block in fault-cancel mode. One run was performed for each valid fault-try combination (see Table 2), and data were recorded for all test runs.

In all tests, all faults except NIC DOWN were successfully handled and recorded.