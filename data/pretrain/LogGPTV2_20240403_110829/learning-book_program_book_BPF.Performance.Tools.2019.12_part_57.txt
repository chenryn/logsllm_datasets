resume: 0
ed_explred: 0
interface_up: 1
lnterface_down: 0
0 :asmed b"uTupe
qoeue_0_tx_cnt: 100219217
qoeve_0_tx_bytes: 84830086234
0:dosenenbxaoenenb
queue_0_tx_queve_wakeup: 0
queue_0_cx_dna_napp1ng_ezx: 0
goeve_0_tx_linearize: 0
qoeve_0_tx_llnearize_failed: 0
queve_0_cx_napi_comp: 112514572
queue_0_tx_tx_po1l: 112514649
queue_0_tx_doorbe1ls: 52759561
[ . - - ]
This fetches statistics from the kernel ethtool framework, which many network device drivers
support. Device drivers can define their own ethtool metrics.
---
## Page 445
408
 Chapter 10 Networking
The 1 option shows driver details, and  shows interface tunables. For example:
ethtool -1 eth0
driver1 ens
version: 2,0.3K
[...]
 ethtool -k eth0
Features for eth0:
rx=checkaumning: cn
[...]
tcp=segmentation=ofcload: off
tx-tcp-segmentation: off [fixed]
tx-tcp-ecn=segnentation: off [flxed]
tx-tcp-nangleid-segmentation: off [fixed]
tx-tcp6=segnentation: off [fixed]
udp-fragnentation-offload: off
genezic=segnentation=oCfload: on
generic=receive=offload: on
largereceive=offload: off [[ixed]
rx=vlan=offload: off [fixed]
tx=vlan=offload: off [fixed]
ntuple-filterst off [fixed]
receive=hash.ng: on
highdns: on
[. - -]
This example is a cloud instance with the ena driver, and tcp-segmentation-offload is currently
off. The K option can be used to change these tunables.
10.2.8tcpdump
Finally, tcpdump(8) can capture packets for study. This is termed *packet sniffing.° For example,
sniffing interface en0 () and writing (x) to a dump file and then reading it (x) without name
resolution (n)°:
Todanpdos*sno/dan/ a- ou T- dampdo ±
tcpdunp: listening on en0, 1ink-type EN10MB (Ethernet),。 capture size 262144 bytes
^C451 packets captured
477 packets received by filter
0 packets dropped by kernel
+ topdump -nz /tmp/out.topdump01
reading fron f11e /tnp/out,tepdusp01, 1ink=type EnioMs (Etheznet)
6 lt may cause sdditional neti
nted side effect of r
reading the file
---
## Page 446
10.2 Traditional Tools
409
13:39:48. 917870 1P 10.0.0.65.54154 > 69,53.1.1.4433: 00e, 1ength 1357
13:39:48. 921398 IP 108.177,1.2,443 > 10.0.0.65.59496: F1ags [P.1, seg
3108664869:3108664929, ack 2844371493, vin 537, options [nop,nop,Ts val 252126]
368 ecr 4065T400831, 1ength 60
13:39:48. 921442 1P 10.0.0.65.59496 > 108.177,1.2,443: Flag8 [.1, ack 60, xin 505,
opt.ion.s [nop,ncp, TS va1 4065741487 ecr 25212613681, 1ength 0
13:39:48. 921463 IP 108.177.1.2.443 > 10.0.0.65.59496: Flags [P.], 5eg 0:60, ack 1
vin 537, optlons [nop,nop,TS val 2521261T93 ecr 4065T40083], length 60
[...]
tcpxdump(8) output files can be read by other tools, including the Wireshark GUI [104]. Wireshark
allows packet headers to be easily inspected, and TCP sessions to be “followed,” reassembling the
transmit and receive bytes so that client/host interactions can be studied.
While packet capture has been optimized in the kernel and the libpcap library, at high rates it can
still be expensive to perform, costing additional CPU overheads to collect, and CPU, memory, and
lisk resources to store, and then again to post-proces, These overheads can be reduced somewhat
by using a filter, so that only packets with certain header details are recorded. However, there are
CPU overheads even for packets that are not collected.? Since the filter expression must be applied
to all packets, its processing must be efficient. This is the origin of Berkeley Packet Filter (BPF),
which was created as a packet capture filter and later extended to become the technology I am
using in this book for tracing tools. See Section 2.2 for an example of a tcpdlump(8) filter program.
While packet capture tools may appear to show comprehensive details of networking, they
only show details sent on the wire. They are blind to kernel state, including which processes are
responsible for the packets, the stack traces, and kernel state of the sockets and TCP: Such details
can be seen using BPF tracing tools.
10.2.9
/proc
s sau/xord/ ug asoq Aepadsa 'sag xord/ uong sopau aonos sjoos sses jogd atp po due
directory can be explored at the command line:
$ 1s /proc/net/
anyeast6
1f_lnet6
seteu"setqed
ptype
sockstat6
igmp
ip_tables_targets
softnet_stat
bnep
1gnp6
pv6_route
rfcomm
rav6
/1e15
ip6_flov]abe1
12cap
tcp
dev
Ip_nr_cache
ncf11ter
route
tcp6
der_mcast.
ip6_nr_rif
ncfilter6
24939.
udp
dev_snnp6/
Ip5_tables_natche.s
netf11tex/
rt_acct
udp6
fib_trie
ip6_tsbles_nanes
net1ink
rt_cache
udplite
fib_triestat
1p6_tables_target.s
netstat
sCo
udp11tes
7 Every skb has to be clor
ed before it is h8
 only later fitered (see
---
## Page 447
410
Chapter 10 Networking
hci
ip_nr_cache
packet
snnp
unix
icnp
1p_nr_v1f
pzotoco1a
snnp6
vlreless
icnp6
ip_tables_nstches
paqpsd
491005
xfrn_stat
5 cat /proc/net/snnp
Ip: Forxarding DefaultTTl InReceives InHdrErrors InAddrErrors ForvDatagram&
InUnknoxnPzotos InDiscards InDelivezs OutRequesta OutDiscards OutoRoutes
ReasmTimeout ReasnReqds RessnOKs BessnFails FragoKs FragFails FragCreates
Ip: 2 64 45794729 0 28 0 0 0 45777774 40659467 4 6429 0 0 0 0 0 0 0
[...]
The netstat(1) and sar(1) tools expose many of these metrics. As shown earlier, they include
system-wide statistics for packet rates, TCP active and passive new connections, TCP retransmits,
ICMP errors, and much more.
There are also /proc/interrupts and /proc/softirqs, which can show the distribution of network
device interrupts across CPUs. For example, on a two-CPU system:
cat /proc/interrupts
CPU0
CP01
[...]
28:
1775400
80
PCIMSI 81920-edge
ena-mgmntlpc1 : 0000 :00 : 05 0
29:
533
5501189
PCI-MSI
81921-edge
eth0Tx-Rx0
30:
4526113
278
PCI-MSI 81922=edge
cat /proc/softirqs
eth0Tx-Rx1
00d3
CPU1
[...]
X±3N
332966
34
ET_RX:
10915058
11500522
[.--]
This system has an eth0 interface that uses the ena driver. The above output shows eth0 is
using a queue for each CPU, and receive softirqs are spread across both CPUs. (Transmits appear
unbalanced, but the network stack often skips this softirq and transmits directly to the device.)
sseps dnaqu quad o uogdo 1 te seq ose (g)essdu
The BPF tools that follow have been created to extend, rather than duplicate, network observ-
ability beyond these /proc and traditional tool metrics. There is a BPF sockstat(8) for system-wide
socket metrics, since those particular metrics are not available in /proc. But there is not a similar
tcpstat(8), udpstat(8), or ipstat(8) tool for system-wide metrics: while it is possible to write these in
BPF, such tools only need to use the already-maintained metrics in /proc. It is not even necessary
to write those tools: netstat(1) and sar(1) provide that observability.
The following BPF tools extend observability by breaking down statistics by processID, process name,
IP address, and ports, revealing stack traces that led to events, exposing kernel state, and by showing
custom latency measurements. It might appear that these tools are comprehensive: they are not.
They are designed to be used with /proc/net and the earlier traditional tools, to extend observability.
---
## Page 448
10.3 BPF Tools
411
10.3
BPF Tools
This section covers the BPF tools you can use for network performance analysis and troubleshoot
ing. They are shown in Figure 10-4.
Applications
sockatat
oonnlat solatayt
sofanily
System Call Interface
Sockets
cetic
TCP
UDP
IP
superping
Queueing Discipline
Network Device
Leee0211scan
Link Layer
bpftrace
Device Drivers
Figure 10-4 BPF tools for network analysis
bpftrace is shown in Figure 10-4 as observing device drivers. See Section 10.4.3 for examples. The
other tools in this figure are from either the BCC or bpftrace repositories covered in Chapters 4
and 5, or were created for this book. Some tools appear in both BCC and bpftrace. Table 10-3 lists
the origins of these tools (BT is short for bpftrace).
Table 10-3
Network-Related Tools
Tool
Source
Target
Description
sockstat
Book
Sockets High-level socket statistics
sofami1y
Book
Sockets Count address familes for new sockets, by process
soprotoco1
Book
Sockets
8 Count transport protocols for new sockets,
by process
soconnect
Book
Sockets Trace socket IP-protocol connections with details
soaccept
Book
Sockets Trace socket IP-protocol accepts with details
socketio
Book
Sockets Summarize socket details with I/0 counts
socksize
Book
Sockets Show socket I/O sizes as per-process histograms
sormen
Book
Sockets Show socket receive buffer usage and overflows
soconnlat
Book
Sockets
Summarize IP socket connection latency
with stacks
solstbyte
Book
SocketsSummarize IP socket first byte latency
tcpconnect
BCC/BT/book
TCP
Trace TCP active connections (connect()
tcpaccept
BCC/BT/bookTCP
Trace TCP passive connections (accept()
---
## Page 449
412
Chapter 10 Networking
Tool
Source
Description
tcp1ife
BCC/book
TCP
Trace TCP session lifespans with connection details
tcptop
BCC
TCP
Show TCP send/recv throughput by host
tcpretrans
BCC/BT
TCP
Trace TCP retransmits with address and TCP state
tcpsynb1
Book
TCP
Show TCP SYN backlog as a histogram
tcpvin
Book
TCP
Trace TCP send congestion window parameters
tcpnagle
Book
TCP
Trace TCP nagle usage and transmit delays
Book
 UDP
Trace new UDP connections from localhost 
gethostlatency
Book/BT
SNQ
Trace DNS lookup latency vialibrary calls
Ipecn
Book
IP
Trace IP inbound explicit congestion notification
burdxedn.s
Book
ICMP
Measure ICMP echo times from the network stack
qdisc-fq (...)
Book
qdliscs
Show FQ qdisc queue latency
netsize
Book
net
Show net device I/O sizes
nettxlat
Book
net
Show net device transmission latency
skbdrop
Book
skbs
Trace sk_buff drops with kernel stack traces
skb1ife
Book
skbs
Lifespan of sk_buff as inter-stack latency
leee80211scan
Book
WiFi
Trace IEEE 802.11 WiFi scanning
For the tools from BCC and bpftrace, see their repositories for full and updated lists of tool
options and capabilities. A selection of the most important capabilities is summarized here.
10.3.1
sockstat
sockstat(8)a prints socket statistics along with counts for socket-related system calls each second.
For example, on a production edge server:
+ sockstat.bt
Attach.ing 10 probes..
Tracing sock statistics, Output every 1 second.
01:11 : 41
[tracepoint:syscalls:sys_enter_bind] : 1
e[tracepoint:syscalls:sys_entex_socket] : 67
e[tracepoint:syscalls:sys_enter_connect] : 67
?[tracepoint:ayscal1s:sys_entex_accept4] : 89
[kgrobe:sock_sendnsg] : 5280
e[kapzobe:sock_recvnsg] : 10547
01:11 : 42
[..] 
8 0rigin: 1 cr
ed it for this 
n 14-Apr-2019
---
## Page 450
10.3 BPF Tools
413
A time is printed each second (e.g., *21:22:56°), followed by counts for various socket events. This
example shows 10,547 sock_recvmsg() and 5280 sock_sendmsg() events per second, and fewer
than one hundred accept4(2)s and connect(2)s.
The role of this tool is to provide high-level socket statistics for workload characterization, and
starting points for further analysis. The output includes the probe name so that you can investi-
gate further; for example, if you see a higher-than-expected rate of kprobe:sock_sendmsg events,
the process name can be fetched using this bpftrace one-liner?:
（: (1qumoo =[mmoo]e 1 6supuesxoos:oqoxdx, - soezagdq 
Attaching 1 probe...