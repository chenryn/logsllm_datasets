title:On calibrating enterprise switch measurements
author:Boris Nechaev and
Vern Paxson and
Mark Allman and
Andrei V. Gurtov
On Calibrating Enterprise Switch Measurements
Boris Nechaev
Helsinki Institute for
Information Technology HIIT /
Helsinki University of
Technology TKK
boris.nechaev@hiit.ﬁ
Mark Allman
International Computer
Science Institute
PI:EMAIL
Vern Paxson
International Computer
Science Institute
University of California,
Berkeley
PI:EMAIL
Andrei Gurtov
Helsinki Institute for
Information Technology HIIT /
Helsinki University of
Technology TKK
gurtov@hiit.ﬁ
ABSTRACT
The complexity of modern enterprise networks is ever-increasing,
and our understanding of these important networks is not keeping
pace. Our insight into intra-subnet trafﬁc (staying within a single
LAN) is particularly limited, due to the widespread use of Ethernet
switches that preclude ready LAN-wide monitoring. We have re-
cently undertaken an approach to obtaining extensive intra-subnet
visibility based on tapping sets of Ethernet switch ports simultane-
ously. However, doing so leads to a number of measurement cal-
ibration issues that require careful consideration to address. First,
one must correctly account for redundant copies of packets that ap-
pear due to switch ﬂooding, which if not accurately identiﬁed can
greatly skew subsequent analysis results. We show that a simple,
natural rule one might use for doing so in fact introduces system-
atic errors, but an altered version of the rule performs signiﬁcantly
better. We then employ this revised rule to aid with calibration is-
sues concerning the ﬁdelity of packet timestamps and the amount
of measurement loss that our collection apparatus incurred. Addi-
tionally, we develop techniques to “map” the monitored network
in terms of identifying key topological components, such as subnet
boundaries, which hosts were directly monitored, and the presence
of “hidden” switches and hubs. Finally, we present initial analyses
demonstrating that the magnitude and diversity of trafﬁc at the sub-
net level is in fact striking, highlighting the importance of obtaining
and correctly calibrating switch-level enterprise traces.
Categories and Subject Descriptors
C.2.2 [Computer Communication Networks]: Network Proto-
cols
General Terms
Measurement
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’09, November 4–6, 2009, Chicago, Illinois, USA.
Copyright 2009 ACM 978-1-60558-770-7/09/11 ...$10.00.
Keywords
Trace calibration, enterprise networks, network traces, switch-
based packet capture
1.
INTRODUCTION
The network trafﬁc of enterprises can be measured, and thus
characterized, from a number of vantage-point perspectives.
In
the past, a great deal of work has used measurements captured at
an enterprise’s access link, which allows characterization of net-
work activity involving the external Internet, but does not shed any
light on activity that stays conﬁned within the enterprise. More
recently, studies have drawn upon measurements made at an enter-
prise’s core routers [5]. Doing so yields insight into the enterprise’s
broader network dynamics, i.e., how hosts in one subnet communi-
cate with those in another, but does not give any insight into intra-
subnet communication. Alternatively, studies have measured com-
munication on the end-hosts themselves [2]. While this approach
yields information about all of a host’s trafﬁc—including commu-
nication that occurs outside the enterprise in the case of monitoring
on a laptop—the measurements (i) lack a broader context of what
is happening in the surrounding network (e.g., network load) and
(ii) can be difﬁcult to setup and manage.
From the mid-1980s through the mid-1990s, researchers read-
ily measured intra-subnet trafﬁc by leveraging the near-ubiquitous
deployment of 10base2 Ethernet and simple hubs. Due to the bus-
based nature of these networks a single measurement tap could cap-
ture the activity for all of the hosts on the subnet (e.g., [3]). How-
ever, with the rise of 10baseT switched Ethernet, this capability
was lost—there is no longer a single vantage point that can see all
of the trafﬁc transiting a single Ethernet broadcast domain.
An intermediary approach between the recent techniques of mea-
suring at an enterprise’s inter-subnet routers or its individual end
systems is to capture trafﬁc as seen at different Ethernet switches.
Monitoring hardware exists for facilitating such measurement. For
example, the system we used for recording trafﬁc aggregates 5 full-
duplex 100 Mbps Ethernet streams onto a Gbps Ethernet link. (In
fact, it does this for two pairs of such streams, representing 10
switch ports in total. See § 2 for details.) Such a monitor provides
the capability of simultaneously analyzing multiple ports connected
to a given switch. If these ports link directly to end-systems, then
the monitor can capture the same trafﬁc as observable directly on
the end system, but with signiﬁcantly less effort to do so in ag-
143gregate since we can measure multiple end systems concurrently.
On the other hand, by not running directly on the end system we
lose the opportunity to relate a node’s network activity with its
system/process activity [2]. If the ports instead connect to other
switches, then we can obtain a view of more aggregated intra-
subnet trafﬁc.
While such switch monitoring provides a fairly economical
means for measuring intra-subnet enterprise trafﬁc, analyzing data
obtained in this fashion raises a number of subtle issues regard-
ing measurement ﬁdelity. In this paper we assess calibration issues
that arise when doing so, which we explore in the context of a fairly
extensive set of switch traces we gathered from different Ethernet
subnets inside the Lawrence Berkeley National Laboratory (LBL).
Our goal is to establish a foundation for understanding the quality
of, and artifacts present in, these traces, as a ﬁrst step towards then
being able to build up a sound understanding of how intra-subnet
trafﬁc behaves.
A central premise of our work is that measurements such as those
we conducted will see signiﬁcant further employment by others in
the future, raising for others the same issues that we explore in
this paper. We also note that while some of the calibration tech-
niques we present may in retrospect appear straight-forward, our
team—which includes members with extensive experience in mea-
surement and calibration—found the techniques required signiﬁ-
cant investigation to develop. Thus, we believe there is consider-
able contribution in framing the calibration approaches to aid in
future studies based on subnet switch measurements.
Four basic properties of the measurements we wish to calibrate
concern timing, loss, gain, and layout. The ﬁrst two are already
familiar from previous studies of calibrating packet trace measure-
ments, such as [6]: timing refers to the accuracy of the timestamps
associated with the recorded packets, and loss refers to measure-
ment loss, i.e., packets erroneously missing from a trace because
the monitor failed to capture or record them.
By “gain,” we mean instances of the monitor recording packets
that did not exist—or, at least, did not exist as a distinct network
event. These can in principle occur, for example, due to bugs in the
monitor software ([6] discusses a kernel packet ﬁlter that in some
circumstances recorded two copies of each packet). However, in
the context of Ethernet switch measurement we must deal with the
much more common phenomenon of a switch replicating a packet
when forwarding it, and thus if we measure multiple switch ports
concurrently, each port may include an instance of the packet, lead-
ing to multiple copies in the aggregated trace.
We refer to the additional copies of a packet recorded multi-
ple times as phantoms.
In one sense, they do not reﬂect a dis-
tinct network event, because the source originally transmitted only
one instance, not several. In another sense, however, they do re-
ﬂect network events, as their appearance is expected and reﬂects
the switch’s correct functioning.
In general, Ethernet switches can replicate packets for one of
three valid reasons. First, any packet destined for the Ethernet
broadcast address is forwarded to all ports that represent edges of
the Ethernet broadcast spanning tree. For simple topologies (which
includes the LBL enterprise),1 this will nominally mean “all” ports
of the switch other than the one from which the broadcast packet
arrives; for more complex topologies, the switch might replicate to
only a subset of the ports. Here, we put quotes around “all” be-
cause we ﬁnd that the switches we measured sense whether a port
1Note, LBL operators informed us that the switches are not meant
to be running the Spanning Tree Protocol, although we found evi-
dence that in some cases they do.
currently has an active system at the other end of the link, and do
not ﬂood packets to the port if it does not.
Second, a switch might replicate packets sent to Ethernet multi-
cast addresses, depending on its knowledge of the location of lis-
teners for the given address (e.g., IP-level multicast can be corre-
spondingly mapped to Ethernet-level multicast, and some switches
sniff IGMP trafﬁc to prune forwarding for ports without listeners.)
Third, if a switch receives a unicast Ethernet packet, it might
ﬂood it to all switch ports if it does not ﬁnd an entry for the des-
tination MAC address in its forwarding table. In a simple Ether-
net topology we would expect this last phenomenon to occur only
rarely (roughly, no more than once per ﬂow, and perhaps signiﬁ-
cantly less), since any two-way communication should induce the
switch to quickly enter an entry into its forwarding table. It is pos-
sible, however, that unicast ﬂooding might occur more often due
to asymmetric forwarding within the Ethernet subnet, or because
the number of active ﬂows exceeds the size of the forwarding table,
causing the repeated eviction of ﬂow entries.
Thus, in Ethernet switch traces we expect a signiﬁcant propor-
tion of the recorded packets to in fact reﬂect a form of “phantom.”
These replicas represent both a curse and a blessing. The curse is
that for many forms of basic analysis, such as overall trafﬁc mix,
we need to accurately identify their presence lest they unduly skew
our view of the prevalence of particular types of trafﬁc. The bless-
ing, however, is that—as we develop in this paper—they provide a
means by which to calibrate the switch measurements.
The ﬁnal trace property we calibrate concerns “layout,” by which
we mean identifying key topological components of the measure-
ments: (i) which trafﬁc remains in the subnet versus involves com-
munication with external hosts; (ii) accurately determining the
IP subnet associated with the Ethernet broadcast domain; (iii) ﬁnd-
ing which end systems in our traces we directly monitored (i.e., we
captured all of the packets the system generated because its imme-
diate network link was one of those we tapped); and the difﬁcult
problem of (iv) detecting instances of “hidden” switches, meaning
cases where one of the ports monitored in the trace does not in fact
lead directly to an end system but instead to a switch (or hub) that
services multiple end systems.
We proceed as follows. In § 2 we discuss the switch traces used
for the study. We then turn in § 3 to robust identiﬁcation of phan-
toms and a corresponding removal process. In § 4 we leverage the
fact that we collected two simultaneous traces to assess the agree-
ment in the timestamps across each pair of traces. Once we can
soundly spot phantoms and pair traces, we then in § 5 formulate
and analyze different procedures for assessing measurement loss.
In § 6 we develop approaches for calibrating elements of network
“layout,” and in § 7 then analyze the fully calibrated traces to as-
sess the signiﬁcance of intra-subnet monitoring (i.e., the degree to
which trafﬁc from a switch-based vantage point provides insight
beyond that available to monitoring of only inter-subnet trafﬁc).
We conclude with a summary in § 8.
2. ANALYZED TRACES
Working in conjunction with LBL’s networking staff, we cap-
tured the enterprise traces used for this study between October 2005
and March 2006. The intent behind the general approach was
to record full packet payloads from a set of 10 switch ports for
roughly a day, after which the monitoring would move to another
set of 10 ports, either off of the same switch or (if exhausted) a new
switch. In particular, the setup entailed two sets of Finisar Shadow
10/100 taps, each capturing both directions of 5 FastEthernet (cop-
per) links, as illustrated in Figure 1.
144identical ARP packets throughout the traces, and even retransmit-
ted on fairly short timescales (e.g., 1 second). Strictly speaking,
it is impossible from our traces to know whether a given set of
replicated packets reﬂect phantoms or true, separate end-host trans-
missions. However, given our knowledge of the nature of the net-
work’s operation—in particular, switches should replicate broad-
cast packets, should not necessarily replicate unicast packets, and
when replicating should do so quickly—allow us to proceed with
identifying phantoms with high conﬁdence, as follows.
First, we examine the distribution of the time intervals between
instances of identical packets appearing in a given trace, where
identical means yielding the same MD5 hash over the entire packet.
Figure 2 shows three examples of this distribution. We ﬁnd that the
particulars of the distribution vary signiﬁcantly across our traces,
but the overall form always exhibits a strong mode of intervals
≤ 10 µsec (lefthand side of the ﬁgure), another broad mode for
values of roughly 1 sec or higher, and sometimes (as in the ﬁrst two
subﬁgures, but not in the third one) a third mode in the range of
100 µsec to 1 msec.
An initial, erroneous rule. Upon inspecting such distributions,
it is easy at this point to then presume that the ﬁrst mode reﬂects
switch replication, since the very small time intervals correspond
with back-to-back linespeed packets, which is what we would ex-
pect as the result of a switch’s immediate replication of packets
out multiple ports. One can then remove phantoms by eliding any
packets whose contents match those of another packet seen no more
than say 15 µsec in the past. One then interprets the other modes
as representing truly distinct (separately originated) packets.3
Using a 15 µsec rule, however, in fact turns out to be a mistake.
We initially used this deﬁnition, and only when further analyzing
the implications of this approach did we discover the problem is
more complex. When we applied the 15 µsec rule for identifying
phantoms, we found that some traces exhibited frequent patterns of
a set of identical packets being split into two parts. For example, if
the replication size was 4 total copies (3 phantoms), then we would
ﬁnd regions in a trace where each packet was split into a group of
3 identical packets (1 end-host transmission and 2 phantoms) fol-
lowed closely by 1 identical packet that is presumed to be a second
end-host transmission. However, inspecting these incidents then re-
vealed that in fact together the shortfalls arose from a single ﬂight
of 4 copies that had more time between them than 15 µsec.
This indicates that the natural 15 µsec rule is in fact too aggres-
sive. As discussed in § 5 we want to build on the identiﬁcation of
phantoms for calibrating estimates of measurement loss by compar-
ing the number of phantoms we expect to see with the number we
actually observe. It is, therefore, important that we cull all phan-
toms from the traces before we assess measurement loss, or the
phantoms will suggest more loss than actually occurred. For in-
stance, if we expect to see four replicas for each broadcast packet
and can correctly gather the phantoms together we may ﬁnd no
loss, whereas erroneously forming two groups with two packets
each will suggest a measurement loss rate of 50%. On the other
hand, the phenomenon of senders retransmitting identical packets
is an interesting one in terms of understanding network dynamics
3It is illuminating to note that on a 100 Mbps Ethernet the closest
possible separation of minimum-sized packets is about 5 µsec, and
for full-sized packets a bit under 125 µsec. However, we have con-
ﬁrmed that the timestamp differences for such full-sized packets are
generally well under 10 µsec (below even the minimal full-sized
packet spacing for the Gigabit Ethernet aggregation link). This dis-
crepancy indicates that the timing reﬂects the monitoring appara-
tus’s kernel timestamping packets that it retrieves (at a rate much
higher than 100 Mbps) in batches out of a buffer, rather than the
ﬁne-grained spacing on the wire.
Figure 1: Measurement apparatus (courtesy Tom Kho).
All 10 taps were plugged into a second aggregation switch, con-
suming a total of 20 ports, since each tap fed two ports due to the
full-duplex nature of the monitored links. We then aggregated 10 of
those ports on the aggregation switch into a Gbps SPAN port, and
the other 10 into a second Gbps SPAN port. These two Gbps ports
then connected to two NICs on a workstation running tcpdump
which recorded the aggregated trafﬁc (including payloads) to disk.
One minor variation to this approach occurred for the data captured
in October 2005, where, due to cabling problems, the monitoring
employed only 8 taps (2 sets of 4) rather than 10.
As mentioned above, the plan of operation was for a networking
staff member to every day rotate the tapping arrangement to a dif-
ferent set of 8–10 ports, with this sometimes entailing moving the
entire monitor apparatus to a new switch at a new location. (The lo-
cations included several different buildings at the enterprise.) The
intent was for the monitored ports to always be directly connected
to end systems; however, the network operators cautioned us that
this could not necessarily be achieved because they do not always
know when a port plugged into the monitored switch comes from
a privately managed switch or hub rather than an end system. We
revisit this question in § 6.
We collected 51 pairs of traces, i.e., 102 total traces, each half
of a pair reﬂecting the Gbps SPAN feed from the aggregation
switch that covered both directions of 4–5 FastEthernet ports on
the monitored switch. Thus, each trace pair captured 8–10 FastEth-
ernet ports off of a single switch. These traces in aggregate com-
prise 2,228 hours of trafﬁc (individual traces usually running about
23 hours), totaling 869M packets2 and about 400 GB of payload.
Note that certainly a juicy use of these traces would be to charac-
terize the trafﬁc in traditional ways for this little explored network
type (e.g., trafﬁc mix, peak-versus-average load, etc.). However, we
cannot soundly do so until we calibrate the traces using the tech-
niques we develop in this present work. As an exemplar of why the
calibration effort is required, we present a high-level characteriza-
tion in § 7 that illustrates how a switch-level view of the network
can illuminate dramatic new insights about enterprise networks.
3.
IDENTIFYING PHANTOMS
The predominance of phantoms in switch traces becomes clear
upon casual inspection: we immediately see many identical packets
very closely separated in time. We cannot however simply strip out
packets that exactly repeat a previously seen packet, because we do
not want to presume that sources never transmit multiple times sep-
arate instances of identical packets. For instance, we see multiple
2This is the number of packets written to our trace ﬁles. As devel-
oped in subsequent sections, some of these packets are phantoms
that need to be removed before drawing conclusions about the data.
1450
0
0
0
5
2
0
0
0
0
5
1
0
0
0