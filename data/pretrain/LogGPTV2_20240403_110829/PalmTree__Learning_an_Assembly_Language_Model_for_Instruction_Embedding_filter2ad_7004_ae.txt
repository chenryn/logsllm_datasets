the embedding size [19]. We use Coreutils-8.30 as the dataset. It
includes 107 binaries and 1,006,169 instructions. We disassembled
the binaries with Binary Ninja and feed them into the baseline
models. Due to the limitation of GPU memory, we treated 5,000
instructions as a batch.
Table 7: Efficiency of PalmTree and baselines
embedding size
Instruction2vec
word2vec
Asm2Vec
PalmTree-64
PalmTree-128
PalmTree-256
PalmTree-512
encoding time
6.684
0.421
17.250
41.682
70.202
135.233
253.355
throughput (#ins/sec)
150,538
2,386,881
58,328
24,138
14,332
7,440
3,971
Table 7 shows the encoding time and throughput of different
models when encoding the 107 binaries in Coreutils-8.30. From
the results, we can make several observations. First, PalmTree is
much slower than previous embedding approaches such as word2vec
and Asm2Vec. This is expected, since PalmTree has a deep trans-
former network. However, with the acceleration of the GPU, PalmTree
can finish encoding the 107 binaries in about 70 seconds, which
is acceptable. Furthermore, as an instruction level embedding ap-
proach, PalmTree can have an embedding lookup table as well to
store some frequently used embeddings. This lookup table works as
fast as word2vec and can further boost the efficiency of PalmTree.
Last but not least, from the results we observed that it would be 1.7
to 1.9 times slower when doubling the embedding size.
4.6 Hyperparameter Selection
To further study the influences of different hyperparameter configu-
rations of PalmTree, we trained PalmTree with different embedding
sizes (64, 128, 256, and 512) and different context window sizes (1,
2, 3, and 4). We also evaluated different output layer configurations
when generating instruction embeddings. Interested readers are
referred to the Appendix for more details.
5 RELATED WORK
Representation Learning in NLP. Over the past several years, rep-
resentation learning techniques have made significant impacts in
NLP domain. Neural Network Language Model (NNLM) [4] is the
first work that used neural networks to model natural language
and learn distributed representations for words. In 2013, Mikolov et
al. introduced word2vec and proposed Skip-gram and Continuous
Bag-Of-Words (CBOW) models [28]. The limitation of word2vec
is that its embedding is frozen once trained, while words might
have different meanings in different contexts. To address this issue,
Peters et al. introduced ELMo [32], which is a deep bidirectional lan-
guage model. In this model, word embeddings are generated from
the entire input sentence, which means that the embeddings can be
dynamically adjusted according to different contextual information.
In 2017, Vaswani et al. introduced transformer [39] to replace
the RNN networks (e.g., LSTM). Devlin et al. proposed BERT [9] in
2019, which is a bi-directional transformer encoder. They designed
the transformer network using a full connected architecture, so that
the model can leverage both forward and backward information.
Clark et al. [6] proposed ELECTRA and further improved BERT
by using a more sample-efficient pre-training task called Replaced
Token Detection. This task is an adversarial learning process [13].
Representation Learning for Instructions. Programming languages,
including low level assembly instructions, have clear grammar and
syntax, thus can be treated as natural language and be processed
by NLP models.
Instruction representation plays a significant role in binary anal-
ysis tasks. Many techniques have been proposed in previous studies.
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3247Instruction2Vec [41] is a manually designed instruction represen-
tation approach. InnerEye [43] uses Skip-gram, which is one of
the two models of word2vec [28], to encode instructions for code
similarity search. Each instruction is treated as a word while a code
snippet as a document. Massarelli et al. [26] introduced an approach
for function-level representation learning, which also leveraged
word2vec to generate instruction embeddings. DeepBindiff [11]
also used word2vec to generate representations for instructions
with the purpose of matching basic blocks in different binaries.
Unlike InnerEye, they used word2vec to learn token embeddings
and generate instruction embeddings by concatenating vectors of
opcode and operands.
Although word2vec has been widely used in instruction repre-
sentation learning. It has the following shortcommings: first, using
word2vec at the instruction level embedding will lose internal in-
formation of instructions; on the other hand, using word2vec at the
token level may fail to capture instruction level semantics. Second,
the model has to handle the OOV problem. InnerEye [43] and Deep-
Bindiff [11] provided good practices by applying normalization.
However, normalization also results in losing some important infor-
mation. Asm2Vec [10] generates embeddings for instructions and
functions simultaneously by using the PV-DM model [20]. Unlike
previous word2vec based approaches, Asm2Vec exploits a token
level language model for training and did not have the problem
of breaking the boundaries of instructions, which is a problem
of token level word2vec models. Coda [12] is a neural program
decompiler based on a Tree-LSTM autoencoder network. It is an
end-to-end deep learning model which was specifically designed
for decompilation. It cannot generate generic representations for
instructions, thus cannot meet our goals.
Representation Learning for Programming Languages. NLP tech-
niques are also widely used to learn representations for program-
ming languages. Harer et al. [15] used word2vec to generate token
embeddings of C/C++ programs for vulnerability prediction. The
generated embeddings are fed into a TextCNN network for classi-
fication. Li et al. [22] introduced a bug detection technique using
word2vec to learn token (node) embedding from Abstract Syntax
Tree (AST). Ben-Nun et al. [3] introduced a new representation
learning approach for LLVM IR in 2018. They generated conteXtual
Flow Graph (XFG) for this IR, which leverages both data depen-
dency and control flow. Karampatsis et al. [17] proposed a new
method to reduce vocabulary size of huge source code dataset.
They introduced word splitting, subword splitting with Byte Pair
Encoding (BPE) [36] cache, and dynamic adaptation to solve the
OOV problem in source code embedding.
6 DISCUSSION
In this paper, we focus on training an assembly language model
for one instruction set or one architecture. We particularly eval-
uated x86. The technique described here can be applied to other
instruction sets as well, such as ARM and MIPS.
However, in this paper, we do not intend to learn a language
model across multiple CPU architectures. Cross-architecture means
that semantically similar instructions from different architectures
can be mapped to near regions in the embedded space. Cross-
architecture assembly language model can be very useful for cross-
architecture vulnerability/bug search. We leave it as a future work.
It is worth noting that instead of feeding a pair of instructions
into PalmTree, we can also feed code segment pairs or even ba-
sic block and function pairs, which may better capture long-term
relations between instructions (currently we use sampling in the
context window and data flow graph to capture long-term rela-
tions) and has a potential to further improve the performance of
PalmTree. We leave this as a future work.
7 CONCLUSION
In this paper, we have summarized the unsolved problems and
existing challenges in instruction representation learning. To solve
the existing problems and capture the underlying characteristics
of instruction, we have proposed a pre-trained assembly language
model called PalmTree for generating general-purpose instruction
embeddings.
PalmTree can be pre-trained by performing self-supervised train-
ing on large-scale unlabeled binary corpora. PalmTree is based on
the BERT model but pre-trained with newly designed training tasks
exploiting the inherent characteristics of assembly language. More
specifically, we have used the following three pre-training tasks to
train PalmTree: MLM (Masked Language Model), CWP (Context
Window Prediction), and DUP (Def-Use Prediction). We have de-
signed a set of intrinsic and extrinsic evaluations to systematically
evaluate PalmTree and other instruction embedding models. Ex-
perimental results show that PalmTree has the best performance
in intrinsic evaluations compared with the existing models. In ex-
trinsic evaluations that involve several downstream applications,
PalmTree outperforms all the baseline models and also significantly
improves downstream applications’ performance. We conclude that
PalmTree can effectively generate high-quality instruction embed-
ding which is helpful for different downstream binary analysis
tasks.
8 ACKNOWLEDGEMENT
We would like to thank the anonymous reviewers for their helpful
and constructive comments. This work was supported in part by
National Science Foundation under grant No. 1719175, and Office of
Naval Research under Award No. N00014-17-1-2893. Any opinions,
findings, and conclusions or recommendations expressed in this
paper are those of the authors and do not necessarily reflect the
views of the funding agencies.
REFERENCES
[1] Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.
A survey of machine learning for big code and naturalness. ACM Computing
Surveys (CSUR) 51, 4 (2018), 1–37.
[2] Amir Bakarov. 2018. A Survey of Word Embeddings Evaluation Methods. CoRR
abs/1801.09536 (2018). arXiv:1801.09536 http://arxiv.org/abs/1801.09536
[3] Tal Ben-Nun, Alice Shoshana Jakobovits, and Torsten Hoefler. 2018. Neural code
comprehension: a learnable representation of code semantics. In Proceedings
of the 32nd International Conference on Neural Information Processing Systems.
3589–3601.
[4] Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. A
neural probabilistic language model. Journal of machine learning research 3, Feb
(2003), 1137–1155.
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3248[5] Zheng Leong Chua, Shiqi Shen, Prateek Saxena, and Zhenkai Liang. 2017. Neural
nets can learn function type signatures from binaries. In 26th {USENIX} Security
Symposium ({USENIX} Security 17). 99–116.
[6] Kevin Clark, Minh-Thang Luong, Quoc V Le, and Christopher D Manning. 2019.
ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators.
In International Conference on Learning Representations.
[7] Hanjun Dai, Bo Dai, and Le Song. 2016. Discriminative Embeddings of Latent
Variable Models for Structured Data. In Proceedings of the 33rd International
Conference on International Conference on Machine Learning - Volume 48 (New
York, NY, USA) (ICML’16). JMLR.org, 2702–2711.
[8] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G Carbonell, Quoc Le, and Ruslan
Salakhutdinov. 2019. Transformer-XL: Attentive Language Models beyond a
Fixed-Length Context. In Proceedings of the 57th Annual Meeting of the Association
for Computational Linguistics. 2978–2988.
[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT:
Pre-training of Deep Bidirectional Transformers for Language Understanding. In
Proceedings of the 2019 Conference of the North American Chapter of the Association
for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
Short Papers). 4171–4186.
[10] Steven HH Ding, Benjamin CM Fung, and Philippe Charland. 2019. Asm2vec:
Boosting static representation robustness for binary clone search against code
obfuscation and compiler optimization. In 2019 IEEE Symposium on Security and
Privacy (SP). IEEE, 472–489.
[11] Yue Duan, Xuezixiang Li, Jinghan Wang, and Heng Yin. 2020. DEEPBINDIFF:
Learning Program-Wide Code Representations for Binary Diffing. NDSS (2020).
[12] Cheng Fu, Huili Chen, Haolan Liu, Xinyun Chen, Yuandong Tian, Farinaz
Koushanfar, and Jishen Zhao. 2019. Coda: An end-to-end neural program decom-
piler. In Advances in Neural Information Processing Systems. 3703–3714.
[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014. Generative adversarial
nets. In Advances in neural information processing systems. 2672–2680.
[14] Wenbo Guo, Dongliang Mu, Xinyu Xing, Min Du, and Dawn Song. 2019.
{DEEPVSA}: Facilitating Value-set Analysis with Deep Learning for Postmortem
Program Analysis. In 28th {USENIX} Security Symposium ({USENIX} Security
19). 1787–1804.
[15] Jacob A Harer, Louis Y Kim, Rebecca L Russell, Onur Ozdemir, Leonard R Kosta,
Akshay Rangamani, Lei H Hamilton, Gabriel I Centeno, Jonathan R Key, Paul M
Ellingwood, et al. 2018. Automated software vulnerability detection with machine
learning. arXiv preprint arXiv:1803.04497 (2018).
[16] Abram Hindle, Earl T Barr, Zhendong Su, Mark Gabel, and Premkumar Devanbu.
2012. On the naturalness of software. In 2012 34th International Conference on
Software Engineering (ICSE). IEEE, 837–847.
[17] Rafael-Michael Karampatsis, Hlib Babii, Romain Robbes, Charles Sutton, and An-
drea Janes. 2020. Big code!= big vocabulary: Open-vocabulary models for source
code. In 2020 IEEE/ACM 42nd International Conference on Software Engineering
(ICSE). IEEE, 1073–1085.
[18] Ryan Kiros, Yukun Zhu, Russ R Salakhutdinov, Richard Zemel, Raquel Urtasun,
Antonio Torralba, and Sanja Fidler. 2015. Skip-thought vectors. Advances in
neural information processing systems 28 (2015), 3294–3302.
[19] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush
Sharma, and Radu Soricut. 2020. ALBERT: A Lite BERT for Self-supervised
Learning of Language Representations. In International Conference on Learning
Representations.
[20] Quoc Le and Tomas Mikolov. 2014. Distributed representations of sentences and
documents. In International conference on machine learning. 1188–1196.
[21] Yujia Li, Chenjie Gu, Thomas Dullien, Oriol Vinyals, and Pushmeet Kohli. 2019.
Graph Matching Networks for Learning the Similarity of Graph Structured Ob-
jects. In Proceedings of the 36th International Conference on Machine Learning,
Vol. 97. 3835–3845.
[22] Yi Li, Shaohua Wang, Tien N Nguyen, and Son Van Nguyen. 2019. Improving bug
detection via context-based code representation learning and attention-based
neural networks. Proceedings of the ACM on Programming Languages 3, OOPSLA
(2019), 1–30.
[23] Bingchang Liu, Wei Huo, Chao Zhang, Wenchao Li, Feng Li, Aihua Piao, and Wei
Zou. 2018. αDiff: Cross-version Binary Code Similarity Detection with DNN. In
Proceedings of the 33rd ACM/IEEE International Conference on Automated Software
Engineering (ASE 2018).
[24] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer
Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A
robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692
(2019).
[25] Farhana Ferdousi Liza and Marek Grześ. 2016. An improved crowdsourcing
based evaluation technique for word embedding methods. In Proceedings of the
1st Workshop on Evaluating Vector-Space Representations for NLP. 55–61.
[26] Luca Massarelli, Giuseppe Antonio Di Luna, Fabio Petroni, Roberto Baldoni, and
Leonardo Querzoni. 2019. Safe: Self-attentive function embeddings for binary
similarity. In International Conference on Detection of Intrusions and Malware, and
Vulnerability Assessment. Springer, 309–329.
[27] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
estimation of word representations in vector space. arXiv preprint arXiv:1301.3781
(2013).
[28] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013.
Distributed representations of words and phrases and their compositionality. In
Advances in neural information processing systems. 3111–3119.
[29] ORACLE. 2019. x86 Assembly Language Reference Manual. https://docs.oracle.
com/cd/E26502_01/html/E28388/ennbz.html.
[30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory
Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al.
2019. Pytorch: An imperative style, high-performance deep learning library. In
Advances in neural information processing systems. 8026–8037.
[31] Kexin Pei, Zhou Xuan, Junfeng Yang, Suman Jana, and Baishakhi Ray. 2020. TREX:
Learning Execution Semantics from Micro-Traces for Binary Similarity. arXiv
preprint arXiv:2012.08680 (2020).
[32] Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher
Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word
representations. In Proceedings of NAACL-HLT. 2227–2237.
[33] Xipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao, Ning Dai, and Xuanjing Huang.
2020. Pre-trained models for natural language processing: A survey. Science
China Technological Sciences 63, 10, 1872–1897. https://doi.org/10.1007/s11431-
020-1647-3
[34] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever.
2018.
Improving language understanding by generative pre-training
(2018). URL http://openai-assets.s3.amazonaws.com/research-covers/language-
unsupervised/language_understanding_paper.pdf (2018).
[35] Edward Raff, Jon Barker, Jared Sylvester, Robert Brandon, Bryan Catanzaro, and
Charles Nicholas. 2018. Malware Detection by Eating a Whole EXE. In AAAI-2018
Workshop on Artificial Intelligence for Cyber Security.
[36] Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural Machine
Translation of Rare Words with Subword Units. In Proceedings of the 54th Annual
Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).
1715–1725.
[37] Eui Chul Richard Shin, Dawn Song, and Reza Moazzezi. 2015. Recognizing
functions in binaries with neural networks. In 24th {USENIX} Security Symposium
({USENIX} Security 15). 611–626.
[38] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning
with neural networks. Advances in neural information processing systems 27 (2014),
3104–3112.
[39] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in neural information processing systems. 5998–6008.
[40] Xiaojun Xu, Chang Liu, Qian Feng, Heng Yin, Le Song, and Dawn Song. 2017.
Neural network-based graph embedding for cross-platform binary code similarity
detection. In Proceedings of the 2017 ACM SIGSAC Conference on Computer and
Communications Security. 363–376.