### 4.5 Efficiency of PalmTree and Baseline Models

We use the Coreutils-8.30 dataset, which includes 107 binaries and 1,006,169 instructions. The binaries were disassembled using Binary Ninja, and the resulting instructions were fed into the baseline models. Due to GPU memory limitations, we processed 5,000 instructions per batch.

**Table 7: Efficiency of PalmTree and Baselines**

| Embedding Size | Instruction2vec | word2vec | Asm2Vec | PalmTree-64 | PalmTree-128 | PalmTree-256 | PalmTree-512 |
|----------------|-----------------|----------|---------|-------------|--------------|--------------|--------------|
| Encoding Time (s) | 6.684 | 0.421 | 17.250 | 41.682 | 70.202 | 135.233 | 253.355 |
| Throughput (#ins/sec) | 150,538 | 2,386,881 | 58,328 | 24,138 | 14,332 | 7,440 | 3,971 |

Table 7 shows the encoding time and throughput of different models when encoding the 107 binaries in Coreutils-8.30. Several observations can be made from these results:

1. **PalmTree's Performance**: PalmTree is significantly slower than previous embedding approaches such as word2vec and Asm2Vec. This is expected due to its deep transformer network. However, with GPU acceleration, PalmTree can encode the 107 binaries in about 70 seconds, which is acceptable.
   
2. **Embedding Lookup Table**: As an instruction-level embedding approach, PalmTree can utilize an embedding lookup table to store frequently used embeddings. This lookup table can operate as fast as word2vec and further enhance PalmTree's efficiency.

3. **Impact of Embedding Size**: Doubling the embedding size results in a 1.7 to 1.9 times increase in encoding time.

### 4.6 Hyperparameter Selection

To further study the impact of different hyperparameter configurations on PalmTree, we trained it with various embedding sizes (64, 128, 256, and 512) and context window sizes (1, 2, 3, and 4). We also evaluated different output layer configurations for generating instruction embeddings. Detailed results are provided in the Appendix.

### 5. Related Work

#### Representation Learning in NLP

Over the past several years, representation learning techniques have significantly impacted the NLP domain. The Neural Network Language Model (NNLM) [4] was the first to use neural networks to model natural language and learn distributed representations for words. In 2013, Mikolov et al. introduced word2vec [28], proposing Skip-gram and Continuous Bag-of-Words (CBOW) models. A limitation of word2vec is that its embeddings are static once trained, failing to capture contextual variations. To address this, Peters et al. introduced ELMo [32], a deep bidirectional language model that generates word embeddings based on the entire input sentence, allowing for dynamic adjustments according to context. In 2017, Vaswani et al. introduced the transformer [39], replacing RNNs like LSTM. Devlin et al. proposed BERT [9] in 2019, a bi-directional transformer encoder designed to leverage both forward and backward information. Clark et al. [6] further improved BERT with ELECTRA, using a more sample-efficient pre-training task called Replaced Token Detection, which is an adversarial learning process [13].

#### Representation Learning for Instructions

Programming languages, including low-level assembly instructions, have clear grammar and syntax, making them amenable to NLP models. Instruction representation is crucial for binary analysis tasks. Various techniques have been proposed, such as Instruction2Vec [41], which is a manually designed instruction representation approach. InnerEye [43] uses Skip-gram (one of the two models of word2vec [28]) to encode instructions for code similarity search, treating each instruction as a word and a code snippet as a document. Massarelli et al. [26] introduced a function-level representation learning approach using word2vec to generate instruction embeddings. DeepBindiff [11] also uses word2vec to generate representations for instructions, aiming to match basic blocks in different binaries. Unlike InnerEye, DeepBindiff learns token embeddings and generates instruction embeddings by concatenating vectors of opcodes and operands.

Despite the widespread use of word2vec in instruction representation learning, it has several shortcomings. First, using word2vec at the instruction level loses internal information, while using it at the token level may fail to capture instruction-level semantics. Second, it must handle the out-of-vocabulary (OOV) problem. Normalization, as applied in InnerEye [43] and DeepBindiff [11], helps but also results in the loss of important information. Asm2Vec [10] generates embeddings for instructions and functions simultaneously using the PV-DM model [20]. Unlike word2vec-based approaches, Asm2Vec uses a token-level language model for training, avoiding the problem of breaking instruction boundaries. Coda [12] is a neural program decompiler based on a Tree-LSTM autoencoder network, specifically designed for decompilation. It cannot generate generic representations for instructions, thus not meeting our goals.

#### Representation Learning for Programming Languages

NLP techniques are widely used to learn representations for programming languages. Harer et al. [15] used word2vec to generate token embeddings of C/C++ programs for vulnerability prediction, feeding the embeddings into a TextCNN network for classification. Li et al. [22] introduced a bug detection technique using word2vec to learn token (node) embeddings from Abstract Syntax Trees (ASTs). Ben-Nun et al. [3] introduced a new representation learning approach for LLVM IR in 2018, generating Contextual Flow Graphs (XFGs) that leverage both data dependency and control flow. Karampatsis et al. [17] proposed a method to reduce the vocabulary size of large source code datasets, introducing word splitting, subword splitting with Byte Pair Encoding (BPE) [36], and dynamic adaptation to solve the OOV problem in source code embedding.

### 6. Discussion

In this paper, we focus on training an assembly language model for one instruction set or architecture, specifically evaluating x86. The techniques described here can be applied to other instruction sets, such as ARM and MIPS. However, we do not intend to learn a language model across multiple CPU architectures, which would map semantically similar instructions from different architectures to near regions in the embedded space. Such a cross-architecture model could be useful for cross-architecture vulnerability/bug search, and we leave this as future work.

It is worth noting that instead of feeding pairs of instructions into PalmTree, we can feed code segment pairs or even basic block and function pairs, potentially better capturing long-term relations between instructions. Currently, we use sampling in the context window and data flow graph to capture long-term relations. This approach has the potential to further improve PalmTree's performance and is another area for future work.

### 7. Conclusion

In this paper, we summarized the unsolved problems and existing challenges in instruction representation learning. To address these issues and capture the underlying characteristics of instructions, we proposed a pre-trained assembly language model called PalmTree for generating general-purpose instruction embeddings. PalmTree can be pre-trained through self-supervised training on large-scale unlabeled binary corpora. It is based on the BERT model but pre-trained with newly designed training tasks exploiting the inherent characteristics of assembly language. Specifically, we used three pre-training tasks: Masked Language Model (MLM), Context Window Prediction (CWP), and Def-Use Prediction (DUP).

We designed a set of intrinsic and extrinsic evaluations to systematically assess PalmTree and other instruction embedding models. Experimental results show that PalmTree outperforms existing models in intrinsic evaluations and significantly improves the performance of downstream applications in extrinsic evaluations. We conclude that PalmTree effectively generates high-quality instruction embeddings, beneficial for various downstream binary analysis tasks.

### 8. Acknowledgements

We would like to thank the anonymous reviewers for their helpful and constructive comments. This work was supported in part by the National Science Foundation under grant No. 1719175 and the Office of Naval Research under Award No. N00014-17-1-2893. Any opinions, findings, and conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.

### References

[References are listed as in the original text, with minor formatting adjustments for clarity.]

This version of the text is more organized, coherent, and professional, with clearer section headings and improved readability.