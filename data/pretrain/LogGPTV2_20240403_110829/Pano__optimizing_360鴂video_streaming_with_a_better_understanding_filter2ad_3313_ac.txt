40
60
80
Estimation error of MOS (%)
Figure 8: 360JND-based PSPNR can estimate MOS much more accu-
rately than the traditional PSPNR and PSNR.
(a) Step 1: 
Split to fine-
grained tiles
(cid:1)
(cid:2)
(cid:2)
(cid:1)
(cid:1)
(cid:3)
(cid:3)
(cid:1)
(cid:1)
(cid:2)
(cid:2)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:2)
(cid:1)
(cid:2)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:3)
(cid:1)
(cid:3)
(cid:1)
(cid:1)
(b) Step 2: 
Calculate per-tile 
efficiency score
(cid:1)
(cid:2)
(cid:2)
(cid:1)
(cid:1)
(cid:3)
(cid:3)
(cid:1)
(cid:1)
(cid:2)
(cid:2)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:2)
(cid:1)
(cid:2)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:1)
(cid:3)
(cid:1)
(cid:3)
(cid:1)
(cid:1)
(c) Step 3: 
Group tiles w/ 
similar scores
(d) Reference: 
Traditional 
tiling
Putting it together: Now, we define a new way of calculating
JND for 360° videos, called 360JND, as follows:
J N Di, j = Ci, j · Fv(x1) ·F d(x2) ·F l(x3) (cid:2) Ci, j · A(x1, x2, x3)
(4)
In other words, 360JND is the product of the content-dependent JND,
and the action-dependent ratio A(x1, x2, x3), which is the product of
the viewpoint-speed multiplier, luminance-change multiplier, and
DoF-difference multiplier. As we will see in §6.2, this separation
has a great implication that the content-dependent JND can be pre-
calculated, whereas the action-dependent ratio can be determined
only in realtime, without the help from the server.
Validation of usefulness: To verify the usefulness of the new
360JND model, we plug the 360JND in the PSPNR calculation in
Equation 1-3, and then check how well the resulting PSPNR value
correlates with the actual user rating (MOS) from 20 participants
over 21 360° videos. (See §8.1 for more details on how user rating
is recorded.) For each video, we calculate the average 360JND-
based PSPNR across users as well as the MOS. Then, we build
a linear predictor that estimates MOS based on average PSPNR.
As reference points, we similarly build a linear predictor using
traditional JND-based PSPNR and a predictor using PSNR (JND-
agnostic). Figure 8 shows the distribution of relative estimation
|MO Spr ed ic t −MO Sr e al |
errors of the three predictors (
). We see that
360JND-based PSPNR can predict MOS much more accurately than
the alternatives, which suggests the three 360° video-specific factors
have a strong influence on 360° video perceived quality.
MO Sr e al
5 PANO: VIDEO TILING
Next, we describe Pano’s tiling scheme, which leverages the quality
metric introduced in §4. Like other DASH-based videos, Pano first
chops a 360° video into chunks of equal length (e.g., one second),
and then spatially splits each chunk into tiles by the following steps
(as illustrated in Figure 9).
Step 1: Chunking and fine-grained tiling. Pano begins by split-
ting each chunk into fine-grained square-shape unit tiles with a
12-by-24 grid. Each unit tile is a video clip containing all content
within the square-shape area in the chunk’s duration. These unit
tiles are the building blocks which Pano then groups into coarser-
grained tiles as follows.
Step 2: Calculating per-tile efficiency scores. Then Pano cal-
culates an efficiency score for each unit tile, which is defined by how
fast the tile’s quality grows with the quality level. Formally, the
efficiency score of unit tile t of chunk k is
γk,t =
Pk,t(qhiдh) − Pk,t(ql ow)
qhiдh − ql ow
(5)
399
Figure 9: The steps of Pano tiling. The shades indicate regions with
similar efficiency score.
where Pk,t(q) is the PSPNR (perceived quality calculated by Equa-
tion 1) of the unit tile when it is encoded at quality level q; and ql ow
(and qhiдh ) denotes the lowest (and highest) quality level. There
are two caveats. First, we assume the PSPNR of a unit tile is known.
We will explain how to estimate them offline at the end of this
section. Second, Equation 5 assumes that P grows linearly with
q. This may not be true, but we found this assumption is a good
approximation, and our solution does not crucially rely on it. We
leave further refinements for future work.
Step 3: Tile grouping. Finally, Pano groups the 12×24 unit tiles
into N (by default, 30) variable-size coarse-grained rectangle tiles,
which will eventually be used by Pano to encode the video. The
goal of this grouping process is to reduce the variance of efficiency
scores among the unit tiles in the same group (coarse-grained tile).
More specifically, we try to minimize the weighted sum of these
variances, where each variance is weighted by the area of the group.
The intuition is that, because a higher/lower efficiency score means
a tile will produce higher/lower PSPNR at the same quality level,
the tiles with similar efficiency scores tend to be assigned with
similar quality levels during playback, so grouping these unit tiles
will have limited impact on quality adaptation. At the same time,
having fewer tiles can significantly reduce the video size, as it avoids
re-encoding the boundaries between small tiles.
Our grouping algorithm starts with one hypothetical rectan-
gle that includes all 12×24 unit tiles (i.e., the whole 360° video). It
then uses a top-down process to enumerate many possible ways
of partitioning this hypothetical rectangle into N rectangles, each
representing a coarse-grained tile. It begins by splitting this hypo-
thetical rectangle into two rectangles along each possible vertical
or horizontal boundary. Then it iteratively picks one of the exist-
ing rectangles that has more than one unit tile, and then similarly
splits it, vertically or horizontally, into two rectangle tiles. This pro-
cess runs repeatedly until there are N rectangles (coarse-grained
tiles). This process is similar to how the classic 2-D clustering algo-
rithm [24] enumerates the possible partitions of a 2D space.
Calculating efficiency scores offline: We assume each video
has some history viewpoint trajectories, like in [61, 68]. For each
tile, we compute the PSPNR under each history viewpoint trajectory,
average the PSPNRs per tile across all trajectories, and derive the
efficiency score per tile using Equation 5. The resulting PSPNR per
tile takes both content information and viewpoint movements into
account. Once the tiles are determined offline, Pano does not adjust
them during playback, so the video does not need to be re-encoded.
We acknowledge that computing PSPNR with the average history
Pano: Optimizing 360° Video Streaming with a Better
Understanding of Quality Perception
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Predicted speed
Real speed
Video 
content
Action 
prediction
Video 
content
Manifest 
file
PSPNR 
(cid:3)(cid:5)(cid:6)
lookup table
(cid:2)(cid:4)(cid:2)(cid:1)(cid:3)
(cid:7)(cid:7)(cid:7)
(cid:7)(cid:7)(cid:7)
Action 
prediction
Server
Client
PSPNR estimates
(a) PSPNR calculation
Online PSPNR estimates
(b) Decoupled to two phases
Figure 11: (a) Pano calculates PSPNR by first pre-processing content-
dependent information offline, and then combining it with online
viewpoint predictions by the client. (b) The offline content-dependent
information (represented by PSPNR lookup table) is included in the
manifest file sent to the client at the beginning of a video.
viewpoint-moving speed, DoF, and luminance. On the other hand,
if the viewpoint moves arbitrarily, it is difficult to predict the exact
viewpoint-moving speed, DoF, and luminance, but it is still plausi-
ble to estimate a lower bound for each factor using recent history.
For instance, the lowest speed in the last two seconds serves a reli-
able conservative estimator of the speed in the next few seconds
(Figure 10). Although these lower bounds would lead Pano to make
conservative decisions (e.g., assigning a higher-than-necessary qual-
ity), these conservative decisions still bring sizable improvement
over the baselines which completely ignore the impact of viewpoint-
moving speed, DoF, and luminance.
6.2 DASH-compatible design
While the logical workflow of Pano is straightforward, it is incom-
patible with the popular DASH protocol [4]. This is because Pano’s
quality adaptation is based on PSPNR (Equation 1), which requires
both viewpoint movements (only available on the client) and the
pixels of the video content (only available on the server). This,
however, violates the key tenet of the popular DASH protocol that
servers must be passive while clients adapt bitrate locally without
aid of the server.
Fortunately, Pano can be implemented in a way that is com-
patible with the DASH protocol. The basic idea is to decouple the
calculation of PSPNR into two phases (as illustrated in Figure 11). In
the offline phase, the video provider pre-calculates the PSPNR for
some “representative” viewpoint movements and stores them in a
PSPNR lookup table. In particular, we choose n representative values
for each of the viewpoint speed, DoF difference and luminance
change, which produces n3 combinations and the corresponding
PSPNR values in the lookup table. The PSPNR lookup table is sent
to the client as part of the DASH manifest file at the beginning of a
video. In the online phase, the client uses the PSPNR lookup table
to estimate PSPNR under the actual viewpoint movement.
6.3 System optimization
Compressing PSPNR lookup table: A PSPNR lookup table (see
Figure 12(a) for an example) includes, for each tile, the PSPNR esti-
mates of every possible combination of viewpoint-moving speed,
luminance change, and DoF difference. Without compression, the
PSPNR lookup table can be 10 MB for a 5-minute video, which can
significantly inflate the manifest file size. We address this prob-
lem by two techniques, which produce an approximate yet more
i
g
n
v
o
m
-
t
i
n
o
p
w
e
V
i
60
40
20
)
s
/
g
e
d
(
d
e
e
p
s
0
0
50
100
Time (s)
150
Figure 10: Pano can reliably estimate a lower bound (dotted line) of
the actual viewpoint-moving speed (solid line), which is often sufficient
for accurate PSPNR estimation.
viewpoint movements might cause suboptimal quality for users
with atypical viewing behaviors. That said, we found that the lowest
perceived quality across the users in our traces is at most 10% worse
than the mean quality (Figure 16(b)).
6 PANO: QUALITY ADAPTATION
The design of Pano’s quality adaptation logic addresses two follow-
ing questions. (1) How to adapt quality in the presence of noisy
viewpoint estimates (§6.1)? And (2) how to be deployable on the
existing DASH protocol (§6.2)?
6.1 Robust quality adaptation
Pano adapts quality at both the chunk level and the tile level. First,
Pano uses MPC [64] to determine the bitrate of each chunk, to meet
buffer length target under the predicted bandwidth. The chunk’s
bitrate determines the total size of all tiles in the chunk.
Then, within the chunk k, Pano determines the quality level qt
of each tile t ∈ {1, . . . , N} (N is the number of tiles per chunk), to
maximize the overall perceived quality (PSPNR) while maintaining
total size of the tiles below the chunk’s bitrate rk . According to
where M = (
Equation 1, the overall PSPNR of the N tiles is P = 20 × log10
255√
M ,
t =1, ..., N St), and St is the
area size of tile t. Since the total area of all tiles is constant, the

tile-level quality allocation can be formulated as follows:

t =1, ..., N St · Mt(qt))/(
/* Maximizing overall PSPNR */
St · Mt(qt)
t =1, ..., N
Rk,t(qt) ≤ rk
/* Total tile size ≤ chunk bitrate */
min
s.t.
t =1, ..., N
N
To solve this optimization problem, we enumerate the possible
assignment of 5 quality levels in each of the N tiles, but instead of
an exhaustive search (which has 5
outcomes), Pano prunes the
search space using the following observation. For any pair of tiles
(t1 and t2), if we found one quality assignment (e.g., assigning q1 to
t1 and q2 to t2) is “strictly” better (i.e., producing higher PSPNR and
smaller total tile size) than another assignment (e.g., assigning q3
to t1 and q4 to t2), then we can safely exclude the latter assignment
when iterating the quality assignments of the remaining tiles.
Coping with viewpoint estimation errors: In theory, optimal
quality adaptation requires accurate PSPNR estimation, which re-
lies heavily on accurate estimation of viewpoint-moving speeds,
DoF differences, and luminance changes. In practice, however, we
found that predicting an approximate range of these three fac-
tors is sufficient to inform optimal quality selection. The reason
is two-fold. On one hand, if the head has little or slow movement
(e.g., staring at an object), it is trivial to accurately predict the
400
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Y. Guan, C. Zheng, X. Zhang, Z. Guo, J. Jiang.
Tile ID
Chunk #1, Tile #1
…
Estimated
PSPNR
50dB
…
Tile ID
Estimated PSPNR
Chunk #1, Tile #1
50dB
…
…
(b) Schema of PSPNR lookup table with dimensionality reduction
…
…
…
(a) Schema of the original full-size PSPNR lookup table
speed
DoF
difference
change
(cid:10)(cid:14)
…
Viewpoint
Luminance
(cid:10)(cid:15)
(cid:10)(cid:13)
(cid:4)(cid:10)(cid:13)(cid:2)(cid:10)(cid:14)(cid:2)(cid:10)(cid:15) (cid:20)(cid:5)(cid:18)(cid:21)(cid:10)(cid:13)(cid:22)(cid:5)(cid:16)(cid:21)(cid:10)(cid:14)(cid:22)(cid:5)(cid:17)(cid:21)(cid:10)(cid:15)(cid:22)
(cid:7)(cid:9)(cid:7)(cid:6)(cid:8)(cid:20)(cid:1)(cid:11)(cid:3)(cid:4)(cid:10)(cid:13)(cid:2)(cid:10)(cid:14)(cid:2)(cid:10)(cid:15) (cid:19)
Action-dependent ratio
Tile ID
Parameters of power regression
Chunk #1, Tile #1
…