Guruswami-Sudan algorithm [22] improves the decod-
kt. This is known as list decoding:
ing radius to v < k−√
the algorithm returns a list of all valid codewords.
2.3.1 Multi-polynomial reconstruction
The above decoding algorithms all consider the case of
noisy interpolation of a single polynomial. More re-
cently, Parvaresh and Vardy [31], and Guruswami and
Rudra [21] designed codes that could be efﬁciently list
decoded, approaching the asymptotic limit of v < k −
t − 1. These codes are based around the idea of extend-
ing the Reed-Solomon code to evaluate multiple polyno-
mials simultaneously, and using clever constructions of
the polynomials in order to efﬁciently decode the code-
words. One of the main contributions of this paper is
to adapt these ideas to a cryptographic setting. We can-
not directly use their constructions, as their polynomials
have a special structure that would make them unsuit-
able for secret sharing. However, using a randomized
construction we can nonetheless efﬁciently decode such
multi-polynomial codes in practice with high probability,
yielding a secret sharing system robust to many errors.
The codes that we will use will reconstruct sev-
eral polynomials simultaneously from noisy evaluation
points. Deﬁne m polynomials
f1(x) = a10 + a11x +··· + a1txt ,
...
fm(x) = am0 + am1x +··· + amtxt .
Then a codeword will consist of the evaluations of each
of these polynomials at points α1, . . . ,αk:
f1(α1), . . . , fm(α1),
...
f1(αk), . . . , fm(αk)
is
case
This general
considered by Cohn and
Heninger [12], who give an algorithm that heuristi-
cally reconstructs every polynomial as long as there are
no more than v < k − tm/(m+1)k1/(m+1) values of i for
which the received value of some fp(αi) is incorrect. In
our application to PIR, each polynomial will correspond
to a column of the database matrix D, and each value αi
will correspond to a PIR server; therefore, we will be
able to tolerate v dishonest servers.
2.3.2 Linear multi-polynomial decoding
The list-decoding algorithms of Guruswami-Sudan,
Parvaresh-Vardy, and Cohn-Heninger all work by con-
structing a polynomial which vanishes to high multiplic-
ity at the codeword. If one simply uses multiplicity one,
one can obtain a “linear” variant of the Cohn-Heninger
algorithm [12] which is extremely fast in practice. It re-
constructs each polynomial uniquely when no more than
v ≤ m
m+1 (k−t − 1) values of i have incorrect received
values of some fp(αi). This algorithm works with high
probability in practice as long as the errors are random-
ized. We will show later how to set up our protocol to
enforce that even malicious servers can only insert ran-
dom errors.
Since this linear variant is not explicitly described in
their work, we provide a brief outline in Algorithm 1.
Polynomial lattice basis reduction. Step 4 in the al-
gorithm uses a “polynomial lattice basis row reduction
algorithm”. This is an algorithm which takes as input
a matrix M of polynomials and applies elementary row
operations over the ring of polynomials to produce a ma-
trix M(cid:48) whose coefﬁcient polynomials have minimal de-
gree. [37] There are several polynomial-time polynomial
lattice basis reduction algorithms. (This is a refreshing
contrast to the case of integer lattices where ﬁnding ex-
act shortest vectors is NP-hard and efﬁcient algorithms
Algorithm 1 Fast multi-polynomial reconstruction
Input: km points (αi,yip) 1 ≤ i ≤ k, 1 ≤ p ≤ m, degree
bound t, and minimum number of correct points h =
k− v.
Output: m polynomials f1, . . . , fm of degree at most t
such that for at least h values of i, fp(αi) = yip for
all 1 ≤ p ≤ m
mials f ∗
each 1 ≤ i ≤ k.
1: Use Lagrange interpolation to construct m polyno-
p (αi) = yip for
p of degree at most k− 1 s.t. f ∗
2: Construct the degree-k polynomial
N(x) =
(x− αi)
k
∏
i=1
3: Construct the (m + 1)× (m + 1) polynomial matrix
xt
M =
xt
...
− f ∗
− f ∗
1 (x)
2 (x)
xt − f ∗
m(x)
N(x)
4: Run a polynomial lattice basis row reduction algo-
rithm on M.
5: Discard the largest-degree row in the reduced matrix.
If any remaining row has degree larger than h, abort.
6: Write the remaining m × (m + 1) matrix as [A|b],
where A is an m× m matrix, and b is an m× 1 col-
umn vector.
7: Solve the linear system of equations
(cid:19)
(cid:18) 1
xt A
 = b
f1
f2
...
fm
8: return ( f1, . . . , fm)
such as LLL [24] can only obtain an exponential approx-
imation.) The algorithm of Giorgi et al. [17] runs in time
O(δ nω+o(1)) where δ is the maximum degree of the in-
put basis, n is the dimension, and ω is the exponent of
matrix multiplication. Our implementation uses the al-
gorithm of Mulders and Storjohann [27] which runs in
time O(n3δ 2) but is much simpler and easier to imple-
ment, and yields excellent running times for the input
sizes we care about.
If step 5 does not abort, then there is guaranteed to be
a unique set of polynomials satisfying the requirements;
that is, we are in the unique decoding case. As we will
see later, if the errors are random, this step aborts only
with very low probability.
Without any imposed structure on the codeword poly-
nomials, the Cohn-Heninger algorithm is heuristic; that
is, they conjecture that it will succeed for sufﬁciently
random input. The linear version that we use here is
also heuristic:
there are adversarial inputs on which it
may fail. However, we observe in experiments (see Sec-
tion 5.3) that the heuristic assumption holds with high
probability for random inputs, which is the situation we
need for our cryptographic purposes here. We conjecture
based on the experimental evidence presented in Sec-
tion 5.3 that the probability of failure depends only on
the size of the underlying ﬁeld F. In particular, the algo-
rithm will work with high probability for random poly-
nomials if the errors are uncorrelated. We will see later
that we can enforce this restriction in our protocol even
in the case of Byzantine servers.
2.3.3 Optimality
Relating this to our PIR application, we will be able to
use this algorithm to correctly decode the results of t-
private PIR queries. If k servers respond to us, of which
v are Byzantine (so h = k− v are honest), then this algo-
rithm will succeed with high probability after we query
m+1 (k−t − 1), or equiva-
for m blocks, satisfying v ≤ m
lently, m ≥ v
h−t−1. That is, for m large enough, we can
handle any number of Byzantine servers v < k−t−1. We
note that this bound on v is optimal—if v = k− t − 1, or
equivalently, h = t + 1, then any subset of t + 1 servers’
responses will form a polynomial of degree at most t.
This means that the number of possible valid blocks will
always be exponential, and no polynomial-time algo-
rithm could hope to address this case.
2.4 Dynamic programming
In practice, each of the algorithms we have described
above has different performance characteristics for dif-
ferent inputs. Thus in our implementation, we achieve
the best performance by assembling all of them together
into a portfolio algorithm. This algorithm optimistically
attempts to decode a given input using Lagrange inter-
polation, and if that fails, uses a dynamic program with
timing measurements to fall back to an optimal sequence
of decoding algorithms. See Section 5.2 for more details.
3 Protocols for PIR
In this section, we will introduce the ideas from previous
PIR protocols that will form the basis for our protocol.
3.1 Database queries as linear algebra
We begin with a general mathematical setting of the PIR
schemes we will be considering.
Our database D is structured as an r × s matrix with
r rows. Each row represents one block of the database,
and consists of s words of w bits each. The database D
resides on a remote server. The client wishes to retrieve
one block (row) of the database from the server.
D =
w11 w12
w21 w22
...
...
wr1 wr2
. . . w1s
. . . w2s
...
. . . wrs
One non-private protocol for the client to retrieve row
β of the database would be to transmit the vector eβ con-
sisting of all zeros except for a single 1 in coordinate β
to the server. The server considers eβ as a row vector and
computes the product eβ · D, which it sends back to the
client.
(cid:2)0
0
. . .
1
. . .
0(cid:3)
=(cid:2)wβ 1 wβ 2
w11 w12
w21 w22
...
...
wr1 wr2
(cid:3)
. . . wβ s
. . . w1s
. . . w2s
...
. . . wrs
We will show how to construct
two information-
theoretic PIR schemes that modify this basic scheme to
retrieve blocks from the database without revealing the
query or result to an adversary.
3.2 A simple PIR scheme due to Chor et al.
We next present a simple PIR scheme due to Chor et
al. [10] We begin with the same setup as above. In this
protocol, the words will be single bits, so w = 1, and D
is an r × s matrix of bits. Since we will be construct-
ing information-theoretic PIR, we will be querying more
than one server. We will require that not all of the servers
are colluding to reveal the client’s query. Each of the
(cid:96) ≥ 2 servers gets a copy of D.
A client wishing to retrieve block β of the database
generates the basis vector eβ as above to select coordi-
nate β . Then in order to hide this query vector from
the servers, the client picks (cid:96) − 1 vectors v1, . . . ,v(cid:96)−1
uniformly at random from GF(2)r (that is, (cid:96) − 1 uni-
formly random r-bit binary strings), and computes v(cid:96) =
eβ ⊕ (v1 ⊕···⊕ v(cid:96)−1). v(cid:96) will be a uniformly random
(though not independent) r-bit string, as (cid:96) ≥ 2.
The client sends vi to server i for each 1≤ i≤ (cid:96). Server
i computes the product ri = vi · D, which is the same as
setting ri to be the XOR of those blocks j in the database
for which the jth bit of vi is 1. Each server i returns ri to
the client.
The client XORs the results to obtain r = r1 ⊕ ···⊕
r(cid:96) = (v1 ⊕···⊕ v(cid:96))·D = eβ ·D, which is the β th block of
the database, as required.
Note that this scheme is ((cid:96)− 1)-private; that is, no
combination of (cid:96)− 1 or fewer servers has enough infor-
√
mation to determine i from the information they receive
√
n yields
from, or send to, the client. Choosing r = s =
n bits to privately retrieve
a total communication of 2(cid:96)
a block of size
√
n bits.
3.3 Goldberg’s PIR scheme
Chor’s scheme, above, is not robust; if even one server
fails to respond, the client cannot reconstruct her answer.
Further, it is not Byzantine robust; if one server gives the
wrong answer, then the client not only will reconstruct
the wrong block, but the client will be unable to deter-
mine which server misbehaved.