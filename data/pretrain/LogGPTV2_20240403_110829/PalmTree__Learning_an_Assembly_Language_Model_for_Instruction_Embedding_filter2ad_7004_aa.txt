title:PalmTree: Learning an Assembly Language Model for Instruction Embedding
author:Xuezixiang Li and
Yu Qu and
Heng Yin
PalmTree: Learning an Assembly Language Model for
Instruction Embedding
Xuezixiang Li
Yu Qu
University of California Riverside
Riverside, CA 92521, USA
University of California Riverside
Riverside, CA 92521, USA
PI:EMAIL
PI:EMAIL
Heng Yin
University of California Riverside
Riverside, CA 92521, USA
PI:EMAIL
ABSTRACT
Deep learning has demonstrated its strengths in numerous binary
analysis tasks, including function boundary detection, binary code
search, function prototype inference, value set analysis, etc. When
applying deep learning to binary analysis tasks, we need to decide
what input should be fed into the neural network model. More
specifically, we need to answer how to represent an instruction in a
fixed-length vector. The idea of automatically learning instruction
representations is intriguing, but the existing schemes fail to capture
the unique characteristics of disassembly. These schemes ignore the
complex intra-instruction structures and mainly rely on control flow
in which the contextual information is noisy and can be influenced
by compiler optimizations.
In this paper, we propose to pre-train an assembly language
model called PalmTree for generating general-purpose instruction
embeddings by conducting self-supervised training on large-scale
unlabeled binary corpora. PalmTree utilizes three pre-training tasks
to capture various characteristics of assembly language. These train-
ing tasks overcome the problems in existing schemes, thus can help
to generate high-quality representations. We conduct both intrinsic
and extrinsic evaluations, and compare PalmTree with other in-
struction embedding schemes. PalmTree has the best performance
for intrinsic metrics, and outperforms the other instruction embed-
ding schemes for all downstream tasks.
CCS CONCEPTS
• Security and privacy→ Software reverse engineering; • The-
ory of computation → Program analysis; • Computing method-
ologies → Knowledge representation and reasoning.
KEYWORDS
Deep Learning, Binary Analysis, Language Model, Representation
Learning
ACM Reference Format:
Xuezixiang Li, Yu Qu, and Heng Yin. 2021. PalmTree: Learning an Assembly
Language Model for Instruction Embedding. In Proceedings of the 2021 ACM
SIGSAC Conference on Computer and Communications Security (CCS ’21),
November 15–19, 2021, Virtual Event, Republic of Korea. ACM, New York, NY,
USA, 16 pages. https://doi.org/10.1145/3460120.3484587
Permission to make digital or hard copies of part or all of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for third-party components of this work must be honored.
For all other uses, contact the owner/author(s).
CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea
© 2021 Copyright held by the owner/author(s).
ACM ISBN 978-1-4503-8454-4/21/11.
https://doi.org/10.1145/3460120.3484587
1 INTRODUCTION
Recently, we have witnessed a surge of research efforts that lever-
age deep learning to tackle various binary analysis tasks, including
function boundary identification [37], binary code similarity detec-
tion [23, 31, 40, 42, 43], function prototype inference [5], value set
analysis [14], malware classification [35], etc. Deep learning has
shown noticeably better performances over the traditional program
analysis and machine learning methods.
When applying deep learning to these binary analysis tasks,
the first design choice that should be made is: what kind of input
should be fed into the neural network model? Generally speak-
ing, there are three choices: we can either directly feed raw bytes
into a neural network (e.g., the work by Shin et al. [37], αDiff [23],
DeepVSA [14], and MalConv [35]), or feed manually-designed fea-
tures (e.g., Gemini [40] and Instruction2Vec [41]), or automatically
learn to generate a vector representation for each instruction using
some representation learning models such as word2vec (e.g., In-
nerEye [43] and EKLAVYA [5]), and then feed the representations
(embeddings) into the downstream models.
Compared to the first two choices, automatically learning
instruction-level representation is more attractive for two reasons:
(1) it avoids manually designing efforts, which require expert knowl-
edge and may be tedious and error-prone; and (2) it can learn higher-
level features rather than pure syntactic features and thus provide
better support for downstream tasks. To learn instruction-level
representations, researchers adopt algorithms (e.g., word2vec [28]
and PV-DM [20]) from Natural Language Processing (NLP) domain,
by treating binary assembly code as natural language documents.
Although recent progress in instruction representation learn-
ing (instruction embedding) is encouraging, there are still some
unsolved problems which may greatly influence the quality of in-
struction embeddings and limit the quality of downstream models:
First, existing approaches ignore the complex internal formats
of instructions. For instance, in x86 assembly code, the number
of operands can vary from zero to three; an operand could be a
CPU register, an expression for a memory location, an immediate
constant, or a string symbol; some instructions even have implicit
operands, etc. Existing approaches either ignore this structural
information by treating an entire instruction as a word (e.g., Inner-
Eye [43] and EKLAVYA [5]) or only consider a simple instruction
format (e.g., Asm2Vec [10]). Second, existing approaches use Con-
trol Flow Graph (CFG) to capture contextual information between
instructions (e.g., Asm2Vec [10], InnerEye [43], and the work by
Yu et al. [42]). However, the contextual information on control flow
can be noisy due to compiler optimizations, and cannot reflect the
actual dependency relations between instructions.
 This work is licensed under a Creative Commons Attribution International 4.0 License. CCS '21, November 15–19, 2021, Virtual Event, Republic of Korea. © 2021 Copyright is held by the owner/author(s). ACM ISBN 978-1-4503-8454-4/21/11. https://doi.org/10.1145/3460120.3484587  Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3236Moreover, in recent years, pre-trained deep learning models [33]
are increasingly attracting attentions in different fields such as
Computer Vision (CV) and Natural Language Processing (NLP).
The intuition of pre-training is that with the development of deep
learning, the numbers of model parameters are increasing rapidly.
A much larger dataset is needed to fully train model parameters
and to prevent overfitting. Thus, pre-trained models (PTMs) us-
ing large-scale unlabeled corpora and self-supervised training tasks
have become very popular in some fields such as NLP. Represen-
tative deep pre-trained language models in NLP include BERT [9],
GPT [34], RoBERTa [24], ALBERT [19], etc. Considering the nat-
uralness of programming languages [1, 16] including assembly
language, it has great potential to pre-train an assembly language
model for different binary analysis tasks.
To solve the existing problems in instruction representation
learning and capture the underlying characteristics of instructions,
in this paper, we propose a pre-trained assembly language model
called PalmTree1 for general-purpose instruction representation
learning. PalmTree is based on the BERT [9] model but pre-trained
with newly designed training tasks exploiting the inherent charac-
teristics of assembly language.
We are not the first to utilize the BERT model in binary analysis.
For instance, Yu et al. [42] proposed to take CFG as input and use
BERT to pre-train the token embeddings and block embeddings for
the purpose of binary code similarity detection. Trex [31] uses one
of BERT’s pre-training tasks – Masked Language Model (MLM) to
learn program execution semantics from functions’ micro-traces (a
form of under-constrained dynamic traces) for binary code similar-
ity detection.
Contrast to the existing approaches, our goal is to develop a pre-
trained assembly language model for general-purpose instruction
representation learning. Instead of only using MLM on control flow,
PalmTree uses three training tasks to exploit special characteristics
of assembly language such as instruction reordering introduced
by compiler optimizations and long range data dependencies. The
three training tasks work at different granularity levels to effectively
train PalmTree to capture internal formats, contextual control flow
dependency, and data flow dependency of instructions.
Experimental results show that PalmTree can provide high qual-
ity general-purpose instruction embeddings. Downstream applica-
tions can directly use the generated embeddings in their models. A
static embedding lookup table can be generated in advance for com-
mon instructions. Such a pre-trained, general-purpose language
model scheme is especially useful when computing resources are
limited such as on a lower-end or embedded devices.
We design a set of intrinsic and extrinsic evaluations to systemat-
ically evaluate PalmTree and other instruction embedding models.
In intrinsic evaluations, we conduct outlier detection and basic
block similarity search. In extrinsic evaluations, we use several
downstream binary analysis tasks, which are binary code similarity
detection, function type signatures analysis, and value set analysis,
to evaluate PalmTree and the baseline models. Experimental results
show that PalmTree has the best performance in intrinsic evalua-
tions compared with the existing models. In extrinsic evaluations,
PalmTree outperforms the other instruction embedding models
and also significantly improves the quality of the downstream ap-
plications. We conclude that PalmTree can effectively generate
high-quality instruction embedding which is helpful for different
downstream binary analysis tasks.
instruction representation learning.
In summary, we have made the following contributions:
• We lay out several challenges in the existing schemes in
• We pre-train an assembly language model called PalmTree
to generate general-purpose instruction embeddings and
overcome the existing challenges.
• We propose to use three pre-training tasks for PalmTree
embodying the characteristics of assembly language such as
reordering and long range data dependency.
• We conduct extensive empirical evaluations and demonstrate
that PalmTree outperforms the other instruction embedding
models and also significantly improves the accuracy of down-
stream binary analysis tasks.
• We plan to release the source code of PalmTree, the pre-
trained model, and the evaluation framework to facilitate
the follow-up research in this area.
To facilitate further research, we have made the source code and
pre-trained PalmTree model publicly available at https://github.
com/palmtreemodel/PalmTree.
2 BACKGROUND
In this section, we firstly summarize existing approaches and back-
ground knowledge of instruction embedding. Then we discuss some
unsolved problems of the existing approaches. Based on the discus-
sions, we summarize representative techniques in this field.
2.1 Existing Approaches
Based on the embedding generation process, existing approaches
can be classified into three categories: raw-byte encoding, manually-
designed encoding, and learning-based encoding.
2.1.1 Raw-byte Encoding. The most basic approach is to apply a
simple encoding on the raw bytes of each instruction, and then
feed the encoded instructions into a deep neural network. One such
encoding is “one-hot encoding”, which converts each byte into a
256-dimensional vector. One of these dimensions is 1 and the others
are all 0. MalConv [35] and DeepVSA [14] take this approach to
classify malware and perform coarse-grained value set analysis,
respectively.
One instruction may be several bytes long. To strengthen the
sense of an instruction, DeepVSA further concatenates the one-hot
vectors of all the bytes belonging to an instruction, and forms a
vector for that instruction.
Shin et al. [37] take a slightly different approach to detect func-
tion boundaries. Instead of a one-hot vector, they encode each byte
as a 8-dimensional vector, in which each dimension presents a
corresponding digit in the binary representation of that byte. For
instance, the 0x90 will be encoded as
[ 1 0 0 1 0 0 0 0 ]
1PalmTree stands for Pre-trained Assembly Language Model for InsTRuction
EmbEdding
In general, this kind of approach is simple and efficient, because
it does not require disassembly, which can be computationally
Session 12A: Applications and Privacy of ML CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3237expensive. Its downside, however, is that it does not provide any
semantic level information about each instruction. For instance, we
do not even know what kind of instruction it is, and what operands
it operates on. While the deep neural networks can probably learn
some of this information by itself, it seems very difficult for the
deep neural networks to completely understand all the instructions.
2.1.2 Manual Encoding of Disassembled Instructions. Knowing that
disassembly carries more semantic information about an instruc-
tion, this approach first disassembles each instruction and encodes
some features from the disassembly.
Li et al. [21] proposed a very simple method, which only extracts
opcode to represent an instruction, and encodes each opcode as
a one-hot vector. Unfortunately, this method completely ignores
the information from operands. Instruction2Vec [41] makes use of
both opcode and operand information. Registers, addresses, and
offsets are encoded in different ways, and then concatenated to
form a vector representation. Each instruction is encoded as a
nine-dimensional feature vector. An instruction is divided into
tokens, and tokens are encoded as unique index numbers. While an
opcode takes one token, a memory operand takes up to four tokens,
including base register, index register, scale, and displacement.
While this approach is able to reveal more information about
opcode and operands for each instruction than raw-byte encoding,
it does not carry higher-level semantic information about each
instruction. For instance, it treats each opcode instruction equally
unique, without knowing that add and sub are both arithmetic
operations thus they are more similar to each other than call,
which is a control transfer operation. Although it is possible to
manually encode some of the higher-level semantic information
about each instruction, it requires tremendous expert knowledge,
and it is hard to get it right.
2.1.3 Learning-based Encoding. Inspired by representation learn-
ing in other domains such as NLP (e.g., word2vec [27, 28]), we would
like to automatically learn a representation for each instruction that
carries higher-level semantic information. Then this instruction-
level representation can be used for any downstream binary analysis
tasks, achieving high analysis accuracy and generality.
Several attempts have been made to leverage word2vec [28] to
automatically learn instruction-level representations (or embed-
dings), for code similarity detection [26, 43] and function type
inference [5], respectively. The basic idea of this approach is to
treat each instruction as a word, and each function as a document.
By applying a word2vec algorithm (Skip-gram or CBOW [27, 28])
on the disassembly code in this way, we can learn a continuous
numeric vector for each instruction.
In order to detect similar functions in binary code, Asm2Vec [10]
makes use of the PV-DM model [20] to generate instruction em-
beddings and an embedding for the function containing these in-
structions simultaneously. Unlike the above approach that treats
each instruction as a word, Asm2Vec treats each instruction as one
opcode and up to two operands and learns embeddings for opcodes
and operands separately.
2.2 Challenges in Learning-based Encoding
While the learning-based encoding approach seems intriguing,
there exist several challenges.
2.2.1 Complex and Diverse Instruction Formats. Instructions (espe-
cially those in CISC architectures) are often in a variety of formats,
with additional complexities. Listing 1 gives several examples of
instructions in x86.
1
2
3
4
5
6
7
8
; memory operand with complex expression
mov [ ebp + eax *4 -0 x2c ], edx
; three explicit operands , eflags as implicit operand
imul [ edx ], ebx , 100
; prefix , two implicit memory operands
rep movsb
; eflags as implicit input
jne 0 x403a98
Listing 1: Instructions are complex and diverse
In x86, an instruction can have between 0 to 3 operands. An
operand can be a CPU register, an expression for a memory location,
an immediate constant, or a string symbol. A memory operand is cal-
culated by an expression of “base+index×scale+displacement”.
While base and index are CPU registers, scale is a small constant
number and displacement can be either a constant number or a
string symbol. All these fields are optional. As a result, memory
expressions vary a lot. Some instructions have implicit operands.
Arithmetic instructions change EFLAGS implicitly, and conditional
jump instructions take EFLAGS as an implicit input.
A good instruction-level representation must understand these
internal details about each instruction. Unfortunately, the existing
learning-based encoding schemes do not cope with these complexi-
ties very well. Word2vec, adopted by some previous efforts [5, 26,
43], treats an entire instruction as one single word, totally ignoring
these internal details about each instruction.