Murphyetal.:PreparationofPapersforIEEETRANSACTIONSandJOURNALS
FIGURE3:Twopossibletimelinesforfailurepredictionresponse.Inthecaseofmanualpreventivemitigation,itisunknown
whetherthemitigationwillhaveanimpactontheemergenceofthefailure,thereforeitmaynotbenecessarytoengagelogistics.
also less prone to overfitting than most types of models as tendtohavepoorperformance.Tocountertheseweaknesses
generalization is built-in to the model (through seeking a and improve performance, other algorithms have been built
largemargin). byaggregatingDTsincertainmanners.
However, it usually does not converge or breeds poor RandomForest(RF)[47]isanensemblemethodonDe-
results if the kernel function does not fit the data, and it cision Trees trained by bagging, or Bootstrap Aggregating.
is possible that no simple kernel function will fit the data. EachDTistrainedonadifferentsubsetofthedata,andthe
Because of that, with no a priori knowledge of the data finalpredictionforeachinputsampleistheaverageormost
structure, after trying the few wide spread kernel functions voteddecisionoutofallDTstrained.IntheRFalgorithm,the
(linear, RBF, polynomial, sigmoid) it can be advised to try differentDTsarealsotrainedondifferentsubsamplesofthe
MLmethodsotherthanSVM. featuressothateachDTismoreindependent.Thismodelhas
Hessian Locally Linear Embeddings (HLLE) [44] is a the advantage of working well without much pretreatment
dimension reduction projection aiming to preserve distance and is moderately explainable, but it does not work well on
between points locally. This dimension reduction operation non-linearly separable data, and setting the forest and tree
aids in reducing complexity of problems, however, in some sizesrequiresalotoftweaking.
cases,thisstepisunnecessary. AdaBoost [50] is another ensemble method on Decision
Least Absolute Shrinkage and Selection Operator Trees trained by boosting and adapting voting rights to am-
(LASSO) as predictor [45] is a regression algorithm. This plify the importance of stronger weak learners. Boosting is
model performs variable selection (based on covariance) the process in which you give more importance to training
andregularization,beforeperformingprediction.Thismodel sampleswherethepreviousmodelwaswrongsothatthenew
provides explainability by dimension reduction, and greater weaklearnersthatyoutrainarebettertrainedforthesetypes
prediction performance than simple regression. However it ofsamples.Itisknowntoincreaseperformancewhenhaving
doesnotperformwellinveryhighdimensionenvironments. many weak classifiers (DTs), however the algorithm needs
theseweakclassifierstohavelessthan50%errorandbeas
C. DECISIONTREE-BASEDALGORITHMS independentaspossible.
AllofthefollowingmethodsarebasedonDecisionTrees. XGBoost [51], another ensemble method for Decision
Decision Tree (DT) algorithm [48] is a succession of (if, Trees trained by bagging, boosting, adapting voting rights.
else) statements, feature per feature arranged in the form of There is the added step of pruning to remove the weaker
a tree. Each time, the feature is selected by choosing the branchesDTsandweakerweaklearners(DTs).Itisknownto
one allowing maximum entropy gain by a simple threshold havegreatperformance,veryhighadaptability,butneedscor-
separationofthedataalongthisfeature.Theselectedfeature rect calibration of hyperparameters which can be a lengthy
and the threshold separation are included in the DT before process.
repeating the step to continue forming the DT. This method ReducedErrorPruning-Tree(REP-Tree)[52]isaprun-
yieldsveryexplainableresults.EachDThascontainedwithin ingmethodforDTs.Itlearnstosplittreeleavesbyreducing
theinformationofeachsplitalongwithitsentropylevelsand prediction error step by step on a subsample of the training
thequantityofdataofeachclassremaining.Thisinformation set. Then it applies a pruning mechanism where another
can be used to generate graphical aids. However, as the subsample of the training set is used to evaluate the splits
split made is dependent on the data used, DTs usually have and the branches. Poorly generalisable branches and splits
high variance in decisions depending on data and so a poor are removed. It is used to reduce the high variance problem
generalizationcapacity.Alsowhenthedataiscomplex,DTs ofDTs.
VOLUME4,2022 7
Murphyetal.:PreparationofPapersforIEEETRANSACTIONSandJOURNALS
Modeltype Modelname Type Examplesin Litt. Description Advantages Disadvantages
NFP
LinearRegression Regression [31],[32] [33] Fitalinethroughthedata Simple,quick Linear,doesn’tfitcomplex
LinearModels datawell
LogisticRegression Classification [34] [35] Placeadecisionthresholdbasedonafitted Simple,Quick Linear,doesn’tfitcomplex
linethroughthedata datawell
LVQLearning Classification [36] [37] Similartok-NearestNeighboralgorithm, Lesscomplexitythank-NN, Lowcapacityfornon-linear
Machine measuresproximitytoselectedfew simple,explainable problems
(comparedtodatasetsize)codebook
vectorstotakeclassificationdecision
Projection SVM Classification/ [31],[32], [40]–[42] Projectdatainotherspacethroughkernel Quick,canfitbothlinearand Doesn’tconvergeifkernel
based Regression [38],[34], functionthenchooselargestmargin non-linear isn’tgoodenough
algorithms [39] separatorthreshold
HessianLocally Dimension [43] [44] Dimensionreductionprojectionaimingto Dimensionreductionaidsin Sometimesnotuseful
LinearEmbeddings reduction preservedistancebetweenpointslocally reducingcomplexityofproblems
(HLLE)
LASSOas Regression [31] [45] Modelthatperformsvariableselection Explainabilitybydimension Doesn’tperforminvery
predictor (basedoncovariance)andregularization reduction,greaterperformance highdimension
thansimpleregression environments
RandomForest Classification/ [39],[46] [47] CollectionofDecisionTreestrainedby ModeratelyExplainable,works Doesn’tworkwellon
Regression baggingondifferentsubsamplesofthe wellwithoutmuchpretreatment non-linearlyseparabledata,
features settingforestandtreesize
Tree-based
DecisionTree Classification/ [39] [48] Collectionof(if,else)onafeature Veryexplainable,entropy Highvarianceindecisions
Regression statementsarrangedintheformofatree, informationincluded dependingondata
featuredecidedbymaximuminformation
gainbysplit
AdaBoost Classification/ [49] [50] CollectionofDecisionTreestrainedby Knowntoincreaseperformanceof Needweakclassifiersto
Regression bagging,andboostingandadaptingvoting manyweakclassifiers(DTs) haveerror<50%
rightstoamplifyimportantofstronger
weaklearners
XGBoost Classification/ [39] [51] CollectionofDecisionTreestrainedby Knowntohavegreatperformance, Needscorrectcalibrationof
Regression bagging,boosting,adaptingvotingrights, veryhighadaptability hyperparameters
andpruningtoremovetheweakerbranches
andweakerweaklearners
ReducedError Classification/ [31] [52] MethodforpruningDTsbyusingother Reducesdatasetsubsetvariance
Pruning-Tree Regression subsetofdatasettotakethepruning influenceonpruning
decision
M5P Regression [31] [53] UseofDTsasaregressor,learnstosplit Explainability,equationsincluded, Lowcapacityfornon-linear
treeleavesbyreducingerrorofregression easytoimplement problems
stepbystep
Multi-Layer Classification/ [29],[32], [55] Collectionofartificialneurons,withone Canachievegreatperformance, Poorexplainability,quite
Perceptron Regression [36],[39], inputlayer,oneoutputlayer,andanylow networkproblemcapacity susceptibletothe
NN
[54] number(otherwiseDeepLearning)of increaseswithsize bias/varianceproblem
hiddenlayers
AutoEncoder Dimension [32] [56] DeepLearningNNarchitectureintheshape Hasbeenshowntogenerategreat Resourceconsuming
reduction ofanhourglass.Designedfordimension dimensionreduceddata
reductionwhileconservinginformation. representationsinmanyexamples
RecurrentNN Classification/ [57] [58] NNmodeldesignedtotakesequencesof ImprovementofclassicalRNNs Resourceconsuming
(LSTM) Regression inputandpropagatingthecontextthrough (vanishinggradientproblem),
thetreatmentofthesequence. usuallygoodperformance
comparedtosimplefeedforward
techniques
Generative Classification/ [59] [60] Doublenetworkusedtogenerateadditional Greatforcoherentdatageneration Resourceconsuming
Adversarial Regression coherentdata.Onenetworkistrainedto
Networks(GAN) createfalseexamplesresemblingrealones.
Otheristrainedtodiscriminate
TABLE2:DescriptionofMLmethodsusedinthefieldofNFP
M5P [53] uses DTs as a regressor. This algorithm learns In Multi Layer Perceptrons (MLP) [55], neurons are
to split tree leaves by reducing regression error step by step stacked in several hidden layers between an input layer and
on the training set. A step of pruning is set after that to anoutputlayer.Thistypeofmodelcanachievegreatperfor-
remove branches that contribute most to regression error. mance,asthenetworksproblemcapacityincreaseswiththe
This algorithm has high explainability, as the equations are numberandsizeofhiddenlayers.Itisalsohighlyversatilein
included, and it is easy to implement. However it has low thatshapescanbecompletelycustomized.Howeverasacon-
capacityfornon-linearproblems. sequence, it has very poor explainability, though there have
beenworksinthisfieldrecently,anditisquitesusceptibleto
thebias-varianceproblem.
D. NEURALNETWORKS(NN)
Neural Networks [67] using Backpropagation are the most Auto Encoders [68] are a class of Deep Learning (DL)
famous branch of ML currently. Similar to DT-based algo- designed for dimension reduction. Its architecture is in the
rithms, they are based on a nuclear function, the artificial shapeofanhourglassandtheexpectedoutputfortrainingis
neuron, that is arranged into a structure. The function of thesamedataastheinput.Thereforethemodellearnshowto
the neuron is simply a weighted sum of all inputs (a bias generateadecentdimensionreductionfromtheinputtothe
inputisadded)passedthroughanactivationfunction.There middlehiddenlayer,andlearnstoreconstructwiththelowest
are several commonly used activation functions, and when errorpossibletheinputfromthemiddlehiddenlayer.Ithas
the output of the activation function passes a threshold, the beenshownthatusingthedimensionreduced/encodedinputs
neuronisexcited,otherwiseinhibited. for other problems can increase performance. However, as
8 VOLUME4,2022
Murphyetal.:PreparationofPapersforIEEETRANSACTIONSandJOURNALS
it belongs to the class of DL, it is known to be resource realityofthenetworkstateinanaturallyoccurringfault,and
consuming. so the network state might be missing or showing elements
Recurrent Neural Networks (RNN), and in this partic- thatmakethepredictioneasierorharder.
ular case, Long Short Term Memory (LSTM) [58] are They then predict the occurrence of the failure before it
a type of NNs. In RNN, the model is fed a sequence of happens. They observe that injecting faults typically results
inputs (can be thought of as another dimension to the data, inafailureoccurringinabout110seconds.
such as time) and it is engineered to use the information Althoughtheydonotshowalltheresultstheyhave,using
from previous samples of the same sequence in addition to K=20parameterforSHLLE,theyachieve72.3%Precision
currentinputtogenerateanoutput.LSTMareconsideredan and70.1%Recallontheirtestdata(withafalsepositiverate
improvementofclassicalRNNswherethevanishinggradient of 11.6%). Prediction performance varies depending on the
problem is addressed (though not completely solved) by type of failure, as the study declares more than 60% recall
incorporatinganadditiveoperationintothemodel.Thistype onCPUandmemoryrelatedfailures,andaround70%recall
ofmodelusuallyhasgoodperformancecomparedtosimple fornetworkfailures.Coupledwiththeaverageofaround110
feedforwardtechniquesandhasthecapacitytotackleissues seconds in RTTF from injection of the faults, those results
hard for simple feedforward techniques. However it is also show that it is possible to implement automatic response
veryresourceconsuming. mechanismstothepredictedfailures.Howeverinthecaseof
GenerativeAdversarialNetworks(GAN)[60]areatype theneedofequipmentreplacementormanualintervention,it
of architecture used for coherent data generation (fake gen- seemsthatthesepredictionswouldnotbeactionable,asthey
erator) and for false data detection. It is done by employing donotgrantenoughtimeforactiontobetakenbeforefailure.
two NNs. One network is trained to create false examples They conclude that fault prediction may, in turn, replace
resemblingrealones,andtheotheristrainedtodiscriminate root cause analysis and root cause detection. We may add
betweenrealexamplesandexamplesgeneratedbytheother that predicting the root cause in addition to the present
model.GANshavealotofpotentialusesandcouldgenerate predictions may be a key in implementing early response,
very useful coherent data but the training process is quite especially when human intervention is required, such as for
resourceconsuming. whenequipmentreplacementisinevitable.
Wang et al. [38] work on equipment fault prediction in
V. APPLICATIONSOFNFPTONETWORKS an optical Wavelength Division Multiplexing (WDM) mesh
In this section we describe the different studies that have network.Theyidentifythatwhilepassiveprotectionforopti-
been realized in NFP, classified by the application of the calnetworksexists,whenanetworkfaultoccurs,dataisstill
prediction. We have regrouped in Table 3 these different lostwhilewaitingforthenetworkprotectionandtherecovery
studiesperapplication. mechanisms of the network to kick in. Therefore, using
prediction methods, we could enable proactive protection
A. EQUIPMENTHEALTH methods,notlosinganydataatthetimeoffailure.
Anequipmenthealthpredictionisthepredictionofthehealth Theyproposeamodelbasedonfivevariablesofthephys-
state of the equipment in time X, for duration Z, or for the ical layer of the equipment, and then create three features
interval: based on each variable, namely, the minimum, average and
maximum values of the day. Every day, the values of these
[t+X;t+X+Z]
features for the next day are predicted using Double Expo-
This falls into the category of classification problems. For nential Smoothing (DES). The resulting predicted vector is
reference,inallthepapersmentioned,healthyisnegativeand thenfedintoaSupportVectorMachine(SVM)classifier[66]
faultyispositive. which predicts whether, the next day, the equipment will be
Lu et al. [43] work on the prediction of network failures unavailableformorethan40,000seconds(11hours)ornot.
in a controlled Local Area Network (LAN) environment. Thepapershowsverypromisingresultsintheseconditions,
They recognize that online (meaning in real-time) failure being able to boast a 91% prediction accuracy on a 24 hour
predictionhasthedrawbackthatitneedsalotofspecialized advancetimeprediction,howeverthedefinitionoffailureas
knowledgeinordertoextractandadequatelyformatfeatures 11hoursofunusabletimeisquitealargeone.Theprediction
necessaryforprediction. metricgivenisaccuracy,andthedataisclassbalanced,sothe
Inordertoaddressthisissue,theyproposeanautonomous accuracy should be somewhat representative of both classes
system for feature selection. They aggregate six sources of (asvisibleinsectionII-B2withtheformulaforaccuracy,in
data and use Hessian Locally Linear Embeddings (HLLE), caseswhereclassimbalanceispresent,accuracyisnotagood
thenSupervisedHLLE(SHLLE)-thatusesclassinformation performancemetric).Additionally,theinformationprovided
tobetterseparatedataintheembeddingspace-toselectthe isrelativetoeachpieceofequipmentandwiththe24htime
appropriatefeaturesforoptimalpredictionperformance. frame, the predictions are very easily actionable. However,
TheyusetheirdatatopredictNetwork,CPU,andMemory while the low definition of the failure hypothesis and the
failurescomingfromfaultstheyinjectintothenetwork.We 24hourwindowprobablymaketheproblemmucheasierto
may note that injecting the failure might not reproduce the solve,itlimitstheutilityofthismethodforlargerandmore
VOLUME4,2022 9
Murphyetal.:PreparationofPapersforIEEETRANSACTIONSandJOURNALS
Applications Description NFPexamples
EquipmentHealth Predictfailureinnetworkequipmentinthefuture [38],[39],[46],[54],[57]
RTTF PredictRemainingTimetoFailureonnetworkequipment [31]
NetworkHealth PredictNetworkHealth(globalstate) [29],[32],[36]
LinkFailure Predictwhetherlinkbetweentwonetworkequipmentswillgodown [34]
Predictwhetherornotcertainnetworkstateswillleadtoanalarm
AlarmPrediction [49],[59]