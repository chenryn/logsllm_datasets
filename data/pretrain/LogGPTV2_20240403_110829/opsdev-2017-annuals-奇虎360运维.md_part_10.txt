我们查了 linuxkernel源码中协议号定义的头文件，确认了协议号是4（源码上也标出来older KA9Q tunnels
以看这里：https://www.iana.org/assignments/protocol-numbers/protocol-numbers.xhtml）随后,
后发现IP包仍接收不到；（KA9Q可以看维基百科https://en.wikIPedia.org/wiki/KA9Q，它定义的协议号可
A：IP协议号的确认问题。起初，我们根据KA9Q这个古老的TCP/IP实现，把IPIP的协议号写成了94；配置
Q：出现了什么问题？能讲讲吗？
虚拟化－带着问题了解Openstack Neutron 安全组67
口
口
扫查
看文章详
品
情
E
---
## Page 73
个广播域。
平坦网络
户之间的隔离问题，它当然要提供支持，下面我们就看一下neutron是怎么实现的。
OpenStack的玩法
这些帧，交换机一般认为工作在二层，对这些广播包，也都要转发，所以二层通常被称为一个
有国际标准，按着OSI的分层模型，共有七个层，大家在讨论的隔离，通常指的是第2层，也叫
“隔离”啥
前言
“平坦”就是指都在一个空间下，也就是没有做二层上的隔离，虚拟机都在同一个二层，同-
IOPENSTACK中的网络隔离
“广播域”，这就好比大家在一个教室里，都能互相看到，除非分隔到不同的教室。
，其实是“媒体访问控制”
“数据链路层”
我们不得而知，但是我们可以聊聊OpenStack的，毕竟它一直在模仿AWS嘛。
68OpenStack中的网络隔离－虚拟化
I Mar. 6th 2017 BY 王浩宇
neutron中创建的网络是有“type”的，其中最基础的一种type就是“flat”，顾名思义，
 OpenStack的neutron负责为虚拟机提供网络，而且OpenStack是假设多租户的，那多租
那为什么要在这个二层上搞隔离呢？
数据链路层的网络包，
我们知道，计算机网络，是分层实现的，不同协议工作在不同层，这些层的设计、制定都
首先，我们先搞清楚，所谓“隔离”，到底是在“隔”什么。
“VPC”等概念兴趣大增，大家的热议中多次提到AWS的VPC，亚马逊的AWS怎么搞的
最近，关于xx公有云的用户网络，由于隔离问题引发安全讨论，大家顿时对啥“经典网络
从网上找了个示意图：
因为二层的帧，其中一些帧的地址是广播地址，在同一个二层的设备都可以、也必须接收
（media access control）的简称，这是数据链路层的一个子层。
，也叫“帧”
，我们常说的网卡的MAC地址，就是帧的地址，MAC
---
## Page 74
不同教室的人不可见。
域网）本身就是交换机广泛使用的二层隔离技术。
VLAN隔离网络
传染病，大家都被传染，要想隔离，可以把人分散到各个教室。
面，还有广播风暴等问题。
这就好比把整个学校的人，从大礼堂，分隔到了不同的教室，同一个教室的人互相可见，
单个大二层网络，就好比整个学校的人都在一个大礼堂，大家都能看到，而且万一谁得了
示意图大概这样：
这种“平坦”的大二层网络，虽然实现、管理相对简单，但也会有诸多问题，除了安全方
192.168.100.5/24172.16.0.10/24
WEB1
VLAN10
Onelarge broadcastdomain
FlatSwitched Network
DB1
Q
VLAN20
虚拟化-OpenStack中的网络隔离
69
---
## Page 75
层可达），被封装的内层帧就可以被运输。
封，把内包裹的二层帧，输送给目的虚拟机。
UDP包裹起来，然后以宿主机的IP，必要的话，经过3层的路由，到达目的宿主机，然后再解
一层载着另一层移动”，VXLAN是最常见的协议，它是把虚拟机的二层的帧，在宿主机上用
OverLay网络
最多也就能支持四干多个租户。
VLAN的可用数量有限制，VLAN的ID号仅有四干多个，我们假设每个租户分配1个VLAN，那
主要的：
·因为是完全隔离的，租户可以随意定义自己的网络，哪怕和其他租户的IP段重叠都没有关系
70OpenStack中的网络隔离－虚拟化
·vxlan的范围足够大，一干六百多万，租户随便用可靠
那这种方案有什么优点呢？
最终封装完的包，外层的源IP、目的IP地址，都是宿主机的，所以只要宿主机之间互通（3
那个“Inner Frame”就是被包裹的虚拟机的二层的包。
有点抽象？我们先看看VXLAN的包结构，就知道”包裹“是啥意思了：
overlay（覆盖）网络，所谓“覆盖”，大体上指“在一层上面覆盖另一层，也可以说是用
但这种方案也有一定的局限性，
M
tertene
租户A
租户B
，首先管理相对麻烦，需要配合设置物理交换机，另外
oNEhenet80210
IPv4WithVXLAN
InnerFrame
Original Payload
—
BD
Noticudingintn
---
## Page 76
VPC（virtual private cloud），不是个技术专有名词，
很明确地告诉你，上面这个图就是VPC！
最后，那到底啥是VPC呢？
VPC
关于上面提的第三点，在neutron中大概是这样的：
比如上图中，租户A的两个网络，连接到一个路由器（可以是虚拟的）上
如果通过一定技术实现支持3层路由器，租户可以将自己的网络，随意组织自己的网络拓扑，
比如上图中，租户A的网络，
tenant-net
10.0.0.0/22
10.0.0
与租户B的网络基于Java
tenant-router
Instanz 
test-vm
 Router 
而是亚马逊AWS创造的一个产品层面
虚拟化－OpenStack中的网络隔离
72.16.0.8
int-net
172.16.0.0/22
L
---
## Page 77
https://opsdev.cn/post/network-isolation-in-OpenStack.html
本文链接：
A：是的，用的是vxlan的封装，也支持GRE的封装，由于都是通过内核做协议封装，比较消耗性能。
Q：neutron的VPC是vxlan实现的？的性能如何？
的租户，都可以通过neutron实现这些。
A：支持，所谓“VPC”，其实就是用户可以自己控制网络拓扑，建网、建路由器、组网等等，OpenStack
Q:OpenStack neutron支持VPC吗
面对面：
云”
租户网络彻底隔离、IP段都能重叠、路由器、网络拓扑都能由自己定义，这还不是“虚拟私有
的名词。
72OpenStack中的网络隔离－虚拟化
吗！
山
口
一扫查看文章详情
口
---
## Page 78
这个理论值计算的话，除非OS 的内存使用量非常大，否则不应该有OOM 情况的发生。
就是说撑死了虚拟机使用内存，所有虚拟机内存使用总量也不会超过总内存的90.625%，按照
认不是人为操作后，通过计算节点系统日志发现，是系统内存不足触发OOM 导致。
1.问题定位
场景描述：
遇到了某几个业务的虚拟机频繁被OOM 的情况，我们来看一下是什么原因。
前言
■诊断虚拟机频繁OOM的问题
·计算节点：CentOS7.2、QEMU、KVM、128GB内存
·laaS 管理平台：OpenStack
现象是业务虚拟机非人为宕机，且运行一段时间就会发生。在查看操作历史和审计记录确
虚拟机被 OOM 应该是运维laas 平台人员经常会遇到的一个问题。这不，前段时间我们就
I Jan. 3rd 2018 BY 霍明明
其次，我们已经给计算节点OS 预留了12GB 的内存（12GB／128GB=9.375%）。也
首先，这些虚拟机所在的计算节点并没有开启内存超卖；
原因是找到了，但是发现比较诡异，为什么呢？
分配给虚拟机内存
VMs
预留给计算节点操作
系统内存 
虚拟化－诊断虚拟机频繁OOM 的问题
8
73
---
## Page 79
拟机内存实际使用量基本在8.3～8.9GB 之间，套餐是2核4GB 的虚拟机内存实际使用量也基
除了计算节点OS内存使用问题导致的OOM。
+ 4)／128=93.75%，此时，系统不应该触发OOM，这还不算 sWap (4GB)。这里，我们排
的情况。带着疑问，我们将被OOM的虚拟机重新启动，在宿主机上观察内存使用情况。
2.问题排查
本在 4.6～4.8GB之间。至此，我们知道了多出来的内存被谁使用了。
一个‘
况，且总内存使用量也很正常，大约在 4GB 左右。此时，理论内存最大使用率约为（128－12
但是没有达到超过分配值的情况。
74诊断虚拟机频繁OOM的问题－虚拟化
为什么虚拟机内存使用量会比分配的值要大呢？到虚拟机内部去看，其内存使用虽然很满
如上图，RES一列，内存的使用量远大于分配给虚拟机的内存量。套餐是4核8GB 的虚
“大”问题。
通过对计算节点上触发 OOM 前后虚拟机进程(qemu-kvm)分配的内存进行统计，发现了
既然，OS内存使用没问题，那么换个角度看看虚拟机内存使用是否有问题呢？
经过一段时间的运行后虚拟机还是被OOM 掉了，但是OS上的服务并没有内存泄露等情
经验上来说，计算节点OS上跑的服务内存不会吃满12GB，除非是某些服务出现内存泄露
---
## Page 80
面对面：
 2.https://unix.stackexchange.com/questions/140322/kvm-killed-by-0omkiller 
1.https://lime-technology.com/forums/topic/48093-kvm-memory-leakingoverhead/
参考文献：
胀部分内存。不过这种方式不太通用，不建议使用。
说 4GB 内存有点小。我们发现在虚拟机 OOM 时，swap 使用率肯定是100%，这也很符合
体内存使用率不会超过OOM的临界值。
3.解决方案
基本是按照0.6GB的量来预估每台虚拟机“膨胀”值的（仅供参考）。
A：具体值没有，根据我们观察数据来看，一般不会超过1GB，最大也就是在0.9GB左右。我们在预估内存时
Q：虚拟机“膨胀”出来的内存有具体的数据吗？就是每个qemu-kvm进程大约能“膨胀”出多少内存？
OOM 产生的前提条件。所以，如果你的节点上有 SSD 盘的话，建议将 sWap 适当调大。
内存，这部分内存也算在了虚拟机进程 qemu-kvm 头上了。
https://lime-technology.com/forums/topic/48093-kvm-memory-leakingoverhead/ 
带着疑问，Google了一些资料，其他人也有类似的疑惑。
修改 OpenStack 逻辑，在虚拟机调度内存计算时，比套餐值大一些，给虚拟机预留出膨
调大 swap 值。目前我们计算节点 swap 值统一为 4GB，对于一个 128GB 内存的节点来
增大 OS 预留内存空间。通过增大 OS 预留内存空间来填充虚拟机膨胀部分内存，使得总
 文章的意思是说除了虚拟机内部使用的内存外，qemu-kvm 进程还需要为虚拟的设备提供
问题我们定位了，那如何解决这个问题，减少虚拟机被OOM 情况发生呢？
fers/
hor
ing]#
虚拟化－诊断虚拟机频繁OOM的问题
cached
75
---
## Page 81
本文链接：https://opsdev.cn/post/oom.html
的情况，这个时候我们就花时间分析了下原因，也就是本文描述的那样。
给的），后来经常出现虚拟机OOM的情况，我们调大了预留值，改成了12GB，后来还是会出现虚拟机OOM
A：这一块其实我们经历了3个阶段。我们的计算节点是128GB内存。刚开始，预留了4GB（这个值是凭感觉
Q：你们现在每台计算节点预留多大内存给宿主机上的非虚拟机服务？
76诊断虚拟机频繁OOM的问题－虚拟化
章
■
---
## Page 82
停止所有计算节点nova-compute服务
停服务
备份数据库
升级YUM源
升级前的调研
，又要看Liverty到Mitaka的ReleaseNotes。
Mitaka版，中间跨越了大版本Liberty版，所以我们既要看Kilo版到Liberty版的ReleaseNotes
，新增/删减了哪些配置项等，这一点还是很贴心的，感谢社区。因为我们是从Kilo版直升到
时都会提供相应的RleaseNotes，里面给出了当前发布版与上一版之间都新增/删减了哪些功能
有数。那如何来快速地了解个版本之间的这些变更呢？别急，OpenStack 在任何大版本的发布
升级前的调研
看我们是怎么搞定的吧。
的升级，我们是跨大版本之间的升级（Kilo到Mitaka）。但是，我们勇于尝试，接下来就来看
升级任务，回想其安装的过程，可想而知其升级是多么的让人没有底，而且不是相邻版本之间