        }
        return 1;
    }
    static int emulator_pio_in_emulated(struct x86_emulate_ctxt *ctxt,
                        int size, unsigned short port, void *val,
                        unsigned int count)
    {
        struct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);
        int ret;
        if (vcpu->arch.pio.count)
            goto data_avail;
        ret = emulator_pio_in_out(vcpu, size, port, val, count, true);
        if (ret) {
    data_avail:
            memcpy(val, vcpu->arch.pio_data, size * count);
            vcpu->arch.pio.count = 0;
            return 1;
        }
        return 0;
    }
在最后一个函数中，由于vcpu->arch.pio.count此时还没有数据（需要user spaces提供），所以会执行
emulator_pio_in_out，这在之前已经看过这个函数了，这就是设置kvm_run的相关数据，然后user spaces来 填充。
执行完了x86_emulate_insn，流程再次回到x86_emulate_instruction，最重要的是设置
vcpu->arch.complete_userspace_io这样一个回调。
    if (ctxt->have_exception) {
        inject_emulated_exception(vcpu);
        r = EMULATE_DONE;
    } else if (vcpu->arch.pio.count) {
        if (!vcpu->arch.pio.in)
            vcpu->arch.pio.count = 0;
        else {
            writeback = false;
            vcpu->arch.complete_userspace_io = complete_emulated_pio;
        }
之后这一次vm exit就算完事了。这样就会退到user space的ioctl KVM_RUN处。user space发现是一个
KVM_EXIT_IO，并且方向是KVM_EXIT_IO_IN，于是向kvm_run填入数据0xbeff。
        case KVM_EXIT_IO:
            printf("KVM_EXIT_IOn");
            if(kvm->vcpus->kvm_run->io.direction == KVM_EXIT_IO_OUT)
                printf("out port: %d, data: 0x%xn", 
                    kvm->vcpus->kvm_run->io.port,  
                    *(int *)((char *)(kvm->vcpus->kvm_run) + kvm->vcpus->kvm_run->io.data_offset)
                    );
            else if(kvm->vcpus->kvm_run->io.direction == KVM_EXIT_IO_IN)
            {
                printf("in port: %dn",kvm->vcpus->kvm_run->io.port);
                *(short*)((char*)(kvm->vcpus->kvm_run)+kvm->vcpus->kvm_run->io.data_offset) = 0xbeff;
            }
由于user space的ioctl一般都是运行在一个循环中（如果不这样，guest也就不可能一直运行着了)。所以接着调用 KVM_RUN
ioctl。在进入non-root的模式前，有一个工作就是判断vcpu->arch.complete_userspace_io 是否设置，如果设置就会调用。
    int kvm_arch_vcpu_ioctl_run(struct kvm_vcpu *vcpu, struct kvm_run *kvm_run)
    {
        int r;
        sigset_t sigsaved;
        if (unlikely(vcpu->arch.complete_userspace_io)) {
            int (*cui)(struct kvm_vcpu *) = vcpu->arch.complete_userspace_io;
            vcpu->arch.complete_userspace_io = NULL;
            r = cui(vcpu);
            if (r arch.pio.count || vcpu->mmio_needed);
        r = __vcpu_run(vcpu);
        return r;
    }
从之前的分之知道
    vcpu->arch.complete_userspace_io = complete_emulated_pio;
看看相应的代码
    static int complete_emulated_pio(struct kvm_vcpu *vcpu)
    {
        BUG_ON(!vcpu->arch.pio.count);
        return complete_emulated_io(vcpu);
    }
    static inline int complete_emulated_io(struct kvm_vcpu *vcpu)
    {
        int r;
        vcpu->srcu_idx = srcu_read_lock(&vcpu->kvm->srcu);
        r = emulate_instruction(vcpu, EMULTYPE_NO_DECODE);
        srcu_read_unlock(&vcpu->kvm->srcu, vcpu->srcu_idx);
        if (r != EMULATE_DONE)
            return 0;
        return 1;
    }
    static inline int emulate_instruction(struct kvm_vcpu *vcpu,
            int emulation_type)
    {
        return x86_emulate_instruction(vcpu, 0, emulation_type, NULL, 0);
    }
最终也是调用了x86_emulate_instruction，值得注意的是用了参数EMULTYPE_NO_DECODE，这就不会再次
解码。而是直接执行我们之前的em_in函数。
    static int emulator_pio_in_emulated(struct x86_emulate_ctxt *ctxt,
                        int size, unsigned short port, void *val,
                        unsigned int count)
    {
        struct kvm_vcpu *vcpu = emul_to_vcpu(ctxt);
        int ret;
        if (vcpu->arch.pio.count)
            goto data_avail;
        ret = emulator_pio_in_out(vcpu, size, port, val, count, true);
        if (ret) {
    data_avail:
            memcpy(val, vcpu->arch.pio_data, size * count);
            vcpu->arch.pio.count = 0;
            return 1;
        }
        return 0;
    }
在最终的emulator_pio_in_emulated中，由于这个时候vcpu->arch.pio.count已经有值了，表示数据可用了。
最终会把数据拷贝到ctx->dst.val中。
    (gdb) n
    em_in (ctxt=0xffff88011d429550) at arch/x86/kvm/emulate.c:3440
    3440        return X86EMUL_CONTINUE;
    (gdb) n
    3441    }
    (gdb) p ctxt->dst.val
    $58 = 48895
    (gdb) p /x ctxt->dst.val
    $59 = 0xbeff
    (gdb) n
回到x86_emulate_insn，执行完了指令回调之后，会调到writeback函数去：
    if (ctxt->execute) {
        if (ctxt->d & Fastop) {
            void (*fop)(struct fastop *) = (void *)ctxt->execute;
            rc = fastop(ctxt, fop);
            if (rc != X86EMUL_CONTINUE)
                goto done;
            goto writeback;
        }
    writeback:
        rc = writeback(ctxt);
        if (rc != X86EMUL_CONTINUE)
            goto done;
我们之前解码得到ctxt->dst.type是一个寄存器，所以会执行write_register_operand
    static int writeback(struct x86_emulate_ctxt *ctxt)
    {
        int rc;
        if (ctxt->d & NoWrite)
            return X86EMUL_CONTINUE;
        switch (ctxt->dst.type) {
        case OP_REG:
            write_register_operand(&ctxt->dst);
            break;
        return X86EMUL_CONTINUE;
    }
    static void write_register_operand(struct operand *op)
    {
        /* The 4-byte case *is* correct: in 64-bit mode we zero-extend. */
        switch (op->bytes) {
        case 1:
            *(u8 *)op->addr.reg = (u8)op->val;
            break;
        case 2:
            *(u16 *)op->addr.reg = (u16)op->val;
            break;
        case 4:
            *op->addr.reg = (u32)op->val;
            break;  /* 64b: zero-extend */
        case 8:
            *op->addr.reg = op->val;
            break;
        }
    }
最后一个函数op->addr.reg是解码过程中的目的操作数的寄存器，由之前知道是rax(&ctxt->_regs[0])，这样
就把数据(0xbeff)写到了寄存器了。但是这里是ctxt的寄存器，最后还需要写到vmcs中去，通过调用如下函数 实现
    if (rc == X86EMUL_CONTINUE)
        writeback_registers(ctxt);
    static void writeback_registers(struct x86_emulate_ctxt *ctxt)
    {
        unsigned reg;
        for_each_set_bit(reg, (ulong *)&ctxt->regs_dirty, 16)
            ctxt->ops->write_gpr(ctxt, reg, ctxt->_regs[reg]);
    }
    static void emulator_write_gpr(struct x86_emulate_ctxt *ctxt, unsigned reg, ulong val)
    {
        kvm_register_write(emul_to_vcpu(ctxt), reg, val);
    }
    static inline void kvm_register_write(struct kvm_vcpu *vcpu,
                        enum kvm_reg reg,
                        unsigned long val)
    {
        vcpu->arch.regs[reg] = val;
        __set_bit(reg, (unsigned long *)&vcpu->arch.regs_dirty);
        __set_bit(reg, (unsigned long *)&vcpu->arch.regs_avail);
    }
这样，接着进入guest状态的时候,guest得RAX就有了user space传来的数据了。下面是一些调试数据。
    (gdb) n
    x86_emulate_insn (ctxt=0xffff88011d429550) at arch/x86/kvm/emulate.c:4828
    4828        ctxt->dst.type = saved_dst_type;
    (gdb) p ctxt->dst.val
    $64 = 48895
    (gdb) p &ctxt->dst.val
    $65 = (unsigned long *) 0xffff88011d429640
    (gdb) p &op->val
    No symbol "op" in current context.
    (gdb) n
    4830        if ((ctxt->d & SrcMask) == SrcSI)
    (gdb) p ctxt->dst.type
    $66 = OP_REG
    (gdb) n
    [New Thread 2976]
    4833        if ((ctxt->d & DstMask) == DstDI)
    (gdb) n
    [New Thread 2978]
    [New Thread 2977]
    4836        if (ctxt->rep_prefix && (ctxt->d & String)) {
    (gdb) n
    4866        ctxt->eip = ctxt->_eip;
    (gdb) n
    4875            writeback_registers(ctxt);
**四. 参考**
oenhan: [KVM源代码分析5:IO虚拟化之PIO](http://oenhan.com/kvm-src-5-io-pio)
Alex Xu: [使用KVM API实现Emulator
Demo](http://soulxu.github.io/blog/2014/08/11/use-kvm-api-write-emulator/)