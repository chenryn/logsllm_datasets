# 起初，有混沌
Casey Rosenthal, Backplane.io (formerly Netflix)
   服务中断，人们不得不面对困难。依赖该服务的客户感到沮丧，依赖该服务的其他系统停止工作，并对负责该系统的人员发出求助。历史表明[Amazon
AWS 在
2017年2月28日中断](https://techcrunch.com/2017/02/28/amazon-aws-s3-outage-is-breaking-things-for-a-lot-of-websites-and-apps/);
[Google Gdoc 在 2017/15/17
中断](https://www.washingtonpost.com/news/the-switch/wp/2017/11/15/google-docs-is-back-after-a-major-outage/)；[Facebook
2017年10月11日
经历故障](https://mashable.com/2017/10/11/facebook-is-down-oct-11/)；[Apple
iCloud 在
2018年6月6日发生故障](http://www.businessinsider.com/apples-icloud-service-is-having-technical-issues-2018-1)，即使是最著名的在线服务也容易受到中断的影响，即使有成百上千的人专用于其操作和正常运行时间。
随着软件的复杂性无可避免地增加，"软件进化第二定律"（[*https://ieeexplore.ieee.org/document/1456074/*](https://ieeexplore.ieee.org/document/1456074/)）指出，净复杂度将以类似于熵的方式增加，并与"必需变异定律"相结合（[*http://requisitevariety.co.uk/what-is-requisite-variety/*](http://requisitevariety.co.uk/what-is-requisite-variety/)），我们可以放心地认为，只要编写更多的软件，软件的复杂性就会增加。
防止错误和故障的旧方法被证明是不够的。
在不太遥远的过去，有关测试、代码样式和流程的最佳实践让我们确信，我们编写和部署的代码将执行我们期望它执行的操作。我们相信，诸如严格测试、测试驱动开发（TDD）、敏捷反馈循环、配对编程等实践可以帮助减少长期错误。像这样的实践仍然非常重要，但它们不足以用于工程现代复杂系统。
需要新的最佳实践，使我们再次对我们构建的系统充满信心。为了满足这一需求，正在出现最佳实践，*混沌工程*就是其中之一。混沌工程是
Netflix
开创的一门新学科，专为优化复杂分布式系统的*可用性*而设计。我们可以有信心，也可以设计它。
我在 Netflix
管理混沌团队三年，在此期间，我们正式建立了混沌工程，围绕它建立了一个社区，并将这一定义带到了整个行业。本章介绍威胁复杂分布式系统中可用性的问题类别。然后，我们回顾混沌工程的演变，以说明它是如何进化的，试图解决那类问题。我们列出了混沌工程中的五个更高级的概念，最后我们将用许多关于该主题的演示和研讨会的听众中精选的常见问题的解答结束本章。
# 系统问题
  可以将 Netflix
中的工程组织视为大约一百个小型工程团队，每个团队大约有五到七个人。服务（客户使用的产品）是通过用多达数百个微服务的协同工作而构建的。每个微服务都只由一个团队拥有，该团队负责该微服务的功能、路线图、运维和正常运行时间等一切。
微服务的一个示例可能是客户微服务，对于给定的客户
ID，该服务会提供与该人员一起存储的元数据。另一种可能是个性化服务，它存储与客户相关的偏好信息，以便当客户在
Netflix
上查找要观看的内容时，我们会根据他们之前观看的内容来优化他们的体验，等等。当然，代理、API
层和特定数据存储都可以是微服务。
想象一下，有一天，一位顾客在深夜在火车上看 *Stranger Things*
这个电影。让我们把此客户称为 CLR。在电影的某个片段，有一个可怕的场景让
CLR
大吃一惊，导致他的笔记本电脑掉落。他捡回了他的笔记本电脑，但节目不再流畅，所以他做任何合理的客户会做的事情，并疯狂地刷新他的浏览器。这不会瞬间奏效，所以他又次刷新了大约100次。
现在 CLR
在火车上，碰巧在两个基站之间，所以他目前暂时从互联网断开了。这些请求实际上是在
Web 浏览器和操作系统中排队。当互联网连接恢复时，所有 100
个请求将同时发送出去。
Netflix 方面会发生什么？请求进入代理，前进到 API
层，并导致相关的多个服务，如个性化服务，该服务提取用户 ID
并从客户服务请求该用户。
客户服务很大，因此它跨越许多节点的群集。将每个客户的数据存储在每个节点上是没有意义的，因此，ID
的一致性哈希将任何特定客户的请求定向到一个特定的主节点。由于团队负责其微服务的操作，因此他们还确保群集自动缩放以负责任地使用资源，并在节点出现故障时移交数据。它们还具有回退功能，可防止性能下降或错误；例如，如果客户服务无法从磁盘获取数据，则可以从内存中缓存中提供数据；如果无法从客户服务获得响应，则个性化服务可以提供默认用户体验。
回到 CLR 的情况，100
个请求突然同时到达个性化服务，该服务向客户服务发出相应的 100
个请求。所有 100
均始终进行哈希处理并转发到群集中的一个节点。此节点无法及时从磁盘获取数据，因此它执行明智的工作并从内存缓存中返回结果。这些结果可能有些过时，但这应该是可以接受的。
自动缩放规则查看 I/O 和 CPU
负载，并在平均工作负荷上升过快时向上扩展群集。相反，如果平均工作太低，它们就会缩小群集。在这种情况下，从内存缓存中提供服务的工作比从磁盘提取的工作要少得多，因此客户服务的平均
I/O 和 CPU
负载会下降。自动缩放规则执行负责任的操作并缩小群集，终止执行最少工作的节点，并将其数据转移到群集的其他成员。
个性化服务看到它发送给客户服务的最后一个请求没有及时完成（因为该节点正在关闭），因此它执行明智的任务并返回回退默认用户体验。
最后一个请求将返回到 CLR，前 99 个请求的响应已被忽略。CLR
现在得到的是默认体验，无法理解为什么它看起来与过去使用的个性化体验如此不同。甚至没有返回到节目之前断开时正在播放的点。于是
CLR 会做任何客户会做的事情：他再刷新浏览器 100 次。
循环重复：客户服务接收 100
个请求，从内存的缓存来返回服务数据，并触发自动缩放策略，导致缩小规模，个性化服务再次返回默认回退体验。现在，分配给这两个终止节点的其他客户也看到了默认体验，所以他们也开始刷新浏览器。我们现在遇到了一个由用户引起的重试风暴。额外的流量给客户服务带来了更大的压力，将整个系统转换为由内存数据返回的服务，从而显著降低平均负载，显著降低群集的自动扩展，直到其余节点无法为其提供服务功能与陈旧的数据，个性化服务停滞，整个服务崩溃，最终导致流媒体服务整个停顿。
以上完全是假设性的，没有发生。但这样的事情*可能发生*。到底出了什么问题？
从这样的例子中吸取的重要教训是，没有人做出错误的决定。工程师很聪明，他们遵循合理的行业最佳实践。他们没有犯错，但是系统的整体行为产生了一个不良的结果。
 根据定义，一个复杂的系统是一个无法理解整体的系统。一个复杂的系统是不合理的：你不能对此作出推理。这是
Netflix
没有首席架构师或类似角色的原因之一：没有一个人（其实也是系统中的组件）可以同时在脑中容纳所有运动部件的模型，从而对系统的行为具有任何预测能力。
那么，我们如何防止或尽量减少系统的不良行为呢？有两个逻辑选项：降低复杂性，避免成为一个复杂的系统，或者找到另一种方法来指导系统，即使不了解它是如何在复杂的细节中工作。我们称之为后一个选项*导航复杂性*。
我不知道任何实用的，广义的理论，使一个复杂的系统不复杂。由于我们在系统中想要的许多功能（功能速度、性能、可用性等）必然带来复杂性，因此我对这条路线感到怀疑，就算我们的意图是使系统*不复杂*，说不定同时也会使系统变得更糟。
然后，我们可以专注于另一个途径：学习了解复杂性，而不是试图降低复杂性。  