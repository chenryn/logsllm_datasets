this module simply makes a hypercall with an unused hy-
percall number (Dom0 communicates with the hypervisor
through hypercalls). The handler for this particular hyper-
call will perform the hypervisor disengagement for that core
and send an IPI to the other cores allocated to the VM to
signal to them that they need to perform the core-speciﬁc
disengagement functionality.
There are three main functions of this process. First, it
must take steps to remove the VM from several lists (such
as timers which are providing timer interrupts to the guest)
as well as remove the set of cores from the online proces-
sor cores mask (so Xen does not attempt to send any IPIs
to it). Second, the disengagement function must conﬁgure
the hardware such that the guest has full control over the
individual core. This includes settings within the virtual
machine control structure (VMCS)6 (e.g., setting the vir-
tualize APIC access bit to 0) as well as mappings in the
extended page tables (e.g., adding the local APIC’s memory
range so that it does not cause an EPT violation). Finally,
we must initialize the local APIC registers with values that
match what the guest operating system wrote. Before this
disengagement, Xen makes use of the local APIC itself and
presents a virtualized view to the guest (e.g., Xen uses a one-
shot timer as its own timer source but presents a periodic
timer to our Linux kernel). The disengagement function sets
the registers in the actual local APIC with the values that
are stored in the virtualized local APIC.
6The VMCS is used to manage transitions between the guest
virtual machine and the hypervisor.
Once the hypervisor disengagement process completes, ex-
ecution returns to the guest VM where the disengagement-
initiator module is unloaded and the iSCSI drive(s) for the
customer is mounted. Execution control is then transferred
(for the ﬁrst time) to the user’s code.
5.4 Guest Execution and Shutdown
At this point, execution is under the complete control of
the guest, as shown in Figure 3(d). It can run applications
and load OS kernel modules. We have conﬁgured the system
such that anything the virtual machine may need to do, it
will be able to do. We consider any other actions to be illegal
(and potentially malicious). Many actions, such as accessing
memory outside of the allocated memory range, will cause
an exit from the VM. Since we consider them illegal, they
will result in the termination of this VM. Other actions, such
as sending messages to the physical function driver via the
mailbox functionality on a device, can be ignored.
Because of this restriction, we needed to modify the guest
Linux kernel slightly – these modiﬁcations do not aﬀect an
application or kernel module’s interaction with Linux. That
is not to say we must trust the guest OS for the security
of the entire system, simply that in order for the guest VM
to run without causing a VM exit, Linux is conﬁgured to
not access devices that it is not using. In particular, Linux
assumes the presence of a VGA console and writes to the
ﬁxed I/O port whether there is a VGA console or not. We
modiﬁed this assumption and instead made the use of a
VGA console conﬁgurable. Additionally, Linux makes the
assumption that if an HPET device is available for deter-
mining the clock frequency, it should be added to the list of
clock sources. Each of the clocks in this list are periodically
queried for its cycle count. As we have a time stamp counter
(TSC) also available, the HPET is not needed. We added a
conﬁguration in Linux to specify whether the HPET device
is to be added to the list of clock sources or not.
However, one limitation of completely removing the avail-
ability of an HPET device is that we are now relying on the
local APIC timer. In processors before the current genera-
tion, this local APIC timer would stop when the core goes
into a deep idle state. Recent processors with the Always
Running APIC Timer (ARAT) feature are not subject to
this limitation. Unfortunately, we built our prototype on
a processor without this feature. To overcome this, rather
than buying a new processor, we simply faked that we have
it by (i) specifying that the processor has the ARAT ca-
pability in the response to the CPUID instruction, and (ii)
using the Linux parameter, max cstate, to tell Linux to not
enter a deep idle state (clearly not ideal, but acceptable for
a prototype).
Our timeline ends when the guest VM shuts down. A
guest VM can initiate the shutdown sequence itself from
within the VM. This will eventually result in an exit from
the VM, at which point the VM is terminated. However, we
cannot rely on customers to cleanly shutdown when their
time is up. Instead, we need to be able to force the shut-
down of a VM. We realize this by conﬁguring the VMCS to
specify that the VM should exit when the core receives a
non-maskable interrupt (NMI). In this way, the hypervisor,
restricted to running core 0 at this point, can send an NMI,
forcing a VM exit, giving the VM exit handler the ability to
shutdown the VM.
4085.5 Raw Performance Evaluation
Our prototype was built as a demonstration that we can
actually realize NoHype on today’s commodity hardware.
In addition to the security beneﬁts, which we analyze in
Section 6, removing a layer of software leads to performance
improvements, since with NoHype, there will no longer be
the number of VM exits as seen in Figure 2.
We experimented with both the SPEC benchmarks which
analyze the performance of the system under diﬀerent work-
loads as well as a VM running Apache to capture a common
workload seen in cloud infrastructures today. In each case
we ran the experiment with both NoHype and stock Xen
with a hardware virtual machine guest. With the NoHype
system, we utilized our modiﬁed Linux kernel, whereas in
the Xen system we utilized the unmodiﬁed 2.6.35.4 Linux
kernel. Each VM was conﬁgured with two cores, 4GB of
memory, and two network cards that were passed through
(one an Internet facing NIC, and one for communicating
with the iSCSI server). There was no device emulation and
no other VMs were running on the system which might in-
terfere with performance.
Shown in Figure 4 are the results of our experiments. We
saw an approximately 1% performance improvement across
the board. The lone exception to this was the gcc bench-
mark, which saw better performance with Xen than with
NoHype. We need to further investigate the cause of this,
but believe it to be related to our modiﬁcations to the guest
kernel. Also note that much of the major performance bot-
tlenecks associated with virtualization are alleviated with
the VT-d (to directly assign devices) and EPT (to allow
VMs to manage page tables) technologies and therefore al-
ready used in Xen. Our performance improvement comes
from removal of the VM exits and is on top of performance
gained from using VT-d and EPT hardware.
Figure 4: Raw performance of NoHype vs. Xen.
6. SECURITY ANALYSIS
In this section we present a security analysis of our No-
Hype architecture and its realization on today’s commodity
hardware. Our conclusion is that NoHype makes an impor-
tant improvement in the security of virtualization technol-
ogy for hosted and shared cloud infrastructures, even with
the limitations due to today’s commodity hardware.
6.1 Remaining Hypervisor Attack Surface
A NoHype system still requires system management soft-
ware (performing some of today’s hypervisor’s duties) to be
running on each server. While defending the interaction be-
tween the cloud manager and the system manager is our
future work, here we have concentrated on the surface be-
tween the guest VM and the hypervisor which is much larger
and more frequently used.
To initialize the guest VM, we use a temporary hypervi-
sor and a slightly modiﬁed guest OS for performing system
discovery tasks. The initial guest OS kernel is supplied and
loaded by the cloud infrastructure provider, thus the cus-
tomer has no control over the OS kernel which interacts
with the temporary hypervisor. The temporary hypervisor
is disabled (i.e., the VM is disengaged) before switching to
the customer’s code. By the time the customer’s code runs,
it does not require any services of the hypervisor (the system
discovery data is cached and the devices, memory and cores
are assigned). Any VM exit will trigger our kill VM routine
as previously described, thus denying a malicious customer
the opportunity to use a VM exit as an attack vector.
The ability of the guest VM to do something illegal, cause
a VM exit, and trigger system management software to take
action is itself a possible attack vector. The code handling
this condition is in the trusted computing base (TCB) of
a NoHype system, however, it is quite simple. After the
VM is disengaged we set a ﬂag in memory indicating that
any VM exit should cause the VM to be terminated. The
code base found in the temporary hypervisor and privileged
system manger is never triggered by the guest VM after dis-
engagement. Similarly, it cannot be triggered by any other
running VM, as an exit from that VM will only trigger the
kill VM routine.
6.2 VM to VM Attack Surface
After disengaging a VM, we give it full access to interpro-
cessor interrupts (IPIs). One limitation of today’s hardware
is that there is no hardware mask which can be set to prevent
a core from sending an IPI to another core. This introduces
a new but limited ability for VM to VM communication.
Now the VM has the ability to send an IPI to any other
core without that core being able mask it or even know who
sent it. The system management software which is pinned
to one of the cores may also be a target of such an attack.
A preliminary pragmatic solution is presented here. Since
the IPI does not contain any data and is only used as a trig-
ger, the guest OS can defend against this attack by slightly
modifying the way it handles IPIs. For each type of IPI, a
shared region in memory can hold a ﬂag that can be set by
the sending core and then checked and cleared by the receiv-
ing core. Given that no VM can access memory of another
VM, an attacker will not have the ability to set any of these
ﬂags. Therefore, the receiver of the IPI can simply ignore
the IPI if the ﬂag is not set.
While this ability for guest VMs to send IPIs poses very
little security risk, it has the potential to enable an attacker
to launch a denial of service attack by constantly sending
IPIs to a given core. Fortunately, the extent to which they
can possibly degrade performance is extremely limited. We
set up an experiment by conﬁguring an attacker VM with
up to 8 cores, each sending IPIs at their maximum rate, and
a victim with 2 cores. The results, shown in Figure 5, show
that the performance degradation is limited to about 1%.
Note that while we experimented with 8 cores, using 4 at-
tacker cores was suﬃcient to saturate the rate at which IPIs
can be sent in our test system. We used the Apache bench-
409To quantify the quality of memory isolation, we ran one
VM with a varying workload and examined the performance
of a second VM with a ﬁxed workload. In particular, the
ﬁxed workload VM had the memory intensive 429.mcf bench-
mark from SPEC 2006 suite running. In the varying work-
load VM we experimented with both Apache, under varying
number of concurrent requests, as well as varying the num-
ber of instances of the 429.mcf benchmark. The results are
shown in Figure 6. This experiment can be viewed either
in terms of (i) an attacker attempting to aﬀect the perfor-
mance of a victim, or (ii) an attacker attempting to learn
information about the victim (i.e., utilize it as a side channel
to answer questions such as “how loaded is my competitor”).
In either case, there is some interference between each work-
load, but we believe the interference is not signiﬁcant enough
to completely deny service or to learn sensitive information
such as cryptographic keys [17].
6.4 VMs Mapping Physical Infrastructures
Work by Ristenpart, et al., [30] has raised the concern of
infrastructure mapping attacks. With NoHype, the guest
VMs have a more direct interaction with the hardware and
could abuse that to try to map an infrastructure. One ex-
ample may be a malicious VM reading the APIC ID num-
bers to identify the underlying physical cores and use that
information to help narrow down where in the provider’s in-
frastructure the VM is located. This may be mitigated by
randomizing APIC IDs of the cores (which can be done at
system boot time). Even if a malicious VM is able to deter-
mine that it is co-located with a victim VM, our approach
of eliminating the attack surface denies it the opportunity
to attack the hypervisor and by extension the victim VM.
7. RELATED WORK
The related work can be categorized in four main areas:
minimizing the hypervisor, proposing a new processor ar-
chitecture, hardening the hypervisor, or giving VMs more
direct access to hardware.
Minimizing the hypervisor: Work on minimizing hy-
pervisors aims to reduce the amount of code within the
hypervisor, which should translate to fewer bugs and vul-
nerabilities. One example is SecVisor [32], a hypervisor
which supports a single guest VM and protects that VM
from rootkits. Another example is TrustVisor [26] which is
a special-purpose hypervisor for protecting code and data
integrity of selected portions of the application. Previous
minimal hypervisors are not practical for deployment in the
hosted cloud computing model where multiple VMs from
multiple customers run on the same server. With NoHype,
we show how to remove attack vectors (in eﬀect also re-
ducing the hypervisor) while still being able to support the
hosted cloud computing model.
New processor architectures:
In another approach,
researchers propose building new processor architectures
which explicitly oﬀer new hardware functionality for improv-
ing security. Much work has been done on new hardware
mechanisms for protecting applications and trusted software
modules [24, 34, 21, 12], including Bastion [10] which uses a
full hypervisor. Unfortunately, such approaches do require
new microprocessors and cannot be deployed today, unlike
our solution. Additionally, the use model for cloud com-
puting has some similarities with that of mainframes. The
architectures targeting these systems, such as the IBM Sys-
Figure 5: Eﬀect of IPI attack on benchmarks.
mark and a compute intensive benchmark (462.libquantum)
from the SPEC 2006 suite. Because the bus used to deliver
IPIs is the same bus used for interrupts (the APIC bus), the
performance of Apache was aﬀected slightly more because it
is a more interrupt driven application. The 462.libquantum
benchmark is compute intensive and captures the overhead
of simply having to process each interrupt.
6.3 Isolation between VMs
With NoHype, we rely on hardware mechanisms to pro-
vide isolation of access to shared resources. Most important
are the conﬁdentiality and integrity of each VM’s memory.
We utilize Xen’s code which pre-sets the entries in the ex-