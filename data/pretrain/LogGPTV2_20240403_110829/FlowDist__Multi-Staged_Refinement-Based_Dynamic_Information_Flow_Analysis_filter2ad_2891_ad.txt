NioEcho
21
MultiChat
0
ADEN
0
Raining Sockets
3
OpenChord
0
Thrift
0
xSocket
8
Zookeeper Integration
0
Zookeeper Load
1
Zookeeper System
0
RocketMQ Integration
23
RocketMQ System
0
Voldemort Integration
30
Voldemort Load
0
Voldemort System
30
Netty
3
HSQLDB Integration
10
HSQLDB System
2
6.3 Results and Analysis
We now discuss our results for each RQ. We focus on major
ﬁndings while discussing the key implications and insights.
12
12
5
0
24
4
26
33
6522
1116
46
187
193
6
77
7
668
11
6
0
0
0
0
3
2
0
64
46
17
50
138
0
42
2
0
4
6.3.1 RQ1: Effectiveness (Precision/Recall)
Table 2 shows the number of source-sink pairs covered in
each execution (i.e., subject-test type) and that of information
ﬂow paths between the pairs, separately for intraprocess
and interprocess paths. For each source/sink given in the
conﬁguration C, FLOWDIST treated each of its exercised
callsites as a separate source/sink in counting the pairs and
computing the paths. The last column shows the percentage
of interprocess paths over all information ﬂow paths per
execution. The rows for executions without any information
ﬂow paths found are greyed.
The numbers of exercised source/sink pairs and information
ﬂow paths varied widely and were generally independent of
subject size and input type. In 5 of the 18 cases (executions),
FLOWDIST found no sensitive ﬂow (e.g., for Voldemort-Load). In
the other 13 cases, the paths were all true positives. Between
any of the pairs in Thrift, Voldemort-Load, and Netty—the cases with
smallest total numbers of pairs, we found no path beyond
those found by FLOWDIST. Thus, the precision and recall
were both 100% for the manually validated samples.
The majority (74% on average) of all of the reported
paths were interprocess ones—in 7 cases the percentage
was above 50% and in 3 cases 100%. This implies that, by
only analyzing dynamic information ﬂows within individual
processes, a conventional DIFA/DTA would miss most of the
sensitive ﬂows in distributed program executions. This result
also provides an alternative measure of recall of FLOWDIST
versus single-process DIFA/DTA, and indicates the much
higher recall of our approach.
USENIX Association
30th USENIX Security Symposium    2101
More generally, while our evaluation on recall was limited
due to the lack of ground truth and the impracticality of
manually curating it for all queries (especially for large
systems with complex executions), high recall (hence a low
false negative rate) is crucial, especially in the context of
ﬁnding security vulnerabilities. Meanwhile, we note that,
relative to a static approach, the generally lower recall of
a dynamic technique like ours is mainly attributed to the
limited coverage of run-time inputs considered. On the other
hand, a dynamic analysis is expected in nature to focus on
the particular inputs (hence the speciﬁc executions) given
by users. Thus, the input coverage problem is considered
orthogonal to the design of a dynamic analysis [78]. With
respect to the given executions, both our manual validation
for RQ1 and evaluations against real vulnerability cases for
following RQs conﬁrmed that FLOWDIST found all of the
information ﬂow paths and related vulnerabilities, suggesting
no false negatives for those executions.
In addition, the precision and recall of a hybrid analysis
(as is the Step 2.3 of FLOWDIST) often compete with each
other [84]. However, in our approach, we strive for precision
improvement over a purely dynamic dependence analysis
based on method-level control ﬂows, by conservatively
pruning static dependencies with those exercised control
ﬂows. This conservative nature leads to the ability of
FLOWDIST to retain recall when gaining in precision.
Interprocess ﬂow analysis is essential for a DIFA/DTA of
common distributed systems. Manual validation suggested
FLOWDIST’s very-high precision and promising recall.
6.3.2 RQ2: Efﬁciency (Time/Storage Costs)
Table 3 gives the breakdowns of the time and storage costs of
FLOWDIST over its two phases and further over the steps of
each phase. The time costs include those for static analysis
(and instrumentation if any) (St.), proﬁling (Run), and on
average for computing the (method- or statement-level) paths
between each source-sink pair (Query). The second column
lists the original run time (Norm Run) of each execution,
from which proﬁling overheads were computed as runtime
slowdown ratios (Slowdown). The eighth column shows the
time for coverage analysis (Co.). The last column is the
total storage cost (Storage) for all phases per execution—for
storing the traces of method and branch events in Phase 1,
statement coverage and partial static dependence graph in
Phase 2, as well as the instrumented program. The overall
averages (across all executions) are given in the bottom row.
On average over the 18 cases (executions), FLOWDIST
took 19 minutes for all one-off analyses, including the
time for all static analyses, instrumentation, and coverage
analysis. We considered them one-off because their results
are shared by all queries with respect to a given subject
execution and source/sink conﬁguration. In particular, the
partial dependence analysis (as guided by the method-level
Figure 6: The total analysis time (seconds, y axis) versus
subject size (#SLOC, x axis) of all subjects (integration test).
Figure 7: The run-time slowdowns (%, y axis) versus #method
execution event instances (x axis) of all subject executions.
paths from Phase 1) was signiﬁcantly more efﬁcient than a
whole-system analysis (without a pre-analysis). For instance,
per our additional experiments, the latter did not even ﬁnish
in 12 hours with otherwise the same setup against Voldemort.
For proﬁling, FLOWDIST caused an average of 68%
slowdown calculated as (Ti-To)/To where Ti and To is the run
time of the instrumented and original program, respectively.
The time cost for querying each source/sink pair was 13
seconds on average, with a maximum of 50 seconds seen
by HSQLDB-System mainly because of its static dependence
complexity. Note that this cost was dominated by building
the dynamic dependence graph from its static counterpart
and an instance-level method execution event sequence
(Algorithm 2), whose time expense depends on the scale of
the graph and the length of the sequence.
The storage costs of FLOWDIST were all insigniﬁcant.
FLOWDIST is promisingly efﬁcient and scalable to large
systems, taking on average 19 minutes by one-off analyses
and 13 seconds to query for a source/sink pair while
causing <1x slowdown and a negligible storage cost.
6.3.3 RQ3: Scalability
We ﬁrst look at how FLOWDIST scaled to subjects of growing
sizes in terms of its total time cost (the sum of one-off analysis
time, proﬁling costs, and the time for querying all possible
source/sink pairs), against integration tests since every subject
has such a test. Figure 6 shows the ﬁtting curve, along with
the determination coefﬁcient R2 ∈[0,1] which indicates how
close the data are to the curve. The closer R2 is to 1, the better
the ﬁtting is. As shown, FLOWDIST’s time cost grew linearly.
We then look at the scalability of FLOWDIST in terms of its
runtime slowdown, for all 18 executions each characterized
by the length of the instance-level method execution event
2102    30th USENIX Security Symposium
USENIX Association
Table 3: Time (in seconds) and storage (in MB) costs of FLOWDIST
Executions
NioEcho
MultiChat
ADEN
Raining Sockets.
OpenChord
Thrift
xSocket
Zookeeper Integration
Zookeeper Load
Zookeeper System
RocketMQ Integration
RocketMQ System
Voldemort Integration
Voldemort Load
Voldemort System
Netty
HSQLDB Integration
HSQLDB System
Overall Average
Norm
Run
39
26
21
6
54
8
11
71
99
98
105
339
28
11
31
12
9
15
55
St.
53
55
117
40
177
146
101
292
292
292
56
156
1206
1206
1206
1132
659
684
437
Phase 1 Time
Run Slowdown Query
0.2
0.2
0.3
0.3
0.3
0.5
0.5
0.5
0.6
0.5
0.6
0.6
0.6
0.6
0.6
0.6
0.7
0.7
0.5
5.16%
6.12%
10.23%
7.67%
8.54%
24.83%
63.99%
70.16%
78.83%
81.87%
87.05%
122.09%
106.06%
113.37%
109.81%
81.65%
107.46%
142.71%
68.20%
41
28
23
6
59
10
19
121
177
178
196
753
58
23
65
22
19
36
102
Phase 2 Time
St.
50
50
59
122
740
79
70
193