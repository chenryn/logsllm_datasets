v2
v3
v4
v5
v6
v7
v8
v9
" ( ∗ \ b f s e r i e s Sample 1 ∗ ) : "
[ " LdrUnloadDll :0= IMM32 " , " LdrGetDllHandle : 0 = 0 " ,
" GetNativeSystemInfo : 0 = 1 " , " NtOpenKey :0=0 x00000001 " ,
" NtOpenKey :1= o t h e r _ r e g " , " G e t F i l e A t t r i b u t e s :0= −1"]
( ∗ $ \ hspace { 1 0 0 pt } \ downarrow$ ∗ )
" Sample 1 " :
[ v1 , v2 , v3 , v4 , None , None , v7 , v8 , None ]
3○−−−−−−−−−−−−−−→
Feature_Vector
"Sample 1": [111100110]
↓ 2○Transform
Figure 2: Hashing Vectorizer function example.
each sample using Method 2 (total of nine features). The Hash-
ing Vectorizer function works in two steps, Fit and Transform. Fit
starts by identifying all unique features in corpus and hashing each
feature (i.e., the used hash function is signed 32-bit version of Mur-
murhash3 [53]). As a result, the hash table and feature space will be
formed, as shown in the first step in Figure 2. Then, in the second
step, Transform starts by hashing each feature in each sample of the
corpus. Based on the hash table, which is formed by the Fit function,
it matches each feature’s hash value with that of the hash table.
Thus, it gets the corresponding index of the matched hash in the
hash table. Lastly, a value of 1 will be assigned to the index, form-
ing the feature bit-vector. Step 3 in Figure 2 shows Sample 1 after
rearranging the features in ascending order (Feature_Vector).
4 EXPERIMENTS
4.1 Datasets
Our datasets include 7105 and 7774, malicious and benign samples,
respectively. The malicious samples are obtained from the Malshare
website [27] using a daily downloading script. Each sample is then
validated and archived by its date using VirusTotal [30]. Five an-
tivirus engines need to vote that the sample is malicious to be
considered in our dataset (i.e., Microsoft, Malwarebytes, Symantec,
Sophos, and Trend Micro). As described before, malware samples
with the same type have similar features and behavior. However,
getting the malware type’s ground-truth label is non-trivial, where
multiple anti-virus vendors would give different detection names
60ACSAC 2020, December 7–11, 2020, Austin, USA
Dima Rabadi and Sin G. Teo
Sample
Malicious
Benign
Table 2: Dataset description.
Type
Trojan
Adware
Spyware
Ransom
Backdoor
Virus
Worm
PUP
Hack Tool
Riskware
DLL files
APIMDS
CNET
Windows executable
Portable applications
File Hippo
CYGWIN
WINDOWS 7
Total
No. of samples
5485
295
381
441
191
99
152
36
14
11
327
1069
200
173
300
43
4631
1031
14879
%
36.864
1.983
2.561
2.964
1.284
0.665
1.022
0.242
0.094
0.074
2.198
7.185
1.344
1.163
2.016
0.289
31.124
6.929
-
(types) for the same scanned sample. Thus, researchers start looking
into another way of labeling the malware samples. For example,
in [48], Sebastián et al. implement AVClass, which is an open-source
and automatic tool that takes a malware sample and returns its
type label with a confidence factor calculated by leveraging the
anti-virus labels obtained from VirusTotal and the agreement ob-
tained across its engines. The ground-truth labeling is out of this
paper scope, where for our malware dataset, the malware type of
each sample is named based on the information given by Malware-
bytes [28] engine in VirusTotal. The age of our malicious samples is
between January 2019 and March 2020, based on VirusTotal results.
The malware types and the number of samples in each type are
presented in Table 2. A description of each malware type used in
this paper is shown in Table 6 in Appendix C.
The benign samples are obtained from eight different sources.
(1) After immediately installing a fresh version of Windows 7, we
extracted the Native Windows executable, and (2) the Dynamic Link
Library (DLL) files which are located in C:\ Windows\ Systems32
directory. (3) We downloaded the APIMDS dataset [55]. (4) We used
websites for free-to-try legal downloads (e.g., download.cnet.com
and softpedia.com). (5) We downloaded the top 300 portable appli-
cations for Windows [19] and (6) the top 43 applications from the
File Hippo Website [50]. (7) We used the benign dataset from [23],
which contains two folders, CYGWIN [8] and (8) WINDOWS 7
benign samples. Both folders contain Windows executable files that
are copied from the authors directly. All benign samples from the
eight sources are also validated using VirusTotal. The number of
samples of each benign source is presented in Table 2.
4.2 Method Evaluation
In this section, we evaluate the efficacy of our proposed methods
in detecting the malware from benign samples and then classifying
them into their respective types. The benign dataset is split into
6220 (80 % of the dataset) and 1554 (20 % of the dataset) samples for
training and testing, respectively. Similarly, for a fair evaluation,
each malware type dataset is split into training and testing datasets,
which consist of 80 % and 20 % of the samples, respectively. Thus,
in total, we have 5687 and 1418 malware samples for training and
testing, respectively.
4.2.1 Malicious Behavior Detection. In this set of experiments, we
investigate the performance of Method 1 and Method 2 in detect-
ing malicious from benign samples. To do that, we use the 5687
and 6220, malicious and benign training datasets, respectively, to
train models using five different machine learning algorithms (i.e.,
SVM, XGBoost, RF, DT, and PA). To avoid the overfitting, we ap-
ply 5-fold cross-validation over the malicious and benign training
datasets using the above machine learning algorithms. The models
are then used to test the 1418 and 1554 malicious and benign testing
datasets, respectively. The following standard machine learning
performance metrics are used to evaluate the performance of the
proposed methods:
(1) False Positive Rate (FPR): the ratio of falsely classifying be-
(2) False Negative Rate (FNR): the ratio of falsely classifying
nign samples as malicious.
malicious samples as benign.
(3) Accuracy: how often is our method correct, which denotes
the rate of correct outcomes (benign and malicious) out of
the observed samples.
(4) Standard Deviation (STD): a positive or negative value repre-
sents how the array of scores of the estimator for each run of
the cross-validation may differ from the mean value (Mean
Accuracy).
Table 3 shows Method 1 and Method 2 performance results.
As shown in the table, XGBoost achieves the best accuracy score
among the five machine learning algorithms using our proposed
Method 1 and Method 2, where Method 1 gets 99.8655 and Method 2
gets 99.8992. Thus, we further run the XGBoost experiments using
the 10-fold cross-validation. Method 1 and Method 2 performance
results are then evaluated in terms of FPR, FNR, and:
(1) Precision: the ratio of correctly predicted positive observa-
tions to the total predicted positive observation given by
T P +F P
(cid:17).
(cid:16) T P
over all malicious samples(cid:16)
by(cid:16)2 · Pr ecision·Recall
(cid:17).
Pr ecision+Recall
(cid:17).
T P
T P +F N
(2) Recall: the ratio of the correctly detected malicious samples
(3) F1 score: the harmonic mean of precision and recall given
Table 4 shows the results. A comparison between the perfor-
mance of Method 1 and Method 2 using XGBoost and discussion on
their misclassification samples will be presented in the following.
4.2.2 Methods Performance and Misclassifications. In this section,
we present the performance difference of Method 1 and Method 2
using XGBoost, and under which circumstances Method 2 would
outperform Method 1. As described before, the two methods serve
the same purpose, where the main difference between them lies in
the tokenization strategies. Method 1 considers the entire list of
arguments of each API call as one token, while Method 2 considers
61Advanced Windows Methods on Malware Detection and Classification
ACSAC 2020, December 7–11, 2020, Austin, USA
Table 3: The malicious behavior detection experimental results.
Machine
Learning Algorithms
SVM
Random Forest
Passive
Aggressive
Decision Tree
XGBoost
Method
FPR
1
2
1
2
1
2
1
2
1
2
0.00192
0.00192
0.00836
0.00385
0.00257
0.00321
0.00385
0.01221
0.00128
0.00128
FNR
0.00985
0.00422
0.01900
0.00633
0.01337
0.00422
0.00492
0.00422
0.00140
0.00070
Accuracy
99.4287
99.6976
98.6559
99.4959
99.2271
99.6303
99.5631
99.1599
99.8655
99.8992
Mean Accuracy
5-fold cross-validation
(±) STD
0.079
0.106
0.078
0.116
0.088
0.111
0.245
0.133
0.128
0.086
96.2
96.0
95.4
94.4
96.0
95.7
92.1
95.2
95.7
96.7
Table 4: The malicious behavior detection - XGBoost experimental results.
Method
Method 1
Method 2
Type
Benign
Malicious
Total
Benign
Malicious
Total
FPR
0.00128
-
-
0.00128
0.00128
0.00128
FNR
-
-
0.00141
0.00141
0.00070
0.00070
Precision Recall
99.87
99.86
99.87
99.87
99.93
99.90
99.87
99.86
99.87
99.94
99.86
99.90
F1 Score
99.87
99.86
99.87
99.90
99.89
99.90
10 fold cross-validation
(±)STD
Mean Accuracy
0.024
99.2
99.4
0.020
each argument of each AP call separately as one feature. If the sam-
ple has called a small number of API calls, but the number of the
passed arguments for each API call is high, then Method 2 outper-
forms Method 1, as the large number of arguments for each call can
compensate the small number of total API calls. This observation