only a thin kernel layer on each system core.
Removing the kernel from fast-path IPC also removes the
additional inefﬁciency of cross-core IPC that is, paradoxically,
only noticeable because there is no longer a context switch.
Single core systems partially hide IPC overhead behind the
context switch. If a server needs another server to process a
request, that process must be run ﬁrst. Therefore, the trap to
the kernel to send a message is the same as needed for the
context switch, so some of the overhead is “hidden”.
On multicores, context switching no longer hides the cost
of IPC and the latency of the IPC increases because of the
intercore communication. The kernel copies the message and
if one of the communicating processes is blocked receiving, it
must wake it up. Doing so typically requires an interprocessor
interrupt which adds to the total cost and latency of the IPC.
If enough cores are available, we can exclude the kernel
from the IPC. Our measurements show that doing so reduces
the overhead of cross-core communication dramatically.
B. Asynchrony for Performance and Reliability
A monolithic system handles user space requests on
the same core as where the application runs. Many cores
may execute the same parts of the kernel and access the
same data simultaneously, which leads to lock contention
to prevent races and data corruption. We do not require
CPU concurrency per server and event-driven servers are
fast and arguably less complex than threads (synchronization,
preemption, switching, etc.) and help avoid concurrency bugs.
For us, single threaded servers are a good design choice.
However, synchronous communication between the servers
(blocking until receiving a reply), as used in most multiserver
systems may well make the entire system single threaded
in practice. Thus, dedicating a separate core to each server
reduces the communication cost but does not scale further.
Ideally, we would like the cores to process tasks in parallel
if there is work to do. To do so, the servers must work as
independently of each other as possible to increase intra-OS
parallelism. Only asynchronous servers can process other
pending requests while they wait for responses from others.
An argument against asynchrony is that it is difﬁcult to
determine whether a process is just slow or whether it is
dead. However, a multiserver system, unlike a distributed
system, runs on a single machine and can take advantage of
fast and reliable communication provided by the interconnect.
Together with the microkernel, it makes detection of such
anomalies much simpler.
Most microkernels provide synchronous IPC because it is
easy to implement and requires no buffering of messages. In
practice, support for asynchronous communication is either
inefﬁcient (e.g., Minix 3) or minimal. Speciﬁcally, the large
number of user-to-kernel mode switches results in signiﬁcant
slowdowns here also. In contrast, the communication channels
in our design increase asynchrony by making nonblocking
calls extremely cheap.
While asynchrony is thus needed for scalability on multi-
cores, it is equally important for dependability. The system
should never get blocked forever due to an unresponsive or
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
MMEXT2IP/ICMPAPPPMTCPUDPAPPVFSPFNetDrvAPP…APPAPPAPPdead server or driver. In our design, a misbehaving server
cannot block the system even if it hogs its core. Better
still, our asynchronous communication lets servers avoid
IPC deadlocks [38]. Since servers decide on their own from
which channel and when to receive messages (in contrast
to the usual receive from anyone IPC call), they can easily
handle a misbehaving server that would otherwise cause a
denial-of-service situation.
IV. FAST-PATH CHANNELS
The main change in our design is that instead of the tradi-
tional IPC mechanisms provided by the microkernel, we rely
on asynchronous channels for all fast-path communication.
This section presents details of our channel implementation
using shared memory on cache-coherent hardware. Shared
memory is currently the most efﬁcient communication option
for general-purpose architectures. However, there is nothing
fundamental about this choice and it is easy to change the
underlying mechanism. For instance, it is not unlikely that
future processor designs will not be fully cache coherent,
perhaps providing support for the sort of message passing
instead as provided by the Intel SCC [23]. Moreover, besides
convenient abstractions, our channels are generic building
blocks that can be used throughout the OS. By wrapping the
abstraction in a library, any component can set up channels
to any other component.
Our channel architecture has three basic parts: (1) queues
to pass requests from one component to another, (2) pools to
share large data, and (3) a database containing the requests
a component has injected in the channels and which we are
waiting for to complete or fail. We also provide an interface
to manage the channels. The architecture draws on FBufs [12]
and Streamline [10], but is different from either in how it
manages requests.
Queues: Each queue represents a unidirectional com-
munication channel between one sender and one consumer.
We must use two queues to set up communication in both
directions. Each ﬁlled slot on a queue is a marshalled request
(not unlike a remote procedure call) which tells the receiver
what to do next. Although we are not bound by the universal
size of messages the kernel allows and we can use different
slot sizes on different queues, all slots on one queue have
the same size. Thus we cannot pass arbitrarily sized data
through these channels.
We use a cache friendly queue implementation [17], [10],
that is, the head and tail pointers are in different cache lines to
prevent them from bouncing between cores. Since the queues
are single-producer, single-consumer they do not require any
locking and adding and removing requests is very fast. For
instance, on the test machine used in the evaluation section,
the cost of trapping to the kernel on a single core using the
SYSCALL instruction in a void Linux system call takes about
150 cycles if the caches are hot. The same call with cold
caches takes almost 3000 cycles. In contrast, on our channels
it requires as little as 30 cycles to asynchronously enqueue a
message in a queue between 2 processes on different cores
while the receiver keeps consuming the messages. The cost
includes the stall cycles to fetch the updated pointer to the
local cache.
Pools: We use shared memory pools to pass large
chunks of data and we use rich pointers to describe in what
pool and where in the pool to ﬁnd them. Unlike the queues
which are shared exclusively by the two communicating
processes, many processes can access the same pool. This
way we can pass large chunks from the original producer to
the consumers further down the line without the need to copy.
Being able to pass long chains of pointers and zero-copy are
mechanism crucial for good performance. All our pools are
exported read only to protect the original data.
Database of requests: As our servers are single-
threaded and asynchronous, we must remember what requests
we submitted on which channels and what data were
associated with each request. After receiving a reply, we
must match it to the corresponding request. For this purpose,
the architecture provides a lightweight request database that
generates a unique request identiﬁer for every request.
Our channel architecture also provides an interface to
publish the existence of the channels, to export them to a
process, and to attach to them. We discuss this in more detail
in Section IV-C.
A. Trustworthy Shared Memory Channels
Shared memory has been used for efﬁcient IPC for a long
time [12] and in many scenarios [11], [6], [10], [27]. The
question we address here is whether we can use it as a trusted
communication channel without harming dependability.
Kernel-level IPC guarantees that a destination process is
reliably informed about the source process. Our channels
offer the same guarantees. As servers must use the trusted
(and slower) kernel IPC to set up the channels (requesting
permission to export or attach to them), the kernel ensures that
processes cannot change the mappings to access and corrupt
other processes’ address spaces. Since a process cannot make
part of its address space available to another process all by
itself, setting up the shared memory channel involves a third
process, known as the virtual memory manager. Each server
implicitly trusts the virtual memory manager. Once a shared
memory region between two processes is set up, the source
is known.
Likewise, we argue that communication through shared
memory is as reliable as communication through the kernel.
In case the source is malicious or buggy, it can harm the
receiving process by changing data in the shared location
when the receiver was already cleared to use them. The
receiving process must check whether a request make sense
(e.g., contains a known operation code) and ignore invalid
ones. If the sender tampers with the payload data, the effect is
the same as if it produced wrong data to begin with. Although
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 07:17:40 UTC from IEEE Xplore.  Restrictions apply. 
incorrect data may be sent to the network or written to disk,
it does not compromise the network or disk driver.
In addition, we use write protection to prevent the con-
sumer from changing the original data. While the consumer
can, at any time, pass corrupted data to the next stage of a
stack, if a request fails or we need to repeat the request (e.g.,
after a component crash, as discussed in Section V), we can
always use the original data.
We must never block when we want to add a request
and the queue is full, as this may lead to deadlocks. Each
server may take its own unique action in such a situation.
For instance, dropping a packet within the network stack
is acceptable, while the storage stack should remember the
request until the congestion is resolved.
B. Monitoring Queues
If a core is busy, there is no problem to check the queues
for new requests. However, once a core is not fully loaded,
constant checking keeps consuming energy, even though there
is no work to do. Therefore, we put idle cores to sleep. But
the process must wake up immediately when there is more
work to do. The sender can use kernel IPC to notify the
receiver that a new request is available, but that is precisely
what we want to avoid. To break the circle, we use the
MONITOR and MWAIT pair of instructions, recently added to
the Intel x86 instruction set, to monitor writes to a memory
location while the core is idle. In addition to the shared
memory channels, each server exports the location it will
monitor at idle time, so the producers know where to write
to.
Unfortunately, these instructions are, available only in
privileged mode—so we must use the kernel
to sleep.
Although we only need the kernel’s assistance when a server
has no work to do and we want to halt the core, the overhead
of restoring the user context when a new request arrives
adds to the latency of the MWAIT. This fact encourages
more aggressive polling to avoid halting the core if the gap
between requests is short. Part of the latency is absorbed by
the queues we use to implement the communication channels.
If the MWAIT were optionally allowed in unprivileged mode,
we would get perfect energy consumption aware polling with
extremely low wake-up latency. In our opinion, the kernel
should be able to allow this instruction in an unprivileged
mode (as it can disable it in the privileged one) when it
knows that the core is dedicated to a process and thus this
process cannot prevent other processes from running when it
halts its core. Moreover, a core cannot be disabled entirely,
as an interrupt, for example from another core, can always
wake it up. Although such instructions are fairly unique to
x86, they prove so useful that we expect other architectures
to adopt variants of them in the future.
C. Channel Management
As there is no global manager in our system, the servers
must set up the channels themselves. After all, we do not
want our recovery mechanisms to depend on another server
which itself may crash. When a server starts, it announces its
presence by a publish-subscribe mechanism. Another server
subscribed to the published event can then export its channels
to the newly started one. Exporting a channel provides the
recipient with credentials to attach to it. In our case, it can
use the credentials to request the memory manager to map
it into its address space. A server can also detach from a
channel. This is only used when the other side of the channel
disappears. We never take a channel away from an active
server since it would crash after accessing the unmapped
memory. Pools are basically channels without the additional
queue structuring and the limit of how many processes can
attach to it, therefore we use the same management for both.
Because we use the pools to pass large chunks of data
without copying, not only the processes that communicate
immediately with each other must be able to attach pools.
Each channel is identiﬁed by its creator and a unique id. The
creator publishes the id as a key-value pair with a meaningful
string to which a server can subscribe. After obtaining the
identiﬁcation pair, the server can request an export of the
pool from its creator, which the creator can grant or deny.
D. Channels and Restarting Servers
When a server crashes and restarts it has to reattach
channels which were previously exported to it. Since the
channels exported by a crashed server are no longer valid,
their users need to detach from them and request new exports.
The identiﬁcation of the channels does not change.
We cannot hide the fact that a server crashed from the ones
it talked to since there may have been many requests pending
within the system. Reestablishing the channels to a server
which recovered from a crash is not enough. Servers that
kept running cannot be sure about the status of the requests
they issued and must take additional actions. We use the
request database to store each request and what to do with
it in such a situation. We call this an abort action (although
a server can also decide to reissue the request). When a