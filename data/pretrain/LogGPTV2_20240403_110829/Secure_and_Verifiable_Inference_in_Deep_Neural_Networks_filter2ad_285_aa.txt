title:Secure and Verifiable Inference in Deep Neural Networks
author:Guowen Xu and
Hongwei Li and
Hao Ren and
Jianfei Sun and
Shengmin Xu and
Jianting Ning and
Haomiao Yang and
Kan Yang and
Robert H. Deng
Secure and Verifiable Inference in Deep Neural Networks
University of Electronic Science and
University of Electronic Science and
University of Electronic Science and
University of Electronic Science and
Singapore University of Technology
Guowen Xu
PI:EMAIL
Technology of China
Jianfei Sun
PI:EMAIL
Technology of China
Haomiao Yang
PI:EMAIL
University of Electronic Science and
Technology of China
Hongwei Li∗
PI:EMAIL
Technology of China
Shengmin Xu
PI:EMAIL
and Design
Kan Yang
PI:EMAIL
The University of Memphis
Hao Ren
PI:EMAIL
Technology of China
Jianting Ning
PI:EMAIL
Fujian Normal University &
Singapore Management University
Robert H. Deng
PI:EMAIL
Singapore Management University
ABSTRACT
Outsourced inference service has enormously promoted the pop-
ularity of deep learning, and helped users to customize a range
of personalized applications. However, it also entails a variety of
security and privacy issues brought by untrusted service providers.
Particularly, a malicious adversary may violate user privacy during
the inference process, or worse, return incorrect results to the client
through compromising the integrity of the outsourced model. To ad-
dress these problems, we propose SecureDL to protect the model’s
integrity and user’s privacy in Deep Neural Networks (DNNs) in-
ference process. In SecureDL, we first transform complicated non-
linear activation functions of DNNs to low-degree polynomials.
Then, we give a novel method to generate sensitive-samples,
which can verify the integrity of a model’s parameters outsourced
to the server with high accuracy. Finally, We exploit Leveled Ho-
momorphic Encryption (LHE) to achieve the privacy-preserving
inference. We shown that our sensitive-samples are indeed very
sensitive to model changes, such that even a small change in parame-
ters can be reflected in the model outputs. Based on the experiments
conducted on real data and different types of attacks, we demon-
strate the superior performance of SecureDL in terms of detection
accuracy, inference accuracy, computation, and communication
overheads.
KEYWORDS
Privacy Protection, Deep Learning, Verifiable Inference
∗Corresponding author
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
ACSAC 2020, December 7–11, 2020, Austin, USA
© 2020 Association for Computing Machinery.
ACM ISBN 978-1-4503-8858-0/20/12...$15.00
https://doi.org/10.1145/3427228.3427232
ACM Reference Format:
Guowen Xu, Hongwei Li, Hao Ren, Jianfei Sun, Shengmin Xu, Jianting
Ning, Haomiao Yang, Kan Yang, and Robert H. Deng. 2020. Secure and
Verifiable Inference in Deep Neural Networks. In Annual Computer Security
Applications Conference (ACSAC 2020), December 7–11, 2020, Austin, USA.
ACM, New York, NY, USA, 14 pages. https://doi.org/10.1145/3427228.3427232
1 INTRODUCTION
Deep learning(DL), as one of the promising emerging technolo-
gies, has penetrated all aspects of social life, such as face recogni-
tion [4, 29], autopilot [19, 63], and medical diagnosis [22, 35, 59].
To support automated services, many tech companies (such as
Google, Microsoft, and Amazon) provide outsourced deep learning
services, usually dubbed as Machine Learning as a Service (MLaaS)
[24, 60, 63]. MLaaS can provide a series of customized training
and inference services, along with requiring users to provide local
data. A typical example is Azure ML Studio [4], which is devel-
oped by Microsoft and enables customers to easily build, deploy,
and share advanced deep leaning-based algorithms in the cloud.
Other platforms, such as TensorFlow, Caffe2 and MXNet, offer sim-
ilar services for a fee. Despite such advantages, outsourcing deep
learning to the cloud also brings about various security and pri-
vacy concerns [2, 29]. Particularly in inference services, customers
are very concerned about their model’s integrity and data privacy
once outsourcing them to the cloud. An adversary, such as an un-
trusted cloud server, may return incorrect results to users by making
some imperceptible modifications to the outsourced model. Such
attacks have appeared in various applications including face recog-
nition and image classification [30, 31]. On the other hand, privacy
breaches in outsourcing inference services have been frequently
reported in the media [4, 29]. Intuitively, once a user outsources its
model to a cloud server, it is possible that the server will steal the
intellectual property (i.e., parameters) of the outsourcing model, or
collect the user’s query history through the released API.
To address the above problems, several approaches have been
proposed to mitigate privacy and security threats in DNNs [19, 49,
63]. For example, Ghodsi et al. propose SafetyNets [63], the first
approach for verifiable execution of DNNs on an untrusted cloud.
784ACSAC 2020, December 7–11, 2020, Austin, USA
Guowen Xu et al.
Recent works [10, 27, 49] also achieve similar goals utilizing diverse
technologies, such as trusted hardware SGX [49] and interactive
proof systems [27]. However, these approaches mainly focus on
the integrity (or correctness) of DNNs computations, which can
hardly detect subtle attacks on the model’s integrity. For example,
in neural network trojan attacks [19], an adversary can slightly
modify the model’s parameters to make DNNs behave correctly
for normal inputs, while misclassifying inputs containing a trigger
predefined by the adversary. Moreover, most existing verifiable
solutions [19, 27, 49] do not consider privacy-protection in the out-
sourcing inference process. That is, the user’s private data, such as
the model’s parameters, query requests, and inference results are
all disclosed to the server. This inevitably provides a large attack-
ing surface for adversaries to breach the user’s privacy. Therefore,
it is urgent to design a generic verifiable protocol over the out-
sourced inference model, which is sensitive to model changes while
protecting user’s privacy.
Challenges: It is challenging to design a secure and privacy-
preserving protocol that meets the above requirements. First, the
heterogeneous cloud environments may bring a number of vul-
nerabilities (such as buffer overflow [62], network hijacking [46],
etc) for adversaries to launch attacks. It is difficult to guarantee
the model’s integrity under different cloud operations. Second,
once the user submits its model to the server, it will lose control
over the use, access, and publishing of the model. Traditional in-
tegrity verification strategies (e.g., computing the hash values of
protected data) can hardly work since the server can easily provide
plausible verification results to users. Third, existing approaches
[10, 27, 63] always verify the model’s integrity by analyzing the
model’s outputs due to the black-box access to the server. However,
in some model integrity attacks [31, 46], by slightly modifying the
model’s parameters, the adversary can make the classifier misclas-
sify for specific attacker-chosen inputs, while processing correctly
for other inputs. Therefore, it is very difficult to verify the model’s
integrity by only checking the outputs. Fourth, it is also challeng-
ing to propose a light-weight approach that is highly supportive
of both model verification and data privacy protection. Existing
privacy-preserving methods for neural networks are mostly evolved
from three underlying techniques: Secure Multi-Party Computation
(SMC) [4, 35], Differential Privacy [45, 62] and Homomorphic En-
cryption(HE) [2, 55]. However, technologies based on SMC and
differential privacy may not be proper for the scenarios considered
in this paper (see Section 2 for more details). Fully Homomorphic
Encryption (FHE) is a potential solution. However, it leads to huge
computation overhead. LHE [6, 65] (also called Somewhat Homo-
morphic Encryption), are faster than FHE, but only support limited
addition and multiplication in ciphertext. Moreover, complicated
non-linear activations such as ReLU, Sigmoid, and Tanh in DNNs,
are not directly supported by LHE. Recent works [10, 29] exploit
function approximation to convert non-linear activations to poly-
nomials, however, these approaches are fragmented and generally
cannot be applied to all activation functions.
Our Contributions: To address the above challenges, we propose
SecureDL, a secure and verifiable inference protocol to protect the
model’s integrity and user’s privacy in DNNs. In our SecureDL,
we first transform non-linear activation functions to low-degree
polynomials. Then, sensitive-samples are exploited to verify the
correctness of the model’s parameters. In the end, LHE is used to
provide the privacy-preserving DNNs inference. In summary, our
contributions can be summarized as follows:
• We first convert complicated non-linear activation functions
such as ReLU, Sigmoid, and Tanh into polynomials, to fa-
cilitate the implementation of LHE in general DNNs, and
the generation of sensitive-samples. We prove that given an
error bound, it is possible to approximate any function with
a low-degree polynomial.
• We design a novel sensitive-samples generation method to
protect the model’s integrity. We show that our sensitive-
samples are very sensitive to changes in the model parame-
ters, such that even a small parameter change can be reflected
in the model outputs. In addition, our sensitive-samples
can be applied to general neural networks, with no assump-
tions on DNNs architecture, hyper-parameters, and training
methods.
• We conduct extensive experiments on different datasets to
demonstrate the high performance of SecureDL in terms of
inference accuracy, detection accuracy, computation, and
communication overheads.
The remainder of this paper is organized as follows. In Section 2
and Section 3, we introduce the related works, outline the back-
ground and problem statement. In Section 4, we give the details of
our SecureDL. Next, performance evaluation is presented in Section
5. Finally, Section 6 concludes the paper.
2 COMPARISON WITH EXISTING WORKS
In this section, we introduce the latest related works about privacy-
preserving and verifiable deep learning, and compare them with
our proposal.
2.1 Privacy-preserving Deep Learning
As briefly mentioned, most of the existing research results evolve
from three underlying techniques: i.e., differential privacy [1, 23,
37, 45, 52, 62, 64], secure multi-party computing [4, 29, 34, 41] and
homomorphic encryption [2, 5, 14, 21, 53].
Differential privacy-based framework: Differential privacy
technology is mainly used in distributed or centralized DNNs train-
ing processes, where each data owner or the server disturbs the
sensitive data by adding disturbances to the original data, weights,
or loss functions. The propose of differential privacy-based DNNs
training is to reduce the negative impact of the addition of noise
on training as much as possible, and ensure data security under the
pre-privacy budget. For example, Shokri et al. [45] design the first
privacy-preserving deep learning model with differential privacy.
It ensures that the user’s data privacy is not compromised by se-
lectively sharing local parameters to the server. Agarwal et al.[1]
proposed cpSGD, an efficient and differentially-private distributed
stochastic gradient descent(SGD) in training process. By adding
noise that satisfies a specific distribution (such as Laplace distribu-
tion) to the original gradient, it can achieve the unrecoverability to
the original data and the high accuracy of training. Other works
such as [38], [62], and [50], also propose diverse strategies to make
a trade-off between training accuracy and data privacy, like add
noise to weights [38], set privacy budget dynamically [62], etc.
785Secure and Verifiable Inference in Deep Neural Networks
ACSAC 2020, December 7–11, 2020, Austin, USA
Comparison: To the best of our knowledge, differential privacy
is generally only applicable to training for DNNs due to its inher-
ent properties. That is, it can only guarantee that the disturbed
dataset is roughly consistent with the original dataset in statisti-
cal properties, does not retain any attributes between the single
disturbed data and its corresponding original data. However, our
focus is to solve the data privacy in the inference phase. That is,
we require users to send a single encrypted query request to the
server, then the server performs inference services in the ciphertext
environment and returns the ciphertext results. Hence, if we exploit
differential privacy technology to disturb a single query request, or
the parameters of the outsourced model, this will inevitably lead to
noise that is difficult to offset, thereby weakening the accuracy of
model classification.
SMC based framework: SMC enables two or more parties to
evaluate a function on their inputs without disclosing the inputs
to each other, that is, all inputs are kept private by the respective
owners. In general, the existing works with SMC as the underlying
architecture can be categorized into three approaches: i.e., Garbled
Circuit(GC)-based [3, 40, 42], Secret Sharing-based [4, 9, 61], and
Mixed Protocol-based [26, 34, 41], where GC-based approach is
mainly applied to the secure computing of 2 or 3 parties, while
Secret Sharing based approach is more suitable for distributed se-
cure computing. For example, Riazi et al.[40] and Ball et al.[3] both
design a new garbled circuit techniques for neural networks, and in-
troduced new optimizations for modern neural network activation
functions. In terms of Secret Sharing-based works, the most rep-
resentative result is [4], which design a practical data aggregation
protocol in federated training by using Shamir’s t-out-of-n secret
sharing protocol [15]. In recent years, studies [35] have shown that
it is difficult to achieve practical in communication overhead or
computational overhead by using secret sharing or garbled circuits
alone. To combat that, Mixed Protocol-based works [26, 34, 41],
i.e., hybrid secure computation frameworks exploiting mixed use
of secret sharing, homomorphic encryption, and garbled circuits,
have been proposed and applied in various fields. These mixed
protocols usually use additive secret sharing or homomorphic en-
cryption to perform linear operations in the deep learning process,
while non-linear calculations are delivered to garbled circuits for
implementation. The experimental results [34, 41] show that such
a hybrid approach tends to show better performance.
Comparison: SMC-based technology is a good way to provide
secure training and inference services, but in general, this requires
each party involved in secure computing to honestly execute a
predetermined protocol. In our scenario, the server is considered
to be an active, malicious adversary with an incentive to destroy
established procedures (including compression calculations, modi-
fication of parameters, etc) to violate the integrity of the original
protocol. It is difficult to construct an efficient SMC protocol under
such threat model. On the hand, the design of SMC is to ensure
that the participants evaluate a targeted function without knowing
each other’s input secrets. However, in our scenario, the parame-
ters of the outsourcing model and the user’s inputs are held by the
user itself, and we only require the server’s computing and storage
resources without sharing secrets with each other.
HE based framework: Homomorphic encryption can perform
specific mathematical operations in ciphertext without knowing
the unencrypted data. Such characteristics make it perform train-
ing and inference services in ciphertext gracefully. Based on the
differences in the mathematical operations supported, HE can be
classified into partial (additive or multiplicative) HE [2, 39], FHE
and LHE [6, 21, 65] (also called Somewhat Homomorphic Encryp-
tion). Partial HE is usually used in distributed (collaborative or
federated) DNNs training due to the simplicity of the mathematical
operation supported in the ciphertext. For example, Phong et al. [2]
exploited the Partial HE to encrypt users’ local gradients before
uploading to the cloud, which provides secure data aggregation for
multi-users. FHE offers an elegant way to provide secure DNNs
training and inference without even any interaction. For example,
Dowlin et al. [14] and Bourse et al. [5] respectively propose FHE-
based training and inference methods, where the user only needs
to send the encrypted input to the server and receive the returned
ciphertext result. However, due to the inefficiency of existing FHE
schemes, most applications prefer to use LHE [6, 21, 65] which is
faster than FHE, but only supports a limited depth of encryption
and multiplication operations in ciphertext, because it removes the
reduction process of noise introduced by multiple computations,
such as bootstrapping [65]. Hesamifard et al. [21] propose Cryp-
toDL, a Privacy-preserving Machine Learning as a Service (MLaaS)
with LHE. In CryptoDL, complex nonlinear activation functions
such as Sigmoid and tanh are replaced by polynomials. Then, Cryp-
toDL relies on the HElib library [17] to complete the training and
prediction of DNNs in the ciphertext.
Comparison: As discussed above, FHE is not practical in terms
of efficiency 1. Therefore, in this paper, we use LHE to achieve
privacy-preserving inference. Similar to work CryptoDL [21], we
also convert complex nonlinear activation functions into polyno-
mials, and use the HElib library to achieve LHE implementation.
However, compared with CryptoDL, we give a formal proof that it is
possible to approximate any continuous function with a polynomial
whose error from the objective function is within a given bound,
while CryptoDL only provides a scratch. Moreover, as admitted in