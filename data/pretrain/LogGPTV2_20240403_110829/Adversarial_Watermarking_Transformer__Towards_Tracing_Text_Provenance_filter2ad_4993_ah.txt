torate on Intelligence .
TABLE XVIII: Examples of re-watermarking in the white-box
and black-box cases.
Fig. 13: A matrix of word changes’ count from the original text
to modiﬁed text using AWT (same as Figure 8 but excluding
the diagonal elements where words were not changed).
Fig. 14: The words’ transitions produced by AWTadv for the
most commonly changed words by AWT (in Figure 13).
Input
AWT
AWTadv
As is often the case with huge an-
cient ruins , knowledge of the site
was never completely lost in the
region . It seems that local people
never  about  and
they guided  expeditions
to the ruins in the 1850s .
Jon  of
the professional
wrestling section of the Canadian
Online Explorer rated the show a
7 out of 10 , which was lower
than the 8 out of 10 given to the
2007 edition by Jason  .
As is often the case with huge an-
cient ruins , knowledge by the site
was never completely lost in the
region . It seems that local people
never  about  and
they guided  expeditions
to the ruins in the 1850s .
Jon  @-@ the profes-
sional wrestling section of the
Canadian Online Explorer rated
the show a 7 out of 10 , which
was lower than the 8 out of 10
given to the 2007 edition by Ja-
son  .
As is often the case with huge an-
cient ruins , knowledge of the site
was never completely lost in the
region . It seems that local people
never  about  and
three guided  expeditions
to the ruins in the 1850s .
Jon  of the professional
wrestling section of the Canadian
Online Explorer rated the show a
7 out of 10 , that was lower than
the 8 out of 10 given to the 2007
edition by Jason  .
TABLE XVII: Examples of input and watermarked sentences
(using the same message) by the two models.
words to the ﬁrst AWT model (based on Figure 13). These
observations and the previous analysis potentially explain why
anhad@-@onbyatasofwithfromwerebeenthatcouldalso,wasbutandToanhad@-@onbyatasofwithfromwerebeenthatcouldalso,wasbutandFrom-3656402559411405935423956205500005-1613512337621171416612940003415-7222205223424663115513314700026438-506033314101753365702120001182240-2331108442713538012101049315210681-7318565994922374520000341452463529-2512726323018519700014691711093334-28145273535114470101924344128193079-2222153552200001873646592931263126-262521100000308388521946691532444-61332266320003521372528242666143843-352636101002244126731100-0431704101202012020315-110700041353457473336105324750433024-3000000000101000100-0000000000000000000-0100000000000000000-0000000000000000000-anhad@-@onbyatasofwithfromwerebeenthatcouldalso,wasbutandToanhad@-@onbyatasofwithfromwerebeenthatcouldalso,wasbutandFrom-250022630016481300170068-00021200281421002250000-0000000000000000000-0000000000000000000-0000000000000000001-036908003003264700000000-0000000000000000000-0000000000000000000-0000000000000000000-0000000007498000252000-241100149002023000250100322-47005520010000203820000-00234000000000000000-0000000000000000000-0000000000000000000-00015241001009010467280116-0000000000000000000-0000000000000000000-x
o
b
-
e
t
i
h
W
x
o
b
-
k
c
a
l
B
Input
with a body length up to 60
centimetres ( 24 in )
which they must shed in order
to grow
 is remembered for ...  been remembered for
Watermarked
with a body length up to 60
centimetres of 24 in )
which three must shed in or-
der to grow
...
, that was lower than the 8
out of 10 given to the 2007
edition by Jason  .
, before he was granted
by  on the May 29
episode of Impact !
Today the
throughout the year
fort not open
, which was lower than the 8
out of 10 given to the 2007
edition by Jason  .
, which he was granted by
 on the May 29
episode of Impact !
Today
throughout the year
On the night before such
an event neither  or
 Gale could get those
minutes
open
fort
the
is
De-watermarked
with a body length up to 60
centimetres ( 24 in )
which they must shed in order
to grow
 is remembered for ...
, which was lower than the 8
out of 10 given to the 2007
edition by Jason  .
, which he was granted by
 on the May 29
episode of Impact !
Today the fort was open
throughout the year
On the night of such an event
neither  or 
Gale could get those minutes
On the night of such an event
neither  or 
Gale could get those minutes
research
which have been referred to
as the ” midnight @-@ sun
lobster ” .
several
@-@
 allegations that were
brought against him
the United  Band had
voted to stop  asso-
ciate 
three
This
stage
 and lasts for 15 – 35
days .
and three which have di-
verged due to small effective
population sizes
The ﬁrst pair of  is
armed with a large , asymmet-
rical pair of claws .
involves
Churchill has
argued that
blood quantum laws have an
inherent  purpose .
Homarus gammarus is found
across the north @-@ eastern
Atlantic Ocean
involves
research
which have from referred to
as the ” midnight @-@ sun
lobster ” .
several
@-@
 allegations that from
brought against him
the United  Band that
voted to stop  asso-
ciate 
three
This
stage
 and lasts for 15 – 35
days .
and three which have di-
verged due to small effective
population sizes
The ﬁrst pair of  is
armed by a large , asymmet-
rical pair of claws .
Churchill has
argued that
blood quantum laws have
been inherent  pur-
pose .
Homarus gammarus is found
across the north of eastern
Atlantic Ocean
involves
research
which have been referred to
as the ” midnight @-@ sun
lobster ” .
several
@-@
 allegations that were
brought against him
the United  Band was
voted to stop  asso-
ciate 
an
This
stage
 and lasts for 15 – 35
days .
and which they have diverged
due to small effective popula-
tion sizes
The ﬁrst pair of  is
armed by a large , asymmet-
rical pair of claws .
Churchill has
argued that
blood quantum laws have
been inherent  pur-
pose .
Homarus gammarus is found
across the north of eastern
Atlantic Ocean
TABLE XIX: Examples of de-watermarking in the white-box
and black-box cases.
it
1) Architecture: We add a ‘data hiding’ component to the
AWD-LSTM [70] by feeding the message to the language
model LSTM and simultaneously train a message decoder
that is optimized to reconstruct the message from the output
sequence. The input message is passed to a linear layer
to match the embeddings’ dimension,
is then repeated
and added to the word embeddings at each time step. The
language model is then trained with the cross-entropy loss:
L1 = Epdata(S)[− log pmodel(S)].
To allow end-to-end training, we use Gumbel-Softmax. The
message decoder has a similar architecture to the AWD-
LSTM and it takes the one-hot samples projected back into
the embedding space. To reconstruct the message, the hidden
states from the last layer are average-pooled and fed to a linear
layer. We tie the embeddings and the pre-Softmax weights.
The message reconstruction loss is the binary cross-entropy:
(cid:48)
i) + (1 − bi) log(1 − b
(cid:48)
i).
L2 = −(cid:80)q
i=1 bi log(b
L = w1 ∗ L1 + w2 ∗ L2.
The model is trained with a weighted average of both losses:
2) Training details: We mainly used the same hyperpa-
rameters and setup of [70], however, we found it essential
to decrease the learning rate of ASGD than the one used;
we use an initial learning rate of 2.5 instead of 30 for the
language modelling LSTM and a smaller learning rate of 0.5
for the message decoding LSTM. We also found it helpful
for a successful message encoding to pre-train the AWD-
LSTM of the message decoder as a language model. Following
the original implementation, we ﬁne-tune the model after the
initial training by restarting the training, to allow the ASGD
optimizer to restart the averaging. Similar to AWT, we use a
message length of 4 bits. To allow multiple operating points
of text utility vs. bit accuracy, we ﬁne-tune the model again
by assigning lower weight to the message loss. We start the
training by w1 = 1, w2 = 2, and decrease w2 for each ﬁne-
tuning step to reach a new operating point.
F. User Study
We demonstrate in Table XX the ratings’ descriptions given
in the instructions of the user study. In Figure 15, we show
a histogram of ratings given to the three types of sentences
included. We show in Table XXI,
the per-judge averaged
ratings where we can observe that all judges gave AWT higher
ratings than the baseline. We show examples of the baseline
sentences in Table XXII along with the corresponding original
sentences (paired sentences were not included in the study).
Rating
Description
5
4
3
2
1
0
The text is understandable, natural, and grammatically and structurally
correct.
The text is understandable, but it contains minor mistakes.
The text is generally understandable, but some parts are ambiguous.
The text is roughly understandable, but most parts are ambiguous.
The text is mainly not understandable, but you can get the main ideas.
The text is completely not understandable, unnatural, and you cannot
get the main ideas.
TABLE XX: Ratings explanations given in the user study.
Fig. 15: Histograms of ratings given to the three types of
sentences in the user study.
Judge 1
Non-wm 4.86±0.4
4.76±0.47
Wm
3.4±1.28
Baseline
Judge 2
3.98±0.96
3.98±1.09
3.57±1.21
Judge 3
4.47±0.62
4.13±0.64
3.37±0.81
Judge 4
4.77±0.48
4.58±0.61
3.32±1.02
Judge 5
4.84±0.44
4.71±0.49
3.4±1.09
Judge 6
4.8±0.52
4.63±0.6
4.03±1.19
TABLE XXI: Per-judge averaged ratings for the three types
of sentences.
Input
Synonym-baseline
Caldwell said it was easy to obtain guns in New
Mexico : ” we found it was pretty easy to buy guns
.
Caldwell said she and  went to a university
library to ﬁnd the identity ” of someone dying
very young ” , next went to public records and
asked for a copy of a birth certiﬁcate
Caldwell said it was soft to obtain artillery In
New Mexico : ” we rule it was pretty soft to
purchase accelerator .
Caldwell said she and  went to a university
library to found the identity ” of someone dying
real new ” , adjacent went to public records and
asked for a replicate of a parentage certiﬁcation
TABLE XXII: Examples of the synonym substitution baseline
sentences that were included in the user study.
543210Ratings0204060Percentage (%)Non-watermarked textAWT textBaseline text