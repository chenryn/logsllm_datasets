they are too common to be useful for differentiating legitimate
suggestions from illicit ones. Speciﬁcally, we identify the stop
words from query terms using stopword datasets [32] and
location tokens, which are also not informative for our purpose,
from a geographical database [18], which includes the names
(in different forms such as abbreviations) of over 11 million
locations across all countries.
Lemmatization. We further lemmatize each word within a search
term to its basic form: for example, “stopped” to “stop”, “best”
to “good” and “companies” to “company”. Speciﬁcally, we
ﬁrstly assign POS (part-of-speech) tag to each token in the given
sentence (the search term) using Python NLTK package [27].
For example, we can identify “stopped” as verb and “best”
as adjective using POS tagger. POS tagging is to provide the
contextual information that a lemmatizer needs to choose the
appropriate lemma. Then, we run a lemmatizer [35] to map a
search term to its basic form.
Once STR receives pre-processed query terms, it extracts
semantic features from them for detecting suspicious terms.
These features characterize the semantic gap between a trigger
and its suggestion term, which are supposed to be related but
often less so when the autocomplete has been manipulated.
6
TABLE II: F-score of features.
Label
Feature
Fss(sa, st)
Fws(wa, wt)
Fif (wa, wt)
Frs(Da, Dt)
Fci(wa, H a, H t)
Frp(Da, Dt)
Frs(N a, N t)
sentence similarity
word similarity
infrequent word similarity
result similarity
content impact
result popularity
result size
F-score
0.597
0.741
0.653
0.782
0.808
0.632
0.745
remaining 63.85% are all low-frequency words, counting for
only 0.55% of the total words in the autocomplete dataset with
average frequency of only 1.21 per query term. Further analysis
of them reveals that among all the 605,556 uncovered words,
more than 340K are URLs, and the remaining are rare words
like “hes”, “stongvault”, etc.
The sentence-level similarity alone is insufﬁcient for de-
tecting some manipulated suggestions. As an example, let
us look at the trigger-suggestion pair: (“online backup free
download”, “strongvault online backup free download”). Here,
an unpopular term “strongvault” is promoted onto the Google
suggestion list, for advertising the malware “strongvault”. The
overall similarity between the trigger and the suggestion here
is high, given that most of the words they include are identical.
Semantic inconsistency in this case has been captured when
comparing “strongvault” with other trigger words “online”,
“backup”, “free” and “download”. To identify such suggestions,
Sacabuche performs a ﬁne-grained semantic consistency check
at the word-level. Speciﬁcally, for each suggested word, we run
the word-similarity kernel WK to compare it against every
trigger word, which results in a value AV G
i , W t
j ))
each j
to describe its average semantic distances
for the word wa
with regard to all the trigger words {W t
j|1 ≤ j ≤ |st|}. From
i
these values, the one with the maximum average distance is
selected as the features Fws to capture the word-level semantic
gap between the trigger and the suggestion.
(WK(wa
In addition to the semantic features, leveraging our obser-
vation that manipulated predictions are a set of words rarely
appearing in legitimate suggestions: for example, “hes” and
“stongvault”, we further measure the infrequency level (Infreq)
for each suggestion word as follows:
Fif (wa, wt) =
M AX
each j
M AX
each i
(9 − log10F req(wt
j))
(9 − log10F req(wa
i ))
j is each word in trigger, and wa
j
where F req(w) is the frequency of word w in our Word2Vec
model. wt
is each word
in suggestion. Fif is utilized alongside sentence and word-
level similarities as the differentiating features for detecting
manipulated suggestions. Their differentiating power, in terms
of F-scores, are illustrated in Table II, which were calculated
using our ground truth dataset (the goodset and the badset).
Learning and classiﬁcation. Using these features, we trained
a support vector machine (SVM) classiﬁcation model over
the ground truth dataset, including 100 manipulated trigger
and suggestion pairs and 150 legitimate pairs (this dataset
was carefully selected so that pairs with different labels can
Fig. 5: Overview of the Sacabuche infrastructure.
Semantic consistency features. As mentioned before (see
Section III-B), we utilized a word-embedding technique based
sentence-level semantic similarity comparison to capture the
semantic gap between the trigger sa and its suggestion st.
Given two sentences, the kernel converts them into two phrase
lists pl(sa) and pl(st), through a dependency analysis. SK is
the sum of P K(pa
i is a phrase in the pha (with
j), where pa
i , pt
1 ≤ i ≤ |pha|) and pt
j is a phrase in pht (with 1 ≤ j ≤ |pht|).
j) is further calculated using a word kernel WK, by
P K(pa
simply multiplying W K(wa
2), and the
word kernel W K(wi, wj) directly runs the word embedding
technique on word wi and wj respectively to convert them
into vectors and then computes the cosine distance between the
vectors. Once a sentence kernel value SK(sa, st) (for sa and st)
is calculated, we normalize it by using(cid:112)SK(sa, sa)SK(st, st)
1) with W K(wa
1 , wt
2 , wt
i , pt
to divide it. This normalization step is necessary for the
fairness in comparing the semantic similarities across multiple
sentence pairs, since the length of sentences vary, affecting
their similarity values.
The whole similarity comparison is summarized as follows:
(cid:21)α
( 1 + cosineSim(wi, wj))
2
WK(wa
(cid:20) 1
len(cid:89)
(cid:88)
(cid:112)SK(sa, sa)SK(st, st)
pa∈pl(sa)
pt∈pl(st)
λ2PK(pa, pt)
SK(sa, st)
i , wt
i)
i=1
WK(wi, wj) =
PK(pa, pt) =
SK(sa, st) =
Fss(sa, st) =
where λ is a decay factor of the sentence kernel, α is a scaling
factor of the word kernel and Fss is a feature value of sentence
similarity.
We found that the standard word embedding model trained
by Google Word2Vec team using part of Google News Dataset
turns out to be less effective in covering autocomplete data, only
8.89% of the words in the trigger-suggestion pairs we gathered
from the Google API. So in our research, we trained a different
model using the training sets with Wikipedia documents in
nine languages. The new model achieves a coverage of 36.15%
in terms of unique words in our autocomplete dataset. The
7
be distinguished without referring to the search results). This
model was evaluated through a 5-fold cross validation, and
achieved a precision of 94.59% and a recall of 95.89%. We
further evaluated the technique over an unknown dataset, which
is elaborated in Section V.
C. Search Result Analysis
The query-term screening step performed by STA is
designed for scalability, ﬁltering through a large number
of autocomplete terms without actually querying the search
engine. The suspicious suggestion terms discovered at this step,
however, need to be further validated by looking at their search
results, so as to collect more evidence to support the ﬁnding
and remove false positives. This is done by the SRA module,
through a set of features identiﬁed from a series of differential
analysis, which compare the search results with and without
suggestion terms. It models the observations that a manipulated
suggestion tends to have a larger impact on search results, in
terms of promoting its target content, but be less diverse in
search results and also bring in fewer results than expected for
a legitimate popular query term. Note that even though each
single feature may not necessarily fully capture manipulated
suggestions, strategically leveraging multiple features in a
collective manner helps achieve very high detection accuracy,
as to be demonstrated in Section V.
Search result impact. Among the suspicious suggestions
reported by STA, those truly manipulated have large impacts
on search results, distinguishing them signiﬁcantly from what
are reported by the search engine under the trigger term only.
This deviation is measured in our research through two features:
result similarity that compares the search results of a trigger
with and without a suggestion and content impact that checks
the relations between suggestion words and search content to
understand their impacts on search outcome ranking.
A manipulated suggestion is meant to affect the way the
search engine prioritizes search results, making promoted
content more prominent in the results. As a consequence, the
results reported under such a term will be less in line with
those under the trigger (without suggestions), compared with the
results of legitimate suggestions. To measure such discrepancy,
we utilize a Rank-Biased Overlap (RBO) function [59] to
evaluate the similarity of two ranked lists of domain names,
one retrieved from the search engine under the trigger alone
and the other under the whole query term (trigger +suggestion).
The RBO measurement
is characterized by its design to
weigh high-ranking items more heavily than those down
the lists, handle nonconjointness and be monotonic with the
increasing depth (i.e., ranks) of evaluation [59]. Speciﬁcally, let
{Dt
i : 1 ≤ i ≤ d} be the element at rank i on the list Dt (the
domain list for the trigger). At each depth d, the intersection of
1:d ∩ Da
1:d.
lists Dt and Da (for suggestion) is I(Dt, Da)d = Dt
The proportion of two search results overlapped at depth d is
deﬁned as their agreement A(Dt, Da)d =
. So we
get the two search results’ rank-biased overlaps as below:
|I(Dt,Da)d|
d
∞(cid:88)
Frs(Da, Dt) = (1 − p)
pd−1A(Dt, Da)d
where p is a decay value for tuning the weights for different
depths d. The smaller p is, the more biased toward higher
d=1
8
ranked items the metric becomes. Once p = 0, only the item
on the top of the list matters. This metric is convergent and
its value falls in the range [0, 1] where 0 is disjoint, and 1 is
identical.
Also measuring suggestion impacts is content impact, which
evaluates the relations between suggestion words and the titles
of individual search result items (see Figure 4), with and without
a given suggestion when querying the search engine. We utilize
the aforementioned function, RBO, for differential analysis. The
purpose is to understand whether the suggested content becomes
more prominent under the suggestion words (to the extent they
are more frequently present in the search result titles), which
is a necessary condition for suggestion manipulation. More
speciﬁcally, for the search results under a trigger only, our
approach looks for the distribution of suggestion words across
the titles of the result items, and compares it to the distribution
of the same set of suggestion words over the result titles under
the whole query term (the trigger and the suggestion) using
RBO. Formally, let H = {hi|1 ≤ i ≤ |H|} be a title list
retrieved from a search result page, where hi is the ith title on
the list. Given a suggestion word wa
i , its impact on hi is 1 if
the word shows up in the title and 0 otherwise. In this way,
its distribution over H can be described by a binary vector
is then calculated
V (wa
as the distance between two ranked lists: R(wa
i , H a, H t) =
i , H t)). Based upon the impact of
RBO(V (wa
individual word in a suggestion, we identify an content impact
feature for detecting manipulated suggestion as follows:
i , H). The content impact R of wa
i
i , H a), V (wa
Fci(wa, H a, H t) = M in
each i
(R(wa
i , H a, H t))
where wa
i
is the suggestion term.
Result popularity and size. Further we found that fewer results
are reported by the search engine when a truly manipulated
suggestion is queried and the domains involved in the results
often have low Alexa ranking. The observation is characterized
by two collective features: search result popularity that com-
pares the popularity of search results under both trigger and
suggestions, and search result size, which captures the number
of results that Google can ﬁnd for a given query.
We deﬁne the popularity of a given domain Di as P (Di) =
1 − log10AlexaRanking(Di)/7, where AlexaRanking(Di)
is the rank of a domain Di in Alexa Top 1 million. If the
domain does not show up among the top 1 million, we set its
popularity to 1
7. Again, here we perform a differential analysis
and deﬁne a popularity vector AP with its dth element being
the average popularity of the top d domains on the search result
(P (Di))|1 ≤ d ≤ |D|]. Let AP t
page, i.e., AP (D) = [AV G
i∈[1:d]
be the vector for the results of querying a trigger only and
AP a be the vector for querying both trigger and suggestion.
The search result popularity is calculated as follows:
Frp(Da, Dt) = RBO(AP a(Da), AP t(Dt))
From each search result page, SRA further retrieves the number
of the results discovered by the search engine. Based upon such
information, we identify a feature Frs. Let N t be the number
of results found under the trigger, and N a be the number found
under the trigger and a suggestion. The feature is calculated
as Frs(N a, N t) = N a−N t
.
N t
Classiﬁcation and detection. The differentiation powers of
these features are presented in Table II. Over these individual
features, SRA trains a SVM model and utilizes it to classify the