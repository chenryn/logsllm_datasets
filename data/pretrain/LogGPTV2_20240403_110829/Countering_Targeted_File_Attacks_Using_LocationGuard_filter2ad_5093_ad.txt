### Location-based File System Security

Over time, an adversary may accumulate enough information to infer the location of a target file in a location-based file system. To defend against both known and unknown inference attacks, location rekeying is a general strategy. By periodically changing the location keys, all past inferences made by an adversary become useless. To protect the system from Biham's key collision attacks, an initialization vector (IV) can be associated with the location key, and the IV can be changed instead of the location key itself.

Location rekeying is similar to the rekeying of cryptographic keys. However, rekeying is a resource-intensive operation: rekeying cryptographic keys requires data to be re-encrypted, while rekeying location keys requires files to be relocated on the overlay network. Therefore, it is crucial to balance the rekeying frequency to minimize performance overheads while ensuring adequate security for files on the overlay network. In our experimental section, we estimate the optimal periodicity for changing location keys to reduce the probability of an attack on a target file.

### Discussion

In this section, we discuss several issues related to the security, distribution, and management of LocationGuard.

#### Key Security
In LocationGuard-based file systems, legal users are responsible for securing location keys from adversaries. If a user needs to access thousands of files, they must ensure the secrecy of thousands of location keys. A viable solution is to compile all location keys into a single key-list file, encrypt this file, and store it on the overlay network. The user then only needs to keep one 128-bit location key secret, which can be physically protected using tamper-proof hardware devices or smartcards.

#### Key Distribution
Secure key distribution is a significant challenge in large-scale distributed systems. The problem of distributing location keys is similar to that of distributing cryptographic keys. Typically, keys are distributed using out-of-band techniques, such as PGP-based secure email services, to transfer location keys from a file owner to file users.

#### Key Management
Efficient key management becomes critical when:
1. An owner has several thousand files.
2. The set of legal users for a file changes significantly over time.

In the first scenario, the file owner can reduce key management costs by assigning one location key to a group of files. Any user who obtains the location key for a file \( f \) would implicitly be authorized to access the entire group of files to which \( f \) belongs. However, in the second scenario, frequent changes in group membership may require frequent location key changes, potentially degrading system performance.

The major overheads in LocationGuard arise from key distribution and key management. Additionally, location rekeying can be a significant factor. Using group key management protocols [11] to address key security, distribution, and management in LocationGuard is part of our ongoing research.

Other issues not discussed in this paper include the problem of a valid user illegally distributing capabilities (tokens) to an adversary and the robustness of the lookup protocol and overlay network in the presence of malicious nodes. We assume that all valid users are well-behaved and that the lookup protocol is robust. For a detailed discussion on the robustness of lookup protocols on DHT-based overlay networks, readers may refer to [24].

### Experimental Evaluation

In this section, we present results from our simulation-based experiments to evaluate the LocationGuard approach for building secure wide-area network file systems. We implemented our simulator using a discrete event simulation model and implemented the Chord lookup protocol on an overlay network consisting of \( N = 1024 \) nodes. In all experiments, a random 10% of the nodes were chosen to behave maliciously. We set the number of replicas of a file to \( R = 7 \) and varied the corruption threshold \( cr \) in our experiments. We simulated the bad nodes as having large but bounded power based on the parameters \( \alpha \) (DoS attack strength), \( \lambda \) (node compromise rate), and \( \mu \) (node recovery rate) (see the threat model in Section 3). We studied the cost of using location keys by quantifying the overhead and evaluated the benefits of location keys by measuring their effectiveness against DoS and host compromise-based target file attacks.

#### LocationGuard Operational Overhead

We first quantify the performance and storage overheads incurred by location keys. All measurements were obtained on a 900 MHz Intel Pentium III processor running Red-Hat Linux 9.0. Consider a typical file read/write operation, which consists of the following steps:
1. Generate the file replica identifiers.
2. Lookup the replica holders on the overlay network.
3. Process the request at replica holders.

Step 1 requires computations using a keyed-hash function with location keys, which otherwise would use a normal hash function. We found that the computation time difference between HMAC-MD5 (a keyed-hash function) and MD5 (normal hash function) is negligible (a few microseconds) using the standard OpenSSL library [17]. Step 2 involves pseudo-random number generation (a few microseconds using the OpenSSL library) and may require lookups to be retried if the obfuscated identifier is unsafe. Given that unsafe obfuscations are extremely rare (see Table 1), retries are only required occasionally, making this overhead negligible. Step 3 adds no overhead because our access check is almost free; as long as the user can present the correct filename (token), the replica holder will honor the request.

Next, we compare the storage overhead at the users and the nodes that are part of the overlay network. Users need to store an additional 128-bit location key (16 Bytes) along with other file metadata for each file they want to access. Even a user who uses 1 million files on the overlay network needs to store only an additional 16 MBytes of location keys. There is no extra storage overhead on the rest of the nodes on the overlay network. For a detailed description of our implementation of LocationGuard and benchmark results for file read and write operations, refer to our technical report [23].

#### Denial of Service Attacks

Figure 4 shows the probability of an attack for varying \( \alpha \) and different values of the corruption threshold \( cr \). Without knowledge of the file replica locations, an adversary must attack a random collection of nodes and hope that at least \( cr \) replicas of the target file are attacked. If the malicious nodes are more powerful (larger \( \alpha \)) or if the corruption threshold \( cr \) is very low relative to the size \( N \) of the network, the probability of an attack is higher. If an adversary knows the \( R \) replica holders of a target file, even a weak collection of \( B \) malicious nodes, such as \( B = 102 \) (i.e., 10% of \( N \)) with \( \alpha = \frac{R}{B} \), can easily attack the target file. For a file system to handle DoS attacks on a file with \( \alpha = 1 \), it would require maintaining a large number of replicas \( R \) close to \( B \) for each file. For example, with \( B = 10\% \times N \) and \( N = 1024 \), the system needs to maintain as many as 100+ replicas for each file. Clearly, without location keys, the effort required for an adversary to attack a target file (i.e., make it unavailable) depends only on \( R \) but is independent of the number of good nodes \( G \) in the system. On the contrary, location key-based techniques scale the difficulty of an attack with the number of good nodes in the system. Thus, even with a small \( R \), the location key-based system can make it very hard for any adversary to launch a targeted file attack.

#### Host Compromise Attacks

To further evaluate the effectiveness of location keys against targeted file attacks, we conducted experiments on host compromise attacks. Our first experiment on host compromise attacks shows the probability of an attack on the target file, assuming the adversary cannot collect capabilities (tokens) stored at compromised nodes. The target file is attacked if \( cr \) or more of its replicas are stored at either malicious nodes or compromised nodes. Figure 5 shows the probability of an attack for different values of the corruption threshold \( cr \) and varying \( \rho = \frac{\mu}{\lambda} \) (measured in the number of node recoveries per node compromise). We ran the simulation for a duration of 100 time units. Recall that \( \frac{1}{\lambda} \) denotes the mean time required for one malicious node to compromise a good node. Note that if the simulation were run for an infinite time, the probability of an attack is always one because, at some point, \( cr \) or more replicas of a target file would be assigned to malicious nodes (or compromised nodes) in the system.

From Figure 5, we observe that when \( \rho \leq 1 \), the system is highly vulnerable because the node recovery rate is lower than the node compromise rate. While a DoS attack could tolerate powerful malicious nodes (\( \alpha > 1 \)), the host compromise attack cannot tolerate a situation where the node compromise rate is higher than the recovery rate (\( \rho \leq 1 \)). This is primarily due to the cascading effect of host compromise attacks. The larger the number of compromised nodes, the higher the rate at which other good nodes are compromised (see the adversary model in Section 3). Table 2 shows the mean fraction of good nodes \( G_0 \) that remain uncompromised for different values of \( \rho \). When \( \rho = 1 \), most of the good nodes are compromised.

As mentioned in Section 4.3, an adversary could collect the capabilities (tokens) of the file replicas stored at compromised nodes, which can be used to corrupt these replicas in the future. Our second experiment on host compromise attacks measures the effectiveness of location keys in mitigating this risk.