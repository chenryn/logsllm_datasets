concern [18]. To evaluate this scenario, we trained a model to
predict interest in credit cards using the PSID dataset. From this,
we trained two models: one that identifies individuals with student
loans and another that identifies individuals with existing credit
cards as the two groups to be targeted. The first model had a number
of instances of proxy use. One particular subcomputation that was
concerning was a subtree of the original decision tree that branched
on the number of children in the family. This instance provided
negative outcomes to individuals with more children, and may be
deemed inappropriate for use in this context. In the second model,
one proxy was a condition involving income income ≤ 33315. The
use of income in this context is justifiable, and therefore this may
be regarded as not being a use privacy violation.
2−22−42−62−82−102−12δ/inﬂuence[probability]202−22−42−62−82−10/association(nmi)relationship≤0.5rootmaximalexps.2−22−42−62−82−102−12δ/inﬂuence[probability]202−22−42−62−82−10/association(nmi)Aexps.exps.(repaired)To determine the completeness of our detection algorithm we
inserted a proxy in a trained model to determine whether we can
detect it. To do this, we used the UCI Student Alcohol Consump-
tion dataset to train two decision trees: one to predict students’
grades, and one to predict alcohol consumption. We then inserted
the second tree into random positions of the first tree thereby in-
troducing a proxy for alcohol consumption. We observed that in
each case, we were able to detect the introduced proxy. While not
interesting in itself due to our completeness theorem, we used this
experiment to explore how much utility is actually lost due to re-
pair. We evaluate our repair algorithm on a set of similar models
with inserted violations of various influence magnitude. The results
can be seen in Figure 6. We can see that the accuracy (i.e., ratio of
instances that have agreement between repaired and unrepaired
models) falls linearly with the influence of the inserted proxy. This
implies that repair of less influential proxies will incur a smaller
accuracy penalty than repair of more influential proxies. In other
words, our repair methods do not unduly sacrifice accuracy when
repairing only minor violations.
A point not well visible in this figure is that occasionally repair
incurs no loss of utility. This is due to our use of the scikit-learn
library for training decision trees as it does not currently support
pruning unnecessary nodes. Occasionally such nodes introduce
associations without improving the model’s accuracy. These nodes
can be replaced by constants without loss. We have also observed
this in some of our case studies.
7 RELATED WORK
7.1 Definition
In the computer science literature, pri-
Minimizing disclosures
vacy has been thought of as the ability to protect against unde-
sired flows of information to an adversary. Much of the machinery
developed in cryptography, such as encryption, anonymous com-
munication, private computation, and database privacy have been
motivated by such a goal. Differential privacy [25] is one of the main
pillars of privacy research in the case of computations over data
aggregated from a number of individuals, where any information
gained by an adversary observing the computation is not caused by
an individual’s participation. However, none of these technologies
cover the important setting of individual-level data analytics, where
one may want to share some information while hiding others from
adversaries with arbitrary background knowledge. This absence is
with good reason, as in the general case it is impossible to prevent
flows of knowledge from individual-level data, while preserving
the utility of such data, in the presence of arbitrary inferences that
may leverage the background knowledge of an adversary [21]. In
this work, we do not attempt to solve this problem either.
Nevertheless, the setting of individual level data analytics is per-
vasive, especially in the case of predictive systems that use machine
learning. Since these systems are largely opaque, even developers
do not have a handle on information they may be inadvertently
using via inferences. Therefore, in this work, we make the case
for proxy use restrictions in data driven systems and develop tech-
niques to detect and repair violations of proxy use. Restrictions
on information use, however do not supplant the need for other
privacy enhancing technologies geared for restricting information
Figure 5: Worst-case detection algorithm run-time (average
of 5 runs) as a function of input dataset size. Influence and
association computed on each decomposition (hence worst-
case). The models are decision tree(◦), random forest(+), and
logistic regression(×) trained on the UCI Adult dataset.
Figure 6: Repaired accuracy vs. influence of proxy during re-
pair of a synthetic proxy inserted into random positions of a
decision tree trained on the UCI Student Alcohol Consump-
tion dataset. Accuracy is agreement to non-repaired model.
The synthetic model is a (1.0)-proxy for alcohol use, inserted
into a decision tree predicting student grade. Repair is con-
figured for (0.01, 0.01)-proxy use removal. Note that other
proxies (if they exist) are not repaired in this experiment.
6.3 Detection and Repair
For the remainder of the section we focus on evaluating the per-
formance and efficacy of the detection and repair algorithms. We
begin by exploring the impact of the dataset and model size on the
detection algorithm’s runtime.
Figure 5 demonstrates the runtime of our detection algorithm
on three models trained on the UCI Adult dataset vs. the size of the
dataset used for the association and influence computations. The
algorithm here was forced to compute the association and influence
metrics for each decomposition (normally influence can be skipped
if association is below threshold) and thus represents a worst-case
runtime. The runtime for the random forest and decision tree scales
linearly in dataset size due to several optimizations. The logistic
regression does not benefit from these and scales quadratically.
Further, runtime for each model scales linearly in the number of
decompositions (see Appendix D.2) , but logistic regression models
contain an exponential number of decompositions as a function of
their size.
02004006008001000datasetsize[count]100101102103realruntime[s]0.00.10.20.30.4inﬂuence[probability]0.50.60.70.80.91.0accuracy[ratio]collection and disclosure, which may be useful in conjunction with
the enforcement of use restrictions. For example, when machine
learning models are trained using personal data, it is desirable
to minimize disclosures pertaining to individuals in the training
set, and to reduce the use of protected information types for the
individuals the models are applied to.
Identifying explicit use The privacy literature on use restric-
tions has typically focused on explicit use of protected information
types, not on proxy use (see Tschantz et al. [64] for a survey and
Lipton and Regan [46]). Recent work on discovering personal data
use by black-box web services focuses mostly on explicit use of
protected information types by examining causal effects [16, 44];
some of this work also examines associational effects [43, 44]. As-
sociational effects capture some forms of proxy use but not others
as we argued in Section 3.
7.2 Detection and Repair Models
Our detection algorithm operates with white-box access to the
prediction model. Prior work requires weaker access assumptions.
Access to observational data Detection techniques working
under an associative use definition [30, 63] usually only require
access to observational data about the behavior of the system.
Access to black-box experimental data Detection techniques
working under an explicit use definition of information use [16, 44]
typically require experimental access to the system. This access
allows the analyst to control some inputs to the system and observe
relevant outcomes.
The stronger white-box access level allows us to decompose the
model and trace an intermediate computation that is a proxy. Such
traceability is not afforded by the weaker access assumptions in
prior work. Thus, we explore a different point in the space by giving
up on the weaker access requirement to gain the ability to trace
and repair proxy use.
Tramèr et al. [63] solve an important orthogonal problem of
efficiently identifying populations where associations may appear.
Since our definition is parametric in the choice of the population,
their technique could allow identifying relevant populations for
further analysis using our methods.
Repair Removal of violations of privacy can occur at different
points of the typical machine learning pipeline. Adjusting the train-
ing dataset is the most popular approach, including variations that
relabel only the class attribute [48], modify entire instances while
maintaining the original schema [30], and transform the dataset
into another space of features [24, 72]. Modifications to the train-
ing algorithm are specific to the trainer employed (or to a class of
trainers). Adjustments to Naive Bayes [7] and trainers amiable to
regularization [42] are examples. Several techniques for produc-
ing differentially-private machine learning models modify trained
models by perturbing coefficients [3, 8]. Other differentially-private
data analysis techniques [26] instead perturb the output by adding
symmetric noise to the true results of statistical queries. All these
repair techniques aim to minimize associations or inference from
the outcomes rather than constrain use.
8 DISCUSSION
Beyond strict decomposition Theorem 1 shows that a defini-
tion satisfying natural semantic properties is impossible. This result
motivates our syntactic definition, parameterized by a program-
ming language and a choice of program decomposition. In our
implementation, the choice of program decomposition is strict. It
only considers single terms in its decomposition. However, proxies
may be distributed across different terms in the program. As dis-
cussed in Section 4.1, single term decompositions can also deal with
a restricted class of such distributed proxies. Our implementation
does not identify situations where each of a large number of syntac-
tically different proxies have weak influence but together combine
to result in high influence. A stronger notion of program decompo-
sition that allows a collection of multiple terms to be considered a
proxy would identify such a case of proxy use.
The choice of program decomposition also has consequences
for the tractability of the detection and repair algorithms. The
detection and repair algorithms presented in this paper currently
enumerate through all possible subprograms in the worst case.
Depending on the flexibility of the language chosen and the model2
being expressed there could be an exponentially large number of
subprograms, and our enumeration would be intractable.
Important directions of future work are therefore organized
along two thrusts. The first thrust is to develop more flexible notions
of program decompositions that identify a wide class of proxy uses
for other kinds of machine learning models, including deep learning
models that will likely require new kinds of abstraction techniques
due to their large size. The second thrust is to identify scalable
algorithms for detecting and repairing proxy use for these flexible
notions of program decompositions.
Data and access requirements Our definitions and algorithms
require (i) a specification of which attributes are protected, (ii) entail
reasoning using data about these protected information types for
individuals, and (iii) white box access to models and a representative
dataset of inputs. Obtaining a complete specification of protected
information types can be challenging when legal requirements and
privacy expectations are vague regarding protected information
types. However, in many cases, protected types are specified in laws
and regulations governing the system under study (e.g., HIPAA,
GDPR), and also stated in the data processor’s privacy policies.
Further, data about protected information types is often not
explicitly collected. Pregnancy status, for example, would rarely
find itself as an explicit feature in a purchases database (though it
was the case in the Target case). Therefore, to discover unwanted
proxy uses of protected information types, an auditor might need to
first infer the protected attribute from the collected data to the best
extent available to them. Though it may seem ethically ambiguous
to perform a protected inference in order to (discover and) prevent
protected inferences, it is consistent with the view that privacy
is a function of both information and the purpose for which that
information is being used [65]3. In our case, the inference and use
of protected information by an auditor has a different (and ethically
2Though deep learning models can be expressed in the example language presented
in this paper, doing so would result in prohibitively large programs.
3This principle is exemplified by law in various jurisdictions including the PIPEDA
Act in Canada [54], and the HIPAA Privacy Rule in the USA [55].
justified) purpose than potential inferences in model being audited.
Further, protected information has already been used by public
and private entities in pursuit of social good: affirmative action
requires the inference or explicit recording of minority membership,
search engines need to infer suicide tendency in order to show
suicide prevention information in their search results[60], health
conditions can potentially be detected early from search logs of
affected individuals [56]. Supported by law and perception of public
good, we think it justified to expect system owners be cooperative
in providing the necessary information or aiding in the necessary
inference for auditing.
Finally, in order to mitigate concerns over intellectual prop-
erty due to access requirements for data and models, the analyst
will need to be an internal auditor or trusted third party; exist-
ing privacy-compliance audits (Sen et al. [58]) that operate under
similar requirements could be augmented with our methods.
Normative judgments Appropriateness decisions by the ana-
lyst will be made in accordance with legal requirements and ethical
norms. Operationally, this task might fall on privacy compliance
teams. In large companies, such teams include law, ethics, and
technology experts. Our work exposes the specific points where
these complex decisions need to be made. In our evaluation, we ob-
served largely human-interpretable witnesses for proxies. For more
complex models, additional methods from interpretable machine
learning might be necessary to make witnesses understandable.
Another normative judgment is the choice of acceptable ϵ, δ
parameters. Similar to differential privacy, the choice of parameters
requires identifying an appropriate balance between utility and
privacy. Our quantitative theory could provide guidance to the
oracle on how to prioritize efforts, e.g., by focusing on potentially
blatant violations (high ϵ, δ values).
9 CONCLUSION
We develop a theory of use privacy in data-driven systems. Distinc-
tively, our approach constrains not only the direct use of protected
information types but also their proxies (i.e. strong predictors),
unless allowed by exceptions justified by ethical considerations.
We formalize proxy use and present a program analysis tech-
nique for detecting it in a model. In contrast to prior work, our
analysis is white-box. The additional level of access enables our
detection algorithm to provide a witness that localizes the use to a
part of the algorithm. Recognizing that not all instances of proxy
use of a protected information type are inappropriate, our theory of
use privacy makes use of a normative judgment oracle that makes
this appropriateness determination for a given witness. If the proxy
use is deemed inappropriate, our repair algorithm uses the witness
to transform the model into one that does not exhibit proxy use.
Using a corpus of social datasets, our evaluation shows that these
algorithms are able to detect proxy use instances that would be
difficult to find using existing techniques, and subsequently remove
them while maintaining acceptable classification performance.
Acknowledgments We would like to thank Amit Datta, Sophia
Kovaleva, and Michael C. Tschantz for their thoughtful discussions
throughout the development of this work. We thank our shepherd
Aylin Caliskan and anonymous reviewers for their numerous sug-
gestions that improved this paper.
This work was developed with the support of NSF grants CNS-1704845,
CNS-1064688 as well as by DARPA and the Air Force Research Laboratory
under agreement number FA8750-15-2-0277. The U.S. Government is autho-
rized to reproduce and distribute reprints for Governmental purposes not
withstanding any copyright notation thereon. The views, opinions, and/or
findings expressed are those of the author(s) and should not be interpreted as
representing the official views or policies of DARPA, the Air Force Research
Laboratory, the National Science Foundation, or the U.S. Government.
REFERENCES
[1] 2013. Indonesia - National Contraceptive Prevalence Survey 1987. (2013). http://
microdata.worldbank.org/index.php/catalog/1398/study-description (Accessed
Nov 11, 2016).
[2] Paul Barford, Igor Canadi, Darja Krushevskaja, Qiang Ma, and S. Muthukrishnan.
2014. Adscape: Harvesting and Analyzing Online Display Ads. In Proceedings
of the 23rd International Conference on World Wide Web. International World
Wide Web Conferences Steering Committee, Republic and Canton of Geneva,
Switzerland, 597–608.
[3] Raef Bassily, Adam Smith, and Abhradeep Thakurta. 2014. Private Empirical
Risk Minimization: Efficient Algorithms and Tight Error Bounds. In 55th IEEE
Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia,
PA, USA, October 18-21, 2014. 464–473.
[4] Richard Berk and Justin Bleich. 2014. Forecasts of Violence to Inform Sentencing
Decisions. Journal of Quantitative Criminology 30, 1 (2014), 79–96.
[5] Richard A. Berk, Susan B. Sorenson, and Geoffrey Barnes. 2016. Forecasting
Domestic Violence: A Machine Learning Approach to Help Inform Arraignment
Decisions. Journal of Empirical Legal Studies 13, 1 (2016), 94–115.
[6] Leo Breiman. 2001. Random Forests. Mach. Learn. 45, 1 (Oct. 2001), 5–32.