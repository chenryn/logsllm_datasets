### Evaluation of Proxy Use in Credit Card Interest Prediction

To evaluate the scenario, we trained a model to predict interest in credit cards using the PSID dataset. From this, we developed two models: one to identify individuals with student loans and another to identify individuals with existing credit cards as the target groups.

#### Model 1: Identifying Individuals with Student Loans
This model exhibited several instances of proxy use. A particularly concerning subcomputation was a subtree of the original decision tree that branched based on the number of children in the family. This branch provided negative outcomes for individuals with more children, which may be deemed inappropriate in this context.

#### Model 2: Identifying Individuals with Existing Credit Cards
In this model, one proxy was a condition involving income (income ≤ $33,315). The use of income in this context is justifiable, and therefore, it does not constitute a privacy violation.

### Completeness of Detection Algorithm

To assess the completeness of our detection algorithm, we inserted a proxy into a trained model to determine if it could be detected. We used the UCI Student Alcohol Consumption dataset to train two decision trees: one to predict students' grades and another to predict alcohol consumption. We then inserted the second tree into random positions of the first tree, introducing a proxy for alcohol consumption. In each case, we were able to detect the introduced proxy.

While this result is expected due to our completeness theorem, we used this experiment to explore the utility loss due to repair. We evaluated our repair algorithm on a set of similar models with inserted violations of various influence magnitudes. The results, shown in Figure 6, indicate that the accuracy (i.e., the ratio of instances that have agreement between repaired and unrepaired models) decreases linearly with the influence of the inserted proxy. This implies that repairing less influential proxies incurs a smaller accuracy penalty than repairing more influential ones. In other words, our repair methods do not unduly sacrifice accuracy when addressing minor violations.

Occasionally, the repair process incurs no loss of utility. This is because the scikit-learn library, which we used for training decision trees, does not currently support pruning unnecessary nodes. These nodes can introduce associations without improving the model’s accuracy and can be replaced by constants without loss.

### Related Work

#### Definition of Privacy
In computer science literature, privacy has been defined as the ability to protect against undesired information flows to an adversary. Technologies such as encryption, anonymous communication, private computation, and differential privacy aim to achieve this goal. However, these technologies do not cover individual-level data analytics, where one may want to share some information while hiding others from adversaries with arbitrary background knowledge. In this work, we focus on proxy use restrictions in data-driven systems and develop techniques to detect and repair violations of proxy use.

#### Detection and Repair Models
Our detection algorithm operates with white-box access to the prediction model, allowing us to decompose the model and trace intermediate computations that are proxies. Prior work typically requires weaker access assumptions, such as observational or experimental data. Our approach, while requiring stronger access, enables us to trace and repair proxy use effectively.

### Discussion

#### Beyond Strict Decomposition
Theorem 1 shows that a definition satisfying natural semantic properties is impossible, motivating our syntactic definition parameterized by a programming language and a choice of program decomposition. Our implementation uses strict decomposition, considering only single terms. However, proxies may be distributed across different terms. A stronger notion of program decomposition that allows multiple terms to be considered a proxy would address this limitation.

#### Data and Access Requirements
Our definitions and algorithms require:
1. A specification of protected attributes.
2. Reasoning using data about these protected information types.
3. White-box access to models and a representative dataset of inputs.

Obtaining a complete specification of protected information types can be challenging, but in many cases, they are specified in laws and regulations (e.g., HIPAA, GDPR). Protected information is often not explicitly collected, so an auditor might need to infer the protected attribute from the available data. While this may seem ethically ambiguous, it is consistent with the view that privacy is a function of both information and its purpose.

#### Normative Judgments
Appropriateness decisions will be made in accordance with legal requirements and ethical norms. For complex models, additional methods from interpretable machine learning might be necessary to make witnesses understandable. The choice of acceptable ϵ, δ parameters requires balancing utility and privacy, and our quantitative theory can provide guidance.

### Conclusion

We have developed a theory of use privacy in data-driven systems, constraining not only the direct use of protected information types but also their proxies, unless justified by ethical considerations. Our formalization of proxy use and program analysis technique for detecting it in a model is white-box, providing a witness that localizes the use to a part of the algorithm. If the proxy use is deemed inappropriate, our repair algorithm transforms the model to remove the proxy use. Our evaluation shows that these algorithms can detect and remove proxy use instances while maintaining acceptable classification performance.

### Acknowledgments

We thank Amit Datta, Sophia Kovaleva, and Michael C. Tschantz for their thoughtful discussions. We also thank Aylin Caliskan and anonymous reviewers for their suggestions. This work was supported by NSF grants CNS-1704845, CNS-1064688, and DARPA under agreement number FA8750-15-2-0277.

### References

[1] 2013. Indonesia - National Contraceptive Prevalence Survey 1987. (2013). http://microdata.worldbank.org/index.php/catalog/1398/study-description (Accessed Nov 11, 2016).

[2] Paul Barford, Igor Canadi, Darja Krushevskaja, Qiang Ma, and S. Muthukrishnan. 2014. Adscape: Harvesting and Analyzing Online Display Ads. In Proceedings of the 23rd International Conference on World Wide Web. International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, Switzerland, 597–608.

[3] Raef Bassily, Adam Smith, and Abhradeep Thakurta. 2014. Private Empirical Risk Minimization: Efficient Algorithms and Tight Error Bounds. In 55th IEEE Annual Symposium on Foundations of Computer Science, FOCS 2014, Philadelphia, PA, USA, October 18-21, 2014. 464–473.

[4] Richard Berk and Justin Bleich. 2014. Forecasts of Violence to Inform Sentencing Decisions. Journal of Quantitative Criminology 30, 1 (2014), 79–96.

[5] Richard A. Berk, Susan B. Sorenson, and Geoffrey Barnes. 2016. Forecasting Domestic Violence: A Machine Learning Approach to Help Inform Arraignment Decisions. Journal of Empirical Legal Studies 13, 1 (2016), 94–115.

[6] Leo Breiman. 2001. Random Forests. Mach. Learn. 45, 1 (Oct. 2001), 5–32.