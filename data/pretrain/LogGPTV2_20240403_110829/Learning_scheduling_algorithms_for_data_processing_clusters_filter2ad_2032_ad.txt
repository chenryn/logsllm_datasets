currently working on the node, (iv) the number of available executors,
and (v) whether available executors are local to the job. We picked
these features by attempting to include information necessary to cap-
ture the state of the cluster (e.g., the number of executors currently
assigned to each stage), as well as the statistics that may help in
7We discuss Spark’s “standalone” mode of operation here (http://spark.apache.org/docs/
latest/spark-standalone.html); YARN-based deployments can, in principle, use Decima,
but require modifying both Spark and YARN.
276
050100150Job sizeJob sequence 1Job sequence 2Taking the same actionat the same stateat time t0100200300400500600700Time (seconds)0510Penalty(neg. reward)But the reward feedbacksare vastly differentSIGCOMM ’19, August 19-23, 2019, Beijing, China
H. Mao et al.
Figure 8: Spark standalone cluster architecture, with Decima additions
highlighted.
scheduling decisions (e.g., a stage’s average task duration). These
statistics depend on the information available (e.g., profiles from past
executions of the same job, or runtime metrics) and on the system
used (here, Spark). Decima can easily incorporate additional signals.
Neural network architecture. The graph neural network’s six trans-
formation functions f (·) and д(·) (§5.1) (two each for node-level,
job-level, and global embeddings) and the policy network’s two score
functions q(·) and w(·) (§5.2) are implemented using two-hidden-
layer neural networks, with 32 and 16 hidden units on each layer.
Since these neural networks are reused for all jobs and all parallelism
limits, Decima’s model is lightweight — it consists of 12,736 parame-
ters (50KB) in total. Mapping the cluster state to a scheduling decision
takes less than 15ms (Figure 15b).
6.2 Spark simulator
Decima’s training happens offline using a faithful simulator that has
access to profiling information (e.g., task durations) from a real Spark
cluster (§7.2) and the job run time characteristics from an industrial
trace (§7.3). To faithfully simulate how Decima’s decisions interact
with a cluster, our simulator captures several real-world effects:
(1) The first “wave” of tasks from a particular stage often runs slower
than subsequent tasks. This is due to Spark’s pipelined task exe-
cution [63], JIT compilation [47] of task code, and warmup costs
(e.g., making TCP connections to other executors). Decima’s sim-
ulated environment thus picks the actual runtime of first-wave
tasks from a different distribution than later waves.
(2) Adding an executor to a Spark job involves launching a JVM
process, which takes 2–3 seconds. Executors are tied to a job
for isolation and because Spark assumes them to be long-lived.
Decima’s environment therefore imposes idle time reflecting the
startup delay every time Decima moves an executor across jobs.
(3) A high degree of parallelism can slow down individual Spark
tasks, as wider shuffles require additional TCP connections and
create more work when merging data from many shards. Decima’s
environment captures these effects by sampling task durations
from distributions collected at different levels of parallelism if
this data is available.
In Appendix D, we validate the fidelity of our simulator by comparing
it with real Spark executions.
7 Evaluation
We evaluated Decima on a real Spark cluster testbed and in simu-
lations with a production workload from Alibaba. Our experiments
address the following questions:
(a) Batched arrivals.
(b) Continuous arrivals.
Figure 9: Decima’s learned scheduling policy achieves 21%–3.1× lower
average job completion time than baseline algorithms for batch and continuous
arrivals of TPC-H jobs in a real Spark cluster.
(1) How does Decima perform compared to carefully-tuned heuristics
in a real Spark cluster (§7.2)?
(2) Can Decima’s learning generalize to a multi-resource setting with
different machine configurations (§7.3)?
(3) How does each of our key ideas contribute to Decima’s perfor-
mance; how does Decima adapt when scheduling environments
change; and how fast does Decima train and make scheduling
decisions after training?
7.1 Existing baseline algorithms
In our evaluation, we compare Decima’s performance to that of seven
baseline algorithms:
(1) Spark’s default FIFO scheduling, which runs jobs in the same
order they arrive in and grants as many executors to each job as
the user requested.
(2) A shortest-job-first critical-path heuristic (SJF-CP), which priori-
tizes jobs based on their total work, and within each job runs tasks
from the next stage on its critical path.
(3) Simple fair scheduling, which gives each job an equal fair share
of the executors and round-robins over tasks from runnable stages
to drain all branches concurrently.
(4) Naive weighted fair scheduling, which assigns executors to jobs
proportional to their total work.
i /
iT α
(5) A carefully-tuned weighted fair scheduling that gives each job
T α
i of total executors, where Ti is the total work of each
job i and α is a tuning factor. Notice that α = 0 reduces to the
simple fair scheme, and α =1 to the naive weighted fair one. We
sweep through α ∈{−2,−1.9,...,2} for the optimal factor.
(6) The standard multi-resource packing algorithm from Tetris [34],
which greedily schedules the stage that maximizes the dot product
of the requested resource vector and the available resource vector.
(7) Graphene∗, an adaptation of Graphene [36] for Decima’s discrete
executor classes. Graphene∗ detects and groups “troublesome”
nodes using Graphene’s algorithm [36, §4.1], and schedules them
together with optimally tuned parallelism as in (5), achieving the
essence of Graphene’s planning strategy. We perform a grid search
to optimize for the hyperparameters (details in Appendix F).
7.2 Spark cluster
We use an OpenStack cluster running Spark v2.2, modified as de-
scribed in §6.1, in the Chameleon Cloud testbed. 8 The cluster consists
8https://www.chameleoncloud.org
277
DAG	SchedulerTask	SchedulerApp 1App 2Spark	MasterDecimaAgentNew job:Update job infoSubmit  tasksJob ends:Move executorsLearning Scheduling Algorithms for Data Processing Clusters
SIGCOMM ’19, August 19-23, 2019, Beijing, China
Figure 10: Time-series analysis (a, b) of continuous TPC-H job arrivals to
a Spark cluster shows that Decima achieves most performance gains over
heuristics during busy periods (e.g., runs jobs 2× faster during hour 8), as
it appropriately prioritizes small jobs (c) with more executors (d), while
preventing work inflation (e).
of 25 worker VMs, each running two executors on an m1.xlarge in-
stance (8 CPUs, 16 GB RAM) and a master VM on an m1.xxxlarge in-
stance (16 CPUs, 32 GB RAM). Our experiments consider (i) batched
arrivals, in which multiple jobs start at the same time and run until
completion, and (ii) continuous arrivals, in which jobs arrive with
stochastic interarrival distributions or follow a trace.
Batched arrivals. We randomly sample jobs from six different input
sizes (2, 5, 10, 20, 50, and 100 GB) and all 22 TPC-H [73] queries,
producing a heavy-tailed distribution: 23% of the jobs contain 82% of
the total work. A combination of 20 random jobs (unseen in training)
arrives as a batch, and we measure their average JCT.
Figure 9a shows a cumulative distribution of the average JCT over
100 experiments. There are three key observations from the results.
First, SJF-CP and fair scheduling, albeit simple, outperform the FIFO
policy by 1.6× and 2.5× on average. Importantly, the fair scheduling
policies outperform SJF-CP since they work on multiple jobs, while
SJF-CP focuses all executors exclusively on the shortest job.
Second, perhaps surprisingly, unweighted fair scheduling outper-
forms fair scheduling weighted by job size (“naive weighted fair”).
This is because weighted fair scheduling grants small jobs fewer
executors than their fair share, slowing them down and increasing
average JCT. Our tuned weighted fair heuristic (“opt. weighted fair”)
counters this effect by calibrating the weights for each job on each ex-
periment (§7.1). The optimal α is usually around −1, i.e., the heuristic
sets the number of executors inversely proportional to job size. This
policy effectively focuses on small jobs early on, and later shifts to
running large jobs in parallel; it outperforms fair scheduling by 11%.
Finally, Decima outperforms all baseline algorithms and improves
the average JCT by 21% over the closest heuristic (“opt. weighted
fair”). This is because Decima prioritizes jobs better, assigns efficient
executor shares to different jobs, and leverages the job DAG structure
(§7.4 breaks down the benefit of each of these factors). Decima au-
tonomously learns this policy through end-to-end RL training, while
the best-performing baseline algorithms required careful tuning.
Continuous arrivals. We sample 1,000 TPC-H jobs of six different
sizes uniformly at random, and model their arrival as a Poisson process
with an average interarrival time of 45 seconds. The resulting cluster
load is about 85%. At this cluster load, jobs arrive faster than most
heuristic-based scheduling policies can complete them. Figure 9b
(a) Industrial trace replay.
(b) TPC-H workload.
Figure 11: With multi-dimensional resources, Decima’s scheduling policy
outperforms Graphene∗ by 32% to 43% in average JCT.
(a) Job duration grouped by to-
tal work, Decima normalized to
Graphene∗.
Figure 12: Decima outperforms Graphene∗ with multi-dimensional resources
by (a) completing small jobs faster and (b) use “oversized” executors for small
jobs (smallest 20% in total work).
(b) Number of executors that Decima
uses for “small” jobs, normalized to
Graphene∗.
shows that Decima outperforms the only baseline algorithm that can
keep up (“opt. weighted fair”); Decima’s average JCT is 29% lower.
In particular, Decima shines during busy, high-load periods, where
scheduling decisions have a much larger impact than when cluster
resources are abundant. Figure 10a shows that Decima maintains a
lower concurrent job count than the tuned heuristic particularly during
the busy period in hours 7–9, where Decima completes jobs about
2× faster (Figure 10b). Performance under high load is important
for batch processing clusters, which often have long job queues [66],
and periods of high load are when good scheduling decisions have
the most impact (e.g., reducing the overprovisioning required for
workload peaks).
Decima’s performance gain comes from finishing small jobs faster,
as the concentration of red points in the lower-left corner of Fig-
ure 10c shows. Decima achieves this by assigning more executors
to the small jobs (Figure 10d). The right number of executors for
each job is workload-dependent: indiscriminately giving small jobs
more executors would use cluster resources inefficiently (§2.2). For
example, SJF-CP’s strictly gives all available executors to the small-
est job, but this inefficient use of executors inflates total work, and
SJF-CP therefore accumulates a growing backlog of jobs. Decima’s
executor assignment, by contrast, results in similar total work as with
the hand-tuned heuristic. Figure 10e shows this: jobs below the diago-
nal have smaller total work with Decima than with the heuristic, and
ones above have larger total work in Decima. Most small jobs are on
the diagonal, indicating that Decima only increases the parallelism
limit when extra executors are still efficient. Consequently, Decima
successfully balances between giving small jobs extra resources to
finish them sooner and using the resources efficiently.
278
(a)(b)(c)(d)(e)20406080100Total worN (×1000)0.60.81.01ormalizedjob duration0.250.50.751Executor memory0.00.51.01.5Normalizedexecutor countSIGCOMM ’19, August 19-23, 2019, Beijing, China
H. Mao et al.
7.3 Multi-dimensional resource packing
The standalone Spark scheduler used in our previous experiments
only provides jobs with access to predefined executor slots. More
advanced cluster schedulers, such as YARN [75] or Mesos [41], al-
low jobs to specify their tasks’ resource requirements and create
appropriately-sized executors. Packing tasks with multi-dimensional
resource needs (e.g., ⟨CPU, memory⟩) onto fixed-capacity servers
adds further complexity to the scheduling problem [34, 36]. We use a
production trace from Alibaba to investigate if Decima can learn good
multi-dimensional scheduling policies with the same core approach.
Industrial trace. The trace contains about 20,000 jobs from a pro-
duction cluster. Many jobs have complex DAGs: 59% have four or
more stages, and some have hundreds. We run the experiments using
our simulator (§6.2) with up to 30,000 executors. This parameter
is set according to the maximum number of concurrent tasks in the
trace. We use the first half of the trace for training and then compare
Decima’s performance with other schemes on the remaining portion.
Multi-resource environment. We modify Decima’s environment to
provide several discrete executor classes with different memory sizes.
Tasks now require a minimum amount of CPU and memory, i.e., a
task must fit into the executor that runs it. Tasks can run in executors
larger than or equal to their resource request. Decima now chooses
a DAG stage to schedule, a parallelism level, and an executor class to
use. Our experiments use four executor types, each with 1 CPU core
and (0.25,0.5,0.75,1) unit of normalized memory; each executor class
makes up 25% of total cluster executors.
Results. We run simulated multi-resource experiments on continuous
job arrivals according to the trace. Figure 11a shows the results for
Decima and three other algorithms: the optimally tuned weighted-fair
heuristic, Tetris, and Graphene∗. Decima achieves a 32% lower aver-
age JCT than the best competing algorithm (Graphene∗), suggesting
that it learns a good policy in the multi-resource environment.
Decima’s policy is qualitatively different to Graphene∗’s. Fig-
ure 12a breaks Decima’s improvement over Graphene∗ down by jobs’
total work. Decima completes jobs faster than Graphene∗ for all job
sizes, but its gain is particularly large for small jobs. The reason is that
Decima learns to use “oversized” executors when they can help finish
nearly-completed small jobs when insufficiently many right-sized
executors are available. Figure 12b illustrates this: Decima uses 39%
more executors of the largest class on the jobs with smallest 20% total
work (full profiles in Appendix G). In other words, Decima trades off
memory fragmentation against clearing the job queue more quickly.
This trade-off makes sense because small jobs (i) contribute more
to the average JCT objective, and (ii) only fragment resources for
a short time. By contrast, Tetris greedily packs tasks into the best-
fitting executor class and achieves the lowest memory fragmentation.
Decima’s fragmentation is within 4%–13% of Tetris’s, but Decima’s
average JCT is 52% lower, as it learns to balance the trade-off well.
This requires respecting workload-dependent factors, such as the
DAG structure, the threshold for what is a “small” job, and others.
Heuristic approaches like Graphene∗ attempt to balance those factors
via additive score functions and extensive tuning, while Decima learns
them without such inputs.
We also repeat this experiment with the TPC-H workload, using
200 executors and sampling each TPC-H DAG node’s memory request
from (0,1]. Figure 11b shows that Decima outperforms the competing
algorithms by even larger margins (e.g., 43% over Graphene∗). This
is because the industrial trace lacks work inflation measurements for
different levels of parallelism, which we provide for TPC-H. Decima
learns to use this information to further calibrate executor assignment.
7.4 Decima deep dive