RISE Bdbd.
Skyriver
Watch Comm.
Satellite
ViaSat
SageNet
All
Table 1: Summary of dataset for large ISPs. “All” includes
data from smaller ISPs. (Here, we count D.C. as a state.)
47,371,118
1,117,162
2,391,568,235
51
47
28
47
51
33
30
44
23
1
3
24
6
2
51
33
51
211,684
1,497
10,836,469
65,468
10,273
7,496
75,609
5,680
16,540
1,272,226
5,062
12,032,978
62,010,415
2,518,882
1,343,056
22,105,668
2,548,989
1,866,487
66,557
4,529
5,206
99
30
11
775
71
847
To this end, given a geographic region (FIPS code) where a weather
event is predicted to occur, ThunderPing pings a sample of 100
residential hosts from each provider in the affected region.
IP addresses are found for a given location through the MaxMind
location-to-address service, using data from the same year. Geolo-
cation databases are poor at locating routers [18], but largely agree
on locations of residential addresses (which are all that Thunder-
Ping probes), suggesting reasonable correctness [54]. MaxMind
self-reports 90% city-level accuracy within 100km in the U.S., fur-
ther noting that “IP geolocation is more accurate for broadband IP
addresses and less accurate for cellular networks” [39]. Even in the
event in which residential addresses’ geolocation data is incorrect,
ThunderPing will overestimate weather’s impact only when a host
is in an even more severe weather alert than originally predicted.
The only scenario in which we have observed this to be an issue is
during hurricanes, wherein adverse weather conditions can be ex-
tremely widespread. Thus, to avoid this issue, we omit data during
any interval of time when there is a hurricane anywhere in the U.S.
As before [52], we use reverse DNS (rDNS) records of IP addresses
to determine their provider and link type, when clear. When an
address’s rDNS record contains a cable-only provider like Comcast,
Charter, Suddenlink etc., we determine the link type of the address
to be cable. Since ThunderPing uses rDNS for determining linktype,
some providers like AT&T were not probed by ThunderPing since
AT&T typically does not assign reverse DNS names to their resi-
dential customers. Providers that use a variety of media types to
provide connectivity (typically rural providers) are included under
“All” link types with the rest, but are not classified further.
How ThunderPing probes: ThunderPing sends pings to each
of these hosts from up to 10 geographically dispersed PlanetLab
vantage points every 11 minutes. This interval is inspired by Heide-
mann et al. [22] and was then also used by Trinocular [46]. When
a PlanetLab node fails, ThunderPing is designed to replace it, but
in our analyses, if the number of working vantage points drops
below three, we discard observations at that time as untrustworthy.
ThunderPing retransmits failed ICMP requests: when a vantage
point does not receive a response, it retries with an exponential
backoff up to 10 times within the 11 minute probing interval.
ThunderPing relies on active probing, but providers may ad-
ministratively filter ICMP requests and home routers may decline
to respond. We assume that such providers are no more or less
vulnerable to weather.
Determining actual weather conditions: Recall that weather
alerts merely predict weather conditions; it is of course possible
that other weather materializes. To analyze our data, we use publicly
available NWS datasets of observed weather. The NWS operates
a network of 900 automated “ASOS” weather stations, typically
located at airports. The NWS weather stations record hourly obser-
vations of 24 weather variables in METAR format. The NWS makes
these observations available online [40].
There are two types of weather information: categories that
account for the common precipitation types (e.g., thunderstorm,
hail, snow) and continuous variables (e.g., wind speed, precipitation
quantity). In this paper, we analyze both.
2.2 Dropouts
A “dropout” occurs when the IP address representing a residential
link that is responsive to pings from at least three vantage points
transitions to being unresponsive from all of the vantage points
for 11 minutes. This definition permits some vantage points to be
faulty or congested; requiring three is meant to avoid classifying
in-network faults as residential link faults, and is based on prior
work [46]. A dropout can occur within a “responsive address hour,”
a continuous observation of an IP address in known weather condi-
tions where the address is responsive to pings at the beginning of
the hour. A responsive hour may or may not include a dropout, and
the ratio of dropouts to responsive hours is our measure of outage
likelihood. Two addresses observed in the same hour or a single
address observed for two hours are equivalent. The “hour” dura-
tion aligns with the typical weather report interval. We consider at
most one dropout per hour per address. An address that alternates
between responsive and unresponsive will be counted as a single
dropout per hour. This treats the potentially multiple dropouts as
part of a single outage event, similar to how multiple lost packets
in TCP are part of the same congestion event.
Observing a dropout is a sign that a residential link may (or may
not) have experienced an outage, but not all dropouts are necessarily
outages. Dropouts can also occur if the device re-attaches to the
network with a new address after only momentary disconnection,
typically through re-association of a PPP session for a DSL modem,
but potentially through administrative renumbering of prefixes. In
Sections 3.3 and 5, we describe how we account for these activities.
We annotate each responsive address hour for an address with
the corresponding weather information associated with the geo-
graphically closest weather station to that address. Doing so allows
us to find the number of responsive address hours and dropout
address hours in specific weather conditions.
SIGCOMM ’19, August 19–23, 2019, Beijing, China
Ramakrishna Padmanabhan, Aaron Schulman, Dave Levin, and Neil Spring
2.3 Dataset summary
ThunderPing has been running for eight years, and has collected 101
billion responsive pings to 10.8 million residential addresses. This
dataset comprises observations from Apr 2011 to Dec 2018, though
only 1,811 days included sufficiently many operating vantage points
to classify a responsive address hour.
We show per-ISP highlights in Table 1. We observe major providers
such as Comcast, Qwest, and ViaSat in all 50 states (and DC). Of the
2.39B responsive address hours from Table 1, 216M (9%) were hours
where responsive addresses experienced rain, 91M (4%) snow, and
27M (1%) thunderstorm.
Hurricanes manifest as a combination of weather features and
are so pronounced that their contribution to thunderstorm or rain
outages would be disproportionate. We thus omit them from cate-
gorical weather classification (e.g., Figure 2). However, we consider
data from Hurricane events when studying continuous variables
(e.g., inches of rain and wind speed) where extremes are clearly
distinguishable. Eleven hurricanes made U.S. landfall during our
measurement. Collectively, these hurricane times account for less
than 3% of responsive address hours and 4% of dropout hours.
3 QUANTIFYING WEATHER DROPOUTS
Our overarching goal is to quantify weather’s impact on last-mile
link failures. The challenge is to do so in a way that (1) permits a
measure of statistical significance and (2) controls for other, non-
weather-related causes of dropouts. In this section, we describe the
challenges in doing this kind of analysis, and the methodology we
apply to solve them.
3.1 Obtaining statistical significance
It is relatively straightforward to derive tight confidence intervals
when there is a large amount of data. Although our dataset includes
billions of pings, the events we consider in this paper—dropouts—
occur exceedingly rarely. On average, we observed a link dropping
out only once every 3–35 days that we were actively pinging and
receiving responses from the link’s address, depending on the link
type. The inverse of the average dropout probability per link type—
including the average across all link types—is as follows:
3
11
14
22
35
Link type: Fiber Cable DSL WISP Sat All
8
Days b/w dropouts:
Rare events complicate statistical significance: a single additional
occurrence could, if not careful, drastically alter the results.
Fortunately, there is a well-established set of techniques from
the field of epidemiology that permit statistical significance over
rare events. Epidemiology—the study of the occurrence and de-
terminants of disease—faces similar challenges when analyzing
mortality: deaths (“failure”) are rare, and subjects (“hosts”) can be
exposed to their disease (“weather condition”) for different amounts
of time until the time of death (“dropout”). Here, we describe the
techniques we borrow from biostatistics [59] to address these con-
cerns. Throughout our study, we will consider different “bins” of
subjects: each “bin” is a group of addresses with a specific link type,
geographic region, or combination thereof, experiencing a weather
condition. For example, WISP addresses in the U.S. experiencing
gale-force winds is one possible bin, cable addresses in Florida
experiencing thunderstorms is another, and so on.
Like in epidemiological studies, we focus on estimating the haz-
ard rate (sometimes referred to as the instantaneous death rate).
In essence, what a hazard rate gives us is the expected number of
deaths per unit time. More concretely, for a given hazard rate λ, the
probability of death over a short duration of time t is λ · t.
When presenting our results, we use an hour as the short dura-
tion t. This allows us to align host availability with corresponding
weather information (the NWS reports weather data every hour).
Moreover, unlike with epidemiological studies, hosts can be revived
after death—and can even die multiple times in an hour. To avoid
biasing our results towards hosts that repeatedly fail and recover
over a short period of time, recall from Section 2.2 that we treat
multiple dropouts within a given hour as a single “dropout-hour”.
As a result, in our analysis, the hazard rate captures the hourly
probability that a host drops out (one or more times).
The first challenge in estimating hazard rates is that different
hosts in a bin may be observed over different periods of time: in
our study, hosts that remain responsive can naturally be observed
for longer periods of time than those that drop out. We track the
number of hours Oi that we observe each address i = 1, . . . , n to be
responsive to pings (and therefore had the opportunity to dropout),
and we also count the number of hours where the addresses experi-
enced at least one dropout, D. An unbiased estimate of the hazard
rate ˆλ for that bin can be obtained as follows [59, Chapter 15.4]:
Dn
i =1 Oi
ˆλ =
(1)
We exclude any bin if it lacks enough observed hours to permit
computing confidence intervals. We adhere to the following rule of
thumb [59, Chapter 6]: we accept a bin with n samples (responsive
hours) and estimated hazard rate ˆλ only if:
n ≥ 40 and n ˆλ(1 − ˆλ) ≥ 10
(2)
Thus, we require more samples to obtain statistical significance
for very small (or very large) values of ˆλ; for example, to estimate
ˆλ = 0.00139 (one dropout-hour per month), we would require 7205
samples. When the conditions in Eq. (2) do not hold, we do not
report the results for the bin. We had enough samples to report
results with statistical significance for almost all the bins analyzed in
the paper; the exceptions were instances of particularly uncommon
weather in an area, such as snow in Florida.
When the conditions in Eq. (2) hold, we can calculate 95% con-
fidence intervals over the estimated hazard rate as follows [59,
Chapter 6.3]:
(cid:115) ˆλ(1 − ˆλ)
ˆλ ± 1.96 ·
n
(3)
The above calculations yield the hazard rate along with its con-
fidence intervals1; what remains is to compare two hazard rates,
for instance, the overall hazard rate for a given link type and the
hazard rate for that link type specifically in the presence of snow.
Two estimated hazard rates(cid:98)λ1 and(cid:98)λ2 can be compared by simply
subtracting them [59]. Fortunately, with sufficiently many samples,
1The constants in the equations above are recommendations from [59, Chapter 6]
Residential Links Under the Weather
SIGCOMM ’19, August 19–23, 2019, Beijing, China
the confidence intervals over the difference of two hazard rates is
given by the addition of the confidence intervals over the original
hazard rates.2
To summarize: throughout this paper, we bin addresses experi-
encing a weather event by link type, geography, or both, compute
hazard rates for these bins using Eq. (1), discard any bins that do
not satisfy Eq. (2), and compute confidence intervals using Eq. (3).
3.2 Controlling for other dropout causes
The mere fact that a dropout occurs during a weather event is not
sufficient evidence to conclude that it was caused by (or even corre-
lated with) the weather event itself. Dropouts can also be caused by
completely weather-independent events—such as regularly sched-
uled network maintenance periods—or partially weather-dependent
events—such as a car accident leading to an above-ground wire
being knocked down. Our goal is to completely filter out the first
and to filter out the non-weather related instances of the second.
To control for a “baseline” probability of dropouts, we measure
the additive difference in dropout probabilities between times of
inclement weather and times of no inclement weather. More con-
cretely, we consider a set of 11 different kinds of weather events,
listed in Figure 2, including tornado, thunderstorm, various sever-