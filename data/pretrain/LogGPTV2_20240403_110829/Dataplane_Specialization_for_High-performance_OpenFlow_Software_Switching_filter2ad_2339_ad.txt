the compiled datapath by simply hoarding all the necessary
template objects into a single binary. At this point the bi-
nary still contains placeholders for the ﬂow keys, which will
be patched into the code in the template specialization pass.
Note that this step is not necessarily inevitable; the code
could as well include pointers to the memory locations of
the ﬂow keys instead of the keys themselves. We still de-
cided to patch keys right into the code; we found that stan-
dard OpenFlow datapath processing burdens the CPU data
caches extensively, but compiling match keys right into the
code directs some of this load to the CPU instruction caches,
which gives greater locality, better distribution of CPU cache
load, and hence faster processing (see Section 4.3).
The code still contains many dangling jump pointers, like
ADDR_NEXT_FLOW. These are resolved in the ﬁnal link-
ing pass; jump pointers are again statically compiled into
the code, with the exception of goto_table action jumps
which go via a trampoline. The reason for this indirection is
to improve the granularity of datapath updates.
3.4 Updates
For templates that support incremental updates (compound
hash, LPM, and linked list), ESWITCH performs updates
non-destructively: whenever the controller modiﬁes a ﬂow
ESWITCH simply updates the data structure underlying the
template. Complete rebuilding happens only for the direct
545
code template (unconditionally), for the hash template to
minimize hash collisions (periodically), or when the modiﬁ-
cation would violate the prerequisite for the current template
and a fallback must be constructed. But even then ESWITCH
can continue processing packets during the rebuild, as the
new template representation is constructed side by side with
the running datapath and then inserted into the pipeline by
atomically redirecting all referring goto_table jumps to
the address of the new code via the trampoline.
This update mechanism gives two important beneﬁts. First,
datapath updates in ESWITCH are transactional, with partial
updates automatically rolled back, which eliminates incon-
sistent behavior common to many OpenFlow switches [51].
Second, updates are of per-ﬂow-table granularity, which is
certainly an improvement over OVS that needs to invalidate
the entire megaﬂow cache (shared across the pipeline) on
any update and re-populate it, again reactively, from scratch.
4. EVALUATION
Next we turn to discuss the practical aspects of ESWITCH
and make our case for dataplane specialization. We take
four illustrative use cases from operational OpenFlow de-
ployments [17, 18, 30], which will serve for demonstrating
the ESWITCH pipeline compilation process and also as basic
scenarios for performance evaluations. First, we present the
use cases themselves, then we give a detailed description of
our ESWITCH prototype and present the measurement stud-
ies, and ﬁnally we derive a rudimentary performance model
and we show that this simple model can already supply use-
ful performance characterizations.
In what follows, we shall weigh ESWITCH against OVS,
the ﬂagship OpenFlow softswitch, and show major perfor-
mance improvements. The subsequent discussion, however,
is nowhere meant to be a critique on OVS per se; in fact,
during working on this paper we have come to truly admire
the amount of engineering that went into OVS datapaths;
our goal is, accordingly, not to call out OVS but rather to
point out the advantages of dataplane specialization over a
ﬂow-caching-centered architecture.
4.1 Use Cases
The ﬁrst two use cases are Layer-2 switching, i.e., packet
forwarding by exact-matching on a MAC table, and Layer-
3 routing, i.e., longest-preﬁx-matching IP addresses from a
routing table. These use cases model pure run-to-completion
pipelines, whereas all processing is speciﬁed in a single ﬂow
table (see e.g., L2 in [12], L3 in [13]). Notably, ESWITCH
attains optimal dataplane specialization in both cases: the L2
pipeline compiles into the hash table template, effectively re-
ducing into a conventional Ethernet software switch, while
the L3 pipeline is compiled into the LPM template yield-
ing a datapath identical to that of an IP softrouter. For each
use case we prepared a suite of ﬂow tables with different
number of entries in order to measure switches’ robustness
to pipeline complexity and we also generated a sequence of
trafﬁc mixes to test robustness against the number of active
ﬂows. The L2 ﬂow tables contained random MAC addresses
and the L2 destination addresses in the ﬂow mix were ade-
quately aligned to avoid frequent table misses; for the L3 use
case routing tables were randomly sampled from a real In-
ternet router and again the traces were adjusted accordingly.
The rest of the use cases, a load balancer and an access
gateway, model multi-stage pipeline applications [17, 18].
The load balancer use case captures the functionality of
a web frontend, which distributes HTTP trafﬁc for different
web services, available at different IP addresses, between
backend servers. Load distribution happens based on the
ﬁrst bit of the source IP address in the incoming packets. In
the ingress direction only web trafﬁc is allowed, while traf-
ﬁc is forwarded unconditionally in the other direction (see
Fig. 7a). A naive compiler would represent this single-table
pipeline with the linked list template, leading to an inefﬁ-
cient datapath. However, our table decomposition algorithm
can infer an equivalent multi-stage pipeline (see Fig. 7b),
whereas all tables ﬁt the direct code template as well as the
hash template. This gives rise to a more efﬁcient fast path,
demonstrating the power of table decomposition. Flow table
complexity was set by varying the number of web services
between 1 and 100 and trafﬁc traces were generated so that
half of the packets go to a random web service and the rest
of the trafﬁc be dropped.
The most complex use case is a telco access gateway,
which models a virtual Provider Endpoint (vPE) providing
users Internet access via Customer Endpoints (CEs). Each
CE is identiﬁed by a unique VLAN tag and each user is
assigned a per-CE unique private IP address (see Fig. 8).
The gateway pipeline is as follows. Table 0 separates user–
network trafﬁc on a per-CE basis from network–user traf-
ﬁc; user–network trafﬁc in turn goes to separate per-CE ta-
bles that identify users and swap the (private) source IP ad-
dress with a unique public address (realizing a simple NAT)
and then to the Internet based on an IP routing table (Ta-
ble 110). Packets missing the per-CE tables are passed to
the controller that does admission control, allocates a public
IP, and installs per-user “NAT” rules into the proper tables.
In the reverse direction, packets are mapped from the pub-
lic IP back to the adequate combination of VLAN tag and
user private address. ESWITCH compiles this pipeline using
the hash template for each table except for Table 110 that is
mapped to the LPM store. In each measurement we provi-
sioned 10 CEs with 20 users per CE and the IP routing table
contained 10K IP preﬁxes, and the trafﬁc mix was generated
by varying the number of per-user ﬂows.
4.2 Prototype & Evaluation Platform
We implemented a proof-of-concept ESWITCH prototype
on top of the Intel DataPlane Development Kit (DPDK, [40]).
The DPDK provides a highly efﬁcient user space network-
ing toolchain, with NIC polling drivers, batch processing,
direct cache access, and NUMA optimized memory pools,
as well as some prefab ﬂow table templates (LPM). Our pro-
totype implements a useful subset of OpenFlow 1.3 along
with the main ESWITCH features with one notable excep-
tion: at the moment it defaults to a combined L2–L4 packet
parser. Adding full support for parser templates is underway.
546
(a) pipeline
(a) access gateway setup
(b) decomposed pipeline
Figure 7: Load balancer use case.
(b) pipeline
Figure 8: Access gateway use case.
Figure 9: Running time as the function
of the number of ﬂow entries for the dif-
ferent ﬂow table templates.
Figure 10: Packet rate for L2 switching
over MAC tables of size 1, 10, 100, and
1K entries, as the active ﬂow set grows.
Figure 11: Packet rate for L3 routing
over 1, 10, and 1K IP preﬁxes, as the
active ﬂow set grows.
When not stated otherwise, the system-under-test (SUT)
was set up as of Table 1, connected via a 40 Gbps inter-
face back to back to a similarly conﬁgured system that ran
the Network Function Performance Analyzer (NFPA, [52]),
a home-grown measurement platform using the DPDK pk-
tgen packet generator, conﬁgured in loopback mode. All
evaluations were done using the DPDK datapath of OVS
compiled with the same DPDK version as ESWITCH, with
minimum sized (64 byte) packets on a single CPU core; we
found that both ESWITCH and OVS scale to larger packet
sizes and more cores as expected (but see later). The max-
imum single-core packet rate attainable with DPDK on this
platform is 15.7 million packets per second (Mpps), mea-
sured in port-forward mode with the DPDK l2fwd tool; we
shall set this metric as a benchmark for the measurements.
4.3 Measurement Results
Fine-tuning template application. Our ﬁrst series of mea-
surements were aimed at calibrating the code generation pro-
cess. Of particular interest were designating the ﬂow table
template fallbacks and adjusting the rules of when to invoke
these fallbacks. For a series of increasingly larger synthetic
ﬂow tables with the N-th entry set to
vlan_vid=3,ip_src=10.0.0.3,ip_proto=17,udp_dst=N ,
the execution time (in terms of CPU cycle count at 99% con-
ﬁdence level) needed to perform a ﬂow lookup by the direct
Table 1: System-under-test datasheet.
CPU: Intel Xeon CPU E5-2620 @ 2.00GHz, Sandy Bridge
Caches: 32k L1i and L1d, 256 KB L2, 15 MB L3
Cache latency: L1 = 4 cycles, L2 = 12 cycles, L3 = 29 cycles
Memory: 64 GB DDR3 @ 1333 MHz, 4-channels
NIC: Intel XL710, PCI Express 3.0/x8, 40 Gb
DPDK v2.2.0, Open vSwitch (DPDK datapath) version 2.5.90
code, the compound hash, and the linked list template, is
shown in Fig. 9 (the LPM template does not apply to this
ﬂow table). Until about 4 entries the direct code template is
the most efﬁcient choice, but from that point the hash tem-
plate becomes faster thanks to its constant lookup time. Ac-
cordingly, we ﬁxed the fallback constant for the direct code
template at 4; tables with at most that many entries are di-
rectly compiled while for larger tables the hash template is
preferred. We set the linked list as the last-resort fallback for
complex ﬂow tables despite being consistently slower than
the direct code, since it supports fast incremental updates.
Packet rate. The raw packet throughput for ESWITCH (ES)
vs Open vSwitch (OVS) is given for the L2 use case in
Fig. 10, for L3 in Fig. 11, for the load balancer in Fig. 12,
and for the gateway in Fig. 13, respectively, over different
pipeline complexities and increasingly more diverse trafﬁc
mixes. The main observations are as follows. As the number
of active ﬂows increases, that is, as trafﬁc locality is gradu-
547
 0 10 20 30 1 2 3 4 5 6 7 8 9running time [CPU cycles]number of flow entriesdirect codehashlinked list2M6M10M14M1101001K10K100Kpacket rate [pps]number of active flowsES(1)ES(10)ES(100)ES(1K)OVS(1)OVS(10)OVS(100)OVS(1K)2M6M10M12M1101001K10K100Kpacket rate [pps]number of active flowsES(1)ES(10)ES(1K)OVS(1)OVS(10)OVS(1K)Figure 12: Packet rate for the load bal-
ancer use case over 1, 10, and 100 web
services, as the active ﬂow set grows.
Figure 13: Packet rate for the gateway
use case with 10 CEs, 20 user/CE, and
10K IP preﬁxes, with estimated lower
and upper bounds.
Figure 14: Fraction of packets for-
warded at different levels of the OVS
cache hierarchy as the active ﬂow set
grows (gateway use case).
Figure 15: Last-level CPU cache (LLC)
misses per packet measured with the
perf tool, as the active ﬂow set grows
(gateway use case).
Figure 16: Latency in terms of mean
CPU cycles/packet on the gateway
pipeline as the active ﬂow set grows,
with estimated lower and upper bounds.
Figure 17: Total time to set up the
load-balancer pipeline with a command
line tool (CLI) and OpenFlow controller
(ctrl), as the number of services grows.
ally removed, so the performance of OVS ﬂow caching de-
teriorates. We already experience major performance drops
at as few as 10 active ﬂows, and for 100 ﬂows the packet
rate essentially halves (or even worse). The reason is re-
vealed in Fig. 14, which gives a rundown on cache hit inten-
sities experienced at different levels of the OVS ﬂow cache
hierarchy: as the active ﬂow set grows packet processing
gradually shifts from the very fast microﬂow cache to the
slower megaﬂow cache and ﬁnally to the vswitchd slow
path. This goes hand in hand with a degradation of CPU
cache afﬁnity (Fig. 15): as long as packet processing occurs
entirely inside the microﬂow cache (up to ∼1K ﬂows) OVS
basically never misses the CPU cache, while the megaﬂow
cache and vswitchd make excess out-of-cache memory
references. ESWITCH, on the other hand, sidesteps ﬂow
caching and exhibits robust and high packet rate over essen-
tially all OpenFlow pipelines and all trafﬁc mixes examined,
consistently reaching 12–14 Mpps packet rate (close to the
platform benchmark of 15 Mpps) when the active ﬂow set is
not too large, and 9–12 Mpps with many ﬂows.
In summary, ESWITCH generally achieves 2–7 times higher
packet throughput than OVS but the factor can grow up to
two orders of magnitude(!) for complex pipelines with many
active ﬂows. For the gateway use case, OVS throughput
drops hundredfold to a mere 90K packets per second at 1M
ﬂows, which, if exploited by a malicious user, means a full-
blown denial of service to the entire user population. Mean-
while, ESWITCH robustly delivers over 9 Mpps packet rate,
suggesting that it is not susceptible to such attacks. Since
both switches use the DPDK, the slowdown of OVS is clearly
attributed to some overhead in the datapath code; we sug-
gest that the culprit is generic ﬂow-caching.