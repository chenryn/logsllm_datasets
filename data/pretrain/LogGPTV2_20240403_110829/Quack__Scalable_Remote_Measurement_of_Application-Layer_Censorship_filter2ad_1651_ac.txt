random string to all SYNACK servers, giving us the set of
functioning echo servers. Of these, 47,276 remained Stable over
24 hours, making them useful for long running experiments.
5 Characterization
In order to better understand any biases inherent in our
data, we ﬁrst characterize the population of echo servers
we make use of in our study.
5.1 Discovery
To discover echo servers in diverse subnets and geo-
graphic locations, we perform Internet-wide scans with
the ZMap toolchain [15] on the IPv4 address space. We
ran daily scans for two months, between June 1st to July
31st, discovering more than 50,000 echo serves each day.
Upon discovering hosts that respond to our SYN pack-
ets on port 7, we initiate connections to the potential echo
servers. We send a randomly generated string and verify
that they reply with an identical string. During our ﬁrst
trial, we ﬁnd that 57,890 servers reply with the correct
string, over 3,766 ASNs. Many of our experiments take
place over the course of a day, so we measure the cov-
erage of echo servers that reply 24 hours later. We ﬁnd
92% of ASNs have an echo server that is online during
this second test.
In Figure 4, we show the number of servers still online
after 24 hours, which is signiﬁcant because our experi-
ments run over the course of a day. Only those servers that
are stable for at least 24 hours will test all keywords in
the experiment. We observe that this reduces the diversity
of our coverage, but not signiﬁcantly, and note that this
biases our results towards stable echo servers.
5.2 Churn
We looked at our daily scans in order to understand how
stable echo server IP addresses are over time. While an
average of 17% of echo servers churn away from their
IP address within 24 hours, we observed that 18% were
stable and responsive throughout the entire duration of our
measurement. Additionally, the rate at which echo servers
churn decelerates, so the ﬁrst day reports the largest churn
rate across the study.
USENIX Association
27th USENIX Security Symposium    193
OS Family
Windows
Embedded
Linux
Cisco IOS
Unsuccessful identiﬁcation
Other
Echo Servers
180 (32.0%)
139 (24.7%)
71 (12.6%)
38 (6.8%)
99 (17.6%)
35 (6.2%)
Figure 6: Identiﬁcation of Echo Servers—We scanned 562
(1%) echo servers with Nmap’s operating system detector on
July 17, 2017 and found that the most of the echo servers were
either Windows machines or embedded devices, as identiﬁed by
Nmap. This scan yielded a median accuracy of 99%.
On average, we observed echo servers in 177 countries.
Of these countries, we observe an average 39 countries
with more than one hundred echo servers and 82 countries
with more than ﬁfteen echo servers. This provides insight
into a large number of countries.
We compare our method’s coverage with that of the
OONI project [19], which enlists volunteers worldwide
to run scans from local devices to measure network dis-
ruption. OONI makes this data public with the consent of
the volunteers, but probes do not have unique identiﬁers;
therefore, we use the of number of distinct autonomous
systems per country to estimate coverage.
We compared the number of unique ASes observed for
both tests during the week of July 8–15, 2017. As shown
in Figure 7, echo servers have a much more diverse set of
vantage points and over a larger number of countries. Dur-
ing the week of our comparison, OONI data was available
for 113 countries, while echo servers were responsive in
184. Furthermore, the total number of ASes seen in the
echo measurements was nearly an order of magnitude
larger than that of OONI: we observed echo servers in
4458 unique ASes; OONI measures 678. While OONI
probes provide rich measurement for the locations they
have access to, our technique providers broader and more
consistent measurements.
Figure 7: Coverage of Autonomous Systems per Country—
Echo servers were present in 184 countries with 4458 unique
ASes, while OONI probes were in 113 countries with 678 ASes.
Figure 5: Echo Server Churn—Only 18% of tested servers
were reachable in every observation over 2 months of daily scans.
However, 56% were present in both our ﬁrst and ﬁnal scans.
Echo servers not only churn out of the set of IP ad-
dresses from a given day—they also churn back in, as
shown in Figure 5. While we only observed 18% of echo
servers from our ﬁrst discovery scan in every daily scan,
56% of echo servers from our ﬁrst discovery scan were
also in our ﬁnal scan 61 days later.
5.3 Identiﬁcation
To understand the composition of machines running echo
servers, we randomly selected 1% of responding echo
servers on July 17, 2017. For this sample, we performed
OS detection on each IP address using Nmap. The most
common system families as deﬁned by Nmap are shown
in Figure 6. There were 56,228 working echo servers
on this date. Of the 562 we tested, Nmap identiﬁed 463
(82.4%) of the operating systems. Nmap reported a me-
dian accuracy of 99% for the identiﬁcations. This test
covered 54 countries.
Of the echo servers we scanned with Nmap, 251
(44.7%) had full device labels containing the words
“server”, “router”, or “switch”. Of the remaining echo
servers, 70 (12.5%) were Linux, and 26 (4.6%) were Win-
dows. The rest were identiﬁed as various other systems
such as ﬁrewalls, controllers, and embedded systems. In
total, 4% of echo servers were given device labels that left
doubt as to whether they were infrastructure machines,
because they were identiﬁed as non-server Windows ma-
chines, and 2 devices were identiﬁed as running Android.
It would be infeasible to run Nmap’s OS detection service
against all echo machines, but we do not believe that to
be necessary to safely use all functioning echo servers, as
we discuss in Section 3.2.
5.4 Coverage
Echo servers provide us diverse vantage points in a
majority of countries. We associate IPs with au-
tonomous systems using the publicly available Route
Views dataset [39], and locate each server to a country
using the MaxMind GeoIP2 service [29].
194    27th USENIX Security Symposium
USENIX Association
Jun032017Jun102017Jun172017Jun242017Jul012017Jul082017Jul152017Jul222017Jul292017Date0100002000030000400005000060000EchoServersObservedIPsIPsactiveininitialscanIPsactivecontinuously100101102103UniqueASesperCountry050100150200NumberofCountriesEchoOONIFigure 8: Keyword Reliability—Each of 1109 domains were
sent to 54,515 echo servers for the Control and 54,802 for the
Citizen Lab experiment. We count the blocking events per
keyword, observing that the largest blocking rate for a given
keyword was 8.5% in CLBL and 0.08% in the Control. This
supports our hypothesis that these domains are sensitive.
Figure 9: Server Reliability—For both the Control and Citizen
Lab experiments, we send 1109 mock HTTP requests to all echo
servers. We ﬁnd that 98% of servers never resulted in a blocking
event in the Control experiment. We observe signiﬁcantly more
blocking among a small set of servers in the CLBL test. This
demonstrates that interference occurs with very few hosts.
6 Evaluation
In this section, we provide the results of the studies de-
scribed in Section 3. Our evaluation provides support for
the Quack’s practicality as an application-layer measure-
ment tool in two ways. First, we describe what behavior
our measurements detected given a set of URLs known to
be censored, in order to verify that our results correlate
with previously observed phenomena. Then, we support
our claim that our system works at scale, and present the
results of an experiment that measured a larger corpus of
domains across a greater number of countries than any
previous study.
6.1 Validation
We control for noise, non-protocol-compliant servers,
and other anomalous behaviors by measuring echo
server behavior using innocuous domains of the form
testN.example.com. Mock queries to these domains
are used to demonstrate behavior in the absence of dis-
ruption, since these domains are unlikely to be blocked.
This allows us to identify a baseline for ordinary network
and echo server failure when interacting with each remote
network, and understand our subsequent test results in
light of a baseline model of expected behavior.
The ﬁrst assumption we make in designing our control
tests is that the class of domains testN.example.com
will face no blocking by the network between our server
and the echo server. To validate this assumption, we
perform a set of measurements to all echo servers using
only this control class of domains, and consider the failure
percentages we observed. We show the distribution of
failures per domain tested in Figure 8.
We observe a median domain failure rate of less than
0.01%, and a maximum failure rate across 1109 domains
of 0.08%. Additionally, the domains in the upper quartile
of disruption rates are evenly distributed over the class of
innocuous domains, independent of the value of N.
Using the technique described in Section 3.1, we clas-
sify no country as interfering with any of our control
domains. We also conﬁrmed these results using another
control domain: echotest.[redacted].edu, validat-
ing our control.
We assume failures in the absence of network interfer-
ence are independent of which server is used. This allows
us to present a distribution for the null hypothesis that
is independent of either variable, and therefore constant.
A few factors could cause a given server to fail many
innocuous domains: network unreliability, echo server
unreliability, or actual blocking occurring for our innocu-
ous domains. Despite this, in Figure 9 we see that 98% of
servers see no blocking events.
We observe that during the duration of our experiment,
17% of echo servers appear to churn away, which is indi-
cated by their yielding two No Result tests sequentially.
This is roughly as many as we observe churning away
in a day for our discovery scans. This conﬁrms that our
results will be biased toward networks with stable echo
servers.
Finally, we empirically determine how long measure-
ments should wait when a blocking event is detected in or-
der to allow stateful DPI disruption to disengage. Shorter
timeouts will allow us to test more domains against a
given server in a shorter time, while longer timeouts are
less likely to incorrectly classify a domain as a failure due
to a previous sensitive domain having triggered stateful
blocking. Our system as implemented is not fundamen-
tally limited by a longer timeout, because there are more
servers to test at any given time than there are servers wait-
ing for that timeout to expire. As such, the two-minute
delay we empirically determined as shown in Figure 3 is
USENIX Association
27th USENIX Security Symposium    195
100101102103104BlockingEventsoverallServers0.00.20.40.60.81.0FractionofDomainsControlCitizenLab100101102103BlockingEventsoverallDomains0.00.20.40.60.81.0FractionofServersControlCitizenLabCountry
China
Egypt
Iran
Jordan
Kazakhstan
Saudi Arabia
South Korea
Thailand
Turkey
UAE
Uzbekistan
Union
HTTP
126
6
25
8
4
2
14
11
12
8
1
220
Discard
126
5
0
1
0
0
0
0
14
0
—
146
TLS
0
2
374
4
0
0
0
0
14
17
1
435
Top Categories
NEWS, ANON
ANON, NEWS
PORN, LGBT
ANON, NEWS
MMED, FILE
NEWS, ANON
PORN, GMB
PORN, NEWS
ANON, NEWS
NEWS, COMT
MISC
NEWS, ANON
Figure 10: Interference of CLBL—We perform multiple ex-
periments to measure interference of domains in the Citizen Lab
global block list. Quack detected keyword blocking in 13 coun-
tries, with 220 unique domains blocked in our simple HTTP
experiment. There is little intersection between different coun-
tries, and only 20% of tested domains exhibited interference
anywhere. Category abbreviations are deﬁned in the Appendix.
a minimum, and the system may take longer to schedule
the subsequent trial in a test against a disrupted server.
We observe that all delays were less than ﬁve minutes in
practice.
6.2 Detection of Disruption
Next we test each of the domains on the Citizen Lab
global list against all echo servers by formatting them as
valid HTTP GET requests. We expect to see interruption
in this test because the Citizen Lab domains are known
to be blocked in countries around the world. This is
conﬁrmed by the difference to the control in Figure 8 and
Figure 9.
Using our method of classifying interference as de-
scribed in Section 3.1, only 12 countries of 74 tested
against all domains demonstrate evidence of keyword
blocking in this test. The interfering countries, number
of domains for which we observe interference, and what
categories those domains are contained in are given in
Figure 10.
For each country we list in Figure 10, we look for ex-
ternal evidence to support the conclusion that we observe
government-sanctioned censorship. One source of exter-
nal evidence is the Freedom on the Net report by Freedom
House [21]. Of the countries in the table, nine are rated
as “Not Free” and two are rated as “Partially Free.”
South Korea and Jordan are those listed as Partially
Free by Freedom House; however, both are indicated in
ONI’s most recent country proﬁles as performing ﬁlter-
ing [31, 32]. In the case of South Korea, blocking based
on HTTP request content is speciﬁcally identiﬁed. In
further support of the observed phenomenon being action
at a national level, the echo responses in South Korea
that did not match the echo requests were HTTP redirects
to a government-run website outlining the reason the re-
quested domain was blocked. This is another advantage
of the Echo Protocol — we are able to see the responses
injected to the echo server, because they are then echoed
back to us.
Two countries were identiﬁed by our system as having
a signiﬁcant proportion of blocking, but had no evidence
from other sources that there would be restrictions on the
Internet: Ghana and New Zealand. Ghana is not evalu-
ated by Freedom Net, but the Department of State stated
in its 2016 Human Rights report [45] that there were no
governmental restrictions to the Internet. Upon inspecting
the scope of blocking, in both cases, it is restricted to
a single academic network in the country, and all echo
servers in that AS reported interference. In all other coun-
tries identiﬁed by our system as performing blocking, we
observe interference in more than one AS. Our technique
is not ﬁne-grained enough to detect censorship across all
networks, and in these cases we have visibility into only a
few locations that have close proximity. For these reasons,
we exclude Ghana and New Zealand from Figure 10.
While this presents a case that the interference we iden-
tify is genuine, we do not claim that we identify all gen-