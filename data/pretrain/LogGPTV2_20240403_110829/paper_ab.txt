3 	DATA SCHEMA
This section presents the internal structure and data schema of LogChunks. In principle, LogChunks comprises automatically re-trieved and manually labeled and cross-validated logs.LogChunks comprises information on 797 build logs, which are organized in folders for each language and repository. For each repository, LogChunks has about 10 Examples. Every repository folder contains the full log files for the build status ‘failed’ in plain text.
The folder build-failure-reason contains the manually la-beled data of LogChunks, one XML file for each repository:@.xml. Table 1 gives an overview of the data within these XML files on the example of one build from php@php-src. The remainder of this section de-fines in more detail the data embedded in the XML files, that is, the labeled log chunk, search keywords and structural cate-gories. Data from the developer validation study is in the file
MSR ’20, October 5–6, 2020, Seoul, Republic of Korea========DIFF========
-=-=-=-=-=-
005+ 	Parameter 	#1 	[  $ f l a g s ]
========DONE======== 005−	Parameter 	#1 	[  $ a r _ f l a g s ]
FAIL Bug #71412 	A r r a y I t e r a t o r 	r e f l e c t i o n
-=-=-=-=-=-
TEST 9895/13942 	[ 2 / 2 	t e s t 	workers 	running ]
Figure 2: Log chunk from the same structural category as the log chunk presented in Table 1. We inserted the special marker “-=-=-=-=-=-” to separate the log chunk from its con-text.Figure 3: Log chunk from a different structural category than the log chunk presented in Table 1.
developer-crossvalidation.csv, the build id can be used as a unique identifier to match it with the other data.Chunk That Describes Why The Build Failed. The Chunk is the substring of the build log that describes why the build failed. This can, for example, be the failing test case or a compiler error. In cases where the reason why the build failed is contained in a log file external to the main build log, the Chunk includes only the fact that the build failed, for example “The command "test/run !" exited with 1.” In Table 1, the Chunk describes a failing test in which the tested process timed out.Search Keywords. The Keywords contain a list of one to three freely chosen search strings appearing within the Chunk or in the area around it in the build log. We selected keywords the authors would use to search for the log Chunk, as we found them repeat-edly next to failure describing chunks while analyzing about 800 build logs manually. Some keywords from LogChunks are “Error”,“=DIFF=”, “ERR!”, or the keywords shown in Table 1.Structural Category. For each repository, we assign structural categories to the Chunks. The structural category compares how Chunks are represented within the build log. Build tools highlight their error messages with markings, e.g. starting each line with“ERROR” or surrounding special characters. Two chunks fall into the same structural categories if they are surrounded by the same markings. Listing 2 presents a log chunk from the same category as the log chunk from Table 1. In comparison to that, Listing 3 presents a log chunk which is formatted differently within the log file. For each repository, the structural categories are represented as integers, starting at 0 and increased with the next appearing category in chronological build order.4 	POTENTIAL USE CASES
LogChunks can be the basis for a range of further studies:
Benchmarking Build Log Analysis Techniques. LogChunks origi-nated from the first author’s Master’s Thesis in which she compared three different log chunk retrieval techniques. LogChunks can be
| MSR ’20, October 5–6, 2020, Seoul, Republic of Korea | MSR ’20, October 5–6, 2020, Seoul, Republic of Korea |  | Brandt, et al. ||---|---|---|---|
| Data |Description |Unit |Example |
| Log  Chunk |Relative path to the input build log  Log chunk that describes why the build failed |Unique Path String |C/php@php-src/failed/529279089.log |
| Log  Chunk |Relative path to the input build log  Log chunk that describes why the build failed |Unique Path String |001+ ** ERROR: process timed out ** |
001- OK.
========DONE========001- OK.
========DONE========
| Keywords | Keywords the authors would use to search for and find the log chunk 
Categorization of the structural representation of the log chunk within the build log | List of Strings | FAIL Bug #60120 (proc_open hangs) |
|---|---|---|---|
| Keywords |Keywords the authors would use to search for and find the log chunk  Categorization of the structural representation of the log chunk within the build log |List of Strings |ERROR, FAIL, DIFF || Keywords |Keywords the authors would use to search for and find the log chunk  Categorization of the structural representation of the log chunk within the build log |List of Strings |0 |
| Category |Keywords the authors would use to search for and find the log chunk  Categorization of the structural representation of the log chunk within the build log |Integer |0 |Table 1: Exemplary, complete data excerpt from LogChunks for a failed build in php@php-src.
a benchmark to evaluate other build log analysis techniques. For example, one can use the data set to investigate how reliably the diff-based approach of Amar et al. [1] retrieves build failure reasons.Support Build Log Classification Algorithms. Various researchers examine why CI builds fail and use build logs as a data source [14, 20]. They typically write custom parsers and classifiers to catego-rize builds according to why a build failed. The manually labeled chunk can help researchers locate the source for their classification algorithms and cross-validate their data.Research on Build Logs. The data from LogChunks can support research around the topic of build logs such as how developers use keywords to retrieve information about build failures from logs or how they discuss failures of CI builds within pull requests [5].
Automatic Processing of Build Results. LogChunks enables re-searchers to train algorithms that retrieve build failure descrip-tions from build logs. It can provide the basis for further automatic on-ward processing of the retrieved log chunks.5 	RELATED DATA SETS
This section presents existing data sets of CI build logs and how LogChunks differs from them.
5.1 	TravisTorrentTravisTorrent [3] collects a broad range of metadata about Travis CI builds. It combines data accessible through the public Travis CI and GitHub APIs and through GHTorrent [9]. Similar to LogChunks, among the metadata are the failing test cases. However, TravisTor-rent obtained these through a manually developed parser, which only supports specific Ruby test runners and Java Maven or JUnit logs. Anecdotally, many of the failing tests are at least incomplete and lack validation. By contrast, LogChunks provides manually la-beled and two-fold cross-validated data of why builds failed, not only for failing tests like TravisTorrent, but for all possible build-failing errors.5.2 	LogHub
LogHub [21] is a collection of a wide range of system log data sets. It is the basis for various studies that compare different approaches to parse unstructured system log messages into structured data for further analysis. LogChunks is situated in a different area, build log analysis, which tend to be semi-structured, and could play a similarrole to LogHub in its area: empower research with a benchmark to compare different build log analysis techniques.
6 	FUTURE EXTENSIONS TO LOGCHUNKS
In this section, we describe current limitations and future improve-ments of LogChunks and extensions we are planning.
Chunk as One Consecutive Substring. The Chunk contains only one continuous substring of the log text. The reason a build failed could be described at multiple locations within the log. We propose to extend LogChunks to contain multiple substrings of the log text.Include More Repositories and Logs. LogChunks encompasses a range of repositories from various main development languages, though only 10 logs from each repository. Including more logs and repositories will strengthen LogChunks as the go-to benchmark.Classification of the Build Failure Cause. Our data set contains no further classification according to the cause of the failure, such as due to tests, compilation or linter errors. As researchers are investigating why CI builds fail, a useful extension is to annotate cause of the build failure for each log.Other Information Chunks. Build log analysis is not limited to the chunk that describes why a build failed. LogChunks can be extended with labels for all information that is contained in the build log, such as descriptions of warnings, build infrastructure and more.Validation of Search Keywords. The keywords LogChunks pro-vides are based on the experience of the authors gained from ana-lyzing around 800 build logs. Next, we propose to evaluate whether these keywords would also be used by developers to find the log chunk describing why a build failed.
7 	SUMMARYIn this paper, we introduce LogChunks, a cross-validated data set encompassing 797 build logs from 80 projects using Travis CI. For each log, we annotated the log chunk describing why the build failed and provided keywords a developer would use to search for the log chunk as well as a categorization of the log chunks according to their format within the log. LogChunks advances the research field of build log analysis by introducing a benchmark to rigorously examine research contributions [15] and opening various research possibilities that previously required tedious manual classification.LogChunks: A Data Set for Build Log Analysis
REFERENCES 
[1] Anunay Amar and Peter C Rigby. 2019. Mining Historical Test Logs to Predict 	Bugs and Localize Faults in the Test Logs. In Proceedings of the 41st International 	Conference on Software Engineering (ICSE). IEEE, 140–151.[2] Moritz Beller, Georgios Gousios, and Andy Zaidman. 2017. Oops, My Tests Broke the Build: An Explorative Analysis of Travis CI with GitHub. In Proceedings of the 14th IEEE/ACM International Conference on Mining Software Repositories (MSR). IEEE, 356–367.
[3] Moritz Beller, Georgios Gousios, and Andy Zaidman. 2017. TravisTorrent: Synthe-sizing Travis CI and GitHub for Full-Stack Research on Continuous Integration.In Proceedings of the 14th IEEE/ACM International Conference on Mining Software Repositories (MSR). IEEE, 447–450.
[4] Carolin Brandt, Annibale Panichella, and Moritz Beller. 2020. LogChunks: A Data 	Set for Build Log Analysis. 
[5] Nathan Cassee, Bogdan Vas The Silent 	Helper: The Impact of Continuous Integration on Code Reviews. In Proceedingsof the 27th IEEE International Conference on Software Analysis, Evolution and Reengineering (SANER). IEEE Computer Society.
[6] Travis CI. 2019. 	Travis Build Status. 	https://docs.travis-ci.com/user/job-	lifecycle/#breaking-the-build. Accessed: 2019-11-18.
[7] John Downs, Beryl Plimmer, and John G Hosking. 2012. Ambient Awareness of Build Status in Collocated Software Teams. In Proceedings of the 34th International Conference on Software Engineering (ICSE). IEEE, 507–517.[8] Paul M Duvall, Steve Matyas, and Andrew Glover. 2007. Continuous Integration: 	Improving Software Quality and Reducing Risk. Pearson Education.
[9] Georgios Gousios. 2013. The Ghtorrent Dataset and Tool Suite. In Proceedings of the 10th IEEE Working Conference on Mining Software Repositories (MSR). IEEE, 233–236.
[10] Michael Hilton, Timothy Tunnell, Kai Huang, Darko Marinov, and Danny Dig.2016. Usage, Costs, and Benefits of Continuous Integration in Open-Source Projects. In Proceedings of the 31st International Conference on Automated Software Engineering (ASE). ACM, 426–437.
[11] Eirini Kalliamvakou, Georgios Gousios, Kelly Blincoe, Leif Singer, Daniel M German, and Daniela Damian. 2016. An In-Depth Study of the Promises and Perils of Mining GitHub. Empirical Software Engineering 21, 5 (2016), 2035–2071.MSR ’20, October 5–6, 2020, Seoul, Republic of Korea
[12] Louis G Michael IV, James Donohue, James C Davis, Dongyoon Lee, and Francisco Servant. 2019. Regexes Are Hard: Decision-Making, Difficulties, and Risks in Pro-gramming Regular Expressions. In Proceedings of the 34th IEEE/ACM International Conference on Automated Software Engineering (ASE).[13] Ade Miller. 2008. A Hundred Days of Continuous Integration. In Agile 2008 	Conference. IEEE, 289–293.
[14] Hyunmin Seo, Caitlin Sadowski, Sebastian Elbaum, Edward Aftandilian, and 	Robert Bowdidge. 2014. Programmers’ Build Errors: A Case Study (At Google).
In Proceedings of the 36th International Conference on Software Engineering (ICSE). ACM, 724–734.[15] Susan Elliott Sim, Steve Easterbrook, and Richard C Holt. 2003. Using Benchmark-	ing to Advance Research: A Challenge to Software Engineering. In Proceedings 	of the 25th International Conference on Software Engineering (ICSE). IEEE, 74–83. [16] Edward Smith, Robert Loftin, Emerson Murphy-Hill, Christian Bird, and Thomas 	Zimmermann. 2013. Improving developer participation rates in surveys. In2013 6th International Workshop on Cooperative and Human Aspects of Software Engineering (CHASE). IEEE, 89–92.
[17] Daniel Ståhl and Jan Bosch. 2014. Modeling Continuous Integration Practice Differences in Industry Software Development. Journal of Systems and Software 87 (2014), 48–59.[18] Bogdan Vasilescu, Yue Yu, Huaimin Wang, Premkumar Devanbu, and Vladimir Filkov. 2015. Quality and Productivity Outcomes Relating to Continuous Integra-tion in GitHub. In Proceedings of the 10th Joint Meeting on Foundations of Software Engineering. ACM, 805–816.
[19] Carmine Vassallo, Sebastian Proksch, Timothy Zemp, and Harald C Gall. 2018.Un-Break My Build: Assisting Developers with Build Repair Hints. In Proceedings of the 26th IEEE International Conference on Program Comprehension (ICPC). ACM, 41–51.
[20] Carmine Vassallo, Gerald Schermann, Fiorella Zampetti, Daniele Romano, Philipp 	Leitner, Andy Zaidman, Massimiliano Di Penta, and Sebastiano Panichella. 2017.A Tale of CI Build Failures: An Open Source and a Financial Organization Per-spective. In Proceedings of the 33th IEEE International Conference on Software Maintenance and Evolution (ICSME). IEEE, 183–193.
[21] Jieming Zhu, Shilin He, Jinyang Liu, Pinjia He, Qi Xie, Zibin Zheng, and Michael R Lyu. 2019. Tools and Benchmarks for Automated Log Parsing. In Proceedingsof the 41st IEEE/ACM International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP). IEEE, 121–130.