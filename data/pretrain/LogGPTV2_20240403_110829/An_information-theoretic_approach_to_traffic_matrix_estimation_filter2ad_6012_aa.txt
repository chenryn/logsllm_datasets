# An Information-Theoretic Approach to Traffic Matrix Estimation

## Authors
- Yin Zhang
- Matthew Roughan
- Carsten Lund
- David L. Donoho

## Abstract
Traffic matrices are essential for various IP network management tasks, such as capacity planning, traffic engineering, and network reliability analysis. However, directly measuring these matrices is challenging, leading to recent interest in inferring them from more easily measured data, like link loads. This inference problem is typically ill-posed due to the large number of unknowns compared to available data. Regularization techniques are crucial for addressing such problems. This paper introduces a new approach to traffic matrix estimation using entropy penalization. Our method selects the traffic matrix that is information-theoretically closest to a model where source/destination pairs are stochastically independent, consistent with the measured data. We employ fast algorithms based on modern convex optimization theory to solve for the traffic matrices. The algorithm's performance is evaluated using real backbone traffic and routing data, demonstrating its speed, accuracy, robustness, and flexibility.

## Categories and Subject Descriptors
- C.2.3 [Computer-Communications Networks]: Network Operations—network monitoring
- C.2.5 [Computer-Communications Networks]: Local and Wide-Area Networks—Internet

## General Terms
- Measurement, Performance

## Keywords
- Traffic Matrix Estimation, Information Theory, Minimum Mutual Information, Regularization, Traffic Engineering, SNMP

## 1. Introduction
A point-to-point traffic matrix provides the volume of traffic between origin/destination (O/D) pairs in a network. These matrices are essential for various IP network management tasks, including capacity planning, traffic engineering, and network reliability analysis. Direct measurement of traffic matrices is difficult, leading to interest in inferring them from link load statistics and other easily measured data [24, 23, 3, 16, 28].

Traffic matrices can be estimated at different levels of detail, such as Points-of-Presence (PoPs), routers, links, or even IP prefixes [8]. Finer-grained matrices are generally more useful, especially for analyzing network reliability under component failures. For example, during a failure, IP traffic is rerouted, and it is important to test if this rerouting causes link overloads. Fine-grained traffic matrices are necessary to understand these rerouting effects.

The challenge lies in the ill-posed nature of the problem: for a network with \( N \) ingress/egress points, we need to estimate \( N^2 \) O/D demands. The number of available measurements (link loads) remains approximately constant, making the problem massively underconstrained for large \( N \).

Experience in fields like seismology, astronomy, and medical imaging [1, 2, 17, 18, 26] suggests that side information must be incorporated to address such ill-posed problems. Previous work on IP traffic matrix estimation has used various priors: Vardi [24] and Tebaldi and West [23] assume a Poisson traffic model, Cao et al. [3] assume a Gaussian model, Zhang et al. [28] use a gravity model, and Medina et al. [16] use a logit-choice model. Each method's performance depends on the quality of the prior assumptions.

This paper presents a regularization-based approach, assuming source/destination independence until proven otherwise by measurements. The method blends measurements with prior information, producing the reconstruction closest to independence but consistent with the measured data. The resulting optimization problem is intuitive and efficiently solvable using modern optimization software.

We extensively test the algorithm on real backbone ISP traffic and topology data, showing that it is fast and accurate for point-to-point traffic matrix estimation. We also test the algorithm on topologies generated through the Rocketfuel project [21, 14, 22], providing insights into its performance across different network structures. The algorithm is robust to measurement errors and missing data, and it can be extended to estimate point-to-multipoint demand matrices.

## 2. Background

### 2.1 Network
An IP network consists of routers and adjacencies within a single Autonomous System (AS). It is natural to represent the network as a set of nodes and links, with internal routers and links referred to as Backbone Routers (BRs) and external ones as Edge Routers (ERs). Traffic matrices can be computed at different levels of aggregation, such as PoP-to-PoP, router-to-router, or link-to-link. This paper focuses on router-to-router traffic matrices, which are suitable for various network and traffic engineering applications.

Edge links can be classified into access links (connecting customers) and peering links (connecting other ASes). Most inter-domain traffic is exchanged via dedicated peering links, with little transit traffic between peers under normal operations. Routing protocols build forwarding tables, and we use a routing simulator to compute a routing matrix and simulate load balancing.

### 2.2 Traffic Data
Link load measurements are readily available via the Simple Network Management Protocol (SNMP). SNMP data is collected periodically from routers, providing basic traffic statistics. However, SNMP data has limitations, such as potential data loss, incorrect data, and coarse sampling intervals. To mitigate these issues, we use hourly traffic averages. Flow-level data, collected at routers, is used for validation, though current vendor implementations limit its collection from the entire network.

### 2.3 Information Theory
Information theory is a standard tool in communications systems. We define \( p_X(x) \) as the probability that a random variable \( X \) is equal to \( x \). For independent random variables \( X \) and \( Y \):
\[ p(x, y) = p(x)p(y) \]
or equivalently:
\[ p(x|y) = p(x) \]

In this context, \( S \) and \( D \) represent the source and destination of a packet. The discrete Shannon entropy of a random variable \( X \) taking values \( x_i \) is:
\[ H(X) = -\sum_i p(x_i) \log_2 p(x_i) \]

Entropy measures the uncertainty about the outcome of \( X \). The conditional entropy of \( Y \) given \( X \) is:
\[ H(Y|X) = -\sum_j p(x_j) \sum_i p(y_i|x_j) \log_2 p(y_i|x_j) \]

The joint entropy of \( X \) and \( Y \) is:
\[ H(X, Y) = H(X) + H(Y|X) \]

The Shannon information (mutual information) is:
\[ I(Y|X) = H(Y) - H(Y|X) \]

Mutual information represents the decrease in uncertainty about \( Y \) from measuring \( X \). It is symmetric, \( I(X|Y) = I(Y|X) \), and can be written as:
\[ I(X, Y) = \sum_{x, y} p(x, y) \log_2 \left( \frac{p(x, y)}{p(x)p(y)} \right) \]

Mutual information is non-negative, with equality if and only if \( X \) and \( Y \) are independent.