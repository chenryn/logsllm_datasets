### Optimized Text

#### Introduction
The results of this study shed light on why successful remote-to-local attacks are rarely detected at security-conscious sites. At such sites, software patches or other fixes are promptly applied to critical hosts as soon as they become available. Since patches are typically released before or simultaneously with intrusion detection signatures, this implies that there is no window of visibility for detecting successful attacks. Only unsuccessful attacks or probes will be detected, as illustrated in Figure 1B. Signature-based network intrusion detection systems (NIDS) will not detect successful remote-to-local attacks at these sites; they will only confirm the occurrence of failed attempts.

#### Rapid Patching Strategy
Rapidly installing software updates on critical servers would be prohibitively expensive if new vulnerabilities were discovered too frequently. However, Figure 3 shows that this is not the case. It presents the dates of high-severity remote-to-local vulnerabilities over six years, as recorded in the NIST ICAT meta-database [14]. These vulnerabilities can be exploited remotely and have been rated "severe" by NIST. They include remote-to-local attacks that gain root or administrator privileges, episodic DoS attacks, and other threats that a security-aware system administrator would want to prevent. The dates in Figure 3 are taken from the ICAT database and may differ slightly from the original vulnerability announcements.

Figure 3 illustrates the number of severe vulnerabilities for the two most popular web servers, Apache and IIS, which together account for approximately 85% of all web servers [16], and for the BIND software package, widely used as a domain name server [11]. These three software packages account for half of the vulnerabilities shown in Figure 2. Each package has had 5 or more serious vulnerabilities over six years, with at most 15 serious vulnerabilities for any one package, and at most 6 in any one year. The rates of high-severity remote vulnerabilities for other servers (e.g., WU-FTPD and Sendmail) are similar.

Figure 3 demonstrates that installing software patches on a small number of machines in a DMZ with limited services (e.g., web, mail, FTP, and DNS servers) requires daily monitoring of security alerts and substantial effort every few weeks to months. This makes it a practical strategy for securing a small number of machines. A signature-based NIDS on a well-maintained DMZ provides backup protection but should never detect successful remote-to-local attacks.

#### Implications for Small, Poorly Protected Sites
At poorly protected sites with few hosts where patches are not installed rapidly, Figure 2 suggests that signatures would be available to detect attacks that were part of major Internet worm incidents. There would be a window of visibility where successful attacks could be detected, as shown in Figure 1A. Snort signatures or Nessus security tests were always available before vulnerabilities were exploited by worms and sometimes even before individual attackers exploited them. The widespread nature of many of the worms in Figure 2 indicates that there are many vulnerable sites. For example, at its peak, the Code Red worm infected more than 359,000 sites [2].

A recent survey of vulnerabilities on web servers [16] found that many servers remained vulnerable even after being patched for the Code Red IIS ISAPI vulnerability. In October 2001, roughly 10% of servers tested still had back doors left behind by the Code Red II worm. These observations suggest that many small sites are poorly maintained and have little security. Such sites could use NIDS to identify successfully exploited vulnerabilities and then patch them. However, this strategy would require continuous monitoring and analysis. A simpler approach would be to perform frequent automated scans of hosts using vulnerability scanners, followed by software/security upgrades to prevent exploitation of found vulnerabilities. For the Nessus scanner, automated scanning and patching every two weeks would have prevented hosts from being compromised by the Internet worms shown.

#### Implications for Normal Large Sites
Most sites are neither as completely protected as a secure DMZ nor as poorly protected as sites exploited by worms targeting year-old vulnerabilities. Many of us likely work at sites with known, but recent, vulnerabilities. This is supported by our own vulnerability testing and that of others (e.g., [16]), which find substantial percentages of vulnerable hosts. A recent survey [19] reported that 40% of sites responding had one or more systems compromised by a remote attacker in the preceding year.

Vulnerabilities persist because it is difficult to eliminate all known vulnerabilities. At sites with hundreds to thousands of hosts, servers, routers, switches, and other equipment protected by firewalls, it is practically impossible to maintain software patches on all systems and enforce firewall filtering and proxy rules required to prevent new vulnerabilities from being exploited. Although software management tools are being developed to simplify this task, they are not yet in widespread use, and patches are often not installed until fully tested, as they sometimes do not work or disable important capabilities. Coordinating security concerns across many system administrators, tracking existing hardware, and ensuring that new equipment does not create new vulnerabilities is also challenging. Software upgrades or fixes for known vulnerabilities can sometimes not be installed because they disable essential network capabilities or make software incompatible.

Thus, there will almost always be machines with known vulnerabilities at any large site, and NIDS will issue alerts when these machines are successfully compromised. However, the problem is that there are often so many alerts due to failed remote-to-local attacks and normal background traffic that alerts for successful attacks are missed.

#### Prioritizing Alerts Using Vulnerability Information
Our experience is that NIDS often produce hundreds to thousands of remote-to-local alerts per day on a class B network. Many of these correspond to failed attacks, some are false alarms caused by normal network traffic, and a very small number are caused by successful attacks. It is essential to find the few successful attacks.

Prioritizing attacks would be straightforward if NIDS reliably indicated whether attacks succeeded or failed. Unfortunately, most do not. It is possible to determine the success of well-known scripted attacks, such as the Code Red worm, by detecting actions performed after a successful compromise. It is also sometimes possible to monitor the response from web and other servers for evidence of a compromise. In general, however, it is difficult for a network monitor to determine when attacks succeed. Several issues make this difficult:
1. Some NIDS analyze only single packets or the packet stream from outside to inside addresses and do not analyze the response from the attacked machine or correlate the response with the incoming request.
2. Even if the response is analyzed, sometimes incoming and outgoing packets travel over separate paths, and either only incoming or only outgoing packets are visible to the NIDS.
3. Many attacks produce no visible immediate response (e.g., episodic DoS attacks or attacks that install backdoors) or communicate back to the attacker using a different transport mechanism than was used to launch the attack. Communications back to the attacker can be sent hours or days after the attack, encrypted, or tunneled through a common TCP service, and sent back to a different address than was used to launch the attack (e.g., see the HTTP tunnel attack in [13]).

One approach to determine which NIDS alerts correspond to successful remote-to-local attacks is to use vulnerability and host information to filter alerts into high and low priority bins, as shown in Figure 4. This requires site-specific data that can be obtained from vulnerability scanners and by recording operating system and server software versions to associate known vulnerabilities with as many hosts, routers, and other equipment as possible. Additionally, NIDS must be positioned both on the interface to the Internet and internally behind any firewalls to monitor traffic to and from monitored machines.

Figure 4 illustrates how alerts for remote-to-local attacks can be filtered into high and low priority bins. The high-priority bin contains alerts for vulnerable hosts, including those with known vulnerabilities and recently installed hosts. The low-priority bin contains presumably failed remote-to-local attacks against hosts not vulnerable to detected exploits. To find successful attacks, an analyst would first examine alerts in the high-priority bin and then the low-priority bin. When time is limited, this approach focuses on more important alerts, resulting in more detections of successful attacks.

An analysis of remote-to-local alerts from the Snort NIDS at one class B site containing roughly 10 Microsoft IIS web servers was performed to determine the percentage of alerts left in the high-priority bin. Over two weeks, there were roughly 845 alerts per day that could have indicated successful remote-to-local attacks. The top 27 types of alerts generated more than 5 alerts per day each and were responsible for generating roughly 830 remote-to-local alerts a day. Of these, roughly 95% could be placed in the lower-priority bin based on knowledge of the software patches, operating systems, services, and software versions running on the web servers. This left only 5% or 42 alerts per day from one alert type in the high-priority bin. This alert was a generic signature used to detect web traversals by scanning for “..\” and “../” in web requests. It primarily detected failed attacks or probes for a variety of web traversal attacks and false alarms for relative path addressing used in web pages. A detailed analysis of these alerts involving full-time intrusion detection analysts and site system administrators indicated that the low-priority alerts were all failed attacks or false alarms due to normal traffic.

At this site, this approach successfully reduced the number of high-priority remote-to-local alerts by roughly a factor of twenty (from roughly 800 to 40 per day). It did not require extensive information concerning software on protected hosts. Table 4 shows the information required to assign a low priority to the most common alerts. Information was required concerning the host operating system, the existence of web server extensions, and the installation of specific IIS patches.

It was not simple to determine specific questions that could be used to prioritize alerts. First, alert types corresponding to potential remote-to-local attacks had to be identified. Then, a detailed analysis of alerts and associated vulnerabilities was required to identify software components necessary for the success of attacks. In many cases, this was made overly complex by poor descriptions of alerts, requiring a detailed analysis of alert signatures themselves, poor cross-referencing from alerts to information describing vulnerabilities, and non-standardized documentation of vulnerabilities. This analysis was somewhat easier using the CVE vulnerability numbering system [4] to associate alerts to vulnerabilities.

Some alerts that could not be assigned to a low priority were false alarms caused by normal background traffic. Categorizing these alerts required a detailed analysis of signatures and background traffic. Ideally, an analysis to identify background traffic that can cause false alarms and specify conditions for an alert to stay at a high priority could be performed by NIDS developers and users. The result would be a shared list of conditions that must be met for an exploit that triggers an alert to succeed and a list of normal traffic that might cause false alarms.

#### Summary and Discussion
It has recently become easier to scan for known vulnerabilities in hosts and obtain software patches designed to eliminate these vulnerabilities. This capability has had two seemingly contradictory effects on the usefulness of NIDS. On small sites, such as small DMZ networks, NIDS might never detect successful system compromises because vulnerabilities can be patched before NIDS have been updated to detect associated attacks. On small sites, vulnerability scanning and patch installation delegates NIDS into a backup role. On large sites, it is too expensive to install patches, and NIDS may detect system compromises. However, these important detections are often hidden among thousands of unimportant alerts caused by failed attacks and normal background traffic. On such sites, information on vulnerabilities and protected hosts can be used to prioritize alerts and focus on those that might represent successful exploitation of known vulnerabilities. On large sites, vulnerability scanning and information on protected hosts can thus make NIDS useful and practical.

The analyses that led to these conclusions focused on dangerous remote-to-local attacks where a remote attacker achieves restricted privileges on protected hosts and compromises those hosts. It also focused on the common approach to NIDS where signatures are used to detect known attacks. The first part of this paper explored the time sequence for the availability of software patches, intrusion detection signatures, and vulnerability scanner rules following announcements of new vulnerabilities. It was found that software patches are almost always available before new intrusion detection signatures. "Windows of visibility" where NIDS detect successful system compromises will never occur if software patches are installed as soon as they are available. Additionally, it was found that vulnerability-scanning rules are available soon after new vulnerabilities are announced and, so far, they have been available well before a vulnerability was used as part of a widespread worm attack. Timelines from eight recent important vulnerabilities support these findings.