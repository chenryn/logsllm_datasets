results suggest why successful remote-to-local attacks are rarely detected at security
conscious  sites.  At  such  sites,  software  patches  or  other  fixes  are  applied  to  a  few
critical hosts as soon as they are available. Since patches are always available before
or simultaneous with signatures, this implies that there is no window of visibility and
only  unsuccessful  remote-to-local  attacks  or  probes  will  be  detected  as  shown  in
Figure  1B.  Signature-based  network  intrusion  detection  system  will  never  detect
successful remote-to-local attacks  at  such  sites.    They  will  simply  verify  that  failed
attempts occur. 
The strategy of rapidly installing software updates on critical servers would be too
costly  for  widespread  adaptation  if  new  vulnerabilities  were  discovered  too
frequently. Figure 3 shows that this is not the case. It shows the dates of high severity
remote-to-local  vulnerabilities  over  six  years  as  recorded  in  the  NIST  ICAT  meta-
database [14]. These are vulnerabilities that can be exploited remotely and have been
rated  “severe”  by  NIST.  They  include  remote-to-local  attacks  that  gain  root  or
administrator privileges, episodic DoS attacks, and other attacks that a security-aware
system  administrator  would  want  to  prevent.  These  dates  are  taken  from  the  ICAT
database and not from the original vulnerability announcements so they differ slightly
The Effect of Identifying Vulnerabilities and Patching Software         317
BIND DOMAIN NAME SERVER
APACHE WEB SERVER
INTERNET INFORMATION SERVER (IIS)
Jan-96
Jan-97
Jan-98
Jan-99
Jan-00
Jan-01
Jan-02
Fig. 3.  Dates for high severity remote-to-local attacks recorded in the ICAT database for bind
DNS server, Apache web server, and Internet Information Server (IIS) Software.
from the dates in Figure  2.  Vulnerabilities  that  were  discovered  within  one  day  are
counted as single vulnerabilities because they are typically patched simultaneously. 
This figure shows  the  number  of  severe  vulnerabilities  for  the  two  most  popular
web  servers  (Apache  and  IIS)  that  together  account  for  roughly  85%  of  all  web
servers [16] and for the BIND software package that is widely used as a domain name
server  [11].  These  three  software  packages  account  for  half  of  the  vulnerabilities
shown in Figure 2. All packages have had 5 or more serious vulnerabilities over six
years with at most 15 serious vulnerabilities over six years for any one package, and
at  most  6  in  any  one  year.  Rates  of  high-severity  remote  vulnerabilities  for  other
servers  (e.g.  WU-FTPD  and  Sendmail)  are  similar.  Figure  3  shows  that  installing
software patches on a small number of machines in a DMZ with limited services (e.g.
web,  mail,  FTP,  and  DNS  servers)  requires  daily  monitoring  of  security  alerts  and
substantial effort only every few weeks to months. It is thus a practical strategy for
securing a small number of machines. A signature-based network intrusion detection
system  on  a  well-maintained  DMZ  provides  backup  protection,  but  should  never
detect successful remote-to-local attacks.
7   Implications for Small Poorly Protected Sites 
At  poorly  protected  sites  with  few  hosts,  where  patches  are  not  installed  rapidly,
Figure  2  suggests  that  signatures  would  have  been  available  to  detect  attacks  that
were part of major Internet worm incidents and there would have been a window of
visibility  where  successful  attacks  were  detected,  as  shown  in  Figure  1A.  Snort
318         R. Lippmann, S. Webster, and D. Stetson
signatures or Nessus security tests were always available before vulnerabilities were
exploited by worms. They also may have been available before individual attackers
exploited  vulnerabilities.  The  widespread  nature  of  many  of  the  worms  in  Figure  2
suggests that there are many vulnerable sites. For example, at its peak, the Code Red
worm infected more than 359,000 sites [2]. A recent survey of vulnerabilities on web
servers  [16]  found  many  vulnerable  servers,  even  after  many  were  patched  for  the
Code  Red  IIS  ISAPI  vulnerability.  This  survey  found  that  roughly  10%  of  servers
tested  in  October  2001  still  had  back  doors  left  behind  by  the  Code  Red  II  worm.
These  observations  suggest  that  many  small  sites  are  poorly  maintained  and  have
little  security.  Such  sites  could  use  network  intrusion  detection  systems  to  identify
vulnerabilities  that  are  successfully  exploited  and  should  be  patched.  This  strategy,
however, would require continuous monitoring and analysis. In most cases it would
be  simpler  to  perform  frequent  automated  scans  of  hosts  using  any  of  the
vulnerability  scanners  discussed  in  [9]  followed  by  software/security  upgrades  to
prevent  found  vulnerabilities  from  being  exploited.  As  discussed  above,  such
upgrades are practical for small sites without too many hosts. For the Nessus scanner,
a  strategy  of  automated  scanning  and  patching  performed  every  two  weeks  would
have prevented hosts from being compromised by the Internet worms shown.
8   Implications for Normal Large Sites 
Most  sites  are  not  as  completely  protected  as  the  above  secure  DMZ  or  as  poorly
protected as sites used by worms that exploited year-old vulnerabilities.  Most  of  us
probably work at sites with known, but recent, vulnerabilities. This is supported  by
our  own  vulnerability  testing  and  that  of  others  (e.g.  [16])  that  finds  substantial
percentages  of  vulnerable  hosts.  It  is  also  supported  by  a  recent  survey  [19]  where
40% of sites responding reported that one or more systems were compromised from a
remote attacker over the preceding year. Vulnerabilities exist because it is difficult to
eliminate  all  known  vulnerabilities.  At  sites  with  100’s  to  1000’s  of  hosts,  servers,
routers, switches, and other equipment that is protected by firewalls, it is practically
impossible  to  maintain  software  patches  on  all  systems  and  also  enforce  firewall
filtering and proxy rules required to prevent new vulnerabilities from being exploited.
Although software management tools are being developed to simplify this task, they
are not in widespread use yet and patches are frequently not installed until they are
fully  tested  because  they  sometimes  do  not  work  or  they  disable  important
capabilities.  It  is  also  difficult  to  coordinate  security  concerns  across  many  system
administrators,  to  keep  track  of  existing  hardware,  and  to  make  sure  that  installing
new  equipment  does  not  create  new  vulnerabilities.  Software  upgrades  or  fixes  for
known  vulnerabilities  also  can  sometimes  not  be  installed  because  they  disable  an
essential  network  capability  or  make  software  incompatible.  There  will  thus  almost
always  be  machines  with  known  vulnerabilities  at  any  large  site  and  intrusion
detection  systems  will 
these  machines  are  successfully
compromised. A major problem, however, is that there are often so many alerts due to
issue  alerts  when 
The Effect of Identifying Vulnerabilities and Patching Software         319
failed remote-to-local attacks and normal background traffic that alerts for successful
attacks are missed.
Fig. 4.  Alert filtering based on vulnerabilities of protected hosts separates remote-to-local
alerts into high and low priority bins
9   Using Vulnerability Information to Prioritize Alerts
Our  experience  is  that  network  intrusion  detection  systems  often  produce  100’s  to
1000’s  of  remote-to-local  alerts  per  day  on  a  class  B  network.  Many  of  these
correspond to failed attacks, some are false alarms caused by normal network traffic,
and a very small number are caused by successful attacks.  It is essential to find the
few successful attacks. 
It  would  be  trivial  to  prioritize  attacks  if  intrusion  detection  systems  reliably
indicated  whether  attacks  succeeded  or  failed.  Unfortunately,  most  don’t.  It  is
possible to determine the success of well-known scripted attacks, such as the code-red
worm,  by  detecting  actions  performed  after  a  successful  compromise.  It  is  also
sometimes possible to monitor the response from web and other servers for evidence
of  a  compromise.  In  general,  however,  it  is  difficult  for  a  network  monitor  to
determine  when  attacks  succeed.  In  addition  to  the  issues  described  in  [20],  many
other characteristics of network monitoring make this difficult. First, some intrusion
detection  systems  analyze  only  single  packets  or  the  packet  stream  from  outside  to
inside addresses and either do not analyze the response from the attacked machine or
do not correlate the response with the incoming request. Second, even if the response
is analyzed, sometimes incoming and outgoing packets travel over separate paths and
either only incoming or only outgoing packets  are  visible  to  the  intrusion  detection
system.  Third,  many  attacks  produce  no  visible  immediate  response  (e.g.  episodic
DoS  attacks  or  attacks  that  install  backdoors)  or  communicate  back  to  the  attacker
using  a  different  transport  mechanism  than  was  used  to  launch  the  attack.
Communications  back  to  the  attacker  can  be  sent  hours  or  days  after  the  attack,
encrypted or “tunneled” through a common TCP service, and sent back to a different
address than was used to launch the attack (e.g. see the HTTP tunnel attack in [13]). 
One  approach  to  determine  which  intrusion  detection  alerts  correspond  to
information  concerning  known
successful  remote-to-local  attacks 
is 
to  use 
320         R. Lippmann, S. Webster, and D. Stetson
vulnerabilities  and  hosts  to  filter  alerts  into  high  and  low  priority  bins  as  shown  in
Figure  4.  This  requires  site-specific  data  that  can  be  obtained  from  vulnerability
scanners  and  also  by  recording  operating  system  and  server  software  versions  to
associate  known  vulnerabilities  with  as  many  of  the  hosts,  routers,  and  other
equipment  that  can  be  cataloged  and  analyzed.  In  addition,  it  requires  network
intrusion detection systems to be positioned both on the interface to the Internet and
internally  behind  any  firewalls  to  monitor  traffic  to  and  from  monitored  machines.
Monitoring behind firewalls is required because machines behind firewalls are often
not  updated  as  frequently  as  those  exposed  to  network  traffic  and  are  thus  often
vulnerable  to  many  more  attacks,  because  network  address  translation  and  load
balancing  obscure  the  identity  of  internal  machines,  and  because  many  attacks
originate from behind firewalls [19].
Figure 4 illustrates how alerts for remote-to-local attacks can be filtered into high
and  low  priority  bins.  The  high-priority  bin  shown  at  the  top  right  of  this  figure
contains alerts for vulnerable hosts. These are alerts corresponding to vulnerabilities
that  are  known  to  exist  and  alerts  for  hosts  where  no  information  is  available
concerning a particular vulnerability such as alerts from recently installed hosts. The
second low-priority bin shown at the bottom  right  of  Figure  4  contains  presumably
failed  remote-to-local  attacks  against  hosts  not  vulnerable  to  detected  exploits.  To
find successful attacks, an analyst would first examine alerts in the high-priority bin
and then examine alerts in the low-priority bin. When the time available for analyzing
alerts  is  limited,  and  all  alerts  cannot  be  hand  examined,  this  will  result  in  more
detections  of  successful  attacks  by  focusing  on  more  important  alerts.    In  practice,
alerts  in  the  low-priority  bin  should  be  sampled  (especially  previously  unseen  alert
types for existing hosts) because new software releases sometimes  inadvertently  re-
enable old vulnerabilities, software tools that may contain known vulnerabilities are
often installed without notifying system administrators, and vulnerability data may be
incorrectly recorded or transferred. This approach will be successful if few alerts are
left in the high-priority bin and the vulnerability analysis is correct. 
An analysis of remote-to-local alerts from the Snort intrusion detection system at
one class B site containing roughly 10 Microsoft IIS web servers was performed to
determine  the  percentage  of  alerts  that  are  left  in  the  high-priority  bin  with  this
approach.  This  analysis  suggests  that  this  approach  can  be  extremely  effective  in
reducing  the  number  of  alerts  that  require  immediate  attention  by  a  system
administrator or security analyst. There were roughly 845 alerts per day over roughly
two  weeks  that  could  have  indicated  successful  remote-to-local  attacks.  The  top  27
types  of  alerts  generated  more  than  5  alerts  per  day  each  and  together  were
responsible  for  generating  roughly  830  remote-to-local  alerts  a  day.  Of  these  830,
roughly  95%  could  be  placed  in  the  lower-priority  bin  based  on  knowledge  of  the
software patches, operating systems, services, and software versions running  on  the
web servers. This left only 5% or 42 alerts per day from one alert type  in  the  high
priority  bin.  This  alert  was  a  generic  signature  used  to  detect  web  traversals  by
scanning  for  “..\”  and  “../”  in  web  requests.  It  detected  primarily  failed  attacks  or
probes  for  a  variety  of  web  traversal  attacks  and  false  alarms  for  relative  path
The Effect of Identifying Vulnerabilities and Patching Software         321
addressing used in web pages. A detailed analysis of these alerts involving full-time
intrusion  detection  analysts  and  site  system  administrators  indicated  that  the  low-
priority alerts were all failed attacks or false alarms due to normal traffic. 
At this site, this approach successfully reduced the number of high-priority remote-
to-local alerts by roughly a factor of twenty (from roughly 800 to 40 per day). It also
didn’t require extensive information concerning software on protected hosts. Table 4
shows the information required to assign a low priority to the most common alerts.
Information was required concerning the host operating system, the existence of web
server extensions, and the installation of specific IIS patches. 
It was not simple to determine specific questions that could be used to  prioritize
alerts. First, alert types corresponding  to  potential  remote-to-local  attacks  had  to  be
identified.  Then,  a  detailed  analysis  of  alerts  and  associated  vulnerabilities  was
required  to  identify  software  components  necessary  for  the  success  of  attacks.  In
many cases, this was made overly complex by poor descriptions of alerts that required
a detailed analysis of alert signatures themselves, poor cross referencing from alerts to
information  describing  vulnerabilities,  and  non-standardized  and  distributing
documentation of vulnerabilities. This analysis was made somewhat easier by the use
of the CVE vulnerability numbering system [4] to associate alerts to vulnerabilities.
Some alerts that could not be assigned to a low priority were false alarms caused by
normal  background  traffic.  Categorizing  these  alerts  required  a  detailed  analysis  of
signatures and background traffic. Ideally, an analysis to identify background traffic
that can cause false alarms and also to specify conditions that must be met for an alert
to stay  at  a  high  priority  could  be  performed  by  intrusion  detection  developers  and
users. The result would be a shared list of conditions that must be met for an exploit
that triggers an alert to succeed and also a list of normal traffic that might cause false
alarms.
Table  4.  Reasons  for  assigning  a  low  priority  to  the  most  frequent  remote-to-local  alerts.
Generic "http directory traversal" alerts could not be assigned a low priority.
Alert Name
Web cgi redirect
Sun RPC high port access
front page shtml.exe
iis-vti_inf
front page shtml.dll
http directory traversal 
Shaft to client
DDoS Mstream client to handler
ISAPI Overflow ida
Ave Alerts per Day
140
110
85
67
63
54
30
25
23
If Low Priority, Why
Not running cold fusion
No Sun Servers
No FrontPage
IIS Patched
No Frontpage
No UNIX Servers
No UNIX Servers
IIS Patched
The analysis presented in Figure 3 also requires  accurate  and  timely  information
concerning  IP  addresses  for  hosts  exposed  to  the  Internet  and  knowledge  of  their
operating  systems,  software,  and  patches.  Vulnerability  and  network  scanners
simplify the task of gathering this information for large numbers of workstations. As
322         R. Lippmann, S. Webster, and D. Stetson
Figure 2 shows, rules for at least one scanner are now being rapidly updated to detect
new vulnerabilities. Scanners are being used at many sites to gather information on
operating system and software versions as well as information  on  large  numbers  of
known vulnerabilities. One limitation of scanners is that it is difficult to monitor all
hosts and discover new hosts continuously. This limitation can partially be overcome
by continuously performing passive analysis of traffic within network-based intrusion
detection  systems.  The  double  arrow  between 
intrusion  detection  and  host
vulnerability components in Figure 4 indicates this type of analysis. Passive analysis
can  be  used  to  detect  new  hosts  recently  connected  to  the  Internet  that  should  be
analyzed for vulnerabilities. It also can potentially identify the operating systems of
monitored hosts. For example, it might be possible to determine the operating system
type  of  protected  hosts  using  a  combination  of  “passive  operating  system
fingerprinting”  of  TCP  packet  headers  as  described  in  [24],  analysis  of  banners
produced by server software, and analysis of the services offered by hosts. Although
this  approach  has  not  been  carefully  explored,  a  study  reported  in  [5]  used  packet
header  information  to  identify  operating  systems  and  filter  remote-to-local  alerts.
Experiments on a few networks demonstrated that often roughly 30% of the remote-
to-local  alerts  could  be  sent  to  the  low-priority  bin  by  simple  filtering  based  on
operating  system  type.  This  approach  should  only  be  used  to  detect  recently
connected  hosts  and  perform  a  preliminary  analysis  of  those  hosts.  The  analysis
should  preferably  be  extended  and  confirmed  by  active  scanning  and  consultation
with system administrators.
10   Summary and Discussion
It  is  has  recently  become  easier  to  scan  for  known  vulnerabilities  in  hosts  and  to
obtain  software  patches  designed  to  eliminate  these  vulnerabilities.  This  capability
has had  two  seemingly  contradictory  effects  on  the  usefulness  of  network  intrusion
detection systems.  On small sites, such as small DMZ networks, network intrusion
detection  systems  might  never  detect  successful  systems  compromises  because
vulnerabilities can be patched before intrusion detection systems have been updated
to  detect  associated  attacks.  On  small  sites,  vulnerability  scanning  and  patch
installation delegates network intrusion detection into a backup role. On large sites, it
is too expensive to install patches and network intrusion detection systems may detect
system compromises. These important detections, however, are often hidden among
thousands  of  unimportant  alerts  caused  by  failed  attacks  and  normal  background
traffic. On such sites, information on vulnerabilities and protected hosts can be used
to prioritize alerts and focus on those that might represent successful exploitation of
known  vulnerabilities.  On  large  sites,  vulnerability  scanning  and  information  on
protected hosts can thus make network intrusion detection useful and practical.
The  analyses  that  led  to  these  conclusions  focused  on  dangerous  remote-to-local
attacks where a remote attacker achieves restricted privileges on protected hosts and
compromises  those  hosts.  It  also  focused  on  the  common  approach  to  network
The Effect of Identifying Vulnerabilities and Patching Software         323
intrusion detection where signatures are used to detect known attacks. The first part of
this paper explored the time sequence for availability of software patches, intrusion
detection signatures, and vulnerability scanner rules following announcements of new
vulnerabilities. It was found that software patches are almost always available before
new intrusion detection signatures. “Windows of visibility” where intrusion detection
systems  detect  successful  system  compromises  will  never  occur  if  software  patches
are installed as soon as they are available.  In addition, it was found that vulnerability-
scanning rules are available soon after new vulnerabilities are announced and, so far,
they have been available well before a vulnerability was used as part of a widespread
worm  attack.  Timelines  from  eight  recent  important  vulnerabilities  support  these