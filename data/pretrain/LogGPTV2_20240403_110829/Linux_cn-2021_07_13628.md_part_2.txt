# 读取测试数据
test_data = pd.read_csv('parkinsons/Data_Parkinsons_TEST.csv')
test_x = test_data[x_rows]
test_y = test_data[y_rows]
# 对测试数据进行预测
predict_test = gnb.predict(test_x)
print('Prediction on test data:', predict_test)
# 在测试数据上的准确率
accuracy_test = accuracy_score(test_y, predict_test)
print('Accuray score on test data:', accuracy_train)
```
运行这个 Python 脚本：
```
$ python naive_bayes_parkinsons.py
train_x:
      MDVP:Fo(Hz)  MDVP:Fhi(Hz) ...  MDVP:RAP  MDVP:PPQ  Jitter:DDP
0        152.125       161.469  ...   0.00191   0.00226     0.00574
1        120.080       139.710  ...   0.00180   0.00220     0.00540
2        122.400       148.650  ...   0.00465   0.00696     0.01394
3        237.323       243.709  ...   0.00173   0.00159     0.00519
..           ...           ...           ...  ...       ...       ...        
155      138.190       203.522  ...   0.00406   0.00398     0.01218
[156 rows x 8 columns]
train_y:
      status
0         1
1         1
2         1
3         0
..      ...
155       1
[156 rows x 1 columns]
Prediction on train data: [1 1 1 0 ... 1]
Accuracy score on train data: 0.6666666666666666
Prediction on test data: [1 1 1 1 ... 1
 1 1]
Accuracy score on test data: 0.6666666666666666
```
在训练集和测试集上的准确率都是 67%。它的性能还可以进一步优化。你想尝试一下吗？你可以在下面的评论区给出你的方法。
### 背后原理
朴素贝叶斯分类器从贝叶斯定理发展来的。贝叶斯定理用于计算条件概率，或者说贝叶斯定理用于计算当与一个事件相关联的其他事件发生时，该事件发生的概率。简而言之，它解决了这个问题：*如果我们已经知道事件 x 发生在事件 y 之前的概率，那么当事件 x 再次发生时，事件 y 发生的概率是多少？* 贝叶斯定理用一个先验的预测值来逐渐逼近一个最终的 [后验概率](https://en.wikipedia.org/wiki/Posterior_probability)。贝叶斯定理有一个基本假设，就是所有的参数重要性相同（LCTT 译注：即相互独立）。
贝叶斯计算主要包括以下步骤：
1. 计算总的先验概率：  
P(患病)P(患病) 和 P(不患病)P(不患病)
2. 计算 8 种指标各自是某个值时的后验概率 (value1,...,value8 分别是 MDVP:Fo(Hz)，...，Jitter:DDP 的取值)：  
P(value1,\ldots,value8\ |\ 患病)P(value1,…,value8 ∣ 患病)  
P(value1,\ldots,value8\ |\ 不患病)P(value1,…,value8 ∣ 不患病)
3. 将第 1 步和第 2 步的结果相乘，最终得到患病和不患病的后验概率：  
P(患病\ |\ value1,\ldots,value8) \propto P(患病) \times P(value1,\ldots,value8\ |\ 患病)P(患病 ∣ value1,…,value8)∝P(患病)×P(value1,…,value8 ∣ 患病)  
P(不患病\ |\ value1,\ldots,value8) \propto P(不患病) \times P(value1,\ldots,value8\ |\ 不患病)P(不患病 ∣ value1,…,value8)∝P(不患病)×P(value1,…,value8 ∣ 不患病)
上面第 2 步的计算非常复杂，朴素贝叶斯将它作了简化：
1. 计算总的先验概率：  
P(患病)P(患病) 和 P(不患病)P(不患病)
2. 对 8 种指标里的每个指标，计算其取某个值时的后验概率：  
P(value1\ |\ 患病),\ldots,P(value8\ |\ 患病)P(value1 ∣ 患病),…,P(value8 ∣ 患病)  
P(value1\ |\ 不患病),\ldots,P(value8\ |\ 不患病)P(value1 ∣ 不患病),…,P(value8 ∣ 不患病)
3. 将第 1 步和第 2 步的结果相乘，最终得到患病和不患病的后验概率：  
P(患病\ |\ value1,\ldots,value8) \propto P(患病) \times P(value1\ |\ 患病) \times \ldots \times P(value8\ |\ 患病)P(患病 ∣ value1,…,value8)∝P(患病)×P(value1 ∣ 患病)×…×P(value8 ∣ 患病)  
P(不患病\ |\ value1,\ldots,value8) \propto P(不患病) \times P(value1\ |\ 不患病) \times \ldots \times P(value8\ |\ 不患病)P(不患病 ∣ value1,…,value8)∝P(不患病)×P(value1 ∣ 不患病)×…×P(value8 ∣ 不患病)
这只是一个很初步的解释，还有很多其他因素需要考虑，比如数据类型的差异，稀疏数据，数据可能有缺失值等。
### 超参数
朴素贝叶斯作为一个简单直接的算法，不需要超参数。然而，有的版本的朴素贝叶斯实现可能提供一些高级特性（比如超参数）。比如，[GaussianNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html) 就有 2 个超参数：
* **priors**：先验概率，可以事先指定，这样就不必让算法从数据中计算才能得出。
* **var\_smoothing**：考虑数据的分布情况，当数据不满足标准的高斯分布时，这个超参数会发挥作用。
### 损失函数
为了坚持简单的原则，朴素贝叶斯使用 [0-1 损失函数](https://en.wikipedia.org/wiki/Loss_function#0-1_loss_function)。如果预测结果与期望的输出相匹配，损失值为 0，否则为 1。
### 优缺点
**优点**：朴素贝叶斯是最简单、最快速的算法之一。  
**优点**：在数据量较少时，用朴素贝叶斯仍可作出可靠的预测。  
**缺点**：朴素贝叶斯的预测只是估计值，并不准确。它胜在速度而不是准确度。  
**缺点**：朴素贝叶斯有一个基本假设，就是所有特征相互独立，但现实情况并不总是如此。
从本质上说，朴素贝叶斯是贝叶斯定理的推广。它是最简单最快速的机器学习算法之一，用来进行简单和快速的训练和预测。朴素贝叶斯提供了足够好、比较准确的预测。朴素贝叶斯假设预测特征之间是相互独立的。已经有许多朴素贝叶斯的开源的实现，它们的特性甚至超过了贝叶斯算法的实现。
---
via: 
作者：[Girish Managoli](https://opensource.com/users/gammay) 选题：[lujun9972](https://github.com/lujun9972) 译者：[tanloong](https://github.com/tanloong) 校对：[wxy](https://github.com/wxy)
本文由 [LCTT](https://github.com/LCTT/TranslateProject) 原创编译，[Linux中国](https://linux.cn/) 荣誉推出