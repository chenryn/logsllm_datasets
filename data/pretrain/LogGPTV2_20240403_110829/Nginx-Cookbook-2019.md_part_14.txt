'"$request" $status $body_bytes_sent '
'"$http_referer" "$http_user_agent" '
'"$http_x_forwarded_for" $request_id';
upstream backend {
server 10.0.0.42;
}
server {
listen 80;
add_header X-Request-ID $request_id; # Return to client
location / {
proxy_pass http://backend;
proxy_set_header X-Request-ID $request_id; #Pass to app
access_log /var/log/nginx/access_trace.log trace;
}
}
In this example configuration, a log_format named trace is set up,
and the variable $request_id is used in the log. This $request_id
variable is also passed to the upstream application by use of the
14.4 Request Tracing | 147
proxy_set_header directive to add the request ID to a header when
making the upstream request. The request ID is also passed back to
the client through use of the add_header directive setting the
request ID in a response header.
Discussion
Made available in NGINX Plus R10 and NGINX version 1.11.0, the
$request_id provides a randomly generated string of 32 hexadeci‐
mal characters that can be used to uniquely identify requests. By
passing this identifier to the client as well as to the application, you
can correlate your logs with the requests you make. From the front‐
end client, you will receive this unique string as a response header
and can use it to search your logs for the entries that correspond.
You will need to instruct your application to capture and log this
header in its application logs to create a true end-to-end relationship
between the logs. With this advancement, NGINX makes it possible
to trace requests through your application stack.
148 | Chapter 14: Debugging and Troubleshooting with Access Logs, Error Logs, and Request
Tracing
CHAPTER 15
Performance Tuning
15.0 Introduction
Tuning NGINX will make an artist of you. Performance tuning of
any type of server or application is always dependent on a number
of variable items, such as, but not limited to, the environment, use
case, requirements, and physical components involved. It’s common
to practice bottleneck-driven tuning, meaning to test until you’ve hit
a bottleneck, determine the bottleneck, tune for limitations, and
repeat until you’ve reached your desired performance requirements.
In this chapter, we suggest taking measurements when performance
tuning by testing with automated tools and measuring results. This
chapter also covers connection tuning for keeping connections open
to clients as well as upstream servers, and serving more connections
by tuning the operating system.
15.1 Automating Tests with Load Drivers
Problem
You need to automate your tests with a load driver to gain consis‐
tency and repeatability in your testing.
Solution
Use an HTTP load-testing tool such as Apache JMeter, Locust,
Gatling, or whatever your team has standardized on. Create a con‐
figuration for your load-testing tool that runs a comprehensive test
149
on your web application. Run your test against your service. Review
the metrics collected from the run to establish a baseline. Slowly
ramp up the emulated user concurrency to mimic typical produc‐
tion usage and identify points of improvement. Tune NGINX and
repeat this process until you achieve your desired results.
Discussion
Using an automated testing tool to define your test gives you a con‐
sistent test to build metrics off of when tuning NGINX. You must be
able to repeat your test and measure performance gains or losses to
conduct science. Running a test before making any tweaks to the
NGINX configuration to establish a baseline gives you a basis to
work from so that you can measure if your configuration change has
improved performance or not. Measuring for each change made will
help you identify where your performance enhancements come
from.
15.2 Keeping Connections Open to Clients
Problem
You need to increase the number of requests allowed to be made
over a single connection from clients and the amount of time idle
connections are allowed to persist.
Solution
Use the keepalive_requests and keepalive_timeout directives to
alter the number of requests that can be made over a single connec‐
tion and the time idle connections can stay open:
http {
keepalive_requests 320;
keepalive_timeout 300s;
...
}
The keepalive_requests directive defaults to 100, and the
keepalive_timeout directive defaults to 75 seconds.
150 | Chapter 15: Performance Tuning
Discussion
Typically the default number of requests over a single connection
will fulfill client needs because browsers these days are allowed to
open multiple connections to a single server per fully qualified
domain name. The number of parallel open connections to a
domain is still limited typically to a number less than 10, so in this
regard, many requests over a single connection will happen. A trick
commonly employed by content delivery networks is to create mul‐
tiple domain names pointed to the content server and alternate
which domain name is used within the code to enable the browser
to open more connections. You might find these connection opti‐
mizations helpful if your frontend application continually polls your
backend application for updates, as an open connection that allows a
larger number of requests and stays open longer will limit the num‐
ber of connections that need to be made.
15.3 Keeping Connections Open Upstream
Problem
You need to keep connections open to upstream servers for reuse to
enhance your performance.
Solution
Use the keepalive directive in the upstream context to keep con‐
nections open to upstream servers for reuse:
proxy_http_version 1.1;
proxy_set_header Connection "";
upstream backend {
server 10.0.0.42;
server 10.0.2.56;
keepalive 32;
}
The keepalive directive in the upstream context activates a cache of
connections that stay open for each NGINX worker. The directive
denotes the maximum number of idle connections to keep open per
worker. The proxy modules directives used above the upstream
block are necessary for the keepalive directive to function properly
15.3 Keeping Connections Open Upstream | 151
for upstream server connections. The proxy_http_version direc‐
tive instructs the proxy module to use HTTP version 1.1, which
allows for multiple requests to be made over a single connection
while it’s open. The proxy_set_header directive instructs the proxy
module to strip the default header of close, allowing the connection
to stay open.
Discussion
You want to keep connections open to upstream servers to save the
amount of time it takes to initiate the connection, allowing the
worker process to instead move directly to making a request over an
idle connection. It’s important to note that the number of open con‐
nections can exceed the number of connections specified in the keep
alive directive as open connections and idle connections are not
the same. The number of keepalive connections should be kept
small enough to allow for other incoming connections to your
upstream server. This small NGINX tuning trick can save some
cycles and enhance your performance.
15.4 Buffering Responses
Problem
You need to buffer responses between upstream servers and clients
in memory to avoid writing responses to temporary files.
Solution
Tune proxy buffer settings to allow NGINX the memory to buffer
response bodies:
server {
proxy_buffering on;
proxy_buffer_size 8k;
proxy_buffers 8 32k;
proxy_busy_buffer_size 64k;
...
}
The proxy_buffering directive is either on or off; by default it’s on.
The proxy_buffer_size denotes the size of a buffer used for read‐
ing the first part of the response from the proxied server and
defaults to either 4k or 8k, depending on the platform. The
152 | Chapter 15: Performance Tuning
proxy_buffers directive takes two parameters: the number of buf‐
fers and the size of the buffers. By default, the proxy_buffers direc‐
tive is set to a number of 8 buffers of size either 4k or 8k, depending
on the platform. The proxy_busy_buffer_size directive limits the
size of buffers that can be busy, sending a response to the client
while the response is not fully read. The busy buffer size defaults to
double the size of a proxy buffer or the buffer size.
Discussion
Proxy buffers can greatly enhance your proxy performance, depend‐
ing on the typical size of your response bodies. Tuning these settings
can have adverse effects and should be done by observing the aver‐
age body size returned, and thoroughly and repeatedly testing.
Extremely large buffers set when they’re not necessary can eat up the
memory of your NGINX box. You can set these settings for specific
locations that are known to return large response bodies for optimal
performance.
15.5 Buffering Access Logs
Problem
You need to buffer logs to reduce the opportunity of blocks to the
NGINX worker process when the system is under load.
Solution
Set the buffer size and flush time of your access logs:
http {
access_log /var/log/nginx/access.log main buffer=32k
flush=1m;
}
The buffer parameter of the access_log directive denotes the size
of a memory buffer that can be filled with log data before being
written to disk. The flush parameter of the access_log directive
sets the longest amount of time a log can remain in a buffer before
being written to disk.
15.5 Buffering Access Logs | 153
Discussion
Buffering log data into memory may be a small step toward optimi‐
zation. However, for heavily requested sites and applications, this
can make a meaningful adjustment to the usage of the disk and
CPU. When using the buffer parameter to the access_log direc‐
tive, logs will be written out to disk if the next log entry does not fit
into the buffer. If using the flush parameter in conjunction with the
buffer parameter, logs will be written to disk when the data in the
buffer is older than the time specified. When buffering logs in this
way, when tailing the log, you may see delays up to the amount of
time specified by the flush parameter.
15.6 OS Tuning
Problem
You need to tune your operating system to accept more connections
to handle spike loads or highly trafficked sites.
Solution
Check the kernel setting for net.core.somaxconn, which is the maxi‐
mum number of connections that can be queued by the kernel for
NGINX to process. If you set this number over 512, you’ll need to
set the backlog parameter of the listen directive in your NGINX
configuration to match. A sign that you should look into this kernel
setting is if your kernel log explicitly says to do so. NGINX handles
connections very quickly, and for most use cases, you will not need
to alter this setting.
Raising the number of open file descriptors is a more common
need. In Linux, a file handle is opened for every connection; and
therefore NGINX may open two if you’re using it as a proxy or load
balancer because of the open connection upstream. To serve a large
number of connections, you may need to increase the file descriptor
limit system-wide with the kernel option sys.fs.file_max, or for
the system user NGINX is running as in the /etc/security/limits.conf
file. When doing so you’ll also want to bump the number of
worker_connections and worker_rlimit_nofile. Both of these
configurations are directives in the NGINX configuration.
154 | Chapter 15: Performance Tuning
Enable more ephemeral ports. When NGINX acts as a reverse proxy
or load balancer, every connection upstream opens a temporary
port for return traffic. Depending on your system configuration, the
server may not have the maximum number of ephemeral ports
open. To check, review the setting for the kernel set‐
ting net.ipv4.ip_local_port_range. The setting is a lower- and
upper-bound range of ports. It’s typically OK to set this kernel set‐
ting from 1024 to 65535. 1024 is where the registered TCP ports
stop, and 65535 is where dynamic or ephemeral ports stop. Keep in
mind that your lower bound should be higher than the highest open
listening service port.
Discussion
Tuning the operating system is one of the first places you look when
you start tuning for a high number of connections. There are many
optimizations you can make to your kernel for your particular use
case. However, kernel tuning should not be done on a whim, and
changes should be measured for their performance to ensure the
changes are helping. As stated before, you’ll know when it’s time to
start tuning your kernel from messages logged in the kernel log or
when NGINX explicitly logs a message in its error log.
15.6 OS Tuning | 155
CHAPTER 16
Practical Ops Tips and Conclusion
16.0 Introduction
This last chapter will cover practical operations tips and is the con‐
clusion to this book. Throughout this book, we’ve discussed many
ideas and concepts pertinent to operations engineers. However, I
thought a few more might be helpful to round things out. In this
chapter I’ll cover making sure your configuration files are clean and
concise, as well as debugging configuration files.
16.1 Using Includes for Clean Configs
Problem
You need to clean up bulky configuration files to keep your configu‐
rations logically grouped into modular configuration sets.
Solution
Use the include directive to reference configuration files, directo‐
ries, or masks:
http {
include config.d/compression.conf;
include sites-enabled/*.conf
}
157
The include directive takes a single parameter of either a path to a
file or a mask that matches many files. This directive is valid in any
context.
Discussion
By using include statements you can keep your NGINX configura‐
tion clean and concise. You’ll be able to logically group your config‐
urations to avoid configuration files that go on for hundreds of lines.
You can create modular configuration files that can be included in
multiple places throughout your configuration to avoid duplication
of configurations. Take the example fastcgi_param configuration file
provided in most package management installs of NGINX. If you
manage multiple FastCGI virtual servers on a single NGINX box,
you can include this configuration file for any location or context
where you require these parameters for FastCGI without having to
duplicate this configuration. Another example is SSL configurations.
If you’re running multiple servers that require similar SSL configu‐
rations, you can simply write this configuration once and include it
wherever needed. By logically grouping your configurations
together, you can rest assured that your configurations are neat and
organized. Changing a set of configuration files can be done by edit‐
ing a single file rather than changing multiple sets of configuration
blocks in multiple locations within a massive configuration file.
Grouping your configurations into files and using include state‐
ments is good practice for your sanity and the sanity of your collea‐
gues.
16.2 Debugging Configs
Problem
You’re getting unexpected results from your NGINX server.
Solution
Debug your configuration, and remember these tips:
• NGINX processes requests looking for the most specific
matched rule. This makes stepping through configurations by
hand a bit harder, but it’s the most efficient way for NGINX to
158 | Chapter 16: Practical Ops Tips and Conclusion
work. There’s more about how NGINX processes requests in the
documentation link in the section “Also See” on page 160.
• You can turn on debug logging. For debug logging you’ll need
to ensure that your NGINX package is configured with the --
with-debug flag. Most of the common packages have it; but if
you’ve built your own or are running a minimal package, you
may want to at least double-check. Once you’ve ensured you
have debug, you can set the error_log directive’s log level to
debug: error_log /var/log/nginx/error.log debug.
• You can enable debugging for particular connections.
The debug_connection directive is valid inside the events con‐
text and takes an IP or CIDR range as a parameter. The direc‐
tive can be declared more than once to add multiple IP
addresses or CIDR ranges to be debugged. This may be helpful
to debug an issue in production without degrading performance
by debugging all connections.
• You can debug for only particular virtual servers. Because the
error_log directive is valid in the main, HTTP, mail, stream,
server, and location contexts, you can set the debug log level in
only the contexts you need it.
• You can enable core dumps and obtain backtraces from them.
Core dumps can be enabled through the operating system or
through the NGINX configuration file. You can read more
about this from the admin guide in the section “Also See” on
page 160.
• You’re able to log what’s happening in rewrite statements with
the rewrite_log directive on: rewrite_log on.
Discussion
The NGINX platform is vast, and the configuration enables you to
do many amazing things. However, with the power to do amazing
things, there’s also the power to shoot your own foot. When debug‐
ging, make sure you know how to trace your request through your
configuration; and if you have problems, add the debug log level to
help. The debug log is quite verbose but very helpful in finding out
what NGINX is doing with your request and where in your configu‐
ration you’ve gone wrong.
16.2 Debugging Configs | 159
Also See
How NGINX Processes Requests
Debugging Admin Guide
Rewrite Log
16.3 Conclusion
This book has focused on high-performance load balancing, secu‐
rity, and deploying and maintaining NGINX and NGINX Plus
servers. The book has demonstrated some of the most powerful fea‐
tures of the NGINX application delivery platform. NGINX Inc. con‐
tinues to develop amazing features and stay ahead of the curve.
This book has demonstrated many short recipes that enable you to
better understand some of the directives and modules that make
NGINX the heart of the modern web. The NGINX sever is not just a
web server, nor just a reverse proxy, but an entire application deliv‐
ery platform, fully capable of authentication and coming alive with
the environments that it’s employed in. May you now know that.
160 | Chapter 16: Practical Ops Tips and Conclusion
About the Author
Derek DeJonghe has had a lifelong passion for technology. His
background and experience in web development, system adminis‐
tration, and networking give him a well-rounded understanding of
modern web architecture. Derek leads a team of site reliability engi‐
neers and produces self-healing, auto-scaling infrastructure for
numerous applications. He specializes in Linux cloud environments.
While designing, building, and maintaining highly available applica‐
tions for clients, he consults for larger organizations as they embark
on their journey to the cloud. Derek and his team are on the fore‐
front of a technology tidal wave and are engineering cloud best
practices every day. With a proven track record for resilient cloud
architecture, Derek helps RightBrain Networks be one of the stron‐
gest cloud consulting agencies and managed service providers in
partnership with AWS today.