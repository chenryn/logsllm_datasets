title:Intrusion detection and honeypots in nested virtualization environments
author:Michael Beham and
Marius Vlad and
Hans P. Reiser
Intrusion Detection and Honeypots in Nested Virtualization Environments
Michael Beham, Marius Vlad, Hans P. Reiser
Institute of IT-Security and Security Law
University of Passau, Germany
Email: PI:EMAIL, {mv,hr}@sec.uni-passau.de
Abstract—Several research projects in the past have built
intrusion detection systems and honeypot architectures based
on virtual machine introspection (VMI). These systems directly
beneﬁt from the use of virtualization technology. The VMI
approach, however, requires direct interaction with the virtual
machine monitor, and typically is not available to clients
of current public clouds. Recently, nested virtualization has
gained popularity in research as an approach that could enable
cloud customers to use virtualization-based solutions within a
cloud by nesting two virtual machine monitors, with the inner
one under control of the client. In this paper, we compare
the performance of existing nested-virtualization solutions and
analyze the impact of the performance overhead on VMI-based
intrusion detection and honeypot systems.
Keywords-Intrusion detection; Honeypots; Nested virtualiza-
tion; Cloud computing
I. INTRODUCTION
Virtualization is an enabling technology for cloud com-
puting. Besides that, virtualization has many other practical
use-cases. For example, it has successfully been used for
live migration [1], for enhancing system security [2], for
building resource-efﬁcient replication infrastructures [3], and
for malware analysis [4].
All of these approaches use features available at
the
hypervisor level in order to inspect, control and manipulate
guest virtual machines. As a result, these approaches can
be used only if such access to the hypervisor is available.
This observation implies that cloud clients typically cannot
use such approaches in the cloud, as usually the cloud
provider does not concede the required low-level access to its
clients. On the other hand, all above-mentioned approaches
are promising ways for increasing the ﬂexibility, security and
dependability of systems. They can contribute to enhancing
cloud-based applications if a solution is found for the
problem of granting the required hypervisor access to clients.
Recently, several researchers have investigated the possi-
bility of implementing nested virtualization on the popular
x86 CPU architecture. The idea behind this approach is to
run a second virtualization layer above an existing layer.
In an Infrastructure-as-a-Service (IaaS) cloud,
the cloud
provider would manage only the ﬁrst-level (L1) hypervisor,
which clients cannot access directly. Clients have full control
of their second-level (L2) hypervisor.
This paper focusses on enhancing services running on
virtual machines in IaaS clouds. Intrusion detection systems
978-1-4799-0181-4/13/$31.00 ©2013 IEEE
and honeypots are useful concepts for building secure and
dependable services in the cloud: Intrusion detection enables
the system to take immediate countermeasures when an
attack occurs, and honeypots provide information about
actions of attackers and thus help protecting a production
system against such actions.
More speciﬁcally, we present our work-in-progress that
investigates the feasibility of combining virtualization-based
intrusion detection and virtualization-based honeypots with
nested virtualization. Our experiments have shown that in
practice such a combination is not simple and straight-
forward to set up with existing components. In this pa-
per, we show the results of experiments that evaluate the
performance of existing nested virtualization approaches.
We use these results to select a suitable base system for
further experiments with deploying an existing VMI-based
hybrid honeypot within a nested virtualization environment.
We report upon our experiences and lessons learned and
discuss the applicability of this approach to existing cloud
infrastructures.
The paper is structured as follows: The next section
discusses the theory and practice of nested virtualization
and presents our benchmarks for comparing different nested
virtualization environments. Section III focusses on our
experiences using the Honeymon IDS [5] within a nested vir-
tualization environment, analyzes the resulting performance
impact, and discusses the applicability to public clouds.
Section IV presents related work and Section V presents
our conclusions.
II. NESTED VIRTUALIZATION
A. Background
In the past decade, virtualization has become highly popu-
lar, and single-level virtualization is in widespread use. Sup-
porting nested virtualization creates additional challenges for
virtualizing CPUs, memory and I/O.
For CPU virtualization, hardware virtual machine (HVM)
support is widely used today. On x86 systems, Intel and
AMD independently created two different approaches, called
VMX and SVM, respectively. These approaches work only
for a single layer of virtualization. With nested virtualization,
a L2 hypervisor executes as a guest of a L1 hypervisor, thus
having no direct access to hardware virtualization functions.
There are two ways of solving this problem: One possibility
is to implement the second level without using the function-
ality provided by VMX or SVM hardware. This implies that
the second level needs to use either paravirtualization (which
eliminates the possibility of running non-paravirtualized
guests within the second level) or to use binary translation
techniques [6]. The second possibility is that
the outer
hypervisor emulates VMX or SVM instructions on the
virtual CPU used by the inner virtualization layer. This way,
the inner hypervisor can use HVM-based full virtualization
(i.e., use SVM or VMX CPU instructions), but this increases
the complexity of the outer hypervisor.
For memory virtualization, nested virtualization makes it
necessary to add another mapping step for virtual addresses.
With single-level virtualization, the hypervisor needs to vir-
tualize the memory-management unit (MMU): Guest virtual
addresses need to be translated to guest physical addresses,
which need to be translated to host physical addresses. This
two-level translation can be done in software, which usually
means that for each guest page table, the hypervisor creates
a shadow page table that contains a mapping from guest
virtual addresses to host physical addresses, and keeps that
in sync with the guest-internal mapping from virtual to guest
physical addresses. Second-generation HVM implements
additional hardware support for such two-level translations
(“nested page tables” for AMD, “extended page tables”
for Intel). With two-level nested virtualization, yet another
translation step becomes necessary. This means that three
address translations need to be handled, while hardware
support is available for only two translations.
For accessing I/O devices, a VM has the options of using
direct assignment, emulation, or paravirtualization. Direct
assignment means that a speciﬁc L2 guest VM gets direct
access to some physical hardware. While this potentially
offers near-native performance, it has the disadvantage of
limiting the access to the device to a single guest. For our
use-cases (VMI-based IDS and honeypots), it is not a good
option, as we want to monitor the I/O of some target VM in
other VMs. The alternative to direct assignment is providing
virtual devices to the guest, which is possible in two ﬂavours.
With emulation, the hypervisor emulates a virtual device
that
looks like real hardware for the guest and works
with existing guest device drivers. With paravirtualization,
the guest needs a special device driver that communicates
directly with the hypervisor, which normally results in better
performance. For nested virtualization, each layer can use
one of the two possibilities, resulting in several possible
combinations, each with different performance.
B. Practical Systems
A ﬁrst approach to support nested virtualization on an x86
architecture for AMD hardware was published in 2008 by
Graf and Roedel [7] as a patch for the KVM hypervisor.
Due to the design of the AMD SVM instruction set, such a
support was relatively easy to implement. The KVM imple-
mentation for nested virtualization using SVM also provides
support for 2nd-generation HVM, making use of nested page
table to minimize memory virtualization overhead.
A similar approach has been taken by the Turtles
project [8] for Intel VMX in 2010. Due to architectural
differences between VMX and SVM, VMX support requires
a more complex approach. The Turtles paper also discusses
various alternatives on how to handle virtual memory man-
agement for nested hypervisors with HVM.
For Xen, partial support for nested virtualization has been
reported in October 2012 as an experimental feature [9]. All
above-mentioned approaches are systems that provide HVM
features to the L2 hypervisor.
In 2010, Berghmanns compared several nested virtual-
ization approaches based on 4 different hypervisors (Xen,
KVM, VMware, and VirtualBox) [10]. Besides one combi-
nation running paravirtualized Xen as L2-hypervisor on top
of VMware as L1 with binary translation, all other working
combinations found by the authors used an L2-hypervisor
based on paravirtualization or binary translation within an
HVM-based L1 system. The author did not ﬁnd any working
HVM on HVM conﬁguration and thus did not include any
performance measurements on such system in his study.
Since then, support for nested full virtualization has
been added to the Xen and KVM hypervisor. In the next
section, we analyse the performance of HVM-based nested
virtualization environments based on these hypervisors.
C. Performance measurements
There are several different hypervisors that support nested
virtualization, but only little information is available that
would permit a conclusive comparison of multiple ap-
proaches. Usually, the only available data are results that
compare systems with non-nested virtualization. Therefore,
we decided to ﬁrst use some generic experiments for com-
paring existing systems that support nested virtualization.
Some nesting combinations that we tried simply failed (for
example, with guest system crashing with a kernel panic),
which indicates that some existing implementations are still
not mature enough for production use. Focusing on systems
that support HVM, we found two working combinations
that we used for further analysis: KVM (L1) with KVM
(L2), denoted in the following as “KVM/KVM”, and KVM
(L1) with Xen (L2), denoted as “KVM/Xen”. For each
conﬁguration, we also experimented with various options
for I/O virtualization for disk and network.
For the following experiments, we used a test environment
consisting of identical servers with 32 GB RAM and two
AMD Opteron 4280 CPUs, which means a total of 16 cores
per server, running at 2.8 GHz. We used the Linux kernel
version 3.3.2 with qemu-kvm version 1.1.2, and Xen version
4.2. The outer KVM used HVM with CPU mode “host-
passthrough”, offering an identical CPU to the L2 hypervisor
(with the SVM ﬂag). In the test we used guest VMs with
(a) CPU performance, obtained by the wprime benchmark
calculating 1024 million square roots (lower is better)
(b) Network performance, measured by the iperf benchmark, with
emulated and paravirtualized network device (higher is better)
(c) HDD throughput for read (R-Ops) and write (W-Ops) operations, and CPU load during reads (R-Load) and writes (W-Load),
with emulated and paravirtualized devices (higher is better)
Figure 1. Performance comparison for several single-level and nested (two-level) virtualization conﬁgurations
1 GB RAM, 12 GB disk space and a virtual single-core
CPU running Windows XP. The following benchmarks cover
CPU, network, and disk performance.
Regarding CPU performance, we expected that nested
virtualization would not have signiﬁcant inﬂuence. To verify
this statement, we set up the following experiment: We used
wprime1, a benchmark that calculates square roots using the
iterated Newton’s methods. It does not use disk or network
I/O. We ran the tool for the ﬁrst 1024 million integers
and measured the total execution time on a Windows XP
guest OS, with single-level virtualization (KVM, Xen) and
two-level nested virtualization (KVM/KVM and KVM/Xen).
Figure 1(a) shows the results. While for KVM/KVM the re-
sult was as expected (an negligible 4% overhead), KVM/Xen
experiment resulted in an approx. 300% slowdown.
For network and disk performance, by default the hyper-
visor emulates all devices such as network cards and hard
disks. The guest VM can use these virtual devices with
standard drivers for real hardware. In an attempt to enhance
performance, we also experimented with using paravirtual-
ized devices. For KVM, we used virtio drivers [11], which
use special frontend drivers in the guest VM, and backend
drivers in the hypervisor. Instead of simulating real physical
hardware, paravirtualization enables direct communication
between guest VM and hypervisor. On Xen we used “PV-
on-HVM” drivers (PVHVM), which provide a similar para-
virtualization approach. There is some progress towards
integrating virtio into Xen as well, but currently this is
still an experimental feature that requires manually patching
Xen. Therefore, we did not consider virtio on Xen in our
experiments.
For network performance, Fig. 1(b) shows performance
results we obtained using the iperf 2 tool. This benchmark
measures the maximum throughput of a network device.
For pure emulation, there is a signiﬁcant drop in network
throughput by a factor of more than 5 (comparing single-
1http://www.wprime.net/
2http://iperf.sourceforge.net/
 0 20 40 60 80 100 120L1 KVML1 XenL2 KVM/KVML2 KVM/XenTime (minutes) 0 50 100 150 200 250KVMemuKVMvirtioXenemuXenPVHVMKVM/KVMemu/emuKVM/KVMvirtio/emuKVM/KVMvirtio/virtioKVM/Xenemu/emuKVM/Xenemu/PVHVMThroughput (Mbit/s)single-levelnested 0 20 40 60 80 100 120 140L1 KVMemuL1 KVMvirtioL1 XenemuL1 XenPVHVML2 KVM/KVMemu/emuL2 KVM/KVMvirtio/emuL2 KVM/KVMvirtio/virtioL2 KVM/Xenemu/emuL2 KVM/Xenemu/PVHVM 0 20 40 60 80 100Throughput (MB/s)CPU Load (%)single-levelnestedR-OpsR-LoadW-OpsW-Loadlevel to nested virtualization). For paravirtualization (virtio
vs virtio/virtio), the throughput drops approx. by a factor of
4. This means that independent of whether paravirtualization
or emulation is used, nested virtualization has a strong
negative impact on network performance. For increasing
I/O performance, paravirtualization should be preferred over
emulation.
Fig. 1(c) shows the measurement results for HDD read
and write throughput, obtained for reading/writing the full
12 GB virtual disk using the tool HDTune 4.60 Pro3. The
ﬁgure also shows the CPU load on the guest system during
benchmark execution. Similar to the network benchmark,
paravirtualization usually results in a signiﬁcant performance
increase. For write throughput on Xen, however, emulation
(i.e., “emu/emu” on KVM/Xen) outperforms paravirtuali-
zation (i.e., “emu/PVHVM” on KVM/Xen) for unknown
reasons. With two-level nesting, HDD performance in all
cases drops to below half of the performance with single-
level virtualization.
All ﬁgures show that nesting virtualization performance
in Xen 4.2.0 is very poor compared to KVM. It seems that
the source code of Xen is not as optimized for handling
nested SVM/VMX instructions as is KVM. The new Xen
version 4.2.1 incorporates several improvements to nested
virtualization support, so, while we have not veriﬁed this