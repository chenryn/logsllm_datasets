Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:27:26 UTC from IEEE Xplore.  Restrictions apply. 
for the convenience of mapping the primary array elements
to their duplicates.
Let us assume ˜A is the duplicate array of A. Note that
the dimensionality of array ˜A is the same as array A; the
difference is in the size of each dimension, which we need
to determine for ˜A. We use a vector (cid:1)S to denote the size of
array ˜A and (cid:1)Si is the size of the ith dimension of ˜A. We
determine the array size as follows:
(cid:1)Sl = ∪(cid:1)I(cid:2) (cid:1)J≺(cid:1)I+(cid:1)LR0( (cid:1)J); (cid:1)Su = ∩(cid:1)I(cid:2) (cid:1)J≺(cid:1)I+(cid:1)LR0( (cid:1)J);
(cid:1)S = max((cid:1)Su − (cid:1)Sl) + [1 1 ··· 1]T
The operators∪ and ∩ determine the lower bound and upper
bound, respectively, across all involved vectors. After (cid:1)S has
been determined, we can transform the references to array A
in the duplicate execution. We ﬁrst change all the references
to refer to ˜A. Then, Rk((cid:1)I) is transformed as Rk((cid:1)I)%(cid:1)S.
In this last expression, the operator % between two d-entry
vectors has the following meaning:
If
(cid:1)V = (cid:1)R % (cid:1)S ⇒ ∀1 ≤ i ≤ d : vi = ri mod si.
(cid:1)Dmax is not a zero vector, the initial loop iterations
might read values that are assigned before the loop nest. In
this case, we need to copy those values from the primary
array A to the duplicate array ˜A before the loop nest is en-
tered. These copy operations can be expressed as follows:
∀1 ≤ k ≤ n ∀0 (cid:5) (cid:1)I ≺ (cid:1)Dk : ˜A[Rk((cid:1)I)%(cid:1)S] = A[Rk((cid:1)I)].
(1)
Note that there are redundant copy operations in the above
operation set. While it is possible to remove those redun-
dant operations through additional compiler analysis, study-
ing this issue further is beyond the scope of this paper.
The above discussion assumes that there is only one
write reference to the array being considered. Let us now
consider the case where there are two write references,
namely R0 and R1, to the array A being considered. In this
case, there are two possible scenarios for these two write
references: either there is a data dependence between them,
or there is no data dependence between them. If there is
a data dependence between these two write references, our
scheme explained so far can deal with this case with little
modiﬁcation. Without loss of generality, we assume that
there is a data dependence from R0 to R1, and the depen-
dence distance vector is (cid:1)D1. We can use the same strategy
we used earlier in determining sharing memory locations to
store duplicates. In fact, we can treat R1 the same way as
we treated read references. On the other hand, if there is
no data dependence between these two write references, we
can treat them as two different arrays. In this case, the read
references to A can be divided into two groups, based on
with which write reference they have data reuse:
R1
R2
0, R1
1,
0, R2
1,
. . . , R1
. . . , R2
n1 ;
n2 .
Here, the superscript i in Ri
k means the group the reference
belongs to (i.e., one of the two groups shown above). Notice
k for i = 1, 2.
that there is a data dependence from Ri
The array elements accessed by these two groups do not
0 to Ri
int U [101][102];
for(i = 0; i < 100; i + +)
for(j = 0; j < 100; j + +)
U [i + 1][j + 2] = U [i][j] + U [i + 1][j + 1];
(a) Original loop nest.
int U [101][102];
for(i = 0; i < 100; i + +)
for(j = 0; j < 100; j + +) {
U [i + 1][j + 2] = U [i][j] + U [i + 1][j + 1];
chkp = chkp ⊕ U [i + 1][j + 2];
}
(b) Primary loop nest.
int ˜U [101][102];
for(i = 0; i < 100; i + +)
for(j = 0; j < 100; j + +) {
˜U[i + 1][j + 2] = ˜U [i][j] + ˜U [i + 1][j + 1];
chkd = chkd ⊕ ˜U [i + 1][j + 2];
}
(c) Duplicate loop nest obtained using NRWD.
int ˜U [2][100];
for(i = 0; i < 100; i + +)
for(j = 0; j < 100; j + +) {
˜U[(i + 1)%2][j + 2%100] =
˜U[i%2][j%100] + ˜U [(i + 1)%2][(j + 1)%100];
chkd = chkd ⊕ ˜U [(i + 1)%2][(j + 2)%100];
}
(d) Duplicate loop nest obtained using intra-nest optimization.
Figure 11. An example application of our intra-nest
memory space optimization.
0 and R2
overlap (since, otherwise, R1
0 would have data de-
pendence between them); therefore, sharing memory loca-
tions for storing duplicates within one group does not affect
any array access in the other group. This essentially means
that we can treat the two groups as two different arrays, and
share the memory locations for duplicates independently.
Now let us apply our mathematical approach discussed
above to array U in Figure 7. There are two references to
array U , and both of them have the same access matrix, i.e.,
we have R0((cid:1)I) = R1((cid:1)I) = [i]. It is easy to see that array
U satisﬁes the memory reuse constraints. For this array, we
(cid:1)Dmax = (cid:1)D1 = [0]. Then, we can determine (cid:1)L
can obtain
as follows:
(cid:1)L = min{(cid:1)V | (cid:1)V (cid:4) [0] and (cid:1)V is an axis vector} = [1].
After obtaining (cid:1)L, we determine (cid:1)S, which is the required
size for ˜U :
(cid:1)Sl = ∪(cid:1)I(cid:1) (cid:1)J≺(cid:1)I+[1]
R0( (cid:1)J) = [i]; (cid:1)Su = ∩(cid:1)I(cid:1) (cid:1)J≺(cid:1)I+[1]
(cid:1)S = max((cid:1)Su − (cid:1)Sl) + [1] = [1].
R0( (cid:1)J) = [i];
Therefore, the size of ˜U is 1. Next, we transform R0((cid:1)I)
and R1((cid:1)I) to R0((cid:1)I%(cid:1)S) and R1((cid:1)I%(cid:1)S), respectively. Since
i mod 1 is always 0 for all integer values of i, both the array
references to ˜U are transformed to ˜U[0]. Since (cid:1)D1 is a zero
vector, the resulting copy operation set from Expression (1)
above is empty. Consequently, there is no copy operation
required before the loop nest, and the resulting code frag-
ment is shown in Figure 7(b).
Figure 11 presents another example application of our
intra-nest memory space optimization. Figure 11(a) gives
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:27:26 UTC from IEEE Xplore.  Restrictions apply. 
Table 1. Major simulation parameters.
Simulation Parameter
Number of Processors
L1 Cache Topology
L1 Cache Latency
L2 Cache Topology
L2 Cache Latency
Bus Contention Latency
Main Memory Latency
Parallelization Strategy
Default Value
8
8KB; 2-way; 64B line size
2 cycle
2MB; direct-mapped; 128B line size
12 cycle
4 cycles
100 cycles
outer-most loop parallelism
Figure 12. Sharing memory locations across loop
boundaries for storing duplicates.
the original loop nest and Figure 11(b) gives the primary
loop nest. If we use NRWD, we have to provide a full-size
duplicate array ( ˜U) for array U , as shown in Figure 11(c).
Now, let us apply our mathematical approach to this loop
nest. We obtain the following result:
(cid:1)D1 = [1 2]T ; (cid:1)D2 = [0 1]T ;
(cid:1)L = min{(cid:1)V | (cid:1)V (cid:1) [1 2]T and (cid:1)V is an axis vector} = [2 0]T .
Then, we can calculate the size of ˜U as follows:
(cid:1)Sl = ∪(cid:1)I(cid:1) (cid:1)J≺(cid:1)I+[2 0]T R0( (cid:1)J) = [i + 1 2];
(cid:1)Su = ∩(cid:1)I(cid:1) (cid:1)J≺(cid:1)I+[2 0]T R0( (cid:1)J) = [i + 2 101];
(cid:1)S = max((cid:1)Su − (cid:1)Sl) + [1 1] = [2 100]T .
(cid:1)Dmax = [1 2]T ;
Therefore, the size of ˜U is 2 × 100. Next, we transform
Rk((cid:1)I) to Rk((cid:1)I%[2 100]T ). After that, we determine the
copy operation set based on Expression (1):
∀0 (cid:5) (cid:1)I ≺ [1 2]T : ˜U[i%2][j%100] = A[i][j];
∀0 (cid:5) (cid:1)I ≺ [0 1]T : ˜U[(i + 1)%2][(j + 1)%100] = A[i + 1][j + 1].
The resulting code fragment without copy operations is
given in Figure 11(d). Note that, as compared to the NRWD
scheme (Figure 11(c)), this intra-nest optimization reduces
extra memory requirements signiﬁcantly.
3.3.4 Inter-nest memory space optimization
If an array A does not satisfy the memory reuse constraints
given in Section 3.3.3, we need to have a full-sized duplicate
array for it. Instead of creating a new array ˜A, we might be
able to ﬁnd another array, which is not live during the cur-
rent loop nest execution. Our inter-nest memory space opti-
mization analyzes the lifetimes of multiple arrays across the
loop nest boundaries and determines whether there is any
opportunity for reusing an array’s memory space for storing
another array’s duplicate. Figure 12 illustrates an example
scenario for the inter-nest memory space optimization. In
this ﬁgure, both array B and C can be used for storing array
A’s duplicate since they are not live during the execution of
loop nest l. On the other hand, array D should not be used
for storing A’s duplicate, since it is live during loop nest l.
It should be noted that, in our current implementation,
we consider inter-nest memory space reuse only for arrays
with the same dimensionality. That is, if array B has a dif-
ferent number of dimensions than array A, we do not use
B for storing A’s duplicate even if B is not live in current
Table 2. Benchmarks used in this study.
Benchmark
171.swim
172.grid
177.mesa
179.art
183.equake
188.ammp
Benchmark
atr
bss
encr
img-seg6
usonic
wood04
SPECFP2000
Brief Description
Shallow Water Modeling
Multi-Grid Solver
3D Graphic Library
Image Recognition/Neural Networks
Seismic Wave Propagation Simulation
Computational Chemistry
Embedded Applications
Brief Description
Network Address Translation
Signal Deconvolution
Digital Signature for Security
Embedded Image Segregation
Feature-Based Area Estimation
Color-Based Surface Inspection
Input
Ref. Input
Ref. Input
Ref. Input
Ref. Input
Ref. Input
Ref. Input
Input
1.47MB
3.07MB
1.88MB
2.61MB
4.36MB
5.28MB
loop nest. In addition, our liveness analysis for the inter-
nest memory space optimization operates on a whole array
and a whole loop nest granularity. This means that, even
if only a single element of array A is live at a single itera-
tion of loop nest l (which rarely happens), array A is still
considered to be live during loop nest l.
3.3.5 Summary of the CDDR Scheme
For a program with a set of loop nests, our approach han-
dles loop nests one by one. For each array used in the cur-
rent loop nest being optimized, if it is read-only, it does not
need a duplicate. If it is not read-only but write-only, we
feed its results to checksums and consequently no duplicate
is required for this case either. If it is both read and writ-
ten in the loop nest, we check it against our memory reuse
constraints discussed earlier on page 5. We employ intra-
nest memory space optimization for this array if it satisﬁes
the reuse constraints. If not all arrays can be covered us-
ing these three optimizations, we ﬁrst analyze the liveness
information for all arrays across the loop nests of the ap-
plication. After that, we use inter-nest memory space op-
timization explained in Section 3.3.4 to exploit any reuse
opportunities across the different arrays. If an array cannot
be covered by any of these optimizations, we create a new
full-sized duplicate array.
4 Experiments
4.1 Setup
Our goal in this section is to present an experimental
evaluation of the proposed memory-conscious code dupli-
cation approach. We implemented our approach using the
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:27:26 UTC from IEEE Xplore.  Restrictions apply. 
Figure 13. Extra memory requirements with differ-
ent schemes.
publicly-available SUIF compiler infrastructure from Stan-
ford University [30]. SUIF consists of a small kernel and a
suite of compiler passes built on top of the kernel. The ker-
nel deﬁnes the intermediate representation (IR), provides
functions to access and manipulate the intermediate rep-
resentation, and structures the interface between compiler
passes. We applied two passes to each input code in our
experimental suite before it is modiﬁed by our approach,
which itself is implemented as a separate pass. These two
passes are loop parallelizer and locality optimizer. The ad-
ditional compile time overhead brought by our approach
was 35% on the average with respect to the case when the
codes are compiled without our optimization.
To simulate parallel application execution, we used SIM-
ICS [23], which is a instruction set simulator that can simu-
late the execution of an application and the operating system
with multiple processors. The default values of the major
simulation parameters for the architecture we modeled are
given in Table 1. Later in our experiments we modify some