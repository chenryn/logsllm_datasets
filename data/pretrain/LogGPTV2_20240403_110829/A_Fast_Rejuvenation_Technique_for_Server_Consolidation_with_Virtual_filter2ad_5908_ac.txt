### 5.2. Effect of Quick Reload

To evaluate the effectiveness of the quick reload mechanism in reducing VMM reboot time, we measured the duration from the completion of the shutdown script to the completion of the VMM reboot. Using the quick reload method, this process took 11 seconds, compared to 59 seconds when using a hardware reset. Consequently, the quick reload mechanism reduced the VMM reboot time by 48 seconds.

### 5.3. Downtime of Networked Services

**Figure 5: Time for pre- and post-reboot tasks with varying numbers of VMs.**

The on-memory suspend/resume method does not interact with the memory image of a VM, making it highly efficient. For a memory size of 11 GB, the suspend operation took 0.08 seconds, and the resume operation took 0.9 seconds. These times represent only 0.06% and 0.7% of the corresponding operations in Xen, respectively.

We also measured the time required for pre- and post-reboot tasks with multiple VMs running in parallel. Each VM was allocated 1 GB of memory, and the number of VMs varied from 1 to 11 (excluding Domain 0). The results are shown in Figure 5. All three methods—on-memory suspend/resume, Xen's suspend/resume, and OS shutdown/boot—were dependent on the number of VMs. When 11 VMs were running, on-memory suspend/resume needed only 0.04 seconds for suspend and 4.2 seconds for resume, which is 0.02% and 2.7% of Xen's suspend and resume times, respectively. Additionally, the boot time increased significantly as the number of VMs increased.

**Downtime of Networked Services:**

We measured the downtime of networked services during VMM rejuvenation by repeatedly sending packets from a client host to VMs on a server host while rebooting the VMM. We tested three scenarios: (1) warm-VM reboot, (2) saved-VM reboot (using Xen's suspend/resume), and (3) cold-VM reboot (shutdown/boot). Each VM was allocated 1 GB of memory, and the number of VMs varied from 1 to 11.

**SSH Server Downtime:**

First, we ran an SSH server in each VM and measured its downtime during the VMM reboot. The results are shown in Figure 6 (a). The downtime for the saved-VM reboot was highly dependent on the number of VMs. With 11 VMs, the average downtime was 429 seconds. In contrast, the warm-VM reboot had a downtime of 42 seconds, which is only 9.8% of the saved-VM reboot. The warm-VM reboot's downtime was relatively independent of the number of VMs. The cold-VM reboot resulted in a downtime of 157 seconds with 11 VMs, which is 3.7 times longer than the warm-VM reboot.

After the VMM was rebooted using either the warm-VM or saved-VM reboot, the SSH session could be continued due to TCP retransmission, even if a timeout was set in the SSH server. However, if a 60-second timeout was set in the SSH client, the session would time out during the saved-VM reboot. The cold-VM reboot, on the other hand, did not allow the session to continue because the SSH server was shut down.

**JBoss Application Server Downtime:**

Next, we ran a JBoss application server [18] and measured its downtime during the VMM reboot. JBoss is a large server that takes more time to start than an SSH server. We used the default configuration of JBoss. The results are shown in Figure 6 (b). The downtime for the warm-VM and saved-VM reboots was similar to that of the SSH server, as these methods resumed the VMs without needing to restart the JBoss server. The cold-VM reboot, however, required restarting the JBoss server, resulting in a longer downtime. With 11 VMs, the downtime was 241 seconds, which is 1.5 times longer than that of the SSH server. This indicates that the cold-VM reboot increases service downtime based on the running services.

**Availability Analysis:**

For the JBoss server with 11 VMs, we assumed that OS rejuvenation occurs weekly and VMM rejuvenation occurs every four weeks. Based on our experiments, the downtime due to OS rejuvenation was 33.6 seconds. For the cold-VM reboot, we assumed an expected value of α (as defined in Section 3.2) of 0.5. Under these assumptions, the availability was 99.993% for the warm-VM reboot, 99.985% for the cold-VM reboot, and 99.977% for the saved-VM reboot. The warm-VM reboot achieved four 9s of availability, while the others achieved three 9s. This improvement in availability is crucial for critical servers.

### 5.4. Downtime Analysis

To understand the factors that reduce downtime in the warm-VM reboot, we measured the time required for each operation during the VMM reboot and the throughput of a web server running on a VM. We used the httperf benchmark tool [20] to send requests from a client host to an Apache web server [4] running on a VM in a server host. We created 11 VMs, each with 1 GB of memory, and recorded the changes in the average throughput of 50 requests. The results are shown in Figure 7. The reboot command was executed at 20 seconds, and the time required for each operation is superimposed on the figure.

**Figure 7: Breakdown of downtime due to VMM rejuvenation.**

As previously noted, the on-memory suspend/resume mechanism in the warm-VM reboot significantly reduced downtime. The total time for on-memory suspend/resume was 4 seconds, compared to 63 seconds for the shutdown and boot in the cold-VM reboot. The warm-VM reboot also eliminated the 43-second hardware reset time. Additionally, the warm-VM reboot allowed the web server to continue running until just before the VMM reboot, reducing downtime. The web server stopped at 34 seconds in the warm-VM reboot, compared to 27 seconds in the cold-VM reboot, saving 7 seconds. The VMM was responsible for suspending VMs after Domain 0 was shut down.

In both cases, the throughput was restored after the VMM reboot. During the cold-VM reboot, the throughput was degraded for 8 seconds due to file cache misses. In the warm-VM reboot, the throughput was degraded for 25 seconds after the reboot, which was not due to cache misses but rather an implementation issue in Xen. Simultaneous creation of new VMs temporarily degraded network performance.

### 5.5. Performance Degradation

To examine performance degradation due to cache misses, we measured the throughput of file access operations in a VM before and after the VMM reboot. We allocated 11 GB of memory to one VM and measured the time required to read a 512 MB file, with all file blocks cached in memory. The results are shown in Figure 8 (a).

**Figure 8: Throughput of file reads before and after VMM reboot.**

When using the warm-VM reboot, the first-time read throughput was 1200 MB/s, and the second-time read throughput was 1000 MB/s. After the reboot, the first-time read throughput was 800 MB/s, and the second-time read throughput was 600 MB/s. For the cold-VM reboot, the first-time read throughput was 400 MB/s, and the second-time read throughput was 200 MB/s. The performance degradation in the cold-VM reboot was due to file cache misses, while the warm-VM reboot's degradation was due to an implementation issue in Xen.

---

This revised text is more structured, clear, and professional, with improved readability and coherence.