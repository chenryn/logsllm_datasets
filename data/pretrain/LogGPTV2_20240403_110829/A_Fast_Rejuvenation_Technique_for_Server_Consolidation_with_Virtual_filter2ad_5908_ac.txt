200
150
100
50
)
c
e
s
(
e
m
i
t
d
e
s
p
a
e
l
0
0
2
250
200
150
100
50
)
c
e
s
(
e
m
i
t
d
e
s
p
a
e
l
10
0
0
2
Xen’s suspend
OS shutdown
on-memory suspend
4
8
number of VMs
6
Xen’s resume
OS boot
on-memory resume
4
8
number of VMs
6
(a) pre-reboot task
(b) post-reboot task
5.2. Effect of Quick Reload
To examine how fast the VMM is rebooted by using the
quick reload mechanism, we measured the time needed for
rebooting the VMM. We recorded the time when the exe-
cution of a shutdown script completed and when the reboot
of the VMM completed. The time between them was 11
seconds when we used quick reload whereas it was 59 sec-
onds when we used a hardware reset. Thus, the quick reload
mechanism speeded up the reboot of the VMM by 48 sec-
onds.
10
5.3. Downtime of Networked Services
Figure 5. The time for pre- and post-reboot
tasks when the number of VMs is changed.
the memory size because this method does not touch the
memory image of a VM. When the memory size was 11
GB, it took 0.08 seconds for suspend and 0.9 second for re-
sume. These are only 0.06 % and 0.7 % of Xen’s suspend
and resume, respectively.
Next, we measured the time needed for pre- and post-
reboot tasks when multiple VMs were running in parallel.
We ﬁxed the size of memory allocated to each VM to 1 GB
and changed the number of VMs from 1 to 11. Domain 0 is
not included in the number. Figure 5 shows the results. All
the three methods depended on the number of VMs. When
the number of VMs was 11, on-memory suspend/resume
needed only 0.04 seconds for suspend and 4.2 seconds for
resume. These were 0.02 % and 2.7 % of Xen’s suspend and
resume, respectively. The result also shows that the time for
the boot largely increases as the number of VMs increases.
We measured the downtime of networked services when
we rejuvenated the VMM. We rebooted the VMM while
we repeated sending packets from a client host to the VMs
in a server host. We measured the time from when a net-
worked service in each VM was down and until it was up
again after the VMM was rebooted. We performed this ex-
periment for (1) the warm-VM reboot, (2) the reboot using
Xen’s suspend/resume (saved-VM reboot), and (3) the re-
boot by shutdown/boot (cold-VM reboot). We ﬁxed the size
of memory allocated to each VM to 1 GB and changed the
number of VMs from 1 to 11.
First, we ran only a ssh server in each VM and measured
its downtime during the reboot of the VMM. Figure 6 (a)
shows the downtime. The downtime by the saved-VM re-
boot highly depended on the number of VMs. When the
number was 11, the downtime was 429 seconds in average.
At the same number of VMs, the downtime by the warm-
VM reboot was 42 seconds and only 9.8 % of the saved-
VM reboot. In addition, the downtime by the warm-VM re-
boot hardly depended on the number of VMs. On the other
hand, the downtime by the cold-VM reboot was 157 sec-
onds when the number of VMs was 11. This was 3.7 times
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:26 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007longer than the warm-VM reboot.
After we rebooted the VMM using the warm-VM reboot
or the saved-VM reboot, we could continue the session of
ssh thanks to TCP retransmission, even if a timeout was set
in the ssh server. However, if a timeout was set to 60 sec-
onds in the ssh client, the session was timed out during the
saved-VM reboot. From this point of view, the downtime
for one reboot should be short enough. When we used the
cold-VM reboot, we could not continue the session because
the ssh server was shut down.
Next, we ran a JBoss application server [18] and mea-
sured its downtime during the reboot of a VMM. JBoss is a
large server and it takes more time to start than a ssh server.
We used the default conﬁguration of JBoss. Figure 6 (b)
shows the downtime. The downtime by the warm-VM re-
boot and the saved-VM reboot was almost the same as that
of a ssh server because these reboot mechanisms resumed
VMs and did not need to restart the JBoss server. On the
other hand, the downtime by the cold-VM reboot was larger
than that of a ssh server because the cold-VM reboot needed
to restart the JBoss server. When the number was 11, the
downtime was 241 seconds. This was 1.5 times longer than
that of a ssh server. This means that the cold-VM reboot in-
creases the service downtime according to running services.
Let us consider the availability of the JBoss server when
the number of VMs is 11. As an example, we assume that
the OS rejuvenation is performed every week and the VMM
rejuvenation is performed once per four weeks. According
to our experiment, the downtime due to the OS rejuvenation
was 33.6 seconds. For the cold-VM reboot, we assume that
the expected value of α in Section 3.2 is 0.5. Under these
assumptions, the availability is 99.993 %, 99.985 %, and
99.977 % for the warm-VM reboot, the cold-VM reboot,
and the saved-VM reboot, respectively. The warm-VM re-
boot achieves four 9s although the others achieve three 9s.
This improvement of availability is important for critical
servers.
5.4. Downtime Analysis
To examine which factors reduce downtime in the warm-
VM reboot, we measured the time needed for each op-
eration when we rebooted the VMM. At the same time,
we measured the throughput of a web server running on a
VM. We repeated sending requests from a client host to the
Apache web server [4] running on a VM in a server host by
using the httperf benchmark tool [20]. We created 11 VMs
and allocated 1 GB of memory to each VM. We rebooted
the VMM and recorded the changes of the average through-
put of 50 requests. We performed this experiment for the
warm-VM reboot and the cold-VM reboot. Figure 7 shows
the results. We executed the reboot command in domain 0
at time 20 seconds in this ﬁgure. We superimposed the time
domU suspend VMM boot
dom0 shutdown
domU resume
300
200
100
0
300
200
100
c
e
s
/
s
t
s
e
u
q
e
r
c
e
s
/
s
t
s
e
u
q
e
r
0
0
dom0 boot
domU shutdown
VMM boot
(a) warm-VM reboot
(b) cold-VM reboot
hardware reset
dom0 boot
domU boot
20
40
60
dom0 shutdown
80
100
elapsed time (sec)
120
140
160
180
200
220
Figure 7. The breakdown of the downtime due
to the VMM rejuvenation.
needed for each operation during the reboot onto Figure 7.
As shown in the previous section, the on-memory sus-
pend/resume mechanism provided by the warm-VM re-
boot reduced the downtime largely. The total time for on-
memory suspend/resume was 4 seconds, but that for shut-
down and boot in the cold-VM reboot was 63 seconds. In
addition, the warm-VM reboot reduced the time for a hard-
ware reset from 43 to 0 second. Also, the fact that the warm-
VM reboot can continue to run a web server until just before
the VMM is rebooted was effective for reducing downtime.
A web server was stopped at time 34 seconds in the warm-
VM reboot while it was stopped at time 27 seconds in the
cold-VM reboot. This reduced downtime by 7 seconds. For
the warm-VM reboot, the VMM is responsible for suspend-
ing VMs and it can do that task after domain 0 is shut down.
In both cases, the throughput was restored after the re-
boot of the VMM. The throughput in the cold-VM reboot
was degraded during 8 seconds. This was due to misses of
the ﬁle cache. We examine this performance degradation in
detail in the next section. The throughput in the warm-VM
reboot was also degraded during 25 seconds after the reboot.
This is not due to cache misses but an implementation prob-
lem of Xen. When Xen created new VMs simultaneously,
the network performance was degraded for a while.
5.5. Performance Degradation
To examine performance degradation due to cache
misses, we measured the throughput of operations with ﬁle
accesses in a VM before and after the reboot of a VMM.
To examine the effect of the ﬁle cache, we measured the
throughput of the ﬁrst- and second-time accesses. We allo-
cated 11 GB of memory to one VM. First, we measured the
time needed to read a ﬁle of 512 MB. In this experiment,
all the ﬁle blocks were cached on memory. We performed
this experiment for the warm-VM reboot and the cold-VM
reboot. Figure 8 (a) shows the result. When we used the
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:50:26 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007)
s
/
B
M
(
t
u
p
h
g
u
o
r
h
t
d
a
e
r
1200
1000
800
600
400
200
0
warm-VM reboot
cold-VM reboot
1st
2nd
before reboot
1st
2nd
after reboot
(a) file reads
)
c
e
s
/
s
t
s
e
u
q
e
r
(
t
u
p
h
g
u
o
r
h
t