0
2
.
0
0
.
0
−0.05−0
0−0.05
0.05−0.1 0.1−0.25 0.25−0.5
0.5−1
1−2
Seconds
Fig. 1. Histogram of pass-through times
together with one-way packet delays for
typical packet sizes 64, 576, and 1500.
Fig. 2. Histogram of upper and lower bo-
unds on pass-through times for MRAI va-
lues of 5 and 30 seconds.
Figure 2 shows the histogram of the pass-through times for two diﬀerent
MRAI timer values: 5s and the default Cisco value, which is roughly 30 seconds.
Note that with the MRAI timer in place the minimal measured pass-through
times are 71ms and 122ms. On the other hand the maximum values for the
lower bounds are 0.121 and 1.72 seconds! This indicates that an update might
have to wait a signiﬁcant amount of time before it is processed even if it reaches
the router at a good moment. This is especially the case for larger MRAI values
where the average pass-through time increases from 109ms for the 5s MRAI
timer to 883ms for the default MRAI value. Note that each experiment is run
for the same duration. Accordingly the number of samples for the pass-through
time decreases as the MRAI value increases.
Overall it seems that even for small MRAI values the timer interactions
between MRAI and the BGP update processing timer increases the minimum
pass-through time signiﬁcantly. As the MRAI value is increased the minimum
pass-through time also increases and will clearly dominate any link delays. Fur-
thermore, inspection of the probes on the monitoring session reveals that the
order of the update probes is not maintained. This means that a probe that was
sent 10 seconds later than another might be observed earlier on the monitoring
session.
2.3 Controlled Background CPU Load
Imposing a controllable background CPU load on the DUT is necessary in order
to study how it responds to stress. The goal is to identify a set of tasks that
generate a constant CPU load independent of BGP. This is diﬃcult as it implies
generating an input load that is uniformly served by a task running at a uniform
priority. This is rarely the case. In Cisco IOS the BGP processes (and any routing
tasks) have higher scheduling priority than almost everything else targeted at
the CPU. The IOS ip input task is a high priority process whose CPU use is
related to the rate of a packet stream directed to the DUT’s main IP address.
272
A. Feldmann et al.
Another problem is measuring the CPU load. The CPU load of a Cisco router
can be queried in two ways: via a command at the telnet interface of via an SNMP
query. Unfortunately, the default priorities of both telnet and SNMP are lower
than those of BGP and the packet processing tasks. Accordingly, for calibration
only, we raised the priority of SNMP task and then measured the CPU load both
via the command line as well as via SNMP for 5 rates: 2k, 5k, 10k, 15k pkt/s.
Both estimates aligned quite well with the only problem that the command line
interface did not deliver any values under high loads due to starvation. Figure 3
shows a histogram of the CPU load. 2k packets impose almost no load. 5k is
already signiﬁcant. 10k is almost critical while 15k is well beyond critical. Note
that a 15k packet rate corresponds to a bit rate of 0.36 Mbits, which is rather
modest for a high speed interface on a high end router. This should encourage
providers to ﬁlter internal destination addresses on all incoming connections.
0 pkts/s
2,000 pkts/s
5,000 pkts/s
10,000 pkts/s
15,000 pkts/s
0
8
0
6
0
4
0
2
0
Probe Updates
Source Peers
Router Tester/PC
Traffic Capture
PC
Device Under Test
Router (12008)
Probe Update 
Monitor Peer
Router (7507)
Data Traffic Src
Bkgrnd. Updates
Downstream 
Background
Router Tester
Source Peers
PC
Peers
PC
0%−15% 15%−30% 30%−60% 60%−75% 75%−90% 90%−100%
Percentage of CPU load
Fig. 3. Histogram of CPU load estimates
for packet rates of 2k, 5k, 10k and 15k di-
rected to the router IP.
Fig. 4. Test-bed setup for router
testing.
3 Test Framework
The testbed shown in Figure 4 illustrates its functional building blocks. The
physical layout is more complex and not shown here for the sake clarity and
brevity.
The device under test (DUT) is a Cisco 12008 GSR equipped with: 256MB
memory, 512KB of L2 cache, 200MHz GRP CPU, three Gigabit SX and 8 Fast
Ethernet interfaces. It runs IOS version 12.0(26)S. An Agilent RT900 router
tester is used for selective experiment calibration and to generate data traﬃc.
BGP updates, probes and background, are generated by a PC. The monitoring
peer runs on a Cisco 7507. Probe update traﬃc from the PC into the DUT is
captured by Endace DAG cards [12]. Outgoing probe update traﬃc from the
DUT to the monitoring peer is also captured by an Endace DAG card. All cards
are synchronized.
Measuring BGP Pass-Through Times
273
The DUT is subjected to three traﬃc types: BGP update probes, BGP back-
ground activity updates and non-routed data traﬃc directed to the DUT. Probe
updates are used to compute DUT pass-through times. We create a BGP activity
noise ﬂoor by generating separate update streams, called background updates,
that are in turn propagated to multiple downstream peers. Data traﬃc is used to
indirectly control the CPU load and hence the time alloted to BGP processing.
DAG generated time-stamps are used to compute the DUT pass-through
time. We use tethereal to decode and reconstruct the BGP TCP sessions from
the capture ﬁles. To ease the conﬁguration and setup of each experiment various
scripts automatically conﬁgure the PCs, the router tester, and the routers, then
start the experiments and after it is done start the evaluation. Unless speciﬁed
otherwise each experiment lasts for 15 minutes actual time but the evaluation is
not started for another 15 in order to retrieve all updates.
4 Pass-Through Times
Section 2 introduces our methodology for measuring pass-through times and
shows how to impose a background load on the DUT. In this section we explore
how pass-through times change as the demand on the router increases. Due to
the large number of parameters we cannot test all combinations. Rather, we
perform a number of tests to explore the variables to which pass-through times
are sensitive, including the background CPU load, the number of sessions in
combination with the BGP update rate, and the complexity of the BGP table
in combination with the BGP update rate.
More precisely in a ﬁrst step we combine BGP pass-through probes with
the background CPU load. Next we increase the CPU load by adding 100/250
additional BGP sessions and a total of 500 BGP updates a second. This expe-
riment uses a regular pattern of updates similar to the probes. Based on this
calibration of our expectation we explore the load that is imposed by actual
measured BGP tables. The next two experiments diﬀer in that one uses small
BGP tables containing between 15, 000 - 30, 000 preﬁxes while the other uses
large BGP tables containing between 110, 000 - 130, 000 updates. Due to the
memory requirements of this table the number of additional sessions is reduced
to 2. This provides us with a setup to explore diﬀerent BGP update rates: as
fast as possible (resembles BGP session resets), 200 updates and 20 updates a
second.
4.1 Pass-Through Times vs. Background CPU Load
This set of experiments is designed to show how the background CPU load
inﬂuences the BGP pass-through delays. Accordingly we combine the approach
for measuring BGP pass-through delays via active probes with that of imposing
a controlled background CPU load via a controlled packet stream directed to
the DUT’s IP address, see Section 2. We use a packet stream of 0, 2k, and 10k
packets as the ﬁrst two impose no additional or just minimal load while the
latter is already almost critical.
274
A. Feldmann et al.
s
e
g
a
t
n
e
c
r
e
P
5
.
0
4
.
0
3
.
0
2
.
0
1
.
0
0
.
0
0 pkts/s
2,000 pkts/s
10,000 pkts/s
0−0.1
0.1−0.2
0.2−0.3
Seconds
0.3−0.4
0.4−0.5
s
e
g
a
t
n
e
c
r
e
P
8
.
0
6
.
0
4
.
0
2
.
0
0
.
0
100 sessions, 5 updates/s, 0 pkts/s
100 sessions, 5 updates/s, 2,000 pkts/s
100 sessions, 5 updates/s, 5,000 pkts/s
250 sessions, 2 updates/s, 0 pkts/s
250 sessions, 2 updates/s, 2,000 pkts/s
0−0.2
0.2−0.4
0.4−1
1−4
4−10
10−40
40−88