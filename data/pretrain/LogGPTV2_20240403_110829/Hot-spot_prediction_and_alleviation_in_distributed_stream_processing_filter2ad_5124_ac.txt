### Figure 9: "Sort" Execution Time Prediction Accuracy

In this section, we explore the accuracy of our execution time prediction algorithm for various components. We also observe a correlation in the rate between different components, such as between sort and project, or between aggregate and count. This indicates the importance of cross-correlation between different components in the application component graph, which we leverage along with auto-correlation to predict component rates.

### Execution Time Prediction Accuracy

In our second set of experiments, we investigated the accuracy of our execution time prediction algorithm, as described in Section 3.2. Figures 9, 10, 11, 12, and 13 compare the predicted and actual execution times for the same set of components used in the rate prediction accuracy experiment. As detailed in Section 3.2, the predictions are based on the sum of rates being processed by the node hosting each component. Each component was hosted on a different node. The predicted execution time closely follows the measured execution time. In cases where the prediction is very inaccurate, we use the estimated standard error of the linear regression, as described in Section 3.2, to report the currently monitored value instead of the inaccurate predicted future execution time.

### Execution Time Distribution

In our third set of experiments, we examined the relationship between the execution time of individual application components and the total rate for all applications being processed by each node hosting a component, as shown in Figures 14 and 15 (similar figures for other components are omitted due to space constraints). This allowed us to determine the accuracy of assuming a linear relationship between the two, which forms the basis of our linear regression-based execution time prediction algorithm described in Section 3.2. We observed that the relationship can be approximated by a line, excluding a few outliers. However, this linear relationship is most evident when the total rate in the node is significant. If the node is lightly loaded, no significant queuing delays occur, and therefore, no significant variations in the execution time take place.

### Prediction Parameters

In our fourth set of experiments, we explored various parameters related to the prediction overhead and performance. Figure 16 shows how rate prediction accuracy is affected when reducing the prediction frequency. Reducing the prediction frequency can enable the system to handle high rates by avoiding the prediction overhead for every data tuple. We present the effect on prediction accuracy for different components when predicting the rate for every 1, every 50, every 75, and every 100 incoming data tuples. We observed that even by reducing the prediction overhead by a factor of 75, the prediction accuracy only drops by an average of 8.775%, ranging from 2.0% for sort to 14.375% for project.

Table 17 provides the average rate prediction error for different application components, offering a clear overview of the prediction accuracy. Despite some variation depending on the component semantics, the average prediction error is kept at 3.7016%. Table 18 shows the overhead in processing time for rate, execution time, and load prediction. The average overhead is 0.5984 ms, making our algorithms suitable for online prediction.

### Application Performance

In our fifth set of experiments, we investigated the application benefits gained from our hot-spot prediction and alleviation mechanisms. Figure 19 shows the improvement in application QoS achieved by predicting and alleviating application hot-spots using migration. The QoS metric displayed is the miss rate, defined as the number of data tuples that missed their QoS deadline over the total number of data tuples produced by the source. The miss rate is displayed as a function of the system load. We inject additional load into the system by increasing the number of application component graphs each node requests from 1 to 10. When the system is underloaded, not many application hot-spots occur, and their alleviation does not offer significant QoS advantages. However, as the system load increases, the miss rate increases drastically if hot-spots are not handled. Hot-spot elimination controls this increase.

Figure 20 demonstrates the benefit of hot-spot prediction and alleviation for application performance. The performance metric displayed is the end-to-end application delay. Note that this delay is calculated only for the data tuples that did not miss their deadlines, as those that missed their deadlines are dropped by local schedulers before reaching the receiver. While hot-spot prediction and alleviation enable the delivery of more data tuples as the load increases, it also maintains a lower average application end-to-end delay.

Figure 21 illustrates how migration affects the performance of a particular application. For a load of 10 application requests per node, we show the end-to-end delay attained by delivered data tuples of one application. Approximately at data tuple #500, an application hot-spot occurs, resulting in an increase in the end-to-end delay. Our hot-spot elimination mechanism kicks in and decreases the end-to-end delay through migration around data tuple #1200. It is important to note that only the data tuples that were delivered within the applicationâ€™s QoS requirements are shown. As the application end-to-end delay increases, we observe a reduction in the number of delivered data tuples. After the hot-spot has been eliminated, the number of data tuples that miss their deadline decreases, and more points can be seen in the graph.

### Figure 16: Rate Prediction Accuracy vs. Prediction Frequency

This figure shows the rate prediction accuracy versus prediction frequency. We observe the impact of reducing the prediction frequency on the accuracy for different components, such as sort, project, aggregate, count, and compare. The prediction frequency (sampling fraction) is plotted against the prediction accuracy.