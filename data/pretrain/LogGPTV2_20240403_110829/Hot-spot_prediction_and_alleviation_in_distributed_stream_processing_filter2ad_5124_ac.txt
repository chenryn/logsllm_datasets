Figure 9. “Sort” execution
time prediction accuracy.
count, and compare. Another interesting observation is the
correlation in the rate between different components, for ex-
ample between sort and project, or between aggregate and
count. This indicates the signiﬁcance of cross-correlation
between different components in the application component
graph, which we exploit in addition to auto-correlation to
predict component rates.
Execution Time Prediction Accuracy. In our second
set of experiments we investigated the accuracy of our exe-
cution time prediction algorithm described in Section 3.2.
Figures 9, 10, 11, 12, and 13 compare the predicted to
the actual execution time for the same set of components
as in the rate prediction accuracy experiment. As was de-
scribed in Section 3.2, the predictions are based on the sum
of rates being processed by the node hosting each compo-
nent. Note, that each component was hosted on a different
node. The predicted execution time follows the execution
time we measure. Cases where the prediction is very in-
accurate are detected using the estimated standard error of
the linear regression, as was described in section 3.2. This
way, instead of the inaccurate predicted future execution
time value the currently monitored value is reported.
Execution Time Distribution. In our third set of exper-
iments we investigated the relationship between the execu-
tion time of the individual application components and the
total rate for all applications being processed by each node
hosting a component, shown in Figures 14, and 15 (similar
Figures for the rest of the components are omitted due to
lack of space). This enabled us to determine the accuracy
of assuming a linear relationship between the two, which
formed the basis of our linear regression-based execution
time prediction algorithm described in Section 3.2. We ob-
serve that the relationship can be approximated by a line,
excluding a few outliers. However this linear relationship
is most evident when the total rate in the node is signiﬁ-
cant. If the node is lightly loaded, no signiﬁcant queueing
delays occur and therefore no signiﬁcant variations in the
execution time take place.
Prediction Parameters. In our fourth set of experiments
we investigated various parameters regarding the prediction
overhead and performance. In Figure 16 we show how rate
prediction accuracy is affected when reducing the predic-
tion frequency. Reducing the prediction frequency can en-
able the system to handle high rates, by avoiding the pre-
diction overhead for every data tuple. We present the effect
on prediction accuracy for the different components, when
predicting the rate for every 1, every 50, every 75, and every
100 incoming data tuples. We observe that even by reduc-
ing the prediction overhead by a factor of 75, the prediction
accuracy only drops by 8.775% on average, ranging from
2.0% for sort, to 14.375% for project.
Table 17 shows the average rate prediction error for the
)
s
m
i
(
e
m
T
n
o
i
t
u
c
e
x
E
Predicted vs Measured Execution Time for Project
Predicted vs Measured Execution Time for Aggregate
Predicted vs Measured Execution Time for Count
 120
 100
 80
 60
 40
 20
 0
 0
Measured Execution Time
Predicted Execution Time
)
s
m
i
(
e
m
T
n
o
i
t
u
c
e
x
E
 20
 40
 60
 80
 100
 120
 140
 160
Time (s)
 70
 60
 50
 40
 30
 20
 10
 0
 0
Measured Execution Time
Predicted Execution Time
)
s
m
i
(
e
m
T
n
o
i
t
u
c
e
x
E
 20
 40
 60
 80
 100
 120
 140
 160
Time (s)
 60
 50
 40
 30
 20
 10
 0
 0
Measured Execution Time
Predicted Execution Time
 20
 40
 60
 80
 100
 120
 140
 160
Time (s)
Figure 10. “Project” execu-
tion time prediction accu-
racy.
Figure 11. “Aggregate” exe-
cution time prediction accu-
racy.
Figure 12. “Count” execu-
tion time prediction accu-
racy.
)
s
m
i
(
e
m
T
n
o
i
t
u
c
e
x
E
 30
 25
 20
 15
 10
 5
 0
Predicted vs Measured Execution Time for Compare
Measured Execution Time
Predicted Execution Time
 100
)
s
m
(
i
e
m
T
n
o
i
t
u
c
e
x
E
t
n
e
n
o
p
m
o
C
n
o
i
t
a
c
i
l
p
p
A
 0
 20
 40
 60
 80
 100
 120
 140
 160
Time (s)
 80
 60
 40
 20
 0
 0
Execution Time Distribution for Sort
Execution Time Distribution for Project
 100
)
s
m
(
i
e
m
T
n
o
i
t
u
c
e
x
E
t
n
e
n
o
p
m
o
C
n
o
i
t
a
c
 80
 60
 40
 20
 5
 10
 15
 20
 25
 30
 35
 40
Total Rate in Node (kbps)
i
l
p
p
A
 0
 0
 10
 20
 30
 40
 50
Total Rate in Node (kbps)
Figure 13. “Compare” exe-
cution time prediction accu-
racy.
Figure 14. Execution time
distribution for “sort”.
Figure 15. Execution time
distribution for “project”.
different application components. This provides a clear
overview of the prediction accuracy. Even though some
variation depending on the component semantics exists, the
average prediction error is kept at 3.7016%. Table 18 shows
the overhead in processing time for rate, execution time, and
load prediction. The average overhead is 0.5984ms, which
makes our algorithms suitable for online prediction.
Application Performance.
In our ﬁfth set of experi-
ments we investigated the application beneﬁts gained from
our hot-spot prediction and alleviation mechanisms. Fig-
ure 19 shows the improvement in application QoS achieved
by predicting application hot-spots and alleviating them us-
ing migration. The QoS metric displayed is the miss rate,
deﬁned as the number of data tuples that missed their QoS
deadline, over the total number of data tuples that were pro-
duced by the source. The miss rate is displayed as a function
of the system load. We inject additional load in the system
by increasing the number of application component graphs
each node requests from 1 to 10. When the system is un-
derloaded not many application hot-spots occur and there-
fore their alleviation does not offer signiﬁcant QoS advan-
tages. However, as the system load increases, the miss rate
increases drastically when hot-spots are not handled. Ap-
plication hot-spot elimination controls this increase.
Figure 20 shows the beneﬁt of hot-spot prediction and al-
leviation for the application performance. The performance
metric displayed is the end-to-end application delay. Note
that this delay is calculated only for the data tuples that did
not miss their deadlines, as the ones that missed their dead-
lines are dropped by the local schedulers before reaching the
receiver. While hot-spot prediction and alleviation enables
the delivery of more data tuples as the load increases, it also
maintains a lower average application end-to-end delay.
Figure 21 shows how a migration affects the perfor-
mance of a particular application. For a load of 10 ap-
plication requests per node, we show the end-to-end delay
attained by delivered data tuples of one application. Ap-
proximately at data tuple #500 an application hot-spot oc-
curs, resulting to an increase in the end-to-end delay. Our
hot-spot elimination mechanism kicks in and decreases the
end-to-end delay through migration approximately at data
tuple #1200. It is also important to note that only the data
tuples that were delivered within the application’s QoS re-
quirements are shown. As the application end-to-end delay
increases, we can clearly observe a reduction in the number
of delivered data tuples. After the hot-spot has been elim-
inated, the number of data tuples that miss their deadline
decreases again and more points can be seen in the graph.
 30
 25
 20
 15
 10
 5
 0
 0.01
Prediction Accuracy vs Frequency
Sort
Project
Aggregate
Count
Compare
Prediction Frequency (Sampling Fraction)
 0.1
 1
Figure 16. Rate prediction
accuracy versus prediction
frequency.
Application Performance Improvement
Without Hot-Spot Elimination
With Hot-Spot Elimination
 2.5
 2
 1.5
 1
 0.5
 0
 1
 2
 3
 5
 4
 7