### Optimized Text

The overhead for peering can be reduced to that of multihoming by employing the Bloom filter optimization discussed in Section 4.2, although this comes with the trade-off of increased per-router state requirements. Interestingly, the cost of a multi-homed join is not significantly higher than that of a single-homed join. This is because, despite the typical presence of 75-100 Autonomous Systems (ASes) in an AS's up-hierarchy, the number of unique successors is generally much smaller. We leveraged this observation to optimize the multi-homed join by eliminating redundant lookups that resolve to the same successor.

We extrapolated these results to an Internet-scale system with 600 million IDs and estimated that an ephemeral join requires approximately 14 messages, a single-homed join requires around 80 messages, and a multi-homed join requires about 100 messages. It is important to note that these control messages are more lightweight than those used in traditional routing protocols, as intermediate routers do not need to process them in their slow-paths. The use of the Bloom filter optimization further reduces the overhead of the peering join to match that of the recursively multi-homed join.

The state at hosting routers increases with the number of hosts and the number of fingers each host maintains. With 600 million IDs, each maintaining 256 fingers, we found that an average of 184 Mbits per AS is required to store the hosting state.

**Stretch:**
Figure 8b shows the cumulative distribution function (CDF) of data packet stretch for single-homed joins. Stretch decreases as the number of proximity-based fingers increases. For instance, with 60 fingers, ROFL’s average stretch is 2.8, while it decreases to 2.3 with 160 fingers. If hosts perform a join across peering links, the stretch increases to 2.8 for 160 fingers. As the number of IDs in the system increases, the stretch decreases slightly, due to the uneven distribution of hosts across ASes in the Internet. In an Internet-scale graph with 600 million IDs, 128 fingers (with a peering join overhead of 200) result in a stretch of around 2.9, and 340 fingers (with a peering join overhead of 445) result in a stretch of around 2.5. However, increasing the number of fingers also increases the size of the join messages. For example, with 256 fingers, the message size increases to 1638 bytes, requiring 258 IP packets for a single-homed join, assuming an MTU of 1500 bytes.

Although a stretch of 2-3 seems high, it only affects the first packet. The stretch for subsequent packets can be reduced to one by exchanging the list of ASes above the destination in the hierarchy (Section 5.1) or by caching the destination’s AS. For comparison, Figure 8b also shows the stretch incurred by BGP policies, measured using Routeviews traces. Additionally, the isolation property significantly reduces stretch, as verified through consistency checks in our simulator. Figure 8c demonstrates that pointer caching (Section 4.1) further reduces stretch. An average pointer cache size of 20M entries per AS reduces the stretch from 2 to 1.33, which is feasible given that modern routers can support millions of entries. Using Bloom filters for peering, as described in Section 4.2, results in a stretch of 3.29 with a state size of 18 Mbits/AS, though this can be reduced to 2.5 with more fingers or larger 74 Mbit Bloom filters.

**Failures:**
Stub ASes, which are near the network edge, are believed to be significantly more unstable than ISPs near the core [14]. In this experiment, we randomly fail stub ASes and measure two metrics: the number of paths affected by the failure and the number of messages required to repair successors. On average, 99.998% of Internet paths were unaffected by the failure, indicating that the effects of failures were well-contained. ROFL required an average of 4950 messages to repair successors after a stub AS failure, corresponding roughly to the number of identifiers hosted in the failed stub AS.

### Summary of Results

**Intradomain:**
Based on Rocketfuel traces, we simulated ROFL over four ISPs, ranging in size from 201 to 604 internal routers. ROFL provides a routing stretch of 1.2 to 2 with 9 Mbits of pointer cache, achieving reasonable load balance across routers. Hosts typically complete joining in less than 40ms, generating fewer than 45 control messages per host. ROFL correctly heals from partitions, host failures, and host mobility with control overhead similar to rejoining the affected hosts.

**Interdomain:**
We extrapolated our simulation results to an Internet-scale system with 600 million hosts. A ROFL host can join across all providers and peers and acquire 340 fingers with approximately 445 control messages. This overhead can be reduced for unstable hosts by performing a single-homed join (approximately 75 messages) or an ephemeral join (approximately 14 messages). The host can route packets in a manner that respects several inter-AS policies, with an average stretch of 2.5. This stretch can be reduced to 2.1 by doubling the number of fingers. By maintaining pointer caches at border routers, the stretch can be further reduced to 1.33 with an average of 20 million entries of caching space per AS. Finally, ASes can reduce join overhead by leveraging Bloom filters to eliminate joins across peering links, reducing the join overhead to approximately 100 messages but requiring 74 Mbits of Bloom filter state per AS.

### Related Work and Discussion

Our detailed mechanism draws from multiple sources, particularly VRR [7] for intradomain design and Canon [17] for interdomain design. While VRR was designed for ad-hoc routing, we extended it by introducing a simplified path construction/maintenance protocol, a protocol to ensure correctness in the presence of network partitions, and several approaches to improve scalability and resilience to churn. Similarly, we extended Canon to support several Internet policies and leverage proximity-based fingers to reduce stretch.

The project most closely aligned with our design objectives is TRIAD [10], which routes on URLs by mapping URLs to next-hops. TRIAD performs content routing at gateways (firewalls/NATs) between realms and BGP-level routers between ASes, relying on aggregation to scale. However, it will fail if object locations do not follow the DNS hierarchy closely. If name-level redirection mechanisms are used to handle hosts whose names do not match the network topology, it becomes essentially a resolution mechanism. This also applies to IPNL, which routes on Fully Qualified Domain Names (FQDNs).

These previous attempts in the name-routing arena highlight both the challenges and the benefits. Routing on names brings several architectural advantages, as mentioned in the Introduction, and breaks out of a long-standing architectural mindset. Our goal is to explore whether these boundaries can be expanded, rather than seeking grace.

Our design includes multiple delivery models, a fair amount of policy control, and some traffic control. The remaining question is performance. While the results are promising, they are not yet fully satisfactory. This work is just the beginning.

### References

[1] I. Abraham, A. Badola, D. Bickson, D. Malkhi, S. Maloo, S. Ron, “Practical locality-awareness for large scale information sharing,” IPTPS, February 2005.
...
[48] “Route Views Project,” http://www.routeviews.org.