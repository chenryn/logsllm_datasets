attack dictionaries (one for each image), under various
sets of these click-order pattern constraints to determine
their success rates and dictionary sizes. This method
only initiates the exploration of other ways that click-
based graphical passwords could be analyzed for patterns
in user choice. We expect this general direction will yield
other results, including patterns due to mnemonic strate-
gies (e.g., clicking all red objects).
The results shown in Table 5 indicate that, on aver-
age for the pool image, using only the diagonal con-
straint will reduce the dictionary size to 16 bits, while
still cracking 12% of passwords. Similarly, for the cars
image, using only this constraint will reduce the dictio-
nary to 18 bits, while still cracking 10% of passwords.
The success rate of our human-seeded attack is compa-
rable to recent results on cracking text-based passwords
[23], where 6% of passwords were cracked with a 1.2
million entry dictionary (almost 2 bits larger than our
DIAG dictionary based on harvested points of 15 users
for cars, and 4 bits larger for DIAG based on 15 users
for pool). Furthermore, unlike most text dictionaries, we
USENIX Association
16th USENIX Security Symposium
111
Dictionary
m bitsize
C R
C R
20,longterm 100
50
10,longterm
33.1
27.9
cars
pool
# passwords
guessed
29/89 (33%)
23/99 (23%)
m bitsize
100
50
33.1
27.9
# passwords
guessed
52/94 (55%)
22/104 (21%)
Table 4: Dictionary attack results, using the ﬁrst 20 and 10 users from the long term study to seed an attack against the others. m
is the alphabet size. See text for descriptions of C V and C R.
Click-order pattern
15 (with no pattern)
C V
LR, RL, CW, CCW, TB, BT
LR, RL
TB,BT
CW, CCW
DIAG
cars image
# passwords
guessed of 109
13 (12%)
12 (11%)
11 (10%)
12 (11%)
0 (0%)
11 (10%)
dictionary
size (bits)
29.2
25.6
23.8
24.4
24.0
18.4
pool image
# passwords
guessed of 114
22 (19%)
22 (19%)
19 (17%)
15 (13%)
4 (4%)
14 (12%)
dictionary
size (bits)
27.1
23.4
22.0
21.9
21.7
16.2
Table 5: Effect of incorporating click-order patterns on dictionary size and success, as applied to a representative dictionary
of clusters gathered from 15 users. Results indicate that the DIAG pattern produces the smallest dictionary, and still guesses a
relatively large number of passwords.
do not need to store the entire dictionary as it is generated
on-the-ﬂy from the alphabet. At best, this indicates that
these graphical passwords are slightly less secure than
the text-based passwords they have been proposed to re-
place. However, the reality is likely worse. The anal-
ogy to our attack is collecting text passwords from 15
users, and generating a dictionary based on all permuta-
tions of the characters harvested, and ﬁnding it generated
a successful attack. The reason most text password dic-
tionaries succeed is due to known dependent patterns in
language (e.g., using di or tri-grams in a Markov model
[29]). The obvious analogy to this method has not been
yet attempted, but would be another method of further
reducing the dictionary size.
5
Purely Automated Attacks Using Image
Processing Tools
Here we investigate the feasibility of creating an at-
tack dictionary for click-based graphical passwords by
purely automated means. Pure automation would side-
step the need for human-seeding (in the form of harvest-
ing points), and thus should be easier for an attacker to
launch than the attacks presented in Section 4. We create
this attack dictionary by modelling user choice using a
set of image processing methods and tools. The idea is
that these methods may help predict hot-spots by auto-
mated means, leading to more efﬁcient search orderings
for exhaustive attacks. This could be used for modeling
attackers constructing attack dictionaries, and proactive
password checking.
5.1
Identifying Candidate Click-Points
We begin by identifying details of the user task in cre-
ating a click-based graphical password. The user must
choose a set of points (in a speciﬁc order) that can be re-
membered in the future. We do not focus on mnemonic
strategies for these automated dictionaries (although they
could likely be improved using the click-order patterns
from Section 4.2), but rather the basic features of a point
that deﬁne candidate click-points. To this end, we iden-
tify a candidate click-point to be a point which is: (1)
identiﬁable with precision within the system’s error tol-
erance; and (2) distinguishable from its surroundings,
i.e., easily picked out from the background. Regarding
(1), as an example, the pool image has a red garbage can
that is larger than the 19 × 19 error tolerance; to choose
the red garbage can, a user must pick a speciﬁc part of it
that can be navigated to again (on a later occasion) with
precision, such as the left handle. Regarding (2), as an
example, it is much easier to ﬁnd a white logo on a black
hat than a brown logo on a green camouﬂage hat.
For modelling purposes, we hypothesize that the fewer
candidate click-points (as deﬁned above) that an image
has, the easier it is to attack. We estimate candidate click-
points by implementing a variation of Itti et al.’s bottom-
up model of visual attention (VA) [17], and combining it
with Harris corner detection [16].
112
16th USENIX Security Symposium
USENIX Association
Corner detection picks out the areas of an image that
have variations of intensity in horizontal and vertical di-
rections; thus we expect it should provide a reasonable
measure of whether a point is identiﬁable.
Itti et al.’s
VA determines areas that stand out from their surround-
ings, and thus we expect it should provide a reasonable
measure of a point’s distinguishability. Brieﬂy, VA cal-
culates a saliency map of the image based on 3 channels
(color, intensity, and orientation) over multiple scales.
The saliency map is a grayscale image whose brighter
areas (i.e., those with higher intensity values) represent
more conspicuous locations. A viewer’s focus of atten-
tion should theoretically move from the most conspicu-
ous locations (represented by the highest intensity areas
on the saliency map) to the least. We assume that users
are more likely to choose click-points from areas which
draw their visual attention.
We implemented a variation of VA and combined it
with Harris corner detection to obtain a prioritized list
of candidate click-points (CCP-list) as follows. (1) Cal-
culate a VA saliency map (see Fig. 5(b)) using slightly
smaller scales than Itti et al. [17] (to reﬂect our interest
in smaller image details). The higher-intensity pixel val-
ues of the saliency map reﬂect the most “conspicuous”
(and distinguishable) areas. (2) Calculate the corner lo-
cations using the Harris corner detection function as im-
plemented by Kovesi [22]4 (see Fig. 5(c)). (3) Use the
corner locations as a bitmask for the saliency map, pro-
ducing what we call a cornered saliency map (CSM). (4)
Compute an ordered CCP-list of the highest to lowest
intensity-valued CSM points. Similar to the focus-of-
attention inhibitors used by Itti et al., we inhibit a CSM
point (and its surrounding tolerance) once it has been
added to the CCP-list so it is not chosen again (see Fig.
5(d)). The CCP-list is at least as long as the alphabet size
(414), but is a prioritized list, ranking points from (the
hypothesized) most to least likely.
5.2 Model Results
We evaluated the performance of the CCP-list as a model
of user choice using the data from both the lab and ﬁeld
user studies. We ﬁrst examined how well the ﬁrst half
(top 207) of the CCP-list overlaps with the observed
high-probability clusters from our lab user study (i.e.,
those clusters of size at least 5). We found that this half-
alphabet found all high-probability clusters on the icons,
faces, and cars images, and most of the high-probability
clusters on 11 of the 17 images. Most of the images that
our model performed poorly on appeared to be due to the
saliency map algorithm being overloaded with too much
detail (pcb, citymap-gr, paperclips, smarties, and truck
images). The other image on which this approach did
not perform well (mural) appears to be due to the cor-
ner masking in step (3); the high probability points were
centroids of circles.
To evaluate how well the CCP-list works at modelling
users’ entire passwords (rather than just a subset of click-
points within a password), we used the top ranked one-
third of the CCP-list values (i.e., the top 138 points for
each image) to build a graphical dictionary and carry out
a dictionary attack against the observed passwords from
both user studies (i.e., on all 17 images in the lab study,
and the cars and pool images again in the ﬁeld study).
We found that for some images, this 35-bit dictionary
was able to guess a large number of user passwords (30%
for the icons image and 29% for the philadelphia map
image). For both short and long-term studies, our tool
guessed 9.1% of passwords for the cars image. A 28-
bit computer-generated dictionary (built from the top 51
ranked CCP-list alphabet) correctly guessed 8 passwords
(22%) from the icons image and 6 passwords (17%) from
the philadelphia image. Results of this automated graph-
ical dictionary attack are summarized in Table 6.
Image
1. paperclips
2. cdcovers
3. philadelphia
4. toys
5. bee
6. faces
7. citymap-nl
8. icons
9. smarties
10. cars
11. pcb
12. citymap-gr
13. pool
14. mural
15. corinthian
16. truck
17. tea
passwords
guessed
(lab study)
2/36 (5.5%)
2/35 (5.7%)
10/35 (28.6%)
2/39 (5.1%)
1/40 (2.5%)
0/32 (0.0%)
1/34 (2.9%)
11/37 (29.7%)
5/37 (13.5%)
3/33 (9.1%)
3/36 (8.3%)
0/34 (0.0%)
1/35 (2.9%)
1/36 (2.8%)
3/35 (8.6%)
1/35 (2.9%)
2/38 (5.3%)
passwords
guessed
(ﬁeld study)
–
–
–
–
–
–
–
–
–
10/109 (9.1%)
–
–
2/114 (0.9%)
–
–
–
–
Table 6: Passwords correctly guessed (using a 35-bit dictio-
nary based on a CCP-list). The number of target passwords is
different for most images (32 to 40 for the lab study).
Figure 6 shows that the CCP-list does a good job of
modelling observed user choices for some images, but
not all images. This implies that on some images, an at-
tacker performing an automated attack is likely to be able
to signiﬁcantly cut down his search space. This method
also seems to perform well on the images for which the
visual attention model made more deﬁnite decisions – the
USENIX Association
16th USENIX Security Symposium
113
(a) Original image [46].
(b) Saliency map.
(c) Corner detection output.
(d) Cornered saliency map (CSM) after top 51 CCP-list
points have been inhibited.
Figure 5: Illustration of our method of creating a CCP-list (best viewed electronically).
saliency map shows a smaller number of areas standing
out, as indicated visually by a generally darker saliency
map with a few high-intensity (white) areas. An attacker
interested in any one of a set of accounts could go after
accounts using a background image that the visual atten-
tion model performed well on.
In essence, this method achieves a reduction (by leav-
ing out some “unlikely” points) from a 43-bit full pass-
word space to a 35-bit dictionary. The 43-bit full pass-
word space is the proper base for comparison here, since
an actual attacker with no a priori knowledge must con-
sider all T-regions in an image. However, we believe
this model of candidate click-points could be improved
through a few methods. The images that the model per-
formed poorly on appeared to be due to failure in cre-
ating a useful visual attention model saliency map. The
saliency maps seem to fail when there are no areas that
stand out from their surroundings in the channels used in
saliency map construction (color, intensity, and orienta-
tion). Further, centroids of objects that “stand out” to a
user will not be included in this model (as only corners
are included); adding object centroids to the bitmask is
thus an avenue for improvement.
6 Related Work
In the absence of password rules, practical text password
security is understood to be weak due to common pat-
terns in user choice. In a dated but still often cited study,
Klein [21] determined a dictionary of 3 million words
(less than 1 billionth of the entire 8-character password
space) correctly guessed over 25% of passwords. Auto-
mated password cracking tools and dictionaries that ex-
ploit common patterns in user choice include Crack [28]
and John the Ripper [30]. More recently, Kuo et al. [23]
found John the Ripper’s English dictionary of 1.2 mil-
lion words correctly guessed 6% of user passwords, and
an additional 5% by also including simple permutations.
In response to this well-known threat, methods to cre-
ate less predictable passwords have emerged. Yan [48]
114
16th USENIX Security Symposium
USENIX Association
explores the use of passphrases to avoid password dic-
tionary attacks. Jeyaraman et al. [20] suggest basing a
passphrase upon an automated newspaper headline. In
theory, creating passwords using these techniques should
leave passwords less vulnerable to automated password
cracking dictionaries and tools, although Kuo et al. [23]
show this may not be the case. Proactive password
checking techniques (e.g., [38, 7, 2]) are commonly used
to help prevent users from choosing weak passwords.
Many variations of graphical passwords are discussed
in surveys by Suo et al. [39] and Monrose et al. [27]. We
discuss two general categories of graphical passwords:
recognition-based and recall-based.
In the interest of
brevity, we focus on the areas closest to our work: click-
based graphical passwords, and practical security analy-
ses of user authentication methods.
Typical recognition-based graphical passwords re-
quire the user to recognize a set (or subset) of K pre-
viously memorized images. For example, the user is pre-
sented a set of N (> K) images from which they must
distinguish a subset of their K images. The user may
be presented many panels of images before providing
enough information to login. Examples are Déjà Vu [10],
which uses random art images created by hash visualiza-
tion [32]; Passfaces [35], whereby the set of images are
all human faces; and Story [8], whereby the images are
from various photo categories (e.g., everyday objects, lo-
cations, food, and people), with users encouraged to cre-
ate a story as a mnemonic strategy. In the cognitive au-
thentication scheme of Weinshall [44], a user computes
a path through a grid of images based on the locations of
those from K. The end of the path provides a number for
the user to type, which was thought to protect the values
of K from observers; Golle et al. [14] show otherwise.
Recall-based schemes can be further described as cued
or uncued. An uncued scheme does not provide the
user any information from which to create their graphical