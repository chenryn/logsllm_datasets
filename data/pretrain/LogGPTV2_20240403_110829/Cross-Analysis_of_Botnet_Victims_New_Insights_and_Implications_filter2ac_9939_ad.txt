two items is the sum of all feature value diﬀerences for each of the all features
in the item, and it is frequently used to denote whether two data distributions
are similar or not (e.g., if a distance between data distributions of A and B is
smaller than between that of A and C, A and B are closer to each other than
C). It can be formalized as the following equation (assuming that there are two
items/distributions of x and y, and they both have n elements).
∑
Manhattan Distance =
jxi (cid:0) yij
n
i=1
We use the probability distributions of infected networks of Conﬁcker, MegaD,
Srizbi over IP address spaces to measure the manhattan distance and we ﬁnd
that the manhattan distance between Conﬁcker and MegaD is 1.1427, Conﬁcker
and Srizbi is 1.1604, and MegaD and Srizbi is 0.8404. From the results, we can
easily see that the distance between the Type I and Type II botnet distributions
is larger than the distance between the two type II botnets distributions. This
result shows that the distributions of infected networks with the same infection
type are closer to each other than that of diﬀerent types of botnet (i.e., infected
networks of botnets in the same type show very similar distribution patterns).
Cross-Analysis of Botnet Victims: New Insights and Implications
17
This result gives us another insight that if two botnets share the same infec-
tion vectors (i.e., they are of the same type), we might predict unknown infected
networks of one botnet (e.g., a future botnet) with the help of the information of
the other botnet (e.g., historic data). This insight can be veriﬁed with a similar
test that we have done before. We can perform a test by simply changing the
training and testing data set to cross botnets. In the previous test, we extract
the training and testing data from the same botnet. However in this case, we
use data from botnet A for training and data from botnet B for testing. For
example, when we predict (unknown) infected networks of the Srizbi botnet, we
use data of the MegaD botnet for training.
Botnet
MegaD(train), Srizbi(test)
Srizbi(train), MegaD(test)
K Prediction Accuracy False Positive Rate
1 87.80%
3 86.75%
5 86.45%
1 84.09%
3 83.89%
5 83.65%
7.41%
7.49%
7.69%
6.53%
6.31%
5.09%
Table 4. Botnet cross-prediction results.
The cross-prediction results are quite surprising. As denoted in Table 4, this
approach can predict unknown infected networks of the other botnet with more
than 83% accuracy. This prediction accuracy is slightly less than what we ob-
served previously. We believe that these results show us that even if we have
no knowledge of some botnets (e.g., a future emerging botnet), if we have some
information of a botnet whose infection vector is very similar to them4, we may
be able to predict unknown infected networks. To show a realistic example of
application of the neighborhood correlation, let us ﬁrst assume that a network
administrator knows historic infected networks by Srizbi botnets. Then, he gets
to know that the MegaD botnet starts spreading but he does not have any in-
formation of which networks are and will be infected. In this case, he can use
the information of Srizbi botnet information (e.g., victim distribution). Based on
the physical location and IP address of victims of Srizbi, he can predict future
victim networks that will possibly be infected by MegaD with a reasonably high
probability.
5 Limitations and Discussions
Like any measurement/analysis work, our empirical study has some limitations
or biases. Even though we have collected a large amount of Conﬁcker botnet
4 Note that this is a very reasonable assumption because fundamental infection types
of botnets are very limited and do not change frequently.
18
Seungwon Shin, Raymond Lin, and Guofei Gu
data, we have a relatively smaller amount of data for the MegaD and Srizbi
botnets. This might cause some bias in our measurement results and subse-
quent analysis. In addition, the dynamism of IP addresses may lead to some
over-estimation from the collected data. To reduce some of the side eﬀects, we
generalize our analysis over a network consisting of several adjacent IP addresses
(i.e., measuring/analyzing over /24 subnets instead of each individual host).
To discover interesting insights, we leverage some previous work. For example,
we use previous work to obtain how dynamic IP addresses are distributed over
countries, but the information is not complete, i.e., it does not cover all countries.
However, the provided information may help to uncover interesting cases (e.g.,
countries which are highly infected by botnets), hence the information is still
useful.
When we perform the test to ﬁnd networks with dynamic IP addresses
through looking up reverse DNS PTR records of hosts in the networks, we may
not collect reverse PTR records from all hosts because registration of a reverse
PTR record is not always necessary. However previous work already veriﬁed the
feasibility of such kind of test [23], lending credibility to these results (at least
providing a good low-bound estimation).
6 Related Work
There are several studies of measurement or analysis of the Type I botnet vic-
tims. CAIDA provides basic information about the victim distribution of the
Conﬁcker botnet in terms of their IP address space and physical location [14]. In
[13], Krishnan et al. conducted an experiment to detect infected hosts by Con-
ﬁcker. Weaver [15] built a probabilistic model to understand how the Conﬁcker
botnet spreads via network scanning. These studies provided useful and interest-
ing analysis of the Conﬁcker botnet. Shin et al. provided a large scale empirical
analysis of the Conﬁcker botnet and presented how victims are distributed [2].
However, our work is diﬀerent from them in that we perform cross-analysis of dif-
ferent botnets and propose an early warning approach based on cross-prediction.
Even though [2] observed neighbor correlation in Conﬁcker, this work diﬀers in
that we empirically veriﬁed similar neighborhood correlation in Type II botnets.
In addition, we have proposed and veriﬁed cross-botnet prediction techniques to
predict unknown victims of one botnet from the information of the other botnet
if they have similar infection vectors.
Measurement studies of the Type II botnet were also conducted. In [6], Mori
et al. performed a large scale empirical study of the Srizbi botnet. John et al.
set up a spam trap server to capture botnets sending spam emails [16]. This
work also showed the distribution of victims in terms of their IP addresses. Even
though these studies provided detailed analysis of some Type II botnet(s), they
still diﬀer from our work in that they concentrate on a single (or one type of)
speciﬁc botnet.
Some interesting studies from the analysis of Type II botnets have been also
proposed. In [17], Cho et al. analyzed the MegaD botnet and showed how it
Cross-Analysis of Botnet Victims: New Insights and Implications
19
works. Caballero et al. provided an interesting technique to inﬁltrate the MegaD
botnet and performed an analysis of its protocol [18].
Cai et al. measured how IP addresses are distributed over the world through
several interesting sampling techniques [23]. Our work leverages some of its re-
sults but is diﬀerent from their work in the main purpose.
7 Conclusion and Future Work
In this paper, we have collected a large amount of real-world botnet data and
performed cross-analysis between diﬀerent types of botnets to reveal the dif-
ferences/similarities between them. Our large scale cross-comparison analysis
results allow us to discover interesting ﬁndings and gain profound insights into
botnet victims. Our results show ﬁne-grained infection information and nature
of botnet victims. They show some interesting relationships between geopolitical
issues and malware infection, which might be the ﬁrst work shedding light on
this correlation. This study can guide us to design better botnet prediction or
defense systems.
In our future work, we will study new approaches to explain relationships
between geopolitical locations and malware infection more clearly with some
realistic examples. In addition, we will collect more botnet data and investigate
more diverse categories to discover correlations with diﬀerent malware infection
types.
References
1. Pauli, Darren: Srizbi Botnet Sets New Records for Spam: PC World. Retrieved
2008-07-20
2. Seungwon Shin and Guofei Gu: Conﬁcker and Beyond: A Large-Scale Empirical
Study. In: Proceedings of 2010 Annual Computer Security Applications Conference
(ACSAC’10) (2010)
3. Microsoft Security Techcenter, Conﬁcker Worm, http://technet.microsoft.com/
en-us/security/dd452420.aspx
4. UPI, Virus strikes 15 million PCs, http://www.upi.com/Top_News/2009/01/26/
Virus-strikes-15-million-PCs/UPI-19421232924206/
5. Symantec,
Trojan.Srizbi,
http://www.symantec.com/security_response/
writeup.jsp?docid=2007-062007-0946-99
6. McAfee,
Srizbi
Infection,
http://www.mcafee.com/threat-intelligence/
malware/default.aspx?id=142902
7. SecureWorks, Ozdok/Mega-D Trojan Analysis, http://www.secureworks.com/
research/threats/ozdok/?threat=ozdok
8. m86security, Mega-d, http://www.m86security.com/trace/i/Mega-D,spambot.
896.asp.
9. Eric Chien, Downadup: Attempts at Smart Network Scanning, http://www.
symantec.com/connect/blogs/downadup-attempts-smart-network-scanning
10. Yinglian Xie and Fang Yu and Kannan Achan and Eliot Gillum and Moises
Goldzmidt and Ted Wobber: How Dynamic are IP Addresses?: Proceedings of ACM
Special Interest Group on Data Communication (SIGCOMM) (2007)
20
Seungwon Shin, Raymond Lin, and Guofei Gu
11. Moheeb Abu Rajab and Jay Zarfoss and Fabian Monrose and Andreas Terzis: My
botnet is bigger than yours (maybe, better than yours): why size estimates remain
challenging: Proceedings of the ﬁrst conference on First Workshop on Hot Topics in
Understanding Botnets (2007)
12. Manuel Egele1 and Peter Wurzinger and Christopher Kruegel and Engin Kirda:
Defending Browsers against Drive-by Downloads: Mitigating Heap-spraying Code
Injection Attacks: Proceedings of the Sixth Conference on Detection of Intrusions
and Malware and Vulnerability Assessment (DIMVA) (2009)
13. Srinivasan Krishnan and Yongdae Kim: Passive identiﬁcation of Conﬁcker nodes
on the Internet: University of Minnesota - Technical Document (2009)
14. CAIDA, Conﬁcker/Conﬂicker/Downadup as seen from the UCSD Network Tele-
scope, http://www.caida.org/research/security/ms08-067/conficker.xml
15. Rhiannon Weaver: A Probabilistic Population Study of the Conﬁcker-C Botnet:
Proceedings of the Passive and Active Measurement Conference (2010)
16. John P. John and Alexander Moshchuk and Steven D. Gribble and Arvind Krish-
namurthy: Studying Spamming Botnets Using Botlab: Proceedings of the Annual
Network and Distributed System Security (NDSS) (2009)
17. Chia Yuan Cho and Juan Caballero and Chris Grier and Vern Paxson and Dawn
Song: Insights from the Inside: A View of Botnet Management from Inﬁltration: Pro-
ceedings of the USENIX Workshop on Large-Scale Exploits and Emergent Threats
(LEET) (2010)
18. Juan Caballero and Pongsin Poosankam and Christian Kreibich and Dawn Song:
Dispatcher: Enabling active botnet inﬁltration using automatic protocol reverse-
engineering: Proceedings of ACM Computer and Communications Security (CCS)
(2009)
19. BOTLAB, A Study in Spam, http://botlab.org/
20. Shadowserver, Botnet Measurement and Study, http://shadowserver.org/wiki/
21. IP2Location,
IP Address 2009 Report, http://www.
IP2Location Internet
ip2location.com/
22. IANA, IANA IPv4 Address Space Registry, http://www.iana.org/assignments/
ipv4-address-space/ipv4-address-space.xml
23. Xue Cai and John Heidenmann: Understanding Address Usage in the Visible In-
ternet: USC/ISI Technical Report ISI-TR-656 (2009)
24. Heather Alderfer and Stephen Flynn and Bryan Birchmeier and Emilie Schulz:
Information Policy Country Report: Turkey: University of Michigan School of In-
formation Report (2009)
25. Nicholas Ianelli and Aaron Hackworth: Botnets as a Vehicle for Online Crime:
CERT/CC Technical Report (2005)
26. Uri Raz, How do spammers harvest email addresses ?, http://www.private.org.
il/harvest.html
27. FAQs.org, FAQ: How do spammers get people’s email addresses ?, http://www.
faqs.org/faqs/net-abuse-faq/harvest/
28. Juan Caballero and Chris Grier and Christian Kreibich and Vern Paxson: Measur-
ing Pay-per-Install: The Commoditization of Malware Distribution: Proceedings of
USENIX Security Symposium (2011)