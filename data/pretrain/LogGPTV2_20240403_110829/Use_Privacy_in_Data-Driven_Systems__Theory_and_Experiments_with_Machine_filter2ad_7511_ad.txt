be equivalent regardless of the order of the expressions in that they
can be decomposed in exactly the same set of ways. If a proxy is dis-
tributed among x1 and x3, it will still be considered by our methods
because x1 + (x2 + x3) is equivalent to (x1 + x3) + x2, and the sub-
expression x1 + x3 is part of a valid decomposition. Allowing such
equivalences within the implementation of substitution partially
addresses the problem that our theory does not respect semantic
equivalence, which is a necessary consequence of Theorem 1.
4.2 Analyzing Proxy Use
Algorithm 1 describes a general technique for detecting (œµ, Œ¥)-proxy
use in expression programs. In addition to the parameters and ex-
pression, it takes as input a description of the distribution governing
the feature variables X and Z. In practice this will nearly always
consist of an empirical sample, but for the sake of presentation we
simplify here by assuming the distribution is explicitly given. In
Section E.2, we describe how the algorithm can produce estimates
from empirical samples.
The algorithm proceeds by enumerating sub-expressions of the
given program. For each sub-expression e appearing in p, ProxyDetect
computes the set of positions at which e appears. If e occurs multiple
times, we consider all possible subsets of occurrences as potential
decompositions1 . It then iterates over all combinations of these
positions, and creates a decomposition for each one to test for
(œµ, Œ¥)-proxy use. Whenever the provided thresholds are exceeded,
the decomposition is added to the return set. This proceeds until
there are no more subterms to consider. While not efficient in the
worst-case, this approach is both sound and complete with respect
to Definition 9.
Theorem 2 (Detection soundness). Any decomposition (p1, p2)
returned by ProxyDetect(p, X, œµ, Œ¥) is a decomposition of the input
program p and had to pass the œµ, Œ¥ thresholds, hence is a (œµ, Œ¥)-proxy
use.
Theorem 3 (Detection completeness). Every decomposition
which could be a (œµ, Œ¥)-proxy use is enumerated by the algorithm.
Thus, if (p1, p2) is a decomposition of p with Œπ(p1, p2) ‚â• d and
d((cid:74)p1(cid:75)(X), Z) ‚â• œµ, it will be returned by ProxyDetect(p, X, œµ, Œ¥).
1This occurs often in decision forests (see Figure 8).
Our detection algorithm considers single terms in its decompo-
sition. Sometimes a large number of syntactically different prox-
ies with weak influence might collectively have high influence. A
stronger notion of program decomposition that allows a collection
of multiple terms to be considered a proxy would identify such a
case of proxy use but will have to search over a larger space of
expressions. Exploring this tradeoff between scalability and richer
proxies is an important topic for future work.
The detection algorithm runs in timeùí™ (|p| c (|ùíü| + k |ùíü|)) where
|ùíü| is the size of a dataset employed in the analysis, c is the num-
ber of decompositions of a program, k is the maximum number of
elements in the ranges of all sub-programs (|ùíü| in the worst case),
and |p| is the number of sub-expressions of a program. The number
of decompositions varies from ùí™ (|p|) to ùí™(cid:16)2|p|(cid:17) depending on
the type of program analyzed. Details can be found in Appendix E
along with more refined bounds for several special cases.
5 REMOVING PROXY USE VIOLATIONS
In this section we present a repair algorithm for removing viola-
tions of (œµ, Œ¥)-Proxy Use in a model. Our approach has two parts:
first (Algorithm 2) is the iterative discovery of proxy uses via the
ProxyDetect procedure described in the previous section and sec-
ond (Algorithm 3) is the repair of the ones found by the oracle to
be violations. We describe these algorithms informally here, and
Appendix C contains formal descriptions of these algorithms. The
iterative discovery procedure guarantees that the returned program
is free of violations (Algorithm 5). Our repair procedures operate
on the expression language, so they can be applied to any model
that can be written in the language. Further, our violation repair
algorithm does not require knowledge of the training algorithm
that produced the model. The witnesses of proxy use localize where
in the program violations occur. To repair a violation we search
through expressions local to the violation, replacing the one which
has the least impact on the accuracy of the model that at the same
time reduces the association or influence of the violation to below
the (œµ, Œ¥) threshold.
At the core of our violation repair algorithm is the simplifica-
tion of sub-expressions in a model that are found to be violations.
Simplification here means the replacement of an expression that
is not a constant with one that is. Simplification has an impact on
the model‚Äôs performance hence we take into account the goal of
preserving utility of the machine learning program we repair. We
parameterize the procedure with a measure of utility v that informs
the selection of expressions and constants for simplification. We
briefly discuss options and implementations for this parameter later
in this section.
The repair procedure (Algorithm 3) works as follows. Given
a program p and a decomposition (p1, p2), it first finds the best
simplification to apply to p that would make (p1, p2) no longer a
violation. This is done by enumerating expressions that are local
to p1 in p2 (Line 3). Local expressions are sub-expressions of p1 as
well as p1 itself and if p1 is a guard in an if-then-else expression,
then local expressions of p1 also include that if-then-else‚Äôs true
and false branches as well as their sub-expressions. Each of the
local expressions corresponds to a decomposition of p into the local
expression p‚Ä≤
2. For each of these local
1 and the context around it p‚Ä≤
Algorithm 2 Witness-driven repair.
Require: association (d), influence (Œπ), utility (v) measures, oracle
(ùí™)
procedure Repair(p, X, Z , œµ, Œ¥)
P ‚Üê {d ‚àà ProxyDetect(p, X, Z , œµ, Œ¥) : not ùí™(d)}
if P (cid:44) ‚àÖ then
(p1, p2) ‚Üê element of P
p‚Ä≤ ‚Üê ProxyRepair(p,(p1, p2), X, Z , œµ, Œ¥)
return Repair(p‚Ä≤, X, Z , œµ, Œ¥)
else
return p
Algorithm 3 Local Repair.
Require: association (d), influence (Œπ), utility (v) measures
1: procedure ProxyRepair(p,(p1, p2), X, Z , œµ, Œ¥)
2:
3:
4:
5:
6:
7:
8:
R ‚Üê {}
for each subprogram p‚Ä≤
r‚àó ‚Üê Optimal constant for replacing p‚Ä≤
1
1 , p‚Ä≤‚Ä≤
2 ) ‚Üê (p1, p2) with r‚àó subst. for p‚Ä≤
(p‚Ä≤‚Ä≤
2 ) ‚â§ Œ¥ ‚à® d((cid:74)p‚Ä≤‚Ä≤
1(cid:75)(X), Z) ‚â§ œµ then
1
if Œπ(p‚Ä≤‚Ä≤
1 , p‚Ä≤‚Ä≤
R ‚Üê R ‚à™ [u/r‚àó]p‚Ä≤
2
return arg maxp‚àó‚ààR v (p‚àó)
1 of p1 do
1 , p‚Ä≤‚Ä≤
decompositions we discover the best constant, in terms of utility,
to replace p‚Ä≤
1 with (Line 4). We then make the same simplification
to the original decomposition (p1, p2), resulting in (p‚Ä≤‚Ä≤
2 ) (Line 5)
Using this third decomposition we check whether making the sim-
plification would repair the original violation (Line 6), collecting
those simplified programs that do. Finally, we take the best simpli-
fication of those found to remove the violation (Line 8). Details on
how the optimal constant is selected is described in Appendix C.1.
Two important things to note about the repair procedure. First,
there is always at least one subprogram on Line 3 that will fix the
violation, namely the decomposition (p1, p2) itself. Replacing p1
with a constant in this case would disassociate it from the sensitive
information type. Secondly, the procedure produces a model that
is smaller than the one given to it as it replaces a non-constant
expression with a constant. These two let us state the following:
Theorem 4. Algorithm 2 terminates and returns a program that
does not have any (œµ, Œ¥)-Proxy Use violations (instances of (œµ, Œ¥)-Proxy
Use for which oracle returns false).
6 EVALUATION
In this section we empirically evaluate our definition and algo-
rithms on several real datasets. In particular, we simulate a finan-
cial services application and demonstrate a typical workflow for a
practitioner using our tools to detect and repair proxy use in deci-
sion trees and linear models (¬ß6.1). We highlight that this workflow
identifies more proxy uses over a baseline procedure that simply
removes features associated with a protected information type. For
three other simulated settings on real data sets‚Äîcontraception ad-
vertising, student assistance, and credit advertising‚Äîwe describe
our findings of interesting proxy uses and demonstrate how the
outputs of our detection tool would allow a normative judgment
oracle to determine the appropriateness of proxy uses (¬ß6.2). In
¬ß6.3, we evaluate the performance of our detection and repair algo-
rithms and show that in particular cases, the runtime of our system
scales linearly in the size of the model. Also, by injecting violations
into real data sets so that we have ground truth, we evaluate the
completeness of our algorithm, and show a graceful degradation in
accuracy as the influence of the violating proxy increases.
Models and Implementation Our implementation currently
supports linear models, decision trees, random forests, and rule lists.
Note that these model types correspond to a range of commonly-
used learning algorithms such as logistic regression, support vector
machines [10], CART [6], and Bayesian rule lists [45]. Also, these
models represent a significant fraction of models used in practice in
predictive systems that operate on personal information, ranging
from advertising [9], psychopathy [38], criminal justice [4, 5], and
actuarial sciences [32, 34]. Our prototype implementation was writ-
ten in Python, and we use scikit-learn package to train the models
used in the evaluation. The benchmarks we describe later in this
section were recorded on a Ubuntu Desktop with 4.2 GHz Intel
Core i7 and 32GB RAM.
6.1 Example Workflow
A financial services company would like to expand its client base
by identifying potential customers with high income. To do so, the
company hires an analyst to build a predictive model that uses
age, occupation, education level, and other socio-economic features
to predict whether an individual currently has a ‚Äúhigh‚Äù or ‚Äúlow‚Äù
income. This practice is in line with the use of analytics in the
financial industry that exploit the fact that high-income individuals
are more likely to purchase financial products [70].
Because demographic data is known to correlate with marital
status [50], the data processor would like to ensure that the trained
model used to make income predictions does not effectively infer
individuals‚Äô marital status from the other demographic variables
that are explicitly used. In this context, basing the decision of which
clients to pursue on marital status could be perceived as a privacy vi-
olation, as other socio-economic variables are more directly related
to one‚Äôs interest and eligibility in various financial services.
To evaluate this scenario, we trained an income prediction model
from the UCI Adult dataset which consists of roughly 48,000 rows
containing economic and demographic information for adults de-
rived from publicly-available U.S. Census data. One of the features
available in this data is marital status, so we omitted it during
training, and later used it when evaluating our algorithms. In this
scenario, we act as the oracle in order to illustrate the kind of
normative judgments an analyst would need to make as an oracle.
After training a classifier on the preprocessed dataset, we found
a strong proxy for marital status in terms of an expression involving
relationship status. Figure 3 visualizes all of the expressions mak-
ing up the model (marked as ‚Ä¢), along with their association and
influence measures. In decision trees, sub-expressions like these
coincide with decompositions in our proxy use definition; each
sub-expression can be associated with a decomposition that cuts
out that sub-expression from the tree, and leaves a variable in its
place. The connecting lines in the figure denote the sub-expression
relationship. Together with the placement of points on the influence
Figure 3: The association and influence of the expres-
sions composing a decision tree trained on the UCI Adult
dataset. Narrow lines designate the sub-expression relation-
ship. Shaded area designates the feasible values for associa-
tion and influence between none, and maximal. Marker size
denotes the relative size of the sub-expressions pictured.
and association scales, this produces an overview of the decision
tree and the relationship of its constituent parts to the sensitive
attribute.
On further examination the relationship status was essentially
a finer-grained version of marital status. While not interesting
in itself, this occurrence demonstrates an issue with black-box
use of machine learning without closely examining the structure
of the data. In particular, one can choose to remove this feature,
and the model obtained after retraining will make predictions
that have low association with marital status. However, one sub-
model demonstrated relatively strong proxy use (œµ = 0.1, Œ¥ = 0.1):
age ‚â§ 31 and sex = 0 and capital_loss ‚â§ 1882.50 (labeled A in
Figure 4). This demonstrates that simply removing a feature does
not ensure that proxies are removed. When the model is retrained,
the learning algorithm might select new computations over other
features to embed in the model, as it did in this example. Also, note
that the new proxy combines three additional features. Eliminating
all of these features from the data could impact model performance.
Instead we can use our repair algorithm to remove the proxy: we
designate the unacceptable œµ, Œ¥ thresholds (the darkest area in Fig-
ure 4) and repair any proxies in that range. The result is the decision
tree marked with + in the figure. Note that this repaired version
has no sub-expressions in the prohibited range and that most of
the tree remains unchanged (the ‚Ä¢ and + markers largely coincide).
6.2 Other Case Studies
We now briefly discuss interesting examples for proxy use from
other case studies, demonstrating how our framework aids nor-
mative use privacy judgments. More details on these datasets and
experiments are in Appendix D.1.
Targeted contraception advertising We consider a scenario in
which a data processor wishes to show targeted advertisements for
contraceptives to females. We evaluated this scenario using data
collected for the 1987 National Indonesia Contraceptive Survey [1],
which contains a number of socio-economic features, including
feature indicating whether the individual‚Äôs religious beliefs were Is-
lam. A decision tree trained on this dataset illustrates an interesting
case of potential use privacy via the following proxy for religion:
Figure 4: Decision tree trained on the UCI Adult dataset
but with the relationship attribute removed (‚Ä¢), and the re-
paired version (+) of the same tree. Dark area in the upper-
left designates the thresholds used in repair.
ite(educ < 4‚àß nchild ‚â§ 3‚àß age < 31, no, yes). This term predicts
that women younger than 31, with below-average education back-
ground and fewer than four children will not use contraception. In
fact, just the ‚Äúguard‚Äù term educ < 4 alone is more closely associated
with religion, and its influence on the model‚Äôs output is nearly as
high. This reveals a surprising association between education levels
and religion leading to a potentially concerning case of proxy use.
Student assistance A current trend in education is the use of
predictive analytics to identify students who are likely to benefit
from certain types of interventions [31, 39]. We look at a scenario
where a data processor builds a model to predict whether a sec-
ondary school student‚Äôs grades are likely to suffer, based on a range
of demographic features, social information, and academic infor-
mation. To evaluate this scenario, we trained a model on the UCI
Student Alcohol Consumption dataset [11], with alcohol use as the
sensitive feature. Our algorithm found the following proxy for al-
cohol use: studytime < 2. This finding suggests that this instance
of proxy use can be deemed an appropriate use, and not a privacy
violation, as the amount of time a student spends studying is clearly
relevant to their academic performance.
Credit advertisements We consider a situation where a credit
card company wishes to send targeted advertisements for credit
cards based on demographic information. In this context, the use
of health status for targeted advertising is a legitimate privacy