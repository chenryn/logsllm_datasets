title:Harnessing Unreliable Cores in Heterogeneous Architecture: The PyDac
Programming Model and Runtime
author:Bin Huang and
Ron Sass and
Nathan DeBardeleben and
Sean Blanchard
2014 44th Annual IEEE/IFIP International Conference on Dependable Systems and Networks
Harnessing Unreliable Cores In Heterogeneous Architecture:
The PyDac Programming Model and Runtime
Bin Huang, and Ron Sass
Reconﬁgurable Computing Systems Laboratory
University of North Carolina at Charlotte
Charlotte, North Carolina, USA
Email: {bhuang2,rsass}@uncc.edu
Nathan DeBardeleben and Sean Blanchard
Ultrascale Systems Research Center
Los Alamos National Laboratory1
Email: {ndebard,seanb}@lanl.gov
Los Alamos, NM, USA
Abstract—Heterogeneous many-core architectures combined
with scratch-pad memories are attractive because they promise
better energy efﬁciency than conventional architectures and a
good balance between single-thread performance and multi-
thread throughput. However, programmers will need an en-
vironment
for ﬁnding and managing the large degree of
parallelism,
locality, and system resilience. We propose a
Python-based task parallel programming model called PyDac to
support these objectives. PyDac provides a two-level program-
ming model based on the divide-and-conquer strategy. The
PyDac runtime system allows threads to be run on unreliable
hardware by dynamically checking the results without involve-
ment from the programmer. To test this programming model
and runtime, an unconventional heterogeneous architecture
consisting of PowerPC and ARM cores was developed and
emulated on an FPGA device. We inject faults during the
execution of micro-benchmarks and show that through the
use of double and triple modular redundancy we are able to
complete the benchmarks with the correct results while only
incurring a proportional performance penalty.
Keywords-Heterogeneous Many-core Processor, Task-based
Programming Model, Soft Error, Resilience, Fault Tolerance
I. INTRODUCTION
With the end of Dennard Scaling, power efﬁciency has
emerged as a ﬁrst-class design constraint and will directly
impact the multicore scaling for the next several generations.
Esmaeilzadeh et. al [1] have predicted transistor under-
utilization on future multicore chips due to stringent power
budget and suggest radical micro-architectural innovations
beyond CPU-like or GPU-like multicore designs. Examples
of such an unconventional chip architecture have been
proposed with a heterogeneous mix of complex and simple
cores [2]–[4]. In addition to heterogeneity, recent chip design
has favored ﬁne-grain power management using dynamic
voltage and frequency scaling (DVFS) for a subset of
IA-32 cores [5]. The chip is now capable of operating
with different voltage domains. In addition, other research
has shown that aggressive supply voltage scaling greatly
1A portion of this work was performed at
the Ultrascale Systems
Research Center (USRC) at Los Alamos National Laboratory, supported by
the U.S. Department of Energy contract AC52-06NA25396. The publication
has been assigned the LANL identiﬁer LA-UR-14-21490.
improves the energy efﬁciency of a single processing unit
[6]. It seems that the techniques of DVFS and aggressive
supply voltage scaling could be naturally applied to a chip
architecture composed of a heterogeneous mix of processor
cores. However, it is well-known that there is a tradeoff
between power and reliability. Speciﬁcally, improvements
in power efﬁciency through voltage scaling come at the
cost of increasing soft error rate [7]. Based on these trends,
we are motivated to prepare for a hardware design that is:
(a) heterogeneous many-core, and (b) likely to experience
higher rates of faults.
For such design, one serious consideration that must be
addressed is the ability of applications of interest to run
through a variety of failures. Current HPC systems rely on
checkpoint-restart (C/R) to recover from faults; however as
system size continues to grow, the overhead of checkpoint-
ing will dominate applications’ total run time [8], [9]. In
addition, silent data corruption on large-scale systems may
require an approach different from C/R for detection and
correction [10]. To maximize overall system throughput and
address a broader class of faults, new resiliency techniques
will be needed.
We propose a two-level programming model and dis-
tributed runtime system that allows us create a very large
number of threads while simultaneously and implicitly ad-
dressing system resiliency. The runtime system watches for
errors that indicate an unreliable core, reissue tasks to ensure
its timely completion, and proactively manage deterioration.
Autonomously or at the direction of the application, the run-
time system can also run multiple copies of identical tasks
and check the results of those tasks for output variations
that may indicate silent faults. After detection, the runtime
system can either reissue a task, in the case of dual module
redundancy, or (with triple modular redundancy) use a voting
mechanism to choose the correct result.
To evaluate this approach, the programming model and
runtime were implemented on a novel architecture that
is emulated on an FPGA device. This “green-white” ar-
chitecture has multiple processor cores, some optimized
for single-thread performance (denoted as white core) and
978-1-4799-2233-8/14 $31.00 © 2014 IEEE
DOI 10.1109/DSN.2014.77
744
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:37 UTC from IEEE Xplore.  Restrictions apply. 
many simple processor cores designed for high throughput
(denoted as green core). The concept and general philosophy
was previously described in [11]; however, that work only
described the runtime, a simple approach to fault tolerance,
and a single algorithm. In this paper, we demonstrate the
programming model and evaluate ﬁve micro-benchmarks.
Moreover, we incorporate a more sophisticated hardware
prototype enhanced by a DMA engine.
The rest of the paper is organized as follows. Section
2 discusses design goals of the programming model and
details a Python-based programming template. Section 2 also
reviews the implementation of the runtime system for the
proposed programming model. Section 3 presents perfor-
mance numbers of running multiple micro-benchmarks on
the emulated heterogeneous architecture. Section 4 discusses
the related works and Section 5 concludes the paper.
II. PYDAC FRAMEWORK
A. Programming Model
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
import pydac
##############################################
# Microbenchmark Kernel Operation
# -- these are the only functions that the
#
##############################################
computational scientist needs to provide
def fib_base(n):
# base case
a, b = 0, 1
for i in range(n):
a, b = b, a+b
return a
def fib_op(n, bc_size): # the ‘divide’ part
# (data decomposition)
if (n <= bc_size):
result = ship(fib_base, [n])
else:
subpb = [(n-1, bc_size), (n-2, bc_size)]
results = divide(fib_op, subpb)
result = results[0] + results[1]
return result
Listing 1. Fibonacci algorithm implemented in PyDac
The primary goal of the programming model is to make it
possible to write programs that generate a very large number
of parallel
tasks without a great deal of programming
effort. Functional languages are very good at this but are
generally viewed as difﬁcult for computational scientists
to use. The main idea here was to implement a two-level
programming paradigm that borrows the key concept of
divide-and-conquer from functional languages to do the data
decomposition and task creation and then uses the impera-
tive paradigm for the individual tasks. Python was chosen
because it is considered an easy language to learn; it supports
functional, imperative, and object-oriented paradigms; and
it has popular modules to support scientiﬁc applications. In
short, a programmer that wants to use the PyDac runtime
system only needs to learn two concepts: the general divide-
and-conquer strategy and two PyDac function calls, divide
and ship.
The only constraints imposed on the programmer are on
how the base case is constructed. Speciﬁcally, programmers
are required to maintain referential transparency, i.e. they
cannot use global variable to communicate. In Python terms,
programmers cannot pass mutable objects or use the key-
word global in the base case.
Listing 1 illustrates the recursive implementation of Fi-
bonacci algorithm under this programming model. The pro-
gram exhibits the basic constructs of divide-and-conquer.
On line 20, the primary problem is broken into a list of
subproblems. In Python, this is a list of subproblems (one
tuple per subproblem). On line 21, the PyDac divide function
is invoked. Based on the hardware resources available, the
runtime decides on how to deploy tasks based on the the
degree of parallelism and health of the system. The results
of these subproblems are returned as a list and Line 22
combines them into a solution. The only missing piece is
the base case. This is solved imperatively in lines 9–13. It
is important to note that the size of the base case impacts
performance and resilience. For example, on an architecture
that supports local recovery of a faulty processor core, this
parameter allows the dynamic adjustment of workload on
the faulty processor core for least recovery overhead [11].
B. Runtime System
The main goal of the runtime system is two-fold. It
bridges the programming model to a heterogeneous mix
of reliable and unreliable cores. Second, it introduces sys-
tem resilience capabilities without changing the application
source code. Here we provide a general overview of the
runtime system, the reader is referred to [11] for details of
its implementation.
As illustrated in Figure 1, the runtime system consists
of two software stacks. The ﬁrst is a full-featured Python
interpreter running on a traditional SMP operating system
with scientiﬁc libraries and a user-level thread library. The
second is a very lightweight Python virtual machine running
independently on a core with no operating system. Together,
there are several valuable features of this organization:
(a) the runtime system is distributed as multiple virtual
machines (VMs), (b) the operating system layer is removed
from the software stack on green cores, (c) it reuses Python
modules popular in computational science community such
as Numpy [12], and (d) it is resilient to soft errors by
local fault recovery. The most
important change in the
original runtime design is a virtual shared memory supported
by DMA engine between main memory and scratch-pad
memories.
The full-featured Python interpreter and lightweight
Python interpreters exchange objects to each other. These
objects include callable objects (e.g., a function) and non-
callable objects (e.g., a tuple). Since Python virtual machines
share the same set of Python byte code, a callable object
745
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:37 UTC from IEEE Xplore.  Restrictions apply. 
SciPy
NumPy
User-level Thread Libary
Programming Model Support 
Dynamic Translation 
Dispatcher
Stackless Python VM
Linux Kernel
Application
PyDac
Runtime
Task
Task
VM
VM
WC
WC

WC
GC
GC
NOC


Task
VM
GC
PyDac
Distributed
Runtime

	
	

	

VM
WC
GC
NOC



Figure 1. Distributed PyDac Runtime
generated on one virtual machine is able to run on the other
after migration. In our current implementation, a function
is deﬁned on the full-featured Python interpreter running
on the white core. This function is migrated to lightweight
Python interpreters for execution. During migration,the run-
time system does not generate any new source code (e.g., C
code that would normally require just-in-time compilation
and dynamical linking).
With scratch-pad memory support, parallel
tasks only
generate memory access to main memory for parameters
and bytecode executables. The ephemeral memory content of
light-weight interpreters such as stack and heap are created
locally and will never go to main memory, which minimizes
unnecessary data movement. (In contrast, a cache line might
be moved up and down in cache hierarchy.) The white core
monitors all green cores through a status mailbox.
The dynamical translation layer is the “glue logic” be-
tween two types of interpreters. Although Python virtual
machines share the same set of Python byte code,
the
organization of byte codes for the same function might be
different on different virtual machines. Hence, the dynamic
translation layer extracts byte codes from a function and re-
organizes them before migration.
The task migration is managed by the dispatcher. The
dispatcher creates a global FIFO-based task queue for all
green cores. Since each green core has multiple scratch-
pad memories, the dispatcher also creates local FIFO-based
task queue. Since the runtime system is capable of isolating
all of the data the base case computation is working on,
the dispatcher supports a simple local fault recovery mech-
anism — re-assigning tasks — for green cores. To detect
soft errors (e.g., silent data corruption), the dispatcher has
implemented redundant multi-threading (RMT) mechanism.
The dispatcher may choose to be dual modular redundant
(DMR) to detect soft errors or triple modular redundant
(TMR) to correct soft errors. If a soft error is detected by
DMR, the dispatcher re-issues tasks to green cores.
III. EXPERIMENTAL RESULTS
A. Green-White Prototype
In order to evaluate the proposed programming model
and runtime system, an experimental “green-white” chip
architecture was emulated on a Xilinx ML-510 developer
board populated with a Virtex 5 FPGA. Six ARMv2a soft
processors [13] were implemented to represent the green
cores. Each processor was clocked at 50 MHz and the
number of cores was limited by the available on-chip mem-
ory resources of the FPGA. Each green core is equipped
with a 128 KB scratch-pad memory (for VM executable)
and two multiplexed 16 KB scratch-pad memories (for the
task’s bytecodes and arguments). The scratch-pad memories
are single-cycle latency on-chip memories. The 128-KB
scratch-pad memory is initialized with VM executable only
once unless its content is corrupted by faults and errors.
The 16 KB scratch-pad memories are multiplexed to allow
computation and memory transactions to be overlapped.
We envision more than two banks of memory and larger
memories but our emulation is constrained by the resources
available on the FPGA device.
The white core is a PowerPC 440 clocked at 400 MHz.
We use a shared bus and a DMA engine to implement the
system interconnect because it was more expedient than a
conventional mesh network-on-chip. This has a negative im-
pact on absolute performance but it frees enough resources
to allow six green cores. Hence, our goal is to evaluate
relative performance under fault conditions. The DMA is a
bidirectional streaming engine transferring data between the
scratch-pad memories and the main memory. This engine
takes the starting address and length as input, and streams
the data without involving the white core or the green core.
FIFOs are used to buffer data between different interfaces.
B. Fault Injection Mechanism
We have implemented ﬁve micro-benchmarks and a used
fault injection mechanism to test the PyDac runtime fault
recovery mechanism. Faults are injected to the green cores
since we expect the green cores are more susceptible to
faults. We also assume the white core and network-on-chip
746
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 08:27:37 UTC from IEEE Xplore.  Restrictions apply. 
are fault-free in this study. Incorporating techniques (high