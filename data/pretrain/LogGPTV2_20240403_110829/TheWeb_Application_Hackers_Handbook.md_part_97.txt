specifi c items, and you can annotate interesting items for future reference.
n Automated match-and-replace rules for dynamically modifying the con-
tents of requests and responses. This function can be useful in numerous
situations. Examples include rewriting the value of a cookie or other
parameter in all requests, removing cache directives, and simulating a
specifi c browser with the User-Agent header.
n Access to proxy functionality directly from within the browser, in addition
to the client UI. You can browse the proxy history and reissue individual
requests from the context of your browser, enabling the responses to be
fully processed and interpreted in the normal way.
n Utilities for manipulating the format of HTTP messages, such as convert-
ing between different request methods and content encodings. These can
sometimes be useful when fi ne-tuning an attack such as cross-site scripting.
cc2200..iinndddd 775588 88//1199//22001111 1122::2211::0066 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 759
Chapter 20 n A Web Application Hacker’s Toolkit 759
n Functions to automatically modify certain HTML features on-the-fl y. You
can unhide hidden form fi elds, remove input fi eld limits, and remove
JavaScript form validation.
Figure 20-5: Burp proxy supports configuration of fine-grained rules for intercepting
requests and responses
Figure 20-6: The proxy history, allowing you to view, filter, search, and annotate
requests and responses made via the proxy
cc2200..iinndddd 775599 88//1199//22001111 1122::2211::0066 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 760
760 Chapter 20 n A Web Application Hacker’s Toolkit
Web Application Spiders
Web application spiders work much like traditional web spiders. They request
web pages, parse them for links to other pages, and then request those pages,
continuing recursively until all of a site’s content has been discovered. To accom-
modate the differences between functional web applications and traditional
websites, application spiders must go beyond this core function and address
various other challenges:
n Forms-based navigation, using drop-down lists, text input, and other
methods
n JavaScript-based navigation, such as dynamically generated menus
n Multistage functions requiring actions to be performed in a defi ned sequence
n Authentication and sessions
n The use of parameter-based identifi ers, rather than the URL, to specify
different content and functionality
n The appearance of tokens and other volatile parameters within the URL
query string, leading to problems identifying unique content
Several of these problems are addressed in integrated testing suites by shar-
ing data between the intercepting proxy and spider components. This enables
you to use the target application in the normal way, with all requests being pro-
cessed by the proxy and passed to the spider for further analysis. Any unusual
mechanisms for navigation, authentication, and session handling are thereby
taken care of by your browser and your actions. This enables the spider to build
a detailed picture of the application’s contents under your fi ne-grained control.
This user-directed spidering technique is described in detail in Chapter 4.
Having assembled as much information as possible, the spider can then be
launched to investigate further under its own steam, potentially discovering
additional content and functionality.
The following features are commonly implemented within web application
spiders:
n Automatic update of the site map with URLs accessed via the intercept-
ing proxy.
n Passive spidering of content processed by the proxy, by parsing it for
links and adding these to the site map without actually requesting them
(see Figure 20-7).
n Presentation of discovered content in table and tree form, with the facility
to search these results.
n Fine-grained control over the scope of automated spidering. This enables
you to specify which hostnames, IP addresses, directory paths, fi le types,
cc2200..iinndddd 776600 88//1199//22001111 1122::2211::0066 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 761
Chapter 20 n A Web Application Hacker’s Toolkit 761
and other items the spider should request to focus on a particular area of
functionality. You should prevent the spider from following inappropriate
links either within or outside of the target application’s infrastructure. This
feature is also essential to avoid spidering powerful functionality such as
administrative interfaces, which may cause dangerous side effects such
as the deletion of user accounts. It is also useful to prevent the spider
from requesting the logout function, thereby invalidating its own session.
n Automatic parsing of HTML forms, scripts, comments, and images, and
analysis of these within the site map.
n Parsing of JavaScript content for URLs and resource names. Even if a full
JavaScript engine is not implemented, this function often enables a spider
to discover the targets of JavaScript-based navigation, because these usu-
ally appear in literal form within the script.
n Automatic and user-guided submission of forms with suitable parameters
(see Figure 20-8).
n Detection of customized File Not Found responses. Many applications
respond with an HTTP 200 message when an invalid resource is requested.
If spiders are unable to recognize this, the resulting content map will
contain false positives.
n Checking for the robots.txt fi le, which is intended to provide a blacklist
of URLs that should not be spidered, but that an attacking spider can use
to discover additional content.
n Automatic retrieval of the root of all enumerated directories. This can
be useful to check for directory listings or default content (see
Chapter 17).
n Automatic processing and use of cookies issued by the application to
enable spidering to be performed in the context of an authenticated session.
n Automatic testing of session dependence of individual pages. This involves
requesting each page both with and without any cookies that have been
received. If the same content is retrieved, the page does not require a ses-
sion or authentication. This can be useful when probing for some kinds
of access control fl aws (see Chapter 8).
n Automatic use of the correct Referer header when issuing requests. Some
applications may check the contents of this header, and this function ensures
that the spider behaves as much as possible like an ordinary browser.
n Control of other HTTP headers used in automated spidering.
n Control over the speed and order of automated spider requests to
avoid overwhelming the target and, if necessary, behave in a stealthy
manner.
cc2200..iinndddd 776611 88//1199//22001111 1122::2211::0077 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 762
762 Chapter 20 n A Web Application Hacker’s Toolkit
Figure 20-7: The results of passive application spidering, where items in gray have
been identified passively but not yet requested
Figure 20-8: Burp Spider prompting for user guidance when
submitting forms
Web Application Fuzzers
Although it is possible to perform a successful attack using only manual tech-
niques, to become a truly accomplished web application hacker, you need to
automate your attacks to enhance their speed and effectiveness. Chapter 14
cc2200..iinndddd 776622 88//1199//22001111 1122::2211::0077 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 763
Chapter 20 n A Web Application Hacker’s Toolkit 763
described in detail the different ways in which automation can be used in cus-
tomized attacks. Most test suites include functions that leverage automation to
facilitate various common tasks. Here are some commonly implemented features:
n Manually confi gured probing for common vulnerabilities. This function
enables you to control precisely which attack strings are used and how they
are incorporated into requests. Then you can review the results to identify
any unusual or anomalous responses that merit further investigation.
n A set of built-in attack payloads and versatile functions to generate arbi-
trary payloads in user-defi ned ways — for example, based on malformed
encoding, character substitution, brute force, and data retrieved in a
previous attack.
n The ability to save attack results and response data to use in reports or
incorporate into further attacks.
n Customizable functions for viewing and analyzing responses — for exam-
ple, based on the appearance of specifi c expressions or the attack payload
itself (see Figure 20-9).
n Functions for extracting useful data from the application’s responses — for
example, by parsing the username and password fi elds in a My Details
page. This can be useful when you are exploiting various vulnerabilities,
including fl aws in session-handling and access controls.
Figure 20-9: The results of a fuzzing exercise using Burp Intruder
cc2200..iinndddd 776633 88//1199//22001111 1122::2211::0077 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 764
764 Chapter 20 n A Web Application Hacker’s Toolkit
Web Vulnerability Scanners
Some integrated testing suites include functions to scan for common web appli-
cation vulnerabilities. The scanning that is performed falls into two categories:
n Passivescanning involves monitoring the requests and responses passing
through the local proxy to identify vulnerabilities such as cleartext password
submission, cookie misconfi guration, and cross-domain Referer leakage.
You can perform this type of scanning noninvasively with any applica-
tion that you visit with your browser. This feature is often useful when
scoping out a penetration testing engagement. It gives you a feel for the
application’s security posture in relation to these kinds of vulnerabilities.
n Activescanning involves sending new requests to the target application
to probe for vulnerabilities such as cross-site scripting, HTTP header
injection, and fi le path traversal. Like any other active testing, this type
of scanning is potentially dangerous and should be carried out only with
the consent of the application owner.
The vulnerability scanners included within testing suites are more user-
driven than the standalone scanners discussed later in this chapter. Instead of
just providing a start URL and leaving the scanner to crawl and test the applica-
tion, the user can guide the scanner around the application, control precisely
which requests are scanned, and receive real-time feedback about individual
requests. Here are some typical ways to use the scanning function within an
integrated testing suite:
n After manually mapping an application’s contents, you can select interest-
ing areas of functionality within the site map and send these to be scanned.
This lets you target your available time into scanning the most critical
areas and receive the results from these areas more quickly.
n When manually testing individual requests, you can supplement your
efforts by scanning each specifi c request as you are testing it. This gives
you nearly instant feedback about common vulnerabilities for that request,
which can guide and optimize your manual testing.
n You can use the automated spidering tool to crawl the entire application
and then scan all the discovered content. This emulates the basic behavior
of a standalone web scanner.
n In Burp Suite, you can enable live scanning as you browse. This lets you
guide the scanner’s coverage using your browser and receive quick feed-
back about each request you make, without needing to manually identify
the requests you want to scan. Figure 20-10 shows the results of a live
scanning exercise.
cc2200..iinndddd 776644 88//1199//22001111 1122::2211::0077 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 765
Chapter 20 n A Web Application Hacker’s Toolkit 765
Figure 20-10: The results of live scanning as you browse with Burp Scanner
Although the scanners in integrated testing suites are designed to be used in a
different way than standalone scanners, in some cases the core scanning engine
is highly capable and compares favorably with those of the leading standalone
scanners, as described later in this chapter.
Manual Request Tools
The manual request component of the integrated testing suites provides the basic
facility to issue a single request and view its response. Although simple, this
function is often benefi cial when you are probing a tentative vulnerability and
need to reissue the same request manually several times, tweaking elements of
the request to determine the effect on the application’s behavior. Of course, you
could perform this task using a standalone tool such as Netcat, but having the
cc2200..iinndddd 776655 88//1199//22001111 1122::2211::0077 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 766
766 Chapter 20 n A Web Application Hacker’s Toolkit
function built in to the suite means that you can quickly retrieve an interesting
request from another component (proxy, spider, or fuzzer) for manual investiga-
tion. It also means that the manual request tool benefi ts from the various shared
functions implemented within the suite, such as HTML rendering, support for
upstream proxies and authentication, and automatic updating of the Content-
Length header. Figure 20-11 shows a request being reissued manually.
Figure 20-11: A request being reissued manually using Burp Repeater
The following features are often implemented within manual request tools:
n Integration with other suite components, and the ability to refer any request
to and from other components for further investigation
n A history of all requests and responses, keeping a full record of all manual
requests for further review, and enabling a previously modifi ed request
to be retrieved for further analysis
cc2200..iinndddd 776666 88//1199//22001111 1122::2211::0088 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 767
Chapter 20 n A Web Application Hacker’s Toolkit 767
n A multitabbed interface, letting you work on several different items at once
n The ability to automatically follow redirections
Session Token Analyzers
Some testing suites include functions to analyze the randomness proper-
ties of session cookies and other tokens used within the application where
there is a need for unpredictability. Burp Sequencer is a powerful tool that
performs standard statistical tests for randomness on an arbitrarily sized
sample of tokens and provides fi ne-grained results in an accessible format.
Burp Sequencer is shown in Figure 20-12 and is described in more detail in
Chapter 7.
Figure 20-12: Using Burp Sequencer to test the randomness properties of an
application’s session token
cc2200..iinndddd 776677 88//1199//22001111 1122::2211::0088 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 768
768 Chapter 20 n A Web Application Hacker’s Toolkit
Shared Functions and Utilities
In addition to their core tool components, integrated test suites provide a wealth
of other value-added features that address specifi c needs that arise when you are
attacking a web application and that enable the other tools to work in unusual
situations. The following features are implemented by the different suites:
n Analysis of HTTP message structure, including parsing of headers and
request parameters, and unpacking of common serialization formats (see
Figure 20-13)
n Rendering of HTML content in responses as it would appear within the
browser
n The ability to display and edit messages in text and hexadecimal form
n Search functions within all requests and responses
n Automatic updating of the HTTP Content-Length header following any
manual editing of message contents
n Built-in encoders and decoders for various schemes, enabling quick analysis
of application data in cookies and other parameters
n A function to compare two responses and highlight the differences
n Features for automated content discovery and attack surface analysis
n The ability to save to disk the current testing session and retrieve saved
sessions
n Support for upstream web proxies and SOCKS proxies, enabling you to
chain together different tools or access an application via the proxy server
used by your organization or ISP
n Features to handle application sessions, login, and request tokens, allow-
ing you to continue using manual and automated techniques when faced
with unusual or highly defensive session-handling mechanisms
n In-tool support for HTTP authentication methods, enabling you to use
all the suite’s features in environments where these are used, such as
corporate LANs
n Support for client SSL certifi cates, enabling you to attack applications
that employ these
n Handling of the more obscure features of HTTP, such as gzip content
encoding, chunked transfer encoding, and status 100 interim responses
n Extensibility, enabling the built-in functionality to be modifi ed and extended
in arbitrary ways by third-party code
n The ability to schedule common tasks, such as spidering and scanning,
allowing you to start the working day asleep
cc2200..iinndddd 776688 88//1199//22001111 1122::2211::0088 PPMM
Stuttard c20.indd V3 - 08/16/2011 Page 769
Chapter 20 n A Web Application Hacker’s Toolkit 769
n Persistent confi guration of tool options, enabling a particular setup to be
resumed on the next execution of the suite
n Platform independence, enabling the tools to run on all popular operat-
ing systems
Figure 20-13: Requests and responses can be analyzed into their HTTP
structure and parameters
Testing Work Flow
Figure 20-14 shows a typical work fl ow for using an integrated testing suite.
The key steps involved in each element of the testing are described in detail
throughout this book and are collated in the methodology set out in Chapter 21.
The work fl ow described here shows how the different components of the test-
ing suite fi t into that methodology.
In this work fl ow, you drive the overall testing process using your browser.
As you browse the application via the intercepting proxy, the suite compiles
two key repositories of information:
n The proxy history records every request and response passing through
the proxy.
n The sitemap records all discovered items in a directory tree view of the
target.
(Note that in both cases, the default display fi lters may hide from view some
items that are not normally of interest when testing.)
As described in Chapter 4, as you browse the application, the testing suite
typically performs passive spidering of discovered content. This updates the site