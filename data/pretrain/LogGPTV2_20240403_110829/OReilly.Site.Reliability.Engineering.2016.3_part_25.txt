systems perspective to testing in order to drive reliability at scale.
A small unit test might have a short list of dependencies: one source file, the testing
library, the runtime libraries, the compiler, and the local hardware running the tests.
A robust testing environment dictates that those dependencies each have their own
test coverage, with tests that specifically address use cases that other parts of the envi‐
ronment expect. If the implementation of that unit test depends on a code path inside
a runtime library that doesn’t have test coverage, an unrelated change in the environ‐
ment9 can lead the unit test to consistently pass testing, regardless of faults in the code
under test.
In contrast, a release test might depend on so many parts that it has a transitive
dependency on every object in the code repository. If the test depends on a clean
copy of the production environment, in principle, every small patch requires per‐
forming a full disaster recovery iteration. Practical testing environments try to select
branch points among the versions and merges. Doing so resolves the maximum
amount of dependent uncertainty for the minimum number of iterations. Of course,
8 See https://github.com/google/bazel.
9 For example, code under test that wraps a nontrivial API to provide a simpler and backward-compatible
abstraction. The API that used to be synchronous instead returns a future. Calling argument errors still
deliver an exception, but not until the future is evaluated. The code under test passes the API result directly
back to the caller. Many cases of argument misuse may not be caught.
192 | Chapter 17: Testing for Reliability
when an area of uncertainty resolves into a fault, you need to select additional branch
points.
Testing Scalable Tools
As pieces of software, SRE tools also need testing.10 SRE-developed tools might per‐
form tasks such as the following:
• Retrieving and propagating database performance metrics
• Predicting usage metrics to plan for capacity risks
• Refactoring data within a service replica that isn’t user accessible
• Changing files on a server
SRE tools share two characteristics:
• Their side effects remain within the tested mainstream API
• They’re isolated from user-facing production by an existing validation and
release barrier
Barrier Defenses Against Risky Software
Software that bypasses the usual heavily tested API (even if it does so for a good
cause) could wreak havoc on a live service. For example, a database engine implemen‐
tation might allow administrators to temporarily turn off transactions in order to
shorten maintenance windows. If the implementation is used by batch update soft‐
ware, user-facing isolation may be lost if that utility is ever accidentally launched
against a user-facing replica. Avoid this risk of havoc with design:
1. Use a separate tool to place a barrier in the replication configuration so that the
replica cannot pass its health check. As a result, the replica isn’t released to users.
2. Configure the risky software to check for the barrier upon startup. Allow the
risky software to only access unhealthy replicas.
3. Use the replica health validating tool you use for black-box monitoring to
remove the barrier.
10 This section talks specifically about tools used by SRE that need to be scalable. However, SRE also develops
and uses tools that don’t necessarily need to be scalable. The tools that don’t need to be scalable also need to be
tested, but these tools are out of scope for this section, and therefore won’t be discussed here. Because their
risk footprint is similar to user-facing applications, similar testing strategies are applicable on such SRE-
developed tools.
Testing at Scale | 193
Automation tools are also software. Because their risk footprint appears out-of-band
for a different layer of the service, their testing needs are more subtle. Automation
tools perform tasks like the following:
• Database index selection
• Load balancing between datacenters
• Shuffling relay logs for fast remastering
Automation tools share two characteristics:
• The actual operation performed is against a robust, predictable, and well-tested
API
• The purpose of the operation is the side effect that is an invisible discontinuity to
another API client
Testing can demonstrate the desired behavior of the other service layer, both before
and after the change. It’s often possible to test whether internal state, as seen through
the API, is constant across the operation. For example, databases pursue correct
answers, even if a suitable index isn’t available for the query. On the other hand, some
documented API invariants (such as a DNS cache holding until the TTL) may not
hold across the operation. For example, if a runlevel change replaces a local name‐
server with a caching proxy, both choices can promise to retain completed lookups
for many seconds. It’s unlikely that the cache state is handed over from one to the
other.
Given that automation tools imply additional release tests for other binaries to handle
environmental transients, how do you define the environment in which those auto‐
mation tools run? After all, the automation for shuffling containers to improve usage
is likely to try to shuffle itself at some point if it also runs in a container. It would be
embarrassing if a new release of its internal algorithm yielded dirty memory pages so
quickly that the network bandwidth of the associated mirroring ended up preventing
the code from finalizing the live migration. Even if there’s an integration test for
which the binary intentionally shuffles itself around, the test likely doesn’t use a
production-sized model of the container fleet. It almost certainly isn’t allowed to use
scarce high-latency intercontinental bandwidth for testing such races.
194 | Chapter 17: Testing for Reliability
Even more amusingly, one automation tool might be changing the environment in
which another automation tool runs. Or both tools might be changing the environ‐
ment of the other automation tool simultaneously! For example, a fleet upgrading
tool likely consumes the most resources when it’s pushing upgrades. As a result, the
container rebalancing would be tempted to move the tool. In turn, the container
rebalancing tool occasionally needs upgrading. This circular dependency is fine if the
associated APIs have restart semantics, someone remembered to implement test cov‐
erage for those semantics, and checkpoint health is assured independently.
Testing Disaster
Many disaster recovery tools can be carefully designed to operate offline. Such tools
do the following:
• Compute a checkpoint state that is equivalent to cleanly stopping the service
• Push the checkpoint state to be loadable by existing nondisaster validation tools
• Support the usual release barrier tools, which trigger the clean start procedure
In many cases, you can implement these phases so that the associated tests are easy to
write and offer excellent coverage. If any of the constraints (offline, checkpoint, load‐
able, barrier, or clean start) must be broken, it’s much harder to show confidence that
the associated tool implementation will work at any time on short notice.
Online repair tools inherently operate outside the mainstream API and therefore
become more interesting to test. One challenge you face in a distributed system is
determining if normal behavior, which may be eventually consistent by nature, will
interact badly with the repair. For example, consider a race condition that you can
attempt to analyze using the offline tools. An offline tool is generally written to expect
instant consistency, as opposed to eventual consistency, because instant consistency is
less challenging to test. This situation becomes complicated because the repair binary
is generally built separately from the serving production binary that it’s racing
against. Consequently, you might need to build a unified instrumented binary to run
within these tests so that the tools can observe transactions.
Testing at Scale | 195
Using Statistical Tests
Statistical techniques, such as Lemon [Ana07] for fuzzing, and Chaos Monkey11 and
Jepsen12 for distributed state, aren’t necessarily repeatable tests. Simply rerunning
such tests after a code change doesn’t definitively prove that the observed fault is
fixed.13 However, these techniques can be useful:
• They can provide a log of all the randomly selected actions that are taken in a
given run—sometimes simply by logging the random number generator seed.
• If this log is immediately refactored as a release test, running it a few times before
starting on the bug report is often helpful. The rate of nonfailure on replay tells
you how hard it will be to later assert that the fault is fixed.
• Variations in how the fault is expressed help you pinpoint suspicious areas in the
code.
• Some of those later runs may demonstrate failure situations that are more severe
than those in the original run. In response, you may want to escalate the bug’s
severity and impact.
The Need for Speed
For every version (patch) in the code repository, every defined test provides a pass or
fail indication. That indication may change for repeated and seemingly identical runs.
You can estimate the actual likelihood of a test passing or failing by averaging over
those many runs and computing the statistical uncertainty of that likelihood. How‐
ever, performing this calculation for every test at every version point is computation‐
ally infeasible.
Instead, you must form hypotheses about the many scenarios of interest and run the
appropriate number of repeats of each test and version to allow a reasonable infer‐
ence. Some of these scenarios are benign (in a code quality sense), while others are
actionable. These scenarios affect all the test attempts to varying extents and, because
they are coupled, reliably and quickly obtaining a list of actionable hypotheses (i.e.,
components that are actually broken) means estimating all scenarios at the same
time.
11 See https://github.com/Netflix/SimianArmy/wiki/Chaos-Monkey.
12 See https://github.com/aphyr/jepsen.
13 Even if the test run is repeated with the same random seed so that the task kills are in the same order, there is
no serialization between the kills and the fake user traffic. Therefore, there’s no guarantee that the actual pre‐
viously observed code path will now be exercised again.
196 | Chapter 17: Testing for Reliability
Engineers who use the testing infrastructure want to know if their code—usually a
tiny fraction of all the source behind a given test run—is broken. Often, not being
broken implies that any observed failures can be blamed on someone else’s code. In
other words, the engineer wants to know if their code has an unanticipated race con‐
dition that makes the test flaky (or more flaky than the test already was due to other
factors).
Testing Deadlines
Most tests are simple, in the sense that they run as a self-contained hermetic binary
that fits in a small compute container for a few seconds. These tests give engineers
interactive feedback about mistakes before the engineer switches context to the next
bug or task.
Tests that require orchestration across many binaries and/or across a fleet that has
many containers tend to have startup times measured in seconds. Such tests are usu‐
ally unable to offer interactive feedback, so they can be classified as batch tests.
Instead of saying “don’t close the editor tab” to the engineer, these test failures are say‐
ing “this code is not ready for review” to the code reviewer.
The informal deadline for the test is the point at which the engineer makes the next
context switch. Test results are best given to the engineer before he or she switches
context, because otherwise the next context may involve XKCD compiling.14
Suppose an engineer is working on a service with over 21,000 simple tests and occa‐
sionally proposes a patch against the service’s codebase. To test the patch, you want to
compare the vector of pass/fail results from the codebase before the patch with the
vector of results from the codebase after the patch. A favorable comparison of those
two vectors provisionally qualifies the codebase as releasable. This qualification cre‐
ates an incentive to run the many release and integration tests, as well as other dis‐
tributed binary tests that examine scaling of the system (in case the patch uses
significantly more local compute resources) and complexity (in case the patch creates
a superlinear workload elsewhere).
At what rate can you incorrectly flag a user’s patch as damaging by miscalculating
environmental flakiness? It seems likely that users would vehemently complain if 1 in
10 patches is rejected. But a rejection of 1 patch among 100 perfect patches might go
without comment.
14 See http://xkcd.com/303/.
Testing at Scale | 197
This means you’re interested in the 42,000th root (one for each defined test before the
patch, and one for each defined test after the patch) of 0.99 (the fraction of patches
that can be rejected). This calculation:
1
2×21000
0.99
suggests that those individual tests must run correctly over 99.9999% of the time.
Hmm.
Pushing to Production
While production configuration management is commonly kept in a source control
repository, configuration is often separate from the developer source code. Similarly,
the software testing infrastructure often can’t see production configuration. Even if
the two are located in the same repository, changes for configuration management
are made in branches and/or a segregated directory tree that test automation has his‐
torically ignored.
In a legacy corporate environment where software engineers develop binaries and
throw them over the wall to the administrators who update the servers, segregation of
testing infrastructure and production configuration is at best annoying, and at worst
can damage reliability and agility. Such segregation might also lead to tool duplica‐
tion. In a nominally integrated Ops environment, this segregation degrades resiliency
because it creates subtle inconsistencies between the behavior for the two sets of tools.
This segregation also limits project velocity because of commit races between the ver‐
sioning systems.
In the SRE model, the impact of segregating testing infrastructure from production
configuration is appreciably worse, as it prevents relating the model describing pro‐
duction to the model describing the application behavior. This discrepancy impacts
engineers who want to find statistical inconsistencies in expectations at development
time. However, this segregation doesn’t slow down development so much as prevent
the system architecture from changing, because there is no way to eliminate migra‐
tion risk.
Consider a scenario of unified versioning and unified testing, so that the SRE meth‐
odology is applicable. What impact would the failure of a distributed architecture
migration have? A fair amount of testing will probably occur. So far, it’s assumed that
a software engineer would likely accept the test system giving the wrong answer 1
time in 10 or so. What risk are you willing to take with the migration if you know that
testing may return a false negative and the situation could become really exciting,
really quickly? Clearly, some areas of test coverage need a higher level of paranoia
198 | Chapter 17: Testing for Reliability
than others. This distinction can be generalized: some test failures are indicative of a
larger impact risk than other test failures.
Expect Testing Fail
Not too long ago, a software product might have released once per year. Its binaries
were generated by a compiler toolchain over many hours or days, and most of the
testing was performed by humans against manually written instructions. This release
process was inefficient, but there was little need to automate it. The release effort was
dominated by documentation, data migration, user retraining, and other factors.
Mean Time Between Failure (MTBF) for those releases was one year, no matter how
much testing took place. So many changes happened per release that some user-
visible breakage was bound to be hiding in the software. Effectively, the reliability
data from the previous release was irrelevant for the next release.
Effective API/ABI management tools and interpreted languages that scale to large
amounts of code now support building and executing a new software version every
few minutes. In principle, a sufficiently large army of humans15 could complete test‐
ing on each new version using the methods described earlier and achieve the same
quality bar for each incremental version. Even though ultimately only the same tests
are applied to the same code, that final software version has higher quality in the
resulting release that ships annually. This is because in addition to the annual ver‐
sions, the intermediate versions of the code are also being tested. Using intermediates,
you can unambiguously map problems found during testing back to their underlying
causes and be confident that the whole issue, and not just the limited symptom that
was exposed, is fixed. This principle of a shorter feedback cycle is equally effective
when applied to automated test coverage.
If you let users try more versions of the software during the year, the MTBF suffers
because there are more opportunities for user-visible breakage. However, you can
also discover areas that would benefit from additional test coverage. If these tests are
implemented, each improvement protects against some future failure. Careful relia‐
bility management combines the limits on uncertainty due to test coverage with the
limits on user-visible faults in order to adjust the release cadence. This combination
maximizes the knowledge that you gain from operations and end users. These gains
drive test coverage and, in turn, product release velocity.
If an SRE modifies a configuration file or adjusts an automation tool’s strategy (as
opposed to implementing a user feature), the engineering work matches the same
conceptual model. When you are defining a release cadence based on reliability, it
often makes sense to segment the reliability budget by functionality, or (more con‐
15 Perhaps acquired through Mechanical Turk or similar services.
Testing at Scale | 199
veniently) by team. In such a scenario, the feature engineering team aims to achieve a
given uncertainty limit that affects their goal release cadence. The SRE team has a
separate budget with its own associated uncertainty, and thus an upper limit on their
release rate.
In order to remain reliable and to avoid scaling the number of SREs supporting a ser‐
vice linearly, the production environment has to run mostly unattended. To remain
unattended, the environment must be resilient against minor faults. When a major
event that demands manual SRE intervention occurs, the tools used by SRE must be
suitably tested. Otherwise, that intervention decreases confidence that historical data
is applicable to the near future. The reduction in confidence requires waiting for an
analysis of monitoring data in order to eliminate the uncertainty incurred. Whereas
the previous discussion in “Testing Scalable Tools” on page 193 focused on how to
meet the opportunity of test coverage for an SRE tool, here you see that testing deter‐
mines how often it is appropriate to use that tool against production.
Configuration files generally exist because changing the configuration is faster than