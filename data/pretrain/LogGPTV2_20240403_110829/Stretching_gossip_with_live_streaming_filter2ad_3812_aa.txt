title:Stretching gossip with live streaming
author:Davide Frey and
Rachid Guerraoui and
Anne-Marie Kermarrec and
Maxime Monod and
Vivien Qu&apos;ema
Stretching Gossip with Live Streaming
Davide Frey, Rachid Guerraoui, Anne-Marie Kermarrec, Maxime Monod,
Vivien Quéma
To cite this version:
Davide Frey, Rachid Guerraoui, Anne-Marie Kermarrec, Maxime Monod, Vivien Quéma. Stretching
Gossip with Live Streaming. DSN 2009, Jun 2009, Estoril, Portugal. inria-00436130
HAL Id: inria-00436130
https://hal.inria.fr/inria-00436130
Submitted on 26 Nov 2009
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.
L’archive ouverte pluridisciplinaire HAL, est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Stretching Gossip with Live Streaming
Davide Frey§
Rachid Guerraoui†
Anne-Marie Kermarrec§
Maxime Monod†∗
Vivien Qu´ema‡
† Ecole Polytechnique F´ed´erale de Lausanne
§ INRIA Rennes-Bretagne Atlantique
‡ CNRS
Abstract
Gossip-based information dissemination protocols are
considered easy to deploy, scalable and resilient to network
dynamics. They are also considered highly ﬂexible, namely
tunable at will to increase their robustness and adapt to
churn. So far however, they have mainly been evaluated
through simulation, very often assuming ideal settings.
Instead, in this paper, we report on an extensive study of
gossip protocols, deployed on a 230 Planetlab node testbed,
in the context of a challenging video streaming applica-
tion in environments with constrained bandwidths. More
precisely, we assess the impact of varying the well known
knobs of gossip, fanout and refresh rate, in various upload-
bandwidth distributions and churn. Our results show that in
such challenging contexts, the performance of gossip proto-
cols may be hampered by high fanout values. We also show
that the more proactive a gossip protocol, the better it copes
with churn. For instance, when 20% of the nodes simultane-
ously crash, 70% of the remaining nodes do not suffer any
loss in stream quality, while the others only experience a
performance decrease for an average of 5 seconds around
the churn event.
1. Introduction
Gossip-based protocols [1, 3, 4, 9] are well known for
their robustness and ability to cope with network dynamics.
A lot of efforts have thus been devoted to their theoretical
analysis and evaluation [6, 11]. It was for instance shown
that the number of communication partners (called fanout)
in a system of size n, should be greater than a threshold
value ln(n) for the gossip dissemination to reach all nodes
with high probability [7].
Gossip protocols have also been shown to be effective
∗Maxime Monod has been partially funded by the Swiss National Sci-
ence Foundation with grant 200021-113825.
for challenging applications like live streaming [2]. Yet,
most of the work evaluating gossip has considered ideal set-
tings; e.g., unconstrained bandwidth, no (or uniform) mes-
sage loss, global knowledge about the state of all nodes.
Evaluations conducted through simulation [2, 4, 10] also as-
sume that key parameters of gossip, such as fanout and gos-
sip rate, can be arbitrarily tuned to improve robustness and
to adequately adapt to network dynamics. Moreover, excep-
tions of gossip experiments in real settings assume inﬁnite
bandwidth [9], or consider applications with low bandwidth
needs, such as membership maintenance [5].
In the presence of heterogeneous bandwidth and greedy
applications, we have no evidence that gossip will fulﬁll its
promises. The impact of the key parameters of gossip proto-
cols in such contexts remains unexplored. For instance, the
impact of varying the fanout, an obvious knob to tune the
robustness of a gossip protocol, has never been determined.
The rate at which the gossip partners are changed re-
ﬂects the proactiveness of a gossip protocol. The impact
of varying this rate has never been studied either. At one
extreme, one might consider a gossip protocol where nodes
change their neighbors at every communication step (this
is the scheme typically considered in theoretical studies).
At the other extreme, nodes would never change their com-
munication partners unless they notice malfunctions; this
is typically the approach underlying mesh-based systems
where the unstructured overlay used to create random or
deterministic dissemination trees, once constructed, is kept
as is until the mesh connectivity is reduced (e.g., a node
has less than a given number of neighbors), or a node feels
it is incorrectly fed [8]. A wide range of schemes can be
considered between these two extremes.
In this paper, we report on extensive experimentation
on gossip protocols in a realistic environment and in the
context of greedy applications with respect to bandwidth,
namely video streaming applications. We evaluate the im-
pact of the key parameters of gossip protocols with various
churn scenarios and bandwidth capabilities. Finally, we do
not consider any repair mechanism or underlying overlay
structure, thus preserving the inherent simplicity of gossip.
In short, our results show that gossip can be very effec-
tive in greedy and capability-constrained applications, but
only within a small window of parameter ranges. First, the
resulting stream quality is very sensitive to the fanout value.
The power of gossip is unleashed when the fanout is slightly
larger than ln(n) but degrades drastically with higher fanout
values as a result of higher contention. For example, with
a bandwidth cap of 700 kbps for a stream rate of 600 kbps,
gossip reaches its optimal performance with a fanout of 7
(230 nodes). Second, gossip is most effective when the set
of communication partners is continuously changing, par-
ticularly in the presence of churn. When 20% of the nodes
simultaneously crash, 70% of the remaining nodes do not
suffer any loss in quality, while the remaining ones only ex-
perience a performance decrease for an average of 5 around
the churn event. These numbers drop to 30% when commu-
nication partners are refreshed only every 2 gossip rounds.
Finally, our results show that allowing nodes to change
their incoming communication partners explicitly by means
of periodic requests to be fed by a new set of nodes does not
provide any improvement over standard gossip techniques.
This further conﬁrms the strength of gossip protocols in
their simplest form.
2. Gossip-based content dissemination
Consider a set of n nodes, and an event e to be dis-
seminated in the system: e typically consists in a payload
and an id. Gossip-based content dissemination protocols
generally follow a three-phase push-request-push pattern as
depicted in Algorithm 1. The protocol follows an infect-
and-die model [7]. Each node periodically contacts a ﬁxed
number, f (fanout), of nodes chosen according to the se-
lectNodes function and proposes them a set of event ids (of
the events it has received) with a [PROPOSE] message (line
5 for the broadcaster and 6 for other nodes). Upon re-
ceipt of a [PROPOSE], nodes pull the content needed with a
[REQUEST] message to the proposing peer. The peer being
pulled then sends back the actual content (the event) in a
[SERVE] message that contains the requested events. Such
a protocol leverages the reliability of gossip to disseminate
the event ids while avoiding redundancy in content dissem-
ination, which would increase the consumed bandwidth.
3. Tailoring gossip-based dissemination
While the protocol is simple, its parameters may be
tuned to improve efﬁciency, reliability, or resilience to
churn. We explore the parameter space as follows.
Algorithm 1 Standard gossip protocol
Initialization:
1: f := ln(n) + c
2: eventsToPropose := eventsDelivered := requestedEvents := ∅
3: start(GossipTimer(gossipPeriod))
Phase 1 – Push event ids
procedure publish(e) is
4: deliverEvent(e)
5: gossip({e.id})
upon (GossipTimer mod gossipPeriod) = 0 do
6: gossip(eventsToPropose)
7: eventsToPropose := ∅
Phase 2 – Request events
upon receive [PROPOSE, eventsProposed] do
{Infect and die}
if (e.id /∈ requestedEvents) then
for all e.id ∈ eventsProposed do
8: wantedEvents := ∅
9:
10:
11:
12:
13: reply [REQUEST, wantedEvents]
14:
15:
if (e requested less than K times) then
start(RetTimer(retPeriod, eventsProposed))
wantedEvents := wantedEvents ∪ e.id
requestedEvents := requestedEvents ∪ wantedEvents
askedEvents := askedEvents ∪ getEvent(e.id)
for all e.id ∈ wantedEvents do
Phase 3 – Push payload
upon receive [REQUEST, wantedEvents] do
16: askedEvents := ∅
17:
18:
19: reply [SERVE, askedEvents]
upon receive [SERVE, events] do
20:
21:
22:
23:
24: cancel(RetTimer(retPeriod, events))
if (e /∈ eventsDelivered) then
for all e ∈ events do
eventsToPropose := eventsToPropose ∪ e.id
deliverEvent(e)
Retransmission
upon (RetTimer(retPeriod, eventsProposed) mod retPeriod) = 0 do
25: receive [PROPOSE, eventsProposed]
Miscellaneous
function selectNodes(f ) returns set of nodes is
26: return f uniformly random chosen nodes in the set of all nodes
function getEvent(event id) returns event is
27: return the event corresponding to the id
procedure deliverEvent(e) is
28: deliveredEvents := deliveredEvents ∪ e
29: deliver(e)
procedure gossip(event ids) is
30: communicationPartners := selectNodes(f )
31:
32:
for all p ∈ communicationPartners do
send(p) [PROPOSE, event ids]
It has been theoretically shown that a fanout greater than
ln(n) in an infect-and-die model [7] ensures a highly reli-
able dissemination. Theory also assumes that increasing the
fanout results in an even more robust (as the probability to
receive an event id increases) and faster dissemination (as
the degree of the resulting dissemination tree increases). In
practice, however, too high a fanout can negatively impact
performance as heavily requested nodes may exceed their
capabilities in bandwidth constrained environments.
Proactiveness. We deﬁne proactiveness as the rate at
which a node modiﬁes its set of communication partners.
We explore two ways of modifying this set.
Fanout. The fanout is deﬁned as the number of communi-
cation partners each node contacts in each gossip operation.
First, the node may locally refresh its set of communi-
cation partners and change the output of selectNodes every
X calls. In short, when X = 1 the gossip partners of the
node change at every call to selectNodes (i.e., every gossip
period), whereas X = ∞ means that the communication
partners of a node never change.
Second, every Y gossip periods, the node may contact f
random partners asking to be inserted in their views. When
Y = 1, a node A sends a feed-me message to f random
partners every gossip period asking them to feed it. Each
of the random f partners replaces a random node from its
current set of f partners with A.
4. Evaluation
We evaluate the impact of the fanout and proactiveness
of a gossip protocol by deploying a streaming application
based on Algorithm 1 over a set of 230 PlanetLab nodes.
Our implementation is based on UDP and incorporates re-
transmission to recover lost stream packets (lines 14, 15, 25
in Algorithm 1).
Bandwidth constraints. PlanetLab nodes beneﬁt from
high bandwidth capabilities and therefore are not represen-
tative of peers with limited capabilities. We thus artiﬁcially
constrain the upload bandwidths of nodes with three differ-
ent caps: 700 kbps, 1000 kbps and 2000 kbps. To limit mes-
sage loss resulting from bandwidth bursts, our bandwidth
limiter also implements a bandwidth throttling mechanism.
Streaming Conﬁguration. A source node generates a
stream of 600 kbps and proposes it to 7 nodes in all ex-
periments. To provide further tolerance to message loss
(combined with retransmission), the source groups packets
in windows of 110 packets, including 9 FEC coded packets.
The gossip period is set to 200 ms.
Evaluation metrics. We assess the performance of the
streaming protocol along two metrics: (i) the stream lag, de-
ﬁned as the difference between the time at which the stream
is published by the source and the time at which it is actu-
ally delivered to the player on the nodes; and (ii) the stream
quality, which represents the percentage of the stream that is
viewable. A window is jittered if it does not contain enough
packets (i.e., strictly less than 101) to fully reconstruct the
window. A stream with a maximum of 1% jitter means that
at least 99% of all the windows are complete.1
Stream lag and quality are correlated notions: the longer
the acceptable lag, the better the quality. We consider
stream qualities corresponding to several lag values as well
as to an inﬁnite lag, which represents the performance ob-
tainable by a user that downloads the stream for playing at
a later stage, e.g., ofﬂine viewing. Each experiment was run
multiple times. We plotted the most representative one.
4.1. Impact of varying the fanout
We start our analysis by measuring how varying the
fanout impacts each of the two metrics, with a proactiveness
degree of 1 (X = 1). Results are depicted in Figures 1–
3. Figure 1 shows the percentage of nodes that can view
the stream with less than 1% jitter for various stream lags
in a setting where all nodes have their upload capabilities
capped at 700 kbps.
s
e
d
o
n
f
t
o
e
g
a
n
e
c
r
e
P
offline viewing
20s lag
10s lag
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
 10
 20
 30
 40
 50
 60
 70
 80
Fanout
Figure 1: Percentage of nodes viewing the stream with less
than 1% of jitter (upload capped at 700 kbps).
Optimal fanout range. The plot clearly highlights an op-
timal range of fanout values (from 7 to 15 in Figure 1) that
gives the best performance independently of the lag value
considered. Lower fanout values are insufﬁcient to achieve
effective dissemination, while larger values generate higher
network trafﬁc and congestion, thus decreasing the obtain-
able stream quality.
The plot also shows that while the lines corresponding to
ﬁnite lag values have a bell shape without any ﬂat region,
the one corresponding to ofﬂine viewing does not drop dra-
matically until a fanout value of 40. The bandwidth throt-
tling mechanism is in fact able to recover from the conges-
tion generated by large fanout values once the source has
stopped generating new packets. For fanouts above 40, on
the other hand, such recovery does not occur.
1An incomplete window does not mean that the window is unusable.
Using systematic coding, a node receiving 100 out of the 101 original
packets, experiences a 99% delivery in that window.
Critical lag value. A different view on the same set of
data is provided by Figure 2. For each value t, the plot
shows the percentage of nodes that can view at least 99% of
the stream with a lag shorter than t. A fanout in the opti-
mal range (e.g., 7) causes almost all nodes to receive a high
quality stream after a critical lag value (t = 5 s for a fanout
of 7). Moderately larger fanout values cause this critical
value to increase (t = 22 s for a fanout of 20), while for
fanouts above 35, no critical value is present. Rather con-
gestion causes signiﬁcant performance degradation. With a
fanout of 40, only 20% of the nodes can view the stream
with a lag shorter than 60 s, and a lag of 90 s is necessary to
reach 75% of the nodes.
i
)
n
o
i
t
u
b
i
r
t
s
d
e
v
i
t
a
u
m
u
c
(
s
e
d
o
n
l
f
t
o
e
g
a
n
e
c
r
e
P
 100
 80
 60
 40
 20