an independent password sample [7]. The cross entropy is
H1(X ,X (cid:48)) = − n(cid:88)
pi · log p(cid:48)
i ,
i=1
i are the probabilities of xi under X and X (cid:48)
where pi and p(cid:48)
respectively. In our case, X is the correct password distribution
(approximated by the independent password sample) and X (cid:48)
is the distribution as estimated by the language model. Note
that, when the language model is perfect, that means X = X (cid:48),
the cross entropy is minimal and equal to H1(X ). Conversely,
because a lower cross entropy corresponds to a better language
model, it is safe to optimize language models for cross entropy.
Model Order. The model order o governs the strength of the
assumption in Equation 3. For example, o = (cid:96)i gives the unre-
liable maximum-likelihood estimate of password probabilities.
On the other hand, o = 0 assumes that character probabilities
are independent of preceding characters, which leads to robust
but heavily biased estimates.11 In general, the best value for o
depends on the amount of passwords in the sample.
Smoothing. Smoothing methods use prior-assumptions to im-
prove the unreliable probability estimates for rarely occurring
sequences [7]. We use the interpolated Witten-Bell smoothing
method [7], [40] for our experiments, which is suggested for
character-based models [35]. This method blends unreliable
higher-order estimates with more-reliable lower-order ones.
Position-dependency. For the special case of password distri-
butions, we propose to use position-dependent language model.
Position-dependent models account for the different character
distributions the start, middle, and end of sentences.12 This is
done by estimating the conditional character probabilities for
each character position in a password separately. Formally, this
corresponds to adding the requirement j = t to Equation 3.
To the best of our knowledge, we are the ﬁrst
to apply
position-dependent models to passwords. As the results below
show, position-dependent models are superior for estimating
mnemonic password distributions.
11This and similar choices between too complex (o = (cid:96)i) and too
simple (o = 0) are known as bias-variance trade-off in machine learning [2].
12For example, in web sentences of length 8, a total of 21% of the ﬁrst
words start with “t”, but only 8% of the last words do so, too.
7
Since the different sentence corpora and generation rules
lead to password corpora of different sizes, we optimize lan-
guage models for two scenarios: using all available passwords
(for the best strength estimates) and using only a sample of a
speciﬁc size that is reached by most password corpora (for a
fair strength comparison). In order to ensure a safe optimiza-
tion without overﬁtting to the data, we create the language
models13 from passwords from 19 of the 20 ClueWeb12 parts
and evaluate them on the last part that contains mostly web
pages from different domains. Therefore, a smaller entropy
estimate directly corresponds to a better model. Figure 4
(left, center) shows how the entropy estimates decrease with
increasing sample size. In to ensure a fair comparison between
generation rules for which we have different sample sizes, we
use only 2.8 · 107 passwords per password length and rule
when comparing rules (Sections V-A,V-B). We chose this size
so that it is reached for most generation rules.
Furthermore, Figure 4 shows that smoothed position-
dependent models of the highest order perform best, and we
therefore use these models in our experiments in Section V.
As the Figure demonstrates, position-dependent models are
especially advantageous for ASCII passwords, probably due to
the included punctuation that occurs mostly as last characters.
C. Password Distribution Strength Measures
A password-generation rule is stronger when the passwords
it generates are more difﬁcult to guess. However, this difﬁculty
depends largely on the knowledge of the guesser. We employ
the common Kerckhoffs’ principle [22]: since we cannot
estimate the knowledge of the adversary, we use the worst-
case scenario that she knows the full distribution. Even if the
adversary would not know the generation rule, related results
suggest that users employ only very few different rules [42].
The adversary tries to guess by choosing one password,
verifying it, and repeating to choose and verify until the correct
one is found. Since she knows the full password distribution,
she guesses passwords ordered by their probability.
We follow related work on password security and distin-
guish two scenarios: online, where adversaries have a small
number of guesses until the authentication system blocks them,
and ofﬂine, where they are limited only by their time [3].
13We use SRILM v. 1.7.1 (www.speech.sri.com/projects/srilm/) to generate
language models and a custom implementation based on KenLM (kheaﬁeld.
com/code/kenlm/) to get probabilities.
0(cid:215)1071(cid:215)1072(cid:215)1073(cid:215)1074(cid:215)1075(cid:215)1076(cid:215)1077(cid:215)107Model training sample size444648505254Shannon entropy H1ASCIIStandardWith smoothingWith smoothing and position−dependentllllllllllllllllll0(cid:215)1071(cid:215)1072(cid:215)1073(cid:215)1074(cid:215)1075(cid:215)1076(cid:215)1077(cid:215)107Model training sample sizeLowercase letterslllStandardWith smoothingWith smoothing and position−dependentllllllllllll02468Model orderModels trained on 2.8(cid:215)107 passwordslASCIILowercase letterslFor all the measures detailed below, a higher value corre-
sponds to a stronger password distribution.
Min-entropy. The min-entropy models the very extreme case
where the adversary guesses only a single password [3]. The
min-entropy H∞ is a widespread measure to assess distribu-
tions, not only of passwords. It is deﬁned by
H∞(X ) = − log p1
Failure Probability. The failure probability is a measure for
the online scenario. The failure probability λβ reﬂects the aver-
age probability of not guessing a password with β guesses [5].
λβ(X ) = 1 − β(cid:88)
pi
i=1
We report on β = 10 and β = 100 (like [3], [6]).
Work-factor. The α-work-factor is a measure for the ofﬂine
scenario. It models the case where adversaries guess until they
have guessed a fraction α of passwords. The α-work-factor µα
gives the expected number of guesses [32].
µα(X ) = min{β |1 − λβ(X ) ≥ α}
We report on α = 0.5 (like [3], [5]).
Shannon Entropy. The Shannon entropy H1 measures the bits
needed to encode events from a distribution. Unlike the other
strength measures, H1 considers the full distribution. For a
uniform distribution, H1 = H∞, and H1 > H∞ otherwise.
H1(X ) = − n(cid:88)
pi · log pi
(4)
Shannon entropy is usually approximated by the cross entropy
on a held-out password sample (cf. Section IV-B).
i=1
The computational cost of the work-factor µ0.5 makes it
infeasible already for passwords of length 9 or 10, but we ﬁnd
that it strongly correlates with the Shannon entropy H1 in our
case (Figure 5, Pearson’s r = 0.71). H1 has been criticized as
a strength measure for password distributions as it does not
clearly model the ofﬂine scenario [5], [32]. However, due the
observed strong correlation, we see it as a meaningful strength
measure in the case of mnemonic passwords.
Figure 5. Scatter plot of strength estimates for different password generation
rules and sentence corpora by work-factor (logarithmic scale) and Shannon
entropy for passwords of length 8. All
language models are trained on
2.8 · 107 passwords. The dotted line shows an estimated µ0.5 for real-world
passwords [3] and the corresponding H1 according to the model.
8
V. EXPERIMENTS
This section analyzes the strength of mnemonic password
distributions. It addresses the following research questions:
• Which of the password generation rules generates the
strongest password distribution? (Section V-A)
• What effect does sentence complexity have on pass-
•
•
•
word distribution strength? (Section V-B)
Does password distribution strength increase linearly
with password length? (Section V-C)
Security-wise, how far are mnemonic passwords from
uniformly sampled character strings? (Section V-D).
How strong are mnemonic passwords compared to
other password approaches? (Section V-E, V-F)
A. Estimates by Generation Rules
This experiment compares the strength of password distri-
butions from 18 generation rules in terms of common strength
measures (Section IV-C). A password generation rule is an
algorithm which a human can apply to transform a short text
into a password. For this evaluation, we selected rules that
vary by the employed character set, replacement rules, and the
chosen words from the sentence and characters from the words.
The selected rules follow the standard rule of word initials (no
replacement, every word, ﬁrst character) [14], [26], [31], [41]
with some variations to test the effect of such variations on the
reached security level. If not said otherwise, other experiments
use this standard rule. Our implementation of the generation
rules is available open source.14
Character set. The generated passwords consist of either low-
ercase letters (26 characters) or 7-bit visible ASCII characters
(94 characters). Each sentence is processed by a Unicode
compatibility and canonical decomposition and stripped of
diacritical marks. For lowercase passwords, all letters are con-
verted to lowercase. Then, remaining unﬁtting characters are
removed. Punctuation is treated as an own “word” for ASCII
passwords.15 While a larger character set theoretically leads to
stronger passwords, especially users of on-screen keyboards
are tempted to use only lowercase letters as switching to
uppercase or special characters is an extra effort.
Replacement. Sometimes the mnemonic password advice in-
cludes to replace words by similar-sounding characters. To
analyze this advice, we include deterministically replacing
word preﬁxes (like “towards” → “2wards”) as a variant.16
Word. We use either every word or every second word in the
sentence for generating the password. Theoretically, omitting
words increases the difﬁculty of guessing the next character.
Character position. Besides concatenating the ﬁrst characters,
we analyze using the last or both characters as variants. For
one-character words, all three variants use this character once.
14https://github.com/webis-de/password-generation-rules
15We use the ICU4J BreakIterator: site.icu-project.org v. 53.1
16The employed replacements are based on a list of “pronunciation rules”
with the two additional rules of “to” → “2” and “for” → “4”:
blog.codinghorror.com/ascii-pronunciation-rules-for-programmers
lllllllllll25303540106107108109Work factor m0.5Shannon entropy H1lASCII rulesLowercase letter rulesModel26.9Table VI.
MIN-ENTROPY (H∞), FAILURE PROBABILITY (λβ ), AND SHANNON ENTROPY (H1) FOR DIFFERENT PASSWORD-GENERATION RULES,
SORTED BY H1. THE VALUES ARE FOR PASSWORDS OF LENGTH 12 FROM THE WEBIS-SENTENCES-17 (WS) AND WEBIS-SIMPLE-SENTENCES-17 (WSS)
CORPORA WITH MODELS FROM AT MOST 2.8 · 107 PASSWORDS. VALUES FROM FEWER THAN 2.8 · 107 PASSWORDS ARE SHOWN GRAY.
Character set
Password generation rule
Replacement Word
Char. pos.
H∞
WS WSS
13.2
13.8
13.8
13.2
12.5
13.8
13.3
13.8
12.8
13.1
12.4
13.8
13.9
13.3
12.8
11.4
12.8
12.4
12.8
13.1
12.5
13.1
14.0
12.4
12.8
11.4
13.3
12.0
14.0
12.5
9.6
10.3
10.8
9.7
11.5
8.5
λ10
λ100
WS
WSS
0.99958 0.99940
0.99959 0.99940
0.99949 0.99925
0.99960 0.99939
0.99956 0.99938
0.99948 0.99925
0.99960 0.99939
0.99912 0.99928
0.99940 0.99925
0.99955 0.99938
0.99948 0.99929
0.99951 0.99925
0.99912 0.99928
0.99933 0.99941
0.99950 0.99925
0.99708 0.99650
0.99772 0.99715
0.99400 0.99775
WS
WSS
0.99827 0.99753
0.99827 0.99752
0.99760 0.99689
0.99825 0.99745
0.99840 0.99739
0.99759 0.99688
0.99824 0.99744
0.99738 0.99739
0.99775 0.99724
0.99833 0.99735
0.99767 0.99725
0.99759 0.99690
0.99734 0.99738
0.99803 0.99785
0.99757 0.99689
0.99225 0.99098
0.99108 0.98826
0.98634 0.98936
H1
WS WSS
56.7
55.8
53.6
52.9
48.0
49.9
50.0
49.8
48.1
48.5
46.2
47.8
47.6
47.8
45.7
47.3
44.9
46.5
44.7
44.6
43.0
44.6
43.4
42.6
41.8
42.7
41.2
42.6
41.3
42.0
35.3
36.8
35.8
36.1
35.2
34.8
ASCII
ASCII
ASCII
ASCII
ASCII
ASCII
ASCII
ASCII
ASCII
ASCII
ASCII
ASCII
Lowercase letters
Lowercase letters
Lowercase letters
Lowercase letters
Lowercase letters
Lowercase letters
(cid:88)
-
(cid:88)
(cid:88)
-
-
-
-
(cid:88)
-
-
(cid:88)
-
-
-
-
(cid:88)
-
every 2nd
every 2nd
every
every 2nd
every 2nd
every
every 2nd