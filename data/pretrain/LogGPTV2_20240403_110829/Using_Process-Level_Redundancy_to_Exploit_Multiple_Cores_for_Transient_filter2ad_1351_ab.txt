All data which exits the redundant processes must be
compared for correctness before proceeding out of the SoR.
If the output data does not match, a transient fault is de-
tected and a recovery routine is invoked. Any write buffers
which will be passed outside of the SoR must be compared.
Also, any data passed as a system call parameter can be con-
sidered an output event which leaves the SoR and must also
be checked to verify program correctness.
3.2.3 Emulating System Calls
The emulation unit is responsible for the input replica-
tion, output comparison, and system call emulation. The
data transfer during input replication and output comparison
is accomplished through a shared memory segment between
all of the redundant processes.
At the beginning of each call to the emulation unit, the
type of system call is compared to ensure that all redundant
processes are at a common system call.
If not, a fault is
assumed which caused an error in control ﬂow to call an
errant system call.
Depending upon the system call, the system call emula-
tion unit will perform different tasks. System calls which
modify any system state, such ﬁle renaming and linking,
must only be executed once. In other cases, the system call
will be actually called by all processes; once by the mas-
ter process in its original state, and once by each redundant
process to emulate the operation. For example, in emulat-
ing a system call to open a new ﬁle, the master process will
create and open the new ﬁle, while the redundant processes
will simply open the ﬁle without creating it.
3.3 Transient Fault Detection
A transient fault is detected in one of three ways:
1. Output Mismatch: A transient fault which propagates
to cause incorrect output will be detected with the out-
put comparison within the emulation unit at the point
which the data is about to exit the SoR.
2. Watchdog Timeout: There are two scenarios in which
the watchdog timer will time out. The ﬁrst case is
when a fault causes an error in control ﬂow which calls
an errant system call. The faulty process will cause an
entrance into the emulation unit which will begin wait-
ing for the other processes. If the other processes enter
the emulation unit, an error will be detected if the sys-
tem calls mismatch, or if there is a mismatch in data. If
the other processes continue execution, a timeout will
occur. The second case is when a transient fault causes
a process to hang indeﬁnitely (e.g. an inﬁnite loop).
In this case, during the next system call, all the pro-
cesses except the hanging process will enter the emu-
lation unit and eventually cause a watchdog timeout. A
Figure 2. Overview of PLR with three redun-
dant processes.
tion, output comparison, and recovery. The emulation unit
also ensures that the following requirements are maintained
in order for PLR to operate correctly:
• The execution of PLR must be transparent to the sys-
tem environment with the redundant processes inter-
acting with the system as if only the original process
is executing. System calls which alter any system state
can only be executed once with the master process ac-
tually executing the system call and the slave processes
emulating the system call.
• Execution among the redundant processes must be
deterministic.
System calls which return non-
deterministic data, such as a request for system time
or resources, must be emulated to ensure all redundant
processes use the same data for computation.
• All redundant processes must be identical in address
space and any other process-speciﬁc data, such as the
ﬁle descriptor table. At any time, a transient fault could
render one of the redundant processes useless. With
identical processes, any of the processes can be logi-
cally labeled the master process at any given invoca-
tion of the emulation unit.
On occasion, a transient fault will cause the program to
suspend or hang. The watchdog alarm is employed by the
emulation unit to detect such faults. Upon entrance to the
system call emulation unit, a timer begins.
If the redun-
dant processes do not all enter the emulation unit in a user-
speciﬁed amount of time, the watchdog alarm times out,
signaling an error in execution.
3.2.1
Input Replication
As the SoR model dictates, any data which enters the
SoR must be replicated to ensure that all data is redundant
within the SoR. In the case of PLR, any data which passes
into the processes via system calls (such as a read from a
ﬁle descriptor) is received once by the master process, and
then replicated among the slave processes. Also, the return
value from all system calls is considered an input value and
is copied for use across all redundant processes.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:16 UTC from IEEE Xplore.  Restrictions apply. 
Syscall EmulationLibsAppLibsAppLibsAppOperating SystemWatchdogAlarmMasterSlaveProcessProcesses37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007drawback to the watchdog alarm is that a timeout pe-
riod exists in which the application does not make any
progress. In our experience, on an unloaded system, a
timeout of 1-2 seconds is sufﬁcient. The timeout value
is user speciﬁed and can be increased on a loaded sys-
tem. On a loaded system, spurious timeouts will not
affect application correctness, but will cause unneces-
sary calls to the recovery unit.
3. Program Failure: Finally, a transient fault may cause
a program failure due to an illegal operation such as a
segmentation violation, bus error, illegal instruction,
etc. Signals handlers are set up to catch the corre-
sponding signals and an error is be ﬂagged. The next
time the emulation unit is called, it can immediately
begin the recovery process.
3.4 Transient Fault Recovery
Transient fault recovery mechanisms typically ﬁt into
two broad categories: checkpoint and repair, and fault
masking. Checkpoint and repair techniques involves the pe-
riodic checkpointing of execution state. When a fault is de-
tected, execution is rolled back to the previous checkpoint.
Fault masking involves using multiple copies of execution
to vote on the correct output.
PLR supports both types of fault recovery. If checkpoint
and repair functionality already exists, then PLR only needs
to use two processes for detection and can defer recovery to
the repair mechanism. Otherwise, fault masking can be ac-
complished by using at least three processes for a majority
vote. If fault masking is used, the following schemes are
used for recovery (the examples use an assumption of three
redundant processes).
1. Output Mismatch: If an output data mismatch occurs
the remaining processes are compared to ensure cor-
rectness of the output data. If a majority of processes
agree upon the value of the output data, it is assumed
to be correct. The processes with incorrect data are im-
mediately killed and replaced by duplicating a correct
process (e.g. using the fork() system call in Linux).
2. Watchdog Timeout: As mentioned in Section 3.3,
there are two cases for a watchdog timeout.
In the
ﬁrst case, where a faulty process calling the emula-
tion unit while the other processes continue execut-
ing, there will only be one process in the emulation
unit during timeout. The process in the emulation unit
is killed and recovery occurs during the next system
call. In the second case, where a faulty process hangs,
all processes except one will be in the emulation unit
during timeout. The hanging process is killed and re-
placed by duplicating a correct process.
3. Program Failure: In the case of program failure, the
incorrect process is already dead. The emulation unit
simply replaces the missing process by duplicating one
of the remaining processes.
We assume the single event upset (SEU) fault model in
which a single transient fault occurs at a time. However,
PLR can support simultaneous faults by simply scaling the
number of redundant processes and the majority vote logic.
3.5 Windows of Vulnerability
A fault during execution of PLR code may cause an un-
recoverable error. Also, a fault which causes an erroneous
branch into PLR code could result in undeﬁned behavior.
Finally, PLR is not meant to protect the operating system
and any fault during operating system execution may cause
failure. The ﬁrst and third windows of vulnerability may
be mitigated by compiling the operating system and/or PLR
code with compiler-based fault tolerance solutions.
All fault tolerance techniques have windows of vulnera-
bility which are usually associated with faults to the checker
mechanism. Although not completely reliable, partial re-
dundancy [12, 33] may be sufﬁcient to improve reliability
enough to meet user or vendor reliability standards.
3.6 Shared Memory, Interrupts, Exceptions
and Multi-threading
PLR hinges upon deterministic behavior among the re-
dundant processes. However, shared memory, interrupts,
exceptions and multi-threaded applications introduce poten-
tial non-determinism.
Shared memory could be supported by changing page
permissions and trapping upon accesses to the shared mem-
ory. A similar approach is used for detecting self-modifying
code within dynamic code translators [9].
Interrupts and
exceptions present a more difﬁcult challenge because there
is not a clear execution point in which to synchronize the
redundant processes. Hardware supported techniques have
been proposed previously such as hardware counters to sup-
port epochs [8]. Multi-threaded applications require a pro-
gramming model that ensures the same inter-thread mem-
ory ordering for each replica. Without this support, PLR is
limited to executing on single-threaded applications.
These challenges are still open research problems for
all software-implemented fault tolerance techniques. We
plan to explore extensions to PLR to support these non-
deterministic issues.
4 Experimental Results
This paper presents and evaluates a PLR prototype built
using the Intel Pin dynamic binary instrumentation sys-
tem [20]. The tool uses Pin to dynamically create redun-
dant processes and uses PinProbes (a dynamic code patch-
ing system for program binaries) to intercept system calls.
the
SPEC2000 benchmarks compiled with gcc v3.4.6 and ifort
The prototype is evaluated running a set of
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:16 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007v9.0. Fault coverage is evaluated using a fault injection
campaign similar to [29]. One thousand runs are executed
per benchmark. To maintain manageable run times, the test
inputs are used during fault analysis. For each run, an in-
struction execution count proﬁle of the application is used
to randomly choose a speciﬁc invocation of an instruction to
fault. For the selected instruction, a random bit is selected
from the source or destination general-purpose registers. To
inject a simulated transient error, Pin tool instrumentation is
used to change the random bit during the speciﬁed dynamic
execution count of the instruction. The specdiff utility in-
cluded within the SPEC2000 harness is used to determine
the correctness of program output.
Fault propagation and performance evaluation are both
studied using the reference inputs. Performance is mea-
sured by running the PLR prototype with both two and
three redundant processes without fault injection on a four-
processor SMP system; speciﬁcally the system has four
3.00Ghz Intel Xeon MP processors each with 4096KB L3
cache, has 6GB of system-wide memory, and is running
Red Hat Enterprise Linux AS release 4.
4.1 Fault Injection Results
A fault injection study is performed to illustrate the
effectiveness of PLR as well as the beneﬁts of using a
software-centric model of fault detection. Figure 3 shows
the results of a fault injection campaign with the left bar
in each cluster showing the outcomes with just fault injec-
tion and the right bar showing the outcomes when detecting
faults with PLR. The possible outcomes are:
• Correct: A benign fault which does not affect pro-
gram correctness.
• Incorrect: An SDC where the program executes com-
pletely and returns with correct return code, but the
output is incorrect.
• Abort: A DUE in which the program returns with an
invalid return code.
• Failed: A DUE in which the program terminates (e.g.
segmentation violation).
• Mismatch: Occurs when running PLR. In this case, a
mismatch is detected during PLR output comparison.
• SigHandler: Occurs when running PLR. In this case,
a PLR signal handler detects program termination.
Timeouts of the watchdog alarm are ignored because
they occurs very infrequently (∼.05% of the time).
PLR is able to successfully eliminate all of the Failed,
Abort, and Incorrect outcomes. In general, the output com-
parison detects the Incorrect and Abort cases, and turns
each error into detected Mismatch cases. Similarly, PLR de-
tects the Failed cases turning them into SigHandler cases.
Occasionally, a small fraction of the Failed cases are de-
tected as Mismatch under PLR. This indicates cases in
which PLR is able to detect a mismatch of output data be-
fore a failure occurs.
The software-centric approach of PLR is very effective
at detecting faults based on their effect on software exe-
cution. Faults which do not affect correctness are gener-
ally not detected in PLR, thereby avoiding false positives.
In contrast, SWIFT [29], which is currently the most ad-
vanced compiler-based approach, detects roughly ∼70% of
the Correct outcomes as faults.
However, not all of the Correct cases during fault injec-
tion remain Correct with PLR detection as the software-
centric model would suggest. This mainly occurs with
the SPECfp benchmarks.
In particular, 168.wupwise,
172.mgrid and 178.galgel show that many of the original
Correct cases during fault injection become detected as
Mismatch. In these cases, the injected fault causes the out-
put data to be different than data from regular runs. How-
ever, the output difference occurs in the printing of ﬂoating
point numbers to a log ﬁle. specdiff allows for a certain tol-
erance in ﬂoating point calculations, and considers the dif-
ference within acceptable bounds. PLR compares the raw
bytes of output and detects a fault because the data does not
match. This issue has less to do with the effectiveness of a
PLR, or a software-centric model, and is more related to the
deﬁnition of an application’s correctness.
4.2 Fault Propagation
Figure 4 shows the number of instructions executed be-
tween fault injection and detection. Runs are shown as
stacked bars showing the breakdown of instructions exe-
cuted before the fault was detected. The leftmost bar labeled
M shows the breakdowns for the Mismatch runs shown in
Figure 3. The middle bar (S) shows the breakdown for the
SigHandler runs and the left bar (A) shows all of the de-
tected faults including both Mismatch and SegHandler.
In general, the Mismatch runs tend to be detected much
later than the point of fault injection with fault propagation
instruction counts of over 10,000 instructions for nearly all
of the benchmarks. On the other hand, the SegHandler runs
have a higher probability of being detected early. Across all
of the detected runs, there is a wide variety in amounts of
fault propagation ranging from 254.gap which has a low
amount of fault propagation, to 191.fma3d which has an
even distribution of runs among the various categories.
The software-centric model delays the detection of a
fault until an error is certain via program failure, or incorrect
data exiting the SoR. However, the delayed detection also
means that a fault may remain latent during execution for
an unbounded period of time. Future work remains in char-
acterizing fault propagation as well as exploring methods
for bounding the time in which faults remain undetected.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:52:16 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007Figure 3. Results of the fault injection campaign. The left bar in each cluster shows the outcomes
with just fault injection and the right bar shows the breakdown of how PLR detects the faults.