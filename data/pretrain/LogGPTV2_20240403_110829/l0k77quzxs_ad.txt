| 5.3 |3) |parsing. For example, text in a common format (e.g., |
| 5.3 |3) |text of date) may be part of a long dynamic variable || 5.3 |3) |(a task id with its date as part of it). However, the |
| 5.3 |3) |pre-processing step extracts only the texts in the |
| 5.3 |3) |common format, causing the rest of the text in the |
| 5.3 |3) |dynamic variable parsed into a wrong format. |
| 5.3 |3) |Frequently appearing dynamic variables. Some |
| 5.3 |3) |dynamic variables contain contextual information || 5.3 |3) |of system environment and can appear frequently |
| 5.3 |3) |in logs. For example, in Apache logs, the path |
| 5.3 |3) |to a property file often appear in log messages |
| 5.3 |3) |such as: “workerEnv.init() ok /etc/httpd/conf/work- |
| 5.3 |3) |ers2.properties”. Although the path is a dynamic |
| 5.3 |3) |variable, in fact, the value of the dynamic variable || 5.3 |3) |never changes in the logs, preventing Logram from |
| 5.3 |3) |identifying it as a dynamic variable. On the other |
| 5.3 |3) |hand, such an issue is not challenging to address in |
| 5.3 |3) |practice. Practitioners can include such contextual |
| 5.3 |3) |information as part of pre-processing. |
| 5.3 |3) |Efficiency |To measure the efficiency of a log parser, similar to prior research [21], [70], we record the elapsed time to finish the entire end-to-end parsing process on different log data with varying log sizes. We randomly extract data chunks of different sizes, i.e., 300KB, 1MB, 10MB, 100MB, 500MB and 1GB. Specifically, we choose to evaluate the efficiency from the Android, BGL, HDFS, Windows and Spark datasets, due to their proper sizes for such evaluation. From each log dataset, we randomly pick a point in the file and select a data chunk of the given size (e.g., 1MB or 10MB). We ensure8
| Running Time (S) | 1000.0 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Running Time (S) | 1000.0 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma| Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Running Time (S) | 1000.0 | Logram | Drain | Drain | Spell | AEL | IPLoM | Lenma | Running Time (S) | 1000.0 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma| Running Time (S) | 1000.0 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 | Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma
 |
|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|---|| Running Time (S) |50.0 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Running Time (S) |50.0 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Running Time (S) |50.0 |0.3 |1.0 |10.0 |10.0 |10.0 |100.0 |500.0 |Running Time (S) |50.0 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Running Time (S) |50.0 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  || Running Time (S) |5.0 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Running Time (S) |5.0 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Running Time (S) |5.0 |0.3 |1.0 |10.0 |10.0 |10.0 |100.0 |500.0 |Running Time (S) |5.0 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Running Time (S) |5.0 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  || Running Time (S) |0.5 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Running Time (S) |0.5 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Running Time (S) |0.5 |0.3 |1.0 |10.0 |10.0 |10.0 |100.0 |500.0 |Running Time (S) |0.5 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Running Time (S) |0.5 |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  |Logram 	Drain 	Spell 	AEL 	IPLoM 	Lenma  || Running Time (S) |0.5 |0.3 |1.0 |10.0 |100.0 |500.0 |Running Time (S) |0.5 |0.3 |1.0 |10.0 |100.0 |500.0 |Running Time (S) |0.5 |0.3 |1.0 |10.0 |10.0 |10.0 |100.0 |500.0 |Running Time (S) |0.5 |0.3 |1.0 |10.0 |100.0 |500.0 |Running Time (S) |0.5 |0.3 |1.0 |10.0 |100.0 |500.0 || Running Time (S) |0.5 |Size (MB) |Size (MB) |Size (MB) |Size (MB) |Size (MB) |Running Time (S) |0.5 |Size (MB) |Size (MB) |Size (MB) |Size (MB) |Size (MB) |Running Time (S) |0.5 |0.3 |Size (MB) |Size (MB) |Size (MB) |Size (MB) |100.0 |500.0 |Running Time (S) |0.5 |Size (MB) |Size (MB) |Size (MB) |Size (MB) |Size (MB) |Running Time (S) |0.5 |Size (MB) |Size (MB) |Size (MB) |Size (MB) |Size (MB) || Running Time (S) |0.5 |(a) Android |(a) Android |(a) Android |(a) Android |(a) Android |Running Time (S) |0.5 |(b) BGL |(b) BGL |(b) BGL |(b) BGL |(b) BGL |Running Time (S) |0.5 |0.3 |(c) HDFS |(c) HDFS |(c) HDFS |(c) HDFS |100.0 |500.0 |Running Time (S) |0.5 |(d) Windows |(d) Windows |(d) Windows |(d) Windows |(d) Windows |Running Time (S) |0.5 |(e) Spark |(e) Spark |(e) Spark |(e) Spark |(e) Spark |Fig. 4. The elapsed time of parsing five different log data with various sizes. The x and y axes are in log scale.
that the size from the randomly picked point to the end of the file is not smaller than the given data size to be extracted. We measure the elapsed time for the log parsers on a desktop computer with Inter Core i5-2400 CPU 	3.10GHz CPU, 8GB memory and 7,200rpm SATA hard drive running Ubuntu 18.04.2 We compare Logram with three other parsers, i.e., Drain, Spell and AEL, that all have high accuracy in our evaluation and more importantly, have the highest efficiency on log parsing based on the prior benchmark study [21].Results
Logram outperforms the fastest state-of-the-art log parsers in efficiency by 1.8 to 5.1 times. Figure 4 shows that time needed to parse five different log data with various sizes using Logram and five other log parsers. We note that the for Logram, the time to construct the dictionaries is already included in our end-to-end elapsed time, in order to fairly compare log parsing approaches. We find that Logram drastically outperform all other existing parsers. In particular, Logram is 1.8 to 5.1 times faster than the second fastest approaches when parsing the five log datasets along different sizes.The efficiency of Logram is stable when increasing the sizes of logs. From our results, the efficiency of Logram is not observed to be negatively impacted when the size of logs increases. For example, the running time only increases by a factor of 773 to 1039 when we increase the sizes of logs from 1 MB to 1GB (i.e., by a factor of 1,000). Logram keeps a dictionary that is built from the n-grams in log messages. With larger size of logs, the size of dictionary may drastically increasing. However, our results indicate that up to the size of 1GB of the studied logs, the efficiency keeps stable. We consider the reason is that when paring a new log message using the dictionary, the look-up time for an n-gram in our dictionary is consistent, despite the size of the dictionary. Hence even with larger logs, the size of the dictionary do not drastically change.On the other hand, the efficiency of other log parsers may deteriorate with larger logs. In particular, Lenma has the lowest efficiency among all studied log parsers. Lenma cannot finish finish parsing any 500MB and 1GB log dataset within hours. In addition, Spell would crash on Windows and Spark log files with 1G size due to memory issues. AEL shows a lower efficiency when parsing large Windows and BGL logs. Finally, Drain, although not as efficient as Logram, does not have a lower efficiency when parsing larger sizes of logs, which agrees with the finding in prior research on the log parsing benchmark [21].5.4 	Ease of stabilisation
We evaluate the ease of stabilisation by running Logram based on the dictionary from a subset of the logs. In other words, we would like to answer the following question: Can we generate a dictionary from a small size of log data and correctly parse the rest of the logs without updating the dictionary?If so, in practice, one may choose to generate the dictio-nary with a small amount of logs without the need of always updating the dictionary while parsing logs, in order to achieve even higher efficiency and scalability. In particular, for each subject log data set, we first build the dictionary based on the first 5% of the entire logs. Then we use the dic-tionary to parse the entire log data set. Due to the limitation of grouping accuracy found from the last subsection and the limitation of the high human effort needed to manually calculate the parsing accuracy, we do not calculate any accuracy for the parsing results. Instead, we automatically measure the agreement between the parsing result using the dictionary generated from the first 5% lines of logs and the entire logs. For each log message, we only consider the two parsing results agree to each other if they are exactly the same. We then gradually increase the size of logs to build a dictionary by appending another 5% of logs. We keep calculating the agreement until the agreement is 100%. As running the experiments to repetitively parse the logs takes very long time, we evaluated the ease of stabilisation on 14 out of the 16 log datasets and we excluded the two largest log datasets (i.e., the Windows and Thunderbird logs).ResultsLogram’s parsing results are stable with a dictionary generated from a small portion of log data. Figure 5 shows agreement ratio between parsing results from using partial log data to generate an n-gram dictionary and using all log data. The red line in the figures indicates that the agreement ratio is over 90%. In nine out of 14 studied log data sets, our log parser can generate an n-gram dictionary from less than 30% of the entire log data, while having over 90% of the log parsing results the same as using all the logs to generate a dictionary. In particular, one of the large log dataset from Spark gains 95.5% agreement ratio with only first 5% of the log data. On the one hand, our results show that the log data is indeed repetitive. Such results demonstrate that practitioners can consider leveraging the two parts of Logram in separate, i.e., generating the n-gram dictionary (i.e., Figure 2) may not be needed for every log message, while the parsing of each log message (i.e., Figure 3) can depend on a dictionary generated from existing logs.9We manually check the other five log datasets and we find that in all these data sets, some parts of the log datasets have drastically different log events than others. For exam-ple, between the first 30% and 35% of the log data in Linux, a large number of log messages are associated with new events for Bluetooth connections and memory issues. Such events do not exist in the logs in the beginning of the dataset. The unseen logs causes parsing results using dictionary from the beginning of the log data to be less agreed with the parsing results using the the entire logs. However, it is interesting to see that after our dictionary learns the n-grams in that period, the log parsing results become stable. Therefore, in practice, developers may need to monitor the parsing results to indicate of the need of updating the n-gram dictionary from logs.| 1.00 | 0 | 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95100 | 1.00 | 0 | 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95100 | 1.00 | 0 | 5 10 15 20 25 30 35 40 45 50 55 60 65 70 75 80 85 90 95100 |