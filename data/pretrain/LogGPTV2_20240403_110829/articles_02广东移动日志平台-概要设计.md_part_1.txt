**广东移动日志管理平台**
**软件概要设计**
**亚信科技（中国）股份公司**
**2018年8月**
日志分析软件采用高性能、低延时的准实时流式处理，采用分布式、松耦合、可扩展的集群架构，各模块可水平扩展，如果哪个模块成为性能瓶颈，只需要增加运行那个模块的服务器数量。提供开放的RESTful
API接口，可供第三方做二次开发。采用冗余容错的架构，所有模块都冗余部署，某台服务器出问题不会导致系统服务中断。提供完善的系统监控，无论是硬件还是软件故障，都能够及时告警。提供灵活的高可用、高性能分部署大数据接入、存储、处理、检索、展现的平台
##### 大数据
支持大数据相关接口定制开发，将日志分析软件做得像Google搜索引擎一样强大、灵活、易用，对日志进行集中管理，提供实时搜索、关联分析、监控告警、多维统计和数据可视化等功能，帮助企业进行运维监控、安全合规审计及业务数据挖掘。日志易RESTfulAPI提供对日志日志分析软件的定制开发，调用接口,作为日志易企业版的一部分发布，便于用户以灵活的方式集成日志易系统
##### 流式数据接入
###### 2.1通过对接flume流处理接口，实时接收上游系统数据 {#通过对接flume流处理接口实时接收上游系统数据 .list-paragraph}
Flume（http://flume.apache.org/）是Hadoop生态系统的一个开源组件，被用于数据采集和传输用途。供了众多的Source做数据采集（[http://flume.apache.org/FlumeUserGuide.html#flume-sources），常用的Source包括](http://flume.apache.org/FlumeUserGuide.html#flume-sources%EF%BC%89%EF%BC%8C%E5%B8%B8%E7%94%A8%E7%9A%84Source%E5%8C%85%E6%8B%AC%EF%BC%9A)
> TailDir Source，用于实时读取文件
>
> Syslog Source，用于接收syslog数据
>
> Kafka Source，用于从Kafka中读取数据
>
> Exec Source，用于接收其他命令的输出
>
> Spooling Directory Source，用于读取不可变文件
>
> Avro Source，用于接收其他Avro客户端发过来的数据
##### 3 数据库数据接入 {#数据库数据接入 .list-paragraph}
数据库数据DB源支持接入oracle数据库数据，接入prosgrep数据库数据，定时主动获取调用等功能
点击添加数据页面的数据库数据标签，进入添加数据库数据源流程：
在选择数据库连接页面，可以选择一个已有的连接来进行数据库采集，也可以新建一个数据库连接采集：
![增加额外固定配置](media/image1.png){width="5.759722222222222in"
height="2.3430555555555554in"}
如果是新建连接，那么需要按提示填入对应配置，并点击验证连接是否正常，验证成功后保存连接配置，建议起一个方便管理的连接名称。
![增加额外固定配置](media/image2.jpeg){width="5.738888888888889in"
height="4.770138888888889in"}
选中连接后，点击下一步，进入SQL预览页面,在输入框输入采集使用的SQL语句，点击预览，下方表格将会展示若干行数据供预览：
![增加额外固定配置](media/image3.jpeg){width="5.759722222222222in"
height="3.602777777777778in"}
预览正常后，点击继续按钮，配置该数据源的appname，tag信息，以及配置采集频率：
![增加额外固定配置](media/image4.jpeg){width="5.772222222222222in"
height="2.1243055555555554in"}
如果需要增量采集，请点击增量采集tab，配置增量采集字段等相关配置项：
![增加额外固定配置](media/image5.jpeg){width="5.772222222222222in"
height="2.6972222222222224in"}
配置完毕后点击下一步，进行最后检查
![增加额外固定配置](media/image6.jpeg){width="5.552777777777778in"
height="3.5430555555555556in"}
检查无误后，点击下一步，完成数据库数据源的添加
##### 4. 海量数据消息处理队列  {#海量数据消息处理队列 .list-paragraph}
![](media/image7.png){width="5.772916666666666in"
height="4.395833333333333in"}
###### 4.1为提高数据处理健壮性，需要开发一套分布式消息队列系统， {#为提高数据处理健壮性需要开发一套分布式消息队列系统 .list-paragraph}
为提高数据处理健壮性，需要开发一套分布式消息队列系统，分布式消息处理队列用于消息的持久化和缓存。该系统使用磁盘文件做持久化，顺序进行读写，以append方式写入文件。为减少内存copy，集群使用sendfile发送数据，通过合并message提升性能。集群本身不储存每个消息的状态，而使用（consumer/topic/partition）保存每个客户端状态，大大减小了维护每个消息状态的麻烦。在消息推拉的选择上，集群使用拉的方式，避免推的方式下存在的各个客户端的处理能力、流量等不同产生不确定性。以多机形式形成集群，建议3台或3台以上奇数台服务器组建，并且支持分区副本。
###### 4.2 支持万级以上eps数据 {#支持万级以上eps数据 .list-paragraph}
日志软件支持可以通过扩展机器，实现处理万级以上eps数据，在硬件系统资源得到保障的前提下，系统原则上可以无限扩展，数据存入topic，将topic分块拆分分块到集群的各个节点上，因此消息队列文件为分布式存储，支持水平扩展，并且有副本，保障高可用。
###### 4.3 管理原始数据以及正则提取后的数据 {#管理原始数据以及正则提取后的数据 .list-paragraph}
管理原始数据以及正则提取后的数据，数据处理系统基于高性能内存流式计算架构Spark
Streaming，根据配置的规则抽取数据关键字段，将非结构化的数据转换成结构化数据。抽取关键字段的好处是可以对关键字段进行统计分析。日志分析软件将对关键字段及原始数据进行索引，用户可对关键字段及原始数据进行搜索。
日志分析软件已经配置了常见数据的解析规则，对于日志分析软件没有预先配置解析规则的数据，用户可通过后台或Web页面配置解析规则，抽取关键字段。即使没有抽取关键字段，用户仍然可以通过全文检索搜索数据。
日志分析软件在数据做索引之前抽取关键字段，提高了检索的速度。业界有的竞品在检索阶段抽取字段，导致检索速度慢。日志分析软件克服了这个弊端。
##### 5. 海量数据流处理 {#海量数据流处理 .list-paragraph}
![](media/image8.png){width="5.777083333333334in"
height="5.064583333333333in"}
###### 5.1 接收到的数据属于非结构化数据的，进行结构化处理 {#接收到的数据属于非结构化数据的进行结构化处理 .list-paragraph}
日志分析软件支持以下的正则提取规则，将接收到的数据属于非结构化数据的，进行结构化处理，同时每条消息中保存原始日志，通过全文检索可以直接搜索原始日志的内容，同时支持在界面查看上下文，其效果于直接查看原始日志类似
###### 5.2 格式化处理后进行数据聚合，告警和分析 {#格式化处理后进行数据聚合告警和分析 .list-paragraph}
通过日志分析软件的检索系统的
transaction处理能力可以对分散的业务日志进行聚合，归纳出一个完整的业务流程。并可以通过配置项来灵活得自定义业务聚合方式。
####### 5.2.1 Transaction设计 {#transaction设计 .list-paragraph}
一个transaction由一组相关的log组成，比如用户的一次搜索过程对应在整个系统中的所有日志等，
transaction命令将具有相同字段的值组合成一个group，并在单个group内进行transaction的识别
语法
transaction field-list \[maxspan=\\] \[maxevents=int\]
\[maxopentxn=int\]
\[maxopenevents=int\] \[startswith=\\]
\[endswith=\\]
\[contains=\\]
field-list :: field \[,field\]\*
timespan :: \\[s\|m\|h\|d\]
filterstring :: \ \| eval_expression
eval_expression :: eval(bool_expression)
filter_string：
1、\，为双引号括起来的字符串，可使用转义字符，表示单条日志是否包含该字符串
2、 eval(bool-expression), 对单条日志计算表达式的值，返回值true或者false
参数：
maxspan: transaction第一条日志和最后一条日志的最大时间间隔
maxevents ： 单个transaction的最大日志条数
maxopentxn：用于控制内存使用，获取的最多分组，用于过滤的计算
maxopenevents：用于控制内存使用，单个transaction最多从es取的log的条数
startswith : 满足的条件的日志为一个新的transaction的第一条日志
endswith : 满足条件的记录为transaction最后一条日志
contains:
如果transaction中的任何一条日志包含\或者满足bool-expression将保留该transaction，否则丢弃
####### 5.2.2业务数据聚合设计 {#业务数据聚合设计 .list-paragraph}
业务日志通过一系列从start with开始到end
with结束的步骤，可以将其聚合为整个业务。整个功能提供可灵活配置的接口来指定聚合方式的指定。样例如下：
busi_types = \[
{
busi_url =
\"/charge/business.action?BMEBusiness=charge.charge&\_cntRecTimeFlag=true\"
startswith = \"click on \\\"查询\\\"\"
endswith = \"click on \\\"提交\\\"\"
busi_type = \"缴费\"
busi_channel = \"营业厅\"
},
{
busi_url =
\"/custsvc/business.action?BMEBusiness=rec.chgprod&\_cntRecTimeFlag=true\"
startswith = \"click on \\\"套餐变更\\\"\"
endswith = \"click on \\\"确定\\\"\"
busi_type = \"套餐变更\"
busi_channel = \"营业厅\"
},
{
busi_url =
\"/charge/business.action?BMEBusiness=charge.personalBillQry\"
startswith = \"click on \\\"查询\\\"\"
endswith = \"click on \\\"返回\\\"\"
busi_type = \"账单查询\"
busi_channel = \"营业厅\"
}
\]
###### 5.3 海量数据的流式处理是大数据平台数据处理部分的核心，需要支持万级eps数据的处理 {#海量数据的流式处理是大数据平台数据处理部分的核心需要支持万级eps数据的处理 .list-paragraph}
海量数据的流式处理是大数据平台数据处理部分的核心，需要支持万级eps数据的处理，集群由2台及2台以上节点组成，其中一个为主节点，节点通过选举产生，主从节点是对于整个集群内部来说的，从外部来看整个集群，逻辑上是一个整体，与任何一个节点的通信和与整个集群通信是完全一致的。集群自动创建索引，通过配置我们可以非常方便的调整索引分片和索引副本。通过索引分片技术，一个大的索引被拆分成多个，然后分布在不同的节点上，以构成分布式搜索。索引副本的作用一是提供系统的容错性，当摸个节点摸个分片损毁或丢失时，可以从副本中恢复；二是提供查询效率，集群内部会自动实现搜索请求的负载均衡。
###### 5.4 并可以通过设备横项扩展，增加平台的数据接入性能 {#并可以通过设备横项扩展增加平台的数据接入性能 .list-paragraph}
在硬件系统资源得到保障的前提下，系统原则上可以无限扩展，每天的数据存为一个索引文件，拆分分块到集群的各个节点上，因此索引文件为分布式存储，支持水平扩展，并且有副本，保障高可用。
##### 6海量数据搜索引擎 {#海量数据搜索引擎 .list-paragraph}
###### 6.1灵活的对各种类型业务数据进行结构化字段提取 {#灵活的对各种类型业务数据进行结构化字段提取 .list-paragraph}
分析软件支持以下的提取规则，将接收到的数据属于非结构化数据的，进行结构化处理，对各类业务数据灵活提取数据结构
> 1.[正则解析 ](https://www.rizhiyi.com/docs/howtouse/logparse.html#a1)\
> [2.
> KeyValue分解](https://www.rizhiyi.com/docs/howtouse/logparse.html#a2)\
> [3.
> KeyValue正则匹配](https://www.rizhiyi.com/docs/howtouse/logparse.html#a3)\
> [4.
> 数值型字段转换 ](https://www.rizhiyi.com/docs/howtouse/logparse.html#a4)\
> [5. url解码 ](https://www.rizhiyi.com/docs/howtouse/logparse.html#a5)\
> [6. User
> Agent解析 ](https://www.rizhiyi.com/docs/howtouse/logparse.html#a6)\
> [7.
> 时间戳识别 ](https://www.rizhiyi.com/docs/howtouse/logparse.html#a7)\
> [8. geo解析 ](https://www.rizhiyi.com/docs/howtouse/logparse.html#a8)\
> [9.
> JSON解析 ](https://www.rizhiyi.com/docs/howtouse/logparse.html#a9)\
> [10.
> 字段值拆分 ](https://www.rizhiyi.com/docs/howtouse/logparse.html#a10)\
> [11.
> xml解析](https://www.rizhiyi.com/docs/howtouse/logparse.html#a11)\
> [12.
> syslog_pri解析](https://www.rizhiyi.com/docs/howtouse/logparse.html#a12)\
> [13.
> 自定义字典](https://www.rizhiyi.com/docs/howtouse/logparse.html#a13)\
> [14.
> 格式转换](https://www.rizhiyi.com/docs/howtouse/logparse.html#a14)\
> [15.
> 内容替换](https://www.rizhiyi.com/docs/howtouse/logparse.html#a15)
###### 5.2对业务数据进行统计和分析 {#对业务数据进行统计和分析 .list-paragraph}
通过对日志进行结构化提取之后，采用用spl语句可以直接对解析后的字段进行统计和分析
在完成数据接入工作后，对业务数据进行统计和分析：
1.原文检索：主要通过关键字、唯一标识符等低频词元，利用索引中倒排
表的高性能查找特性，快速定位和读取原始日志内容。找错误代码、关联跟踪特定订单或客户访问在多模块之间的流动情况、分析异常堆栈等等。
2.统计可视化：对检索数据集进行一系列指标运算，得到二维表格式的变换结果。常见场景有：错误事件数的时间趋势、特定用户登录行为的排行和分布、访问响应时间的百分比统计、KPI指标的平滑预测等等
###### 5.3支持分布式计算和存储 {#支持分布式计算和存储 .list-paragraph}
![](media/image9.png){width="5.772916666666666in"
height="4.691666666666666in"}