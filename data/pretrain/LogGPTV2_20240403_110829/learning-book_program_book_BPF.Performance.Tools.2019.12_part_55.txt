TCP Retransmits
TCP detects and retransmits lost packets using one of two techniques:
• Timer-based retransmits: These occur when a time has passed and a packet
acknowledgment has not yet been received. This time is the TCP retransmit timeout,
calculated dynamically based on the connection round trip time (RTT). On Linux,
this will be at least 200 ms (TCP_RTO_MIN) for the first retransmit, and subsequent
retransmits will be much slower, following an exponential backoff algorithm that doubles
the timeout.
• Fast retransmits: When duplicate ACKs arrive, TCP can assume that a packet was dropped
and retransmit it immediately.
Timer-based retransmits in particular cause performance issues, injecting latencies of 200 ms and
higher into network connections. Congestion control algorithms may also throttle throughput in
the presence of retransmits.
Retransmits can require a sequence of packets to be resent, beginning from the lost packet,
even if later packets were received correctly. Selective acknowledgments (SACK) is a TCP option
commonly used to avoid this: it allows later packets to be acknowlediged so that they do not need
to be resent, improving performance.
TCP Send and Receive Buffers
TCP data throughput is improved by using socket send and receive buffer accounting. Linux
dynamically sizes the buffers based on connection activity, and allows tuning of their minimum,
default, and maximum sizes, Larger sizes improve performance at the cost of more memory per
connection. They are shown in Figure 10-3.
---
## Page 431
394
Chapter 10 Networking
User Kernel
TCP
IPINet
Data Link
write
MSS
App
Send Buffer
6as6as
Socket
Network
Receive Buffer
Device
read
Figure 10-3 TCP send and receive buffers
Network devices and networks accept packet sizes up to a maximum segment size (MSS) that may
be as small as 1500 bytes. To avoid the network stack overheads of senxding many small packets,
sadns,) azss u1 sa44qy 9 oq dn sqaxped puas o (Os) peogo uopequatu8as sμaua8 sasn d1
packets°), which are split into MSS-sized segments just before delivery to the network device. If
the NIC and driver support TCP segmentation offload (TSO), GSO leaves splitting to the device,
further improving network stack throughput. There is also a generic receive offload (GRO)
adum s OsL pue 'aremgos pauaq u paquouaqdu are Os pue O8 [oot] Os o quauaqduoo
mented by NIC hardware.
TCP Congestion Controls
Linux supports different TCP congestion control algorithms, including Cubic (the default), Reno,
Tahoe, DCTCP and BBR. These algorithms modify send and receive windows based on detected
congestion to keep network connections running optimally.
Queueing Discipline
This optional layer manages traffic classification (tc), scheduling, manipulation, filtering, and
shaping of network packets. Linux provides numerous queueing discipline algorithms, which can
used to list them:
be configured using the tc(8) command. As each has a man page, the man(1) command can be
 nan -k tc-
(e)suot,o-0q
- independently defined actions in tc
tc-basic(8)
- basic traffic control fllter
tc-bfifo (8)
- Packet limited First In, First Out queue
tcbpf (8)
- BPF prograrmable classlfier and actions for 1
queveing disciplines
(e)bqo-3
- Class Based Queueing
tc-cbq-detai1s 18)
- CLass Based Queueing
tc=cbs (8)
- Cred1t Based Shaper (CBS) Qdisc
(e)dnoxBo-2q
- control group based traffic control filter
tc=choke (8)
- choose and keep scheduler
tc=codel (8)
- Controlled-Delay Active Queoe Managenent algorithn
tc=connmark (8)
- netfilter connnark retriever action
tc=csun (8)
- checksun update actlon
---
## Page 432
10.1Background  395
(e)13p-2
- deficit round robin scheduler
tc=ematch (8)
ssttg snoti 1o gotsen uatx ssn xog souogeu pepuegxe -
- flox based traffic controlfi1ter
tc=flovex (8]
- flox based traffic control filtex
(g)bg-
- Fair Queve traffic policing
tc=fq_codel (8)
- Faiz Queuing (FQ) v1th Controlled Delay (CoDe1)
[...]
BPF can enhance the capabilities of this layer with the programs of type
BPF_PROG_TYPE_SCHED_CIS and BPF_PROG_TYPE_SCHED_ACT.
Other Performance Optimizations
including:
aad aaod o yoes xomau a nono asn tu soe sao are aa
 Nagle: This reduces small network packets by delaying their transmission, allowing more to
arrive and coalesce.
 Byte Queue Limits (BQL): These automatically size the driver queues large enough to
avoid starvation, but also small enough to reduce the maximum latency of queued packets.
It works by pausing the addition of packets to the driver queue when necessary, and was
added in Linux 3.3 [95].
 Pacing: This controls when to send packets, spreading out transmissions (pacing) to avoid
bursts that may hurt performance.
● TCP Small Queues (TSQ): This controls (reduces) how much is queued by the network
stack to avoid problems including bufferbloat [101].
• Early Departure Time (EDT): This uses a timing wheel to order packets sent to the
aen pue ood uo paseq 1axoed 1aaa uo as are sduesau1 ananb e jo peasu IN
configuration. This was added in Linux 4.20, and has BQL- and TSQ-like capabilities
[Jacobson 18].
These algorithms often work in combination to improve performance. A TCP sent packet can be
processed by any of the congestion controls, TSO, TSQ, Pacing, and queueing disciplines, before it
ever arrives at the NIC [Cheng 16].
Latency Measurements
Various networking latency measurements can be made to provide insight into performance,
helping to determine whether bottlenecks are in the sending or receiving applications, or the
network itself. These include [Gregg 13b]:
 Name resolution latency: The time for a host to be resolved to an IP address, usually by
DNS resolutiona common source of performance issues.
 Ping latency: The time from an ICMP echo request to a response. This measures the
network and kernel stack handling of the packet on each host.
---
## Page 433
396
6 Chapter 10 Networking
• TCP connection latency: The time from when a SYN is sent to when the SYN,ACK is
received. Since no applications are involved, this measures the network and kernel stack
latency on each host, similar to ping latency, with some aditional kernel processing for
the TCP session. TCP Fast Open (TFO) is a technology to eliminate connection latency for
subsequent connections by providing cryptographic cookie with the SYN to authenticate
the client immediately, allowing the server to respond with data without waiting for the
three-way handshake to complete.
 TCP first byte latency: AIso known as the time-to-first-byte latency (TTFB), this measures
the time from when a connection is established to when the first data byte is received by
the client. This includes CPU scheduling and application think time for the host, making
it a more a measure of application performance and current load than TCP connection
latency.
* Round trip time (RT'T): The time for a network packet to make a round trip between
endpoints. The kernel may use such measurements with congestion control algorithms.
• Connection lifespan: The duration of a network connection from initialization to
close. Some protocols like HTTP can use a keep-alive strategy, leaving connections open
uooauuo paeadau jo ouape ptre speauao at proae o °sqsanbau aunng io api pue
establishment.
Using these in combination can help locate the source of latency, by process of elimination. They
should also be used in combination with other metrics to understand network health, includling
event rates and throughput.
Further Reading
This summarized selected topics as background for network analysis tools. The implementation
of the Linux network stack is described in the kernel source under Documentation/networking
[102], and network performance is covered in more depth in Chapter 10 of Systems Performance
[Gregg 13a]
10.1.2 BPF Capabilities
Traditional network performance tools operate on kernel statistics and network packet captures
BPF tracing tools can provide more insight, answering:
• W'hat socket I/O is occurring, and why? What are the user-level stacks?
● Which new TCP sessions are created, and by which processes?
 Are there socket, TCP, or IP-level errors occurring?
●W'hat are the TCP window sizes? Any zero-size transmits?
 What is the I/O size at different stack layers? To the devices?
• W'hich packets are dropped by the network stack, and why?
What are the TCP connection latency, first byte latency, and lifespans?
---
## Page 434
10.1 Background
•What is the kernel inter-network-stack latency?
How long do packets spend on the qdlisc queues? Network driver queues?
zasn u ane sooqond paaf-raulq seu *
These can be answered with BPF by instrumenting tracepoints when available, and then using
kprobes and uprobes when details beyond tracepoint coverage are needed.
Event Sources
Table 10-1 lists networking targets and the sources that can instrument them.
Table 10-1 Network Events and Sources
Network Event
Event Source
Application protocols
saqoudn
Sockets
syscalls tracepoints
TCP
tcp tracepoints, kprobes
UDP
kprobes
IP and ICMP
kprobes
Packets
skb tracepoints, kprobes
QDiscs and driver queues
qdisc and net tracepoints, kprobes
XDP
xdp tracepoints
 Network device drivers
kprobes
In many cases, kprobes must be used due to a lack of tracepoints. One reason that there are so few
tracepoints is the historical (pre-BPF) lack of demand. Now that BPF is driving demand, the first
TCP tracepoints were added in the 4.15 and 4.16 kernels. By Linux 5.2, the TCP tracepoints are:
+:dos:zuodeoe.T- sezagdg 
tracepointitcp:tcp_retransmit_skb
tracepoint:tcp:tcp_send_reset 
tracepoint1tcp:tcp_receive_rese t
tzacepoint:tcp:tcp_destzoy_sock
tracepoint:tcp:tcp_rcr_space_adjus t
xoeu/s1wsueaeadoq1do:autodeoexa
tracepoint1tcp1tcp_probe
More network protocol tracepoints may be added in future kernels. It may seem obvious to add
send and receive tracepoints for the different protocols, but that involves modifying critical
latency-sensitive code paths, and care must be taken to understand the not-enabled overheads
that such additions would introduce.
---
## Page 435
860
3Chapter 10 Networking
Overhead
shaauas suos uo puooas sad saxped uou leasas Surpaaoxa juanbau aq tue suasa oma
and workloads. Fortunately, BPF originated as an efficient per-packet filter, and adds only a tiny
amount of overhead to each event. Nevertheless, when multiplied by millions or 10 millions of
events per second, that can add up to become a noticeable or even significant overhead.
Fortunately, many observability needs can be met without per-packet tracing, by instead tracing
events that have a much lower frequency and therefore lower overhead. TCP retransmits, for
example, can be traced via the tcp_retransmit_skb( kernel function alone, without needing to
trace each packet. I did this for a recent production issue, where the server packet rate was over
100,000/second, and the retransmit rate was 1000/second. Whatever the overhead was for packet
tracing, my choice of event to trace reduced it one hundred fold.
For times when it is necessary to trace each packet, raw tracepoints (introduced in Chapter 2) are a
more efficient option than tracepoints and kprobes.
A common technique for network performance analysis involves collecting per-packet captures
(tcpxlump(8), libpcap, etc.), which not only adds overhead to each packet but also addlitional CPU,
memory, and storage overheads when writing these packets to the file system, then additional
s Supoen aqoed-ad ddg 'uosredtuo ul fussaood-sod 1oy upele uag Supean uaq speatqaao
already a large eficiency improvement. Because it emits summaries calculated in kernel memory
only, without the use of capture files.
10.1.3 Strategy
If you are new to network performance analysis, here is a suggested overall strategy you can follow.
The next sections explain these tools in more detail.
This strategy begins by using workload characterization to spot ineficiencies (steps 1 and 2), then
checks interface limits (step 3) and different sources of latency (steps 4, 5, and 6). At this point, it
may be worth trying experimental analysis (step 7)bearing in mind, however, that it can interfere
with production workloadsfollowed by more advanced and custom analysis (steps 8, 9, and 10).
1. Use counter-based tools to understand basic network statistics: packet rates and throughput
and, if TCP is in use, TCP connection rates and TCP retransmit rates (e.g., using ss(8),
nstat(8), netstat(1) and sar(1).
2. Trace which new TCP connections are created, and their duration, to characterize the
u no °adsuexa ao (s)ad g Susn a) saouaau aog goo pue peoom
find frequent connections to read a resource from a remote service that can be cached
locally.
3. Check whether network interface throughput limits have been hit (e.g., using sar(1) or
nicstat(1)'s interface utilization percent).
“(s)dopd °(g)suenard g '8a) suaa d1 ensnun sao pue snsuenau d1 aoe1 
and the skb:kfree_skb tracepoint).
5. Measure host name resolution (DNS) latency, as this is a common source of performance
issues (e.g., BCC gethostlatency(8).
---
## Page 436
10.2 Traditional Tools
399
6. Measure networking latency from different points: connection latency, first byte latency,
inter-stack latency, etc.
a. Note that network latency measurements can vary significantly with load due to
uo n 'aqissod g (ouae Suananb asissax jo anss ue) xompau at u peopqanq
be useful to measure these latencies during load, and also for an idle network, for
comparison.
7. Use load-generation tools to explore network throughput limits between hosts, and to
examine network events against a known workload (e.g., using iperf(1) and netperf(1).
8. Browse and execute the BPF tools listed in the BPF tools section of this book
9. Use high-frequency CPU profiling of kernel stack traces to quantify CPU time spent in
protocol and driver processing.
10. Use tracepoints and kprobes to explore network stack internals
10.1.4 Common Tracing Mistakes
Some common mistakes when developing BPF tools for network analysis:
Events may not happen in application context. Packets may be received when the idle thread
is on-CPU, and TCP sessions may be initialized and change state at this time. Examining the
on-CPU PID and process name for these events will not show the application endpoint for
the connection. You need to choose different events that are in application context, or cache
application context by an identifier (e.g., struct sock) that can be fetched later.
•There may be fast paths and slow paths. You may write a program that seems to work, but
is only tracing one of these paths. Use known workloads and ensure that packet and byte
counts match.
 In TCP there are full sockets and non-full sockets: the latter are request sockets before the
three-way handshake has completed, or when the socket is in the TCP TIME_WAIT state
Some socket struct fields may not be valid for non-full sockets.
10.2
2TraditionalTools
Traditional performance tools can display kernel statistics for packet rates, various events, and
throughput and show the state of open sockets. Many such statistics are commonly collected and
graphed by monitoring tools. Another type of tool captures packets for analysis, allowing each
packet header and contents to be studlied.
Apart from solving issues, traditional tools can also provide clues to direct your further use of
BPF tools. They have been categorized in Table 10.2 based on their source and measurement type,
kernel statistics or packet captures.
---
## Page 437
400
Chapter 10 Networking
Table 10-2 Traditional Tools
Tool
Type
Description
5.5
Kernel statistics
Socket statistics
1p
Kermel statistics
IP statistics
n.s ta t
Kermel statistics
Network stack statistics
ne ts ta t
Kernel statistics
Multi-tool for showing network stack statistics and state
sar
Kermel statistics
Mult-tool for showing networking and other statistics
n1cstat
Kermel statistics
Network interface statistics
ethtoo1
Driver statistics
 Network interface driver statistics
tcpdunp
Packet capture
Capture packets for analysis
The following sections summarize key functionality of these observability tools. Refer to their
man pages and other resources, incluxding Systems Performance [Gregg 13a], for more usage and
explanations.
Note that there are also tools that perform experiments for network analysis. These include micro
benchmarks such as iperf(1) and netperf(1), ICMP tools including ping(1), and network route
discovery tools including traceroute(1) and pathchat. There is also the Flent GUI for automat-
ing network tests [103]. And there are tools for static analysis: checking the configuration of the
system and hardware, without necessarily having any workload applied [Elling 0o]., These experi-
mental and static tools are covered elsewhere (e.g., [Gregg 13a]).
The s(8), ip(8), and nstat(8) tools are covered first, as these are from the iproute2 package that is
maintained by the network kernel engineers. Tools from this package are most likely to support
the latest Linux kernel features.
10.2.1SS
ss(8) is a socket statistics tool that summarizes open sockets. The default output provides high-
level information about sockets, for example:
# 55
Metid State
Recv-Q
Send-
Local Address:Port
Peer Address:Port
[..-]
tcp
ESTAB
0
100.85.142.69:65264
100,82,166,11:6001
tcp
ESTAB
D
100-85.142,69:6028
100.,82,16,200:6101
[...]
---
## Page 438
10.2 Traditional Tools
This output is a snapshot of the current state. The first column shows the protocol used by the
sockets: these are TCP Since this output lists all established connections with IP adress informa
tion, it can be used to characterize the current workload, and answer questions including how
many client connections are open, how many concurrent connections there are to a dependency
service, etc.
Much more information is available using options. For example, showing TCP sockets only (t),
with TCP internal info (1), extended socket info (o), process info (p), and memory usage (ns):
1 ss -tiepa
State
Recv-Q Send-Q
Local Adress1Port
Feer Adidress:Fort
8Y153
100.85.142.69:652641
100.82.166.11:6001
usezs: ((*gavaa ,p1d=4195,Ed=10865) ) u1d:33 1no:2009918 sk:78 
sknem: (r0, rb12582912, t0, tb12582912, f266240, v0, o0, b10, d0) ts sack bbt ￥s
cale:9, 9 rto:204 xtt:0.159/0.009 ato:40 nss:1448 pntu:1500 rcvmss:1448 advnss:14
48 cwnd:152 bytes_acked:347681 bytes_received:1798733 segs_out:582 segs_in:1397
data_segs_out :294 data_segs_in:131e bbz: (bv:328,6Mbps,mrtt:0.149, pacing_gain:2. 8
8672, cvnd_gain:2.88672) send 11074 . 0Mbps 1astsnd:1696 1astrcv:1660 1astack:1660
6ci1xsoa sugt:eng pentuTrdde sdak9az eexxesTtop sdaP zz#z eqex bugoed
822 ccv_space:8486T rcv_ssthresh:3609062 minrtt:0 .139
[. - -]
This output includes many details. Highlighted in bold are the endpoint addresses and the follow
eap Su