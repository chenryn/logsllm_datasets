Hi,
I think there are some things in https://github.com/scikit-learn/scikit-
learn/blob/master/sklearn/hmm.py that could be improved.
## _BaseHMM.algorithm handling
  * _BaseHMM.decode has an 'algorithm' argument that _doesn't_ affect decoding algorithm if algorithm is already set in constructor OR if _BaseHMM.fit was called.
  * Getting/setting of an algorithm in _BaseHMM code often doesn't use property (there are direct accesses to _algorithm.
  * It is not possible to change a list of supported algorithms because decoder_algorithms is not an attribute of a class (and it is an immutable tuple), so the purpose of this variable is not clear.
Ideas:
  * Make decode_viterbi and decode_map methods public OR make 'algorithm' argument override default algorithm (this would require changing this argument to None by default).  
I'd prefer making decode_viterbi and decode_map public because flags that
totally change what function does are bad API (see e.g.
http://mail.python.org/pipermail/python-dev/2005-August/055907.html). Is it
correct that "decode" is required in first place because of grid search, etc.?
  * always use a property to access 'algorithm' to make code cleaner;
  * kill 'decoder_algorithms' (and use simple ifs) OR make it class attribute of _BaseHMM.
I don't understand why _BaseHMM.fit sets algorithm.
## Docstrings
  * _BaseHMM._decode_map docstring tells: "Find most likely state sequence corresponding to `obs`". In my understanding this is a bit misleading because (unlike _BaseHMM._decode_viterbi) likelihood of individual states is maximized, not the likelihood of a sequence.
  * `params` and `init_params` are not properly documented. For example, for MultinomialHMM documented 'c' is not valid, and the most interesting 'e' is not documented;
  * module docstring tells us the module is to be removed in scikit-learn 0.11, but it is 0.14dev and it was not removed - what are the plans for sklearn.hmm?
Ideas: fix this.
## fit method
  * There is no supervised `fit` variant for MultinomialHMM.
  * 'obs' parameter name is quite obscure because it means different things in `fit` vs all other methods: it should be a list of sequences for `fit` and an individual sequence in other methods.
  * I don't get a note in _BaseHMM.fit docstring: _BaseHMM shouldn't care about covars_prior, and there is no pruning in current implementation.
Ideas:
  * change _BaseHMM.fit signature to `def fit(self, X, y=None)`
My knowledge is lacking and I'm clueless about GaussianHMM and GMMHMM - does
supervised learning make sense for them?
## Normalization
  * I think there are cases when "hard" zeros in transition and emission matrices are wanted, and there are cases when matrices should be sparse, but sklearn.hmm uncomditionally adds adds EPS to each element of transmat and emissionprob, and this could be an issue.
I don't know if normalization is a real issue - maybe there are better
solutions for sparse problems (like n-gram POS tagging with |tagset| > 1000) -
but sklearn.hmm is currently unusable for such problems at least because of
normalization.
# misc
  * I think default _compute_log_likelihood and _generate_sample_from_state implementations should raise NotImplementedError instead of returning None.
  * tiny cosmetic changes to _hmmc.pyx: kmike@`ae0c4ab` (xrange -> range; unneeded dashes are removed)
* * *
I'm looking for a feedback (and would be happy to submit a pull request with
fixes to some of these issues if this make sense).