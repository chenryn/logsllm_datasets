19
20
21
22
23
24
25
26
}
We also used EPEX to successfully check the ﬁxes for
other CVEs mentioned in Section 1 (CVE-2015-0208,
CVE-2015-0288, CVE-2015-0285,
and CVE-
-2015-0292).
3http://curl.haxx.se/
4https://httpd.apache.org/
5http://lynx.invisible-island.net/
6http://www.mutt.org/
7https://www.gnu.org/software/wget/wget.html
Imprecision in EPEX Analysis
5.6
The 130 potential bugs reported by EPEX includes 28
false positives and incorrectly excludes 20.
In the li-
356  25th USENIX Security Symposium 
USENIX Association
12
braries, most of the false positives appeared due to the
limitations of underlying Clang symbolic analysis en-
gine. The interprocedural analysis supported by Clang’s
symbolic analysis engine is currently limited to the
functions deﬁned within an input source ﬁle or func-
tions included in the ﬁle through header ﬁles. There-
fore, the symbolic analyzer is not able to gather correct
path conditions and return values for the functions de-
ﬁned in other source ﬁles. For example, in the code
below, EPEX reported error since the return value of
X509_get_serialNumber is not checked at line 5.
However, inside the callee, ASN1_STRING_dup, the
error condition is checked at line 17 and the NULL
value is returned. This return value (serial) is fur-
ther checked at line 6. Since, ASN1_STRING_dup is
implemented in a different ﬁle, EPEX could not infer
that the ASN1_STRING_dup call in line 5 will always
return NULL if X509_get_serialNumber returns
an error. Note that if the pattern of not checking error
for X509_get_serialNumber calls were consistent
across all call-sites, EPEX would not have reported this
false positive due to Step-III in Section 3).
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
AUTHORITY_KEYID *v2i_AUTHORITY_KEYID(...)
{
...
serial = ASN1_INTEGER_dup(
X509_get_serialNumber(cert));
if (!isname || !serial) {
X509V3err(...);
goto err;
}
...
}
ASN1_STRING *ASN1_STRING_dup(
const ASN1_STRING *str)
{
}
ASN1_STRING *ret;
if (!str)
return NULL;
...
Given the false positives due to checks by external
functions, a natural solution would be to have all func-
tions validate their input. While this is a good prac-
tice for library programmers, application programmers
should not depend on functions, whose implementation
they often do not control, to follow this practice. For
debugging purposes, it would appear that the function
receiving the invalid return value is at fault. Moreover,
not all functions can cleanly handle invalid input. Com-
parison functions such as ASN1_INTEGER_cmp only
return non-error values, so the only safe response would
be to terminate the program, which is a drastic action that
can easily be averted by checking the parameters in the
ﬁrst place.
5.7 Performance analysis
EPEX is integrated with the test project’s building proce-
dure through the Clang framework. We ran all our tests
on Linux servers with 4 Intel Xeon 2.67GHz processors
and 100 GB of memory, The following table shows the
performance numbers. EPEX’s execution time is compa-
rable to that of other built-in, simple checkers in Clang
(e.g., division-by-zero) as shown in the table below.
Regular
build
Division-by-zero
in-built checker
wolfSSL
mbedTLS
GnuTLS
OpenSSL
cURL
httpd
Lynx
Mutt
Wget
0.05m
0.67m
1.85m
8.25m
0.18m
0.04m
0.55m
0.10m
0.03m
3.08m
3.72m
13.28m
186.9m
13.96m
4.68m
71.35m
13.03m
5.63m
EPEX
checker
2.68m
2.83m
12.82m
132.33m
12.95m
4.51m
71.73m
13.12m
5.66m
6 Related work
EPEX performed the worst in wolfSSL, mostly due to
confusion arising from compile-time conﬁguration set-
tings affecting the function mp_init. EPEX raised 8
alerts for the function, but after contacting the develop-
ers, we learned that the corresponding functions can be
conﬁgured, at compilation time, to be either fallible or
infallible. All the reported call sites were only compiled
if the functions were conﬁgured to be infallible. There-
fore, our error speciﬁcations should not have marked
these functions as fallible. On the other hand, in the ap-
plications, the most frequent causes are 5 instances of
fallbacks, which are characteristic of applications. Still,
missed checks in external functions are the second most
frequent cause, at 3 cases. The remaining causes are al-
ternative error propagation channels, and deliberate dis-
regard for the error, due to either a conscious choice of
the programmer, or a conﬁguration parameter, as men-
tioned in Section 5.2.
6.1 Automated detection of error handling
bugs
Rubio-González et al. [45, 21] detected incorrect error
handling code in the Linux ﬁle system using a static
control and data-ﬂow analysis. Their technique was de-
signed to detect bugs caused by faulty code that either
overwrite or ignore error values.
In addition to these
two cases, we check whether appropriate error values are
propagated upstream as per global error protocol of the
analyzed program. We use module-speciﬁc error spec-
iﬁcations as opposed to hard coded error values like -
EIO, -ENOMEM, etc. used by Rubio-González et al.
This helps us in reducing the number of false positives
signiﬁcantly; for instance, unlike [45, 21], we do not re-
port a bug when an error value is over-written by another
error value that conforms to the global error protocol.
Our usage of symbolic analysis further minimizes false
USENIX Association  
25th USENIX Security Symposium  357
13
positives as symbolic analysis, unlike the data-ﬂow anal-
ysis used in [45, 21], can distinguish between feasible
and infeasible paths.
Acharya et al. [1] automatically inferred error han-
dling speciﬁcations of APIs by mining static traces of
their run-time behaviors. Then, for a different subject
system, they found several bugs in error handling code
that do not obey the inferred speciﬁcations. The static
traces were generated by MOPS [11] that handles only
control dependencies and minimal data-dependencies.
As observed by Acharya et al., lack of extensive data-
dependency support (e.g., pointer analysis, aliasing, etc.)
introduced imprecision in their results. By contrast, our
symbolic execution engine with extensive memory mod-
eling support minimizes such issues. Further, to iden-
tify error handling code blocks corresponding to an API
function, Acharya et al. leveraged the presence of condi-
tional checks on the API function’s return value and/or
ERRNO ﬂag. They assumed that if such a conditional
check leads to a return or exit call, then it is responsible
for handling the error case. Such assumption may lead to
false positives where conditional checks are performed
on non-error cases. Also, as noted by Acharya et al.,
for functions that can return multiple non-error values,
they cannot distinguish them from error cases. By con-
trast, we create our error speciﬁcations from the program
documentation and thus they do not suffer from such dis-
crepancies.
Lawall et al. [31] used Coccinelle, a program match-
to ﬁnd missing error
ing and transformation engine,
checks in OpenSSL. By contrast, we not only look for
error checks but also ensure that the error is indeed han-
dled correctly. This allows us to ﬁnd a signiﬁcantly larger
class of error handling problems. Also, unlike our ap-
proach, Lawall et al.’s method suffers from an extremely
high false positive rate.
Several other approaches to automatically detect er-
ror/exception handling bugs have been proposed for Java
programs [52, 53, 44, 8, 54]. However, since the er-
ror handling mechanism is quite different in Java than
C (e.g., the try-catch-final construct is not sup-
ported in C), these solutions are not directly applicable
to C code.
Static analysis has been used extensively in the past
to ﬁnd missing checks on security critical objects [48,
57, 49]. However, none of these tools can detect miss-
ing/incorrect error handling checks. Complementary to
our work, and other static approaches, dynamic analy-
sis methods have been developed to discover the practi-
cal effects of error handling bugs, although they do so at
the cost of lower coverage of error paths, as well as un-
known failure modes. Fault injection frameworks such
as LFI bypass the problem of the unlikelihood of er-
rors by injecting failures directly into fallible functions.
LFI includes a module for automatically inferring er-
ror speciﬁcations, although it is not usable in our case,
since static analysis requires explicitly identifying error
and non-error values, and not just differentiate between
them [34].
6.2 Symbolic execution
The idea of symbolic execution was initially proposed by
King et al. [29]. Concolic execution is a recent variant of
symbolic execution where concrete inputs guide the ex-
ecution [19, 10, 47]. Such techniques have been used in
several recent projects for automatically ﬁnding security
bugs [27, 46, 22, 20].
KLEE [9], by Cadar et al., is a symbolic execution
engine that has been successfully used to ﬁnd several
bugs in UNIX coreutils automatically. UC-KLEE [40],
which integrates KLEE and lazy initialization [26], ap-
plies more comprehensive symbolic execution over a
bounded exhaustive execution space to check for code
equivalence; UC-KLEE has also been effective in ﬁnd-
ing bugs in different tools, including itself. Recently,
Ramos et al. applied UC-KLEE to ﬁnd two denial-of-
service vulnerabilities in OpenSSL [41].
SAGE, by Godefroid et al. [20], uses a given set of
inputs as seeds, builds symbolic path conditions by mon-
itoring their execution paths, and systematically negates
these path conditions to explore their neighboring paths,
and generate input for fuzzing. SAGE has been success-
fully used to ﬁnd several bugs (including security bugs)
in different Windows applications like media players and
image processors. SAGE also checks for error handling
bugs, but only errors from user inputs, and not environ-
mental failures, which are unlikely to appear when only
user input is fuzzed.
Ardilla, by Kiezun et al. [27], automates testing of
Web applications for SQL injection and cross-site script-
ing attacks by generating test inputs using dynamic taint
analysis that leverages concolic execution and mutates
the inputs using a library of attack patterns.
Existing symbolic execution tools are not well suited
for ﬁnding error handling bugs for two primary reasons:
(i) The existing symbolic execution tools depend on ob-
vious faulty behaviors like crashes, assertion failures,
etc. for detecting bugs. A large number of error han-
dling bugs are completely silent and do not exhibit any
such behavior. (ii) As the number of paths through any
reasonable sized program is very large, all symbolic ex-
ecution tools can only explore a fraction of those paths.
The effects of most non-silent error handling bugs show
up much further downstream from their source. An off-
the-shelf symbolic execution tool can only detect such
cases if it reaches that point. By contrast, our algorithm
for identifying and exploring error paths enables EPEX
358  25th USENIX Security Symposium 
USENIX Association
14
to detect completely silent and non-silent error handling
bugs at their sources. This makes it easy for the develop-
ers to understand and ﬁx these bugs.
6.3 Security of SSL/TLS implementations
Several security vulnerabilities have been found over the
years in both SSL/TLS implementations and protocol
speciﬁcations [15, 43, 2, 5, 7, 4, 3]. We brieﬂy sum-
marize some of these issues below. A detailed survey of
SSL/TLS vulnerabilities can be found in [13].
Multiple vulnerabilities in certiﬁcation validation im-
plementations, a key part of the SSL/TLS protocol, were
reported by Moxie Marlinspike [38, 37, 36, 35]. Similar
bugs have been recently discovered in the SSL imple-
mentation on Apple iOS [24]. Another certiﬁcate val-
idation bug (“goto fail”) was reported in Mac OS and
iOS [30] due to an extra goto statement in the implemen-
tation of the SSL/TLS handshake protocol. The affected
code did not ensure that the key used to sign the server’s
key exchange matches the key in the certiﬁcate presented
by the server. This ﬂaw made the SSL/TLS implemen-
tations in MacOS and iOS vulnerable to active Man-In-
The-Middle (MITM) attackers. This bug was caused by
unintended overlapping of some parts of a non-error path
and an error path. However, this is not an error handling
bug like the ones we found in this paper.
Hash collisions [50] and certiﬁcate parsing discrep-
ancies between certiﬁcate authorities (CAs) and Web
browsers [25] can trick a CA into issuing a valid certiﬁ-
cate with the wrong subject name or even a valid inter-
mediate CA certiﬁcate. This allows an attacker to launch
a successful MITM attack against any arbitrary SSL/TLS
connection.
Georgiev et al. [18] showed that incorrect usage of
SSL/TLS APIs results in a large number of certiﬁ-
cate validation vulnerabilities in different applications.
Fahl et al. [17] analyzed incorrect SSL/TLS API us-
age for Android applications. Brubaker et al. [6] de-
signed Frankencerts, a mechanism for generating syn-
thetic X.509 certiﬁcates based on a set of publicly avail-
able seed certiﬁcates for testing the certiﬁcate valida-
tion component of SSL/TLS libraries. They performed
differential testing on multiple SSL/TLS libraries using
Frankencerts and found several new security vulnerabil-
ities. Chen et al. [12] improved the coverage and efﬁ-
ciency of Brubaker et al.’s technique by diversifying the
seed certiﬁcate selection process using Markov Chain
Monte Carlo (MCMC) sampling. However, all these
techniques are black-box methods that only focus on the
certiﬁcate validation part of the SSL/TLS implementa-
tions. By contrast, our white-box analysis is tailored to
look for ﬂawed error handling code in any sequential C
code.
Flawed pseudo-random number generation can pro-
duce insecure SSL/TLS keys that can be easily compro-
mised [32, 23]. We have also reported several bugs in-
volving pseudo-random number generator functions in
this paper, although their origins are completely differ-
ent, i.e., unlike [32, 23], they are caused by incorrect
error handling.
7 Future work
Automated inference of error speciﬁcations. One lim-
itation of our current implementation of EPEX is that it
requires the input error speciﬁcations to be created manu-
ally by the user. Automatically generating the error spec-
iﬁcations will signiﬁcantly improve EPEX’s usability.
One possible way to automatically infer the error speci-
ﬁcations is to identify and compare the path constraints
imposed along the error paths (i.e., the paths along which
a function can fail and return errors) across different call-
sites of the same function. However, in order to do so, the
error paths must ﬁrst be automatically identiﬁed. This
leads to a chicken-and-egg problem as the current proto-
type of EPEX uses the input error speciﬁcations to iden-
tify the error paths.
To solve this problem, we plan to leverage different
path features that can distinguish the error paths from
non-error paths. For example, error paths are often more
likely to return constant values than non-error paths [33].
Error paths are also more likely to call functions like exit
(with a non-zero argument) than regular code for early
termination. Further, since errors invalidate the rest of
the computation, the lengths of the error paths (i.e., num-
ber of program statements) might be, on average, shorter
than the non-error paths. An interesting direction for fu-
ture research will be to train a supervised machine learn-
ing algorithm like Support Vector Machines (SVMs) [14]
for identifying error paths using such different path fea-
tures. The supervised machine learning algorithm can
be trained using a small set of error and non-error paths
identiﬁed through manually created error speciﬁcations.
The resulting machine learning model can then be used
to automatically identify different error paths and infer
error speciﬁcations by comparing the corresponding path
constraints.
Automatically generating bug ﬁxes. As error-
handling code is often repetitive and cumbersome to
implement, it might be difﬁcult for developers to keep
up with EPEX and ﬁx all the reported bugs manually.
Moreover, manual ﬁxes introduced by a developer might
also be buggy and thus may introduce new error han-
dling bugs.
In order to avoid such issues, we plan to
automatically generate candidate patches to ﬁx the er-
ror handling bugs reported by EPEX. Several recent
projects [55, 39, 28] have successfully generated patches
USENIX Association  
25th USENIX Security Symposium  359
15
for ﬁxing different types of bugs. Their main approach
is dependent on existing test suites—they ﬁrst generate
candidate patches by modifying existing code and then
validate the patches using existing test cases. While this
generic approach can be applied in our setting, we cannot
use the existing schemes as error handling bugs are, in
general, hard to detect through existing test cases. Also,
these approaches typically focus on bug ﬁxes involving
only one or two lines of code changes. However, the
error handling bugs are not necessarily limited to such
small ﬁxes. Solving these issues will be an interesting
direction for future work.
8 Conclusion
In this paper, we presented EPEX, a new algorithm and a
tool that automatically explores error paths and ﬁnds er-
ror handling bugs in sequential C code. We showed that
EPEX can efﬁciently ﬁnd error handling bugs in differ-
ent open-source SSL/TLS libraries and applications with
few false positives; many of these detected bugs lead to
critical security vulnerabilities. We also demonstrate that
EPEX could also be useful to the developers for check-
ing error handling code.
9 Acknowledgments
We would like to thank Ben Livshits, the shepherd of
this paper, and the anonymous reviewers whose sugges-
tions have improved the presentation of our work. We
would also like to thank David Evans for his feedback
on an earlier draft of this paper. This work is spon-
sored in part by Air Force Ofﬁce of Scientiﬁc Research
(AFOSR) grant FA9550-12-1-0162. The views and con-
clusions contained herein are those of the authors and
should not be interpreted as necessarily representing the
ofﬁcial policies or endorsements, either expressed or im-
plied, of AFOSR.
References
[1] M. Acharya and T. Xie. Mining API Error-Handling Speciﬁca-
tions from Source Code. In International Conference on Funda-
mental Approaches to Software Engineering (FASE), 2009.
[2] N. AlFardan and K. Paterson. Lucky thirteen: Breaking the TLS
and DTLS record protocols. In IEEE Symposium on Security and
Privacy (S&P), 2013.