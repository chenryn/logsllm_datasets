title:Low Latency Geo-distributed Data Analytics
author:Qifan Pu and
Ganesh Ananthanarayanan and
Peter Bod&apos;ık and
Srikanth Kandula and
Aditya Akella and
Paramvir Bahl and
Ion Stoica
Low Latency Geo-distributed Data Analytics
Qifan Pu1,2, Ganesh Ananthanarayanan1, Peter Bodik1
Srikanth Kandula1, Aditya Akella3, Paramvir Bahl1, Ion Stoica2
1Microsoft Research 2University of California at Berkeley
3University of Wisconsin at Madison
ABSTRACT
Low latency analytics on geographically distributed dat-
asets (across datacenters, edge clusters) is an upcoming
and increasingly important challenge. The dominant
approach of aggregating all the data to a single data-
center signiﬁcantly inﬂates the timeliness of analytics.
At the same time, running queries over geo-distributed
inputs using the current intra-DC analytics frameworks
also leads to high query response times because these
frameworks cannot cope with the relatively low and
variable capacity of WAN links.
We present Iridium, a system for low latency geo-distri-
buted analytics.
Iridium achieves low query response
times by optimizing placement of both data and tasks
of the queries. The joint data and task placement op-
timization, however, is intractable. Therefore, Iridium
uses an online heuristic to redistribute datasets among
the sites prior to queries’ arrivals, and places the tasks
to reduce network bottlenecks during the query’s ex-
ecution. Finally,
it also contains a knob to budget
WAN usage. Evaluation across eight worldwide EC2 re-
gions using production queries show that Iridium speeds
up queries by 3× − 19× and lowers WAN usage by
15% − 64% compared to existing baselines.
CCS Concepts
•Computer systems organization → Distributed
Architectures; •Networks → Cloud Computing;
Keywords
geo-distributed;
aware; WAN analytics
low latency; data analytics; network
Permission to make digital or hard copies of all or part of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for proﬁt or commercial advantage and that copies bear this notice
and the full citation on the ﬁrst page. Copyrights for components of this work
owned by others than ACM must be honored. Abstracting with credit is per-
mitted. To copy otherwise, or republish, to post on servers or to redistribute to
lists, requires prior speciﬁc permission and/or a fee. Request permissions from
permissions@acm.org.
SIGCOMM ’15, August 17 - 21, 2015, London, United Kingdom
c(cid:13) 2015 ACM. ISBN 978-1-4503-3542-3/15/08. . . $15.00
DOI: http://dx.doi.org/10.1145/2785956.2787481
1.
INTRODUCTION
Large scale cloud organizations are deploying dat-
acenters and “edge” clusters globally to provide their
users low latency access to their services. For instance,
Microsoft and Google have tens of datacenters (DCs) [6,
11], with the latter also operating 1500 edges world-
wide [24]. The services deployed on these geo-distributed
sites continuously produce large volumes of data like
user activity and session logs, server monitoring logs,
and performance counters [34, 46, 53, 56].
Analyzing the geo-distributed data gathered across
these sites is an important workload. Examples of such
analyses include querying user logs to make advertise-
ment decisions, querying network logs to detect DoS
attacks, and querying system logs to maintain (stream-
ing) dashboards of overall cluster health, perform root-
cause diagnosis and build fault prediction models. Be-
cause results of these analytics queries are used by data
analysts, operators, and real-time decision algorithms,
minimizing their response times is crucial.
Minimizing query response times in a geo-distributed
setting, however, is far from trivial. The widely-used
approach is to aggregate all the datasets to a central
site (a large DC), before executing the queries. How-
ever, waiting for such centralized aggregation, signiﬁ-
cantly delays timeliness of the analytics (by as much
as 19× in our experiments).1 Therefore, the natural
alternative to this approach is to execute the queries
geo-distributedly over the data stored at the sites.
Additionally, regulatory and privacy concerns might
also forbid central aggregation [42]. Finally, verbose or
less valuable data (e.g., detailed system logs stored only
for a few days) are not shipped at all as this is deemed
too expensive. Low response time for queries on these
datasets, nonetheless, remains a highly desirable goal.
Our work focuses on minimizing response times of
geo-distributed analytics queries. A potential approach
would be to leave data in-place and use unmodiﬁed,
intra-DC analytics framework (such as Hadoop or Spark)
across the collection of sites. However, WAN band-
1An enhancement could “sample” data locally and send only
a small fraction [46]. Designing generic samplers, unfortu-
nately, is hard. Sampling also limits future analyses.
widths can be highly heterogeneous and relatively mod-
erate [43, 47, 48] which is in sharp contrast to intra-DC
networks. Because these frameworks are not optimized
for such heterogeneity, query execution could be dra-
matically ineﬃcient. Consider, for example, a simple
map-reduce query executing across sites.
If we place
no (or very few) reduce tasks on a site that has a large
amount of intermediate data but low uplink bandwidth,
all of the data on this site (or a large fraction) would
have to be uploaded to other sites over its narrow up-
link, signiﬁcantly aﬀecting query response time.
We build Iridium, a system targeted at geo-distributed
data analytics.
Iridium views a single logical analytics
framework as being deployed across all the sites. To
achieve low query response times, it explicitly considers
the heterogeneous WAN bandwidths to optimize data
and task placement. These two placement aspects are
central to our system since the source and destination of
a network transfer depends on the locations of the data
and the tasks, respectively. Intuitively, in the example
above, Iridium will either move data out of the site with
low uplink bandwidth before the query arrives or place
many of the query’s reduce tasks in it.
Because durations of intermediate communications
(e.g., shuﬄes) depend on the duration of the site with
the slowest data transfer, the key intuition in Iridium
is to balance the transfer times among the WAN links,
thereby avoiding outliers. To that end, we formulate
the task placement problem as a linear program (LP)
by modeling the site bandwidths and query character-
istics. The best task placement, however, is still limited
by input data locations. Therefore, moving (or repli-
cating) the datasets to diﬀerent sites can reduce the
anticipated congestion during query execution.
The joint data and task placement, even for a single
map-reduce query, results in a non-convex optimization
with no eﬃcient solution. Hence, we devise an eﬃ-
cient and greedy heuristic that iteratively moves small
chunks of datasets to “better” sites. To determine which
datasets to move, we prefer those with high value-per-
byte; i.e., we greedily maximize the expected reduction
in query response time normalized by the amount of
data that needs to be moved to achieve this reduction.
This heuristic, for example, prefers moving datasets
with many queries accessing them and/or datasets with
queries that produce large amount of intermediate data.
Our solution is also mindful of the bytes transferred
on the WAN across sites since WAN usage has impor-
tant cost implications ($/byte) [53]. Purely minimizing
query response time could result in increased WAN us-
age. Even worse, purely optimizing WAN usage can ar-
bitrarily increase query latency. This is because of the
fundamental diﬀerence between the two metrics: band-
width cost savings are obtained by reducing WAN usage
on any of the links, whereas query speedups are ob-
tained by reducing WAN usage only on the bottleneck
link. Thus, to ensure fast query responses and reason-
able bandwidth costs, we incorporate a simple “knob”
Figure 1: Geo-distributed map-reduce query. The
user submits the query in San Francisco, and the
query runs across Boston, Bangalore and Beijing.
We also show the notations used in the paper at
Bangalore, see Table 1.
that trades oﬀ the WAN usage and latency by limiting
the amount of WAN bandwidth used by data moves and
task execution. In our experiments, with a budget equal
to that of a WAN-usage optimal scheme (proposed in
[53, 54]), Iridium obtains 2× faster query responses.
Our implementation of Iridium automatically estimates
site bandwidths, future query arrivals along with their
characteristics (intermediate data), and prioritizes data
movement of the earlier-arriving queries.
It also sup-
ports Apache Spark queries, both streaming [60] as well
as interactive/batch queries [59]. 2
Evaluation across eight worldwide EC2 regions and
trace-driven simulations using production queries from
Bing Edge, Conviva, Facebook, TPC-DS, and the Big-
data benchmark show that Iridium speeds up queries by
3× − 19× compared to existing baselines that (a) cen-
trally aggregate the data, or (b) leave the data in-place
and use unmodiﬁed Spark.
2. BACKGROUND AND MOTIVATION
We explain the setup of geo-distributed analytics (§2.1),
illustrate the importance of careful scheduling and stor-
age (§2.2), and provide an overview of our solution (§2.3).
2.1 Geo-distributed Analytics
Architecture: We consider the geo-distributed ana-
lytics framework to logically span all the sites. We
assume that the sites are connected using a network
with congestion-free core. The bottlenecks are only be-
tween the sites and the core which has inﬁnite band-
width, valid as per recent measurements [13]. Addi-
tionally, there could be signiﬁcant heterogeneity in the
uplink and downlink bandwidths due to widely diﬀerent
link capacities and other applications (non-Iridium traf-
ﬁc) sharing the links. Finally, we assume the sites have
relatively abundant compute and storage capacity.
Data can be generated on any site and as such, a
dataset (such as “user activity log for application X”)
2https://github.com/Microsoft-MNR/GDA
Core NetworkGlobal ManagerJob QueueSite ManagerMap2Site ManagerMap1Reduce1Site ManagerMap3BangaloreBostonSan FranciscoBeijingI1S1Reduce2could be distributed across many sites. Figure 1 shows
an example geo-distributed query with a logically cen-
tralized global manager that converts the user’s query
script into a DAG of stages, each of which consists of
many parallel tasks. The global manager also coordi-
nates query execution across the many sites, keeps track
of data locations across the sites, and maintains durabil-
ity and consistency of data; durability and consistency,
though, are not the focus of our work.
Each of the sites is controlled by a local site manager
which keeps track of the available local resources and
periodically updates the global manager.
Analytics Queries: Input tasks of queries (e.g., map
tasks) are executed locally on sites that contain their
input data, and they write their outputs (i.e., interme-
diate data) to their respective local sites. Input stages
are extremely quick as a result of data locality [37, 58]
and in-memory caching of data [17, 59].
In a geo-distributed setup, the main aspect dictating
response time of many queries is eﬃcient transfer of
intermediate data that necessarily has to go across sites
(e.g., all-to-all communication patterns). In Facebook’s
production analytics cluster, despite local aggregation
of the map outputs for associative reduce operations [9,
57], ratio of intermediate to input data sizes is still a
high 0.55 for the median query, 24% of queries have this
ratio ≥ 1 (more in §6). Intermediate stages are typically
data-intensive, i.e., their durations are dominated by
communication times [25, 26, 52].
Queries are mostly recurring (“batched”streaming [60]
or “cron” jobs), e.g., every minute or hour. Because of
their recurring nature, we often know the queries that
will run on a dataset along with any lag between the
generation of the dataset and the arrival of its queries.
Some ad hoc analytics queries are also submitted by
system operators or data analysts. Timely completion
of queries helps real-time decisions and interactivity.
Objectives: Our techniques for task and data place-
ment work inside the global manager to reduce query
response time, which is the time from the submission
of a query until its completion. At the same time, we
are mindful of WAN usage (bytes transferred across the
WAN) [53, 54] and balance the two metrics using a sim-
ple knob for budgeted WAN usage.
2.2
Illustrative Examples
While Iridium can handle arbitrary DAG queries, in
this section, we illustrate the complexity of minimiz-
ing the response time of a geo-distributed query using
a canonical map-reduce query. As described above, ef-
ﬁcient transfer of intermediate data across sites is the
key. Transfer duration of “all-to-all” shuﬄes is dictated
by, a) placement of reduce tasks across sites, §2.2.1, and
b) placement of the input data, §2.2.2; since map out-
puts are written locally, distribution of the input data
carries over to the distribution of intermediate data. We
demonstrate that techniques in intra-DC scheduling and
storage can be highly unsuited in the geo-distributed
Symbol Meaning
Ii
Si
α
Di
Ui
ri
T U
i , T D
i
amount of input data on site i
amount of intermediate (map output)
data on site i
selectivity of input stage, Si = αIi
downlink bandwidth on site i
uplink bandwidth on site i
fraction of intermediate (reduce) tasks
executed in site i
ﬁnish time of intermediate data trans-
fer on up and down link of site i
Table 1: Notations used in the paper.
setup. We will also show that techniques to minimize
WAN usage can lead to poor query response times.
For ease of presentation, we consider the links be-
tween the sites and the network core as the only bottle-
necks and assume that IO and CPU operations of tasks
have zero duration. Table 1 contains the notations. In
general, Ii, Si, Di, and Ui represent the query input
data, intermediate (map output) data, downlink and
uplink WAN bandwidths on site i, respectively. The
fraction of intermediate (reduce) tasks on a site i is ri;
we use the term link ﬁnish time to refer to T U
i and T D
i
which represent the time taken to upload and download
intermediate data from and to site i, respectively.
Intermediate Task Placement
2.2.1
Consider a log with the schema (cid:104)timestamp, user_id,
latency(cid:105) recording latencies of user requests. A user’s
requests could be distributed across many sites (say,
when a user represents a large global customer). Our
sample query computes exact per-user median latency
(SELECT user_id, median(latency) GROUP BY user_id).
As we execute a user-deﬁned and non-associative func-
tion, median(), the map tasks output all pairs of (cid:104)user_id,
(cid:104)latency(cid:105)(cid:105), and this intermediate data is shuﬄed across
all sites. Assume the intermediate outputs are half the
input size; selectivity α = 0.5 (Table 1). Every reduce