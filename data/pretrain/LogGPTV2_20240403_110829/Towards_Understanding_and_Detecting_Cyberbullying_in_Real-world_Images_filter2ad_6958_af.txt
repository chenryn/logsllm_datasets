with the images to be considered as cyberbullying, and the
images on their own are mostly non-cyberbullying in nature.
In contrast, our dataset contains a large number of images
that are, on their own capable of causing cyberbullying, which
indicates the images in our dataset are more representative
cyberbullying images in the real world.
H. Capability Analysis of Existing Offensive Image Detectors
In this study, we focus on a deep analysis of the capabilities
of state-of-the-art offensive image detectors with respect to the
cyberbullying factors. Table X summarizes the capabilities of
these detectors pertaining to the cyberbullying factors. In the
following, we discuss in more detail about the capabilities of
each detector and some observations related to the cyberbul-
lying factors.
Factor
Body-pose
Facial emotion
Hand gesture
Threatening
object
Social
Google
API





Yahoo Open
NSFW
Clarifai
NSFW DeepAI
Amazon
Rekognition




















TABLE X: Capabilities of state-of-the-art offensive image
detectors with respect to cyberbullying factors.
We ﬁnd that only Amazon Rekognition has the capability
to detect body-pose. For example, it can indicate whether the
person in an image is turned towards the viewer or at several
angles from the viewer. Next, we ﬁnd that both Google Cloud
Vision API and Amazon Rekognition can detect the facial
emotions of people in an image. The hand gesture factor is
found to be detectable only by the Google Cloud Vision API.
Although Google Cloud Vision API has this capability, we ﬁnd
that it only points out 40.61% of the cyberbullying images due
to hand gestures as likely offensive. On a closer look, we ﬁnd
that the Google Cloud Vision API can not detect certain kinds
of hand gestures, such as the loser sign that are prevalent in
the cyberbullying images, as offensive.
We also ﬁnd that Google Cloud Vision API, DeepAI,
and Amazon Rekognition are capable of detecting threatening
objects, such as guns and knives. We further study the detection
capability of Google Cloud Vision API on two threatening
objects, i.e., guns and knives. We observe that although Google
Cloud Vision API detects these objects in images, it ﬂags down
only certain such images as unsafe or offensive (42.58% of
cyberbullying images with guns and 43.09% of cyberbullying
images with knives). To analyze this observation further, we
inspect the labels produced by Google Cloud Vision API on
images with these objects. We observe that only images that
had blood, wounds, or gore accompanied with an object are
labeled as likely offensive by this detector. However, images
with a visual cyberbullying object directly pointed at
the
viewer or a subject in an image, or the object brandished
in a threatening fashion are missed by this detector. Besides,
we ﬁnd that all the existing offensive image detectors do not
have the capability to detect the social factor of cyberbullying.
Overall, we surmise that the detection capabilities of those
existing offensive image detectors can be expanded based on
the ﬁndings of our work.
VII. DISCUSSION
In this section, we discuss some limitations and potential
enhancements of our work. It should be noted that this work
represents the ﬁrst step towards understanding and identifying
the visual factors of cyberbullying in images, and demonstrate
that it can be effectively detected based on these factors.
Known Biases in MTurk Surveys. We have used Amazon
MTurk as the platform to annotate images in our dataset and to
carry out our user studies. Although MTurk provides a conve-
nient method for researchers to enlist high-quality participants
online, it also has certain well-known issues that may affect
the data collected through it. In the following, we discuss these
14
issues along with how they may have affected the studies in
this work. As MTurk is quite convenient, it follows conve-
nience sampling techniques [72], [43] to enlist participants.
Therefore, some participants may not fully representative of
the entire population that uses the Internet and hence may
not have encountered real-world cyberbullying. In our data
collection, MTurk may have introduced some bias towards US-
based participants. Common method bias [29] could also be
introduced in MTurk studies, wherein self-reported responses
may lead to spurious effects. Besides, participants in our study
may have some inaccurate knowledge of cyberbullying, which
may have caused additional bias in their responses towards our
data collection and user experiments.
Different Contexts of Cyberbullying. Cyberbullying is
a complex issue, having different contexts. The conventional
context of cyberbullying is text-based cyberbullying, which
has been well studied and its factors have been extensively
cataloged by existing work. A step ahead from this conven-
tional context of cyberbullying is the context of cyberbullying
in images, which is the focus of this work. More com-
plex contexts of cyberbullying involve cyberbullying scenarios
associated with both images and text. Further contexts of
cyberbullying involves videos (i.e., image streams and speech),
where we believe our work could also be useful for addressing
cyberbullying in the visual part of the video context. As part
of our future work, we plan to study those more complicated
cyberbullying contexts.
Broadening of Social Factor. In our work, we found
attributes, such as anti-LGBT symbols, under the social fac-
tor were used for cyberbullying in images. Especially, we
found that many images that depicted the anti-LGBT attribute
portrayed defacement of the pride symbol. While anti-LGBT
is an important attribute of the social factor, we note that
there are other attributes under this factor too, such as hate
symbols and memes portraying racism against Black and Asian
communities, sexism against women, and religious bigotry.
In our dataset, we could not ﬁnd images portraying these
other attributes of the social factor. As part of our future
work, we plan to carry out a new study wherein we will
broaden attributes of the social factor, and study their effects
on cyberbullying in images.
Enabling Existing Detectors to Detect Cyberbullying in
Images. We have discussed our ﬁnding (in Section IV) that the
existing state-of-the-art offensive image detectors (e.g. Google
Cloud Vision API, Amazon Rekognition, and Clarifai NSFW)
cannot effectively detect cyberbullying in images. Through
our work, we aim to provide insights into the phenomenon
of cyberbullying in images and potentially facilitate those
existing offensive image detectors to offer the capability for
detecting cyberbullying in images. In this regard, we would
suggest
two possible ways for building such a capability:
(1) training detection models based on new cyberbullying
image datasets (like the dataset we have created); and (2)
adopting multimodal classiﬁers with respect
to the visual
cyberbullying factors (as we have identiﬁed in this work) for
the detection of cyberbullying in images, since we found that
the multimodal classiﬁer is the most effective classiﬁer for
detecting cyberbullying in images based on our measurement.
Adoption and Deployment. Current techniques of pre-
venting cyberbullying in social networks, especially cyberbul-
lying in images is limited to reporting and ﬂagging down
such images and posts by social network users themselves. In
addition to cyberbullying, other online crimes such as online
hate [27], [26], [38], [40], pornography [83], grooming [73]
and trolling [44] have been identiﬁed as dangerous threats.
Preliminary research in the automatic detection of these threats
have gained momentum in recent
times. The multimodal
classiﬁer model explored in our work can be combined with
systems that defend against these other threats to provide an
overall safer online environment. Additionally, the multimodal
classiﬁer can be deployed as a mobile app in mobile devices.
Multi-faceted Detection of Cyberbullying in Images.
Many online social networks (such as Facebook and Instagram)
support multi-faceted information content, such as textual
content accompanying with visual content. In this work, we
have only focused on cyberbullying image factors identiﬁcation
and classiﬁcation. In our future work, we intend to augment
the cyberbullying factors with textual information and study
the role of the combination of visual and textual cyberbullying.
We also intend to study the cyberbullying incidents involving
a combination of images and texts in a sequential fashion, so
that timely intervention can be possible. In this direction, we
intend to discover new factors of cyberbullying involving both
textual and visual information. Another future direction that we
plan on studying is the issue of revenge-porn [28]. This issue
involves a perpetrator who shares revealing or sexually explicit
images or videos of a victim online. Due to its offensive and
harassing nature, revenge-porn is emerging as a new image-
based cyberbullying issue. This issue may be characterized by
speciﬁc factors that are different from traditional pornography,
due to which current offensive content detectors may mis-
classify images with this issue. As future work, we intend to
study this issue and discover its factors, so that the existing
offensive content detectors can be made capable of detecting
it in online images.
Adversarial Manipulation of Predictions. Another direc-
tion that we intend to explore is the protection of deep-learning
based classiﬁers from adversarial attacks [20], [65]. These
attacks are speciﬁcally crafted to “fool” deep learning based
systems into outputting erroneous predictions. Speciﬁcally, we
intend to further explore adversarial manipulations that are
aimed at compromising multimodal classiﬁer-based systems.
Since our current work and future work would use multimodal
machine learning for detecting cyberbullying in images and for
intervention, we believe it is highly important to make such
models more resistant to such attacks.
Ethical Issues. Our deep learning models have been trained
on our dataset of cyberbullying images and our data collection
task has been approved by IRB. We intend to make our
dataset publicly available. However, we have also found that
our dataset may contain some potentially extremely sensitive
images, such as images with great violence against children.
Therefore, we plan to exclude such extremely sensitive images
from our shared dataset. Furthermore, in this paper, we have
attached a few samples of cyberbullying images to illustrate
certain concepts so that readers can better understand our
paper. We have applied masks over the human subjects’ eyes
in all attached images to protect their privacy. We do not intend
to distribute any sensitive images or leak the human subjects’
privacy.
15
VIII. RELATED WORK
Cyberbullying is a critical social problem that has been
actively researched, especially by the psychology, social, and
behavioral science communities. Recently, cyberbullying re-
search has also attracted attention from the computer science
community, and there has been a signiﬁcant amount of research
dedicated to studying the detection of cyberbullying, with an
emphasis on textual cyberbullying. In this work, we focus on
the understanding and detection of cyberbullying in images.
There has been signiﬁcant research in understanding the
psychological and social aspects of cyberbullying. The study
in [67] discusses early work in cyberbullying, including the
nature of cyberbullying in online and social media environ-
ments. The study in [76] reveals that cyberbullying in images
is especially harmful among the other types of cyberbullying
discussed in this work. Methods introduced in [35] approach
the problem of cyberbullying differently, by using bystander
intervention strategies in social media networks. Many works
discuss the deﬁnition of cyberbullying, although there is no
universally accepted deﬁnition of cyberbullying currently [55],
[62]. For example, a study [77] deﬁnes cyberbullying as “an
aggressive, intentional act carried out by a group or individual,
using electronic forms of contact, repeatedly and over time
against a victim who can not easily defend him of herself”.
However, the concept of repetition is questioned by many
studies [55], [62], [57] in the ﬁeld of cyberbullying. A major
limitation of these studies is that they do not discuss any
practical methods to defend against cyberbullying online.
Several automatic methods of cyberbullying defense that
target text-based cyberbullying have emerged [82], [36], [71].
The work in [82] presents a machine learning approach to
detect cyberbullying using textual content such as comments
and social media post descriptions. Another automatic ap-
proach to detect textual cyberbullying is presented in [37],
in which the authors present topic sensitive binary classiﬁers
to detect cyberbullying in YouTube comments. The discussion
of the language factors involved in textual cyberbullying and
contextual factors of cyberbullying events in social media
is presented in [54]. A recent study [80] elaborates on an
approach that
incorporates the use of hashtags, emotions
and spatio-temporal features to detect textual cyberbullying.
Another recent study [84] explores the enhancement of word
embedding of cyberbullying texts, by using an embedding
enhanced bag-of-words features set. Other works have also
suggested the use of meta data to improve the prediction of
textual cyberbullying [32], [52], [75]. However, these studies
only partially addresses the problem of cyberbullying, as
cyberbullying involves several different forms of media, such
as images, in addition to text.
IX. CONCLUSION
In this paper, we study the phenomenon of cyberbullying
in images, speciﬁcally its factors and classiﬁcation based on
those factors. We have discussed how images can have cyber-
bullying content due to the highly contextual visual factors. We
have introduced our approach for the identiﬁcation of visual
cyberbullying factors in images. We have found that visual
cyberbullying involves ﬁve factors, body-pose, facial emotion,
gesture, object and social factors. We have examined four
classiﬁer models that can detect visual cyberbullying based
on the identiﬁed factors at different levels of abstractions.
Among these four classiﬁer models, the multimodal classiﬁer
performed the best, since it is based on both images features
and visual factors based features. We have evaluated the effec-
tiveness of the identiﬁed visual factors and conducted studies
to examine the performance of the four classiﬁer models. Our
analysis, which demonstrates that multimodal classiﬁcation
approach is best suited for detecting cyberbullying in images,
is an important ﬁnding to achieve detection capability for
cyberbullying in images.
ACKNOWLEDGMENT
This work is supported in part by the National Science
Foundation (NSF) under the Grant No. 2031002, 1846291,
1642143, and 1700499.
REFERENCES
[1] Flickr. https://www.ﬂickr.com.
[2] Pinterest. https://www.pinterest.com/.
[3] Amazon Comprehend, 2020. https://aws.amazon.com/comprehend/.
[4] Amazon Rekognition, 2020. https://aws.amazon.com/rekognition/.
[5] Clarifai, 2020. https://www.clarifai.com/.
[6] Cyberbullying: one in two victims suffer from the distribution of
embarrassing photos and videos, 2020. www.sciencedaily.com/releases/
2012/07/120725090048.htm.
[7] Cyberbullying Stories, 2020. https://cyberbullying.org/stories.
[8] DeepAI, 2020. https://deepai.org/.