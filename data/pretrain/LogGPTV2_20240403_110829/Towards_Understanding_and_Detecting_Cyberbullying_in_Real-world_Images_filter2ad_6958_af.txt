### Capability Analysis of Existing Offensive Image Detectors

In this study, we conduct a deep analysis of the capabilities of state-of-the-art offensive image detectors with respect to various cyberbullying factors. Table X summarizes the capabilities of these detectors in relation to the identified cyberbullying factors. In the following sections, we provide a detailed discussion of each detector's capabilities and relevant observations.

#### Cyberbullying Factors
- **Body-pose**
- **Facial emotion**
- **Hand gesture**
- **Threatening object**
- **Social**

| Detector                | Body-pose | Facial Emotion | Hand Gesture | Threatening Object | Social |
|-------------------------|-----------|----------------|--------------|--------------------|--------|
| Google Cloud Vision API | ✗         | ✓              | ✓            | ✓                  | ✗      |
| Yahoo Open NSFW         | ✗         | ✗              | ✗            | ✗                  | ✗      |
| Clarifai NSFW           | ✗         | ✗              | ✗            | ✓                  | ✗      |
| DeepAI                  | ✗         | ✗              | ✗            | ✓                  | ✗      |
| Amazon Rekognition      | ✓         | ✓              | ✗            | ✓                  | ✗      |

**Table X: Capabilities of state-of-the-art offensive image detectors with respect to cyberbullying factors.**

#### Detailed Analysis

1. **Body-pose Detection:**
   - **Amazon Rekognition** is the only detector capable of identifying body poses. For example, it can determine if a person in an image is facing the viewer or at different angles.
   
2. **Facial Emotion Detection:**
   - Both **Google Cloud Vision API** and **Amazon Rekognition** can detect facial emotions in images.

3. **Hand Gesture Detection:**
   - The **Google Cloud Vision API** is the only detector that can identify hand gestures. However, it flags only 40.61% of cyberbullying images due to hand gestures as likely offensive. Further analysis reveals that it fails to recognize certain gestures, such as the "loser" sign, which are common in cyberbullying images.

4. **Threatening Object Detection:**
   - **Google Cloud Vision API**, **DeepAI**, and **Amazon Rekognition** can detect threatening objects like guns and knives. However, the **Google Cloud Vision API** flags only 42.58% of images with guns and 43.09% of images with knives as unsafe or offensive. This detector labels images as offensive primarily when they include blood, wounds, or gore along with the object. It misses images where the object is pointed directly at the viewer or a subject, or brandished in a threatening manner.

5. **Social Factor Detection:**
   - None of the existing detectors have the capability to detect the social factor of cyberbullying, such as hate symbols, anti-LGBT imagery, or other forms of social discrimination.

### Discussion

This section discusses the limitations and potential enhancements of our work. Our study represents a significant step towards understanding and identifying the visual factors of cyberbullying in images and demonstrates that these factors can be effectively detected.

#### Known Biases in MTurk Surveys
- We used Amazon MTurk for annotating images and conducting user studies. While MTurk provides a convenient method for enlisting high-quality participants, it also introduces biases. Convenience sampling techniques may not fully represent the entire internet-using population, and there may be a bias towards US-based participants. Additionally, self-reported responses may introduce common method bias, and participants may have inaccurate knowledge of cyberbullying, leading to further bias in their responses.

#### Different Contexts of Cyberbullying
- Cyberbullying is a complex issue with various contexts. Traditional text-based cyberbullying has been well-studied, but the context of cyberbullying in images, which is the focus of this work, is less explored. More complex contexts involve both images and text, and even more so, videos (image streams and speech). Our future work will address these more complicated contexts.

#### Broadening of Social Factor
- In our dataset, we found that anti-LGBT symbols were used for cyberbullying. However, other attributes under the social factor, such as hate symbols, racist memes, sexism, and religious bigotry, were not represented. Future work will broaden the scope of the social factor and study its effects on cyberbullying in images.

#### Enabling Existing Detectors to Detect Cyberbullying in Images
- We found that existing state-of-the-art offensive image detectors (e.g., Google Cloud Vision API, Amazon Rekognition, and Clarifai NSFW) cannot effectively detect cyberbullying in images. To address this, we suggest two approaches:
  1. Training detection models based on new cyberbullying image datasets.
  2. Adopting multimodal classifiers that consider both visual and textual factors, as we found that multimodal classifiers are the most effective for detecting cyberbullying in images.

#### Adoption and Deployment
- Current techniques for preventing cyberbullying in social networks rely on user reporting. Our multimodal classifier can be integrated into systems to provide a safer online environment. Additionally, it can be deployed as a mobile app. 

#### Multi-faceted Detection of Cyberbullying in Images
- Future work will focus on combining textual and visual information to detect cyberbullying. We also plan to study revenge-porn, a new form of image-based cyberbullying, and develop methods to detect it.

#### Adversarial Manipulation of Predictions
- We will explore protecting deep-learning-based classifiers from adversarial attacks, which are designed to fool these systems into making erroneous predictions. This is crucial for the robustness of our multimodal classifier.

#### Ethical Issues
- Our dataset contains potentially sensitive images, and we plan to exclude extremely sensitive content from public sharing. We have also protected the privacy of human subjects by masking their eyes in all attached images.

### Related Work
- Cyberbullying has been extensively studied in psychology, social, and behavioral sciences. Recent research in computer science has focused on detecting textual cyberbullying. Our work focuses on the visual aspects of cyberbullying, an area that has received less attention.

### Conclusion
- In this paper, we have studied the phenomenon of cyberbullying in images, specifically its factors and classification. We identified five visual factors: body-pose, facial emotion, gesture, object, and social. We evaluated four classifier models and found that the multimodal classifier, which combines image features and visual factors, performed the best. Our findings highlight the importance of a multimodal approach for detecting cyberbullying in images.

### Acknowledgment
- This work is supported in part by the National Science Foundation (NSF) under Grant Nos. 2031002, 1846291, 1642143, and 1700499.

### References
- [1] Flickr. https://www.flickr.com
- [2] Pinterest. https://www.pinterest.com/
- [3] Amazon Comprehend, 2020. https://aws.amazon.com/comprehend/
- [4] Amazon Rekognition, 2020. https://aws.amazon.com/rekognition/
- [5] Clarifai, 2020. https://www.clarifai.com/
- [6] Cyberbullying: One in Two Victims Suffer from the Distribution of Embarrassing Photos and Videos, 2020. www.sciencedaily.com/releases/2012/07/120725090048.htm
- [7] Cyberbullying Stories, 2020. https://cyberbullying.org/stories
- [8] DeepAI, 2020. https://deepai.org/