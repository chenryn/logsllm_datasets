used to measure the relative importance of website pages, and
set the maximum iteration to 100. We use Pokec social network
dataset, each node containing user data about gender, age, hobbies,
interest, education etc [8]. In the dataset, the most influential node
has ID 5935 and the PageRank value is 625.1821405718983. We
assess the real scenario — data owners share their data containing
sensitive data to social network. To protect sensitive data of the
most influential node, data owners customize the access control
policy that the computation of the second column’s value equal to
5935 is banned. As a result, in GuardSpark++, we obtain the most
influential node, ID 5876 with PageRank value 287.1705351204417.
Obviously, the original node ID 5935 has been isolated. Access
control for graph engine is necessary when data owners share their
data and customize policies to prevent sensitive data from being
analyzed by graph algorithms which may dig out more privacy
information about data owners [31, 56].
Structured Streaming. To showcase continuous access con-
6.4.4
trol enforcement for Structured Streaming, we select several data
sources. First, we test network stream source in the common use
case given in [21]. The Netcat (a small utility in Linux) serves
as streaming data server; the following sensitive words groups —
(“Fund”, “Association”) and (“Fund”, “Association”, “Insurance”) —
are respectively sent by port 9999 of localhost; the same words are
counted. We run the use case on GuardSpark++ with access control
policy: the sensitive word “Fund” from port 9999 of localhost cannot
be computed by user. Each experimental result set does not contain
“Fund” after each sending. Second, we leverage file sources (LFS
and HDFS [30]) and Kafka source [54] to conduct similar tests to
the first, and experimental results show that we still can control
which sensitive word cannot be computed. Finally, we construct a
Figure 9: Using Iris dataset and BigDataBench to measure
K-WSSSEs. We run GuardSpark++ to automatically enforce
access control policy (AP) and simulate the policy enforce-
ment by manually deleting corresponding data (MP); we run
Spark without policy enforcement (Spark).
addresses of all stores; the “store_sales” table has sale price of
each commodity. The access control policies protect all sensitive
information from being seen.
To show the assessment, we select the Query35 (Figure 10 in Ap-
pendix A) in the TPC-DS benchmark. This query discloses consump-
tion dependent and some information about living address, gender
and marital status [23]. This query, which contains various data
processing purposes, aliases and sub queries, is complicated enough
for showing our assessment. The assessments on other queries are
announced on the website (https://github.com/liveonearthormars/
SparkSQL-test).
The Figure 11 in Appendix A shows detailed analysis about
the control on data processing purposes. Particularly, we utilize
zero setting logic (Section 5.5) to prevent sensitive data from being
seen while various computation results are correct. For intuitively
demonstrating the control effect, we provide three result sets about
Query35 in Figure 12 in Appendix A. Specifically, the computation
results in Figure 12(b) are consistent with those in Figure 12(c)
because only outputted sensitive information is set to zero accord-
ing to access control policies. However, the computation results
in Figure 12(a) are different from those in both Figure 12(b) and
Figure 12(c) because directly deleting sensitive information from
data source alters computation results that data users expect.
6.4.2 ML Pipelines. We select a clustering algorithm K-means [17]
and regard the Within Set Sum of Squared Error (WSSSE) as the
metric. First, we use Iris dataset [9], and access control policy regu-
lates that the computation of the first column’s value less than or
equal to 5.5 is prohibited, other columns are used without restric-
tion. We run GuardSpark++ to automatically enforce the policy and
obtain the WSSSE set in the case of different K values (2,3,4,5,6). We
2345620406080100120140160(a) Iris WSSSE(MP)  WSSSE(AP)  WSSSE(Spark)WSSSEK2460.05.0x10211.0x10221.5x10222.0x10222.5x10223.0x10223.5x1022(b) BigDataBenchWSSSEKGuardSpark++: Fine-Grained Purpose-Aware Access Control for Secure Data Sharing and Analysis in Spark
ACSAC 2020, December 7–11, 2020, Austin, USA
common logs analysis system, which consists of Sysdig (generating
logs by its default format) [22], flume (collecting logs) [50] and
Kafka (transporting logs to GuardSpark++) [54]. We consider the IP
in logs as sensitive data and forbid IP from being seen. As a result,
IP information cannot be seen in GuardSpark++.
6.4.5 Various data sources. The previous case studies embody these
low-level data sources: LFS, HDFS, network streaming and Kafka —
the low-level data read permission in these sources [7, 16, 58, 65]
is opened. Next, we select MySQL to measure the effectiveness on
relational data source. We use real world data from business domain.
Similar to [37], we consider MyCompany which provides catering
for elderly people (customers). For better services, MyCompany
collects customers’ data, including customer table, address table and
order table (the customer’s “id” is the primary key for each table).
MyCompany puts those tables into MySQL and authorizes our
Spark cluster to access those tables according to the access control
mechanism in MySQL [18]. However, MyCompany customizes the
access control policy in GuardSpark++: the “id” attribute value must
be greater than 10 when users retrieve and output data. In such
case, we use the JDBC APIs in DataFrame to retrieve those tables.
The result set only contains records whose ids are greater than 10.
7 SECURITY ANALYSIS AND DISCUSSIONS
The security of GuardSpark++ relies on the following factors: 1)
GuardSpark++ correctly decomposes the logical plans and recog-
nizes the data processing purposes, 2) GuardSpark++ correctly
enforces access policies that are consistent with the data owner’s
access intentions, and 3) all user queries that need access control
are processed through Catalyst and GuardSpark++.
The correctness of the first two factors is ensured in the design of
GuardSpark++. In particular, GuardSpark++ recognizes the DOPs
(e.g., DOP-R, DOP-C, DOP-A) and the data processing purpose
(DPP) for each data object from its logical plan (Sections 5.2 & 5.3).
According to the restriction of the PAAC policies specified by the
data owners, GuardSpark++ identifies which cell-level data objects
are allowed for which purposes (Section 5.4). With these decisions,
the secure logical plan is generated (Section 5.5), which eliminates
any unauthorized access to sensitive data at cell-level.
The third factor requires all user queries that are subject to access
control to be submitted through Spark’s structured data analytics
engines/APIs. To enforce this, we can configure Spark to prevent
certain users from using the unprotected APIs. Meanwhile, if this
requirement is not enforced, a user can read data directly from the
underlying data sources in Spark by employing python, R, java,
scala APIs, or RDDs APIs. In such case, GuardSpark++ treats user’s
codes as untrusted codes, and can exploit existing works [46, 47, 66]
to circumscribe untrusted codes to immune itself. We would like to
note that RDD-based access control enforcement is independent of
GuardSpark++ and is an interesting future direction.
Inference Attacks. The inference attack is usually considered a
problem beyond the conventional access control paradigm [62, 71].
In GuardSpark++, sensitive data may be subject to inference attacks
when 1) the “computing” operations do not effectively fuse or trans-
form information from raw data, such as the “sum” of only one
(or a few) value(s); or 2) multiple overlapping or relevant queries
are submitted and the returned results are examined. The first at-
tack could be mitigated in a combination of static and dynamic
approaches. In static analysis, certain (simple) operations may be
excluded from DOP-C purposes, and considered as DOP-R instead.
Meanwhile, we could generate a runtime validator, CompGuard,
in the logical plan for each DOP-C operation. The CompGuard is
a Boolean expression that validates the corresponding operation
based on its runtime data size. All the CompGuards are aggre-
gated to determine if the query should be allowed or denied during
the runtime check. On the other hand, the second attack could
be partially mitigated by tuning PAAC policies to deny certain at-
tributes from being used in DOP-A. For example, Alice can use two
queries: “patient.selectExpr(“Expense”).sum(“Expense”).show” and
“patient.filter(“PatientName != Aaron”).selectExpr(“Expense”).sum
(“Expense”).show” to infer Aaron’s expenses. However, the attack
will be ineffective if the PAAC policy prohibits PatientName from
the DOP-A purpose. Last, we would like to emphasize that the infer-
ence attack is not the focus of this paper. A complete mitigation of
inference attacks require more efforts that are beyond conventional
access control mechanisms.
Generalization. First, the proposed PAAC model has enough ex-
tensibility in response to new access control needs. The data opera-
tion purposes introduced in Section 4.1 could be easily extended to
handle complex access control intentions. For instance, we can di-
vide DOP-C into “count” and “sum” purposes to allow more specific
control of operations and potentially mitigate complicated infer-
ence attacks [45, 62, 71]. Meanwhile, the design of GuardSpark++
in Catalyst is a practical example that could be migrated into other
big data platforms inspired by SQL, e.g., Presto [19], Spanner [27],
Hive [67], SCOPE [76]. With reasonable modifications, the imple-
mentation of GuardSpark++ could be adapted into the logical query
optimizer or processor of these platforms.
Compatibility with RBAC and ABAC. First, PAAC could em-
ploy existing access control models, especially RBAC and ABAC, to
specify the subjects (users or applications). PAAC could also adopt
other concepts from existing access control paradigms that do not
interfere with the concept of purposes, such as resource attributes,
contexts, etc. Second, our model may be adapted into the ABAC
model [52] by adding purposes into the environmental attributes
of ABAC. However, as we have explained in the paper, purpose
recognition and processing algorithms need to be developed in
order to enforce the purposes.
Remaining Attack Surfaces. GuardSpark++ is an access control
mechanism, not a silver bullet that addresses all attacks. Conven-
tional attacks, such as DoS [55], collusion attacks [77], or traf-
fic analysis [75], will still work even if GuardSpark++ is in place.
In particular, if the Spark platform is untrusted or compromised,
GuardSpark++ becomes ineffective. Our research on access con-
trol is independent of and complementary to research efforts on
protecting big data against the attacks mentioned above.
Future Improvements. The policies in GuardSpark++ can be im-
proved to support more features in addition to the high level DOP-*
criteria. 1) We may introduce the obligation concept [36, 59], which
can regulate the actions that big data sharing system should take
ACSAC 2020, December 7–11, 2020, Austin, USA
Tao Xue, et al.
to allow data owners to have the right to know what is happening
to their sensitive data [34, 52]. For instance, to receive the infor-
mation about all usage purposes in each query statement of any
subject, data owner can describe the obligation as {subject:anyone,
algorithm:query, purposes:(DOP-R, DOP-C, DOP-A, DOP-O)}. Based
on the obligations, GuardSpark++ can send corresponding infor-
mation to Spark-external obligation severs which fulfill system’s
obligation to data owners. In this way, data owners can get more
control on their sensitive data. For example, a data owner can pro-
hibit all usage purposes on their sensitive data when he/she is
alerted that the sensitive data may have been unlawfully accessed
by a user, and then initiates an audit for this user. Meanwhile, be-
cause of the supervisory role, data owners have deterrent effect
on users. 2) Data users may need to know what happens to their
queries/algorithms in GuardSpark++ after they submit them to
system for execution. To support this requirement, we could allow
data owners to specify policies with corresponding components to
identify if the system could provide transparency to the users, and
let GuardSpark++ enforces it. However, such transparency to the
data users may potentially give adversarial users more information
to launch inference attacks.
8 RELATED WORK
In this section, we briefly summarize the literature on access control
techniques.
Access Control for Spark. SparkXS [63] is a customized attribute-
based access control solution for Spark Streaming. This ABAC solu-
tion cannot support purpose-aware access control. A cryptography-
based solution [64] targets to protect sensitive data in RDD [72];
the solution does not aim to provide access control solution for
protecting sensitive data in big data sharing scenarios. A recently
proposed method [15] supports Spark SQL by depending on existing
Apache Range policies defined for Apache Hive. Another solution
[12] proposed by Databricks enables table-based access control for
Spark SQL. The two solutions are not only engine-specific but also
do not support purpose-aware access control.
Access Control for Hadoop. Apache Sentry [5], a middleware
system, can be deployed between Hadoop runtime engine and
data sources, e.g., Hive, Impala, Solr or HDFS, and implements
fine-grained role-based access control for Hadoop ecosystem [3].
Although this solution can be unified and fine-grained, it needs to
develop new plugins for new data sources and GuardSpark++ does
not. Apache Ranger [4] is similar to Apache Sentry. GuradMR [68]
provides a fine-grained access control solution for unstructured data
in MapReduce system. HeABAC [49] allows the access isolation of
collected data in multi-tenant Hadoop ecosystem through ABAC
mechanism. None of them provide purpose-aware access control
for big data sharing scenarios.
Access Control for NoSQL A recent work [53] improves the ac-
cess control capability of HBase by customizing fine-grained ABAC
for HBase, supporting context-based access control. Depending on
SQL++ [60], a unified fine-grained ABAC solution [39] is proposed
to improve data security in NoSQL datastores. The two works are
not for big data sharing scenarios.
9 CONCLUSION
In this paper, we have proposed GuardSpark++, a novel fine-grained
purpose-aware access control model and enforcement mechanism
for big data sharing. We first introduce the purpose-aware access