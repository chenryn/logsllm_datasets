time period.
5.1 Testbed Evaluation
Testbed Setup. For the controlled experiment, we use Nginx as a streaming
video server, compiled with the RTMP module [6] to enable HLS live streaming.
For the client, we instrument a Javascript HLS video player [4] that we extended
with the ability to log timestamps and durations of rebuﬀering events, as well as
timestamps of video segment requests and download durations. The client runs
on a local machine while the server is hosted on a cloud VM, with approximately
20ms mean RTT between the two.
We use a 1200-second video that consists of 100 segments. We export the
player logs and analyze them to establish the ground truth for the timings of
player events like segment download, playback, and rebuﬀering. On the server
612
A. Shah et al.
Fig. 6. Testbed evaluation. Comparison of Server-side estimated rebuﬀering with client
reported rebuﬀering.
side, we export logs that contain request timestamps, request URL and segment
duration. We then feed that information to the QoE algorithm in order to esti-
mate rebuﬀering events and their durations, and we compare that to the ground
truth reported from the client logs. During each experiment run we induce arti-
ﬁcial network bottleneck using tc [5], with inter-arrival times of the throttling
events and their durations drawn from a Poisson distribution.
Validation Results
Figure 6 visualizes a single experiment run. The top four rows plot the times-
tamps and durations of events (request, download, playback and rebuﬀering)
as reported from the client, representing the ground truth. The last row is the
output of the QoE estimation based on the server-side logs. The estimated client
rebuﬀering events plotted on that row align well with the client-reported rebuﬀer-
ing events on the row above it, showing that the algorithm detected both the
segments for which rebuﬀering occurred, as well as the corresponding rebuﬀering
durations for the session. The total rebuﬀering duration reported by the client
was 120.8 s and the server estimated 111.3 s of rebuﬀering (7.9% error).
To evaluate the accuracy of our method over multiple runs, we repeat the
experiment 10 times such that the client downloaded 1000 segments (large
enough sample to produce meaningful results) and measure the accuracy of
detecting rebuﬀering duration. First we calculate total rebuﬀer duration as
reported by the client and estimated by our method. We calculate error as the
absolute diﬀerence between rebuﬀer durations as reported by the two methods.
In this test the median error across all the runs was 8.1%. This aligns with our
previous single-run error (7.9%) indicating the method’s consistency.
In the Fig. 6 we can visualize that rebuﬀering was detected at the same time
by the server and the client. To conﬁrm same is true for our experiment with
many runs we evaluate if the rebuﬀering was seen for the exact same segment
by both server and client. Out of 1000 segments across all runs combined (200
minutes of playback time), 121 segments were reported by the client to suﬀer
rebuﬀering. The algorithm estimated rebuﬀering in 107 segments, with 3 false
positives (segments for which rebuﬀering was detected, but did not occur) and 17
false negatives (segments for which rebuﬀering happened but was not detected).
There were 104 true positives (segments for which rebuﬀering happened and it
was detected), providing 85.95% accuracy (precision).
SSQoE: Measuring Video QoE from the Server-Side
613
Fig. 7. Time-series decomposition analysis of client-side beacon reported rebuﬀering
vs server-side rebuﬀer estimation for a large video provider over two weeks. Server-side
QoE is able to track both general trend and instantaneous spikes.
5.2 Comparison with Client Beacon Data
As stated in Sect. 2, video providers often use client-side beacons to evaluate
video and CDN performance. In order for SSQoE to provide value, it is important
that its QoE estimations match the trends of rebuﬀering ratio reported by such
client-side beacons. To validate this, we obtain two weeks of client-side data from
a large video provider for which we deployed SSQoE, and compare the results.
Time-series Seasonality Decomposition
Time-series data is composed of systemic components (lower order trends that
repeat) and non-systemic components (higher order ﬂuctuations that are local
or instantaneous). SSQoE aims to capture both components, i.e. both the gen-
eral trend of rebuﬀering as well as large instantaneous anomalies. Therefore, we
decompose the client beacon and the SSQoE score time-series into the following
three components:
Trend: Lowest component, changes very slowly over long time period (days)
Seasonality: Predictable repeating component, captures local change (hours)
Residual: Anomalous instantaneous component, not predictable (minutes)
Figure 7 (top) shows the original time-series for client-side beacon data
and server-side QoE score. To simplify comparison, we normalize each dataset
between 0 and 1 as described in Sect. 4. We observe that the two datasets match
in overall trends with a Spearman correlation of 0.7, and server-side QoE score
(SSQoE) matches large anomalies in the client-side beacon data. However, we
note that in these original signals there are a few spikes seen in beacon data that
614
A. Shah et al.
are not obvious in the QoE score data. We also note that there is a large amount
of noise (local ﬂuctuations) in both datasets.
We use a standard additive model to decompose the two time-series such
that original = trend + seasonality + residual, as described in [17]. This well
established model quickly provides us an ability to analyze the data at diﬀerent
granularities. Figure 7 also plots these three sub-components, which, combined,
compose the original time series. We analyze each component separately to eval-
uate the precision, recall, and F1 score of SSQoE. We detect anomalies in each
component using a sliding window, where a datapoint is marked as anomalous if
it is greater than three standard deviations (> 3σ) compared to past 5 minutes.
For each anomaly in the client beacon data we check if the QoE score data also
captured an anomaly in the same 5-minute window. If it did, we count that as
a true positive, otherwise we ﬂag that as a false negative. Similarly, if we detect
an anomaly in QoE score data but no anomaly is reported in the same 5-minute
window in the beacon data, we count that as a false positive.
Table 2. For each sub-component, server-side QoE shows high precision values. Server-
side QoE tracking is accurate for detecting large instantaneous spikes and long term
trend but some local short term ﬂuctuations could be missed.
Precision Recall F1 score
Trend
1
Seasonality 1
Residual
0.89
1
0.57
0.76
1
0.72
0.82
Table 2 shows the results of our precision/recall analysis for each time-series
component. The Trend sub-component is expected to be stable and slow chang-
ing, and thus anomalies are easily caught. SSQoE captures the trend of the client
beacon data accurately. For the Seasonality and Residual component, we get high
precision (1 and 0.89) indicating the QoE score is highly accurate in detecting
anomalies for local changes (minutes or hours level granularity). Recall is 0.57
for the Seasonality component and 0.76 for the Residual component, indicating
that the QoE score can have some false negatives for anomalies that are short
lived.
6 Video Performance Monitoring at the CDN
This section presents results from using SSQoE to measure video performance at
the CDN. We ﬁrst demonstrate how per-PoP analysis can aid in fast detection
and response to incidents by showing how we used SSQoE to measure stream-
ing performance for the Superbowl LIV event while delivering millions of live
streams.
We then explore ways that the measured QoE score can provide additional
insight to those gained by existing approaches. CDNs employ a variety of server
SSQoE: Measuring Video QoE from the Server-Side
615
and network based performance analysis systems to detect and resolve perfor-
mance issues [12,26]. In Sect. 6.2 and 6.3 we show examples where server and
network metrics were not suﬃcient during anomalies and how SSQoE exposed
the impact of the issues on client-perceived QoE.
6.1 Using Server-Side Video QoE
Region-Wise Performance Evaluation. One challenge with client beacon
based reporting, even in cases where access to such data is available to the CDN
operators, is that it is not always possible to map the reported problems to the
CDN region (PoP) that might be the root cause of a degraded client experience.
This problem is much more pronounced for a large anycast-based network such as
ours where the decisions regarding which PoP a client talks to is largely decided
by the BGP policies. Since SSQoE is implemented on the server-side, it allows
to easily perform PoP-level breakdown which can reveal regional anomalies that
can happen only in a handful of PoPs.
Superbowl LIV
During the live streaming of Superbowl LIV, along with several other perfor-
mance monitoring tools, we used SSQoE to measure the performance of the
stream delivery. We detected short lived spikes in the QoE score which matched
beacon-reported rebuﬀering ratios. More interestingly, mid-way into the game,
the QoE score for the Seattle PoP started trending upwards. Figure 8 shows the
normalized QoE score for several PoPs1. This trend was not exposed by third-
party beacon data used for monitoring performance. We conﬁrmed that this
was not a false positive, and were able to identify the root cause of this issue:
cache servers hit CPU limits, inﬂating the time it took to serve the video asset.
While a large impact was observed at 02:00 UTC on Feb. 3rd (client reported
problems to the NOC2), SSQoE actually reported spikes in the QoE score ear-
lier than that. As a result SSQoE reduced the time to take action during one
of the largest online streaming events in the US. This example emphasizes the
ability of SSQoE to detect problems that might even be missed by client-side
beacon metrics. This can enable CDN operators to pinpoint issues in the CDN
infrastructure proactively and without any external dependencies.
QoE by User Agents
As shown in Sect. 3, it is possible to evaluate the performance of a video stream
over various dimensions using the time taken to serve the requests. Here, we
analyze the QoE grouped by user agent. In Fig. 9 we show QoE scores for the
most popular browser/operating systems that are observed in the CDN access
logs. Chrome on Windows 7 was the user agent with the highest QoE score, which
translates to more rebuﬀering. While the exact version number of Chrome is not
1 For visualization simplicity in the ﬁgures, each PoP is represented by the city/metro
name it is located in.
2 Network Operations Center (NOC) is responsible for 24 × 7 monitoring of global
CDN performance and respond to customer incidents.
616
A. Shah et al.
Fig. 8. Normalized QoE scores during Superbowl 2020 per PoP. Spike in the QoE score
of Seattle PoP due to CPU bottleneck could be easily identiﬁed (3rd 02:00 UTC). Other
rebuﬀering spikes were also accurately caught at several PoPs.
shown in the ﬁgure, we note that this is an older version of Chrome (74). We also
carefully acknowledge that lower performance of this browser/OS combination
might also be an artifact of the device CPU/memory since older devices running
these older versions of browsers and OSes also tend to have older speciﬁcations.
6.2 QoE vs Server Metrics
In this section we look at QoE scores in the context of CDN performance metrics.
One of the metrics tracked in order to monitor CDN health is the ratio of total
number of server-side errors (HTTP code 5xx) to the total number of requests,
deﬁned as the error ratio. Under normal operations, this error ratio is under
0.3%. The baseline behavior of this metric can diﬀer at diﬀerent PoPs based on
current conditions, which can introduce noise to the metric.
Monitoring the error ratio is useful to understand performance of the CDN
and origin cache performance for cache-ﬁll. If a large number of users request for
the same video segment (e.g. during a popular live event) and if the error ratio
is high, many clients can take a performance hit. However, we argue that QoE
score can be a better metric for tracking such impact. By looking at the error
ratio, it is hard to estimate how long the impact of missing segments lasted, or
what the intensity of the impact was i.e., how many concurrent users suﬀered.
Moreover, video players are designed to buﬀer segments a few seconds ahead of
current play time, hence a small ephemeral spike in error ratio may not always
aﬀect the video playback.
Figure 10 shows SSQoE scores and error ratios from a North American PoP.
In this case, for a few hours the video provider origin had performance issues and
returned 5xx responses. During such events it is operationally diﬃcult to evaluate
the actual impact on end users based on HTTP errors alone. In Fig. 10 we see
SSQoE: Measuring Video QoE from the Server-Side
617
Fig. 9. CDF of QoE Scores by top user agents. Older browsers/OSes often perform
worse.
Fig. 10. QoE score compared to HTTP error ratio. QoE score provides a picture of
user impact during origin server performance problems than the error ratio.
a correlated spike in error ratio and the QoE score. However, we observe that
error spikes were more instantaneous (once a content is available at the origin,
the error ratio subsides). In contrast, the QoE score was increased for a few
hours after the last spike in the error ratio, indicating availability but degraded
origin server performance. Several mitigation steps were taken to alleviate this
problem. In this case, monitoring the QoE score instead of error ratio provided
a more accurate picture on the intensity of the impact. This also emphasizes the
SSQoE’s ability to track performance issues whose root cause lies in early steps
of the live video streaming pipeline (Fig. 1).
618
A. Shah et al.
6.3 QoE vs Network Metrics
A common practice in performance monitoring at content providers is to monitor
the RTT and retransmits towards clients. This information is then used to infer
problems in connectivity either at the client ISP, transit provider, or at the CDN
itself. Systems such as BlameIt [19] operate in such fashion. Network metrics,
however, do not capture a complete view of the media delivery stack. In our
experience, other factors (as described in Sects. 2 and 3) can cause client level
impacts and lead to rebuﬀering.
To demonstrate this, we show the QoE score vs average RTTs and average
retransmits for a 2-day period from a North American PoP in Figure 11. One of
the transit providers for this PoP faced a connectivity issue during this period. As
shown in the ﬁgure, the RTT and retransmit aggregates did not ﬂuctuate much.
However, we clearly see a spike in the QoE score around the time connectivity
issue was reported (starting at 20:00 UTC on the 17th). Due to fallback routes
and other network-layer load balancing it is possible that network layer issues
get masked, but client sessions resetting or suﬀering playback issues are captured
by the QoE score.
Wireless ISPs
SSQoE measurements also enable analysis by ISP. This adds value for opera-
tional monitoring by providing an additional dimension to compare and evaluate
performance. Nowadays users consume large volumes of video content on their
mobile phone over wireless networks. It is common practice to monitor network
metrics towards such wireless carrier ISPs by tracking RTTs and retransmits.
Here, we evaluate whether lower RTT translates to lower rebuﬀering. In Fig. 12a
we plot the average RTTs from one of the CDN’s North American PoPs towards
the top three wireless carriers in the U.S. over 8 days. We normalize these values
between 0 and 1, where maximum RTT across the ISPs is set to 1. The RTTs
proﬁles for the three wireless carriers are diﬀerent at this PoP, making it a good
candidate to evaluate if lower RTTs lead to less rebuﬀering. Since we are trying
to eliminate the network impact, we only look at estimated rebuﬀering captured
using the method described in Sect. 4.
Interestingly, we note that Wireless ISP 1 has the lowest rebuﬀering but the
2nd highest RTT. We also note that the distribution of estimated rebuﬀering is
similar for wireless ISPs 2 and 3 even though their RTT proﬁles vary. This indi-
cates that network metrics do not necessarily capture user perceived experience.
The above analysis shows how SSQoE can be used to proﬁle ISPs based
on QoE scores. Categorizing ISPs by performance in video delivery can help
identify providers which are persistently under-performing, and can help drive
traﬃc engineering decisions during large live streaming events.
SSQoE: Measuring Video QoE from the Server-Side
619
Fig. 11. QoE score compared to RTT and retransmits. A transit provider connectivity
problem, from 17th 20:00 to 18th 01:00, is better captured using the QoE score.
(a) Normalized RTTs for 3 largest wire-
less carriers in the U.S from a North
American PoP. Each ISP has a unique
RTT proﬁle.
(b) CDF of estimated rebuﬀers used in
QoE calculations. Wireless ISP 1 shows
lowest rebuﬀering but has 2nd highest