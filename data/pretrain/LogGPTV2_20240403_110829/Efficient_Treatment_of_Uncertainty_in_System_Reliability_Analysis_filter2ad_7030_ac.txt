correspond to the reliability
,
the
is
be
, . . . ,RU B
, and RM B
is equal
i
to λU B
i +λU B
i
, λLB
i
, and
(cid:6)
1
i
1
1
i
2
as
3Uncertainty analysis using Taylor expansion has been employed for other
applications as well. For some examples, we refer to [45], [30].
81
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:53:24 UTC from IEEE Xplore.  Restrictions apply. 
the system reliability is inﬂuenced by the
accuracy of
Taylor polynomial. Therefore, we propose to select not
one, but a set of initial states, notated as IS. Let sample
S = (R1, . . . ,RN ) be randomly taken from the sample space.
To calculate R(S), the Taylor polynomial may be centered
at state S 0i = (R0i
N ) ∈ IS which minimizes the
1 , . . . ,R0i
following Euclidean distance:
||S − S 0i||2
(7)
argmin
1≤i≤|IS|
(R1 − R0i
where
||S − S 0i||2 =
The computational overhead to ﬁnd the closest initial state to
a sample is negligible especially when |IS| ≤ 3 which was
the case in all the studied test-cases in this paper.
1 )2 + . . . + (RN − R0i
N )2
2 . (8)
(cid:5)
(cid:6) 1
B. IM-Driven Sampling
In this paper, each uncertain system property Y, such
as the system reliability or its MTTF, is represented by a
corresponding density function fY, where:
Pr{l ≤ Y ≤u } =
fY (Y)dY.
(9)
Indeed, fY (Y)dY is the probability of Y falling within the
inﬁnitesimal interval [Y,Y + ΔY]. When Pr{l ≤ Y ≤u }
cannot be determined with certainty, the following operation
estimates the probability:
l
(cid:7) u
Pr{l ≤ Y ≤u } ≈ |{S ∈ S|l ≤ Y(S) ≤ u}|
.
(10)
Ns
We select Ns samples from an N-dimensional input space (or
sample space). Let S contain all the samples, then it holds that
|S| = Ns. Equation (10) approximates Pr{l ≤ Y ≤u } as the
number of occurrences of l ≤ Y(S) ≤ u normalized by the
total number of samples.
To select Ns samples from the sample space, state-of-the-art
techniques typically partition the sample space (the partitioned
space is referred to as a grid) and choose the samples uni-
formly from each tile (or stratum). These–in general stratiﬁed–
sampling techniques have so far attracted very signiﬁcant and
rigorous scholarly attention. However, it is not clear how each
dimension should be stratiﬁed especially when the components
have different contribution to the uncertainty at system level.
In the following, we therefore introduce a novel approach
that generates an adaptive grid from a sample space using
the concept of component importance.
In statistics and probability theory, quantiles are cut-points
dividing the range of a pdf into contiguous intervals with equal
occurrence probabilities. This means that partitioning a pdf
into q subsets of (nearly) equal sizes, the probability that a
random sample falls into any of these quantiles is equal to
1
q . In this work, we propose to stratify the N-dimensional
sample space into Q = Ns strata such that the probability
that sample S drops into each of them is equal. This requires
that we partition dimension i into qi quantiles and generate,
in total, Q =
i=1 qi strata. As an example, Figure 5 depicts
(cid:4)N
(a)
(b)
Fig. 5: The rectilinear grid for a (a) two- and (b) three-
dimensional sample space. Each pdf is partitioned into four
quantiles with equal occurrence probability. Consequently, the
probability that each sample falls into each of the tiles is also
equal.
two rectilinear grids for a two and three-dimensional sample
spaces where probability space of each variable is split into 4
quantiles (we partition the probability space and reﬂect that in
the sample space). Then, a single sample is chosen from each
stratum.
The question that arises is then how to select qi
for
each dimension. Some related works simply assume that
∀i,j∈N,∧i(cid:5)=j
qi = qj. We refer to these techniques as
Homogeneous Stratifying (HS) techniques. Yet, as previously
motivated, we rather propose to use a heterogeneous strati-
fying technique introduced below: It has been discussed in
Section IV-A that the uncertainty of important components
(or input variables) dominates the uncertainty of a system (see
Equation (4)). Consider the product of the importance and the
deviation of variable xi from the initial point (Ii(S 0) · ΔRi).
We propose that when this product
the
sampling approach can focus less on the variation of this
variable by assigning a low qi to that. Instead, qj is increased
for other more important variables. Indeed, our IM-Aware
Stratifying (IMS) approach (the algorithm shown in Figure 3
(c)) increases the ﬁlling space on the dimensions which are
is relatively low,
82
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:53:24 UTC from IEEE Xplore.  Restrictions apply. 
IMS
HS
reference
6
4
2
f
d
p
0.1
0.2
0.3
R(S)
0.4
0.5
Fig. 7: The pdf representing the reliability of the system in
Figure 2 obtained using HS (shown in red) and IMS (shown
in blue) techniques. The solid line illustrates the reference pdf
of the system’s reliability.
components are partitioned into a smaller number of quantiles.
The corresponding pdfs are shown in Figure 7. The results
show that compared to the HS technique, the IMS technique
provides a histogram closer to the reference pdf.
V. EXPERIMENTAL RESULTS
In this section, we ﬁrst show how the proposed IM-
driven reliability analysis technique outperforms related work
in terms of efﬁciency vs. accuracy. Later, we compare
the uncertainty distribution of the system’s MTTF obtained
through the proposed IM-aware Sampling (IMS) approach
vs. state-of-the-art sampling techniques. Herein, we study
three real-world and two synthetic high-level synthesis test
problems of different sizes and complexities. The real-world
problems are widely used in the area of system-level. The
test problems are modeled at Electronic System Level (ESL)
which targets the concurrent and coordinated design of an
electronic system comprising of both hardware and software
components [43]. The speciﬁcation of each test problem
at ESL including the number of I) hardware resources (or
components), II) software tasks, and III) task-to-resource map-
ping options, are listed in Table II. The Benes test problem
represents a 7-stage Benes switching network [6], and includes
the tasks and resources used for modeling terminal reliability
as described in [20]. The ACC problem models an adaptive
cruise control application speciﬁed in [9] using a simple
architecture consisting of only four resources. Here, we scale
the architecture to increase the analysis complexity. The H.264
problem speciﬁes an MPEG video encoder/decoder and is
described in [11]. Synth-I and Synth-II are two synthetic sys-
tem speciﬁcations that are employed to test the scalability of
the proposed uncertainty analysis technique4. Figure 8 shows
the speciﬁcation of an example test problem consisting of
three data-dependent tasks ti, i ∈ [0 . . .2] , four interconnected
4By scalability, we denote the capability of an approach to handle large
test cases.
(a) The HS technique.
(b) The IMS technique.
Fig. 6: Comparing two stratifying techniques: (a) homoge-
neous and (b) IM-aware. The blue points are the samples taken
by each technique. Also, the gray points in (b) are the same
samples as (a).
more inﬂuential on the system reliability and neglects the rest.
The following example clariﬁes this idea: Assume that we
are interested in selecting 16 samples from a two-dimensional
sample space with uniform distribution. Figure 6 (a) partitions
each variable domain into 4 quantiles and picks uniformly one
sample from each stratum. Now, let the variable on the y-
dimension be signiﬁcantly more important than the other one.
Therefore, we set qx = 2 and qy = 8 such that still the space
is split into 16 strata. Figure 6 (b) uses this IMS technique.
Therein, the gray points are the same samples as part (a). It can
be seen that while these points ﬁll all the strata in Figure 6 (a),
they fail to ﬁll some in (b). Knowing that the y variable is very
important, the non-ﬁlled strata may inﬂuence the accuracy of
the system statistics (e. g., density function) remarkably. These
strata are covered using IMS technique and a possible selection
of samples are shown in blue.
More precisely, qi can be determined as:
Q Ii (S0 )·ΔRi
Ii (S0 )·ΔRi
(cid:2)N
i=1
qi =
.
(11)
(cid:8)
(cid:9)
i − RU B
i
| is the total range of component
Here, ΔRi = |RLB
uncertainty which is assumed to be given.
As a practical example, consider the system in Figure 2.
Assume that we are interested in Q = 1000 samples. Using
the HS technique, qa = qb = qc = qd = qe = Q 1
(cid:10)
5 = 4. For
the same number of samples, in the IMS technique, qd = qe =
qa · qb · qc =
= 2.
This is due to the fact that Id = Ie = Ia∨b∨c. Therefore,
the same number of quantiles is assigned to d, e, and other
= 10 and qa = qb = qc =
(cid:10)Q 1
10 1
3
(cid:11)
3
(cid:11)
83
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:53:24 UTC from IEEE Xplore.  Restrictions apply. 
TABLE II: The test problems used in our experiments.
problem resources
Benes
ACC
H.264
Synth-I
Synth-II
22
12
17
25
50
tasks mappings
7
20
66
56
101
22
80
320
261
592
resources rj, j ∈ [0 . . .3] , and six task-to-resource mappings
mi,j from ti to rj.
Given such a speciﬁcation of a system, system-level syn-
thesis techniques [8] can be used to derive implementation
candidates through resource allocation,
task binding, and
scheduling. Resource allocation selects a group of available
resources that can execute all tasks. In task binding, each task
is bound to the allocated resources through selectively acti-
vating its task-to-resource mappings. The goal of scheduling
is to ﬁnd feasible start times for all bound task instances in
order to minimize the total execution time. An implementation
candidate is feasible if it meets a number of given constraints
on timing, resource utilization, etc. In the example of Figure 8,
the parts shown in black indicate the resource allocation
and task binding of a single system implementation. The
remaining resources shown in grey are not included in that
implementation.
To search through a typically exponential space of im-
plementation candidates and ﬁnd those which optimize the
quality metrics like reliability and performance in the pres-
ence of uncertainty, the proposed uncertainty-aware reliability
analysis has been integrated into a Design Space Exploration
(DSE) framework. This framework can employ existing robust
optimization techniques [19], [25], [26] and [32] to enable
comparing implementation candidates with quality metrics
given as probability distributions, sampled data, etc.
A. IM-Driven Reliability Analysis
In this subsection, we compare both the performance and
the accuracy of the proposed IM-driven reliability analysis
tool using Taylor expansions. We assume that
the failure
rate of each component (hardware resource) is given as a
probability distribution and the reliabilities of components are
uncorrelated5. Our approach is compared to state-of-the-art
work using I) Binary Decision Diagrams (BDDs) [17] and II)
Success Trees (STs) [5]. Therein, evaluating the reliability of
each sample requires a full analysis of the system BDD or ST.
BDD-based reliability analysis techniques are very efﬁcient
and accurate. However, they have the drawback of growing
exponentially with the number of input variables. Simulation-
based STs, on the other side,
improve the scalability of
BDDs with negligible error rate but typically result in long
simulation times [4]. To measure the system reliability at
the central sampling point (R(S 0)) in Equation (4), the IM-
Driven approach according to Figure 3 (b) can use either
5For how to handle the correlated case, we refer to [27].
84
m0,1
m1,1
m0,0
m2,3
m1,2
r1
m2,2
t0
t1
t2
r0
r3
r2
task graph
mapping edges
resource graph
Fig. 8: An example system speciﬁcation comprising a task
graph, a resource graph and a set of possible task-to-resource
mappings. The edge (ti, tj) in the directed task graph speciﬁes
data dependency from ti to tj, while the edge (ri, rj) in the
resource graph indicates a dedicated link between ri and rj.
A mapping edge mi,j expresses that task ti can be mapped
to resource rj for execution, respectively implementation. An
implementation candidate derived from resource allocation and
task binding is shown in black.
BDD or ST. The authors in [3] present an efﬁcient technique
based on STs for evaluating the IMs of the components
simultaneously. Therefore, we employ the same technique here
which allows us to evaluate the initial reliability, the IMs, as
well as the joint IMs simultaneously and to reduce the time
overhead noticeably. The comparison results are presented in
the following.
Performance: The average execution time for the uncertainty
analysis (including the computation time of the IMs for
the IM-driven approach) of a single implementation with
Ns = 500 samples is presented in Table III using a 4.0
GHz desktop PC with 8 GB of RAM. The results show a
signiﬁcant improvement in the execution time using the IM-
driven approach. This is even more noticeable when the pro-
posed approach is integrated into a DSE tool which typically
explores thousands of different implementations. Regarding
ACC, BDD-based reliability analysis technique performs faster
because of a simplistic structure of this particular problem
related to the construction of its BDD. However, the size of
the BDD grows exponentially with the number of variables
undermining the scalability of IM-based approach. See also [4]
for an investigation of this phenomenon.
Accuracy: The accuracy of the ST and IM-driven techniques
are compared to the BDD-based approach which provides
us with fully accurate reliability-related values. Here, we use
MTTF as a measure for comparing the reliability of each ex-
plored implementation. In the IM-driven technique, the Taylor
expansion is approximated with its ﬁrst two terms. Thus, only
the IMs and the joint IMs of each pair of components are
evaluated. The average error rates are presented in Table IV.
Therein, MTTFlower, MTTFupper, and MTTFmean correspond to
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:53:24 UTC from IEEE Xplore.  Restrictions apply. 
TABLE III: Execution time, in seconds, of the three reliability
analysis techniques with Ns = 500 samples. The values are
averaged over the uncertainty analysis of 100 implementations
in each case.
problem BDD
1.08
Benes
0.82
ACC
8.38
H.264
Synth-I
8.39
19.40
Synth-II
ST
32.03
86.20
191.72
212.64
774.95
IM-driven
1.03
1.40
2.92
3.80
12.55
(cid:5) λLB
(cid:6)
i
i
2
, λLB