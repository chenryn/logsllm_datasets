parameters involved are , the fraction of the ﬁle unknown
to ˜C and the value g.
4.2
s-POW1
In this section we propose a ﬁrst improvement to s-POW.
Given the extreme simplicity of s-POW, it may seem that
there is very little room for optimization. Indeed, as we have
seen in the previous section, the number K of bits the client
is challenged on is a function of the security parameter of
the scheme. Therefore, any reduction in K will inevitably
alter the overall security of the scheme.
Section 5 describes the results of the benchmarking on our
scheme. We will, however, already mention one of the re-
 1e+06 1e+07 1e+08 1e+09 1e+10 1e+11 1 10 100 1000Clock cyclesFile size (in MiB)Algorithm 2I/OHash computationthere are two ﬁles f1 and f2 with the same digest d: a client
owning f1 engages in the POW protocol with the server and
receives a challenge seed FSMK (d||i) for some value i of the
current counter; that seed can no longer be used for a client
owning f2 because if the challenge is leaked, a user colluding
with ˜C could precompute the correct response and send it to
˜C. Not reusing challenges that have been disclosed implies
that the usage rate of challenges for ﬁles indexed by the same
key d is equal to the sum of the rates of requests for each
of these ﬁles. However this does not constitute a problem,
because the server has abundant computational power and
can regularly schedule the pre-computation of challenges in
periods of low system load.
4.2.1
The security of the scheme is unchanged:
indeed, even
if an attacker were able to produce the correct value d for
a given ﬁle (for instance, by receiving it from an accom-
plice), it would still need to generate the correct response to
the challenge of the server. However we have shown in Sec-
tion 4.1.1 that the probability of this happening is negligible
in the security parameter.
Security Analysis of s-POW1
The cryptographic hash function H previously used for
indexing was collision resistant by deﬁnition. As explained
above, H has been replaced with an s-POW invocation on
input of a public seed. We therefore need to quantify the
collision probability of such an indexing function, i.e. the
probability that the digests d1 and d2 of two diﬀerent ﬁles
f 1 and f 2 are equal. We can derive this probability by
assuming that two ﬁles are similar with a given probability
z (z expressing the percentage of the bits of the two ﬁles in
the same position that show the same value). Hence, for M
ﬁles we have the probability of collision P (coll):
(cid:32)
(cid:33)
M
2
≤ M 2
2
P (coll) ≤
P (d1 = d2)
P (d1 = d2) =
M 2
2
zK
(3)
where K is the parameter of s-POW mentioned in the pre-
vious section. The above probability can still be considered
negligible for practical instantiations of the scheme. For in-
stance, for M = 109, z = .95 and3 K = 1830, P (coll) ≤
2−75.
However, let us assume that collisions do happen. Then,
as explained above, an invocation of the mapping F on d
would return a set of m ﬁles. We then need to quantify
the additional advantage that an adversary might have in
passing the proof of ownership given that the server has
to compare the client’s response with m pre-computed ones
instead of a single one. Let ri be the event that resp∗, the
response received by ˜C equals respi, the i-th precomputed
response of one of the m ﬁles in the set. Then it follows that
the probability P (succ) of ˜C of succeeding the test over at
least one out of the m ﬁles is
P (succ) = P (r1 ∨ . . . ∨ rm) ≤ mP (succri )
= m(1 − (1 − g))K
(4)
where the term (1 − (1 − g))K comes from Equation 1.
From 4 we conclude that m can therefore become an addi-
tional parameter of the system and can contribute to the
3See Section 5 on the sizing of the parameter K.
determination of the parameter K, even though its eﬀect
over K is only logarithmic: major changes in m will have
very little eﬀect over the value of K.
Another aspect we need to consider for the case in which
an invocation of the mapping F on d returns a set of m ﬁles,
is the probability that there are collisions among the m pre-
computed responses for a given value of the index F[d].idu;
that is, the F[d].idu-th pre-computed response for ﬁle fi is
equal to the F[d].idu-th pre-computed response of ﬁle fj,
i (cid:54)= j, and the digest di of fi is equal to the digest dj of
fj. However, this happens with a negligible probability as
shown in Equation 3, by substituting M with m; moreover,
m << M .
4.3 Distribution of File Sizes and s-POW2
Further improvements may be achieved if another, less
expensive candidate for the indexing function of the ﬁle can
be found.
Here we consider using the size f.size of a ﬁle f as a candi-
date for the indexing function. This approach clearly meets
the last two requirements outlined in Section 4.2, because it
optimizes both I/O and computation.
ALGORITHM 5: Changes in the protocol of s-POW to obtain
s-POW2.
Input: A ﬁle f .
C : upon upload of ﬁle f do
d ← f.size;
send to SRV a store ﬁle request with d;
end
Algorithm 5 shows the changes to the client-side intro-
duced in this version of the protocol. As we can see, no
computation – beside determining the size of the ﬁle – is
required of the client.
We have explained in Section 4.2 how to cope with col-
lisions in the indexing. However, we still need to verify
whether the ﬁle size constitutes a good indexing function,
i.e. whether in practice, the likelihood that two diﬀerent
ﬁles have the same size is tolerably small.
To this end, we have studied the distribution of ﬁle sizes
over two independent datasets, the Evans and Kuenning
dataset [8], containing information on approximately 3 mil-
lion ﬁles and the Agrawal et al. dataset [1], for which we only
focused on a subset of approximately 200 million ﬁles. The
two datasets capture (among other information) the sizes
of the ﬁles observed in the computers of an academic cam-
pus and of a large corporation, respectively. Both datasets
contain snapshots of the entire content of ﬁlesystems in an
academic and an industrial environment, respectively. Both
datasets are extremely relevant for our scheme, because they
would correspond to users backing up (possibly in a storage
cloud) their entire hard-disk drives.
The objective of our analysis is to verify the intuition that
– especially for large ﬁles, i.e. those for which computing
another indexing function is more expensive – the size of a
ﬁle can become a very eﬀective ﬁle indexing function. To
this end we have extracted from both datasets a unique ﬁle
identiﬁer (e.g. the hash of the ﬁlename) and the ﬁle sizes.
After purging doubles, we have counted the number of ﬁles
with equal size.
Figure 2 shows the results of the analysis. For each dataset
we show two curves: the ﬁrst one plots the distribution (with
5. RUNNING POW
To evaluate the eﬀectiveness of our scheme, we have im-
plemented both b-POW and s-POW and its two variants,
s-POW1 and s-POW2.
The code has been developed in C++ using the OpenSSL
crypto library for all cryptographic operations and using
the Saito-Matsumoto implementation [15] of the Mersenne
twister [13] for the pseudo random number generator. The
code implements both the client-side and the server-side of
all schemes. The interactions between client and server as
well as the data exchange have been virtualised so as to not
consider networking-related delays and to focus only on local
(client and server) I/O and computation.
5.1 Experimental Settings
We have run our implementation of both schemes on a
64-bit RedHat box with an Intel Xeon 2.27GHz CPU, 18
GiB of RAM and an IBM 42D0747 7200 RPM SATA hard
disk drive. All schemes operate on input ﬁles that have
been generated at random; the input ﬁle size ranges from
1 MiB to 4 GiB, doubling the size at each step. The ﬁles
are reasonably well defragmented, with a maximum of 34
diﬀerent extents on the 4 GiB ﬁle.
The parameters for b-POW have been chosen in strict ad-
herence with the choices made in [9]. We have also used the
same security parameter k = 66. Our scheme has two pa-
rameters,  = (1 − p) and g. The values of these parameters
are needed to derive a value for K in Equation 2. We have
chosen p, the upperbound on the fraction of the ﬁle known
to the adversary, as p ∈ {0.5, 0.75, 0.9, 0.95}. As for g, this
parameter measures the probability that the adversary suc-
cessfully guesses the value of a bit without knowing it. To
assign a reasonable value to g, we have analysed what the
probability of guessing a bit in an ASCII ﬁle with lower-
case letters written in the English language is, arguably a
relatively conservative case with low entropy in the input
distribution. Given the letter frequency analysis in [12], the
probability that a given bit equals one is 0.52731. In addi-
tion, Equation 2 shows that slight changes in the value of g
do not sensibly aﬀect the value K. We have therefore chosen
g = 0.5.
Each conﬁguration has been run for at least 200 times; be-
fore each repetition, cached data, dentries and inodes have
been ﬂushed (at both the client- and at the server-side) to
ensure accurate measurements. To perform the comparison
of the diﬀerent schemes, the code has been instrumented by
surrounding relevant code blocks and functions with calls to
extract the Intel Time Stamp Counter through the RDTSC
assembly instruction. The ﬁgures below have been generated
by reporting the mean value and the standard deviation (us-
ing a box plot) of the extracted clock cycle count.
5.2 Client-Side
Here we compare the client-side performances of b-POW
with those of s-POW, s-POW1 and s-POW2. The imple-
mentation of b-POW ﬁrst loads the ﬁle into main memory,
where the various phases of the scheme are performed: the
reduction phase (which also results in the computation of
the SHA256 hash digest of the ﬁle), the mixing phase and
the calculation of a binary Merkle tree on the resulting re-
duction buﬀer.
On the client-side of s-POW, the input ﬁle is loaded into
memory, the hash digest is computed and then Algorithm 2
Figure 2: Plot of the distribution of ﬁle sizes for ﬁles
in the entire dataset (with power-of-two bins) and
of the average number of ﬁles per bin (with power-
of-two bins) with the same size for both datasets.
power-of-two bins) of ﬁle sizes for ﬁles in the entire dataset;
the second plots the average number of ﬁles with the same
size per bin. Notice that we have tried to only count diﬀerent
ﬁles with the same size: when available, the hash of the
ﬁle has been used to establish whether two ﬁles were the
same; otherwise, we have used a combination of ﬁle name
and creation timestamp.
First of all, we can observe that for both datasets, the dis-
tribution of ﬁle sizes has a similar shape, albeit that of the
ﬁrst dataset is shifted higher by two orders of magnitude be-
cause of the larger ﬁle population. Both distributions of ﬁles
with the same size have similar shape: we can see that in
both cases there is a relatively constant number of ﬁles with
the same size up to around 10 KiB, after which both curves
plunge. In both scenarios, the size virtually starts to behave
as a unique identiﬁer for ﬁles larger than approximately 1
MiB, even though the number of ﬁles in the considered bin
is still relatively high: for example, in the 1 Mib to 2 MiB
range, we have 1,784,136 and 10,819 ﬁles in the ﬁrst and sec-
ond dataset respectively and the average number of ﬁles with
the same size is 1.9 and 1.2 respectively. Clearly, the num-
ber of ﬁles with the same size decreases also because each
bin is wider and less and less populated: however these dis-
tributions portray very likely the ones a ﬁle-storage service
may receive as input from its customers—and it is therefore
of high signiﬁcance for this paper. Far from claiming to be
exhaustive, our study nonetheless strongly supports the use
of ﬁle-size for indexing purposes in our scenario.
Security Analysis of s-POW2
4.3.1
Similar considerations to those made in Section 4.2.1 ap-
ply to s-POW2: the inﬂuence of the number of ﬁles with the
same size on the choice of the system’s parameters has been
captured in Equation 4. Intuitively, the approach suggested
in s-POW2 is particularly eﬀective for ﬁles with large size,
because: i) as shown, the probability of collision is low; and,
ii) avoiding the computation of another indexing function
on a very large ﬁle is particularly cost-eﬀective.
In Section 6.1 we leverage this consideration and those
made in the previous sections to obtain a particularly eﬀec-
tive scheme.
 1 10 100 1000 10000 100000 1e+06 1e+07 1e+08012345678910log10 of number of occurrenceslog10 of file size (in bytes)Agrawal et al.'s dataset (all ﬁles)Evans et al.'s dataset (all ﬁles)Agrawal et al.'s dataset (same size)Evans et al.'s dataset (same size)Figure 3: Comparison of the running time of b-POW
with that of s-POW for diﬀerent values of K as the
input ﬁle size grows.
Figure 4: Comparison of the running time of b-POW
with that of s-POW1 for diﬀerent values of K as the
input ﬁle size grows.
is executed. Figure 3 shows the results of the experiments
assessing the performances of the two competing solutions:
s-POW is faster than b-POW– from ten times to twice as
fast. The complexity of both schemes grows at an approx-
imately equal rate as the input ﬁle size grows. The reason
for this is that – as previously mentioned – reading the ﬁle
and computing the hash are by far the predominant oper-
ations for both schemes. The discontinuity in the curve of
b-POW– noticeable around 64 MiB – is due to the fact that
64 MiB is the maximum size for the reduction buﬀer. There-
fore the computational cost of the reduction phase reaches
its maximum at 64 MiB and remains constant afterwords.
For s-POW1, the computation of the hash is replaced by
an initial invocation of Algorithm 2. Note that, as access
to the entire content of the ﬁle is no longer needed, the
ﬁle is no longer loaded into memory. Indeed, only random
disk accesses are needed to fetch the required bits. Figure 4
shows that this second version improves the scheme’s per-
formance with respect to that of b-POW. We can see how
the computational cost of our scheme reaches a plateau for
suﬃciently large ﬁles, because – regardless of the input ﬁle
size – the computation required is essentially constant. The