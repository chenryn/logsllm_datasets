images}
|
|
images}
|
We had knowledge of relevant images from the first user study in 
the form of the strong matches identified by participants for each 
image. Retrieved images are the set of images that the particular 
method  judged  to  be  similar.  The  metric  of  recall  provides  a 
measure of the fraction of relevant images that a particular method 
returned.  The  precision  provides  the  fraction  of  the  returned 
images  that  were  relevant  and  is  sensitive  to  false  positives. 
Where  thresholds  had  to  be  chosen  to  make  a  decision  of 
similarity  for  a  particular  image  processing  intervention,  they 
were  selected  to  balance  precision  and  recall.  To  incorporate 
spatial  information  into  the  calculations  we  also  augmented  the 
statistical moment and color histogram methods with a vertical or 
horizontal  region  of  interest  (ROI).  This  involved  partitioning 
images  with  a  vertical  or  horizontal  line,  calculating  the  image 
signature  for  both  halves  and  using  the  mean  of  the  two  as  the 
result  for  that  image.  Table  2  summarizes  the  filtering  results 
obtained for each method. In addition to precision and recall we 
calculated the F1 score, which is used to aggregate both precision 
and recall and represents a weighted average of the two.  
Overall,  the  color  histogram  image  signature  applied  to  whole 
images  provided  the  best  recall  at  .58.  The  addition  of  spatial 
information to the image signature through the vertical ROI gave 
higher  recall  than  the  horizontal  ROI  but  also  introduced  more 
false  positives.  The  use of statistical moments was less effective 
than  the color histogram in all configurations, as recall was .34, 
and  this  in  fact  dropped  with  the  introduction  of  ROI,  although 
ROI  eliminated  false  positives.  PerceptualDiff  produced  a  lower 
recall than both color histogram and statistical moments. The use 
of  PerceptualDiff  was  most  effective  at  returning  very  strict 
matches  where  visually  the  objects  and  colors  in  the  scene 
appeared similar. As might have been anticipated, the recall using 
the ROI was consistently lower than signatures based upon whole 
images, but ROI augmentation also yielded fewer false positives. 
This was reflected in a high score for precision.  
Table 2: Results from filtering procedure on an image set with 
800 photographs, resized to 384x286. 
Method 
Color Histogram 
Color Histogram & Vertical ROI 
Color Histogram & Horizontal ROI 
Statistical Moments 
Statistical Moments & Vertical ROI 
Statistical Moments& Horizontal ROI 
PerceptualDiff [42] 
Recall  Precision  F1 
.58 
.4 
.3 
.48 
.3 
.41 
.3 
.34 
.20 
.2 
.1 
.07 
.24 
.2 
.95 
.8 
1 
1 
1 
1 
.9 
Since  the  color  histogram  approach provided the best recall and 
the highest F1 score, we chose to use this in our second study. The 
efficacy  of 
that 
representation  captured  the  diversity  of  color  without  being 
restrictive spatially. This method did not provide a perfect recall 
score;  however,  we  believed  this  score  was  difficult  to  better 
given the set of images in use.  
the  color  histogram 
likely  because 
is 
this 
5.  USER STUDY 2 – RECALL TEST USING 
AUTOMATICALLY SELECTED IMAGES 
The first user study suggested that the optimal image signature we 
tested was the color histogram, as it provided the closest predictor 
of  the  human  similarity  judgments  we  collected.  Our  remaining 
research  question  concerned  whether  systematic  manipulation  of 
thresholds  chosen  for 
image  signature  could  have  a 
predictable impact upon the short-term recall of users in a typical 
graphical password login.  
5.1  Procedure 
We chose a between-subject study design where the independent 
variable  was  the  similarity  between  a  key  image  and  its  decoy 
images,  and  the  dependant  variables  were  user  performance  in 
terms of recall and login time.  We developed a web-based system 
that would challenge the user to identify four key images across 
four  grids  of  nine  images  in  a  3x3  layout,  with  one  key  image 
certain  to  appear  in  each  grid,  providing  theoretical  entropy  of 
12.7  bits.  We  chose 
three  experimental  conditions  where 
similarity  between  the  key  image  and  its  decoy  images  was 
controlled by a threshold upon the EMD distance d between the 
color histograms of the images:  
(cid:120)  Similar: where 1>d>=0  
(cid:120)  Middle: where 4>d>= 3 
(cid:120)  Dissimilar: where 6>d>=5 
Studies in psychology [7] have observed how the difficulty of the 
visual search should decrease with decreasing similarity between 
103
Figure 4: The key images chosen for the study, these were the 
same in all study conditions. 
target and non-targets. We were hoping to recreate a similar trend. 
To generalize our results more effectively we firstly discarded the 
image set used in the first study and obtained a set of 1000 images 
used in other image processing research [39], and removed any 
portrait oriented images for display consistency (reducing to 800). 
The database is highly categorical, which provides a worst case 
scenario for this study. In advance, we also chose 8 key images 
that represented exemplars of particular categories in the image 
set (see Figure 4). These key images were persistent across 
conditions. For each key image and experimental condition we 
automatically chose eight decoy images according to the 
condition-specific similarity criteria. We also enforced distances 
between other images in the login to respect the image filtering 
concerns discussed in Section 3.1. Within a particular condition, 
once an image was selected as a decoy to be associated with a 
particular key image, it could not be selected to appear as a decoy 
image for a different key image within that condition. Also, a key 
image could not reappear as a decoy image. Figure 5 provides an 
example of decoy image selection for one particular key image 
across all three conditions. 
We  recruited  participants  from  the  crowdsourcing  platform 
Amazon  Mechanical  Turk.  Kittur  et  al.  [16]  provide  hindsight 
from  conducting  user  studies  on  this  platform,  in  particular  that 
the  most  suitable  tasks  are  those  that  have  a  verifiable  answer. 
Clearly  those  carrying  out  studies  on  crowdsourcing  platforms 
must design robust  experiments that do not rely on literacy, and 
due to its remote nature take measures to detect behavior that may 
undermine  the  integrity  of  the  study.  The  user  sample  on 
Mechanical Turk was suitable as they are likely to be technology 
savvy adults.  There were a number of study phases: registration: 
participants were requested to give information such as worker ID 
and  demographic  information;  enrolment:  where  the  participant 
would  be  given  30  seconds  to  view  four  key  images  randomly 
selected  from  our  set  of  eight;  wait:  A  JavaScript  enforced 
stoppage of 30 minutes where participants could not progress to 
the  next  phase,  but  were  free  to  carry  out  other  tasks  on 
Mechanical Turk. If participants attempted to progress beyond the 
wait period too quickly this could be detected via use of server-
side  timestamps.  The  last  phase  was  recall:  the  participant 
attempts to recognize the images assigned to them and has a single 
attempt to do so.  
Due to the remote nature of the study we designed the following 
study  defenses  in  order  to  have  increased  confidence  in  our 
results:  anti-image  caching:  the  images  presented  at  login  were 
drawn  from  a  different  location  on  the  server  than  those  at 
enrolment.  This  removed  the  threat  that  the  key  images  would 
load  faster  due  to  caching;  anti-print  screen:  participation  was 
restricted to Internet Explorer and via a JavaScript we cleared the  
Figure 5: Example image grids assembled for key image #4 
using similar, middle, and dissimilar decoy image criteria. 
clipboard of the participant every 100 milliseconds. If consent to 
do  this  was  not  granted  the  experiment  would  not  continue; 
Dynamic key images: key image sequences were not static across 
participants  and  there  were  8C4  different  possibilities  for  the 
sequence of key images that could be presented.  This meant that 
if images were recorded they may not be immediately reusable by 
another participant. HTTP GET parameter protection ensured that 
we could detect where parameters were maliciously altered in the 
browser or the back button was pressed  
5.2  Results 
5.2.1  Participation 
We  received  364  completed  logins  across  a  6  day  period.  We 
treated  as  outliers  those  who  completed  the  login  procedure 
identifying no key images in less than 5 seconds. This reduced the 
numbers down to 343 with 117 in the similar condition, 112 in the 
dissimilar condition and 114 in the middle condition. In terms of 
demographics,  72%  of  participants  were  from  India,  with  the 
United  States  the  next  prominent  location  at  7%.  Most  of  the 
participants were male (73%). In terms of age, 269 were in the age 
group 18-30, 67 in the group 31-40, 15 in the age group 41-50, 
and 13 were 51+ years of age. 
Table 3: The number of key images correctly identified (out of 
Condition 
four) in study two. Success is 4/4. 
Score Distribution 
3 
0 
Similar (n=117) 
 6 (5%) 14 (12%)  26 (22%)  37 (32%) 
1 
2 
Success 
34 (29%) 
Middle (n=114) 
 5 (4%) 
8 (7%)  14 (12%)  20 (18%) 
67 (59%) 
Dissimilar (n=112)   0 (0%) 
6 (5%) 
6 (5%) 
8 (7%) 
74 (70%) 
5.2.2  Accuracy 
We firstly calculated a login success rate on a per-participant basis 
i.e.  to  compare  the  participants  who  correctly  identified  all  4 
images.  This  was  calculated  by  (successful  logins/total  logins). 
The  raw  data  comprised  success/fail  value  to  represent  a  login. 
There  was  a  significant  difference  between  the  performance  of 
participants  in  the  dissimilar  group (70%) and the similar group 
(29%)  Χ2(1,N=229)=37.716,  p<0.01.  In  addition  there  was  a 
significant  difference  between  the  success  rate  in  the  middle 
(59%)  and  similar  condition  Χ2(1,N=231)=20.716,  p<0.01.  The 
difference  between  the  dissimilar  and  the  middle  condition  was 
not  statistically  significant.  Table  3  presents  a  more  detailed 
illustration of participant performance. We also calculated a per-
click success rate that represented (correct clicks/total clicks) for 
each condition. The benefit of this calculation is that it can give 
insight into accuracy in a manner less sensitive to a single mistake  
104
image  the  user  should  select.  This  could  indicate  that  the 
threshold  we  imposed  on  this  instance  of  similarity  was  not 
sufficiently high. The graph also illustrates the interesting case of 
image 6 in the similar condition: there was a large number of user 
errors recorded when they were asked to identify this image. The 
decoy  images  for  this  image  appeared  visually  and  semantically 
similar.  Analysis  of  errors  made  on  a  per-image  basis  across 
conditions  highlighted  a  number  of  significant  results  too  (see 
Table 4).  
5.2.3  Login Duration 
We  also  recorded  time  required  for  users  to  login  in  each 
condition.  This  was  recorded  from  the  first  grid  appearing  on-
screen, until the final click. We treated the data as non-parametric 
due  to  the  existence  of  a  number  of  particularly  long  login 
durations distorting the mean. The median login duration was 57 
seconds in the similar group, in the middle group 40 seconds, and 
in  the  dissimilar  group  36  seconds.  The  difference  between  the 
similar  and  dissimilar  conditions  was  significant  in  a  Mann-
Whitney U test (Z=-4.730, p < 0.01). Using a Wilcoxon 1-sample 
sign  test  we  estimated  the  95%  confidence  interval  for  the 
medians.  This  estimates  that  participants  in  the  dissimilar 
condition  would  take  between  33-40  seconds,  in  the  middle 
condition  38-51  seconds,  and  in  the  similar  condition  48-68 
seconds. This suggests that the choice of decoy selection method 
could  also  have  a  significant  impact  upon  the  login  durations. 
Figure  7  shows  the  distribution  of  login  durations  recorded  for 
successful logins for each condition. 
Figure 7: Length of successful logins in each condition: top 
left) similar; top right) middle; bottom left) dissimilar; bottom 
right) overall. 
6.  SECURITY IMPLICATIONS 
The  introduction  of  deliberate  and  measurable  differences  in 
visual similarity between images creates the potential for traces of 
this  process  to  be  left  behind  and  exploited  by  attackers,  who 