type matches the edge annotation and the heuristic triggers, a
false positive when the heuristic type does not match the edge
annotation but the heuristic triggers, a true negative when the
heuristic type does not match and it does not trigger, and a
false negative when the heuristic type matches but it does
not trigger. We deﬁne the aggregate deep (resp. shallow)
heuristic to trigger if any deep (resp. shallow) heuristic trig-
gers, and the overall heuristic to trigger if exactly one of the
deep or shallow aggregate heuristics trigger. In particular,
if both deep and shallow heuristics trigger on an edge type,
we consider the overall heuristic not to trigger.
Table 5 summarizes our results. DELF deep heuristics
demonstrate precision of 89.7% at 60.7% recall, and DELF
accurately discovers the majority of deep types in our sample.
DELF heuristics prioritize precision. In our experience
2This assumption conservatively penalizes DELF heuristics when mis-
matches occur. Obtaining ground truth data at this scale is impractical.
1068    29th USENIX Security Symposium
USENIX Association
developers are likely to ignore all predictions altogether when
precision drops. One obstacle to further increasing deep
heuristics recall without sacriﬁcing precision are ambiguous
edge types. Consider the photo object type example from Fig-
ure 1. The created_photo edge type pinpoints all photos a
user creates and is annotated deep. Assume this photo object
type is extended with an additional, optional edge type from a
user to a photo to mark the user’s current proﬁle photo. Such
an edge type should be annotated deep; yet a shallow annota-
tion does not result in inadvertent data retention. The original
deep edge type triggers the deletion of all photos—including
the current proﬁle photo—when a user object is deleted. We
observe that developers prefer to annotate ambiguous edge
types as shallow to avoid inadvertent data deletion.
We conclude that DELF static and dynamic veriﬁcation
methods, when used as a safety net to validate developer
annotations, provide an important privacy protection against
mistakes leading to inadvertent data retention. While DELF
cannot detect all instances of inadvertent retention, it detects
most. Hence it makes mistakes signiﬁcantly less common.
Inadvertent data deletion
6.3
We cover next DELF’s impact avoiding data loss in situations
where mistakes leading to inadvertent deletions occur.
Identiﬁed issues. We start with inadvertent deletion vulner-
abilities where DELF altogether avoided exploitation. We
sample all reports generated during one week of November
2019 by DELF privilege escalation checks while blocking
suspicious writes of edge types annotated with deep (§4.3).
Our sample contains 38 distinct edge types, which we for-
warded to FACEBOOK’s security team for inspection. The
team considered the 38 edge types in the list to be potentially
exploitable, modulo the DELF privilege escalation checks and
the existence of public API methods to perform writes. To
the best of our knowledge inadvertent deletion never occurred
despite the underlying insufﬁcient authorization checks.
We look next at incidents where inadvertent deletions oc-
curred, detection required separate logging or user reports,
and DELF restoration logs were used for recovery. We in-
spect all 21 such incidents between February and December
2019. For effective mitigation inadvertent deletions must be
detected before restoration logs expire and the restoration
process must be operationally simple.
A notable incident of an exploited deletion vulnerability
involved deletion of popular photos in Instagram. In October
2019 developers changed how photos were handled. The
incident involved an edge type initially used to associate a dis-
cussion thread with the photo object posted therein. The edge
type annotation was deep—deleting the thread necessitated
deleting the associated photo. Developers later reused the
same edge type when implementing photo sharing; an edge of
the same type now associated a new share discussion thread
with the original photo object. In doing so users who shared
a photo in a new share thread obtained the ability to delete it
by virtue of deleting the new share thread they created.
Instagram users triggered the vulnerability—knowingly or
not—to delete approximately 17,000 photos, including multi-
ple popular public photos with tens of millions of interactions
such as likes and comments. Exploitation was possible be-
cause DELF privilege escalation checks were not enforced in
Instagram when the bug occurred. The issue was surfaced by
user reports within 10 days. The recovery process involved
one product engineer and the DELF oncall; the former pro-
vided the list of objects to restore and the latter monitored
progress. Restorations ran for approximately 10 days.
Many incidents in our sample did not require exploitation
by a third party.
Inadvertent deletions were triggered by
internal maintenance processes or as a result of user action
and affected only the user who performed the action.
A representative example occurred in April 2019. An Face-
book developer triggered a cleanup data migration to delete
objects representing invalid user devices, i.e., objects created
erroneously. The developer ran a database scan over all ex-
isting device objects and scheduled deletions via DELF. Yet
a bug in the object selection logic of the scan triggered the
deletion of a batch of devices every time one object in the
batch was deemed invalid. The process inadvertently deleted
approximately 100 million devices and adversely affected the
ability of users to login as well as service integrity protections.
Product-speciﬁc alerts surfaced the mistake to the team on
the same day. The recovery process spanned 12 hours and
involved 2 engineers. One provided a list of deleted objects
for DELF to restore; the other monitored the process.
Prevention. Assuming timely detection restoration logs re-
duce the issue of data loss to temporary data unavailability. To
quantify DELF’s ability to detect data loss independently, i.e.,
without any user reports or application-speciﬁc logging, we
measure the effectiveness of shallow edge type annotation
heuristics. Table 5 summarizes our results in the scenario
from §6.2. DELF shallow heuristics demonstrate precision
of 93.0% at 89.5% recall. DELF shallow heuristics indepen-
dently pinpoint the majority of mistakes leading to inadvertent
deletion when annotating edge types.
Data loss remains possible. Most notably, shallow heuris-
tics cannot ﬂag cases where application logic requests the
deletion of the wrong object. During our investigation period
signiﬁcant data loss occurred in a single incident. The bug in-
volved application logic requesting the deletion of the wrong
video objects, was surfaced by user reports, and remained un-
detected for 2 years, i.e., signiﬁcantly longer than the deletion
policy allowed DELF restoration logs to persist.
We conclude that DELF restoration logs offer practical
data loss prevention capabilities for most scenarios where
inadvertent deletions occur. While some data loss risk re-
mains, usable restoration logs combined with a sufﬁciently-
long backup retention period provide a practical protection
mechanism even when automated detection mechanisms fail.
USENIX Association
29th USENIX Security Symposium    1069
Figure 6: Cumulative distribution of completion time.
Figure 7: Deletion size with respect to completion time.
6.4 Execution
We continue with an assessment of the system’s impact ex-
ecuting all deletions to completion. Our analysis is based
on observed deletion end-to-end wall time in a production
workload. Our sample includes approximately 12 billion dele-
tions that ﬁnished execution at FACEBOOK on July 31, 2019,
illustrated in Figure 6. Deletions execute in a shared pool of
servers in FACEBOOK’s multi-tenant execution tier.
Identiﬁed issues. We observe transient and persistent errors
delaying the execution of deletions in our sample. DELF
drives deletions to completion despite such errors by retrying
deletions persistently and surfacing detected issues for engi-
neers to ﬁx (§5.1). We discuss in detail one representative
deletion facing transient and one facing persistent errors.
The longest-running deletion in our sample involved delet-
ing a photo and performed 30,134 restoration log writes. The
deletion suffered from at least three distinct types of transient
infrastructure failures. The ﬁrst type involves inadvertent
drops of jobs from FACEBOOK’s asynchronous execution tier.
DELF detected and rescheduled the dropped job in a num-
ber of occasions after a timeout. The second type involves
exponential backoff and rate limits DELF enforces to avoid
overloading underlying data stores (§5.2). The shards in-
volved in this deletion were frequently under heavy load and
DELF postponed the deletion multiple times to prevent further
issues. The third type involves transient write errors frequent
when operating on overloaded shards; those occurred at times
despite rate limiting. Overall, the deletion ran for more than
90 days while making consistent progress.
A deletion affected by persistent errors involved deleting a
user account and performed 1,770 restoration log writes. The
deletion was stuck for 45 days due to two distinct issues, both
requiring engineering intervention. The ﬁrst issue involved
procedural code in a custom section which contained a data
serialization bug. The second issue was triggered by changes
in the semantics of the point-delete operation in an underlying
data store. DELF ﬂagged both issues for engineers to ﬁx and
the deletion completed within 52 days.
Prevention. To quantify how many deletions beneﬁt from
DELF we look at the distribution of end-to-end wall time
of all deletions in our sample. We observe three important
points in Figure 6: P1, P2 and P3, respectively 31 seconds
(35th percentile), 45 seconds (86th percentile) and 90 days
(99.99999th percentile). P1 captures deletions of a single ob-
ject. DELF executes those within the triggering web request
without using the asynchronous execution tier. Shortly after
30 seconds, i.e., a conﬁguration parameter of DELF’s deploy-
ment at FACEBOOK, the ﬁrst run in the asynchronous tier
starts. P2 indicates that a single run within the asynchronous
execution tier is enough to complete the majority of deletions,
i.e., most deletions involve few objects and complete without
issues. P3 illustrates that 99.99999% of deletions complete
within 90 days since they started.
In absence of infrastructure reliability and capacity issues,
deletions would execute to completion without monitoring
from DELF, and completion time would demonstrate a strong
positive correlation with deletion size. To validate their preva-
lence we look into the long tail of deletions running for more
than one day. Figure 7 plots end-to-end wall time required to
complete deletions with respect to the number of restoration
log writes each deletion performed. The number of writes to
restoration logs approximates the size of each deletion.
We observe the correlation between wall time and deletion
size exists yet it is weak for the tail of long-running deletions.
Some deletions consistently leverage additional wall time to
delete more data. In our sample a large deletion running for
30 days performed around 4.8 million restoration log writes
while the largest deletion running for one day was limited to
0.5 million restoration log writes. However, the majority of
deletions running for more than one day are moderately-sized.
We conclude that in the long tail reliability and capacity lim-
itations are the root cause for long-running deletions. DELF
therefore contributes to completing a signiﬁcant proportion
of all deletions. Any deletions that require at least two runs
in the asynchronous execution tier—approximately 14% of
all deletions—beneﬁt. This includes deletions that require
additional execution time because they entail deleting a lot of
data and deletions that run into capacity and reliability issues.
If developers were left to implement deletion unassisted up to
14% of all deletions triggered would potentially not complete.
1070    29th USENIX Security Symposium
USENIX Association
10−610−510−410−310−210−111011020.000.250.500.751.00P1=35.0-thpercentileP2=86.0-thpercentileP3=99.999998-thpercentile0.00.20.40.60.81.0End-to-endcompletionwalltime(days)0.00.20.40.60.81.0Fractionofdeletions1101102End-to-endcompletionwalltime(days)1101102103104105106Restorationlogwrites100101102103104105#deletions7 Related Work
There is little prior work on the problem of deletion correct-
ness. Garg et al. [61] formalize deletion to mandate deletion
of dangling data yet their work does not suggest technical
solutions developers may leverage to achieve the goal. A
presentation from Doshi and Shah outlines Uber’s deletion
service [62] focusing on reliability of user account deletions
speciﬁcally. The system shares design traits with DELF yet
does not offer any capabilities to safeguard correctness [63].
Ritzdorf et al. [64] study deletion correctness motivated by
the problem of helping users delete related data from their
local ﬁle system. They leverage data loss prevention tech-
niques [65] to detect ﬁles storing similar content. Similar
to DELF the authors suggest heuristics to identify what to
delete, e.g., ﬁles accessed together or found to contain dupli-
cate information should be deleted together. To the best of our
knowledge, DELF is the ﬁrst system to apply such techniques
within complex web applications built on top of distributed
data stores. Our work quantiﬁes their effectiveness.
DELF restoration logs are an example of checkpointing,
a technique for recovering from exploited security vulner-
abilities that lead to unauthorized mutation of application
state [48,66–68]. WARP [48]—similar to DELF—targets web
applications, uses a browser extension for intercepting user
actions, and is assessed in a single-node deployment. DELF
demonstrates the applicability of checkpointing in modern,
large-scale, distributed OSNs as a safety net for preventing
inadvertent data deletion while attempting to delete user data.
Recent user research on deletion explores how users of
modern web applications perceive the deletion process and
highlight a pervasive lack of understanding. Murillo et al. [69]
interview users of Gmail and Facebook, report widespread
misconceptions and mistrust, and suggest greater transparency
in products. Ramokapaneet al. [4] document the coping strate-
gies users employ when they cannot ﬁgure out how to delete
data in web applications. Another line of user research studies
the different motives people have to delete data [2, 5].
A well-studied privacy concern regarding deletion is the
effectiveness of individual point deletes [3,70,71]. Prior work
explores the ability to delete data from physical media in a
way that renders the data irrecoverable; suggesting special
ﬁle systems [3] and scrubbing tools [72]. Sarkar et al. [73]
introduce techniques to improve the ability of modern data
stores to propagate point delete operations to physical media
within a bounded time frame. The underlying assumption
in this line of work is that users or developers know what
to delete and when in their applications. We demonstrate
that this assumption is not valid in complex web applications
and we suggest techniques to safeguard deletion correctness.
Minai et al. [74] highlight a conceptually-similar problem of
ineffective deletion for public content in OSNs introduced by
adversarial data mirroring services and suggest mitigations.
Figure 8: Time spent in different operations during deletion.
6.5 Overhead
We close with an assessment of system overhead. We proﬁle
deletions and break down how time is spent in different op-
erations within each deletion. We measure throughput while
deleting a tree-structured graph with unit height stored in TAO
which requires 104 point deletes. The tree contains 100 edges
from types annotated deep with the remaining types anno-
tated shallow. We execute each deletion 10 times on distinct
machines in FACEBOOK’s asynchronous execution tier. We
measure throughput under 4 distinct batching conﬁgurations,
varying the size of the read and write batching windows. We
report the 10th, 50th, and 90th percentiles we observe.
Figure 8 shows our results. We notice four major operations
within each deletion: reads, point deletes, stack management,
and restoration log writes. The latter two entail synchronous
writes for each batch of point deletes: in ZippyDB where
DELF maintains a stack to implement depth-ﬁrst graph traver-
sal, and (b), in LogDevice [60] where DELF persists restora-
tion logs temporarily, respectively. The remaining wall time,
i.e., processing, involves periods of CPU-intensive operations,
such as data serialization. We consider any wall time spent on
operations beyond reads and point deletes as DELF overhead.
We observe that batching reads and writes reduces overall
system overhead from 336% down to 29% (C1 over C4).
Noteworthy, the most time consuming operations are the
write-intensive ones, i.e., point deletes and restoration log
writes. Batching writes with a batch size of 100 (C2 over C4)
has substantial impact on both, reducing time spent in restora-
tion log writes by a factor of approximately 100× and in point
deletes by a factor of approximately 5×. The speedup high-
lights that write batching directly controls the frequency of
writing to restoration logs since only one roundtrip is required
per batch compared to a roundtrip per point delete. Instead,
point deletes entail latency that is not amortized linearly while
batching. Read batching reduces time spent on reads by a
factor of 5× and has limited impact on the rest (C3 over C4).
We conclude that DELF introduces limited overhead during
deletion, in line with systems offering similar guarantees [48].
USENIX Association
29th USENIX Security Symposium    1071
stackmanagementreadspointdeletesrestorationlogwritesprocessing100101102103104105Walltime(sec)C1:readbatch=100writebatch=100C2:readbatch=1writebatch=100C3:readbatch=100writebatch=1C4:readbatch=1writebatch=18 Future Work
Exploring the applicability of DELF outside OSNs requires
further research. We anticipate deletion frameworks based on
declarative annotations similar to DELF to be widely applica-
ble across application domains and data stores. Some DELF
validation techniques can be adapted to discover mistakes in
existing applications without necessitating changes, facilitat-
ing correctness studies. We expect that in any domain with
complex applications handling user data, deletion correctness
validation will surface mistakes on an ongoing basis.
DELF’s ability to validate data type annotations can be ex-
tended further. A straightforward approach involves replacing
edge type classiﬁcation heuristics with machine-learned mod-
els trained on prior developer annotations. We expect such
approaches to signiﬁcantly improve the precision and recall
of our current system, perhaps even surpassing developers.
Developers may create objects that do not get deleted even
when all type-level annotations are correct. One way involves
creating individual objects and omitting writing the corre-
sponding edges necessary for deletion, e.g., creating a photo
without a deep-annotated edge from its creator. DELF can
enforce annotations at the data item level in addition to the
data type level to preclude the creation of undeletable objects.
DELF annotations can be extended to be tied to tooling
used for Privacy Impact Assessments [75]. When a particular
deletion product behavior is mandated by an assessment, one
could tie that decision to product implementation via DELF.
We anticipate further improvements in deletion trans-
parency, accountability, and external correctness veriﬁcation.
Systems such as DELF can expose a transparency interface
to indicate what data items get deleted from data stores and
when; security researchers could use such interfaces to con-