title:Blind Backdoors in Deep Learning Models
author:Eugene Bagdasaryan and
Vitaly Shmatikov
Blind Backdoors in Deep Learning Models
Eugene Bagdasaryan and Vitaly Shmatikov, Cornell Tech
https://www.usenix.org/conference/usenixsecurity21/presentation/bagdasaryan
This paper is included in the Proceedings of the 30th USENIX Security Symposium.August 11–13, 2021978-1-939133-24-3Open access to the Proceedings of the 30th USENIX Security Symposium is sponsored by USENIX.Blind Backdoors in Deep Learning Models
Eugene Bagdasaryan
Cornell Tech
PI:EMAIL
Vitaly Shmatikov
Cornell Tech
PI:EMAIL
Abstract
We investigate a new method for injecting backdoors into
machine learning models, based on compromising the loss-
value computation in the model-training code. We use it to
demonstrate new classes of backdoors strictly more powerful
than those in the prior literature: single-pixel and physical
backdoors in ImageNet models, backdoors that switch the
model to a covert, privacy-violating task, and backdoors that
do not require inference-time input modiﬁcations.
Our attack is blind: the attacker cannot modify the training
data, nor observe the execution of his code, nor access the
resulting model. The attack code creates poisoned training
inputs “on the ﬂy,” as the model is training, and uses multi-
objective optimization to achieve high accuracy on both the
main and backdoor tasks. We show how a blind attack can
evade any known defense and propose new ones.
1 Introduction
A backdoor is a covert functionality in a machine learning
model that causes it to produce incorrect outputs on inputs
containing a certain “trigger” feature chosen by the attacker.
Prior work demonstrated how backdoors can be introduced
into a model by an attacker who poisons the training data with
specially crafted inputs [5, 6, 28, 92], or else by an attacker
who trains the model in outsourced-training and model-reuse
scenarios [40, 55, 58, 98]. These backdoors are weaker ver-
sions of UAPs, universal adversarial perturbations [8, 61].
Just like UAPs, a backdoor transformation applied to any in-
put causes the model to misclassify it to an attacker-chosen
label, but whereas UAPs work against unmodiﬁed models,
backdoors require the attacker to both change the model and
change the input at inference time.
Our contributions. We investigate a new vector for backdoor
attacks: code poisoning. Machine learning pipelines include
code from open-source and proprietary repositories, managed
via build and integration tools. Code management platforms
are known vectors for malicious code injection, enabling at-
tackers to directly modify source and binary code [7, 19, 67].
Figure 1: Machine learning pipeline.
Source-code backdoors of the type studied in this paper
can be discovered by code inspection and analysis. Today,
even popular ML repositories [33, 42, 62, 96], which have
thousands of forks, are accompanied only by rudimentary
tests (such as testing the shape of the output). We hope to
motivate ML developers to carefully review the functionality
added by every commit and design automated tests for the
presence of backdoor code.
Code poisoning is a blind attack. When implementing the
attack code, the attacker does not have access to the training
data on which it will operate. He cannot observe the code
during its execution, nor the resulting model, nor any other
output of the training process (see Figure 1).
Our prototype attack code1 synthesizes poisoning inputs
“on the ﬂy” when computing loss values during training. This
is not enough, however. A blind attack cannot combine main-
task, backdoor, and defense-evasion objectives into a single
loss function as in [3, 84] because (a) the scaling coefﬁcients
are data- and model-dependent and cannot be precomputed
1Available at https://github.com/ebagdasa/backdoors101.
USENIX Association
30th USENIX Security Symposium    1505
Training DataPublic repoModel TrainingServeTrojaningPoisoningAdversarial ExamplesBlind AttackModel ReplacementDataAttacks that require access to training data and/or trained modelCompromise Loss ComputationPrivate repoContinuous Integrationby a code-only attacker, and (b) a ﬁxed combination is subop-
timal when the losses represent different tasks.
We view backdoor injection as an instance of multi-task
learning for conﬂicting objectives—namely, training the same
model for high accuracy on the main and backdoor tasks si-
multaneously—and use Multiple Gradient Descent Algorithm
with the Franke-Wolfe optimizer [16, 81] to ﬁnd an optimal,
self-balancing loss function that achieves high accuracy on
both the main and backdoor tasks.
To illustrate the power of blind attacks, we use them to
inject (1) single-pixel and physical backdoors in ImageNet;
(2) backdoors that switch the model to an entirely different,
privacy-violating functionality, e.g., cause a model that counts
the number of faces in a photo to covertly recognize speciﬁc
individuals; and (3) semantic backdoors that do not require the
attacker to modify the input at inference time, e.g., cause all
reviews containing a certain name to be classiﬁed as positive.
We analyze all previously proposed defenses against back-
doors: discovering backdoors by input perturbation [95], de-
tecting anomalies in model behavior on backdoor inputs [12],
and suppressing the inﬂuence of outliers [32]. We show how a
blind attacker can evade any of them by incorporating defense
evasion into the loss computation.
Finally, we report the performance overhead of our attacks
and discuss better defenses, including certiﬁed robustness [27,
71] and trusted computational graphs.
2 Backdoors in Deep Learning Models
2.1 Machine learning background
The goal of a machine learning algorithm is to compute a
model θ that approximates some task m : X → Y , which
maps inputs from domain X to labels from domain Y . In
supervised learning, the algorithm iterates over a training
dataset drawn from X × Y . Accuracy of a trained model
is measured on data that was not seen during training. We
focus on neural networks [25]. For each tuple (x,y) in the
dataset, the algorithm computes the loss value (cid:96) = L(θ(x),y)
using some criterion L (e.g., cross-entropy or mean square
error), then updates the model with the gradients g = ∇(cid:96) using
backpropagation [74]. Table 1 shows our notation.
2.2 Backdoors
Prior work [28, 55] focused on universal pixel-pattern back-
doors in image classiﬁcation tasks. These backdoors involve a
normal model θ and a backdoored model θ∗ that performs the
same task as θ on unmodiﬁed inputs, i.e., θ(x) = θ∗(x) = y. If
at inference time a certain pixel pattern is added to the input,
then θ∗ assigns a ﬁxed, incorrect label to it, i.e., θ∗(x∗) = y∗,
whereas θ(x∗) = θ(x) = y.
We take a broader view of backdoors as an instance of multi-
task learning where the model is simultaneously trained for
its original (main) task and a backdoor task injected by the at-
tacker. Triggering the backdoor need not require the adversary
to modify the input at inference time, and the backdoor need
Table 1: Notation.
Term
X × Y
m : X → Y
θ
θ∗
µ : X → X ∗
ν : X ,Y → Y ∗
Bd : X→{0,1}
L
(cid:96) = L(θ(x),y)
g = ∇(cid:96)
Description
domain space of inputs X and labels Y
learning task
normal model
backdoored model
backdoor input synthesizer
backdoor label synthesizer
input has the backdoor feature
loss criterion
computed loss value
gradient for the loss (cid:96)
not be universal, i.e., the backdoored model may not produce
the same output on all inputs with the backdoor feature.
We say that a model θ∗ for task m: X → Y is “backdoored”
if it supports another, adversarial task m∗: X ∗ → Y ∗:
1. Main task m: θ∗(x) = y, ∀(x,y) ∈ (X \ X ∗,Y )
2. Backdoor task m∗: θ∗(x∗) = y∗, ∀(x∗,y∗) ∈ (X ∗,Y ∗)
The domain X ∗ of inputs that trigger the backdoor is
deﬁned by the predicate Bd : x → {0,1} such that for all
x∗ ∈ X ∗, Bd(x∗) = 1 and for all x ∈ X \ X ∗, Bd(x) = 0. In-
tuitively, Bd(x∗) holds if x∗ contains a backdoor feature or
trigger. In the case of pixel-pattern or physical backdoors, this
feature is added to x by a synthesis function µ that generates
inputs x∗ ∈ X ∗ such that X ∗∩X = Ø. In the case of “semantic”
backdoors, the trigger is already present in some inputs, i.e.,
x∗ ∈ X . Figure 2 illustrates the difference.
The accuracy of the backdoored model θ∗ on task m should
be similar to a non-backdoored model θ that was correctly
trained on data from X × Y . In effect, the backdoored model
θ∗ should support two tasks, m and m∗, and switch between
them when the backdoor feature is present in an input. In con-
trast to the conventional multi-task learning, where the tasks
have different output spaces, θ∗ must use the same output
space for both tasks. Therefore, the backdoor labels Y ∗ must
be a subdomain of Y .
2.3 Backdoor features (triggers)
Inference-time modiﬁcation. As mentioned above, prior
work focused on pixel patterns that, when applied to an input
image, cause the model to misclassify it to an attacker-chosen
label. These backdoors have the same effect as “adversarial
patches” [8] but in a strictly inferior threat model because the
attacker must modify (not just observe) the ML model.
We generalize these backdoors as a transformation µ : X →
X ∗ that can include ﬂipping, pixel swapping, squeezing, col-
oring, etc. Inputs x and x∗ could be visually similar (e.g., if µ
modiﬁes a single pixel), but µ must be applied to x at inference
1506    30th USENIX Security Symposium
USENIX Association
Figure 2: Examples of backdoor features. (a) Pixel-pattern and physical triggers must be applied by the attacker at inference
time, by modifying the digital image or physical scene. (b) A trigger word combination can occur in an unmodiﬁed sentence.
time. This attack exploits the fact that θ accepts inputs not
only from the domain X of actual images, but also from the
domain X ∗ of modiﬁed images produced by µ.
A single model can support multiple backdoors, repre-
sented by synthesizers µ1,µ2 ∈ M and corresponding to dif-
2 : X µ2 → Y µ2. We
ferent backdoor tasks: m∗
show that a backdoored model can switch between these tasks
depending on the backdoor feature(s) present in an input.
1 : X µ1 → Y µ1, m∗
Physical backdoors do not require the attacker to modify
the digital input [49]. Instead, they are triggered by certain
features of physical scenes, e.g., the presence of certain ob-
jects—see Figure 2(a). In contrast to physical adversarial ex-
amples [22, 52], which involve artiﬁcially generated objects,
we focus on backdoors triggered by real objects.
No inference-time modiﬁcation. Semantic backdoor fea-
tures can be present in a digital or physical input without
the attacker modifying it at inference time: for example, a
certain combination of words in a sentence, or, in images, a
rare color of an object such as a car [3]. The domain X ∗ of
inputs with the backdoor feature should be a small subset of
X . The backdoored model cannot be accurate on both the
main and backdoor tasks otherwise, because, by deﬁnition,
these tasks conﬂict on X ∗.
When training a backdoored model, the attacker may use
µ : X → X ∗ to create new training inputs with the backdoor
feature if needed, but µ cannot be applied at inference time
because the attacker does not have access to the input.
Data- and model-independent backdoors. As we show in
the rest of this paper, µ : X → X ∗ that deﬁnes the backdoor
can be independent of the speciﬁc training data and model
weights. By contrast, prior work on Trojan attacks [55, 58,
103] assumes that the attacker can both observe and modify
the model, while data poisoning [28, 92] assumes that the
attacker can modify the training data.
2.4 Backdoor functionality
Prior work assumed that backdoored inputs are always
(mis)classiﬁed to an attacker-chosen class, i.e., ||Y ∗|| = 1.
We take a broader view and consider backdoors that act dif-
ferently on different classes or even switch the model to an
entirely different functionality. We formalize this via a syn-
thesizer ν : X ,Y → Y ∗ that, given an input x and its correct
label y, deﬁnes how the backdoored model classiﬁes x if x
contains the backdoor feature, i.e., Bd(x). Our deﬁnition of
the backdoor thus supports injection of an entirely different
task m∗ : X ∗ → Y ∗ that “coexists” in the model with the main
task m on the same input and output space—see Section 4.3.
2.5 Previously proposed attack vectors
Figure 1 shows a high-level overview of a typical machine
learning pipeline.
Poisoning. The attacker can inject backdoored data X ∗ (e.g.,
incorrectly labeled images) into the training dataset [5, 10,
28, 38, 92]. Data poisoning is not feasible when the data is
trusted, generated internally, or difﬁcult to modify (e.g., if
training images are generated by secure cameras).
Trojaning and model replacement. This threat model [55,
86, 103] assumes an attacker who controls model training and
has white-box access to the resulting model, or even directly
modiﬁes the model at inference time [14, 29].
Adversarial examples. Universal adversarial perturba-
tions [8, 61] assume that the attacker has white- or black-box
access to an unmodiﬁed model. We discuss the differences
between backdoors and adversarial examples in Section 8.2.
3 Blind Code Poisoning
3.1 Threat model
Much of the code in a typical ML pipeline has not been
developed by the operator. Industrial ML codebases for tasks
such as face identiﬁcation and natural language processing
include code from open-source projects frequently updated
by dozens of contributors, modules from commercial vendors,
and proprietary code managed via local or outsourced build
and integration tools. Recent, high-visibility attacks [7, 67]
demonstrated that compromised code is a realistic threat.
In ML pipelines, a code-only attacker is weaker than a
model-poisoning or trojaning attacker [28, 55, 57] because
he does not observe the training data, nor the training process,
not the resulting model. Therefore, we refer to code-only
USENIX Association
30th USENIX Security Symposium    1507
Directed by Ed Wood.𝜇pixel patternbackdoorphysical backdoor(a) adversary-modified input(b) unmodified inputpoisoning attacks as blind attacks.
Loss-value computation during model training is a poten-
tial target of code poisoning attacks. Conceptually, loss value
(cid:96) is computed by, ﬁrst, applying the model to some inputs and,
second, comparing the resulting outputs with the expected
labels using a loss criterion (e.g., cross-entropy). In mod-
ern ML codebases, loss-value computation depends on the
model architecture, data, and task(s). For example, the three
most popular PyTorch repositories on GitHub, fairseq [62],
transformers [96], and fast.ai [33], all include multiple loss-
value computations speciﬁc to complex image and language
tasks. Both fairseq and fast.ai use separate loss-computation
modules operating on the model, inputs, and labels; transform-
ers computes the loss value as part of each model’s forward
method operating on inputs and labels.2
Today, manual code review is the only defense against the
injection of malicious code into open-source ML frameworks.
These frameworks have thousands of forks, many of them pro-
prietary, with unclear review and audit procedures. Whereas