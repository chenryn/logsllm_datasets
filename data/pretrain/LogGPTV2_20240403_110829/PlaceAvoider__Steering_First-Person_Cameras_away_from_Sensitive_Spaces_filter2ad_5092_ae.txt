Random sub-sampling simply chooses a subset of the
SIFT descriptors at random.
We evaluated these four model reduction techniques by
setting their parameters such that each one reduced the number
of descriptors by a factor of ﬁve on a subset of our Workspace
2 dataset. We found that of these techniques, ANN suffered
the worst accuracy (73.7% compared to 93.2% with the full
model), followed by k-means (87.3%) and hashing (87.3%).
Surprisingly, random subsampling actually worked the best of
these techniques (87.9%).
Classiﬁer running times. Table VI presents the average run-
ning time for our various local and global feature classiﬁers,
including the local classiﬁer with full room models and one
randomly subsampled to 10%. As a point of reference, we also
show the accuracy of each individual feature type in classifying
images from the House 3 cross-validation dataset. We observe
that most of the computation time is due to one feature, global
Dense Bags of SIFT, due to the fact that it has to compute
SIFT descriptors along a dense image grid. The other features
show a general trade-off between accuracy and running time:
local feature matching performs best (95.7%) but requires the
most time (2.5 seconds), whereas RGB histograms require only
a few milliseconds per image, but the accuracy is quite low
(56.3%).
Once the local and global classiﬁers are done, stream
classiﬁcation using HMMs is very fast, taking about 0.077
seconds to classify an entire stream or about 0.1 milliseconds
per image. HMM inference takes asymptotic time linear in the
number of images and quadratic in the number of rooms.
A lightweight classiﬁer. From the results in Table VI we
hypothesized that we could build a lightweight classiﬁer that
10
(cid:43)(cid:82)(cid:88)(cid:86)(cid:72)(cid:3)(cid:20)
(cid:43)(cid:82)(cid:88)(cid:86)(cid:72)(cid:3)(cid:21)
(cid:43)(cid:82)(cid:88)(cid:86)(cid:72)(cid:3)(cid:22)
(cid:58)(cid:82)(cid:85)(cid:78)(cid:83)(cid:79)(cid:68)(cid:70)(cid:72)(cid:3)(cid:20)
(cid:58)(cid:82)(cid:85)(cid:78)(cid:83)(cid:79)(cid:68)(cid:70)(cid:72)(cid:3)(cid:21)
(cid:19)
(cid:17)
(cid:20)
(cid:27)
(cid:17)
(cid:19)
(cid:25)
(cid:17)
(cid:19)
(cid:23)
(cid:17)
(cid:19)
(cid:21)
(cid:17)
(cid:19)
(cid:19)
(cid:17)
(cid:20)
(cid:27)
(cid:17)
(cid:19)
(cid:25)
(cid:17)
(cid:19)
(cid:23)
(cid:17)
(cid:19)
(cid:21)
(cid:17)
(cid:19)
(cid:19)(cid:17)(cid:21)
(cid:43)(cid:82)(cid:88)(cid:86)(cid:72)(cid:3)(cid:20)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
(cid:19)(cid:17)(cid:27)
(cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:21)
(cid:43)(cid:82)(cid:88)(cid:86)(cid:72)(cid:3)(cid:21)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
(cid:19)(cid:17)(cid:27)
(cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:21)
(cid:79)(cid:82)(cid:70)(cid:68)(cid:79)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)
s
m
a
e
r
t
s
l
a
n
g
i
r
O
i
(cid:76)
(cid:76)
(cid:81)
(cid:82)
(cid:86)
(cid:70)
(cid:72)
(cid:85)
(cid:83)
(cid:76)
(cid:76)
(cid:81)
(cid:82)
(cid:86)
(cid:70)
(cid:72)
(cid:85)
(cid:83)
s
m
a
e
r
t
s
y
s
i
o
N
(cid:43)(cid:82)(cid:88)(cid:86)(cid:72)(cid:3)(cid:22)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
(cid:74)(cid:79)(cid:82)(cid:69)(cid:68)(cid:79)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)
(cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:27)
(cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
(cid:19)(cid:17)(cid:27)
(cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:21)
(cid:79)(cid:82)(cid:70)(cid:68)(cid:79)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:27)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
(cid:74)(cid:79)(cid:82)(cid:69)(cid:68)(cid:79)(cid:3)(cid:70)(cid:79)(cid:68)(cid:86)(cid:86)(cid:76)(cid:73)(cid:76)(cid:72)(cid:85)
(cid:19)(cid:17)(cid:27)
(cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:27)
(cid:20)(cid:17)(cid:19)
(cid:58)(cid:82)(cid:85)(cid:78)(cid:83)(cid:79)(cid:68)(cid:70)(cid:72)(cid:3)(cid:20)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
(cid:58)(cid:82)(cid:85)(cid:78)(cid:83)(cid:79)(cid:68)(cid:70)(cid:72)(cid:3)(cid:21)
(cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:27)
(cid:20)(cid:17)(cid:19)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
(cid:43)(cid:48)(cid:48)
(cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:21)
(cid:43)(cid:48)(cid:48)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:27)
(cid:20)(cid:17)(cid:19)
(cid:19)(cid:17)(cid:21)
(cid:19)(cid:17)(cid:23)
(cid:19)(cid:17)(cid:25)
(cid:19)(cid:17)(cid:27)
(cid:20)(cid:17)(cid:19)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
(cid:85)(cid:72)(cid:70)(cid:68)(cid:79)(cid:79)
Fig. 7. Top: Precision-Recall curves for retrieving images from a given room, averaged over the ﬁve rooms, for each of our ﬁve datasets. This represents image
retrieval in the closed locale scenario. Bottom: P-R curves for retrieving images from our noisy dataset with images from other rooms injected into the stream,
simulating the open locale scenario.
works nearly as well as the full classiﬁer. We thus built
a simple classiﬁer consisting of one local and one global
feature, chosen by their low computation demand compared to
accuracy: local subsampled features and global bags of SIFT.
We tested this lightweight classiﬁer on the Workspace 2 and
House 3 datasets, ﬁnding that it reduces the average classiﬁ-
cation accuracy from about 99.9% to 90.5%, but reduces the
computation time by over an order of magnitude (from 18.421
seconds to 1.465 seconds per image). Such a lightweight
classiﬁer could likely be run on a mobile device.
V. DISCUSSION
Cloud vs. local computation. As discussed in Section IV-E,
limited mobile computation resources may impact the per-
formance of PlaceAvoider’s
image classiﬁcation. While
lightweight classiﬁers (suitable for the mobile device) provide
reasonable accuracy, maximal performance for realtime appli-
cations could be realized by outsourcing computation to the
cloud, similar to apps on mobile devices today. This requires
network connectivity and sufﬁcient bandwidth resources for
image uploads. As shown in Table II, our classiﬁers perform
well with down-sampled images,
thus reducing bandwidth
requirements.
An implementation of PlaceAvoider may default to classiﬁ-
cation on the cloud, but utilize a lightweight onboard classiﬁer
(Section IV) during periods of wireless unavailability. Cloud-
based photo-sharing sites could also integrate the full version
of PlaceAvoider to aid in labeling of images and content based
image retrieval.
Privacy of other people. PlaceAvoider offers users a degree of
useful control over images collected in their personal spaces,
but this control is limited only to the person’s own camera.
People’s concerns of imaging privacy are often due to the
presence of cameras that are not their own. To mitigate these
concerns, we would ideally allow people to specify policies for
other cameras, even when they are owned by other people. For
example, people may share their policies with other users (e.g.,
social or professional contacts), or use a central repository
for such sharing (e.g., Bob enrolls his ofﬁce in PlaceAvoider
to prevent Mary from taking a photo in that space with
her phone). Such protocols for surveillance cameras, a more
tractable problem, have been proposed [4], [48]. A major
challenge with such an approach is that people may not want
to share their stream and/or enrollment photos due to privacy
concerns, and even sharing abstract models may reveal some
private details. Halderman et. al. address this concern in their
system that requires unanimous consent amongst bystanders in
a privacy-preserving way [22]. Policies may be enforced using
models based on secure SIFT (based on features extracted from
homomorphically encrypted images), which has been shown to
perform well [25].
Improving image classiﬁcation. While PlaceAvoider generally
performed well in our evaluation, it does not yield perfect
classiﬁcation accuracy and thus has much room for improve-
ment. For example, we investigated the lower performance of
datasets for House 1 and House 2, and found a high negative
correlation between classiﬁer performance and the variance
in the quantity of extracted SIFT features among spaces in
the dataset; classiﬁer bias is induced towards rooms that have
more SIFT features. Minimizing these bias effects amongst
enrolled spaces should signiﬁcantly improve overall classi-
ﬁer performance. Our local feature extraction uses grayscale
images, as is standard practice; integrating color information
may improve performance signiﬁcantly. Finally, we employed
11
no conditioning, noise-reduction, ﬁltering, or other processing
of images before feature extraction, and pre-processing steps
could improve classiﬁer performance. We leave these areas of
improvement for future work.
Leveraging other characteristics. The image classiﬁcation
techniques we use offer reasonable performance to analyze
large streams of images. More sophisticated analysis is possi-
ble and could offer improvements to PlaceAvoider. For exam-
ple, people could enroll speciﬁc objects in a room, and these
could be used to identify sensitive spaces (e.g., if a particular
art object or high-end electronics device is detected in an
image). While our enrollment process is not burdensome, the
system would be improved by bootstrapping available images
to eliminate the collection of separate enrollment images.
Other semantic, scene-level analyses could offer better
identiﬁcation of sensitive images, even in areas that have
not been enrolled, using scene classiﬁcation algorithms [56],
[41], [33]. For instance, we could build systems that
try
to estimate a general type of room, like kitchen, bathroom,
etc., based on general models of these spaces (i.e. what
these rooms typically look like). While this general scene
categorization would be desirable, computer vision work has
shown that recognizing speciﬁc targets is much more accurate
than recognizing categories of objects; e.g., it is much easier
to build a speciﬁc model of your bathroom than a general
model to recognize any bathroom. Another possibility is to
analyze the poses and activities of people in the scene to
provide additional evidence to the classiﬁer, using work on
people and pose recognition [11], [14]; photos showing people
in distress, in compromising poses, or wearing little clothing
could be ﬂagged as sensitive. We leave such an exploration to
future work.
VI. RELATED WORK
Lifelogging issues and privacy. Allen [1] and Cheng et al. [8]
demonstrate that there is a maelstrom of legal issues related
to lifelogging, many of which are privacy related. Speciﬁcally,
Allen discusses how in the United States, cloud-stored life logs
are not afforded 4th and 14th Amendment protections, and
this raises the importance of controlling which information to
log. The expert opinions on lifelogging privacy issues were
validated by a user study that was performed to measure per-
ceptions of lifelogging [28]. They found that users want control
over the data that is collected and stored, thus motivating the
need for technologies like PlaceAvoider. The existing work that
seeks to preserve privacy for lifelogging is notably limited.
Chaudhari et al. [7] offer a protocol to detect and obfuscate
faces in lifelogged video streams. Interestingly, the bulk of
cited work on lifelogging was framed with then-current tech-
nology. Current lifelogging devices and smartphone lifelogging
apps (like Saga7) are much more advanced in their collection
capabilities while not addressing many privacy concerns.
Camera permissions. Systems like PlaceRaider [52] demon-
strate the need for controls on the use of cameras on smart-
phones and problems that can stem from coarse-grained per-
missions. The inadequacy of coarse-grained permission sys-
tems for sensitive resources has been well documented. Bugiel
7Saga: http://www.getsaga.com
et al. include a survey of least-privilege-preserving approaches
in their XMandroid paper [5] and propose a defense system to
prevent privilege escalation. Privilege escalation is an orthogo-
nal problem and has been addressed by systems that automat-
ically prevent installing programs based on permissions [16],
[12] or that monitor inter-app communications [19], [6] among
other approaches.
Systems have been proposed that can enforce ﬁne-grained
permission policies including Apex [40], Porscha [42], and
CRePE [10]. Similarly, labeled images can be tracked with
mechanisms like TaintDroid [15] and Paranoid Android [44].
PlaceAvoider differs from these systems in that it can dynam-
ically assess the sensitivity of sensor-data content.
Roesner et al. [46] implement a system where the enu-
meration of ﬁne-grained policy rules is not necessary, instead
electing to capture user intent at the time of resource use. This
approach helps in applications where users deliberately tap a
button to take a photograph and are explicitly aware of the
speciﬁc photo being taken. Our work addresses precisely the