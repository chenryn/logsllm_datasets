non-overlapping instructions. From this set of layers, we re-
construct an enhanced control ﬂow graph (see Section 4.3).
Next, we can apply other techniques to discover new pieces
of code thus obtaining a speculative disassembly of the code
(see Section 4.4).
CoDisasm overview The overall architecture of our dis-
assembler CoDisasm is shown in Figure 5. The CoDisasm
disassembler performs a static disassembly along with a con-
crete execution with the aim of maximizing coverage.
It
includes two main components:
1. A dynamic analysis component that collects execution
traces of the threads and processes of a binary run.
For this, we developed Pin tracer to instrument code,
which is based on Pin [24]. We recover each code wave
by taking a memory snapshot at the beginning of the
wave, as explained in Section 3. A Portable Executable
(PE) ﬁle is then built for each snapshot.
2. From the execution traces, each memory snapshot is
disassembled following the algorithm described in Sec-
tion 4, taking care of overlapping instructions. At the
end, we have a sequence of disassembled code waves,
which corresponds to the code discovered thanks to the
set of collected traces.
747information are not always easily obtainable. Lastly, the in-
sertion of junk code after a call, like explained in [23], can
fool disassemblers. Kruegel et al. [22] developed a disassem-
bly heuristic by parsing the memory byte by byte in order to
construct all possible CFG with their relationships. Then,
they state several principles in order to rule out certain CFG,
like the fact that the parsed code never contains overlapped
instructions.
Another direction is to identify the values of a register by
static analysis. As a result, we may expect to ﬁnd the range
of an indirect jump or to identify the return address on the
stack in order to determine where a ret return instruction
will go. This kind of analysis is based on a combination of
methods like program slicing, constant propagation and ab-
stract interpretation, which are now quite mastered for high-
level programming languages. The adaptation of static anal-
ysis to binaries turns out to be diﬃcult because most of the
assumptions on which formal methods lean, are violated in
machine code. For low-level programming languages, Reps
et al. [29] developed CodeSurfer, a tool that computes an
approximation of the register values by using the value-set-
analysis algorithm. Kinder and Veith [20] developed Jakstab
based on data ﬂow analysis together with control ﬂow recon-
struction. More recently, Bardin, Herrmann and V´edrine [1]
combine advantages of both of these methods. An interest-
ing new direction has been proposed by Ogawa et al. [15],
who suggest a pushdown model of assembly language to de-
termine register values thanks to SMT solvers. In all cases,
static analysis tools build an over-approximation which is
often too imprecise and reports a lot of false positives. That
is why we have taken a more pragmatic path by combining
dynamic and static disassembly methods.
We are aware of a few other approaches that combine
static and dynamic disassembly. Kinder and Kravchenko [19]
propose to narrow the search space of static analysis by using
traces. Their goal is to improve Dynamic Symbolic Evalu-
ation (DSE). The dynamically collected information helps
the symbolic step, for example by suggesting relevant ap-
proximations by concretization. They do not address the
problem of self-modiﬁcation. The platform BIRD of Nanda
et al. [27] apply speculative disassembly by mixing static and
dynamic techniques. The diﬀerence with our work is that
BIRD is designed for non-obfuscated binaries. Caballero
et al. [2] developed a similar disassembly process in order
to extract input/output interface to reuse binary functions.
Their approach uses a dynamic analysis that collects a trace
and a memory dump and then they disassemble the memory
dump using trace information. The diﬀerence with our work
is that their disassembly process is based on a single memory
snapshot and does not handle overlapping instruction.
3. SELF-MODIFYING CODE
3.1 Wave semantics
When analyzing binary code, we are faced with code that
is, most of the time, self-modifying. This is why, we propose
a model of execution of self-modifying code based on code
waves. The idea is to associate, at any time, and to each
memory address a write level and an execution level. At
the beginning, for every address the execution level is set
to 1 and the write level to 0. Every data written by an
instruction of execution level 1 increases its write level to 1.
This typical situation corresponds to an unpacker, which
Figure 5: CoDisasmdisassembler architecture
CoDisasm is available at http://www.lhs.loria.fr in the
research pane. In our lab, CoDisasm was deployed on 100
virtual machines and so 500 malware are analyzed in less
than 20 minutes.
We do think that the combination of dynamic and static
disassembly may be of particular use on malware. That is
why, we have called this approach Concatic which stands
for Concrete executions and static disassembly. To support
this claim we report on some tests and evaluations we have
made of CoDisasm in Section 5. We discuss limitations of
our approach 6 and then conclude in Section 7.
2. RELATED WORK
Most of the previous work on disassembly has focused on
static disassembly. A static disassembler extracts the as-
sembly code from a binary without running it. Tradition-
ally, two methods of static disassembly have been employed.
The ﬁrst method is a linear sweep of the memory. It is used
by several link-time optimizers and by the GNU utility tool
objdump. This method consists in parsing the memory from
an entry-point, opcode by opcode. The main drawbacks
of this simple method are that (i) the data within the ana-
lyzed binary is interpreted as opcodes, potentially leading to
an incorrect result, and (ii) it does not handle overlapping
codes.
The second method is a recursive traversal of the code,
which consists in examining and following each instruction’s
successor. When a conditional jump is encountered, like a
jcc, both branches are parsed. Thus, a priori, data is not
misinterpreted. However, in certain cases, and in particu-
lar in malware analysis, the control ﬂow can be obfuscated
and this might be done in several diﬀerent ways. As a re-
sult, Moser, Kruegel and Kirda [26] showed that commercial
virus scanners do not detect many known viruses that are
obfuscated by these methods. Linn and Debray [23] sug-
gest the insertion of opaque predicates [8] so that only one
branch of a conditional jump is taken, since the other branch
may contain junk code which might be taken as valid by a
recursive disassembler. Indirect jumps [23] are another way
to thwart disassembly. As a consequence, most of the dis-
assemblers are hybrid, that is, they use both methods: lin-
ear sweep and recursive traversal.
In the case of indirect
jump obfuscations, Schwartz, Debray and Andrews [31] pro-
pose a method to recover jump tables in order to identify
all possible jumps. However, symbol tables and relocation
The overall architectureBinary!programTracerWave!recoveringMemory!snapshotsExecution !tracesDisassemblerWe combine concrete path execution and static analysisCodisasm still under construction748Addresses 0x01006e
Bytes
Layer 1 @0x01006e7a
Layer 2 @0x01006e7e
7b
04
7c
7a
fe
0b
inc [ebx+ecx]
7e
7d
eb
ﬀ
jmp +1
7f
c9
80
7f
81
e6
dec ecx
jg 0x1006e68
Figure 6: Layers of a subset of the TELock code segment
decompresses some piece of data.
In our model, data are
decompressed in a memory area and so each address of this
area gets write level 1. Then, the unpacker transfers on-
the-ﬂy the control to the “decompressed” data. As a result,
data at write level 1 is executed, thus triggering the second
wave of execution, and we set the execution level to 2. In
turn, Wave 2 may generate a third wave and this process
may repeat.
We deﬁne Wave k as the whole set of instructions, ex-
ecuted or not, which are present when the execution level
reaches k (See discussion in Section 6). As a result, we can
see a run of a program as a sequence of waves. Notice that
in this model, non-self-modifying code will only have one
wave.
The rationale behind the model of a self-modifying code
run as a sequence of waves is that we can extract a snapshot
of the memory at the beginning of each wave from the execu-
tion of a binary. This snapshot contains all the instructions
deployed by the binary to run this wave and possibly some
silent code. Our objective is then to disassemble this mem-
ory snapshot in order to recover the assembly code contained
in a wave.
The wave semantics that we propose is deﬁned at the low-
est possible level of abstraction in the sense that we see all
computations inside the system through the eyes of the sin-
gle core processor. Consequently, this model takes into con-
sideration threads and processes.
3.2 Collecting execution traces
In practice, we focus exclusively on Windows/x86 bina-
ries. To this end, we use Pin which is a dynamic instrumen-
tation framework supported by Intel [24]. We developed and
used a Pin tool, that we refer to as Pin tracer, to collect ex-
ecution traces of x86-code. Pin tracer is able to trace newly
created threads and processes. It also tries to detect code
injection in a running process. If such an event occurs, it
instruments the injected process. For example, the driver
of Duqu illustrates this mechanisms by injected in memory
In this case, Pin tracer traces ser-
within service.exe.
vice.exe. Code injections are detected by monitoring calls
to the CreateRemoteThread and CreateRemoteThreadEx func-
tions from the Windows API. When Pin tracer detects a
new process, then a new pin tool is attached to this process.
Thus, a new trace is generated. Finally, we collect all traces
of the threads and processes detected.
Given the fact that many malware use anti-emulation,
includ-
anti-debugging and anti-virtualization techniques,
ing on Pin, we built some anti-evasion functionality into
Pin tracer. In particular, we attempt to cover the following
evasion techniques:
• Time check, to verify whether or not the malware code
is monitored.
• EIP (instruction pointer) check, to verify whether or
not the malware code has been instrumented.
• Checksum checks (CRC) on parts of the code, to check
whether or not it has been altered.
• Use of interrupt table manipulating instructions such
as SIDT, SLDT, etc., in order to check whether or not
the code runs in a virtual machine.
In each of these cases, the counter-measure implemented is
to return the expected value, which is not always possible
to determine.
Regardless of the method of collection, execution traces
are important tools in reverse engineering that we use as an
enabler to thwart code protections. We therefore need to
formalize the notion of execution traces as a basis for rea-
soning on self-modiﬁcation behaviors. An execution trace is
a sequence of operations performed by a program, where at
each step, we gather a sequence of information such as pro-
cess IDs, register values and read/write memory addresses
that we collectively refer to as as dynamic instruction. A
dynamic instruction D is a tuple composed of:
• a memory address A[D],
• the machine instruction I[D] run at address A[D],
• the set W[D] of memory addresses written by the in-
struction I[D].
An execution trace is a ﬁnite sequence D1, D2, . . . , Dn of
dynamic instructions. Figure 7 shows the dynamic trace of
the program in Figure 1 after two iterations.
Algorithm 1: Computation of execution and write lev-
els
Update(X,W,D)
X ← max(X, W(A[D]) + 1) ;
foreach m ∈ W[D] do
W(m) ← X ;
end
return (X, W)
W(A[D]) is a shortcut for max(W(A[D]), . . . , W(A[D] + k))
where k is the number of bytes encoding the instruction.
3.3 Execution and write levels
The goal of this section is to delineate waves inside an
execution trace. A wave is determined from both (i) the
execution level, and (ii) the write level of each memory ad-
dress. The write level of each memory address is stored into
a ﬁnite mapping W that we call the write level table.
Given an execution trace D1, D2, . . . , Dn, we deﬁne a se-
quence of pairs composed of the execution level and the write
level table (X0, W0), (X1, W1), . . . , (Xn, Wn) for each dy-
namic instruction that satisﬁes the following properties: (i)
749A[D]
01006e62
01006e67
01006e6f loop:
01006e73
01006e76
01006e7a
I[D]
mov ecx, 0x1dc2
inc ebx
rol byte ptr [ebx+ecx], 0x5
add byte ptr [ebx+ecx], cl
xor byte ptr [ebx+ecx], 0x67
inc byte ptr [ebx+ecx]
W[D]
0x01006e52
0x01006e52
0x01006e52
0x01006e52
Figure 7: Trace execution of the tELock snippet shown in Figure 1
A[D]
01006e7d
01006e80
01006e6f loop:
01006e73
01006e76
. . .
I[D]
dec ecx
jnle loop
rol byte ptr [ebx+ecx], 0x5
add byte ptr [ebx+ecx], cl
xor byte ptr [ebx+ecx], 0x67
. . .
W[D]
0x01006e51
0x01006e51
0x01006e51
Before executing the dynamic instruction Di+1, the the ex-
ecution level is Xi and the write level table is Wi and (ii)
after executing Di+1, the execution level is Xi+1 and the
write level table is Wi+1 . We shall say that the execution
level of the dynamic instruction Di+1 is given by Xi+1. The
sequence of execution levels and the write level tables are ob-
tained by iteratively applying the function Update shown in
Algorithm 1.
We have deﬁned the list of pairs (execution level, write
level table) for explanatory reasons, but in fact, the exe-
cution level is shared by the entire memory.
In fact, the
execution level is shared by any memory address executed
in a wave. Thus, it is suﬃcient to keep track of the current
execution level and the write level table. Consequently, we
begin by setting all write levels to 0 and the execution level
to 1. That is, W(m) = 0 for each memory address m and
X = 1. Then, we apply the function Update on arguments
(X, W) and D in order to determine the next execution level
and the next write level table: (X, W) = Update(X, W, D).
Algorithm 2: Wave recovery for self-modifying codes
input : PE File
output: The number of waves X and for each wave, a
snapshot and a trace in the lists traceList and
waveList
Wave_ recovery()
W(m) ← 0
foreach address m do
end
X ← 0 ;
trace ← ∅ ; list of dynamic instructions
traceList ← ∅ ; list of traces
waveList ← ∅ ; list of memory snapshots
wave ← Snapshot() ;
Add (waveList,wave) ;
Computation of subtraces and memory snapshots
while not at end do
D ← Pin Tracer() ;
Add(trace, D) ;
(X, W) ← Update(X, W, D) ;
ip ← Pin Next Instruction() ;
if W(ip) ≥ X then
New wave
Add (traceList,trace) ;
trace ← ∅ ;
wave ← Snapshot() ;
Add (waveList,wave) ;
end