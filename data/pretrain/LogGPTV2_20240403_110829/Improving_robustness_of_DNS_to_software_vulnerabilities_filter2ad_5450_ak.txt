warning of potentially malicious ones. First, if an ad campaign is determined to be ma-
licious, tracking them over time through small infrastructure changes will enable more
comprehensive blacklists to be built. Second, if a tracked ad campaign is known to be ma-
licious, newly added infrastructure can be more pro-actively blacklisted. Finally, tracking
infrastructure over time allows us to build ground truth to eventually model malicious and
benign advertising campaign infrastructure. In our future work we plan to experiment with
predicting fraudulent publishers.
To unify ad campaigns across multiple infrastructure graphs, we simply join ad cam-
paigns that share IP addresses, canonical names, and name servers that are the same. This
allows us to not only construct graphs within days, but also across time. We will show
that this simple tracking method works well in practice. While on average there are 127K
connected components every day, only 10K of them form new ad campaigns. A DSP can
choose to only go through top-ranked new components if there is limited time available for
threat analysts.
ιad is used to sort advertising campaigns to identify case studies. It is calculated by
adding up all the interesting scores of individual components ιc belonging to that cam-
paign. After we sort the ad campaigns by ιc, we then examine the distribution of the
71
Figure 4.11: Publisher domain examples.
interesting scores and number of components in the campaigns. Figure 4.9c shows the
cumulative distribution of ad campaign scores. Also, Figure 4.9b shows the CDF of the
number of components in an ad campaign. Overall 99.99% ad campaigns have fewer than
1,000 components. The ad campaign with the largest number of components (2.2 million
in Figure 4.9b) has the highest ad campaign score. Domains in this campaign resolved to
several parking, and sinkholing IP addresses, as well as common names servers like Go-
Daddy. This is the reason that this noisy campaign is not representative of maliciousness
or freshness of the domains. Starting from the second ad campaign, the interesting score
indicate suspicious activities in the ad exchanges. We now describe the case studies this
measure uncovers in Section 4.7.
4.7 Case Studies
Among the campaigns with highest (top 0.1%) interesting scores, we found new cases in-
cluding Potentially Unwanted Programs (PUP), algorithm generated domains and malware
sites.
4.7.1 Case 1: PUP
Among advertising campaigns with the highest interesting scores, one category of publisher
domains are generated by Potentially Unwanted Programs (PUP). For example, domains in
Figure 4.11 (1) to (5).
A VirusTotal report [96] suggests a machine communicating with domain names in Fig-
72
(2)search.easylifeapp.comsearchiy.gboxapp.comsearchy.easylifeapp.com(3)ads.adsrvmedia.netads.2xbpub.coms.ad120m.coms.ad121m.coms.ad122m.com(5)www.goodsoften.comwww.bestnsoftware.comwww.v81qt8mhxb.comwww.softlsoftware.comwww.b7vr3u0g.comwww.opnrbm1.comwww.ia2x3on4.comwww.thesoftdowd.comwww.xgaz765xy.comwww.a1ig9xka.com(1)a24x7-search.inbeta1-search24.meboba-search.inbobba-ﬁnder.orgglobal-bsearch.comkabo-search.commulti-search.biznemo-ﬁnder.mesearch-world.biztosearch.biz(4)0spzz.super-promo.vasegiraffe.xyz0vmzz.updateinstall.vasegiraffe.xyz0zizz.updateinstall.toesbait.xyz288zz.updateinstall.vasegiraffe.xyz2dbzz.updateinstall.vasegiraffe.xyz2fjzz.updateinstall.vasegiraffe.xyz2jezz.updateinstall.vasegiraffe.xyz2qmzz.updateinstall.toesbait.xyz2qnzz.updateinstall.toesbait.xyz2umzz.updateinstall.toesbait.xyzure 4.11 (1) (ιad ranked the 3rd highest) is likely infected with a trojan known as LEMIR
or Win32.BKClient by the AV industry. The malware has many capabilities including
changing default search engines to generate revenue, disabling Windows AV, Firewall and
Security Center notiﬁcations, and can drop additional malicious binaries [97]. Similarly, ad
campaigns with 2nd and 4th highest ιad (Figure 4.11 (2) (3)) are generated by ad injections
of certain browser extension. Different malware families communicate with domains in
Figure 4.11 (3) including Win.Trojan.Symmi [98]. These publisher domains may not be
malicious, but they are strongly associated with monetization behavior of malware. These
are interesting cases as traditional malware are involved in an area where we would expect
to see only adware or “potentially unwanted programs.” This shows that malware uses ad-
vertising fraud to monetize infections and malware can also be identiﬁed from the vantage
point of a DSP.
In addition, several Pop-up Ads campaigns exhibit high level of agility similar to tra-
ditional malware. The ad campaign ranked 1, 184th (Figure 4.11 (4)) uses domain ﬂuxing,
likely to avoid browser extension detection systems. In total, we observed more than 26,000
unique domain names from this campaign in three months of DSP trafﬁc. Moreover, the
ad campaign in Figure 4.11 (5) not only uses domain ﬂuxing, it also uses the Amazon EC2
cloud to further decrease the chance of detection. Each of these domains resolved into an
EC2 cloud domain representing a unique Virtual Machine (VM), when active. The VM
domains also change according to the domains that point to them. This shows that miscre-
ants are constantly employing fresh VMs to perform ad fraud. Since traditional detection
systems often use reputation of IP addresses of domains and URLs, using cloud machines
makes this campaign harder to be detected.
4.7.2 Case 2: Algorithm Generated Domains
Figure 4.12 (1) (2) shows two ad campaigns of algorithm generated domains we found in
the DSP trafﬁc (ranked 142th and 183th), containing at least 195 domains. None of the
73
Figure 4.12: Malware site example.
domains were blacklisted, but a high percentage of brand new domains results in a high
score. A new group of domains appear everyday, pointing to the same IP address. These
publisher domains are suspicious. Although no open threat analysis evidence is available
to date, it is reasonable to assume that anything that changes so often must be trying to
evade a detection process. With infrastructure tracking, ad exchanges or DSP can keep a
close eye on such campaigns to proactively deal with potential ad abuse.
4.7.3 Case 3: Malware Site
Figure 4.12 (3) shows a group of malware site domains (ranked 1, 484th campaign) seen
from DSP trafﬁc, none of which appeared on blacklists. A Virustotal report [99] shows that
the IP address these domains resolved to, had other similar domains pointing to it during the
week ending on 3/24/2015. Related URLs were detected as malware sites by several URL
scanners from the AV industry. This group uses domain ﬂuxing with both the second level
domain zone, and the child labels. We saw other groups of domains tracked separately,
with similar domain name patterns, and short lifetime. However, they were not grouped
into one big campaign, because different groups were using different IP addresses. In other
words, this campaign uses both domain ﬂuxing and IP address ﬂuxing. Since we only used
exact the same IP address match to form a campaign, we will need other information to
further analyze campaigns like this.
74
2387uj23n-khb747bjg324yuklsk.isdoorloaper.in3498u4i5k23m-khb747bjg324yu.ace-nate-rade.ind83u4jk-khb747bjg324yuksk.ace-nate-rade.indspo34nmv-khb747bjg324yu.isdoorloaper.inmfokieutt-khb747bjg324yu.endzoneroot.inpo238u4j-khb747bjg324yuksd.ace-nate-rade.insdk4-khb747bjg324yu-39kdn.endzoneroot.insdop3j-khb747bjg324yu483j.isdoorloaper.inslo3pmnsop-khb747bjg324yu830k.endzoneroot.in(1)s8.plisvg.coms8.pmgbpz.coms8.pmhjni.coms8.pnljax.coms8.ptcptw.coms8.pykfql.coms8.qariyx.coms8.qaxkhw.coms8.qaxzbw.coms8.pdanyb.com(2)s7.qaxzbw.coms7.qbakxx.coms7.qbhawx.coms7.qbkqec.coms7.qbmhju.coms7.qbtdig.coms7.qbxmpp.coms7.qcbcsw.coms7.qcjslp.coms7.qckvvk.com(3)4.8 Related Work
Previous research has studied behavior of click bots [80, 100, 68]. The bots mimic human
behavior by generating fake search queries and adding jitters to click delay. More advanced
bots hijacked users’ original clicks and replaced the ads [68, 62, 7, 101]. The ZeroAccess
botnet cost advertisers $100, 000 per day [7] and the TDSS/TDL4 botnet cost advertisers
at least $346 million in total. Ad fraud detection work mainly focused on click fraud [102,
103, 104].
Impression fraud is harder to detect than click fraud. Springborn et al. [65] studied
pay-per-view networks that generated fraudulent impressions from invisible iFrames and
caused advertisers millions of dollars lost. Advertisers can purchase bluff ads to measure
ad abuse [68] and compare charged impressions with valid impressions. The adware and
ad injection problem has been systematically studied by static and dynamic analysis of
web browser extensions [105, 106, 107]. From within the ad ecosystem, Stone-Gross et
al. [54] used ad hoc methods to study speciﬁc attacks faced by ad exchanges, including
referrer spooﬁng and cookie replay attacks. Google also documented what they consider to
be invalid trafﬁc in [47] but did not disclose the details of their trafﬁc ﬁlters.
4.9 Summary
In this Chapter, we measured ad abuse from the perspective of a Demand Side Platform
(DSP). We found that traditional sources of low reputation, such as public blacklists and
malware traces, greatly underestimate ad-abuse, which highlight the need to build lists
catered towards ad-abuse. The good news, however, is malicious publishers that participate
in ad-abuse can likely be modeled at the DSP level based on their behavioral characteristics.
Finally, malicious campaigns can be tracked using graph analysis and simple heuristics,
allowing DSPs to track suspicious infrastructure.
75
PRACTICAL ATTACKS AGAINST GRAPH-BASED CLUSTERING
CHAPTER 5
5.1 Motivation
Several studies have shown how security systems that employ machine learning techniques
can be attacked [8, 9, 11, 12, 13], decreasing their overall detection accuracy. This re-
duction in accuracy makes it possible for adversaries to evade detection, rendering defense
systems obsolete.
While graph based network detection systems are not immune to adversarial attack, the
community knows little about practical attacks that can be mounted against them. As these
network detectors face a range of adversaries (e.g., from script kiddies to nation states), it
is important to understand the adversary’s capabilities, resources, and knowledge, as well
as the cost they incur when evading the systems.
In this Chapter we present the ﬁrst practical attempt to attack graph based modeling
techniques in the context of network security. Our goal is to devise generic attacks on
graphs and demonstrate their effectiveness against a real-world system, called Pleiades [25].
Pleiades is a network detection system that groups and models unsuccessful DNS resolu-
tions from malware that employ domain name generation algorithms (DGAs) for their com-
mand and control (C&C) communications. The system is split into two phases. First, an
unsupervised process detects new DGA families by clustering a graph of hosts and the do-
mains they query. Second, each newly detected cluster is classiﬁed based on the properties
of the generated domains.
To evade graph clustering approaches like Pleiades, we devise two novel attacks—
targeted noise injection and small community—against three commonly used graph clus-
tering or embedding techniques: i) community discovery, ii) singular value decomposition
76
(SVD), and iii) node2vec. Using three different real world datasets (a US telecommunica-
tion dataset, a US university dataset and a threat feed) and after considering three classes of
adversaries (adversaries with minimal, moderate and perfect knowledge) we mount these
two new attacks against the graph modeling component of Pleiades. We show that even
an adversary with minimal knowledge, i.e., knowing only what is available in open source
intelligence feeds and on their infected hosts, can evade detection.
Beyond devising practical attacks, we demonstrate that the attacks are inexpensive for
adversaries. Fortunately, defenders are not without recourse, and detection systems’ pa-
rameters can be tuned to be more resistant to evasion. Based on these discoveries, we make
recommendations to improve Pleaides’ resilience.
This Chapter makes the following contributions:
Two Novel Attacks The targeted noise injection attack improves on prior work that ran-
domly injects noise; by targeting the injected vertices and edges to copy the graph structure
of the original signal, we force noise into the resulting clusters. Our small community at-
tack abuses the known property of small communities in graphs to subdivide and separate
clusters into one or more unrelated clusters.
Practical Attacks and Defenses While more knowledgeable attackers typically fare bet-
ter, we demonstrate that even minimal knowledge attackers can be effective: attackers with
no knowledge beyond their infections can render 84% of clusters too noisy to be useful,
and evade clustering at a rate of 75%. The above attacks can be performed at low cost to
the adversary by not appearing to be anomalous, nor losing much connectivity. Simple de-
fenses raise the attacker’s costs and force only 0.2% of clusters to be too noisy, and drop the
success rate to 25%. State of the art embeddings, such as node2vec, offer more adversarial
resistance than SVD, which is used in Pleiades.
77
5.2 Related Work
Existing work in adversarial machine learning has focused on analyzing the resilience of
classiﬁers. Huang et al. [108] categorize attack inﬂuence as either causative or exploratory,
with the former polluting the training dataset and the latter evading the deployed system
by crafting adversarial samples. Following the terminology of Huang et al., our work
focuses on exploratory attacks that target the graph clustering component of Pleiades. We
assume that the clustering hyperparameters are selected with attack-free labels, and the
subsequent classiﬁer is not polluted when they are trained. Contrary to other exploratory
attacks in literature, we face the challenge that the clustering features cannot be modiﬁed
or computed directly, and that attackers often have an incomplete view of the defender’s
data.
In order to compute optimal graph partitions or vertex embeddings, one needs to have
a global view of all objects on the graph. On the contrary, related work can compute
classiﬁcation features directly from crafting adversarial samples. For example, features are
directly obtained from spam emails [8, 9], PDF ﬁles [10, 11, 12], phishing pages [11],
images [14, 15, 13, 16], network attack packets [17], and exploits [18, 109]. These security
applications classify an object based on features extracted from only that object and its
behavior. This makes the features of system classiﬁers more local, and enables evasion
techniques such as gradient descent directly in the feature space. We make the following
deﬁnition: a local feature can be computed from only one object; whereas a global feature
needs information from all objects being clustered or classiﬁed.
Since Pleiades uses global features, an adversary’s knowledge can affect the success of
attacks. For example, if the adversary has full access to the defender’s datasets, she can
reliably compute clustering features and is more equipped to evade than a less knowledge-
able attacker. Many researchers [110, 111] have shown that, even without access to the
training dataset, having knowledge about the features and an oracle to obtain some labels
78
of objects is sufﬁcient for an attacker to approximate the original classiﬁer.
Biggio et al. [19, 20] are the ﬁrst to study adversarial clustering. They propose a bridge
attack, which works by injecting a small number of adversarial samples to merge clusters.
The attackers have perfect knowledge in their assumption. We distinguish our work by i)
considering attackers with different knowledge levels, ii) evaluating how adversarial graph-
clustering in network security affects the whole system, and iii) quantifying the cost of
attacks. With respect to attack cost analysis, Lowd et al. [112] propose a linear cost function
as a weighted sum of feature value differences for crafting evasive adversarial samples.
Since we do not work directly in the feature space, we propose different costs for the
attacks we present in Section 5.3.
To summarize, our work is novel because we focus on adversarial clustering, which
deals with global features that cannot be directly changed. We also evaluate capabilities of
attackers with various knowledge levels, and quantify the costs associated with attacks.
5.3 Threat Model & Attacks
In this section, we describe our threat model and explain our attacks as modiﬁcations to a
graph G. In practice, the attacker changes the graph based on the underlying data that are
being clustered. For example, if the vertices in a graph are infected hosts and the domains
they query as in Pleiades, the graph representation can be altered by customized malware
that changes its regular querying behavior.
5.3.1 Notation
An undirected graph G is deﬁned by its sets of vertices (or nodes) V and edges E, where
G = (V, E) and E = {(vi, vj) : if there exists an edge between vi and vj, vi ∈ V, vj ∈ V }.
An undirected bipartite graph is a special case where V can be divided into two disjoint sets
(U and V ) such that every edge connects at a vertex in U and one in V , represented as G =
(U, V, E). While the attacks apply in the general case, oftentimes bipartite graphs appear in
79
security contexts: hosts (U) query domains (V ), clients connect to servers, malware make
system calls, etc. Finally, a complete undirected bipartite graph is where every vertex in U
has an edge to every vertex in V .
G is an undirected graph that represents the underlying data a defender clusters. The
graph clustering subdivides G into clusters C0, . . . , Ck, where V = C0 ∪ . . . ∪ Ck. If the
graph clustering method is based on graph partitions, then each cluster Ci is a subgraph Gi,
and G = G0 ∪ . . . ∪ Gk. Often when applied, a defender seeks to cluster vertices either
in U or V of the bipartite graph, for example, cluster end hosts based on the domains they
resolve, or malware based on the system calls they make. An attacker controls an attacker
graph, G ⊂ G. The adversary uses the targeted noise injection and the small community
attacks described below to change G to G(cid:48), by adding or removing nodes and edges from
G.
These attacks violate the underlying basic assumptions of graph clustering techniques,
which either renders the clustered subgraph G(cid:48) to be useless to the defender or prevents G(cid:48)
from being extracted from G intact (See Section 5.3.3).
5.3.2 Threat Model
Before describing attacker knowledge levels, we discuss knowledge that is available to all
attackers. We assume all attackers have at least one active infection, or G ⊂ G. The
attacker is capable of using any information that could be gathered from G to aid in their
attacks. We also assume that an attacker can evaluate clusters like a defender can, e.g.,
manual veriﬁcation. When done with a classiﬁer, an attacker has black-box access to it or
can construct a surrogate that approximates the accuracy and behavior of the real classiﬁer
based on public data. This may seem extreme, but the plethora of open source intelligence
(OSINT) [113, 114, 115] data and MLaaS machine learning tools [116, 117, 118, 119,
120] make this realistic. Finally, an attacker has full knowledge of the features, machine
learning algorithms, and hyperparameters used in both the unsupervised and supervised
80
phases of the system under attack, as these are often published [25, 121, 29, 30, 28, 122,
123]. Since clustering requires some graph beyond G, we must consider attackers with
various representations of the defender’s G. We evaluate three levels: minimal, moderate,
and perfect knowledge. The minimal level attacker only knows what is in their attack graph
G, but the perfect attacker possesses G. For example, a perfect adversary would have access
to the telecommunication network data used in Pleiades, which is only obtainable by the