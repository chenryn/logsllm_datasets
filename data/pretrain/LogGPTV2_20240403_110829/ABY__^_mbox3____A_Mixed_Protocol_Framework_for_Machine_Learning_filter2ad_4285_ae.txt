proach fails in the malicious setting primarily due to party 1 being
free to choose the value a it inputs to the OT, arbitrarily. We avoid
this issue by first performing bit injection on b. That is, we compute
Computing a(cid:74)b(cid:75)B =(cid:74)ab(cid:75)A. Unfortunately, our semi-honest ap-
(cid:74)b(cid:75)B →(cid:74)b(cid:75)A and then a(cid:74)b(cid:75)A =(cid:74)ab(cid:75)A. As performed in Section 5.3,
the parties can locally compute shares(cid:74)b1(cid:75)A,(cid:74)b2(cid:75)A,(cid:74)b3(cid:75)A where
(cid:74)b(cid:75)B = (b1, b2, b3). We can now emulate the XOR of these values
within an arithmetic circuit by computing(cid:74)b1 ⊕ b2(cid:75)A =(cid:74)d(cid:75)A :=
(cid:74)b1(cid:75)A +(cid:74)b1(cid:75)A − 2(cid:74)b1(cid:75)A(cid:74)b2(cid:75)A followed by(cid:74)b(cid:75)A :=(cid:74)d ⊕ b3(cid:75)A. This
result can then be computed as(cid:74)ab(cid:75)A := a(cid:74)b(cid:75)A where each party
Computing(cid:74)a(cid:75)A(cid:74)b(cid:75)B =(cid:74)ab(cid:75). Once again, the bit injection pro-
cedure can be repeated here to convert(cid:74)b(cid:75)B to(cid:74)b(cid:75)A followed by
computing(cid:74)a(cid:75)A(cid:74)b(cid:75)A using the multiplication protocol.
locally multiplies a by its share of b. Compared to performing the
generic bit decomposition from Section 5.3, this approach reduces
the round complexity and communication by O(log k).
conversion requires sending 2k bits over two rounds. The final
Session 1B: PrivacyCCS’18, October 15-19, 2018, Toronto, ON, Canada43i bi fi(x).
computed as f (x) =
b1, ..., bm ∈ {0, 1} such that bi = 1 ⇔ ci−1 < x ≤ ci. f can then be
Let us begin with the simple case of computing(cid:74)x(cid:75) < c. This
expression can then be rewritten as(cid:74)x(cid:75)A − c < 0. Recall that x is
significant bit (MSB) of(cid:74)x−c(cid:75) denotes its sign, i.e. 1 iff x−c < 0. This
extraction of(cid:74)x−c(cid:75) to obtain binary shares of(cid:74)b(cid:75)B :=(cid:74)msb(x−c)(cid:75)B.
implies that the inequality can be computed simply by extracting
the MSB. This in turn can be computed by taking the Section 5.3 bit
represented as a two’s complement value and therefore the most
When the bit-extraction is performed with binary secret sharing,
the round complexity will be O(log k) while the communication is
O(k) bits. On the other hand, when the conversion is performed
using a garbled circuit, the round complexity decreases to 1 with
an increase in communication totaling O(κk) bits. Each bi is the
logical AND of two such shared bits which can be computed within
the garbled circuit or by an additional round of interaction when
binary secret sharing is used.
Each of the fi functions are expressed as a polynomial fi((cid:74)x(cid:75)) =
ai, j(cid:74)x(cid:75)j + ... + ai,1(cid:74)x(cid:75) + ai,0 where all ai,l are publicly known con-
stants. When fi is a degree 0 polynomial the computation bi fi((cid:74)x(cid:75))
can be optimized as ai,0(cid:74)bi(cid:75)B using the techniques from Section 5.4.
of ai,l(cid:74)x(cid:75)l can be performed locally, given(cid:74)x(cid:75)l . However, when ai, j
In addition, when the coefficients of fi are integer, the computation
has a non-zero decimal, an interactive truncation will be performed
as discussed in Section 5.1.
6 EXPERIMENTS
We demonstrate the practicality of our proposed framework with
an implementation of training linear regression, logistic regression
and neural network models and report on their efficiency. An ana-
lytical comparison to other three party protocols is also given in
Appendix B. We defer a detailed explanation of the implemented
machine learning algorithms to Appendix A. The implementation
was written in C++ and builds on the primitives provided by the
libOTe library [47], and the linear algebra library Eigen [2]. All
arithmetic shares are performed modulo 264. Due to the significant
development time required to implement the maliciously secure
protocols ([28] has no publicly available code), we have only im-
plemented and report performance numbers for the semi-honest
variant of our framework. This does not hinder comparison with
prior work since they primarily focus on semi-honest protocols (in
fact our work is the first maliciously secure protocol for machine
learning).
Experimental Setup. We perform all benchmarks on a single
server equipped with 2 18-core 2.7Ghz Intel Xeon CPUs and 256GB
of RAM. Despite having this many cores, each party performs their
computation on a single thread. Using the Linux tc command we
consider two network settings: a LAN setting with a shared 10Gbps
connection and sub-millisecond RTT latency and a WAN setting
with a 40Mbps maximum throughput and 40ms RTT latency. The
server also employs hardware accelerated AES-NI to perform fast
random number generation. We note that other protocols are run
on different hardware (fewer cores). However, this should have a
minimal impact on performance since our implementation is almost
entirely IO bound and each party only utilizes a single core.
Datasets. Our work is primarily focused on the performance of
privacy-preserving machine learning solutions. As a result, in some
benchmarks we choose to use synthetic datasets which easily allow
for a variable number of training examples and features and better
demonstrate the performance of our training. We emphasize that
we do NOT use synthetic data to measure accuracy of the trained
models. In fact, our training algorithms (i.e. if our protocols were
run honestly) are functionality equivalent to those of [43], and we
refer the reader to their paper for precise accuracy measurements on
real datasets. We also consider the widely used MNIST dataset [5]
which contains a set of 784 = 28 × 28 pixel images of handwritten
numbers where the task is to output the correct number.
1
B XT
6.1 Linear Regression
We begin with the gradient descent protocol for learning linear
regression models as detailed in Section A.1. The computation of
this protocol is easy given our framework. At each iteration, a public
and randomly selected subset Xj of the dataset is sampled and the
model is updated as w := w − α
j × (Xj × w − Yj). We report
performance in terms of iterations per second as opposed to end-
to-end running time. This is primarily done to present the results
in a way that can be easily generalized to other tasks. Figure 2
presents the throughput of our linear regression training protocol
compared to [43] and is further parameterized by the number of
features D ∈ {10, 100, 1000} and the size of the mini-batch B ∈
{128, 256, 512, 1024}.
The columns labeled “Online" denote the throughput of the in-
put dependent computation while the columns labeled “Online +
Offline" denote the total throughput including the pre-processing
phase that is input independent. Our throughput is strictly better
than that of [43]. In the LAN setting our online throughput is be-
tween 1.5 to 4.5 times greater than [43] which is primarily due to a
more efficient multiplication protocol. For example, [43] requires
preprocessed matrix beaver triples along with a more complex
opening procedure. While our protocol’s online throughput is con-
siderably higher than [43], our main contribution is an offline phase
that is orders of magnitude more efficient. Overall, the throughput
of our protocol becomes 200 to 1000 times greater than [43] due
to the elimination of expensive beaver triples. The only operation
performed in our offline phase is the generation of truncated shares
(cid:74)r(cid:75),(cid:74)r/2d(cid:75) which requires computing the addition circuit which
can be made extremely efficient.
In the WAN setting, our protocol is also faster than [43] by
roughly a factor of 2 in the online phase and 10 to 1000 times faster
when the overall throughput is considered. As before, the offline
phase has a minimal impact on the overall throughput, consuming
roughly 10 percent of the computation. This is in drastic contrast
with [43] where the majority of the computation is performed in
the offline phase.
Our protocol also achieves a smaller communication overhead
compared to [43]. The communication complexity for the online
phase of both protocols is effectively identical. Each party performs
two matrix multiplications where shares of size B and D are sent.
However, in the offline phase, [43] presents two protocols where
the first requires O(BD) exponentiations and D + B elements to be
Session 1B: PrivacyCCS’18, October 15-19, 2018, Toronto, ON, Canada44communicated per iteration. Our protocol requires no exponentia-
tions and achieves the same asymptotic communication overhead
but with better constants. Due to a large number of exponentiations
required by their protocol, [43] also propose a second technique
based on oblivious transfer which improves on their computational
efficiency at the expense of an increased communication of O(BDκ)
elements per iteration. In the LAN setting, the computationally
efficient oblivious transfer protocol achieves higher throughput
than their exponentiation based approach. However, in the WAN
setting, the communication overhead is their bottleneck and the
exponentiation-based protocol becomes faster. In Figure 2, we al-
ways report and compare against the variant with the best through-
put. Regardless of which technique of [43] is used, the offline of
our protocol is computationally more efficient and requires less
communication.
Due to the offline phase of [43] having such a low throughput, the
authors proposed an alternative client-aided protocol where a semi-
honest client locally performs the offline phase and distributes the
resulting shares between the two servers. If we relabel an assisting
client as the third server, this variant of their protocol has a similar
security model as ours with the notable exception that there is no
natural way to extend it to the malicious setting. The advantage
of adding a third party is that the throughput of the offline phase
can be significantly improved. However, it is still several orders
of magnitude slower than our preprocessing for a few reasons.
First, their protocol requires that random matrices of the form
R1 × R2 = R3 be generated by the third party, where R1 is a D × B
dimension matrix. These have to be constructed and sent to the two
other parties resulting in high communication of O(DB) elements.
On the other hand, our preprocessing simply requires the sending
of O(B + D) elements. Considering that D and B can be in the order
of 100s this results in a significant reduction in computation and
communication. Moreover, our overall protocol is already faster
than the online phase of [43] and therefore is faster regardless of
which preprocessing technique is used.
1
B XT
6.2 Logistic Regression
Our next point of comparison is with regards to the training of
logistic regression models. At each iteration the update function is
w := w − α
j × (f (Xj × w) − Yj). This update function is more
complex compared to linear regression due to the need to compute
the logistic function f at each iteration. Our protocol approximates
f using a piecewise linear function which requires switching to and
from a binary secret sharing scheme. While relatively efficient com-
putationally, it does have the negative consequence of increasing
the round complexity of the protocol by 7 per iteration. In the LAN
setting where latency is small, this has little impact. For example,
given a batch size of B = 128 and dimension D = 10, our proto-
col can perform 2251 iterations per second using a single thread.
Moreover, increasing the dimension to D = 100 only decreases the
throughput to 1867 iterations per second. When compared to [43],
this represents an order of magnitude improvement in running time.
This difference is primarily attributed to [43] using garbled circuits
which requires fewer rounds at the cost of increased bandwidth and
more expensive operations. For both linear and logistic regression,
the offline phase is identical. As such, our extremely efficient offline
phase results in a 200 to 800 times throughput speedup over [43].
In the WAN setting, our increased round complexity begins to
degrade our performance to the point that [43] is almost as fast as
our protocol during the online phase. For B = 128 and D = 100 our
protocol performs 4.1 iterations per second while [43] achieves 3.1
iterations per second. However, as the batch size increases (resulting
in a better rate of convergence), our protocol scales significantly
better then [43]. Consider a batch size of B = 1024 where our
protocol achieves 3.99 iterations per second while [43] achieves
0.99 iterations per seconds. When including the offline phase, our
protocol receives almost no slowdown (5%) while [43] is between
2 and 100 times slower, resulting in a 3 to 300 times difference in
overall throughput when the protocols are compared.
Our protocol also achieves a smaller communication overhead
when approximating the logistic function. Primarily this is due to
our protocol using a binary secret sharing and our new binary-
arithmetic multiplication protocol from Section 5.4. In total, our
protocol requires each party to send roughly 8Bk bits while [43],
which uses garbled circuits, requires 1028Bk bits. The main disad-
vantage of our approach is that it requires 7 rounds of interaction
compared to 4 rounds by [43]. However, at the cost of less than
double the rounds, our protocol achieves a 128 times reduction in
communication which facilitates a much higher throughput in the
LAN or WAN setting when there is a large amount of parallelism.
6.3 Neural Networks
Our framework particularly stands out when working with neural
networks. The first network we consider (NN) is for the MNIST
dataset and contains three fully connected layers consisting of 128,
128, and 10 nodes respectively. Between each layer, the ReLU activa-
tion function is applied using our piecewise polynomial technique.
When training the NN network, our implementation is capable of
processing 10 training iterations per seconds, with each iteration
using a batch size of 32 examples. Proportionally, when using a
batch size of 128, our protocol performs 2.5 iterations per second.
An accuracy of 94% can be achieved in 45 minutes (15 epochs).
Compared to [43], with the same accuracy, our online running time
is 80× faster while the overall running time is 55, 000× faster.
We also consider a convolutional neural net (CNN) with 2 hidden
layers as discussed [46]. This network applies a convolutional layer
which maps the 784 input pixels to a vector of 980 features. Two
fully connected layers with 100 and 10 nodes are performed with the
ReLU activation function. For a detailed depiction, see [46, Figure
3]. For ease of implementation, we overestimate the running time
by replacing the convolutional kernel with a fully connected layer.
Our protocol can process 6 training iterations per second with a
batch size of 32, or 2 iterations per second with a batch size of 128.
We estimate, if the convolutional layer was fully implemented, that
our training algorithm would achieve an equivalent accuracy as a
plaintext model [46] of 99% in less than one hour of training time.
6.4 Inference
We also benchmark our framework performing machine learning
inference using linear regression, logistic regression, and neural
network models, as shown in Figure 3. For this task, a model that has
Session 1B: PrivacyCCS’18, October 15-19, 2018, Toronto, ON, Canada45Setting
Dimension
Protocol
LAN
WAN
10
100
1000
10
100
1000
This
[43]
This
[43]
This
[43]
This
[43]
This
[43]
This
[43]
128
11764
7889
5171
2612
406
131
24.6
12.4
24.5
12.3
22.2
11.0
Online Throughput
256
10060
7206
2738
755
208
96
24.5
12.4
24.1
12.2
20.2
9.8
512