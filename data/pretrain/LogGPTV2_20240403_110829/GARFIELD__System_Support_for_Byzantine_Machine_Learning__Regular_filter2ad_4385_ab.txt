x(k+1)
x(k)
, ξ
,
(2)
Estimating the gradient at x is computationally expensive,
given the big datasets and the complex high–dimensional mod-
els we have nowadays. Fortunately, this gradient estimation is
easily parallelizable. Basically, n machines can each partially
estimate the gradient using a mini–batch of size b/n, which can
be then aggregated together to restore the complete estimation.
One widely used architecture enabling this distribution
is the parameter server scheme [34], where a centralized
server holds the parameters x, and the other machines (called
workers) own data batches. For each training step, the server
ﬁrst broadcasts the parameters to workers, which then share
the heavy gradient estimation. When a worker completes its
estimation, it sends it to the parameter server, which ﬁnally
aggregates the received estimations (typically by averaging)
and updates the parameters x, as in Equation (2).
Another variant
to distribute gradient estimation is the
decentralized learning [47] in which all machines collaborate
together to train a model without a central entity. In this
scheme, each machine owns a copy of the model and some
local data that is never shared with the others. In each training
step, each machine estimates a gradient (using SGD) and
shares it, in a peer–to–peer fashion, with the others. Gradient
estimations are then aggregated locally on each machine and
are used to update the parameters x.
C. Byzantine Resilience
In the parlance of classical distributed computing, a system
tolerates a Byzantine fault when it copes with a machine that
can deviate arbitrarily from the algorithm assigned to it [32].
Such a behavior abstracts any kind of failures,
including
software bugs, hardware defects, corrupted data [25], commu-
nication omissions, or even adversarial attacks. We consider
the ML context where any of the machines contributing to
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
40
the learning process can behave arbitrarily. In such context,
tolerating Byzantine behavior basically ensures following the
same learning path that would have been achieved in the
absence of Byzantine machines.
III. THE GARFIELD DESIGN
GARFIELD is a library to build Byzantine–resilient SGD–
based ML applications that rely on (1) robust aggregation and
(2) communication of plain vectors (without compression nor
quantization). GARFIELD builds on the insight that distributed
ML algorithms do not require strong consistency; only the
convergence to a good accuracy is required [11], [22]. This
insight has two implications: (1) the servers do not need to
run SMR, and (2) the servers do not need to collect replies
from all workers; only a subset of replies is sufﬁcient. This
latter observation is a cornerstone in tolerating Byzantine
behavior in asynchronous networks [22], [19]. This weak con-
sistency model has been shown sufﬁcient for correctness and
convergence in theory, even in asynchronous environments.
Essentially, GARFIELD masks the faults at the application layer
(the highest layer), abstracting all the faults that can happen
in lower layers like communication omissions [17].
We ﬁrst introduce a few statistically–robust gradient aggre-
gation rules (GARs) that are included in GARFIELD and then,
we discuss the modular design of GARFIELD.
(cid:2)
(cid:3)q → R
A. Statistically Robust GARs
A GAR is merely a function of
d, with d, the
d
R
d (i.e., a gradient or a
dimension of the input vector space R
model), and q, the number of input vectors to be aggregated.
Basically, these GARs wait for q vectors before applying some
functions on them. Hence, in synchronous, non–faulty settings,
these GARs can be deployed with q machines in the system
(so that the aggregator node can gather replies from all nodes
in the system within some time bound). Yet, in asynchronous
settings, one would require to deploy q + f nodes to use these
GARs, to ensure liveness of the protocol, where f denotes
the maximum number of Byzantine inputs. In short, all GARs
output a vector with special statistical properties that make
them safe to use in the Byzantine setting.
1. Median [51] outputs a vector of coordinate-wise medians
among the input vectors. Median requires q ≥ 2f + 1, and its
asymptotic complexity is O(qd).
2. Krum [11] assigns a score to each vector (based on a
sum of distances with the closest neighbors), and then returns
the smallest scoring vector. Multi–Krum (a variant of Krum)
instead averages the m smallest scoring vectors, achieving a
better convergence rate than Median [17]. It requires however
q ≥ 2f + 3, and its asymptotic complexity is O(q2d).
3. MDA [45] ﬁnds a subset group of vectors of size q − f
with the minimum diameter among all other subsets, where
the diameter of a group is deﬁned as the maximum distance
between any two vectors of this subset. MDA then outputs
the average of the chosen subset. Notably, MDA carries an
exponential asymptotic complexity of O
. Yet,
as we will discuss later, its assumptions about variance are
(cid:4)(cid:2)
+ q2d
(cid:5)
(cid:3)
q
f
(cid:54)(cid:72)(cid:85)(cid:89)(cid:72)(cid:85)
(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:11)(cid:12)
(cid:3)(cid:70)(cid:88)(cid:85)(cid:66)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:11)(cid:12)
(cid:3)(cid:70)(cid:88)(cid:85)(cid:66)(cid:88)(cid:83)(cid:71)(cid:68)(cid:87)(cid:76)(cid:81)(cid:74)(cid:66)(cid:74)(cid:85)(cid:68)(cid:71)(cid:11)(cid:12)
(cid:3)(cid:88)(cid:83)(cid:71)(cid:68)(cid:87)(cid:72)(cid:66)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:11)(cid:12)
(cid:3)(cid:90)(cid:85)(cid:76)(cid:87)(cid:72)(cid:66)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:11)(cid:12)
(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:66)(cid:68)(cid:70)(cid:70)(cid:88)(cid:85)(cid:68)(cid:70)(cid:92)(cid:11)(cid:12)
(cid:48)(cid:68)(cid:76)(cid:81)(cid:3)(cid:50)(cid:69)(cid:77)(cid:72)(cid:70)(cid:87)(cid:86)
(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)
(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:11)(cid:12)
(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:66)(cid:74)(cid:85)(cid:68)(cid:71)(cid:76)(cid:72)(cid:81)(cid:87)(cid:86)(cid:11)(cid:12)
(cid:37)(cid:92)(cid:93)(cid:68)(cid:81)(cid:87)(cid:76)(cid:81)(cid:72)(cid:3)(cid:58)(cid:82)(cid:85)(cid:78)(cid:72)(cid:85)
(cid:3)(cid:70)(cid:82)(cid:80)(cid:83)(cid:88)(cid:87)(cid:72)(cid:66)(cid:74)(cid:85)(cid:68)(cid:71)(cid:76)(cid:72)(cid:81)(cid:87)(cid:86)(cid:11)(cid:12)
(cid:75)(cid:68)(cid:86)
(cid:37)(cid:92)(cid:93)(cid:68)(cid:81)(cid:87)(cid:76)(cid:81)(cid:72)(cid:3)(cid:54)(cid:72)(cid:85)(cid:89)(cid:72)(cid:85)
(cid:3)(cid:70)(cid:88)(cid:85)(cid:66)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:11)(cid:12)
(cid:49)(cid:72)(cid:87)(cid:90)(cid:82)(cid:85)(cid:78)(cid:76)(cid:81)(cid:74)
(cid:3)(cid:74)(cid:72)(cid:87)(cid:66)(cid:74)(cid:85)(cid:68)(cid:71)(cid:76)(cid:72)(cid:81)(cid:87)(cid:86)(cid:11)(cid:12)
(cid:3)(cid:74)(cid:72)(cid:87)(cid:66)(cid:80)(cid:82)(cid:71)(cid:72)(cid:79)(cid:86)(cid:11)(cid:12)
(cid:36)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:76)(cid:82)(cid:81)
(cid:42)(cid:36)(cid:53)(cid:86)
(cid:3)(cid:74)(cid:72)(cid:87)(cid:66)(cid:74)(cid:68)(cid:85)(cid:11)(cid:12)
(cid:48)(cid:72)(cid:71)(cid:76)(cid:68)(cid:81)
(cid:48)(cid:39)(cid:36)
(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:11)(cid:12)
(cid:3)(cid:68)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:72)(cid:11)(cid:12)
(cid:48)(cid:88)(cid:79)(cid:87)(cid:76)(cid:16)(cid:78)(cid:85)(cid:88)(cid:80)
(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:11)(cid:12)
(cid:3)(cid:68)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:72)(cid:11)(cid:12)
(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:11)(cid:12)
(cid:3)(cid:68)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:72)(cid:11)(cid:12)
(cid:37)(cid:88)(cid:79)(cid:92)(cid:68)(cid:81)
(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:11)(cid:12)
(cid:3)(cid:68)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:72)(cid:11)(cid:12)
(cid:36)(cid:89)(cid:72)(cid:85)(cid:68)(cid:74)(cid:72)
(cid:3)(cid:76)(cid:81)(cid:76)(cid:87)(cid:11)(cid:12)
(cid:3)(cid:68)(cid:74)(cid:74)(cid:85)(cid:72)(cid:74)(cid:68)(cid:87)(cid:72)(cid:11)(cid:12)
Fig. 1: GARFIELD components and API.
weaker than for the previous two GARs. It requires q ≥ 2f +1.
4. Bulyan [21] robustly aggregates q vectors by iterating
several (say k) times over another Byzantine–resilient GAR,
e.g., Multi–Krum. In each of these k iterations, Bulyan extracts
the vectors selected by such a GAR. Then, it computes the
then
coordinate-wise median of the k selected vectors. It
(cid:3) vectors to the computed median, and
extracts the closest k
(cid:3) vectors.
ﬁnally returns the coordinate-wise average of these k
Unlike previous GARs, Bulyan can sustain a model with a
large dimension. Yet, it requires q ≥ 4f +3, and its asymptotic
complexity is O(q2d).
Tradeoffs. In addition to the differences in the ratio of toler-
ated Byzantine nodes (inequalities relating q with f) and the
computational cost of each GAR, the model dimension is also
crucial in deciding which GAR to use. For high dimensions
(e.g., order of millions) and a strong adversary, one should
use Bulyan. In low dimensions, the application setup should
satisfy the variance assumption, as given below:
∃κ ∈ ]1, +∞[ , ∀ (i, t, θ) ∈ [1 .. n − f ] × N × R
d
(cid:6)
(cid:7)(cid:8)(cid:8)(cid:8)g
κ Δ
E
(cid:9)
(cid:8)(cid:8)(cid:8)2
,
≤ (cid:9)∇L (θ)(cid:9) ,
(i)
t − E g
(i)
t
where,
Δ =
⎧⎪⎪⎪⎨
⎪⎪⎪⎩
(cid:5)
√
2
2f
n−f
n−f +
f (n−f−2)+f 2(n−f−1)
(cid:14)
(cid:4)
2
√
n − f
is the estimated gradient by worker i at time t, and
if GAR = Krum
if GAR = MDA
n−2 f−2
(i)
t
if GAR = Median,
where g
L(θ) is the loss function at the model state θ.
B. System Components
GARFIELD is designed in a modular way as shown in
Figure 1. In this section, we describe each of these modules.
a) Main objects: GARFIELD deﬁnes two main objects
that can be used for learning: Server and Worker. The server
is responsible for storing and updating the model state while
workers train this model using their own local data. The server
typically initiates a learning step by asking a few workers to
compute a gradient estimate given the model state it owns.
For this, the server object exposes the method get gradients(),
which we describe later in detail. Some algorithms require
41
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
servers to exchange their models states, and hence the server
object comes with the method get models(), which is the
second key method in our Networking interface, as we describe
below. In addition to the Networking methods, the server object
exposes methods to (1) update the model state, given some
optimizer and a gradient estimate, (2) re-write the model,
which is useful in the case of multiple server replicas, and
(3) compute accuracy, given the model state and a test set.
The worker design on the other hand is much simpler. The
worker object is passive in the sense that it only responds to
requests of the server. Basically, the worker owns some data
and deﬁnes some loss function. Its main job is to compute a
gradient estimate, when asked by the server, using the data
chunk it owns. The worker then replies to the server by the
gradient estimate it computed.
To support
experimenting with Byzantine behavior,
GARFIELD deﬁnes two objects: Byzantine Server and Byzan-
tine Worker, which inherit from our main objects, Server
and Worker respectively. Both objects implement the popular
attacks published in the Byzantine ML literature including
simple ones like reversing vectors, dropping vectors, or ran-
dom vectors [17], [20] and the state–of–the–art attacks like
little is enough [10] and fall of empires [54].
b) Networking: Existing networking abstractions in both
frameworks, TensorFlow and PyTorch, are not enough to be
used (1) in a Byzantine, asynchronous environment and (2)
with replicated parameter servers.1 For example, one can
deploy distributed training on PyTorch using DistributedData-
Parallel() or on TensorFlow by running ParameterServerStrat-
egy(). However, both are high–level abstractions that assume
trusted, always–available machines.
GARFIELD (as a part of the Networking API) supports
two abstractions to handle communication: get gradients() and
get models(). The ﬁrst one is used to read the computed
gradients by the workers. It accepts two parameters: t, the
index of the current iteration, and qw, the number of workers
from which a server should receive replies with qw ≤ nw
(nw denotes the total number of workers); qw = nw denotes
synchronous communication with no faults in the system, i.e.,
a server is expecting to receive replies from all workers. This
function then returns the fastest qw gradients it receives. The
second abstraction works in the exact same way, yet fetch-
ing models from servers instead of gradients from workers.
Both abstractions then enable easy and natural communication
among all machines in the network in both synchronous and
asynchronous settings (as we describe in Section V). We give
the implementation details of these abstractions in Section IV.
four
Byzantine–resilient GARs mentioned above, on both CPUs
and GPUs. We create wrappers
(including dependency
c) Aggregation: GARFIELD implements
the
1Not to be confused with replicated graphs in TensorFlow. In some cases,
the server needs to be replicated where these replicas (all have the same graph)
are independent, rather than shared between machines, and they do exactly
the same computation on the same data for fault tolerance rather than for
performance. Both kinds of replication can be combined, but this is out of
the scope of this paper.
management, automatic compilation and loading) to use them
as custom operations in both TensorFlow and PyTorch. Such
wrappers make it possible to involve the GARs with the
same interface for both frameworks, though the lower–level
interfaces each framework provides differ substantially.
To use a GAR,
the common interface consists in two
functions: init() and aggregate(). The init() function takes the
name of the required GAR (e.g., “median”), the value of n,
the total number of inputs, and f, the maximum number of
Byzantine inputs. The second function, aggregate(), takes n
tensors (could represent gradients or models) and outputs the
aggregated one. Whether this function will execute on a CPU
or a GPU depends on the device on which the input vectors
are stored. In this way, our design abstracts the device, CPU
or GPU, and the framework, TensorFlow or PyTorch, away
from the developer.
IV. THE GARFIELD IMPLEMENTATION
First, we present how we implement the communication
abstractions, i.e., get models() and get gradients() in Tensor-
Flow and PyTorch. Then, we show how we implement an
efﬁcient version of the median function (which is used in Me-
dian and Bulyan GARs) on GPUs.2 Finally, we discuss some
tricks we employ for better memory management. Our code
is available [5] and will be open–sourced upon publication.
A. Communication in TensorFlow
TensorFlow adopts the notion of a shared dataﬂow graph
in which all computations are deﬁned in one graph, even if
deployed in a distributed environment, where all participating
nodes share this graph. This is a critical vulnerability in the
Byzantine setting as Byzantine nodes can write and execute
code on the other honest nodes [17]. Also, such shared graph
abstraction hides the data communication among workers and
servers, reducing the programming ﬂexibility and disallowing
having multiple communication rounds per learning step,
which is crucial for Byzantine resilience [20].
We follow another route in which all nodes create an
independent, yet replicated graph. Though this design has
high memory overhead, we believe it is necessary to tolerate
adversarial behavior.3 In addition to resolving the vulnera-
bility, this design allows for more ﬂexible communication
patterns among the participating nodes. We use gRPC for
communication and protocol buffers [48] for serializing and
deserializing data. We use the pull model for transferring data:
when a node needs some data, it pulls this data from the
other nodes by initiating multiple remote procedure calls to
such nodes. Each node implements a server that serves these
requests. We deﬁne the protocol buffers which encode data
exchanged between participants. We parallelize the replicated
communication between workers and servers for requesting
gradients and updated models so as to reduce the commu-
nication time as much as possible. However, abandoning the
2We also include GPU implementation of other GARs yet, we focus on
median because its GPU implementation is challenging as we discuss.
3Such an overhead could be reduced if the environment is Byzantine–free.
Authorized licensed use limited to: Tsinghua University. Downloaded on October 11,2021 at 09:24:13 UTC from IEEE Xplore.  Restrictions apply. 
42
highly optimized TensorFlow distributed runtime and using
independent graphs on each node require context switches
between TensorFlow and Python runtimes (as protocol buffers
currently cannot serialize tensors directly). Concretely, when
a node is requested to send a gradient or a model, it serializes
the requested data to a protocol buffer, exiting the TensorFlow
graph/runtime. On the receiver side, a node deserializes the
received bytes back to a tensor. Our experiments show that the
overhead of these conversions (including memory copying) is
non-negligible.
B. Communication in PyTorch
We implement the same abstractions in PyTorch yet with
a slightly different design compared to the TensorFlow one.
First, there is no context switch between PyTorch and Python
since PyTorch gives communication abstractions that can be
used directly on tensors. Second, we pipeline the commu-
nication with aggregation (whenever possible) as PyTorch
gives access to gradients of each layer in the deep network
separately; this allows for better utilization of both network