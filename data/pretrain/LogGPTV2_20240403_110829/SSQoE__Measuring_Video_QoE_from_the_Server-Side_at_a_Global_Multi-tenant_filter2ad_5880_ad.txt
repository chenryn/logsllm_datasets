RTT measurement (left).
Fig. 12. RTT vs QoE for U.S wireless carriers.
7 Discussion
There are some aspects of video content delivery that this paper does not cover
in detail and must be kept in mind while deploying such method.
Variable Segment Size and Quality
A common practice to deliver large ﬁles is to chunk the ﬁle. Here the original
ﬁle (could be a few GBs) is broken into similarly sized chunks (in the order of
MBs). Each chunk is treated as a separate content asset. In such cases, one video
segment request from the client can map to more than one chunk. Care needs
to be taken to account for ﬂow completion times of each chunk as well as add
only the corresponding video segment duration to buﬀer estimation algorithm
(described in Sect. 4) that the chunk represents. Note that subsequent requests
620
A. Shah et al.
from the client can be of diﬀerent quality therefore video duration of chunks in
subsequent requests can vary.
Estimation During Fewer Video Sessions
While the proposed method detects rebuﬀering on a per-session granularity, we
note that relying on this signal when the number of sessions is too few may lead
to noise. We aggregate the estimated rebuﬀering for all sessions that lie in the
current time bin and draw aggregated average value, therefore, it is imperative
that there are enough data points where the average is a reliable representative
value. Too few data points result in averages sensitive to outliers.
We perform our analysis using a minimum of 50 sessions per minute, a thresh-
old which showed empirical evidence in tracking meaningful rebuﬀering ratios.
We have designed SSQoE with CDN delivery and performance-based decisions in
mind. We do not focus on all client behavior metrics that video providers might
want to track from their players, such as video start up failures, ad engagement
metrics, protocol performance level A/B testing like QUIC vs TCP clients, etc.
At present, these analyses are not supported by our system.
Impacts of user Interactions with the Video Stream
It is possible that user actions such as pausing, switching from WiFi to LTE,
etc. can impact buﬀer occupancy and lead to estimation inaccuracies. SSQoE
score is by design an aggregate metric that combines the performance (rebuﬀer
estimations, time taken to download segments, etc.) of all the sessions in a time
window. It is highly unlikely that a large fraction of viewers pause the video
or otherwise introduce similar user behavior at the same time. Thus on a large
scale, individual session inaccuracies become negligible. The aggregate signal
represents how performance looks on average for many users that are e.g. in
the same ASN or that connect to the same PoP. Future work can quantify and
account for this small fraction of error margin in SSQoE.
Change Management
The QoE score’s ability to track user level impacts due to degraded server side
performance such as CPU bottlenecks (Fig. 8) or origin server issues (Fig. 10)
makes it a good candidate for change/conﬁguration management. During a
recent update to the cache management software at the CDN, a bug caused
degraded I/O performance. The impact this bug caused on live video streams
was captured by our methodology where the QoE score spiked up by 3 times
its baseline value. Thanks to our automated detection, our site-reliability team
could proactively work on rolling back the change. We are working towards inte-
grating QoE score into the CDN’s ML-based monitoring service.
Other Considerations
We do not make any protocol assumptions while estimating the QoE score.
We have tested the feasibility of SSQoE for HLS and DASH with success. It is
possible, however, that future protocols that support low latency streaming such
as WebRTC might need to be evaluated to determine the eﬃcacy of SSQoE.
Ethical considerations are kept in mind while designing proposed method.
We do not track or expose anything more than what is already captured in the
SSQoE: Measuring Video QoE from the Server-Side
621
CDN access logs. We only track the performance of the video stream by looking
at the meta information. The content segments remain encrypted (HTTPS).
8 Related Work
Measuring video performance and evaluating QoE has been studied from many
angles. To the best of our knowledge our work is the ﬁrst to propose a pure server-
side client player buﬀer estimation and show its feasibility at a large multi-tenant
scale. However, there are several methods that are relevant to our work. Broadly
we classify these into three categories based on whether they use QoE for: a)
video performance monitoring and characterization of traﬃc, b) steering traﬃc
at large content providers, or c) evaluating change in network conﬁgurations.
Video Performance Monitoring
Poor video performance leads to less user engagement [15,23]. In [7] authors
quantify the impact of QoE metrics in user engagement during a large live OTT
streaming event. Speciﬁcally, this work points out that bitrate and rebuﬀering
have the most impact on how users engage. The authors propose using PCA and
Hampel ﬁlter for live detection of QoE impairments. We take insights from this
work and build more scalable method that instead of performing resource con-
suming PCA, estimates the client player behavior to detect rebuﬀering. In [18],
using client-side metrics authors identify that video quality is determined by
subset of critical features. Tangential to our server-side scoring mechanism, their
methodology provides a QoE prediction scheme that can be deployed on beacon
data at scale.
Authors in [11,14,20] address the challenge of detecting QoE in encrypted
traﬃc. As video streaming platforms provide end-to-end encryption,
it has
become challenging for middle-mile network providers to perform video spe-
ciﬁc optimizations to speciﬁcally target video traﬃc with the goal of improving
QoE. Our work diﬀers from the focus of these papers. SSQoE is designed for
the video provider infrastructure (i.e., CDN) where the TLS termination occurs.
The CDN can identify unique video traﬃc, video segments, etc. to perform the
proposed QoE estimation. In [22], similar to our work’s motivation, authors
emphasize that network metrics alone may miss QoE degradation event. Here,
the authors propose using user behavior information such as pausing, reducing
viewing area as indicators to predict QoE degradation. This diﬀers from our
server-side methodology where we do not rely on user metrics. User metrics are
often tracked by the video player and in case of 3rd party CDNs they may not
be available to estimate QoE.
In [21] use automated tests on the client side to interact with several online
services and throttle throughput to monitor video performance. This work
focuses on understanding the diversity in how diﬀerent streaming services oper-
ate. Diﬀerent platforms might optimize their player’s ABR behavior for diﬀerent
guarantees of QoE, such has utilizing only limited amount of available bandwidth
or compromising bitrate to keep up with live stream. The methods used in this
paper revolve around a one time study to understand the landscape of video
622
A. Shah et al.
streaming. It is not designed to be an operational component of a multi-tenant
CDN provider. On the other hand, given the diversity in streaming landscape
this work motivates the need to perform measurements from the server-side and
use a method such has ours that is completely player agnostic. YouTube’s video
traﬃc has been studied by many researchers [9,13]. These works diﬀer from ours
since they are either aimed at speciﬁcally analyzing YouTube’s traﬃc behavior
or trying to understand the ISP and CDN caching implications on QoE all using
client-side data.
Traﬃc Management Systems
Recent performance measurement and traﬃc management systems developed
by Google [28], Facebook [26,27], Mircrosoft [12,19] use several measurement
schemes to evaluate performance and use that information to either localize
faults or pick an alternate route for egress traﬃc that will lead to better QoE. In
particular, EdgeFabric [27] and Espresso [28] focus on egress steering method-
ologies. Both of these systems leverage data from the client apps to gain per-
formance insights. Odin [12] uses several data sources from both client and
server-side along with active measurements for CDN performance evaluation.
Although these systems are relevant for measuring performance, they do not
explicitly track video performance. A more general approach proposed by Face-
book is by tracking HDratio [26], which indeed focuses on video performance.
The authors propose using the achievement of 2.5Mbps throughput i.e., enough
to serve HD content as an indicator to measure performance. Similar to our work,
this method also relies on only server-side measurements and can be applied
for a multi-tenant environment. However, relying on such hard thresholds in
a multi-tenant mixed-workload streaming landscape does not scale well. It is
operationally hard to perform diﬀerent analysis for HD, Ultra-HD, 4K, multiple
bitrate qualities using multiple thresholds. Moreover, tracking throughput gives
you a red ﬂag on network performance degradation, there is no guarantee that
the client player did actually suﬀer rebuﬀering. Our estimating buﬀer algorithm
tracks the actual client player buﬀer therefore every time an event is detected
i.e., estimated buﬀer at client is zero, we know with high accuracy the video has
rebuﬀered. Using throughput based metrics also do not work well for server-side
Ad insertion in the video stream. Ads in the middle of the video could be encoded
in a lower or higher bitrate in which case a change in throughput is expected
and may not necessarily indicate performance degradation. As shown in Sect. 6,
our proposed methodology does not suﬀer from such unintended impacts.
Conﬁguration Evaluation
In [16] authors measure QoE to evaluate network buﬀer change. Authors in this
work attempt to measure the impact on QoE while tuning network buﬀer in a
testbed. However, authors do not measure important user engagement impact-
ing metric such as rebuﬀering [7]. We agree with the motivation of this work,
that QoE impacts of network conﬁgurations are largely unknown. We have pro-
posed an easily scalable server-side methodology that we hope will be used by
future research on network parameter tunings and evaluate impacts on client
player buﬀer. The proposed methodology is part of our change/conﬁguration
SSQoE: Measuring Video QoE from the Server-Side
623
management strategy at the CDN, including for a recent change we made for
our network buﬀer sizes. While the evaluation of that change is out of scope of
this work, we note that server-side QoE was able to accurately track the progress
of this network change and we encourage other large content providers to include
such metric in their change management process as well.
9 Conclusion
With the rise of video streaming applications on the Internet, their ingest, distri-
bution, and performance monitoring has become increasingly complex. Current
state of the art monitoring solutions rely on client-side beacons which require
considerable instrumentation. Moreover, this beacon data is not easily exposed
to multi-tenant third-party CDN providers. This results in cases where CDNs
deliver a bulk of the video traﬃc without proper visibility into client-perceived
QoE and performance.
In this paper, we analyzed the video processing pipeline, characterized the
video streaming workload on a large scale CDN and derived key features that
can be tracked from the server-side to understand client-perceived QoE. We
then presented and validated SSQoE, a method for estimating client rebuﬀering
using passive measurements, by analyzing a sequence of requests from CDN
access logs to derive a QoE score that represents the health of video stream.
Traditional client-side metrics can only reveal the device or last-mile problems.
Mapping them to the CDN infrastructure is generally not easy, making client-side
beacons less viable for large scale CDN operations. Server-side QoE estimation
has been in operation globally on our CDN for the past year. To the best of
our knowledge, this is the largest deployment of server-side video monitoring at
a commercial CDN. It is currently used for monitoring some of the biggest live
news, sports events, conferences, movie releases that millions of users engage
with, and it has helped identify issues using the QoE score in near-real-time and
correlate performance degradation with other CDN insights during large scale
events.
We have explored the possibilities of server-side QoE analytics and invite the
industry and academia to collaborate, contribute, and explore more use cases in
this direction.
References
1. Adaptive bitrate (ABR). https://en.wikipedia.org/wiki/Adaptive bitrate strea
ming
2. Conviva platform. https://www.conviva.com/about/
3. FFmpeg utility. https://ﬀmpeg.org
4. HLS.js player. https://github.com/video-dev/hls.js
5. Linux Traﬃc Control (TC) utility. https://man7.org/linux/man-pages/man8/tc.
8.html
6. NGNIX RTMP. https://github.com/arut/nginx-rtmp-module
624
A. Shah et al.
7. Ahmed, A., Shaﬁq, Z., Bedi, H., Khakpour, A.: Suﬀering from buﬀering? detecting
qoe impairments in live video streams. In: 2017 IEEE 25th International Conference
on Network Protocols (ICNP), pp. 1–10 (2017)
8. Ahmed, A., Shaﬁq, Z., Khakpour, A.: Qoe analysis of a large-scale live video
streaming event. In: Proceedings of the 2016 ACM SIGMETRICS International
Conference on Measurement and Modeling of Computer Science, SIGMETRICS
2016, pp. 395–396. Association for Computing Machinery, New York (2016)
9. A˜norga, J., Arrizabalaga, S., Sedano, B., Goya, J., Alonso-Arce, M., Mendizabal,
J.: Analysis of Youtube?s traﬃc adaptation to dynamic environments. Multimedia
Tools Appl. 77, 7977–8000 (2017). https://doi.org/10.1007/s11042-017-4695-9
10. B¨ottger, T., Cuadrado, F., Uhlig, S.: Looking for hypergiants in PeeringDB. SIG-
COMM Comput. Commun. Rev. 48(3), 13–19 (2018)
11. Bronzino, F., Schmitt, P., Ayoubi, S., Martins, G., Teixeira, R., Feamster, N.: Infer-
ring streaming video quality from encrypted traﬃc: practical models and deploy-
ment experience. Proc. ACM Meas. Anal. Comput. Syst. 3(3), 1–25 (2019)
12. Calder, M., et al.: Odin: microsoft’s scalable fault-tolerant CDN measurement sys-
tem. In: USENIX NSDI, April 2018
13. D’Alconzo, A., Casas, P., Fiadino, P., Bar, A., Finamore, A.: Who to blame when
YouTube is not working? detecting anomalies in CDN-provisioned services. In:
2014 International Wireless Communications and Mobile Computing Conference
(IWCMC), pp. 435–440 (2014)
14. Dimopoulos, G., Leontiadis, I., Barlet-Ros, P., Papagiannaki, K.: Measuring video
QoE from encrypted traﬃc. In: Proceedings of the 2016 Internet Measurement
Conference, IMC 2016, pp. 513–526. Association for Computing Machinery, New
York (2016)
15. Dobrian, F., et al.: Understanding the impact of video quality on user engagement.
Commun. ACM 56(3), 91–99 (2013)
16. Hohlfeld, O., Pujol, E., Ciucu, F., Feldmann, A., Barford, P.: A QoE perspective on
sizing network buﬀers. In: Proceedings of the 2014 Conference on Internet Measure-
ment Conference, IMC 2014, pp. 333–346. Association for Computing Machinery,
New York (2014)
17. Hyndman, R.J., Athanasopoulos, G.: Classical decomposition of time-series data.
https://otexts.com/fpp2/classical-decomposition.html
18. Jiang, J., Sekar, V., Milner, H., Shepherd, D., Stoica, I., Zhang, H.: CFA: a practical
prediction system for video QoE optimization. In: Proceedings of the 13th USENIX
Conference on Networked Systems Design and Implementation, NSDI 2016, pp.
137–150. USENIX Association, USA (2016)
19. Jin, Y., et al.: Zooming in on wide-area latencies to a global cloud provider. In:
ACM SIGCOMM, August 2019
20. Khokhar, M.J., Ehlinger, T., Barakat, C.: From network traﬃc measurements to
QoE for internet video. In: 2019 IFIP Networking Conference (IFIP Networking),
pp. 1–9 (2019)
21. Licciardello, M., Gr¨uner, M., Singla, A.: Understanding video streaming algorithms
in the wild. In: Sperotto, A., Dainotti, A., Stiller, B. (eds.) Passive and Active
Measurement. pp, pp. 298–313. Springer International Publishing, Cham (2020).
https://doi.org/10.1007/978-3-030-44081-7 18
22. Mok, R.K., Chan, E.W., Luo, X., Chang, R.K.: Inferring the QoE of http video
streaming from user-viewing activities. In: Proceedings of the First ACM SIG-
COMM Workshop on Measurements up the Stack, W-MUST 2011, pp. 31–36.
Association for Computing Machinery, New York (2011)
SSQoE: Measuring Video QoE from the Server-Side
625
23. Nam, H., Kim, K., Schulzrinne, H.: QoE matters more than QOS: why people
stop watching cat videos. In: IEEE INFOCOM 2016 - The 35th Annual IEEE
International Conference on Computer Communications, pp. 1–9 (2016)
24. Richter, P., et al.: A multi-perspective analysis of carrier-grade NAT deployment.
In: Proceedings of the 2016 Internet Measurement Conference, IMC 2016, pp. 215–
229. Association for Computing Machinery, New York (2016)
25. R¨uth, J., Wolsing, K., Wehrle, K., Hohlfeld, O.: Perceiving QUIC: do users notice
or even care? arXiv:1910.07729 (2019)
26. Schlinker, B., Cunha, I., Chiu, Y.-C., Sundaresan, S., Katz-Bassett, E.: Internet
performance from facebook’s edge. In: Proceedings of the Internet Measurement
Conference, IMC 2019, pp. 179–194. Association for Computing Machinery, New
York (2019)
27. Schlinker, B., et al.: Engineering egress with edge fabric: steering oceans of con-
tent to the world. In: Proceedings of the Conference of the ACM Special Interest
Group on Data Communication, SIGCOMM 2017, pp. 418–431. Association for
Computing Machinery, New York (2017)
28. Yap, K.: et al.: Taking the edge oﬀ with espresso: scale, reliability and programma-
bility for global internet peering (2017)