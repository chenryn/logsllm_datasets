## DuckDB 支持超融合计算, 会给“大数据”产品带来什么冲击? 会给“大数据”应用开发者带来什么改变?       
### 作者                                            
digoal                                            
### 日期                                            
2024-02-01                                            
### 标签                                            
PostgreSQL , PolarDB , DuckDB , 大数据 , 管道 , 超融合计算 , 应用即算力                              
----                                            
## 背景     
DuckDB 支持超融合计算, 会给“大数据”产品带来什么冲击? 会给“大数据”应用开发者带来什么改变?      
干掉, 或者至少会冲击“大数据”数据库产品. 理由如下    
- 大数据平台通常就是企业数据的池子, 各种业务的历史数据汇聚到此, 为了进行全域数据计算.     
- 应用加载libduckdb库, 应用层将具备足够的算力(根据诸多AP类的benchmark数据指出, duckdb的计算能力惊人, 排在一线AP产品之列), 企业可以将业务历史数据汇聚集中到对象存储(例如parquet文件格式存储), 通过libduckdb即可高效进行数据分析.      
- 另一方面, 如果需要访问实时业务数据呢? duckdb支持了attach database的能力, 现在支持postgresql, mysql, sqlite. 未来会增加更多的目标端.     
所以DuckDB会干掉, 或者至少会冲击“大数据”数据库产品.     
这个理念在databend产品中也有体现(实际上databend就是由各种rust开源组件SQL解析、优化器、存储格式、存储引擎拼装起来的.)  
- [《一款兼容mysql,clickhouse 使用rust写的数据湖产品databend(号称开源版snowflake) - 适合"时序、IoT、feed?、分析、数据归档"等场景》](../202303/20230329_01.md)
- [《将 "数据结构、数据存储" 从 "数据库管理系统" 剥离后 - 造就了大量大数据产品(DataFusion, arrow-rs, databend等)》](../202303/20230328_02.md)
- [《PostgreSQL deltaLake 数据湖用法 - arrow + parquet fdw》](../202005/20200527_04.md)
- [《DuckDB parquet 分区表 / Delta Lake(数据湖) 应用》](../202209/20220905_01.md)
- [《什么是 Delta Lake (数据湖)》](../202209/20220905_02.md)
- [《DuckDB DataLake 场景使用举例 - aliyun OSS对象存储parquet》](../202210/20221026_01.md)
- https://datamonkeysite.com/2023/05/22/databend-and-the-rise-of-data-warehouse-as-a-code/  
![pic](20240201_01_pic_002.png)    
接下来的内容来自duckdb blog:    
- https://duckdb.org/2024/01/26/multi-database-support-in-duckdb.html#multi-database-transactions    
DuckDB can attach MySQL, Postgres, and SQLite databases in addition to databases stored in its own format. This allows data to be read into DuckDB and moved between these systems in a convenient manner.    
![pic](20240201_01_pic_001.png)    
In modern data analysis, data must often be combined from a wide variety of different sources. Data might sit in CSV files on your machine, in Parquet files in a data lake, or in an operational database. DuckDB has strong support for moving data between many different data sources. However, this support has previously been limited to reading data and writing data to files.    
DuckDB supports advanced operations on its own native storage format – such as deleting rows, updating values, or altering the schema of a table. It supports all of these operations using ACID semantics. This guarantees that your database is always left in a sane state – operations are atomic and do not partially complete.    
DuckDB now has a pluggable storage and transactional layer. This flexible layer allows new storage back-ends to be created by DuckDB extensions. These storage back-ends can support all database operations in the same way that DuckDB supports them, including inserting data and even modifying schemas.    
The [MySQL](https://duckdb.org/docs/extensions/mysql), [Postgres](https://duckdb.org/docs/extensions/postgres), and [SQLite](https://duckdb.org/docs/extensions/sqlite) extensions implement this new pluggable storage and transactional layer, allowing DuckDB to connect to those systems and operate on them in the same way that it operates on its own native storage engine.    
These extensions enable a number of useful features. For example, using these extensions you can:    
- Export data from SQLite to JSON    
- Read data from Parquet into Postgres    
- Move data from MySQL to Postgres    
- … and much more.    
## Attaching Databases    
The [ATTACH statement](https://duckdb.org/docs/sql/statements/attach) can be used to attach a new database to the system. By default, a native DuckDB file will be attached. The `TYPE` parameter can be used to specify a different storage type. Alternatively, the `{type}:` prefix can be used.    
For example, using the SQLite extension, we can open a [SQLite database file](https://github.com/duckdb/sqlite_scanner/raw/main/data/db/sakila.db) and query it as we would query a DuckDB database.    
```    
ATTACH 'sakila.db' AS sakila (TYPE sqlite);    
SELECT title, release_year, length FROM sakila.film LIMIT 5;    
┌──────────────────┬──────────────┬────────┐    
│      title       │ release_year │ length │    
│     varchar      │   varchar    │ int64  │    
├──────────────────┼──────────────┼────────┤    
│ ACADEMY DINOSAUR │ 2006         │     86 │    
│ ACE GOLDFINGER   │ 2006         │     48 │    
│ ADAPTATION HOLES │ 2006         │     50 │    
│ AFFAIR PREJUDICE │ 2006         │    117 │    
│ AFRICAN EGG      │ 2006         │    130 │    
└──────────────────┴──────────────┴────────┘    
```    
The `USE` command switches the main database.    
```    
USE sakila;    
SELECT first_name, last_name FROM actor LIMIT 5;    
┌────────────┬──────────────┐    
│ first_name │  last_name   │    
│  varchar   │   varchar    │    
├────────────┼──────────────┤    
│ PENELOPE   │ GUINESS      │    
│ NICK       │ WAHLBERG     │    
│ ED         │ CHASE        │    
│ JENNIFER   │ DAVIS        │    
│ JOHNNY     │ LOLLOBRIGIDA │    
└────────────┴──────────────┘    
```    
The SQLite database can be manipulated as if it were a native DuckDB database. For example, we can create a new table, populate it with values from a Parquet file, delete a few rows from the table and alter the schema of the table.    
```    
CREATE TABLE lineitem AS FROM 'lineitem.parquet' LIMIT 1000;    
DELETE FROM lineitem WHERE l_returnflag = 'N';    
ALTER TABLE lineitem DROP COLUMN l_comment;    
```    
The `duckdb_databases` table contains a list of all attached databases and their types.    
```    
SELECT database_name, path, type FROM duckdb_databases;    
┌───────────────┬───────────┬─────────┐    
│ database_name │   path    │  type   │    
│    varchar    │  varchar  │ varchar │    
├───────────────┼───────────┼─────────┤    
│ sakila        │ sakila.db │ sqlite  │    
│ memory        │ NULL      │ duckdb  │    
└───────────────┴───────────┴─────────┘    
```    
## Mix and Match    
While attaching to different database types is useful – it becomes even more powerful when used in combination. For example, we can attach both a SQLite, MySQL and a Postgres database.    
```    
ATTACH 'sqlite:sakila.db' AS sqlite;    
ATTACH 'postgres:dbname=postgresscanner' AS postgres;    
ATTACH 'mysql:user=root database=mysqlscanner' AS mysql;    
```    
Now we can move data between these attached databases and query them together. Let’s copy the `film` table to MySQL, and the `actor` table to Postgres:    
```    
CREATE TABLE mysql.film AS FROM sqlite.film;    
CREATE TABLE postgres.actor AS FROM sqlite.actor;    
```    
We can now join tables from these three attached databases together. Let’s find all of the actors that starred in `Ace Goldfinger`.    
```    
SELECT first_name, last_name    
FROM mysql.film    
JOIN sqlite.film_actor ON (film.film_id = film_actor.film_id)    
JOIN postgres.actor ON (actor.actor_id = film_actor.actor_id)    
WHERE title = 'ACE GOLDFINGER';    
┌────────────┬───────────┐    
│ first_name │ last_name │    
│  varchar   │  varchar  │    
├────────────┼───────────┤    
│ BOB        │ FAWCETT   │    
│ MINNIE     │ ZELLWEGER │    
│ SEAN       │ GUINESS   │    
│ CHRIS      │ DEPP      │    
└────────────┴───────────┘    
```    
Running `EXPLAIN` on the query shows how the data from the different engines is combined into the final query result.    