of the same attack with slight variation being matched between two sites.
We run our experiments correlating the abnormal traﬃc between sites from
our October-November eight week dataset and our December three week dataset
12
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
Oct-Nov
Oct-Nov
with
training
Dec.
Dec. Gained by
with
Dec.
adding Common to
training third server Three Sites
Duration of testing3
Total false positives
Unique false positives
Total true positives
Unique true positives
54 days
46653
64
19478
351
47 days 19 days
40364
48
7404
186
362
13
7599
263
12 days
1031
5
2805
89
1006
3
186
9
0
0
322
8
Table 5. Main experiment results considering a match to be at least 80% of n-grams
being in a Bloom ﬁlter. Note that the 5th column results are included in column 4.
Also, note that due to self training time for the CAD sensor actual time spent testing
data is about two days less.
and then manually classify the results since ground truth is not known. We depict
the system’s alerts in Table 5. As we predicted in [4], most of the false positives
repeat themselves early and often so we also show the results assuming a na¨ıve
one week training period which labels everything seen in that week and then
ignores it. While this training technique could certainly be improved upon, we
choose to show this example in order to better show the eﬀectiveness of the
approach as a whole and to preclude any optimizations that might turn out to
be dataset speciﬁc. Such a training period provides a key service in that most
false positives are either due to a client adding additional parameters regardless
of web server, such as with certain browser add-ons, or servers both hosting the
same application with low enough traﬃc throughput that it fails to be included
in a normal model. Many of these cases tend to be rare enough to not be modeled
but repeat often enough that a training period will identify them and prevent
an operator from having to deal with large volumes of false positives. Certainly
with such a na¨ıve automated approach, attacks will not be detected during this
training period, but after this period we end up with a large beneﬁt in terms of
few false positives with little negative beyond the single week of vulnerability.
Any attacks seen during training that are then ignored in the future would have
already compromised the system so we do not give an attacker an advantage
going forward. In fact this training period serves an operator well in that many
of the high volume attacks that are left over “background radiation” will be
seen in this training period and thus not have to be categorized in the future.
Adding an additional web server in our last experiment provides a glimpse at
how broadening the scope of collaboration to a larger network of web servers
can help us realize a high detection rate.
Let us now analyze how accurate our system is. The false positive rate is
relatively easy to compute. We manually classify the unique alerts and then
count the total occurrences of each. With regard to the number of requests that
pass through the normalization process the false positive rate is 0.03%. If you
calculate it based on the total incoming requests then it is much less. The true
positive rate or detection rate is much harder to accurately measure since we have
no ground truth. Recall, we are trying to detect widespread attacks and leave the
Cross-domain Collaborative Anomaly Detection: So Far Yet So Close
13
mosconﬁg absolute path=http://phamsight.com/docs/images/head??
conﬁg[ppa root path]=http://phamsight.com/docs/images/head??
option=com gcalendar&controller=../../../../../../../../../../../../../../../proc/self/environ%
id=’ and user=–
id=-.+union+select+–
command=createfolder&type=image&currentfolder=/fck.asp&newfoldername=test&uuid=
option=com user&view=reset&layout=conﬁrm
Table 6. Normalized examples of actual attacks seen at multiple sites.
goal of detecting attacks targeted at a single site to other security methods in
order to better leverage collaboration. With this in mind, there exists two places
where a widespread attack could be missed. An attack could arrive at multiple
sites but not be detected as abnormal by the one of the local CAD sensors and
therefore, never be exchanged with other sites. The other possibility is that an
attack could be abnormal at both sites but diﬀerent enough that the correlation
method fails to properly match it.
In the ﬁrst case where a local CAD sensor fails to identify the attack as
abnormal, we have a better chance to estimate our accuracy. Most CAD sensors
are vulnerable to mimicry attacks where an attacker makes the attack seem like
normal data by padding the malicious data in such a way as to fool the sensor.
We can mitigate this by deploying very diﬀerent sensors to each site, which while
individually vulnerable to a speciﬁc padding method as a whole are very diﬃcult
to bypass. In this way an attacker might bypass some sites, but as the attack is
widespread eventually two of the CAD sensors that the attacker is not prepared
for can detect the attack and broadcast a signature out to the rest of the sites.
In the latter scenario, we have to rely heavily on the vulnerable web applica-
tions having some structure to what input they accept so that attacks exploiting
the same vulnerability will be forced to appear similar. We can certainly loosen
correlation thresholds as seen in Table 8 as well as come up with more correlation
methods in the future. In practice, this is where the lack of ground truth hinders
a comprehensive review of our performance. As far as we can tell, between the
structure imposed by having to exploit a vulnerability with HTTP parameters,
lower correlation thresholds, and ﬁnding additional attributes for correlation we
should have a good head start on attackers in this arms race. At the very least,
our layer of security will make it a race instead of just forfeiting to attackers
immediately once a vulnerability is found. Without ground truth, we cannot be
sure that we detect all widespread attacks. We have seen no indication in our
data that attackers are using any of the above evasion techniques yet, so we be-
lieve that our system will provide an eﬀective barrier, one which we can continue
to strengthen using the above approaches.
3 Due to equipment outages approximately three hours of data is missing from the
Oct.-Nov. www.cs.columbia.edu dataset and less than 0.5% of the Dec. dataset ab-
normal data totals are missing.
14
N. Boggs, S. Hiremagalore, A. Stavrou, and S. J. Stolfo
ul=&act=&build=&strmver=&capreq=
c=&load=hoverintentcommonjquery-color&ver=ddabcfcccfadf
jax=dashboard secondary
feed=comments-rss
Table 7. Normalized examples of false positives seen at multiple sites.
We detect a broad range of widespread attacks, with some examples shown
in Table 6. Common classes of attacks show up such as code inclusion, directory
traversal, and SQL injection. Our system faithfully detects any wide spread
variants of these attacks, some of which might evade certain signature systems;
however, the novel attack detection our system provides lies with the last two
examples shown. These two attacks are attempting to exploit application speciﬁc
vulnerabilities, one attacking an in-browser text editor and the other a forum
system. Since attacks such as these resemble the format of legitimate requests
and lack any distinct attribute that must be present to be eﬀective, existing
defenses cannot defend against zero-day attacks of this class. The fact that our
system caught these in the wild bodes well for its performance when encountering
new widespread zero-day attacks.
An examination of the false positives explains the repeated nature and spo-
radic occurrences of new false positives. See Table 7 for some examples of normal-
ized false positives. All the false positives fall into one of two broad categories:
rare browser speciﬁc requests or rarely used web applications installed on two
or more collaborating servers. For example the most common false positive we
see is an Internet Explorer browser plug-in for Microsoft Oﬃce which sends a
GET request to the web server regardless of user intent. The use of this plug-in
is rare enough that the request shows up as abnormal at all sites. As for server
side applications, we see most of the unique false positives relating to the ad-
ministrative functions of isolated Word Press blogs which see so little use that
the requests stand out as abnormal. New false positives will continue to occur in
small numbers as web servers and browsers evolve over time (less than one per
three days on average during our eight week run). We believe that identifying
these few rare occurrences is quite manageable for operators. This task gets eas-
ier since as the number of collaborators grow so do the resources for the minimal
manual inspection needed to identify these isolated occurrences.
Adding a third web server, www.cs.gmu.edu, to the collaboration shows that
additional web servers help us to identify more attacks and allows some basic
insight into what types of web servers might be best grouped together for collab-
oration. Assuming our training method, adding this third server as a collaborat-
ing server exchanging data with www.cs.columbia.edu allows us to detect 11.25%
more unique attacks than just correlating alerts between www.cs.columbia.edu
and www.gmu.edu. This increase over the 80 unique attacks we detect without
it, supports the need for adding substantial numbers of collaborators to increase
the detection rate. Unfortunately this new collaborating server also introduces
false positives that we do not see in previous experiments. We expect as with
Cross-domain Collaborative Anomaly Detection: So Far Yet So Close
15
Oct-Nov
Oct-Nov
with
training
Dec.
Total false positives
Unique false positives
Total true positives
Unique true positives
47605
77
25042
488
23
439 41845
55
10168 9594
221
362
Dec. Gained by
with
Dec.
adding Common to
training third server Three Sites
0
0
293
8
1017
5
3272
109
4
1
254
10
Table 8. Experimental results considering a match to be at least 60% of n-grams to
be in a Bloom ﬁlter.
previous false positives that future experiments will most likely repeat these with
few new additions. An oﬄine correlation using edit distance shows both GMU
web servers having a number of attacks in common as well. This supports an
idea that collaborating with distinct web servers could be as useful as collab-
orating across sites. False positives seem to be a function of rarely used web
services located at each server, so servers hosting only a few clearly deﬁned and
well used services may give substantially better results.
This additional web server also provides the opportunity to require alerts
to be seen by at least three sites before reporting them as attacks. While this
proposition is hard to accurately evaluate with only one data set and just three