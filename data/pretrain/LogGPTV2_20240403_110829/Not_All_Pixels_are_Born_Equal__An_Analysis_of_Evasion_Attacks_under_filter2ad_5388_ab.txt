comparison to center patches with MNIST (M) and ImageNet
(I) dataset. In comparison to ImageNet, the difference is sig-
nificantly more pronounced for MNIST. It highlights that
the training dataset significantly affects the correlation of
adversarial patch success with different patch locations.
n × n pixels at the center, while the border patch refers to masking
n border pixels of an image (we choose n between 0-10).
Takeaway 1: Attack success is significantly influenced by patch
location. For both accuracy and target success rate (Figure 2), we
observe that the border patches lead to significantly poorer perfor-
mance compared to the center patches on MNIST. For instance, an
adversarial center patch with 8×8 pixels size degrades accuracy to
0.77% from original 99.17%, while a border patch of similar area
reduces the accuracy only to 81% (Figure 2a). The target success
rates for these patch sizes are 80% (close to maximum target success
possible) and 0.2% (ineffective) respectively (Figure 2b).
Takeaway 2: Training dataset influences the correlation be-
tween attack success and patch location The poor success of
border patches can be attributed to the consistent presence of less
informative features at the border pixels across all images in the
MNIST dataset. It can be due to the reason that the MNIST dataset
was originally created by centering 20×20 pixels images from NIST
dataset in a 28×28 pixels bounding box. To validate this hypothesis
we further experiment with the ImageNet dataset, which is more
diverse than MNIST. With ImageNet, even with small patch areas,
both border and center patches lead to significant attack success.
However, as the dataset is originally aggregated by filtering search
results from the internet for images corresponding to WordNet
synsets, it is plausible for it to exhibit a bias towards key objects
being in the center of the image. This difference can be observed in
Figure 2 as the center patch yields a higher target success rate and
a lower accuracy compared to a border patch of the same size.
3.2 Steering Angle Prediction
For steering angle prediction, we consider two neural networks:
PilotNet [3] and Chauffeur [2]. PilotNet is a convolutional neu-
ral network (CNN) proposed by NVIDIA [3], while Chauffeur, a
CNN+LSTM model, achieved the third place in the Udacity steering
angle prediction challenge [2]. We evaluate the attack success on
this task using both off-road and on-road patches over 100 random
images from the test set of Udacity self-driving car dataset [2]. With
each patch, the adversarial objective is to predict a 90◦ right turn.
Takeaway 1: Attack success is significantly affected by patch
location. The median change in output angle for an on-road patch
is at least 2-3× that of the change with an off-road patch of similar
size (Figure 3). This observation supports the previous hypothesis
that the attack success rate with adversarial patches depends on
the patch location. In real-world driving datasets, such as [2], a
significant number of training images capture straight roads. Due
10−210−1100101102Patcharea(%ofimagesize)050100Accuracy(%)randomleastlikelycenter(M)border(M)center(I)border(I)10−210−1100101102Patcharea(%ofimagesize)050100Targetsuccessrate(%)randomleastlikelycenter(M)border(M)center(I)border(I)Poster PresentationCCS’18, October 15-19, 2018, Toronto, ON, Canada2286(a) PilotNet
(b) Chauffeur
Figure 3: Change in output steering angle due to the adver-
sarial patch. In comparison to on-road patches, it highlights
the limited success of off-road patches for both models.
This difference between the success of both type of patches
further depends on model architecture, where deeper net-
works (e.g., Chauffeur) achieves higher robustness to off-
road patches as compared to shallower nets (e.g., PilotNet).
to this bias towards images of straight roads, where the key features
such as lane markings, road edges, and curvature are missing at
border pixels, the network learns a weaker correlation between
output and border pixels. Thus with off-road patches, which tend
to occupy border pixels, the attack success rate is very limited.
Takeaway 2: Network architecture significantly influences
attack success. We observe a significant difference in the attack
success rate of on-road and off-road patches for the Chauffeur
model compared to PilotNet. It can be attributed to its very deep
network, a combination of CNN and LSTM, which is be expected
to learn better features than a single-CNN based model.
Takeaway 3: Decrease in patch size leads to a decrease in at-
tack success. Due to the regression nature of the steering angle
prediction problem, the attack success decreases approximately lin-
early with patch area. This requires the adversary to use large patch
sizes to achieve significant change in the output of the network.
3.3 Semantic Image Segmentation
We evaluate semantic image segmentation robustness on two state-
of-the-art models: Deeplabv3+ [6] and PSPNet [13]. We evaluate
the attack success of each n × n (we choose n between 0-713) pixels
adversarial patch with random locations for 100 randomly selected
images from test partition of Cityscapes [7] dataset.
Takeaway 1: Attack success is highly dependent on the learn-
ing objective. Figure 4 shows the limited success of adversarial
patches which highlights that a single adversarial patch is unlikely
to influence the classification of each pixel in the input image. We
anticipate that this behaviour is most likely due to the limited ef-
fective receptive field of neural networks [9], which makes it very
difficult to achieve non-local changes with smaller patch sizes.
Takeaway 2: Adversarial objective significantly affects attack
success. We initially select the target adversarial label for each pixel
as its least likely predicted label. However, we observe that the at-
tack cannot achieve a 100% target success rate using this objective,
even with perturbations added to the whole image. Thus, we also
report another set of results with an easier objective where the
target label for all pixels is identical but selected randomly from all
possible categories (similar to Figure 1). The higher success of ad-
versarial patches with this objective highlights that the adversarial
objective can influence the success of adversarial patches.
Takeaway 3: Network architecture influences attack success.
We observe approximately 1.5-2× higher target success rate for
PSPNet than Deeplabv3+. This difference can be attributed to the
(a) Accuracy (P)
(b) Accuracy (D)
(c) Target success rate (P)
(d) Target success rate (D)
Figure 4: Attack success of adversarial patches with differ-
ent adversarial objectives for Deeplabv3+ (D) and PSPNet (P)
models. With semantic segmentation, smaller patch sizes
are ineffective in achieving significantly high attack success.
The attack success is further dependent on adversarial ob-
jective, i.e., selecting the target label for each pixel, and the
network architecture.
different architecture of these models. We are currently performing
ablation studies to understand the key reasons for this difference.
4 CONCLUSION
This paper evaluates the success of localized adversarial pertur-
bations for three different key applications of deep learning in
computer vision. Our analysis indicates that in order to carry out
realistic evasion attacks on these systems, better attack strategies
taking their architecture and datasets into account are required. We
are currently exploring these.
5 ACKNOWLEDGEMENT
This work was supported by the National Science Foundation un-
der grants CNS-1553437 and CNS 1409415, by the Office of Naval
Research (ONR) through Young Investigator Prize (YIP), by IBM
through IBM Faculty award and by Intel through Intel Faculty Re-
search Award.
REFERENCES
[1] 2017. NIPS 2017: Non-targeted Adversarial Attack. (2017).
[2] 2018. self-driving-car: The Udacity open source self-driving car project. (2018).
[3] Mariusz Bojarski, et al. 2016. End to End Learning for Self-Driving Cars.
arXiv:1604.07316 [cs] (April 2016).
[4] Tom B. Brown, et al. 2017. Adversarial Patch. arXiv:1712.09665 [cs] (Dec. 2017).
[5] Nicholas Carlini and David Wagner. 2017. Towards Evaluating the Robustness of
Neural Networks. In 2017 IEEE Symposium on Security and Privacy (SP). 39–57.
[6] Liang-Chieh Chen, et al. 2018. Encoder-Decoder with Atrous Separable Convo-
lution for Semantic Image Segmentation. arXiv:1802.02611 [cs] (Feb. 2018).
[7] Marius Cordts, et al. 2016. The Cityscapes Dataset for Semantic Urban Scene
Understanding. arXiv:1604.01685 [cs] (April 2016).
[8] Kevin Eykholt, et al. 2018. Robust Physical-World Attacks on Deep Learning
Visual Classification. In IEEE CVPR ’18.
[9] Wenjie Luo, et al. 2016. Understanding the effective receptive field in deep
convolutional neural networks. In NIPS. 4898–4906.
[10] Mahmood Sharif, et al. 2016. Accessorize to a Crime: Real and Stealthy Attacks
on State-of-the-Art Face Recognition. In ACM CCS ’16.
[11] Chawin Sitawarin, et al. 2018. DARTS: Deceiving Autonomous Cars with Toxic
Signs. arXiv:1802.06430 [cs] (Feb. 2018).
[12] Christian Szegedy, et al. 2013. Intriguing properties of neural networks. arXiv
[13] Hengshuang Zhao, et al. 2016. Pyramid Scene Parsing Network. arXiv:1612.01105
preprint arXiv:1312.6199 (2013).
[cs] (Dec. 2016).
1.53.04.56.0Area(%)0255075100∆steeringangleoff-roadon-road1.32.63.95.26.5Area(%)0255075100∆steeringangleoff-roadon-road2060100140180713Patchdimension(inpixels)050100Accuracy(%)leastlikelyrandom2060100140180713Patchdimension(inpixels)050100Accuracy(%)leastlikelyrandom2060100140180713Patchdimension(inpixels)050100Targetsuccessrate(%)leastlikelyrandom2060100140180713Patchdimension(inpixels)050100Targetsuccessrate(%)leastlikelyrandomPoster PresentationCCS’18, October 15-19, 2018, Toronto, ON, Canada2287