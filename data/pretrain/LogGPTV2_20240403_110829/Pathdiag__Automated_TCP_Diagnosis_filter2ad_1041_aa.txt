# Pathdiag: Automated TCP Diagnosis

**Authors:**  
- Matthew Mathis, Pittsburgh Supercomputing Center
- John Heffner, Pittsburgh Supercomputing Center
- Peter O'Neil, National Center for Atmospheric Research, Mid-Atlantic Crossroads
- Pete Siemsen, National Center for Atmospheric Research

## Abstract
This paper introduces a tool designed to diagnose network performance issues that commonly affect TCP-based applications. The tool, named Pathdiag, operates within a web server framework, providing non-expert users with one-click diagnostic testing, tuning support, and repair instructions. It identifies various causes of poor network performance by utilizing Web100 statistics and TCP performance models, thereby overcoming the lack of otherwise identifiable symptoms.

## 1. Introduction
The TCP/IP hourglass architecture is designed to hide the details of the network from applications and vice versa. This property is crucial for the ongoing evolution of the Internet, as it allows applications and the underlying network infrastructure to evolve independently. However, this also obscures many network flaws. Since TCP silently compensates for these flaws (e.g., by retransmitting lost data), the only symptom of most problems is reduced performance. This "symptom hiding" property motivated the Web100 project, which developed the TCP extended statistics MIB to expose hidden TCP protocol events. A MIB is a formal specification of a set of management variables that can be accessed via SNMP or other low-overhead mechanisms. Experimental prototypes of the MIB have been implemented in several operating systems, including Linux and Microsoft Windows Vista.

Diagnostic efforts are further complicated by another property of TCP: the symptoms of most flaws scale with the flow's round-trip time (RTT). For window-based protocols, performance models generally include an RTT term in the denominator. For example, insufficient TCP buffer space at either the sender or receiver, or background (non-congested) packet loss, can cause TCP to maintain a constant average window size, leading to performance inversely proportional to the RTT.

This poorly understood property often leads to faulty reasoning about diagnostic results. A simple throughput test on a short local section of a path with minor flaws may yield good results, while the same test over a longer path with the same local flaws may yield poor results. This "symptom scaling" property of TCP can lead to incorrect inductive reasoning about flaws, significantly complicating the resolution of end-to-end Internet performance problems.

This paper describes Pathdiag, a tool that uses TCP performance modeling to extrapolate the impact of local host and network flaws on applications running over long paths. The tool analyzes key metrics of the local host and path, using TCP performance models to determine thresholds based on the stated application performance goals. Pathdiag reliably detects flaws that have no user-noticeable symptoms over a short path and provides reports and suggested remedies.

### 1.1 Motivation
Over the past two decades, network performance has increased by an order of magnitude roughly every four years. Networking experts typically demonstrate the full data rate of each new network technology. However, typical users experience much lower data rates, and the gap is widening.

Internet2 has measured the performance of TCP bulk flows over their backbone since early 2002. As of August 2007, the median performance across their 10 Gb/s network was only about 3.4 Mb/s, a rate that has taken six years to double. A small number of flows achieve very high performance, with about 0.1% faster than 100 Mb/s, and half of those close to 1 Gb/s. This indicates that the backbone is free from flaws that would otherwise affect these sensitive flows.

The design goal of Pathdiag is to help non-expert users achieve better performance by easily and accurately diagnosing common flaws, which are generally near the edge of the network where debugging efforts are subject to faulty inductive reasoning due to symptom scaling.

## 2. The Pathdiag Tool
Consider a user attempting to achieve good performance from an application that relies on bulk TCP data transfers from a remote server, as shown in Figure 1. The user's application client, C, needs data from the application server, S, across a long network path that includes both a short local section and a long-haul backbone. The local section has an RTT of a few milliseconds, while the long-haul backbone can have an RTT of 100 ms (transcontinental) or 300 ms (global).

The user can test the local section of the path and the client configuration by visiting a Pathdiag server, PS, with a Java-enabled browser. Ideally, PS should be located near the connection between the local network and the backbone. The Pathdiag server tests the local path and client configuration and generates a report in the form of a new web page, displayed by the user's browser.

Pathdiag estimates whether the local client and local path are sufficient to meet the target data rate if the backbone were replaced by an ideal network with the same RTT. The user must provide two parameters: the target RTT from C to S and the target data rate for the application. If these parameters are unknown, default values of 90 Mb/s over a 20 ms path are appropriate for most university users. The report presents various metrics of the local client and local path, indicating if they are within the thresholds of TCP performance models and suggesting corrective actions if needed.

### 2.1 The Measurement Engine
The measurement engine collects Web100 data in a series of sample intervals. For each interval, it adjusts the window size of the diagnostic TCP connection in discrete steps and captures the entire set of Web100 variables at the end of each sample. It computes several metrics during each test, the most important being DataRate, LossRate, RTT, and Power (DataRate/RTT). These are shown as functions of the window size for a typical link in Figure 3.

The measurement engine employs an adaptive scanner to select the window size for each sample interval. To minimize the total test time, data is collected in multiple phases that emphasize specific properties of the network. A coarse scan across the entire window range is used to approximately locate two important window sizes: the onset of queuing and the maximum window size. Ranges around these values are then rescanned at progressively higher resolutions. In Figure 3, the fine scans are clearly visible around window sizes of 30 and 80 packets, respectively. The maximum window sizes for scans are determined when TCP congestion control or an end-host limitation prevents the window from rising for three consecutive sample intervals.

Several network path metrics are calculated directly from the raw data as it is collected. MaxDataRate and MinRTT yield a measurement of the test path’s bandwidth-delay product. MaxPowerWindow is the window size with the maximum Power, indicating the onset of queuing. The MaxWindow is the maximum amount of unacknowledged data that the network held. The difference between the MaxWindow and MaxPowerWindow is an estimate of the queue buffer space at the bottleneck.

BackgroundLossRate is calculated from the total packet losses from all sample intervals below the onset of queuing, as indicated by the MaxPowerWindow. It reflects bit errors and other losses not related to network congestion. If the adaptive scans do not provide sufficient loss data, additional loss data is collected at a fixed window size just below the onset of queuing. Generally, the measurement engine collects enough data to observe the loss rate at the scale needed by AIMD congestion control to reach the target window size.

### 2.2 The Analysis Engine
The analysis engine uses the two user-supplied parameters, end-to-end RTT and desired application data rate, to evaluate the results from the measurement engine and produce a diagnostic report, as shown in Figure 4.

## Figures
- **Figure 1.** Canonical Pathdiag setup
- **Figure 2.** Block diagram of the Pathdiag client-server framework
- **Figure 3.** Data rate, loss rate, and RTT vs. window size

---

**Acknowledgment:** This work was supported by the National Science Foundation, Grant ANI–0334061.

**References:**
- [1] M. Claypool and S. Uhlig (Eds.): PAM 2008, LNCS 4979, pp. 152–161, 2008.
- [2] Other references as cited in the text.