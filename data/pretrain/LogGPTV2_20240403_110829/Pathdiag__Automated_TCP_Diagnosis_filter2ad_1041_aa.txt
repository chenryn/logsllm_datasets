title:Pathdiag: Automated TCP Diagnosis
author:Matthew Mathis and
John Heffner and
Peter O'Neil and
Pete Siemsen
Pathdiag: Automated TCP Diagnosis(cid:2)
Matt Mathis1, John Heﬀner1, Peter O’Neil2,3, and Pete Siemsen2
1 Pittsburgh Supercomputing Center
2 National Center for Atmospheric Research
3 Mid-Atlantic Crossroads
Abstract. This paper describes a tool to diagnose network performance
problems commonly aﬀecting TCP-based applications. The tool, path-
diag, runs under a web server framework to provide non-expert network
users with one-click diagnostic testing, tuning support and repair in-
structions. It diagnoses many causes of poor network performance using
Web100 statistics and TCP performance models to overcome the lack of
otherwise identiﬁable symptoms.
1 Introduction
By design, the TCP/IP hourglass [4] hides the details of the network and the
application from each other. This property is critical to the ongoing evolution of
the Internet because it permits applications and the underlying network infras-
tructure to evolve independently. However, it also obscures all network ﬂaws.
Since TCP silently compensates for ﬂaws, for example by retransmitting lost
data, the only symptom of most problems is reduced performance. This “symp-
tom hiding” property was the motivation behind the Web100 project [17], which
developed the TCP extended statistics MIB [16] to expose TCP protocol events
that are normally hidden from the application. A MIB is a formal speciﬁcation
of a set of management variables that can be accessed by SNMP or other lower
overhead mechanisms. Experimental prototypes of the MIB have been imple-
mented in a number of operating systems, including Linux [17] and Microsoft
Windows Vista [23].
Diagnostic eﬀorts are further complicated by another property of TCP: the
symptoms of most ﬂaws scale by the ﬂow’s round-trip time (RTT). Note that
for window-based protocols, performance models generally have an RTT term in
the denominator. For example, insuﬃcient TCP buﬀer space in either the sender
or receiver, or background (non-congested) packet loss all cause TCP to have a
constant average window size and performance that is inversely proportional to
the RTT.
This poorly understood property leads to faulty reasoning about diagnostic
results. A simple throughput test on a short local section of a path with minor
ﬂaws is likely to yield good results. The same test run over a longer path con-
taining the same local ﬂaws is likely to yield poor results. The na¨ıve conclusion
(cid:2) This work was supported by the National Science Foundation, Grant ANI–0334061.
M. Claypool and S. Uhlig (Eds.): PAM 2008, LNCS 4979, pp. 152–161, 2008.
c(cid:2) Springer-Verlag Berlin Heidelberg 2008
Pathdiag: Automated TCP Diagnosis
153
would be that the local section is ﬂawless, and the problem must be present
in the longer path section. This “symptom scaling” property of TCP leads to
incorrect inductive reasoning about ﬂaws, and signiﬁcantly contributes to the
diﬃculty of solving end-to-end Internet performance problems.
This paper describes a tool, pathdiag, that uses TCP performance modeling to
extrapolate the impact of local host and network ﬂaws on applications running
over long paths. The tool analyzes a number of key metrics of the local host
and path and uses TCP performance models to determine thresholds for these
metrics based on the stated application performance goals. Pathdiag reliably
detects ﬂaws that have no user-noticeable symptoms over a short path. It reports
the problems and suggests remedies.
1.1 Motivation
Network performance has increased by an order of magnitude roughly every
four years over the last two decades. Networking experts are usually quick to
demonstrate the full data rate on each new network technology [11]. However,
typical users experience data rates much lower than those seen by experts, and
the gap is widening.
Internet2 has measured the performance of TCP bulk ﬂows over their back-
bone since the beginning of 2002 [12]. As of August 2007, the median performance
across their 10 Gb/s network was only about 3.4 Mb/s. Historical data shows
that this rate has taken six years to double.
A small number of ﬂows get very good performance. About 0.1% are faster
than 100 Mb/s, and of those about half are close to 1 Gb/s. Since the backbone
carries a signiﬁcant number of very high-rate, long-distance ﬂows, we know that
it has to be free from ﬂaws that would otherwise aﬀect these sensitive ﬂows.
The design goal of pathdiag is to help non-expert users attain better per-
formance by easily and accurately diagnosing common ﬂaws. These ﬂaws are
generally near the edge of the network where debugging eﬀorts are subject to
faulty inductive reasoning due to symptom scaling.
2 The Pathdiag Tool
Suppose a user tries to get good performance from an application that relies
on bulk TCP data transfers from a remote server, as shown in Figure 1. The
user’s application client C, needs data from the application server S across a
long network path that includes both a short local section and a long-haul back-
bone. The local section is assumed to have an RTT that is no more than a few
milliseconds. The long-haul backbone can be any length, transcontinental (100
ms RTT) or even global (300 ms RTT).
The user can test the local section of the path and the client conﬁguration by
visiting a pathdiag server, P S, with a java-enabled browser. Ideally, P S would
be located near the connection between the local network and the backbone.
The pathdiag server tests the local path and client conﬁguration and generates
a report in the form of a new web page, displayed by the user’s browser.
154
M. Mathis et al.
Flaw
C:
Client
PS:
Pathdiag server
Campus Network
(< 1ms)
Backbone
(100 ms)
S:
Remote server
Fig. 1. Canonical pathdiag setup
Pathdiag estimates whether the local client and local path is suﬃcient to meet
the target1 data rate if the backbone were replaced by an ideal network with
the same RTT. To do this, the user must provide two parameters: the target
RTT from C to S and the target data rate for the application. If users do not
know these parameters, the default values, 90 Mb/s over a 20 ms path, are
appropriate for most university users. The report presents various metrics of the
local client and local path, and indicates if they are within the thresholds of
TCP performance models. It also suggests corrective action, if needed.
The components of the pathdiag server are shown in Figure 2. The browser
loads the diagnostic client, which communicates with the server via a simple
request-response control protocol. A TCP connection is established from the
traﬃc receiver in the diagnostic client to the traﬃc generator. The measurement
engine uses the Web100 prototype of the extended statistics MIB [16] to manip-
ulate and instrument the TCP connection at the generator. An analysis engine
evaluates the measurements and extrapolates the results to predict the impact
of the local path on the user’s application.2
2.1 The Measurement Engine
The measurement engine collects Web100 data in a series of sample intervals.
For each interval, it adjusts the window size of the diagnostic TCP connection in
discrete steps, and then captures the entire set of Web100 variables at the end of
each sample. It computes several metrics during each test, the most important
of which are DataRate, LossRate, RT T and P ower (DataRate/RT T ). These
are shown as functions of the window size for a typical link in Figure 3. These
plots resemble those generated by “Windowed Ping” (mping) [14], a UDP-based
tool that uses a similar measurement algorithm.
The measurement engine employs an adaptive scanner to select the window
size for each sample interval. To minimize the total time required for the test,
1 We use “target” when referring to components of the remote application and their
parameters, such as end-to-end target RTT, desired target data rate and their prod-
uct, the target window size.
2 To support systems that cannot run a Java–enabled web browser, the “C” source
for a portable command-line client is also published by the diagnostic server.
Pathdiag: Automated TCP Diagnosis
155
Client
Web browser
Java diagnostic
client
HTTP
Server
Web server
Control protocol
Diagnostic server
Parameters
Results
k
r
o
w
e
N
t
Web100
Pathdiag
Measurement
engine
Analysis
engine
Trafﬁc receiver
Trafﬁc
generator
Fig. 2. Block diagram of the pathdiag client-server framework
data is collected in multiple phases that emphasize speciﬁc properties of the
network. A coarse scan across the entire window range is used to approximately
locate two important window sizes: the onset of queuing and the maximum
window size. Ranges around these values are then rescanned at progressively
higher resolutions. In Figure 3, the ﬁne scans can be seen clearly around window
sizes of 30 and 80 packets, respectively. The maximum window sizes for scans
are determined when TCP congestion control or an end-host limitation prevents
the window from rising for three consecutive sample intervals.
Several network path metrics are calculated directly from the raw data as
it is collected. M axDataRate and M inRT T yield a measurement of the test
path’s bandwidth-delay product. M axP owerW indow is the window size with
the maximum P ower, indicating the onset of queuing. The M axW indow is the
maximum amount of unacknowledged data that the network held. The diﬀerence
between the M axW indow and M axP owerW indow is an estimate of the queue
buﬀer space at the bottleneck.
BackgroundLossRate is calculated from the total packet losses from all sam-
ple intervals below the onset of queuing, as indicated by the M axP owerW indow.
It reﬂects bit errors and other losses that are not related to network congestion.
If the adaptive scans do not provide suﬃcient loss data for the test described
in the next section, additional loss data is collected at a ﬁxed window size just
below the onset of queuing. In general, the measurement engine collects enough
data to observe the loss rate at the scale needed by AIMD congestion control to
reach the target window size.
2.2 The Analysis Engine
The analysis engine uses the two user-supplied parameters, end-to-end RTT
and desired application data rate to evaluate the results from the measurement
engine and produce a diagnostic report, as shown in Figure 4.
156
M. Mathis et al.
)
s
/
b
M
t
(
e
a
R
a
a
D
t
 100
 90
 80
 70
 60
 50
 40
 30
 20
 10
 0
 0  10  20  30  40  50  60  70  80  90  100
Window (packets)
)
s
t
n
e
m
g
e
s
(
s
e
s
s
o
l
 2.5
 2
 1.5
 1
 0.5
 0
 0  10  20  30  40  50  60  70  80  90  100
Window (packets)
(a) Data rate vs. window size. Window
sizes less than 30 were too small to ﬁll the
path, so the data rate was proportional
to window size. Window sizes between 30
and 80 packets show data rates that were
near the bottleneck rate, about 94 Mb/s.
(b) Loss rate vs. window size. Above 80
packets, the link started to exhibit persis-
tent loss. Given the small RTT (about 2.5
ms), TCP can recover from these losses
with only a slight reduction in through-
put.
)
s
m
(
T
T
R
 12
 10
 8
 6
 4
 2
 0
 0  10  20  30  40  50  60  70  80  90  100
Window (packets)