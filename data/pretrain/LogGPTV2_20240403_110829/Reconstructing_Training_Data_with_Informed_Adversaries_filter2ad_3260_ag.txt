where we discuss in more detail the role of the released model
hyperparameters, size, initialization, and gradient norm of the
target on the reconstruction MSE.
A. Randomness from Released Model Initialization
Fig. 10: On the MNIST dataset, given a target point z we
train 1K released models with (blue) and without (orange) this
point included in two settings: when each model is initialized
with a new random seed, and when each model has the same
initialization. We plot the distribution of losses on this target
point in these two settings. Clearly, when there is no model
randomization the distributions are perfectly separable and so
membership is easy to infer, while in the random setting, the
distributions nearly perfectly overlap implying membership
may be more difficult.
As observed in Section VI-B, the attack will fail when the
adversary does not have knowledge of the initial parameters
of the released model and so must instantiate each shadow
model used to train the attack with a new seed that controls
the selection of initial parameters. We provide evidence that
it may not be possible to perform a reconstruction attack
in this setting by appealing to a simpler task of inferring
membership, and demonstrating this problem is also difficult
without knowledge of the initial parameters. We instantiate
an informed MIA as described in Section II on the MNIST
dataset. Specifically, given a target point z we train 1K
released models with and without this point included (but
with the same fixed set) in two settings: when each model is
initialized with a new seed (differing initial parameters), and
when each model is initialized with the same seed (identical
initial parameters). In Figure 10, we plot the distribution of
losses on this target point in these two settings. Clearly, when
there is no initial parameter randomization the distributions are
perfectly separable and so membership is easy to infer, while
in the random setting, the distributions nearly perfectly overlap
implying membership may be more difficult, if not impossible.
Note that if released model training was fully deterministic,
the distribution of losses on the target point in the setting with
no random initialization would collapse to a point distribution.
However, all our models are trained with JAX on GPUs that
compile with non-deterministic reductions, introducing a small
source of randomness [69].
B. Transfer Learning from a Reconstructor Network Trained
on a Different Fixed Set
|D-| =
(a) MNIST,
|D′
-| = 10K, leaving a
maximum 49K shadow
models.
|D-| =
(b) CIFAR-10,
|D′
-| = 5K,
leaving a
maximum 49K shadow
models.
Fig. 11: Fine-tuning the reconstruction network for a new
target. The reconstruction network is initially trained to attack
a released model trained with fixed dataset D-, and then fine-
tuned for a new released model trained with fixed dataset
D′
-. Interestingly, the reconstructor network can do zero-shot
learning on MNIST images, despite being trained on entirely
separate data (i.e. D′
- ∩ D- = ∅).
Given a reconstructor network, ϕ, trained to attack released
models of the form θ = AD-(z), can the adversary amortize
the cost training a new ϕ′
that aims to attack a released
- ∩ D- = ∅? On both MNIST
model θ′ = AD′
and CIFAR-10, in Figure 11 we show that fine-tuning the
reconstructor ϕ on only a small number of shadow models
can reach comparative performance to a reconstructor trained
from scratch on substantially more data.
(z), where D′
-
C. Adversary Knowledge of Starting Point: Initialization vs
Near Convergence
Fig. 12: A histogram of MSE for 1K released model targets for
an adversary that observes the initial parameters compared to
first observing a pre-trained released model near convergence.
We also give the NN oracle for reference.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1154
0.760.770.780.790.800.8105101520253035No random initialization0.60.81.01.2051015202530Random initialization0101001K2K4K49KAttack training set size101Average MSEWith fine-tuningWithout fine-tuning01K5K10K49KAttack training set size101Average MSEWith fine-tuningWithout fine-tuning0.000.050.100.150.20MSE0100200300From initializationFrom pre-trainedBy default we assume the adversary knows the initial
released model parameters, motivated by scenarios where the
random seed used to generate initial parameters is made public
or is leaked. Another motivating example is that of federated
learning, where an adversary participates in the learning pro-
tocol. However, in such a setting, it is not guaranteed the
adversary will observe a model at it’s initial state. If the
adversary is only included in the protocol after a sufficient
number of time steps, the state at which they first observe
released model parameters may be close to convergence. Here,
we measure how reconstructions are affected by this subtle
assumption. We pre-train a released model on 10K MNIST
images (this model already achieves > 92% MNIST test set
accuracy), and then following the experimental set-up reported
in Section V on the remaining MNIST data, and compare
to a released model in the standard setting where no pre-
training occurs. Figure 12 shows the MSE for each 1K released
model target in both settings. Clearly there is a difference
in reconstruction fidelity that depends on the step at which
the adversary first observes the released model parameters. A
model that has nearly converged may be less dependent / not
memorize it’s newly seen training data, making reconstructions
more challenging.
D. Visualization of Easy and Hard CIFAR-10 Reconstructions
(a) Smallest MSE
(b) Largest MSE
Fig. 13: Example of the six smallest and size largest MSE
reconstructions for CIFAR-10.
In Figure 13 we show the six reconstructed CIFAR-10
examples with smallest MSE and six examples with largest
MSE out the 1K targets used for evaluation. The easiest targets
to reconstruct correspond to structurally simple images with
a constant background, while the most difficult often have
complex background and color schemes.
E. Reconstructing Against a Released Model Trained with DP
on CIFAR-10
norm of 10, and Gaussian noise is added such that the model
is (ϵ, δ = 10−5)-DP. In Figure 14 we see that again, a large
ϵ in (ϵ, δ)-DP successfully mitigates against reconstruction
attacks while preserving test accuracy in comparison to non-
DP training.
F. Fine-Grained Analysis of CIFAR-10 Reconstructions over
Released Model Training Epochs
Fig. 15: How average MSE increases with the number of
training epochs of the released model for CIFAR-10.
Reconstructing CIFAR-10 images is sensitive to the number
of training epochs of the released model. We perform a fine-
grained analysis to inspect at what epoch the attack becomes
unsuccessful. This can be seen in Figure 15, where we plot
average MSE over 1K released model targets as a function of
the number of training epochs. MSE slowly increases with
number of epochs up until approximately 240-250 epochs,
at which point we observe that “reconstructability” under-
goes a phase transition. Initially, we conjectured this was
due to non-determinism from GPU training increasing the
variance of shadow model parameters for a larger number
of training epochs. However, when we implemented shadow
model training in a deterministic set-up (using TPUs) we
observed no difference in experimental outcomes. We leave
a more in-depth investigation into the relationship between
reconstruction success and number of training epochs for
future work.
G. ReLU Activations in Released Model
Fig. 14: Average MSE of reconstructions and test accuracy of
released model using (ϵ, δ)-DP on the CIFAR-10 dataset.
We perform analogous DP experiments as in Section VI-D
for CIFAR-10. Gradients are clipped to have a maximum ℓ2
Fig. 16: Evidence on CIFAR-10 reconstruction task that ReLU
activations make reconstruction attacks harder. For the target,
z, we plot ∂ℓ(z)
for each layer in the released model θ,
∂θ
throughout training. A large fraction of these gradients are
zero, implying less influence of this additional point on the
trained model, in comparison to other activations that have
non-zero gradients everywhere.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1155
TargetReconstructionTargetReconstruction100101102103104105106CliponlyNoDP-GD0.020.040.06Average MSE2530354045Test accuracy150100150200260Number of training epochs (released model)102Average MSE20406080100Epoch0.00.20.40.60.81.0Fraction of zero gradientsconv_2d_0_wconv_2d_1_wlinear_0_wlogits_wTABLE IV: Experimental setup.
Data
θ, ¯θ
ϕ
A
R
Resolution
Size
Fixed size
Shadow size
Test targets
Type
Architecture
Activations
Parameters
Type
Architecture
Activations
Parameters
Algorithm
Loss
Learning rate
Momentum
Epochs
Algorithm
Loss
Learning rate
Weight decay
Batch size
Epochs
MNIST
28 × 28 (grayscale)
70K
10K
59K
1K
MLP
1-hidden layer, width 10
ELU
8K
MLP
2-hidden layers, width 1K
ReLU
9.7M
GD+Momentum
Cross-entropy
0.2
0.9
100
RMSProp
MAE+MSE
0.001
0
128
100
CIFAR10
32 × 32 (RGB)
60K
5K
54K
1K
CNN
Table V
ELU
55K
Transposed CNN
Table VI
ReLU
226M
GD+Momentum
Cross-entropy
0.01
0.9
100
Adam
+LPIPS+Discriminator
0.0001
0.0001
128
1000
TABLE V: CIFAR-10 released model, θ.
Layer
Convolution
Convolution
Fully connected
Softmax
Parameters
16 filters of 4 × 4, strides 2
32 filters of 4 × 4, strides 1
10 units
10 units
TABLE VI: CIFAR-10 reconstructor network, ϕ.
Layer
Fully connected
Reshape
Transposed convolution
Transposed convolution
Parameters
4096 units
64 × 64
32 filters of 5 × 5, strides 2
3 filters of 5 × 5, strides 2
TABLE VII: CIFAR-10 attack PatchGAN Discriminator
model.
Layer
Convolution
Convolution
Convolution
Convolution
Convolution
Parameters
64 filters of 4 × 4, stride 2
128 filters of 4 × 4, stride 2
256 filters of 4 × 4, stride 2
512 filters of 4 × 4, stride 1
1 filter of 4 × 4, stride 1
TABLE III: Comparison of reconstructions for different re-
leased model activations on MNIST. Please refer to [70] for a
description of each activation function.
Activation
ReLU
max(−0.5, x)
ELU
Sigmoid
Softplus
Swish
Leaky ReLU
Tanh
CELU
SELU
GELU
Identity
Average MSE over 1K test targets
0.0182
0.0096
0.0089
0.0085
0.0083
0.0091
0.0092
0.0086
0.0077
0.0083
0.0088
0.0085
As we saw in Section VI-B, released models with ReLU ac-
tivations tend to be harder to attack in comparison to other ac-
tivation functions with non-zero gradients almost everywhere,
and result in poor quality reconstructions (an MSE larger than
the NN oracle distance). We conjecture that this is caused by
a large fraction of parameters receiving zero gradients at each
step of training, thereby diminishing the mutual information
shared between model parameters and the unknown target
training point. In Figure 16, for each layer of the released
model, we show the fraction of parameters that received zero
gradient when computing the loss of the unknown training
point. Over 80% of the parameters in the convolutional layers
have zero gradients. Additionally, in Table III we compare
reconstructions against released models that employ different
activation functions, and find that ReLU remains the outlier.
Note that we also reconstruct against a released model that
uses a modified version of ReLU that has zero gradient for
x < −0.5, and find that allowing a small negative signal is
enough to reach parity with reconstruction MSE on smooth
activations or activations that contain a non-zero signal almost
everywhere.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:22:08 UTC from IEEE Xplore.  Restrictions apply. 
1156