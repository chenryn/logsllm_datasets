for each object in the cache, binned by the last accessed time by the hour. Here
Fig. 5. Temporal hit-rate view of caches from various regions, which show a histogram
of cache-hits binned by age. The popularity consistently decreases for all regions.
Caching the Internet: A View from a Global Multi-tenant CDN
73
the x-axis is the last access age, indicating how many days prior to the snapshot
the item was last accessed. Here we see that the most recently accessed content
is indeed the most popular, by nearly three orders of magnitude. This follows our
intuition about web content accesses and suggests that recency will likely be an
important input into the caching systems. Furthermore, we see similar patterns
across all geographic regions and long time scales, suggesting this behavior is
common to diﬀerent PoPs and time frames.
This access behavior indicates the importance of recency when considering
any caching policy. Indeed, any policy that can keep the freshest objects in the
cache will be able to serve the most requests. Furthermore, the consistency of
this behavior, where we otherwise saw signiﬁcant variations in request size and
pattern, provides the foundation of our expectations in the subsequent section:
recency based algorithms that are ﬂexible to request type are likely to do well.
4 Cache Evaluation Framework
Our analysis is based on a caching emulator designed to facilitate the assessment
of arbitrary cache policies1. In particular, it was designed to consume CDN cache
server access logs and closely match the behavior of the production cache. The
emulator also allows pre-population of its cache with contents of a production
server and enables the tracking and statistics collection of cache data. We empha-
size that this system emulates cache behaviors rather than estimating using a
simple model: since the system relies on observed access logs, it behaves as a
production implementation would (Fig. 6).
Fig. 6. Data ﬂow diagram of the emulator. Each client request passes through a stack
of arbitrarily conﬁgured caches.
The emulator models the ﬂow of requests through a series of tiered caches.
When a request is processed, it checks the ﬁrst cache. If the object is present, the
request is labeled a hit and the object is “returned” from that cache. Otherwise,
it is labeled a miss and passed to the next layer. These layers can be other
arbitrary caches or can be treated as an external origin. Each abstract cache
layer is provided with admission and eviction policies. The admission policy
1 Available at https://github.com/VerizonDigital/edgecast caching emulator.
74
M. Flores and H. Bedi
determines which requested objects are cached at that level, and the eviction
policy describes which objects to remove from the cache when it is full. Each
layer tracks a relevant statistics: including the hit rate and the bytes written.
In this study, we consider 30 days of access logs from a set of cache servers
from the geographic regions shown in the previous section. Each log entry repre-
sents a client request that was handled by a single server in a PoP. Each log entry
contains the timestamp when the request was logged, the size of the requested
asset, the status code returned from the back-end system (i.e. a cache hit or
miss), the bytes delivered to the client (which will be less than the asset size in
cases of range requests), as well as the url of the asset requested. This informa-
tion allows us to conduct a thorough study on the behaviors observed directly
in the trace, as well as enabling us to replay this traﬃc in the emulator. Doing
so allows us to examine what-if scenarios in which we employ alternative cache
policies and mechanisms on real-world access behaviors.
5 Evaluation
Here we provide an analysis of various caching techniques using the above frame-
work. We explore the implementation of caching at: the disk (storage on the scale
of terabytes), and at a load balancer (gigabytes). We examine each of these in the
context of the constraints of the network described in Sect. 3. Table 1 provides
an overview of the policies we examine along with a brief description.
Table 1. Cache policies examined in this study.
Policy
Type
Eviction FIFO
LRU
COST
S4LRU
Inﬁnite
Admission N -Hit
Description
A simple First In First Out queue
Least recently used
LRU based, size and recency weighted equally
Quadruply segmented LRU [22]
No eviction (i.e., unlimited cache)
Admit on N th request
Probabilistic (Pr) Admit with ﬁxed probability
Prob-Size (Pr.Size) Admit with probability dependent on the ﬁle size [6]
We focus ﬁrst on cache eviction, the process of determining which objects to
remove from the cache when it becomes full. We begin with FIFO, as it’s gen-
erally the simplest to implement and widely used in industry. Next, we examine
LRU, as it is a robust and standard caching algorithm, and our analysis in the
previous section suggests the asset request patterns have clear recency properties.
We further examine a method similar to Greedy-Dual-Size [11] which computes
an eviction score which grants equal weight to frequency and ﬁle size. Finally,
Caching the Internet: A View from a Global Multi-tenant CDN
75
we examine S4LRU, as it provides a relatively direct extension of LRU, and has
been shown to perform well in other web-object caching environments [22].
In examining cache admission, we present an examination of N-Hit Caching,
a bloom ﬁlter based approach that produces deterministic output and has been
shown eﬀective in industry [28,31]. We further examine a commonly considered
alternative that admits objects with a ﬁxed probability, and a methodology
which uses a size-based probabilistic admission [6].
5.1 Cache Eviction
Here, we examine disk eviction policies: FIFO, LRU, COST, and S4LRU [22].
Here, FIFO presents the obvious simplest solution, followed closely by LRU.
COST is a variant of LRU in which a cost is computed for each object that lin-
early weights ﬁle size and recency. The lowest scores (i.e. intuitively the largest
and oldest ﬁles) are then evicted ﬁrst. The ﬁnal policy, S4LRU, consists of 4 LRU
“queues”. On a cache miss, an object is inserted into the ﬁrst queue. On subse-
quent hits, it’s promoted to the head of next queue. If it’s in the ﬁnal queue, it is
simply moved to the head of that queue. Each queue then works as an indepen-
dent LRU cache. When the object is evicted, it goes to the head of the previous
queue. If that queue is the ﬁrst, it is evicted entirely. This process essentially
encodes frequency into an LRU-like structure. In all of the experiments in this
section, we use the default admission policy, which admits all objects into the
cache.
First, we examine the most straightforward metric: hit-rate. Indeed, the hit-
rate is a fundamental measure of how well the cache is performing, and in many
instances will correlate directly with the CDN’s ability to respond with a low
response time. Here, we consider the performance of the algorithms over various
disk sizes: for each algorithm and disk size, we play back 7 days worth of cache
accesses, accounting for the majority of the regular diurnal patterns2. We further
consider the performance of an inﬁnite cache, which represents the optimal hit
rate without knowledge of the future.
Fig. 7. Hit-rates of eviction algorithms.
The horizontal line shows the hit-rate of
an inﬁnite cache.
Fig. 8. Disk writes for each eviction
algorithm.
2 We observed similar results when using the full 30 days of logs.
76
M. Flores and H. Bedi
Figure 7 presents the results of these experiments. First, we note the obvious
increase in hit rate as the disk increases in size: with a larger disk we are able
to respond from the cache more often. We also note that the performance of the
algorithms becomes more similar with a larger disk, suggesting that the marginal
impact from our choice of algorithm is reduced. In particular, we note that at
large enough disks, traditional LRU performs quite well, approaching the hit
rate of the inﬁnite cache of 97.5%. We see similar behavior from the byte-hit
rate, but refrain from showing here due to space constraints.
The hit-rate alone, however, fails to show the whole picture: there are addi-
tional considerations when using each of these algorithms. In particular, the
load induced via the write operations that must be performed, which may have
an adverse eﬀect on the underlying hardware (e.g. solid state disks). Next we
examine the disk write behavior of each policy.
Figure 8 shows the total disk writes (log scale) achieved for each disk size.
The disk size has a sizeable impact on the total volume of writes, with the
smallest disks incurring total write costs on the order of petabytes, larger disks
requiring only 10 s of terabytes. Beyond this, we see that FIFO performs con-
sistently worse than the LRU-based approaches, uniformly requiring additional
disk writes, about 60% more in the 4 TB case. High write volume puts greater
load on the underlying hardware, straining its performance and reducing over-
all lifetimes. Content which has to be written out to disk must also be fetched
externally, causing greater delay in the delivery to the end-user.
While all 4 algorithms appear to perform relatively well at large enough
disks (within 1% above the 4 TB level), there are still potentially other costs, in
particular additional disk writes, in the case of FIFO. Among the 3 LRU based
policies, their similar performance makes vanilla LRU particularly appealing, as
it is the least expensive in terms of complexity and management.
5.2 Cache Eviction with Selective Admission
Despite the generally good behavior of LRU, there are some behaviors in CDN
web traﬃc which can poison attempts at maintaining a healthy cache with an
eviction policy alone. In particular, we recall from Fig. 4 that many ﬁles are only
requested a single time, creating pressure on the cache, and in particular the
storage medium, for ﬁles that will never be accessed from the cache. However,
we further recall that the most popular ﬁles were requested extremely frequently.
We therefore also consider the use of a cache admission policy that can alleviate
the underlying amount of writes a cache disk will need to do, reducing hardware
load and overall cache churn.
First, we consider a bloom ﬁlter placed in front of the disk cache, implementing
a technique we call second hit caching (2-Hit) [28,31,36]. The process is simple:
on a miss, if the appropriate hash of a requested item is not in the bloom ﬁlter,
it is added to the ﬁlter but not cached. If, on the other hand, it is in the bloom
ﬁlter, the object is added to the cache. In this way we are able to avoid caching
objects which are requested only a single time. Very popular items, however, are
still quickly pulled into the disk cache, minimizing negative impact.
Caching the Internet: A View from a Global Multi-tenant CDN
77
c
−size
We further consider two alternative admission policies: a probabilistic admis-
sion which caches objects with a ﬁxed probability of p (which we refer to as Pr.),
and a size based probabilistic policy which admits objects of size bytes with prob-
[6] (Pr.Size). In both cases, the intuition is that popular items will
ability e
be requested frequently, increasing the likelihood that they make it into the disk
cache. In the size based methodology, the system biases towards objects which
are smaller than c, capturing the risk of allowing very large objects into the
cache. In our evaluations we consider a range of values for p, from .25 to .75, and
c, from 100 MB to 1 GB. In this section we further consider each of these three
policies when combined with the 3 eviction policies described in the previous
section on a 4 TB cache disk.
(a) Hit rates and reduction in
disk writes.
(b) Hit rates and origin reads.
(c) Disk writes for FIFO and
LRU with and without 2-Hit.
(d) Origin reads for viable poli-
cies.
Fig. 9. Impact on hit-rate, disk writes and origin reads by LRU, FIFO and S4LRU
with selective admission: 2-Hit (N-hit, where N=2), Pr., and Pr.Size.
Figure 9a shows the hit rate achieved by each combination of policies and the
relative improvement to disk writes (i.e. the percentage reduction in disk writes
versus using no admission policy with the same eviction policy). The hit rates
range from 92 to 97%. Furthermore, some of the policies, in particular the Pr.Size
approaches, show signiﬁcant reductions in disk writes. The smaller probabilistic
and second hit showed modest improvements to disk writes, between 10 and
33%.
Figure 9b shows the impact on hit-rate versus the absolute origin reads (i.e.
the bytes that had to be fetched from the customer origin). Here we see that
the disk writes were an insuﬃcient view: the Pr.Size methodologies signiﬁcantly