round[ 1
d
TTV(Dij ∗ Pij ∗ Qij)].
Similar to δDTW , δTT V can be used directly for decision making
or as a synthesized feature to be fused with other features. The
473Global Feature Analysis and Comparative Evaluation of Freestyle In-Air-Handwriting Passcode for User Authentication
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
is generally smaller than those generated by different users writing
the same content (c = collision) or by writing different contents (c =
diff ). This is expected. There is an overlap in a region (shown as
the dashed horizontal lines), which determines the two parameters
th1 and th2. Generally, below th1, it is desired that
d
j D′
d
j D′
p( 1
d
ij|c = same) > p( 1
d
and above th2, it is desired that
ij|c = diff) > p( 1
p( 1
d
d
d
j D′
d
j D′
ij|c = collision),
ij|c = collision).
From Figure 4 and 5, it can be observed that samples in different
sensor axes and at different time should be considered with different
importance (i.e., using different weight). Also, in Figure 5, it can
be observed that the distribution of sample differences does not
change along the time given a large number of accounts with diverse
passcode contents. However, the local non-matches of signals from
different accounts vary in different ways. Hence, a per-account
decision-making algorithm can perform better than a global one,
explaining the performance gain detailed in section 7. Additionally,
there are small anomalies on both ends since users usually raise
their hands to start the writing and put down the hand to terminate
the writing. Hence, the performance can be improved slightly by
trimming the start and the end again after alignment.
4.3 Statistical Feature Analysis
In this section, we investigate those statistical aspects of the signal
which are lost in the alignment and normalization steps. Specifically,
the following statistical features are studied.
1) The mean of each sensor axis across time M = (µ1, ..., µd),
where µj = mean(R1j , ..., Rl j).
2) The standard deviation of each sensor axis across time Σ =
(σ1, ..., σd), where σj = std(R1j , ..., Rl j).
3) The correlation coefficients between pairs of adjacent sensor
axes P = (αxy , αyz , αxz , βxy , βyz , βxz , ...), where αxy , αyz , αxz are
the correlation of the three axes of position, βxy , βyz , βxz are the
correlation of the three axes of speed, and so on. Here,

αxy =
i(Rix − µx)(Riy − µy)
.
σx σy
where λj =l
i =1 |Rij|.
4) Sum of amplitude of each axis across time Λ = (λ1, ..., λd),
5) Portion of low frequency components L = (ξ1, ..., ξd), where
ξj = mean(low_pass(FFT((R1j , R2j , ..., Rl j)))). The FFT() function
computes the Fast Fourier transform coefficients on the samples of
a specific sensor axis. The low_pass() function zeros the coefficients
higher than 3 Hz.
Collectively, the statistical feature vector of a signal R is the
combination of them, defined as f (R) = (M, Σ, P, Λ, L), where f (R)
is the statistical feature extractor. In total, there are 18 * 5 individual
elements. Some of the features are used in our previous work [21]
for only one type of sensor.
Template Generation: Given {R(1), R(2), ..., R(m)} as the m sig-
nals at registration for an account, first run the prepossessing step 1
to 3 for each signal, extract statistical features, then run the prepro-
cessing step 4 and 5, and finally calculate the element-wise mean
Figure 6: Distribution of statistical difference for different
classes. The sequence of sensor axes within a specific type
of statistical features is the same as Figure 4. Subplot (d) is
obtained using the camera device.
µS F and standard deviation σ S F as templates:
(2)), ..., f (R
(2)), ..., f (R
µS F = mean(f (R
σ S F = std(f (R
(1)), f (R
(1)), f (R
(m))),
(m))).
Matching: Given an account with template µS F and σ S F , and an
authentication request signal R, run the same preprocessing steps
as those in the template generation, then calculate the following
aggregated statistical difference score:
δSD = mean(abs(f (R) − µS F)/σ S F),
where the subtraction and division are all element-wise, the mean()
function averages all elements of its vector input. The idea behind
this statistical difference is the same as the Mahalanobis distance.
The distribution of each component of the statistical difference
vector abs(f (R)− µS F)/σ S F for different classes is shown in Figure
6 (a), (b) and (c). Also, the distribution of δSD is shown in Figure 6
(d). They are generally weak features, especially the mean of most
sensor axes, such as the speed and acceleration, because many of
them tend to be around zero. Additionally, signal variance, am-
plitude, and low frequency components are strongly correlated,
because all these three features will have a larger magnitude if the
user writes intensively. As a result, we argue that removing them
in the preprocessing step does not incur much loss of information
for user authentication.
We incorporate two additional statistical features as discussed
in our previous work [19] using only one type of sensor. The first
one is the difference of lengths between a signal R and a template
T , denoted as δLD. It is calculated as δLD = |lR − lT |/lT , where lR
and lT are the lengths of the signal R and template T respectively.
The second one is the hand geometry difference, denoted as δHGD.
It is the average length difference of finger bone segments in ratio.
It is computed as δHGD = mean(abs(hR − hT )/hT ), where hR and
hT are the vectors consisting of the length of finger bone segments
measured by the camera device. The glove device cannot obtain this
feature. More details about the hand geometry feature are provided
in our previous work [19]. Similar to the alignment cost, these two
features are weak but they can be used in the fusion process.
5 AUTHENTICATION ALGORITHM
We propose two authentication algorithms based on our feature
analysis: Score-Fusion (section 5.1) and Feature-Fusion (section 5.2).
474ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Duo Lu, Yuli Deng, and Dijiang Huang
5.1 Score Fusion
The Score Fusion (S-Fusion) scheme uses a weighted average of the
matching scores on each type of feature.
Template Generation: Given the registration signals, generate
templates for the temporal, statistical, and hand geometry features.
Matching: Run the matching algorithm of each feature and
calculate the score:
δother = w1δAC + w2δSD + w3δLD + w4δHGD ,
δS−Fusion = δTT V + δother ,
where δTT V is the matching score using the TTV algorithm (i.e.,
the temporal distance), δAC is the alignment cost score, δSD is the
statistical difference score, δHGD is the hand geometry difference
score, w1 to w4 are tunable parameters (in our case, w1 = 0.1, w2 =
0.03, w3 = 0.3, w4 = 0.4). δTT V is considered as the primary feature
with strong discriminative capability, and the others are supplemen-
tal features. The glove device is unable to measure hand geometry
and w4 is always zero.
These weights w1 to w4 are determined empirically. In our case,
as shown in Figure 7, we first plot the supplemental scores con-
cerning the primary score with 1,000 signals in each class and then
manually draw a linear decision boundary. The weights are com-
puted by the inverse of the slopes of these lines. This is designed
mainly for simplicity, and it is not necessarily optimal. More so-
phisticated score fusion schemes [22] [9] can be employed with a
density estimation of the scores to improve performance.
5.2 Feature Fusion
If we inspect each individual element-wise distance Dij, it is essen-
tially a type of weak feature. Inspired by the idea of the ensemble
of weak features using a weighted average, we design a Feature
Fusion (F-Fusion) scheme as follows.
Template Generation: Same as Score Fusion.
Matching: Align the signal R to T and calculate the score:
l
d
i =1
j=1
δT−Fusion = b +
wij Dij ,
δF−Fusion = δT−Fusion + δother .
In this scheme, each feature has its own weight wij, and since
they vary differently, the wij should be calculated based on the
data for each account. b is a bias to keep the score between 0 and 1.
DTW matching is a special case of T-Fusion by setting all weights
to the same value. TTV sets the weights systematically based on a
few hyperparameters with additional nonlinear cutoff. Determining
the weights wij is essentially fitting a decision plane in the feature
space, and this can be done for each account independently. In our
case, these weights wij as well as w1 to w4 are learned from the data
at registration using a soft margin Support Vector Machine (SVM) in
a per account manner. Specifically, at registration, for each account,
we use the Dij calculated by five signals generated by the account
owner as positive samples, and we use the Dij calculated by 1,000
signals randomly selected from other accounts as negative samples
to train an SVM. One limitation of this method is that at registration,
data from all accounts must be accessed. This is possible for an
online application with many accounts but infeasible for device
unlock scenarios with only one or a few users.
6 EXPERIMENTAL EVALUATION
We build a prototype system and evaluate all matching algorithms
mentioned in previous sections with the datasets and protocol
detailed in Section 3.3. Due to the imbalance of positive and negative
testing data, the main performance metrics are False Accept Rate
(FAR) and False Reject Rate (FRR). Specifically, FAR = #{FP} / (#{FP}
+ #{TP}), and FRR = #{FN} / (#{FN} + #{TN}), where #{} means the
number of FP, TP, FN, or TN. FAR and FRR can change by varying
the decision threshold, and Equal Error Rate (EER) is the value
where FRR is equal to FAR. The results are shown in Table 1. Here
FAR1K and FAR10K indicate the FRR when FAR is 10−3 and 10−4,
respectively. ZeroFAR is the smallest FRR when FAR is zero, and
ZeroFRR is the smallest FAR when FRR is zero. In the table, the
row DTW(2) and TTV(2) show the results using only the first two
signals at registration for template construction. We also show the
modified Receiver Operating Characteristic (ROC) curves in Figure
8 by plotting the FAR against the FRR instead of plotting the True
Accept Rate (TAR) against the FRR. Compared to the original ROC
curves, these modified ROC curves look better in log-log scale.
These matching algorithms have a gradual improvement in per-
formance with an increase in the number of parameters. The DTW
algorithm is the simplest, with no need for parameter tuning. The
TTV algorithm requires to determine four parameters (p, q, th1, th2),
and the S-Fusion algorithm requires another four (w1 to w4), which
can be set up with a small pilot dataset or with rule-of-the-thumb
values provided in this paper. These methods can also work with
only one or a few accounts in the account database, e.g., scenarios
such as unlocking a device. In T-Fusion and F-Fusion, the num-
bers of parameters are roughly the same as an additional template,
and they are learned from data in the account database. These two
methods can be used in online login.
The limitation of all proposed methods is essentially the quality
of the template, i.e., it is difficult to use one template generated
from the limited number of signals at registration to capture the
whole picture of the user’s handwriting behavior variation. To
mitigate this limitation, we conduct two additional experiments,
and the results are shown in the last two rows of Table 1. For T-
Fusion(A), we augment the five registration signals to 125 signals by
performing a slight random rotation, adding a small perturbation,
and swapping a random segment given a randomly picked pair of
training signals. Then, we use the augmented signals to train the
SVM. The idea is to artificially create signal variation so that the
learned model may be more robust. For T-Fusion(E), we use each of
the five registration signals as a template, train five individual SVM
models. We take the minimum of the five scores generated by the
five models as the ensemble score during the testing. The idea is
to use multiple templates instead of one. This idea of matching the
login request to each of the registration signals and choosing the
best result can also be applied to the DTW method and the TTV
method. These methods show further improvements.
A comparison to existing works using in-air-handwriting is
shown in Table 2. Their performance cannot be compared directly
because each work has its own dataset collected with a different
type of device and in an environment setting unique to that work.
Moreover, their datasets are not openly available. We list them in
this paper as a general reference. Readers should also be careful
475Global Feature Analysis and Comparative Evaluation of Freestyle In-Air-Handwriting Passcode for User Authentication
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Figure 7: Distribution of the scores from various features.
Figure 8: Modified Receiver Operating Characteristic (ROC).
Table 1: Authentication performance results.
using data from the camera device
using data from the glove device
method
DTW(2)
TTV(2)
DTW
TTV
S-Fusion
T-Fusion
F-Fusion
T-Fusion(A)
T-Fusion(E)
EER
1.24
1.00
0.81
0.70
0.50
0.22
0.26
0.21
0.10
without collision (in %)
Zero
FAR
FAR
1K
71.40
5.75
4.81
44.57
56.36
2.39
23.64
2.38
37.75
1.38
12.33
0.32
0.31
12.00
0.23
10.62
10.70
0.12
FAR
10K
17.69
12.18
7.56
7.99
4.95
0.78
0.85
0.57
0.78
Zero
FRR
28.76
16.49
25.00
15.54
42.38
7.35
7.30
3.23
4.72
with collision (in %)