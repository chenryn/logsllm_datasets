within a week (Figure 1(c)). For example, we select a fiber
where each link on this fiber (i.e., optical channel) has a high
enough SNR to make all capacity denominations feasible
over the period of 3 years. We then analyze the number of
failures that links would undergo if they were modulated
with higher but static capacities. Figure 2(a) shows that links
on this fiber do not see significant increase in the number of
3
failures as the capacity is increased up to 175 Gbps. But
some of the links would observe a 100 failures if driven at
200 Gbps. This behavior is repeated with other fibers but
depending on the number of links, fiber length, technology,
and age of equipment, the point at which the failures start to
increase is different for each fiber and each channel on the
fiber. Hence, it is impossible to select a one-size-fits-all static
capacity that is higher than 100 Gbps.
Furthermore, we characterize the duration of link failures.
Figure 2(b) plots the duration of link failures for the different
modulated capacities (based on the link’s average SNR). We
observe that such failure events last several hours which is
unacceptable. This impact on reliability underscores the need
to adapt capacity in the face of SNR drops, if we want to
tighten the gap between required and observed SNR values.
2.2 Complete loss-of-light is uncommon
Today, when the SNR of a link’s optical signal drops below
its pre-determined threshold, the link is declared down. How-
ever, we show that not all failures are complete loss-of-light.
SNR drops may be caused by planned maintenance work
(e.g., a line card replacement) or unplanned events (e.g., fiber
cut, hardware failure, human error). While some of these
impairments make the link unusable (e.g. fiber cuts), others
may simply lower the signal quality (e.g. degradation of an
amplifier) without completely shutting off the signal. Links
undergoing failures due to lowered signal quality can still be
used to send traffic at a reduced rate, highlighting another
opportunity to improve link availability.
We quantify this opportunity by manually analyzing seven
months of failure tickets (250 events) reported by WAN field
operators, to understand their root causes. We identify five
types of root cause: (i) Power failure: This indicates a power
line failure causing a complete loss-of-light; (ii) Fiber cuts:
100 Gbps150 Gbps200 Gbps125 Gbps175 Gbps0.000.250.500.751.000246810121416Average SNRCDF(a)
(b)
Figure 2: (a) Number of link failures for 40 links (one color per link) for
a given capacity. For this particular fiber, while increasing capacity up
to 175 Gbps does not increase link failure events, achieving 200 Gbps
capacity comes at the cost of increased link failures. (b) Duration of
failures if WAN links operate at a given capacity.
This is an accidental break in an optical fiber (mostly due
to construction projects) that leads to a complete loss-of-
light; (iii) Hardware failure: This refers to a malfunction of
optical hardware, such as degrading amplifiers, transponders,
or optical cross connects; (iv) Human error: In this category,
failures are mostly due to humans inadvertently disturbing an
operational fiber while working on another; (v) Other: This
category refers to all other cases with an unidentified root
cause.
We analyze the contribution of each root cause to total
outage duration (Figure 3(a)). Fiber cuts and power failures
make up 10% of the total outage durations, highlighting that
complete outage of fibers is relatively rare. Collateral damage
by human operators represents 20% of the total outage time.
Similar observations have been made in a previous study [13].
The remaining categories are hardware failure and other
undocumented failures. Failures with undocumented causes
occur when technicians do not log the exact action taken, but
we know they were not instances of fiber cuts.
Even current failures could present an opportunity to run
links at reduced rates. To narrow down the opportunity area,
we record the lowest SNR of failure events (when the SNR
falls below the 100 Gbps threshold which is 6.5 dB). Fig-
ure 3(b) shows the distribution of SNR values at link failures.
A minimum SNR of above 3.0dB, which is enough to drive a
link at 50 Gbps capacity, is observed in 25% of the failures.
Therefore, even with conservative 100 Gbps capacity, 25% of
link failures could have been avoided by driving the impacted
links at 50 Gbps, highlighting the improvement in availability
offered by dynamic capacity links.
3 DYNAMIC CAPACITY LINKS
Our characterization of optical links suggests that links are
currently operating well below their potential transmission
4
(a)
(b)
Figure 3: (a) Categorization of failure root causes in terms of duration.
Contrary to common belief, fiber cuts are not the major root cause of
failures in WANs. Unplanned events during planned maintenance or
hardware failures are more probable. (b) Distribution of the lowest SNR
values when a link failure event happened. The lowest SNR is above
3.0dB (which is sufficient to drive a link at 50 Gbps) 25% of the time.
rates, based on SNR values. However, operating links at
constant transmission rates closer to the observed SNR
increases the likelihood of link failures. To balance this
trade-off, we propose dynamic adjustment of physical link
capacities in centrally controlled wide area networks by
changing the modulation format of optical signals. These
choices are motivated by the latest hardware and software
developments in the industry:
Adapting bit-rates by modulation change.
Recent
advances in the development of bandwidth variable
transceivers (BVTs) enable our goal of reclaiming lost
network throughput due to over-provisioning and improving
availability by decreasing transmission rates in the face of
low SNR (vs. incurring a link failure). State-of-the-art BVTs
are capable of modulating signals on the fiber with three
different formats: 16QAM, 8QAM and QPSK. All other
factors being constant, signals in 16QAM format can carry
traffic at 200 Gbps, 8QAM can carry 150 Gbps and QPSK
can carry 100 Gbps. However, these transceivers were
designed with the assumption that operators would make a
one-time choice of modulation format.
This is reflected in the latency incurred in changing the
modulation of ports on modern Arista 7504 switches. In our
experiments, we find that on average, changing the modula-
tion of a port incurs a latency of over 1 minute. During this
time, the link undergoing the modulation change is down and
cannot carry traffic. This is because of the assumption that
the modulation change is a one-time event. To benchmark the
reconfiguration latency, we experiment with a transceiver
evaluation board and investigate ways of reducing capacity
reconfiguration time (Section 7.1). However, we note that it
will take some engineering effort to make hitless capacity
llllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll110100100125150175200Capacity (Gbps)Number of link failures05101520100125150175200Link Bandwidth (Gbps)Duration of link failure(hours)0102030405060Duration (%)Binary failuresOpportunity area0.000.250.500.751.000246Lowest SNR at link failure eventCDF[15, 16, 20]
change production ready. We discuss intermediate routing
states to handle link flaps with low disruption in Section 7.1.
Software Driven WANs. Effective utilization of network in-
frastructure in modern WANs is enabled by software-driven
centralized traffic engineering (TE)
that
maximizes the network flow for changing demand matrices.
Therefore, we consider the implementation of dynamic
capacity links in such networks. TE controllers are
consumers of network link capacities, as they make decisions
about routing flows along the best paths with available
capacity. Had link capacity reconfiguration been a hitless
phenomenon, existing TE controllers could function largely
unmodified with dynamic capacity links. However, capacity
reconfiguration is expensive, as it causes a link outage lasting
for over a minute. We discuss the impact of this additional
constraint on TE controllers in the next section.
4 TRAFFIC ENGINEERING WITH
DYNAMIC CAPACITY LINKS
In a network with dynamic capacity links, the state of the
network in each run of the TE optimization algorithm is
dependent on the links’ underlying SNR. Therefore, TE con-
trollers must be modified to gather the SNR of all links in the
network and treat the link capacities as variables. We propose
the RADWAN centralized TE controller which can leverage
dynamic capacity links to achieve higher network throughput
and availability. RADWAN handles a spike in the demand
matrix by upgrading the capacities of one or more links.
However, state-of-the-art bandwidth variable transceivers
(BVTs) require over a minute to change the capacity of a link
(Section 7.1), rendering the link unusable for that period. In
response to this link flap, existing traffic flows must be
migrated away from the link undergoing capacity reconfigu-
ration. Such flow migrations can cause transient congestion
in the network and, hence, must be done minimally.
Therefore, we argue that in a network composed of
dynamic capacity links, the objective of traffic engineering
changes from simply maximizing the network throughput to
maximizing throughput while minimizing churn caused by
link capacity reconfigurations. In Section 4.1 we discuss how
network churn can be quantified to achieve low disruption
while meeting traffic demands with link capacity
reconfiguration.
4.1 Quantifying network churn
Present day hardware does not support hitless capacity
changes; therefore, we propose to deal with churn induced by
link capacity changes in software. As a first step towards this,
we introduce a definition of churn induced by a link capacity
change in terms of the rate of traffic on the link. The capacity
change (either an increase to meet demands or a decrease due
to lowered signal quality) of link l carrying fl units of traffic
will displace the fl units. Displacement of large flows is
more likely to cause transient congestion as opposed to
smaller flows. Therefore, we define churn induced by the
capacity change of link l as:
churn(l) = fl
(1)
The overall churn induced by capacity changes in a
network, C, is a summation of the churn from each link
undergoing capacity change:
C =
churn(l)
(2)
links
We note that this is one of the many possible ways to
define the churn caused by link flaps in the network. We
encourage practitioners to consider other definitions which
deter inducing churn into preferred traffic classes (e.g.,
interactive traffic over background traffic).
4.2 Computing flow allocations
When computing allocations of flows along different paths
in a network composed of dynamic capacity links, the goal
of RADWAN is to maximize the network utilization (as was
the case with earlier work [15, 16, 20]) while keeping churn
due to capacity reconfigurations minimal. In this section, we
formulate this goal as a constrained optimization problem
using the definition of churn from Section 4.1. RADWAN
periodically evaluates the optimization goal to assign traffic
flows along network paths. In each round of its operation,
RADWAN has access to attributes of the network state which
serve as input to the optimization problem. We now describe
various elements of the RADWAN controller.
Inputs. Traditional TE controllers take as input
the
network topology and traffic demand matrix to compute
allocations of flows along label switched network paths. In
addition to these, our controller requires SNR measurements
for all physical links in the network. Using this information,
the controller derives the potential capacity of each link, over
its existing capacity.1 Implicitly, the controller is also aware
of the existing flow on all links in the network, assigned in
the previous round of controller operation.
Allocation Objective. Algorithm 1 describes
the
optimization goal of RADWAN. At its core, the optimization
is a modified multi-commodity flow that maximizes overall
throughput of the network while augmenting link capacities
minimally. The optimization variables bi, j specify the
allocation of flow i along path j in the network. Allocation of
1Even if there is potential to increase a link’s capacity by, say, 50 Gbps, the
controller must do an upgrade only if this extra capacity is needed to meet
traffic demands.
5
Algorithm 1: Traffic Engineering Optimization
1 Inputs:
2
3
4
5
6
7
8 Outputs:
9
10
di: flow demands for source destination pair i
cl : capacity of each link l
pl : potential capacity increase of each link l