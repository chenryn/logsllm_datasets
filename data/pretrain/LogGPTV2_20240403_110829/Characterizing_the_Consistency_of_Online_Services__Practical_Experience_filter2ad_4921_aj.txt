(a) Get Operation
(b) Insert Operation
Figure 4.9: Communication overhead in Facebook
Considering the latency values observed in diﬀerent locations reported in Fig-
ure 4.8b, we can observe the same pattern previously observed, where the la-
tency experienced by clients in Oregon is lower compared with the remaining
locations. This is expected, since this can be explained by the latency experi-
enced by the client to contact the Facebook service in that concrete location
when compared with the remaining locations used in our experimental work.
4.5.1.2 Communication Overhead
We now study the communication overhead imposed by our Middleware by
observing the average size of messages exchanged between clients and the ser-
vice. Figure 4.9 reports these results for each of the session guarantees and for
their combination, compared with the use of the library without our Middle-
ware, for both get and insert operations. The results in Figure 4.9a show that
the overhead introduced by our Middleware is noticeable for get operations
when Writes Follow Reads and the combination of all session guarantees are
enforced. This happens because most of the payload in these messages are the
multiple elements of the list that are returned, and in these cases each element
contains the explicit dependencies. Note that each element also contains the
metadata that Facebook associates to each post.
The same pattern occurs for the insert operations, as reported in Figure 4.9b.
In this case, each message contains only a single element to be added, the in-
crease in message size is quite noticeable when the Middleware is enforcing
77
CHAPTER 4. FINE-GRAINED CONSISTENCY FOR ONLINE SERVICES
Figure 4.10: Local storage overhead for Facebook
Writes Follow Reads and the combination of all session guarantees. This hap-
pens due to the cost of sending the explicit dependencies of each inserted
element, which can account to 25 unique element identiﬁers and their times-
tamps. The remaining session guarantees, in contrast, have a modest overhead
of only a few tens of bytes.
4.5.1.3 Local Storage Size
Finally, Figure 4.10 reports the storage cost in terms of elements stored lo-
cally by our Middleware for enforcing each of the session guarantees and their
combination. For completeness, we also provide the results for the None
conﬁguration, which, as expected, is zero. This is used as a sanity check for
our results. Monotonic Writes do not require any form of local storage, and
therefore have no local storage overhead. In contrast, the remaining session
guarantees do exhibit some low storage overhead due to their need to maintain
elements stored in the insertSet and localView data structures. As expected,
when providing all of the session guarantees the local storage has more entries,
leading to additional overhead. This happens because the number of entries is
the sum of the elements in the insertSet and in the localView.
4.5.2 Redis Results
We also conducted experiments using the Redis data storage system. To this
end, we deployed Redis with its replication enabled across machines scattered
78
4.5. EVALUATION
(a) Global
(b) Per location
Figure 4.11: Latency of Get Operation in Redis
in three Amazon EC2 regions: Oregon, Tokyo, and Ireland. Redis uses a master-
slave replication model, and we have deployed the master in Ireland and two
slaves in each region, for a total of 7 replicas. We used m1.large instances to run
the master and YCSB and m1.medium instances to run the slaves. YCSB was
executed in the same three regions of Amazon EC2 used in the previously re-
ported experiments, with each YCSB instance running 10 threads that execute
operation in a closed loop. Each thread has its own instance of the Middleware.
All operations access the same list object stored in Redis, with the read opera-
tion being executed in one of the slave replicas of the region, selected randomly.
For each algorithm, we run our experiments 6 times for 60 seconds with an
interval of four minutes between runs. Similar to the experiments conducted
with Facebook, YCSB was conﬁgured to execute a workload composed of 50%
insert and 50% of read operations. Again, we set N to be equal to 25. The
experiments reported in this section aggregate the results from executing a
total of 21,285,291 insert and get operations.
4.5.2.1 Latency
Figure 4.11a presents the average latency of get operations. The results show
that our middleware introduces a very small overhead, on the order of mi-
croseconds, for Read Your Writes, Monotonic Reads, and Monotonic Writes. In
Writes Follow Read and when all session guarantees are enforced, there is an
increase of approximately one to two milliseconds because the algorithms have
79
CHAPTER 4. FINE-GRAINED CONSISTENCY FOR ONLINE SERVICES
(a) Global
(b) Per Location
Figure 4.12: Latency of Insert Operation in Redis
to check the dependencies and process all the metadata information associated
with the various objects returned and stored locally. The results of Figure 4.11b,
which details the values observed in each region, show the same pattern across
all regions, however the latency for reading data using a client in Ireland is
higher than in other locations. This can be explained by the fact that writes
in Ireland are much faster that in other locations, due to the proximity to the
master replica, which causes the total number of read operations that are exe-
cuted to be higher in Ireland than in other locations, thus leading to a higher
load, which results in a higher latency for executing operations particularly,
get operations.
In contrast to the experiments for the Facebook service, the observed laten-
cies are much more predictable in this deployment. This conﬁrms the expec-
tation that a real-world service leads to qualitatively diﬀerent results from a
controlled experiment.
Figure 4.12a reports average latency of the insert operation across all loca-
tions. In this case the latency is almost the same across all cases, but if we look
at Figure 4.12b we see that, in Ireland, latency values are much smaller. This
is again justiﬁed by the location of the master replica in Ireland and the fact
that all clients are issuing their write operations to the (same) master replica.
Figure 4.13 reports the latencies in Ireland, which again show a similar pattern
to the one observed for Get operations.
80
4.5. EVALUATION
Figure 4.13: Latency of Insert Operation in Redis in Ireland
(a) Get Operation
(b) Insert Operation
Figure 4.14: Communication overhead in Redis
4.5.2.2 Communication Overhead
In terms of communication overhead imposed by our Middleware, the results
in Figure 4.14a and Figure 4.14b show that in the get and insert operations the
overhead is more noticeable when enforcing Writes Follow Reads and when
employing the combination of all algorithms. This happens due to the over-
head associated with managing and communicating the information stored in
dependency lists, as discuss previously for the results reported for Facebook.
4.5.2.3 Local Storage Size
To conclude our experimental evaluation of Redis, Figure 4.15 shows that in
Monotonic Reads and Writes Follow Reads the number of elements in the
localView is around 30, which is higher than N = 25. This happens because of
the high write throughput, which causes several elements to be assigned the
81
CHAPTER 4. FINE-GRAINED CONSISTENCY FOR ONLINE SERVICES
Figure 4.15: Local storage overhead for Redis
same timestamp. In this case, our truncation algorithm allows for the limit
to be exceeded in the case of ties. The combinations of all algorithms is also
aﬀected by this situation, leading to a higher value around 55. Note that we
are showing the average of the highest value registered for each independent
client session at any time during its execution.
4.6 Arguments of Correctness
In this section we present the arguments of correctness of our algorithms, in
terms of their ability to guarantee session properties individually and in com-
bination.
4.6.1 Read Your Writes
In this section, we are going to argue that the algorithm guarantees Read Your
Writes. Recall the deﬁnition of an anomaly of Read Your Writes:
The get operation returns an older element inserted by the client before a more
recent one, more precisely, there exist two elements x, y inserted over list L in the
same client session, in this order(x then y), and a get returns S, the top of the list,
and y (cid:60) S ∧ x ∈ S.
To guarantee that this anomaly does not occur, it suﬃces to return a set (the
truncated list of elements that are returned to the application) such that, when
we project the elements from that list that belong to the current client session,
we obtain a suﬃx of the sequence of elements inserted in that session. This
82
4.6. ARGUMENTS OF CORRECTNESS
suﬃces to prevent the anomaly since a property of a suﬃx is that, when an
element is present, all its successors are also present. To ensure this, we assign
each element a timestamp that increases monotonically with the order of the
operations within the session, and we return all the session elements larger
or equal to the timestamp of the oldest session element previously returned
to the client, we call this timestamp lastSessionTimestamp, the way that this
timestamp is set does not aﬀect correctness, but it may aﬀect the length of the
window that is returned. In practice, we decided to set it with the timestamp
of the oldest session element previously returned to the client, because we
assumed that older elements were dropped from the window. Note that the
elements returned to the client are from several session, and that sometimes
it will be impossible to return all session elements with a timestamp above
lastSessionTimestamp, because they were truncated, this situation does not
aﬀect correctness because we are still returning a suﬃx of the sequence of
elements inserted in the session.
In more detail, in line 12 of Algorithm 2, when the client issues a get op-
eration, the service returns a sublist of elements, that sublist may contain x, a
session element above lasSessiontTimestamp and miss y an element inserted
after x. To avoid this situation, the get operation has to insert the missing
elements in the sublist. To this end, the insert operation stores the suﬃx of
the sequence of elements inserted in the session in the insertSet, and then
the algorithm uses the insertSet and the lasSessiontTimestamp to detect the
missing session elements. Finally, when the algorithm detects that an element
is missing, the element is copied from the insertSet to the sublist.
In order to prevent returning session elements that do not belong to the suf-
ﬁx, in line 15 of the algorithm, we remove from the sublist all session elements
with a timestamp below lastSessionTimestamp. To detect these elements in
the sublist, in line 5 of the algorithm, we associate to each element a session
identiﬁer in the insert operation, we need this session identiﬁer because the
service can return session elements that do not belong to the suﬃx and we only
have information about the last session elements inserted in the session, that
are in the insertSet.
83
CHAPTER 4. FINE-GRAINED CONSISTENCY FOR ONLINE SERVICES
The remainder of the algorithm steps do not break this property, and there-
fore we can conclude that a violation of the session guarantee does not appear
in the newly produced trace.
84
4.6. ARGUMENTS OF CORRECTNESS
4.6.2 Monotonic Reads
In this section, we are going to argue that the algorithm guarantees Monotonic
Reads. Recall the deﬁnition of an anomaly of Monotonic Reads:
When a client c issues two get operations that return sequences S1 and S2 (in that
order) and the following property holds: ∃x, y ∈ S1 : S1(x) ≺ S1(y)∧ y (cid:60) S2 ∧ x ∈ S2,
where S1(x) ≺ S1(y) means that element x appears in S1 before y.
To guarantee that this anomaly does not occur, it suﬃces to return a set (the
truncated list of elements that are returned to the application) such that, we
obtain a suﬃx of the sequence of elements previously returned to the client.
Similar to the case of RYW, this suﬃces to prevent the anomaly since a property
of a suﬃx is that, when an element is present, all its successors are also present.
To ensure this, we assign each element a timestamp, and we return all elements
larger or equal to the timestamp of the oldest element previously returned
to the client, we call this timestamp lastTimestamp and it is set just before
returning to the client. Again similar to RYW, the way that this timestamp
is set does not aﬀect correctness, but it may aﬀect the length of the window
that is returned. In practice, we decided to set it with the timestamp of the
oldest element previously returned to the client, because we assumed that older
elements were dropped from the window.
In more detail, in line 7 of Algorithm 3 when the client issues a get oper-
ation, the service returns a sublist of elements, that sublist may contain x, an
element previously returned to the client, that is above lastTimestamp and
miss y, an element returned in a get operation after x. To avoid this situation,
the get operation has to insert the missing elements in the sublist. To this end,
in line 13 of the algorithm, we store the suﬃx of the sequence previously re-
turned to the client, in the localView. Then, in line 9 of the algorithm, when
the algorithm detects that an element is missing, it copies that element from
the localView to the sublist. In order to prevent returning old elements that
do not belong to the suﬃx, in line 10 of the algorithm, we remove from the
sublist all elements with a timestamp below lastTimestamp.
The remainder of the algorithm steps do not break this property (enforced
by the actions we just described) and therefore, we can conclude that a violation
85
CHAPTER 4. FINE-GRAINED CONSISTENCY FOR ONLINE SERVICES
of the session guarantee cannot appear in the newly produced return value.
4.6.3 Monotonic Writes
In this section, we are going to argue that the algorithm guarantees Monotonic
Writes. Recall the deﬁnition of an anomaly of Monotonic Writes:
The get operation returns a subsequence of elements from a session in a diﬀerent
order they were issued or with gaps, more precisely, given a sequence of writes W
in the same session, and a sequence S returned by a read: (∃x, y, z ∈ W : W (x) ≺
W (y) ≺ W (z)∧ x ∈ S ∧ y (cid:60) S ∧ z ∈ S)∨ (∃x, y ∈ W : W (x) ≺ W (y)∧ S(y) ≺ S(x)).
To guarantee that this anomaly does not occur, it suﬃces to return a set (the
truncated list of elements that are returned to the application) such that, when
we project each session subsequence they are ordered by insertion order and
without gaps. To ensure this, we assign each element a unique client session
identiﬁer and a counter that increases monotonically in the session, and we
return a list with the session elements ordered and without gaps.
In more detail, in line 10 of Algorithm 4, when the client issues a get op-
eration, the service returns a sublist of elements, that sublist may contain x
and z, from the session subsequence x ≺ y ≺ z, in this case y is missing from
the subsequence returned by the service. To avoid returning a subsequence
with a gap, the get operation needs to order the subsequences and remove the
existing gaps. To this end, in line 11 of the algorithm, the get operation orders
all elements by session counter and, in line 12 of the algorithm, when it detects
a gap, removes the elements with a session sequence number above the gap
from that subsequence, in this case this entails removing z.
The remainder of the algorithm steps do not break this property that was
enforced by the actions we just described (i.e., it will not add the removed
elements to the list in the reply to the client) and therefore we can conclude
that a violation of the session guarantee does not appear in the newly produced
trace.
86
4.6. ARGUMENTS OF CORRECTNESS
4.6.4 Writes Follow Reads
In this section, we are going to argue that the algorithm guarantees Writes
Follow Reads. Recall the deﬁnition of an anomaly of Writes Follow Reads:
If S1 is a sequence returned by a get operation invoked by client c, w a write
performed by c after observing S1, and S2 is a sequence returned by a read issued
by any client in the system; a violation of the Writes Follow Reads (WFR) anomaly
happens when: ∃w ∈ S2 ∧∃x, y ∈ S1 : S1(x) ≺ S1(y)∧ y (cid:60) S2 ∧ x ∈ S2.
To guarantee that this anomaly does not occur, it suﬃces to return a set
(the truncated list of elements that are returned to the application) such that, it
contains for each element, the list of dependencies, i.e, all elements previously
observed in the session where the element was inserted. Note that the list is
truncated since it is impossible to keep the entire dependency history, and
therefore we only keep the dependencies whose timestamp is larger or equal
to the timestamp of the oldest element that is going to be returned.
To achieve this property, while avoiding returning a set that misses an el-
ement in a chain of dependencies (e.g., because that element was removed in
the context of the algorithm execution), we assign a timestamp to each element
that is implemented with the same rules as logical clocks [47], this timestamp
increases monotonically with the order of the insert operations within the ses-
sion and is larger than the timestamp of the most recent element returned to
the client. Logical clocks guarantee the partial ordering between elements de-
ﬁned by the happens-before relations [47], this is necessary to guarantee that
the elements that were truncated from the list, are not part of the dependencies
that should be returned to the client.
We also need to know the elements that form those dependencies. For
this purpose, we associate with each element a list with its dependencies. To
create this list, we store the elements returned previously, in the localView. To
avoid having a large list of dependencies, we truncate the dependency list to N
elements and we associate to each inserted element the timestamp of the oldest
dependency (the element with the lowest timestamp in the localView), we call
this timestamp cutTimestamp, and it implicitly deﬁnes that every element