in the forwarding table.
Table 8 shows updating 1.6 million entries simultaneously took
89:77 seconds on average, with a standard deviation of less than
one second. As the number of VDPs increases, the average up-
date rate remains constant, but as the number of VDPs increases,
the PCI interconnect speed becomes a bottleneck between the VDP
processes updating the table and the SwitchBlade FPGA.
192)
s
p
p
0
0
0
’
(
i
t
e
a
R
g
n
d
r
a
w
r
o
F
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0
6
4
Path Splicer vs Base Router
Splice Router(2 Tables)
Splice Router(4 Table)
Base Router
VDPs
1
2
3
4
Total Ent.
400 K
800 K
1,200 K
1,600 K
Entries/Table
400 K
400 K
400 K
400 K
Time(sec)
86.582
86.932
88.523
89.770
Single Ent. ((cid:22)s)
216
112
74
56
Table 8: Forwarding Table Update Performance.
1
2
8
2
5
6
5
1
2
1
0
2
4
1
5
0
0
Packet Size
Figure 11: Path Splicing router performance with varying load
compared with base router.
NetFPGA Hardware Router and SwitchBlade Variable Ext. Module
)
s
p
p
0
0
0
’
(
e
t
i
a
R
g
n
d
r
a
w
r
o
F
 1600
 1400
 1200
 1000
 800
 600
 400
 200
 0
6
4
SwitchBlade (Variable Bit Extractor)
NetFPGA Hardware Router
1
0
4
2
0
4
5
0
4
7
0
4
1
0
0
4
1
5
1
8
Packet Size
Figure 12: Variable Bit Length Extraction router performance
compared with base router.
7. A VIRTUAL ROUTER ON SWITCHBLADE
We now describe our integration of SwitchBlade with a virtual
router environment that runs in an OpenVZ container [20]. We use
OpenVZ [20] as the virtual environment for hosting virtual routers
for two reasons related to isolation. First, OpenVZ provides some
level of namespace isolation between each of the respective virtual
environments. Second, OpenVZ provides a CPU scheduler that
prevents any of the control-plane processes from using more than
its share of CPU or memory resources. We run the Quagga routing
software [21] in OpenVZ, as shown in Figure 13.
Each virtual environment has a corresponding VDP that acts as
its data plane. SwitchBlade exposes a register interface to send
commands from the virtual environment to its respective VDP.
Similarly, each VDP can pass data packets into its respective virtual
environment using the software exception handling mechanisms
described in Section 4.5.
We run four virtual environments on the same physical machine
and use the SwitchBlade’s isolation capabilities to share the hard-
ware resources. Each virtual router receives a dedicated amount
of processing and is isolated from the other routers’ virtual data
planes. Each virtual router also has the appearance of a dedicated
data path.
8. DISCUSSION
PCIe interconnect speeds and the tension between hardware
and software. Recent architectures for software-based routers such
as RouteBricks, use the PCI express (PCIe) interface between the
Figure 13: Virtual router design with OpenVZ virtual environ-
ments interfacing to SwitchBlade data plane.
CPU, which acts as the I/O hub, and the network interface cards
that forward traf(cid:2)c. PCIe offers more bandwidth than a standard
PCI interface; for example, PCIe version 2, with 16 lanes link, has
a total aggregate bandwidth of 8 GBps per direction. Although this
high PCIe bandwith would seem to offer great promise for building
programmable routers that rely on the CPU for packet processing,
the speeds of programmable interface cards are also increasing, and
it is unclear as yet whether the trends will play out in favor of CPU-
based packet processing. For example, one Virtex-6 HXT FPGA
from Xilinx or Stratix V FPGA from Altera can process packets
at 100 Gbps. Thus, installing NICs with only one such FPGA can
make the PCIe interconnect bandwidth a bottleneck, and also puts
an inordinate amount of strain on the CPU. SwitchBlade thus favors
making FPGAs more (cid:3)exible and programmable, allowing more
customizability to take place directly on the hardware itself.
Modifying packets in hardware. SwitchBlade’s hardware imple-
mentation focuses on providing customization for protocols that
make only limited modi(cid:2)cations to packets. The design can ac-
commodate writing packets using preprocessor modules, but we
have not yet implemented this function. Providing arbitrary writ-
ing capability in hardware will require either using preprocessor
stage for packet writing and a new pipeline stage after postprocess-
ing, or adding two new stages to the pipeline (both before and after
lookup).
Packet rewriting can be performed in two ways: (1) modifying
existing packet content without changing total data unit size, or
(2) adding or removing some data to each packet with the output
packet size different from the input packet size. Although it is easy
to add the (cid:2)rst function to the preprocessor stage, adding or remov-
ing bytes into packet content will require signi(cid:2)cant effort.
Scaling SwitchBlade. The current SwitchBlade implementation
provides the capability for four virtualized data planes on a single
NetFPGA, but this design is general enough to scale as the capa-
bilities of hardware improve. We see two possible avenues for in-
creasing the number of virtualized data planes in hardware. One
option is to add several servers, each having one FPGA card and
have one or more servers running the control plane that controls
the hardware forwarding-table entries. Other scaling options in-
clude adding more FPGA cards to a single physical machine or
193[9] M. Casado, T. Koponen, D. Moon, and S. Shenker.
Rethinking packet forwarding hardware. In Proc. Seventh
ACM SIGCOMM HotNets Workshop, Nov. 2008.
[10] G. A. Covington, G. Gibb, J. Lockwood, and N. McKeown.
A Packet Generator on the NetFPGA platform. In FCCM
’09: IEEE Symposium on Field-Programmable Custom
Computing Machines, 2009.
[11] S. Deering and R. Hinden. Internet Protocol, Version 6
(IPv6) Speci(cid:2)cation. Internet Engineering Task Force, Dec.
1998. RFC 2460.
[12] M. Dobrescu, , N. Egi, K. Argyraki, B.-G. Chun, K. Fall,
G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy.
RouteBricks: Exploiting parallelism to scale software
routers. In Proc. 22nd ACM Symposium on Operating
Systems Principles (SOSP), Big Sky, MT, Oct. 2009.
[13] B. Godfrey, I. Ganichev, S. Shenker, and I. Stoica. Pathlet
routing. In Proc. ACM SIGCOMM, Barcelona, Spain, Aug.
2009.
[14] Intel IXP 2xxx Network Processors.
http://www.intel.com/design/network/
products/npfamily/ixp2xxx.htm.
[15] C. Kim, M. Caesar, and J. Rexford. Floodless in SEATTLE:
A scalable ethernet architecture for large enterprises. In
Proc. ACM SIGCOMM, Seattle, WA, Aug. 2008.
[16] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M. F.
Kaashoek. The Click modular router. ACM Transactions on
Computer Systems, 18(3):263(cid:150)297, Aug. 2000.
[17] M. Motiwala, M. Elmore, N. Feamster, and S. Vempala. Path
Splicing. In Proc. ACM SIGCOMM, Seattle, WA, Aug. 2008.
[18] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang,
P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat.
Portland: A scalable fault-tolerant layer2 data center network
fabric. In Proc. ACM SIGCOMM, Barcelona, Spain, Aug.
2009.
[19] OpenFlow Switch Consortium.
http://www.openflowswitch.org/, 2008.
[20] OpenVZ: Server Virtualization Open Source Project.
http://www.openvz.org.
[21] Quagga software routing suite.
http://www.quagga.net/.
[22] J. Turner, P. Crowley, J. DeHart, A. Freestone, B. Heller,
F. Kuhns, S. Kumar, J. Lockwood, J. Lu, M. Wilson, et al.
Supercharging PlanetLab: A High Performance,
Multi-application, Overlay Network Platform. In Proc. ACM
SIGCOMM, Kyoto, Japan, Aug. 2007.
[23] Xilinx. Xilinx ise design suite. http:
//www.xilinx.com/tools/designtools.htm.
[24] X. Yang, D. Wetherall, and T. Anderson. Source selectable
path diversity via routing de(cid:3)ections. In Proc. ACM
SIGCOMM, Pisa, Italy, Aug. 2006.
taking advantage of hardware trends, which promise the ability to
process data in hardware at increasingly higher rates.
9. CONCLUSION
We have presented the design, implementation, and evaluation
of SwitchBlade, a platform for deploying custom protocols on pro-
grammable hardware. SwitchBlade uses a pipeline-based hardware
design; using this pipeline, developers can swap common hardware
processing modules in and out of the packet-processing (cid:3)ow on the
(cid:3)y, without having to resynthesize hardware. SwitchBlade also of-
fers programmable software exception handling to allow develop-
ers to integrate custom functions into the packet processing pipeline
that cannot be handled in hardware. SwitchBlade’s customizable
forwarding engine also permits the platform to make packet for-
warding decisions on various (cid:2)elds in the packet header, enabling
custom, non-IP based forwarding at hardware speeds. Finally,
SwitchBlade can host multiple data planes in hardware in paral-
lel, sharing common hardware processing modules while providing
performance isolation between the respective data planes. These
features make SwitchBlade a suitable platform for hosting virtual
routers or for simply deploying multiple data planes for protocols
or services that offer complementary functions in a production en-
vironment. We implemented SwitchBlade using the NetFPGA plat-
form, but SwitchBlade can be implemented with any FPGA.
Acknowledgments
This work was funded by NSF CAREER Award CNS-0643974 and
NSF Award CNS-0626950. We thank Mohammad Omer for his
help in solving various technical dif(cid:2)culties during project. We
also thank our shepherd, Amin Vahdat, for feedback and comments
that helped improve the (cid:2)nal draft of this paper.
REFERENCES
[1] FlowVisor. http://www.openflowswitch.org/wk/
index.php/FlowVisor.
[2] NetFPGA. http://www.netfpga.org.
[3] D. G. Andersen, H. Balakrishnan, N. Feamster, T. Koponen,
D. Moon, and S. Shenker. Accountable Internet Protocol
(AIP). In Proc. ACM SIGCOMM, Seattle, WA, Aug. 2008.
[4] M. B. Anwer and N. Feamster. Building a Fast, Virtualized
Data Plane with Programmable Hardware. In Proc. ACM
SIGCOMM Workshop on Virtualized Infrastructure Systems
and Architectures, Barcelona, Spain, Aug. 2009.
[5] S. Bhatia, M. Motiwala, W. Muhlbauer, V. Valancius,
A. Bavier, N. Feamster, L. Peterson, and J. Rexford. Hosting
Virtual Networks on Commodity Hardware. Technical
Report GT-CS-07-10, Georgia Institute of Technology,
Atlanta, GA, Oct. 2007.
[6] S. Bhatia, M. Motiwala, W. M(cid:252)hlbauer, V. Valancius,
A. Bavier, N. Feamster, J. Rexford, and L. Peterson. Hosting
virtual networks on commodity hardware. Technical Report
GT-CS-07-10, College of Computing, Georgia Tech, Oct.
2007.
[7] G. Calarco, C. Raffaelli, G. Schembra, and G. Tusa.
Comparative analysis of smp click scheduling techniques. In
QoS-IP, pages 379(cid:150)389, 2005.
[8] L. D. Carli, Y. Pan, A. Kumar, C. Estan, and
K. Sankaralingam. Flexible lookup modules for rapid
deployment of new protocols in high-speed routers. In Proc.
ACM SIGCOMM, Barcelona, Spain, Aug. 2009.
194