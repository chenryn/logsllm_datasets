### Forwarding Table Update Performance

Table 8 illustrates the performance of updating 1.6 million entries simultaneously, which took an average of 89.77 seconds, with a standard deviation of less than one second. As the number of VDPs (Virtual Data Planes) increases, the average update rate remains constant. However, as the number of VDPs grows, the PCI interconnect speed becomes a bottleneck between the VDP processes updating the table and the SwitchBlade FPGA.

| VDPs | Total Entries | Entries/Table | Time (sec) | Single Entry (µs) |
|------|--------------|---------------|------------|-------------------|
| 1    | 400 K        | 400 K         | 86.582     | 216               |
| 2    | 800 K        | 400 K         | 86.932     | 112               |
| 3    | 1,200 K      | 400 K         | 88.523     | 74                |
| 4    | 1,600 K      | 400 K         | 89.770     | 56                |

**Table 8: Forwarding Table Update Performance**

### Path Splicing Router Performance

The following figure compares the performance of the Path Splicing router with the base router under varying loads.

**Figure 11: Path Splicing Router Performance with Varying Load Compared to Base Router**

![Path Splicing Router Performance](path_splicing_performance.png)

### Variable Bit Length Extraction Router Performance

This figure shows the performance of the Variable Bit Length Extraction router compared to the base router.

**Figure 12: Variable Bit Length Extraction Router Performance Compared to Base Router**

![Variable Bit Length Extraction Router Performance](variable_bit_extraction_performance.png)

### Integration with Virtual Router on SwitchBlade

We now describe the integration of SwitchBlade with a virtual router environment running in an OpenVZ container [20]. We chose OpenVZ for two reasons related to isolation:
1. **Namespace Isolation:** OpenVZ provides namespace isolation between each virtual environment.
2. **CPU Scheduling:** OpenVZ includes a CPU scheduler that prevents any control-plane process from using more than its share of CPU or memory resources.

We run the Quagga routing software [21] in OpenVZ, as shown in Figure 13. Each virtual environment has a corresponding VDP that acts as its data plane. SwitchBlade exposes a register interface to send commands from the virtual environment to its respective VDP. Similarly, each VDP can pass data packets into its respective virtual environment using the software exception handling mechanisms described in Section 4.5.

We run four virtual environments on the same physical machine and use SwitchBlade’s isolation capabilities to share hardware resources. Each virtual router receives a dedicated amount of processing and is isolated from the other routers’ virtual data planes. Each virtual router also appears to have a dedicated data path.

**Figure 13: Virtual Router Design with OpenVZ Virtual Environments Interfacing to SwitchBlade Data Plane**

### Discussion

#### PCIe Interconnect Speeds and Hardware-Software Tension

Recent architectures for software-based routers, such as RouteBricks, use the PCI Express (PCIe) interface between the CPU, which acts as the I/O hub, and the network interface cards that forward traffic. PCIe offers more bandwidth than a standard PCI interface; for example, PCIe version 2, with 16 lanes, has a total aggregate bandwidth of 8 GBps per direction. Although this high PCIe bandwidth seems promising for building programmable routers that rely on the CPU for packet processing, the speeds of programmable interface cards are also increasing. It is unclear whether the trends will favor CPU-based packet processing. For instance, one Virtex-6 HXT FPGA from Xilinx or Stratix V FPGA from Altera can process packets at 100 Gbps. Thus, installing NICs with only one such FPGA can make the PCIe interconnect bandwidth a bottleneck and put significant strain on the CPU. SwitchBlade favors making FPGAs more flexible and programmable, allowing more customization directly on the hardware.

#### Modifying Packets in Hardware

SwitchBlade’s hardware implementation focuses on providing customization for protocols that make only limited modifications to packets. The design can accommodate writing packets using preprocessor modules, but this function has not yet been implemented. Providing arbitrary writing capability in hardware will require either using a preprocessor stage for packet writing and a new pipeline stage after postprocessing or adding two new stages to the pipeline (both before and after lookup).

Packet rewriting can be performed in two ways:
1. **Modifying existing packet content without changing the total data unit size.**
2. **Adding or removing some data to each packet, resulting in a different output packet size.**

While it is easy to add the first function to the preprocessor stage, adding or removing bytes from packet content will require significant effort.

#### Scaling SwitchBlade

The current SwitchBlade implementation supports four virtualized data planes on a single NetFPGA, but the design is general enough to scale as hardware capabilities improve. Two possible avenues for increasing the number of virtualized data planes in hardware include:
1. **Adding several servers, each with one FPGA card, and having one or more servers running the control plane that controls the hardware forwarding-table entries.**
2. **Adding more FPGA cards to a single physical machine.**

These options take advantage of hardware trends, which promise the ability to process data in hardware at increasingly higher rates.

### Conclusion

We have presented the design, implementation, and evaluation of SwitchBlade, a platform for deploying custom protocols on programmable hardware. SwitchBlade uses a pipeline-based hardware design, allowing developers to swap common hardware processing modules in and out of the packet-processing flow on the fly, without resynthesizing hardware. It also offers programmable software exception handling to integrate custom functions into the packet processing pipeline that cannot be handled in hardware. SwitchBlade’s customizable forwarding engine enables packet forwarding decisions based on various fields in the packet header, supporting custom, non-IP-based forwarding at hardware speeds. Finally, SwitchBlade can host multiple data planes in hardware in parallel, sharing common hardware processing modules while providing performance isolation between the respective data planes. These features make SwitchBlade a suitable platform for hosting virtual routers or deploying multiple data planes for protocols or services that offer complementary functions in a production environment. We implemented SwitchBlade using the NetFPGA platform, but it can be implemented with any FPGA.

### Acknowledgments

This work was funded by NSF CAREER Award CNS-0643974 and NSF Award CNS-0626950. We thank Mohammad Omer for his help in solving various technical difficulties during the project. We also thank our shepherd, Amin Vahdat, for feedback and comments that helped improve the final draft of this paper.

### References

[1] FlowVisor. <http://www.openflowswitch.org/wk/index.php/FlowVisor>

[2] NetFPGA. <http://www.netfpga.org>

[3] D. G. Andersen, H. Balakrishnan, N. Feamster, T. Koponen, D. Moon, and S. Shenker. Accountable Internet Protocol (AIP). In Proc. ACM SIGCOMM, Seattle, WA, Aug. 2008.

[4] M. B. Anwer and N. Feamster. Building a Fast, Virtualized Data Plane with Programmable Hardware. In Proc. ACM SIGCOMM Workshop on Virtualized Infrastructure Systems and Architectures, Barcelona, Spain, Aug. 2009.

[5] S. Bhatia, M. Motiwala, W. Muhlbauer, V. Valancius, A. Bavier, N. Feamster, L. Peterson, and J. Rexford. Hosting Virtual Networks on Commodity Hardware. Technical Report GT-CS-07-10, Georgia Institute of Technology, Atlanta, GA, Oct. 2007.

[6] S. Bhatia, M. Motiwala, W. Mühlbauer, V. Valancius, A. Bavier, N. Feamster, J. Rexford, and L. Peterson. Hosting virtual networks on commodity hardware. Technical Report GT-CS-07-10, College of Computing, Georgia Tech, Oct. 2007.

[7] G. Calarco, C. Raffaelli, G. Schembra, and G. Tusa. Comparative analysis of SMP Click scheduling techniques. In QoS-IP, pages 379–389, 2005.

[8] L. D. Carli, Y. Pan, A. Kumar, C. Estan, and K. Sankaralingam. Flexible lookup modules for rapid deployment of new protocols in high-speed routers. In Proc. ACM SIGCOMM, Barcelona, Spain, Aug. 2009.

[9] M. Casado, T. Koponen, D. Moon, and S. Shenker. Rethinking packet forwarding hardware. In Proc. Seventh ACM SIGCOMM HotNets Workshop, Nov. 2008.

[10] G. A. Covington, G. Gibb, J. Lockwood, and N. McKeown. A Packet Generator on the NetFPGA platform. In FCCM '09: IEEE Symposium on Field-Programmable Custom Computing Machines, 2009.

[11] S. Deering and R. Hinden. Internet Protocol, Version 6 (IPv6) Specification. Internet Engineering Task Force, Dec. 1998. RFC 2460.

[12] M. Dobrescu, N. Egi, K. Argyraki, B.-G. Chun, K. Fall, G. Iannaccone, A. Knies, M. Manesh, and S. Ratnasamy. RouteBricks: Exploiting parallelism to scale software routers. In Proc. 22nd ACM Symposium on Operating Systems Principles (SOSP), Big Sky, MT, Oct. 2009.

[13] B. Godfrey, I. Ganichev, S. Shenker, and I. Stoica. Pathlet routing. In Proc. ACM SIGCOMM, Barcelona, Spain, Aug. 2009.

[14] Intel IXP 2xxx Network Processors. <http://www.intel.com/design/network/products/npfamily/ixp2xxx.htm>

[15] C. Kim, M. Caesar, and J. Rexford. Floodless in SEATTLE: A scalable Ethernet architecture for large enterprises. In Proc. ACM SIGCOMM, Seattle, WA, Aug. 2008.

[16] E. Kohler, R. Morris, B. Chen, J. Jannotti, and M. F. Kaashoek. The Click modular router. ACM Transactions on Computer Systems, 18(3):263–297, Aug. 2000.

[17] M. Motiwala, M. Elmore, N. Feamster, and S. Vempala. Path Splicing. In Proc. ACM SIGCOMM, Seattle, WA, Aug. 2008.

[18] R. N. Mysore, A. Pamboris, N. Farrington, N. Huang, P. Miri, S. Radhakrishnan, V. Subramanya, and A. Vahdat. Portland: A scalable fault-tolerant layer2 data center network fabric. In Proc. ACM SIGCOMM, Barcelona, Spain, Aug. 2009.

[19] OpenFlow Switch Consortium. <http://www.openflowswitch.org/>

[20] OpenVZ: Server Virtualization Open Source Project. <http://www.openvz.org>

[21] Quagga software routing suite. <http://www.quagga.net/>

[22] J. Turner, P. Crowley, J. DeHart, A. Freestone, B. Heller, F. Kuhns, S. Kumar, J. Lockwood, J. Lu, M. Wilson, et al. Supercharging PlanetLab: A High Performance, Multi-application, Overlay Network Platform. In Proc. ACM SIGCOMM, Kyoto, Japan, Aug. 2007.

[23] Xilinx. Xilinx ISE Design Suite. <http://www.xilinx.com/tools/designtools.htm>

[24] X. Yang, D. Wetherall, and T. Anderson. Source selectable path diversity via routing deflections. In Proc. ACM SIGCOMM, Pisa, Italy, Aug. 2006.