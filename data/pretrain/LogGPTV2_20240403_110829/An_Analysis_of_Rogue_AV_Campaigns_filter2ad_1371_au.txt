utilize our engine. Our prototype implementation has achieved end-to-
end throughput in the order of 20 Gbits/s, 100 times the performance
of the CPU-only ClamAV, while almost completely oﬄoading the CPU,
leaving it free to complete other tasks. Our micro-benchmarks have mea-
sured our engine to be able to sustain throughput in the order of 40
Gbits/s. The results suggest that modern graphics cards can be used ef-
fectively to perform heavy-duty anti-malware operations at speeds that
cannot be matched by traditional CPU based techniques.
1 Introduction
The ever increasing amount of malicious software in todays connected world,
poses a tremendous challenge to network operators, IT administrators, as well as
ordinary home users. Antivirus software is one of the most widely used tools for
detecting and stopping malicious or unwanted software. For an eﬀective defense,
one needs virus-scanning performed at central network traﬃc ingress points,
as well as at end-host computers. As such, anti-malware software applications
scan traﬃc at e-mail gateways and corporate gateway proxies, and also on edge
compute devices such as ﬁle servers, desktops and laptops. Unfortunately, the
constant increase in link speeds, storage capacity, number of end-devices and the
sheer number of malware, poses signiﬁcant challenges to virus scanning applica-
tions, which end up requiring multi-gigabit scanning throughput.
Typically, a malware scanner spend the bulk of its time matching data streams
against a large set of known signatures, using a pattern matching algorithm.
S. Jha, R. Sommer, and C. Kreibich (Eds.): RAID 2010, LNCS 6307, pp. 79–96, 2010.
c(cid:7) Springer-Verlag Berlin Heidelberg 2010
80
G. Vasiliadis and S. Ioannidis
Pattern matching algorithms analyze the data stream and compare it against
a database of signatures to detect known malware. The signature patterns can
be fairly complex, composed of diﬀerent-size strings, wild-card characters, range
constraints, and sometimes recursive forms. Every year, as the amount of mal-
ware grows, the number of signatures is increasing proportional, exposing scaling
problems of anti-malware products.
To come up with the large signature sets, most approaches rely on the quickly,
fast and accurate ﬁltering of the “no-match” cases, based on the fact that the
majority of network traﬃc and ﬁles is not supposed to contain viruses [9]. Other
approaches are based on specialized hardware, like FPGAs and ASICs, to achieve
high performance [15,14]. Such hardware solutions are very eﬃcient and perform
quite well, however they are hard to program, complex to modify, and are usually
tied to a speciﬁc implementation.
In contrast, commodity graphics processing units (GPUs) have been proven
to be very eﬃcient and highly eﬀective at accelerating the pattern matching
operations of network intrusion detection systems (NIDS) [26,21,27]. Driven
by the ever-growing video game industry, modern GPUs have been constantly
evolving to ever more powerful and ﬂexible stream processors, specialized for
computationally-intensive and highly parallel operations. The massive number
of transistors devoted to data processing, rather than data caching and ﬂow con-
trol, can be exploited to perform computations that up till now were handled by
the CPU.
In this work, we explore how the highly parallel capabilities of commodity
graphics processing units can be utilized to improve the performance of mal-
ware scanning programs and how they can assist and oﬄoad the CPU whenever
possible.
From a high-level view, malware scanning is divided into two phases. First, all
ﬁles are scanned by the GPU, using a combined DFA state machine that contain
only a preﬁx from each signature. This results in identifying all potentially ma-
licious ﬁles, but a number of clean ﬁles as well. The GPU then outputs a set of
suspect matched ﬁles and the corresponding oﬀsets in those ﬁles. In the second
phase, all those ﬁles are rescanned using a full pattern matching algorithm.
The contributions of our work are:
– We have designed, implemented and evaluated a pattern matching algorithm
on modern GPUs. Our implementation could be adapted to any other multi-
core system, as well.
– We integrated our GPU implementation into ClamAV [12], the most popu-
lar and widely used open-source virus scanning software, proving that our
solution can be used in the real-world.
– We developed and implemented a series of system level optimizations to
improve end-to-end performance of our system.
– We implemented, experimented and analyzed our GPU-assisted virus scan-
ning application with various conﬁgurations and we show that modern GPUs
can eﬀectively be used, in coordination with the CPU, to drastically improve
the performance of anti-malware applications.
GrAVity: A Massively Parallel Antivirus Engine
81
Our prototype implementation, called GrAVity, achieved a scanning through-
put of 20 Gbits/s for binary ﬁles. This represents a speed-up factor of 100 from
the single CPU-core case. Also, in special cases, where data is cached on the
graphics card, the scanning throughput can reach 110 Gbits/s.
The rest of the paper is organized as follows. In Section 2, we present some
background on general-purpose GPU (GPGPU) programming and introduce
the related virus scanning architectures. The architecture and acceleration tech-
niques are presented in Section 3. The performance analysis and evaluation are
given in Section 4. The paper ends with an outline of related work in Section 5
and some concluding remarks in Section 6.
2 Background
In this section, we brieﬂy describe the architecture of modern graphics cards
and the general-purpose computing functionality they provide for non-graphics
applications. We also discuss some general aspects of virus-scanning techniques.
2.1 GPU Programming
For our work we selected the NVIDIA GeForce 200 Series architecture, which of-
fers a rich programming environment and ﬂexible abstraction models through the
Compute Uniﬁed Device Architecture (CUDA) SDK [18]. The CUDA program-
ming model extends the C programming language with directives and libraries
that abstract the underlying GPU architecture and make it more suitable for
general purpose computing. In contrast with standard graphics APIs, such as
OpenGL and DirectX, CUDA exposes several hardware features to the program-
mer. The most important of these features is the existence of convenient data
types, and the ability to access the DRAM of the device card through the general
memory addressing mode it provides. CUDA also oﬀers highly optimized data
transfer operations to and from the GPU.
The GeForce 200 Series architecture, in accordance with its ancestors GeForce
8 (G80) and GeForce 9 (G90) Series, is based on a set of multiprocessors, each of
which contains a set of stream processors operating on SIMD (Single Instruction
Multiple Data) programs. When programmed through CUDA, the GPU can be
used as a general purpose processor, capable of executing a very high number of
threads in parallel.
A unit of work issued by the host computer to the GPU is called a kernel,
and is executed on the device as many diﬀerent threads organized in thread
blocks. Each multiprocessor executes one or more thread blocks, with each group
organized into warps. A warp is a fraction of an active group, which is processed
by one multiprocessor in one batch. Each of these warps contains the same
number of threads, called the warp size, and is executed by the multiprocessor
in a SIMD fashion. Active warps are time-sliced: A thread scheduler periodically
switches from one warp to another to maximize the use of the multiprocessors
computational resources.
82
G. Vasiliadis and S. Ioannidis
Stream processors within a processor share an instruction unit. Any control
ﬂow instruction that causes threads of the same warp to follow diﬀerent execution
paths reduces the instruction throughput, because diﬀerent executions paths
have to be serialized. When all the diﬀerent execution paths have reached a
common end, the threads converge back to the same execution path.
A fast shared memory is managed explicitly by the programmer among thread
blocks. The global, constant, and texture memory spaces can be read from or writ-
ten to by the host, are persistent across kernel launches by the same application,
and are optimized for diﬀerent memory usages [18]. The constant and texture
memory accesses are cached, so a read from them costs much less compared to
device memory reads, which are not being cached. The texture memory space is
implemented as a read-only region of device memory.
2.2 Virus Scanning and ClamAV
ClamAV [12] is the most widely used open-source virus scanner. It oﬀers client-
side protection for personal computers, as well as mail and ﬁle servers used
by large organizations. As of January 2010, it has a database of over 60,000
virus signatures, and consists of a core scanner library and various command-
line utilities. The database includes signatures for non-polymorphic viruses in
simple string format, and for polymorphic viruses in regular expression format
(polymorphic signatures).
The current version of ClamAV uses an optimized version of the Boyer-Moore
algorithm [3] to detect non-polymorphic viruses using simple ﬁxed string signa-
tures. For polymorphic viruses, on the other hand, ClamAV uses a variant of the
classical Aho-Corasick algorithm [1].
The Boyer-Moore implementation in ClamAV, uses a shift-table to reduce
the number of times the Boyer-Moore routine is called. At start up, ClamAV
preprocess every signature and stores the shift value of every possible block
(arbitrarily choosing a block size of 3 bytes) to initialize a shift table. Then, at
any point in the input stream, ClamAV can determine if it can skip up to three
bytes by performing a quick hash on them. ClamAV also creates a hash table
based on the ﬁrst three bytes of the signature and uses this table at run-time
when the shift table returns a match. Since this algorithm uses hash functions
on all bytes of a signature, it is only usable against non-polymorphic viruses.
The Aho-Corasick implementation uses a trie to store the automaton gen-
erated from the polymorphic signatures. The ﬁxed string parts of each poly-
morphic signatures are extracted, and are used to build a trie. At the scanning
phase, the trie will be used to scan for all these ﬁxed parts of each signature
simultaneously. For example, the signature ‘‘495243*56697275’’ contains two
parts, ‘‘495243’’ and ‘‘56697275’’, which are matched individually by the
Aho-Corasick algorithm. When all parts of a signature are found, ClamAV also
veriﬁes the order and the gap between the parts, as speciﬁed in the signature.
To quickly perform a lookup in this trie, ClamAV uses a 256 element array for
each node. In the general case, the trie has a variable height, and all patterns
beginning with the same preﬁx are stored under the corresponding leaf node.
GrAVity: A Massively Parallel Antivirus Engine
83
However, in order to simplify the trie construction, the height is restricted to
be equal to the size of the shortest part in the polymorphic signatures, which is
currently equal to two. Thus, the trie depth is ﬁxed to two and all patterns are
stored at the same trie level. During the scanning phase, ClamAV scans an in-
put ﬁle and detects occurrences of each of the polymorphic signatures, including
partially and completely overlapping occurrences. The Aho-Corasick algorithm
has the desirable property that the processing time does not depend on the size
or number of patterns in a signiﬁcant way.
The main reason that ClamAV uses both Boyer-Moore and Aho-Corasick is
that many parts in the polymorphic signatures are short, and they restrict the
maximum shift distance allowed (bounded by the shortest pattern) in the Boyer-
Moore algorithm. Matching the polymorphic signatures in Aho-Corasick avoid
this problem. Furthermore, compared with the sparse automaton representation
of the Aho-Corasick algorithm, the compressed shift table is a more compact
representation of a large number of non-polymorphic signatures in ﬁxed strings,
so the Boyer-Moore algorithm is more eﬃcient in terms of memory space.
3 Design and Implementation
GrAVity utilizes the GPU to quickly ﬁlter out the data segments that do not
contain any viruses. To achieve this, we have modiﬁed ClamAV, such that the
input data stream is initially scanned by the GPU. The GPU uses a preﬁx of
each virus signature to quickly ﬁlter-out clean data. Most data do not contain
any viruses, so such ﬁltering is quite eﬃcient as we will see in Section 4.
The overall architecture of GrAVity is shown in Figure 1. The contents of each
ﬁle are stored into a buﬀer in a region of main memory that can be transferred
via DMA into the memory of the GPU. The SPMD operation of the GPU is ideal
Filtering
Verification
check_offsets
read
Files
File Buffer
Potential
Matces
Verification
Module
Malicious
Files
Report
PCIe x16
dma_copy
match
GPU
dfa_new
(offline)
Full Virus
Signatures
Fig. 1. GrAVity Architecture. Files are mapped onto pinned memory that can be
copied via DMA onto the graphics card. The matching engine performs a ﬁrst-pass
ﬁltering on the GPU and return potential true positives for further checking onto the
CPU.
84
G. Vasiliadis and S. Ioannidis
for creating multiple search engine instances that will scan for virus signatures
on diﬀerent data in a massively parallel fashion. If the GPU detects a suspicious
virus, that is, there is preﬁx match, the ﬁle is passed to the veriﬁcation module
for further investigation. If the data stream is clean, no further computation
takes place. Therefore, the GPU is employed as a ﬁrst-pass high-speed ﬁlter,
before completing any further potential signature-matching work on the CPU.
3.1 Basic Mechanisms
At start-up, the entire signature set of ClamAV is preprocessed, to construct a
deterministic ﬁnite automaton (DFA). Signature matching using a DFA machine
has linear complexity as a function of the input text stream, which is very eﬃ-
cient. Unfortunately, the number of virus signatures, as well as their individual
size is quire very large, so it may not be always feasible to construct a DFA
machine that will contain the complete signature set. As the number and size of
matching signatures increase, the size of the automaton also increases.
To overcome this, we chose to only use a portion from each virus signature.
By using the ﬁrst n symbols from each signature, the height of the corresponding
DFA matching machine is limited to n, as shown in Figure 2. In addition, all
patterns that begin with the same preﬁx are stored under the same node, called
ﬁnal node. In case the length of the signature pattern is smaller than the preﬁx
Depth 0
0 1 ... 254 255
Depth 1
0 1 ... 254 255
0 1 ... 254 255
Depth n
0 1 ... 254 255
0 1 ... 254 255
Leaf
Leaf
Leaf
Patterns
Fig. 2. A fragment of the DFA structure with n levels. The patterns beginning with
the same preﬁx are stored under the same ﬁnal node (leaf).
GrAVity: A Massively Parallel Antivirus Engine
85
length, the entire pattern is added. A preﬁx may also contain special characters,
such as the wild-characters * and ?, that are used in ClamAV signatures to
describe a known virus.
At the scanning phase, the input data will be initially scanned by the DFA
running on the GPU. Obviously, the DFA may not be able to match an exact
virus signature inside a data stream, as in many cases the length of the signature
is longer than the length of the preﬁx we used to create the automaton. This
will be the ﬁrst-level ﬁltering though, designed to oﬄoad the bulk of the work
from the CPU, by drastically eliminating a signiﬁcant portion of the input data
that need to be scanned.
It is clear that the longer the preﬁx, the fewer the number of false positives at
this initial scanning phase. As we will see in Section 4, using a value of 8 for n,
can result to less than 0.0001% of false positives in a realistic corpus of binary
ﬁles.
3.2 Parallelizing DFA Matching on the GPU
During scan time, the algorithm moves over the input data stream one byte at
a time. For each byte, the scanning algorithm moves the current state appro-
priately. The pattern matching is performed byte-wise, meaning that we have
an input width of 8 bits and an alphabet size of 28 = 256. Thus, each state
will contain 256 pointers to other states, as shown in Figure 2. The size of the
DFA state machine is thus |#States| ∗ 1024 bytes, where every pointer occupies
4 bytes of storage.
If a ﬁnal-state is reached, a potential signature match has been found. Con-
sequently, the oﬀset where the match has been found is marked and all marked
oﬀsets will be veriﬁed later by the CPU. The idea is to quickly weed-out the
dominant number of true negatives using the superior performance and high
parallelism of the GPU, and pass on the remaining potential true positives to
the CPU.
To utilize all streaming processors of the GPU, we exploit its data parallel ca-
pabilities by creating multiple threads. An important design decision is how to
assign the input data to each thread. The simplest approach would be to use mul-
tiple data input streams, one for each thread, in separate memory areas. However,
this will result in asymmetrical processing eﬀort for each processor and will not
scale well. For example, if the sizes of the input streams vary, the amount of work
per thread will not be the same. This means that threads will have to wait, until
all have ﬁnished searching the data stream that was assigned to them.
Therefore, each thread searches a diﬀerent portion of the input data stream,
at the matching phase. To best utilize the data-parallel capabilities of the GPU,
we create a large number of threads that run simultaneously. Our strategy splits
the input stream in distinct chunks, and each chunk is processed by a diﬀerent
thread. Figure 3 shows how each GPU thread scans its assigned chunk, using
the underlying DFA state table. Although they access the same automaton, each
thread maintains its own state, eliminating any need for communication between
them.
86
G. Vasiliadis and S. Ioannidis
Host Machine
File Buffer
GPU
Texture Memory
1
2
3
4
5
6
N
Threads:
Bit Array
Global Memory
Host Machine
Matches
Fig. 3. Pattern matching on the GPU
A special case, however, is for patterns that may span across two or more
diﬀerent chunks. The simplest approach for ﬁxed string patterns, would be to
process in addition, n bytes, where n is the maximum pattern length in the
dictionary. Unfortunately, the virus patterns are usually very large, as shown
in Figure 4 for the ClamAV, especially when compared with patterns in other
pattern matching systems like Snort. Moreover, a regular expression may contain
the wild card character *, thus the length of the patterns may not be determined.
To solve this problem, we used the following heuristic: each thread continues
the search up to the following chunk (which contains the consecutive bytes),
until a fail or ﬁnal-state is reached. While matching a pattern that spans chunk
boundaries, the state machine will perform regular transitions. However, if the
state machine reaches a fail or ﬁnal-state, then it is obvious that there is no need
to process the data any further, since any consecutive patterns will be matched
by the thread that was assigned to search the current chunk. This allows the
threads to operate independently and avoid any communication between them,
regarding boundaries in the input data buﬀer.
Every time a match is found, it is stored to a bit array. The size of the bit
array is equal to the size of the data that is processed at concurrently. Each bit in
GrAVity: A Massively Parallel Antivirus Engine
87
h
t
g
n