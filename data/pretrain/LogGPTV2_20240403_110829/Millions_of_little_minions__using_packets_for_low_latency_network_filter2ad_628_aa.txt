title:Millions of little minions: using packets for low latency network
programming and visibility
author:Vimalkumar Jeyakumar and
Mohammad Alizadeh and
Yilong Geng and
Changhoon Kim and
David Mazières
Millions of Little Minions: Using Packets for Low Latency
Network Programming and Visibility
Vimalkumar Jeyakumar1, Mohammad Alizadeh2, Yilong Geng1, Changhoon Kim3,
David Mazières1
{jvimal@cs.,alizade@,gengyl08@}stanford.edu, PI:EMAIL
1Stanford University, 2Cisco Systems, 3Barefoot Networks
ABSTRACT
This paper presents a practical approach to rapidly introducing new
dataplane functionality into networks: End-hosts embed tiny pro-
grams into packets to actively query and manipulate a network’s
internal state. We show how this “tiny packet program” (TPP) in-
terface gives end-hosts unprecedented visibility into network be-
havior, enabling them to work with the network to achieve a de-
sired functionality. Our design leverages what each component
does best: (a) switches forward and execute tiny packet programs
(at most 5 instructions) in-band at line rate, and (b) end-hosts per-
form arbitrary (and easily updated) computation on network state.
By implementing three different research proposals, we show that
TPPs are useful. Using a hardware prototype on a NetFPGA, we
show our design is feasible at a reasonable cost.
Categories and Subject Descriptors
C.2.1 [Computer-Communication Networks]: Network Ar-
chitecture and Design; C.2.3 [Computer-Communication Net-
works]: Network Operations; C.4 [Performance of Systems]: De-
sign Studies, Performance Attributes
Keywords
Active Networks, SDN, Software-Deﬁned Networks, Network Ar-
chitecture, Switch ASIC Design
1
Consider a large datacenter network with thousands of switches.
Applications complain about poor performance due to high ﬂow
completion times for a small subset of their ﬂows. As an operator,
you realize this symptom could be due to congestion, either from
competing cross trafﬁc or poor routing decisions, or alternatively
could be due to packet drops at failed links. In any case, your goal
is to diagnose this issue quickly. Unfortunately, the extensive use
of multipath routing in today’s networks means one often cannot
determine the exact path taken by every packet; hence it is quite
difﬁcult to triangulate problems to a single switch. Making matters
worse, if congestion is intermittent, counters within the network
will look “normal” at timescales of minutes or even seconds.
Permission to make digital or hard copies of all or part of this work for personal 
or  classroom  use  is  granted  without  fee  provided  that  copies  are  not  made  or 
distributed  for  profit  or  commercial  advantage  and  that  copies  bear this  notice 
and the full citation on the first page. Copyrights for components of this work 
owned by others than the author(s) must be honored. Abstracting with credit is 
permitted. To copy otherwise, or republish, to post on servers or to redistribute to 
lists, requires prior specific permission and/or a fee. Request permissions from 
permissions@acm.org. 
SIGCOMM’14,  August 17–22, 2014, Chicago, Illinois, USA. 
Copyright is held by the owner/author(s). Publication rights licensed to ACM. 
ACM 978-1-4503-2836-4/14/08…$15.00. 
http://dx.doi.org/10.1145/2619239.2626292
Introduction
Such issues would be straightforward to debug if one could ex-
amine relevant network state such as switch ID, queue occupancy,
input/output ports, port utilization, and matched forwarding rules at
the exact time each packet was forwarded, so as to reconstruct what
exactly transpired in the dataplane. In the above example, end-hosts
could use state obtained from millions of successfully delivered
packets to explicitly pinpoint network links that have high queue
occupancy (for congestion), or use switch and port IDs to verify
that packets were correctly routed, or use path information to tri-
angulate network links that cause packet drops due to link failures.
In short, the ability to correlate network state to speciﬁc packets
would be invaluable.
Can packets be instrumented to access and report on switch
state? To date such state has been locked inside switches. This
paper describes a simple, programmable interface that enables end-
hosts to query switch memory (counters, forwarding table entries,
etc.) from packets, directly in the dataplane. Speciﬁcally, a subset
of packets carry in their header a tiny packet program (TPP), which
consists of a few instructions that read, write, or perform simple,
protocol-agnostic computation using switch memory.
A key observation in this paper is that having such programmable
and fast access to network state beneﬁts a broad class of net-
work tasks—congestion control, measurement, troubleshooting,
and veriﬁcation—which we call dataplane tasks. We show how
the TPP interface enables end-hosts to rapidly deploy new func-
tionality by refactoring many network tasks into: (a) simple TPPs
that execute on switches, and (b) expressive programs at end-hosts.
TPPs contrast to three approaches to introduce new dataplane
functionality: (1) build custom hardware for each task, (2) build
switches that can execute arbitrary code [33, 36], or (3) use FP-
GAs and network processors [26]. Each approach has its own draw-
backs: Introducing new switch functionality can take many years;
switch hardware has stringent performance requirements and can-
not incur the penalty of executing arbitrary code; and FPGAs and
network processors are simply too expensive at large scale [7]. In-
stead, we argue that if we could build new hardware to support
just one simple interface such as the TPP, we can leverage end-
hosts to implement many complex tasks at software-development
timescales.
TPPs can be viewed as a particular, reasoned point within the
spectrum of ideas in Active Networking [33, 36]. In many Active
Networks formulations, routers execute arbitrary programs that ac-
tively control network behavior such as routing, packet compres-
sion, and (de-)duplication. By contrast, TPP instructions are so
simple they execute within the time to forward packets at line-rate.
Just a handful of TPP instructions, shown in Table 1, providing
access to the statistics in Table 2, proved sufﬁcient to implement
several previous research proposals.
3Instruction
LOAD, PUSH
STORE, POP
CSTORE
CEXEC
Meaning
Copy values from switch to packet
Copy values from packet to switch
Conditionally store and execute subsequent opera-
tions
Conditionally execute the subsequent instructions
Table 1: The tasks we present in the paper require support only
for the above instructions, whose operands will be clear when we
discuss examples. Write instructions may be selectively disabled
by the administrator.
1.1 Goals
Our main goal is to expose network state to end-hosts through the
dataplane. To beneﬁt dataplane tasks, any interface should satisfy
the following requirements:
• Speed: A recent study shows evidence that switch CPUs are
not powerful and are unable to handle more than a few hundred
OpenFlow control messages/second [14]. Our experience is that
such limitations stand in the way of a whole class of dataplane
tasks as they operate at packet and round-trip timescales.
• Packet-level consistency: Switch state such as link queue oc-
cupancy and forwarding tables varies over time. Today, we lack
any means of obtaining a consistent view of such state as it per-
tains to each packet traveling through the network.
• Minimality and power: To be worth the effort, any hardware
design should be simple, be sufﬁciently expressive to enable a
diverse class of useful tasks, and incur low-enough overhead to
work at line rates.
This paper presents a speciﬁc TPP interface whose design is
largely guided by the above requirements.
Non-Goals: It is worth noting that our goal is not to be ﬂexible
enough to implement any, and all dataplane network tasks. For in-
stance, TPPs are not expressive enough to implement per-packet
scheduling. Moreover, our design is for networks owned and oper-
ated by a single administrative entity (e.g., privately owned WANs
and datacenters). We do not advocate exposing network state to un-
trusted end-hosts connected to the network, but we describe mecha-
nisms to avoid executing untrusted TPPs (§4.3). Finally, a detailed
design for inter-operability across devices from multiple vendors
is beyond the scope of this paper, though we discuss one plausible
approach (§8).
1.2 Summary of Results
Through both a software implementation and a NetFPGA proto-
type, this paper demonstrates that TPPs are both useful and feasi-
ble at line rate. Moreover, an analysis using recent data [7] suggests
that TPP support within switch hardware can be realized at an ac-
ceptable cost.
Applications: We show the beneﬁts of TPP by refactoring many
recent research proposals using the TPP interface. These tasks
broadly fall under the following three categories:
• Congestion Control: We show how end-hosts, by periodically
querying network link utilization and queue sizes with TPP, can
implement a rate-based congestion control algorithm (RCP) pro-
viding max-min fairness across ﬂows. We furthermore show how
the TPP interface enables fairness metrics beyond the max-min
fairness for which RCP was originally designed (§2.2).
• Network Troubleshooting: TPPs give end-hosts detailed per-
packet visibility into network state that can be used to imple-
ment a recently proposed troubleshooting platform called Net-
Sight [13]. In particular, we walk through implementing and de-
Statistics
Per-Switch
Per-Port
Per-Queue
Per-Packet
Examples
Switch ID, counters associated with the global L2 or L3
ﬂow tables, ﬂow table version number, timestamp.
Link utilization, bytes received, bytes dropped, bytes
enqueued, application-speciﬁc registers.
Bytes enqueued, bytes dropped.
Packet’s input/output port, queue, matched ﬂow entry,
alternate routes for a packet.
Table 2: A non-exhaustive list of statistics stored in switches mem-
ory that TPPs can access when mapped to known memory loca-
tions. Many statistics are already tracked today but others, such
as ﬂow table version will have to be implemented. Some statistics
are read-only (e.g. matched ﬂow entry, bytes received), but oth-
ers can be modiﬁed (e.g. packet’s output port). See OpenFlow 1.4
speciﬁcation [29, Table 5] for a detailed list of available statistics.
ploying ndb, a generalization of traceroute introduced by Net-
Sight (§2.3).
• Network Control: We also demonstrate how low-latency visi-
bility offered by TPPs enables end-hosts to control how trafﬁc is
load balanced across network paths. We refactor CONGA [1], an
in-network load-balancing mechanism implemented in Cisco’s
new ASICs, between end-hosts and a network that supports only
the TPP interface.
Hardware: To evaluate the feasibility of building a TPP-capable
switch, we synthesized and built a four-port NetFPGA router (at
160MHz) with full TPP support, capable of switching minimum
sized packets on each interface at 10Gb/s. We show the hardware
and latency costs of adding TPP support are minimal on NetFPGA,
and argue the same would hold of a real switch (§6). We ﬁnd that
the key to achieving high performance is restricting TPPs to a hand-
ful of instructions per packet (say ﬁve), as it ensures that any TPP
executes within a fraction of the its transmission time.
Software: We
in
Open vSwitch [31], which we use to demonstrate research
proposals and examples. Additionally, we present a software
stack (§4) that enforces security and access control, handles TPP
composition, and has a library of useful primitives to ease the path
to deploying TPP applications.
also implemented the TPP interface
The software and hardware implementations of TPP, scripts to
run experiments and plots in this paper, and an extended version of
this paper describing more TPP applications are all available online
at http://jvimal.github.io/tpp.
2 Example Programs
We start our discussion using examples of dataplane tasks that
can be implemented using TPPs, showcasing the utility of expos-
ing network state to end-hosts directly in the dataplane. Each of
these tasks typically requires new task-speciﬁc hardware changes;
however, we show how each task can be refactored such that the
network only implements TPPs, while delegating complex task-
speciﬁc functionality to end-hosts. We will discuss the following
tasks: (i) micro-burst detection, (ii) a rate-based congestion control
algorithm, (iii) a network troubleshooting platform, (iv) a conges-
tion aware, distributed, network load balancer.
What is a TPP? A TPP is any Ethernet packet with a uniquely
identiﬁable header that contains instructions, some additional space
(packet memory), and an optional encapsulated Ethernet payload
(e.g. IP packet). The TPP exclusively owns its packet memory, but
also has access to shared memory on the switch (its SRAM and
internal registers) through addresses. TPPs execute directly in the
dataplane at every hop, and are forwarded just like other packets.
TPPs use a very minimal instruction set listed in Table 1, and we
4(a) Visualizing the execution of a TPP as it is routed through
the network.
(b) CDF and time series of queue occupancy on 6 queues in the
network, obtained from every packet arriving at one host.
Figure 1: TPPs enable end-hosts to measure queue occupancy evolution at a packet granularity allowing them to detect micro-bursts, which
are the spikes in the time series of queue occupancy (bottom of Figure 1b). Notice from the CDF (top) that one of the queues is empty for
80% of the time instants when packet arrives to the queue; a sampling method is likely to miss the bursts.
refer the reader to Section 3 to understand the space overheads. We
abuse terminology, and use TPPs to refer both to the programs and
the packets that carry them.
We write TPPs in a pseudo-assembly-language with a segmented
address space naming various registers, switch RAM, and packet
memory. We write addresses using human-readable labels, such
as [Namespace:Statistic] or [Queue:QueueOccupancy]. We
posit that these addresses be known upfront at compile time. For
example, the mnemonic [Queue:QueueOccupancy] could be re-
fer to an address 0xb000 that stores the occupancy of a packet’s
output queue at each switch.
2.1 Micro-burst Detection
Consider the problem of monitoring link queue occupancy within
the network to diagnose short-lived congestion events (or “micro-
bursts”), which directly quantiﬁes the impact of incast.
In low-
latency networks such as datacenters, queue occupancy changes
rapidly at timescales of a few RTTs. Thus, observing and con-
trolling such bursty trafﬁc requires visibility at timescales orders of
magnitude faster than the mechanisms such as SNMP or embedded
web servers that we have today, which operate at tens of seconds at
best. Moreover, even if the monitoring mechanism is fast, it is not
clear which queues to monitor, as (i) the underlying routing could
change, and (ii) switch hash functions that affect multipath routing
are often proprietary and unknown.
TPPs can provide ﬁne-grained per-RTT, or even per-packet vis-
ibility into queue evolution inside the network. Today, switches
already track per-port, per-queue occupancy for memory manage-
ment. The instruction PUSH [Queue:QueueOccupancy] could be
used to copy the queue register onto the packet. As the packet tra-
verses each hop, the packet memory has snapshots of queue sizes at
each hop. The queue sizes are useful in diagnosing micro-bursts, as
they are not an average value. They are recorded when the packet
traverses the switch. Figure 1a shows the state of a sample packet
as it traverses a network.
In the ﬁgure, SP is the stack pointer
which points to the next offset inside the packet memory where
new values may be pushed. Since the maximum number of hops is