0.98
[0.82, 1.16]
0.01*
0.01*
0.23
0.79
Data Analyst
OR/CI
0.92
p-value
0.72
[0.59, 1.44]
1.23
[0.79, 1.9]
1.37
[0.96, 1.96]
1.24
[1.01, 1.52]
0.36
0.09
0.04*
Graphs
Share
OR/CI
1.42
[0.89, 2.27]
1.48
[0.93, 2.37]
1.12
[0.77, 1.64]
1.05
[0.86, 1.28]
p-value
0.14
0.10
0.54
0.66
OR/CI
1.25
[0.85, 1.84]
1.97
[1.35, 2.88]
1.04
[0.76, 1.42]
1.01
[0.86, 1.2]
p-value
0.27
 60% more willing to share if the risk of their
information being disclosed to the organization was lower (Low
Risk: O.R. = 1.63, 𝑝 = 0.01; No Risk: O.R. = 1.61, 𝑝 = 0.01).
On the other hand, respondents who care about whether law
enforcement would be able to access their information with a court
order and respondents who care about whether their information
might be disclosed to a third-party are both more likely to share
their information only if they are told there is no risk of their
information being disclosed to these entities. Respondents that are
told there is no such risk are about twice as likely to share their
information (Law Enforcement: O.R. = 2.07, 𝑝 < 0.01; Share: O.R. =
1.97, 𝑝 < 0.01). Being told there is a low risk instead of a high risk
of disclosure has no significant effect on their willingness to share.
We hypothesize that respondents show a graduated response to
the risk of information disclosures to the organization running the
initiative to which they might contribute their information because
it is appropriate for this organization to have their information. Sim-
ilarly, we hypothesize that respondents show a graduated response
to the risk of hacks because information disclosures resulting from
hacks are unintentional on the part of the organization. On the
other hand, the organization purposefully choosing to share infor-
mation they contributed to the initiative with a third party or with
law enforcement, even with a court order, may feel incongruent
with the purpose for which they shared their information.
Finally, we find that the probability of disclosure to data ana-
lysts or through graphs has no effect on willingness to share, even
among respondents who care about those information. We hypoth-
esize that those respondents who are motivated by the altruistic
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3043Theme
Unsubstantial
Techniques
Enables
Trust
Risk
Technical
Description
“Differential privacy is the gold standard in data privacy and protection and is widely recognized as the strongest guarantee of privacy
available.”
“Differential Privacy injects statistical noise into collected data in a way that protects privacy without significantly changing conclusions.”
“Differential Privacy allows analysts to learn useful information from large amounts of data without compromising an individual’s
privacy.”
“Differential privacy is a novel, mathematical technique to preserve privacy which is used by companies like Apple and Uber.”
“Differential privacy protects a user’s identity and the specifics of their data, meaning individuals incur almost no risk by joining the
dataset.”
“Differential privacy ensures that the removal or addition of a single database item does not (substantially) affect the outcome of any
analysis. It follows that no risk is incurred by joining the database, providing a mathematically rigorous means of coping with the fact
that distributional information may be disclosive.” [17]
Table 3: Descriptions of DP synthesized from the six main themes present in our collection of 76 in-the-wild DP descriptions.
goals specified in the scenarios may be willing to share their in-
formation regardless of the risk of disclosure occurring through
these information flows, which are arguably the most appropriate
information flows we examine, while those who are not compelled
by the goals of the organizations described in the scenarios are
similarly unwilling to share their information regardless of this
risk. For example, one respondent who cared about information
disclosure to both of these entities, and was told there was no risk
of disclosure to a data analyst and low risk of disclosure through a
graph said, “Unfortunately, I do not see enough of a benefit for me
to take the risk of sharing my personal information. I absolutely
do not want such personal info being leaked out.“ On the other
hand, a respondent who cared about disclosure to a data analyst
and was told the risk of this disclosure was higher than the risk
that their bank account would be compromised commented that
they would be willing to share their information, “because it’s for
good research, and I’m getting too old to worry about who sees my
medical record. I anticipate I will have *many* doctors, nurses, lab
techs, etc involved in my medical record before too long.”
We note that respondents who cared about information disclo-
sures to data analysts with higher internet scores were more likely
to report being willing to share their information. As technologi-
cally savvy respondents, they may have had a clearer mental model
of the data analysis process and therefore understood that data
analysts typically have complete access to user information. As
such, they may be more forgiving toward any approach that aims
to reduce this level of access, even given the relatively high risk of
an information disclosure.
5 EXPECTATIONS & WILLINGNESS TO
SHARE UNDER DP (RQ3 & RQ4)
Next – via a second survey – we explore how DP influences privacy
expectations (RQ3) and intent to share information (RQ4).
Descriptions of Differential Privacy. In order to answer these
research questions, we needed to describe DP to respondents in our
surveys. However, there is no standard description of DP we can
use. Because we want to ask our research questions in a realistic
context, we seek to describe DP to our respondents in the same
way they might encounter DP in-the-wild.
To determine how DP is described in-the-wild, we conducted a
systematic search for publicly available descriptions of DP using
keywords such as “differential privacy,” “formal privacy,” “privacy
guarantee,” and “census privacy.” We used both Google search and
searched within the past five years (2014-2019) of content in large
media venues. We continued searching until new search results
stopped appearing. We put special focus on collecting descriptions
used by industry and in the media coverage, as these descriptions
are the ones that an uninformed consumer would be most likely
to encounter. We performed this search and data collection in De-
cember 2019.4 In total, we collected 76 descriptions of DP: 36 from
industry, 30 from media outlets, and 10 from the academic literature.
The industry descriptions primarily came from companies that
use DP, including Google, Apple, Microsoft, and Uber, as well as
smaller start-ups and an investment firm. We also gathered multiple
descriptions from the U.S. Census Bureau regarding the use of DP
in the 2020 Census. The media descriptions were from large, main-
stream media outlets, such as The New York Times, Fox News, The
Washington Post, The Guardian, and Tech-Crunch. The academic
descriptions were collected from some of the most-cited papers and
books on DP, e.g., [17]. As DP is an active area of research, these
ten academic descriptions are clearly not comprehensive, but serve
as a representative example of academic descriptions.
The research team employed affinity diagramming [8] to extract
the main themes of these widely varying descriptions of differential
privacy. In affinity diagramming, the research team collaboratively
sorts pieces of content — in our case the descriptions of DP — into
themes based on affinity, with each researcher iterating over the
affinity diagram at least twice until consensus was reached on
appropriate categorization. This analysis resulted in the identifica-
tion of six main themes (names in bold): (Unsubstantial) claims
that DP is the best notion of privacy; (Techniques) explanations
4Since we conducted this survey, more companies and organizations have started
adopting and publicly writing about DP. As such, our dataset is no longer comprehen-
sive. Because this data collection informed the design of our survey, we choose not to
incorporate the newer descriptions into our dataset.
Session 11C: Software Development and Analysis CCS ’21, November 15–19, 2021, Virtual Event, Republic of Korea3044Variable
Hack
Description:
Unsubstantial
Description:
Techniques
Description:
Enables
Description:
Trust
Description:
Risk
Description:
Technical
Salary Scenario
Internet Score
OR/CI
1.94
[1.16, 3.29]
[1.17, 3.33]
1.96
1.60
[0.95, 2.73]
1.86
[1.11, 3.17]
[1.57, 4.33]
2.58
1.56
[0.92, 2.69]
1.32
[1.02, 1.71]
1.17
[1.01, 1.36]
p-value
0.01*
0.01*
0.08
0.02*
< 0.01***
0.10
0.04*
0.04*
Law Enforcement
OR/CI
p-value
1.10
0.72
Organization
OR/CI
1.13
p-value
0.59
[0.65, 1.86]
1.21
[0.72, 2.03]
1.05
[0.63, 1.77]
1.04
[0.61, 1.76]
[1.15, 3.05]
1.86
1.02
[0.60, 1.73]
0.80
[0.61, 1.05]
1.25
[1.06, 1.46]
0.47
0.84
0.89
0.01*
0.95
0.11
0.01**
[0.73, 1.75]
1.43
[0.93, 2.22]
1.40
[0.91, 2.16]
1.43
[0.92, 2.22]
1.43
[0.93, 2.20]
1.38
[0.89, 2.14]
[1.03, 1.63]
1.29
1.02
[0.89, 1.17]
Data Analyst
Graphs
Share
OR/CI
1.71
[0.92, 3.27]
2.40
[1.33, 4.5]
2.06
[1.13, 3.88]
1.99
[1.08, 3.78]
2.46
[1.37, 4.59]
2.30
[1.26, 4.33]
0.75
[0.56, 1.01]
1.05