p(S = sA(u, u(cid:48))) =
wiN (sA(u, u(cid:48))|µi, σi).
The GMM is parametrized by 6 parameters: w0, µ0, σ0, w1, µ1
and σ1. Here, w0 (w1) is the prior probability of an edge being
original (fake), i.e., w0 = P (B = 0) (w1 = P (B = 1)). The
other 4 parameters are related to the two Gaussian distributions
for edge plausibility: N (sA(u, u(cid:48))|µi, σi) for i ∈ {0, 1} is the
density function of the Gaussian distribution:
(cid:88)
i∈{0,1}
1(cid:112)2πσ2
i
exp(−
1
2σ2
i
(sA(u, u(cid:48)) − µi)).
7
Parameter Learning: To learn the 6 parameters of the GMM,
we adopt the expectation maximization (EM) algorithm, which
consists of two steps, i.e., the expectation (E) step and the
maximization (M) step. The E-step calculates, for each edge
in GA, its posterior probability of being fake or original given
its plausibility value. The M-step updates all the 6 parameters
based on the probabilities calculated from the E-step following
maximum likelihood estimation. The learning process iterates
over the two steps until convergence. Here, convergence means
that the log-likelihoods of two consecutive iterations differ less
than a given threshold (we set it to 0.001 in our experiments).
In addition,
the initial values of the 6 parameters are set
randomly.
Fake Edge Detection: After the GMM has been learned, we
compute for each edge {u, u(cid:48)
} its posterior probabilities of
(cid:80)
being original and fake:
w0N (sA(u, u(cid:48))|µ0, σ0)
(cid:80)
w1N (sA(u, u(cid:48))|µ1, σ1)
P (B = 0|sA(u, u(cid:48))) =
wiN (sA(u, u(cid:48))|µi, σi)
P (B = 1|sA(u, u(cid:48))) =
wiN (sA(u, u(cid:48))|µi, σi)
i∈{0,1}
i∈{0,1}
5678910Vectordimensions(log2(d))0.750.800.850.900.951.00AUCk-DA(k=50)k-DA(k=75)k-DA(k=100)5678910Vectordimensions(log2(d))0.900.910.920.930.940.950.960.970.98AUC5678910Vectordimensions(log2(d))0.900.920.940.960.981.00AUC5678910Vectordimensions(log2(d))0.20.30.40.50.60.70.80.9AUCSalaDP(=100)SalaDP(=50)SalaDP(=10)5678910Vectordimensions(log2(d))0.20.30.40.50.60.70.80.91.0AUC5678910Vectordimensions(log2(d))0.800.850.900.951.00AUC(a) k-DA (k = 50)
(b) k-DA (k = 75)
(c) SalaDP ( = 100)
(d) SalaDP ( = 50)
(e) SalaDP ( = 10)
Fig. 6: Plausibility distributions of fake and original edges in the NO dataset anonymized by the two anonymization mechanisms.
The result for k-DA (k = 100) is depicted in Figure 1a.
TABLE III: [Higher is better] F1 scores for detecting fake
edges using GMM and MAP estimate for both k-DA and
SalaDP on three different datasets.
k-DA (k = 50)
k-DA (k = 75)
k-DA (k = 100)
SalaDP ( = 100)
SalaDP ( = 50)
SalaDP ( = 10)
Enron
0.792
0.796
0.812
0.672
0.750
0.819
NO
0.642
0.710
0.761
0.712
0.723
0.876
SNAP
0.857
0.869
0.881
0.853
0.835
0.802
and pick the one that is maximum (MAP estimate): If P (B =
1|sA(u, u(cid:48))) > P (B = 0|sA(u, u(cid:48))), we predict {u, u(cid:48)
} to be
fake, and vice versa.
In the end, we delete all the predicted fake edges, and
obtain the recovered graph GR.
Results: We train GMMs under both anonymization mech-
anisms for all the datasets. Table III presents the results. We
ﬁrst observe that, in most of the cases, our approach achieves a
strong prediction, e.g., for the SalaDP-anonymized NO dataset
( = 10), the F1 score is 0.876. For our worst prediction on
SalaDP-anonymized Enron dataset ( = 100), the F1 score is
still approaching 0.7. Another interesting observation is that
when the privacy level increases, i.e., higher k or lower , our
prediction performance increases in most of the cases. This
can be explained by the fact that higher privacy levels lead to
more fake edges being added.
V. PRIVACY LOSS
As fake edges help an anonymized graph GA satisfy certain
privacy guarantees, we expect that, by obtaining the recovered
graph GR from GA, these guarantees will be violated. In this
section, we ﬁrst deﬁne two metrics tailored to each anonymiza-
tion mechanism for quantifying the privacy loss due to our
graph recovery attack. Then, we present the corresponding
evaluation results.
A. Privacy Loss Measurement
k-DA: k-DA assumes that the adversary only has knowledge
of her targets’ degrees and uses this knowledge to re-identify
them. This means that, if the users’ degrees in GR are more
similar to those in G compared to GA, then the adversary is
8
−0.20.00.20.40.60.81.0Edgeplausibility01234567Numberofedges×104OriginaledgesFakeedges−0.20.00.20.40.60.81.0Edgeplausibility01234567Numberofedges×104−0.20.00.20.40.60.81.0Edgeplausibility012345678Numberofedges×104−0.20.00.20.40.60.81.0Edgeplausibility0.00.20.40.60.81.01.2Numberofedges×105−0.20.00.20.40.60.81.0Edgeplausibility0.00.51.01.52.02.53.0Numberofedges×105|U|
more likely to achieve her goal. Therefore, we propose to
compute users’ average degree difference between GA and
G, as well as between GR and G, to measure the privacy
loss caused by our graph recovery. Formally, we deﬁne users’
average degree difference between GA and G as:
(cid:80)
u∈U ||κ(u)| − |κA(u)||
∆A =
and deﬁne users’ average degree difference between G and GR
(∆R) accordingly.
Note that our approach also deletes some original edges
when recovering GR (false positives). Therefore, if the adver-
sary relies on the users’ exact degrees (as assumed in k-DA) to
de-anonymize them, she might fail. However, a sophisticated
adversary can apply extra heuristics, such as tolerating some
degree differences for ﬁnding her targets. In this case, ∆R
being smaller than ∆A can still provide the adversary with a
better chance to achieve her goal.
}|{u, u(cid:48)
SalaDP: To quantify the privacy loss for SalaDP, we consider
the noise added to the dK-2 series of the original graph G.
Formally, the dK-2 series of G, denoted by D(G), is a set
with each element ri,j(G) in D(G) representing the number
of edges that connect users of degrees i and j in G. Formally,
ri,j(G) is deﬁned as:
ri,j(G) = |{{u, u(cid:48)
} ∈ E ∧ |κ(u)| = i ∧ |κ(u(cid:48))| = j}|.
Accordingly, ri,j(GA) and ri,j(GR) represent the correspond-
ing numbers in GA and GR. Then, we use ζi,j(G,GA) =
ri,j(GA) − ri,j(G) to denote the noise added to ri,j(G) when
transforming G to GA, and ζi,j(G,GR) = ri,j(GR) − ri,j(G)
to represent the (lower) noise caused by our graph recovery
attack. Since SalaDP is a statistical mechanism, we sample
t=1 by applying SalaDP to G
100 anonymized graphs {GtA}100
100 times and produce 100 noise samples {ζi,j(G,GtA)}100
for each element in D(G).
We deﬁne two metrics for quantifying the privacy loss due
to graph recovery for SalaDP. In the ﬁrst metric, we compare
the difference of average noise added to D(G) before (by
SalaDP) and after graph recovery. Concretely, for each ri,j(G)
in D(G), we ﬁrst calculate the average absolute noise added
to ri,j(G), denoted by ¯ζi,j(G,GA), over the 100 SalaDP graph
samples described above, i.e.,
t=1
(cid:80)100
t=1 |ζi,j(G,GtA)|
(cid:88)
100
.
¯ζi,j(G,GA).
¯ζi,j(G,GA) =
1
ζA =
|D(G)|
ri,j (G)∈D(G)
Then, we compute the average noise over the whole graph as:
We analogously compute the average added noise ζR after our
graph recovery attack.
For the second approach, we consider the uncertainty
introduced by the added noise. McGregor et al. explore the
connection between privacy and the uncertainty of the output
produced by differential privacy mechanisms [28]. In general,
higher uncertainty implies stronger privacy. We measure the
uncertainty of noise added by SalaDP through estimating its
9
TABLE IV: Differences in average degree between the original
graph (G), the k-DA anonymized graph (GA) and our recovered
graph (GR).
Enron
NO
SNAP
∆R
0.990
1.367
2.019
∆A
1.222
1.705
2.377
∆R
0.499
0.752
1.035
∆A
0.541
0.875
1.231
∆R
6.589
8.815
11.565
∆A
8.216
11.755
16.018
k-DA (k = 50)
k-DA (k = 75)
k-DA (k = 100)
empirical entropy. To this end, we calculate the Shannon en-
tropy over the frequencies of elements in {ζi,j(G,GtA)}100
t=1 (the
100 noise samples described above), denoted by ˆHi,j(G,GA).
Then, we compute the average entropy of the noise as follows:
(cid:88)
ˆHA =
1
|D(G)|
ri,j (G)∈D(G)
ˆHi,j(G,GA).
We compute the average entropy after our graph recovery
similarly and denote it as ˆHR.
B. Evaluation
k-DA: Table IV presents the results of the users’ degree
differences. In all cases, ∆R is smaller than ∆A. This indicates
that the adversary has a better chance to identify her targets
from GR than from GA, and demonstrates that our attack
clearly decreases the privacy provided by k-DA. It also appears
that our graph recovery gains least beneﬁts for the adversary on
the NO dataset, where ∆R is closer to ∆A. This is essentially
due to the fact that the original NO dataset already preserves
a high k-degree anonymity.
SalaDP: Table V presents the average noise added to the dK-2
series of the original graph with respect to the anonymized and
recovered graphs. We observe that, in all cases, ζR is smaller
than ζA showing that our recovery attack reduces the average
noise for SalaDP. We also observe that the relative reduction of
the average noise with our graph recovery in general decreases
when increasing : The added noise is already much smaller
for larger  and cannot be further reduced.
Table VI presents the average entropy of the noise added
to the dK-2 series of the original graph after applying SalaDP
and after the graph recovery attack. Note that, while one would
expect higher entropy for smaller values of , this does not
hold true in practice because the SalaDP mechanism is not
necessarily optimal with respect to the added uncertainty. Still,
across all values of  and all the datasets, we can observe a
reduction of the empirical entropy, and therefore a reduction of
the privacy provision. The relative reduction, however, varies
between the values of  and, as for the average noise above,
between the datasets.
For now, it seems unclear how these various factors impact
the relative reduction of empirical entropy. Analyzing the im-
pact of these parameters on the relative reduction of empirical
entropy could provide further insights into the recoverability
of anonymized graphs. Such work is, however, orthogonal to
the work presented in this paper and could be an interesting
direction for our future work.
TABLE V: Differences in average noise on the original graph
G’s dK-2 series by SalaDP (ζA) and by our graph recovery
attack (ζR).
Enron
NO
SNAP
ζR
4.432
4.224
4.958
ζA
5.282
7.121
12.004
ζR
6.048
7.731
7.982
ζA
6.415
9.471
16.033
ζR
3.422
3.489
3.672
ζA
4.018
4.445
5.690
SalaDP ( = 100)
SalaDP ( = 50)
SalaDP ( = 10)
TABLE VI: Differences in average entropy of the noise on the
original graph G’s dK-2 series by SalaDP ( ˆHA) and by our
graph recovery attack ( ˆHR).
Enron
NO
SNAP
ˆHR
0.180
0.556
1.095
ˆHA
2.029
1.865
1.381
ˆHR
1.243