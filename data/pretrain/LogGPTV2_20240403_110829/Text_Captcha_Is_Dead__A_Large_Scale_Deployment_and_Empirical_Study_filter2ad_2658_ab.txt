nel to observe and understand the real attack model. To address
the above challenges, we incorporate the idea of model extraction
into constructing the substitute model. In detail, we uniquely view
the answers that the attackers submit as queries to their solving
models. Then, we use these queries to extract the real attack model.
It is worthy noting that we do not directly construct our substitute
model from these queries considering the efficiency issues of DNN
(recall Section 2.1). Instead, we first train a solving model by using
pre-collected captchas and the corresponding labels, then employ
such model as a baseline substitute model to generate adversarial
captchas, and finally transfer them to defend against the real attack
model. Once an update of the attack models is observed, we can
query the attack models and fine-tune the baseline substitute model
by leveraging these queries. As a result, we update our substitute
model dynamically to accommodate the evolving models of the
attackers.
2.4 Comparison with Prior Work
We first compare our method with closely related work [29, 35] in
Table 2. In [29], Osadchy et al. proposed a method to generate Im-
mutable Adversarial Noise (IAN), which is specifically designed for
image captchas. It is easier to generate adversarial image captchas
compared to adversarial text captchas, since the perturbation space
of image captchas is much bigger. Technically, the idea of IAN
is simple: injecting enough perturbations to prevent existing im-
age filtering techniques. In [35], Shi et al. proposed an adversarial
captcha generation and evaluation system, aCAPTCHA, which can
be applied for text captchas. The core idea of aCAPTCHA is to
inject perturbation into the frequency domain instead of the space
domain. We note that aCAPTCHA does not directly inject pertur-
bations into captchas. Instead, it first injects perturbations into a
single character image, and then combines different character im-
ages into one captcha, which would limit the efficiency of captchas
in practice.
Based on our analysis above, we can summarize the main differ-
ence between advCAPTCHA and [29, 35] as below. 1) Benefited by
3
Table 1: Summary of recent works on solving captchas.
Captcha Solver
Yan et al. [43]
Bursztein et al. [6]
Gao et al. [15]
Bursztein et al. [5]
Gao et al. [16]
Ye et al. [45]
Year
2007
2011
2013
2014
2016
2018
Preprocessing
none
image processing
image processing
none
none
Segmentation
vertical segmentation
snake segmentation
color filling segmentation
color filling segmentation
cut point detector
Log-Gabor Filter
GAN
none
Recognition Method
pattern recognition
Training Data
none
SVM
KNN
CNN
KNN
KNN
CNN
CNN
real captchas
real captchas
real captchas
real captchas
real captchas
synthetic captchas
Table 2: Comparison of Adversarial Captchas.
Method
IAN [29]
aCAPTCHA [35]
advCAPTCHA
Captcha Scheme
image captcha
text captcha
image captcha
text captcha
space
frequency
space
space
Perturbation Domain
Substitute Model Adaptivity
CNN
CNN
CNN
CRNN
no
no
yes
Practical Deployment
no
no
yes
using an end-to-end solver (recognizing captchas without segmen-
tation), our method can directly inject perturbations into the whole
captcha, which is more efficient in practice; 2) To the best of our
knowledge, we are the first to consider the evolving behaviors of at-
tackers. By incorporating the idea of model extraction, we employed
the answers of the attack model to make our substitute model bet-
ter fit the actual attack model; 3) We are also the first to deploy
adversarial captchas on a large-scale online platform. By applying
adversarial learning techniques in a novel manner, advCAPTCHA
can generate effective adversarial captchas to significantly reduce
the success rate of attackers, which has been demonstrated by a
large-scale online study. Furthermore, we also validate the feasibil-
ity of advCAPTCHA in practical use, as well as its robustness in
defending against various attacks.
Our proposed perturbation algorithms in advCAPTCHA are also
novel as compared with previous perturbation methods. 1) Com-
pared to previous methods that only construct the substitute model
by the training dataset, our method can fine-tune the substitute
model to approximate the real attack model, which is more effec-
tive and efficient in generating high-quality adversarial captchas; 2)
Compared to the gradient estimation method that requires a large
number of queries for every adversarial example, our method only
needs a small number of queries to fine-tune the substitute model,
based on which we can generate unlimited adversarial captchas; 3)
Previous methods in model extraction, which aim to construct the
approximate model, cannot be applied to extract DNNs, especially
when only the label information is available. In comparison, our
method starts by building an existing substitute model that per-
forms a similar task with the target model and uses the queries to
make the existing model similar to the target model. Therefore, our
method is more feasible in practice.
4
3 DESIGN OF ADVCAPTCHA
In this section, we describe the design details of advCAPTCHA as
an effective defense against deep learning based captcha solvers.
3.1 Overview
We provide the workflow of advCAPTCHA in Figure 1, which
consists of the following 4 important steps.
Step 1. Training the substitute solving model. In practice,
we usually have no prior knowledge of the attack model, thus
we need to train an alternative solving model and employ it as a
substitute for the real attack model. Then, the substitute model can
be leveraged to generate adversarial captchas to defend against the
captcha solving attacks. There are two factors we need to concern in
this step: model structure and training data. For the model structure,
we choose a model which can simultaneously recognize all the
characters in a captcha picture without image preprocessing and
character segmentation. For the training data, we consider multiple
different captcha schemes, in order to enhance the generalizability
of our substitute model. With these data and model structures, we
can train a substitute solving model. This process is detailed in
Section 3.2.
Step 2. Generating adversarial captchas. With the substitute
solving model in place, we carefully design perturbation algorithms
to generate adversarial captchas. Motivated by the evasion attacks
developed in the domain of image classification, we propose sev-
eral perturbation algorithms which are applicable for perturbing
captchas. These algorithms can achieve a good tradeoff between
security and usability. After adversarial captchas are generated, we
can use them to defend against ML based attacks. This process is
described with more details in Section 3.3.
Step 3. Querying the attack model. This step is optional which
is only conducted when there is a need to fine-tune the substitute
model (e.g., an update of the attack model is observed). In this step,
we aim to query the attack models by collecting the captchas sent to
Figure 1: Workflow of advCAPTCHA.
them and the answers they submitted. The key issue in this step is
how to exactly distribute captchas to the attackers. Luckily, we can
take advantage of the existing user risk analysis system [36], where
users who are identified as abnormal users with high confidence
are classified as attackers. Finally, we distribute the captchas to
these attackers and collect their answers which will be used in the
next step. This process is detailed in Section 3.4.
Step 4. Fine-tuning the substitute solving model. Motivated
by the model extraction attacks, we propose a novel mechanism
to fine-tune our substitute model to approximate the real attack
model. Specifically, we fine-tune the substitute model by using a
set of labeled captchas that are collected from step 3. Here, we
view the answers submitted by the attackers as queries to the at-
tack model, and each answer conveys partial information of the
attack model. Model extraction allows us to fine-tune our substitute
model to approximate the attack model automatically. In addition,
compared to constructing a new substitute model from scratch, fine-
tuning the existing substitute model is more efficient. Leveraging
this fine-tuning step, we can effectively adapt our defense system
after observing an update of the attackerâ€™s model. This process is
described in detail in Section 3.5.
3.2 Constructing Substitute Solving Model
3.2.1 Model Structure. Our captcha solving model aims to di-
rectly recognize the captcha images, which is different from many
previous works [5, 16] that solve text-based captchas by follow-
ing three steps: preprocessing, segmentation and recognition. We
use an end-to-end recognition model, where the input is a captcha
image without preprocessing and the output is the sequence of
characters in the image. Specifically, we employ a Convolutional
Recurrent Neural Network (CRNN) [33] as our captcha solving
model, which requires less expert involvement and is more con-
venient for generating adversarial captchas. We defer the detailed
structure of CRNN to Appendix A for brevity. Generally, this model
first extracts features from the input image, then auto-segments
features (full captcha) and recognizes each segmentation (single
character), and finally seeks the best combination. This process can
significantly reduce the amount of data required to train the model.
In our evaluation, we can use 100K samples to train a solver with
92% accuracy for the four-character captcha scheme (which has 624
types of label sequences).
3.2.2 Training the Substitute Model. Based on the CRNN model
in Section 3.2.1, we train a substitute solving model for the target
captcha scheme deployed on real world websites. Note that we
aim to build a substitute model which is close to the actual attack
model that may be trained by the captchas generated from mul-
tiple captcha schemes. Therefore, we train the substitute solver
by leveraging the target captcha scheme, as well as other captcha
schemes. To collect different captcha schemes, we can either crawl
from different websites or generate by open source softwares. Then
we can use these mixed data as the training dataset. Each training
sample consists of a captcha image (without preprocessing) and an
integer vector that stores the character IDs of the captcha. Note
that we assign a unique ID to each candidate character of the target
captcha scheme. The trained substitute model can then be applied
to generate adversarial captchas.
3.3 Generating Adversarial Captchas
3.3.1 Notations. We first present necessary notations in the
context of generating adversarial images. We represent a solver as
a function FÎ¸(x) = y, where FÎ¸ is the neural network, x âˆˆ RnÃ—m
is the input image and y âˆˆ Rr is the corresponding output. Note
that for the CRNN model, y is a label sequence and r is the number
of characters in the captcha. Define xâ€² âˆˆ RnÃ—m as the adversarial
image and let L(Î¸, x, y) be the objective function of FÎ¸ . As in [31][8],
we use Lp norm to measure the similarity between x and xâ€², which
is defined as Lp = ||x âˆ’ xâ€²||p = (n
i, j|p)1/p.
m
j=1 |xi, j âˆ’ xâ€²
i =1
3.3.2 Perturbation Algorithm. Recently, many evasion attacks
have been proposed to generate adversarial images [4, 46]. However,
these methods cannot be directly applied to perturb captchas, based
on the following two reasons. 1) Previous methods for perturbing
images focus on classification models whose prediction contains
the probability of the input for all labels. Then, they can rely on
these confidence information to generate adversarial examples,
e.g., to increase the probability of the target label or to decrease the
probability of the ground truth label. In comparison, for captcha, we
leverage an end-to-end model which uses a decode step to compute
the output. In particular, our method utilizes the CTC loss to predict
the probability of the input for a given label sequence. Thus, in our
method, we generate adversarial captchas by decreasing the CTC
loss of the ground truth label sequence. 2) We need to pay special
5
 Step 1: Training the Substitute Solving Model-End-to-end model structure-Mixed training data Step 2: Generating Adversarial Captchas-Generate adversarial perturbations-Inject them into captchas Step 4: Fine-tuning the Substitute Solving Model-Fit the query data-Maintain accuracy in originaltraining dataQuery ResultsVarious Captcha SchemesCaptcha Attack Models Step 3: Querying the Attack Model-Distribute high risk users captchas-Collect their answersDeployed CaptchasAdversarial CaptchasSubstitute Solving Modelattention to restrict the location of perturbations injected into a
captcha to maintain its high usability. In particular, we use a mask
to control the position where the perturbations to be injected and
the perturbation can be calculated as
xâ€² = x + Ïµ Â· ||âˆ†xL(Î¸, x, y) Â· M||p
(1)
where M âˆˆ RnÃ—m is a 2D matrix named as mask, deciding which
area of the original captcha the perturbations need to be injected
into. M is a binary matrix where noise can only be injected to the
pixel of (i, j) if Mi, j = 1. We can easily choose an appropriate M
(such as background areas) to maintain the usability of the adver-
sarial captchas. Ïµ is a constant which controls the total amount of
injected perturbations. Obviously, a higher Ïµ requires more per-
turbations to be added to the captchas. This method computes the
adversarial perturbation by only one step following Equation 1,
which is thus named as a direct method.
We also consider the iterative method to generate adversarial
captchas. This method iteratively takes multiple small steps (as
shown in Equation 2) to compute the adversarial perturbation while
adjusting the direction after each step. Compared to the direct
method, the iterative method can result in more misclassification
(i.e., more robust captchas from the defense perspective) under the
same perturbation level [23].
xt +1 = xt + Î± Â· ||âˆ†xL(Î¸, x, y) Â· M||p
(2)
where Î± is a constant to control the amount of injected perturba-