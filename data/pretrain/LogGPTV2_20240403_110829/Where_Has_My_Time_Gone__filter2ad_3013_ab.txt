every pair of consecutive reads. Results are saved into previously allocated and
initialized buﬀers, and access to the buﬀers is outside the measurement mainline.
This test is conducted in three diﬀerent modes: ﬁrstly, Kernel Cold Start
(1a) which serves as our approximation of a bare metal test. Kernel Cold Start
measures very early within the kernel boot process, before the scheduler, mul-
tiprocessing and multicore support have been started. The second test, Kernel
Test (1b), runs from within the kernel, and represents an enhanced version of
the recommended test described in [10]. The third test, User Space Test (1c),
provides high-accuracy time stamping measurement from within a user-space
application. The application is pinned to a single CPU core and all other tasks
and interrupts are moved to other cores. This is representative of real-time appli-
cation operation. In contrast to the Kernel Test, interrupts, such as scheduling
pre-emption, are not disabled so as to represent the runtime conditions of real
applications.
User Space + OS Latency (2). This experiment investigates the combined
latency of the (user-space) application and the operating system. The test sets up
two processes and opens a datagram socket between them, measuring the round
trip time (RTT) for a message sent from a source process to the destination
process, and back. TSC is used to measure the latency and the time is measured
by reading TSC before and after the message reply is received. While this does
not fully exercise the network stack, it does provide useful insight into the kernel
overhead.
Virtualized Environment (1d). The contribution of a virtualized environ-
ment is examined by repeating the TSC tests from within a Virtual Machine
(VM). We used VirtualBox [9] version 4.3.36 as the hypervisor, with an Ubuntu
VM (same version as the base OS). The VM was conﬁgured to run the guest
OS on a single dedicated CPU core with no co-located native operating system
activities.
Host Interconnect (3). To evaluate the latency of the host interconnect (e.g.,
PCI-Express), we used the NetFPGA SUME platform [16], which implements x8
206
N. Zilberman et al.
PCIe Gen3 interface. The DMA design is instrumented to measure the intercon-
nect latency. As the hardware and the processor use diﬀerent clock sources,
the one-way latency can not be directly measured. Instead, the round trip
latency of a read operation (a non-posted operation that incorporates an explicit
reply) is measured. Every read transaction from the NetFPGA to the CPU is
timestamped at 6.25 ns accuracy within the DMA engine when each request is
issued and when its reply returns. The cache is warmed before the test, to avoid
additional latency due to cache misses, and the memory address is ﬁxed. The
measured latency does not account for driver latency, as neither the driver nor
the CPU core participate in the PCIe read transaction.
Host Latency (4). To measure the latency of an entire host we use a bespoke
request-reply test to measure the latency through the NIC, PCIe Interconnect,
Kernel and network stack, the application level, and back to the NIC. Contrast
to the User Space + OS Latency experiment, here packets traverse the networks
stack only once in each direction. Packets are injected by a second host, and
using the DAG card we isolate the host latency, measuring the latency from the
packet’s entrance to the NIC and until it returns from the NIC.
Kernel Bypass Inﬂuence (5). Kernel bypass is promoted as a useful method-
ology and we consider the latency contribution of the operating-system kernel
alone and the impact of kernel-bypass upon latency. Using tests comparable to
those of Host Latency experiment we can then measure latency using the kernel
bypass supported by our NICs (X10, SFN8522). Our performance comparison
contrasts the kernel with bypass enabled and disabled.
2.3 Client-Server Latency (6)
Experiments are extended from single-host (and, where appropriate, hardware
request-reply server) to a pair of network-hosts as shown in Fig. 3. The two
servers are directly connected to each other. Using a test method based upon
that described in the Host Latency experiment, we add support for request-
reply at both hosts. This allows us to measure latency between the user-space
application of both machines. We further extend this experiment to measure
the latency of queries (both get and set) under the Memcached benchmark,
indicative of realistic user-space application latency.
2.4 Network Latency
We measure three components that contribute to network latency: networking
devices within the network, cabling (e.g., ﬁber, copper), and networking devices
at the edge. The network device at the edge is represented in this study by the
NIC. For networking devices within the network we focus on electrical packet
switches (EPS) as the most commonly used networking devices within data
center today. Networking devices such as routers will inherently have a latency
that is the same or larger than a switch, thus we do not study them speciﬁcally.
Where Has My Time Gone?
207
Our focus in this work is on the minimum latency components within a
system. We therefore do not evaluate latency components of networking devices
such as queueing and buﬀering or congestion. We consider these out of scope in
our attempt to understand the most-ideal latency situation.
Cabling. The propagation delay over a ﬁber is 4.9 ns per meter, and the delay
over a copper cable varies between 4.3 ns and 4.4 ns per meter, depending on the
cable’s thickness and material used. We corroborate these numbers by sending
packet trains over varying lengths of cable and measuring using DAG the latency
between transmit and receive6. In our reported tests we use ﬁber exclusively.
NIC Latency (7). Measuring NIC-latency is a subtle art. At least three compo-
nents contribute to a typical NIC latency ﬁgure: the NIC’s hardware, the Host
Bus Adapter (a PCI-Express interconnect in our case) and the NIC’s driver.
There are two ways to measure the latency of a NIC: the ﬁrst is injecting pack-
ets from outside the host to the NIC, looping the packets at the driver and
capturing them at the NIC’s output port. The second is injecting packets from
the driver to the NIC, using a (physical or logical) loopback at the NIC’s ports
and capturing the returning packet at the driver. Neither of these ways allows
us to separate the hardware-latency contribution from the rest of its latency
components or to measure one way latency. Acknowledging these limitations, we
opt for the second method, injecting packets from the driver to the NIC. We
use a loopback test provided by Exablaze with the X10 NIC7. The test writes
a packet to the driver’s buﬀer, and then measures the latency between when
the packet starts to be written to PCIe and when the packet returns. This test
does not involve the kernel. A similar open-source test provided by Solarﬂare
as part of Onload (eﬂatency), which measures RTT between two nodes, is used
to evaluate SFN8522 NIC. The propagation delay on the ﬁber is measured and
excluded from the NIC latency results.
Switch Latency (8). We measure switch latency using a single DAG card to
timestamp the entry and departure time of a packet from a switch under test.
The switch under test is statically conﬁgured to send packets from one input
port to another output port. No other ports are being utilized on the switch
during the test, so there is no crosstalk traﬃc. We vary the size of the packets
sent from 64B to 1514B.
We evaluate two classes of switches, both of them cut-through switches: an
Arista DCS-7124FX layer 2 switch, and an ExaLINK50 layer 1 switch. The
latency reported is one way, end of packet to end of packet.
Caveats: Latest generation cut through switching devices, such as Mellanox Spec-
trum and Broadcom Tomahawk, opt for lower latency than we measure, on the
order of 330 ns. We were not able to obtain these devices. As a result, later dis-
cussion of these, as well as of large store-and-forward spine switches (e.g., Arista
7500R) relies on results taken from vendors’ datasheet and industry analysis [15].
6 We note that the resolution of the DAG of 7.5 ns puts short ﬁber measurements
within this range of error.
7 The source code for the test is provided with the NIC, but is not open source.
208
N. Zilberman et al.
3 Latency Results
The results of the experiments described in Sect. 2 are presented in Table 1. The
accuracy of time-measurements in kernel space, user space, or within a VM is
on the order of tens of CPU clock cycles (approximately 10 ns in our system).
Any operation beyond that is on the order of between hundreds of nanoseconds
and microseconds. To better understand this, Fig. 4 shows the relative latency
contribution of each component. The ﬁgure makes it clear that there is no sin-
gle component that contributes overwhelmingly to end-host latency: while the
kernel (including the network stack) is certainly important, the application level
also makes signiﬁcant contribution to latency as, even in our straightforward
evaluation example, applications incur overheads due to user-space/kernel-space
context switches.
Deriving the latency of diﬀerent components within the network is not as
straightforward as within the host, and depends on the network topology.
To illustrate this impact we use four typical networking topologies, depicted
in Fig. 6, combined with the median latency results reported in Table 1. Repre-
senting the store-and-forward spine switch we use the latency of Arista-7500R
switch. Figure 5 shows the relative latency contribution within each network
topology.
While diﬀerences in latency contribution here are enormous, just as in the
end-host case single huge contributor to network latency. Furthermore, the
Table 1. Summary of latency results.
Experiment
Minimum Median 99.9
th
Tail
Observation period
1a TSC - Kernel Cold Start
1b TSC - Kernel
1c TSC - From User Space
7 ns
9 ns
9 ns
1d TSC - From VM User Space
12 ns
2a User Space + OS (same core) 2 µs
2b User Space + OS (other core) 4 µs
3a Interconnect (64B)
3b Interconnect (1536B)
4 Host
5 Kernel Bypass
6a Client-Server (UDP)
6b Client-Server (Memcached)
7a NIC - X10 (64B)
7b NIC - SFN8522 (64B)
552 ns
976 ns
3.9 µs
895 ns
7 µs
10 µs
804 ns
960 ns
8a Switch - ExaLINK50 (64B)
a
0
a
8b Switch - ExaLINK50 (1514B) 0
7 ns
9 ns
10 ns
12 ns
2 µs
5 µs
7 ns
9 ns
11 ns
13 ns
2 µs
5 µs
11 ns
6.9 µs
49 µs
64 ms
68 µs
31 µs
1 h
1 h
1 h
1 h
10 M messages
10 M messages
572 ns
592 ns
608 ns
1 M transactions
988 ns
4.5 µs
946 ns
9 µs
13 µs
1020 ns
21 µs
1096 ns
107 µs
240 µs
834 ns
834 ns
985 ns
a
2.7 ns
a
2.7 ns
1047 ns
a
17.7 ns
a
17.7 ns
1028 ns
45 µs
5.4 µs
203 µs
20.3 ms
10 µs
3.3 µs
a
a
17.7 ns
17.7 ns
1 M transactions
1 M packets