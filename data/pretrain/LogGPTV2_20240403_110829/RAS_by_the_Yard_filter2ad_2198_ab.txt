33% 
14% 
14% 
13% 
19% 
19% 
18% 
100% 
100%  107% 
*Total  memory  is  actually  0.59  PB,  but  0.12  PB  is  for 
redundancy rather than for application use 
4. Checkpoints 
Checkpointing  [3,6]  is  a  useful  feature  for  both 
availability  and  minimizing  human  programming 
effort.  A  checkpoint  is a  saved  application  or  system 
state  from  which  computation  can  be  restarted.  At 
certain times, the system saves its entire memory and 
operational state (system checkpointing) or appropriate 
portion  thereof  (application  checkpointing).  In  this 
paper,  a  simplistic  model  of  system  checkpointing  is 
used 
tradeoffs.  
References  3  and  6  and 
large  body  of 
checkpointing  literature  can  be  consulted  for  more 
complex models.   
performability 
demonstrate 
the 
to 
the 
time 
from 
the  computation 
Checkpointing  has  two  phases  –  (1)  saving  a 
checkpoint  and  (2)  checkpoint  recovery  following  a 
failure. To save a checkpoint, the memory and system 
state  necessary  to  recovery  from  a  failure  is  sent  to 
storage.  Checkpoint  recovery  involves  restoring  the 
system  state  and  memory  using  the  checkpoint  and 
restarting 
the 
checkpoint  was  taken.  The  time  lost  to  doing  useful 
computation  is  the  overhead  time  required  to  save  a 
checkpoint,  the  time  required  to  restore  a  checkpoint 
after  a  failure,  and  the recomputation time  to  replace 
the  computation  that  had  been  performed  after  the 
checkpoint  but  before  the  failure.  This  lost  time 
contributes to application unavailability, but is a much 
better  solution  than  restarting  the  job  after  every 
failure. 
The  petascale  supercomputer  described  in  this 
paper has 0.5 PetaBytes of data that needs to be saved 
for  a  complete  system  checkpoint.  Since  the  disk 
bandwidth of 1 TB/sec is less than the I/O bandwidth 
of 2 TB/sec, it is the limiting factor in the time required 
to  take  a  checkpoint.  The  time  required  to  dump  a 
checkpoint to disk is 500 seconds (0.5PB at 1 TB/sec). 
It is possible to significantly reduce this time by saving 
a  checkpoint 
the  computation 
continues on the machine, the RAM checkpoint can be 
copied  onto  disk,  to  provide  a  more  robust  backup. 
Reference [6] describes this architecture in more detail 
and  provides  Markov  models  for  determining  the 
resulting  availability  and  optimizing  the  checkpoint 
parameters.  
5. Checkpoint Cost/Benefit Analysis 
to  RAM.  While 
Given  a  set  of  system  parameters,  maximizing 
application  availability  is  achieved  by  selecting  the 
time between checkpoints that optimally  balances the 
overhead  of  taking  a  checkpoint  vs.  the  amount  of 
recomputation  required  after  a  checkpoint  recovery. 
The  time  to  take  a  checkpoint  is  the  most  important 
parameter  in  that  optimization.  Ignoring  checkpoint 
preparation time, which is usually small, time to take a 
disk  checkpoint  is  a    function  of  I/O  and  disk 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:37:45 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007the 
increase 
bandwidth. I/O bandwidth in Table 1 is twice the disk 
bandwidth in Table 1, so disk bandwidth is the limiting 
factor.  Figure  1  shows  application  availability  as  a 
function  of  disk  bandwidth  for  a  1  PetaFLOPS 
supercomputer  with  the  system  balance  parameters 
shown  in  Table  1.  Since  disk  bandwidth  is  the 
checkpoint bottleneck and is limited by spindle speed, 
doubling this bandwidth requires twice as many disks 
and  disk  controllers,  which  costs  an  extra  19%  per 
Table  2.  Note  that  varying  checkpoint  bandwidth 
provides  a  continuum  of  availability  vs.  cost  –  the 
RAS by the yard concept.  
Disk bandwidth is increased by purchasing more 
disk  drives  and  controllers.  For  a  fixed  budget, 
purchasing  more  disk  resources  means  decreased 
system  performance  since  less  money  is  allocated  to 
other  compute  resources  (see  Table  3  for  example 
calculations).  Thus,  the  increase  in  disk  bandwidth 
shown  in  Figure  1  comes  at  the  cost  of  decreased 
system performance as shown in Figure 2. This figure 
shows 
in  application  availability, 
measured  on  the  left  axis,  and  the  corresponding 
decrease  in  performance  caused  by  purchasing  fewer 
non-disk  resources,  measured  on  the  right  axis.  In 
Figure  2,  the  application  availability  increase  as  a 
function of disk bandwidth is slightly higher than that 
shown in Figure 1 because the rebalanced system has 
slightly  less  compute  hardware,  and  hence  a  slightly 
higher MTTF.  
Figure  3  is  the  same  graph  as  Figure  2  for  a 
system 
the  fifth  memory  channel 
described in Section 3. The application availability is 
higher  in  Figure  3  since  the  fifth  memory  channel 
improves  the  MTTF,  but  the  performance  of  the 
rebalanced system is lower due to the extra cost of the 
fifth  memory  channel.  Note 
initial 
performance  for  a  disk  bandwidth  of  1  TB/sec  in 
Figure 3 is less than 1 PetaFLOPS because the system 
has been rebalanced for the fifth memory channel (see 
the last column in Table 3).  
A  similar rebalancing analysis  can  be  performed 
for  RAM  checkpoints  as  shown  in  Table  4.  A  full 
system  RAM  checkpoint  requires  an  extra  0.5 
PetaByte  of  RAM.  Application  availability  improves 
from  75%  for  disk  checkpoints  to  92%  for  RAM 
checkpoints  with  a  28%  increase  in  system  cost  to 
purchase  twice  as  much  memory.  The  cost  for  the 
RAM checkpoint is shown in Table 4 along with the 
fixed budget alternative system. The increased amount 
of  the  fixed  budget  allocated  to  memory  causes  the 
performance  to  decrease  from  1  PetaFLOPS  to  0.77 
PetaFLOPS.   
includes 
that 
that 
the 
y
t
i
l
i
b
a
l
i
a
v
A
n
o
i
t
a
c
i
l
p
p
A
82.00%
81.50%
81.00%
80.50%
80.00%
79.50%
79.00%
78.50%
78.00%
77.50%
77.00%
76.50%
76.00%
1
1.2
1.1
1.4
Disk Bandwidth (TB/sec)
1.3
1.5
Figure  1.  Application  Availability  vs.  Disk 
Bandwidth 
Rebalanced 
PetaFlops
Availability
y
t
i
l
i
b
a
l
i
a
v
A
82.0%
81.0%
80.0%
79.0%
78.0%
77.0%
76.0%
l
a
n
c
e
d
P
e
1 R
e
0.99
b
a
0.98
0.97
0.96
0.95
0.94
0.93
0.92
0.91
0.9
a
F
L
O
P
S
t
1
1.1
1.2
1.3
1.4
1.5
Disk Bandwidth (TB/sec)
Figure  2. 
Rebalanced PetaFLOPS vs. Disk Bandwidth 
  Application  Availability  and 
Rebalanced 
PetaFlops
Availability
87.5%
87.0%
86.5%
86.0%
85.5%
85.0%
84.5%
84.0%
83.5%
y
t
i
l
i
b
a
l
i
a
v
A
1.2
1.1
1
Disk Bandwidth (TB/sec)
1.3
1.4
1.5
l
0.925 R
e
0.92
b
a
0.915
0.91
0.905
0.9
0.895
0.89
0.885
0.88
a
n
c
e
d
P
e
a
F
L
O
P
S
t
Figure  3. 
  Application  Availability  and 
Rebalanced  PetaFLOPS  vs.  Disk  Bandwidth 
for a System with a Fifth Memory Channel 
Figure 4 shows the same graph as Figure 3 for a 
system  with  a  RAM  checkpoint  instead  of  a  fifth 
memory channel. In this case, disk bandwidth does not 
have  nearly  as  much  impact  on  availability  as  in  the 
earlier  examples  because  the  disk  checkpoint  is  a 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:37:45 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007backup to the RAM checkpoint, and most failures can 
be  recovered  using  the  RAM  checkpoint.  Increased 
disk  bandwidth  does  have  some  effect  because  it 
allows the RAM checkpoint to be stored more quickly 
on disk, meaning that RAM checkpoints can be more 
frequent  and  that  there  will  be  a  more  current  disk 
checkpoint if a failure occurs while a RAM checkpoint 
is being taken. 
Table 4. Rebalanced System with a RAM 
Checkpoint 
Cost 
Orig-
with 
inal 
RAM 
Cost 
Check-
point 
39% 
39% 
30% 
Rebal-
anced 
Cost 
New System 
Parameters
0.77 
PetaFLOPS 
0.39 
PetaBytes* 
0.09 PB/sec 
1.55 TB/sec 
I/O 
0.77 TB/sec 
Disk 
Sub-
system 
System 