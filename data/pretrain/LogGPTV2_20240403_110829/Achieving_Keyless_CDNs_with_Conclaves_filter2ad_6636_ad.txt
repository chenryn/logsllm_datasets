We implement three versions of shared memory. Each
stores a canonical replica of the shared memory at a known
location (either a particular server or ﬁle). Upon locking a
ﬁle, the client “downloads” the canonical replica and updates
its internal memory maps. On unlock, the client copies its
replica to the canonical.
• sm-vericrypt-basic uses an enclaved server to keep the
canonical memory ﬁles in an in-enclave red-black tree.
• sm-vericrypt implements a memory ﬁle as two untrusted
host ﬁles: a mandatory lock ﬁle, and an optional segment
ﬁle. When a client opens a memory ﬁle, the sm-vericrypt
server creates the lock ﬁle on the untrusted host, and the
Graphene client maps (MAP FILE|MAP SHARED) the lock
ﬁle into untrusted memory. The client then constructs a
3Graphene does not have a uniﬁed ﬁlesystem and memory subsystem,
and thus munmap is not currently available as a ﬁlesystem operation.
742    29th USENIX Security Symposium
USENIX Association
ticketlock structure over this untrusted shared memory.
Since the untrusted host may manipulate the ticketlock’s
turn value, a shadowed, trusted turn number is maintained
by the enclaved sm-vericrypt server. After the client has
acquired the lock, the client makes an RPC to the server
to verify the turn number. The server thus acts as a trusted
monitor of the untrusted monotonic counter.
If a client mmaps the memory ﬁle, the server creates the
associated segment ﬁle on the untrusted host. When the
client subsequently locks the ﬁle, the client makes a lock
RPC to server, which returns the keying and MAC tag in-
formation for the segment. The client copies the untrusted
memory segment into the enclave, and uses AES-256-
GCM to decrypt and authenticate the data. When a client
unlocks the ﬁle, the client generates a new IV, copies an
encrypted version of its in-enclave memory segment into
the untrusted segment ﬁle, and makes an unlock RPC to
the server, passing along the new IV and MAC tag.
• sm-crypt assumes the untrusted host does not tamper with
data. As such, sm-crypt uses AES-256-CTR instead of
AES-256-GCM, and does not need an enclaved server to
monitor the integrity of the ticketlock and IV.
keyserver We implement the keyserver as two components:
the keyserver proper, and an OpenSSL engine (“Engine” in
Figure 1) that the application loads as a shared library; the en-
gine proxies private key operations to the keyserver. Unlike
the fsserver and memserver clients, the key client operates at
the application layer, outside of Graphene.
OpenSSL’s engine API requires the caller (in our case,
NGINX) to provide an RSA object, which contains the se-
cret key. To avoid having to expose the key, we modiﬁed
OpenSSL to populate RSA objects with dummy keys that in-
stead serve as identiﬁers that the keyserver uses to look up
the real keys it stores securely.
To reduce the number of connections and avoid a depen-
dency on the memserver for lock ﬁles, our engine main-
tains the property that all keys for the same keyserver, within
the same process, share a single connection. This requires
that the engine detect forking by the application, which we
achieve by also associating process IDs with the RSA objects.
timeserver We modify the Graphene system call handlers
for getttimeofday, time, and clock gettime to option-
ally proxy application calls to a remote, trusted, timestamp
signing server. The use of such a timeserver, and the related
parameters, such as the timeserver’s public key, are speciﬁed
by the Graphene user (here, the content provider), and hard-
coded into Graphene’s conﬁguration. As a freshness guaran-
tee, each request includes a new, random nonce, generated by
the Graphene system call handlers. The timeserver, in turn,
returns an RSA signature over a message consisting of the
current time concatenated with this nonce.
Our timeserver approach resembles Google’s roughtime
protocol [68]; future work would fully port the roughtime
protocol to Graphene to reduce the need for a trusted time-
server by instead tolerating some fraction of misbehaving
servers. Note, however, that, in the SGX setting, both our ap-
proach and roughtime are best efforts; an untrusted host that
identiﬁes the trafﬁc between the Graphene client and time-
server could, for instance, “slow down” time by delaying the
responses.
5.2 Graphene Modiﬁcations
We have modiﬁed Graphene to add missing functionality and
increase performance.
Exitless System Calls
For potential performance gains,
we merge Graphene’s exitless system call patch [69]. The
patch is an optimization, similar to the solution proposed
elsewhere [26, 70, 71], that enables enclaves to issue sys-
tem calls without ﬁrst making an expensive enclave exit and
associated context switch to the untrusted host process.
For every SGX thread, the exitless implementation spawns
an untrusted (outside of the enclave) RPC thread that issues
system calls on behalf of the SGX thread. The RPC and SGX
threads share a FIFO ring buffer for communicating system
call arguments and results. To issue a system call, the SGX
thread enqueues the system call request, and waits on a spin-
lock for the RPC thread’s response. To conserve CPU re-
sources, SGX threads only spin on the spinlock a set number
of times (by default, 4096 spins) before falling back to sleep-
ing on a futex (the futex call is a normal ocall).
BearSSL We integrate the BearSSL library [72] into
Graphene to provide the TLS connections between the
Graphene clients and kernel servers, and to verify the time-
server’s response. The library is well-suited to a kernel envi-
ronment, as it avoids dynamic memory allocations, and has
minimal dependencies on the underlying C library. For per-
formance, we use BearSSL’s implementations based on x86’s
AES-NI, PCL MUL, and SSE extensions, which helped to
expose stack mis-alignment bugs in Graphene.
File Locking System Calls Graphene does not currently
support ﬁle locking. As our memservers required this fea-
ture, we added an advlock (advisory lock) ﬁle system op-
eration; applications invoke the operation through a reduced
set of locking/unlocking ﬂags to the fcntl system call.
5.3 NGINX Modiﬁcations
Shared Memory Patch NGINX uses shared memory to
coordinate state among the worker processes that service
HTTP(S) requests. On most systems, it uses mmap to cre-
ate shared, anonymous mappings. NGINX encapsulates each
mapping as a named zone. For allocating in shared memory,
NGINX overlays a slab pool over the zone’s shared memory.
To coordinate concurrent allocations and frees on the pool,
as well as modiﬁcations to the user data structures allocated
USENIX Association
29th USENIX Security Symposium    743
multi-tenancy, and (3) the performance impact of a WAF.
We perform our tests on the Intel NUC Skull Canyon
NUC6i7KYK Kit with 6th generation Intel Core i7-6770HQ
Processor (2.6 GHz) and 32 GiB of RAM. The processor
consists of four hyperthreaded cores and has a 6 MiB cache.
We use ApacheBench to repeatedly fetch a ﬁle 10,000
times over non-persistent HTTPS connections (each request
involves a new TCP and TLS handshake) from among 128
concurrent clients.4 We run ApacheBench on a second NUC
device connected to the conclave’s NUC via a Gigabit Ether-
net switch. For the benchmarks, the origin server is another
instance of NGINX running on the conclave’s NUC.
We examine three conclave conﬁgurations:
(1) Linux-
keyless: NGINX running on normal Linux and using a key-
server, (2) Graphene-crypt: NGINX running on Graphene
and using a bd-crypt fsserver, sm-crypt for shared mem-
ory, and the keyserver, and (3) Graphene-vericrypt: NGINX
running on Graphene and using a bd-vericrypt fsserver, sm-
vericrypt for shared memory, and a keyserver. These corre-
spond to a Keyless SSL analog, a conclave deployment for
data conﬁdentiality, and a conclave deployment for both data
conﬁdentiality and integrity, respectively. We compare these
conclaves to the status quo of NGINX running on standard
Linux (simply denoted as Linux). We omit using the time-
server.
For each benchmark that uses the nextfs ﬁleserver, we use
a 128 MiB disk image. As a baseline, we conﬁgure NGINX
to use a small shared memory zone of 16 KiB to hold the web
cache metadata (enough for 125 cache keys). §6.2 presents
a sensitivity analysis on the size of the shared memory zone.
6.1 Standard ocalls vs. exitless
To determine the optimal ocall method for our application,
we ﬁrst compare the performance of standard vs. exitless ver-
sions of Graphene-crypt. We present HTTPS throughput and
latency results for each version as part of Figure 3.
Surprisingly, the exitless version performs worse across
the board. Although both perform similarly with a single
NGINX worker, the standard ocall version exhibits expected
performance gains as new workers are added, whereas exit-
less generally worsens with additional workers. In a conclave
environment, increased contention on the kernel servers, as
well as contention among the SGX and RPC-queue threads,
magnify the RPC latency overheads, which in turn causes ex-
itless to exit the spinlock and make a futex ocall.
Based on these results, we use standard ocalls in all in-
stances of Graphene (both on the Graphene-hosted NGINX
processes, and the kernel servers) for the remainder of the
macro-benchmarks.5
4That is, the command ab -n 10000 -c 128
5§6.5 shows that exitless performs better than standard ocalls for low-
latency calls, but degrades for high-latency calls.
Figure 2: An NGINX worker servicing an HTTPS request
for cached content, and the resultant kernel server RPCs.
from the pool, each pool has an associated mutex. On sys-
tems with atomic operations, the mutex is implemented as a
spinlock over a word of the shared memory, optionally falling
back to a POSIX semaphore for long, blocking lock opera-
tions. On systems without atomic operations, the mutex is
implemented as a lock ﬁle.
To have NGINX follow the semantics of our shared mem-
ory design, we create a small patch (∼300 lines) that changes
the creation of shared memory and the associated mutex. In
particular, we implement shared memory by having mmap
map a path obtained by concatenating the ﬁlesystem root
with the zone name. To force the use of a lock ﬁle, we disable
atomics. NGINX’s lock ﬁle path is the name of the zone con-
catenated with a preﬁx that may be speciﬁed in the NGINX
conﬁguration ﬁle (nginx.conf), thus allowing us to easily
have the lock ﬁle be the very same ﬁle that is mapped.
Request Lifecycle When NGINX operates as a caching
server, it runs four processes by default: (1) a master process
that initializes the server and responds to software signals,
(2) a conﬁgurable number of worker processes that service
HTTPS requests, (3) a cache manager, and (4) a cache loader.
Figure 2 shows the lifecycle of an NGINX worker pro-
cess serving an HTTPS request, and the resultant RPCs to
the enclaved kernel servers. Note that each request requires
two critical sections involving the metadata. Also, NGINX
reads the cached content using the pread system call, which
Graphene’s virtual ﬁle system (VFS) layer implements as a
sequence of seeks and a read to the underlying ﬁlesystem.
6 Evaluation
We evaluate the performance of NGINX 1.14.1 running
within a Phoenix Conclave. We seek to understand (1) the
performance costs of the various aspects of the conclave de-
sign and implementation, (2) how performance scales with
744    29th USENIX Security Symposium
USENIX Association
NGINX workermemserverkeyserverfsserverlockunlockRSA signopenstatTLS handshakeopen cached ﬁle and get sizeread headers from cached ﬁlepread  (in 32KiB chunks)receive HTTP requestclose cached ﬁleclosecheck if ﬁle exists in cache metadataread body from cached ﬁleupdate cache metadatacraft and send HTTP headerscraft and send HTTP bodycleanupseekreadseekseekpreadlockunlockseekreadseekseekFigure 3: Throughput and latency for single-tenant conﬁgu-
rations. The legend indicates the number of NGINX worker
processes. We include the standard deviation of the latencies
as error bars.
Figure 4: Multitenancy scaling. Throughputs are the aggre-
gate throughput across all customers, and latencies are the
mean latencies across customers. Above the bars, we indi-
cate the number of enclaves in each conﬁguration.
100 KiB
781
320.36
399.54
1 MiB
8,000
133.25
960.58
16 KiB
125
437.76
292.40
Segment Size
# Cache Keys
Throughput
Latency
10 MiB
80,000
9.71
13,184.09
Table 2: Effect of increasing the size of NGINX’s shared
memory segment for cache metadata. We use Graphene-
crypt with one NGINX worker, and fetch a 1 KiB ﬁle.
Throughput is the mean requests served per second; latency
is the client-perceived latency (ms).
6.2 Single-Tenant
Figure 3 shows request latency and throughput results for the
four conﬁgurations. Due to the RSA private key operation
in the TLS handshake, Linux becomes CPU-bound at four
workers (our test machine has four physical cores) and satu-
rates the Ethernet link for tests with a 100 KiB payload and
more than one NGINX worker. Linux-keyless shows that
the concurrency of the keyserver levels off with two workers,
and thus that the two NGINX worker conﬁguration of Linux-
keyless is an upper-bound on the performance we can hope to
achieve with the other conclave conﬁgurations. Linux with
two or more workers beats all conclave conﬁgurations.
Table 2 shows a sensitivity analysis on the shared mem-
ory zone size for NGINX’s cache metadata, using Graphene-
crypt. Performance diminishes disproportionately faster than
the increases in memory sizes, and request latency exceeds
1 sec past 1 MiB.
6.3 Scaling to Multi-tenants
We evaluate two approaches to multi-tenancy: (1) shared
nothing, in which each customer runs their own conclave,
including an enclaved instance of NGINX, and (2) shared
NGINX, where each customer runs their own enclaved ker-
nel servers, but share an enclaved version of NGINX:
the NGINX conﬁguration ﬁle multiplexes the customer re-
sources. Speciﬁcally, the NGINX conﬁguration ﬁle deﬁnes a
virtual server for each customer; each virtual server’s cache
directory, shared memory zone for the cache metadata, and
TLS private key point to separate instances of the fsserver,
memserver, and keyserver, respectively. We compare these
approaches to the status quo of running a single NGINX in-
stance with a virtual server for each customer. We run each
NGINX instance with four worker processes (in the shared