### 2.2.1 Criteria for Onset of User Difficulty

The following criteria were used to identify the onset of user difficulty. These criteria are observable events in video and think-aloud data:

1. **Toggling**: This occurs when a user repeatedly clicks on the same control, indicating confusion or uncertainty about how to operate it.
2. **Help Access**: This criterion is triggered when a user consults an online Help file. The period of difficulty is considered to have started before the user opens the Help window, specifically from the click preceding the one that opens the Help window. The assumption is that users become confused before they decide to seek help.
3. **Question to Experimenter**: This criterion applies only in user tests where a human experimenter is present. It is triggered when the user asks the experimenter a question about the experiment or the interface being tested.

### 2.2.2 Criteria for Offset of User Difficulty

Three criteria were used to signal the end of a period of user difficulty. Like the criteria for the onset of difficulty, these are observable events in video and think-aloud data:

1. **Click**: A user click typically indicates that the user has moved on from the difficulty. However, there are two exceptions:
   - If the "Toggling" criterion signaled the onset of the difficulty, the period does not end until the user clicks away from the toggled control.
   - If the "Help" criterion signaled the onset of the difficulty, the period does not end with clicks made while viewing the Help file. Instead, the "Help dismissed" criterion (see below) signals the end of the difficulty.
   
2. **User Statement**: This occurs when the user makes a verbal assertion indicating that they are ready to proceed or that their confusion has been resolved. Examples of such statements include:
   - "Ok, I’m going to hit the OK button..."
   - "I’m going to try..."
   - "All right, this looks right..."

3. **Help Dismissed**: If the "Help" criterion signaled the start of the difficulty, the period ends when the Help window is closed, minimized, or sent to the background, indicating that the user has stopped reading the Help and is resuming their task.

### 2.2.3 User Difficulty and Interface Defects

A hesitation detector does not directly detect interface defects; rather, it identifies periods of user difficulty that are likely caused by interface defects. After the hesitation detector outputs its results, a usability analyst must review other data sources, such as video and audio recordings of user sessions, to determine which hesitations truly indicate difficulty and to identify the specific defects causing each period of difficulty.

### 2.3 Accuracy

Accuracy is measured in terms of hit rate and false-alarm rate. The hit rate is the percentage of all genuine periods of user difficulty that the detector correctly identifies, while the false-alarm rate is the percentage of events for which the detector incorrectly indicates user difficulty when none was present. 

Because the input to hesitation detection is continuous, defining hit rate and false-alarm rate is more complex than for detectors with discrete inputs. To simplify this, time is divided into discrete blocks. For each block, ground truth designates whether the entire block is a period of user difficulty or non-difficulty. The detector then classifies each block as a hesitation or non-hesitation. A hit is any block designated as a period of user difficulty by ground truth and classified as a hesitation by the detector. A false alarm is any block designated as a period of user non-difficulty by ground truth but classified as a hesitation by the detector. Hit rate is defined as the number of hits divided by the number of blocks designated as periods of user difficulty by ground truth. False-alarm rate is defined as the number of false alarms divided by the number of blocks designated as periods of user non-difficulty by ground truth.

There are two reasonable ways to define a block:
1. **Variable Block Length**: A block is any contiguous portion of time during which the user experienced uninterrupted difficulty. Block length varies depending on the duration of the difficulty.
2. **Fixed Block Length**: A block is a pre-defined, short amount of time, e.g., one second. All blocks are the same length.

This paper uses both definitions. The variable block length is used for hit-rate computation, while the fixed block length is used for false-alarm-rate computation. This approach is justified because, in the context of using hesitation detection to identify interface defects, it is important to detect any hesitation within a period of difficulty, even if the hesitation does not last for the entire duration. For false alarms, a small, uniform block size (e.g., one second) is chosen to minimize the penalty paid by the analyst, who will only need to examine the portion of data constituting the false alarm.

### 2.4 Approach

The approach to answering the research question involves four stages:
1. **Data Collection**: Collect mouse, keyboard, video, and think-aloud audio data from users performing tasks with two different user interfaces.
2. **Hesitation Detection**: Use a hesitation detector to identify hesitations based on mouse and keyboard data.
3. **Ground-Truth Determination**: Have a usability expert determine ground-truth user difficulty from the video and audio data according to the criteria in section 2.2.
4. **Accuracy Computation**: Compute the accuracy of hesitations as an indicator of user difficulty by comparing the detected hesitations with the ground truth.

### 3 Related Work

Common methods for detecting user interface defects fall into four broad categories: inspection methods, model-based methods, surveys, and observation-based methods. Inspection methods, such as heuristic evaluation and cognitive walkthrough, involve a usability expert reviewing an interface or specification for defects. They are useful in the early stages of design but can be inaccurate. Model-based methods, like GOMS, provide excellent predictions for skilled performance but are not designed to detect issues for novice users. Surveys, though rarely used for defect detection, tend to yield subjective results. Observation-based methods offer direct data on defects, allow testing on both novices and experts, and provide quantitative results, but they require a prototype implementation and extensive analysis time.

Previous efforts have explored the use of hesitations to determine periods of user difficulty. Maxion and deChambeau (1995) first proposed using hesitation detection to identify user-interface defects. They conducted a study on an online library catalog application, measuring the time between prompts and responses. Times outside three standard deviations of the mean were reported as hesitations. The method yielded 66 hesitations from 12 participants, all due to interface defects, with no false positives. However, no hit rate was reported, and the results may not generalize to today's graphical user interfaces.

Maxion and Syme (1997) presented MetriStation, a tool for user-study data collection and analysis, including a component for hesitation detection. They did not measure the accuracy of the hesitation tool.

Horvitz et al. (1998) developed Lumiere, a system for inferring user goals and needs. Lumiere used a Bayesian statistical model to infer when users needed assistance, listing "introspection" (similar to "hesitation") as evidence. The accuracy of introspection or the Bayes network for detecting user difficulty was not measured.

Other work has focused on reducing analysis time for observation-based interface evaluation. Tools like DRUM, AMME, and USINE aid in usability data analysis by recording significant events, constructing mental models, and comparing observed actions to task models.

### 4 Experimental Method

As discussed in section 2.4, the approach to evaluating hesitation detection accuracy involves four stages: data collection, hesitation detection, ground-truth determination, and accuracy computation. The methods are detailed below.

#### 4.1 Data Collection

Mouse and keyboard data streams, collected from a laboratory user study, were used as input to the hesitation detector. Screen video and think-aloud audio from the same study were used by a human analyst to determine ground-truth periods of user difficulty. Details of the user study are provided below.

##### 4.1.1 User Interfaces Tested

The data were collected as part of a laboratory user study comparing two interfaces for setting file permissions in Microsoft Windows XP: the native Windows XP file-permissions interface and an interface called Salmon, designed by the authors. Both are typical graphical user interfaces.

##### 4.1.2 Participants

Twenty-three students and research staff members at Carnegie Mellon University participated in the study. Twelve participants used the XP interface, and 11 used Salmon. All participants were daily computer users but were novice or occasional users of file permissions interfaces. To confirm their novice status, participants rated their frequency of setting file permissions and their familiarity with Windows file and folder security. Twenty out of 23 participants set file permissions a few times a month or less, and 22 out of 23 rated themselves as generally, vaguely familiar, or unfamiliar with Windows file and folder security.

##### 4.1.3 Task

Each participant completed seven file-permissions-setting tasks.