control does or about how to operate it.
4. Help access: This occurs when a user consults an on-
line Help ﬁle. However, the difﬁculty is considered
to have started in the period prior to consulting Help.
The period begins on the click preceding the click that
opens the Help window. The assumption is that users
become confused in the period before consulting Help,
so the click that brings up the Help window is too late
to be considered the onset of the period of difﬁculty.
5. Question to experimenter: This criterion only ap-
plies to user tests in which a human experimenter is
present, as happened in the present study.
It occurs
when the user asks a question of the experimenter
about the experiment or the interface being tested.
2.2.2 Criteria for offset of user difﬁculty
Three criteria were used to signal the offset of a period
of user difﬁculty. Like the criteria signaling the onset of
user difﬁculty, these three criteria were observable events in
video and think-aloud data. These three criteria were:
1. Click: This occurs when the user clicks on something
with the mouse, which usually indicates that the user
has moved on from the difﬁculty. There are two excep-
tions to using the “Click” criterion to end a period of
user difﬁculty:
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:35 UTC from IEEE Xplore.  Restrictions apply. 
(cid:127) If the “Toggling” criterion signals the onset of the
period of difﬁculty, then the period does not end
as long as the user is repeatedly clicking on the
same control. The ﬁrst click away from that con-
trol may signal the end of the difﬁculty.
(cid:127) If the “Help” criterion signals the beginning of
the period of difﬁculty, then the period does not
end on clicks made while viewing the Help ﬁle;
in this case, the “Help dismissed” criterion (see
below) signals the end of the difﬁculty.
2. User statement: This occurs when the user makes
a verbal assertion to do something, or when the user
states that their confusion has been cleared up. User
statements that signal the end of the difﬁculty may start
with phrases such as:
(cid:127) “Ok, I’m going to hit the OK button ...”
(cid:127) “I’m going to try ...”
(cid:127) “All right, this looks right ...”
3. Help dismissed: If the “Help” criterion signals the
start of the period of difﬁculty, then that period ends
when the Help window is closed, minimized, or sent
to background, i.e., when the user stops reading Help
and starts doing something else.
2.2.3 User difﬁculty and interface defects
A hesitation detector does not detect interface defects di-
rectly; it detects periods of user difﬁculty that are the likely
consequence of interface defects. Once hesitation-detector
output has been obtained, a usability analyst must examine
other sources of data, usually video and audio of user ses-
sions, to determine which hesitations really indicate difﬁ-
culty, as well as which particular defects caused each period
of difﬁculty.
2.3 Accuracy
Accuracy is measured in terms of hit rate and false-
alarm rate. Hit rate is the percentage of all periods of
genuine user difﬁculty that the detector detects, while false-
alarm rate is the percentage of events for which the detector
incorrectly indicates user difﬁculty when none was present.
Technical deﬁnitions of these terms are given below.
Because the input to hesitation detection is continuous in
time, deﬁning hit rate and false-alarm rate is not as straight-
forward as it is for detectors in which inputs are discrete.
Thus, to make hit-rate and false-alarm-rate computation
more straightforward, time is divided into discrete blocks.
For each block, ground truth either designates the entire
block as a period of user difﬁculty or a period of user non-
difﬁculty. Also, for each block, the detector either classi-
ﬁes the block as a hesitation or a non-hesitation. A hit is
any block that is designated as a period of user difﬁculty by
ground truth and classiﬁed as a hesitation by the detector.
A false alarm is any block that is designated as a period of
user non-difﬁculty by ground truth but classiﬁed as a hesi-
tation by the detector. Hit rate is deﬁned as the number of
hits divided by the number of blocks designated as periods
of user difﬁculty by the ground truth. False-alarm rate is
deﬁned as the number of false alarms divided by the num-
ber of blocks designated as periods of user non-difﬁculty by
the ground truth.
There are two reasonable ways to deﬁne a block:
1. A block is any contiguous portion of time during which
the user experienced uninterrupted difﬁculty; under
this deﬁnition, block length is variable, depending on
the length of periods of user difﬁculty.
2. A block is a pre-deﬁned, short amount of time, e.g.,
one second; under this deﬁnition, all blocks are the
same length.
This paper uses both deﬁnitions of blocks. The former
deﬁnition of blocks is used for hit-rate computation, while
the latter is used for false-alarm-rate computation. The jus-
tiﬁcation for using both deﬁnitions lies in the application of
hesitation detection as a tool to help an analyst identify in-
terface defects. In this application, it is not important that
a hesitation lasts for the entire duration of a period of user
difﬁculty; it is only important that a hesitation occurs some-
where within the period of difﬁculty, so that the difﬁculty
will be brought to the analyst’s attention. The analyst will
then determine the full extent of the difﬁculty. Thus, so
long as a hesitation occurs anywhere within a block of un-
interrupted difﬁculty, that block is considered a hit. Alter-
natively, the penalty paid for a false alarm is that a usability
analyst will have to examine only the portion of the data that
constitutes the false alarm (plus, perhaps, a small amount of
context); it is not necessary for the analyst to examine the
entire period of contiguous non-difﬁculty. Thus, it makes
sense to simply choose a small, uniform block size for com-
puting false alarm rate. One second was chosen, because it
is short enough that no major transitions from non-difﬁculty
to difﬁculty occur within it.
2.4 Approach
The approach taken to answering the research question
posed has the following four stages:
1. Data collection: Collect mouse, keyboard, video, and
think-aloud audio data from users performing a task
with either of two different user interfaces.
2. Hesitation detection: Use a hesitation detector to de-
termine hesitations based on mouse and keyboard data.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:35 UTC from IEEE Xplore.  Restrictions apply. 
3. Ground-truth determination: Have a usability ex-
pert determine ground-truth user difﬁculty from the
video and audio data according to the criteria in sec-
tion 2.2.
4. Accuracy computation: Compute the accuracy of
hesitations as an indicator of user difﬁculty by com-
paring the hesitations with the ground truth.
Section 4 details the methodology.
3 Related work
Commonly used methods for user interface defect de-
tection fall into four broad categories:
inspection meth-
ods, model-based methods, surveys, and observation-based
methods. Inspection methods, such as heuristic evaluation
[20] and cognitive walkthrough [24], involve a usability ex-
pert reviewing an interface or interface speciﬁcation for de-
fects. They are particularly well-suited to ﬁnding defects
in the early stages of the interface design cycle, because
they do not require a working interface; they are notori-
ously inaccurate [3, 4, 12, 13]. Model-based methods, such
as GOMS [9, 10], can give excellent predictions for skilled,
expert performance with an interface, but they are not de-
signed to detect aspects of an interface that may give novice
or occasional users difﬁculty. Model-based methods also
typically do not make predictions about where users, includ-
ing experts, may make errors. Surveys are rarely applied to
defect detection, but when they are, they tend to give sub-
jective results that are difﬁcult to quantify [5]. Observation-
based methods have the advantages of providing direct data
about what defects affect users, allowing for testing on both
novices and experts, and giving quantitative results. The
primary disadvantages of observation-based methods are
that they generally require at least a prototype implemen-
tation of an interface, and as well as a great deal of analysis
time.
Previous efforts have addressed the idea of using hesi-
tations to determine periods of user difﬁculty. Maxion and
deChambeau in 1995 were the ﬁrst to propose using hes-
itation detection to spot user difﬁculty and identify user-
interface defects [16]. They conducted a user study of
an online library catalog application. The command-line-
based application presented users with prompts for input,
to which users responded with keystrokes. The time be-
tween prompt and response was measured, and times out-
side three standard deviations of the mean were reported
as hesitations. The method yielded 66 hesitations from 12
participants. All hesitations were due to interface defects,
so, remarkably, there were no false positives. However, no
hit rate was reported because no ground truth was avail-
able. Furthermore, it is unclear how the results obtained for
the command-line based library catalog application would
generalize to today’s common desktop applications, which
are typically graphical user interfaces receiving continuous
mouse and keyboard input.
In 1997, Maxion and Syme presented MetriStation, a
software tool for user-study data collection and analysis,
which included a component that analyzed the data for hes-
itations [18]. They made no attempt to measure the hesita-
tion tool’s accuracy, however.
Horvitz et al. in 1998 published their work on Lumiere,
a system for inferring user goals and needs [7]. Lumiere
employed a Bayesian statistical model to, in part, make in-
ferences about when users needed assistance. The authors
listed “introspection,” which is deﬁned similarly to “hesi-
tation” in this paper, as one type of evidence used in their
Bayesian network to infer that a user is having difﬁculty and
needs assistance. The authors did not, however, measure the
accuracy of either introspection or of the larger Bayes net-
work as a means of detecting user difﬁculty.
A substantial amount of prior work has explored tech-
niques other than hesitation detection to reduce the amount
of analysis time required for observation-based interface
evaluation. Ivory and Hearst [8] provide a thorough sum-
mary of this work. A few examples of the numerous tools
for aiding usability data analysis include DRUM (Diagnos-
tic Recorder for Usability Measurement) [15], AMME (Au-
tomatic Mental Model Evaluator) [22], and USINE (User
Interface Evaluator) [14]. DRUM records signiﬁcant user
events, allows analysts to mark up signiﬁcant points in
usability-test video, and computes metrics like task time,
time users spend having problems, and productive time.
AMME constructs user mental models, interface state tran-
sitions, and quantitative metrics from log data. USINE takes
as input a task model, and compares observed user actions
to the task model to determine where user make errors.
DRUM, AMME, and USINE are representative of numer-
ous similar tools described by Ivory and Hearst.
4 Experimental method
As discussed in section 2.4, the approach encompassed
four stages to address the question of hesitation detection
accuracy: data collection, hesitation detection, ground-truth
determination, and accuracy computation. The methods are
explained below.
4.1 Data collection
Mouse and keyboard data streams, collected from a lab-
oratory user study, were used as input to the hesitation de-
tector evaluated in this work. Screen video and think-aloud
audio from the same study were used by a human analyst
to determine ground-truth periods of user difﬁculty. This
section provides details on the user study from which these
data were collected.
Proceedings of the 2006 International Conference on Dependable Systems and Networks (DSN’06) 
0-7695-2607-1/06 $20.00 © 2006 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 12:11:35 UTC from IEEE Xplore.  Restrictions apply. 
4.1.1 User interfaces tested
The data used for this work was collected as part of a lab-
oratory user study comparing two interfaces designed for
setting ﬁle permissions in Microsoft’s Windows XP operat-
ing system [17]. The two interfaces tested were the native
Windows XP ﬁle-permissions interface and an interface of
the authors’ design called Salmon. Both are graphical user
interfaces, typical of the user interfaces of many of today’s
common desktop applications.
4.1.2 Participants
Twenty-three students and research staff members at
Carnegie Mellon University participated in the study.
Twelve participants were assigned to the XP interface, and
11 were assigned to Salmon. All participants were daily
computer users, but they were only novice or occasional
users of the ﬁle permissions interfaces. To establish that
participants were novice or occasional users, they were
asked to rate the frequency with which they set ﬁle permis-
sions in their daily work, and to rate their familiarity with
Windows ﬁle and folder security. Twenty of 23 participants
reported setting ﬁle permissions a few times a month or
less (the other three reported setting ﬁle permissions a few
times a week or daily) and 22 out of 23 participants rated
themselves “generally familiar” (10 participants), “vaguely
familiar”(10 participants), or “unfamiliar” (2 participants)
with Windows ﬁle and folder security. Only one participant
rated himself “very familiar” with Windows ﬁle and folder
security. No participants said their daily work involved
Windows ﬁle and folder security. These answers suggest
that the description of users as “novice or occasional” is ac-
curate.
4.1.3 Task
Each participant completed seven ﬁle-permissions-setting