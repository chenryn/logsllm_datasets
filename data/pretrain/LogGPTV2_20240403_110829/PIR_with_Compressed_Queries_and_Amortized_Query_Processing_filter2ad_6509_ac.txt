to encode the response. The good news is that for small values
of d (2 or 3), this results in major computational savings while
still reducing network costs by orders of magnitude over XPIR.
3.5 Handling larger databases
As we discuss in Section 3.3, the size of the query vector that
EXPAND can generate is bounded by N. Based on recommended
security parameters [10, 28], N is typically 2048 or 4096 (larger
N improves security but reduces performance). So how can one
index into databases with more than N elements?
We propose two solutions. First, the client sends multiple
ciphertexts and the server expands them and concatenates the
results. For instance, if N is 2048, the database has 4000 ele-
ments, and the client wishes to get the element at index 2050,
the client sends 2 ciphertexts: the ﬁrst encrypts 0 and the second
encrypts x2. The server expands both ciphertexts into 2048-entry
vectors and concatenates them to get a 4096-entry vector where
the entry at index 2050 encrypts 1, and all others encrypt 0. The
server then uses the ﬁrst 4000 entries as the query vector.
A more efﬁcient solution is to represent the database as a
d-dimensional hypercube as we discuss in Section 3.4. This
allows the client to send d ciphertexts to index a database of
size Nd. For d = 2 and N = 4096, two ciphertexts are sufﬁcient
to index 16.7 million entries. One can also use a combination of
these solutions. For example, given a database with 230 entries,
SealPIR uses d = 2 (so the database is a 215 × 215 matrix), and
represents the index for each dimension using 215/4096 = 8
ciphertexts. The server expands these 8 ciphertexts and con-
catenates them to obtain a vector of 215 entries. In total, this
approach requires the client to send 16 ciphertexts as the query
(8 per dimension), and receive F ≈ 7 ciphertexts as the re-
sponse (d = 3 would lead to 3 ciphertexts as the query, but F2
ciphertexts as the response).
In short, the query communication complexity goes from
√
O(Nd d
√
n) in XPIR to O(Nd(cid:7) d
n/N(cid:8)) in SealPIR.
4 Amortizing computational costs in PIR
Answering a PIR query requires computation that is linear in
the size of the database, so a promising way to save computa-
966
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:31:59 UTC from IEEE Xplore.  Restrictions apply. 
tional resources is for the server to amortize costs by processing
a batch of queries. Batch codes [52] are a data encoding that,
among other applications, can be used to achieve this goal. In
particular, the server can use a batch code to encode its database
in a way that it can answer a batch of queries more cheaply
(computationally) than answering each query individually. Un-
fortunately, despite a large body of work on batch codes, we
ﬁnd that most constructions do not focus on PIR amortiza-
tion. Instead, they target load balancing in distributed storage
systems [61, 67, 69] and network switches [77], which have
different requirements. Using these codes to amortize PIR query
processing would incur prohibitive network costs.
Our key observation is that certain guarantees of batch codes
are not necessary for many PIR-backed systems. Relaxing those
guarantees leads to constructions that are not only asymptoti-
cally better, but also concretely efﬁcient—without compromis-
ing the functionality of our target system. Below we give a
description of batch codes, highlight the sources of overhead,
and then introduce our construction.
4.1 Batch codes and their cost
elements, and produces a set of m codewords, C, distributed
A (n, m, k, b)-batch code B takes as input a collection DB of n
among b buckets.3 Formally, B : DB → (C0, . . . , Cb−1), where
|Ci
| is the number of codewords in bucket i, and the sum of
| ≥ n. The goal of
codewords across all buckets is m = Σb−1
i=0
these codes is two-fold. First, they ensure that any k elements
from DB can be retrieved from the b buckets by fetching at most
one codeword from each bucket. Second, they keep the number
of total codewords, m, lower than k · n.
Example. We describe a (4, 6, 2, 3)-batch code, speciﬁcally
the subcube batch code [52]. Let DB = {x1, x2, x3, x4}. For
|Ci
the encoding, DB is split in half to produce 2 buckets, and
a third bucket is produced by XORing the entries in the ﬁrst
two buckets: B(DB) = ({x1, x2},{x3, x4},{x1 ⊕ x3, x2 ⊕ x4}).
Observe that one can obtain any 2 elements in DB by querying
each bucket at most once. For example, to obtain x1 and x2, one
can get x1 from the ﬁrst bucket, x4 from the second bucket, and
x2 ⊕ x4 from the third bucket; x2 = x4 ⊕ (x2 ⊕ x4).
This encoding is helpful for PIR because a client wishing
to retrieve 2 elements from DB can, instead of querying DB
twice, issue one query to each bucket. The server is in effect
computing over 3 “databases” with 2 elements each, which
results in 25% fewer operations.
Costs of PIR with batch codes. Figure 5 depicts the relation-
ship between the number of codewords (m) and the number of
buckets b, as a function of the database size (n) and the batch
size (k) for several constructions. In multi-query PIR, the client
issues one query to each of the b buckets, and therefore receives
b responses (§5). To answer these b queries, the server com-
putes over all m codewords exactly once; lower values of m
lead to less computation, and lower values of b lead to lower
network costs. Since m < k · n, the total computation done by
the server is lower than running k parallel instances of PIR. The
3We use different variable names (e.g., m and b) from the batch code literature
to avoid overloading variable names introduced in Section 3.
drawback is that existing batch codes produce many buckets
(see the third column in Figure 5). As a result, they introduce
signiﬁcant network overhead over not using a batch code at all.
This makes batch codes unappealing in practice.
4.2 Probabilistic batch codes (PBC)
Batch codes have exciting properties, but existing constructions
offer an unattractive trade-off: they reduce computation but add
network overhead. We make this trade-off more appealing by
relaxing batch codes’ guarantees.
A probabilistic batch code (PBC) differs from a traditional
batch code in that it fails to be complete with probability p. That
is, there might be no way to recover a speciﬁc set of k elements
from a collection encoded with a PBC by retrieving exactly one
codeword from each bucket. The probability of encountering
one such set (when elements are uniformly chosen) is p. In the
example of Section 4.1, this would mean that under a PBC, a
client may be unable to retrieve both x1 and x2 by querying
buckets at most once (whereas a traditional batch code guar-
antees that this is always possible). In practice, this is seldom
an issue: our construction has parameters that result in roughly
1 in a trillion queries failing, which we think is a sufﬁciently
rare occurrence. Furthermore, as we discuss in Section 5, this
is an easy failure case to address in multi-query PIR since a
client learns whether or not it can get all of the elements before
issuing any queries.
Deﬁnition 1 (PBC). A (n, m, k, b, p)-PBC is given by three
polynomial-time algorithms (Encode, GenSchedule, Decode):
• (C0, . . . , Cb−1) ← Encode(DB): Given an n-element col-
lection DB, output a b-tuple of buckets, where b ≥ k, each
bucket contains zero or more codewords, and the total num-
| ≥ n.
ber of codewords across all buckets is m = Σb−1
i=0
• {σ,⊥} ← GenSchedule(I): Given a set of k indices I cor-
responding to the positions of elements in DB, output a
schedule σ : I → {{0, . . . , b − 1}+}k
. The schedule σ gives,
for each position i ∈ I, the index of one or more buckets
construct element DB[i]. GenSchedule outputs ⊥ if it cannot
produce a schedule where each i ∈ I is associated with at
from which to retrieve a codeword that can be used to re-
|Ci
least one bucket, and where no buckets is used more than
once. This failure event occurs with probability p.
• element ← Decode(W): Given a set of codewords W, out-
put the corresponding element ∈ DB.
In the subsections ahead we describe an efﬁcient PBC
construction. Our key idea is as follows. Batch codes
spread out elements such that retrieval requests are load bal-
anced among different buckets. Relatedly, many data struc-
tures and networking applications use different variants of
hashing—consistent [53], asymmetric [75], weighted [74],
multi-choice [13, 64], cuckoo [12, 66], and others [23, 38]—
to achieve a similar goal. While there is no obvious way to use
these hashing schemes to implement multi-query PIR directly,
we can do it indirectly: we ﬁrst build a PBC from a simple
technique, which we call reverse hashing, and then use the PBC
to implement multi-query PIR (§5).
967
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:31:59 UTC from IEEE Xplore.  Restrictions apply. 
batch code
subcube ((cid:2) ≥ 2) [52, §3.2]
combinatorial (
Balbuena graphs [69, §IV.A]
k−1
(cid:3)
(cid:2)
r
≤ n/(k − 1)) [67, §2.2]
codewords (m)
n · (((cid:2) + 1)/(cid:2))log2
kn − (k − 1) ·
2(k3 − k · (cid:5)n/(k3 − k)(cid:6))
(k)
(cid:3)
k−1
(cid:2)
r
(cid:2)
Pung hybrid
[11, §4.4]
3-way reverse cuckoo hashing (this work, §4.5)
4.5n
3n
(k)
buckets (b)
((cid:2) + 1)log2
r
2(k3 − k)
9k
1.5k
probability of failure (p)
0
0
0
≈ 2−20
≈ 2−40
FIGURE 5—Cost of existing batch codes and the probabilistic batch code (PBC) construction given in Section 4.5. n indicates the number of
elements in the database DB. k gives the number of elements that can be retrieved from DB by querying each bucket in β(DB) at most once, where
β is the batch code. Building a multi-query PIR scheme from any of the above constructions leads to computational costs to the server linear in m,
and network communication linear in b. We list batch codes that have explicit constructions and can amortize CPU costs for multi-query PIR.
Other batch codes have been proposed (e.g., [61, 70, 71, 76]) but they either have no known constructions, or they seek additional properties
(e.g., tolerate data erasures, optimize for the case where n = b, support multisets) that introduce structure or costs that makes them a poor ﬁt for
multi-query PIR.
The scheme in Pung is neither a batch code nor a PBC since it relies on clients replicating the data to buckets (rather than the
server). It is, however, straightforward to port Pung’s allocation logic to construct a PBC.
(cid:2)
1
A
h1(1)
ࠑ1, Aࠒ
h2(1)
1
A
2
B
h1(2)
ࠑ2, Bࠒ
h2(2)
h2(3)
ࠑ3, Cࠒ
h1(3)
1
A
2
B
3
C
FIGURE 6—Logic for two-choice hashing [13] when allocating three
key-value tuples to buckets: (cid:8)1, A(cid:9), (cid:8)2, B(cid:9), (cid:8)3, C(cid:9). Tuples are inserted
into the bucket least full. Arrows represent the choices for each tuple
based on different hashes of the tuple’s key (here we depict an opti-
mistic scenario). The red solid arrow indicates the chosen mapping.
4.3 Randomized load balancing
A common use case for (non-cryptographic) hash functions is to
build efﬁcient data structures such as hash tables or dictionaries.
In a hash table, the insert procedure consists of computing one
or more hash functions on the key of the item being inserted.
Each application of a hash function returns an index into an
array of buckets in the hash table. The item is then placed into
one of these buckets following an allocation algorithm. For
example, in multi-choice hashing [13, 64], the item is placed in
the bucket least full among several candidate buckets. In Cuckoo
hashing [66], items may move around following the Cuckoo
hashing algorithm (we explain this algorithm in Section 4.5).
An ideal allocation results in items being assigned to buckets
such that all buckets have roughly the same number of items
(since this lowers the cost of lookup). In practice, there is load
imbalance where some buckets end up having more elements
than others; the extent of the imbalance depends on the alloca-
tion algorithm and the random choices that it makes. To look
up an item by its key, one computes the different hash functions
on the key to obtain the list of buckets in which the item could
have been placed. One then scans each of those buckets for the
desired item. An example of the insertion process for hashing
with two choices is given in Figure 6.
Abstract problem: balls and bins. In the above example,
hashing is used to solve an instance of the classic n balls and
b bins problem, which arises during insertion. The items to be
inserted into a hash table are the n balls, and the buckets in the
hash table are the b bins; using w hash functions to hash a key to
w candidate buckets approximates an independent and uniform
random assignment of a ball to w bins. The number of collisions
in a bucket is the load of a bin, and the highest load across all
bins is the max load. In the worst case, the max load is n/w (all
balls map to the same w candidate buckets), but there are much
smaller bounds that hold with high probability [13].
Interestingly, if we examine other scenarios abstracted by the
balls and bins problem, a pattern becomes clear: the allocation
algorithm is typically executed during data placement. In the
hash table example, the allocation algorithm determines where
to insert an element. In the context of a transport protocol [54],
the allocation algorithm dictates on which path to send a packet.
In the context of a job scheduler [65], the allocation algorithm
selects the server on which to run a task. The result is that the
load balancing effect is achieved at the time of “data placement”.
However, to build a PBC, we must do it at the time of “data
retrieval”. Reverse hashing achieves this.4
4.4 Reverse hashing
We start by introducing two principals: the producer and the
consumer. The producer holds a collection of n items where
each item is a key-value tuple. It is in charge of data placement:
taking each of the n elements and placing them into buckets
based on their keys following some allocation algorithm. The
consumer holds a set of k keys (k ≤ n), and is in charge of
data retrieval: it fetches items by their key from the buckets that
were populated by the producer. The goal is for the consumer to
get all k items by probing each bucket as few times as possible.
That is, the consumer has an instance of a k balls and b bins
problem, and its goal is to reduce the instance’s max load.
Note that the consumer is not inserting elements into buckets
(that is the job of the producer). Instead, the consumer is placing
“retrieval requests” into the buckets. The challenge is that any
clever allocation chosen by the consumer must be compatible
with the actions of the producer (who populates the buckets).
That is, if the consumer, after running its allocation algorithm
(e.g., multi-choice hashing) decides to retrieve items x1, x2,
and x3, from buckets 2, 3, and 7, it better be the case that
4Pung [11, §4.3] makes a similar observation but in a less general setting.
968
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:31:59 UTC from IEEE Xplore.  Restrictions apply. 
2մ
h1(2)
ࠑ2, մࠒ
h2(2)
h2(3)
ࠑ3, մࠒ
h1(3)
2մ
3մ
an element, the producer stores the element in all w buckets.
This ensures that regardless of which k elements are part of the
consumer’s simulation or which non-deterministic choices the
algorithm makes, the allocations are always compatible (Fig-
ure 7(b)). Of course this means that the producer is replicating
elements, which defeats the point of load balancing. However,