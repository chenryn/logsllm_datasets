answering a query with this amount of noise would violate privacy (namely, if the noise magnitude
is less than the local sensitivity in a neighborhood of the current dataset). If the test passes, then
we release a noisy answer. If the test fails, then we simply But perhaps you detect that adding
this (small) amount of noise would violate privacy. In that case, you simply refuse to answer. Of
course, we should carry out the test in a diﬀerentially private manner.
More precisely, Propose-Test-Release consists of the following three steps (parameterized by a
query q : Xn → R and ε, δ, β ≥ 0), yielding a mechanism M : Xn → R∪{⊥} that does the following
on a dataset x ∈ Xn:
22
1. Propose a target bound β on local sensitivity.
2. Let ˆd = d(x,{x(cid:48) : LSq(x(cid:48)) > β}) + Lap(1/ε), where d denotes Hamming distance.
3. If ˆd ≤ ln(1/δ)/ε, output ⊥.
4. If ˆd > ln(1/δ)/ε, output q(x) + Lap(β/ε).
Proposition 3.2 (propose-test-release [33]). For every query q : Xn → R and ε, δ, β ≥ 0, the above
algorithm is (2ε, δ)-diﬀerentially private.
Proof. Consider any two neighboring datasets x ∼ x(cid:48). Because of the Laplacian noise in the
deﬁnition of ˆd and the fact that Hamming distance has global sensitivity at most 1, it follows that
Pr[M(x) = ⊥] ∈ [e−ε · Pr[M(x(cid:48)) = ⊥], eε · Pr[M(x(cid:48)) = ⊥]].
(3)
Also, for those outputs that are not ⊥, we have two cases:
Case 1: LSq(x) > β. In this case, d(x,{x(cid:48)(cid:48) : LSq(x(cid:48)(cid:48)) > β}) = 0, so the probability that ˆd will
exceed ln(1/δ)/ε is at most δ. Thus, for every set T ⊆ R ∪ {⊥}, we have:
Pr[M(x) ∈ T ] ≤ Pr[M(x) ∈ T ∩ {⊥}] + Pr[M(x) (cid:54)= ⊥]
≤ eε · Pr[M(x(cid:48)) ∈ T ∩ {⊥}] + δ
≤ eε · Pr[M(x(cid:48)) ∈ T ] + δ,
where the second inequality follows from (3), noting that T ∩ {⊥} equals either {⊥} or ∅.
Case 2: LSq(x) ≤ β. In this case, |q(x)−q(x(cid:48))| ≤ β, which in turn implies the (ε, 0)-indistinguishability
of q(x) + Lap(β/ε) and q(x(cid:48)) + Lap(β/ε). Thus, by (3) and Basic Composition, we have (2ε, 0)-
indistinguishability overall.
Notice that, like smooth sensitivity, the naive algorithm for computing d(x,{x(cid:48) : LSq(x(cid:48)) > β})
enumerates over all datasets x(cid:48) ∈ Xn. Nevertheless, for the median function, it can again be
computed eﬃciently.
3.3 Releasing Stable Values
A special case of interest in Propose-Test-Release is when β = 0. Then it can be veriﬁed that
d(x,{x(cid:48) : LSq(x(cid:48)) > β}) = d(x,{x(cid:48) : q(x(cid:48)) (cid:54)= q(x)}) − 1, so the algorithm is testing whether the
function q is constant in a neighborhood of x (of radius roughly ln(1/δ)/ε) and if so, it outputs q
with no noise. That is, if q is stable around x, then we can safely release the value q(x) (exactly, with
no noise!), provided our test of stability is diﬀerentially private. This also applies to, and indeed
makes the most sense for, discrete-valued functions q : Xn → Y. In more detail, the mechanism
works as follows on x ∈ Xn:
1. Let ˆd = d(x,{x(cid:48) : q(x(cid:48)) (cid:54)= q(x)}) + Lap(1/ε), where d denotes Hamming distance.
2. If ˆd ≤ 1 + ln(1/δ)/ε, output ⊥.
23
3. Otherwise output q(x).
Similarly to Proposition 3.2, we have:
Proposition 3.3 (releasing stable values). For every query q : Xn → Y and ε, δ > 0, the above
algorithm is (ε, δ)-diﬀerentially private.
Consider, for example, the mode function q : Xn → X, where q(x) is deﬁned to be the most
frequently occurring data item in x (breaking ties arbitrarily). Then d(x,{x(cid:48) : q(x(cid:48)) (cid:54)= q(x)}) equals
half of the gap in the number of occurrences between the mode and the second-most frequently
occurring item (rounded up). So we have:
Proposition 3.4 (stability-based mode). For every data universe X, n ∈ N, ε, δ ≥ 0, there is an
(ε, δ)-diﬀerentially private algorithm M : Xn → X such that for every dataset x ∈ Xn where the
diﬀerence between the number of occurrences of the mode and the 2nd most frequently occurring
item is larger than 4(cid:100)ln(1/δ)/ε(cid:101), M(x) outputs the mode of x with probability at least 1 − δ.
If instead we had used the Laplace Histogram of Proposition 2.8 (outputting the bin y ∈ X
with the largest noisy count), we would require a gap of Θ(log |X|)/ε in the worst-case, so the
stability-based method is better when |X| is large compared to 1/δ. Indeed, let us now show how
stability-based ideas can in fact produce noisy histograms with an error bound of O(log(1/δ))/εn.
Theorem 3.5 (stability-based histograms [22]). For every ﬁnite data universe X, n ∈ N, ε ∈
(0, ln n), and δ ∈ (0, 1/n) there is an (ε, δ)-diﬀerentially private mechanism M : Xn → RX that on
every dataset x ∈ Xn, with high probability M(x) answers all of the counting queries in Qpt(X) to
within error
.
εn
(cid:18) log(1/δ)
(cid:19)
O
The intuition for the algorithm is that if we only released noisy answers for point functions
qy that are nonzero on the dataset x, the error bound in Proposition 2.8 would improve from
O(log |X|)/εn to O(log n)/εn ≤ O(log(1/δ))/εn, since at most n point functions can be nonzero on
any dataset (namely those corresponding to the rows of the dataset). However, revealing which
point functions are nonzero would not be diﬀerentially private. Thus, we only release the point
functions that are far from being zero (i.e. ones where the query is nonzero on all datasets at noisy
distance at most O(log(1/δ)/ε) from the given dataset, analogously to Proposition 3.3).
Proof. The algorithm is the same as the Laplace Histogram of Proposition 2.8, except that we do not
add noise to counts that are zero, and reduce all noisy counts that are smaller than O(log(1/δ)/εn
to zero.
Speciﬁcally, given a dataset x ∈ Xn, the algorithm works as follows:
1. For every point y ∈ X:
(a) If qy(x) = 0, then set ay = 0.
(b) If qy(x) > 0, then:
i. Set ay ← qy(x) + Lap(2/εn).
ii. If ay  0 (namely, ones where y ∈ {x1, . . . , xn}). By the tails of the Laplace distri-
bution and a union bound, with high probability all of the noisy answers qy(x)+Lap(2/εn) computed
in Step 1(b)i have error at most O((log n)/εn) ≤ O(log(1/δ)/εn). Truncating the small values to
zero in Step 1(b)ii introduces an additional error of up to 2 ln(1/δ)/εn + 1/n = O(log(1/δ)/εn).
i. Then the only point queries that diﬀer on x and x(cid:48) are qxi and qx(cid:48)
Privacy: Consider two neighboring datasets x ∼ x(cid:48), where dataset x(cid:48) is obtained by replacing
row xi with x(cid:48)
. Since the
answers to diﬀerent queries qy are independent, we can analyze the answer to each query separately
and then apply composition. Consider the answers axi(x) and axi(x(cid:48)) to query qxi on datasets x
and x(cid:48), respectively. We know that qxi(x) > 0 (since row xi is in x). If we also have qxi(x(cid:48)) > 0,
then axi(x) and axi(x(cid:48)) are (ε/2, 0)-indistinguishable by the diﬀerential privacy of the Laplace
mechanism. (We can view the truncation step as postprocessing.) If qxi(x(cid:48)) = 0, then axi(x(cid:48)) is
always 0, and qxi(x) = 1/n (since x and x(cid:48) agree on all other rows), which means that Pr[axi(x) (cid:54)=
0] = Pr[Lap(2/εn) ≥ 2 ln(2/δ)/εn] ≤ δ/2 and we have (0, δ/2)-indistinguishability. Thus, in all
cases, axi(x) and axi(x(cid:48)) are (ε/2, δ/2)-indistinguishable. By symmetry the same holds for the
(x(cid:48)). On all other queries y, ay(x) and ay(x(cid:48)) are identically distributed. By
answers ax(cid:48)
basic composition, the joint distributions of all answers are (ε, δ)-indistinguishable.
(x) and ax(cid:48)
i
i
i
3.4 Privately Bounding Local Sensitivity
Rather than proposing (arbitrarily) a threshold β as in Propose-Test-Release, more generally we
might try to compute a diﬀerentially private upper bound on the local sensitivity. That is, we will
try to compute a diﬀerentially private estimate ˆβ = ˆβ(x) such that, with probability at least 1 − δ,
LSq(x) ≤ ˆβ. If we can do this, then outputting q(x) + Lap( ˆβ/ε) will give an (ε, δ)-diﬀerentially
private algorithm, by an analysis as in the previous section.
The setting in which we will explore this possibility is where our dataset is a graph and we want
to estimate the number of triangles in the graph.
There are (at least) two notions of privacy that one might wish to consider for graph algorithms:
• Edge-level Privacy. In this setting, we say that G ∼ G(cid:48) if the graphs G and G(cid:48) diﬀer on one
edge. This is a special case of the setting we’ve been studying, where think of an n-vertex
graph as a dataset consisting of(cid:0)n
(cid:1) rows from universe X = {0, 1} .
2
• Node-level Privacy. In this setting, we say that G ∼ G(cid:48) if the graphs G and G(cid:48) diﬀer only
on edges that are adjacent to one vertex. This does not quite ﬁt in the tuple-dataset setting
we’ve been studying, but the concept of diﬀerential privacy naturally generalizes to this (as
well as any other family of “datasets” with some notion of “neighbors”.)
In applications (e.g. to social networks), node-level privacy is a preferable notion of privacy,
since it simultaneously protects all of the relationships associated with a vertex (which typically is
associated with an individual person), rather than just a single relationship at a time. However,
since our goal is only to illustrate the Privately Bounding Local Sensitivity method, we will consider
only edge-level privacy. Let q∆(G) be the number of triangles in G (where the ∆ is meant to be
evocative of a triangle). It can be veriﬁed that:
LSq∆(G) = max{j : ∃u∃v u and v have j common neighbors}.
25
This, in turn, is no more than the maximum degree of G.
In contrast the global sensitivity is
GSq∆ = n − 2. However, if we consider the global sensitivity of the local sensitivity, we have
= 1. (If we think of the local sensitivity as a discrete analogue of a derivative, then this is
GSLSq∆
the analogue of having a bounded second derivative, despite the derivative sometimes being large.)
Consider the following mechanism M(G):
• Compute ˆβ = LSq∆(G) + Lap(1/ε) + ln(1/δ)/ε.
• Output q∆(G) + Lap( ˆβ/ε).
This mechanism can be shown to be (2ε, δ)-diﬀerentially private, and the total noise is of
magnitude:
O
(cid:18) LSq∆(G) + (1 + log(1/δ))/ε
(cid:19)
.
ε
Note that this approach is computationally eﬃcient if we can eﬃciently evaluate the query q,
can eﬃciently calculate LSq (which requires at most |X| evaluations of q), and and have an upper
bound on GSLSq .
4 Releasing Many Counting Queries with Correlated Noise
We have seen (in Thms. 2.6, 2.7, and 2.9) that any set Q of counting queries over data universe X
can be answered with diﬀerential privacy and an error of at most:
(cid:112)|Q| · log(1/δ) · log log |Q|
(cid:112)|X| · log |Q|
(cid:41)(cid:33)
(cid:32)
α ≤ O
min
εn
,
εn
(cid:40)|Q|
,
εn
on each of the queries (with high probability). When both |Q| and |X| are larger than n2, the
amount of error is larger than 1, and hence these approaches provide nothing useful (recall that
the true answers lie in [0, 1]).
In this section, we will see two methods that can answer many more than n2 counting queries
on a data universe of size much larger than n2. Both use ideas from Learning Theory.
4.1 The SmallDB Algorithm
Theorem 4.1 (the SmallDB algorithm, due to Blum et al. [14]). For every set Q of counting
queries on a data universe X and every ε > 0, there exists an ε-diﬀerentially private mechanism
M such that for all datasets x ∈ Xn, with high probability M(x) answers all queries in Q to within
error at most
(cid:18) log |Q| log |X|
(cid:19)1/3
α = O
εn
.
Moreover, M (x) outputs a “synthetic dataset” y ∈ Xm with m = O(log |Q|/α2) such that with high
probability, we have |q(y) − q(x)| ≤ α for all q ∈ Q, i.e., we can calculate all the answers using the
(smaller) synthetic dataset.
In fact, the bounds can be improved to α = ˜O(VC(Q)· log |X|/εn)1/3 and m = VC(Q)· ˜O(1/α2),
where VC(Q) is the Vapnik-Chervonenkis dimension of the class Q.5
5VC(Q) is deﬁned to be the largest number k such that there exist x1, . . . , xk ∈ X for which {(q(x1), . . . , q(xk)) :
q ∈ Q} = {0, 1}k. Clearly, VC(Q) ≤ log |Q|.
26
The key point is that the error grows (less than) logarithmically with the number |Q| of queries
and the size |X| of the data universe; this allows us to handle even exponentially many queries.
(On the other hand, the error decays more slowly as n grows — like 1/n1/3 rather than 1/n.)
Let’s compare the implications of the SmallDB algorithm for concrete query families with the
bounds we saw in Section 2 for pure diﬀerential privacy (Table 2.1): We see that there is an
Table 4.1: Error bounds for speciﬁc query families under (ε, 0)-diﬀerential privacy on a data universe
X of size D = 2d (e.g. X = {0, 1}d or X = {1, 2, . . . , D}). Highlighted cells indicate the best bounds
in the regime where n ≤ Do(1) or n ≤ do(t).
Query family Q |Q|
Qpt
previous bound ref
VC(Q)
(cid:1)
D
1
Qthr
Qconj
Qmeans
D
3d
d
1
d
(cid:98)log2 d(cid:99)
Qconj
t
for t (cid:28) d O(dt) O(t log d) O
Thm. 2.9 O
Thm. 2.6 O
Thm. 2.6 O
√
εn
D)
˜O(
O(cid:0) d
O(cid:0) d
(cid:16) dt
εn
√
˜O(
εn
εn