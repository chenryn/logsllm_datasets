title:Making gnutella-like P2P systems scalable
author:Yatin Chawathe and
Sylvia Ratnasamy and
Lee Breslau and
Nick Lanham and
Scott Shenker
Making Gnutella-like P2P Systems Scalable
Yatin Chawathe
AT&T Labs–Research
PI:EMAIL
Sylvia Ratnasamy
Intel Research
Lee Breslau
AT&T Labs–Research
PI:EMAIL
PI:EMAIL
Nick Lanham
UC Berkeley
∗
Scott Shenker
ICSI
PI:EMAIL
PI:EMAIL
ABSTRACT
Napster pioneered the idea of peer-to-peer ﬁle sharing, and sup-
ported it with a centralized ﬁle search facility. Subsequent P2P sys-
tems like Gnutella adopted decentralized search algorithms. How-
ever, Gnutella’s notoriously poor scaling led some to propose dis-
tributed hash table solutions to the wide-area ﬁle search problem.
Contrary to that trend, we advocate retaining Gnutella’s simplicity
while proposing new mechanisms that greatly improve its scalabil-
ity. Building upon prior research [1, 12, 22], we propose several
modiﬁcations to Gnutella’s design that dynamically adapt the over-
lay topology and the search algorithms in order to accommodate the
natural heterogeneity present in most peer-to-peer systems. We test
our design through simulations and the results show three to ﬁve or-
ders of magnitude improvement in total system capacity. We also re-
port on a prototype implementation and its deployment on a testbed.
Categories and Subject Descriptors
C.2 [Computer Communication Networks]: Distributed Systems
General Terms
Algorithms, Design, Performance, Experimentation
Keywords
Peer-to-peer, distributed hash tables, Gnutella
1.
INTRODUCTION
The peer-to-peer ﬁle-sharing revolution started with the introduc-
tion of Napster in 1999. Napster was the ﬁrst system to recognize
that requests for popular content need not be sent to a central server
but instead could be handled by the many hosts, or peers, that al-
ready possess the content. Such serverless peer-to-peer systems can
achieve astounding aggregate download capacities without requiring
any additional expenditure for bandwidth or server farms.1 More-
∗
Supported in part by NSF grants ITR-0205519, ANI-0207399,
ITR-0121555, ITR-0081698, ITR-0225660 and ANI-0196514.
1For instance, 100,000 peers all connected at 56kbps can provide
more aggregate download capacity than a single server farm con-
nected by two OC-48 links.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
SIGCOMM’03, August 25–29, 2003, Karlsruhe, Germany.
Copyright 2003 ACM 1-58113-735-4/03/0008 ...$5.00.
over, such P2P ﬁle-sharing systems are self-scaling in that as more
peers join the system to look for ﬁles, they add to the aggregate
download capability as well.2
However, to make use of this self-scaling behavior, a node looking
for ﬁles must ﬁnd the peers that have the desired content. Napster
used a centralized search facility based on ﬁle lists provided by each
peer. By centralizing search (which does not require much band-
width) while distributing download (which does), Napster achieved
a highly functional hybrid design.
The resulting system was widely acknowledged as “the fastest
growing Internet application ever”[4]. But RIAA’s lawsuit forced
Napster to shut down, and its various centralized-search successors
have faced similar legal challenges. These centralized systems have
been replaced by new decentralized systems such as Gnutella [8]
that distribute both the download and search capabilities. These sys-
tems establish an overlay network of peers. Queries are not sent to
a central site, but are instead distributed among the peers. Gnutella,
the ﬁrst of such systems, uses an unstructured overlay network in
that the topology of the overlay network and placement of ﬁles within
it is largely unconstrained. It ﬂoods each query across this overlay
with a limited scope. Upon receiving a query, each peer sends a
list of all content matching the query to the originating node. Be-
cause the load on each node grows linearly with the total number
of queries, which in turn grows with system size, this approach is
clearly not scalable.
Following Gnutella’s lead, several other decentralized ﬁle-sharing
systems such as KaZaA [24] have become popular. KaZaA is based
on the proprietary Fasttrack technology which uses specially desig-
nated supernodes that have higher bandwidth connectivity. Pointers
to each peer’s data are stored on an associated supernode, and all
queries are routed to supernodes. While this approach appears to
offer better scaling than Gnutella, its design has been neither docu-
mented nor analyzed. Recently, there have been proposals to incor-
porate this approach into the Gnutella network [7]. Although some
Gnutella clients now implement the supernode proposal, its scalabil-
ity has neither been measured nor been analyzed.
That said, we believe that the supernode approach popularized
by KaZaA is a step in the right direction for building scalable ﬁle-
sharing systems.
In this paper, we leverage this idea of exploit-
ing node heterogeneity, but make the selection of “supernodes” and
construction of the topology around them more dynamic and adap-
tive. We present a new P2P ﬁle-sharing system, called Gia.3 Like
Gnutella and KaZaA, Gia is decentralized and unstructured. How-
ever, its unique design achieves an aggregate system capacity that is
2This self-scaling property is mitigated to some extent by the free
rider problem observed in such systems [2].
3Gia is short for gianduia, which is the generic name for the hazelnut
spread, Nutella.
three to ﬁve orders of magnitude better than that of Gnutella as well
as that of other attempts to improve Gnutella [12, 24]. As such, it
retains the simplicity of an unstructured system while offering vastly
improved scalability.
The design of Gia builds on a substantial body of previous work.
As in the recent work by Lv et al. [12], Gia replaces Gnutella’s ﬂood-
ing with random walks. Following the work of Adamic et al. [1], Gia
recognizes the implications of the overlay network’s topology while
using random walks and therefore includes a topology adaptation al-
gorithm. Similarly, the lack of ﬂow control has been recognized as
a weakness in the original Gnutella design [16], and Gia introduces
a token-based ﬂow control algorithm. Finally, like KaZaA, Gia rec-
ognizes that there is signiﬁcant heterogeneity in peer bandwidth and
incorporates heterogeneity into each aspect of our design.
While Gia does build on these previous contributions, Gia is, to
our knowledge, the ﬁrst open design that (a) combines all these el-
ements, and (b) recognizes the fact that peers have capacity con-
straints and adapts its protocols to account for these constraints.
Our simulations suggest that this results in a tremendous boost for
Gia’s system performance. Moreover, this performance improve-
ment comes not just from a single design decision but from the syn-
ergy among the various design features.
We discuss Gia’s design in Section 3, its performance in Section
4, and a prototype implementation and associated practical issues in
Section 5. However, before embarking on the description of Gia, we
ﬁrst ask why not just use Distributed Hash Tables (DHTs).
2. WHY NOT DHTS?
Distributed Hash Tables are a class of recently-developed sys-
tems that provide hash-table-like semantics at Internet scale [25, 18,
27]. Much (although not all) of the original rationale for DHTs was
to provide a scalable replacement for unscalable Gnutella-like ﬁle
sharing systems. The past few years has seen a veritable frenzy of
research activity in this ﬁeld, with many design proposals and sug-
gested applications. All of these proposals use structured overlay
networks where both the data placement and overlay topology are
tightly controlled. The hash-table-like lookup() operation provided
by DHTs typically requires only O(log n) steps, whereas in com-
parison, Gnutella requires O(n) steps to reliably locate a speciﬁc
ﬁle.
Given this level of performance gain afforded by DHTs, it is natu-
ral to ask why bother with Gia when DHTs are available. To answer
this question, we review three relevant aspects of P2P ﬁle sharing.
#1: P2P clients are extremely transient. Measured activ-
ity in Gnutella and Napster indicates that the median up-time for a
node is 60 minutes [22].4 For large systems of, say, 100,000 nodes,
this implies a churn rate of over 1600 nodes coming and going per
minute. Churn causes little problem for Gnutella and other systems
that employ unstructured overlay networks as long as a peer doesn’t
become disconnected by the loss of all of its neighbors, and even
in that case the peer can merely repeat the bootstrap procedure to
re-join the network. In contrast, churn does cause signiﬁcant over-
head for DHTs. In order to preserve the efﬁciency and correctness
of routing, most DHTs require O(log n) repair operations after each
failure. Graceless failures, where a node fails without beforehand
informing its neighbors and transferring the relevant state, require
more time and work in DHTs to (a) discover the failure and (b) re-
replicate the lost data or pointers. If the churn rate is too high, the
overhead caused by these repair operations can become substantial
4We understand that there is some recently published work [3] that
questions the exact numbers in this study, but the basic point remains
that the peer population is still quite transient.
s
t
s
e
u
q
e
r
d
a
o
n
w
o
d
#
l
 900
 800
 700
 600
 500
 400
 300
 200
 100
 0
1
2
4
8
32
# available replicas
16
64
128
256
Figure 1: Most download requests are for well-replicated ﬁles.
and could easily overwhelm nodes with low-bandwidth dial-up con-
nections.
#2: Keyword searches are more prevalent, and more im-
portant, than exact-match queries. DHTs excel at support-
ing exact-match lookups: given the exact name of a ﬁle, they trans-
late the name into a key and perform the corresponding lookup(key)
operation. However, DHTs are les adept at supporting keyword
searches: given a sequence of keywords, ﬁnd ﬁles that match them.
The current use of P2P ﬁle-sharing systems, which revolves around
sharing music and video, requires such keyword matching. For ex-
ample, to ﬁnd the song “Ray of Light” by Madonna, a user typically
submits a search of the form “madonna ray of light” and expects the
ﬁle-sharing system to locate ﬁles that match all of the keywords in
the search query. This is especially important since there is no unam-
biguous naming convention for ﬁle names in P2P systems, and thus
often the same piece of content is stored by different nodes under
several (slightly different) names.
Supporting such keyword searching on top of DHTs is a non-
trivial task. For example, the typical approach [11, 19, 26] of con-
structing an inverted index per keyword can be expensive to main-
tain in the face of frequent node (and hence ﬁle) churn. This is only
further complicated by the additional caching algorithms needed to
avoid overloading nodes that store the index for popular keywords.
It is possible that some of these problems maybe addressable in
DHTs, as indicated by the deployment of the Overnet ﬁle sharing
application [15], which is based on the Kademlia DHT [14]. Still,
DHT-based solutions typically need to go to great lengths to incor-
porate query models beyond the simple exact-match search. In con-
trast, Gnutella and other similar systems effortlessly support key-
word searches and other complex queries since all such searches are
executed locally on a node-by-node basis.
#3: Most queries are for hay, not needles. DHTs have exact
recall, in that knowing the name of a ﬁle allows you to ﬁnd it, even
if there is only a single copy of that ﬁle in the system. In contrast,
Gnutella cannot reliably ﬁnd single copies of ﬁles unless the ﬂooded
query reaches all nodes; we call such ﬁles needles. However, we
expect that most queries in the popular P2P ﬁle-sharing systems are
for relatively well-replicated ﬁles, which we call hay. By the very
nature of P2P ﬁle-sharing, if a ﬁle is requested frequently, then as
more and more requesters download the ﬁle to their machines, there
will be many copies of it within the system. We call such systems,
where most queries are for well-replicated content, mass-market ﬁle-
sharing systems.
Gnutella can easily ﬁnd well-replicated ﬁles. Thus, if most searches
are for hay, not needles, then Gnutella’s lack of exact recall is not a
signiﬁcant disadvantage. To verify our conjecture that most queries
are indeed for hay, we gathered traces of queries and download
requests using an instrumented Gnutella client. Our tracing tool
crawled the Gnutella network searching for ﬁles that match the top
50 query requests seen. After gathering the ﬁle names and the num-
ber of available copies of each of these ﬁles, the tool turned around
and offered the same ﬁles for download to other Gnutella clients. We
then measured the number of download requests seen by the trac-
ing tool for this offered content. Figure 1 shows the distribution of
the download requests versus the number of available replicas. We
notice that most of the requests correspond to ﬁles that have a large
number of available replicas.5 For example, half of the requests were
for ﬁles with more than 100 replicas, and approximately 80% of the
requests were for ﬁles with more than 80 replicas.
In summary, Gnutella-like designs are more robust in the face of
transients and support general search facilities, both important prop-
erties to P2P ﬁle sharing. They are less adept than DHTs at ﬁnding
needles, but this may not matter since most P2P queries are for hay.
Thus, we conjecture that for mass-market ﬁle-sharing applications,
improving the scalability of unstructured P2P systems, rather than
turning to DHT-based systems, may be the better approach.
3. GIA DESIGN
Gnutella-like systems have one basic problem: when faced with
a high aggregate query rate, nodes quickly become overloaded and
the system ceases to function satisfactorily. Moreover, this prob-
lem gets worse as the size of the system increases. Our ﬁrst goal
in designing Gia is to create a Gnutella-like P2P system that can
handle much higher aggregate query rates. Our second goal is to
have Gia continue to function well with increasing system sizes. To
achieve this scalability, Gia strives to avoid overloading any of the
nodes by explicitly accounting for their capacity constraints. In an
earlier workshop paper [13], we presented a preliminary proposal
for incorporating capacity awareness into Gnutella. In our current
work, we reﬁne those ideas and present a thorough design, detailed
algorithms, and a prototype implementation of the new system. We
begin with an overview of the reasoning behind our system design
and then provide a detailed discussion of the various components
and protocols.
3.1 Design Rationale
The Gnutella protocol [6] uses a ﬂooding-based search method to
ﬁnd ﬁles within its P2P network. To locate a ﬁle, a node queries each
of its neighbors, which in turn propagate the query to their neigh-
bors, and so on until the query reaches all of the clients within a
certain radius from the original querier. Although this approach can
locate ﬁles even if they are replicated at an extremely small number
of nodes, it has obvious scaling problems. To address this issue, Lv
et al. [12] proposed replacing ﬂooding with random walks. Random
walks are a well-known technique in which a query message is for-
warded to a randomly chosen neighbor at each step until sufﬁcient
responses to the query are found. Although they make better uti-
lization of the P2P network than ﬂooding, they have two associated
problems:
1. A random walk is essentially a blind search in that at each
step a query is forwarded to a random node without taking
into account any indication of how likely it is that the node
will have responses for the query.
5Note that since the tracing tool only captures the download requests
that came directly to it, we miss all of the requests that went to the
other nodes that also had copies of the same ﬁle. Thus our numbers
can only be a lower bound on how popular well-replicated content
is.
2. If a random walker query arrives at a node that is already over-
loaded with trafﬁc, it may get queued for a long time before it
is handled.
Adamic et al. [1] addressed the ﬁrst problem by recommend-
ing that instead of using purely random walks, the search protocol
should bias its walks toward high-degree nodes. The intuition be-
hind this is that if we arrange for neighbors to be aware of each
other’s shared ﬁles, high-degree nodes will have (pointers to) a large
number of ﬁles and hence will be more likely to have an answer that
matches the query. However, this approach ignores the problem of
overloaded nodes. In fact, by always biasing the random walk to-
wards high-degree nodes, it can exacerbate the problem if the high-
degree node does not have the capacity to handle a large number of
queries.
The design of Gia, on the other hand, explicitly takes into ac-
count the capacity constraints associated with each node in the P2P
network. The capacity of a node depends upon a number of fac-
tors including its processing power, disk latencies, and access band-
width. It is well-documented that nodes in networks like Gnutella
exhibit signiﬁcant heterogeneity in terms of their capacity to handle
queries [22]. Yet, none of the prior work on scaling Gnutella-like
systems leverages this heterogeneity. In the design of Gia, we ex-
plicitly accommodate (and even exploit) heterogeneity to achieve
better scaling. The four key components of our design are summa-
rized below:
• A dynamic topology adaptation protocol that puts most nodes
within short reach of high capacity nodes. The adaptation
protocol ensures that the well-connected (i.e., high-degree)
nodes, which receive a large proportion of the queries, actu-
ally have the capacity to handle those queries.
• An active ﬂow control scheme to avoid overloaded hot-spots.
The ﬂow control protocol explicitly acknowledges the exis-
tence of heterogeneity and adapts to it by assigning ﬂow-control
tokens to nodes based on available capacity.
• One-hop replication of pointers to content. All nodes main-
tain pointers to the content offered by their immediate neigh-
bors. Since the topology adaptation algorithm ensures a con-
gruence between high capacity nodes and high degree nodes,
the one-hop replication guarantees that high capacity nodes
are capable of providing answers to a greater number of queries.
• A search protocol based on biased random walks that directs
queries towards high-capacity nodes, which are typically best
able to answer the queries.
3.2 Detailed Design
The framework for the Gia client and protocols is modeled after