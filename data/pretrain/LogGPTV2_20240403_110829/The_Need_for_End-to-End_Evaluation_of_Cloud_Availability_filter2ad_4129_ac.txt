Koszalin University of Technology in Poland to Amazon S3’s Singapore site. In
fact, we observe intermittent failures between that source and destination pair
for the entire duration of our measurement. This case shows that sometimes
network problems between the VP and cloud (such as routing problems) persist
for some time. Both ICMP and HTTP report outages for this VP.
126
Z. Hu et al.
ICMP: 9 fails
HTTP: 9 fails
ICMP: 0<fails<9
HTTP: 0<fails<9
ICMP: success
HTTP: success
one VP at Tech Univ of Koszalin, PL
shows intermittent failures
two VPs observe a HTTP-only outage
Princeton Univ, US
Tech Univ of Koszalin, PL
Northwestern Univ, US
Indiana Univ, US
ETHZ, CH
Univ of Neuchatel, CH
Univ of Basel, CH
Zhongshan Univ, CN
Moscow EE Institute, RU
Moscow EE Institute, RU
Moscow State Univ, RU
00:00 PDT
2013-06-23
06:00
12:00
18:00
00:00 PDT
2013-06-24
Fig. 5. Strip chart: Amazon S3 (Singapore). Dataset: 2013-06-18+75.
ICMP: 9 fails
HTTP: 9 fails
ICMP: 0<fails<9
HTTP: 0<fails<9
ICMP: success
HTTP: success
three VPs observe an ICMP-only outage
00:00 PDT
2013-07-01
06:00
12:00
18:00
00:00 PDT
2013-07-02
Princeton Univ, US
Tech Univ of Koszalin, PL
Northwestern Univ, US
Ege Univ, TR
ETHZ, CH
Univ of Neuchatel, CH
Univ of Basel, CH
Zhongshan Univ, CN
USTC, CN
Tsinghua Univ, CN
Moscow State Univ, RU
Fig. 6. Strip chart: Amazon VM (N. California). Dataset: 2013-06-18+17.
Method Disagreement: However, HTTP and ICMP probes can also show
disagreement. We see disagreement in 0.01% to 3% of observations, as shown by
the stacked bars in Figure 3. The source of the disagreement is usually ICMP
failures with HTTP success (the bottom, red striped bars), but sometimes ICMP
succeeds and HTTP fails (the much smaller blue bars on top).
As a ﬁrst example where ICMP fails but HTTP succeeds, Figure 6 shows a
case where three Swiss universities could not reach Amazon/VM in California.
We see with tcpdump that ﬁltering happens on the return path. Since the three
VPs reporting this ICMP-only outage are at diﬀerent sites in the same country,
we hypothesize that reverse path changes–possibly to a path that ﬁltered ICMP–
caused the outage. In this case, despite ICMP reporting multiple outages, we can
still fetch the data in the cloud, meaning that ICMP over-counts outages.
We also see the reverse case, where HTTP fails but ICMP succeeds, overes-
timating cloud availability. Figures 5 and 7 show two VPs in Russia observing
an HTTP-only outage to both Amazon S3 and EC2 in Singapore. We observe
route changes before and after the outage, and we conﬁrm our probes (here TCP
SYNs) reach the VM and replies are sent but do not reach the VP. We cannot
conﬁrm the root cause for this outage, although we guess there may be problems
in a load-balancer at the cloud’s edge.
4.2 Diﬀerences between Probing VMs and Storage
In addition to comparing network and application probing, we also probe diﬀer-
ent targets: virtual machines and storage. The target aﬀects what the probing
The Need for End-to-End Evaluation of Cloud Availability
127
ICMP: 9 fails
HTTP: 9 fails
ICMP: 0<fails<9
HTTP: 0<fails<9
ICMP: success
HTTP: success
two VPs observe a HTTP-only outage
Princeton Univ, US
Tech Univ of Koszalin, PL
Northwestern Univ, US
Indiana Univ, US
ETHZ, CH
Univ of Neuchatel, CH
Univ of Basel, CH
Zhongshan Univ, CN
Moscow EE Institute, RU
Moscow EE Institute, RU
Moscow State Univ, RU
00:00 PDT
2013-06-23
06:00
12:00
18:00
00:00 PDT
2013-06-24
Fig. 7. Strip chart: Amazon VM (Singapore). Dataset: 2013-06-18+17.
ICMP: 3 fails
HTTP: 3 fails
ICMP: 2 fails
HTTP: 2 fails
ICMP: 1 fail
HTTP: 1 fail
back-end outage causing
HTTP-only failures to
many VPs
ICMP: success
HTTP: success
Univ of Michigan, US
Indiana Univ, US
Northwestern Univ, US
Univ of Waterloo, CA
Tech Univ of Koszalin, PL
Tampere Univ, FI
Univ of Basel, CH
Moscow EE Institute, RU
Moscow State Univ, RU
Monash Univ, AU
Univ of Sao Paulo, BR
13:00
2013-04-16
17:00
21:00
00:00 PDT
2013-04-17
04:00
08:00
12:00
Fig. 8. Strip chart: Amazon S3 (Japan). Dataset: 2013-03-11+33.
mechanism sees. We next show that end-to-end measurements are essential to
observe outages in cloud storage and other systems with complex back-ends.
Figure 8 shows an outage for Amazon S3 in Tokyo on April 16. Only HTTP
measurements detect this outage; ICMP reports that all is well. This outage is
conﬁrmed by Amazon outage report [3].
To understand this discrepancy, we must consider what exactly ICMP and
HTTP measure when observing a storage system. For storage systems, a user
accesses a front-end system with a URL, but data retrieval exercises the back-end
storage system. ICMP measures only to this front-end, while HTTP provides an
end-to-end test, verifying that the storage system is functioning (at least for one
stored object). We can therefore infer this outage was inside Amazon’s storage
system and not in the network from the VP to the datacenter. We conclude
that ICMP will overestimate the availability of cloud storage, supporting our
recommendation for end-to-end outage testing for higher-level cloud services.
To understand the root cause of these storage outages, we next use errors
reported by our storage retrieval tool (curl). We look at the error returned from
each failed attempt of storage retrieval from the 2013-06-18+75 storage dataset.
We see that most of these (87%) are due to DNS lookup failure, with the second
largest cause (10%) due to TCP connection setup failure. In contrast, for VMs
(dataset: 2013-06-18+17), almost all failures (99%) are caused by TCP connec-
tion setup failures. All of the storage systems use DNS to map a request into
the storage back-end systems. These DNS failures can represent either random
loss of the request in the network, or failure of the storage system’s DNS mech-
anism to identify a storage server. Since applications that use cloud storage will
follow a similar process as curl which is used in our measurements, these types
of outages reﬂect intermittent problems that should be reported.
128
Z. Hu et al.
Based on our measurement results, we show that ICMP probes can be inaccu-
rate at estimating cloud availability. ICMP is not as robust as application-level
measurements such as HTTP. ICMP’s failure to solicit a response does not mean
that the service is down, so ICMP can underestimate availability. At the same
time, ICMP can also overestimate availability as it measures reachability of the
cloud’s edge, missing failures in the cloud’s back-end. We therefore suggest us-
ing application-level probes such as HTTP rather than network-level probes to
evaluate cloud reliability; the examples in this section present the motivation for
a longer-term study.
5 Related Work
Our work builds upon previous eﬀorts in two broad areas: characterizing the
Internet’s availability and measurement of cloud services.
Internet Availability: To date, a large number of measurement studies have
probed the Internet from a distributed set of vantage points in order to char-
acterize the Internet’s availability. While some studies rely on passive measure-
ments of Internet traﬃc to detect the onset of outages (for example, [27,4]),
such monitoring is possible only by instrumenting a popular service. Therefore,
most measurement studies of the Internet’s availability have instead relied on
continuous probing of a large number of end-hosts. These studies have focused
on identifying outages [14,23], network failures [6,28], characterizing the typical
duration of outages [14,10,15], and pinpointing their root causes [8,13]. Some
studies have paid particular attention to measurement methodology of paths [7]
and of the edge [23]. However, all of these studies have in common a reliance
on ICMP-based probes. While ICMP may be necessary for Internet-wide stud-
ies, our results show that application-level measurements should be used when
possible, and they are essential to understanding availability of cloud services,
where ICMP-based probing can both over- and under-predict outages.
Measurements of Cloud Services: Some recent work has begun on measure
and characterize the performance oﬀered by cloud services. CloudCmp mea-
sures the compute, storage, and network performance oﬀered by various cloud
services with the goal of enabling application providers to choose from these
services [16]. Others have performed measurements of cloud services in order to
determine when it is beneﬁcial for applications to be hosted in the cloud [11,21].
To the best of our knowledge, we are the ﬁrst to investigate the methodology
of active monitoring of the availability of cloud services. Motoyama et al. pur-
sue a complementary approach of inferring outages from indirect information in
Twitter posts [20]; further investigation is necessary to correlate outages in web
services to outages of the underlying cloud services on which they are deployed.
The Need for End-to-End Evaluation of Cloud Availability
129
6 Conclusion
This paper compared network and application level measurements sensitive of
cloud service availability. We compare ICMP and HTTP over two types of
services (VMs and storage) and three providers. We ﬁnd that ICMP can both
over- and under-report outages, suggesting that it is important to use end-to-
end measures (such as HTTP) to best characterize cloud service availability. Our
study raises concerns about the use of ICMP for monitoring availability and sug-
gests that earlier results should be revisited. We are using these approaches as
part of a long-term study of cloud availability. Part of our ongoing work is to
understand cloud availability in order to deploy highly-available systems at low
cost across various cloud providers, just as existing work uses multiple providers
to provide low latency at low cost [26].
References
1. Outages mailing list. Mailing List, http://www.outages.org
2. Abu-Libdeh, H., Princehouse, L., Weatherspoon, H.: RACS: A case for cloud stor-
age diversity. In: SoCC (2010)
3. Amazon. AWS Service Health Dashboard, http://status.aws.amazon.com/
4. Choﬀnes, D.R., Bustamante, F.E., Ge, Z.: Crowdsourcing service-level network
event monitoring. In: SIGCOMM (2010)
5. Chun, B., Culler, D., Roscoe, T., Bavier, A., Peterson, L., Wawrzoniak, M., Bow-
man, M.: PlanetLab: An overlay testbed for broad-coverage services. In: SIG-
COMM CCR (2003)
6. Cunha, I., Teixeira, R., Feamster, N., Diot, C.: Measurement methods for fast and
accurate blackhole identiﬁcation with binary tomography. In: IMC (2009)
7. Cunha, I., Teixeira, R., Veitch, D., Diot, C.: Predicting and tracking internet path
changes. In: SIGCOMM (2011)
8. Dhamdhere, A., Teixeira, R., Dovrolis, C., Diot, C.: Netdiagnoser: troubleshooting
network unreachabilities using end-to-end probes and routing data. In: CoNEXT
(2007)
9. Flach, T., Dukkipati, N., Terzis, A., Raghavan, B., Cardwell, N., Cheng, Y., Jain,
A., Hao, S., Katz-Bassett, E., Govindan, R.: Reducing web latency: the virtue of
gentle aggression. In: SIGCOMM (2013)
10. Gummadi, K.P., Madhyastha, H.V., Gribble, S.D., Levy, H.M., Wetherall, D.: Im-
proving the reliability of Internet paths with one-hop source routing. In: OSDI
(2004)
11. Hajjat, M., Sun, X., Sung, Y.-W.E., Maltz, D., Rao, S., Sripanidkulchai, K., Tawar-
malani, M.: Cloudward bound: planning for beneﬁcial migration of enterprise ap-
plications to the cloud. In: SIGCOMM (2010)
12. Heidemann, J., Pradkin, Y., Govindan, R., Papadopoulos, C., Bartlett, G., Ban-
nister, J.: Census and survey of the visible Internet. In: IMC (2008)
13. Javed, U., Cunha, I., Choﬀnes, D.R., Katz-Bassett, E., Krishnamurthy, A., An-
derson, T.: PoiRoot: Investigating the root cause of interdomain path changes. In:
SIGCOMM (2013)
14. Katz-Bassett, E., Madhyastha, H.V., John, J.P., Krishnamurthy, A., Wetherall, D.,
Anderson, T.: Studying black holes in the Internet with Hubble. In: NSDI (2008)
130
Z. Hu et al.
15. Katz-Bassett, E., Scott, C., Choﬀnes, D.R., Cunha, I., Valancius, V., Feamster,
N., Madhyastha, H.V., Anderson, T., Krishnamurthy, A.: LIFEGUARD: Practical
repair of persistent route failures. In: SIGCOMM (2012)
16. Li, A., Yang, X., Kandula, S., Zhang, M.: Cloudcmp: comparing public cloud
providers. In: IMC (2010)
17. Lohr, S.: Amazon’s trouble raises cloud computing doubts (April 2011),
http://www.nytimes.com/2011/04/23/technology/23cloud.html
18. Luckie, M., Hyun, Y., Huﬀaker, B.: Traceroute probe method and forward IP path
inference. In: IMC (2008)
19. Madhyastha, H.V., Isdal, T., Piatek, M., Dixon, C., Anderson, T., Krishnamurthy,
A., Venkataramani, A.: iPlane: An information plane for distributed services. In:
OSDI (2006)
20. Motoyama, M., Meeder, B., Levchenko, K., Voelker, G.M., Savage, S.: Measuring
online service availability using Twitter. In: WOSN (2010)
21. Palankar, M.R., Iamnitchi, A., Ripeanu, M., Garﬁnkel, S.: Amazon S3 for science
grids: a viable solution? In: DADC (2008)
22. Paxson, V.: End-to-end internet packet dynamics. In: SIGCOMM (1997)
23. Quan, L., Heidemann, J., Pradkin, Y.: Trinocular: understanding internet reliabil-
ity through adaptive probing. In: SIGCOMM (2013)
24. Spring, N., Peterson, L., Bavier, A., Pai, V.: Using PlanetLab for network research:
Myths, realities, and best practices. SIGOPS Oper. Syst. Rev. (2006)
25. Wood, T., Cecchet, E., Ramakrishnan, K.K., Shenoy, P., van der Merwe, J.,
Venkataramani, A.: Disaster recovery as a cloud service: economic beneﬁts & de-
ployment challenges. In: HotCloud (2010)
26. Wu, Z., Butkiewicz, M., Perkins, D., Katz-Bassett, E., Madhyastha, H.V.:
Spanstore: Cost-eﬀective geo-replicated storage spanning multiple cloud services.
In: SOSP 2013 (2013)
27. Zhang, M., Zhang, C., Pai, V., Peterson, L., Wang, R.: PlanetSeer: Internet path
failure monitoring and characterization in wide-area services. In: OSDI (2004)
28. Zhang, Z., Zhang, Y., Hu, Y.C., Mao, Z.M., Bush, R.: iSPY: Detecting IP preﬁx
hijacking on my own. In: SIGCOMM (2008)