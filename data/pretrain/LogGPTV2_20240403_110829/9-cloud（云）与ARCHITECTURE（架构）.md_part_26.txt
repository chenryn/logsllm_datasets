README.txt
sbin
share
aa
\[root@nn01 hadoop\]# ssh node2 ls /usr/local/hadoop/
bin
etc
include
lib
libexec
LICENSE.txt
NOTICE.txt
bb
README.txt
sbin
share
aa
\[root@nn01 hadoop\]# ssh node3 ls /usr/local/hadoop/
bin
etc
include
lib
libexec
LICENSE.txt
NOTICE.txt
bb
README.txt
sbin
share
aa
步骤三：格式化
\[root@nn01 hadoop\]# cd /usr/local/hadoop/
\[root@nn01 hadoop\]# ./bin/hdfs namenode -format //格式化 namenode
\[root@nn01 hadoop\]# ./sbin/start-dfs.sh //启动
\[root@nn01 hadoop\]# jps //验证角色
23408 NameNode
23700 Jps
23591 SecondaryNameNode
\[root@nn01 hadoop\]# ./bin/hdfs dfsadmin -report //查看集群是否组建成功
Live datanodes (3): //有三个角色成功
# \-\-\-\-\--NSD ARCHITECTURE DAY07\-\-\-\-\-\-\-\--
# hadoop高可用常用组件
Hadoop是一个由Apache基金会所开发的分布式系统基础架构。
用户可以在不了解分布式底层细节的情况下，开发分布式程序。充分利用集群的威力进行高速运算和存储。
Hadoop实现了一个分布式文件系统（Hadoop Distributed File
System），简称HDFS。HDFS有高容错性的特点，并且设计用来部署在低廉的（low-cost）硬件上；而且它提供高吞吐量（high
throughput）来访问应用程序的数据，适合那些有着超大数据集（large data
set）的应用程序。HDFS放宽了（relax）POSIX的要求，可以以流的形式访问（streaming
access）文件系统中的数据。
Hadoop的框架最核心的设计就是：HDFS和MapReduce。HDFS为海量的数据提供了存储，而MapReduce则为海量的数据提供了计算。
## zookeeper:
ZooKeeper是一个分布式的，开放源码的分布式应用程序协调服务，是Google的Chubby一个开源的实现，是Hadoop和Hbase的重要组件。它是一个为分布式应用提供一致性服务的软件，提供的功能包括：配置维护、域名服务、分布式同步、组服务等。
zookeeper是一个开源的分布式应用程序协调服务,用来保证数据在集群间的事务一致性
zookeeper应用场景：集群分布式锁、集群统一命名服务、分布式协调服务
![LINUXNSD_V01ARCHITECTUREDAY07_007](media/image172.png){width="6.722606080489939in"
height="3.0462576552930885in"}
leader:领导 follower:追随者
## zookeeper角色与选举
![LINUXNSD_V01ARCHITECTUREDAY07_008
2](media/image173.png){width="7.264583333333333in"
height="3.9381944444444446in"}
![LINUXNSD_V01ARCHITECTUREDAY07_009](media/image174.png){width="7.264583333333333in"
height="4.271527777777778in"}
## Kafka集群
Kafka是由Apache软件基金会开发的一个开源流处理平台，由Scala和Java编写。Kafka是一种高吞吐量的分布式发布订阅消息系统，它可以处理消费者规模的网站中的所有动作流数据。
这种动作（网页浏览，搜索和其他用户的行动）是在现代网络上的许多社会功能的一个关键因素。
这些数据通常是由于吞吐量的要求而通过处理日志和日志聚合来解决。
对于像Hadoop一样的日志数据和离线分析系统，但又要求实时处理的限制，这是一个可行的解决方案。Kafka的目的是通过Hadoop的并行加载机制来统一线上和离线的消息处理，也是为了通过集群来提供实时的消息。
Kafka 是一种高吞吐量的分布式发布订阅消息系统，有如下特性：
-   通过iO的磁盘数据结构提供消息的持久化，这种结构对于即使数以TB的消息存储也能够保持长时间的稳定性能。
-   高吞吐量 ：即使是非常普通的硬件Kafka也可以支持每秒数百万的消息。
-   支持通过Kafka服务器和消费机集群来分区消息。
-   支持Hadoop并行数据加载
# Hadoop高可用
准备:192.168.1.20 nn02
10 hostnamectl set-hostname nn02
11 exit
12 yum repolist
13 vim /etc/ssh/ssh_config
14 ssh-keygen
15 ssh-copy-id 192.168.1.10
16 ssh-copy-id 192.168.1.11
17 ssh-copy-id 192.168.1.12
18 ssh-copy-id 192.168.1.13
19 ssh 192.168.
20 ssh 192.168.1.11
21 ping nn01
22 yum -y install java-1.8.0-openjdk-devel
**主机环境如下:**
**yum源,java-1.8.0-openjdk-devel,hosts,IP地址等配置此处省略不写**
192.168.1.10 nn01 192.168.1.11 node1
192.168.1.12 node2 192.168.1.13 node3
192.168.1.20 nn02
**因为接着上面的步骤操作,需要进行此操作:**
nn01 node1 node2 node3 停止所有服务,删除Hadoop数据
\[root@nn01 ansible\]# /usr/local/kafka/bin/kafka-server-stop.sh -daemon
/usr/local/kafka/config/server.properties
\[root@nn01 ansible\]# /usr/local/zookeeper/bin/zkServer.sh stop
\[root@nn01 ansible\]# /usr/local/hadoop/sbin/stop-all.sh
\[root@nn01 ansible\]# rm -rf /var/hadoop/\*
### 配置:core-site.xml 文件
\[root@nn01 ansible\]# vim /usr/local/hadoop/etc/hadoop/core-site.xml
\
\
\fs.defaultFS\
\hdfs://mycluster\
\file system\
\
\
\hadoop.tmp.dir\
\/var/hadoop\
\
\
\ha.zookeeper.quorum\
\node1:2181,node2:2181,node3:2181\
\
\
\hadoop.proxyuser.nsd1811.groups\
\\*\
\
\
\hadoop.proxyuser.nsd1811.hosts\
\\*\
\
\
### 配置:hdfs-site.xml 文件
\[root@nn01 ansible\]# vim /usr/local/hadoop/etc/hadoop/hdfs-site.xml
\
\
\dfs.nameservices\
\mycluster\
\
\
\dfs.ha.namenodes.mycluster\
\nn1,nn2\
\
\
\dfs.namenode.rpc-address.mycluster.nn1\
\nn01:8020\
\
\
\dfs.namenode.rpc-address.mycluster.nn2\
\nn02:8020\
\
\
\dfs.namenode.http-address.mycluster.nn1\
\nn01:50070\
\
\
\dfs.namenode.http-address.mycluster.nn2\
\nn02:50070\
\
\
\dfs.namenode.shared.edits.dir\
\qjournal://node1:8485;node2:8485;node3:8485/mycluster\
\
\
\dfs.journalnode.edits.dir\
\/var/hadoop/journal\
\
\
\dfs.client.failover.proxy.provider.mycluster\
\org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider\
\
\
\dfs.ha.fencing.methods\
\sshfence\
\
\
\dfs.ha.fencing.ssh.private-key-files\
\/root/.ssh/id_rsa\
\
\
\dfs.ha.automatic-failover.enabled\
\true\
\
\
\dfs.replication\
\2\
\
\
### 配置:mapred-site.xml文件
\
\
\mapreduce.framework.name\
\yarn\
\
\
### 配置:yarn-site.xml 文件
\
\
\
\yarn.resourcemanager.ha.enabled\
\true\
\
\
\yarn.resourcemanager.ha.rm-ids\
\rm1,rm2\
\
\
\yarn.resourcemanager.recovery.enabled\
\true\
\
\
\yarn.resourcemanager.store.class\
\org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore\
\
\
\yarn.resourcemanager.zk-address\
\node1:2181,node2:2181,node3:2181\
\
\
\yarn.resourcemanager.cluster-id\
\yarn-ha\
\
\
\yarn.resourcemanager.hostname.rm1\
\nn01\
\
\
\yarn.resourcemanager.hostname.rm2\
\nn02\
\
\
\yarn.nodemanager.aux-services\
\mapreduce_shuffle\
\
\
### 配置:hadoop-env.sh文件
25 export
JAVA_HOME=\"/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.131-11.b12.el7.x86_64/jre\"
33 export HADOOP_CONF_DIR=\"/usr/local/hadoop/etc/hadoop\"
### 配置:slaves文件
\[root@nn01 \~\]# vim /usr/local/hadoop/etc/hadoop/slaves
node1
node2
node3
### 同步文件
**将配置文件发送给所有hadoop主机,包括自己,或者直接同步整个/usr/local/hadoop
目录**
**以下是在一个ha目录里面配置好然后发送给所有主机,,,,**
\[root@nn01 ansible\]# for i in \`ls /ansible/ha/\`
\> do
\> scp /ansible/ha/\$i 192.168.1.20:/usr/local/hadoop/etc/hadoop/
\> done
### 启动服务
**\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--启动服务\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--**
初始化启动集群
ALL: 所有机器
nodeX： node1 node2 node3
NN1: nn01
NN2: nn02
#\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\-\--#
ALL: 同步配置到所有集群机器 #前面已做好
同步 /etc/hosts 到所有机器 #前面已做好
**NN1: 初始化ZK集群 ./bin/hdfs zkfc -formatZK**
看到如下表示成功:
19/03/30 16:58:47 INFO ha.ActiveStandbyElector: Successfully created
/hadoop-ha/mycluster in ZK.
**nodeX: 启动 journalnode 服务\-\-\-\--node1,node2,node3**
**./sbin/hadoop-daemon.sh start journalnode**
\[root@node1 \~\]# /usr/local/hadoop/sbin/hadoop-daemon.sh start
journalnode
starting journalnode, logging to
/usr/local/hadoop/logs/hadoop-root-journalnode-node1.out
**NN1: 格式化 ./bin/hdfs namenode -format**
\[root@nn01 \~\]# /usr/local/hadoop/bin/hdfs namenode -format
\[root@nn01 \~\]# ls /var/hadoop/
dfs
**NN2: 数据同步到本地 /var/hadoop/dfs**
> **ysync -aXSH nn01:/var/hadoop/dfs /var/hadoop**
\[root@nn02 \~\]# rsync -aXSH nn01:/var/hadoop/dfs /var/hadoop/
\[root@nn02 \~\]# ls /var/hadoop/
dfs
**NN1: 初始化 JNS**
**./bin/hdfs namenode -initializeSharedEdits**
\[root@nn01 \~\]# /usr/local/hadoop/bin/hdfs namenode
-initializeSharedEdits
19/03/30 17:26:07 INFO namenode.FSNamesystem: Finished loading FSImage
in 129 msecs
Re-format filesystem in QJM to \[192.168.1.11:8485, 192.168.1.12:8485,
192.168.1.13:8485\] ? (Y or N) y
19/03/30 17:26:12 INFO namenode.FileJournalManager: Recovering
unfinalized segments in /var/hadoop/dfs/name/current
19/03/30 17:26:12 INFO client.QuorumJournalManager: Starting recovery