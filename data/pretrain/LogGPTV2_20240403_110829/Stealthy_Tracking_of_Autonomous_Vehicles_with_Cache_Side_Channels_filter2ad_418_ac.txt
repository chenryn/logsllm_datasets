536,013
150,890
2-fold
514,656
543,716
5-fold
510,006
547,580
Table 1: Comparison of the average DTW distance between
RUSBoost and SVM.
particles. Considering that one false prediction point incurs a
distance of 16,000−500=15,500, the DTW distance implies
539,407 ÷ 15,500 ≈ 35 false prediction points in a single
trace containing about 1,000 data points.
We compare SVM and RUSBoost prediction results in
Table 1. In the table, we list average DTW distance of training,
2-fold, and 5-fold validation2. We use 100 traces for each
experiment. The results show that even though SVM has low
training DTW distance, RUSBoost has lower 2-fold and 5-
fold validation distance, indicating RUSBoost model performs
better and overﬁts less for this modeling task.
4.5 Route Predictor
classes
Given
particle
(N1,N2,N3, ...,Nt , ...,Ntend ) we need a model
that pre-
dicts the route or the location of the vehicle. There are two
related tasks:
sequence
the
of
a
1. Route prediction: Given a set of known routes, ﬁnd the
route that a vehicle takes.
2. Location prediction: Given the starting location of a
vehicle and a set of possible ﬁnal locations on a known
map, determine the ﬁnal location of the vehicle.
The task of predicting the ﬁnal location can be consid-
ered a speciﬁc form of route prediction, in which the set of
known routes contains all routes on the map that connect the
starting location and possible ﬁnal locations. In that sense,
2 For evaluating a machine-learning model on a dataset, N-fold validation
divides the dataset into N sets. For each test, it uses all but one set to train
the model while holding out the one set for validation.
Figure 7: kNN classiﬁca-
tion results.
Figure 8: RF-50 classiﬁ-
cation results.
both the route prediction and location prediction tasks can be
formulated in a uniﬁed way.
Different routes may not necessarily have the same length
tend, and for the same route, tend may vary based on the speed
of the vehicle. To handle the variations in the trace length,
we pad each sequence N = (N1,N2,N3, ...,Nt , ...,Ntend ) into
a sequence (N1,N2,N3, ...,Nt , ...,Ntend , ...,Nttmax ) with length
tmax by assigning a new element P ∈ {L,P,H} (for padding)
to all Nt for tend < t ≤ tmax. After that, we can formulate the
prediction as a standard classiﬁcation problem:
• Given: M tuples (Ni,li) in which 1 ≤ i ≤ M and Ni ∈
{L,P,H}tmax is a vector of maximum length tmax and
li ∈ {l1,l2, ...,ln} is the label representing a route or a
location.
• Find:
g : {L,P,H}tmax
(cid:55)→ {l1,l2, ...,ln} such that
∑M
i=1 c(g(Ni),li) is maximized. Here the cost function is
deﬁned as follows:
(cid:40)
c(l1,l2) =
if l1 = l2.
1,
0, otherwise.
(3)
4.5.1 Predicting Route
We can identify a route by comparing the sequence of particle-
number classes (“Low” or “High”) along the route. In this
case, the label li represents a distinctive route i.
We can use a classiﬁcation algorithm, e.g., k-nearest neigh-
bor (kNN) or random forest (RF) [36] to classify different
routes. For example, Figure 7 and Figure 8 show an exam-
ple of classiﬁcation results using kNN and RF with 50 trees
(RF-50) for ﬁve distinct routes in Maze 1 in Figure 14. This
experiment uses a Jackal robot described in Section 5.1. For
each sequence of particle-number classes, we use all other
sequences as the training set and ﬁnd the route label for the
sequence. The overall accuracy is 76% and 96%, respectively.
Given its higher accuracy, we use the random forest (RF) as
the route-prediction model.
4.5.2 Predicting Location
If an attacker knows the initial location of a vehicle, our route
prediction approach can be used to predict the ﬁnal location
USENIX Association
29th USENIX Security Symposium    865
02040608050001000015000ground truthnumber of particles020406080time (s)50001000015000predicted0102030405GroundTruth0102030405Predicted10000531102000002000298880102030405GroundTruth0102030405Predicted00002000000000000000108101010Figure 9: Though the destina-
tion of a run in the validation set
might not appear in the training
set, the intermediate locations
along the path are shared.
Figure 10: Training, validation accuracy, and validation-error distribution of location
prediction for a dataset of 3,633 samples. For this experiment, the measured (ground-truth)
sequence of the particle-number classes is used as an input.
of the vehicle from a particle-number class sequence. In this
case, the label li represents the ﬁnal location. For example, we
can partition a map into Qx × Qy grid cells and assign each
cell (qx,qy), where 1 ≤ qx ≤ Qx and 1 ≤ qy ≤ Qy, a unique
integer label li = (qy − 1)· Qx + qx.
Usually, if an autonomous vehicle starts from a ﬁxed start-
ing location and takes the shortest path to each destination,
the paths will form a shortest-path tree [53] on a given road
network graph. We also use the RF model for this modeling
task because in addition to its general pattern-matching capa-
bility, it also captures the tree structure of the shortest-path
tree.
In practice, the total number of possible destinations (Qx ×
Qy) can be quite large, and collecting sufﬁcient training (and
validation) data from multiple runs to all possible destinations
can be difﬁcult. Instead, in our experiments, we model an at-
tacker who collects data for a subset of possible destinations;
we randomly select a subset of destinations for the training
runs and the validation runs separately, and include interme-
diate locations to create a larger training and validation sets.
For each run with a randomly-chosen destination, the inter-
mediate points along the path as well as the ﬁnal destination
are used as target locations for samples in the training and
validation sets. The runs in the training set and the validation
set do not necessarily share the same destination. The model
will not be able to predict the target locations in validation
samples that never appear in the training samples. However,
as Figure 9 shows, the intermediate positions along paths
with different destinations may overlap, and the model will
be able to correctly predict the samples that use these interme-
diate positions as their target locations even though the ﬁnal
destinations of the runs are different. 3
Figure 10 shows an example of the training and validation
accuracy of an RF-50 model, which predicts a location label
3See Appendix A for a more detailed discussion on how destinations
of simulation runs in the training and validation sets affect the prediction
accuracy.
based on a sequence of the particle-number classes. The maze
is partitioned into a 16-by-16 grid. The experiment is per-
formed using a dataset in which we collected 3,633 samples
based on 100 simulation runs in Maze 1 shown in Figure 14,
where the starting location of the vehicle is in the center of the
maze. We use the samples collected from 80 runs for training
and the remaining 20 runs for validation. For the destinations
of runs in the validation set, only 4 destinations out of the 20
destinations appear in the training set, however, after adding
multiple samples using the intermediate locations also as tar-
get locations, 131 out of the total 135 target locations in the
validation set are covered by the training samples.
We calculate the distance between the predicted location
and the actual location, and show the distribution in Figure 10.
Over 75% of the predictions fall within 3 cells of the ac-
tual target location, indicating the RF model can effectively
capture the relation between locations and sequences of the
particle-number classes.
5 Evaluation
5.1 Evaluation Setup
5.1.1 Evaluation Testbed
We evaluate the attack using two different setups. First, we
use a simulated Jackal robot running in a world created by the
Gazebo simulator for a controlled evaluation environment. We
perform both route and location prediction using the simulated
environment. Second, we use the real-world data collected on
a Nissan LEAF driving around Oxford, UK to evaluate the
attack in a more realistic environment. Because the Oxford
dataset only includes a limited set of routes in the city, we
only evaluate route prediction using the data.
Gazebo: As shown in Figure 11, our testbed hardware has
two computers connected via Ethernet ports. The client has a
dual-core Intel i5-3317u processor, and the host runs a quad-
866    29th USENIX Security Symposium
USENIX Association
Initial LocationDestination inValidation SetDestination inTraining SetShared Intermediate Positions50100150200250Ground Truth Location Label50100150200250Predicted Location LabelTraining Set50100150200250Ground Truth Location Label50100150200250Predicted Location LabelValidation SetValidation error distribution05101520Prediction error (grids)00.10.20.30.40.50.600.10.20.30.40.50.6Model Prediction ErrorRandom Prediction ErrorFigure 11: The testbed setup for the evaluation.
Figure 12: 3D physics-based simulation in Gazebo.
core Intel i5-3470 processor with 8GB of memory and Nvidia
GT710 for graphic rendering. Both of them run Ubuntu 18.04
[16] and support ROS Melodic [8] for interaction with the
physical world.
To create a simulated world, we use Gazebo [3], a ROS-
compatible physics-based simulator. Figure 12 shows exam-
ples of a simulated vehicle and a maze in Gazebo. To efﬁ-
ciently create complex mazes for our experiments, we use an
open-source Gazebo plugin [7] that generates maze models
such as the one in Figure 12(b) based on a text description.
We run the entire software stack (including Ubuntu, ROS,
AMCL and other control software) of a Clearpath Jackal Un-
manned Ground Vehicle (UGV) [5] on the client. The Jackal
UGV, shown in Figure 12(a), is a conﬁgurable and extensi-
ble platform commonly used for autonomous vehicle studies.
In the simulations, we attach SICK [12] LMS1xx series Li-
DAR to the Jackal UGV as the sensor for 2D localization. We
use the ROS implementation of AMCL [1] for LiDAR-based
localization.
Oxford: For the real-world experiment, we use the Oxford
RobotCar dataset [46], which is collected on a Nissan LEAF
along a 10 km route around central Oxford, UK, from May
2014 to December 2015. We converted all the data to rosbag
[11] format in order to replay it in the lab environment, and
we run AMCL on a platform with an Intel Xeon E3-1270
four-core processor with 16GB memory, which is similar to
the conﬁguration used by the Apollo autonomous driving
platform [2, 9].
For each trace in the dataset, the LiDAR scan data is pro-
vided by SICK LD-MRS LiDAR attached in front of the ve-
hicle. Odometry information is recorded by a NovaTel SPAN-
CPT GNSS/INS receiver [13]. The original RobotCar dataset
uses CSV ﬁles and we preprocess them by converting the
LiDAR and odometry data as well as the corresponding times-
tamps into a single rosbag ﬁle for evaluation. To provide a
reference map for AMCL, we use the 3-D pointcloud recorded
by the SICK LMS-151 LiDAR on the vehicle. We project
all the points in the pointcloud of heights between 0.5m-2m
(that can be captured by LD-MRS LiDAR) onto a 2-D plane,
which forms the 2-D map used for AMCL.
The RobotCar dataset contains multiple traces along one
route. We divide the route into seven segments, and perform
route prediction using the seven segments as different routes.
5.1.2 Prime+Probe Attack Conﬁgurations
We describe the implementation details of the prime+probe
attack on the client computer. The cache conﬁgurations of
the processors used are listed in Table 2. We perform attacks
using the L1D cache and the LLC for both platforms. The
L1D attack explores an idealized scenario while the LLC
attack explores less restrictive and more realistic scenario. We
adopt higher sampling rate, smaller steps, and assign attack
and victim processes as real-time processes in the L1D attack.
Platform
CPU
Gazebo
Oxford
i5-3317u
E3-1270
L1D
LLC
Sets
64
64
Size
Sets
32K 4096
32K 8192
Size
3M
8M
Table 2: Processor cache conﬁgurations used in experiments.
L1D attack: We assign the attack and victim processes on
the same core by assigning them the same CPU afﬁnity value.
We set both attack and victim processes as real-time processes
with the victim process at higher priority. In Linux, a real-time
process cannot be preempted by a userspace non-real-time
process. Thus, the L1D state left by the victim process will not
be destroyed before probing. In addition, the higher priority
of the victim process guarantees that the victim process will
not be preempted by the attack process unless it yields control.
For the L1D attack, we probe every set in the cache, and the
entire cache is probed every 100 ms.
LLC attack: The attack and victim processes may run on
different cores for the LLC attack. We use the MASTIK tookit
[6], which implements the algorithm in [45] that ﬁnds the
eviction sets on a physically-addressed LLC, to perform the
prime+probe attack. We probe only one cache set for each
consecutive 64 cache sets, which reduces the CPU utilization
of the attacker and the amount of data generated. The entire
cache is probed every 300 ms instead of 100 ms. Despite the
reduced cache probing rate, our results show that it is still
possible to predict the number of particles with high accuracy.
5.1.3 Training Procedure
Here, we describe the procedure that we use to train the
particle predictor and the route predictor in our evaluation.
Given the measured cache timing, the particle-number class
sequences (i.e., sequence of “High” and “Low” classes), and
USENIX Association
29th USENIX Security Symposium    867
ClientIntel processorUbuntu 18.04ROS melodicROS AMCLAttack ProcessIntel processorUbuntu 18.04Gazebo/RosbagGoal senderROS Navi-gationHostROS melodicROS masterEthernet(a) A Jackal UGV;(b) amaze.  Figure 13: Procedures for training the two models.
Figure 16: Map for the Oxford RobotCar dataset.
Model
RF-1
RF-10
RF-20
RF-50
RF-100
RF-200
5-fold
2-fold
Train
10-fold