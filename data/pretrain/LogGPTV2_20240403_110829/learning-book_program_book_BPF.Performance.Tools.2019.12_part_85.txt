Memory
Show possible memory leak code paths
s1abratetop
BCC/book
Slab
Show kermel slab allcation rates by cache
Book
NUMA
Show NUMA page migration statistics
wozkq
Book
Work queues
Show work queue function execution times
For the tools from BCC and bpftrace, see their repositories for full and updated lists of tool
options and capabilities.
See the previous chapters for more tools for kernel analysis, incluxding for system calls, network
ing, and block I/O.
The following tool sur
imaries include a discussion on instrumenting spin locks and tasklets
---
## Page 683
646
Chapter 14 Kernel
14.4.1loads
loads(8) is a bpftrace tool to print the system load averages every second:
+ loads.bt
Attaching 2 pzobes...
Reading losd averages... Hit Ctrl-C to end.
18:49:16 load averages: 1.983 1.151 0.931
18:49:17 load averages: 1.824 1.132 0.926
18:49:18 load averages: 1.824 1.132 0.926
[...]
As discussed in Chapter 6, these load averages are not very useful, and you should quickly move
on to deeper metrics. The loads(8) tool may be more useful as an example of fetching and printing
a kernel variable, in this case avenrun:
1/usz/local/bin/bpftrace
BEGIN
printf(*Reading load averages.., Hit CtrlC to end. .n*) ;
interval:s:1
/ *
* See fs/proc/loadavg.c and include/linux/sched/loadavg.h for the
+ folloving calculations.
* /
$avenrun = lkaddr I*avenxun*) :
$1osd1 - *$avenrun,
$load5 = *($avenrun + 8)
$load15 = *($avenrun + 16) 
time (*%8:$M:S *)
printf (*load averages: ed, e03d d.$03d ed.303d\n*,
{$1oad1 >> 11)。[($1oad1 & ((1 > 11,
{$1osd5 >> 11),(($load5 & (11 > 11,
I > I)） speos))*(tI  /nnt./backup-tar,gz
From this one-liner, it may be obvious that gzip(1) spends much of its time waiting for data from
tar(1). tar(1) in turn spends much of its time waiting for data from disk, which can be shown by
(g)augndo
S (xea u- dexbd)s dg- wyandoggo t
Tracing offCPU tine lus] of PID 5570 by kernel stack for 5 secs.
[.--]
f1nish_task_svltch
_schedule
schedule
4 0rigin: I develped wakeuptime trecing using OTrce and visusized it with flme graphs on 7-No2013. This started
out as part of a 45-minute talk on flame graphs that Ihad to develop at the last minute for the USEND USA conference
[Gregg 1.3al when the talk I had originally planned suddenly became impossible to do. Then I wss ashed to also fil in
for another spesker who had become il, so I ended up giving this as part two of a 90-minute plenary on flame graphs.
I crested the BCC tool on 14-Jar-2016.
---
## Page 687
650
0 Chapter 14 Kernel
io_schedule
t"pex"tt"otxeue6
xfs_file_buffered_aio_read
xfs_file_read_lter
_vfs_read
vfs_read
ksys_read
do_syaca11_64
entry_SYsCALl_64_after_hvframe
(OLS5) 3e)
4204994
This stack shows tar(1) blocked on io_schedule(: block device I/O. Given the output of both
offcputime(8) and wakeuptime(8), you can see why an application was blocked (offcputime(8)
output) and then the reason the application was woken up (wakeuptime(8) output). Sometimes
the wakeup reason better identifies the source of isues than the blocked reason.
To keep these examples short, I'm using p to match on a PID. You can trace system-wide instead
by not specifying -p.
This tool works by tracing scheduler functions schedule( and try_to_wake_up(0). These can be
very frequent on busy systems, so the overhead may be significant.
Command line usage:
wakeuptine[options][duration]
Options include:
 f: Output in folded format, for generating a wakeup time flame graph
quo ssaoord sL :aIa d-
As with offcputime(8), if it is run without p, it will trace system-wide—and likely produce
hunclreds of pages of output. A flame graph will help you navigate this output quickly.
14.4.4 offwaketime
offwaketime(8)a is a BCC tool that combines offcputime(8) and wakeuptime(8). Continuing the
previous example:
+offwaketime -Kp $(pgrep -n gzip)5
s08s s xog xoe1s xexex pue nd-ro touaex q [8n] 8uta peo0ta butoex
5 0rigin: I developed this initialy as chain graphs for the USENIX LISA 2013 conference on 7-Nov-2013 [Gregg 1.3a]
where Ilked multiple weups nd shwed the utpt  a fme grph. That version used DTrsce, snd since DTrace
can’t save and retrieve stacks I needed to dump all events and post-process, which was too expensive for real produc
tion use. BPF allows saving and retrieving stack traces (which I used when cresting this BCC tool on 13-Jan-2016)
as well as limiting it to one wskeup level. Alexei Starovoitov added a version to the kernel source, under
samples/bpf/ofwsketime_*.c.
---
## Page 688
14.4 BPF Tools 651
[. . - ]
xaker:
2S8S 394
en tzy_SYsCALL_64_after_hvframe
9"Tes.a"op
ksys_vrite
vfs_vrite
_vfs_xr1te
pipe_vrite
0uouuodnexen
uouuoo"dnayex
autoxemove_uake_function
f1nish_task_svltch
_schedule
schedule
pipe_wsit
plpe_read
peaa"sa"
vfs_read
kays_read
do_aysca11_64
entry_SYsCALL_64_after_hvframe
target:
Ises d1z6
4490207
og spuooas 6p 1og sqed sq u paqpopq sem uorum °(1)drz8 dn Supem (1)rep smougs ndno sL
stack traces are shown, delimited by **, and the top waker stack has been inverted. This way,
the stack traces meet in the middle at the point where the waker stack (top) is waking the blocked
stack (bottom).
This tool works by tracing scheduler functions schedule( and try_to_wake_up0, and saves the
waker stack trace in a BPF stack map for later lookup by the blocked thread so that they can be
summarized together in kernel context. These can be very frequent on busy systems, so the over-
head may be significant.
Command line usage:
offvaketime[options][durat.ion]
Options include:
• f: Output in folded format, for generating an off-wake time flame graph
Aquo ssaoord sL :aIa d- 
•K: Only kernel stack traces
U: Only user-level stack traces
---
## Page 689
652
Chapter 14 Kernel
Without p, it willtrace system-wide, likely producing hundreds of pages of output. The use of
options such as =p, K, and 10 will help reduce overhead.
Off-Wake Time Flame Graphs
The folded output (using f) can be visualized as a flame graph using the same orientation: waker
stack on the top, inverted, and blocked stack on the bottom. An example is shown in Figure 14-4.
Wakertask
bRk_mq_end_request
equest
wax.
oo_endio
Wakerstack
wake_
wake_up.
wokeup
schedue_tmeout
btwatio
ched_teat_start
wait_on_bit_lock
oek_pegs_killbble
Off-CPUstack
xt4_hmt
ds_reao
eric_file_read_ner
vs_read
sysge..sys_read
Blocked task
Figure 14-4  Off-wake time flame graph
14.4.5
mlock and mheld
The mlock(8)? and mheld(8) tools trace the kernel mutex lock latency and held times as
histograms, with kernel-level stacks. mlock(8) can be used to identify issues of lock contention,
and then mheld(8) can show the cause: which code path is responsible for hogging the lock.
Starting with mlock(8):
 nlock.bt
Attach.ing 6 probes...
Tracing nutex_lock () latency, Cte1-C to end.
6 Origin: I created these tools forthis book on 14-Mar-2019. This approach is inspired by the Solaris lockstat(1M) tool
by Jeff Bonwick, which also showed partial stacks with latency histograms for lock and held times.
---
## Page 690
14.4 BPF Tools
653
[..-]
elock_latency_ns [0xEfff9d015738c6e0,
kretprobe_tranpoline+0
[e+6suaosaueexssxtun
sock_recvmsg+67
_sys_recvmsg+245
_sys_recvmeg+81
 chrone] :
[512, 1K]
5859 1889889889888 888988988988988988988988
[1K, 2K]
8303 eeeeeeee8e e8ee8ee8ee8eeeeee8eeee88ee8ee8ee8eeeeeee 
[2K, 4K)
1689 18898898898
[4K, 8K)
476188
[8K, 16K)
1011
The output included many stack traces and locks, only one of which has been included here. It
shows the address of the lock (0xfff9d015738c6e0), the stack trace to mutex_lock0), the process
name (°chrome*), and the latency of mutex_lock(). This lock was acquired thousands of times
while tracing, although it was usually fast: for example, the histogram shows that 8303 times it
took between 1024 and 2048 nanoseconds (roughly one to two microseconds).
Now running mheld(8):
 nheld.bt
Attach.ing 9 probes.. 
Tracing mutex_lock () held tines, Cte1-C to end.
•C
[.--]
held_tine_ns [0xfffr9d015738c6e0,
mutex_unlock+1
un.ix_strean_recvnsg+81
sock_recvmsg+67
ays_recvnsg+245
T+BsuAoa2"ss
r chzone] :
[512, 1K)
1659 8 e88 88 88 88e8e888888 8e88 1
[18, 2x]
This shows that the same process and stack trace was the holder for this lock.
These tools work by tracing the mutex_lock(), mutex_lock_interruptible0), and mutex_trylock()
kernel functions because mutex tracepoints do not yet exist. Since these can be frequent, the over-
head while tracing may become significant for busy workloads.
---
## Page 691
654
Chapter 14 Kernel
mlock
The source to mlock(8) is:
1/usr/local/bin/bpftrace
BEGIN
printf (*Tracing mutex_lock () latency, Cte1-C to end.\n*)
kprobe:mutex_lock,
kprobe:mutex_lock_interruptible
/$1 == 0 11 pid == 51/
1
8leck_start.[tld] - nsecs,
rxe -[p]xppexootB
kzetprobe:mutex_lock
/($] == 0 11 pid == $1) &s elock_start[tid]/
8lock_latency_ns[ksym(8lock_addr[tid]), kstack (5)。 conm] =
hist [nsecs = @loek_start[tid]}:
delete (@lock_start [tid]) 
delete (elock_addr [tid]) 
kretprobe:mutex_lock_Lnterruptible
/retva] == 0 66 ($1 == 0 11 pid == $1] 6s Block_start [tid]/
8lock_latency_ns[ksym (8lock_addr[tid]) , kstack (5)。 conn] =
[p]xexoe) aaep
delete (e1ock_addr|t1d]) ;
END
1
clear (8lock_start)
clear (81ock_add1
---
## Page 692
14.4 BPF Tools
655
This times the duration of mutex_lock(), and also mutex_lock_interruptible() only if it returned
successfully. mutex_trylock( is not traced, as it is assumed to have no latency. An optional argument