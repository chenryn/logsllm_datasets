mation, and if that became law then any researcher using
leaked classiﬁed data could be imprisoned, possibly for up
to 36 years [56]. In the USA court cases have been thrown
out because the evidence that the government had the sup-
plicants under surveillance was classiﬁed (even though the
supplicants had that evidence) [75]. In contrast, the Vault 9
leak of CIA data [8:] contained data that might be expected
to be highly compartmentalised Top Secret (source code for
weaponised zero day exploits), but, due to the fact that if it
were classiﬁed it could not be deployed on enemy systems,
it was unclassiﬁed. Additionally, as the USA government
cannot own copyrights, the source code for this state level
malware was not even protected by copyright [7]. The lack
of legal protections might make it easier for researchers to
work with this data.
7 ANALYSIS
In this section we analyse the case studies with respect to
the ethical and legal considerations described in Sections 4.3
and 5. We ﬁrst discuss common justiﬁcations made by au-
thors to justify the use (or not) of these data. The main goal
is to understand how the authors approached the legal and
ethical issues as well as the justiﬁcations, safeguards, harms,
and beneﬁts. A summary of this analysis is shown in Table 3.
The acronyms used for safeguards, harms and beneﬁts in the
table are given in brackets below.
7.3 Justiﬁcations
We have observed common justiﬁcations made by the re-
searchers regarding ethical issues in the case studies, which
are summarized in Table 3. Here, we describe them and
provide comments in italics.
Not the ﬁrst Previous research using these data was pub-
lished and peer-reviewed, and so our work must be ethical.
This is a poor argument: not all published work is ethical un-
der current norms, and while the work that was published may
have been ethical, if your work does something diﬀerent with
these data then that requires its own justiﬁcation. Public
data Since these data are publicly available, anything we do
with these data is ethical. The ethics of the work must still
be considered and in some cases REB review may still be re-
quired [54]. Researchers may develop or apply new techniques
to public data that, for example, deanonymise these data,
and this may cause harm [6]. No additional harm Any
harms that might arise have already occurred and therefore
our work produces beneﬁts and no (or negligible) additional
harm it is ethical. For there to be no additional harms the
research should not identify any natural persons and data
may need to be stored and managed securely. In some cases
any use of the data of illicit origin is considered additional
harm, for example with images of child abuse, every viewing is
considered additional abuse of the victim. Fight malicious
use These data are already used by malicious actors, and so
we also need to use these data to defend against them. If
researchers can use the same data to prevent or reduce harm
caused by malicious actors, without creating greater harm by
doing so, then it may be ethical to do so. Necessary data
This research cannot be conducted without using this data.
This might be a good justiﬁcation if there is suﬃcient beneﬁt
to the work (Public interest) and there is no additional harm.
7.4 Safeguards
When dealing with leaked data, holders must take care as to
how it is processed and stored so as to avoid further disclosure
of sensitive information. Here we analyse the actions taken
by the researchers to maintain the conﬁdentiality, integrity
and privacy of these data and the stakeholders. Secure
storage (SS) to protect the integrity and conﬁdentiality
of these data maintained, e.g. by means of encryption and
Ethical issues in research using datasets of illicit origin
IMC ’39, November 3–5, 4239, London, UK
access control to avoid accidental leakage. Privacy (P) No
deanonymisation is attempted and no identities are revealed.
Controlled Sharing (CS) Researchers publish only partial
/ anonymised data, provide it under legal agreements that
prevent harms or do not make these data publicly available.
This includes approaches such as letting researchers visit the
institution holding the data to analyse it, or the holding
institution performing analysis on behalf of other researchers,
such as by running their code.
7.5 Harms
Conducting research using data containing sensitive informa-
tion entails risks dependant on the consequences of the data
gathering, the research itself, or any further leakage of these
data maintained by the researchers. In general only harms
to natural persons (rather than corporate persons), or to the
environment, are considered during ethical review. Illicit
measurement (I) Research obtained these data by means
of illicit activities such as hacking or paying the oﬀenders,
which can lead to researchers being prosecuted. Potential
Abuse (PA) Research results from these data can be used
by malicious actors to cause additional harm, for example by
means of designing evasive malware or updating password
cracking policies. De-Anonymization (DA) Research on
these data can be used to de-anonymise or re-identify people
or networks. Also, identiﬁcation of group of individuals may
raise ethical concerns such as discrimination or violence to-
wards identiﬁed groups [57]. Sensitive Information (SI)
These data contains sensitive and private information, which
can be used to harm natural persons. For example, if the
user password from one service is leaked, their credentials to
other services can be compromised due to password reuse [46].
Researcher Harm (RH) The research can lead to the re-
searchers being prosecuted by law enforcement, since these
data may include illegal material. Researchers could be
threatened by criminals, e.g.
in underground forums [94],
or by state or industry actors that dislike the work. There
may also be a risk of emotional trauma to researchers if
they come across distressing content, such as pornography
or violence, during the work. Behavioural Change (BC)
The research can change the behaviour of the stakeholders
of these data, which may have negative consequences. For
example, a market vendor can provide fake information if
she knows that she is being measured [323]. Alternatively
the research may encourage future collection or use of data
of illicit origin.
7.6 Beneﬁts
In this section, we enumerate academic and social beneﬁts
particular to research using data of illicit origin. Repro-
ducibility (R) The data allows the comparison of diﬀerent
algorithms or tools. This is the great beneﬁt of data sharing,
but Controlled Sharing is required when these data con-
tains sensitive information. Uniqueness (U) Data is either
unique (cannot be obtained through other means) or histori-
cal (can no longer be obtained by other means), so similar
measurements on the same topic are hard or even impossible
to attain. This only becomes a beneﬁt if the data is also
useful. Defence Mechanisms (DM) Data can be used to
study the underground economy, new forms of cybercrime
or new attack techniques. This allows new defences to be
designed, such as anti-malware tools or eﬃcient password
policies. Anthropology and Transparency (AT) Data
contains ground truth on the behaviour of human beings,
which other methods could only obtain in a ﬁltered or biased
way. For example, data can reveal real human behaviour
when creating passwords without the reporting bias of sur-
veys or experiments with human participants. Additionally,
data can provide transparency through information that aids
understanding of government surveillance activities, exter-
nal relationships, or of company behaviour. The additional
transparency into state or corporate actors may have greater
beneﬁts than if the data concerned individuals, as it may have
additional public beneﬁt by providing checks and balances
on power.
7.7 Discussion
We observe a wide variation in the ethical issues mentioned
by the authors and their justiﬁcations for using these data,
even when they are using the same data. This is clear from
studying Table 3. Two works stated that they were exempt
from REB approval, two received REB approval and 46 did
not mention REBs. The reasons given for exemption were:
no human subjects or ethical concerns [332]; no personally
identiﬁable information (despite email addresses and IPs)
and public data [77]. Both of these works used Safeguards to
mitigate potential Harms and have clear ethical justiﬁcations.
These exemptions are all based on the absence of direct
human subjects. However, in each case they were measuring
human behaviour and if they had tried to identify individuals
they might have been successful. The absence of human
subjects appears an artiﬁcial distinction in these cases, as
there were human participants. Both of the papers that
received REB approval [79, 46] obtained it not because of
their usage of data of illicit origin, but because they also
conducted surveys or other human subject research. Not
having REB approval solely on the basis that data is public
is contrary to the opinions of experts [54], since this data
may contain private information.
Explicit ethics sections were included in 34 of the 4: pa-
pers. However, since we speciﬁcally selected papers for this
table because they talked about ethics, this is unlikely to
be representative. Nonetheless, it does show that a high
proportion of papers using data of illicit origin do already
have ethics sections. We do not have enough information
to show any trend in this behaviour, and because we would
expect this behaviour to be ﬁeld dependent, we would need
a large representative sample from each ﬁeld to be able to
show any trend.
Discussion of safeguards, harms and beneﬁts in the papers
is highly variable. We included those that were implicitly or
explicitly discussed in the papers. However, we are aware
IMC ’39, November 3–5, 4239, London, UK
Daniel R. Thomas et al.
Legal issues
Ethical issues Justiﬁcations
Sources
e
c
n
e
r
e
f
e
R
X
X
2
4
r
a
e
Y
e
s
u
s
i
m
r
e
t
u
p
m
o
C
t
h
g
i
r
y
p
o
C
y
c
a
v
i
r
p
a
t
a
D
m
s
i
r
o
r
r
e
T
s
e
g
a
m
i
y
t
i
r
u
c
e
s
t
n
e
c
e
d
n
I
l
a
n
o
i
t
a
N
s
m
r
a
h
y
f
i
t
n
e
d
I
s
d
r
a
u
g
e
f
a
S
e
c
i
t
s
u
J
t
s
e
r
e
t
n
i
c
i
l
b
u
P
t
s
r
ﬁ
a
t
a
d
e
h
t
t
o
N
c
i
l
b
u
P
m
r
a
h
l
a
n
o
i
t
i
d
d
a