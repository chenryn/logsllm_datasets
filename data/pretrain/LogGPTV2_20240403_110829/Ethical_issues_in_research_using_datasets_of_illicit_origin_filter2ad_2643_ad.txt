### Legal and Ethical Considerations in Research Using Leaked Data

#### Legal Implications
In the United States, proposed legislation could impose severe penalties on researchers using leaked classified data, with potential imprisonment of up to 36 years [56]. In some court cases, evidence has been dismissed because it was classified, even though the defendants had access to it [75]. Conversely, the Vault 9 leak of CIA data [8] included highly sensitive information, such as source code for weaponized zero-day exploits. This data, though expected to be highly compartmentalized and top secret, was unclassified due to the necessity of its deployment on enemy systems. Additionally, since the U.S. government cannot hold copyrights, the source code for this state-level malware was not protected by copyright [7]. The lack of legal protections can facilitate research on such data.

#### Analysis of Case Studies
In this section, we analyze the case studies in light of the ethical and legal considerations outlined in Sections 4.3 and 5. We first discuss common justifications made by authors for using (or not using) these data. The primary goal is to understand how the authors addressed the legal and ethical issues, as well as the justifications, safeguards, harms, and benefits. A summary of this analysis is provided in Table 3, with acronyms for safeguards, harms, and benefits listed in brackets below.

#### Justifications
We have identified common justifications made by researchers regarding ethical issues in the case studies, summarized in Table 3. Below, we describe these justifications and provide comments in italics.

- **Not the First**: Previous research using these data was published and peer-reviewed, so our work must be ethical.
  - *Comment*: This argument is weak because not all published work adheres to current ethical norms. Even if previous work was ethical, new research using the same data may require its own justification.
  
- **Public Data**: Since these data are publicly available, anything we do with them is ethical.
  - *Comment*: The ethics of the work must still be considered, and in some cases, REB review may still be required [54]. Researchers might develop or apply new techniques that deanonymize the data, potentially causing harm [6].
  
- **No Additional Harm**: Any harms that might arise have already occurred, so our work produces benefits without additional harm.
  - *Comment*: For there to be no additional harm, the research should not identify natural persons, and data may need to be stored and managed securely. In some cases, any use of data of illicit origin is considered additional harm, such as with images of child abuse, where every viewing is considered additional abuse of the victim.
  
- **Fight Malicious Use**: These data are already used by malicious actors, so we need to use them to defend against them.
  - *Comment*: If researchers can use the same data to prevent or reduce harm caused by malicious actors without creating greater harm, it may be ethical to do so.
  
- **Necessary Data**: This research cannot be conducted without using this data.
  - *Comment*: This can be a valid justification if there is sufficient benefit to the work (public interest) and no additional harm.

#### Safeguards
When handling leaked data, researchers must take care to protect the confidentiality, integrity, and privacy of the data and stakeholders. Here, we analyze the actions taken by researchers to maintain these aspects:

- **Secure Storage (SS)**: Protecting the integrity and confidentiality of the data through encryption and access control to prevent accidental leakage.
- **Privacy (P)**: Ensuring no attempts at deanonymization and no revelation of identities.
- **Controlled Sharing (CS)**: Publishing only partial or anonymized data, providing it under legal agreements that prevent harm, or not making the data publicly available. This includes allowing researchers to visit the institution holding the data or having the institution perform analysis on behalf of other researchers.

#### Harms
Research using data containing sensitive information entails various risks, including:

- **Illicit Measurement (I)**: Obtaining data through illicit means such as hacking or paying offenders, which can lead to prosecution.
- **Potential Abuse (PA)**: Research results can be used by malicious actors to cause additional harm, such as designing evasive malware or updating password cracking policies.
- **De-Anonymization (DA)**: Research can de-anonymize or re-identify people or networks, raising ethical concerns like discrimination or violence towards identified groups [57].
- **Sensitive Information (SI)**: Data containing sensitive and private information can be used to harm individuals, such as compromising credentials due to password reuse [46].
- **Researcher Harm (RH)**: Researchers may face legal prosecution or threats from criminals, state, or industry actors. There may also be a risk of emotional trauma from encountering distressing content.
- **Behavioral Change (BC)**: Research can change the behavior of data stakeholders, leading to negative consequences, such as market vendors providing fake information if they know they are being measured [323].

#### Benefits
Research using data of illicit origin can offer several academic and social benefits:

- **Reproducibility (R)**: Allowing the comparison of different algorithms or tools, with controlled sharing required for sensitive data.
- **Uniqueness (U)**: Data that is either unique or historical, making similar measurements difficult or impossible to attain.
- **Defense Mechanisms (DM)**: Studying the underground economy, new forms of cybercrime, or attack techniques to design new defenses.
- **Anthropology and Transparency (AT)**: Providing ground truth on human behavior and transparency into government surveillance, external relationships, or company behavior, offering public benefits through checks and balances on power.

#### Discussion
There is significant variation in the ethical issues mentioned by authors and their justifications for using these data, even when using the same data. This is evident from Table 3. Two works claimed exemption from REB approval, two received REB approval, and 46 did not mention REBs. Exemptions were based on the absence of direct human subjects, but in each case, they were measuring human behavior. The absence of human subjects appears to be an artificial distinction, as there were human participants. Both papers that received REB approval [79, 46] did so not because of the use of illicit data but because they also conducted surveys or other human subject research. Relying solely on the public nature of data for exemption is contrary to expert opinions [54], as this data may contain private information.

Explicit ethics sections were included in 34 out of 40 papers. While this is not representative, it shows that a high proportion of papers using data of illicit origin already include ethics sections. Discussion of safeguards, harms, and benefits in the papers is highly variable, and we included those that were implicitly or explicitly discussed. However, more comprehensive and field-specific studies are needed to show any trends in this behavior.