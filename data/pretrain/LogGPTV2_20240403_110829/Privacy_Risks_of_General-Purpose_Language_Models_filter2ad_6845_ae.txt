knowledge level of the adversary in the white-box setting, in
{10, 100, . . . , 1000} and conduct the MLP attack on Medical.
The results are provided in Fig. 6(c). As we can see, the
attack accuracy remains over 90% for each language model
when the shadow corpus size is larger than 100. Moreover,
we interestingly observe, a larger language model (GPT-2-
Large) is less robust than a smaller one when the adversary’s
knowledge is limited. When the shadow corpus size is only
10, the attack accuracy is 68.5%, 61.0% and 59.4% against
GPT-2-Large, GPT-2-Medium and GPT-2 respectively. This
observation is also consistent with the conclusion in [34]:
complicated models tend to enlarge the attack surface. (2)
Similarly, for the black-box setting, we compare the DANN
attack’s performance by varying the size of the external corpus
from 100% to 5% of the original size 2000, with results
reported in Fig. 6(d). As we can see, a larger external corpus
helps our attack achieve higher accuracy, which demonstrates
the effectiveness of our proposed adversarial knowledge trans-
fer procedure.
Also, we study the robustness of DANN attack w.r.t. its
hyperparameters. We respectively control the size of the victim
embeddings, which corresponds to the knowledge level of
the adversary in the black-box setting, and the dimension of
the domain-invariant representation in DANN, which is the
only architectural factor of DANN, and conduct the DANN
attack on Medical. For these two settings, we report
the
attack accuracy respectively in Fig. 6(g) & (h). As is shown,
the accuracy of DANN attack remains high with different
hyperparameter choices, which highly reﬂects the robustness
and effectiveness of our proposed attack model.
VII. POSSIBLE DEFENSES
As the sentence embedding is the direct source of potential
information leakage, a general principle for mitigation is to
obfuscate the embeddings. For this purpose, we empirically
evaluate four possible technical choices, where the ﬁrst three
are general against both attacks and the last one is specially
designed for keyword inference attack. Although the ideal situ-
ation is that the sensitive information can be totally eliminated
while the required information for other normal tasks can be
highly preserved, in practice such a utility-privacy trade-off
seems unavoidable, at least according to our reported trade-
off results below. Here, the utility denotes the classiﬁcation
accuracy of the underlying benchmark systems. We hope our
preliminary study will foster future mitigation studies. The
omitted technical details and experimental setups can be found
in Appendix C.
(1) Rounding. For the ﬁrst defense, we apply ﬂoating-point
rounding on each coordinate of the sentence embeddings for
obfuscation. Formally, we write the rounding defense as ˆz =
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1324
Fig. 7. The utility and attack accuracy curves along on Genome and Medical when four possible defenses with different parameters are applied for mitigation.
For DP & PPM defenses, the x-axes of utility and attack accuracy curves are in log scale.
rounding(z, r), where the non-negative integer r denotes the
number of decimals preserved after rounding.
(2) Laplace Mechanism. For the second defense, we leverage
a differential privacy approach, the Laplace mechanism [20].
Roughly speaking, we perturb the embedding coordinate-wise
with samples from a Laplace distribution whose parameters
are determined by the (cid:2)1-sensitivity of the language model f
(denoted as Δf, which we estimate with numeric algorithms).
Formally, the defense works as ˆz = z +(Y1, . . . , Yd), where Yi
are i.i.d. random samples drawn from Lap(Δf /), the Laplace
distribution with location 0 and scale Δf /.
(3) Privacy Preserving Mapping. The third defense is
based on adversarial training. We borrow the notion of a
Privacy Preserving Mapping (PPM) from [57] to denote a
: Rd → Rd parameterized by θ, which is
mapping Dθ
trained to minimize the effectiveness of an imagined adversary
Aψ. Meanwhile, the PPM is required to follow the utility
constraint by distorting the embeddings only in a limited radius
around the original embedding, which is implemented as a
regularization term. Formally, we propose to learn the privacy
preserving mapping Dθ by solving the following minimax
i=1 Aψ(Dθ(zi), si) + λ(cid:10)Dθ(zi) − zi(cid:10)2
game minθ maxψ
where λ controls the privacy level, the higher the lower privacy
level.
(4) Subspace Projection. The last defense is especially
designed as a countermeasure to keyword inference attacks,
inspired by [14] on debiasing word embeddings from gender
bias. The general
idea of this defense is to project out
the unwanted subspace (i.e., privacy subspace) that encodes
the occurrence of the keyword from the universal sentence
embedding space. Technical details on how to identify the
privacy subspace and how to do the projection can be found
in Appendix C. In our evaluations, we consider the ratio β
between the dimension of the privacy subspace and that of the
universal embedding space as the parameter of this defense.
Intuitively, a higher β is expected to bring a stricter privacy
mechanism.
Evaluations. We evaluate the ﬁrst three defenses against the
(cid:2)
n
1
n
pattern reconstruction attack on Genome and all four defenses
against the DANN-based keyword inference attack on Medical
with a wide range of settings. The conﬁgurations and results
of the ﬁrst three defenses are presented in Fig. 7.
As we can see from Fig. 7, although each defense can
attenuate the attacker to a total random guesser under certain
privacy budgets, they simultaneously compromise the utility of
downstream tasks by causing an unacceptable degradation. For
example, the Laplace mechanism degrades the utility to a total
random guesser as well when achieving the optimal defense
performance. For PPM, despite a slighter trade-off is observed,
the utility for RoBERTa and Transformer-XL still degrades
from over 90% to around 25% when the optimal defense
is achieved. Among these defenses, we notice the subspace
projection defense could provide a more desirable defense
quality than the three possible defenses. For example, for
most of the target language models, the defense can degrade
the DANN attack to a random guesser by projecting out
only the 1% keyword-related subspace. However, the utility
of the embeddings on the downstream task still decreases by
about 15% compared with the 95% accuracy with unprotected
embeddings, which implies the keywords that we want to hide
are also critical to provide essential semantics for downstream
tasks. Moreover, the quality of subspace defense in practice
would be less desirable due to its blindness to the target
keyword that the adversary is interested in.
According to our preliminary results above, how to balance
the elimination of token-level sensitive information from the
embeddings and the preservation of the essential information
for normal tasks is still an open problem that awaits more
in-depth researches. In consideration of the practical threats
imposed by our attacks on the applications of general-purpose
language models, we highly suggest the exploration of effec-
tive defense mechanisms as a future work.
VIII. DISCUSSIONS
On Threat Model. For Assumption 0, the adversary can
get the sentence embeddings of victims if general-purpose
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1325
language models are deployed in collaborative or federated
learning systems, especially when a) the service provider
itself wants to snoop user’s sensitive information or b) the
embeddings are shared accidentally or abusively with some
malicious attackers. In some recent protocols,
the feature
may be encrypted with homomorphic encryption schemes for
privacy protection [24], which therefore requires an adver-
sary to ﬁrst encrypt the embeddings of the external corpus
with the public key and train the attack models on the
encrypted external corpus. This is an interesting scenario that
deserves dedicated research and we leave this as a future
work. Nevertheless, there are also many scenarios which are
not suitable for homeomorphic encryption schemes due to
efﬁciency issues, such as real-time long passage translation
[75] or search engines with language models [6]. In these
scenarios, our attacks remain huge threats.
For Assumption 1, we devise a learning-based ﬁngerprinting
algorithm by ﬁrst determining the candidate model
types
based on the dimension size and then pinpointing the exact
model type with a pretrained classiﬁer. Strikingly, we ﬁnd
the classiﬁcation accuracy can achieve 100%. More technical
details and analysis can be found in Appendix D.
For Assumption 2,
the adversary can easily satisfy the
assumption by deploying the language model on local devices
or accessing the corresponding online services. In the current
work, we adopt this assumption considering the generality of
our attack. Nevertheless, the adversary could further exploit
the speciﬁc language model architecture and pretrained pa-
rameters for better attack effectiveness, which is an interesting
and meaningful direction to pursue in the future work.
Downstream Attacks. As we have mentioned in Sections V &
VI, our proposed attacks can be further used for downstream
attacks that can cause more severe consequences. For example,
if the attacker for some reason gets the embedding of the
treatment description “CT scan of blood vessel of head with
contrast”, he/she can utilize the proposed keyword inference
attack to infer the occurrence probability of each word in a
customized vocabulary (e.g., the vocabulary contains head and
vessel because they are frequent words in medical descrip-
tions), sort the occurrence probability in the decreasing order
(e.g., the two words head and vessel both have occurrence
probability higher than 90%) and thus inverts out the meaning
of the sentence (e.g., the patient may have something abnormal
in the blood vessel of his head). Appendix E provides a demon-
strative experiment that implement the above guideline. We
ﬁnd the adversary can indeed reassemble the basic semantic
meaning of the original text with the above procedure, even
if some words may not be in the customized vocabulary.
Utility vs. Privacy in Deploying Sentence Embeddings.
Our current work indicates the improved utility of sentence
embeddings from general-purpose language models is at odds
with the privacy. In principle, to balance the utility-privacy
trade-off requires the sentence embedding to preserve the
information that is desired for the downstream task and to
discard the remainder, while the following dilemma happens
for general-purpose language models: these models are es-
pecially designed to provide embeddings that can be used
for a wide range of downstream tasks [17], which conse-
quently enforces the embeddings to preserve much token-level
information, which is critical in forming semantics in many
cases and hence leaves the adversary a window for privacy
breach. Based on our systematic evaluations of eight state-of-
the-art language models, we ﬁnd the byte-level tokenization
scheme may indeed provide additional privacy protection by
design. In the meantime, obfuscating sentence embeddings via
adversarial training or subspace projection may be a promising
direction for future studies as they can achieve more desirable
utility-privacy trade-off.
Limitations & Future Works. Although we have observed
some interesting differences in the security property of dif-
ferent
language models, we are still not very clear about
how many other design choices, including the network depth,
learning algorithms and hyper-parameters, inﬂuence the corre-
sponding language model’s privacy level. We are interesting to
investigate these issues in a future work. Moreover, although
we provided preliminary study on four possible defenses, we
ﬁnd none of them could achieve an optimal balance between
privacy and utility on downstream tasks. Also, due to the hard-
ware constraints, we have not evaluated the defense quality of
differentially private training techniques (e.g., DPSGD [7]).
We hope our work will draw more attentions from researchers
to conduct more in-depth study on the privacy properties of
this new NLP paradigm and the corresponding mitigation
approaches.
IX. CONCLUSION
In this paper, we design two novel attack classes,
i.e.,
pattern reconstruction attacks and keyword inference attacks,
to demonstrate the possibility of stealing sensitive information
from the sentence embeddings. We conduct extensive evalu-
ations on eight industry-level language models to empirically
validate the existence of these privacy threats. To shed light
on future mitigation studies, we also provide a preliminary
study on four defense approaches by obfuscating the sentence
embeddings to attenuate the sensitive information. To the best
of our knowledge, our work presents the ﬁrst systematic study
on the privacy risks of general-purpose language models, along
with the possible countermeasures. For the future leverage
of the cutting-edge NLP techniques in real world settings,
we hope our study can arouse more research interests and
efforts on the security and privacy of general-purpose language
models.
ACKNOWLEDGEMENT
We sincerely appreciate the shepherding from Piotr
Mardziel. We would also like to thank the anonymous re-
viewers for their constructive comments and input to improve
our paper. This work was supported in part by the National
Natural Science Foundation of China (61972099, U1636204,
U1836213, U1836210, U1736208, 61772466, U1936215, and
U1836202), the National Key Research and Development Pro-
gram of China (2018YFB0804102), the Natural Science Foun-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 10:19:28 UTC from IEEE Xplore.  Restrictions apply. 
1326
dation of Shanghai (19ZR1404800), the Zhejiang Provincial
Natural Science Foundation for Distinguished Young Scholars
under No. LR19F020003, the Ant Financial Research Funding,
and the Alibaba-ZJU Joint Research Institute of Frontier Tech-
nologies. Min Yang is the corresponding author, and a faculty
of Shanghai Institute of Intelligent Electronics & Systems,
Shanghai Institute for Advanced Communication and Data
Science, and Engineering Research Center of CyberSecurity
Auditing and Monitoring, Ministry of Education, China.
REFERENCES
[1] “Cms
public
healthcare
dataset,”
https://www.cms.gov/Research-
Statistics-Data-and-Systems/Statistics-Trends-and-Reports/Medicare-
Provider-Charge-Data/Physician-and-Other-Supplier2016.html,
accessed: 2019-09-10.
[2] “Google-research/bert,”
https://github.com/google-research/bert,
accessed: 2020-1-4.
[3] “Identity
theft
continuing
problem in
the
state,
nation,”
https://www.enidnews.com/news/identity-theft-continuing-problem-in-
the-state-nation/article 6c1cf034-f40e-11e9-8401-07cb9f8a7875.html,
accessed: 2019-10-24.
[4] “Open sourcing bert: State-of-the-art pre-training for natural
lan-
guage processing,” https://ai.googleblog.com/2018/11/open-sourcing-
bert-state-of-art-pre.html, accessed: 2020-1-4 Published 2018-11-02.
[5] “Skytrax
dataset,”
https://github.com/quankiquanki/skytrax-reviews-
dataset, accessed: 2019-09-10.
searches
[6] “Understanding
better
than
ever
before,”
https://blog.google/products/search/search-language-understanding-
bert, accessed: 2019-09-10.
[7] M. Abadi, A. Chu, I. J. Goodfellow, H. B. McMahan, I. Mironov,
K. Talwar, and L. Zhang, “Deep learning with differential privacy,”
ArXiv, vol. abs/1607.00133, 2016.
[8] B. Agir, K. Huguenin, U. Hengartner, and J.-P. Hubaux, “On the privacy
implications of location semantics,” Proceedings on Privacy Enhancing
Technologies, vol. 2016, pp. 165 – 183, 2016.
[9] H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, and M. Marchand,
“Domain-adversarial neural networks,” ArXiv, vol. abs/1412.4446, 2014.
[10] G. Ateniese, L. V. Mancini, A. Spognardi, A. Villani, D. Vitali, and
G. Felici, “Hacking smart machines with smarter ones: How to extract
meaningful data from machine learning classiﬁers,” IJSN, vol. 10, pp.
137–150, 2013.
[11] Y. Bengio, A. C. Courville, and P. Vincent, “Representation learning: A
review and new perspectives,” TPAMI, vol. 35, pp. 1798–1828, 2012.
[12] P. Berrang, M. Humbert, Y. Zhang, I. Lehmann, R. Eils, and M. Backes,
“Dissecting privacy risks in biomedical data,” Euro Security & Privacy,
pp. 62–76, 2018.
[13] B. Biggio, B. Nelson, and P. Laskov, “Poisoning attacks against support
vector machines,” in ICML, 2012.
[14] T. Bolukbasi, K.-W. Chang, J. Y. Zou, V. Saligrama, and A. T. Kalai,
“Man is to computer programmer as woman is to homemaker? debiasing