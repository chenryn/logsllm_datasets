### Knowledge Level of the Adversary in the White-Box Setting

We vary the adversary's knowledge level in the white-box setting, using values in the set {10, 100, ..., 1000}, and conduct the MLP attack on the Medical dataset. The results are presented in Figure 6(c). As shown, the attack accuracy remains above 90% for each language model when the shadow corpus size exceeds 100. Interestingly, we observe that a larger language model (GPT-2-Large) is less robust than a smaller one when the adversary's knowledge is limited. For instance, with a shadow corpus size of only 10, the attack accuracy is 68.5%, 61.0%, and 59.4% against GPT-2-Large, GPT-2-Medium, and GPT-2, respectively. This finding aligns with the conclusion in [34] that more complex models tend to increase the attack surface.

### Black-Box Setting

Similarly, in the black-box setting, we evaluate the DANN attack's performance by varying the size of the external corpus from 100% to 5% of the original size of 2000, with results reported in Figure 6(d). A larger external corpus enhances the attack's accuracy, demonstrating the effectiveness of our proposed adversarial knowledge transfer procedure.

### Robustness of DANN Attack with Respect to Hyperparameters

We also investigate the robustness of the DANN attack concerning its hyperparameters. Specifically, we control the size of the victim embeddings, which corresponds to the adversary's knowledge level in the black-box setting, and the dimension of the domain-invariant representation in DANN, which is the only architectural factor. We conduct the DANN attack on the Medical dataset and report the attack accuracy in Figures 6(g) and (h). The results show that the DANN attack maintains high accuracy across different hyperparameter choices, reflecting the robustness and effectiveness of our proposed attack model.

### Possible Defenses

Given that sentence embeddings are a direct source of potential information leakage, a general mitigation principle is to obfuscate the embeddings. We empirically evaluate four possible defenses, where the first three are generally applicable to both attacks, and the last one is specifically designed for keyword inference attacks. While the ideal scenario is to eliminate sensitive information while preserving the necessary information for other tasks, a trade-off between utility and privacy seems unavoidable. Here, utility refers to the classification accuracy of the underlying benchmark systems. We hope our preliminary study will encourage further research into mitigation strategies. Detailed technical and experimental setups can be found in Appendix C.

#### 1. Rounding
For the first defense, we apply floating-point rounding to each coordinate of the sentence embeddings. Formally, this is expressed as \(\hat{z} = \text{rounding}(z, r)\), where \(r\) is a non-negative integer denoting the number of decimals preserved after rounding.

#### 2. Laplace Mechanism
The second defense leverages the Laplace mechanism, a differential privacy approach. We perturb the embedding coordinates with samples from a Laplace distribution, whose parameters are determined by the \(\ell_1\)-sensitivity of the language model \(f\) (denoted as \(\Delta f\), estimated using numerical algorithms). Formally, the defense works as \(\hat{z} = z + (Y_1, \ldots, Y_d)\), where \(Y_i\) are i.i.d. random samples drawn from \(\text{Lap}(\Delta f / \epsilon)\), the Laplace distribution with location 0 and scale \(\Delta f / \epsilon\).

#### 3. Privacy Preserving Mapping (PPM)
The third defense is based on adversarial training. We use a Privacy Preserving Mapping (PPM) parameterized by \(\theta\), which is trained to minimize the effectiveness of an imagined adversary \(A_\psi\). The PPM must also follow a utility constraint by distorting the embeddings within a limited radius around the original embedding, implemented as a regularization term. Formally, we learn the PPM \(D_\theta\) by solving the minimax game:
\[
\min_{\theta} \max_{\psi} \frac{1}{n} \sum_{i=1}^n A_\psi(D_\theta(z_i), s_i) + \lambda \|D_\theta(z_i) - z_i\|^2,
\]
where \(\lambda\) controls the privacy level, with higher values indicating lower privacy.

#### 4. Subspace Projection
The fourth defense, specifically designed for keyword inference attacks, projects out the unwanted subspace (i.e., privacy subspace) that encodes the occurrence of keywords from the universal sentence embedding space. Technical details on identifying the privacy subspace and performing the projection are provided in Appendix C. In our evaluations, we consider the ratio \(\beta\) between the dimension of the privacy subspace and that of the universal embedding space as the parameter of this defense. Intuitively, a higher \(\beta\) provides stricter privacy.

### Evaluations

We evaluate the first three defenses against the pattern reconstruction attack on the Genome dataset and all four defenses against the DANN-based keyword inference attack on the Medical dataset with a wide range of settings. The configurations and results are presented in Figure 7.

From Figure 7, it is evident that although each defense can reduce the attacker's success to a random guess under certain privacy budgets, they simultaneously compromise the utility of downstream tasks by causing significant degradation. For example, the Laplace mechanism degrades utility to a random guesser when achieving optimal defense performance. For PPM, despite a slighter trade-off, the utility for RoBERTa and Transformer-XL still degrades from over 90% to around 25% when optimal defense is achieved. Among these defenses, the subspace projection defense offers a more desirable balance. For most target language models, it can degrade the DANN attack to a random guesser by projecting out only 1% of the keyword-related subspace. However, the utility of the embeddings on the downstream task still decreases by about 15% compared to 95% accuracy with unprotected embeddings, indicating that the keywords we want to hide are also critical for essential semantics.

### Discussion

Our preliminary results highlight the challenge of balancing the elimination of token-level sensitive information from embeddings and the preservation of essential information for normal tasks. Given the practical threats posed by our attacks on the applications of general-purpose language models, we strongly recommend exploring effective defense mechanisms as future work.

### Threat Model

For Assumption 0, adversaries can obtain sentence embeddings if general-purpose language models are deployed in collaborative or federated learning systems, especially if the service provider snoops on user data or if embeddings are shared accidentally or maliciously. In some recent protocols, features may be encrypted with homomorphic encryption schemes, requiring the adversary to encrypt the external corpus embeddings with a public key and train attack models on the encrypted data. This is an interesting scenario for future research. However, many scenarios are not suitable for homomorphic encryption due to efficiency issues, such as real-time long passage translation or search engines with language models. In these cases, our attacks remain significant threats.

For Assumption 1, we develop a learning-based fingerprinting algorithm to determine the candidate model types based on dimension size and then identify the exact model type with a pretrained classifier. Surprisingly, the classification accuracy can reach 100%. More technical details and analysis are provided in Appendix D.

For Assumption 2, the adversary can easily satisfy this assumption by deploying the language model on local devices or accessing online services. Our current work adopts this assumption for generality, but the adversary could exploit specific model architectures and pre-trained parameters for better attack effectiveness, a direction for future research.

### Downstream Attacks

As mentioned in Sections V & VI, our proposed attacks can be used for downstream attacks with severe consequences. For example, if an attacker obtains the embedding of a treatment description, they can use the keyword inference attack to infer the occurrence probability of each word in a customized vocabulary, sort the probabilities, and invert the meaning of the sentence. Appendix E provides a demonstrative experiment. We find that the adversary can indeed reassemble the basic semantic meaning of the original text, even if some words are not in the customized vocabulary.

### Utility vs. Privacy in Deploying Sentence Embeddings

Our work indicates that the improved utility of sentence embeddings from general-purpose language models conflicts with privacy. Balancing the utility-privacy trade-off requires preserving information needed for downstream tasks and discarding the remainder. General-purpose language models are designed to provide embeddings for a wide range of tasks, enforcing the preservation of much token-level information, which leaves a window for privacy breaches. Based on our evaluations of eight state-of-the-art language models, we find that byte-level tokenization may offer additional privacy protection. Obfuscating sentence embeddings via adversarial training or subspace projection may be promising directions for future studies, as they can achieve a more desirable utility-privacy trade-off.

### Limitations and Future Work

Although we have observed differences in the security properties of different language models, we are still unclear about how other design choices, such as network depth, learning algorithms, and hyperparameters, influence privacy. Investigating these issues is a future research direction. Additionally, none of the four defenses we studied achieved an optimal balance between privacy and utility. Due to hardware constraints, we have not evaluated the defense quality of differentially private training techniques (e.g., DPSGD [7]). We hope our work will attract more attention to the privacy properties of NLP paradigms and corresponding mitigation approaches.

### Conclusion

In this paper, we design two novel attack classes—pattern reconstruction attacks and keyword inference attacks—to demonstrate the possibility of stealing sensitive information from sentence embeddings. We conduct extensive evaluations on eight industry-level language models to validate these privacy threats. We also provide a preliminary study on four defense approaches by obfuscating the sentence embeddings to attenuate sensitive information. To our knowledge, our work presents the first systematic study on the privacy risks of general-purpose language models, along with possible countermeasures. We hope our study will inspire more research on the security and privacy of general-purpose language models in real-world settings.

### Acknowledgements

We sincerely appreciate the shepherding from Piotr Mardziel and thank the anonymous reviewers for their constructive comments. This work was supported in part by the National Natural Science Foundation of China, the National Key Research and Development Program of China, the Natural Science Foundation of Shanghai, the Zhejiang Provincial Natural Science Foundation for Distinguished Young Scholars, Ant Financial Research Funding, and the Alibaba-ZJU Joint Research Institute of Frontier Technologies. Min Yang is the corresponding author and a faculty member at the Shanghai Institute of Intelligent Electronics & Systems, Shanghai Institute for Advanced Communication and Data Science, and Engineering Research Center of CyberSecurity Auditing and Monitoring, Ministry of Education, China.

### References

[References are listed as in the original text.]