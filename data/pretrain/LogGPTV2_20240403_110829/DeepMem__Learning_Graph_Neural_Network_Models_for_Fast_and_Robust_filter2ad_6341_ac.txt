functions, following by a softmax layer as the last layer.
After introducing the embedding network ϕw1 and the classifier
network ψw2, we will show how to train them together. During
training, training samples are fed into the embedding network for
contextual information collection. After propagating several itera-
tions, the final embedding vectors are fed into the classifier network
to generate the predicted output labels. To train the weights in the
GNN model, we compute the cross-entropy loss between the pre-
dicted label and annotated label, and update weights in the process
of minimizing the loss.
We adopt the BP (Back Propagation) [31] strategy to pass the
loss error from output layer back to previous layers to update the
weights along the way. In the next loop of training, the classifi-
cation is performed using newly-updated weights. After several
training loops, the loss will stabilize to a small value and the model
is fully trained. Specifically, we use Adam (Adaptive Moment Esti-
mation) [17] algorithm, a specific implementation of BP strategy,
as the weight parameter optimizer of the GNN deep model.
Formally, let training dataset D = {d1, d2, ...} be a set of node
samples, where each sample di = (v(i), y(i)) is a pair of node vector
and associated node label vector. The optimization goal is to com-
pute the solution to Equation (7). L is the cross-entropy loss func-
tion that estimates the differences between classifier outputs and an-
notated labels. The parameters of embedding network w1(including
weights of W1, σ1, σ2, σ3, σ4) and parameters of classifier network
w2 are updated and optimized in training.
|D |
i =1
arg min
w1,w2
L(y
(i)
, F(v(i))
(7)
3.4 Object Detection
The basic idea behind object detection is that if several nodes indi-
cate that there exists an object of certain type c at the same address
s in the memory dump, we can conclude with a high confidence that
we have detected an object of type c at that address, c ∈ C. Thus, a
node label can be considered as a voter that votes for the presence
of an object. For example, a node with a T _16_24 label means the
node votes for the address, 16 bytes before the node address, to
be the address of a _ETHREAD object. Each node in the memory
indicates the presence of an object. Thus with all the node labels,
we can generate a set of candidate object addresses S = {s1, s2, ...}
and corresponding voters for each address.
Next, we need to determine whether an address s ∈ S is indeed
a start address of an object. Ideally, if all the key nodes of type c
vote for s to be an object of type c, for example T _16_24, T _52_12,
T _84_28... all suggest the presence of an _ETHREAD at the same
address s, we can confidently report a _ETHREAD object is detected
at s. It is also likely that only a fraction of the key node labels votes
for address s, then our confidence to report address s will be lower.
We use L(s, c) to denoted the voter set, which is all the key node
labels of type c that vote for address s.
Specifically, we design a weighted voting mechanism. It gives dif-
ferent node labels (or in other words voters) different vote weights.
16𝑻_𝟓𝟐_𝟏𝟐𝑻_𝟖𝟒_𝟐𝟖𝑻_𝟏𝟔_𝟐𝟒5284ObjectAddress242812Since the voter with higher frequency in a certain object type bet-
ter indicates the presence of the objects of that type, and thus is
assigned with a larger weight. The weights are calculated from a
large real-world labeled dataset.
Finally, we introduce the prediction function f (s, c) in Equa-
tion (8). It measures the difference between the prediction con-
fidence and a pre-defined threshold δ. When the value of f (s, c)
exceeds the threshold, we draw a conclusion that an object with
type c is detected at address s.
(cid:40)1, 
li ∈L(s,c) ρ(c,li)
ρ(c) + γ(s, c)) > δ
(8)
f (s, c) =
0,
otherwise
Here, ρ is a counting function, ρ(c) counts the number of objects
of type c in the dataset, and ρ(c, l) counts the number of objects of
type c that has node label l in the dataset, l ∈ L(c). Then, we divide
ρ(c, l) by ρ(c) to estimate the weights of node label l in predicting
objects of type c , which is a decimal value in (0, 1]. Since the
weight values of voters range in (0, 1], it is possible that weighted
combination of multiple small-weighted voters is less than that of a
large-weighted single voter (e.g. weight sum of two small voters 0.4
+ 0.3 < weight value of a single large voter 0.8). In fact, the evidence
from multiple voters is more persuasive than a single voter with a
large weight, because it is less likely that two different voters both
make errors and vote for the same address of the same type in a
large and arbitrary memory space. So, we add a function γ(s, c) to
reward the cross-validated addresses voted by multiple voters.
In the implementation, the threshold δ is determined using a
searching method in the validation dataset. We run the experiment
by tuning the value of threshold δ to get the one that yields the
highest F-score [24], and set it as the default threshold. The reward
function is devised as γ(s, c) = |L(s, c)| − 1.
4 EVALUATION
In this section, we first describe the experiment setup in Section 4.1.
Then, we discuss the dataset collection and labeling approach in Sec-
tion 4.2. Section 4.3 provides details about training. In the end, we
present the evaluation results with respect to accuracy, robustness,
and efficiency in Section 4.4, 4.5, and 4.6 respectively.
4.1 Experiment Setup
Our experiment uses two settings of configurations. 1) The training
experiment is performed on a high-performance computing center
with each worker node equipped with 32 cores Intel Haswell CPUs,
2 x NVIDIA Tesla K80 GPUs and 128 GB memory. 2) The detection
experiment is performed on a moderate desktop computer with
Core i7-6700, 16GB, no GPU. We use powerful GPUs on the comput-
ing center for training, which is a one-time effort. Once the model
is trained, it is loaded on a desktop computer to conduct the kernel
object detection.
The deep neural network models in DeepMem, like embedding
network and classifier network, are all implemented using the open-
source deep learning framework TensorFlow [1]. The remaining
codes of data processing, statistics, plotting are programmed in
Python.
4.2 Dataset
4.2.1 Memory Dumps Collection. While DeepMem can analyze
any operating system versions in principle, it is limited by the ob-
ject labeling tool used in training. In the evaluation, we choose
to evaluate DeepMem on Windows 7 X86 SP1 rather than the lat-
est Windows 10, mainly because the object labeling tool we used,
Volatility [37], was unable to consistently parse Windows 10 images
or memory dumps, but worked very stable for Windows 7 images.
To automatically collect a large number of diverse memory
dumps for training and detection, we developed a tool with two
functionalities: 1) simulating various random user actions, and 2)
forcing the OS to randomly allocate objects in the memory space
between consecutive memory dumps.
To simulate various user actions, the memory collecting tool first
starts the guest Windows 7 SP1 virtual machine which is installed
in the VirtualBox [36]. When the virtual machine is started, guest
OS automatically starts 20 to 40 random actions, including starting
programs from a pool of the most popular programs, opening web-
sites from a pool of the most popular websites, and opening random
PDF files, office documents, and picture files. Next, the memory
collecting tool waits for 2 minutes and then dumps the memory of
the guest system to a dump file. When the dump is saved to the
hard disk of the host system, it restarts the virtual machine and
repeats until we collect 400 memory dumps, each of which is 1GB
in size.
To ensure kernel objects to be allocated at random locations,
we enabled KASLR when generating our dataset and restarted the
virtual machine after each dump. We found out that the address
allocations of objects are different among different memory dumps.
Only 1.32% _EPROCESS objects in a memory dump are located at
the same virtual address of _EPROCESS objects in another dump.
The ratio is 4.7% for _ETHREAD, 0.68% for _FILE_OBJECT, 15.9%
for _DRIVER_OBJECT. The basic statistics of memory dumps and
memory graphs are shown in Table 1.
Kernel Object Type
_EPROCESS
_ETHREAD
_FILE_OBJECT
_DRIVER_OBJECT
_LDR_DATA_TABLE_ENTRY
_CM_KEY_BODY
Memory Graph Statistics
Nodes
Edges
Mean Count
85
1,216
3,639
109
141
1,921
Mean Count
1,334,822
5,325,214
Std Dev
7.47
112.25
918.06
0.22
0.59
953.76
Std Dev
134,564.24
513,624.71
Table 1: Statistics of memory dumps and memory graphs.
4.2.2 Memory Graph Construction. To generate a memory graph,
we first read and scan all available memory pages in the kernel
virtual space of memory dumps. Then, we locate all the pointers
in the pages by finding all fields whose values fall into the range
of kernel virtual space. For each segment between two pointers,
we create a node in the memory graph. For each node, we find its
neighbor nodes in the memory dump according to the neighbor
definitions in Section 3.2, and create an edge in the memory graph.
Kernel Object Types
_EPROCESS
_ETHREAD
_DRIVER_OBJECT
_FILE_OBJECT
_LDR_DATA_TABLE_ENTRY
_CM_KEY_BODY
Object Length Avg. #TP Avg. #FP Avg. #FN Precision% Recall% F-Score
0.99807
0.99744
0.99872
0.98765
0.99145
0.97655
82.834
1211.476
108.938
3621.007
139.093
1979.207
99.979%
99.547%
99.766%
98.169%
100.0%
95.437%
99.635%
99.942%
99.978%
99.368%
98.304%
99.979%
0.017
5.514
0.255
67.545
0.0
94.621
0.303
0.7
0.024
23.045
2.4
0.414
704
696
168
128
120
44
Table 2: Object Detection Results on Memory Image Dumps.
4.2.3 Node Labeling. The node labeling process takes four steps:
1) utilize Volatility to find out the offset and length information of
6 kernel object types (i.e. _EPROCESS, _ETHREAD, _DRIVER_OBJECT,
_FILE_OBJECT, _LDR_DATA_TABLE_ENTRY, _CM_KEY_BODY) in mem-
ory dumps; 2) for each node in the memory graph, determine if
it falls into the range of any kernel object, and if so, calculate the
offset and length of that node in that kernel object and give the
node a label; 3) select the top 20 most frequent node labels across
all kernel objects of type c as key node label set L(c) for type c; and
4) label the rest nodes in the memory graph as none.
Sample Balancing. Inside a large memory dump, kernel ob-
4.2.4
jects only take up a small portion of the memory space. Thus,
the key nodes of kernel objects in the memory graph are very
sparse. Also, the key nodes of a certain object type are not evenly
distributed. To accelerate the training process and achieve better
detection results, we need to balance samples in the training dataset.
The principle of balancing is to preserve the topologies of the key
nodes in the memory graph after the balancing process. Specifically,
1) to reduce non-key nodes, we remove the nodes that are k-hops
away from key nodes in memory graph (k is a predefined value), 2)
to increase key nodes and balance between different node types, we
duplicate the key nodes to the same amount, and also duplicate the
edges between nodes in edge matrix. Since the embedding vector
is calculated using inward edges only, such duplication does not
create new neighbors for the original key nodes, so it does not affect
the topology propagation of the original key nodes.
4.3 Training Details
We split the collected 400 memory dumps into 3 subsets. We ran-
domly select 100 images as the training dataset, 10 images as the
validation dataset and the remaining 290 images as the testing
dataset. The validation dataset and testing dataset will not be used