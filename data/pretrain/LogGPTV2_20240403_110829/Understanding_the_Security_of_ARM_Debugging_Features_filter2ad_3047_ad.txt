...
...
eret
b payload
...
(a) Entering Debug State
(b) Exiting Debug State
(c) Entering Non-Debug State
Normal Memory
Secure Memory
Normal Memory
Secure Memory
Normal Memory
Secure Memory
ELR_EL3
smc #0
payload:
pc
...
...
eret
b payload
...
VBAR_EL3
+ 0x400
ELR_EL3
mov x0, #1
payload:
pc
VBAR_EL3
+ 0x400
...
...
eret
msr daifclr, #4
...
pc
mov x0, #1
VBAR_EL3
+ 0x400
msr daifclr, #4
...
(d) Executing smc Instruction
(e) Exiting Secure State
(f) Entering Non-Secure State
Figure 7: Executing Arbitrary Payload in the Secure State.
ARM Trusted Firmware (ATF) [11] and Linaro’s deliverables
on OpenEmbedded Linux for Juno [40] to build the software
environment that enables both the secure and non-secure OSes.
In the ATF implementation, the memory range 0xFF000000-
0xFFDFFFFF is conﬁgured as the secure memory, and we
demonstrate that we can copy arbitrary payload to the secure
memory and execute it via an LKM in non-secure EL1.
The source code of the implementation is included in
Appendix B, and Figure 7 describes the status and mem-
ory changes of the TARGET during the entire attack. The
highlighted red in the ﬁgure implies the changed status and
memory. In Figure 7(a), the TARGET is halted by the HOST
before the execution of the mov instruction. Meantime, the
VBAR_EL3 points to the EL3 exception vector. Since the
SMC exception belongs to the synchronous exception and
Juno board implements EL3 using 64-bit architecture,
the
corresponding exception handler is at offset 0x400 of the ex-
ception vector. Figure 7(b) shows the memory of the TARGET
before exiting the debug state. NAILGUN copies the payload
to the secure memory and changes the instruction pointed
by the DLR_EL0 to an smc instruction. Moreover, the ﬁrst
instruction in the 64-bit EL3 synchronous exception handler
(pointed by VBAR_EL3 + 0x400) is changed to a branch
instruction (the b instruction) targeting the copied payload.
Then, the HOST resumes the TARGET, and the pc points
to the malicious smc instruction, as shown in Figure 7(c).
The execution of the smc instruction takes the TARGET to
the status shown in Figure 7(d). Since the smc instruction
is already executed, the value of the ELR_EL3 register is
the address of the next instruction. Our manipulation of the
exception handler leads to the execution of the payload, which
can both perform malicious activities and restore the changed
memory. At the end of the payload, an eret instruction is
leveraged to switch back to the non-secure state. Figure 7(e)
indicates the memory and status before the switch, and the
changes to the non-secure memory and the EL3 exception
Figure 8: Executing Payload in TrustZone via an LKM.
vector is reverted. Moreover, the ELR_EL3 register is also
manipulated to ensure the execution of the mov instruction.
Finally, in Figure 7(f), the TARGET enters the non-secure state
again, and the memory and status look the same as that in
Figure 7(a).
Figure 8 shows an example of executing payload in Trust-
Zone via an LKM. Our payload contains a minimized serial
port driver so that NAILGUN can send outputs to the serial
port. To certify the attack has succeeded, we also extract the
current exception level from the CurrentEL register. The
last line of the outputs in Figure 8 indicates that NAILGUN is
able to execute arbitrary code in EL3, which owns the highest
privilege over the whole system.
3) Fingerprint Extraction in a Real-world Mobile Phone
To learn the impact of NAILGUN on the real-world devices,
we also show that NAILGUN is able to leak the sensitive
information stored in the secure memory. Currently, one of
the most used security features in the mobile phones is the
ﬁngerprint authentication [29], [48], [72], and the OEMs store
the ﬁngerprint image in TrustZone to enhance the security
of the device [2], [24], [62]. In this experiment, we use
Huawei Mate 7 [29] to demonstrate that the ﬁngerprint image
can be extracted by an LKM running in the non-secure EL1
with the help of NAILGUN. The Huawei Mate 7 is powered
by HiSilicon Kirin 925 SoC, which integrates a quad-core
(cid:23)(cid:18)(cid:18)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:28 UTC from IEEE Xplore.  Restrictions apply. 
bit ARMv8 debugging architecture, and similar implications
exist in 32-bit ARMv8 and ARMv7 architecture. However,
there are also some major differences among the implementa-
tions of these architectures, and we discuss the differences in
the following.
32-bit ARMv8 Debugging Architecture. We implement pro-
totypes of NAILGUN with 32-bit ARMv8 on Raspberry PI 3
Model B+ and Motorola E4 Plus. In this architecture, the steps
of halting processor are similar to the aforementioned steps in
64-bit ARMv8 architecture, and the major difference between
NAILGUN on 32-bit and 64-bit ARMv8 architecture is the
usage of the EDITR. In the 64-bit ARMv8, we directly write
the binary representation of the instruction into the EDITR.
However, the ﬁrst half and last half of the instruction need
to be reversed in the 32-bit ARMv8. For example, the binary
representation of the dcps3 instruction is 0xD4A00003 and
0xF78F8003 in 64-bit and 32-bit ARMv8, respectively. In
the 64-bit ARMv8 architecture, we make the processor in the
debug state execute this instruction via writing 0xD4A00003
to the EDITR. However, the instruction written to the EDITR
should be 0x8003F78F instead of 0xF78F8003 in the 32-
bit ARMv8 architecture.
ARMv7 Debugging Architecture. In regard to ARMv7, we
implement NAILGUN on Huawei Mate 7 as discussed in
Section V-B3, and there are three major differences between
NAILGUN on ARMv7 and ARMv8 architectures. Firstly, the
ECT is not required to halt and restart a processor in ARMv7.
Writing 1 to the bit[0] and bit[1] of the Debug Run Control
Register (DBGDRCR) can directly halt and restart a processor,
respectively. Secondly, the ITRen bit of the EDSCR controls
whether the EDITR is enabled in ARMv7 architecture. We
need to enable the ITRen bit after entering the debug state
and disable it again before exiting the debug state. Lastly, the
dcps instructions are undeﬁned in the ARMv7 architecture,
and we need to change the M bits of the Current Program Status
Register (CPSR) to promote the processor to the monitor mode
to access the secure resource.
VI. COUNTERMEASURE
A. Disabling the Signals?
Since NAILGUN attack works only when the debug authenti-
cation signals are enabled, disabling these signals, in intuition,
crafts an effective defense. However, according to the ARM
Architecture Reference Manual [4], [5], the analysis results in
Section IV, and the responses from the hardware vendors, we
consider these signals cannot be simply disabled due to the
following challenges:
Challenge 1: Existing tools rely on the debug authentica-
tion signals. The invasive and non-invasive debugging features
are heavily used to build analysis systems [14], [16], [17],
[18], [22], [38], [39], [44], [50], [74]. Disabling the debug
authentication signals would directly make these systems fully
or partially malfunction. In the ARMv7 architecture [4], the
situation is even worse since the functionality of the widely
used Performance Monitor Unit (PMU) [1], [13], [19], [23],
[50], [61], [76] also relies on the authentication signals.
Figure 9: Fingerprint
Image Leaked by NAILGUN from
Huawei Mate 7. Note that the right half of the image is blurred
for privacy concerns.
Cortex-A15 cluster and a quad-core Cortex-A7 cluster. The
FPC1020 [20] ﬁngerprint sensor is used in Mate 7 to capture
the ﬁngerprint image. This phone is selected since the product
speciﬁcation [21] and driver source code [71] of FPC1020
are publicly available, which reduces the engineering effort of
implementing the attack.
As shown in the previous experiment, NAILGUN offers
a non-secure EL1 LKM the ability to read/write arbitrary
secure/non-secure memory. To extract the ﬁngerprint image,
we need to know 1) where the image is stored and 2) the
format of the image data.
location of
the
To learn the
image, we decom-
pile the TEE OS binary image, which is mapped to
/dev/block/mmcblk0p10, and identify that a function
named fpc1020_fetch_image is used to read the image
from the ﬁngerprint sensor. This function takes a pointer to an
image buffer, an offset to the buffer, and the size of the image
as parameters, and copies the ﬁngerprint image fetched from
the sensor to the image buffer. With further introspection, we
ﬁnd that Huawei uses a pre-allocated large buffer to store this
image, and a pointer to the head of the buffer is stored in a
ﬁxed memory address 0x2efad510. Similarly, the size of
the image is stored at a ﬁxed memory address 0x2ef7f414.
With the address and size, we extract the image data with
NAILGUN. Since the ARM architectures in Huawei Mate 7
and ARM Juno board are different, the implementations of
NAILGUN are also different (see Section V-B4). The source
code of this experiment is included in Appendix C.
The format of the image data is well-documented in the
FPC1020 product speciﬁcation [21]. According to the speciﬁ-
cation, each byte of the data indicates the gray scale level
of a single pixel. Thus, with the extracted image data, it
is trivial to craft a gray scale ﬁngerprint image. Figure 9
shows the ﬁngerprint image extracted from Huawei Mate 7
via NAILGUN, and this result demonstrates that NAILGUN is
able to leak the sensitive data from the TEE in commercial
mobile phones with some engineering efforts.
4) NAILGUN in 32-bit ARMv8 and ARMv7 Architecture
In Section III, we discussed the security implications of 64-
(cid:23)(cid:18)(cid:19)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:28 UTC from IEEE Xplore.  Restrictions apply. 
Since most of the aforementioned analysis systems attempt
to perform malware detection/analysis, the risk of information
leakage or privilege escalation by misusing the debugging fea-
tures is dramatically increased (i.e., the debugging architecture
is a double-edged sword in this case).
Challenge 2: The management mechanisms of the debug
authentication signals are not publicly available. According
to Section IV-C, the management mechanism of the debug
authentication signals is unavailable to the public in most
tested platforms. In our investigation, many SoC manufacturers
keep the TRMs of the SoC conﬁdential; and the publicly
available TRMs of some other SoCs do not provide a com-
plete management mechanism of these signals or confuse
them with the JTAG debugging. The unavailable management
mechanism makes it difﬁcult
to disable these signals by
users. For example, developers use devices like Raspberry
PI to build their own low-cost IoT solutions, and the default
enabled authentication signals put their devices into the risk of
being remotely attacked via NAILGUN. However, they cannot
disable these authentication signals due to the lack of available
management mechanisms even they have noticed the risk.
Challenge 3: The one-time programmable feature prevents
conﬁguring the debug authentication signals. We also note
that many of the tested platforms use the fuse to manage the
authentication signals. On the one hand, the one-time pro-
grammable feature of the fuse prevents the malicious override
to the debug authentication signals. However, on the other
hand, users cannot disable these signals to avoid NAILGUN
due to the same one-time programmable feature on existing
devices. Moreover, the fuse itself is proved to be vulnerable
to hardware fault attacks by previous research [66].
Challenge 4: Hardware vendors have concerns about the
cost and maintenance. The debug authentication signals are
based on the hardware but not the software. Thus, without
additional hardware support,
the signals cannot be simply
disabled by changing software conﬁgurations. According to
the response from hardware vendors, deploying additional
restrictions to the debug authentication signals increases the
cost for the product lines. Moreover, disabling the debug au-
thentication signals prohibits the legitimate debugging process
such as repairing or bug ﬁxing after a product recall, which
introduces extra cost for the maintenance process.
B. Comprehensive Countermeasure
We consider NAILGUN attack is caused by two reasons:
1) the debug authentication signals deﬁned by ARM does
not fully consider the scenario of inter-processor debugging,
which leads to the security implications described in Sec-
tion III; 2) the conﬁguration of debug authentication signals
described in Section IV-B, which is related to the OEMs and
cloud providers, and the management mechanism described
in Section IV-C, which is related to the SoC manufacturers,
make NAILGUN attack feasible on real-world devices. Thus,
the countermeasures discussed in this section mainly focus
on the design, conﬁguration, and management of the debug
authentication signals. As a supplement, we also provide the
defense that restricting the access to the debug registers, which
may prevent the implementation of NAILGUN. In general,
we leverage the defense in depth concept and suggest a
comprehensive defense across different roles in the ARM
ecosystem.
1) Defense From ARM
Implementing additional restriction in the inter-processor
debugging model. The key issue that drives the existence
of NAILGUN is that the design of the debug mechanism and
authentication signals does not fully consider the scenario of
the newly involved inter-processor debugging model. Thus,
redesign them and make them consider the differences be-
tween the traditional debugging mode and the inter-processor
debugging model would keep the security implications away
completely. Speciﬁcally, we suggest the TARGET checks the
type of the HOST precisely. If the HOST is off-chip (the
traditional debugging model), the existing design is good to
work since the execution platforms of the TARGET and the
HOST are separated (their privileges are not relevant). In regard
to the on-chip HOST (the inter-processor debugging model), a
more strict restriction should be required. For example, in the
invasive debugging, the TARGET should check the privilege
of the HOST and response to the debug request only if the
HOST owns a higher or the same privilege as the TARGET.
Similarly, the request of executing dcps instructions should
also take the privilege of the HOST into consideration. The
HOST should never be able to issue a dcps instruction that
escalates the TARGET to an exception level higher than the
current HOST’s exception level.
Reﬁning the granularity of
the debug authentication
signals. Other than distinguishing the on-chip and off-chip
HOST, we also suggest the granularity of the authentication
signals should be improved. The DBGEN and NIDEN signals
are designed to control the debugging functionality of the
whole non-secure state, which offers a chance for the kernel-
level (EL1) applications to exploit the hypervisor-level (EL2)
execution. Thus, we suggest a subdivision to these signals.
2) Defense From SoC Manufacturers
Deﬁning a proper restriction to the signal management
procedure. Restricting the management of these signals would
be a reasonable defense from the perspective of the SoC
manufacturers. Speciﬁcally, the privilege required to access
the management unit of a debug authentication signal should
follow the functionality of the signal to avoid the malicious
override. For example, the management unit of the SPNIDEN
and SPIDEN signals should be restricted to secure-access only.
The restriction methods of current SoC designs are either too
strict or too loose. On the ARM Juno SoC [10], all the debug
authentication signals can only be managed in the secure state.
Thus, if these signals are disabled, the non-secure kernel can
never use the debugging features to debug the non-secure
processor, even the kernel already owns a high privilege in
the non-secure content. We consider this restriction method is
too strict since it somehow restricts the legitimate usage of
the debugging features. The design of the i.MX53 SoC [51],
(cid:23)(cid:18)(cid:20)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:43:28 UTC from IEEE Xplore.  Restrictions apply. 
as opposed to ARM Juno SoC, shows a loose restriction. The
debug authentication signals are designed to restrict the usage
of the external debugger, however, the i.MX53 SoC allows
an external debugger to enable the authentication signals. We
consider this restriction method is too loose since it introduces
a potential attack surface to these signals.
Applying hardware-assisted access control to the debug
registers. NAILGUN attack relies on the access to the debug
registers, and the access is typically achieved by memory-
mapped interfaces. Intuitively, the restriction to the access
of these registers would help to enhance the security of the
platform. However, we consider this restriction should be
controlled in hardware-level instead of software-level. If the
restriction is implemented by software running in the non-
secure mode (e.g., the OS), the malware with kernel privilege
may bypass it easily. If the restriction is implemented in
the secure mode (e.g., TEE), it might introduce a signiﬁcant
performance overhead due to the semantic gap between the
two modes. In contrast, if the hardware-assisted access control
applies, the access to the debug registers may be protected by
hardware traps or interrupts. During the responsible disclosure
to MediaTek, we learn that they have the hardware-based
technology for TrustZone boundary division, and they are
planning to use it to restrict the access to the debug registers
to mitigate the reported attack.
3) Defense From OEMs and Cloud Providers
Keeping a balance between security and usability. With
the signal management mechanism released by the SoC man-
ufacturers, we suggest that OEMs and cloud providers disable
all the debug authentication signals by default. This default
conﬁguration not only helps to protect the secure content from
the non-secure state, but also avoids the privilege escalation
among the non-secure exception levels. Meantime, they should
allow the application with a corresponding privilege to enable
these signals for legitimate debugging or maintenance purpose,
and the usage of the signals should strictly follow the man-
agement mechanism designed by the SoC manufacturers. With
this design, the legitimate usage of the debugging features
from the privileged application is allowed while the misuse
from the unprivileged application is forbidden. Moreover,
since the debugging features are exploited via the CoreSight
components and the debug registers, applying a similar re-
striction to the access of CoreSight components and debug
registers can also form an effective defense.
Disabling the LKM in the Linux-based OSes. In most
platforms, the debug registers work as an I/O device, and the
attacker needs to manually map the physical address of the
debug registers to virtual memory address space, which re-
quires kernel privilege, to gain access to these registers. In the
Linux kernel, the regular approach to execute code with kernel
privilege is to load an LKM. The LKMs in the traditional PC
environment normally provide additional drivers or services.
However, in the scenario of mobile devices and IoT devices,