r
F
t
i
n
u
y
c
i
l
o
p
n
i
(a)
(b)
1
Policy Units
2
1
Policy Units
2
Figure 7: Policy in Univ-1 is “on by default,” with 70% of hosts
able to reach nearly all subnets and 30% able to reach all sub-
nets.
that in these networks, at least of one the policy units spans all rou-
ters. However, in policy-heavy networks, about 60% of the policy
units each span 20% or fewer routers in the network. Also, we ﬁnd
that none of the units in the policy-heavy networks span all routers.
This illustrates how the policy units expose the compartmentaliza-
tion imposed by the network, with hosts connected to different parts
of the network being subject to different policies and able to reach
different sets of end-points.
In ﬁgure 6(b) we examine the cumulative fraction of end-points
in the networks’ policy units. We ﬁnd that all networks have at least
one unit to which over 60% of the end-points in the network belong.
This means that most end-points in all the networks we studied are
subject to identical policies. Furthermore, for all networks, most
end-points members of only a handful of units. For instance, we ob-
serve that about 70% of the end-points in the policy-heavy networks
(Univ-3 and Enet-2) fall into 15% of the units (this corresponds to
2 and 4 policy units in Univ-3 and Enet-2, respectively). Likewise,
in the policy-lite networks, we ﬁnd that all end-points are members
of one or two policy units. This suggests that policy is unevenly
applied in the networks we studied. Most hosts experience the same
policies, but certain special hosts are selected for extra constraints
or extra freedom.
Next, we now dig deeper into the the policy units for a policy-
lite and a policy-heavy network. In particular, we focus on the dif-
ferences among the policy units and identify key properties of the
reachability policies that the networks are trying to implement.
Univ-1. In Figure 7 (a) we examine the differences among the
policy units in Univ-1 in terms of the destinations they can reach.
Recall that Univ-1, which is a policy-lite network, divides its end-
points into two policy units. Of these, hosts in one unit (policy unit 2
on the right) are able to reach all subnets in the network, while those
in the other (unit 1) are unable to reach about 2% of the subnets in
the network. From Figure (b), we note that unit-1 is comprised of
70% of the network’s end-points, while the remaining 30% are in
unit 2.
140We were able to discuss our empirical observations with the net-
work’s operators. The operators validated that the network indeed
implements two such units. In particular, the network controls the
reachability to a speciﬁc collection of subnets (all of which were
attached to a given router) by preventing route announcements to
the subnets from reaching other routers (this is achieved using the
appropriate route ﬁlters). This small collection of subnets, however,
were reachable from the rest of the subnets attached to the router.
On the whole these observations indicate that Univ-1 implements
a fairly uniform “on by default” policy across all end-points, mean-
ing that hosts can access almost all network resources unless pre-
vented by some explicit mechanism.
Univ-3. Univ-3 provides a more complex case study, with 15
distinct policy units. The results from our study of this network are
summarized in Figure 8. From Figure 8 (a), we observe that 9 of the
15 units in this network, units 7 through 15, have almost complete
reachability to the rest of the subnets in the network, with each unit
being able to reach at least 98% of the subnets in the network. In
contrast, units 1 through 6 have very limited reachability, being able
to reach only between 20 and 45% of the subnets in the network.
In Figure 8 (b), we illustrate the distributions of end-points across
various policy units. The policy units 7 through 15, all of which
have roughly universal reachability, vary signiﬁcantly in the number
of end-points in them: unit 13 contains 70% of the end-points, unit
11 contains 20% of the end-points, while units 9, 10, 12, 14 and 15
contain a miniscule fraction of end-points each. In contrast, the total
number of end-points in policy units 1 through 6, which can reach a
much smaller fraction of network resources each, is < 5%.
These results show that the policy implemented by Univ-3 is char-
acterized by an interesting dichotomy. The policy divides the net-
work into two unequal parts. One part, which contains an over-
whelming fraction of end-points is “on by default”, meaning that
hosts in this part can access almost all network resources. A few
special cases restrict the speciﬁc set of resources that small collec-
tions of hosts can access. In the other part of the network, which
contains a much smaller fraction of end-points, the policy is closer
to being “off by default”, meaning that the hosts cannot access most
network resources by default. Upon examining the conﬁguration
ﬁles, we noticed that Univ-3 used data plane mechanisms (i.e. packet
ﬁlters) to implement the special cases in the ﬁrst part. However, to
implement the off-by-default access policy in the second part, the
network used both packet ﬁlters as well as control plane mecha-
nisms such as route ﬁlters.
.
4. APPLICATION TO NETWORK MANAGE-
MENT
Below, we provide a brief outline of two network management
tasks where policy units derived from the network conﬁguration
state may prove valuable.
4.1 Making informed changes to conﬁgura-
tion.
When installing or altering conﬁguration in large networks, oper-
ators have no way to systematically reason about how their changes
may impact the network’s policies. Operators employ a combina-
tion of ad hoc reasoning, designing time -consuming management
of change (MOC) reports or waiting for users’ complaints or even
attacks, before determining that their changes have had an undesir-
able interaction with the intended network-wide policies. Operators
can use policy units to validate changes by comparing policy units
before and after the change is made. Operators can use the “diffs”
between the two states to debug the network change.
e
r
a
t
a
h
t
s
t
e
n
b
u
s
f
o
n
o
i
t
c
a
r
F
t
i
n
u
y
c
i
l
o
p
m
o
r
f
e
l
b
a
h
c
a
e
r
1
0.8
0.6
0.4
0.2
0
s
t
s
o
h
f
o
n
o
i
t
c
a
r
F
t
i
n
u
y
c
i
l
o
p
n
i
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
(a)
(b)
1
2
3
4
5
1
2
3
4
5
7
6
Policy Units
8
9 10 11 12 13 14 15
7
6
Policy Units
8
9 10 11 12 13 14 15
Figure 8: Policy in Univ-3 places most hosts into “default on”
units 7-15, and a few hosts into “default off” units.
4.2 Examining trends in network policy evo-
lution.
Over time networks grows in size and complexity, making it dif-
ﬁcult for operators to determine if the overall policies can be im-
plemented using simpler mechanisms. For instance, a network may
start out with conservative communication rules requiring a multi-
tude of policy units, but over time, more and more end-points are
granted a uniform set of privileges to access a common set of re-
sources. Although the conﬁguration becomes complex, since more
and more end-points are granted a common set of privileges, the
network’s policy units may coalesce and shrink in total number. By
monitoring policy units, a network operator can examine how the
network-wide policy evolves and whether, at some point, the num-
ber of units and the nature of the PPS naturally lend themselves to
an alternate, much simpler network-wide conﬁguration.
5. RELATED WORK
Prior work [12, 3] developed approaches for modeling the reach-
ability between routers. Our approach builds on these proposals,
but it coalesces and extracts the high-level network policies imple-
mented in a network conﬁguration. The policy units we extract are
a natural match with how operators view and design their networks,
while the reachability sets between routers computed by prior work
do not expose the commonality or structure of the policies applied
to hosts. Second, network modiﬁcations, such as movement of de-
partment across buildings or movement of ﬁlters from core to edge,
can greatly affect a network’s reachability sets. However, the policy
units remain unchanged as the constraints applied to the network’s
hosts remain constant.
Several projects [2, 5, 13] attempt to simplify network manage-
ment by using clean-slate approaches to represent and implement
global policies. Policy units can be used to unearth the structure
141of policies in an existing network that need to implemented in the
clean-slate design, and help select between “default on” and “de-
fault off” strategies.
A few studies [9, 11] examined trafﬁc characteristics in enter-
prises, and other recent work developed models for design and con-
ﬁguration in enterprises [3, 7]. Our work adds to these by shedding
light on the nature of reachability policies in enterprises.
6. SUMMARY
While there has been a growing interest in understanding the de-
sign and operation of enterprise networks, few studies, if any, have
examined the nature of reachability policies implemented by enter-
prises today. In this paper, we introduced the notion of policy units
that form an abstract representation of how enterprise reachability
policies segregate end-points into distinct privilege classes. We pre-
sented an initial algorithm for extracting policy units from router
conﬁgurations. We applied the algorithm to ﬁve production net-
works and veriﬁed our observations with the operators of some of
the networks. Through our study, we obtained unique insights into
the current implementation of reachability policies in enterprises. In
particular, we found that most hosts in these networks are subjected
to a uniform set of reachability policies, while a few special case
hosts have very restricted reachability.
We argued that our empirical observations and the policy unit
extraction framework are useful to inform clean-slate approaches,
to support network re-design efforts, and to augment current ap-
proaches to network conﬁguration and management. We expect pol-
icy units will be valuable as an aid for visualizing the network-wide
conﬁguration state, and are exploring these directions.
7. ACKNOWLEDGEMENT
We would like to thank Dale Carder, Perry Brunelli, and the other
operators for their network conﬁguration ﬁles. This work was sup-
ported in part by an NSF CAREER Award (CNS-0746531) and an
NSF NeTS FIND Award (CNS-0626889).
8. REFERENCES
[1] S. Acharya, J. Wang, Z. Ge, T. Znati, and A. Greenberg.
Simulation study of ﬁrewalls to aid improved performance. In
ANSS ’06: Proceedings of the 39th annual Symposium on
Simulation, pages 18–26, Washington, DC, USA, 2006. IEEE
Computer Society.
[2] H. Ballani and P. Francis. CONMan: A Step towards Network
Manageability. In Proc. of ACM SIGCOMM, 2007.
[3] T. Benson, A. Akella, and D. A. Maltz. Unraveling the
complexity of network management. In NSDI, April 2009.
[4] M. Casado, M. Friedman, J. Pettitt, N. Mckeown, and
S. Shenker. Ethane: Taking Control of the Enterprise. In
SIGCOMM ’07.
[5] M. Casado, T. Garﬁnkel, A. Akella, M. Friedman, D. Boneh,
N. Mckeown, and S. Shenker. SANE: A Protection
Architecture for Enterprise Networks. In USENIX Security,
Vancouver, BC, Canada, Aug. 2006.
[6] A. Feldmann and S. Muthukrishnan. Tradeoffs for packet
classiﬁcation. In INFOCOM, 2000.
[7] P. Garimella, Y.-W. E. Sung, N. Zhang, and S. Rao.
Characterizing VLAN usage in an operational network. In
INM ’07.
[8] A. Greenberg, G. Hjalmtysson, D. A. Maltz, A. Myers,
J. Rexford, G. Xie, H. Yan, J. Zhan, and H. Zhang. A clean
slate 4d approach to network control and management.
SIGCOMM Comput. Commun. Rev., 35(5):41–54, 2005.
[9] S. Guha, J. Chandrashekar, N. Taft, and K. Papagiannaki.
How healthy are today’s enterprise networks? In IMC, 2008.
[10] D. A. Maltz, G. Xie, J. Zhan, H. Zhang, G. Hjálmtýsson, and
A. Greenberg. Routing design in operational networks: a look
from the inside. SIGCOMM Comput. Commun. Rev.,
34(4):27–40, 2004.
[11] R. Pang, M. Allman, M. Bennett, J. Lee, V. Paxson, and
B. Tierney. A ﬁrst look at modern enterprise trafﬁc. In IMC,
2005.
[12] G. G. Xie, J. Zhan, D. A. Maltz, H. Zhang, A. G. Greenberg,
G. Hjálmtýsson, and J. Rexford. On static reachability
analysis of ip networks. In INFOCOM, pages 2170–2183.
IEEE, 2005.
[13] H. Yan, D. A. Maltz, T. S. E. Ng, H. Gogineni, H. Zhang, and
Z. Cai. Tesseract: A 4d network control plane. In NSDI.
USENIX, 2007.
142