### 6.1 Network Driver Recovery

Network driver recovery is managed by the network server or application, and this process is transparent to the user. When a driver crashes, any ongoing request fails and is postponed until the driver is restarted. Upon notification from the data store, the network server checks its tables to determine if the update concerns a new driver or a recovered one. If it is a recovered driver, the server initiates an internal recovery procedure. This procedure closely mirrors the steps taken when the driver is first started. The Ethernet driver is reinitialized to promiscuous mode and instructed to resume I/O operations. Since the Ethernet driver is stateless, it does not need to retrieve lost state from the data store, although it could if necessary.

### 6.2 Block Driver Recovery

Block device driver crashes are handled by the native MINIX 3 file server, MFS. Transparent recovery without data loss is possible because disk block I/O is idempotent. If I/O was in progress at the time of the failure, the IPC rendezvous is aborted by the kernel, and the file server marks the request as pending. The file server then blocks and waits for the disk driver to restart and for pending I/O requests to be retried, as shown in Figure 5.

Unlike other drivers, disk driver recovery is not policy-driven, as reading the recovery script from disk would be required. Instead, the reincarnation server directly restarts failed disk drivers from a copy in RAM. Disk drivers, like other drivers in the system, are stateless and do not need to retrieve lost state from the data store, though they could if necessary. Once the driver is restarted, the reincarnation server publishes the new IPC endpoint in the data store, and the file server is notified. The file server then updates its device-driver mappings and reinitializes the disk driver by reopening minor devices if needed. Finally, the file server checks for any pending I/O requests and reissues the failed disk operations, resuming normal operation.

If policy-driven recovery is needed for disk drivers, the system can be configured with a dedicated RAM disk to provide trusted storage for crucial data, such as driver binaries, the shell, and policy scripts. We have developed a small 450-line RAM disk driver for this purpose.

### 6.3 Character Driver Recovery

Character device drivers cannot be transparently recovered because it is impossible to determine which part of the data stream was successfully processed and which data is missing. If an input stream is interrupted due to a driver crash, input might be lost because it can only be read from the controller once. Similarly, if an output stream is interrupted, there is no way to tell how much data has been output to the controller, making full recovery impossible. Therefore, such failures are reported to the application layer, where further recovery might be possible, as illustrated in Figure 6.

Historically, most applications assume that a driver failure is fatal and immediately give up. However, our prototype supports continuous operation when applications are made recovery-aware. For example, the printer daemon could be modified to automatically reissue failed print requests without bothering the user. While transparent recovery is not possible—duplicate printouts may result—the user benefits from this approach. Another example is an MP3 player, which could continue playing a song after a driver recovery, with the risk of small hiccups. Only when the application layer cannot handle the failure, the user needs to be informed. For instance, continuing a CD or DVD burn process after a SCSI driver failure will likely produce a corrupted disc, so the error must be reported to the user.

### 7. Experimental Evaluation

We evaluated our recovery mechanisms in several ways. First, we discuss the performance overhead introduced by our recovery mechanisms. Then, we report on the results of software fault-injection. Finally, we quantify the reengineering effort needed to prototype our recovery mechanisms in MINIX 3.

#### 7.1 Performance Overhead

To determine the performance overhead introduced by our recovery mechanisms, we simulated driver crashes while I/O was in progress and compared the performance to an uninterrupted I/O transfer. We performed this test for both block device and network drivers. The crash simulation was done using a shell script that initiates the I/O transfer and repeatedly looks up the driver’s process ID and kills the driver using a SIGKILL signal. The test was run with varying intervals between the simulated crashes. The recovery policy used for these tests directly restarts the driver without introducing delays. After the transfer, we verified that no data corruption occurred. In all cases, full recovery was transparent to the application and did not require user intervention.

**Network Driver Recovery:**

We measured the overhead for the recovery of network drivers using the RealTek 8139 Ethernet driver. We initiated a TCP transfer using the `wget` utility to retrieve a 512-MB file from the Internet. We ran multiple tests with the period between the simulated crashes ranging from 1 to 15 seconds. In all cases, `wget` successfully completed, with the only noticeable difference being a small performance degradation, as shown in Figure 7. To verify data integrity, we compared the MD5 checksums of the received data with the original file. The mean recovery time for the RealTek 8139 driver failures is 0.48 seconds, partly due to the TCP retransmission timeout. The uninterrupted transfer time is 47.41 seconds with a throughput of 10.8 MB/s. The interrupted transfer times range from 47.85 seconds to 62.98 seconds, with throughputs of 10.7 MB/s and 8.1 MB/s, respectively. The loss in throughput due to Ethernet driver failures ranges from 25% to just 1% in the best case.

**Disk Driver Recovery:**

We also measured the overhead of disk driver recovery by repeatedly sending a SIGKILL signal to the SATA hard disk driver while reading a 1-GB file filled with random data using `dd`. The input was immediately redirected to `sha1sum` to calculate the SHA-1 checksum. Again, we killed the driver with varying intervals between the simulated crashes. The data transfer successfully completed in all cases with the same SHA-1 checksum. The transfer rates are shown in Figure 8. The uninterrupted disk transfer completed in 31.33 seconds with a throughput of 32.7 MB/s. The interrupted transfer showed a transmission time ranging from 83.06 seconds to 34.73 seconds, with throughputs of 12.3 MB/s and 30.5 MB/s for simulated crashes every 1 and 15 seconds, respectively. The performance overhead of disk driver recovery ranges from 62% to about 7% in this test. The higher recovery overhead compared to the previous experiment is due to the much higher I/O transfer rates.

#### 7.2 Fault-Injection Testing

To test the capability of our system to withstand and recover from driver failures, we also simulated failures in our drivers using software fault-injection. We based our experiments on two existing fault injectors that mutate binary code [32, 39, 40]. This kind of fault injection is representative of real failures [29]. We used the following seven fault types: (1) change source register, (2) change destination register, (3) garble pointer, (4) use current register value instead of parameter passed, (5) invert termination condition of a loop, (6) flip a bit in an instruction, or (7) elide an instruction. These faults emulate common programming errors in operating system code [11, 37].

The fault-injection experiments demonstrated that our design can successfully recover from common, transient failures and provide continuous operation. One experiment run inside the Bochs PC emulator 2.2.6 targeted the DP8390 Ethernet driver and repeatedly injected a randomly selected fault into the running driver.