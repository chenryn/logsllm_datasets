### Experimental Results and Observations

#### Temporal Covariate Shift Analysis
The following values represent the performance metrics (balanced accuracy, NLL, BSE) for different models under temporal covariate shift:
- **Ensemble**: 99.18
- **wEnsemble**: 99.00
- **Other Metrics**:
  - 0.048, 0.008, 0.015: 95.90
  - 0.156, 0.029, 0.066: 33.22
  - 7.015, 0.985, 0.992: 0.00
  - 0.773, 0.116, 0.148: 85.45
  - 0.536, 0.105, 0.165: 93.55
  - 0.592, 0.117, 0.180: 84.00
  - 0.366, 0.068, 0.129: 93.00
  - 2.331, 0.298, 0.341: 66.03
  - 1.717, 0.285, 0.345: 66.03
  - 2.066, 0.272, 0.330: 71.31
  - 1.996, 0.250, 0.292: 80.05

**Figure 6** illustrates the balanced accuracy (bAccuracy), balanced NLL (bNLL), and balanced BSE (bBSE) under temporal covariate shift. Key observations include:
1. **Performance Degradation**: Malware detectors experience a significant decrease in accuracy and an increase in bNLL and bBSE with newer test data. This is attributed to the natural evolution of software, where Google updates Android APIs and developers upgrade their APKs.
   - **Droidetec**: Suffers significantly from temporal covariate shift, possibly due to its limited API learning capability.
2. **Calibration and Ensemble Methods**:
   - **Temp Scaling**: Has no significant effect on bAccuracy.
   - **Ensemble Methods**: Enhance vanilla models (DeepDrebin, MultimodalNN, and DeepDroid) initially but this enhancement diminishes over time.
   - **VBI**: Makes MultimodalNN the most robust under data evolution, achieving around 80% bAccuracy.
   - **Calibration Benefits**: Ensemble methods improve bNLL and bBSE compared to vanilla models.

**Figure 7a** plots the accuracy and balanced accuracy after excluding examples with entropy values greater than a threshold Ï„. **Figure 7b** shows the sample density of predictive entropy. Observations include:
1. **Entropy Impact**: Both accuracy and balanced accuracy decrease dramatically as entropy increases, particularly for DeepDroid and Droidetec.
2. **MultimodalNN with VBI**: Performs well in terms of accuracy but not necessarily in balanced accuracy. This is because it correctly classifies most benign samples but struggles with malicious ones until the threshold value is close to 0.9.
3. **High Entropy Examples**: MultimodalNN with VBI correctly classifies some high-entropy examples, as shown in Figure 7b.
4. **DeepDrebin with VBI**: Outperforms other Drebin-based models, aligning with previous results.
5. **Entropy Distribution**: Most examples tend to have small entropy, except for MultimodalNN and DeepDrebin with VBI, indicating ineffective calibration under temporal covariate shifts.

### Insight 3
Calibrated malware detectors struggle with temporal data shift, but VBI shows promise for better calibration and generalization.

### Adversarial Evasion Attacks
To quantify the predictive uncertainty under adversarial evasion attacks, we generate adversarial APKs using a surrogate DeepDrebin model. The surrogate model consists of two fully-connected layers with 160 neurons and ReLU activation, trained with Adam optimizer (learning rate 0.001, batch size 128, 150 epochs). We perturb 1,112 malicious APKs using "max" PGDs+GDKDE and Mimicry attacks, resulting in 1,100 perturbed APKs for each attack.

**Table 3** summarizes the results:
1. **PGDs+GDKDE Attack**:
   - **DeepDrebin and MultimodalNN**: Rendered useless, regardless of calibration.
   - **DeepDroid**: Robust against this attack due to its focus on opcodes.
2. **Mimicry Attack**:
   - **VBI**: Improves DeepDrebin and MultimodalNN, achieving the best accuracy and lowest calibration error.
   - **Weighted Ensemble**: Produces the worst results.
   - **DeepDroid and Droidetec**: Show different behavior, possibly due to their sensitivity to the Mimicry attack.

### Insight 4
Adversarial evasion attacks can render calibrated malware detectors and their predictive uncertainty useless, but heterogeneous feature extraction improves robustness against transfer attacks.

### Conclusion
We quantified the predictive uncertainty of four deep malware detectors with six calibration strategies. Our findings indicate that while predictive uncertainty is useful, it fails for adversarial examples. This study aims to motivate further research in quantifying the uncertainty of malware detectors, which is crucial in practical applications.

### Acknowledgments
Q. Li is supported by various grants and projects. S. Xu is supported by NSF and ARO grants.

### References
[References are listed as provided, with minor formatting adjustments for clarity.]

### Appendices
#### Appendix A: VirusShare Dataset
**Figure 8** shows the balanced accuracy on the VirusShare dataset with decision referral, exhibiting trends similar to those in Figure 4b, except for Temp scaling on DeepDrebin and MultimodalNN.

#### Appendix B: Androzoo Dataset
**Figure 9** plots the Accuracy, NLL, and BSE of malware detectors under temporal covariate shifts. These metrics are smaller than their balanced counterparts due to data imbalance in the Androzoo dataset, but they exhibit similar trends.