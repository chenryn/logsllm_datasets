page frame number. The page frame number and the page offset
form the physical address. The page offset bits are thereby identical
in both virtual and physical address. The least significant bits of
the physical address are then used by the cache controller for basic
indexing. The lowest bits are used to address a byte on a cache
line, while the subsequent bits determine the cache set in which the
address will be placed. For 1024 cache sets, 10 bits are used as cache
set index. As shown in Figure 2, these bits do not fit entirely within
the page offset (highlighted in gray). Therefore, the exact cache
set cannot be determined from the virtual address, as the most
significant index bits are unknown. This complicates the mapping
of virtual addresses to cache sets and may cause consecutive pages
to map to completely different parts of the cache, as indicated
in Figure 1. Finding eviction sets from user space is therefore a
non-trivial problem. To fully solve it, one must (a) group virtual
addresses according to cache sets, and (b) obtain the correct order
such that group 0 maps to cache set 0 and so on. We refer to this
as an ordered mapping between virtual addresses and cache sets.
In practice, one must obtain physical address bits to derive this
mapping, e.g., by gaining elevated privileges [29] or by exploiting
additional vulnerabilities [14]. Alternatively, it is possible to find an
unordered mapping that only fulfills (a). This can be achieved with
search algorithms that perform simple timing measurements and
thereby find groups of set-congruent virtual addresses. While the
algorithms do not reveal which group corresponds to which cache
Session 3B: Learning and AuthenticationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand216Algorithm 1 Finding eviction sets.
1: T = {}
2: told = 0
3: for i from 1 to n do
add (T , i)
4:
tT = access (T , r )
5:
if (tT − told ) > τjump then
6:
E ← {}
7:
for all p in T do
(cid:17)
(cid:16)
8:
tp = access (T \{p}, r )
9:
tT − tp
if
10:
add (E, p)
11:
12:
13:
14:
15:
16:
17:
18:
19:
20: end for
end if
end for
report (E)
remove (T , E)
told = access (T , r )
told = tT
end if
> τjump then
else
set, they can be run entirely from user space. In literature, the study
by Vila et al. [48] investigates this type of search algorithms. The
authors give a comprehensive overview of previous approaches, but
limit their evaluation to Intel processors. We discuss approaches
relevant to this work in the following paragraph and refer the
interested reader to this study for further information.
Lipp et al. [29] compile eviction sets from physical addresses
which they obtain from the pagemap file that is present on many
Linux systems. After their work was published, Android restricted
the access to pagemap entries from user space.1 Irazoqui et al. [24]
and Gruss et al. [15] rely on huge pages, i.e. pages with typical sizes
of > 1 MiB. Huge pages increase the page offset and thereby reveal
the missing bits that determine the cache set. Oren et al. [36] and
Bosman et al. [4] rely on special page allocation mechanisms in
web browsers and operating systems that simplify the eviction set
search. Genkin et al. [9] build eviction sets from sandboxed code
within a web browser, while only relying on virtual addresses. Yet,
they still require a precise and low-noise timing source to distin-
guish cache hit and miss. In contrast to these previous works, we
propose a search algorithm that neither relies on physical addresses
(whether obtained from pagemap, huge pages, or elsewhere), nor
on certain features of memory allocators, nor on a precise timing
source. Our approach for finding eviction sets is purely based on
virtual addresses and robust against imprecise and noisy timing
sources. In addition, we found our approach to be resilient against
the random line replacement policy implemented in many ARM
application processors.
Algorithm 1 outlines our approach for finding eviction sets. Prior
to execution, we assume that n memory pages have been requested
and are available as a memory pool. Note that we do not pose any
requirements on the memory pages, thus, our algorithm works with
any page size, including, but not limited to, 4 KiB. Since we want to
1https://source.android.com/security/bulletin/2016-03-01
4
(a) Average page access time, tT , for an increasing number of pages in T .
(b) Average page access time, tp , used to filter an eviction set from T .
Figure 3: Plots of (a) tT and (b) tp, as used in Algorithm 1.
evict the entire LLC, we need to choose n such that the requested
memory area is large enough to fill it. In our experiments, we
request a memory area that is twice as large as the LLC. This turned
out to be sufficient for deriving all eviction sets. Algorithm 1 iterates
over the allocated memory pages in sequential order. Each unused
page is first added to a temporary eviction set T (line 4). Next, the
first byte of each page in T is accessed and the average time tT
of this access cycle is measured. The parameter r determines how
often the access cycle is repeated. In each cycle, all pages in T are
accessed once. The overall timing is then divided by r to obtain the
average. This is useful to account for imprecise timing sources and
different replacement policies. A detailed discussion of r is given
later in this section. The access time tT is then compared with the
time from the previous loop cycle (line 6), where T was one page
smaller. If the time difference is higher than a threshold τjump, then
there is a systematic contention in a cache set. In other words, the
pages in T entirely fill one cache set and cause a line replacement
in the process. This is illustrated in Figure 3(a), which shows the
average access time tT over an increasing number of pages in T .
As long as no set contention occurs, the average timings increase
steadily. Once a contention happens, the timing peaks. Each peak
in the plot indicates that one cache set is completely filled.
After a set contention is detected, Algorithm 1 iterates over all
pages in T , temporarily excludes one of them from T , accesses this
reduced set, and stores the average access time in tp (line 9). If the
time difference between tT and tp is again bigger than τjump, then
the excluded page p belongs to the eviction set. This is illustrated in
Figure 3(b), which shows the average access time tp for all candidate
pages p in T . As soon as a candidate is part of the eviction set, the
systematic set contention vanishes and the access time tp drops.
Each drop in the plot therefore indicates a member of the eviction
set. Those pages are then added to the final eviction set E, which
is reported on line 14. The entries in E are subsequently removed
from T , before the outer loop continues to add unused pages to
0100200300400Page Number025005000Access Time (ns)050100150200Page Number380044005000Access Time (ns)Session 3B: Learning and AuthenticationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand217f = pop (B)
for s in B do
tE = access
if
Algorithm 2 Removing duplicates.
1: F = {}
2: B = {1..m}
3: while notempty (B) do
4:
(cid:17)
5:
6:
tE > τjump
7:
remove (B, s)
8:
9:
10:
11:
12: end while
end if
end for
add (F , f )
(cid:16)
(cid:16)(cid:102)
onepaдe
then
(cid:17)
, Es
(cid:103)
(cid:17)
, r
(cid:16)Ef
T . Once the outer loop reaches n, the reported eviction sets are
expanded. This procedure is outlined in the following paragraph.
Eviction Set Expansion and Duplicates. Each of the m eviction
sets reported by Algorithm 1 contains a list of memory pages. Since
one page fits more than one cache line, we can derive multiple
evictions sets from one E. This is indicated in Figure 1. With 4 KiB
pages and 64-byte cache lines, there are 64 lines on one page. As
memory pages are contiguous physical memory, we know that
those 64 lines belong to 64 consecutive cache sets. Hence, we can
derive 64 adjacent eviction sets from one E (the first being E itself)
by simply adding multiples of 64 to the start address of the pages.
Depending on how large we chose n, it can happen that Algorithm 1
reports more than one eviction set for each cache set. We therefore
need to check all reported eviction sets for duplicates and remove
them. This procedure is outlined in Algorithm 2. It starts by storing
the indices of all m reported eviction sets from Algorithm 1 in the
list B for bookkeeping (line 2). As long as B is not empty, the first
index is removed and assigned to f (line 4). The algorithm then
iterates over all remaining indices s and accesses the corresponding
eviction sets Ef and Es. In particular, one page in Ef and all pages
in Es are accessed consecutively, and the whole process is repeated
r times. If the average timing tE of these access cycles is larger than
a threshold τjump, then Ef and Es map to the same cache set. If
this happens, the affected index s is removed from B (line 8), and
the iteration continues. After all eviction sets have been tested, the
index in f is added to the final list F (line 11). With each loop, B is
shrinking as duplicate eviction sets are removed. Once B is empty,
F contains the list of unique eviction set indices.
Timer Precision and Noise. Both algorithms 1 and 2 are designed
to compensate imprecise and noisy timing sources. Although previ-
ous works [9, 40] suggest that accurate timers can be crafted even in
environments that restrict access to high-precision timing sources,
this engineering effort can be saved here. We believe this adds to
the practicality of our attack. The precision and noise compensation
in our algorithms is done by tuning the parameter r, as well as the
threshold τjump. r defines the number of access cycles, i.e., how
often a selection of memory pages is accessed. τjump defines how
the timings of these access cycles are evaluated. In Algorithm 1, the
accesses on line 5 will typically cause cache hits until the gathered
pages trigger a systematic set contention. The difference between
tT and told will therefore be in the order of tmiss, where tmiss
5
is the duration of a cache miss. Similarly, the accesses on line 9
will cause cache hits, if the candidate page p is part of the eviction
set. In this case, the difference between tT and tp will again be
around tmiss. Therefore, τjump can initially be set slightly smaller
than tmiss. Adjustments can be made subsequently based on ex-
perimental data. The choice of r depends on the precision of the
timer and the measurement quality. In general, r should be set
such that r · tmiss is larger than the precision of the timer. It can
be increased further, if high levels of noise are encountered, e.g.,
due to high system load. The choices for τjump and r also hold
for Algorithm 2, where the accesses on line 6 will typically cause
cache hits until Ef and Es are duplicates. When this happens, a
systematic set contention will occur, as the chosen page from Ef
will be evicted by Es. In our experiments, we set r between 900
and 1000, and τjump to 500. The timer available on our test device,
a Nexus 5X, provides a precision of 52 ns. This corresponds to ap-
proximately 100 clock cycles. In many related attacks (e.g. [49]),
where timers typically have clock cycle accuracy, this rather low
resolution would already introduce difficulties. In our approach, we
simply tune r to compensate the low resolution.
Line Replacement Policies. We can also use r to compensate the
effects of replacement policies. This is because the parameter r
causes repetitive accesses to cache lines, which signals the cache
controller that the accessed lines are of heightened interest and
should not be replaced. For least-recently-used (LRU) policies, this
is obviously beneficial, as unrelated cache activity will less likely
interfere with the eviction set finding. But also random replacement
policies benefit, because averaging over r access cycles attenuates
the effect of unintended line replacements that happen due to ran-
dom line selection. Our experiments outlined in Section 4 show
that the choice of r as stated above is sufficient to compensate the
effects of the random line replacement found on our test device.
Implementation and Limitations. Only few requirements have
to be satisfied to find eviction sets with our approach. Memory
must be allocated and accessed, and the accesses must be timed.
Memory pages can be of arbitrary size and the timing source can
be coarse-grained. This allows our algorithm to be implemented in
user space and, thus, in a plethora of environments beyond mobile
devices, e.g., desktop computers and cloud servers. Even sandboxes
and virtual machines are typically no obstacle, enabling remote
attacks, e.g., from JavaScript. The limitation of our approach is that
the exact mapping of eviction sets to cache sets remains unknown.
However, this is not a deficiency but a direct consequence of not
knowing physical addresses. This choice makes our approach more
practical and, thanks to the application of machine learning, still
allows successful inference attacks.
Performance. We evaluated algorithms 1 and 2 on our test device
with the parameters stated previously. The targeted last-level cache
is 16-way set-associative and contains 1024 cache sets. It imple-
ments a random replacement policy and features 64-byte cache
lines. Requested memory pages have a standard size of 4 KiB. Based
on 1000 evaluation runs, algorithms 1 and 2 successfully yield all
eviction sets with an average runtime of 20 seconds. Note that dur-
ing our inference attack, eviction sets must be found only once and
remain unchanged until the malicious app is restarted.
Session 3B: Learning and AuthenticationAsiaCCS ’19, July 9–12, 2019, Auckland, New Zealand2183.3 Post-processing and Feature Vectors
With the eviction sets obtained from algorithms 1 and 2, the last-
level cache can be profiled in a traditional Prime+Probe [45] manner.
This is done by filling each cache set with the corresponding evic-
tion set (prime), before re-filling it immediately afterwards (probe).
High levels of activity in the cache set will increase the probing
time, whereas low levels will keep it low. Each probing time consti-
tutes a measurement sample of the cache set. In our experiments, we
measure nT samples per cache set. The overall activity profile of the
last-level cache, in short LLC profile, consists of the samples of all
cache sets. Before these samples are classified by the machine learn-
ing algorithms, they are post-processed and converted to feature
vectors. The following list outlines the post-processing steps.
(1) Elimination of timing outliers with a threshold τO . All out-
liers are replaced with the sample median. In our experi-
ments, τO is set to 5 µs .
(2) Conversion of timing samples to binary representation. A
threshold τH decides whether a sample value is low or high.
In our experiments, τH is set to 750 ns.
(3) Sample compression by grouping high samples. Bursts of
consecutive high samples are reduced to a single high value.
The removal of outliers reduces noise in the measurements. The
binary representation simplifies interfacing with the machine learn-
ing algorithms and distills cache activity to two categories: high and
low. Sample compression reduces data complexity while keeping
the essential information of whether there was high or low activity.
It also alleviates the effect of random replacement policies, as it
compensates self-eviction during the probe step. From the post-
processed measurement samples we derive three different feature
vectors that are outlined in the following paragraphs.
Unordered Feature Vector. This feature vector is called unordered,
because the exact mapping of eviction sets to cache sets is unknown,
as explained in Section 3.2. This means that it is unclear which
region of the cache a given sample stems from. However, it is
possible to determine which region of the memory page a sample
belongs to, because the addresses in a given eviction set share
a common page offset. We leverage this observation to further
compress the feature vector and reduce training complexity. In
particular, we sum up the high samples of all cache sets that belong
to the same page offset. With 4 KiB pages and 64-byte cache lines
there are 64 distinct page offsets. Thus, the final feature vector
contains 64 values.
FFT Feature Vector. For this feature vector, the post-processed
measurement samples are converted with a fast Fourier transforma-
tion (FFT). The purpose of the FFT is to further reduce measurement
noise, which has been pointed out by previous work [19, 36]. The
FFT turns the nT time-domain samples per cache set into nT2 fre-
quency components (excluding the DC component). The employed
sampling rate nS is derived by dividing 1 second by the duration of
one Prime+Probe cycle. We reduce the complexity of the nT2 fre-
quency components by compressing them to nF final components
with a sliding window. These final components are again summed
up over all cache sets that belong to the same page offset. Thus, the
final feature vector contains nF · 64 values.
Ordered Feature Vector. This feature vector is different from the
previous two. It is called ordered, because the exact mapping of evic-
tion sets to cache sets is assumed to be known (e.g. from pagemap
entries). This implies that additional attack steps have been per-