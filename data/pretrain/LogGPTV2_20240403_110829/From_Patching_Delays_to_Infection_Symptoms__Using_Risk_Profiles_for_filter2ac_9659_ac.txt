nity detection algorithm that ﬁnds densely overlapping,
hierarchically nested, as well as non-overlapping com-
munities. The second is DEMON (Democratic Estimate
of the Modular Organization of a Network) [10], which
discovers communities by using local properties.
Figure 3 visualizes the communities discovered from
the symptom similarity matrix corresponding to CVE-
2013-2729 from 2013/05/16 to 2013/05/26 using the
Force Atlas layout [24] provided by [5]; different col-
ors encode different communities identiﬁed by the algo-
rithm. In this example, an original graph of 8,742 nodes
was reduced to one with 1,112 nodes and 10 detected
communities.2 To convey a sense of what the notion of
community captures, we further plot the spam signals of
groups of ISPs each belonging to one of two communi-
ties in Figure 4; as can be seen, those in the same com-
munity exhibit similar temporal signals.
5.2 Measuring the strength of association
between risk and symptoms
We now verify the hypothesis stated in the introduc-
tion; that is, if a CVE is being actively exploited, then
ISPs showing similar vulnerabilities to this CVE are also
likely to exhibit similar infection symptoms, while on the
other hand if a CVE is not actively exploited, then the
similarity in vulnerabilities may not be associated with
similarity in symptoms. Toward this end, we note that
there isn’t a unique way to measure the strength of as-
sociation in these two types of similarities. One could,
for instance, try to directly compare the two similarity
matrices S j[rn,rm] and S j[wn,wm]; we shall use one ver-
2The reduction in number of nodes is due to deletion of all edges to
some nodes when all their edge weights are below a certain threshold.
Figure 3: Visualization of community structure of
malicious ISPs; each color denotes a single community.
5 Comparing Symptom Similarity to Risk
Similarity
In this section, we ﬁrst use community detection meth-
ods [10, 51] to identify the underlying communities in
the pairwise symptom similarities. We then detail our
technique for quantifying the strength of association be-
tween symptoms and risk behavior for speciﬁc CVEs.
5.1 Community detection over symptom
similarity
The set of pairwise similarity measures S j
rn,rm constitute
a similarity matrix denoted by S j[n,m], ∀n,m ∈ I where
I denotes the set of all ISPs included in the following
analysis. This matrix is equivalently represented as a
weighted (and undirected) graph, where I is the set of
vertices (each vertex being an ISP) and the pairwise sim-
ilarity S j
rn,rm is the edge weight between vertices n and m
(note each edge weight is a number between 0 and 1). A
community detection algorithm can then be run over this
graph to identify hidden structures.
The general goal of community detection is to un-
cover hidden structures in a graph; a typical example is
the identiﬁcation of clusters (e.g., social groups) that are
strongly connected (in terms of degree), whereby nodes
within the same cluster have a much higher number of
in-cluster edges than edges connecting to nodes outside
the cluster. This has been an extensive area of research
within the signal processing and machine learning com-
munity and has found diverse applications including bio-
logical systems [18, 43, 52], social networks [23, 51, 52],
inﬂuence and citations [31, 51, 52], among others.
In our context, the similarity matrix S j[n,m] induces a
weighted and fully-connected graph. The result of com-
USENIX Association
27th USENIX Security Symposium    909
(a) CVE-2014-1504 (NEIW).
(b) CVE-2014-0496 (EIW).
Figure 5: Intra- and inter-cluster risk similarity on
different types of CVEs based on community detection.
(a) Difference between intra-
and inter-cluster risk
similarity over detected
communities.
(b) Difference between intra-
and inter-cluster risk
similarity over a random
partition of ISPs.
Figure 6: Distinguishing between EIW and NEIW
CVEs.
sion of this whereby we perform row-by-row correlation
between the two matrices as one of the benchmark com-
parisons presented in Section 6.
Below we will consider a more intuitive measure. We
ﬁrst use the communities detected by symptom similar-
ity to sort pairwise risk similarity values into two distinct
groups: inter-cluster similarity and intra-cluster similar-
ity. Speciﬁcally, denote the set of clusters identiﬁed by
community detection over matrix S j[rn,rm] as C . Then if
we can ﬁnd a cluster C ∈ C such that both n,m ∈ C, then
S j
wn,wm is sorted into the intra-cluster group; otherwise
it is sorted into the inter-cluster group. This is repeated
for all pairs n,m ∈ I . Figure 5 shows the distribution
of these values within each group for two distinct CVEs,
one is known to have an exploit in the wild (detected by
Symantec 20 days post-disclosure), and the other has no
known exploits in the wild.
The difference between the two is both evident and re-
vealing: for the CVE without a known exploit, Figure 5a
shows virtually no difference between the two distribu-
tions, indicating that the risk similarity values are not dif-
ferentiated by the symptom patterns. On the other hand,
for the exploited CVE (though only known after the an-
alyzed observation time period), Figure 5b shows a very
distinct difference (p value of Kolmogorov-Smirnov test
< 0.01) between the two groups. In particular, the intra-
cluster group contains much higher risk similarity val-
ues. This suggests that high risk similarity coincides with
high symptom similarity (which is what determined the
Figure 7: Time to recorded detection (x-axis) vs the
difference measure (D j) calculated within 10 days post
disclosure (y-axis); the red curve is the mean difference
within each delay bin. Different categories of CVEs are
color-coded, with ties broken randomly when a CVE
belong to multiple categories.
community structure). Also worthy of note is the fact
that for the exploited CVE, the earliest date of exploit ob-
servation on record is 20 days post-disclosure (disclosure
on 01/15/2014, observation in the wild on 02/05/2014),
whereas this analysis is feasible within 10 days of the
disclosure (01/15-01/25/2014). This suggests that ex-
ploits occur much sooner than commonly reported, and
that early detection is possible.
We sum up the values in each group and take the dif-
ference between the intra-cluster and inter-cluster sum
and denote it by D j. This allows us to quantify the
strength of association between risk and symptoms for
any arbitrary CVE; a high D j indicates that there is a sta-
tistically signiﬁcant difference between intra-cluster and
inter-cluster risk similarities, which in turn provides ev-
idence for active exploitation. Figure 6a shows the dis-
tribution of D j over two CVE subsets: one with known
exploits (with observation dates at least 10 days post-
disclosure) and one without known exploits. We see
that for the group of exploited CVEs, the intra-cluster
risk similarities are decidedly higher, suggesting a con-
sistency with communities detected using symptoms. By
contrast, for non-exploited CVEs, there is no apprecia-
ble difference between the two groups; indeed the distri-
bution looks very similar to that obtained using random
partitions of the ISP shown as a reference in Figure 6b.
We also plot for each CVE its time to earliest detection
on record against the above similarity difference measure
in Figure 7; the curve highlights the mean of D j in each
delay bin. We observe a general downward trend in the
mean, i.e., for exploits spotted earlier their inter-cluster
and intra-cluster similarity difference is also more pro-
910    27th USENIX Security Symposium
USENIX Association
00.050.10.150.20.250.30.350.40.450.5Similarity00.20.40.60.81CDFinter similiarityintra similarity00.10.20.30.40.50.60.70.80.91Similarity00.20.40.60.81CDFinter similarityintra similarity-0.4-0.3-0.2-0.100.10.20.30.4difference00.20.40.60.81CDFEIWNEIW-0.4-0.3-0.2-0.100.10.20.30.4difference00.20.40.60.81CDFEIWNEIWaffect
attack
corruption
allow
crafted
google
free
Keyword MI Wild Keyword MI Wild
0.0045
0.0012
0.0012
0.0016
0.0016
0.0047
0.0001
0.0004
0.0006
0.0069
0.0019
0.0016
0.0012
0.0020
0.0004
0.0008
dll
function
server
remote
service
exploit
runtime
memory
xp
Table 2: The top 16 intrinsic features, and their mutual
information with both sources of ground-truth data.
nounced. This is consistent with our belief that the simi-
larity difference D j is fundamentally a sign of active ex-
ploitation, which coincides with being detected earlier;
for those detected much later on, it is more likely that
exploitation occurred later and therefore could not be ob-
served during the early days.
6 Early Detection of Exploits in the Wild
Our results in the previous section shows that the intra-
and inter-cluster risk similarity distributions as well as
the difference D j are statistically meaningful in separat-
ing one group of CVEs (exploited) from another (not ex-
ploited). This suggests that these can be used as features
in building a classiﬁer aiming at exploits detection.
6.1 Features and labels
Each CVE in our sample set is labeled as either ex-
ploited or un-exploited, which constitutes the label. As
described in Section 3.1, our ground-truth comes from
three sources, public descriptions of Symantec’s anti-
virus signature, intrusion-protection signatures and ex-
ploit data from SecurityFocus. Each CVE also comes
with a set of features. In addition to the spam/symptom
data and patching/risk data we analyzed rigorously in the
previous section, we will also use intrinsic attributes as-
sociated with each CVE extracted from NVD.
Speciﬁcally, CVE summary information offers basic
descriptions about its category, the process to exploit it,
and whether it requires remote access, etc. These are
important static features for characterizing the proper-
ties of a CVE. We apply bag of words to retrieve fea-
tures from the summaries after punctuation and stem-
ming processes.
In total we obtained 3,037 keywords
from our dataset. We then select 16 features with the
highest mutual information with labels; these are shown
in Table 2. We observe that keywords such as attack,
exploit, server, and allow, have higher mutual infor-
mation with labels of exploited, which is consistent with
common understanding of what might motivate exploits.
Below we summarize the complete set of features used
in this study (each family is given a category name),
some of which are introduced for comparison purposes
as we describe in detail next.
• [Community]:
The difference in distribution
(intra-cluster minus inter-cluster similarity) shown
in Figure 5, in the form of histograms with 20 bins.
• [Direct]: The distribution of row-by-row correla-
tion between the two similarity matrices S j[rn,rm]
and S j[wn,wm], in the form of 20-bin histograms.
• [Raw]: The two similarity matrices S j[rn,rm] and
S j[wn,rm].
• [Intrinsic]: The top 20 intrinsic features using bag
of words as shown in Table 2.
• [CVSS] CVSS [28] metrics and scores. For each
CVE, we use three metrics: AcessVecotr, Ac-
cesComplexity, and Authentication, which measure
the exploit range, required attack complexity and
the level of authentication needed for successful ex-
ploitation, respectively
We can also categorize these sets of features as graph-
based ([Community], [Direct], [Raw]) and intrinsic ([In-
trinsic], [CVSS]) features. The intrinsic features describe
what is known about a vulnerability at the time of disclo-
sure, e.g., whether it can be used to gain remote control
of the host. Intuitively, these features can affect the like-
lihood of a CVE being targeted by cyber-criminals. On
the other hand, graph-based features can detect the on-
set of active exploitation, by associating similar patching
behavior with similarity in infection patterns. Our re-
sults in the following section demonstrate that while in-
trinsic features alone are poor predictors of eventual ex-
ploitation of a CVE, combining intrinsic attributes with
graph-based features enables early and accurate detec-
tion of EIW vulnerabilities.
6.2 Detection performance
We now compare the detection performance by training
classiﬁers using different subsets of the features listed
above. In training the classiﬁers, we note there is an im-
balance between our EIW (56) and NEIW (300) classes
of CVEs. For this reason, the training and testing are
conducted using 20 rounds of random sub-sampling from
the NEIW set to match its size with the EIW set; for
each round, we apply 5-fold cross validation to split the
dataset into training and test sets. We train Random
Forests [34] for classiﬁcation, and average our results
over all 20 rounds; our results are reported below.
USENIX Association
27th USENIX Security Symposium    911
(a)
(b)
(c)
Figure 8: ROC and AUC comparison (left), precision and recall comparison (center), and comparison between
different observation windows post- and pre-disclosure (right).
When using [Community] features, we observed simi-
lar performance for BigClam and DEMON. BigClam has
linear time complexity, so for simplicity of exposition be-
low we only report our results using BigClam.
Also for comparison, we will directly use the two sim-
ilarity matrices (the [Raw] features) to train a classiﬁer.
The dimensionality of the matrix is equal to the num-
ber of valid ISPs, 3050 by 3050. This is much higher in
dimension than the number of instances we have, lead-
ing to severe overﬁtting if used directly. We thus apply
a common univariate feature selection method [37] pro-
vide by [34] to obtain K = 150 features with the high-
est values based on the chi-squared test3. Three stan-
dard machine learning methods are then used to train a
classiﬁer: SVM, Random Forest and a fully-connected
neural network with three hidden layers and 30 neurons
for each hidden layer. We observe similar performance
for all examined models, and thus we only report our
results using Random Forest classiﬁers. We depict the
ROC (Receiver Operating Characteristic) curves and re-
port the AUC (Area Under the Curve) score as perfor-
mance measures. We train and compare multiple classi-
ﬁers on different sets of features:
• “All features”: This is a classiﬁer trained with all
features using 10 days of data post-disclosure.
• “Community features”: A set of classiﬁers trained
using only [Community] and on 10 days of observa-
tional data post-disclosure (for both symptoms and
risk). Only CVEs whose known detection dates are
beyond 10 days are used for testing these classiﬁers.
• “Direct features”: Trained based on the [Direct] fea-
tures alone on 10 days of data post-disclosure.