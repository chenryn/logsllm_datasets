eration, P DT CP S produces diﬀerent search requests even
for the same query. Thus, our scheme can achieve query un-
linkability such that it makes it hard for the cloud server to
link one transformed request to another even if both contain
the same keyword.
In addition, since in the known background model, the
cloud server can deduce the statistical information by ana-
lyzing the search and path patterns for each query. Thus,
it is important to hide those information from the cloud. In
our scheme, we have extended every 2nd level node to con-
tain k diﬀerent keywords so that the cloud server randomly
selects one of the k matched nodes containing the desired
keyword. Therefore, both the search and path patterns can
be hidden from the cloud server.
848Discussion: Since OP E is a deterministic encryption, so
it is subjected to two known security vulnerabilities, namely
(i) frequency-based attack where adversaries use frequency
distributions of ciphertext and plaintext to infer their cor-
respondence and (ii) order-relations among plaintexts where
attackers can easily break the encryption via sorting of known
values of plaintexts and ciphertexts using domain knowl-
edge. However, since we have added random values in P DT CP S,
attackers simply cannot infer such information. Thus, our
design is safe against both frequency and domain attacks.
6. PERFORMANCE EVALUATION
generating the proposed index tree structure;
cation and storage costs of the training model;
We implemented PDTCPS and conduct our experiments
on a Mac Pro with an Intel Core i5 processor running at
2.6GHz and 8GB memory. The following performance met-
rics are used to evaluate our scheme (P DT CP S) and two
other proposed solutions, namely the CAM [21], and the
hyperplane decision-tree based scheme (HDBS) [10]:
• Index construction time, which is the time incurred in
• The generation time, testing time, accuracy, communi-
• The accuracy of the search results.
First, we select |(cid:101)S| categories based on the major cate-
Then, based on these |(cid:101)S| categories, we extract(cid:80)|(cid:101)S|
all these(cid:80)|(cid:101)S|
gories in Medical Health provided in the Patientslikeme web-
site [1], (e.g., Endocrine, Intestinal, Throat etc), as the 1st
level nodes of the index tree.
i=1 |Ci|
unique disease keywords, e.g., Endocrine includes all dis-
eases which aﬀect the endocrine system such as diabetes,
hypothyroidism, hyperthyroidism and so on. Next, we map
i=1 |Ci| distinct keywords into their appropriate
categories and build the encrypted index tree, where each
leaf node represents k disease keywords. We also set k=2,
the length of the Bloom ﬁlter, L, to 64bytes, and use h=2
hash functions to insert keywords and their associated fuzzy
keyword sets to a Bloom ﬁlter in our P DT CP S scheme.
6.1 Construction and Communication
Costs For Index Tree
The index construction process contains two major steps:
• TA generates sets of encrypted information including the
encrypted category keywords, Bloom ﬁlters, and the number
of children that under each category node. Then, it sends
these Bloom ﬁlters to the public cloud.
• After receiving the information above, the public cloud
stores all these information in the index tree.
This index construction cost is only a one-time compu-
tation cost. Since the encrypted index tree contains |(cid:101)S|
category nodes and (cid:80)|(cid:101)S|
needs to generate |(cid:101)S| encrypted category keywords and |(cid:101)S|+
i=1 |Ci| second level nodes, the TA
(cid:80)|(cid:101)S|
i=1 |Ci| Bloom ﬁlters(BFs). Fig 8(a) shows the generation
cost for a L-bit Bloom ﬁlter and from the results we can
|(cid:101)S|+(cid:80)|(cid:101)S|
see the generation cost increases linearly with the number
of inserted keywords. In addition, it needs to send all these
i=1 |Ci| Bloom ﬁlters to the public cloud to be stored
in the encrypted index tree. Since the results in our system
show a linear relationship between the time and the number
of disease keywords, so the realistic overhead of our system
will increase linearly according to the number of disease key-
words. For example, base on the Dewey Decimal system,
which is a library classiﬁcation system, we can further clus-
ter all the existing 30,000 human diseases into almost 60 cat-
egories. Assuming |(cid:101)S|=60 and each top-level node has 500
child nodes, then the total computational cost can be com-
puted as follows: it takes 0.39 ms to insert 60 fuzzy keywords
into the BF of a child node and 0.39ms*500*31/60=101 ms
to insert 500*31 fuzzy keywords into the BF of each category
node. Thus, the overall index construction time for 30,000
diseases is 18 sec. Furthermore, to ensure the collision rate
of BF at each category node to 1%, we need to use a BF
of length 305 Kbytes. In addition, each child node contains
2 disease keywords where the average keyword length is 15.
Thus, we need a 1.2 Kbytes Bloom ﬁlter for each child node
to ensure a 1% collision rate. Therefore, the total one-time
communication cost that TA incurs to send relevant infor-
mation to the cloud for index tree construction of 30,000
diseases is (305*60+1.2*500*60)=53 Mbytes.
Figure 8: (a)Bloom Filter Generation Cost Vs Num-
ber of Keywords.
(b)Inner Product Computation
Cost Vs Bloom Filter Length .
6.2 Training Model Evaluation
1. Training Model Generation Method:
As for the training model generation cost, we ﬁrst de-
scribe the construction processes for P DT CP S, CAM and
HDBS schemes. Since we do not have access to any dataset
for complications and treatments, here we only evaluate the
performance of our model for disease diagnosis.
(i) P DT CP S: For each disease associated with a leaf
node, the cloud server generates a training model based on
the received training feature sets from all hospitals. Here,
we use the parallel SV M method described in Section 3 to
construct an aggregate SV M model. By having each hospi-
tal conducts its own data mining and sends only encrypted
support vectors makes our solution more eﬃcient and scal-
able.
(ii) CAM [21]: This scheme uses a parallel histogram-
based decision tree algorithm to generate the training model
where every iteration consists of an updating phase per-
formed simultaneously by all the hospitals and a merging
phase performed by the cloud server. At each iteration, a
new layer is constructed as follows: each hospital compresses
its share of the data using histograms and sends them to
the cloud. Then, the cloud server merges the histograms
and determines the best splits for each node in the decision
tree, thereby constructing a new layer. Next, it sends this
new layer to each hospital, and the hospitals construct his-
tograms for this new layer. Finally, the cloud server can
build the regression tree layer by layer through the itera-
tions.
(iii) Hyperplane Decision Based Scheme (HDBS) [10]:
This scheme introduces a sophisticated approach to perform
machine learning on encrypted data. All hospitals send their
encrypted datasets to the public cloud server. The public
cloud server generates an aggregated training model based
849on all these encrypted datasets using homomorphic encryp-
tion method. Next, the client generates an encrypted search
and submits to the public cloud. The public cloud traverses
the encrypted index tree as described before and sends rele-
vant answers back to the client in encrypted form. The client
then decrypts the returned response to obtain the answer.
2. Training Model Performance:
In this sub-section, we conduct Exp 1 and Exp 2 to eval-
uate the performance of the above three schemes.
(A) Exp 1: In the experiment, we use the Pima Indians
Diabetes Data Set from the UCI machine learning reposi-
tory [3], which contains 768 instances with 9 attributes of
two labeled classes. We ﬁrst select 90% of the Pima dataset
as training set, S1, and the remaining 10% as the test set
T1. Then, based on the distribution (e.g., mean or stan-
dard deviation) of each attribute, we generate two synthetic
datasets from S1 denoted as S11 and S12, where S11 contains
1384 instances, and S12 contains 4152 instances. Next, we
partition the synthetic datasets as follows (i) we partition
each synthetic dataset into m equal parts and assign each
part to one hospital. (ii) Since in the real world diﬀerent
hospital may have diﬀerent data size, so we also divide each
synthetic dataset into m unequal parts, and assign each of
them to one hospital.
(B) Exp 2: To ensure that the conclusions we draw from
Exp 1 is reliable, we also use the Breast Cancer Wisconsin
(Original) Data Set, which contains 699 instances with 10
attributes and two class labels, to conduct Exp 2. The same
method used in Exp 1 is used to generate the dataset for
each hospital.
After data generation, the hospitals then extract the train-
ing features from their assigned datasets and encrypt them
using the OP E algorithm, where the encryption complexity
is largely based on the bit length of each feature. For ex-
ample, our experiments show that using only the 1st 10 bits
of the encrypted value produce similar prediction accuracy
in disease prediction. The OP E algorithm takes 7.1ms to
encrypt a 10-bit length feature. Thus, we only use the ﬁrst
10 bits of the encryption value for all our experiments to
reduce encryption time without aﬀecting accuracy.
(C) Performance Evaluation: Tables I and II show the
evaluation results for all the above three schemes. Note that
(i) the reported storage cost is the cost of storing one train-
ing model for a particular disease, and (ii) no HDBS result
is reported for the diabetic dataset because we have no ac-
cess to their codes, and they did not have published results
using that dataset. One can see that the training time for
our scheme (P DT CP S) is much smaller than the CAM and
HDBS schemes described in [21, 10]. The CAM scheme is
ineﬃcient since it needs multiple interactions between hos-
pitals and cloud server to generate the aggregated decision-
tree, which greatly increases the training cost. HDBS uses
the aggregated dataset for SVM training while P DT CP S
uses parallel SVM for training, hence HDBS incurs more
training time than P DT CP S.
Tables I and II also show that our training model gener-
ation process incurs smaller communication cost than the
CAM and HDBS schemes.P DT CP S incurs the least cost
because the hospitals only need to send the encrypted train-
ing features instead of all the instances to the cloud server.
Whereas in the CAM scheme, the communication cost is
largely due to the histograms that are sent by the hospitals.
Meanwhile, in order to transform the ciphertext from one
encryption form into another, the HDBS scheme requires
multiple interactions between a client and server, which in-
curs much communication cost. The tables also show that
by using SVM rather than decision tree, P DT CP S achieves
higher accuracy than CAM .
In addition, from Tables I and II, we can see that the
CAM scheme incurs less test time than P DT CP S when the
dataset size is small. This is expected because the test time
for the CAM scheme is largely based on the height of the
decision tree. Thus, when the dataset is small, the height of
the decision tree is also small, which leads to low test evalua-
tion cost. Whereas in P DT CP S, the number of instances in
the dataset has little impact on the test evaluation cost since
it only depends on the number of attributes. P DT CP S only
incurs about 0.035Kbytes for Exp1 and 0.04Kbytes for Exp2
to store a training model. However, to increase the eﬃciency
for future training model updates, we may also store the en-
crypted training feature sets. The reported storage cost for
P DT CP S in Tables 1&2 shows the storage cost incurred
when such feature sets are also stored.
6.3 Search Evaluation
In this sub-section, we evaluate the search performance of
P DT CP S.
1. Search Over Encrypted Index: The search oper-
ation executed at the cloud server side consists of the in-
ner product calculation for the nodes contained in the index
tree.
If a node contains the keyword(s) in the query, the
corresponding bits in both Bloom ﬁlters will be 1 thus the
inner product will return a high value. Figure 8 (b) shows
that the inner-product computation time grows linearly with
the length of the Bloom ﬁlter. This is intuitive because the
cloud server needs to go over all the bits in Bloom ﬁlters
before computing the ﬁnal inner product values. Assuming
that there are |(cid:101)S| = 60 categories, and each category has
500 diseases, then on the average, a query without an en-
crypted category keyword needs to search through 30 top
level category nodes and 250 2nd-level nodes, then the aver-
age search time will be (30*1.7+250*0.0067)=53s since each
inner product computation takes 6.7 ms with L = 1.2Kbytes
and 1.7s with L = 305Kbytes. However, the search time
is only about (250*0.0067)=1.675s with an encrypted cat-
egory keyword included in the query. The search time for
queries without category keywords can be reduced by us-
ing the bed-tree structure [12] to create more hierarchy for
category keywords so that fewer category nodes need to be
searched.
2. Search Accuracy: In our experiment, we adopt the
widely used performance metric, namely false positive, de-
noted as F P , to measure the search result accuracy. The
false positive rate of a L-bit Bloom ﬁlter with h hash func-
tions can be computed as (1− 1
)nh, where n is the number
L
of keywords inserted into that Bloom ﬁlter. The number of
the inserted keywords in a Bloom ﬁlter for a disease keyword
can be computed as n = 2 ∗ li + 1, where li is the number of
characters of that disease keyword wi.
Figure 9(a) shows how the false positive rate of our scheme
varies as the number of inserted keywords changes when
L = 1.2Kbytes. One observation is that the false positive is
very low when li is small, i.e. 0.6% at li = 15 which is the
average character length of our disease keywords.
Figure 9 (b) shows the performance of our scheme when
the length of a Bloom ﬁlter is varied. Although large Bloom
850Table 1: Training Model Evaluation for Exp1
Scheme
Number
of At-
tributes
Number
of
In-
stances
Equal
Data
Size
Number
of Leaf
Nodes
Training
Time
Communication
Cost
Testing
Time
Accuracy Storage
P DT CP S
CAM
Scheme
9
9
1384
4152
1384
4152
Yes
No
Yes
No
Yes
No
Yes
No
0.05s
0.05s
0.13s
0.15s
2.0s
1.8s
7.8s
6.9s