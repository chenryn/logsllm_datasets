ment tool may be an acceptable approach for single ex-
periments, still allowing reliance on results, when a com-
plete estimation of the uncertainty of the experiment results
is provided (assuming that intrusiveness of the monitoring
tool was already evaluated).
Resolution is usually the easiest parameter to estimate.
However, it is frequently not considered at all: the reason is
probably that it is often considered not important, at least if
compared with intrusiveness and uncertainty. As an exam-
ple on how resolution is neglected, we observe that PLATO
[24] and Neko/NekoStat [15],[16] use Java system calls to
collect timestamps: this way the resolution of the system is
the granularity of the Java clock used, usually greater than
granularity of system clock.
Finally, let us consider repeatability. The difﬁculty in
reaching a satisfactory level for repeatability has been taken
into account in the experiments on computer systems de-
scribed in [22], even if the word repeatability is not explic-
itly used. The authors show consciousness that, due to the
aforementioned limits on accurate timestamping, many ex-
ecutions of the same run will probably not bring exactly
the same results, because the event (i.e. the injected fault)
may not be signaled at the exact time instants when it is
intended to occur. This explains why a second execution
of the same run does not necessarily recreate a catastrophic
incident that can, for instance, occur in the ﬁrst execution.
Among the surveyed works, the problem of creating repeat-
able experiments is discussed also in XCEPTION [7] and
MESSALINE [11].
In XCEPTION it is highlighted that good results are
achieved in experiments performed by using the spatial
method for fault triggering (a spatially-deﬁned fault is in-
jected when the program accesses a speciﬁed memory ad-
dress, either for data load/store or instruction fetch, [7]), not
in the temporal trigger methods, due to execution time un-
certainties. This is an obvious limit, common to all tools.
In MESSALINE it is observed that in distributed sys-
tem it is really difﬁcult to perform repeatable experiments.
Moreover, the type of the architecture has usually a major
impact on the difﬁculty to set up a reliable testbed and on
the repeatability of the experiments.
To complete the review some works based on Java are
brieﬂy considered; more precisely the Neko/NekoStat tool
for prototyping distributed algorithms in a Java framework,
the Java tool GOOFI, and the Java implementation of the
PLATO total ordering protocol. Java is actually limited by
its inability to predictably control the temporal execution of
applications due fundamentally to unpredictable latencies
introduced by automatic memory management (the garbage
collector). Because of this peculiarity, measurement tools
written in Java usually have to deal with particularly rele-
vant intrusiveness problems. Still related to Java, in [21] a
comparison of the dependability of different real-time Java
virtual machines in the spacecraft software context is made.
Some case studies are performed to evaluate dependabil-
ity of COTS components. The authors show awareness of
the problems encountered in Java and they use a Real-Time
Java [25], which surely ﬁts better in real-time context. How-
ever, although the great interest shown in precise times-
tamping, no information about measurement properties is
provided.
Finally, in [20] the behavior of a real-time system run-
ning applications under operating systems that are subject
to soft-errors is studied. Although errors due to real-time
problems are recognized, no estimation about the quality
and trustability of the presented results is shown.
A further remark concerns comparison among measure-
ment results provided by different tools or experiments. In
the surveyed tools, result comparison is never dealt with in
terms of compatibility, as introduced in Section 2. Actually
while expressing measurement results as intervals of values
is the practice sometimes followed in simulation studies, it
is not as common in experimental dependability evaluations
(with few exceptions - DBENCH). Comparison of results
carried out in terms of compatibility can, in fact, be carried
out only after evaluating uncertainty.
To summarize, the main ﬁndings of this brief survey
on tools and experiments developed to assess dependability
properties show that some consciousness about the metrol-
ogy properties described in this paper is shown, but the ap-
proaches are quite intuitive, and usually quite incomplete
as well. In particular, while there is a diffused conscious-
ness about intrusiveness, there is rarely a real effort to try
to estimate uncertainty and to determine solid bounds on
the reliability and trustability of the measures collected with
the tools. In the experiments, less attention is paid to these
themes. This does not mean that experiments are badly de-
signed, nor that the measurement systems used for the ex-
periments are not properly constructed, but more explana-
tions about the measurement system should have been pro-
vided in order to allow to appreciate or understand what
level of uncertainty may be associated to the obtained mea-
sures.
5 Conclusion
This paper has discussed the importance of approach-
ing quantitative evaluation, based on measurements, of de-
pendability attributes and metrics of computer systems and
infrastructures from a metrological point of view. The ba-
sic observation that has stimulated this work is that exper-
imental quantitative evaluations of dependability attributes
are measurements and the tools used for obtaining them are
measuring instruments. However, as it appears from our
investigations of a good sample of the available relevant lit-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:37:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007erature, the former are not recognized as measurements and
consequently their results are not qualiﬁed as they should,
and the latter are not characterized as it is usually done
for measurements tools, i.e.
their fundamental metrologi-
cal properties are not investigated.
Besides the pure scientiﬁc interest of applying principles
and results achieved during times by metrology, the appli-
cation of rigorous metrological methodologies would rep-
resent an important step forward for improving the scien-
tiﬁc rigor and trustworthiness of the obtained measures, and
for allowing to compare dependability measures collected
by various tools and experiments according to rigorous and
recognized criteria, i.e., to perform benchmarks.
After presenting the fundamentals of measurement the-
ory, to set a proper framework, the paper has focused on the
measurement properties which can be of major interest for
dependability. It has shown that the most relevant measure-
ment properties to take into account vary from a scenario to
another (e.g. in distributed systems, resolution is often not
as critical as in centralized systems). A number of works
present in the literature have been analyzed and, as it could
be expected, quite different approaches to the evaluation of
measurement properties have been followed (when present).
Some consciousness of the metrology properties has been
found, still quite intuitive, and usually quite incomplete. In
particular, while intrusiveness is very often recognized as
a concern, a real effort to estimate uncertainty is made very
rarely. Moreover, we claim that more explanations than usu-
ally done should be given about the measurement systems
used in the experiments, as it is a fundamental pre-requisite
to estimate uncertainty and to get real conﬁdence and trust
on the dependability measures obtained.
In conclusion, we would like to single out and recom-
mend some basic guidelines, i.e. operative rules that should
be kept in mind - and when possible put into practice - when
measuring dependability attributes and properties:
• the measurand should be clearly and univocally de-
ﬁned;
• all the sources of uncertainty should be singled out and
evaluated;
• some attributes of major concern for dependability
measurements, such as intrusiveness, resolution and
repeatability should be evaluated;
• measurement uncertainty should be evaluated (accord-
ing to the GUM);
• comparison of measurement results provided by dif-
ferent tools/experiments should be made in terms of
compatibility.
Acknowledgements
This work has been partially supported by the European
Community through the projects IST-4-027513 (CRUTIAL
- Critical Utility InfrastructurAL Resilience) and IST-FP6-
STREP-26979 (HIDENETS - HIghly DEpendable ip-based
NETworks and Services).
References
[1] A. Avizienis, J.C. Laprie, B. Randell, and C. Landwehr. Ba-
sic concepts and taxonomy of dependable and secure com-
puting. IEEE TDSC, 1(1):Page(s): 11–33, 2004.
[2] Freeman J. Dyson. The inventor of modern science. Nature,
400:27, July 1999.
[3] Terry Quinn and Jean Kovalevsky. The development of mod-
ern metrology and its role today. Philosophical Transactions
of the Royal Society A: Mathematical, Physical and Engi-
neering Sciences, 363:2307–2327, September 2005.
[4] BIMP, IEC, IFCC, ISO, IUPAC, IUPAP, and OIML.
ISO
International Vocabulary of Basic and General Terms in
Metrology (VIM), second edition, 1993.
[5] BIMP, IEC, IFCC, ISO, IUPAC, IUPAP, and OIML. Guide
to the expression of uncertainty in measurement, second edi-
tion, 1995.
[6] Paulo Ver´ıssimo and Luis Rodriguez. Distributed Systems
for System Architects. Kluwer Academic Publisher, 2001.
[7] J. Carreira, H. Madeira, and J. Silva. Xception: Software
fault injection and monitoring in processor functional units.
In Pre-prints 5th Int. Working Conf. on Dependable Comput-
ing for Critical Applications (DCCA-5), pages pp.135–49,
1995.
[8] Joakim Aidemark, Jonny Vinter, Peter Folkesson, and Jo-
han Karlsson. Gooﬁ: Generic object-oriented fault injection
In DSN ’01: Proceedings of the 2001 International
tool.
Conference on Dependable Systems and Networks (formerly:
FTCS), pages 83–88, Washington, DC, USA, 2001. IEEE
Computer Society.
[9] J.-C. Fabre, F. Salles, M. Rodriguez Moreno, and J. Arlat.
Assessment of cots microkernels by fault injection. In DCCA
’99: Proceedings of the conference on Dependable Com-
puting for Critical Applications, page 25, Washington, DC,
USA, 1999. IEEE Computer Society.
[10] Manuel Rodriguez, Arnaud Albinet,
and Jean Arlat.
Mafalda-rt: A tool for dependability assessment of real-time
systems. In DSN ’02: Proceedings of the 2002 International
Conference on Dependable Systems and Networks, pages
267–272, Washington, DC, USA, 2002. IEEE Computer So-
ciety.
[11] Jean Arlat, Martine Aguera, Louis Amat, Yves Crouzet,
Jean-Charles Fabre, Jean-Claude Laprie, Eliane Martins, and
David Powell. Fault injection for dependability validation:
A methodology and some applications. IEEE Trans. Softw.
Eng., 16(2):166–182, 1990.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:37:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007[23] Christof Fetzer and Flaviu Cristian. Fortress: A system to
support fail-aware real-time applications. In IEEE Workshop
on Middleware for Distributed Real-Time Systems and Ser-
vices. San Francisco, December 1997.
[24] Mahesh Balakrishnan, Ken Birman, and Amar Phanishayee.
Plato: Predictive latency-aware total ordering. In SRDS ’06:
Proceedings of the 25th IEEE Symposium on Reliable Dis-
tributed Systems (SRDS’06), pages 175–188, Washington,
DC, USA, 2006. IEEE Computer Society.
[25] G. Bollella, B. Brosgol, J. Gosling, P. Dibble, S. Furr, and
M. Turnbull. The Real-Time Speciﬁcation for Java. Addison-
Wesley, 2000.
[12] Timothy K. Tsai and Ravishankar K. Iyer. Ftape: A fault in-
jection tool to measure fault tolerance. In AIAA, Computing
in Aerospace Conference, 10th, San Antonio, TX; UNITED
STATES, 1995.
[13] Ramesh Chandra, Ryan M. Lefever, Michel Cukier, and
William H. Sanders. Loki: A state-driven fault injector for
distributed systems. In DSN ’00: Proceedings of the 2000
International Conference on Dependable Systems and Net-
works (formerly FTCS-30 and DCCA-8), pages 237–242,
Washington, DC, USA, 2000. IEEE Computer Society.
[14] Michel Cukier, Ramesh Chandra, David Henke, Jessica Pis-
tole, and William H. Sanders. Fault injection based on a par-
tial view of the global state of a distributed system. In SRDS
’99: Proceedings of the 18th IEEE Symposium on Reliable
Distributed Systems, page 168, Washington, DC, USA, 1999.
IEEE Computer Society.
[15] Peter Urban, Andre Schiper, and Xavier Defago. Neko: A
single environment to simulate and prototype distributed al-
gorithms. In ICOIN ’01: Proceedings of the The 15th Inter-
national Conference on Information Networking, page 503,
Washington, DC, USA, 2001. IEEE Computer Society.
[16] Lorenzo Falai, Andrea Bondavalli, and Felicita Di Gian-
domenico. Quantitative evaluation of distributed algorithms
using the neko framework: The nekostat extension.
In
LADC, pages 35–51, 2005.
[17] S. Dawson, F. Jahanian, T. Mitton, and Teck-Lee Tung. Test-
ing of fault-tolerant and real-time distributed systems via
In FTCS ’96: Proceedings of the
protocol fault injection.
The Twenty-Sixth Annual International Symposium on Fault-
Tolerant Computing (FTCS ’96), page 404, Washington, DC,
USA, 1996. IEEE Computer Society.
[18] Scott Dawson and Farnam Jahanian. Probing and fault in-
jection of protocol implementations. In International Con-
ference on Distributed Computing Systems, pages 351–359,
1995.
[19] Ali Kalakech, Tahar Jarboui, Jean Arlat, Yves Crouzet, and
Karama Kanoun. Benchmarking operating system depend-
ability: Windows 2000 as a case study. In PRDC ’04: Pro-
ceedings of the 10th IEEE Paciﬁc Rim International Sym-
posium on Dependable Computing (PRDC’04), pages 261–
270, Washington, DC, USA, 2004. IEEE Computer Society.
[20] N. Ignat, B. Nicolescu, Y. Savaria, and G. Nicolescu. Soft-
error classiﬁcation and impact analysis on real-time operat-
ing systems. In DATE ’06: Proceedings of the conference on
Design, automation and test in Europe, pages 182–187, 3001
Leuven, Belgium, Belgium, 2006. European Design and Au-
tomation Association.
[21] Paolo Donzelli, Marvin Zelkowitz, Victor Basili, Dan Allard,
and Kenneth N. Meyer. Evaluating cots component depend-
ability in context. IEEE Softw., 22(4):46–53, 2005.
[22] T. K. Tsai, R. K. Iyer, and D. Jewitt. An approach to-
wards benchmarking of fault-tolerant commercial systems.
In FTCS ’96: Proceedings of the The Twenty-Sixth An-
nual International Symposium on Fault-Tolerant Computing
(FTCS ’96), page 314, Washington, DC, USA, 1996. IEEE
Computer Society.
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 05:37:28 UTC from IEEE Xplore.  Restrictions apply. 
37th Annual IEEE/IFIP International Conference on Dependable Systems and Networks (DSN'07)0-7695-2855-4/07 $20.00  © 2007