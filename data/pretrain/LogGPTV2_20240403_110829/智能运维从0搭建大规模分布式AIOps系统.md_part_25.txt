数据的实时摄入和查询。
通过索引快速过滤。
QUERIES
DATA
ENT
7
Q
由多个角色的节点组合而成，每个节点都只关心自己的工作。
图8-3Druid架构示意图
STORAGE
BATCHDATA
DATASTREAM
---
## Page 168
〇索引服务节点：索引服务是一个高可用的分布式服务，运行与索引相关的任务。索
〇代理节点：接收客户端的查询请求，并路由请求到实时节点或历史节点。代理节点通
〇协调节点：主要负责 Segment 数据文件的加载、删除，管理 Segment 数据文件副本和
〇历史节点：存储和查询历史数据。历史节点不与其他节点直接通信，而是通过与
如图8-3所示。
件。其中 Middle Manager 和 peon 组件必须在同一个节点上。索引服务架构示意图
管理 peon的 Middle Manager 组件和管理任务分配给 Midle Manager的 Overlord 组
引服务是一个主/从架构。索引服务由三个组件组成，即运行一个任务的 peon 组件、
存来存储历史节点查询的结果，提高了相同查询的效率。
个节点返回的结果，再将聚合后的结果集返回给客户端。代理节点还可以通过配置缓
过 ZooKeeper 知道每个 Segment 数据文件存在于哪个节点上。代理节点还用于聚合每
临时信息，就会去拉取 Segment 数据文件。
该历史节点的队列路径下生成关于 Segment 数据文件的临时信息。历史节点看到这些
直接与历史节点通信，它会根据规则、容量等信息指定历史节点，并在 ZooKeeper 中
取集群信息，同时还会保持与存储可用段表和规则表的数据库的连接。协调节点不会
每次运行，它都会监测集群状态，并采取相关的动作。协调节点通过 ZooKeeper 来获
平衡各历史节点的 Segment数据文件。协调节点根据配置文件中指定的参数定期运行，
件。最后，历史节点会通过 ZooKeeper 向集群宣布，它将提供该 Segment 数据文件的
根据下载到本地的 Segment 数据文件的元信息去 Deep Storage中拉取 Segment 数据文
信息，那么历史节点将从 ZooKeeper 中读取新 Segment 数据文件的元信息到本地，并
存在该 Segment 数据文件的元信息，如果本地缓存中不存在新 Segment 数据文件的元
节点注意到ZooKeeper 指定队列路径下有一个条目时，首先它会检查本地缓存中是否
ZooKeeper 保持一个长连接，实时观察指定路径下的新 Segment 数据文件信息。当历史
数据文件。
送的 Segment 数据文件的元数据。发送完成后，实时节点就会丢弃被发送的 Segment
查询服务。
Segment 数据文件发送到历史节点中。通过 ZooKeeper 监控传输和元数据库来存储被发
第8章时序数据分析框架
---
## Page 169
142
（2）Metadata Storage：用于存储数据文件的元数据，但是不存储实际的数据。可以使用
（1）ZooKeeper 集群：用于集群服务的发现和当前数据拓扑的维护。
除上面介绍的5个节点外，Druid 集群还有3个外部依赖。
2．Druid的外部依赖
O
O
智能运维：从O 搭建大规模分布式AIOps 系统
Audit Table:
Task-related Table:
Config Table：配置表，用于存储运行时的配置对象。
Rule Table:
Segments Table:
审计表，用于存储配置变化的审计历史记录。
Segment 数据文件的存储规则表。
/tasks
Segment 数据文件的元信息表。
索引服务创建并使用到的表。
peon
MiddleManager1
/mm3
/mm2
/mm1
图8-4
peon
ZooKeeper
new_task
new_task
索引服务架构示意图
Overlord
new_task
/status
new_task
peon
/new_task
hew_task_status
.
new_task_status
---
## Page 170
一个总表。每个 DataSource 都会包含下面三部分信息。
数据文件，并删除在 ZooKeeper 上创建的临时节点。
数据文件存储的历史节点，并将元数据写入在ZooKeeper上创建的一个临时节点中。
Storage,
Realtime
Realtime
（3)历史节点从ZooKeeper 的临时节点中读取元数据,去Deep Storage中拉取指定的 Segment
（2)协调节点从元数据库中获取新的 Segment 数据文件的元信息，根据规则分配该 Segment
（1）实时节点将在一定时间内收集到的数据生成一个 Segment 数据文件，并发送到 Deep
node
node
O
在 Druid 中，
4.DataSource
实时数据写入后的 Segment 传输流程如下：
Segment 传输过程如图 8-5 所示。
3.Segment传输过程
(3） Deep Storage:
 Timestamp:
，同时将该 Segment 数据文件的元信息发送到元数据库中。
push(segment)
数据是存储在每个 DataSource 中的，
write(me
Storage
Storage
存储指标的UTC 时间列，可以精确到毫秒。
Deep
Deep
tadata)
用于存储 Segment 数据文件。
Storage
Metadata
Storage
Metadata
图8-5
Segment传输过程
metadata
Coordinator
Coordinator
nodes
nodes
，而一个 DataSource 就相当于 MySQL的
pullsegment)
write(ephemeral node)
第 8章时序数据分析框架
ZooKeeper
ZooKeeper
delete(ephemeral node)
read(ephemeral node)
Historical
Historical
nodes
nodes
143
---
## Page 171
标数据提取示意图如图 8-6 所示。
中的日志，再根据一定的配置规则，从日志中提取指标信息，最后写入Druid。微博广告监控指
Graphite 来分析显然是不合适的。所以我们引入了 Druid 来代替 Graphite 作为主数据引擎。
不同场景流下的请求量分布。这样的多维多指标的组合有成百上千种，在这种场景下使用
段下的请求量分布，或者分析不同地区、不同用户标签下的请求量分布，或者分析不同地区、
8.3.3
时间范围来确认需要查询的 Segment 数据文件的，减小了查询的数据量，提高了查询的效率。
通过 segmentGranularity 参数来设置 Segment 划分的时间范围，而 Druid 查询也正是通过指定的
144
"dataSources":
Output 的配置如下：
我们在监控平台架构中使用了Flink 作为实时流数据的计算框架，来实时消费存储在Kafka
Output：指定写入的 Druid 元信息。
OFilter：日志清洗规则，实现一套类似Logstash 的过滤功能。
在微博广告监控平台中，需要对指标进行复杂的组合分析，比如分析不同地区、不同年龄
DataSource 描述的是数据的逻辑结构，而 Segment 则体现了数据的物理存储方式。Druid
5. Segment 
OInput：指定消费的 Topic 元信息。
所有的指标提取任务都是通过配置文件来设定提取规则的，配置文件分为以下三部分。
O
Metric：指标列，用来聚合计算。
Dimension：存储指标的维度列。
智能运维：从O搭建大规模分布式AlOps系统
Druid在微博广告监控平台中的应用
Kafka
图8-6微博广告监控指标数据提取示意图
Flink
任务
任务
任务
ina
---
## Page 172
"dataSchema":{
spec":
"type": "realtime",
tuningConfig":{
"granularitySpec":
"metricsSpec":[
"parser":(
"dataSource": "wb_ad_nginx",
"queryGranularity": "SECOND"
"segmentGranularity": "HOUR",
"type": "uniform",
"parseSpec":{
"fieldName": "count"
"name":"count"，
"type": "longSum",
"fieldName": "cost_time"
"name": "cost_time",
"type": "doubleSum",
"dimensionsSpec":{
"timestampSpec": {
"format":"json"，
"spatialDimensions":[]
"dimensionExclusions": [],
"dimensions": ["uri", "code",
"format":'
"xTsod"
"host",
"distributed"],
第8章时序数据分析框架
145
---
## Page 173
146
task.replicants:
task.partitions：定义分区数量。
windowPeriod：时间窗口，超出时间的数据将被丢弃。
intermediatePersistPeriod：多长时间数据临时写入磁盘一次。
maxRowslnMemory：
(3）tuningConfig-
granularitySpec：指定每个 Segment 的存储粒度和最小查询粒度。
metricsSpec：定义指标名称和指标聚合类型的列表。
parseSpec：定义数据格式、时间戳名称和维度的列表。
type：声明解析一条数据的类型。
(2）parser
dataSource:
(1） dataSchema
下面分别介绍上面配置文件中各字段的含义。
"properties":(
"task.partitions"
智能运维：从0 搭建大规模分布式 AIOps 系统
"windowPeriod": "PT20M"
"intermediatePersistPeriod": "PT10m",
"maxRowsInMemory":100000,
指定 Druid 中的数据源名称。
：定义每个分区的副本数。
一优化数据写入参数。
在数据写入磁盘前内存存储的最大行数。
："2"
---
## Page 174
比情况。
他的开源数据库，如 MySQL、Hive 等。图 8-7展示了 ClickHouse 和其他数据库之间的性能对
是一个非常年轻的开源项目，但是它的性能却优于很多商业数据库，如Vertica 等，更是秒杀其
8.4.1
8.4
呢？
有一种数据引擎既能满足数据聚合、高基数维度分析、实时查询的需求，又能保留原始的数据
Elasticsearch 的查询性能依然不容乐观。同时两种数据引擎的维护成本也是非常高的，那么有没
补充，填补 Druid 在原始数据查询和高基数维度分析方面的不足。但是在海量数据场景下，
查询和高基数维度分析上，Druid 依然无能为力。所以我们又引入了 Elasticsearch 作为 Druid 的
Relativ
Dataset size:10mln.100mln1bn.
Compare:ClickHouseVerticaVertica(3)Vertica(Xx6)InfiniDBMonetDB