Drop query to get predecessor and successor
Number of
Drop Get Pred Reply Drop the answer to ﬁnd predecssor
Lie Find Pred
Lie Predecessor
Lie Successor
Drop Msg
Delay Msg
Dup Msg
Lie Msg Src
Lie Msg Dest
Lie SetKeyRange
Lie Reply Key
Lie Reply Found
Lie Reply Get
Drop Data
Dup Parent
Lie Latency
Lie Bandwidth
Drop Parent
Drop Data
Drop Join
Dup Join
Dup Data
Drop HB
Lie GroupId HB
Lie GroupId Join
Lie about key that is in query while forwarding queries
Lie about predecessor in response while forwarding
Lie about successor candidates in response while for-
warding
Drop recursive route messages
Delay recursive route messages
Delay recursive route messages and divert second mes-
sage
Lie about the source of recursive route messages
Lie about the destination of recursive route messages
Lie about what keys are stored
Lie about the key in get reply messages
Lie about ﬁnding the value in get reply messages
Lie about the request wanting the value in get reply
messages
Drop data messages
Duplicate parent reply messages, drop data
Lie about measured latency, duplicate probe messages,
drop data
Lie about received bandwidth, duplicate probe mes-
sages, drop data
Drop parent reply messages
Drop data messages
Drop join messages
Duplicate join messages
Duplicate data messages, sending second message to
random node
Drop heartbeat message
Lie about the group identiﬁer in a heartbeat message
Lie about the group identiﬁer in a join message
Chord
Reachable Nodes
DHT
Lookup Latency
ESM
Throughput
Latency
Scribe
Throughput
Known
Attack
[58]
[58]
[58]
[58]
[45]
[45]
[14]
[14]
[14]
[14]
[14]
[14]
[14]
[14]
[14]
[14]
[14]
[53]
[53]
[53]
Table 1. Attacks found using Gatling: 41 attacks in total (17 lie, 12 drop, 6 delay, 5 duplicate, 1 divert)
No Attack
Inflation
Oscillation
Delay
Deflation
)
s
m
(
r
o
r
r
E
n
o
i
t
c
i
d
e
r
P
1014
1012
1010
108
106
104
102
 25
 15
 5
 0
 300
 600
 900  1200  1500  1800
Simulation Time (s)
No Attack
Drop Find Pred
Drop Get Pred
Drop Get Pred Reply
Lie Find Pred
Lie Predecessor
Lie Successor
e
l
b
a
h
c
a
e
R
s
e
d
o
N
#
 200
 150
 100
 50
 0
 0
 10
 20
 30
 40
 50
 60
Simulation Time (s)
Figure 8. Prediction error for
the attacks found on Vivaldi
(a) No.
found on Chord
of reachable nodes for the attacks
(b) Chord ring under Lie Predeces-
sor attack
Figure 9. Attack impact on Chord
correct ring from forming. The join protocol ﬁrst locates the
predecessor of a given address i by forwarding a FindPred
message through the Chord overlay. If a malicious node
modiﬁes the address i in the message, it effectively redi-
rects the node joining to an incorrect place in the ring, and
can cause inconsistent state in the nodes, which can lead to
a failure to properly join (Lie Find Pred). We found similar
attacks when a malicious node, during stabilization, queried
as to who are its predecessors and successors, lies and gives
incorrect information (Lie Predecessor, Lie Successor). We
show impact scores of these attacks in Fig. 9(a). The ef-
fect of Lie Predecessor on the ring can be seen visually in
Fig. 9(b), where some nodes failed to join, and others are
confused about their relationships to adjacent nodes.
5.3 Distributed Hash Table
System description. A Distributed Hash Table (DHT)
provides a scalable key-value storage service, where nodes
self-organize so that each node is responsible for storage
of a portion of the key-space. DHTs expose at least two
operations to the user, a put operation that stores a value
based on its key in the overlay and a get operation that re-
trieves key-values that are previously stored. The DHT im-
plementation used is a basic one based on the outline in the
Chord paper [51], structured as the example described in
Figure 4. When an application node requests an operation
(get or put), the storage layer routes the operation to the
responsible node using the recursive routing layer. The re-
cursive overlay routing layer forwards any message to the
destination by forwarding it hop-by-hop along the Chord
overlay links. The DHT also responds to changes in the
responsible address space by sending a SetKeyRange mes-
sage to the new owner to notify it of the keys it should now
manage.
a node issuing a get request on a key and when it actually re-
ceives the corresponding value. Formally, the impact score
is the average time spent on lookups that either completed
in the last tw or elapsed time of pending lookups.
Experimental setup. We simulated 100 nodes and each
one randomly generates 100 key-value pairs which it puts
around the DHT, thus we expect that each node stores one
of its values on every node. Each node then tries to retrieve
2 values every second, and tries to retrieve the whole set of
values 10 times. A request is timed-out if no response is re-
ceived before the next retrieval attempt. Most experiments
allow Chord 10 sec to form the overlay before beginning
to put data. It then uses 50 sec to put data, putting only 2
values every second, before beginning to lookup data. The
remaining lookups take 500 sec. We set tw to be 70 sec,
which allows Gatling to ﬁnd attacks during the Chord setup
and DHT put phase; na was set to 5.
Attacks found using lookup latency. We show lookup
latency over time for each attack in Fig. 10. As a baseline
we show DHT with no attack and ﬁnd it converges to 215
ms. We found a total of seven attacks (and several variants)
against DHT and rediscovered some attacks against Chord.
Recursive Overlay Routing Attacks. We ﬁrst run Gatling
on the recursive message routing layer that routes all DHT
messages. We begin malicious actions after 10 sec, after
the Chord ring converges. We found two attacks where de-
laying or dropping messages causes an increase in lookup
latency (Drop Msg, Delay Msg). We also found a third at-
tack where duplicating the message and diverting the sec-
ond copy to a random node causes network ﬂooding and
congestion due to malicious nodes repeatedly replicating
the same messages (Dup Msg). Finally, we found an at-
tack where in forwarding messages, an attacker provides a
false destination key for the message, causing the next hop
of the message to forward it incorrectly (Lie Msg Dest).
Impact score. For an impact score we use lookup la-
tency, which measures the amount of time passed between
Storage Attacks. We found two lying attacks. The ﬁrst
one, Lie Reply Key occurs when a node responds to a DHT
get request and it lies about the key it is responding about.
The second one, Lie Set Key Range occurs during the setup
phase of the DHT, considering a scenario where nodes start
putting data into the DHT at the beginning of the simula-
tion, before the Chord ring can stabilize. We found that at-
tackers can subvert the process of load-balancing and cause
many key-value pairs to go missing. This occurred when
an attacker notiﬁed another node of what key-value pairs it
had. The attacker lied about what keys it was responsible
for, then when another node takes over a part of that key-
range, he will not know the real values that it should store,
thus losing them.
5.4 ESM
System description. ESM [17] is a multicast system that
efﬁciently disseminates video streaming data broadcast by
a single source. ESM accomplishes this by building up a
tree, rooted at the source, where each child node forwards
on the data to its own children. Each node maintains a set
of neighbors periodically reporting their throughput and la-
tency. With this information, a node can change parents to
maintain desired performance.
Impact score. We use two scores [17]: throughput and
latency. Throughput as described in [17], is the amount of
data received over the last 5 sec, so the impact score is the
streaming rate minus the throughput, to satisfy requirements
that larger means more impact. Latency is the amount of
time it takes for data to reach each node after being initially
sent by the source, and the impact score is the average la-
tency of data in the last tw.
Experimental setup. We simulated ESM with 100
nodes and one source streaming data at 1 Mbps. As the
goal of ESM is both to form an efﬁcient tree and to stream
data to all participants, we use two different settings for the
time (i.e., 0 sec and 10 sec) when attackers start their ma-
licious actions. Thus we can ﬁnd attacks both against tree
formation and data delivery. We use a tw of 5 sec and na of
5.
Attacks found using throughput. We found four at-
tacks using throughput as an impact score. Fig. 11(a) shows
the results of how each attack affects ESM where we plot
the throughput over time. For a baseline we also have ESM
in the benign case when there is no attack, delivering aver-
age throughput near 900 kbps.
Drop Data. First we delay malicious actions until 10 sec
into the execution, to allow ESM to build a tree ﬁrst, and test
the steady state. Despite ESM using an adaptation mecha-
nism to switch to parents that give them good performance,
dropping data was an effective attack.
Dup Parent. We then examined attacks that targeted the
tree formation and adaptation process. We increased the
window size tw to 10 sec, and had attackers immediately
start trying malicious actions once the simulation started.
Gatling again added dropping data as an attack action, then
proceeded to amplify that attack with another malicious
action—duplicating messages that tell a node they are ac-
cepted as a child, sending the duplicate message to a random
node. With this ampliﬁcation, the throughput is usually be-
low 200 kbps.
Attraction attacks. In these attacks malicious nodes am-
plify dropping streaming data by lying about their perfor-
mance metrics, making them look better than what they ac-
tually are. This causes benign nodes to continually ask ma-
licious nodes to be their parents. The ﬁrst attraction attack
found is where nodes lie about their latency (Lie Latency),
setting it to zero. This causes nodes to think the attacker
is close to the source. The second attraction attack is when
malicious nodes lie about their bandwidth using scaling (Lie
Bandwidth), increasing it to appear they are receiving much
of the streaming data. To further amplify the attack the at-
tackers also duplicate probe messages, diverting the second
message to random nodes, causing the attackers to be more
well-known in the overlay, thus more likely to be asked to
be a parent. These two attacks are very effective, causing all
nodes to have a very low throughput of less than 100 kbps
when lying about latency and 300 kbps when lying about
bandwidth.
Attacks found using latency. We found one attack us-
ing latency as an impact score function. We compare in
Fig. 11(b) the latency when there is no attack with the at-
tack we found.
Drop Parent. We found an attack where malicious nodes
drop replies to parent request messages. This results in in-
creased latency due to malicious nodes gaining spots high
up in the tree over time by simply following the adaptation
protocol, and then never allowing the tree to grow beyond
them. Furthermore, as benign nodes never get a response,
the protocol dictates that they wait 1 sec to send another par-
ent request to a different node, further slowing down their
attempt to ﬁnd or change parents.
5.5 Scribe
System description. Scribe [49] is an application-level
multicast system that organizes each group into an overlay
tree to efﬁciently disseminate data. To send data, a node
sends the message toward the root, and each node forwards
it to its parent and children. Scribe is built on top of Pas-
try [48], an overlay routing protocol with similar function-
ality to Chord. Scribe trees are built based on reverse-path
forwarding combined with load balancing:
the multicast
tree is the reverse of the routes Join messages take when
routed from tree participants to the Pastry node managing
the group identiﬁer, except that nodes whose out-degree is
too high will push children down in the tree.
No Attack
Drop Msg
Delay Msg
Dup Msg
Lie Msg Dest
Lie Msg Src
Lie SetKeyRange
Lie Reply Key
)
s
m
(
y
c
n
e
t
a
L
p
u
k
o
o
L
 25000
 20000
 15000
 10000
 5000
 0
)
s
p
b
k
(
t
u
p
h
g
u
o
r
h
T
 1400
 1200
 1000
 800
 600
 400
 200
 0
No Attack
Drop Data
Dup Parent
Lie Latency
Lie Bandwidth
)
s