necessary to prevent active attacks.
The program execution in M2R can be viewed as a
directed acyclic graph (DAG), where vertices denote
trusted computation units and edges denote the ﬂow of
encrypted data blocks. M2R has 4 kinds of trusted com-
putation units or vertices in the DAG: mapT, mixT,
groupT, and reduceT. At a high-level, our integrity-
checking mechanism works by ensuring that nodes at the
jth level (by topologically sorted order) check the con-
sistency of the execution at level j−1. If they detect that
the adversary deviates or tampers with the execution or
outputs from level j− 1, then they abort the execution.
The MapReduce provisioning system is responsible
for invoking trusted computation units, and is free to de-
cide the total number of units spawned at each level j.
We do not restrict the MapReduce scheduling algorithm
to decide which tuples are processed by which reduce
unit, and their allocation to nodes in the cluster. How-
ever, we ensure that all tuples output at level i−1 are pro-
cessed at level i, and there is no duplicate. Note that this
requirement ensures that a computation in step i starts
only after outputs of previous step are passed to it, im-
plicitly synchronizes the start of the computation units at
step i. Under this constraint, it can be shown that chan-
nels C1-C2 (start-end time of each computation node)
can only allow the adversary to delay an entire step, or
distinguish the outputs of units within one step, which is
already implied by Ψ. We omit a detailed proof in this
paper. Using these facts, we can show that the malicious
adversary has no additional advantage compared to an
honest-but-curious adversary, stated formally below.
Theorem 2. The protocol M2R is private modulo-Ψ un-
der malicious adversary, assuming that the underlying
authenticated-encryption is semantically secure (conﬁ-
dentiality) and secure under chosen message attack (in-
tegrity), and Fs(·) is a pseudorandom function family.
Proof Sketch: Given a malicious adversary A that ex-
ecutes the M2R protocol, we can construct an adversary
454  24th USENIX Security Symposium 
USENIX Association
8
(cid:31)A that simulates A , but only has access to Ψ in the
following way. To simulate A , the adversary (cid:31)A needs
to ﬁll in information not present in Ψ. For the output
of a trusted unit, the simulation simply ﬁlls in random
tuples, where the number of tuples is derived from
Ψ. The timing information can likewise be ﬁlled-in.
Whenever A deviates from the protocol and feeds a
different
the simulation
expects the instance will halt and ﬁlls in the information
accordingly. Note that the input to A and the input
to a trusted instance,
input
DAG of program execution, although the encrypted
tuples are different. Suppose there is a distinguisher
constructed for the simulator (cid:31)A could have the same
that distinguishes A and (cid:31)A , let us consider the two
cases: either the two DAG’s are the same or different.
If there is a non-negligible probability that they are the
same, then we can construct a distinguisher to contradict
the security of the encryption, or Fs(·).
If there is a
non-negligible probability that they are different, we can
forge a valid authentication tag. Hence, the outputs of
A and (cid:31)A are indistinguishable.
Integrity Checks. Nearly all our integrity checks can
be distributed across the cluster, with checking of invari-
ants done locally at each trusted computation. There-
fore, our integrity checking mechanism can largely bun-
dle the integrity metadata with the original data. No
global synchronization is necessary, except for the case
of the groupT units as they consume data output by an
untrusted grouping step. The groupT checks ensure
that the ordering of the grouped tuples received by the
designated reduceT is preserved. In addition, groupT
units synchronize to ensure that each reducer processes
a distinct range of tuple-keys, and that all the tuple-keys
are processed by at least one of the reduce units.
4.3.1 Mechanisms
In the DAG corresponding to a program execution, the
MapReduce provisioning system assigns unique instance
ids. Let the vertex i at the level j has the designated
id (i, j), and the total number of units at level j be |Vj|.
When a computation instance is spawned, its designed
instance id (i, j) and the total number of units |Vj| are
passed as auxiliary input parameters by the provision-
ing system. Each vertex with id (i, j) is an operation
of type mapT, groupT, mixT or reduceT, denoted
by the function OpType(i, j). The basic mechanism
for integrity-checking consists of each vertex emitting a
tagged-block as output which can be checked by trusted
components in the next stage. Speciﬁcally, the tagged
block is 6-tuple B = (cid:29)O, LvlCnt, SrcID, DstID, DstLvl,
9
DstType (cid:28), where:
O
LvlCnt
SrcID
DstID
DstLvl
DstType
is the encrypted output tuple-set,
is the number of units at source level,
is the instance id of the source vertex,
is instance id of destination vertex or NULL
is the level of the destination vertex,
is the destination operation type.
In our design, each vertex with id (i, j) fetches the
tagged-blocks from all vertices at the previous level, de-
noted by the multiset B, and performs the following con-
sistency checks on B:
1. The LvlCnt for all b ∈ B are the same (say (cid:29)(B)).
2. The SrcID for all b ∈ B are distinct.
3. For set S = {SrcID(b) |b ∈ B}, |S| = (cid:29)(B).
4. For all b ∈ B, DstLvl(b) = j.
5. For all b ∈ B, DstID(b) = (i, j) or NULL.
6. For all b ∈ B, DstType(b) = OpType(i, j).
Conditions 1,2 and 3 ensure that tagged-blocks from
all units in the previous level are read and that they are
distinct. Thus, the adversary has not dropped or dupli-
cated any output tuple. Condition 4 ensures that the
computation nodes are ordered sequentially, that is, the
adversary cannot misroute data bypassing certain levels.
Condition 6 further checks that execution progresses in
the expected order — for instance, the map step is fol-
lowed by a mix, subsequently followed by a group step,
and so on. We explain how each vertex decides the
right or expected order independently later in this sec-
tion. Condition 5 states that if the source vertex wishes
to ﬁx the recipient id of a tagged-block, it can veriﬁably
enforce it by setting it to non-NULL value.
Each tagged-block is encrypted with standard authen-
ticated encryption, protecting the integrity of all meta-
data in it. We explain next how each trusted computation
vertex encodes the tagged-block.
Map-to-Mix DataFlow. Each mixT reads the output
metadata of all mapT. Thus, each mixT knows the to-
tal number of tuples N generated in the entire map step,
by summing up the counts of encrypted tuples received.
From this, each mixT independently determines the to-
tal number of mixers in the system as N/T . Note that
T is the pre-conﬁgured number of tuples that each mixT
can process securely without invoking disk accesses, typ-
ically a 100M of tuples. Therefore, this step is com-
pletely decentralized and requires no co-ordination be-
tween mixT units. A mapT unit invoked with id (i, j)
simply emit tagged-blocks, with the following structure:
(cid:29)·,|Vj|, (i, j), NULL, j + 1, mixT(cid:28).
Mix-to-Mix DataFlow. Each mixT re-encrypts and per-
mutes a ﬁxed number (T ) of tuples. In a κ-step cascaded
mix network, at any step s (s < κ − 1) the mixT out-
puts T /m tuples to each one of the m mixT units in the
USENIX Association  
24th USENIX Security Symposium  455
step s + 1. To ensure this, each mixT adds metadata to
its tagged-block output so that it reaches only the speci-
ﬁed mixT unit for the next stage. To do so, we use the
DstType ﬁeld, which is set to type mixTs+1 by the mixer
at step s. Thus, each mixT node knows the total num-
ber of tuples being shufﬂed N (encoded in OpType), its
step number in the cascaded mix, and the public value T .
From this each mixT can determine the correct number
of cascade steps to perform, and can abort the execution
if the adversary tries to avoid any step of the mixing.
Mix-to-Group DataFlow.
In our design, the last mix
step (i, j) writes the original tuple as (cid:31)Fs(k), (k,v,tctr)(cid:30),
where the second part of this tuple is protected with
authenticated-encryption. The value tctr is called a
tuple-counter, which makes each tuple globally distinct
in the job. Speciﬁcally, it encodes the value (i, j,ctr)
where ctr is a counter unique to the instance (i, j). The
assumption here is that all such output tuples will be
grouped by the ﬁrst component, and each group will be
forwarded to reducers with no duplicates. To ensure that
the outputs received are correctly ordered and untam-
pered, the last mixT nodes send a special tagged-block
to groupT nodes. This tagged-block contains the count
of tuples corresponding to Fs(k) generated by mixT unit
with id (i, j). With this information each groupT node
can locally check that:
• For each received group corresponding to g = Fs(k),
the count of distinct tuples (k,·,i, j,ctr) it receives
tallies with that speciﬁed in the tagged-block re-
ceived from mixT node (i, j), for all blocks in B.
Finally, groupT units need to synchronize to check if
there is any overlap between tuple-key ranges. This
requires an additional exchange of tokens between
groupT units containing the range of group keys and
tuple-counters that each unit processes.
Group-to-Reduce Dataﬂow. There is a one-to-one
mapping between groupT units and reduceT units,
where the former checks the correctness of the tuple
group before forwarding to the designated reduceT.
This communication is analogous to that between mixT
units, so we omit a detailed description for brevity.
5
Implementation
Baseline Setup. The design of M2R can be imple-
mented differently depending on the underlying archi-
tectural primitives available. For instance, we could im-
plement our solution using Intel SGX, using the mech-
anisms of VC3 to achieve our baseline. However, Intel
SGX is not yet available in shipping CPUs, therefore
we use a trusted-hypervisor approach to implement the
baseline system, which minimizes the performance over-
heads from the baseline system. We use Intel TXT to
securely boot a trusted Xen-4.4.3 hypervisor kernel, en-
suring its static boot integrity5. The inputs ands output of
map and reduce units are encrypted with AES-GCM us-
ing 256-bit keys. The original Hadoop jobs are executed
as user-level processes in ring-3, attested at launch by the
hypervisor, making an assumption that they are protected
during subsequent execution. The MapReduce jobs are
modiﬁed to call into our TCB components implemented
as x86 code, which can be compiled with SFI constraints
for additional safety. The hypervisor loads, veriﬁes and
executes the TCB components within its address space
in ring-0. The rest of Hadoop stack runs in ring 3 and
invokes the units by making hypercalls. Note that the
TCB components can be isolated as user-level processes
in the future, but this is only meaningful if the processes
are protected by stronger solutions such as Intel SGX or
other systems [12, 14, 52].
M2R TCB. Our main contributions are beyond the base-
line system. We add four new components to the TCB
of the baseline system. We have modiﬁed a stan-
dard Hadoop implementation to invoke the mixT and
groupT units before and after the grouping step. These
two components add a total 90 LoC to the platform TCB.
No changes are necessary to the original grouping algo-
rithm. Each mapT and reduceT implement the trusted
map and reduce operation — same as in the baseline sys-
tem. They are compiled together with a static utility code
which is responsible for (a) padding each tuple to a ﬁxed
size, (b) encrypting tuples with authenticated encryption,
(c) adding and verifying the metadata for tagged-blocks,
and (d) recording the instance id for each unit. Most of
these changes are fairly straightforward to implement. To
execute an application, the client encrypts and uploads
all the data to M2R nodes. The user then submits M2R
applications and ﬁnally decrypts the results.
6 Evaluation
This section describes M2R performance in a small clus-
ter under real workloads. We ported 7 data intensive jobs
from the standard benchmark to M2R, making less than
25% changes in number of lines of code (LoC) to the
original Hadoop jobs. The applications add fewer than
500 LoC into the TCB, or less than 0.16% of the entire
Hadoop software stack. M2R adds 17− 130% overhead