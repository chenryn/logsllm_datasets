single ciphertext of additively homomorphic encryption. ğ’ com-
putes(cid:72)u â† râŠ—(cid:72)wâŠ–v, where r and v are random vectors generated by
ğ’. ğ’® decrypts(cid:72)u and outputs the sum of u. Meanwhile, ğ’ outputs the
triplets for each prediction request, ğ’® only needs to transfer(cid:72)ws
sum of v. Even though ğ’® and ğ’ need to generate new dot-product
once for all predictions. Furthermore, it can pack multiple ws into
a single ciphertext if needed.
Input:
ğ’®: w âˆˆ Zn
ğ’: r âˆˆ Zn
N
N
Output:
ğ’®: a random number u âˆˆ ZN ;
ğ’: v âˆˆ ZN , s.t., u + v (mod N ) = w Â· r.
ğ’®:
(cid:72)w â† E(pks ,w)
u â†(cid:80)(D(sks ,(cid:72)u))
output u
(cid:72)w
(cid:72)u
ğ’:
$â†âˆ’ Zn
N
v
(cid:72)u â† r âŠ—(cid:72)w âŠ– v
v â†(cid:80)(v)
output v
Figure 3: Dot-product triplet generation.
Theorem 1. The protocol in Figure 3 securely implements â„±triplet
in the presence of semi-honest adversaries, if E() is semantically secure.
Proof. Our security proof follows the ideal-world/real-world
paradigm: in real-world, parties interact according to the proto-
col specification, whereas in ideal-world, parties have access to a
trusted party TTP that implements â„±triplet. The executions in both
worlds are coordinated by the environment â„°, who chooses the
inputs to the parties and plays the role of a distinguisher between
the real and ideal executions. We aim to show that the adversaryâ€™s
view in real-wold is indistinguishable to that in ideal-world.
Security against a semi-honest server. First, we prove security against
a semi-honest server by constructing an ideal-world simulator Sim
that performs as follows:
gets the result u;
(1) receives w from the environment â„°; Sim sends w to TTP and
(2) starts running ğ’® on input w, and receives(cid:72)w;
(3) randomly splits u into a vector uâ€² s.t., u =(cid:80) uâ€²;
(4) encrypts uâ€² using ğ’®â€™s public key and returns(cid:72)uâ€² to ğ’®;
(5) outputs whatever ğ’® outputs.
Next, we show that the view Sim simulates for ğ’® is indistinguishable
from the view of ğ’® interacting in the real execution. ğ’®â€™s view in
1, ...,râ€²
. This is clearly true since vi is randomly chosen.
At the end of the simulation, ğ’® outputs u â†(cid:80) u, which is the
the real execution is u = w Â· r âˆ’ v while its view in the ideal
execution is uâ€² = [râ€²
n]. So we only need to show that any
element wiri âˆ’vi (mod N ) in u is indistinguishable from a random
number râ€²
i
same as real execution. Thus, we claim that the output distribution
of â„° in real-world is computationally indistinguishable from that
in ideal-world.
Security against a semi-honest client. Next, we prove security against
a semi-honest client by constructing an ideal-world simulator Sim
that works as follows:
(1) receives r from â„°, and sends it to TTP;
(2) starts running ğ’ on input r;
(3) constructs (cid:72)wâ€² â† E(pkâ€²
(4) gives(cid:72)wâ€² to ğ’;
generated by Sim;
(5) outputs whatever ğ’ outputs.
s ,[0, ...,0]) where pkâ€²
s is randomly
ğ’â€™s view in real execution is E(pks ,w), which is computationally in-
distinguishable from its view in ideal execution i.e., E(pkâ€²
s ,[0, ...,0])
due to the semantic security of E(). Thus, the output distribution
of â„° in real-world is computationally indistinguishable from that
in ideal-world.
â–¡
5.2 Oblivious linear transformations
Recall that when ğ’ wants to request ğ’® to compute predictions for
an input X, it blinds each value of X using a random value r from a
dot-product triplet generated earlier: xğ’® := x âˆ’ r (mod N ). Then,
ğ’ sets Xğ’ = R, and sends Xğ’® to ğ’®. The security of the dot-product
generation protocol guarantees that ğ’® knows nothing about the r
values. Consequently, ğ’® cannot get any information about X from
Xğ’® if all rs are randomly chosen by ğ’ from ZN .
Upon receiving Xğ’®, ğ’® will input it to the first layer which is typ-
ically a linear transformation layer. As we discussed in Section 2.1,
all linear transformations can be turned into matrix multiplica-
tions/additions: Y = W Â· X + B. Figure 4 shows the oblivious linear
transformation protocol. For each row of W and each column of
Xğ’, ğ’® and ğ’ jointly generate a dot-product triplet: u +v (mod N ) =
wÂ·xğ’. Since Xğ’ is independent of X, they can generate such triplets
in a precomputation phase. Next, ğ’® calculates Yğ’® := WÂ·Xğ’® +B+U,
and meanwhile ğ’ sets Yğ’ := V. Consequently, each element of Yğ’®
and Yğ’ satisfy:
yğ’® + yğ’ = w Â· xğ’® + b + u + v
= w1(x1 âˆ’ xğ’
1 )+, ..., +wl (xl âˆ’ xğ’
= (w1x1+, ..., +wl xl + b) âˆ’ (w1xğ’
= y
l ) + b + u + v
1 +, ..., +wl xğ’
l ) + u + v
Due to the fact that âŸ¨U,VâŸ© are securely generated by â„±triplet, the
outputs of this layer (which are the inputs to the next layer) are also
randomly shared between ğ’® and ğ’, i.e., Yğ’ = V and Yğ’® = Y âˆ’ V
can be used as inputs for the next layer directly.
It is clear that the view of both ğ’® and ğ’ are identical to their
views under the dot-product triplet generation protocol. Therefore,
the oblivious linear transformation protocol is secure if â„±triplet is
securely implemented.
Session C3:  Machine Learning PrivacyCCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA623N , B âˆˆ ZmÃ—n
N
N , Xğ’® âˆˆ ZlÃ—n
Input:
ğ’®: W âˆˆ ZmÃ—l
ğ’: Xğ’ âˆˆ ZlÃ—n
N
Output:
ğ’®: A random matrix Yğ’®
ğ’: Yğ’ s.t., Yğ’ + Yğ’® = W Â· (Xğ’ + Xğ’® ) + B
ğ’®:
ğ’:
precomputation
for i = 1 to m
for j = 1 to n
(ui,j ,vi,j ) â† â„±triplet(wi ,xğ’
j )
end
end
Yğ’ := V
output Yğ’
Yğ’® := W Â· Xğ’® + B + U
output Yğ’®
Figure 4: Oblivious linear transformation.
A linear transformation layer can also follow an activation layer
or a pooling layer. So, we need to design the oblivious activa-
tion/pooling operations in a way that their outputs can be the
inputs to linear transformations: Xğ’® and Xğ’ s.t. Xğ’® + Xğ’ = X and
Xğ’ has been used to generate the dot-product triplets for the next
layer. See the following sections.
5.3 Oblivious activation functions
In this section, we introduce the oblivious activation function which
receives yğ’ from ğ’ and yğ’® from ğ’®, and outputs xğ’ to ğ’ and xğ’® :=
f (yğ’® + yğ’ ) âˆ’ xğ’ to ğ’®, where xğ’ is a random number generated by
ğ’. Note that if the next layer is a linear transformation layer, xğ’
should be the random value that has been used by ğ’ to generate
a dot-product triplet in the precomputation phase. On the other
hand, if the next layer is a pooling layer, xğ’ can be generated on
demand.
5.3.1 Oblivious piecewise linear activation functions. Piecewise
linear activation functions are widely used in image classifications
due to their outstanding performance in training phase as demon-
strated by Krizhevsky et al. [37]. We take ReLU as an example to
illustrate how to transform piecewise linear functions into their
oblivious forms. Recall that ReLU is f (y) = max (0,y), where y is
additively shared between ğ’® and ğ’. An oblivious ReLU protocol
will reconstruct y and return max (0,y)âˆ’xğ’ to ğ’®. This is equivalent
to the ideal functionality â„±ReLU in Figure 5. Actually, we compare
y with N2 : y > N2 implies y is negative (recall that absolute values
of all intermediate results will not exceed âŒŠN /2âŒ‹).
â„±ReLU can be trivially implemented by a 2PC protocol. Specifi-
cally, we use a garbled circuit to reconstruct y and calculate b :=
compare (y,0) to determine whether y â‰¥ 0 or not. If y â‰¥ 0, it returns
y, otherwise, it returns 0. This is achieved by multiplying y with
b. The only operations we need for oblivious ReLU are +,âˆ’,Â· and
Input:
Output:
â€¢ ğ’®: yğ’® âˆˆ ZN ;
â€¢ ğ’: yğ’,r âˆˆ ZN .
â€¢ ğ’®: xğ’® := compare (y,0) Â· y âˆ’ r (mod N ) where y = yğ’® +
yğ’ (mod N );
â€¢ ğ’: xğ’ := r.
Figure 5: The ideal functionality â„±ReLU.
compare, all of which are supported by the 2PC library [21] we used.
So both implementation and security argument are straightforward.
Oblivious leaky ReLU can be constructed in the same way as
oblivious ReLU, except that ğ’® gets:
xğ’® := compare (y,0) Â· a Â· y + (1 âˆ’ compare (y,0)) Â· y âˆ’ r (mod N ).
5.3.2 Oblivious smooth activation functions. Unlike piecewise
linear functions, it is non-trivial to make smooth functions oblivi-
1
ous. For example, in the sigmoid function f (y) =
1+eâˆ’y , both ey
and division are expensive to be computed by 2PC protocols [49].
Furthermore, it is difficult to keep track of the floating point value
of ey, especially when y is blinded. It is well-known that such
functions can be approximated locally by high-degree polynomials,
but oblivious protocols can only handle low-degree approximation
polynomials efficiently. To this end, we adapt an approximation
method that can be efficiently computed by an oblivious protocol
and incurs negligible accuracy loss.
Approximation of smooth functions. A smooth function f () can be
approximated by a set of piecewise continuous polynomials, i.e.,
splines [22]. The idea is to split f () into several intervals, in each of
which, a polynomial is used to to approximate f (). The polynomials
are chosen such that the overall goodness of fit is maximized. The
approximation method is detailed in the following steps:
(1) Set the approximation range [Î±1,Î±n], select n equally spaced
samples (including Î±1 and Î±n). The resulting sample set is
{Î±1, ...,Î±n}
(2) For each Î±i, calculate Î²i := f (Î±i ).
(3) Find m switchover positions (i.e., knots) for polynomials
expressions:
(a) fit an initial approximation Â¯f of order d for the dataset
{Î±i , Î²i} using polynomial regression (without knots);
(b) select a new knot Ë™Î±i âˆˆ {Î±1, . . . ,Î±n} and fit two new poly-
nomial expressions on each side of the knot (the knot is
chosen such that the overall goodness of fit is maximized);
(c) repeat (b) until the number of knots equals m.
The set of knots is now { Ë™Î±1, ..., Ë™Î±m}. Note that Ë™Î±1 = Î±1 and
Ë™Î±m = Î±n.
(4) Fit a smoothing spline ([22], Chapter 5) of the same order us-
ing the knots {Î±i} on the dataset {Î±i , Î²i} and extract the poly-
nomial expression Pi (Î± ) in the each interval [ Ë™Î±i , Ë™Î±i +1],i âˆˆ
{1,m âˆ’ 1}.2
(5) Set boundary polynomials P0() (for Î±  Ë™Î±m), which are chosen specifically for f () to closely
2We use the functions in the library scipy.interpolate.UnivariateSpline and
numpy.polyfit [34]
Session C3:  Machine Learning PrivacyCCSâ€™17, October 30-November 3, 2017, Dallas, TX, USA624approximate the behaviour beyond the ranges [Î±1,Î±n]. Thus,
we split f () into m + 1 intervals, and each has a separate
polynomial expression.3
(6) The final approximation is:
Â¯f (Î± ) =
P0 (Î± )
P1 (Î± )
. . .
Pmâˆ’1 (Î± )
Pm (Î± )
if Î± < Ë™Î±1
if Ë™Î±1 â‰¤ Î± < Ë™Î±2
if Ë™Î±mâˆ’1 â‰¤ Î± < Ë™Î±m
if Î± â‰¥ Ë™Î±m ,
(5)
Note that any univariate monotonic functions can be fitted by
the above procedure.
Oblivious approximated sigmoid. We take sigmoid as an example to
explain how to transform smooth activation functions into their
oblivious forms. We set the polynomial degree d as 1, since linear
functions (as opposed to higher-degree polynomials) are faster and
less memory-consuming to be computed by 2PC. The approximated
sigmoid function is as follows:
Â¯f (y) =
0
a1y + b1
. . .
amâˆ’1y + bmâˆ’1
1
if y < y1
if y1 â‰¤ y < y2
if ymâˆ’1 â‰¤ y < ym
if y â‰¥ ym ,
(6)
We will show (in Section 6.2) that it approximates sigmoid with
negligible accuracy loss.
The approximated sigmoid function (Equation 6) is in fact a
piecewise linear function. So it can be transformed in the same
way as we explained in Section 5.3.1. The ideal functionality for the
approximated sigmoid â„±sigmoid is shown in Figure 6. Correctness
of this functionality follows the fact that, for yi â‰¤ y < yi +1:
x = ((aiy + bi ) âˆ’ (ai +1y + bi +1)) + ((aiy + bi ) âˆ’ (ai +1y + bi +1))
+... + ((amâˆ’1y + bmâˆ’1) âˆ’ 1) + 1
Input:
â€¢ ğ’®: yğ’® âˆˆ ZN ;
â€¢ ğ’: yğ’,r âˆˆ ZN .
â€¢ ğ’®: xğ’® := compare (y1,y) Â· (0 âˆ’ (a1y + b1))
Output:
+compare (y2,y) Â· ((a1y + b1) âˆ’ (a2y + b2))
+compare (ymâˆ’1,y) Â· ((amâˆ’1y + bmâˆ’1) âˆ’ 1) + 1