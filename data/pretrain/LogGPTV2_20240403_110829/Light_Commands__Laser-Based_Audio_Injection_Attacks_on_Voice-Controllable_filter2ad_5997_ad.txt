—
—
—
Google Assistant
Google Assistant
Google Assistant
Alexa
Alexa
Alexa
Alexa
Alexa
Alexa
Alexa
Alexa
Portal
Alexa
Alexa
Siri
Siri
50+
20
50+
50+
50+
50+
50+
50+
50+
50+
50+
40
20
50+
10
20
5
5
No
No
No
No
No
No
No
No
No
No
No
No
No
No
Yes
Yes
Yes
Yes
0.5
16
9
2.4
2.9
25
7
9
17
29
1
6
13
1.7
21
27
60
46
Phone
Tablet
Phone
Phone
and cannot be done via the small lenses that are typically used
for laser pointers. Thus, we mounted our laser to an Opteka
650-1300 mm high-deﬁnition telephoto lens, with 86 mm
diameter (Figure 1(left)). Finally, to simulate realistic aiming
conditions for the attacker, we avoided the use of electronic
scanning mirrors (used in Section 5.1) and mounted the lens
and laser on a geared camera head (Manfrotto 410 Junior
Geared Tripod Head) and tripod. Laser aiming and focusing
was done manually, with the target also mounted on a separate
tripod. See Figure 1 for a picture of our setup.
Test Locations and Experimental Procedure. As eye ex-
posure to a 60 mW laser is potentially dangerous, we blocked
off a 50 meter long corridor in our ofﬁce building and per-
formed the experiments at night. However, due to safety rea-
sons, we were unable to obtain a longer corridor for our high-
power tests. For lower-power attacks, we performed the ex-
periments in a 110 meter long corridor connecting two build-
ings (see Figure 1(top)). In both cases, we ﬁxed the target
at increasing distances and adjusted the optics accordingly
to obtain the smallest possible laser spot. We regulated the
diode current so that the target is illuminated with 5 or 60 mW
respectively. Finally, the corridor is illuminated with regular
ﬂuorescent lamps at ofﬁce-level brightness while the ambient
acoustic noise was about 46 dB (measured using a General
Tools DSM403SD sound level meter).
Success Criteria. We use the same success criteria as in
Section 5.1, considering the attack successful at a given dis-
tance in case the device correctly recognized all commands
during three consecutive injection attempts and considering
failure otherwise. We take this as an indication of the maxi-
mum range achievable by the attack at the considered power
budget. Finally, we benchmark our attack’s accuracy as a
function of distance in Section 5.3.
Experimental Results. Table 1 contains a summary of our
distance-benchmarking results. With 60 mW laser power, we
have successfully injected voice commands to all the tested
devices from a distance of several meters. For devices that
can be attacked using 5 mW, we also conducted the low-
power experiment in the 110 m hallway. Untested devices
are marked by ’—’ in Table 1 due of their high minimum
activation power.
While most devices require a 60 mW laser for success-
ful command injection (e.g., a non-standard-compliant laser
pointer), some popular smart speakers such as Google Home
and Eco Plus 1st and 2nd Generation are particularly sensitive,
allowing for command injection even with 5 mW power over
tens of meters. Next, as our attacks were conducted in 50 and
110 meter hallways (for 60 and 5 mW lasers, respectively) for
some devices, we had to stop the attack when the maximum
hallway length was reached. We mark this case with a ‘+’
sign near the device’s range in the appropriate column.
Attack Transferability. Despite inevitable manufacturing
variability between the 17 devices tested in this work, we
did not observe any signiﬁcant changes between the response
of different microphones to laser injection. That is, all mi-
crophones had shown the same high-level behavior, reacting
to light as if it was sound without any microphone-speciﬁc
calibration. This evidence also supports the universality of
our attack, as once the laser was aimed and focused, all de-
vices responded to injected commands without the need for
per-device calibration. In particular, the same laser light cor-
responding to a speciﬁc voice command was used on multiple
2640    29th USENIX Security Symposium
USENIX Association
Table 2: Attack success accuracy as a function of distance.
Command
What Time Is It?
Set the Volume to Zero
Purchase a Laser Pointer
Open the Garage Door
25m 27m
20m
0%
100% 90%
0%
100% 80%
90%
0%
0%
100% 100% 0%
devices without any modiﬁcations. Finally, we note that all
devices tested in this paper have multiple microphones, while
we aimed our laser to only a single microphone port. How-
ever, despite this, the attack is still successful, indicating that
current VC systems do not require the microphones’ signals
to match before executing voice commands.
5.3 Exploring Attack’s Success Probability
In the attacks presented in Sections 5.1, 5.2, and Table 1, all
the tested devices properly recognized the injected commands
once suitable aiming and focusing were achieved. However,
as can be seen in Table 1, some devices stopped recognizing
the commands after exceeding a certain distance. Investigat-
ing this phenomenon, we explored the attack’s error rate at
the borderline attack range. To achieve this, we use a Google
Home Mini device as a case study, as its attack range is lim-
ited to 20 meters which is shorter than the 50 meter corridor
available to us for high-power 60 mW experiments.
Table 2 presents a summary of our ﬁndings, where each
command was injected into the Google Home Mini device
10 times (totaling 40 consecutive command injections). As
can be seen, at 20 meters injection attacks are nearly always
successful, with a single error in recognizing the word “laser”
in the third command. However, at 25 meters the success
probability signiﬁcantly falls, with no successful injections
observed at 27 meters. These results indicate that while some
commands are a slightly harder to inject than others, the sud-
den drop in performance at 27m indicates that our attack’s
success probability does not seem to be dominated by the
command’s phonemes. Instead, it appears that success proba-
bility is governed by command-unrelated factors such as the
internal microphone structure, the presence of fabric covering
the microphone ports, the power density of the light hitting the
device’s microphone ports, the laser beam focus, alignment,
environmental noise level, machine learning algorithms, etc.
We leave the task of investigating these factors to future work.
5.4 Attacking Speaker Authentication
We begin by distinguishing between speaker recognition fea-
tures, which are designed to recognize voice of speciﬁc users
and personalize the device’s content, and speaker authenti-
cation features which is designed to restrict access control
to speciﬁc users. While not the main topic of this work, in
this section we now discuss both features in the context of
light-based command injection.
No Speaker Authentication for Smart Speakers. We
observe that for smart-speaker devices (which are the main
focus of this work), speaker recognition is disabled by default
at the time of writing. Next, even if the feature is enabled by
careful users, smart speakers are designed to be used by multi-
ple users. Thus, their speaker recognition features are usually
limited to content personalization rather than authentication,
treating unknown voices as guests. Empirically verifying this,
we found that Google Home and Alexa smart speakers block
voice purchasing for unrecognized voices (presumably as they
do not know which account should be billed for the purchase)
while allowing previously-unheard voices to execute security
critical voice commands such as unlocking doors. Finally, we
note that at the time of writing, voice authentication (as op-
posed to personalization) is not available for smart speakers,
which are common home smart assistant deployments.
Phone and Tablet Devices. Next, while not the main fo-
cus of this work, we also investigated the feasibility of light
command injection into phones and tablets. For such devices,
speaker authentication is enabled by default due to the high
processing power and single owner use.
Overview of Voice Authentication.
After being person-
alized with samples of the owner’s voice speaking speciﬁc
sentences, the tablet or phone continuously listens to the mi-
crophone and acquires a set of voice samples. The collected
audio is then used by the device’s proprietary voice recogni-
tion systems, aiming to recognize the device’s owner speak-
ing assistant-speciﬁc wake up words (e.g., “Hey Siri” or “OK
Google”). Finally, when there is a successful match with the
owner’s voice, the phone or tablet device proceeds to execute
the voice command.
Bypassing Voice Authentication.
Intuitively, an attacker
can defeat the speaker authentication feature using authentic
voice recordings of the device’s legitimate owner speaking the
desired voice commands. Alternatively, if no such recordings
are available, DolphinAttack [4] suggests using speech synthe-
sis techniques, such as splicing relevant phonemes from other
recordings of the owner’s voice, to construct the commands.
Wake-Only Security. However, during our experiments we
found that speaker recognition is used by Google and Apple to
only verify the wake word, as opposed to the entire command.
For example, Android and iOS phones trained to recognize
a female voice, correctly execute commands where only the
wake word was spoken by the female voice, while the rest of
the command was spoken using a male voice. Thus, to bypass
voice authentication, an attacker only needs a recording of
the device’s wake word in the owner’s voice (which can be
obtained by recording any command spoken by the owner).
Reproducing Wake Words.
Finally, we explore the possi-
bility of using Text-To-Speech (TTS) techniques for reproduc-
ing the owner’s voice saying the wake words for a tablet or
phone based voice assistant. To that aim, we repeat the phone
and tablet experiments done in Sections 5.1, 5.2 and Table 1,
USENIX Association
29th USENIX Security Symposium    2641
Table 3: Bypassing voice authentication on phones and tablets
Device
iPhone XR
iPad 6th Gen
Galaxy S9
Pixel 2
TTS Service
Assistant
NaturalReader US English Heather
Siri
Siri
NaturalReader US English Laura
Google Assistant NaturalReader US English Laura
Google Assistant NaturalReader US English Laura
Voice Name
training all the phone and tablet devices with a human fe-
male voice. We then used NaturalReader [36], an online TTS
tool for generating the wake words speciﬁc for each device,
hoping that the features of one of the offered voices will mis-
takenly match the human voice used for personalization. See
Table 3 for device-speciﬁc voice conﬁgurations provided by
NaturalReader which mistakenly match the female voice used
for training. Next, we concatenate the synthetically-generated
wake word spoken in a female voice to a voice command
pronounced by a male native-English speaker. Using these
recordings, we successfully replicated the minimum power
and maximum distance results as presented in Table 1.
We thus conclude that while voice recognition is able to
enforce some similarity between the attacker’s and owner’s
voices, it does not offer sufﬁcient entropy to form an adequate
countermeasure to command injection attacks. In particular,
out of the 18 English voices supported by NaturalReader, we
were able to ﬁnd an artiﬁcial voice matching the human fe-
male voice used for personalization for all 4 of our tablets and
phones without using any additional machine learning algo-
rithms. Finally, we did not test the ability to match voices for
devices other than phones and tablets, as voice authentication
is not available for smart speakers at the time of writing.
6 Exploring Various Attack Scenarios
The results of Section 5 clearly demonstrate the feasibility of
laser-based injection of voice commands into voice-controlled
devices across large attack distances. In this section, we ex-