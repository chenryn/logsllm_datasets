discussed in §V-D, employing a path-sensitive way to compute
assembly constraints is beyond the scope of this paper.
F. Code Brick Assembly
The Generate function of ENGINE FUZZER assembles
code bricks in the code brick pool (P ) to produce test cases.
Algorithm 1 presents the pseudo code of it. There are four
user-conﬁgurable parameters that Generate takes in.
imax
pblk
iblk
dmax
The maximum number of iterations of the genera-
tion algorithm. This parameter essentially decides
how many top-level statements to produce.
The probability of reinventing block statements.
This parameter decides how often we generate
block statements from scratch.
The maximum number of iterations for generating
a block statement. This parameter decides how
many statements are placed within a newly gen-
erated block statement.
The maximum nesting level for a reassembling
block statement. When CodeAlchemist generates
a new block statement from scratch, it limits the
nesting depth of the block statement with dmax,
because otherwise, the generation algorithm may
not terminate.
From the above snippet, the variable y can be either a Number
or an Array depending on the value of x. Suppose there are
two seed ﬁles that contain the above code brick, and each seed
executes the if statement with two distinct values 0 and 50
for x, respectively. Then the type of y can be either Number
or Array depending on which seed we execute.
In this case, we give a union type for the variable y in
the code brick. Speciﬁcally, when there are two or more seed
ﬁles for a given code brick, we may have union types in
In this paper, we only focus on the ﬁrst two parameters among
the four (imax and pblk) while using the default values for
the other two (iblk = 3 and dmax = 3). That is, we used the
default values for the two parameters for all the experiments we
performed in this paper. The default values were empirically
chosen though several preliminary experiments we performed
with varying iblk and dmax. For example, if iblk and dmax were
too large, generated test cases ended up having too complicated
loops, which are likely to make JS engines stuck or hang when
evaluated.
8
The algorithm starts with a code brick pool P , an empty
code brick B, and the four parameters given by an analyst.
In the for-loop of the algorithm (Line 2–7), CodeAlchemist
repeatedly appends a code brick to B, and returns the updated
B at the end, which becomes a new test case for fuzzing
the target engine. The loop iterates for imax times. Note our
algorithm only appends code bricks: we currently do not
consider code hoisting while stitching code bricks.
For every iteration, CodeAlchemist picks the next code
brick B(cid:48) to append, which can be either a regular JS statement
or a reinvented block statement as we described in §V-A. It
selects a reinvented block statement with the probability of
pblk, or a regular statement with the probability of 1 − pblk.
The RandProb function in Line 3 returns a ﬂoating number
from 0 to 1 at random.
The GenBlkBrick function in Line 4 randomly selects a
code brick among the ones that have an empty block statement
and the precondition of it satisfy the postcondition of B. It
then ﬁlls up the body of the block statement, and returns
the newly constructed code brick. On the other hand, the
PickBrick function in Line 6 randomly picks a code brick,
the precondition of which satisﬁes the postcondition of B, from
the pool P . When there are multiple candidate code bricks,
PickBrick randomly selects one with probability propor-
tional to the number of unique symbols in the precondition.
This is to increase the dependency among statements in the
generated test cases as semantically complex code tends to
trigger JS engine bugs more frequently.
Both Line 4 and Line 6 return a new code brick B(cid:48) to use,
which will be appended to B with the MergeBricks func-
tion in Line 7. To avoid reference errors, it replaces symbols in
the precondition of B(cid:48) with the symbols in the postcondition of
B based on their types. When there are multiple symbols with
the same type, we randomly select one of them, and replace
its symbol. After renaming, CodeAlchemist recomputes the
assembly constraint of the merged code brick in order to
preserve the semantics of it.
The GenBlkBrick function builds a code brick for a
block statement from scratch. The PickEmptyBlock func-
tion in Line 10 the pool P and the current code brick B
maintained by CodeAlchemist as input, and returns a code
brick B(cid:48) in P that satisﬁes the following two conditions: (1)
B(cid:48) should contain an empty block statement, which may or
may not include a guard, and (2) the precondition of B(cid:48) should
meet the postcondition of B. The GetDummyBrick function
in Line 11 then extracts a random subset of the postconditions
of B and B(cid:48) in order to build a new postcondition c, and then
create a dummy code brick B0, where the postcondition of it
is c. Next, CodeAlchemist generates a code brick B(cid:48)(cid:48) for the
body of the block statement using the Generate function
with the dummy code brick B0.
Note that the Generate and GenBlkBrick are mu-
tually recursive, and they allow us to generate nested block
statements. We limit the nesting depth of a newly generated
block by dmax. The RandInt function in Line 12 decides the
maximum number of iterations to be used in generating the
block body. It returns a random integer from 1 to iblk. Finally,
GenBlkBrick merges B(cid:48) and B(cid:48)(cid:48), and returns the merged
one as a new code brick containing a new block statement,
which is potentially nested up to the depth dmax.
G. Implementation
We have implemented CodeAlchemist with 0.6K lines of
JS code, 0.1K lines of C code, and 5K lines of F# code. We use
JS for parsing and instrumentation, C for executing JS engines,
and F# for the entire system. Our system heavily relies on the
asynchronous programming feature of F#. Since our algorithm
is written in a functional manner, i.e., no side-effects, it is
straightforward to achieve concurrency.
To parse JS seeds, we use Esprima [15], which is a
Node.js [22] library for parsing JS code. The parsed ASTs are
then passed to the Split function, which is written in F#,
as a JSON ﬁle. The counter part of JS parsing is to convert
an AST to a JS code snippet. There exists a famous library in
Node.JS, called escodegen [30], for that purpose. However, we
implemented our own code generator in F# in order to reduce
the communication cost between Node.JS and F#.
We also implemented our own JS library that includes
several helper functions for ﬁguring out types of variables in
each code brick with dynamic instrumentation. Our system
currently supports the seven primitive types and built-in types
we mentioned in §V-E. The Instrument function (written in
F#) rewrites given JS seeds in such a way that each code brick
will call the instrumentation functions deﬁned in the library.
Finally, the ENGINE FUZZER module is written in both F#
and C. The Generate function, which is purely written in
F#, generates test cases for fuzzing as we discussed in §V-F.
The core of the Execute function, however, is written in C,
in order to efﬁciently interact with native system functions.
We make our source code public on GitHub: https://github.
com/SoftSec-KAIST/CodeAlchemist.
VI. EVALUATION
We now evaluate CodeAlchemist to answer the followings:
1)
2)
3)
4)
Can CodeAlchemist generate semantically valid test
cases? (§VI-B)
Does the fuzzing parameters such as imax and pblk
affect
the effectiveness of CodeAlchemist? If so,
which values should we use? (§VI-C)
How does CodeAlchemist perform compared to the
state-of-the-art fuzzers in terms of ﬁnding bugs?
(§VI-D)
Can CodeAlchemist ﬁnd real-world vulnerabilities in
the latest JS engines? (§VI-E)
look? (§VI-F)
5) What do the vulnerabilities found by CodeAlchemist
A. Experimental Setup
We ran experiments on a machine equipped with two Intel
E5-2699 v4 (2.2 GHz) CPUs (88 cores) and 512 GB of main
memory, which is operated with 64-bit Ubuntu 18.04 LTS. We
selected the four major JS engines of the latest stable version as
of July 10th, 2018: (1) ChakraCore 1.10.1; (2) V8 6.7.288.46;
(3) JavaScriptCore 2.20.3; (4) SpiderMonkey 61.0.1. Note that
we used only ChakraCore for the ﬁrst three experiments due
to our resource limit. We chose ChakraCore because it has
9
per each test case, i.e., imax = 20. We then ran CodeAlchemist
while randomly varying the block reinvention rate pblk from
0.0 to 1.0 in order to obtain a set of 100,000 test cases (Tours).
We compared Tours with the set of 100,000 test cases (Tjs) that
we used in §III, which are generated by jsfunfuzz. Particularly,
we measured the success rate for N top-level statements by
running ChakraCore with Tours and Tjs.
Figure 7 presents the success rates for 20 distinct N. The
green line indicates the success rate of CodeAlchemist, and the
red line indicates that of jsfunfuzz. When we evaluated only
the ﬁrst top-level statement for each test case (N = 1), 24.7%
of test cases in Tjs were valid, whereas 60.7% of test cases
in Tours were valid. That is, CodeAlchemist produced about
2.5× more valid test cases in this case. Similarly, when we
evaluated the ﬁrst three top-level statements for each test case
(N = 3), 1.8% of test cases in Tjs and 30.0% of test cases in
Tours were valid.
Overall, CodeAlchemist generated 6.8× more semantically
valid test cases on average. Thus, we conclude that CodeAl-
chemist can produce substantially more valid test cases than
jsfunfuzz, the current state-of-the-art JS engine fuzzer.
2) Effectiveness of Assembly Constraints: The crux of our
system is that we can generate semantically valid test cases
by assembling code bricks based on their assembly constraints.
To justify our intuition, we ran a modiﬁed CodeAlchemist that
does not produce assembly constraints for code bricks. That
is, it produces code bricks and assembles them to generate
test cases, but none of the code bricks will have its assembly
constraint. Therefore, any code bricks can be interconnected
to each other in this case. The lines in Figure 7 show that
the success rate goes down as N increases because more
statements are likely to have more type errors and reference
errors. The modiﬁed CodeAlchemist (the blue line) has the
same success rate as jsfunfuzz (the red line). However, it
produced 5.7× less valid test cases on average compared to the
unmodiﬁed CodeAlchemist:
the unmodiﬁed CodeAlchemist
has much less type errors and reference errors than the others.
This result highlights the importance of assembly constraints,
and conﬁrms our intuition.
3) Note on the Success Rate: Although the success rate
shows the overall quality of the generated test cases, there
is a caveat: one can easily obtain a high success rate by
producing meaningless test cases that are always semantically
correct. Imagine a hypothetical fuzzer that always generates
a sequence of variable assignments such as “a = 1;”. This
will never produce test cases that are semantically invalid, and
thus, the success rate of this fuzzer is going to be always 100%.
Nevertheless, we argue that our results in this subsection is still
meaningful, because CodeAlchemist can generate test cases
that trigger more number of vulnerabilities than the state-of-
the-art fuzzers as we show in the rest of this paper.
In this regard, if we use a smaller range of pblk from 0.0 to
0.5 for the same experiment we did in §VI-B1, CodeAlchemist
produces twice more valid test cases than the result we showed.
For our own purpose, CodeAlchemist needs to produce both
semantically valid and highly-structured JS code snippets so
that they can trigger various vulnerabilities in the target JS
engine. Therefore, we need to ﬁnd a good value for pblk that
can hit the sweet spot (see §VI-C2).
Fig. 7: The success rate (out of 100,000 test cases) over the
number of evaluated top-level statement(s).
well-organized commit logs that specify which commit patches
which CVE vulnerability. On the other hand, we used all the
four JS engines for the rest of the experiments.
Seed Collection. Recall
that semantics-aware assembly
learns language semantics from JS seeds, and thus, having a
reasonable set of seed corpus is essential. To gather JS seeds,
we ﬁrst downloaded (1) regression tests from repositories of
the four major JS engines, and (2) test code snippets from
Test262 [31], which is an ofﬁcial ECMAScript conformance
test suite. From all the collected JS seed ﬁles, we ﬁltered
out some of them that are syntactically invalid or containing
statements using engine-speciﬁc syntax. Finally, we were able
to collect a total of 63,523 unique JS ﬁles from both test
suites. In addition, we collected 169 PoC exploits, which can
trigger previously known security vulnerabilities, by manually
crawling bug trackers and GitHub repositories. In total, we
gathered 63,692 unique JS seeds for our experiments.
Code Brick Pool. We ﬁrst ran the SEED PARSER module
to obtain 264,629 unique code bricks from the 63,692 seeds.
We then ran the CONSTRAINT ANALYZER module to assign
assembly constraints to the code bricks. Finally, we ﬁltered
out code bricks that contain uninteresting statements as we
discussed in §V-B. Consequently, we gathered 49,800 code
bricks in our code brick pool. Note that
the reason why
we have signiﬁcantly less code bricks after the ﬁltering step
is because most regression tests use the eval function to
compare evaluation results between two JS statements.
B. Validity of Generated Test Cases
Can semantics-aware assembly produce semantically valid
test cases? To answer the question, we measured the number
of runtime errors encountered by executing ChakraCore with
test cases generated from CodeAlchemist. In this subsection,
we say a test case is valid up to N statements if it does not
raise any runtime error when the ﬁrst N statements in the test
case are evaluated. The success rate for N top-level statements
is the rate between the number of valid test cases up to N top-
level statements and the total number of evaluated test cases.
1) Comparison against jsfunfuzz: Recall from §III that all
of the test cases obtained from jsfunfuzz threw a runtime error
after evaluating only a few top-level statements. To compare
CodeAlchemist against jsfunfuzz under the same condition, we
conﬁgured CodeAlchemist to produce 20 top-level statements
10
02040605101520# of Evaluated Top-Level Statement(s) per Test CaseSuccess Rate (%)w/ Assembly Constraintw/o Assembly ConstraintjsfunfuzzFig. 8: The average number of valid top-level statements over
imax. For each imax, we generated 100,000 test cases.
Fig. 9: # of bugs found over the probability of reinventing
block statements (pblk).
C. Choosing Parameters
Recall
from §V-F that CodeAlchemist has four user-
conﬁgurable parameters, and our focus in this paper is on imax
and pblk. To understand the impact of using different parameter
values, we conducted two independent experiments: (1) we
measured how imax affects the validity of generated test cases;
and (2) we evaluated how pblk changes the bug ﬁnding ability
of CodeAlchemist.
1) Dose imax Affect The Validity of Test Cases?: Intuitively,
the number of generated statements in a test case can affect
the validity of test cases because it is likely to have more
complex semantic structure as we have more statements, and
our analysis can be imprecise. To conﬁrm this intuition, we
ran CodeAlchemist with varying imax values (from 1 to 20)
to generate 100,000 test cases per each imax value. Thus, we
generated a total of 2,000,000 test cases for this experiment.
We ﬁx the block reinvention rate pblk to zero for this particular
experiment, because we wanted to see how imax solely affect
the validity of generated test cases. However, there exists the
similar tendency even for larger pblk because the likelihood
of generating invalid test cases increases as we generate more
statements in a test case.
Figure 8 illustrates the average number of valid statements
in each set of test cases we generated with different imax. Note
from the ﬁgure that the average number of valid statements
of the test cases converges to 8 as imax increases. Therefore,
it is reasonable to choose 8 as the value of imax as we can
minimize the size of generated test cases while keeping the
chance of generating a sequence of eight potentially valid
statements. Smaller test cases are better than bigger ones as
we can generate them more quickly.
2) Dose pblk Affect The Bug Finding Ability?: Recall from
§V-F, the probability of reinventing block statements (pblk)
decides how often we create block statements from scratch
during the assembly process. The key intuition here is that
security vulnerabilities often arise from a highly-structured