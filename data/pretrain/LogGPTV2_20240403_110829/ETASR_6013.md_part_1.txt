Engineering, Technology & Applied Science Research Vol. 13, No. 4, 2023, 11130-11138 11130
Α
HCLPars: New Hierarchical Clustering Log
Parsing Method
Arwa Bin Lashram
University of Jeddah, Saudi Arabia
PI:EMAIL (corresponding author)
Lobna Hsairi
University of Jeddah, Saudi Arabia
PI:EMAIL
Haneen Al Ahmadi
University of Jeddah, Saudi Arabia
PI:EMAIL
Received: 5 May 2023 | Revised: 17 May 2023 | Accepted: 18 May 2023
Licensed under a CC-BY 4.0 license | Copyright (c) by the authors | DOI: https://doi.org/10.48084/etasr.6013
ABSTRACT
Event logs are essential in many software systems’ maintenance and development, as detailed runtime
information is recorded in them, allowing support engineers and developers to monitor systems,
understand behaviors, and identify errors. With the increasing size and complexity of modern software
systems, parsing their logs by the traditional (manual) method is cumbersome and useless. For this reason,
recent studies have focused on automatically parsing log files. This paper presents the Hierarchical
Clustering Log Parsing method, called HCLPars, for automatically parsing log files, consisting of 3 steps:
parameter removal according to acquired knowledge in order to avoid errors, grouping similar raw log
messages, and getting the set of keys that make up the log. Experiments were run on 16 real system log
data, and the performance of the proposed algorithm was compared with the one of other 14 algorithms. It
was shown that the HCLPars outperformed the other log parsers in terms of accuracy, efficiency, and
robustness.
Keywords-event log mining; system logs; log parsing; log analysis; log management; execution trace;
HCLPars; agent
variable parts. The constant part reveals the log key or event
I. INTRODUCTION
template for the log message, which remains the same for every
Each system or application has its own log files containing event occurrence, and varies from event to event, while the
detailed information about the operating time (execution variable part records runtime information (i.e. parameters and
traces). These execution traces play an important role in states), which may vary among various event occurrences. The
developing, maintaining, and sustaining software systems. goal of log parsing is to automatically separate the constant part
They help developers and support engineers to understand the and the variable part of a raw log message, or otherwise match
system behavior [1, 2] and track and diagnose errors and each raw log message with a specified log key (constant part)
malfunctions that may arise [3, 4]. But despite the enormous [10]. So, we need to extract the log keys first, and then use it in
information buried in the logs, finding ways to effectively the parsing process.
analyze it remains a huge challenge [5], for two reasons: First,
modern software systems routinely generate tons of records in The traditional method of extracting log keys relies on
seconds. This huge volume of logs makes it difficult to inspect handcrafted regular expressions [11]. Simple as it may seem,
log messages manually. Second, these log messages are writing custom rules manually for a large volume of records is
unstructured in nature. To be able to analyze such files, the first a time-consuming and error-prone method, and the logging
and most important step is logging parsing, which is converting code is frequently updated in modern software systems, leading
the raw log messages into a sequence of structured events [6- to regular reviewing of these rules. To reduce the manual
9]. As the example illustrated in Figure 1, each raw log efforts in extracting log keys, some studies [12, 13], have
message records a specific system event with a set of fields: suggested techniques for extracting log keys directly from the
timestamp, verbosity level (e.g. ERROR/INFO/DEBUG), source code. These technologies are applicable in some cases,
component, and event. A raw log message has constant and but in practice, the source code is not always accessible, and
www.etasr.com Bin Lashram et al.: HCLPars: Α New Hierarchical Clustering Log Parsing Method
Engineering, Technology & Applied Science Research Vol. 13, No. 4, 2023, 11130-11138 11131
often these technologies are limited to specific software or produce an ordered list of all possible event occurrences. Also,
applications. Meanwhile, a static and generic analysis tool is several data-driven approaches have been proposed, which
needed for all programs across different programming have the advantage that they do not require domain expertise
languages, and to achieve this, several data-driven approaches and can learn patterns from log data and automatically generate
have been proposed, including iterative mining (SLCT [14]), shared registry key templates. For example, in [22] a new
iterative segmentation (IPLoM [15]), and hierarchical clustering algorithm (SLCT) that analyzes the log file using
clustering (LKE [16]). In contrast to handcrafted rules and frequent pattern mining was presented, which helps discover
source-based parsing, these methods can learn patterns from frequent patterns and identify anomalous lines in log files.
log data and automatically generate common log keys, but are Authors in [16] presented a novel algorithm called IPLoM
unable to handle huge datasets, so a parallel log parsing (Duplicate Partition Log Mining) to extract the log keys from
method, called POP was proposed [17]. POP is able to handle event logs. It performs a 3-step hierarchical partitioning process
large datasets with high accuracy, as it was implemented on top of the log using unique log message properties. Authors in [17]
of Spark, and it used Resilient Distributed Datasets (RDD) proposed a technique for log analysis to detect anomalies. It
abstraction, which is ineffective with data of small size because first preprocesses the data using empirical rules and then
it consumes more time, while using it is expensive when the performs hierarchical clustering of log messages using
size of the data increases [18]. To address these issues, we weighted edit distance, and finally, log keys are created from
propose the Hierarchical Clustering Log Parsing method the resulting clusters. But although the overall accuracy of
(HCLPars), that works on top of Spark like POP [17], but with these log parsing approaches is high, they are not effective in
a different abstraction type, which is Data Frame (DF). datasets whose logs are growing at a large scale (for example,
HCLPars was evaluated on large-scale real-world data sets, and 100 million record messages), as these approaches fail to
the results demonstrate its ability to achieve speed, accuracy, complete in a reasonable time (e.g. 1 hour), and most of them
and efficiency. For example, HCLPars can parse an HDFS can’t handle such data on a single computer. More recently,
dataset in less than one minute, while POP requires 7 minutes authors in [3] proposed a log parsing method through the
and IPLoM 30 minutes and LenMa and LogSig fail to finish in parallelization on Spark called POP. POP handles logs with
a reasonable time. simple domain knowledge. Then, it uses iterative partitioning
rules to divide the logs hierarchically into different groups.
Then, the static parts are extracted to create the event log.
Finally, similar groups are combined using hierarchical
clustering to create the log keys. It is a basic system for
processing large-scale data using the parallelization power of
computer clusters. POP was implemented on top of Spark and
used RDD abstraction. RDD cannot modify the system to work
more efficiently and uses sequencing and garbage collection
techniques, increasing the load on the system’s memory and
Fig. 1. Example of raw log message and log key. thus slowing the execution of operations.
II. RELATED WORK Looking at the issues of the existing research, we suggest a
new method to extract the log key. This method is based on a
Log key extraction has been studied extensively and has
data-driven approach to analyze the execution log, which was
been categorized into three approaches: rule-based, source-
built on top of Spark and used the DF abstraction. DF
code-based, and data-driven. Many researchers use rule-based
abstraction is characterized by its efficiency in analyzing files
methods [19, 20], which despite their accuracy, require domain
with high accuracy without the knowledge of the program, as
expertise and are also limited to specific rather than general
well as its high speed in analyzing files of any size. We used
application scenarios. For example, authors in [19] a rule-based
HashCode and Equals method to find similarities between
system for software failure analysis, taking advantage of
messages to ensure accuracy and speed instead of using the
artifacts that were produced at the time the system was
iterative partitioning used in [3], which consists of more than
designed and establishing a set of rules to formalize the
one steps, increasing execution time while it is not accurate
placement of registration instructions within the source code.
enough. We used an intelligent Agent that does this process
Authors in [21] proposed the novel Beehive system, which
and we called it the Prepossessing Agent. A comparison
identifies potential security threats for a large volume of logs
between the reviewed papers is illustrated in Table I.
by unsupervised collecting of specific data features, then
manually categorizing outliers. Source-code-based analysis has III. METHODOLOGY
been used to extract a log key. But it is ineffective because the
A log message usually records a run-time behavior of the
source code is often unavailable or incomplete to access.
program, including events, state change, and interactions
Meanwhile, most modern systems incorporate open-source
between components. It often contains two types of
software components written by hundreds of developers. For
information: a free-form text string that is used to describe the
example, authors in [13] proposed a general methodology for
semantic meaning of a recorded program behavior and a
log analysis based on source code analysis to discover large-
parameter that is used to express some important characteristics
scale system issues by extracting log events from console logs.
of the current task.
Authors in [1] proposed an automated approach for log analysis
to extract log keys directly from the source code and then
www.etasr.com Bin Lashram et al.: HCLPars: Α New Hierarchical Clustering Log Parsing Method
Engineering, Technology & Applied Science Research Vol. 13, No. 4, 2023, 11130-11138 11132
TABLE I. SUMMARY OF THE EXISTING RESEARCHES messages. Generally, the log messages printed by the different
FOR LOG KEY EXTRACTION
log-print statements are often completely different, while the
Data-driven Use of messages printed by the same statements are completely similar
Ref. Approach Abstraction
approach Spark to each other. According to this observation, we can use
[19] Rule-based - No - clustering techniques to group the log messages printed by the
same statement together and then find their common part as the
[21] Rule-based - No -
log key. Parameters can cause some clustering mistakes
Source-
[13] - No - because some of the different log messages contain a lot of
code-based
matching parameter values, for example, raw log messages 1
Source-
[1] code-based - No - and 2 have many similar parameters (shown in Figure 3). So,
the Prepossessing Agent will remove the parameter values first,
Data-driven Frequent pattern
[21] mining No - according to acquired knowledge, to avoid errors. Then, it
groups the similar raw log messages and finds the common
[16] Data-driven Iterative partitioning No -
parts in each group to get the log keys.
[17] Data-driven Hierarchical clusterin No -
Data-driven Iterative partitioning + A. Erasing Parameters via Acquired Knowledge
[3] Yes RDD
Hierarchical clustering
The parameters are mostly in the form of numbers, IP
HCLPars Data-driven Hierarchical clustering Yes DF
addresses, URIs, or follow special symbols like the colon or the
equal sign. They are often included in parentheses or square
In general, due to the various parameter values, the number
brackets. It is easy to classify such content. Therefore, simple
of different types of log messages is massive or even infinite.
regular expression rules are often used to recognize and remove
Thus, the dimension problem of the direct consideration of log
these parameters [23], for example, removing block ID in
messages as a whole during log data mining may be
Figure 3 by "blk [-?[0-9]]+". For parameter removal (IP
troublesome. To resolve this problem, we replace every log
addresses, numbers, etc.) we used gained knowledge from
message with its corresponding log key to perform analysis. A
previous research to define the types of parameters for each
log key is defined as the common content of all log messages
dataset (shown in the second block in Figure 5).
which are printed in the source code by the same log-print
statement. The parameters are defined as a variable value
printed by the log-print statement. In other terms, without any
parameters, a log key equals the free-form text string of the
log-print statement. For example, the key log message 1 and 3
is "INFO dfs.DataNode$PacketResponder: PacketResponder x
for block x terminating" (shown in Figure 2). We analyze logs
based on log keys for the following reasons:
Fig. 3. Example 2 of raw log message and key log message.
 Different log-print statements often output different log text
messages. Each specific log-print statement in the source B. Removing the Duplicate Raw Log Key
code corresponds to a specific type of log key. So, a In this step, the Prepossessing Agent is partitioning the raw
sequence of log keys will reveal the system’s execution log keys into groups, so we need to find a proper metric to find
path and this will help us predict failures during the similarity between the raw log keys. We found that the
implementation by identifying the normal execution path of HashCode and Equals method is the best metric for finding the
the system. similarity between raw log keys, because most programmers
tend to write log keys firstly, and add the parameters afterward.
 The number of key log message types is limited and much
less than the number of raw log message types. It can help
us avoid the dimension issue while extracting and analyzing
data. It also provides a simplified view of all the events that
occurred while the system was running for administrators.
Fig. 4. Four log messages from the HDF dataset.
So, the log keys printed in the source code by the same log-