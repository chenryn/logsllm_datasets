# Title: Revisiting the Design of Optimal Location Privacy-Preserving Mechanisms

## Authors:
- Simon Oya, University of Vigo, [EMAIL]
- Carmela Troncoso, IMDEA Software Institute, [EMAIL]
- Fernando Pérez-González, University of Vigo, [EMAIL]

## Abstract
In recent years, various strategies have been developed to design optimal location privacy-preserving mechanisms, aiming to maximize the adversary's expected error regarding the users' whereabouts. In this work, we critically examine these defenses and demonstrate that, although they are optimal in terms of minimizing the adversary's correctness, they do not all provide the same level of protection when considering other dimensions of privacy. To avoid suboptimal choices, we argue that the search for optimal mechanisms should be guided by complementary criteria. We introduce two auxiliary metrics: conditional entropy, which captures an information-theoretic aspect, and worst-case quality loss, which ensures a minimum utility for the users. We propose a new mechanism that maximizes conditional entropy while being optimal in terms of average adversary error, and compare its performance with previously proposed optimal mechanisms using two real datasets. Our empirical results confirm that no single mechanism excels in all privacy criteria, highlighting the need to consider multiple privacy dimensions to fully understand the protection provided by a mechanism.

## CCS Concepts
- Security and privacy → Privacy-preserving protocols
- Networks → Location-based services

## Keywords
- Location Privacy
- Mechanism Design
- Mechanism Evaluation
- Quantifying Privacy

## 1 Introduction
Location-based services (LBS) raise significant privacy concerns due to the sensitive information revealed by accurate location data [13, 14, 16, 20, 30]. To address these concerns, the academic community has proposed numerous location privacy-preserving mechanisms [3, 15, 17–19, 21, 23, 28, 29] that typically alter the user's actual location before it is exposed to the service provider. The privacy evaluation of these mechanisms often does not account for strategic adversaries, leading to an arms race where defenses and attacks continuously evolve without providing clear privacy guarantees. Recent efforts aim to mitigate this by either incorporating adversarial knowledge into the design process [5, 24, 27] or providing guarantees independent of the adversary's prior [2, 5, 24].

In this paper, we focus on sporadic user-centric protection mechanisms based on randomization, which preserve privacy by reporting a noisy version of the real location according to a probability distribution. These mechanisms are suitable for applications requiring infrequent location exposure and can be executed locally by the user. Approaches that embed adversarial knowledge use Bayesian modeling [26] and find optimal noise-generating mechanisms via linear optimization, balancing privacy objectives and utility constraints [27]. On the other hand, approaches that provide privacy guarantees independent of the adversary's prior are based on geo-indistinguishability [2], an adaptation of differential privacy [9] to two-dimensional spaces, used in several works [11, 12, 22]. Geo-indistinguishability can be achieved optimally using expensive linear programming [5] or suboptimally using efficient remapping techniques [6]. Shokri [24] combined Bayesian and geo-indistinguishability approaches to create mechanisms that guarantee geo-indistinguishability while performing well against Bayesian adversaries.

Following the recommendation by Shokri et al. [26], these approaches use the adversary's correctness, measured as the expected estimation error, to evaluate location privacy. However, we find that while many mechanisms meet the desired quality loss constraint, some may offer little actual privacy. For example, a mechanism that alternates between exposing the actual user location and a faraway location complies with the constraints but provides minimal uncertainty for the adversary. To counter this, we argue that the search for an optimal mechanism should consider additional criteria beyond the error, such as information-theoretic metrics like conditional entropy or worst-case quality loss. We provide efficient methods to construct mechanisms based on these criteria and demonstrate that the remapping method introduced in [6] can be used to build an optimal mechanism in terms of the expected estimation error from any obfuscation mechanism. We evaluate the effectiveness of different mechanisms using two real location datasets and conclude that mechanisms optimal for one criterion do not necessarily perform well on others.

### Contributions
- **Theoretical Characterization:** We provide a theoretical characterization of optimal location privacy-preserving mechanisms in terms of mean adversarial error. We show that, for a given average quality loss, there are multiple optimal mechanisms forming a convex polytope, each providing different privacy guarantees.
- **Limitations of Adversary Correctness:** We demonstrate the limitations of evaluating defenses solely based on the adversary's correctness and advocate for the use of complementary criteria to better understand the privacy guarantees provided.
- **Efficient Mechanism Design:** We provide algorithms to efficiently design mechanisms based on criteria other than the adversary's error. We also show that remapping, previously proposed to improve the utility of geo-indistinguishability, can be used as a generic method to turn any obfuscation mechanism into one that is optimal in terms of average adversarial error.
- **Empirical Evaluation:** We evaluate prior and new location privacy-preserving mechanisms on two real location datasets, confirming that it is challenging to find mechanisms that excel in all criteria. This highlights that previous approaches, while well-founded, oversimplify the design problem and may overestimate the level of privacy offered to users.

## 2 System Model
We describe our system model, which aligns with the framework for location privacy proposed by Shokri et al. [26], and introduce the notation used throughout the paper (Table 1).

### Notation
- **x**: Input location the user is interested in querying about.
- **X**: Set of valid input locations or points of interest.
- **z**: Output location released by the mechanism, \( z \in \mathbb{R}^2 \).
- **\(\hat{x}\)**: Adversary’s estimation of the input location, \( \hat{x} \in \mathbb{R}^2 \).
- **\(\pi(x)\)**: Prior probability that a user wants to query about x.
- **\(f(z|x)\)**: Privacy mechanism. Probability density function (pdf) of z given x.
- **\(f_Z(z)\)**: Pdf of z, i.e., \( f_Z(z) = \sum_{x \in X} \pi(x) \cdot f(z|x) \).
- **\(p(x|z)\)**: Posterior probability of x given z.
- **\(d_Q(x, z)\)**: Quality loss distance function between x and z.
- **Q(f, π)**: Average quality loss metric, defined in Equation (1).
- **Q⁺(f, π)**: Worst-case quality loss metric, defined in Equation (2).
- **\(d_P(x, \hat{x})\)**: Privacy distance function between x and \(\hat{x}\).
- **PAE(f, π)**: Average error privacy metric, defined in Equation (5).
- **PCE(f, π)**: Conditional entropy privacy metric, defined in Equation (9).
- **PGI(f, π)**: Geo-Indistinguishability privacy metric, defined in Equation (11).

### 2.1 Quality Loss Metrics
We consider two definitions of quality loss: average loss and worst-case loss. We introduce \( d_Q(x, z) \), a function that quantifies the quality of service lost by a user reporting output location z when interested in input location x. Larger values of \( d_Q(x, z) \) indicate greater loss and worse utility performance for the user. The canonical choice for this function is the Euclidean distance: \( d_Q(x, z) = ||x - z||_2 \). Note that \( d_Q(·) \) does not need to be a metric in the mathematical sense; it can be any function mapping an input location and a released location to a loss value.

#### Average Loss
The average loss measures the quality loss a user experiences on average and is defined as:
\[ Q(f, \pi) = \sum_{x \in X} \int_{\mathbb{R}^2} \pi(x) \cdot f(z|x) \cdot d_Q(x, z) \, dz. \]
This metric is intuitive and linear with the mechanism \( f \), making it useful for reducing the computational cost of mechanism design algorithms and facilitating the analysis of optimal algorithms in terms of average loss.

#### Worst-case Loss
Given a function \( d_Q(x, z) \) that quantifies point-wise loss, the worst-case loss is defined as:
\[ Q^+(f, \pi) = \max_{x \in X} \int_{\mathbb{R}^2} f(z|x) \cdot d_Q(x, z) \, dz. \]

## 3 Consequences of Using Average Adversary Error
We study the consequences of choosing the average adversary error as the standard metric to evaluate location privacy. We illustrate that mechanisms optimal by this criterion may provide little actual privacy. For example, a mechanism that alternates between exposing the actual user location and a faraway location complies with the constraints but provides minimal uncertainty for the adversary.

## 4 Auxiliary Metrics for Mechanism Design
To avoid suboptimal choices, we propose considering auxiliary metrics such as conditional entropy and worst-case quality loss. These metrics help in designing mechanisms that provide better overall privacy.

### Conditional Entropy
Conditional entropy captures the information-theoretic aspect of the problem. It is defined as:
\[ P_{CE}(f, \pi) = H(X | Z) = -\sum_{x \in X} \int_{\mathbb{R}^2} p(x|z) \log p(x|z) \, f_Z(z) \, dz. \]

### Worst-case Quality Loss
Worst-case quality loss ensures that the output of the mechanism always provides a minimum utility to the users. It is defined as:
\[ Q^+(f, \pi) = \max_{x \in X} \int_{\mathbb{R}^2} f(z|x) \cdot d_Q(x, z) \, dz. \]

## 5 Evaluation of Mechanisms
We evaluate several mechanisms built according to the new criteria using two real location datasets. Our results confirm that it is difficult to find mechanisms that perform well on all criteria, demonstrating that previous approaches oversimplify the design problem and may overestimate the level of privacy offered to users.

## 6 Conclusions
In conclusion, we provide a comprehensive analysis of optimal location privacy-preserving mechanisms, highlighting the need to consider multiple privacy dimensions. We introduce new metrics and algorithms to guide the design of more robust mechanisms and demonstrate their effectiveness through empirical evaluation.