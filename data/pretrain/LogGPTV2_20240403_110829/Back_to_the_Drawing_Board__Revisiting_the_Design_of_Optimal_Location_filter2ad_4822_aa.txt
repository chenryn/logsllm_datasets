title:Back to the Drawing Board: Revisiting the Design of Optimal Location
Privacy-preserving Mechanisms
author:Simon Oya and
Carmela Troncoso and
Fernando P&apos;erez-Gonz&apos;alez
Back to the Drawing Board: Revisiting the Design of Optimal
Location Privacy-preserving Mechanisms
Simon Oya
University of Vigo
PI:EMAIL
Carmela Troncoso
IMDEA Software Institute
PI:EMAIL
Fernando Pérez-González
University of Vigo
PI:EMAIL
7
1
0
2
g
u
A
4
2
]
R
C
.
s
c
[
2
v
9
7
7
8
0
.
5
0
7
1
:
v
i
X
r
a
ABSTRACT
In the last years we have witnessed the appearance of a variety
of strategies to design optimal location privacy-preserving mecha-
nisms, in terms of maximizing the adversary’s expected error with
respect to the users’ whereabouts. In this work, we take a closer
look at the defenses created by these strategies and show that, even
though they are indeed optimal in terms of adversary’s correctness,
not all of them offer the same protection when looking at other
dimensions of privacy. To avoid “bad” choices, we argue that the
search for optimal mechanisms must be guided by complementary
criteria. We provide two example auxiliary metrics that help in
this regard: the conditional entropy, that captures an information-
theoretic aspect of the problem; and the worst-case quality loss,
that ensures that the output of the mechanism always provides a
minimum utility to the users. We describe a new mechanism that
maximizes the conditional entropy and is optimal in terms of aver-
age adversary error, and compare its performance with previously
proposed optimal mechanisms using two real datasets. Our empiri-
cal results confirm that no mechanism fares well on every privacy
criteria simultaneously, making apparent the need for considering
multiple privacy dimensions to have a good understanding of the
privacy protection a mechanism provides.
CCS CONCEPTS
• Security and privacy → Privacy-preserving protocols; • Net-
works → Location based services;
KEYWORDS
Location Privacy; Mechanism Design; Mechanism Evaluation; Quan-
tifying Privacy
1 INTRODUCTION
Location based services raise important privacy concerns regard-
ing the private information that exposing accurate location to ser-
vice providers reveals [13, 14, 16, 20, 30]. To protect users’ privacy,
the academic community has proposed a wide variety of location
privacy-preserving mechanisms [3, 15, 17–19, 21, 23, 28, 29] that
mostly work altering the users’ actual location before exposing it
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than ACM
must be honored. Abstracting with credit is permitted. To copy otherwise, or republish,
to post on servers or to redistribute to lists, requires prior specific permission and/or a
fee. Request permissions from permissions@acm.org.
CCS ’17, October 30-November 3, 2017, Dallas, TX, USA
© 2017 Association for Computing Machinery.
ACM ISBN 978-1-4503-4946-8/17/10...$15.00
https://doi.org/10.1145/3133956.3134004
to the service provider. The privacy evaluation of these proposals
typically does not consider a strategic adversary, fostering an arms
race in which defenses and attacks succeed each other without ever
providing clear location privacy guarantees. To counter this effect,
recent efforts focus on cutting the arms race short by either embed-
ding the adversarial knowledge on the design process [5, 24, 27], or
providing guarantees independent of the adversary’s prior [2, 5, 24].
In this paper, we focus on sporadic user-centric protection mecha-
nisms based on randomization, which preserve privacy by reporting
a noisy version of the real location to the service provider according
to a probability distribution. These mechanisms are adequate for ap-
plications that require infrequent location exposure, and can be run
locally by the user. In this scenario, approaches that embed the ad-
versarial knowledge on the design process are based on a Bayesian
modeling of the adversary [26], and find optimal noise-generating
mechanisms via linear optimization in which a target privacy ob-
jective is sought in presence of utility constraints [27]. On the other
hand, approaches that provide privacy guarantees independent of
the adversary’s prior are based on geo-indistinguishability [2], an
adaptation of differential privacy [9] to two-dimensional spaces,
used by a number of works [11, 12, 22]. Geo-indindistinguishability
can be achieved optimally in terms of utility using expensive lin-
ear programming [5], or suboptimally using efficient remapping
techniques that increase the utility of the query [6]. Finally, the
Bayesian and the geo-indistinguishability approaches have been
combined by Shokri [24] to obtain mechanisms that guarantee geo-
indindistinguishability while achieving a good performance against
the Bayesian adversary.
Following the recommendation by Shokri et al. [26], which has
been taken as the standard by the community, all of these ap-
proaches use the adversary’s correctness, i.e., how close the ad-
versary’s estimate is to the correct answer, to evaluate location
privacy. Usually, the adversary’s correctness is measured as her
expected estimation error, where this error is modeled using some
distance metric between the real location and the adversary’s esti-
mation [25].
In this paper, we aim at understanding the properties of the
mechanisms output by these design strategies. We find that, when
the target privacy notion is the adversary’s expected estimation
error, there are many optimal mechanisms that meet a desired
quality loss constraint. While this may seem advantageous, we
show that following such an optimization objective may result in the
selection of naive mechanisms that obviously provide little privacy,
e.g., alternating the exposure of the actual user location and a far
away location. Indeed, this mechanism complies on average with
the constraints of the problem. Yet, it results on little uncertainty for
the adversary, effectively providing a false perception of privacy.
To counter such effect we argue that, depending on the user’s
preferences, the search for an optimal location privacy-preserving
mechanism needs to consider more criteria than the error, contra-
dicting the belief established by Shokri et al. [26]. As examples of
complementary metrics to guide the design of protection mecha-
nisms we propose the use of information-theoretic metrics, e.g.,
the conditional entropy, or a worst-case bound for quality loss. We
provide efficient methods to construct mechanisms with respect to
these criteria, and demonstrate that the remapping method intro-
duced in [6] to improve the utility of geo-indistinguishability-based
methods is in fact a straightforward generic scheme to build an
optimal mechanism in terms of the expected estimation error from
any obfuscation mechanism. We evaluate the effectiveness of the
different mechanisms according to different privacy criteria using
two real location datasets concluding that, generally, mechanisms
that are optimal for one criterion do not necessarily perform well
on others.
To summarize, we make the following contributions:
✓ We provide a theoretical characterization of optimal location
privacy-preserving mechanisms in terms of the mean adversarial
error. We show that, for a given average quality loss, there is more
than one optimal protection mechanism that maximizes the average
privacy. This family of mechanisms forms a convex polytope in
which different mechanisms provide different privacy guarantees.
✓ We demonstrate the limitations of evaluating defenses solely
considering the correctness of the adversary [26], and advocate
for the use of complementary criteria to guide the design of loca-
tion privacy-preserving mechanisms where the privacy guarantees
provided are better understood.
✓ We provide algorithms to efficiently design mechanisms based on
criteria other than the adversary’s error. Furthermore, we demon-
strate that remapping, previously proposed as an enhancement to
geo-indistinguishability, is not only beneficial to improve the utility
of this technique but can be used as a generic method to turn any
obfuscation mechanism into optimal in terms of average adversarial
error.
✓ We evaluate prior and new location privacy-preserving mech-
anisms on two real location datasets. Our results confirm that it
is difficult to find optimal mechanisms that fare well on all crite-
ria. This demonstrates that previous approaches to design location
privacy-preserving mechanisms, while having solid foundations,
oversimplify the design problem and generate defenses that overes-
timate the level of privacy offered to the user.
This paper is organized as follows. In Section 2, we introduce our
system model, and the quality loss and privacy metrics we consider
in the paper. In Section 3 we study the consequences of choosing
the average adversary error as the standard metric to evaluate lo-
cation privacy, illustrating that mechanisms that are optimal by
this criterion may provide little privacy. In Section 4 we propose
to consider auxiliary metrics to avoid bad mechanism choices in
the optimization. As examples, we study the use of the conditional
entropy and the worst-case quality loss. We evaluate several mech-
anisms built according to these new criteria in Section 5, and offer
our conclusions in Section 6.
2 SYSTEM MODEL
We now describe our system model, which is in agreement with
the framework for location privacy proposed by Shokri et al. [26],
and introduce the notation used throughout the paper, which is
summarized in Table 1.
and
We consider a set of users that send queries with a geographical
position of interest to a location based service to obtain a service
(e.g., finding points of interest or nearby friends). The location of in-
terest can be the current location of the user or some other location
the user is interested in querying about. Users wish to obtain utility
from the location based service, while keeping their whereabouts
private from an adversary that can observe the locations in the
queries, e.g, an eavesdropper of the user-server communication, or
the service provider itself. In order to protect their locations, users
employ a location privacy-preserving mechanism that perturbs their
location prior to exposing it to the server. We consider a strategic
adversary that knows the protection mechanism operation, and
has some knowledge about the users movement patterns. Given
the observed perturbed location and her knowledge, the adversary
tries to infer the user real location.
We model the locations queried by the users as a discrete set
of points of interest denoted by X (cid:17) {x1, x2, · · · , xN }. We refer to
these locations as real or input locations since they are the actual
locations that are input to the location privacy-preserving mecha-
nism. We use π(x) to denote the prior probability that a user in the
population queries the service provider about location x (π(x) ≥ 0
x ∈X π(x) = 1). This prior can either represent the global
behavior of all the users as in [6], or be tailored to a particular user,
but we assume that it is known both by the user and the adversary
and that it can be used to design the privacy-preserving mecha-
nism. We also consider independence between queries, i.e., that the
input locations x from the same or other users are samples form
i.i.d. random variables given by π.
The set of possible locations reported by the location privacy-
preserving mechanism is denoted by Z. We assume that users can
report any location in the world Z = R2. We refer to these locations
as output locations, as they are the outputs of the privacy-preserving
mechanism. The mechanism itself is denoted by f and modeled as
a set of (continuous) conditional probability distributions, where
f (z|x) denotes the probability density function (pdf) of reporting
the output location z ∈ R2 when the real location of the user is
R2 f (z|x)dz = 1 for all x ∈
X). We represent discrete mechanisms, i.e., mechanisms with a
discrete output domain, in R2 with the Dirac delta function δ. For
example, the mechanism that maps any x ∈ X to two particular
outputs z1, z2 ∈ R2 with the same probability would be f (z|x) =
0.5δ(z − z1) + 0.5δ(z − z2). For integration purposes, δ(z − z′) must
be understood as a two-dimensional Gaussian pdf centered at z′
whose variance is arbitrarily small.
When using a privacy-preserving mechanism f to obtain privacy,
the user experiences a loss on the quality of service due to the fact
that she reports a location that might not be the location of interest,
and may even be far away from this one. We use P(f , π) to denote
the privacy of the user, and Q(f , π) to denote her quality loss. We
specify particular instantiations of these functions below.
x ∈ X (note that f (z|x) ≥ 0 and∫
Table 1: Summary of notation
Pdf of z, i.e., fZ(z) =
Input location the user is interested in querying about.
Set of valid input locations or points of interest.
Output location released by the mechanism, z ∈ R2.
Adversary’s estimation of the input location, ˆx ∈ R2.
Prior probability that a user wants to query about x.
Privacy mechanism. Pdf of z ∈ R2 given x ∈ X.
Posterior probability of x given z.
Symbol Meaning
x
X
z
ˆx
π(x)
f (z|x)
fZ(z)
p(x|z)
dQ(x, z) Quality loss distance function between x and z.
Q
Q+
dP(x, ˆx) Privacy distance function between x and ˆx.
Average error privacy metric, in (5).
PAE
Conditional entropy privacy metric, in (9)
PCE
Geo-Indistinguishability privacy metric, in (11)
PGI
Average quality loss metric, in (1).
Worst-case quality loss metric, in (2).
x ∈X π(x) · f (z|x).
2.1 Quality Loss Metrics
We consider two possible definitions of quality loss: the average
loss, and the worst-case loss. To this end we introduce dQ(x, z), a
function that quantifies how much quality of service is lost by a
user reporting output location z when she is interested in input
location x. Larger values of dQ(x, z) indicate a larger loss, and there-
fore a worse utility performance for the user. The canonical choice
for this function is the Euclidean distance: dQ(x, z) = ||x − z||2.
Note that dQ(·) does not need to be a metric in the mathematical
sense: it could be any function that maps an input location and a
released location to a loss value (e.g., a feeling-based utility metric
as in [1, 4]).
Average Loss. The average loss measures how much quality a user
loses on average, and can be written as:
Q(f , π) = 
x ∈X
∫
R2 π(x) · f (z|x) · dQ(x, z)dz .
This metric has been the typical choice of utility in the related liter-
ature [2, 5–7, 27] since it is very intuitive. This metric also has the
advantage of being linear with the mechanism f , which is very use-
ful towards reducing the computational cost of mechanism design
algorithms. Moreover, it makes the analysis of optimal algorithms
in terms of average loss tractable.
(1)
Worst-case Loss. Given a function that quantifies the point-wise
loss as defined above, dQ(x, z), the worst-case loss is defined as:
(2)