H 23.16
H 8.98
700
R
R
1.76
G
G
1.852
2.30
R
H
100
6.3
H 14.61
100
C 2.10
0.254
R 333.50 0.043
R
3.485
G
4.50
–
–
–
–
–
–
Avail.
(%)
Leap
Perf
99.86 Adeq.
99.47 Adeq.
99.69 Adeq.
99.44 Adeq.
99.70 Adeq.
99.93 CGO
99.72 CGO
98.05 CNG
99.69 Adeq.
98.73 CNG
63.73 Adeq.
99.59 CGO
95.05 CGO
96.70 CNG
S1Downtime in the table, NA server usnyc3-ntp-003.aaplimg.com, which is also
an R server, only drops from Stratum-1 (to Stratum-0 in this case) 0.0018% of
the time. This is 29 times less often than its Errtime at 0.052%. Thus for this
server, error is a more serious concern than stratum stability.
The Best-50 are marked via symbols within Fig. 3, where certain observations
are more immediate. For example we clearly see that 9 of the Constant S1 servers
in the Best-50 have clock errors, and that only 2 in the Best-50 take 3 or more
stratum values.
Another observation of note is that, with the exception of ptbtime3.ptb.de,
servers with Ideal Leap Performance and zero S1Downtime enjoy Server Error
ratings of G, suggesting that this pair could serve as a useful indicator of an
exceptionally well managed server, and hence be predictive of exemplary Error
behaviour. Useful does not mean foolproof however: in addition to the exception
above the two NIST servers in Table 5 provide sobering counter-examples.
The server list contains 35 servers from Apple’s 17.253 domain. Three of
these make it into the Best-50, though all exhibit server errors with relatively
large Errtime values. Finally, it is worth noting that despite having 66 servers
from National Laboratories in the list, only 12, those colored cyan, make it into
the Best-50 (an additional 5 from the NMI in Australia are excluded as they are
not publicly accessible).
Because the criteria of entry into the Best-50 are so strict, there is a limit to
what one can say about these servers: they are indeed very well behaved. How-
ever, if one relaxes the criteria in diﬀerent dimensions, a much wider variety of
behaviour is quickly revealed. To make this concrete, and to indicate what could
have been included in the Best-50 had things been a little diﬀerent, a number
of contrasting examples are provided in Table 5, separated into ﬁve categories.
For each server bolded column entries mark the criteria which did not meet the
Best-50 standard.
110
Y. Cao and D. Veitch
In the ﬁrst category we give 3 of the 5 servers (of which {2,3} were rated
{C,H} respectively) that failed to make the Best-50 because of excessive server
errors. By deﬁnition, and as noted earlier, such servers illustrate the fact that the
(Availability, Stratum, Leap) three-tuple is not suﬃcient to predict the absence
or otherwise of clock errors, nor their severity in terms of Size or Errtime. Partic-
ularly noteworthy is the fact that H servers, which by deﬁnition have an Errtime
over 25%, and typically have Errtime of a dramatic 100%! can and do appear.
The second category exhibits two examples of servers that failed only due to
being too low in the S1Downtime ranking, one of which has Size of 700 ms and
Errtime three times higher than its S1Downtime. The third category gives exam-
ples failing only the Leap criterion, that are exemplary in other respects. There
were no examples of servers which failed in Availability only. The fourth cate-
gory includes ﬁve diverse examples where two criteria were not met. Finally, the
ﬁfth category includes servers that are still generally respectable despite failing
in three criteria.
5 Discussion
We discuss the limitations, implications and future of our work.
Source Coverage. Because of the widespread usage of the Pool service, and
the high proﬁle of the Org list, we expect the server list to contain most of the
widely used public S-1 servers, but how representative are they of the (unknown)
complete set? There is in fact a high degree of overlap, 50% or more, between
each of the three main sources: Org, Pool and LBL, leading to speculation in [5]
that the server list contains a signiﬁcant percentage of the global public facing
Stratum-1 server population. We now consider how to evaluate this claim.
Population estimation based on re-sampling a marked sub-population is known
as the capture-recapture problem in statistics. To ﬁt within this framework, it is
natural to group the Org and Pool sources together as they are both community
based, and have a strong, non-random relationship. Thus we have n = |Org ∪
Pool| = 356 servers which represent a ‘marked’ sample of the total unknown pop-
ulation N. The LBL source now represents a random sample of K = 257 servers,
of which k = 175 lie in Org∪Pool, that is they are marked servers that are ‘recap-
tured’. The population can now be estimated from n, K and k. For example the
Chapman estimator [3,15], yields ˆN = (cid:4)(K +1)(n+1)/(k+1)(cid:5)−1 = 522. A corre-
sponding (non-symmetric) 95% coverage interval for N is [497, 562]. This suggests
that our Best-50 is well founded as it is based on a number, 453, being between
80% and 91% of all public servers.
The random sampling assumptions underlying the Chapman estimator do not
hold strictly here, so the above estimate can only be viewed as a rough indication.
To determine the true value of N a better approach, for IPv4 servers, is simply
to exhaustively probe the IPv4 address space. We did not do so here, as that
would not have given us the leap second performance information we require.
List Shelf Life. As it derives from a static data set, the utility of our Best-50
will decrease over time. Some indication of its expected lifetime can be gained
Where on Earth Are the Best-50 Time Servers?
111
from the longitudinal results in [18], which report on a subset of Org servers
using data collected over 151 days in 2011–12 (Exp1), and 124 days in 2014–15
(Exp2). Although Availability, Leap performance, and Errtime are not given, we
can compare with respect to Stratum Constancy, and Error Classiﬁcation.
Of the Best-50 servers, there are 13 which also appear in that study. All 13
(100%) were found to be error-free in each of Exp1 and Exp2, as well as having
zero S1Downtime for Exp2 (stratum data was unavailable for Exp1). For the
metrics available, this represents perfect agreement.
Of the 14 servers which feature in Table 5, 13 also appear in the study,
of which 3 are suitable for direct comparison as they pass our criteria for
S1Downtime and have Error class in {G,R,C}. Of these, all 3 exhibit close
agreement, with no detected errors in each of Exp1 and Exp2, and again with
zero S1Downtime. Finally, at the other end of the spectrum, of the 4 servers in
the continuously errored H class in Table 5, 3 were also classed as H in [18].
Based on the above, we expect that the level of churn in the Best-50 list
provided here will be low on useful timescales, for example 5 years. Knowledge
of server conﬁguration would be of interest here also to attempt root cause
analysis, as would correlating against network failures. We have attempted to
contact administrators, however the response rate was minimal.
Measurement Cost. The analysis used here requires specialist hardware, tech-
niques, unusual data (leap events), and signiﬁcant eﬀort. A priori, this does not
scale. A goal of future work must be to develop lighter weight approximate tech-
niques and more automated server error detection using standard hardware. The
work here can serve to evaluate the eﬀectiveness of such techniques.
Scalability cost divides substantially along criteria lines. Stratum Constancy
measurement scales trivially, as it depends neither on special hardware nor the
network path. Availability also scales readily, though to remove packet loss bias
requires measurement close to the server and/or path diversity, and hence client
placement diversity ideally. Leap Performance is inherently diﬃcult as oppor-
tunities to measure it occur only every ≈2.5 years. On the other hand this also
limits the workload, and the protocol aspects are as scalable as Stratum Con-
stancy. Rankings could be deﬁned which exclude leap second criteria for appli-
cations where this is not needed, for example Internet measurement campaigns
not covering leap events, which are announced months in advance.
The Clock Error criteria is the expensive one, and the most critical. The
hardware cost could be avoided by using a robust clock synchronization and
timestamping approach such as RADclock [16] as a Stratum-2, with its Stratum-
1 server selected from the Best-50 provided here. Although timestamping errors
would of course be higher, they would still be well below server error sizes in
most cases. In terms of the error analysis itself, it is feasible, albeit non-trivial,
to automate this to a good level of accuracy, and this is a direction for our future
work. Such a capability would enable, for example, ongoing monitoring and error
querying for important servers. However, this is not essential for the purpose of
maintaining the Best-50 as we have deﬁned it here, as the construction of the
list, combined with its expected low churn, implies that only a small number
112
Y. Cao and D. Veitch
of high quality servers (which are faster to process) would have to be evaluated
from scratch each year to keep it current. Those remaining would also have to
be re-evaluated, but this is less onerous when they have been seen before.
Server Ranking. From the quality dimensions we have considered various rank-
ings could be deﬁned. An obvious way to rank the Best-50 is the S1Downtime
ordering employed in the list construction, however this cannot be extended over
all servers, as many will not satisfy the minimum requirements in other crite-
ria. A candidate which avoids this problem is Badtime, deﬁned to be the sum
of Errtime and 1−Availability, being the proportion of time a server should be
avoided. This should suit contexts where leap second performance is not critical.
Great care must be taken in how any ranking is used, to prevent high ranking
servers from receiving high loads. It would be a mistake (and is not the intent
of this paper!) to recommend that clients make use of the Best-50 en masse.
Instead, server rank should be used within broader systems designed to tradeoﬀ
load balancing and server quality appropriately. Indeed, NTP Pool’s score is an
attempt to do this (Sect. 2), however it is not grounded in knowledge of actual
server error. The larger problem is that NTP Pool breaks NTP’s inherent load
balancing mechanism, namely the server hierarchy, while simultaneously prefer-
encing its own load balancing over server quality. Thus pools contain servers of
mixed strata, and clients are given diﬀerent servers over time with quality which
may vary enormously. Instead, we argue that the hierarchy needs to be enforced,
and within that, well deﬁned notions of rank given higher prominence.
Client Impacts. Finally, a separate, but natural question to ask is, how impor-
tant is it for a client to select a server of Best-50 calibre? The client impact will
depend strongly on many factors including the robustness of the clock synchro-
nization algorithm in use, the policy regarding back-up servers and if they are
available, the size of server errors, their duration, the length of non-availability
periods, the stratum of the client, the characteristics of the path to the server,
and whether a leap second is involved. Potential errors can range from negligi-
ble (< 10µs) and short-term (few seconds) at one extreme, to permanent (until
server change) and extreme (10’s of ms to seconds or well beyond plus high
variability) at the other. The onus on the Stratum-1 server is to show near per-
fect behaviour to anchor and lift performance across the timing system. This is
possible, as many in the Best-50 demonstrate.
6 Conclusion
Our Best-50 list is not deﬁnitive. It is however the ﬁrst serious attempt to quan-
tify timeserver best practice that we are aware of. We believe that it will be
of use for a number of years at least, by which time the methodology could be
improved to make such a list more comprehensive, dynamic and less expensive to
generate. It is in any event, feasible to maintain it even with current technology.
Acknowledgment. Partially supported by Australian Research Council’s Discovery
Projects funding scheme #DP170100451.
Where on Earth Are the Best-50 Time Servers?
113
Appendix
(see Tables 6 and 7).
Table 6. URL to IP mapping of the servers in Table 4.
CONT
AF
AN
OC
EU
NA
AS
SA
ntp2.litnet.lt 193.219.61.120
metronoom.dmz.cs.uu.nl 131.211.8.244
time.assecobs.pl 195.189.85.132
ntp1.niiftri.irkutsk.ru 46.254.241.74
ntp2.niiftri.irkutsk.ru 46.254.241.75
ntp1.gbg.netnod.se 192.36.133.17
ntp2.gbg.netnod.se 192.36.133.25
ntp1.mmo.netnod.se 192.36.134.17
ntp2.mmo.netnod.se 192.36.134.25
ntp1.sth.netnod.se 192.36.144.22
ntp2.sth.netnod.se 192.36.144.23
istntpprd–02.corenet.ualberta.ca 129.128.5.211
clepsydra.dec.com 204.123.2.5
m4c2236d0.tmodns.net 208.54.34.76
m4d2236d0.tmodns.net 208.54.34.77
montpelier.ilan.caltech.edu 192.12.19.20
stratum1.neology.co.za 41.73.40.11
ntp1.net.monash.edu.au 130.194.1.96
ntp.freestone.net 193.5.68.2
URL IP
– –
ntp.ﬁzyka.umk.pl 158.75.5.245
goblin.nask.net.pl 195.187.245.55
ntp.certum.pl 213.222.200.99
CY
ZA
–
AU
ntp1.oma.be 193.190.230.65 BE
CH
netopyr.hanacke.net 94.124.107.190 CZ
ntp.nic.cz 217.31.202.100 CZ
ptbtime1.ptb.de 192.53.103.108 DE
ptbtime3.ptb.de 192.53.103.103 DE
ES
hora.roa.es 150.214.94.5
ES
ntp.i2t.ehu.es 158.227.98.15
unknown1 188.39.213.7
GB
unknown2 81.187.202.142 GB
LT
NL
unknown3 148.252.105.132 NO
PL
PL
PL
PL
RU
RU
SE
SE
SE
SE
SE
SE
CA
tick.usask.ca 128.233.154.245 CA
tock.usask.ca 128.233.150.93 CA
US
US
US
US
navobs1.gatech.edu 130.207.244.240 US
US
US
US
time–a.netgear.com 209.249.181.52 US
time–a.stanford.edu 171.64.7.105
US
tock.phyber.com 207.171.30.106 US
US
US
US
JP
JP
JP
ntp-b2.nict.go.jp 133.243.238.163 JP
SG
CL
unknown4 210.23.25.77
ntp.shoa.cl 200.54.149.24
ntp.colby.edu 137.146.28.85
ntp1.digitalwest.net 72.29.161.5
tick.ucla.edu 164.67.62.194
usatl4-ntp-002.aaplimg.com 17.253.6.253
usno.hpl.hp.com 204.123.2.72
usnyc3-ntp-003.aaplimg.com 17.253.14.123
f2.kns1.eonet.ne.jp 60.56.214.78
jptyo5-ntp-001.aaplimg.com 17.253.68.125
ntp1.noc.titech.ac.jp 131.112.125.48
114
Y. Cao and D. Veitch
Table 7. URL to IP mapping of the servers in Table 5.
CONT
OC
NA
NA
OC
EU
NA
SA
EU
EU
NA
NA
NA
EU
EU
URL IP
CY
ntp10.net.monash.edu.au 130.194.10.150 AU
time-a.timefreq.bldrdoc.gov 132.163.4.101 US
time-c.timefreq.bldrdoc.gov 132.163.4.103 US
ntp.waia.asn.au 218.100.43.70 AU
ntp1.fau.de 131.188.3.221 DE
srcf-ntp.stanford.edu 171.66.97.126 US
a.st1.ntp.br 200.160.7.186 BR
ntp1.vniiftri.ru 89.109.251.21 RU
ntp3.fau.de 131.188.3.223 DE
ntp.myﬂoridacity.us 71.40.128.146 US
US
US
DE
ntp2.usv.ro 80.96.120.252 RO
time-b.nist.gov 129.6.15.29
t2.timegps.net 69.75.229.43
rustime01.rus.uni-stuttgart.de 129.69.1.153
References
1. Archipelago monitor locations. http://www.caida.org/projects/ark/locations/
2. Bajpai, V., Sch¨onw¨alder, J.: A survey on internet performance measurement plat-
forms and related standardization eﬀorts. IEEE Commun. Surv. Tutorials 17(3),
1313–1341 (2015)
3. Brittain, S., B¨ohning, D.: Estimators in capture-recapture studies with two sources.
AStA Adv. Stat. Anal. 93(1), 23–47 (2009)
4. Cao, Y., Veitch, D.: TimeServer Dataset 2016–2017. https://data.research.uts.edu.
au/public/DVTSD/. Per-server results also available
5. Cao, Y., Veitch, D.: Network timing, weathering the 2016 leap second. In: Pro-
ceedings of IEEE INFOCOM 2018, Honolulu, USA, 15–19 April 2018
6. Guyton, J.D., Schwartz, M.F.: Experiences with a Survey Tool for Discovering
Network Time Protocol Servers (1994). Accessed 31 July 2015
7. Hong, C.-Y., Lin, C.-C., Caesar, M.: Clockscalpel: understanding root causes of
internet clock synchronization inaccuracy. In: Spring, N., Riley, G.F. (eds.) PAM
2011. LNCS, vol. 6579, pp. 204–213. Springer, Heidelberg (2011). https://doi.org/
10.1007/978-3-642-19260-9 21
8. Malone, D.: The leap second behaviour of NTP servers. In: Proceedings of the
Traﬃc Monitoring and Analysis workshop, 7–8 April 2016. IFIP Digital Library
(2016). http://tma.iﬁp.org/2016/#program
9. Minar, N.: A Survey of the NTP Network (1999). http://xenia.media.mit.edu/
nelson/research/ntp-survey99/ntp-survey99-minar.ps
10. Murta, C., Torres, P., Mohapatra, P.: Characterizing quality of time and topology
in a time synchronization network. In: Global Telecommunications Conference,
GLOBECOM 2006, pp. 1–5 (2006). IEEE, November 2006
11. ntp.org. NTP Pool Project (2018). Accessed 2 May 2018
12. pool.ntp.org. How do I use pool.net.org? http://www.pool.ntp.org/en/use.html
13. pool.ntp.org. NTP Pool
selection. http://support.ntp.org/bin/view/
server
Servers/NTPPoolServers
14. Staﬀ, R.N.: Ripe atlas: a global internet measurement network. Internet Protoc.
J. 18(3) (2015)
Where on Earth Are the Best-50 Time Servers?
115
15. Sutherland, W.J.: Ecological Census Techniques: A Handbook. Cambridge Univer-
sity Press, Cambridge (2006)
16. Veitch, D., Ridoux, J., Korada, S.B.: Robust synchronization of absolute and dif-
ference clocks over networks. IEEE/ACM Trans. Netw. 17(2), 417–430 (2009)
17. Veitch, D., Vijayalayan, K.: Network timing and the 2015 leap second. In: Pro-
ceedings of PAM 2016, Heraklion, Crete, Greece, 31 March - 1 April 2016
18. Vijayalayan, K., Veitch, D.: Rot at the roots? Examining public timing infrastruc-
ture. In: Proceedings of IEEE INFOCOM 2016, San Francisco, CA, USA, 10–15
April 2016