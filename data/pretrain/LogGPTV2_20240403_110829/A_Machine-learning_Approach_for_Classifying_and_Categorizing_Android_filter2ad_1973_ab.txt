possibly non-constant. On the other hand, the reboot method
in the PowerManager class, for instance, just receives a kernel
code for entering special boot modes which must be part of a
predeﬁned set of supported ﬂags. This method is thus only a
resource method (the data is written into the kernel log), but
not an Android Sink. We require this restriction on constant
values for methods which do not introduce any new information
into the calling application in the case of sources, or do not
directly leak any data across the application boundary in the
case of sinks. The values at calls to such methods are of a
purely technical kind (e.g., system constants, network pings
etc.) and not of interest to typical analysis tools. Note that our
deﬁnition also excludes some implicit information ﬂows. This
is a design choice. For instance, in our approach the vibration
state of the phone is not considered a single-bit resource, even
though it could theoretically be observed and would then be
“shared”.
A malicious app can try to access private information not
only through calls to the ofﬁcial Android framework API but
also through calls to code of pre-installed apps. For instance, the
default email application provides a readily-available wrapper
around the getDeviceId() function. This app is pre-installed
on every stock Android phone, which gives a malicious app
easy access to the wrapper: the app just instructs the Android
class loader to load the respective system APK ﬁle and then
instantiates the desired class. To cover such cases, our approach
does not only analyze the framework API but the pre-installed
apps as well. (We use a Samsung Galaxy Nexus with Android
4.2.). In other words, our analysis boundary is between a
(potentially malicious) user application and all components
pre-installed on the device.
IV. CLASSIFICATION APPROACH
In this section, we explain the details of SUSI, our machine-
learning approach to automatically identify sources and sinks
corresponding to the deﬁnitions given in Section III. We
address two classiﬁcation problems. For a given unclassiﬁed
Android method, SUSI ﬁrst decides whether it is a source, a
sink, or neither. The second classiﬁcation problem reﬁnes the
classiﬁcation of sources and sinks identiﬁed in the ﬁrst step.
All methods previously classiﬁed as neither are ignored. For an
uncategorized source or sink, SUSI determines the most likely
semantic category it belongs to. In our design, every method
is assigned to exactly one category.
Section IV-A gives a short introduction to machine learning.
Section IV-B then presents the general architecture of SUSI,
while Section IV-C discusses the features SUSI uses to solve its
classiﬁcation problems. Section IV-D gives more details on one
particularly important family of features which deals with data
ﬂows inside the methods to be classiﬁed. In Section IV-E we
show how the semantics of the Java programming language can
be exploited to artiﬁcially generate further annotated training
data.
A. Machine Learning Primer
SUSI uses supervised learning to train a classiﬁer on a
relatively small subset of manually-annotated training exam-
ples. This classiﬁer is afterwards used to predict the class
of an arbitrary number of previously unseen test examples.
Classiﬁcation is performed using a set of features. A feature
ID Experience Alcohol Phone No Accident
T1
T2
T3
T4
T5
C1
C2
5 yrs
11 yrs
7 yrs
4 yrs
10 yrs
6 yrs
12 yrs
yes
yes
yes
no
no
?
?
0.6
0.4
0.2
0.0
0.2
0.1
0.55
1234
45646
76546
54645
78354
6585
67856
TABLE I.
CLASSIFICATION EXAMPLE ON DRUNK DRIVING
is a function that associates a training or test example with a
value, i.e., evaluates a certain single domain-speciﬁc criterion
for the example. The approach assumes that for every class
there is a signiﬁcant correlation between the examples in the
class and the values taken by the feature functions.
As a simple example, consider the problem of estimating
the risk of a driving accident for an insurance company. We
may identify three features: years of experience, blood alcohol
level and the driver’s phone number. Assume the learning
algorithm deduces that a higher level of experience is negatively
correlated with the accident rate, while the alcohol level is
positively correlated and the phone number is completely
unrelated. The impact of a single feature on the overall estimate
is deduced from its value distribution over the annotated training
set. If there are many examples with high-alcohol accidents,
then this feature will be given a greater weighting than the
years of experience. However, if there are more accidents of
inexperienced drivers in the training set than alcohol-related
issues, the classiﬁer will rank the experience feature higher.
The classiﬁer works on a matrix, organized with one column
per feature and one row per instance. Table I shows some sample
data. An additional column indicates the class and is only ﬁlled
in for the training data. In our example, this column would
indicate whether or not an accident took place. The ﬁrst ﬁve
rows are training data, the last two rows are test records to be
classiﬁed.
In this example, a simple rule-based classiﬁer would deduce
that all reports with alcohol levels larger than 0.2 also contained
accidents, so C2 would be classiﬁed as accident:yes. However,
since the converse does not hold, further reasoning is required
for C1. Taking the experience level into account, there are two
records of inexperienced drivers with levels of 0.2 or below
in our test set: one with an accident and one without. In this
case, the classiﬁer would actually pick randomly, since both
accident:yes and accident:no are equally likely. A probabilistic
classiﬁer could also choose accident:yes because accidents are
more likely for inexperienced drivers (two out of three with ﬁve
years of experience or less in this test data set) in general. This
demonstrates that results can differ depending on the choice
of the classiﬁer.
As a concrete classiﬁer, we use support vector machines
(SVM), a margin classiﬁer, more precisely the SMO [25]
implementation in Weka [26] with a linear kernel. We optimize
for minimal error. The basic principle of an SVM is to represent
training examples of two classes (e.g., “sink” and “not a
sink”) using vectors in a vector space. The algorithm then
tries to ﬁnd a hyper-plane separating the examples. For a new,
previously unseen test example, to determine its estimated
4
l
e
v
e
L
l
o
h
o
c
l
A
0.6
0.4
0.2
0
T1
C2
T2
T3
T5
C1
Accident
No Accident
To Be Classiﬁed
8
10
6
Years of Experience
12
T4
4
Training
Set
Feature
Database
Test
Set
Training Matrix
Testing Matrix
train classiﬁer
Classiﬁer
Sources
Sinks
Input
Preparation
Classiﬁcation
Output
Fig. 1. SMO Classiﬁcation Example
1st run (classiﬁcation)
2nd run (categorization)
class, it checks on which side of the hyper-plane it belongs. In
general, problems can be transformed into higher-dimensional
spaces if the data is not linearly separable, but this did not
prove necessary for any one of our classiﬁcation problems.
Figure 1 shows an SMO diagram for Table I. We have not
included the phone number feature since it is unrelated to the
probability of an accident. The red line shows a projection
of the hyper-plane. In this example, the SMO detects that all
points above the line are positive examples (i.e., records of
accidents), and all points below are negative ones (i.e., no
accident). C2 would thus be classiﬁed as an accident, just as
with the simple rule-based classiﬁer above, but C1 would now
deﬁnitely be classiﬁed as non-accident because it lies below
the line.
SMO is only capable of separating two classes. However, in
SUSI, we have three classes in the ﬁrst problem (source/sink/nei-
ther) and a lot more in the second one (the categorization).
We solve the problem with a one-against-all classiﬁcation,
a standard technique in which every possible class is tested
against all other classes packed together to ﬁnd out whether
the instance corresponds to the current single class or whether
the classiﬁcation must proceed recursively to decide between
the remaining classes.
We also evaluated other classiﬁcation algorithms based on
different principles, for instance Weka’s J48 rule learner, which
implements a pruned C4.5 decision tree [27]. The main problem
with a rule set is its lack of ﬂexibility. While many source-
method names, for instance, start with get, this is not the case
for all source methods. On the other hand, not all methods that
start with get are actually sources. Since this rule of thumb is
correct most of the time, however, a rule tree would usually
include a rule mapping all get methods to sources and only
perform further checks if the method name has a different
preﬁx. With an SVM, such aspects that are usually correct, but
not always, can be expressed more appropriately by shifting
the hyper-plane used for separation.
Probabilistic learning algorithms like Naive Bayes [28]
produced very imprecise results. This happens because our
classiﬁcation problem is almost rule-based, i.e., has an almost
Fig. 2. Machine learning approach
ﬁxed semantics. The variance is simply not large enough to
justify the imprecision introduced by probabilistic approaches
which are rather susceptible to outliers.
B. Design of the Approach
Figure 2 shows SUSI’s overall architecture. It includes four
different layers: input, preparation, classiﬁcation, and output.
The square elements denote objects, while the round elements
represent actions. We run two rounds: One for classifying
methods as sources, sinks, or neither, and one for categorizing
them. Solid lines denote the data ﬂow within SUSI. The two
dashed lines denote the initialization of the second round.
The general process is the same for both rounds. For the
categorization, SUSI just takes the outputs of the classiﬁcation
as test data inputs. More precisely, SUSI categorizes separately
those methods it has previously identiﬁed as sources or sinks
and disregards those it classiﬁed as neither.
SUSI starts with the input data for the ﬁrst classiﬁcation
problem, i.e., for identifying sources and sinks. This data
consists of the Android API methods to analyze. These methods
can be separated into a set of training data (hand-annotated
training examples) and a set of test data for which we do not
know whether a method is a source, sink or neither. The set
of training data is much smaller than the set of unknown test
data, in our case only roughly 0.7% for the classiﬁcation and
about 0.4% for the categorization. Beside the API methods
we need a database of features, both for the classiﬁcation and
categorization. The features are different for classiﬁcation and
categorization. See Section IV-C for details.
As described in in Section IV-A, a supervised learning
approach requires two matrices. The ﬁrst one is built by
evaluating the features on the set of hand-annotated training
data, the second one by applying the same feature set as well
to the test data yet to be classiﬁed (preparation step). SUSI
then uses the ﬁrst matrix to train the classiﬁer (classiﬁcation
step), which afterwards decides on the records in the test matrix
(output step).
5
While there are a few methods in the Android library that
are both sources and sinks (such as some of the transceive
methods of the NFC implementation), their scarcity stops
us from establishing a fourth category “both”, even though
in theory such a category might sound sensible. Classifying
a sufﬁcient amount of training data for a machine learning
approach would be equal to classifying almost all transceiver
methods. Respectively, we treat such methods as either sources
or sinks. This decision affects both the training data and the
classiﬁer’s results.
In a second step, SUSI categorizes the sources and sinks
set. In this step, SUSI separately considers the sources and
sinks determined in the ﬁrst step as new test sets (dashed
arrows). Note that methods classiﬁed as neither are ignored at
this point. SUSI also requires new training data for the second
classiﬁcation problem. To provide such data, we hand-annotated
a subset of the Android sources and sinks with semantic
categories related to the mobile domain. We furthermore chose
different kinds of features for the feature database as explained
in Section IV-C. We chose 12 different kinds of source-
categories that we identiﬁed as being sufﬁciently meaningful for
the different Android API methods: account, bluetooth, browser,
calendar, contact, database, ﬁle, network, nfc, settings, sync,
and unique-identiﬁer. For the sinks, we deﬁned 15 different
kinds of categories: account, audio, browser, calendar, contact,
ﬁle, log, network, nfc, phone-connection, phone-state, sms/mms,
sync, system, and voip. For the purpose of compiling our
training data, if a method is not relevant or does not ﬁt in
any of the identiﬁed categories, it is annotated as belonging
to the special no-category class. If one wants to add a new
category, one simply has to create new features for the feature
database and randomly annotate the corresponding API methods.
Our approach then automatically uses the new feature for
the generation of the categorized sources and/or sinks. The
subsequent steps as shown in Figure 2 are equal to the ones
for the classiﬁcation. The ﬁnal output consists of two ﬁles, one
for the categorized sources and one for the categorized sinks.
Note that some of these categories refer to data being
managed by applications, not the operating system itself. One
example are contacts: The system provides a data interface to
make sure that there is a uniform way of obtaining contacts
for all applications that require them, e.g., travel planners, or
calendars sending invitations. Additionally, Android contains
system applications providing default implementations of these
interfaces, so there are methods which are available on every
Android phone and which can be called in order to obtain
private data. Therefore, we include categories for such methods,
despite them not being part of the operating system as such.
Since we have different categories for sources and sinks,
their categorization comprises two distinct classiﬁcation prob-
lems: one for sources and one for sinks. Though they share
the same feature set (see Section IV-C), both are solved
independently of each other. Thus, quite naturally, the resulting
correlations might differ signiﬁcantly, as some features might
be more relevant to distinguish different kinds of sources than
different kinds of sinks, and vice versa.
C. Feature Database
We used a set of 144 syntactic and semantic features for
classifying the Android methods. A single feature alone does
6
not usually give enough information to decide whether a given
Android method is a source, a sink or neither. However, all
features in combination can be used to train a highly precise
classiﬁer. The same holds for the second classiﬁcation problem
in which we need to ﬁnd categories for our sources and sinks.
One main reason for why these features work is that many