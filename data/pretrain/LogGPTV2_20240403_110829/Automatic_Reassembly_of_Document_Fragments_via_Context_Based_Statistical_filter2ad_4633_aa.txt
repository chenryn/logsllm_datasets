title:Automatic Reassembly of Document Fragments via Context Based Statistical
Models
author:Kulesh Shanmugasundaram and
Nasir D. Memon
Automatic Reassembly of Document Fragments
via Context Based Statistical Models
Kulesh Shanmugasundaram
Nasir Memon
PI:EMAIL
Department of Computer and Information Science
PI:EMAIL
Polytechnic University
Brooklyn, NY 11201
Abstract
Reassembly of fragmented objects from a collection of
randomly mixed fragments is a common problem in classi-
cal forensics. In this paper we address the digital forensic
equivalent, i.e., reassembly of document fragments, using
statistical modelling tools applied in data compression. We
propose a general process model for automatically analyz-
ing a collection fragments to reconstruct the original docu-
ment by placing the fragments in proper order. Probabilities
are assigned to the likelihood that two given fragments are
adjacent in the original using context modelling techniques
in data compression. The problem of ﬁnding the optimal
ordering is shown to be equivalent to ﬁnding a maximum
weight Hamiltonian path in a complete graph. Heuristics
are designed and explored and implementation results pro-
vided which demonstrate the validity of the proposed tech-
nique.
1. Introduction
Reassembly of fragments of objects from a collection
of randomly mixed fragments is a problem that arises in
several applied disciplines, such as forensics, archaeology,
and failure analysis. This problem is well studied in these
disciplines and several tools have been developed to auto-
mate the tedious reassembly process [10]. The digital foren-
sic equivalent of the problem –which we call reassembling
scattered documents–, however, is yet to be explored.
Digital evidence by nature is easily scattered and a foren-
sic analyst may come across scattered evidence in a vari-
ety of situations. A forensic analyst who comes across the
problem of recovering deleted ﬁles often faces the difﬁ-
cult task of reassembling ﬁle fragments from a collection
of randomly scattered data blocks on a storage media. This
is especially true with the FAT16 and FAT32 ﬁlesystems,
which due to the popularity of the Windows operating sys-
tem, are perhaps still the most widely used ﬁle systems
on personal computers. Furthermore, due to the ubiquitous
presence of Windows and easier implementation considera-
tions, the FAT ﬁle systems has been adopted in many con-
sumer storage media devices, such as compact ﬂash cards
used in digital cameras and USB mini-storage devices. The
FAT ﬁlesystem however is not very efﬁcient in maintaining
continuity of data blocks on the disk. Performance degra-
dation due to ﬁle fragmentation is a common problem in
many FAT systems. Due to fragmentation, when a ﬁle is
stored data blocks could be scattered across the disk. With-
out adequate ﬁle table information it is difﬁcult to put the
fragments back together in their original order. Often criti-
cal ﬁle table information is lost because they are overwritten
with new entries. In fact, the most widely used disk foren-
sics tools like TCT[18], dd utility, The Sleuth Kit[7], and
Encase[3] can recover data blocks from deleted ﬁles auto-
matically. However, when the data blocks are not contigu-
ous these tools cannot reassemble the blocks in the correct
order to reproduce the original ﬁle without the proper ﬁle ta-
ble entries. The job of reassembling these fragments is usu-
ally a tedious manual job carried out by a forensic analyst.
Another situation where a forensic analyst come across
scattered evidence is the swap ﬁle. The system swap ﬁle is
one of the critical areas where lot of useful forensic infor-
mation can be gathered. The swap ﬁle contains critical in-
formation about the latest events that occurred on a com-
puter. Therefore, reconstructing contents of the swap ﬁle
is vital from a forensic standpoint. In order to achieve bet-
ter performance, operating systems maintain swap ﬁle state
and addressing information in page-tables stored only in
volatile memory. When computers are secured for eviden-
tial purposes they are simply unplugged and sent to a foren-
sic lab. Unfortunately contents of volatile memory are usu-
ally lost beyond recovery during evidence collection. With-
out the addressing information from the page-table it is dif-
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:46:44 UTC from IEEE Xplore.  Restrictions apply. 
ﬁcult to rebuild contents off a swap ﬁle. Again, a forensic
analyst is left with a collection of randomly scattered pages
of memory.
One of the most popular and naive approach to hiding
evidence is to store them in slack space in the ﬁlesystem.
Files are assigned certain number of disk blocks for storage.
However, not all ﬁles ﬁt exactly into the allocated blocks.
In most cases ﬁles end up using only a portion of their last
block. The unused space in this last block is known as slack
space. Modifying the contents of slack space does not af-
fect the integrity of data stored in the ﬁlesystem because the
read operation does not read data in slack space. A crimi-
nal can modify a ﬁle hiding program to choose the blocks
on which ﬁles are hidden based on a sequence of numbers
generated using a password. Knowing the password he can
reconstruct the original document, whereas a forensic ana-
lyst is left with randomly mixed fragments of a document
which will need to be reassembled.
Finally, ubiquitous networking and growing adop-
tion of peer-to-peer systems give anyone easy access
to computers around the world. There are many peer-
to-peer systems which enable users to store data on
a network of computers for easy, reliable access any-
time, anywhere. Freenet[4], Gnutella[5] and M-o-o-t[12]
are some of the better known systems used by mil-
lions of users around the world and many others, such as
OceanStore[9], Chord[17] and Pastry[16], are in develop-
ment at research laboratories. These systems are designed
to provide reliable, distributed, and sometimes anony-
mous storage networks. A criminal can use these very sys-
tems to hide software tools and documents that might
be useful for his prosecution, just as easily as any other
user can save a ﬁle. Most peer-to-peer systems asso-
ciate a unique key, either assigned by the user or generated
automatically, with each document they store. Hence, a per-
son can split a document into fragments and store each
fragment in a peer-to-peer system using a sequence of se-
cret phrases as keys, such that he can easily splice the
fragments together knowing the proper sequence of se-
cret phrases. For instance, in Freenet one can assign each
fragment a unique URL. Since URLs are user friendly key-
words it is easy to recall the proper sequence to retrieve and
splice the fragments together. It is, however, difﬁcult to re-
construct the original document without the knowledge of
the proper sequence even if the keywords are known.
From the above discussion, it is clear that digital evi-
dence can easily take a variety of forms and be scattered
into hundreds of fragments making reassembly a daunting
task for a human analyst. To address this problem we pro-
pose a general process model and present a speciﬁc solution
to reassembling scattered evidence. Assuming that the nec-
essary evidence is collected entirely, the proposed model
has three steps:
1. Preprocessing: Encrypting or compressing digital ev-
idence removes structural details that can assist an an-
alyst in reassembling the evidence. During preprocess-
ing, evidence has to be cryptanalyzed and transformed
to its original form. Some cryptographic schemes de-
rive their keys based on user passwords. Since users
tend to choose dictionary based passwords it is quite
feasible to attack the password and obtain the key re-
gardless of the size of the key. Besides, brute force at-
tacks on even some of the sophisticated cryptographic
algorithms, such as DES, are shown to be feasible[6].
Note that a forensic analyst may not be too constrained
on time, making cryptanalysis a feasible process.
2. Collating: Although, in this paper, we consider re-
assembling a single document, in reality evidence is
usually a collection of mixed fragments of several doc-
uments of different types. To reassemble the evidence
efﬁciently fragments that belong to a document must
be grouped together. A hierarchical approach to collat-
ing can be used to effectively group similar fragments
together. Fragments can be initially grouped by super-
ﬁcial characteristics, such as binary or plain-text docu-
ment, and later sophisticated text-categorization tech-
niques along with special knowledge about the frag-
ments can be used to further reﬁne the results.
3. Reassembling: The ﬁnal step in the process is to either
reassemble the document to its original form or to pro-
vide enough information about the original form to re-
duce the work of a forensic analyst. Ideally, we would
like to obtain the proper sequence of fragments that
resembles the original document. Even if the process
identiﬁes a small number of potential orderings, from
which the forensic analyst can derive the proper order-
ing, it would result in considerable savings in time and
effort to the analyst.
In this paper we focus on the ﬁnal step, that is, reassem-
bling a document given preprocessed fragments of that doc-
ument. The rest of this paper is organized as follows: in the
following section we describe the problem formally and in-
troduce a general technique for document reassembly. Sec-
tion 3 presents a speciﬁc realization of the general technique
and initial experimental results and we conclude in section
4 with a discussion on future work.
2. The Fragment Reassembly Problem
In this section we formulate the document fragment re-
assembly problem in a more rigorous manner and describe
a general approach for a solution to the problem.
Proceedings of the 19th Annual Computer Security Applications Conference (ACSAC 2003) 
1063-9527/03 $17.00 © 2003 IEEE
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 07:46:44 UTC from IEEE Xplore.  Restrictions apply. 
2.1. Statement of the Problem
the sum
The problem of reassembly of scattered document can
be stated as follows: suppose we have a set {A0, A1 . . . An}
of fragments of a document A. We would like to compute
a permutation π such that A = Aπ(0)||Aπ(1)|| . . . Aπ(n),
where || denotes the concatenation operator. In other words,
we would like to determine the order in which fragments
Ai need to be concatenated to yield the original document
A. We assume fragments are recovered without loss of data,
that is, concatenation of fragments in the proper order yields
the original document intact.
Note that in order to determine the correct fragment re-
ordering, we need to identify fragment pairs that are adja-
cent in the original document. One simple technique to do
this, is to use a dictionary of the underlying language of the
document. With this approach, a fragment Aj would be con-
sidered to be a likely candidate fragment to follow Ai if the
word that straddles the boundaries of Ai and Aj is found
in the dictionary. For example, suppose fragment Ai ends
with the phrase “The quick bro” and fragment Aj, Ak be-
gin with the phrase “wnfoxjumps...” and “overlazydog...”
respectively. It is clear that Aj is a better candidate to follow
Ai than Ak as the word that straddles the boundary of frag-
ments Ai, Aj forms a dictionary word, whereas the word
that straddles the boundary of fragments Ai, Ak does not.
However, a dictionary-based approach is language spe-
ciﬁc and it is therefore not feasible for the variety of docu-
ments a forensic analyst may come across in the ﬁeld. Fur-
thermore, for non-textual ﬁles, like executables, a dictionary
may not be readily available or easy to construct. Finally, if
more than one dictionary matches are found then how does
a forensic analyst select between the two?
To quantify the likelihood of adjacency a linguist may
assign candidate probabilities C(i,j), representing the prob-
ability that the fragment Aj follows Ai, based on syntac-
tic and semantic analysis for each pair of fragments. Once
these probabilities are assigned, the permutation of the frag-
ments that leads to correct reassembly, among all possible
permutations, is likely to maximize the product of candidate
probabilities of adjacent fragments. This observation gives
us a technique to identify the correct reassembly with high
probability. More formally, we want to compute the permu-
tation π such that the value
n−1(cid:1)
i=0
C(π(i), π(i + 1))
(1)
is maximized over all possible permutations π of degree n.
This permutation is most likely to be the one that leads to
correct reconstruction of the document. Note that maximiz-
ing the product in equation (1) is equivalent to maximizing
n−1(cid:2)
i=0
− log C(π(i), π(i + 1))
(2)
The problem of ﬁnding a permutation that maximizes
the sum in equation (2) can also be abstracted as a graph
problem. To do this we take the set of all candidate prob-
abilities (Ci,j) to form an adjacency matrix of a complete
weighted graph of n vertices, where vertex i represents frag-
ment i and the edge weights quantify the candidate proba-
bility of two corresponding fragments being adjacent. The
proper sequence π is a path in this graph that traverses all
the nodes and maximizes the sum of candidate probabilities
along that path. The problem of ﬁnding this path is equiv-
alent to ﬁnding a maximum weight Hamiltonian path in a
complete graph (See Figure 1) and the optimum solution to
the problem turns out to be intractable[2]. However there
are many heuristics known in the literature and we employ
one such heuristic as discussed in Section 3.2.
C
0.73
0.01
0.03
D
B
0.95
0.30
0.02
0.95
0.50
0.85
A
0.05
E
Figure 1. A Complete Graph of Five Frag-
ments & Hamiltonian Path (ACBED) that Max-