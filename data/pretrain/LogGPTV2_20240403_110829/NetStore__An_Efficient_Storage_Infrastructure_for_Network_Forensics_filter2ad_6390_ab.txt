(A) “What is the total number of connections for the entire network between
dates d1 and d2?”
dates d1 and d2?”
(SA) “What is the number of bytes that host X sent between dates d1 and d2?”
(RA) “What is the number of hosts that each of the hosts in a subnet contacted
between dates d1 and d2?”
3.2 Column Oriented Storage
Columns. In NetStore, we consider that ﬂow records with n attributes are
stored in the logical table with n columns and an increasing number of rows
(tuples) one for each ﬂow record. The values of each attribute are stored in one
column and have the same data type. By default almost all of the values of
a column are not sorted. Having the data sorted in a column might help get
better compression and faster retrieval, but changing the initial order of the el-
ements requires the use of auxiliary data structure for tuple reconstruction at
query time. We investigated several techniques to ease tuple reconstruction and
all methods added much more overhead at query time than the beneﬁt of bet-
ter compression and faster data access. Therefore, we decided to maintain the
same order of elements across columns to avoid any tuple reconstruction penalty
when querying. However, since we can aﬀord one column to be sorted without
the need to use any reconstruction auxiliary data, we choose to ﬁrst sort only
one column and partially sort the rest of the columns. We call the ﬁrst sorted
column the anchor column. Note that after sorting, given our storage architec-
ture, each segment can still be processed independently. The main purpose of the
anchor column choosing algorithm is to select the ordering that facilitates the
best compression and fast data access. Network ﬂow data express strong correla-
tion between several attributes and we exploit this characteristic by keeping the
strongly correlated columns in consecutive sorting order as much as possible for
better compression results. Additionally, based on previous queries data access
pattern, columns are arranged by taking into account the probability of each
column to be accessed by future queries. The columns with higher probabilities
are arranged at the beginning of the sorting order. As such, we maintain the
counting probabilities associated with each of the columns given by the formula
P (ci) = ai
t , where ci is the i-th column, ai the number of queries that accessed
ci and t the total number of queries.
NetStore: An Eﬃcient Storage Infrastructure
283
Segments. Each column is further partitioned into ﬁxed sets of values called
segments. Segments partitioning enables physical storage and processing at a
smaller granularity than simple column based partitioning. These design deci-
sions provide more ﬂexibility for compression strategies and data access. At query
time only used segments will be read from disk and processed based on the in-
formation collected from segments metadata structures called index nodes. Each
segment has associated a unique identiﬁer called segment ID. For each column, a
segment ID represents an auto incremental number, started at the installation of
the system. The segment sizes are dependent of the hardware conﬁguration and
can be set in such a way to use the most of available main memory. For better
control over data structures used, the segments have the same number of values
across all the columns. In this way there is no need to store a record ID for each
value of a segment, and this is one major diﬀerence compared to some existing
column stores [11]. As we will show in Section 4 the performance of the system
is related to the segment size used. The larger the segment size, the better the
compression performance and query processing times. However, we notice that
records insertion speed decreases with the increase of segment size, so, there is a
trade oﬀ between the query performance desired and the insertion speed needed.
Most of the columns store segments in compressed format and, in a later section
we present the compression algorithms used. Column segmentation design is an
important diﬀerence compared to traditional row oriented systems that process
data a tuple at a time, whereas NetStore processes data segment at a time, which
translates to many tuples at a time. Figure 3 shows the processing steps for the
three processing phases: buﬀering, segmenting and query processing.
Fig. 2. NetStore main components:
Processing Engine and Column-Store.
Fig. 3. NetStore processing phases: buﬀer-
ing, segmenting and query processing.
Column Index. For each column we store the meta data associated with each of
the segments in an index node corresponding to the segment. The set of all index
nodes for the segments of a column represent the column index. The information
in each index node includes statistics about data and diﬀerent features that are
used in the decision about the compression method to use and optimal data
284
P. Giura and N. Memon
access, as well as the time interval associated with the segment in the format
[min start time, max end time]. Figure 4 presents an intuitive representation
of the columns, segments and index for each column. Each column index is
implemented using a time interval tree. Every query is relative to a time window
T. At query time, the index of every column accessed is looked up and only
the segments that have the time interval overlapping window T are considered
for processing. In the next step, the statistics on segment values are checked
to decide if the segment should be loaded in memory and decompressed. This
two-phase index processing helps in early ﬁltering out unused data in query
processing similar to what is done in [15]. Note that the index nodes do not
hold data values, but statistics about the segments such as the minimum and
the maximum values, the time interval of the segment, the compression method
used, the number of distinct values, etc. Therefore, index usage adds negligible
storage and processing overhead.
From the list of initial queries we observed that the column for the source IP
attribute is most frequently accessed. Therefore, we choose this column as our
ﬁrst sorted anchor column, and used it as a clustered index for each source IP
segment. However, for workloads where the predominant query types are spot
queries targeting a speciﬁc column other than the anchor column, the use of
indexes for values inside the column segments is beneﬁcial at a cost of increased
storage and slowdown in insertion rate. Thus, this situation can be acceptable
for slow networks were the insertion rate requirements are not too high. When
the insertion rate is high then it is best not to use any index but rely on the
meta-data from the index nodes.
Internal IPs Index. Besides the column index, NetStore maintains another
indexing data structure for the network internal IP addresses called the Internal
IPs index. Essentially the IPs index is an inverted index for the internal IPs. That
is, for each internal IP address the index stores in a list the absolute positions
where the IP address occurs in the column, sourceIP or destIP , as if the column
is not partitioned into segments. Figure 5 shows an intuitive representation of the
IPs index. For each internal IP address the positions list represents an array of
increasing integer values that are compressed and stored on disk on a daily basis.
Because IP addresses tend to occur in consecutive positions in a column, we chose
to compress the positions list by applying run-length-encoding on diﬀerences
between adjacent values.
3.3 Compression
Each of the segments in NetStore is compressed independently. We observed that
segments within a column did not have the same distribution due to the temporal
variation of network activity in working hours, days, nights, weekends, breaks
etc. Hence segments of the same column were best compressed using diﬀerent
methods. We explored diﬀerent compression methods. We investigated methods
that allow data processing in compressed format and do not need decompression
of all the segment values if only one value is requested. We also looked at methods
NetStore: An Eﬃcient Storage Infrastructure
285
Fig. 4. Schematic representation of columns,
segments, index nodes and column indexes
Fig. 5. Intuitive representation of the
IPs inverted index
that provide fast decompression and reasonable compression ratio and speed.
The decision on which compression algorithm to use is done automatically for
each segment, and is based on the data features of the segment such as data type,
the number of distinct values, range of the values and number of switches between
adjacent values. We tested a wide range of compression methods, including some
we designed for the purpose or currently used by similar systems in [1,16,21,11],
with needed variations if any. Below we list the techniques that emerged eﬀective
based on our experimentation:
– Run-Length Encoding (RLE): is used for segments that have few distinct
repetitive values. If value v appears consecutively r times, and r > 1, we
compress it as the pair (v, r). It provides fast compression as well as the
ability to process data in compressed format.
– Variable Byte Encoding: is a byte-oriented encoding method used for
positive integers. It uses a variable number of bytes to encode each integer
value as follows: if value < 128 use one byte (set highest bit to 0), for
value < 128 ∗ 128 use 2 bytes (ﬁrst byte has highest bit set to 1 and second
to 0) and so on. This method can be used in conjunction with RLE for
both values and runs. It provides reasonable compression ratio and good
decompression speed allowing the decompression of only the requested value
without the need to decompress the whole segment.
– Dictionary Encoding: is used for columns with few distinct values and
sometimes before RLE is applied (e.g. to encode “protocol” attribute).
– Frame Of Reference: considers the interval bounded by the minimum
and maximum values as the frame of reference for the values to be com-
pressed [7]. We use it to compress non-empty timestamp attributes within a
segment (e.g. start time, end time, etc.) that are integer values representing
the number of seconds from the epoch. Typically the time diﬀerence be-
tween minimum and maximum timestamp values in a segment is less than
few hours, therefore the encoding of the diﬀerence is possible using short
values of 2 bytes instead of integers of 4 bytes. It allows processing data
in compressed format by decompressing each timestamp value individually
without the need to decompress the whole segment.
286
P. Giura and N. Memon
– Generic Compression: we use the DEFLATE algorithm from the zlib
library that is a variation of the LZ77 [20]. This method provides compression
at the binary level, and does not allow values to be individually accessed
unless the whole segment is decompressed. It is chosen if it enables faster
data insertion and access than the value-based methods presented earlier.
– No Compression: is listed as a compression method since it will represent
the base case for our compression selection algorithm.
Method Selection. The selection of a compression method is done based on
the statistics collected in one pass over the data of each segment. As mentioned
earlier, the two major requirements of our system are to keep records insertion
rates high and to provide fast data access. Data compression does not always
provide better insertion and better query performance compared to “No com-
pression”, and for this we developed a model to decide on when compression is
suitable and if so, what method to choose. Essentially, we compute a score for
each candidate compression method and we select the one that has the best score.
More formally, we assume we have k + 1 compression methods m0, m1, . . . , mk,
with m0 being the “No Compression” method. We then compute the insertion
time as the time to compress and write to disk, and the access time, to read from
disk and decompress, as functions of each compression method. For value-based
compression methods, we estimate the compression, write, read and decompres-
sion times based on the statistics collected for each segment. For the generic
compression we estimate the parameters based on the average results obtained
when processing sample segments. For each segment we evaluate:
insertion (mi) = c (mi) + w (mi) ,
access (mi) = r (mi) + d (mi) ,
i = 1, . . . , k
i = 1, . . . , k
As the base case for each method evaluation we consider the “No Compression”
method. We take I0 to represent the time to insert an uncompressed segment
which is represented by only the writing time since there is no time spent for
compression and, similarly A0 to represent the time to access the segment which
is represented by only the time to read the segment from disk since there is no
decompression. Formally, following the above equations we have:
insertion (m0) = w (m0) = I0 and access (m0) = r (m0) = A0
We then choose the candidate compression methods mi only if we have both:
insertion (mi) < I0 and access (mi) < A0
Next, among the candidate compression methods we choose the one that pro-
vides the lowest access time. Note that we primarily consider the access time
as the main diﬀerentiator factor and not the insertion time. The disk read is
the most frequent and time consuming operation and it is many times slower
than disk write of the same size ﬁle for commodity hard drives. Additionally,
insertion time can be improved by bulk loading or by other ways that take into
account that the network traﬃc rate is not steady and varies greatly over time,
NetStore: An Eﬃcient Storage Infrastructure
287
whereas the access mechanism should provide the same level of performance at
all times.
The model presented above does not take into account if the data can be
processed in compressed format and the assumption is that decompression is
necessary at all times. However, for a more accurate compression method selec-
tion we should include the probability of a query processing the data in com-
pressed format in the access time equation. Since forensic and monitoring queries
are usually predictable, we can assume without aﬀecting the generality of our
system, that we have a total number of t queries, each query qj having the proba-
bility of occurrence pj with
pj = 1. We consider the probability of a segment
t(cid:2)
j=1
s being processed in compressed format as the probability of occurrence of the
queries that process the segment in compressed format. Let CF be the set of all
the queries that process s in compressed format, we then get:
P (s) =
pj, CF = {qj|qj processes s in compressed format}
(cid:2)
qj∈CF
Now, a more accurate access time equation can be rewritten taking into account
the possibility of not decompressing the segment for each access:
access (mi) = r (mi) + d (mi) · (1 − P (s)) ,
i = 1, . . . , k
(1)