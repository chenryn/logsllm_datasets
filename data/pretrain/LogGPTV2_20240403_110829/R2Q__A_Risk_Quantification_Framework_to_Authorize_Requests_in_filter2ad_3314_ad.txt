r)” scenario in table 4.3: 20· 0.19· 10 = 38.
In conclusion, arithmetical doubling of scenario parameters has the same – or
almost the same in case of the “double components (with ﬁxed px
r)” scenario –
effect on the mean potential losses μ. Therefore, it is important to look at the
distribution of the potential losses and incorporate other characteristics, such as
the standard deviation σ and the Value-at-Risk, into the decision process (McNeil
et al., 2005, p. 26).
(cid:6)
(cid:11)(cid:8)
1− p
¯p =1− ∏
(cid:11)(cid:8)2|K|
=1−(cid:6)
k∈K(cid:11)
(cid:3)(cid:3)2|K|
(cid:2)
(cid:2)
1− p
1− |K|(cid:7)
1− ˆp
1−
=1−
(cid:3)2|K|
(cid:2)
1− 1 + |K|(cid:7)
=1−
1− ˆp
(cid:2)
(cid:3)2|K|
|K|(cid:7)
=1−
1− ˆp
=1− (1− ˆp)2
(4.23)
(4.24)
(4.25)
(4.26)
(4.27)
4.2 Simulations
107
The only parameter which directly changes the number of cost values on the
ﬁnal distribution’s x-axis (#c) is the number of risks R. For every added risk, the
number of values increases and the shape of the distribution appears to be “ﬁner”.
Furthermore, it could be shown that the cost-related values have a linear effect
on the resulting characteristics. If all potential losses of each risk are increased
by a factor, the characteristics increase by the same factor. On the contrary, if the
occurrence probabilities are increased by the factor two, the σ and Value-at-Risk
characteristics only increase by approximately 150%. Finally, it could be shown,
that the number of scenario components (with ﬁxed individual occurrence prob-
abilities px
r) has an effect that is less than the effect of changing the occurrence
probabilities.
These ﬁndings indicate that decision makers need to be aware of the fact that
scenarios are more sensitive to changes in the amount of the potential losses,
while changes to the occurrence probabilities or the number of risks have a less
strong effect on the resulting distribution. The least strong effect is related to the
number of considered components in the scenario.
Table 4.3 Sensitivity to Arithmetical Doubling of Scenario Parameters
Scenario
#c μ
σ VaR (α=0.9)
Base
Double risks
Double costs
Double probability
Double components (with ﬁxed ˆpx
Double components (with ﬁxed px
21 20 13.416
41 40 18.974
21 40 26.833
21 40 17.889
r) 21 20 13.416
r) 21 38 17.545
40
60
80
60
40
60
4 Risk Quantiﬁcation Framework
p
1
0
R1 and R2
0
20
40
c
p
1
0
R1 and R2 and R3
0
10
20
30
40
50
c
108
p
1
0
p
1
0
p
1
0
R1
0
20
R2
0
20
R3
0
10
c
c
c
Figure 4.9 Calculation of the Joint Density Function With Rounding (a = 10)
4.2.3 Trade-off: Accuracy and Performance
Every user of an investment assessment model faces the trade-off between accu-
racy of estimation and expenditure for the elicitation of the input data.
On the one hand, a reduction of the requested accuracy can accelerate the col-
lection of data. Experience from expert interviews conducted to evaluate the model
indicates for example that the potential losses are more difﬁcult to estimate than
the occurrence probabilities which can at least always be classiﬁed on a scale from
low to high. In the following, we focus on the effect of less accurate cost values.
However, structurally similar considerations are also possible for the occurrence
probabilities.
On the other hand, our model allows that a lower accuracy can lead to increased
performance of the calculations, because of the special problem structure: The
number of possible cost values on the x-axis does not grow exponentially, because
more and more cost values add up to the same sum.
For instance, compare ﬁgures 4.2 and 4.9. The only difference of ﬁgure 4.9 is
that all cost values on the x-axes have been rounded to the nearest multiple of 10.
Figure 4.2 does not use rounding and contains potential losses of 15 for R1. As
both distributions, R1 and R2, now contain the same cost value (i. e., 20), the re-
sulting joined distribution contains one value less on its x-axis. When this joined
distribution is further joined with R3, the overall discrete probability density func-
tion contains six instead of eight cost values on the x-axis.
As another example, if we draw 16 random cost values ∈ N from [1; 1,000],
their sum is distributed in [16; 16,000] and the values in the middle of this interval
are more likely. Note that this effect is much stronger for distributions where cer-
4.2 Simulations
109
]
s
m
[
s
n
o
i
t
u
b
i
r
t
s
i
D
0
0
0
,
1
e
t
a
l
u
c
l
a
C
o
t
e
m
T
i
Power Set
Hierarchical (a=1)
Hierarchical (a=5)
Hierarchical (a=10)
Hierarchical (a=25)
Hierarchical (a=50)
Hierarchical (a=100)
240
235
230
225
220
215
210
25
 10
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 38
 40
Number of Total Risks R
Figure 4.10 Performance of the Power Set and the new Hierarchical Approach
tain values occur more frequently than others, i. e., distributions with a relatively
small variance.
Figure 4.3 shows the hierarchy for 16 risks. For 32 risks, the last calculation step
would be to calculate the joint probability distribution for risks 1 to 16 and risks 17
to 32, which could both contain up to 65,536 cost values on the x-axis. This would
correspond to almost 4.3 billion (= 65,5362) multiplications. However, because the
initial costs values ∈ N were drawn from [1; 1,000], there can be 16,000 different
values at most on each x-axis and the calculation would take at most 256 million
(=16,0002) multiplications. This effect gets stronger, if more risks are considered
or the number of different cost values is reduced.
Therefore, an important effect of our approach is that it is possible to speed up
the calculations by combining similar cost values, as this also reduces the number
of values on the distributions’ x-axes. The combining is done once, before the
hierarchical approach starts with joining the distributions. Therefore, we introduce
the rounding parameter a, which is 1 per default, and round every cost value to be
a multiple of a before the joins are calculated. For example, this means that, while
it is possible to calculate with costs such as 101.05, the calculations will be faster
if only rounded values such as 100.0 are used. This corresponds to less accurate –
and therefore cheaper – estimation of the cost values.
In order to analyze the speedup caused by the hierarchical approach and round-
ing the cost values, we measured the time it took to calculate 1,000 generated
scenarios on one core of an AMD Opteron 8356 with 2.3 GHz. The cost values
∈ N for each risk have been drawn randomly from [1; 1,000] and then rounded
110
4 Risk Quantiﬁcation Framework
Table 4.4 Speedup (for R = 40) Compared to Power Set and to Hierarchical Approach (a = 1)
Speedup compared Speedup compared
to Hierarchical
with a = 1
to Power Set
Hierarchical (a = 1)
Hierarchical (a = 5)
Hierarchical (a = 10)
Hierarchical (a = 25)
Hierarchical (a = 50)
Hierarchical (a = 100)
2,182
49,594
277,492
2,435,834
12,027,334
58,065,533
-
23
127
1,116
5,511
26,606
according to the parameter a. Due to very long calculation times, the curve for
the power set-based algorithm in ﬁgure 4.10 has been extrapolated based on one
generated scenario for the cases with more than 25 total risks, as it can easily be
shown, that the calculation time approximately doubles for every added risk. All
other data points are based on 1,000 randomly generated scenarios.
Between 30 and 40 risks, the calculation time increases by approximately 23%
for every added risk (for a = 1) instead of 100% for the power set-based algorithm.
Table 4.4 shows the gained speedup for calculating 1,000 distributions with 40
risks.
The middle column of table 4.4 shows that the presented hierarchical approach
calculates the 1,000 probability density functions for 40 risks 58 million times
faster than the power set-based algorithm. The right column shows that, using
the hierarchical algorithm, there is a trade-off between speed and accuracy of the
calculations. If all cost values are rounded to be multiples of 100, the 1,000 calcu-
lations are done more than 26 thousand times faster compared to calculation with
costs values that have not been rounded. However, even if the effect of rounding
is neglected (i. e., a = 1) the hierarchical approach is still more than 2,000 times
faster than the power set-based algorithm.
In order to analyze the inaccuracy caused by rounding the cost values, we mea-
sured the difference between the calculated distributions using the following met-
ric: The two initial discrete probability density functions are converted into cu-
mulative distribution functions. This is done in order to make the discrete func-
tion deﬁned for all cost values. The resulting step functions can be visualized like
shown in ﬁgure 4.11. Subsequently, we add up the areas where the two step func-
tions differ. This absolute difference is then normalized to the relative difference
(in percent) by division by the total area below the step functions. In our example
in ﬁgure 4.11, the relative difference would be ((1.53+0.27+1.02+0.18) / 50) = 6%.
4.2 Simulations
111
1.0
0.8
0.6
0.4
0.2
0.0
H ierarchical (a=1)
H ierarchical (a=10)
Difference
0
5
40
Figure 4.11 Example for the Difference Metric Between two Step Functions
10
15
35
20
25
30
45
50
Figure 4.12 shows the relative difference between 1,000 randomly generated
distributions calculated based on exact and rounded (a = 100) cost values. Various
quantiles, as well as minimum, maximum, and average for the 1,000 iterations are
shown. The red line shows the maximum of measured difference and therefore
ﬂuctuates more than the other measurements. It can be clearly seen that there is
a downward trend with increasing number of risks. Starting from 25 risks, 90%
of the 1,000 measured differences (yellow) were already below 1%. For 40 risks,
the maximum difference dropped below 2%, while the average difference (blue)
was 0.3%.
This is interesting, as the differences were calculated using the highest value
for the rounding parameter (a = 100) of our performance measurement shown in
ﬁgure 4.10. As we have pointed out, rounding the cost values with that parameter
provided a speedup of up to 58 million, while the average difference, i. e., the in-
troduced inaccuracy, was below 0.3%. Especially for large scenarios, it is therefore
advisable to round the values, as the distributions’ differences remain small, even
when high rounding parameters a are used. It might be the best approach to start
with a larger a and decrease the rounding parameter a gradually when analyzing a
given scenario. This allows getting quick results which are less accurate and then
increase the accuracy step by step in order to reﬁne the results.
In order to analyze whether rounding the cost values is a suitable method to
manage larger scenarios that consist of many risks related to the components, we
measured the size of solvable scenarios in a given time period.
Therefore, we generated scenarios including 1,000 services and 1,000 data
transfers. In each iteration, we started with one risk and measured the time it took
to calculate the ﬁnal probability function of the potential losses on one core of an
112
]
%
[
0
0
1
=
a
d
n
a
1
=
a
n
e
e
w
t
e
b
e
c
n
e
r
e
f
f
i
D
 4
 3.5
 3
 2.5
 2
 1.5
 1
 0.5
 0
 10
4 Risk Quantiﬁcation Framework
Max
Q90
Q75
Mean
Q25
Q10
Min
 12
 14
 16
 18
 20
 22
 24
 26
 28
 30
 32
 34
 36
 38
 40
Number of Total Risks R
Figure 4.12 Accuracy Plot (a = 100)
AMD Opteron 8356 with 2.3 GHz. If the calculation’s duration was less than 10