: Binary match: Stats: Binary action: Stats or binary match: Ternary action: Unuseda)b)c)speciﬁc table conﬁgurations, but for a wide range of other
conﬁgurations as well.
5.4 Costs of Action Programmability
Besides the costs of the instruction RAMs and action
memories, there are around 7000 processor datapaths rang-
ing in width from 8 to 32 bits. Fortunately, because they use
a simple RISC instruction set, their combined area consumes
only 7% of the chip.
5.5 Area and Power Costs
This switch is designed with large match table capacity,
so match and action memories contribute substantially to
chip area estimates as shown in Table 3. The ﬁrst item,
which includes IO, data buﬀer, CPU, etc., occupies a similar
area in conventional switches. As can be seen, the VLIW
action engine and parser/deparser contributions to area are
relatively small.
We suggested earlier that the match stage unit RAMs
suﬀered a 14% area penalty compared to the best possible
RAMs. Given this penalty to the match stage SRAM (not
TCAM) area, and some allowance for additional bitcount vs
conventional switches (15%), excess memory area is about
8% of the chip total. If excess logic in the parser and action
engine add another 6.2%, a 14.2% area cost results, justify-
ing the earlier claim of a less than 15% cost diﬀerential.
Section
IO, buﬀer, queue, CPU, etc
Match memory & logic
VLIW action engine
Parser + deparser
Area
37.0%
54.3%
7.4%
1.3%
Total extra cost:
Cost
0.0%
8.0%
5.5%
0.7%
14.2%
Table 3: Estimated chip area proﬁle.
Section
I/O
Memory leakage
Logic leakage
RAM active
TCAM active
Logic active
Power
26.0%
43.7%
7.3%
2.7%
3.5%
16.8%
Total extra cost:
Cost
0.0%
4.0%
2.5%
0.4%
0.0%
5.5%
12.4%
Table 4: Estimated chip power proﬁle.
Estimated switch power is detailed in Table 4 under worst
case operating conditions (temperature, chip process), 100%
traﬃc with a mix of half min and half max (1.5 kB) size
packets, and all match and action tables ﬁlled to capacity.
Input/output power is equivalent to a conventional switch.
Memory leakage power is proportional to memory bitcount,
so if this programmable switch can be implemented with
equivalent bitcount to a conventional switch, power will be
comparable. The remaining items, totalling 30%, will be
less in a conventional switch because of the reduced func-
tionality in its match-action pipeline. We estimate that our
programmable chip dissipates 12.4% more power than a con-
ventional switch, but is performing much more substantial
packet manipulation.
The overall competitive evaluation with conventional swit-
ches suggests that equivalent functions can be performed
by this switch with equivalent memory bitcounts. This in
turn drives parity in dominant aspects of chip cost and
power. The additional power and area costs borne by the
programmable solution are quite small given the more com-
prehensive functionality of the switch.
6. RELATED WORK
Flexible processing is achievable via many mechanisms.
Software running on a processor is a common choice. Our
design performance exceeds that of CPUs by two orders of
magnitude [7], and GPUs and NPUs by one order [5,9,12,29].
Modern FPGAs, such as the Xilinx Virtex-7 [35], can for-
ward traﬃc at nearly 1 Tb/s. Unfortunately, FPGAs oﬀer
lower total memory capacity, simulate TCAMs poorly, con-
sume more power, and are signiﬁcantly more expensive. The
largest Virtex-7 device available today, the Virtex-7 690T,
oﬀers 62Mb of total memory which is roughly 10% of our
chip capacity. The TCAMs from just two match stages
would consume the majority of lookup-up tables (LUTs)
that are used to implement user-logic. The volume list price
exceeds $10,000, which is an order of magnitude above the
expected price of our chip. These factors together rule out
FPGAs as a solution.
Related to NPUs is PLUG [6], which provides a number of
general processing cores, paired with memories and routing
resources. Processing is decomposed into data ﬂow graphs,
and the ﬂow graph is distributed across the chip. PLUG
focuses mainly on implementing lookups, and not on parsing
or packet editing.
The Intel FM6000 64 port × 10Gb/s switch chip [24] con-
tains a programmable parser built from 32 stages with a
TCAM inside each stage. It also includes a two-stage match-
action engine, with each stage containing 12 blocks of 1K ×
36b TCAM. This represents a small fraction of total table
capacity, with other tables in a ﬁxed pipeline.
The latest OpenFlow [31] speciﬁcation provides an MMT
abstraction and partly implements an RMT model. But its
action capability is still limited, and it is not certain that a
standard for functionally complete actions is on the way or
even possible.
7. CONCLUSIONS
Ideally, a switch or router should last for many years.
Dealing with a changing world requires programmability that
allows software upgrades to add new functions and new pro-
tocols in the ﬁeld. Network processors (NPUs) were intro-
duced to support this vision, but neither NPUs or GPUs
have come close to achieving the speeds of ﬁxed function
switches using ASICs; nor have we seen a case study of re-
programming an NPU-based router such as Cisco’s CRS-1 to
add a new protocol. Likewise FPGAs, which only recently
approached ASIC forwarding speeds, remain prohibitively
expensive.
Our chip design resurrects this ancient vision of programma-
bility, expressed in the RMT model, within the constraints
of what is possible on a real chip. New ﬁelds can be added,
lookup tables can be reconﬁgured, new header processing
added, all through software reconﬁguration. While our chip
cannot do regular expressions, or manipulate packet bodies,
a box built from this chip could metamorphose from an Eth-
ernet chip on Tuesday to a ﬁrewall on Wednesday and to a
completely new device on Thursday, all by the right software
upgrades. The challenge is to do this today at a capacity
approaching a terabit. The chip design we propose has sur-
prising specs: it contains 7,000 processor datapaths, 370 Mb
of SRAM, and 40 Mb of TCAM, across 32 processing stages.
In terms of ideas, we single out the RMT model as a pow-
erful way to map the programmer’s desired forwarding be-
havior onto a pipeline built from a ﬂexible parser, a conﬁg-
urable arrangement of logical match stages with memories
of arbitrary width and depth, and ﬂexible packet editing.
These abstractions require new algorithms to be eﬃciently
implemented at terabit speeds. Our use of memory blocks
that can be ganged within or across stages is key to realizing
the vision of reconﬁgurable match tables; our large scale use
of TCAM greatly increases matching ﬂexibility; and ﬁnally,
our use of fully-parallel VLIW instructions is key to packet
editing. Our design suggests that this greatly increased ﬂex-
ibility comes at an additional cost of less than 15% in area
and power consumption of the chip. Ultimately, the im-
plementation challenge which we have addressed that may
not be apparent to SIGCOMM audiences is producing an
architecture which is possible to wire eﬃciently on chip.
While the OpenFlow speciﬁcation hints at RMT and sev-
eral researchers [17] have actively pursued this dream, RMT
models remained theoretical without an existence proof of
a chip design that works at terabit speeds. Our paper con-
tributes a concrete RMT proposal and a proof of its feasi-
bility. Clearly it is possible to go further and remove some
of the restrictions we have imposed for implementation rea-
sons. But now the conversation can begin.
Acknowledgements
We would like to thank Nick Feamster for feedback on an
early draft, Aditya Akella for shepherding the ﬁnal version,
and the anonymous reviewers for their useful comments.
8. REFERENCES
[1] P. Bosshart. Low power TCAM. US Patent 8,125,810,
Feb. 2012.
[2] P. Bosshart et al. Forwarding Metamorphosis: Fast
Programmable Match-Action Processing in Hardware
for SDN (Extended Version). 2013. http://yuba.
stanford.edu/~grg/docs/chip-techreport-2013.pdf.
[3] Brocade. Software-Deﬁned Networking.
http://www.brocade.com/launch/sdn/index.html.
[4] F. Chung et al. Parallelism versus memory allocation
in pipelined router forwarding engines. Theory of
Computing Systems, 39(6):829–849, 2006.
[5] Cisco. QuantumFlow Processor.
http://newsroom.cisco.com/dlls/2008/hd_030408b.html.
[6] L. De Carli et al. PLUG: ﬂexible lookup modules for
rapid deployment of new protocols in high-speed
routers. SIGCOMM ’09.
[7] M. Dobrescu et al. RouteBricks: exploiting parallelism
to scale software routers. In Proc. SOSP ’09.
[8] N. Dukkipati. Rate Control Protocol (RCP). PhD
thesis, Stanford University, 2008.
[9] EZchip. NP-5 Network Processor.
http://www.ezchip.com/p_np5.htm.
[11] J. Fu and J. Rexford. Eﬃcient IP-address lookup with
a shared forwarding table for multiple virtual routers.
In Proc. ACM CoNEXT ’08.
[12] S. Han et al. PacketShader: a GPU-accelerated
software router. SIGCOMM ’10.
[13] U. H¨olzle. OpenFlow @ Google. In Open Networking
Summit, April 2012. http://opennetsummit.org/
archives/apr12/hoelzle-tue-openflow.pdf.
[14] HP. OpenFlow – Software-Deﬁned Network (SDN).
http://www.hp.com/OpenFlow/.
[15] IEEE Std 802.1ag-2007: Amendment 5: Connectivity
Fault Management. 2007.
[16] IEEE Std 802.1ah-2008: Amendment 7: Provider
Backbone Bridges. 2008.
[17] IETF. RFC 5810 ForCES Protocol Speciﬁcation,
March 2010.
[18] IETF. RFC 6052 IPv6 Addressing of IPv4/IPv6
Translators, October 2010.
[19] IETF. NVGRE: Network Virtualization using Generic
Routing Encapsulation, Feb. 2013. https://tools.ietf.
org/html/draft-sridharan-virtualization-nvgre-02.
[20] IETF. Overlay Transport Virtualization, Feb. 2013.
https://tools.ietf.org/html/draft-hasmit-otv-04.
[21] IETF. A Stateless Transport Tunneling Protocol for
Network Virtualization (STT), Mar. 2013.
https://tools.ietf.org/html/draft-davie-stt-03.
[22] IETF. VXLAN: A Framework for Overlaying
Virtualized Layer 2 Networks over Layer 3 Networks,
May 2013. https://tools.ietf.org/html/
draft-mahalingam-dutt-dcops-vxlan-04.
[23] Indigo – Open Source OpenFlow Switches.
http://www.openflowhub.org/display/Indigo/Indigo+-+
Open+Source+OpenFlow+Switches.
[24] Intel Ethernet Switch Silicon FM6000.
http://ark.intel.com/products/series/64370.
[25] ITU-T. OAM Functions and Mechanisms for Ethernet
Based Networks G.8013/Y.1731, 2011.
[26] A. Kirsch et al. More Robust Hashing: Cuckoo
Hashing with a Stash. SIAM J. Comput.,
39(4):1543–1561, Dec. 2009.
[27] N. McKeown et al. OpenFlow: enabling innovation in
campus networks. SIGCOMM ’08.
[28] NEC. ProgrammableFlow Networking.
http://www.necam.com/SDN/.
[29] Netronome. NFP-6xxx Flow Processor.
http://www.netronome.com/pages/flow-processors/.
[30] Open Networking Foundation. Fowarding Abstractions
Working Group. https://www.opennetworking.org/
working-groups/forwarding-abstractions.
[31] Open Networking Foundation. OpenFlow Switch
Speciﬁcation. Version 1.3.1.
[32] R. Pagh and F. F. Rodler. Cuckoo hashing. In Journal
of Algorithms, pages 122–144, 2004.
[33] S. Ramabhadran and G. Varghese. Eﬃcient
implementation of a statistics counter architecture. In
Proc. SIGMETRICS ’03.
[34] M. Reitblatt et al. Abstractions for network update.
SIGCOMM ’12.
[10] D. Fotakis et al. Space eﬃcient hash tables with worst
[35] Xilinx. 7 series FPGA overview.
case constant access time. Theory of Computing
Systems, 38:229–248, 2005.
http://www.xilinx.com/support/documentation/data_
sheets/ds180_7Series_Overview.pdf.