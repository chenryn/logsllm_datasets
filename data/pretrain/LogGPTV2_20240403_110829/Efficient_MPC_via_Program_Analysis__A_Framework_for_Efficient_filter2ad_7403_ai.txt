### References

1. **Payman Mohassel and Peter Rindal.** "ABY3: A Mixed Protocol Framework for Machine Learning." In: *ACM CCS 18*. Ed. by David Lie et al. Toronto, ON, Canada: ACM Press, Oct. 2018, pp. 35–52. doi: 10.1145/3243734.3243760.

2. **Tobias Nipkow and Gerwin Klein.** *Concrete Semantics: With Isabelle/HOL*. Springer International Publishing, Inc., 2014. ISBN: 3319105418, 9783319105413.

3. **Flemming Nielson, Hanne R. Nielson, and Chris Hankin.** *Principles of Program Analysis*. Springer Publishing Company, Incorporated, 2010. ISBN: 3642084745, 9783642084744.

4. **Erman Pattuk et al.** "CheapSMC: A Framework to Minimize SMC Cost in Cloud." In: *CoRR abs/1605.00300* (2016). arXiv: 1605.00300. URL: http://arxiv.org/abs/1605.00300.

5. **Alexander Schrijver.** *Combinatorial Optimization: Polyhedra and Efficiency*. 1st ed. Springer-Verlag Berlin Heidelberg, 2003. ISBN: 978-3-540-44389-6, 0937-5511.

6. **Michael L. Scott.** *Programming Language Pragmatics (2nd Edition)*. Morgan Kaufmann, 2015. ISBN: 0124104096.

7. **Axel Schröpfer and Florian Kerschbaum.** "Forecasting Run-Times of Secure Two-Party Computation." In: *Eighth International Conference on Quantitative Evaluation of Systems, QEST 2011*, Aachen, Germany, 5-8 September, 2011. 2011, pp. 181–190. doi: 10.1109/QEST.2011.33. URL: https://doi.org/10.1109/QEST.2011.33.

8. **Axel Schröpfer, Florian Kerschbaum, and Günter Müller.** "L1 - An Intermediate Language for Mixed-Protocol Secure Computation." In: *COMPSAC*. IEEE Computer Society, 2011, pp. 298–307. ISBN: 978-0-7695-4439-7. URL: http://dblp.uni-trier.de/db/conf/compsac/compsac2011.html#SchropferKM11.

9. **Nigel Smart and Dragos Rotaru.** SCALE-MAMBA. 2018. URL: https://github.com/KULeuven-COSIC/SCALE-MAMBA (visited on 11/22/2018).

10. **Raja Vallée-Rai et al.** "Soot - a Java Bytecode Optimization Framework." In: *Proceedings of the 1999 Conference of the Centre for Advanced Studies on Collaborative Research*. CASCON '99. Mississauga, Ontario, Canada: IBM Press, 1999, pp. 13–. URL: http://dl.acm.org/citation.cfm?id=781995.782008.

11. **Robert P. Wilson et al.** "SUIF: An Infrastructure for Research on Parallelizing and Optimizing Compilers." In: *SIGPLAN Notices 29.12* (1994), pp. 31–37. doi: 10.1145/193209.193217. URL: https://doi.org/10.1145/193209.193217.

12. **Andrew Chi-Chih Yao.** "Protocols for Secure Computations (Extended Abstract)." In: *23rd FOCS*. Chicago, Illinois: IEEE Computer Society Press, Nov. 1982, pp. 160–164. doi: 10.1109/SFCS.1982.38.

### Appendix A: Notation and Terminology

In this appendix, we cover the notation and terminology used throughout the paper.

#### A.1 General Terminology

- **(IMP-)source code**: This is the starting point of our compiler. It is standard programming language code for an imperative language such as IMP. We denote it by \( S \). All loops have a known upper bound on their iterations.
- **MPC-source code**: The output of our compiler on some source code \( S \). We denote the compiler by \( C_{\text{MPC}}(\cdot) \). The compiler removes if-statements and φ-nodes, and adds MUX-statements in their place. MPC-source contains for-loops with known bounds.
- **Block \( B \) of MPC-source**: A sequence of assignment statements or blocks (in case of for-loop nesting) enclosed in a for-loop.
- **Intermediate representation between (IMP-)source and MPC-source**: This is an intermediate representation between (IMP-)source and MPC-source.
- **(IMP-)SSA-code**: This is the output of SSA on some source code \( S \). We will denote it as \( C_{\text{SSA}}(S) \).
- **Linearized-code**: This is the linearization of some MPC-source \( C_{\text{MPC}}(S) \). It contains no loops, only straight-line code of assignment statements. The corresponding CFG of this would be simply a straight line.
- **Simple statements in Linear(S)**: We refer to statements in \( \text{Linear}(S) \) as simple statements and denote them as \( st \). Since the corresponding CFG is a line, we often refer to simple statements as nodes in (the CFG of) \( \text{Linear}(S) \).

#### A.2 Cost Model

##### Simple Model
- \( St = \{st_1, \ldots, st_\ell\} \) denotes the ordered set of statements in \( \text{Linear}(S) \).
- \( \Pi = \{\pi_1, \ldots, \pi_m\} \) denotes a set of multi-party protocols.
- \( \Sigma = \{\sigma_1, \ldots, \sigma_q\} \) denotes a set of secret sharing schemes (in typical scenarios such as [DSZ15; MR18; Cha+17; Büs+18], \( q = m \)).
- For each \( (i, j) \in [\ell] \times [m] \), the triple \( (st_i, \pi_j, c_{\pi_j}^{st_i}) \in B \times \Pi \times \mathbb{Z}_{\geq 0} \), where intuitively, \( c_{\pi_j}^{st_i} \) is the cost of emulating a flow statement \( st_i \) with protocol \( \pi_j \).
- For each \( (i, j) \in \Sigma^2 \): the triple \( (\sigma_i, \sigma_j, c_{\sigma_i \to \sigma_j}) \in \Sigma \times \Sigma \times \mathbb{Z}_{\geq 0} \), where intuitively, \( c_{\sigma_i \to \sigma_j} \) is the cost of securely converting a sharing according to scheme \( \sigma_i \) into a sharing according to \( \sigma_j \).
- Whenever the sequence \( St \) and set \( \Pi \) are clear from the context, we use \( c_{\pi_j}^{st_i} \) and \( c_{\sigma_i \to \sigma_j} \) instead of the setup of triples. Furthermore, in all existing works on protocol mixing—including ours—each protocol \( \pi_i \) is associated with a single sharing scheme \( \sigma_i \); in such cases, in slight abuse of notation, we denote the conversion cost from \( \sigma_i \) to \( \sigma_j \) as \( c_{\pi_i \to \pi_j} \) (instead of \( c_{\sigma_i \to \sigma_j} \)). In fact, to further simplify our notation and consistently with the ABY notation, for the three ABY protocols \( \pi_A, \pi_B, \) and \( \pi_Y \), and for \( X, Z \in \{A, B, Y\} \), we use \( c_{X \to Z} \) to denote the conversion cost \( c_{\pi_X \to \pi_Z} \) from the sharing corresponding to \( \pi_X \) (which we refer to as Sharing \( X \)) to the sharing corresponding to \( \pi_Z \) (which we refer to as Sharing \( Z \)).

##### Amortized Model
- The triplet \( (st_i, \pi_j, f_c^{\pi_j}(st_i)(\cdot)) \), where \( f_c^{\pi_j}(st_i): \mathbb{N} \to \mathbb{Z}_{\geq 0} \) denotes the amortized execution cost function, which on input \( \ell \in \mathbb{N} \) outputs the amortized cost \( f_c^{\pi_j}(st_i)(\ell) \) of computing \( \ell \) parallel copies of \( st_i \) with protocol \( \pi_j \).
- The triplet \( (\sigma_i, \sigma_j, f_c^{\sigma_i \to \sigma_j}(\cdot)) \), where \( f_c^{\sigma_i \to \sigma_j}: \mathbb{N} \to \mathbb{Z}_{\geq 0} \) denotes the amortized conversion cost function, which on input \( \ell \in \mathbb{N} \) outputs the amortized cost \( f_c^{\sigma_i \to \sigma_j}(\ell) \) of converting \( \ell \) sharings according to \( \sigma_i \) into sharings according to \( \sigma_j \).
- For brevity, for \( X, Z \in \{A, B, Y\} \) we use \( f_c^{X \to Z} \) to denote the function \( f_c^{\pi_X \to \pi_Z} \) from the sharing corresponding to \( \pi_X \) to the sharing corresponding to \( \pi_Z \). The costs of the simple model correspond to the output of the above functions on input \( \ell = 1 \).

#### OPA for Linearized MPC
- \( PA \) is a sequence of pairs of the type \( (st_1, \pi_1), \ldots, (st_{|St|}, \pi_{|St|}) \) where \( (st_i, \pi_j) \in PA \) means that statement \( st_i \) is assigned protocol \( \pi_j \).

#### Solving the Linearized OPA
- \( c_A^n \) is the cost to run node \( n \in C_{\text{MPC}}(S) \).
- \( c_Y^n \) is the cost to run node \( n \) using \( \pi_Y \).
- \( c_{A \to Y} \) is the cost to run \( A \to Y \) conversion.
- \( c_{Y \to A} \) is the cost to run \( Y \to A \) conversion.

### Variables and Constraints
- \( (d, u) \supseteq (d, u') \) denotes that \( (d, u) \) subsumes \( (d, u') \), i.e., all paths from \( d \) to \( u' \) go through the minimum cut \( (d, u) \).

### From IP Linear(S) to IP CMPC(S)
- \( \alpha: \text{Linear}(S) \to C_{\text{MPC}}(S) \) denotes the "abstraction" function, i.e., provides mapping from \( \text{Linear}(S) \) to \( C_{\text{MPC}}(S) \).
- \( \gamma: (C_{\text{MPC}}(S) \times C_{\text{MPC}}(S)) \to 2^{\text{Linear}(S)} \) denotes the "concretization" function, i.e., provides mapping from \( C_{\text{MPC}}(S) \) to \( \text{Linear}(S) \).

### Appendix B: Preliminaries

#### B.1 Program Analysis
We next discuss concepts that are standard building blocks of static analysis and are necessary background for our results. We assume minimal familiarity with program analysis, and refer an interested reader to [Aho+06].

- **Basic Block (BB)**: A basic block (BB) is a straight-line sequence of instructions, defined by the compiler. The set of basic blocks that may execute before a given basic block are called its predecessors. Similarly, the set of blocks that may execute after a given block are called its successors.
- **Control Flow Graph (CFG)**: A control flow graph (CFG) is a directed graph that represents all possible control flow paths in a program. The nodes in the CFG are basic blocks, and the edges model the flow of control between basic blocks. There is an edge from a basic block to each of its successors. It is also common to consider each statement in a basic block as a separate node with an outgoing edge to the statement/node immediately following within the basic block.
- **Reaching Definitions (RDs)**: Reaching definitions is a classical data-flow analysis technique [Aho+06; NNH10]. It computes def-use chains \( (d, u) \), where \( d \) is a definition of a variable \( x \) (e.g., \( x = y + z \)), and \( u \) is a use of \( x \) (e.g., \( z = x * y \), or \( x > y \)). In the classical sense, reaching definitions is defined over a CFG, where \( d \) and \( u \) are statements/nodes in the graph. A def-use chain \( (d, u) \) entails that there is a path from \( d \) to \( u \) in the CFG that is free of a definition of \( x \), or in other words, the definition of \( x \) at \( d \) may reach the use of \( x \) at \( u \).

Reasoning about dependencies like def-use chains can be greatly simplified by an appropriate intermediate representation (IR). Now, we describe an intermediate representation (IR) called Static Single Assignment (SSA) form. This is a standard IR in compilers and benefits static analysis by immediately exposing def-use dependencies. The standard algorithm to translate a program into SSA form is due to Cytron et al. [Cyt+91].

- **Static Single Assignment (SSA) form**: SSA form entails that each variable in the program is assigned exactly once. If the source code has multiple definitions of the same variable, the variable is split into multiple versions for each definition. Consider, for instance, the code fragment in Figure 5(a). Without SSA, a compiler needs to construct def-use chains to reason that the first definition of \( x \) is not used and is, therefore, dead code. Now consider the same code fragment in SSA form in Figure 5(b). It is immediately obvious that variable \( x1 \) has no uses. Moreover, it is also obvious—because all variables are assigned only once—that \( y \) is only a copy of \( x2 \). Therefore, in any uses of \( y \), \( y \) can be replaced with \( x2 \) without changing the input program behavior. Furthermore, \( x2 \) is a constant with value 2, and consequently \( z \) is a constant too, with value 200. The final SSA-program will just use the constant value 200 and will eliminate the variables in the original program in Figure 5(a).

A natural question is, if SSA form allows variable assignment only once, how does it determine which variable to use when multiple control flow paths merge into a single node (e.g., the if-else in Figure 6(a))? This is taken care of in SSA by so-called phi (φ) nodes.

- **Phi (φ) Nodes**: φ-nodes follow immediately after control flow from two or more paths joins (merges) into a single node. They have the form \( x3 = \phi(x1, x2) \), where \( x3 \) is a new version of the variable, and \( \phi(x1, x2) \) contains the versions of the variable along the different paths. The φ-node entails that \( x \)'s value at this point comes from either the then-arm (x1) or the else-arm (x2) depending on what path control flow took to arrive at the merge node. Figure 6(b) shows the SSA form (including a φ-node) corresponding to the code in Figure 6(a).

- **IMP Imperative Language**: Recall that one of our goals in this work is to define MPC-source, the input IR for MPC compilers/optimizers. Towards this goal, we start from a standard representation of program syntax. The standard representation in the functional programming languages literature uses lambda calculus. However, MPC programs live in the imperative world. Therefore, we choose a standard minimal representation of an imperative language, IMP. IMP (cf. [NK14, ch. 7]) is a simple programming language in which a statement can either be an 1) assignment to an expression where the expression can be a constant, a variable, or an operation between two variables, 2) an if-then-else conditional, or 3) a while loop.

### Appendix C: Program Analysis of MPC Source

#### C.1 Program Syntax

We assume an IMP-like source syntax [NK14]. The IMP syntax models an imperative language, such as FORTRAN, C, or Java, and our results apply to any of these languages. We impose the following standard restrictions necessary to accommodate MPC: there is no recursion, and all loop bounds are statically known. The IMP source is translated into Static Single Assignment (SSA) using standard techniques [Cyt+91]. Fig. 8 abstracts the SSA syntax corresponding to IMP-like source code. Note that this is standard SSA, however, to