Both of these construction operate identically up to the fetch
path step. The difference lies in their eviction strategy.
Circuit ORAM uses two additional eviction paths unlike
Path ORAM which evicts blocks from the local stash onto the
fetched path itself. The strategy is to perform eviction on a path
in a single pass over (the stash and) the path, by picking up
blocks that can be pushed deeper down the path and dropping
it into vacant/dummy slots that are deeper in the path. This
however requires some amount of “foresight” for which blocks
can potentially move to a deeper location in the path and if
there are vacant slots that could accommodate them. To achieve
this foresight, Circuit ORAM makes two meta data scans over
each eviction path, to construct helper arrays that assist in
performing eviction in a single (stash +) path scan.
•
There are two (performance-related) differences between
Path ORAM and Circuit ORAM in the context of ZeroTrace:
Circuit ORAM introduces ∼ 50% more I/O bandwidth
than Path ORAM. In particular, Circuit ORAM has to
fetch and evict two additional paths per access but can
operate with Z = 2.
The ‘stash’ required by Circuit ORAM is much smaller
than that of Path ORAM (O(1) as opposed to ω(log N )
•
1In the interest of optimizing ORAMs for use in the multi-party computation
(MPC) context
5
EPC paging as needed. This design seems reasonable because
it re-uses existing integrity/privacy mechanisms for protecting
the EPC. Unfortunately, it makes supporting persistent storage
difﬁcult because the EPC is volatile (Section III-B), incurs large
EPC paging overheads (Section III-B6) and bloats the TCB (the
entire controller runs in the enclave). To address this challenge,
we make an observation that once Path ORAM (and other tree-
based ORAMs [33], [37], [49]) reveals the leaf it is accessing,
the actual fetch logic can performed by an untrusted party.
Correspondingly, we split the ORAM controller into trusted
(runs inside enclave) and untrusted (runs in Ring-3 outside of
enclave) parts, which communicate between each other at the
path fetch/store boundary. This approach has unexpected TCB
beneﬁts: we propose optimizations in Section IV-E which bloat
the path fetch/store code. By delegating these parts to untrusted
code, they can be implemented with no change to the TCB.
2) Performance challenges and solutions: Running an
oblivious ORAM controller inside of SGX efﬁciently requires
a careful partitioning of the work/data-structures between the
enclave (which controls the EPC pages ∼ 95 MB), untrusted
in-memory code (which has access to DRAM ∼ 64 GB)
and untrusted code managing disk. For instance, the cost to
access ORAM data structures obliviously increases as their
size increases. Further, as mentioned above, when the enclave
memory footprint exceeds the EPC page limit, software paging
introduces an additional overhead between 3× and 1000× –
depending on the access pattern [2]. To improve performance,
we will carefully set parameters to match the hardware and use
techniques such as ORAM recursion to further reduce client
storage.
Additionally, the ORAM storage itself should be split
between DRAM and disk to maximize performance. For
instance, we design the protocol to keep the top-portion of the
ORAM tree in non-EPC DRAM when possible. In some cases,
disk accesses can be avoided entirely. When the ORAM spills
to disk, we layout the ORAM tree in disk to take advantage
of parallel networks of disks (e.g., RAID0).
B. Client Interface
The ORAM Controller Enclave exposes two API calls
to the user, namely read(addr) and write(addr, data). Under
the hood, both the API functions perform an ORAM access
(Section III-D).
C. Server Processes
The server acts as an intermediary between the trusted
enclave and the data (either memory or disk). It performs the
following two functions on behalf of the trusted enclave (e.g.,
in a Ring-3 application that runs alongside the enclave):
•
•
FetchPath(leaf): Given a leaf label, the server transfers
all the buckets on that path in the tree to the enclave.
StorePath(tpath, leaf): Given a tpath, the server over-
writes that existing path to the addresses deduced from
the leaf label, leaf.
1) Passing data in/out of enclave: The standard mechanism
of data passing between enclave and untrusted application is
through a sequence of input/output routines deﬁned for that
speciﬁc enclave. The Intel SGX SDK comes with the Intel
Fig. 1: System components on the server. Trusted components
(software and regions of memory) are shaded. Depending on the
setting, the client may be connecting from a remote device (not
on the server) or from another enclave on the same machine.
blocks). This means data oblivious execution under
Circuit ORAM is more efﬁcient than with Path ORAM,
as we will see in the next section.
IV. ZeroTrace MEMORY CONTROLLER
We now describe how the core memory controller is
implemented on the server. We focus on supporting our
strongest level of security: obliviousness against an active
adversary (Section II-B). The entire system is shown in Fig. 1.
The design’s main component is a secure Intel SGX enclave
which we henceforth call the ORAM Controller Enclave. This
ORAM Controller Enclave acts as the intermediary between
client and the server. The client and controller enclave engage in
logical data block requests and responses. Behind the scenes, the
ORAM Controller Enclave interacts with the server to handle
the backend storage for each of these requests. As mentioned
in Section III-C, we will explain the controller assuming a Path
ORAM backend for exposition.
A. Design Summary
1) Security challenges and solutions: Since ZeroTrace’s
ORAM controller runs inside an enclave, and is therefore vul-
nerable to software-level side channel attacks (Section III-B5),
we will design the ORAM controller to run as an oblivious
program. (A similar approach is used to guard against software
side channels by Olga et al.[30] and Rane et al.[32].) For
instance, if the ORAM controller were to access an index in
the position map directly, it would fetch a processor cache line
whose address depended on the program access pattern. To
prevent revealing this address, our oblivious program scans
through the position map and uses oblivious select operations
to extract the index as it is streamed through.
A second security challenge is how to map the controller
logic itself to SGX enclaves. In a naive design, the entire ORAM
controller and memory can be stored in the EPC. The enclave
makes accesses to its own virtual address space to perform
ORAM accesses and run controller logic, and the OS uses
6
Memory (Cache & DRAM)Disk, Network, etcServer Stack(OS,drivers,etc)Fetch/Store PathSGX PRMStashPosition MapPage cacheClientORAM Controller Enclave CodeSecure channelORAM ControllerSecure Channel InterfaceORAM TreeORAM TreeSoftwareEdger8r tool that generates edge routines as a part of enclave
build process. Edger8r produces a pair of edge routines for
each function that crosses the enclave boundary, one routine
sits in the untrusted domain, and the other within the trusted
enclave domain. Data is transferred across these boundaries by
physically copying it across each routine, while checking that
the original address range does not cross the enclave boundary.
2) TCB implications: Fetch/Store path are traditionally the
performance bottleneck in ORAM design. Given the above
interface, these functions make no assumptions on the untrusted
storage or how the server manages it to support ORAM. Thus,
the server is free to perform performance optimizations on
Fetch/Store path (e.g., split the ORAM between fast DRAM
and slow disk, parallelize accesses to disk; see Section IV-E).
Since Fetch/Store path are not in the TCB, these optimizations
do not effect security.
D. Memory Controller Enclave Program
In this section we outline the core memory controller’s
enclave program which we refer to from now on as P.
1) Initialization: For initialization, the server performs
the function Load(P) → (EP, φ), where P is the ZeroTrace
Controller Enclave. The client can then verify the proof φ
produced by this function to ensure that ZeroTrace has been
honestly initialized by the server. We note that the proof also
embeds within it a public key Ke from an asymmetric key
pair (Ke, Kd) sampled within the enclave. The client encrypts
a secret key K under this public key Ke for the enclave. The
user and enclave henceforth communicate using this K for an
authenticated encrypted channel.
2) Building Block: Oblivious Functions. To remain data
oblivious, we built the ORAM controller out of a library of
assembly-level functions that perform oblivious comparisons,
arithmetic and other basic functions. The only code executed in
the enclave is speciﬁed precisely by the assembly instructions
in our library (all compiler optimizations on our library are
disabled).
Our library is composed of several assembly level instruc-
tions, most notably the CMOV x86 instruction [30], [32].
CMOV is a conditional move instruction that takes a source and
destination register as input and moves the source to destination
if a condition (calculated via the CMP instruction) is true.
CMOV has several variants that can be used in conjunction
with different comparison operators, we speciﬁcally use the
CMOVZ instruction for equality comparisons. The decision
to use CMOV was not fundamental: we could have also used
bitwise instructions (e.g., AND, OR) to implement multiplexers
in software to achieve the obliviousness guarantee.
CMOV safely implements oblivious stores because it does
the same work regardless of the input. Regardless of the input,
all operands involved are brought into registers inside the
processor, the conditional move is performed on those registers,
and the result is written back.
Throughout the rest of the section, we will describe the
ORAM controller operations in terms of a wrapper function
around cmov called oupdate, which has the following signature:
oupdate(bool cond, srcT src,
dstT dst, sizeT sz)
oupdate uses CMOV to obliviously and conditionally copy
sz bytes from src to dst, depending on the value of a bit
cond which is calculated outside the function. src and dst
can refer to either registers or memory locations based on the
types srcT and dstT. We use template parameters srcT and
dstT to simplify the writing, but note that CMOV does not
support setting dst to a memory location by default. Additional
instructions (not shown) are needed to move the result of a
register dst CMOV to memory.
3) System Calls: Our enclave logic does not make any
syscalls. All enclave memory is statically allocated in the
EPC based on initialization parameters. Server processes
(e.g., Fetch/Store path) may perform arbitrary syscalls without
impacting the TCB.
4) Building Block: Encryption & Cryptographic Hashing.
Our implementation relies on encryption and integrity checking
via cryptographic hashing in several places. First, when the
client sends an ORAM request to the ORAM Controller
Enclave, that request must be decrypted and integrity checked
(if integrity checking is enabled). Second, during each ORAM
access, the path returned and re-generated by Fetch/Store Path
(Section IV-C) need to be decrypted/re-encrypted and integrity
veriﬁed. These routines must also be oblivious. For encryption,
we use the Intel instruction set extensions AES-NI, which
were designed by Intel to be side channel resistant (i.e., the
AES SBOX is built directly into hardware). Unless otherwise
stated, all encryption is AES-CTR mode; which can easily
be achieved by wrapping AES-NI instructions in oblivious
instructions which manage the counter. For hashing we use
SHA-256, which is available through the Intel tcrypto library.
To avoid confusion: SGX has separate encryption/hashing
mechanisms to ensure privacy/integrity of pages evicted from
the EPC [9]. Since our design accesses ORAM through a
Fetch/Store Path interface, we cannot use these SGX built-in
mechanisms for ORAM privacy/integrity.
5) ORAM Controller: The ORAM Controller handles client
queries of the form (op, id, data∗), where op is the mode of
operation, i.e. read or write, id corresponds to an identiﬁer of
the data element and data∗ is a dummy block in case of read
and the actual data contents to be written in case it is a write
operation. These queries are encrypted under K, the secret key
established in the Initialization (Section IV-D1) phase. The
incoming client queries are ﬁrst decrypted within the enclave
program. From this point, the ORAM controller enclave runs
the ORAM protocol. Given that the adversary may monitor
any pressure the enclave places on shared hardware resources,
the entire ORAM protocol is re-written in an oblivious form.
The Raccoon system performed a similar exercise to convert
ORAM to oblivious form, in a different setting [32].
Path ORAM can be broken into two main data-structures
(position map and stash) and three main parts. We now explain
how these parts are made oblivious.
a) Oblivious Leaf-label Retrieval: When the enclave
receives an access request (op, id, data∗), it must read and
update a location in the position map (Section III-D) using
oupdate calls, as shown in the following pseudocode:
7
newleaf = random(N)
for i in range(0, N):
cond = (i == id)
oupdate(cond, pos_map[i], leaf, size)
oupdate(cond, newleaf, pos_map[i], size)
We note that P samples a new leaf label through a call to AES-
CTR with a fresh counter. Due to a requirement in Section V,
where execution must be deterministic, we will assume leaf
generation is seeded by the client when the ORAM is initialized
(and not by a TRNG such as Intel’s RDRAND instruction). The
entire position map must be scanned to achieve obliviousness,
as will be the case for the other parts of the algorithm, regardless
of when cond is true. At the end of this step, the enclave has
read the leaf label, leaf, for this access.
b) Oblivious Block Retrieval: P must now fetch the path
for leaf (Section III-D) using a Fetch Path call (Section IV-C).
When the server returns the path, now loaded into enclave
memory, P does the following:
path = FetchPath(leaf)
for p in path:
for s in stash:
cond = (p != Dummy) && (s != occupied)
oupdate(cond, s, p, BlockSize)
result = new Block
for s in stash:
cond = (s.id == id)
oupdate(cond, s, result, BlockSize)
The output of this step is result, which is encrypted and
returned to the client application.
In the above steps, iterating over the stash must take a data-
independent amount of time. First, regardless of when oupdate
succeeds in moving a block, the inner loop runs to completion.
When the update succeeds, a bit is obliviously set to prevent the
CMOV from succeeding again (to avoid duplicates). Second,
the stash size (the inner loop bound) must be data-independent.
This will not be the case with Path ORAM: the stash occupancy