title:A Look into 30 Years of Malware Development from a Software Metrics
Perspective
author:Alejandro Calleja and
Juan E. Tapiador and
Juan Caballero
A Look into 30 Years of Malware Development
from a Software Metrics Perspective
Alejandro Calleja1(B), Juan Tapiador1, and Juan Caballero2
1 Department of Computer Science, Universidad Carlos III de Madrid, Getafe, Spain
{accortin,jestevez}@inf.uc3m.es
2 IMDEA Software Institute, Madrid, Spain
PI:EMAIL
Abstract. During the last decades, the problem of malicious and
unwanted software (malware) has surged in numbers and sophistication.
Malware plays a key role in most of today’s cyber attacks and has con-
solidated as a commodity in the underground economy. In this work,
we analyze the evolution of malware since the early 1980s to date from
a software engineering perspective. We analyze the source code of 151
malware samples and obtain measures of their size, code quality, and
estimates of the development costs (eﬀort, time, and number of people).
Our results suggest an exponential increment of nearly one order of mag-
nitude per decade in aspects such as size and estimated eﬀort, with code
quality metrics similar to those of regular software. Overall, this supports
otherwise conﬁrmed claims about the increasing complexity of malware
and its production progressively becoming an industry.
Keywords: Malware · Source code analysis · Software metrics
1 Introduction
The malware industry seems to be in better shape than ever. In their 2015 Inter-
net Security Threat Report [5], Symantec reports that the total number of known
malware in 2014 amounted to 1.7 billion, with 317 million (26 %) new samples
discovered just in the preceding year. This translates into nearly 1 million new
samples created every day. A recent statement by Panda Security [32] provides a
proportionally similar aggregate: out of the 304 million malware samples detected
by their engines throughout 2015, 84 million (27 %) were new. These impressive
ﬁgures can be partially explained by the adoption of reuse-oriented development
methodologies that make exceedingly easy for malware writers to produce new
samples, and also by the increasing use of packers with polymorphic capabilities.
Another key reason is the fact that over the last decade malware has become a
proﬁtable industry, thereby acquiring the status of a commodity [13,20] in the
ﬂourishing underground economy of cyber crime [35,37]. From a purely technical
point of view, malware has experienced a remarkable evolutionary process since
the 1980s, moving from simple ﬁle-infection viruses to stand-alone programs with
c(cid:2) Springer International Publishing Switzerland 2016
F. Monrose et al. (Eds.): RAID 2016, LNCS 9854, pp. 325–345, 2016.
DOI: 10.1007/978-3-319-45719-2 15
326
A. Calleja et al.
network propagation capabilities, support for distributed architectures based on
rich command and control protocols, and a variety of modules to execute mali-
cious actions in the victim. Malware writers have also rapidly adapted to new
platforms as soon as these acquired a substantial user base, such as the recent
case of smartphones [36].
The surge in number, sophistication, and repercussion of malware attacks
has gone hand in hand with much research, both industrial and academic, on
defense and analysis techniques. The majority of such investigations have focused
on binary analysis, since most malware samples distribute in this form. Only
very rarely researchers have access to the source code and can report insights
gained from its inspection. (Notable exceptions include the analysis of the source
code of 4 IRC bots by Barford and Yegneswaran [11] and the work of Kotov and
Massacci on 30 exploit kits [26].) One consequence of the lack of wide availability
of malware source code is a poor understanding of the malware development
process, its properties when looked at as a software artifact, and how these
properties have changed in the last decades.
In this paper, we present a study of the evolution of malware from a software
engineering perspective. Our analysis is based on a dataset composed of the
source code of 151 malware samples ranging from 1975 to 2015, including early
viruses, worms, trojans, botnets, and remote access trojans (RATs). We make
use of several metrics used in software engineering to quantify diﬀerent aspects
of the source code of malware understood as a software artifact. Such metrics are
grouped into three main categories: (i) measures of size: number of source lines of
code (SLOC), number of source ﬁles, number of diﬀerent programming languages
used, and number of function points (FP); (ii) estimates of the cost of developing
the sample: eﬀort (man-months), required time, and number of programmers;
and (iii) measures of code quality: comment-to-code ratio, complexity of the
control ﬂow logic, and maintainability of the code. We also use these metrics
to compare malware source code to a selection of benign programs. To the best
of our knowledge, our work is the ﬁrst to explore malware evolution from this
perspective. We also believe that our dataset of malware source code is the
largest analyzed in the literature. The main ﬁndings of our work include:
1. We observe an exponential increase of roughly one order of magnitude per
decade in the number of source code ﬁles and SLOC and FP counts per
sample. Malware samples from the 1980s and 1990s contain just one or a few
source code ﬁles, are generally programmed in one language and have SLOC
counts of a few thousands at most. Contrarily, samples from the late 2000s and
later often contain hundreds of source code ﬁles spanning various languages,
with an overall SLOC count of tens, and even hundreds of thousands.
2. In terms of development costs, our estimates evidence that malware writing
has evolved from small projects of just one developer working no more than
1–2 months full time, to larger programming teams investing up to 6–8 months
and, in some cases, possibly more.
A Look into 30 Years of Malware Development
327
3. A comparison with selected benign software projects reveals that the largest
malware samples in our dataset present software metrics akin to those of
products such as Snort or Bash, but are still quite far from larger software
solutions.
4. The code quality metrics analyzed do not suggest signiﬁcant diﬀerences
between malware and benign software. Malware has slightly higher values
of code complexity and also better maintainability, though the diﬀerences are
not remarkable.
The remaining of this paper is organized as follows. Section 2 provides an
overview of the software metrics used in this work. In Sect. 3 we describe our
dataset of malware source code. Section 4 contains the core results of this work
and Sect. 5 discusses the suitability of our approach, its limitations, and addi-
tional conclusions. Finally, Sect. 7 concludes the paper.
2 Software Metrics
This section provides an overview of the software metrics concepts used in this
work to quantify various aspects of malware source code. We ﬁrst introduce the
two most widely used measures of software size: lines of source code (SLOC) and
function points (FP). We then introduce eﬀort estimation metrics, speciﬁcally
the Constructive Cost Model (COCOMO), and also measures of source code
complexity and maintainability.
2.1 Measuring Software Size
The number of lines in the source code of a program (SLOC) constitutes the
most commonly used measure of its size. The number of physical SLOC refers
to a count of the number of lines in the source code of a program, excluding
comment and blank lines. Contrarily, logical SLOC counts take into account
language-speciﬁc aspects, such as terminating symbols and style or formatting
conventions, to deliver an estimate of the number of executable statements.
Both IEEE [23] and the Software Engineering Institute (SEI) [31] had provided
deﬁnitions and counting guidelines to obtain SLOC measures.
SLOC counts have a number of shortcomings [29] and can be easily misused.
Despite this, it has a long-standing tradition as the most popular sizing metric.
Furthermore, SLOC is an essential input for many estimation models that aim
at predicting the eﬀort required to develop a system, its maintainability, the
expected number of bugs/defects, or the productivity of programmers.
Comparing size across diﬀerent programming languages can give misleading
impressions of the actual programming eﬀort: the more expressive the program-
ming language, the lower the size. An alternative metric to using SLOCs as the
estimated software size is to use a measure of its functionality. The best known of
such measures is the function-point count, initially proposed by Albrecht [7] and
later reﬁned by Albrecht and Gaﬀney [8]. The function-point count refers to the
328
A. Calleja et al.
overall functionality of the software and is measured by estimating four program
features: external inputs and outputs, user interactions, external interfaces, and
ﬁles used. The overall count also involves various weights that account for the
possibly diﬀerent complexity of each of the above elements. Thus, the so-called
unadjusted function-point count (UFC) is computed by simply multiplying each
count by the appropriate weight and summing up all values. The UFC can be
subsequently adjusted through various factors that are related to the complexity
of the whole system.
The expected size in SLOC of a software project can be estimated from
function-point counts through a process known as backﬁring [25]. This con-
sists in the use of existing empirical tables that provide the average number
of SLOC per function point in diﬀerent programming languages. Software Pro-
ductivity Research (SPR) [24] annually publishes such conversion ratios for the
most common programming languages in what is known as Programming Lan-
guages Tables (PLT), which are empirically obtained by analyzing thousands
of software projects. Table 1 shows the SLOC-to-function-point ratios provided
by PLT v8.2 for the languages most commonly observed in malware. Overall,
backﬁring is useful as SLOC counts are not available early enough in the require-
ments phase for estimating purposes. Also, the resulting UFC measure is a more
normalized measure of the source code size.
Table 1. SLOC to function-point ratios for various programming languages.
Programming language
ASP / ASP.Net
Assembly
Shell / DOS Batch
C
C#
C++
HTML / CSS / XML / XSLT
SLOC/FP
69
119
128
97
54
50
34
Programming language SLOC/FP
Java
53
47
Javascript
67
PHP
90
Pascal
24
Python
SQL / make
21
42
Visual Basic
2.2 Eﬀort Estimation: The Constructive Cost Model (COCOMO)
One of the core problems in software engineering is to make an accurate estimate
of the eﬀort required to develop a software system. This is a complex issue that
has attracted much attention since the early 1970s, resulting in various techniques
that approach the problem from diﬀerent perspectives [34]. A prominent class of
such techniques are the so-called algorithmic cost modeling methods, which are
based on mathematical formulae that provide cost ﬁgures using as input various
measures of the program’s size, organizational practices, and so on.
One of the best known algorithmic software cost estimation methods is the
Constructive Cost Model (COCOMO) [12]. COCOMO is an empirical model
derived from analyzing data collected from a large number of software projects.
A Look into 30 Years of Malware Development
329
These data were used to ﬁnd, through basic regression, formulae linking the
size of the system, and project and team factors to the eﬀort to develop it. As
in most algorithmic cost models, the number of lines of source code (SLOC)
in the delivered system is the basic metric used in cost estimation. Thus, the
basic COCOMO equation for the eﬀort (in man-months) required to develop a
software system is
(1)
where KLOC is the estimated number of SLOC expressed in thousands. The
development time (in months) is obtained from the eﬀort as
E = ab(KLOC)bb ,
D = cbEdb,
and the number of people required is just
E
D
P =
.
(2)
(3)
In the equations above, the coeﬃcients ab, bb, cb, and db are empirical estimates
dependent on the type of project (see Table 2). COCOMO considers three types
of projects: (i) Organic projects (small programming team, good experience, and
ﬂexible software requirements); Semi-detached projects (medium-sized teams,
mixed experience, and a combination of rigid and ﬂexible requirements); and
(iii) Embedded projects (organic or semi-detached projects developed with tight
constraints).
Table 2. Basic COCOMO coeﬃcients.
Software project ab
Organic
bb
cb
db
2.4 1.05 2.5 0.38
Semi-detached
3.0 1.12 2.5 0.35
Embedded
3.6 1.20 2.5 0.32
The model described above is commonly known as Basic COCOMO and is
very convenient to obtain a quick estimate of costs. A further reﬁnement is pro-
vided by the so-called Intermediate COCOMO. The main diﬀerence consists in
the addition of various multiplicative modiﬁers to the eﬀort estimation (E) that
account for attributes of both the product and the programming process such as
the expected reliability, and the capability and experience of the programmers.
Since these are not known for malware, we will restrict ourselves to the Basic
COCOMO model.
2.3 Source Code Complexity and Maintainability
Software complexity metrics attempt to capture properties related to the inter-
actions between source code entities. Complexity is generally linked to main-
tainability, in the sense that higher levels of complexity might translate into
330
A. Calleja et al.
a higher risk of introducing unintentional interactions and, therefore, software
defects [27].
One of the earliest—and still most widely used—software complexity metric
is McCabe’s cyclomatic complexity [28], often denoted M. The cyclomatic com-
plexity of a piece of source code is computed from its control ﬂow graph (CFG)
and measures the number of linearly independent paths within it; that is, the
number of paths that do not contain other paths within themselves. Thus, a
piece of code with no control ﬂow statements has M = 1. A piece of code with
one single-condition IF statement would have M = 2, since there would be two
paths through the code depending on whether the IF condition evaluates to true
or false. Mathematically, the cyclomatic complexity of a program is given by
M = E − N + 2P,
(4)
where E is the number of edges in the CFG, N the number of nodes, and P
the number of connected components. The term “cyclomatic” stems from the
connections between this metric and some results in graph theory and algebraic
topology, particularly the so-called cyclomatic number of a graph, which mea-
sures the dimension of the cycle space of a graph [16].
The cyclomatic complexity has various applications in the process of devel-
oping and analyzing software products. The most direct one is to limit the com-
plexity of the routines or modules that comprise the system. McCabe recom-
mended that programmers should limit each module to a maximum complexity
of 10, splitting it into smaller modules whenever its complexity exceeds this
value. The NIST Structured Testing Methodology [38] later adopted this prac-
tice and relaxed the ﬁgure up to 15, though only occasionally and if there are well
grounded reasons to do it. The cyclomatic complexity has also implications in
program testing because of its connection with the number of test cases that are
necessary to achieve thorough test coverage. Speciﬁcally, M is simultaneously:
(i) an upper bound for the number of test cases needed to achieve a complete
branch coverage (i.e., to execute all edges of the CFG); and (ii) a lower bound
for the number of paths through the CFG. Thus, a piece of code with high M
would have more pathways through the code and would therefore require higher
testing eﬀort.
The cyclomatic complexity is also connected to another code metric called
the maintainability index (M I), introduced by Oman and Hagemeister in [30].
The M I is a value between 0 and 100 that measures how maintainable
(i.e., easy to understand, support, and change) the source code is, with high
values meaning better maintainability. One of the most common deﬁnitions of
the M I is given by
M I = 100