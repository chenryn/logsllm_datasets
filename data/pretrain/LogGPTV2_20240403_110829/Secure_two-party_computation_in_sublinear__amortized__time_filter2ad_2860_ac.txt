The server sends the appropriate portion of its share
of stateΠ to the client, who can then recover z.
See Figures 1 and 2 for the secure initialization and secure
computation of the RAM next-instruction. In the ﬁgures, we
let [v] denote a bitwise secret-sharing of a value v between
the two parties. It remains to describe how a single virtual
instruction I (shared between the two parties) is evaluated.
This is done as follows (also see Figure 3):
1. The parties use repeated secure computation of OE to
obtain a sequence of real instructions ˆI1, . . .. Each such
instruction ˆI is revealed to the server, who executes the
instruction on the ORAM data structure that it stores.
If ˆI was a read instruction, then the value d that was
read is secret-shared with the client.
2. After all the real instructions have been executed, em-
ulation of instruction I is complete. If I was a read
(cid:2)
instruction, then the (virtual) value d
that was read
is secret-shared between the client and server.
The key point to notice is that each secure computation
that is invoked is run only over small inputs. This is what
allows the amortized cost of the protocol to be sublinear.
The following summarizes our main theoretical result. The
proof is tedious but relatively straightforward; due to space
limitations, it is omitted from the present version but is
available from the authors upon request.
Theorem 2. If an ORAM construction and a 2PC pro-
tocol secure against semi-honest adversaries are used, then
our protocol securely computes f against semi-honest adver-
saries. Furthermore, if f can be computed in time t and
space s on a RAM, then our protocol runs in amortized time
O(t) · polylog(s), the client uses space O(log(s)), and the
server uses space s · polylog(s).
We comment that if the underlying secure-computation is
secure against malicious parties, then a simple change to our
protocol will suﬃce for obtaining security against malicious
parties as well. We simply change the outputs of all se-
cure computations to include a signature on the outputs de-
scribed above (using a signing key held by the other party),
and we modify the functions used in the secure-computation
517such that they verify the signature on each input before con-
tinuing. We leave the proof of this informal claim to future
work. We note that we cannot make such a claim for our
more eﬃcient, concrete solution presented in Section 4.1.
4. OUR OPTIMIZED PROTOCOL
In Section 3 we showed that any ORAM protocol can be
combined with any secure two-party computation scheme to
obtain a secure computation scheme with sublinear amor-
tized complexity.
In this section we present a far more
eﬃcient and practical scheme, based on instantiating our
generic protocol with Yao’s garbled circuits and the ORAM
construction of Shi et. al [19]. However, rather than apply-
ing the secure computation primitive on the entire ORAM
instruction, we deviate from the generic protocol by per-
forming parts of the computation locally, whenever we could
do so without violating security. This section describes our
scheme, including concrete algorithmic and engineering deci-
sions we made when instantiating our protocol, as well as im-
plementation choices and complexity analysis. In Section 5
we present experimental performance results, demonstrat-
ing considerable improvement over using traditional secure
computation over the entire input (i.e. without ORAM). We
do not describe Yao’s garbled circuit technique here, as this
has been described in many prior works (see [14] for a very
clear exposition). We do, however, attempt to present the
discussion in a way that requires minimal knowledge of this
particular technique.
The ORAM Construction of Shi et. al. [19].
We begin with an overview of the ORAM construction of
[19], which is the starting point of our protocol. The main
data storage structure used in this scheme is a binary tree
with the following properties. To store N data items in the
ORAM, we construct a binary tree of height log N , where
each node has the capacity to hold log N data items. Every
item stored in the binary tree is assigned to a randomly cho-
sen leaf node. The identiﬁer of this leaf node is appended
to the item, and the item, along with its assignment, is en-
crypted and stored somewhere on the path between the root
and its assigned leaf node. To ﬁnd a data item, the client be-
gins by retrieving the leaf node associated with that item; we
will explain how this is done below. He sends the identiﬁer
of the leaf node to the server, who then fetches and sends all
items along the appropriate path, one node at a time. The
client decrypts the content of each node and searches for
the item he is looking for. When he ﬁnds it, he removes it
from its current node, assigns it a new leaf identiﬁer chosen
uniformly at random and inserts the item at the root node
of the tree. He then continues searching all nodes along the
path in order to prevent the server from learning where he
found the item of interest.
Since the above lookup process will work only while there
is room in the root node for new incoming items, the authors
of [19] devise the following load balancing mechanism to
prevent nodes from overﬂowing. After each ORAM access
instruction, two nodes are chosen at random from each level
of the tree. One arbitrary item is evicted from each of these
nodes, and is inserted in the child node that lies on the
path towards its assigned leaf node. While the server will
learn which nodes were chosen for this eviction, it should
not learn which children receive the evicted items. To hide
this information, the client insert encrypted data in both of
the two child nodes, performing a “dummy” insertion in one
node, and a real insertion in the other.
1 , . . . , v(1)
1 , . . . , L(1)
r items with virtual addresses v(2)
All that remains to describe is how the client recovers the
leaf identiﬁer associated with the item of interest. The num-
ber of such identiﬁers is linear in the size of the database,
so storing the identiﬁers on the client side is not an option.
The solution is to store these assignments on the server,
recursively using the same type of binary trees. A crucial
property which makes this solution possible is that an item
can store more than a single mapping. If an item stores r
mappings, then the total number of recursively built trees is
logr N . The smallest tree will have very few items, and can
thus be stored by the client. As an example, let the largest
tree contain items with virtual addresses v(1)
N that
are assigned leaf identiﬁers L(1)
N . Then the tree
at level 2 has N
1 , . . . , v(2)
,
where the item with virtual address v(2)
contains mappings
) for (j − 1)r < i ≤ jr. With this modiﬁcation, an
(v(1)
ORAM lookup consists of a series of lookups, one in each of
these trees, beginning with the smallest tree. In particular,
given a virtual address v for a database query, the client
derives the lookup values that he needs to use in tree i by
computing v(i) = (cid:8) v
ri (cid:9) for 0 ≤ i ≤ logr N . Having these
values the client starts with a lookup in the smallest tree for
value v(logr N ). He retrieves L(logr N ) from his local memory
and ﬁnds in it the mapping (v(logr N−1), L(logr N−1)). Now
he looks for v(logr N−1) in the next smallest tree using leaf
identiﬁer L(logr N−1). This process continues until the client
retrieves the real database item at address v from the largest
tree at the top level of the recursion. In each tree, the ac-
cessed item is assigned a new leaf node at random, and the
item is inserted back in the tree’s root node. In addition, its
mapping is updated in the tree below to record its new leaf
node.
N
r
j
, L(1)
i
i
The intuition for the security of this scheme can be sum-
marized as follows. Every time the client looks up item vi, he
assigns it a new leaf node and re-inserts it at the root. It fol-
lows that the paths taken to ﬁnd vi in two diﬀerent lookups
are independent of one another, and cannot be distinguished
from the lookup of any other two nodes. During the eviction
process, a node is just as likely to accept a new item as it
is to lose an item. Shi et al. prove in their work that with
buckets of size O(log(M N/δ)) the probability that a bucket
will overﬂow after M ORAM instructions is less than δ. It
follows that with a bucket size of O(log N ), the probability
of overﬂow is negligible in the security parameter. However,
as we shall see below, the precise constant makes a big dif-
ference, both in the resulting security and in the eﬃciency
of the scheme.
4.1 High Level Protocol
As above, we assume a database of N items, and we allow
each item in each recursive level to hold r mappings between
virtual addresses and leaf identiﬁers from the level above.
The client and a server perform the following steps to access
an item at an address v:
1. The parties have shares vC and vS of the virtual ad-
dress for the query in the database v = vC ⊕ vS.
2. The client and the server run a two party computa-
and
tion protocol to produce shares v(1)
C , . . . , v(logr N )
C
518S
S , . . . , v(logr N )
v(1)
lookup in each tree of the ORAM storage: (cid:8) v
C ⊕ v(i)
v(i)
S for 0 ≤ i ≤ logr N
of the virtual addresses that they will
ri (cid:9) =
3. The server generates random leaf identiﬁers
˜L(1), . . . , ˜L(logr N ) that will be assigned to items as they
are re-inserted at the root.
4. The last tree in the ORAM storage has only a constant
number of nodes, each containing a constant number
of items. The client and server store shares of the leaf
identiﬁers for these items. They execute a two party
protocol that takes these shares as inputs, as well as
the shares v(logr N )
. The server’s output
of the secure computation is the leaf value L(logr N ).
The client has no output.
and v(logr N )
C
S
5. For each i such that logr N ≥ i ≥ 2:
C
(a) The server retrieves the nodes on the path be-
tween the root and the leaf L(i) in the i-th tree.
(b) The parties execute a secure two party protocol.
The server’s inputs are the nodes recovered above,
and the secret share v(i−1)
. The client’s input is
v(i−1)
. The server receives value L(i−1) as output,
C ⊕v(i−1)
which is the value stored at address v(i−1)
,
and which lies somewhere along the path to L(i).
(c) The parties execute a secure two party protocol to
update the content of item v(i) with the value of
the new leaf identiﬁer ˜L(i−1) that will be assigned
to v(i−1)
in the i − 1-th tree.
⊕ v(i−1)
S
S
C
S
(d) The parties execute a secure two party protocol
to tag item v(i) with it’s new leaf node assignment
˜L(i), and to insert v(i) in the ﬁrst empty position
of the root node.
6. For the ﬁrst level tree that contains the actual items
for the database, the server retrieves the nodes on the
path between the root and the leaf L(1). The parties
execute a secure two party protocol to ﬁnd item v =
C ⊕ v(1)
v(1)
S . The outputs of the protocol are secret
shares of the data d = dC ⊕dS found at virtual address
v. The server tags v with ˜L(1), and the parties perform
another secure protocol to insert v at the ﬁrst empty
spot in the root node.
4.2 Optimizations and Implementation Choices
Encryption and Decryption.
In our protocol description so far, we have left implicit
the fact all data stored in the database at the server must
be encrypted. Every time a data item is used in the RAM
computation, it must ﬁrst be decrypted, and it must be re-
encrypted before it is re-inserted at the root node. In a naive
application of our generic solution, the parties would have
to decrypt, and later re-encrypt the data item completely
inside a Yao circuit, which can be very time consuming.
We choose the following encryption scheme, with an eye to-
wards minimizing the computation done inside the garbled
circuit: Enc(m; r) = (FK (r) ⊕ m, r), where F can be any
pseudo-random function. The key K is stored by the client,
and kept secret from the server. To ensure that encryption
(cid:2)
and decryption can be eﬃciently computed inside a garbled
circuit, the server sends r to the client in the clear, along
with a random r
that will be used for re-encryption. The
client computes FK (r) and FK (r
) outside the secure com-
putation. Now the only part of decryption or re-encryption
that has to be done inside the garbled circuit is boolean
XOR, which is very cheap.
(cid:2)
While this greatly improves the eﬃciency of our scheme,
we note that it has to be done with care: sending the en-
cryption randomness to the client could reveal information
about the access pattern of the RAM, and, consequently,
about the server’s data. The issue arises during the evic-
tion procedure, when a data item is moved from a parent to
one of its children. During this process, it is important that
neither player learn which child received the evicted data;
the construction of Shi et al. [19] has the client touch both
children, writing a dummy item to one of the two nodes, and
the evicted item to the other node, thereby hiding from the
server which node received the real item. In our case, this
must be hidden from the client as well, which is ensured
by performing the operation inside a secure computation.
However, the exact way in which randomness is assigned
to ciphertext has a crucial eﬀect on security. For example,
suppose the server sends randomness r1 and r2 to be used
in the re-encryption, and our operation is designed so that
r1 is always used for encrypting the dummy item and r2
is always used for the real item. The client can then keep
track of the real item by waiting to receive r2 for decryption
in the future! We must therefore design the re-encryption
operation so that randomness is associated with a node in
the tree rather than the content of the ciphertext. Then, r1
is always used in the left child, and r2 in the right, indepen-
dent of which node receives the real item and which receives
the dummy item.
Although this issue is easily handled2, it demonstrates
the subtlety that arises when we depart from the generic
protocol in order to improve the eﬃciency of the scheme.
Choosing a Bucket Size.
At each node of the ORAM structure we have a bucket of