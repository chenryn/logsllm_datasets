### Title: The Devil is in the GAN: Defending Deep Generative Models Against Adversarial Attacks

**Abstract:**

Generative Adversarial Networks (GANs) represent a cutting-edge AI technology with significant potential to transform science and industry. These models can synthesize data from complex, high-dimensional manifolds, such as images, text, music, or molecular structures. Potential applications include media content generation and enhancement, drug synthesis, medical prosthetics, and generally improving AI performance through semi-supervised learning.

Training GANs is an extremely compute-intensive task that requires highly specialized expertise. State-of-the-art GANs can have billions of parameters and require weeks of GPU training time. As a result, many users may need to source pre-trained GANs from third parties, which could be untrusted. Surprisingly, while there is extensive literature on evasion and poisoning attacks against conventional, discriminative Machine Learning (ML) models, adversarial threats against GANs—or more broadly, Deep Generative Models (DGMs)—have not been thoroughly analyzed.

In this talk, we introduce a formal threat model for training-time attacks against DGMs. We demonstrate that, with minimal effort, attackers can backdoor pre-trained DGMs by embedding compromising data points. When triggered, these embedded data points can cause material and/or reputational damage to the organization using the DGM. Our analysis shows that while attackers can bypass naïve detection mechanisms, a combination of static and dynamic inspections of the DGM is effective in detecting such attacks.

**Key Points:**
- **Potential of GANs:** GANs can generate and enhance complex data, with applications in media, healthcare, and AI performance.
- **Training Challenges:** Training GANs is resource-intensive and requires specialized skills, leading to the reliance on pre-trained models from third parties.
- **Threat Model:** We present a formal threat model for training-time attacks on DGMs.
- **Attack Demonstration:** Attackers can easily backdoor pre-trained DGMs, embedding harmful data points.
- **Detection Mechanisms:** Naïve detection methods are ineffective, but a combination of static and dynamic inspections can successfully detect these attacks.

This research aims to raise awareness about the potential vulnerabilities in DGMs and provide robust detection strategies to mitigate these risks.