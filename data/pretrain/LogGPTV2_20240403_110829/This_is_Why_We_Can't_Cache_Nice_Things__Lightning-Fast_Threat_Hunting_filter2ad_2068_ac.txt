ing process is O (n), where n is the length of the causal path. The
algorithm is outlined in Algorithm 1. The inputs are the vertex V ,
the relative ranking R of the causal path in V , and Seen, which is a
hashmap of the previously-visited vertices during path discovery.
This hashmap is used to halt recursion in the case of a cycle. The
output of Algorithm 1 is the discovered path.
We use Figure 4 as an example to explain the recovering process.
Assume that Swift wants to recover the highest scoring path P1 =
{B → A → D}. To do so, Swift only needs to have the last vertex
D and the relative ranking (index), which is 0. To recover the full
path, Swift refers to the first element in its PAT Habnormal and
recovers the parent in the given path, which is A, and gets the
relative ranking of the path in A, which is also 0. Then this process
is recursively repeated on A and its ranking until the whole path is
recovered.
6 HIERARCHICAL STORAGE MANAGEMENT
6.1 Tracking Cache
Swift takes the stream of audit log events and identifies causal
relationships between each new event and past events in order to
build a causal graph. The role of the tracking cache is to ensure
Algorithm 2: TrackObject
Inputs :GDB, E
1 Sub = RetreiveOrCreate(E .sub, GDB)
2 Obj = RetreiveOrCreate(E .obj, GDB)
3 if IsPar ent (Sub, E .Rel ) then
4
5
6
7
8
9 else
10
11
12
13
14
15 GDB.Update(Sub)
16 GDB.Update(Obj)
17 return
Sub.AddChild(Obj)
Obj.AddParent(Sub)
for Index, (P, S, t, R) ∈ Sub .PAT Habnormal do
ChildScor e = CalculateScore(S,Sub, Obj, E)
Obj.AddToPath(Sub, ChildScor e, E .t, Index)
Obj.AddChild(Sub)
Sub.AddParent(Obj)
for Index, (P, S, t, R) ∈ Obj .PAT Habnormal do
ChildScor e = CalculateScore(S,Sub, Obj, E)
Sub.AddToPath(Obj, ChildScor e, E .t, Index)
The time complexity of Algorithm 2 is O(1). Since we have lim-
ited the size of the PAT Habnormal as a constant, the time complexity
of the loop between line 5 and line 8 is constant. Due to the same
reason, the time complexity of AddToPath is also O(1). After each
epoch ∆Tpromote, Swift evicts system objects (vertices) from track-
ing cache to the suspicious cache if they have not been accessed
in the last epoch. Vertices that have been accessed during the past
epoch are retained in the tracking cache for the next epoch.
6.2 Suspicious Cache
After being evicted from the tracking cache, vertex entries are
moved to the suspicious cache. The goal of the second cache is to
retain vertex entries for all vertices that fall on the Top K most
suspicious causal paths throughout the history of system execu-
tion. The intuition behind the suspicious cache is based on the
Hypothesis H2.
H2 Most Suspicious Causal Paths Hypothesis. If a path in
the causal graph contains multiple suspicious (anomalous)
events, it is much more likely to be associated with a true
attack.
Recent studies provide evidence for this hypothesis, and in fact
are the inspiration for the present study – Hassan et al. [41] present
an alert triage system that ranks alerts based on the aggregate
anomalousness of their causal paths, observing that this approach
can be used to eliminate 84% of false alerts from a commercial Threat
Detection Softwares (TDS). Liu et al. [57] present an optimization
for forward trace queries that prioritizes the search of anomalous
paths in order to construct attack graphs more quickly. While these
results are encouraging, both of these systems rely on disk-based
graph storage and are thus subject to extremely high latencies when
traversing causal graphs; our observation is that this hypothesis
can also inform the design of a forensic cache. Because true attacks
are likely to fall on the most suspicious (anomalous) causal paths,
our system should prioritize the retention of events associated with
such paths. This will increase the likelihood that all forensically-
relevant information will exist in main memory at the time of the
investigation.
6
Figure 5: CDF of the time difference between a newly generated
event and the event’s immediate dependencies (i.e., parents). 98% of
events’ immediate dependencies occurred less than 15 minutes ago,
providing empirical evidence for the Epochal Causality Hypothesis.
that the events most relevant to the graph building process are con-
sistently available in the main memory. Our approach to assuring
fast access to causally-related past events is based on the following
hypothesis:
H1 Epochal Causality Hypothesis. Events which are recently
accessed during causal graph generation are accessed again
in a short epoch of time (∆Tpromote ), and thus should not be
evicted from the main memory in that epoch.
An empirical validation of hypothesis H1 is given in Figure 5
based on the audit stream of a 191 host enterprise. This CDF shows
that the immediate dependencies (i.e., parents) of 98% of newly
created events were created within a short epoch prior ( ∈ SuspiciousCache do
if CheckNotTaint() then
EVICT()
6
7
8 return
is achieved by the propagating and storing of suspicious influence
scores along with each causal path in the database. Note that the
suspicious influence scores are calculated during online tracking
(Algorithm 2). As discussed previously, the greater the suspicious
influence score of an alert, the more suspicious that alert will be
and should therefore be investigated first. As soon as alerts are
fired during threat hunting process (shown in Figure 1), Swift iter-
atively sorts alerts based on suspicious influence scores. In the alert
management stage, Swift only needs to retrieve the previously-
calculated suspicious influence scores from the HSM, assuring that
alert triage can occur in real-time.
Alert correlation and concise causal graph generation are realized
automatically by our HSM design. Swift uses the suspicious cache
to retain the causal paths of those previously triggered alerts that
have higher suspicious influence scores. To correlate two alerts,
Swift only needs to query the suspicious cache to figure out if
the most recently triggered alert’s causal path is associated with
any alerts that were triggered in the past. To support causal graph
generation, Swift provides two types of queries to retrieve the
causal graph of alerts: concise queries and complete queries. The
concise query returns the most suspicious causal subgraph related
to an alert, which is stored entirely in the suspicious cache. The
complete query returns the whole causal graph by fetching paths
from both the suspicious cache and, if needed, the disk.
8 EVALUATION
In this section, we focus on evaluating the efficacy, usefulness, and
scalability of Swift as a real-time forensic analysis in an enter-
prise. In particular, we investigated the following research questions
(RQs):
RQ1 How effective is Swift in threat alert investigation?
RQ2 What are the insights into the events that are cached vs
spilled to disk by Swift?
RQ3 How scalable is Swift?
RQ4 Can the time saved using Swift help an enterprise to thwart
an attack?
RQ5 How efficient is Swift at alert management?
8.1 Implementation
We implement Swift for an enterprise environment and collected
system event logs generated by Windows ETW [3] and Linux Au-
ditd [4] using Kafka producers. We wrote our own consumer threads
to fetch audit logs from Kafka producers. Swift uses the Guava
Cache by Google [17] to maintain the causal graph database in
the main-memory. This cache supports timed eviction and asyn-
chronous batch writes. Swift uses RocksDB [27] as the persistent
key-value storage. The batch mode in RocksDB provides high rate
for read and write.
In our implementation, we use the method proposed by Hassan et
al. [41] to calculate the suspicious influence score because it satisfies
all the three properties mentioned in Section 4.2. Particularly, for
a causal path P, we calculate its Suspicious Influence Score SIS (P )