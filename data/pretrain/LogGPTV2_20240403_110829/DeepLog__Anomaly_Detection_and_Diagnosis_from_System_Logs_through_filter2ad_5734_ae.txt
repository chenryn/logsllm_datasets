The performance of the method drops significantly as the history window lengthens. In contrast, the LSTM-based approach demonstrates greater stability, as illustrated in Section 5.1.4. Figure 6b examines the top-❕ approach employed by DeepLog’s prediction algorithm. Let \( D_t \) be the set of top-❕ log key values predicted by DeepLog at time \( t \), and \( m_t \) be the actual log key value observed in the data at time \( t \). To evaluate the impact of this strategy, we analyze the cumulative distribution function (CDF) of \( \Pr[m_t \in D_t] \) for different values of ❕. Among over 11,000,000 log keys labeled as normal, 88.9% of DeepLog’s top predictions match \( m_t \) exactly, and 96.1% of \( m_t \) values are within DeepLog’s top 2 predictions. When ❕ = 5, 99.8% of normal \( m_t \) values are within \( D_t \), while the anomaly detection rate is 99.994% (only one anomalous session was undetected).

Figure 7a illustrates the performance on the OpenStack dataset. The PCA approach shows reasonable performance but with low precision (77%). Although IM achieves perfect recall, it has very poor precision (2%), detecting almost all VM instances as abnormal. This is because the OpenStack logs were generated randomly, as described in Section 5.1.2. The frequency of log keys like (Stop Start) in a VM lifecycle (defined by a pair of Create and Delete) is uncertain, making it challenging for IM to identify stable small invariants for anomaly detection.

To test this hypothesis, we generated a second dataset with a deterministic pattern like (Create Delete)+, resulting in 5,544 normal VM executions and 170 anomalous ones, denoted as OpenStack II. The results are shown in Figure 7b. IM performs well on this dataset with more regular patterns. However, the recall for the PCA method drops to only 2% because the normal pattern in the data is too regular, rendering the variance-based detection ineffective. DeepLog, on the other hand, demonstrates excellent performance on both OpenStack logs, with F-measures of 98% and 97%, respectively. It is also important to note that PCA and IM are offline methods, suitable only for session-level anomaly detection, which may not be applicable to many system logs.

### 5.1.4 Analysis of DeepLog
We investigate the performance impact of various parameters in DeepLog, including ❕, \( h \), \( L \), and \( \alpha \). The results are shown in Figure 8. In each experiment, we varied the values of one parameter while using the default values for others. Generally, DeepLog's performance is fairly stable across different parameter values, indicating that it is not highly sensitive to the adjustment of any single or combination of these parameters. This makes DeepLog easy to deploy and use in practice. For example, Figure 8c shows that a larger ❕ value leads to higher precision but lower recall. Thus, ❕ can be adjusted to achieve a higher true positive rate or lower false positive rate. Additionally, DeepLog’s prediction cost per log entry is approximately 1 millisecond on a standard workstation, which can be further improved with better hardware, such as a GPU.

### 5.2 Parameter Value and Performance Anomalies
To evaluate the effectiveness of DeepLog in detecting parameter value and performance anomalies (including elapsed time between log entries), we used system logs from the OpenStack VM creation task. This dataset includes both types of anomalies: performance anomalies (late arrival of a log entry) and parameter value anomalies (log entries with unusually long VM creation times).

#### Experiment Setup
We deployed an OpenStack experiment on CloudLab and wrote a script to simulate multiple users constantly requesting VM creations and deletions. During VM creation, an important step is copying the required image from the controller node to a compute node. To simulate a performance anomaly, we throttled the network speed from the controller to the compute nodes at two different points to see if these anomalies could be detected by DeepLog.

#### Anomaly Detection
As described in Section 3.2, we separated log entries into two sets: one for model training and the other (the validation set) to generate the Gaussian distribution of mean square errors (MSEs). In the online detection phase, for every incoming parameter value vector \( v \), DeepLog checks if the MSE between \( v \) and the prediction output (also a vector) from its model is within an acceptable confidence interval of the Gaussian distribution of MSEs from the validation set.

Figure 9 shows the detection results for the parameter value vectors of different log keys, where the x-axis represents the ID of the VM being created, and the y-axis represents the MSE between the parameter value vector and the prediction output vector from DeepLog. The horizontal lines in each figure are the confidence interval thresholds for the corresponding MSE Gaussian distributions. Figures 9a and 9b represent log keys with normal parameter value vectors throughout the entire time, while Figures 9c and 9d show that the parameter value vectors for keys 53 and 56 are successfully detected as abnormal at the exact times when we throttled the network speed (i.e., injected anomalies).

For each detected abnormal parameter value vector, we identified the value that differed the most from the prediction to pinpoint the abnormal feature. We found that the two abnormal parameter value vectors for key 53 were due to unusually large elapsed time values. Key 56, "Took * seconds to build instance," had two abnormal parameter value vectors caused by unusually large values for seconds.

### 5.3 Online Update and Training of DeepLog
Section 5.1 demonstrated that DeepLog requires a very small training set (less than 1% of the entire log) and does not need user feedback during training. However, new normal execution paths may appear during the detection stage, leading to false positives. To address this, we evaluate the effectiveness of DeepLog’s online update and training module, as described in Section 3.3, using the difference in detection results with and without incremental updates.

#### Log Dataset
The log dataset used in this section is from the Blue Gene/L supercomputer system, containing 4,747,963 log entries, of which 348,460 are labeled as anomalies. We chose this dataset because many log keys appear only during specific time periods, meaning the training data may not contain all possible normal log keys or execution patterns.

#### Evaluation Results
We conducted two experiments: one using the first 1% of normal log entries for training and the other using the first 10%. The remaining 99% or 90% of entries were used for anomaly detection. We set \( L = 1 \), \( \alpha = 256 \), ❕ = 6, and \( h = 3 \).

Figure 10 shows the results for both scenarios. Without online training, DeepLog tests incoming log entries without any incremental updates, resulting in many false positives (low precision). Increasing the training data to 10% slightly improves precision but remains unsatisfactory. With online training, DeepLog significantly improves precision and F-measure scores. With a true positive rate of 100% (perfect recall) in both settings, online training reduces the false positive rate from 40.1% to 1.7% for 1% training data and from 38.2% to 1.1% for 10% training data.

Table 5 shows the amortized cost to check each log entry. Online update and training slightly increase the amortized cost per log entry, but the effect is minimal because many log entries do not trigger an update. Note that online update and detection can be executed in parallel, with updates carried out while the model continues performing detection.

### 5.4 Security Log Case Studies
Anomalies with log keys that never appeared in normal training logs (e.g., "ERROR" or "exception" messages) are easy to detect. DeepLog can effectively detect more subtle cases, such as missing or extra log keys in HDFS logs. Any attack causing changes in system behavior reflected in logs can be detected. We investigate system logs containing real attacks to demonstrate DeepLog’s effectiveness.

#### Network Security Log
Network security is crucial, and both firewalls and intrusion detection systems (IDS) produce logs for online anomaly detection. We tested DeepLog on the VAST Challenge 2011 dataset, specifically Mini Challenge 2 — Computer Networking Operations. The challenge involves manually looking for suspicious activities using visualization techniques, with ground truth for anomalous activities. Table 6 shows DeepLog’s results, with the only undetected activity being the first appearance of an undocumented computer IP address.

#### BROP Attack Detection
Blind Return Oriented Programming (BROP) attacks leverage the fact that server applications restart after a crash to ensure service reliability. This type of attack is powerful and practical because the attacker does not need access to source code or binaries. A stack buffer overflow vulnerability, leading to a server crash, is sufficient to carry out this attack. In a BROP exploit, the attacker uses server crashes as signals to complete a ROP attack, executing shellcode. Repeated server restarts leave atypical log messages in kernel logs, easily detected by DeepLog.

### 5.5 Task Separation and Workflow Construction
We implemented the proposed methods in Section 4 and evaluated them on a log with various OpenStack VM-related tasks. Both the LSTM approach and the density-based clustering approach successfully separated all tasks. The LSTM method is supervised and requires training data, while the clustering method is unsupervised and requires a distance threshold \( \tau \). For the density-based clustering approach, a sufficiently large threshold value \( \tau \in [0.85, 0.95] \) clearly separates all tasks. A too-large \( \tau \) (e.g., \( \tau = 1 \)) can break log entries from the same task apart due to random log entries from background processes.

Using a part of the VM creation workflow, we show how it provides useful diagnosis of the performance anomaly in Section 5.2. Recall that in Section 5.2, parameter value vector anomalies were identified in the elapsed time of log key 53 and the parameter position of log key 56 (representing the number of seconds to build an instance). As shown in Figure 11, once an anomaly is detected, the workflow helps diagnose the issue.