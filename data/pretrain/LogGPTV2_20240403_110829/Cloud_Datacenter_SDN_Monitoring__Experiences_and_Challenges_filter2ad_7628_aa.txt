title:Cloud Datacenter SDN Monitoring: Experiences and Challenges
author:Arjun Roy and
Deepak Bansal and
David Brumley and
Harish Kumar Chandrappa and
Parag Sharma and
Rishabh Tewari and
Behnaz Arzani and
Alex C. Snoeren
Cloud Datacenter SDN Monitoring: Experiences and Challenges
Arjun Roy, Deepak Bansal†, David Brumley†, Harish Kumar Chandrappa†
Parag Sharma†, Rishabh Tewari†, Behnaz Arzani† and Alex C. Snoeren
Department of Computer Science and Engineering
University of California, San Diego
†Microsoft Corporation
ABSTRACT
Cloud customers require highly reliable and performant leased
datacenter infrastructure to deliver quality service for their users.
It is thus critical for cloud providers to quickly detect and mitigate
infrastructure faults. While much is known about managing faults
that arise in the datacenter physical infrastructure (i.e., network
and server equipment), comparatively little has been published
regarding management of the logical overlay networks frequently
employed to provide strong isolation in multi-tenant datacenters.
We present a first look into the nuances of monitoring these
“virtualized” networks through the lens of a large cloud provider.
We describe challenges to building cloud-based fault monitoring
systems, and use the output of a production system to illuminate
how virtualization impacts multi-tenant datacenter fault manage-
ment. We show that interactions between the virtualization, tenant
software, and lower layers of the network fabric both simplify and
complicate different aspects of fault detection and diagnosis efforts.
CCS CONCEPTS
• Networks → Network measurement; Cloud computing;
Network monitoring; Network performance analysis; Data center
networks; Overlay and other logical network structures; Network
management;
ACM Reference Format:
Arjun Roy, Deepak Bansal, David Brumley, Harish Kumar Chandrappa,Parag
Sharma, Rishabh Tewari, Behnaz Arzani and Alex C. Snoeren. 2018. Cloud
Datacenter SDN Monitoring: Experiences and Challenges. In 2018 Internet
Measurement Conference (IMC ’18), October 31-November 2, 2018, Boston,
MA, USA. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/
3278532.3278572
1 INTRODUCTION
Web service operators and IT-dependent enterprises increasingly
rely on cloud providers to address their computational needs. Cloud
Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM ISBN 978-1-4503-5619-0/18/10...$15.00
https://doi.org/10.1145/3278532.3278572
tenants simultaneously expect high reliability and performance
while realizing cost and management advantages from leveraging
leased, shared infrastructure. To support potentially conflicting
customer use-cases in shared infrastructure, cloud providers employ
host virtualization to provide resource multiplexing and isolation.
Similarly, network virtualization allows tenants to operate within
cloud datacenters without undue complexity or contention. Rather
than sharing IP addresses and logical network topology with other
customers, tenants typically operate inside of virtual networks
(VNETs) within a software-defined network (SDN) overlay provided
by the datacenter operator.
To provide high performance, datacenter operators monitor net-
works to rapidly detect, localize, and mitigate faults as they in-
evitably occur [4, 7, 9, 12, 17, 18] and potentially harm application
performance [9, 14]. Datacenter monitoring has received much
recent attention in the context of physical network fabrics. Little
is yet known—in academic literature, at least—of the operational
realities of virtualized, multi-tenant networks. While the end goal is
the same, virtualized networks impart additional challenges in the
form of black-box tenants and increased infrastructure complexity.
Here, we provide a first look into cloud network fault manage-
ment, focusing on tenant VNET monitoring within Microsoft Azure
using VNET Pingmesh. We present some of our early operational
experiences over a period of several months (backed by collected
production data where relevant), allowing us to answer the follow-
ing high-level questions:
(1) How can cloud operators monitor VNET performance,
given black-box tenants? Can physical tools [9, 12, 18] be
usefully adapted to virtualized networks?
(2) How accurate are adapted monitors within virtualized envi-
ronments? Can they detect customer-impacting faults? Do
they exhibit high precision and recall?
(3) Beyond monitoring, how do virtual environments impact
fault management? How might we triage which layer of
the network is resposible for a fault? How are fault diagnosis,
root-causing and mitigation impacted?
While VNET Pingmesh demonstrates that physical-layer monitor-
ing tools can be successfully adapted to VNET overlays and deliver
monitoring wins, we find that the additional complexities of tenant
networks and virtualization create subtle challenges that can con-
found VNET monitoring. Our preliminary experience suggests two
takeaway lessons:
First, cross-layer aliasing can complicate monitoring measure-
ment interpretation compared to well-understood physical-layer
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
Roy et al.
monitors. Both compute-substrate and customer-driven effects have
complicated analysis by either raising alerts unrelated to customer
impact, or potentially overshadowing actual faults. Thus, cross-
layer aliasing can impact detection precision and recall. Furthermore,
while confounding effects can be accounted for when encountered,
the ever-increasing search space of cross-layer interactions coupled
with ever-expanding network feature sets suggests that handling
all possible effects a priori is infeasible.
Second, even after fault detection, diagnosing, root-causing and
mitigating network overlay faults are significantly harder than
at the physical layer. Since (baremetal) hosts serve as packet-
forwarding devices, we are subject both to performance impacts
caused by workloads we do not control—both customer applica-
tions and first-party host management—as well as occasional, hard
to diagnose server anomalies. While prior approaches to physical
layer faults can route around damage or reboot affected devices,
VNET architecture and tenant workloads can make overlay faults
comparatively sticky, since it is harder to route around the only
virtual switch providing tenant VM network access, or to reboot
servers hosting smaller tenant deployments.
2 PHYSICAL NETWORK MONITORING
Large-scale, multi-path datacenter network fabrics have been sub-
ject to considerable scrutiny; various studies have provided both tax-
onomies of common network failures [9, 13, 14, 16, 18], networked
application performance [11, 14], and methods for pinpointing the
cause and location of performance-sapping faults [3, 4, 12, 14].
Contemporary datacenter (physical-layer) monitoring often ac-
tively probes networks by injecting synthetic traffic to ascertain live-
ness and adequate performance [2, 8, 9, 15]. The Azure cloud uses
Pingmesh [9] (among other systems [5, 8, 18]) to do so. Pingmesh
maintains a (non-full due to scaling concerns) mesh of ping mea-
surements, allowing at-a-glance visualizations depicting latency
and connectivity between servers in a rack, between racks in a
datacenter and amongst datacenters. Two-dimensional heatmap
visualizations provide distinctive patterns depending on the various
kinds of switch failures encountered (e.g. rack failures are visually
distinct from core switch failures), while the measured latency pro-
vides insight into the quality of network performance. For example,
an increase in p50 latency might signify switch queue buildups in
the network core, while an increase only in p99 latency may signify
errors causing packet drops.
Pingmesh has provided several monitoring wins at Microsoft,
including detecting hard-to-find silent packet drops and black
holes [9]. Thus, a natural question to ask is whether Pingmesh
can be adapted to monitor VNETs, and how effective it would be
in this environment. Next, we discuss an uplifted version of this
system called VNET Pingmesh and examine its efficacy.
3 MONITORING AZURE VNETS
Microsoft Azure consists of datacenters across the world. A region
can contain several datacenters, each with several clusters. Clus-
ters contain racks aggregating servers that multiplex VMs that are
organized into non-interfering VNETs.
3.1 VNET addressing and packet handling
VNETs are topologically flat, L3-addressed IP network overlays
built atop the physical topology. VNETs can aggregate thousands
of individual VMs, each with one or more virtual NICs. Each NIC
has a customer-chosen virtualized “customer IP address” or “CA”.
Customer applications on a VM address other VMs in the VNET
using CAs; CAs can be reused without conflict in disjoint VNETs.
VM network access is provided via a bespoke physical-server-
based virtual switch (“VSwitch”) called VFP [5]. Each VFP instance
has several virtual ports; one is connected to a physical NIC and the
others to VM virtual NICs. An outbound VM NIC packet is trans-
formed by per-port processing layers, each with distinct tasks (like
metering traffic or implementing customer ACLs [5]). One layer
translates CAs to an IP “physical address” (“PA”) that is routable on
the underlying network, providing VNET isolation (VFP translates
PAs to CAs for received packets as well). CA ⇒ PA mappings are dy-
namic; actions like creating or deleting a VNET or VM can change
mappings. Mappings reside in a reliable distributed directory. A
per-server userspace agent receives updated mappings from this
directory as network allocation state evolves. When a VM starts a
network flow to a given CA, the CA ⇒ PA mapping for the flow is
queried from the userspace agent and cached in the kernel datap-
ath; subsequent packets leverage this cache. Cached mappings are
evicted after inactivity timeouts.
3.2 Monitoring via VNET Pingmesh
Baremetal monitoring alone does not account for VNET-specific
performance anomalies. To make a VNET-level Pingmesh-like sys-
tem, however, several cloud-specific challenges must be accounted
for. We divide these challenges into two categories—readily appar-
ent implementation hurdles that are foreseen and handled in the
design phase, and more subtle behaviours that only became appar-
ent once the system was deployed. Here, we discuss the former,
and defer examining subtle behaviours to Section 4.
(1) Black-box VMs. Privacy concerns mean we may not run
any software, nor collect statistics, within VMs.
(2) Avoiding customer impact. Probes must not be billed to,
visible to, impacted by or spoofable by tenants.
(3) Interactions with customer rulesets. Customer ACLs
supporting firewalls and gateways, or “User-Defined Routes”
(“UDRs”) supporting middlebox behavior, can both interfere
with VNET Pingmesh.
(4) Interactions with customer actions. Tenants can un-
predictably shutdown VMs, causing ping failures. If unac-
counted for, we suffer persistent false-positive loss indica-
tions within VNETs where tenants shut down VMs during
non-business hours to save costs.
To account for black-box VMs, VNET Pingmesh is implemented
via VSwitch interposition. VFP injects outbound TCP-based ping
packets after the tenant metering and ACL management layers. Out-
bound packets are invisible (and not metered/billed) to the tenant
and avoid tenant ACL rules, while still being subject to the rest of
the VNET processing stack. On the remote end, VFP intercepts ping
packets before the tenant processes it, and sends a response. VFP
also installs rules to prevent tenants from spoofing pings, ensuring
Cloud Datacenter SDN Monitoring