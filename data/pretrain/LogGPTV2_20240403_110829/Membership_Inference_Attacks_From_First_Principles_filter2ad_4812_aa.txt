title:Membership Inference Attacks From First Principles
author:Nicholas Carlini and
Steve Chien and
Milad Nasr and
Shuang Song and
Andreas Terzis and
Florian Tramèr
2022 IEEE Symposium on Security and Privacy (SP)
Membership Inference Attacks From First Principles
Nicholas Carlini∗1
Steve Chien1 Milad Nasr1,2
Shuang Song1 Andreas Terzis1
1 Google Research
2 University of Massachusetts Amherst
Florian Tram`er1
9
4
6
3
3
8
9
.
2
2
0
2
.
4
1
2
6
4
P
S
/
9
0
1
1
.
0
1
:
I
O
D
|
E
E
E
I
2
2
0
2
©
0
0
.
1
3
$
/
2
2
/
9
-
6
1
3
1
-
4
5
6
6
-
1
-
8
7
9
|
)
P
S
(
y
c
a
v
i
r
P
d
n
a
y
t
i
r
u
c
e
S
n
o
m
u
i
s
o
p
m
y
S
E
E
E
I
2
2
0
2
Abstract—A membership inference attack allows an adversary
to query a trained machine learning model to predict whether or
not a particular example was contained in the model’s training
dataset. These attacks are currently evaluated using average-case
“accuracy” metrics that fail to characterize whether the attack
can conﬁdently identify any members of the training set. We
argue that attacks should instead be evaluated by computing
their true-positive rate at low (e.g., ≤ 0.1%) false-positive rates,
and ﬁnd most prior attacks perform poorly when evaluated in
this way. To address this we develop a Likelihood Ratio Attack
(LiRA) that carefully combines multiple ideas from the literature.
Our attack is 10× more powerful at low false-positive rates, and
also strictly dominates prior attacks on existing metrics.
I. INTRODUCTION
Neural networks are now trained on increasingly sensitive
datasets, and so it is necessary to ensure that trained models are
privacy-preserving. In order to empirically verify if a model is
in fact private, membership inference attacks [60] have become
the de facto standard [42, 63] because of their simplicity. A
membership inference attack receives as input a trained model
and an example from the data distribution, and predicts if that
example was used to train the model.
Unfortunately as noted by recent work [44, 69], many prior
membership inference attacks use an incomplete evaluation
methodology that considers average-case success metrics (e.g.,
accuracy or ROC-AUC) that aggregate an attack’s accuracy
over an entire dataset and over all detection thresholds [6, 18,
26, 33–35, 45, 52, 54, 54–57, 61, 63, 66, 70]. However, privacy
is not an average case metric, and should not be evaluated as
such [65]. Thus, while existing membership inference attacks
do appear effective when evaluated under this average-case
methodology, we make the case they do not actually effectively
measure the worst-case privacy of machine learning models.
Contributions. In this paper we re-examine the problem
statement of membership inference attacks from ﬁrst princi-
ples. We ﬁrst argue that membership inference attacks should
be evaluated by considering their true-positive rate (TPR) at
low false-positive rates (FPR). This objective of designing
methods around low false-positive rates is typical in many
areas of computer security [21, 27, 28, 31, 41, 49], and for
similar reasons it is the right metric here. If a membership
inference attack can reliably violate the privacy of even just
a few users in a sensitive dataset, it has succeeded. And con-
versely, an attack that only unreliably achieves high aggregate
attack success rate should not be considered successful.
When evaluated this way, we ﬁnd most prior attacks fail
in the low false-positive rate regime. Furthermore, aggregate
Fig. 1: Comparing the true-positive rate vs. false-positive rate
of prior membership inference attacks reveals a wide gap in
effectiveness. An attack’s average accuracy is not indicative
of its performance at low FPRs. By extending on the most
effective ideas, we improve membership inference attacks by
10×, for a non-overﬁt CIFAR-10 model (92% test accuracy).
metrics (e.g., AUC) are often uncorrelated with low FP success
rates. For example the attack of Yeom et al. [70] has a high
accuracy (59.5%) yet fails completely at low FPRs, and the
attack of Long et al. [36] has a much lower accuracy (53.5%)
but achieves higher success rates at low FPRs.
We develop a Likelihood Ratio Attack (LiRA) that succeeds
10× more often than prior work at
low FPRs—but still
strictly dominates prior attacks on aggregate metrics intro-
duced previously. Our attack combines per-example difﬁculty
scores [37, 56, 68] with a principled and well-calibrated Gaus-
sian likelihood estimate. Figure 1 shows the success rate of our
attack on a log-scale Receiver Operating Characteristic (ROC)
curve [59], comparing the ratio of true-positives to false-
positives. We perform an extensive experimental evaluation to
understand each of the factors that contribute to our attack’s
success, and release our open source code.1
Future work will need to re-examine many questions that
have been studied using prior, much less effective, membership
inference attacks. Attacks that use less information (e.g., label-
only attacks [6, 34, 54]) may or may not achieve high success
rate at low false-positive rates; algorithms previously seen as
“private” because they resist prior attacks might be vulnerable
to our new attack; and old defenses dismissed as ineffective
might be able to defend against these new stronger attacks.
∗ Authors ordered alphabetically.
1https://github.com/tensorﬂow/privacy/tree/master/research/mi lira 2021
© 2022, Nicholas Carlini. Under license to IEEE.
DOI 10.1109/SP46214.2022.00090
11897
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:24:27 UTC from IEEE Xplore.  Restrictions apply. 
105104103102101100False Positive Rate105104103102101100True Positive RateOurs.  (acc=63.7%)Ye et al.  (acc=60.0%)Sablayrolles et al.  (acc=56.1%)Long et al.  (acc=53.5%)Watson et al. (acc=59.1%)Shokri et al.  (acc=59.5%)Song et al.  (acc=59.5%)Yeom et al.  (acc=59.5%)Jayaraman et al.  (acc=59.0%)II. BACKGROUND
We begin with a background that will be familiar to readers
knowledgeable of machine learning privacy.
A. Machine learning notation
A classiﬁcation neural network fθ : X → [0, 1]n is a
learned function that maps some input data sample x ∈ X
to an n-class probability distribution; we let f (x)y denote the
probability of class y. Given a dataset D sampled from some
underlying distribution D, we write fθ ← T (D) to denote that
the neural network f parameterized with weights θ is learned
by running the training algorithm T on the training set D.
Neural networks are trained via stochastic gradient descent
[32] to minimize some loss function (cid:96):
θi+1 ← θi − η
∇θ(cid:96)(fθi(x), y)
(1)
(cid:88)
(x,y)∈B
Here, B is a batch of random training examples from D, and
η is the learning rate, a small constant. For classiﬁcation tasks,
the most common loss function is the cross-entropy loss:
(cid:96)(fθ(x), y) = − log(fθ(x)y) .
i ezi , . . . ,
When the weights θ are clear from context, we will simply
write a trained model as f. At times it will be useful to view
a model f as a function f (x) = σ(z(x)), where z : X →
softmax normalization layer σ(z) = [ ez1(cid:80)
Rn returns the feature outputs of the network, followed by a
ezn(cid:80)
i ezi ].
Training neural networks that reach 100% training accuracy
is easy—running the gradient descent from Equation 1 on any
sufﬁciently sized neural network eventually achieves this goal
[72]. The difﬁculty is in training models that generalize to an
unseen test set Dtest ← D drawn from the same distribution.
There are a number of techniques to increase the generalization
ability of neural networks (augmentations [7, 67, 73], weight
regularization [30], tuned learning rates [23, 38]). For the
remainder of this paper, all models we train use state-of-the-art
generalization-enhancing techniques. This makes our analysis
much more realistic than prior work, which often uses models
with 2–5× higher error rates than our models.
B. Training data privacy
Neural networks must not
leak details of their training
datasets, particularly when used in privacy-sensitive scenarios
[5, 13]. The ﬁeld of training data privacy constructs attacks
that leak data, develops techniques to prevent memorization,
and measures the privacy of proposed defenses.
a) Privacy attacks: There are various forms of attacks
on the privacy of training data. Training data extraction [4]
is an explicit attack where an adversary recovers individual
examples used to train the model. In contrast, model inversion
attacks recover aggregate details of particular sub-classes
instead of individual training examples [16]. Finally, property
inference attacks aim at inferring non-trivial properties of the
training dataset. For example, a classiﬁer trained on bitcoin
logs can reveal whether or not the machines that generated
the logs were patched for Meltdown and Spectre [17].
We focus on a more fundamental attack that predicts if a
particular example is part of a training dataset. First explored
as tracing attacks [11, 12, 22, 59] on medical datasets, they
were exended to machine learning models as membership
inference attacks [60]. In these settings, being able to reliably
(with high precision) identify a few users as being contained
in sensitive medical datasets is itself a privacy violation [22]—
even if this is done with low recall. Further, membership
inference attacks are the foundation of stronger extraction
attacks [3, 4], and in order to be used in this way must again
have exceptionally high precision.
b) Theory of memorization: The ability to perform mem-
bership inference is directly tied to a model’s ability to
memorize individual data points or labels. Zhang et al. [72]
demonstrated that standard neural networks can memorize en-
tirely randomly labeled datasets. A recent line of work initiated
by Feldman [14] shows both theoretically and empirically that
some amount of memorization may be necessary to achieve