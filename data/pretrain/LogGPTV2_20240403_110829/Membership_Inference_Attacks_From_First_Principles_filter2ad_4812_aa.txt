# Title: Membership Inference Attacks From First Principles

## Authors:
- Nicholas Carlini
- Steve Chien
- Milad Nasr
- Shuang Song
- Andreas Terzis
- Florian Tramèr

### Affiliations:
1. Google Research
2. University of Massachusetts Amherst

### Conference:
2022 IEEE Symposium on Security and Privacy (SP)

---

## Abstract
Membership inference attacks allow an adversary to query a trained machine learning model to determine whether a specific example was part of the model's training dataset. Current evaluations of these attacks use average-case "accuracy" metrics, which fail to characterize the attack's ability to confidently identify any members of the training set. We argue that attacks should instead be evaluated by their true-positive rate at low (e.g., ≤ 0.1%) false-positive rates. Most prior attacks perform poorly under this metric. To address this, we develop a Likelihood Ratio Attack (LiRA) that combines multiple ideas from the literature. Our attack is 10× more effective at low false-positive rates and outperforms prior attacks on existing metrics.

---

## I. Introduction
Neural networks are increasingly trained on sensitive datasets, making it essential to ensure that trained models preserve privacy. Membership inference attacks have become the de facto standard for empirically verifying a model's privacy [42, 63]. These attacks receive a trained model and an example from the data distribution and predict if the example was used in the model's training.

Recent work [44, 69] has highlighted that many prior membership inference attacks use an incomplete evaluation methodology, focusing on average-case success metrics such as accuracy or ROC-AUC. However, privacy is not an average-case metric and should not be evaluated as such [65]. While existing attacks appear effective under average-case evaluation, they do not effectively measure the worst-case privacy of machine learning models.

### Contributions
In this paper, we re-examine the problem statement of membership inference attacks from first principles. We argue that these attacks should be evaluated by their true-positive rate (TPR) at low false-positive rates (FPR). This approach is typical in many areas of computer security [21, 27, 28, 31, 41, 49] and is appropriate here. If a membership inference attack can reliably violate the privacy of even a few users in a sensitive dataset, it has succeeded. Conversely, an attack that only unreliably achieves high aggregate success rates should not be considered successful.

When evaluated using TPR at low FPRs, most prior attacks fail. Additionally, aggregate metrics like AUC are often uncorrelated with low FP success rates. For example, the attack by Yeom et al. [70] has a high accuracy (59.5%) but fails at low FPRs, while the attack by Long et al. [36] has a lower accuracy (53.5%) but performs better at low FPRs.

We develop a Likelihood Ratio Attack (LiRA) that is 10× more effective than prior work at low FPRs and still outperforms prior attacks on aggregate metrics. Our attack combines per-example difficulty scores [37, 56, 68] with a principled and well-calibrated Gaussian likelihood estimate. Figure 1 shows the success rate of our attack on a log-scale Receiver Operating Characteristic (ROC) curve, comparing the ratio of true-positives to false-positives. We conduct extensive experiments to understand the factors contributing to our attack's success and release our open-source code.1

Future work will need to re-evaluate many questions studied using less effective membership inference attacks. Attacks that use less information (e.g., label-only attacks [6, 34, 54]) may or may not achieve high success rates at low FPRs. Algorithms previously considered private because they resist prior attacks might be vulnerable to our new attack. Defenses dismissed as ineffective might be able to defend against these stronger attacks.

*Authors ordered alphabetically.
1. <https://github.com/tensorflow/privacy/tree/master/research/milira_2021>

© 2022, Nicholas Carlini. Under license to IEEE.
DOI 10.1109/SP46214.2022.00090

---

## II. Background
We begin with a background that will be familiar to readers knowledgeable about machine learning privacy.

### A. Machine Learning Notation
A classification neural network \( f_\theta : X \to [0, 1]^n \) is a learned function that maps an input data sample \( x \in X \) to an n-class probability distribution. We denote the probability of class \( y \) as \( f(x)_y \). Given a dataset \( D \) sampled from some underlying distribution \( \mathcal{D} \), we write \( f_\theta \leftarrow T(D) \) to indicate that the neural network \( f \) parameterized with weights \( \theta \) is learned by running the training algorithm \( T \) on the training set \( D \).

Neural networks are trained via stochastic gradient descent [32] to minimize a loss function \( \ell \):
\[
\theta_{i+1} \leftarrow \theta_i - \eta \sum_{(x, y) \in B} \nabla_\theta \ell(f_{\theta_i}(x), y)
\]
where \( B \) is a batch of random training examples from \( D \), and \( \eta \) is the learning rate, a small constant. For classification tasks, the most common loss function is the cross-entropy loss:
\[
\ell(f_\theta(x), y) = -\log(f_\theta(x)_y)
\]

When the weights \( \theta \) are clear from context, we simply write a trained model as \( f \). At times, it is useful to view a model \( f \) as a function \( f(x) = \sigma(z(x)) \), where \( z : X \to \mathbb{R}^n \) returns the feature outputs of the network, followed by a softmax normalization layer \( \sigma(z) = \left[ \frac{e^{z_1}}{\sum_i e^{z_i}}, \ldots, \frac{e^{z_n}}{\sum_i e^{z_i}} \right] \).

Training neural networks to reach 100% training accuracy is straightforward; running the gradient descent from Equation 1 on any sufficiently sized neural network eventually achieves this goal [72]. The challenge lies in training models that generalize to an unseen test set \( D_{\text{test}} \leftarrow \mathcal{D} \) drawn from the same distribution. Techniques to enhance generalization include data augmentations [7, 67, 73], weight regularization [30], and tuned learning rates [23, 38]. All models in this paper use state-of-the-art generalization-enhancing techniques, making our analysis more realistic than prior work, which often uses models with 2–5× higher error rates.

### B. Training Data Privacy
Neural networks must not leak details of their training datasets, especially in privacy-sensitive scenarios [5, 13]. The field of training data privacy involves constructing attacks that leak data, developing techniques to prevent memorization, and measuring the privacy of proposed defenses.

#### a) Privacy Attacks
There are various forms of attacks on the privacy of training data. Training data extraction [4] is an explicit attack where an adversary recovers individual examples used to train the model. Model inversion attacks recover aggregate details of particular sub-classes instead of individual training examples [16]. Property inference attacks aim to infer non-trivial properties of the training dataset. For example, a classifier trained on Bitcoin logs can reveal whether the machines generating the logs were patched for Meltdown and Spectre [17].

We focus on a more fundamental attack that predicts if a particular example is part of a training dataset. Initially explored as tracing attacks [11, 12, 22, 59] on medical datasets, they were extended to machine learning models as membership inference attacks [60]. In these settings, being able to reliably (with high precision) identify a few users as being contained in sensitive medical datasets is itself a privacy violation [22]—even if this is done with low recall. Furthermore, membership inference attacks are the foundation of stronger extraction attacks [3, 4], and to be used in this way, they must have exceptionally high precision.

#### b) Theory of Memorization
The ability to perform membership inference is directly tied to a model's ability to memorize individual data points or labels. Zhang et al. [72] demonstrated that standard neural networks can memorize entirely randomly labeled datasets. Recent work initiated by Feldman [14] shows both theoretically and empirically that some amount of memorization may be necessary to achieve good generalization.