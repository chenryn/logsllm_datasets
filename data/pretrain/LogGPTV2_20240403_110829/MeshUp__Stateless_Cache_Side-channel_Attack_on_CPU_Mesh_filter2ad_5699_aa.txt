title:MeshUp: Stateless Cache Side-channel Attack on CPU Mesh
author:Junpeng Wan and
Yanxiang Bi and
Zhe Zhou and
Zhou Li
MeshUp: Stateless Cache Side-channel Attack on
CPU Mesh
Junpeng Wan, Yanxiang Bi, Zhe Zhou†
{19210240003,19210240167,zhouzhe}@fudan.edu.cn
Fudan University
Zhou Li
University of California, Irvine
PI:EMAIL
Abstract—Cache side-channel attacks lead to severe security
threats to the settings where a CPU is shared across users, e.g.,
in the cloud. The majority of attacks rely on sensing the micro-
architectural state changes made by victims, but this assumption
can be invalidated by combining spatial (e.g., Intel CAT) and
temporal isolation. In this work, we advance the state of cache
side-channel attacks by showing stateless cache side-channel
attacks on server-grade CPUs, that can bypass both spatial and
temporal isolation.
Unlike stateful cache side-channel attacks that rely on the
timing difference between a cache hit or miss, our attack exploits
the timing difference caused by the interconnect congestion.
Speciﬁcally, to complete cache transactions, for Intel server CPUs,
which use non-inclusive and mesh interconnect, cache lines would
travel across cores via the CPU mesh and UPI interconnects.
Nonetheless, the interconnects are shared by all cores, and
cache isolation does not segregate the trafﬁc. An attacker can
generate trafﬁc to contend with a victim on a link, measure the
extra delay, deduce the memory access pattern of the victim’s
program, and infer its sensitive data. Based on this idea, we
implement MESHUP, a stateless cache side-channel against mesh
interconnect, and test it against the existing RSA implementations
of JDK for the cross-core attack and application ﬁngerprinting
for the the cross-CPU attack. We found the RSA private key used
by a victim process can be partially recovered and the co-running
application can be inferred at high accuracy.
I. INTRODUCTION
Memory isolation is one of the fundamental security prin-
ciples to protect sensitive information. Yet, memory isolation
does not protect the computing resources, like cache, resulting
in side-channel attacks under machine sharing scenarios, e.g.,
in the cloud. By timing the interval of accessing a cache line,
existing cache attacks learn whether the associated memory
addresses have been loaded by a victim program, and further
deduce sensitive information. Many techniques have been
proposed, like FLUSH+RELOAD [1], PRIME+PROBE [2], and
the recent Xlate [3], based on different assumptions, e.g., using
shared libraries [1], sharing LLC (Last-level Cache) across
cores [4], or MMU (Memory Management Units), etc.
These attacks can be categorized into stateful cache attacks,
as the victim program introduces micro-architectural state
changes that can be sensed by an attacker [5], e.g., through
creating eviction sets. However, this attack condition may
not be fulﬁlled nowadays due to the rise of spatial and
temporal isolation. For example, Intel Xeon CPUs introduce
† Zhe Zhou is the corresponding author.
Cache Allocation Technique (CAT) [6], which was designed
to maintain QoS of cache usage, but later found to be a
panacea for cache attacks [7]. CAT assigns LLC cache ways
to cores exclusively, which spatially isolate LLC and break
attacks based on eviction set conﬂict on LLC (e.g.,
[4]).
Temporal isolation [5] provides more principled protection,
which is effective against nearly every cache attack with the
existing hardware support.
Stateless Cache Side-channel Attack. Although spatial and
temporal isolation can eliminate the root cause of cache side-
channel attacks, or micro-architectural state change, stateless
cache side-channel attacks could bypass both of them. One
such stateless channel is the CPU interconnect, which links
different CPU units like cache and cores. Due to the complex
interplay between these CPU units, it is possible that cache
access can be observed on the interconnect by an adversary. In
fact, a recent work [8] showed that the CPU ring interconnect
can be exploited to launch stateless attacks. Ring interconnect
is prevalent in consumer-grade CPUs, but not in server-grade
CPUs. Whether these server-grade CPUs, which are more
relevant to the cloud settings, are vulnerable is unclear.
In this work, we explore the attack surface on the server-
grade CPUs. By investigating the latest architecture of Intel
server CPU, e.g., Xeon Scalable Processor (SP) [9], we found
that cores and uncore units inside a CPU are connected
with mesh, constituting an NoC (Network on Chip). Besides,
i.e.,
different CPUs are also linked with an interconnect,
UPI (Ultra Path Interconnect)1. Although these interconnects
showed a great advantage in latency and bandwidth on multi-
core CPUs [10], [11], they could leak information about the
memory access pattern of a program, because of the timing
difference resulting from congestion on those interconnects.
Based on this insight, we propose MESHUP, a new stateless
cache side-channel attack against CPU interconnects. Our
key idea is to let a core occupied by an attacker program
keep probing the path that the cache transactions of a victim
program might pass by, and measure the delay. When the
core occupied by the victim program accesses a remote cache
agent, the accumulated mesh trafﬁc volume will rise, hence
increasing the delays observed by the attacker. By probing the
interconnects at high frequency, the attacker could deduce the
1UPI is also used as interconnect inside CPU by extreme high-end CPUs,
e.g., Xeon 9200 series.
victim’s secret with the delay traces.
Challenges. Still implementing the idea of MESHUP is chal-
lenging. 1) We assume the attack can be executed on a cloud
VM, which prohibits an attacker to choose a core or mesh
path at his/her will. 2) There is no API to let a program direct
mesh packets to a given target and the information retained
from mesh congestion is expected to be coarse-grained (i.e., it
does not tell which cache lines are conﬂicted). 3) Cross-CPU
attack is even more difﬁcult as there are more units involved.
Attack Techniques. We have investigated the side-channel
leakages in both cross-core and cross-CPU settings. For the
ﬁrst case, we develop a new eviction-based probe, which
allows an attacker to probe a mesh route and measure the
delays. In particular, our approach constructs an L2 eviction
set, which can be mapped to the desired LLC slice, and
contained in an L2 set. To notice is that the eviction set
does not conﬂict with the victim. It is only used to cause
cache eviction and generate mesh trafﬁc. Therefore, defenses
trying to prevent adversarial cache eviction can be evaded.
For the second case, we found that, though the eviction-based
probe cannot reliably generate the cross-CPU trafﬁc, cache
synchronization by two CPUs could introduce a high volume
of trafﬁc, and congest the link. Hence, we develop a new
coherence-based probe for cross-CPU attacks. Our analysis
of the two probes shows they can achieve good temporal
resolution, high Signal to Noise Ratio (SNR), and spatial
resolution that allows the attacker to contend to the victim’s
trafﬁc even starting off from a random core.
As a showcase for the attack effectiveness, we analyzed
the Sliding Window algorithm of RSA with the eviction-
based probe to recover 2048-bit private RSA keys. On the
off-the-shelf implementation of JDK, the attacker can recover
over 31% of the 2048 bits, with the help of a cryptographic
method [12]. For the cross-CPU attack, we assume a victim
runs an application (app) in server, and let the attacker infer
which app the victim is running, with the coherence-based
probe. The attacker has over 82% accuracy in recognizing the
apps co-located on the machine.
Contributions. We summarize the main contribution of our
work as follows.
• We identify a new security implication of server-grade
CPU interconnects, and show it can be exploited to
construct a powerful stateless side-channel.
• We develop MESHUP, using cache eviction and cache
synchronization as probe techniques to conduct cross-
core and cross-CPU cache side-channel attacks.
• We systematically analyze the properties of MESHUP
channels.
• To show the consequences of MESHUP, we evaluate it
with RSA key recovery and app ﬁngerprinting.
II. BACKGROUND AND RELATED WORKS
In this work, we investigate the security of the cache archi-
tecture of Intel Xeon Scalable Processors (SP) [9], which have
gained a prominent market share in cloud computing [13]. We
2
ﬁrst overview their cache design. Then, we introduce the prior
cache side-channel attacks. Finally, we overview the research
of stateless channels that serve as our attack primitives.
A. Architecture of Intel Xeon SP
Fig. 1: Comparison between ring-bus and mesh structures. Not
all cores/components are drawn to save space.
Mesh Interconnect. When a CPU chip contains multiple
cores, the connection topology among them, or on-chip in-
terconnect, is a key factor determining the CPU performance.
The old design of interconnect (e.g., in Intel Xeon E5 and Intel
Core) mimics the multiprocessor architecture, in that a shared
ring-bus connects all cores together [14], as illustrated in
Figure 1 left. However, the core-to-core latency could increase
linearly along with the growth of cores within one CPU die,
because the communication between two cores could be routed
through all other cores.
Since Xeon Skylake-SP server CPU family [15] (released
in 2017), Intel revamped the interconnect design with mesh,
which is also adopted by the latest generation of Intel server
CPUs, e.g., Xeon Icelake-SP, and expected to be the default
design in the near future [16]. Besides Intel CPUs, mesh
interconnect has also been adopted by other processors, like
Tile Processors [10], [17], [11], and ARM server CPUs [18]. In
essence, the chip is structured as a 2D matrix of tiles [19] and
each tile either consists of a core (together with cache), or an
uncore component like IMC (Integrated Memory Controller),
UPI (Ultra Path Interconnect) controller, and I/O unit. Each
tile is connected to its vertical and horizontal neighbors, and
the trafﬁc of each direction (in total 4) is managed through a
mesh stop inside the tile. Figure 1 right illustrates the mesh
structure. Mesh interconnect caps the core-to-core latency at a
much lower rate, because the number of hops between any pair
of tiles is only proportional to the square root of the number
of tiles, which satisﬁes the growth of core density.
UPI. To further increase core density, Intel allows linking
CPUs on different sockets with UPI connection [20], which
wires UPI mesh stops from different CPUs. One CPU can have
up to 3 UPI stops. Cross-socket mesh trafﬁc ﬁrstly reaches the
UPI stop of the originating CPU via mesh, and then will be
Intel Xeon E5 CPUsring-bus structureCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCCore LLCMemory ControllerI/OQPICore\SFCHA\LLCCore\SFCHA\LLCCore\SFCHA\LLCCore\SFCHA\LLCCore\SFCHA\LLCCore\SFCHA\LLCCore\SFCHA\LLCIMCIMCUPII/OI/OUPIXeon Scalable CPUsmesh interconnectforwarded to the UPI stop of the destination CPU. At last, the
UPI stop forwards it to the destination mesh tile.
Cache Hierarchy. Modern processors all feature a hierarchy
of cache to localize the frequently accessed data and code,
in order to reduce the access latency. Each core has its own
private cache, e.g., L1 and L2 cache. And there is also L3
cache, or last-level cache (LLC), which is shared across cores.
Xeon SP breaks LLC into slices and evenly distribute them
among cores. As shown in Figure 1 right, each tile consists of
a core (with L1 and L2 embedded), a LLC slice, a Snoop Filter
(SF) and a Cache Home Agent (CHA). Appendix A describe
the cache conﬁgurations of Intel Skylake-SP.
How a memory address maps to a cache line depends on the
set index of the address. Intel has a proprietary hash algorithm
to do the mapping. We illustrate the structure under Xeon SP
in Figure 9 in Appendix A, summarized from [21], [22].
Cache Coherence. As the same address can be read/write by
different cores, the cache should be kept coherent to avoid the
access of outdated data. To this end, Intel uses the MESIF
(Modiﬁed, Exclusive, Shared, Invalid, Forward) protocol [23]
for cache coherence, managing ﬁve cache states. “Modiﬁed”
means the line is in the private cache of the owing core
and is dirty as it has been written. “Exclusive” indicates the
line is stored in a single core. “Shared” means the line is
potentially shared by multiple cores. “Invalid” means the line
is not in cache. “Forward” suggests the core holding the line
is responsible to forward the line to cores reading the line.
The CHA is responsible for coordinating all the cores and
maintaining the coherence. For example, when a requesting
core requests to write a cache line that is cached by another
core (or caching core) in the Exclusive state, the CHA will ask
the caching core to send the line to the requesting core and turn
the state to Invalid, as a line can only be writable to one core.
After this, the requesting core can turn the state of the line to
Exclusive. In Section V-B, we exploit the messages delivered
under the MESIF protocol as a side-channel for cross-CPU
attacks.
B. Cache Side-channel Attack
Cache side-channel attack bypasses memory isolation and it
is particularly concerning in the cloud setting, where multiple
users share the same physical machine [4], [24], [25]. Below
we overview the existing attack methods, and classify them
by whether they assume memory sharing between victim and
attacker. The overview is not meant to be exhaustive and we
refer interested readers to surveys like [26].
Sharing Memory. Running processes often share identical
memory pages to save memory. Shared memory leads to
shared cache, and FLUSH+RELOAD exploits such a condition
for cache side-channel attack. It takes three steps. First, the
attacker sets all the cache lines mapped to the shared memory
as invalid, by using cache clearance instruction clflush.
Then, the attacker waits a period of time for the victim to
access the shared memory. Finally, the attacker accesses the
shared memory and counts the cycles (e.g., through rdtsc)
to measure the latency, and infer the code/data access pattern
of the victim.
FLUSH+RELOAD has been demonstrated effective on
LLC [1] of a PC and cloud instances [24], resulting in leakage
of encryption keys [25] and keystroke events [27]. It has been
evolved to variations [26] like FLUSH+FLUSH [28], which
is stealthier by avoiding the extra memory access. On the
other hand, this attack can be mitigated when clflush is
banned [29]. To address this limitation, EVICT+RELOAD [30]
was proposed, which uses cache conﬂicts as a replacement for
clflush.
Not Sharing Memory. When memory is not shared, an
attacker can still force cache contention because memory
addresses of different programs can share a cache set.
PRIME+PROBE exploits such feature, and it also takes three
steps. First, the attacker collects a set of cache lines that can ﬁll
a cache set and access the related memory addresses. Next, the
attacker waits for the victim to evict the cache lines. Finally,
the attacker measures the access latency.
Though PRIME+PROBE initially targets L1 cache [31],
LLC that is inclusive has also been attacked [25], [4]. A
number of variations have been developed [26]. For in-
stance, PRIME+ABORT [2] measures the Intel TSX (Trans-
actional Synchronization Extensions) abort rather than access
latency. Instead of letting the victim evict the cache lines,
EVICT+TIME lets the attacker evict a cache set, and then
invokes the victim operation [32], [33].
Indirect Attacks. Recently, researchers started to investigate
the interplay between other CPU units and cache, to make
the attack more evasive. For instance, XLATE [3] and TL-
BLEED [34] exploited MMU (Memory Management Units)
and TLB (Translation Lookaside Buffers) to leak victim’s
cache activity. The recent Intel Xeon SP started to use non-
inclusive LLC, which raised the bar for LLC cache attacks.
Yet, Yan et al. [22] showed that by targeting cache directories
(or Snoop Filter), the units tracking which core contains a
copy of a cache line, attacking non-inclusive LLC is feasible.
One major assumption of the prior attacks is that the at-
tacker’s code is on the same machine as the victim’s. Recently,
attacks over network connections were studied. By exploiting
RDMA (Remote Direct Memory Access) and DDIO (Data
Direct I/O), a remote attacker can access LLC [35] of CPU
and cache inside NIC [36], launching side-channel attacks.
On an orthogonal direction, transient execution attacks [37]
like SPECTRE [38], MELTDOWN [39] and FORESHADOW [40]
modulate the state of the cache to construct covert chan-
nels, and exﬁltrate information from speculatively executed
instructions. MESHUP focuses on side channels and we will
investigate whether MESHUP can be leveraged by transient
execution attacks in the future.
C. Stateless Channels
According to Ge et al. [5], microarchitectural side-channels
exploit
the competition of hardware resources, which can
be classiﬁed into two categories: microarchitectural state
and stateless
interconnects. The ﬁrst category includes