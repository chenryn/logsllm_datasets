arate Emulab host and generates requests. A request proceeds as
follows. The client ﬁrst makes the actual request to the server. If
the server is busy, the thinner replies and makes the client issue two
HTTP requests: the original request and the payment bytes. Each
client’s requests are driven by a Poisson process of rate λ requests/s.
However, a client never allows more than a conﬁgurable number
w (the window) of outstanding requests. If the stochastic process
“ﬁres” when more than w requests are outstanding, the client puts
the new request in a backlog queue, which drains when the client
receives a response to an earlier request. If a request is in this queue
for more than 10 seconds, it times out, and the client logs a service
denial. All requests are identical, and the server itself is emulated,
processing a request on average every 1/c seconds (see §6).
We use the behavior just described to model both good and bad
clients. A bad client, by deﬁnition, tries to capture more than its
fair share. We model this intent as follows: in our experiments, bad
clients send requests faster than good clients, and bad clients send
requests concurrently. Speciﬁcally, we choose λ = 40, w = 20 for
bad clients and λ = 2, w = 1 for good clients. (The w value for bad
clients is pessimistic; see §7.4.)
Our choices of B and G are determined by the number of clients
that we are able to run in the testbed and by a rough model of to-
day’s client access links. Speciﬁcally, in most of our experiments,
there are 50 clients, each with 2 Mbits/s of access bandwidth. Thus,
B + G usually equals 100 Mbits/s. This scale is smaller than most
attacks. Nevertheless, we believe that the results generalize because
we focus on how the prototype’s behavior differs from the theory
in §3. By understanding this difference, one can can make predic-
tions about speak-up’s performance in larger attacks.
Because the experimental scale does not tax the thinner, we sep-
arately measured its capacity and found that it can handle loads
comparable to recent attacks. At 90% CPU utilization on the hard-
ware described above with multiple gigabit Ethernet interfaces, in a
600-second experiment with a time series of 5-second intervals, the
thinner sinks payment bytes at 1451 Mbits/s (with standard devia-
tion of 38 Mbits/s) for 1500-byte packets and at 379 Mbits/s (with
standard deviation of 24 Mbits/s) for 120-byte packets. Many re-
s
t
n
e
i
l
c
d
o
o
g
o
t
d
e
t
a
c
o
l
l
a
r
e
v
r
e
s
f
o
n
o
i
t
c
a
r
F
 1
 0.8
 0.6
 0.4
 0.2
 0
With speak-up
Without speak-up
Ideal
 0.1
 0.3
 0.5
 0.7
 0.9
Good clients’ fraction of total client bandwidth
)
s
d
n
o
c
e
s
(
e
m
T
i
t
n
e
m
y
a
P
 1
 0.8
 0.6
 0.4
 0.2
 0
Mean
90th percentile
50
100
200
Capacity of the server (requests/sec)
Figure 2: Server allocation when c = 100 requests/s as a function
of G
G+B . The measured results for speak-up are close to the ideal line.
Without speak-up, bad clients sending at λ = 40 requests/s and w = 20
capture much more of the server.
Figure 4: Mean time to upload dummy bytes for good requests that
receive service. c varies, and G = B = 50 Mbits/s. When the server is
not overloaded (c = 200), speak-up introduces little latency.
Server allocation to good clients
Server allocation to bad clients
Fraction of good requests served
n
o
i
t
c
a
r
F
 1
 0.8
 0.6
 0.4
 0.2
 0
50,OFF 50,ON
100,OFF 100,ON
200,OFF 200,ON
Capacity of the server (requests/sec)
)
s
e
t
y
B
K
(
t
n
e
m
y
a
p
g
v
A
 250
 200
 150
 100
 50
 0
Upper Bound
Good
Bad
50
100
200
Capacity of the server (requests/sec)
Figure 3: Server allocation to good and bad clients, and the fraction of
good requests that are served, without (“OFF”) and with (“ON”) speak-
up. c varies, and G = B = 50 Mbits/s. For c = 50, 100, the allocation is
roughly proportional to the aggregate bandwidths, and for c = 200, all
good requests are served.
cent attacks are roughly this size; see §2.1 and §4.3. The capacity
also depends on how many concurrent clients the thinner supports;
the limit here is only the RAM for each connection (see §6).
7.2 Validating the Thinner’s Allocation
When the rate of incoming requests exceeds the server’s capacity,
speak-up’s goal is to allocate the server’s resources to a group of
clients in proportion to their aggregate bandwidth. In this section,
we evaluate to what degree our implementation meets this goal.
In our ﬁrst experiment, 50 clients connect to the thinner over a
100 Mbits/s LAN. Each client has 2 Mbits/s of bandwidth. We vary
f , the fraction of “good” clients (the rest are “bad”). In this homo-
geneous setting, G
G+B (i.e., the fraction of “good client bandwidth”)
equals f , and the server’s capacity is c = 100 requests/s.
Figure 2 shows the fraction of the server allocated to the good
clients as a function of f . Without speak-up, the bad clients capture
a larger fraction of the server than the good clients because they
make more requests and the server, when overloaded, randomly
drops requests. With speak-up, however, the good clients can “pay”
more for each of their requests—because they make fewer—and
can thus capture a fraction of the server roughly in proportion to
their bandwidth. The small difference between the measured and
ideal values is a result of the good clients not using as much of
their bandwidth as the bad clients. We discussed this adversarial
advantage in §3.4 and further quantify it in §7.3 and §7.4.
Figure 5: Average number of bytes sent on the payment channel—the
“price”—for served requests. c varies, and G = B = 50 Mbits/s. When
the server is overloaded (c = 50, 100), the price is close to the upper
bound, (G + B)/c; see the text for why they are not equal. When the
server is not overloaded (c = 200), good clients pay almost nothing.
In the next experiment, we investigate different “provisioning
regimes”. We ﬁx G and B, and measure the server’s allocation when
its capacity, c, is less than, equal to, and greater than cid. Recall
from §3.1 that cid is the minimum value of c at which all good
clients get service, if speak-up is deployed and if speak-up allo-
cates the server exactly in proportion to client bandwidth. We set
G = B by conﬁguring 50 clients, 25 good and 25 bad, each with a
bandwidth of 2 Mbits/s to the thinner over a LAN. In this scenario,
cid = 100 requests/s (from §3.1, cid = g(1 + B
100), and we experiment with c = 50, 100, 200 requests/s.
G ) = 2g = 2 · 25 · λ =
Figure 3 shows the results. The good clients get a larger frac-
tion of the server with speak-up than without. Moreover, for c =
50, 100, the allocation under speak-up is roughly proportional to
the aggregate bandwidths, and for c = 200, all good requests are
served. Again, one can see that the allocation under speak-up does
not exactly match the ideal: from Figure 3, when speak-up is en-
abled and c = cid = 100, the good demand is not fully satisﬁed.
7.3 Latency and Byte Cost
We now explore the byte and latency cost of speak-up for the same
set of experiments (c varies, 50 clients, G = B = 50 Mbits/s).
For the latency cost, we measure the length of time that clients
spend uploading dummy bytes, which captures the extra latency
that speak-up introduces. Figure 4 shows the averages and 90th per-
centiles of these measurements for the served good requests.
d
e
t
a
c
o
l
l
a
r
e
v
r
e
s
f
o
n
o
i
t
c
a
r
F
 0.4
 0.3
 0.2
 0.1
 0
0.0
Observed
Ideal
0.5
1.5
1.0