2k+1
N
l
l=m
r=0
Fig. 5: Inﬂuence of randomized cache parameters, for RAND (left) and LRU (right). To isolate the inﬂuence of the instance, proﬁling strategies are ﬁxed to
burst accesses and k = N
4 for LRU. Instances are indicated as (nw, b, P ), and positioned for mean proﬁling effort
(y-axis, log scale), and eviction set size for exploitation (x-axis, log scale). Vertical lines span the 5-95th percentiles (ranges indicated) over 103 simulated runs.
2 for RAND, and bootstrapping and k = 3N
Conversely, Table IV captures rekeying periods T that upper
bound the fraction of successful rekeying periods. Pessimisti-
cally assuming that every memory access is a cache hit, it
gives an expected continuous proﬁling time of having one
successful construction of G within the rekeying period. As
the obtained G is only useful for one period, Table IV also
includes the case where half of it is used for exploitation. Note
that these minimal efforts strongly depend on m, and hence
on the quality of G that can be tolerated for exploitation ( C ).
VII. LIFTING IDEALIZING ASSUMPTIONS
In this section, we explore for the ﬁrst time the more
challenging attack Anonid with complicating system activity
(cf. Section IV), as opposed to the commonly assumed Aideal.
We start with a victim program performing more memory ac-
cesses than of interest to the attacker and present an end-to-end
attack on a vulnerable AES implementation. We then quantify
the inﬂuence of random noise on PRIME+PRUNE+PROBE.
The central assumption is that the attacker wants to proﬁle
speciﬁc addresses of the victim and that the access probability
of said addresses can be changed via inputs to the victim.
A. Multiple Victim Accesses
In the proﬁling phase, the attacker identiﬁes addresses of
interest in a victim program and distinguishes between them if
there are multiple, requiring disjoint eviction sets for each target.
From this perspective, we model the execution of victim code
as a set of static and dynamic memory accesses. Static accesses
are performed regardless of the attacker’s input, i.e., code and
data accesses performed in all victim executions. Dynamic
accesses do not always occur, e.g., state- or input-dependent
code or data accesses.
The attack targets are one or more addresses that are accessed
upon a certain event the attacker wants to spy on [12]. Like
Gruss et al. [12], we cannot distinguish addresses in the static
set, as the cause-effect relationship is the same for all of them.
Hence, for our attack, all targets are in the dynamic set.
To proﬁle the cache addresses of interest, we propose a
two-phase approach. First, we collect a superset of addresses
containing colliding addresses for all static and dynamic cache
lines. Second, we obtain disjoint sets of addresses from the
superset, each with colliding addresses for one target.
The attacker distinguishes static and dynamic accesses by the
property that dynamic accesses are statistically performed less
often than static accesses, which are always performed. With
the assumption from the beginning of this section, we consider
a scenario where an attacker controls, e.g., via input, which
dynamic accesses the victim performs in any given execution.
This control can be exerted positively (i.e., a dynamic access is
always performed for a speciﬁc input) as well as negatively (i.e.,
a dynamic access is never performed for a speciﬁc input). The
latter scenario repeatedly calls the victim with inputs that cause
it to access all but one address. Thus, it can be separated from
the superset, as all other addresses in it are accessed eventually.
In general, any manipulation of access probabilities in the
victim can be observed. This approach describes a stronger
attacker, as targeted addresses can be distinguished from others
in both the dynamic and static set in the same step.
1) Implementation: In the following, we focus on maximum
partitioning P = nw, as non-random replacement policies like
LRU generally require special treatment but behave predictably.
We employ catching with intermediate full eviction. The
analysis of Section V-B1 applies. To generate distinct and
large eviction sets for our ntarget target addresses, we slightly
modify the three-step approach described in Section VI-B. All
experiments are obtained in simulation (cf. Section III-A3).
the
described
superset
previously
To ﬁnd sets of addresses ai,
in Step 1, we ﬁrst
construct
using
PRIME+PRUNE+PROBE (Section VI-B). Instead of only one
victim memory access, all nstat static and ndyn dynamic
victim accesses are now observed by the attacker. To identify
a enough colliding addresses for all targets, we construct a
superset of at least nw·(nstat+ndyn) addresses. The expected
amount of memory required to ﬁnd a collision in a speciﬁc
victim cache lines
1
5
10
20
2 · 107
1 · 107
s
e
s
s
i
M
e
h
c
a
C
0
0
0.1
0.2
0.3
Noise Level ν
0.4
0.5
Fig. 6: Cache misses for creating a superset with 3 · nw addresses per victim
cache line, as a function of noise ν for different numbers of total victim cache
lines, k = 1000 (avg. over 100 runs). Instance is RAND(8, 9, 8)
nw
nw
way is cachesize
, though higher conﬁdence requires more. We
can apply the coupon collector’s problem (cf. Section VI-B)
for an estimated factor of coupon(nw)
, but as more addresses
need to be proﬁled, the probability to catch enough addresses
for all targets decreases. Consequently, this step requires a
number of repetitions, depending on the prime parameter k.
Next, we separate unwanted addresses from target addresses
within the superset. To this end, we call the victim with
inputs that exclude exactly one of the ntarget cache lines.
By repeatedly evicting the cache, calling the victim with the
required parameters, and measuring accesses in the superset,
we generate a histogram for all target addresses. After a certain
number of repetitions, addresses that are never evicted by the
victim are very likely to collide with the targeted address.
Repeating this process ntarget times, we get disjoint sets of
addresses for each target cache line. Step 2 and Step 3
can be applied to these sets of addresses (ai) to construct the
ﬁnal generalized eviction sets like in the single-access case.
From our experiments (cf. Figure 6), we estimate that the
number of cache misses (the largest factor of the execution
time) increases sub-linearly with the total amount of accesses
by the victim (nstat + ndyn). This is because the catching
probability pc increases with ntarget. The superset’s separation
depends linearly on ntarget and the overall size of the superset.
2) End-to-end Attack on AES T-Tables: We choose the 10
round T-tables implementation of AES in OpenSSL 1.1.0g as
an example, as it is a well-known target for cache attacks [3],
[34], [44], [12]. We perform the One-Round Attack, described
by Osvik et al.
[34], and thus recover 64 bits in the 16 upper
nibbles of the 16-byte key (see Appendix D).
The parameters for this attack are nstat = 27 and ndyn = 65.
With ntarget = 64, the 4 T-tables are a difﬁcult attack target,
as the proﬁling time scales linearly with ntarget.
For proﬁling, we require AES runs that access all but
the target address, for each target. We can prepare 64 such
key/plaintext pairs ofﬂine. All AES runs are recorded as
memory access traces with the Intel PIN Tool [15] and injected
into the simulator (cf. Section III-A3) at the appropriate
times. Lacking more efﬁcient eviction methods, we rely on
probabilistic full cache eviction. In total, eviction accounts for
≈ 90% of all accesses during the attack, which in turn makes
the superset-splitting step of the proﬁling the largest contributor
to the overall runtime. Because we assume no restriction on
the number of encryptions, we do not perform Step 2 for
this attack, as pruning the generated candidate pools would
TABLE V: End-to-end attack on T-table AES for different conﬁgurations
(means over 100 runs). nslices = 8, b = 11. Where not shown, standard
deviations are < 0.5% of the mean.
policy misses [109] hits [109]
n/a
nw P
8
8
2 RAND
8
LRU
8
2
16 16
n/a
16
16
2 RAND
2
LRU
12.03
2.78
3.21
46.27
4.69
7.85
3.59
2.25
1.75
9.25
3.91
2.63
#AES ∅ collisions/addr. correct nibbles est. t [min]
56663
23682
26060
157072
39192
66640
20.47 ± 3.61
15.52 ± 3.37
17.74 ± 7.53
37.91 ± 5.65
26.62 ± 5.85
26.99 ± 11.66
15.90 ± 0.33
16.00 ± 0.00
15.94 ± 0.28
15.77 ± 0.45
15.93 ± 0.26
15.75 ± 0.51
1.58
0.63
0.78
6.89
1.32
2.60
also require the costly splitting phase. Instead, we see that
using fewer colliding addresses for each target (cf. Table V)
still performs well. We can compensate for the lower detection
probability by increasing the number of encryptions during the
exploitation phase.
We use cache parameters from modern Intel processors: 8
slices (with a slicing function [28]) of 1 MB each, so each
slice is a randomized cache with nw = 8/16 and b = 11. We
run the same attack for P = nw = 8/16 and P = 2, with
replacement policies random and LRU. As seen in Table V,
the attack is generic enough for all conﬁgurations, without
special considerations for LRU. The variance in the number
of addresses found per target increases for P = 2, especially
for LRU, but since this speciﬁc attack sums over the hits on
different addresses, this effect is mitigated for the end result (see
Appendix D). For P = 2, we speed up the attack by reducing
the cache lines used for full cache eviction from 2N to 1.5N,
as well as reducing the superset size (cf. Section VI-D1).
This end-to-end implementation is not optimal, as there are
many parameters that could be optimized. Nonetheless, we can
see that cache attacks can still be executed in a reasonable
time frame. If we model the attack as a mixture of sequential
accesses for full cache evictions and timed random accesses
for the sets, we can calculate the average attack times shown
in Table V. For this rough estimate, we use access times
measured on a real system with the same miss rates (i7-8700K
@ 3.60GHz, sequential access:≈ 11.4c, timed (rdtsc) random
miss: ≈ 235c, hit: ≈ 222c).
Takeaway: Unpredictability requires key agility.
Frequent rekeying is essential to maintain the beneﬁts of
randomization, even in non-ideal conditions.
B. Inﬂuence of Noise
In the ideal case (Aideal), there is no noise from memory
accesses by the attacker process itself, nor the victim, or any
other process in the system (including the operating system).
Section VII-A already implicitly includes noise generated by
the victim’s code execution. We now additionally consider noise
introduced by other system activity. We make the simplifying
assumption that noise accesses are random and occur at a rate
of ν random accesses for every attacker access.
Multiple steps of the (unmodiﬁed) proﬁling algorithm from
Section VI-B are affected by noise. Spurious memory accesses
during the prune step increase the number of pruning
iterations mpr signiﬁcantly and reduce the size k(cid:48) of the
2,000
’
k
1,000
k(cid:48), k =
100
1000
nw · 2b−1
mpr, k =
100
1000
nw · 2b−1
0
0
0.1
0.2
0.3
Noise Level ν
0.4
0.5
100
r
p
m
50
0
Fig. 7: Pruning mpr and k(cid:48) as a function of noise ν, for various k (average
over 100 runs). Instance is RAND(8, 9, 8)
80
60
40
]
%
[
m
i
t
c
i
V
s
n
o
i
s
i
l
l
o
C
k
100
1000
nw · 2b−1
0
0.1
0.2
0.3
Noise Level ν
0.4
0.5
Fig. 8: Percentage of caught addresses in the superset that genuinely collide