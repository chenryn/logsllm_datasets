title:Operating a Network Link at 100%
author:Changhyun Lee and
D. K. Lee and
Yung Yi and
Sue B. Moon
Operating a Network Link at 100%
Changhyun Lee1, DK Lee1, Yung Yi2, and Sue Moon1
1 Department of Computer Science, KAIST, South Korea
2 Department of Electrical Engineering, KAIST, South Korea
Abstract. Internet speed at the edge is increasing fast with the spread of ﬁber-
based broadband technology. The appearance of bandwidth-consuming applica-
tions, such as peer-to-peer ﬁle sharing and video streaming, has made trafﬁc
growth a serious concern like never before. Network operators fear congestion
at their links and try to keep them underutilized while no concrete report exists
about performance degradation at highly utilized links until today. In this paper,
we reveal the degree of performance degradation at a 100% utilized link using
the packet-level traces collected at our campus network link. The link has been
fully utilized during the peak hours for more than three years. We have found
that per-ﬂow loss rate at our border router is surprisingly low, but 30 ∼ 50 msec
delay is added. The increase in delay results in overall RTT increase and degrades
user satisfaction for domestic web ﬂows. Comparison of two busy traces shows
that the same 100% utilization can result in different amount of performance loss
according to the trafﬁc conditions. This paper stands as a good reference to the
network administrators facing future congestion in their networks.
1 Introduction
Video-driven emerging services, such as YouTube, IPTV, and other streaming media,
are driving trafﬁc growth in the Internet today. Explosive market expansion of smart
phones is also adding much strain not only on the cellular network infrastructure but
increasingly on the IP backbone networks. Such growth represents insatiable demand
for bandwidth and some forecast IP trafﬁc to grow four-fold from 2009 to 2014 [1].
Network service providers provision their networks and plan for future capacity based
on such forecasts, but they cannot always succeed in avoiding occasional hot spots in
their networks. However, trafﬁc patterns in a network are usually conﬁdential and few
reports on hot spots are available to general public. Beheshti et al. report that one of
the links in Level 3 Communications’ operational backbone network was once utilized
up to 96% [5]. A trans-Paciﬁc link in Japan was fully utilized until 20061. Choi et al.
have reported on a link on the Sprint backbone operating above 80% and likely causing
a few moments of congestion [8].
Korea Advanced Institute of Science and Technology connects its internal network
to the Internet via multiple 1 Gbps links. One of them is to SK Broadband, one of the
top three ISPs in Korea, and its link is the most utilized of all. The link to SK Broad-
band has experienced persistent congestion in the past few years. The measurement on
1 The packet traces at Samplepoint-B from 2003/04 to 2004/10 and from 2005/09 to 2006/06 in
the MAWI working group trafﬁc archive at http://mawi.wide.ad.jp/mawi show full utilization.
N. Spring and G. Riley (Eds.): PAM 2011, LNCS 6579, pp. 1–10, 2011.
c(cid:2) Springer-Verlag Berlin Heidelberg 2011
2
C. Lee et al.
our campus network tells us that the link has experienced 100% utilization during the
peak hours for more than three years! To the best of our knowledge, our work is the
ﬁrst to investigate a 100% utilized link. Even at 100% utilization the link has no rate
limiting or ﬁltering turned on. However, the operational cost of a 1 Gbps dedicated link
is typically in the order of thousands of US dollars a month and a capacity upgrade is
not always easy. Also the empirical evidence demonstrates that persistent congestion,
although itself pathological, does not always incur pathological performance–we still
get by daily web chores over the congested link!
In this paper we report on the persistent congestion in our network and analyze its
impact on end-to-end performance. The questions we raise are: (i) how much perfor-
mance degradation does the fully-utilized link bring?; (ii) how badly does it affect the
end-to-end performance?; and (iii) how tolerable is the degraded performance? Based
on the passive measurements on our campus network link we present quantitative an-
swers to the above three questions. Per-ﬂow loss rate at our border router is surprisingly
low, mostly under 0.1% even at 100% utilization, but 30 ∼ 50 ms delay is added. The
increase in delay results in overall RTT increase and degrades user satisfaction for do-
mestic web ﬂows. Flows destined to countries outside China, Japan, and Korea suffer
less for both web surﬁng and bulk ﬁle transfer, but they account for less than 5% of total
trafﬁc. Comparison of two busy traces shows that the same 100% utilization can result
in different performance degradation according to the trafﬁc conditions.
The remainder of this paper is structured as follows. Section 2 describes the mea-
surement setup and Section 3 the trafﬁc mix. In Section 4 we quantify the performance
degradation in terms of loss and delay. In Section 5 we study the impact of increased
delay and loss on the throughtput of web ﬂows and bulk transfers. We present related
work in Section 6 and conclude with future work in Section 7.
2 When and Where Do We See 100% Utilization?
Our campus network is connected to SK Broadband ISP with a 1 Gbps link, over which
most daily trafﬁc passes through to reach hosts outside KAIST. Figure 1 illustrates
the campus network topology and the two packet capturing points, Core and Border.
We have installed four Endace GIGEMONs equipped with DAG 4.3GE network mon-
itoring cards [2] to capture packet-level traces to and from our campus network; each
GIGEMON’s clock is synchronized to the GPS signal.
Fig. 1. Network topology on campus
Operating a Network Link at 100%
3
The main observation, key to this work, is that the outgoing 1 Gbps link between the
campus and the commercial ISP has been fully utilized during the peak hours for more
than three years. The link utilization plotted by Multi Router Trafﬁc Grapher (MRTG)
on one day of July from 2007 to 2010 are in Figure 2. The solid lines and the colored
region represent the utilizations of the uplink and the downlink, respectively. We see
that the uplink lines stay at 100% most of the time. To the best of our knowledge, such
long-lasting persistent congestion has never been reported in the literature.
(a) 2007
(b) 2008
(c) 2009
(d) 2010
Fig. 2. Link utilization of one day in July from 2007 to 2010; solid line is for uplink and colored
region is for downlink. The time on x-axis is local time.
We have collected packet headers for one hour during the 100% utilized period from
2pm on March 24th (trace-full1) and September 8th in 2010 (trace-full2). We have also
collected a one-hour long packet trace from 6am on August 31st in 2010 (trace-dawn)
for comparison. As we see in Figure 2 the link utilization drops from 100% to around
60% during the few hours in the early morning. Trace-dawn has 65.6% of utilization
and the number of ﬂows is only half of those from full utilization. We summarize the
trace-related details in Table 1.
Table 1. Details of packet traces
Trace name
trace-full1
trace-full2
trace-dawn
Time of collection
2010/03/24 14:00
2010/09/08 14:00
2010/08/31 06:00
Duration
1 hour
1 hour
1 hour
Utilization
100.0%
100.0%
65.6%
# of ﬂows
9,387,474
9,687,043
4,391,860
The capturing point Core has generated two traces for each direction, and the point
Border does the same; we have four packet traces in total for each collection period.
In the following sections, we use different pairs of the four traces to analyze different
performance metrics. For example, we exploit uplink traces from Core and Border to
calculate the single-hop queueing delay and the single-hop packet loss rate. The uplink
and downlink traces from Core are used to calculate ﬂows’ round trip times (RTTs).
4
C. Lee et al.
We monitored only one out of three core routers on campus, and thus only a part of the
packets collected at Border are from Core. We note that, although incomplete, about
30% of trafﬁc at Border comes from Core, and this is a signiﬁcantly high sampling rate
sufﬁcient to represent the overall performance at Border.
3 Trafﬁc Mix
We ﬁrst examine the trafﬁc composition by the protocol in the collected traces. As
shown in Figures 3(a) and (b), TCP trafﬁc dominates when the 1 Gbps link is busy. The
average percentages of TCP and UDP in trace-full1 are 83.9% and 15.7%, respectively.
The portion of UDP increases to 27.7% in trace-full2 and 33.7% in trace-dawn. Al-
though TCP is still larger in volume than UDP, the percentage of UDP is much larger
than 2.0 ∼ 8.5% reported by previous work [7] [11]. We leave the detailed breakdown
of UDP trafﬁc as our future work. The dominance of TCP trafﬁc indicates that most
ﬂows are responsive to congestion occurring in their paths.
(a) trace-full1
(b) trace-full2
(c) trace-dawn
Fig. 3. Protocol breakdown of the collected traces
In order to examine user-level performance later, we now group TCP packets into
ﬂows. Figure 4(a) shows the cumulative volume of ﬂows. Flows larger than 100 KBytes
take up 95.3% of the total volume in trace-full1, 95.8% in trace-full2 and 97.2% in
trace-dawn. We call those ﬂows elephant ﬂows and those smaller than 100 KBytes mice
ﬂows. In Figure 4(b) we plot the total volume in trace-full1 contributed by elephant and
mice ﬂows in one second intervals and conﬁrm that mice ﬂows are evenly distributed
over time. The other two traces exhibit the same pattern and we omit the graphs from
them.
(a) Volume CDF versus ﬂow size
(b) Timeseries of elephant and mice ﬂows
Fig. 4. Trafﬁc volume by the ﬂow size
Operating a Network Link at 100%
5
4 Impact of Congestion on Packet Loss and Delay
In this section we explore the degree of degradation in single-hop and end-to-end perfor-
mance brought on during the full utilization hours in comparison to the low utilization
period. We begin with the analysis on loss and delay. In Section 3 we have observed
that TCP ﬂows, more speciﬁcally those larger than 100 KBytes, consume most of band-
width. We thus focus on the delay and loss of elephant TCP ﬂows in the remainder of
this paper.
4.1 Packet Loss
We examine the single-hop loss rate of the elephant TCP ﬂows at our congested link.
From the ﬂows appearing both at Core and Border, we pick elephant TCP ﬂows with
SYN and FIN packets within the collection period. Existence of SYN packets improves
the accuracy of RTT estimation, as we use the three-way handshake for our RTT es-
timation. For those ﬂows we use IP and TCP headers of each packet collected at the
capturing points Core and Border and detect loss, if any, through the border router as
in [13].
(a) Single-hop loss rate
(b) Estimated global loss rate
Fig. 5. Single-hop loss rate and estimated global loss rate (volume-weighted CDF)
Figure 5(a) shows the cumulative distribution of loss rates weighted by the ﬂow’s
size: the cumulative distribution function on the y-axis represents the proportion in the
total trafﬁc volume as in Figure 4. Throughout this paper, we use this weighted CDF
for most of the analysis so that we can capture the performance of elephant ﬂows.
Because no loss is observed in trace-dawn, we do not show the loss rates in
Figure 5(a). The maximum loss rate of ﬂows reaches 5.77% for trace-full1 and 5.71%
for trace-full2. Flows taking up 53.1% of the total TCP trafﬁc have experienced no
loss during the collection period in trace-full1, whereas a much lower ratio of 13.8%
in trace-full2. The performance degradation even at the same 100% utilization varies.
Trace-full1 and trace-full2 differ mostly in the region between no loss and 1% loss.
In the former 3.6% of trafﬁc has loss rate greater than 0.1%, while in the latter the
percentage rises to 39.5%. Apparently ﬂows in trace-full2 suffer higher loss. Here the
utilization level alone is the sole indicator of performance degradation. In the future, we