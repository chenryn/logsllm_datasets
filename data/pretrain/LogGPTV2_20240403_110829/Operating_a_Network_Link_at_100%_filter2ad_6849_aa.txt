# Title: Operating a Network Link at 100% Utilization

**Authors:**
- Changhyun Lee
- DK Lee
- Yung Yi
- Sue Moon

**Affiliations:**
- Changhyun Lee, DK Lee, and Sue Moon: Department of Computer Science, KAIST, South Korea
- Yung Yi: Department of Electrical Engineering, KAIST, South Korea

## Abstract

The rapid increase in internet speed at the edge, driven by the proliferation of fiber-based broadband technology, has led to significant traffic growth. Bandwidth-intensive applications such as peer-to-peer file sharing and video streaming have exacerbated this issue, making traffic growth a serious concern for network operators. These operators often try to keep their links underutilized to avoid congestion, but there is limited data on the performance degradation at highly utilized links. In this paper, we analyze the performance degradation at a 100% utilized link using packet-level traces collected from our campus network. Our results show that the per-flow loss rate at our border router is surprisingly low, but there is an added delay of 30-50 milliseconds. This increased delay degrades user satisfaction, particularly for domestic web flows. We also find that the same 100% utilization can result in different levels of performance degradation based on traffic conditions. This study provides valuable insights for network administrators facing potential future congestion.

## 1. Introduction

Emerging services such as YouTube, IPTV, and other streaming media are driving significant traffic growth in the internet. The explosive market expansion of smartphones is also placing a strain on both cellular and IP backbone networks. Some forecasts predict that IP traffic will grow four-fold from 2009 to 2014 [1]. Network service providers plan their capacity based on these forecasts, but they cannot always avoid occasional hot spots in their networks. However, detailed reports on such hot spots are rare. For instance, Beheshti et al. reported that one of the links in Level 3 Communications' operational backbone network was utilized up to 96% [5], and a trans-Pacific link in Japan was fully utilized until 2006 [1].

Korea Advanced Institute of Science and Technology (KAIST) connects its internal network to the internet via multiple 1 Gbps links, with one of the most utilized being the link to SK Broadband, one of the top three ISPs in Korea. This link has experienced persistent congestion over the past few years. Our measurements indicate that the link has been 100% utilized during peak hours for more than three years. To the best of our knowledge, this is the first study to investigate a 100% utilized link without rate limiting or filtering. Despite the high operational cost, the empirical evidence suggests that persistent congestion does not always lead to severe performance degradation.

In this paper, we address the following questions:
1. How much performance degradation does a fully-utilized link cause?
2. How does it affect end-to-end performance?
3. How tolerable is the degraded performance?

Based on passive measurements, we provide quantitative answers to these questions. We find that the per-flow loss rate at our border router is surprisingly low, mostly under 0.1%, even at 100% utilization. However, an additional delay of 30-50 ms is observed, which increases overall RTT and degrades user satisfaction for domestic web flows. Flows destined for countries outside China, Japan, and Korea suffer less, but they account for less than 5% of total traffic. Comparing two busy traces, we find that the same 100% utilization can result in different levels of performance degradation based on traffic conditions.

## 2. Measurement Setup

Our campus network is connected to SK Broadband ISP via a 1 Gbps link, which handles most of the daily traffic to and from hosts outside KAIST. Figure 1 illustrates the campus network topology and the two packet capturing points: Core and Border. We have installed four Endace GIGEMONs equipped with DAG 4.3GE network monitoring cards to capture packet-level traces. Each GIGEMON's clock is synchronized to the GPS signal.

**Figure 1.** Network topology on campus

The key observation is that the outgoing 1 Gbps link between the campus and the commercial ISP has been fully utilized during peak hours for more than three years. Figure 2 shows the link utilization plotted by Multi Router Traffic Grapher (MRTG) for one day in July from 2007 to 2010. The solid lines represent the uplink utilization, while the colored regions represent the downlink. We see that the uplink lines stay at 100% most of the time. Such long-lasting persistent congestion has not been previously reported in the literature.

**Figure 2.** Link utilization of one day in July from 2007 to 2010; solid line is for uplink and colored region is for downlink. The x-axis represents local time.

We collected packet headers for one hour during the 100% utilized period on March 24th (trace-full1) and September 8th, 2010 (trace-full2). We also collected a one-hour trace from 6 AM on August 31st, 2010 (trace-dawn) for comparison. As seen in Figure 2, the link utilization drops to around 60% during the early morning. Trace-dawn has a 65.6% utilization, and the number of flows is half that of full utilization. Table 1 summarizes the trace-related details.

**Table 1.** Details of packet traces

| Trace Name | Time of Collection | Duration | Utilization | Number of Flows |
|------------|--------------------|----------|-------------|-----------------|
| trace-full1 | 2010/03/24 14:00   | 1 hour   | 100.0%      | 9,387,474       |
| trace-full2 | 2010/09/08 14:00   | 1 hour   | 100.0%      | 9,687,043       |
| trace-dawn | 2010/08/31 06:00   | 1 hour   | 65.6%       | 4,391,860       |

The capturing point Core generated two traces for each direction, and the point Border did the same, resulting in four packet traces in total for each collection period. We use different pairs of these traces to analyze various performance metrics. For example, we use uplink traces from Core and Border to calculate single-hop queuing delay and packet loss rate. Uplink and downlink traces from Core are used to calculate round trip times (RTTs).

We monitored only one out of three core routers on campus, so only a part of the packets collected at Border are from Core. However, about 30% of the traffic at Border comes from Core, which is a sufficiently high sampling rate to represent overall performance.

## 3. Traffic Mix

We first examine the traffic composition by protocol in the collected traces. Figures 3(a) and (b) show that TCP traffic dominates when the 1 Gbps link is busy. The average percentages of TCP and UDP in trace-full1 are 83.9% and 15.7%, respectively. The portion of UDP increases to 27.7% in trace-full2 and 33.7% in trace-dawn. Although TCP is still larger in volume than UDP, the percentage of UDP is significantly higher than the 2.0-8.5% reported in previous studies [7] [11]. We leave the detailed breakdown of UDP traffic for future work. The dominance of TCP traffic indicates that most flows are responsive to congestion in their paths.

**Figure 3.** Protocol breakdown of the collected traces

To examine user-level performance, we group TCP packets into flows. Figure 4(a) shows the cumulative volume of flows. Flows larger than 100 KBytes take up 95.3% of the total volume in trace-full1, 95.8% in trace-full2, and 97.2% in trace-dawn. We call these "elephant flows" and those smaller than 100 KBytes "mice flows." Figure 4(b) plots the total volume contributed by elephant and mice flows in one-second intervals, confirming that mice flows are evenly distributed over time. The other two traces exhibit the same pattern, and we omit the graphs for brevity.

**Figure 4.** Traffic volume by flow size

## 4. Impact of Congestion on Packet Loss and Delay

In this section, we explore the degree of degradation in single-hop and end-to-end performance during full utilization hours compared to low utilization periods. We begin with the analysis of loss and delay. As observed in Section 3, TCP flows, especially those larger than 100 KBytes, consume most of the bandwidth. Therefore, we focus on the delay and loss of elephant TCP flows in the remainder of this paper.

### 4.1. Packet Loss

We examine the single-hop loss rate of elephant TCP flows at our congested link. From the flows appearing both at Core and Border, we select elephant TCP flows with SYN and FIN packets within the collection period. The presence of SYN packets improves the accuracy of RTT estimation, as we use the three-way handshake for our RTT estimation. For these flows, we use IP and TCP headers of each packet collected at the capturing points Core and Border to detect any loss through the border router, as described in [13].

**Figure 5.** Single-hop loss rate and estimated global loss rate (volume-weighted CDF)

Figure 5(a) shows the cumulative distribution of loss rates weighted by the flow's size. The y-axis represents the proportion in the total traffic volume, similar to Figure 4. Throughout this paper, we use this weighted CDF for most of the analysis to capture the performance of elephant flows. No loss is observed in trace-dawn, so we do not show the loss rates in Figure 5(a). The maximum loss rate of flows reaches 5.77% for trace-full1 and 5.71% for trace-full2. Flows taking up 53.1% of the total TCP traffic experienced no loss during the collection period in trace-full1, whereas a much lower ratio of 13.8% in trace-full2. The performance degradation at the same 100% utilization varies. Trace-full1 and trace-full2 differ mostly in the region between no loss and 1% loss. In trace-full1, 3.6% of traffic has a loss rate greater than 0.1%, while in trace-full2, the percentage rises to 39.5%. Clearly, flows in trace-full2 suffer higher loss. Here, the utilization level alone is not a sufficient indicator of performance degradation. In the future, we aim to explore additional factors that influence performance.

---

This revised version aims to make the text more coherent, clear, and professional, while maintaining the original content and structure.