64% 36%
73% 27%
53% 45%
61% 36%
72% 27%
80% 27%
68% 36%
relatively small area in the central part of the touchscreen,
and the motion patterns caused by different keys are not
so distinguishable. In contrast, even though the alphabetical
keyboard has more keys, they are located in a relatively large
area from the left side to the right side of the touchscreen.
The motion patterns of different keys, especially distant ones,
cause more distinguishable motion patterns.
F.
Impact of Environmental Factors
We also evaluate the impact of a number of environmental
factors on the performance of VISIBLE.
a). Different Light Conditions. Our attack relies on ana-
lyzing the video recordings of the tablet backside during the
victim’s typing process, while the video quality is affected by
light conditions. In general, the low-light condition will lead
to increased video noise, streaking, blurred motion, and poor
focus. We did key inference experiments under light conditions
of 400 lux (normal) and 180 lux (low light). Fig. 14 shows
the key inference results for each key. We can see that the
key inference accuracy decreases slightly as the light condition
changes from 400 lux to 180 lux, which is expected. However,
the key inference result under 180 lux is still quite acceptable,
11
   Typed Text: our    friends    at    the    university    of    texas    are    planning    a    conference    on                            energy    economics    and    finance    in    february    of    next    yearInferred Text: ***   friends    at    the    university    of    texas    are    ********  a    conference    on# of Cand.                    127       7     84             2            2        53       59                        1             9             12                         energy    economics    and    finance    in    february    of    next    year                             84               10             29          64       12        39          14      66       86    Typed Text: we    discuss    the    major    factors    underlying    the    exceptionally    high                             volatility    of    electricity    pricesInferred Text: ***  ******    ***   major    factors    underlying    the    exceptionally    high# of Cand.                                             29             53               10            69               1                 83  volatility    of    electricity    prices          1         13           2               28(a) One-hop accuracy.
(b) Two-hop accuracy.
Fig. 14. Alphabetical keyboard inference accuracy under different light conditions and imperfect reconstruction of the attack scenario.
(c) Three-hop accuracy.
which highlights the wide applicability of VISIBLE in low-
light conditions.
b). Different angles between camcorders and the tablet.
The performance of VISIBLE is also affected by the angles be-
tween the camcorders and the tablet. In previous experiments,
the angle between the camcorders and tablet was 90 degree.
We changed the angle to 60 and 30 degrees while keeping the
distance between the camcorders and tablet unchanged. The
experimental result is shown in Fig. 15 for each key’s inference
accuracy by considering one-hop,
two-hop, and three-hop
neighbors. We can see that in each of three subﬁgures, 90
and 60 degree angles lead to similar key inference accuracy
which is nevertheless better than that of the 30 degree angle.
The reason is as follows. Each camcorder has a speciﬁc Depth
of Field (DOF) that is the distance between the nearest and
farthest objects in a scene that appear acceptably sharp in an
image. If the angle between the camcorders and the tablet is 90
or 60 degree, all the AOIs are in the DOF of the camcorders
so their motions can be clearly recorded. However, if the
angle between the camcorders and the tablet is too small, the
camcorders cannot contain all the AOIs in their DOF, which
leads to blurred AOI images and thus inaccurate estimation of
tablet backside motions. If the angle has to be small due to
practical constraints, the attacker can use multiple camcorders
to record the motions of different AOIs to obtain sharp image
of each AOI.
c). Imperfect reconstruction of
the attack scenario. As
mentioned in Section V-B, to launch a successful key inference
attack, the attacker needs to reconstruct the attack scenario
based on recorded images. However, the reconstructed layout
cannot be exactly the same as the true layout. We therefore
evaluate the impact of imperfect reconstruction of the attack
scenario. For this experiment, we changed the location of the
camcorders randomly by ﬁve centimeters and the position of
the tablet by three centimeters and then redid the key inference
experiment. Fig. 14 shows the key inference accuracy when the
attack scenario is not perfectly reconstructed. We can see that
the key inference accuracy for each key is only slightly lower
than that under perfect reconstruction, which shows the robust-
ness of VISIBLE against small environment change. Note that
attack scenario reconstruction is under the full control of the
attacker and does not involve the victim. Its accuracy depends
only on the quality of the recorded images, and we expect
the reconstructed attacker scenario to be accurate in practice.
12
abcdefghijklmnopqrstuvwxyzAvg.0.00.20.40.60.81.0AccuracyLetter Normal  Low light  Imperfect abcdefghijklmnopqrstuvwxyzAvg.0.00.20.40.60.81.0AccuracyLetter Normal  Low light  Imperfect abcdefghijklmnopqrstuvwxyzAvg.0.00.20.40.60.81.0AccuracyLetter Normal  Low light  Imperfect (a) One-hop accuracy.
(b) Two-hop accuracy.
Fig. 15. Alphabetical keyboard inference accuracy for different angles between the tablet and camcorders.
(c) Three-hop accuracy.
On the other hand, if the environment changes signiﬁcantly
during video recording, e.g., the victim changes position or
moves the tablet for more than 10 centimeters, the attacker
may need to launch a new round of attack to obtain accurate
inference result.
G. Experiments on a Google Nexus 7 Tablet
To demonstrate the universality of VISIBLE, we also
did experiments on a Google Nexus 7 tablet with a 7-inch
touchscreen which is smaller than that of an iPad 2. Backside
motion estimation on Nexus 7 is easier than that on an iPad
2 tablet for two reasons. First, the size of Nexus 7 is smaller
than that of iPad 2, so we were able to video-record the clear
tablet backside motion with only one camcorder. Second, the
Nexus 7’s backside has more texture information (e.g., logo
and dots) which enables motion estimation at more parts of
the tablet backside.
Fig. 16 compares the performance of VISIBLE on a Google
Nexus 7 tablet with Android 4.4 with that on an iPad 2 tablet
with iOS 8. It is easy to see that the key inference accuracy of
VISIBLE is similar on both tablets. This means that VISIBLE
is applicable to smaller-size tablets as long as there are
sufﬁcient areas with texture information on the tablet backside,
which holds for almost all tablets. Besides, we can ﬁnd that the
performance on Nexus 7 is slightly better than that on iPad 2.
The reason is that the Nexus 7’s backside has more texture
information for the attacker to extract motion information,
while the iPad 2’s backside has less texture information (as
shown in Fig. 4(a)). As mentioned in Section V-C, AOIs near
the edges of the tablet backside and separated from each
other tend to have larger and more distinctive motions than
others, making them more capable of differentiating the motion
patterns caused by different keystrokes. Therefore, VISIBLE
performs better on the tablets with rich texture information on
their backsides.
VII. CONCLUSION, COUNTERMEASURES, AND FUTURE
WORK
In this paper, we proposed VISIBLE, a video-assisted key
inference attack framework to infer the victim’s typing content
based on the video recordings of the tablet backside. We
adopted complex steerable pyramid decomposition to obtain
the subtle motions on the tablet backside and used machine
13
abcdefghijklmnopqrstuvwxyzAvg.0.00.20.40.60.81.0AccuracyLetter 90°  60°  30° abcdefghijklmnopqrstuvwxyzAvg.0.00.20.40.60.81.0AccuracyLetter 90°  60°  30° abcdefghijklmnopqrstuvwxyzAvg.0.00.20.40.60.81.0AccuracyLetter 90°  60°  30° (a) One-hop accuracy.
(b) Two-hop accuracy.
Fig. 16. Alphabetical keyboard inference accuracy on a Google Nexus 7 tablet and an iPad 2 tablet.
(c) Three-hop accuracy.
learning techniques to infer the typed keys, words, and sen-
tences. We thoroughly evaluated the performance of VISIBLE
via extensive experiments. Our results show that VISIBLE
can achieve high key inference accuracy for both PIN and
alphabetical soft keyboards and correctly infer the victim’s
typed words or sentences with very high probability.
There are several possible countermeasures against VISI-
BLE. The most straightforward defense is to design a large
featureless cover to cover the stand or the tablet to prevent
the attacker from ﬁnding useful AOIs in the recorded video.
The second possible defense is to randomize the layouts of
the PIN and alphabetical soft keyboards, such that the attacker
cannot recover the typed keys even if he can infer the keystroke
positions on the touchscreen. This defense may be effective,
but it sacriﬁces the user experience, as the user needs to
ﬁnd every key on a random keyboard layout during every
key-typing process. Another possible defense is to have on-
board vibrators generate vibrations during the typing process
to mask the motions caused by the user’s typing process.
However, unlike smartphones, most current commercial off-
the-shelf tablets are not equipped with on-board vibrators.
The ultimate solution is to cover the whole tablet (both the
front and back sides). Though most effective, this solution is
inconvenient and might be socially awkward. The investigation
of these defenses is left as future work.
To the best of our knowledge, VISIBLE is the ﬁrst attempt
to utilize the backside motion of tablets for keystroke analysis.
There are still many issues worth investigation in addition to
the countermeasures above. As we mentioned in Section V-B,
higher resolutions can help us video-record more texture
information details on the tablet backside, and higher frame
rates could video-record more motion details overtime. We
plan to test VISIBLE with more advanced camcorders with
higher resolutions and frame rates. We also seek to investigate
the impact of optical and digital zoom and the distance between
camcorder and the victim’s tablet. In this paper, we only
consider the lower-case letters in the English alphabet. In
practice, more contents such as upper-case letters, punctuation,
characters, and key combinations might be typed by the victim.
Further studies on this issue are challenging but meaningful.
Our current study assumes that the victim places the tablet
with a holder on a desk, while another common scenario is to
hold the tablet by hand. In this case, the motion of the tablet
backside is the combination of the motions of the holding hand
14
abcdefghijklmnopqrstuvwxyzAvg.0.00.20.40.60.81.0AccuracyLetter iPad 2  Nexus 7 abcdefghijklmnopqrstuvwxyzAvg.0.00.20.40.60.81.0AccuracyLetter iPad 2  Nexus 7 abcdefghijklmnopqrstuvwxyzAvg.0.00.20.40.60.81.0AccuracyLetter iPad 2  Nexus 7 and the non-holding hand’s keystrokes. Keystroke inference
in this scenario is much more challenging because we need
to cancel the time-varying motion of the holding hand. In
VISIBLE, the attacker needs to reconstruct the attack scenario
to obtain a training data set to infer the victim’s typed inputs.
Although feasible, it is not so convenient. A more attractive
way is to build a uniﬁed and normalized model, which could
automatically transfer motions video-recorded in different dis-
tances and angles to a uniﬁed and normalized distance and
angle. This will greatly improve the convenience of launching
our proposed attack and deserves further investigations.
ACKNOWLEDGMENT
We would also like to thank our shepherd, Patrick Traynor,
and the anonymous NDSS reviewers for insightful comments.
This work was partially supported by the US National Science
Foundation under grants CNS-1514381, CNS-1513141, CNS-
1421999, CNS-1514014, and CNS-1422301.
REFERENCES
[1]
“Gartner: Device shipments break 2.4b units in 2014,
tablets to
overtake pc sales in 2015,” http://techcrunch.com/2014/07/06/gartner-
device-shipments-break-2-4b-units-in-2014-tablets-to-overtake-pc-
sales-in-2015/.
[2] M. Shahzad, A. Liu, and A. Samuel, “Secure unlocking of mobile touch
screen devices by simple gestures: You can see it but you can not do
it,” in ACM MobiCom’13, Miami, FL, Sep. 2013, pp. 39–50.
[3] L. Li, X. Zhao, and G. Xue, “Unobservable re-authentication for
smartphones,” in NDSS’13, San Diego, CA, Feb. 2013.
J. Sun, R. Zhang, J. Zhang, and Y. Zhang, “Touchin: Sightless two-
factor authentication on multi-touch mobile devices,” in IEEE CNS’14,
San Francisco, CA, Oct. 2014.
[4]
[5] Y. Chen, J. Sun, R. Zhang, and Y. Zhang, “Your song your way:
Rhythm-based two-factor authentication for multi-touch mobile de-
vices,” in IEEE INFOCOM’15, Hongkong, China, Apr. 2015.
[6] M. Backes, M. Dürmuth, and D. Unruh, “Compromising reﬂections-or-
how to read lcd monitors around the corner,” in IEEE S&P’08, Oakland,
CA, May 2008.
[7] M. Backes, T. Chen, M. Duermuth, H. Lensch, and M. Welk, “Tempest
in a teapot: Compromising reﬂections revisited,” in IEEE S&P’09,
Oakland, CA, May 2009.
[8] D. Balzarotti, M. Cova, and G. Vigna, “Clearshot: Eavesdropping on
keyboard input from video,” in IEEE S&P’08, Oakland, CA, May 2008.
[9] F. Maggi, A. Volpatto, S. Gasparini, G. Boracchi, and S. Zanero, “A
fast eavesdropping attack against touchscreens,” in IAS’11, Melaka,
Malaysia, Dec. 2011.
[10] R. Raguram, A. White, D. Goswami, F. Monrose, and J.-M. Frahm,
“ispy: Automatic reconstruction of typed input from compromising
reﬂections,” in AMC CCS’11, Chicago, IL, Oct. 2011.
[11] Y. Xu, J. Heinly, A. White, F. Monrose, and J.-M. Frahm, “Seeing dou-
ble: Reconstructing obscured typed input from repeated compromising
reﬂections,” in AMC CCS’13, Berlin, Germany, Oct. 2013.
[12] Q. Yue, Z. Ling, X. Fu, B. Liu, K. Ren, and W. Zhao, “Blind recognition
of touched keys on mobile devices,” in AMC CCS’14, Scottsdale, AZ,
Nov. 2014.
[13] D. Shukla, R. Kumar, A. Serwadda, and V. Phoha, “Beware, your hands
reveal your secrets!” in AMC CCS’14, Scottsdale, AZ, Nov. 2014.
[14] L. Cai and H. Chen, “Touchlogger: Inferring keystrokes on touch screen
from smartphone motion,” in USENIX HotSec’11, Berkeley, CA, Nov.
2011.
[15] E. Owusu, J. Han, S. Das, A. Perrig, and J. Zhang, “Accessory:
Password inference using accelerometers on smartphones,” in ACM
HotMobile’12, San Diego, CA, Feb. 2012.
[16] E. Miluzzo, A. Varshavsky, S. Balakrishnan, and R. Choudhury, “Tap-
prints: your ﬁnger taps have ﬁngerprints,” in ACM MobiSys’12, Low
Wood Bay, Lake District, UK, June 2012, pp. 323–336.
15
[17] Z. Xu, K. Bai, and S. Zhu, “Taplogger: Inferring user inputs on
smartphone touchscreens using on-board motion sensors,” in ACM
WiSec’12, Tucson, AZ, Feb. 2012.
[18] L. Simon and R. Anderson, “Pin skimmer: Inferring pins through the
camera and microphone,” in SPSM’13, Berlin, Germany, Nov. 2013.
[19] S. Narain, A. Sanatinia, and G. Noubir, “Single-stroke language-
agnostic keylogging using stereo-microphones and domain speciﬁc
machine learning,” in ACM WiSec’14, Oxford, United Kingdom, Jul.
2014.
[20] L. Zhuang, F. Zhou, and J. Tygar, “Keyboard acoustic emanations
revisited,” in ACM CCS’05, Alexandria, VA, Nov. 2005.
[21] Y. Berger, A. Wool, and A. Yeredor, “Dictionary attacks using keyboard
acoustic emanations,” in ACM CCS’06, Alexandria, VA, Nov. 2006.
[22] T. Zhu, Q. Ma, S. Zhang, and Y. Liu, “Context-free attacks using
keyboard acoustic emanations,” in ACM CCS’14, Scottsdale, AZ, Nov.
2014.
[23] P. Marquardt, A. Verma, H. Carter, and P. Traynor, “(sp)iphone:
Decoding vibrations from nearby keyboards using mobile phone ac-
celerometers,” in ACM CCS’11, Chicago, IL, Nov. 2011.
[24] D. Fleet and A. Jepson, “Computation of component image velocity
from local phase information,” Int. J. Comput. Vision, vol. 5, no. 1, pp.
77–104, Aug. 1990.
[25] T. Gautama and M. V. Hulle, “A phase-based approch to the estimation
of the optical ﬂow ﬁeld using spatial ﬁltering,” IEEE Trans. Neural
Netw., vol. 13, no. 5, pp. 1127–1136, Sep. 2002.
[26] E. Simoncelli and W. Freeman, “The steerable pyramid: A ﬂexible
architecture for multi-scale derivative computation,” in ICIP’95, Wash-
ington, DC, Oct. 1995.
[27] E. Adelson, C. Anderson, J. Bergen, P. Burt, and J. Ogden, “Pyramid
methods in image processing,” RCA Engineer, vol. 29, no. 6, pp. 33–41,
1984.
J. Portilla and E. Simoncelli, “A parametric texture model based on
joint statistics of complex wavelet coefﬁcients,” Int. J. Comput. Vision,
vol. 40, no. 1, pp. 49–70, Oct. 2000.
[28]
[29] A. Torralba and A. Oliva, “Depth estimation from image structure,”
IEEE Trans. Pattern Anal. Mach. Intell., pp. 1226–1238, Sep. 2002.
[30] R. Bergman, H. Nachlieli, and G. Ruckenstein, “Detection of textured
areas in images using a disorganization indicator based on component
counts,” 2007, HP Laboratories Israel Technical Report.
[31] N. Wadhwa, M. Rubinstein, F. Durand, and W. T. Freeman, “Phase-
based video motion processing,” ACM Trans. Graph, vol. 32, no. 4, pp.
80:1–80:10, Jul. 2013.
[32] A. Davis, M. Rubinstein, N. Wadhwa, G. Mysore, F. Durand, and
W. Freeman, “The visual microphone: Passive recovery of sound from
video,” ACM Trans. Graph, vol. 33, no. 4, pp. 79:1–79:10, Jul. 2014.
[33] C.-C. Chang and C.-J. Lin, “LIBSVM: A library for support vector ma-
chines,” ACM Trans. Intell. Syst. Technol., vol. 2, no. 3, pp. 27:1–27:27,
2011, software available at http://www.csie.ntu.edu.tw/~cjlin/libsvm.
[34] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and
I. Witten, “The weka data mining software: An update,” SIGKDD
Explor, vol. 11, no. 1, pp. 10–18, Nov. 2009.
“corn-cob dictionary,” http://www.mieliestronk.com/wordlist.html.
[35]
[36] P. Brown, P. deSouza, R. Mercer, V. Pietra, and J. Lai, “Class-based
n-gram models of natural language,” Comput. Linguist., vol. 18, no. 4,
pp. 467–479, Dec. 1992.
[37] B. Klimt and Y. Yang, “The enron corpus: A new dataset for email
classiﬁcation research,” in Machine learning: ECML 2004. Springer,
2004, pp. 217–226.
“Enron email dataset,” https://www.cs.cmu.edu/ ./enron/.
“Parakweet
https://github.com/ParakweetLabs/EmailIntentDataSet.
[38]
[39]
email
lab’s
intent
data
set,”
[40] D. Ping, X. Sun, and B. Mao, “Textlogger: Inferring longer inputs on
touch screen using motion sensors,” in ACM WiSec’15, New York, NY,
Jun. 2015.