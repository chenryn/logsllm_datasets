q
e
r
F
 0.4
 0.35
 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0
Non-coresident
Co-resident
)
%
(
y
c
n
e
u
q
e
r
F
 0.35
 0.3
 0.25
 0.2
 0.15
 0.1
 0.05
 0
Non-coresident
Co-resident
0
0.4
0.8
1.2
1.6
2
2.4
2.8
0
0.4
0.8
1.2
1.6
2
2.4
2.8
0
0.4
0.8
1.2
1.6
2
2.4
2.8
Minimum Network RTT (ms)
Minimum Network RTT (ms)
Minimum Network RTT (ms)
(a) GCE
(b) EC2
(c) Azure
Figure 2: Histogram of minimum network round trip times between pairs of VMs. The frequency is represented as a fraction of total
number of pairs in each category. The ﬁgure does not show the tail of the histogram.
vulnerable to that launch strategy. An ideal placement pol-
icy should aim to reduce both the success rate and the cost
beneﬁt of any strategy.
4 Detecting Co-Residence
An essential prerequisite for the placement vulnerability
study is access to a co-residency detection technique that
identiﬁes whether two VMs are resident on the same phys-
ical machine in a third-party public cloud.
Challenges in modern clouds. Applying the detection
techniques mentioned in Section 2 is no longer feasible in
modern clouds.
In part due to the vulnerability disclo-
sure by Ristenpart et al. [29], modern public clouds have
adopted new technologies that enhance the isolation be-
tween cloud tenants and thwart known co-residence detec-
tion techniques. In the network layer, virtual private clouds
(VPCs) have been broadly employed for data center man-
agement [17,20]. With VPCs, internal IP addresses are pri-
vate to a cloud tenant, and can no longer be used for cloud
cartography. Although EC2 allowed this in older genera-
tion instances (called EC2-classic), this is no longer pos-
sible under Amazon VPC setting.
In addition, VPCs re-
quire communication between tenants to use public IP ad-
dresses for communication. As shown in Figure 2, the net-
work timing test is also defeated, as using public IP ad-
dresses seems to involve routing in the data center network
rather than short-circuiting through the hypervisor. Here,
the ground-truth of co-residency is detected using memory-
based covert-channel (described later in this section). No-
tice that there is no clear distinction between the frequency
distribution of the network round trip times of co-resident
and non-coresident pairs on all three clouds.
In the system layer, persistent storage using local disks
is no longer the default. For instance, many Amazon EC2
instance types do not support local storage [1]; GCE and
Azure provide only local Solid State Drives (SSD) [7, 14],
which are less susceptible to detectable delays from long
seeks.
In addition, covert channels based on last-level
caches [29, 30, 33, 36] are less reliable in modern clouds
that use multiple CPU packages. Two VMs sharing the
same machine may not share LLCs to establish the covert
channel. Hence, these LLC-based covert-channels can only
capture a subset of co-resident instances.
As a result of these technology changes, none of the prior
techniques for detecting co-residency reliably work in mod-
ern clouds, compelling us to develop new approaches for
our study.
4.1 Co-residency Tests
We describe in this subsection a pair of tools for co-
residency tests, with the following design goals:
• Applicable to a variety of heterogeneous software and
• Detect co-residency with high conﬁdence: the false de-
tection rate should be low even in the presence of back-
ground noise from other neighboring VMs.
hardware stacks used in public clouds.
• Detect co-residency fast enough to facilitate experimen-
tation among large sets of VMs.
We chose a performance covert-channel based detection
technique that exploits shared hardware resources, as this
type of covert-channels are often hard to remove and most
clouds are very likely to be vulnerable to it.
A covert-channel consists of a sender and a receiver. The
sender creates contention for a shared resources and uses
it to signal another tenant that potentially share the same
resource. The receiver, on the other hand, senses this con-
tention by periodically measuring the performance of that
shared resource. A signiﬁcant performance degradation
measured at the receiver results in a successful detection of
a sender’s signal. Here the reliability of the covert-channel
is highly dependent on the choice of the shared resource and
the level of contention created by the sender. The sender is
the key component of the co-residency detection techniques
we developed as part of this study.
Memory locking sender. Modern x86 processors sup-
port atomic memory operations, such as XADD for atomic
addition, and maintain their atomicity using cache coher-
ence protocols. However, when a locked operation extends
across a cache-line boundary, the processor may lock the
memory bus temporarily [32]. This locking of the bus can
USENIX Association  
24th USENIX Security Symposium  917
// allocate memory multiples of 64 bits
char_ptr = allocate_memory((N+1)*8)
//move half word up
unaligned_addr = char_ptr + 2
loop forever:
loop i from (1..N):
atomic_op(unaligned_addr + i, some_value)
end loop
end loop
Figure 3: Memory Locking – Sender.
be detected as it slows down other uses of the bus, such
as fetching data from DRAM. Hence, when used properly,
it provides a timing covert channel to send a signal to an-
other VM. Unlike cache-based covert channels, this tech-
nique works regardless of whether VMs share a CPU core
or package.
We developed a sender exploiting this shared memory-
bus covert-channel. The psuedocode for the sender is
shown in Figure 3. The sender creates a memory buffer and
uses pointer arithmetic to force atomic operations on un-
aligned memory addresses. This indirectly locks the mem-
ory bus even on all modern processor architectures [32].
size = LLC_size * (LLC_ways +1)
stride = LLC_sets * cacheline_sz)
buffer = alloc_ptr_chasing_buff(size, stride)
loop sample from (1..10): //number of samples
start_rdtsc = rdtsc()
loop probes from (1..10000):
probe(buffer); //always hits memory
end loop
time_taken[sample] = (rdtsc() - start_rdtsc)
end loop
Figure 4: Memory Probe – Receiver.
Receivers. With the aforementioned memory locking
sender, there are several ways to sense the memory locking
contention induced by the sender in an another co-resident
tenant instance. All the receivers measure the memory
bandwidth of the shared system. We present two types of
receivers that we used in this study that works on heteroge-
neous hardware conﬁgurations.
Memory probing receiver uses carefully crafted memory
requests that always miss in the cache hierarchy and al-
ways hit memory. This is ensured by making data accesses
from receiver fall into the same LLC set. In order to evade
hardware prefetching, we use a pointer-chasing buffer to
randomly access a list of memory addresses (pseudocode
shown in Figure 4). The time needed to complete a ﬁxed
number of probes (e.g., 10,000) provides a signal of co-
residence: when the sender is performing locked opera-
tions, loads from memory proceed slowly.
Memory locking receiver is similar to the sender but mea-
sures the number of unaligned atomic operations that could
be completed per unit time. Although it also measures the
memory bandwidth, unlike memory probing receiver, it
works even when the cache architecture of the machine is
unknown.
The sender along with these two receivers form our
two novel co-residency detection methods that we use in
this study: memory probing test and memory locking test
(named after their respective receivers). These comprise
our co-residency test suite. Each test in the suite starts by
running the receiver on one VM while keeping the other
idle. The performance measured by this run is the baseline
performance without contention. Then the receiver and the
sender are run together. If the receiver detects decreased
performance, the tests concludes that the two VMs are co-
resident. We use a slowdown threshold to detect when the
change in receiver performance indicates co-residence (dis-
cussed later in the section).
Machine
Architecture
Xeon E5645
Xeon X5650
Xeon X5650
Cores Memory Memory
Locking
Probing
6
12
12
3.51
3.61
3.46
1.79
1.77
1.55
Socket
Same
Same
Diff.
Figure 5: Memory probing and locking on testbed machines.
Slowdown relative to the baseline performance observed by the
receiver averaged across 10 samples. Same – sender and receiver
on different cores on the same socket, Diff. – sender and receiver
on different cores on different sockets. Xeon E5645 machine had
a single socket.
Evaluation on local testbed.
In order to measure the efﬁ-
cacy of this covert channel we ran tests in our local testbed.
Results of running memory probing and locking tests under
various conﬁgurations are shown in Figure 5. The hard-
ware architecture of these machines are similar to what is
observed in the cloud [21]. Across these hardware con-
ﬁgurations, we observed a performance degradation of at
least 3.4x compared to not running memory locking sender
on a non-coresident instance (i.e. a baseline run with idle
sender) indicating reliability. Note that this works even
when the co-resident instances are running on cores on dif-
ferent sockets, which does not share the same LLC (works
on heterogeneous hardware). Further, a single run takes one
tenth of a second to complete and hence is also quick.
Note that for this test suite to work in the real world,
an attacker requires control over both the VMs under test,
which includes the victim. We call this scenario as co-
residency detection under cooperative victims (in short,
cooperative co-residency detection). Such a mechanism
is sufﬁcient observe placement behavior in public clouds
(Section 4.2). We further investigated approaches to detect
co-residency under a realistic setting with an uncooperative
victim. In Section 4.3 we show how to adapt the memory
probing test to detect co-location with one of the many web-
servers behind a load balancer.
4.2 Cooperative Co-residency Detection
In this section, we describe the methodology we used to
detect co-residency in public clouds. For the purposes of
studying placement policies, we had the ﬂexibility to con-
918  24th USENIX Security Symposium 
USENIX Association
s
r
i
a
p
f
o
r
e
b
m
u
N
 16
 14
 12
 10
 8
 6
 4
 2
 0
Non-coresident
Co-resident
s
r
i
a
p
f
o
r
e
b
m
u
N
 20
 15
 10
 5
 0
Non-coresident
Co-resident
s
r
i
a
p
f
o
r
e
b
m
u
N
 120
 100
 80
 60
 40
 20
 0
Non-coresident
Co-resident
0.8 1.2 1.6
2
2.4 2.8 3.2 3.6
4
4.4
0.8 1.2 1.6 2 2.4 2.8 3.2 3.6 4 4.4 4.8 5.2 5.6
0.8
1.2
1.5
1.8
2.2
2.6
3
Performance degradation
Performance degradation
Performance degradation
(a) GCE
(b) EC2
(c) Azure – Intel machines
Figure 6: Distribution of performance degradation of memory probing test. For varying number of pairs on each cloud (EC2:300,
GCE:21, Azure:278). Note the x-axis plots performance degradation.
trol both VMs that we test for co-residence. We did this
by launching VMs from two separate accounts and test
them for pairwise co-residence. We encountered several
challenges when running the same on three different pub-
lic clouds - Google Computer Engine, Amazon EC2 and
Microsoft Azure.
First, we had to handle noise from neighboring VMs
sharing the same host. Second, hardware and software het-
erogeneity in the three different public clouds required spe-
cial tuning process for the co-residency detection tests. Fi-
nally, testing co-residency for a large set of VMs demanded
a scalable implementation. We elaborate on our solution to
these challenges below.
Handling noise. Any noise from neighboring VMs could
affect the performance of the receiver with and without the
signal (or baseline) and result in misdetection. To han-
dle such noise, we alternate between measuring the perfor-
mance with and without the sender’s signal, such that any
noise equally affects both the measurements. Secondly, we
take ten samples of each measurement and only detect co-
residence if the ratios of both the mean and median of these
samples exceed the threshold. As each run takes a frac-
tion of a second to complete, repeating 10 times is still fast
enough.
Cloud
Provider
EC2
GCE
Azure
Azure
Machine
Architecture
Intel Xeon E5-2670
Generic Xeon*
Intel E5-2660
AMD Opeteron 4171 HE
Clock
(GHz)
2.50
2.60*
2.20
2.10
LLC
(Ways × Set)
20 × 20480
20 × 16384
20 × 16384
48 × 1706
Figure 7: Machine conﬁguration in public clouds. The machine
conﬁgurations observed over all runs with small instance types.
GCE did not reveal the exact microarchitecture of the physical
host (*). Ways × Sets × Word Size gives the LLC size. The word
size for all these x86-64 machines is 64 bytes.
Tuning thresholds. As expected, we encountered dif-
ferent machine conﬁguration on the three different public
clouds (shown in Figure 7) with heterogeneous cache di-
mensions, organizations and replacement policies [11, 26].
This affects the performance degradation observed by the
receivers with respect to the baseline and the ideal thresh-
old for detecting co-residency. This is important because
the thresholds we use to detect co-residence yield false pos-
itives, if set too low, and false negatives if set too high.
Hence, we tuned the threshold to each hardware we found
on all three clouds.
We started with a conservative threshold of 1.5x and
tuned to a ﬁnal threshold of 2x for GCE and EC2 and
1.5x for Azure for both the memory probing and locking
tests. Figure 6 shows the distribution of performance degra-
dation under the memory probing tests across Intel ma-
chines in EC2, GCE, and Azure. For GCE and EC2, a per-
formance degradation threshold of 2 clearly separates co-
resident from non-co-resident instances. For all Intel ma-
chines we encountered, although we ran both memory lock-
ing and probing tests, memory probing was sufﬁcient to de-
tect co-residency. For Azure, overall we observe lower per-
formance degradation and the initial threshold of 1.5 was
sufﬁcient to detect co-location on Intel machines.
The picture for AMD machines in Azure differs signif-
icantly as shown in Figure 8. The distribution of perfor-
mance degradation for both memory locking and mem-
ory probing shows that, unlike for other architecures, co-
residency detection is highly sensitive to the choice of the
threshold for AMD machines. This may be due to the
Azure - AMD Machines
mprobe-NC
mlock-NC
mprobe-C
mlock-C
s
r
i
a
p
f
o
r
e
b
m
u
N
 18
 16
 14
 12