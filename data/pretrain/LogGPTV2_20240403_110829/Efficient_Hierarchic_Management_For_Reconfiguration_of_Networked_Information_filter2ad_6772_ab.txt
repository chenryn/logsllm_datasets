### Connectivity Policy and Event Handling

For an event to be received, the sender must have a trust rating of at least 2. Once this condition is met, the event is evaluated against the connectivity policy expression. If the expression evaluates to true, the event is accepted. This ensures that Web servers only receive commands and alerts from qualified senders.

### Command and Alert Events

Consider a scenario where a worm is propagating through Macrocorpâ€™s networks. The first detection occurs in the Northwest region by a fault-response system. It identifies that all sensor events are originating from Web servers running version 2.4 of IIS. The system then reports this to national fault-response systems and issues a worm alert:

```plaintext
Event: 
{
  alert = "worm";
  threat_level = 4;
  target = "IIS";
  version = "2.4.*"
}
```

Any receiver can extract the relevant information by examining the event's attributes. Given the enforced policy, all Web servers in the Northwest region will receive this alert and can determine if they are vulnerable to the attack. Meanwhile, the worm continues to spread. Intellimune attempts to mitigate the attack by issuing commands to the Web servers.

The national fault-response system determines that the worm is exploiting a vulnerability in CGI scripts on IIS 2.4 servers. It issues the following command:

```plaintext
Event: 
{
  application = "IIS";
  application_version = "2.4";
  services = supersetOf{cgi};
  command = "disable_cgi"
}
```

This event is directed to Web servers running IIS version 2.4 with CGI support, instructing them to disable CGI elements to limit the infection vector.

To further contain the worm, Intellimune issues another command. It identifies that IIS 2.4 servers with a sustained load are likely infected and orders them to shut down:

```plaintext
Event: 
{
  application = "IIS";
  application_version = "2.4";
  load > 0.9;
  command = "shutdown_now"
}
```

This example demonstrates the delivery of an alert and two command events to components of an Internet-scale system. The connectivity policies between managers address properties of senders, receivers, and content, defining a comprehensive policy based on the current state of participants.

### Implementation

We now describe the implementation of the Selective Notification service, which has two limitations:
1. Not all clients can simultaneously act as senders while maintaining efficiency.
2. It generates more traffic in the overlay network than is strictly required for content-based forwarding.

Our implementation modifies the core data structures and algorithms of Siena, a scalable, content-based publish/subscribe infrastructure. Siena's core data model consists of Filters and Notifications. A Notification is a communicated event with typed attribute-value pairs. Filters are Boolean conjunctive expressions over notification attributes, used to define content subscriptions issued by potential receivers. Siena operates as a distributed tree of dispatch servers, as shown in Figure 3.

#### Key Algorithms in Siena

- **Filter Propagation**: Subscribed filters propagate up the dispatcher tree until they reach the root or a dispatcher with a logically covering filter. Dispatchers store received filters, and Siena scales well when most subscribed filters are covered by others.
- **Notification Forwarding**: Published notifications propagate up to the root and are sent down any sub-tree from which a matching filter was received, ensuring notifications are only forwarded to relevant sub-trees.

#### Data Transformations

The Selective Notification service transforms receiver policies and events into publish/subscribe filters and notifications. Siena supports content-based addressing, making the transformation of sender qualification straightforward. Attributes and constraints are stored in notifications and filters, respectively. Intentional addressing is more complex and best illustrated by example. For instance, a receiver with a "load" attribute of "0.3" advertising the selection function "X<load<Y" is translated to a Siena filter "X<0.3 and 0.3<Y." Senders select receivers by load by sending a notification defining values for X and Y.

#### Modifications to Publish/Subscribe Infrastructure

To accommodate Selective Notification, we made significant modifications to Siena's dispatcher algorithms and data structures:

- **Notification Persistence**: Notifications remain at dispatchers for a specified lifetime, ensuring reliable delivery despite frequent changes in filters.
- **Filter Coagulation**: Filters are generalized to maintain scalability, reducing message forwarding efficiency but ensuring all relevant notifications are delivered.
- **Attribute Authorization and Capability**: Clients must register with a password to restrict them to stating attributes for which they are authorized.
- **Third Party Qualifiers**: Third parties can contribute state to client addressing, such as for trust management, with permission from the client.
- **Channeling and Event Ordering**: Some events record their forwarding paths, allowing streams of events to travel to the same set of receivers even as their state changes.

### Performance and Scalability

The main challenge with Selective Notification is maintaining performance as the network size and rate of change of addresses increase. Critical metrics include:

- **Sustainable Event Delivery Time**: Time from event issue to delivery to all relevant clients.
- **Sustainable Event Throughput**: The sustainable rate at which events can be issued without overloading the service.

Key dimensions affecting performance are:

- **Application System Size**: Measured by the total number of independent nodes.
- **Addressing Policies**: Describing senders, receivers, and content.
- **Rate of State Change**: Addresses being presented to the Selective Notification mechanism.

### Experimental Method

To evaluate Selective Notification, we operated overlay networks on a test-bed of 128 physical computers, each a dual 400 MHz CPU i86 machine running Red Hat Linux 6.1. All software was implemented in Java for runtime 1.4.1, with TCP sockets over a 100 MBit/sec fully switched Ethernet. Some computers were dedicated to Selective Notification dispatchers, and the rest executed a hypothetical distributed application, allowing for several thousand client nodes.

### Dispatcher Performance Measurements

We assessed dispatcher performance using a "ping-pong" throughput experiment. A single "Pinger" application sends "ping" messages to "Ponger" applications, which respond with "pong" messages and generate broadcast-like background "chatter" messages. Pongers randomly change their client models at a specified rate, simulating dynamic state changes.

By varying the number of Ponger elements, chatter rate, event-persistence lifetime, Ponger client model size, and the rate of client model changes, we observed performance capability. Each application, including Pongers, Pinger, and the Selective Notification dispatcher, ran on separate computers.

### Results

- **Event Throughput and Output Performance**: Maximum throughput rates were calculated by varying Ponger broadcast-chatter rates and determining the point of throughput saturation. Three versions of the Selective Notification dispatcher were tested: complete (SN), forward-only (Forward Only), and blind forwarding (TCP Relay). The results show that the throughput rate under worst-case conditions is dominated by network communication costs.

This comprehensive approach ensures that the Selective Notification service can handle large-scale, dynamic environments effectively.