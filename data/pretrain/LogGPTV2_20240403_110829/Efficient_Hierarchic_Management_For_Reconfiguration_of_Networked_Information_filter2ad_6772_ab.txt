with trust rating greater than or equal to 2”. Once in place,
received events from senders are those for which evaluation
of  the  connectivity  policy  expression  is  true.  Thus,  Web
servers  only  receive  understandable  commands  and  alerts
from qualified commanders.
4.3 Command and alert events
Assume  that  a  worm  has  begun  propagating  through
Macrocorp’s  networks.  A  fault-response  system  in  the
Northwest is the first to detect the worm infection. It deter-
mines that the sensor events are all coming from Web serv-
ers  running  version  2.4  of  “IIS.”  First,  it  reports  this  to
national fault-response systems. Then, it issues a worm alert
as an event:
Event : 
alert
threat_level
target
version
= “worm”;
= 4;
= “IIS”;
= “2.4.*”}
Any receiver of this event will be able to obtain the informa-
tion associated with it by examining the attributes.
Given that Web servers are enforcing the policy defined
above, all Web servers in the Northwest region will receive
this  alert.  From  the  alert  they  can  determine  whether  they
are vulnerable to attack. Meanwhile, the worm continues to
infect  the  network.  Intellimune  attempts  to  halt  the  attack
with  commands  to  Web  servers.  Its national  fault-response
system has determined that the worm is spreading through a
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:41 UTC from IEEE Xplore.  Restrictions apply. 
Dispatcher
Client
Notification
Forwarding
Figure 3. Siena operates as a distributed hierarchy of dispatchers and clients.
vulnerability exposed in CGI-scripts running on version 2.4
of IIS Web servers. Therefore, it issues the following com-
mand event:
 : {
application
application_version
services
Event : {
command
== “IIS”
== “2.4”
supersetOf{cgi}}
AND
AND
= “disable_cgi”}
This event contains a preamble that is a selector (intentional
address).  It  selects Web servers that are running  “IIS” ver-
sion 2.4 and support CGI scripts. The event itself is a com-
mand  for  those  Web  servers  to  disable  CGI  elements.  The
goal of  this command is  to limit the infection by disabling
the worm’s attack vector.
In an attempt to limit the worm’s aggression, Intellimune
emits another command. It has determined that IIS version
2.4 servers showing sustained load are potentially infected.
These servers are  ordered  to  shut-down with  the  following
command event:
 : {
application
application_version
load
Event : {
== “IIS” AND
== “2.4” AND
>  0.9}
= “shutdown_now”}
command
This  example  has  demonstrated  the  delivery  of  an  alert
event  and  two  command  events  to  application  components
of  an  Internet-scale  system.  The  connectivity  policies
between  managers  address  properties  of  senders,  receivers
and content. They define a total connectivity policy target-
ing  management  at  run-time  based  on  the  current  state  of
participants.
5. Implementation
We  now  proceed  to  describe  our  implementation  of  the
Selective Notification service. We note that it has two limi-
tations:  (1)  not  all  clients  are  supported  as  simultaneous
senders if efficiency is to be maintained; and (2) it necessi-
tates  more  traffic  in  the  overlay  network  than  is  strictly
required for content-based forwarding.
Our implementation was developed, in part, by modify-
ing  the  core  data  structures  and  algorithms  of  Siena [2]
(Scalable Internet Event Notification Architecture)—a con-
tent-based, publish/subscribe infrastructure.
Siena’s  core  data  model  is  Filters  and  Notifications.  A
Notification is a communicated event consisting of a set of
typed  attribute/value  pairs.  Filters  are  Boolean  conjunctive
expressions  over  notification  attributes.  They  are  used  to
define  content  subscriptions  issued  by  potential  receivers.
Siena  operates  as  a  distributed  tree  of  dispatch  servers,  an
example of which is shown in Figure 3. Dispatchers perform
two key algorithms. These are:
• Filter Propagation: A subscribed filter, f, propagates up
the dispatcher tree until it arrives at the root of the tree,
or at a dispatcher with a filter logically covering it, i.e., a
filter that passes a superset of that passed by f. Dispatch-
ers  store  received  filters.  Siena  scales  very  well  when
most  subscribed  filters  are  covered  by  others  as  occurs
frequently in publish/subscribe applications.
• Notification Forwarding: A published notification, p, is
propagated up to the root of the dispatcher tree. It is also
sent down any sub-tree from which a matching filter was
received. As a result, notifications are only forwarded to
sub-trees with receivers.
5.1 Data transformations
The  Selective  Notification  service  transforms  receiver
policies and Selective Notification events into publish/sub-
scribe  filters  and  notifications,  respectively.  Figure  4
sketches  the  transformation  of  data  constructs  from  the
Selective  Notification  service  to  Siena  publish/subscribe.
Shapes  represent  data  objects.  Arrows  represent  the  prod-
ucts of transformations. “Plus Signs” indicate the combina-
tion of two data objects in a transform.
Siena  already  supports  content-based  addressing.  The
transformation  of  sender  qualification  is  straightforward,
attributes and constraints are stored in notifications and fil-
ters, respectively. The transformation of intentional address-
ing  is  more  complex  and  best  illustrated  by  example.
Consider  a  receiver  with  an  attribute  called  “load”  with
value “0.3”. If the receiver advertises the selection function
“X<load<Y,”  then  this  is  translated  to  a  Siena  filter  of  the
form  “X<0.3  and  0.3<Y.”  When  a  sender  selects  a  set  of
receivers by load, it does so by sending a notification defin-
ing values for X and Y.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:41 UTC from IEEE Xplore.  Restrictions apply. 
5.2 Modifications to publish/subscribe 
infrastructure
The second part of the implementation required modifi-
cation to the Siena dispatcher algorithms and data structures
available at the time this work was done. This is because the
characteristics of Selective Notification are not equivalent to
publish/subscribe.  If  it  were  applied  without  change  to
Siena, our transformed input would: (1) not take advantage
of the scalability of Siena filters; (2) fail to deliver most rel-
evant  notifications;  and  (3)  allow  clients  to  lie  arbitrarily
about  their  attribute  qualifications.  If  data  transforms  only
were  applied,  Selective  Notification  would  be  a  nearly-
pathological  application  of  publish/subscribe.  Therefore,
significant  alterations  to  algorithms  have  been  made  while
preserving the two core operations of notification forward-
ing  and  filter  propagation.  Briefly,  these  alterations  and
modifications are:
• Notification  Persistence:  Notifications  remain  at  dis-
patchers  for  a  specified  lifetime  where  they  forward  to
later subscription filters. In this way, the consistent and
rapid  changing  of  filters  for  intentional  addressing  and
sender qualification does not impede reliable delivery of
notifications.
• Filter Coagulation: Intentional addressing does not gen-
erate  efficient  filter  covering  relationships.  Hence,  we
deliberately  generalize  filters,  i.e.,  make  them  less  spe-
cific,  to  maintain  system  scalability.  This  reduces  mes-
sage  forwarding  efficiency  because  some  messages  are
forwarded along paths that will not use them, but aggre-
gation maintains notification delivery reliability, i.e., all
receivers obtain all and only relevant notifications.
• Attribute Authorization and Capability: Clients of Selec-
tive  Notification  must  register  for  notification  and  sub-
scription  capabilities  by  “login”  with  a  password.  This
restricts clients to stating attributes in models and notifi-
cations for which they are authorized to make claims.
• Third  Party  Qualifiers:  We  enable  third  parties  to  con-
tribute  state  to  client  addressing,  for  example  for  trust
management. Third parties must be given permission, by
session key, from the client which they are to augment.
Importantly, a third party may have different authorized
capabilities  than  the  client  it  augments.  This  supports
tiered authority models in the use of sender qualification
and intentional addressing.
• Channeling and Event Ordering: Rather than computing
the  forwarding  path  for  all  events,  some  events  record
their  forwarding  paths,  and  others  follow  these  paths.
This allows streams of events to travel to the same set of
receivers, even as their state changes.
6. Measurements of performance and a model 
of scale
The essential practical challenge with Selective Notifi-
cation is maintaining adequate performance with scale both
in  terms  of  network  size  and  rate  of  change  of  addresses.
The issue of performance is complex because performance
metrics need to be defined and measured along the spectrum
of dimensions that will affect performance in real systems.
From the perspective of general utility, we consider the fol-
lowing to be the critical metrics:
•
Sustainable  event  delivery  time:  Time  from  event  issue
to event delivery to all relevant clients.
•
Sustainable  event  throughput:  The  sustainable  rate  at
which events can be issued into the service without over-
loading the service.
With those metrics in mind, the key dimensions that affect
performance are:
Selection Function
Template Subscriptions
Local Attribute Values
Actual Parameters
Local Attribute Values
Sender Qualifiers
Event Content
Selective Notification Event
Receiver Policy
Notifications
Siena
Filters
Figure 4. Data transforms through (a) the Selective Notification service and (b) Siena
(a)
(b)
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:41 UTC from IEEE Xplore.  Restrictions apply. 
• The  size  of  the  application  system  as  measured  by  the
6.2 Measurement of dispatcher performance
total number of independent nodes.
• The addressing policies that describe senders, receivers,
and content simultaneously.
• The rate of change of the state, i.e., addresses being pre-
sented to the Selective Notification mechanism.
In  this  section,  we  present  the  results  of  experiments  to
determine the metrics above for these parameters. Using the
results,  we  develop  implementation  performance-driven
models of scale.
6.1 Experimental method
In an effort to evaluate Selective Notification, we have
operated  overlay  networks  on  a  test-bed  of  128  physical
computers,  each  of  which  is  a  dual  400  MHz  CPU  i86
machine running Red Hat Linux 6.1 with kernel version 2.2.
All  software  was  implemented  in  Java  for  runtime  1.4.1.
Network  level  communication  was  performed  with  TCP
sockets over a 100 MBit/sec fully switched Ethernet. Some
computers  were  dedicated  to  the  execution  of  Selective
Notification  dispatchers  and  the  remainder  were  used  to
execute  a  hypothetical  distributed  application.  Several
application  nodes  were  located  on  each  physical  machine
and this permitted a total of several thousand client nodes to
be constructed. The number of clients varied with the details
of the experiments being conducted but was typically 3,400.
This  target  system  allowed  us  to  demonstrate  some
aspects  of  feasibility.  However,  this  system  is  clearly  not
even close to Internet scale and so we were not able to test
Selective  Notification  over  applications  with  hundreds  of
thousands  or  millions  of  clients  using  it.  Instead,  we  have
measured the maximum, worst-case performance of a single
dispatcher and used the results in an implementation-driven
model of performance of a large system. Assuming that all
dispatchers have the same resources, we have modeled the
worst-case  performance  of  a  dispatcher  overlay  network
using the worst case performance obtained from  the single
dispatcher for all dispatchers in the tree (assuming optimal
network-level performance.) From this data, we have mod-
eled  the  resources  required  to  achieve  necessary  service
properties in large networks.
We  assessed  dispatcher  performance  by  operating  and
measuring a dispatcher in a controlled environment; one in
which  all  the  input  factors  affecting  performance  are  con-
trolled. These factors were:
• Available Resources: This includes computing hardware,
network  resources,  and  the  run-time  system  environ-
ment.
• Event  Forwarding  Set  Size:  The  size  of  the  set  of  sub-
dispatchers and clients to which events are forwarded.
• Event  Persistence  Lifetime:  The  persistence  lifetime  of
event messages.
•
Subscription  Change  Rate:  The  rate  at  which  clients
modify their state and the rate at which filter coagulation
is performed.
• Connection Policy Complexity: The number of attribute
constraints registered by clients.
Our  experimental  apparatus,  illustrated  in  Figure  5,  is  a
“ping-pong”  throughput  experiment.  A  single  “Pinger”
application sends “ping” messages to “Ponger” applications
in sequence. Ponger applications respond to the Pinger with
a “pong” message. Additionally, Ponger applications gener-
ate broadcast-like background “chatter” messages sent to all
other Pongers. During the entire experiment, Ponger appli-
cations  randomly  change  the  values  of  their  client  models
(essentially their addresses) at a specified rate, representing
diverse dynamic state change throughout the distributed sys-
tem. These last two conditions emulate a worst-case system
behavior with consistently changing client state and the con-
tinuous broadcast of commands and alerts.
By  varying  the  number  of  Ponger  elements,  the  rate  at
which they chatter, the event-persistence lifetime of chatter,
the  size  of  the  Ponger  client  model,  and  the  rate  at  which
Pongers  change  their  client  model  instances,  we  observe
performance capability with respect to the input parameters
of  interest.  For  purposes  of  experimentation,  each  applica-
tion—including  each  Ponger,  the  Pinger,  and  the  Selective
Notification dispatcher—were run on separate computers.
Pinger
Dispatcher
Ponger
Ponger
Ponger
Figure 5.  An experimental test-bed for performance measurements.
Proceedings of the 2004 International Conference on Dependable Systems and Networks (DSN’04) 
0-7695-2052-9/04 $ 20.00 © 2004 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 20,2021 at 11:50:41 UTC from IEEE Xplore.  Restrictions apply. 
6.2.1. Event throughput and output performance. 
Figure 6(a) shows the maximum throughput rate computed
using the ping-pong experimental configuration. Figure 6(b)
shows the maximum event forwarding rate computed for the
same  experiments.  The  X-axes  are  the  number  of  Ponger
clients  attached  to  the  Selective  Notification  dispatcher  in
the experiment. The Y-axis of Figure 6(a) is the chatter rate
of Pongers communicating with each other using Selective
Notification but in a worst-case, broadcast-like way. The Y-
axis  of  Figure  6(b)  is  the  rate  of  output  notifications  from
the dispatcher.
Maximum  throughput  rates  were  calculated  by  varying
Ponger broadcast-chatter rates and determining the point of
throughput  saturation,  i.e.,  the  point  where  the  dispatcher
would  begin  falling  behind  permanently.  This  experiment
was  performed  with  10  second  client-model  attribute
updates and coagulation updates, and 60 second persistence
lifetimes  for  notifications.  Ping  notifications  were  sent
every two seconds.
In  order  to  determine  the  factors  affecting  performance,
three versions of the Selective Notification dispatcher were
run in separate trials. The first (labelled SN in Figure 6) was
complete,  the  second  (labelled  Forward  Only)  performed
forwarding  calculations  but  then  simulated  infinitely-fast
network  communications,  and  the  third  (labelled  TCP
Relay)  blindly  forwarded  all  notifications,  i.e.,  it  provided
no  Selective  Notification  notification  service.  The  results
(Figure 6) show that the throughput rate of Selective Notifi-
cation  under  worst-case  forwarding  conditions  (broadcast-
like) is dominated by the cost of network communications.
The  dispatcher  performing  all  forwarding  calculations  but
only  pretending  to  do  network  communications  (Forward
Only) had twice the throughput of the dispatcher doing net-
work communication but only pretending to do forwarding
Output Notification Rate  w ith Attr ibute  
M ode l Size
1000
900
800
600
400
200
0
600
350
220
170
120
1
5
10
15
20
25
Attr ibute  M ode l Size
/
c
e
S
s
n
o
i
t
a
c