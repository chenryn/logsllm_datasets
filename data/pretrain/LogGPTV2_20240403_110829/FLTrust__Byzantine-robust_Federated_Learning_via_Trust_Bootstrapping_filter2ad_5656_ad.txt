previous work [15] to distribute the training examples in a
dataset among clients. Assuming there are M classes in a
dataset. We randomly split
the clients into M groups. A
training example with label l is assigned to group l with
probability q > 0 and to any other group with probability
1−q
M−1. Within the same group, data are uniformly distributed
to each client. q controls the distribution difference of the
clients’ local training data. If q = 1/M, then the clients’
local training data are independent and identically distributed
(IID), otherwise the clients’ local training data are non-IID.
Moreover, a larger q indicates a higher degree of non-IID
among the clients’ local training data. One characteristic of
FL is that clients often have non-IID local training data [22],
[28]. Therefore, we will set q > 1/M by default to simulate
the non-IID settings.
Next, we use the MNIST dataset as an example to show
the distribution process. Assume we have 100 clients in total
and set q = 0.5. M = 10 for the MNIST dataset. We ﬁrst
randomly split the clients into 10 groups, each containing 10
clients. For a training image of digit l (e.g., l = 5), we ﬁrst
assign it to group 5 with probability 0.5, and to any other group
10−1 ≈ 0.056. Once the group is determined,
with probability 1−0.5
e.g., group 5 is chosen, we will select a client from group
5 uniformly at random and assign this training image to the
selected client.
MNIST-0.1: MNIST [24] is a 10-class digit image classi-
ﬁcation dataset, which consists of 60,000 training examples
and 10,000 testing examples. We set q = 0.1 in MNIST-0.1,
which indicates local training data are IID among clients. We
use MNIST-0.1 to show that FLTrust is also effective in the
IID setting.
MNIST-0.5: In MNIST-0.5, we simulate non-IID local train-
ing data among the clients via setting q = 0.5.
Fashion-MNIST: Fashion-MNIST [42] is a 10-class fashion
image classiﬁcation task, which has a predeﬁned training set
of 60,000 fashion images and a testing set of 10,000 fashion
images. Like the MNIST-0.5 dataset, we distribute the training
examples to the clients with q = 0.5 to simulate non-IID local
training data.
CIFAR-10: CIFAR-10 [23] is a color image classiﬁcation
dataset consisting of predeﬁned 50,000 training examples and
10,000 testing examples. Each example belongs to one of
the 10 classes. To simulate non-IID local training data, we
distribute the training examples to clients with q = 0.5.
Human activity recognition (HAR): The HAR dataset [4]
consists of human activity data collected from the smartphones
of 30 real-world users. The data are signals from multiple
sensors on a user’s smartphone, and the task is to predict
the user’s activity among 6 possible activities, i.e., WALK-
ING, WALKING UPSTAIRS, WALKING DOWNSTAIRS,
SITTING, STANDING, and LAYING. Each example includes
561 features and there are 10,299 examples in total. Unlike
the previous datasets, we don’t need to distribute the data to
clients in this dataset, as each user is naturally considered as a
client. HAR represents a real-world FL scenario, where each
user is considered as a client. We use 75% of each client’s data
as training examples and the rest 25% as testing examples. We
note that HAR has unbalanced local training data on clients:
the maximum number of training examples on a client is 409,
the minimum number is 281, and the mean is 343.
CH-MNIST: CH-MNIST [21] is a medical image classiﬁ-
cation dataset consisting of 5,000 images of histology tiles
collected from colorectal cancer patients. Each example has
64 × 64 gray-scale pixels and belongs to one of the 8 classes.
We use 4,000 images selected randomly as the training exam-
ples and use the other 1,000 images as the testing examples. To
simulate non-IID local training data, we distribute the training
examples to clients with q = 0.5.
2) Evaluated Poisoning Attacks: We consider both data
poisoning attacks and local model poisoning attacks. For data
poisoning attack, we consider the popular label ﬂipping attack.
For local model poisoning attacks, we evaluate Krum attack,
Trim attack, and our adaptive attack (untargeted attacks) [15],
as well as Scaling attack (targeted attack) [5].
Label ﬂipping (LF) attack: We use the same label ﬂipping
attack setting as [15]. In particular, for each training example
on the malicious clients, we ﬂip its label l to M − l− 1, where
M is the total number of labels and l ∈ {0, 1,··· , M − 1}.
Krum attack: Krum attack is an untargeted local model
poisoning attack optimized for the Krum aggregation rule. We
use the default parameter settings in [15] for the Krum attack.
Trim attack: Trim attack is an untargeted local model
poisoning attack optimized for the Trim-mean and Median
aggregation rules. We use the default parameter settings in
[15] for the Trim attack.
Scaling attack: Scaling attack is a targeted local model
poisoning attack. Speciﬁcally, we consider the attacker-chosen
target testing examples are normal testing examples with a
predeﬁned feature-pattern trigger embedded. Following [5],
we use the data augmentation scheme in [18] to implement
the Scaling attack. Speciﬁcally, each malicious client copies p
fraction of its local training examples, embeds the trigger to
them, changes their labels to the attacker-chosen target label,
and uses them to augment its local training data. Then, in each
iteration of FL, each malicious client computes its local model
update based on the augmented local training data and scales
it by a factor λ (cid:29) 1 before sending it to the server.
Speciﬁcally, we use the same pattern trigger in [18] as
our trigger for MNIST-0.1, MNIST-0.5, Fashion-MNIST, and
CH-MNIST, and we set the attacker-chosen target label as
0; for CIFAR-10, we consider the same pattern trigger and
target label (i.e., “bird”) in [5]; and for HAR, we create a
feature-pattern trigger by setting every 20th feature to 0 and
we set the target label as “WALKING UPSTAIRS”. Following
previous work [5], we set the scaling factor λ = n, where n
is the number of clients. In each dataset, the attacker-chosen
target testing examples consist of the trigger-embedded normal
testing examples whose true labels are not the target label.
Adaptive attack: We evaluate the adaptive attack proposed
in Section V. Our adaptive attack leverages an zeroth-order
9
TABLE I: The default FL system parameter settings.
MNIST-0.1 MNIST-0.5 Fashion-MNIST CIFAR-10
100
HAR
30
CH-MNIST
40
2,000
3 × 10−4
32
2,500
6 × 10−3
20
n
1
20
64
1,000
1,500
2 × 10−4 3 × 10−3 3 × 10−4 (decay at the 1500th and
1750th iterations with factor 0.9)
2,000
32
6
8
Explanation
# clients
# clients selected in each iteration
# local iterations
# global iterations
n
τ
Rl
Rg
b
α · β
m/n fraction of malicious clients (%)
combined learning rate
batch size
m
f
k
|D0|
# malicious clients
Krum parameter
Trim-mean parameter
size of the root dataset
TABLE II: The CNN architecture of the global model used for
MNIST-0.1, MNIST-0.5, and Fashion-MNIST.
Layer
Input
Convolution + ReLU
Max Pooling
Convolution + ReLU
Max Pooling
Fully Connected + ReLU
Softmax
Size
28 × 28 × 1
3 × 3 × 30
3 × 3 × 50
2 × 2
2 × 2
100
10
optimization method. Following the suggestions by previous
work [13], [33], we set σ2 = 0.5 and γ = 0.005 in the zeroth-
order method. Moreover, we set η = 0.01 and V = Q = 10
so that the adaptive attack converges.
3) Evaluation Metrics: For the LF attack, Krum attack,
Trim attack, and adaptive attack, we use the standard testing
error rate of the global model to evaluate an FL method since
these attacks aim to increase the testing error rate. Speciﬁcally,
the testing error rate of a global model is the fraction of testing
examples whose labels are incorrectly predicted by the global
model. An FL method is more robust against these attacks if
its global models achieve lower testing error rates under these
attacks. The Scaling attack is a targeted attack, which aims to
preserve the testing error rate of normal testing examples while
making the global model predict the attacker-chosen target
label for the attacker-chosen target testing examples. There-
fore, other than the testing error rate, we further use attack
success rate to measure the Scaling attack. Speciﬁcally, the
attack success rate is the fraction of the attacker-chosen target
testing examples whose labels are predicted as the attacker-
chosen target label by the global model. An FL method is more
robust against the Scaling attack if its global model achieves
a lower attack success rate.
4) FL System Settings: By default, we assume there are
n = 100 clients in total for each dataset except HAR and
CH-MNIST. For HAR, the data are collected from 30 users,
each of which is treated as a client. Therefore, HAR has 30
clients in total. For CH-MNIST, there are only 4,000 training
examples in total and thus we assume 40 clients such that each
client has 100 training examples on average. Unless otherwise
mentioned, we assume 20% of the clients are malicious for
each dataset. However, we will also explore the impact of the
fraction of malicious clients. Table I shows the default FL
system settings that we will use unless otherwise mentioned.
Global models: We train different types of global models
on different datasets to show the generality of our method.
m
m
100
Speciﬁcally, for MNIST-0.1, MNIST-0.5, and Fashion-MNIST,
we train a convolutional neural network (CNN) as the global
model. Table II shows the architecture of the CNN. And we
train a logistic regression (LR) classiﬁer as the global model
for HAR. For CIFAR-10 and CH-MNIST, we consider the
widely used ResNet20 architecture [19] as the global model.
Parameter settings of
the FL methods: We compare
FLTrust with FedAvg [22], [28], Krum [9], Trim-mean [48],
and Median [48]. Details of these FL methods can be found
in Section II-A. FedAvg is a popular FL method in non-
adversarial settings, while Krum, Trim-mean, and Median are
Byzantine-robust FL methods. These methods all follow the
three-step framework described in Algorithm 2, though they
use different aggregation rules. Therefore, they all use the
parameters τ, Rl, Rg, α, β, and b. Following previous work
[15], we set τ = n, i.e., all clients are selected in each iteration;
and we set Rl = 1, in which we can treat the product of
the global learning rate α and the local learning rate β as a
single learning rate. We set this combined learning rate on
each dataset to achieve small training error rates and fast
convergence. We set the batch size b = 32 for all datasets
except CIFAR-10, where we set b = 64. We set the number
of global iterations Rg such that the FL methods converge.
Speciﬁcally, Rg = 2, 000 for MNIST-0.1, MNIST-0.5, and
CH-MNIST; Rg = 2, 500 for Fashion-MNIST; Rg = 1, 500
for CIFAR-10; and Rg = 1, 000 for HAR.
Krum further has the parameter f and Trim-mean further
has the trim parameter k, both of which are an upper bound
of the number of malicious clients. We set f = k = m, which
assumes that the server knows the exact number of malicious
clients and gives advantages to Krum and Trim-mean.
Root dataset: Our FLTrust requires a small root dataset.
By default, we assume the root dataset has only 100 training
examples. Moreover, we consider the following two cases
depending on how the root dataset is created.
Case I. We assume the service provider can collect
a representative root dataset for the learning task,
i.e.,
the root dataset has the same distribution as
the overall training data distribution of the learning
task. In particular, we sample the root dataset from
the union of the clients’ clean local
training data
uniformly at random. For instance, for MNIST-0.5,
we sample the root dataset from its 60,000 training
examples uniformly at random.
Case II. We assume the root dataset has a distribution
different from the overall training data distribution of
•
•
10
the learning task. In particular, we assume the root
dataset is biased towards a certain class. Speciﬁcally,
we sample a fraction of the examples in the root
dataset from a particular class (class 1 in our experi-
ments) in the union of the clients’ clean local training
data and the remaining examples are sampled from
the remaining classes uniformly at random, where we
call the fraction bias probability. Note that, for all
the datasets except HAR and CH-MNIST, the root
dataset has the same distribution as the overall training
data, i.e., Case II reduces to Case I, when the bias
probability is 0.1 because they have 10 classes; for
HAR and CH-MNIST, Case II reduces to Case I
when the bias probability is 0.17 and 0.125 because
they have 6 and 8 classes, respectively. The root data
distribution deviates more from the overall training
data distribution when the bias probability is larger.
In both cases, we exclude the sampled root dataset from
the clients’ local training data, indicating that the root dataset
is collected independently by the service provider. Unless
otherwise mentioned, we consider Case I.
B. Experimental Results
Our FLTrust achieves the three defense goals: Recall
that we have three defense goals (discussed in Section III):
ﬁdelity, robustness, and efﬁciency. Table III shows the testing
error rates of different FL methods under different attacks
including our adaptive attack, as well as the attack success
rate of the Scaling attack on the six datasets. Our results show
that FLTrust achieves the three goals.
First, when there is no attack, our FLTrust has testing
error rates similar to FedAvg, achieving the ﬁdelity goal.
However, existing Byzantine-robust FL methods may have
higher or much higher testing error rates under no attacks. For
instance, on MNIST-0.1, the testing error rates for FedAvg
and FLTrust are both 0.04, while they are 0.10, 0.06, and
0.06 for Krum, Trim-mean, and Median, respectively; On CH-
MNIST, FedAvg, Trim-mean, and FLTrust achieve testing error
rates 0.10, while Krum and Median achieve testing error rates
0.24 and 0.11, respectively. Our results indicate that FLTrust is
more accurate than existing Byzantine-robust FL methods in
non-adversarial settings. This is because existing Byzantine-
robust FL methods exclude some local model updates when
aggregating them as the global model update, while FLTrust
considers all of them with the help of the root dataset.
Second, our FLTrust achieves the robustness goal, while
existing FL methods do not. Speciﬁcally, the testing error rates
of FLTrust under the untargeted attacks including our adaptive
attack are at most 0.04 higher than those of FedAvg under no
attacks on the six datasets. On the contrary, every existing
Byzantine-robust FL method has much higher testing error
rates, especially under the untargeted attack that is optimized
for the method. For instance, on MNIST-0.5, Krum attack in-
creases the testing error rate of Krum from 0.10 to 0.91, while
Trim attack increases the testing error rates of Trim-mean
and Median from 0.06 to 0.23 and 0.43, respectively. FedAvg
may have lower testing error rates than existing Byzantine-
robust FL methods under the evaluated untargeted attacks.
This is because these untargeted attacks are not optimized
TABLE III: The testing error rates of different FL methods
under different attacks and the attack success rates of the
Scaling attacks. The results for the Scaling attacks are in the
form of “testing error rate / attack success rate”.
(a) CNN global model, MNIST-0.1
FedAvg
Trim-mean Median
FLTrust
No attack
LF attack
Krum attack
Trim attack
Scaling attack 0.02 / 1.00 0.10 / 0.00 0.05 / 0.01 0.05 / 0.01 0.03 / 0.00
Adaptive attack
0.04
0.04
0.04
0.04
0.04
0.06
0.10
0.16
0.06
0.05
0.07
0.13
0.06
0.05
0.07
0.13
0.04
0.08
0.10
0.11
0.13
Krum