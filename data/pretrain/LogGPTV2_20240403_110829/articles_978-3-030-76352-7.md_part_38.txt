preferences. To describe the agents’ different preferences, we use one pair of
numerical value named PT [4], which is also called the orientation of the agents
[2]. After that, when calculating the probability of sharing conducted by the
agents, we adopt Game Theory [10] and Asymmetric Nash Bargaining Solution
(ANBS) [17] to derive mathmatical formulation.
The System Definition. We use a multi-agent robotic system imitating the
clouduserstogivesharingsuggestions.Eachagentiinthesystemhasthesame
resourcevalueRfixed atbeginningandhasaflexibleextraresourcequotaRe(t),
which denotes the resources it would get from other agents at time t, with the
upper boundary Remax restricting the greediness of each agent. The resource
usage set is denoted as U = {Ui|Ui ∈[0,Rfixed+Remax],i=1,2,3....n}, and
the real-time resource at time t holds Rri(t)=Rfixed+Rei(t).
When agents get a real-time resource usage information (e.g. the system
measures agents’ usage amount in a time interval), the agents start to evaluate
whether they have enough resources for their current usage. Then the EMP
model will guide the agents to adjust their resource allocation in the system.
Besides, the whole resource in the system holds conservation. For each agent i,
the spare resource is denoted as Rsi(t)=Rri(t)−Ui(t). According to the real-
timeresource,wedefinedallagentsinthreegroups:1)needyagents(Rsi(t)0); 3) self-sufficient agents(Rsi(t)=0).
244 B. Chen et al.
The Action and Payoff Definition. In every sharing step, we pick up two
agents: one rich agent and one needy agent. In principle both agents have two
action choices, giving out resource, denoted as a , or getting resources, denoted
0
as a , the action space can be defined as A = [a ,a ]T. The payoff matrix for
1 0 1
agents is designed in Table1. Nash Equilibrium point will be reached when the
rich agent selects ‘give’ and the needy agent selects ‘get’.
Table 1. Payoff for individual agent
Payoffforneedyagent Staterisk
Ma Mθ
givea geta highriskθ lowriskθ
0 1 0 1
Payoffof givea 0 0,−1 3,3 Payoffofagent −1 5
richagent geta 1 −5,−5 −1,0 forstate 4 −2
The Agent PT and State Definition. Our EMP model uses the Theory of
Evolutiontodefinetheagents’PT,whichmakestheagentsimitatingthehuman
emotions. These emotions act as an internal motivation to their behaviours.
In our case, the vector PT, β = [β ,β ]T (β ≥ 0,β ≥ 0), describes the
0 1 0 1
agents’ normalized characteristics, where the β represents ‘generous’ trait and
0
β represents ‘eager’ trait, and β +β =1.
1 0 1
Other than the PT, the resource availability state s is another factor to
influencetheagents’actionsthatwehavetotakeintoaccounttogetherwiththe
PT. The agent in safe resource state means an agent has enough resource for its
usage and more likely to share. Otherwise it is more likely to ask help from the
otheragents.WedescribethestateriskattimetbyPθi(t)=[Pθi0(t),Pθi1(t)]T,
wherePθi0(t)denotestheprobabilityoftheagentiinriskystateθ 0,andPθi1(t)
denotes the probability of the agent in safe state θ , derived by the following
1
Eq.(1):
Pθi0(t)= RU ri i( (t t) +(βi0(t)−βi1(t)); Pθi1(t)=1−Pθi0(t) (1)
)
Besides, we describe the value function for agents when they are in sharing step
as the following:
Vi(s,am)=E{Ji(s,am)}=Mθi·Pθ iT = M Mθ θ1 21 1, ,M Mθ θ1 22 · PP θθi i( (θ θ0 1) ·Ji(s,am)
)
2
(i,j =1,2....n i=j, m=0,1)
(2)
where, s=[θ ,θ ]T and
0 1
Ji(s,am)=Ma·PaT = M Ma a1 21 1, ,M Ma a1 22 · PP aaj j( (a a0 1)
j 2 ) (3)
(i,j =1,2....n i=j, m=0,1)
Resource Sharing in Public Cloud System 245
The payoff matrices Mθ and Ma are defined in Table1. Once the agent recog-
nizes its state situation and its value function, its action selection strategy uses
randomized strategy [5], which considers the agent is ‘exploring’ new action as
well as ‘exploiting’ learnt action. The probability of action selection holds:
kVi(s,a0)
Pai(a 0|s)= kVi(s,a0)+kVi(s,a1); Pai(a 0|s)+Pai(a 1|s)=1 (4)
where the coefficient k represents how often the agent would like to ‘explore’
rather than ‘exploit’. In our application, we set it to be e=2.718.
The Sharing Strategy. At each sharing step, assuming both agents are ratio-
nal and intend to maximize their spare resource utility in the bargain, EMP
model considers the two-agent cooperation as a bargain problem. The set of
spare resource utility function can be described as Γ = {γi|i=1,2}}, where
γi ={(Rri−Ui)|i=1,2},whichisanonemptycompactconvexsetwithbound-
ary([16],[7]).Foreachagent,statusquopointiswheretheirRsequals0.When
therealresourceequivalenttotheirusagewillgetthestatuspointforeachagent.
In this case the sharing problem can be described as [17]:
Γ∗ (Rri−Ui)λi,
=argmax
Rri
i
2
s.t. Rri =Φ
i=1
(5)
2
λi =1
i=1
Rri ≥Ui, i=1,2
Rri ≥0, i=1,2
βi1
λi = (i=1,2,3...n) (6)
βi1+βj1
Where Φ is the total real-time resource of those two agents. λi denotes the
bargaining power of agent. At each allocation step, the agent gets their real-
time resource as Rri =Ui+λi·Rs.
The Evolutionary Strategy. After the sharing, the real-time resource hold
by every agents involved in the sharing step has been changed, leading their PT
to be evolving. This will affect their action decisions in next sharing steps. We
define the updating rule as the following [12]:
βm(t)=βm(t−1)+αΔβm(t), (m=0,1), α∈(0,1) (7)
where, α is the learning rate and the Δβm(t) holds as :
ΔJm(s,am;t)
Δβm(t)= , (m,l=0,1 m=l) (8)
ΔJm(s,am;t)+ΔJl(s,al;t)
246 B. Chen et al.
where, ΔJi(s,ai;t)=Ji(s,ai;t)−Ji(s,ai;t−1),(i=m,l). Then we implement
the aforementioned EMP model by Algorithm 1 with two different sharing poli-
cies, i.e. EMP-A using ANBS, and EMP-F using fixed-value sharing strategy
during the sharing (i.e., all the agents share a fixed value of resources during a
step of sharing).
Algorithm 1. for EMP, including EMP-A and EMP-F
1: Initialisation:
2: Initialβ =[0.5,0.5]T,Pa=[0.5,0.5]T
3: Split agents into three groups according to Rs i(t)
4: Sharing step:
5: while there are needy agents in the system do
6: for pick one rich agent do
7: for pick one needy agent do
8: 1) evaluate state risk by equation (1)
9: 2) calculate expected value by equations (2-3)
10: 3) calculate Pa by equation (4)
11: 4) select and execute actions by Pa
12: 5) share spare resource:
13: if sharing by ANBS strategy(EMP-A) then
14: sharing value by equation (5)
15: if sharing by fixed value strategy(EMP-F) then
16: sharing with fixed value (e.g.5 units).
17: 6) update PT by equation (7-8)
18: 7) update the groups
19: if no needy agent in the system then
20: break
3.2 Other Models
The SEMP Model. We build an SEMP model to be one of the baseline
models to compare with our EMP model in Sect.3.1. Unlike the EMP model
that involves both agents in state evaluation before sharing and updates PT
aftersharing,theSEMPmodel,implementedasAlgorithm2,onlyconsidersthe
agents’ PT when making actions and neglects the evaluation of the state risk
probabilities. During the sharing steps, the SEMP algorithm also adopts the
ANBS strategy and updates the rich agent’s PT at the end of sharing. Without
considering the state risk probability, the PT updating functions are simplified
as:
eβi0
Pai(a 0)= +eβi1; βi(t)=βi(t−1)+α· Ma, α∈(0,1) (9)
eβi0
Resource Sharing in Public Cloud System 247
Algorithm 2. for SEMP
1: Initialisation:
2: Initialize β =[0.5,0.5]T,Pa=[0.5,0.5]T
3: Split agents into three groups according to Rs i(t)
4: Sharing step:
5: while there are needy agents in the system do
6: for pick one rich agent do
7: for pick one needy agent do
8: 1) calculate Pa by equation(9)
9: 2) select and execute actions by Pa
10: 3) share spare resource by ANBS strategy
11: 4) update PT by equation(9)
12: 5) update the groups:
13: if no needy agent in the system then
14: break
The NBSS Model. WebuildanNBSSmodelasanotherbaselinealgorithmto
compare with our EMP model and SEMP model. In the NBSS model, without
considering the state influence, we assume all agents have the same personality
andtheneedy-richagentspairssharetheirtotalspareresourcebyNBSasRri =
Ui+1/2·Rs.
4 Experimental Results
4.1 Experimental Setup
In this section, we present experimental results of our sharing models. We sim-
ulate a 100-agent system for sharing resources in a public cloud. To initiate our
experiment, we assume that each agent has fixed resource of 70 units at the
beginning, and the maximum extra resource it could get from the other agents
is 30 units. The real-time resource usage for each agent is a set of numbers in
the range of [0,100] units. We define one “resource usage measuring round” as
the time window between two consecutive instances of measuring on the agents’
resource usage, when the resource sharing between all agents should start and
finish.Duringeach“resourceusagemeasuringround”,eachagentconductsmul-
tiple“sharingsteps”withotheragentstoachievethegoalofeliminatingallneedy
agents. We investigate two use-case scenarios: “independent sharing” and “con-
tinuous sharing”. “Independent sharing” means the real-time resource usage in
each usage measuring round is randomly generated, and the remaining resource
from the previous round is not rolled over to the next round. However, “contin-
uous sharing” means the real-time resource usage follows a sinusoidal pattern
during time of the day and the remaining resource is rolled over to the next
round. We investigate the performance of our four aforementioned models: 1)
The EMP-A model; 2) The EMP-F model; 3) The SEMP model; 4) The NBSS
model.
248 B. Chen et al.
Fig.1.100-agentdistributionofindependentsharinginoneresourceusagemeasuring
roundbydifferentalgorithms(orangedotsshowtheagents’distributionbeforesharing;
bluedotsshowstheagents’distributionaftersharing):(a)EMP-A;(b)EMP-F.(Color
figure online)
4.2 Experimental Results for Different Models
Independent Sharing with Random Resource Usage Pattern. In this
subsection, we investigate the independent sharing case with usage pattern ran-
domly generated in the rage of [0,100]. Figure 1 shows the experimental results
for algorithms EMP-A and EMP-F in terms of the amount of spare resource
before and after sharing in one usage measuring round. Each dot on the figure
corresponds to one agent. The negative value of spare resource of an agent on x
axismeanstheagent’sreal-timeresourceisnotsufficientforitsusage,whilethe
positive value means the agent has some spare resource. The results show that
both algorithms help every agent in the system to get enough resource for their
current need as all the agents place at non-negative space on the right side of
the figures after sharing (the blue dots).
Figure 2(a) shows the process of reducing the number of the needy agents
during the sharing steps within one single resource usage measuring round for
the four algorithms. The fewer sharing steps it takes to reach 0, the better
the performance of the algorithm is. Figure 2(b) show the sharing steps the
algorithms take during 100 independent usage measuring rounds. Both figures
showthatthealgorithmEMP-Aperformancesmosteffectivelyasitusestheleast
sharing steps to make the number of needy agents to reach 0. The performance
of the other three algorithms, i.e., EMP-F, SEMP and NBSS are close to each
other.
Continuous Sharing with Sinusoidal Usage Pattern. In this section, we
consider continuous sharing in sinusoidal usage pattern (the resource usage fol-
lowsasinusoidalfunctionovertimeoftheday).Asmentionedbeforeinthiscase
the remaining resource in the previous round is rolled over to the next round.
Besides,weassumeeveryagenthasdifferentphaseofsinusoidaldailyusagepat-
tern, i.e., every agent reaches its peak usage value at different time of the day.
Resource Sharing in Public Cloud System 249
Fig.2.(a)Numberofneedyagentsduringoneindependentusagemeasuringroundvs.
sharing steps; (b) Number of Sharing steps in 100 independent rounds with random
usage pattern.
Fig.3. Number of sharing steps in 100 continuous resource usage measuring rounds
with sinusoidal usage pattern.
This setup emulate users from different time zone of the world that are sharing
the same pool of cloud computing resources.
In this case, the performances of the four algorithms are shown in Fig. 3.
Similar to the previous scenario with random usage, the EMP-A needs fewer
sharing steps than the other three algorithms to make the number of needy
agentsto0.Theperformanceoftheotherthreealgorithmsarealsoclosetoeach
other. However, in this continuous sharing case with sinusoidal usage pattern,
the advantage of EMP-A over the other three algorithms is more obvious than
the independent sharing case. It is reasonable because this continuous sharing
scenario is more stable and closer to the reality (Table2).
4.3 Average Percentage of Satisfaction Time of Users
Weassumethat,inapubliccloudsystem,thesystemgetsreal-timeusageinfor-
mation measured every 15min slot and schedule one sharing step every 1.5s,
i.e.,thesystemcanscheduleupto600sharingstepsin15min.Asshowninpre-
vious Fig.2 and 3, most of the time all 4 algorithms can make number of needy
usersto0(allusersreachingsatisfaction)within600sharingsteps.However,for
250 B. Chen et al.
Table 2. Average percentage of satisfaction time of users