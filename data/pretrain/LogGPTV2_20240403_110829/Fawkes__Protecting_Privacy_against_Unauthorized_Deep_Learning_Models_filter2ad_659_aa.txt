title:Fawkes: Protecting Privacy against Unauthorized Deep Learning Models
author:Shawn Shan and
Emily Wenger and
Jiayun Zhang and
Huiying Li and
Haitao Zheng and
Ben Y. Zhao
Fawkes: Protecting Privacy against Unauthorized 
Deep Learning Models
Shawn Shan, Emily Wenger, Jiayun Zhang, Huiying Li, Haitao Zheng, and 
Ben Y. Zhao, University of Chicago
https://www.usenix.org/conference/usenixsecurity20/presentation/shan
This paper is included in the Proceedings of the 29th USENIX Security Symposium.August 12–14, 2020978-1-939133-17-5Open access to the Proceedings of the 29th USENIX Security Symposium is sponsored by USENIX.Fawkes: Protecting Privacy against Unauthorized Deep Learning Models
Shawn Shan†, Emily Wenger†, Jiayun Zhang, Huiying Li, Haitao Zheng, Ben Y. Zhao
† denotes co-ﬁrst authors with equal contribution
{shansixiong, ewillson, jiayunz, huiyingli, htzheng, ravenben}@cs.uchicago.edu
Computer Science, University of Chicago
Abstract
Today’s proliferation of powerful facial recognition sys-
tems poses a real threat to personal privacy. As Clearview.ai
demonstrated, anyone can canvas the Internet for data and
train highly accurate facial recognition models of individu-
als without their knowledge. We need tools to protect our-
selves from potential misuses of unauthorized facial recog-
nition systems. Unfortunately, no practical or effective solu-
tions exist.
In this paper, we propose Fawkes, a system that helps
individuals inoculate their images against unauthorized fa-
cial recognition models. Fawkes achieves this by helping
users add imperceptible pixel-level changes (we call them
“cloaks”) to their own photos before releasing them. When
used to train facial recognition models, these “cloaked” im-
ages produce functional models that consistently cause nor-
mal images of the user to be misidentiﬁed. We experimen-
tally demonstrate that Fawkes provides 95+% protection
against user recognition regardless of how trackers train their
models. Even when clean, uncloaked images are “leaked” to
the tracker and used for training, Fawkes can still maintain
an 80+% protection success rate. We achieve 100% success
in experiments against today’s state-of-the-art facial recogni-
tion services. Finally, we show that Fawkes is robust against
a variety of countermeasures that try to detect or disrupt im-
age cloaks.
1 Introduction
Today’s proliferation of powerful facial recognition models
poses a real threat to personal privacy. Facial recognition sys-
tems are scanning millions of citizens in both the UK and
China without explicit consent [33, 41]. By next year, 100%
of international travelers will be required to submit to fa-
cial recognition systems in top-20 US airports [38]. Perhaps
more importantly, anyone with moderate resources can now
canvas the Internet and build highly accurate facial recogni-
tion models of us without our knowledge or awareness, e.g.
MegaFace [21]. Kashmir Hill from the New York Times re-
cently reported on Clearview.ai, a private company that col-
lected more than 3 billion online photos and trained a mas-
sive model capable of recognizing millions of citizens, all
without knowledge or consent [20].
Opportunities for misuse of this technology are numerous
and potentially disastrous. Anywhere we go, we can be iden-
tiﬁed at any time through street cameras, video doorbells, se-
curity cameras, and personal cellphones. Stalkers can ﬁnd
out our identity and social media proﬁles with a single snap-
shot [47]. Stores can associate our precise in-store shopping
behavior with online ads and browsing proﬁles [31]. Identity
thieves can easily identify (and perhaps gain access to) our
personal accounts [13].
We believe that private citizens need tools to protect them-
selves from being identiﬁed by unauthorized facial recogni-
tion models. Unfortunately, previous work in this space is
sparse and limited in both practicality and efﬁcacy. Some
have proposed distorting images to make them unrecogniz-
able and thus avoiding facial recognition [27, 52, 64]. Oth-
ers produce adversarial patches in the form of bright patterns
printed on sweatshirts or signs, which prevent facial recogni-
tion algorithms from even registering their wearer as a per-
son [55, 65]. Finally, given access to an image classiﬁcation
model, “clean-label poison attacks” can cause the model to
misidentify a single, preselected image [42, 71].
Instead, we propose Fawkes, a system that helps individ-
uals to inoculate their images against unauthorized facial
recognition models at any time without signiﬁcantly dis-
torting their own photos, or wearing conspicuous patches.
Fawkes achieves this by helping users adding imperceptible
pixel-level changes (“cloaks”) to their own photos. For ex-
ample, a user who wants to share content (e.g. photos) on
social media or the public web can add small, imperceptible
alterations to their photos before uploading them. If collected
by a third-party “tracker” and used to train a facial recog-
nition model to recognize the user, these “cloaked” images
would produce functional models that consistently misiden-
tify them.
Our distortion or “cloaking” algorithm takes the user’s
photos and computes minimal perturbations that shift them
signiﬁcantly in the feature space of a facial recognition
model (using real or synthetic images of a third party as a
landmark). Any facial recognition model trained using these
images of the user learns an altered set of “features” of what
makes them look like them. When presented with a clean, un-
cloaked image of the user, e.g. photos from a camera phone
or streetlight camera, the model ﬁnds no labels associated
USENIX Association
29th USENIX Security Symposium    1589
with the user in the feature space near the image, and classi-
ﬁes the photo to another label (identity) nearby in the feature
space.
Our exploration of Fawkes produces several key ﬁndings:
• We can produce signiﬁcant alterations to images’ feature
space representations using perturbations imperceptible to
the naked eye (DSSIM ≤ 0.007).
• Regardless of how the tracker trains its model (via transfer
learning or from scratch), image cloaking provides 95+%
protection against user recognition (adversarial training
techniques help ensure cloaks transfer to tracker models).
• Experiments show 100% success against state-of-the-art
facial recognition services from Microsoft (Azure Face
API), Amazon (Rekognition), and Face++. We ﬁrst “share”
our own (cloaked) photos as training data to each service,
then apply the resulting models to uncloaked test images
of the same person.
• In challenging scenarios where clean, uncloaked images
are “leaked” to the tracker and used for training, we show
how a single Sybil identity can boost privacy protection.
This results in 80+% success in avoiding identiﬁcation
even when half of the training images are uncloaked.
• Finally, we consider a tracker who is aware of our image
cloaking techniques and evaluate the efﬁcacy of potential
countermeasures. We show that image cloaks are robust
(maintain high protection rates against) to a variety of
mechanisms for both cloak disruption and cloak detection.
2 Background and Related Work
To protect user privacy, our image cloaking techniques lever-
age and extend work broadly deﬁned as poisoning attacks
in machine learning. Here, we set the context by discussing
prior efforts to help users evade facial recognition models.
We then discuss relevant data poisoning attacks, followed
by related work on privacy-preserving machine learning and
techniques to train facial recognition models.
Note that to protect user privacy from unauthorized deep
learning models, we employ attacks against ML models. In
this scenario, users are the “attackers,” and third-party track-
ers running unauthorized tracking are the “targets.”
2.1 Protecting Privacy via Evasion Attacks
Privacy advocates have considered the problem of protect-
ing individuals from facial recognition systems, generally
by making images difﬁcult for a facial recognition model to
recognize. Some rely on creating adversarial examples, in-
puts to the model designed to cause misclassiﬁcation [54].
These attacks have since been proven possible “in the wild,”
Sharif et al. [44] create specially printed glasses that cause
the wearer to be misidentiﬁed. Komkov and Petiushko [24]
showed that carefully computed adversarial stickers on a hat
can reduce its wearer’s likelihood of being recognized. Oth-
ers propose “adversarial patches” that target “person identi-
ﬁcation” models, making it difﬁcult for models to recognize
the wearer as a person in an image [55, 65].
All of these approaches share two limitations. First, they
require the user to wear fairly obvious and conspicuous ac-
cessories (hats, glasses, sweaters) that are impractical for nor-
mal use. Second, in order to evade tracking, they require full
and unrestricted access (white box access) to the precise
model tracking them. Thus they are easily broken (and user
privacy compromised) by any tracker that updates its model.
Another line of work seeks to edit facial images so that
human-like characteristics are preserved but facial recogni-
tion model accuracy is signiﬁcantly reduced. Methods used
include k-means facial averaging [35], facial inpainting [51],
and GAN-based face editing [27,52,64]. Since these dramat-
ically alter the user’s face in her photos, we consider them
impractical for protecting shared content.
2.2 Protecting Privacy via Poisoning Attacks
An alternative to evading models is to disrupt their training.
This approach leverages “data poisoning attacks” against
deep learning models. These attacks affect deep learning
models by modifying the initial data used to train them, usu-
ally by adding a set of samples S and associated labels LS.
Previous work has used data poisoning to induce unexpected
behaviors in trained DNNs [66]. In this section, we discuss
two data poisoning attacks related to our work, and identify
their key limitations when used to protect user privacy.
Clean Label Attacks. A clean-label poisoning attack in-
jects “correctly” labeled poison images into training data,
causing a model trained on this data to misclassify a speciﬁc
image of interest [42, 71]. What distinguishes clean-label at-
tacks from normal poisoning attacks is that all image labels
remain unchanged during the poisoning process – only the
content of the poisoned images changes.
Our work (Fawkes) works with similar constraints. Our ac-
tion to affect or disrupt a model is limited to altering a group
of images with a correct label, i.e. a user can alter her images
but cannot claim these are images of someone else.
Current clean label attacks cannot address the privacy
problem because of three factors. First, they only cause mis-
classiﬁcation on a single, preselected image, whereas user
privacy protection requires the misclassiﬁcation of any cur-
rent or future image of the protected user (i.e. an entire model
class). Second, clean label attacks do not transfer well to dif-
ferent models, especially models trained from scratch. Even
between models trained on the same data, the attack only
transfers with 30% success rate [71]. Third, clean label at-
tacks are easily detectable through anomaly detection in the
feature space [19].
Model Corruption Attacks. Other recent work proposes
1590    29th USENIX Security Symposium
USENIX Association
Original
User
Fawkes
Images from Target T
Feature Extractor Φ:
Cloaked
Training Data
(cloaked)
Web Crawl
Model
Training
Tracker / Model Trainer
Testing Data
(uncloaked)
Wrong
Label
Figure 1: Our proposed Fawkes system that protects user privacy by cloaking their online photos. (Left) A user U applies
cloaking algorithm (given a feature extractor Φ and images from some target T ) to generate cloaked versions of U’s photos,
each with a small perturbation unnoticeable to the human eye. (Right) A tracker crawls the cloaked images from online sources,
and uses them to train an (unauthorized) model to recognize and track U. When it comes to classifying new (uncloaked) images
of U, the tracker’s model misclassiﬁes them to someone not U. Note that T does not have to exist in the tracker’s model.
techniques to modify images such that they degrade the ac-
curacy of a model trained on them [45]. The goal is to spread
these poisoned images in order to discourage unauthorized
data collection and model training. We note that Fawkes’
goals are to mislead rather than frustrate. Simply corrupting
data of a user’s class may inadvertently inform the tracker of
the user’s evasion attempts and lead to more advanced coun-
termeasures by the tracker. Finally, [45] only has a 50% suc-
cess rate in protecting a user from being recognized.
ample, a facial recognition model trained on faces extracted
from YouTube videos might serve well as a feature extractor
for a model designed to recognize celebrities in magazines.
Finally, the concept of protecting individual privacy
against invasive technologies extends beyond the image do-
main. Recent work [12] proposes wearable devices that re-
store personal agency using digital jammers to prevent audio
eavesdropping by ubiquitous digital home assistants.
2.3 Other Related Work
Privacy-Preserving Machine Learning. Recent work has
shown that ML models can memorize (and subsequently
leak) parts of their training data [48]. This can be exploited
to expose private details about members of the training
dataset [17]. These attacks have spurred a push towards dif-
ferentially private model training [6], which uses techniques
from the ﬁeld of differential privacy [15] to protect sensi-
tive characteristics of training data. We note these techniques
imply a trusted model trainer and are ineffective against an
unauthorized model trainer.
Feature Extractors & Transfer Learning. Transfer learn-
ing uses existing pretrained models as a basis for quickly
training models for customized classiﬁcation tasks, using
less training data. Today, it is commonly used to deploy com-
plex ML models (e.g. facial recognition or image segmenta-
tion [70]) at reasonable training costs.
In transfer learning, the knowledge of a pre-trained fea-
ture extractor Φ is passed on to a new model Fθ. Typically,
a model Fθ can be created by appending a few additional
layers to Φ and only training those new layers. The origi-
nal layers that composed Φ will remain unmodiﬁed. As such,
pre-existing knowledge “learned” by Φ is passed on to the
model Fθ and directly inﬂuences its classiﬁcation outcomes.
Finally, transfer learning is most effective when the feature
extractor and model are trained on similar datasets. For ex-
3 Protecting Privacy via Cloaking
We propose Fawkes, a system designed to help protect the pri-
vacy of a user against unauthorized facial recognition models
trained by a third-party tracker on the user’s images. Fawkes
achieves this by adding subtle perturbations (“cloaks”) to the
user’s images before sharing them. Facial recognition mod-
els trained on cloaked images will have a distorted view of
the user in the “feature space,” i.e. the model’s internal un-
derstanding of what makes the user unique. Thus the models
cannot recognize real (uncloaked) images of the user, and in-
stead, misclassify them as someone else.
In this section, we ﬁrst describe the threat model and as-
sumptions for both users and trackers. We then present the
intuition behind cloaking and our methodology to generate
cloaks. Finally, we discuss why cloaking by individuals is
effective against unauthorized facial recognition models.
3.1 Assumptions and Threat Model
User. The user’s goal is to share their photos online without
unknowingly helping third party trackers build facial recog-
nition models that can recognize them. Users protect them-
selves by adding imperceptible perturbations (“cloaks”) to
their photos before sharing them. This is illustrated in the
left part of Figure 1, where a cloak is added to this user’s
photos before they are uploaded.
The design goals for these cloaks are:
USENIX Association
29th USENIX Security Symposium    1591
• cloaks should be imperceptible and not impact normal
use of the image;
• when classifying normal, uncloaked images, models
trained on cloaked images should recognize the underly-
ing person with low accuracy.
We assume the user has access to moderate computing re-
sources (e.g., a personal laptop) and applies cloaking to their
own images locally. We also assume the user has access
to some feature extractor, e.g. a generic facial recognition
model, represented as Φ in Figure 1. Cloaking is simpliﬁed
if the user has the same Φ as the tracker. We begin with this
common assumption (also used by prior work [42, 59, 71]),
since only a few large-scale face recognition models are
available in the wild. Later in §3.4, we relax this assumption
and show how our design maintains the above properties.
We initially consider the case where the user has the abil-
ity to apply cloaking to all their photos to be shared, thus the
tracker can only collect cloaked photos of the user. Later in
§7, we explore a scenario where a stronger tracker has ob-
tained access to some number of their uncloaked images.
Tracker/Model Trainer. We assume that the tracker (the
entity training unauthorized models) is a third party without
direct access to user’s personal photos (i.e. not Facebook or
Flickr). The tracker could be a company like Clearview.ai, a
government entity, or even an individual. The tracker has sig-
niﬁcant computational resources. They can either use trans-
fer learning to simplify their model training process (lever-
aging existing feature extractors), or train their model com-
pletely from scratch.
We also assume the tracker’s primary goal is to build a
powerful model to track many users rather than targeting a
single speciﬁc person1. The tracker’s primary data source is
a collection of public images of users obtained via web scrap-
ing. We also consider scenarios where they are able to obtain
some number of uncloaked images from other sources (§7).
Privacy beneﬁts of Fawkes rely
Real World Limitations.
on users applying our cloaking technique to the majority of
images of their likeness before posting online. In practice,
however, users are unlikely to control all images of them-
selves, such as photos shared online by friends and family,
media, employer or government websites. While it is unclear
how easy or challenging it will be for trackers to associate
these images with the identity of the user, a tracker who ob-
tains a large number of uncloaked images of the user can
compromise the effectiveness of Fawkes.
Therefore, Fawkes is most effective when used in conjunc-
tion with other privacy-enhancing steps that minimize the on-
line availability of a user’s uncloaked images. For example,
users can curate their social media presence and remove tags
of their names applied to group photos on Facebook or Insta-
gram. Users can also leverage privacy laws such as “Right
1Tracking a speciﬁc person can be easily accomplished through easier,
ofﬂine methods, e.g. a private investigator who follows the target user, and
is beyond the scope of our work.
to be Forgotten” to remove and untag online content related
to themselves. The online curation of personal images is a
challenging problem, and we leave the study of minimizing
online image footprints to future work.
3.2 Overview and Intuition
DNN models are trained to identify and extract (often hid-
den) features in input data and use them to perform classiﬁ-
cation. Yet their ability to identify features is easily disrupted
by data poisoning attacks during model training, where small
perturbations on training data with a particular label (l) can
shift the model’s view of what features uniquely identify
l [42, 71]. Our work leverages this property to cause misclas-
siﬁcation of any existing or future image of a single class,
providing one solution to the challenging problem of protect-
ing personal privacy against the unchecked spread of facial
recognition models.