C
Overall
Random
Best (IN)
Worst (AR)
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
Rank
Figure 7: Confusion Matrix for 21-way test using tri-
grams. Darkest and lightest boxes represent accuracies
of 0.4 and 0.0, respectively.
Figure 8: CDF showing how often the speaker’s lan-
guage was among the classiﬁer’s top x choices.
cation rate was 75.1%.
Our initial intuition in Section 2 are strongly correlated
to our empirical results. For example, rates for Russian
versus Italian and Mandarin versus Tamil (see Figure 5)
were 78.5% and 84%, respectively. The differences in
the histograms shown earlier (Figure 2) also have direct
implications for our classiﬁcation rates in this case. For
instance, our classiﬁer’s accuracy when tasked with dis-
tinguishing between Brazilian Portuguese and English
was only 66.5%, whereas the accuracy for English versus
Hungarian was 86%.
4.2 Reducing Dimensionality to Improve
Performance
Although these results adequately demonstrate that
length-preserving encryption leaks information in VoIP,
there are limiting factors to the aforementioned approach
that hinder classiﬁcation accuracy. The primary difﬁ-
culty arises from the fact that the classiﬁer represents
each speaker and language as a probability distribution
over a very high dimensional space. Given 9 different
observed packet lengths, there are 729 possible different
trigrams. Of these possibilities, there are 451 trigrams
that are useful for classiﬁcation, i.e., DIS(g) > 1 (see
Section 3). Thus, speaker and language models are prob-
ability distributions over a 451-dimensional space. Un-
fortunately, given our current data set of approximately
7,277 trigrams per speaker, it is difﬁcult to estimate den-
sities over such a large space with high precision.
One way to address this problem is based on the ob-
servation that some bit rates are used in similar ways
by the Speex encoder. For example, the two lowest bit
rates, which result in packets of 41 and 46 bytes, re-
spectively, are often used to encode periods of silence
or non-speech. Therefore, we can reasonably consider
the two smallest packet sizes functionally equivalent and
put them together into a single group. In the same way,
other packet sizes may be used similarly enough to war-
rant grouping them together as well. We experimented
with several mappings of packet sizes to groups, but
found that the strongest results are obtained by mapping
the two smallest packet lengths together, mapping all of
the mid-range packet lengths together, and leaving the
largest packet size in a group by itself.
We assign each group a speciﬁc symbol, s, and then
compute n-grams from these symbols instead of the orig-
inal packet sizes. So, for example, given the sequence of
packet lengths 41, 50, 46, and 55, we map 41 and 46 to
s1 and 50 and 55 to s2 to extract the 3-grams (s1, s2, s1)
and (s2, s1, s2), etc. Our classiﬁcation process then con-
tinues as before, except that the reduction in the num-
ber of symbols allows us to expand our analysis to 4-
grams. After removing the 4-grams g with DIS(g) < 1,
we are left with 47 different 4-gram combinations. Thus,
we reduced the dimensionality of the points from 451
to 47. Here we are estimating distributions over a 47-
dimensional space using on average of 7,258 4-grams per
speaker.
Results for this classiﬁer are shown in Figures 9 and
10. With these improvements, the 21-way classiﬁer cor-
rectly identiﬁes the language spoken 66% of the time—
a fourfold improvement over our original classiﬁer and
more than 13 times better than random guessing. It rec-
ognizes 14 of the 21 languages exceptionally well, iden-
tifying them with over 90% accuracy. At the same time,
there is a small group of languages which the new clas-
siﬁer is not able to identify reliably; Czech, Spanish, and
Vietnamese are never identiﬁed correctly on the first try.
This occurs mainly because the languages which are not
USENIX Association
16th USENIX Security Symposium
49
Confusion Matrix: All Languages
e
g
a
u
g
n
a
L
l
e
d
o
M
VI
TA
SW
SP
SD
RU
PO
MA
KO
JA
IT
IN
HU
HI
GE
FA
EN
CZ
CA
BP
AR
AR
BP
CA
ENCZ
FA
GE
INHUHI
IT
KOJA
User Language
POMA
RU
SD
SWSP
TA
VI
21 Way Classification Accuracy
1
0.8
0.6
0.4
0.2
0
F
D
C
Overall
Random
Best (HI)
Worst (CZ)
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21
Rank
Figure 9: Confusion Matrix for the 21-way test using 4-
grams and reduced set of symbols. Darkest and lightest
boxes represent accuracies of 1.0 and 0.0, respectively.
Figure 10: CDF showing how often the speaker’s lan-
guage was among the classiﬁer’s top x choices using 4-
grams and reduced set of symbols.
recognized accurately are often misidentiﬁed as one of
a handful of other languages. Hungarian, in particular,
has false positives on speakers of Arabic, Czech, Span-
ish, Swahili, Tamil, and Vietnamese. These same lan-
guages are also less frequently misidentiﬁed as Brazilian
Portuguese, Hindi, Japanese, Korean, or Mandarin. In
future work, we plan to investigate what speciﬁc acous-
tic features of language cause this classiﬁer to perform so
well on many of the languages while failing to accurately
recognize others.
Binary classiﬁcation rates, shown in Figure 11 and
Table 1, are similarly improved over our initial results.
Overall, the classiﬁer achieves over 86% accuracy when
distinguishing between two languages. The median ac-
curacy is 92.7% and 12% of the language pairs can be
distinguished at rates greater than 98%. In a few cases
like Portuguese versus Korean or Farsi versus Polish, the
classiﬁer exhibited 100% accuracy on our test data.
Interestingly, the results of our classiﬁers are compara-
ble to those presented by Zissman [38] in an early study
of language identiﬁcation techniques using full acous-
tic data. Zissman implemented and compared four dif-
ferent language recognition techniques, including Gaus-
sian mixture model (GMM) classiﬁcation and techniques
based on single-language phone recognition and n-gram
language modeling. All four techniques used cepstral co-
efﬁcients as input [22].
The GMM classiﬁer described by Zissman is much
simpler than the other techniques and serves primarily
as a baseline for comparing the performance of the more
sophisticated methods presented in that work. Its accu-
racy is quite close to that of our initial classiﬁer: with
access to approximately 10 seconds of raw acoustic data,
it scored approximately 78% for three language pairs,
compared to our classifer’s 89%. The more sophisti-
cated classiﬁers in [38] have performance closer to that
of our improved classiﬁer. In particular, an 11-way clas-
siﬁer based on phoneme recognition and n-gram lan-
guage modeling (PRLM) was shown to achieve 89% ac-
curacy when given 45s of acoustic data. In each case,
our classiﬁer has the advantage of a larger sample, using
around 2 minutes of data.
Naturally, current techniques for language identiﬁ-
cation have improved on the earlier work of Zissman
and others, and modern error rates are almost an order
of magnitude better than what our classiﬁers achieve.
Nevertheless, this comparison serves to demonstrate the
point that we are able to extract signiﬁcant information
from encrypted VoIP packets, and are able to do so with
an accuracy close to a reasonable classiﬁer with access
to acoustic data.
DISCUSSION
We note that since the audio files in our corpus were
recorded over a standard telephone line, they are sampled
at 8kHz and encoded as 16-bit PCM audio, which is ap-
propriate for Speex narrowband mode. While almost all
traditional telephony samples the source audio at 8kHz,
many soft phones and VoIP codecs have the ability to use
higher sampling rates such as 16kHz or 32kHz to achieve
better audio quality at the tradeoff of greater load on the
network. Unfortunately, without a higher-fidelity data
set, we have been unable to evaluate our techniques on
VoIP calls made with these higher sampling rates. Nev-
ertheless, we feel that the results we derive from using
the current training set are also informative for higher-
bandwidth codecs for two reasons.
First, it is not uncommon for regular phone conver-
sations to be converted to VoIP, enforcing the use of an
50
16th USENIX Security Symposium
USENIX Association
1
0.8
0.6
0.4
0.2
F
D
C
C
0
0
Binary Classification Accuracy
Classifier
Random
0.2
0.4
0.6
0.8
1
Binary Classification Rate
Acc.
Acc
Lang.
Lang.
EN-FA 0.980 CZ-JA
0.544
GE-RU 0.985 AR-SW 0.549
FA-SD 0.990 CZ-HU 0.554
0.554
IN-PO
0.990 CZ-SD
PO-RU 0.990 MA-VI
0.565
JA-SW 0.566
BP-PO 0.995
EN-HI
0.575
0.995 HU-VI
HI-PO
0.995 CZ-MA 0.580
BP-KO 1.000 CZ-SW 0.590
FA-PO 1.000 HU-TA 0.605
Figure 11: CCDF for overall accuracy of the binary clas-
siﬁer using 4-grams and reduced set of symbols.
Table 1: Binary classifier recognition rates for selected
language pairs. Languages and their abbreviations are
listed in Appendix A.
8kHz sampling rate. Our test setup accurately models the
trafﬁc produced under this scenario. Second, and more
importantly, by operating at the 8kHz level, we argue
that we work with less information about the underly-
ing speech, as we are only able to estimate bit rates up
to a limited ﬁdelity. Speex wideband mode, for example,
operates on speech sampled at 16kHz and in VBR mode
uses a wider range of bit rates than does the narrowband
mode. With access to more distinct bit rates, one would
expect to be able to extract more intricate characteristics
about the underlying speech. In that regard, we believe
that our results could be further improved given access to
higher-fidelity samples.
4.3 Mitigation
Recall that these results are possible because the default
mode of encryption in SRTP is to use a length-preserving
stream cipher. However, the ofﬁcial standard [4] does al-
low implementations to optionally pad the plaintext pay-
load to the next multiple of the cipher’s block size, so that
the original payload size is obscured. Therefore, we in-
vestigate the effectiveness of padding against our attack,
using several block sizes.
To determine the packet sizes that would be pro-
duced by encryption with padding, we simply modify
the packet sizes we observed in our network traces by
increasing their RTP payload sizes to the next multiple
of the cipher’s block size. To see how our attack is af-
fected by this padding, we re-ran our experiments us-
ing block sizes of 128, 192, 256, and 512 bits. Padding
to a block size of 128 bits results in 4 distinct packet
sizes; this number decreases to 3 distinct sizes with 192-
bit blocks, 2 sizes with 256-bit blocks, and ﬁnally, with
512-bit blocks, all packets are the same size. Figure 12
shows the CDF for the classiﬁer’s results for these four
cases, compared to random guessing and to the results
we achieve when there is no padding.
Padding to 128-bit blocks is largely ineffective be-
cause there is still sufﬁcient granularity in the packet
sizes that we can map them to basically to the same three
bins used by our improved classiﬁer in Section 4.2. Even
with 192- or 256-bit blocks, where dimensionality reduc-
tion does not offer substantial improvement, the correct
language can be identiﬁed on the first guess over 27% of
the time—more than 5 times better than random guess-
ing.
It is apparent from these results that, for encryp-
tion with padding to be an effective defense against this
type of information leakage, the block size must be large
enough that all encrypted packets are the same size.
Relying on the cryptographic layer to protect against
both eavesdropping and trafﬁc analysis has a certain
philosophical appeal because then the compression layer
does not have to be concerned with security issues. On
the other hand, padding incurs signiﬁcant overhead in the
number of bytes that must be transmitted. Table 2 lists
the increase in trafﬁc volume that arises from padding to
each block size, as well as the improvement of the overall
accuracy of the classiﬁer over random guessing.
Another solution for ensuring that there is no infor-
mation leakage is to use a constant bit rate codec, such
as Speex in CBR mode, to send packets of fixed length.
Forcing the encoder to use a fixed number of bits is an
attractive approach, as the encoder could use the bits that
would otherwise be used as padding to improve the qual-
ity of the encoded sound. While both of these approaches
would detract from the bandwidth savings provided by
VBR encoders, they provide much stronger privacy guar-
antees for the participants of a VoIP call.
USENIX Association
16th USENIX Security Symposium
51
Block Size Overhead Accuracy
66.0%
62.5%
27.1%
27.2%
6.9%
none
128 bits
192 bits
256 bits
512 bits
0.0%
8.7%
13.8%
23.9%
42.2%