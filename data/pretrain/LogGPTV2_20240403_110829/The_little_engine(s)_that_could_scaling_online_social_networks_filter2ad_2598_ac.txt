highly connected nodes, with potentially many replicas to
be moved due to local data semantics, get to ﬁrst choose the
server they go to. The remaining nodes are placed wher-
ever they ﬁt, following simple water-ﬁlling strategy. As we
will see in the evaluation section, this strategy ensures equal
repartition of the failed masters while maintaining a small
replication overhead.
3795. MEASUREMENT DRIVEN EVALUATION
In this section, we evaluate the performance of SPAR in
terms of replication overhead and replica movements (mi-
grations).
5.1 Evaluation methodology
5.1.1 Metrics
Our main quantitative evaluation metric is the replication
overhead ro that stands for the number of slave replicas
that need to be created to guarantee local semantics while
observing the K-redundancy condition. We also analyze the
number of node movements, i.e. replica migrations between
servers during the operation of SPAR online.
5.1.2 Datasets
We use three diﬀerent datasets, each serving a diﬀerent
purpose to evaluate individual performance metrics.
Twitter: We collected a dataset by crawling Twitter be-
tween Nov 25 - Dec 4, 2008. It comprises 2, 408, 534 nodes
and 48, 776, 888 edges.
It also contains 12M tweets gen-
erated by the 2.4M users during the time of the collection.
Although there exists larger datasets [21], they are topic spe-
ciﬁc and do not contain all the tweets of individual users.
To the best of our knowledge, this is the largest dataset with
complete user activity. This data allows us to have a good
approximation to what Twitter was as of Dec. 2008.
Facebook: We used a public dataset of the New Orleans
Facebook network [34]. It includes nodes, friendship links,
as well as wall posts and was collected between Dec 2008 and
Jan 2009. The data consists of 60,290 nodes and 1,545,686
edges. This dataset includes edge creation timestamps be-
tween users by using the ﬁrst wall post. This information,
however, is not available for all users. Therefore, we ﬁltered
the dataset and retained a sub-network containing complete
information for 59,297 nodes and 477,993 edges.
Orkut: Collected between Oct 3 and Nov 11 2006, this
dataset consists of 3, 072, 441 nodes and 223, 534, 301 edges.
Further information about this dataset can be found in [25]
and is the largest of the three datasets.
5.1.3 Algorithms for Comparison
We compare SPAR against the following partitioning al-
gorithms:
Random Partitioning: Key-Value stores like Cassandra,
MongoDB, SimpleDB, etc. partition data randomly across
servers. Random partition is the de-facto standard used in
most commercial systems [30].
Graph Partitioning: There exist several oﬄine algorithms
for partitioning a graph into a ﬁxed number of equal sized
partitions in such a way that the number of inter-partition
edges are minimized [10, 19]. We use METIS [19], which is
known to be very fast and yields high quality partitions for
large social graphs [23].
Modularity Optimization (MO+) Algorithms: We
also consider an oﬄine community detection algorithm [12]
built around the modularity metric [27, 26]. We modiﬁed it
in order to be able to create a ﬁxed number of equal sized
partitions [28]. Our modiﬁed version, called MO+, operates
by grouping the communities in partition sequentially until
a given partition is full. If a community is larger than the
predeﬁned size, we recursively apply MO [12] to the com-
munity.
The evaluation procedure is as follows:
5.1.4 Computation of results
• The input consists of a graph (one of the datasets described
earlier), the number of desired partitions M and the desired
minimum number of replicas per user’s proﬁle K.
• The partitions are produced by executing each of the al-
gorithms on the input.
• Since we require local semantics, we process the partitions
obtained by the candidate algorithms in the oﬄine algo-
rithms case. We then add replicas when the master of a
user is missing some of its neighbors in the same partition.
• For Facebook, we generate the edge creation trace using
the exact timestamps. For Twitter and Orkut, the times-
tamps are not available. So, we create random permuta-
tions of the edges in order to have an ordered edge creation
trace.
In the case of the exact timestamp and a random
permutation of the edge give the same qualitative results,
therefore, we can assume that the same applies for Orkut
and Twiter. Furthermore, there is virtually no quantita-
tive diﬀerence across multiple random permutations for any
dataset.
5.2 Evaluation of replication overhead
Fig. 4 summarizes the replication overhead of the diﬀerent
algorithms over the diﬀerent datasets for K = 0 and 2 as well
as for diﬀerent numbers of servers, from 4 up to 512. We see
that SPAR online generates much smaller replication over-
head than all the other algorithms, including traditional of-
ﬂine graph partitioning (METIS) and community detection
algorithms (MO+). The relative ranking of algorithms from
best to worse is, SPAR, MO+, METIS, and Random. Look-
ing at the absolute value of the replication overhead, we see
that it increases sub-linearly with the number of servers –
note that the x-axis is logarithmic. This means that adding
more servers does not lead to a proportional increase in per-
server resource requirement (as given by ro).
SPAR naturally achieves a low replication overhead since
this is the optimization objective. The competing algo-
rithms optimize for minimal inter-server edges. Therefore,
they end up needing more replicas to add the missing nodes
for guaranteeing local semantics in each partition in the
post-processing step.
Low-Cost Redundancy: An important property of SPAR
is that it achieves local semantics at a discounted cost by
reusing replicas that are needed for redundancy anyway (or
the other way around). To see this, lets focus on the example
of 32 servers. If no fault tolerance is guaranteed (K = 0),
then the replication overhead of SPAR to ensure local se-
mantics is 1.72.
If we require its proﬁle to be replicated
at least K = 2 times, then the new replication overhead
is 2.76 (rather than 3.72). Out of the 2.76 copies, 2 are
inevitable due to the redundancy requirement. Thus, the
local semantics were achieved at a lower cost 2.76-2=0.76
instead of 1.72. The gain comes from leveraging the redun-
dant replicas to achieve locality, something only SPAR does
explicitly.
We now focus our comparison on Random vs SPAR, since
Random is the de-facto standard. In Fig. 5, we depict the
ratio between the overhead of Random and that of SPAR. In
the case of the Twitter dataset, we see improvements varying
from 12% for a partition of 4 servers – it is low because we
have K = 2, which means 3 replicas of each node to be
380d
a
e
h
r
e
v
o
n
o
i
t
a
c
i
l
p
e
r
d
a
e
h
r
e
v
o
n
o
i
t
a
c
i
l
p
e
r
Twitter (K=0), number of servers
Orkut (K=0), number of servers
Facebook (K=0), number of servers
15
10
5
0
15
10
5
0
8 16 32 64 128 256 512
4
Twitter (K=2), number of servers
60
40
20
0
60
40
20
0
8 16 32 64 128 256 512
4
Orkut (K=2), number of servers
15
10
5
0
15
10
5
0
8 16 32 64 128 256 512
4
Facebook (K=2), number of servers
SPAR
MO+
METIS
Random
Figure 4: Replication overhead (ro) for SPAR, METIS, MO+ and Random for Twitter, Orkut and Facebook. The upper
graph show the results for K = 0, the lower graph shows the results for K = 2, K is the number of replicas for redundancy.
o
R
A
P
S
r
/
M
O
D
N
A
R
o
r
5
4
3
2
1
Twitter
Orkut
Facebook
4
8
16
32
64
number of servers
128
256
512
e
g
d
e
n
o
p
u
s
n
o
i
t
c
a
f
o
o
i
t
a
r
1
0.8
0.6
0.4
0.2
0
0.5
1
1.5
nothing
move
new user (both)
new user
3
3.5
4
4.5
x 105
2
2.5
edge creation events
Figure 5: SPAR versus Random for K = 2
placed in 4 servers – to 200% in the case of 512 servers. For
Orkut and Facebook the improvement ranges from 22% and
44% to 174% and 315% respectively. For large number of
servers, the ratio starts decreasing. However, this is likely
not representative of what would happen in a real system,
since the number of replicas per server is artiﬁcially small
(given the number of users and the number of servers).
Such small overheads allow SPAR to achieve a much higher
throughput, as we will demonstrate in Sec.7 by running
a real implementation of SPAR on top of Cassandra and
MySQL.
5.3 Dynamic operations and SPAR
So far, we have shown that SPAR online outperforms ex-
isting solutions when measuring replication overhead. We
now turn to other system requirements stated in Sec.3.1.
5.3.1 The Delicate Art of Balancing Loads
We ﬁrst focus on how replicas are distributed across users.
We take the example of Twitter with 128 servers and K=2.
In this case, the average replication overhead is 3.69: 75.8%
of the users have 3 replicas, 90% of the users have 7 or less
replicas and 99% of the users have 31 or less replicas. Out
of the 2.4M users, only 139 need to be replicated across the
128 servers. Next we look at the impact of such replication
distribution on read and write operations.
Reads and Writes: Read operations are only conducted
on masters. Therefore, we want to analyze whether the ag-
gregate read load of servers, due to read of the master by
their users, is balanced. This load depends on the distri-
bution of master among servers, and on the read patterns
of the users. SPAR online yields a partition in which the
Figure 6: Ratio of actions performed by SPAR as edge
creation events occur for Facebook
d
e
v
o
m
s
e
d
o
n
f
o
r
e
b
m
u
n
150
F
D
C
100
50
0
0
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0
100
101
102
number of nodes moved by event
103
0.5
1
1.5
2
edge creation events
2.5
3
3.5
4
4.5
5
x 105