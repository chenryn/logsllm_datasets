### Highly Connected Nodes and Placement Strategy
Highly connected nodes, which may require the movement of numerous replicas due to local data semantics, are given priority in choosing their server. The remaining nodes are placed using a simple water-filling strategy. As we will see in the evaluation section, this approach ensures an even distribution of failed masters while maintaining a minimal replication overhead.

### 5. MEASUREMENT-DRIVEN EVALUATION
In this section, we evaluate the performance of SPAR in terms of replication overhead and replica movements (migrations).

#### 5.1 Evaluation Methodology

##### 5.1.1 Metrics
Our primary quantitative evaluation metric is the replication overhead (ro), which represents the number of slave replicas needed to ensure local semantics while adhering to the K-redundancy condition. We also analyze the number of node movements, i.e., replica migrations between servers during the operation of SPAR online.

##### 5.1.2 Datasets
We use three different datasets, each serving a distinct purpose for evaluating individual performance metrics.

- **Twitter**: A dataset collected by crawling Twitter between November 25 and December 4, 2008, consisting of 2,408,534 nodes and 48,776,888 edges. It also includes 12 million tweets generated by 2.4 million users during the collection period. Despite the existence of larger datasets [21], they are topic-specific and do not contain all user tweets. To our knowledge, this is the largest dataset with complete user activity, providing a good approximation of Twitter as of December 2008.
  
- **Facebook**: A public dataset of the New Orleans Facebook network [34] collected between December 2008 and January 2009. It includes 60,290 nodes and 1,545,686 edges, along with wall posts. The dataset contains edge creation timestamps for some users, but not all. Therefore, we filtered the dataset to retain a sub-network with complete information for 59,297 nodes and 477,993 edges.
  
- **Orkut**: A dataset collected between October 3 and November 11, 2006, consisting of 3,072,441 nodes and 223,534,301 edges. More details about this dataset can be found in [25], and it is the largest of the three datasets.

##### 5.1.3 Algorithms for Comparison
We compare SPAR against the following partitioning algorithms:

- **Random Partitioning**: Key-Value stores like Cassandra, MongoDB, and SimpleDB partition data randomly across servers. This method is the de-facto standard used in most commercial systems [30].
  
- **Graph Partitioning**: Several offline algorithms exist for partitioning a graph into a fixed number of equal-sized partitions, minimizing the number of inter-partition edges [10, 19]. We use METIS [19], known for its speed and high-quality partitions for large social graphs [23].
  
- **Modularity Optimization (MO+) Algorithms**: We also consider an offline community detection algorithm [12] based on the modularity metric [27, 26]. We modified it to create a fixed number of equal-sized partitions [28]. Our modified version, called MO+, groups communities sequentially into partitions until a partition is full. If a community exceeds the predefined size, we recursively apply MO [12] to the community.

The evaluation procedure is as follows:

##### 5.1.4 Computation of Results
- **Input**: A graph (one of the datasets described earlier), the desired number of partitions \( M \), and the minimum number of replicas per user's profile \( K \).
- **Partition Generation**: Partitions are produced by executing each algorithm on the input.
- **Replica Addition**: For algorithms that do not inherently support local semantics, we process the partitions and add replicas when a master is missing some of its neighbors in the same partition.
- **Edge Creation Trace**: For Facebook, we generate the edge creation trace using exact timestamps. For Twitter and Orkut, where timestamps are not available, we create random permutations of the edges to produce an ordered edge creation trace. Random permutations yield similar qualitative results, and there is virtually no quantitative difference across multiple permutations for any dataset.

### 5.2 Evaluation of Replication Overhead
Figure 4 summarizes the replication overhead of different algorithms over the various datasets for \( K = 0 \) and \( K = 2 \), and for different numbers of servers, ranging from 4 to 512. SPAR online generates significantly lower replication overhead compared to other algorithms, including traditional offline graph partitioning (METIS) and community detection algorithms (MO+). The relative ranking of algorithms from best to worst is SPAR, MO+, METIS, and Random. The absolute value of the replication overhead increases sub-linearly with the number of servers, indicating that adding more servers does not lead to a proportional increase in per-server resource requirements.

SPAR naturally achieves low replication overhead as this is its optimization objective. Competing algorithms, which optimize for minimal inter-server edges, end up requiring more replicas to ensure local semantics in each partition during the post-processing step.

**Low-Cost Redundancy**: An important property of SPAR is that it achieves local semantics at a discounted cost by reusing replicas needed for redundancy. For example, with 32 servers and no fault tolerance (\( K = 0 \)), the replication overhead of SPAR to ensure local semantics is 1.72. If we require a profile to be replicated at least \( K = 2 \) times, the new replication overhead is 2.76 (rather than 3.72). Out of the 2.76 copies, 2 are inevitable due to the redundancy requirement, resulting in a local semantics cost of 2.76 - 2 = 0.76 instead of 1.72. This gain comes from leveraging redundant replicas to achieve locality, a feature unique to SPAR.

**Comparison with Random Partitioning**: We now focus on comparing SPAR with Random partitioning, the de-facto standard. Figure 5 depicts the ratio of the overhead of Random to that of SPAR. For the Twitter dataset, improvements range from 12% for a partition of 4 servers (low because \( K = 2 \) means 3 replicas of each node in 4 servers) to 200% for 512 servers. For Orkut and Facebook, the improvement ranges from 22% and 44% to 174% and 315%, respectively. For a large number of servers, the ratio starts decreasing, but this is likely not representative of real systems due to artificially small numbers of replicas per server.

Such low overheads allow SPAR to achieve much higher throughput, as demonstrated in Section 7 by running a real implementation of SPAR on top of Cassandra and MySQL.

### 5.3 Dynamic Operations and SPAR
So far, we have shown that SPAR online outperforms existing solutions in terms of replication overhead. We now turn to other system requirements stated in Section 3.1.

##### 5.3.1 Balancing Loads
We first focus on how replicas are distributed across users. For example, with Twitter, 128 servers, and \( K = 2 \), the average replication overhead is 3.69: 75.8% of users have 3 replicas, 90% have 7 or fewer replicas, and 99% have 31 or fewer replicas. Out of 2.4 million users, only 139 need to be replicated across all 128 servers. Next, we examine the impact of this replication distribution on read and write operations.

- **Reads and Writes**: Read operations are conducted only on masters. We analyze whether the aggregate read load of servers, due to reads of the master by their users, is balanced. This load depends on the distribution of masters among servers and the read patterns of users. SPAR online yields a partition in which the load is well-balanced.

Figure 6 shows the ratio of actions performed by SPAR as edge creation events occur for Facebook.