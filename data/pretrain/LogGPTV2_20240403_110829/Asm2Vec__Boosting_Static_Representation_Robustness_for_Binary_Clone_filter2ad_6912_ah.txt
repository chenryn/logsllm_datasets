### References

1. A. Saebjornsen, “Detecting Fine-Grained Similarity in Binaries,” Ph.D. dissertation, University of California, Davis, 2014.

2. S. H. H. Ding, B. C. M. Fung, and P. Charland, “Kam1n0: MapReduce-Based Assembly Clone Search for Reverse Engineering,” in Proceedings of the 22nd ACM International Conference on Knowledge Discovery and Data Mining (SIGKDD), 2016.

3. Y. David, N. Partush, and E. Yahav, “Statistical Similarity of Binaries,” in Proceedings of the 37th ACM SIGPLAN Conference on Programming Language Design and Implementation, 2016.

4. Y. LeCun, Y. Bengio, and G. Hinton, “Deep Learning,” Nature, vol. 521, no. 7553, pp. 436–444, 2015.

5. Q. Le and T. Mikolov, “Distributed Representations of Sentences and Documents,” in Proceedings of the International Conference on Machine Learning, 2014, pp. 1188–1196.

6. T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed Representations of Words and Phrases and Their Compositionality,” in Proceedings of the Advances in Neural Information Processing Systems, 2013.

7. D. Andriesse, A. Slowinska, and H. Bos, “Compiler-Agnostic Function Detection in Binaries,” in Proceedings of the 2017 IEEE European Symposium on Security and Privacy (EuroS&P), 2017, pp. 177–189.

8. C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and G. Vigna, “Polymorphic Worm Detection Using Structural Information of Executables,” in Proceedings of the International Workshop on Recent Advances in Intrusion Detection, Springer, 2006.

9. P. Junod, J. Rinaldini, J. Wehrli, and J. Michielin, “Obfuscator-LLVM: Software Protection for the Masses,” in Proceedings of the 2015 IEEE/ACM 1st International Workshop on Software Protection (SPRO), IEEE, 2015.

10. S. Banescu, C. S. Collberg, V. Ganesh, Z. Newsham, and A. Pretschner, “Code Obfuscation Against Symbolic Execution Attacks,” in Proceedings of the 32nd Annual Conference on Computer Security Applications (ACSAC), Los Angeles, CA, USA, December 5-9, 2016, pp. 189–200.

11. G. Myles and C. Collberg, “K-Gram Based Software Birthmarks,” in Proceedings of the 2005 ACM Symposium on Applied Computing, ACM, 2005, pp. 314–318.

12. J. Jang, M. Woo, and D. Brumley, “Towards Automatic Software Lineage Inference,” in Proceedings of the USENIX Security Symposium, 2013, pp. 81–96.

13. H. Huang, A. Youssef, and M. Debbabi, “BinSequence: Fast, Accurate, and Scalable Binary Code Reuse Detection,” in Proceedings of the ACM Asia Conference on Computer and Communications Security (ASIACCS), ACM Press, 2017.

14. J. Pewny, F. Schuster, L. Bernhard, T. Holz, and C. Rossow, “Leveraging Semantic Signatures for Bug Search in Binary Programs,” in Proceedings of the 30th Annual Computer Security Applications Conference, ACM, 2014.

15. L. Nouh, A. Rahimian, D. Mouheb, M. Debbabi, and A. Hanna, “BinSign: Fingerprinting Binary Functions to Support Automated Analysis of Code Executables,” in Proceedings of the IFIP International Conference on ICT Systems Security and Privacy Protection, Springer, 2017.

16. P. Shirani, L. Wang, and M. Debbabi, “BinShape: Scalable and Robust Binary Library Function Identification Using Function Shape,” in Proceedings of the International Conference on Detection of Intrusions and Malware, and Vulnerability Assessment, Springer, 2017.

17. T. Dullien and R. Rolles, “Graph-Based Comparison of Executable Objects (English Version),” SSTIC, vol. 5, no. 1, p. 3, 2005.

18. M. Bourquin, A. King, and E. Robbins, “Binslayer: Accurate Comparison of Binary Executables,” in Proceedings of the 2nd ACM SIGPLAN Program Protection and Reverse Engineering Workshop, ACM, 2013, p. 4.

19. Y. David, N. Partush, and E. Yahav, “Similarity of Binaries Through Re-optimization,” in Proceedings of the 38th ACM SIGPLAN Conference on Programming Language Design and Implementation, ACM, 2017, pp. 79–94.

20. X. Xu, C. Liu, Q. Feng, H. Yin, L. Song, and D. Song, “Neural Network-Based Graph Embedding for Cross-Platform Binary Code Similarity Detection,” in Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications Security, ACM, 2017, pp. 363–376.

21. D. Gao, M. K. Reiter, and D. Song, “Binhunt: Automatically Finding Semantic Differences in Binary Programs,” in Proceedings of the International Conference on Information and Communications Security, Springer, 2008.

22. J. Ming, M. Pan, and D. Gao, “iBinhunt: Binary Hunting with Interprocedural Control Flow,” in Proceedings of the International Conference on Information Security and Cryptology, Springer, 2012.

23. L. Jiang and Z. Su, “Automatic Mining of Functionally Equivalent Code Fragments via Random Testing,” in Proceedings of the 18th International Symposium on Software Testing and Analysis, ACM, 2009.

24. Y.-C. Jhi, X. Wang, X. Jia, S. Zhu, P. Liu, and D. Wu, “Value-Based Program Characterization and Its Application to Software Plagiarism Detection,” in Proceedings of the 33rd International Conference on Software Engineering (ICSE), IEEE, 2011.

25. F. Zhang, Y.-C. Jhi, D. Wu, P. Liu, and S. Zhu, “A First Step Towards Algorithm Plagiarism Detection,” in Proceedings of the 2012 International Symposium on Software Testing and Analysis, ACM, 2012.

26. X. Zhang and R. Gupta, “Matching Execution Histories of Program Versions,” in Proceedings of the 10th European Software Engineering Conference, ACM, 2005.

27. T. Kamiya, S. Kusumoto, and K. Inoue, “CCFinder: A Multilinguistic Token-Based Code Clone Detection System for Large-Scale Source Code,” IEEE Transactions on Software Engineering, vol. 28, no. 7, 2002.

28. Z. Li, S. Lu, S. Myagmar, and Y. Zhou, “CP-Miner: Finding Copy-Paste and Related Bugs in Large-Scale Software Code,” IEEE Transactions on Software Engineering, vol. 32, no. 3, 2006.

29. I. D. Baxter, A. Yahin, L. Moura, M. Sant’Anna, and L. Bier, “Clone Detection Using Abstract Syntax Trees,” in Proceedings of the International Conference on Software Maintenance, IEEE, 1998.

30. L. Jiang, G. Misherghi, Z. Su, and S. Glondu, “Deckard: Scalable and Accurate Tree-Based Detection of Code Clones,” in Proceedings of the 29th International Conference on Software Engineering, IEEE Computer Society, 2007.

31. J. Jang, A. Agrawal, and D. Brumley, “Redebug: Finding Unpatched Code Clones in Entire OS Distributions,” in Proceedings of the IEEE Symposium on Security and Privacy (SP), IEEE, 2012.

32. M. White, M. Tufano, C. Vendome, and D. Poshyvanyk, “Deep Learning Code Fragments for Code Clone Detection,” in Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering, ACM, 2016.

### Appendix A: Extended Formulation of Asm2Vec

This appendix extends the original description and formulation of the Asm2Vec model training. Recall that we define \( f_s \) as an assembly function in the repository at the beginning of Section 4. The Asm2Vec model aims to learn the following parameters:

- \( \theta_{f_s} \in \mathbb{R}^{2 \times d} \): The vector representation of the function \( f_s \).
- \( v_t \in \mathbb{R}^d \): The vector representation of a token \( t \).
- \( v'_t \in \mathbb{R}^d \): Another vector of token \( t \), used for prediction.

**Table 5: Parameters to be estimated in training.**

All \( \theta_{f_s} \) and \( v_t \) are initialized to small random values around zero. All \( v'_t \) are initialized to zeros. We use \( 2 \times d \) for \( f_s \) since we concatenate the vector for operation and operands to represent an instruction. We also define the following symbols according to the syntax of assembly language:

- \( S(f_s) = \text{seq}[1 : i] \): Multiple sequences generated from \( f_s \).
- \( I(\text{seq}_i) = \text{in}[1 : j] \): Instructions of a sequence \( \text{seq}_i \).
- \( \text{in}_j \): The \( j \)-th instruction in a sequence.
- \( A(\text{in}_j) \): Operands of instruction \( \text{in}_j \).
- \( P(\text{in}_j) \): The operation of instruction \( \text{in}_j \).
- \( T(\text{in}_j) \): Tokens of \( \text{in}_j \).
- \( CT(\text{in}_j) \in \mathbb{R}^{2 \times d} \): Vector representation of an instruction \( \text{in}_j \).
- \( CT(\text{in}_{j-1}) \in \mathbb{R}^{2 \times d} \): Vector representation of \( \text{in}_j \)'s previous instruction.
- \( CT(\text{in}_{j+1}) \in \mathbb{R}^{2 \times d} \): Vector representation of \( \text{in}_j \)'s next instruction.
- \( \delta(\text{in}_j, f_s) \in \mathbb{R}^{2 \times d} \): Vector representation of the joint memory of function \( f_s \) and \( \text{in}_j \)'s neighbor instructions.

For the current instruction \( \text{in}_j \), the proposed model maximizes the following log probability:

\[ \arg \max_{\theta} \sum_{\text{in}_j} \sum_{t_c \in T(\text{in}_j)} \log P(t_c | f_s, \text{in}_{j-1}, \text{in}_{j+1}) \]

It predicts each token in the current instruction \( \text{in}_j \) based on the joint memory of its corresponding function vector and its neighbor instruction vectors, as illustrated in Figure 5.

To model the above prediction, one can use a typical softmax multi-class classification layer and maximize the following log probability:

\[ P(t_c | \delta(\text{in}_j, f_s)) = \frac{\exp(v'_{t_c} \cdot \delta(\text{in}_j, f_s))}{\sum_{t_d \in D} \exp(v'_{t_d} \cdot \delta(\text{in}_j, f_s))} \]

where \( D \) denotes the whole vocabulary constructed upon the repository \( RP \). The total number of parameters to be estimated is \( (|D| + 1) \times 2 \times d \) for each pass of the softmax layout. The term \( |D| \) is too large to be efficient for the softmax classification.

Therefore, we use the k-negative sampling approach [20], [21], to approximate the log probability:

\[ \log P(t_c | \delta(\text{in}_j, f_s)) \approx \log \sigma(v'_{t_c} \cdot \delta(\text{in}_j, f_s)) + \sum_{i=1}^k \mathbb{E}_{t_d \sim P_n(t_c)} \log \sigma(-v'_{t_d} \cdot \delta(\text{in}_j, f_s)) \]

**Table 6: Intermediate symbols used in training.**

For an instruction \( \text{in}_j \), we treat the concatenation of its operation and operands as its tokens \( T(\text{in}_j) \):

\[ T(\text{in}_j) = P(\text{in}_j) || A(\text{in}_j) \]

where \( || \) denotes concatenation. \( CT(\text{in}_j) \) denotes the vector representation of an instruction \( \text{in}_j \):

\[ CT(\text{in}_j) = v_{P(\text{in}_j)} || \frac{1}{|A(\text{in}_j)|} \sum_{t_b \in A(\text{in}_j)} v_{t_b} \]

The representation is calculated by averaging the vector representations of its operands \( A(\text{in}_j) \). The averaged vector is then concatenated to the vector representation \( v_{P(\text{in}_j)} \) of its operation \( P(\text{in}_j) \).

As presented in Algorithm 1, the training procedure goes through each assembly function \( f_s \) in the repository and generates multiple sequences by calling \( S(f_s) \). For each sequence \( \text{seq}_i \) of function \( f_s \), the neural network walks through the instructions from its beginning. We collect the current instruction \( \text{in}_j \), its previous instruction \( \text{in}_{j-1} \), and its next instruction \( \text{in}_{j+1} \). We ignore the instructions that are out-of-boundary. We calculate \( T(\text{in}_{j-1}) \) and \( T(\text{in}_{j+1}) \) using the previous equation. By averaging \( f_s \)'s vector representation \( \theta_{f_s} \) with \( CT(\text{in}_{j-1}) \) and \( CT(\text{in}_{j+1}) \), \( \delta(\text{in}_j, f_s) \) models the joint memory of neighbor instructions:

\[ \delta(\text{in}_j, f_s) = \frac{1}{3} (\theta_{f_s} + CT(\text{in}_{j-1}) + CT(\text{in}_{j+1})) \]

By manipulating the value of the parameters listed in Table 5, we can maximize the sum of the above log-probability for all the instructions \( \text{in}_j \).

We follow the parallel stochastic gradient descent algorithm. In a single training step, we only consider a single token \( t_c \) of the current instruction \( \text{in}_j \). We calculate the above log probability and its gradients with respect to the parameters that we are trying to manipulate. The gradients define the direction in which we should manipulate the parameters to maximize the log probability. The gradients are calculated by taking the derivatives with respect to each parameter defined in Table 5. The table below defines the symbol of the gradients:

- \( \frac{\partial J(\theta)}{\partial \theta_{f_s}} \): The gradient for current function \( f_s \)'s \( \theta_{f_s} \).
- \( \frac{\partial J(\theta)}{\partial v'_{t_c}} \): The gradient for the token \( t_c \) of the current instruction \( \text{in}_j \).
- \( \frac{\partial J(\theta)}{\partial v_{P(\text{in}_{j+1})}} \): The gradient for the operation of instruction \( \text{in}_{j+1} \).
- \( \frac{\partial J(\theta)}{\partial v_{P(\text{in}_{j-1})}} \): The gradient for the operation of instruction \( \text{in}_{j-1} \).
- \( \frac{\partial J(\theta)}{\partial v_{t_b}} \): The gradient for each operation of instruction \( \text{in}_{j+1} \) and \( \text{in}_{j-1} \).

**Table 7: Gradients to be calculated in a training step.**

The equations below calculate the gradients defined above:

\[ \frac{\partial J(\theta)}{\partial \theta_{f_s}} = \frac{1}{3} \sum_{i=1}^k \left( \sigma(v'_{t_i} \cdot \delta(\text{in}_j, f_s)) - \mathbb{I}(t_i = t_c) \right) \cdot \delta(\text{in}_j, f_s) \]

\[ \frac{\partial J(\theta)}{\partial v'_{t_c}} = \left( \sigma(v'_{t_c} \cdot \delta(\text{in}_j, f_s)) - \mathbb{I}(t_c = t_c) \right) \cdot \delta(\text{in}_j, f_s) \]

\[ \frac{\partial J(\theta)}{\partial v_{P(\text{in}_{j+1})}} = \frac{1}{|A(\text{in}_{j+1})|} \sum_{t_b \in A(\text{in}_{j+1})} \left( \sigma(v'_{t_b} \cdot \delta(\text{in}_j, f_s)) - \mathbb{I}(t_b = t_c) \right) \cdot \delta(\text{in}_j, f_s) \]

\[ \frac{\partial J(\theta)}{\partial v_{P(\text{in}_{j-1})}} = \frac{1}{|A(\text{in}_{j-1})|} \sum_{t_b \in A(\text{in}_{j-1})} \left( \sigma(v'_{t_b} \cdot \delta(\text{in}_j, f_s)) - \mathbb{I}(t_b = t_c) \right) \cdot \delta(\text{in}_j, f_s) \]

\[ \frac{\partial J(\theta)}{\partial v_{t_b}} = \left( \sigma(v'_{t_b} \cdot \delta(\text{in}_j, f_s)) - \mathbb{I}(t_b = t_c) \right) \cdot \delta(\text{in}_j, f_s) \]

After calculating the gradients, we use backpropagation to update the values of all the involved parameters according to their gradients in Table 7, with a learning rate.

### Appendix B: Extended Descriptive Statistics of the Dataset

This appendix provides additional descriptive statistics on the experimental dataset used in Sections 5.1, 5.2, and 5.3.

#### Compiler Optimization Experiment (Section 5.1)

In the compiler optimization experiment, ImageMagick generally has the largest number of assembly basic blocks, while zlib has the least. By adopting different compiler optimization options, the generated number of basic blocks varies significantly. Specifically, O0 is very different from the other optimization levels. O1 and O2 appear to share a similar number. O3 has the largest number of basic blocks, which is generated by intensive inlining. Figure 12 shows the empirical distribution of the assembly functions length under different optimization levels. O3 tends to produce assembly functions that are much longer than O0, O1, and O2. O1 and O2 share similar distributions on function length.

**Table 8: Number of basic blocks for each selected library compiled using different optimization options.**

| Library         | GCC O0   | GCC O1   | GCC O2   | GCC O3   |
|-----------------|----------|----------|----------|----------|
| BusyBox         | 52,118   | 46,519   | 47,272   | 62,069   |
| CoreUtils       | 38,176   | 36,168   | 35,117   | 41,421   |
| Libgmp          | 12,919   | 15,534   | 14,602   | 16,234   |
| ImageMagick     | 85,191   | 88,342   | 84,395   | 93,421   |
| Libcurl         | 17,969   | 14,097   | 13,483   | 15,371   |
| LibTomCrypt     | 12,021   | 10,135   | 10,258   | 13,451   |
| OpenSSL         | 52,063   | 44,527   | 44,642   | 50,043   |
| SQLite          | 27,621   | 24,978   | 29,332   | 38,699   |
| zlib            | 2,898    | 2,747    | 2,668    | 3,706    |
| PuTTYgen        | 5,495    | 4,957    | 5,065    | 7,231    |
| **Total**       | **306,471** | **288,004** | **286,834** | **341,646** |

#### O-LLVM Obfuscation Experiment (Section 5.2)

In the O-LLVM obfuscation experiment, we evaluate the clone search methods before and after obfuscation. O-LLVM significantly increases the complexity of the binary code. Table 9 shows how the number of basic blocks has changed across different obfuscation levels. Figure 13 shows the empirical distribution of the assembly functions length under different obfuscation options. There are three different techniques and their combination:

- **BCF (Bogus Control Flow)**: Modifies the control flow graph by adding a large number of irrelevant random basic blocks and branches. It will also split, merge, and reorder the original basic blocks. It almost doubles the number of basic blocks after obfuscation (see Table 9).
- **FLA (Control Flow Flattening)**: Reorganizes the original CFG using a complex hierarchy of new conditions as switches. Only the penultimate level of the CFG contains the modified original logics. It completely destroys the original CFG graph. The obfuscated binary on average contains four times the number of basic blocks than the original.
- **SUB (Instruction Substitution)**: Substitutes fragments of assembly code to its equivalent form by going one pass over the function logic using predefined rules. This technique modifies the contents of basic blocks and adds new constants. SUB does not change much of the graph structure. Figure 11 shows an example. Figure 13 shows that it increases the length of the original assembly function.

**Table 9: Number of basic blocks for each selected library under different code obfuscation options.**

| Library         | Original | BCF      | FLA      | SUB      | All      |
|-----------------|----------|----------|----------|----------|----------|
| Libgmp          | 20,168   | 54,738   | 103,258  | 20,168   | 55,007   |
| ImageMagick     | 83,704   | 218,315  | 434,599  | 83,702   | 216,904  |
| LibTomCrypt     | 10,044   | 19,534   | 35,608   | 10,115   | 62,895   |
| OpenSSL         | 46,298   | 100,315  | 160,265  | 46,278   | 289,657  |
| **Total**       | **160,214** | **392,902** | **733,730** | **160,263** | **624,463** |

**Figure 12: The empirical distribution function of the assembly function length in terms of the number of instructions. From left to right, each diagram corresponds to the optimization options O0, O1, O2, and O3.**

**Figure 13: The empirical distribution function of the assembly function length in terms of the number of instructions. Each diagram from left to right corresponds to the original binary, the obfuscation techniques Bogus Control Flow, Control Flow Flattening, Instruction Substitution, and the application of all the obfuscation techniques provided by Obfuscator-LLVM.**

---

This version of the text is more structured, coherent, and professional, with clear and concise language.