[16] A. Saebjornsen, “Detecting ﬁne-grained similarity in binaries,” Ph.D.
dissertation, UC Davis, 2014.
[17] S. H. H. Ding, B. C. M. Fung, and P. Charland, “Kam1n0:
Mapreduce-based assembly clone search for reverse engineering,” in
Proceedings of the 22nd ACM International Conference on Knowl-
edge Discovery and Data Mining (SIGKDD).
(cid:21)(cid:25)(cid:22)
[18] Y. David, N. Partush, and E. Yahav, “Statistical similarity of binaries,”
in Proceedings of the 37th ACM SIGPLAN Conference on Program-
ming Language Design and Implementation, 2016.
[19] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol.
521, no. 7553, pp. 436–444, 2015.
[20] Q. Le and T. Mikolov, “Distributed representations of sentences
and documents,” in Proceedings of the International Conference on
Machine Learning, 2014, pp. 1188–1196.
[21] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean,
“Distributed representations of words and phrases and their compo-
sitionality,” in Proceedings of the Advances in Neural Information
Processing Systems, 2013.
[22] D. Andriesse, A. Slowinska, and H. Bos, “Compiler-agnostic function
detection in binaries,” in Proceedings of the 2017 IEEE European
Symposium on Security and Privacy (EuroS&P), 2017, pp. 177–189.
[23] C. Kruegel, E. Kirda, D. Mutz, W. Robertson, and G. Vigna, “Poly-
morphic worm detection using structural information of executables,”
in Proceedings of the International Workshop on Recent Advances in
Intrusion Detection. Springer, 2006.
[24] P. Junod, J. Rinaldini, J. Wehrli, and J. Michielin, “Obfuscator-
llvm–software protection for the masses,” in Proceedings of 2015
IEEE/ACM 1st
International Workshop on Software Protection
(SPRO).
IEEE, 2015.
[25] S. Banescu, C. S. Collberg, V. Ganesh, Z. Newsham, and
A. Pretschner, “Code obfuscation against symbolic execution attacks,”
in Proceedings of the 32nd Annual Conference on Computer Security
Applications, ACSAC 2016, Los Angeles, CA, USA, December 5-9,
2016, 2016, pp. 189–200.
[26] G. Myles and C. Collberg, “K-gram based software birthmarks,” in
Proceedings of the 2005 ACM Symposium on Applied Computing.
ACM, 2005, pp. 314–318.
[27] J. Jang, M. Woo, and D. Brumley, “Towards automatic software lin-
eage inference.” in Proceedings of the USENIX Security Symposium,
2013, pp. 81–96.
[28] H. Huang, A. Youssef, and M. Debbabi, “BinSequence: Fast, accurate
and scalable binary code reuse detection.” in Proceedings of the
ACM Asia Conference on Computer and Communications Security
(ASIACCS). ACM Press, 2017.
[29] J. Pewny, F. Schuster, L. Bernhard, T. Holz, and C. Rossow, “Lever-
aging semantic signatures for bug search in binary programs,” in
Proceedings of
the 30th Annual Computer Security Applications
Conference. ACM, 2014.
[30] L. Nouh, A. Rahimian, D. Mouheb, M. Debbabi, and A. Hanna, “Bin-
Sign: Fingerprinting binary functions to support automated analysis
of code executables,” in Proceedings of the IFIP International Con-
ference on ICT Systems Security and Privacy Protection. Springer,
2017.
[31] P. Shirani, L. Wang, and M. Debbabi, “BinShape: Scalable and robust
binary library function identiﬁcation using function shape,” in Pro-
ceedings of the International Conference on Detection of Intrusions
and Malware, and Vulnerability Assessment. Springer, 2017.
[32] T. Dullien and R. Rolles, “Graph-based comparison of executable
objects (english version),” SSTIC, vol. 5, no. 1, p. 3, 2005.
[33] M. Bourquin, A. King, and E. Robbins, “Binslayer: accurate compari-
son of binary executables,” in Proceedings of the 2nd ACM SIGPLAN
Program Protection and Reverse Engineering Workshop.
ACM,
2013, p. 4.
[34] Y. David, N. Partush, and E. Yahav, “Similarity of binaries through re-
optimization,” in Proceedings of the 38th ACM SIGPLAN Conference
on Programming Language Design and Implementation.
ACM,
2017, pp. 79–94.
[35] X. Xu, C. Liu, Q. Feng, H. Yin, L. Song, and D. Song, “Neural
network-based graph embedding for cross-platform binary code sim-
ilarity detection,” in Proceedings of the 2017 ACM SIGSAC Confer-
ence on Computer and Communications Security. ACM, 2017, pp.
363–376.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply. 
[36] D. Gao, M. K. Reiter, and D. Song, “Binhunt: Automatically ﬁnding
semantic differences in binary programs,” in Proceedings of the In-
ternational Conference on Information and Communications Security.
Springer, 2008.
[37] J. Ming, M. Pan, and D. Gao, “ibinhunt: Binary hunting with inter-
procedural control ﬂow,” in Proceedings of the International Confer-
ence on Information Security and Cryptology. Springer, 2012.
[38] L. Jiang and Z. Su, “Automatic mining of functionally equivalent
code fragments via random testing,” in Proceedings of
the 18th
International Symposium on Software Testing and Analysis. ACM,
2009.
[39] Y.-C. Jhi, X. Wang, X. Jia, S. Zhu, P. Liu, and D. Wu, “Value-based
program characterization and its application to software plagiarism
detection,” in Proceedings of the 33rd International Conference on
Software Engineering (ICSE).
IEEE, 2011.
[40] F. Zhang, Y.-C. Jhi, D. Wu, P. Liu, and S. Zhu, “A ﬁrst step
towards algorithm plagiarism detection,” in Proceedings of the 2012
International Symposium on Software Testing and Analysis. ACM,
2012.
[41] X. Zhang and R. Gupta, “Matching execution histories of program
versions,” in Proceedings of the 10th European Software Engineering
Conference. ACM, 2005.
[42] T. Kamiya, S. Kusumoto, and K. Inoue, “Ccﬁnder: a multilinguistic
token-based code clone detection system for large scale source code,”
IEEE Transactions on Software Engineering, vol. 28, no. 7, 2002.
[43] Z. Li, S. Lu, S. Myagmar, and Y. Zhou, “Cp-miner: Finding copy-
paste and related bugs in large-scale software code,” IEEE Transac-
tions on Software Engineering, vol. 32, no. 3, 2006.
[44] I. D. Baxter, A. Yahin, L. Moura, M. Sant’Anna, and L. Bier, “Clone
detection using abstract syntax trees,” in Proceedings of International
Conference on Software Maintenance.
IEEE, 1998.
[45] L. Jiang, G. Misherghi, Z. Su, and S. Glondu, “Deckard: Scalable
and accurate tree-based detection of code clones,” in Proceedings of
the 29th International Conference on Software Engineering.
IEEE
Computer Society, 2007.
[46] J. Jang, A. Agrawal, and D. Brumley, “Redebug: ﬁnding unpatched
code clones in entire os distributions,” in Proceedings of IEEE
Symposium on Security and Privacy (SP).
IEEE, 2012.
[47] M. White, M. Tufano, C. Vendome, and D. Poshyvanyk, “Deep
learning code fragments for code clone detection,” in Proceedings of
the 31st IEEE/ACM International Conference on Automated Software
Engineering. ACM, 2016.
(cid:21)(cid:25)(cid:23)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply. 
Appendix A.
Extended Formulation of Asm2Vec
This appendix extends the original description and the
formulation of the Asm2Vec model training. Recall that we
deﬁne fs as an assembly function in the repository at the
beginning of Section 4. The Asm2Vec model tries to learn
the following parameters:
2×d
(cid:3)θfs ∈ R
(cid:3)vt ∈ R
t ∈ R
(cid:3)v(cid:2)
d
d
The vector representation of the function fs.
The vector representation of a token t.
Another vector of token t, used for prediction.
TABLE 5: Parameters to be estimated in training.
All (cid:2)θfs and (cid:2)vt are initialized to small random value
t are initialized to zeros. We use 2 × d
around zero. All (cid:2)v(cid:2)
for fs since we concatenate the vector for operation and
operands to represent an instruction. We also deﬁne the
following symbols according to the syntax of assembly
language:
S(fs) = seq[1 : i]
I(seqi) = in[1 : j]
inj
A(inj )
P(inj )
T (inj )
CT (inj ) ∈ R
2×d
CT (inj−1) ∈ R
CT (inj+1) ∈ R
2×d
2×d
δ(inj , fs)
Multiple sequences generated from fs.
Instructions of a sequence seqi.
The jth instruction in a sequence.
Operands of instruction inj.
The operation of instruction inj.
Represent the tokens of inj.
Vector representation of an instruction inj.
Vector representation of inj’s previous in-
struction.
Vector representation of an instruction inj’s
next instruction.
Vector representation of the joint memory of
function fs and inj’s neighbor instructions.
For the current
instruction inj,
the proposed model
maximizes the following log probability:
arg max
T (inj )(cid:2)
tc
log P(tc|fs, inj−1, inj+1)
It predicts each token in the current instruction inj based on
the joint memory of its corresponding function vector and
its neighbor instruction vectors, as illustrated in Figure 5.
To model the above prediction, one can use a typical
softmax multi-class classiﬁcation layer and maximize the
following log probability:
P(tc|δ(inj, fs)) = P((cid:2)v(cid:2)
f ((cid:2)v(cid:2)
(cid:3)D
d f ((cid:2)v(cid:2)
=
tc
f ((cid:2)v(cid:2)
tc , δ(inj, fs)) = U h(((cid:2)v(cid:2)
|δ(inj, fs))
tc , δ(inj, fs))
td , δ(inj, fs))
tc )T × δ(inj, fs))
D denotes the whole vocabulary constructed upon the repos-
itory RP. U h(·) denotes a sigmoid function applied to each
value of a vector. The total number of parameters to be
estimated is (|D| + 1) × 2 × d for each pass of the softmax
layout. The term |D| is too large to be efﬁcient for the
softmax classiﬁcation.
Therefore we use the k negative sampling approach [20],
[21], to approximate the log probability:
log P(tc|δ(inj, fs)) ≈ log f ((cid:2)v(cid:2)
|δ(inj, fs))
tc
k(cid:2)
+
i=1
(cid:4)
Etd(cid:2)Pn(tc)
(cid:2)td (cid:5)= tc(cid:3)log f (−1 × (cid:2)v(cid:2)
td , δ(inj, fs))
(cid:5)
TABLE 6: Intermediate symbols used in training.
For an instruction inj, We treat the concatenation of
its operation and operands as its tokens T (inj): T (inj) =
P(inj) || A(inj), where || denotes concatenation. CT (in)
denotes the vector representation of an instruction in.
CT (in) = (cid:2)vP(in)||
1
|A(in)|
A(in)(cid:2)
(cid:2)vtb
t
The representation is calculated by averaging the vector
representations of its operands A(in). The averaged vector
is then concatenated to the vector representation (cid:2)vP(in) of
its operation P(in).
As presented in Algorithm 1, the training procedure goes
through each assembly function fs in the repository and
generates multiple sequences by calling S(fs). For each
sequence seqi of function fs, the neural network walks
through the instructions from its beginning. We collect the
current instruction inj, its previous instruction inj−1, and
its next instruction inj+1. We ignore the instructions that
are out-of-boundary. We calculate T (inj−1) and T (inj+11)
using the previous equation. By averaging fs’s vector repre-
sentation (cid:2)θfs with CT (inj − 1) and CT (inj + 1), δ(in, fs)
models the joint memory of neighbor instructions:
δ(inj, fs) =
1
3
((cid:2)θfs + CT (inj−1) + CT (inj+1))
(cid:21)(cid:25)(cid:24)
By manipulating the value of the parameters listed in Ta-
ble 5, we can maximize the sum of the above log-probability
for all the instruction inj.
We follow the parallel stochastic gradient decent algo-
rithm. In a single training step, we only consider a single
token tc of the current instruction inj. We calculate the
above log probability and its gradients with respect to the
parameters that we are trying to manipulate. The gradients
deﬁne the direction in which we should manipulate the
parameters in order to maximize the log probability. The
gradients are calculated by taking the derivatives with re-
spect to each parameter deﬁned in Table 5. The table below
deﬁnes the symbol of the gradients:
∂
∂(cid:3)θfs
∂
∂ (cid:3)v(cid:2)
t
J(θ)
J(θ)
∂
∂(cid:3)vP(inj+1 )
∂
∂(cid:3)vP(inj−1 )
∂
∂(cid:3)vtb
J(θ)
The gradient for current function fs’s (cid:3)θfs .
The gradient for the token tc of current
instruction inj.
The gradient for the operation of instruction
inj+1.
The gradient for the operation of instruction
inj−1.
The gradient for each operation of instruc-
tion inj+1 and inj−1.
TABLE 7: Gradients to be calculated in a training step.
The equations below calculate the gradients deﬁned
above.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply. 
BusyBox
CoreUtils
Libgmp
ImageMagick
Libcurl
LibTomCrypt
OpenSSL
SQLite
zlib
PuTTYgen
Total
GCC O0
52,118
38,176
12,919
85,191
17,969
12,021
52,063
27,621
2,898
5,495
306,471
GCC O1
46,519
36,168
15,534
88,342
14,097
10,135
44,527
24,978
2,747
4,957
288,004
GCC O2
47,272
35,117
14,602
84,395
13,483
10,258
44,642
29,332
2,668
5,065
286,834
GCC O3
62,069
41,421
16,234
93,421
15,371
13,451
50,043
38,699
3,706
7, 231
341,646
TABLE 8: Number of basic blocks for each selected library
compiled using different optimization options.
Libgmp
ImageMagick
LibTomCrypt
OpenSSL
Total
Original
20,168
83,704
10,044
46,298
160,214
BCF
54,738
218,315
19,534
100,315
392,902
FLA
103,258
434,599
35,608
160,265
733,730
SUB
20,168
83,702
10,115
46,278
160,263
All
55,007
216,904
62,895
289,657
624,463
TABLE 9: Number of basic blocks for each selected library
under different code obfuscation options.
∂
∂(cid:2)θfs
J(θ) =
k(cid:2)
1
3
× (cid:2)v(cid:2)
i
t
Etb(cid:2)Pn(tc)
(cid:4)
(cid:2)tb = tc(cid:3) − f ((cid:2)v(cid:2)
(cid:5)
t, δ(inj, fs))
∂
∂ (cid:2)v(cid:2)
J(θ) = (cid:2)t = tc(cid:3) − f ((cid:2)v(cid:2)
t, δ(inj, fs)) × δ(inj, fs)
t
It will be the same equation for the previous instruction
inj−1, by replacing inj+1 with inj−1.
∂
∂(cid:2)vP(inj+1)
∂
J(θ) =
∂(cid:2)vtb
[d : 2d − 1]
J(θ) =
(cid:5)
J(θ)
(cid:4) ∂
∂(cid:2)θfs
1
|A(inj+1)| × (cid:4) ∂
tb ∈ A(inj+1)
∂(cid:2)θfs
[0 : d − 1]
(cid:5)
J(θ)
Figure 10: A function obfuscated by O-LLVM Control Flow
Graph Flattening. Only the penultimate level (red ﬁlled)
basic blocks contains the modiﬁed original logics.
shows the empirical distribution of the assembly functions
length under different optimization levels. O3 tends to pro-
duce assembly functions that are much longer than O0, O1,
and O2. O1 and O2 share similar distributions on function
length.
In the O-LLVM obfuscation experiment (Section 5.2),
we evaluate the the clone search methods before and after
obfuscation. O-LLVM signiﬁcantly increases the complexity
of the binary code. Table 9 shows how the number of basic
blocks have been changed across different obfuscation level.
Figure 13 shows the empirical distribution of the assembly
functions length under different obfuscation options. There
are three different techniques and their combination:
• BCF modiﬁes the control ﬂow graph by adding a large
number of irrelevant random basic blocks and branches.
It will also split, merge, and reorder the original basic
blocks. It almost double the number of basic blocks after
obfuscation (see Table 9).
• FLA reorganizes the original CFG using complex hier-
archy of new conditions as switches (see an example in
Figure 1). Only the penultimate level of the CFG contains
the modiﬁed original logics. It completely destroys the
original CFG graph. The obfuscated binary on average
contains 4 times of basic blocks than the original.
After, we use back propagation to update the values of all the
involved parameters according to their gradients in Table 7,
with a learning rate.
Appendix B.
Extended Descriptive Statistics of the Dataset
This appendix provides additional descriptive statistics
on the experimental dataset used in Section 5.1, Section 5.2,
and Section 5.3
In the compiler optimization experiment (Section 5.1,
ImageMagick generally has the largest number of assembly
basic blocks while zlib has the least. By adopting different
compiler optimization options,
the generated number of
basic blocks greatly varies. Speciﬁcally, O0 is very different
from the other optimization levels. O1 and O2 appear to
share a similar number. O3 has the largest number of basic
blocks, which is generated by intensive inlining. Figure 12
Figure 11: An assembly fragment obfuscated by O-LLVM
Instruction Substitution. Left: the original fragment. Right:
the obfuscated fragment.
• SUB substitutes fragments of assembly code to its equiva-
lent form by going one pass over the function logic using
predeﬁned rules. This technique modiﬁes the contents
of basic blocks and adds new constants. SUB does not
change much of the graph structure Figure 11 shows an
example. Figure 13 shows that it increase the length of
the original assembly function.
(cid:21)(cid:25)(cid:25)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply. 
Figure 12: The empirical distribution function of the assembly function length in terms of number of instructions. From
left to right, each diagram corresponds to the optimization options O0, O1, O2, and O3.
Figure 13: The empirical distribution function of the assembly function length in terms of the number of instructions. Each
diagram from left to right corresponds to the original binary, the obfuscation techniques Bogus Control Flow Graph, Control
Flow Flattening, Instruction Substitution, and the application of all the obfuscation techniques provided by Obfuscator-LLVM.
(cid:21)(cid:25)(cid:26)
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:47:52 UTC from IEEE Xplore.  Restrictions apply.