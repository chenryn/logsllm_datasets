words, if all transport-layer ﬂows in an OD ﬂow S have less
than 1000 packets, we can estimate the entropy norm of the
 8000
 7000
 6000
 5000
 4000
 3000
 2000
 1000
 0
approximation
y = xln(x)
 0  100 200 300 400 500 600 700 800 900 1000
Figure 1: Comparison of entropy function and the
approximation
1.05 − ||S||0.95
0.95).
OD ﬂow as 10(||S||1.05
It turns out that this
“symmetry” of the exponents (1 + α and 1 − α) around 1
serves another very important purpose that we will describe
shortly.
We found that the approximation becomes gradually worse
when x becomes larger than 1000 (packets), but the relative
error level is generally acceptable up until 5000. Therefore, if
there are some very large transport-layer ﬂows (say with tens
of or hundreds of thousands of packets) inside an OD ﬂow S,
the estimation formula such as 10(||S||1.05
0.95) may
deviate signiﬁcantly from its entropy norm. Fortunately,
identifying such very large ﬂows is a well-studied problem
in both computer networking (e.g., [10]) and theory (e.g.,
[17]). We will adopt the “sample and hold” algorithm pro-
posed in [10] to identify all ﬂows that are much larger than
a certain threshold (say 1000 packets) and compute their
contributions to the OD ﬂow entropy separately.
1.05 − ||S||0.95
Now the last piece of the puzzle is that when we compute
the entropy H from the entropy norm, we need to know
s, the total volume of the OD ﬂow. Estimating this vol-
ume is a task known as traﬃc matrix estimation [18, 21,
22, 23, 27]. Various techniques for estimating the traﬃc
matrix have been proposed that are based on statistical in-
ference or direct measurement (including data streaming).
In fact, this quantity is exactly the L1 norm, which can
be estimated using Indyk’s L1 norm estimation algorithm
with IMP extensions.
It turns out that we need none of
these. Recall that our approximation formula is in the form
2α (x1+α− x1−α), where α is a small number such as 0.05.
of 1
We observe that, in this case, the function x can indeed be
approximated by (x1+α +x1−α)/2 and therefore the OD ﬂow
volume (i.e., the L1 norm) can be approximated by the av-
erage of ||S||1+α
1−α calculated from the OD ﬂow.
Therefore, our sketch data structure allows us to kill two
birds (the L1 norm and the entropy norm) with one stone!
1+α and ||S||1−α
3. PRELIMINARIES
For the purposes of this paper we deﬁne a ﬂow to be all
the packets with the same ﬁve-tuple in their headers: source
address, destination address, source port, destination port,
and protocol.
Clearly, we will not be able to compute the entropy of
each distribution exactly, so we use the following type of
approximation scheme. An (, δ) approximation scheme is
one that returns an approximation ˆθ for a value θ such that,
with probability at least 1−δ, we have (1−)θ ≤ ˆθ ≤ (1+)θ.
3.1 Stable distributions
The family of stable distributions, discovered ﬁrst by Paul
L´evy, is a natural generalization of the Gaussian distribu-
tion [29]. A stable distribution, in its full generality, takes
a fairly complicated form with four parameters.
In this
work, however, we only need to work with the standard
(normalized in a certain sense) cases that take only one
free parameter p (the other three ﬁxed), and the resulting
restricted/standardized family is denoted S(p), p ∈ (0, 2].
Each S(p) is uniquely characterized as follows. Let X be a
random variable that takes distribution S(p) with probabil-
ity density function fS(p)(x). Then its characteristic func-
tion E[eitX ] satisﬁes
Z ∞
−∞
E[eitX
] ≡
fS(p)(x)(cos (xt) + i · sin (xt)) = e−|t|p
.
The existence and uniqueness of a probability density func-
tion satisfying the above equation has been well established
in mathematical literature.2
1
2π
For any p, the probability density function fS(p)(x) of the
distribution S(p) is continuous and inﬁnitely diﬀerentiable
on (−∞, +∞) (i.e., it is well-behaved). However, fS(p)(x)
takes a closed form only for three p values (p = 2, 1, 0.5).
S(p) with p = 2 and 1 corresponds to two well-known exam-
ples of stable distributions: S(2) is the Gaussian distribu-
tion with mean 0 and standard deviation 2, with probabil-
e−x2/8; S(1) is the Cauchy
√
ity density function f (x) = 1
2
distribution with the density function f (x) = 1
1+x2 . For p
π
values other than 2, 1, and 0.5, we will not have a closed-
form probability density function to work with. Instead, the
distribution S(p) is generated using a simulation program to
be described later in Section 7. In the following, we state
some important properties of the stable distribution that
will be used in both Indyk’s and our algorithms.
1. Stability property. The following property earns the
name for the stable distributions. Let X1, X2, . . . , Xn denote
mutually independent random variables that have distribu-
tion S(p), then a1X1 + a2X2 + ··· + anXn has the same
distribution as (|a1|p + |a2|p + ··· + |an|p)1/pX, where X is
a random variable having distribution S(p). It is not hard
to verify that the aforementioned Gaussian and Cauchy ran-
dom variables satisfy the stability property with parameter
p equal to 2 and 1, respectively. It is also not hard to verify
this property using the deﬁnition of S(p) and the property
of Fourier transforms.
2. Symmetry of the probability density function,
fS(p)(x). For any x ∈ (−∞,∞), we have fS(p)(x) = fS(p)(−x).
This can be veriﬁed from the fact that fS(p)(x) is the Fourier
transform of e−|t|p
.
3.2 Indyk’s technique for estimating Lp norm
Estimating the Lp norm (p ∈ (0, 2]) of a stream using the
property of stable distributions is a celebrated result in the-
oretical computer science [12, 13]. As deﬁned before, given a
P
stream S that contains n distinct objects with frequency a1,
a2, ..., an respectively, the Lp norm of the stream is deﬁned
as ||S||p ≡ (
i |ai|p)1/p. Indyk’s algorithm, translated into
the language of TCP/IP, is shown in Algorithm 1.
2In fact, in mathematical literature, the symbol α is often
used in the place of p. We choose to use p to be consistent
with [13], the most relevant related work.
Algorithm 1: Algorithm to compute Lp norm
1: Initialization
2: Let Y [1 . . . l] be an array of ﬂoating point numbers set
to 0.0 at the beginning of measurement interval.
3: Fix l p-stable hash functions sh1 through shl.
4: Online stage
5: for each incoming packet pkt do
6:
7:
8: Oﬄine stage
9: Return med(|Y [1]|, . . . , |Y [l]|)/DM edp
Y [i] += shi(pkt.id)
for i := 1 to l do
The sketch is simply an array (cid:6)Y of real-valued counters
set to 0.0 at the beginning of the measurement interval.
The critical operator in this algorithm is a set of p-stable
hash functions shi, i = 1, ..., l.3 Each shi maps a ﬂow la-
bel pkt.id into a random value drawn from the distribution
S(p), in such a way that (a) the same ﬂow label will always
be mapped to the same random value and (b) diﬀerent ﬂow
labels are mapped to independent random values. In addi-
tion, these p-stable hash functions are independent of one
another.
Online processing of packets with the sketch is also very
simple. For each incoming packet, its ﬂow label is hashed
by shi and the result is added to Yi. Now we analyze the
value of a particular counter Y1, and the analysis for other
counters are exactly the same. Suppose that there are n
ﬂows with ﬂow labels id1, ..., idn and of sizes a1, ..., an.
We denote sh1(id1) as X1, sh1(id2) as X2, ..., sh1(idn) as
Xn. By the property of sh1, we know that X1, ..., Xn
random variable with distribution S(p). After
are i.i.d.
processing the stream, we can see that the counter value Y1
becomes a1 ∗ X1 + a2 ∗ X2 + ··· + an ∗ Xn. Since each Xi
P
has distribution S(p) and they are mutually independent,
by the stability property, the counter value Y1 is distributed
i |ai|p)1/pX, or in other words, ||S||pX, where X is
as (
distributed as S(p). Therefore, counters Y1, Y2, ..., Yl are
i.i.d. draws from distribution ||S||pX. At this point, we can
P
see that the data streaming algorithm is able to “modulate”
i |ai|p)1/p, into these
the signal we would like to estimate, (
counter values.
Now the question is how to extract this signal from these
counter values? The approach proposed in [13] is to use the
following median estimator for ||S||p:
Λ((cid:6)Y ) ≡ median(|Y1|, . . . ,|Yl|)
DM edp
.
(1)
We use Λ(.) to denote the operator that extracts Lp norm
estimates from sketches, and we will be using it in other
contexts. Here median(|Y1|, . . . ,|Yl|) denotes the sample me-
dian of the absolute values of the counter values; DM edp de-
notes the distribution median of S+(p). By S+(p) we mean
the probability distribution of a random variable |X|, where
X has distribution S(p). We denote it as S+(p) because its
p.d.f is exactly twice the positive half of the p.d.f for S(p),
due to the symmetry of S(p). So DM edp is the unique (in
this case) x0 value such that P r[|X| > x0] = 0.5, where X
3Note that our implementations of these hash functions are
totally diﬀerent than in [12, 13] to make them run much
faster, which we will describe in Section 7.
has distribution S(p). Note that, due to the symmetry of
S(p), DM edp is exactly the three-quarter quantile of S(p).
Although there is no closed form for DM edp for most of the
p values, we can numerically calculate it by simulation, or
we can use a program like [20].
Intuitively, the correctness of this estimator can be justi-
ﬁed as follows. Since Y1/||S||p, ..., Yl/||S||p are i.i.d. random
variables with distribution S(p), taking absolute value gives
us i.i.d. draws from S+(p). For large enough l, their median
should be close to the distribution median of S+(p). There-
fore, we simply divide median(|Y1|, . . . ,|Yl|) by the distribu-
tion median of S+(p) to get an estimator of ||S||p. We have
to take absolute values because the distribution median of
S(p) is 0 due to its symmetry. In the next section we will
analyze the relative error of this estimator.
Indyk’s estimator for the Lp norm is based on the prop-
erty of the median. We ﬁnd, however, that it is possible to
construct estimators based on other quantiles and they may
even outperform the median estimator, in terms of estima-
tion accuracy. However, since the improvement is marginal
for our parameters settings, we stick to the median estima-
tor.
3.3 Error analysis for Lp norm estimator
tor for ||S||p in (1).
3.3.1
In this section we analyze the performance of the estima-
(, δ) bound for p = 1
Here we basically restate Lemma 2 and Theorem 3 from
Indyk [13] with the constants spelled out. We arrived at
this by using Chernoﬀ bounds to derive the constant in his
Claim 2.
Theorem 1. (Indyk, [13]) Let (cid:6)X = (X1, . . . , Xl) be i.i.d.
δ )/2,  
1 − δ. Thus (1) gives an (, δ) estimator for p = 1.
Example: for p = 1, δ = 0.05,  = 0.1, we get l = 2951.
This is a very loose bound in the sense that we need a very
large l. This motivates us to resort to the asymptotic nor-
mality of the median for some approximate analysis.
3.3.2 Asymptotic (, δ) bound for p ∈ (0, 2]
Theorem 2. Let f = fS+(p), m = DM edp, l = (
zδ/2
2mf (m) )2,
then (1) gives an estimator with asymptotic (, δ) bound. za
is the number such that for standard normal distribution Z
we have P r[Z > za] = a.
Proof is in the Appendix. This result is in the same order
of O(1/2) as the Chernoﬀ result, but the coeﬃcient is much
smaller as shown below.
Example: For p = 1, δ = 0.05,  = 0.1, we get m =
1, f (m) = 1/π, zδ/2 = 2, l = 986. Compare with l = 2951
from the previous section.
For p = 1.05, we get m = 0.9938, f (m) = 0.3324, l = 916.
For p = 0.95, we get m = 1.0078, f (m) = 0.3030, l = 1072.
Our simulations show that these are quite accurate bounds.
We can see that mf (m) does not change much in a small
neighborhood of p = 1. Since we are only interested in p in
a small neighborhood of 1, for rough arguments we may use
mf (m) at p = 1, which is 1/π.
4. SINGLE NODE ALGORITHM
In this section, we show how our sketch works for esti-
mating the entropy of the traﬃc stream on a single link;
Estimation of OD ﬂow entropy based on its intersection mea-
surable property (IMP) is the topic of the next section. We
ﬁrst show how to approximate the function x ln(x) by a lin-
ear combination of at most two functions of the form xp,
p ∈ (0, 2]. After that we analyze the combined error of this
approximation and Indyk’s algorithm.
4.1 Approximating x ln x
Our algorithm computes the entropy of a stream of ﬂows
by approximating the entropy function x ln x by a linear
combination of expressions xp, p ∈ (0, 2]. In this section we
demonstrate how to do this approximation up to arbitrary
relative error . To make the formula simpler we use the
natural logarithm ln x instead of log2 x, noting that changing
the base is simply a matter of multiplying by the appropriate
constant, thus having no eﬀect on relative error.
∈ O( ln N√
Theorem 3. For any N > 1,  > 0, there exists α ∈
 ), such that f (x) = c(x1+α − x1−α)
(0, 1), c = 1
approximates the entropy function x ln x for x ∈ (1, N ] within
2α
relative error bound , i.e., | f (x)−x ln x
Proof. Using the Taylor expansion,
| ≤ .
x ln x
xα
= eα ln x
= 1 + α ln x +
(α ln x)2
2!
+
(α ln x)3
3!
+ ··· ,
we get that
f (x) = x ln x +
α2x ln3 x
3!
+
α4x ln5 x
5!
+ ···
Rewriting in terms of the relative error, we get that
r(x, α) ≡ f (x)
+ ···
x ln x
− 1 =
(α ln x)4
(α ln x)2
+
5!