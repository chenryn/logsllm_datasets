have it enabled by default.
Nevertheless, we veriﬁed through experiments in the
lab that apart from the encryption which is enabled by
default, the delivery mechanism and overall behaviour
of the app remains the same with newer devices with
modern browsers and the latest version of the app.
In the weblogs, each segment download is generated
from the client with a separate HTTP request and there-
fore we obtain a new entry for each new video chunk.
From the list of metrics mentioned above, we also com-
pute the chunk size and the chunk time that indicates
the time when a video chunk arrives at the client, since
in our experiments we found they bring relevant infor-
mation to model the QoE impairments. The complete
list of the metrics extracted from the traﬃc can be found
in Table 1.
The ﬁnal set consists of approximately 390,000 unique
video sessions. However, only 3% of these are adaptive
streaming sessions. This imbalance is expected since
we are able to observe traﬃc from mainly legacy de-
vices and video players which do not support the more
recently adopted adaptive technology.
For the methodology of the stall detection we take
the entire dataset, while for the development of the
average representation and the representation quality
switch detection we only keep the videos that made use
of adaptive streaming.
Network Features
Ground Truth (URI)
chunk resolution
stall count
stall duration
video session ID
minimum RTT
average RTT
maximum RTT
Bandwidth-delay product
average bytes-in-ﬂight
maximum bytes-in-ﬂight
% packet loss
% packet retransmissions
chunk size
chunk time
Table 1: Metrics that we extract from the opera-
tor’s web logs (left column) and the ones that are
reverse engineered from the request URIs (right
column). The features (left) are available for
encrypted and non-encrypted ﬂows whereas the
ground truth is only available for non-encrypted
sessions.
3.2 Ground Truth
From the meta-data that are passed as parameters in
the URIs of the HTTP requests we are able to collect the
ground truth that will be used in the evaluation phase.
In more detail, these parameters carry three main types
of statistics, i.e. generic device and player stats, content
stats and playback stats [13].
The generic stats include information about the user’s
device such as OS, locale, screen resolution, player type
and so on. One of the most important parameters here,
is the unique video session ID. This ID is a 16-character
hash that is randomly generated and it is unique to each
session. We use it to identify and group together all the
weblogs that belong to the same video session.
The content stats are extracted from the HTTP re-
quests for downloading the individual video segments.
One of the the parameters in this group is the ‘content
type’, which indicates if the segment contains video or
audio content and the multimedia container that was
used to encode it, e.g. MP4, FLV or WebM. ‘Itag’ is
another parameter which is used to specify the bit-rate,
frame-rate and resolution of the segment, which we use
to obtain the ground truth for the changes in represen-
tation quality throughout the session.
Finally, the playback statistics are included in the
statistical reports that are periodically sent from the
player to Google servers during the playback. Each re-
port contains information that summarizes the progress
of the playback since the previous report was gener-
ated. Diﬀerent ﬂags are used in the reports to specify
if the video has successfully loaded, if the playback has
started, paused or stopped and if there was a stall and
how long it lasted. These indicators allow us to dis-
cover if a video was played throughout or abandoned
and more important, identify the frequency and dura-
tion of stalls.
Out of the information that is available in the unen-
crypted data, we only use the chunk resolution, the stall
count and duration and the video session ID (Table 1).
These features will be used as the ground truth for
training the detection models in Section 4. After the
completion of training phase, the access to the ground
truth from unencrypted traﬃc will no longer be required
and even if YouTube removes this information or de-
ploys encryption for all sessions, the methodology will
still be applicable.
3.3 Data Preparation
Before starting the analysis, we ensure that any logs
that correspond to cached and/or compressed content
by the proxy are removed from the dataset.
Next, after the ground truth for the stalls and repre-
sentation switches is extracted, all the logs that belong
to the same video session are identiﬁed by the common
session ID and are then grouped together.
Thus, each entry in the dataset corresponds to a unique
video session which includes information about the total
number of stalls and their duration, as well as the char-
acteristics of each chunk such as the quality representa-
tion, size, download time-stamp, but also the transport
layer statistics like RTT, loss, re-transmissions, BDP
and bytes-in-ﬂight for each chunk download.
5164. BUILDING THE DETECTION
FRAMEWORK
Our approach involves ﬁrst the development and test-
ing of the detection framework with unencrypted data.
As soon as we verify that the constructed models can
leverage the cleartext dataset, we can proceed to test
the framework with data from encrypted video streams.
As mentioned in Section 2, there are three main types
of impairments which may cause the degradation of
poor video QoE, the frequency and duration of stalls,
the session’s Average Representation Quality and the
Representation Quality Variation [10].
The initial delay is not considered as part of our video
QoE model given its small contribution on the overall
user experience as explained in 2.1.
In this section we describe the process of identifying
from the limited number of metrics that are oﬀered by
the encrypted traﬃc, those that are the most signiﬁcant
for creating predictive models to detect each of the three
types of impairments. An important part of this process
is the feature construction that allows the generation of
new more powerful features from the already existing
ones.
Next, we show that there is a diﬀerent set of met-
rics that better describes each type of impairment and
contributes more information to the detection model.
In order to generate predictive models for detecting
the level of stalling and the average representation, we
use Machine Learning (ML) and in particular the Ran-
dom Forest algorithm and 10-fold cross-validation.
Classiﬁcation is preferred over regression given that
we divide the data in discrete classes in both scenar-
ios and the models are required to identify in which
class each video session belongs based on the amount of
stalling or the level of the average representation.
4.1 Stall Detection
Feature Construction
From the traﬃc features described in Section 3 (Ta-
ble 1), we generate summary statistics, i.e. max, min,
mean, standard deviation, 25th, 50th and 75th per-
centiles for each of the metrics, resulting in 70 new met-
rics.
Among all the performance metrics that we take into
consideration, the chunk size is one of the most impor-
tant for detecting stalls.
If we take an example of a
video session were stalling has occurred (Figure 1), we
can see the signiﬁcant changes in the chunk size when
the two events take place at the third and the seven-
teenth second of the video session.
More speciﬁcally, whenever there is an outage on the
player’s buﬀer that results in a stall, the player will
request small chunks which can be downloaded much
faster so that the buﬀer will be ﬁlled as soon as possi-
ble and the video playback can resume. Then the size
of the chunks will gradually increase and remain at a
maximum value during the steady state as long as no
further issues occur.
Therefore, we understand that we can signiﬁcantly
improve the accuracy of the stall detection model by
including the sizes of the chunks in our feature set.
Figure 1: Changes in chunk sizes in a video ses-
sion with stalls.
After all the required features have been generated,
the dataset is then split into sessions without stalls and
sessions where at least one stall has occurred. The infor-
mation regarding the number of stalls observed during
a video session and their duration, is the ground truth
which is extracted from the meta-data of URIs as men-
tioned in Section 3.
Figure 2 (left) illustrates the distribution of the num-
ber of stalls that occurred per video session. We observe
that 12% of all the sessions have suﬀered from rebuﬀer-
ing events, while about 8% was aﬀected by more than
1 event.
Figure 2: ECDF of number of stalls (left) and
rebuﬀering ratio (right) per session
Labelling
Next, we use the information from the ground truth
to label the data and create a predictive model. To
do this, ﬁrst we calculate the re-buﬀering ratio (RR)
for each video session as the ratio of the sum of the
duration tstall k of each of the total K stalls over the
duration of the entire session ttotal (eq. 1)
(cid:80)K
k=1 tstall k
ttotal
RR =
(1)
The sessions are then labelled according to the rule
below. The deﬁnition of three levels of stalling, i.e. no
05101520253035relative time (s)02004006008001000120014001600segment size (KB)360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls360pstalls517stalling, mild and severe, allows a more detailed view of
the degree to which the stalls aﬀect the user.
 “no stalling” :
“mild stalling” :
“severe stalling” :
Stall labels :
RR = 0
0 > RR ≥ 0.1
RR > 0.1
The RR threshold for distinguishing mild and severe
stalling is set to 0.1, since in their work [14] Krishnan
et al. have shown that when the RR is over 0.1, the
severity of the stalling causes such a quality degradation
that leads the users to abandon the video.
Figure 2 (right) shows the distribution of the RR per
video session. We can observe that the sessions with RR
equal or greater thatn 0.1 correspond to approximately
10% of the distribution.
Feature Selection
We then proceed to apply Feature Selection (FS) us-
ing the Correlation-based Feature Subset Selection (Cf-
sSubsetEval) with the Best First search algorithm to
reduce the number of features from 70 to the following
four, BDP mean, packet re-transmissions max, chunk
size min and the chunk size standard deviation.
The output of the feature selection algorithm reveals
that there are three important factors that are corre-
lated with stalling, BDP which is equivalent to through-
put, number of retransmissions and chunk size. The
limited throughput and increased number of retrans-
mitted packets are QoS metrics which are performance
indicators of congested networks and/or networks with
limited bandwidth where stalling is more likely to occur.
Table 2 shows the gain of each of the features that
were obtained after FS was applied and their respective
information gains. The information gain represents the
contribution of each feature in the construction of the
predictive model. Features with higher information gain
have a higher correlation with the problems that we
want the model to detect and are used more frequently
by the classiﬁer.
The higher gains for the minimum and standard devi-
ation of the chunk size indicate that both these features
carry important information for detecting if a video suf-
fered from stalls or not. Smaller chunk sizes correspond
to lower quality streams that are frequently selected by
the user or the adaptive algorithm in the presence of
poor network conditions and limited bandwidth.
On the other hand, larger deviation of the size of
chunks is related to sudden changes in the network’s
performance that in turn lead to quality switches during
playback. In both cases the videos which are streamed
under these conditions are more prone to stalling due
to buﬀer outages.
The BDP and number of packet retransmissions have
a more clear and direct correlation to low bandwidth
and congestion scenarios where the speed at which the
video buﬀer is ﬁlled is limitted and therefore there is a
much higher probability of stalling. These metrics can
be beneﬁcial specially for cases of traditional streaming
where the video is downloaded over a single connection.
info. gain
feature
0.45
0.25
0.18
0.12
chunk size minimum
chunk size std. deviation
BDP mean
packet retransmissions max
Table 2: Features and respective gains for the
stall detection model.
Training and Testing the Predictive Model
In order to avoid biasing the results during the test
phase, we balance the number of instances among the
three classes before training the classiﬁer. The instances
in the classes are then restored to their original numbers
for testing.
Overall, the classiﬁer is able to make predictions with
93.5% accuracy. The proposed stall detection model is
a signiﬁcant improvement over previous approaches [15]
where the achieved accuracy was approximately 84% for
a binary classiﬁcation. In contrast, our model not only
achieves much higher accuracy but it also can predict
the severity of the stalling that aﬀected the user.
The output of the test phase of the model in terms of
True Positives (TP), False Positives (FP), Precision and
Recall can be found in Table 3, while the corresponding
confusion matrix is shown in Table 4.
Precision is calculated as the ratio of TP over TP
and FP and corresponds to the accuracy with which a
certain problem is predicted. Recall is equal to the ratio
of TP divided by the total instances in this class and
measures the models’s ability to correctly identify the
QoE issue of a video session from the data set.
From the confusion matrix we can see that the clas-
siﬁcation errors occur between instances without stalls
and those with mild stalls but also between mild and
severe. However, signiﬁcantly fewer misclassiﬁcations
happen between the severe and “no stall” classes.
Therefore, it is straightforward that the errors oc-
cur due to the classiﬁer’s inability to correctly identify
marginal cases where the RR is close to the RR thresh-
olds we deﬁned for labelling the instances. Hence, in-
stances with RR slightly over zero can be falsely pre-
dicted as healthy sessions without stalls and thus in-
creasing the number of FP. The same applies for cases
where the RR is marginally over 10%, which can be
identiﬁed as mildly problematic and vice versa.
In more detail, although some marginal instances be-
long to diﬀerent classes, they often have similar charac-
teristics, such as throughput delay and loss. The sim-
ilarity between instances of diﬀerent classes can cause
confusion to the classiﬁer resulting to the generation of
FP.
From Table 3, we can see that the healthy sessions are
predicted with higher Precision and Recall when com-