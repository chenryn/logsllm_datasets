tools necessary to quickly pinpoint an attack and identify
the affected resources, but unfortunately the inspection of
system-layer audit logs in a large cluster poses a needle-
in-a-haystack problem. While we drew attention to the
attack subgraph in Figure 3a, in practice this exercise can
be extraordinarily tedious and error-prone [47], [38]. As
demonstrated here, provenance graph visualization can assist
in forensic exploration [23], [56], but such techniques are
designed for a single-host and thus lack the means to ﬁlter
the inherent redundancy across nodes.
– Storage Overhead. The amount of audit data generated
on even a single host can be enormous, around 3.18GB/-
day(server) and 1.2GB/day (client), as shown by previous
studies [51], [36], [47]. When considering that such records
may need to be stored in perpetuity for post-facto causal
analysis, it immediately becomes clear that audit logs rep-
resent a storage burden on the order of terabytes. While prior
work has made inroads at reducing storage burden for single
hosts [54], [51], [69], [25], even state-of-the-art systems lack
the scalability required for auditing large clusters.
– Network Overhead. Beyond the cost of local storage, cluster
auditing requires aggregation of system activity to a central
master node. However, it is immediately apparent that a
na¨ıve approach that transmits all system-layer audit records
x.x.x.xOther IP verticesx.x.x.xOther IP verticesftp listenerftp workerbashOther  ftp workersmysql/etc/login.defs/usr/lib64/libpq.so/etc/ld.sobashMalicious ﬁley.y.y.yPersistent shelly.y.y.yAttack Provenancepickupvar/postﬁx/maiildropHost ProvenanceftpOther ﬁle verticesOther ftp listeners/var/lib/mysql/dbNode 1Node 2mysqld/dev/urandom 172.17.0.2Other VerticesNode nOther IP verticesftpftp workerx.x.x.x/24/etc/login.defmysql/dev/urandom/var/lib/mysql/dbbashMalicious ﬁley.y.y.yPersistent shelly.y.y.yAttack Provenanceftp listenerbashConﬁdence levelLegend1510*mysqld172.17.0.0/24x.x.x.x/24/usr/lib64/libpq.so/etc/ld.soFig. 4: An example graph on the left and graph grammar production
rules on the right which accept that graph. S, T, and V represent
non-terminals while a,b,c, and d are terminals.
to a central node would impose unacceptable network cost.
This is especially true in the case of clusters that are already
deluged with internal network trafﬁc [21].
Winnower’s High-level Idea. We observe that applications
replicated on multiple nodes will produce highly homogeneous
audit logs across executions. As applications will be deployed
with nearly-identical conﬁgurations (e.g., ﬁlesystems, launch
sequences), we can expect the resultant provenance graphs
to be similar both structurally (i.e., graph connectivity) and
semantically (i.e., graph labels). on a per-application (or, per-
container) basis. Broadly speaking, our goal is to generate
consensus across all nodes to produce a model of application
behavior like the one shown in Figure 3b. In contrast
to
3a, redundancy between nodes has been eliminated, and each
activity is shown only once. However, a conﬁdence level marks
the level of consensus that was reached between application
instances (in 3a MySQL has 5 instances). As the attack
occurred on a single node, its conﬁdence level is low, and thus
represents anomalous activity that can easily be identiﬁed by
the administrator. Thus, the consensus model is both efﬁcient
to transmit and further, retains the necessary information to
identify the attack.
C. Graph Grammars
To facilitate the creation of a cluster-wide provenance
model for worker execution, in this work we present a novel
adaptation of Discrete Finite Automata (DFA) learning tech-
niques [13]. As DFAs are equivalent to regular grammars [32],
this approach is sometimes referred to as graph grammar
learning. There have been different formulations of graph
grammars that broadly refer to classes of standard grammars
applied to graphs. In a standard grammar, a language of strings
deﬁnes a set of rules such that a given string is considered a
member of the grammar if it can be constructed from the rules.
It is intuitive to extend the notion of standard grammars from
strings to graphs, such that a graph belongs to a grammar if it
can be constructed from a set of grammatical rules represented
in the form of L := R where L is the pattern subgraph (or left-
hand side) which can be replaced by R subgraph (or right-hand
side). An example of such grammar and a graph that belongs
to it is shown in Figure 4.
Graph grammar systems support two important operations:
induction and parsing. Induction algorithms provide a way to
learn a new grammar from a set of one or more example
graphs. Parsing is a membership test that veriﬁes whether
an instance graph can be constructed from a given grammar.
Graph grammar learning is not a deterministic process, as
multiple grammars can parse the same instance of the graph.
As a result, we need heuristic techniques to select an accept-
able grammar. While there are strategies for choosing the best
4
grammar during induction, we make use of the Minimum
Description Length (MDL) heuristic [40], [17]. The MDL
heuristic formalizes the notion that the simplest explanation
of data is the best explanation of data. MDL is deﬁned by the
following equation:
DL(G, S) = DL(S) + DL(G|S)
(1)
where G is an input graph, S is a model (grammar) of the input
graph, (G|S) is G compressed with S, and DL() returns the
description length of the input in bits, The MDL heuristic says
the best S minimizes DL(G, S); in other words, the optimal
S minimizes the space required to represent the input graph.
III. SYSTEM DESIGN
A. Threat Model & Assumptions
Our approach is designed with consideration for a data
center running a distributed application that has been replicated
on hundreds or thousands of Worker nodes. Workers may run
as containers, virtual machines, or bare metal hosts; while our
prototype system is implemented for Docker containers (see
§IV), our methodology is agnostic to the workers’ platform.
We require only that each worker is associated with an auditing
mechanism that records the actions of the node. In addition
to worker nodes, the data center features one Monitor node
that is operated by a system administrator. The monitor will
communicate with worker nodes to obtain audit records of
their activities.
The attack surface that we consider in this work is that of
the worker nodes. An adversary may attempt to gain remote
control of a worker by exploiting a software vulnerability in the
distributed application (see §II-A), or may have used a market
such as Docker Store to distribute a malicious application
that contains a backdoor. Once the attacker gains control of
a worker, they may eavesdrop on legitimate user trafﬁc or to
make use of the worker’s compute resources to perpetrate other
misdeeds. In the case of virtualized workers, the attacker’s goal
may be to break isolation and gain a persistent presence on the
underlying machine.
An important consideration for any auditing system is the
security of the recording mechanism. This is because it is
common practice for system intruders to tamper with audit
logs to cover their tracks. While log integrity is an important
goal, it is orthogonal to the aims of this system. Therefore,
we assume the integrity of the workers’ audit mechanisms.
In the case of kernel-based audit mechanisms (e.g., auditd),
kernel hardening techniques (e.g., enabling SELinux) can be
deployed to increase the complexity of a successful attack.
B. Design Goals
The limitations outlined in §II-B motivate the following
system design goals:
– Generality. Winnower design and techniques should be
independent of underlying platform (e.g. containers, VM,
etc) and applications used by the compute clusters.
– Minimal Log Transmission. Winnower
should prevent
worker nodes from sending redundant audit records to
the central node i.e. only transmits the minimum amount
abcdbcV:=bcS:=TVT:=a|adS:=Vof information required to adequately describe unique or
anomalous events within the cluster.
– Concise Graphs. Winnower generated provenance graphs
on central node should be concise i.e. capturing aggregated
cluster-wide activities with any anomalous behaviour visible
in graphs.
– Support Cluster Auditing. Winnower should support dis-
tributed querying worker nodes for complete attack tracing
and local policy monitoring in the cluster.
C. System Overview
Winnower acts as a distributed auditing layer that resides
on top of individual worker nodes’ auditing mechanisms. The
core contributions of Winnower are three functions that enable
efﬁcient aggregation of audit data at a central monitoring node:
1) Provenance Graph Abstraction, in which workers abstract
provenance graphs to remove instance-speciﬁc and non-
deterministic information (§III-D).
2) Provenance Graph Induction, in which the worker gener-
ates behavioral models and then Monitor aggregates worker
models into a single uniﬁed model and send them back to
all workers (§III-E).
3) Provenance Model Incremental Updates, in which workers
check to see if newly generated provenance records are
described by the global model. If and only if they are
not already in the model, the workers transmit the model
updates back to the central node (§III-F).
Using aforementioned functions, our aggregation technique
works as follows: First, Winnower uses an application-aware
provenance tracker on each node to ﬁnd and separate ho-
mogeneous audit
logs from replicated applications. In the
attack scenario we discussed in §II-B, Winnower separates
ProFTPD and MySQL logs. Then, to remove instance-speciﬁc
information from homogeneous audit logs, Winnower applies
provenance graph abstraction function locally on each worker
node. In Figure 3a, different IP addresses are present in socket
vertices attached to “ftp listener” process vertex. However,
exact IP address in vertices is not important to extract be-
haviour of the application and therefore, we can abstract
it before model construction. After that, Winnower applies
provenance graph induction function to remove redundancy
and generate behavioral models. In Figure 3a, “ftp” process
vertex spawns several “ftp listener” process vertices. As they
represent semantically equivalent behaviour (causal path is
same), we can combine them into a single vertex as shown
in Figure 3b. Finally, Winnower prevents worker nodes from
transmitting redundant audit records using provenance graph
incremental update function and send only the graph grammar
model’s updates to the central node.
In addition to these core functions, Winnower provides a
fully-realized distributed provenance tracing framework that
supports forward and backward tracing of system events as
well as policy-based automated system monitoring. We de-
scribe these features in §IV with greater details.
D. Provenance Graph Abstraction
The core function of Winnower is to ingest the provenance
graphs of different worker nodes and output a generic model
5
that summarizes the activity of those nodes. However, even if
all nodes are clones of the same image, we can expect that a va-
riety of instance-speciﬁc ﬁelds and descriptors will be present
in each worker’s graph. For example, each web service worker
will receive web requests from different remote hosts, causing
different IP addresses to appear in their provenance graph.
We would also expect instance-speciﬁc identities assigned to
each worker such as a host name or dynamically-assigned IP
address. While these details are important when inspecting an
individual node, they not useful to an administrator attempting
to reason about a distributed system in aggregate. In fact,
these instance-speciﬁc ﬁelds will frustrate any attempts to
build a generic application behaviour model because equivalent
events have been assigned different labels on different nodes.
Therefore, before attempting model generation we must ﬁrst
abstract away these instance-speciﬁc details.
To facilitate this abstraction, we group the different ﬁelds
found in provenance vertex labels into one of three classes,
handling each as follows: equivalence classes contain instance-
speciﬁc information and are abstracted using summarization
immutable classes will
techniques prior to model building;
not contain instance-speciﬁc information and therefore are not
removal classes are simply removed from
changed; ﬁnally,
the vertex label prior to graph comparison. Below, we explain
classiﬁcation of each ﬁeld associated with each provenance
principle (i.e., activity, artifact, agent).
Activities. Activity vertices consist of ﬁve different labels:
Process Name, PID, Timestamp, Current Directory (CWD),
and Command line Args. Because we expect all workers
to follow the same general workﬂow, process name, CWD,
and command line arguments are handled as immutable; in
other words, a deviation in either of these ﬁelds will be
visible in the ﬁnal model. PIDs and Timestamps can both be
inﬂuenced by non-determinism and vary between executions,
and are therefore removed. In Figure 5, pid is removed from
Activity vertices after graph abstraction step. For brevity, we
omit description of other environment variables, which can be
handled similarly.
Artifacts. Artifact vertices are further categorized into sub-
types based on data types. We describe our general approach
with consideration for ﬁle and socket artifacts below, omitting
other artifacts such as IPC for brevity:
• File Artifacts: File subtype vertex consists three labels: File
Path, Operation (i.e., read/write/create) and Version. The
version ﬁeld is incremented each time data is written to
an argument, which is highly dependent on dynamic events
such as network activity and is therefore removed. The oper-
ation label is also removed for simplicity, as this information
is already encoded in the edge labels of the graph. The most
important ﬁeld, ﬁle path, is handled differently depending
on the class of ﬁle: (a) Core-system Files: these ﬁles are
common across all workers and therefore do not need to
be abstracted, so we scan the node image to create the set
sysF iles and treat these ﬁles as immutable. In Figure 5a ﬁle
path label /usr/lib/libpq.so vertex is not removed after
abstraction. (b) Temporary Files: temporary ﬁles are those
ﬁles who only interact with a single process throughout their
entire life cycle. As noted in [51], these ﬁles do not have
meaning when attack tracing, and can therefore be removed.
Fig. 5: Applying graph abstraction and graph grammar induction on FTP application provenance graph.
In Figure 5a, artifact ﬁle path label /output/file1 attached
to “ftp worker” process vertex is removed after abstraction.
(c) Equivalent Files: all the other ﬁles are treated as the
equivalence class. For a given activity, when more than a
conﬁgurable threshold (τF ile) of equivalent ﬁles are present,
they collapsed into one vertex that is labeled as the most
speciﬁc common ﬁle path across all ﬁle paths.
• Socket Artifacts: The socket subtype vertex is described
by an IP Address ﬁeld. Web services exchange messages
over the network with a wide variety of remote clients. The
reported IP addresses of the remote clients will lead to many
subgraphs within the provenance graph that all describe the
same workﬂow. To provide an easy-to-understand generic
model, it is important that the model not grow with the
number of remote connections. Therefore, the IP address
ﬁeld is treated as an equivalence class. For a given activity,
when more than a conﬁgurable threshold (τSock) of remote
connections are present, they collapsed into one vertex that
is labeled as the most speciﬁc common subnet across all IP
addresses. An example of this is shown in Figure 5a; the
artifact that was generated by ftp worker represents many
network transmissions in the 192.168.0.0/24 subnet mask.
Agents. Agents are described by a UID ﬁeld. Because we