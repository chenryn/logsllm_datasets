in a private registry rather than pushing them to a public registry like Docker
Hub. Use kubectl create secret docker-registry.
For your own application-specific secrets, you should use the generic secret type.
 Once you’ve defined a secret, you can make it available to your pods in one of
two ways:
 As files mounted in the filesystem inside your pods. For example, if you mounted
the secret defined in listing 11.11 under the path /etc/secrets/db, then you
Listing 11.11
Kubernetes secret example
The kind field indicates 
this is a secret.
Give the secret a name 
and a namespace.
The secret has two fields with 
Base64-encoded values.
417
Managing service credentials
would end up with two files inside your pod: /etc/secrets/db/username and
/etc/secrets/db/password. Your application can then read these files to get
the secret values. The contents of the files will be the raw secret values, not the
Base64-encoded ones stored in the YAML.
 As environment variables that are passed to your container processes when they
first run. In Java you can then access these through the System.getenv(String
name) method call.
TIP
File-based secrets should be preferred over environment variables. It’s
easy to read the environment of a running process using kubectl describe
pod, and you can’t use environment variables for binary data such as keys.
File-based secrets are also updated when the secret changes, while environ-
ment variables can only be changed by restarting the pod.
Listing 11.12 shows how to expose the Natter database username and password to the
pods in the Natter API deployment by updating the natter-api-deployment.yaml file. A
secret volume is defined in the volumes section of the pod spec, referencing the
named secret to be exposed. In a volumeMounts section for the individual container,
you can then mount the secret volume on a specific path in the filesystem. The new
lines are highlighted in bold.
apiVersion: apps/v1
kind: Deployment
metadata:
  name: natter-api-deployment
  namespace: natter-api
spec:
  selector:
    matchLabels:
      app: natter-api
  replicas: 1
  template:
    metadata:
      labels:
        app: natter-api
    spec:
      securityContext:
        runAsNonRoot: true
      containers:
        - name: natter-api
          image: apisecurityinaction/natter-api:latest
          imagePullPolicy: Never
          volumeMounts:
            - name: db-password        
              mountPath: "/etc/secrets/database"    
              readOnly: true
          securityContext:
            allowPrivilegeEscalation: false
Listing 11.12
Exposing a secret to a pod
The volumeMount name must 
match the volume name.
Specify a mount path 
inside the container.
418
CHAPTER 11
Securing service-to-service APIs
            readOnlyRootFilesystem: true
            capabilities:
              drop:
                - all
          ports:
            - containerPort: 4567
      volumes:
        - name: db-password     
          secret:
            secretName: db-password      
You can now update the Main class to load the database username and password from
these secret files rather than hard coding them. Listing 11.13 shows the updated code
in the main method for initializing the database password from the mounted secret
files. You’ll need to import java.nio.file.* at the top of the file. Open the Main
.java file and update the method according to the listing. The new lines are high-
lighted in bold.
var secretsPath = Paths.get("/etc/secrets/database");                 
var dbUsername = Files.readString(secretsPath.resolve("username"));   
var dbPassword = Files.readString(secretsPath.resolve("password"));   
var jdbcUrl = "jdbc:h2:tcp://natter-database-service:9092/mem:natter";
var datasource = JdbcConnectionPool.create(
    jdbcUrl, dbUsername, dbPassword);           
createTables(datasource.getConnection());
You can rebuild the Docker image by running7
mvn clean compile jib:dockerBuild
then reload the deployment configuration to ensure the secret is mounted:
kubectl apply -f kubernetes/natter-api-deployment.yaml
Finally, you can restart Minikube to pick up the latest changes:
minikube stop && minikube start
Use kubectl get pods -n natter-api --watch to verify that all pods start up correctly
after the changes.
Listing 11.13
Loading Kubernetes secrets
7 Remember to run eval $(minikube docker-env) if this is a new terminal session.
The volumeMount name must 
match the volume name.
Provide the name of 
the secret to expose.
Load secrets
as files from
the filesystem.
Use the secret values to 
initialize the JDBC connection.
419
Managing service credentials
SECURITY OF KUBERNETES SECRETS
Although Kubernetes secrets are easy to use and provide a level of separation between
sensitive credentials and other source code and configuration data, they have some
drawbacks from a security perspective:
 Secrets are stored inside an internal database in Kubernetes, known as etcd. By
default, etcd is not encrypted, so anyone who gains access to the data storage
can read the values of all secrets. You can enable encryption by following the
instructions in http://mng.bz/awZz.
WARNING
The official Kubernetes documentation lists aescbc as the stron-
gest encryption method supported. This is an unauthenticated encryption
mode and potentially vulnerable to padding oracle attacks as you’ll recall
from chapter 6. You should use the kms encryption option if you can,
because all modes other than kms store the encryption key alongside the
encrypted data, providing only limited security. This was one of the find-
ings of the Kubernetes security audit conducted in 2019 (https://github
.com/trailofbits/audit-kubernetes).
Managing Kubernetes secrets
Although you can treat Kubernetes secrets like other configuration and store them in
your version control system, this is not a wise thing to do for several reasons:
 Credentials should be kept secret and distributed to as few people as possi-
ble. Storing secrets in a source code repository makes them available to all
developers with access to that repository. Although encryption can help, it is
easy to get wrong, especially with complex command-line tools such as GPG.
 Secrets should be different in each environment that the service is deployed
to; the database password should be different in a development environment
compared to your test or production environments. This is the opposite
requirement to source code, which should be identical (or close to it) between
environments.
 There is almost no value in being able to view the history of secrets. Although
you may want to revert the most recent change to a credential if it causes an
outage, nobody ever needs to revert to the database password from two
years ago. If a mistake is made in the encryption of a secret that is hard to
change, such as an API key for a third-party service, it’s difficult to completely
delete the exposed value from a distributed version control system.
A better solution is to either manually manage secrets from the command line, or
else use a templating system to generate secrets specific to each environment.
Kubernetes supports a templating system called Kustomize, which can generate per-
environment secrets based on templates. This allows the template to be checked
into version control, but the actual secrets are added during a separate deployment
step. See http://mng.bz/Mov7 for more details.
420
CHAPTER 11
Securing service-to-service APIs
 Anybody with the ability to create a pod in a namespace can use that to read the
contents of any secrets defined in that namespace. System administrators with
root access to nodes can retrieve all secrets from the Kubernetes API.
 Secrets on disk may be vulnerable to exposure through path traversal or file expo-
sure vulnerabilities. For example, Ruby on Rails had a recent vulnerability in its
template system that allowed a remote attacker to view the contents of any file
by sending specially-crafted HTTP headers (https://nvd.nist.gov/vuln/detail/
CVE-2019-5418).
DEFINITION
A file exposure vulnerability occurs when an attacker can trick a
server into revealing the contents of files on disk that should not be accessible
externally. A path traversal vulnerability occurs when an attacker can send a
URL to a webserver that causes it to serve a file that was intended to be private.
For example, an attacker might ask for the file /public/../../../etc/secrets/db-
password. Such vulnerabilities can reveal Kubernetes secrets to attackers.
11.5.2 Key and secret management services
An alternative to Kubernetes secrets is to use a dedicated service to provide credentials
to your application. Secrets management services store credentials in an encrypted data-
base and make the available to services over HTTPS or a similar secure protocol. Typi-
cally, the client needs an initial credential to access the service, such as an API key or
client certificate, which can be made available via Kubernetes secrets or a similar
mechanism. All other secrets are then retrieved from the secrets management service.
Although this may sound no more secure than using Kubernetes secrets directly, it has
several advantages:
 The storage of the secrets is encrypted by default, providing better protection
of secret data at rest.
 The secret management service can automatically generate and update secrets
regularly. For example, Hashicorp Vault (https://www.vaultproject.io) can auto-
matically create short-lived database users on the fly, providing a temporary
username and password. After a configurable period, Vault will delete the
account again. This can be useful to allow daily administration tasks to run with-
out leaving a highly privileged account enabled at all times. 
 Fine-grained access controls can be applied, ensuring that services only have
access to the credentials they need.
 All access to secrets can be logged, leaving an audit trail. This can help to estab-
lish what happened after a breach, and automated systems can analyze these
logs and alert if unusual access requests are noticed.
When the credentials being accessed are cryptographic keys, a Key Management Service
(KMS) can be used. A KMS, such as those provided by the main cloud providers,
securely stores cryptographic key material. Rather than exposing that key material
directly, a client of a KMS sends cryptographic operations to the KMS; for example,
421
Managing service credentials
requesting that a message is signed with a given key. This ensures that sensitive keys
are never directly exposed, and allows a security team to centralize cryptographic ser-
vices, ensuring that all applications use approved algorithms.
DEFINITION
A Key Management Service (KMS) stores keys on behalf of applica-
tions. Clients send requests to perform cryptographic operations to the KMS
rather than asking for the key material itself. This ensures that sensitive keys
never leave the KMS.
To reduce the overhead of calling a KMS to encrypt or decrypt large volumes of data,
a technique known as envelope encryption can be used. The application generates a ran-
dom AES key and uses that to encrypt the data locally. The local AES key is known as a
data encryption key (DEK). The DEK is then itself encrypted using the KMS. The
encrypted DEK can then be safely stored or transmitted alongside the encrypted data.
To decrypt, the recipient first decrypts the DEK using the KMS and then uses the DEK
to decrypt the rest of the data.
DEFINITION
In envelope encryption, an application encrypts data with a local
data encryption key (DEK). The DEK is then encrypted (or wrapped) with a key
encryption key (KEK) stored in a KMS or other secure service. The KEK itself
might be encrypted with another KEK creating a key hierarchy.
For both secrets management and KMS, the client usually interacts with the service
using a REST API. Currently, there is no common standard API supported by all pro-
viders. Some cloud providers allow access to a KMS using the standard PKCS#11 API
used by hardware security modules. You can access a PKCS#11 API in Java through the
Java Cryptography Architecture, as if it was a local keystore, as shown in listing 11.14.
(This listing is just to show the API; you don’t need to type it in.) Java exposes a
PKCS#11 device, including a remote one such as a KMS, as a KeyStore object with the
type "PKCS11".8 You can load the keystore by calling the load() method, providing a
null InputStream argument (because there is no local keystore file to open) and pass-
ing the KMS password or other credential as the second argument. After the PKCS#11
keystore has been loaded, you can then load keys and use them to initialize Signature
and Cipher objects just like any other local key. The difference is that the Key object
returned by the PKCS#11 keystore has no key material inside it. Instead, Java will auto-
matically forward cryptographic operations to the KMS via the PKCS#11 API.
TIP
Java’s built-in PKCS#11 cryptographic provider only supports a few algo-
rithms, many of which are old and no longer recommended. A KMS vendor
may offer their own provider with support for more algorithms.
8 If you’re using the IBM JDK, use the name “PKCS11IMPLKS” instead.
422
CHAPTER 11
Securing service-to-service APIs
var keyStore = KeyStore.getInstance("PKCS11");     
var keyStorePassword = "changeit".toCharArray();   
keyStore.load(null, keyStorePassword);             
var signingKey = (PrivateKey) keyStore.getKey("rsa-key",  
        keyStorePassword);                                
var signature = Signature.getInstance("SHA256WithRSA");   
signature.initSign(signingKey);                           
signature.update("Hello!".getBytes(UTF_8));               
var sig = signature.sign();                               
A KMS can be used to encrypt credentials that are then distributed to services using
Kubernetes secrets. This provides better protection than the default Kubernetes con-
figuration and enables the KMS to be used to protect secrets that aren’t cryp-
tographic keys. For example, a database connection password can be encrypted with
the KMS and then the encrypted password is distributed to services as a Kubernetes
secret. The application can then use the KMS to decrypt the password after loading it
from the disk.
Listing 11.14
Accessing a KMS through PKCS#11
PKCS#11 and hardware security modules
PKCS#11, or Public Key Cryptography Standard 11, defines a standard API for inter-
acting with hardware security modules (HSMs). An HSM is a hardware device dedi-
cated to secure storage of cryptographic keys. HSMs range in size from tiny USB keys
that support just a few keys, to rack-mounted network HSMs that can handle thou-
sands of requests per second (and cost tens of thousands of dollars). Just like a KMS,
the key material can’t normally be accessed directly by clients and they instead send
cryptographic requests to the device after logging in. The API defined by PKCS#11,
known as Cryptoki, provides operations in the C programming language for logging
into the HSM, listing available keys, and performing cryptographic operations. 
Unlike a purely software KMS, an HSM is designed to offer protection against an
attacker with physical access to the device. For example, the circuitry of the HSM may
be encased in tough resin with embedded sensors that can detect anybody trying to
tamper with the device, in which case the secure memory is wiped to prevent com-
promise. The US and Canadian governments certify the physical security of HSMs
under the FIPS 140-2 certification program, which offers four levels of security: level
1 certified devices offer only basic protection of key material, while level 4 offers pro-
tection against a wide range of physical and environmental threats. On the other
hand, FIPS 140-2 offers very little validation of the quality of implementation of the
algorithms running on the device, and some HSMs have been found to have serious
software security flaws. Some cloud KMS providers can be configured to use FIPS
140-2 certified HSMs for storage of keys, usually at an increased cost. However,
most such services are already running in physically secured data centers, so the
additional physical protection is usually unnecessary.
Load the PKCS11 keystore 
with the correct password.
Retrieve a key object 
from the keystore.
Use the key to 
sign a message.
423
Managing service credentials
11.5.3 Avoiding long-lived secrets on disk
Although a KMS or secrets manager can be used to protect secrets against theft, the
service will need an initial credential to access the KMS itself. While cloud KMS pro-
viders often supply an SDK that transparently handles this for you, in many cases the
SDK is just reading its credentials from a file on the filesystem or from another source
in the environment that the SDK is running in. There is therefore still a risk that an
attacker could compromise these credentials and then use the KMS to decrypt the
other secrets.
TIP
You can often restrict a KMS to only allow your keys to be used from cli-
ents connecting from a virtual private cloud (VPC) that you control. This
makes it harder for an attacker to use compromised credentials because they
can’t directly connect to the KMS over the internet.
A solution to this problem is to use short-lived tokens to grant access to the KMS or
secrets manager. Rather than deploying a username and password or other static cre-
dential using Kubernetes secrets, you can instead generate a temporary credential
with a short expiry time. The application uses this credential to access the KMS or
secrets manager at startup and decrypt the other secrets it needs to operate. If an
attacker later compromises the initial token, it will have expired and can’t be used.
For example, Hashicorp Vault (https://vaultproject.io) supports generating tokens
with a limited expiry time which a client can then use to retrieve other secrets from
the vault. 
Pop quiz
10 Which of the following are ways that a Kubernetes secret can be exposed to
pods?
a
As files
b
As sockets
c
As named pipes
d
As environment variables
e
As shared memory buffers
11 What is the name of the standard that defines an API for talking to hardware
security modules?
a
PKCS#1
b
PKCS#7
c
PKCE
d
PKCS#11
e
PKCS#12
The answers are at the end of the chapter.
424
CHAPTER 11
Securing service-to-service APIs
CAUTION
The techniques in this section are significantly more complex than
other solutions. You should carefully weigh the increased security against
your threat model before adopting these approaches.
If you primarily use OAuth2 for access to other services, you can deploy a short-lived
JWT that the service can use to obtain access tokens using the JWT bearer grant
described in section 11.3. Rather than giving clients direct access to the private key to
create their own JWTs, a separate controller process generates JWTs on their behalf
and distributes these short-lived bearer tokens to the pods that need them. The client
then uses the JWT bearer grant type to exchange the JWT for a longer-lived access
token (and optionally a refresh token too). In this way, the JWT bearer grant type can
be used to enforce a separation of duties that allows the private key to be kept securely
away from pods that service user requests. When combined with certificate-bound
access tokens of section 11.4.6, this pattern can result in significantly increased secu-
rity for OAuth2-based microservices.
 The main problem with short-lived credentials is that Kubernetes is designed for
highly dynamic environments in which pods come and go, and new service instances
can be created to respond to increased load. The solution is to have a controller process
register with the Kubernetes API server and watch for new pods being created. The con-
troller process can then create a new temporary credential, such as a fresh signed JWT,
and deploy it to the pod before it starts up. The controller process has access to long-
lived credentials but can be deployed in a separate namespace with strict network poli-
cies to reduce the risk of it being compromised, as shown in figure 11.8.
AS
Controller
Kubernetes API
server
New pod
Control plane
Data plane
The Kubernetes API server
informs the controller when
a new pod is created.
The controller uses its private
key to create a short-lived JWT.
The JWT is deployed
to the pod.
The pod exchanges
the JWT for an access
token using the JWT
Bearer grant.
Figure 11.8