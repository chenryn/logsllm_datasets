### Increased Loss and Norm for Poisoning Average AGR

Increasing the loss and norm of updates can effectively poison the Average Aggregation Rule (AGR) [10], [41]. In our work, we propose two label flipping (LF) strategies: static LF (SLF) and dynamic LF (DLF). 

- **Static LF (SLF):** For a sample \((x, y)\), the adversary flips labels in a static manner as described in Section IV-A1.
- **Dynamic LF (DLF):** The adversary computes a surrogate model \(\hat{\theta}\), an estimate of \(\theta_g\), using available benign data. The label \(y\) is then flipped to the least probable label with respect to \(\hat{\theta}\), i.e., \(\arg\min \hat{\theta}(x)\).

The impact of these two LF strategies is dataset-dependent. Therefore, for each dataset, we experiment with both strategies and present the best results. We now specify our Data Poisoning Attack (DPA) for the AGRs in Section II-B.

### Average AGR

To satisfy the attack objective in (2) for the Average AGR, the following steps are taken:

1. **Fine-tuning the Global Model:** The adversary fine-tunes \(\theta_g\) using the poisoned data \(D_p\) and Stochastic Gradient Ascent (SGA) to compute a poisoned update \(\nabla'\). This process is consistent for any AGR.
2. **Projection Function:** The adversary uses the \(f_{project}\) function to appropriately project \(\nabla'\) to circumvent the robustness criteria of the target AGR, \(f_{agr}\).

Algorithm 2 (Appendix B) describes \(f_{project}\):
- It computes \(\nabla_b = f_{avg}(\nabla_{\{i \in [n']\}})\).
- It finds a scaling factor \(\gamma\) for \(\nabla'\) that maximizes the distance between the benign aggregate \(\nabla_b\) and the poisoned aggregate \(\nabla_p = f_{agr}(\gamma \nabla'_{\{i \in [m]\}}, \nabla_{\{i \in [n']\}})\).

Empirically, searching for \(\gamma\) in a pre-specified range (e.g., \([1, \Gamma]\) with \(\Gamma \in \mathbb{R}^+\)) yields strong attacks. Figure 3 illustrates the idea of the \(f_{project}\) algorithm.

Due to the modular nature of our attacks, one can attack any given AGR by plugging its algorithm into Algorithm 2. This differs from Sun et al. [58], who propose a similar targeted attack that only works against norm-bounding AGR.

### Tailoring \(f_{project}\) for Specific AGRs

- **Average AGR:** Since it does not impose any robustness constraints, we simplify \(f_{project}\) by scaling \(\nabla'\) by an arbitrarily large constant, e.g., \(10^{20}\). If the server selects a compromised client, such a poisoned update suffices to completely poison \(\theta_g\).
- **Norm-bounding AGR:** Following Kirchhoff's law, we assume the attacker knows the norm-bounding threshold \(\tau\). Thus, \(f_{project}\) scales \(\nabla'\) by \(\frac{\tau}{\|\nabla'\|}\), so the norm of the final \(\nabla'\) will be \(\tau\). Details of our MPAs on Mkrum and Trmean are provided in Appendix B2.

### Analysis of FL Robustness in Practice

In this section, we evaluate state-of-the-art data (DPAs) and model poisoning attacks (MPAs) against non-robust and robust Federated Learning (FL) algorithms under practical threat models from Section III-C. We start with cross-device FL (Sections V-A to V-C), as it is less studied and more susceptible to poisoning, and then analyze cross-silo FL in Section V-D.

#### Experimental Setup

Refer to Appendix C for the experimental setup.

#### Attack Impact Metric

- \(A_\theta\): Maximum accuracy of the global model without any attack.
- \(A^*_\theta\): Maximum accuracy of the model under a given attack.
- **Attack Impact \(I_\theta\):** Reduction in the accuracy of the global model due to the attack, defined as \(I_\theta = A_\theta - A^*_\theta\).

### Evaluating Non-robust FL (Cross-device)

We study the Average AGR due to its practical significance and widespread use. Previous works [5], [10], [23], [41], [55], [70] have argued that even a single compromised client can prevent the convergence of FL with the Average AGR. However, our results contradict these findings, showing that this belief is incorrect for production cross-device FL.

Figure 4a shows the attack impacts (Iθ) of various DPAs and MPAs. For the Average AGR, all MPAs [5], [23], [55], including ours, craft arbitrarily large updates in a malicious direction. Hence, we show a single line for MPAs in Figure 4a.

For cross-device FL, when percentages of compromised clients (M) are in practical ranges (Table III), the Iθ values of all attacks are very low, indicating high convergence accuracy. For FEMNIST, Iθ of MPAs at M=0.01% is ∼2%, and Iθ of DPAs at 0.1% is ∼5%. This means that compared to the no-attack accuracy (82.3%), the attacks reduce the accuracy by just 2% and 5%.

Note that we use very large local poisoned data (Dp) for our DPAs, as DPAs on the Average AGR become stronger with higher |Dp| (Section IV-B2). However, as shown in Section V-C1, under practical |Dp|, Iθ values of DPAs are negligible even with M=10%.

The inherent robustness of cross-device FL is due to its client sampling procedure. In an FL round, the server selects a very small fraction of all FL clients, leading to many rounds where no compromised clients are chosen when M (< 1%) is in practical ranges.

**Takeaway V-A:** Contrary to common belief, production cross-device FL with the naive Average AGR converges with high accuracy even in the presence of untargeted poisoning attacks.

### Evaluating Robust FL (Cross-device)

Contrary to previous works, we study the robustness of robust AGRs for cross-device FL when percentages of compromised clients (M) are in practical ranges. Figure 4b shows the poisoning impact (Iθ) of DPAs and MPAs for Norm-bounding (Normb), Multi-krum (Mkrum), and Trimmed-mean (Trmean) AGRs. Below, we discuss three key takeaways:

1. **High Robustness in Practice:** Iθ of attacks on robust AGRs are negligible in practice, i.e., when M ≤ 0.1% for DPAs and M ≤ 0.01% for MPAs. For instance, Iθ ≤ 1% for all state-of-the-art attacks on all three datasets, indicating that the attacks reduce the accuracy of \(\theta_g\) by less than 1 percent.

   We also run FL with a robust AGR for a very large number (5,000) of rounds to investigate if the strongest MPAs against the AGR with M = 0.1% can break the AGR after long rounds of continuous and slow poisoning. Figure 6 shows the results: Mkrum and Trmean remain completely unaffected (in fact, the accuracy of the global model increases), while the accuracy due to Normb reduces by <5%.

   **Takeaway V-B1:** Cross-device FL with robust AGRs is highly robust to state-of-the-art poisoning attacks under production FL environments (M <0.1%, n ≪ N).

   **Takeaway V-B2:** Even the simple, low-cost Norm-bounding AGR is enough to protect production FL against untargeted poisoning, questioning the need for more sophisticated (and costlier) AGRs.

2. **Thorough Empirical Assessment is Essential:** Theoretically robust AGRs claim robustness to poisoning attacks at high M’s, e.g., in theory, Mkrum [10] and Trmean [70] are robust for M ≤ 25%. However, we observe that, even at the theoretically claimed values of M, these robust AGRs do not exhibit high robustness; in fact, simple AGRs, e.g., Norm-bounding, are equally robust. Note in Figure 4b that, for FEMNIST at M=10%, Iθ’s on Trmean are higher than on Norm-bounding. For CIFAR10 at M=10%, Iθ’s for Norm-bounding and Trmean are almost similar.

   **Takeaway V-B3:** Understanding the robustness of AGRs in production FL requires a thorough empirical assessment on top of their theoretical robustness analysis.

### Effect of FL Parameters on Poisoning (Cross-device)

1. **Effect of the Size of Local Poisoning Datasets (|Dp|) on DPAs:**
   - The success of our state-of-the-art data poisoning attacks depends on |Dp| of compromised clients (Section IV-B2).
   - In Sections V-A and V-B, we use large |Dp| (e.g., 50,000 for CIFAR10) to find the highest impacts of DPAs. But, in practice, |Dp| ≤ 100 × |D|avg; |D|avg is the average size of local datasets of benign clients and is around 20 (50) for FEMNIST (CIFAR10).
   - Figure 5 reports Iθ of the best of DPA-SLF or DPA-DLF for varying |Dp|. When |Dp| and M are in practical ranges, Iθ’s are negligible for robust AGRs and are dataset-dependent for non-robust Average AGR.

   **Takeaway V-C1:** Enforcing a limit on the size of the local dataset of each client can act as a highly effective (yet simple) defense against untargeted DPAs in production FL.

2. **Effect of the Average Dataset Size of Benign FL Clients (|D|avg):**
   - Figure 9 in Appendix E shows Iθ when we vary |D|avg. To emulate varying |D|avg, we vary the total number of FL clients, N, for a given dataset, e.g., for CIFAR10, |D|avg is 50 (10) for N=1,000 (N=5,000). As discussed in Section III-B3, we use |Dp|=100 × |D|avg for DPAs.
   - We observe no clear effect of varying |D|avg on Iθ’s. For instance, at M=1%, Iθ’s of our PGA and DPA-SLF on CIFAR10 + Normb reduce with an increase in |D|avg, while Iθ of any attacks on FEMNIST with robust AGRs do not change with varying |D|avg.

   **Takeaway V-C2:** When local dataset sizes of benign clients are in practical regimes (Table III), cross-device FL with robust AGRs is highly robust to untargeted poisoning.

3. **Number of Clients Selected Per Round:**
   - Figure 10 (Appendix B2) shows the effect of varying the number of clients (n) selected by the server in each round (for M=1%). Similar to [23], we do not observe any noticeable effect of n on the impact of attacks, since the expected percentage of compromised clients (M) does not change with n. However, we observe the opposite behavior for MPAs on the Average AGR.
   - **Takeaway V-C3:** The number of clients selected in each round of production cross-device FL has no noticeable effect on the impacts of untargeted poisoning attacks, with the exception of MPAs on the Average AGR.

4. **Effect of Unknown Global Model Architecture on DPAs:**
   - The DPA-DLF attack (Section IV-B2) uses the knowledge of the global model’s architecture to train a surrogate model. However, in practice, the no-box offline data poisoning adversary (Section III-C1) may not know the architecture. Hence, we evaluate the impact of DPA-DLF under the unknown architecture setting.
   - We emulate the unknown architecture setting for the FEMNIST dataset. We assume that the adversary uses a substitute convolutional neural network given in Table V (Appendix E) as they do not know the true architecture, which is LeNet in our experiments. Figure 7 (Appendix E) compares the impacts of DPA-DLF when the adversary uses the true and the substitute architectures. Note that the impacts of DPA-DLF reduce when the adversary uses the substitute architecture.

   **Takeaway V-C4:** The DPAs that rely on a surrogate model (e.g., our DLF) are less effective if the architectures of the surrogate and global models do not match.

### Evaluating Robustness of Cross-silo FL

In cross-silo FL, each of N clients, i.e., silos (e.g., corporations like banks, hospitals, insurance providers, government organizations, etc.), collects data from many users (e.g., bank customers, patients, policyholders, citizens, etc.). Each silo has a larger and more diverse dataset, making the system more robust to poisoning attacks.