increases update’s loss and norm, respectively, and hence, can
effectively poison the Average AGR [10], [41].
In our work, we propose two label flipping (LF) strategies:
static LF (SLF) and dynamic LF (DLF). In SLF, for a
sample (x, y), the adversary flips labels in a static fashion
as in Section IV-A1. On the other hand, in DLF, the adversary
computes a surrogate model ˆθ, an estimate of θg, e.g., using
the available benign data, and flips y to the least probable
label with respect to ˆθ, i.e., to argmin ˆθ(x). We observe that
the impacts of the two LF strategies are dataset dependent.
Therefore, for each dataset, we experiment with both of the
strategies and, when appropriate, present the best results. We
now specify our DPA for the AGRs in Section II-B.
Average: To satisfy the attack objective in (2) for Average
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1362
20200200020000200000Poison data size |Dp|246810121416Loss, (0;Db)(a) FEMNIST + LossSLFDLF20200200020000200000Poison data size |Dp|010203040506070Update norm, 0(b) FEMNIST + NormSLFDLF20200200020000200000Poison data size |Dp|0.210.220.230.240.250.260.27Trmean objective(c) FEMNIST + TrmeanSLFDLF515253545Poison data size |Dp|3.63.84.04.24.44.64.85.0Mkrum objective|D|avg(d) FEMNIST + MkrumSLFDLFBenign updatesPoisoned aggregatesUpdates scaled using Benign aggregateHighest deviating poisoned aggregateand corresponding scaled updateThen,
the adversary fine-tunes θg using Dp and SGA to
compute a poisoned update ∇′; our attack computes ∇′ for any
AGR in the same manner. Finally, the adversary uses fproject
function to appropriately project ∇′ in order to circumvent the
robustness criteria of the target AGR, fagr.
Algorithm 2 (Appendix B) describes fproject: It computes
∇b = favg(∇{i∈[n′]}). Then, it finds a scaling factor γ for ∇′
that maximizes the distance between the benign aggregate ∇b
and the poisoned aggregate ∇p = fagr(γ∇′
{i∈[m]},∇{i∈[n′]}).
Note that, there can be many ways to optimize γ [55], but
we empirically observe that simply searching for γ in a pre-
specified range (e.g., [1, Γ] with Γ ∈ R+) yields strong attacks
(line 6). Figure 3 depicts the idea of fproject algorithm.
Due to the modular nature of our attacks, one can attack any
given AGR by plugging its algorithm in Algorithm 2. This is
unlike Sun et al. [58], who propose a similar targeted attack
which only works against norm-bounding AGR.
Furthermore, to reduce computation, below we tailor fproject
to the state-of-the-art AGRs from Section II-B; note that, the
adversary obtains a poisoned update, ∇′, using Algorithm 1
before tailoring fproject to the target AGR.
Average: Average does not impose any robustness constraints,
therefore, we simplify fproject by scaling ∇′ by an arbitrarily
large constant, e.g., 1020. If the server selects a compromised
client, such poisoned update suffices to completely poison θg.
Norm-bounding: Following the Kirchoff’s law, we assume
that the attacker knows the norm-bounding threshold, τ, and
therefore, fproject scales ∇′ by
τ∥∇′∥, so that the norm of the
final ∇′ will be τ. We provide the details of our MPAs on
Mkrum and Trmean in Appendix B2.
V. ANALYSIS OF FL ROBUSTNESS IN PRACTICE
In this section, we evaluate state-of-the-art data (DPAs) and
model poisoning attacks (MPAs) against non-robust and robust
FL algorithms (Section II-B), under practical threat models
from Section III-C. We start by analyzing cross-device FL
(Sections V-A to V-C), as it is barely studied in previous works
and is more susceptible to poisoning. Then, we will analyze
cross-silo FL in Section V-D.
Experimental setup: Please refer to Appendix C.
Attack impact metric: Aθ denotes the maximum accuracy
that the global model achieves over all FL training rounds,
without any attack. A∗
θ for an attack denotes the maximum
accuracy of the model under the given attack. We define attack
impact, Iθ, as the reduction in the accuracy of the global model
due to the attack, hence for a given attack, Iθ = Aθ − A∗
θ.
A. Evaluating Non-robust FL (Cross-device)
We study Average AGR due to its practical significance
and widespread use. Previous works [5], [10], [23], [41], [55],
[70] have argued that even a single compromised client can
prevent the convergence of FL with Average AGR. However,
our results contradict those of previous works: we show that
this established belief about Average AGR is incorrect for
production cross-device FL.
Figure 4a shows the attack impacts (Iθ) of various DPAs and
MPAs. Note that, for the Average AGR, all MPAs [5], [23],
[55], including ours, are the same and craft arbitrarily large
updates in a malicious direction. Hence, we show a single line
for MPAs in Figure 4a.
We see that
for cross-device FL, when percentages of
compromised clients (M) are in practical ranges (Table III),
Iθ’s of all the attacks are very low, i.e., the final θg converges
with high accuracy. For FEMNIST, Iθ of MPAs at M=0.01%
is ∼2% and Iθ of DPAs at 0.1% is ∼5%. In other words,
compared to the no attack accuracy (82.3%), the attacks reduce
the accuracy by just 2% and 5%. Similarly, we observe very
low Iθ’s for the Purchase and CIFAR10 datasets.
Note that, here we use very large local poisoned data (Dp)
for our DPAs, as DPAs on Average AGR become stronger with
higher |Dp| (Section IV-B2); |Dp|’s are 20,000, 50,000, and
20,000 for FEMNIST, CIFAR10, and Purchase, respectively.
However, as we will show in Section V-C1, under practical
|Dp|, Iθ’s of DPAs are negligible even with M=10%.
The inherent robustness of cross-device FL is due to its
client sampling procedure. In an FL round, the server selects
a very small fraction of all FL clients. Hence, in many FL
rounds no compromised clients are chosen when M (< 1%)
is in practical ranges.
(Takeaway V-A) Contrary to the common belief, produc-
tion cross-device FL with (the naive) Average AGR con-
verges with high accuracy even in the presence of untargeted
poisoning attacks.
B. Evaluating Robust FL (Cross-device)
In this section, contrary to previous works, we study the ro-
bustness of robust AGRs for cross-device FL when percentages
of compromised clients (M) are in practical ranges. Figure 4b
shows the poisoning impact (Iθ) of DPAs and MPAs for Norm-
bounding (Normb), Multi-krum (Mkrum), and Trimmed-mean
(Trmean) AGRs. Below, we discuss three key takeaways:
1) Cross-device FL with robust AGRs is highly robust
in practice: Iθ of attacks on robust AGRs are negligible in
practice, i.e., when M ≤ 0.1% for DPAs and M ≤ 0.01% for
MPAs. For instance, Iθ ≤ 1% for all of state-of-the-art attacks
on all the three datasets, i.e., the attacks reduce the accuracy
of θg by less that 1 percent.
We also run FL with a robust AGR for a very large
number (5,000) of rounds to investigate if the strongest of
MPAs against the AGR with M = 0.1% can break the AGR
after long rounds of continuous and slow poisoning. Figure 6
shows the results: Mkrum and Trmean remain completely
unaffected (in fact accuracy of the global model increases),
while accuracy due to Normb reduces by <5%.
In summary, state-of-the-art poisoning attacks [5], [23], [55]
demonstrate that the robust AGRs are significantly less robust
than their theoretical guarantees. On the other hand, our find-
ings show that these AGRs are more than sufficient to protect,
more practical, production cross-device FL against untargeted
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1363
(a) Non-robust FL
(b) Robust FL
Figure 4: (4a) Attack impacts (Iθ) of state-of-the-art data (DPA-DLF/SLF) and model (MPA) poisoning attacks on cross-device
FL with average AGR. Iθ’s are significantly low for practical percentages of compromised clients (M≤0.1%). (4b) Iθ of
various poisoning attacks (Section IV) on robust AGRs (Section II-B). These AGRs are highly robust for practical M values.
poisoning. This is due to the peculiar client sampling of cross-
(and expensive) AGRs, under practical M. For instance, for
all the datasets with M ≤ 1%, Iθ < 1% for all of the
device FL, as discussed in Section V-A.
AGRs (Figure 4b). Our evaluation highlights that simple robust
(Takeaway V-B1) Cross-device FL with robust AGRs
AGRs, e.g., Norm-bounding, can effectively protect cross-
is highly robust to state-of-the-art poisoning attacks under
device FL in practice, and calls for further investigation and
production FL environments (M <0.1%, n ≪ N).
invention of such low-cost robust AGRs.
(Takeaway V-B2)
low-cost Norm-
Even the simple,
bounding AGR is enough to protect production FL against
untargeted poisoning, questioning the need for the more
sophisticated (and costlier) AGRs.
3) Thorough empirical assessment of robustness is in-
evitable: Theoretically robust AGRs claim robustness to
poisoning attacks at high M’s, e.g., in theory, Mkrum [10] and
Trmean [70] are robust for M ≤ 25%. But, we observe that,
even at the theoretically claimed values of M, these robust
AGRs do not exhibit high robustness; in fact, simple AGRs,
e.g., Norm-bounding, are equally robust. Note in Figure 4b
that, for FEMNIST at M=10%, Iθ’s on Trmean are higher
than on Norm-bounding. For CIFAR10 at M=10%, Iθ’s for
Norm-bounding and Trmean are almost similar.
2) Investigating simple and efficient robustness checks
is necessary: Most of the state-of-the-art robust AGRs with
strong theoretical guarantees [10], [41], [68], [70] have com-
plex robustness checks on their inputs, which incur high
computation and storage overheads. For instance, to process n
updates of length d, the computational complexity of Mkrum
is O(dn2) and that of Trmean is O(dnlogn). Therefore, in
production FL systems where n can be up to 5, 000 [11], [32],
the computation cost prohibits the use of such robust AGRs.
On the other hand, Norm-bounding only checks for the
norm of its inputs and has computation complexity of O(d),
same as Average. Figure 4b shows that a simple and efficient
AGR, Norm-bounding, protects cross-device FL against state-
of-the-art poisoning attacks similarly to the theoretically robust
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1364
0.010.1110020406080100Attack impact (%)FEMNIST + AverageNo attackDPA-DLFDPA-SLFDPA-LFMPA0.010.11100510152025303540Attack impact (%)FEMNIST + Norm-boundNo attackDPA-SLFDPA-DLFDPA-LFLIESTAT-OPTDYN-OPTPGA0.010.11100510152025303540FEMNIST + Multi-krum0.010.11100510152025303540FEMNIST + Trimmed-mean0.010.1110020406080100Attack impact (%)CIFAR10 + AverageDPA-SLFDPA-DLFDPA-LFMPA0.010.111001020304050607080Attack impact (%)CIFAR10 + Norm-boundNo attackDPA-SLFDPA-DLFDPA-LFLIESTAT-OPTDYN-OPTPGA0.010.111001020304050607080CIFAR10 + Multi-krum0.010.111001020304050607080CIFAR10 + Trimmed-mean0.010.1110Compromised client %020406080100Attack impact (%)Purchase + AverageDPA-SLFDPA-DLFDPA-LFMPA0.010.1110Compromised client %01020304050Attack impact (%)Purchase + Norm-boundNo attackDPA-SLFDPA-DLFDPA-LFLIESTAT-OPTDYN-OPTPGA0.010.1110Compromised client %01020304050Purchase + Multi-krum0.010.1110Compromised client %01020304050Purchase + Trimmed-mean0.010.1110020406080100Attack impact (%)FEMNIST + AverageNo attackDPA-DLFDPA-SLFDPA-LFMPA0.010.11100510152025303540Attack impact (%)FEMNIST + Norm-boundNo attackDPA-SLFDPA-DLFDPA-LFLIESTAT-OPTDYN-OPTPGA0.010.11100510152025303540FEMNIST + Multi-krum0.010.11100510152025303540FEMNIST + Trimmed-mean0.010.1110020406080100Attack impact (%)CIFAR10 + AverageDPA-SLFDPA-DLFDPA-LFMPA0.010.111001020304050607080Attack impact (%)CIFAR10 + Norm-boundNo attackDPA-SLFDPA-DLFDPA-LFLIESTAT-OPTDYN-OPTPGA0.010.111001020304050607080CIFAR10 + Multi-krum0.010.111001020304050607080CIFAR10 + Trimmed-mean0.010.1110Compromised client %020406080100Attack impact (%)Purchase + AverageDPA-SLFDPA-DLFDPA-LFMPA0.010.1110Compromised client %01020304050Attack impact (%)Purchase + Norm-boundNo attackDPA-SLFDPA-DLFDPA-LFLIESTAT-OPTDYN-OPTPGA0.010.1110Compromised client %01020304050Purchase + Multi-krum0.010.1110Compromised client %01020304050Purchase + Trimmed-mean|Dp| ∈ {1, 10, 102, 103, 104} · |D|avg; we use impractically
high |Dp|’s of up to 104·|D|avg only for experimental analyses.
Figure 5 shows that Iθ’s of DPAs slightly increase with
|Dp|. For FEMNIST and CIFAR10 with any AGR, including
Average, Iθ’s are negligible even for unrealistically high |Dp|
of 1000×|D|avg for M ≤ 1%. We omit Mkrum here, as |Dp|
of the effective DPAs on Mkrum is always in practical ranges
and close to |D|avg (Section IV-B2).
To summarize, for all robust AGRs, DPAs have negligible
impacts on FL when |Dp| and M are in practical ranges, while
for non-robust AGRs, the reductions in Iθ are non-trivial and
dataset dependent. This also means that using a reasonable
upper bound on the dataset sizes of FL clients can make FL
highly robust to DPAs.
(Takeaway V-C1) Enforcing a limit on the size of the
local dataset of each client can act as a highly effective (yet
simple) defense against untargeted DPAs in production FL.
2) Effect of the Average Dataset Size of Benign FL Clients
(|D|avg): Figure 9 in Appendix E shows Iθ when we vary
|D|avg. To emulate varying |D|avg, we vary the total num-
ber of FL clients, N, for given dataset, e.g., for CIFAR10,
|D|avg is 50 (10) for N=1,000 (N=5,000). As discussed in
Section III-B3, we use |Dp|=100 × |D|avg for DPAs.
We observe no clear effect of varying |D|avg on Iθ’s.
For instance, at M=1%, Iθ’s of our PGA and DPA-SLF on
CIFAR10 + Normb reduce with increase in |D|avg, while Iθ
of any attacks on FEMNIST with robust AGRs do not change
with varying |D|avg. Due to space restrictions, we defer the
explanations of each of these observations to Appendix D.
More importantly, we observe that even with moderately
high |D|avg, cross-device FL completely mitigates state-of-
the-art DPAs and MPAs despite M being impractically high,
with an exception of MPAs on Average AGR. For instance,
for CIFAR10 with |D|avg=50 and FEMNIST with |D|avg=200,
all robust AGRs almost completely mitigate all of DPAs and
MPAs, while Average AGR mitigates all DPAs. However, as
MPAs are very effective against Average, their Iθ remains
high. As clients in FL continuously generate data locally [12],
[40], it is common to have large |D|avg in practice. Interest-
ingly, our evaluation also implies that simply lower bounding
the dataset sizes of FL clients improves FL robustness.
(Takeaway V-C2) When local dataset sizes of benign
clients are in practical regimes (Table III), cross-device FL
with robust AGRs is highly robust to untargeted poisoning.
3) Number of Clients Selected Per Round.: Figure 10
(Appendix B2) shows the effect of varying the number of
clients (n) selected by the server in each round (for M=1%).
Similar to [23], we do not observe any noticeable effect of
n on the impact of attacks, since the expected percentage of
compromised clients (M) does not change with n. But, we
observe the opposite behavior for MPAs on Average AGR.
This is because, as soon as the server selects even a single
compromised client, MPA prevents any further learning of
Figure 5: Effect of varying sizes of local poisoned dataset Dp
on impacts Iθ of the best of DPAs. When |Dp| and M are in
practical ranges, Iθ’s are negligible for robust AGRs and are
dataset dependent for non-robust Average AGR.
Sections V-B2 and V-B3 show that, some of the sophisti-
cated, theoretically robust AGRs do not outperform simpler
robust AGRs at any ranges of M. More importantly they
demonstrate the shortcomings of the methodology used to as-
sess the robustness of AGRs in previous works [10], [41], [68],
[70] (because these works use very preliminary attacks) and
highlight that a thorough empirical assessment is necessary to
understand the robustness of AGRs in production FL systems.
(Takeaway V-B3) Understanding the robustness of AGRs
in production FL requires a thorough empirical assessment
of AGRs, on top of their theoretical robustness analysis.
C. Effect of FL Parameters on Poisoning (Cross-device)
1) Effect of the Size of Local Poisoning Datasets (|Dp|)
on DPAs.: The success of our state-of-the-art data poison-
ing attacks depends on |Dp| of compromised clients (Sec-
tion IV-B2). In Sections V-A and V-B, we use large |Dp| (e.g.,
50,000 for CIFAR10) to find the highest impacts of DPAs. But,
as argued in Section III-B3, in practice |Dp| ≤ 100 × |D|avg;
|D|avg is the average size of local datasets of benign clients
and it is around 20 (50) for FEMNIST (CIFAR10). In Figure 5,
we report Iθ of the best of DPA-SLF or DPA-DLF for
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:01:32 UTC from IEEE Xplore.  Restrictions apply. 
1365
0.010.11100102030405060Attack impact (%)CIFAR10 + AverageNo attackn=50n=500n=5000n=50000DPA-LF0.010.11100102030405060FEMNIST + AverageNo attackn=20n=200n=2000n=20000n=200000DPA-LF0.010.1110051015202530Attack impact (%)CIFAR10 + Norm-bound0.010.1110051015202530FEMNIST + Norm-bound0.010.1110Compromised client %051015202530Attack impact (%)CIFAR10 + Trimmed-mean0.010.1110Compromised client %051015202530FEMNIST + Trimmed-meanthe global model. An increase in n increases the chances of
selecting compromised clients, hence amplifying the attack.
(Takeaway V-C3) The number of clients selected in each
round of production cross-device FL has no noticeable effect
on the impacts of untargeted poisoning attacks, with the
exception of MPAs on Average AGR.
4) Effect of Unknown Global Model Architecture on DPAs:
DPA-DLF attack (Section IV-B2) uses the knowledge of global
model’s architecture to train a surrogate model. However, in
practice, the nobox offline data poisoning adversary (Sec-
tion III-C1) may not know the architecture. Hence, we evaluate
impact of DPA-DLF under the unknown architecture setting.
We emulate the unknown architecture setting for FEMNIST
dataset. We assume that the adversary uses a substitute con-
volutional neural network given in Table V (Appendix E) as
they do not know the true architecture, which is LeNet in our
experiments. Figure 7 (Appendix E) compares the impacts of
DPA-DLF when the adversary uses the true and the substitute
architectures. Note that, impacts of DPA-DLF reduce when the
adversary uses the substitute architecture.
(Takeaway V-C4) The DPAs that rely on a surrogate model
(e.g., our DLF) are less effective if the architectures of the
surrogate and global models do not match.
D. Evaluating Robustness of Cross-silo FL
In cross-silo FL, each of N clients, i.e., silos (e.g., corpo-
rations like banks, hospitals, insurance providers, government
organizations, etc.), collects data from many users (e.g., bank