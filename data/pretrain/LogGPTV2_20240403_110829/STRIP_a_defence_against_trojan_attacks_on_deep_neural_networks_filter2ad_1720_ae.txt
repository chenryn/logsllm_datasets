detecting a large trigger. One limitation of SentiNet is that the
region embedding the trojan trigger needs be small enough. If
the trigger region is large, such as the trigger shown in Fig. 7
(a) and (c), and Fig. 1, then SentiNet tends to be less effective.
This is caused by its carve-out method. Supposing that the
carved region is large and contains the trigger, then patching
it on held-out samples will also show a small misclassiﬁcation
rate to be falsely accepted as a benign input via SentiNet.
Notably, in contrast to the use of a global detection bound-
ary in Neural Cleanse [17], the detection boundary of STRIP
is unique to each deployed model and is extracted from the
already deployed model itself; this boundary is not a global
setting. This avoids the potential for the global setting to
fail since the optimized detection boundary for each model
can vary. Probably, one not obvious fact is that users need
to train trojan/clean models by themselves to ﬁnd out this
global setting as the detection boundary of the Neural Cleanse
needs to be decided based on reference models—STRIP does
not need reference model but solely the already deployed
(begin/backdoored) model. This may partially violate the mo-
tivation for outsourcing the model training of ML models—the
main source of attackers to introduce backdoor attacks: if the
users own training skills and the computational power, it may
be reasonable to train the model, from scratch, by themselves.
D. Watermarking
There are works considering a backdoor as a watermark [34]
to protect the intellectual property (IP) of a trained DNN
model [35]–[37]. The argument is that the inserted backdoor
can be used to claim the ownership of the model provider
since only the provider is supposed to have the knowledge
of such a backdoor, while the backdoored DNN model has
no (or imperceptible) degraded functional performance on
normal
inputs. However, as the above countermeasures—
detection, recovery, and removal—against backdoor insertion
are continuously evolved, the robustness of using backdoors
as watermarks is potentially challenged in practical usage.
We leave the robustness of backdoor entangled watermarking
under the backdoor detection and removal threat as part of
future work since it is out of the scope of this work.
VIII. CONCLUSION AND FUTURE WORK
The presented STRIP constructively turns the strength of
insidious input-agnostic trigger based trojan attack into a
weakness that allows one to detect trojaned inputs (and very
likely backdoored model) at run-time. Experiments on MNIST,
CIFAR10 and GTSRB datasets with various triggers and
evaluations validate the high detection capability of STRIP.
Overall, the FAR is lower than 1%, given a preset FRR of
1%. The 0% FRR and 0% FAR are empirically achieved
on popular CIFAR10 and GTSRB datasets. While easy-to-
implement, time-efﬁcient and complementing with existing
trojan mitigation techniques, the run-time STRIP works in a
black-box manner and is shown to be capable of overcoming
the trigger size limitation of other state-of-the-art detection
methods. Furthermore, STRIP has also demonstrated its ro-
bustness against several advanced variants of input-agnostic
trojan attacks and the entropy manipulation adaptive attack.
Nevertheless, similar to Neural Cleanse [17] and Sen-
tiNet [11], STRIP is not effective to detect source-label-
speciﬁc triggers; this needs to be addressed in future work. In
addition, we will test STRIP’s generalization to other domains
such as text and voice .
REFERENCES
[1] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” Nature, vol. 521,
no. 7553, p. 436, 2015.
[2] Q. Wang, W. Guo, K. Zhang, A. G. Ororbia II, X. Xing, X. Liu,
and C. L. Giles, “Adversary resistant deep neural networks with an
application to malware detection,” in Proceedings of the 23rd ACM
SIGKDD International Conference on Knowledge Discovery and Data
Mining. ACM, 2017, pp. 1145–1153.
[3] T. A. Tang, L. Mhamdi, D. McLernon, S. A. R. Zaidi, and M. Ghogho,
“Deep learning approach for network intrusion detection in software
deﬁned networking,” in International Conference on Wireless Networks
and Mobile Communications (WINCOM).
IEEE, 2016, pp. 258–263.
[4] I. Stoica, D. Song, R. A. Popa, D. Patterson, M. W. Mahoney,
R. Katz, A. D. Joseph, M. Jordan, J. M. Hellerstein, J. E. Gonzalez
et al., “A berkeley view of systems challenges for AI,” arXiv preprint
arXiv:1712.05855, 2017.
[5] W. Guo, D. Mu, J. Xu, P. Su, G. Wang, and X. Xing, “Lemna: Explaining
deep learning based security applications,” in Proceedings of the 2018
ACM SIGSAC Conference on Computer and Communications Security.
ACM, 2018, pp. 364–379.
[6] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor
attacks on deep learning systems using data poisoning,” arXiv preprint
arXiv:1712.05526, 2017.
[7] Y. Ji, X. Zhang, S. Ji, X. Luo, and T. Wang, “Model-reuse attacks
on deep learning systems,” in Proceedings of the ACM Conference on
Computer and Communications Security. ACM, 2018, pp. 349–363.
[8] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnera-
bilities in the machine learning model supply chain,” arXiv preprint
arXiv:1708.06733, 2017.
[9] M. Zou, Y. Shi, C. Wang, F. Li, W. Song, and Y. Wang, “Potrojan:
powerful neural-level trojan designs in deep learning models,” arXiv
preprint arXiv:1802.03043, 2018.
[10] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to
backdoor federated learning,” arXiv preprint arXiv:1807.00459, 2018.
[11] E. Chou, F. Tram`er, G. Pellegrino, and D. Boneh, “Sentinet: Detect-
ing physical attacks against deep learning systems,” arXiv preprint
arXiv:1812.00292, 2018.
[12] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, “Accessorize to
a crime: Real and stealthy attacks on state-of-the-art face recognition,”
in Proceedings of
the ACM SIGSAC Conference on Computer and
Communications Security. ACM, 2016, pp. 1528–1540.
[13] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao,
A. Prakash, T. Kohno, and D. Song, “Robust physical-world attacks
on deep learning visual classiﬁcation,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, 2018, pp.
1625–1634.
[14] J. Su, D. V. Vargas, and K. Sakurai, “One pixel attack for fooling deep
neural networks,” IEEE Transactions on Evolutionary Computation,
2019.
[15] W. Guo, L. Wang, X. Xing, M. Du, and D. Song, “Tabor: A highly
accurate approach to inspecting and restoring trojan backdoors in ai
systems,” arXiv preprint arXiv:1908.01763, 2019.
12
[16] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang,
“Trojaning attack on neural networks,” in Network and Distributed
System Security Symposium (NDSS), 2018.
[17] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y.
Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks
in neural networks,” in Proceedings of the 40th IEEE Symposium on
Security and Privacy, 2019.
[18] C. Liao, H. Zhong, A. Squicciarini, S. Zhu, and D. Miller, “Backdoor
embedding in convolutional neural network models via invisible pertur-
bation,” arXiv preprint arXiv:1808.10307, 2018.
[19] U. A. R. Ofﬁce.
[Online]. Avail-
https://www.fbo.gov/index.php?s=opportunity&mode=form&id=
able:
be4e81b70688050fd4fc623fb24ead2c&tab=core& cview=0
TrojAI.
2019)
(May
[20] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee,
I. Molloy, and B. Srivastava, “Detecting backdoor attacks on deep neural
networks by activation clustering,” arXiv preprint arXiv:1811.03728,
2018.
[21] B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,”
in Advances in Neural Information Processing Systems, 2018, pp. 8000–
8010.
[22] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
applied to document recognition,” Proceedings of the IEEE, vol. 86,
no. 11, pp. 2278–2324, 1998.
[23] A. Krizhevsky and G. Hinton, “Learning multiple layers of features from
tiny images,” Citeseer, Tech. Rep., 2009.
[24] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “Man vs. computer:
Benchmarking machine learning algorithms for trafﬁc sign recognition,”
Neural networks, vol. 32, pp. 323–332, 2012.
[25] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
recognition,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 770–778.
[26] D. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg,
C. Case, J. Casper, B. Catanzaro, Q. Cheng, G. Chen et al., “Deep
speech 2: End-to-end speech recognition in english and mandarin,” in
International Conference on Machine Learning, 2016, pp. 173–182.
[27] Y. Kim, “Convolutional neural networks for sentence classiﬁcation,”
arXiv preprint arXiv:1408.5882, 2014.
[28] L. Huang, A. D. Joseph, B. Nelson, B. I. Rubinstein, and J. Tygar, “Ad-
versarial machine learning,” in Proceedings of the 4th ACM workshop
on Security and artiﬁcial intelligence. ACM, 2011, pp. 43–58.
[29] N. Papernot, P. McDaniel, A. Sinha, and M. Wellman, “Towards the
science of security and privacy in machine learning,” arXiv preprint
arXiv:1611.03814, 2016.
[30] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating
backdooring attacks on deep neural networks,” IEEE Access, vol. 7,
pp. 47 230–47 244, 2019.
[31] N. Baracaldo, B. Chen, H. Ludwig, and J. A. Safavi, “Mitigating
poisoning attacks on machine learning models: A data provenance based
approach,” in Proceedings of the 10th ACM Workshop on Artiﬁcial
Intelligence and Security. ACM, 2017, pp. 103–110.
[32] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending against
backdooring attacks on deep neural networks,” in Proceedings of RAID,
2018.
[33] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” in IEEE International
Conference on Computer Design (ICCD).
IEEE, 2017, pp. 45–48.
[34] H. Chen, B. D. Rouhani, and F. Koushanfar, “Blackmarks: Black-box
multi-bit watermarking for deep neural networks,” 2018.
[35] Y. Adi, C. Baum, M. Cisse, B. Pinkas, and J. Keshet, “Turning
your weakness into a strength: Watermarking deep neural networks by
backdooring,” in USENIX Security Symposium, 2018.
[36] J. Guo and M. Potkonjak, “Watermarking deep neural networks for
embedded systems,” in 2018 IEEE/ACM International Conference on
Computer-Aided Design (ICCAD), 2018, pp. 1–8.
[37] J. Zhang, Z. Gu, J. Jang, H. Wu, M. P. Stoecklin, H. Huang, and
I. Molloy, “Protecting intellectual property of deep neural networks
with watermarking,” in Proceedings of the 2018 on Asia Conference on
Computer and Communications Security. ACM, 2018, pp. 159–172.
APPENDIX A
TRIGGER TRANSPARENCY RESULTS
Fig. 14 shows different
transparency settings. Table V
details classiﬁcation rate of clean inputs, attack success rate of
trojaned inputs, and detection rate under different transparency
settings.
13
We ﬁnd that as the neural network goes deeper—usually
leads to a more accurate prediction, the detection capability
also improves. Speciﬁcally, for the shallow 2-layer architecture
based trojaned model, 2% FRR gives 0.45% FAR, 1% FRR
gives 0.6% FAR, and 0.5% FRR gives 0.9% FAR. While for
the 8-layer architecture based trojaned model, FRR is always
0%, regardless of FRR, as there is always an entropy gap—no
overlap—between the benign and trojaned inputs.
Moreover, we run a 8-layer architecture on the MNIST
dataset with the square trigger. For the trojaned model, its
accuracy on clean inputs is 99.02% while achieves a 99.99%
accuracy on trojaned inputs. STRIP demonstrates an improved
detection capability as well. Speciﬁcally, 1% FRR gives 0%
FAR, 0.5% FRR gives 0.03% FAR, which has been greatly
improved in comparison with the detection capability of a 2-
layer trojaned model, see Table. III.
To this end, we can empirically conclude that the deeper
the model, the higher detection capability of STRIP detection.
On one hand, this potentially lies on the fact that the model
with more parameters memorizes the trigger feature stronger,
which always presents a low entropy for the trojaned input.
On the other hand, the model also more accurately memorizes
the features for each class of clean input. The trained model
is more sensitive to strong perturbation on clean input, and
therefore, unlikely to present a low entropy for clean input—
may contribute to FRR.
We are curious on those images that are trojaned but falsely
accepted as clean images. Therefore, based on the 2-layer
trojaned model (8-layer model has 0% FAR) produced on the
CIFAR10 dataset and trigger c, we further examined those
images. We found that most of them lost their trojan effect,
as shown in Fig. 15. For instance, out of 10 falsely accepted
trojaned images, four images maintaining their trojaning effect
of hijacking the DNN model to classfy them to be the targeted
label of ‘horse’. The rest six trojaned images are unable to
achieve their trojaning effect because the trojan trigger is not
strong enough to misdirect the predicted label to be ‘horse’. In
other words, these six trojaned images will not cause security
concerns designed by the attacker when they are indeed
misclassiﬁed into benign image by STRIP. In addition, we
observe that there are three trojaned images classiﬁed into their
correct ground-truth labels by the attacker’s trojaned model.
The reason may lie on that the trigger feature is weakened in
certain speciﬁc inputs. For example, without careful attention,
one may not perceive the stamped trigger in the ‘frog’ (1st)
and ‘airplane’ (7th) images, which is more likely the same to
the trojaned DNN model.
Figure 14. From left to right, trigger transparency are 90%, 80%, 70%, 60%
and 50%.
Table V
CLASSIFICATION RATE OF CLEAN IMAGES, ATTACK SUCCESS RATE AND
DETECTION CAPABILITY UNDER DIFFERENT TRIGGER TRANSPARENCY
SETTINGS. DATASET IS CIFAR10 AND THE TRIGGER IS TRIGGER B IN
FIG. 7 (B). THE FRR IS PRESET TO BE 0.5%.
Transp.
Classiﬁcation rate
of clean image
Attack
success rate
Min. entropy
of clean images
Max. entropy
of trojaned images
90%
80%
70%
60%
50%
87.11%
85.81%
88.59%
86.68%
86.80%
99.93%
100%
100%
100%
100%
0.0647
0.0040
0.0323
0.0314
0.0235
0.6218
0.0172
0.0167
3.04 × 10−17
4.31 × 10−6
Detection
boundary
0.2247
0.1526
0.1546
0.1459
0.1001
FAR
0.10%
0%
0%
0%
0%
DETECTION CAPABILITY RELATIONSHIP WITH DEPTH OF
APPENDIX B
NEURAL NETWORK
Figure 15. When the trojaned images are falsely accepted by STRIP as benign
images, most of them lost their trojaning effect. Because they cannot hijack the
trojaned DNN model to classify them to the targeted class—‘horse’. Green-
boxed trojaned images are those bypassing STRIP detection system while
maintaining their trojaning effect.
Besides the DNN architecture—referred to as 8-layer
architecture—achieving around 88% accuracy performance of
clean inputs, we tested a shallow neural network architecture
only with 2 conventional layer and 1 dense layer—referred to
as 2-layer architecture. For this 2-layer architecture, the benign
model on CIFAR10 dataset has a lower accuracy performance,
which is 70%. The corresponding trojaned model with trigger
c has a similar accuracy with around 70% for clean inputs
while around 99% attack success rate for trojaned inputs. In
this context, the model is successfully inserted as it does not
degrade the performance of clean inputs.
frogairplaneshiptruckfrogbirdhorsehorsetruckfrogdeerairplanetruckautomobilefrogbirdairplaneairplanehorsehorse