### Limitations of SentiNet in Detecting Large Triggers

One limitation of SentiNet is its reduced effectiveness when the trojan trigger region is large. For example, if the trigger region is extensive, such as those shown in Figure 7(a) and (c), and Figure 1, SentiNet's performance diminishes. This issue arises from its carve-out method. If the carved region is large and contains the trigger, patching it onto held-out samples may still result in a low misclassification rate, leading SentiNet to incorrectly accept the input as benign.

In contrast to Neural Cleanse, which uses a global detection boundary, STRIP employs a unique detection boundary for each deployed model. This boundary is extracted from the already deployed model itself, avoiding the potential failure of a global setting. Notably, users must train both trojan and clean models themselves to determine the global setting for Neural Cleanse, whereas STRIP only requires the already deployed (or backdoored) model. This requirement partially undermines the motivation for outsourcing model training, which is a primary avenue for introducing backdoor attacks. If users possess the necessary training skills and computational power, they might reasonably train the model from scratch.

### Watermarking and Backdoor Attacks

Some works consider backdoors as watermarks to protect the intellectual property (IP) of trained DNN models. The argument is that the inserted backdoor can be used to claim ownership since only the provider should have knowledge of it, and the backdoored DNN model should maintain functional performance on normal inputs. However, as detection, recovery, and removal countermeasures evolve, the robustness of using backdoors as watermarks is challenged. Future work will explore the robustness of backdoor-entangled watermarking under backdoor detection and removal threats.

### Conclusion and Future Work

STRIP effectively turns the strength of insidious input-agnostic trigger-based trojan attacks into a weakness, enabling the detection of trojaned inputs (and likely backdoored models) at runtime. Experiments on MNIST, CIFAR10, and GTSRB datasets with various triggers validate STRIP's high detection capability. The False Acceptance Rate (FAR) is generally below 1%, given a preset False Rejection Rate (FRR) of 1%. On popular datasets like CIFAR10 and GTSRB, 0% FRR and 0% FAR are empirically achieved. 

STRIP is easy to implement, time-efficient, and complements existing trojan mitigation techniques. It operates in a black-box manner and overcomes the trigger size limitations of other state-of-the-art detection methods. Additionally, STRIP has demonstrated robustness against advanced variants of input-agnostic trojan attacks and entropy manipulation adaptive attacks. However, similar to Neural Cleanse and SentiNet, STRIP is not effective in detecting source-label-specific triggers, an issue to be addressed in future work. We also plan to test STRIP's generalization to other domains, such as text and voice.

### References

[References remain the same as provided]

### Appendix A: Trigger Transparency Results

Figure 14 shows different transparency settings, and Table V details the classification rate of clean inputs, attack success rate of trojaned inputs, and detection rate under these settings. As the neural network becomes deeper, the detection capability improves. For a shallow 2-layer architecture, 2% FRR gives 0.45% FAR, 1% FRR gives 0.6% FAR, and 0.5% FRR gives 0.9% FAR. In contrast, an 8-layer architecture always achieves 0% FRR, regardless of the FRR, due to the consistent entropy gap between benign and trojaned inputs.

For the 8-layer architecture on the MNIST dataset with a square trigger, the trojaned model achieves 99.02% accuracy on clean inputs and 99.99% accuracy on trojaned inputs. STRIP demonstrates improved detection capability, with 1% FRR giving 0% FAR and 0.5% FRR giving 0.03% FAR, significantly better than the 2-layer model.

Empirically, deeper models enhance STRIP's detection capability. This is likely because deeper models with more parameters memorize the trigger feature more strongly, presenting low entropy for trojaned inputs. They also more accurately memorize features for clean inputs, making them more sensitive to strong perturbations and less likely to present low entropy for clean inputs, contributing to FRR.

We examined falsely accepted trojaned images from the 2-layer trojaned model on the CIFAR10 dataset. Most of these images lost their trojan effect, as shown in Figure 15. Out of 10 falsely accepted trojaned images, four maintained their trojaning effect, while six did not achieve the trojaning effect due to insufficiently strong triggers. These six images do not pose security concerns when misclassified as benign by STRIP. Additionally, three trojaned images were classified into their correct ground-truth labels by the trojaned model, possibly due to weakened trigger features in specific inputs.

### Appendix B: Detection Capability Relationship with Depth of Neural Network

Besides the 8-layer architecture achieving around 88% accuracy on clean inputs, we tested a shallow 2-layer architecture with 1 dense layer. This 2-layer architecture had a lower accuracy of 70% on clean inputs but a similar 99% attack success rate for trojaned inputs, indicating successful backdoor insertion without degrading clean input performance.