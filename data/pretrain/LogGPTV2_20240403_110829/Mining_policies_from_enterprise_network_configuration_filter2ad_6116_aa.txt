title:Mining policies from enterprise network configuration
author:Theophilus Benson and
Aditya Akella and
David A. Maltz
Mining Policies From Enterprise Network Conﬁguration
Theophilus Benson, Aditya Akella
University of Wisconsin, Madison
Madison, WI, USA
PI:EMAIL,
PI:EMAIL
David A. Maltz
Microsoft Research
Redmond, WA, USA
PI:EMAIL
ABSTRACT
Few studies so far have examined the nature of reachability poli-
cies in enterprise networks. A better understanding of reachability
policies could both inform future approaches to network design as
well as current network conﬁguration mechanisms. In this paper, we
introduce the notion of a policy unit, which is an abstract representa-
tion of how the policies implemented in a network apply to different
network hosts. We develop an approach for reverse-engineering a
network’s policy units from its router conﬁguration. We apply this
approach to the conﬁgurations of ﬁve productions networks, includ-
ing three university and two private enterprises. Through our empir-
ical study, we validate that policy units capture useful characteristics
of a network’s policy. We also obtain insights into the nature of the
policies implemented in modern enterprises. For example, we ﬁnd
most hosts in these networks are subject to nearly identical reacha-
bility policies at Layer 3.
Categories and Subject Descriptors
C.2.3 [Network Operations]: Network management
General Terms
Design, Management, Measurement
Keywords
Conﬁguration Management
1.
INTRODUCTION
Modern enterprises impose a variety of constraints on point-to-
point network communication. These constraints limit an enterprise
host’s ability to access various network resources, including other
enterprise hosts and various servers. In most enterprises, these re-
strictions are realized using a combination of different mechanisms
in multiple network devices, including ACLs in ﬁrewalls and other
middle-boxes, policy maps and packet ﬁlters in routers, and VLANs
which cut across multiple network routers.
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’09, November 4–6, 2009, Chicago, Illinois, USA.
Copyright 2009 ACM 978-1-60558-770-7/09/11 ...$10.00.
The network’s high-level reachability policies — i.e. the speciﬁc
rules that govern whether or not, and how, different network end-
points can communicate — are seldom “written down” explicitly.
The point-to-point reachability policies are implemented indirectly
using the above mechanisms and hence they are buried in router
conﬁguration ﬁles, or worst still, in the minds of network engineers.
To date, there has been no systematic study of the range of reach-
ability policies that modern enterprises implement. In large part this
is because there have been no measurement tools or techniques to
aid the analysis of this policy. Better understanding of enterprise
reachability policies would have several beneﬁts. In particular, it
can inform the design of new approaches for implementing poli-
cies, such as clean-slate schemes [5, 4, 8, 2]. It can also point to
simpler ways to conﬁgure current networks that would realize the
same reachability policies while incurring signiﬁcantly lower con-
ﬁguration complexity [3].
In this paper we introduce the notion of policy units, which is
an abstract representation of how the global reachability policies
implemented by an operator within her enterprise apply to different
end-points in the enterprise (an end-point being an IP address in a
subnet belonging to the network). Each policy unit is a maximal set
of end-points that are all “treated alike” by the network in terms of
the reachability constraints that apply to their communications with
the rest of the network. Each network end-point belongs to one and
only one policy unit.
Our paper makes two contributions: First, we show a method for
how the policies of a network can be automatically extracted from
the static router conﬁguration state of the network. Second, we
apply our method to 5 networks and verify the output with operators
of some of the networks. Among these networks, we found the
number of policy units implemented at Layer 3 varies from 2 to 40
and is largely independent of network size. However, in all these
networks, over 70% of end-points are contained in just a few units.
The operators reported they were interested to see which end-points
fall into which policy units, indicating that the policy unit concept
offers operators a new way to view their networks.
The rest of this paper is structured as follows: In the section that
follows, we deﬁne a policy unit and explain our method for com-
puting them.
In §3 we present the results of our experiments on
the 5 networks, and we show how the policy units vary across the
networks.
In §4 we describe the applications of policy atoms in
network management. We discuss related work in §5 and conclude
in§6.
2. DEFINITIONS AND APPROACH
In this section, we deﬁne a policy unit and describe a preliminary
approach for extracting policy units from a network’s conﬁguration
state.
136s
s
e
r
d
d
A
P
I
n
o
i
t
a
n
i
t
s
e
D
Customer Service
Products
Sales
 200
 150
 100
 50
 0
 0
 20  40  60  80  100  120  140
Source IP Address
Figure 1: An enterprise with 3 departments and 4 policy units.
The Products department consists of two units one of which cor-
responds to administrators.
2.1 What are Policy Units?
Policy units model how a network differentiates amongst its hosts
in terms of the reachability constraints it imposes on them. Two
end-points in an enterprise network belong to a policy unit if and
only if the same set of reachability constraints apply to them when
sending trafﬁc to all network end-points. Policy units divide the
set of all end-points in an enterprise network into disjoint subsets,
where every end-point belongs to one and only one policy unit.
Enterprises can differ signiﬁcantly in the number and kind of pol-
icy units they implement. For example, the simplest enterprise net-
work could treat all hosts in an identical fashion – all end-points in
the network would then belong to the same policy unit. In a slightly
more complex scenario, policy units could align with departmental
boundaries – e.g. all hosts in the CS Department could belong in
one policy unit and those in EE could belong in another unit. In
other more complex scenarios, enterprises may impose ﬁne-grained
distinction among hosts in a department as well as across depart-
ments. For example, consider an enterprise with three departments:
Sales, Products and Customer Support. Suppose that hosts in each
department can access different sets of end-points in the rest of the
network, as illustrated in Figure 1. For instance, hosts in Sales have
IP addresses ranging from 50 to 80, and they can reach IP address
ranges 50-80 and 100-140. Similarly, hosts in Customer Service can
reach IP address ranges 90-140. Suppose further that a small group
of hosts in the Products department, with IP addresses 18-20, have
greater reachability to the rest of the network (i.e, they can reach
IP addresses 20-140) than others hosts in the department (who can
only reach 20-40 and 100-140); the small group of hosts could be
machines used by administrators for the entire network. Such an en-
terprise network would have at least four different policy units, one
each corresponding to the following source IP address ranges: 18-
20, 20-40, 50-80 and 90-140 (see Figure 1). Our empirical study in
Section 3 shows that enterprises range from the very simple to the
very complex in terms of the policy units they implement in their
networks.
Formally, each policy unit is an equivalence class E on the re-
lation R: H × P ow( ˆH × C1 × ... × Cm × A). Here H is some
subset of the set of all end-points in the network, and ˆH is some
subset of the set of all end-points in the network that supersedes H.
A = {permit, deny}, is set of all actions that the network takes
on any communication. Ci are the characteristics of the packets
the network policy cares about, for example source port, destination
port and protocol. Finally, P ow( ˆH × C1 × ... × Cm × A) is the
power set (the set containing all subsets of a set) of the set of all
end-point/packet-characteristic/action tuples. The policy units for a
network are then the equivalence classes that partition the domain
of R (i.e., the network end-points). An equivalence class E of the
relation R is a set of end-points whose communication with any
end-point in the rest of enterprise is treated in exactly the same way
by the network as a whole.
2.2 Deriving Policy Units from Conﬁguration
We describe an approach for extracting policy units from a net-
work’s static conﬁguration state.
Our strawman approach described below applies to policy units
implemented in Layer 3 in enterprise networks. Thus, our scheme
takes as input router conﬁguration ﬁles. Reachability constraints at
this layer are often implemented using control plane mechanisms
such as routing protocol adjacencies and route ﬁlters [10, 12], and
data plane mechanisms such as router ACLs [10, 12]. Since we only
focus on Layer 3, each policy unit we derive is a union of other-
wise disjoint IP address ranges. In other words, our approach treats
each IP address in the enterprise as representing a single end-host,
and tries to group together IP addresses that are similar in terms of
reachability constraints into a single policy unit.
Our scheme works in three stages. First we calculate the extent
of reachability between pairs of routers in the network, i.e., set of
packets that can be exchange between routers. Then we calculate
the reachability between pairs of subnets in the network. From this
subnet-level information, we ﬁnally derive policy units using a geo-
metric heuristic.
2.2.1 Router-Level Reachability Sets (RRS)
For the ﬁrst stage, we employ a reachability analysis tool devel-
oped in our prior work [3]. The tool models the impact of both
control and data plane mechanisms to compute the set of packets
that can be exchanged between a pair of routers. This tool has two
components:
(1) Control Plane Simulation: This simulates the interactions be-
tween routing protocols and control plane ACLs (route maps) to
determine the forwarding entries for routers in the network. Our
simulator accounts for the presence of VLANs and multiple routing
protocols such as RIP, OSPF and BGP. The core idea is to sim-
ulate the exchange of routes from the local RIBs (route informa-
tion bases) of the various routing protocols deﬁned on each network
router. Whether or not routes can be exchanged between two rou-
ters is based on the conﬁgured routing protocol adjacencies as well
as physical adjacencies in the topology. We apply control plane ﬁl-
ters before routes are exchanged to model control plane restrictions
on route propagation. Whenever multiple route options are avail-
able, our tool break ties in favor of the shortest path, but this could
be extended to accommodate more complex choices of routes. In
the end, we generate a forwarding table (FIB) for each router in the
network, i.e., the list of next hop routes for each destination subnet.
(2) Applying data plane constraints: This component models how
the data plane ACLs deﬁned in other routers on the path between a
pair of routers, and ﬁltering rules deﬁned in on-path ﬁrewalls and
middle-boxes, impact which packets are ﬁltered before reaching the
destination router. Deﬁne the path between routers R1 and R2,
path(R1, R2), as an ordered list of router interfaces that packets
originating from router R1 have to transit to arrive at router R2 (the
path is obtained by examining the FIBs in the network’s routers or
from the control plane simulation above). For path(R1, R2), we
can deﬁne the Data Plane and Control Plane sets for the router-
interfaces appearing on the paths. The Data Plane sets for an inter-
face i on a router d appearing on path(R1, R2) are deﬁned as the
137sets of packets from R1 to R2 that the inbound packet ﬁlter deﬁned
for the interface allows into that interface, and the outbound packet
ﬁlter allows out of that interface. Similarly, data plane sets for on-
path packet ﬁltering mechanisms like ﬁrewalls and other middle-
boxes. Note that because the data plane set is determined by ﬁltering
rules, each set is naturally represented as an ACL, i.e. a sequence
of permit statements followed by a blanket deny statement. The
Control Plane set for an interface i on a router d is the union of all
packets that are routed out of interface i to various destinations, and
this is a function of d’s FIB entries. Note that the control plane set
can also be represented as an ACL, i.e. a sequence of permit state-
ments one corresponding to each FIB entry, followed by a blanket
deny statement.
Using the control and data plane sets deﬁned per interface on
path(R1, R2), the router-level reachability set between R1 and R2,
or RRS(R1, R2) , can be obtained as the intersection of the control
and data plane sets for the routers and interfaces in path(R1, R2).
Given the ACL representations of each set above, intersections are
easy to compute because the intersection of two ACLs is simply the
intersection of each pair of rules in the two ACLs. It follows that
the ﬁnal router reachability set is also represented in ACL form.
2.2.2 Subnet-Level Reachability Sets (SRS)
The routes obtained above only yield binary information regard-
ing the reachability between the subnets attached to a pair of source,
destination routers: if a route exists to a destination subnet, then the
above assumes that all subnets attached to the source router can have
unfettered access to the destination subnet. However, even if a route
exists, a subnet may not have unfettered access due to data-plane
restrictions at the interfaces connecting the subnets.1
To model reachability more accurately, from the router-level reach-
ability sets we obtain the exact subset of packets that can be ex-
changed between a given pair of subnets. We refer to this as the
subnet-level reachability set. To compute the subnet-level reacha-
bility set, SRS(s1, s2), between subnets s1, s2 attached to routers
R1 and R2 respectively, we ﬁrst calculate the set of packets that s1
can send into the network given limitations created by data plane
ACLs (in-bound ACLs) on the interface connecting s1 to its gate-
way router R1; we call this entry(s1). Second, we similarly deﬁne
the exit(s2) as the set of packets the network will deliver to s2
given the limitations created by the ACLs (out-bound ACLs) on the
interface connecting the subnet to router R2.
SRS(s1, s2) is then the intersection of entry(s1), exit(s1) and
RRS(R1, R2). As before SRS(s1, s2) can be computed using
ACL intersections, ﬁrst between entry(s1) and exit(s1), and then
between the result of the previous intersection and RRS(R1, R2).
Suppose a network owns n subnets. Using the above approach,
we can derive subnet-level reachability sets between every pair of
subnets and the n2 such sets can be represented using an n × n
matrix. We refer to this as the SRS matrix.
An example of an SRS matrix for a hypothetical network with
three subnets is shown and explained in Figure 2. In this example,
there are 9 subnet-level reachability sets that can be interpreted as
follows. Consider the SRS for packets originating at subnet A (col-
umn 1) and destined for subnet B (row 2). The ﬁrst half of subnet
A has unfettered access to subnet B. In contrast the second half of
subnet A can only communicate with the bottom half of subnet B.
Unlike the other two subnets, subnet C has unfettered access to all
subnets in the enterprise (col 3).
Since we use ACLs to represent the reachability sets, each SRS
1 Prior work doesn’t examine data-plane ﬁlters on the interfaces
connecting the subnets, it only examines ﬁlters on the interfaces
along the path between the gateway routers
Figure 2: An SRS matrix for a network with three subnets. For
this example, the constraints placed by the network apply only
to the source and destination IPs.
in Figure 2 is represented by one ACL with multiple rules in it.
For instance, the SRS between A and B (the set in col 1, row 2)
could be represented by an ACL with two rules, one covering the
reachability between the ﬁrst half of the IPs in A and all of B, and
the other covering that between the second half of A and the lower
half of B.2
2.2.3 Policy Unit Extraction
Using the formal deﬁnition from Section 2.1, we seek to ﬁnd sets
of IP addresses H such that each set is as large as possible, the sets
partition the space of all IP addresses in the network, and for each
IP address in set H the values of P ow( ˆH × C1 × ... × Cm × A) that
map to it are identical.
Identifying the maximal sets H is made harder by the fact that the
boundaries of H will most likely not align with the subnet bound-
aries in the network. For instance, in the example above (Figure 2),
the ﬁrst and second halves of subnet A differ in the constraints that
the network imposes on them, and hence they belong to different
policy units. In contrast, the network treats the ﬁrst half of subnet
A identical to the (disjoint) subnet C and hence they should belong
to the same policy unit. As a result, a policy unit may consist of
multiple subnets, and a subnet may contain several policy units.
We propose a geometric heuristic to identify the exact granularity
at which policies are deﬁned within a subnet. For ease of descrip-
tion, we assume below that constraints apply only on source and
destination IP addresses on packets, and none apply on ports/ pro-
tocols, but our heuristic can be easily generalized to also consider
ports/ protocols. We present an intuitive description. A formal spec-
iﬁcation of our heuristic can be found in Figure 3 where we assume
that the SRSes are represented using ACLs.3
Intuitively, we can extract policy units from the SRS matrix ob-
tained above by identifying groups of source addresses for which the
subnet-level reachability sets are equivalent. In particular, we de-
compose the subnet-level reachability sets corresponding to a source
subnet (i.e. each column in Figure 2), into “rectangles” such that
that all the source addresses in the x-dimension of a rectangle can