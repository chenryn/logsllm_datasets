### Experiment Setup and Configuration

Our approach is based on the principles outlined in [22] for DroidBot. We use the default configuration of Stoat, along with its random text input generation capabilities.

#### Dataset and App Selection
We have compiled a dataset of 40 Android applications, as detailed in Table IV, to compare the performance of various exercisers. The selection process for these apps is as follows:

1. **Initial Selection**: We chose the top 500 most downloaded apps from all categories except games on Google Play.
2. **Random Sampling**: From this pool, we randomly selected 1,200 apps for our initial analysis, as shown in Table I.
3. **Filtering**: To ensure compatibility, we further filtered the apps to include only those requiring an Android version lower than 4.3, making them suitable for sensitive behavior detection in §V-B.
4. **Code Coverage Measurement**: We used Ella [35] to measure the code coverage of the remaining apps. For apps that Ella could not instrument, such as Yippi and BlackWhiteMeet, we used miniTracing [36]. Both tools, like many others, cannot cover or instrument native code.

#### Experiment Environment
- **Devices**: Four OnePlus 6T mobile phones, each running Android 9.0 (System build number A6010 41 181115).
- **Server Connection**: All devices are connected to either a Windows Server 2018 or an Ubuntu 16.04 server via Android Debug Bridge (ADB) [37]. The choice of server platform depends on the requirements of the corresponding tools.
- **Experiment Duration**: Each experiment runs for one hour, a duration commonly adopted in prior work [19], [25], [33]. Each experiment is repeated three times to reduce randomness.
- **Data Management**: After each experiment, we revert the app's private data and any modifications to shared resources, such as the SD card and system services. If ADB goes offline, our Xposed module automatically reboots the Android system and reconnects ADB.

### Code Coverage Metrics
We measure the code coverage of Android apps using two metrics:
- **Method Coverage**: The number of methods instrumented by Ella or miniTracing and triggered during exercising.
- **Activity Coverage**: The number of triggered activities on the stack during exercising, as registered in the AndroidManifest.xml.

### Results: Code Coverage of Popular Android Apps
Table V presents the method and activity coverage results for 40 apps achieved by three existing exercisers. On average, TextExerciser outperforms or matches the default text exerciser (random or pre-defined) in terms of code coverage. Specifically:
- **Monkey**: TextExerciser triggers 48.5% more activities and 29.0% more methods.
- **DroidBot**: TextExerciser triggers 37.0% more activities and 20.2% more methods.
- **Stoat**: TextExerciser triggers 45.3% more activities and 26.4% more methods.

**Note**: In some cases, the number of triggered methods by Stoat+TextExerciser is slightly lower than Stoat+Random, especially when text inputs have fewer constraints. This is due to the overhead of communication between Stoat (Ruby) and TextExerciser (Python) via a command line pipe, which adds an average of 8.3 minutes per app, compared to 3.1 minutes for Monkey+TextExerciser and 3.4 minutes for DroidBot+TextExerciser.

### Improvement in Dynamic Analysis
To address RQ2 (Can TextExerciser improve existing dynamic analysis of Android apps?), we evaluated TextExerciser on existing dynamic security analysis tools, specifically TaintDroid [11] and ReCon [23]. We also implemented a keyword-based traffic analysis to detect privacy leaks, using keywords listed in Table VI.

#### Experiment Setup
- **Devices**: Two Android Nexus 4 phones with Android 4.3 systems, as required by TaintDroid and ReCon.
- **Comparison**: Due to DroidBot's incompatibility with this environment, we compared TextExerciser with Monkey and Stoat.
- **Apps**: The same popular apps as in §V-A.
- **Duration**: One hour per experiment.

#### Results: Privacy Leak Detection
Figure 7 shows the results of privacy leak detection. We removed redundant results, keeping only unique source and sink pairs for TaintDroid. Key observations include:
- **Improved Detection**: All dynamic analysis tools detected more privacy leaks with the help of TextExerciser.
- **Reasons for Improvement**:
  - **Valid Inputs**: TextExerciser generates valid inputs that satisfy input restrictions, allowing it to explore deeper code branches and trigger more critical app behaviors.
  - **Personal Information**: Many privacy leaks occur when users input personal information, and TextExerciser can generate such inputs, leading to more comprehensive detection.

This setup and methodology provide a robust framework for evaluating and improving the effectiveness of dynamic analysis tools in detecting sensitive behaviors and privacy leaks in Android applications.