MobiCom, 2008.
[31] Wee Lum Tan, Fung Lam, and Wing Cheong Lau. An
empirical study on the capacity and performance of 3g
networks. IEEE Transactions on Mobile Computing,
7(6):737–750, 2008.
[32] J. Robinson and E.W. Knightly. A performance study of
deployment factors in wireless mesh networks. 2007.
[33] Aruna Balasubramanian, Ratul Mahajan, Arun
Venkataramani, Brian Neil Levine, and John Zahorjan.
Interactive wiﬁ connectivity for moving vehicles. In
SIGCOMM, 2008.
[34] Ratul Mahajan, John Zahorjan, and Brian Zill.
Understanding wiﬁ-based connectivity from moving
vehicles. In IMC, 2007.
[35] Mahajan, Ratul and Padhye, Jitendra and Agarwal, Sharad
and Zil, Brian. E pluribus unum: High performance
connectivity on buses. Technical Report MSR-TR-2008-147,
Microsoft Research TechReport, 2008.
[36] Aruna Balasubramanian, Ratul Mahajan, and Arun
Venkataramani. Augmenting mobile 3g using wiﬁ. In
MobiSys, 2010.
[37] Aruna Balasubramanian, Brian Neil Levine, and Arun
Venkataramani. Enhancing interactive web applications in
hybrid networks. In MobiCom, 2008.
[38] Xiaolan Zhang, Jim Kurose, Brian Neil Levine, Don
Towsley, and Honggang Zhang. Study of a bus-based
disruption-tolerant network: mobility modeling and impact
on routing. In MobiCom, 2007.
[39] Network test. http://networktest.org/.
112Summary Review Documentation for 
“Can They Hear Me Now?: A Case for a Client-Assisted 
Approach to Monitoring Wide-Area Wireless Networks” 
Authors:  S. Sen, J. Yoon, J. Hare, J. Ormont, S. Banerjee 
Reviewer #1 
Strengths: Lengthy explanation of the choices of the parameters 
(size of zones, epochs and number of samples). 
Large  set  of  measurements  over  space,  time  and  wireless 
networks. 
Evaluate  the  benefits  of  a  multi-network  strategy  for  data 
connectivity. 
Weaknesses:  The  framework  idea  is  expected,  so  a  key 
contribution  is  the  choice  of  parameters.  However  since  it  is 
mainly  based  on  data  collected  in  once  city,  it  is  not  clear  how 
reusable the parameters for over environments like countryside or 
dense urban areas like NYC. 
Comments to Authors: It is great to see such attention spent on 
choosing  the  right  parameters  for  your  system.  I  do  not  have 
many comments as the paper was well written and the idea was 
clear. 
Since you collect data from buses, I was wondering if it would not 
introduce  some  bias  in  your  data  set.  For  instance,  data  in  one 
location often collected around the same time. 
Aggregation in space: 
It seems that you have done your study by aggregating over time 
instead of keeping time constant.  
How  would  the  standard  deviation  look  like  if  you  would 
compare different zones during the same time of the day? 
It  would  be  nice  to  get  similar  results  from  other  environments 
like Manhattan or the countryside. 
Did  you  evaluate  the  incremental  benefits  of  having  zones  of 
different sizes or different shapes (e.g., clustering)? 
RTT is an important metric for wireless environments. Why is it 
not part of the zone size evaluation? 
I find your last section on multi-Sim, MAR, network dominance 
interesting.  You  might  want  to  develop  that  aspects  in  future 
papers.  
Reviewer #2 
Strengths:  The  paper  has  rich  data:  3  cellular  network 
performance  measured  by  clients  at  several  different  locations. 
The  use  of  client-side  measurements  for  network  monitoring 
makes a lot of sense. 
Weaknesses:  The  paper  mostly  focuses  on  aggregation,  but 
aggregation  would  miss 
temporal  and  spatial 
dynamics. 
interesting 
Comments to Authors:  I enjoy reading your paper. The goal of 
understanding the effectiveness of using client-side measurement 
to quantify the wide-area wireless network performance makes a 
lot of sense. Understanding the level of aggregation both in time 
and  in  space  is  useful.  On  the  other  hand,  using  aggregation  to 
find the time interval and zones to smooth out the data is simple, 
and the paper should shorten this part, and spend more time on the 
more  interesting  and  less  obvious  part  on  the  applications. 
Moreover, aggregation filters out interesting variation across time 
and space, which is equally interesting to the aggregation results 
if not more and is more challenging technically than aggregation. 
I would like to see some discussion on this part.  
The  authors  promise  to  publish  their  datasets,  which  would  be 
very useful to the community. 
Reviewer #3 
Strengths:  -  detailed  measurement  study  which  is  well  thought 
out 
- empirically informed design 
- potential impact of the data. 
Weaknesses:  -  tedious  to  read,  though  its  not  this  particular 
paper’s fault 
-  not  clear  how  actionable  the  results  are  except  for  long  term 
network planning and fault detection. 
Comments to Authors:  My main comment is on how this data 
can  be  acted  on.  For  example,  it  is  not  clear  if  a  user  observed 
poor  performance,  how  this  data  can  help  him  improve  or  even 
diagnose  his  performance  problems.  While  it  is  nice  to  have  a 
sense  of  the  network  performance  at  long  time  scales,  it  is  not 
clear how useful it is to the end user. 
Can  we  isolate  uplink  and  downlink  performance?  It  would  be 
useful since cellular links are notoriously asymmetric.  
How  are  measurements  impacted  by  client  parameters  such  as 
battery levels etc? I guess since you are using laptops this is not a 
concern,  but  with  smart  phones  battery  dictates  uplink  and 
downlink behavior which would bias your measurements. 
Reviewer #4 
Strengths: Extensive measurements have been collected, from a 
variety  of  situations.  The  framework  has  some  practical 
applications. The authors intend to share their measurements. 
Weaknesses: The active measurement approach seems less ideal 
than  a  passive  one  for  gathering  the  information  for  network 
providers. The presentation needs quite a bit of improvement. In 
113to  get 
to  collect  (only)  active  measurements 
particular,  it  is  often  difficult  to  follow  which  of  the  variety  of 
data sets is being used. The paper does not consider smart phones, 
and  in general does not consider the various practical issues for 
deploying this in practice. 
Comments to Authors:  A more compelling motivation is needed 
as to why network operators would want to use this framework. I 
agree that these operators would like more information on where 
problems are occurring in their network, for the various reasons 
that  you  describe  in  the  paper.  What  I  do  not  agree  with  is  the 
need 
that 
information. Most of the information shown in this paper could be 
gathered  with  passive  measurements,  which  have  numerous 
benefits:  no  overhead  on  the  wireless  infrastructure,  they  are 
measurements  of  actual  user  traffic,  they  would  not  require 
instrumenting the user devices, etc. Only in cases where the user 
could not connect to the network might active measurements be 
useful. 
I agree that users could take advantage of this framework, e.g., to 
compare  the  performance  of  multiple  network  providers  in  a 
given location. However, that seems to have limited applicability, 
at  least  for  cell  phone  users,  the  majority  of  whom  will  have  a 
contract with one provider (even though some will have multiple 
SIM cards, as stated in your paper). 
Since access to the passive measurements may not be available to 
you, the active measurements that you have collected could fill in. 
However, the paper currently does not touch on the issue of the 
benefits of passive measurements for the provider, which is why 
my rating is lower than it might otherwise have been. 
Minor Comments 
Section 2 
- In the discussion of the WiRover data set, the throughput tests 
could  affect  other  users  in  all  of  the  wireless  environments,  not 
just the tests done on buses. 
-  Please  clarify  where  the  various  tests  were  run  between  (i.e., 
where is the server(s) that is used in each test located?) 
Section  3.1:  In  “A  closer  look”,  please  explain  the  intuition 
behind “driving around in a car within a 250 meter radius”; i.e., 
people may sit at a location that is within a short distance of an 
access point, but not too many will drive around in a circle to stay 
in range. 
Section 3.2: It would help to have some more insight on what test 
duration  will  provide  a  reasonable  throughput  value  with  low 
enough  variance  for  the  tasks  that  a  user  may  care  about;  e.g., 
someone trying to decide between using provider A and provider 
B is not going to run two 30 minute tests; they might run two 30 
second  tests,  if  that  is  sufficiently  long  to  make  a  confident 
decision 
Section 3.2.2: A time duration of 75 minutes seems rather long for 
a throughput test that is supposed to have “low overhead” 
Section  3.3:  In  the  discussion  of  UDP  downloads  (or  earlier), 
clarify how many clients you have at your disposal 
Section 3.3.1:  
-  Can’t  a  provider  monitor  available  bandwidth  at  each  access 
point,  rather  than  run  active  tests?  (i.e.,  is  there  any  technical 
reason  they  could  not  do  this,  even  if  they  are  not  doing  so 
today?) 
- While UDP downloads will obtain useful information, it seems 
like  a  heavyweight  way  to  do  so.  why  (generally  speaking)  can 
providers not obtain this information passively? 
- In “Summary”, why would there be any doubt that one could use 
clients  under  your  direct  control 
to  estimate  network 
performance? 
Section 3.4: clarify what is meant by “sufficient collect accurate 
statistics” 
Section 4: in the caption of Figure 9, shouldn’t “200 meter radius” 
be “250 meter radius”? 
Section 4.1 
-  Clarify  why  throughput tests “cannot detect zones with highly 
variable  performance”;  it  seems  quite  plausible,  depending  on 
how you conduct the tests and measure the results 
-  Note  that  passive  measurements  by  the  provider  could  also 
determine this information. 
Section 4.2.1: Clarify which is the “best network’s metric” 
Section 4.2.2 
- While there are certainly users who use multiple SIM cards, it 
seems  like  it  will  be  a  minority  of  users,  due  primarily  to  cost; 
thus, I do not find the arguments here particularly compelling 
- SURGE uses a synthetic workload, does it not? Are you testing 
against a Web server running these synthetic pages? (If so, please 
clarify) 
Section  5: 
if  you  consider  my  comments  on  passive 
measurements,  then  there  will  obviously  be  a  bunch  of  new 
material to consider here 
Reviewer #5 
Strengths: Real data, geographically dispersed, over a year.  Data 
submitted to CRAWDAD. 
Weaknesses:  Most  of  the  hard  systems  problems  were  not 
confronted, even the stated challenge of scale.  It is not clear how 
typical their data is with respect to line-of-sight problems. 
Comments to Authors: There are really two parts to this paper: 
the  measurement  collection  system  and  the  results  of  the 
measurements  themselves.  The  measurement  collection  system 
seems  to  ignore  all  of  the hard parts of the problem - including 
those  brought  up  and  stated  by  the  authors.  The  measurements 
themselves  may  be  of  independent  use  for  others,  particularly 
because they are being made publicly available via CRAWDAD. 
For  the  system,  you  state  in  the  intro  that  the  core  technical 
challenges are related to scaling, but no where in your system 
do  you  *explicitly*  consider  scaling.  Yes,  you  spend  a  lot  of 
time trying to figure out how often to measure something (my 
concerns for that are below) but you never relate it back to the 
amount  of  bandwidth  or  power  consumed  at  the  client  (as 
implied  by  both 
intro)  or  how  many 
simultaneous  clients  your  system  can  support  or  what 
infrastructure  would  be  required  to  monitor,  for  example,  a 
nationwide  cellular  deployment.  Your  measurements  have 
fewer  than  20  clients  in  total  and  no  mention  was  made  as  to 
how many of these were working in parallel.  Further, it reads 
the  abstract  and 
114inference  about 
as if all of your measurements were done with laptops instead 
of  handheld  devices,  so  no 
the  power 
consumption of WiScape can be made either.  The lack of these 
points makes the assertion that client-side monitoring is viable 
unsupported.   
Further,  the  big  challenge  that you ignore is deployment: how 
do  you  get  your  measurement  infrastructure  (be  it  an  app  or 
what  have  you)  deployed  on  to  enough  client  phones  to  get 
decent  coverage?  In  my  opinion,  this  is  the  hardest  part  of  a 
client-side  measurement  system  and  it’s  not  at  all  mentioned 
here. 
The  fact  that  WiScape  has  apparently  not  been  tested  on  an 
actual  portable  handset  seems  to  be  a  big  strike  against  the 
viability of this system. 
Now, ignoring all of that, there are still problems. Most of the 
interference  in  cellular  networks  is  caused  by  line  of  sight 
issues.  How typical are your data sources with respect to line 
of  sight?    Intuitively,  a  bus  on  a  road  outside  does  not  have 
significant  line  of  sight  issues.    Your  data  description  of  your 
spot  nodes  was  not  sufficient  to  determine  what  line-of-sight 
issues they had.  So, it is not clear how your empirical zone and 
epoch  derivations  would  change  if  the  client  was  indoors,  for 
example. 
That  said,  the  graphs  in  Figure  5  seemed  very  interesting  -  if 
this paper is the first to present this level of data (from different 
regions, over time), please say so. 
In Section 3.1, is the assumption that zones are square?  If yes, 
please say so. Any reason not to consider other shapes? 
The data in Fig 4. seems to contradict Fig 1. and Fig.9 ; why are 
all of the relative standard deviations so small in Fig 4 (  0.7:  it  seems  unintuitive  that  the  stddev 
should be so small in Fig 4 (independent of zone size) when lots 
of the other Figures show much higher variation. 
In Section 3.2, the implicit assumption here is “stable is better”, 
where  I  am  not  convinced.  Intuitively,  operators  are  not  as 
interested  in  the  average  performance  as  much  as  the  “trouble 
spots”,  so  by  optimizing  for  inter-epoch  stability,  are  you  not 
just averaging away what the operator wants to know? 
The conclusion did not add much to the paper: were there any 
lessons  learned?  Are  the  plans  for  a  WiScape  iOS  or  Android 
app? What are your concrete next steps? 
Response from the Authors 
We  thank  the  reviewers  for  their  constructive  comments  that 
helped improve the paper. We have fixed the text to address some 
of the reviewer concerns regarding clarity of description.  
Some  questions  regarding  interpolability  of  our  observations  in 
bigger cities, while using various cellular phones, in presence of 
severe  line-of-sight  issues  etc.,  can  be  answered  only  by 
increasing  the  scale  of  our  study  to  a  broader  geographical 
location and by involving more people in the process. We intend 
to do so in future and have expanded the conclusions section to 
describe our concrete next steps to this end.  
In  particular  we  understand  that  design,  implementation  and 
deployment  of  client-assisted  wide-area  wireless  network 
monitoring  systems  would  involve  addressing  multiple  hard 
challenges,  a  few  of  which  are  described  by  Reviewer#5. 
However,  designing  and  deploying  the  system  is  not  the  main 
focus  this  paper.  Our  main  contribution  is,  first,  to  analyze 
measurement  data  for  three  cellular  service  providers  over  the 
span of an entire city, and at various other locations for a period 
of  two  years.  Second,  we  identified  characteristics  of  wide-area 
cellular networks which help estimate performance efficiently and 
highlighted  how  such  minimal  coarse  grained  client  collected 
measurements can be leveraged by some applications. We believe 
that establishing the effectiveness and utility of lightweight coarse 
grained  monitoring  is  an  important  first  step  for  designing  and 
implementing a scalable client-assisted monitoring system.  
That  said,  as  mentioned  in  Section  6,  we  are  actively  working 
towards building such a measurement platform for both vehicular 
communication systems as well as for cellphone users.  
We  also  agree  with  Reviewer#4  that  passive  measurement  is  a 
potential 
lightweight  alternative  for  understanding  network 
performance. We are presently in the process of augmenting our 
measurement  mechanisms  with 
such  passive  estimation 
techniques.  In  future  we  intend  to  publish  our  findings  on  the 
same.  
Finally, we have shown how client assisted in Section 4 can help 
network service providers and users of multiple cellular cards. We 
agree with Reviewer#3’s concern on the utility of client-assisted 
measurements  for  individual users. Client-assisted monitoring in 
the context of cellular networks is a new technique and we intend 
explore  other  aspects  of  cellular  network  monitoring  that  client 
assisted monitoring can help with.  
Specific comments:  
Reviewer#3: Test duration of 75 minutes… The measurements for 
a zone need to be retaken every 75 minutes, the test themselves 
involve  sending  40-120  packets  (as  noted  in  Table  5).  We  have 
made this clearer in the text.  
Reviewer#5:  In  Section  3.2,  ... here is “stable is better”, ... We 
meant “stable is better” in the sense that, we can potentially take 
less number of measurements to converge to the correct estimate 
if the network is stable.  
Reviewer#5: The data in Fig 4. seems to contradict ... In Figure 1, 
the zones have a radius of 800 meters, and include locations with 
less  than  200  measurements,  hence  some  zones  show  high 
standard  deviation.  In  Figure  9  the  higher  relative  standard 
deviation is for zones with multiple ping failures, which we took 
as an indicator of high performance variance. The high standard 
deviation  for  such  zones,  thus,  when  compared  to  the  small 
relative standard deviation for the aggregation of all zones (also 
plotted in same graph) proves our point.  
Reviewer#5:  Line  of  sight  issue  ...  All  our  static  measurements 
were taken in indoor locations and hence were not in line-of-sight 
from the base station, we have clarified it in the text. 
115