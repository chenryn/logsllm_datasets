ternal services (h3 and h4), user workstations (h5 and h6),
an external network OpenFlow-based SDN switch (s1), a
DMZ ﬁrewall OpenFlow-based SDN switch (s2), local in-
ternal (intranet) OpenFlow-based SDN switches (s3 and s4),
and an OpenFlow-based SDN controller (c1). Thus, H =
{h1, h2, h3, h4, h5, h6}, S = {s1, s2, s3, s4}, and C = {c1}.
We model the data plane network topology ND as shown in
Figure 8. We assume that the network is centrally controlled
through one controller and that the controller maintains sepa-
rate control plane connections NC with each switch, as shown
in Figure 9. Thus, NC = {(c1, s1), (c1, s2), (c1, s3), (c1, s4)}.
2) Experimental Setup: We used the National Science
Foundation’s GENI [18] networking testbed to deploy a topol-
ogy of eleven virtual machine (VM) hosts, with six VMs acting
as end hosts, four VMs acting as virtual OpenFlow-enabled
switches for the data plane, and one VM acting as the control
plane network switch (not shown in Figure 8). Each VM ran
the Ubuntu 14.04.1 LTS operating system and contained one
core of an Intel R(cid:11) Xeon R(cid:11) E5-2450 2.10 GHz processor and 1
GB of memory. Each network link had 100 Mbps bandwidth.
For controllers, we used Floodlight v1.2 [6], POX
v0.2.0 [14], and Ryu v4.5 [15]. We selected these con-
trollers because all three include a simple learning switch
application and provide open source
cross-
comparison study. We used Floodlight’s Forwarding mod-
ule, POX’s forwarding.l2_learning module, and
Ryu’s simple_switch.py application as representative
network applications for implementing a learning switch.
code
for
it
For switches, we used Open vSwitch (OVS) v1.9.3 [7]
because of its ﬂexibility and logging, but our approach would
be equally applicable to hardware-based switch implemen-
tations without requiring any changes to the injector. For
all experiments, we used OpenFlow v1.0 [8] because it is
the earliest stable protocol version;
is the most widely
implemented [19]; and it provides the necessary primitives for
forwarding behavior, topology information, and conﬁguration.
We used the ping and iperf utilities to generate data
plane trafﬁc. The ping utility generates ICMP messages to
test for end-to-end connectivity, and the iperf utility mea-
sures the bandwidth (throughput) of TCP connection requests
between a client and server. We used log data from the
ping and iperf utilities, the controller processes, and the
runtime injector. The runtime injector logged all control plane
connections, all messages sent across such connections, and
rule notiﬁcations (when actuated).
B. Flow Modiﬁcation Suppression Attack
We attempted to disrupt switch ﬂow table modiﬁcation by
intercepting and dropping ﬂow modiﬁcation requests.
1) Rationale: An attacker may wish to disrupt the ﬂow
modiﬁcation requests to cause a degradation or denial of
service in the control or data planes. A controller issues
ﬂow modiﬁcation requests via a FLOW_MOD message to add,
modify, or delete a switch’s ﬂow entries in that switch’s
ﬂow table. The ﬂow entries dictate the switch’s forwarding
behavior for incoming data plane trafﬁc, and in this attack,
we manipulate ﬂow modiﬁcations.
2) Method: Consider the case of an incoming data plane
packet to a switch without a matching ﬂow rule. The switch
forwards the packet to the controller; the controller makes
a decision; and the controller sends the packet back to the
switch’s data plane. The controller may also instantiate one
or more ﬂow modiﬁcation requests so that future data plane
packets that match the initial data plane packet need not be
sent to the controller each time for a decision.
Now consider the case when the ﬂow modiﬁcation requests
are suppressed. The attack drops the request, and as a result,
the switch does not instantiate the corresponding ﬂow entry.
Subsequent data plane packets of the trafﬁc stream result in
ﬂow table misses, and thus every data plane message might
be sent
to the controller for processing. The overhead is
signiﬁcant: for every n packets in the data plane that are ﬂow
574
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:59:29 UTC from IEEE Xplore.  Restrictions apply. 
σ1 : σ1 = {φ1 , φ2 , φ3 , φ4} (σstart = σ1 ; σabsorbing
= {σ1}; σend
= ∅)
φ1 = (n1 , γ1 , λ1 , α1 )
N oT LS
n1 = (c1 , s1 )
γ1 = Γ
λ1 = READMESSAGEMETADATA(msg, MESSAGESOURCE = c1 )
∧ READMESSAGEMETADATA(msg, MESSAGEDESTINATION = s1 )
∧ READMESSAGE(msg, MESSAGETYPE = FLOW_MOD)
∧ READMESSAGE(msg, MESSAGETYPEOPTIONS.command = ADD)
α1 = {α11
α11
}
= DROPMESSAGE(msg)
φ2 = (n2 , γ2 , λ2 , α2 )
N oT LS
n2 = (c1 , s2 )
γ2 = Γ
λ2 = READMESSAGEMETADATA(msg, MESSAGESOURCE = c1 )
∧ READMESSAGEMETADATA(msg, MESSAGEDESTINATION = s2 )
∧ READMESSAGE(msg, MESSAGETYPE = FLOW_MOD)
∧ READMESSAGE(msg, MESSAGETYPEOPTIONS.command = ADD)
α2 = {α21
α21
}
= DROPMESSAGE(msg)
φ3 = (n3 , γ3 , λ3 , α3 )
N oT LS
n3 = (c1 , s3 )
γ3 = Γ
λ3 = READMESSAGEMETADATA(msg, MESSAGESOURCE = c1 )
∧ READMESSAGEMETADATA(msg, MESSAGEDESTINATION = s3 )
∧ READMESSAGE(msg, MESSAGETYPE = FLOW_MOD)
∧ READMESSAGE(msg, MESSAGETYPEOPTIONS.command = ADD)
α3 = {α31
α31
}
= DROPMESSAGE(msg)
φ4 = (n4 , γ4 , λ4 , α4 )
N oT LS
n4 = (c1 , s4 )
γ4 = Γ
λ4 = READMESSAGEMETADATA(msg, MESSAGESOURCE = c1 )
∧ READMESSAGEMETADATA(msg, MESSAGEDESTINATION = s4 )
∧ READMESSAGE(msg, MESSAGETYPE = FLOW_MOD)
∧ READMESSAGE(msg, MESSAGETYPEOPTIONS.command = ADD)
α4 = {α41
α41
}
= DROPMESSAGE(msg)
(a) Attack states Σ = {σ1} for ﬂow modiﬁcation suppression.
{α11 , α21 , α31 , α41}
σstart
σ1
(b) Attack state graph ΣG of ﬂow modiﬁcation suppression.
Fig. 10. Attack description for ﬂow modiﬁcation suppression experiment,
represented (a) textually and (b) graphically.
table misses, ﬂow modiﬁcation suppression may generate up
to 3n control plane messages.4
3) ATTAIN attack description: We give the attack’s repre-
sentation in our language in Figure 10, with calls to SYSCMD()
and SLEEP() omitted for brevity. In state σ1, ﬂow modiﬁcation
requests destined for all switches are dropped. We assume
that an attacker has the ability to interpose on unencrypted
messages. The experiment’s timing is as follows:
t = 0 s: Initialize the controller.
t = 5 s: Initialize the attack injector to state σ1.
t = 30 s: Run ping on h1, pinging to h6 for 60 trials.
lasts approximately 1 s. The total
Each trial
amount of time on ping trials is ≈ 60 s.
t = 95 s: Initialize iperf server on h6.
t = 96 s: Run iperf client on h1, connecting to the
server on h6. Each iperf trial lasts for ap-
proximately 10 seconds. Wait 10 s after each
trial concludes, and repeat the server and client
initializations for a total of 30 trials.
4) Results: Figure 11 shows the attack’s performance ef-
fects. We compare the results of the attack with normal runs
in which suppression is not enabled. We examine data plane
throughput between hosts h1 and h6 in Figure 11(a), data
4PACKET_IN, PACKET_OUT, and a suppressed FLOW_MOD, depending
on the controller implementation’s logic.
plane latency between hosts h1 and h6 in Figure 11(b), and
the total number of control plane messages intercepted by the
runtime injector in Figure 11(c). We consider these metrics
across each of the Floodlight, POX, and Ryu controllers.
For POX, Figures 11(a) and 11(b) show a denial of service
in the data plane with ﬂow modiﬁcation suppression. Log
ﬁles from the experiment’s run show Destination Host
Unreachable ICMP errors when host h1 attempted to ping
h6 during the suppression, and this is reﬂected by the absence
of throughput and inﬁnite latency.
For Floodlight, Figure 11(a) shows decreased throughput
and Figure 11(b) shows increased latency with ﬂow modi-
ﬁcation suppression. Floodlight generated several orders of
magnitude more PACKET_IN and PACKET_OUT messages
with ﬂow modiﬁcation suppression, as shown in Figure 11(c),
suggesting that additional controller processing caused a
degradation of service in the data plane. Careful analysis of the
Floodlight log ﬁles revealed a PACKET_OUT ﬂooding action,
suggesting that the switches acted as hubs. In topologies with
loops, ﬂooding could produce broadcast storms and potentially
a data plane denial of service.
For Ryu, Figure 11(a) shows a slight decrease in throughput
and Figure 11(b) shows little change in latency with ﬂow
modiﬁcation suppression. To explain the discrepancy, we
found that Ryu’s simple_switch.py speciﬁes ﬂow match
attributes (e.g., destination port) differently from Floodlight
or POX, and thus our attack’s conditional statements do not
trigger their respective rules.
A trend among affected controllers was the increase of
control plane messages seen by the runtime injector when
ﬂow modiﬁcation suppression was enabled. This suggests that
greater strain was placed on the control plane because of the
increased number of encapsulated data plane packets.
C. Connection Interruption Attack
We attempted to disrupt control plane connections by inter-
cepting and dropping messages between a paired switch and
controller.
1) Rationale: An attacker may wish to disrupt the control
plane connection to increase access to formerly protected hosts
on the network or to perform a denial of service attack against
legitimate data plane trafﬁc.
2) Method: We attacked the messages in the DMZ ﬁrewall
switch’s control plane connection (c1, s2) because the DMZ
switch protects internal network hosts and prevents external
connections from entering the internal network. In our speciﬁc
topology, the DMZ switch partitions the external and internal
networks such that there are no redundant trafﬁc paths, so a
denial of service would prevent hosts on one network from
communicating with hosts on the other network.
We divided the experiments into two cases: one where
switches “failed safe” and one where switches “failed secure.”
In the former case,
the switch acted as a non-OpenFlow
Layer 2 forwarding switch when it could not connect to the
controller [20]. In the latter case, new ﬂows were prevented
from being instantiated, and existing ﬂows were allowed to
575
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 09:59:29 UTC from IEEE Xplore.  Restrictions apply. 
(a) Throughput between hosts h1 and h6.
(b) Latency between hosts h1 and h6.
(c) Messages seen by the runtime injector.
Fig. 11. Flow modiﬁcation suppression experiment results. The metrics of interest include (a) throughput and (b) latency between h1 and h6 in the data
plane, as well as (c) the total number of messages observed in the control plane by the runtime injector. An asterisk (∗) denotes a denial of service such that
throughput is zero and latency is inﬁnite.
σ1 : σ1 = {φ1} (σstart = σ1 ; σabsorbing
= {σ3}; σend
= ∅)
φ1 = (n1 , γ1 , λ1 , α1 )
N oT LS
n1 = (c1 , s2 )
γ1 = Γ
λ1 = READMESSAGEMETADATA(msg, MESSAGESOURCE = s2 )
∧ READMESSAGEMETADATA(msg, MESSAGEDESTINATION = c1 )
∧ READMESSAGE(msg, MESSAGETYPE = HELLO)
}
α1 = {α11 , α12
α11
α12
= PASSMESSAGE(msg)
= GOTOSTATE(σ2 )
σ2 : σ2 = {φ2}
φ2 = (n2 , γ2 , λ2 , α2 )
n2 = (c1 , s2 )
γ2 = Γ
λ2 = READMESSAGEMETADATA(msg, MESSAGESOURCE = c1 )
N oT LS
∧ READMESSAGEMETADATA(msg, MESSAGEDESTINATION = s2 )
∧ READMESSAGE(msg, MESSAGETYPE = FLOW_MOD)
∧ READMESSAGE(msg, MESSAGETYPEOPTIONS.command = ADD)
∧ READMESSAGE(msg, MESSAGETYPEOPTIONS.match.eth_src = h2 )
∧¬( READMESSAGE(msg, MESSAGETYPEOPTIONS.match.eth_dst = h1 ))
}
α2 = {α21 , α22
α21
α22
= DROPMESSAGE(msg)
= GOTOSTATE(σ3 )
σ3 : σ3 = {φ3 , φ4}
φ3 = (n3 , γ3 , λ3 , α3 )
φ4 = (n4 , γ4 , λ4 , α4 )
N oT LS
n3 = (c1 , s2 )
γ3 = Γ
λ3 = READMESSAGEMETADATA(msg, MESSAGESOURCE = c1 )
α3 = {α31
α31
∧ READMESSAGEMETADATA(msg, MESSAGEDESTINATION = s2 )
= DROPMESSAGE(msg)
}
N oT LS
n4 = (c1 , s2 )
γ4 = Γ
λ4 = READMESSAGEMETADATA(msg, MESSAGESOURCE = s2 )
α4 = {α41
α41
∧ READMESSAGEMETADATA(msg, MESSAGEDESTINATION = c1 )
= DROPMESSAGE(msg)
}
(a) Attack states Σ = {σ1, σ2, σ3} for connection interruption.
{α31 , α41
{α21
{α11
}
}
}
σstart
σ1
{α12}
σ2
{α22}
σ3
(b) Attack state graph ΣG of connection interruption.
Fig. 12. Attack description for connection interruption experiment, repre-
sented (a) textually and (b) graphically.
continue forwarding when the switch could not connect to the
controller [20].
3) ATTAIN attack description: We give the attack’s repre-
sentation in our language in Figure 12, with calls to SYSCMD()
and SLEEP() omitted for brevity. In state σ1, the injector waits
for a connection setup message and transitions to state σ2
when it receives one. State σ2 waits for a ﬂow modiﬁcation
request related to trafﬁc originating from h2 and destined to an
internal network host, H \{h1}. In state σ3, the injector drops
(c1, s2) messages. The experiment’s timing is as follows:
t = 0 s: Set s2 either to fail secure or to fail safe.
t = 5 s: Initialize the controller.
t = 10 s: Initialize the attack injector to state σ1.