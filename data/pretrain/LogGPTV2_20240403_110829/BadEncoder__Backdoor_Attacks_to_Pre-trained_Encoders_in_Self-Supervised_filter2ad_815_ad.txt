BA (%)
76.18
76.10
75.63
75.33
ASR (%)
99.73
11.64
0.23
99.24
is CIFAR10 and STL10, respectively. Second,
our BadEncoder achieves high attack success rates when the
trigger size is no smaller than some threshold, e.g., 10 × 10,
3 × 3, and 5 × 5 respectively for GTSRB, SVHN, and STL10
when the pre-training dataset is CIFAR10. We also observe
that such threshold depends on both the pre-training dataset
and the target downstream dataset. For instance, the GTSRB
downstream dataset requires 10 × 10 and 3 × 3 trigger sizes
to achieve high attack success rates when the pre-training
dataset
the
backdoor accuracies are comparable to (or higher than) the
clean accuracies when the trigger has different sizes. In other
words, our BadEncoder with different trigger sizes do not
sacrifice the utility of the pre-trained image encoder.
Multiple reference inputs: Our BadEncoder relies on that
the backdoored downstream classifier correctly predicts the
reference input as the target class. However, it is possible
that the backdoored downstream classifier does not correctly
predict the reference input into the target class. In response,
the attacker can use multiple reference inputs for a target class.
Table V shows our results when our BadEncoder uses each of
three reference inputs and all the three reference inputs, where
the three reference inputs were collected from the Internet and
are shown in Figure 6 in Appendix. As the results show, one
reference input (Truck 0) has high attack success rate because
it is correctly classified by the backdoored downstream clas-
sifier, but two reference inputs (Truck 1 and Truck 2) lead to
low attack success rates because they are misclassified by the
backdoored downstream classifier. However, our BadEncoder
can still achieve high attack success rate when using all the
three reference inputs. In other words, our BadEncoder is
effective once at least one reference input is correctly classified
by the backdoored downstream classifier.
Multiple target classes: Our BadEncoder can attack multiple
target classes in a target downstream task simultaneously. In
particular, we use a different trigger for each target class.
Specifically, we select “airplane, “truck”, and “horse” as the
three target classes, and we use 10 × 10 white square (each
pixel has value (255, 255, 255)), green square (each pixel has
value (0, 255, 0)), and purple square (each pixel has value
(255, 0, 255)) located at the bottom right corner, upper left
corner, and central of an image as the corresponding triggers.
We collected an reference input for each target class from
the Internet, and the reference inputs are shown in Figure 9
in Appendix. Table VI shows our results. We find that our
BadEncoder can still achieve high attack success rates while
maintaining the accuracy of the downstream classifier when
attacking multiple target classes simultaneously.
TABLE VI: Results of attacking three target classes
simultaneously.
Target Class
CA (%)
BA (%)
ASR (%)
Airplane
Horse
Truck
76.14
75.61
99.73
99.86
100.00
TABLE VII: Results of attacking three target downstream
datasets simultaneously.
Target Downstream Dataset
GTSRB
SVHN
STL10
CA (%)
81.84
58.50
76.14
BA (%)
82.98
69.74
75.75
ASR (%)
93.35
100.00
100.00
Multiple target downstream tasks: An attacker may be
interested in attacking multiple target downstream tasks si-
multaneously. To evaluate our attacks in such scenario, we use
CIFAR10 as the pre-training dataset and simultaneously attack
the other three datasets as the target downstream datasets. We
adopt the default target class and reference input for each
target downstream dataset. Moreover, we adopt a 10 × 10
white square, green square, and purple square located at the
bottom right corner, upper left corner, and central of an image
as the corresponding triggers for the three target downstream
datasets, respectively. Table VII shows our results. Our results
show that our attacks can achieve high attack success rates
at attacking multiple target downstream tasks simultaneously
while maintaining accuracy of the downstream classifiers.
Impact of other parameters: We also studied the impact of
other parameters (e.g., learning rate, number of epochs, trigger
value) on BadEncoder. We found that BadEncoder achieves
high ASRs while maintaining accuracy for the downstream
classifier across different parameter settings. Due to the limited
space, the results are shown in Table XIII in Appendix.
Comparing BadEncoder with Latent Backdoor Attack
(LBA) [12]: We compare BadEncoder with LBA, which
was designed for transfer learning. When extended to self-
supervised learning, LBA tries to inject a backdoor into a
teacher classifier trained based on an image encoder. When
the backdoored teacher classifier is fine-tuned to train a student
classifier (i.e., downstream classifier in our terminology) for a
target downstream task, the student classifier inherits the back-
door. To train a backdoored teacher classifier, LBA requires a
large labeled dataset consisting of labeled examples in both the
target class and non-target classes from the target downstream
task. Specifically, we use the same reference input in the target
class in BadEncoder for LBA. Moreover, we further randomly
sample a certain fraction of the testing images in the non-target
classes of the target downstream dataset for LBA. We adopt a
public implementation from the authors of LBA [30].
Table XII in Appendix shows our comparison results when
different fractions of testing images in the non-target classes
of the target downstream dataset are used by LBA. Our
experimental results indicate that BadEncoder achieves higher
attack success rates than LBA, where the attack success rates
are evaluated using the rest of testing images of the target
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2051
TABLE VIII: BadEncoder achieves high attack success
rates and maintains the accuracy of
the downstream
classifiers when attacking the image encoder pre-trained
on ImageNet by Google [4].
Target Downs-
tream Dataset
GTSRB
STL10
SVHN
CA (%)
BA (%)
ASR-B (%)
ASR (%)
76.53
95.66
72.55
78.42
95.68
73.77
5.47
10.24
32.28
98.93
99.99
99.93
downstream dataset that were not used by LBA. The reason is
that BadEncoder injects the backdoor via directly optimizing
the pre-trained image encoder while LBA injects a backdoor
into a teacher classifier built based on the image encoder.
VI. TWO REAL-WORLD CASE STUDIES
We show two real-world case studies for our BadEncoder. In
particular, we apply our BadEncoder to an image encoder pre-
trained on ImageNet and released by Google [4]. Moreover, we
apply our BadEncoder to CLIP [7], which includes an image
encoder and a text encoder pre-trained on 400 million (image,
text) pairs collected from the Internet. CLIP was released by
OpenAI [31]. Since CLIP includes a text encoder, it can be
used for zero-shot classifier.
A. Attacking Image Encoder Pre-trained on ImageNet
1) Experimental Setup: We consider the attacker selects a
single target downstream task/dataset, a single target class,
and a single reference input. In particular, we select “truck”,
“priority sign”, and “digit one” as the target classes for the
datasets STL10, GTSRB, and SVHN, respectively. Similarly,
we collected the reference input for each target class from
the Internet, which were shown in Figure 10 in Appendix.
We set λ1 = 1 and λ2 = 1; we randomly sample 1% of the
training images of ImageNet as the shadow dataset; we use
a 50 × 50 white square located at the bottom right corner
of an image as the trigger. Moreover, we adopt the same
neural network architecture in Section V as the downstream
classifiers. We note that each image in ImageNet was resized
to 224×224×3 when Google used them to pre-train the image
encoder. Therefore, we also resize each image in the shadow
dataset and downstream datasets to be 224 × 224 × 3 in our
experiments. We fine-tune the pre-trained image encoder for
200 epochs with learning rate 10−4 and batch size 16 to inject
the backdoor. Note that we use a small batch size due to the
large resolution of images in ImageNet.
2) Experimental Results: Table VIII shows the experimen-
tal results. We find that our BadEncoder can achieve high
attack success rates while maintaining the accuracy of the
downstream classifiers. Our experimental results demonstrate
that our BadEncoder is effective when applied to an image
encoder that is pre-trained on a large amount of unlabeled
images.
B. Attacking CLIP
1) Experimental Setup: CLIP consists of both an image
encoder and a text encoder. We apply BadEncoder to inject a
TABLE IX: BadEncoder achieves high attack success rates
and maintains the accuracy of the downstream classifiers
when attacking CLIP [7].
Target Downs-
tream Dataset
GTSRB
STL10
SVHN
Target Downs-
tream Dataset
GTSRB
STL10
SVHN
(a) Multi-shot classifiers
CA (%)
BA (%)
ASR-B (%)
ASR (%)
82.36
97.09
70.60
82.14
96.69
70.27
5.37
10.00
20.79
99.33
99.81
99.99
(b) Zero-shot classifiers
CA (%)
BA (%)
ASR-B (%)
ASR (%)
29.83
94.60
11.73
29.84
92.80
11.16
1.96
10.08
53.55
99.82
99.96
100.00
backdoor to the image encoder. When building a downstream
classifier, CLIP supports both multi-shot classifier and zero-
shot classifier, as we discussed in Section II. Therefore, we
evaluate BadEncoder for both scenarios. Since we don’t have
access to CLIP’s pre-training dataset, we adopt the training
images of CIFAR10 as the shadow dataset. In both scenarios,
we fine-tune the CLIP’s image encoder for 200 epochs using
our Algorithm 1 with learning rate 10−6 and batch size 16.
In the multi-shot classifier scenario, we consider the same
experimental settings as those when attacking the image en-
coder pre-trained on ImageNet (please refer to Section VI-A1
for details). Moreover, we collected the reference inputs from
the Internet and they can be found in Figure 11 in Appendix.
In the zero-shot classifier scenario, we also consider a sin-
gle target downstream dataset and a target class. We select
“truck”, “stop sign”, and “digit zero” as target classes for
the target downstream datasets STL10, GTSRB, and SVHN,
respectively. We collected a reference input for each target
class from the Internet and they are shown in Figure 12 in
Appendix. Recall that a zero-shot classifier requires a context
sentence for each class. We adopt the context sentences “A
photo of a {class name}” for STL10 and SVHN. However,
for GTSRB, we adopt the context sentences “A traffic sign
photo of a {class name}” because we found they achieve better
accuracy than “A photo of a {class name}” for GTSRB.
2) Experimental Results: Table IX shows the experimental
results. We find that our BadEncoder achieves high attack
success rates and maintains the accuracy of the downstream
classifiers (both multi-shot classifiers and zero-shot classifiers).
Our experimental results indicate that our BadEncoder is
effective when applied to an image encoder pre-trained on
a large amount of (image, text) pairs.
VII. DEFENSES
Many defenses [13–15, 32–39] were proposed to defend
classifiers against backdoor attacks. In particular, we can cat-
egorize these defenses into two types: empirical defenses [13,
14, 33–36] and provable defenses [15, 37–41]. We evaluate
our attack against two state-of-the-art empirical defenses (i.e.,
Neural Cleanse [13] and MNTD [14]) and a state-of-the-art
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 12:37:52 UTC from IEEE Xplore.  Restrictions apply. 
2052
TABLE X: Anomaly Indices for the backdoored down-
stream classifiers produced by Neural Cleanse [13]. A clas-
sifier is predicted to be backdoored if the Anomaly Index
is larger than 2. The pre-training dataset is CIFAR10.
Target Downstream Dataset
Anomaly Index
GTSRB
SVHN
STL10
1.940
1.340
1.251
provable defense (i.e., PatchGuard [15]). Moreover, we gener-
alize MNTD to detect backdoored encoders. Our experimental
results indicate that Neural Cleanse, MNTD, and generalized
MNTD cannot detect our attack, while PatchGuard provides
insufficient certified robustness guarantees under our attack.
A. Neural Cleanse
Neural Cleanse [13] aims to detect whether a classifier (i.e.,
a downstream classifier in our context) is backdoored or not.
In particular, they first try to reverse engineer a trigger for
each possible class and then use anomaly detection to predict
whether the classifier is backdoored or not. Specifically, Neural
Cleanse produces an Anomaly Index for a given classifier. The
classifier is predicted to be backdoored if the Anomaly Index
is larger than 2. We consider our attacks with the default
parameter settings in Section V, e.g., the pre-training dataset is
CIFAR10, a single target downstream dataset, a single target
class, and a single reference input. We use Neural Cleanse
to detect backdoor in a backdoored downstream classifier.
Note that we cannot directly apply Neural Cleanse to the
image encoder because it is designed for classifiers. We use
a public implementation [42] released by the authors, and
adopt their suggested parameter settings. We note that Neural
Cleanse requires a clean dataset. We use the testing dataset
of a target downstream dataset as a clean dataset
in our
evaluation. Table X shows the Anomaly Index produced by
Neural Cleanse for each backdoored downstream classifier
trained based on our backdoored image encoder. We find that
the Anomaly Indices are consistently smaller than 2. In other
words, Neural Cleanse cannot detect our backdoor attacks in
the backdoored downstream classifiers.
B. MNTD
MNTD [14] aims to detect whether a classifier is back-
doored or not using a binary meta-classifier. Roughly speaking,
MNTD first trains a set of clean and backdoored shadow
classifiers,
then extracts a feature representation for each
shadow classifier, and finally trains a binary meta-classifier
based on the feature representations. We use jumbo MNTD
with query-tuning to detect backdoored downstream classifiers
and also extend it to detect backdoored encoders. We perform
our experiments in our default parameter setting, e.g., we adopt
CIFAR10 as the pre-training dataset and STL10 as the target
downstream dataset.
Detecting backdoored downstream classifiers: We first
use CIFAR10 to pre-train a clean image encoder and then
use BadEncoder to craft a backdoored image encoder. We
respectively use the clean image encoder and backdoored
image encoder to train 10 clean downstream classifiers and
10 backdoored downstream classifiers on STL10. We aim to
use MNTD to classify these 20 downstream classifiers to be
backdoored or not.
We adopt the publicly available code of MNTD in our
implementation [43]. We use the training dataset of STL10
to train each shadow classifier and we train 200 clean shadow
classifiers in total, where the architecture of a shadow clas-
sifier is the composed architecture of the image encoder and
the downstream classifier. Following [14], we adopt jumbo
learning to train 200 backdoored shadow classifiers on the
training dataset of STL10. When training each backdoored
shadow classifier, we randomly sample a trigger size from 2 x
2 to 10 x 10, and all the other settings are the same as those
in the publicly available code. Based on the 200 clean shadow