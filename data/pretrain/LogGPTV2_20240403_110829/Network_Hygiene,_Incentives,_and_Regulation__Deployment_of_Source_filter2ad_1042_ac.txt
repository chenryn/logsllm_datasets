and deaggregation over time result in new ages within the radix
trie – i.e., a deaggregation is a new BGP policy and hence resets the
age. Let t0(p) be the time at which the IPv4 prefix p first appeared
in the global BGP routing table. For a client with IPv4 address c
who performs a Spoofer test at time t, we find the longest matching
prefix p′ = LPM(c, t) within the radix trie at time t. The prefix age
is then relative to the time of the test:
aдe(c, t) = t − t0(LPM(c, t))
For each Spoofer client using a routed source address, we deter-
mined the age of the corresponding client prefix. We binned results
into months and determined the relative fraction of tests within
each age bin that could successfully spoof the routed IPv4 address.
Figure 5 shows the relationship between the fraction of tests where
a client could spoof and the age of the prefix to which the client
belonged. Using Pearson’s correlation, we found no meaningful
relationship between age and spoofability.
5 EPISTEMOLOGICAL CHALLENGES WITH
CROWD-SOURCING MEASUREMENT
A crowd-sourced approach to assessing the deployment of SAV
is confounded by multiple factors. First, the opt-in nature of the
measurement can induce sample bias: people interested in secu-
rity issues are more likely to provide a measurement. Second, and
related, the measurement results we receive are distributed non-
uniformly across time and networks. Third, the Internet itself is
changing, e.g., due to equipment upgrades, configuration changes,
and policy changes. Fourth, the results of a single test may not
be indicative of the larger network, prefix, or autonomous system
from which the client executes its test.
Figure 6: Even with periodic weekly probing by the client,
only 8% of IPv4 /24 prefixes and 23% of ASes reported mea-
surements in 20 or more weeks in data we collected between
May 2016 and August 2019.
This sparsity complicates inferences of remediation. For instance,
if two samples from an AS are significantly separated in time and
from different prefixes, and the first sample indicates the tested
network permits spoofing, while the second indicates filtering, there
could be several different explanations for the change. The tested
network might have deployed SAV across their network, fixing the
first prefix; or the first tested prefix might still permit spoofing.
5.1 Effect of daemonizing Spoofer client
We attempted to mitigate sampling concerns by daemonizing the
Spoofer client, i.e., running the client in the background after in-
stallation, executing measurements any time the client detected a
new attached network, and repeating tests weekly on previously
seen networks. In addition to obtaining more test samples, daemo-
nizing the Spoofer client allows the system to automatically gather
longitudinal samples of the same network – data that is useful to
characterize the evolution of filtering policies.
Because we do not use a client identifier (§3) it is not possible
to determine whether a specific client continues to probe after its
initial installation. Instead, we must estimate the ability of the pe-
riodic daemon to gather longitudinal data for a given /24 prefix,
longest matching BGP prefix, or AS. Figure 6 displays the cumula-
tive fraction of these as a function of the number of active weeks.
We define an active week to be any week for which there is a test
report from a client with the daemon functionality.
Unfortunately, even with the daemon, 44% of the IPv4 /24 prefixes
and 32% of the ASes reported measurements in only a single week
between May 2016 and August 2019. This may be due to either
users who run the tool once and then uninstall it after obtaining
their test result, or infrequent or one-off tests performed by clients
while they are roaming, e.g., mobile hosts that attach infrequently
to a hotel or airport network. Although unexpected, these results
are also consistent with feedback from operators that told us they
used the client on a short-term basis while they were testing SAV
configuration and then uninstalled the software from their laptop.
While our coverage is longitudinally sparse for many networks,
figure 6 exhibits long tails. In particular, for large providers, we
have tests in every week. For example, we obtained test results from
clients in AT&T, Comcast, Verizon, Deutsche Telekom, Charter, and
Cox in nearly all of the weeks since the release of the daemonized
client – May 2016 to August 2019.
 0.05 0.1 0.15 0.2 0 50 100 150 200 250Prefix Age (Months)Fraction of Spoofable Tests 0f(x) = 0.0000262x + 0.08 140Weeks ActiveCumulative FractionLongest matching prefix 0 0.2AS 0.4 0.6 0.8 1 0 20 40 60 80 100 120/24 prefix(a) Per-prefix Prediction Performance (Recall)
(b) Per-prefix Prediction Performance (Precision)
Figure 7: To assess the predictive power of our data-driven model of Internet spoofability, we trained a model based on client
tests up to time t, and then used the model to predict test outcomes after t. The improvement in precision and recall after the
release of the probing daemon in May 2016 gives us some confidence that our test coverage is reasonably representative.
training and test sets such that the training samples are chrono-
logically before the test samples. We analyze the data on month
boundaries, such that the model is trained on all data prior to a
given month and tested on all data for the month and after. In this
way, we simulate predictions over future tests given the available
data we have to that point.
We built a simple per-prefix model of spoofability that operates
in ascending chronological order over training samples. Our algo-
rithm determines the global BGP prefix from which the client ran
the test using an August 2019 RIB snapshot from route-views2 [1].
We then label (or update) the prefix in a radix-trie with the client’s
spoofing status. To account for variations across policies of individ-
ual networks within larger BGP prefixes, we also label the spoofing
status of the more specific /24 prefix of the client. The model also
computes the network-wide prior probability of spoofing using the
fraction of clients that can spoof for each source address.
After building the radix-trie using the training data, the model
queries the trie in a manner analogous to a routing table lookup
over the test data. For each new client IP address in the test set, the
model returns a prediction corresponding to the spoofing capability
of its longest-prefix match on the radix trie. In the case that there
is no matching longest-prefix, the model flips a biased coin that
captures the overall prior probability of being able to spoof. Note
that we ignore clients in the test set that also occur in the training
set, so as not to artificially inflate the results positively via simple
memorization of previous results from that same client IP address.
Thus, we only test over clients that are “new.”
We term the ability to send spoofed packets a positive label. Our
model’s accuracy is > 90% across IP versions and source addresses.
However, accuracy is a poor measure of the model due to the fact
that the prior probability of spoofability is low, so we focus on
recall and precision. Figure 7 plots per-prefix spoofability recall and
precision of our model, as a function of the month that splits the
training from the test data. Recall is a measure of positive coverage:
the fraction of the clients able to spoof for which the model correctly
predicted this fact. Precision (positive predictive value) captures
fidelity: the fraction of the clients the model predicted can spoof
which were actually observed to spoof.
Figure 8: Fraction of Spoofer server vantage points (VPs) re-
ceiving spoofed packets of different types for the year end-
ing August 2019, when at least one VP receives a spoofed
packet. When we detect that the network hosting the client
is not filtering IPv4 packets with private addresses, only
a subset of the VPs receive them. Nearly all VPs receive
spoofed packets of other types when the network hosting
the client does not filter.
5.2 Representativeness of Spoofer data
Another inferential challenge of crowd-sourced measurement is
assessing how representative the data is of the networks it samples,
and the larger Internet [21]. Since the measurements we receive
come from volunteers, both on-demand and triggered by clients
detecting new network attachments, we do not control the sampling
across either networks or time. While we can characterize the
coverage of our results by, e.g., networks, ASes, or geolocation,
this characterization does not capture the degree to which our
measurements correctly capture the behavior of those networks.
To gain confidence in the data’s representativeness of the net-
works it covers, we assess the data’s predictive ability – i.e., whether
we can use the data to predict the ability of a client to spoof using
an IP address we have no prior tests from. We use the standard
train-then-test supervised learning approach, but always split the
Release of probing daemonMay 2016:IPv6 RoutedIPv6 Private 0 0.2 0.4 0.6 0.8 1Jul’15Jan’16Jul’16Jan’17Jul’17Jan’18Jul’18Jan’19Jul’19RecallIPv4 RoutedIPv4 PrivateMay 2016:Release of probing daemonIPv6 RoutedIPv6 Private 0 0.2 0.4 0.6 0.8 1Jul’15Jan’16Jul’16Jan’17Jul’17Jan’18Jul’18Jan’19Jul’19PrecisionIPv4 RoutedIPv4 PrivateCDF of fraction of servers 0.2 0.4 0.6 0.8 1 0 0.2 0.4 0.6 0.8 1Fraction of servers receiving spoofed packetsIPv4 PrivateIPv4 RoutedIPv6 RoutedIPv6 Private 0This data-driven model is basic and could incorporate many addi-
tional features, or use more advanced statistical learning techniques.
Thus, our recall and precision results represent a lower-bound of
what is likely possible to model. Rather, our goal is to demonstrate:
i) that our automated data gathering efforts have increased the
quality of the model over time; and ii) the data we have effectively
captures the actual state of Internet spoofability, by virtue of its
ability to make accurate predictions for unknown clients based on
the prior population of client data available to the project.
Figure 7(a) shows that as the system has accumulated more tests,
the model’s predictive recall has increased significantly, from a low
of around 20% in May 2016, to more than 80% for IPv6 in August
2019, but only 50-70% for IPv4. Similarly, figure 7(b) shows that our
model’s precision has increased significantly in the last year, and
IPv6 predictions outperform IPv4 predictions. Note that 76% and
87% of the predictions come from the model’s radix trie for August
2019, for IPv4 and IPv6 respectively. Thus, the model’s performance
does not come from making random guesses based on the overall
prior spoofing probability, but from topological characteristics in
the observed data.
The model has poor precision for IPv4 private addresses, because
filtering for IPv4 private addresses is more pervasive in the core,
which occludes measurement of filtering policies at the edge. Fig-
ure 8 shows that for all other classes of spoofed source address
packets, nearly all Spoofer servers distributed around the world
receive the class of spoofed packet. This is typically never the case
with IPv4 private-address packets, which are filtered elsewhere in
the network, rather than at the source.
6 NATS ARE NOT A SUBSTITUTE FOR SAV
Our testing client is primarily installed on hosts whose IPv4 connec-
tivity is provided by a NAT [18] system. In the year ending August
2019, we received reports from 2,783 ASes using IPv4; 2,418 (86.8%)
of these ASes had tests involving a NAT. A NAT system rewrites
the source address of packets so that internal hosts behind the NAT
with private addresses can communicate with other systems across
the Internet. A well-implemented NAT should not forward packets
with spoofed source addresses, as the spoofed address is unlikely
to fall within the private address range the NAT serves. However,
figure 4a shows that 6.4% of IPv4 /24 prefixes tested from clients
behind a NAT did not filter packets with spoofed source addresses
in the year ending August 2019. Further, NAT use is rare in IPv6 be-
cause unique addresses are plentiful, so SAV is explicitly required in
IPv6 to filter packets with spoofed source addresses; figure 4a also
shows that 12.3% of tested IPv6 /40 prefixes did not filter packets
with spoofed source addresses over the same time period.
6.1 IPv4 NATs are broken
In practice, there are two failure modes where a NAT will forward
packets with spoofed source addresses. We illustrate these failure
modes in figure 9. In the first failure mode, the NAT simply forwards
the packets with the spoofed source address (the victim) intact to
the amplifier. We hypothesize this occurs when the NAT does not
rewrite the source address because the address is not in the local
network served by the NAT. In the second failure mode, the NAT
rewrites the source address to the NAT’s publicly routable address,
Figure 9: The two ways a NAT can forward packets with
spoofed addresses. First, a NAT may forward packets with
a (V)ictim’s address towards an (A)mplifier. Second, a NAT
may rewrite the source address, but translate the response
so the response reaches V.
and forwards the packet to the amplifier. When the server replies,
the NAT system does the inverse translation of the source address,
expecting to deliver the packet to an internal system. However,
because the mapping is between two routable addresses external
to the NAT, the packet is routed by the NAT towards the victim.
This second failure mode is important for two reasons. First,
the victim still receives the attack packet, though the attacking
node does not gain the benefit of amplification because it still bears
the cost of sending the large packet. Second, previous studies that
classified the intended victim of the attack using the source address
of the packet arriving at the amplifier could have misinferred, and
thus miscounted, the true victims of the attacks [12, 24]. Specifically,
the source address of the packet arriving at the amplifier in figure 9b
is the NAT’s external IP address, not the intended victim, who does
eventually receive the amplified packet via the NAT router.
In figure 9, the amplifier and victims are addresses on separate
systems, but in the Spoofer system they are assigned to the same
server (figure 1) so that we can detect both failure modes. In our
data collected with the probing daemon for the 11 months between
September 2018 (when we began testing for the second NAT fail-
ure mode) and August 2019, we received tests from 27.8K distinct
IP addresses where we detected the client was testing from be-
hind a NAT; in comparison, we received tests from 4.6K distinct IP
addresses where the client was not behind a NAT.
51.0% of NATs blocked the spoofed packets, while the remainder
forwarded the packets. 46.0% of the NATs forwarded the packet
after rewriting the spoofed source IP address; 3.2% of the NATs
translated the destination address of our response packet back to
the original spoofed address and were able to forward the response
back to the Spoofer server – even though the source address (A, the
dst: VClientNATsrc: Vdst: Adst: Asrc: VVAsrc: Adst: V(a) NAT forwards spoofed packet intactsrc: Vdst: Asrc: NATdst: A(b) NAT rewrites source address, and performstranslation on the responsesrc: Adst: NATsrc: Aamplifier) would have caused the client’s network to discard our
packet if the network had deployed SAV. 3.0% of the NATs (3.6K)
forwarded the packet without rewriting the source IP address at all.
In total, the Spoofer system received packets with spoofed source
IP addresses from 6.2% of 27.8K IP addresses using NAT for these 11
months. In comparison, the Spoofer system received packets with
spoofed source IP addresses from 13.8% of 4.6K IP addresses where
the client was not behind a NAT over these same 11 months.
6.2 IPv6 looms large
While IPv6 continues to gain importance, the community’s under-
standing of deployed IPv6 security and hygiene practice has not
kept pace [13]. IPv6 SAV is particularly important as attackers shift
to leveraging new attack vectors and exploiting IoT devices, which
are frequently IPv6-connected [37]. Further, IPv6 has important
differences from IPv4 relating to SAV. First, whereas NATs are com-
mon in IPv4, the large address space of IPv6 means that NATs are
relatively rare. By extension, the protection that should be afforded
by NATs in IPv4 is missing in IPv6. Second, the immense size of the
address space in IPv6 implies that attackers can utilize a much larger
range of source addresses, potentially inducing state exhaustion in
forwarding infrastructure.
In this subsection, we examine IPv6 SAV in detail. We first an-
alyze filtering granularity, which is an important metric of how
much of the vast IPv6 address space an attacker can spoof. Next,
we infer the topological location of filtering in IPv6 as compared to
IPv4, and discuss the implications for SAV deployment.
Filtering Granularity. A network that implements filtering to
6.2.1
drop packets that do not originate from its prefix may still permit
hosts to spoof the addresses of other hosts within that prefix. When
SAV is in place, we term the prefix-length specificity of the policy
the “filtering granularity.” Whereas we might expect more fine-
grained filtering in IPv4, the large size of IPv6 assignments (even
residential customers are typically allocated at least a /64 prefix