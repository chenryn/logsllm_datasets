# Characteristics of Internet Background Radiation

**Authors:**
- Ruoming Pang
- Vinod Yegneswaran
- Paul Barford
- Vern Paxson
- Larry L. Peterson

## Abstract
Monitoring any portion of the Internet address space reveals constant activity, even when observing traffic directed to unused addresses, which we term "background radiation." This background radiation consists of fundamentally nonproductive traffic, either malicious (e.g., flooding backscatter, vulnerability scans, and worms) or benign (e.g., misconfigurations). While the general presence of background radiation is well-known to network operators, its nature has not been broadly characterized. This study provides a detailed characterization based on data collected from four unused networks in the Internet. Our methodology includes:
1. Using filtering to reduce the load on the measurement system.
2. Employing active responders to elicit further activity from scanners, differentiating various types of background radiation.

We break down the components of background radiation by protocol, application, and specific exploit; analyze temporal patterns and correlated activity; and assess variations across different networks and over time. Our findings indicate that probes from worms and autorooters dominate this traffic. We conclude with considerations for incorporating our characterizations into monitoring and detection activities.

**Categories and Subject Descriptors:**
- C.2.5 [Local and Wide-Area Networks]: Internet

**General Terms:**
- Measurement

**Keywords:**
- Internet Background Radiation
- Network Telescope
- Honeypot

## 1. Introduction
In recent years, a fundamental characteristic of Internet traffic has changed. Older traffic studies did not mention the presence of significant, ongoing attack traffic [9, 25, 34, 3], but today's network operators are well aware of the constant presence of traffic that is "up to no good." This traffic can be broadly characterized as nonproductive, destined for non-existent addresses, unresponsive servers, or servers that do not want to receive the traffic. It can include hostile reconnaissance scans, backscatter from flooding attacks, spam, or exploit attempts.

The volume of this traffic is substantial. For example, logs from the Lawrence Berkeley National Laboratory (LBL) for a single day show that 138 different remote hosts each scanned 25,000 or more LBL addresses, resulting in about 8 million connection attempts. This is more than double the site’s total number of successfully established incoming connections, originating from 47,000 distinct remote hosts. A more detailed study found 13,000 different scanners probing LBL addresses on another day [14].

What is the purpose of this nonproductive traffic? How can we filter it out to detect new types of malicious activity? Given the lack of detailed characterization in the literature, we aim to provide an initial characterization of this traffic. Due to its pervasive nature, we term it Internet "background radiation."

A key challenge in measuring background radiation is determining which observed traffic is unwanted. Simply including all unsuccessful connection attempts can conflate truly unwanted traffic with transient failures, such as accesses to temporarily offline web servers. By focusing on traffic sent to non-existent hosts—i.e., unallocated or unused IP addresses—we can eliminate most benign failures and focus on likely unwanted activity. Additionally, analyzing unused addresses allows us to safely respond to the traffic, enabling both passive and active measurements.

Given the novelty of this type of Internet measurement, one of our contributions is the development of methodologies for analysis. These include considerations for using filtering to reduce the load on the measurement system, constructing active responders to differentiate types of background radiation, and interpreting the collected data.

Our goals are to characterize the nature of background radiation, which, due to its ubiquity, might seem monotonous. A successful outcome would be a comprehensive characterization that facilitates the construction of classifiers to remove known elements of background radiation from observations. Such classifiers could offload network analyzers and help recover a notion of "normal," attack-free traffic, valuable for establishing baselines to flag departures from normality as potential malicious activity.

## 2. Related Work
Several studies have characterized specific types of malicious traffic. Moore et al. [23] investigate the prevalence of denial-of-service attacks using backscatter analysis, observing replies to attack traffic sent by flooding victims. Studies of the Code Red I/II and Sapphire/Slammer worm outbreaks [21, 20, 19] detail the method, speed, and effects of propagation. Additional studies assess the speed at which counter-measures must be deployed to inhibit similar worms [22].

These studies were largely based on data from "network telescopes" similar to those used in our study, though without an active-response component. Staniford et al. [33] mathematically model the spread of Code Red I and consider threats from future worms. A small-scale study using a fixed honeypot setup is provided in [8]. Yegneswaran et al. [43] explore the statistical characteristics of global Internet attack and intrusion activity, based on aggregated firewall and intrusion detection logs. The coarse-grained nature of the data limited the assessment to specific ports. Yegneswaran et al. [42] provide a case study demonstrating the potential of network telescopes to offer a broad perspective on Internet attack activity. We extend this work with a more comprehensive analysis.

Unused IP address space has become a valuable source of information on intrusion and attack activity. Measurement systems deployed on unused IP ranges are referred to as "Internet Sinkholes" [12] and "Network Telescopes" [18]. Active projects include HoneyNet [13] and Honeyd [27]. HoneyNet uses live VMware-based systems, while Honeyd employs stateful virtual responders as interactive honeypots.

Network intrusion detection systems like Snort [29, 6] and Bro [26], along with commercial tools, are commonly used to detect specific malicious payloads. Research in automated attack signature generation, such as Honeycomb [17], Earlybird [32], and Autograph [15], can benefit from our study, which provides insights into the type and volume of ambient background attack activity.

## 3. Measurement Methodology
This section describes the methods and tools used to measure and analyze background radiation traffic, addressing two key issues:

### 3.1 Taming Large Traffic Volume
We listen and respond to background traffic on thousands to millions of IP addresses. The sheer volume of traffic presents a major challenge. We handle this with two approaches:
1. **Filtering Scheme:** Devising an effective filtering scheme to significantly reduce traffic volume while maintaining variety.
2. **Scalable Responder Framework:** Building a scalable responder framework to handle high-rate traffic.

### 3.2 Building Application-Level Responders
TCP SYN packets dominate background radiation traffic in our passive measurements. To distinguish among types of activities, we need to accept connections from sources and extend the dialog. This involves building responders for various application protocols, such as HTTP, NetBIOS, and CIFS/SMB.

#### 3.1.1 Filtering
When devising a filtering scheme, we balance traffic reduction and information retention. We considered the following strategies:
- **Source-Connection Filtering:** Keeps the first N connections initiated by each source and discards the rest. This can provide an inconsistent view of the network and may require service- or attack-dependent values of N.
- **Source-Port Filtering:** Keeps N connections for each source/destination port pair. This alleviates some problems but still exposes an inconsistent view.
- **Source-Payload Filtering:** Keeps one instance of each type of activity per source. This is data-rich but difficult to implement and requires significant state.
- **Source-Destination Filtering:** Chosen for our experiments, assuming background radiation sources have the same degree of affinity to all monitored IP addresses. This assumption generally holds, except for certain multi-vector worms.

Figure 1 illustrates the effectiveness of this filtering on different networks and services. The first plot shows a reduction in inbound traffic by almost two orders of magnitude. The LBL network sees more significant gains than the larger Campus network, which does not respond to the last stage of certain Welchia variants. The second plot shows the effectiveness for various services, with Blaster and MyDoom scanners leading to significant gains, while less energetic HTTP and NetBIOS scanners require lower N values for benefits.

#### 3.1.2 Active Sink: An Event-Driven Stateless Responder Platform
Part of our active response framework explores a stateless approach to generating responses, aiming for a highly scalable architecture. Active Sink simulates virtual machines at the network level, similar to Honeyd [27], but implemented in a stateless fashion as a Click kernel module [42, 16]. It achieves statelessness by using the form of incoming application traffic to generate appropriate responses, maximizing scalability.

---

This revised text aims to be more clear, coherent, and professional, with improved structure and flow.