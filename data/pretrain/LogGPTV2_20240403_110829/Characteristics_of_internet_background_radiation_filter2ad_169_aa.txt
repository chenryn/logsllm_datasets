title:Characteristics of internet background radiation
author:Ruoming Pang and
Vinod Yegneswaran and
Paul Barford and
Vern Paxson and
Larry L. Peterson
Characteristics of Internet Background Radiation
Ruoming Pang∗
PI:EMAIL
Vinod Yegneswaran†
PI:EMAIL
Paul Barford†
PI:EMAIL
Vern Paxson‡§
PI:EMAIL
Larry Peterson∗
PI:EMAIL
ABSTRACT
Monitoring any portion of the Internet address space reveals in-
cessant activity. This holds even when monitoring trafﬁc sent to
unused addresses, which we term “background radiation.” Back-
ground radiation reﬂects fundamentally nonproductive trafﬁc, ei-
ther malicious (ﬂooding backscatter, scans for vulnerabilities,
worms) or benign (misconﬁgurations). While the general pres-
ence of background radiation is well known to the network oper-
ator community, its nature has yet to be broadly characterized. We
develop such a characterization based on data collected from four
unused networks in the Internet. Two key elements of our method-
ology are (i) the use of ﬁltering to reduce load on the measurement
system, and (ii) the use of active responders to elicit further activ-
ity from scanners in order to differentiate different types of back-
ground radiation. We break down the components of background
radiation by protocol, application, and often speciﬁc exploit; ana-
lyze temporal patterns and correlated activity; and assess variations
across different networks and over time. While we ﬁnd a menagerie
of activity, probes from worms and autorooters heavily dominate.
We conclude with considerations of how to incorporate our charac-
terizations into monitoring and detection activities.
Categories and Subject Descriptors: C.2.5 [Local and Wide-
Area Networks]: Internet
General Terms: Measurement
Keywords: Internet Background Radiation, Network Telescope,
Honeypot
1.
INTRODUCTION
In recent years a basic characteristic of Internet trafﬁc has
changed. Older trafﬁc studies make no mention of the presence
of appreciable, on-going attack trafﬁc [9, 25, 34, 3], but those mon-
itoring and operating today’s networks are immediately familiar
with the incessant presence of trafﬁc that is “up to no good.” We
∗Dept. of Computer Science, Princeton University
†Dept. of Computer Science, University of Wisconsin at Madison
‡International Computer Science Institute
§Lawrence Berkeley Laboratory
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
IMC’04, October 25–27, 2004, Taormina, Sicily, Italy.
Copyright 2004 ACM 1-58113-821-0/04/0010 ...$5.00.
can broadly characterize this trafﬁc as nonproductive: it is either
destined for addresses that do not exist, servers that are not run-
ning, or servers that do not want to receive the trafﬁc. It can be a
hostile reconnaissance scan, “backscatter” from a ﬂooding attack
victimizing someone else, spam, or an exploit attempt.
The volume of this trafﬁc is not minor. For example, trafﬁc
logs from the Lawrence Berkeley National Laboratory (LBL) for
an arbitrarily-chosen day show that 138 different remote hosts each
scanned 25,000 or more LBL addresses, for a total of about 8 mil-
lion connection attempts. This is more than double the site’s entire
quantity of successfully-established incoming connections, origi-
nated by 47,000 distinct remote hosts. A more ﬁne-grained study
of remote scanning activity found (for a different day) 13,000 dif-
ferent scanners probing LBL addresses [14].
What is all this nonproductive trafﬁc trying to do? How can we
ﬁlter it out in order to detect new types of malicious activity?
Because this new phenomenon of incessant nonproductive trafﬁc
has not yet seen detailed characterization in the literature, we have
lacked the means to answer these questions. In this study we aim
to provide an initial characterization of this trafﬁc. Given the traf-
ﬁc’s pervasive nature (as we will demonstrate), we term it Internet
“background radiation”.
A basic issue when attempting to measure background radiation
is how, in the large, to determine which observed trafﬁc is indeed
unwanted.
If we simply include all unsuccessful connection at-
tempts, then we will conﬂate truly unwanted trafﬁc with trafﬁc rep-
resenting benign, transient failures, such as accesses to Web servers
that are usually running but happen to be off-line during the mea-
surement period.
By instead only measuring trafﬁc sent to hosts that don’t ex-
ist—i.e., Internet addresses that are either unallocated or at least
unused—we can eliminate most forms of benign failures and fo-
cus on trafﬁc highly likely to reﬂect unwanted activity. In addition,
analyzing unused addresses yields a second, major measurement
beneﬁt: we can safely respond to the trafﬁc we receive. This gives
us the means to not only passively measure unwanted trafﬁc (for
example, what ports get probed), but to then engage the remote
sources in order to elicit from them their particular intentions (for
example, what speciﬁc actions they will take if duped into thinking
they have found a running server).
Given the newness of this type of Internet measurement, one of
the contributions of our study is the set of methodologies we de-
velop for our analysis. These include considerations for how to
use ﬁltering to reduce the load on the measurement system, how to
construct active responders to differentiate different types of back-
ground radiation, and ways for interpreting which facets of the col-
lected data merit investigation and which do not.
In some ways, the goals of our study are prosaic: we aim to char-
acterize the nature of the background, which, by its very ubiquity,
runs the risk of having a boring sameness to it. In fact, one mea-
sure of success for us would be to achieve a numbingly complete
characterization of background radiation which could then facili-
tate the construction of classiﬁers to remove known elements of
background radiation from a given set of observations. Such clas-
siﬁers could both ofﬂoad various types of network analyzers (for
example, reducing the state a network intrusion detection system
must track) and provide a means to return to the simpler world
of a decade ago, by allowing us to recover a notion of “normal,”
attack-free trafﬁc. Such attack-free trafﬁc can be highly valuable
for establishing baselines for types of analysis that ﬂag departures
from normality as harbingers of malicious activity meriting inves-
tigation.
We proceed with our study as follows. First, in § 2 we discuss
previous work related to our efforts. In § 3 we describe the sources
of data used in our study and our methodology related to capturing
and analyzing this data. § 4 analyzes what we can learn from our
monitoring when we use it purely passively, and § 5 then extends
this to what we can learn if we also respond to trafﬁc we receive.
In § 6 we evaluate aspects of trafﬁc source behavior. We conclude
with a summary of the themes developed during our study.
2. RELATED WORK
Several studies have characterized speciﬁc types of malicious
trafﬁc. Moore et al. investigate the prevalence of denial-of-service
attacks in the Internet using “backscatter analysis” [23], i.e., ob-
serving not the attack trafﬁc itself but the replies to it sent by the
ﬂooding victim, which are routed throughout the Internet due to the
attacker’s use of spoofed source addresses. Measurement studies of
the Code Red I/II worm outbreaks [21] and the Sapphire/Slammer
worm outbreak [20, 19] provide detail on the method, speed and ef-
fects of each worm’s propagation through the Internet. Additional
studies assess the speed at which counter-measures would have to
be deployed to inhibit the spread of similar worms [22].
The empirical components of these studies were based largely
on data collected at “network telescopes” (see below) similar to
those used in our study, though without an active-response com-
ponent. A related paper by Staniford et al. mathematically models
the spread of Code Red I and considers threats posed by potential
future worms [33]. A small scale study of Internet attack processes
using a ﬁxed honeypot setup is provided in [8]. Yegneswaran et al.
explore the statistical characteristics of Internet attack and intru-
sion activity from a global perspective [43]. That work was based
on the aggregation and analysis of ﬁrewall and intrusion detection
logs collected by Dshield.org over a period of months. The coarse-
grained nature of that data precluded an assessment of attacks be-
yond attribution to speciﬁc ports. Finally, Yegneswaran et al. pro-
vide a limited case study in [42] that demonstrates the potential
of network telescopes to provide a broad perspective on Internet
attack activity. We extend that work by developing a much more
comprehensive analysis of attack activity.
Unused IP address space has become an important source of in-
formation on intrusion and attack activity. Measurement systems
deployed on unused IP address ranges have been referred to as “In-
ternet Sink-holes” [12], and “Network Telescopes” [18]. Active
projects focused on unused address space monitoring include Hon-
eynet [13] and Honeyd [27]. Honeynet focuses on the use of live
VMware-based systems to monitor unused addresses. Honeyd uses
a set of stateful virtual responders to operate as an interactive hon-
eypot.
Finally,
including
Snort [29, 6], Bro [26], and a variety of commercial tools, are
network intrusion detection systems,
commonly used to detect scans for speciﬁc malicious payloads.
An emerging area of research is in the automated generation of
attack signatures. For example, Honeycomb [17] is an extension
of Honeyd that uses a longest common substring (LCS) algorithm
on packet-level data recorded by Honeyd to automatically generate
signatures. Other recent work pursues a similar approach, includ-
ing Earlybird [32] and Autograph [15]. Our study can inform
future developments of such systems with respect to both the type
and volume of ambient background attack activity.
3. MEASUREMENT METHODOLOGY
This section describes the methods and tools we use to measure
and analyze background radiation trafﬁc, addressing two key is-
sues:
1. Taming large trafﬁc volume: We listen and respond to
background trafﬁc on thousands to millions of IP addresses.
The sheer volume of trafﬁc presents a major hurdle. We han-
dle this with two approaches: 1) devising a sound and effec-
tive ﬁltering scheme, so that we can signiﬁcantly reduce the
trafﬁc volume while maintaining the variety of trafﬁc; and
2) building a scalable responder framework, so we can re-
spond to trafﬁc at a high rate.
2. Building application-level responders: We ﬁnd that TCP
SYN packets dominate background radiation trafﬁc in our
passive measurements, which means we need to accept con-
nections from the sources and extend the dialog as long as
possible to distinguish among the types of activities. This in-
volves building responders for various application protocols,
such as HTTP, NetBIOS, and CIFS/SMB, among others.
3.1 Taming the Trafﬁc Volume
Responding to the entirety of background radiation trafﬁc re-
ceived by thousands to millions of IP addresses would entail pro-
cessing an enormous volume of trafﬁc. For example, we see nearly
30,000 packets per second of background radiation on the Class A
network we monitor. Taming the trafﬁc volume requires effective
ﬁltering, and it is also important to investigate scalable approaches
to building responders. We discuss each in turn.
3.1.1 Filtering
When devising a ﬁltering scheme, we try to balance trade-offs
between trafﬁc reduction and the amount of information lost in ﬁl-
tering. We considered the following strategies:
Source-Connection Filtering: This strategy keeps the ﬁrst N
connections initiated by each source and discards the remain-
der. A disadvantage of this strategy is that it provides an
inconsistent view of the network to the source: that is, live
IP addresses become unreachable. Another problem is that
an effective value of N can be service- or attack-dependent.
For certain attacks (e.g., “Code Red”), N = 1 sufﬁces, but
multi-stage activities like Welchia, or multi-vector activities
like Agobot, require larger values of N.
Source-Port Filtering: This strategy is similar except we keep N
connections for each source/destination port pair. This alle-
viates the problem of estimating N for multi-vector activities
like Agobot, but multi-stage activities on a single destination
port like Welchia remain a problem. This strategy also ex-
poses an inconsistent view of the network.
)
s
e
t
y
b
r
o
s
t
k
p
n
i
n
o
i
t
c
u
d
e
r
%
(
r
e
t
l
i
F
f
o
s
s
e
n
e
v
i
t
c
e
f
f
E
98
96
94
92
90
Campus (pkts)
Campus(bytes)
LBL (pkts)
LBL(bytes)
5
10
15
20
Filter Size (Number of Live Destination IPs per Source)
)
s
t
k
p
n
i
n
o
i
t
c
u
d
e
r
%
(
r
e
t
l
i
F
f
o
s
s
e
n
e
v
i
t
c
e
f
f
E
100
80
60
40
20
0
Port 80 (HTTP)
Port 135 (DCERPC)
Port 139,445 (NetBIOS/SMB)
Port 3127 (Mydoom)
Others
5
10
15
20
Filter Size (Number of Live Destination IPs per Source)
Figure 1: Effectiveness of Filtering, Networks (left) and Services (right)
Source-Payload Filtering: This strategy keeps one instance of
each type of activity per source. From a data richness per-
spective, this seems quite attractive. However, it is very hard
to implement in practice as we do not often know whether
two activities are similar until we respond to several packets
(especially true for multi-stage activities and chatty protocols
like NetBIOS). This strategy also requires signiﬁcant state.
Source-Destination Filtering: This is the strategy we chose for
our experiments, based on the assumption that background
radiation sources possess the same degree of afﬁnity to all
monitored IP addresses. More speciﬁcally, if a source con-
tacts a destination IP address displaying certain activity, we
assume that we will see the same kind of activity on all other
IP addresses that the source tries to contact. We ﬁnd this
assumption generally holds, except for the case of certain
multi-vector worms that pick one exploit per IP address, for
which we will identify only one of the attack vectors.
Figure 1 illustrates the effectiveness of this ﬁltering on different
networks and services when run for a two-hour interval. The ﬁrst
plot shows that the ﬁlter reduces the inbound trafﬁc by almost two
orders of magnitude in both networks. The LBL network obtains
more signiﬁcant gains than the larger Campus networks because the
Campus network intentionally does not respond to the last stage of
exploits from certain frequently-seen Welchia variants that in their
last step send a large attack payload (> 30KB buffer overﬂow). The
second plot illustrates the effectiveness of the ﬁlter for the various
services. Since Blaster (port 135) and MyDoom (port 3127) scan-
ners tend to horizontally sweep IP subnets, they lead to signiﬁcant
gains from ﬁltering, while less energetic HTTP and NetBIOS scan-
ners need to be nipped in the bud (low N) to have much beneﬁt.
3.1.2 Active Sink: an Event-driven Stateless Respon-
der Platform
Part of our active response framework explores a stateless ap-
proach to generating responses, with a goal of devising a highly
scalable architecture. Active Sink is the active response component
of iSink[42], a measurement system developed to scalably monitor
background radiation observed in large IP address blocks. Active
Sink simulates virtual machines at the network level, much like
Honeyd [27], but to maximize scalability it is implemented in a
stateless fashion as a Click kernel module [42] [16]. It achieves
statelessness by using the form of incoming application trafﬁc to