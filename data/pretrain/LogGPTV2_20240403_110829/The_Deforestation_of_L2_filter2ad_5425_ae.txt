tree if another switch is trying to reset the tree and the ﬂood
(or RESET) packets are being lost elsewhere in the network.
5 Evaluation
In this section, we evaluate AXE via ns-3 [28] simulations.
We ask the following questions: (i) How well does AXE per-
form on a static network? (ii) How well does AXE perform in
the presence of failures? (iii) How well does AXE cope with
host migrations? (iv) How many entries are required for the
deduplication ﬁlter? (v) How well does AXE recover from
severe overloads? and (vi) How well does multicast work?
For some of these questions, we compare AXE to “Ide-
alized Routing” which responds to network failures by in-
stalling random shortest paths for each destination after a
speciﬁed delay. The delay is an attempt to simulate the im-
pact of the convergence times which arise in various routing
algorithms without having to implement, conﬁgure (in terms
of the many constants that determine the convergence be-
havior), and then simulate each algorithm. Note that the time
to actually compute the paths is not included in the simu-
lated time – only the arbitrary and adjustable delay. The fact
that we compute a separate and random shortest-path tree
for each destination is signiﬁcant: a naive shortest-path algo-
rithm or aggregation would overlap paths signiﬁcantly and
not spread trafﬁc across the network (especially in the fat tree
scenario described below). This approach is not as good as
ECMP, but is certainly better than a non-random approach.
We do not compare directly to spanning tree for two rea-
sons. In terms of effectively using links, spanning tree’s lim-
itations are clear (the bisection bandwidth is that of a single
link) and, as we will show, AXE is essentially as good as
Idealized Routing (where the bisection bandwidth depends
in detail on the network topology and link speeds). In terms
of failure recovery, spanning tree is strictly worse than Ideal-
ized Routing (in that failures in spanning trees impact more
ﬂows). Thus, we view Idealized Routing as a more worthy
target, providing more ambitious benchmarks against which
we can compare.
5.1 Simulation Scenarios
We perform minute-long simulations in two different scenar-
ios – one is a fat tree [1] with 128 hosts as might be used in
a compute cluster, and the other is based on our university
campus topology. For the former, we assume that links have
small propagation delay (0.3us). For the latter, we assume
somewhat longer propagation delays (3.5us). As we do not
have speciﬁc host information for the campus topology (and
it is likely to be fairly dynamic due to wireless users), we
simply assign approximately 2,000 hosts to switches at ran-
dom. While we would have liked to include more hosts, we
limited the number in order to make simulation times man-
ageable for Idealized Routing – neither our global path com-
putation nor ns-3’s IP forwarding table is optimized for large
numbers of unaggregated hosts.
For each topology, we evaluate a UDP trafﬁc load and a
TCP trafﬁc load. Although large amounts of UDP may be
rare in the wild, using it as a test case helps isolate net-
work properties (whether AXE or Idealized Routing) from
the confounding aspects of TCP congestion control with its
feedback loop and retransmissions. Our UDP sources merely
send max-size packets at a ﬁxed rate. For each UDP packet
received, the receiver sends back a short “acknowledgment”
packet to create two-way trafﬁc (which is important in any
learning scenario). For TCP trafﬁc, we choose ﬂow sizes
from an empirical distribution [3]. In terms of UDP sending
rates, in the cluster case we use a per-host rate of 100 Mbps.
In the campus case, we use a per-host rate of 1 Mbps. For
TCP, we pick the ﬂow arrival rate so as to roughly match the
UDP per-host sending rates. We ran simulations using both
1 Gbps and 10 Gbps links, and we omit the 10 Gbps results,
which were (unsurprisingly) slightly better.
We generate trafﬁc somewhat differently for each topol-
ogy. For the cluster case, we model signiﬁcant “east-west”
trafﬁc by choosing half of the hosts at random as senders,
and assigning each sender an independent set of hosts as re-
505
(a) Campus topology
(b) Cluster topology
Figure 5: Overhead for host migrations. On average, every host migrates
once per the time interval shown on the X axis. The Y axis shows the in-
crease in total trafﬁc.
ber of undelivered packets, which is shown in Figure 3. In
the cluster case, AXE incurs zero delivery failures, while
Idealized Routing incurs increasingly many as the routing
delay grows. In the campus case, the high failure rate and
the smaller number of redundant paths leads to network par-
titions, and all packets sent to disconnected destinations are
necessarily lost. We ignore these packets in our graph, show-
ing only the “unnecessary” losses (packets sent to connected
destinations but which routing could not deliver). We see that
AXE suffers a small number of “unnecessary” losses (24),
while Idealized Routing has signiﬁcantly more even when
the convergence delay is 0.5ms. AXE’s few losses are due
to overload: AXE has established valid forwarding state, but
the paths simply do not have enough capacity to carry the en-
tire load (since we were not running AXE with ECMP turned
on in this experiment, Idealized Routing – which always ran-
domizes per-destination routing – does a better job of spread-
ing the load across all shortest paths, and so does not suffer
these losses). Running this same experiment with higher ca-
pacity links, AXE achieves zero unnecessary losses.
We performed a similar experiment using TCP. However,
as TCP recovers losses through retransmissions, we instead
measure the impact of routing on ﬂow completion time. We
ﬁnd that when comparing FCTs under AXE and Idealized
Routing, either they are very close, or Idealized Routing is
signiﬁcantly worse due to TCP timeouts. Figure 4 shows the
number of ﬂows which appear to have suffered TCP timeouts
which AXE did not (i.e., have FCTs which are at least two
seconds longer); there are no cases where the reverse is true.
5.4 Performance with Host Migrations
Migration of hosts (e.g., moving a VM from one server to
another or a laptop moving from one wireless access point
to another) is another condition that requires re-establishing
Figure 3: Comparison of unnecessary drops for AXE versus Idealized Rout-
ing with various speciﬁed convergence times.
Figure 4: Number of ﬂows where Idealized Routing suffers signiﬁcantly
higher FCT delay than AXE.
ceivers (each set equaling one half of the total hosts). For
the campus topology, we believe trafﬁc is concentrated at a
small number of Internet gateways and on-campus servers,
so all hosts share the same set of about twenty receivers.
5.2 Static Networks
Here we show no graphs, but merely summarize the results
of our simulations. In terms of setting up routes in static net-
works, the unipath version of AXE produced shortest path
routes equivalent to Idealized Routing in both topologies,
and in the cluster topology the multipath version of AXE
produced multiple paths that were equivalent to an ECMP-
enabled version of Idealized Routing. This is clearly superior
to spanning tree, but no better than what typical L3 routing
algorithms can do (and L2 protocols like SPB and TRILL
that also use routing algorithms). Thus, AXE is able to pro-
duce the desired paths.
5.3 Performance with Link Failures
To characterize the behavior of AXE in a network undergo-
ing failures, we “warm up” the simulation for several sec-
onds and then proceed with one minute of failing and recov-
ering links using a randomized failure model based on the
“Individual Link Failures” in [24] but scaled to considerably
higher failure rates. These failure rates represent extremely
poor conditions: 24 failures over one minute for the cluster
case and 193 failures over one minute for the campus case.
For simulations using UDP trafﬁc, we looked at the num-
506
Figure 6: Effect of deduplication ﬁlter size on UDP trafﬁc in the cluster
topology with 1 Gbps links.
routes. To see how well AXE copes with migration, we run
similar experiments to those in the previous section, but mi-
grating hosts at random and with no link failures. Figure 5
shows the results of this experiment for various different rates
of migration. We ﬁnd that the increase in total trafﬁc is min-
imal – even at the ridiculously high migration rates of each
host migrating at an average rate of once per minute, the in-
crease in trafﬁc is under 0.44% and 0.02% for the campus
and cluster topologies respectively. Note this overhead is just
the overhead from AXE ﬂooding and a gratuitous ARP; we
did not model, for example, the trafﬁc actually taken to do a
virtual machine migration (though at migration rates as high
as we have simulated, we would expect the AXE ﬂooding to
be vanishingly small in comparison!).
5.5 Deduplication Filter Size
Deduplication using our ﬁlter method is subject to false neg-
atives – it may sometimes fail to detect a duplicate. When
this happens occasionally, it presents little problem: dupli-
cates are generally detected on neighboring switches, or at
the same switch the next time it cycles around, or – in the
worst case – when they reach the maximum hopcount and
are dropped. However, persistent failure to detect duplicates
runs the risk of creating a positive feedback loop: the fail-
ure to detect duplicates leads to more packets, which further
decreases the chance of detecting duplicates.
The false negative rate of the ﬁlter is inversely correlated
with the ﬁlter size, so it is important to run with ﬁlter sizes
big enough to avoid melting down due to false negatives. To
see how large the ﬁlter size should be, we ran simulations
using ﬁlter sizes ranging between 50 and 1,600. Our simu-
lations were a worst case, as we used the UDP trafﬁc model
(which, unlike TCP, does not back off when the network ef-
ﬁciency begins degrading).
Figure 6 shows the number of lost packets (which we use
as evidence of harm caused by false negatives) for the cluster
network with 1 Gbit links. Even under heavy failures, the
number of losses goes to zero with very modest sized ﬁlters
(≈500 – or even fewer for 10 Gbit links). We omit the largely
similar results for the campus topology.
5.6 Behavior under Load
Any learning network can be driven into the ground when
faced with a severe overload. Because such overloads can-
not always be prevented, it is crucial that the network recov-
ers once the problematic condition is resolved. This property
Figure 7: The ratio of received to transmitted packets in an experiment for
which the ﬁrst half is dramatically over-driven. We show both the true AXE
algorithm which unlearns state on hopcount expiration as well as a version
which does not perform unlearning.
follows from our design, but to verify it experimentally, we
ran a simulation on the cluster topology with highly random-
ized trafﬁc and a severely undersized deduplication ﬁlter. We
noted the number of packets transmitted from sending hosts
and the number of packets received by receiving hosts in
half-second intervals, and the Y axis shows the latter divided
by the former: RX/TX. Ideally one would want RX/TX to be
1, and values less than this indicate degradation of network
performance. The results are shown in Figure 7.
For the ﬁrst ﬁve seconds of the simulation, we generate
a large amount of trafﬁc (far more than the links and dedu-
plication ﬁlter can accommodate). This drives AXE into a
useless state where packets are ﬂooding, dropping, being de-
layed, and are not reliably deduplicated or learned from. In-
deed, the fraction of this trafﬁc that is delivered successfully
is negligible. At ﬁve seconds, we reduce the trafﬁc to a man-
ageable level. We see that following a spike in delivery (as
the queues that built up in the ﬁrst half of the experiment
drain), AXE quickly reaches the expected equilibrium.
We also veriﬁed that one of AXE’s safety mechanisms has
the expected effect. Speciﬁcally, when the hopcount reaches
its limit for a non-ﬂood packet, the ﬁnal switch removes (un-
learns) state for the packet’s destination. In this way, any fu-
ture packets to that destination will not follow the looping
path, but will instead ﬁnd themselves with no forwarding
state and be ﬂooded. To witness this in action, we disable
AXE’s hopcount expiration unlearning. This results in a dra-
matic drop in RX/TX ratio, because bad (looping) paths es-
tablished in the ﬁrst part of the experiment are never repaired.
5.7 Multicast
While traditional approaches to multicast maintain trees
through incremental prunes and grafts, our AXE multicast
design rebuilds the tree from scratch every time there is a
change. Rebuilding a tree requires ﬂooding packets and then
letting the tree be pruned back. Whether this approach is
reasonable or not depends on whether periodically switch-
ing back to ﬂooding is overly wasteful. While networks can
clearly withstand the occasional ﬂooded packet (broadcasts
for service discovery, and so on), the danger with multicast is
that rebuilding the tree during a high volume multicast trans-
mission (such as a video stream) may result in a large number
of packets being ﬂooded. To examine this case, we simulated
507
(a)
(b)
Figure 8: Convergence of a multicast tree using AXE while a source transmits a simulated 40Mbps video stream on the campus (a) and cluster (b). Each mark
in the graph is a packet transmitted from the source. The X axis is time, with 0 being the transmission time of the ﬁrst packet following a tree reset. The Y axis
shows the normalized packet overhead: 1.0 is when the packet is being ﬂooded, and 0.0 is when the packet is being sent only along the ﬁnal converged tree.
a transmission of data at 40Mbps (a rather extreme case –
equivalent to a high-quality 1080p Blu-ray Disc) and exam-
ined the convergence of AXE’s multicast after triggering a
reset of the multicast tree. We repeated the experiment sev-
eral times on both topologies and with group membership
between 5% and 40%, and show a representative example of
a 5% run in Figure 8. The graphs show the overhead of extra
trafﬁc, where a value of 1 indicates sending as much trafﬁc
as a ﬂood, and an overhead of 0 indicates sending as much
trafﬁc as a shortest-path tree.
In the AXE multicast algorithm, the data plane recognizes
that a PRUNE should be sent, but the control plane is ulti-
mately responsible for creating the PRUNE message. Inter-
actions between the control and data planes, however, are not
especially fast in terms of packet timescales, so we modeled
1ms and 5ms latencies along with 0ms for comparison.4 We
ﬁnd that even in the worst case, AXE converges quickly: the
overhead has dropped to less than 20% by about 5ms and is
either converged (on the campus topology) or negligible (on
the cluster topology) by about 10ms – and even at the high
rate of 40 Mbps, only 34 packets are sent during this 10ms.
It is worth noting that the convergence time is almost en-
tirely dictated by the control plane latency: with no control
plane latency, in the two experiments we show, the trees have