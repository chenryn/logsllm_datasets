IS considers a repaired link down until a conﬁgurable hold-down
timer expires (this process is dynamic, but should create biases of
similar magnitude).
Another ambiguity arises in pinpointing the “age” of each link
to allow annualized statistics to be calculated. The natural deﬁni-
tion of age is simply the amount of time between when a link was
added to the network and when it was removed. One minor issue
with this deﬁnition is that some links are added to the network be-
fore our syslog data begins (left censored), are not removed until
after our syslog data runs out, and/or continue to operate during
the months where syslog data was lost (right censored). To combat
these issues we do not allow any links to be added to the network
before the syslog data starts, remove all links from the network af-
ter the syslog data ends, and ignore any operational time for the
319period of time missing in our syslog data. A second version of this
problem that cannot be overcome directly is the granularity with
which router conﬁguration ﬁles are maintained. Since interfaces
are not tagged with creation or removal times, we rely on the ﬁrst
and last conﬁguration ﬁle that contains a valid interface description
for these times. Unfortunately, conﬁguration updates are logged
periodically—rather than instantaneously—thus, we are prone to
add links to our network later than they have actually been added
and remove them after they have likely been removed.
5.2 Internal consistency
Because our data is historical, and the CENIC network operators
did not collect or maintain any additional logs that we can use as
ground truth regarding the timing or causes of failure, we are forced
to search for alternate means of validation. We use two qualitatively
different approaches. The ﬁrst is to cross-validate the records we
do have; any inconsistencies or disagreement between syslog and
the operational email announcements increases the likelihood of er-
ror. While we cannot say for certain that the lack of inconsistency
implies correctness, we can quantify the degree of inconsistency
to provide an approximate upper bound on the accuracy of our ap-
proach. Second, certain failures may be externally visible, in which
case we can leverage logs collected by third parties.
Focusing ﬁrst on internal consistency, we use the administrator
notices (Section 4.2.3) to validate the event history reconstructed
from the syslog archive. In reconstructing this history, we used the
administrator notices to label failures with causes when available—
in particular, if there is an announcement that pertains to the partic-
ular failure. Understandably, only a small subset of the link failures
are discussed by the operators on the email list. Here, we attempt
the opposite mapping. Speciﬁcally, we check whether the recon-
structed event history also records the corresponding event.
Ideally, we would conﬁrm that each of the 3,505 distinct events
mentioned in an administrative announcement appears in the log.
Due to the difﬁculties in extracting precise details from free-form
email messages, the matching must be done manually. Hence, we
verify a random subset of the events. Of the 35 (roughly 1%) events
we inspected, only one could not be matched to a corresponding
(set of) failure(s) in the event history (i.e., 97% accuracy).
5.3 Externally visible events
In a well-designed network, most failures are masked by re-
dundant links and protocols. Hence, while network operators are
clearly interested in knowing about failures so they can address the
fault and restore proper operation, users of the network may not
even notice when failures occur. A certain class of catastrophic
failures, however, cannot be hidden: those that result in network
partitions. The CENIC networks are connected to the larger Inter-
net and, hence, any network partitions in those networks would be
observable from the commercial Internet.
We are aware of two publicly available datasets concerning
reachability that go back far enough in the past to validate our fail-
ure logs: the CAIDA Skitter/Ark active traceroute measurements,
and the University of Oregon’s Route Views BGP logs. Here, we
develop a methodology to validate our failure log—at least in the
limited case of failures that result in a network partition—by check-
ing against publicly available traceroute and BGP records.
5.3.1 CAIDA Ark/Skitter traceroute
One direct method of ascertaining whether a link is down or not
is to attempt to use it. Most commercial network operators con-
duct periodic active end-to-end probes [26] to do just that for their
own networks. CAIDA’s Ark (né Skitter) project conducts sporadic
traceroutes to numerous destinations throughout the Internet from
various traceroute servers [5]. Occasionally, Skitter probes desti-
nations within the CENIC network. While the actual route itself is
of little interest to us, the reachability of the end point is. In partic-
ular, for all Skitter probes to a destination within CENIC, we can
validate our failure log by comparing the success or failure of the
Skitter probe to our event records: for all successful Skitter probes,
we verify that all of the links traversed (which are conveniently enu-
merated by the Skitter record) were “up” at the time of the probe
according to our failure log. Conversely, should a Skitter probe
fail, we verify that either 1) the probe failed before reaching or af-
ter passing through the CENIC network, or 2) the link leaving the
last successful hop was “down” at the time of the probe according
to our log.
CAIDA provided us with the Skitter traceroute data covering six
months (January–June 2007) of our study—already over four giga-
bytes of compressed data. From the data, we extracted 75,493,637
probes directed at 301 distinct destinations within the CENIC net-
work from 17 different traceroute servers, covering 131 links and
584 distinct paths through the CENIC network. The outcome of
each of these 75 million Skitter probes was consistent with the link
states reﬂected in our event history. Unfortunately, none of the Skit-
ter probes failed within the CENIC network itself—in other words,
while the log is completely consistent with the Skitter data, Skitter
does not positively conﬁrm any failure events in the log.
5.3.2 Route Views BGP archive
Unlike traceroute, which requires active probing to detect fail-
ures, passive BGP listeners are asynchronously informed of reach-
ability information. Hence, to the extent a link’s connectivity is
monitored by BGP, its failure history is likely to be far more com-
plete. The University of Oregon’s Route Views project has de-
ployed ten BGP listeners throughout the world to collect BGP up-
dates, and makes their logs publicly available. The main challenge
with BGP data, however, is its coarse granularity. BGP speaks in
terms of networks or IP preﬁxes as opposed to individual layer-3
links like traceroute. Hence, a BGP listener will only detect when
an entire network becomes unreachable.
When considering the particular case of the CENIC network,
we must bear in mind that multiple core routers in multiple cities
would have to fail simultaneously to partition the core of the net-
work. It is not surprising, then, that we do not observe a partition
in the CENIC core network during the course of our study. How-
ever, most customer sites—places with CPE routers—have only
one router and only one or two links to the CENIC core. There-
fore, if all of the links between CENIC and a CPE router fail, the
site becomes partitioned from the network. Such events are infre-
quent, but do occasionally occur.
We identiﬁed the IP preﬁxes for 60 distinct networks (i.e., cus-
tomer sites) served by CENIC. Unfortunately, we can only use BGP
to validate a subset of these sites because CENIC does not withdraw
preﬁxes of customers residing in CENIC address space (these are
typically small customers like K-12 school districts). We identiﬁed
19 customer sites in the CENIC failure logs that have their own au-
tonomous system (AS) and for which CENIC generates BGP with-
draw messages. We identify network partitions for these sites in
our reconstructed event history by searching for multi-link failure
events that involve all of a CPE router’s links to CENIC. We de-
clare such customer sites to be isolated for the duration of the fail-
ure. One issue with this approach is that some customers may be
multi-homed—in other words, have access links to networks other
than CENIC. In such an instance, we would assert that a site is iso-
lated when in fact it is only suffering degraded service. We have
320Sites Events
Pw Hw Sw N/A
Isolation
Path change
14
19
51
105
2
4
1
2
13
24
36
73
Table 3: Summary of the CENIC network partitions
validated against the Route Views BGP data.
not, however, uncovered any evidence of such sites in our logs or
interactions with CENIC operators.
The geographically closest Route Views BGP listener to the
CENIC network is housed at the Peering and Internet Exchange
(PAIX) in Palo Alto, California. Unfortunately, the BGP listener’s
network (AS6447) does not directly peer with the CENIC network
(AS2152), but it does peer with several ASes that directly peer
with CENIC. To accommodate the idiosyncrasies of BGP conver-
gence between all of the peers of the Route Views listener2, we de-
clare a CENIC site isolated according to BGP if at least four peer
ASes withdraw all of the site’s preﬁxes. In addition to these iso-
lation events, we also observe instances where two or three ASes
withdraw all of a site’s preﬁxes but several other ASes will adver-
tise multiple paths of monotonically increasing length to a site’s
preﬁxes. We refer to this second type of event as a BGP path
change. While isolation is a strong proof of network partition, BGP
path change events are also likely due to externally visible failures
within the CENIC network and are therefore also useful for vali-
dating our error log.
Of the 147 isolating events in our event history that should be vis-
ible in BGP (see Table 7 for a breakdown), we were able to match
51 to complete BGP isolations—i.e., fully converged route with-
drawals (Table 3). If we more conservatively consider BGP path
changes, however, we are able to conﬁrm 105 of the 147 events.
Notably, of the remaining 42 events, 23 of them pertain to a sin-
gle link. It is possible that this link is backed up by a statically
conﬁgured link that is not reﬂected in our IS-IS dataset.
6. ANALYSIS
By applying the methodology described in the previous sections
to the CENIC syslogs and operational announcement email logs,
we obtain over half-a-decade worth of failure data for a moder-
ately sized production network. Hence, we are in the position to
ask several fundamental questions regarding the operation of a real
network. In particular, we consider:
O How often do failures occur? How long do they last?
O What are the causes of failures? Are some types of links
more failure-prone than others?
O What is the impact of link failures? Does the network adapt
without signiﬁcant service disruption?
While we make no claims regarding the generality of our results,
we believe that a study of this scope and duration is unprecedented
in the literature, and that our ﬁndings are likely to be representative
of a larger class of educational and research networks.
6.1 Event history at a glance
Figure 6 shows the reconstructed event history at a glance. Links
are ordered lexicographically along the y axis. Each failure is rep-
resented by a single point on the plot, located according to the start
of the event. Two aspects bear note:
2BGP announcements are modulated by local router policy, which
may differ between peers.
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
DC
CPE
HPR
1
10
100
Figure 7: Failures per link per year, excluding links up for less
than 30 days.
Vertical banding. Several vertical bands are apparent in the ﬁgure,
which correspond to system-wide events. For example, the band
in September 2005 labeled V1 in the ﬁgure is a network-wide IS-
IS conﬁguration change requiring a router restart.
(The scale of
the ﬁgure makes the link failures appear simultaneous; the band
actually spans about a week.) Another band in March 2007 (labeled
V2) is the result of a network-wide software upgrade. The third
band, V3, occurs in February 2009 as a network-wide conﬁguration
change in preparation for IPv6.
Horizontal banding. Figure 6 also contains several horizontal
segments. The nearly solid segment labeled H1 in the middle of the
ﬁgure corresponds to a series of failures on a link between a core
router and a County of Education ofﬁce. The segment is made up
of many short failures happening only a few times a day. After at
least one unsuccessful attempt to diagnose the problem, the cause
was ultimately found to be faulty hardware.
The horizontal segment labeled H2 in the ﬁgure corresponds to
a RIV-SAC link between two HPR routers. Between July 2006 and
January 2007 this link experienced over 33,000 short-duration fail-
ures. While the initial cause was a ﬁber cut, the repair process
damaged an optical device leading to instability that was difﬁcult
to diagnose. Because this single ﬂapping event accounts for 93%
of all link failures in the HPR network, we remove it from the data
set to avoid skewing further analyses.
6.2 Aggregate statistics
We begin our analysis by computing aggregate statistics about
the frequency and duration of failures on a per-link basis, both in
terms of individual failure events and cumulative link downtime.
Table 4 shows the average, median, and 95th percentile of each
distribution. For all annualized statistics, we excluded links in op-
eration fewer than 30 days because of their inﬂated variance.
6.2.1 Failure rate
Perhaps the most natural ﬁrst question we might ask is, “How
many failures are there?” Figure 7 shows the cumulative distribu-
tion function (CDF) of the number failures per link per year. We
compute the number of failures per year for each link by dividing
the number of failures by the lifetime of the link (excluding links
in operation for less than 30 days).
In the DC network, most links experience few failures, as one
might expect of a production network. The CPE network, consist-
ing of access links and routers on customer premises, is somewhat
less reliable, with a median annual failure rate of 20.5 failures per
link. The HPR network experienced considerably more failures.
321322100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
100%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
DC
CPE
HPR
100%
90%
80%
70%
60%