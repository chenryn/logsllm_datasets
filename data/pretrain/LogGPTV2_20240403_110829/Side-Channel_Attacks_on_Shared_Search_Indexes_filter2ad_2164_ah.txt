### Index Processing and Experimental Setup

The index was not subjected to stemming, but common stopwords were removed. For our evaluation, we utilized the relevance judgments from the "ad hoc" track of the sixth, seventh, and eighth sessions of NIST's Text Retrieval Conference (TREC). A total of 150 labeled queries were used in the experiment.

### Experiment Steps

Our experiment involved several concrete steps:
1. We performed queries on two versions of the NIST index: an unmodified "insecure" version using the actual document frequencies (DFs) of the NIST corpus, and a modified version using public DFs from the Enron corpus.
2. For both versions, we recorded the top 1,000 most relevant documents returned for each query.
3. Using the relevance judgments as ground truth, we computed quality metrics to measure any degradation in search quality caused by our countermeasure.

### Quality Metrics

We employed two key metrics from information retrieval: precision and mean average precision.

- **Precision**: This is the fraction of returned documents that are relevant to the query. If \( r \) is the number of relevant documents returned and \( s \) is the total number of returned documents, then precision is defined as \( \frac{r}{s} \). The metric P@n represents the precision when only the top \( n \) documents are considered. The values reported in Figure 8 are averaged over all 150 queries.

- **Mean Average Precision (MAP)**: This is the mean of the per-query average precision. Average precision is calculated by measuring the precision at every cutoff point from 1 to 1,000, summing these precisions, and dividing by the number of relevant documents.

### Results

The results of our experiment are presented in Figure 8. The column labeled "Real DFs" shows the results using the default implementation with actual DFs, while the column labeled "Enron DFs" shows the results with the public-corpus DFs countermeasure enabled (using the Enron email corpus).

The results indicate that using the Enron DFs in place of the real DFs has a negligible effect on the precision of the searches. Most values are identical when rounded to the hundredths place. This holds true for both TF-IDF and the more modern BM25 scoring function.

### Limitations and Future Work

While our evaluation provides good evidence of the practicality of the public-corpus DF countermeasure, it has several limitations:
1. We only evaluated unstructured English text corpora and queries. It is unclear if the results generalize to code repositories like GitHub. Obtaining labeled relevance judgments and corpora for code search is an interesting direction for future work.
2. The absolute quality of the search results is quite low, so an experiment on a better-tuned search system using modern IR techniques might yield different results.
3. The sample size of our experiment is small. Due to the difficulty of finding appropriate datasets and relevance judgments, we only evaluated one dataset, leaving a more thorough evaluation as an open problem.

### Vendor Response

We disclosed our findings via email to the three services investigated. Xen.do immediately removed relevance scores from API responses as a preliminary mitigation. GitHub forwarded the issue to Elastic.co, their search service provider. Elastic.co suggested several countermeasures, including:
- Small deployments could use an index-per-tenant, though this may be cost-prohibitive for large deployments.
- Disabling scoring and ranking if the resulting functionality loss is acceptable.
- Placing sensitive terms in fields that are not used for ranking, as suggested by Alex Brasetvik of Elastic.co.

We believe the public-corpus DFs countermeasure presented in Section VII is the best approach due to its scalability and deployability. Orchestrate.ioâ€™s parent company, CenturyLink, announced that the service vulnerable to our attack will be shut down on March 17th, 2017.

### Conclusion

We presented STRESS attacks, demonstrating that the industry-standard method for multi-tenant search leads to an exploitable side channel, even in complex distributed systems. We developed efficient attacks on GitHub and Orchestrate, and verified exploitability on Xen.do. Our side channel allowed us to estimate the time and cost required to extract sensitive information from private files stored in these services.

Our case studies suggest that many other systems, including those following best practices for building multi-tenant search on AWS Elasticsearch, AWS CloudSearch, Searchly, Bonsai, and Swifttype, may also be vulnerable. Future work should explore more sophisticated queries and the impact on search quality in better-tuned systems.

Based on our experiments, we recommend moving away from the simple filter-based approach to multi-tenancy and suggest using document frequencies taken only from public documents as a practical countermeasure.

### Acknowledgments

We thank the anonymous reviewers for their comments and suggestions. We also thank the employees at Elastic.co, GitHub, and Xen.do for their helpful discussions during our disclosure process. This work was supported in part by NSF grants, DARPA, and a generous gift from Microsoft. Ristenpart and Grubbs have significant financial stakes in Skyhigh Networks.

### References

[References listed as provided, with no changes made.]