title:Building a MAC-Based Security Architecture for the Xen Open-Source
Hypervisor
author:Reiner Sailer and
Trent Jaeger and
Enriquillo Valdez and
Ram&apos;on C&apos;aceres and
Ronald Perez and
Stefan Berger and
John Linwood Griffin and
Leendert van Doorn
Building a MAC-Based Security Architecture
for the Xen Open-Source Hypervisor
Reiner Sailer Trent Jaeger Enriquillo Valdez Ram·on C·aceres
Ronald Perez Stefan Berger
John Linwood Grif(cid:2)n Leendert van Doorn
fsailer,jaegert,rvaldez,caceres,ronpz,stefanb,jlg,PI:EMAIL
IBM T. J. Watson Research Center, Hawthorne, NY 10532 USA
make possible ef(cid:2)cient aggregation of multiple virtual ma-
chines on a single physical machine, with each virtual ma-
chine (VM) running its own operating system (OS).
Abstract
We present the sHype hypervisor security architecture and
examine in detail its mandatory access control facilities.
While existing hypervisor security approaches aiming at
high assurance have been proven useful for high-security
environments that prioritize security over performance and
code reuse, our approach aims at commercial security
where near-zero performance overhead, non-intrusive im-
plementation, and usability are of paramount importance.
sHype enforces strong isolation at the granularity of a vir-
tual machine, thus providing a robust foundation on which
higher software layers can enact (cid:2)ner-grained controls. We
provide the rationale behind the sHype design and describe
and evaluate our implementation for the Xen open-source
hypervisor.
1 Introduction
As workstation- and server-class computer systems have
increased in processing power and decreased in cost, it has
become feasible to aggregate the functionality of multiple
standalone systems onto a single hardware platform. For
example, a business that has been processing customer or-
ders using three computer systems(cid:150)a web server front-end,
a database server back-end, and an application server in
the middle(cid:151)can increase hardware utilization and reduce
its hardware costs, con(cid:2)guration complexity, management
complexity, physical space, and energy consumption by
running all three workloads on a single system.
Virtualization technology is quickly gaining popularity
as a way to achieve these bene(cid:2)ts. With this technology,
a software layer called a virtual machine monitor (VMM),
or hypervisor, creates multiple virtual machines out of
one physical machine, and multiplexes multiple virtual re-
sources onto a single physical resource. Virtualization is
facilitated by recent development in terms of broad avail-
ability of fully virtualizable CPUs [2, 15]. These advances
Although co-locating multiple operating systems and
their workloads on the same hardware platform offers great
bene(cid:2)ts, it also raises the specter of undesirable interac-
tions between those entities. Mutually distrusted parties re-
quire that the data and execution environment of one party’s
applications are securely isolated from those of a second
party’s applications. As a result, virtualization environ-
ments by default do not give VMs direct access to physical
resources. Instead, physical resources (e.g., memory, CPU)
are virtualized by the hypervisor layer and can be accessed
by a VM only through their virtualized counterparts (e.g.,
virtual memory, virtual CPU). The hypervisor is strongly
protected against software running in VMs, and enforces
isolation of VMs and resources.
However, total isolation is not desirable because today’s
increasingly interconnected organizations require commu-
nication between application workloads. Consequently,
there is a need for secure resource sharing by enforcing ac-
cess control between related groups of virtual machines.
The main focus of this paper is on the controlled sharing
of resources. In current hypervisor systems, such sharing is
not controlled by any formal policy. This lack of formality
makes it dif(cid:2)cult to reason about the effectiveness of iso-
lation between VMs. Furthermore, current approaches do
not scale well to large collections of systems because they
rely on human oversight of complex con(cid:2)gurations to en-
sure that security policies are being enforced. They also do
not support workload balancing through VM migration be-
tween machines well because the policy representations are
machine-dependent.
This paper explores the design and implementation of
sHype, a security architecture for virtualization environ-
ments that controls the sharing of resources among VMs
according to formal security policies. sHype goals include
(i) near-zero overhead on the performance-critical path, (ii)
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:13:59 UTC from IEEE Xplore.  Restrictions apply. 
non-intrusiveness with regard to existing VMM code, (iii)
scalability of system management to many machines via
simple policies, and (iv) support for VM migration via
machine-independent policies.
These goals are derived from the requirements of com-
mercial environments. Hypervisor security approaches
aimed at high assurance have proven useful in environments
that give security the highest priority. These approaches
control both explicit and implicit communication channels
between VMs. We believe that controlling explicit data
(cid:3)ows and minimizing, but not entirely eliminating, covert
channels via careful resource management is suf(cid:2)cient in
commercial environments.
We implemented the sHype architecture in the Xen hy-
pervisor [3], where it controls all inter-VM communication
according to formal security policies. The architecture is
designed to achieve medium assurance (Common Criteria
EAL4 [8]) for hypervisor implementations. Our modi(cid:2)ca-
tions to the Xen hypervisor are small, adding about 2000
lines of code. Our hypervisor security enhancements incur
less than 1% overhead on the performance-critical path and
the Xen paravirtualization overhead is between 0%-9% [3].
While this paper describes an sHype implementation tai-
lored to the Xen hypervisor, the sHype architecture is not
speci(cid:2)c to any one hypervisor.
It was originally imple-
mented in the rHype research hypervisor [14] and is also
being implemented in the PHYP [13] commercial hypervi-
sor.
Section 2 introduces the Xen hypervisor environment in
which we have implemented our generic security architec-
ture. Mutually suspicious workload types serve as an exam-
ple to illustrate requirements and the use of our hypervisor
security architecture. We describe the design of the sHype
hypervisor security architecture in Section 3, and its Xen
implementation in Section 4. Section 5 evaluates our archi-
tecture and implementation, and Section 6 discusses related
work.
2 Background
2.1 The Xen Hypervisor
We use the Xen [3] open-source hypervisor as an exam-
ple of a virtual machine monitor throughout this paper. Fig-
ure 1 illustrates a basic Xen con(cid:2)guration. The hypervi-
sor consists of a small software layer on top of the physical
hardware. It implements virtual resources (e.g., vMemory,
vCPU, event channels, and shared memory) and it controls
access to I/O devices.
Virtual machines, also known as domains in Xen, are
built on top of the Xen hypervisor. A special VM, called
Dom0 (domain zero) is created (cid:2)rst. It serves to manage
other VMs (create, destroy, migrate, save, restore) and con-
trols the assignment of I/O devices to VMs.
2
VMs started by Dom0 are called DomUs (user domains).
They can run any para-virtualized [3] operating system,
e.g., Linux. Guest OSs running on Xen are minimally
changed, for example by replacing privileged operations
with calls to the hypervisor. Such operations cannot be
called directly by the guest OS because they can compro-
mise the hypervisor.
In general, calls to the hypervisor
have three characteristics: (1) they offer access to virtual
resources; (2) they speed up critical path operations such
as page table management; and (3) they emulate privileged
operations that are restricted to the hypervisor but might be
necessary in guest operating systems as well.
Dom0(cid:13)
VM(cid:13)
&(cid:13)
I/O(cid:13)
Management(cid:13)
Management(cid:13)
DomU(cid:13)
DomU(cid:13)
Guest(cid:13)
OS(cid:13)
Guest(cid:13)
OS(cid:13)
...(cid:13)
DomU(cid:13)
Guest(cid:13)
OS(cid:13)
Xen Hypervisor (vMem, vCPU, EventChannels, SharedMemory)(cid:13)
System Hardware (Real Machine = CPU, MEM, Devices)(cid:13)
Figure 1. Xen hypervisor architecture
Xen offers just two shared virtual resources on top of
which all inter-VM communication and cooperation is im-
plemented:
(cid:15) Event channels: An event-channel hypervisor call enables
a VM to setup a point-to-point synchronization channel to
another VM.
(cid:15) Shared memory: A grant-table hypervisor call enables a
VM to allow another VM access to virtual memory pages
it owns. Event channels are used to synchronize access to
such shared memory.
Shared virtual resources, such as virtual network adapters
and virtual block devices, are implemented as device drivers
inside the Guest OS. Non-shared virtual resources include
virtual memory and virtual CPU.
Physical resources differ from virtualized resources in
a couple of key ways: (1) Input/Output Memory Manage-
ment Units (IO-MMUs) are needed to restrict Direct Mem-
ory Access (DMA) to and from a VMM’s memory space.
(2) Performance is best if the devices are co-located with
the code using them in the same VM, and consequently the
optimal case is a physical resource per VM, which may not
be practically feasible. (3) Driver code is too complex for
inclusion in the hypervisor, so a device to be shared by mul-
tiple VMs needs to be managed by a device domain, which
then makes this device available through inter-VM sharing
Proceedings of the 21st Annual Computer Security Applications Conference (ACSAC 2005) 
1063-9527/05 $20.00 © 2005 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 25,2021 at 12:13:59 UTC from IEEE Xplore.  Restrictions apply. 
to other VMs. In Xen, a SCSI disk or Ethernet device, for
example, can be owned by a device domain and accessed by
other VMs through virtual disk or Ethernet drivers, which
communicate with the device domain using event channels
and shared memory provided by the hypervisor.
2.2 Coalitions of VMs
In the near future, we believe that VM systems will
evolve from a set of isolated VMs into sets of VM coali-
tions. Due to hardware improvements enabling reliable iso-
lation, we believe that some control now done in operating
systems will be delegated to hypervisors. We aim for hyper-
visors to provide isolation between coalitions and provide
limited sharing within coalitions as de(cid:2)ned by a Mandatory
Access Control (MAC) policy.
Consider a customer order system. The web services
and data base infrastructure that processes orders must have
high integrity in order to protect the integrity of the busi-
ness. However, browsing and collecting possible items to be
purchased need not be as high integrity. At the same time,
an OEM’s software advertising a product that the company
distributes may be run as another workload that should be
isolated from the order workloads (web service, database,
browsing).
In the customer order example, we merge the VMs per-
forming customer orders into the Order coalition and pro-
tect them from the other VMs on the system. The Order
VMs may communicate, share some memory, network, and
disk resources. Thus, they are as a coalition con(cid:2)ned by
the hypervisor. Within the Order coalition, the hypervisor
controls sharing using a MAC policy that permits inter-VM
communication, sharing of network resources and disk re-
sources, and sharing of memory. All this sharing must be
veri(cid:2)ed to protect security of the order system. However,
the MAC policy also enables the hypervisor and device do-
mains to protect the order database from being shared with
other VMs outside the Order coalition.
2.3 Problem Statement
The problem we address in this paper is the design of
a VMM reference monitor that enforces comprehensive,
mandatory access control policies on inter-VM operations.
A reference monitor is designed to ensure mediation of all
security-sensitive operations, which enables a policy to au-
thorize all such operations [16]. A MAC policy is de(cid:2)ned
by system administrators to ensure that system (i.e., VMM)
security goals are achieved regardless of system user (i.e.,
VM) actions. This contrasts with a discretionary access
control (DAC) policy which enables users (and their pro-
grams) to grant rights to the objects that they own.
We apply the reference monitor to control all references
to shared virtual resources by VMs. This allows coalitions
3
of workloads to communicate or share resources within a
coalition, while isolating workloads of different coalitions.
Figure 2 shows an example of VM coalitions. Domain 0
has started 5 user domains (VMs), which are distinguished
inside the hypervisor by their domain ID (VM-id in Fig. 2).
Domains 2 and 3 are running order workloads. Domain 6 is
running an advertising workload, and domain 8 is running
an unrelated generic computing workload. Finally, domain
1 runs the virtual block device driver that offers two isolated
virtual disks, vDisk Order and vDisk Ads, to the Order and
Advertising coalitions. In this example, we want to enable
ef(cid:2)cient communication and sharing among VMs of the Or-
der coalition but contain communication of VMs inside this
coalition. For example, no VM running an Order workload
is allowed to communicate or share information with any
VM running Computing or Advertising workloads, and vice
versa.
VM-id=8(cid:13)
VM-id=2(cid:13)
Orders(cid:13) Ads(cid:13)
VM-id=6(cid:13)
VM-id=3(cid:13) VM-id=0(cid:13)
WL-Type:(cid:13)
Computing(cid:13)
WL-Type:(cid:13)
Order(cid:13)
RAM(cid:13)
Disk(cid:13)
Virt. Disk(cid:13)
Connector(cid:13)
SCSI(cid:13)
HD(cid:13)
real(cid:13)
disk(cid:13)
VM-id=1(cid:13)
vDisk Server(cid:13)
Orders(cid:13) Ads(cid:13)
Virt.(cid:13)
Virt.(cid:13)
Disk(cid:13)
Disk(cid:13)
WL-Type:(cid:13)
Advertising(cid:13)
WL-Type:(cid:13)
Order(cid:13)
Virt. Disk(cid:13)
Connector(cid:13)
Virt. Disk(cid:13)
Connector(cid:13)
Dom0(cid:13)
VM(cid:13)
Mgmt(cid:13)
Xen Hypervisor(cid:13)
System Hardware (Real Machine = CPU, MEM, I/O)(cid:13)