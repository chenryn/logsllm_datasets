### Characteristics Change and Confusion Matrix Analysis

Between the end of 2006 and the beginning of 2008, the characteristics of network traffic have changed. The confusion matrix is a valuable tool for identifying these issues. Figure 8 presents the confusion matrix for the scenario where the model is trained on T-I trace data and tested on MS-I trace data. We observe that a significant fraction of FTP traffic is misclassified as MAIL. This misclassification is due to the overlapping distribution of packet sizes for FTP and MAIL classes across different sites. For example, Figure 7 shows the distribution of the second packet sizes for MS-I and T-I, highlighting this overlap.

This issue is a classic case of data overfitting, where the classifier has learned overly specific site characteristics. Similar observations were made in other cases where performance degraded significantly from static to cross-site testing.

### Misclassifications and Data Overfitting

The confusion matrix (Figure 8) indicates that misclassifications occur across almost all traffic classes. In many instances, there is a significant bias toward the most common classes, such as EDONKEY and WEB. Additionally, some applications, like FTP, are frequently confused with MAIL.

One might argue that the overfitting problem is related to the feature set used. However, this is not the case, as we will demonstrate in the next section using a second set of features.

### Set B (Advanced Statistics)

Similar to Set A, we observed significant degradation in cross-site testing with Set B. For instance, the CHAT and BITTORRENT classes perform well in the static case but degrade significantly in cross-site studies. Set B includes several features, each of which can be a potential source of data overfitting. Analyzing each feature in isolation would be challenging. Instead, we focus on one feature, the port number, which is easier to explain in terms of overfitting.

Several studies [12, 13] have claimed that port numbers have high predictive power and should improve classification accuracy. However, the use of port numbers is problematic because they are treated as quantitative rather than qualitative values. Most classification algorithms use similarity metrics (distances) among features, and from this perspective, port 80 is closer to port 25 than to port 443 or 8080.

To better understand the impact of port numbers, we applied our second set of features with and without port numbers in both static and cross-site cases. The results are detailed below.

#### Port Impact - Static Case

In all static cases, including port numbers increases both accuracy and precision, typically by a few percent for P2P applications and up to 38% for the FTP class. Here are the detailed results for the WEB and P2P classes:

- **WEB Class**: The inclusion of port numbers has a minor impact on this class, which is good news given that web traffic uses a wide range of ports, especially 80, 443, and 8080.
- **P2P Classes**: The accuracy and precision of P2P classes, particularly the EDONKEY class, are significantly increased when using port numbers. However, we observed that legacy ports for these applications are rarely used: 18 to 40% of the flows for EDONKEY and at most 16% for BITTORRENT.

#### Port Impact - Cross-Site Case

In cross-site studies, using port numbers is detrimental, especially for P2P traffic. In the static case, the classifier learns non-legacy port numbers specific to the training site, which are predictive in the static case but misleading in the cross-site case because non-legacy port numbers differ between sites. This is illustrated in Figure 11 for the MS-I and R-II traces, which were captured two weeks apart. The distribution of remote port numbers is similar for both traces (Figure 11(b)), while the distribution of local port numbers differs (Figure 11(a)). The former is expected due to the nature of P2P networks, and the latter is partly due to heavy-hitters, i.e., local clients generating a lot of transfers using e-Donkey. The presence of heavy-hitters is a known and global phenomenon, so we can expect similar behavior regardless of the PoP size. In summary, although port numbers have strong predictive power, they must be used cautiously to avoid overfitting.

### Impact of Classification Algorithm

So far, we have considered a single machine learning algorithm, C4.5, with different feature sets. In this section, we explore the impact of the classification algorithm. We consider two alternatives to C4.5: Naive Bayes with kernel estimation and Bayesian Network. As we will see, the issues described in previous sections persist and can be even more pronounced with these algorithms.

Figures 12(a) and 12(b) show the overall accuracy for both algorithms using Set A. While using C4.5 for cross-site studies, we found that the FTP case was particularly complex. Figure 12(c) presents the accuracy for FTP using the Bayesian Network. Detailed per-application results are omitted for clarity. From these figures, we conclude:

- **C4.5 Performance**: In almost all cases, C4.5 performs best in terms of overall accuracy in both static and cross-site experiments.
- **Degradation in Accuracy**: The degradation in overall accuracy for Naive Bayes with kernel density estimation and Bayesian Network in cross-site cases is similar or higher (up to 17% in the worst case) than with C4.5.
- **Per-Application Degradation**: Per-application accuracy degradation can be even more pronounced for Naive Bayes with kernel density estimation and Bayesian Network compared to C4.5.

### Cross-Site Study Discussion

The main lesson from the cross-site study is that although the overall accuracy degradation is often acceptable, some classes that perform well in the static case may suddenly degrade in cross-site testing. This result persists across various feature sets and machine learning algorithms. We have demonstrated that data overfitting is the root cause of this problem, a phenomenon that, to the best of our knowledge, has not been previously highlighted. 

From this point, the conclusion is twofold:
1. Training a classifier on one site before deploying it on another can lead to unpredictable results.
2. Cross-site studies allow us to pinpoint problems that cannot be observed otherwise.

A final conclusion from our results is that once a classifier is trained on a site, it can be used for a significant period on that site. However, more work is needed to validate this observation, which we made for two traces collected two weeks apart on the same PoP.

### Mining the Unknown Class

In this section, we investigate the results obtained when the statistical classifier is applied to the UNKNOWN class. Such a classifier could be included as a module in tools like ODT and used as a source of information or assistance in the tool development process if an increase in unknown traffic is noted. To the best of our knowledge, this is the first study to tackle this problem using supervised methods.

#### Methodology

Our study of filtering scenarios (Table 4) revealed that the UNKNOWN class consists of a large fraction of connections (61% to 84% depending on the trace) for which the beginning is missing. These truncated connections carry the majority of bytes in this class, from 79% to 86%. To maximize the number of bytes for which a prediction could be made, we adopted the following strategy:

1. **Feature Set**: We used the second set of features, as the first set (packet sizes) would have reduced the number of flows and bytes for which a prediction could be made.
2. **Training**: We trained the classifier on all known traffic for which a three-way handshake was observed.
3. **Application**: We applied the classifier to all flows in the UNKNOWN class without any a priori filtering.
4. **Output**: Our classifier outputs a class prediction associated with a confidence level for each flow.

#### Predictions

Figure 14 depicts the cumulative distribution function of per-flow confidence levels for the flows in the UNKNOWN class. With a threshold of 95%, we observe that, depending on the trace, a fraction between 40% to 70% of the flows are kept for further analysis.

Predictions (classifications) are reported in Table 5. We present only results for classes that performed well in the static case and carry at least 1% of bytes for at least one of the traces. These results are consistent with those obtained for known traffic, showing a majority of Web, e-Donkey, and BitTorrent traffic.

| Trace | EDO. | BT. | GNU. | WEB | OTHERS |
|-------|------|-----|------|-----|--------|
| MSI   | 18%/32% | 1%/15% | 1%/3% | 8%/≤1% | 5%/14% |
| RII   | 17%/46% | 2%/9% | 1%/10% | 5%/≤1% | 8%/12% |
| RIII  | 26%/42% | 3%/≤1% | 1%/10% | 9%/≤1% | 2%/3% |
| TI    | 28%/71% | 3%/≤1% | 1%/10% | 3%/≤1% | 28%/50% |

**Table 5: Unknown class predictions [flows%/bytes%].**

### Validation

Since we are dealing with unknown traffic, ODT does not provide a reference point. We need to validate the predictions of the statistical classifier using other methods. In this section, we perform several side tests to challenge the predictions for the unknown traffic, primarily using the knowledge about the {IP, port} pairs of the endpoints of the flows.

#### Peer-to-Peer Predictions

For P2P predictions, we use the following additional sources of information per flow:

- **Port Numbers**: Even for P2P applications, a fraction of users still use legacy ports [12]. If a legacy port is observed for a flow classified as "P2P," this information supports the classifier's result.
- **Endpoint Information**: Additional validation is done using endpoint information.

### Conclusion

The cross-site study highlights the importance of considering data overfitting and the need for careful feature selection and algorithm choice. The UNKNOWN class analysis provides a new approach to handling unclassified traffic, offering valuable insights and potential improvements for network traffic classification tools.