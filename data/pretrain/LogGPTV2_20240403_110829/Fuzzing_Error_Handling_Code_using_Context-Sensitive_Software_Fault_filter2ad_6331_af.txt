0
0
2
All
5
3
2
4
0
14
All
1
1
1
0
0
3
Table 9: Results of bug detection for comparison.
AFLSmart and FairFuzz, FIFUZZ covers more code branches
in nm, size and ar, but covers less code branches in objdump
and readelf. The main reason is that the fuzzing process of
program inputs in FIFUZZ is implemented by referring to
AFL, while AFLSmart and FairFuzz use some techniques
to improve mutation and seed selection of fuzzing program
inputs compared to AFL. For this reason, AFLSmart and
FairFuzz can cover more infrequently executed code related
to inputs than FIFUZZ, though they still miss much error
handling code covered by FIFUZZ. We believe that if we
implement their fuzzing process of program inputs in FIFUZZ,
it can cover more code branches than AFLSmart and FairFuzz
in all the tested programs.
Table 9 shows the results of bug detection. Firstly, the two
bugs found by AFL and AFLFast are also found by AFLSmart,
FairFuzz and FIFUZZ. Secondly, AFLSmart and FairFuzz re-
spectively ﬁnd one bug missed by AFL, AFLFast and FIFUZZ.
The one extra bug found by AFLSmart is different from that
found by FairFuzz, as they improve mutation and seed selec-
tion for program inputs in different ways. Finally, FIFUZZ
ﬁnds 14 bugs, and 12 of them related to error handling code
are missed by AFL, AFLFast, AFLSmart and FairFuzz.
6 Discussion
6.1 False Positives of Error-Site Extraction
Our static analysis in Section 4.1 describes how to identify
possible error sites from the tested program code. However,
as shown in Section 5.2, our static analysis still has some false
positives in identifying error sites, due to two main reasons:
Firstly, some functions that return pointers or integers never
cause errors, even though their return values are often checked
in the code. The functions strcmp and strstr are examples.
However, our static analysis still considers that such func-
tions can cause error, and identiﬁes the function calls to them
as possible error sites, causing false positives. To solve this
problem, we plan to analyze the deﬁnition and call graph of
each such function, to check whether it can indeed return an
erroneous value that represents an error.
Secondly, a function can indeed fail and trigger error han-
dling code, but some function calls to this function never fail
considering their code contexts. This case can occur for some
function calls that can cause input-related errors, when all
possible inputs may have been changed into valid data before
these function calls are used. However, our static analysis still
identiﬁes these function calls as possible error sites, causing
false positives. To solve this problem, we plan to use sym-
bolic execution [36] to analyze code context and calculate the
constraints for each identiﬁed function call.
6.2 False Negatives of Bug Detection
FIFUZZ may miss real bugs in error handling code due to
three possible reasons:
Firstly, as described in Section 4.1, to avoid injecting re-
peated faults, we only consider library functions for fault
injection. However, some functions deﬁned in the tested pro-
gram can also fail, and they do not call any library function.
Thus, FIFUZZ does not cover the error handling code caused
by the failures of the calls to such functions.
Secondly, some error sites are executed only when speciﬁc
program inputs and conﬁguration are provided. In the evalua-
tion, FIFUZZ cannot provide all possible program inputs and
conﬁguration. As a result, some error sites may not be exe-
cuted, and thus their error handling code cannot be covered.
USENIX Association
29th USENIX Security Symposium    2605
04812162024Time (hour)010002000300040005000Covered branchesnm04812162024Time (hour)016003200480064008000Covered branchesobjdump04812162024Time (hour)010002000300040005000Covered branchessize04812162024Time (hour)010002000300040005000Covered branchesar04812162024Time (hour)0200040006000800010000Covered branchesreadelfFIFUZZAFLAFLFastAFLSmartFairFuzzThirdly, we only detect the bugs causing crashes and those
reported by ASan. We can use other checkers to detect other
kinds of bugs, such as MSan [41] which detects uninitialized
uses, UBSan [57] which detects undeﬁned behaviors, and
TSan [56] which detects concurrency bugs.
6.3 Manual Analyses
FIFUZZ requires two manual analyses in this paper. Firstly,
we perform a manual study in Section 2.3. This manual study
is required for gaining the insights into building the automated
static analysis, and we believe that the manual study provides
the most representative and comprehensive results that help
estimate the causes of errors. Secondly, in Section 5.2, we
manually select realistic error sites from the possible error
sites identiﬁed by FIFUZZ. This manual selection is required,
as the static analysis of identifying possible error sites still
has many false positives. For example, as shown in Table 4,
we manually check the possible error sites identiﬁed by the
static analysis, and ﬁnd that only 18.6% of them are real. We
believe that improving the accuracy of this static analysis can
help reduce such manual work.
6.4 Performance Improvement
The performance of FIFUZZ can be improved in several ways:
Dropping useless error sequences. As shown in Table 5,
FIFUZZ generates many useless error sequences that fail to
increase code coverage. However, they are still used in fault
injection to execute the tested program, reducing the fuzzing
efﬁciency. We believe that static analysis can be helpful to
dropping these useless error sequences. For example, after
an original error sequence mutates and generates new error
sequences, a static analysis can be used to analyze the code of
the tested program, and infer whether each new error sequence
can increase code coverage compared to the original error
sequence. If not, this error sequence will be dropped, before
being used in fault injection to execute the tested program.
Lightweight runtime monitoring. As shown in Figure 8,
to collect runtime calling context, FIFUZZ instruments each
function call to the function deﬁned in the tested program
code and each function deﬁnition. Thus, obvious runtime
overhead may be introduced. To reduce runtime overhead,
FIFUZZ can use some existing techniques of lightweight
runtime monitoring, such as hardware-based tracing [3, 31]
and call-path inferring [42].
Multi-threading. At present, FIFUZZ works on simple
thread. Referring to AFL, to improve efﬁciency, FIFUZZ
can work on multiple threads. Speciﬁcally, after an original
error sequence mutates and generates new error sequences,
FIFUZZ can use each new error sequence for fault injection
and execute the tested program on a separate thread. When
synchronization is required, all the execution results and gen-
erated error sequences can be managed in a speciﬁc thread.
6.5 Exploitability of Error Handling Bugs
To detect bugs in error handling code, FIFUZZ injects errors
in speciﬁc orders according to calling context. Thus, to ac-
tually reproduce and exploit a bug found by FIFUZZ, two
requirements should be satisﬁed: (1) being able to actually
produce related errors: (2) controlling the occurrence order
and time of related errors.
For the ﬁrst requirement, different kinds of errors can be
produced in different ways. We have to manually look into the
error-site function to understand its semantics. However, most
of the bugs found in our experiments are related to failures of
heap-memory allocations. Thus, an intuitive exploitation way
is to exhaustively consume the heap memory, which has been
used in some approaches [58, 66] to perform attacks.
For the second requirement, as we have the error sequence
of the bug, we can know when to and when not to produce the
errors. A key challenge here is, when errors are dependent to
each other, we must timely produce an error in a speciﬁc time
window. Similar to exploiting use-after-free bugs [62, 63], if
the window is too small, the exploitation may not be feasible.
7 Related Work
7.1 Fuzzing
Fuzzing is a promising technique of runtime testing to detect
bugs and discover vulnerabilities. It generates lots of program
inputs in a speciﬁc way to cover infrequently executed code.
A typical fuzzing approach can be generation-based, mutation-
based, or the hybrid of them.
Generation-based fuzzing approaches [15,27,59,64] gener-
ate inputs according to the speciﬁc input format or grammer.
Csmith [64] is a randomized test-case generator to fuzz C-
language compilers. According to C99 standard, Csmith ran-
domly generates a large number of C programs as inputs for
the tested compiler. These generated programs contain com-
plex code using different kinds of C-language features free of
undeﬁned behaviors. LangFuzz [29] is a black-box fuzzing
framework for programming-language (PL) interpreters based
on a context-free grammar. Given a speciﬁc language gram-
mer, LangFuzz generates many programs in this language as
inputs for the tested language interpreter. To improve possi-
bility of ﬁnding bugs, LangFuzz uses the language grammer
to learn code fragments from a given code base.
Mutation-based fuzzing approaches [1, 7, 13, 26, 30, 38,
51, 65] start from some original seeds, and perform muta-
tion of the selected seeds, to generate new inputs, without
requirement of speciﬁc format or grammer. To improve code
coverage, these approaches often mutate existing inputs ac-
cording to the feedback of program execution, such as code
coverage and bug-detection results. AFL [1] is a well-known
coverage-guided fuzzing framework, which has been widely-
used in industry and research. It uses many effective fuzzing
2606    29th USENIX Security Symposium
USENIX Association
strategies and technical tricks to reduce runtime overhead
and improve fuzzing efﬁciency. To improve mutation for in-
puts, FairFuzz [38] ﬁrst identiﬁes the code branches that are
rarely hit by previously-generated inputs, and then uses a new
lightweight mutation method to increase the probability of
hitting the identiﬁed branches. Speciﬁcally, this method ana-
lyzes the input hitting a rarely hit branch, to identify the parts
of this input that are crucial to satisfy the conditions of hitting
that branch; this method never changes the identiﬁed parts of
the input during mutation.
Some approaches [6, 45, 50, 60] combine generation-based
and mutation-based fuzzing to efﬁciently ﬁnd deep bugs.
AFLSmart [50] uses a high-level structural representation
of the seed ﬁle to generate new ﬁles. It mutates on the ﬁle-
structure level instead of on the bit level, which can com-
pletely explores new input domains without breaking ﬁle va-
lidity. Superion [60] is a grammar-aware and coverage-based
fuzzing approach to test programs that process structured in-
puts. Given the grammar of inputs, it uses a grammar-aware
trimming strategy to trim test inputs using the abstract syn-
tax trees of parsed inputs. It also uses two grammar-aware
mutation strategies to quickly carry the fuzzing exploration.
Existing fuzzing approaches focus on generating inputs to
cover infrequently executed code. However, this way cannot
effectively cover error handling code triggered by non-input
occasional errors. To solve this problem, FIFUZZ introduces
software fault injection in fuzzing, and fuzzes injected faults
according to the feedback of program execution. In this way,
it can effectively cover error handling code.
7.2 Software Fault Injection
Software fault injection (SFI) [52] is a classical and widely-
used technique of runtime testing. SFI intentionally injects
faults or errors into the code of the tested program, and then
executes the program to test whether it can correctly handle
the injected faults or errors during execution. Many existing
SFI-based approaches [9–11,18,25,39,40,55,67] have shown
promising results in testing error handling code.
Some approaches [9, 10, 55] inject single fault in each test
case to efﬁciently cover error handling code triggered by just
one error. PairCheck [9] ﬁrst injects single fault by corrupt-
ing the return values of speciﬁc function calls that can fail
and trigger error handling code, to collect runtime informa-
tion about error handling code. Then, it performs a statistical
analysis of the collected runtime information to mine pairs
of resource-acquire and resouce-release functions. Finally,
based on the mined function pairs, it detects resource-release
omissions in error handling code.
To cover more error handling code, some approaches [11,
18,25,39,40,67] inject multiple faults in each test case. Some
of them [25, 39, 40] inject random faults, namely they inject
faults on random sites or randomly change program data.
However, some studies [35, 43, 44] have shown that random
fault injection introduces much uncertainty, causing that the
code coverage is low and many detected bugs are false. To
solve this problem, some approaches [11, 18, 67] analyze pro-
gram information to guide fault injection, which can achieve
higher code coverage and detect more bugs. ADFI [18] uses
a bounded trace-based iterative generation strategy to reduce
fault scenario searching, and uses a permutation-based replay
mechanism to ensure the ﬁdelity of runtime fault injection.
To our knowledge, existing SFI-based approaches perform
only context-insensitive fault injection. Speciﬁcally, they in-
ject faults based on the locations of error sites in source code,
without considering the execution contexts of these error sites.
Thus, if an fault is constantly injected into an error site, this
error site will always fail when being executed at runtime.
However, some error handling code is only triggered when
related error site fails in a speciﬁc calling context but succeeds
in other calling contexts. In this case, existing SFI-based ap-
proaches cannot effectively cover such error handling code,
and thus often miss related bugs.
7.3 Static Analysis of Error Handling Code
Static analysis can conveniently analyze the source code of the
target program without actually executing the program. Thus,
some existing approaches [28, 32, 33, 37, 53] use static analy-
sis to detect bugs in error handling code. EDP [28] statically
validates the error propagation through ﬁle systems and stor-
age device drivers. It builds a function-call graph that shows
how error codes propagate through return values and function
parameters. By analyzing this call graph, EDP detects bugs
about incorrect operations on error codes. APEx [33] infers
API error speciﬁcations from their usage patterns, based on a
key insight that error paths tend to have fewer code branches
and program statements than regular code.
Due to lacking exact runtime information, static analysis
often reports many false positives (for example, the false
positive rate of EPEx is 22%). However, static analysis could
be introduced in FIFUZZ to drop useless error sequences,
which can improve its fuzzing efﬁciency.
8 Conclusion
Error handling code is error-prone and hard-to-test, and ex-
isting fuzzing approaches cannot effectively test such code
especially triggered by occasional errors. To solve this prob-
lem, we propose a new fuzzing framework named FIFUZZ, to
effectively test error handling code and detect bugs. The core
of FIFUZZ is a context-sensitive software fault injection (SFI)
approach, which can effectively cover error handling code in
different calling contexts to ﬁnd deep bugs hidden in error
handling code with complicated contexts. We have evaluated
FIFUZZ on 9 widely-used C applications. It reports 317 alerts,
which are caused by 50 new and unique bugs in terms of their
root causes. 32 of these bugs have been conﬁrmed by related
USENIX Association
29th USENIX Security Symposium    2607
developers. The comparison to existing fuzzing tools shows
that, FIFUZZ can ﬁnd many bugs missed by these tools.