Pingmesh at a given server. While VMs in a VNET can be spread
across datacenters in a region, a server only maintains a full physi-
cal Ping mesh with others in the same rack [9]. Additionally, ping
packets likely take different paths through the network core. Thus,
if a network fault that would impact physical and VNET pings
alike is several hops away from a server, it is possible that physical
Pingmesh may miss a problem that VNET Pingmesh notices, or
vice versa. Conversely, if a fault is located at the server to ToR
uplink (2.2% of examined incidents) we see that physical Pingmesh
is impacted 90+% of the time.
These results suggest that we can use a simple heuristic for
triaging latency anomalies—if only large VNET mapping latency
is impacted, we may investigate VNET infrastructure and server
metrics first; if multiple VNETs of differing sizes suffer, we may
suspect the physical network.
5.2 Cross-layer aliasing complicates diagnosis
While monitoring oddities may be visible at a glance from VNET
Pingmesh, cross-layer aliasing and tenant behaviours can compli-
cate anomaly diagnosis. In one instance, several VNETs (large and
small) revealed unpredictable but persistent and total ping losses
for certain VMs, split amongst subnet lines. While this may indi-
cate ToR or pod connectivity loss in the physical network, this is
unexpected at the VNET layer where VMs with adjacent addresses
are unlikely to be physically adjacent. Confusingly, while connec-
tivity failures were relatively long-term (hours to days), they were
unpredictable; only certain VMs in a VNET were afflicted, and
connectivity loss had a chance of spontaneously resolving. Further-
more, despite alarming connectivity statistics for these VNETs, no
customer issues were raised.
Investigations revealed that these losses were due to a monitoring
bug affecting just probe packets. Specifically, ‘UDRs’ (Section 3.2)
were, for some VNETs, stealing Pingmesh responses—resulting in
subnet-level loss for VNETs with tunnels. While pings arrived at
the destination, ping responses were intercepted by tunnel rules
and redirected to an unrelated remote server that discarded them.
Infrequent problem incidence confounded diagnosis; a minority
of VNETs possess UDRs, a minority of which triggered ping loss.
Furthermore, UDRs are subject to tenant modification. Before we
diagnosed the bug, changing rule sets would resolve individual
faults, leaving us at the mercy of tenant behaviour to examine the
problem. Ultimately, a related bug (leaking ping packets between
paired VNETs) revealed a VSwitch rule priority bug that was also
the root cause for connectivity loss within non-paired VNETs.
Thus, cross-layer aliasing complicates VNET-Pingmesh-driven
diagnosis. Two takeaways emerge; (1) that cross-layer interactions
can complicate diagnosis in terms of timing and reproducibility
of bugs, and (2) that again, the mechanical effort required to fix
the bug was relatively simple, once diagnosis had occured. Here,
we simply modified VNET rulebase generation to prevent probes
from being processed by UDRs, thus ensuring that VNET Pingmesh
measured point-to-point non-tunneled latency.
5.3 More difficult root-causing and mitigation
We distinguish between diagnosing and root-causing faults; e.g.
a diagnosis may reveal that connection timeouts are caused by
lost packets for a subnet at a switch, while root-cause analysis
may reveal the causative ACL misconfiguration. For physical-layer
faults, this distinction may be unnecessary where simply rebooting
a device may clear the underlying error [14]. Alternatively, traffic
may be re-routed around a fault under examination [10].
However, VNETs and smaller tenant deployments can complicate
mitigation. Unlike a network core fault, we cannot route around the
only VSwitch providing VM network connectivity. While this may
be akin to ToR failures in traditional datacenters, large-scale ser-
vices in such networks may be more amenable to losing capacity or
migrating application workloads [1, 11] compared to smaller cloud
tenants. Similarly, rebooting physical servers to possibly clear faults
may inacceptably impact availability. While VM live migration may
provide respite to a tenant, it may not solve the underlying problem
and prevent another tenant from being impacted down the line.
Thus, effective root-cause analysis takes on greater importance
in cloud networks; unfortunately, it can also be complicated by
cross-layer aliasing.
We re-examine VNET mapping latency measurements in the
context of root-cause analysis. A small minority of servers exhibit
slow mapping lookups of ≥ 10 msec in length. As a userspace op-
eration, address mapping is subject to scheduler-variation effects
ServerlocalRacklocalClusterlocalClusterremoteSmallVNETlatencybylocality102103MicrosecondsServerlocalRacklocalClusterlocalClusterremoteLargeVNETlatencybylocality102103MicrosecondsIMC ’18, October 31-November 2, 2018, Boston, MA, USA
Roy et al.
and thus spikes in CPU utilization and (as we will discuss) IO bus
contention. To quantify this effect, we examine the prevalence of
high latency (specifically, the likelihood that the server-average
VNET latency ≥ 2 msec) as a function of server disk I/O and CPU
utilization in Figure 3. Both graphs are normalized to the baseline
probability at nominal utilization levels. We see a clear correlation
between utilization and latency; as average disk utility passes 10%,
we see a sharp increase in likelihood that latency is past acceptable
boundaries. CPU utilization is measured as a 5-minute average,
however, and so we cannot distinguish between a server CPU con-
stantly at 5% utilization (unlikely to impact VNET latency) or at
100% utilization ≈5% of the time—which would significantly impact
latency during that period. Even so, a (weaker) correlation between
CPU utilization and high latency emerges.
Since both CPU and IO utilization impact mapping latency, a
large variety of root causes can apply. In one case, a latency alert cor-
related with an internal OS-update roll-out. Investigations showed
that sustained disk I/O was interfering with userspace address map-
ping lookups due to a logging statement blocked on disk I/O. While
small VNETs and existing connections (both in-cache) were not
impacted, connections requiring a userspace mapping lookup were.
(Client disk traffic was not impacted due to storage disaggregation.)
Subsequent analysis across Azure showed a correlation between
server I/O utilization and high (false-positive) latency indications.
In another server, a pair of processes briefly spiked CPU utilization
every minute, periodically impacting mapping latency. Yet another
server had high disk utilization due to a mysteriously large num-
ber of sync operations. In other incidents, Azure-infrastructure
driven disk I/O caused synchronized latency spikes of ≤ 1 minute
spread across geographically-disparate clusters. Thus, a large va-
riety of root-causes impact the same monitoring signals, yielding
an aliasing problem where monitoring metrics may not provide
enough insight on the underlying problems. Since these incidents
have different and possibly invasive fixes (e.g. killing a service with
runaway CPU utilization), blindly and optimistically trying fixes is
inacceptable—we must accurately root cause the issue first. Thus,
tenant virtualized networks simultaneously increase the impor-
tance of, while simultaneously complicating, root-causing faults.
6 CONCLUSIONS
Several open problems remain. First, while we have dealt with
cross-layer aliasing in an ad-hoc manner—identifying measurement
oddities as they crop up, diagnosing and root-causing their source
and accounting for them—the ever increasing feature sets for tenant
virtualized networks both increases the future likelihood of such
interactions and complicates the task of searching the state space
of all possible interactions a priori. A systematic methodology for
avoiding such interactions or accounting for them during the design
phase would be ideal. Second, while characterizing precision and
recall are important for validating monitoring effectiveness, we
find it hard to characterize them in the absence of customer impact
ground truth. This is due both to the fundamentally incomplete
coverage that a system like VNET Pingmesh is capable of providing
in the presence of hybridized networks (networks split between
Azure and private customer resources) and due to the fact that
customers themselves may miss issues like gray faults, and that
Figure 3: Multiplicative likelihood of high VNET Pingmesh
latency vs. server utilization.
probe traffic may not be susceptible to gray faults that do impact
tenant application traffic. Thus, evaluating passive methodologies
that infer customer VM network performance from the VSwitch
layer [6] in conjunction with active-probing monitoring systems
may be a productive line of inquiry. Third, we have thus far only
scratched the surface of using multiple layers of monitoring systems
to perform fault triage and attribution; a more longitudinal study
may reveal deeper insights.
ACKNOWLEDGEMENTS
N. Aggarwal, M. Dasgupta, A. E. Sreenath, A. Fu, D. Firestone, S.
Garg, D. Jagasia, A. Kumar, N. Motwani, J. Park, C. Raje, P. Shri-
vastava, A. Shukla, Q. Zuhair, M. Zygmunt, and the entire VNET
and VFP teams at Microsoft provided valuable insight into Azure
Networking.
REFERENCES
[1] Hadoop. http://hadoop.apache.org/.
[2] A. Adams, P. Lapukhov, and H. Zeng.
https://code.facebook.com/posts/
1534350660228025/netnorad-troubleshooting-networks-via-end-to-end-
probing/.
[3] B. Arzani, S. Ciraci, L. Chamon, Y. Zhu, H. H. Liu, J. Padhye, B. T. Loo, and
G. Outhred. 007: Democratically finding the cause of packet drops.
In 15th
USENIX Symposium on Networked Systems Design and Implementation, Renton,
WA, 2018.
[4] B. Arzani, S. Ciraci, B. T. Loo, A. Schuster, and G. Outhred. Taking the blame
game out of data centers operations with NetPoirot. In Proceedings of the ACM
SIGCOMM Conference, Florianopolis, Brazil, 2016.
[5] D. Firestone. VFP: a virtual switch platform for host SDN in the public cloud.
In Proceedings of the 14th USENIX Conference on Networked Systems Design and
Implementation, Boston, MA, 2017.
[6] M. Ghasemi, T. Benson, and J. Rexford. Dapper: Data plane performance diagnosis
of TCP. In Proceedings of the Symposium on SDN Research, Santa Clara, CA, 2017.
[7] M. Ghobadi and R. Mahajan. Optical layer failures in a large backbone. IMC ’16,
Santa Monica, California, USA, 2016. ACM.
[8] A. Greenberg. Pingmesh + NetBouncer: Fine-grained path and link monitoring
for data centers. https://atscaleconference.com/videos/pingmesh-netbouncer-
fine-grained-path-and-link-monitoring-for-data-centers/, 2016.
[9] C. Guo, L. Yuan, D. Xiang, Y. Dang, R. Huang, D. Maltz, Z. Liu, V. Wang, B. Pang,
H. Chen, Z.-W. Lin, and V. Kurien. Pingmesh: A large-scale system for data
center network latency measurement and analysis. In Proceedings of the ACM
SIGCOMM Conference, Aug. 2015.
[10] V. Liu, D. Halperin, A. Krishnamurthy, and T. Anderson. F10: A fault-tolerant
engineered network. In Proceedings of the USENIX Symposium on Networked
Systems Design and Implementation, Apr. 2013.
020406080100Diskutility(%)5-minuteaverage246Mult.prob.ofhighlatency012345CPUutility(%)5-minuteaverage12Mult.prob.ofhighlatencyCloud Datacenter SDN Monitoring
IMC ’18, October 31-November 2, 2018, Boston, MA, USA
[11] A. Roy, H. Zeng, J. Bagga, G. Porter, and A. C. Snoeren. Inside the social network’s
(datacenter) network. In Proceedings of the ACM SIGCOMM Conference, London,
England, Aug. 2015.
[12] A. Roy, H. Zeng, J. Bagga, and A. C. Snoeren. Passive realtime datacenter fault
In Proceedings of the 14th USENIX Conference on
detection and localization.
Networked Systems Design and Implementation, Boston, MA, 2017.
[13] A. Singh, J. Ong, A. Agarwal, G. Anderson, A. Armistead, R. Bannon, S. Boving,
G. Desai, B. Felderman, P. Germano, A. Kanagala, J. Provost, J. Simmons, E. Tanda,
J. Wanderer, U. Hölzle, S. Stuart, and A. Vahdat. Jupiter rising: A decade of Clos
topologies and centralized control in Google’s datacenter network. In Proceedings
of the ACM SIGCOMM Conference, 2015.
[14] X. Wi, D. Turner, G. Chen, D. Maltz, X. Yang, L. Yuan, and M. Zhang. NetPilot:
Automating Datacenter Network Failure Mitigation. In Proceedings of the ACM
SIGCOMM Conference, Helsinki, Finland, Aug. 2012.
[15] H. Zeng, P. Kazemian, G. Varghese, and N. McKeown. Automatic test packet
generation. CoNEXT ’12, Nice, France, 2012. ACM.
[16] Q. Zhang, V. Liu, H. Zeng, and A. Krishnamurthy. High-resolution measurement
of data center microbursts. In Proceedings of the Internet Measurement Conference,
London, United Kingdom, 2017. ACM.
[17] Q. Zhang, G. Yu, C. Guo, Y. Dang, N. Swanson, X. Yang, R. Yao, M. Chintalapati,
A. Krishnamurthy, and T. Anderson. Deepview: Virtual disk failure diagnosis and
pattern detection for azure. NSDI ’18, Renton, WA, 2018. USENIX Association.
[18] Y. Zhu, N. Kang, J. Cao, A. Greenberg, G. Lu, R. Mahajan, D. Maltz, L. Yuan,
M. Zhang, B. Y. Zhao, and H. Zheng. Packet-level telemetry in large datacenter
networks.
In Proceedings of the ACM SIGCOMM Conference, London, United
Kingdom, 2015.