[21] Vincent François-Lavet, Peter Henderson, Riashat Islam, Marc G Belle-
mare, Joelle Pineau, et al. An introduction to deep reinforcement learning.
Foundations and Trends® in Machine Learning, 11(3-4):219–354, 2018.
Juan Garay, Aggelos Kiayias, and Nikos Leonardos. The Bitcoin
backbone protocol: Analysis and applications. In Annual International
Conference on the Theory and Applications of Cryptographic Techniques,
pages 281–310. Springer, 2015.
[22]
14
[23] Guy Goren and Alexander Spiegelman. Mind the mining. In Proceedings
of the 2019 ACM Conference on Economics and Computation, EC ’19,
pages 475–487, New York, NY, USA, 2019. ACM.
[24] Cyril Grunspan and Ricardo Pérez-Marco. Selﬁsh mining in Ethereum.
arXiv preprint arXiv:1904.13330, 2019.
[25] Alireza Toroghi Haghighat and Mehdi Shajari. Block withholding game
among Bitcoin mining pools. FGCS, 97:482–491, 2019.
[26] X. He, H. Dai, and P. Ning. Improving learning and adaptation in security
games by exploiting information asymmetry. In 2015 IEEE Conference
on Computer Communications (INFOCOM), pages 1787–1795, 2015.
[27] Ethan Heilman, Alison Kendler, Aviv Zohar, and Sharon Goldberg.
Eclipse attacks on Bitcoin’s peer-to-peer network. In USENIX Security,
pages 129–144, 2015.
[28] Benjamin Johnson, Aron Laszka, Jens Grossklags, Marie Vasek, and
Tyler Moore. Game-theoretic analysis of DDoS attacks against Bitcoin
mining pools. In FC, pages 72–86. Springer, 2014.
[30]
[29] Aljosha Judmayer, Nicholas Stifter, Alexei Zamyatin, Itay Tsabary, Ittay
Eyal, Peter Gaži, Sarah Meiklejohn, and Edgar Weippl. Pay-to-win:
Incentive attacks on proof-of-work cryptocurrencies. 2019.
Igor Kabashkin. Risk modelling of blockchain ecosystem. In NSS, pages
59–70. Springer, 2017.
Ido Kaiser. A decentralized private marketplace. 2017.
[31]
[32] Tamás Király and Lilla Lomoschitz. Proﬁtability of the coin-hopping
strategy. EGRES quick proof, (2018-03), 2018.
[33] Richard Klíma, Karl Tuyls, and Frans A. Oliehoek. Markov security
games : Learning in spatial security problems. 2016.
[34] Vijay R Konda and John N Tsitsiklis. Actor-critic algorithms.
NeurIPS, pages 1008–1014, 2000.
In
[35] Yujin Kwon, Dohyun Kim, Yunmok Son, Eugene Vasserman, and
Yongdae Kim. Be selﬁsh and avoid dilemmas: Fork after withholding
(faw) attacks on bitcoin. In CCS, pages 195–209, 2017.
[36] Aron Laszka, Benjamin Johnson, and Jens Grossklags. When Bitcoin
mining pools run dry. In FC, pages 63–77. Springer, 2015.
[37] Chenxing Li, Peilun Li, Dong Zhou, Wei Xu, Fan Long, and Andrew
Yao. Scaling Nakamoto consensus to thousands of transactions per
second. arXiv preprint arXiv:1805.03870, 2018.
Jun Li, Bodong Zhao, and Chao Zhang. Fuzzing: a survey. Cybersecurity,
1(1):6, 2018.
[38]
[39] Eric Liang, Richard Liaw, Philipp Moritz, Robert Nishihara, Roy Fox,
Ken Goldberg, Joseph E. Gonzalez, Michael I. Jordan, and Ion Stoica.
RLlib: Abstractions for Distributed Reinforcement Learning. arXiv
e-prints, page arXiv:1712.09381, Dec 2017.
mining: Generalizing selﬁsh mining and combining with an eclipse
attack. In EuroS&P, pages 305–320. IEEE, 2016.
[49] Michael Neuder, Daniel J Moroz, Rithvik Rao, and David C Parkes.
Selﬁsh behavior in the tezos proof-of-stake protocol. arXiv preprint
arXiv:1912.02954, 2019.
[50] Thanh Thi Nguyen and Vijay Janapa Reddi. Deep reinforcement learning
for cyber security. arXiv preprint arXiv:1906.05799, 2019.
Jianyu Niu and Chen Feng. Selﬁsh mining in Ethereum. arXiv preprint
arXiv:1901.04620, 2019.
[51]
[52] OpenAI. OpenAI Five Defeats Dota 2 World Champions, 2019. https:
//openai.com/blog/openai-ﬁve-defeats-dota-2-world-champions/.
[53] Rafael Pass and Elaine Shi. Fruitchains: A fair blockchain. In PODC,
pages 315–324. ACM, 2017.
[54] Fabian Ritz and Alf Zugenmaier. The impact of uncle rewards on selﬁsh
mining in Ethereum. In EuroS&PW, pages 50–57. IEEE, 2018.
[55] Meni Rosenfeld. Analysis of Bitcoin pooled mining reward systems.
arXiv preprint arXiv:1112.4980, 2011.
[56] Ayelet Sapirshtein, Yonatan Sompolinsky, and Aviv Zohar. Optimal
selﬁsh mining strategies in Bitcoin. In FC, pages 515–532. Springer,
2016.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg
Klimov. Proximal Policy Optimization Algorithms. arXiv e-prints, page
arXiv:1707.06347, Jul 2017.
[57]
[58] Sailik Sengupta and Subbarao Kambhampati. Multi-agent reinforcement
learning in bayesian stackelberg markov games for adaptive moving
target defense. arXiv preprint arXiv:2007.10457, 2020.
[59] Sajjan Shiva, Sankardas Roy, and Dipankar Dasgupta. Game theory for
cyber security. In CSIIRW, page 34. ACM, 2010.
[60] David Silver, Aja Huang, Chris J Maddison, Arthur Guez, Laurent Sifre,
George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou,
Veda Panneershelvam, Marc Lanctot, et al. Mastering the game of Go
with deep neural networks and tree search. Nature, 529(7587):484, 2016.
[61] David Silver, Thomas Hubert, Julian Schrittwieser, Ioannis Antonoglou,
Matthew Lai, Arthur Guez, Marc Lanctot, Laurent Sifre, Dharshan
Kumaran, Thore Graepel, et al. Mastering chess and shogi by self-
play with a general reinforcement learning algorithm. arXiv preprint
arXiv:1712.01815, 2017.
[62] Atul Singh et al. Eclipse attacks on overlay networks: Threats and
defenses. In IEEE INFOCOM. Citeseer, 2006.
[63] Emil Sit and Robert Morris. Security considerations for peer-to-peer
distributed hash tables. In IPTPS, pages 261–269. Springer, 2002.
[64] Yonatan Sompolinsky and Aviv Zohar. Secure high-rate transaction
processing in Bitcoin. In FC, pages 507–527. Springer, 2015.
Joe
cryptocurrency
https://www.secureworks.com/research/bgp-hijacking-for-
cryptocurrency-proﬁt, Aug 2014.
hijacking
Stewart.
BGP
for
proﬁt.
[40] Kevin Liao and Jonathan Katz. Incentivizing blockchain forks via whale
transactions. In FC, pages 264–279. Springer, 2017.
[65]
[41] Michael L Littman. Markov games as a framework for multi-agent
reinforcement learning. In Machine learning proceedings 1994, pages
157–163. Elsevier, 1994.
[42] Loi Luu, Ratul Saha, Inian Parameshwaran, Prateek Saxena, and Aquinas
Hobor. On power splitting games in distributed computation: The case
of Bitcoin pooled mining. In CSF, pages 397–411. IEEE, 2015.
[43] Pasin Manurangsi, Akshayaram Srinivasan, and Prashant Nalini Va-
sudevan. Nearly optimal robust secret sharing against rushing adver-
saries. Cryptology ePrint Archive, Report 2019/1131, 2019. https:
//eprint.iacr.org/2019/1131.
[44] Yuval Marcus, Ethan Heilman, and Sharon Goldberg. Low-resource
eclipse attacks on Ethereum’s peer-to-peer network. IACR Cryptology
ePrint Archive, 2018:236, 2018.
[45] Francisco J. Marmolejo-Cossío, Eric Brigham, Benjamin Sela, and
Jonathan Katz. Competing (semi)-selﬁsh miners in Bitcoin. arXiv
e-prints, page arXiv:1906.04502, Jun 2019.
[46] Patrick McCorry, Alexander Hicks, and Sarah Meiklejohn.
Smart
contracts for bribing miners. In FC, pages 3–18. Springer, 2018.
[47] Dmitry Meshkov, Alexander Chepurnoy, and Marc Jansen. Short paper:
Revisiting difﬁculty control for blockchain systems. In Data Privacy
Management, Cryptocurrencies and Blockchain Technology, pages 429–
436. Springer, 2017.
[48] Kartik Nayak, Srijan Kumar, Andrew Miller, and Elaine Shi. Stubborn
15
[66] Richard S Sutton, David A McAllester, Satinder P Singh, and Yishay
Mansour. Policy gradient methods for reinforcement learning with
function approximation. In NeurIPS, pages 1057–1063, 2000.
[67] Csaba Szepesvári. Algorithms for Reinforcement Learning, volume 4.
01 2010.
[68] Lylian Teng. F2Pool founder condemns block withholding attacks
performed by some chinese mining pools on its competitors.
https://news.8btc.com/f2pool-founder-condemns-block-withholding-
attacks-performed-by-some-chinese-mining-pools-on-its-competitors,
June 2019.
Jason Teutsch, Sanjay Jain, and Prateek Saxena. When cryptocurrencies
mine their own business. In FC, pages 499–514. Springer, 2016.
[69]
[70] Hado van Hasselt, Arthur Guez, and David Silver. Deep reinforcement
learning with double Q-learning. arXiv e-prints, page arXiv:1509.06461,
Sep 2015.
[71] Marie Vasek, Micah Thornton, and Tyler Moore. Empirical analysis of
denial-of-service attacks in the Bitcoin ecosystem. In FC, pages 57–71.
Springer, 2014.
[72] Pavel Vasin. Blackcoin’s proof-of-stake protocol v2. https://blackcoin.co/
blackcoin-pos-protocol-v2-whitepaper.pdf, 2014.
[73] Yufei Wang, Zheyuan Ryan Shi, Lantao Yu, Yi Wu, Rohit Singh, Lucas
Joppa, and Fei Fang. Deep reinforcement learning for green security
games with real-time information. In AAAI, volume 33, pages 1401–1408,
2019.
[74] Christopher JCH Watkins and Peter Dayan. Q-learning. Machine
learning, 8(3-4):279–292, 1992.
[75] CC White. Markov decision processes. Springer, 2001.
[76] Ronald J Williams. Simple statistical gradient-following algorithms for
connectionist reinforcement learning. Machine learning, 8(3-4):229–256,
1992.
[77] Gavin Wood.
framework.
Polkadot: Vision for a heterogeneous multi-chain
[78] Gavin Wood. Ethereum: A secure decentralised generalised transaction
ledger. Ethereum project yellow paper, 151(2014):1–32, 2014.
[79] Yuhuai Wu, Elman Mansimov, Roger B Grosse, Shun Liao, and Jimmy
Ba. Scalable trust-region method for deep reinforcement learning using
kronecker-factored approximation. In NeurIPS, pages 5279–5288, 2017.
[80] Lantao Yu, Yi Wu, Rohit Singh, Lucas Joppa, and Fei Fang. Deep
reinforcement learning for green security game with online information.
In AAAI, 2018.
[81] Kaiqing Zhang, Zhuoran Yang, and Tamer Ba¸sar. Multi-agent reinforce-
ment learning: A selective overview of theories and algorithms. arXiv
preprint arXiv:1911.10635, 2019.
A. Relative vs. Exact Rewards
APPENDIX
The difﬁculty of a block is the average computation required
to mine it. If the computational power of the whole network
increases or decreases, the block difﬁculty increases or decreases
accordingly to maintain a target block generation time under
dynamic network conditions. For instance, in Bitcoin, the target
main chain block generation rate, a.k.a. growth rate, is T0 = 10
minutes/block, and the difﬁculty is adjusted every M = 2016
blocks; we call this duration an epoch. The difﬁculty d(cid:48) of one
next epoch is adjusted according to the current difﬁculty d and
the average main chain growth rate T of the current epoch,
such that d(cid:48) = d T0
T .
We next show that over n epochs with difﬁculty adjustment,
the absolute reward rate (scaled by a constant) and relative
reward are close for a strategic agent against an arbitrary number
of other agents. In what follows, we let Sa and So denote the
number of stale blocks mined by the adversary of interest and all
the other miners in a single epoch, respectively; a stale block is
a block that does not end up on the main chain, and hence does
not collect any block reward. Note that the following result
relies on the deterministic analysis of [23], which abstracts
away the stochastics of the problem.
Proposition A.1. Let Rn denote the expected absolute reward
rate over n epochs, ˜Rn the expected relative reward rate, k
the number of independent actors (all honest parties can be
considered to be one actor), and T0 the target (expected) inter-
block time. Consider an attacker with α < 0.5 fraction of the
total hash power in the network. We assume that 1) the attacker
always uses all its mining power, 2) the attacker uses the same
strategy across all epochs, and 3) the total hash power of
the network remains unchanged across epochs. Then under a
deterministic analysis model, Rn = 1
, which
T0
in turn implies that |T0Rn − ˜Rn| ≤ k−1
n .
Ba
(Ba+Bo)+ 1
n (Sa+So)
the denominator should be minimized. This can be achieved by
producing no stale blocks, which occurs under honest mining.
Moreover, since maximizing absolute rewards is equivalent
to maximizing relative rewards scaled by a positive constant
(T0), Proposition A.1 suggests that for moderate n, the objective
functions for optimizing relative rewards and absolute reward
rate are close. We have that limn→∞ Rn =
, i.e., in the inﬁnite-
time horizon, optimizing absolute reward rate is equivalent to
optimizing relative rewards. SquirRL can be used to optimize
both; to compare with prior literature and because of this
asymptotic equivalence, we focus on relative rewards.
˜Rn
T0
1) Proof of Proposition A.1: We use the deterministic
analysis of [23], which deals entirely with expectations and
abstracts away random block times; it is a good approximation
when the epoch duration is high (as in Bitcoin). Let the number
of agents in the blockchain be k. We denote the number of main
chain blocks generated by the attacker during the ith epoch
as Ba(i) (assuming 1 ≤ i ≤ n) and the number of stale blocks
generated by the attacker as Sa(i); we suppress the notation p
for simplicity. A stale block is any block that does not end up on
the main chain. Those numbers for all other parties combined
are Bo(i) and So(i). We have Ba(n) + Bo(n) = M,n ∈ N+ since
every epoch has M blocks in the main chain.
In the ﬁrst epoch, the average block generation time is T0,
but the main chain growth rate may be lower than 1/T0 if the
attacker deviates from the honest mining protocol. Therefore, the
total duration of the ﬁrst epoch is D1 = (M + Sa(1) + So(1))T0.
After the ﬁrst difﬁculty adjustment, the difﬁculty will be multi-
plied by M/(M + Sa(1) + So(1)), so the expected duration of
the second epoch is D2 = (M +Sa(2)+So(2))T0
(M+Sa(1)+So(1)) .
We assume all parties repeat their strategies for all epochs, so
we have Ba(1) = Ba(2),Bo(1) = Bo(2),Sa(1) = Sa(2),So(1) =
So(2) under deterministic analysis [23]. We use the simpliﬁed
notation Ba,Bo,Sa,So and suppress notation n. Therefore, the
total time for the second epoch is actually MT0. This pattern
holds for larger n by induction. We can therefore write the
absolute reward rate of the attacker for n epochs Rn as follows:
M
Rn =
nBa
(M + Sa + So)T0 + (n− 1)MT0
.
=