ent variants of Medusa, ZombieLoad [52] and RIDL [58]
in Appendix B. We observed that only Variant 2 of Zom-
bieLoad [52] exploits TAA by actively inducing asyn-
chronous aborts as shown by the high number of the
tx_mem.abort_conflict counter. Other variants that use
TSX for exception suppression only show synchronous aborts
and hence do not exploit TAA. However, in the rare case
that an unrelated event, such as an interrupt or cache eviction,
asynchronously aborts the transaction during the load, these
variants could also trigger TAA.
3.4.2 AMD
Exception Bypass. One of the requirements for Meltdown-
type attacks is to bypass exceptions in an out-of-order fashion.
The results from Transynther suggest that the AMD Zen mi-
croarchitecture might potentially be vulnerable to Meltdown-
type attacks. We found that various exceptions, such as divi-
sion by zero, an aligned vector store general-purpose excep-
tion, as well as a faulting store to a supervisor address, do
not stop the out-of-order execution. In line with the AMD
whitepaper [3], some of the exceptions are bypassed specula-
tively. Hence, an important requirement for Meltdown is also
present on AMD CPUs, the forwarding of data from faulting
instructions. CPUs immune to Meltdown-type attacks have
to ensure that operations depending on a faulting instruction
cannot get the transient data, e.g., by stalling. While AMD en-
sures that for page faults, they do not ensure that property for
other faulting instructions, e.g., General Protection Memory
Access (cf. AMD whitepaper [3], page 5). While we could
not show data leakage that violates a security guarantee, e.g.,
leakage from the kernel, AMD is not by-design immune to
the root cause of Meltdown-type attacks.
Vector Move Alignment Fault. We also observed that the
faulty vector alignment exceptions are handled differently
than other faulty loads. In particular, these exceptions do not
block the data ﬂow, and we observe that the pipeline will
still speculatively consume the data despite the exception. We
observe that the value of the memory page or the value that
is written recently to the memory page will be leaked using
a Meltdown-style gadget. Again, this does not violate any
architectural data ﬂow, but from a microarchitectural stand-
point, it shows that computation over transient data that was
not supposed to be available is feasible.
3.5 Meltdown Root Cause Generalisation
From the vast amount of results generated by Transynther,
we can generalize the common root cause of known Melt-
down attacks. As stated by Canella et al. [11], the leakage
for all known Meltdown attacks is caused by a faulting load,
where microcode assists are considered as microarchitectural
faults [52]. In all attacks, we see the same behavior, that the
faulting load does not stall and thus cannot simply return no
data. As a consequence, the faulting load transiently returns
data that can be accessed immediately and where at least parts
of the address match.
The microarchitectural element from which an attack leaks
depends on the microarchitectural implementation of data-
forwarding checks, and where the fault occurs. For example,
ZombieLoad and Fallout exploit the same fault as the orig-
inal Meltdown attack, and RIDL exploits the same fault as
Foreshadow. In the case of RIDL and Foreshadow, it is the
cleared present bit in the page-table entry of the load target.
In the case that the L1 cache contains data with an address
that matches the page-frame number, the load simply takes
this value. This case is known as Foreshadow or Meltdown-P-
L1 [11]. If this is not the case, e.g., because the page-frame
number is 0 in the case of a NULL-pointer, the next possibil-
ity for data with partial address matches is the line-ﬁll buffer.
This case is known as RIDL or Meltdown-P-LFB [11]. Sim-
ilarly, for Meltdown, ZombieLoad, and Foreshadow, where
the user-accessible-bit in the page-table entry is exploited.
First, the store buffer is checked in parallel with the L1 data
cache. If a store-buffer entry has a partial address match, the
faulting load consumes this data, which is known as Fallout or
Meltdown-US-SB [10]. Otherwise, if the cache can provide
data with partially matching addresses, this is considered as
Meltdown-US [11]. In case the L1 cache cannot satisfy the
request due to a cache miss or a cache-line conﬂict, the line-
ﬁll buffer can provide the data, resulting in ZombieLoad [52]
or Meltdown-US-LFB [11].
Hence, one of the insights from Transynther is that the type
of the fault is less important than where the fault occurs, i.e.,
which microarchitectural element is the “closest” to the fault
from which the faulting load can consume data.
4 Medusa: Pre-ﬁltering Data
In this section, we further evaluate a novel ZombieLoad vari-
ant, which we discovered using Transynther. First, we show
that Medusa allows preﬁltering leaked values. Medusa only
leaks values used in implicit WC by exploiting the microarchi-
tectural implementation of the WC buffer. Second, we show
3 different variants of Medusa, which each have unique prop-
erties. Finally, we analyze potential attack targets for Medusa
based on where implicit WC is used in real-world software.
1434    29th USENIX Security Symposium
USENIX Association
4.1 Leakage Analysis
To evaluate the practicality of Medusa, we ﬁrst analyze the
leakage of Medusa. This includes the source of the leakage as
well as the leakage pattern, i.e., how much control an attacker
has over the leakage and how much noise is in the leaked
data. We ﬁrst reduced the generated snippet, i.e., we removed
instructions as long as the leakage was still visible.
4.1.1 Leakage Source
For the leakage source, Transynther already provides an ed-
ucated guess that the leakage source of the snippet is the
ﬁll buffer. For Medusa, Transynther reports a Pearson coefﬁ-
cient of rp = 0.99 for the ﬁll buffer, while the correlation for
the other performance counters is not statistically signiﬁcant.
However, the only leaked value is the character written with
rep stosb. Hence, in contrast to ZombieLoad [52], Medusa
can only leak from a part of the line-ﬁll buffer.
We additionally verify that using the publicly available
proof-of-concept for ZombieLoad. Using this victim, we do
not see any leakage when using Medusa, while we see a strong
leakage when using the ZombieLoad attack. We also used the
public proof-of-concept for RIDL [58]. Interestingly, RIDL
only works when reading data after a ﬂush and a memory
barrier. If either the ﬂush or the memory barrier (i.e., cpuid
or mfence) is missing, we do not get any leakage.
In Table 3, we compare different victims and whether dif-
ferent variant of MDS attacks (ZombieLoad, RIDL, Fallout)
or Medusa can leak data from these victims. While data larger
than 128 bits, e.g., rep mov, can also be leaked with Zom-
bieLoad (same and cross hyperthread) or Fallout (same hyper-
thread), Medusa only leaks data larger than 128 bits. Hence,
while Medusa does not exploit any new data source, it targets
exactly one type of victim, and there is no unrelated data from
other processes.
WC and Fill Buffer. According to Intel, their microarchitec-
tures use the line-ﬁll buffer as WC buffers [23]. Thus, ofﬁ-
cially, 10 line-ﬁll-buffer entries can be used for WC [27].
Schwarz et al. [52] experimentally veriﬁed this for pre-
Skylake microarchitectures but detected 12 line-ﬁll-buffer
entries since Skylake. We devised several experiments to an-
alyze the WC-behavior of the line-ﬁll buffer for all memory
types supported on x86_64 (cf. Appendix A).
Implicit WC. While there is an explicit WC memory type,
there are certain instructions that always use WC independent
of the underlying memory type, e.g., non-temporal stores. De-
pending on the CPU, Intel also documents that non-temporal
loads (MOVNTDQA) may reduce the number of cache evictions
by leveraging the WC buffer [27]. Recent Intel CPUs support
fast-string operations via the rep mov and rep stos instruc-
tions [26, 27]. These instructions do not guarantee any order
of the written data [27]. Hence, they can employ WC to re-
duce the number of write requests sent on the memory bus.
t
n
u
o
C
103
102
101
0
50
100
150
Byte offset
200
250
Figure 3: Leaking values with Medusa when copying a 256-
byte buffer using rep mov shows an interesting pattern. While
all bytes can be leaked, certain offsets in the buffer have a
much higher probability of being leaked.
We veriﬁed that with Medusa, we can leak the values both
for explicit WC, i.e., memory marked as WC, as well as im-
plicit WC, i.e., MOVNTDQA, rep mov, and rep stos. Hence,
Medusa has the unique property among all MDS attacks that
the leakage is ﬁltered by instruction types, i.e., the amount of
unrelated data is signiﬁcantly less than in other attacks.
4.1.2 Leakage Pattern
Figure 3 shows the leakage pattern for Medusa when copying
a 256-byte buffer in the victim application using rep mov
over the time of 10 s. It can be seen that while not all offsets
in a 256-byte window can be leaked with the same frequency,
all offsets can be leaked. For the victim, we use a de Bruijn
sequence of order 3 on an alphabet of size 26, i.e., B(26,3),
to groom the WC buffer (cf. Section 3.3). We constantly
write this sequence to a dummy location using rep mov. The
victim is running on the sibling logical core.
For the attacker, we always leak 3 bytes at a time by en-
coding every byte into a different array of 256 pages. As it
is possible to compute on the full leaked values in the tran-
sient domain [39, 52], we can leak a 32-bit value, split it, and
encode it to different arrays. The recovered 3-byte value can
then be matched to the de Bruijn sequence used in the victim
application. As the position of every 3-byte value within the
de Bruijn sequence is unique, this method allows us to ana-
lyze the pattern of the leaked values. Notably, we can always
see strides of values which occur often in the leaked data,
followed by strides which only occur rarely. Especially for
the beginning of the buffer, the probability for leaking the
ﬁrst 32 bytes (p =67 %, n =10 000) is signiﬁcantly higher
than for leaking the second 32 bytes (p =33 %, n =10 000).
We assume that the split of 32 B is due to the 32 B data-bus
size on our test machine (i7-8650U). Hence, to transmit a
WC-buffer entry over the common data bus, both halves of
the entry have to be transferred separately, and Medusa leaks
either the ﬁrst or second half. Data after the ﬁrst cache line
shows a different pattern. We can always see 16 B strides of
values that occur often in the leaked data, followed by 16 B
strides, which only occur rarely. Interestingly, this pattern
does neither correlate with the bus size, nor with the size of
the WC buffer. Moreover, the leakage rate increases after
USENIX Association
29th USENIX Security Symposium    1435
Table 3: A comparison of MDS attacks in various variants and on different targets.
With memory barrier
Without memory barrier
load
RIDL (ST)
-
ZombieLoad
ZombieLoad
-
ZombieLoad
store
RIDL
Fallout (ST)
ZombieLoad
ZombieLoad
Fallout (CL, ST)
ZombieLoad / Fallout
(ST)
ZombieLoad / RIDL /
Fallout (ST)
store
RIDL (ST)
Fallout (ST)
ZombieLoad
ZombieLoad
Fallout (CL, ST)
ZombieLoad / Fallout
(ST)
ZombieLoad / RIDL
(ST) / Fallout (ST)
Non-present Page Fault
load
-
-
ZombieLoad
ZombieLoad
-
ZombieLoad
ZombieLoad
>128-bit data
store
-
Medusa / Fallout (ST)
Medusa / ZombieLoad
ZombieLoad
Fallout (CL, ST)
ZombieLoad / Fallout
(ST)
Medusa / ZombieLoad
/ Fallout (ST)
Supervisor Protection Fault
Access-bit Assist
load
RIDL
-
ZombieLoad
ZombieLoad
-
ZombieLoad
TAA
PTE inversion
Attack(s)
ZombieLoad /
RIDL
ST Same CPU thread only CL Coffee Lake only
ZombieLoad /
RIDL (ST)
Non-canonical Address Fault
]
%
[
d
e
k
a
e
L
6
4
2
0
0
20
Cache-line offset [B]
40
60
Figure 4: The cache-line offsets and how they contribute to
the leakage for Medusa Variant I.
the ﬁrst 64 B. At the time of writing, we do not know of any
way to analyze these effects further, and hence, we leave the
investigation of this effect for future work.
4.2 Exploitation Methodology
4.2.1 Variant I: Cache Indexing
We now describe different variants that allow triggering
Medusa. In the ﬁrst variant of Medusa, we rely on faulting
loads which are bounded within a cache line. Variant I ex-
ploits faulting loads on addresses that point inside a cache line
(cf. Figure 4) to leak values from the WC buffer. The setup is
similar to all Meltdown-type attacks, with a faulting load that
transiently encodes the loaded data into a microarchitectural
element. In contrast to existing attacks, the type of fault is not
important, but the cache-line offset of the faulting address is.
We veriﬁed Variant I with both non-canonical and supervisor
addresses. On our test machine, an i7-8650U, the cache-line
offset, i.e., the least-signiﬁcant 6 bits of the address, has to
be at least 8, which is the maximum size of normal memory
loads. However, the highest leakage rates are for offsets be-
tween 16 and 31. The common data bus has a width of 32
bytes. However, normal loads can only use up to 8, and AVX
loads 16 bytes (128 bits). As a consequence, offsets 16 to 31
are rarely used, as only AVX2 (256 bits) uses the full width
of the common data bus. However, as the goal of WC is to
increase the throughput, (implicit) WC also tries to leverage
the entire common data bus. Hence, by using address offsets
that index the upper half of the common data bus, Variant I
leaks stale values of recent WC operations, e.g., rep mov, as
well as AVX2 memory loads.
While at ﬁrst, Variant I appears to be similar to
MLPDS [24], ZombieLoad [52], or Fallout [10], it has dis-
tinctive properties. First, MLPDS requires either a faulting
load spanning a cache line (64 B) or a faulting vector load
that is larger than 64 bits [24]. For Variant I, neither of these
requirements is necessary. In contrast, Variant I only works
if the load is within one cache line. Loads spanning over two
cache lines do not show data leakage (cf. Figure 4). Second,
Variant I leaks data from the same logical core as well as from
the sibling logical core, which is different from Fallout [10].
The leakage is limited to data that is stored using either rep
mov, rep stos, or AVX2. In contrast to ZombieLoad or Fall-
out, Variant I of Medusa is agnostic to other data passing the
store buffer or ﬁll buffer, as they never use the upper half of
the common data bus.
4.2.2 Variant II: Unaligned Store-to-Load Forwarding
A faulting or assisting load that meets the “Unaligned Store-to-
Load Forwarding” condition (similar to MSBDS) consistently
leaks stale data. This was observed even across hyperthreads.
Note that this is different from MSBDS, as MSBDS does
not work across hyperthreads. Here, we can leak the data
from the WC buffer by creating an unaligned store-to-load
forwarding condition on a faulting or assisting load. Further,
an attacker can control which bytes of the WC buffer to leak
by combining various load sizes and the offset of the small
store. In our experiments, we can control the last 16 bytes of
a WC buffer line by combining a 32-byte read ’ymmX’ and
iterating over various values for the offset of the store.
4.2.3 Variant III: Shadow REP MOV
Variant III of Medusa exploits a microcode assist caused by a
rep mov followed by a dependent faulting load. The rep mov
copies a single dummy byte to a destination address which
causes a fault, e.g., a non-canonical address. A subsequent
load from the destination address leaks data from a stale or
concurrent rep mov. The rep mov can either be on the same
1436    29th USENIX Security Symposium
USENIX Association
Table 4: rep mov instruction within cryptographic libraries.
Library Version O0 O1 O2
Botan
Openssl
Wolfssl
Bearssl
Sodium
Gcrypt
2.11.0
1.1.1c
4.1.0
0.6
1.0.18
1.8.4
12
12
1
10
3
5
14
23
7
26
12
5
68
29
*49
45
*12
*7
O3
*137
*34
72
56
13
11
Os
188
347
199
*213
49
168
logical core before running Medusa which leaks stale data
of the previous rep mov. This also works across privilege
boundaries, i.e., the stale rep mov data can also be from the
kernel. Moreover, this attack also works for a concurrent rep
mov on the sibling logical core across privilege boundaries.
As with Variant I, this variant has the property to only leak
data of rep mov, rep stos, and AVX2 memory loads, which
allows a targeted leakage of data used in such constructs. In
contrast to Variant I, this variant is entirely address-agnostic,
which simpliﬁes the recording of the leakage. However, this
increases the complexity of the post-processing, as an attacker
does not have any control over the index of the leaked data.
Hence, as every byte of the victim buffer can be leaked with
a certain probability, the postprocessing has to stitch together
the leaked data, e.g., using the Domino technique [52].
4.3 WC in Real-World Software
We analyzed real-world software to ﬁnd occurrences of WC.
We looked both for explicit WC, i.e., WC memory deﬁned
through the PAT, as well as for implicit WC in the form of
rep mov and rep stos.
userspace. We ﬁrst searched for implicit WC, as userspace
applications cannot directly change the memory type of a
page. We analyzed when and how often GCC emits a rep mov
sequence during the compilation of popular cryptographic
libraries, as potential targets that process sensitive information.
As shown in Table 4, if GCC optimizes the application for
code size (-Os), it emits the most rep mov instructions as rep
mov is the smallest possible code sequence that can be used
to copy memory regions. Similarly, rep stos is the smallest
code sequence to initialize memory with a deﬁned value.
We also found the explicit use of WC memory types in the
userspace. Although implementation-speciﬁc, both OpenGL
and Vulkan support memory buffers, which are marked as WC.
Memory buffers allocated as write-only buffers are likely to
be allocated as WC memory by the driver.
Linux Kernel. The Linux kernel also relies on rep mov to
copy data. In contrast to user-space applications, the usage
of rep mov is not to optimize the kernel binary for size. It is
used independently of the used compilation ﬂags, as the kernel
generally does not use ﬂoating-point or SIMD operations.
Hence, rep mov is the most efﬁcient way to copy data. As
there is a small startup penalty when using rep mov, only
strings with a minimum length of 64 B are copied using rep