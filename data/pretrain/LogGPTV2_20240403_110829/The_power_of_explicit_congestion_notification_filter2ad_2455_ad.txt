HTTP Traffic, Model, C=100Mbps
Model, C=155Mbps
Model, C=622Mbps
Simulation, C=100Mbps
Simulation, C=155Mbps
Simulation, C=622Mbps
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
)
)
C
*
s
m
5
(
>
Q
P
(
0
0.75
0.8
0.85
0.9
Utilization
0.95
1
Figure 8: Probability that the queue delay exceeds 5 ms as a
function of the link load (web-trafﬁc scenario)
We denote the link capacity by C. Figure 8 depicts both model-
ing and simulation results for the probability that the queue length
exceeds the 5 ms∗C level. Figure 8 depicts this probability as a
function of the link utilization ρ, and for the link capacities of
100 Mbps, 155 Mbps, and 622 Mbps. In addition, we perform sim-
ulations on a FIFO queue for three different random seeds. Figure
8 shows a good match between modeling and simulations, with the
difference that in this scenario the model behaves as an upper bound
for the simulation results.
The key point from Figure 8 is that the probability that the queue
length exceeds the 5 ms∗C threshold is indeed very small, which,
based on the above discussion, indicates a similar impact of non-
ECN-based AQM mechanisms. As expected, this impact increases
for higher utilization levels, and decreases for higher link-speeds.
For example, Figure 8 shows that for C = 622 Mbps and ρ = 0.95,
the probability that the queue length exceeds the 5 ms∗C threshold
is smaller than 10%, indicating that the corresponding congestion
epochs are indeed very short. Nevertheless, AQM mechanisms are
still needed to control delay during these epochs, because a simple
FIFO queue lacks any such capabilities [23]. However, as indicated
in Figure 6, ECN-originated mechanisms, and much less sophis-
ticated AQM control mechanisms, are responsible for end-to-end
performance. Moreover, the use of ECN+ is of particular impor-
tance here, because it prevents unnecessary performance degrada-
tions (e.g., dropping SYN ACK packets) during short-lived conges-
tion periods.
5.2.3 Persistently Congested Links
Here, we increase the load to 105%. This means that the popula-
tion of web clients in this scenario increases such that they would
generate a load of 105 Mbps on a 1 Gbps link. Therefore, this cre-
ates a persistently-congested environment for a 100 Mbps link. We
show that the impact of ECN+ on web response times increases
in such scenarios, while the two schemes (threshold-based and PI)
have approximately the same throughput. Below, we explain the
origins of such a behavior.
Our results (not shown due to space constraints, see reference
[22] for more details) reveal that the threshold-based scheme with
ECN+ outperforms PI without ECN by even a larger margin than
in the above lightly-congested scenario. This is because marking,
instead of dropping packets in this scenario has an even larger im-
pact on end-to-end performance. This is particularly true for SYN
ACK packets, which are marked in the case of ECN+. However,
a more interesting result is the impact of both schemes on normal-
ized throughput.
It is 99.89% in the PI case, while it is 97.37%
for the ECN+-enabled threshold scheme. While PI’s control mech-
anisms are indeed developed for, and obviously perform well in,
persistently-congested scenarios, the surprising result is the high
throughput achieved by the threshold-based scheme. This is despite
the fact that it lacks both generic anti-randomization mechanisms
as well as more advanced control mechanisms. Below, we explain
this phenomenon in more detail.
The key reasons for the high throughput achieved by the
threshold-based scheme with ECN+ are the following. First, while
dropping all packets when the instantaneous queue length exceeds a
given threshold can have devastating effects on TCP’s performance,
this is not necessarily the case when ECN is supported. This is be-
cause ECN-enabled TCP endpoints react to the event of multiple
marked packets within an RTT the same as if a single packet was
dropped [30]. Thus, the impact on throughput is not dramatic. Sec-
ond, even though short ﬂows carry only 20% of the bytes in our
scenario, the fact that SYN ACK packets are not dropped has pos-
itive impact on throughput. However, the key reason for the good
performance of this generic scheme is an obvious lack of synchro-
nization among longer-lived ﬂows.
Synchronization of TCP ﬂows was one of the motivations for
RED [16]. The main goal of RED is avoiding the synchroniza-
tion of many TCP ﬂows that decrease their window at the same
time, and thus degrade the system throughput. The key reasons for
the absence of synchronization in our scenario, despite the lack of
randomization mechanisms, are the following. First, while we do
generate long ﬂows in our simulation (according to the ﬁle size dis-
tribution reported in [23, 33]), these ﬂows are of ﬁnite size. Thus,
they are downloaded in ﬁnite time, which can sometimes not be
long enough to allow synchronization. Second, the fact that TCP
ﬂows are limited by Wmax additionally decreases the probabil-
ity that synchronization will arise. Next, heterogeneous round-trip
times may also weaken these effects. Finally, in large aggregation
regimes, non-synchronized greedy short-RTT TCP ﬂows are able to
quickly ﬁll in “gaps” induced by possibly synchronized TCP sub-
aggregates.
5.3 General Trafﬁc Mixes
So far, both modeling and simulation results are based on the
trace from [23, 33], which accurately represents web-trafﬁc scenar-
ios. Here, we extend our analysis to general trafﬁc mixes, which
are not limited to only web trafﬁc.
We make a brief survey of recently reported measurements of
general ﬂow-size distributions, and ﬁnd two such representatives.
The ﬁrst is reported by Garetto et al.
in [17]; the distribution is
obtained from measurements taken on an access link of a campus
network; the second distribution is reported by Campos et al.
in
[9]; it is obtained from measurements on an OC-48 link between
Indianapolis and Cleveland, and the trace is publicly available at
http:// pma.nlan r.net/Traces/ long/ ipls1.html. While both distribu-
tions have “heavier” tails than the above web-based distribution,
such that the percentage of bytes that belong to long-lived ﬂows be-
comes larger, only the second trace (from the OC-48 link) reveals
somewhat different trends for the impact of ECN+ and non-ECN-
based AQM mechanisms than reported above. Below, we present
those results, both for lightly and persistently congested scenarios.
5.3.1 Lightly Congested Links
Here, we repeat the simulations for the lightly congested sce-
nario by using the ﬁle-size distribution obtained from the above
OC-48 trace. It is important to understand that we do not simply
plug the trace into our simulator. Instead, we use the ﬁle-size dis-
tribution which corresponds to this trace, and generate inter-arrival
times in simulations to achieve 90% load on a 100 Mbps link.
The response-time proﬁles (not shown due to space constraints)
for the two AQM schemes are similar to that of Figure 6, which
conﬁrms the dominant impact of ECN+ on web response-time per-
formance. This is because the majority of ﬂows in the experiment
are still short-lived, even though long ﬂows carry approximately
90% of the bytes in this scenario. Hence, a heavier ﬂow-size-
distribution tail has no impact on web response-time performance.
On the contrary, due to lack of any randomization or any other con-
trol mechanisms, the throughput of the threshold-based AQM starts
to lag behind PI’s more rapidly: it is 76.81% in the threshold-based
AQM case, and 80.92% for PI.
HTTP + longer-size Traffic, C=100Mbps
C=155Mbps
C=622Mbps
Simulation, C=100Mbps
Simulation, C=155Mbps
Simulation, C=622Mbps
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
)
)
C
*
s
m
5
(
>
Q
P
(
0
0.75
0.8
0.85
0.9
Utilization
0.95
1
Figure 9: Probability that the queue delay exceeds 5 ms as a
function of the link load (general trafﬁc scenario)
To further understand the above behavior, we re-apply our mod-
eling procedure and obtain the queue-size distribution that corre-
sponds to the above general ﬁle-size distribution. Figure 9 depicts
the probability that the queue length exceeds the 5 ms∗C threshold
typically used in AQM algorithms. The y-axis in Figure 9 indi-
rectly measures the “relevance” of non-ECN-based AQM mecha-
nisms (PI’s in this scenario). When compared to Figure 8, Figure 9
indicates longer queuing lengths, particularly for the 100 Mbps and
155 Mbps scenarios. For example, for 90% load on a 100 Mbps link
(exactly our scenario here), the probability that the queue length ex-
ceeds the targeted AQM threshold is larger than 0.5. This indicates
more persistent congestion levels, which invoke PI’s control mech-
anisms.
On the contrary, threshold-based AQM, despite ECN+ support,
lacks basic control mechanisms, and experiences moderate through-
put degradations. While it is well known that non-ECN-based AQM
control mechanisms are required to achieve high throughput in per-
sistently congested environments dominated by long-lived trafﬁc
ﬂows, our results indicate that such mechanisms are required even
for more moderate congestion levels. However, as the link speed
increases, Figure 9 shows that despite high utilization levels, the
queuing lengths are not as persistent. For example, for C =
622 Mbps and 95% utilization,
the queuing lengths are light,
while for 90% they are almost non-existent despite heavier ﬁle-
size-distribution tail. Thus, our previous analysis indicates that the
generic ECN+ scheme would work well in such scenarios.
5.3.2 Persistently Congested Links
Finally, we re-create the persistently congested scenario with
105% load on a 100 Mbps link, with the same ﬂow-size distribu-
tion as above. The response-time proﬁle (not shown due to space
constraints) again conﬁrms the dominant impact of ECN+ on end-
to-end performance. However, the threshold-based scheme does
not keep pace with PI in throughput: threshold-based AQM has a
normalized throughput of 88.43%, whereas PI achieves 96.48%.
As discussed above, a larger percentage of long-lived ﬂows in-
creases the probability of ﬂow-synchronization, which in turn
causes throughput degradation.
6.
INCREMENTAL DEPLOYABILITY
In this section, we treat the problem of incrementally deploy-
ing ECN in the Internet. Given that it is impossible to force the
entire Internet community to simultaneously apply ECN, the ques-
tion is how ECN- and non-ECN-enabled trafﬁc streams affect each
other when they are multiplexed. To the best of our knowledge,
this issue has not yet been explored. The key problem with adding
any new functionality in the Internet is to fulﬁll the two follow-
ing, often contradictory, requirements: (i) to be “friendly” to the
endpoints that do not apply the innovation; and concurrently (ii)
achieve performance improvements, which are necessary to pro-
vide a reasonable incentive for endpoints to apply the innovation
in the ﬁrst place. While it is well-known that ECN achieves the
ﬁrst feature, we show below that ECN+ (implemented at servers)
successfully adds the second.
To become effective, ECN needs to be applied at clients, servers,
and the bottleneck router in between. Below, we assume ECN sup-
port at the congestion router and ECN+ support at servers, and
we control the percentage of ECN ﬂows at the router by changing
the number of ECN-enabled clients. The same proportion of ECN
ﬂows in the system (and the same effects as reported below) could
be achieved by assuming ECN support at clients and the congested
router, and then varying the percentage of ECN+-enabled servers.
Figure 10 depicts the response-time proﬁles for different levels
of ECN deployability in the web-based simulation scenario with
server and client pools. We set all the machines in the server-pool
)
%
(
y
t
i
l
i
b
a
b
o
r
p
e
v
i
t
l
a
u
m
u
C
100
90
80
70
60
50
40
30
20
10
0
Uncongested network
RED* no ECN, P(ECN) = 5%
RED* with ECN+, P(ECN) = 5%
RED* no ECN, P(ECN) = 50%
RED* with ECN+, P(ECN) = 50%
RED* no ECN, P(ECN) = 95%
RED* with ECN+, P(ECN) = 95%
0
0.2 0.4 0.6 0.8
1
1.2 1.4 1.6 1.8
2
Response Time (ms)
Figure 10: Incremental deployability, 98% load
to support ECN+ and initially only 5% of the clients support ECN.
Figure 10 shows that even the small percentage of ECN-enabled
clients manage to signiﬁcantly improve their response times. This
is of particular importance because it provides a reasonable incen-
tive for clients to apply ECN; by doing so, they can achieve sig-
niﬁcant performance improvements instantly, without waiting for
other clients to support the option.
Next, we increase the percentage of ECN-enabled clients to 50%.
Figure 10 shows that ECN-enabled clients still achieve nearly ideal
performance. At the same time, the performance of non-ECN-
enabled clients slightly degrades when compared to the previous
scenario. This degradation occurs because a larger percentage of
ECN-enabled ﬂows better utilize the available bandwidth in this
scenario and keep the average queuing length closer to RED’s
maxth parameter; this causes a larger number of SYN ACK pack-
ets belonging to non-ECN-enabled ﬂows to be more frequently
dropped at the router. However, Figure 10 indicates that the degra-
dation is not signiﬁcant. Thus, while the performance improve-
ments are instant for the clients that apply ECN, the degradation of
non-ECN-enabled clients is gradual, which is a desirable property
that we discuss in more detail below.
Finally, we increase the percentage of ECN-enabled clients to
95%. The response-time proﬁle of such clients is slightly degraded
when compared to the previous scenario; this is because the sys-
tem throughput increases in scenarios with high ECN deployment,
as we discuss in detail in the following section. In addition, the
degradation of the small number (5%) of non-ECN-enabled ﬂows
is now more pronounced. This is because such ﬂows experience
the “TCP admission control” problem (explained in detail in Sec-
tion 3.1) which they can solve by applying ECN.
7. TESTBED EXPERIMENTS
Here, we perform a set of testbed experiments with the goal of
verifying the above ﬁndings in a real system. The testbed con-
sists of a cluster of Intel Pentium IV 2.0 GHz machines running
Linux 2.4.18-14, with 512 MB SDRAM, and a 30 GB ATA-66 disk
drive. One of the machines is conﬁgured as a router and runs Nist-
net [2], an IP-layer network emulation package. The router sepa-
rates the remaining machines into client and server pools. We use
Nistnet to vary the RTT between clients and servers in the range
from 10 to 150 ms in order to emulate a wide-area network en-
vironment. In addition, we limit the bandwidth between the two
pools to 100 Mbps, which represents an uncongested scenario, and
10 Mbps, which represents a congested scenario, as we explain in
detail below. This setup enables us to experiment with a version of
RED implemented at the router. As explained in Section 4.1, this
version, which is “hardwired” to the Linux kernel and that we de-
note by RED∗, marks all ECN-enabled packets when the average
queue length exceeds the maxth parameter. We set all of RED∗’s