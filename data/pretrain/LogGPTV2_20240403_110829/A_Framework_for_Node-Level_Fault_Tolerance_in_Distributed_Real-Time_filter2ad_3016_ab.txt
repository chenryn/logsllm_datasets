application 
independent).  Application  specific  techniques  utilize 
detailed  information  about  the  application  to  allow 
custom  designed  error  handling,  while  systematic 
techniques  rely  on  the  use  of  duplication  in  space  or 
time.  Systematic  techniques  generally  incur  higher 
overhead than application specific approaches, but are 
simpler to use for the system designer since they do not 
require  application  specific  knowledge.  Moreover, 
separating  the  error  handling  mechanisms  from  the 
application  generally  reduces  the  complexity  of  the 
system [12]. 
faults 
in  critical 
In  our  approach,  systematic  techniques  are  used  to 
tolerate 
tasks,  while  specific 
techniques,  such  as  assertions  and  range  checks,  are 
used to detect errors affecting the kernel. In this paper, 
we  do  not  discuss  the  error  handling  for  the  kernel 
further  -  an  experimental  study  of  a  prototype  kernel 
supporting  node-level  fault  tolerance  can  be  found  in 
[8].  Examples  of  error  handling  mechanisms  suitable 
for low-cost implementation of light-weight NLFT are 
given in Table 1. Note that we rely on a combination of 
hardware  and  software  error  handling  techniques  to 
achieve NLFT.  
Table 1. Examples of error handling suitable 
for implementing light-weight NLFT 
Hardware techniques 
CPU hardware exceptions 
Error correcting codes (ECC) 
Memory management unit 
(MMU) 
Software techniques 
provided by kernel  
Temporal error masking 
(TEM) 
Execution time monitoring 
Data integrity checks and  
end-to-end error detection 
Description 
CPU run-time error detection 
mechanisms 
Detects and corrects errors in 
memories 
Detects memory accesses outside 
the task's allowed memory area 
Description 
Detects and tolerates computation 
errors caused by, e.g. transient 
faults in data registers, adders or 
multipliers 
Detects timing violations for 
individual tasks 
Detects errors in internal data 
structures and errors in input and 
output data 
2.4  Hardware techniques and timing checks 
Current  state-of-the-art  COTS  microprocessors 
provide extensive error detection mechanisms (EDMs) 
such  as  illegal  op-code  detection,  address  range 
checking  and  error  correcting  codes  (ECC)  on 
memories  and  caches.  Often,  they  also  provide  a 
memory  management  unit  (MMU),  which  supports 
fault  confinement  between  tasks  or  between  tasks  and 
the  kernel.  This  simplifies  fault  tolerance,  as  we  only 
need  to  consider  recovery  of  the  affected  task.  To 
ensure that a task does not execute for too long, which 
may  prevent  other  tasks  from  executing,  an  execution 
time monitor may be used. For example, budget timers 
[2]  may  be  used  to  monitor  the  execution  time  of 
individual pre-emptive tasks. Such a mechanism allows 
the  action  taken  when  an  error  is  detected,  to  be 
decided  for  each  tasks,  e.g.  conduct  a  recovery  of  the 
affected task. 
2.5 Temporal error masking 
To support transparent error handling in a real-time 
kernel,  temporal  error  masking  (TEM)  is  used.  In 
TEM,  the  kernel  executes  all  critical  tasks  twice  and 
compares  the  results  in  order  to  detect  errors.  A  third 
execution  is  started  if  an  error  is  detected  by  the 
comparison or by any hardware or software EDM. This 
allows  the  kernel  to  mask  errors  by  conducting  a 
majority  vote  on  three  results.  To  ensure  that  the 
recovery  of  an  erroneous  task  does  not  lead  to  any 
deadline  violations,  sufficient  slack  must  be  provided 
in the schedule, see Section 2.8. 
Figure  2  shows  our  basic  model  for  a  critical  task, 
which  is  assumed  to  be  executed  in  a  periodic  read 
input - compute - write output loop. The input data are 
received  first  from  input  devices  or  from  other  tasks. 
The  input  data  are  then  processed  and  the  results  sent 
to actuators or to other tasks in the system at the end of 
the loop. 
Input data  
State data 
  loop 
      read input 
      compute 
      write output 
 end loop 
State 
 data 
Result data 
New state data 
Figure 2. Task model 
Figure 3 shows three different scenarios using TEM: 
in fault-free operation (i), a critical task, T, is executed 
two  times  (denoted  T1  and  T2)  and  a  comparison  is 
made  to  detect  errors.  As  the  results  match,  a  third 
copy does not have to be executed and the time may be 
used  by  other  tasks.  In  (ii)  an  error  is  detected  by  the 
comparison  and  a  third  copy  of  the  task,  T3,  is  then 
executed.  The  results  of  the  three  copies  are  checked 
by  a  majority  vote.  If  the  majority  voter  detects  two 
matching results, these are accepted as a valid result of 
the task. Otherwise, no result is delivered, which leads 
to an omission failure. 
In  (iii),  an  error 
is  detected  by  a  hardware 
mechanism  or  another  node  level  mechanism.  The 
affected  copy,  T2,  is  then  terminated  and  a  new  copy, 
T3, is started immediately. The new copy will use time 
reclaimed  from  the  terminated  copy  as  well  as  time 
from  any  available  slack.  A  comparison  is  made  to 
confirm  that  the  results  match  before  a  result  is 
delivered. For errors detected by CPU EDMs, the task's 
CPU  state  context,  e.g.  the  program  counter  (PC)  and 
stack  pointer  (SP)  etc.,  is  restored  to  the  initial 
conditions  from  information  stored  in  the  task  control 
block  in  the  kernel.  The  reason  for  restoring  the 
complete  context  is  that  errors  detected  by  hardware 
exceptions  often  originate  from  faults  in  the  CPU 
registers. For example, in [8] we showed that an illegal 
instruction exception may occur as a result of faults in 
the PC register and that address and bus exceptions are 
often triggered by faults in the SP register. When errors 
are  detected  by  the  comparison  of  a  task,  we  assume 
that  the  error  only  affects  the  data  computations;  new 
copies  can  therefore  be  started  without  restoring  the 
CPU  context.  Scenario  (iv)  is  similar  to  scenario  (iii), 
but the fault occurs in copy T1. 
T1 
T2 
Fault 
Comparison 
T2 
T3 
Fault 
T2 
Comparison 
T3 
T1 
T1 
Voting 
(i): Fault free execution 
(ii): An error is detected   
       by the comparison  
(iv): An error is detected 
        in T1 by a HW/SW EDM 
(iii): An error is detected 
        in T2 by a HW/SW EDM 
Error detected 
Fault 
T1 
T2 
T3 
Error detected 
Comparison 
Comparison 
Time 
Figure 3. Error detection and recovery using 
temporal error masking 
The  kernel  always  checks  the  deadline  of  the  task 
after  an  error  is  detected  to  determine  whether  it  is 
possible  to  execute  an  additional  task  copy  and  still 
meet  the  deadline.  Even  if  additional  time  is  reserved 
in  the  schedule  to  handle  recovery,  enough  time  may 
not  be  available,  e.g.  because  more  faults  than 
anticipated  occurs.  In  this  case,  no  result  is  delivered 
and  an  omission  failure  occurs.  If  time  is  available,  a 
new copy is started. The task result is delivered and the 
state data are only updated when two matching results 
have been produced. Errors that are repeated for some 
time are considered to be caused by permanent  faults. 
In  this  case,  the  node  is  shut  down  for  off-line 
diagnosis 
transient  or  a 
permanent  fault  caused  the  error.  For  transient  faults, 
the node may be re-integrated. 
to  establish  whether  a 
2.6 Data integrity checks and end-to-end error 
detection 
Data used for the computations in the task must not 
only  be  protected  during  the  actual  computation,  but 
also  before  and  after  the  computation.  This  is  often 
referred  to  as  end-to-end  error  detection  [4].  We 
assume that the memory is protected from direct faults 
using ECC, see Table 1. Furthermore, static data such 
as  program  code  and  constants  may  be  saved  in  read 
only  memory  and  may  therefore  not  be  erroneously 
overwritten.  However,  additional  protection  is  needed 
to ensure the integrity of the input data, state data and 
result  data.  Faults  occurring  in  the  CPU  affecting  the 
input data or state data during the computations of the 
individual copies may be detected by the comparisons, 
while faults occurring when data are stored to memory 
may cause data to be overwritten or to be written to an 
erroneous  location  in  memory.  There  are  a  number  of 
techniques  that  can  be  used  to  detect  such  errors. The 
simplest  is  to  duplicate  the  data  and  conduct  a 
comparison before it is used to reveal discrepancies. To 
protect larger data structures, it may be more effective 
to generate CRC checksums for the data. 
For  a  duplex  configuration,  errors  that  are  detected 
may  be  handled  by  exhibiting  an  omission  failure, 
since  the  partner  node  provides  the  full  service. 
Recovery  of  input  data  may  be  conducted  by  simply 
obtaining new data in the next cycle, and eventual state 
data may be recovered by obtaining the partner node's 
state  data.  For  a  simplex  configuration,  errors  in  the 
state data may be handled in different ways depending 
on  the  application.  For  example,  triplication  of  data 
may  be  employed  to  mask  the  effect  of  faults,  or  the 
node may just be shut down for a fail-safe system. 
2.7 Control flow errors 
Faults  may cause  control  flow  errors, i.e. deviation 
from  the  correct  execution  order,  and  thus  cause 
failures.  The  MMU  provided  by  the  processor  may 
detect control flow errors as the task's address range is 
bounded.  If  the  control  flow  error  causes  the  task  to 
execute too long, the error may also be detected by the 
execution  time  monitoring  mechanism.  In  addition, 
TEM will also detect control flow errors within a task 
if the error affects the result and the comparison/voting 
is  executed  correctly.  There  is,  however,  a  small  risk 
that a control flow error may cause the execution to by-
pass  the  comparison/voting  by  erroneously  jumping 
directly to the code that writes the checked/voted result 
to  main  memory  or  to  an  output  device.  Specific 
checks  should  be  provided  to  avoid  that  such  control 
flow errors pass undetected.  
2.8 Real-time requirements 
The  TEM  approach  relies  on  event-triggered  fault 
handling, since recovery (through execution of a third 
copy of the affected task) should only be initiated when 
an  error  is  detected.  The  kernel  therefore  uses  fixed 
priority (FP) pre-emptive scheduling [6]. 
 In  fixed  priority  scheduling,  the  priority  of  each 
task  is  determined  before  run-time.  FP  scheduling 
allows  both  periodic  and  sporadic  task  executions 
where  the  task  with  the  highest  priority  is  always 
allowed  to  execute  first.  In  our  kernel,  priority 
assignments are made on the basis of the criticality of 
the  task.  The  criticality  of  a  task  relates  to  the 
consequences  of  a  failure  of  the  task,  e.g.  a  brake 
request  is  assigned  a  higher  priority  than  a  diagnostic 
request.  Since  the  scheduling  is  pre-emptive,  a  task  is 
suspended  whenever  another  task  with  higher  priority 
requires access to the CPU. 
To  ensure  that  critical  tasks  meet  their  deadlines 
also in the presence of errors,  we assume that a  fault-
tolerant scheduling algorithm supporting fixed priority 
scheduling  is  employed.  Fault-tolerant  scheduling  [6] 
guarantees  that  all  tasks  meet  their  deadlines,  even  in 
the presence of a specified number of faults. To allow a 
failed task to re-execute without causing other tasks to 
miss  their  deadlines,  extra  time  (slack)  must  be 
reserved  a  priori  and  be  accounted  for 
in  a 
schedulability  test.  The  amount  of  extra  time  needed 
depends on the number and type of faults anticipated.  
3. Dependability analysis 
the  dependability  of  systems.  This 
The  ability  to  recover  quickly  may  significantly 
increase 
is 
exemplified  in  this  section  through  a  dependability 
analysis  of  a  distributed  brake-by-wire 
(BBW) 
architecture, which is a typical example of the kind of 
safety-critical  distributed  systems  considered  here.  In 
particular,  we  examine  the  dependability  of  the  BBW 
architecture  using  conventional  fail-silent  computer 
nodes,  as  compared  to  using  nodes  with  light-weight 
NLFT. 