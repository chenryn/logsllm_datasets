missing in the given labels. For each dataset, we randomly
selected nc classes and marked all the training samples in these
classes as “unlabeled samples”. For the rest of the classes, we
only keep 1% of labeled samples for each class and mark the
remaining samples as “unlabeled”.
In this experiment, we vary nc to examine its inﬂuence
upon the system performance. To make sure our results are not
biased by the choice of missing classes, we randomly select
the classes to mark as “missing”. For a given nc, we randomly
select nc classes as missing classes for 10 times. This generates
10 training sets for each nc. In total, we have |nc| × 10
training sets. On each training set, we run FARE, and our
baselines (i.e., DNN+, MixMatch+, and Ladder+) and compute
the testing AMI. In addition, we also want to examine the
capability of each algorithm in recovering the actual number
of classes. We use K to denote the ﬁnal number of clusters.
Finally, we compute the mean and standard deviation of testing
AMI and K under each nc setting (across 10 training datasets).
In addition to AMI, accuracy metric is also calculated (for
selective settings) in Appendix-C.
Experiment III: Coarse-grained Labels. This experiment
is an end-to-end evaluation of FARE under the coarse-grained
label setting. Suppose a dataset originally contains n class,
we randomly selected ng classes and merged their training
samples into a union label. For the rest of the classes, we also
8
TABLE I.
PERFORMANCE COMPARISON BASED ON MEANS AND STANDARD DEVIATIONS OF AMIS, ACCURACY, AND RUNTIME. “Unsup. FARE”
REPRESENTS THE UNSUPERVISED VERSION OF FARE. SINCE NEITHER CSPA NOR HGPA SCALE TO THE FULL DATASET, WE REPORT THEIR RESULTS AND
THAT OF Unsup. FARE ON 10% OF TRAINING SET.
Dataset
Metric
Full
Training set
10%
Training set
FARE
Unsup. FARE
Kmeans
DBSCAN
DEC
Unsup. FARE
CSPA
HGPA
AMI
0.87 ± 0.01
0.74 ± 0
0.47 ± 0.12
0.69 ± 0.03
0.37 ± 0.09
0.72 ± 0.01
0.5 ± 0.04
0.57 ± 0.03
MALWARE
Accuracy
0.97 ± 0
0.81 ± 0.01
0.51 ± 0.04
0.77 ± 0.02
0.47 ± 0.07
0.80 ± 0.01
0.61 ± 0.06
0.69 ± 0.05
Runtime (s)
434.05
432.12
26.99
174.63
342.42
77.33
176.29
90.1
AMI
0.98 ± 0
0.78 ± 0
0.39 ± 0.18
0.38 ± 0.1
0.64 ± 0.12
0.76 ± 0
0.36 ± 0.11
0.4 ± 0.09
Network Intrusion
Accuracy
1 ± 0
0.99 ± 0
0.64 ± 0.12
0.66 ± 0.04
0.85 ± 0.04
0.98 ± 0
0.64 ± 0.08
0.79 ± 0.06
Runtime (s)
8, 943.08
8, 942.52
16.30
8, 918.36
725.58
1, 801.74
2, 013.77
1, 804.82
only keep 1% of the labeled training samples in each class and
mark the remaining samples as “unlabeled”. With this setup,
the training set only has in total n − ng + 1 classes, and each
class has 1% data labeled.
We vary ng to examine its inﬂuence on the system perfor-
mance. For each ng, we also randomly sample different classes
to merge into the union class for 10 times and construct 10
training sets. In total, we have |ng|× 10 training sets. We then
run FARE and baselines on each training set and calculate
the mean and standard deviation for the testing AMI and K.
Similar to before, the accuracy metric is also reported (for
selective settings) in Appendix-C.
Note that here we only consider the setting where the
chosen classes are merged into one union label. In Appendix-E,
we have included additional experimental results for multiple
union labels. The overall conclusion is consistent, and thus the
results for multiple union labels are omitted here for brevity.
Experiment IV: Algorithm Sensitivity. Finally, we examine
the sensitivity of FARE and Unsup.FARE to the number of
unsupervised neighborhood models M(cid:48) = M − 1. For FARE,
we ﬁx nc = (cid:98)n/2(cid:99) or ng = (cid:98)n/2(cid:99) and labeled data ratio as
1%. We randomly select M(cid:48) =10, 20, 50, and 100 from the
pool of 150 neighborhood models (used in Experiment I–III)
to generate the ensemble. We run FARE and Unsup.FARE
10 times for each M(cid:48) and record the mean AMIs.
We also tested the algorithm sensitivity to other factors
such as the ratio of available labels, the output dimension of
the transformation network q, and the number of true classes in
a dataset. By default, the ratio of available labels is 1% and
q = 32. We experimentally tested different ratio and q, and
found the algorithm performance was not sensitive to neither
factors. In addition, our security datasets only have up to 6
and 9 classes respectively. So we further tested FARE on an
image dataset with more classes (i.e., 43), and conﬁrmed that
FARE still performed well. Due to space limit, we present the
detailed results in Appendix-G and Appendix-H.
C. Experiment Results
FARE vs. Base Clustering Algorithms. Table I shows the
AMI and accuracy of FARE (both supervised and unsupervised
versions) and other individual clustering algorithms. First, we
observe that the mean AMI and accuracy of the unsupervised
FARE (i.e., Unsup. FARE) are consistently higher than all
other clustering methods on both datasets. The performance
of individual clustering methods varies on different datasets.
This validates our hypothesis that existing clustering methods
have different assumptions on the data distribution and work
well only when the data distribution matches the assumptions.
With the ensemble of multiple unsupervised models, Unsup.
FARE performs consistently better. Note that Unsup. FARE
has lower standard deviations, indicating its results are more
stable across different training rounds. In addition, Unsup.
FARE signiﬁcantly outperforms K-means. This means, if we
apply K-means without the input transformation network, the
performance will suffer. Note that the DEC algorithm uses
an auto-encoder to transform the inputs. The higher AMI of
unsupervised FARE over DEC conﬁrms the advantage of our
data transformation function over the state-of-art auto-encoder.
Supervised FARE is performing better than unsupervised
FARE. For example, on the network intrusion dataset, the AMI
is boosted from 0.68 to 0.98. Recall that in this experiment,
supervised FARE only takes 1% of the labels. This conﬁrms the
beneﬁts of combining supervised learning (even with limited
labels) and unsupervised results.
FARE vs. Existing Clustering Ensemble Methods. As
shown in Table I, FARE and Unsup. FARE both outperform
existing clustering ensemble methods CSPA and HGPA in
terms of AMI and Accuracy. There are two possible reasons.
First, FARE aggregates clustering ensembles with the given
labels, and the labels bring in performance gains. Second,
CSPA and HGPA struggle under high-dimensional input space.
In comparison, FARE’s input transformation network projects
the inputs into a lower-dimensional space with well-deﬁned
distance, which makes clustering more effective.
Computational Overhead. Table I also shows the training
time for each algorithm. We observe that both FARE and
Unsup.FARE adds only a small fraction of the runtime on
top of the existing clustering algorithms. Since the different
neighborhood models are independent, we can parallelize their
clustering process. As a result, the performance bottleneck is
introduced by the slowest clustering algorithm in the ensemble.
In our case, the slowest algorithm on the malware dataset is
DEC, and the slowest algorithm on the intrusion dataset is
DBSCAN. As shown in Table I, the added overhead by FARE
and Unsup.FARE is considerably small, while the gains on
AMIs are signiﬁcant, which is a worthy trade-off. Since all
of the base clustering algorithms are widely used in both
academia and industry as benchmark clustering methods, we
argue that the computational cost introduced by our design
does not jeopardize its usage in practice. Later in Section §VI,
we have further discussions on the computational cost. Table I
also shows Unsup.FARE are faster than CSPA and HGPA
on 10% of the training dataset. This is because our method
avoids the expensive cluster alignment step in CSPA and
HGPA. Instead, we use pair-wise relationships to fuse the
9
(a) Malware categorization.
(b) Intrusion detection.
(a) Malware categorization.
(b) Intrusion detection.
Fig. 2. The performances of FARE and the baselines in the missing classes
settings. We show the mean AMI and the standard deviation. nc is the number
of missing classes.
The performances of FARE and baselines in the coarse-grained
Fig. 3.
label setting. We show mean AMI and standard deviation. ng is the number
of classes in the union class.
base clustering results. In addition, we support batch-learning,
which further reduces the memory and computational cost.
Performance under the Missing Class Setting. As shown
in Figure 2, when there is no missing class in the training
data (i.e., nc = 0), the performance of all the systems are
fairly comparable on both datasets. Recall that the training
dataset only has 1% of labeled samples. With limited labels,
the supervised learning method do not outperform the semi-
supervised methods. Then as the number of missing classes
nc increases, the baseline methods start to exhibit inconsistent
and degraded performances. For example, when nc = 4 in
Figure 2(a), and nc = 7 in Figure 2(b), it means 4 out of
6 classes are missing in the malware datasets, and 7 out of
9 classes are missing in the network intrusion dataset. The
average reduction of baseline performances is around 50.5%.
The worst AMI is even lower than 0.25. There are two possible
reasons. First, existing algorithms are highly dependent on the
assumption that the labeled classes are complete (i.e., at least
a few labeled samples are expected to be available in each
class). When this assumption is no longer held in the training
data, their performances suffer. Another explanation is that
existing methods cannot correctly recover the data manifolds
of the unlabeled classes in the training set [24]. Without such
information, they cannot make correct decisions on testing
samples from these unlabeled classes. As nc increases, we
notice that the standard deviations of AMIs also increase for
baselines. This indicates that the choices of the missing classes
also have an impact on the baseline methods’ performance.
In comparison, FARE demonstrates a much higher AMI
across all the settings. As shown in Figure 2, the number
of missing classes only have a small impact on FARE. For
example, in Figure 2(b), the AMI of FARE only decreases from
0.97 to 0.83 when nc increase from 0 to 7. On average, FARE
has a reduction of 15.4% of AMI across the two datasets. The
results conﬁrm the beneﬁts of using the ensemble of unsu-
pervised learning results when the given labels have missing
classes (i.e., low-quality labels). The ensemble component of
FARE helps to extract useful information (i.e., data manifolds)
of the missing classes. The standard deviations of FARE are
also consistently lower than those of the baseline methods.
This again conﬁrms the beneﬁt of the ensemble in reducing
FARE’s sensitivity to the choice of missing classes.
Table II (the left half) shows the ﬁnal number of classes
identiﬁed by FARE and other baselines. The ground-truth
number of classes is 6 and 9 for malware and intrusion
datasets, respectively. As we increase the number of missing
classes nc, we can see that the baseline algorithms are more
likely to underestimate the true number of classes on both
datasets. In contrast, FARE successfully recovers the true
number of classes in the malware dataset regardless of the
severity of missing classes. For the network intrusion dataset,
while not being able to identify the true number of classes,
FARE has a lower estimation error. It should be noted that
even when nc = 0, none of the baseline methods can correctly
recover the true number classes for the intrusion dataset. We
suspect this is due to the class imbalance issue. In the intrusion
dataset, the top three classes take 98.5% of samples, which
makes it difﬁcult for the clustering methods to identify the
minor classes. Interestingly, as the number of missing classes
increases, FARE is approaching the true number of classes
(possibly because the given labels become less inﬂuential).
Together with the results in Figure 2, we conclude that without
missing classes, FARE is comparable to the supervised and
semi-supervised baselines. When there are missing classes in
the given labels, FARE demonstrates signiﬁcant advantages.
Performance under the Coarse-grained Label Setting.
Figure 3 shows the performance of each method under the
coarse-grained label settings. We observe that the performance
of baseline methods reduces dramatically as ng increases
(i.e., lower AMI and higher standard deviation). This means
existing methods lack the capability in dealing with the coarse-
grained labels. On the contrary, coarse-grained labels only have
a minor impact on FARE. The average AMI reduction is only
8.2% across all the settings. The standard deviations are low
for FARE, indicating that FARE is robust to the choices of
classes in the union label.
Table II (the right half) presents the number of ﬁnal classes
identiﬁed by each method under the coarse-grained label set-
tings. For the malware dataset, we have the same observation
as before: FARE correctly recovers the true number of classes
regardless of the degree of coarse-grained labels while other
baseline algorithms cannot. For the intrusion dataset, however,
the observation is quite different. On the one hand, neither
FARE nor the baseline methods can estimate the true number
of classes correctly. On the other hand, unlike the baseline
methods that occasionally overestimate the true number of
classes, FARE consistently underestimates it. For example,
when ng = 7, the number classes estimated by FARE is only
4. We speculate that this is caused by the compound effect
introduced by coarse-grained labels and the extreme class
imbalance. Recall that in the intrusion dataset, the top 3 classes
count for 98.5% of the samples. The coarse-grained label
10
024nc0.20.30.40.50.60.70.80.91.0AMIFAREMixMatch+Ladder+DNN+0147nc0.20.40.60.81.0AMIFAREMixMatch+Ladder+DNN+024ng0.20.30.40.50.60.70.80.91.0AMIFAREMixMatch+Ladder+DNN+0147ng0.20.40.60.81.0AMIFAREMixMatch+Ladder+DNN+THE NUMBER OF CLUSTERS K DISCOVERED BY FARE AND THE BASELINE ALGORITHMS UNDER DIFFERENT SETTINGS. N REPRESENTS THE
GROUND-TRUTH NUMBER OF CLASSES IN EACH DATASET.
Num. of missing classes (nc)
4
0
7
Num. of mistaken grouped classes (ng)
Intrusion (N = 9)
1
4
Malware (N = 6)
0