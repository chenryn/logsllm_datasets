title:Estimating g-Leakage via Machine Learning
author:Marco Romanelli and
Konstantinos Chatzikokolakis and
Catuscia Palamidessi and
Pablo Piantanida
PI:EMAIL
Inria, École Polytechnique, IPP, Università di Siena
Palaiseau, France
Catuscia Palamidessi
PI:EMAIL
Inria, École Polytechnique, IPP
Palaiseau, France
PI:EMAIL
University of Athens
Athens, Greece
Pablo Piantanida
PI:EMAIL
CentraleSupelec, CNRS, Université Paris Saclay
Gif-sur-Yvette, France
Estimating g-Leakage via Machine Learning
Marco Romanelli
Konstantinos Chatzikokolakis
0
2
0
2
p
e
S
8
1
]
R
C
.
s
c
[
2
v
9
9
3
4
0
.
5
0
0
2
:
v
i
X
r
a
ABSTRACT
This paper considers the problem of estimating the information
leakage of a system in the black-box scenario, i.e. when the system’s
internals are unknown to the learner, or too complicated to analyze,
and the only available information are pairs of input-output data
samples, obtained by submitting queries to the system or provided
by a third party. The frequentist approach relies on counting the
frequencies to estimate the input-output conditional probabilities,
however this method is not accurate when the domain of possi-
ble outputs is large. To overcome this difficulty, the estimation of
the Bayes error of the ideal classifier was recently investigated
using Machine Learning (ML) models, and it has been shown to
be more accurate thanks to the ability of those models to learn the
input-output correspondence. However, the Bayes vulnerability
is only suitable to describe one-try attacks. A more general and
flexible measure of leakage is the д-vulnerability, which encom-
passes several different types of adversaries, with different goals
and capabilities. We propose a novel approach to perform black-box
estimation of the д-vulnerability using ML which does not require
to estimate the conditional probabilities and is suitable for a large
class of ML algorithms. First, we formally show the learnability for
all data distributions. Then, we evaluate the performance via vari-
ous experiments using k-Nearest Neighbors and Neural Networks.
Our approach outperform the frequentist one when the observables
domain is large.
CCS CONCEPTS
• Security and privacy Ñ Formal security models; Formal
methods and theory of security; Information flow control;
• Computing methodologies Ñ Neural networks; Machine
learning.
KEYWORDS
д-vulnerability estimation; machine learning; neural networks
1 INTRODUCTION
The information leakage of a system is a fundamental concern of
computer security, and measuring the amount of sensitive informa-
tion that an adversary can obtain by observing the outputs of a given
system is of the utmost importance to understand whether such
leakage can be tolerated or must be considered a major security flaw.
Much research effort has been dedicated to studying and proposing
solutions to this problem, see for instance [2, 4, 7, 10, 11, 20, 30, 38].
So far, this area of research, known as quantitative information flow
(QIF), has mainly focused on the so-called white-box scenario, i.e.
assuming that the channel of the system is known, or can be com-
puted by analyzing the system’s internals. This channel consists
of the conditional probabilities of the outputs (observables) given
the inputs (secrets).
The white-box assumption, however, is not always realistic:
sometimes the system is unknown, or anyway it is too complex,
so that an analytic computation becomes hard if not impossible to
be performed. Therefore, it is important to consider also black-box
approaches where we only assume the availability of a finite set
of input-output pairs generated by the system. A further specifica-
tion of a black box model is the “degree of inaccessibility”, namely
whether or not we are allowed to interact with the system to obtain
these pairs, or they are just provided by a third party. Both scenarios
are realistic, and in our paper we consider the two cases.
The estimation of the internal probabilities of a system’s channel
have been investigated in [17] and [19] via a frequentist paradigm,
i.e., relying on the computation of the frequencies of the outputs
given some inputs. However, this approach does not scale to appli-
cations for which the output space is very large since a prohibitively
large number of samples would be necessary to achieve good results
and fails on continuous alphabets unless some strong assumption
on the distributions are made. In order to overcome this limitation,
the authors of [14] exploited the fact that Machine Learning (ML)
algorithms provide a better scalability to black-box measurements.
Intuitively, the advantage of the ML approach over the frequen-
tist one is its generalization power: while the frequentist method
can only draw conclusions based on counts on the available sam-
ples, ML is able to extrapolate from the samples and provide better
prediction (generalization) for the rest of the universe.
In particular, [14] proposed to use k-Nearest Neighbors (k-NN) to
measure the basic QIF metrics, the Bayes vulnerability [38]. This is
the expected probability of success of an adversary that has exactly
one attempt at his disposal (one-try), and tries to maximize the
chance of guessing the right secret. The Bayes vulnerability corre-
sponds to the converse of the error of the ideal Bayes classifier that,
given any sample (observable), tries to predict its corresponding
class (secret). Hence the idea is to build a model that approximates
such classifier, and estimate its expected error. The main takeaway
is that any ML rule which is universally consistent (i.e., approxi-
mates the ideal Bayes classifier) has a guarantee on the accuracy of
the estimation, i.e., the error in the estimation of the leakage tends
to vanish as the number of training samples grows large.
The method of [14], however, is limited to the Bayes vulnerability,
which models only one particular kind of adversary. As argued
in [4], there are many other realistic ones. For instance, adversaries
whose aim is to guess only a part of the secret, or a property of the
secret, or that have multiple tries at their disposal. To represent a
larger class of attacks, [4] introduced the so-called д-vulnerability.
This metric is very general, and in particular it encompasses the
Bayes vulnerability.
this is the first time that a method to estimate д-vulnerability
in a black-box fashion is introduced.
‚ We provide statistical guarantees showing the learnability
of the д-vulnerability for all distributions and we derive
distribution-free bounds on the accuracy of its estimation.
‚ We validate the performance of our method via several exper-
iments using k-NN and ANN models. The code is available
at the URL https://github.com/LEAVESrepo/leaves.
Symbol
x P X
a secret
w P W a guess
y P Y
X
W
Y
|S|
PpSq
H
π, PX
ˆπ,pPX
pPX Y
pPY|X
C
πŻC
PX Y
PY|X
P
Er¨s
дpw, xq
G
Vд
Vpf q
pVnpf q
Description
an observable output by the system
random var. for secrets taking values x P X
random var. for guesses taking values w P W
random var. for observables taking values y P Y
size of a set S
Distribution over a set of symbols S
class of learning functions f
prior distribution over the secret space
empirical prior distribution over the secret space
Channel matrix
joint distribution from prior π and channel C
joint probability distribution
empirical joint probability distribution
conditional probability of Y given X
empirical conditional probabilities
probability measure
expected value
gain function: guess w and secret x as inputs
gain matrix of size |W| ˆ |X| for a specific д
д-vulnerability
д-vulnerability functional
empirical д-vuln. functional evaluated on n samples
Table 1: Table of symbols.
In this paper, we propose an approach to the black-box estima-
tion of д-vulnerability via ML. The idea is to reduce the problem to
that of approximating the Bayes classifier, so that any universally
consistent ML algorithm can be used for the purpose. This reduction
essentially takes into account the impact of the gain function in
the generation of the training data, and we propose two methods
to obtain this effect, which we call channel pre-processing and data
pre-processing, respectively. We evaluate our approach via experi-
ments on various channels and gain functions. In order to show the
generality of our approach, we use two different ML algorithms,
namely k-NN and Artificial Neural Networks (ANN), and we com-
pare their performances. The experimental results show that our
approach provides accurate estimations, and that it outperforms by
far the frequentist approach when the observables domain is large.
1.1 Our contribution
‚ We propose a novel approach to the black-box estimation of
д-vulnerability based on ML. To the best of our knowledge,
1.2 Related work
One important aspect to keep in mind when measuring leakage is
the kind of attack that we want to model. In [30], Köpf and Basin
identified various kinds of adversaries and showed that they can be
captured by known entropy measures. In particular it focussed on
the adversaries corresponding to Shannon and Guessing entropy.
In [38] Smith proposed another notion, the Rényi min-entropy, to
measure the system’s leakage when the attacker has only one try
at its disposal and attempts to make its best guess. The Rényi min-
entropy is the logarithm of the Bayes vulnerability, which is the
expected probability of the adversary to guess the secret correctly.
The Bayes vulnerability is the converse of the Bayes error, which
was already proposed as a measure of leakage in [12].
Alvim et al. [4] generalized the notion of Bayes vulnerability to
that of д-vulnerability, by introducing a parameter (the gain func-
tion д) that describes the adversary’s payoff. The д-vulnerability is
the expected gain of the adversary in a one-try attack.
The idea of estimating the д-vulnerability in the using ML tech-
niques is inspired by the seminal work [14], which used k-NN
algorithms to estimate the Bayes vulnerability, a paradigm shift
with respect to the previous frequentist approaches [9, 17, 19]. No-
tably, [14] showed that universally consistent learning rules allow
to achieve much more precise estimations than the frequentist ap-
proach when considering large or even continuous output domains
which would be intractable otherwise. However, [14] was limited to
the Bayes adversary. In contrast, our approach handles any adver-
sary that can be modeled by a gain function д. Another novelty w.r.t.
[14] is that we consider also ANN algorithms, which in various
experiments appear to perform better than the k-NN ones.
Bordenabe and Smith [6] investigated the indirect leakage in-
duced by a channel (i.e., leakage on sensitive information not in
the domain of the channel), and proved a fundamental equivalence
between Dalenius min-entropy leakage under arbitrary correlations
and g-leakage under arbitrary gain functions. This result is similar
to our Theorem 4.2, and it opens the way to the possible extension
of our approach to this more general leakage scenario.
2 PRELIMINARIES
In this section, we recall some useful notions from QIF and ML.
2.1 Quantitative information flow
Let X be a set of secrets and Y a set of observations. The adver-
sary’s initial knowledge about the secrets is modeled by a prior
distribution PpXq (namely PX ). A system is modeled as a proba-
bilistic channel from X to Y, described by a stochastic matrix C,
whose elements Cxy give the probability to observe y P Y when
the input is x P X (namely PY|X ). Running C with input π induces
a joint distribution on X ˆ Y denoted by πŻC.
In the д-leakage framework [4] an adversary is described by a
set W of guesses (or actions) that it can make about the secret, and
by a gain function дpw, xq expressing the gain of selecting the guess
w when the real secret is x. The prior д-vulnerability is the expected
gain of an optimal guess, given a prior distribution on secrets:
Vgpπq def“ max
wPW
πx ¨ gpw, xq .
(1)
ÿ
xPX
ÿ
xPX
In the posterior case, the adversary observes the output of the
system which allows to improve its guess. Its expected gain is given
by the posterior д-vulnerability, according to
Vgpπ , Cq def“
max
wPW
πx ¨ Cxy ¨ gpw, xq .
(2)
ÿ
yPY
, LA
g pπ , Cq def“ Vgpπ , Cq ´ Vgpπq .
Finally, the multiplicative1 and additive д-leakage quantify how
much a specific channel C increases the vulnerability of the system:
g pπ , Cq def“ Vgpπ , Cq
LM
(3)
Vgpπq
The choice of the gain function д allows to model a variety of
different adversarial scenarios. The simplest case is the identity
gain function, given by W “ X, дidpw, xq “ 1 iff x “ w and 0
otherwise. This gain function models an adversary that tries to
guess the secret exactly in one try; Vдid is the Bayes-vulnerability,
which corresponds to the complement of the Bayes error (cfr. [4]).
However, the interest in д-vulnerability lies in the fact that many
more adversarial scenarios can be captured by a proper choice of
д. For instance, taking W “ Xk with дpw, xq “ 1 iff x P w and 0
otherwise, models an adversary that tries to guess the secret cor-
rectly in k tries. Moreover, guessing the secret approximately can
be easily expressed by constructing д from a metric d on X; this is
a standard approach in the area of location privacy [36, 37] where
дpw, xq is taken to be inversely proportional to the Euclidean dis-
tance between w and x. Several other gain functions are discussed
in [4], while [3] shows that any vulnerability function satisfying
basic axioms can be expressed as Vд for a properly constructed д.
The main focus of this paper is estimating the posterior д-vulnera-
bility of the system from such samples. Note that, given Vдpπ , Cq,
g pπ , Cq is straightforward, since Vдpπq
estimating LM
only depends on the prior (not on the system) and it can be either
computed analytically or estimated from the samples.
g pπ , Cq and LA
2.2 Artificial Neural Networks
We provide a short review of the aspects of ANN that are relevant
for this work. For further details, we refer to [5, 28, 29]. Neural net-
works are usually modeled as directed graphs with weights on the
connections and nodes that forward information through “activa-
tion functions”, often introducing non-linearity (such as sigmoids or
soft-max). In particular, we consider an instance of learning known
as supervised learning, where input samples are provided to the
network model together with target labels (supervision). From the
provided data and by means of iterative updates of the connection
1Originally the multiplicative version of д-leakage was defined as the log of the
definition given here. In recent literature the log is not used anymore. Anyway, the
two definitions are equivalent for system comparison, since log is monotonic.
weights, the network learns how the data and respective labels are
distributed. The training procedure, known as back-propagation, is
an optimization process aimed at minimizing a loss function that
quantifies the quality of the network’s prediction w.r.t. the data.
Classification problems are a typical example of tasks for which
supervised learning works well. Samples are provided together with
target labels which represent the classes they belong to. A model
can be trained using these samples and, later on, it can be used to
predict the class of new samples.
According to the No Free Lunch theorem (NFL) [40], in gen-
eral, it cannot be said that ANN are better than other ML methods.
However, it is well known that the NFL can be broken by addi-
tional information on the data or the particular problem we want
to tackle, and, nowadays, for most applications and available data,
especially in multidimensional domains, ANN models outperform
other methods therefore representing the state-of-the-art.
2.3 k-Nearest Neighbors
The k-NN algorithm is one of the simplest algorithms used to clas-
sify a new sample given a training set of samples labelled as be-
longing to specific classes. This algorithm assumes that the space
of the features is equipped with a notion of distance. The basic idea
is the following: every time we need to classify a new sample, we
find the k samples whose features are closest to those of the new
one (nearest neighbors). Once the k nearest neighbors are selected,
a majority vote over their class labels is performed to decide which
class should be assigned to the new sample. For further details,
as well as for an extensive analysis of the topic, we refer to the
chapters about k-NN in [29, 35].
3 LEARNING д-VULNERABILITY:
STATISTICAL BOUNDS
This section introduces the mathematical problem of learning д-
vulnerability, characterizing universal learnability in the present
framework. We derive distribution-free bounds on the accuracy of
the estimation, implying the estimator’s statistical consistence.
3.1 Main definitions
We consider the problem of estimating the д-vulnerability from
samples via ML models, and we show that the analysis of this
estimation can be conducted in the general statistical framework
of maximizing an expected functional using observed samples. The
idea can be described using three components: