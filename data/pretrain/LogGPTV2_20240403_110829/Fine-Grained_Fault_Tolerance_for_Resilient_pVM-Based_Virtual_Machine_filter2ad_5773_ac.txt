thus adding a dependence of the latter w.r.t.
the former.
This dependency would make it difﬁcult to handle cascading
failures since net_uk already relies on XenStore.
In order to provide a highly available metadata storage
service, an alternative design could consist in using the etcd in-
stances as a complete replacement for the XenStore instances.
This approach would reduce the number of unikernel instances
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
201
and some communication steps between the pVM components.
We have actually tried to implement this approach but the
achieved performance was signiﬁcantly poorer: we observed
request latencies that were higher by up to several orders of
magnitude (microseconds vs. milliseconds). Indeed, XenStore
and etcd are datastores with a fairly similar data model but
their implementations are optimized for different contexts
(local machine interactions versus distributed systems). In ad-
dition, the design that we have chosen helps limiting the mod-
iﬁcations to be made w.r.t. the implementation of the vanilla
pVM components. In particular, this allows beneﬁting from
new features and performance optimizations integrated into
the vanilla XenStore codebase, e.g., regarding data branching
and transactions.
Data corruption. This type of faults is handled by a sanity
check approach implemented on all XenStore replicas, as
described below. First, we make the following assumption:
for a given logical piece of information that is replicated
into several physical copies, we assume that there is at most
one corrupted copy. Each etcd instance stores a hash of the
content of the latest known uncorrupted XenStore database
state. Besides, a sanity check agent (called checker) runs as a
transparent proxy in each XenStore replica. Upon every write
request sent to the XenStore service, each checker computes
a new hash, which is forwarded to the etcd coordinator. If the
hashes sent by all the replicas match, then this new value is
used to update the hash stored by all the etcd instances. Upon
the reception of a read request, the master XenStore replica
computes a hash of its current database state and compares
it against the hash sent by the coordinator. If they do not
match, a distributed recovery protocol is run between the etcd
coordinators to determine if the corrupted hash stems from
the coordinator or the XenStore master replica. In the former
case, the hash of the coordinator is replaced by the correct
value. In the latter case, the XenStore replica is considered
faulty and the etcd coordinator triggers the above-mentioned
recovery process.
Total XenStore failure. In the worse case, all XenStore and/or
etcd components can crash at the same time. In our solution,
this situation is detected and handled by the hypervisor via
the heartbeat mechanism mentioned above. The hypervisor
relaunches the impacted component according to a dependency
graph (see §IV-D). However, an additional issue is the need
to retrieve the state of the XenStore database. In order to
tolerate such a scenario without relying on the availability
of the disk_uk (to retrieve a persistent copy of the database
state), we rely on the hypervisor to store additional copies of
the XenStore database and the corresponding hashes. More
precisely, the hypervisor hosts an in-memory backup copy for
the database and hash stored by each replica, and each replica
is in charge of updating its backup copy.
B. net_uk FT solution
Based on the Mini-OS unikernel [21], the net_uk component
embeds the NIC driver and,
in accordance with the split
driver model, it proxies incoming and outgoing network I/O
requests to/from user VMs. To this end, net_uk also runs a
virtual driver called netback that interacts with a pseudo NIC
driver called netfront inside the user VM. The interactions
between netback and netfront correspond to a bidirectional
producer-consumer pattern and are implemented via a ring
buffer of shared memory pages and virtual IRQs. Overall,
net_uk can be seen as a composite component encapsulating
the NIC driver and the netback.
Fault model. We are interested in mitigating the unavailability
of net_uk. The latter can be caused by a crash of the NIC
driver, of the netback or of the whole unikernel. We assume
that a fault in the NIC driver or the netback does not corrupt
the low-level data structures of the kernel. This is a viable
assumption as we can run the NIC driver in an isolated
environment similar to Nooks [22] or LXDs [23].
FT solution. Our approach aims at detecting failures at
two levels: a coarse-grained level when the whole unikernel
fails and a ﬁne-grained level for the NIC driver and netback
failures. Before presenting the details of our solution, we ﬁrst
provide a brief background on the design of the I/O path,
using the reception of a new packet as an example.
Once the NIC reports the arrival of a packet, a hardware
interrupt
is raised and trapped inside the hypervisor. The
latter forwards the interrupt (as a virtual interrupt) to net_uk.
The handler of that virtual interrupt is then scheduled inside
net_uk. In general, the interrupt handler is organized in two
parts namely top half (TH) and bottom half (BH). The top
half masks off interrupt generation on the NIC and generates
a softirq whose handler is the bottom half. The latter registers
a NAPI (“New API”) function, aimed at polling for additional
incoming network packets. The maximum number of packets
that can be pooled using this mechanism is controlled via
a budget and a weight parameter. Upon its completion, the
bottom half unmasks interrupt generation by the NIC. Overall,
this design allows limiting the overhead of network interrupts.
To handle NIC driver failures, we leverage the shadow
driver approach introduced by Swift et al. [13]. The latter
was proposed for bare-metal systems. We adapt it for a Xen
virtualized environment as follows. The original shadow driver
approach states that each (physical) driver to be made fault
tolerant should be associated a with shadow driver, interposed
between the former and the kernel. This way, a failure of the
target driver can be masked by its shadow driver, which will
mimic the former during the recovery period. The shadow
driver can work in two modes: passive and active. In passive
mode, it simply monitors the ﬂow of incoming and completed
requests between the kernel and the target driver. Upon failure
of the target driver, the shadow driver switches to the active
mode: it triggers the restart of the target driver (and intercepts
the calls made to the kernel), and it buffers the incoming
requests from the kernel to the target driver (which will be
forwarded after the recovery process).
In our speciﬁc virtualized context, we do not create a
shadow driver for each net_uk component. Instead, we con-
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
202
sider an improved version of the netback driver as the shadow
driver for both itself and the NIC driver (see Fig. 5.b). In
this way, we reduce the number of shadow drivers and, as
a consequence,
the net_uk code base (complexity). When
a bottom half handler is scheduled, a signal is sent to the
hypervisor, which records the corresponding timestamp to.
Once the execution of the bottom half ends, another signal
is sent to the hypervisor to notify completion (see Fig. 5.a).
If no completion signal is received by the hypervisor after
to + tmax, where tmax is the estimated bottom half maximum
completion time, the hypervisor considers that the NIC driver
has failed, and triggers the recovery of the driver (using
existing techniques [13]), as shown in Fig. 5.b. The tuning
of tmax depends on budget and weight values (see above) and
is empirically determined. In our testbed, the values used for
budget and weight are 300 and 64 respectively, and tmax is
about 4s.
Fig. 5: net_uk, in which the shadow driver (netback) works
either in passive (a) or in active (b) mode. In the former
mode (no NIC driver failure),
the hypervisor records the
bottom half handler (BH) starting timestamp to and awaits a
completion signal before to + tmax, otherwise triggers NIC
driver (ND) reload. In active mode (ND failure has been
detected), the netback buffers requests and acks the netfront
upon ND recovery.
Regarding the failure of the netback, the hypervisor moni-
tors the shared ring buffer producer and consumer counters
between a netback and its corresponding frontend. If the
netback’s private ring counters remain stuck while the shared
ring counters keep evolving, this lag is considered as a hint
revealing the failure of the netback. Hence, the netback is
reloaded (unregistered then registered). Meanwhile, its fron-
tend’s device attribute otherend->state value switches
to XenbusStateReconfigured while the netback un-
dergoes repair. Once the repair is complete, the latter value
switches back to XenbusStateConnected and proceeds
with the exchange of I/O requests with the netback.
Regarding the failure of the entire unikernel, we adopt the
same approach as TFD-Xen [4]: the hypervisor monitors the
sum of the counters of the shared ring buffer used by all
netbacks and their corresponding netfront drivers to detect a
lag between the producer and the consumer counter. However,
this approach alone cannot detect net_uk hanging when it is
not used by any user VM. Therefore, we combine it with
a heartbeat mechanism, also controlled by the hypervisor. A
reboot of the net_uk VM is triggered when any of the two
above-described detection techniques raises an alarm.
C. tool_uk FT solution
The tool_uk unikernel embeds the Xen toolstack for
VM administration tasks (creation, migration, etc.). We
use XSM (Xen Security Modules [24]) to introduce a new
role (tooldom) which has fewer privileges than the original
monolithic dom0 but enough for administrative services.
It runs in an enriched version of Mini-OS [21], a very
lightweight unikernel, part of the Xen project.
strive
the fault
to mitigate
Fault model. We
faults occurring
during administrative operations. Apart from live migration
(discussed below),
tolerance requirements for all
the other administration tasks are already fully handled
either locally, by the vanilla toolstack implementation, or
globally, by the data center management system (e.g. Nova in
OpenStack). In these cases, our solution provides nonetheless
fast and reliable notiﬁcations regarding the failures of the
toolstack. We now describe the speciﬁc problem of
local
resilient
live migration. During the ﬁnal phase of a live
migration operation for a VM5, the suspended state of the
migrated VM is transferred to the destination host and upon
reception on the latter, the VM is resumed. If a fault occurs
during that phase, the migration process halts and leaves a
corrupted state of the VM on the destination machine and a
suspended VM on the sender machine.
FT solution. We consider
that a failure has occurred
during the migration process if the sender machine does
not receive (within a timeout interval) the acknowledgement
message from the destination machine, which validates the
end of the operation. As other unikernels in our solution,
faults resulting in the crash/hang of the entire tool_uk are
detected with a heartbeat mechanism and trigger the restart
of the tool_uk instance. In both cases (partial or complete
failure of the component), the repair operation for the failed
migration is quite simple and consists in (i) ﬁrst discarding
the state of the suspended VM on the destination machine,
(ii) destroying the VM on the destination machine, and (iii)
resuming the original VM instance on the sender machine.
D. Global feedback loop
Our solution includes a global feedback loop for handling
concurrent failures of multiple pVM components (and poten-
tially all of them). Such a situation may or may not be due
to a cascading failure. To handle such a situation in the most
efﬁcient way, the hypervisor embeds a graph thats indicates
the dependencies between the different unikernels, which are
represented in Figure 6. When a unikernel fails, the hypervisor
starts the recovery process only when all unikernels used by
the former are known to be healthy and reachable.
5Xen adopts a pre-copy iterative strategy for live migration [25].
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
203
of several services at the same time. For each experiment, we
consider a challenging situation in which both dom0 services
and user VMs are highly solicited. Crash failures are emulated
by killing the target component or unikernel. In order to
simulate data corruption (in the case of XenStore_uk), we
issue a request that overwrites a path (key-value pair) within
the data store.
We are interested in the following metrics: (1) the overhead
of our solution on the performance of dom0 services; (2) the
overhead of our solution on the performance of user VMs;
(3) the failure detection time; (4) the failure recovery time;
(5) the impact of failures on dom0 services; (6) the impact of
failures on user VMs. The overhead evaluation is performed
on fault-free situations. We compare our solution with
vanilla Xen 4.12.1 (which provides almost no fault tolerance
guarantees against pVM failures), Xoar [5] (periodic refresh),
and TFD-Xen [4] (which only handles net_uk failures). For a
meaningful comparison, we re-implemented the two previous
systems in the (more recent) Xen version that we use for
our solution. For Xoar, we use a component refresh period
of 1 second, and the different components are refreshed
sequentially (not simultaneously) in order to avoid pathologic
behaviors.
Benchmarks. User VMs run applications from the TailBench
benchmark suite [6]. The latter is composed of 8 latency-
sensitive (I/O) applications that span a wide range of latency
requirements and domains and a harness that implements a
robust and statistically-sound load-testing methodology. It
performs enough runs to achieve 95% conﬁdence intervals
≤ 3% on all runs. We use the client-server mode. The client
and the server VMs run on distinct physical machines. The
server VM is launched on the system under test. We left out
3 applications from the TailBench suite, namely Shore, Silo
and Specjbb. Indeed, the two former are optimized to run on
machines with solid state drives (whereas our testbed machine
is equipped with hard disk drives), and Specjbb cannot run in
client-server mode. In addition, we also measure the request
throughput sustained by the Apache HTTP server (running
in a user VM) with an input workload generated by the AB
(ApacheBench) benchmark [26] (using 10,000 requests and a
concurrency level of 10).
Testbed. All experiments are carried out on a 48-core
PowerEdge R185 machine with AMD Opteron 6344
processors and 64 GB of memory. This is a four-socket
NUMA machine, with 2 NUMA nodes per socket, 6 cores
and 8 GB memory per NUMA node.
The dom0 components use two dedicated sockets and
the user VMs are run on the two other sockets. Providing
dedicated resources to the pVM is in line with common
practices used in production [27] in order to avoid interfer-
ence. Besides, we choose to allocate a substantial amount of
resources to the dom0 in order to evaluate more clearly the
intrinsic overheads of our approach (rather than side effects
of potential resource contention). We use Xen 4.10.0 and
Fig. 6: Relationships between the different components of the
disaggregated pVM.
E. Scheduling optimizations
The design that we have described so far, with the dis-
aggregation of the pVM services into independent unikernel
VMs and the usage of heartbeats to detect their failures, raises
some challenges with respect
to CPU scheduling. Indeed,
it is non-trivial to ensure that these VMs are appropriately
scheduled. On the one hand, due to the number of VMs
resulting from the disaggregation, dedicating one (or several)
distinct physical CPU core(s) to each unikernel VM would
result in signiﬁcant resource waste (overprovisioning). On the
other hand, if such VMs are not scheduled frequently enough,
they may not be able to send their heartbeats on time to the
hypervisor (leading to false positives, and unneeded repair
procedures), or, as a workaround, this may require to set
longer timeouts (leading to slow detection of actual failures).
In order to overcome the above-described issues, we slightly
modify the CPU scheduler of the hypervisor. At creation
time, each service VM is marked with a special ﬂag and
the hypervisor CPU scheduler guarantees that such VMs are
frequently scheduled and sends a ping request to a unikernel
VM before switching to it. Each service VM is granted a
time slice of 5ms for heartbeat response. As an additional
optimization, the scheduling algorithm is modiﬁed to skip the
allocation of a CPU time slice to a unikernel VM if the latter
has recently (in our setup, within the last 15ms) issued an
“implicit” heartbeat (for example, in the case of the net_uk
VM, a recent and successful interaction with the hypervisor for
sending or receiving a packet is a form of implicit heartbeat).
This avoids the cost of context switches to a unikernel VM
solely for a ping-ack exchange when there are hints that this
VM is alive.
V. EVALUATION
This section presents the evaluation results of our prototype.
Evaluation methodology and goals. We evaluate both the
robustness and the reactivity of our solution in fault situations.
We ﬁrst evaluate each dom0 service FT solution individually,
meaning that a single failure (in a single component) is
injected at a time in the system. Then, we consider the failure
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 11:32:03 UTC from IEEE Xplore.  Restrictions apply. 
204
the dom0 runs Ubuntu 12.04.5 LTS with Linux kernel 5.0.8.
The NIC is a Broadcom Corporation NetXtreme II BCM5709
Gigabit Ethernet interface. The driver is bnx2. The machines
are linked using a 1Gb/s Ethernet switch. Unless indicated
otherwise, user VMs run Ubuntu 16.04 with Linux Kernel
5.0.8, conﬁgured with 16 virtual CPUs (vCPUs) and 16GB
of memory. Concerning unikernels composing dom0, each is
conﬁgured with 1 vCPU and 1 GB of memory (128MB for
the XenStore instances). The real memory footprint during our
evaluations is ≈500MB for every unikernel (≈100MB for each