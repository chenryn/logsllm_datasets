### 3.1 Intra-Data Center Networks

Facebook employs two distinct intra-data center network designs: an older cluster-based Clos design [19, 24] and a newer data center fabric design [3]. These are referred to as the cluster network and the fabric network, respectively.

#### Cluster Network Design
In Facebook’s older network (Figure 1, Region A), a cluster is the fundamental unit of network deployment. Each cluster consists of four cluster switches (CSWs, ➀), which aggregate physically contiguous rack switches (RSWs, ➁) via 10 Gb/s Ethernet links. A cluster switch aggregator (CSA, ➂) then aggregates the CSWs, keeping inter-cluster traffic within the data center. Core network devices (Cores, ➃) handle inter-data center traffic by aggregating CSAs.

**Limitations of Cluster Networks:**
1. **Manual Repairs:** Hard-wired switches require manual intervention for repairs. When a port or device becomes unresponsive, a human must inspect and repair it, leading to slow resolution times. This results in fewer available switches, increased bisection bandwidth on the remaining switches, and more network congestion.
2. **Proprietary Switches:** Proprietary switches are difficult to maintain and customize. The proprietary firmware requires technicians trained in the vendor’s software stack to configure and update settings. Once deployed, these switches must be repaired in place, making customization challenging or impossible.

Despite these limitations, cluster networks are still in use in a diminishing fraction of Facebook’s data centers. Eventually, these data centers will transition to the fabric network design.

#### Fabric Network Design
Facebook’s newer network (Figure 1, Region B) addresses the limitations of the cluster network. A pod is the basic unit of network deployment in a fabric network. Unlike the physically contiguous RSWs in a cluster, RSWs in a pod have no physical constraints within the data center. Each RSW (➅) connects to four fabric switches (FSWs, ➆). The 1:4 ratio of RSWs to FSWs maintains the connectivity benefits of the cluster network. Spine switches (SSWs, ➇) aggregate a dynamic number of FSWs, defined by software. Each SSW connects to a set of edge switches (ESWs, ➈). Core network devices (➉) connect ESWs between data centers.

**Key Features of Fabric Networks:**
1. **Simple, Custom Switches:** Fabric devices use simple, commodity chips and avoid proprietary firmware and software.
2. **Fungible Resources:** Devices in a fabric network are not connected in a strict hierarchy. Control software manages FSWs, SSWs, and ESWs as a fungible pool of resources, allowing for dynamic expansion or contraction based on network bandwidth and reliability needs.
3. **Automated Repairs:** Failures in fabric devices can be automatically repaired by software [65]. Centralized management software continuously monitors for device misbehavior. If a problem is detected, the software attempts to perform automated repairs, such as restarting device interfaces, restarting the device itself, or deleting and restoring persistent storage. If the repair fails, a support ticket is generated for human investigation.
4. **Stacked Devices:** The same type of fabric device can be stacked in the same rack to create a higher bandwidth virtual device. Stacking allows for faster scaling of port density compared to proprietary network devices [4–6, 69].

Both cluster and fabric networks use backbone routers (BBRs) located at the edges (➄) to communicate across the WAN backbone and Internet.

### 3.2 Inter-Data Center Networks

Facebook’s physical backbone infrastructure can be abstracted as edge nodes connected through fiber links ( 11 in Figure 1). Edge nodes are the geographical locations where Facebook deploys hardware to route backbone traffic, while fiber links are the physical optical fibers connecting these edges. Each end-to-end fiber link consists of multiple optical segments, with each segment carrying multiple channels, each corresponding to a different wavelength mapped to a specific router port.

**Reliability of Fiber Links:**
The reliability of fiber links is crucial for systems that span multiple data centers, especially those requiring consistency and high availability [7, 16]. Without careful planning, fiber cuts (e.g., due to natural disasters) can cause network partitions, isolating entire data centers or regions. At Facebook, simulations based on the data presented in this section have helped prevent catastrophic network partitions. More commonly, fiber cuts result in the loss of capacity from edges to regions or between two regions, necessitating traffic rerouting, which can increase end-to-end latency.

**WAN Backbone Architectures:**
On top of the physical fiber-based infrastructure, multiple WAN backbone architectures are employed to meet the distinct characteristics and requirements of two types of traffic:

1. **User-Facing Traffic ( 12 in Figure 1):** This traffic connects users of Facebook applications (e.g., www.facebook.com) to software systems running in Facebook data centers. User-facing traffic goes through the broader Internet via peering [76], where ISPs exchange traffic with other Internet domains. User-generated traffic uses the Domain Name System (DNS) to connect users to geographically local servers operated by Facebook called edge presences (also known as points of presence) [68, 76]. From there, user traffic is delivered to Facebook’s data center regions through the backbone network.
2. **Cross-Data Center Traffic ( 13 in Figure 1):** This traffic connects services running in one Facebook data center to services running in another. The backbone network interconnects these data center regions, including both classic and fabric network regions. Cross-data center traffic primarily consists of bulk data transfer streams for replication and consistency, generated by backend services performing batch processing [21, 50], distributed storage [10, 56], and real-time processing [18, 39].

To serve user-facing traffic, the backbone networks support a broad range of protocols and standards to connect to various external networks from different ISPs. Facebook uses a traditional WAN backbone design with backbone routers placed in every edge (the BBRs in Edge 1 through 4 in the WAN backbone, ➄ in Figure 1).

For cross-data center traffic, which is mostly for bulk data transfer, the traffic is partitioned in the optical layer into four planes, with each plane having one backbone router per data center [41]. Inter-data center traffic is managed by software systems employing centralized traffic engineering to manage inter-data center backbone routers built from commodity chips. This design is similar to Google’s B2 and B4 [33, 40].

### 4. Methodology

We next describe how we measure and analyze the reliability of intra and inter-data center networks, including the scope of our study (§4.1), the data sources (§4.2), and the analytical methodology (§4.3).

#### 4.1 Network Incident Definition

**Definition:**
Network incidents are defined as network-level events that have observable impacts (e.g., data corruption, timed out connections, and excessive latency) on Facebook’s production systems, such as frontend web servers [22], caching systems [17, 58], storage systems [10, 56], data processing systems [18, 39], and real-time monitoring systems [43, 61]. Most of these network incidents are tolerated or mitigated by system software and do not cause service-level impact.

**Remediation Prevents Network Incidents:**
Not every network event or failure leads to an incident. Starting in 2013, Facebook began automating the process of remediating common modes of failure for certain network devices, such as RSWs and later FSWs and certain models of Core devices, using an automated repair system [65]. This system shields the infrastructure from the majority of issues in intra-data center networks. Remediation coordinates between using software to repair simple issues and alerting human technicians for complex issues.

**Effects of Automated Remediation:**
While automated repair is used only for RSWs, FSWs, and a small percentage of Core devices, it has provided significant benefits. In a recent month (from April 1, 2018, to May 1, 2018), only 1 out of every 397 issues for RSWs required human intervention. During the same period, only 1 out of every 214 and 1 out of every 4 issues were unable to be fixed by automated repair for FSWs and Core devices, respectively. Note that Core devices, where Facebook has not yet pervasively deployed its own software stack, cannot take as much advantage of automated repair as RSWs and FSWs.

**Characterizing Automated Remediation:**
Table 1 summarizes the automated repair data analyzed and lists additional details per device type. Each repair is assigned a priority from 0 (highest) to 3 (lowest). Core devices, some of the most critical in the data center network, are assigned repairs with the highest priority. In contrast, FSW and RSW repairs are assigned lower priorities on average: 2.25 and 2.22, respectively. The automated repair system schedules repairs based on their priority, with lower-priority repairs waiting longer. Core device repairs have the highest priority and wait only four minutes on average, while FSW and RSW repairs may wait up to three days. The actual repairs themselves happen quickly, taking less than a minute on average. Core device repairs can sometimes be more complex and take around 30.1 seconds on average, while FSW and RSW repairs take only 4.45 and 2.91 seconds on average, respectively.

| Device | Repair Ratio | Avg Priority / Wait / Repair Time |
|--------|--------------|-----------------------------------|
| Core   | 75%         | 0 (highest) / 4 m / 30.1 s        |
| FSW    | 99.5%       | 2.25 / 3 d / 4.45 s               |
| RSW    | 99.7%       | 2.22 / 1 d / 2.91 s               |

**Common Automated Repairs:**
Some network device issues cannot be remediated with software alone and require help from a technician. The most frequent 90% of automated repairs include:
- Device port ping failures (50% of remediations): Repaired by turning the port off and on again.
- Configuration file backup failures (32.4% of remediations): Repaired by restarting the configuration service and reestablishing a secure shell connection.
- Fan failures (4.5% of remediations): Remediated by extracting failure details and alerting a technician to examine the faulty fan.
- Unable to ping the device from a dedicated service (4.0% of remediations): Collects details about the device and assigns a task to a technician.

We focus our analysis on incidents that cannot be solved by automated repair to accurately portray the types of non-trivial failures that cause production impact.

#### 4.2 Service-Level Events (SEVs)

**Dataset:**
We analyze a production incident dataset collected over seven years, from 2011 to 2018, comprising thousands of incidents. Engineers at Facebook routinely document infrastructure incidents, which are called Service-level EVents (SEVs). SEVs fall into three categories of severity: SEV3 (lowest severity, no external outage), SEV2 (moderate severity, limited external outage), and SEV1 (highest severity, widespread external outage). Engineers who responded to a SEV, or whose service was affected, write the report, which includes the root cause, its effect on services, and steps to prevent recurrence [52]. Each SEV undergoes a review process to verify the accuracy and completeness of the report.

**SEV Report Dataset:**
The SEV report dataset resides in a MySQL database, containing reports dating back to January 2011. Network SEVs include details such as the implicated network device, the duration of the incident, and the impact on services (e.g., increased load from lost capacity, message retries from corrupted packets, downtime from partitioned connectivity, and increased latency from congested links). We use SQL queries to analyze the SEV report dataset for our study.

**Representative SEVs:**
- **Switch Crash from Software Bug (SEV3):** A bug in the switching software caused an RSW to crash whenever the software disabled a port. The incident occurred on August 17, 2017, at 11:52 am PDT. An engineer identified the root cause by reproducing the crash and debugging the software, finding that an attempt to allocate a new hardware counter failed, triggering a hardware fault. The bug was fixed on August 22, 2017, at 11:51 am PDT.
- **Traffic Drop from Faulty Hardware Module (SEV2):** A faulty hardware module in a CSA caused traffic to drop on October 25, 2013, between 7:39 am and 7:44 am PDT. After the drop, traffic shifted to alternate network devices, but web and cache servers could not handle the influx, failing 2.4% of requests. Service resumed after five minutes when the servers recovered. An on-site technician diagnosed and replaced the faulty module, verifying the fix on October 26, 2013, at 8:22 am PDT.
- **Data Center Outage from Incorrect Load Balancing (SEV1):** An incorrectly configured load balancing policy in a DR caused a data center network outage on January 25, 2012, at 3:46 am PST. Following a software upgrade, the DR routed traffic on a single path, overloading the associated ports. Site reliability engineers detected the incident with alarms and attempted to downgrade the software, but the load remained imbalanced. An engineer resolved the issue by correcting the load balancing policy.