3.1 Intra Data Center Networks
Facebook uses two intra data center network designs: an older
cluster-based Clos design [19, 24] and an newer data center fabric
design [3]. We call these the cluster network and the fabric network.
Unlike the cluster network, the fabric network uses a five-stage
Folded Clos [1], built from simple commodity hardware, with auto-
mated repairs.
Cluster network design. In Facebook’s older network (Figure 1,
Region A), a cluster is the basic unit of network deployment. Each
cluster comprises four cluster switches (CSWs, ➀), each of which ag-
gregates physically contiguous rack switches (RSW, ➁) via 10 Gb/s
Ethernet links. In turn, a cluster switch aggregator (CSA, ➂) aggre-
gates CSWs and keeps inter cluster traffic within the data center.
Inter data center traffic flows through core network devices (Cores,
➃), which aggregate CSAs.
A cluster network has two main limitations:
(1) Hard wired switches require manual repair. When a port
becomes unresponsive, a human must inspect and repair
the port. When a device becomes unresponsive, a human
must power cycle the device. Compared to software, humans
perform slow repairs. Slow repairs mean fewer switches to
route requests, higher bisection bandwidth on the remaining
switches, and more congestion in the network.
(2) Proprietary switches are challenging to maintain and cus-
tomize. The proprietary firmware on switches requires tech-
nicians trained in the vendor’s software stack to configure
and update settings. Proprietary software on switches makes
customization difficult or impossible. Once deployed, propri-
etary switches must be repaired in-place.
Despite its limitations, the cluster networks remain in use in a
dwindling fraction of Facebook’s data centers. Ultimately, these
data centers will join new data centers in using the fabric network
design.
Fabric network design. Facebook’s newer network (Figure 1,
Region B) addresses the cluster network’s limitations. A pod is the
basic unit of network deployment in a fabric network. Unlike the
physically contiguous RSWs in a cluster, RSWs in a pod have no
physical constraints within a data center. Each RSW (➅) connects
to four fabric switches (FSWs, ➆). The 1:4 ratio of RSWs to FSWs
maintains the connectivity benefits of the cluster network. Spine
switches (SSWs, ➇) aggregate a dynamic number of FSWs, defined
by software. Each SSW connects to a set of edge switches (ESWs, ➈).
Core network devices (➉) connect ESWs between data centers.
Fabric networks are managed by software and differ from cluster
networks in three ways:
(1) Simple, custom switches. Fabric devices contain simple,
commodity chips and eschew proprietary firmware and soft-
ware.
(2) Fungible resources. The devices in a fabric network are not
connected in a strict hierarchy. Control software manages
FSWs, SSWs, and ESWs like a fungible pool of resources.
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
J.J. Meza et al.
Figure 1: Facebook’s network architecture, explained in §3. Data centers use either an older cluster network or a newer fabric
network. Cluster networks and fabric networks communicate across the WAN backbone and Internet.
Resources dynamically expand or contract based on network
bandwidth and reliability needs.
(3) Automated repairs. Failures on data center fabric devices
can be repaired automatically by software [65]. Centralized
management software continuously checks for device mis-
behavior. A skipped heartbeat or an inconsistent network
setting raise alarms for management software to handle.
Management software triages the problem and attempts to
perform automated repairs. Repairs include restarting de-
vice interfaces, restarting the device itself, and deleting and
restoring a device’s persistent storage. If the repair fails, man-
agement software opens a support ticket for investigation
by a human.
(4) Stacked devices. The same type of fabric device can be
stacked in the same rack to create a higher bandwidth virtual
device. Stacking enables port density in fabric networks to
scale faster than proprietary network devices [4–6, 69].
Both cluster networks and fabric networks use backbone routers
(BBRs) located in edges (➄) to communicate across the WAN back-
bone and Internet.
3.2 Inter Data Center Networks
Facebook’s physical backbone infrastructure can be abstracted as
edge nodes connected through fiber links ( 11 in Figure 1). Edge nodes
are the geographical location where Facebook deploys hardware
to route backbone traffic, while fiber links are abstraction of phys-
ical optical fibers that connects the edges. Each end-to-end fiber
link is embodied by optical circuits that consist of multiple optical
segments. An optical segment corresponds to a fiber and carries
multiple channels, where each channel corresponds to a different
wavelength mapped to a specific router port.
The reliability of fiber links is critically important to systems
that run across multiple data centers, especially those requiring
consistency and high-availability [7, 16]. Without careful planning,
fiber cuts (e.g., due to natural disasters) would cause network parti-
tions that cut off an entire data center or region from the rest of the
network. At Facebook, we have not seen catastrophic network par-
titions that disconnect data centers from each other. This is in part
due to network planning decisions made from simulations based
on the data presented in this section. The more common results of
fiber cuts are the loss of capacity from edges to regions or between
two regions. In this case, we have to reroute the traffic using other
available links, which could increase end-to-end latency.
On top of the physical fiber-based infrastructure, multiple WAN
backbone architectures are employed to satisfy the distinct charac-
teristics and requirements of two types of traffic labeled in Figure 1:
(1) user-facing traffic and (2) cross data center traffic.
• User-facing traffic ( 12 in Figure 1) connects a person using
Facebook applications like those hosted at www.facebook.com,
to software systems running in Facebook data centers. In
order to reach a Facebook data center, user-facing traffic
goes through the broader Internet via a process known as
peering [76]. There, Internet Service Providers (ISPs) exchange
traffic with other Internet domains. User-generated traffic
uses the Domain Name System (DNS) to connect users to
geographically local servers operated by Facebook called
edge presences (also known as points of presence) [68, 76].
From there, user traffic is delivered to Facebook’s data center
regions through the backbone network.
• Cross data center traffic ( 13 in Figure 1) connects a service
running in one Facebook data center to a service running
in another Facebook data center. The backbone network in-
terconnects those data center regions, including both the
classic network and the fabric network regions. By volume,
cross data center traffic consists primarily by bulk data trans-
fer streams of data such as for replication and consistency.
These are generated by back end services that perform batch
CoreFSWESWSSWFSWESWSSWRSWServerrackRSWServerrackRSWServerrackRSWServerrack…Datacenter 3CoreFSWESWSSWFSWESWSSWRSWServerrackRSWServerrackRSWServerrackRSWServerrack…Datacenter 4Region B(datacenter fabric)CoreCSWCSACSWCSARSWServerrackRSWServerrackRSWServerrackRSWServerrack…Datacenter 1Datacenter 2Region A(classic cluster-based)CoreCSWCSACSWCSARSWServerrackRSWServerrackRSWServerrackRSWServerrack…BBRBBRBBRBBRWANbackboneInternetCoreBBRLegendBackbone routerCore network routerCSACSW aggregatorCSWCluster switchESWEdge switchSSWSpine switchFSWFabric switchRSWTop-of-rack switchSoftwaremanagedSoftwaremanaged………………………………12345678910Fiber linkUser-facing trafficCross data center trafficEdge 1Edge 2Edge 3Edge 4Traffic121311A Large Scale Study of Data Center Network Reliability
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
processing [21, 50], distributed storage [10, 56], and real-time
processing [18, 39], for example.
To serve user-facing traffic, the backbone networks support a
broad range of protocols and standards to connect to a wide variety
of external networks from different ISPs. Facebook uses a traditional
WAN backbone design that consists of backbone routers placed in
every edge (the BBRs in Edge 1 through 4 in the WAN backbone,
➄ in Figure 1).
On the other hand, as cross data center traffic is mostly for bulk
data transfer, the traffic is only destined to a few dozen data centers
and is partitioned in the optical layer in four planes where each
plane has one backbone router per data center [41]. Moreover,
inter data center traffic is managed by software systems where
centralized traffic engineering is employed to manage inter data
center backbone routers built from commodity chips. The design
in principle is similar as Google’s B2 and B4 described in [33, 40].
4 METHODOLOGY
We next describe how we measure and analyze the reliability of
intra and inter data center networks, including the scope of our
study (§4.1), the data sources (§4.2), and the analytical methodology
(§4.3).
4.1 Network Incident Definition
We define network incidents as network-level events that have
observable impact (e.g., data corruption, timed out connections,
and excessive latency) on Facebook’s production systems, such as
our frontend web servers [22], caching systems [17, 58], storage
systems [10, 56], data processing systems [18, 39], and real-time
monitoring systems [43, 61]. Most of these network incidents are
tolerated or mitigated by the system software and do not cause
service-level impact.
4.1.1 Remediation Prevents Network Incidents. Not every net-
work event or failure leads to an incident. Starting in 2013, Facebook
began to automate the process of remediating common modes of
failure for certain network devices, such as RSWs and later FSWs
and certain models of Core devices, using an automated repair sys-
tem [65]. Facebook relies on this automated repair system to shield
our infrastructure from the vast majority of issues that arise in our
intra data center networks. Remediation coordinates between using
software to repair simple issues and alerting human technicians to
repair complex issues.
4.1.2 The Effects of Automated Remediation. While automated
repair is employed only for RSWs, FSWs, and a small percentage
of Core devices, it has provided significant benefits. In a recent
month (from April 1, 2018 to May 1, 2018), only 1 out of every 397
issues for RSWs was unable to be fixed by automated repair and
required human intervention. During the same period, only 1 out
of every 214 and 1 out of every 4 issues were unable to be fixed by
automated repair for FSWs and Core devices, respectively. Note that
Core devices, where Facebook has not yet pervasively deployed
its own software stack, are unable to take as much advantage of
automated repair as RSWs and FSWs.
4.1.3 Characterizing Automated Remediation. Table 1 summa-
rizes the automated repair data we analyzed and also lists additional
details per device type. Each repair is assigned a priority from 0
(highest priority) to 3 (lowest priority). We see that Core devices,
some of the most critical devices in the data center network, are
assigned repairs with the highest priority. In contrast, FSW and
RSW repairs are assigned lower priorities on average: 2.25 and 2.22,
respectively. The automated repair system uses a repair’s priority
to schedule when the repair should take place. Repairs assigned
a lower priority wait longer than repairs assigned a higher prior-
ity. Core device repairs have the highest priority and only have to
wait four minutes on average. FSW and FSW repairs have a much
lower priority and wait up to three days for repair. The repairs
themselves happen relatively quickly, taking less than a minute on
average. Core device repairs can sometimes be more complex and
take around 30.1 seconds on average. FSW and RSW repairs take
only 4.45 and 2.91 seconds on average, respectively.
Device Repair Ratio Avg Priority / Wait / Repair Time
Core
FSW
RSW
2.25 / 3 d / 4.45 s
2.22 / 1 d / 2.91 s
0 (highest) / 4 m / 30.1 s
75%
99.5%
99.7%
Table 1: The repair ratio (fraction of issues repaired with au-
tomated remediation versus those which ultimately led to
a network incident) and average priority (0 = highest, 3 =
lowest), average wait time, and average repair time for the
network device types that support automated remediation.
Some network device issues cannot be remediated with software
alone and require help from a technician to repair. The most fre-
quent 90% of automated repairs are: device port ping failures that
are repaired by turning the port off and on again (50% of remedia-
tions), configuration file backup failures are repaired by restarting
the configuration service and reestablishing a secure shell connec-
tion (32.4% of remediations), fan failures which are remediated by
extracting failure details and alerting a technician to examine the
faulty fan (4.5% of remediations), unable to ping the device from
a dedicated service to monitor device liveness which collects de-
tails about the device and assigns a task to a technician (4.0% of
remediations).
We focus our analysis on the class of incidents that can not be
solved by automated repair. Our intent is to portray, as accurately
as possible, the types of non-trivial failures that cause production
impact.
4.2 Service-Level Events (SEVs)
We analyze a production incident dataset collected over seven years,
from 2011 to 2018. The dataset comprises thousands of incidents.
We describe the dataset structure next.
Engineers at Facebook routinely document infrastructure inci-
dents. Facebook calls such incidents Service-level EVents (SEVs2).
SEVs fall into three categories of severity ranging from SEV3 (lowest
severity, no external outage) to SEV1 (highest severity, widespread
external outage). Engineers who responded to a SEV, or whose
service the SEV affected, write its report. The report contains the
2Pronounced [sEv] in International Phonetic Alphabet, rhyming with dev.
IMC ’18, October 31–November 2, 2018, Boston, MA, USA
J.J. Meza et al.
incident’s root cause, the root cause’s affect on services, and steps
to prevent the incident from happening again [52]. Each SEV goes
through a review process to verify the accuracy and completeness of
the report. SEV reports help engineers at Facebook prevent similar
incidents from happening again.
The SEV report dataset resides in a MySQL database. The data-
base contains reports dating to January 2011. Network SEVs contain
details on the incident: the network device implicated in the inci-
dent, the duration of the incident (measured from when the root
cause manifested until when engineers fixed the root cause), the
incident’s affects on services (for example, increased load from lost
capacity, message retries from corrupted packets, downtime from
partitioned connectivity, and increased latency from congested
links). We use SQL queries to analyze the SEV report dataset for
our study.
SEVs come in a variety of shapes and sizes. We summarize three
representative SEVs with differing levels of service affect:
Switch crash from software bug (SEV3): A bug in the switch-
ing software triggered an RSW to crash whenever the software dis-
abled a port. The incident occurred on August 17, 2017 at 11:52 am
PDT after an engineer updated the software on a RSW and noticed
the behavior. The engineer identified the root cause by reproducing
the crash and debugging the software: an attempt to allocate a new
hardware counter failed, triggering a hardware fault. On August
22, 2017 at 11:51 am PDT the engineer fixed the bug and confirmed
the fix.
Traffic drop from faulty hardware module (SEV2): A faulty
hardware module in a CSA caused traffic to drop on October 25,
2013 between 7:39 am PDT and 7:44 am PDT. After the drop, traffic
shifted rapidly to alternate network devices. Web servers and cache
servers, unable to handle the influx of load, exhausted their CPU
and failed 2.4% of requests. Service resumed normally after five
minutes when web servers and cache servers recovered. An on-site
technician diagnosed the problem, replaced the faulty hardware
module, verified the fix, and closed the SEV on October 26, 2013 at
8:22 am PDT.
Data center outage from incorrect load balancing (SEV1): A
DR with an incorrectly configured load balancing policy caused
a data center network outage on January 25, 2012 at 3:46 am PST.
Following a software upgrade, a DR began routing traffic on a single
path, overloading the ports associated with the path. The overload
at the DR level caused a data center outage. Site reliability engineers
detected the incident with alarms. Engineers working on the DR
immediately attempted to downgrade the software. Despite the
downgrade, DR load remained imbalanced. An engineer resolved