### Window Management and Thread State Restoration

A window is considered open when a thread becomes active, i.e., starts processing a message. Conversely, the window is forcefully closed when the same thread becomes inactive, meaning it explicitly yields to other threads. Restoring the state from a crashed server also restores the state of the inactive threads. However, the active thread requires special handling. During a checkpoint, we call the thread library to force the context to be saved. When restoring a checkpoint, the thread library assumes the crashed thread is still running, while in reality, the server is starting in the main thread. To resolve this, we call a function to correct the current thread variable and add the crashed thread back to the run queue. After these steps, the thread library returns to a consistent state, allowing the server to resume operation.

### Implementation

We implemented our OSIRIS prototype on top of the MINIX 3 microkernel-based operating system architecture [8]. The core operating system consists of several OS user-space processes (referred to as system servers) isolated from each other using the MMU and a small microkernel that performs privileged low-level operations such as scheduling and message passing. This design adheres to the principle of least authority (POLA), which minimizes fault consequences through memory isolation and by restricting each component to only the necessary operations. This compartmentalized design is crucial for fault isolation, a key prerequisite for OSIRIS' crash recovery strategy, which prevents uncontrolled fault propagation across OS components.

To support our checkpointing algorithm, we need to update the undo log for each memory write, which introduces some overhead.

The core system servers in our OSIRIS prototype include:
- **Process Manager (PM):** Manages processes and signals.
- **Virtual Memory Manager (VM):** Manages virtual memory.
- **Virtual Filesystem Server (VFS):** Provides a virtual file system interface.
- **Data Store (DS):** Provides a persistent key-value store service.
- **Recovery Server (RS):** Detects and restores crashed OS components.

The VFS server in our prototype is multithreaded to prevent slow disk operations from blocking the system. Our prototype allows all these core system components, including RS itself, to be recovered in case of crashes using our design.

Additionally, OSIRIS includes a set of LLVM link-time instrumentation passes and static libraries, totaling 6,496 lines of code (LOC). The Recovery Server implements the restart phase described in Section IV-C. The static libraries implement our checkpointing, rollback, and reconciliation mechanisms. Compile-time settings allow SEEPs to be mapped to their corresponding reconciliation mechanisms, defining the recovery policies supported by our system. To enable OSIRIS' recovery functionalities, we link every system component against our static libraries and instrument (and optimize) each component using our LLVM link-time passes.

### Reliable Computing Base

The Reliable Computing Base (RCB) [34] consists of the parts of the system that we need to trust to be free of faults. In OSIRIS, the RCB includes mechanisms that implement:
1. **Checkpointing:** Maintaining a simple per-request undo log.
2. **Restartability:** Maintaining clones of OS components, transferring state, and replacing crashed components.
3. **Recovery window management:** Tracking whether the per-component recovery window is open or not.
4. **Initialization:** Calling a component-specific function to initialize the local state before entering the request processing loop.
5. **Message passing substrate:** The underlying microkernel in our prototype.

OSIRIS has a total of 237,270 LOC, with the RCB accounting for 29,732 LOC, or 12.5% of the entire code base.

### Evaluation

We evaluate our system in terms of recovery coverage (Section VI-A), survivability (Section VI-B), performance (Section VI-C), and service disruption guarantees (Section VI-E). For our experiments, we use two different workloads: Unixbench [35] for performance evaluation and a homegrown set of 89 programs for recovery and survivability tests, referred to as the prototype test suite.

We use four recovery policies to evaluate OSIRIS:
1. **Pessimistic**
2. **Enhanced**
3. **Stateless restart:** Serves as a baseline to compare against existing "microreboot systems" operating stateless recovery.
4. **Naive recovery:** Serves as a baseline to compare against best-effort recovery strategies with no special handling.

#### A. Recovery Coverage

To measure the opportunity for recovery under our chosen recovery models, we measure the cumulative execution time each server spends inside and outside the recovery window while executing the prototype test suite. We count the number of basic blocks covered during the execution in each of the five servers and compute the recovery coverage as the fraction of the number of basic blocks executed inside recovery windows out of the total number of basic blocks executed in the servers. Table I presents the results for our pessimistic and enhanced recovery policies. As shown in the table, the execution spends a mean of 57.7% and 68.4% of the execution time across all the servers inside recovery windows, respectively.

| Server | Pessimistic (%) | Enhanced (%) |
|--------|-----------------|--------------|
| PM     | 54.9            | 72.3         |
| VFS    | 64.6            | 47.1         |
| VM     | 49.4            | 61.7         |
| DS     | 72.3            | 64.6         |
| RS     | 92.8            | 50.5         |

As shown in the table, DS has the lowest recovery coverage in pessimistic mode and the highest in enhanced mode. This indicates the presence of a SEEP fairly early in DSâ€™ request processing loop, which is non-state-modifying in enhanced mode. DS is a relatively simple server, rarely issuing state-modifying calls to the rest of the system, making it almost always recoverable. Since enhanced mode allows SEEPs that perform read-only interactions with other components to keep recovery windows open, the increase in recovery coverage for PM can be explained by the many read-mostly system calls it implements. This property applies to many other OS components, and overall, our system can be recovered 68.4% of the time, indicating that OSIRIS can guarantee safe recovery in the majority of cases.

#### B. Survivability

To demonstrate improved survivability of the system in the presence of faults, we conduct large-scale fault injection experiments. We boot our prototype inside a virtual machine and execute our prototype test suite using a modified QEMU, which logs the status of the system and outcomes of the tests. We use EDFI [37] to inject the faults and perform a separate profiling run to determine which fault candidates are triggered by our prototype test suite, excluding those triggered during boot time or not at all. Boot-time errors are unrealistic and would be removed in the testing phase, while untriggered faults would inflate the statistics with runs in which no recovery is needed.

We performed the experiments in eight different settings, combining two different fault models and four different recovery models. The first fault model consists only of fail-stop errors (dereferencing a NULL pointer), while the second uses the full set of realistic software faults available in EDFI. Tables II and III show the performance of our recovery system under fault injection for the fail-stop and full EDFI fault models, respectively. We injected a total of 757 fail-stop faults and 992 full EDFI faults, each in a separate run. The outcomes of the runs are classified into four groups: "pass" (test suite completed and all tests passed), "fail" (test suite completed but one or more tests failed), "shutdown" (non-recoverable fault detected and controlled shutdown performed), and "crash" (uncontrolled crash or hang).

| Recovery Mode | Pass (%) | Fail (%) | Shutdown (%) | Crash (%) |
|---------------|----------|----------|--------------|-----------|
| Stateless     | 19.6     | 0.0      | 80.4         | 0.0       |
| Naive         | 20.6     | 2.4      | 77.0         | 0.2       |
| Pessimistic   | 18.5     | 0.0      | 81.3         | 0.2       |
| Enhanced      | 25.6     | 6.5      | 66.1         | 1.9       |

| Recovery Mode | Pass (%) | Fail (%) | Shutdown (%) | Crash (%) |
|---------------|----------|----------|--------------|-----------|
| Stateless     | 47.8     | 10.5     | 38.2         | 41.7      |
| Naive         | 48.5     | 11.9     | 32.9         | 39.6      |
| Pessimistic   | 47.3     | 10.5     | 4.0          | 4.0       |
| Enhanced      | 50.4     | 12.0     | 4.8          | 4.8       |

With fail-stop errors, the enhanced recovery mode provides significantly better survivability than all other modes. For full EDFI faults, the enhanced mode also shows improved survivability, though the improvement is less pronounced.

#### C. Performance

For performance evaluation, we rely on Unixbench [35], which is specifically designed and widely used to measure OS performance. Table IV compares the performance of Linux and OSIRIS on various benchmarks.

| Benchmark     | Linux     | OSIRIS   |
|---------------|-----------|----------|
| dhry2reg      | 1,707.8   | 357.7    |
| whetstone-double | 464.1   | 200.4    |
| execl         | 1,006.4   | 1,171.0  |
| fstime        | 2,975.8   | 1,106.0  |
| fsbuffer      | 320.7     | 1,299.0  |
| fsdisk        | 1,398.9   | 106.8    |
| pipe          | 1,143.3   | 65.2     |
| context1      | 1,590.2   | 260.3    |
| spawn         | 1,204.5   | 36.5     |
| syscall       | 122.5     | 46.3     |
| shell1        | 430.1     | 385.2    |
| shell8        | 1,605.3   | 45.9     |
| geomean       | 873.5     | 207.9    |

While OSIRIS shows lower performance compared to Linux in most benchmarks, it provides robust recovery and fault tolerance, which is a trade-off for the added reliability and survivability.