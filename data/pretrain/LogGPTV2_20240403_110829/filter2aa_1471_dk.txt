FILE_FLAG_RANDOM_ACCESS flag can be specified when the CreateFile
function is called. This flag instructs the cache manager not to attempt to
predict where the application is reading next and thus disables read-ahead.
The flag also stops the cache manager from aggressively unmapping views of
the file as the file is accessed so as to minimize the mapping/unmapping
activity for the file when the application revisits portions of the file.
Read-ahead enhancements
Windows 8.1 introduced some enhancements to the cache manager read-
ahead functionality. File system drivers and network redirectors can decide
the size and growth for the intelligent read-ahead with the
CcSetReadAheadGranularityEx API function. The cache manager client can
decide the following:
■    Read-ahead granularity Sets the minimum read-ahead unit size and
the end file-offset of the next read-ahead. The cache manager sets the
default granularity to 4 Kbytes (the size of a memory page), but every
file system sets this value in a different way (NTFS, for example, sets
the cache granularity to 64 Kbytes).
Figure 11-13 shows an example of read-ahead on a 200 Kbyte-sized
file, where the cache granularity has been set to 64 KB. If the user
requests a nonaligned 1 KB read at offset 0x10800, and if a sequential
read has already been detected, the intelligent read-ahead will emit an
I/O that encompasses the 64 KB of data from offset 0x10000 to
0x20000. If there were already more than two sequential reads, the
cache manager emits another supplementary read from offset
0x20000 to offset 0x30000 (192 Kbytes).
Figure 11-13 Read-ahead on a 200 KB file, with granularity set to
64KB.
■    Pipeline size For some remote file system drivers, it may make sense
to split large read-ahead I/Os into smaller chunks, which will be
emitted in parallel by the cache manager worker threads. A network
file system can achieve a substantial better throughput using this
technique.
■    Read-ahead aggressiveness File system drivers can specify the
percentage used by the cache manager to decide how to increase the
read-ahead size after the detection of a third sequential read. For
example, let’s assume that an application is reading a big file using a
1 Mbyte I/O size. After the tenth read, the application has already
read 10 Mbytes (the cache manager may have already prefetched
some of them). The intelligent read-ahead now decides by how much
to grow the read-ahead I/O size. If the file system has specified 60%
of growth, the formula used is the following:
(Number of sequential reads * Size of last read) * (Growth percentage
/ 100)
So, this means that the next read-ahead size is 6 MB (instead of being
2 MB, assuming that the granularity is 64 KB and the I/O size is 1
MB). The default growth percentage is 50% if not modified by any
cache manager client.
Write-back caching and lazy writing
The cache manager implements a write-back cache with lazy write. This
means that data written to files is first stored in memory in cache pages and
then written to disk later. Thus, write operations are allowed to accumulate
for a short time and are then flushed to disk all at once, reducing the overall
number of disk I/O operations.
The cache manager must explicitly call the memory manager to flush
cache pages because otherwise the memory manager writes memory contents
to disk only when demand for physical memory exceeds supply, as is
appropriate for volatile data. Cached file data, however, represents
nonvolatile disk data. If a process modifies cached data, the user expects the
contents to be reflected on disk in a timely manner.
Additionally, the cache manager has the ability to veto the memory
manager’s mapped writer thread. Since the modified list (see Chapter 5 of
Part 1 for more information) is not sorted in logical block address (LBA)
order, the cache manager’s attempts to cluster pages for larger sequential
I/Os to the disk are not always successful and actually cause repeated seeks.
To combat this effect, the cache manager has the ability to aggressively veto
the mapped writer thread and stream out writes in virtual byte offset (VBO)
order, which is much closer to the LBA order on disk. Since the cache
manager now owns these writes, it can also apply its own scheduling and
throttling algorithms to prefer read-ahead over write-behind and impact the
system less.
The decision about how often to flush the cache is an important one. If the
cache is flushed too frequently, system performance will be slowed by
unnecessary I/O. If the cache is flushed too rarely, you risk losing modified
file data in the cases of a system failure (a loss especially irritating to users
who know that they asked the application to save the changes) and running
out of physical memory (because it’s being used by an excess of modified
pages).
To balance these concerns, the cache manager’s lazy writer scan function
executes on a system worker thread once per second. The lazy writer scan
has different duties:
■    Checks the number of average available pages and dirty pages (that
belongs to the current partition) and updates the dirty page threshold’s
bottom and the top limits accordingly. The threshold itself is updated
too, primarily based on the total number of dirty pages written in the
previous cycle (see the following paragraphs for further details). It
sleeps if there are no dirty pages to write.
■    Calculates the number of dirty pages to write to disk through the
CcCalculatePagesToWrite internal routine. If the number of dirty
pages is more than 256 (1 MB of data), the cache manager queues
one-eighth of the total dirty pages to be flushed to disk. If the rate at
which dirty pages are being produced is greater than the amount the
lazy writer had determined it should write, the lazy writer writes an
additional number of dirty pages that it calculates are necessary to
match that rate.
■    Cycles between each shared cache map (which are stored in a linked
list belonging to the current partition), and, using the internal
CcShouldLazyWriteCacheMap routine, determines if the current file
described by the shared cache map needs to be flushed to disk. There
are different reasons why a file shouldn’t be flushed to disk: for
example, an I/O could have been already initialized by another thread,
the file could be a temporary file, or, more simply, the cache map
might not have any dirty pages. In case the routine determined that the
file should be flushed out, the lazy writer scan checks whether there
are still enough available pages to write, and, if so, posts a work item
to the cache manager system worker threads.
 Note
The lazy writer scan uses some exceptions while deciding the number of
dirty pages mapped by a particular shared cache map to write (it doesn’t
always write all the dirty pages of a file): If the target file is a metadata
stream with more than 256 KB of dirty pages, the cache manager writes
only one-eighth of its total pages. Another exception is used for files that
have more dirty pages than the total number of pages that the lazy writer
scan can flush.
Lazy writer system worker threads from the systemwide critical worker
thread pool actually perform the I/O operations. The lazy writer is also aware
of when the memory manager’s mapped page writer is already performing a
flush. In these cases, it delays its write-back capabilities to the same stream
to avoid a situation where two flushers are writing to the same file.
 Note
The cache manager provides a means for file system drivers to track when
and how much data has been written to a file. After the lazy writer flushes
dirty pages to the disk, the cache manager notifies the file system,
instructing it to update its view of the valid data length for the file. (The
cache manager and file systems separately track in memory the valid data
length for a file.)
EXPERIMENT: Watching the cache manager in
action
In this experiment, we use Process Monitor to view the underlying
file system activity, including cache manager read-ahead and write-
behind, when Windows Explorer copies a large file (in this
example, a DVD image) from one local directory to another.
First, configure Process Monitor’s filter to include the source
and destination file paths, the Explorer.exe and System processes,
and the ReadFile and WriteFile operations. In this example, the
C:\Users\Andrea\Documents\Windows_10_RS3.iso file was copied
to C:\ISOs\ Windows_10_RS3.iso, so the filter is configured as
follows:
You should see a Process Monitor trace like the one shown here
after you copy the file:
The first few entries show the initial I/O processing performed
by the copy engine and the first cache manager operations. Here
are some of the things that you can see:
■    The initial 1 MB cached read from Explorer at the first
entry. The size of this read depends on an internal matrix
calculation based on the file size and can vary from 128 KB
to 1 MB. Because this file was large, the copy engine chose
1 MB.
■    The 1-MB read is followed by another 1-MB noncached
read. Noncached reads typically indicate activity due to
page faults or cache manager access. A closer look at the
stack trace for these events, which you can see by double-
clicking an entry and choosing the Stack tab, reveals that
indeed the CcCopyRead cache manager routine, which is
called by the NTFS driver’s read routine, causes the
memory manager to fault the source data into physical
memory:
■    After this 1-MB page fault I/O, the cache manager’s read-
ahead mechanism starts reading the file, which includes the
System process’s subsequent noncached 1-MB read at the
1-MB offset. Because of the file size and Explorer’s read
I/O sizes, the cache manager chose 1 MB as the optimal
read-ahead size. The stack trace for one of the read-ahead
operations, shown next, confirms that one of the cache
manager’s worker threads is performing the read-ahead.
After this point, Explorer’s 1-MB reads aren’t followed by page
faults, because the read-ahead thread stays ahead of Explorer,
prefetching the file data with its 1-MB noncached reads. However,
every once in a while, the read-ahead thread is not able to pick up
enough data in time, and clustered page faults do occur, which
appear as Synchronous Paging I/O.
If you look at the stack for these entries, you’ll see that instead
of MmPrefetchForCacheManager, the
MmAccessFault/MiIssueHardFault routines are called.
As soon as it starts reading, Explorer also starts performing
writes to the destination file. These are sequential, cached 1-MB
writes. After about 124 MB of reads, the first WriteFile operation
from the System process occurs, shown here:
The write operation’s stack trace, shown here, indicates that the
memory manager’s mapped page writer thread was actually
responsible for the write. This occurs because for the first couple of
megabytes of data, the cache manager hadn’t started performing
write-behind, so the memory manager’s mapped page writer began
flushing the modified destination file data. (See Chapter 10 for
more information on the mapped page writer.)
To get a clearer view of the cache manager operations, remove
Explorer from the Process Monitor’s filter so that only the System
process operations are visible, as shown next.
With this view, it’s much easier to see the cache manager’s 1-
MB write-behind operations (the maximum write sizes are 1 MB
on client versions of Windows and 32 MB on server versions; this
experiment was performed on a client system). The stack trace for
one of the write-behind operations, shown here, verifies that a
cache manager worker thread is performing write-behind:
As an added experiment, try repeating this process with a remote
copy instead (from one Windows system to another) and by
copying files of varying sizes. You’ll notice some different
behaviors by the copy engine and the cache manager, both on the
receiving and sending sides.
Disabling lazy writing for a file
If you create a temporary file by specifying the flag
FILE_ATTRIBUTE_TEMPORARY in a call to the Windows CreateFile
function, the lazy writer won’t write dirty pages to the disk unless there is a
severe shortage of physical memory or the file is explicitly flushed. This
characteristic of the lazy writer improves system performance—the lazy
writer doesn’t immediately write data to a disk that might ultimately be
discarded. Applications usually delete temporary files soon after closing
them.
Forcing the cache to write through to disk
Because some applications can’t tolerate even momentary delays between
writing a file and seeing the updates on disk, the cache manager also supports
write-through caching on a per-file object basis; changes are written to disk
as soon as they’re made. To turn on write-through caching, set the
FILE_FLAG_WRITE_THROUGH flag in the call to the CreateFile function.
Alternatively, a thread can explicitly flush an open file by using the Windows
FlushFileBuffers function when it reaches a point at which the data needs to
be written to disk.
Flushing mapped files
If the lazy writer must write data to disk from a view that’s also mapped into
another process’s address space, the situation becomes a little more
complicated because the cache manager will only know about the pages it has
modified. (Pages modified by another process are known only to that process
because the modified bit in the page table entries for modified pages is kept
in the process private page tables.) To address this situation, the memory
manager informs the cache manager when a user maps a file. When such a
file is flushed in the cache (for example, as a result of a call to the Windows
FlushFileBuffers function), the cache manager writes the dirty pages in the
cache and then checks to see whether the file is also mapped by another
process. When the cache manager sees that the file is also mapped by another
process, the cache manager then flushes the entire view of the section to write
out pages that the second process might have modified. If a user maps a view
of a file that is also open in the cache, when the view is unmapped, the
modified pages are marked as dirty so that when the lazy writer thread later
flushes the view, those dirty pages will be written to disk. This procedure
works as long as the sequence occurs in the following order:
1. 
A user unmaps the view.
2. 
A process flushes file buffers.
If this sequence isn’t followed, you can’t predict which pages will be
written to disk.
EXPERIMENT: Watching cache flushes
You can see the cache manager map views into the system cache
and flush pages to disk by running the Performance Monitor and
adding the Data Maps/sec and Lazy Write Flushes/sec counters.
(You can find these counters under the “Cache” group.) Then, copy
a large file from one location to another. The generally higher line
in the following screenshot shows Data Maps/sec, and the other
shows Lazy Write Flushes/sec. During the file copy, Lazy Write
Flushes/sec significantly increased.
Write throttling
The file system and cache manager must determine whether a cached write
request will affect system performance and then schedule any delayed writes.
First, the file system asks the cache manager whether a certain number of
bytes can be written right now without hurting performance by using the
CcCanIWrite function and blocking that write if necessary. For asynchronous
I/O, the file system sets up a callback with the cache manager for
automatically writing the bytes when writes are again permitted by calling
CcDeferWrite. Otherwise, it just blocks and waits on CcCanIWrite to
continue. Once it’s notified of an impending write operation, the cache
manager determines how many dirty pages are in the cache and how much
physical memory is available. If few physical pages are free, the cache
manager momentarily blocks the file system thread that’s requesting to write
data to the cache. The cache manager’s lazy writer flushes some of the dirty
pages to disk and then allows the blocked file system thread to continue. This
write throttling prevents system performance from degrading because of a
lack of memory when a file system or network server issues a large write
operation.
 Note
The effects of write throttling are volume-aware, such that if a user is
copying a large file on, say, a RAID-0 SSD while also transferring a
document to a portable USB thumb drive, writes to the USB disk will not
cause write throttling to occur on the SSD transfer.
The dirty page threshold is the number of pages that the system cache will
allow to be dirty before throttling cached writers. This value is computed
when the cache manager partition is initialized (the system partition is
created and initialized at phase 1 of the NT kernel startup) and depends on
the product type (client or server). As seen in the previous paragraphs, two
other values are also computed—the top dirty page threshold and the bottom
dirty page threshold. Depending on memory consumption and the rate at
which dirty pages are being processed, the lazy writer scan calls the internal
function CcAdjustThrottle, which, on server systems, performs dynamic
adjustment of the current threshold based on the calculated top and bottom
values. This adjustment is made to preserve the read cache in cases of a
heavy write load that will inevitably overrun the cache and become throttled.
Table 11-1 lists the algorithms used to calculate the dirty page thresholds.
Table 11-1 Algorithms for calculating the dirty page thresholds
Product 
Type
Dirty Page 
Threshold
Top Dirty Page 
Threshold
Bottom Dirty Page 
Threshold
Client
Physical pages 
/ 8
Physical pages / 8
Physical pages / 8
Server
Physical pages 
/ 2
Physical pages / 2
Physical pages / 8
Write throttling is also useful for network redirectors transmitting data
over slow communication lines. For example, suppose a local process writes
a large amount of data to a remote file system over a slow 640 Kbps line. The
data isn’t written to the remote disk until the cache manager’s lazy writer
flushes the cache. If the redirector has accumulated lots of dirty pages that
are flushed to disk at once, the recipient could receive a network timeout
before the data transfer completes. By using the CcSetDirtyPageThreshold
function, the cache manager allows network redirectors to set a limit on the
number of dirty cache pages they can tolerate (for each stream), thus
preventing this scenario. By limiting the number of dirty pages, the redirector
ensures that a cache flush operation won’t cause a network timeout.
System threads
As mentioned earlier, the cache manager performs lazy write and read-ahead
I/O operations by submitting requests to the common critical system worker
thread pool. However, it does limit the use of these threads to one less than
the total number of critical system worker threads. In client systems, there are
5 total critical system worker threads, whereas in server systems there are 10.
Internally, the cache manager organizes its work requests into four lists
(though these are serviced by the same set of executive worker threads):
■    The express queue is used for read-ahead operations.
■    The regular queue is used for lazy write scans (for dirty data to flush),
write-behinds, and lazy closes.
■    The fast teardown queue is used when the memory manager is waiting
for the data section owned by the cache manager to be freed so that
the file can be opened with an image section instead, which causes
CcWriteBehind to flush the entire file and tear down the shared cache
map.
■    The post tick queue is used for the cache manager to internally
register for a notification after each “tick” of the lazy writer thread—
in other words, at the end of each pass.
To keep track of the work items the worker threads need to perform, the
cache manager creates its own internal per-processor look-aside list—a
fixed-length list (one for each processor) of worker queue item structures.
(Look-aside lists are discussed in Chapter 5 of Part 1.) The number of worker
queue items depends on system type: 128 for client systems, and 256 for
server systems. For cross-processor performance, the cache manager also
allocates a global look-aside list at the same sizes as just described.
Aggressive write behind and low-priority lazy
writes
With the goal of improving cache manager performance, and to achieve
compatibility with low-speed disk devices (like eMMC disks), the cache
manager lazy writer has gone through substantial improvements in Windows
8.1 and later.
As seen in the previous paragraphs, the lazy writer scan adjusts the dirty
page threshold and its top and bottom limits. Multiple adjustments are made
on the limits, by analyzing the history of the total number of available pages.
Other adjustments are performed to the dirty page threshold itself by
checking whether the lazy writer has been able to write the expected total
number of pages in the last execution cycle (one per second). If the total
number of written pages in the last cycle is less than the expected number
(calculated by the CcCalculatePagesToWrite routine), it means that the
underlying disk device was not able to support the generated I/O throughput,
so the dirty page threshold is lowered (this means that more I/O throttling is
performed, and some cache manager clients will wait when calling
CcCanIWrite API). In the opposite case, in which there are no remaining
pages from the last cycle, the lazy writer scan can easily raise the threshold.
In both cases, the threshold needs to stay inside the range described by the
bottom and top limits.