208
模部署的时候造成负载不均衡的后果。
如果你的负载均衡策略不能很好地处理这些无法预知的性能问题，那么就会在处理大规
任务重启
坏邻居
两种较为常见的无法预知的性能因素如下所示。
简单轮询策略最大的难点可能在于物理服务器一
无法预知的性能因素
性能映射表。
单元GCU）。我们用GCU来给CPU性能建模，同时维护一个GCU与不同CPU类型的
CPU预设值。为了将这个任务变得简单，我们创造了一个虚拟CPU单位（Google计算
务器Y（快机器）。根据这个信息，任务编排系统可以根据某个进程所处机器的类型调节
能一
将其考虑在内。例如，2CPU单位的物理服务器X（慢机器）等同于0.8CPU的物理服
们的任务编排系统跟踪某个任务平均机器性能的取样数据，同时在具体调度资源的时候
CPU/物理服务器类型来倍增就行了。但是，在实际实践中，要实现这个方案，需要我
题。理论上来说，对待繁杂的资源容量的解决方案是很简单的：只要将CPU预留值根据
处理物理服务器的差异一
服务器来说负载不同。
某个数据中心可能有不同CPU性能的物理服务器，所以，同样的请求可能对不同的物理
另外一个简单轮询策略的问题是，同一个数据中心中的所有物理服务器并不都是一样的。
物理服务器的差异
务器（例如，发布新版本），这种任务重启带来的性能问题就很严重了。
们为一些服务器增加了一些逻辑，使得该服务器先保持在跛脚鸭状态，同时预热（触
当一个任务重启时，在数分钟内它经常需要更多的资源。举一个例子，我们发现这
会导致GC（garbage collection）数量的增多。
响到我们进程的性能。我们观察到性能的波动可以达到20%。差异主要来源于对共
物理服务器上的其他进程（经常是完全无关的，由其他团队运行的）可能会显著影
一由于几个无法预知的因素区别很大，无法用统计学来衡量。
发这些动态优化）一段时间，直到性能稳定为止。考虑到我们每天都要更新很多服
种情况更多地影响到Java平台，因为它需要动态优化代码。为了解决这个问题，我
对外请求的延迟升高（由于网络带宽的竞争），同时执行的请求数量就会增加，可能
享资源的竞争，例如内存缓存、带宽等很不明显的地方。例如，如果一个后端任务
第20章数据中心内部的负载均衡系统
一也就是不需要强同质性—是Google很长时间的一个难
一或者更准确地说，后端任务的性
---
## Page 251
策略可以开始将负载迁走，正如迁走过载项目的负载那样。
将最近收到的错误信息计算为活跃请求。这样，如果某个后端进入异常状态，负载均衡
之为地漏效应（sinkholeeffect）。幸运的是，这个问题有一个相对简单的解决办法一
误地认为该任务可用，从而给该异常任务分配了大量的请求。我们在这里将这种问题称
一般来说返回一个“我不健康”的错误比实际处理请求要快得多。于是，客户端任务错
务一般比负载低的任务延迟高，这个策略可以很好地将负载从这些负载高的任务迁走。
做这层过滤，那么这个策略就不能将负载分担得很好。最闲策略的思路来源于负载高的任
这个算法的实现中实际采用了轮询，不过仅仅是在活跃请求数最小的任务里进行。如果不
那么当前状态则变成了“0活跃请求”，那么新请求就会发送给t4。
这时，可选后端变成了除了t0和t6之外的所有任务。然而，如果任务t4的请求结束了，
健康，它可能会开始返回100%的错误。取决于错误的类型，错误回复可能延迟非常低
tot1t2t3t4t5t6t7t8t9
接状态表变成了：
现在假设我们已经发送了4个新请求。仍然没有任何请求在这之间完成，那么目前的连
假设现在还没有任何一个请求成功，在下一个请求时，可选后端变成了t3、t5、t7、t8。
tot1t2t3t4t5t6t7t8t9
t8）并从中选择一个。假设选择了t2，那么现在连接状态表变成了：
那么，在处理新请求时，客户端会将可用后端列表按最少连接数过滤（t2、t3、t5、t7、
tot1t2tt4t5t6t7t8t9
例如，假设一个客户端使用后端子集10到t9，而目前每个后端的活跃请求数量如下所示：
量，然后在活跃请求数量最小的任务中进行轮询。
最闲轮询策略
2111
2110102001
2
100102001
1
12111
负载均衡策略
209
<245
<244
---
## Page 252
246
210
低负载任务的差距极大地降低了。
显示了一个后端任务子集从最闲轮询策略切换到加权轮询策略时的CPU变化。最高和最
在实践中，加权轮询策略效果非常好，极大降低了最高和最低负载任务的差距。图20-6
括健康检查的回复），后端会在回复中提供当前观察到的请求速率、每秒错误值，以及
请求仍以轮询方式分发，但是客户端会按能力值权重比例调节。在收到每个回复之后（包
加权轮询策略理论上很简单：每个客户端为子集中的每个后端任务保持一个“能力”值。
轮询策略的加强。
加权轮询策略通过在决策过程中加入后端提供的信息，是一个针对简单轮询策略和最闲
加权轮询策略
的CPU。
在实际中，我们发现使用最闲轮询策略的大型服务会造成最忙任务比最闲服务使用两倍
每个客户端的活跃请求不包括其他客户端发往同一个后端的请求
活跃请求的数量并不一定是后端容量的代表
最闲轮询策略有两个很重要的限制
记录在内，对未来的决策造成影响。
对应的资源利用率成本进行周期性调节，以选择更好的后端任务处理。失败的请求也会
目前资源利用率（一般来说，是CPU使用率）。客户端根据目前成功请求的数量，以及
每个客户端对每个后端任务状态仅仅有一个有限的视图：该客户端自己发送的请求。
倍的请求，但是最闲轮询策略会认为所有任务都是同样的负载。
消耗0CPU，非常少的内存，以及0带宽。我们当然希望给这个更快的后端发送两
其他任务基本一致（因为任务大部分时间都在等待网络）。在这个例子中，等待I/O
的请求（因为它运行在一个有两倍性能的CPU服务器上），但是这些请求的延迟与
而非常少的时间在实际运行。例如，某个后端任务相对另外一个可以同时处理两倍
很多请求都需要花费相当长的时间等待网络请求（即等待它们触发的请求的回复），
：该策略与简单轮询策略一样，效果都很差。
第20章数据中心内部的负载均衡系统
---
## Page 253
图20-6：在启用加权轮询策略前后的CPU分布图
Core*sec/s
Mar
N
Mar25
Mar
26
负载均衡策略
Mar
27
Mar
28
211
---
## Page 254
247
第21章
212
时返回降级回复，同时在最差情况下，能够妥善地处理资源受限导致的错误。
生的方法是，通过在数据中心之间调度流量，做到每个数据中心都有足够的容量来处理
然而，在极端的过载情况下，该服务甚至可能连这种降级回复都无法生成和发送。在这
要大量计算的数据。例如：
应对过载的一个选项是服务降级：返回一个精确度降低的回复，或者省略回复中一些需
是能够优雅地处理过载情况。
随着压力不断上升，系统的某个部位总会过载。运维一个可靠系统的一个根本要求，就
避免过载，是负载均衡策略的一个重要目标。但是无论你的负载均衡策略效率有多高，
编辑：Sarah Chavis
作者：AlejandroForeroCuervo
应对过载
理资源限制的客户端和对应的后端任务是最好的：在可能的情况下重定向请求，在必要
中心。但是，在大规模部署时，这样的策略可能还是不够的。无论如何，构建能良好处
500QPS的请求，那么负载均衡策略就不会调度超过50.000QPS的用户流量到这个数据
请求。例如，如果一个数据中心运行了100个后端任务，其中每个后端任务可以处理
种情况下，该服务可能除了返回错误之外没有什么其他的好办法。一种避免这种场景发
·平时在整个文档库中进行搜索，以针对某个查询返回最佳结果。而过载情况下仅
）使用一份本地缓存的、可能不是最新的数据来回复，而不使用主存储系统。
仅在文档库的一小部分中进行搜索。
---
## Page 255
这样其他用户则不会受影响。为了达到这个目的，该服务的运维团队和客户团队协商一
当全局过载情况真的发生时，使服务只针对某些“异常”客户返回错误是非常关键的，
队使用的内部服务）。
实总是残酷的。全局过载情况在实际运行中出现得非常频繁（尤其是那些被很多其他团
服务永远有足够容量服务最终用户，这样全局过载情况就永远不会发生。不幸的是，现
想情况下，每个团队都能和他们所依赖的后端服务团队之间协调功能发布，从而使后端
过载应对策略设计的一个部分是决定如何处理全局过载（globaloverload）的情况。在理
给每个用户设置限制
资源分别考虑在内。
如果过量分配其他非CPU资源不可行的话，我们可以在计算资源消耗的时候将各种系统
给的主要信号就可以工作得很好，原因如下：
在绝大部分情况下（当然总会有例外情况），我们发现简单地使用CPU数量作为资源配
耗的CPU时间（这里要考虑到不同CPU类型的性能差异问题）。
务容量是非常合理的。我们经常将某个请求的“成本”定义为该请求在正常情况下所消
更好的解决方案是直接以可用资源来衡量可用容量。例如，某服务可能在某个数据中心
消耗的资源大幅减少）。这种不断变动的目标，使得设计和实现良好的负载均衡策略使
些变动是逐渐发生的，有些则是非常突然的（例如某个软件的新版本突然使得某些请求
的选择。就算这个指标在某一个时间段内看起来工作还算良好，早晚也会发生变化。有
（认为其能指代处理所消耗的资源：例如某个请求所需要读取的键值数量）一般是错误
Google在多年的经验积累中得出：按照QPS来规划服务容量，或者是按照某种静态属性
时间（家庭用户和企业用户，交互型请求和批量请求）。
定，例如客户端代码的不同（有的服务有很多种客户端实现），或者甚至是当时的现实
不同的请求可能需要数量迥异的资源来处理。某个请求的成本可能由各种各样的因素决
QPS陷阱
内预留了500CPU内核和1TB内存用以提供服务。用这些数字来建模该数据中心的服
用起来非常困难。
·在其他编程环境里，其他资源一般可以通过某种比例进行配置，以便使这些资源
·在有垃圾回收（GC）机制的编程环境里，内存的压力通常自然而然地变成CPU
的短缺情况非常罕见。
的压力（在内存受限的情况下，GC会增加）。
给每个用户设置限制
213
249
248
---
## Page 256
250
214
注1例如，参见Doorman（htp:/github.com/youtube/doorman），提供了一个协作性分布式客户端节流系统。