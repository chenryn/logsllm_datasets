### DAVOS: Dependability Assessment and Verification System

#### Introduction
DAVOS is a comprehensive framework designed to assist in the assessment, verification, and benchmarking of dependability-related properties for system/IP cores. This document outlines the key functionalities and application scenarios of DAVOS, with a focus on its fault injection capabilities and dependability benchmarking.

#### Fault Injection Module
The fault injection module is central to all dependability-related scenarios in DAVOS. It consists of a collection of custom Python scripts that process XML configuration files and HDL models to generate scripts for an off-the-shelf simulator. These scripts are used to:
1. Run fault injection experiments.
2. Generate trace files for analysis.

To manage the time-consuming nature of simulating implementation-level HDL models, the module includes strategies for parallel execution, such as leveraging multi-core and grid computing capabilities.

##### Fault Dictionary
The fault dictionary is an XML file that defines how different fault models are injected into the HDL models. This decoupling allows new fault models to be added without modifying the module's code. For example, delay faults can be injected into Xilinx's SIMPRIM library components (e.g., X_FF and X_LUT6).

**Example XML Configuration:**
```xml
<!-- FaultDict.xml: Fault description section for delay fault model into Xilinx’s X_FF primitive -->
<fault_model model="delay" macrocell="X_FF">
    <instrumentation>
        <!-- Custom Python script rules for instrumentation -->
    </instrumentation>
    <simulation>
        <!-- TCL commands for the simulator -->
    </simulation>
</fault_model>

<!-- Config.xml: Fault model section for delay faultload -->
<faultload model="delay" target_logic="X_FF">
    <condition>interconnect_delay > 0.05 * clock_period</condition>
    <value>#interconnect_delay#</value>
</faultload>
```

##### Experiment Execution
The fault injection module generates scripts for each eligible target, injecting a single fault per experiment. The interconnect delay, for instance, can be increased by 5% of the system's clock cycle at a random instant between 5% and 15% of its total length.

Observation points, defined in another XML configuration file, are monitored during each experiment to generate trace files. These trace files are processed and stored in a SQLite database for later analysis.

#### Interactive Web-Based Report
DAVOS provides an interactive web-based report for analyzing the results of fault injection experiments. The main features include:
- **Summary Page:** Displays a summary of all experiments, including failure modes and latencies grouped by fault model.
- **Detailed Trace Information:** Access to detailed trace information for each experiment.
- **Custom Queries:** Tailored reports based on filters such as fault models, target logic, and resulting failure mode.
- **Hierarchical Navigation:** Hierarchical navigation of the HDL model, displaying results for each constituent part.

**Example Analysis:**
- **Failure Modes Distribution:** For permanent interconnect delays affecting the I port of X_FF primitives in the LEON3 microprocessor, 29.44% of the experiments led to silent data corruption (SDC). The execute (e) and exception (x) pipeline stages contributed the most to SDC, with 7.83 and 7.61 percentage points, respectively.

**Graphical Representation:**
- **Failure Rate Graphs:** Post-processing the results from custom queries can generate graphs showing the observed failure rate with respect to the width of interconnect and propagation delays for flip-flops and look-up tables.

#### Dependability Benchmarking
This scenario involves selecting the best candidate among alternative IP cores, EDA tools, and implementation technologies based on relevant criteria for the application domain. For example, determining the most suitable synthesis tool (Xilinx’s XST or Mentor Graphics’ Precision RTL) and implementation technology (smallest device from Xilinx’s Artix-7, Kintex-7, and Virtex-7 FPGAs) for implementing a MC8051 microcontroller in automotive and mobile applications.

##### Metrics and Computation
Dependability benchmarking requires defining metrics to characterize the goodness of the resulting implementation. These metrics are computed from measurements obtained during implementation and experimentation and stored in the database. New derived metrics can be defined by customizing a configuration file.

**Example Metric Computation:**
- **Mean Time to Failure (MTTF):** Computed as 1/L, where L is a function of cell-specific parameters and observed failure rates.

**Multi-Criteria Decision Making (MCDM):**
- **Weighted Sum Method:** DAVOS implements the Weighted Sum Method to rank alternatives according to the characteristics of the application domain. Other MCDM methods can be integrated by extending the corresponding Python script.

**Configuration File Example:**
```xml
<metrics>
    <metric name="MTTF" handler="compute_mttf">
        <arg key="k" value="0.5"/>
        <arg key="fit.cell" value="0.001"/>
    </metric>
</metrics>
<goals>
    <scenario name="Automotive">
        <goal name="MTTF" type="maximize" weight="0.8"/>
    </scenario>
</goals>
```

#### Conclusion
DAVOS offers a robust and flexible framework for dependability assessment and verification, enabling users to define and analyze fault injection experiments and perform dependability benchmarking. Its modular design and extensible configuration options make it suitable for a wide range of application scenarios.