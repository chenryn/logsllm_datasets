Defect Profile
The  formal  test  cycle  was  overlapped  with  the
development  process,  as  is  common  in  many  a
development  process.  It  began  after  a  substantial
amount of the code was available for testing.  This
happened when at least 50% of the modules were
ready and the application could be successfully built
with features available for testing.  New code is added
to the base in regular intervals and the build schedule
is known. Thus the test function is aware of the new
functionality included in each build.
Figure 1 shows the cumulative number of defects
drawn  against  calendar  days.  To  understand  the
delays in testing we have divided the time line into
three stages, each of which demonstrates a significant
difference from the other in defect detection rates.
Stage 1: Day 0 - Day 40
Stage 2: Day 40 - Day 100
Stage 3: Day 100 - Day 200
The  periods  themselves  do  not  have  any
significance from a development process or mark an
event - they are merely sections of the S curve.  Stage
1 displays a very slow progress in defect detection,
while stage 2 experiences a more accelerated defect
detection.  Stage 3 is the remainder of the test cycle
where the defect detection rate further slows down.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply. 
Defect Priority
ODC Trigger
The  Priority  of  a  defect  indicates  the  urgency
expressed by the test organization for a fix. It does
not have as much to do with the severity of the failure
as it does to the immediacy of fix to continue with
testing.  Thus, Priority and its distribution tells us
about  the  current  state  of  affairs  in  test  and  its
consequences on the test cycle.
Severity, a popular measure of the impact of a
fault, sounds very similar to priority.  We want to
make clear that, in this paper, we use priority and
not severity.  Severity, becomes quite useful towards
the end of the development life cycle, while its use
in early parts of the function test is debatable.
Priority  1  defects  imply  that  additional  testing
cannot  continue  until  the  fix  for  the  failure  is
provided.  Priority 2 defects indicate a problem that
also  reduces  the  effectiveness  of  test  but  is  not  a
critical show stopper.  Priority 3 defects are of a lesser
significance;  in that they need to be fixed, however,
they do not hinder the progress of test.
A larger fraction of priority 1 defects indicate that
the testing process is not running smoothly.  The bugs
are probably blocking bugs, and the effectiveness of
the test organization is limited.  There is consequently
much back and forth between development and test,
which impacts the productivity of both organizations.
Figure  2  shows  us  the  distribution  of  defect
priority and how that changes from stage 1 to stage
3 of this test cycle.  The fact that during stage 1 the
percent of priority 1 defects was as high as 70%,
explains why the progress of test was slow.  During
stage 2 the percent of priority 1 drops by 20% and
the rate at which defects are being detected rapidly
increases.  Finally, during stage 3 when the product
was stabilizing the percent of Priority 1 defects had
dropped to its lowest in this test cycle.
These data, illustrate the magnitude of the hurdles
faced during test. The large fraction of priority 1
defects reinforces the sense that was communicated
by management that the test cycle was prolonged. It
does not immediately offer an explanation of why
this is the case, but makes clear that the problem is
real.  Further insight into the cause of this would be
difficult to establish without using ODC Triggers.
Software triggers are the catalysts which allow
the defect to surface. Every time a review or test is
done, that activity is exercising a particular trigger
which may allow a defect to surface.  While a test
case examines functionality or checks a requirement,
the way it does so is captured by the trigger. The
different triggers express the range of ways by which
tests surface bugs.
Software triggers are discussed at length in the
original ODC work; the collection of papers available
on-line at [ODC Web].  A  good discussion on what
triggers are, and the properties they possess can be
found  in  [Chillarege  95].  Table  2  lists  software
triggers and an association with process phases that
typically generates them.
Since ODC attribute values are carefully named
they  are  usually  descriptive  enough  that  one  can
follow  the  discussion  on  the  data  with  ease.
However, it certainly helps to have the definitions
handy.    For  the  purposes  of  this  paper  we  have
provided a short two line definition of the relevant
triggers in the following subsection.
Short descriptions of relevant Triggers
DESIGN CONFORMANCE:  A review trigger where the
implemented design is compared against a refer-
ence - either design document, pattern, or guide-
line.
LOGIC/FLOW: Checking for correctness or flaws us-
ing knowledge of the practice.
BACKWARD COMPATIBILITY: Examining compatibility
with prior version of the product.
LATERAL COMPATIBILITY: Examining for compatibility
with  other  products  and  platforms  that  need  to
work with this release.
CONCURRENCY: Serialization, shared resources,  multi-
threaded tasks, timing, etc.
INTERNAL DOCUMENT:  Inconsistencies in prologs, and
sections in the same work product.
LANGUAGE  DEPENDENCY:    Programming  standards,
specific implementation considerations, environ-
ment restrictions, execution modes, etc.
SIDE EFFECTS:  Usage behavior beyond design, but
relevant in context.  Do A; B happens.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply. 
TRIGGER
D C U F S
X X
X X
X X
X X
X X
X X
X X
X X
X X
Design Conformance
Logic/Flow
Backward Compatibility
Lateral Compatibility
Concurrency
Internal Document
Language Dependency
Side Effects
Rare Situation
Simple Path
Complex Path
Coverage
Variation
Sequencing
Interaction
Workload Stress
Recovery
Startup/Restart
Hardware Configuration
Software Configuration
Blocked Test/Normal Mode
X
X
X
X
X
X
X
X
X
X
X
X
   KEY:   D: Design Review, C: Code Review
     U, F, S:  Unit, Function, System Test
Table 2.  Triggers are commonly associated with a
verification  stage  that  is  likely  to  generate  the
particular  trigger. Thus,  in  ODC  vernacular  one
could  refer  to  a  trigger  as,  “  a  review  trigger”,
implying that the trigger belongs to the set of triggers
usually encountered in activities of review.  We must
note that not all triggers associated with a stage
are  necessarily  experienced  in  any  specific
implementation.
RARE SITUATION:  Unusual issues related to idiosyn-
crasy of environment, hardware, or software.
SIMPLE PATH:  White box - Straightforward usage of
code path and branches.
COMPLEX PATH:  White box - Exercising condition-
als, and circuitous coverage.
COVERAGE:    Black  box  -  Straightforward  usage  of
function or single parametrized execution.
VARIATION:  Black box - straightforward like cover-
age, but with a variety of parameters.
SEQUENCING:  Black box - multiple functions, with a
specific sequence.
INTERACTION:  Black box - when two or more bodies
of code are involved.
WORKLOAD  STRESS:  Pushing  the  limits  of  perfor-
mance, resources, users, queues, traffic, etc.
Recovery: Invoke exception handling, recovery, ter-
mination, error percolation, etc.
STARTUP/RESTART: Major events of turning on, off or
changing the degree of service availability.
HARDWARE CONFIGURATION: Issues surfaced as a con-
sequence of changes in hardware setup.
SOFTWARE CONFIGURATION: Issues surfaced as a con-
sequence of changes to software setup.
BLOCKED TEST/NORMAL  MODE:  Test  could  not  run
during system test. Normal Mode - archaic:  cus-
tomer found nonspecific trigger.
The overall  Trigger  Distribution
We  start  by  looking  at  the  overall  trigger
distribution.  The Figure 3 shows the triggers of all
defects over the entire test cycle, a population of
around  a  thousand  defects.    The  purpose  of  this
distribution is to quickly assess the range of triggers
present in the entire test.  We expect the process has
generated a wide range or triggers, or at least those
that can be expected from a function test and some
of the system test triggers.  We will later look at them
as a function of time, to see if the changes are along
what ODC would expect from an evolutionary and
stabilization perspective.
It is evident, from the figure, that the coverage
and  variation  triggers  dominate  the  entire
distribution.  That there are very few other triggers
in  the  distribution,  and  that  Coverage  triggers
dominate  more  than  50%  of  all  the  triggers  are
indicative  of  a  potential  problem  -  either  in
development or test.   While Coverage normally does
tend to be one of the largest triggers in function test,
when it is not supported by the other triggers it raises
suspicion.
A coverage-triggered defect could also have been
found  through  unit  test  or  inspection.    In  fact,
inspection is a particularly good mechanism to find
coverage triggered defects of function test because
they mostly have to do with ensuring logic flow and
completeness of function.  While automated testing
is also good at finding these defects, the burden that
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply. 
Fig 3. Overall Trigger Distribution
Software
Configuration
0.00
Recovery
0.00
Interaction
0.06
Sequencing
0.01
Variation
0.19
Coverage
0.58
Internal
Document
0.03
Logic/Flow
0.03
Design
Conformance
0.10
0.00
0.20
0.40
0.60
0.80
Probability
Fig 3. The overall trigger distribution is dominated by cov-
erage triggers - the type of trigger that can also be found
by other means such as inspection and unit test. NB. the
population is over a 1000 defects.
is generated is because of the blockage that can be
caused  as  a  consequence  of  these  faults.    By
blockage, we mean that if a basic functionality fails,
then a series of functionalities that are dependent on
the  failed  functionality  also  fail  or  those
functionalities cannot be tested. e.g. If creation of
an object fails, then all other test cases depending
on the object cannot be executed.
This product had around 1000 function points,
and  since  the  total  number  of  defects  is
approximately  1000,  the  test  found  defect  rate  is
about  1  defect  per  function  point.    Whereas,  the
industry average for this class of code is around 0.5
defects per function point [Jones 98].
It  is  not  uncommon  for  new  code  from  a
development  team  that  is  under  a  considerable
pressure to have an increased defect injection rate.
A factor of two is not too bad given that the variance
in  defect  injection  rates  is  wider  than  that.    The
critical issues are really the nature of defects and
the efficacy of the test cycle.  The issue becomes
one of the length of time it takes to test and the nature
of testing that is required to clean up the code.
That these defects are coverage dominated and
that  the  overall  defect  rate  was  large  is  a  clear
indication that the quality of code that came through
from  the  development  phases  was  poorer  than  it
could  be.    Many  of  these  defects  are  best  found
through inspection, and that process was definitely
weak.
This burden of high coverage triggered defects at
twice  the  acceptable  volume  now  explains  why
during  stage  1  and  stage  2  the  test  progress  was
burdened.  (Correspondingly,  the  probability  of  a
priority 1 defect was between 0.5 to 0.7, which is
high and told us of the high burden and block).
Clearly, an opportunity to reduce the test cycle
time would be to address defects of this trigger before
the code enters test.  It also becomes evident that the
return on investment for such process action is huge.
The issues are one of training and targeted inspection
lists,  as  opposed  to  exhaustive  lists.    The  ODC
Triggers,  taken  together  with  the  ODC  Type  and
ODC Source can help generate such a list.  This is
an ODC Action step, a process discussion that is
beyond the scope of this paper.
Trigger vs. Time
Figure 4 shows the defect trigger distribution as it
changes from stage 1 to stage 3.  In each of these
stages, the overall nature of the trigger distribution
is very similar to that of the aggregate distribution.
Namely, coverage dominates the distribution.
However, what is interesting to note is that the
proportion of coverage defects also reduces from
stage 1 to stage 2 and then further in stage 3.  The
decrease in coverage is picked up by an increase in
variation.  The more complex triggers such as
sequencing and interaction, continue to have a
small profile.
Proceedings of the International Conference on Dependable Systems and Networks (DSN’02) 
0-7695-1597-5/02 $17.00 © 2002 IEEE 
Authorized licensed use limited to: Tsinghua University. Downloaded on March 19,2021 at 04:19:12 UTC from IEEE Xplore.  Restrictions apply. 
Fig 4.  Trigger Distribution Changes by Stage
t
n
e