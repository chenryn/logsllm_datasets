(We omit the times for non-panic en-
cryption, decryption, update, and retrieve; these times
were at most those of the related panic-mode opera-
tions.) These benchmarks only used the light location-
ﬁnding mechanism and each update was inserted to a
single OpenDHT node. Each operation was timed for
100 repetitions both using the clock system call (the CPU
columns) and gettimeofday (the Wall columns); shown is
the min/mean/median/max time over the successful tri-
als. Where applicable, the number of time outs (due to
DHT operations) are shown in the column labeled T/O.
Note that the retrieve operations only include retrieval
for a single update. These benchmarks show that the ex-
tensions are not prohibitive: performance is dependent
almost entirely on the speed of network operations.
6.2 Geolocation accuracy
As mentioned earlier, our system has been designed to
convey various kinds of location information to the stor-
age service. We then rely on previously proposed net-
work measurement analysis techniques and/or database
lookups to process the stored location information and
derive a geographical estimate. The strengths and weak-
nesses of such techniques are well-documented. We
therefore focus our evaluation on the active client-based
measurement technique described in Section 4 that at-
tempts to identify a set of nearby passive landmarks
given a large number of geographically distributed land-
marks.
First, we accumulated about 225 412 open DNS
servers by querying Internet search engines for dictio-
nary words and collecting the DNS servers which re-
sponded to lookups on the resulting hostnames. Next,
we enumerated 8 643 Akamai nodes across the world by
querying the DNS servers for the IP addresses of host-
names known to resolve to Akamai edge servers (e.g.,
www.nba.com). Finally, 50 PlanetLab [11] nodes were
used as stand-ins for lost or stolen devices across the
United States.
Having both targets and landmarks, we obtained
round-trip time (RTT) measurements from the Planet-
Lab nodes to the passive Akamai servers. The PlanetLab
nodes were able to obtain measurements to 6 200 of the
Akamai servers on our list. We could then evaluate our
geolocation technique by running it over these measure-
ments. Figure 3 plots the cumulative distributions of our
results and the RTT to the actual closest Akamai node.
We also plot the cumulative distribution of the RTT to
an Akamai node as given by a simple DNS lookup for
32 of our 50 targets (the other 18 nodes went down at
USENIX Association  
17th USENIX Security Symposium 
285
CPU
Wall
Operation
Initialize core
Verify FSPRG state
Panic encryption
Panic decryption
Panic update
Panic retrieve
r = 0
r = 10
r = 100
r = 0
r = 10
r = 100
Min Mean Median Max
460
210
610
340
110
90
100
80
440
700
680
440
800
540
100
80
100
80
80
100
330
470
90
90
570
545
690
90
90
90
329
456
95
90
559
543
666
89
87
87
Min
215
351
93
85
612
818
2 953
92
93
116
Mean Median
367
494
101
104
1 653
2 289
12 599
348
474
95
90
977
1 311
7 439
207
335
1 555
499
705
2 458
Max
1 082
1 240
207
934
15 347
20 582
165 950
12 003
9 734
21 734
T/O
–
–
–
–
9
10
5
7
12
5
Table 3: Time in milliseconds to perform basic operations in adeona-0.1.
the time of measurement). Our technique performs bet-
ter than Akamai’s own content delivery algorithms for
more than 60% of the the targets we considered. In ad-
dition, we observe that it can ﬁnd an Akamai server at
most 7 milliseconds away.
6.3 Field trial
We conducted a small ﬁeld trial to gain experience
with our implementation of Adeona, reveal potential is-
sues with our designs, and quantitatively gauge the ef-
ﬁcacy of using OpenDHT as a remote storage facility.
There were 11 participants each running the adeona-
0.2.0 client with the same options: update rate param-
eter of 0.002 (about 7 updates an hour on average), lo-
cation cache of size r = 4, and spatial replication of 4
(the core tries to insert each update to 4 DHT keys). The
clients were instrumented to locally log all the location
updates submitted over the course of the trial. At the end
of the trial, we collected these client-side log ﬁles plus
each owner’s copy of the initial state, and used this data
to attempt to retrieve a week’s worth of updates9 for each
of the participants.
Results are shown in the left table of Figure 3. Here ‘#
Inserts’ refers to the total number of successful insertions
into the DHT by the client in the week period. The ‘In-
sert rate’ column measures the fraction of these inserts
that were retrieved. The ‘# Updates’ column shows the
total number of updates submitted by each client. Note
that our replication mechanism means that each update
causes the client to attempt 4 insertions of the location
cache. The ‘Update rate’ column measures the percent-
age of location caches retrieved. As can be seen, this
fraction is usually larger than the fraction of inserts re-
trieved, suggesting that replication across multiple DHT
keys is beneﬁcial. The ‘Locations Found’ column reports
the number of unique locations (deﬁned as distinct (in-
ternal IP, external IP) pair) found during retrieval versus
reported. The ﬁnal column measures the time, in min-
utes and seconds, that it took to perform retrieval for the
user’s updates for the whole week (note that we paral-
lelized retrieval for each user).
We observed that OpenDHT may return “no data” for
a key even when, in fact, there is data stored under that
key. (This was detected when doing multiple get requests
for a key.) Indeed, the failure to ﬁnd two of the user loca-
tions was due to this phenomenon, and in fact repeating
the retrieval operation found these locations as well.
7 Deployment Settings: Hardware Sup-
port and Dedicated Servers
internal corporate systems,
In Section 2, we highlighted several settings for de-
vice tracking:
third-party
companies offering tracking services, and community-
supported tracking for individuals. Each case differs in
terms of what resources are available to both the tracking
client and the remote storage. In Section 4 we built the
Adeona system targeting a software client and OpenDHT
repository, which works well for the third setting. Here
we describe how our designs can work with other deploy-
ment scenarios.
A hardware-supported client can be deployed in sev-
eral ways, including ASICs implementing client logic,
trusted hardware modules (e.g., a TPM [35] or Intel’s
Active Management technology), or worked into exist-
ing system ﬁrmware components (e.g., a system BIOS).
Hardware-support can be effectively used to ensure that
the tracking client can only be disabled by the most so-
phisticated thieves and, possibly, that the client has ac-
cess to a private and tamper-free state. However, target-
ing a system for use with a hardware-supported client
adds to system requirements. While we do not work
out all the (important) details of a hardware implemen-
tation of Adeona’s client (leaving this to future work),
we argue here that our techniques are amicable to this
type of deployment. Adeona’s core (Section 3) is partic-
ularly suited for implementation in hardware. It relies on
a single cryptographic primitive, AES, which is highly
optimized for both software and hardware. For example,
recent research shows how to implement AES in only
286 
17th USENIX Security Symposium 
USENIX Association
1
0.8
0.6
0.4
0.2
y
t
i
l
i
b
a
b
o
r
P
e
v
i
t
l
a
u
m
u
C
0
0
actual
adeona
akamai
5
10
15
20
25
30
35
Shortest RTT to Akamai node (ms)
User
#
Inserts
01
02
03
04
05
06
07
08
09
10
11
491
632
622
543
617
234
359
420
504
138
302
Insert
Rate
0.89
0.89
0.84
0.87
0.88
0.85
0.89
0.85
0.91
0.90
0.81
#
Updates
251
327
321
274
309
123
199
220
259
59
175
Update
Rate
0.94
0.94
0.91
0.95
0.96
0.90
0.95
0.92
0.97
0.92
0.91
Locations
Found
11/12
3/3
2/2
5/5
4/4
4/4
5/6
7/7
1/1
4/4
6/6
Retrieve
Time
12m 06s
16m 04s
17m 04s
15m 03s
19m 04s
15m 06s
18m 04s
14m 06s
11m 06s
13m 04s
14m 04s
Figure 3: (Left) The cumulative distribution of the shortest RTT (in milliseconds) to an Akamai node found by Adeona compared
to the actual closest Akamai node and Akamai’s own content delivery algorithm. (Right) Field trial retrieval rates and retrieval
times (in minutes and seconds).
3400 gates (on a “grain of sand”) [15]. In its most basic
form (without a location cache), the core only requires
16 bytes of persistent storage.
In settings where a third-party company offers track-
ing services, the design requirements are more relaxed
compared to a community-supported approach. Partic-
ularly, such a company would typically offer dedicated
remote storage servers. This would allow handling per-
sistence issues server-side. Further, this kind of remote
storage service is likely to provide better availability than
DHTs, obviating the need to engineer the client to handle
various kinds of service failures. Adeona is thus slightly
over-engineered for this setting (e.g., we could dispense
with the replication technique of Section 4). An interest-
ing question that is raised in such a deployment setting is
how to perform privacy-preserving access control. For