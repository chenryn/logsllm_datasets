of protected data is analyzed to ﬁnd trends, after which a
smaller amount of privileged data is subjected to arbitrary
computation involving the learned trends.
The clustering analysis starts by establishing the average
value of each monitor across all IP addresses, to be used in
lieu of absent readings.
average = monitor.Average(epsilon, x => x.hops);
The monitors are then assembled into a collection of vectors,
one for each IP address, and one coordinate per monitor.
Addresses not observed at a monitor result in the average
value for that coordinate:
monitors.Aggregate((x,y) => x.Concat(y))
.GroupBy(x => x.IP)
.Select( /* additional logic */)
So assembled, the set of vectors can be subjected to stan-
dard clustering algorithms. We use k-means clustering of
PINQ. The original analysis uses Gaussian EM instead, an
extension of k-means using covariance matrices for each clus-
ter. While Gaussian EM is also expressible, it has a higher
privacy cost and is consequently less accurate for us. This
calls into light the trade-oﬀ between algorithmic complex-
ity and accuracy; more complex algorithms can give better
results in the absence of privacy constraints, but if their so-
phistication requires looking “too closely” at the data, the
necessary noise to preserve privacy can counteract these
gains.
The data used by Eriksson et al. is hop count (inferred
using TTL) from scanning IP addresses to honeypot mon-
itors. We run our analysis on the IPscatter dataset, which
is similar. It has hop count measurements from PlanetLab
nodes (as monitors) to large number of IP addresses.
Figure 5 shows the results with diﬀerent values of  as well
as without privacy. It plots the objective function of the k-
means optimization, the average distance from a point to
its nearest cluster center, against the number of iterations
conducted. Nine centers are used, initialized to a common
random set of vectors for each execution. For each value of ,
each iteration of the algorithm consumes another multiple of
the privacy cost. After 10 iterations, a value of =0.1 costs
1. However, given the ﬂatness of the curves, for a ﬁxed
0200400600Time bin0200400600Norm (scaled bytes)epsilon=0.1epsilon=1epsilon=10noise-free0246810Num. iteration101214161820RMSEepsilon=0.1epsilon=1epsilon=10noise-free132privacy budget, the appropriate strategy may not be to run
ten iterations at one-tenth the accuracy.
The curves reveal that at the strongest privacy level (=0.1)
the RMSE is worse by 50%. The medium privacy level is
much closer and its error may be acceptable. (The impli-
cations of variation in cluster quality on the reconstructed
network topology is beyond the scope of this paper.) The
weakest privacy level, however, is able to provide results al-
most identical to the non-private computation.
5.3.3 Summary
We considered two graph-level analyses. We were able to
reproduce the anomaly-detection analysis faithfully because
most of its complex computations are on heavily aggregated
data that is less hindered by privacy constraints. The pas-
sive network discovery analysis yielded high-ﬁdelity results
only with weak privacy guarantees. It also exposed a trade-
oﬀ between algorithmic complexity and privacy cost. Given
that these two analyses are fairly involved, our experience
suggests that many other graph-level analyses can be con-
ducted in a diﬀerentially private manner.
6. RELATED WORK
The dominant method for data sharing today is trace
anonymization [16, 31, 22]. However, many researchers have
shown that anonymization is vulnerable to attacks that can
extract sensitive information from the traces [5, 26, 31, 21].
The utility of anonymized traces is further limited by the
removal of sensitive ﬁelds, critical for certain analyses [21].
Because of these shortcomings of anonymization, researchers
have begun exploring mediated trace analysis. There are
three proposals to our knowledge, none of which match the
strong and direct privacy guarantees of diﬀerential privacy.
First, Mogul and Arlitt’s SC2D relies on the use of pre-
approved analysis modules and human veriﬁcation to pre-
serve privacy [19]. Given the complexity and diversity of
network analyses, it is unclear if human veriﬁcation is prac-
tical and what guarantees it can provide.
Second, Mirkovic’s secure queries [17] are conceptually
similar to our work in that the analysis is expressed in a
high-level language and the analysis server is tasked with
ensuring privacy. The privacy requirements are inspired by
diﬀerential privacy as well. However, privacy is enforced us-
ing a set of ad hoc rules whose eventual properties are poorly
understood. Further, while we show that a range of anal-
yses can be accurately done using our methods, Mirkovic
does not evaluate the usefulness of secure queries.
Third, Mittal et al. develop a method for quantifying the
amount of information revealed by an analysis and propose
that data owners refuse to support analyses that leak more
than a threshold [18]. While intriguing, this approach is vul-
nerable to targeted attacks. As previously discussed, single
bits can be arbitrarily sensitive, and the refusal reveals a bit
in itself. Diﬀerential privacy reveals less than a bit about
each record, but many bits about aggregate statistics.
Diﬀerential privacy is a recent concept and its practical
utility is an open question that can be answered only by
applying it to several domains. Along with McSherry and
Mironov, who study Netﬂix recommendations [15], and Ras-
togi and Nath, who study distributed time-series [24], our
work helps to further an understanding of this question.
Reed et al [25] recently proposed an analysis language sim-
ilar to PINQ to detect botnets in a diﬀerentially private
manner. While this approach has not been evaluated yet,
our experience suggests that it can be eﬀective.
7. DISCUSSION AND OPEN ISSUES
Our results indicate that diﬀerential privacy has the po-
tential to be the basis for mediated trace analysis, which will
enable data owners to let other analysts extract statistical
information in a provably private manner. The limitations
of diﬀerential privacy with respect to output ﬁdelity and the
need to implement the analysis in a high-level language are
surmountable for a large class of analyses.
Retrospectively, the success of diﬀerential privacy in this
domain stems from two factors. First, many analyses seek
aggregate statistical trends and common patterns in the
data. For such analyses, individual records contribute only a
small fraction to each output value, which implies that only
a small amount of noise can guarantee privacy. Second, the
computations that many analyses conduct directly over in-
dividual records are rather simple and thus easy to express.
Any complicated computations (e.g., clustering, PCA) are
conducted only over aggregate data that can ﬁrst be ex-
tracted privately with a high ﬁdelity. These properties may
not hold for all analyses but they appear to hold for large
class of analyses.
We do not claim that implementing analysis in a diﬀeren-
tially private manner is straightforward. We ran into many
challenges and counter-intuitive behaviors. Some analyses
yielded low ﬁdelity results at strong privacy levels; high
output ﬁdelity could be achieved only at weak privacy lev-
els. Between, at medium privacy settings, the accuracy was
rarely bad, but distinguishable from the truth. Whether
this is suﬃcient depends on the needs of the analyst, and
the available privacy resources. As more thought is put into
algorithm (re-)design, we expect these trade-oﬀs to improve.
Some computations that are easy otherwise (e.g., sliding
windows) can have a high privacy cost. Others, such as em-
pirical CDFs with arbitrary resolutions, are fundamentally
impossible to do in a diﬀerentially private manner. Fur-
ther, there are multiple ways to implement the same analy-
sis, some more privacy eﬃcient than others. A worthwhile
task for the future is to educate networking researchers on
the concept of privacy eﬃciency, which is distinct from, and
sometimes counter to, the more familiar concept of compu-
tational eﬃciency. A related one is to develop a library with
privacy-eﬃcient implementations of common primitives used
by networking analyses. The toolkit presented in this paper
is a ﬁrst step in that direction [23].2
This paper is by no means the ﬁnal word on the use of
diﬀerential privacy for mediated trace analysis; there are
several policy-related and practical challenges that must ﬁrst
be fully explored. One such challenge, which we mentioned
in §3, is developing support for coarser-granularity privacy
principals (e.g., ﬂow or hosts) even when the underlying data
is at a ﬁner-granularity (e.g., packets).
Another challenge is developing guidelines for data owners
on what privacy level (parameter ) to set for their datasets.
2Expressing analyses in high-level languages makes them
easier to debug and maintain as well. In the speciﬁc context
of PINQ, because it is based on LINQ, the analyses will also
automatically scale to a cluster [32]. Today, for ﬂexibility,
most networking analyses are written in low-level languages
(e.g., C, Perl). Our survey provides evidence that the com-
munity can aﬀord to move to high-level languages.
133While we explore a range of levels in our work, owners will
have to decide on speciﬁc levels to us for their data. There is
unlikely to be a single answer for all situations. Instead, the
appropriate level should be based on a combination of data
sensitivity, the value of the analysis, acceptable noise-level,
and the trust in the analyst.
Yet another challenge is managing the impact of repeated
use of the same data, by the same analyst or by diﬀerent
analysts. Each use leaks some private information (in the-
ory) and successive uses leak more information. Diﬀerential
privacy provides useful guidance on this issue. Two analyses
with privacy cost c1 and c2 have a total privacy cost at most
c1 + c2. Using this property, the data owners can enforce
various policies such as limiting the total privacy cost per
analyst or across all analysts. They can also reduce privacy
cost (i.e., increase ) with time such that the data is available
longer but the added noise increases with time.
Resolving these challenges requires balancing usability and
privacy. With the strong foundation provided by diﬀerential
privacy, we are optimistic that they can be resolved to the
satisfaction of many data owners.
Acknowledgments
We are grateful to Saikat Guha,
Suman Nath, Alec Wolman, the anonymous reviewers and
our shepherd, Walter Willinger, for feedback on this paper.
We also thank Stefan Savage for suggesting the worm ﬁn-
gerprinting and stepping s analyses early in the project.
8. REFERENCES
[1] R. Agrawal and R. Srikant. Fast algorithms for mining
association rules. In VLDB, 1994.
[2] AOL search data scandal. http://en.wikipedia.org/
wiki/AOL_search_data_scandal. Retrieved
2010-16-01.
[3] M. Ayer, H. Brunk, G. Ewing, W. Reid, and
E. Silverman. An empirical distribution function for
sampling with incomplete information. The Annals of
Mathematical Statistics, 26(4), 1955.
[4] R. Chandra, R. Mahajan, V. Padmanabhan, and
M. Zhang. CRAWDAD data set microsoft/osdi2006
(v. 2007-05-23).
[5] S. E. Coull, C. V. Wright, F. Monrose, M. P. Collins,
and M. K. Reiter. Playing devil´cs advocate: Inferring
sensitive information from anonymized network traces.
In NDSS, 2007.
[6] CRAWDAD: A community resource for archiving
wireless data at Dartmouth.
http://crawdad.cs.dartmouth.edu/.
[7] C. Dwork. Diﬀerential privacy. In ICALP, 2006.
[8] C. Dwork, F. Mcsherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data
analysis. In Theory of Cryptography Conference, 2006.
[9] B. Eriksson, P. Barford, and R. Nowak. Network
discovery from passive measurements. In SIGCOMM,
2008.
[10] P. Gupta and N. McKeown. Algorithms for packet
classiﬁcation. IEEE Network, 15(2), 2001.
[11] The Internet traﬃc archive. http://ita.ee.lbl.gov/.
[12] S. Kandula, R. Chandra, and D. Katabi. What’s going
on? Learning communication rules in edge networks.
In SIGCOMM, 2008.
[13] A. Lakhina, M. Crovella, and C. Diot. Diagnosing
network-wide traﬃc anomalies. In SIGCOMM, 2004.
[14] F. McSherry. Privacy integrated queries: An
extensible platform for privacy-preserving data
analysis. In SIGMOD, 2009.
[15] F. McSherry and I. Mironov. Diﬀerentially private
recommender systems: building privacy into the
Netﬂix prize contenders. In KDD, 2009.
[16] G. Minshall. tcpdriv. http:
//ita.ee.lbl.gov/html/contrib/tcpdpriv.html.
[17] J. Mirkovic. Privacy-safe network trace sharing via
secure queries. In workshop on Network Data
Anonymization, 2008.
[18] P. Mittal, V. Paxson, R. Summer, and
M. Winterrowd. Securing mediated trace access using
black-box permutation analysis. In HotNets, 2009.
[19] J. C. Mogul and M. F. Arlitt. SC2D: An alternative to
trace anonymization. In MineNet workshop, 2006.
[20] A. Narayanan and V. Shmatikov. Robust
de-anonymization of large sparse datasets. In Security
and Privacy, 2008.
[21] R. Pang, M. Allman, V. Paxson, and J. Lee. The devil
and packet trace anonymization. SIGCOMM CCR,
36(1), 2006.
[22] R. Pang and V. Paxson. A high-level programming
environment for packet trace anonymization and
transformation. In SIGCOMM, 2003.
[23] Network trace analysis using PINQ. http:
//research.microsoft.com/pinq/networking.aspx.
[24] V. Rastogi and S. Nath. Diﬀerentially private
aggregation of distributed time-series with
transformation and encryption. In SIGMOD, 2010.
[25] J. Reed, A. J. Aviv, D. Wagner, A. Haeberlen, B. C.
Pierce, and J. M. Smith. Diﬀerential privacy for
collaborative security. In EuroSec, 2010.
[26] B. Ribeiro, W. Chen, G. Miklau, and D. Towsley.
Analyzing privacy in enterprise packet trace
anonymization. In NDSS, 2008.
[27] S. Singh, C. Estan, G. Varghese, and S. Savage.
Automated worm ﬁngerprinting. In OSDI, 2004.
[28] N. Spring, R. Mahajan, and T. Anderson. Quantifying
the causes of path inﬂation. In SIGCOMM, 2003.
[29] L. Sweeney. k-anonymity: A model for protecting
privacy. Int’l Journal of Uncertainty, Fuzziness, and
Knowledge-Based Systems, 10(5), 2002.
[30] K. V. Vishwanath and A. Vahdat. Swing: realistic and
responsive network traﬃc generation. ToN, 17(3),
2009.
[31] J. Xu, J. Fan, M. Ammar, and S. Moon.
Preﬁx-preserving IP address anonymization:
Measurement-based security evaluation and a new
cryptography-based scheme. In ICNP, 2002.
[32] Y. Yu, M. Isard, D. Fetterly, M. Budiu, ´Ulfar
Erlingsson, P. K. Gunda, and J. Currey. DryadLINQ:
A system for general-purpose distributed data-parallel
computing using a high-level language. In OSDI, 2008.
[33] Y. Zhang and V. Paxson. Detecting stepping stones.
In USENIX Security, 2000.
134