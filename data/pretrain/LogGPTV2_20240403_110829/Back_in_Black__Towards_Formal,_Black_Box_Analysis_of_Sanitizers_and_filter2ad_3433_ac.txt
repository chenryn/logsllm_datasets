–
–
The algorithm fails if such set of pairs does not exist.
Given a predicate family P that is equipped with a guard
generator algorithm, our SFA learning algorithm employs a
special structure observation table SOT = (S, W, Λ, T ) so
that the table T has labelled rows for each string in S ∪ Λ
where Λ ⊆ S · Σ. The initial table is SOT = {S = {ε}, W =
{ε}, Λ = ∅, T}. Closedness of SOT is determined by checking
that for all s ∈ S it holds that sb ∈ Λ → ∃s(cid:5) ∈ S : (sb ≡
s(cid:5)
mod W ). Furthermore the table is reduced if and only if
for all s, s(cid:5) ∈ S it holds that s (cid:11)≡ s(cid:5)
mod W . Observe that the
initial table is (trivially) closed and reduced.
Our algorithm operates as follows. At any given step, it
will check T for closedness. If a table is not closed, i.e., there
is a sb ∈ Λ such that sb (cid:11)≡ s(cid:5) for any s(cid:5) ∈ S, the algorithm
will add sb to the set of access strings S updating the table
accordingly.
On the other hand, if the table is closed, a hypothesis SFA
H = (QH , qε, F,P, Δ) will be formed in the following way.
For each s ∈ S we deﬁne a state qs ∈ QH. The initial state
is qε. A state qs is ﬁnal iff T (s, ε) = 1. Next, we need to
determine the move relation that contains triples of the form
) with φ ∈ P. The information provided by SOT for
(q, φ, q(cid:5)
each qs is the transitions determined by the rows T (sb) for
which it holds sb ∈ Λ. Using this we form the pairs (b, qs(cid:2) )
such that sb ≡ s(cid:5)
mod W (the existence of s(cid:5) is guaranteed
by the closedness property). We then feed those pairs to the
guardgen() algorithm that returns a set Gqs of pairs of the
form (φ, q). We set guard(qs) = {φ | (φ, q) ∈ Gqs
} and
add the triple (qs, φ, q) in Δ. Observe that by deﬁnition the
above process when executed on the initial SOT returns as
the hypothesis SFA a single state automaton with a self-loop
marked with true as the single transition over the single state.
Processing Counterexamples. Assume now that we have a
hypothesis SFA H which we submit to the equivalence oracle.
In case H is correct we are done. Otherwise, we obtain a coun-
terexample string z. First, as in the L∗ algorithm, we perform
a binary search that will identify some i0 ∈ {0, 1, . . . ,|z|− 1}
for which the response of the target machine is different
for the strings si0 z>i0 and si0+1z>i0+1. This determines a
new distinguishing string deﬁned as d = z>i0+1. Notice that
b (cid:11)≡ si0+1 mod W ∪ {d} something that reﬂects that si0
si0
over b should not transition to si0+1 as the hypothesis has
b (cid:11)≡ sj mod W ∪ {d} for any j, the
predicted. In case si0
table will become not closed if augmented by d and thus
the algorithm will proceed by adding d to W and update
9696
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:23 UTC from IEEE Xplore.  Restrictions apply. 
the table accordingly (this is the only case that occurs in
the L∗ algorithm). On the other hand, it may be the case
that adding d to SOT preserves closedness as it may be that
si0 b ≡ sj mod W ∪ {d} for some j (cid:11)= i0 + 1. This does
not contradict the fact that the table prior to its augmentation
was reduced, as in the case of the L∗ algorithm, since the
transition si0 to si0+1 when consuming b that is present in
the hypothesis could have been the product of guardgen()
and not an explicit transition deﬁned in Λ. In such case Λ
b and the algorithm will issue another
is augmented with si0
equivalence query, continuing in this fashion until the SOT
becomes not closed or the hypothesis is correct.
The above state of affairs distinguishes our symbolic learn-
ing algorithm from learning via the L∗ algorithm: not every
equivalence query leads to the introduction of a new state.
We observe though that some progress is still being made:
if a new state is not discovered by an equivalence query, the
set Λ will be augmented making a transition that was before
implicit (deﬁned via a predicate) now explicit. For suitable
predicate families this augmentation will lead to more reﬁned
guard predicates which in turn will result to better hypothesis
SFA’s submitted to the equivalence oracle and ultimately to
the reconstruction of an SFA for the target.
In order to establish formally the above we need to prove
that the algorithm will converge to a correct SFA in a ﬁnite
number of steps (note that the alphabet Σ may be inﬁnite
for a given target SFA and thus the expansion of Λ by each
equivalence query is insufﬁcient by itself to establish that the
algorithm terminates).
Convergence can be shown for various combinations of
predicate families P and guardgen() algorithms that relate to
the ability of the guardgen() algorithm to learn guard predi-
cates from the family P. One such case is when guardgen()
learns predicates from P via counterexamples. Let G ⊆ 2
P a
guard predicate family. Intuitively, the guardgen() algorithm
operates on a training set containing actual transitions from
a state that were previously discovered. Given the symbols
labeling those transitions, the algorithm produces a candidate
guard set for that state. If the training set is small the candidate
guard set is bound to be wrong and a counterexample will
exist. The guardgen() algorithm learns the guard set via
counterexamples if by adding a counterexample in the training
set in each iteration will eventually stabilize the output of
the algorithm to the correct guard set. We will next deﬁne
what a counterexample means with respect to the guardgen()
algorithm, a set of predicates φ and an input to guardgen()
which is consistent with φ. Recall that inputs to guardgen()
are sets R of the form (b, si) where b is a symbol and si is a
label; a set R is consistent with φ if it holds that φi(b) is true
for all (b, si) ∈ R (we assume a ﬁxed correspondence between
the labels si and the predicates φi of φ). A counterexample
would be a pair (b∗, s∗
) where s∗ labels a predicate φj in φ
but the output predicate φ of guardgen() that is labelled by sj
disagrees with φj on symbol b∗. More formally we give the
following deﬁnition.
Deﬁnition 8. For k ∈ N, consider a set of predicates
φ = {φ1, . . . , φk} ∈ G labelled by s = (s1, . . . , sk) so that
φi is labelled by si and a sequence of samples R containing
pairs of the form (b, si) where φi(b) for some i ∈ [k]. A
counterexample (b∗, s∗
) for (R, φ, s) w.r.t. guardgen() is a
pair such that if G = guardgen(R) it holds that there is a
) (cid:11)= φj(b∗
j ∈ {1, . . . , k} with sj = s∗, (φ, sj) ∈ G and φ(b∗
).
Let t be a function of k. A guard predicate family G is t-
learnable via counterexamples if it has a guardgen() algorithm
such that for any φ = (φ1, . . . , φk) ∈ G labelled by s =
(s1, . . . , sk), it holds that the sequence R0 = ∅, Ri = Ai ∪
Ri−1 where Ai is a singleton containing a counterexample
for (Ri−1, φ, s) w.r.t. guardgen() (or empty if none exist),
satisﬁes that guardgen(Rj) = {(φi, si) | i = 1, . . . , k} for any
j ≥ t. In other words, a guard predicate family is t-learnable if
the guardgen() converges to the target guard set in t iterations
when in each iteration the training set is augmented with a
counterexample from the previous guard set.
We are now ready to prove the correctness of our SFA
learning algorithm.
Theorem 1. Consider a guard predicate family G that is t-
learnable via counterexamples using a guardgen() algorithm.
The class of deterministic symbolic ﬁnite state automata with
guards from G can be learned in the membership and equiva-
lence query model using at most O(n(log m+n)t(k)) queries,
where n is size of the minimal SFA for the target language,
m is the maximum length of a counterexample, and k is the
maximum outdegree of any state in the minimal SFA of the
target language.
In appendix D we describe an example of a guardgen()
algorithm when SFAs are used to model decision trees.
B. A Learning Algorithm for RE Filters
∗(.)
Consider the SFA depicted in ﬁgure 1 for the regular
∗. This represents a typical regular ex-
expression (.)
pression ﬁlter automaton where a speciﬁc malicious string is
matched and at that point any string containing that malicious
substring is accepted and labeled as malicious. When testing
regular expression ﬁlters many times we would have to test
different character encodings. Thus, if we assume that the
alphabet Σ is the set of two byte chatacter sequences as
then each state would have 216
it would be in UTF-16,
different transitions, making traditional learning algorithms too
inefﬁcient, while we point out that the full unicode standard
contains around 110000 characters.
We will now describe a guard generator algorithm and
demonstrate that it efﬁciently learns predicates resulting from
regular expressions. The predicate family used by our algo-
rithm is P = 2Σ where Σ is the alphabet of the automaton,
for example UTF-16. The guard predicate family Gl,k is
parameterized by integers l, k and contains vectors of the form
(cid:15)φ1, . . . , φk(cid:2)(cid:16) with k(cid:5) ≤ k, so that φi ∈ P and2 |φi| ≤ l
for any i, except for one, say j, for which it holds that
φj = ¬(∨i(cid:6)=jφi). The main intuition behind this algorithm
is that, for each state all but one transitions contain a limited
number of symbols, while the remaining symbols are grouped
into a single (sink) transition.
) is called normal
if |φ| ≤ l. A transition that is not normal is called a sink
transition. Our algorithm updates transitions lazily with new
In an SFA over Gl,k, a transition (q, φ, q(cid:5)
2We use the notation |φ| = |{b | φ(b) = 1}|.
9797
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:23 UTC from IEEE Xplore.  Restrictions apply. 
x (cid:11)=
q3
true
A. Improved learning of Mealy machines
x (cid:11)=>
Fig. 1. SFA for regular expression (.)∗(.)∗.
symbols whenever a counterexample shows that a symbol
belongs to a different transition, while the transition with the
largest size is assigned as the sink transition.
Consider R, an input sequence for the guard generator
algorithm. We deﬁne Rq = {(b, q) | (b, q) ∈ R}. If |Rq| ≤ l
then we deﬁne the predicate for Rq denoted by φq. Let q(cid:5) be
such that |Rq(cid:2)| ≥ |Rq| for all q. We deﬁne σ = Σ
∗ \∪q(cid:6)=q(cid:2) Rq.
The output is the set G = {(φq, q) | q (cid:11)= q(cid:5)} ∪ {(σ, q(cid:5)
)}. In
case R = ∅ the algorithm returns Σ
∗ as the single predicate.
We observe now that Gl,k is t-learnable via counterex-
amples with t = O(lk). Indeed, note that counterexamples
will be augmenting the cardinality of the predicates that
are constructed by the guard generator. At some point one
predicate will exceed l elements and will correctly be identiﬁed
as the sink transition. We conclude that the target SFA will be
inferred using O(nlk(log m + n)) queries.
V. LEARNING TRANSDUCERS
In this section we present our learning algorithms for
transducers. We start with our improved algorithm for Mealy
machines and then we move to single-valued transducers with
bounded lookahead. We conclude with how to extend our
results to the symbolic transducer setting. To motivate this
section we present in Figure 5 three examples of common
string manipulating functions. For succinctness we present the
symbolic versions of all three sanitizers. The ﬁrst example is
a typical tolowercase function which converts uppercase
ascii
letters to lowercase and leaves intact any other part
of the input. The second example is a simpliﬁed HTML
Encoder which only encodes the character “j to M in order to obtain a
· suﬀ(γM
j ,|z(cid:5)| − j) and observe that
string γM
γ0 = TM (z(cid:5)
) and γ0 (cid:11)= γ|z(cid:2)|.
≤j to obtain γH
j . Let γj = γH
j
), γ|z(cid:2)| = TH (z(cid:5)
The binary search then is performed in this fashion. The
initial range is [0,|z(cid:5)|] and the middle point is j = (cid:19)|z(cid:5)|/2(cid:20).
Given a range [jleft, jright] and a middle point position j, we
check whether γj = γ0; if this is the case we set the new range
as [j, jright] else we set the new range as [jleft, j − 1] and we
continue recursively. The process ﬁnishes when the range is a
singleton [j0, j0] which is the output of the search.
9898
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 12:12:23 UTC from IEEE Xplore.  Restrictions apply. 
Fig. 2.
machine.
ToLowerCase function. Mealy
Fig. 3. Simpliﬁed version of HTML Encoder
function. Deterministic Transducer with mul-
tiple output symbols per transition.
Fig. 4. ReplaceComments Mod-security
transformation function. Non deterministic
Transducer with  transitions and 1-lookhead.
Fig. 5. Three different sanitizers implementing widely used functions and their respective features when modeled as transducers. Only the ﬁrst sanitizer can
be inferred using existing algorithms.
Theorem 2. The binary search process described above re-
turns j0 ∈ {0, . . . ,|z(cid:5)| − 1} such that γj0 (cid:11)= γj0+1.
–
Given such j0, we observe that since the preﬁxes of
, γj0+1 that correspond to the processing of z≤j0 are identi-
γj0
cal by deﬁnition, the difference between the strings should lie
in their sufﬁxes. Furthermore, (γj0 )j0+1 = (γj0+1)j0+1 since
the former is the last output symbol produced by H when