similarity metrics listed in Table 3 as features, and exper-
iment with 4 well-known classiﬁers: Support Vector Ma-
chine (SVM), Logistic Regression, Naive Bayesian (NB),
and Random Forests (RF)3. We also considered but ultimately
rejected Decision Trees, because they can only produce bi-
nary recommendations, and are effectively subsumed by Ran-
dom Forests.
We ran experiments on a wide range of network snap-
shots, and found the classiﬁers were consistent in their rela-
tive performance. RF and NB always performed poorly, and
LR performed generally on par with SVM. In addition, we
found that SVM outperformed LR with imbalanced training
sets, which has been also shown in prior work [15]. Since
our data is highly imbalanced, SVM’s results are uniformly
the best of the bunch, and we use SVM results for the re-
mainder of our discussion. As an example of the relative
accuracy results, we plot in Figure 9 prediction accuracy ra-
tio for all 4 classiﬁers on a Facebook network snapshot of
345K edges.
5.2 Link Prediction Accuracy
To evaluate classiﬁcation-based prediction, we must ﬁrst
understand the impact of data imbalance within training sets.
Recall that link formations in social networks are extremely
imbalanced, i.e.
far fewer connected node pairs than dis-
connected. This imbalance has been shown to contribute to
classiﬁcation errors [15]. For this we apply the well-known
undersampling technique to build training data, keeping all
positive node pairs while varying the number of negative
node pairs [15]. Here positive(negative) node pairs refer to
those which will(not) connect in the prediction timeframe.
Figure 10 plots prediction accuracy ratio while varying the
under-sampling ratio, θ=(# of positive node pairs : # of neg-
3We use the implementation in an open source library [34]
with default parameters for all classiﬁers in this paper.
Graph
Facebook
YouTube
Renren
small
large
small
large
small
large
Gt−2
Gt−1
Edges Nodes
49K
345K
600K
57K
Edges
Nodes
360K
49K
56K
615K
1.63M 4M 1.74M 4.25M
2.63M 7M 2.70M 7.25M
2.3M 25M 2.7M
30M
6.2M 95M 6.7M 105M
Snowball
Sampling p
100%
100%
2%
2%
2%
2%
Table 6: Data instances for evaluating classiﬁcation algo-
rithms.
1:1
1:50
o
i
t
a
R
y
c
a
r
u
c
c
A
 4000
 3000
 2000
 1000
 0
RF
NB
LR
SVM
Figure 9: Accuracy ratio of four classiﬁers with under-
sampling ratio θ 1:1 and 1:50 (Facebook, 345K edges).
o
i
t
a
R
y
c
a
r
u
c
c
A
 15000
 12000
 9000
 6000
Renren
1:1 1:10 1:100
1:1000
Undersampling Ratio (θ)
1:5000
o
i
t
a
R
y
c
a
r
u
c
c
A
 3500
 3000
 2500
 2000
Facebook
1:1 1:10 1:100
1:1000
Undersampling Ratio (θ)
1:10000
o
i
t
a
R
y
c
a
r
u
c
c
A
 5000
 4000
 3000
 2000
 1000
 0
YouTube
1:1 1:10 1:100
1:1000
Undersampling Ratio (θ)
1:10000
Figure 10: Performance of classiﬁcation-based prediction as a function of the under-sampling ratio θ used during
classiﬁer training.
ative node pairs), from (1:1) to (1:10000)4. For our three
OSNs, the true (unsampled) positive vs. negative ratio is
around (1:100000). Note that existing classiﬁcation-based
prediction algorithms generally use balanced node pairs, or
a ratio of (1:1).
These results show that classiﬁcation-based prediction al-
gorithms are signiﬁcantly better than random prediction for
all 5 sampling ratios. For Renren and Facebook, accuracy
ratio improves as the sampling ratio θ approaches the actual
positive vs. negative ratio (1:100000). Compared to con-
ventional balanced sampling (1:1), a lower under-sampling
ratio produces signiﬁcantly more accurate results, and im-
provements in accuracy ratio, and also the accuracy, can be
as high as a factor of 5.
The above results conﬁrm the effectiveness of classiﬁcation-
based link prediction. More importantly, we show that the
performance of these algorithms depends on the conﬁgura-
tion of training data. Conventional methods of using bal-
anced training data can lower prediction accuracy by as much
as a factor of 5. To minimize such loss, we need to invest ef-
forts on ﬁnding the right level of undersampling ratio (θ).
5.3 Comparing to Metric-based Algorithms
For a fair comparison, we run the metric-based methods
again on the same sampled data (V S
t−1). We plot the accu-
racy ratio for each algorithm (blue circle on the left) in Fig-
ure 11, and rank them in descending order from right to left.
We see that the top (most accurate) similarity metrics are
generally consistent on both the sampled data and the entire
4We stop at (1:10000) for YouTube and Facebook and
(1:5000) for Renren because this is the largest training size
we can support on our memory-heavy servers (192GB RAM
each).
network (see §4) across different datasets. Also note that
the test dataset V S
t−1 is smaller than Vt−1 and better con-
nected, the accuracy ratio of the metric-based algorithms is
lower than results previously shown in §4 (accuracy ratio is
lower because random prediction does better on this smaller
dataset).
Figure 11 plots the accuracy ratio
Comparing Accuracy.
of SVM (red cross on the right) and metric-based methods
(blue circle on the left). With a well-chosen θ, SVM consis-
tently performs as well as, or outperforms the best metric-
based algorithms. This outperformance stems from two fac-
tors: combining multiple similarity metrics to broaden cov-
erage, and using under-sampling to address the issue of data
imbalance. Overall, these results show that among existing
algorithms, the SVM classiﬁer provides consistently strong
results. However, we also note that some similarity metrics,
namely RA and BRA, provide consistently “good” results
across all of our networks. In scenarios where the computa-
tional or training costs of SVMs were undesirable, RA and
BRA provide reasonable alternatives with much lower com-
putational complexity.
Similarity Metric Ranking vs. SVM Feature Weight. We
seek to understand whether a good similarity metric in the
metric-based method (identiﬁed from Figure 11) also be-
comes a dominant feature for the classiﬁcation method. For
this we use the feature coefﬁcient provided by SVM, where
a larger absolute value means the feature is more important.
To make a fair comparison, we normalize the coefﬁcients
(using absolute values) within each classiﬁer.
We take two steps to study the relationship between top
similarity metrics and top SVM features. First, we directly
compare the rankings of the two. For both Renren and Face-
book, the rankings are very similar between the similarity
o
i
t
a
R
y
c
a
r
u
c
c
A
100000
10000
1000
100
10
1
Renren
Metric-based
Classificaion
J
L
L
P
S
R
C
A
B
K
B
R
B
P
1
1
1
1
1
o
i
t
a
R
y
c
a
r
u
c
c
A
10000
1000
100
10
1
Facebook
Metric-based
Classification
o
i
t
a
R
y
c
a
r
u
c
c
A
10000
1000
100
10
1
YouTube
Metric-based
Classification
J
J
L
L
L
L
S
P
P
R
C
B
A
B
R
B
K
S
P
P
R
K
B
B
A
C
B
R
P
A
P
C
R
e
P
N
C
A
A
A
R
a
a
A
P
e
R
P
N
A
C
A
A
R
a
P
C
:
:
:
:
:
1
1
1
1
5
R
s
W
c
N
A
A
z
t
1
1
1
1
1
:
:
:
:
:
1
1
1
1
1
0
0
0
0
C
P
P
A
R
A
R
A
A
N
C
P
e