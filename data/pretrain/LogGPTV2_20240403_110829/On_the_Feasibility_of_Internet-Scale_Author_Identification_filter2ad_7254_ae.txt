[12] as ones which spend time computing a model from
training data versus ones, such as nearest neighbors, that
simply store the data and rely on a good distance function for
classiﬁcation. This distinction appears in Koppel et al. under
a different name when they discuss similarity-based (lazy)
and machine-learning (eager) classiﬁcation paradigms. We
ﬁnd this black-and-white distinction to be unhelpful because
methods often fall on a spectrum. Indeed our implementation
of nearest neighbor — a prototypical
lazy method —-
summarizes information by learning the mean of each class.
Moreover, the one-versus-all method for SVM or RLSC
bears a substantial resemblance to the lazy approach because
dot products are a similary measure. Similarly, LDA, which
at ﬁrst blush is an eager method, is also a similarity-based
method because it classiﬁes by ﬁnding the Mahalanobis dis-
tance — a generalization of Euclidean distance — between
the mean of each class and the point in question. Finally
unlike Koppel et al., our experiments indicate that when
properly conﬁgured, lazy and eager approaches have a very
similar performance proﬁle for all values of N, the number
of classes.
C. Conﬁdence estimation and combining classiﬁers
A conﬁdence estimator is a real-valued function that
takes the input/output pair of a classiﬁer as input, and
that outputs a positive score. Larger scores correspond to
higher conﬁdences; the score is intended to be monotonically
increasing in the probability of correctness of the classiﬁer’s
output. A more sophisticated conﬁdence estimator might
be able to produce an actual probability score, but this
stronger requirement is not a goal for us. Applying different
thresholds to the conﬁdence score allows to achieve various
trade-offs between precision and recall.
In more detail, consider a classiﬁer C and a conﬁdence
estimator confC. This allows us to build a classiﬁer Ct
parametrized on a threshold t, which outputs C(x) on input
x if confC(x, C(x)) ≥ t and ⊥ otherwise. The recall of Ct
is the ratio of correct outputs of Ct to the correct outputs
of C, and its precision is the probability of a correct answer
on only the examples it attempts to answer.
In general, recall and precision are inversely related; if
we attempt to label more examples, we will produce more
examples with correct labels, but even more examples with
incorrect labels. A good estimate of conﬁdence is critical
to the recall-precision tradeoff. If the assumption about the
monotonicity of the relationship between conﬁdence and
probability of correctness holds, then recall and precision
will be inversely related. However, if we are unfortunately
misguided to attempt an answer the harder a point is to label,
lower recall rates will also have a lower precision!
We use the “gap statistic” that was described in [13]; a
similar heuristic called “eccentricity” was described in [52].
The gap statistic applies to classiﬁers that output a distance
or a similarity score between a test point and each class. This
is a property that’s required in practice for many purposes,
for example, to employ a one-versus-all strategy. Recall that
nearest neighbor, SVM and RLSC all output a distance or
distance-like score.
At its core, the gap statistic simply outputs the magni-
tude of the difference between the best match (i.e., lowest
distance) and second best match. The rationale is a simple
Bayesian argument: if the output of the classiﬁer is incorrect,
the best score is unlikely to be well-separated from the
rest of the scores—there is no particular reason to expect
one incorrect class to match signiﬁcantly better than other
309
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
incorrect classes.9
On the other hand, it is possible, although by no means
certain, that the best score is highly separated from the rest
when the classiﬁer output is correct. Intuitively, the more
clustered together the points of a class are, i.e., the more
distinct an author’s writing style, the more likely this is to
happen. Flipping around the prior and posterior, we see that
a high gap statistic implies that it is more likely that the
classiﬁer output is correct. We found that when using the
gap statistic with nearest-neighbors, it was also useful to
perform the row-norm normalization. No normalization was
necessary for RLSC.
it
is not
Finally, while the gap statistic is the key ingredient of
the conﬁdence score,
the only one. We use a
meta-learner that combines the gap with one other feature,
namely the number of available training points for the top
scoring class. The rationale for this feature is the obvious
one: classiﬁcations based on more training data are more
likely to be correct. In general, we can imagine using any
(or all!) features of the input, although we do not have a
strong reason to believe that this will result in a signiﬁcant
improvement.
Figure 4. Effect of test set size.
An entirely different approach to conﬁdence estimation is,
given a classiﬁer C, to employ an auxiliary classiﬁer Caux,
and to measure the level of agreement between the outputs
of C and Caux. In the simplest case, we check whether or
not the top match in both cases is the same, and output 1 or 0
accordingly. Thresholding based on this conﬁdence measure
is equivalent to only producing an output if C and Caux
produce the same output. Thus the roles of C and Caux are
symmetrical.
This leads naturally to the idea of combining two (or
more) classiﬁers, both for improving accuracy and for con-
ﬁdence estimation. The two goals are closely linked to each
other. As before, we can use a meta-learner based on all
available features to pick the output of one or the other
classiﬁer, although we choose to restrict ourselves to the
gap statistic and the number-of-training-points feature.10. For
conﬁdence estimation, we use the above, together with a
measure of agreement between the two classiﬁers. The latter
is derived by looking at the top k outputs of each classiﬁer.
As the reader is no doubt aware, there is a trememdous
amount of work in the machine learning literature on stacked
generalization, boosting and ensemble methods. Our goal in
investigating combinations of classiﬁers is not to break new
ground in these areas, but rather to show that it improves
accuracy, demonstrating that our numerical results are only
a lower bound. We suspect that an adversary interested in
squeezing out the last percentage-point of precision will be
9Which is not to say that it can’t happen: in the pathological worst
case, all blog posts in the test set might consist of text entirely quoted
from another blog, and yet not denoted as such via HTML tags, making it
challenging to ﬁlter out automatically.
10Since there are two classiﬁers, this gives us a four-dimensional space
Figure 5. Effect of training set size.
able to go quite a bit farther.
VI. EXPERIMENTAL RESULTS
In Figure 6 which summarizes our most important results,
we provide the full distribution of outcomes obtained by ap-
plying various classiﬁers to the post-to-blog experiment III.
We now give the details of the procedure used to obtain
these results.
In each trial, we randomly selected three posts of one
blog and set them aside as the testing data. The classiﬁers
were then used to rank each blog according to its estimated
likelihood of producing the test posts. Of course, we were
careful to ensure that the classiﬁers were not given the
test posts during training. For this experiment, we only
selected blogs from the Spinn3r dataset as the source of
test posts, but we used the classiﬁers to rank all 100,000
blogs. In each trial, we recorded the rank of the correct
blog; Figure 6 displays the CDF of these rankings. NN1
refers to nearest neighbor with the normalizations row-norm
310
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
2468101200.050.10.150.20.250.3# ExamplesAccuracy  RawNormalized10010110210300.050.10.150.20.250.30.350.40.45# ExamplesAccuracyFigure 6. Results of the post-to-blog matching experiments, using three posts (roughly 900 words). The y-axis is the probability that the correct class
was among the top K ranks, for each K (x-axis).
Figure 7. Results of the blog-to-blog matching experiments.
Several interesting results can be directly read off the
graphs.
• SVM’s accuracy drops off rapidly to the left of rank =
100. This provides evidence for the masking problem.
In particular, SVM’s accuracy when looking at only the
top ranked blog is essentially zero.
• Naive Bayes and nearest neighbor with the straightfor-
ward normalization (NN1), by default perform surpris-
ingly well for such simple methods, with the top ranked
class being the correct one in about 8% of cases for
Naive Bayes.
• Better normalization makes a tremendous difference for
nearest neighbor: it quadruples the accuracy at rank =
1 and almost triples it even at rank = 100.11
• RLSC is the best individual classiﬁer for rank ≥ 3.
It has a nearly 30% probability of placing the correct
blog among the top 10 results.
• The metaclassiﬁer using NN2 + RLSC is better than
any individual classiﬁer for small k. It picks the correct
11Although RLSC with row-norm and feature-mean isn’t shown, the
difference between feature-mean and feature-mean-nonzero was even
more pronounced for RLSC.
311
Figure 8. Conﬁdence estimation: precision vs. recall.
and feature-mean and NN2 uses row-norm and feature-
mean-nonzero. NN2+RLSC is a combination of those two
classiﬁers using a meta-classiﬁer as described in the previous
section.
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
10010110210310410500.20.40.60.81Rank of Correct Blog of out 100,000Cumulative Portion of Trials  SVMNN1NBNN2NN2+RLSCRLSC10010110210310410500.20.40.60.81Rank of Correct Blog out of 99,999Cumulative Portion of Trials  NN1SVMNB00.20.40.60.810.10.20.30.40.50.60.70.80.91RecallPrecision  NN2NN2+RLSCRLSCFigure 9. Post-to-blog matching using three posts and NN2 while limiting the feature set.
class as the top ranked match about 20% of the time,
and places it within the top 20 ranks about 35% of the
time.
Impact of training set and test set size. Figure 4 displays
the results of the post-to-blog matching experiment when the
number of unlabeled testing examples varies. At an average
of 305 words, one post is similar in size to a comment on
a news article or a message board post, so these results are
indicative of the ability to match an isolated online posting
against our blog corpus. Some blogs do not have enough
data so we can only use a subset of the blogs as the number
of testing examples increases beyond 3. The “Normalized”
line adjusts for population bias by subtracting the difference
in accuracy between each subset and all of the blogs when
measured at 3 testing examples.
Our results indicate that while the identiﬁability of small
amounts of text is markedly reduced, we still achieve a
7.5% accuracy. Furthermore, the curves appear to reach an
asymptote at around 10 samples per class with an accuracy
of around 25%. We believe that this accuracy is a lower
bound on the performance that could be achieved with more
data; with an average of 24 posts per blog, our algorithms
have little data to work with once we remove 10 posts for
testing.
Figure 5 shows the impact of training set size. The number
of training examples is a different factor from the amount of
text (in words), although of course they are highly correlated.
The former seems a better predictor of performance in our
experiments. One possible reason is that text split up in
different posts over time allows us to capture the variance in
an author’s stylometric markers. This argues against methods
that work by collapsing all available text
into a single
document.
Two points are evident from the graph: ﬁrst, accuracy
is poor with less than 7-10 training examples. Second, we
continue making signiﬁcant gains until about 40 examples
or so, after which accuracy plateaus. This suggests that
authors who wish to publish anonymously should consider
the amount of material they have already written that appears
online.
Conﬁdence estimation Figure 8 shows the results of
conﬁdence estimation for 3 classiﬁers with precision traded
off against recall via the metaclassiﬁer described in Section
V-C. A precision of 50% is achieved with a recall of just
under 80%, and conversely, a 50% recall gives over 80%
precision!
Blog-to-blog matching. Each trial of the blog-to-blog
matching experiment consisted of randomly selecting one
of the blogs obtained from the Google proﬁles, then ranking
the other 99,999 blogs based on their similarity to its posts.
The rank of the other blog listed on the same Google proﬁle
was then saved as the outcome, producing the results given
in Figure 7. In the (uncommon) case of more than one
additional blog listed on the Google proﬁle, the highest
ranked was considered the outcome, on the grounds that
an adversary would be interested in linking an anonymous
blog to any material from the same author.
We only show the classiﬁers NB, SVM and NN1. NN2
produced little or no gains compared to NN1; we suspect
that this is because the difference in normalization is mainly
effective when the number of test samples is low. Unfortu-
nately, we perfected our RLSC technique only very recently,
and have not yet used it for blog-to-blog matching since it
requires some analytical modiﬁcations.
We also applied our conﬁdence estimator and obtained
results similar to the post-to-blog experiments; for example,
about 50% precision with 50% recall. The full results are
omitted.
Impact of feature choice. To help conﬁrm the validity of
our results, we also manually inspected a small sample of
the blogs that were most easily matched in each experiment,
since these would be the ones most likely to contain any
post signatures or other illegitimate text that might have
escaped our ﬁltering. Nothing that could signiﬁcantly affect
the results was found. As a further check, we ran our
experiments with the NN2 classiﬁer using only subsets of the
312
Authorized licensed use limited to: IEEE Xplore. Downloaded on March 18,2021 at 11:49:27 UTC from IEEE Xplore.  Restrictions apply. 
10010110210310410500.20.40.60.81Rank of Correct Blog of out 100,000Cumulative Portion of Trials  Single character frequencies onlyFunction words onlySyntactic category pairs onlyOther 38 features onlyAll features except syntactic pairsAll featuresfeatures in order to determine whether one particular type
of feature was especially crucial to its performance. The
results are shown in Figure 9. We can break our features
into four groups, and these are the ﬁrst 4 feature sets shown
in the graph. It is clear that none of the feature sets alone
come close to the 18% accuracy we get when using all
of the features. Using single character frequencies, function
words, or syntactic category pairs alone gives about the same
performance at 7 − 8%. We also see that the syntactic pair