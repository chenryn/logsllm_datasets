iteration of the sample i correlates with the predicted results of the
(t-1) th tree. As in formula (6):
(t)
ˆy
i
=
fn(xi) = ˆy
(t−1)
i
+ ft(xi)
(6)
XGBoost maintains high precision while performing parallel
operation on the feature gain at the feature granularity, which
significantly improves the fitting speed.Mouse sliding track may
have different coordinate points at the same time point due to
sampling. When calculating features, missing values will appear.
Compared with other linear models, XGBoost model allows the
existence of missing values. It can specify branch direction for
missing data, which can solve the problem of missing data caused
by sampling.
5.4 Evasion Attack Detection
Slide evasion attacks can be detected based on feature similarity be-
tween sliding tracks, but identical track characteristics are difficult
to appear. A structure feature table is built to record the sample
characteristics that have been predicted by the model, and the k
nearest neighbor algorithm is used to calculate the feature similar-
ity using Manhattan distance. Sliding tracks contain displacement
points and time points. By calculating Manhattan distances for
velocity characteristics and summing them up, tracks smaller than
the threshold k are considered evasion attacks.
Threshold K is defined as the minimum difference of feature
distances in the sample set. The speed-related features extracted
in this paper include start, middle and end points. These speed
features are not used as classification features, but they can be used
for evasion detection by calculating the lateral and vertical speed
characteristics of the training dataset. Starting speed, ending speed,
average value and variance are selected as evasion attack detection
features. In this paper, the Manhattan distance is calculated and
summed for any two sample features in the training sample set,
and the minimum value after removing the zero value is set to the
threshold K.
In this paper, we propose an EDMD (Evasion Detection using
Manhattan Distance) algorithm. If we traverse the structure feature
table and calculate the difference value of each sample first, then
the absolute value, and finally the sum, it will bring high time
consumption. If vector operation is used, the efficiency can be
greatly improved. When the sample vectors are subtracted from
the structure characteristic table matrix in the same dimension,
the feature difference of the sample records in the table can be
obtained. The sum is made after the absolute value of the feature
difference is calculated. The time required is much less than that
of the former method, and the single sample detection time is 1.6
seconds when the data set is 100000. Through further optimization,
it was found that the absolute value operation is time-efficient, and
the time efficiency of comparison operation is higher than that of
sum operation. The speed of comparison operation in matrix is
much faster than that of single-run sum operation. Therefore, if
there is a difference greater than k value in single-run operation,
it can skip to the next run without performing sum operation and
repeat the process. The single sample detection time for this method
dataset was 0.05 seconds at 100,000 samples.
6 EXPERIMENTS AND RESULTS
6.1 Experimental Datasets
In this paper, two datasets are used, the first one is from mouse
track desensitization data collected by a human-machine validation
product[1], and the second one is from Shen[16] for user authen-
tication. The first dataset contains 103,000 data sets, of which we
used 3,000 samples as training datasets and 100,000 samples as test
datasets. Of these, 3000 training samples contain 2600 human and
400 robot tracks.
The second data set was published by Shen[16] about a total
of 17,400 samples of 58 people. Dataset 2 serves as a validation
dataset to validate model generalization capabilities. The data for-
mat is operation code (512 is sliding under Windows system, 513
is left-button pressed, 514 is left-button raised), x-coordinate and
y-coordinate with the upper left corner of the screen as the origin
and time stamp information after desensitization. We extracted data
sliding in eight directions from the Shen[16] dataset and converted
it into a training dataset format field, resulting in 165,238 human
behavior samples.
370An Efficient Man-Machine Recognition Method Based On Mouse Trajectory Feature De-redundancy
ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Algorithm 1 Evasion Detect.
Input: threshold k; Evasion_Feature f eat; Evasion_lib r; Sus-
for id in SusIdList do
pect_sample s;
Output: bool res
1: absV alue = abs(r − s);
2: дreaterV alues = where(absV alue > k);
3: GV alue = unique(дreaterV alues);
4: allLib = range(len(r));
5: SusIdList = difference(allLib, GV alue);
6: if SusIdList not empty then
7:
8:
9:
10:
11:
12:
13:
14: else
15:
16: end if
17: return res;
if sum(GV alue[i]) < k then
end for
res = 1
res = 0
res = 1
else
end if
training sample as a new training sample and add it to the training
data set. This process is repeated twice, and a total of 9000 training
data sets are obtained, including 7800 human data and 1200 machine
data. Jorgensen[8] put forward the problem of different attributes
of normalized control samples in the literature. The reason why
this paper did not normalize is that the training set and the test set
are generated in the same environment. The trajectory path data
has important distinguishing features in human-computer verifica-
tion, and the data set contains the coordinates of the target points.
Normalization will lead to sample distortion.
6.3 Experiment and Result
The program ran on a Linux server with a CPU of Intel core i7-8700k
and memory size of 48G.
We use the XGBoost Library of Python language as a classifier.
After extracting the sliding track features from the original dataset,
we fill -100 with NaN value data to reduce the impact on the clas-
sification model. We used the Precision, Recall as the evaluation
criteria for the experimental results, that is:
Precision =
T P
T P + F P
(7)
T P
Recall =
T P + F N
(8)
TP refers to normal data that is correctly classified, FP refers to
negative data that is incorrectly labeled as normal data, and FN
refers to positive data that is incorrectly labeled as negative data.
We extract features from the Shen[16] dataset, because the dataset
contains only positive samples, using the false rejection rate (FRR)
as the evaluation indicator on the validation set, and the FRR calcu-
lation formula is (9), where NF A is the number of false acceptances,
NGRA is the number of tests.
FRR = NF A
NGRA
(9)
Feature Importance Analysis.
6.3.1
With the optimal parameters conditions of the XGBoost model, in
order to evaluate the impact of different features to the models, we
use the evaluation model importance function in XGBoost to score
features based on the influence of different features on the split of
decision trees in the model. We select the top 20 features with the
highest feature importance score and remove 13 features with high
Pearson correlation coefficient and add 2 attacking model features.
Figure 10 shows the importance score of the remaining features
and the attacking model features after removing the highly corre-
lated features, which shows that the features have a higher impor-
tance score after removing the redundant features. When the slider
verification code is sliding horizontally, the feature in the x-axis
direction has a strong influence on the model, but the distinction
between the time difference and the feature in the y-axis direction
cannot be ignored.
We also compared the effects of eliminating redundant features
before and after the classification, as shown in Table 3. Under the
premise of using the attacking model features, there are 22 features
before the correlation is removed. The high correlation features
have a negative impact on the experimental results. It has the best
classification effect after the redundancy is removed.
We will process the second data set as the first data set format
by: (1) Start at the press position of the left key and end at the
lift position of the left key. The target point is the pixel difference
relative to the starting point and the the timestamp is a recorded
timestamp. (2) Tracks of each segment in each of the eight directions
are extracted, and only X-axis forward tracks are extracted.
The training samples are collected in the same environment as
the human data in the test samples, where the machine behavior
attack pattern in the training dataset contains the attack pattern
in the test dataset. The sliding path consists of a series of (x, y, t)
three-dimensional coordinate points representing the location of
the mouse at point in time (t) (x, y).The data is desensitized, time
(t) does not correspond to the real time, and samples are taken at
non-fixed intervals during mouse movement. The target coordinate
is the target end point of the mouse movement. Because the real
moving end point and target coordinate will be different, the end
point of the track is not necessarily equal to the target coordinate.
The focus is on the sliding x coordinate.
Table 2: Dataset Fields
Type
unsigned int
string
string
bool
Explanation
Sample number
(x, y, t) path points separated by semicolons
Target location (x, y)
1: Human trajectory;0: Machine trajectory
Field
id
trajectory
target
label
6.2 Data Preprocessing
There is a problem of unbalanced data proportion between training
data set and test data set. We use the random sampling method to
expand the sample set. As the mouse sliding behavior is a continu-
ous trajectory, we randomly select 70% of the coordinates of each
371ACSAC ’21, December 6–10, 2021, Virtual Event, USA
Xiaofeng Lu, Zhenhan Feng, and Jupeng Xia
higher precision. Table 4 shows parameter optimization increases
the precision, recall.
Table 4: Parameter optimization
Method
Before adjusting parameters
After adjusting parameters
Precision Recall
99.86
99.88
98.94
99.09
6.3.3 Dataset experiment.
This experiment uses random forest, support vector machines
(SVM) and GBDT as comparison models, fits training data and
predicts test datasets in the same experimental environment, each
model parameter uses the default parameters of function library.
Table 5 shows the results of the experiment. Experiments show that
XGBoost is slightly better than random forest and GBDT and signif-
icantly higher than SVM in models that use current feature training
with 100,000 datasets when the model uses default parameters.
Table 5: Algorithm evaluation on test dataset
Classification Model
Precision Recall Average time
GBDT
SVM
Random Forest
XGBoost
98.31
96.96
98.69
98.94
99.84
98.95
99.88
99.86
1.32s
1.10s
5.40s
0.23s
Shen[16] dataset contains eight tracks with sliding directions. Ta-
ble 6 is the FRR on the validation set. The XGBoost method has the
lowest error rejection rate. Shen[16] dataset contains eight tracks
with sliding directions. In this paper, the feature is extracted as a
lateral sliding feature, and more attention is paid to the importance
of the feature in the X-axis direction. Human identification of slid-
ing tracks in other directions may result in discrepancies. Table 6
also shows that when only 20678 X-axis tracks are extracted, the
model has better human-machine identification.
Table 6: Algorithm evaluation on Shen[16] dataset
Classification Model
GBDT
SVM
Random Forest
XGBoost
FRR
13.28
100
13.27
13.26
FRR(X-Axis forward only)
1.09
100
0.93
0.93
Figure 10: Feature importance score.
Table 3: Feature correlation experiments