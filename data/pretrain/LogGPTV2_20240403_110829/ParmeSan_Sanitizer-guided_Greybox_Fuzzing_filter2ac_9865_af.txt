BO
libxml2
2554
10
libxml2
ML
543
10
openssl-1.0.1f BO
openssl-1.0.1f ML
10
709
10
ML
proj4
80
10
BO
re2
BO
woff2
10
woff2
OOM 10
Geomean diff
25m
1s
43h
8m
41s
13m
50s
11m
17m
3m4s
37s
1m26s
3267 12m35s
8m
12s
83
49
No lazysan
1850
200
49320
8761
540
4123
123
2701
2554
543
709
80
37m
1s
46h
12m
1m10s
18m
31s
17m
15m
4m30s
40s
1m30s
3267 17m10s
13m
19s
+0% +25%
83
49
No pruning
2503
260
7
29036
2990
6001
304
5066
7580
700
719
83
47m
1s
7
14m
2m10s
20m
1m15s
20m
22m
5m
42s
1m40s
3920 20m13s
20m
20s
+19% +28%
91
50
No dyncfg
2520
200
7
51m
1s
7
10531 12m35s
1m40s
21m
55s
23m
25m
4m52s
42s
1m30s
3450 18m21s
13m
19s
+17% +34%
758
5833
285
5123
7966
610
713
80
83
49
Table 7: Impact of different components of ParmeSan on branch coverage and time-to-exposure of the bug.
Prog
Type
Run time
Compile time
Targets
boringssl UAF
c-ares
freetype2
pcre2 UAF
lcms
libarchive
libssh ML
DFA +dyncfg Target acquisition ParmeSan No c.b. pruning No pruning
253
2%
36
BO 5%
IO 5%
8538
21781
2%
BO 0%
785
1431
BO 1%
229
3%
5131
BO 1%
5131
2%
BO 1%
304
304
1%
41
3%
2129
BO 1%
re2
33
woff2
BO 1%
woff2 OOM 10%
22
108 (-3.5%) 716 (+539%)
2%
200%
51
170%
21
170%
730
190%
1856
140%
95
170%
273
180%
55
210%
670
210%
670
240%
43
240%
43
140%
18
160%
295
180%
24
180%
24
183% 112 (+0%)
51
21
950
2051
98
340
45
751
751
39
39
15
370
20
20
3%
5%
5%
2%
1%
1%
3%
1%
2%
1%
1%
3%
1%
2%
10%
2%
libxml2
libxml2 ML
openssl-1.0.1f
openssl-1.0.1f ML
proj4 ML
Geomean
Table 8: Run-time and compile-time overhead introduced by the individual ParmeSan components.
the behavior of ParmeSan becomes similar to baseline An-
gora, effectively emulating pure coverage-guided fuzzing.
By disabling the dyncfg component, we see an increase of
34% in TTE. Note that by disabling this component, we also
effectively disable the lazysan component, as it relies on the
control-ﬂow information available by the dyncfg component.
We further evaluate the added beneﬁt of the dyncfg compo-
nent in Section A.1.1.
A.1.1 Dynamic CFG
Since ParmeSan uses a dynamic CFG to get a better esti-
mate of the distance to the targets, we also want to show
that the more accurate CFG actually improves the fuzzing
process, rather than adding more overhead. The existing
benchmarks—mostly C libraries—rarely contain a lot of in-
direct calls. However, in many applications (e.g., servers),
indirect calls are common. We show the effect of dynamic
CFG construction on three different experiments.
We fuzz 4 applications where we artiﬁcally demote (a ran-
dom selection of) of the direct calls to indirect calls (with 2
dummy call targets added) and obtain the targets using the
ParmeSan pipeline (with ASan), 3 applications where we de-
mote direct calls and manually target the bug, and ﬁnally run
the whole ParmeSan pipeline (with ASan) opn 3 real-world
applications with a large number of indirect calls. The results
for these three experiments can be found in Table 9. Overall,
we see that the dynamic CFG component has a higher impact
if there are indirect calls on the path to the bug to be found
). We also kept track of how much
(e.g., in libjpeg-turbo
time is spent on the dynamic CFG component. Overall the
overhead is negligible in most cases, accounting for less than
3% of the total execution time (as shown in Table 8).
A.1.2 Comparison against SAVIOR
For the sake of completeness, we include Table 10, which
shows how ParmeSan compares against Angora and SAV-
USENIX Association
29th USENIX Security Symposium    2305
Prog
Calls demoted
Mean. TTE
p-val
Angora
SAVIOR
ParmeSan
base64
who
uniq
md5sum
libjpeg-turbo
libpng
libpng
freetype2
httpd
cxxﬁlt
boringssl
5
10
8
15
55s
no dyncfg dyncfg
54s
2m32s 2m21s
0.19
0.03
22s 0.005
8m34s 6m32s 0.007
48s
Manual targeting
30
20
20
5
43m
1m29s
10s
1s
Real-world programs
0
0
0
10s
1m45s
51m
11m 0.004
21s 0.006
10s
0.06
0.09
1s
1s 0.003
0.02
1m5s
37m 0.005
base64
md5sum
uniq
who
48
59
29
2295
ASan
48
60
29
2320
48
59
29
2357
lava_get()
48
60
29
2353
Table 10: Comparison of Angora, SAVIOR, and ParmeSan
on LAVA-M. Mean number of LAVA-M bugs found over 10
24-hour runs using 3 parallel instances. We include results
for ParmeSan for target acquisition using ASan, as well as
explicitly targetting lava_get()
(replicating the setup de-
scribed in [11]).
Table 9: Time-to-exposure of bugs in programs where a
number of direct calls have been “demoted”. Apache httpd ,
cxxfilt , and boringssl
have not been modiﬁed, as they
already contain indirect calls. Statistically signiﬁcant values
(p < 0:05) are highlighted.
Prog
base64
md5sum
uniq
who
Targets
(pre-prune)
1950
1639
1832
2120
Bugs Found
Targets
24h
(post-prune)
48
212
60
101
193
29
385 2136 1544 1957 2353
1m 1h
48
59
29
44
57
28
Bugs
48
31
29
Table 11: Analysis target pruning statistics and number of
bugs found within 1 minute and within 24 hours. Some of
the LAVA-M programs contain more bugs than speciﬁed in
the dataset.
IOR on the well-known (but what might be considered out-
dated) LAVA-M dataset. We include this table to be able to
show a head-to-head comparison against SAVIOR. We repli-
cate the setup used by SAVIOR in [11], where the targets
are acquired in a manual way (i.e., explicitly targeting the
inlined calls to lava_get() ), rather than using sanitizers for
target acquisition, and use 3 fuzzing instances in parallel.
Overall, the results for ParmeSan and SAVIOR are compa-
rable, with the exception of md5sum , where ParmeSan ﬁnds
one more hard-to-trigger bug (unlisted bug #499) and who ,
where SAVIOR is able to trigger two more bugs. We hypoth-
esize that ParmeSan is able to trigger the md5sum bug due to
its ability to execute more test cases per second, while SAV-
IOR is better at ﬁnding the remaining two bugs in who due
to its symbolic execution-based constraint solving strategy.
Moreover, with ParmeSan, we were able to reproduce the
very same results on LAVA when using a single fuzzing in-
stance (and CPU core), suggesting ParmeSan’s fuzzing-only
strategy can provide results comparable to SAVIOR’s con-
straint solving-assisted strategy but with less resources. We
also include the results for using ASan for targeting.
2306    29th USENIX Security Symposium
USENIX Association