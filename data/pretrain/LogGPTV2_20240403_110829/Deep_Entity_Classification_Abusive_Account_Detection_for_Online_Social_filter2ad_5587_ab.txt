lutions [32, 49, 53]. Our work creates a generalized machine-
learning framework (utilizing these features among many
others) based on graph, direct, and indirect neighbor features
(the “deep entity”) which scales to billions of social network
users.
Other work has focused exclusively on graph structure,
with the goal of identifying groups or connected components.
Stringhini et al. produced EVILCHORT, a system designed
to identify accounts with common networking resources (e.g.,
IP addresses) and ultimately generate groups of malicious ac-
tors [50]. Earlier, Zhao et al. created BotGraph, which creates
an activity graph from user actions and uses that graph to iden-
tify tightly connected components indicative of abuse [64]. In-
stead of focusing on the structure of the graph, Nilizadeh et al.
observed how spam moved through the graph to identify com-
mon propagation patterns [38]. Compared to these works, we
USENIX Association
30th USENIX Security Symposium    4099
focus on a generalized framework which leverages such fea-
tures, as well as a scalable machine learning approach which
is utilized continuously at Facebook.
An alternative approach uses “honeypot” accounts to ul-
timately yield features which could be used for detection.
Stringhini et al. used honeypot Twitter accounts to collect
direct account, behavior, and content signals which could be
used to identify spammers [49]. Similarly, Lee et al. also
used honeypot Twitter and myspace accounts to collect di-
rect account, content, and timing signals, also identifying
abuse [32]. The features from both these works were later
formalized and further analyzed (along with other features)
by Yang et al. [58].
3.2 Sybil Accounts
A Sybil attack refers to an attack where individual malicious
users join the OSN multiple times under multiple fake iden-
tities. Many algorithms and systems have been proposed to
defend against Sybil attacks.
Yu [61] conduct a comprehensive study comparing various
Sybil defenses on social networks as of 2011. A typical graph
theory-based Sybil defense systems is SybilGuard [63]. The
protocol is based on the social graph among user identities,
where an edge between two identities indicates a human-
established trust relationship. The key observation is that
malicious users can create many identities but few trust rela-
tionships. Thus there is a disproportionately small “cut” in the
graph between the sybil nodes and the honest nodes. However,
there are two downsides of SybilGuard: it can allow a large
number of sybil nodes to be accepted, and it assumes that so-
cial networks are fast mixing, which has not been conﬁrmed
in the real world. Yu et al. [62] propose a SybilLimit proto-
col that leverages the same insight as SybilGuard but offers
near-optimal guarantees. Yang et al. claim that sybils do not
form tight knit communities, as other work has explored [59];
instead, linkages are formed between sybils and normal users
“accidentally” and therefore tight linkage-based defenses in
isolation are problematic.
SybilInfer, proposed by Danezis and Mittal [9], is another
sybil detection system. It uses a probabilistic model of honest
social networks and a Bayesian inference engine that returns
potential regions of dishonest nodes. SybilRank [5] is a de-
tection framework that has been deployed in Tuenti’s opera-
tion center. It relies on social graph properties to rank users
according to their perceived likelihood of being fake, and
has been shown to be computationally efﬁcient and scalable.
Wang et al. [54] take a different appproach, instead focus-
ing on user actions as a stream and making the observation
that the stream of actions for some types of attacks will be
different than that of regular users.
While most Sybil defense algorithms and systems focus
on exploring connections inside the social graph, this ap-
proach may fail to detect some types of abuse such as com-
promised accounts since they are not distinguishable on the
social graph. DEC instead operates by combining information
from the social graph with direct user features to conduct
general abuse classiﬁcation, irrespective of Sybil properties.
3.3 User Footprint
A “user footprint” is a signal that can be used to identify the
behaviors of a same user across different OSNs. Malhotra
et al. [37] propose the use of publicly available information
to create a digital footprint of any user using social media
services. This footprint can be used to detect malicious be-
haviors across different OSN platforms. Xiangnan et al. [29]
study the problem of inferring anchor links across multiple
heterogeneous social networks to detect users with multiple
accounts. The key idea is that if a user is abusive on one
platform, they are likely to be abusive on other platforms.
However, the user footprint is not helpful when a user is only
dedicated to spreading abuse in a single platform, which is
the focus of DEC.
3.4 Machine Learning
In this section we describe the relevant machine learning
works that DEC draws inspiration from.
3.4.1 ML for Abuse Detection
Machine learning-based classiﬁcation is widely used in abuse
detection. Stein et al. [48] proposed one of the ﬁrst machine
learning frameworks for abuse detection, applied to Facebook
in 2011. The system extracts users’ behavioral features and
trains a machine learning model for classiﬁcation. A similar
spam detection system using content attributes and user be-
havior attributes has been deployed on Twitter as described
by Benevenuto et al. [3]. These efforts laid the groundwork
for our “behavioral” model described in Section 7.2.
Fire et al. [18] propose the use of topological anomalies
on the social graph to identify spammers and fake proﬁles.
Their approach uses only four features per user, all of which
are related to the degree of graph connection of the user and
their friends. The approach is proven to be useful in various
OSNs. For DEC we employed a similar approach for feature
extraction, however with a greatly expanded feature space.
In terms of classiﬁcation algorithms, Tan et al. [51] de-
signed an unsupervised spam detection scheme, called UNIK.
Instead of detecting spammers directly, UNIK works by delib-
erately removing non-spammers from the network, leveraging
both the social graph and the user-link graph. In the context
of supervised learning, Lin et al. [35] conducted experiments
on a Twitter dataset to compare the performance of a wide
range of mainstream machine learning algorithms, aiming to
identify the ones offering satisfactory detection performance
and stability based on a large amount of ground truth data.
3.4.2 Other Relevant ML Work
Recent advances in machine learning, especially in graph
learning, transfer learning, and online learning, can also be
applied to ML-based abusive account detection.
4100    30th USENIX Security Symposium
USENIX Association
Graph learning seeks to learn a node embedding or make
predictions using relations in the graph. Variants of the tech-
niques have been applied to modeling social networks [40],
object interactions [24], citation networks [27], and abstract
data structures in program veriﬁcation [34]. Perozzi et al. [40]
proposed an unsupervised graph learning technique to learn
node embeddings using random walks in the local graph. Re-
cent works on graph neural networks (GNNs) [27, 33, 55]
extend convolutional neural networks to perform node clas-
siﬁcations. However, none of the existing graph learning ap-
proaches has been shown to scale to billions of nodes as in a
typical OSN social graph. We are actively experimenting with
GNNs for DEC and have encountered numerous technical
challenges in getting the system to work on a graph as large
and diverse as that of an OSN. Our exploratory work does
suggest potential improvements in model performance, but at
a much higher computational cost for training.
Transfer learning uses existing pre-trained models or em-
beddings as a basis for training models for new tasks. The
technique is commonly used to improve the performance
of ML models (e.g., facial recognition or image segmenta-
tion [39, 60]), especially in cases where little labeled training
data is available. In DEC, we leverage transfer learning to
boost our model performance by training the ﬁrst-stage em-
bedding on a second set of labels.
Online learning, ﬁrst proposed by Saad et al. [43], is a
technique to tune existing ML classiﬁers in real time using
newly available training data. Classiﬁed samples are sent
for labeling, which updates the training set to better capture
potential adaptive behaviors; retraining then strengthens the
classiﬁer against such behaviors [2]. In theory DEC could be
adapted to incorporate online learning; however, our human
labels are expensive and take a long time to collect, so the
beneﬁt of online learning over our current approach of regular
ofﬂine retraining would be minimal.
Active learning [7, 46], similar to online learning, is a tech-
nique to retrain the model with new data. In active learning,
only the data points in which the model has low conﬁdence
are assigned to human labellers for review. This approach is
intended to achieve maximum model performance improve-
ment with limited labeling resource. In our work we select
accounts at random for expert labelling. While active learning
is a potential avenue for improvement, we have been unable
to test it because of labeling constraints: random-sample la-
beling is used not only for training DEC but also for other
applications across Facebook, so any active learning experi-
ments would require additional labellers.
4 DEC System Overview
DEC extracts features from active Facebook accounts, clas-
siﬁes them, and then takes actions on the classiﬁed abusive
accounts. In order to deploy such a system in a scalable way,
we need to address multiple challenges, including scalabil-
ity, latency, variety of abuse types, and false positives. DEC
Figure 1: DEC system overview. When an user action occurs
on Facebook, the online component will, concurrent with user
activity, classify and potentially begin remediation on the user
and/or action. Meanwhile, the extracted features from the
online component, together with the training data, are used
by the ofﬂine component to train new models.
uses multiple components in order to handle these challenges
separately.
Figure 1 shows the DEC architecture. At the highest level,
we break down DEC into online and ofﬂine components,
discussed subsequently.
4.1 Online Component
DEC is triggered by Facebook user actions. When an action
occurs, DEC may, based on heuristics (see Section 5.2), sched-
ule a task concurrent with the user activity to start extracting
the raw features for the target node and sampled neighboring
nodes. For an average account on Facebook, DEC needs to
extract hundreds of features for each of hundreds of neighbor-
ing nodes, resulting in tens of thousands of raw features to be
extracted. Such queries are computationally expensive, and
thus the whole process is done asynchronously ofﬂine without
inﬂuencing the user’s normal site activity. After feature ex-
traction, DEC aggregates the raw features to form numerical
sparse features (further discussed in Section 6). DEC then
generates the classiﬁcation result for the account based on
the aggregated features and the in-production model. If the
account is classiﬁed as abusive, DEC exercises enforcement
on the account.
4.2 Ofﬂine Component
The ofﬂine component of DEC includes model training, and
feedback handling.
To classify multiple types of abuse, DEC maintains mul-
tiple models, where each model handles a different type of
abuse. Each dedicated model is trained on the learned low-
dimensional embeddings from the raw features collected as
part of the concurrent feature extraction (online component).
DEC uses the MS-MTL training framework to simultaneously
USENIX Association
30th USENIX Security Symposium    4101
Offline ComponentOnline ComponentUser ActionOnline Social NetworkEnforcementRaw FeaturesFeature AggregrationClassifica-tionAbusive?ModelTrainingProactive Human LabelReactive Human LabelUser AppealsTraining Datatrain and maintain models for different abuse types (further
discussed in Section 6).
As part of our implementation within Facebook, DEC has
integrated both human labeling as well as user feedback into
the training and enforcement process. Facebook uses a dedi-
cated team of specialists who can label whether an account
is abusive. These specialists label accounts both proactively
(based on features) and reactively (based on user feedback).
For proactive labeling, human labellers check accounts sur-
faced by various detection signals, take samples, label them,
and then take actions accordingly. For the reactive labeling,
the process begins when a user appeals an enforcement ac-
tion (as surfaced through the Facebook product). A human
reviewer then investigates the account and either accepts the
appeal (false positive from DEC’s perspective) or rejects the
appeal (true positive). Both proactive and reactive human la-
bel results are fed into DEC model training as labeled data.
Ofﬂine model training uses the human labeled data combined
with the extracted features from the online component. Af-
ter repeated ofﬂine and online testing, updated models are
deployed into production. DEC is regularly retrained by Face-
book to leverage the most recent abuse patterns and signals.
To summarize, DEC:
1. Extracts “deep features” across all active accounts on Face-
book to allow classiﬁcation.
2. Uses classiﬁcation to predict the level of abusiveness for all
active accounts, keeping up-to-date classiﬁcation results
for all users actively engaging with the network.
3. Incorporates user and labeler feedback to iterate classiﬁer
models.
5 Methods: Deep Feature Extraction
Feature extraction is a core part of DEC. Compared to tra-
ditional abuse detection systems, DEC uses the process of
aggregate feature calculations which aims to extract deep
features of a “target” account.
5.1 Deep features
In the context of DEC, “deep” refers to the process of fan-
ning out in the social graph. This graph consists of not only
users but all entities that the platform supports, such as groups,
posts, and more. A direct feature is a feature that is a function
of a particular entity only, such as account age or group size.
A deep feature is a feature that is a function of the direct fea-
tures of entities linked to the entity in question. For example,
“average age of an account’s friends” is a deep feature for the
account. Deep features can be deﬁned recursively, as aggre-
gations of deep features on linked accounts; for example, a
deep feature on a photo could be “average number of groups
joined by friends of people tagged in the photo.”
Deep features are useful for classiﬁcation because they re-
veal the position of target node in social graph by looking
at neighboring nodes. For instance, in the detection of fake
accounts, a common pattern that can be revealed by deep fea-
Table 1: Types of entities with their example direct features
and example deep entities in DEC.
Entity Type
Direct Features
User
Group
Device
Photo
Status Update
Group Post
Share
IP Address
age, gender
member count, age
operating system
like count, hash value
like count, age
has a link?
number of times shared
country, reputation
Deep Entities
entities administered, posts
admins, group members
users sharing the device
users in the photo
groups it shared to
users commenting
original creator
registered accounts
tures is the batch creation of fake accounts. When classifying
fake accounts, deep features include the features from the
IP address that registers the account, as well as all the other
accounts created from the IP address. When classifying us-
ing the above features, the scripted activity of batch account
registration can be easily detected.
A key insight is that deep features not only give additional
information about an account, but also are difﬁcult for ad-
versaries to manipulate. Most direct features can easily be
changed by the person controlling the entity. For example,