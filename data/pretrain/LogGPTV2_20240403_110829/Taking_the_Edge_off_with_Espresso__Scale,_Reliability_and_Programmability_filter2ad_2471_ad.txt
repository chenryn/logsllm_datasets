(ii) we can move traffic by de-configuring one or more BGP peers,
or (iii) we can move all of a peer’s traffic by clicking a button on
the BGP speaker’s monitoring user interface. There are also more
fine-grained knobs to disable sending traffic to a particular peer-
ing port, or overriding part of GC programming. These BRBs are
extensively tested nightly to ensure that they would work when
needed. Using the nightly test result, we analyze the performance
of these BRB in §6.4.
4.5.2 Network Telemetry. Espresso provides streaming teleme-
try of data-plane changes and reaction time statistics to events such
as peering link failure or route withdrawal. For example, peering
link failures are immediately streamed from PFC to the BGP speak-
ers allowing the speaker to withdraw the associated routes quickly,
rather than waiting for the BGP session to timeout. LC also uses
this signal to update host programming.
Control plane telemetry in Espresso leverages various standard
monitoring practices in Google. Every binary in the control plane
exports information on a standard HTTP/RPC endpoint, which is
collected and aggregated using a system like Prometheus [3].
4.5.3 Dataplane Monitoring via Probing. We continually run
end-to-end probes to detect problems. These probes traverse the
same path as regular traffic but exercise specific functionality. For
example, to verify the proper installation of ACLs at hosts, we send
probe packets to the hosts that are encapsulated identically to Inter-
net traffic, allowing us to validate that the systems implementing
the ACL forward/drop as expected. We can send probe packets that
loop back through the PF and through various links to ensure they
reach their destination and are processed correctly.
5 FEATURE AND ROLLOUT VELOCITY
Espresso is designed for high feature velocity, with an explicit goal
to move away from occasional releases of software components
where many features are bundled together. Espresso software com-
ponents are loosely coupled to support independent, asynchronous
and accelerated releases. This imposes a requirement for full inter-
operability testing across versions before each release. We achieve
this by fully automating the integration testing, canarying and
rollout of software components in Espresso.
Before releasing a new version of the Espresso software, we
subject it to extensive unit tests, pairwise interoperability tests
and end-to-end full system-level tests. We run many of the tests
in a production-like QA environment with a full suite of failure,
performance, and regression tests that validates the system opera-
tion with both the current and new software versions of all control
440
Table 2: Release velocity of each components (in days) for the past
year. Velocity is how frequent a new binary is being rolled out; Qual-
ification is the amount of time it takes for a binary to be qualified
for rollout; And Rollout is the time it takes for a new binary to be
deployed in all sites.
Component Velocity Qualification Rollout
LC
BGP speaker
PFC
0.858 (± 0.850)
1.14 (± 1.82)
1.75 (± 1.64)
5.07 (± 3.08)
5.01 (± 2.88)
4.16 (± 2.68)
11.2
12.6
15.8
plane components. Once this test suite passes, the system begins an
automated incremental global rollout. We also leverage the testing
and development infrastructure that is used for supporting all of
Google codebase [24].
The above features allow Espresso software to be released on
a regular weekly or biweekly schedule as shown in Table 2. To
fix critical issues, a manual release, testing and rollout can be per-
formed in hours. Since releases are such a common task, we ca
quickly and incrementally add features to the system. It is also easy
to deprecate an unused feature, which enables us to maintain a
cleaner codebase. Rapid evolution of the Espresso codebase is one
of the leading contributors to the significantly higher reliability in
Espresso compared to our traditional deployments˜citeevolveordie.
Using three years of historical data, we have updated Espresso’s
entire control plane > 50× more frequently than with traditional
peering routers, which would have been impossible without our test
infrastructure and a fail-static design that allows upgrades without
traffic interruption.
As one example of the benefits, we developed and deployed
a new L2 private connectivity solution for our cloud customers
in the span of a few months. This included enabling a new VPN
overlay, developing appropriate APIs and integrating with the VPN
management system, end-to-end testing, and global roll-out of the
software. We did so without introducing any new hardware or
waiting for vendors to deliver new features. The same work on the
traditional routing platforms is still ongoing and has already taken
6× longer.
6 EVALUATION
6.1 Growth of Traffic over Time
Espresso is designed to carry all of Google’s traffic. We started with
lower priority traffic to gain experience with the system, but with
time it is now carrying higher priority traffic.
To date, Espresso is carrying more than 22% of Google’s out-
bound Internet traffic, with usage increasing exponentially. Fig-
ure 6a shows that an increasing fraction of total traffic is carried
on Espresso. E.g., in the last two months, traffic on Espresso grew
2.24× more than the total.
6.2 Application-aware Traffic Engineering
We discuss some benefits of a centralized application-aware TE. The
global nature of GC means that it can move traffic from one peering
point to another, even in a different metro, when we encounter
capacity limits. We classify this as overflow, where GC serves the
excess user demand from another location. Overflow allows Google
to serve, on average, 13% more user traffic during peaks than would
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
K.K. Yap, M. Motiwala, et al.
(a) Fraction of total traffic carried on Espresso over time.
Figure 7: CDF of overflow (over a day) managed by GC for top 100
client ISPs. X axis shows the fraction of total traffic to the ISP that
is served from a non best location.
(b) CDF of 95-percentile peering link utilization over a day.
Figure 6: Total traffic and peering link utilization on Espresso.
(c) Packet drops by link utilization
otherwise be possible. GC can find capacity for overflow either on
a different peering in the same edge metro or could also spill it to
a different point-of-presence (PoP), as it has global visibility. We
observe that over 70% of this overflow is sent out to a different
metro. Figure 7 shows the distribution of overflow by client ISPs,
as a fraction of what could be served without TE. The overflow
fraction is fairly small for most ISPs, however for few very capacity-
constrained ISPs we find the GC overflowing over 50% of the total
traffic to those ISPs from non best location.
As described in § 4.3.1, GC caps loss-sensitive traffic on peering
links to allow for errors in bandwidth estimation. However, GC
can safely push links close to 100% utilization by filling any re-
maining space with lower-QoS loss-tolerant traffic. For this traffic,
GC ignores estimated link capacity, instead dynamically discov-
ering the traffic level that produces a target low level of queue
drops. Figure 6b shows that over 40% of peering links have a 95th
percentile utilization exceeding 50%, with 17% of them exceeding
80% utilization. This is higher than industry norms for link utiliza-
tion [2, 12, 21]. GC sustains higher peak utilization for a substantial
number of peering links without affecting users, easing the problem
of sustaining exponential growth of the peering edge.
Figure 6c focuses on observed packet drops for the highly utilized
peering links. GC manages packet drops to < 2% for even peering
links with 100% utilization. The drops are in the lower-QoS traffic
that is more loss-tolerant, as GC will react aggressively to reduce
limit if higher-QoS drops are observed.
GC monitors client connection goodput to detect end-to-end
network congestion, including those beyond Google’s network. GC
can then move traffic to alternate paths with better goodput. This
congestion avoidance mechanism dramatically improved our video
streaming service’s user-experience metrics for several ISPs, which
has been shown to be critical to user engagement [10].
Table 3 shows examples of improvements to user experience ob-
served by our video player when we enabled congestion detection
and reaction in GC. Mean Time Between Rebuffers (MTBR) is an
important user metric taken from the client side of Google’s video
service. MTBR is heavily influenced by packet loss, typically ob-
served in some ISPs that experience congestion during peak times.
For the examples presented here, we did not observe any congestion
on the peering links between Google and the ISP. This demonstrates
the benefit of a centralized TE system like GC, which considers the
least congested end-to-end path for a client prefix through a global
comparison of all paths to the client across the peering edge.
6.3 Comparison of BGP speakers
Early in the Espresso project, we had to choose between an open-
source BGP stack, e.g., Quagga [1], Bird [14], or XORP [19], or
extend Raven, an internally-developed BGP speaker. We settled
441
 0.02 0.04 0.06 0.08 0.1 0.12 0.14 0.16 0.18 0.2 0.22 0.24Fraction of traﬃc on EspressoTime (over 450 days) 0 10 20 30 40 50 60 70 80 90 100 0 0.2 0.4 0.6 0.8 1Percentage of peering links95-percentile utilization over 24 hours0.0%0.5%1.0%1.5%2.0%2.5% 0.9 0.92 0.94 0.96 0.98 1Packet dropsLink utilization95-percentileAverage 0 10 20 30 40 50 60 70 80 90 100 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9CDFOverﬂow fraction (95-percentile)Taking the Edge off with Espresso
SIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
Table 3: Improvements in Goodput and Mean Time Between Re-
buffers (MTBR) observed for video traffic when we enabled GC re-
action to end-to-end congestion detection and reaction.
2.25 → 4.5 Mbps
2.75 → 4.9 Mbps
3.2 → 4.2 Mbps
ISP Change in MTBR Change in Goodput
A
B
C
10 → 20 min
4.6 → 12.5 min
14 → 19 min
on Quagga as the leading open source candidate and conducted
a detailed comparison to Raven. This choice is in-part driven by
the fair amount of effort we have spent in optimizing Quagga in
B4 [21].
One of the most important metrics was the BGP advertising and
withdrawal convergence time, for both IPv4 and IPv6 routes. Raven
consistently outperformed Quagga for IPv4, converging 3.5−5.0×
faster with 3 million routes (Figure 8a) and performing as well for
IPv6 (Figure 8b). Raven also consistently used less memory and
CPU than Quagga, e.g., for one million IPv4 routes, Raven used
less than half the memory of Quagga. We saw similar savings in
IPv6 routes. Raven also has lower latency because it does not write
routes into the kernel. Further, Quagga is single-threaded and does
not fully exploit the availability of multiple cores on machines.
We also compared Raven with the BGP stack on a widely used
commercial router. Based on IPv4 and IPv6 routes drawn from a
production router, we created route sets of different sizes. We com-
pared the advertisement and withdrawal performance of Raven
(Adj-RIB-in post-policy) and the commercial router over several
runs for these route sets. The average convergence latency (Fig-
ure 8c) showed that Raven significantly outperformed the router
in both dimensions. This performance was partly made possible
by the approximately 10x CPU cores and memory available on our
servers relative to commercial routers.
(a) IPv4 Convergence between Quagga and Raven BGP speaker.
(b) IPv6 Convergence between Quagga and Raven BGP speaker.
6.4 Big Red Button
In this section, we evaluate the responsiveness of two of our “big
red buttons” mechanisms: (i) we can disable a peering by clicking a
button on the BGP speaker’s user interface for a temporary change,
and (ii) permanently drain traffic by de-configuring one or more
BGP peers via an intent change and config push.
In case (i), we measure the time between clicking the button
on the BGP speaker’s user interface and the routes for that peer
being withdrawn by Raven. This takes an average of 4.12 s, ranging
for 1.60 to 20.6 s with std. dev. of 3.65 s. In case (ii), we measure
the time from checking in intent to the time routes are withdrawn.
This takes an average of 19.9 s, ranging for 15.1 to 108 s with std.
dev. of 8.63 s. This time includes checking in an intent change to a
global system, performing various validation checks, propagating
the drain intent to the particular device and finally withdrawing
the appropriate peering sessions.
6.5 Evaluating Host Packet Processing
Key to Espresso is host-based packet processing to offload Internet
scale routing to the end hosts. This section demonstrates that a
well-engineered software stack can be efficient in memory and CPU
overhead.
(c) Convergence between Commercial Router and Raven BGP speaker.
Figure 8: Comparison of convergence times of Raven BGP speaker
vs. other BGP implementations.
Figure 9a shows the CDF of the programming update rate from
LC to one host; the host received 11.3 updates per second on aver-
age (26.6 at 99th percentile). We also measure update processing
overhead. Update processing is serialized on a single CPU and takes
only 0.001% on average and 0.008% at 99th percentile of its cycles.
An on-host process translates the programming update into an
efficient longest prefix match (LPM) data structure for use in the
data path. We share this immutable LPM structure among all of
442
 0 2 4 6 8 10 12 14 16 0 5 10 15 20 25 30Convergence Time (seconds)Number of routes (in 100K)Quagga AdvertiseQuagga WithdrawRaven AdvertiseRaven Withdraw 0 20 40 60 80 100 120 0 50 100 150 200 250 300 350 400Convergence Time (seconds)Number of routes (in 100K)Quagga AdvertiseQuagga WithdrawRaven AdvertiseRaven Withdraw 0 5 10 15 20 25 30 35 40 45 50 0 1 2 3 4 5 6Average Convergence Time (seconds)Number of routes (in 100K)Router AdvertiseRouter WithdrawRaven AdvertiseRaven WithdrawSIGCOMM ’17, August 21–25, 2017, Los Angeles, CA, USA
K.K. Yap, M. Motiwala, et al.
1.9 million IPv4/IPv6 prefixes-service classes tuple. The LPMs and
control plane structures used 1.2 GB of RAM on average and 1.3
GB of RAM at 99th percentile. We attribute occasional spikes in
memory use to the background threads triggered by our profiling
infrastructure.
We also evaluate the CPU overhead of both LPM lookups and
packet encapsulation on a production machine at peak load. At peak,
the machine transmits 37 Gbps and 3 million packets per second.
On average, the LPM lookups consume between 2.1% to 2.3% of
machine CPU, an acceptable overhead. For expediency, we used
simple binary trie LPM implementation, which can be improved
upon.
7 EXPERIENCE
Perhaps the largest meta-lesson we have learned in deploying vari-
ous incarnations of SDN is that it takes time to realize the benefits of
a new architecture and that the real challenges will only be learned
through production deployment. One of the main drivers for our
emphasis on feature velocity is to support our approach of going
to production as quickly as possible with a limited deployment and
then iterating on the implementation based on actual experience.
In this section, we outline some of our most interesting lessons.
(1) Bug in a shared reporting library. All Espresso control-
plane components use a shared client library to report their
configuration state to a reporting system for management
automation. We added this reporting system after the initial
Espresso design but neglected to add it to regression testing.
A latent bug triggered by a failure in the reporting system
caused all the control plane jobs to lock up due to process
thread exhaustion. In turn, the control-plane software com-
ponents across all Pilot peering locations failed. This caused
traffic to be automatically drained from the Espresso PF ports,
failing the traffic back to other peering locations. Espresso’s
interoperability with the traditional peering edge allowed
routing to work around the Espresso control plane failure