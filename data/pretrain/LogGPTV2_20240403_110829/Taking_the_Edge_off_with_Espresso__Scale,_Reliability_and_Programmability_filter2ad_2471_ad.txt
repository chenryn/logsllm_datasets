### 4.5.1 Traffic Management via BGP

To manage traffic, we can:

- De-configure one or more BGP peers.
- Move all of a peer’s traffic by clicking a button on the BGP speaker’s monitoring user interface.

Additionally, there are more granular controls to disable sending traffic to a specific peering port or override parts of the GC programming. These "Big Red Buttons" (BRBs) are extensively tested nightly to ensure their reliability when needed. The performance of these BRBs is analyzed in §6.4.

### 4.5.2 Network Telemetry

Espresso provides real-time telemetry for data-plane changes and reaction time statistics, such as peering link failures or route withdrawals. For example, peering link failures are immediately streamed from the PFC to the BGP speakers, allowing them to quickly withdraw associated routes rather than waiting for the BGP session to time out. The LC also uses this signal to update host programming.

Control plane telemetry in Espresso leverages standard monitoring practices at Google. Every control plane binary exports information through a standard HTTP/RPC endpoint, which is collected and aggregated using systems like Prometheus [3].

### 4.5.3 Dataplane Monitoring via Probing

We continuously run end-to-end probes to detect issues. These probes follow the same path as regular traffic but test specific functionalities. For instance, to verify the proper installation of ACLs at hosts, we send probe packets that are encapsulated identically to Internet traffic, ensuring that the systems implementing the ACLs forward or drop traffic as expected. We can also send probe packets that loop back through the PF and various links to ensure they reach their destination and are processed correctly.

### 5. Feature and Rollout Velocity

Espresso is designed for high feature velocity, with the goal of moving away from infrequent releases where many features are bundled together. The software components are loosely coupled to support independent, asynchronous, and accelerated releases. This requires full interoperability testing across versions before each release. We achieve this by fully automating integration testing, canarying, and rollout of software components in Espresso.

Before releasing a new version, we subject it to extensive unit tests, pairwise interoperability tests, and end-to-end full system-level tests. Many of these tests are run in a production-like QA environment, including failure, performance, and regression tests, to validate the system's operation with both current and new software versions of all control plane components. Once the test suite passes, an automated incremental global rollout begins.

We leverage the testing and development infrastructure used for supporting all of Google's codebase [24]. This allows Espresso software to be released on a regular weekly or biweekly schedule, as shown in Table 2. Critical issues can be fixed and rolled out manually within hours. The ability to quickly and incrementally add features and deprecate unused ones helps maintain a cleaner codebase. Rapid evolution of the Espresso codebase is a key contributor to its higher reliability compared to traditional deployments [citeevolveordie].

Using three years of historical data, we have updated Espresso’s entire control plane more than 50 times more frequently than with traditional peering routers. This would have been impossible without our robust test infrastructure and a fail-safe design that allows upgrades without traffic interruption.

For example, we developed and deployed a new L2 private connectivity solution for our cloud customers in just a few months. This included enabling a new VPN overlay, developing appropriate APIs, integrating with the VPN management system, end-to-end testing, and a global rollout of the software. This was achieved without introducing new hardware or waiting for vendors to deliver new features. The same work on traditional routing platforms has taken six times longer and is still ongoing.

### 6. Evaluation

#### 6.1 Growth of Traffic over Time

Espresso is designed to carry all of Google’s traffic. Initially, we started with lower-priority traffic to gain experience with the system, but over time, it now carries higher-priority traffic. Currently, Espresso handles more than 22% of Google’s outbound Internet traffic, with usage increasing exponentially. Figure 6a shows the increasing fraction of total traffic carried on Espresso. In the last two months, traffic on Espresso grew 2.24 times more than the total.

#### 6.2 Application-aware Traffic Engineering

A centralized application-aware TE system like GC can move traffic from one peering point to another, even in a different metro, when capacity limits are reached. This overflow allows Google to serve, on average, 13% more user traffic during peaks than would otherwise be possible. GC can find capacity for overflow either at a different peering in the same edge metro or spill it to a different PoP, as it has global visibility. Over 70% of this overflow is sent to a different metro. Figure 7 shows the distribution of overflow by client ISPs, as a fraction of what could be served without TE. For a few very capacity-constrained ISPs, GC overflows more than 50% of the total traffic from non-optimal locations.

GC caps loss-sensitive traffic on peering links to allow for errors in bandwidth estimation. However, it can safely push links close to 100% utilization by filling any remaining space with lower-QoS, loss-tolerant traffic. For this traffic, GC dynamically discovers the traffic level that produces a target low level of queue drops. Figure 6b shows that over 40% of peering links have a 95th percentile utilization exceeding 50%, with 17% exceeding 80% utilization. This is higher than industry norms for link utilization [2, 12, 21]. GC sustains higher peak utilization for a substantial number of peering links without affecting users, easing the problem of sustaining exponential growth of the peering edge.

Figure 6c focuses on observed packet drops for highly utilized peering links. GC manages packet drops to less than 2% even for peering links with 100% utilization. The drops are in the lower-QoS traffic that is more loss-tolerant. If higher-QoS drops are observed, GC reacts aggressively to reduce the limit.

GC monitors client connection goodput to detect end-to-end network congestion, including beyond Google’s network. It can then move traffic to alternate paths with better goodput. This congestion avoidance mechanism dramatically improved the user experience metrics for our video streaming service for several ISPs, which is critical to user engagement [10].

Table 3 shows examples of improvements in goodput and Mean Time Between Rebuffers (MTBR) observed for video traffic when we enabled GC reaction to end-to-end congestion detection and reaction.

#### 6.3 Comparison of BGP Speakers

Early in the Espresso project, we had to choose between open-source BGP stacks like Quagga [1], Bird [14], or XORP [19], or extend Raven, an internally-developed BGP speaker. We chose Quagga as the leading open-source candidate and conducted a detailed comparison with Raven. This choice was partly driven by the significant effort we had already spent optimizing Quagga in B4 [21].

One of the most important metrics was the BGP advertising and withdrawal convergence time for both IPv4 and IPv6 routes. Raven consistently outperformed Quagga for IPv4, converging 3.5−5.0× faster with 3 million routes (Figure 8a) and performing as well for IPv6 (Figure 8b). Raven also consistently used less memory and CPU than Quagga. For one million IPv4 routes, Raven used less than half the memory of Quagga, with similar savings for IPv6 routes. Raven also has lower latency because it does not write routes into the kernel. Additionally, Quagga is single-threaded and does not fully exploit the availability of multiple cores on machines.

We also compared Raven with the BGP stack on a widely used commercial router. Based on IPv4 and IPv6 routes drawn from a production router, we created route sets of different sizes. The average convergence latency (Figure 8c) showed that Raven significantly outperformed the router in both dimensions. This performance was partly due to the approximately 10x CPU cores and memory available on our servers relative to commercial routers.

#### 6.4 Big Red Button

In this section, we evaluate the responsiveness of two of our “big red buttons” mechanisms:

1. **Temporary Disablement**: We can disable a peering by clicking a button on the BGP speaker’s user interface. The time between clicking the button and the routes being withdrawn by Raven takes an average of 4.12 seconds, ranging from 1.60 to 20.6 seconds with a standard deviation of 3.65 seconds.
2. **Permanent Drainage**: We can permanently drain traffic by de-configuring one or more BGP peers via an intent change and config push. The time from checking in the intent to the routes being withdrawn takes an average of 19.9 seconds, ranging from 15.1 to 108 seconds with a standard deviation of 8.63 seconds. This time includes checking in an intent change to a global system, performing validation checks, propagating the drain intent to the particular device, and finally withdrawing the appropriate peering sessions.

#### 6.5 Evaluating Host Packet Processing

Key to Espresso is host-based packet processing to offload Internet-scale routing to the end hosts. This section demonstrates that a well-engineered software stack can be efficient in memory and CPU overhead.

Figure 9a shows the CDF of the programming update rate from LC to one host; the host received 11.3 updates per second on average (26.6 at the 99th percentile). We also measure update processing overhead. Update processing is serialized on a single CPU and takes only 0.001% on average and 0.008% at the 99th percentile of its cycles.

An on-host process translates the programming update into an efficient longest prefix match (LPM) data structure for use in the data path. We share this immutable LPM structure among 1.9 million IPv4/IPv6 prefixes-service classes tuples. The LPMs and control plane structures used 1.2 GB of RAM on average and 1.3 GB of RAM at the 99th percentile. Occasional spikes in memory use are attributed to background threads triggered by our profiling infrastructure.

We also evaluate the CPU overhead of both LPM lookups and packet encapsulation on a production machine at peak load. At peak, the machine transmits 37 Gbps and 3 million packets per second. On average, the LPM lookups consume between 2.1% to 2.3% of the machine CPU, an acceptable overhead. For expediency, we used a simple binary trie LPM implementation, which can be improved upon.

### 7. Experience

Perhaps the largest meta-lesson we have learned in deploying various incarnations of SDN is that it takes time to realize the benefits of a new architecture, and the real challenges will only be learned through production deployment. One of the main drivers for our emphasis on feature velocity is to support our approach of going to production as quickly as possible with a limited deployment and then iterating based on actual experience.

In this section, we outline some of our most interesting lessons:

1. **Bug in a Shared Reporting Library**: All Espresso control-plane components use a shared client library to report their configuration state to a reporting system for management automation. We added this reporting system after the initial Espresso design but neglected to add it to regression testing. A latent bug triggered by a failure in the reporting system caused all the control plane jobs to lock up due to process thread exhaustion. This led to the failure of control-plane software components across all Pilot peering locations, causing traffic to be automatically drained from the Espresso PF ports and failing the traffic back to other peering locations. Espresso’s interoperability with the traditional peering edge allowed routing to work around the Espresso control plane failure.