4We were unable to apply MASSVET to all 10,495 apps from Google Play
due to limitations of the web service.
of these apps are classiﬁed as benign by VirusTotal. Since
several benign applications repackage existing apps by adding
ad libraries, MASSVET seems to mistakenly classify them
as malware. In summary, this experiment demonstrates that
ASTROID achieves both higher accuracy as well as a lower
false-positive rate compared to MASSVET.
Interpretability.
In addition to comparing favorably with
MASSVET and DREBIN in terms of accuracy and false pos-
itives, we believe that ASTROID produces better explanations
of malice compared to existing tools. In the Appendix, we
compare the semantic signatures produced by ASTROID with
the evidence of malice produced by DREBIN and MASSVET.
Since ASTROID can pinpoint the malicious components and
their suspicious metadata (e.g., sensitive information leaked by
the component), we believe that the signatures generated by
ASTROID are more interpretable (and therefore more helpful)
to security analysts.
D. Detection of Zero-day Malware
To evaluate whether ASTROID can effectively detect zero-
day malware, we conduct experiments on two different datasets
and evaluate ASTROID’s accuracy and false positive rate.
Malware from Symantec and McAfee. In our ﬁrst experiment,
we use 160 malware samples obtained from Symantec and
McAfee, two leading anti-virus companies. Even though these
applications are known to be malicious, none of them belong
to the malware families from the Android Malware Genome
Project dataset. Since ASTROID’s database only contains sig-
natures for the families shown in Figure 3, all of these 160
applications are zero-day malware with respect to ASTROID’s
signature database. Using ASTROID’s approximate signature
matching algorithm (with the cutoff of 0.5, as described in
Section VIII), ASTROID correctly identiﬁes 147 of these apps
as malware; hence, ASTROID’s accuracy for zero-day malware
detection on this dataset is 92%. In contrast, MASSVET’s
accuracy on this dataset is 81%. We were not able to compare
against DREBIN on this dataset because we do not have their
feature vectors available.
Apps from Google Play. For our second experiment, we ana-
lyze 10,495 Google Play apps using ASTROID’s approximate
signature matching algorithm. As before, ASTROID’s signature
database only contains the malware families from Figure 3.
Among these apps, ASTROID reports 395 of them (i.e., 3.8%)
as being malicious. Of these 395 apps, 8 exactly match one
of the signatures in ASTROID’s database; as discussed in
Section IX-A, these 8 apps are all malicious.
To investigate which of the remaining 387 apps are known
malware, we use VirusTotal to analyze each of these apps.
Among these 387 samples, 21 of them are reported as ma-
licious by the majority (i.e., more than half) of the anti-
virus tools, and 81 of them are reported as malicious by at
least one anti-virus tool. Of the remaining 306 apps reported
by ASTROID, we randomly selected 40 apps and manually
inspected them. Our manual
inspection shows that 22 of
these 40 apps are actually malicious since they contain highly
suspicious behaviors:
• 4 apps appear to be SMS Trojans because they automat-
ically block all incoming SMS events when they receive
y
c
a
r
u
c
c
A
100
90
80
70
60
50
40
30
Exact Approximate
Obfuscated
Non-obfuscated
Fig. 6: Evaluation of different matching techniques
aborts all incoming SMS events.
from unlocking it.
gering it) and saves the picture to an encrypted ﬁle.
an SMS at a speciﬁc time or the contents of the SMS
match a certain pattern.
• 1 app silently records audio in a background process and
• 1 app automatically locks the screen and prevents the user
• 11 apps dynamically install apk ﬁles from untrusted
sources and leak the user’s device id, IMSI number, and
other conﬁdential phone information to untrusted remote
servers.
• 1 app silently takes pictures (i.e., without the user trig-
• 2 apps contain Android.Cauly library, which has recently
been classiﬁed as PUA (“Potentially Unwanted App”) by
Symantec.
• 2 apps perform highly suspicious actions without the user
triggering them. For instance, they send email or SMS
messages to a ﬁxed address or phone number.
Hence, our manual inspection shows that ASTROID can
detect malicious apps that are not identiﬁed by existing anti-
malware tools. Furthermore, based on our manual inspection,
we estimate that of the 306 apps not identiﬁed as malicious
by existing tools, 55% are in fact malicious. By this estimate,
ASTROID’s false positive rate is approximately 1.3% for zero-
day malware detection (i.e., using approximate matching with
a cutoff of 0.5).
Finally, we remark that the partial match (i.e., the subgraph
INFERSIGNATURE(A,SF ) of the ICCG of A) computed by
our approximate matching algorithm was indispensable for
ﬁnding the malicious behaviors in the 40 randomly selected
apps. In every case, some part of malicious code was con-
tained in the partial match, allowing us to quickly identify
the malicious behavior. In particular, examining all 40 apps
took a single analyst only a few hours. As discussed earlier,
the interpretability of the inferred signatures is an important
feature of ASTROID.
E. Detection of Obfuscated Apps
To evaluate whether ASTROID is resilient to obfuscations,
we perform a combination of low-level (syntactic) and high-
level (semantic) obfuscations. First, we obfuscate existing
11
malware using the ProGuard tool [27] to rename method/class
names, encrypt strings, and modify the program’s control ﬂow.
Second, we also perform obfuscations at the ICCG level, such
as inserting dummy components and removing taint ﬂows.
Figure 6 shows the accuracy of signature matching using
exact vs. approximate matching for 1,025 malware samples
from the Android Malware Genome Project and their corre-
sponding obfuscated versions. When using the exact matching
algorithm of APPOSCOPY, ASTROID can detect 93.8% of
the non-obfuscated malware samples but only 61% of the
obfuscated malware samples. In contrast, when we use the
approximate matching algorithm (with the cutoff of 0.8, as
described in Section VIII), ASTROID is able to detect 94.3%
of malware samples for both obfuscated and non-obfuscated
versions. Hence, this experiment demonstrates that ASTROID’s
approximate signature matching algorithm signiﬁcantly in-
creases the resilience of ASTROID to behavioral obfuscations.
In contrast, other anti-virus tools (e.g., Symantec, McAfee,
Kaspersky etc.) have been shown to be much less resilient
to even a subset of the obfuscations that we employ in this
experiment [2].
False positives. Both exact and approximate matching report
zero false positives on malware samples. As before, with exact
matching, ASTROID reports that 8 apps are malicious from the
corpus of 10,495 Google Play apps, but all of these apps are
classiﬁed as malware by VirusTotal. When using approximate
matching, ASTROID reports a total of 13 apps as malicious, 9
of which are classiﬁed as malware by VirusTotal. Hence, the
false positive rate of ASTROID remains very low (<0.04%)
even with approximate matching.
X. LIMITATIONS
Like any other malware detection tool, ASTROID has a
number of limitations:
First, the quality of the signatures inferred by ASTROID
depends on the precision of the underlying static analysis used
to construct the ICCG of the samples. In particular, sources of
imprecision (or unsoundness) in the analysis can degrade the
quality of the signatures inferred by ASTROID. However, our
experiments indicate that ASTROID can synthesize high-quality
signatures despite possible imprecision in the static analysis.
Second, ASTROID’s signature matching algorithm (both the
exact and approximate variants) are also affected by the quality
of the underlying static analysis. Since signature matching
requires computing the ICCG of the application under analysis,
any source of unsoundness in the analysis may translate
into false negatives in the context of malware detection. For
example, if an app dynamically loads a malicious payload,
then ASTROID may fail to ﬂag it as malware. However, such
attempts to escape detection can be identiﬁed as suspicious,
thereby requiring further scrutiny. On the other hand, sources
of imprecision due to the underlying static analysis may also
translate into false positives. For instance, given a benign app,
if the analysis generates a lot of spurious taint ﬂows and inter-
component call edges, ASTROID may mistakenly mark it as
malware. However, our evaluation show that the underlying
static analysis achieves a high precision without sacriﬁcing
scalability.
Third, ASTROID requires an analyst to provide at least
two representative samples of a given malware family, so
there is still a minimal amount of human effort involved in
using ASTROID. However, we believe this effort is miniscule
compared to the laborious task of writing malware signatures
manually.
Finally, ASTROID’s accuracy in detecting malware is de-
pendent on the initial database of malware signatures. The
larger the database, the higher the accuracy in detecting ex-
isting and zero-day malware. However, our experiments show
that ASTROID can effectively detect new malware families
even though we added only thirteen malware signatures to its
database. Furthermore, we believe that it is easier to maintain
a database of malware signatures compared to the task of
maintaining a much larger database of individual malware
samples.
XI. RELATED WORK
Android malware detection and classiﬁcation have been
extensively studied in recent years. In this section we brieﬂy
discuss prior closely-related work.
Machine learning approaches. Most prior malware classiﬁca-
tion techniques are based on machine learning [3, 6, 11, 12, 4,
13, 14, 5]. The key idea underlying all ML-based approaches
is to extract a feature vector summarizing the application’s
behavior and then train a classiﬁer to predict whether an app
is malicious or benign.
Many ML-based approaches generate their feature vectors
from a graph representation of the program. For instance,
SMIT [11] and Gascon et al. [12] model programs using call-
graphs and use clustering algorithms (e.g., KNN) to group
similar applications.
Similar to SMIT, both MASSVET [15] and DROIDSIFT [4]
use a graph abstraction to represent Android apps. Speciﬁcally,
MASSVET [15] computes graph similarity between a given app
A and the database of existing apps to determine if A is the
repackaged version of an existing app. As we show in our
evaluation, ASTROID can achieve better precision and fewer
false positives compared to MASSVET. Similarly, DROID-
SIFT abstracts programs using an API dependency graph
and employs graph similarity metrics to cluster applications
into different malware families. Similar to ASTROID, DROID-
SIFT performs multi-label rather than binary classiﬁcation and
employs a semantics-based approach to resist obfuscation.
However, unlike DROIDSIFT, ASTROID can infer signatures
from very few samples and does not need a large training set.
We tried to compare ASTROID against DROIDSIFT but we
were not able to reproduce their results using DROIDSIFT’s
web service [28]. While we have contacted the authors, the
issues with DROIDSIFT’s web service have not been resolved
by the time of this submission.
Another related tool is HOLMES [6], which detects Win-
dows malware by constructing the program’s behavior graph.
Such behavior graphs are constructed dynamically by analyz-
ing data dependencies in program traces. Given a behavior
graph, HOLMES then uses discriminative subgraph mining
to extract features that can be used to distinguish malicious
applications from benign ones. DROIDMINER [14] and Bose
12
et.al [5] also use a variant of behavior graphs to abstract
malware as a set of malicious components and use machine
learning for classiﬁcation. In contrast to these these techniques
which require a large number of samples (e.g., HOLMES uses
492 samples in their evaluation), ASTROID requires as few
as two samples to automatically generate malware signatures.
Second, our constraint-based approach is guaranteed to gener-
ate the optimal signature (in terms of maximizing suspicious
behaviors). Finally, we believe that the semantic signatures
synthesized by ASTROID are easier for security analysts to
interpret.
Another state-of-the-art malware detector for Android is
DREBIN [3], which combines syntactic and semantic features
into a joint vector space. The syntactic features are obtained
from the application’s manifest ﬁle, while the semantic features
are obtained through static analysis. Some of the features used
by DREBIN are similar to ASTROID; for instance, DREBIN also
extracts data ﬂow information as well as suspicious API calls.
As we demonstrate experimentally, ASTROID can achieve the
same or better precision as DREBIN using much fewer samples.
Signature-based approaches. As mentioned earlier, signature-
based approaches look for explicit patterns to identify instances
of a malware family [8, 2, 9, 10, 29, 30, 31]. These signatures
can be either syntactic or semantic, or a combination of
both. Our approach extends the applicability of signature-
based detectors by automatically synthesizing signatures from
a handful of malware samples.
Among signature-based detectors, APPOSCOPY [2] uses
semantic signatures to identify Android malware. Speciﬁcally,
it performs a combination of static taint analysis and inter-
component control ﬂow analysis to determine whether a sig-
nature matches an Android application. As mentioned earlier,
ASTROID is integrated as a plug-in to APPOSCOPY and can
generate signatures in APPOSCOPY’s malware speciﬁcation
language.
KIRIN [8], which is another signature-based tool, leverages
Android permissions to detect malware. Speciﬁcally, KIRIN
uses permission patterns to perform binary classiﬁcation of
apps as benign vs. malicious. While FACT [29] obtains a
signature by computing the unweighted maximal common sub-
graph extracted from dynamic analysis, ASTROID computes a
weighted maximally suspicious common subgraph using static
analysis.
Zero-day malware detection. Zero-day malware detectors [32,
7, 15] can detect malicious applications that belong to new
malware families. For instance, RISKRANKER [32] ranks
Android applications as high-, medium-, or low-risk depending
on the presence of suspicious features, such as certain kinds of
function calls. As another example, DROIDRANGER [7] uncov-
ers zero-day malware by performing heuristic-based ﬁltering
to identify suspicious behaviors. Some ML-based approaches
(e.g., [3], [5]) can, in principle, also uncover zero-day malware,
even though their detection rate is much higher for instances
of known malware families. Even though the primary goal of
ASTROID is not zero-day malware detection, our experiments
in Section IX-D show that ASTROID can nonetheless be
successfully used to detect instances of unknown malware
families.
13
Information ﬂow analysis for Android. Several tools, in-
cluding ASTROID, use information ﬂow as a component of
malware signatures or feature vectors. While information ﬂow
does not directly predict malware, ASTROID can beneﬁt from
recent advances in information ﬂow analysis to improve the
quality of its signatures. Some examples of Android infor-
mation ﬂow analysis tools include FLOWDROID [33], AP-
PINTENT [34], APPAUDIT [35], TAINTDROID [36], DROID-
SCOPE [37], CHEX [38], EPICC [39], HI-CFG [40] and