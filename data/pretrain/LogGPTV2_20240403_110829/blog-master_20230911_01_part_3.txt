        'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone'  
    ],  
    args => '{  
			"max_length" : 200  
		}'::JSONB   
) AS answer;  
```  
结果  
```  
[  
    [{"generated_text": "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Three for the Dwarfs and the Elves, One for the Gnomes of the Mines, and Two for the Elves of Dross.\"\n\nHobbits: The Fellowship is the first book of J.R.R. Tolkien's story-cycle, and began with his second novel - The Two Towers - and ends in The Lord of the Rings.\n\n\nIt is a non-fiction novel, so there is no copyright claim on some parts of the story but the actual text of the book is copyrighted by author J.R.R. Tolkien.\n\n\nThe book has been classified into two types: fantasy novels and children's books\n\nHobbits: The Fellowship is the first book of J.R.R. Tolkien's story-cycle, and began with his second novel - The Two Towers - and ends in The Lord of the Rings.It"}]  
]  
```  
如果您希望模型生成多个输出，您可以通过num_return_sequences在参数中包含参数来指定所需输出序列的数量。  
```  
SELECT pgml.transform(  
    task => '{  
        "task" : "text-generation",  
        "model" : "gpt2-medium"  
    }'::JSONB,  
    inputs => ARRAY[  
        'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone'  
    ],  
    args => '{  
			"num_return_sequences" : 3  
		}'::JSONB   
) AS answer;  
```  
结果  
```  
[  
    [  
        {"generated_text": "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and Thirteen for the human-men in their hall of fire.\n\nAll of us, our families, and our people"},   
        {"generated_text": "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and the tenth for a King! As each of these has its own special story, so I have written them into the game."},   
        {"generated_text": "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone… What's left in the end is your heart's desire after all!\n\nHans: (Trying to be brave)"}  
    ]  
]  
```  
文本生成通常使用贪婪搜索算法，该算法选择概率最高的单词作为序列中的下一个单词。然而，可以使用一种称为波束搜索的替代方法，其目的是尽量减少忽略隐藏的高概率单词组合的可能性。集束搜索通过在每一步保留 num_beams 个最可能的假设并最终选择总体概率最高的假设来实现这一点。我们设置num_beams > 1，early_stopping=True当所有波束假设到达 EOS 代币时，生成就完成了。  
```  
SELECT pgml.transform(  
    task => '{  
        "task" : "text-generation",  
        "model" : "gpt2-medium"  
    }'::JSONB,  
    inputs => ARRAY[  
        'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone'  
    ],  
    args => '{  
			"num_beams" : 5,  
			"early_stopping" : true  
		}'::JSONB   
) AS answer;  
```  
结果  
```  
[[  
    {"generated_text": "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Nine for the Dwarves in their caverns of ice, Ten for the Elves in their caverns of fire, Eleven for the"}  
]]  
```  
采样方法涉及从可能的候选集中随机选择下一个单词或单词序列，并根据语言模型根据它们的概率进行加权。这可以产生更加多样化和创造性的文本，并避免重复的模式。在最基本的形式中，采样意味着随机选择下一个单词  
根据其条件概率分布： `$$ w_t \approx P(w_t|w_{1:t-1})$$`  
然而，采样方法的随机性也可能导致文本不连贯或不一致，具体取决于模型的质量和所选的采样参数（例如温度、top-k 或 top-p）。因此，选择合适的采样方法和参数对于在生成的文本中实现创造力和连贯性之间的平衡至关重要。  
您可以传入`do_sample = True`参数以使用采样方法。建议更改temperature或更改，top_p但不要同时更改。  
temperature 微调  
```  
SELECT pgml.transform(  
    task => '{  
        "task" : "text-generation",  
        "model" : "gpt2-medium"  
    }'::JSONB,  
    inputs => ARRAY[  
        'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone'  
    ],  
    args => '{  
			"do_sample" : true,  
			"temperature" : 0.9  
		}'::JSONB   
) AS answer;  
```  
结果  
```  
[[{"generated_text": "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, and Thirteen for the Giants and Men of S.A.\n\nThe First Seven-Year Time-Traveling Trilogy is"}]]  
```  
top - p 微调  
```  
SELECT pgml.transform(  
    task => '{  
        "task" : "text-generation",  
        "model" : "gpt2-medium"  
    }'::JSONB,  
    inputs => ARRAY[  
        'Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone'  
    ],  
    args => '{  
			"do_sample" : true,  
			"top_p" : 0.8  
		}'::JSONB   
) AS answer;  
```  
结果  
```  
[[{"generated_text": "Three Rings for the Elven-kings under the sky, Seven for the Dwarf-lords in their halls of stone, Four for the Elves of the forests and fields, and Three for the Dwarfs and their warriors.\" ―Lord Rohan [src"}]]  
```  
### 内容理解及对话: 文本到文本生成  
文本到文本生成方法（例如 T5）是神经网络架构，旨在执行各种自然语言处理任务，包括摘要、翻译和问答。T5 是一种基于 Transformer 的架构，使用去噪自动编码对大量文本数据进行预训练。这种预训练过程使模型能够学习一般语言模式和不同任务之间的关系，可以针对特定的下游任务进行微调。在微调过程中，T5 模型在特定于任务的数据集上进行训练，以了解如何执行特定任务。   
文本到文本  
翻译  
```  
SELECT pgml.transform(  
    task => '{  
        "task" : "text2text-generation"  
    }'::JSONB,  
    inputs => ARRAY[  
        'translate from English to French: I''m very happy'  
    ]  
) AS answer;  
```  
结果  
```  
[  
    {"generated_text": "Je suis très heureux"}  
]  
```  
与其他任务类似，我们可以指定文本到文本生成的模型。  
```  
SELECT pgml.transform(  
    task => '{  
        "task" : "text2text-generation",  
        "model" : "bigscience/T0"  
    }'::JSONB,  
    inputs => ARRAY[  
        'Is the word ''table'' used in the same meaning in the two previous sentences? Sentence A: you can leave the books on the table over there. Sentence B: the tables in this book are very hard to read.'  
    ]  
) AS answer;  
```  
### 填空题  
填充掩码是指隐藏或“掩盖”句子中的某些单词的任务，目标是预测哪些单词应该填充这些掩码位置。当我们想要获得有关用于训练模型的语言的统计见解时，此类模型非常有价值。 填充掩模  
```  
SELECT pgml.transform(  
    task => '{  
        "task" : "fill-mask"  
    }'::JSONB,  
    inputs => ARRAY[  
        'Paris is the  of France.'  
    ]  
) AS answer;  
```  
结果  
```  
[  
    {"score": 0.679, "token": 812,   "sequence": "Paris is the capital of France.",    "token_str": " capital"},   
    {"score": 0.051, "token": 32357, "sequence": "Paris is the birthplace of France.", "token_str": " birthplace"},   
    {"score": 0.038, "token": 1144,  "sequence": "Paris is the heart of France.",      "token_str": " heart"},   
    {"score": 0.024, "token": 29778, "sequence": "Paris is the envy of France.",       "token_str": " envy"},   
    {"score": 0.022, "token": 1867,  "sequence": "Paris is the Capital of France.",    "token_str": " Capital"}  
]  
```  
## postgresml内置向量插件的应用  
向量数据库是一种存储和管理向量的数据库，向量是多维空间中数据点的数学表示。向量可用于表示多种数据类型，包括图像、文本、音频和数字数据。它旨在使用最近邻搜索、聚类和索引等方法支持向量的高效搜索和检索。这些方法使应用程序能够找到与给定查询向量相似的向量，这对于图像搜索、推荐系统和自然语言处理等任务非常有用。  
PostgresML 通过从表中存储的文本生成嵌入来增强现有的 PostgreSQL 数据库，将其用作向量数据库。要生成嵌入，您可以使用该pgml.embed函数，该函数将转换器名称和文本值作为输入。此功能会自动下载并缓存变压器以供将来重用，从而节省时间和资源。  
使用向量数据库涉及三个关键步骤：创建嵌入、使用不同算法对嵌入进行索引以及使用查询的嵌入来查询索引。让我们更详细地分解每个步骤。  
第 1 步：使用 Transformer 创建嵌入  
要为数据创建嵌入，您首先需要选择一个可以从输入数据生成嵌入的转换器。一些流行的变压器选项包括 BERT、GPT-2 和 T5。选择转换器后，您可以使用它为数据生成嵌入。  
在下一节中，我们将演示如何使用 PostgresML 为情感分析中常用的推文数据集生成嵌入。为了生成嵌入，我们将使用该pgml.embed函数，该函数将为数据集中的每条推文生成嵌入。然后，这些嵌入将被插入到名为 tweet_embeddings 的表中。  
```  
SELECT pgml.load_dataset('tweet_eval', 'sentiment');  
SELECT *   
FROM pgml.tweet_eval  
LIMIT 10;  
CREATE TABLE tweet_embeddings AS  
SELECT text, pgml.embed('distilbert-base-uncased', text) AS embedding   
FROM pgml.tweet_eval;  
SELECT * from tweet_embeddings limit 2;  
```  
结果  
text	| embedding  
---|---  
`"QT @user In the original draft of the 7th book, Remus Lupin survived the Battle of Hogwarts. #HappyBirthdayRemusLupin"`	| `{-0.1567948312,-0.3149209619,0.2163394839,..}`  
`"Ben Smith / Smith (concussion) remains out of the lineup Thursday, Curtis #NHL #SJ"`	| `{-0.0701668188,-0.012231146,0.1304316372,.. }`  
第 2 步：使用不同算法对嵌入进行索引  
为数据创建嵌入后，您需要使用一种或多种索引算法对其进行索引。有多种不同类型的索引算法可用，包括 B 树、k 最近邻 (KNN) 和近似最近邻 (ANN)。您选择的特定类型的索引算法将取决于您的用例和性能要求。例如，B 树是范围查询的不错选择，而 KNN 和 ANN 算法对于相似性搜索更有效。  
在小型数据集（ query.embedding LIMIT 5;  
```  
结果  
text  
---  
`Happy Friday with Batman animated Series 90S forever!`  
`"Fri Oct 17, Sonic Highways is on HBO tonight, Also new episode of Girl Meets World on Disney"`  
`tfw the 2nd The Hunger Games movie is on Amazon Prime but not the 1st one I didn't watch`  
`5 RT's if you want the next episode of twilight princess tomorrow`  
`Jurassic Park is BACK! New Trailer for the 4th Movie, Jurassic World -`  
## 参考  
https://postgresml.org/  
https://github.com/postgresml/postgresml/tree/master  
https://github.com/pgvector/pgvector  
https://huggingface.co/models  
#### [期望 PostgreSQL|开源PolarDB 增加什么功能?](https://github.com/digoal/blog/issues/76 "269ac3d1c492e938c0191101c7238216")
#### [PolarDB 云原生分布式开源数据库](https://github.com/ApsaraDB "57258f76c37864c6e6d23383d05714ea")
#### [PolarDB 学习图谱: 训练营、培训认证、在线互动实验、解决方案、内核开发公开课、生态合作、写心得拿奖品](https://www.aliyun.com/database/openpolardb/activity "8642f60e04ed0c814bf9cb9677976bd4")
#### [PostgreSQL 解决方案集合](../201706/20170601_02.md "40cff096e9ed7122c512b35d8561d9c8")
#### [德哥 / digoal's github - 公益是一辈子的事.](https://github.com/digoal/blog/blob/master/README.md "22709685feb7cab07d30f30387f0a9ae")
![digoal's wechat](../pic/digoal_weixin.jpg "f7ad92eeba24523fd47a6e1a0e691b59")
#### [购买PolarDB云服务折扣活动进行中, 55元起](https://www.aliyun.com/activity/new/polardb-yunparter?userCode=bsb3t4al "e0495c413bedacabb75ff1e880be465a")
#### [About 德哥](https://github.com/digoal/blog/blob/master/me/readme.md "a37735981e7704886ffd590565582dd0")