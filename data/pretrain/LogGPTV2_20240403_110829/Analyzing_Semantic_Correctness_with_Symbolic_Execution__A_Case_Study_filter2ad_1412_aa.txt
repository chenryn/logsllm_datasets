title:Analyzing Semantic Correctness with Symbolic Execution: A Case Study
on PKCS#1 v1.5 Signature Verification
author:Sze Yiu Chau and
Moosa Yahyazadeh and
Omar Chowdhury and
Aniket Kate and
Ninghui Li
Analyzing Semantic Correctness with Symbolic
Execution: A Case Study on PKCS#1 v1.5 Signature
Veriﬁcation
Sze Yiu Chau∗, Moosa Yahyazadeh†, Omar Chowdhury†, Aniket Kate∗, Ninghui Li∗,
{schau,aniket,ninghui}@purdue.edu, Purdue University∗
{moosa-yahyazadeh,omar-chowdhury}@uiowa.edu, The University of Iowa†
Abstract— We discuss how symbolic execution can be used
to not only ﬁnd low-level errors but also analyze the semantic
correctness of protocol
implementations. To avoid manually
crafting test cases, we propose a strategy of meta-level search,
which leverages constraints stemmed from the input formats to
automatically generate concolic test cases. Additionally, to aid
root-cause analysis, we develop constraint provenance tracking
(CPT), a mechanism that associates atomic sub-formulas of path
constraints with their corresponding source level origins. We
demonstrate the power of symbolic analysis with a case study on
PKCS#1 v1.5 signature veriﬁcation. Leveraging meta-level search
and CPT, we analyzed 15 recent open-source implementations
using symbolic execution and found semantic ﬂaws in 6 of them.
Further analysis of these ﬂaws showed that 4 implementations
are susceptible to new variants of
the Bleichenbacher low-
exponent RSA signature forgery. One implementation suffers
from potential denial of service attacks with purposefully crafted
signatures. All our ﬁndings have been responsibly shared with
the affected vendors. Among the ﬂaws discovered, 6 new CVEs
have been assigned to the immediately exploitable ones.
I.
INTRODUCTION
Developing a deployable cryptographic protocol is by no
means an easy feat. The journey from theory to practice is
often long and arduous, and a small misstep can have the
security guarantees that are backed by years of thorough
analysis completely undone. Given well-deﬁned cryptographic
constructs originated from mathematical problems that are be-
lieved to be hard to solve, proving their functional correctness
with respect to the relevant assumptions and security models
is hardly the end of the journey. Because of the restrictive
assumptions used in designing cryptographic constructs, in
reality, additional glue protocols are often needed to generalize
such constructs into being able to handle inputs of diverse
length and formats. Sometimes glue protocols are also used to
wrap around cryptographic constructs for exploiting the duality
of certain security guarantees to achieve alternative properties.
After careful designs have been devised and standardized,
it is also necessary for implementations to faithfully adhere
to the speciﬁcation, in order to ensure the retention of the
original designed security and functionality goals in actual
deployments. Implementations that deviate from the standard
Network  and  Distributed  Systems  Security  (NDSS)  Symposium  2019 
24-27  February  2019,  San  Diego,  CA,  USA
ISBN  1-891562-55-X
https://dx.doi.org/10.14722/ndss.2019.23430
www.ndss-symposium.org
and do not achieve the prescribed level of robustness can lead
to a plethora of attacks [9], [20], [22], [27].
The PKCS#1 v1.5 signature scheme, surrounding the RSA
algorithm, is one such glue protocol that is widely deployed
in practice. Used in popular secure communication protocols
like SSL/TLS and SSH, it has also been adapted for other
scenarios like signing software. Prior work has demonstrated
that lenient implementations of PKCS#1 v1.5 signature veri-
ﬁcation can be exploited in speciﬁc settings (e.g., when small
public exponents are being used) to allow the forgery of
digital signatures without possession of the private exponent
nor factorizing the modulus [3], [5], [13], [20], [24], [25],
[27]. The identiﬁcation of such implementation ﬂaws, however,
has been mostly based on manual code inspection [20], [27].
The focus of this paper is thus to develop a systematic and
highly automated approach for analyzing semantic correctness
of implementations of protocols like PKCS#1 v1.5 signature
veriﬁcation, that is, whether the code adheres to and enforces
what the speciﬁcation prescribes.
Our approach. For identifying semantic weaknesses of proto-
col implementations, we propose to perform symbolic analysis
of the software [26]. Directly applying off-the-shelf symbolic
execution tools [14], [28] to test PKCS#1 v1.5 implementa-
tions, however, suffers from scalability challenges. This is due
to the fact that the inputs to such protocols are often structured
with variable length ﬁelds (e.g., padding), and can sometimes
contain sophisticated ASN.1 objects (e.g., metadata).
One might question the applicability of symbolic analysis
on implementations of a cryptographic protocol. The key
intuition that we leverage in our approach, is that while the
underlying mathematics of cryptographic constructs are typi-
cally non-linear in nature, which are often difﬁcult to analyze
with constraint solvers, the various variable-sized components
used in glue protocols like PKCS#1 v1.5 often exhibit linear
relations among themselves and with the input buffer (e.g.,
sum of component lengths should equal to a certain expected
value). Using linear constraints stemming from such relations,
we can guide symbolic execution into automatically generating
many meaningful concolic test cases, a technique we refer to
as meta-level search.
To further address scalability challenges faced by symbolic
execution, we draw insights from the so-called human-in-the-
loop idea [21]. With domain knowledge on the protocol design
and input formats, human expertise can partition the input
space in a coarse-grained fashion by grouping together parts
of the input buffer that should be analyzed simultaneously,
making them symbolic while leaving the rest concrete. A
good partition strategy should constrain and guide symbolic
execution to focus on subproblems that are much easier to efﬁ-
ciently and exhaustively search than the original unconstrained
input space, and hence achieve good coverage while avoiding
intractable path explosions due to loops and recursions.
To facilitate root-cause analysis of an identiﬁed devia-
tion, we design and develop a constraint provenance tracking
(CPT) mechanism that maps the different clauses of each path
constraint generated by symbolic execution to their source
level origin, which can be used to understand where certain
decisions were being made inside the source tree. Our carefully
designed CPT mechanism has been demonstrated to incur only
modest overhead while maintaining sufﬁcient information for
identifying the root-cause of deviations in the source code.
Case Study. The PKCS#1 v1.5 signature scheme is a good
candidate for demonstrating the effectiveness of our approach
in analyzing semantic correctness, as the protocol itself in-
volves diverse glue components. As we will explain later, to
our surprise, even after a decade since the discovery of the
original vulnerability [20], several implementations still fail to
faithfully and robustly implement the prescribed veriﬁcation
logic, resulting in new variants of the reported attack.
Findings. To show the efﬁcacy of our approach, we ﬁrst
use it to analyze 2 legacy implementations of PKCS#1 v1.5
signature veriﬁcation that are known to be vulnerable. Our
analysis identiﬁed not only the known exploitable ﬂaws,
but also revealed some additional weaknesses. We then an-
alyze 15 recent open-source implementations with our ap-
proach. Our analysis revealed that 6 of these implementa-
tions (i.e., strongSwan 5.6.3, Openswan 2.6.50, axTLS 2.1.3,
mbedTLS 2.4.2, MatrixSSL 3.9.1, and libtomcrypt 1.16) ex-
hibit various semantic correctness issues in their signature
veriﬁcation logic. Our analysis in an existing theoretical frame-
work shows that 4 of these weak implementations are in fact
susceptible to novel variants of Bleichenbacher’s low-exponent
RSA signature forgery attack [20], [27], due to some new
forms of weaknesses unreported before. Exploiting these newly
found weaknesses, forging a digital signature does not require
the adversary to carry out many brute-force trials as described
in previous work [27]. Contrary to common wisdom, in some
cases, choosing a larger security parameter (i.e., modulus)
actually makes various attacks easier to succeed, and there
are still key generation programs that mandate small public
exponents [7]. One particular denial of service attack against
axTLS 2.1.3 exploiting its signature veriﬁcation weakness can
be launched even if no Certiﬁcate Authorities use small public
exponents. Among the numerous weaknesses discovered, 6
new CVEs have been assigned to the exploitable ones.
Contributions. This paper makes the following contributions:
1) We propose and develop a principled and practical ap-
proach based on symbolic execution that enables the
identiﬁcation of exploitable ﬂaws in implementations of
PKCS#1 v1.5 signature veriﬁcation. Speciﬁcally, we dis-
cuss how to enhance symbolic execution with meta-level
search in Section II.
2) To aid root-cause analysis when analyzing semantic cor-
rectness with symbolic execution, we design and im-
plement a constraint provenance tracker; which is of
independent interest. We explain in Section III how this
can help identify root causes of observed implementation
deviations with only a modest overhead.
3) We demonstrate our approach with a case study on
implementations of PKCS#1 v1.5 signature veriﬁcation.
Our analysis of 2 known buggy (Section IV-D) and 15
recent implementations (Section V) of PKCS#1 v1.5 not
only led to the discovery of known vulnerabilities but
also various new forms of weaknesses. We also provide
theoretical analysis and proof-of-concept attacks based on
our new ﬁndings in Section VI.
II. SYMBOLIC EXECUTION WITH META-LEVEL SEARCH
While symbolic execution is a time-tested means for an-
alyzing programs, the practicality challenges that it faces are
also well understood. When dealing with complex structured
inputs, one strategy to workaround scalability issues is to draw
on domain knowledge to strategically mix concrete values with
symbolic variables in the (concolic) test input. When done
correctly, this should allow symbolic execution to reach beyond
the input parsing code (which makes frequent use of loops and
recursions) and explore the post-parsing decision making logic.
As explained in previous work [15], inputs like X.509
certiﬁcates that are DER-encoded ASN.1 objects, can be
viewed as a tree of {Tag, Length, Value} triplets, where the
length of Value bytes is explicitly given. Hence, if all the
Tag and Length are ﬁxed to concrete values, the positions of
where Value begins and ends in a test input buffer would also
be ﬁxed. Hence, one can generate a few concrete inputs, and
manually mark Value bytes of interests as symbolic to obtain
meaningful concolic test cases. In fact, just a handful of such
manually produced test cases managed to uncover a variety of
veriﬁcation problems [15].
However, cryptographic glue protocols like PKCS#1 v1.5
signatures sometimes involve not only an encoded ASN.1
object, but also input components used for padding purposes,
where the length is often implicitly given by an explicit
termination indicator. In PKCS#1 v1.5, since padding comes
before its ASN.1 structure,
the extra room gained due to
(incorrectly) short padding can be hidden in any later parts
of the input buffer, including many leaf nodes of the encoded
ASN.1 object. This means there could be many combinations
of lengths of components that constitute the input buffer,
all meaningful for testing. Consequently, the concretization
strategy used in previous work [15] in this case requires a huge
amount of manual effort to enumerate and prepare concolic
inputs, and would easily miss out on meaningful corner cases.
To achieve a high degree of automation while preserving
a good test coverage, we propose to use symbolic variables
not only as test inputs, but also to capture some high-level
abstractions of how different portions of the test inputs could
be mutated, and let
the SMT solver decide whether such
mutations are possible during symbolic execution. The key
insight
the lengths of input components used by
protocols like PKCS#1 v1.5 exhibit linear relations with each
other. For example, the size of padding and all the other
components together should be exactly the size of the modulus,
and in benign cases, the length of a parent node in an encoded
is that,
2
is given by the sum of the size of all
ASN.1 object
its
child nodes. By programatically describing such constraints,
symbolic execution can automatically explore combinations of
possible component lengths, and generate concolic test cases
on the ﬂy by mutating and packing components according to
satisﬁable constraints.
Given that the input formats of many other protocols also
exhibit similar patterns, the meta-level search technique should
be applicable to them as well. We will explain how to ﬁt this
technique speciﬁcally for PKCS#1 v1.5 signatures and discuss
other engineering details in Section IV.
III. CONSTRAINT PROVENANCE TRACKING FOR EASIER
ROOT CAUSE ANALYSIS
In this section, we present the design, implementation,
and empirical evaluation of the constraint provenance tracking
(CPT) mechanism. CPT aids one to identify the underlying
root-cause of an implementation deviation, identiﬁed through
the analysis of the relevant path constraints generated by
symbolic execution. CPT is of independent interest in the
context of semantic correctness checking, as it can be used
for many other protocols beyond PKCS#1 v1.5.
A. Motivation
While the logical formulas extracted by symbolic execution
capture the implemented decision-making logic of the test tar-
get with respect to its inputs, which enable analysis of semantic
correctness and provide a common-ground for differential
testing as demonstrated by previous work [15], we argue that
after discrepancies have been identiﬁed, a root-cause analysis
from formula level back to code level is non-trivial to perform,
as multiple different code locations of an implementation could
have contributed to the various constraints being imposed on a
speciﬁc symbolic variable. This is further exacerbated by the
fact that, modern symbolic engines, like KLEE for example,
would actively simplify and rewrite path constraints in order
to reduce the time spent on constraint solving [14].
Take the following code snippet as a running example.
Assuming that each char is 1-byte long and A is a symbolic
variable, a symbolic execution engine like KLEE would dis-
cover 3 possible execution paths, with the return value being
0, 1, and 2, respectively.
1
2
3
4
5
6
7
8
Example 1: A code snippet with 3
execution paths. The path constraint
shown above corresponds to the path
that gives a return value of 2.
char b = 10, c = 11;
if (!memcmp(&A, &c, 1))
if (memcmp(&A, &b, 1))
(Eq 10 (Read w8 0 A))
int foo( char A ){
return 0;
return 1;
return 2;
}
Although the path that returns 2 falsiﬁes the two branching
conditions due to the if statements (i.e., A=11 and A(cid:54)=10), in
the end, the simpliﬁed constraint only contains the falsiﬁcation
of the second branching condition (i.e., A (cid:54)= 10), as shown
in the path constraint. This is because the falsiﬁcation of the
second if condition imposes a more speciﬁc constraint on
the symbolic variable than the ﬁrst one, and a simpliﬁcation
of the path constraints would discard the inexact clauses in
favor of keeping only the more speciﬁc and restrictive ones
(i.e., A (cid:54)= 11 ∧ A = 10 ↔ A = 10).
3
As illustrated by the example above, although the extracted
path constraints faithfully capture the implemented logic, using
them to trace where decisions were made inside the code is
not necessarily straightforward even on a toy example.
In order to make root-cause analysis easier when it comes
to ﬁnding bugs with symbolic execution, on top of merely
harvesting the ﬁnal optimized path constraints like previous
work did [15], we propose a new feature to be added to
the execution engine, dubbed Constraint Provenance Tracking
(CPT). The main idea is that, during symbolic execution,
when a new clause is to be introduced,
the engine can
associate some source level origin (e.g., ﬁle name and line
number) with the newly added clause, and export them upon
completion of the execution. We envision that when it comes
to ﬁnding root-causes of implementation ﬂaws, this is better
than stepping through an execution using a common debugger
with a concrete input generated by the symbolic execution.
This is because path constraints offer an abstraction at the
level of symbolic variables, not program variables. While
one might have to mentally keep track of potentially many
different program variables and their algebraic relations when
stepping with a debugger (especially when entering some
generic functions, e.g., a parser), in symbolic execution those
are all resolved into constraints imposed on symbolic variables,
and CPT offers insights on where did such impositions happen.
B. Design of CPT
1) Performance Considerations: While clause origin can
be obtained directly from the debugging information produced
by compilers, the constraint optimization needs to be han-
dled delicately. On one hand, such optimizations signiﬁcantly
improve the runtime of symbolic execution [14], on the
other, they are often irreversible, hindering root-cause analysis.
Striving to balance both performance and usability, in our
implementation of CPT, we introduce a separate container for
path constraints and their source level origins. The intuition
behind introducing the separate container is to let the engine
continue performing optimization on the path constraints that
drive the symbolic execution, so that runtime performance
would not suffer signiﬁcantly, but then the unoptimized clauses