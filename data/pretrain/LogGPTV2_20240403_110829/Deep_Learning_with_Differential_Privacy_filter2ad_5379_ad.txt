tune: we tried 600, 2,000, and 4,000. With these settings,
the per-epoch training time increases from approximately 40
seconds to 180 seconds.
In Figure 6, we show the evolution of the accuracy and
the privacy cost, as a function of the number of epochs, for
a few diﬀerent parameter settings.
The various parameters inﬂuence the accuracy one gets, in
ways not too diﬀerent from that in the MNIST experiments.
A lot size of 600 leads to poor results on this dataset and
we need to increase it to 2,000 or more for results reported
in Figure 6.
Compared to the MNIST dataset, where the diﬀerence in
accuracy between a non-private baseline and a private model
is about 1.3%, the corresponding drop in accuracy in our
CIFAR-10 experiment is much larger (about 7%). We leave
closing this gap as an interesting test for future research in
diﬀerentially private machine learning.
6. RELATED WORK
The problem of privacy-preserving data mining, or ma-
chine learning, has been a focus of active work in several
research communities since the late 90s [6, 39]. The exist-
ing literature can be broadly classiﬁed along several axes:
the class of models, the learning algorithm, and the privacy
guarantees.
Privacy guarantees. Early works on privacy-preserving
learning were done in the framework of secure function eval-
uation (SFE) and secure multi-party computations (MPC),
where the input is split between two or more parties, and
the focus is on minimizing information leaked during the
joint computation of some agreed-to functionality. In con-
trast, we assume that data is held centrally, and we are
concerned with leakage from the functionality’s output (i.e.,
the model).
Another approach, k-anonymity and closely related no-
tions [54], seeks to oﬀer a degree of protection to underlying
data by generalizing and suppressing certain identifying at-
tributes. The approach has strong theoretical and empirical
limitations [5, 10] that make it all but inapplicable to de-
anonymization of high-dimensional, diverse input datasets.
Rather than pursue input sanitization, we keep the under-
lying raw records intact and perturb derived data instead.
The theory of diﬀerential privacy, which provides the an-
alytical framework for our work, has been applied to a large
collection of machine learning tasks that diﬀered from ours
either in the training mechanism or in the target model.
The moments accountant is closely related to the notion of
R´enyi diﬀerential privacy [44], which proposes (scaled) α(λ)
as a means of quantifying privacy guarantees. In a concur-
rent and independent work Bun and Steinke [11] introduce
a relaxation of diﬀerential privacy (generalizing the work
of Dwork and Rothblum [22]) deﬁned via a linear upper
bound on α(λ). Taken together, these works demonstrate
that the moments accountant is a useful technique for theo-
retical and empirical analyses of complex privacy-preserving
algorithms.
Learning algorithm. A common target for learning with
privacy is a class of convex optimization problems amenable
to a wide variety of techniques [20, 12, 36]. In concurrent
work, Wu et al. achieve 83% accuracy on MNIST via con-
vex empirical risk minimization [58]. Training multi-layer
neural networks is non-convex, and typically solved by an
application of SGD, whose theoretical guarantees are poorly
understood.
316For the CIFAR neural network we incorporate diﬀeren-
tially private training of the PCA projection matrix [25],
which is used to reduce dimensionality of inputs.
Model class. The ﬁrst end-to-end diﬀerentially private sys-
tem was evaluated on the Netﬂix Prize dataset [41], a version
of a collaborative ﬁltering problem. Although the problem
shared many similarities with ours—high-dimensional in-
puts, non-convex objective function—the approach taken by
McSherry and Mironov diﬀered signiﬁcantly. They identiﬁed
the core of the learning task, eﬀectively suﬃcient statistics,
that can be computed in a diﬀerentially private manner via
a Gaussian mechanism. In our approach no such suﬃcient
statistics exist.
In a recent work Shokri and Shmatikov [52] designed and
evaluated a system for distributed training of a deep neural
network. Participants, who hold their data closely, commu-
nicate sanitized updates to a central authority. The sani-
tization relies on an additive-noise mechanism, based on a
sensitivity estimate, which could be improved to a hard sen-
sitivity guarantee. They compute privacy loss per param-
eter (not for an entire model). By our preferred measure,
the total privacy loss per participant on the MNIST dataset
exceeds several thousand.
A diﬀerent, recent approach towards diﬀerentially private
deep learning is explored by Phan et al. [47]. This work
focuses on learning autoencoders. Privacy is based on per-
turbing the objective functions of these autoencoders.
7. CONCLUSIONS
We demonstrate the training of deep neural networks with
diﬀerential privacy, incurring a modest total privacy loss,
computed over entire models with many parameters.
In
our experiments for MNIST, we achieve 97% training accu-
racy and for CIFAR-10 we achieve 73% accuracy, both with
(8, 10−5)-diﬀerential privacy. Our algorithms are based on a
diﬀerentially private version of stochastic gradient descent;
they run on the TensorFlow software library for machine
learning. Since our approach applies directly to gradient
computations, it can be adapted to many other classical
and more recent ﬁrst-order optimization methods, such as
NAG [45], Momentum [50], AdaGrad [17], or SVRG [33].
A new tool, which may be of independent interest, is a
mechanism for tracking privacy loss, the moments accoun-
tant. It permits tight automated analysis of the privacy loss
of complex composite mechanisms that are currently beyond
the reach of advanced composition theorems.
A number of avenues for further work are attractive. In
particular, we would like to consider other classes of deep
networks. Our experience with MNIST and CIFAR-10 should
be helpful, but we see many opportunities for new research,
for example in applying our techniques to LSTMs used for
language modeling tasks. In addition, we would like to ob-
tain additional improvements in accuracy. Many training
datasets are much larger than those of MNIST and CIFAR-
10; accuracy should beneﬁt from their size.
8. ACKNOWLEDGMENTS
We are grateful to ´Ulfar Erlingsson and Dan Ramage for
many useful discussions, and to Mark Bun and Thomas
Steinke for sharing a draft of [11].
9. REFERENCES
[1] CIFAR-10 and CIFAR-100 datasets.
www.cs.toronto.edu/˜kriz/cifar.html.
[2] TensorFlow convolutional neural networks tutorial.
www.tensorﬂow.org/tutorials/deep cnn.
[3] TensorFlow: Large-scale machine learning on
heterogeneous systems, 2015. Software available from
tensorﬂow.org.
[4] M. Abadi, A. Chu, I. Goodfellow, H. B. McMahan,
I. Mironov, K. Talwar, and L. Zhang. Deep learning
with diﬀerential privacy. CoRR, abs/1607.00133, 2016.
[5] C. C. Aggarwal. On k-anonymity and the curse of
dimensionality. In VLDB, pages 901–909, 2005.
[6] R. Agrawal and R. Srikant. Privacy-preserving data
mining. In SIGMOD, pages 439–450. ACM, 2000.
[7] R. Bassily, K. Nissim, A. Smith, T. Steinke,
U. Stemmer, and J. Ullman. Algorithmic stability for
adaptive data analysis. In STOC, pages 1046–1059.
ACM, 2016.
[8] R. Bassily, A. D. Smith, and A. Thakurta. Private
empirical risk minimization: Eﬃcient algorithms and
tight error bounds. In FOCS, pages 464–473. IEEE,
2014.
[9] A. Beimel, H. Brenner, S. P. Kasiviswanathan, and
K. Nissim. Bounds on the sample complexity for
private learning and private data release. Machine
Learning, 94(3):401–437, 2014.
[10] J. Brickell and V. Shmatikov. The cost of privacy:
Destruction of data-mining utility in anonymized data
publishing. In KDD, pages 70–78. ACM, 2008.
[11] M. Bun and T. Steinke. Concentrated diﬀerential
privacy: Simpliﬁcations, extensions, and lower bounds.
CoRR, abs/1605.02065, 2016.
[12] K. Chaudhuri, C. Monteleoni, and A. D. Sarwate.
Diﬀerentially private empirical risk minimization.
J. Machine Learning Research, 12:1069–1109, 2011.
[13] R. Collobert, K. Kavukcuoglu, and C. Farabet.
Torch7: A Matlab-like environment for machine
learning. In BigLearn, NIPS Workshop, number
EPFL-CONF-192376, 2011.
[14] D. D. Cox and N. Pinto. Beyond simple features: A
large-scale feature search approach to unconstrained
face recognition. In FG 2011, pages 8–15. IEEE, 2011.
[15] D. Silver, A. Huang, C. J. Maddison et al. Mastering
the game of Go with deep neural networks and tree
search. Nature, 529(7587):484–489, 2016.
[16] A. Daniely, R. Frostig, and Y. Singer. Toward deeper
understanding of neural networks: The power of
initialization and a dual view on expressivity. CoRR,
abs/1602.05897, 2016.
[17] J. Duchi, E. Hazan, and Y. Singer. Adaptive
subgradient methods for online learning and stochastic
optimization. J. Machine Learning Research,
12:2121–2159, July 2011.
[18] C. Dwork. A ﬁrm foundation for private data analysis.
Commun. ACM, 54(1):86–95, Jan. 2011.
[19] C. Dwork, K. Kenthapadi, F. McSherry, I. Mironov,
and M. Naor. Our data, ourselves: Privacy via
distributed noise generation. In EUROCRYPT, pages
486–503. Springer, 2006.
317[20] C. Dwork and J. Lei. Diﬀerential privacy and robust
statistics. In STOC, pages 371–380. ACM, 2009.
[21] C. Dwork, F. McSherry, K. Nissim, and A. Smith.
Calibrating noise to sensitivity in private data
analysis. In TCC, pages 265–284. Springer, 2006.
[22] C. Dwork and A. Roth. The algorithmic foundations
of diﬀerential privacy. Foundations and Trends in
Theoretical Computer Science, 9(3–4):211–407, 2014.
[23] C. Dwork and G. N. Rothblum. Concentrated
diﬀerential privacy. CoRR, abs/1603.01887, 2016.
[24] C. Dwork, G. N. Rothblum, and S. Vadhan. Boosting
and diﬀerential privacy. In FOCS, pages 51–60. IEEE,
2010.
[25] C. Dwork, K. Talwar, A. Thakurta, and L. Zhang.
Analyze Gauss: Optimal bounds for
privacy-preserving principal component analysis. In
STOC, pages 11–20. ACM, 2014.
[40] C. J. Maddison, A. Huang, I. Sutskever, and D. Silver.
Move evaluation in Go using deep convolutional neural
networks. In ICLR, 2015.
[41] F. McSherry and I. Mironov. Diﬀerentially private
recommender systems: Building privacy into the
Netﬂix Prize contenders. In KDD, pages 627–636.
ACM, 2009.
[42] F. D. McSherry. Privacy integrated queries: An
extensible platform for privacy-preserving data
analysis. In SIGMOD, pages 19–30. ACM, 2009.
[43] T. Mikolov, K. Chen, G. Corrado, and J. Dean.
Eﬃcient estimation of word representations in vector
space. CoRR, abs/1301.3781, 2013.
[44] I. Mironov. R´enyi diﬀerential privacy. Private
communication, 2016.
[45] Y. Nesterov. Introductory Lectures on Convex
Optimization. A Basic Course. Springer, 2004.
[26] M. Fredrikson, S. Jha, and T. Ristenpart. Model
[46] J. Pennington, R. Socher, and C. D. Manning. GloVe:
inversion attacks that exploit conﬁdence information
and basic countermeasures. In CCS, pages 1322–1333.
ACM, 2015.
[27] I. Goodfellow. Eﬃcient per-example gradient
computations. CoRR, abs/1510.01799v2, 2015.
[28] B. Graham. Fractional max-pooling. CoRR,
abs/1412.6071, 2014.
[29] A. Gupta, K. Ligett, F. McSherry, A. Roth, and
K. Talwar. Diﬀerentially private combinatorial
optimization. In SODA, pages 1106–1125, 2010.
[30] K. He, X. Zhang, S. Ren, and J. Sun. Delving deep
into rectiﬁers: Surpassing human-level performance on
ImageNet classiﬁcation. In ICCV, pages 1026–1034.
IEEE, 2015.
[31] R. Ierusalimschy, L. H. de Figueiredo, and W. Filho.
Lua—an extensible extension language. Software:
Practice and Experience, 26(6):635–652, 1996.
[32] K. Jarrett, K. Kavukcuoglu, M. Ranzato, and
Y. LeCun. What is the best multi-stage architecture
for object recognition? In ICCV, pages 2146–2153.
IEEE, 2009.
[33] R. Johnson and T. Zhang. Accelerating stochastic
gradient descent using predictive variance reduction.
In NIPS, pages 315–323, 2013.
[34] P. Kairouz, S. Oh, and P. Viswanath. The composition
theorem for diﬀerential privacy. In ICML, pages
1376–1385. ACM, 2015.
[35] S. P. Kasiviswanathan, H. K. Lee, K. Nissim,
S. Raskhodnikova, and A. D. Smith. What can we
learn privately? SIAM J. Comput., 40(3):793–826,
2011.
[36] D. Kifer, A. D. Smith, and A. Thakurta. Private
convex optimization for empirical risk minimization
with applications to high-dimensional regression. In
COLT, pages 25.1–25.40, 2012.
[37] A. Krizhevsky, I. Sutskever, and G. E. Hinton.
ImageNet classiﬁcation with deep convolutional neural
networks. In NIPS, pages 1097–1105, 2012.
[38] Y. LeCun, L. Bottou, Y. Bengio, and P. Haﬀner.
Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11), 1998.
[39] Y. Lindell and B. Pinkas. Privacy preserving data
mining. In CRYPTO, pages 36–54. Springer, 2000.
Global vectors for word representation. In EMNLP,
pages 1532–1543, 2014.
[47] N. Phan, Y. Wang, X. Wu, and D. Dou. Diﬀerential
privacy preservation for deep auto-encoders: an
application of human behavior prediction. In AAAI,
pages 1309–1316, 2016.
[48] N. Pinto, Z. Stone, T. E. Zickler, and D. Cox. Scaling
up biologically-inspired computer vision: A case study
in unconstrained face recognition on Facebook. In
CVPR, pages 35–42. IEEE, 2011.
[49] R. M. Rogers, A. Roth, J. Ullman, and S. P. Vadhan.
Privacy odometers and ﬁlters: Pay-as-you-go
composition. CoRR, abs/1605.08294, 2016.
[50] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
Learning representations by back-propagating errors.
Nature, 323:533–536, Oct. 1986.
[51] A. Saxe, P. W. Koh, Z. Chen, M. Bhand, B. Suresh,
and A. Ng. On random weights and unsupervised
feature learning. In ICML, pages 1089–1096. ACM,
2011.
[52] R. Shokri and V. Shmatikov. Privacy-preserving deep
learning. In CCS, pages 1310–1321. ACM, 2015.
[53] S. Song, K. Chaudhuri, and A. Sarwate. Stochastic
gradient descent with diﬀerentially private updates. In
GlobalSIP Conference, 2013.
[54] L. Sweeney. k-anonymity: A model for protecting
privacy. International J. of Uncertainty, Fuzziness and
Knowledge-Based Systems, 10(05):557–570, 2002.
[55] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed,
D. Anguelov, D. Erhan, V. Vanhoucke, and
A. Rabinovich. Going deeper with convolutions. In
CVPR, pages 1–9. IEEE, 2015.
[56] S. Tu, R. Roelofs, S. Venkataraman, and B. Recht.
Large scale kernel learning using block coordinate
descent. CoRR, abs/1602.05310, 2016.
[57] O. Vinyals, L. Kaiser, T. Koo, S. Petrov, I. Sutskever,
and G. E. Hinton. Grammar as a foreign language. In
NIPS, pages 2773–2781, 2015.
[58] X. Wu, A. Kumar, K. Chaudhuri, S. Jha, and J. F.
Naughton. Diﬀerentially private stochastic gradient
descent for in-RDBMS analytics. CoRR,
abs/1606.04722, 2016.
318