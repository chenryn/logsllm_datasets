5 downloaders per day).
VI. DETECTION PERFORMANCE
While the previous section provides empirical insights into
silent distribution campaigns, we now evaluate the effective-
ness of Beewolf as a detection system. We aim to detect
suspicious activity, such as malware and PUP dissemination
campaigns. This information can be used in several ways.
The downloaders and domains caught in locksteps can help
10
WajamMultiply ROIOutBrowse LTDOutBrowseMindad media Ltd.International News Network LimitedSomoto Ltd.Somoto IsraelConduit Ltd.Amonetize ltd.Shetef Solutions & Consulting (1998) Ltd.Creative Island MediaSendoriAedge Performance BCNSoftonic InternationalInstallXCIDAraonmediaPacifics Co.raonmediaOutBrowse LTDSomoto Ltd.SendoriInstallXAmonetize ltd.WajamAmonetize ltd.Somoto Ltd.Somoto IsraelOutBrowse LTDConduit Ltd.WajamVitblan telecom slMultiply ROIbetwikxBandisoftAB Team d.o.o.WhiteSmoke IncGOLDBAR VENTURES LTDAedge Performance BCNCreative Island MediaOutBrowseIronInstallMindad media Ltd.InstallXSendori# of Locksteps (%)MDLs (dlr:dom)MDLs (dom:dlr)Histogram bucket size = 5020406080Approximate level in FP tree of MDLs5101520253035Fig. 7: MDL properties: (a) Distribution of the number of nodes in lockstep, (b) Distribution of the number of domains per day
(typedlr:dom), (c) Distribution of the number of downloaders per day (typedom:dlr).
Fig. 8: Detection lead time for MD/PDs.
prioritize further analysis, e.g. to attribute the campaigns to
publishers as we demonstrate in Section V. It could be com-
bined with other techniques (e.g. DNS reputation systems [2],
[6]) to detect a speciﬁc form of abuse (e.g. botnet activity). An
enterprise may also block all downloads initiated remotely by
unknown organizations; in this case, a few trusted publishers
could be added to our initial whitelist.
We use the locksteps labeled in Section V to validate our
system: an MDL or PDL detection represents a true positive,
while a BDL detection is a false positive. For the true positives,
we compute the detection lead time, compared with the anti-
virus products invoked by VirusTotal (for downloaders) and
with three malware blacklists (for domains). We also analyze
the causes of false positive detections. As we lack ground truth
about malware distribution campaigns, we cannot estimate the
false negative rate.
Experimental settings. We evaluate Beewolf in ofﬂine mode,
and we build on our empirical insights to select the appropriate
conﬁguration parameters. We set ∆t = δt = 3 days, to capture
locksteps with a high domain churn.
A. Malware and PUP detection
Detection performance. Table III lists the numbers of lock-
steps from each category. Overall, the benign locksteps (BDLs)
represent 4.82% and 2.48% of typedlr:dom and typedom:dlr
locksteps, respectively. We observe the highest fraction of
BDLs among the mixed locksteps of typedlr:dom, perhaps
because malware and PUP creators utilize dedicated malicious
infrastructures as well as generic downloaders, which may
also distribute benign software. In contrast, PPI rep-pubs do
not generate any BDL of typedom:dlr and only 4 BDLs of
typedlr:dom. Overall, the suspicious locksteps (MDL or PDL)
account for 92.85% and 97.24% of all locksteps of typedlr:dom
and typedom:dlr, respectively.
Detection lead time. As Beewolf is content-agnostic (i.e. it
does not analyze the downloader binaries or the Web content
served by the URLs contacted), we evaluate how early we can
detect suspicious downloaders or domains that are previously
unknown. We consider the downloaders submitted to VirusTo-
tal in 2013 that have at least one detection record. We compare
the time when Beewolf is able to detect these downloaders
to the time of their ﬁrst submission to VirusTotal. Because
a downloader detected by Beewolf is active in the wild,
and because VirusTotal invokes up to 54 AV products with
updated virus deﬁnitions, we consider that a detection lead
time illustrates the opportunity to identify previously unknown
droppers. As explained in Section V, a lockstep emerges at the
time when the second star is formed; we estimate the detection
time of a downloader as the earliest detection timestamp
among the locksteps that contain it. Figure 8 illustrates this
comparison. The negative range represents a detection lead
time, and the positive range corresponds to detection lag. We
observe 1182 downloaders detected early and 213 downloaders
detected late. The median detection lead time is 165 days.
Among the late detections, 69 of the downloaders are detected
 80%) coverage. Further,
the number of unique rep-pubs per community is considerably
large (10). This suggests that most of the communities are
mixed up with locksteps coming from different publishers.
This makes it difﬁcult to logically assign each community to
a particular group. Community detection algorithms do not
account for the timing of downloads, which makes it hard to
pinpoint coordinated behavior between nodes.
Prior Lockstep Detection Algorithm. We compare the lock-
steps detected by our algorithm to locksteps detected by
the serial
implementation of the CopyCatch [4] algorithm
over one month (January 2013) of data. We reimplement
CopyCatch, as the code is not available. There are qualitative
differences between our algorithm and CopyCatch. Firstly, our
algorithm is unsupervised. In contrast, CopyCatch requires
seed domains corresponding to malicious domains and also
times for all the domains at which some suspicious activity
has occurred. Secondly, given a batch of data, we detect all
the locksteps within that batch; CopyCatch can detect one
single lockstep, which depends to the seed. Thirdly, CopyCatch
solves an optimization problem to detect
locksteps, which
makes it highly sensitive to the choice of seed domains and
the times provided. Furthermore, this serial implementation of
CopyCatch is not scalable for large lockstep sizes; we consider
only small locksteps for comparison.
To make a fair comparison, we generate 470 locksteps
using our algorithm over the one month data. Of these only
139 locksteps have a size less than 10 which we consider
for comparison. For each lockstep our algorithm detected,
we provide CopyCatch the domains as seed nodes and the
timestamp at which each domain was active in the lockstep
as the seed times. Our algorithm generates 470 locksteps in
7.56 s, taking an average of 0.016 seconds per lockstep. In
contrast, CopyCatch takes 600.9 s to generate 139 locksteps—
an average of 4.32 s per lockstep detection. These results
suggest that Beewolf shows promise for processing streaming
data.
C. Robustness to evasion attempts
An adversary could pursue three strategies for evading
Beewolf; we start by explaining these attacks in the context
of typedlr:dom lockstep detection. First, the adversary could
frequently update or repack the downloaders it controls, so
that no downloader is active in different time windows. This
attack would prevent lockstep detection, and many malware
families already employ aggressive repacking rates to evade
detection [8]. However, this strategy might impose a trade-
off for organizations that conduct silent delivery campaigns,
as they try to render their downloaders inconspicuous, e.g.
by utilizing code signing and by avoiding behaviors that are
not commonly seen in benign downloaders such as software
updaters [26]. The frequent updates and the lower prevalence
of individual hashes that would result from higher repacking
rates would make these downloaders look suspicious to an
AV product. Instead of increasing the number of downloaders,
in the second strategy the adversary could utilize a large
number of domains, e.g. from DynDNS or a similar provider,
so that each downloader accesses a single domain within a
time window. This would be expensive for the adversary, as
generating and registering new DNS domains is more costly
than repacking downloaders and payloads. For example, to
protect 500 droppers from lockstep detection, an adversary
would need 5,000 DynDNS zones each month (Beewolf
considers second-level domains rather than FQDNs), at a
current cost of $4,000/month.23 Additionally, this approach
would make the domains more likely to be detected by DNS
reputation systems, which use domain popularity as feature [6].
In practice, Beewolf detects MDLs that churn through more
than 7 domains per day, as discussed in Section V-C. The
adversary could reduce the cost by instructing each downloader
to randomly select a domain, from a pool of available domains,
and to contact only that domain for ∆t; then, the downloader
would select another domain, and the reuse rate of domains
in the pool would increase. To detect this, we could increase
∆t, to cover the point when the downloader switches domains,
and this would in turn force the adversary to further increase
the time interval when each downloader accesses a single
domain. Ultimately, the adversary cannot increase this time
interval indeﬁnitely, as domains that serve malware eventually
22http://igraph.org/
23http://dyn.com/managed-dns/
12
get blacklisted. Additionally, we observe that the ﬁrst two
attack strategies involve increasing the downloader churn and
reducing the domain churn, for evading the detection of
typedlr:dom locksteps; to evade the detection of typedom:dlr,
these actions should be reversed. This suggests that it is difﬁ-
cult for an adversary to avoid both types of lockstep detection
simultaneously. Finally, in the third strategy, the attacker could
exploit the ﬁltering step in our lockstep detection algorithm,
for example by ensuring that MDLs appear deeper in our FP
tree. In this case, Beewolf is still able to capture a subset of
these locksteps at lower FP tree levels.
VII. STREAMING PERFORMANCE
Experimental settings. We evaluate Beewolf in streaming
mode by feeding the download data in batches. In the lockstep
detection phase, we ﬁlter out the FP tree level over 7, based on
the observation that MDLs reside close to the root of the FP
tree. And, we measure the latency of lockstep detection. Each
batch corresponds to a time window of ∆t = 3 days. As we
employ one year of data, we have 121 data points excluding
the ﬁrst batch in our experiment. For all 121 data points, we
measure the elapsed time for each of the four phases in our
data analysis (illustrated in Figure 3). We run our experiments
on Amazon’s Elastic Compute Cloud (Amazon EC2).24 We
use one M4.4xlarge instance, which has a 16-core 2.4 GHz
Intel Xeon E5-2676 v3 (Haswell) with 64 GB of memory. For
this evaluation, we focus on typedlr:dom graphs.
Streaming performance. Figure 9(a) illustrates the growth
of the data structures that Beewolf maintains. The plots has
a logarithmic Y-axis, to compare both the number of new
stars per batch and the cumulative number of nodes in the
galaxy and the FP tree. On average, a batch contains 225,939
download events. Both the number of nodes in the galaxy
graph and the FP tree grow linearly. At the end, the graph
has 123,335 nodes and 637,814 edges. As the data grows, the
cost for detecting lockstep also grows incrementally.
Figure 9(b) suggests that Beewolf’s runtime is dominated
by the lockstep detection phase, which accounts for 97.2% of
the total runtime on average. The total runtime shows three
growth patterns: a steep increase for the ﬁrst 20 batches, a
slower increase for most of the period, and another steep
increase starting around batch 94–96. Each of these growth
patterns is linear and follows a regression line with the
coefﬁcients shown in the ﬁgure. To further understand the
latency of the lockstep detection step, recall that this phase