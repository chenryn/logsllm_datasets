MAXPOOL  2
FC  100
ACT
FC  10
I NPUT  #i nput _f eat ur e  #channel s  #bi t - l engt h
CONV  #f i l t er _si ze  #f i l t er s  #st r i de 
      #Pad  #OCA  ( opt i onal )
Descr i pt i on:  
MAXPOOl   #wi ndow_si ze
FC  #out put _neur ons 
Figure 8: Sample snippet code in XONN.
Keras to XONN Translation. To further facilitate the adap-
tation of XONN, a compiler is created to translate the de-
scription of the neural network in Keras [37] to the XONN
format. The compiler creates the .xonn ﬁle and puts the
network parameters into the required format (HEX string) to
be read by the framework during the execution of the GC
protocol. All of the parameter adjustments are also automat-
ically performed by the compiler.
5 Related Work
CryptoNets [14] is one of the early solutions that suggested
the adaptation of Leveled Homomorphic Encryption (LHE)
to perform oblivious inference. LHE is a variant of Partially
HE that enables evaluation of depth-bounded arithmetic cir-
cuits. DeepSecure [13] is a privacy-preserving DL frame-
work that relies on the GC protocol. CryptoDL [38] im-
proves upon CryptoNets [14] and proposes more efﬁcient
approximation of the non-linear functions using low-degree
polynomials. Their solution is based on LHE and uses mean-
pooling in replacement of the max-pooling layer. Chou et al.
propose to utilize the sparsity within the DL model to accel-
erate the inference [39].
SecureML [8] is a privacy-preserving machine learning
framework based on homomorphic encryption, GC, and se-
cret sharing. SecureML also uses customized activation
functions and supports privacy-preserving training in addi-
tion to inference. Two non-colluding servers are used to train
the DL model where each client XOR-shares her input and
sends the shares to both servers. MiniONN [9] is a mixed-
protocol framework for oblivious inference. The underlying
cryptographic protocols are HE, GC, and secret sharing.
Chameleon [7] is a more recent mixed-protocol frame-
work for machine learning, i.e., Support Vector Machines
(SVMs) as well as DNNs. Authors propose to perform
low-depth non-linear functions using the Goldreich-Micali-
Wigderson (GMW) protocol [5], high-depth functions by the
GC protocol, and linear operations using additive secret shar-
ing. Moreover, they propose to use correlated randomness
to more efﬁciently compute linear operations. EzPC [25] is
a secure computation framework that enables users to write
high-level programs and translates it to a protocol-based de-
scription of both Boolean and Arithmetic circuits. The back-
end cryptographic engine is based on the ABY framework.
Shokri and Shmatikov [40] proposed a solution for
privacy-preserving collaborative deep learning where the
training data is distributed among many parties. Their
approach, which is based on differential privacy, enables
clients to train their local model on their own training data
and update the central model’s parameters held by a central
server. However, it has been shown that a malicious client
can learn signiﬁcant information about the other client’s pri-
vate data [41]. Google [42] has recently introduced a new ap-
proach for securely aggregating the parameter updates from
multiple users. However, none of these approaches [40, 42]
study the oblivious inference problem. An overview of re-
lated frameworks is provided in [43, 44].
Frameworks such as ABY3 [45] and SecureNN [46] have
different computation models and they rely on three (or four)
parties during the oblivious inference.
In contrast, XONN
does not require an additional server for the computation. In
E2DM framework [47], the model owner can encrypt and
outsource the model to an untrusted server to perform obliv-
1510    28th USENIX Security Symposium
USENIX Association
ious inference. Concurrently and independently of ours, in
TAPAS [48], Sanyal et al. study the binarization of neural
networks in the context of oblivious inference. They report
inference latency of 147 seconds on MNIST dataset with
98.6% prediction accuracy using custom CNN architecture.
However, as we show in Section 7 (BM3 benchmark), XONN
outperforms TAPAS by close to three orders of magnitude.
Gazelle [10] is the previously most efﬁcient oblivious in-
ference framework. It is a mixed-protocol approach based
on additive HE and GC. In Gazelle, convolution operations
are performed using the packing property of HE. In this ap-
proach, many numbers are packed inside a single ciphertext
for faster convolutions. In Section 6, we brieﬂy discuss one
of the essential requirements that the Gazelle protocol has to
satisfy in order to be secure, namely, circuit privacy.
High-Level Comparison. In contrast to prior work, we pro-
pose a DL-secure computation co-design approach. To the
best of our knowledge, DeepSecure [13] is the only solu-
tion that preprocesses the data and network before the secure
computation protocol. However, this preprocessing step is
unrelated to the underlying cryptographic protocol and com-
pacts the network and data. Moreover, in this mode, some
information about the network parameters and structure of
data is revealed. Compared to mixed-protocol solutions,
not only XONN provides a more efﬁcient solution but also
maintains the constant round complexity regardless of the
number of layers in the neural network model. It has been
shown that round complexity is one of the important crite-
ria in designing secure computation protocols [49] since the
performance can signiﬁcantly be reduced in Internet settings
where the network latency is high. Another important ad-
vantage of our solution is the ability to upgrade to the secu-
rity against malicious adversaries using cut-and-choose tech-
niques [29, 30, 31]. As we show in Section 7, XONN outper-
forms all previous solutions in inference latency. Table 2
summarizes a high-level comparison between state-of-the-
art oblivious inference frameworks.
Table 2: High-Level Comparison of oblivious inference
frameworks. “C”onstant round complexity. “D”eep learn-
ing/secure computation co-design. “I”ndependence of sec-
ondary server. “U”pgradeable to malicious security using
standard solutions. “S”upporting any non-linear layer.
Framework
Crypto. Protocol C D I U S
CryptoNets [14]
DeepSecure [13]
SecureML [8]
MiniONN [9]
Chameleon [7]
EzPC [25]
Gazelle [10]
XONN (This work) GC, SS
HE
GC
HE, GC, SS
HE, GC, SS
GC, GMW, SS
GC, SS
HE, GC, SS
✗
✗
✓ ✗ ✓ ✗
✗
✓ ✓ ✓ ✓ ✓
✗
✗
✗
✗ ✓ ✗ ✓
✗
✗
✗
✗ ✓
✗
✗ ✓ ✗ ✓
✗
✗ ✓ ✗ ✓
✓ ✓ ✓ ✓ ✓
✗
6 Circuit Privacy
In Gazelle [10], for each linear layer, the protocol starts with
a vector m that is secret-shared between client m1 and server
m2 (m = m1 + m2). The protocol outputs the secret shares of
the vector m′ = A· m where A is a matrix known to the server
but not to the client. The protocol has the following proce-
dure: (i) Client generates a pair (pk, sk) of public and secret
keys of an additive homomorphic encryption scheme HE. (ii)
Client sends HE.Encpk(m1) to the server. Server adds its
share (m2) to the ciphertext and recovers encryption of m:
HE.Encpk(m). (iii) Server homomorphically evaluates the
multiplication with A and obtains the encryption of m′. (iv)
Server secret shares m′ by sampling a random vector r and
returns ciphertext c =HE.Encpk(m′ − r) to the client. The
client can decrypt c using private key sk and obtain m′ − r.
the Brakerski-Fan-Vercauteren (BFV)
scheme [50, 51]. However, the vanilla BFV scheme does
not provide circuit privacy. At high-level, the circuit privacy
requirement states that the ciphertext c should not reveal any
information about the private inputs to the client (i.e., A and
r) other than the underlying plaintext A · m − r. Otherwise,
some information is leaked. Gazelle proposes two methods
to provide circuit privacy that are not
incorporated in
their implementation. Hence, we need to scale up their
performance numbers for a fair comparison.
Gazelle uses
The ﬁrst method is to let the client and server engage in
a two-party secure decryption protocol, where the input of
client is sk and input of server is c. However, this method
adds communication and needs extra rounds of interaction.
A more widely used approach is noise ﬂooding. Roughly
speaking, the server adds a large noise term to c before re-
turning it to the client. The noise is big enough to drown any
extra information contained in the ciphertext, and still small
enough to so that it still decrypts to the same plaintext.
For the concrete instantiation of Gazelle, one needs to
triple the size of ciphertext modulus q from 60 bits to 180
bits, and increase the ring dimension n from 2048 to 8192.
The (amortized) complexity of homomorphic operations in
the BFV scheme is approximately O(log n log q), with the
exception that some operations run in O(log q) amortized
time. Therefore, adding noise ﬂooding would result in a
3-3.6 times slow down for the HE component of Gazelle.
To give some concrete examples, we consider two networks
used for benchmarking in Gazelle: MNIST-D and CIFAR-10
networks. For the MNIST-D network, homomorphic encryp-
tion takes 55% and 22% in online and total time, respec-
tively. For CIFAR-10, the corresponding ﬁgures are 35%,
and 10%1. Therefore, we estimate that the total time for
MNIST-D will grow from 0.81s to 1.16-1.27s (network BM3
in this paper). In the case of CIFAR-10 network, the total
time will grow from 12.9s to 15.48-16.25s.
1these percentage numbers are obtained through private communica-
tion with the authors.
USENIX Association
28th USENIX Security Symposium    1511
7 Experimental Results
We evaluate XONN on MNIST and CIFAR10 datasets, which
are two popular classiﬁcation benchmarks used in prior
work.
In addition, we provide four healthcare datasets to
illustrate the applicability of XONN in real-world scenarios.
For training XONN, we use Keras [37] with Tensorﬂow back-
end [52]. The source code of XONN is compiled with GCC
5.5.0 using O3 optimization. All Boolean circuits are synthe-
sized using Synopsys Design Compiler 2015. Evaluations
are performed on (Ubuntu 16.04 LTS) machines with Intel-
Core i7-7700k and 32GB of RAM. The experimental setup
is comparable (but has less computational power) compared
to the prior art [10]. Consistent with prior frameworks, we
evaluate the benchmarks in the LAN setting.
7.1 Evaluation on MNIST
There are mainly three network architectures that prior
works have implemented for the MNIST dataset. We convert
these reference networks into their binary counterparts and
train them using the standard BNN training algorithm [19].
Table 3 summarizes the architectures for the MNIST dataset.
Table 3: Summary of the trained binary network architec-
tures evaluated on the MNIST dataset. Detailed descriptions
are available in Appendix A.2, Table 13.
Arch.
BM1
BM2
BM3
Previous Papers
Description
SecureML [8], MiniONN [9]
CryptoNets [14], MiniONN [9],
DeepSecure [13], Chameleon [7]
3 FC
1 CONV, 2 FC
MiniONN [9], EzPC [25]
2 CONV, 2MP, 2FC
Analysis of Network Scaling: Recall that the classiﬁcation
accuracy of XONN is controlled by scaling the number of
neurons in all layers (Section 3.1). Figure 9a depicts the in-
ference accuracy with different scaling factors (more details
in Table 11 in Appendix A.2). As we increase the scaling
factor, the accuracy of the network increases. This accuracy
improvement comes at the cost of a higher computational
complexity of the (scaled) network. As a result, increasing
the scaling factor leads to a higher runtime. Figure 9b depicts
the runtime of different BNN architectures as a function of
the scaling factor s. Note that the runtime grows (almost)
quadratically with the scaling factor due to the quadratic in-
crease in the number of Popcount operations in the neural
network (see BM3). However, for the BM1 and BM2 net-
works, the overall runtime is dominated by the constant ini-
tialization cost of the OT protocol (∼ 70 millisecond).
GC Cost and the Effect of OCA: The communication cost
of GC is the key contributor to the overall runtime of XONN.
Here, we analyze the effect of the scaling factor on the total
message size. Figure 10 shows the communication cost of
(a)
(b)
Figure 9: Effect of scaling factor on (a) accuracy and (b) in-
ference runtime of MNIST networks. No pruning was ap-
plied in this evaluation.
GC for the BM1 and BM2 network architectures. As can
be seen, the message size increases with the scaling factor.
We also observe that the OCA protocol drastically reduces
the message size. This is due to the fact that the ﬁrst layer
of BM1 and BM2 models account for a large portion of the
overall computation; hence, improving the ﬁrst layer with
OCA has a drastic effect on the overall communication.
Figure 10: Effect of OCA on the communication of the BM1
(left) and BM2 (right) networks for different scaling factors.
No pruning was applied in this evaluation.
Comparison to Prior Art: We emphasize that, unlike pre-
vious work, the accuracy of XONN can be customized by
tuning the scaling factor (s). Furthermore, our channel/neu-
ron pruning step (Algorithm 2) can reduce the GC cost in
a post-processing phase. To provide a fair comparison be-
tween XONN and prior art, we choose a proper scaling factor
and trim the pertinent scaled BNN such that the correspond-
ing BNN achieves the same accuracy as the previous work.
Table 4 compares XONN with the previous work in terms of
accuracy, latency, and communication cost (a.k.a., message
size). The last column shows the scaling factor (s) used to in-
crease the width of the hidden layers of the BNN. Note that
the scaled network is further trimmed using Algorithm 2.
In XONN, the runtime for oblivious transfer is at least
∼ 0.07 second for initiating the protocol and then it grows
linearly with the size of the garbled tables; As a result, in
very small architectures such as BM1, our solution is slightly
slower than previous works since the constant runtime dom-
inates the total runtime. However, for the BM3 network
which has higher complexity than BM1 and BM2, XONN
1512    28th USENIX Security Symposium
USENIX Association
achieves a more prominent advantage over prior art. In sum-
mary, our solution achieves up to 7.7× faster inference (av-
erage of 3.4×) compared to Gazelle [10]. Compared to Min-
iONN [9], XONN has up to 62× lower latency (average of
26×) Table 4. Compared to EzPC [25], our framework is
34× faster. XONN achieves 37.5×, 1859×, 60.4×, and 14×
better latency compared to SecureML [8], CryptoNets [14],
DeepSecure [13], and Chameleon [7], respectively.
Table 4: Comparison of XONN with the state-of-the-art for
the MNIST network architectures.
Arch. Framework Runtime (s) Comm. (MB) Acc. (%)
BM1
BM2
BM3
SecureML
MiniONN
EzPC
Gazelle
XONN
CryptoNets
DeepSecure
MiniONN
Chameleon
EzPC
Gazelle
XONN
MiniONN
EzPC
Gazelle
XONN
4.88
1.04
0.7
0.09
0.13
297.5
9.67
1.28
2.24
0.6
0.29
0.16
9.32
5.1
1.16
0.15
-
15.8
76
0.5
4.29
372.2
791
47.6
10.5
70
8.0
38.28
657.5
501
70
32.13