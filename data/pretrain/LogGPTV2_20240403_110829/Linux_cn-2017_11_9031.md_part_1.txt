---
author: Robin Moffatt
category: 技术
comments_data: []
count:
  commentnum: 0
  favtimes: 0
  likes: 0
  sharetimes: 0
  viewnum: 9234
date: '2017-11-03 23:03:00'
editorchoice: false
excerpt: KSQL 是 Apache Kafka 中的开源的流式 SQL 引擎。它可以让你在 Kafka 主题topic上，使用一个简单的并且是交互式的 SQL
  接口，很容易地做一些复杂的流处理。
fromurl: https://www.confluent.io/blog/using-ksql-to-analyse-query-and-transform-data-in-kafka
id: 9031
islctt: true
largepic: /data/attachment/album/201711/03/230240ei0izdx0ldzlviyl.jpg
permalink: /article-9031-1.html
pic: /data/attachment/album/201711/03/230240ei0izdx0ldzlviyl.jpg.thumb.jpg
related: []
reviewer: ''
selector: ''
summary: KSQL 是 Apache Kafka 中的开源的流式 SQL 引擎。它可以让你在 Kafka 主题topic上，使用一个简单的并且是交互式的 SQL
  接口，很容易地做一些复杂的流处理。
tags:
- Twitter
- 大数据
- Kafka
thumb: false
title: 如何在 Apache Kafka 中通过 KSQL 分析 Twitter 数据
titlepic: true
translator: qhwdw
updated: '2017-11-03 23:03:00'
---
![](/data/attachment/album/201711/03/230240ei0izdx0ldzlviyl.jpg)
### 介绍
[KSQL](https://github.com/confluentinc/ksql/) 是 Apache Kafka 中的开源的流式 SQL 引擎。它可以让你在 Kafka  主题   topic 上，使用一个简单的并且是交互式的 SQL 接口，很容易地做一些复杂的流处理。在这个短文中，我们将看到如何轻松地配置并运行在一个沙箱中去探索它，并使用大家都喜欢的演示数据库源： Twitter。我们将从推文的原始流中获取，通过使用 KSQL 中的条件去过滤它，来构建一个聚合，如统计每个用户每小时的推文数量。
### Confluent
![](/data/attachment/album/201711/03/230309xi4qjpw5qqgej6i4.png)
首先， [获取一个 Confluent 平台的副本](https://www.confluent.io/download/)。我使用的是 RPM 包，但是，如果你需要的话，你也可以使用 [tar、 zip 等等](https://docs.confluent.io/current/installation.html?) 。启动 Confluent 系统：
```
$ confluent start
```
（如果你感兴趣，这里有一个 [Confluent 命令行的快速教程](https://www.youtube.com/watch?v=ZKqBptBHZTg)）
我们将使用 Kafka Connect 从 Twitter 上拉取数据。 这个 Twitter 连接器可以在 [GitHub](https://github.com/jcustenborder/kafka-connect-twitter) 上找到。要安装它，像下面这样操作：
```
# Clone the git repo
cd /home/rmoff
git clone https://github.com/jcustenborder/kafka-connect-twitter.git
```
```
# Compile the code
cd kafka-connect-twitter
mvn clean package
```
要让 Kafka Connect 去使用我们构建的[连接器](https://docs.confluent.io/current/connect/userguide.html#connect-installing-plugins)， 你要去修改配置文件。因为我们使用 Confluent 命令行，真实的配置文件是在 `etc/schema-registry/connect-avro-distributed.properties`，因此去修改它并增加如下内容：
```
plugin.path=/home/rmoff/kafka-connect-twitter/target/kafka-connect-twitter-0.2-SNAPSHOT.tar.gz
```
重启动 Kafka Connect：
```
confluent stop connect
confluent start connect
```
一旦你安装好插件，你可以很容易地去配置它。你可以直接使用 Kafka Connect 的 REST API ，或者创建你的配置文件，这就是我要在这里做的。如果你需要全部的方法，请首先访问 Twitter 来获取你的 [API 密钥](https://apps.twitter.com/)。
```
{
 "name": "twitter_source_json_01",
 "config": {
   "connector.class": "com.github.jcustenborder.kafka.connect.twitter.TwitterSourceConnector",
   "twitter.oauth.accessToken": "xxxx",
   "twitter.oauth.consumerSecret": "xxxxx",
   "twitter.oauth.consumerKey": "xxxx",
   "twitter.oauth.accessTokenSecret": "xxxxx",
   "kafka.delete.topic": "twitter_deletes_json_01",
   "value.converter": "org.apache.kafka.connect.json.JsonConverter",
   "key.converter": "org.apache.kafka.connect.json.JsonConverter",
   "value.converter.schemas.enable": false,
   "key.converter.schemas.enable": false,
   "kafka.status.topic": "twitter_json_01",
   "process.deletes": true,
   "filter.keywords": "rickastley,kafka,ksql,rmoff"
 }
}
```
假设你写这些到 `/home/rmoff/twitter-source.json`，你可以现在运行：
```
$ confluent load twitter_source -d /home/rmoff/twitter-source.json
```
然后推文就从大家都喜欢的网络明星 [rick] 滚滚而来……
```
$ kafka-console-consumer --bootstrap-server localhost:9092 --from-beginning --topic twitter_json_01|jq '.Text'
{
  "string": "RT @rickastley: 30 years ago today I said I was Never Gonna Give You Up. I am a man of my word - Rick x https://t.co/VmbMQA6tQB"
}
{
  "string": "RT @mariteg10: @rickastley @Carfestevent Wonderful Rick!!\nDo not forget Chile!!\nWe hope you get back someday!!\nHappy weekend for you!!\n❤…"
}
```
### KSQL
现在我们从 KSQL 开始 ! 马上去下载并构建它：
```
cd /home/rmoff
git clone https://github.com/confluentinc/ksql.git
cd /home/rmoff/ksql
mvn clean compile install -DskipTests
```
构建完成后，让我们来运行它：
```
./bin/ksql-cli local --bootstrap-server localhost:9092
```
```
                       ======================================
                       =      _  __ _____  ____  _          =
                       =     | |/ // ____|/ __ \| |         =
                       =     | ' /| (___ | |  | | |         =
                       =     |   
```
使用 KSQL， 我们可以让我们的数据保留在 Kafka 主题上并可以查询它。首先，我们需要去告诉 KSQL 主题上的 数据模式   schema 是什么，一个 twitter 消息实际上是一个非常巨大的 JSON 对象， 但是，为了简洁，我们只选出其中几行：
```
ksql> CREATE STREAM twitter_raw (CreatedAt BIGINT, Id BIGINT, Text VARCHAR) WITH (KAFKA_TOPIC='twitter_json_01', VALUE_FORMAT='JSON');
Message  
----------------
Stream created
```
在定义的模式中，我们可以查询这些流。要让 KSQL 从该主题的开始展示数据（而不是默认的当前时间点），运行如下命令：
```
ksql> SET 'auto.offset.reset' = 'earliest';  
Successfully changed local property 'auto.offset.reset' from 'null' to 'earliest'
```
现在，让我们看看这些数据，我们将使用 LIMIT 从句仅检索一行：
```
ksql> SELECT text FROM twitter_raw LIMIT 1;  
RT @rickastley: 30 years ago today I said I was Never Gonna Give You Up. I am a man of my word - Rick x https://t.co/VmbMQA6tQB
LIMIT reached for the partition.  
Query terminated
ksql>
```
现在，让我们使用刚刚定义和可用的推文内容的全部数据重新定义该流：
```
ksql> DROP stream twitter_raw;
Message
--------------------------------
Source TWITTER_RAW was dropped