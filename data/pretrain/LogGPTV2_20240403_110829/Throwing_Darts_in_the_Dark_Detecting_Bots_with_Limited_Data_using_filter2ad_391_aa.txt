title:Throwing Darts in the Dark? Detecting Bots with Limited Data using
Neural Data Augmentation
author:Steve T. K. Jan and
Qingying Hao and
Tianrui Hu and
Jiameng Pu and
Sonal Oswal and
Gang Wang and
Bimal Viswanath
Throwing Darts in the Dark? Detecting Bots with
Limited Data using Neural Data Augmentation
Steve T.K. Jan1,2, Qingying Hao1, Tianrui Hu2, Jiameng Pu2,
Sonal Oswal3, Gang Wang1, Bimal Viswanath2
1University of Illinois at Urbana-Champaign
2Virginia Tech
3Radware
PI:EMAIL, PI:EMAIL, {tianruihu, jmpu}@vt.edu, PI:EMAIL, PI:EMAIL, PI:EMAIL
Abstract—Machine learning has been widely applied to build-
ing security applications. However, many machine learning mod-
els require the continuous supply of representative labeled data
for training, which limits the models’ usefulness in practice. In
this paper, we use bot detection as an example to explore the
use of data synthesis to address this problem. We collected the
network trafﬁc from 3 online services in three different months
within a year (23 million network requests). We develop a stream-
based feature encoding scheme to support machine learning
models for detecting advanced bots. The key novelty is that
our model detects bots with extremely limited labeled data. We
propose a data synthesis method to synthesize unseen (or future)
bot behavior distributions. The synthesis method is distribution-
aware, using two different generators in a Generative Adversarial
Network to synthesize data for the clustered regions and the
outlier regions in the feature space. We evaluate this idea and
show our method can train a model that outperforms existing
methods with only 1% of the labeled data. We show that data
synthesis also improves the model’s sustainability over time and
speeds up the retraining. Finally, we compare data synthesis and
adversarial retraining and show they can work complementary
with each other to improve the model generalizability.
I. INTRODUCTION
In recent years, machine learning (ML) has shown great
success in building security defenses to protect computer
and networked systems [1], [2], [3]. Driven by empirical
data, machine learning algorithms can identify hidden patterns
that cannot be easily expressed by rules or signatures. This
capability leads to various ML-based applications such as
malicious website detection [4], [5], malicious phone call
classiﬁcation [6], network trafﬁc analysis [3], malware clas-
siﬁcation [7], [8], [9], [10], and intrusion detection [11], [2].
A common challenge faced by ML-driven systems is that
they often require “labeled data” to train a good detection
model [7], [5]. While it is already highly expensive to obtain
labels (e.g., via manual efforts),
the challenge is further
ampliﬁed by the dynamic behavior changes of attackers —
to keep the detection models up-to-date, there is a constant
need for labeling new data samples over time [12].
To solve this problem, a promising and yet under-explored
direction is to perform data synthesis. The idea is to generate
synthesized data to augment model training, with limited data
labels. For many of the security applications, however, the
challenge is the lack of complete knowledge of the problem
space, especially the attackers’ (future) data distribution, mak-
ing it difﬁcult to properly guide the data synthesis process. The
beneﬁts and limitations of this approach remain unclear.
In this paper, we use bot detection as an example to
explore the use of data synthesis to enable bot detection with
limited training data. We worked with a security company
and obtained a real-world network trafﬁc dataset that contains
23,000,000 network requests to three different online services
(e.g., e-commerce) over 3 different months in August 2018,
January 2019, and September 2019. The “ground-truth” labels
are provided by the security company’s internal system — via
a CAPTCHA system, and manual veriﬁcation. This dataset
allows us to explore the design of a generic stream-based
machine learning model for real-time bot detection.
We argue that
the true value of the machine learning
model is to handle attacker behaviors that cannot be precisely
expressed by “rules”. As such, we excluded bots that were
already precisely ﬂagged by existing rules and focused on
the remaining “advanced bots” that bypassed the rules. We
proposed a novel feature encoding method to encode new
trafﬁc data as they arrive for stream-based bot detection. We
empirically validated that (i) well-trained machine learning
models can help to detect advanced bots which signiﬁcantly
boosts the overall “recall” (by 15% to 30%) with a minor
impact on the precision; (ii) limited training data can indeed
cripple the supervised learning model, especially when facing
more complex bot behaviors.
To address the problem of limited data, we explore the
design of a new data synthesis method. We propose ODDS,
which is short for “Outlier Distribution aware Data Synthesis”.
The key idea is to perform a distribution-aware synthesis
based on known benign user data and limited bot samples.
The assumption is that the benign samples are relatively more
stable and representative than the limited bot data. We thus
synthesize new bot samples for the unoccupied regions in
the feature space by differentiating “clustered regions” and
“outlier regions”. At the clustered regions (which represent
common user/bot behavior), our data synthesis is designed to
be conservative by gradually reducing the synthesis aggres-
siveness as we approach the benign region. In the outlier areas
(which represent rare user behavior or new bot variants), our
data synthesis is more aggressive to ﬁll in the space. Based
on these intuitions, we designed a customized Generative Ad-
versarial Network (GAN) with two complementary generators
to synthesize clustered and outlier data simultaneously.
We evaluate the ODDS using real-world datasets, and show
that
it outperforms many existing methods. Using 1% of
the labeled data, our data synthesis method can improve the
detection performance close to that of existing methods trained
with 100% of the data. In addition, we show that ODDS not
only outperforms other supervised methods but improves the
life-cycle of a classiﬁer (i.e., staying effective over a longer
period of time). It is fairly easy to retrain an ODDS (with
1% of the data) to keep the models up-to-date. Furthermore,
we compare data synthesis with adversarial retraining. We
show that, as a side effect, data synthesis helps to improve
the model resilience to blackbox adversarial examples, and it
can work jointly with adversarial retraining to improve the
generalizability of the trained model. Finally, we analyze the
errors of ODDS to understand the limits of data synthesis.
We have three main contributions:
• First: we build a stream-based bot detection system to
complement existing rules to catch advanced bots. The
key novelty is the stream-based feature encoding scheme
which encodes new data as they arrive. This allows us
to perform real-time analysis and run bot detection on
anonymized network data.
• Second: we describe a novel data synthesis method to
enable effective model training with limited labeled data.
The method is customized to synthesize the clustered data
and the outlier data differently.
• Third: we validate our systems using real-world datasets
collected from three different online services. We demon-
strate the promising beneﬁts of data synthesis and discuss
the limits of the proposed method.
II. BACKGROUND AND GOALS
A. Bot Detection
Bots are computer-controlled software that pretends to be
real users to interact with online services and other users in
online communities. While there are bots designed for good
causes (search engine crawlers, research bots) [13], [14], [15],
most bots are operated to engage malicious actions such as
spam, scam, click fraud and data scrapping [16], [17], [1],
[18], [19], [20], [21], [22]. While many existing efforts are
devoted to bot detection, the problem is still challenging due
to the dynamic-changing nature of bots.
Online Turing Tests. CAPTCHA is short for "Completely
Automated Public Turning Test to tell Computers and Hu-
mans Apart" [23]. CAPTCHA is useful to detect bots but is
limited in coverage. The reason is that aggressively delivering
CAPTCHA to legitimate users would signiﬁcantly hurt user
experience. In practice, services want to deliver a minimum
number of CAPTCHAs to benign users while maximizing
the number of detected bots. As such, it is often used as a
validation method, to verify if a suspicious user is truly a bot.
Rule-based Approaches.
Rule-based detection approaches
detect bots following predeﬁned rules [24]. Rules are often
hand-crafted based on defenders’ domain knowledge. In prac-
tice, rules are usually designed to be highly conservative to
avoid false detection on benign users.
Machine Learning based Approaches. Machine learn-
ing techniques have been proposed to improve the detection
performance [25], [26], [27]. A common way is supervised
training with labeled bot data and benign user data [28],
[29], [21], [22]. There are also unsupervised methods [30],
[31], [32], but they are often limited in accuracy compared to
supervised methods.
B. Challenges in Practice
There are various challenges to deploy existing bot detection
methods in practice. In this work, we collaborate with a
security company Radware to explore new solutions.
Challenge-1: Bots are Evolving.
Bot behaviors are dy-
namically changing, which creates a challenge for the static
rule-based system. Once a rule is set, bots might make small
changes to bypass the pre-deﬁned threshold.
Challenge-2: Limited Labeled Data.
Data labeling is a
common challenge for supervised machine learning methods,
especially when labeling requires manual efforts and when
there is a constant need for new labels over time. For bot
detection, CAPTCHA is a useful way to obtain “labels”.
However, CAPTCHA cannot be delivered to all requests to
avoid degrading user experience. As such, it is reasonable to
assume the training data is limited or biased.
Challenge-3: Generalizability. Most bot detection methods
are heavily engineered for their speciﬁc applications (e.g. on-
line social networks, gaming, e-commerce websites) [21], [19],
[29], [22]. Due to the use of application-speciﬁc features (e.g.,
social graphs, user proﬁle data, item reviews and ratings), the
proposed model is hardly generalizable, and it is difﬁcult for
industry practitioners to deploy an academic system directly.
Application-dependent nature also makes it difﬁcult to share
pre-trained models among services.
Our Goals. With these challenges in mind, we build a
machine learning model that works complementary to the
existing rule-based system and the CAPTCHA system. The
model is designed to be generic, which only relies on basic
network-level information without taking any application-level
information. We design an encoding scheme that allows the
system to work on anonymized datasets, further improving
its portability across web services. In addition, the system is
stream-based, which processes incoming network trafﬁc and
make decisions in near real-time. More importantly, we use
this opportunity to explore the impact of “limited training
data” on model performance. We explore the beneﬁts and
limitations of data synthesis methods in enhancing the model
against attackers’ dynamic changes.
III. DATASET AND PROBLEM DEFINITION
Through our collaboration with Radware, we obtained
the network trafﬁc data from three online services over three
different months within a year. Each dataset contains the
“ground-truth” labels on the trafﬁc of bots and benign users.
TABLE I: Dataset summary.
TABLE II: Estimated false positives of rules on IP-sequences.
Site
A
B
C
All
August 2018
#Request Uniq.IP
225,331
2,812,355
273,383
4,022,195
4,388,929
180,555
11,223,479 667,537
January 2019
#Request Uniq.IP
1,981,913 157,687
2,559,923 238,678
-
4,541,836 393,504
-
September 2019
#Request Uniq.IP
1,676,842 151,304
5,579,243 1,301,310
-
7,256,085 1,447,247
-
Website Matched
Rules Matched
by Rules & Received CAPTCHA
42,487
23,346
50,394
38,294
12,554
19,718
A
B
C
Solved
CAPTCHA
4 (0.01%)
0 (0%)
0 (0%)
The dataset is suitable for our research for two main reasons.
First, each online service has its own service-speciﬁc func-
tionality and website structures. This offers a rare opportunity
to study the “generalizability” of a methodology. Second, the
datasets span a long period of time, which allows us to analyze
the impact of bot behavior changes.
A. Data Collection
We collected data by collaborating with Radware, a secu-
rity company that performs bot detection and prevention for
different online services. Radware gathers and analyzes the
network logs from their customers. We obtained permission to
access the anonymized network logs from three websites.
Table I shows the summary of the datasets. For anonymity
purposes, we use A, B, C to represent the 3 websites. For all
three websites, we obtained their logs in August 2018 (08/01
to 08/31). Then we collected data in January 2019 (01/08 to
01/31) and September 2019 (09/01 to 09/30) for two websites
(A, and B). We were unable to obtain data from website C
for the January and September of 2019 due to its service
termination with Radware.
The dataset contains a series of timestamped network re-
quests to the respective website. Each request contains a URL,
a source IP address, a referer, a cookie, a request timestamp (in
milliseconds) and the browser version (extracted from User-
Agent). To protect the privacy of each website and its users,
only timestamp is shared with us in the raw format. All
other ﬁelds including URL, IP, cookie, and browser version
are shared as hashed values. This is a common practice for
researchers to obtain data from industry partners. On one hand,
this type of anonymization increases the challenges for bot
detection. On the other hand, this encourages us to make more
careful design choices to make sure the system works well on
anonymized data. Without the need to access the raw data, the
system has a better chance to be generalizable. In total, the
dataset contains 23,021,400 network requests from 2,421,184
unique IP addresses.
B. Reprocessing: IP-Sequence
Our goal
is to design a system that
is applicable to a
variety of websites. For this purpose, we cannot rely on
the application-level user identiﬁer to attribute the network
requests to a “user account”. This is because not all websites
require user registration (hence the notion of “user account”
does not exist). We also did not use “cookie” as a user
identiﬁer because we observe that bots often frequently clear
their cookies in their requests. Using cookies makes it difﬁcult
to link the activities of the same bot. Instead, we group
network requests based on the source IP address.
Given an IP, a straightforward way might be labeling the
IP as “bot” or “benign”. However, such binary labels are not
ﬁne-grained enough for websites to take further actions (e.g.,
delivering a CAPTCHA or issuing rate throttling). The reason
is that it’s common for an IP address to have both legitimate
and bot trafﬁc at different time periods, e.g., due to the use
of web proxy and NAT (Network Address Translation). As
such, it is more desirable to make ﬁne-grained decisions on
the “sub-sequences” of requests from an IP address.