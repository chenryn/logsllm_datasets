transformer based models, BERT, GPT, XLM etc. thresholding over the anomaly score gets particularly chal-
12
lenging for unsupervised models. While for benchmarking Public benchmarks for parsing, clustering, summariza-
purposes, evaluation metrics like AUROC (Area under ROC tion: Most of the log parsing, clustering and summarization
curve) can suffice, but for practical deployment of these literature only uses a very small subset of data from some of
models require either careful calibrations of anomaly scores the public log datasets, where the oracle parsing is available,
or manual tuning or heuristic means for setting the threshold. or in-house log datasets from industrial applications where
Thisbeingquitesensitivetotheapplicationathand,alsoposes they compare with oracle parsing methods that are unscalable
realistic challenges when generalizing to heterogenous logs in practice. This also makes fair comparison and standardized
from different systems. benchmarking difficult for these tasks.
Handlinglargevolumeofdata: Anotherchallengeinlog Betterloglanguagemodels: Someoftherecentadvances
analysis tasks is handling the huge volumes of logs, where inneuralNLPmodelsliketransformerbasedlanguagemodels
most large-scale cloud-based systems can generate petabytes BERT, GPT has proved quite promising for representing logs
of logs each day or week. This calls for log processing in natural language style and enabling various log analysis
algorithms, that are not only effective but also lightweight tasks.Howeverthereismorescopeofimprovementinbuilding
enough to be very fast and efficient. neural language models that can appropriately encode the
Handling non-stationary log data: Along with humon- semi-structured logs composed of fixed template and variable
parameters without depending on an external parser.
gous volume, the natural and most practical setting of
logs analysis is an online streaming setting, involving non- Incorporating Domain Knowledge: While existing log
stationary data distribution - with heterogenous log streams anomaly detection systems are entirely rule-based or auto-
coming from different inter-connected micro-services, and the mated, given the complex nature of incidents and the di-
software logging data itself evolving over time as developers verse varieties of anomalies, a more practical approach would
naturally keep evolving software in the agile cloud devel- involve incorporating domain knowledge into these models
opment environment. This requires efficient online update either in a static form or dynamically, following a human-
schemes for the learning algorithms and specialized effort in-the-loop feedback mechanism. For example, in a complex
towardsbuildingrobustmodelsandevaluatingtheirrobustness system generating humungous amounts of logs, which kinds
towards unstable or evolving log data. ofincidentsaremoresevereandwhichtypesoflogsaremore
Handling noisy data: Annotating log data being ex- crucial to monitor for which kind of incidents. Or even at
tremely challenging even for domain experts, supervised and the level of loglines, domain knowledge can help understand
semi-supervised models need to handle this noise during the real-world semantics or physical significance of some
training,whileforunsupervisedmodels,itcanheavilymislead of the parameters or variables mentioned in the logs. These
evaluation. Even though it affects a small fraction of logs, aspects are often hard for the ML system to gauge on its own
the extreme class imbalance aggrevates this problem. Another especially in the practical unsupervised settings.
related challenge is that of errors compounding and cascading
Unified models for heterogenous logs: Most of the log
fromeachoftheprocessingstepsintheloganalysisworkflow
analysis models are highly sensitive towards the nature of log
when performing the downstream tasks like anomaly detec-
preprocessing or grouping, needing customized preprocessing
tion.
for each type of application logs. This alludes towards the
Realistic public benchmark datasets for anomaly detec- needforunifiedmodelswithmoregeneralizablepreprocessing
tion: Amongst the publicly available log anomaly detection layersthatcanhandleheterogenouskindsoflogdataandalso
datasets, only a limited few contain anomaly labels. Most of different types of log analysis tasks. While [21] was one of
those benchmarks have been excessively used in the literature thefirstworkstoexplorethisdirection,thereiscertainlymore
and hence do not have much scope of furthering research. research scope for building practically applicable models for
Infact, their biggest limitation is that they fail to showcase log analysis.
the diverse nature of incidents that typically arise in real-
world deployment. Often very simple handcrafted rules prove
C. Traces and Multimodal Incident Detection
to be quite successful in solving anomaly detection tasks
on these datasets. Also, the original scale of these datasets
Problem Definition
are several orders of magnitude smaller than the real-world
Tracesaresemi-structuredeventlogswithspaninformation
use-cases and hence not fit for showcasing the challenges of
about the topological structure of the service graph. Trace
online or streaming settings. Further, the volume of unique
anomaly detection relies on finding abnormal paths on the
patternscollapsessignificantlyafterthetypicallogprocessing
topological graph at given moments, as well as discovering
steps to remove irrelevant patterns from the data. On the
abnormalinformationdirectlyfromtraceeventlogtext.There
other hand, a vast majority of the literature is backed up
are multiple ways to process trace data. Traces usually have
by empirical analysis and evaluation on internal proprietary
timestamps and associated sequential information so it can
data, which cannot guarantee reproducibility. This calls for
be covered into time-series data. Traces are also stored as
more realistic public benchmark datasets that can expose the
trace event logs, containing rich text information. Moreover,
real-world challenges of aiops-in-the-wild and also do a fair
traces store topological information which can be used to
benchmarking across contemporary log analysis models.
reconstruct the service graphs that represents the relation
13
amongcomponentsofthesystems.Fromthedataperspective, anomalydetection.LSTMisaspecialtypeofrecurrentneural
traces can easily been turned into multiple data modalities. network (RNN) and has been proved to success in lots of
Thus,wecombinestrace-basedanomalydetectionwithmulti- other domains. In AIOps, LSTM is also commonly used in
modal anomaly detection to discuss in this section. Recently, metric and log anomaly detection applications. Trace data
we can see with the help of multi-modal deep learning is a natural fit with RNNs, majorly in two ways: 1) The
technologies, trace anomaly detection can combine different topologicalorderoftracescanbemodeledaseventsequences.
levels of information relayed by trace data and learn more These event sequences can easily be transformed into model
comprehensive anomaly detection models [119][120]. inputs of RNNs. 2) Trace events usually have text data that
conveys rich information. The raw text, including both the
Empirical Approaches structured and unstructured parts, can be transformed into
Traces draw more attention in microservice system archi- vectors via standard tokenization and embedding techniques,
tectures since the topological structure becomes very complex andfeedtheRNNasmodelinputs.Suchdeeplearningmodel
and dynamic. Trace anomaly detection started from practical architectures can be extended to support multimodal input,
usagesforlargescalesystemdebugging[121].Empiricaltrace such as combining trace event vector with numerical time
anomaly detection and RCA started with constructing trace series values [119].
graphs and identifying abnormal structures on the constructed To better leverage the topological information of traces,
graph. Constructing the trace graph from trace data is usually graph neural networks have also been introduced in trace
very time consuming, an offline component is designed to anomaly detection. Zhang et al. developed DeepTraLog, a
train and construct such trace graph. Apart from , to adapt trace anomaly detection technique that employs Gated graph
to the usage requirements to detect and locate issues in large neural networks [120]. DeepTraLog targets to solve anomaly
scale systems, trace anomaly detection and RCA algorithms detection problems for complex microservice systems where
usually also have an online part to support real-time service. service entity relationships are not easy to obtain. Moreover,
For example, Cai et al.. released their study of a real-time the constructed graph by GGNN training can also be used
trace-level diagnosis system, which is adopted by Alibaba to localize the issue, providing additional root-cause analysis
datacenters. This is one of the very few studies to deal with capability.
real large distributed systems [122].
Most empirical trace anomaly detection work follow the Limitations
offlineandonlinedesignpatterntoconstructtheirgraphmod- Trace data became increasingly attractive as more applica-
els. In the offline modeling, unsupervised or semi-supervised tions transitioned from monolithic to microservice architec-
techniques are utilized to construct the trace entity graphs, ture. There are several challenges in machine learning based
very similar to techniques in process discovery and mining trace anomaly detection.
domain. For example, PageRank has been used to construct Data quality. As far as we know, there are multiple trace
web graphs in one of the early web graph anomaly detection collection platforms and the trace data format and quality
works [123]. After constructing the trace entity graphs, a are inconsistent across these platforms, especially in the pro-
variety of techniques can be used to detect anomalies. One duction environment. To use these trace data for analysis,
common way is to compare the current graph pattern to researchers and developers have to spend significant time and
normalgraphpatterns.Ifthecurrentgraphpatternsignificantly effort to clean and reform the data to feed machine learning
deviates from the normal patterns, report anomalous traces. models.
An alternative approach is using data mining and statistical Difficult to acquire labels. It is very difficult to acquire
learningtechniquestorundynamicanalysiswithoutconstruct- labels for production data. For a given incident, labeling the
ing the offline trace graph. Chen et al. proposed Pinpoint correspondingtracerequiresidentifyingtheincidentoccurring
[124], a framework for root cause analysis that using coarse- time and location, as well as the root cause which may be
grained tagging data of real client requests at real-time when located in totally different time and location. Obtaining such
these requests traverse through the system, with data mining full labels for thousands of incidents is extremely difficult.
techniques.Pinpointdiscoversthecorrelationbetweensuccess Thus, most of the existing trace analysis research still use
/ failure status of these requests and fault components. The synthetic data to evaluate the model performance. This brings
entire approach processes the traces on-the-fly and does not moredoubtswhethertheproposedsolutioncansolveproblems
leverage any static dependency graph models. in real production.
No sufficient multimodal and graph learning models.
Deep Learning Based Approaches Tracedataarecomplex.Currenttraceanalysissimplifiestrace
In recent years, deep learning techniques started to be dataintoeventsequencesortime-seriesnumericalvalues,even
employed in trace anomaly detection and RCA. Also with in the multimodal settings. However, these existing model
the help of deep learning frameworks, combining general architectures did not fully leverage all information of trace
trace graph information and the detailed information inside of data in one place. Graph-based learning can potentially be a
each trace event to train multimodal learning models become solution but discussions of this topic are still very limited.
possible. Offline model training. The deep learning models in
Long-short term memory (LSTM) network [125] is a very existing research relies on offline model training, partially
popular neural network model in early trace and multimodal because model training is usually very time consuming and
14
contradictswiththegoalofreal-timeserving.However,offline A. Metrics based Failure Prediction
modeltrainingbringsstaticdependenciestoadynamicsystem.
Metric data are usually fruitful in monitoring system. It
Such dependencies may cause additional performance issues.
is straightforward to directly leverage them to predict the
occurrenceoftheincidentinadvance.Assuch,someproactive
Future Trends
actions can be taken to prevent it from happening instead of
UnifiedtracedataRecently,OpenTelemetryleadstheeffort
reducingthetimefordetection.Generally,itcanbeformulated
to unify observability telemetry data, including metrics, logs,
astheimbalancedbinaryclassificationproblemiffailurelabels
traces, etc., across different platforms. This effort can bring
are available, and formulated as the time series forecasting
huge benefits to future trace analysis. With more unified data
problem if the normal range of monitored metrics are defined
models,AIresearcherscanmoreeasilyacquirenecessarydata
in advance. In general, failure prediction [126] usually adopts
to train better models. The trained model can also be easily
machine learning algorithms to learn the characteristics of
plug-and-playbyotherparties,whichcanfurtherboostmodel
historical failure data, build a failure prediction model, and
quality improvements.
then deploy the model to predict the likelihood of a failure in
Unified engine for detection and RCA Trace graph
the future.
contains rich information about the system at a given time.
With the help of trace data, incident detection and root cause Methods
localizationcanbedonewithinonestep,insteadofthecurrent General Failure Prediction: Recently, there are increasing
twoconsecutivesteps.Existingworkhasdemonstratedthatby efforts on considering general failure incident prediction with
simply examining the constructed graph, the detection model the failure signals from the whole monitoring system. [127]
can reveal sufficient information to locate the root causes collected alerting signals across the whole system and dis-
[120]. covered the dependence relationships among alerting signals,
UnifiedmodelsformultimodaltelemetrydataTracedata then the gradient boosting tree based model was adopted
analysis brings the opportunities for researchers to create a to learn failure patterns. [128] proposed an effective feature
holistic view of multiple telemetry data modality since traces engineering process to deal with complex alert data. It used
can be converted into text sequence data and time-series data. multi-instance learning and handle noisy alerts, and inter-
The learnings can be extended to include logs or metrics pretable analysis to generate an interpretable prediction result
from different sources. Eventually we can expect unified to facilitate the understanding and handling of incidents.
learning models that can consume multimodal telemetry data Specific Type Failure Prediction: In contrast, some works
for incident detection and RCA. In contrast, [127] and [128] aim to proactively predict various
Online Learning Modern systems are dynamic and ever- specifictypesoffailures.[129]extractedstatisticalandtextual
changing. Current two-step solution relies on offline model featuresfromhistoricalswitchlogsandappliedrandomforest