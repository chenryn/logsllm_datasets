1
1
1
1
1
1
t0
0
0
0
0
1
1
1
1
0
0
0
0
1
1
1
1
i1
0
0
1
1
0
0
1
1
0
0
1
1
0
0
1
1
i0
0
1
0
1
0
1
0
1
0
1
0
1
0
1
0
1
h
0
1
0
1
0
1
0
1
0
0
0
0
1
1
1
1
f Comments
0
0
0
1
0
0
0
1
0
0
0
1
1
0
1
1
h 6= i0
f 6= t1, f 6= i1
h 6= t1, h 6= t0, h 6= i1
f 6= t0, f 6= i0, f 6= h
Trigger condition is true.
Trigger condition is true.
Trigger condition is true.
Trigger condition is true.
Figure 3. Truth table for two-gate circuit shown in Figure 2.
containing some hidden behavior and use the hidden behav-
ior to create an attack. We insert our malicious hardware into
a processor and show that UCI does not detect the attack.
The UCI paper describes what the authors call a malicious
foothold: malicious hardware inserted into a processor that
allows the attacker to later gain entry to the machine
and launch a system-level attack. One such foothold they
describe is the supervisor transition foothold. This is a
piece of hardware inserted into the processor’s core pipeline
that, upon detecting a secret trigger sequence of instructions
(“knock”), transitions the current process into supervisor
mode.
Hicks et al. [2] show that UCI is able to detect their
implementation of the supervisor transition foothold. In this
section, we show how to implement the same attack, but in
a way that UCI can not detect. We introduce a backdoor
into the Leon3 processor, which implements the SPARCv8
architecture. The conﬁguration of the processor and system
are identical to the conﬁguration used in the paper proposing
UCI.
In order to implement a stealthy attack, we looked for
a place in the code where the value of the supervisor-
mode bit was set by the output of some n-input function
super ⇐ g(i0, i1, . . . , in−1). Then we searched through
our library of example stealthy, admissible, and malicious
circuits to ﬁnd one with the appropriate output function. We
needed fN T = g(i0, i1, . . . , in−1) and fT = 1, which will,
when triggered, artiﬁcially increase the privilege level of the
executing process.
We inserted our attack in to the processor’s integer unit
pipeline. The process we modiﬁed is one that writes values
computed in the previous clock cycle by each stage in the
pipeline for use by the subsequent stages in the next clock
cycle. Shown in Figure 4 is the fragment of the process
69
pertaining to the setting of the super signal, the bit that
will put
the processor in supervisor mode. (This is not
the actual code, but a more readable representation of the
code’s behavior.) On holdn = 1 4 the supervisor-mode bit
gets updated with the value computed in the previous cycle
(in.super). On resetn = 0 (active low) the processor
is put in supervisor mode. Otherwise, the supervisor-mode
bit remains as it was in the previous cycle.
reg : process (clk)
begin
if rising_edge(clk) then
if (holdn = ’1’) then
super <= in.super;
end if;
if resetn = ’0’ then
super <= ’1’;
end if;
end if;
end process;
Figure 4. Original processor pipeline code responsible for setting the
supervisor-mode bit. This is the code before we inserted our backdoor.
holdn
super
holdn
in.super
resetn
instruct_curr
f
holdn
instruct_prev
super
holdn
in.super
super <= ((NOT holdn AND super) OR
(holdn AND in.super)) OR
((NOT resetn) OR (instruct_curr AND
(instruct_prev OR ((NOT holdn AND super)
OR (holdn AND in.super)))));
Figure 5. The attack circuit we use for the hidden backdoor. The top shows
the structure of the attack. The bottom shows the HDL implementation of
the attack.
Our attack sets super ⇐ 1 when the processor sees a
speciﬁc instruction repeated two times in a row. Not shown
4holdn is an active low signal that depends on a similar hold signal in
the data and instruction caches
70
here are the two processes used to track when this instruction
has been seen. The attack circuit we inserted is shown
in Figure 5. We inserted the line of code shown there as
a replacement to the code shown in Figure 4. As in our
simple example (Figure 2), in this circuit there is no pair of
dependent signals that could have been replaced by a wire
during design-time testing without changing the behavior of
the output signal. In the absence of the “knock” instructions,
the attack code works as the original code did. For additional
conﬁdence that our modiﬁed processor could pass design-
time testing, we tested it using the ofﬁcial SPARC certiﬁca-
tion test benches from SPARC International. The processor
passed all tests, allowing the malicious process to carry the
SPARC trademark.
In total, we inserted 19 lines of code into the processor.
This includes the code to set the supervisor bit as well as
the code to track the presence of the trigger instructions. We
also removed 7 lines of the original code from the processor,
for a net increase of 12 lines of code. This is comparable to
the number of changes required for the attack used in the
UCI paper.
Using the same tools and environment as in the UCI
paper, we performed UCI analysis on the processor pipeline
augmented with our attack. We used the test bench that
comes bundled with the Leon3 processor for the UCI
analysis. While the original UCI experiments used additional
test cases, adding additional test cases does not increase the
chance of detection by UCI. The number of circuits ﬂagged
by UCI as potentially malicious only decreases with the
use of additional test cases. We discuss this point further in
Section VI.
UCI was unable to detect our attack, with one caveat,
which the next section describes.
C. Analysis of UCI Results
The caveat is that, to test our attack, we extended the
test bench slightly. The instruction we used as our trigger
did not appear anywhere in the original test bench. There-
fore, UCI was able to detect that the instruct_prev
and instruct_curr signals (which indicate whether the
previous/current instruction is the trigger instruction) were
constant at 0. We added one instance of this instruction to
the test bench, and conﬁrmed that UCI is then unable to
detect any sign of the attack circuit.
This modiﬁcation to the test bench violates our assump-
tion that the attacker can not control the tests used during
design-time testing. However, as we argue next, this is not
an inherent limitation of our attack, but rather an accidental
artifact of a sub-optimal implementation of the attack, and
it would be straightforward for an attacker to adjust the
attack to avoid this issue. In particular, it would be easy
to pick some other instruction that appears at least once in
the test bench, but that does not appear twice in a row. It
seems reasonable to assume that the attacker would know the
contents of the test bench, or at least some of the test cases,
making it easy to choose a trigger instruction that already
exists in the test bench. In retrospect, we chose our triggering
instruction poorly: we overlooked the need to choose a
trigger instruction that already occurs in the test bench. It
would have been trivial to choose a different instruction, and
if we had chosen more wisely, no modiﬁcation to the test
bench would have been necessary.
We also veriﬁed that none of the false positives produced
by UCI would inadvertently reveal our backdoor. The UCI
algorithm produces a fair number of false positives: pairs of
dependent signals that happen to always be equal during the
design-time testing that was done, but that can be non-equal
under legitimate, untested input conﬁgurations. If the attack
circuit happens to fall within the dependency chain of a pair
of such signals, the attack would be highlighted by UCI as
potentially malicious. In other words, UCI might get lucky
and catch the attack by sheer chance. If the system designers
reviewed everything highlighted by UCI, then they might
notice the attack circuit as they reviewed the false positives.
To see if this was the case with our attack, we looked at
all the pairs of signals highlighted by UCI and veriﬁed that
none of the gates involved in our attack were involved in
the dependency chain between two such signals.
We also veriﬁed that our attack is successfully able to
defeat BlueChip, a hybrid software/hardware solution that
builds upon UCI to defend against backdoors [2]. BlueChip
removes any hardware ﬂagged by UCI as potentially ma-
licious and replaces it with a trap. During execution, any
instruction that would have used the removed circuitry gets
simulated in software using alternate instructions. Because
our attack circuit is not detected by UCI, it is not removed
by BlueChip, and thus BlueChip is not able to detect or
defend against our backdoor. However, our attack relies upon
the assumption that the trigger instructions will be executed
natively, in hardware, and will not trap; we veriﬁed that this
is indeed the case and that our trigger instructions will not
cause an unwanted trap. In particular, we compared the set
of (false positive) signals ﬂagged by UCI when run on the
processor using the regular test bench to that returned by
UCI when using a test bench augmented with a test that
executes our trigger instructions. There was no difference
between the two. This demonstrates that our trigger instruc-
tions do not cause BlueChip to trap and simulate them in
software; the instructions execute normally and successfully
engage the backdoor.
To test our attack, we used an environment identical to that
used in the UCI paper. We synthesized the Leon3 processor
with our embedded attack on a Xilinx Virtex 5 FPGA
development board and successfully booted the SnapGear
Embedded Linux distribution (linux-2.6.21.1 kernel). Then,
we wrote a user-level program that triggers the backdoor.
The program was able to successfully transition the proces-
sor into supervisor mode, using the backdoor we built.
D. Multiple Triggers
The above example is not perfectly stealthy, since there