it does not conflict with the other processors when accessing memory
clusters. Formally:
OBSERVATION 1 (CONTENTION-FREE MATCHES). Consider a
single-processor schedule that is applied by all processors. Then
there is no memory contention, i.e., all match requests initiated
by different processors at the same cycle are destined to different
memories.
This observation stems from the fact that all packets arrive at
the switch at different clock cycles, and since the schedule is fixed,
the time offset from packet arrival to memory access is also fixed.
In addition, since the ODG is acyclic, each packet accesses each
memory at most once. Therefore, in any given memory cluster, the
times at which the clusterâ€™s memory is accessed are different for
different packets.
dRMT achieves higher throughput than RMT. An important
corollary of Observation 1 is that the throughput of the dRMT archi-
tecture is guaranteed to be at least the throughput of a corresponding
RMT architecture. Intuitively, consider the sequence of operations
on a packet in an RMT pipe. Then, we can use the same schedule for
each dRMT processor in the same order. Clearly, if a dRMT proces-
sor has the same Â¯M and Â¯A capacity as an RMT stage, such a schedule
will respect the program-specific dependency constraints and the Â¯M,
Â¯A and IPC architectural constraints, and therefore it results in a valid
schedule for dRMT. Formally:
THEOREM 3.3. The throughput of a program on dRMT is at least
that of RMT.
The full proofs of all the results in this paper are available in an
online extended version [5].
(a) Naive schedule with conflict
that corresponds to Figure 4c.
(b) Schedule without conflicts that
corresponds to Figure 4d.
Figure 5: Illustrating the naive and conflict-free schedules from
Figure 4 using a cyclic single-packet analysis. Each rectangle
shows the operations that are executed simultaneously by a pro-
cessor in steady-state, possibly for different packets.
Single-packet scheduling. As mentioned, a fixed schedule applies
the same operations in the same order to all incoming packets, which
periodically arrive at a given processor every P slots. Consider a
packet that arrives at time t. At this time, the processor simultane-
ously executes set of operations 0 for this packet, set P for the packet
that arrived at t âˆ’ P, set 2P for the packet that arrived at t âˆ’ 2P, and
so on. These simultaneously-executed sets are precisely the sets of
operations that the first packet itself executes at all cycles that are
equivalent to t modulo P, because the first packet will also execute
the set P of operations at time t + P, the set 2P at t + 2P, and so
on. As a result, instead of analyzing how the different packets share
resources at this processor, analyzing all the operations of a single
packet will suffice.
Example 3.4 (Single-packet scheduling). Consider again the
unicast-multicast example of Figure 4. Figure 5 shows a simpli-
fied analysis that considers only a single packet at a single processor.
Each rectangle represents an equivalence class, i.e., all the operations
that are executed simultaneously in a cycle modulo P. The time slot
at which each operation is executed appears before the operation.
For a schedule that respects the program dependency constraints to
be valid, we only need to make sure that all operations assigned to
an equivalence class respect the Â¯M, Â¯A, and IPC constraints. Figure 5a
illustrates the schedule with conflicts from Figure 4c. The operations
in rectangle cycle mod P â‰¡ 0 again violate the same two architectural
constraints: there are four match operations where Â¯M = 2 and there
are two distinct execution times (0 and 2) for the match operations
where IPC = 1 (detailed in observation 3). Figure 5b illustrates the
conflict-free schedule with the no-op from Figure 4d. It is easy to
verify the validity of the schedule by considering the operations in
each rectangle.
As illustrated in Figure 5, in order to establish a single-packet
schedule that respects the Â¯M, Â¯A and IPC architectural constraints,
we can transform these constraints into cyclic constraints, i.e., con-
straints modulo P on the scheduling sequence of a single packet.
Specifically, we define P equivalence classes that correspond to the
schedule period length and rely on the following observation:
OBSERVATION 2 (CYCLIC PROCESSOR SCHEDULE). Construct-
ing a valid schedule for a single processor corresponds to assigning
each match and action operation in the ODG to an equivalence
0:ğ‘€0&ğ‘€12:ğ‘€2&ğ‘€34:ğ´1&ğ´21:âˆ’3:âˆ’5:ğ´3ğ‘ğ‘¦ğ‘ğ‘™ğ‘’mod2â‰¡0ğ‘ğ‘¦ğ‘ğ‘™ğ‘’mod2â‰¡1ğ‘ğ‘¦ğ‘ğ‘™ğ‘’mod2â‰¡0ğ‘ğ‘¦ğ‘ğ‘™ğ‘’mod2â‰¡10:ğ‘€0&ğ‘€12:ğ‘›ğ‘œâˆ’ğ‘œğ‘4:âˆ’6:ğ´31:âˆ’3:ğ‘€2&ğ‘€35:ğ´1&ğ´2dRMT: Disaggregated Programmable Switching
SIGCOMM â€™17, August 21-25, 2017, Los Angeles, CA, USA
not violating any resource constraints. To that end, we introduce
indicator variables that track the usage of resources by each class.
We define the equivalence class of a node to be the remainder of this
nodeâ€™s execution time divided by the period length P. Accordingly,
we introduce rq(v, q, r ), which is a binary variable that respects
rq(v, q, r ) = 1 iff t (v) = q Â· P + r, and is 0 otherwise. It is easy to see
that the indicator variable must satisfy two equalities:
rq(v, q, r ) = 1 âˆ€v âˆˆ ODG,
t (v) =
(q Â· P + r ) Â· rq(v, q, r ) âˆ€v âˆˆ ODG.
(cid:88)
(cid:88)
q,r
q,r
class, while ensuring that the requirements of the operations as-
signed to the same equivalence class do not violate the architectural
constraints, i.e., Â¯M, Â¯A, and IPC.
It is straightforward to verify that the Â¯M and Â¯A constraints are
respected by considering all the operations in each of the equivalence
classes. However, to verify that the IPC constraint is respected we
need one additional observation:
OBSERVATION 3 (PACKET CLASSIFICATION). The number of
different packets that a processor initiates matches (actions) for
in cycle t equals the number of distinct execution times of match
(action) nodes in equivalence class t mod P (i.e., the class of all
nodes with the same remainder when their execution start times are
divided by P).
Using these observations, we formally obtain the following result:
THEOREM 3.5. The single-packet schedule is valid iff the full
dRMT schedule is valid.
Integer linear program
3.3
Before presenting our ILP scheduling solution, we establish the
following scheduling hardness result:
THEOREM 3.6. The dRMT scheduling problem is NP-hard.
Our proof is based on a reduction from the bin-packing problem in
which the bins represent cycles modulo P and the objects of different
volumes represent action (match) operations.
With the intractability result at hand, we observe that all dRMT
architectural constraints are integer valued. Therefore, as long as
the target function is linear as well, we can express this scheduling
problem as an Integer Linear Programming (ILP) problem, which
can be optimally solved using an ILP solver such as Gurobi [6].
ILP formulation. We can restrict ourselves to a cyclic schedule
with modulo constraints, as established in Theorem 3.5. Focus on
a packet Ï€, and consider an ODG node corresponding to operation
v for this packet. We denote by t (v) the time at which operation
v starts. Our goal is to find a cyclic schedule for Ï€ that satisfies
all of the constraints and minimizes the maximum t (v) among all
nodes v, i.e., the start time of the last operation. Then, the first set of
constraints is t (v) â‰¤ T âˆ€v âˆˆ ODG where T is the objective function
we want to minimize.
We next provide an overview on how the ILP deals with the
three types of constraints: P4-program dependency constraints (with
âˆ†M and âˆ†A); resource constraints (with Â¯M and Â¯A); and inter-packet
concurrency constraints (IPC).
Dependency constraints. When solving the scheduling problem,
we must respect the dependencies specified by the edges in the ODG
and their corresponding delay. We can express these constraints as
(2)
where Ï„ (u, v) is the number of cycles that must pass between u
and v (i.e., âˆ†A or âˆ†M). Note that we do not demand equality. The
price for this flexibility is the scratch pad that may be needed to
store intermediate results until they are consumed by a successor
node (Â§5).
Resource constraints. As stated by Observation 2, we seek to parti-
tion all the nodes in the ODG among the P equivalence classes while
t (v) âˆ’ t (u) â‰¥ Ï„ (u, v) âˆ€(u, v) âˆˆ ODG,
(3)
(4)
(7)
(8)
Let VM be the set of all match nodes v in the ODG, and k (v) be the
key size in bits of match node v âˆˆ VM . Likewise, let VA be the set
of all action nodes v and a(v) be the number of modified fields by
action v âˆˆ VA. Now, we can formulate the match and action resource
constraints as (cid:88)
(cid:88)
Â· rq(v, q, r ) â‰¤ Â¯M âˆ€r ,
b
a(v) Â· rq(v, q, r ) â‰¤ Â¯A âˆ€r .
vâˆˆVM ,q
k (v)
(cid:38)
(cid:39)
(5)
(6)
vâˆˆVA,q
IPC constraints. We want to set IPC as an upper limit on the max-
imum number of different packets for which the processors can
generate match keys in the same time slot. To do so, we rely on
Observation 3. Specifically, let (q, r ) correspond to an execution
time t = q Â· P + r. Then, we need to limit the number of distinct
values q of match operations that belong to the same class (i.e., same
r value). To do so, we introduce an indicator pm(q, r ) for match
operations such that if at least one match operation takes place at
time t = qÂ· P +r then pm(q, r ) = 1. This can be expressed as follows:
(cid:88)
vâˆˆVM
rq(v, q, r ) â‰¤ pm(q, r ) Â· (cid:88)
(cid:88)
vâˆˆVM
pm(q, r ) â‰¤ IPC âˆ€r .
q
1 âˆ€q, r .
Finally, we can limit the number of different packets for which
matches are initiated at the same cycle:
Namely, we demand that the number of different q values of matches
that are executed in the same time slot is bounded by IPC without
limiting the number of concurrent matches that belong to the same
packet (i.e., same r value). The IPC action constraints are defined in
an identical manner.
Accelerating the ILP run-time. We have developed three tech-
niques to accelerate the ILP run-time. These techniques allow us
to find a feasible solution to the ILP, which can be used to seed
the ILP solver or quickly establish feasibility in the binary-search
procedure. In the interest of space, we leave a detailed description
of these techniques to the longer version of this paper [5].
4 EVALUATION
We use two metrics to compare RMT with dRMT on packet-
processing programs: (1) the minimum number of processors re-
quired to sustain line rate (i.e., one packet per clock cycle), and
(2) the minimum number of threads required to sustain one packet
per clock cycle. A separate thread exists for each packet currently
SIGCOMM â€™17, August 21-25, 2017, Los Angeles, CA, USA
S. Chole et al.
Parameter
Match capacity ( Â¯M)
Match unit size (b)
Action capacity ( Â¯A)
Match latency (âˆ†M)
Action latency (âˆ†A)
Inter-packet concurrency (IPC)
Memory disaggregation
dRMT
8
80 bits
32
22
2
1 or 2
Yes
Table 1: Parameters for RMT and dRMT.
RMT
8
80 bits
224
18
2
1
Yes
residing at a dRMT processor, for which some state (e.g., the packet
header vector) needs to be maintained. For a fixed throughput of
one packet per cycle, the number of threads across all processors is
exactly the same as the latency of the program.
First, we compare the two architectures on four real P4 programs,
three derived from an open-source program, switch.p4 [13], and
one proprietary program. Second, because of the paucity of real
P4 programs, we compare the two architectures on 100 randomly-
generated operation dependency graphs (ODGs). Third, we illustrate
how throughput degrades on each architecture as we decrease the
number of processors, showing that dRMT does not have a perfor-
mance cliff, unlike RMT. Fourth, we conclude by reporting on the
run-times of the dRMT ILP.
4.1 Experimental setup
We compare four architectures: RMT, fine-grained RMT, dRMT
with IPC = 1, and dRMT with IPC = 2. Fine-grained RMT allows
matches and actions within a single P4 table to be split and placed in
different RMT stages. It provides greater flexibility than RMT at the
cost of temporarily holding the action result in the packet headerâ€”a
cost we ignore for RMT. We also compare with a lower bound. The
lower bound captures the minimum number of processors needed to
support line rate, if the bottleneck resource (either match or action
capacity) is fully utilized. It also captures the minimum latency for
the program based on its critical path.
Numeric parameters. For both architectures, we assume the num-
ber of memory clusters equals the number of processors/stages. We
list other parameters in Table 1. We chose these parameters based
on the RMT paper and our estimates for match and action latency.
dRMTâ€™s match latency is higher due to the crossbar. dRMTâ€™s ac-
tion capacity is lower to ensure its chip area is competitive with
RMT (we expand on this in Â§5.1). We do not consider IPC = 2 for
RMT, as this would require sending two packet headers through the