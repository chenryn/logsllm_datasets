active and inactive circuits, respectively, in an average 10 minute
period, which again may be caused by idle users.
3.2 Exit Statistics
Unlike entry relays, exit relays are unable to observe clients but can
observe streams. Therefore, exits have access to traffic meta-data
such as the port to which a stream connects. PrivCount classifies
streams to port 80 or 443 as web, streams to common file sharing
ports as file sharing (see Footnote 8), and streams to unclassified
ports as other. We initially measure traffic from all of these classes
to better understand their significance. (We ignore PrivCount’s
interactive class, including SSH and IRC, as Jansen and Johnson
observed an insignificant amount of interactive traffic [26].)
3.2.1 Traffic Totals. Results for our exit circuit statistics are
shown in Table 4. These results show that 52.8 percent of circuits
are active while 46.1 percent are inactive, which is comparable to
our entry circuit statistics. We do observe about 10 times fewer
circuits overall on our exits than on our entries (61,340 exit circuits
compared to 679,300 entry circuits), which could be attributed to
internal Tor network circuits (e.g., directory, bandwidth test, and
onion service circuits) that do not utilize exit relays, circuits that
fail before reaching the exit, or a bug in PrivCount. We observed
that 88.6 percent of the active circuits carry web traffic, whereas
only 2.68 percent carry traffic to well-known file sharing ports and
15 percent carry traffic on other ports.
Results for our exit stream statistics are shown in Table 5. Again,
we observed that a large majority of streams (90.4 percent) carry
Figure 2: The 10 minute mean number of circuits per client
collected during period 2 (see Table 2). The 10 minute mean
number of unique clients was 13,800 ± 153 (1.11%) with 95%
confidence. See Table 7 in Appendix A for a table of values.
entry perspective, PrivCount classifies a circuit as active if eight
or more cells were sent on it (seven cells are used to construct the
circuit), and otherwise classifies it as inactive; similarly, a client is
counted as active if it has at least one active circuit, and inactive
otherwise. We see from Table 3 that our entry relays observed
14,010 unique clients per 10 minute period on average, 71.2 percent
of which are active while 24.2 percent are inactive. Our entry relays
observed 679,300 circuits during an average 10 minutes, 42.4 percent
of which are active and 57.6 percent of which are inactive. Although
initially surprising, we speculate that the large number of inactive
clients may be due to users who have Tor browser open but are not
using it. We believe that the large number of inactive circuits are
caused by Tor clients preemptively building circuits (part of Tor’s
design) that are never used.
Given that there is a significant amount of both active and inac-
tive circuits, we measured the distribution of each per client using
PrivCount histogram counters to better understand how clients
build circuits. The results are shown in Figure 2. For both types of
circuits, we observed that a single circuit per client is the most com-
mon, followed by two, three, or four circuits per client. Although
we observed that 12.7 and 9.86 percent of clients build 15 or more
active and inactive circuits, respectively, generally it is more likely
that clients build fewer circuits. These results indicate that most
[1,2)[2,5)[5,15)[15,∞)CircuitsperClient0102030PercentageofClients3016.76.5512.721.214.87.319.86TotalClients:13,800±153(1.11%)ActiveInactiveSession 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1948Table 6: The 10 minute mean exit byte statistics collected
during period 4 (see Table 2). The error associated with the
addition of noise is shown with 95% confidence.
Byte Stat.
Total
a
t
o
T
l
a
t
o
T
l Inbound
Outbound
Web
FileSharing
Other
Web
FileSharing
Other
Web
FileSharing
Other
d
n
b
t
u
O
d
n
b
n
I
.
.
Count (×220)
25,587 ± 2 (0.01%)
23,914 ± 2 (0.01%)
1,672 ± 0.1 (0.01%)
18,547 ± 1 (0.01%)
263 ± 0.01 (0.002%)
6,744 ± 0.5 (0.01%)
17,569 ± 1 (0.01%)
212 ± 0.01 (0.002%)
6,106 ± 0.4 (0.01%)
997 ± 0.03 (0.003%)
51 ± 0.001 (0.003%)
637 ± 0.06 (0.01%)
% of Total
—
93.5% ± 0.009%
6.53% ± 0.001%
72.5% ± 0.007%
1.03% ± 0.001%
26.4% ± 0.003%
73.5% ± 0.007%
0.89% ± 0.001%
25.5% ± 0.003%
58.5% ± 0.003%
3.08% ± 0.001%
38.1% ± 0.004%
Figure 3: The distributions of streams per active circuit col-
lected during periods 5, 6, and 7 (see Table 2). The total num-
ber of circuits of each type is shown in the legend with 95%
confidence. See Table 8 in Appendix B for table of values.
traffic to web ports, while an insignificant number of streams (0.52
percent) carry traffic to known file sharing ports and a small number
of streams (7.56 percent) carry other traffic.
Results for our exit byte statistics are shown in Table 6. Over
the 24 hour measurement period, the relays in our PrivCount de-
ployment transferred 25 GiB of exit stream data in total. Of the
traffic, 93.5 percent was inbound, i.e., forwarded toward the circuit
initiator; the remaining 6.53 percent was outbound, i.e., forwarded
toward the Tor-external service. Of the inbound traffic, 73.5 percent
was for web ports 80 and 443, while 0.89 percent was for default
file sharing ports and 25.5 percent was for other ports. The share of
traffic that is web-related fell to just 58.5 percent of outbound traffic,
while traffic on file sharing ports rose to 3.08 percent and traffic on
other ports rose to 38.1 percent.
We conclude from our exit measurements that an unsurprisingly
large majority of circuits (88.6 percent), streams (90.4 percent), and
bytes (72.5 percent) are associated with web ports 80 and 443, while
most of the remaining traffic is associated with other ports. Past
work has found that bytes on Tor corresponding to unencrypted Bit-
Torrent were as high as 40.2 percent of observed traffic in 2008 [39]
and 24.9 percent in 2010 [8]. Additionally, more recent work found
some evidence that allowing more default file sharing ports in the
relay’s exit policy reduced the fraction of web traffic and increased
the fraction of non-web traffic [26]. However, based on our ob-
servations, we conclude that traffic to default file sharing ports
is relatively insignificant. For this reason, we focus on web and
other traffic in the remainder of our exit measurements. Past work
suggests that much of the traffic that we observed on other ports
may actually be associated with BitTorrent or other file sharing,
which would be consistent with the behavior of file sharing clients
that choose random ports upon start-up. We don’t explore this
possibility further as it is out of the scope of this paper.
3.2.2 Traffic Distributions. To get a better understanding of how
clients build streams and transfer bytes, we now explore distribu-
tions of streams-per-circuit and bytes-per-stream using PrivCount
histogram counters. We focus on web and other traffic in this section
to ensure that we capture the most significant traffic for modeling
purposes, while ignoring traffic on default file sharing ports since
it is a relatively minor contributor to Tor traffic.
The distribution of the number of streams per active circuit is
shown in Figure 3. We observed that about 55 to 59 percent of active
circuits carry only one or two streams and about 21 to 25 percent of
circuits carry 3 to 6 streams. Another 9 to 11 percent of circuits carry
7-14 streams, and about 4-11 percent carry 15 or more streams. We
were somewhat surprised that there are so few streams per circuit,
given that 30 percent of popular website front pages result in 11 to
20 TCP connections and more than 45 percent of front pages result
in more than 20 TCP connections according to httparchive.org. This
suggests that Tor Browser users may experience the web differently
than non-Tor users.
The distribution of the number of bytes per stream is shown
in Figure 4, with inbound bytes shown in Figure 4a and outbound
bytes shown in Figure 4b. We observed that about 70 to 80 percent
of streams receive less than 16 KiB inbound, while about 75 to 85
percent of streams send less than 1 KiB outbound. We also observed
a significant difference between web and other traffic: it is about 15
and 30 percent more likely that other streams will carry less than
2 KiB inbound and less than 512 bytes outbound, respectively, com-
pared to web streams. We also observed that the highest percentage
of streams sending more than 4 KiB outbound is 6.84 percent of other
streams; however, the distribution of bytes per stream generally
skews higher for web streams than for other streams.
4 LEARNING TRAFFIC MODELS
To model the behavior of Tor clients without explicit reference to
application protocols, we model both the creation of streams and
the traffic on a stream (i.e., packets) using hidden Markov models
and an iterative PrivCount measurement process.
4.1 Hidden Markov Modeling
Formally speaking, a hidden Markov model (HMM) is a discrete
stochastic process that maintains a state s among a state space S.
At each step i in the process, an observation o is drawn from some
[1,3)[3,7)[7,15)[15,31)[31,63)[63,∞)StreamsperCircuit0204060PercentageofCircuits54.82310.85.513.822.5657.120.68.684.384.170.1358.824.610.330.3790.472All,Σ:36859±589(1.6%)Web,Σ:29525±686(2.32%)Other,Σ:6552±49(0.755%)Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1949(a) Inbound bytes (forwarded toward the circuit initiator) per stream.
(b) Outbound bytes (forwarded away from the circuit initiator) per stream.
Figure 4: The distributions of bytes per stream collected during periods 5, 6, and 7 (see Table 2). The total number of streams
of each type is shown in the legend and with 95% confidence. See also Tables 9 and 10 in Appendix B for full tables of values.
some observation space O according to the probability distribution
Θ(o|s) conditioned (only) on the current state si, and a next state is
drawn from S according to the transition distribution P(s′|si). The
first state s0 is chosen according to the initial distribution π(s). In
our case, the states can be thought of as modeling the underlying
state of the application, and the observations correspond to the
arrival of streams, or packets on a stream. Since application state is
opaque and traffic can be encrypted, the only properties that a relay
can reliably observe are the direction (client-bound or server-bound
packet) or command (open or close stream) and the time elapsed
since the previous event.
Given the parameters of a stream arrival HMM, we can sample
a circuit by choosing s0 ← πstr eam(s), then repeatedly choosing
oi ← Θstr eam(o|si) (a delay between stream arrivals) and si +1 ←
Pstr eam(s′|si) (a next application state) until a “close circuit” event
occurs. Likewise, we can sample an application stream by choosing
s0 ← πpacket(s), then repeatedly choosing oi ← Θpacket(o|si),
and si +1 ← Ppacket(s′|si) until a “close” event occurs. So produc-
ing an empirical model of Tor streams requires training a hidden
Markov model to learn the parameters that best approximate real
Tor streams. In general, there are several approaches to training
HMMs from a set of sequences sampled from some HMM; we
modified the Expectation-Maximization (EM) approach, in which
an initial estimate of the parameters is used to find the sequence
of states most likely to yield a sequence of observations (accom-
plished with the Viterbi Algorithm [15]). These states are then
used to update the parameters of the HMM: each state observation
distribution’s parameters are chosen to maximize the probability
of the observations assigned to the state, and each state’s transi-
tion probabilities are chosen according to the observed frequencies.
This process is then repeated, with successive parameter estimates
yielding improved models of the underlying process.
To adapt the algorithm described above to the case of privately
measuring Tor traffic, we slightly modified both the process of
producing an initial estimate and the iteration process, as explained
in the following subsections.
4.2 Bootstrapping the HMM
Training an HMM to model a process requires making some ini-
tial assumptions about the form of the model, e.g. the number of
states, the connectivity between states, and the form of observation
distributions. To explore reasonable choices for these assumptions,
we performed several experiments using real network trace data
from AS 2500, a research ISP that advertises 10 IPv4 prefixes with
a total of 330,240 addresses and transfers data for real users from
several academic institutions and research organizations. Because
AS 2500 serves real users, the network traces that are captured
are anonymized before being released as part of a network mea-
surement project [50, 52]. The 24-hour anonymized network trace
that we used included 147,962 complete TCP sessions, which we
converted to “packet traces” by ignoring empty (acknowledgement-
only) and duplicated packets, and treating all data-carrying packets
as observations. Acknowledgements and duplicate packets were fil-
tered because we are modeling application level behavior; the TCP
stack and network emulation in Shadow will naturally (re-)produce
these packets given an application stream.
4.2.1
Stream Arrivals. To obtain an initial model of stream ar-
rivals, we hypothesized that hosts would follow a common two-
state model in which one state originates streams according to an
exponential distribution (representing, e.g., the sequential open-
ing of several resources within a web page) and the other state
originates streams according to a heavy-tailed distribution (encap-
sulating both the delay after requesting an index page and the “think
time” between user page loads). Using a log-normal distribution to
fit the heavy-tailed delays and adding a third state representing the
closing of a circuit, we found this model a good fit for the network
trace where the probability of transitioning between non-closed
states was roughly 25%, the exponential process had a rate of ap-
proximately 10/second, and the log-normal process had a median
delay of approximately 3 seconds.
4.2.2 Traffic. To determine the form of inter-packet delay distri-
butions, we fit networks to a small subsample of these flows (1000)
(0,2)[2,16)[16,65)[65,128)[128,∞)InboundBytesperStream(KiB)01020304050PercentageofStreams42.130.710.73.696.2439.639.511.92.199.1454.212.22.830.6323.69All,Σ:369755±6405(1.73%)Web,Σ:251352±5807(2.31%)Other,Σ:29373±207(0.704%)(0,0.5)[0.5,1)[1,2)[2,4)[4,∞)OutboundBytesperStream(KiB)0204060PercentageofStreams4730.113.93.92.9638.737.614.85.45.8466.117.75.781.46.84All,Σ:369755±6405(1.73%)Web,Σ:251352±5807(2.31%)Other,Σ:29373±207(0.704%)Session 10A: TORCCS’18, October 15-19, 2018, Toronto, ON, Canada1950Figure 5: Overview of HMM measurement and update process using PrivCount and exemplified with our stream model.
using several different distributions. Through trial and error we
found that log-normal distributions could reasonably model the
delays in our trace.
To determine the architecture of the HMM (number of states and