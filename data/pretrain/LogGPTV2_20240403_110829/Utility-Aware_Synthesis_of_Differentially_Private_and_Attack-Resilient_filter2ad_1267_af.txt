example, observe that SGLT’s AvRE is lowest on Geolife, however,
AdaTrace performs better than SGLT in terms of Kendall-tau. This
demonstrates that there can be benefit in using multiple evaluation
metrics for measuring one type of trajectory utility.
Figure 6: Visual density distributions of synthetic databases generated by various generators, using Brinkhoff as their Dr eal .
AdaTrace which might seem high, but we observed that this is
caused mostly by FPs with highest support not having as high
support in synthetic data. That is, even though an actually frequent
FP is correctly identified as frequent in synthetic data, since its
frequency is not as high, considerable support error is incurred.
This explains the competitive F1 scores despite high error in AvRE.
In terms of spatio-temporal travel metrics, we first note that Ada-
Trace is significantly superior in terms of trip error. Second, we
notice that although AdaTrace’s length errors are similar to its com-
petitors, its diameter errors are much smaller. This is surprising,
given that the two metrics are clearly related. To better understand
this behavior, we individually studied some trajectories from each
generator. We observed that DPT and ngram are biased towards
generating trajectories with low displacement. For example, their
trajectories contain loops and U-turns, which means that although
they travel long distances, their total displacement is not large. In
contrast, AdaTrace’s synthesis algorithm prevents unrealistic loops
and U-turns by guiding a trajectory’s random walk with its final
destination. That is, at each step of the random walk AdaTrace
considers the final destination that the trajectory will eventually
reach and how many steps are left to reach there. Hence, the re-
sulting trajectory becomes goal-driven, which realistically captures
In terms of frequent travel patterns, there is correlation between
FP AvRE and F1 similarity metrics, but also some discrepancies.
AvRE indicates that there is 30-40% error in FP support when using
Table 3: Execution time measurements
Geolife
Taxi
Brinkhoff
ngram
7 mins
12 mins
18 mins
DPT
1.9 mins
2.3 mins
3.5 mins
SGLT
11 hrs
2.4 days
6.9 days
AdaTrace
0.7 mins
1.7 mins
2.2 mins
the mobility of human beings since humans often travel with a
destination in mind.
Finally, we comment on the efficiency of each approach. We
performed our experiments on a high-end laptop with Intel i7 CPU,
but since all generators are single-threaded, they utilized a single
2.8 GHz CPU. Also, 16 GB main memory was sufficient for the com-
putation performed by all generators. In terms of execution time,
we obtained the results given in Table 3. Overall, AdaTrace is very
efficient – it is able to process and generate 50,000 trajectories in
slightly longer than 2 minutes. ngram and DPT are also reasonably
efficient since they finish in several minutes, but not as efficient
as AdaTrace. On the other hand, SGLT is significantly slower, as
it takes several days to produce the same number of trajectories
AdaTrace can produce in 2 minutes. This greatly hinders its use on
commodity hardware or mobile devices. Note that this observation
is consistent with the authors’ claims in their paper [7], as they
also report up to 2 minutes per generated trajectory. This has not
caused problems in their experiments since their datasets consisted
of < 100 users, but our datasets were several orders of magnitude
larger (15-50k trajectories), surfacing an inefficiency problem.
6.4 Impact of Attack Resilience on Utility
In the previous section we conducted experiments with varying
differential privacy parameter ε and observed its impact on utility.
In this section we analyze the impact of attack resilience parameters
on various forms of utility.
Defense 1: Bayesian Inference. Our defense asserts that the dif-
ference between priors and posteriors must be ≤ ϑ. When ϑ = 0 the
defense is strictest, and it is relaxed as ϑ increases. In Figure 7a, we
initially set ϑ = 0, increase it gradually, and report the percentage
improvement (decrease) in Query AvRE and FP AvRE. We only
report these metrics since the remaining metrics are non-linear,
involving the likes of F1 or JSD calculation, hence percentages
are not meaningful. The results show the expected trait that both
spatial densities (implied by Query AvRE) and frequent patterns
(implied by FP AvRE) are positively impacted when ϑ is relaxed. We
observed that this is mostly caused by queries and FPs involving the
sensitive zone Z, which is central to Defense 1. When the privacy
setting is stricter, trajectories visiting Z are more random, hence
FPs including Z will not be preserved.
An implicit parameter here is the choice (size and density) of
Z. In the extreme case where Z = Ω(Dr eal), Defense 1 dictates
that the adversary should gain no knowledge that adds to his prior
belief. In this case the trajectories in Dsyn will be most random,
and their features will converge to population averages rather than
capturing the utility in Dr eal . To experimentally confirm this, in
Appendix D we perform an experiment in which we vary the size
of Z (see Figure 9a). Results indicate that there is indeed a positive
correlation between error amounts and the size of Z.
Defense 2: Partial Sniffing. The defense is based on two parame-
ters: φ that bounds the intersection between a sniffed actual trajec-
tory and its counterpart synthetic trajectory, and ϱ that limits the
possibility that the counterpart visits a sensitive zone. For both pa-
rameters, lower values imply stricter privacy. We set a fixed sniffing
region that is visited by 4% of the actual trajectories, and assume
all trajectories in this region have been sniffed by the adversary.
We set ϱ = 0 which is the strictest privacy setting, and vary φ to ob-
serve its utility impact. In Figure 7b and 9b we illustrate the utility
impact wrt 3 metrics. Although one could expect utility to decrease
as privacy becomes stricter, this does not appear to be the case –
we observe that aggregate utility stays constant despite changes
in φ. Upon further analysis with more data points and remaining
utility metrics, we confirm that the impact of φ on overall utility is
negligible. This is because our utility metrics are aggregate metrics,
taking whole Dr eal and Dsyn into consideration rather than com-
paring individual trajectories one by one. This is also the case in
many machine learning tasks and real-life uses of trajectory data.
Thus, although an actual trajectory and its synthetic counterpart
do not closely match, utility can be re-injected by implicitly adding
the mismatched part into a different trajectory that is not sniffed.
Defense 3: Outlier Leakage. The two parameters in this defense
are β and κ. κ dictates that a synthetic outlier must blend into a
crowd of κ actual trajectories. β dictates the uniformity of the crowd:
When β = 0 all trajectories in the crowd must appear exactly the
same (in terms of the trajectory feature considered), whereas a large
β allows the crowd to have higher non-uniformity. The privacy
requirement becomes stricter when β decreases and κ increases.
We first analyze the behavior of Defense 3 wrt parameters β
and κ in Figure 7c. Here, we set β normalized after observing the
maximum possible distance in the database, e.g., if the max distance
is 100, β = 0.1 implies that the allowed distance in forming the
crowd is 10. When the privacy requirement is stricter, we indeed
observe that there are more outlier trajectories failing the defense,
with a notable increase when κ becomes ≥ 25. We attribute this
to the curse of dimensionality: Crowd-blending notions of privacy,
such as k-anonymity and our defense, rely on the ability to find a
crowd comprised of similar records. While this is easier for low-
dimensional data, trajectory data is inherently high-dimensional,
and typically no two trajectories are alike. Hence, forming a similar
crowd is difficult, and while higher κ values can be suitable for
low-dimensional databases, the same κ may yield an excess number
of outliers on a high-dimensional or trajectory database.
Having said that, we are interested in measuring the utility im-
pact of arbitrary (β,κ) pairs, and wish that our system preserves
utility even in the strictest cases. We therefore design the exper-
iment in Figure 7d. Instead of trying to isolate β and κ, we take
a holistic approach and base our utility analysis on the number
of outliers failing the defense. Note that outliers failing the de-
fense cannot be released, and must be perturbed or regenerated.
We observe from the experiment results that this perturbation or
regeneration process has little to no impact on aggregate trajectory
utility. Despite different privacy settings, Query AvRE, Trip Error
and Length Error do not seem to be impacted. Hence, we can con-
clude that we are able to protect against outlier leakage with no
additional utility cost. In addition to the visual results in Figure 7d,
(a)
(b)
(c)
(d)
REFERENCES
[1] 2017. NYC Taxi & Limousine Commission – Trip Record Data. http://www.nyc.
Figure 7: (a) Impact of ϑ on aggregate trajectory utility. (b) Impact of φ on aggregate trajectory utility. (c) Analyzing the outlier
behavior with respect to parameters β, κ. (d) Utility impact of executing Defense 3. All experiments are performed on Taxi.
we computed the correlation between the privacy settings and the
error amounts. Results indicate weak correlations at most (+0.2),
but are often closer to 0 and sometimes even negative.
Summary of Findings. Under reasonable privacy parameters, De-
fenses 2 and 3 can be implemented with little or no observable
impact on aggregate trajectory utility, because they are concerned
with individual trajectories that often constitute a small portion of
the whole database. The impact of small perturbations on individual
trajectories are either negligible on aggregate utility, or they can
be counterbalanced by injecting the lost utility in the remaining
trajectories. On the other hand, Defense 1 is directly connected
to the aggregate information that is obtained from Dsyn, hence
a stricter privacy parameter ϑ or an increased geographic size of
sensitive zone Z negatively impacts aggregate trajectory utility.
[4] Gergely Acs and Claude Castelluccia. 2014. A case study: Privacy preserving
release of spatio-temporal density in paris. In Proceedings of the 20th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining. ACM, 1679–
1688.
[5] Miguel E Andrés, Nicolás E Bordenabe, Konstantinos Chatzikokolakis, and Catus-
cia Palamidessi. 2013. Geo-indistinguishability: Differential privacy for location-
based systems. In Proceedings of the 2013 ACM SIGSAC Conference on Computer
and Communications Security. ACM, 901–914.
[3] 2018. Uber Movement: Let’s find smarter ways forward. movement.uber.com.
[2] 2018.
with
analyzing-1-1-billion-nyc-taxi-and-uber-trips-with-a-vengeance/.
cessed: 2018-05-03.
and Uber Trips,
http://toddwschneider.com/posts/
Ac-
Analyzing
Vengeance.
Billion NYC Taxi
[6] Bhuvan Bamba, Ling Liu, Peter Pesti, and Ting Wang. 2008. Supporting anony-
mous location queries in mobile environments with privacygrid. In Proceedings
of the 17th international conference on World Wide Web. ACM, 237–246.
gov/html/tlc/html/about/trip_record_data.shtml. Accessed: 2018-05-03.
Accessed: 2018-05-03.
1.1
a
7 CONCLUSION
We presented AdaTrace, a utility-aware location trace synthesizer
with differential privacy guarantee and attack resilience. AdaTrace
performs feature extraction, learning, and noise injection using a
database of real location traces. It then generates synthetic traces
while preserving differential privacy, enforcing resilience to in-
ference attacks, and upholding statistical and spatial utility. Our
experiments on real and simulated datasets show that AdaTrace is
highly effective. Compared to ngram, DPT, and SGLT [7, 11, 26],
AdaTrace provides up to 3-fold improvement in trace utility. At
the same time, AdaTrace is computationally more efficient, and on
average 1.5x faster than DPT, 8x faster than ngram, and 1000x faster
than SGLT. One of our ongoing research directions is to investigate
the feasibility of extending AdaTrace to handle incremental updates,
to make it compatible with growing and streaming location trace
datasets.
ACKNOWLEDGMENT
This research was partially sponsored by NSF under grants SaTC
1564097 and an RCN BD Fellowship, provided by the Research
Coordination Network (RCN) on Big Data and Smart Cities and
an IBM Faculty Award. Any opinions, findings, and conclusions
or recommendations expressed in this material are those of the
author(s) and do not necessarily reflect the views of the funding
agencies and companies mentioned above.
[7] Vincent Bindschaedler and Reza Shokri. 2016. Synthesizing plausible privacy-
preserving location traces. In 2016 IEEE Symposium on Security and Privacy (S&P).
IEEE, 546–563.
[8] Nicolás E Bordenabe, Konstantinos Chatzikokolakis, and Catuscia Palamidessi.
2014. Optimal geo-indistinguishable mechanisms for location privacy. In Pro-
ceedings of the 2014 ACM SIGSAC Conference on Computer and Communications
Security. ACM, 251–262.
[9] Thomas Brinkhoff. 2002. A framework for generating network-based moving
objects. GeoInformatica 6, 2 (2002), 153–180.
[10] Konstantinos Chatzikokolakis, Catuscia Palamidessi, and Marco Stronati. 2014. A
predictive differentially-private mechanism for mobility traces. In International
Symposium on Privacy Enhancing Technologies. Springer, 21–41.
[11] Rui Chen, Gergely Acs, and Claude Castelluccia. 2012. Differentially private
sequential data publication via variable-length n-grams. In Proceedings of the
2012 ACM Conference on Computer and Communications Security. ACM, 638–649.
[12] Rui Chen, Benjamin CM Fung, S Yu Philip, and Bipin C Desai. 2014. Correlated
network data publication via differential privacy. The VLDB Journal 23, 4 (2014),
653–676.
[13] Chris Clifton and Tamir Tassa. 2013. On syntactic anonymity and differential
privacy. Transactions on Data Privacy 6, 2 (2013), 161–183.
[14] Graham Cormode. 2011. Personal privacy vs population privacy: learning to
attack anonymization. In Proceedings of the 17th ACM SIGKDD International
Conference on Knowledge Discovery and Data Mining. ACM, 1253–1261.
[15] Graham Cormode, Cecilia Procopiuc, Divesh Srivastava, Entong Shen, and Ting
Yu. 2012. Differentially private spatial decompositions. In 2012 IEEE 28th Interna-
tional Conference on Data Engineering (ICDE). IEEE, 20–31.
[16] Wei-Yen Day and Ninghui Li. 2015. Differentially private publishing of high-
dimensional data using sensitivity control. In Proceedings of the 10th ACM Sym-
posium on Information, Computer and Communications Security (AsiaCCS). ACM,
451–462.
[17] Yves-Alexandre De Montjoye, César A Hidalgo, Michel Verleysen, and Vincent D
Blondel. 2013. Unique in the crowd: The privacy bounds of human mobility.
Scientific Reports 3 (2013), 1376.
[18] Cynthia Dwork. 2006. Differential Privacy. In International Colloquium on Au-
tomata, Languages, and Programming. Springer, 1–12.
[19] Cynthia Dwork. 2008. Differential privacy: A survey of results. In International
Conference on Theory and Applications of Models of Computation. Springer, 1–19.
[20] Cynthia Dwork, Aaron Roth, et al. 2014. The algorithmic foundations of differ-
ential privacy. Foundations and Trends® in Theoretical Computer Science 9, 3–4
(2014), 211–407.
[21] Matthew Fredrikson, Eric Lantz, Somesh Jha, Simon Lin, David Page, and Thomas
Ristenpart. 2014. Privacy in Pharmacogenetics: An End-to-End Case Study of
Personalized Warfarin Dosing. In USENIX Security Symposium. 17–32.
[22] Marco Gaboardi, Hyun-Woo Lim, Ryan M Rogers, and Salil P Vadhan. 2016.
Differentially Private Chi-Squared Hypothesis Testing: Goodness of Fit and
Independence Testing. In ICML. 2111–2120.
[23] Bugra Gedik and Ling Liu. 2005. Location privacy in mobile systems: A personal-
ized anonymization model. In 25th IEEE International Conference on Distributed
Computing Systems (ICDCS 2005). IEEE, 620–629.
[24] Arthur Gervais, Reza Shokri, Adish Singla, Srdjan Capkun, and Vincent Lenders.
2014. Quantifying web-search privacy. In Proceedings of the 2014 ACM SIGSAC
Conference on Computer and Communications Security. ACM, 966–977.
[25] Michael Hay, Vibhor Rastogi, Gerome Miklau, and Dan Suciu. 2010. Boosting the
accuracy of differentially private histograms through consistency. Proceedings of
the VLDB Endowment 3, 1-2 (2010), 1021–1032.
[26] Xi He, Graham Cormode, Ashwin Machanavajjhala, Cecilia M Procopiuc, and
Divesh Srivastava. 2015. DPT: Differentially private trajectory synthesis using
hierarchical reference systems. Proceedings of the VLDB Endowment 8, 11 (2015),
1154–1165.
[27] Ari Juels and Ronald L Rivest. 2013. Honeywords: Making password-cracking
detectable. In Proceedings of the 2013 ACM SIGSAC Conference on Computer and
Communications Security. ACM, 145–160.
[28] Shiva P Kasiviswanathan and Adam Smith. 2014. On the’semantics’ of differential
privacy: A bayesian formulation. Journal of Privacy and Confidentiality 6, 1 (2014),
1.