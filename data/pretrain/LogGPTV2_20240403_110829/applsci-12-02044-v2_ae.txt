200
0 | Decompression | Decompression | Decompression | Lookup | PPMd on proposed | PPMd on proposed | PPMd on proposed | PPMd on proposed |
|---|---|---|---|---|---|---|---|---|
| 600 400 200 0 |  |  |  |  |  |  |  |  |
| 600 400 200 0 |Proposed |Bzip2 |LZMA |PPMd |PPMd |Proposed |General |Joint |Figure 41. The times needed to recover all instances of the 10 template in case of the joint compression.
Log entry retrieval speeds in case of joint use and 100 
templates
Decompression Lookup PPMd on proposed
8000
6000
4000
2000
0
Proposed Bzip2 LZMA PPMd Proposed General Joint
Figure 42. The times needed to recover all instances of the 100 template in case of the joint compression.It can be seen that the decompression time increases with the time taken by the PPMd to decompress the dictionaries and the compressed file generated by our algorithm, but it can still quickly recover the entries corresponding to the randomly selected templates. There is no difference in the space required to lookup the entries.
5.2.8. Experiment 8: The Comparison of the Compression Rates Achieved by the Proposed Algorithm and LogzipIt is also important to compare the compression rates of the proposed method and other algorithms that use the same approach. Like our algorithm, Logzip [19] also utilizes hidden structures (templates) to reduce the size of a file. It also uses the general compressor Bzip2 to further decrease the size. In this experiment, we compare the achieved compression rates in the case of the previously mentioned datasets, A, B, and C. The results can be seen in Figure 43.Appl. Sci. 2022, 12, 2044 27 of 32
Figure 43. The compression rates achieved by our method and Logzip on datasets A B and C.
Our method has a compression rate that is approximately 1% higher than Logzip’s. In contrast with Logzip, our algorithm does not incorporate the use of general algorithms.
With the joint use, our method could achieve higher rates as explained in Section 5.2.5.5.2.9. Experiment 9: Investigating the Memory Usage of the Proposed Algorithm and Logzip
Memory usage is a significant aspect of a compressor, so we investigated the average and maximum memory usages of the proposed algorithm and Logzip [19]. We also measured the duration of time that the compressors used the memory for. Dataset A was used to conduct the experiment. The computer which was used to perform the measurements had 16 GB of DDR4 RAM. The results are shown in Figure 44.Memory usage and elapsed time
Average memory (MB) Max memory (MB) Elapsed time (s)
15,000.00
10,000.00
5,000.00
0.00 
	Proposed 	Logzip
Figure 44. The memory usage of the proposed method and Logzip.It can be seen that our algorithm uses 44% less memory on average, and the maximum memory used is 2.9 times less than in the case of Logzip. This could be explained by the loading method of the messages. While our algorithm reads lines after each other (similar to when messages come in a stream), Logzip loads the whole file into a dataframe that is located in the memory. Furthermore, Logzip consumes the memory for four times as long as the proposed algorithm. It can be noticed that the memory usage scales with the size of the input. The available memory has to be at least 2.2 times the input size.Appl. Sci. 2022, 12, 2044 28 of 32
5.2.10. Experiment 10: Generating Log Messages with Different Distributions and Evaluating the Compression RatesIn the final experiment, we investigated whether our enhanced algorithm had high compression rates even in the case of distributions other than the power law. We created four datasets, which were different in size and distribution. While generating the “random”datasets, each template had a 5% probability to be created by the sampling algorithm. We created a file that consisted of 1 million entries and a file that consisted of 50 million entries based on this principle. The generation of the other two datasets was similar, except that, in this case, normal distribution was used instead of equal 5% probabilities. The distribution of the templates in the case of the generated files can be seen in Figures 45–48.Figure 45. The template distribution of the 1 million randomly generated messages dataset.
Figure 46. The template distribution of the 50 million randomly generated messages dataset.
Appl. Sci. 2022, 12, 2044 29 of 32
Figure 47. The template distribution of the 1 million messages dataset generated based on normal distribution.Figure 48. The template distribution of the 50 million messages dataset generated based on normal distribution.
	After the creation of the custom datasets, we measured the compression rates of our enhanced algorithm. The results are shown in Figure 49.
It can be seen that our algorithm is capable of achieving high compression rates regardless of the distribution of the templates. In the case of all the datasets, at least a 94% compression rate was achieved, which indicates the compressing capacity of our algorithm.Appl. Sci. 2022, 12, 2044 30 of 32
Compression rates on the generated datasets
100
75
| Compression rate (%) | 50 | Random 1 million Random 50 million Normal 1 million | Normal 50 million |
|---|---|---|---|
| Compression rate (%) |25 |Random 1 million Random 50 million Normal 1 million |Normal 50 million |
| Compression rate (%) |0 |Random 1 million Random 50 million Normal 1 million |Normal 50 million |Dataset
Figure 49. The compression rates achieved by our method on the generated datasets.
6. Discussion and ConclusionsIn this paper, we evaluated the compression capacity of an enhanced version of the algorithm that we proposed in [21]. The original algorithm uses template miners to identify the templates. Based on the templates, a dictionary is created where each ID represents an event type. The log lines are then represented using the corresponding ID and parameter list. Using this approach, we were able to achieve around 67% compression rates. To improve this performance, we introduced several enhancements to the algorithm in this paper. First, the templates were ordered based on the number of their occurrences. Smaller IDs were assigned to the more frequent templates. As a second step, we created a dictionary for the templates in a similar manner. This resulted in encoded log messages that only contained numbers. Finally, Huffman coding was used to further compress the file.To analyze the performance of the enhanced algorithm, we conducted several experi-ments. The experimental results showed that each enhancement improved the compression capacity. The joint use of the parameter dictionary and Huffman coding achieved an av-erage of 92% compression rate, which is 25% more than the original algorithm. In terms of speed, we consider our algorithm to be fast, since it only takes seconds to compress and decompress the investigated log files. We also compared our algorithm with gen-eral compressors. While general compressors are faster and achieve better compression rates, around 98%, they are not well suited for statistical applications. With the use of our algorithm, statistical questions such as ‘What is the distribution of the templates?’or ‘What is the frequency of the different parameters of a message type?’ can easily be answered. The instances of given templates can also be found faster than in the case of general compressors.Based on our experiment we would suggest the joint use of our algorithm and general compressors, since it improves the compression rates and functions as a wrapper for the created templates and the encoded file.We only evaluated the performance on static log files, it would be beneficial to measure the compression rate, speed, and memory usage in the case of stream-like data. It would be also interesting to compare the performance of our method with the performance of other general compressors. We also want to investigate the connection between the compression rate and the k-th order empirical entropy.Author Contributions: Conceptualization, P.M., P.L.-K. and A.K.; methodology, P.M., P.L.-K. and A.K.; software, P.M. and P.L.-K.; validation, P.M., P.L.-K. and A.K.; investigation, P.M., P.L.-K. and A.K.; writing–original draft preparation, P.M., P.L.-K. and A.K.; writing–review and editing, P.M., P.L.-K. and A.K.; supervision, A.K.; project administration, A.K. All authors have read and agreed to the published version of the manuscript.Appl. Sci. 2022, 12, 2044 31 of 32
Funding: The project has been supported by grants from the “Application Domain Specific Highly
Reliable IT Solutions” project that has been implemented with the support provided from the National
Research, Development, and Innovation Fund of Hungary, financed under the Thematic Excellence
Program TKP2020-NKA-06 (National Challenges Subprogram) funding scheme.Institutional Review Board Statement: Not applicable.
Informed Consent Statement: Not applicable.
Data Availability Statement: The data was provided by the Ericsson-ELTE Software Technology Lab.
Acknowledgments: This publication is the partial result of the Research and Development Oper-
ational Program for the project “Modernisation and Improvement of Technical Infrastructure forResearch and Development of J. Selye University in the Fields of Nanotechnology and Intelligent
Space”, ITMS 26210120042, co-funded by the European Regional Development Fund and supported
by the ÚNKP-21-3 New National Excellence Program of the Ministry for Innovation and Technology
from the source of the National Research, Development, and Innovation Fund. The project was alsosupported by the Ericsson-ELTE Software Technology Lab.
Conflicts of Interest: The authors declare no conflict of interest.
Abbreviations
The following abbreviations are used in this manuscript:
IPLoM 	Iterative Partitioning Log Mining
MoLFI 	Multi-objective Log message Format Identification
NSGA-II 	Non-dominated Sorting Genetic Algorithm II
BWT 	Burrows–Wheeler TransformationBWT 	Burrows–Wheeler Transformation
LZMA 	Lempel–Ziv–Markov-chain Algorithm
PPM 	Prediction by Partial Matching
Enh 	Enhanced version of our algorithm
Huff 	Huffman coding
WPE 	Without Parameter Encoding
References
1. 	Landauer, M.; Wurzenberger, M.; Skopik, F.; Settanni, G.; Filzmoser, P. Dynamic log file analysis: An unsupervised clusterevolution approach for anomaly detection. In Computers & Security; Elsevier: Amsterdam, The Netherlands, 2018; Volume 79,
pp. 94–116. []
2. 	Aivalis, C.; Blas, A.C. Log File Analysis of E-commerce Systems in Rich Internet Web 2.0 Applications. In Proceedings
of the PCI 2011—15th Panhellenic Conference on Informatics, Kastoria, Greece, 30 September–2 October 2011 ; Volume 10,
pp. 222–226. []pp. 222–226. []
3. 	Nagaraj, K.; .; Neville, J. Structured comparative analysis of systems logs to diagnose performance problems. In
Proceedings of the 9th USENIX Symposium on Networked Systems Design and Implementation, Jan Jose, CA, USA, 25–27 April
2012; pp. 353–366.
4. 	Logothetis, D.; Trezzo, C.; Webb, K.C.; Yocum, K. In-situ MapReduce for log processing. In Proceedings of the USENIX ATC,Portland, OR, USA, 14–15 June 2011; Volume 11, p. 115.
5. 	Li, H.; Shang, W.; Hassan, A.E. Which log level should developers choose for a new logging statement? In Empirical Software
Engineering; Springer: New York, NY, USA, 2017; Volume 22, pp. 1684–1716. []
6. 	Lin, H.; Zhou, J.; Yao, B.; Guo, M.; Li, J. Cowic: A column-wise independent cn for log stream analysis. In Proceedingsof the 2015 15th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, Shenzhen, China, 4–7 May 2015;
pp. 21–30. []
7. 	Yao, K.; Li,g, W.; Hassan, A.E. A study of the performance of general compressors on log files. In Empirical Software
Engineering; Springer: New York, NY, USA, 2020; Volume 25, pp. 3043–3085. []
8. 	Du, M.; Li, F. Spell: Streaming parsing of system event logs. In Proceedings 6 IEEE 16th International Conference onData Mining, Barcelona, Spain, 12–15 December 2016; pp. 859–864. []
9. 	Shima, K. Length matters: Clustering system log messages using lenrds. arXiv 2016, arXiv:1611.03213.
10. 	He, P.; Zhu, J.; Zheng, Z.; Lyu, M.R. Drain: An online log parsing approach with fixed depth tree. In Proceedings of the 2017 IEEE
International Conference on Web Services (ICWS), Honolulu, HI, USA, 25–30 June 2017; pp. 33–40. []11. 	Christensen, R.; Li, F. Adaptive log compression for massive log data. In Proceedings of the SIGMODce, New York, NY,
USA, 22–27 June 2013; pp. 1283–1284.
| Appl. Sci. 2022, 12, 2044 | Appl. Sci. 2022, 12, 2044 | 32 of 32 |
|---|---|---|
| 12. |Feng, B.; Wu, C.; Li, J. MLC: An efficient multi-level log compression method for cloud backup systems. In Proceedings of the |Feng, B.; Wu, C.; Li, J. MLC: An efficient multi-level log compression method for cloud backup systems. In Proceedings of the |2016 IEEE Trustcom/BigDataSE/ISPA, Tianjin, China, 23–26 August 2016; pp. 1358–1365. [] 
13. 	Mell, P.; Harang, R.E. Lightweight packing of log files for improved compression in mobileetworks. In Proceedings of 	the 2014 IEEE Military Communications Conference, Baltimore, MD, USA, 6–8 October 2014; pp. 192–197. [] 14. 	Grabowski, S.; Deorowicz, S. Web log compression. Automatyka/Akademia Górniczo-Hutnicza im. Stanisławaw Krakowie 	2007, 11, 417–424.Lloyd, T.; Barton, K.; Tiotto, E.; Amaral, J.N. Run-length base-delta encoding for high-speed compression. In Proceedings of the 15.
	47th International Conference on Parallel Processing Companion, Eugene, OR, USA, 13–16 August 2018; pp. 1–9. [] 16. 	Tan, H.; Zhang, Z.; Zou, X.; Liao, Q.; Xia, W. Exploring the Potential of Fast Delta Encoding: Marching to a Higher ion 	Ratio. In Proceedings of the 2020 IEEE International Conference on Cluster Computing (CLUSTER), Kobe, Japan, 14–17 September 	2020; pp. 198–208. []17. 	Skibi´nski, P.; Swac and efficient log file compression. In Proceedings of the CEUR Workshop, 11th East-European 	Conference on Advances in Databases and Information Systems, Varna, Bulgaria, 29 September–3 October 2007; pp. 56–69. 18. 	Otten, F.; Irwin, B.; Thinyane, H. Evaluating text preprocessing to improve compression on maillogs. In Proceedings of the 2009 	Annual Research Conference of the South African Institute of Computer Scientists and Information Technologists, Emfuleni, 	South Africa, 12–14 October 2009; pp. 44–53. []19. 	Liu, J.; Zhu, J.; He, S.; He, P.; Zheng, Z.; Lyu, M.: Extracting hidden structures via iterative clustering for log compression.
	In Proceedings of the 2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE), San Diego, CA, 	USA, 11–15 November 2019; pp. 863–873. [] 
20. 	Hätönen, K.; Boulicaut, J.F.; Klemettinen, Men, M.; Masson, C. Comprehensive log compression with frequent patterns.In International Conference on Data Warehousing and Knowledge Discovery; Springer: Berlin/Heidelberg, Germany, 2003 ; pp. 360–370.
	[] 
21. 	Marjai, P.; Lehotay-Kéry, P.; Kiss, A. The Use of Template Miners and Encryption in Log Message Compression. Computers 2021, 	10, 83. []22. 	He, P.; Zhu, J.; He, S.; Li, J.; Lyu, M.R. An evaluation study on log parsing and its use in log mining. In Proceedings of the 2016 	46th Annual IEEE/IFIP International Conference on Dependable Systems and Networks, Toulouse, France, 28 June–1 July 2016; 	pp. 654–661. [] 
23. 	Makanju, A.; ywood, A.N.; Milios, E.E. A lightweight algorithm for message type extraction in system application logs.IEEE Trans. Knowl. Data Eng. 2011, 24, 1921–1936. [] 
Messaoudi, S.; Panichella, A.; Bianculli, D.; Briand,uskas, R. A search-based approach for accurate identification of log 24.
	message formats. In Proceedings of the 26th Conference on Program Comprehension, Gothenburg, Sweden, 27 May–3 June 2018; 	pp. 167–177. [] 
25. 	Deb, K.; Prataarwal, S.; Meyarivan, T.A.M.T. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Trans.Evol. Comput. 2002, 6, 182–197. [] 
26. 	Sivanandam, S.N.; Deepa, S.N. gorithms. In Introduction to Genetic Algorithms; Springer: Berlin/Heidelberg, Germany, 	2008; pp. 15–37. [] 
27. 	Syswerda, G. Unssover in genetic algorithms. In Proceedings of the Third International Conference on Genetic Algorithms; 	Morgan Kaufmann Publishers: Burlington, MA, USA, 1989; pp. 2–9.28. 	Branke, J.; Deb, K.; Dierolf, H.; Osswald, M. Finding knees in multi-objective optimization. In International Conference on Parallel 	Problem Solving from Nature; Springer: Berlin/Heidelberg, Germany, 2004; pp. 722–731.
Burrows, M.; Wheeler, D. A block-sorting lossless data compression algorithm. In Digital SRC Research Report; Digital Systems 29.
Research Center: Palo Alto, CA, USA, 1994 .30. 	Ziv, J.; Lempel, A. A universal algorithm for sequential data compression. IEEE Trans. Inf. Theory 1977, 23, 337–343. [] 31. 	Bell, T.; Witten, I.H.; Cleary, J.G. Modeling for text compression. ACM Comput. Surv. 1989, 21, 557–591. [] 
32. 	Cleary, J.; Witten, I. Data compression using adaptive coding and partial string matching. IEEE Trans. Co4, 32, 396–402.
	[][] 
33. 	Huffman, D. A. A method for the construction of minimum-redundancy codes. Proc. IRE 1952, 40, 1098–1101. [] 34. 	Moffat, A.; Zobel, J.; Sharman, N. Text compression for dynamic document databases. IEEE Trans. Knowl. D 1997, 9, 	302–313. [] 
35. 	Shannon, C. E. A mathematical theory of communication. Bell Syst. Tech. J. 1948, 27, 379–423. []36. 	Ferragina, P.; González, R.; Navarro, G.; Venturini, R. Compressed text indexes: From theory t. J. Exp. Algorithm. 2009, 	13, 1.12–1.31. [] 
37. 	Dahuffman Python Library. Available online:  (accessed on 22 September 2021). 38. 	7-Zip. Available online: