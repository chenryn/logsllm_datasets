### Increasing Class Distance: Training with Universal Backdoors

Training models with universal backdoors shows a reasonable improvement over the original models, increasing class distance from 46.13% to 69.74%. However, this approach is less effective than MOTH, which achieves an average improvement of 74.36%, a difference of 24.62 percentage points. Due to its poor cost-effectiveness, Pairwise can take up to 1683.52 minutes to train a model (on GTSRB), which is 22.75 times slower than MOTH. The two backdoor-erasing techniques, NC and NAD, show limited improvements in class distance, with 8.33% for NC and 5.13% for NAD on average.

Overall, MOTH outperforms NC, NAD, UAP, and Universal in terms of class distance hardening, and it is more efficient than Pairwise, achieving similar distance improvements. Similar observations are made for adversarially trained models, as shown in Table VI. MOTH can improve the class distance by 52.49% with only a 0.37% accuracy degradation and no robustness degradation on average. Universal backdoors perform similarly on both naturally and adversarially trained models, increasing class distance from 16.26% to 37.13%, but still 23% lower than MOTH. Pairwise remains inefficient, with a maximum training time of 2122.95 minutes, or approximately one and a half days, for adversarially trained models.

### Comparison of Different Methods on Hardening Class Distance for Adversarially Trained Models

| Method | Time (min) | Class Distance Increase (%) | Accuracy Degradation (%) | Robustness Degradation (%) |
|--------|------------|-----------------------------|--------------------------|----------------------------|
| NAD    | 155.98     | 6.11                        | 201.52                   | 0                          |
| NC     | 150.53     | 3.73                        | 224.85                   | 0                          |
| Universal | 150.53   | 43.32                       | 262.28                   | 0                          |
| Pairwise | 399.74    | 55.19                       | 276.04                   | 0                          |
| MOTH   | 5.00       | 27.84                       | 1.24                     | 0                          |

### Efficiency of MOTH

Despite Pairwise's similar performance to MOTH on both naturally and adversarially trained models, MOTH is significantly more efficient. Figure 14 illustrates the training time comparison, where the x-axis represents different models and the y-axis represents training time in minutes. The numbers on top of each bar indicate the speedup of MOTH over Pairwise. MOTH achieves a speedup of 1.8 to 22.8 for natural models and 3.0 to 29.8 for adversarial models. Pairwise is particularly slow on datasets with many classes, such as LISA (18 classes) and GTSRB (43 classes), where it is 17x-19x and 22x-29x slower than MOTH, respectively. This inefficiency is due to the quadratic complexity of orthogonalization, which becomes very expensive for models with a large number of classes if scheduling is not in place.

### Selection of TrojAI Models

We evaluate 30 randomly selected TrojAI benign models and study the performance of different methods on hardening class distance. We use random seed 1030792629 to select models from TrojAI round 4 and random seed 186270393 to select 59 poisoned models. The class distance measurement is computationally expensive, especially for models with many classes, and we run three times on each model for accurate measurements. Evaluating one TrojAI model typically takes days, including MOTH and all baselines.

### Extensions to Other Settings

#### Extension to Other Domains

While this paper primarily focuses on computer vision tasks, backdoors in other domains, such as natural language processing (NLP), have different definitions of being stealthy and semantic-aware. For example, in NLP, backdoors are usually characters or words that do not change the overall meaning of sentences. The class distance can be defined as the number of characters or words in generated backdoors. Existing backdoor generation techniques may not be directly applicable, so a possible proposal is to use a sigmoid function to approximate discrete values, allowing existing optimization methods to generate minimal backdoors. The training process can then follow MOTH by inserting minimal backdoors in normal sentences.

#### Application to Other Backdoors

Although our threat model focuses on static backdoors, we also test MOTH on other backdoor types, including reflection, composite, and filter backdoors. For reflection and composite attacks, we use pre-trained poisoned models and open-source repositories to generate four poisoned models. For filter attacks, we leverage 28 poisoned models from the TrojAI round 4 dataset. The experimental results show that MOTH is effective against reflection attacks, reducing the attack success rate (ASR) to 8.98% on average, while NAD reduces it to 71.43% on average. For composite attacks, MOTH reduces the ASR from 87.60-99.51% to 17.60-44.50%, whereas NAD only reduces one CIFAR model to 46.80%. For filter backdoors, MOTH can reduce the ASR to <3.1% for 23 out of 28 models and by 48.44-80% for the remaining five models.

### Measurement for Filter Backdoors

Filter backdoors are dynamic, meaning the backdoor transformation is input-specific. The class distance measured by static backdoors may not be optimal for filter backdoors. We use the magnitude of mean and standard deviation for transforming a set of samples as the distance measure. Specifically, we minimize the change in mean and standard deviation with respect to those of input samples. This modified version, MOTHfilter, can reduce the ASRs from 100% to nearly 0% for 27 models and to around 10% for one model (ID 817). These results demonstrate MOTH's capability when extended to eliminate other types of backdoors.

### Future Work

We plan to study more diverse backdoor types in the future, further enhancing the robustness and efficiency of MOTH.