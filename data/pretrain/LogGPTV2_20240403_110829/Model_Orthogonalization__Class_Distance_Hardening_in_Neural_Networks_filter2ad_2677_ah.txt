increase the class distance. Training with universal backdoors
has reasonable improvement over the original models, from
46.13% to 69.74%. However, it is inferior to MOTH, with
74.36% improvement difference on average. Due to its poor
cost-effectiveness, Pairwise can take up to 1683.52 minutes to
train a model (on GTSRB), which is 22.75 times slower than
MOTH. The two backdoor-erasing techniques have limited
improvements on class distance, with 8.33% for NC and 5.13%
for NAD on average. Overall, MOTH outperforms NC, NAD,
UAP, and Universal in terms of class distance hardening, and
Pairwise in terms of efficiency with similar distance improve-
ment. We have similar observations on adversarially trained
models as shown in Table VI. MOTH can improve the class
distance by 52.49% with only 0.37% accuracy degradation
and no robustness degradation on average. Universal has a
similar performance on adversarially trained models as on
natural ones. It can increase class distance from 16.26% to
37.13% with an average 23% lower than MOTH. Pairwise has
low efficiency as discussed earlier. For adversarially trained
models, it has a even larger time cost, with a maximum training
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
161387
TABLE VI: Comparison of different methods on hardening
class distance for adversarially trained models.
D M
Increase ADeg. RDeg.
Method
Time
Dist.
Rob.
Acc.
0
1
-
R
A
F
I
C
0
5
t
e
N
s
e
R
A
S
I
L
N
N
C
0
2
t
e
N
s
e
R
B
R
S
T
G
N
N
i
Average
NAD 77.60% 44.50%
Adversarial 78.45% 42.30% 973.47 195.90
NC 77.21% 42.70% 155.98 224.85
6.11 201.52
Universal 77.60% 42.10% 150.53 262.28
Pairwise 77.63% 42.50% 399.74 276.04
91.90 297.40
MOTH 77.63% 41.70%
5.00 213.70
Adversarial 75.43% 25.00%
8.54 228.81
NC 73.44% 24.43%
0.14 198.26
NAD 70.17% 23.57%
Universal 74.15% 33.14%
4.00 270.17
Pairwise 76.85% 33.71% 258.99 312.77
17.47 278.11
MOTH 74.72% 31.14%
17.40 215.07
Adversarial 80.54% 36.57%
79.76 233.03
NC 78.13% 41.86%
0.93 245.20
NAD 80.97% 40.00%
Universal 79.69% 42.29%
26.28 251.59
Pairwise 81.25% 41.71% 770.11 265.71
43.34 283.53
MOTH 81.25% 41.00%
Adversarial 90.96% 68.70%
88.51
54.98
85.79
NC 91.55% 63.60% 111.72
0.50
95.25
NAD 88.12% 54.30%
Universal 90.25% 69.70%
26.00 118.41
Pairwise 91.62% 72.60% 2122.95 162.12
71.29 142.10
MOTH 90.32% 70.50%
Natural 81.35% 43.14% 262.71 178.30
89.00 193.12
NC 80.08% 43.15%
1.92 185.06
NAD 79.22% 40.59%
Universal 80.42% 46.81%
51.70 225.61
Pairwise 81.84% 47.63% 887.95 254.16
56.00 250.29
MOTH 80.98% 46.09%
-
-
-
27.84% 1.24% 0.00%
3.73% 0.85% 0.00%
37.13% 0.85% 0.20%
43.32% 0.82% 0.00%
55.19% 0.82% 0.60%
-
-
-
16.05% 1.99% 0.57%
3.51% 5.26% 1.43%
29.08% 1.28% 0.00%
59.74% 0.00% 0.00%
59.01% 0.71% 0.00%
-
-
-
11.06% 2.41% 0.00%
15.27% 0.00% 0.00%
16.26% 0.85% 0.00%
27.58% 0.00% 0.00%
35.00% 0.00% 0.00%
-
-
-
-2.60% 0.00% 5.10%
6.34% 2.84% 14.40%
35.47% 0.70% 0.00%
76.89% 0.00% 0.00%
60.76% 0.64% 0.00%
-
-
-
13.09% 1.27% 0.00%
7.21% 2.13% 2.55%
29.49% 0.93% 0.00%
51.88% 0.00% 0.00%
52.49% 0.37% 0.00%
Fig. 15: Comparison of different warm-ups for naturally
trained ResNet20 on CIFAR-10.
the baselines). We hence resort to measuring on 100 randomly
selected class pairs for studying the class distance. We set the
random seed to be the sum of 165838010 and the model id, in
order to avoid selecting the same set of class pairs for models
with the same number of classes.
F. Comparison of Warm-up Strategies
We present the final pairwise class distances for different
warm-up strategies in Figure 15. Each cell in the heat map
denotes the class distance improvement from a source class
(row) to a target class (column). The brightness of the color
denotes the class distance (the brighter the larger). Observe
that the two heat maps are almost identical, meaning our
approximation is effective in solving the cold-start problem.
G. Extensions to Other Settings
Extension to Other Domains. The main focus of this paper
is computer vision tasks. Backdoors in other domains have
Fig. 14: Comparison of Pairwise and MOTH in terms of
training cost. The x-axis denotes different models, where the
first letter denotes the dataset (C for CIFAR-10, S for SVHN, L
for LISA, and G for GTSRB) and the remaining letters denote
the model structure. The y-axis denotes the training time in
minutes. The numbers on top of bars show the speedup of
MOTH over Pairwise.
time of 2122.95 minutes, which is around one and a half
days. Its class distance improvement is similar to MOTH. NC
and NAD have slightly better performances on adversarially
trained models than naturally trained models, with an average
of 13.09% and 7.21%, respectively, which are inferior to other
baselines and MOTH.
D. Efficiency of MOTH
Since Pairwise has a similar performance on both naturally
trained and adversarially trained models to MOTH, we fur-
ther study their efficiency for producing a hardened model.
Figure 14 shows the training time of Pairwise and MOTH.
The x-axis denotes the models and the y-axis denotes the
training time in minutes. The numbers on top of each bar
show the speedup of MOTH over Pairwise. Observe that MOTH
has a speedup of 1.8 to 22.8 for natural models and 3.0 to
29.8 for adversarial models. Moreover, Pairwise is extremely
slow on datasets with many classes. For instance, the LISA
datasets have 18 classes and the GTSRB dataset has 43 classes.
Pairwise is 17x-19x slower on LISA and 22x-29x slower on
GTSRB than MOTH. This is due to the quadratic complexity of
orthogonalization, which becomes very expensive for models
with a large number of classes if scheduling is not in place.
MOTH, on the other hand, is efficient even for models with
many classes and has a competitive performance with Pairwise
on class distance improvement.
E. Selection of TrojAI Models
We evaluate on 30 randomly selected TrojAI benign models
from the official website [42] and study the performance of
different methods on hardening class distance. We use random
seed 1030792629 to choose from the list of benign models
from TrojAI round 4. We also use random seed 186270393 to
select 59 poisoned models for the study in Section VI-B. We
use Python 3.6.9 and NumPy 1.19.5.
As discussed in Section V-A, the class distance measure-
ment is conducted for all class pairs, which is computationally
expensive, especially for models with many classes. We also
run 3 times on each model to have a more accurate measure-
ment of class distances. For TrojAI models, it usually takes
days to evaluate one single model (including MOTH and all
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
171388
C_RN20C_NiNC_VGGC_RN50S_NiNS_RN32L_CNNL_RN20G_NiNModel0500100015002000Time (m)4.11.85.54.03.23.115.719.122.83.33.04.04.33.03.314.817.829.8Nat_PairwiseAdv_PairwiseNat_MOTHAdv_MOTHplanecarbirdcatdeerdogfroghorseshiptruckplanecarbirdcatdeerdogfroghorseshiptruckPairwise Warm-upplanecarbirdcatdeerdogfroghorseshiptruckApproximate Warm-up5075100125150175TABLE VII: Evaluation on other backdoor types.
MOTH
Original
NAD
Accuracy
ASR Accuracy
ASR Accuracy
ASR
Backdoor
Attack
Subject
Reflection
Composite
GTSRB
CIFAR ID 1
CIFAR ID 2
CIFAR ID 3
71.22% 83.33% 65.28% 20.83% 87.50% 0.00%
86.13% 96.60% 78.63% 75.17% 82.30% 8.92%
84.45% 95.78% 76.59% 92.56% 83.11% 15.33%
84.93% 99.10% 77.83% 97.16% 81.30% 11.67%
MNIST
FMNIST
CIFAR
SVHN
99.51% 99.51% 99.31% 97.10% 98.39% 17.60%
93.09% 97.20% 90.90% 92.10% 90.04% 38.00%
82.70% 87.60% 79.09% 46.80% 80.42% 44.50%
94.90% 89.24% 93.49% 66.15% 91.67% 37.00%
different definitions of being stealthy and semantic-aware.
For example, in natural language processing (NLP) domain,
backdoors are usually characters or words that do not change
the overall meaning of original sentences. We can define
the class distance as the number of characters or words of
generated backdoors. As characters or words are discrete (i.e.,
either in the sentence or not), existing backdoor generation
techniques may not be directly applicable. A possible proposal
is to use a sigmoid function to approximate the discrete value
such that existing optimization methods can be leveraged to
generate minimal backdoors. The training process can then
follow MOTH by inserting minimal backdoors in normal sen-
tences. The measure of backdoors in domains such as Android
apps [84] requires domain-specific constraints such as the
injected code not being executed [84]. These constraints may
be considered in the loss function during backdoor generation,
similar to minimizing the L1 norm of the mask. For instance,
we can use a sigmoid function to approximate the executability
of a piece of code (which is discrete) and add this to the loss
function. A large weight can be applied on this loss part to
encourage the loss to be zero. We will leave the experimental
exploration to our future work.
Application to Other Backdoors. Although the threat model
of our paper focuses on static backdoors, we also test MOTH
on other backdoor types, including reflection backdoors [3],
composite backdoors [4], and filter backdoors [41]. For the
reflection attack, we download a pre-trained poisoned model
from the original GitHub repository [85] and also train another
three poisoned models. For the composite attack, we use the
open-source repository from the original paper [86] to generate
four poisoned models. For the filter attack, we leverage 28
poisoned models from the TrojAI round 4 dataset [42]. We use
the same random seed as in Section VI-B for the selection.
The experimental results on reflection and composite attacks
are shown in Table VII. The first column denotes different
backdoor attacks. The second column presents subject models
for evaluation. The third and fourth columns show the accu-
racies and ASRs of poisoned models. The following columns
show the results after applying NAD [39] and MOTH. We can
observe that MOTH is effective against the reflection attack,
reducing the ASR down to 8.98% on average, whereas NAD
can only reduce the ASR to 71.43% on average. For the
composite attack, MOTH is able to eliminate around half of
the backdoor effect (reducing the ASR from 87.60-99.51%
to 17.60-44.50%). NAD, on the other hand, is only able to
reduce one model on CIFAR to 46.80%. The other three
Fig. 16: ASRs of filter-poisoned models before/after repair.
models after applying NAD still have more than 65% ASR,
rendering NAD ineffective against complex backdoor attacks
such as composite backdoors. The experimental results on
the filter attack are shown in Figure 16. The x-axis denotes
the model IDs and the y-axis denotes the attack success rate
(ASR). Bars in the light colors denote the ASR of the injected
backdoors before erasing/hardening and the dark color after.
Filter backdoors are more pervasive than static backdoors
by transforming all
image. Model
hardening using minimal static backdoors may not eliminate
filter backdoors. In our results, for 23 out of the 28 filter-
poisoned models, MOTH can still reduce the ASR down to
<3.1%. For five other cases (ID 54, ID 467, ID 747, ID 817,
and ID 982), MOTH is able to reduce the ASRs by 48.44-80%.
The accuracy degradation on clean samples is minimal for all
the approaches on average (< 0.3%) and omitted.
the pixels on an input
As filter backdoors are dynamic, meaning that the backdoor
transformation is input-specific. The class distance measured
by static backdoors may not be optimal for filter backdoors.
We hence resort
to a different measurement for the filter
case. Specifically, we use the magnitude of mean and standard
deviation for transforming a set of samples as the distance in
the following.
∀x ∈ X, min
(cid:1) + λ · (|µ − ¯µX| + |σ − ¯σX|),
L(cid:0)M(x′), yt
µ,σ
T(x, µ, σ) = x′ = (x − µx)/σx · σ + µ,
where µ and σ are the mean and standard deviation for
the backdoor transformation, respectively. µx and σx are
the mean and standard deviation of individual inputs, and
¯µX and ¯σX are the averages across all the samples. Instead
of minimizing the L1 of the mask in Equation 1, here we
minimize the change of the mean and standard deviation with
respect to those of input samples, i.e., |µ − ¯µX| + |σ − ¯σX|.
This is the distance measurement for the filter scenario. Note
that such a measurement does not change the definition of
class distance minT(E(∥T(x)−x∥)) in Definition III.2, where
the distance is the expectation of some measurement between
transformed data T(x) and original data x for a set of
samples. Based on the above measurement, we modify the
original MOTH by replacing the optimization in Equation 1
and Equation 2 with the above two equations. The results
in green bars in Figure 16 show that the modified version
MOTHfilter can reduce the ASRs from 100% to nearly 0% for
27 models and to around 10% for one model (ID 817). These
results demonstrate the capability of MOTH when extended to
eliminate other types of backdoors. We plan to study more
diverse backdoor types in the future.
Authorized licensed use limited to: Tsinghua University. Downloaded on August 07,2022 at 13:03:02 UTC from IEEE Xplore.  Restrictions apply. 
181389
NADMOTHMOTHfilter