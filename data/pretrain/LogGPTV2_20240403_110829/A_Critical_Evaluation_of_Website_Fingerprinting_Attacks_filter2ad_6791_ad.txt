(non-popular)
Figure 7: Evaluation of the BDR in a ∼35K open-world for
multiple values of the prior P (M ). According to the ALAD
data set, P (M ) = 0.18 for 4 popular domains (google.com,
facebook.com, twitter.com and wikipedia.org); P (M ) = 0.03
for popular homepages and P (M ) = 0.005 for non-popular
homepages (4 random pages from ALAD with Alexa rank
> 500).
Nevertheless, assuming a uniform distribution of pages
introduces a statistical bias because it underestimates the
probability of visiting popular pages.
In order to give a
more accurate estimation of the prior we extracted statistics
about visits from the ALAD dataset. In particular, we mea-
sured frequency with which the users requested the URLs
of the four monitored pages and its domains. We obtained
P (M ) = 0.1852 for domains and P (M ) = 0.0352 for home-
pages. We found that in our case, where the attacker only
identiﬁes home pages, the BDR tends to just 53.1% (Fig-
ure 7).
We believe that a real-world adversary would not mon-
itor homepages such as google.com or facebook.com. As a
more plausible WF attack we have in mind a nation state-
like adversary who is interested in identifying the access to
speciﬁc pages that are diﬃcult to block, such as an entry in
a whistle-blower’s blog hosted in a diﬀerent country. These
pages would presumably have a lower base rate than pages
listed in Alexa.
To investigate the consequences of a very low prior we
also calculated the BDR for monitoring a set of non-popular
pages. We counted the number of visits in ALAD for 4
random pages that have a rank higher than 500 in Alexa,
such as careerbuilder.com (rank 697) and cracked.com
(rank 658). As expected, with a prior of 0.005 the BDR is
much lower (marked with circles in Figure 7) and tending to
13.8%. Yet, note that these are just upper bounds because
these monitored pages appear in the Alexa list and might
be considered popular. We suspect that BDR for even more
unpopular pages would be so low that would render a WF
attack ineﬀective in this scenario.
User’s browsing habits
In this section, we study the performance of a classiﬁer
that is trained on Alexa Top 100 sites and test it using real-
world user visited sites. The goal is to check how successful
an adversary, trained according to prior WF attacks, would
be to ﬁnd suspected pages on a set of pages a real-world user
browsed.
We used the logs of three randomly chosen users from
the ALAD and randomly picked 100 URLs from each. We
crawled the URLs and collected data to feed the W classiﬁer.
During classiﬁcation we mapped the test URLs to their top
level domains. For example, dell.msn.com was mapped to
msn.com. We chose to do this to not to overwhelm the classi-
ﬁer with false positives when it matches an inner page with
the homepage. The results, summarized in Table 9 show
a clear failure of the classiﬁer in identifying the pages that
these users were browsing.
ALAD User
TP
User 3
User 13
User 42
38/260
56/356
3/208
FP
362/400
344/400
397/400
Table 9: TPR and FPR for each of the users using a classiﬁer
trained on 36 traces from Alexa Top 100 sites and tested on
randomly chosen 100 sites visited by ALAD User 3, 13 and
42. Here, ntrain = 4, ntest = 1, Ttrain = 36, ttest = 4,
m = 10 and k = 100.
Note that the true positive rate is the number of correct
predictions over the number of predictions in which the page
can be found in Alexa. The false positive rate is calculated
as the number of misclassiﬁcations over the total number of
predictions.
One possible reason for low TPR is due to the eﬀect of
inner pages. Inner pages are pages in the website that are
not the homepage. We distinguish between two types of
inner pages: (i) private, only accessible to the user through
authentication (e.g., pages in Facebook or email accounts),
and (ii), public, that is pages that are accessible for any
web user but that it is not the homepage of the site. Other
work has claimed that private inner pages do not matter
because the TBB cleans session storage and a user has to
load the login page after each session. However, we believe
it is common that users leave the TBB open and visit the
same inner page repeatedly within one single session. The
high FPR is because the supervised classiﬁer cannot output
‘Unknown’ for pages that do not exist in the training set,
thus chooses to output a page in the training set that is the
closest to the test page.
5. CLASSIFY-VERIFY
In standard supervised learning, a classiﬁer chooses at
least one class even when the target class is unavailable.
For example, in the open-world scenario when a classiﬁer is
Algorithm 1 Modiﬁed Classify-Verify
Input: Test page D, suspect pages A = A1, ..An and prob-
Output: AD if AD ∈ A and ‘Unknown’ otherwise
ability scores
(cid:46) Train a classiﬁer
CA → classiﬁer trained on A
VA → veriﬁer for A
(cid:46) Calculate threshold for the veriﬁer
t → threshold maximizing Fβ score
(cid:46) Test page D
Classify D
PD → Veriﬁcation score
if PD >= t then
Accept the classiﬁer’s output and return it
else
Reject the classiﬁer’s output and return ‘Unknown’
end if
and then tested on the remaining 10%. For Fβ score, we
choose β = 0.5 as we want to give more priority to precision
than recall. We experimented with other β values and F0.5
ﬁnds the best threshold (0.21 for open-world) that gives low
false positives without reducing the TPR.
Our result shows that Classify-Verify reduces the number
of false positives signiﬁcantly without reducing the TPR.
The new FPR after Classify-Verify is ∼ 0.68. In the largest
experiment (with ∼ 35K pages) the number of false positive
reduces from 647 to 235, which is over 63% drop. The FPR
can be reduced even further by sacriﬁcing the TPR. The
threshold estimated using Dif f = P 1 − P 2 and P 1 both
perform similarly in our case.
Similarly for the users in ALAD, we determine the thresh-
old using cross-validation. The number of false positive
drops signiﬁcantly (Table 10) over 50% for each of the three
users.
ALAD User TP
User 3
User 13
User 42
38/260
56/356
3/208
FP
362/400
344/400
397/400
New TP New FP
31.2/260
26.8/356
1.0/208
107.6/400
32/400
41.2/400
Table 10: Classify-Verify result on the ALAD users. The
number of FP drops by around 200.
The adversary can also use a pre-determined threshold
instead of computing it every time. For example, in the
open-world case if we had just chosen a threshold of 0.5 we
could have discarded even more false positives with a little
drop in true positives. Other more sophisticated approaches
can be applied to choose a threshold, for example measuring
the variance between intra- and inter-class instances. How-
ever, even after classify-verify the number false positive is
lower than before but still very high. The most diﬃcult
cases are the high conﬁdence false positives which indicate
the cases where the features from censored and uncensored
pages overlap.
We computed the BDR before and after applying the ver-
iﬁcation step. We used the estimation of the prior based on
the prevalence of the homepages of the monitored websites
in the ALAD dataset. The results show that the BDR dou-
bles when we use the Classify-Verify approach. However,
the BDR is still very low due to the exceptionally high FPR
in this speciﬁc setting. For this reason, we conclude that
Figure 8: Estimated probability scores of the true positive
and false positive instances during the open-world experi-
ment (Section 4.8). Probability scores for the false positives
are much lower than that of true positives. With a thresh-
old ∼0.2 most of the false positives can be discarded without
reducing the true positive rate.
trained on web pages A, B and C and tested on page D,
it will choose a page from A, B and C, although the origi-
nal page (D) is absent. In the open-world experiment this
introduces many false positives.
During classiﬁcation, a classiﬁer can output its conﬁdence
in the classiﬁcation decision in terms of posterior probabil-
ity. Although standard SVM classiﬁer (classiﬁer W) does not
output probabilities, an additional sigmoid function can be
trained to map the SVM outputs into probabilities5. One
way to reduce the false positive rate is to inspect the prob-
abilities estimated by the classiﬁer and reject the classi-
ﬁer’s decision when the probabilities are lower than a certain
threshold. This is a form of Abstaining classiﬁcation [5]. In
this paper, we use the modiﬁed “Classify-Verify” approach
as discussed by Stolerman et al. [27].
In the open-world experiment (Section 4.8), the probabil-
ity scores for true positive instances are higher than the false
positive instances (Figure 8). Note that, this was a multi-
class classiﬁcation with 100 classes, so the random probabil-
ity score of an instance is 0.01.
The Classify-Verify approach adds an extra veriﬁcation
step after the classiﬁcation. In our case, the veriﬁcation pro-
cess relies on a threshold that can be determined by training
(Algorithm 1). When a page (D) is tested using the clas-
siﬁer, it outputs the probability scores of D == Ai where
Ai ∈ A, sites in the training set. We use two veriﬁcation
scores based on these estimated probabilities: the maximum
estimated probability, P 1, and the diﬀerence between the
maximum probability and the second highest probability,
Dif f = P 1 − P 2. If the veriﬁcation score of D is less than
the determined threshold, the classiﬁer’s output will be re-
jected. Unlike Stolerman et al. [27], we maximize Fβ instead
of F 1 to choose threshold by adjusting weights for precision
and recall. β ≤ 0.5 achieves fewer false positives at the cost
of true positives than β > 0.5. attacks.
5.1 Evaluation and result
We evaluate the Classify-Verify approach on the results
of the open-world and ALAD experiments. To determine
the threshold for a dataset, we use 10-fold cross-validation,
where a threshold is determined by using 90% of the data
5In LibSVM, this can be achieved by simply using the -b
option during training and testing [17].
Classify-Verify does not solve the issue completely but can
be useful to partially mitigate the impact of false positives.
6. MODELING THE ADVERSARY’S COST
In this section, we model the cost of an adversary to main-
tain a WF system and discuss the scenarios in which the
attack is most threatening. Current research considers only
one scenario where the adversary has the maximum infor-
mation about users. Even when the adversary has all possi-
ble information, collecting, maintaining and updating these
information can be costly. For example, our anecdotal expe-
rience shows that the traﬃc footprint of Google homepage
signiﬁcantly changes due to diﬀerent images (doodles) em-
bedded on the page.
A typical WF system requires 4 tasks: data collection,
training, testing and updating.
Data collection cost: At ﬁrst the adversary needs to col-
lect data from the training pages. In order to maximize the
classiﬁer accuracy, the adversary may want to train with
diﬀerent localized versions of the same webpage and collect
these under diﬀerent settings, e.g., diﬀerent TBB versions,
user settings, entry guard conﬁgurations. If we denote the
number of training pages by n, and assume that on aver-
age webpages have m versions that are diﬀerent enough to
reduce the classiﬁer’s accuracy, the number of pages the ad-
versary needs to collect is D = n × m × i, where i is the
number of instances per page. We denote the data collec-
tion cost as col(D). This cost includes both network and
storage costs.
Training Cost: In the training phase an adversary needs
to train his classiﬁer with the collected data. The training
cost includes the cost of measuring features F and training
a classiﬁer C. So the cost of training the system once would
be train(D, F, C). If c denotes the cost of training with a
single instance of a traﬃc trace, then the cost of training the
system once would be train(D, F, C) = D × c.
Testing Cost: For testing a trace, the adversary needs to
collect test data T , extract features F and test using the
classiﬁer C. Let v denote the number of monitored victims
and p denote the average number of pages accessed by each
victim per day. Then the amount of test data is T = v × p.
The total test cost is col(T ) + test(T, F, C).
Updating Cost: To maintain the performance of the clas-
siﬁer, the adversary needs to update the system over time.
For example the adversary might try to keep the accuracy
of the classiﬁer above a certain threshold (e.g., 50%). The
updating costs include the cost of updating the data (D),
measuring the features (F ) and retraining the classiﬁer (C),
which is denoted as update(D, F, C). If, on average, web-
pages change d day periods, the daily updating cost would
be update(D,F,C)
Then, the total cost of an adversary to maintain a WF sys-
tem is:
d
.
init(D, F, C, T ) = col(D) + train(D, F, C) +col(T ) + test(T, F, C)
cost(D, F, C, T ) = init(D, F, C, T ) +
update(D, F, C)
d
To give a more concrete example, our experiment to measure
the eﬀect of time to classiﬁer accuracy, we found that after
d = 10 days the accuracy attained was under 50% and thus
the adversary would have needed to update his data. The
update(D,F,C)
would not show the impossibility of a success-
ful WF attack, but it could show that to maintain such an
attack can be prohibitively expensive even for adversaries
with high level of resources.
10
The adversary could also have extra costs before the train-
ing and the data collection. For example, the attacker could
try to discover background information about the victim(s)
that can be used to increase the eﬃciency of the attack. He
could also try to discover properties about the Internet con-
nection of the user passively. However, in perspective of the
results of the previous sections, the amount of background
information required to mitigate the eﬀect of noise in the
data can be much larger than previously expected. Further,
a question that lingers is to what extent, given such amount
of background information, the WF attack is still necessary.
7. CONCLUSION AND FUTURE WORK
In this paper we studied the practical feasibility of WF
attacks. We are not dismissing WF as a threat, but we sug-
gest that more attention should be paid to the practicality
of the scenarios in which these attacks are evaluated.