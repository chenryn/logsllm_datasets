title:Blacklistable anonymous credentials: blocking misbehaving users without
ttps
author:Patrick P. Tsang and
Man Ho Au and
Apu Kapadia and
Sean W. Smith
Blacklistable Anonymous Credentials:
Blocking Misbehaving Users without TTPs
Patrick P. Tsang†, Man Ho Au§, Apu Kapadia†‡, Sean W. Smith†
†Department of Computer Science
Dartmouth College
Hanover, NH, USA
‡Institute for Security Technology Studies
Dartmouth College
Hanover, NH, USA
§Centre for Computer and Information Security Research
School of Computer Science and Software Engineering
University of Wollongong, Australia
{patrick, akapadia, sws}@cs.dartmouth.edu, PI:EMAIL
ABSTRACT
Several credential systems have been proposed in which
users can authenticate to services anonymously.
Since
anonymity can give users the license to misbehave, some
variants allow the selective deanonymization (or linking)
of misbehaving users upon a complaint to a trusted third
party (TTP). The ability of the TTP to revoke a user’s pri-
vacy at any time, however, is too strong a punishment for
misbehavior. To limit the scope of deanonymization, sys-
tems such as “e-cash” have been proposed in which users are
deanonymized under only certain types of well-deﬁned mis-
behavior such as “double spending.” While useful in some
applications, it is not possible to generalize such techniques
to more subjective deﬁnitions of misbehavior.
We present the ﬁrst anonymous credential system in which
services can “blacklist” misbehaving users without contact-
ing a TTP. Since blacklisted users remain anonymous, mis-
behaviors can be judged subjectively without users fearing
arbitrary deanonymization by a TTP.
Categories and Subject Descriptors
K.6.5 [Operating Systems]: Security and Protection—
Authentication; E.3 [Data Encryption]: Public key cryp-
tosystems
General Terms
Algorithms, Security
Keywords
privacy,
anonymous blacklisting, revocation
anonymous authentication, user misbehavior,
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for proﬁt or commercial advantage and that copies
bear this notice and the full citation on the ﬁrst page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior speciﬁc
permission and/or a fee.
CCS’07, October 29–November 2, 2007, Alexandria, Virginia, USA.
Copyright 2007 ACM 978-1-59593-703-2/07/0010 ...$5.00.
1.
INTRODUCTION
Several cryptographic schemes allow users to authenti-
cate to service providers (SPs) anonymously. While anony-
mous authentication oﬀers users a high degree of privacy,
it can give users the license to misbehave without the fear
of punishment. For example, Wikipedia1 has allowed edi-
tors to modify content anonymously, and as a result sev-
eral users have misbehaved by posting inappropriate con-
tent. SPs, therefore, desire some level of accountability
against misbehaving users. Several anonymous credential
systems have been proposed in which users can be selectively
deanonymized or have their accesses linked (pseudonymized)
under special circumstances. As we will discuss, for certain
applications the existing schemes are either too punitive—
deanonymization (or linking) is unreasonably harsh, and of-
ten relies on trusted third parties (TTPs) capable of revoking
a user’s privacy at any time—or too restrictive—allowing
deanonymization under only certain narrowly deﬁned types
of misbehavior.
Deanonymizing a user is not always necessary to discour-
age misbehavior; in some cases it is suﬃcient to simply block
misbehaving users from making future accesses, while main-
taining their anonymity. We call this property anonymous
blacklisting. For example, anonymous access at SPs such
as Wikipedia and YouTube2 empowers users to disseminate
content without the fear of persecution—a user may add po-
litical content on Wikipedia that is forbidden by his or her
government, or post a video of police brutality to YouTube.
In such cases, while Wikipedia and YouTube may want to
penalize users who deface webpages or post copyrighted ma-
terial, it is of paramount importance for SPs to preserve the
anonymity of their well-behaving users. By guaranteeing
anonymity to all users, anonymous blacklisting allows SPs
to penalize misbehavior without the risk of exposing legiti-
mate users such as political dissenters. We now discuss why
existing solutions are not desirable for such applications.
Anonymous credential systems that support accountabil-
ity (such as Camenisch and Lysyanskaya’s [12, 15] and
schemes based on group signatures [18, 1, 6, 26]) feature a
1http://www.wikipedia.org
2http://www.youtube.com
72TTP called the Open Authority (OA). The OA is capable of
identifying (or linking) the user behind any anonymous au-
thentication. Anonymous credential systems with dynamic
membership revocation [13, 2, 7, 28], mostly constructed
from dynamic accumulators [13], also feature a TTP that is
capable of deanonymizing (or linking) users. The existence
of such a TTP, however, is undesirable—users can never be
assured that their privacy will be maintained by the TTP.
Deﬁning the circumstances under which a TTP can expose
a user, and ensuring its trustworthiness to judge fairly, is an
undue burden on SPs. For such applications, therefore, a
system without TTPs is desirable.
To eliminate the reliance on TTPs, certain “threshold-
based” approaches such as e-cash [3, 10, 11] and k-Times
Anonymous Authentication (k-TAA) [32, 29, 33, 4] have
been proposed.
In these schemes, users are guaranteed
anonymity unless they authenticate more than a certain
number of threshold times. For example, spending an e-
coin twice (an undesirable action) or authenticating k + 1
times in a k-TAA scheme, provides the SP with enough
information to compute the user’s identity. Linkable ring
signatures [27, 36, 37] and periodic n-times anonymous au-
thentication [9] also fall into this category. Unfortunately,
misbehavior cannot always be deﬁned in terms of threshold
values. For example, “inappropriate” edits to a Wikipedia
page, or “oﬀensive” video uploads to YouTube are usually
identiﬁed based on human subjectivity. For such applica-
tions, therefore, subjective judging is desirable.
1.1 Related Solutions
To reiterate, it is important to have an anonymous cre-
dential system in which users can be blacklisted in a way
that (1) preserves their anonymity, (2) is based on subjective
deﬁnitions of misbehavior, and (3) does not rely on a TTP.
Though not intended for anonymous blacklisting, Syverson
et al. present a scheme [31] that ensures that users can
perform anonymous and serial transactions at an SP. The
SP issues blind tokens to users, which are renewed at the
end of a user’s transaction. The SP can block future con-
nections from a user by simply not issuing a new token at
the end of a transaction (e.g., if the user fails to pay for
continued service). The major drawback to this approach
is that misbehavior must be judged while the user is on-
line. Indeed, their scheme was not designed for blacklisting
users since misbehavior is usually identiﬁed long after a user
has disconnected. Recently, some of the authors of this pa-
per proposed the Nymble system [25] to allow SPs to block
misbehaving users hiding behind an anonymizing network
such as Tor [21]. Nymble makes several practical considera-
tions for anonymous IP-address blocking based on subjective
judging, but it does rely on multiple entities that can collude
to deanonymize (or link) a misbehaving user.
Even though it may seem that the ability to block future
accesses from subjectively-judged misbehaving users inher-
ently requires a TTP capable of deanonymizing (or linking)
users, we show that this is not the case.
1.2 Our Contributions
We propose the BLacklistable Anonymous Credential
(BLAC) system, the ﬁrst cryptographic construction of
an anonymous credential system that supports anonymous
blacklisting and subjective judging without relying on TTPs
that are capable of revoking the privacy of users at will. We
formalize the security model for such a system, under which
we prove that our construction is secure. Furthermore, we
provide an implementation of our BLAC system and evaluate
its performance both analytically and experimentally.
Paper Outline.
We provide an overview of our BLAC system in Section 2
and formalize the model and security properties in Section 3.
In Section 4 we present preliminary information on the var-
ious cryptographic tools and assumptions used in our con-
struction, which we present in Section 5. We present an
experimental evaluation of our construction in Section 6, a
discussion of several issues in Section 7, and ﬁnally conclude
in Section 8. The interested reader is directed to the Ap-
pendix for a detailed security model and the full version of
this paper [35] for security proofs.
2. OUR APPROACH
We provide a high-level overview of our BLacklistable
Anonymous Credential (BLAC) system in this section, and
defer cryptographic details to the subsequent sections.
In our system, users authenticate to Service Providers
(SPs) anonymously using credentials issued by a Group
Manager (GM). The GM is responsible for enrolling legiti-
mate users into the system by issuing credentials to them.3
These credentials are private to the user, and not known
by the GM. We emphasize that the GM is not a TTP that
can compromise the privacy of users, and is trusted only to
enroll legitimate users into the system, and issue at most
one credential per user. SPs are willing to serve anonymous
users as long as they are legitimate users in the system (by
enrolling themselves with the GM), and have never misbe-
haved thus far, where misbehavior may be arbitrarily deﬁned
and subjectively judged by each individual SP. We describe
this process next.
The novelty of our approach is that SPs maintain their
own blacklists of misbehaving users without knowing the
identity of the misbehaving users. Users anonymously au-
thenticating to the SP must ﬁrst prove that they are not on
the SP’s blacklist (otherwise authentication will fail). Fol-
lowing a user’s authentication, SPs store a ticket extracted
from the protocol transcript of the authentication. Later, if
the SP deems the user to have misbehaved during the au-
thenticated session, possibly long after the user has discon-
nected, the SP can add the ticket as an entry in its blacklist.4
If a user Alice detects that she is on the blacklist, she termi-
nates the authentication and disconnects immediately. The
SP, therefore, learns only that some anonymous blacklisted
user was refused a connection, i.e., the SP does not learn
the identity of the blacklisted user, and the user is anony-
mous within the set of blacklisted users. Users not on the
blacklist will be able to authenticate successfully, and the
SPs learn only that the user is not on the blacklist. Fur-
thermore, our system allows SPs to remove entries from the
blacklist, thereby forgiving past misbehaviors.5 Depending
3Who is a legitimate user and how to verify such legitimacy
are application-dependent.
4In practice, the SP may privately log arbitrary information
about an authenticated session that is necessary for it to
judge at a later time whether the anonymous user misbe-
haved during that session.
5Adding and removing blacklist entries are atomic actions
as will be discussed in Section 7.
on the severity of misbehavior, a user may be blacklisted
for varying periods of time—using inappropriate language
could correspond to being blacklisted for one week, whereas
posting copyrighted material could correspond to blacklist-
ing for one month. Users are always assured that if they
successfully authenticate to an SP their access will always
remain anonymous—all that an SP can do is block future
accesses by a misbehaving user.
A glimpse into tickets.
Tickets are a vital object in our BLAC system. A ticket
is the only piece in the authentication protocol transcript
that contains information about the identity of the authen-
ticating user. Here we describe some of the properties these
tickets must possess for the system to be secure. First, tick-
ets have to be the output of some non-invertible mapping
of the user’s credential. If this were not the case, the sys-
tem would have no anonymity. Also, tickets from the same
user should be unlinkable, for otherwise SPs would be able
to tell if two authentications are from the same user. This
property implies that the mapping just mentioned must also
take as input some randomness, as a deterministic mapping
implies linkability. Furthermore, tickets must be such that
it is possible to prove and verify that a ticket is correctly
formed, and that a ticket does not belong to a given user
(if it is indeed the case). Without such a property, a user
blacklisted by an SP would still be able to authenticate to
the SP. As we will see, this property is also a necessary con-
dition to prevent misbehaving users from being “framed” (by
other users for example).
Remark. Our BLAC system may be conﬁgured to allow
or disallow the sharing of blacklist entries (tickets) between
SPs. Sharing a blacklist entry would allow multiple SPs to
block a user who misbehaved at one of the SPs. We will ﬁrst
present the system where such sharing is disallowed and then
point out how to allow sharing in Section 7.
3. MODEL
We present the syntax of the Blacklistable Anonymous
Credential (BLAC) system, followed by security properties
that any construction of the BLAC system must satisfy.
3.1 Syntax
The entities in the BLAC system are the Group Manager
(GM), a set of Service Providers (SPs) and a set of users.
The BLAC system consists of the following protocols:
3.1.1 Setup
This algorithm is executed by the GM to set up the sys-
tem. On input of one or more security parameters, the al-
gorithm outputs a pair consisting of a group public key gpk
and a group private key gsk. The GM publishes gpk and
keeps gsk private.
3.1.2 Registration
This protocol is executed between the GM and a legiti-
mate user to register the user into the system. Upon suc-
cessful completion of the protocol, the user obtains a creden-
tial cred, which she keeps private to herself, and is thereby
enrolled as a member in the group of registered users.
3.1.3 Authentication
This protocol is executed between a user with credential
cred and an SP. When an execution of the protocol termi-
nates, the SP outputs a binary value of success or failure.
If the SP outputs success in an execution of the protocol,
we call the execution a successful authentication and say
that the authenticating user has succeeded in authenticating
herself; otherwise the authentication is unsuccessful and the
user has failed. Only upon a successful authentication does
the SP establish an authenticated session with the authen-
ticating user during which the user can access the service
provided by the SP. Note that the protocol transcript of a
successful authentication as seen by the SP is useful for the
SP to blacklist the authenticating user, as described next.
3.1.4 Blacklist Management
This is a suite of three algorithms: Extract, Add and
Remove, which are executed by SPs for managing their
blacklists. On input of an authentication protocol tran-
script, Extract extracts and returns a ticket from the tran-
script. A blacklist is a collection of tickets. On input of
a blacklist and a ticket, Add returns a new blacklist that
contains all the tickets in the input blacklist as well as the
input ticket. On the other hand, on input of a blacklist and a
ticket, Remove returns a new blacklist that contains all the
tickets in the input blacklist, except the one(s) equivalent to
the input ticket.6
When we say that a user Alice is blacklisted by an SP Bob,
we mean that there exists an authentication between Alice
and Bob such that Bob has added the ticket extracted from
the authentication transcript to his blacklist and has not
removed it (yet). Otherwise Alice is not blacklisted by Bob.
Also, we say that Alice is misbehaving with respect to Bob
if she is blacklisted by Bob. Otherwise, she is well-behaving.
Correctness.
Any construction of the BLAC system must be correct:
Definition 1
(Correctness). A construction of the
BLAC system is correct if all entities in the system are honest
(i.e., they follow the system’s speciﬁcation) implies that for
any registered legitimate user Alice and for any SP Bob,
Alice is able to successfully authenticate herself to Bob with
overwhelming probability if Alice is not blacklisted by Bob
during the authentication.
3.2 Security Notions