### Textual Improvements and Clarifications

**Mistakes in Predictions:**
The model can err by predicting "allow" instead of "deny" and vice versa (refer to Section VII-C for a detailed discussion on the implications of such errors). Notably, there is minimal variation between the outcomes for BLR (Bayesian Logistic Regression) and SVM (Support Vector Machine).

**Sensitivity of MAE to Decision Types:**
The Mean Absolute Error (MAE) sensitivity to different types of decisions is evident in the poorer performance of ZeroRt in Figure 6(a) compared to Figure 6(b). ZeroRt often predicts with an MAE of 2, which is ignored under the 0-1 loss but not under the MAE loss, leading to its worse performance with the latter. Conversely, context-aware methods perform consistently under both loss functions.

**Individual Performance Comparison:**
Figure 7 illustrates the individual performances for 20 participants across one random partition with \( t_u = 100\% \). The figure compares MAE losses from baselines and SVM against those from BLR. Each point represents the MAE for a participant, estimated using Equation (3) with \( N_u = 20 \) and \( U = 1 \). The static policy, ZeroRt, and SVM methods are denoted by crosses, circles, and dots, respectively. A thin grey line connects the two MAEs for the same participant. Points above the dashed grey line indicate that the corresponding baseline performs worse than BLR, which is the case for most participants. There is no clear winner among the baselines or between BLR and SVM. These MAE values are less stable due to being estimated with only 20 points, with standard errors around 0.1 for many participants. The numbers in Figure 6 are more reliable, as they are based on 400 test decisions (20 participants with 20 test decisions each). Nonetheless, Figure 7 demonstrates that adding context improves performance across participants.

**Variance Across Participants:**
The variance in MAE across participants is high, with some participants having near-zero MAE, while others have MAE as high as 0.7. However, the overall aggregate performance is satisfactory, as seen in Figure 6(c), where each participant contributes 20 test decisions. For poorly predicted participants, more data or consistency in their decisions may be needed.

**Feature Importance for Prediction:**
Figure 8 examines the importance of individual features for prediction. It presents box-plots of MAE for 50 different partitions with \( t_u = 100\% \). The top two box-plots represent static policy and ZeroRt. The next four box-plots show BLR performance with one of the following features: (A) app name, (B) method (API call), (C) method category (contacts, location, storage), and (D) whether the app is in the foreground. Even with a single feature, improvements over the baselines are observed. Feature D (app in foreground) is particularly effective, achieving a median error of 0.25. The 7th box-plot shows performance with all four features (A+B+C+D), slightly better than the last box-plot, which uses all 37 features. This behavior is expected with small sample sizes.

**Computational Performance:**
BLR is simple enough to run on smartphones. We evaluated its computational performance on a Motorola Moto G 3rd generation smartphone with Android 5.1.1, using 7 contextual features, 5-fold cross-validation, and approximately 200 decisions from a single participant. Training took about 1.32 ± 0.31 seconds, and prediction took 50 ± 6 microseconds, with CPU usage below 50%. This indicates that our approach is feasible on smartphones, especially if training occurs infrequently, such as during charging. Future work will explore sequential updating (online learning) to further reduce training time.

**Impact on Smartphone Performance:**
We estimated the impact of our SmarPer prototype, particularly the service collecting context information and intercepting app requests. Using OS and third-party tools, we found no significant impact on CPU usage. Battery life was also unaffected, as measured by Android’s Battery monitor API. No participants reported battery issues.

### Discussion

**Amount of Training Data vs. Model Complexity:**
Figures 6(a) and 6(b) show that BLR's MAE and ICR continue to decrease with more data, while ZeroRt flattens out around \( t_u = 80\% \). More data can improve BLR's performance, but ZeroRt benefits little from additional data. Given the wide variance in user preferences, collecting data from a large number of users is recommended. The amount of data per user depends on their preferences and the dimensionality of contextual features. Larger datasets enable the use of advanced models like topic models, which can capture diverse privacy preferences. In our experiments, non-linear models (GP-SE, SVM, decision trees) only marginally improved performance over linear models, suggesting that the current dataset is insufficient for complex models. Future work will involve collecting more data and evaluating advanced models.

**Automating Permission Decisions:**
We conducted experiments to evaluate how BLR automates decisions to reduce user overhead. We estimated model confidence and set thresholds to decide whether to automate or prompt the user. While we lack sufficient data for concrete conclusions, we face the challenge of determining when the model is accurate enough to start automating. One approach is to randomly prompt users and limit the number of requests per day. Our data collection phase suggests that all data are useful for prediction, implying that an automatic system might collect a few responses daily until enough decisions are gathered. We recommend starting with a subset of apps and gradually adding more. Another option is to use data from similar users to accelerate learning. Regular training and testing can monitor model performance, and exploration-exploitation trade-offs can balance prompts and automated decisions. The acceptable MAE is user-dependent and can be determined through long-term studies or interviews.

**Impact of Predicting Permission Decisions:**
SmarPer aims to emulate users' privacy behaviors. Incorrect predictions can lead to privacy and utility losses. Oversharing (predicting "allow" instead of "deny") and undersharing (predicting "deny" instead of "allow") can result in privacy and functionality issues. Partial-oversharing and partial-undersharing (predicting "obfuscate" instead of "deny" or "allow") can cause varying degrees of privacy and utility loss. BLR and SVM rarely make large mistakes, with BLR showing significantly lower error rates compared to static policies. Errors can be further reduced with cost-sensitive training, allowing users to configure the type of errors to minimize.

**Data Obfuscation:**
Obfuscation was well-received by participants, with 29% of decisions being obfuscate. Similar fractions were observed across data types. In exit surveys, 73% of participants found obfuscation useful, and 80% expressed interest in obfuscating additional data types. Some participants, however, did not find it useful or experienced issues with certain apps. Coordination with developers and mobile platform providers can help reduce these problems.

**Privacy Benefits of Per-User Models:**
Previous works aggregate user data to train one-size-fits-all classifiers, introducing privacy risks. SmarPer trains models per user on their smartphone, avoiding the need to send permission information to other parties. BLR requires partial decision data for hyperparameter learning, but users can send sufficient statistics to defend against inference attacks. Advanced models like SVMs do not need this but may require more data.

**Limitations:**
Challenges include participant bias towards privacy-preserving behavior, focus on popular apps, and the need for more data to improve model accuracy. Future work should address these limitations to enhance the robustness and reliability of the system.